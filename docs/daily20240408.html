<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Sculpting Holistic 3D Representation in Contrastive Language-Image-3D\n  Pre-training", "author": "Yipeng Gao and Zeyu Wang and Wei-Shi Zheng and Cihang Xie and Yuyin Zhou", "abstract": "  Contrastive learning has emerged as a promising paradigm for 3D open-world\nunderstanding, i.e., aligning point cloud representation to image and text\nembedding space individually. In this paper, we introduce MixCon3D, a simple\nyet effective method aiming to sculpt holistic 3D representation in contrastive\nlanguage-image-3D pre-training. In contrast to point cloud only, we develop the\n3D object-level representation from complementary perspectives, e.g.,\nmulti-view rendered images with the point cloud. Then, MixCon3D performs\nlanguage-3D contrastive learning, comprehensively depicting real-world 3D\nobjects and bolstering text alignment. Additionally, we pioneer the first\nthorough investigation of various training recipes for the 3D contrastive\nlearning paradigm, building a solid baseline with improved performance.\nExtensive experiments conducted on three representative benchmarks reveal that\nour method significantly improves over the baseline, surpassing the previous\nstate-of-the-art performance on the challenging 1,156-category Objaverse-LVIS\ndataset by 5.7%. The versatility of MixCon3D is showcased in applications such\nas text-to-3D retrieval and point cloud captioning, further evidencing its\nefficacy in diverse scenarios. The code is available at\nhttps://github.com/UCSC-VLAA/MixCon3D.\n", "link": "http://arxiv.org/abs/2311.01734v2", "date": "2024-04-05", "relevancy": 2.8941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5502}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sculpting%20Holistic%203D%20Representation%20in%20Contrastive%20Language-Image-3D%0A%20%20Pre-training&body=Title%3A%20Sculpting%20Holistic%203D%20Representation%20in%20Contrastive%20Language-Image-3D%0A%20%20Pre-training%0AAuthor%3A%20Yipeng%20Gao%20and%20Zeyu%20Wang%20and%20Wei-Shi%20Zheng%20and%20Cihang%20Xie%20and%20Yuyin%20Zhou%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20emerged%20as%20a%20promising%20paradigm%20for%203D%20open-world%0Aunderstanding%2C%20i.e.%2C%20aligning%20point%20cloud%20representation%20to%20image%20and%20text%0Aembedding%20space%20individually.%20In%20this%20paper%2C%20we%20introduce%20MixCon3D%2C%20a%20simple%0Ayet%20effective%20method%20aiming%20to%20sculpt%20holistic%203D%20representation%20in%20contrastive%0Alanguage-image-3D%20pre-training.%20In%20contrast%20to%20point%20cloud%20only%2C%20we%20develop%20the%0A3D%20object-level%20representation%20from%20complementary%20perspectives%2C%20e.g.%2C%0Amulti-view%20rendered%20images%20with%20the%20point%20cloud.%20Then%2C%20MixCon3D%20performs%0Alanguage-3D%20contrastive%20learning%2C%20comprehensively%20depicting%20real-world%203D%0Aobjects%20and%20bolstering%20text%20alignment.%20Additionally%2C%20we%20pioneer%20the%20first%0Athorough%20investigation%20of%20various%20training%20recipes%20for%20the%203D%20contrastive%0Alearning%20paradigm%2C%20building%20a%20solid%20baseline%20with%20improved%20performance.%0AExtensive%20experiments%20conducted%20on%20three%20representative%20benchmarks%20reveal%20that%0Aour%20method%20significantly%20improves%20over%20the%20baseline%2C%20surpassing%20the%20previous%0Astate-of-the-art%20performance%20on%20the%20challenging%201%2C156-category%20Objaverse-LVIS%0Adataset%20by%205.7%25.%20The%20versatility%20of%20MixCon3D%20is%20showcased%20in%20applications%20such%0Aas%20text-to-3D%20retrieval%20and%20point%20cloud%20captioning%2C%20further%20evidencing%20its%0Aefficacy%20in%20diverse%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/UCSC-VLAA/MixCon3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01734v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sculpting%20Holistic%203D%20Representation%20in%20Contrastive%20Language-Image-3D%0A%20%20Pre-training&entry.906535625=Yipeng%20Gao%20and%20Zeyu%20Wang%20and%20Wei-Shi%20Zheng%20and%20Cihang%20Xie%20and%20Yuyin%20Zhou&entry.1292438233=%20%20Contrastive%20learning%20has%20emerged%20as%20a%20promising%20paradigm%20for%203D%20open-world%0Aunderstanding%2C%20i.e.%2C%20aligning%20point%20cloud%20representation%20to%20image%20and%20text%0Aembedding%20space%20individually.%20In%20this%20paper%2C%20we%20introduce%20MixCon3D%2C%20a%20simple%0Ayet%20effective%20method%20aiming%20to%20sculpt%20holistic%203D%20representation%20in%20contrastive%0Alanguage-image-3D%20pre-training.%20In%20contrast%20to%20point%20cloud%20only%2C%20we%20develop%20the%0A3D%20object-level%20representation%20from%20complementary%20perspectives%2C%20e.g.%2C%0Amulti-view%20rendered%20images%20with%20the%20point%20cloud.%20Then%2C%20MixCon3D%20performs%0Alanguage-3D%20contrastive%20learning%2C%20comprehensively%20depicting%20real-world%203D%0Aobjects%20and%20bolstering%20text%20alignment.%20Additionally%2C%20we%20pioneer%20the%20first%0Athorough%20investigation%20of%20various%20training%20recipes%20for%20the%203D%20contrastive%0Alearning%20paradigm%2C%20building%20a%20solid%20baseline%20with%20improved%20performance.%0AExtensive%20experiments%20conducted%20on%20three%20representative%20benchmarks%20reveal%20that%0Aour%20method%20significantly%20improves%20over%20the%20baseline%2C%20surpassing%20the%20previous%0Astate-of-the-art%20performance%20on%20the%20challenging%201%2C156-category%20Objaverse-LVIS%0Adataset%20by%205.7%25.%20The%20versatility%20of%20MixCon3D%20is%20showcased%20in%20applications%20such%0Aas%20text-to-3D%20retrieval%20and%20point%20cloud%20captioning%2C%20further%20evidencing%20its%0Aefficacy%20in%20diverse%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/UCSC-VLAA/MixCon3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01734v2&entry.124074799=Read"},
{"title": "Localization Is All You Evaluate: Data Leakage in Online Mapping\n  Datasets and How to Fix It", "author": "Adam Lilja and Junsheng Fu and Erik Stenborg and Lars Hammarstrand", "abstract": "  The task of online mapping is to predict a local map using current sensor\nobservations, e.g. from lidar and camera, without relying on a pre-built map.\nState-of-the-art methods are based on supervised learning and are trained\npredominantly using two datasets: nuScenes and Argoverse 2. However, these\ndatasets revisit the same geographic locations across training, validation, and\ntest sets. Specifically, over $80$% of nuScenes and $40$% of Argoverse 2\nvalidation and test samples are less than $5$ m from a training sample. At test\ntime, the methods are thus evaluated more on how well they localize within a\nmemorized implicit map built from the training data than on extrapolating to\nunseen locations. Naturally, this data leakage causes inflated performance\nnumbers and we propose geographically disjoint data splits to reveal the true\nperformance in unseen environments. Experimental results show that methods\nperform considerably worse, some dropping more than $45$ mAP, when trained and\nevaluated on proper data splits. Additionally, a reassessment of prior design\nchoices reveals diverging conclusions from those based on the original split.\nNotably, the impact of lifting methods and the support from auxiliary tasks\n(e.g., depth supervision) on performance appears less substantial or follows a\ndifferent trajectory than previously perceived. Splits can be found at\nhttps://github.com/LiljaAdam/geographical-splits\n", "link": "http://arxiv.org/abs/2312.06420v2", "date": "2024-04-05", "relevancy": 2.8735, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6317}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5715}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5209}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Localization%20Is%20All%20You%20Evaluate%3A%20Data%20Leakage%20in%20Online%20Mapping%0A%20%20Datasets%20and%20How%20to%20Fix%20It&body=Title%3A%20Localization%20Is%20All%20You%20Evaluate%3A%20Data%20Leakage%20in%20Online%20Mapping%0A%20%20Datasets%20and%20How%20to%20Fix%20It%0AAuthor%3A%20Adam%20Lilja%20and%20Junsheng%20Fu%20and%20Erik%20Stenborg%20and%20Lars%20Hammarstrand%0AAbstract%3A%20%20%20The%20task%20of%20online%20mapping%20is%20to%20predict%20a%20local%20map%20using%20current%20sensor%0Aobservations%2C%20e.g.%20from%20lidar%20and%20camera%2C%20without%20relying%20on%20a%20pre-built%20map.%0AState-of-the-art%20methods%20are%20based%20on%20supervised%20learning%20and%20are%20trained%0Apredominantly%20using%20two%20datasets%3A%20nuScenes%20and%20Argoverse%202.%20However%2C%20these%0Adatasets%20revisit%20the%20same%20geographic%20locations%20across%20training%2C%20validation%2C%20and%0Atest%20sets.%20Specifically%2C%20over%20%2480%24%25%20of%20nuScenes%20and%20%2440%24%25%20of%20Argoverse%202%0Avalidation%20and%20test%20samples%20are%20less%20than%20%245%24%20m%20from%20a%20training%20sample.%20At%20test%0Atime%2C%20the%20methods%20are%20thus%20evaluated%20more%20on%20how%20well%20they%20localize%20within%20a%0Amemorized%20implicit%20map%20built%20from%20the%20training%20data%20than%20on%20extrapolating%20to%0Aunseen%20locations.%20Naturally%2C%20this%20data%20leakage%20causes%20inflated%20performance%0Anumbers%20and%20we%20propose%20geographically%20disjoint%20data%20splits%20to%20reveal%20the%20true%0Aperformance%20in%20unseen%20environments.%20Experimental%20results%20show%20that%20methods%0Aperform%20considerably%20worse%2C%20some%20dropping%20more%20than%20%2445%24%20mAP%2C%20when%20trained%20and%0Aevaluated%20on%20proper%20data%20splits.%20Additionally%2C%20a%20reassessment%20of%20prior%20design%0Achoices%20reveals%20diverging%20conclusions%20from%20those%20based%20on%20the%20original%20split.%0ANotably%2C%20the%20impact%20of%20lifting%20methods%20and%20the%20support%20from%20auxiliary%20tasks%0A%28e.g.%2C%20depth%20supervision%29%20on%20performance%20appears%20less%20substantial%20or%20follows%20a%0Adifferent%20trajectory%20than%20previously%20perceived.%20Splits%20can%20be%20found%20at%0Ahttps%3A//github.com/LiljaAdam/geographical-splits%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06420v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localization%20Is%20All%20You%20Evaluate%3A%20Data%20Leakage%20in%20Online%20Mapping%0A%20%20Datasets%20and%20How%20to%20Fix%20It&entry.906535625=Adam%20Lilja%20and%20Junsheng%20Fu%20and%20Erik%20Stenborg%20and%20Lars%20Hammarstrand&entry.1292438233=%20%20The%20task%20of%20online%20mapping%20is%20to%20predict%20a%20local%20map%20using%20current%20sensor%0Aobservations%2C%20e.g.%20from%20lidar%20and%20camera%2C%20without%20relying%20on%20a%20pre-built%20map.%0AState-of-the-art%20methods%20are%20based%20on%20supervised%20learning%20and%20are%20trained%0Apredominantly%20using%20two%20datasets%3A%20nuScenes%20and%20Argoverse%202.%20However%2C%20these%0Adatasets%20revisit%20the%20same%20geographic%20locations%20across%20training%2C%20validation%2C%20and%0Atest%20sets.%20Specifically%2C%20over%20%2480%24%25%20of%20nuScenes%20and%20%2440%24%25%20of%20Argoverse%202%0Avalidation%20and%20test%20samples%20are%20less%20than%20%245%24%20m%20from%20a%20training%20sample.%20At%20test%0Atime%2C%20the%20methods%20are%20thus%20evaluated%20more%20on%20how%20well%20they%20localize%20within%20a%0Amemorized%20implicit%20map%20built%20from%20the%20training%20data%20than%20on%20extrapolating%20to%0Aunseen%20locations.%20Naturally%2C%20this%20data%20leakage%20causes%20inflated%20performance%0Anumbers%20and%20we%20propose%20geographically%20disjoint%20data%20splits%20to%20reveal%20the%20true%0Aperformance%20in%20unseen%20environments.%20Experimental%20results%20show%20that%20methods%0Aperform%20considerably%20worse%2C%20some%20dropping%20more%20than%20%2445%24%20mAP%2C%20when%20trained%20and%0Aevaluated%20on%20proper%20data%20splits.%20Additionally%2C%20a%20reassessment%20of%20prior%20design%0Achoices%20reveals%20diverging%20conclusions%20from%20those%20based%20on%20the%20original%20split.%0ANotably%2C%20the%20impact%20of%20lifting%20methods%20and%20the%20support%20from%20auxiliary%20tasks%0A%28e.g.%2C%20depth%20supervision%29%20on%20performance%20appears%20less%20substantial%20or%20follows%20a%0Adifferent%20trajectory%20than%20previously%20perceived.%20Splits%20can%20be%20found%20at%0Ahttps%3A//github.com/LiljaAdam/geographical-splits%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06420v2&entry.124074799=Read"},
{"title": "SPOT: Self-Training with Patch-Order Permutation for Object-Centric\n  Learning with Autoregressive Transformers", "author": "Ioannis Kakogeorgiou and Spyros Gidaris and Konstantinos Karantzalos and Nikos Komodakis", "abstract": "  Unsupervised object-centric learning aims to decompose scenes into\ninterpretable object entities, termed slots. Slot-based auto-encoders stand out\nas a prominent method for this task. Within them, crucial aspects include\nguiding the encoder to generate object-specific slots and ensuring the decoder\nutilizes them during reconstruction. This work introduces two novel techniques,\n(i) an attention-based self-training approach, which distills superior\nslot-based attention masks from the decoder to the encoder, enhancing object\nsegmentation, and (ii) an innovative patch-order permutation strategy for\nautoregressive transformers that strengthens the role of slot vectors in\nreconstruction. The effectiveness of these strategies is showcased\nexperimentally. The combined approach significantly surpasses prior slot-based\nautoencoder methods in unsupervised object segmentation, especially with\ncomplex real-world images. We provide the implementation code at\nhttps://github.com/gkakogeorgiou/spot .\n", "link": "http://arxiv.org/abs/2312.00648v3", "date": "2024-04-05", "relevancy": 2.7992, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5425}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPOT%3A%20Self-Training%20with%20Patch-Order%20Permutation%20for%20Object-Centric%0A%20%20Learning%20with%20Autoregressive%20Transformers&body=Title%3A%20SPOT%3A%20Self-Training%20with%20Patch-Order%20Permutation%20for%20Object-Centric%0A%20%20Learning%20with%20Autoregressive%20Transformers%0AAuthor%3A%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Konstantinos%20Karantzalos%20and%20Nikos%20Komodakis%0AAbstract%3A%20%20%20Unsupervised%20object-centric%20learning%20aims%20to%20decompose%20scenes%20into%0Ainterpretable%20object%20entities%2C%20termed%20slots.%20Slot-based%20auto-encoders%20stand%20out%0Aas%20a%20prominent%20method%20for%20this%20task.%20Within%20them%2C%20crucial%20aspects%20include%0Aguiding%20the%20encoder%20to%20generate%20object-specific%20slots%20and%20ensuring%20the%20decoder%0Autilizes%20them%20during%20reconstruction.%20This%20work%20introduces%20two%20novel%20techniques%2C%0A%28i%29%20an%20attention-based%20self-training%20approach%2C%20which%20distills%20superior%0Aslot-based%20attention%20masks%20from%20the%20decoder%20to%20the%20encoder%2C%20enhancing%20object%0Asegmentation%2C%20and%20%28ii%29%20an%20innovative%20patch-order%20permutation%20strategy%20for%0Aautoregressive%20transformers%20that%20strengthens%20the%20role%20of%20slot%20vectors%20in%0Areconstruction.%20The%20effectiveness%20of%20these%20strategies%20is%20showcased%0Aexperimentally.%20The%20combined%20approach%20significantly%20surpasses%20prior%20slot-based%0Aautoencoder%20methods%20in%20unsupervised%20object%20segmentation%2C%20especially%20with%0Acomplex%20real-world%20images.%20We%20provide%20the%20implementation%20code%20at%0Ahttps%3A//github.com/gkakogeorgiou/spot%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00648v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPOT%3A%20Self-Training%20with%20Patch-Order%20Permutation%20for%20Object-Centric%0A%20%20Learning%20with%20Autoregressive%20Transformers&entry.906535625=Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Konstantinos%20Karantzalos%20and%20Nikos%20Komodakis&entry.1292438233=%20%20Unsupervised%20object-centric%20learning%20aims%20to%20decompose%20scenes%20into%0Ainterpretable%20object%20entities%2C%20termed%20slots.%20Slot-based%20auto-encoders%20stand%20out%0Aas%20a%20prominent%20method%20for%20this%20task.%20Within%20them%2C%20crucial%20aspects%20include%0Aguiding%20the%20encoder%20to%20generate%20object-specific%20slots%20and%20ensuring%20the%20decoder%0Autilizes%20them%20during%20reconstruction.%20This%20work%20introduces%20two%20novel%20techniques%2C%0A%28i%29%20an%20attention-based%20self-training%20approach%2C%20which%20distills%20superior%0Aslot-based%20attention%20masks%20from%20the%20decoder%20to%20the%20encoder%2C%20enhancing%20object%0Asegmentation%2C%20and%20%28ii%29%20an%20innovative%20patch-order%20permutation%20strategy%20for%0Aautoregressive%20transformers%20that%20strengthens%20the%20role%20of%20slot%20vectors%20in%0Areconstruction.%20The%20effectiveness%20of%20these%20strategies%20is%20showcased%0Aexperimentally.%20The%20combined%20approach%20significantly%20surpasses%20prior%20slot-based%0Aautoencoder%20methods%20in%20unsupervised%20object%20segmentation%2C%20especially%20with%0Acomplex%20real-world%20images.%20We%20provide%20the%20implementation%20code%20at%0Ahttps%3A//github.com/gkakogeorgiou/spot%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00648v3&entry.124074799=Read"},
{"title": "SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric\n  hybrid solution", "author": "Hala Djeghim and Nathan Piasco and Moussab Bennehar and Luis Rold\u00e3o and Dzmitry Tsishkou and D\u00e9sir\u00e9 Sidib\u00e9", "abstract": "  Neural implicit surface representation methods have recently shown impressive\n3D reconstruction results. However, existing solutions struggle to reconstruct\nurban outdoor scenes due to their large, unbounded, and highly detailed nature.\nHence, to achieve accurate reconstructions, additional supervision data such as\nLiDAR, strong geometric priors, and long training times are required. To tackle\nsuch issues, we present SCILLA, a new hybrid implicit surface learning method\nto reconstruct large driving scenes from 2D images. SCILLA's hybrid\narchitecture models two separate implicit fields: one for the volumetric\ndensity and another for the signed distance to the surface. To accurately\nrepresent urban outdoor scenarios, we introduce a novel volume-rendering\nstrategy that relies on self-supervised probabilistic density estimation to\nsample points near the surface and transition progressively from volumetric to\nsurface representation. Our solution permits a proper and fast initialization\nof the signed distance field without relying on any geometric prior on the\nscene, compared to concurrent methods. By conducting extensive experiments on\nfour outdoor driving datasets, we show that SCILLA can learn an accurate and\ndetailed 3D surface scene representation in various urban scenarios while being\ntwo times faster to train compared to previous state-of-the-art solutions.\n", "link": "http://arxiv.org/abs/2403.10344v2", "date": "2024-04-05", "relevancy": 2.7476, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5781}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5406}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5298}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SCILLA%3A%20SurfaCe%20Implicit%20Learning%20for%20Large%20Urban%20Area%2C%20a%20volumetric%0A%20%20hybrid%20solution&body=Title%3A%20SCILLA%3A%20SurfaCe%20Implicit%20Learning%20for%20Large%20Urban%20Area%2C%20a%20volumetric%0A%20%20hybrid%20solution%0AAuthor%3A%20Hala%20Djeghim%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%20and%20D%C3%A9sir%C3%A9%20Sidib%C3%A9%0AAbstract%3A%20%20%20Neural%20implicit%20surface%20representation%20methods%20have%20recently%20shown%20impressive%0A3D%20reconstruction%20results.%20However%2C%20existing%20solutions%20struggle%20to%20reconstruct%0Aurban%20outdoor%20scenes%20due%20to%20their%20large%2C%20unbounded%2C%20and%20highly%20detailed%20nature.%0AHence%2C%20to%20achieve%20accurate%20reconstructions%2C%20additional%20supervision%20data%20such%20as%0ALiDAR%2C%20strong%20geometric%20priors%2C%20and%20long%20training%20times%20are%20required.%20To%20tackle%0Asuch%20issues%2C%20we%20present%20SCILLA%2C%20a%20new%20hybrid%20implicit%20surface%20learning%20method%0Ato%20reconstruct%20large%20driving%20scenes%20from%202D%20images.%20SCILLA%27s%20hybrid%0Aarchitecture%20models%20two%20separate%20implicit%20fields%3A%20one%20for%20the%20volumetric%0Adensity%20and%20another%20for%20the%20signed%20distance%20to%20the%20surface.%20To%20accurately%0Arepresent%20urban%20outdoor%20scenarios%2C%20we%20introduce%20a%20novel%20volume-rendering%0Astrategy%20that%20relies%20on%20self-supervised%20probabilistic%20density%20estimation%20to%0Asample%20points%20near%20the%20surface%20and%20transition%20progressively%20from%20volumetric%20to%0Asurface%20representation.%20Our%20solution%20permits%20a%20proper%20and%20fast%20initialization%0Aof%20the%20signed%20distance%20field%20without%20relying%20on%20any%20geometric%20prior%20on%20the%0Ascene%2C%20compared%20to%20concurrent%20methods.%20By%20conducting%20extensive%20experiments%20on%0Afour%20outdoor%20driving%20datasets%2C%20we%20show%20that%20SCILLA%20can%20learn%20an%20accurate%20and%0Adetailed%203D%20surface%20scene%20representation%20in%20various%20urban%20scenarios%20while%20being%0Atwo%20times%20faster%20to%20train%20compared%20to%20previous%20state-of-the-art%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10344v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCILLA%3A%20SurfaCe%20Implicit%20Learning%20for%20Large%20Urban%20Area%2C%20a%20volumetric%0A%20%20hybrid%20solution&entry.906535625=Hala%20Djeghim%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%20and%20D%C3%A9sir%C3%A9%20Sidib%C3%A9&entry.1292438233=%20%20Neural%20implicit%20surface%20representation%20methods%20have%20recently%20shown%20impressive%0A3D%20reconstruction%20results.%20However%2C%20existing%20solutions%20struggle%20to%20reconstruct%0Aurban%20outdoor%20scenes%20due%20to%20their%20large%2C%20unbounded%2C%20and%20highly%20detailed%20nature.%0AHence%2C%20to%20achieve%20accurate%20reconstructions%2C%20additional%20supervision%20data%20such%20as%0ALiDAR%2C%20strong%20geometric%20priors%2C%20and%20long%20training%20times%20are%20required.%20To%20tackle%0Asuch%20issues%2C%20we%20present%20SCILLA%2C%20a%20new%20hybrid%20implicit%20surface%20learning%20method%0Ato%20reconstruct%20large%20driving%20scenes%20from%202D%20images.%20SCILLA%27s%20hybrid%0Aarchitecture%20models%20two%20separate%20implicit%20fields%3A%20one%20for%20the%20volumetric%0Adensity%20and%20another%20for%20the%20signed%20distance%20to%20the%20surface.%20To%20accurately%0Arepresent%20urban%20outdoor%20scenarios%2C%20we%20introduce%20a%20novel%20volume-rendering%0Astrategy%20that%20relies%20on%20self-supervised%20probabilistic%20density%20estimation%20to%0Asample%20points%20near%20the%20surface%20and%20transition%20progressively%20from%20volumetric%20to%0Asurface%20representation.%20Our%20solution%20permits%20a%20proper%20and%20fast%20initialization%0Aof%20the%20signed%20distance%20field%20without%20relying%20on%20any%20geometric%20prior%20on%20the%0Ascene%2C%20compared%20to%20concurrent%20methods.%20By%20conducting%20extensive%20experiments%20on%0Afour%20outdoor%20driving%20datasets%2C%20we%20show%20that%20SCILLA%20can%20learn%20an%20accurate%20and%0Adetailed%203D%20surface%20scene%20representation%20in%20various%20urban%20scenarios%20while%20being%0Atwo%20times%20faster%20to%20train%20compared%20to%20previous%20state-of-the-art%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10344v2&entry.124074799=Read"},
{"title": "Opti-CAM: Optimizing saliency maps for interpretability", "author": "Hanwei Zhang and Felipe Torres and Ronan Sicre and Yannis Avrithis and Stephane Ayache", "abstract": "  Methods based on class activation maps (CAM) provide a simple mechanism to\ninterpret predictions of convolutional neural networks by using linear\ncombinations of feature maps as saliency maps. By contrast, masking-based\nmethods optimize a saliency map directly in the image space or learn it by\ntraining another network on additional data.\n  In this work we introduce Opti-CAM, combining ideas from CAM-based and\nmasking-based approaches. Our saliency map is a linear combination of feature\nmaps, where weights are optimized per image such that the logit of the masked\nimage for a given class is maximized. We also fix a fundamental flaw in two of\nthe most common evaluation metrics of attribution methods. On several datasets,\nOpti-CAM largely outperforms other CAM-based approaches according to the most\nrelevant classification metrics. We provide empirical evidence supporting that\nlocalization and classifier interpretability are not necessarily aligned.\n", "link": "http://arxiv.org/abs/2301.07002v3", "date": "2024-04-05", "relevancy": 2.7407, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5997}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5063}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Opti-CAM%3A%20Optimizing%20saliency%20maps%20for%20interpretability&body=Title%3A%20Opti-CAM%3A%20Optimizing%20saliency%20maps%20for%20interpretability%0AAuthor%3A%20Hanwei%20Zhang%20and%20Felipe%20Torres%20and%20Ronan%20Sicre%20and%20Yannis%20Avrithis%20and%20Stephane%20Ayache%0AAbstract%3A%20%20%20Methods%20based%20on%20class%20activation%20maps%20%28CAM%29%20provide%20a%20simple%20mechanism%20to%0Ainterpret%20predictions%20of%20convolutional%20neural%20networks%20by%20using%20linear%0Acombinations%20of%20feature%20maps%20as%20saliency%20maps.%20By%20contrast%2C%20masking-based%0Amethods%20optimize%20a%20saliency%20map%20directly%20in%20the%20image%20space%20or%20learn%20it%20by%0Atraining%20another%20network%20on%20additional%20data.%0A%20%20In%20this%20work%20we%20introduce%20Opti-CAM%2C%20combining%20ideas%20from%20CAM-based%20and%0Amasking-based%20approaches.%20Our%20saliency%20map%20is%20a%20linear%20combination%20of%20feature%0Amaps%2C%20where%20weights%20are%20optimized%20per%20image%20such%20that%20the%20logit%20of%20the%20masked%0Aimage%20for%20a%20given%20class%20is%20maximized.%20We%20also%20fix%20a%20fundamental%20flaw%20in%20two%20of%0Athe%20most%20common%20evaluation%20metrics%20of%20attribution%20methods.%20On%20several%20datasets%2C%0AOpti-CAM%20largely%20outperforms%20other%20CAM-based%20approaches%20according%20to%20the%20most%0Arelevant%20classification%20metrics.%20We%20provide%20empirical%20evidence%20supporting%20that%0Alocalization%20and%20classifier%20interpretability%20are%20not%20necessarily%20aligned.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.07002v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opti-CAM%3A%20Optimizing%20saliency%20maps%20for%20interpretability&entry.906535625=Hanwei%20Zhang%20and%20Felipe%20Torres%20and%20Ronan%20Sicre%20and%20Yannis%20Avrithis%20and%20Stephane%20Ayache&entry.1292438233=%20%20Methods%20based%20on%20class%20activation%20maps%20%28CAM%29%20provide%20a%20simple%20mechanism%20to%0Ainterpret%20predictions%20of%20convolutional%20neural%20networks%20by%20using%20linear%0Acombinations%20of%20feature%20maps%20as%20saliency%20maps.%20By%20contrast%2C%20masking-based%0Amethods%20optimize%20a%20saliency%20map%20directly%20in%20the%20image%20space%20or%20learn%20it%20by%0Atraining%20another%20network%20on%20additional%20data.%0A%20%20In%20this%20work%20we%20introduce%20Opti-CAM%2C%20combining%20ideas%20from%20CAM-based%20and%0Amasking-based%20approaches.%20Our%20saliency%20map%20is%20a%20linear%20combination%20of%20feature%0Amaps%2C%20where%20weights%20are%20optimized%20per%20image%20such%20that%20the%20logit%20of%20the%20masked%0Aimage%20for%20a%20given%20class%20is%20maximized.%20We%20also%20fix%20a%20fundamental%20flaw%20in%20two%20of%0Athe%20most%20common%20evaluation%20metrics%20of%20attribution%20methods.%20On%20several%20datasets%2C%0AOpti-CAM%20largely%20outperforms%20other%20CAM-based%20approaches%20according%20to%20the%20most%0Arelevant%20classification%20metrics.%20We%20provide%20empirical%20evidence%20supporting%20that%0Alocalization%20and%20classifier%20interpretability%20are%20not%20necessarily%20aligned.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.07002v3&entry.124074799=Read"},
{"title": "OMH: Structured Sparsity via Optimally Matched Hierarchy for\n  Unsupervised Semantic Segmentation", "author": "Baran Ozaydin and Tong Zhang and Deblina Bhattacharjee and Sabine S\u00fcsstrunk and Mathieu Salzmann", "abstract": "  Unsupervised Semantic Segmentation (USS) involves segmenting images without\nrelying on predefined labels, aiming to alleviate the burden of extensive human\nlabeling. Existing methods utilize features generated by self-supervised models\nand specific priors for clustering. However, their clustering objectives are\nnot involved in the optimization of the features during training. Additionally,\ndue to the lack of clear class definitions in USS, the resulting segments may\nnot align well with the clustering objective. In this paper, we introduce a\nnovel approach called Optimally Matched Hierarchy (OMH) to simultaneously\naddress the above issues. The core of our method lies in imposing structured\nsparsity on the feature space, which allows the features to encode information\nwith different levels of granularity. The structure of this sparsity stems from\nour hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy\namong parallel clusters through Optimal Transport. Our OMH yields better\nunsupervised segmentation performance compared to existing USS methods. Our\nextensive experiments demonstrate the benefits of OMH when utilizing our\ndifferentiable paradigm. We will make our code publicly available.\n", "link": "http://arxiv.org/abs/2403.06546v2", "date": "2024-04-05", "relevancy": 2.5685, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5312}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5106}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4994}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OMH%3A%20Structured%20Sparsity%20via%20Optimally%20Matched%20Hierarchy%20for%0A%20%20Unsupervised%20Semantic%20Segmentation&body=Title%3A%20OMH%3A%20Structured%20Sparsity%20via%20Optimally%20Matched%20Hierarchy%20for%0A%20%20Unsupervised%20Semantic%20Segmentation%0AAuthor%3A%20Baran%20Ozaydin%20and%20Tong%20Zhang%20and%20Deblina%20Bhattacharjee%20and%20Sabine%20S%C3%BCsstrunk%20and%20Mathieu%20Salzmann%0AAbstract%3A%20%20%20Unsupervised%20Semantic%20Segmentation%20%28USS%29%20involves%20segmenting%20images%20without%0Arelying%20on%20predefined%20labels%2C%20aiming%20to%20alleviate%20the%20burden%20of%20extensive%20human%0Alabeling.%20Existing%20methods%20utilize%20features%20generated%20by%20self-supervised%20models%0Aand%20specific%20priors%20for%20clustering.%20However%2C%20their%20clustering%20objectives%20are%0Anot%20involved%20in%20the%20optimization%20of%20the%20features%20during%20training.%20Additionally%2C%0Adue%20to%20the%20lack%20of%20clear%20class%20definitions%20in%20USS%2C%20the%20resulting%20segments%20may%0Anot%20align%20well%20with%20the%20clustering%20objective.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20approach%20called%20Optimally%20Matched%20Hierarchy%20%28OMH%29%20to%20simultaneously%0Aaddress%20the%20above%20issues.%20The%20core%20of%20our%20method%20lies%20in%20imposing%20structured%0Asparsity%20on%20the%20feature%20space%2C%20which%20allows%20the%20features%20to%20encode%20information%0Awith%20different%20levels%20of%20granularity.%20The%20structure%20of%20this%20sparsity%20stems%20from%0Aour%20hierarchy%20%28OMH%29.%20To%20achieve%20this%2C%20we%20learn%20a%20soft%20but%20sparse%20hierarchy%0Aamong%20parallel%20clusters%20through%20Optimal%20Transport.%20Our%20OMH%20yields%20better%0Aunsupervised%20segmentation%20performance%20compared%20to%20existing%20USS%20methods.%20Our%0Aextensive%20experiments%20demonstrate%20the%20benefits%20of%20OMH%20when%20utilizing%20our%0Adifferentiable%20paradigm.%20We%20will%20make%20our%20code%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06546v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMH%3A%20Structured%20Sparsity%20via%20Optimally%20Matched%20Hierarchy%20for%0A%20%20Unsupervised%20Semantic%20Segmentation&entry.906535625=Baran%20Ozaydin%20and%20Tong%20Zhang%20and%20Deblina%20Bhattacharjee%20and%20Sabine%20S%C3%BCsstrunk%20and%20Mathieu%20Salzmann&entry.1292438233=%20%20Unsupervised%20Semantic%20Segmentation%20%28USS%29%20involves%20segmenting%20images%20without%0Arelying%20on%20predefined%20labels%2C%20aiming%20to%20alleviate%20the%20burden%20of%20extensive%20human%0Alabeling.%20Existing%20methods%20utilize%20features%20generated%20by%20self-supervised%20models%0Aand%20specific%20priors%20for%20clustering.%20However%2C%20their%20clustering%20objectives%20are%0Anot%20involved%20in%20the%20optimization%20of%20the%20features%20during%20training.%20Additionally%2C%0Adue%20to%20the%20lack%20of%20clear%20class%20definitions%20in%20USS%2C%20the%20resulting%20segments%20may%0Anot%20align%20well%20with%20the%20clustering%20objective.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20approach%20called%20Optimally%20Matched%20Hierarchy%20%28OMH%29%20to%20simultaneously%0Aaddress%20the%20above%20issues.%20The%20core%20of%20our%20method%20lies%20in%20imposing%20structured%0Asparsity%20on%20the%20feature%20space%2C%20which%20allows%20the%20features%20to%20encode%20information%0Awith%20different%20levels%20of%20granularity.%20The%20structure%20of%20this%20sparsity%20stems%20from%0Aour%20hierarchy%20%28OMH%29.%20To%20achieve%20this%2C%20we%20learn%20a%20soft%20but%20sparse%20hierarchy%0Aamong%20parallel%20clusters%20through%20Optimal%20Transport.%20Our%20OMH%20yields%20better%0Aunsupervised%20segmentation%20performance%20compared%20to%20existing%20USS%20methods.%20Our%0Aextensive%20experiments%20demonstrate%20the%20benefits%20of%20OMH%20when%20utilizing%20our%0Adifferentiable%20paradigm.%20We%20will%20make%20our%20code%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06546v2&entry.124074799=Read"},
{"title": "Part-Attention Based Model Make Occluded Person Re-Identification\n  Stronger", "author": "Zhihao Chen and Yiyuan Ge", "abstract": "  The goal of occluded person re-identification (ReID) is to retrieve specific\npedestrians in occluded situations. However, occluded person ReID still suffers\nfrom background clutter and low-quality local feature representations, which\nlimits model performance. In our research, we introduce a new framework called\nPAB-ReID, which is a novel ReID model incorporating part-attention mechanisms\nto tackle the aforementioned issues effectively. Firstly, we introduce the\nhuman parsing label to guide the generation of more accurate human part\nattention maps. In addition, we propose a fine-grained feature focuser for\ngenerating fine-grained human local feature representations while suppressing\nbackground interference. Moreover, We also design a part triplet loss to\nsupervise the learning of human local features, which optimizes\nintra/inter-class distance. We conducted extensive experiments on specialized\nocclusion and regular ReID datasets, showcasing that our approach outperforms\nthe existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2404.03443v2", "date": "2024-04-05", "relevancy": 2.5424, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5095}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5094}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5065}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Part-Attention%20Based%20Model%20Make%20Occluded%20Person%20Re-Identification%0A%20%20Stronger&body=Title%3A%20Part-Attention%20Based%20Model%20Make%20Occluded%20Person%20Re-Identification%0A%20%20Stronger%0AAuthor%3A%20Zhihao%20Chen%20and%20Yiyuan%20Ge%0AAbstract%3A%20%20%20The%20goal%20of%20occluded%20person%20re-identification%20%28ReID%29%20is%20to%20retrieve%20specific%0Apedestrians%20in%20occluded%20situations.%20However%2C%20occluded%20person%20ReID%20still%20suffers%0Afrom%20background%20clutter%20and%20low-quality%20local%20feature%20representations%2C%20which%0Alimits%20model%20performance.%20In%20our%20research%2C%20we%20introduce%20a%20new%20framework%20called%0APAB-ReID%2C%20which%20is%20a%20novel%20ReID%20model%20incorporating%20part-attention%20mechanisms%0Ato%20tackle%20the%20aforementioned%20issues%20effectively.%20Firstly%2C%20we%20introduce%20the%0Ahuman%20parsing%20label%20to%20guide%20the%20generation%20of%20more%20accurate%20human%20part%0Aattention%20maps.%20In%20addition%2C%20we%20propose%20a%20fine-grained%20feature%20focuser%20for%0Agenerating%20fine-grained%20human%20local%20feature%20representations%20while%20suppressing%0Abackground%20interference.%20Moreover%2C%20We%20also%20design%20a%20part%20triplet%20loss%20to%0Asupervise%20the%20learning%20of%20human%20local%20features%2C%20which%20optimizes%0Aintra/inter-class%20distance.%20We%20conducted%20extensive%20experiments%20on%20specialized%0Aocclusion%20and%20regular%20ReID%20datasets%2C%20showcasing%20that%20our%20approach%20outperforms%0Athe%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03443v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part-Attention%20Based%20Model%20Make%20Occluded%20Person%20Re-Identification%0A%20%20Stronger&entry.906535625=Zhihao%20Chen%20and%20Yiyuan%20Ge&entry.1292438233=%20%20The%20goal%20of%20occluded%20person%20re-identification%20%28ReID%29%20is%20to%20retrieve%20specific%0Apedestrians%20in%20occluded%20situations.%20However%2C%20occluded%20person%20ReID%20still%20suffers%0Afrom%20background%20clutter%20and%20low-quality%20local%20feature%20representations%2C%20which%0Alimits%20model%20performance.%20In%20our%20research%2C%20we%20introduce%20a%20new%20framework%20called%0APAB-ReID%2C%20which%20is%20a%20novel%20ReID%20model%20incorporating%20part-attention%20mechanisms%0Ato%20tackle%20the%20aforementioned%20issues%20effectively.%20Firstly%2C%20we%20introduce%20the%0Ahuman%20parsing%20label%20to%20guide%20the%20generation%20of%20more%20accurate%20human%20part%0Aattention%20maps.%20In%20addition%2C%20we%20propose%20a%20fine-grained%20feature%20focuser%20for%0Agenerating%20fine-grained%20human%20local%20feature%20representations%20while%20suppressing%0Abackground%20interference.%20Moreover%2C%20We%20also%20design%20a%20part%20triplet%20loss%20to%0Asupervise%20the%20learning%20of%20human%20local%20features%2C%20which%20optimizes%0Aintra/inter-class%20distance.%20We%20conducted%20extensive%20experiments%20on%20specialized%0Aocclusion%20and%20regular%20ReID%20datasets%2C%20showcasing%20that%20our%20approach%20outperforms%0Athe%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03443v2&entry.124074799=Read"},
{"title": "MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and\n  Reconstruction in Unbounded Scenes", "author": "Chenyang Wu and Yifan Duan and Xinran Zhang and Yu Sheng and Jianmin Ji and Yanyong Zhang", "abstract": "  Localization and mapping are critical tasks for various applications such as\nautonomous vehicles and robotics. The challenges posed by outdoor environments\npresent particular complexities due to their unbounded characteristics. In this\nwork, we present MM-Gaussian, a LiDAR-camera multi-modal fusion system for\nlocalization and mapping in unbounded scenes. Our approach is inspired by the\nrecently developed 3D Gaussians, which demonstrate remarkable capabilities in\nachieving high rendering quality and fast rendering speed. Specifically, our\nsystem fully utilizes the geometric structure information provided by\nsolid-state LiDAR to address the problem of inaccurate depth encountered when\nrelying solely on visual solutions in unbounded, outdoor scenarios.\nAdditionally, we utilize 3D Gaussian point clouds, with the assistance of\npixel-level gradient descent, to fully exploit the color information in photos,\nthereby achieving realistic rendering effects. To further bolster the\nrobustness of our system, we designed a relocalization module, which assists in\nreturning to the correct trajectory in the event of a localization failure.\nExperiments conducted in multiple scenarios demonstrate the effectiveness of\nour method.\n", "link": "http://arxiv.org/abs/2404.04026v1", "date": "2024-04-05", "relevancy": 2.5324, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6453}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6421}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.58}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MM-Gaussian%3A%203D%20Gaussian-based%20Multi-modal%20Fusion%20for%20Localization%20and%0A%20%20Reconstruction%20in%20Unbounded%20Scenes&body=Title%3A%20MM-Gaussian%3A%203D%20Gaussian-based%20Multi-modal%20Fusion%20for%20Localization%20and%0A%20%20Reconstruction%20in%20Unbounded%20Scenes%0AAuthor%3A%20Chenyang%20Wu%20and%20Yifan%20Duan%20and%20Xinran%20Zhang%20and%20Yu%20Sheng%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang%0AAbstract%3A%20%20%20Localization%20and%20mapping%20are%20critical%20tasks%20for%20various%20applications%20such%20as%0Aautonomous%20vehicles%20and%20robotics.%20The%20challenges%20posed%20by%20outdoor%20environments%0Apresent%20particular%20complexities%20due%20to%20their%20unbounded%20characteristics.%20In%20this%0Awork%2C%20we%20present%20MM-Gaussian%2C%20a%20LiDAR-camera%20multi-modal%20fusion%20system%20for%0Alocalization%20and%20mapping%20in%20unbounded%20scenes.%20Our%20approach%20is%20inspired%20by%20the%0Arecently%20developed%203D%20Gaussians%2C%20which%20demonstrate%20remarkable%20capabilities%20in%0Aachieving%20high%20rendering%20quality%20and%20fast%20rendering%20speed.%20Specifically%2C%20our%0Asystem%20fully%20utilizes%20the%20geometric%20structure%20information%20provided%20by%0Asolid-state%20LiDAR%20to%20address%20the%20problem%20of%20inaccurate%20depth%20encountered%20when%0Arelying%20solely%20on%20visual%20solutions%20in%20unbounded%2C%20outdoor%20scenarios.%0AAdditionally%2C%20we%20utilize%203D%20Gaussian%20point%20clouds%2C%20with%20the%20assistance%20of%0Apixel-level%20gradient%20descent%2C%20to%20fully%20exploit%20the%20color%20information%20in%20photos%2C%0Athereby%20achieving%20realistic%20rendering%20effects.%20To%20further%20bolster%20the%0Arobustness%20of%20our%20system%2C%20we%20designed%20a%20relocalization%20module%2C%20which%20assists%20in%0Areturning%20to%20the%20correct%20trajectory%20in%20the%20event%20of%20a%20localization%20failure.%0AExperiments%20conducted%20in%20multiple%20scenarios%20demonstrate%20the%20effectiveness%20of%0Aour%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04026v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Gaussian%3A%203D%20Gaussian-based%20Multi-modal%20Fusion%20for%20Localization%20and%0A%20%20Reconstruction%20in%20Unbounded%20Scenes&entry.906535625=Chenyang%20Wu%20and%20Yifan%20Duan%20and%20Xinran%20Zhang%20and%20Yu%20Sheng%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang&entry.1292438233=%20%20Localization%20and%20mapping%20are%20critical%20tasks%20for%20various%20applications%20such%20as%0Aautonomous%20vehicles%20and%20robotics.%20The%20challenges%20posed%20by%20outdoor%20environments%0Apresent%20particular%20complexities%20due%20to%20their%20unbounded%20characteristics.%20In%20this%0Awork%2C%20we%20present%20MM-Gaussian%2C%20a%20LiDAR-camera%20multi-modal%20fusion%20system%20for%0Alocalization%20and%20mapping%20in%20unbounded%20scenes.%20Our%20approach%20is%20inspired%20by%20the%0Arecently%20developed%203D%20Gaussians%2C%20which%20demonstrate%20remarkable%20capabilities%20in%0Aachieving%20high%20rendering%20quality%20and%20fast%20rendering%20speed.%20Specifically%2C%20our%0Asystem%20fully%20utilizes%20the%20geometric%20structure%20information%20provided%20by%0Asolid-state%20LiDAR%20to%20address%20the%20problem%20of%20inaccurate%20depth%20encountered%20when%0Arelying%20solely%20on%20visual%20solutions%20in%20unbounded%2C%20outdoor%20scenarios.%0AAdditionally%2C%20we%20utilize%203D%20Gaussian%20point%20clouds%2C%20with%20the%20assistance%20of%0Apixel-level%20gradient%20descent%2C%20to%20fully%20exploit%20the%20color%20information%20in%20photos%2C%0Athereby%20achieving%20realistic%20rendering%20effects.%20To%20further%20bolster%20the%0Arobustness%20of%20our%20system%2C%20we%20designed%20a%20relocalization%20module%2C%20which%20assists%20in%0Areturning%20to%20the%20correct%20trajectory%20in%20the%20event%20of%20a%20localization%20failure.%0AExperiments%20conducted%20in%20multiple%20scenarios%20demonstrate%20the%20effectiveness%20of%0Aour%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04026v1&entry.124074799=Read"},
{"title": "Modeling 3D Surface Manifolds with a Locally Conditioned Atlas", "author": "Przemys\u0142aw Spurek and Sebastian Winczowski and Maciej Zi\u0119ba and Tomasz Trzci\u0144ski and Kacper Kania and Marcin Mazur", "abstract": "  Recently proposed 3D object reconstruction methods represent a mesh with an\natlas - a set of planar patches approximating the surface. However, their\napplication in a real-world scenario is limited since the surfaces of\nreconstructed objects contain discontinuities, which degrades the quality of\nthe final mesh. This is mainly caused by independent processing of individual\npatches, and in this work, we postulate to mitigate this limitation by\npreserving local consistency around patch vertices. To that end, we introduce a\nLocally Conditioned Atlas (LoCondA), a framework for representing a 3D object\nhierarchically in a generative model. Firstly, the model maps a point cloud of\nan object into a sphere. Secondly, by leveraging a spherical prior, we enforce\nthe mapping to be locally consistent on the sphere and on the target object.\nThis way, we can sample a mesh quad on that sphere and project it back onto the\nobject's manifold. With LoCondA, we can produce topologically diverse objects\nwhile maintaining quads to be stitched together. We show that the proposed\napproach provides structurally coherent reconstructions while producing meshes\nof quality comparable to the competitors.\n", "link": "http://arxiv.org/abs/2102.05984v2", "date": "2024-04-05", "relevancy": 2.5004, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5094}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4877}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Modeling%203D%20Surface%20Manifolds%20with%20a%20Locally%20Conditioned%20Atlas&body=Title%3A%20Modeling%203D%20Surface%20Manifolds%20with%20a%20Locally%20Conditioned%20Atlas%0AAuthor%3A%20Przemys%C5%82aw%20Spurek%20and%20Sebastian%20Winczowski%20and%20Maciej%20Zi%C4%99ba%20and%20Tomasz%20Trzci%C5%84ski%20and%20Kacper%20Kania%20and%20Marcin%20Mazur%0AAbstract%3A%20%20%20Recently%20proposed%203D%20object%20reconstruction%20methods%20represent%20a%20mesh%20with%20an%0Aatlas%20-%20a%20set%20of%20planar%20patches%20approximating%20the%20surface.%20However%2C%20their%0Aapplication%20in%20a%20real-world%20scenario%20is%20limited%20since%20the%20surfaces%20of%0Areconstructed%20objects%20contain%20discontinuities%2C%20which%20degrades%20the%20quality%20of%0Athe%20final%20mesh.%20This%20is%20mainly%20caused%20by%20independent%20processing%20of%20individual%0Apatches%2C%20and%20in%20this%20work%2C%20we%20postulate%20to%20mitigate%20this%20limitation%20by%0Apreserving%20local%20consistency%20around%20patch%20vertices.%20To%20that%20end%2C%20we%20introduce%20a%0ALocally%20Conditioned%20Atlas%20%28LoCondA%29%2C%20a%20framework%20for%20representing%20a%203D%20object%0Ahierarchically%20in%20a%20generative%20model.%20Firstly%2C%20the%20model%20maps%20a%20point%20cloud%20of%0Aan%20object%20into%20a%20sphere.%20Secondly%2C%20by%20leveraging%20a%20spherical%20prior%2C%20we%20enforce%0Athe%20mapping%20to%20be%20locally%20consistent%20on%20the%20sphere%20and%20on%20the%20target%20object.%0AThis%20way%2C%20we%20can%20sample%20a%20mesh%20quad%20on%20that%20sphere%20and%20project%20it%20back%20onto%20the%0Aobject%27s%20manifold.%20With%20LoCondA%2C%20we%20can%20produce%20topologically%20diverse%20objects%0Awhile%20maintaining%20quads%20to%20be%20stitched%20together.%20We%20show%20that%20the%20proposed%0Aapproach%20provides%20structurally%20coherent%20reconstructions%20while%20producing%20meshes%0Aof%20quality%20comparable%20to%20the%20competitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2102.05984v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%203D%20Surface%20Manifolds%20with%20a%20Locally%20Conditioned%20Atlas&entry.906535625=Przemys%C5%82aw%20Spurek%20and%20Sebastian%20Winczowski%20and%20Maciej%20Zi%C4%99ba%20and%20Tomasz%20Trzci%C5%84ski%20and%20Kacper%20Kania%20and%20Marcin%20Mazur&entry.1292438233=%20%20Recently%20proposed%203D%20object%20reconstruction%20methods%20represent%20a%20mesh%20with%20an%0Aatlas%20-%20a%20set%20of%20planar%20patches%20approximating%20the%20surface.%20However%2C%20their%0Aapplication%20in%20a%20real-world%20scenario%20is%20limited%20since%20the%20surfaces%20of%0Areconstructed%20objects%20contain%20discontinuities%2C%20which%20degrades%20the%20quality%20of%0Athe%20final%20mesh.%20This%20is%20mainly%20caused%20by%20independent%20processing%20of%20individual%0Apatches%2C%20and%20in%20this%20work%2C%20we%20postulate%20to%20mitigate%20this%20limitation%20by%0Apreserving%20local%20consistency%20around%20patch%20vertices.%20To%20that%20end%2C%20we%20introduce%20a%0ALocally%20Conditioned%20Atlas%20%28LoCondA%29%2C%20a%20framework%20for%20representing%20a%203D%20object%0Ahierarchically%20in%20a%20generative%20model.%20Firstly%2C%20the%20model%20maps%20a%20point%20cloud%20of%0Aan%20object%20into%20a%20sphere.%20Secondly%2C%20by%20leveraging%20a%20spherical%20prior%2C%20we%20enforce%0Athe%20mapping%20to%20be%20locally%20consistent%20on%20the%20sphere%20and%20on%20the%20target%20object.%0AThis%20way%2C%20we%20can%20sample%20a%20mesh%20quad%20on%20that%20sphere%20and%20project%20it%20back%20onto%20the%0Aobject%27s%20manifold.%20With%20LoCondA%2C%20we%20can%20produce%20topologically%20diverse%20objects%0Awhile%20maintaining%20quads%20to%20be%20stitched%20together.%20We%20show%20that%20the%20proposed%0Aapproach%20provides%20structurally%20coherent%20reconstructions%20while%20producing%20meshes%0Aof%20quality%20comparable%20to%20the%20competitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2102.05984v2&entry.124074799=Read"},
{"title": "Attention-Driven Reasoning: Unlocking the Potential of Large Language\n  Models", "author": "Bingli Liao and Danilo Vasconcellos Vargas", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities, but their\nreasoning abilities and underlying mechanisms remain poorly understood. We\npresent a novel approach to enhance LLMs' reasoning through attention mechanism\noptimization, without additional training data. We identify inefficiencies in\nthe attention distribution caused by non-semantic tokens and propose an\nalgorithm to re-balance the skewed distribution, enabling the model to abstract\nmore nuanced knowledge. Our experiments demonstrate significantly improved\nreasoning capabilities, particularly for non-STEM questions. We provide\ninsights into the role of attention patterns in LLMs' reasoning and propose a\nmethod to enhance these abilities, paving the way for more powerful and\nversatile language models.\n", "link": "http://arxiv.org/abs/2403.14932v2", "date": "2024-04-05", "relevancy": 2.4926, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5274}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4849}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4833}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attention-Driven%20Reasoning%3A%20Unlocking%20the%20Potential%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20Attention-Driven%20Reasoning%3A%20Unlocking%20the%20Potential%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Bingli%20Liao%20and%20Danilo%20Vasconcellos%20Vargas%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%2C%20but%20their%0Areasoning%20abilities%20and%20underlying%20mechanisms%20remain%20poorly%20understood.%20We%0Apresent%20a%20novel%20approach%20to%20enhance%20LLMs%27%20reasoning%20through%20attention%20mechanism%0Aoptimization%2C%20without%20additional%20training%20data.%20We%20identify%20inefficiencies%20in%0Athe%20attention%20distribution%20caused%20by%20non-semantic%20tokens%20and%20propose%20an%0Aalgorithm%20to%20re-balance%20the%20skewed%20distribution%2C%20enabling%20the%20model%20to%20abstract%0Amore%20nuanced%20knowledge.%20Our%20experiments%20demonstrate%20significantly%20improved%0Areasoning%20capabilities%2C%20particularly%20for%20non-STEM%20questions.%20We%20provide%0Ainsights%20into%20the%20role%20of%20attention%20patterns%20in%20LLMs%27%20reasoning%20and%20propose%20a%0Amethod%20to%20enhance%20these%20abilities%2C%20paving%20the%20way%20for%20more%20powerful%20and%0Aversatile%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14932v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-Driven%20Reasoning%3A%20Unlocking%20the%20Potential%20of%20Large%20Language%0A%20%20Models&entry.906535625=Bingli%20Liao%20and%20Danilo%20Vasconcellos%20Vargas&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%2C%20but%20their%0Areasoning%20abilities%20and%20underlying%20mechanisms%20remain%20poorly%20understood.%20We%0Apresent%20a%20novel%20approach%20to%20enhance%20LLMs%27%20reasoning%20through%20attention%20mechanism%0Aoptimization%2C%20without%20additional%20training%20data.%20We%20identify%20inefficiencies%20in%0Athe%20attention%20distribution%20caused%20by%20non-semantic%20tokens%20and%20propose%20an%0Aalgorithm%20to%20re-balance%20the%20skewed%20distribution%2C%20enabling%20the%20model%20to%20abstract%0Amore%20nuanced%20knowledge.%20Our%20experiments%20demonstrate%20significantly%20improved%0Areasoning%20capabilities%2C%20particularly%20for%20non-STEM%20questions.%20We%20provide%0Ainsights%20into%20the%20role%20of%20attention%20patterns%20in%20LLMs%27%20reasoning%20and%20propose%20a%0Amethod%20to%20enhance%20these%20abilities%2C%20paving%20the%20way%20for%20more%20powerful%20and%0Aversatile%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14932v2&entry.124074799=Read"},
{"title": "Hyperedge Interaction-aware Hypergraph Neural Network", "author": "Rongping Ye and Xiaobing Pei and Haoran Yang and Ruiqi Wang", "abstract": "  Hypergraphs provide an effective modeling approach for modeling high-order\nrelationships in many real-world datasets. To capture such complex\nrelationships, several hypergraph neural networks have been proposed for\nlearning hypergraph structure, which propagate information from nodes to\nhyperedges and then from hyperedges back to nodes. However, most existing\nmethods focus on information propagation between hyperedges and nodes,\nneglecting the interactions among hyperedges themselves. In this paper, we\npropose HeIHNN, a hyperedge interaction-aware hypergraph neural network, which\ncaptures the interactions among hyperedges during the convolution process and\nintroduce a novel mechanism to enhance information flow between hyperedges and\nnodes. Specifically, HeIHNN integrates the interactions between hyperedges into\nthe hypergraph convolution by constructing a three-stage information\npropagation process. After propagating information from nodes to hyperedges, we\nintroduce a hyperedge-level convolution to update the hyperedge embeddings.\nFinally, the embeddings that capture rich information from the interaction\namong hyperedges will be utilized to update the node embeddings. Additionally,\nwe introduce a hyperedge outlier removal mechanism in the information\npropagation stages between nodes and hyperedges, which dynamically adjusts the\nhypergraph structure using the learned embeddings, effectively removing\noutliers. Extensive experiments conducted on real-world datasets show the\ncompetitive performance of HeIHNN compared with state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2401.15587v2", "date": "2024-04-05", "relevancy": 2.4666, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5764}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4543}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4493}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hyperedge%20Interaction-aware%20Hypergraph%20Neural%20Network&body=Title%3A%20Hyperedge%20Interaction-aware%20Hypergraph%20Neural%20Network%0AAuthor%3A%20Rongping%20Ye%20and%20Xiaobing%20Pei%20and%20Haoran%20Yang%20and%20Ruiqi%20Wang%0AAbstract%3A%20%20%20Hypergraphs%20provide%20an%20effective%20modeling%20approach%20for%20modeling%20high-order%0Arelationships%20in%20many%20real-world%20datasets.%20To%20capture%20such%20complex%0Arelationships%2C%20several%20hypergraph%20neural%20networks%20have%20been%20proposed%20for%0Alearning%20hypergraph%20structure%2C%20which%20propagate%20information%20from%20nodes%20to%0Ahyperedges%20and%20then%20from%20hyperedges%20back%20to%20nodes.%20However%2C%20most%20existing%0Amethods%20focus%20on%20information%20propagation%20between%20hyperedges%20and%20nodes%2C%0Aneglecting%20the%20interactions%20among%20hyperedges%20themselves.%20In%20this%20paper%2C%20we%0Apropose%20HeIHNN%2C%20a%20hyperedge%20interaction-aware%20hypergraph%20neural%20network%2C%20which%0Acaptures%20the%20interactions%20among%20hyperedges%20during%20the%20convolution%20process%20and%0Aintroduce%20a%20novel%20mechanism%20to%20enhance%20information%20flow%20between%20hyperedges%20and%0Anodes.%20Specifically%2C%20HeIHNN%20integrates%20the%20interactions%20between%20hyperedges%20into%0Athe%20hypergraph%20convolution%20by%20constructing%20a%20three-stage%20information%0Apropagation%20process.%20After%20propagating%20information%20from%20nodes%20to%20hyperedges%2C%20we%0Aintroduce%20a%20hyperedge-level%20convolution%20to%20update%20the%20hyperedge%20embeddings.%0AFinally%2C%20the%20embeddings%20that%20capture%20rich%20information%20from%20the%20interaction%0Aamong%20hyperedges%20will%20be%20utilized%20to%20update%20the%20node%20embeddings.%20Additionally%2C%0Awe%20introduce%20a%20hyperedge%20outlier%20removal%20mechanism%20in%20the%20information%0Apropagation%20stages%20between%20nodes%20and%20hyperedges%2C%20which%20dynamically%20adjusts%20the%0Ahypergraph%20structure%20using%20the%20learned%20embeddings%2C%20effectively%20removing%0Aoutliers.%20Extensive%20experiments%20conducted%20on%20real-world%20datasets%20show%20the%0Acompetitive%20performance%20of%20HeIHNN%20compared%20with%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15587v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperedge%20Interaction-aware%20Hypergraph%20Neural%20Network&entry.906535625=Rongping%20Ye%20and%20Xiaobing%20Pei%20and%20Haoran%20Yang%20and%20Ruiqi%20Wang&entry.1292438233=%20%20Hypergraphs%20provide%20an%20effective%20modeling%20approach%20for%20modeling%20high-order%0Arelationships%20in%20many%20real-world%20datasets.%20To%20capture%20such%20complex%0Arelationships%2C%20several%20hypergraph%20neural%20networks%20have%20been%20proposed%20for%0Alearning%20hypergraph%20structure%2C%20which%20propagate%20information%20from%20nodes%20to%0Ahyperedges%20and%20then%20from%20hyperedges%20back%20to%20nodes.%20However%2C%20most%20existing%0Amethods%20focus%20on%20information%20propagation%20between%20hyperedges%20and%20nodes%2C%0Aneglecting%20the%20interactions%20among%20hyperedges%20themselves.%20In%20this%20paper%2C%20we%0Apropose%20HeIHNN%2C%20a%20hyperedge%20interaction-aware%20hypergraph%20neural%20network%2C%20which%0Acaptures%20the%20interactions%20among%20hyperedges%20during%20the%20convolution%20process%20and%0Aintroduce%20a%20novel%20mechanism%20to%20enhance%20information%20flow%20between%20hyperedges%20and%0Anodes.%20Specifically%2C%20HeIHNN%20integrates%20the%20interactions%20between%20hyperedges%20into%0Athe%20hypergraph%20convolution%20by%20constructing%20a%20three-stage%20information%0Apropagation%20process.%20After%20propagating%20information%20from%20nodes%20to%20hyperedges%2C%20we%0Aintroduce%20a%20hyperedge-level%20convolution%20to%20update%20the%20hyperedge%20embeddings.%0AFinally%2C%20the%20embeddings%20that%20capture%20rich%20information%20from%20the%20interaction%0Aamong%20hyperedges%20will%20be%20utilized%20to%20update%20the%20node%20embeddings.%20Additionally%2C%0Awe%20introduce%20a%20hyperedge%20outlier%20removal%20mechanism%20in%20the%20information%0Apropagation%20stages%20between%20nodes%20and%20hyperedges%2C%20which%20dynamically%20adjusts%20the%0Ahypergraph%20structure%20using%20the%20learned%20embeddings%2C%20effectively%20removing%0Aoutliers.%20Extensive%20experiments%20conducted%20on%20real-world%20datasets%20show%20the%0Acompetitive%20performance%20of%20HeIHNN%20compared%20with%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15587v2&entry.124074799=Read"},
{"title": "CenterGrasp: Object-Aware Implicit Representation Learning for\n  Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation", "author": "Eugenio Chisari and Nick Heppert and Tim Welschehold and Wolfram Burgard and Abhinav Valada", "abstract": "  Reliable object grasping is a crucial capability for autonomous robots.\nHowever, many existing grasping approaches focus on general clutter removal\nwithout explicitly modeling objects and thus only relying on the visible local\ngeometry. We introduce CenterGrasp, a novel framework that combines object\nawareness and holistic grasping. CenterGrasp learns a general object prior by\nencoding shapes and valid grasps in a continuous latent space. It consists of\nan RGB-D image encoder that leverages recent advances to detect objects and\ninfer their pose and latent code, and a decoder to predict shape and grasps for\neach object in the scene. We perform extensive experiments on simulated as well\nas real-world cluttered scenes and demonstrate strong scene reconstruction and\n6-DoF grasp-pose estimation performance. Compared to the state of the art,\nCenterGrasp achieves an improvement of 38.5 mm in shape reconstruction and 33\npercentage points on average in grasp success. We make the code and trained\nmodels publicly available at http://centergrasp.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2312.08240v2", "date": "2024-04-05", "relevancy": 2.4527, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6671}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5753}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5744}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CenterGrasp%3A%20Object-Aware%20Implicit%20Representation%20Learning%20for%0A%20%20Simultaneous%20Shape%20Reconstruction%20and%206-DoF%20Grasp%20Estimation&body=Title%3A%20CenterGrasp%3A%20Object-Aware%20Implicit%20Representation%20Learning%20for%0A%20%20Simultaneous%20Shape%20Reconstruction%20and%206-DoF%20Grasp%20Estimation%0AAuthor%3A%20Eugenio%20Chisari%20and%20Nick%20Heppert%20and%20Tim%20Welschehold%20and%20Wolfram%20Burgard%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Reliable%20object%20grasping%20is%20a%20crucial%20capability%20for%20autonomous%20robots.%0AHowever%2C%20many%20existing%20grasping%20approaches%20focus%20on%20general%20clutter%20removal%0Awithout%20explicitly%20modeling%20objects%20and%20thus%20only%20relying%20on%20the%20visible%20local%0Ageometry.%20We%20introduce%20CenterGrasp%2C%20a%20novel%20framework%20that%20combines%20object%0Aawareness%20and%20holistic%20grasping.%20CenterGrasp%20learns%20a%20general%20object%20prior%20by%0Aencoding%20shapes%20and%20valid%20grasps%20in%20a%20continuous%20latent%20space.%20It%20consists%20of%0Aan%20RGB-D%20image%20encoder%20that%20leverages%20recent%20advances%20to%20detect%20objects%20and%0Ainfer%20their%20pose%20and%20latent%20code%2C%20and%20a%20decoder%20to%20predict%20shape%20and%20grasps%20for%0Aeach%20object%20in%20the%20scene.%20We%20perform%20extensive%20experiments%20on%20simulated%20as%20well%0Aas%20real-world%20cluttered%20scenes%20and%20demonstrate%20strong%20scene%20reconstruction%20and%0A6-DoF%20grasp-pose%20estimation%20performance.%20Compared%20to%20the%20state%20of%20the%20art%2C%0ACenterGrasp%20achieves%20an%20improvement%20of%2038.5%20mm%20in%20shape%20reconstruction%20and%2033%0Apercentage%20points%20on%20average%20in%20grasp%20success.%20We%20make%20the%20code%20and%20trained%0Amodels%20publicly%20available%20at%20http%3A//centergrasp.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08240v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CenterGrasp%3A%20Object-Aware%20Implicit%20Representation%20Learning%20for%0A%20%20Simultaneous%20Shape%20Reconstruction%20and%206-DoF%20Grasp%20Estimation&entry.906535625=Eugenio%20Chisari%20and%20Nick%20Heppert%20and%20Tim%20Welschehold%20and%20Wolfram%20Burgard%20and%20Abhinav%20Valada&entry.1292438233=%20%20Reliable%20object%20grasping%20is%20a%20crucial%20capability%20for%20autonomous%20robots.%0AHowever%2C%20many%20existing%20grasping%20approaches%20focus%20on%20general%20clutter%20removal%0Awithout%20explicitly%20modeling%20objects%20and%20thus%20only%20relying%20on%20the%20visible%20local%0Ageometry.%20We%20introduce%20CenterGrasp%2C%20a%20novel%20framework%20that%20combines%20object%0Aawareness%20and%20holistic%20grasping.%20CenterGrasp%20learns%20a%20general%20object%20prior%20by%0Aencoding%20shapes%20and%20valid%20grasps%20in%20a%20continuous%20latent%20space.%20It%20consists%20of%0Aan%20RGB-D%20image%20encoder%20that%20leverages%20recent%20advances%20to%20detect%20objects%20and%0Ainfer%20their%20pose%20and%20latent%20code%2C%20and%20a%20decoder%20to%20predict%20shape%20and%20grasps%20for%0Aeach%20object%20in%20the%20scene.%20We%20perform%20extensive%20experiments%20on%20simulated%20as%20well%0Aas%20real-world%20cluttered%20scenes%20and%20demonstrate%20strong%20scene%20reconstruction%20and%0A6-DoF%20grasp-pose%20estimation%20performance.%20Compared%20to%20the%20state%20of%20the%20art%2C%0ACenterGrasp%20achieves%20an%20improvement%20of%2038.5%20mm%20in%20shape%20reconstruction%20and%2033%0Apercentage%20points%20on%20average%20in%20grasp%20success.%20We%20make%20the%20code%20and%20trained%0Amodels%20publicly%20available%20at%20http%3A//centergrasp.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08240v2&entry.124074799=Read"},
{"title": "Fusing Dictionary Learning and Support Vector Machines for Unsupervised\n  Anomaly Detection", "author": "Paul Irofti and Iulian-Andrei H\u00eeji and Andrei P\u0103tra\u015fcu and Nicolae Cleju", "abstract": "  We study in this paper the improvement of one-class support vector machines\n(OC-SVM) through sparse representation techniques for unsupervised anomaly\ndetection. As Dictionary Learning (DL) became recently a common analysis\ntechnique that reveals hidden sparse patterns of data, our approach uses this\ninsight to endow unsupervised detection with more control on pattern finding\nand dimensions. We introduce a new anomaly detection model that unifies the\nOC-SVM and DL residual functions into a single composite objective,\nsubsequently solved through K-SVD-type iterative algorithms. A closed-form of\nthe alternating K-SVD iteration is explicitly derived for the new composite\nmodel and practical implementable schemes are discussed. The standard DL model\nis adapted for the Dictionary Pair Learning (DPL) context, where the usual\nsparsity constraints are naturally eliminated. Finally, we extend both\nobjectives to the more general setting that allows the use of kernel functions.\nThe empirical convergence properties of the resulting algorithms are provided\nand an in-depth analysis of their parametrization is performed while also\ndemonstrating their numerical performance in comparison with existing methods.\n", "link": "http://arxiv.org/abs/2404.04064v1", "date": "2024-04-05", "relevancy": 2.4455, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5132}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4805}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4737}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fusing%20Dictionary%20Learning%20and%20Support%20Vector%20Machines%20for%20Unsupervised%0A%20%20Anomaly%20Detection&body=Title%3A%20Fusing%20Dictionary%20Learning%20and%20Support%20Vector%20Machines%20for%20Unsupervised%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Paul%20Irofti%20and%20Iulian-Andrei%20H%C3%AEji%20and%20Andrei%20P%C4%83tra%C5%9Fcu%20and%20Nicolae%20Cleju%0AAbstract%3A%20%20%20We%20study%20in%20this%20paper%20the%20improvement%20of%20one-class%20support%20vector%20machines%0A%28OC-SVM%29%20through%20sparse%20representation%20techniques%20for%20unsupervised%20anomaly%0Adetection.%20As%20Dictionary%20Learning%20%28DL%29%20became%20recently%20a%20common%20analysis%0Atechnique%20that%20reveals%20hidden%20sparse%20patterns%20of%20data%2C%20our%20approach%20uses%20this%0Ainsight%20to%20endow%20unsupervised%20detection%20with%20more%20control%20on%20pattern%20finding%0Aand%20dimensions.%20We%20introduce%20a%20new%20anomaly%20detection%20model%20that%20unifies%20the%0AOC-SVM%20and%20DL%20residual%20functions%20into%20a%20single%20composite%20objective%2C%0Asubsequently%20solved%20through%20K-SVD-type%20iterative%20algorithms.%20A%20closed-form%20of%0Athe%20alternating%20K-SVD%20iteration%20is%20explicitly%20derived%20for%20the%20new%20composite%0Amodel%20and%20practical%20implementable%20schemes%20are%20discussed.%20The%20standard%20DL%20model%0Ais%20adapted%20for%20the%20Dictionary%20Pair%20Learning%20%28DPL%29%20context%2C%20where%20the%20usual%0Asparsity%20constraints%20are%20naturally%20eliminated.%20Finally%2C%20we%20extend%20both%0Aobjectives%20to%20the%20more%20general%20setting%20that%20allows%20the%20use%20of%20kernel%20functions.%0AThe%20empirical%20convergence%20properties%20of%20the%20resulting%20algorithms%20are%20provided%0Aand%20an%20in-depth%20analysis%20of%20their%20parametrization%20is%20performed%20while%20also%0Ademonstrating%20their%20numerical%20performance%20in%20comparison%20with%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04064v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusing%20Dictionary%20Learning%20and%20Support%20Vector%20Machines%20for%20Unsupervised%0A%20%20Anomaly%20Detection&entry.906535625=Paul%20Irofti%20and%20Iulian-Andrei%20H%C3%AEji%20and%20Andrei%20P%C4%83tra%C5%9Fcu%20and%20Nicolae%20Cleju&entry.1292438233=%20%20We%20study%20in%20this%20paper%20the%20improvement%20of%20one-class%20support%20vector%20machines%0A%28OC-SVM%29%20through%20sparse%20representation%20techniques%20for%20unsupervised%20anomaly%0Adetection.%20As%20Dictionary%20Learning%20%28DL%29%20became%20recently%20a%20common%20analysis%0Atechnique%20that%20reveals%20hidden%20sparse%20patterns%20of%20data%2C%20our%20approach%20uses%20this%0Ainsight%20to%20endow%20unsupervised%20detection%20with%20more%20control%20on%20pattern%20finding%0Aand%20dimensions.%20We%20introduce%20a%20new%20anomaly%20detection%20model%20that%20unifies%20the%0AOC-SVM%20and%20DL%20residual%20functions%20into%20a%20single%20composite%20objective%2C%0Asubsequently%20solved%20through%20K-SVD-type%20iterative%20algorithms.%20A%20closed-form%20of%0Athe%20alternating%20K-SVD%20iteration%20is%20explicitly%20derived%20for%20the%20new%20composite%0Amodel%20and%20practical%20implementable%20schemes%20are%20discussed.%20The%20standard%20DL%20model%0Ais%20adapted%20for%20the%20Dictionary%20Pair%20Learning%20%28DPL%29%20context%2C%20where%20the%20usual%0Asparsity%20constraints%20are%20naturally%20eliminated.%20Finally%2C%20we%20extend%20both%0Aobjectives%20to%20the%20more%20general%20setting%20that%20allows%20the%20use%20of%20kernel%20functions.%0AThe%20empirical%20convergence%20properties%20of%20the%20resulting%20algorithms%20are%20provided%0Aand%20an%20in-depth%20analysis%20of%20their%20parametrization%20is%20performed%20while%20also%0Ademonstrating%20their%20numerical%20performance%20in%20comparison%20with%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04064v1&entry.124074799=Read"},
{"title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model", "author": "Xinrun Du and Zhouliang Yu and Songyang Gao and Ding Pan and Yuyang Cheng and Ziyang Ma and Ruibin Yuan and Xingwei Qu and Jiaheng Liu and Tianyu Zheng and Xinchen Luo and Guorui Zhou and Binhang Yuan and Wenhu Chen and Jie Fu and Ge Zhang", "abstract": "  In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.\n", "link": "http://arxiv.org/abs/2404.04167v1", "date": "2024-04-05", "relevancy": 2.4341, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.47}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model&body=Title%3A%20Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model%0AAuthor%3A%20Xinrun%20Du%20and%20Zhouliang%20Yu%20and%20Songyang%20Gao%20and%20Ding%20Pan%20and%20Yuyang%20Cheng%20and%20Ziyang%20Ma%20and%20Ruibin%20Yuan%20and%20Xingwei%20Qu%20and%20Jiaheng%20Liu%20and%20Tianyu%20Zheng%20and%20Xinchen%20Luo%20and%20Guorui%20Zhou%20and%20Binhang%20Yuan%20and%20Wenhu%20Chen%20and%20Jie%20Fu%20and%20Ge%20Zhang%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20CT-LLM%2C%20a%202B%20large%20language%20model%20%28LLM%29%20that%0Aillustrates%20a%20pivotal%20shift%20towards%20prioritizing%20the%20Chinese%20language%20in%0Adeveloping%20LLMs.%20Uniquely%20initiated%20from%20scratch%2C%20CT-LLM%20diverges%20from%20the%0Aconventional%20methodology%20by%20primarily%20incorporating%20Chinese%20textual%20data%2C%0Autilizing%20an%20extensive%20corpus%20of%201%2C200%20billion%20tokens%2C%20including%20800%20billion%0AChinese%20tokens%2C%20300%20billion%20English%20tokens%2C%20and%20100%20billion%20code%20tokens.%20This%0Astrategic%20composition%20facilitates%20the%20model%27s%20exceptional%20proficiency%20in%0Aunderstanding%20and%20processing%20Chinese%2C%20a%20capability%20further%20enhanced%20through%0Aalignment%20techniques.%20Demonstrating%20remarkable%20performance%20on%20the%20CHC-Bench%2C%0ACT-LLM%20excels%20in%20Chinese%20language%20tasks%2C%20and%20showcases%20its%20adeptness%20in%20English%0Athrough%20SFT.%20This%20research%20challenges%20the%20prevailing%20paradigm%20of%20training%20LLMs%0Apredominantly%20on%20English%20corpora%20and%20then%20adapting%20them%20to%20other%20languages%2C%0Abroadening%20the%20horizons%20for%20LLM%20training%20methodologies.%20By%20open-sourcing%20the%0Afull%20process%20of%20training%20a%20Chinese%20LLM%2C%20including%20a%20detailed%20data%20processing%0Aprocedure%20with%20the%20obtained%20Massive%20Appropriate%20Pretraining%20Chinese%20Corpus%0A%28MAP-CC%29%2C%20a%20well-chosen%20multidisciplinary%20Chinese%20Hard%20Case%20Benchmark%0A%28CHC-Bench%29%2C%20and%20the%202B-size%20Chinese%20Tiny%20LLM%20%28CT-LLM%29%2C%20we%20aim%20to%20foster%0Afurther%20exploration%20and%20innovation%20in%20both%20academia%20and%20industry%2C%20paving%20the%0Away%20for%20more%20inclusive%20and%20versatile%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04167v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model&entry.906535625=Xinrun%20Du%20and%20Zhouliang%20Yu%20and%20Songyang%20Gao%20and%20Ding%20Pan%20and%20Yuyang%20Cheng%20and%20Ziyang%20Ma%20and%20Ruibin%20Yuan%20and%20Xingwei%20Qu%20and%20Jiaheng%20Liu%20and%20Tianyu%20Zheng%20and%20Xinchen%20Luo%20and%20Guorui%20Zhou%20and%20Binhang%20Yuan%20and%20Wenhu%20Chen%20and%20Jie%20Fu%20and%20Ge%20Zhang&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20CT-LLM%2C%20a%202B%20large%20language%20model%20%28LLM%29%20that%0Aillustrates%20a%20pivotal%20shift%20towards%20prioritizing%20the%20Chinese%20language%20in%0Adeveloping%20LLMs.%20Uniquely%20initiated%20from%20scratch%2C%20CT-LLM%20diverges%20from%20the%0Aconventional%20methodology%20by%20primarily%20incorporating%20Chinese%20textual%20data%2C%0Autilizing%20an%20extensive%20corpus%20of%201%2C200%20billion%20tokens%2C%20including%20800%20billion%0AChinese%20tokens%2C%20300%20billion%20English%20tokens%2C%20and%20100%20billion%20code%20tokens.%20This%0Astrategic%20composition%20facilitates%20the%20model%27s%20exceptional%20proficiency%20in%0Aunderstanding%20and%20processing%20Chinese%2C%20a%20capability%20further%20enhanced%20through%0Aalignment%20techniques.%20Demonstrating%20remarkable%20performance%20on%20the%20CHC-Bench%2C%0ACT-LLM%20excels%20in%20Chinese%20language%20tasks%2C%20and%20showcases%20its%20adeptness%20in%20English%0Athrough%20SFT.%20This%20research%20challenges%20the%20prevailing%20paradigm%20of%20training%20LLMs%0Apredominantly%20on%20English%20corpora%20and%20then%20adapting%20them%20to%20other%20languages%2C%0Abroadening%20the%20horizons%20for%20LLM%20training%20methodologies.%20By%20open-sourcing%20the%0Afull%20process%20of%20training%20a%20Chinese%20LLM%2C%20including%20a%20detailed%20data%20processing%0Aprocedure%20with%20the%20obtained%20Massive%20Appropriate%20Pretraining%20Chinese%20Corpus%0A%28MAP-CC%29%2C%20a%20well-chosen%20multidisciplinary%20Chinese%20Hard%20Case%20Benchmark%0A%28CHC-Bench%29%2C%20and%20the%202B-size%20Chinese%20Tiny%20LLM%20%28CT-LLM%29%2C%20we%20aim%20to%20foster%0Afurther%20exploration%20and%20innovation%20in%20both%20academia%20and%20industry%2C%20paving%20the%0Away%20for%20more%20inclusive%20and%20versatile%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04167v1&entry.124074799=Read"},
{"title": "Robust Gaussian Splatting", "author": "Fran\u00e7ois Darmon and Lorenzo Porzi and Samuel Rota-Bul\u00f2 and Peter Kontschieder", "abstract": "  In this paper, we address common error sources for 3D Gaussian Splatting\n(3DGS) including blur, imperfect camera poses, and color inconsistencies, with\nthe goal of improving its robustness for practical applications like\nreconstructions from handheld phone captures. Our main contribution involves\nmodeling motion blur as a Gaussian distribution over camera poses, allowing us\nto address both camera pose refinement and motion blur correction in a unified\nway. Additionally, we propose mechanisms for defocus blur compensation and for\naddressing color in-consistencies caused by ambient light, shadows, or due to\ncamera-related factors like varying white balancing settings. Our proposed\nsolutions integrate in a seamless way with the 3DGS formulation while\nmaintaining its benefits in terms of training efficiency and rendering speed.\nWe experimentally validate our contributions on relevant benchmark datasets\nincluding Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and\nthus consistent improvements over relevant baselines.\n", "link": "http://arxiv.org/abs/2404.04211v1", "date": "2024-04-05", "relevancy": 2.4162, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4894}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.481}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4792}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Gaussian%20Splatting&body=Title%3A%20Robust%20Gaussian%20Splatting%0AAuthor%3A%20Fran%C3%A7ois%20Darmon%20and%20Lorenzo%20Porzi%20and%20Samuel%20Rota-Bul%C3%B2%20and%20Peter%20Kontschieder%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20common%20error%20sources%20for%203D%20Gaussian%20Splatting%0A%283DGS%29%20including%20blur%2C%20imperfect%20camera%20poses%2C%20and%20color%20inconsistencies%2C%20with%0Athe%20goal%20of%20improving%20its%20robustness%20for%20practical%20applications%20like%0Areconstructions%20from%20handheld%20phone%20captures.%20Our%20main%20contribution%20involves%0Amodeling%20motion%20blur%20as%20a%20Gaussian%20distribution%20over%20camera%20poses%2C%20allowing%20us%0Ato%20address%20both%20camera%20pose%20refinement%20and%20motion%20blur%20correction%20in%20a%20unified%0Away.%20Additionally%2C%20we%20propose%20mechanisms%20for%20defocus%20blur%20compensation%20and%20for%0Aaddressing%20color%20in-consistencies%20caused%20by%20ambient%20light%2C%20shadows%2C%20or%20due%20to%0Acamera-related%20factors%20like%20varying%20white%20balancing%20settings.%20Our%20proposed%0Asolutions%20integrate%20in%20a%20seamless%20way%20with%20the%203DGS%20formulation%20while%0Amaintaining%20its%20benefits%20in%20terms%20of%20training%20efficiency%20and%20rendering%20speed.%0AWe%20experimentally%20validate%20our%20contributions%20on%20relevant%20benchmark%20datasets%0Aincluding%20Scannet%2B%2B%20and%20Deblur-NeRF%2C%20obtaining%20state-of-the-art%20results%20and%0Athus%20consistent%20improvements%20over%20relevant%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04211v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Gaussian%20Splatting&entry.906535625=Fran%C3%A7ois%20Darmon%20and%20Lorenzo%20Porzi%20and%20Samuel%20Rota-Bul%C3%B2%20and%20Peter%20Kontschieder&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20common%20error%20sources%20for%203D%20Gaussian%20Splatting%0A%283DGS%29%20including%20blur%2C%20imperfect%20camera%20poses%2C%20and%20color%20inconsistencies%2C%20with%0Athe%20goal%20of%20improving%20its%20robustness%20for%20practical%20applications%20like%0Areconstructions%20from%20handheld%20phone%20captures.%20Our%20main%20contribution%20involves%0Amodeling%20motion%20blur%20as%20a%20Gaussian%20distribution%20over%20camera%20poses%2C%20allowing%20us%0Ato%20address%20both%20camera%20pose%20refinement%20and%20motion%20blur%20correction%20in%20a%20unified%0Away.%20Additionally%2C%20we%20propose%20mechanisms%20for%20defocus%20blur%20compensation%20and%20for%0Aaddressing%20color%20in-consistencies%20caused%20by%20ambient%20light%2C%20shadows%2C%20or%20due%20to%0Acamera-related%20factors%20like%20varying%20white%20balancing%20settings.%20Our%20proposed%0Asolutions%20integrate%20in%20a%20seamless%20way%20with%20the%203DGS%20formulation%20while%0Amaintaining%20its%20benefits%20in%20terms%20of%20training%20efficiency%20and%20rendering%20speed.%0AWe%20experimentally%20validate%20our%20contributions%20on%20relevant%20benchmark%20datasets%0Aincluding%20Scannet%2B%2B%20and%20Deblur-NeRF%2C%20obtaining%20state-of-the-art%20results%20and%0Athus%20consistent%20improvements%20over%20relevant%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04211v1&entry.124074799=Read"},
{"title": "Finsler-Laplace-Beltrami Operators with Application to Shape Analysis", "author": "Simon Weber and Thomas Dag\u00e8s and Maolin Gao and Daniel Cremers", "abstract": "  The Laplace-Beltrami operator (LBO) emerges from studying manifolds equipped\nwith a Riemannian metric. It is often called the Swiss army knife of geometry\nprocessing as it allows to capture intrinsic shape information and gives rise\nto heat diffusion, geodesic distances, and a multitude of shape descriptors. It\nalso plays a central role in geometric deep learning. In this work, we explore\nFinsler manifolds as a generalization of Riemannian manifolds. We revisit the\nFinsler heat equation and derive a Finsler heat kernel and a\nFinsler-Laplace-Beltrami Operator (FLBO): a novel theoretically justified\nanisotropic Laplace-Beltrami operator (ALBO). In experimental evaluations we\ndemonstrate that the proposed FLBO is a valuable alternative to the traditional\nRiemannian-based LBO and ALBOs for spatial filtering and shape correspondence\nestimation. We hope that the proposed Finsler heat kernel and the FLBO will\ninspire further exploration of Finsler geometry in the computer vision\ncommunity.\n", "link": "http://arxiv.org/abs/2404.03999v1", "date": "2024-04-05", "relevancy": 2.3939, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4879}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4784}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.47}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Finsler-Laplace-Beltrami%20Operators%20with%20Application%20to%20Shape%20Analysis&body=Title%3A%20Finsler-Laplace-Beltrami%20Operators%20with%20Application%20to%20Shape%20Analysis%0AAuthor%3A%20Simon%20Weber%20and%20Thomas%20Dag%C3%A8s%20and%20Maolin%20Gao%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20The%20Laplace-Beltrami%20operator%20%28LBO%29%20emerges%20from%20studying%20manifolds%20equipped%0Awith%20a%20Riemannian%20metric.%20It%20is%20often%20called%20the%20Swiss%20army%20knife%20of%20geometry%0Aprocessing%20as%20it%20allows%20to%20capture%20intrinsic%20shape%20information%20and%20gives%20rise%0Ato%20heat%20diffusion%2C%20geodesic%20distances%2C%20and%20a%20multitude%20of%20shape%20descriptors.%20It%0Aalso%20plays%20a%20central%20role%20in%20geometric%20deep%20learning.%20In%20this%20work%2C%20we%20explore%0AFinsler%20manifolds%20as%20a%20generalization%20of%20Riemannian%20manifolds.%20We%20revisit%20the%0AFinsler%20heat%20equation%20and%20derive%20a%20Finsler%20heat%20kernel%20and%20a%0AFinsler-Laplace-Beltrami%20Operator%20%28FLBO%29%3A%20a%20novel%20theoretically%20justified%0Aanisotropic%20Laplace-Beltrami%20operator%20%28ALBO%29.%20In%20experimental%20evaluations%20we%0Ademonstrate%20that%20the%20proposed%20FLBO%20is%20a%20valuable%20alternative%20to%20the%20traditional%0ARiemannian-based%20LBO%20and%20ALBOs%20for%20spatial%20filtering%20and%20shape%20correspondence%0Aestimation.%20We%20hope%20that%20the%20proposed%20Finsler%20heat%20kernel%20and%20the%20FLBO%20will%0Ainspire%20further%20exploration%20of%20Finsler%20geometry%20in%20the%20computer%20vision%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03999v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finsler-Laplace-Beltrami%20Operators%20with%20Application%20to%20Shape%20Analysis&entry.906535625=Simon%20Weber%20and%20Thomas%20Dag%C3%A8s%20and%20Maolin%20Gao%20and%20Daniel%20Cremers&entry.1292438233=%20%20The%20Laplace-Beltrami%20operator%20%28LBO%29%20emerges%20from%20studying%20manifolds%20equipped%0Awith%20a%20Riemannian%20metric.%20It%20is%20often%20called%20the%20Swiss%20army%20knife%20of%20geometry%0Aprocessing%20as%20it%20allows%20to%20capture%20intrinsic%20shape%20information%20and%20gives%20rise%0Ato%20heat%20diffusion%2C%20geodesic%20distances%2C%20and%20a%20multitude%20of%20shape%20descriptors.%20It%0Aalso%20plays%20a%20central%20role%20in%20geometric%20deep%20learning.%20In%20this%20work%2C%20we%20explore%0AFinsler%20manifolds%20as%20a%20generalization%20of%20Riemannian%20manifolds.%20We%20revisit%20the%0AFinsler%20heat%20equation%20and%20derive%20a%20Finsler%20heat%20kernel%20and%20a%0AFinsler-Laplace-Beltrami%20Operator%20%28FLBO%29%3A%20a%20novel%20theoretically%20justified%0Aanisotropic%20Laplace-Beltrami%20operator%20%28ALBO%29.%20In%20experimental%20evaluations%20we%0Ademonstrate%20that%20the%20proposed%20FLBO%20is%20a%20valuable%20alternative%20to%20the%20traditional%0ARiemannian-based%20LBO%20and%20ALBOs%20for%20spatial%20filtering%20and%20shape%20correspondence%0Aestimation.%20We%20hope%20that%20the%20proposed%20Finsler%20heat%20kernel%20and%20the%20FLBO%20will%0Ainspire%20further%20exploration%20of%20Finsler%20geometry%20in%20the%20computer%20vision%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03999v1&entry.124074799=Read"},
{"title": "Deep-learning Segmentation of Small Volumes in CT images for\n  Radiotherapy Treatment Planning", "author": "Jianxin Zhou and Kadishe Fejza and Massimiliano Salvatori and Daniele Della Latta and Gregory M. Hermann and Angela Di Fulvio", "abstract": "  Our understanding of organs at risk is progressing to include physical small\ntissues such as coronary arteries and the radiosensitivities of many small\norgans and tissues are high. Therefore, the accurate segmentation of small\nvolumes in external radiotherapy is crucial to protect them from\nover-irradiation. Moreover, with the development of the particle therapy and\non-board imaging, the treatment becomes more accurate and precise. The purpose\nof this work is to optimize organ segmentation algorithms for small organs. We\nused 50 three-dimensional (3-D) computed tomography (CT) head and neck images\nfrom StructSeg2019 challenge to develop a general-purpose V-Net model to\nsegment 20 organs in the head and neck region. We applied specific strategies\nto improve the segmentation accuracy of the small volumes in this anatomical\nregion, i.e., the lens of the eye. Then, we used 17 additional head images from\nOSF healthcare to validate the robustness of the V Net model optimized for\nsmall-volume segmentation. With the study of the StructSeg2019 images, we found\nthat the optimization of the image normalization range and classification\nthreshold yielded a segmentation improvement of the lens of the eye of\napproximately 50%, compared to the use of the V-Net not optimized for small\nvolumes. We used the optimized model to segment 17 images acquired using\nheterogeneous protocols. We obtained comparable Dice coefficient values for the\nclinical and StructSeg2019 images (0.61 plus/minus 0.07 and 0.58 plus/minus\n0.10 for the left and right lens of the eye, respectively)\n", "link": "http://arxiv.org/abs/2404.04202v1", "date": "2024-04-05", "relevancy": 2.3679, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4812}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4789}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4607}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep-learning%20Segmentation%20of%20Small%20Volumes%20in%20CT%20images%20for%0A%20%20Radiotherapy%20Treatment%20Planning&body=Title%3A%20Deep-learning%20Segmentation%20of%20Small%20Volumes%20in%20CT%20images%20for%0A%20%20Radiotherapy%20Treatment%20Planning%0AAuthor%3A%20Jianxin%20Zhou%20and%20Kadishe%20Fejza%20and%20Massimiliano%20Salvatori%20and%20Daniele%20Della%20Latta%20and%20Gregory%20M.%20Hermann%20and%20Angela%20Di%20Fulvio%0AAbstract%3A%20%20%20Our%20understanding%20of%20organs%20at%20risk%20is%20progressing%20to%20include%20physical%20small%0Atissues%20such%20as%20coronary%20arteries%20and%20the%20radiosensitivities%20of%20many%20small%0Aorgans%20and%20tissues%20are%20high.%20Therefore%2C%20the%20accurate%20segmentation%20of%20small%0Avolumes%20in%20external%20radiotherapy%20is%20crucial%20to%20protect%20them%20from%0Aover-irradiation.%20Moreover%2C%20with%20the%20development%20of%20the%20particle%20therapy%20and%0Aon-board%20imaging%2C%20the%20treatment%20becomes%20more%20accurate%20and%20precise.%20The%20purpose%0Aof%20this%20work%20is%20to%20optimize%20organ%20segmentation%20algorithms%20for%20small%20organs.%20We%0Aused%2050%20three-dimensional%20%283-D%29%20computed%20tomography%20%28CT%29%20head%20and%20neck%20images%0Afrom%20StructSeg2019%20challenge%20to%20develop%20a%20general-purpose%20V-Net%20model%20to%0Asegment%2020%20organs%20in%20the%20head%20and%20neck%20region.%20We%20applied%20specific%20strategies%0Ato%20improve%20the%20segmentation%20accuracy%20of%20the%20small%20volumes%20in%20this%20anatomical%0Aregion%2C%20i.e.%2C%20the%20lens%20of%20the%20eye.%20Then%2C%20we%20used%2017%20additional%20head%20images%20from%0AOSF%20healthcare%20to%20validate%20the%20robustness%20of%20the%20V%20Net%20model%20optimized%20for%0Asmall-volume%20segmentation.%20With%20the%20study%20of%20the%20StructSeg2019%20images%2C%20we%20found%0Athat%20the%20optimization%20of%20the%20image%20normalization%20range%20and%20classification%0Athreshold%20yielded%20a%20segmentation%20improvement%20of%20the%20lens%20of%20the%20eye%20of%0Aapproximately%2050%25%2C%20compared%20to%20the%20use%20of%20the%20V-Net%20not%20optimized%20for%20small%0Avolumes.%20We%20used%20the%20optimized%20model%20to%20segment%2017%20images%20acquired%20using%0Aheterogeneous%20protocols.%20We%20obtained%20comparable%20Dice%20coefficient%20values%20for%20the%0Aclinical%20and%20StructSeg2019%20images%20%280.61%20plus/minus%200.07%20and%200.58%20plus/minus%0A0.10%20for%20the%20left%20and%20right%20lens%20of%20the%20eye%2C%20respectively%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04202v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-learning%20Segmentation%20of%20Small%20Volumes%20in%20CT%20images%20for%0A%20%20Radiotherapy%20Treatment%20Planning&entry.906535625=Jianxin%20Zhou%20and%20Kadishe%20Fejza%20and%20Massimiliano%20Salvatori%20and%20Daniele%20Della%20Latta%20and%20Gregory%20M.%20Hermann%20and%20Angela%20Di%20Fulvio&entry.1292438233=%20%20Our%20understanding%20of%20organs%20at%20risk%20is%20progressing%20to%20include%20physical%20small%0Atissues%20such%20as%20coronary%20arteries%20and%20the%20radiosensitivities%20of%20many%20small%0Aorgans%20and%20tissues%20are%20high.%20Therefore%2C%20the%20accurate%20segmentation%20of%20small%0Avolumes%20in%20external%20radiotherapy%20is%20crucial%20to%20protect%20them%20from%0Aover-irradiation.%20Moreover%2C%20with%20the%20development%20of%20the%20particle%20therapy%20and%0Aon-board%20imaging%2C%20the%20treatment%20becomes%20more%20accurate%20and%20precise.%20The%20purpose%0Aof%20this%20work%20is%20to%20optimize%20organ%20segmentation%20algorithms%20for%20small%20organs.%20We%0Aused%2050%20three-dimensional%20%283-D%29%20computed%20tomography%20%28CT%29%20head%20and%20neck%20images%0Afrom%20StructSeg2019%20challenge%20to%20develop%20a%20general-purpose%20V-Net%20model%20to%0Asegment%2020%20organs%20in%20the%20head%20and%20neck%20region.%20We%20applied%20specific%20strategies%0Ato%20improve%20the%20segmentation%20accuracy%20of%20the%20small%20volumes%20in%20this%20anatomical%0Aregion%2C%20i.e.%2C%20the%20lens%20of%20the%20eye.%20Then%2C%20we%20used%2017%20additional%20head%20images%20from%0AOSF%20healthcare%20to%20validate%20the%20robustness%20of%20the%20V%20Net%20model%20optimized%20for%0Asmall-volume%20segmentation.%20With%20the%20study%20of%20the%20StructSeg2019%20images%2C%20we%20found%0Athat%20the%20optimization%20of%20the%20image%20normalization%20range%20and%20classification%0Athreshold%20yielded%20a%20segmentation%20improvement%20of%20the%20lens%20of%20the%20eye%20of%0Aapproximately%2050%25%2C%20compared%20to%20the%20use%20of%20the%20V-Net%20not%20optimized%20for%20small%0Avolumes.%20We%20used%20the%20optimized%20model%20to%20segment%2017%20images%20acquired%20using%0Aheterogeneous%20protocols.%20We%20obtained%20comparable%20Dice%20coefficient%20values%20for%20the%0Aclinical%20and%20StructSeg2019%20images%20%280.61%20plus/minus%200.07%20and%200.58%20plus/minus%0A0.10%20for%20the%20left%20and%20right%20lens%20of%20the%20eye%2C%20respectively%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04202v1&entry.124074799=Read"},
{"title": "SADA: Semantic adversarial unsupervised domain adaptation for Temporal\n  Action Localization", "author": "David Pujol-Perich and Albert Clap\u00e9s and Sergio Escalera", "abstract": "  Temporal Action Localization (TAL) is a complex task that poses relevant\nchallenges, particularly when attempting to generalize on new -- unseen --\ndomains in real-world applications. These scenarios, despite realistic, are\noften neglected in the literature, exposing these solutions to important\nperformance degradation. In this work, we tackle this issue by introducing, for\nthe first time, an approach for Unsupervised Domain Adaptation (UDA) in sparse\nTAL, which we refer to as Semantic Adversarial unsupervised Domain Adaptation\n(SADA). Our contributions are threefold: (1) we pioneer the development of a\ndomain adaptation model that operates on realistic sparse action detection\nbenchmarks; (2) we tackle the limitations of global-distribution alignment\ntechniques by introducing a novel adversarial loss that is sensitive to local\nclass distributions, ensuring finer-grained adaptation; and (3) we present a\nnovel set of benchmarks based on EpicKitchens100 and CharadesEgo, that evaluate\nmultiple domain shifts in a comprehensive manner. Our experiments indicate that\nSADA improves the adaptation across domains when compared to fully supervised\nstate-of-the-art and alternative UDA methods, attaining a performance boost of\nup to 6.14% mAP.\n", "link": "http://arxiv.org/abs/2312.13377v2", "date": "2024-04-05", "relevancy": 2.3581, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5469}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SADA%3A%20Semantic%20adversarial%20unsupervised%20domain%20adaptation%20for%20Temporal%0A%20%20Action%20Localization&body=Title%3A%20SADA%3A%20Semantic%20adversarial%20unsupervised%20domain%20adaptation%20for%20Temporal%0A%20%20Action%20Localization%0AAuthor%3A%20David%20Pujol-Perich%20and%20Albert%20Clap%C3%A9s%20and%20Sergio%20Escalera%0AAbstract%3A%20%20%20Temporal%20Action%20Localization%20%28TAL%29%20is%20a%20complex%20task%20that%20poses%20relevant%0Achallenges%2C%20particularly%20when%20attempting%20to%20generalize%20on%20new%20--%20unseen%20--%0Adomains%20in%20real-world%20applications.%20These%20scenarios%2C%20despite%20realistic%2C%20are%0Aoften%20neglected%20in%20the%20literature%2C%20exposing%20these%20solutions%20to%20important%0Aperformance%20degradation.%20In%20this%20work%2C%20we%20tackle%20this%20issue%20by%20introducing%2C%20for%0Athe%20first%20time%2C%20an%20approach%20for%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20in%20sparse%0ATAL%2C%20which%20we%20refer%20to%20as%20Semantic%20Adversarial%20unsupervised%20Domain%20Adaptation%0A%28SADA%29.%20Our%20contributions%20are%20threefold%3A%20%281%29%20we%20pioneer%20the%20development%20of%20a%0Adomain%20adaptation%20model%20that%20operates%20on%20realistic%20sparse%20action%20detection%0Abenchmarks%3B%20%282%29%20we%20tackle%20the%20limitations%20of%20global-distribution%20alignment%0Atechniques%20by%20introducing%20a%20novel%20adversarial%20loss%20that%20is%20sensitive%20to%20local%0Aclass%20distributions%2C%20ensuring%20finer-grained%20adaptation%3B%20and%20%283%29%20we%20present%20a%0Anovel%20set%20of%20benchmarks%20based%20on%20EpicKitchens100%20and%20CharadesEgo%2C%20that%20evaluate%0Amultiple%20domain%20shifts%20in%20a%20comprehensive%20manner.%20Our%20experiments%20indicate%20that%0ASADA%20improves%20the%20adaptation%20across%20domains%20when%20compared%20to%20fully%20supervised%0Astate-of-the-art%20and%20alternative%20UDA%20methods%2C%20attaining%20a%20performance%20boost%20of%0Aup%20to%206.14%25%20mAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13377v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SADA%3A%20Semantic%20adversarial%20unsupervised%20domain%20adaptation%20for%20Temporal%0A%20%20Action%20Localization&entry.906535625=David%20Pujol-Perich%20and%20Albert%20Clap%C3%A9s%20and%20Sergio%20Escalera&entry.1292438233=%20%20Temporal%20Action%20Localization%20%28TAL%29%20is%20a%20complex%20task%20that%20poses%20relevant%0Achallenges%2C%20particularly%20when%20attempting%20to%20generalize%20on%20new%20--%20unseen%20--%0Adomains%20in%20real-world%20applications.%20These%20scenarios%2C%20despite%20realistic%2C%20are%0Aoften%20neglected%20in%20the%20literature%2C%20exposing%20these%20solutions%20to%20important%0Aperformance%20degradation.%20In%20this%20work%2C%20we%20tackle%20this%20issue%20by%20introducing%2C%20for%0Athe%20first%20time%2C%20an%20approach%20for%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20in%20sparse%0ATAL%2C%20which%20we%20refer%20to%20as%20Semantic%20Adversarial%20unsupervised%20Domain%20Adaptation%0A%28SADA%29.%20Our%20contributions%20are%20threefold%3A%20%281%29%20we%20pioneer%20the%20development%20of%20a%0Adomain%20adaptation%20model%20that%20operates%20on%20realistic%20sparse%20action%20detection%0Abenchmarks%3B%20%282%29%20we%20tackle%20the%20limitations%20of%20global-distribution%20alignment%0Atechniques%20by%20introducing%20a%20novel%20adversarial%20loss%20that%20is%20sensitive%20to%20local%0Aclass%20distributions%2C%20ensuring%20finer-grained%20adaptation%3B%20and%20%283%29%20we%20present%20a%0Anovel%20set%20of%20benchmarks%20based%20on%20EpicKitchens100%20and%20CharadesEgo%2C%20that%20evaluate%0Amultiple%20domain%20shifts%20in%20a%20comprehensive%20manner.%20Our%20experiments%20indicate%20that%0ASADA%20improves%20the%20adaptation%20across%20domains%20when%20compared%20to%20fully%20supervised%0Astate-of-the-art%20and%20alternative%20UDA%20methods%2C%20attaining%20a%20performance%20boost%20of%0Aup%20to%206.14%25%20mAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13377v2&entry.124074799=Read"},
{"title": "Advancements in Radar Odometry", "author": "Matteo Frosi and Mirko Usuelli and Matteo Matteucci", "abstract": "  Radar odometry estimation has emerged as a critical technique in the field of\nautonomous navigation, providing robust and reliable motion estimation under\nvarious environmental conditions. Despite its potential, the complex nature of\nradar signals and the inherent challenges associated with processing these\nsignals have limited the widespread adoption of this technology. This paper\naims to address these challenges by proposing novel improvements to an existing\nmethod for radar odometry estimation, designed to enhance accuracy and\nreliability in diverse scenarios. Our pipeline consists of filtering, motion\ncompensation, oriented surface points computation, smoothing, one-to-many radar\nscan registration, and pose refinement. The developed method enforces local\nunderstanding of the scene, by adding additional information through smoothing\ntechniques, and alignment of consecutive scans, as a refinement posterior to\nthe one-to-many registration. We present an in-depth investigation of the\ncontribution of each improvement to the localization accuracy, and we benchmark\nour system on the sequences of the main datasets for radar understanding, i.e.,\nthe Oxford Radar RobotCar, MulRan, and Boreas datasets. The proposed pipeline\nis able to achieve superior results, on all scenarios considered and under\nharsh environmental constraints.\n", "link": "http://arxiv.org/abs/2310.12729v2", "date": "2024-04-05", "relevancy": 2.288, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6001}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5641}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.547}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Advancements%20in%20Radar%20Odometry&body=Title%3A%20Advancements%20in%20Radar%20Odometry%0AAuthor%3A%20Matteo%20Frosi%20and%20Mirko%20Usuelli%20and%20Matteo%20Matteucci%0AAbstract%3A%20%20%20Radar%20odometry%20estimation%20has%20emerged%20as%20a%20critical%20technique%20in%20the%20field%20of%0Aautonomous%20navigation%2C%20providing%20robust%20and%20reliable%20motion%20estimation%20under%0Avarious%20environmental%20conditions.%20Despite%20its%20potential%2C%20the%20complex%20nature%20of%0Aradar%20signals%20and%20the%20inherent%20challenges%20associated%20with%20processing%20these%0Asignals%20have%20limited%20the%20widespread%20adoption%20of%20this%20technology.%20This%20paper%0Aaims%20to%20address%20these%20challenges%20by%20proposing%20novel%20improvements%20to%20an%20existing%0Amethod%20for%20radar%20odometry%20estimation%2C%20designed%20to%20enhance%20accuracy%20and%0Areliability%20in%20diverse%20scenarios.%20Our%20pipeline%20consists%20of%20filtering%2C%20motion%0Acompensation%2C%20oriented%20surface%20points%20computation%2C%20smoothing%2C%20one-to-many%20radar%0Ascan%20registration%2C%20and%20pose%20refinement.%20The%20developed%20method%20enforces%20local%0Aunderstanding%20of%20the%20scene%2C%20by%20adding%20additional%20information%20through%20smoothing%0Atechniques%2C%20and%20alignment%20of%20consecutive%20scans%2C%20as%20a%20refinement%20posterior%20to%0Athe%20one-to-many%20registration.%20We%20present%20an%20in-depth%20investigation%20of%20the%0Acontribution%20of%20each%20improvement%20to%20the%20localization%20accuracy%2C%20and%20we%20benchmark%0Aour%20system%20on%20the%20sequences%20of%20the%20main%20datasets%20for%20radar%20understanding%2C%20i.e.%2C%0Athe%20Oxford%20Radar%20RobotCar%2C%20MulRan%2C%20and%20Boreas%20datasets.%20The%20proposed%20pipeline%0Ais%20able%20to%20achieve%20superior%20results%2C%20on%20all%20scenarios%20considered%20and%20under%0Aharsh%20environmental%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12729v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancements%20in%20Radar%20Odometry&entry.906535625=Matteo%20Frosi%20and%20Mirko%20Usuelli%20and%20Matteo%20Matteucci&entry.1292438233=%20%20Radar%20odometry%20estimation%20has%20emerged%20as%20a%20critical%20technique%20in%20the%20field%20of%0Aautonomous%20navigation%2C%20providing%20robust%20and%20reliable%20motion%20estimation%20under%0Avarious%20environmental%20conditions.%20Despite%20its%20potential%2C%20the%20complex%20nature%20of%0Aradar%20signals%20and%20the%20inherent%20challenges%20associated%20with%20processing%20these%0Asignals%20have%20limited%20the%20widespread%20adoption%20of%20this%20technology.%20This%20paper%0Aaims%20to%20address%20these%20challenges%20by%20proposing%20novel%20improvements%20to%20an%20existing%0Amethod%20for%20radar%20odometry%20estimation%2C%20designed%20to%20enhance%20accuracy%20and%0Areliability%20in%20diverse%20scenarios.%20Our%20pipeline%20consists%20of%20filtering%2C%20motion%0Acompensation%2C%20oriented%20surface%20points%20computation%2C%20smoothing%2C%20one-to-many%20radar%0Ascan%20registration%2C%20and%20pose%20refinement.%20The%20developed%20method%20enforces%20local%0Aunderstanding%20of%20the%20scene%2C%20by%20adding%20additional%20information%20through%20smoothing%0Atechniques%2C%20and%20alignment%20of%20consecutive%20scans%2C%20as%20a%20refinement%20posterior%20to%0Athe%20one-to-many%20registration.%20We%20present%20an%20in-depth%20investigation%20of%20the%0Acontribution%20of%20each%20improvement%20to%20the%20localization%20accuracy%2C%20and%20we%20benchmark%0Aour%20system%20on%20the%20sequences%20of%20the%20main%20datasets%20for%20radar%20understanding%2C%20i.e.%2C%0Athe%20Oxford%20Radar%20RobotCar%2C%20MulRan%2C%20and%20Boreas%20datasets.%20The%20proposed%20pipeline%0Ais%20able%20to%20achieve%20superior%20results%2C%20on%20all%20scenarios%20considered%20and%20under%0Aharsh%20environmental%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12729v2&entry.124074799=Read"},
{"title": "Chat-UniVi: Unified Visual Representation Empowers Large Language Models\n  with Image and Video Understanding", "author": "Peng Jin and Ryuichi Takanobu and Wancai Zhang and Xiaochun Cao and Li Yuan", "abstract": "  Large language models have demonstrated impressive universal capabilities\nacross a wide range of open-ended tasks and have extended their utility to\nencompass multimodal conversations. However, existing methods encounter\nchallenges in effectively handling both image and video understanding,\nparticularly with limited visual tokens. In this work, we introduce Chat-UniVi,\na Unified Vision-language model capable of comprehending and engaging in\nconversations involving images and videos through a unified visual\nrepresentation. Specifically, we employ a set of dynamic visual tokens to\nuniformly represent images and videos. This representation framework empowers\nthe model to efficiently utilize a limited number of visual tokens to\nsimultaneously capture the spatial details necessary for images and the\ncomprehensive temporal relationship required for videos. Moreover, we leverage\na multi-scale representation, enabling the model to perceive both high-level\nsemantic concepts and low-level visual details. Notably, Chat-UniVi is trained\non a mixed dataset containing both images and videos, allowing direct\napplication to tasks involving both mediums without requiring any\nmodifications. Extensive experimental results demonstrate that Chat-UniVi\nconsistently outperforms even existing methods exclusively designed for either\nimages or videos. Code is available at\nhttps://github.com/PKU-YuanGroup/Chat-UniVi.\n", "link": "http://arxiv.org/abs/2311.08046v3", "date": "2024-04-05", "relevancy": 2.2796, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.591}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5627}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5351}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Chat-UniVi%3A%20Unified%20Visual%20Representation%20Empowers%20Large%20Language%20Models%0A%20%20with%20Image%20and%20Video%20Understanding&body=Title%3A%20Chat-UniVi%3A%20Unified%20Visual%20Representation%20Empowers%20Large%20Language%20Models%0A%20%20with%20Image%20and%20Video%20Understanding%0AAuthor%3A%20Peng%20Jin%20and%20Ryuichi%20Takanobu%20and%20Wancai%20Zhang%20and%20Xiaochun%20Cao%20and%20Li%20Yuan%0AAbstract%3A%20%20%20Large%20language%20models%20have%20demonstrated%20impressive%20universal%20capabilities%0Aacross%20a%20wide%20range%20of%20open-ended%20tasks%20and%20have%20extended%20their%20utility%20to%0Aencompass%20multimodal%20conversations.%20However%2C%20existing%20methods%20encounter%0Achallenges%20in%20effectively%20handling%20both%20image%20and%20video%20understanding%2C%0Aparticularly%20with%20limited%20visual%20tokens.%20In%20this%20work%2C%20we%20introduce%20Chat-UniVi%2C%0Aa%20Unified%20Vision-language%20model%20capable%20of%20comprehending%20and%20engaging%20in%0Aconversations%20involving%20images%20and%20videos%20through%20a%20unified%20visual%0Arepresentation.%20Specifically%2C%20we%20employ%20a%20set%20of%20dynamic%20visual%20tokens%20to%0Auniformly%20represent%20images%20and%20videos.%20This%20representation%20framework%20empowers%0Athe%20model%20to%20efficiently%20utilize%20a%20limited%20number%20of%20visual%20tokens%20to%0Asimultaneously%20capture%20the%20spatial%20details%20necessary%20for%20images%20and%20the%0Acomprehensive%20temporal%20relationship%20required%20for%20videos.%20Moreover%2C%20we%20leverage%0Aa%20multi-scale%20representation%2C%20enabling%20the%20model%20to%20perceive%20both%20high-level%0Asemantic%20concepts%20and%20low-level%20visual%20details.%20Notably%2C%20Chat-UniVi%20is%20trained%0Aon%20a%20mixed%20dataset%20containing%20both%20images%20and%20videos%2C%20allowing%20direct%0Aapplication%20to%20tasks%20involving%20both%20mediums%20without%20requiring%20any%0Amodifications.%20Extensive%20experimental%20results%20demonstrate%20that%20Chat-UniVi%0Aconsistently%20outperforms%20even%20existing%20methods%20exclusively%20designed%20for%20either%0Aimages%20or%20videos.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PKU-YuanGroup/Chat-UniVi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08046v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chat-UniVi%3A%20Unified%20Visual%20Representation%20Empowers%20Large%20Language%20Models%0A%20%20with%20Image%20and%20Video%20Understanding&entry.906535625=Peng%20Jin%20and%20Ryuichi%20Takanobu%20and%20Wancai%20Zhang%20and%20Xiaochun%20Cao%20and%20Li%20Yuan&entry.1292438233=%20%20Large%20language%20models%20have%20demonstrated%20impressive%20universal%20capabilities%0Aacross%20a%20wide%20range%20of%20open-ended%20tasks%20and%20have%20extended%20their%20utility%20to%0Aencompass%20multimodal%20conversations.%20However%2C%20existing%20methods%20encounter%0Achallenges%20in%20effectively%20handling%20both%20image%20and%20video%20understanding%2C%0Aparticularly%20with%20limited%20visual%20tokens.%20In%20this%20work%2C%20we%20introduce%20Chat-UniVi%2C%0Aa%20Unified%20Vision-language%20model%20capable%20of%20comprehending%20and%20engaging%20in%0Aconversations%20involving%20images%20and%20videos%20through%20a%20unified%20visual%0Arepresentation.%20Specifically%2C%20we%20employ%20a%20set%20of%20dynamic%20visual%20tokens%20to%0Auniformly%20represent%20images%20and%20videos.%20This%20representation%20framework%20empowers%0Athe%20model%20to%20efficiently%20utilize%20a%20limited%20number%20of%20visual%20tokens%20to%0Asimultaneously%20capture%20the%20spatial%20details%20necessary%20for%20images%20and%20the%0Acomprehensive%20temporal%20relationship%20required%20for%20videos.%20Moreover%2C%20we%20leverage%0Aa%20multi-scale%20representation%2C%20enabling%20the%20model%20to%20perceive%20both%20high-level%0Asemantic%20concepts%20and%20low-level%20visual%20details.%20Notably%2C%20Chat-UniVi%20is%20trained%0Aon%20a%20mixed%20dataset%20containing%20both%20images%20and%20videos%2C%20allowing%20direct%0Aapplication%20to%20tasks%20involving%20both%20mediums%20without%20requiring%20any%0Amodifications.%20Extensive%20experimental%20results%20demonstrate%20that%20Chat-UniVi%0Aconsistently%20outperforms%20even%20existing%20methods%20exclusively%20designed%20for%20either%0Aimages%20or%20videos.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PKU-YuanGroup/Chat-UniVi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08046v3&entry.124074799=Read"},
{"title": "EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised\n  Semantic Segmentation", "author": "Chanyoung Kim and Woojung Han and Dayun Ju and Seong Jae Hwang", "abstract": "  Semantic segmentation has innately relied on extensive pixel-level annotated\ndata, leading to the emergence of unsupervised methodologies. Among them,\nleveraging self-supervised Vision Transformers for unsupervised semantic\nsegmentation (USS) has been making steady progress with expressive deep\nfeatures. Yet, for semantically segmenting images with complex objects, a\npredominant challenge remains: the lack of explicit object-level semantic\nencoding in patch-level features. This technical limitation often leads to\ninadequate segmentation of complex objects with diverse structures. To address\nthis gap, we present a novel approach, EAGLE, which emphasizes object-centric\nrepresentation learning for unsupervised semantic segmentation. Specifically,\nwe introduce EiCue, a spectral technique providing semantic and structural cues\nthrough an eigenbasis derived from the semantic similarity matrix of deep image\nfeatures and color affinity from an image. Further, by incorporating our\nobject-centric contrastive loss with EiCue, we guide our model to learn\nobject-level representations with intra- and inter-image object-feature\nconsistency, thereby enhancing semantic accuracy. Extensive experiments on\nCOCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art\nUSS results of EAGLE with accurate and consistent semantic segmentation across\ncomplex scenes.\n", "link": "http://arxiv.org/abs/2403.01482v4", "date": "2024-04-05", "relevancy": 2.2107, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5569}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5534}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5502}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EAGLE%3A%20Eigen%20Aggregation%20Learning%20for%20Object-Centric%20Unsupervised%0A%20%20Semantic%20Segmentation&body=Title%3A%20EAGLE%3A%20Eigen%20Aggregation%20Learning%20for%20Object-Centric%20Unsupervised%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Chanyoung%20Kim%20and%20Woojung%20Han%20and%20Dayun%20Ju%20and%20Seong%20Jae%20Hwang%0AAbstract%3A%20%20%20Semantic%20segmentation%20has%20innately%20relied%20on%20extensive%20pixel-level%20annotated%0Adata%2C%20leading%20to%20the%20emergence%20of%20unsupervised%20methodologies.%20Among%20them%2C%0Aleveraging%20self-supervised%20Vision%20Transformers%20for%20unsupervised%20semantic%0Asegmentation%20%28USS%29%20has%20been%20making%20steady%20progress%20with%20expressive%20deep%0Afeatures.%20Yet%2C%20for%20semantically%20segmenting%20images%20with%20complex%20objects%2C%20a%0Apredominant%20challenge%20remains%3A%20the%20lack%20of%20explicit%20object-level%20semantic%0Aencoding%20in%20patch-level%20features.%20This%20technical%20limitation%20often%20leads%20to%0Ainadequate%20segmentation%20of%20complex%20objects%20with%20diverse%20structures.%20To%20address%0Athis%20gap%2C%20we%20present%20a%20novel%20approach%2C%20EAGLE%2C%20which%20emphasizes%20object-centric%0Arepresentation%20learning%20for%20unsupervised%20semantic%20segmentation.%20Specifically%2C%0Awe%20introduce%20EiCue%2C%20a%20spectral%20technique%20providing%20semantic%20and%20structural%20cues%0Athrough%20an%20eigenbasis%20derived%20from%20the%20semantic%20similarity%20matrix%20of%20deep%20image%0Afeatures%20and%20color%20affinity%20from%20an%20image.%20Further%2C%20by%20incorporating%20our%0Aobject-centric%20contrastive%20loss%20with%20EiCue%2C%20we%20guide%20our%20model%20to%20learn%0Aobject-level%20representations%20with%20intra-%20and%20inter-image%20object-feature%0Aconsistency%2C%20thereby%20enhancing%20semantic%20accuracy.%20Extensive%20experiments%20on%0ACOCO-Stuff%2C%20Cityscapes%2C%20and%20Potsdam-3%20datasets%20demonstrate%20the%20state-of-the-art%0AUSS%20results%20of%20EAGLE%20with%20accurate%20and%20consistent%20semantic%20segmentation%20across%0Acomplex%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01482v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EAGLE%3A%20Eigen%20Aggregation%20Learning%20for%20Object-Centric%20Unsupervised%0A%20%20Semantic%20Segmentation&entry.906535625=Chanyoung%20Kim%20and%20Woojung%20Han%20and%20Dayun%20Ju%20and%20Seong%20Jae%20Hwang&entry.1292438233=%20%20Semantic%20segmentation%20has%20innately%20relied%20on%20extensive%20pixel-level%20annotated%0Adata%2C%20leading%20to%20the%20emergence%20of%20unsupervised%20methodologies.%20Among%20them%2C%0Aleveraging%20self-supervised%20Vision%20Transformers%20for%20unsupervised%20semantic%0Asegmentation%20%28USS%29%20has%20been%20making%20steady%20progress%20with%20expressive%20deep%0Afeatures.%20Yet%2C%20for%20semantically%20segmenting%20images%20with%20complex%20objects%2C%20a%0Apredominant%20challenge%20remains%3A%20the%20lack%20of%20explicit%20object-level%20semantic%0Aencoding%20in%20patch-level%20features.%20This%20technical%20limitation%20often%20leads%20to%0Ainadequate%20segmentation%20of%20complex%20objects%20with%20diverse%20structures.%20To%20address%0Athis%20gap%2C%20we%20present%20a%20novel%20approach%2C%20EAGLE%2C%20which%20emphasizes%20object-centric%0Arepresentation%20learning%20for%20unsupervised%20semantic%20segmentation.%20Specifically%2C%0Awe%20introduce%20EiCue%2C%20a%20spectral%20technique%20providing%20semantic%20and%20structural%20cues%0Athrough%20an%20eigenbasis%20derived%20from%20the%20semantic%20similarity%20matrix%20of%20deep%20image%0Afeatures%20and%20color%20affinity%20from%20an%20image.%20Further%2C%20by%20incorporating%20our%0Aobject-centric%20contrastive%20loss%20with%20EiCue%2C%20we%20guide%20our%20model%20to%20learn%0Aobject-level%20representations%20with%20intra-%20and%20inter-image%20object-feature%0Aconsistency%2C%20thereby%20enhancing%20semantic%20accuracy.%20Extensive%20experiments%20on%0ACOCO-Stuff%2C%20Cityscapes%2C%20and%20Potsdam-3%20datasets%20demonstrate%20the%20state-of-the-art%0AUSS%20results%20of%20EAGLE%20with%20accurate%20and%20consistent%20semantic%20segmentation%20across%0Acomplex%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01482v4&entry.124074799=Read"},
{"title": "Open-vocabulary object 6D pose estimation", "author": "Jaime Corsetti and Davide Boscaini and Changjae Oh and Andrea Cavallaro and Fabio Poiesi", "abstract": "  We introduce the new setting of open-vocabulary object 6D pose estimation, in\nwhich a textual prompt is used to specify the object of interest. In contrast\nto existing approaches, in our setting (i) the object of interest is specified\nsolely through the textual prompt, (ii) no object model (e.g., CAD or video\nsequence) is required at inference, and (iii) the object is imaged from two\nRGBD viewpoints of different scenes. To operate in this setting, we introduce a\nnovel approach that leverages a Vision-Language Model to segment the object of\ninterest from the scenes and to estimate its relative 6D pose. The key of our\napproach is a carefully devised strategy to fuse object-level information\nprovided by the prompt with local image features, resulting in a feature space\nthat can generalize to novel concepts. We validate our approach on a new\nbenchmark based on two popular datasets, REAL275 and Toyota-Light, which\ncollectively encompass 34 object instances appearing in four thousand image\npairs. The results demonstrate that our approach outperforms both a\nwell-established hand-crafted method and a recent deep learning-based baseline\nin estimating the relative 6D pose of objects in different scenes. Code and\ndataset are available at https://jcorsetti.github.io/oryon.\n", "link": "http://arxiv.org/abs/2312.00690v3", "date": "2024-04-05", "relevancy": 2.2094, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5686}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.556}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5422}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Open-vocabulary%20object%206D%20pose%20estimation&body=Title%3A%20Open-vocabulary%20object%206D%20pose%20estimation%0AAuthor%3A%20Jaime%20Corsetti%20and%20Davide%20Boscaini%20and%20Changjae%20Oh%20and%20Andrea%20Cavallaro%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20We%20introduce%20the%20new%20setting%20of%20open-vocabulary%20object%206D%20pose%20estimation%2C%20in%0Awhich%20a%20textual%20prompt%20is%20used%20to%20specify%20the%20object%20of%20interest.%20In%20contrast%0Ato%20existing%20approaches%2C%20in%20our%20setting%20%28i%29%20the%20object%20of%20interest%20is%20specified%0Asolely%20through%20the%20textual%20prompt%2C%20%28ii%29%20no%20object%20model%20%28e.g.%2C%20CAD%20or%20video%0Asequence%29%20is%20required%20at%20inference%2C%20and%20%28iii%29%20the%20object%20is%20imaged%20from%20two%0ARGBD%20viewpoints%20of%20different%20scenes.%20To%20operate%20in%20this%20setting%2C%20we%20introduce%20a%0Anovel%20approach%20that%20leverages%20a%20Vision-Language%20Model%20to%20segment%20the%20object%20of%0Ainterest%20from%20the%20scenes%20and%20to%20estimate%20its%20relative%206D%20pose.%20The%20key%20of%20our%0Aapproach%20is%20a%20carefully%20devised%20strategy%20to%20fuse%20object-level%20information%0Aprovided%20by%20the%20prompt%20with%20local%20image%20features%2C%20resulting%20in%20a%20feature%20space%0Athat%20can%20generalize%20to%20novel%20concepts.%20We%20validate%20our%20approach%20on%20a%20new%0Abenchmark%20based%20on%20two%20popular%20datasets%2C%20REAL275%20and%20Toyota-Light%2C%20which%0Acollectively%20encompass%2034%20object%20instances%20appearing%20in%20four%20thousand%20image%0Apairs.%20The%20results%20demonstrate%20that%20our%20approach%20outperforms%20both%20a%0Awell-established%20hand-crafted%20method%20and%20a%20recent%20deep%20learning-based%20baseline%0Ain%20estimating%20the%20relative%206D%20pose%20of%20objects%20in%20different%20scenes.%20Code%20and%0Adataset%20are%20available%20at%20https%3A//jcorsetti.github.io/oryon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00690v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-vocabulary%20object%206D%20pose%20estimation&entry.906535625=Jaime%20Corsetti%20and%20Davide%20Boscaini%20and%20Changjae%20Oh%20and%20Andrea%20Cavallaro%20and%20Fabio%20Poiesi&entry.1292438233=%20%20We%20introduce%20the%20new%20setting%20of%20open-vocabulary%20object%206D%20pose%20estimation%2C%20in%0Awhich%20a%20textual%20prompt%20is%20used%20to%20specify%20the%20object%20of%20interest.%20In%20contrast%0Ato%20existing%20approaches%2C%20in%20our%20setting%20%28i%29%20the%20object%20of%20interest%20is%20specified%0Asolely%20through%20the%20textual%20prompt%2C%20%28ii%29%20no%20object%20model%20%28e.g.%2C%20CAD%20or%20video%0Asequence%29%20is%20required%20at%20inference%2C%20and%20%28iii%29%20the%20object%20is%20imaged%20from%20two%0ARGBD%20viewpoints%20of%20different%20scenes.%20To%20operate%20in%20this%20setting%2C%20we%20introduce%20a%0Anovel%20approach%20that%20leverages%20a%20Vision-Language%20Model%20to%20segment%20the%20object%20of%0Ainterest%20from%20the%20scenes%20and%20to%20estimate%20its%20relative%206D%20pose.%20The%20key%20of%20our%0Aapproach%20is%20a%20carefully%20devised%20strategy%20to%20fuse%20object-level%20information%0Aprovided%20by%20the%20prompt%20with%20local%20image%20features%2C%20resulting%20in%20a%20feature%20space%0Athat%20can%20generalize%20to%20novel%20concepts.%20We%20validate%20our%20approach%20on%20a%20new%0Abenchmark%20based%20on%20two%20popular%20datasets%2C%20REAL275%20and%20Toyota-Light%2C%20which%0Acollectively%20encompass%2034%20object%20instances%20appearing%20in%20four%20thousand%20image%0Apairs.%20The%20results%20demonstrate%20that%20our%20approach%20outperforms%20both%20a%0Awell-established%20hand-crafted%20method%20and%20a%20recent%20deep%20learning-based%20baseline%0Ain%20estimating%20the%20relative%206D%20pose%20of%20objects%20in%20different%20scenes.%20Code%20and%0Adataset%20are%20available%20at%20https%3A//jcorsetti.github.io/oryon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00690v3&entry.124074799=Read"},
{"title": "GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System", "author": "Yidong Gong and Pradeep Kumar", "abstract": "  We hypothesize that the absence of a standardized benchmark has allowed\nseveral fundamental pitfalls in GNN System design and evaluation that the\ncommunity has overlooked. In this work, we propose GNNBench, a plug-and-play\nbenchmarking platform focused on system innovation. GNNBench presents a new\nprotocol to exchange their captive tensor data, supports custom classes in\nSystem APIs, and allows automatic integration of the same system module to many\ndeep learning frameworks, such as PyTorch and TensorFlow. To demonstrate the\nimportance of such a benchmark framework, we integrated several GNN systems.\nOur results show that integration with GNNBench helped us identify several\nmeasurement issues that deserve attention from the community.\n", "link": "http://arxiv.org/abs/2404.04118v1", "date": "2024-04-05", "relevancy": 2.2061, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4752}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4273}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4211}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GNNBENCH%3A%20Fair%20and%20Productive%20Benchmarking%20for%20Single-GPU%20GNN%20System&body=Title%3A%20GNNBENCH%3A%20Fair%20and%20Productive%20Benchmarking%20for%20Single-GPU%20GNN%20System%0AAuthor%3A%20Yidong%20Gong%20and%20Pradeep%20Kumar%0AAbstract%3A%20%20%20We%20hypothesize%20that%20the%20absence%20of%20a%20standardized%20benchmark%20has%20allowed%0Aseveral%20fundamental%20pitfalls%20in%20GNN%20System%20design%20and%20evaluation%20that%20the%0Acommunity%20has%20overlooked.%20In%20this%20work%2C%20we%20propose%20GNNBench%2C%20a%20plug-and-play%0Abenchmarking%20platform%20focused%20on%20system%20innovation.%20GNNBench%20presents%20a%20new%0Aprotocol%20to%20exchange%20their%20captive%20tensor%20data%2C%20supports%20custom%20classes%20in%0ASystem%20APIs%2C%20and%20allows%20automatic%20integration%20of%20the%20same%20system%20module%20to%20many%0Adeep%20learning%20frameworks%2C%20such%20as%20PyTorch%20and%20TensorFlow.%20To%20demonstrate%20the%0Aimportance%20of%20such%20a%20benchmark%20framework%2C%20we%20integrated%20several%20GNN%20systems.%0AOur%20results%20show%20that%20integration%20with%20GNNBench%20helped%20us%20identify%20several%0Ameasurement%20issues%20that%20deserve%20attention%20from%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04118v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNNBENCH%3A%20Fair%20and%20Productive%20Benchmarking%20for%20Single-GPU%20GNN%20System&entry.906535625=Yidong%20Gong%20and%20Pradeep%20Kumar&entry.1292438233=%20%20We%20hypothesize%20that%20the%20absence%20of%20a%20standardized%20benchmark%20has%20allowed%0Aseveral%20fundamental%20pitfalls%20in%20GNN%20System%20design%20and%20evaluation%20that%20the%0Acommunity%20has%20overlooked.%20In%20this%20work%2C%20we%20propose%20GNNBench%2C%20a%20plug-and-play%0Abenchmarking%20platform%20focused%20on%20system%20innovation.%20GNNBench%20presents%20a%20new%0Aprotocol%20to%20exchange%20their%20captive%20tensor%20data%2C%20supports%20custom%20classes%20in%0ASystem%20APIs%2C%20and%20allows%20automatic%20integration%20of%20the%20same%20system%20module%20to%20many%0Adeep%20learning%20frameworks%2C%20such%20as%20PyTorch%20and%20TensorFlow.%20To%20demonstrate%20the%0Aimportance%20of%20such%20a%20benchmark%20framework%2C%20we%20integrated%20several%20GNN%20systems.%0AOur%20results%20show%20that%20integration%20with%20GNNBench%20helped%20us%20identify%20several%0Ameasurement%20issues%20that%20deserve%20attention%20from%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04118v1&entry.124074799=Read"},
{"title": "Embedded Heterogeneous Attention Transformer for Cross-lingual Image\n  Captioning", "author": "Zijie Song and Zhenzhen Hu and Yuanen Zhou and Ye Zhao and Richang Hong and Meng Wang", "abstract": "  Cross-lingual image captioning is a challenging task that requires addressing\nboth cross-lingual and cross-modal obstacles in multimedia analysis. The\ncrucial issue in this task is to model the global and the local matching\nbetween the image and different languages. Existing cross-modal embedding\nmethods based on the transformer architecture oversee the local matching\nbetween the image region and monolingual words, especially when dealing with\ndiverse languages. To overcome these limitations, we propose an Embedded\nHeterogeneous Attention Transformer (EHAT) to establish cross-domain\nrelationships and local correspondences between images and different languages\nby using a heterogeneous network. EHAT comprises Masked Heterogeneous\nCross-attention (MHCA), Heterogeneous Attention Reasoning Network (HARN), and\nHeterogeneous Co-attention (HCA). The HARN serves as the core network and it\ncaptures cross-domain relationships by leveraging visual bounding box\nrepresentation features to connect word features from two languages and to\nlearn heterogeneous maps. MHCA and HCA facilitate cross-domain integration in\nthe encoder through specialized heterogeneous attention mechanisms, enabling a\nsingle model to generate captions in two languages. We evaluate our approach on\nthe MSCOCO dataset to generate captions in English and Chinese, two languages\nthat exhibit significant differences in their language families. The\nexperimental results demonstrate the superior performance of our method\ncompared to existing advanced monolingual methods. Our proposed EHAT framework\neffectively addresses the challenges of cross-lingual image captioning, paving\nthe way for improved multilingual image analysis and understanding.\n", "link": "http://arxiv.org/abs/2307.09915v2", "date": "2024-04-05", "relevancy": 2.2012, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5256}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5252}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Embedded%20Heterogeneous%20Attention%20Transformer%20for%20Cross-lingual%20Image%0A%20%20Captioning&body=Title%3A%20Embedded%20Heterogeneous%20Attention%20Transformer%20for%20Cross-lingual%20Image%0A%20%20Captioning%0AAuthor%3A%20Zijie%20Song%20and%20Zhenzhen%20Hu%20and%20Yuanen%20Zhou%20and%20Ye%20Zhao%20and%20Richang%20Hong%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Cross-lingual%20image%20captioning%20is%20a%20challenging%20task%20that%20requires%20addressing%0Aboth%20cross-lingual%20and%20cross-modal%20obstacles%20in%20multimedia%20analysis.%20The%0Acrucial%20issue%20in%20this%20task%20is%20to%20model%20the%20global%20and%20the%20local%20matching%0Abetween%20the%20image%20and%20different%20languages.%20Existing%20cross-modal%20embedding%0Amethods%20based%20on%20the%20transformer%20architecture%20oversee%20the%20local%20matching%0Abetween%20the%20image%20region%20and%20monolingual%20words%2C%20especially%20when%20dealing%20with%0Adiverse%20languages.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%20Embedded%0AHeterogeneous%20Attention%20Transformer%20%28EHAT%29%20to%20establish%20cross-domain%0Arelationships%20and%20local%20correspondences%20between%20images%20and%20different%20languages%0Aby%20using%20a%20heterogeneous%20network.%20EHAT%20comprises%20Masked%20Heterogeneous%0ACross-attention%20%28MHCA%29%2C%20Heterogeneous%20Attention%20Reasoning%20Network%20%28HARN%29%2C%20and%0AHeterogeneous%20Co-attention%20%28HCA%29.%20The%20HARN%20serves%20as%20the%20core%20network%20and%20it%0Acaptures%20cross-domain%20relationships%20by%20leveraging%20visual%20bounding%20box%0Arepresentation%20features%20to%20connect%20word%20features%20from%20two%20languages%20and%20to%0Alearn%20heterogeneous%20maps.%20MHCA%20and%20HCA%20facilitate%20cross-domain%20integration%20in%0Athe%20encoder%20through%20specialized%20heterogeneous%20attention%20mechanisms%2C%20enabling%20a%0Asingle%20model%20to%20generate%20captions%20in%20two%20languages.%20We%20evaluate%20our%20approach%20on%0Athe%20MSCOCO%20dataset%20to%20generate%20captions%20in%20English%20and%20Chinese%2C%20two%20languages%0Athat%20exhibit%20significant%20differences%20in%20their%20language%20families.%20The%0Aexperimental%20results%20demonstrate%20the%20superior%20performance%20of%20our%20method%0Acompared%20to%20existing%20advanced%20monolingual%20methods.%20Our%20proposed%20EHAT%20framework%0Aeffectively%20addresses%20the%20challenges%20of%20cross-lingual%20image%20captioning%2C%20paving%0Athe%20way%20for%20improved%20multilingual%20image%20analysis%20and%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09915v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20Heterogeneous%20Attention%20Transformer%20for%20Cross-lingual%20Image%0A%20%20Captioning&entry.906535625=Zijie%20Song%20and%20Zhenzhen%20Hu%20and%20Yuanen%20Zhou%20and%20Ye%20Zhao%20and%20Richang%20Hong%20and%20Meng%20Wang&entry.1292438233=%20%20Cross-lingual%20image%20captioning%20is%20a%20challenging%20task%20that%20requires%20addressing%0Aboth%20cross-lingual%20and%20cross-modal%20obstacles%20in%20multimedia%20analysis.%20The%0Acrucial%20issue%20in%20this%20task%20is%20to%20model%20the%20global%20and%20the%20local%20matching%0Abetween%20the%20image%20and%20different%20languages.%20Existing%20cross-modal%20embedding%0Amethods%20based%20on%20the%20transformer%20architecture%20oversee%20the%20local%20matching%0Abetween%20the%20image%20region%20and%20monolingual%20words%2C%20especially%20when%20dealing%20with%0Adiverse%20languages.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%20Embedded%0AHeterogeneous%20Attention%20Transformer%20%28EHAT%29%20to%20establish%20cross-domain%0Arelationships%20and%20local%20correspondences%20between%20images%20and%20different%20languages%0Aby%20using%20a%20heterogeneous%20network.%20EHAT%20comprises%20Masked%20Heterogeneous%0ACross-attention%20%28MHCA%29%2C%20Heterogeneous%20Attention%20Reasoning%20Network%20%28HARN%29%2C%20and%0AHeterogeneous%20Co-attention%20%28HCA%29.%20The%20HARN%20serves%20as%20the%20core%20network%20and%20it%0Acaptures%20cross-domain%20relationships%20by%20leveraging%20visual%20bounding%20box%0Arepresentation%20features%20to%20connect%20word%20features%20from%20two%20languages%20and%20to%0Alearn%20heterogeneous%20maps.%20MHCA%20and%20HCA%20facilitate%20cross-domain%20integration%20in%0Athe%20encoder%20through%20specialized%20heterogeneous%20attention%20mechanisms%2C%20enabling%20a%0Asingle%20model%20to%20generate%20captions%20in%20two%20languages.%20We%20evaluate%20our%20approach%20on%0Athe%20MSCOCO%20dataset%20to%20generate%20captions%20in%20English%20and%20Chinese%2C%20two%20languages%0Athat%20exhibit%20significant%20differences%20in%20their%20language%20families.%20The%0Aexperimental%20results%20demonstrate%20the%20superior%20performance%20of%20our%20method%0Acompared%20to%20existing%20advanced%20monolingual%20methods.%20Our%20proposed%20EHAT%20framework%0Aeffectively%20addresses%20the%20challenges%20of%20cross-lingual%20image%20captioning%2C%20paving%0Athe%20way%20for%20improved%20multilingual%20image%20analysis%20and%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09915v2&entry.124074799=Read"},
{"title": "No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D\n  Scene Segmentation", "author": "Xiangyang Zhu and Renrui Zhang and Bowei He and Ziyu Guo and Jiaming Liu and Han Xiao and Chaoyou Fu and Hao Dong and Peng Gao", "abstract": "  To reduce the reliance on large-scale datasets, recent works in 3D\nsegmentation resort to few-shot learning. Current 3D few-shot segmentation\nmethods first pre-train models on 'seen' classes, and then evaluate their\ngeneralization performance on 'unseen' classes. However, the prior pre-training\nstage not only introduces excessive time overhead but also incurs a significant\ndomain gap on 'unseen' classes. To tackle these issues, we propose a\nNon-parametric Network for few-shot 3D Segmentation, Seg-NN, and its Parametric\nvariant, Seg-PN. Without training, Seg-NN extracts dense representations by\nhand-crafted filters and achieves comparable performance to existing parametric\nmodels. Due to the elimination of pre-training, Seg-NN can alleviate the domain\ngap issue and save a substantial amount of time. Based on Seg-NN, Seg-PN only\nrequires training a lightweight QUEry-Support Transferring (QUEST) module,\nwhich enhances the interaction between the support set and query set.\nExperiments suggest that Seg-PN outperforms previous state-of-the-art method by\n+4.19% and +7.71% mIoU on S3DIS and ScanNet datasets respectively, while\nreducing training time by -90%, indicating its effectiveness and efficiency.\n", "link": "http://arxiv.org/abs/2404.04050v1", "date": "2024-04-05", "relevancy": 2.1938, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5718}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5351}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5236}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20No%20Time%20to%20Train%3A%20Empowering%20Non-Parametric%20Networks%20for%20Few-shot%203D%0A%20%20Scene%20Segmentation&body=Title%3A%20No%20Time%20to%20Train%3A%20Empowering%20Non-Parametric%20Networks%20for%20Few-shot%203D%0A%20%20Scene%20Segmentation%0AAuthor%3A%20Xiangyang%20Zhu%20and%20Renrui%20Zhang%20and%20Bowei%20He%20and%20Ziyu%20Guo%20and%20Jiaming%20Liu%20and%20Han%20Xiao%20and%20Chaoyou%20Fu%20and%20Hao%20Dong%20and%20Peng%20Gao%0AAbstract%3A%20%20%20To%20reduce%20the%20reliance%20on%20large-scale%20datasets%2C%20recent%20works%20in%203D%0Asegmentation%20resort%20to%20few-shot%20learning.%20Current%203D%20few-shot%20segmentation%0Amethods%20first%20pre-train%20models%20on%20%27seen%27%20classes%2C%20and%20then%20evaluate%20their%0Ageneralization%20performance%20on%20%27unseen%27%20classes.%20However%2C%20the%20prior%20pre-training%0Astage%20not%20only%20introduces%20excessive%20time%20overhead%20but%20also%20incurs%20a%20significant%0Adomain%20gap%20on%20%27unseen%27%20classes.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%0ANon-parametric%20Network%20for%20few-shot%203D%20Segmentation%2C%20Seg-NN%2C%20and%20its%20Parametric%0Avariant%2C%20Seg-PN.%20Without%20training%2C%20Seg-NN%20extracts%20dense%20representations%20by%0Ahand-crafted%20filters%20and%20achieves%20comparable%20performance%20to%20existing%20parametric%0Amodels.%20Due%20to%20the%20elimination%20of%20pre-training%2C%20Seg-NN%20can%20alleviate%20the%20domain%0Agap%20issue%20and%20save%20a%20substantial%20amount%20of%20time.%20Based%20on%20Seg-NN%2C%20Seg-PN%20only%0Arequires%20training%20a%20lightweight%20QUEry-Support%20Transferring%20%28QUEST%29%20module%2C%0Awhich%20enhances%20the%20interaction%20between%20the%20support%20set%20and%20query%20set.%0AExperiments%20suggest%20that%20Seg-PN%20outperforms%20previous%20state-of-the-art%20method%20by%0A%2B4.19%25%20and%20%2B7.71%25%20mIoU%20on%20S3DIS%20and%20ScanNet%20datasets%20respectively%2C%20while%0Areducing%20training%20time%20by%20-90%25%2C%20indicating%20its%20effectiveness%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04050v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Time%20to%20Train%3A%20Empowering%20Non-Parametric%20Networks%20for%20Few-shot%203D%0A%20%20Scene%20Segmentation&entry.906535625=Xiangyang%20Zhu%20and%20Renrui%20Zhang%20and%20Bowei%20He%20and%20Ziyu%20Guo%20and%20Jiaming%20Liu%20and%20Han%20Xiao%20and%20Chaoyou%20Fu%20and%20Hao%20Dong%20and%20Peng%20Gao&entry.1292438233=%20%20To%20reduce%20the%20reliance%20on%20large-scale%20datasets%2C%20recent%20works%20in%203D%0Asegmentation%20resort%20to%20few-shot%20learning.%20Current%203D%20few-shot%20segmentation%0Amethods%20first%20pre-train%20models%20on%20%27seen%27%20classes%2C%20and%20then%20evaluate%20their%0Ageneralization%20performance%20on%20%27unseen%27%20classes.%20However%2C%20the%20prior%20pre-training%0Astage%20not%20only%20introduces%20excessive%20time%20overhead%20but%20also%20incurs%20a%20significant%0Adomain%20gap%20on%20%27unseen%27%20classes.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%0ANon-parametric%20Network%20for%20few-shot%203D%20Segmentation%2C%20Seg-NN%2C%20and%20its%20Parametric%0Avariant%2C%20Seg-PN.%20Without%20training%2C%20Seg-NN%20extracts%20dense%20representations%20by%0Ahand-crafted%20filters%20and%20achieves%20comparable%20performance%20to%20existing%20parametric%0Amodels.%20Due%20to%20the%20elimination%20of%20pre-training%2C%20Seg-NN%20can%20alleviate%20the%20domain%0Agap%20issue%20and%20save%20a%20substantial%20amount%20of%20time.%20Based%20on%20Seg-NN%2C%20Seg-PN%20only%0Arequires%20training%20a%20lightweight%20QUEry-Support%20Transferring%20%28QUEST%29%20module%2C%0Awhich%20enhances%20the%20interaction%20between%20the%20support%20set%20and%20query%20set.%0AExperiments%20suggest%20that%20Seg-PN%20outperforms%20previous%20state-of-the-art%20method%20by%0A%2B4.19%25%20and%20%2B7.71%25%20mIoU%20on%20S3DIS%20and%20ScanNet%20datasets%20respectively%2C%20while%0Areducing%20training%20time%20by%20-90%25%2C%20indicating%20its%20effectiveness%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04050v1&entry.124074799=Read"},
{"title": "ToolEENet: Tool Affordance 6D Pose Estimation", "author": "Yunlong Wang and Lei Zhang and Yuyang Tu and Hui Zhang and Kaixin Bai and Zhaopeng Chen and Jianwei Zhang", "abstract": "  The exploration of robotic dexterous hands utilizing tools has recently\nattracted considerable attention. A significant challenge in this field is the\nprecise awareness of a tool's pose when grasped, as occlusion by the hand often\ndegrades the quality of the estimation. Additionally, the tool's overall pose\noften fails to accurately represent the contact interaction, thereby limiting\nthe effectiveness of vision-guided, contact-dependent activities. To overcome\nthis limitation, we present the innovative TOOLEE dataset, which, to the best\nof our knowledge, is the first to feature affordance segmentation of a tool's\nend-effector (EE) along with its defined 6D pose based on its usage.\nFurthermore, we propose the ToolEENet framework for accurate 6D pose estimation\nof the tool's EE. This framework begins by segmenting the tool's EE from raw\nRGBD data, then uses a diffusion model-based pose estimator for 6D pose\nestimation at a category-specific level. Addressing the issue of symmetry in\npose estimation, we introduce a symmetry-aware pose representation that\nenhances the consistency of pose estimation. Our approach excels in this field,\ndemonstrating high levels of precision and generalization. Furthermore, it\nshows great promise for application in contact-based manipulation scenarios.\nAll data and codes are available on the project website:\nhttps://yuyangtu.github.io/projectToolEENet.html\n", "link": "http://arxiv.org/abs/2404.04193v1", "date": "2024-04-05", "relevancy": 2.1869, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5679}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5368}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5295}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ToolEENet%3A%20Tool%20Affordance%206D%20Pose%20Estimation&body=Title%3A%20ToolEENet%3A%20Tool%20Affordance%206D%20Pose%20Estimation%0AAuthor%3A%20Yunlong%20Wang%20and%20Lei%20Zhang%20and%20Yuyang%20Tu%20and%20Hui%20Zhang%20and%20Kaixin%20Bai%20and%20Zhaopeng%20Chen%20and%20Jianwei%20Zhang%0AAbstract%3A%20%20%20The%20exploration%20of%20robotic%20dexterous%20hands%20utilizing%20tools%20has%20recently%0Aattracted%20considerable%20attention.%20A%20significant%20challenge%20in%20this%20field%20is%20the%0Aprecise%20awareness%20of%20a%20tool%27s%20pose%20when%20grasped%2C%20as%20occlusion%20by%20the%20hand%20often%0Adegrades%20the%20quality%20of%20the%20estimation.%20Additionally%2C%20the%20tool%27s%20overall%20pose%0Aoften%20fails%20to%20accurately%20represent%20the%20contact%20interaction%2C%20thereby%20limiting%0Athe%20effectiveness%20of%20vision-guided%2C%20contact-dependent%20activities.%20To%20overcome%0Athis%20limitation%2C%20we%20present%20the%20innovative%20TOOLEE%20dataset%2C%20which%2C%20to%20the%20best%0Aof%20our%20knowledge%2C%20is%20the%20first%20to%20feature%20affordance%20segmentation%20of%20a%20tool%27s%0Aend-effector%20%28EE%29%20along%20with%20its%20defined%206D%20pose%20based%20on%20its%20usage.%0AFurthermore%2C%20we%20propose%20the%20ToolEENet%20framework%20for%20accurate%206D%20pose%20estimation%0Aof%20the%20tool%27s%20EE.%20This%20framework%20begins%20by%20segmenting%20the%20tool%27s%20EE%20from%20raw%0ARGBD%20data%2C%20then%20uses%20a%20diffusion%20model-based%20pose%20estimator%20for%206D%20pose%0Aestimation%20at%20a%20category-specific%20level.%20Addressing%20the%20issue%20of%20symmetry%20in%0Apose%20estimation%2C%20we%20introduce%20a%20symmetry-aware%20pose%20representation%20that%0Aenhances%20the%20consistency%20of%20pose%20estimation.%20Our%20approach%20excels%20in%20this%20field%2C%0Ademonstrating%20high%20levels%20of%20precision%20and%20generalization.%20Furthermore%2C%20it%0Ashows%20great%20promise%20for%20application%20in%20contact-based%20manipulation%20scenarios.%0AAll%20data%20and%20codes%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//yuyangtu.github.io/projectToolEENet.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04193v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToolEENet%3A%20Tool%20Affordance%206D%20Pose%20Estimation&entry.906535625=Yunlong%20Wang%20and%20Lei%20Zhang%20and%20Yuyang%20Tu%20and%20Hui%20Zhang%20and%20Kaixin%20Bai%20and%20Zhaopeng%20Chen%20and%20Jianwei%20Zhang&entry.1292438233=%20%20The%20exploration%20of%20robotic%20dexterous%20hands%20utilizing%20tools%20has%20recently%0Aattracted%20considerable%20attention.%20A%20significant%20challenge%20in%20this%20field%20is%20the%0Aprecise%20awareness%20of%20a%20tool%27s%20pose%20when%20grasped%2C%20as%20occlusion%20by%20the%20hand%20often%0Adegrades%20the%20quality%20of%20the%20estimation.%20Additionally%2C%20the%20tool%27s%20overall%20pose%0Aoften%20fails%20to%20accurately%20represent%20the%20contact%20interaction%2C%20thereby%20limiting%0Athe%20effectiveness%20of%20vision-guided%2C%20contact-dependent%20activities.%20To%20overcome%0Athis%20limitation%2C%20we%20present%20the%20innovative%20TOOLEE%20dataset%2C%20which%2C%20to%20the%20best%0Aof%20our%20knowledge%2C%20is%20the%20first%20to%20feature%20affordance%20segmentation%20of%20a%20tool%27s%0Aend-effector%20%28EE%29%20along%20with%20its%20defined%206D%20pose%20based%20on%20its%20usage.%0AFurthermore%2C%20we%20propose%20the%20ToolEENet%20framework%20for%20accurate%206D%20pose%20estimation%0Aof%20the%20tool%27s%20EE.%20This%20framework%20begins%20by%20segmenting%20the%20tool%27s%20EE%20from%20raw%0ARGBD%20data%2C%20then%20uses%20a%20diffusion%20model-based%20pose%20estimator%20for%206D%20pose%0Aestimation%20at%20a%20category-specific%20level.%20Addressing%20the%20issue%20of%20symmetry%20in%0Apose%20estimation%2C%20we%20introduce%20a%20symmetry-aware%20pose%20representation%20that%0Aenhances%20the%20consistency%20of%20pose%20estimation.%20Our%20approach%20excels%20in%20this%20field%2C%0Ademonstrating%20high%20levels%20of%20precision%20and%20generalization.%20Furthermore%2C%20it%0Ashows%20great%20promise%20for%20application%20in%20contact-based%20manipulation%20scenarios.%0AAll%20data%20and%20codes%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//yuyangtu.github.io/projectToolEENet.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04193v1&entry.124074799=Read"},
{"title": "Continual Policy Distillation of Reinforcement Learning-based\n  Controllers for Soft Robotic In-Hand Manipulation", "author": "Lanpei Li and Enrico Donato and Vincenzo Lomonaco and Egidio Falotico", "abstract": "  Dexterous manipulation, often facilitated by multi-fingered robotic hands,\nholds solid impact for real-world applications. Soft robotic hands, due to\ntheir compliant nature, offer flexibility and adaptability during object\ngrasping and manipulation. Yet, benefits come with challenges, particularly in\nthe control development for finger coordination. Reinforcement Learning (RL)\ncan be employed to train object-specific in-hand manipulation policies, but\nlimiting adaptability and generalizability. We introduce a Continual Policy\nDistillation (CPD) framework to acquire a versatile controller for in-hand\nmanipulation, to rotate different objects in shape and size within a\nfour-fingered soft gripper. The framework leverages Policy Distillation (PD) to\ntransfer knowledge from expert policies to a continually evolving student\npolicy network. Exemplar-based rehearsal methods are then integrated to\nmitigate catastrophic forgetting and enhance generalization. The performance of\nthe CPD framework over various replay strategies demonstrates its effectiveness\nin consolidating knowledge from multiple experts and achieving versatile and\nadaptive behaviours for in-hand manipulation tasks.\n", "link": "http://arxiv.org/abs/2404.04219v1", "date": "2024-04-05", "relevancy": 2.17, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5718}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5414}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5319}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Policy%20Distillation%20of%20Reinforcement%20Learning-based%0A%20%20Controllers%20for%20Soft%20Robotic%20In-Hand%20Manipulation&body=Title%3A%20Continual%20Policy%20Distillation%20of%20Reinforcement%20Learning-based%0A%20%20Controllers%20for%20Soft%20Robotic%20In-Hand%20Manipulation%0AAuthor%3A%20Lanpei%20Li%20and%20Enrico%20Donato%20and%20Vincenzo%20Lomonaco%20and%20Egidio%20Falotico%0AAbstract%3A%20%20%20Dexterous%20manipulation%2C%20often%20facilitated%20by%20multi-fingered%20robotic%20hands%2C%0Aholds%20solid%20impact%20for%20real-world%20applications.%20Soft%20robotic%20hands%2C%20due%20to%0Atheir%20compliant%20nature%2C%20offer%20flexibility%20and%20adaptability%20during%20object%0Agrasping%20and%20manipulation.%20Yet%2C%20benefits%20come%20with%20challenges%2C%20particularly%20in%0Athe%20control%20development%20for%20finger%20coordination.%20Reinforcement%20Learning%20%28RL%29%0Acan%20be%20employed%20to%20train%20object-specific%20in-hand%20manipulation%20policies%2C%20but%0Alimiting%20adaptability%20and%20generalizability.%20We%20introduce%20a%20Continual%20Policy%0ADistillation%20%28CPD%29%20framework%20to%20acquire%20a%20versatile%20controller%20for%20in-hand%0Amanipulation%2C%20to%20rotate%20different%20objects%20in%20shape%20and%20size%20within%20a%0Afour-fingered%20soft%20gripper.%20The%20framework%20leverages%20Policy%20Distillation%20%28PD%29%20to%0Atransfer%20knowledge%20from%20expert%20policies%20to%20a%20continually%20evolving%20student%0Apolicy%20network.%20Exemplar-based%20rehearsal%20methods%20are%20then%20integrated%20to%0Amitigate%20catastrophic%20forgetting%20and%20enhance%20generalization.%20The%20performance%20of%0Athe%20CPD%20framework%20over%20various%20replay%20strategies%20demonstrates%20its%20effectiveness%0Ain%20consolidating%20knowledge%20from%20multiple%20experts%20and%20achieving%20versatile%20and%0Aadaptive%20behaviours%20for%20in-hand%20manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04219v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Policy%20Distillation%20of%20Reinforcement%20Learning-based%0A%20%20Controllers%20for%20Soft%20Robotic%20In-Hand%20Manipulation&entry.906535625=Lanpei%20Li%20and%20Enrico%20Donato%20and%20Vincenzo%20Lomonaco%20and%20Egidio%20Falotico&entry.1292438233=%20%20Dexterous%20manipulation%2C%20often%20facilitated%20by%20multi-fingered%20robotic%20hands%2C%0Aholds%20solid%20impact%20for%20real-world%20applications.%20Soft%20robotic%20hands%2C%20due%20to%0Atheir%20compliant%20nature%2C%20offer%20flexibility%20and%20adaptability%20during%20object%0Agrasping%20and%20manipulation.%20Yet%2C%20benefits%20come%20with%20challenges%2C%20particularly%20in%0Athe%20control%20development%20for%20finger%20coordination.%20Reinforcement%20Learning%20%28RL%29%0Acan%20be%20employed%20to%20train%20object-specific%20in-hand%20manipulation%20policies%2C%20but%0Alimiting%20adaptability%20and%20generalizability.%20We%20introduce%20a%20Continual%20Policy%0ADistillation%20%28CPD%29%20framework%20to%20acquire%20a%20versatile%20controller%20for%20in-hand%0Amanipulation%2C%20to%20rotate%20different%20objects%20in%20shape%20and%20size%20within%20a%0Afour-fingered%20soft%20gripper.%20The%20framework%20leverages%20Policy%20Distillation%20%28PD%29%20to%0Atransfer%20knowledge%20from%20expert%20policies%20to%20a%20continually%20evolving%20student%0Apolicy%20network.%20Exemplar-based%20rehearsal%20methods%20are%20then%20integrated%20to%0Amitigate%20catastrophic%20forgetting%20and%20enhance%20generalization.%20The%20performance%20of%0Athe%20CPD%20framework%20over%20various%20replay%20strategies%20demonstrates%20its%20effectiveness%0Ain%20consolidating%20knowledge%20from%20multiple%20experts%20and%20achieving%20versatile%20and%0Aadaptive%20behaviours%20for%20in-hand%20manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04219v1&entry.124074799=Read"},
{"title": "Self-Correcting Self-Consuming Loops for Generative Model Training", "author": "Nate Gillman and Michael Freeman and Daksh Aggarwal and Chia-Hong Hsu and Calvin Luo and Yonglong Tian and Chen Sun", "abstract": "  As synthetic data becomes higher quality and proliferates on the internet,\nmachine learning models are increasingly trained on a mix of human- and\nmachine-generated data. Despite the successful stories of using synthetic data\nfor representation learning, using synthetic data for generative model training\ncreates \"self-consuming loops\" which may lead to training instability or even\ncollapse, unless certain conditions are met. Our paper aims to stabilize\nself-consuming generative model training. Our theoretical results demonstrate\nthat by introducing an idealized correction function, which maps a data point\nto be more likely under the true data distribution, self-consuming loops can be\nmade exponentially more stable. We then propose self-correction functions,\nwhich rely on expert knowledge (e.g. the laws of physics programmed in a\nsimulator), and aim to approximate the idealized corrector automatically and at\nscale. We empirically validate the effectiveness of self-correcting\nself-consuming loops on the challenging human motion synthesis task, and\nobserve that it successfully avoids model collapse, even when the ratio of\nsynthetic data to real data is as high as 100%.\n", "link": "http://arxiv.org/abs/2402.07087v2", "date": "2024-04-05", "relevancy": 2.1697, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5775}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5524}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5184}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Correcting%20Self-Consuming%20Loops%20for%20Generative%20Model%20Training&body=Title%3A%20Self-Correcting%20Self-Consuming%20Loops%20for%20Generative%20Model%20Training%0AAuthor%3A%20Nate%20Gillman%20and%20Michael%20Freeman%20and%20Daksh%20Aggarwal%20and%20Chia-Hong%20Hsu%20and%20Calvin%20Luo%20and%20Yonglong%20Tian%20and%20Chen%20Sun%0AAbstract%3A%20%20%20As%20synthetic%20data%20becomes%20higher%20quality%20and%20proliferates%20on%20the%20internet%2C%0Amachine%20learning%20models%20are%20increasingly%20trained%20on%20a%20mix%20of%20human-%20and%0Amachine-generated%20data.%20Despite%20the%20successful%20stories%20of%20using%20synthetic%20data%0Afor%20representation%20learning%2C%20using%20synthetic%20data%20for%20generative%20model%20training%0Acreates%20%22self-consuming%20loops%22%20which%20may%20lead%20to%20training%20instability%20or%20even%0Acollapse%2C%20unless%20certain%20conditions%20are%20met.%20Our%20paper%20aims%20to%20stabilize%0Aself-consuming%20generative%20model%20training.%20Our%20theoretical%20results%20demonstrate%0Athat%20by%20introducing%20an%20idealized%20correction%20function%2C%20which%20maps%20a%20data%20point%0Ato%20be%20more%20likely%20under%20the%20true%20data%20distribution%2C%20self-consuming%20loops%20can%20be%0Amade%20exponentially%20more%20stable.%20We%20then%20propose%20self-correction%20functions%2C%0Awhich%20rely%20on%20expert%20knowledge%20%28e.g.%20the%20laws%20of%20physics%20programmed%20in%20a%0Asimulator%29%2C%20and%20aim%20to%20approximate%20the%20idealized%20corrector%20automatically%20and%20at%0Ascale.%20We%20empirically%20validate%20the%20effectiveness%20of%20self-correcting%0Aself-consuming%20loops%20on%20the%20challenging%20human%20motion%20synthesis%20task%2C%20and%0Aobserve%20that%20it%20successfully%20avoids%20model%20collapse%2C%20even%20when%20the%20ratio%20of%0Asynthetic%20data%20to%20real%20data%20is%20as%20high%20as%20100%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07087v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Correcting%20Self-Consuming%20Loops%20for%20Generative%20Model%20Training&entry.906535625=Nate%20Gillman%20and%20Michael%20Freeman%20and%20Daksh%20Aggarwal%20and%20Chia-Hong%20Hsu%20and%20Calvin%20Luo%20and%20Yonglong%20Tian%20and%20Chen%20Sun&entry.1292438233=%20%20As%20synthetic%20data%20becomes%20higher%20quality%20and%20proliferates%20on%20the%20internet%2C%0Amachine%20learning%20models%20are%20increasingly%20trained%20on%20a%20mix%20of%20human-%20and%0Amachine-generated%20data.%20Despite%20the%20successful%20stories%20of%20using%20synthetic%20data%0Afor%20representation%20learning%2C%20using%20synthetic%20data%20for%20generative%20model%20training%0Acreates%20%22self-consuming%20loops%22%20which%20may%20lead%20to%20training%20instability%20or%20even%0Acollapse%2C%20unless%20certain%20conditions%20are%20met.%20Our%20paper%20aims%20to%20stabilize%0Aself-consuming%20generative%20model%20training.%20Our%20theoretical%20results%20demonstrate%0Athat%20by%20introducing%20an%20idealized%20correction%20function%2C%20which%20maps%20a%20data%20point%0Ato%20be%20more%20likely%20under%20the%20true%20data%20distribution%2C%20self-consuming%20loops%20can%20be%0Amade%20exponentially%20more%20stable.%20We%20then%20propose%20self-correction%20functions%2C%0Awhich%20rely%20on%20expert%20knowledge%20%28e.g.%20the%20laws%20of%20physics%20programmed%20in%20a%0Asimulator%29%2C%20and%20aim%20to%20approximate%20the%20idealized%20corrector%20automatically%20and%20at%0Ascale.%20We%20empirically%20validate%20the%20effectiveness%20of%20self-correcting%0Aself-consuming%20loops%20on%20the%20challenging%20human%20motion%20synthesis%20task%2C%20and%0Aobserve%20that%20it%20successfully%20avoids%20model%20collapse%2C%20even%20when%20the%20ratio%20of%0Asynthetic%20data%20to%20real%20data%20is%20as%20high%20as%20100%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07087v2&entry.124074799=Read"},
{"title": "On Inherent Adversarial Robustness of Active Vision Systems", "author": "Amitangshu Mukherjee and Timur Ibrayev and Kaushik Roy", "abstract": "  Current Deep Neural Networks are vulnerable to adversarial examples, which\nalter their predictions by adding carefully crafted noise. Since human eyes are\nrobust to such inputs, it is possible that the vulnerability stems from the\nstandard way of processing inputs in one shot by processing every pixel with\nthe same importance. In contrast, neuroscience suggests that the human vision\nsystem can differentiate salient features by (1) switching between multiple\nfixation points (saccades) and (2) processing the surrounding with a\nnon-uniform external resolution (foveation). In this work, we advocate that the\nintegration of such active vision mechanisms into current deep learning systems\ncan offer robustness benefits. Specifically, we empirically demonstrate the\ninherent robustness of two active vision methods - GFNet and FALcon - under a\nblack box threat model. By learning and inferencing based on downsampled\nglimpses obtained from multiple distinct fixation points within an input, we\nshow that these active methods achieve (2-3) times greater robustness compared\nto a standard passive convolutional network under state-of-the-art adversarial\nattacks. More importantly, we provide illustrative and interpretable\nvisualization analysis that demonstrates how performing inference from distinct\nfixation points makes active vision methods less vulnerable to malicious\ninputs.\n", "link": "http://arxiv.org/abs/2404.00185v2", "date": "2024-04-05", "relevancy": 2.1621, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5491}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5348}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5332}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Inherent%20Adversarial%20Robustness%20of%20Active%20Vision%20Systems&body=Title%3A%20On%20Inherent%20Adversarial%20Robustness%20of%20Active%20Vision%20Systems%0AAuthor%3A%20Amitangshu%20Mukherjee%20and%20Timur%20Ibrayev%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Current%20Deep%20Neural%20Networks%20are%20vulnerable%20to%20adversarial%20examples%2C%20which%0Aalter%20their%20predictions%20by%20adding%20carefully%20crafted%20noise.%20Since%20human%20eyes%20are%0Arobust%20to%20such%20inputs%2C%20it%20is%20possible%20that%20the%20vulnerability%20stems%20from%20the%0Astandard%20way%20of%20processing%20inputs%20in%20one%20shot%20by%20processing%20every%20pixel%20with%0Athe%20same%20importance.%20In%20contrast%2C%20neuroscience%20suggests%20that%20the%20human%20vision%0Asystem%20can%20differentiate%20salient%20features%20by%20%281%29%20switching%20between%20multiple%0Afixation%20points%20%28saccades%29%20and%20%282%29%20processing%20the%20surrounding%20with%20a%0Anon-uniform%20external%20resolution%20%28foveation%29.%20In%20this%20work%2C%20we%20advocate%20that%20the%0Aintegration%20of%20such%20active%20vision%20mechanisms%20into%20current%20deep%20learning%20systems%0Acan%20offer%20robustness%20benefits.%20Specifically%2C%20we%20empirically%20demonstrate%20the%0Ainherent%20robustness%20of%20two%20active%20vision%20methods%20-%20GFNet%20and%20FALcon%20-%20under%20a%0Ablack%20box%20threat%20model.%20By%20learning%20and%20inferencing%20based%20on%20downsampled%0Aglimpses%20obtained%20from%20multiple%20distinct%20fixation%20points%20within%20an%20input%2C%20we%0Ashow%20that%20these%20active%20methods%20achieve%20%282-3%29%20times%20greater%20robustness%20compared%0Ato%20a%20standard%20passive%20convolutional%20network%20under%20state-of-the-art%20adversarial%0Aattacks.%20More%20importantly%2C%20we%20provide%20illustrative%20and%20interpretable%0Avisualization%20analysis%20that%20demonstrates%20how%20performing%20inference%20from%20distinct%0Afixation%20points%20makes%20active%20vision%20methods%20less%20vulnerable%20to%20malicious%0Ainputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00185v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Inherent%20Adversarial%20Robustness%20of%20Active%20Vision%20Systems&entry.906535625=Amitangshu%20Mukherjee%20and%20Timur%20Ibrayev%20and%20Kaushik%20Roy&entry.1292438233=%20%20Current%20Deep%20Neural%20Networks%20are%20vulnerable%20to%20adversarial%20examples%2C%20which%0Aalter%20their%20predictions%20by%20adding%20carefully%20crafted%20noise.%20Since%20human%20eyes%20are%0Arobust%20to%20such%20inputs%2C%20it%20is%20possible%20that%20the%20vulnerability%20stems%20from%20the%0Astandard%20way%20of%20processing%20inputs%20in%20one%20shot%20by%20processing%20every%20pixel%20with%0Athe%20same%20importance.%20In%20contrast%2C%20neuroscience%20suggests%20that%20the%20human%20vision%0Asystem%20can%20differentiate%20salient%20features%20by%20%281%29%20switching%20between%20multiple%0Afixation%20points%20%28saccades%29%20and%20%282%29%20processing%20the%20surrounding%20with%20a%0Anon-uniform%20external%20resolution%20%28foveation%29.%20In%20this%20work%2C%20we%20advocate%20that%20the%0Aintegration%20of%20such%20active%20vision%20mechanisms%20into%20current%20deep%20learning%20systems%0Acan%20offer%20robustness%20benefits.%20Specifically%2C%20we%20empirically%20demonstrate%20the%0Ainherent%20robustness%20of%20two%20active%20vision%20methods%20-%20GFNet%20and%20FALcon%20-%20under%20a%0Ablack%20box%20threat%20model.%20By%20learning%20and%20inferencing%20based%20on%20downsampled%0Aglimpses%20obtained%20from%20multiple%20distinct%20fixation%20points%20within%20an%20input%2C%20we%0Ashow%20that%20these%20active%20methods%20achieve%20%282-3%29%20times%20greater%20robustness%20compared%0Ato%20a%20standard%20passive%20convolutional%20network%20under%20state-of-the-art%20adversarial%0Aattacks.%20More%20importantly%2C%20we%20provide%20illustrative%20and%20interpretable%0Avisualization%20analysis%20that%20demonstrates%20how%20performing%20inference%20from%20distinct%0Afixation%20points%20makes%20active%20vision%20methods%20less%20vulnerable%20to%20malicious%0Ainputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00185v2&entry.124074799=Read"},
{"title": "Learning by Self-Explaining", "author": "Wolfgang Stammer and Felix Friedrich and David Steinmann and Manuel Brack and Hikaru Shindo and Kristian Kersting", "abstract": "  Current AI research mainly treats explanations as a means for model\ninspection. Yet, this neglects findings from human psychology that describe the\nbenefit of self-explanations in an agent's learning process. Motivated by this,\nwe introduce a novel approach in the context of image classification, termed\nLearning by Self-Explaining (LSX). LSX utilizes aspects of self-refining AI and\nhuman-guided explanatory machine learning. The underlying idea is that a\nlearner model, in addition to optimizing for the original predictive task, is\nfurther optimized based on explanatory feedback from an internal critic model.\nIntuitively, a learner's explanations are considered \"useful\" if the internal\ncritic can perform the same task given these explanations. We provide an\noverview of important components of LSX and, based on this, perform extensive\nexperimental evaluations via three different example instantiations. Our\nresults indicate improvements via Learning by Self-Explaining on several\nlevels: in terms of model generalization, reducing the influence of confounding\nfactors, and providing more task-relevant and faithful model explanations.\nOverall, our work provides evidence for the potential of self-explaining within\nthe learning phase of an AI model.\n", "link": "http://arxiv.org/abs/2309.08395v2", "date": "2024-04-05", "relevancy": 2.1369, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5585}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5469}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5049}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20by%20Self-Explaining&body=Title%3A%20Learning%20by%20Self-Explaining%0AAuthor%3A%20Wolfgang%20Stammer%20and%20Felix%20Friedrich%20and%20David%20Steinmann%20and%20Manuel%20Brack%20and%20Hikaru%20Shindo%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Current%20AI%20research%20mainly%20treats%20explanations%20as%20a%20means%20for%20model%0Ainspection.%20Yet%2C%20this%20neglects%20findings%20from%20human%20psychology%20that%20describe%20the%0Abenefit%20of%20self-explanations%20in%20an%20agent%27s%20learning%20process.%20Motivated%20by%20this%2C%0Awe%20introduce%20a%20novel%20approach%20in%20the%20context%20of%20image%20classification%2C%20termed%0ALearning%20by%20Self-Explaining%20%28LSX%29.%20LSX%20utilizes%20aspects%20of%20self-refining%20AI%20and%0Ahuman-guided%20explanatory%20machine%20learning.%20The%20underlying%20idea%20is%20that%20a%0Alearner%20model%2C%20in%20addition%20to%20optimizing%20for%20the%20original%20predictive%20task%2C%20is%0Afurther%20optimized%20based%20on%20explanatory%20feedback%20from%20an%20internal%20critic%20model.%0AIntuitively%2C%20a%20learner%27s%20explanations%20are%20considered%20%22useful%22%20if%20the%20internal%0Acritic%20can%20perform%20the%20same%20task%20given%20these%20explanations.%20We%20provide%20an%0Aoverview%20of%20important%20components%20of%20LSX%20and%2C%20based%20on%20this%2C%20perform%20extensive%0Aexperimental%20evaluations%20via%20three%20different%20example%20instantiations.%20Our%0Aresults%20indicate%20improvements%20via%20Learning%20by%20Self-Explaining%20on%20several%0Alevels%3A%20in%20terms%20of%20model%20generalization%2C%20reducing%20the%20influence%20of%20confounding%0Afactors%2C%20and%20providing%20more%20task-relevant%20and%20faithful%20model%20explanations.%0AOverall%2C%20our%20work%20provides%20evidence%20for%20the%20potential%20of%20self-explaining%20within%0Athe%20learning%20phase%20of%20an%20AI%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08395v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20by%20Self-Explaining&entry.906535625=Wolfgang%20Stammer%20and%20Felix%20Friedrich%20and%20David%20Steinmann%20and%20Manuel%20Brack%20and%20Hikaru%20Shindo%20and%20Kristian%20Kersting&entry.1292438233=%20%20Current%20AI%20research%20mainly%20treats%20explanations%20as%20a%20means%20for%20model%0Ainspection.%20Yet%2C%20this%20neglects%20findings%20from%20human%20psychology%20that%20describe%20the%0Abenefit%20of%20self-explanations%20in%20an%20agent%27s%20learning%20process.%20Motivated%20by%20this%2C%0Awe%20introduce%20a%20novel%20approach%20in%20the%20context%20of%20image%20classification%2C%20termed%0ALearning%20by%20Self-Explaining%20%28LSX%29.%20LSX%20utilizes%20aspects%20of%20self-refining%20AI%20and%0Ahuman-guided%20explanatory%20machine%20learning.%20The%20underlying%20idea%20is%20that%20a%0Alearner%20model%2C%20in%20addition%20to%20optimizing%20for%20the%20original%20predictive%20task%2C%20is%0Afurther%20optimized%20based%20on%20explanatory%20feedback%20from%20an%20internal%20critic%20model.%0AIntuitively%2C%20a%20learner%27s%20explanations%20are%20considered%20%22useful%22%20if%20the%20internal%0Acritic%20can%20perform%20the%20same%20task%20given%20these%20explanations.%20We%20provide%20an%0Aoverview%20of%20important%20components%20of%20LSX%20and%2C%20based%20on%20this%2C%20perform%20extensive%0Aexperimental%20evaluations%20via%20three%20different%20example%20instantiations.%20Our%0Aresults%20indicate%20improvements%20via%20Learning%20by%20Self-Explaining%20on%20several%0Alevels%3A%20in%20terms%20of%20model%20generalization%2C%20reducing%20the%20influence%20of%20confounding%0Afactors%2C%20and%20providing%20more%20task-relevant%20and%20faithful%20model%20explanations.%0AOverall%2C%20our%20work%20provides%20evidence%20for%20the%20potential%20of%20self-explaining%20within%0Athe%20learning%20phase%20of%20an%20AI%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08395v2&entry.124074799=Read"},
{"title": "Improving Detection in Aerial Images by Capturing Inter-Object\n  Relationships", "author": "Botao Ren and Botian Xu and Yifan Pu and Jingyi Wang and Zhidong Deng", "abstract": "  In many image domains, the spatial distribution of objects in a scene\nexhibits meaningful patterns governed by their semantic relationships. In most\nmodern detection pipelines, however, the detection proposals are processed\nindependently, overlooking the underlying relationships between objects. In\nthis work, we introduce a transformer-based approach to capture these\ninter-object relationships to refine classification and regression outcomes for\ndetected objects. Building on two-stage detectors, we tokenize the region of\ninterest (RoI) proposals to be processed by a transformer encoder. Specific\nspatial and geometric relations are incorporated into the attention weights and\nadaptively modulated and regularized. Experimental results demonstrate that the\nproposed method achieves consistent performance improvement on three benchmarks\nincluding DOTA-v1.0, DOTA-v1.5, and HRSC 2016, especially ranking first on both\nDOTA-v1.5 and HRSC 2016. Specifically, our new method has an increase of 1.59\nmAP on DOTA-v1.0, 4.88 mAP on DOTA-v1.5, and 2.1 mAP on HRSC 2016,\nrespectively, compared to the baselines.\n", "link": "http://arxiv.org/abs/2404.04140v1", "date": "2024-04-05", "relevancy": 2.1317, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5232}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5222}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Detection%20in%20Aerial%20Images%20by%20Capturing%20Inter-Object%0A%20%20Relationships&body=Title%3A%20Improving%20Detection%20in%20Aerial%20Images%20by%20Capturing%20Inter-Object%0A%20%20Relationships%0AAuthor%3A%20Botao%20Ren%20and%20Botian%20Xu%20and%20Yifan%20Pu%20and%20Jingyi%20Wang%20and%20Zhidong%20Deng%0AAbstract%3A%20%20%20In%20many%20image%20domains%2C%20the%20spatial%20distribution%20of%20objects%20in%20a%20scene%0Aexhibits%20meaningful%20patterns%20governed%20by%20their%20semantic%20relationships.%20In%20most%0Amodern%20detection%20pipelines%2C%20however%2C%20the%20detection%20proposals%20are%20processed%0Aindependently%2C%20overlooking%20the%20underlying%20relationships%20between%20objects.%20In%0Athis%20work%2C%20we%20introduce%20a%20transformer-based%20approach%20to%20capture%20these%0Ainter-object%20relationships%20to%20refine%20classification%20and%20regression%20outcomes%20for%0Adetected%20objects.%20Building%20on%20two-stage%20detectors%2C%20we%20tokenize%20the%20region%20of%0Ainterest%20%28RoI%29%20proposals%20to%20be%20processed%20by%20a%20transformer%20encoder.%20Specific%0Aspatial%20and%20geometric%20relations%20are%20incorporated%20into%20the%20attention%20weights%20and%0Aadaptively%20modulated%20and%20regularized.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20achieves%20consistent%20performance%20improvement%20on%20three%20benchmarks%0Aincluding%20DOTA-v1.0%2C%20DOTA-v1.5%2C%20and%20HRSC%202016%2C%20especially%20ranking%20first%20on%20both%0ADOTA-v1.5%20and%20HRSC%202016.%20Specifically%2C%20our%20new%20method%20has%20an%20increase%20of%201.59%0AmAP%20on%20DOTA-v1.0%2C%204.88%20mAP%20on%20DOTA-v1.5%2C%20and%202.1%20mAP%20on%20HRSC%202016%2C%0Arespectively%2C%20compared%20to%20the%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04140v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Detection%20in%20Aerial%20Images%20by%20Capturing%20Inter-Object%0A%20%20Relationships&entry.906535625=Botao%20Ren%20and%20Botian%20Xu%20and%20Yifan%20Pu%20and%20Jingyi%20Wang%20and%20Zhidong%20Deng&entry.1292438233=%20%20In%20many%20image%20domains%2C%20the%20spatial%20distribution%20of%20objects%20in%20a%20scene%0Aexhibits%20meaningful%20patterns%20governed%20by%20their%20semantic%20relationships.%20In%20most%0Amodern%20detection%20pipelines%2C%20however%2C%20the%20detection%20proposals%20are%20processed%0Aindependently%2C%20overlooking%20the%20underlying%20relationships%20between%20objects.%20In%0Athis%20work%2C%20we%20introduce%20a%20transformer-based%20approach%20to%20capture%20these%0Ainter-object%20relationships%20to%20refine%20classification%20and%20regression%20outcomes%20for%0Adetected%20objects.%20Building%20on%20two-stage%20detectors%2C%20we%20tokenize%20the%20region%20of%0Ainterest%20%28RoI%29%20proposals%20to%20be%20processed%20by%20a%20transformer%20encoder.%20Specific%0Aspatial%20and%20geometric%20relations%20are%20incorporated%20into%20the%20attention%20weights%20and%0Aadaptively%20modulated%20and%20regularized.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20achieves%20consistent%20performance%20improvement%20on%20three%20benchmarks%0Aincluding%20DOTA-v1.0%2C%20DOTA-v1.5%2C%20and%20HRSC%202016%2C%20especially%20ranking%20first%20on%20both%0ADOTA-v1.5%20and%20HRSC%202016.%20Specifically%2C%20our%20new%20method%20has%20an%20increase%20of%201.59%0AmAP%20on%20DOTA-v1.0%2C%204.88%20mAP%20on%20DOTA-v1.5%2C%20and%202.1%20mAP%20on%20HRSC%202016%2C%0Arespectively%2C%20compared%20to%20the%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04140v1&entry.124074799=Read"},
{"title": "DiffOp-net: A Differential Operator-based Fully Convolutional Network\n  for Unsupervised Deformable Image Registration", "author": "Jiong Wu", "abstract": "  Existing unsupervised deformable image registration methods usually rely on\nmetrics applied to the gradients of predicted displacement or velocity fields\nas a regularization term to ensure transformation smoothness, which potentially\nlimits registration accuracy. In this study, we propose a novel approach to\nenhance unsupervised deformable image registration by introducing a new\ndifferential operator into the registration framework. This operator, acting on\nthe velocity field and mapping it to a dual space, ensures the smoothness of\nthe velocity field during optimization, facilitating accurate deformable\nregistration. In addition, to tackle the challenge of capturing large\ndeformations inside image pairs, we introduce a Cross-Coordinate Attention\nmodule (CCA) and embed it into a proposed Fully Convolutional Networks\n(FCNs)-based multi-resolution registration architecture. Evaluation experiments\nare conducted on two magnetic resonance imaging (MRI) datasets. Compared to\nvarious state-of-the-art registration approaches, including a traditional\nalgorithm and three representative unsupervised learning-based methods, our\nmethod achieves superior accuracies, maintaining desirable diffeomorphic\nproperties, and exhibiting promising registration speed.\n", "link": "http://arxiv.org/abs/2404.04244v1", "date": "2024-04-05", "relevancy": 2.1304, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5561}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5191}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5144}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiffOp-net%3A%20A%20Differential%20Operator-based%20Fully%20Convolutional%20Network%0A%20%20for%20Unsupervised%20Deformable%20Image%20Registration&body=Title%3A%20DiffOp-net%3A%20A%20Differential%20Operator-based%20Fully%20Convolutional%20Network%0A%20%20for%20Unsupervised%20Deformable%20Image%20Registration%0AAuthor%3A%20Jiong%20Wu%0AAbstract%3A%20%20%20Existing%20unsupervised%20deformable%20image%20registration%20methods%20usually%20rely%20on%0Ametrics%20applied%20to%20the%20gradients%20of%20predicted%20displacement%20or%20velocity%20fields%0Aas%20a%20regularization%20term%20to%20ensure%20transformation%20smoothness%2C%20which%20potentially%0Alimits%20registration%20accuracy.%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20to%0Aenhance%20unsupervised%20deformable%20image%20registration%20by%20introducing%20a%20new%0Adifferential%20operator%20into%20the%20registration%20framework.%20This%20operator%2C%20acting%20on%0Athe%20velocity%20field%20and%20mapping%20it%20to%20a%20dual%20space%2C%20ensures%20the%20smoothness%20of%0Athe%20velocity%20field%20during%20optimization%2C%20facilitating%20accurate%20deformable%0Aregistration.%20In%20addition%2C%20to%20tackle%20the%20challenge%20of%20capturing%20large%0Adeformations%20inside%20image%20pairs%2C%20we%20introduce%20a%20Cross-Coordinate%20Attention%0Amodule%20%28CCA%29%20and%20embed%20it%20into%20a%20proposed%20Fully%20Convolutional%20Networks%0A%28FCNs%29-based%20multi-resolution%20registration%20architecture.%20Evaluation%20experiments%0Aare%20conducted%20on%20two%20magnetic%20resonance%20imaging%20%28MRI%29%20datasets.%20Compared%20to%0Avarious%20state-of-the-art%20registration%20approaches%2C%20including%20a%20traditional%0Aalgorithm%20and%20three%20representative%20unsupervised%20learning-based%20methods%2C%20our%0Amethod%20achieves%20superior%20accuracies%2C%20maintaining%20desirable%20diffeomorphic%0Aproperties%2C%20and%20exhibiting%20promising%20registration%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04244v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffOp-net%3A%20A%20Differential%20Operator-based%20Fully%20Convolutional%20Network%0A%20%20for%20Unsupervised%20Deformable%20Image%20Registration&entry.906535625=Jiong%20Wu&entry.1292438233=%20%20Existing%20unsupervised%20deformable%20image%20registration%20methods%20usually%20rely%20on%0Ametrics%20applied%20to%20the%20gradients%20of%20predicted%20displacement%20or%20velocity%20fields%0Aas%20a%20regularization%20term%20to%20ensure%20transformation%20smoothness%2C%20which%20potentially%0Alimits%20registration%20accuracy.%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20to%0Aenhance%20unsupervised%20deformable%20image%20registration%20by%20introducing%20a%20new%0Adifferential%20operator%20into%20the%20registration%20framework.%20This%20operator%2C%20acting%20on%0Athe%20velocity%20field%20and%20mapping%20it%20to%20a%20dual%20space%2C%20ensures%20the%20smoothness%20of%0Athe%20velocity%20field%20during%20optimization%2C%20facilitating%20accurate%20deformable%0Aregistration.%20In%20addition%2C%20to%20tackle%20the%20challenge%20of%20capturing%20large%0Adeformations%20inside%20image%20pairs%2C%20we%20introduce%20a%20Cross-Coordinate%20Attention%0Amodule%20%28CCA%29%20and%20embed%20it%20into%20a%20proposed%20Fully%20Convolutional%20Networks%0A%28FCNs%29-based%20multi-resolution%20registration%20architecture.%20Evaluation%20experiments%0Aare%20conducted%20on%20two%20magnetic%20resonance%20imaging%20%28MRI%29%20datasets.%20Compared%20to%0Avarious%20state-of-the-art%20registration%20approaches%2C%20including%20a%20traditional%0Aalgorithm%20and%20three%20representative%20unsupervised%20learning-based%20methods%2C%20our%0Amethod%20achieves%20superior%20accuracies%2C%20maintaining%20desirable%20diffeomorphic%0Aproperties%2C%20and%20exhibiting%20promising%20registration%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04244v1&entry.124074799=Read"},
{"title": "SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians", "author": "Hiba Dahmani and Moussab Bennehar and Nathan Piasco and Luis Roldao and Dzmitry Tsishkou", "abstract": "  Implicit neural representation methods have shown impressive advancements in\nlearning 3D scenes from unstructured in-the-wild photo collections but are\nstill limited by the large computational cost of volumetric rendering. More\nrecently, 3D Gaussian Splatting emerged as a much faster alternative with\nsuperior rendering quality and training efficiency, especially for small-scale\nand object-centric scenarios. Nevertheless, this technique suffers from poor\nperformance on unstructured in-the-wild data. To tackle this, we extend over 3D\nGaussian Splatting to handle unstructured image collections. We achieve this by\nmodeling appearance to seize photometric variations in the rendered images.\nAdditionally, we introduce a new mechanism to train transient Gaussians to\nhandle the presence of scene occluders in an unsupervised manner. Experiments\non diverse photo collection scenes and multi-pass acquisition of outdoor\nlandmarks show the effectiveness of our method over prior works achieving\nstate-of-the-art results with improved efficiency.\n", "link": "http://arxiv.org/abs/2403.10427v2", "date": "2024-04-05", "relevancy": 2.1226, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5618}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5273}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5216}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SWAG%3A%20Splatting%20in%20the%20Wild%20images%20with%20Appearance-conditioned%20Gaussians&body=Title%3A%20SWAG%3A%20Splatting%20in%20the%20Wild%20images%20with%20Appearance-conditioned%20Gaussians%0AAuthor%3A%20Hiba%20Dahmani%20and%20Moussab%20Bennehar%20and%20Nathan%20Piasco%20and%20Luis%20Roldao%20and%20Dzmitry%20Tsishkou%0AAbstract%3A%20%20%20Implicit%20neural%20representation%20methods%20have%20shown%20impressive%20advancements%20in%0Alearning%203D%20scenes%20from%20unstructured%20in-the-wild%20photo%20collections%20but%20are%0Astill%20limited%20by%20the%20large%20computational%20cost%20of%20volumetric%20rendering.%20More%0Arecently%2C%203D%20Gaussian%20Splatting%20emerged%20as%20a%20much%20faster%20alternative%20with%0Asuperior%20rendering%20quality%20and%20training%20efficiency%2C%20especially%20for%20small-scale%0Aand%20object-centric%20scenarios.%20Nevertheless%2C%20this%20technique%20suffers%20from%20poor%0Aperformance%20on%20unstructured%20in-the-wild%20data.%20To%20tackle%20this%2C%20we%20extend%20over%203D%0AGaussian%20Splatting%20to%20handle%20unstructured%20image%20collections.%20We%20achieve%20this%20by%0Amodeling%20appearance%20to%20seize%20photometric%20variations%20in%20the%20rendered%20images.%0AAdditionally%2C%20we%20introduce%20a%20new%20mechanism%20to%20train%20transient%20Gaussians%20to%0Ahandle%20the%20presence%20of%20scene%20occluders%20in%20an%20unsupervised%20manner.%20Experiments%0Aon%20diverse%20photo%20collection%20scenes%20and%20multi-pass%20acquisition%20of%20outdoor%0Alandmarks%20show%20the%20effectiveness%20of%20our%20method%20over%20prior%20works%20achieving%0Astate-of-the-art%20results%20with%20improved%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10427v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWAG%3A%20Splatting%20in%20the%20Wild%20images%20with%20Appearance-conditioned%20Gaussians&entry.906535625=Hiba%20Dahmani%20and%20Moussab%20Bennehar%20and%20Nathan%20Piasco%20and%20Luis%20Roldao%20and%20Dzmitry%20Tsishkou&entry.1292438233=%20%20Implicit%20neural%20representation%20methods%20have%20shown%20impressive%20advancements%20in%0Alearning%203D%20scenes%20from%20unstructured%20in-the-wild%20photo%20collections%20but%20are%0Astill%20limited%20by%20the%20large%20computational%20cost%20of%20volumetric%20rendering.%20More%0Arecently%2C%203D%20Gaussian%20Splatting%20emerged%20as%20a%20much%20faster%20alternative%20with%0Asuperior%20rendering%20quality%20and%20training%20efficiency%2C%20especially%20for%20small-scale%0Aand%20object-centric%20scenarios.%20Nevertheless%2C%20this%20technique%20suffers%20from%20poor%0Aperformance%20on%20unstructured%20in-the-wild%20data.%20To%20tackle%20this%2C%20we%20extend%20over%203D%0AGaussian%20Splatting%20to%20handle%20unstructured%20image%20collections.%20We%20achieve%20this%20by%0Amodeling%20appearance%20to%20seize%20photometric%20variations%20in%20the%20rendered%20images.%0AAdditionally%2C%20we%20introduce%20a%20new%20mechanism%20to%20train%20transient%20Gaussians%20to%0Ahandle%20the%20presence%20of%20scene%20occluders%20in%20an%20unsupervised%20manner.%20Experiments%0Aon%20diverse%20photo%20collection%20scenes%20and%20multi-pass%20acquisition%20of%20outdoor%0Alandmarks%20show%20the%20effectiveness%20of%20our%20method%20over%20prior%20works%20achieving%0Astate-of-the-art%20results%20with%20improved%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10427v2&entry.124074799=Read"},
{"title": "Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via\n  Mixture Density Networks", "author": "Jordan Thompson and Brian Y. Cho and Daniel S. Brown and Alan Kuntz", "abstract": "  Tendon-driven continuum robot kinematic models are frequently computationally\nexpensive, inaccurate due to unmodeled effects, or both. In particular,\nunmodeled effects produce uncertainties that arise during the robot's operation\nthat lead to variability in the resulting geometry. We propose a novel solution\nto these issues through the development of a Gaussian mixture kinematic model.\nWe train a mixture density network to output a Gaussian mixture model\nrepresentation of the robot geometry given the current tendon displacements.\nThis model computes a probability distribution that is more representative of\nthe true distribution of geometries at a given configuration than a model that\noutputs a single geometry, while also reducing the computation time. We\ndemonstrate one use of this model through a trajectory optimization method that\nexplicitly reasons about the workspace uncertainty to minimize the probability\nof collision.\n", "link": "http://arxiv.org/abs/2404.04241v1", "date": "2024-04-05", "relevancy": 2.1104, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5898}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.516}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5143}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Modeling%20Kinematic%20Uncertainty%20of%20Tendon-Driven%20Continuum%20Robots%20via%0A%20%20Mixture%20Density%20Networks&body=Title%3A%20Modeling%20Kinematic%20Uncertainty%20of%20Tendon-Driven%20Continuum%20Robots%20via%0A%20%20Mixture%20Density%20Networks%0AAuthor%3A%20Jordan%20Thompson%20and%20Brian%20Y.%20Cho%20and%20Daniel%20S.%20Brown%20and%20Alan%20Kuntz%0AAbstract%3A%20%20%20Tendon-driven%20continuum%20robot%20kinematic%20models%20are%20frequently%20computationally%0Aexpensive%2C%20inaccurate%20due%20to%20unmodeled%20effects%2C%20or%20both.%20In%20particular%2C%0Aunmodeled%20effects%20produce%20uncertainties%20that%20arise%20during%20the%20robot%27s%20operation%0Athat%20lead%20to%20variability%20in%20the%20resulting%20geometry.%20We%20propose%20a%20novel%20solution%0Ato%20these%20issues%20through%20the%20development%20of%20a%20Gaussian%20mixture%20kinematic%20model.%0AWe%20train%20a%20mixture%20density%20network%20to%20output%20a%20Gaussian%20mixture%20model%0Arepresentation%20of%20the%20robot%20geometry%20given%20the%20current%20tendon%20displacements.%0AThis%20model%20computes%20a%20probability%20distribution%20that%20is%20more%20representative%20of%0Athe%20true%20distribution%20of%20geometries%20at%20a%20given%20configuration%20than%20a%20model%20that%0Aoutputs%20a%20single%20geometry%2C%20while%20also%20reducing%20the%20computation%20time.%20We%0Ademonstrate%20one%20use%20of%20this%20model%20through%20a%20trajectory%20optimization%20method%20that%0Aexplicitly%20reasons%20about%20the%20workspace%20uncertainty%20to%20minimize%20the%20probability%0Aof%20collision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04241v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Kinematic%20Uncertainty%20of%20Tendon-Driven%20Continuum%20Robots%20via%0A%20%20Mixture%20Density%20Networks&entry.906535625=Jordan%20Thompson%20and%20Brian%20Y.%20Cho%20and%20Daniel%20S.%20Brown%20and%20Alan%20Kuntz&entry.1292438233=%20%20Tendon-driven%20continuum%20robot%20kinematic%20models%20are%20frequently%20computationally%0Aexpensive%2C%20inaccurate%20due%20to%20unmodeled%20effects%2C%20or%20both.%20In%20particular%2C%0Aunmodeled%20effects%20produce%20uncertainties%20that%20arise%20during%20the%20robot%27s%20operation%0Athat%20lead%20to%20variability%20in%20the%20resulting%20geometry.%20We%20propose%20a%20novel%20solution%0Ato%20these%20issues%20through%20the%20development%20of%20a%20Gaussian%20mixture%20kinematic%20model.%0AWe%20train%20a%20mixture%20density%20network%20to%20output%20a%20Gaussian%20mixture%20model%0Arepresentation%20of%20the%20robot%20geometry%20given%20the%20current%20tendon%20displacements.%0AThis%20model%20computes%20a%20probability%20distribution%20that%20is%20more%20representative%20of%0Athe%20true%20distribution%20of%20geometries%20at%20a%20given%20configuration%20than%20a%20model%20that%0Aoutputs%20a%20single%20geometry%2C%20while%20also%20reducing%20the%20computation%20time.%20We%0Ademonstrate%20one%20use%20of%20this%20model%20through%20a%20trajectory%20optimization%20method%20that%0Aexplicitly%20reasons%20about%20the%20workspace%20uncertainty%20to%20minimize%20the%20probability%0Aof%20collision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04241v1&entry.124074799=Read"},
{"title": "3D Facial Expressions through Analysis-by-Neural-Synthesis", "author": "George Retsinas and Panagiotis P. Filntisis and Radek Danecek and Victoria F. Abrevaya and Anastasios Roussos and Timo Bolkart and Petros Maragos", "abstract": "  While existing methods for 3D face reconstruction from in-the-wild images\nexcel at recovering the overall face shape, they commonly miss subtle, extreme,\nasymmetric, or rarely observed expressions. We improve upon these methods with\nSMIRK (Spatial Modeling for Image-based Reconstruction of Kinesics), which\nfaithfully reconstructs expressive 3D faces from images. We identify two key\nlimitations in existing methods: shortcomings in their self-supervised training\nformulation, and a lack of expression diversity in the training images. For\ntraining, most methods employ differentiable rendering to compare a predicted\nface mesh with the input image, along with a plethora of additional loss\nfunctions. This differentiable rendering loss not only has to provide\nsupervision to optimize for 3D face geometry, camera, albedo, and lighting,\nwhich is an ill-posed optimization problem, but the domain gap between\nrendering and input image further hinders the learning process. Instead, SMIRK\nreplaces the differentiable rendering with a neural rendering module that,\ngiven the rendered predicted mesh geometry, and sparsely sampled pixels of the\ninput image, generates a face image. As the neural rendering gets color\ninformation from sampled image pixels, supervising with neural rendering-based\nreconstruction loss can focus solely on the geometry. Further, it enables us to\ngenerate images of the input identity with varying expressions while training.\nThese are then utilized as input to the reconstruction model and used as\nsupervision with ground truth geometry. This effectively augments the training\ndata and enhances the generalization for diverse expressions. Our qualitative,\nquantitative and particularly our perceptual evaluations demonstrate that SMIRK\nachieves the new state-of-the art performance on accurate expression\nreconstruction. Project webpage: https://georgeretsi.github.io/smirk/.\n", "link": "http://arxiv.org/abs/2404.04104v1", "date": "2024-04-05", "relevancy": 2.11, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5269}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5182}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Facial%20Expressions%20through%20Analysis-by-Neural-Synthesis&body=Title%3A%203D%20Facial%20Expressions%20through%20Analysis-by-Neural-Synthesis%0AAuthor%3A%20George%20Retsinas%20and%20Panagiotis%20P.%20Filntisis%20and%20Radek%20Danecek%20and%20Victoria%20F.%20Abrevaya%20and%20Anastasios%20Roussos%20and%20Timo%20Bolkart%20and%20Petros%20Maragos%0AAbstract%3A%20%20%20While%20existing%20methods%20for%203D%20face%20reconstruction%20from%20in-the-wild%20images%0Aexcel%20at%20recovering%20the%20overall%20face%20shape%2C%20they%20commonly%20miss%20subtle%2C%20extreme%2C%0Aasymmetric%2C%20or%20rarely%20observed%20expressions.%20We%20improve%20upon%20these%20methods%20with%0ASMIRK%20%28Spatial%20Modeling%20for%20Image-based%20Reconstruction%20of%20Kinesics%29%2C%20which%0Afaithfully%20reconstructs%20expressive%203D%20faces%20from%20images.%20We%20identify%20two%20key%0Alimitations%20in%20existing%20methods%3A%20shortcomings%20in%20their%20self-supervised%20training%0Aformulation%2C%20and%20a%20lack%20of%20expression%20diversity%20in%20the%20training%20images.%20For%0Atraining%2C%20most%20methods%20employ%20differentiable%20rendering%20to%20compare%20a%20predicted%0Aface%20mesh%20with%20the%20input%20image%2C%20along%20with%20a%20plethora%20of%20additional%20loss%0Afunctions.%20This%20differentiable%20rendering%20loss%20not%20only%20has%20to%20provide%0Asupervision%20to%20optimize%20for%203D%20face%20geometry%2C%20camera%2C%20albedo%2C%20and%20lighting%2C%0Awhich%20is%20an%20ill-posed%20optimization%20problem%2C%20but%20the%20domain%20gap%20between%0Arendering%20and%20input%20image%20further%20hinders%20the%20learning%20process.%20Instead%2C%20SMIRK%0Areplaces%20the%20differentiable%20rendering%20with%20a%20neural%20rendering%20module%20that%2C%0Agiven%20the%20rendered%20predicted%20mesh%20geometry%2C%20and%20sparsely%20sampled%20pixels%20of%20the%0Ainput%20image%2C%20generates%20a%20face%20image.%20As%20the%20neural%20rendering%20gets%20color%0Ainformation%20from%20sampled%20image%20pixels%2C%20supervising%20with%20neural%20rendering-based%0Areconstruction%20loss%20can%20focus%20solely%20on%20the%20geometry.%20Further%2C%20it%20enables%20us%20to%0Agenerate%20images%20of%20the%20input%20identity%20with%20varying%20expressions%20while%20training.%0AThese%20are%20then%20utilized%20as%20input%20to%20the%20reconstruction%20model%20and%20used%20as%0Asupervision%20with%20ground%20truth%20geometry.%20This%20effectively%20augments%20the%20training%0Adata%20and%20enhances%20the%20generalization%20for%20diverse%20expressions.%20Our%20qualitative%2C%0Aquantitative%20and%20particularly%20our%20perceptual%20evaluations%20demonstrate%20that%20SMIRK%0Aachieves%20the%20new%20state-of-the%20art%20performance%20on%20accurate%20expression%0Areconstruction.%20Project%20webpage%3A%20https%3A//georgeretsi.github.io/smirk/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04104v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Facial%20Expressions%20through%20Analysis-by-Neural-Synthesis&entry.906535625=George%20Retsinas%20and%20Panagiotis%20P.%20Filntisis%20and%20Radek%20Danecek%20and%20Victoria%20F.%20Abrevaya%20and%20Anastasios%20Roussos%20and%20Timo%20Bolkart%20and%20Petros%20Maragos&entry.1292438233=%20%20While%20existing%20methods%20for%203D%20face%20reconstruction%20from%20in-the-wild%20images%0Aexcel%20at%20recovering%20the%20overall%20face%20shape%2C%20they%20commonly%20miss%20subtle%2C%20extreme%2C%0Aasymmetric%2C%20or%20rarely%20observed%20expressions.%20We%20improve%20upon%20these%20methods%20with%0ASMIRK%20%28Spatial%20Modeling%20for%20Image-based%20Reconstruction%20of%20Kinesics%29%2C%20which%0Afaithfully%20reconstructs%20expressive%203D%20faces%20from%20images.%20We%20identify%20two%20key%0Alimitations%20in%20existing%20methods%3A%20shortcomings%20in%20their%20self-supervised%20training%0Aformulation%2C%20and%20a%20lack%20of%20expression%20diversity%20in%20the%20training%20images.%20For%0Atraining%2C%20most%20methods%20employ%20differentiable%20rendering%20to%20compare%20a%20predicted%0Aface%20mesh%20with%20the%20input%20image%2C%20along%20with%20a%20plethora%20of%20additional%20loss%0Afunctions.%20This%20differentiable%20rendering%20loss%20not%20only%20has%20to%20provide%0Asupervision%20to%20optimize%20for%203D%20face%20geometry%2C%20camera%2C%20albedo%2C%20and%20lighting%2C%0Awhich%20is%20an%20ill-posed%20optimization%20problem%2C%20but%20the%20domain%20gap%20between%0Arendering%20and%20input%20image%20further%20hinders%20the%20learning%20process.%20Instead%2C%20SMIRK%0Areplaces%20the%20differentiable%20rendering%20with%20a%20neural%20rendering%20module%20that%2C%0Agiven%20the%20rendered%20predicted%20mesh%20geometry%2C%20and%20sparsely%20sampled%20pixels%20of%20the%0Ainput%20image%2C%20generates%20a%20face%20image.%20As%20the%20neural%20rendering%20gets%20color%0Ainformation%20from%20sampled%20image%20pixels%2C%20supervising%20with%20neural%20rendering-based%0Areconstruction%20loss%20can%20focus%20solely%20on%20the%20geometry.%20Further%2C%20it%20enables%20us%20to%0Agenerate%20images%20of%20the%20input%20identity%20with%20varying%20expressions%20while%20training.%0AThese%20are%20then%20utilized%20as%20input%20to%20the%20reconstruction%20model%20and%20used%20as%0Asupervision%20with%20ground%20truth%20geometry.%20This%20effectively%20augments%20the%20training%0Adata%20and%20enhances%20the%20generalization%20for%20diverse%20expressions.%20Our%20qualitative%2C%0Aquantitative%20and%20particularly%20our%20perceptual%20evaluations%20demonstrate%20that%20SMIRK%0Aachieves%20the%20new%20state-of-the%20art%20performance%20on%20accurate%20expression%0Areconstruction.%20Project%20webpage%3A%20https%3A//georgeretsi.github.io/smirk/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04104v1&entry.124074799=Read"},
{"title": "Label Propagation for Zero-shot Classification with Vision-Language\n  Models", "author": "Vladan Stojni\u0107 and Yannis Kalantidis and Giorgos Tolias", "abstract": "  Vision-Language Models (VLMs) have demonstrated impressive performance on\nzero-shot classification, i.e. classification when provided merely with a list\nof class names. In this paper, we tackle the case of zero-shot classification\nin the presence of unlabeled data. We leverage the graph structure of the\nunlabeled data and introduce ZLaP, a method based on label propagation (LP)\nthat utilizes geodesic distances for classification. We tailor LP to graphs\ncontaining both text and image features and further propose an efficient method\nfor performing inductive inference based on a dual solution and a\nsparsification step. We perform extensive experiments to evaluate the\neffectiveness of our method on 14 common datasets and show that ZLaP\noutperforms the latest related works. Code:\nhttps://github.com/vladan-stojnic/ZLaP\n", "link": "http://arxiv.org/abs/2404.04072v1", "date": "2024-04-05", "relevancy": 2.1052, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5284}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4767}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Label%20Propagation%20for%20Zero-shot%20Classification%20with%20Vision-Language%0A%20%20Models&body=Title%3A%20Label%20Propagation%20for%20Zero-shot%20Classification%20with%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Vladan%20Stojni%C4%87%20and%20Yannis%20Kalantidis%20and%20Giorgos%20Tolias%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%20performance%20on%0Azero-shot%20classification%2C%20i.e.%20classification%20when%20provided%20merely%20with%20a%20list%0Aof%20class%20names.%20In%20this%20paper%2C%20we%20tackle%20the%20case%20of%20zero-shot%20classification%0Ain%20the%20presence%20of%20unlabeled%20data.%20We%20leverage%20the%20graph%20structure%20of%20the%0Aunlabeled%20data%20and%20introduce%20ZLaP%2C%20a%20method%20based%20on%20label%20propagation%20%28LP%29%0Athat%20utilizes%20geodesic%20distances%20for%20classification.%20We%20tailor%20LP%20to%20graphs%0Acontaining%20both%20text%20and%20image%20features%20and%20further%20propose%20an%20efficient%20method%0Afor%20performing%20inductive%20inference%20based%20on%20a%20dual%20solution%20and%20a%0Asparsification%20step.%20We%20perform%20extensive%20experiments%20to%20evaluate%20the%0Aeffectiveness%20of%20our%20method%20on%2014%20common%20datasets%20and%20show%20that%20ZLaP%0Aoutperforms%20the%20latest%20related%20works.%20Code%3A%0Ahttps%3A//github.com/vladan-stojnic/ZLaP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04072v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Propagation%20for%20Zero-shot%20Classification%20with%20Vision-Language%0A%20%20Models&entry.906535625=Vladan%20Stojni%C4%87%20and%20Yannis%20Kalantidis%20and%20Giorgos%20Tolias&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%20performance%20on%0Azero-shot%20classification%2C%20i.e.%20classification%20when%20provided%20merely%20with%20a%20list%0Aof%20class%20names.%20In%20this%20paper%2C%20we%20tackle%20the%20case%20of%20zero-shot%20classification%0Ain%20the%20presence%20of%20unlabeled%20data.%20We%20leverage%20the%20graph%20structure%20of%20the%0Aunlabeled%20data%20and%20introduce%20ZLaP%2C%20a%20method%20based%20on%20label%20propagation%20%28LP%29%0Athat%20utilizes%20geodesic%20distances%20for%20classification.%20We%20tailor%20LP%20to%20graphs%0Acontaining%20both%20text%20and%20image%20features%20and%20further%20propose%20an%20efficient%20method%0Afor%20performing%20inductive%20inference%20based%20on%20a%20dual%20solution%20and%20a%0Asparsification%20step.%20We%20perform%20extensive%20experiments%20to%20evaluate%20the%0Aeffectiveness%20of%20our%20method%20on%2014%20common%20datasets%20and%20show%20that%20ZLaP%0Aoutperforms%20the%20latest%20related%20works.%20Code%3A%0Ahttps%3A//github.com/vladan-stojnic/ZLaP%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04072v1&entry.124074799=Read"},
{"title": "Finding AI-Generated Faces in the Wild", "author": "Gonzalo J. Aniano Porcile and Jack Gindi and Shivansh Mundra and James R. Verbus and Hany Farid", "abstract": "  AI-based image generation has continued to rapidly improve, producing\nincreasingly more realistic images with fewer obvious visual flaws.\nAI-generated images are being used to create fake online profiles which in turn\nare being used for spam, fraud, and disinformation campaigns. As the general\nproblem of detecting any type of manipulated or synthesized content is\nreceiving increasing attention, here we focus on a more narrow task of\ndistinguishing a real face from an AI-generated face. This is particularly\napplicable when tackling inauthentic online accounts with a fake user profile\nphoto. We show that by focusing on only faces, a more resilient and\ngeneral-purpose artifact can be detected that allows for the detection of\nAI-generated faces from a variety of GAN- and diffusion-based synthesis\nengines, and across image resolutions (as low as 128 x 128 pixels) and\nqualities.\n", "link": "http://arxiv.org/abs/2311.08577v3", "date": "2024-04-05", "relevancy": 2.1029, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5635}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5382}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4982}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Finding%20AI-Generated%20Faces%20in%20the%20Wild&body=Title%3A%20Finding%20AI-Generated%20Faces%20in%20the%20Wild%0AAuthor%3A%20Gonzalo%20J.%20Aniano%20Porcile%20and%20Jack%20Gindi%20and%20Shivansh%20Mundra%20and%20James%20R.%20Verbus%20and%20Hany%20Farid%0AAbstract%3A%20%20%20AI-based%20image%20generation%20has%20continued%20to%20rapidly%20improve%2C%20producing%0Aincreasingly%20more%20realistic%20images%20with%20fewer%20obvious%20visual%20flaws.%0AAI-generated%20images%20are%20being%20used%20to%20create%20fake%20online%20profiles%20which%20in%20turn%0Aare%20being%20used%20for%20spam%2C%20fraud%2C%20and%20disinformation%20campaigns.%20As%20the%20general%0Aproblem%20of%20detecting%20any%20type%20of%20manipulated%20or%20synthesized%20content%20is%0Areceiving%20increasing%20attention%2C%20here%20we%20focus%20on%20a%20more%20narrow%20task%20of%0Adistinguishing%20a%20real%20face%20from%20an%20AI-generated%20face.%20This%20is%20particularly%0Aapplicable%20when%20tackling%20inauthentic%20online%20accounts%20with%20a%20fake%20user%20profile%0Aphoto.%20We%20show%20that%20by%20focusing%20on%20only%20faces%2C%20a%20more%20resilient%20and%0Ageneral-purpose%20artifact%20can%20be%20detected%20that%20allows%20for%20the%20detection%20of%0AAI-generated%20faces%20from%20a%20variety%20of%20GAN-%20and%20diffusion-based%20synthesis%0Aengines%2C%20and%20across%20image%20resolutions%20%28as%20low%20as%20128%20x%20128%20pixels%29%20and%0Aqualities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08577v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20AI-Generated%20Faces%20in%20the%20Wild&entry.906535625=Gonzalo%20J.%20Aniano%20Porcile%20and%20Jack%20Gindi%20and%20Shivansh%20Mundra%20and%20James%20R.%20Verbus%20and%20Hany%20Farid&entry.1292438233=%20%20AI-based%20image%20generation%20has%20continued%20to%20rapidly%20improve%2C%20producing%0Aincreasingly%20more%20realistic%20images%20with%20fewer%20obvious%20visual%20flaws.%0AAI-generated%20images%20are%20being%20used%20to%20create%20fake%20online%20profiles%20which%20in%20turn%0Aare%20being%20used%20for%20spam%2C%20fraud%2C%20and%20disinformation%20campaigns.%20As%20the%20general%0Aproblem%20of%20detecting%20any%20type%20of%20manipulated%20or%20synthesized%20content%20is%0Areceiving%20increasing%20attention%2C%20here%20we%20focus%20on%20a%20more%20narrow%20task%20of%0Adistinguishing%20a%20real%20face%20from%20an%20AI-generated%20face.%20This%20is%20particularly%0Aapplicable%20when%20tackling%20inauthentic%20online%20accounts%20with%20a%20fake%20user%20profile%0Aphoto.%20We%20show%20that%20by%20focusing%20on%20only%20faces%2C%20a%20more%20resilient%20and%0Ageneral-purpose%20artifact%20can%20be%20detected%20that%20allows%20for%20the%20detection%20of%0AAI-generated%20faces%20from%20a%20variety%20of%20GAN-%20and%20diffusion-based%20synthesis%0Aengines%2C%20and%20across%20image%20resolutions%20%28as%20low%20as%20128%20x%20128%20pixels%29%20and%0Aqualities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08577v3&entry.124074799=Read"},
{"title": "MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and\n  Decoder", "author": "Liao Pan and Yang Feng and Wu Di and Liu Bo and Zhang Xingle", "abstract": "  In the field of multi-object tracking (MOT), recent Transformer based\nend-to-end models like MOTR have demonstrated exceptional performance on\ndatasets such as DanceTracker. However, the computational demands of these\nmodels present challenges in training and deployment. Drawing inspiration from\nsuccessful models like GPT, we present MO-YOLO, an efficient and\ncomputationally frugal end-to-end MOT model. MO-YOLO integrates principles from\nYou Only Look Once (YOLO) and RT-DETR, adopting a decoder-only approach. By\nleveraging the decoder from RT-DETR and architectural components from YOLOv8,\nMO-YOLO achieves high speed, shorter training times, and proficient MOT\nperformance. On the Dancetrack, MO-YOLO not only matches MOTR's performance but\nalso surpasses it, achieving over twice the frames per second (MOTR 9.5 FPS,\nMO-YOLO 19.6 FPS). Furthermore, MO-YOLO demonstrates significantly reduced\ntraining times and lower hardware requirements compared to MOTR. This research\nintroduces a promising paradigm for efficient end-to-end MOT, emphasizing\nenhanced performance and resource efficiency.\n", "link": "http://arxiv.org/abs/2310.17170v3", "date": "2024-04-05", "relevancy": 2.0915, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5269}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5241}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MO-YOLO%3A%20End-to-End%20Multiple-Object%20Tracking%20Method%20with%20YOLO%20and%0A%20%20Decoder&body=Title%3A%20MO-YOLO%3A%20End-to-End%20Multiple-Object%20Tracking%20Method%20with%20YOLO%20and%0A%20%20Decoder%0AAuthor%3A%20Liao%20Pan%20and%20Yang%20Feng%20and%20Wu%20Di%20and%20Liu%20Bo%20and%20Zhang%20Xingle%0AAbstract%3A%20%20%20In%20the%20field%20of%20multi-object%20tracking%20%28MOT%29%2C%20recent%20Transformer%20based%0Aend-to-end%20models%20like%20MOTR%20have%20demonstrated%20exceptional%20performance%20on%0Adatasets%20such%20as%20DanceTracker.%20However%2C%20the%20computational%20demands%20of%20these%0Amodels%20present%20challenges%20in%20training%20and%20deployment.%20Drawing%20inspiration%20from%0Asuccessful%20models%20like%20GPT%2C%20we%20present%20MO-YOLO%2C%20an%20efficient%20and%0Acomputationally%20frugal%20end-to-end%20MOT%20model.%20MO-YOLO%20integrates%20principles%20from%0AYou%20Only%20Look%20Once%20%28YOLO%29%20and%20RT-DETR%2C%20adopting%20a%20decoder-only%20approach.%20By%0Aleveraging%20the%20decoder%20from%20RT-DETR%20and%20architectural%20components%20from%20YOLOv8%2C%0AMO-YOLO%20achieves%20high%20speed%2C%20shorter%20training%20times%2C%20and%20proficient%20MOT%0Aperformance.%20On%20the%20Dancetrack%2C%20MO-YOLO%20not%20only%20matches%20MOTR%27s%20performance%20but%0Aalso%20surpasses%20it%2C%20achieving%20over%20twice%20the%20frames%20per%20second%20%28MOTR%209.5%20FPS%2C%0AMO-YOLO%2019.6%20FPS%29.%20Furthermore%2C%20MO-YOLO%20demonstrates%20significantly%20reduced%0Atraining%20times%20and%20lower%20hardware%20requirements%20compared%20to%20MOTR.%20This%20research%0Aintroduces%20a%20promising%20paradigm%20for%20efficient%20end-to-end%20MOT%2C%20emphasizing%0Aenhanced%20performance%20and%20resource%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17170v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MO-YOLO%3A%20End-to-End%20Multiple-Object%20Tracking%20Method%20with%20YOLO%20and%0A%20%20Decoder&entry.906535625=Liao%20Pan%20and%20Yang%20Feng%20and%20Wu%20Di%20and%20Liu%20Bo%20and%20Zhang%20Xingle&entry.1292438233=%20%20In%20the%20field%20of%20multi-object%20tracking%20%28MOT%29%2C%20recent%20Transformer%20based%0Aend-to-end%20models%20like%20MOTR%20have%20demonstrated%20exceptional%20performance%20on%0Adatasets%20such%20as%20DanceTracker.%20However%2C%20the%20computational%20demands%20of%20these%0Amodels%20present%20challenges%20in%20training%20and%20deployment.%20Drawing%20inspiration%20from%0Asuccessful%20models%20like%20GPT%2C%20we%20present%20MO-YOLO%2C%20an%20efficient%20and%0Acomputationally%20frugal%20end-to-end%20MOT%20model.%20MO-YOLO%20integrates%20principles%20from%0AYou%20Only%20Look%20Once%20%28YOLO%29%20and%20RT-DETR%2C%20adopting%20a%20decoder-only%20approach.%20By%0Aleveraging%20the%20decoder%20from%20RT-DETR%20and%20architectural%20components%20from%20YOLOv8%2C%0AMO-YOLO%20achieves%20high%20speed%2C%20shorter%20training%20times%2C%20and%20proficient%20MOT%0Aperformance.%20On%20the%20Dancetrack%2C%20MO-YOLO%20not%20only%20matches%20MOTR%27s%20performance%20but%0Aalso%20surpasses%20it%2C%20achieving%20over%20twice%20the%20frames%20per%20second%20%28MOTR%209.5%20FPS%2C%0AMO-YOLO%2019.6%20FPS%29.%20Furthermore%2C%20MO-YOLO%20demonstrates%20significantly%20reduced%0Atraining%20times%20and%20lower%20hardware%20requirements%20compared%20to%20MOTR.%20This%20research%0Aintroduces%20a%20promising%20paradigm%20for%20efficient%20end-to-end%20MOT%2C%20emphasizing%0Aenhanced%20performance%20and%20resource%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17170v3&entry.124074799=Read"},
{"title": "SnAG: Scalable and Accurate Video Grounding", "author": "Fangzhou Mu and Sicheng Mo and Yin Li", "abstract": "  Temporal grounding of text descriptions in videos is a central problem in\nvision-language learning and video understanding. Existing methods often\nprioritize accuracy over scalability -- they have been optimized for grounding\nonly a few text queries within short videos, and fail to scale up to long\nvideos with hundreds of queries. In this paper, we study the effect of\ncross-modal fusion on the scalability of video grounding models. Our analysis\nestablishes late fusion as a more cost-effective fusion scheme for long-form\nvideos with many text queries. Moreover, it leads us to a novel, video-centric\nsampling scheme for efficient training. Based on these findings, we present\nSnAG, a simple baseline for scalable and accurate video grounding. Without\nbells and whistles, SnAG is 43% more accurate and 1.5x faster than CONE, a\nstate of the art for long-form video grounding on the challenging MAD dataset,\nwhile achieving highly competitive results on short videos.\n", "link": "http://arxiv.org/abs/2404.02257v2", "date": "2024-04-05", "relevancy": 2.0832, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5401}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5194}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5145}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SnAG%3A%20Scalable%20and%20Accurate%20Video%20Grounding&body=Title%3A%20SnAG%3A%20Scalable%20and%20Accurate%20Video%20Grounding%0AAuthor%3A%20Fangzhou%20Mu%20and%20Sicheng%20Mo%20and%20Yin%20Li%0AAbstract%3A%20%20%20Temporal%20grounding%20of%20text%20descriptions%20in%20videos%20is%20a%20central%20problem%20in%0Avision-language%20learning%20and%20video%20understanding.%20Existing%20methods%20often%0Aprioritize%20accuracy%20over%20scalability%20--%20they%20have%20been%20optimized%20for%20grounding%0Aonly%20a%20few%20text%20queries%20within%20short%20videos%2C%20and%20fail%20to%20scale%20up%20to%20long%0Avideos%20with%20hundreds%20of%20queries.%20In%20this%20paper%2C%20we%20study%20the%20effect%20of%0Across-modal%20fusion%20on%20the%20scalability%20of%20video%20grounding%20models.%20Our%20analysis%0Aestablishes%20late%20fusion%20as%20a%20more%20cost-effective%20fusion%20scheme%20for%20long-form%0Avideos%20with%20many%20text%20queries.%20Moreover%2C%20it%20leads%20us%20to%20a%20novel%2C%20video-centric%0Asampling%20scheme%20for%20efficient%20training.%20Based%20on%20these%20findings%2C%20we%20present%0ASnAG%2C%20a%20simple%20baseline%20for%20scalable%20and%20accurate%20video%20grounding.%20Without%0Abells%20and%20whistles%2C%20SnAG%20is%2043%25%20more%20accurate%20and%201.5x%20faster%20than%20CONE%2C%20a%0Astate%20of%20the%20art%20for%20long-form%20video%20grounding%20on%20the%20challenging%20MAD%20dataset%2C%0Awhile%20achieving%20highly%20competitive%20results%20on%20short%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02257v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SnAG%3A%20Scalable%20and%20Accurate%20Video%20Grounding&entry.906535625=Fangzhou%20Mu%20and%20Sicheng%20Mo%20and%20Yin%20Li&entry.1292438233=%20%20Temporal%20grounding%20of%20text%20descriptions%20in%20videos%20is%20a%20central%20problem%20in%0Avision-language%20learning%20and%20video%20understanding.%20Existing%20methods%20often%0Aprioritize%20accuracy%20over%20scalability%20--%20they%20have%20been%20optimized%20for%20grounding%0Aonly%20a%20few%20text%20queries%20within%20short%20videos%2C%20and%20fail%20to%20scale%20up%20to%20long%0Avideos%20with%20hundreds%20of%20queries.%20In%20this%20paper%2C%20we%20study%20the%20effect%20of%0Across-modal%20fusion%20on%20the%20scalability%20of%20video%20grounding%20models.%20Our%20analysis%0Aestablishes%20late%20fusion%20as%20a%20more%20cost-effective%20fusion%20scheme%20for%20long-form%0Avideos%20with%20many%20text%20queries.%20Moreover%2C%20it%20leads%20us%20to%20a%20novel%2C%20video-centric%0Asampling%20scheme%20for%20efficient%20training.%20Based%20on%20these%20findings%2C%20we%20present%0ASnAG%2C%20a%20simple%20baseline%20for%20scalable%20and%20accurate%20video%20grounding.%20Without%0Abells%20and%20whistles%2C%20SnAG%20is%2043%25%20more%20accurate%20and%201.5x%20faster%20than%20CONE%2C%20a%0Astate%20of%20the%20art%20for%20long-form%20video%20grounding%20on%20the%20challenging%20MAD%20dataset%2C%0Awhile%20achieving%20highly%20competitive%20results%20on%20short%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02257v2&entry.124074799=Read"},
{"title": "PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce\n  Lidar", "author": "Tzofi Klinghoffer and Xiaoyu Xiang and Siddharth Somasundaram and Yuchen Fan and Christian Richardt and Ramesh Raskar and Rakesh Ranjan", "abstract": "  3D reconstruction from a single-view is challenging because of the ambiguity\nfrom monocular cues and lack of information about occluded regions. Neural\nradiance fields (NeRF), while popular for view synthesis and 3D reconstruction,\nare typically reliant on multi-view images. Existing methods for single-view 3D\nreconstruction with NeRF rely on either data priors to hallucinate views of\noccluded regions, which may not be physically accurate, or shadows observed by\nRGB cameras, which are difficult to detect in ambient light and low albedo\nbackgrounds. We propose using time-of-flight data captured by a single-photon\navalanche diode to overcome these limitations. Our method models two-bounce\noptical paths with NeRF, using lidar transient data for supervision. By\nleveraging the advantages of both NeRF and two-bounce light measured by lidar,\nwe demonstrate that we can reconstruct visible and occluded geometry without\ndata priors or reliance on controlled ambient lighting or scene albedo. In\naddition, we demonstrate improved generalization under practical constraints on\nsensor spatial- and temporal-resolution. We believe our method is a promising\ndirection as single-photon lidars become ubiquitous on consumer devices, such\nas phones, tablets, and headsets.\n", "link": "http://arxiv.org/abs/2312.14239v2", "date": "2024-04-05", "relevancy": 2.0763, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5508}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5072}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4921}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PlatoNeRF%3A%203D%20Reconstruction%20in%20Plato%27s%20Cave%20via%20Single-View%20Two-Bounce%0A%20%20Lidar&body=Title%3A%20PlatoNeRF%3A%203D%20Reconstruction%20in%20Plato%27s%20Cave%20via%20Single-View%20Two-Bounce%0A%20%20Lidar%0AAuthor%3A%20Tzofi%20Klinghoffer%20and%20Xiaoyu%20Xiang%20and%20Siddharth%20Somasundaram%20and%20Yuchen%20Fan%20and%20Christian%20Richardt%20and%20Ramesh%20Raskar%20and%20Rakesh%20Ranjan%0AAbstract%3A%20%20%203D%20reconstruction%20from%20a%20single-view%20is%20challenging%20because%20of%20the%20ambiguity%0Afrom%20monocular%20cues%20and%20lack%20of%20information%20about%20occluded%20regions.%20Neural%0Aradiance%20fields%20%28NeRF%29%2C%20while%20popular%20for%20view%20synthesis%20and%203D%20reconstruction%2C%0Aare%20typically%20reliant%20on%20multi-view%20images.%20Existing%20methods%20for%20single-view%203D%0Areconstruction%20with%20NeRF%20rely%20on%20either%20data%20priors%20to%20hallucinate%20views%20of%0Aoccluded%20regions%2C%20which%20may%20not%20be%20physically%20accurate%2C%20or%20shadows%20observed%20by%0ARGB%20cameras%2C%20which%20are%20difficult%20to%20detect%20in%20ambient%20light%20and%20low%20albedo%0Abackgrounds.%20We%20propose%20using%20time-of-flight%20data%20captured%20by%20a%20single-photon%0Aavalanche%20diode%20to%20overcome%20these%20limitations.%20Our%20method%20models%20two-bounce%0Aoptical%20paths%20with%20NeRF%2C%20using%20lidar%20transient%20data%20for%20supervision.%20By%0Aleveraging%20the%20advantages%20of%20both%20NeRF%20and%20two-bounce%20light%20measured%20by%20lidar%2C%0Awe%20demonstrate%20that%20we%20can%20reconstruct%20visible%20and%20occluded%20geometry%20without%0Adata%20priors%20or%20reliance%20on%20controlled%20ambient%20lighting%20or%20scene%20albedo.%20In%0Aaddition%2C%20we%20demonstrate%20improved%20generalization%20under%20practical%20constraints%20on%0Asensor%20spatial-%20and%20temporal-resolution.%20We%20believe%20our%20method%20is%20a%20promising%0Adirection%20as%20single-photon%20lidars%20become%20ubiquitous%20on%20consumer%20devices%2C%20such%0Aas%20phones%2C%20tablets%2C%20and%20headsets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14239v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlatoNeRF%3A%203D%20Reconstruction%20in%20Plato%27s%20Cave%20via%20Single-View%20Two-Bounce%0A%20%20Lidar&entry.906535625=Tzofi%20Klinghoffer%20and%20Xiaoyu%20Xiang%20and%20Siddharth%20Somasundaram%20and%20Yuchen%20Fan%20and%20Christian%20Richardt%20and%20Ramesh%20Raskar%20and%20Rakesh%20Ranjan&entry.1292438233=%20%203D%20reconstruction%20from%20a%20single-view%20is%20challenging%20because%20of%20the%20ambiguity%0Afrom%20monocular%20cues%20and%20lack%20of%20information%20about%20occluded%20regions.%20Neural%0Aradiance%20fields%20%28NeRF%29%2C%20while%20popular%20for%20view%20synthesis%20and%203D%20reconstruction%2C%0Aare%20typically%20reliant%20on%20multi-view%20images.%20Existing%20methods%20for%20single-view%203D%0Areconstruction%20with%20NeRF%20rely%20on%20either%20data%20priors%20to%20hallucinate%20views%20of%0Aoccluded%20regions%2C%20which%20may%20not%20be%20physically%20accurate%2C%20or%20shadows%20observed%20by%0ARGB%20cameras%2C%20which%20are%20difficult%20to%20detect%20in%20ambient%20light%20and%20low%20albedo%0Abackgrounds.%20We%20propose%20using%20time-of-flight%20data%20captured%20by%20a%20single-photon%0Aavalanche%20diode%20to%20overcome%20these%20limitations.%20Our%20method%20models%20two-bounce%0Aoptical%20paths%20with%20NeRF%2C%20using%20lidar%20transient%20data%20for%20supervision.%20By%0Aleveraging%20the%20advantages%20of%20both%20NeRF%20and%20two-bounce%20light%20measured%20by%20lidar%2C%0Awe%20demonstrate%20that%20we%20can%20reconstruct%20visible%20and%20occluded%20geometry%20without%0Adata%20priors%20or%20reliance%20on%20controlled%20ambient%20lighting%20or%20scene%20albedo.%20In%0Aaddition%2C%20we%20demonstrate%20improved%20generalization%20under%20practical%20constraints%20on%0Asensor%20spatial-%20and%20temporal-resolution.%20We%20believe%20our%20method%20is%20a%20promising%0Adirection%20as%20single-photon%20lidars%20become%20ubiquitous%20on%20consumer%20devices%2C%20such%0Aas%20phones%2C%20tablets%2C%20and%20headsets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14239v2&entry.124074799=Read"},
{"title": "Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation", "author": "Ji-Jia Wu and Andy Chia-Hao Chang and Chieh-Yu Chuang and Chun-Pei Chen and Yu-Lun Liu and Min-Hung Chen and Hou-Ning Hu and Yung-Yu Chuang and Yen-Yu Lin", "abstract": "  This paper addresses text-supervised semantic segmentation, aiming to learn a\nmodel capable of segmenting arbitrary visual concepts within images by using\nonly image-text pairs without dense annotations. Existing methods have\ndemonstrated that contrastive learning on image-text pairs effectively aligns\nvisual segments with the meanings of texts. We notice that there is a\ndiscrepancy between text alignment and semantic segmentation: A text often\nconsists of multiple semantic concepts, whereas semantic segmentation strives\nto create semantically homogeneous segments. To address this issue, we propose\na novel framework, Image-Text Co-Decomposition (CoDe), where the paired image\nand text are jointly decomposed into a set of image regions and a set of word\nsegments, respectively, and contrastive learning is developed to enforce\nregion-word alignment. To work with a vision-language model, we present a\nprompt learning mechanism that derives an extra representation to highlight an\nimage segment or a word segment of interest, with which more effective features\ncan be extracted from that segment. Comprehensive experimental results\ndemonstrate that our method performs favorably against existing text-supervised\nsemantic segmentation methods on six benchmark datasets.\n", "link": "http://arxiv.org/abs/2404.04231v1", "date": "2024-04-05", "relevancy": 2.0617, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5275}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.511}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4964}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Image-Text%20Co-Decomposition%20for%20Text-Supervised%20Semantic%20Segmentation&body=Title%3A%20Image-Text%20Co-Decomposition%20for%20Text-Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Ji-Jia%20Wu%20and%20Andy%20Chia-Hao%20Chang%20and%20Chieh-Yu%20Chuang%20and%20Chun-Pei%20Chen%20and%20Yu-Lun%20Liu%20and%20Min-Hung%20Chen%20and%20Hou-Ning%20Hu%20and%20Yung-Yu%20Chuang%20and%20Yen-Yu%20Lin%0AAbstract%3A%20%20%20This%20paper%20addresses%20text-supervised%20semantic%20segmentation%2C%20aiming%20to%20learn%20a%0Amodel%20capable%20of%20segmenting%20arbitrary%20visual%20concepts%20within%20images%20by%20using%0Aonly%20image-text%20pairs%20without%20dense%20annotations.%20Existing%20methods%20have%0Ademonstrated%20that%20contrastive%20learning%20on%20image-text%20pairs%20effectively%20aligns%0Avisual%20segments%20with%20the%20meanings%20of%20texts.%20We%20notice%20that%20there%20is%20a%0Adiscrepancy%20between%20text%20alignment%20and%20semantic%20segmentation%3A%20A%20text%20often%0Aconsists%20of%20multiple%20semantic%20concepts%2C%20whereas%20semantic%20segmentation%20strives%0Ato%20create%20semantically%20homogeneous%20segments.%20To%20address%20this%20issue%2C%20we%20propose%0Aa%20novel%20framework%2C%20Image-Text%20Co-Decomposition%20%28CoDe%29%2C%20where%20the%20paired%20image%0Aand%20text%20are%20jointly%20decomposed%20into%20a%20set%20of%20image%20regions%20and%20a%20set%20of%20word%0Asegments%2C%20respectively%2C%20and%20contrastive%20learning%20is%20developed%20to%20enforce%0Aregion-word%20alignment.%20To%20work%20with%20a%20vision-language%20model%2C%20we%20present%20a%0Aprompt%20learning%20mechanism%20that%20derives%20an%20extra%20representation%20to%20highlight%20an%0Aimage%20segment%20or%20a%20word%20segment%20of%20interest%2C%20with%20which%20more%20effective%20features%0Acan%20be%20extracted%20from%20that%20segment.%20Comprehensive%20experimental%20results%0Ademonstrate%20that%20our%20method%20performs%20favorably%20against%20existing%20text-supervised%0Asemantic%20segmentation%20methods%20on%20six%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04231v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-Text%20Co-Decomposition%20for%20Text-Supervised%20Semantic%20Segmentation&entry.906535625=Ji-Jia%20Wu%20and%20Andy%20Chia-Hao%20Chang%20and%20Chieh-Yu%20Chuang%20and%20Chun-Pei%20Chen%20and%20Yu-Lun%20Liu%20and%20Min-Hung%20Chen%20and%20Hou-Ning%20Hu%20and%20Yung-Yu%20Chuang%20and%20Yen-Yu%20Lin&entry.1292438233=%20%20This%20paper%20addresses%20text-supervised%20semantic%20segmentation%2C%20aiming%20to%20learn%20a%0Amodel%20capable%20of%20segmenting%20arbitrary%20visual%20concepts%20within%20images%20by%20using%0Aonly%20image-text%20pairs%20without%20dense%20annotations.%20Existing%20methods%20have%0Ademonstrated%20that%20contrastive%20learning%20on%20image-text%20pairs%20effectively%20aligns%0Avisual%20segments%20with%20the%20meanings%20of%20texts.%20We%20notice%20that%20there%20is%20a%0Adiscrepancy%20between%20text%20alignment%20and%20semantic%20segmentation%3A%20A%20text%20often%0Aconsists%20of%20multiple%20semantic%20concepts%2C%20whereas%20semantic%20segmentation%20strives%0Ato%20create%20semantically%20homogeneous%20segments.%20To%20address%20this%20issue%2C%20we%20propose%0Aa%20novel%20framework%2C%20Image-Text%20Co-Decomposition%20%28CoDe%29%2C%20where%20the%20paired%20image%0Aand%20text%20are%20jointly%20decomposed%20into%20a%20set%20of%20image%20regions%20and%20a%20set%20of%20word%0Asegments%2C%20respectively%2C%20and%20contrastive%20learning%20is%20developed%20to%20enforce%0Aregion-word%20alignment.%20To%20work%20with%20a%20vision-language%20model%2C%20we%20present%20a%0Aprompt%20learning%20mechanism%20that%20derives%20an%20extra%20representation%20to%20highlight%20an%0Aimage%20segment%20or%20a%20word%20segment%20of%20interest%2C%20with%20which%20more%20effective%20features%0Acan%20be%20extracted%20from%20that%20segment.%20Comprehensive%20experimental%20results%0Ademonstrate%20that%20our%20method%20performs%20favorably%20against%20existing%20text-supervised%0Asemantic%20segmentation%20methods%20on%20six%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04231v1&entry.124074799=Read"},
{"title": "Approximate UMAP allows for high-rate online visualization of\n  high-dimensional data streams", "author": "Peter Wassenaar and Pierre Guetschel and Michael Tangermann", "abstract": "  In the BCI field, introspection and interpretation of brain signals are\ndesired for providing feedback or to guide rapid paradigm prototyping but are\nchallenging due to the high noise level and dimensionality of the signals. Deep\nneural networks are often introspected by transforming their learned feature\nrepresentations into 2- or 3-dimensional subspace visualizations using\nprojection algorithms like Uniform Manifold Approximation and Projection\n(UMAP). Unfortunately, these methods are computationally expensive, making the\nprojection of data streams in real-time a non-trivial task. In this study, we\nintroduce a novel variant of UMAP, called approximate UMAP (aUMAP). It aims at\ngenerating rapid projections for real-time introspection. To study its\nsuitability for real-time projecting, we benchmark the methods against standard\nUMAP and its neural network counterpart parametric UMAP. Our results show that\napproximate UMAP delivers projections that replicate the projection space of\nstandard UMAP while decreasing projection speed by an order of magnitude and\nmaintaining the same training time.\n", "link": "http://arxiv.org/abs/2404.04001v1", "date": "2024-04-05", "relevancy": 2.0529, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5571}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4807}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Approximate%20UMAP%20allows%20for%20high-rate%20online%20visualization%20of%0A%20%20high-dimensional%20data%20streams&body=Title%3A%20Approximate%20UMAP%20allows%20for%20high-rate%20online%20visualization%20of%0A%20%20high-dimensional%20data%20streams%0AAuthor%3A%20Peter%20Wassenaar%20and%20Pierre%20Guetschel%20and%20Michael%20Tangermann%0AAbstract%3A%20%20%20In%20the%20BCI%20field%2C%20introspection%20and%20interpretation%20of%20brain%20signals%20are%0Adesired%20for%20providing%20feedback%20or%20to%20guide%20rapid%20paradigm%20prototyping%20but%20are%0Achallenging%20due%20to%20the%20high%20noise%20level%20and%20dimensionality%20of%20the%20signals.%20Deep%0Aneural%20networks%20are%20often%20introspected%20by%20transforming%20their%20learned%20feature%0Arepresentations%20into%202-%20or%203-dimensional%20subspace%20visualizations%20using%0Aprojection%20algorithms%20like%20Uniform%20Manifold%20Approximation%20and%20Projection%0A%28UMAP%29.%20Unfortunately%2C%20these%20methods%20are%20computationally%20expensive%2C%20making%20the%0Aprojection%20of%20data%20streams%20in%20real-time%20a%20non-trivial%20task.%20In%20this%20study%2C%20we%0Aintroduce%20a%20novel%20variant%20of%20UMAP%2C%20called%20approximate%20UMAP%20%28aUMAP%29.%20It%20aims%20at%0Agenerating%20rapid%20projections%20for%20real-time%20introspection.%20To%20study%20its%0Asuitability%20for%20real-time%20projecting%2C%20we%20benchmark%20the%20methods%20against%20standard%0AUMAP%20and%20its%20neural%20network%20counterpart%20parametric%20UMAP.%20Our%20results%20show%20that%0Aapproximate%20UMAP%20delivers%20projections%20that%20replicate%20the%20projection%20space%20of%0Astandard%20UMAP%20while%20decreasing%20projection%20speed%20by%20an%20order%20of%20magnitude%20and%0Amaintaining%20the%20same%20training%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04001v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximate%20UMAP%20allows%20for%20high-rate%20online%20visualization%20of%0A%20%20high-dimensional%20data%20streams&entry.906535625=Peter%20Wassenaar%20and%20Pierre%20Guetschel%20and%20Michael%20Tangermann&entry.1292438233=%20%20In%20the%20BCI%20field%2C%20introspection%20and%20interpretation%20of%20brain%20signals%20are%0Adesired%20for%20providing%20feedback%20or%20to%20guide%20rapid%20paradigm%20prototyping%20but%20are%0Achallenging%20due%20to%20the%20high%20noise%20level%20and%20dimensionality%20of%20the%20signals.%20Deep%0Aneural%20networks%20are%20often%20introspected%20by%20transforming%20their%20learned%20feature%0Arepresentations%20into%202-%20or%203-dimensional%20subspace%20visualizations%20using%0Aprojection%20algorithms%20like%20Uniform%20Manifold%20Approximation%20and%20Projection%0A%28UMAP%29.%20Unfortunately%2C%20these%20methods%20are%20computationally%20expensive%2C%20making%20the%0Aprojection%20of%20data%20streams%20in%20real-time%20a%20non-trivial%20task.%20In%20this%20study%2C%20we%0Aintroduce%20a%20novel%20variant%20of%20UMAP%2C%20called%20approximate%20UMAP%20%28aUMAP%29.%20It%20aims%20at%0Agenerating%20rapid%20projections%20for%20real-time%20introspection.%20To%20study%20its%0Asuitability%20for%20real-time%20projecting%2C%20we%20benchmark%20the%20methods%20against%20standard%0AUMAP%20and%20its%20neural%20network%20counterpart%20parametric%20UMAP.%20Our%20results%20show%20that%0Aapproximate%20UMAP%20delivers%20projections%20that%20replicate%20the%20projection%20space%20of%0Astandard%20UMAP%20while%20decreasing%20projection%20speed%20by%20an%20order%20of%20magnitude%20and%0Amaintaining%20the%20same%20training%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04001v1&entry.124074799=Read"},
{"title": "Evaluating Pedestrian Trajectory Prediction Methods with Respect to\n  Autonomous Driving", "author": "Nico Uhlemann and Felix Fent and Markus Lienkamp", "abstract": "  In this paper, we assess the state of the art in pedestrian trajectory\nprediction within the context of generating single trajectories, a critical\naspect aligning with the requirements in autonomous systems. The evaluation is\nconducted on the widely-used ETH/UCY dataset where the Average Displacement\nError (ADE) and the Final Displacement Error (FDE) are reported. Alongside\nthis, we perform an ablation study to investigate the impact of the observed\nmotion history on prediction performance. To evaluate the scalability of each\napproach when confronted with varying amounts of agents, the inference time of\neach model is measured. Following a quantitative analysis, the resulting\npredictions are compared in a qualitative manner, giving insight into the\nstrengths and weaknesses of current approaches. The results demonstrate that\nalthough a constant velocity model (CVM) provides a good approximation of the\noverall dynamics in the majority of cases, additional features need to be\nincorporated to reflect common pedestrian behavior observed. Therefore, this\nstudy presents a data-driven analysis with the intent to guide the future\ndevelopment of pedestrian trajectory prediction algorithms.\n", "link": "http://arxiv.org/abs/2308.05194v3", "date": "2024-04-05", "relevancy": 2.0473, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5719}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5218}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4778}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Pedestrian%20Trajectory%20Prediction%20Methods%20with%20Respect%20to%0A%20%20Autonomous%20Driving&body=Title%3A%20Evaluating%20Pedestrian%20Trajectory%20Prediction%20Methods%20with%20Respect%20to%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Nico%20Uhlemann%20and%20Felix%20Fent%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20assess%20the%20state%20of%20the%20art%20in%20pedestrian%20trajectory%0Aprediction%20within%20the%20context%20of%20generating%20single%20trajectories%2C%20a%20critical%0Aaspect%20aligning%20with%20the%20requirements%20in%20autonomous%20systems.%20The%20evaluation%20is%0Aconducted%20on%20the%20widely-used%20ETH/UCY%20dataset%20where%20the%20Average%20Displacement%0AError%20%28ADE%29%20and%20the%20Final%20Displacement%20Error%20%28FDE%29%20are%20reported.%20Alongside%0Athis%2C%20we%20perform%20an%20ablation%20study%20to%20investigate%20the%20impact%20of%20the%20observed%0Amotion%20history%20on%20prediction%20performance.%20To%20evaluate%20the%20scalability%20of%20each%0Aapproach%20when%20confronted%20with%20varying%20amounts%20of%20agents%2C%20the%20inference%20time%20of%0Aeach%20model%20is%20measured.%20Following%20a%20quantitative%20analysis%2C%20the%20resulting%0Apredictions%20are%20compared%20in%20a%20qualitative%20manner%2C%20giving%20insight%20into%20the%0Astrengths%20and%20weaknesses%20of%20current%20approaches.%20The%20results%20demonstrate%20that%0Aalthough%20a%20constant%20velocity%20model%20%28CVM%29%20provides%20a%20good%20approximation%20of%20the%0Aoverall%20dynamics%20in%20the%20majority%20of%20cases%2C%20additional%20features%20need%20to%20be%0Aincorporated%20to%20reflect%20common%20pedestrian%20behavior%20observed.%20Therefore%2C%20this%0Astudy%20presents%20a%20data-driven%20analysis%20with%20the%20intent%20to%20guide%20the%20future%0Adevelopment%20of%20pedestrian%20trajectory%20prediction%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05194v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Pedestrian%20Trajectory%20Prediction%20Methods%20with%20Respect%20to%0A%20%20Autonomous%20Driving&entry.906535625=Nico%20Uhlemann%20and%20Felix%20Fent%20and%20Markus%20Lienkamp&entry.1292438233=%20%20In%20this%20paper%2C%20we%20assess%20the%20state%20of%20the%20art%20in%20pedestrian%20trajectory%0Aprediction%20within%20the%20context%20of%20generating%20single%20trajectories%2C%20a%20critical%0Aaspect%20aligning%20with%20the%20requirements%20in%20autonomous%20systems.%20The%20evaluation%20is%0Aconducted%20on%20the%20widely-used%20ETH/UCY%20dataset%20where%20the%20Average%20Displacement%0AError%20%28ADE%29%20and%20the%20Final%20Displacement%20Error%20%28FDE%29%20are%20reported.%20Alongside%0Athis%2C%20we%20perform%20an%20ablation%20study%20to%20investigate%20the%20impact%20of%20the%20observed%0Amotion%20history%20on%20prediction%20performance.%20To%20evaluate%20the%20scalability%20of%20each%0Aapproach%20when%20confronted%20with%20varying%20amounts%20of%20agents%2C%20the%20inference%20time%20of%0Aeach%20model%20is%20measured.%20Following%20a%20quantitative%20analysis%2C%20the%20resulting%0Apredictions%20are%20compared%20in%20a%20qualitative%20manner%2C%20giving%20insight%20into%20the%0Astrengths%20and%20weaknesses%20of%20current%20approaches.%20The%20results%20demonstrate%20that%0Aalthough%20a%20constant%20velocity%20model%20%28CVM%29%20provides%20a%20good%20approximation%20of%20the%0Aoverall%20dynamics%20in%20the%20majority%20of%20cases%2C%20additional%20features%20need%20to%20be%0Aincorporated%20to%20reflect%20common%20pedestrian%20behavior%20observed.%20Therefore%2C%20this%0Astudy%20presents%20a%20data-driven%20analysis%20with%20the%20intent%20to%20guide%20the%20future%0Adevelopment%20of%20pedestrian%20trajectory%20prediction%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05194v3&entry.124074799=Read"},
{"title": "SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in\n  Transmission and Distribution Towers", "author": "Weile Li and Muqing Shi and Zhonghua Hong", "abstract": "  Traditional deep learning-based object detection networks often resize images\nduring the data preprocessing stage to achieve a uniform size and scale in the\nfeature map. Resizing is done to facilitate model propagation and fully\nconnected classification. However, resizing inevitably leads to object\ndeformation and loss of valuable information in the images. This drawback\nbecomes particularly pronounced for tiny objects like distribution towers with\nlinear shapes and few pixels. To address this issue, we propose abandoning the\nresizing operation. Instead, we introduce Positional-Encoding Multi-head\nCriss-Cross Attention. This allows the model to capture contextual information\nand learn from multiple representation subspaces, effectively enriching the\nsemantics of distribution towers. Additionally, we enhance Spatial Pyramid\nPooling by reshaping three pooled feature maps into a new unified one while\nalso reducing the computational burden. This approach allows images of\ndifferent sizes and scales to generate feature maps with uniform dimensions and\ncan be employed in feature map propagation. Our SCAResNet incorporates these\naforementioned improvements into the backbone network ResNet. We evaluated our\nSCAResNet using the Electric Transmission and Distribution Infrastructure\nImagery dataset from Duke University. Without any additional tricks, we\nemployed various object detection models with Gaussian Receptive Field based\nLabel Assignment as the baseline. When incorporating the SCAResNet into the\nbaseline model, we achieved a 2.1% improvement in mAPs. This demonstrates the\nadvantages of our SCAResNet in detecting transmission and distribution towers\nand its value in tiny object detection. The source code is available at\nhttps://github.com/LisavilaLee/SCAResNet_mmdet.\n", "link": "http://arxiv.org/abs/2404.04179v1", "date": "2024-04-05", "relevancy": 2.0384, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5205}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.508}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5069}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SCAResNet%3A%20A%20ResNet%20Variant%20Optimized%20for%20Tiny%20Object%20Detection%20in%0A%20%20Transmission%20and%20Distribution%20Towers&body=Title%3A%20SCAResNet%3A%20A%20ResNet%20Variant%20Optimized%20for%20Tiny%20Object%20Detection%20in%0A%20%20Transmission%20and%20Distribution%20Towers%0AAuthor%3A%20Weile%20Li%20and%20Muqing%20Shi%20and%20Zhonghua%20Hong%0AAbstract%3A%20%20%20Traditional%20deep%20learning-based%20object%20detection%20networks%20often%20resize%20images%0Aduring%20the%20data%20preprocessing%20stage%20to%20achieve%20a%20uniform%20size%20and%20scale%20in%20the%0Afeature%20map.%20Resizing%20is%20done%20to%20facilitate%20model%20propagation%20and%20fully%0Aconnected%20classification.%20However%2C%20resizing%20inevitably%20leads%20to%20object%0Adeformation%20and%20loss%20of%20valuable%20information%20in%20the%20images.%20This%20drawback%0Abecomes%20particularly%20pronounced%20for%20tiny%20objects%20like%20distribution%20towers%20with%0Alinear%20shapes%20and%20few%20pixels.%20To%20address%20this%20issue%2C%20we%20propose%20abandoning%20the%0Aresizing%20operation.%20Instead%2C%20we%20introduce%20Positional-Encoding%20Multi-head%0ACriss-Cross%20Attention.%20This%20allows%20the%20model%20to%20capture%20contextual%20information%0Aand%20learn%20from%20multiple%20representation%20subspaces%2C%20effectively%20enriching%20the%0Asemantics%20of%20distribution%20towers.%20Additionally%2C%20we%20enhance%20Spatial%20Pyramid%0APooling%20by%20reshaping%20three%20pooled%20feature%20maps%20into%20a%20new%20unified%20one%20while%0Aalso%20reducing%20the%20computational%20burden.%20This%20approach%20allows%20images%20of%0Adifferent%20sizes%20and%20scales%20to%20generate%20feature%20maps%20with%20uniform%20dimensions%20and%0Acan%20be%20employed%20in%20feature%20map%20propagation.%20Our%20SCAResNet%20incorporates%20these%0Aaforementioned%20improvements%20into%20the%20backbone%20network%20ResNet.%20We%20evaluated%20our%0ASCAResNet%20using%20the%20Electric%20Transmission%20and%20Distribution%20Infrastructure%0AImagery%20dataset%20from%20Duke%20University.%20Without%20any%20additional%20tricks%2C%20we%0Aemployed%20various%20object%20detection%20models%20with%20Gaussian%20Receptive%20Field%20based%0ALabel%20Assignment%20as%20the%20baseline.%20When%20incorporating%20the%20SCAResNet%20into%20the%0Abaseline%20model%2C%20we%20achieved%20a%202.1%25%20improvement%20in%20mAPs.%20This%20demonstrates%20the%0Aadvantages%20of%20our%20SCAResNet%20in%20detecting%20transmission%20and%20distribution%20towers%0Aand%20its%20value%20in%20tiny%20object%20detection.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/LisavilaLee/SCAResNet_mmdet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04179v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAResNet%3A%20A%20ResNet%20Variant%20Optimized%20for%20Tiny%20Object%20Detection%20in%0A%20%20Transmission%20and%20Distribution%20Towers&entry.906535625=Weile%20Li%20and%20Muqing%20Shi%20and%20Zhonghua%20Hong&entry.1292438233=%20%20Traditional%20deep%20learning-based%20object%20detection%20networks%20often%20resize%20images%0Aduring%20the%20data%20preprocessing%20stage%20to%20achieve%20a%20uniform%20size%20and%20scale%20in%20the%0Afeature%20map.%20Resizing%20is%20done%20to%20facilitate%20model%20propagation%20and%20fully%0Aconnected%20classification.%20However%2C%20resizing%20inevitably%20leads%20to%20object%0Adeformation%20and%20loss%20of%20valuable%20information%20in%20the%20images.%20This%20drawback%0Abecomes%20particularly%20pronounced%20for%20tiny%20objects%20like%20distribution%20towers%20with%0Alinear%20shapes%20and%20few%20pixels.%20To%20address%20this%20issue%2C%20we%20propose%20abandoning%20the%0Aresizing%20operation.%20Instead%2C%20we%20introduce%20Positional-Encoding%20Multi-head%0ACriss-Cross%20Attention.%20This%20allows%20the%20model%20to%20capture%20contextual%20information%0Aand%20learn%20from%20multiple%20representation%20subspaces%2C%20effectively%20enriching%20the%0Asemantics%20of%20distribution%20towers.%20Additionally%2C%20we%20enhance%20Spatial%20Pyramid%0APooling%20by%20reshaping%20three%20pooled%20feature%20maps%20into%20a%20new%20unified%20one%20while%0Aalso%20reducing%20the%20computational%20burden.%20This%20approach%20allows%20images%20of%0Adifferent%20sizes%20and%20scales%20to%20generate%20feature%20maps%20with%20uniform%20dimensions%20and%0Acan%20be%20employed%20in%20feature%20map%20propagation.%20Our%20SCAResNet%20incorporates%20these%0Aaforementioned%20improvements%20into%20the%20backbone%20network%20ResNet.%20We%20evaluated%20our%0ASCAResNet%20using%20the%20Electric%20Transmission%20and%20Distribution%20Infrastructure%0AImagery%20dataset%20from%20Duke%20University.%20Without%20any%20additional%20tricks%2C%20we%0Aemployed%20various%20object%20detection%20models%20with%20Gaussian%20Receptive%20Field%20based%0ALabel%20Assignment%20as%20the%20baseline.%20When%20incorporating%20the%20SCAResNet%20into%20the%0Abaseline%20model%2C%20we%20achieved%20a%202.1%25%20improvement%20in%20mAPs.%20This%20demonstrates%20the%0Aadvantages%20of%20our%20SCAResNet%20in%20detecting%20transmission%20and%20distribution%20towers%0Aand%20its%20value%20in%20tiny%20object%20detection.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/LisavilaLee/SCAResNet_mmdet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04179v1&entry.124074799=Read"},
{"title": "Validation of critical maneuvers based on shared control", "author": "Mauricio Marcano and Joseba Sarabia and Asier Zubizarreta and Sergio D\u00edaz", "abstract": "  This paper presents the validation of shared control strategies for critical\nmaneuvers in automated driving systems. Shared control involves collaboration\nbetween the driver and automation, allowing both parties to actively engage and\ncooperate at different levels of the driving task. The involvement of the\ndriver adds complexity to the control loop, necessitating comprehensive\nvalidation methodologies. The proposed approach focuses on two critical\nmaneuvers: overtaking in low visibility scenarios and lateral evasive actions.\nA modular architecture with an arbitration module and shared control algorithms\nis implemented, primarily focusing on the lateral control of the vehicle. The\nvalidation is conducted using a dynamic simulator, involving 8 real drivers\ninteracting with a virtual environment. The results demonstrate improved safety\nand user acceptance, indicating the effectiveness of the shared control\nstrategies in comparison with no shared-control support. Future work involves\nimplementing shared control in drive-by-wire systems to enhance safety and\ndriver comfort during critical maneuvers. Overall, this research contributes to\nthe development and validation of shared control approaches in automated\ndriving systems.\n", "link": "http://arxiv.org/abs/2404.04011v1", "date": "2024-04-05", "relevancy": 2.0283, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5308}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5025}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4591}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Validation%20of%20critical%20maneuvers%20based%20on%20shared%20control&body=Title%3A%20Validation%20of%20critical%20maneuvers%20based%20on%20shared%20control%0AAuthor%3A%20Mauricio%20Marcano%20and%20Joseba%20Sarabia%20and%20Asier%20Zubizarreta%20and%20Sergio%20D%C3%ADaz%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20validation%20of%20shared%20control%20strategies%20for%20critical%0Amaneuvers%20in%20automated%20driving%20systems.%20Shared%20control%20involves%20collaboration%0Abetween%20the%20driver%20and%20automation%2C%20allowing%20both%20parties%20to%20actively%20engage%20and%0Acooperate%20at%20different%20levels%20of%20the%20driving%20task.%20The%20involvement%20of%20the%0Adriver%20adds%20complexity%20to%20the%20control%20loop%2C%20necessitating%20comprehensive%0Avalidation%20methodologies.%20The%20proposed%20approach%20focuses%20on%20two%20critical%0Amaneuvers%3A%20overtaking%20in%20low%20visibility%20scenarios%20and%20lateral%20evasive%20actions.%0AA%20modular%20architecture%20with%20an%20arbitration%20module%20and%20shared%20control%20algorithms%0Ais%20implemented%2C%20primarily%20focusing%20on%20the%20lateral%20control%20of%20the%20vehicle.%20The%0Avalidation%20is%20conducted%20using%20a%20dynamic%20simulator%2C%20involving%208%20real%20drivers%0Ainteracting%20with%20a%20virtual%20environment.%20The%20results%20demonstrate%20improved%20safety%0Aand%20user%20acceptance%2C%20indicating%20the%20effectiveness%20of%20the%20shared%20control%0Astrategies%20in%20comparison%20with%20no%20shared-control%20support.%20Future%20work%20involves%0Aimplementing%20shared%20control%20in%20drive-by-wire%20systems%20to%20enhance%20safety%20and%0Adriver%20comfort%20during%20critical%20maneuvers.%20Overall%2C%20this%20research%20contributes%20to%0Athe%20development%20and%20validation%20of%20shared%20control%20approaches%20in%20automated%0Adriving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04011v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Validation%20of%20critical%20maneuvers%20based%20on%20shared%20control&entry.906535625=Mauricio%20Marcano%20and%20Joseba%20Sarabia%20and%20Asier%20Zubizarreta%20and%20Sergio%20D%C3%ADaz&entry.1292438233=%20%20This%20paper%20presents%20the%20validation%20of%20shared%20control%20strategies%20for%20critical%0Amaneuvers%20in%20automated%20driving%20systems.%20Shared%20control%20involves%20collaboration%0Abetween%20the%20driver%20and%20automation%2C%20allowing%20both%20parties%20to%20actively%20engage%20and%0Acooperate%20at%20different%20levels%20of%20the%20driving%20task.%20The%20involvement%20of%20the%0Adriver%20adds%20complexity%20to%20the%20control%20loop%2C%20necessitating%20comprehensive%0Avalidation%20methodologies.%20The%20proposed%20approach%20focuses%20on%20two%20critical%0Amaneuvers%3A%20overtaking%20in%20low%20visibility%20scenarios%20and%20lateral%20evasive%20actions.%0AA%20modular%20architecture%20with%20an%20arbitration%20module%20and%20shared%20control%20algorithms%0Ais%20implemented%2C%20primarily%20focusing%20on%20the%20lateral%20control%20of%20the%20vehicle.%20The%0Avalidation%20is%20conducted%20using%20a%20dynamic%20simulator%2C%20involving%208%20real%20drivers%0Ainteracting%20with%20a%20virtual%20environment.%20The%20results%20demonstrate%20improved%20safety%0Aand%20user%20acceptance%2C%20indicating%20the%20effectiveness%20of%20the%20shared%20control%0Astrategies%20in%20comparison%20with%20no%20shared-control%20support.%20Future%20work%20involves%0Aimplementing%20shared%20control%20in%20drive-by-wire%20systems%20to%20enhance%20safety%20and%0Adriver%20comfort%20during%20critical%20maneuvers.%20Overall%2C%20this%20research%20contributes%20to%0Athe%20development%20and%20validation%20of%20shared%20control%20approaches%20in%20automated%0Adriving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04011v1&entry.124074799=Read"},
{"title": "Prompt-prompted Mixture of Experts for Efficient LLM Generation", "author": "Harry Dong and Beidi Chen and Yuejie Chi", "abstract": "  With the development of transformer-based large language models (LLMs), they\nhave been applied to many fields due to their remarkable utility, but this\ncomes at a considerable computational cost at deployment. Fortunately, some\nmethods such as pruning or constructing a mixture of experts (MoE) aim at\nexploiting sparsity in transformer feedforward (FF) blocks to gain boosts in\nspeed and reduction in memory requirements. However, these techniques can be\nvery costly and inflexible in practice, as they often require training or are\nrestricted to specific types of architectures. To address this, we introduce\nGRIFFIN, a novel training-free MoE that selects unique FF experts at the\nsequence level for efficient generation across a plethora of LLMs with\ndifferent non-ReLU activation functions. This is possible due to a critical\nobservation that many trained LLMs naturally produce highly structured FF\nactivation patterns within a sequence, which we call flocking. Despite our\nmethod's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains\nthe original model's performance with little to no degradation on a variety of\nclassification and generation tasks, all while improving latency (e.g.\n1.25$\\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code is available at\nhttps://github.com/hdong920/GRIFFIN.\n", "link": "http://arxiv.org/abs/2404.01365v2", "date": "2024-04-05", "relevancy": 2.0207, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5182}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4989}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4883}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Prompt-prompted%20Mixture%20of%20Experts%20for%20Efficient%20LLM%20Generation&body=Title%3A%20Prompt-prompted%20Mixture%20of%20Experts%20for%20Efficient%20LLM%20Generation%0AAuthor%3A%20Harry%20Dong%20and%20Beidi%20Chen%20and%20Yuejie%20Chi%0AAbstract%3A%20%20%20With%20the%20development%20of%20transformer-based%20large%20language%20models%20%28LLMs%29%2C%20they%0Ahave%20been%20applied%20to%20many%20fields%20due%20to%20their%20remarkable%20utility%2C%20but%20this%0Acomes%20at%20a%20considerable%20computational%20cost%20at%20deployment.%20Fortunately%2C%20some%0Amethods%20such%20as%20pruning%20or%20constructing%20a%20mixture%20of%20experts%20%28MoE%29%20aim%20at%0Aexploiting%20sparsity%20in%20transformer%20feedforward%20%28FF%29%20blocks%20to%20gain%20boosts%20in%0Aspeed%20and%20reduction%20in%20memory%20requirements.%20However%2C%20these%20techniques%20can%20be%0Avery%20costly%20and%20inflexible%20in%20practice%2C%20as%20they%20often%20require%20training%20or%20are%0Arestricted%20to%20specific%20types%20of%20architectures.%20To%20address%20this%2C%20we%20introduce%0AGRIFFIN%2C%20a%20novel%20training-free%20MoE%20that%20selects%20unique%20FF%20experts%20at%20the%0Asequence%20level%20for%20efficient%20generation%20across%20a%20plethora%20of%20LLMs%20with%0Adifferent%20non-ReLU%20activation%20functions.%20This%20is%20possible%20due%20to%20a%20critical%0Aobservation%20that%20many%20trained%20LLMs%20naturally%20produce%20highly%20structured%20FF%0Aactivation%20patterns%20within%20a%20sequence%2C%20which%20we%20call%20flocking.%20Despite%20our%0Amethod%27s%20simplicity%2C%20we%20show%20with%2050%25%20of%20the%20FF%20parameters%2C%20GRIFFIN%20maintains%0Athe%20original%20model%27s%20performance%20with%20little%20to%20no%20degradation%20on%20a%20variety%20of%0Aclassification%20and%20generation%20tasks%2C%20all%20while%20improving%20latency%20%28e.g.%0A1.25%24%5Ctimes%24%20speed-up%20in%20Llama%202%2013B%20on%20an%20NVIDIA%20L40%29.%20Code%20is%20available%20at%0Ahttps%3A//github.com/hdong920/GRIFFIN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01365v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-prompted%20Mixture%20of%20Experts%20for%20Efficient%20LLM%20Generation&entry.906535625=Harry%20Dong%20and%20Beidi%20Chen%20and%20Yuejie%20Chi&entry.1292438233=%20%20With%20the%20development%20of%20transformer-based%20large%20language%20models%20%28LLMs%29%2C%20they%0Ahave%20been%20applied%20to%20many%20fields%20due%20to%20their%20remarkable%20utility%2C%20but%20this%0Acomes%20at%20a%20considerable%20computational%20cost%20at%20deployment.%20Fortunately%2C%20some%0Amethods%20such%20as%20pruning%20or%20constructing%20a%20mixture%20of%20experts%20%28MoE%29%20aim%20at%0Aexploiting%20sparsity%20in%20transformer%20feedforward%20%28FF%29%20blocks%20to%20gain%20boosts%20in%0Aspeed%20and%20reduction%20in%20memory%20requirements.%20However%2C%20these%20techniques%20can%20be%0Avery%20costly%20and%20inflexible%20in%20practice%2C%20as%20they%20often%20require%20training%20or%20are%0Arestricted%20to%20specific%20types%20of%20architectures.%20To%20address%20this%2C%20we%20introduce%0AGRIFFIN%2C%20a%20novel%20training-free%20MoE%20that%20selects%20unique%20FF%20experts%20at%20the%0Asequence%20level%20for%20efficient%20generation%20across%20a%20plethora%20of%20LLMs%20with%0Adifferent%20non-ReLU%20activation%20functions.%20This%20is%20possible%20due%20to%20a%20critical%0Aobservation%20that%20many%20trained%20LLMs%20naturally%20produce%20highly%20structured%20FF%0Aactivation%20patterns%20within%20a%20sequence%2C%20which%20we%20call%20flocking.%20Despite%20our%0Amethod%27s%20simplicity%2C%20we%20show%20with%2050%25%20of%20the%20FF%20parameters%2C%20GRIFFIN%20maintains%0Athe%20original%20model%27s%20performance%20with%20little%20to%20no%20degradation%20on%20a%20variety%20of%0Aclassification%20and%20generation%20tasks%2C%20all%20while%20improving%20latency%20%28e.g.%0A1.25%24%5Ctimes%24%20speed-up%20in%20Llama%202%2013B%20on%20an%20NVIDIA%20L40%29.%20Code%20is%20available%20at%0Ahttps%3A//github.com/hdong920/GRIFFIN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01365v2&entry.124074799=Read"},
{"title": "Heterogeneous Graph Contrastive Learning with Meta-path Contexts and\n  Adaptively Weighted Negative Samples", "author": "Jianxiang Yu and Qingqing Ge and Xiang Li and Aoying Zhou", "abstract": "  Heterogeneous graph contrastive learning has received wide attention\nrecently. Some existing methods use meta-paths, which are sequences of object\ntypes that capture semantic relationships between objects, to construct\ncontrastive views. However, most of them ignore the rich meta-path context\ninformation that describes how two objects are connected by meta-paths.\nFurther, they fail to distinguish negative samples, which could adversely\naffect the model performance. To address the problems, we propose MEOW, which\nconsiders both meta-path contexts and weighted negative samples. Specifically,\nMEOW constructs a coarse view and a fine-grained view for contrast. The former\nreflects which objects are connected by meta-paths, while the latter uses\nmeta-path contexts and characterizes details on how the objects are connected.\nThen, we theoretically analyze the InfoNCE loss and recognize its limitations\nfor computing gradients of negative samples. To better distinguish negative\nsamples, we learn hard-valued weights for them based on node clustering and use\nprototypical contrastive learning to pull close embeddings of nodes in the same\ncluster. In addition, we propose a variant model AdaMEOW that adaptively learns\nsoft-valued weights of negative samples to further improve node representation.\nFinally, we conduct extensive experiments to show the superiority of MEOW and\nAdaMEOW against other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2212.13847v3", "date": "2024-04-05", "relevancy": 2.0134, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5096}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5012}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4929}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Graph%20Contrastive%20Learning%20with%20Meta-path%20Contexts%20and%0A%20%20Adaptively%20Weighted%20Negative%20Samples&body=Title%3A%20Heterogeneous%20Graph%20Contrastive%20Learning%20with%20Meta-path%20Contexts%20and%0A%20%20Adaptively%20Weighted%20Negative%20Samples%0AAuthor%3A%20Jianxiang%20Yu%20and%20Qingqing%20Ge%20and%20Xiang%20Li%20and%20Aoying%20Zhou%0AAbstract%3A%20%20%20Heterogeneous%20graph%20contrastive%20learning%20has%20received%20wide%20attention%0Arecently.%20Some%20existing%20methods%20use%20meta-paths%2C%20which%20are%20sequences%20of%20object%0Atypes%20that%20capture%20semantic%20relationships%20between%20objects%2C%20to%20construct%0Acontrastive%20views.%20However%2C%20most%20of%20them%20ignore%20the%20rich%20meta-path%20context%0Ainformation%20that%20describes%20how%20two%20objects%20are%20connected%20by%20meta-paths.%0AFurther%2C%20they%20fail%20to%20distinguish%20negative%20samples%2C%20which%20could%20adversely%0Aaffect%20the%20model%20performance.%20To%20address%20the%20problems%2C%20we%20propose%20MEOW%2C%20which%0Aconsiders%20both%20meta-path%20contexts%20and%20weighted%20negative%20samples.%20Specifically%2C%0AMEOW%20constructs%20a%20coarse%20view%20and%20a%20fine-grained%20view%20for%20contrast.%20The%20former%0Areflects%20which%20objects%20are%20connected%20by%20meta-paths%2C%20while%20the%20latter%20uses%0Ameta-path%20contexts%20and%20characterizes%20details%20on%20how%20the%20objects%20are%20connected.%0AThen%2C%20we%20theoretically%20analyze%20the%20InfoNCE%20loss%20and%20recognize%20its%20limitations%0Afor%20computing%20gradients%20of%20negative%20samples.%20To%20better%20distinguish%20negative%0Asamples%2C%20we%20learn%20hard-valued%20weights%20for%20them%20based%20on%20node%20clustering%20and%20use%0Aprototypical%20contrastive%20learning%20to%20pull%20close%20embeddings%20of%20nodes%20in%20the%20same%0Acluster.%20In%20addition%2C%20we%20propose%20a%20variant%20model%20AdaMEOW%20that%20adaptively%20learns%0Asoft-valued%20weights%20of%20negative%20samples%20to%20further%20improve%20node%20representation.%0AFinally%2C%20we%20conduct%20extensive%20experiments%20to%20show%20the%20superiority%20of%20MEOW%20and%0AAdaMEOW%20against%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.13847v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Graph%20Contrastive%20Learning%20with%20Meta-path%20Contexts%20and%0A%20%20Adaptively%20Weighted%20Negative%20Samples&entry.906535625=Jianxiang%20Yu%20and%20Qingqing%20Ge%20and%20Xiang%20Li%20and%20Aoying%20Zhou&entry.1292438233=%20%20Heterogeneous%20graph%20contrastive%20learning%20has%20received%20wide%20attention%0Arecently.%20Some%20existing%20methods%20use%20meta-paths%2C%20which%20are%20sequences%20of%20object%0Atypes%20that%20capture%20semantic%20relationships%20between%20objects%2C%20to%20construct%0Acontrastive%20views.%20However%2C%20most%20of%20them%20ignore%20the%20rich%20meta-path%20context%0Ainformation%20that%20describes%20how%20two%20objects%20are%20connected%20by%20meta-paths.%0AFurther%2C%20they%20fail%20to%20distinguish%20negative%20samples%2C%20which%20could%20adversely%0Aaffect%20the%20model%20performance.%20To%20address%20the%20problems%2C%20we%20propose%20MEOW%2C%20which%0Aconsiders%20both%20meta-path%20contexts%20and%20weighted%20negative%20samples.%20Specifically%2C%0AMEOW%20constructs%20a%20coarse%20view%20and%20a%20fine-grained%20view%20for%20contrast.%20The%20former%0Areflects%20which%20objects%20are%20connected%20by%20meta-paths%2C%20while%20the%20latter%20uses%0Ameta-path%20contexts%20and%20characterizes%20details%20on%20how%20the%20objects%20are%20connected.%0AThen%2C%20we%20theoretically%20analyze%20the%20InfoNCE%20loss%20and%20recognize%20its%20limitations%0Afor%20computing%20gradients%20of%20negative%20samples.%20To%20better%20distinguish%20negative%0Asamples%2C%20we%20learn%20hard-valued%20weights%20for%20them%20based%20on%20node%20clustering%20and%20use%0Aprototypical%20contrastive%20learning%20to%20pull%20close%20embeddings%20of%20nodes%20in%20the%20same%0Acluster.%20In%20addition%2C%20we%20propose%20a%20variant%20model%20AdaMEOW%20that%20adaptively%20learns%0Asoft-valued%20weights%20of%20negative%20samples%20to%20further%20improve%20node%20representation.%0AFinally%2C%20we%20conduct%20extensive%20experiments%20to%20show%20the%20superiority%20of%20MEOW%20and%0AAdaMEOW%20against%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.13847v3&entry.124074799=Read"},
{"title": "Watermark-based Detection and Attribution of AI-Generated Content", "author": "Zhengyuan Jiang and Moyang Guo and Yuepeng Hu and Neil Zhenqiang Gong", "abstract": "  Several companies--such as Google, Microsoft, and OpenAI--have deployed\ntechniques to watermark AI-generated content to enable proactive detection.\nHowever, existing literature mainly focuses on user-agnostic detection.\nAttribution aims to further trace back the user of a generative-AI service who\ngenerated a given content detected as AI-generated. Despite its growing\nimportance, attribution is largely unexplored. In this work, we aim to bridge\nthis gap by providing the first systematic study on watermark-based, user-aware\ndetection and attribution of AI-generated content. Specifically, we\ntheoretically study the detection and attribution performance via rigorous\nprobabilistic analysis. Moreover, we develop an efficient algorithm to select\nwatermarks for the users to enhance attribution performance. Both our\ntheoretical and empirical results show that watermark-based detection and\nattribution inherit the accuracy and (non-)robustness properties of the\nwatermarking method.\n", "link": "http://arxiv.org/abs/2404.04254v1", "date": "2024-04-05", "relevancy": 2.0008, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5184}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4878}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4857}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Watermark-based%20Detection%20and%20Attribution%20of%20AI-Generated%20Content&body=Title%3A%20Watermark-based%20Detection%20and%20Attribution%20of%20AI-Generated%20Content%0AAuthor%3A%20Zhengyuan%20Jiang%20and%20Moyang%20Guo%20and%20Yuepeng%20Hu%20and%20Neil%20Zhenqiang%20Gong%0AAbstract%3A%20%20%20Several%20companies--such%20as%20Google%2C%20Microsoft%2C%20and%20OpenAI--have%20deployed%0Atechniques%20to%20watermark%20AI-generated%20content%20to%20enable%20proactive%20detection.%0AHowever%2C%20existing%20literature%20mainly%20focuses%20on%20user-agnostic%20detection.%0AAttribution%20aims%20to%20further%20trace%20back%20the%20user%20of%20a%20generative-AI%20service%20who%0Agenerated%20a%20given%20content%20detected%20as%20AI-generated.%20Despite%20its%20growing%0Aimportance%2C%20attribution%20is%20largely%20unexplored.%20In%20this%20work%2C%20we%20aim%20to%20bridge%0Athis%20gap%20by%20providing%20the%20first%20systematic%20study%20on%20watermark-based%2C%20user-aware%0Adetection%20and%20attribution%20of%20AI-generated%20content.%20Specifically%2C%20we%0Atheoretically%20study%20the%20detection%20and%20attribution%20performance%20via%20rigorous%0Aprobabilistic%20analysis.%20Moreover%2C%20we%20develop%20an%20efficient%20algorithm%20to%20select%0Awatermarks%20for%20the%20users%20to%20enhance%20attribution%20performance.%20Both%20our%0Atheoretical%20and%20empirical%20results%20show%20that%20watermark-based%20detection%20and%0Aattribution%20inherit%20the%20accuracy%20and%20%28non-%29robustness%20properties%20of%20the%0Awatermarking%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04254v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Watermark-based%20Detection%20and%20Attribution%20of%20AI-Generated%20Content&entry.906535625=Zhengyuan%20Jiang%20and%20Moyang%20Guo%20and%20Yuepeng%20Hu%20and%20Neil%20Zhenqiang%20Gong&entry.1292438233=%20%20Several%20companies--such%20as%20Google%2C%20Microsoft%2C%20and%20OpenAI--have%20deployed%0Atechniques%20to%20watermark%20AI-generated%20content%20to%20enable%20proactive%20detection.%0AHowever%2C%20existing%20literature%20mainly%20focuses%20on%20user-agnostic%20detection.%0AAttribution%20aims%20to%20further%20trace%20back%20the%20user%20of%20a%20generative-AI%20service%20who%0Agenerated%20a%20given%20content%20detected%20as%20AI-generated.%20Despite%20its%20growing%0Aimportance%2C%20attribution%20is%20largely%20unexplored.%20In%20this%20work%2C%20we%20aim%20to%20bridge%0Athis%20gap%20by%20providing%20the%20first%20systematic%20study%20on%20watermark-based%2C%20user-aware%0Adetection%20and%20attribution%20of%20AI-generated%20content.%20Specifically%2C%20we%0Atheoretically%20study%20the%20detection%20and%20attribution%20performance%20via%20rigorous%0Aprobabilistic%20analysis.%20Moreover%2C%20we%20develop%20an%20efficient%20algorithm%20to%20select%0Awatermarks%20for%20the%20users%20to%20enhance%20attribution%20performance.%20Both%20our%0Atheoretical%20and%20empirical%20results%20show%20that%20watermark-based%20detection%20and%0Aattribution%20inherit%20the%20accuracy%20and%20%28non-%29robustness%20properties%20of%20the%0Awatermarking%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04254v1&entry.124074799=Read"},
{"title": "Adaptive Line-Of-Sight guidance law based on vector fields path\n  following for underactuated unmanned surface vehicle", "author": "Jie Qi and Ronghua Wanga and Nailong Wu", "abstract": "  The focus of this paper is to develop a methodology that enables an unmanned\nsurface vehicle (USV) to efficiently track a planned path. The introduction of\na vector field-based adaptive line of-sight guidance law (VFALOS) for accurate\ntrajectory tracking and minimizing the overshoot response time during USV\ntracking of curved paths improves the overall line-of-sight (LOS) guidance\nmethod. These improvements contribute to faster convergence to the desired\npath, reduce oscillations, and can mitigate the effects of persistent external\ndisturbances. It is shown that the proposed guidance law exhibits k-exponential\nstability when converging to the desired path consisting of straight and curved\nlines. The results in the paper show that the proposed method effectively\nimproves the accuracy of the USV tracking the desired path while ensuring the\nsafety of the USV work.\n", "link": "http://arxiv.org/abs/2403.17448v2", "date": "2024-04-05", "relevancy": 1.9986, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5144}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4911}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4883}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Line-Of-Sight%20guidance%20law%20based%20on%20vector%20fields%20path%0A%20%20following%20for%20underactuated%20unmanned%20surface%20vehicle&body=Title%3A%20Adaptive%20Line-Of-Sight%20guidance%20law%20based%20on%20vector%20fields%20path%0A%20%20following%20for%20underactuated%20unmanned%20surface%20vehicle%0AAuthor%3A%20Jie%20Qi%20and%20Ronghua%20Wanga%20and%20Nailong%20Wu%0AAbstract%3A%20%20%20The%20focus%20of%20this%20paper%20is%20to%20develop%20a%20methodology%20that%20enables%20an%20unmanned%0Asurface%20vehicle%20%28USV%29%20to%20efficiently%20track%20a%20planned%20path.%20The%20introduction%20of%0Aa%20vector%20field-based%20adaptive%20line%20of-sight%20guidance%20law%20%28VFALOS%29%20for%20accurate%0Atrajectory%20tracking%20and%20minimizing%20the%20overshoot%20response%20time%20during%20USV%0Atracking%20of%20curved%20paths%20improves%20the%20overall%20line-of-sight%20%28LOS%29%20guidance%0Amethod.%20These%20improvements%20contribute%20to%20faster%20convergence%20to%20the%20desired%0Apath%2C%20reduce%20oscillations%2C%20and%20can%20mitigate%20the%20effects%20of%20persistent%20external%0Adisturbances.%20It%20is%20shown%20that%20the%20proposed%20guidance%20law%20exhibits%20k-exponential%0Astability%20when%20converging%20to%20the%20desired%20path%20consisting%20of%20straight%20and%20curved%0Alines.%20The%20results%20in%20the%20paper%20show%20that%20the%20proposed%20method%20effectively%0Aimproves%20the%20accuracy%20of%20the%20USV%20tracking%20the%20desired%20path%20while%20ensuring%20the%0Asafety%20of%20the%20USV%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17448v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Line-Of-Sight%20guidance%20law%20based%20on%20vector%20fields%20path%0A%20%20following%20for%20underactuated%20unmanned%20surface%20vehicle&entry.906535625=Jie%20Qi%20and%20Ronghua%20Wanga%20and%20Nailong%20Wu&entry.1292438233=%20%20The%20focus%20of%20this%20paper%20is%20to%20develop%20a%20methodology%20that%20enables%20an%20unmanned%0Asurface%20vehicle%20%28USV%29%20to%20efficiently%20track%20a%20planned%20path.%20The%20introduction%20of%0Aa%20vector%20field-based%20adaptive%20line%20of-sight%20guidance%20law%20%28VFALOS%29%20for%20accurate%0Atrajectory%20tracking%20and%20minimizing%20the%20overshoot%20response%20time%20during%20USV%0Atracking%20of%20curved%20paths%20improves%20the%20overall%20line-of-sight%20%28LOS%29%20guidance%0Amethod.%20These%20improvements%20contribute%20to%20faster%20convergence%20to%20the%20desired%0Apath%2C%20reduce%20oscillations%2C%20and%20can%20mitigate%20the%20effects%20of%20persistent%20external%0Adisturbances.%20It%20is%20shown%20that%20the%20proposed%20guidance%20law%20exhibits%20k-exponential%0Astability%20when%20converging%20to%20the%20desired%20path%20consisting%20of%20straight%20and%20curved%0Alines.%20The%20results%20in%20the%20paper%20show%20that%20the%20proposed%20method%20effectively%0Aimproves%20the%20accuracy%20of%20the%20USV%20tracking%20the%20desired%20path%20while%20ensuring%20the%0Asafety%20of%20the%20USV%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17448v2&entry.124074799=Read"},
{"title": "Contrastive UCB: Provably Efficient Contrastive Self-Supervised Learning\n  in Online Reinforcement Learning", "author": "Shuang Qiu and Lingxiao Wang and Chenjia Bai and Zhuoran Yang and Zhaoran Wang", "abstract": "  In view of its power in extracting feature representation, contrastive\nself-supervised learning has been successfully integrated into the practice of\n(deep) reinforcement learning (RL), leading to efficient policy learning in\nvarious applications. Despite its tremendous empirical successes, the\nunderstanding of contrastive learning for RL remains elusive. To narrow such a\ngap, we study how RL can be empowered by contrastive learning in a class of\nMarkov decision processes (MDPs) and Markov games (MGs) with low-rank\ntransitions. For both models, we propose to extract the correct feature\nrepresentations of the low-rank model by minimizing a contrastive loss.\nMoreover, under the online setting, we propose novel upper confidence bound\n(UCB)-type algorithms that incorporate such a contrastive loss with online RL\nalgorithms for MDPs or MGs. We further theoretically prove that our algorithm\nrecovers the true representations and simultaneously achieves sample efficiency\nin learning the optimal policy and Nash equilibrium in MDPs and MGs. We also\nprovide empirical studies to demonstrate the efficacy of the UCB-based\ncontrastive learning method for RL. To the best of our knowledge, we provide\nthe first provably efficient online RL algorithm that incorporates contrastive\nlearning for representation learning. Our codes are available at\nhttps://github.com/Baichenjia/Contrastive-UCB.\n", "link": "http://arxiv.org/abs/2207.14800v2", "date": "2024-04-05", "relevancy": 1.9929, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5034}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4936}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contrastive%20UCB%3A%20Provably%20Efficient%20Contrastive%20Self-Supervised%20Learning%0A%20%20in%20Online%20Reinforcement%20Learning&body=Title%3A%20Contrastive%20UCB%3A%20Provably%20Efficient%20Contrastive%20Self-Supervised%20Learning%0A%20%20in%20Online%20Reinforcement%20Learning%0AAuthor%3A%20Shuang%20Qiu%20and%20Lingxiao%20Wang%20and%20Chenjia%20Bai%20and%20Zhuoran%20Yang%20and%20Zhaoran%20Wang%0AAbstract%3A%20%20%20In%20view%20of%20its%20power%20in%20extracting%20feature%20representation%2C%20contrastive%0Aself-supervised%20learning%20has%20been%20successfully%20integrated%20into%20the%20practice%20of%0A%28deep%29%20reinforcement%20learning%20%28RL%29%2C%20leading%20to%20efficient%20policy%20learning%20in%0Avarious%20applications.%20Despite%20its%20tremendous%20empirical%20successes%2C%20the%0Aunderstanding%20of%20contrastive%20learning%20for%20RL%20remains%20elusive.%20To%20narrow%20such%20a%0Agap%2C%20we%20study%20how%20RL%20can%20be%20empowered%20by%20contrastive%20learning%20in%20a%20class%20of%0AMarkov%20decision%20processes%20%28MDPs%29%20and%20Markov%20games%20%28MGs%29%20with%20low-rank%0Atransitions.%20For%20both%20models%2C%20we%20propose%20to%20extract%20the%20correct%20feature%0Arepresentations%20of%20the%20low-rank%20model%20by%20minimizing%20a%20contrastive%20loss.%0AMoreover%2C%20under%20the%20online%20setting%2C%20we%20propose%20novel%20upper%20confidence%20bound%0A%28UCB%29-type%20algorithms%20that%20incorporate%20such%20a%20contrastive%20loss%20with%20online%20RL%0Aalgorithms%20for%20MDPs%20or%20MGs.%20We%20further%20theoretically%20prove%20that%20our%20algorithm%0Arecovers%20the%20true%20representations%20and%20simultaneously%20achieves%20sample%20efficiency%0Ain%20learning%20the%20optimal%20policy%20and%20Nash%20equilibrium%20in%20MDPs%20and%20MGs.%20We%20also%0Aprovide%20empirical%20studies%20to%20demonstrate%20the%20efficacy%20of%20the%20UCB-based%0Acontrastive%20learning%20method%20for%20RL.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20provide%0Athe%20first%20provably%20efficient%20online%20RL%20algorithm%20that%20incorporates%20contrastive%0Alearning%20for%20representation%20learning.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/Baichenjia/Contrastive-UCB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.14800v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20UCB%3A%20Provably%20Efficient%20Contrastive%20Self-Supervised%20Learning%0A%20%20in%20Online%20Reinforcement%20Learning&entry.906535625=Shuang%20Qiu%20and%20Lingxiao%20Wang%20and%20Chenjia%20Bai%20and%20Zhuoran%20Yang%20and%20Zhaoran%20Wang&entry.1292438233=%20%20In%20view%20of%20its%20power%20in%20extracting%20feature%20representation%2C%20contrastive%0Aself-supervised%20learning%20has%20been%20successfully%20integrated%20into%20the%20practice%20of%0A%28deep%29%20reinforcement%20learning%20%28RL%29%2C%20leading%20to%20efficient%20policy%20learning%20in%0Avarious%20applications.%20Despite%20its%20tremendous%20empirical%20successes%2C%20the%0Aunderstanding%20of%20contrastive%20learning%20for%20RL%20remains%20elusive.%20To%20narrow%20such%20a%0Agap%2C%20we%20study%20how%20RL%20can%20be%20empowered%20by%20contrastive%20learning%20in%20a%20class%20of%0AMarkov%20decision%20processes%20%28MDPs%29%20and%20Markov%20games%20%28MGs%29%20with%20low-rank%0Atransitions.%20For%20both%20models%2C%20we%20propose%20to%20extract%20the%20correct%20feature%0Arepresentations%20of%20the%20low-rank%20model%20by%20minimizing%20a%20contrastive%20loss.%0AMoreover%2C%20under%20the%20online%20setting%2C%20we%20propose%20novel%20upper%20confidence%20bound%0A%28UCB%29-type%20algorithms%20that%20incorporate%20such%20a%20contrastive%20loss%20with%20online%20RL%0Aalgorithms%20for%20MDPs%20or%20MGs.%20We%20further%20theoretically%20prove%20that%20our%20algorithm%0Arecovers%20the%20true%20representations%20and%20simultaneously%20achieves%20sample%20efficiency%0Ain%20learning%20the%20optimal%20policy%20and%20Nash%20equilibrium%20in%20MDPs%20and%20MGs.%20We%20also%0Aprovide%20empirical%20studies%20to%20demonstrate%20the%20efficacy%20of%20the%20UCB-based%0Acontrastive%20learning%20method%20for%20RL.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20provide%0Athe%20first%20provably%20efficient%20online%20RL%20algorithm%20that%20incorporates%20contrastive%0Alearning%20for%20representation%20learning.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/Baichenjia/Contrastive-UCB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.14800v2&entry.124074799=Read"},
{"title": "Prompt-based Pseudo-labeling Strategy for Sample-Efficient\n  Semi-Supervised Extractive Summarization", "author": "Gaurav Sahu and Olga Vechtomova and Issam H. Laradji", "abstract": "  Semi-supervised learning (SSL) is a widely used technique in scenarios where\nlabeled data is scarce and unlabeled data is abundant. While SSL is popular for\nimage and text classification, it is relatively underexplored for the task of\nextractive text summarization. Standard SSL methods follow a teacher-student\nparadigm to first train a classification model and then use the classifier's\nconfidence values to select pseudo-labels for the subsequent training cycle;\nhowever, such classifiers are not suitable to measure the accuracy of\npseudo-labels as they lack specific tuning for evaluation, which leads to\nconfidence values that fail to capture the semantics and correctness of the\ngenerated summary. To address this problem, we propose a prompt-based\npseudo-labeling strategy with LLMs that picks unlabeled examples with more\naccurate pseudo-labels than using just the classifier's probability outputs.\nOur approach also includes a relabeling mechanism that improves the quality of\npseudo-labels. We evaluate our method on three text summarization datasets:\nTweetSumm, WikiHow, and ArXiv/PubMed. We empirically show that a\nprompting-based LLM that scores and generates pseudo-labels outperforms\nexisting SSL methods on ROUGE-1, ROUGE-2, and ROUGE-L scores on all the\ndatasets. Furthermore, our method achieves competitive G-Eval scores\n(evaluation with GPT-4) as a fully supervised method that uses 100% of the\nlabeled data with only 16.67% of the labeled data.\n", "link": "http://arxiv.org/abs/2311.09559v2", "date": "2024-04-05", "relevancy": 1.9905, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5139}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5063}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4779}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Prompt-based%20Pseudo-labeling%20Strategy%20for%20Sample-Efficient%0A%20%20Semi-Supervised%20Extractive%20Summarization&body=Title%3A%20Prompt-based%20Pseudo-labeling%20Strategy%20for%20Sample-Efficient%0A%20%20Semi-Supervised%20Extractive%20Summarization%0AAuthor%3A%20Gaurav%20Sahu%20and%20Olga%20Vechtomova%20and%20Issam%20H.%20Laradji%0AAbstract%3A%20%20%20Semi-supervised%20learning%20%28SSL%29%20is%20a%20widely%20used%20technique%20in%20scenarios%20where%0Alabeled%20data%20is%20scarce%20and%20unlabeled%20data%20is%20abundant.%20While%20SSL%20is%20popular%20for%0Aimage%20and%20text%20classification%2C%20it%20is%20relatively%20underexplored%20for%20the%20task%20of%0Aextractive%20text%20summarization.%20Standard%20SSL%20methods%20follow%20a%20teacher-student%0Aparadigm%20to%20first%20train%20a%20classification%20model%20and%20then%20use%20the%20classifier%27s%0Aconfidence%20values%20to%20select%20pseudo-labels%20for%20the%20subsequent%20training%20cycle%3B%0Ahowever%2C%20such%20classifiers%20are%20not%20suitable%20to%20measure%20the%20accuracy%20of%0Apseudo-labels%20as%20they%20lack%20specific%20tuning%20for%20evaluation%2C%20which%20leads%20to%0Aconfidence%20values%20that%20fail%20to%20capture%20the%20semantics%20and%20correctness%20of%20the%0Agenerated%20summary.%20To%20address%20this%20problem%2C%20we%20propose%20a%20prompt-based%0Apseudo-labeling%20strategy%20with%20LLMs%20that%20picks%20unlabeled%20examples%20with%20more%0Aaccurate%20pseudo-labels%20than%20using%20just%20the%20classifier%27s%20probability%20outputs.%0AOur%20approach%20also%20includes%20a%20relabeling%20mechanism%20that%20improves%20the%20quality%20of%0Apseudo-labels.%20We%20evaluate%20our%20method%20on%20three%20text%20summarization%20datasets%3A%0ATweetSumm%2C%20WikiHow%2C%20and%20ArXiv/PubMed.%20We%20empirically%20show%20that%20a%0Aprompting-based%20LLM%20that%20scores%20and%20generates%20pseudo-labels%20outperforms%0Aexisting%20SSL%20methods%20on%20ROUGE-1%2C%20ROUGE-2%2C%20and%20ROUGE-L%20scores%20on%20all%20the%0Adatasets.%20Furthermore%2C%20our%20method%20achieves%20competitive%20G-Eval%20scores%0A%28evaluation%20with%20GPT-4%29%20as%20a%20fully%20supervised%20method%20that%20uses%20100%25%20of%20the%0Alabeled%20data%20with%20only%2016.67%25%20of%20the%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09559v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-based%20Pseudo-labeling%20Strategy%20for%20Sample-Efficient%0A%20%20Semi-Supervised%20Extractive%20Summarization&entry.906535625=Gaurav%20Sahu%20and%20Olga%20Vechtomova%20and%20Issam%20H.%20Laradji&entry.1292438233=%20%20Semi-supervised%20learning%20%28SSL%29%20is%20a%20widely%20used%20technique%20in%20scenarios%20where%0Alabeled%20data%20is%20scarce%20and%20unlabeled%20data%20is%20abundant.%20While%20SSL%20is%20popular%20for%0Aimage%20and%20text%20classification%2C%20it%20is%20relatively%20underexplored%20for%20the%20task%20of%0Aextractive%20text%20summarization.%20Standard%20SSL%20methods%20follow%20a%20teacher-student%0Aparadigm%20to%20first%20train%20a%20classification%20model%20and%20then%20use%20the%20classifier%27s%0Aconfidence%20values%20to%20select%20pseudo-labels%20for%20the%20subsequent%20training%20cycle%3B%0Ahowever%2C%20such%20classifiers%20are%20not%20suitable%20to%20measure%20the%20accuracy%20of%0Apseudo-labels%20as%20they%20lack%20specific%20tuning%20for%20evaluation%2C%20which%20leads%20to%0Aconfidence%20values%20that%20fail%20to%20capture%20the%20semantics%20and%20correctness%20of%20the%0Agenerated%20summary.%20To%20address%20this%20problem%2C%20we%20propose%20a%20prompt-based%0Apseudo-labeling%20strategy%20with%20LLMs%20that%20picks%20unlabeled%20examples%20with%20more%0Aaccurate%20pseudo-labels%20than%20using%20just%20the%20classifier%27s%20probability%20outputs.%0AOur%20approach%20also%20includes%20a%20relabeling%20mechanism%20that%20improves%20the%20quality%20of%0Apseudo-labels.%20We%20evaluate%20our%20method%20on%20three%20text%20summarization%20datasets%3A%0ATweetSumm%2C%20WikiHow%2C%20and%20ArXiv/PubMed.%20We%20empirically%20show%20that%20a%0Aprompting-based%20LLM%20that%20scores%20and%20generates%20pseudo-labels%20outperforms%0Aexisting%20SSL%20methods%20on%20ROUGE-1%2C%20ROUGE-2%2C%20and%20ROUGE-L%20scores%20on%20all%20the%0Adatasets.%20Furthermore%2C%20our%20method%20achieves%20competitive%20G-Eval%20scores%0A%28evaluation%20with%20GPT-4%29%20as%20a%20fully%20supervised%20method%20that%20uses%20100%25%20of%20the%0Alabeled%20data%20with%20only%2016.67%25%20of%20the%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09559v2&entry.124074799=Read"},
{"title": "The Unreasonable Effectiveness Of Early Discarding After One Epoch In\n  Neural Network Hyperparameter Optimization", "author": "Romain Egele and Felix Mohr and Tom Viering and Prasanna Balaprakash", "abstract": "  To reach high performance with deep learning, hyperparameter optimization\n(HPO) is essential. This process is usually time-consuming due to costly\nevaluations of neural networks. Early discarding techniques limit the resources\ngranted to unpromising candidates by observing the empirical learning curves\nand canceling neural network training as soon as the lack of competitiveness of\na candidate becomes evident. Despite two decades of research, little is\nunderstood about the trade-off between the aggressiveness of discarding and the\nloss of predictive performance. Our paper studies this trade-off for several\ncommonly used discarding techniques such as successive halving and learning\ncurve extrapolation. Our surprising finding is that these commonly used\ntechniques offer minimal to no added value compared to the simple strategy of\ndiscarding after a constant number of epochs of training. The chosen number of\nepochs depends mostly on the available compute budget. We call this approach\ni-Epoch (i being the constant number of epochs with which neural networks are\ntrained) and suggest to assess the quality of early discarding techniques by\ncomparing how their Pareto-Front (in consumed training epochs and predictive\nperformance) complement the Pareto-Front of i-Epoch.\n", "link": "http://arxiv.org/abs/2404.04111v1", "date": "2024-04-05", "relevancy": 1.9867, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5342}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4819}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4398}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Unreasonable%20Effectiveness%20Of%20Early%20Discarding%20After%20One%20Epoch%20In%0A%20%20Neural%20Network%20Hyperparameter%20Optimization&body=Title%3A%20The%20Unreasonable%20Effectiveness%20Of%20Early%20Discarding%20After%20One%20Epoch%20In%0A%20%20Neural%20Network%20Hyperparameter%20Optimization%0AAuthor%3A%20Romain%20Egele%20and%20Felix%20Mohr%20and%20Tom%20Viering%20and%20Prasanna%20Balaprakash%0AAbstract%3A%20%20%20To%20reach%20high%20performance%20with%20deep%20learning%2C%20hyperparameter%20optimization%0A%28HPO%29%20is%20essential.%20This%20process%20is%20usually%20time-consuming%20due%20to%20costly%0Aevaluations%20of%20neural%20networks.%20Early%20discarding%20techniques%20limit%20the%20resources%0Agranted%20to%20unpromising%20candidates%20by%20observing%20the%20empirical%20learning%20curves%0Aand%20canceling%20neural%20network%20training%20as%20soon%20as%20the%20lack%20of%20competitiveness%20of%0Aa%20candidate%20becomes%20evident.%20Despite%20two%20decades%20of%20research%2C%20little%20is%0Aunderstood%20about%20the%20trade-off%20between%20the%20aggressiveness%20of%20discarding%20and%20the%0Aloss%20of%20predictive%20performance.%20Our%20paper%20studies%20this%20trade-off%20for%20several%0Acommonly%20used%20discarding%20techniques%20such%20as%20successive%20halving%20and%20learning%0Acurve%20extrapolation.%20Our%20surprising%20finding%20is%20that%20these%20commonly%20used%0Atechniques%20offer%20minimal%20to%20no%20added%20value%20compared%20to%20the%20simple%20strategy%20of%0Adiscarding%20after%20a%20constant%20number%20of%20epochs%20of%20training.%20The%20chosen%20number%20of%0Aepochs%20depends%20mostly%20on%20the%20available%20compute%20budget.%20We%20call%20this%20approach%0Ai-Epoch%20%28i%20being%20the%20constant%20number%20of%20epochs%20with%20which%20neural%20networks%20are%0Atrained%29%20and%20suggest%20to%20assess%20the%20quality%20of%20early%20discarding%20techniques%20by%0Acomparing%20how%20their%20Pareto-Front%20%28in%20consumed%20training%20epochs%20and%20predictive%0Aperformance%29%20complement%20the%20Pareto-Front%20of%20i-Epoch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04111v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unreasonable%20Effectiveness%20Of%20Early%20Discarding%20After%20One%20Epoch%20In%0A%20%20Neural%20Network%20Hyperparameter%20Optimization&entry.906535625=Romain%20Egele%20and%20Felix%20Mohr%20and%20Tom%20Viering%20and%20Prasanna%20Balaprakash&entry.1292438233=%20%20To%20reach%20high%20performance%20with%20deep%20learning%2C%20hyperparameter%20optimization%0A%28HPO%29%20is%20essential.%20This%20process%20is%20usually%20time-consuming%20due%20to%20costly%0Aevaluations%20of%20neural%20networks.%20Early%20discarding%20techniques%20limit%20the%20resources%0Agranted%20to%20unpromising%20candidates%20by%20observing%20the%20empirical%20learning%20curves%0Aand%20canceling%20neural%20network%20training%20as%20soon%20as%20the%20lack%20of%20competitiveness%20of%0Aa%20candidate%20becomes%20evident.%20Despite%20two%20decades%20of%20research%2C%20little%20is%0Aunderstood%20about%20the%20trade-off%20between%20the%20aggressiveness%20of%20discarding%20and%20the%0Aloss%20of%20predictive%20performance.%20Our%20paper%20studies%20this%20trade-off%20for%20several%0Acommonly%20used%20discarding%20techniques%20such%20as%20successive%20halving%20and%20learning%0Acurve%20extrapolation.%20Our%20surprising%20finding%20is%20that%20these%20commonly%20used%0Atechniques%20offer%20minimal%20to%20no%20added%20value%20compared%20to%20the%20simple%20strategy%20of%0Adiscarding%20after%20a%20constant%20number%20of%20epochs%20of%20training.%20The%20chosen%20number%20of%0Aepochs%20depends%20mostly%20on%20the%20available%20compute%20budget.%20We%20call%20this%20approach%0Ai-Epoch%20%28i%20being%20the%20constant%20number%20of%20epochs%20with%20which%20neural%20networks%20are%0Atrained%29%20and%20suggest%20to%20assess%20the%20quality%20of%20early%20discarding%20techniques%20by%0Acomparing%20how%20their%20Pareto-Front%20%28in%20consumed%20training%20epochs%20and%20predictive%0Aperformance%29%20complement%20the%20Pareto-Front%20of%20i-Epoch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04111v1&entry.124074799=Read"},
{"title": "CLUE: A Clinical Language Understanding Evaluation for LLMs", "author": "Amin Dada and Marie Bauer and Amanda Butler Contreras and Osman Alperen Kora\u015f and Constantin Marc Seibold and Kaleb E Smith and Jens Kleesiek", "abstract": "  Large Language Models (LLMs) have shown the potential to significantly\ncontribute to patient care, diagnostics, and administrative processes. Emerging\nbiomedical LLMs address healthcare-specific challenges, including privacy\ndemands and computational constraints. However, evaluation of these models has\nprimarily been limited to non-clinical tasks, which do not reflect the\ncomplexity of practical clinical applications. Additionally, there has been no\nthorough comparison between biomedical and general-domain LLMs for clinical\ntasks. To fill this gap, we present the Clinical Language Understanding\nEvaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical\ntasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters\nand four existing tasks designed to test the practical applicability of LLMs in\nhealthcare settings. Our evaluation covers several biomedical and general\ndomain LLMs, providing insights into their clinical performance and\napplicability. CLUE represents a step towards a standardized approach to\nevaluating and developing LLMs in healthcare to align future model development\nwith the real-world needs of clinical application. We publish our evaluation\nand data generation scripts: https://github.com/dadaamin/CLUE\n", "link": "http://arxiv.org/abs/2404.04067v1", "date": "2024-04-05", "relevancy": 1.9604, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5444}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5129}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs&body=Title%3A%20CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs%0AAuthor%3A%20Amin%20Dada%20and%20Marie%20Bauer%20and%20Amanda%20Butler%20Contreras%20and%20Osman%20Alperen%20Kora%C5%9F%20and%20Constantin%20Marc%20Seibold%20and%20Kaleb%20E%20Smith%20and%20Jens%20Kleesiek%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20the%20potential%20to%20significantly%0Acontribute%20to%20patient%20care%2C%20diagnostics%2C%20and%20administrative%20processes.%20Emerging%0Abiomedical%20LLMs%20address%20healthcare-specific%20challenges%2C%20including%20privacy%0Ademands%20and%20computational%20constraints.%20However%2C%20evaluation%20of%20these%20models%20has%0Aprimarily%20been%20limited%20to%20non-clinical%20tasks%2C%20which%20do%20not%20reflect%20the%0Acomplexity%20of%20practical%20clinical%20applications.%20Additionally%2C%20there%20has%20been%20no%0Athorough%20comparison%20between%20biomedical%20and%20general-domain%20LLMs%20for%20clinical%0Atasks.%20To%20fill%20this%20gap%2C%20we%20present%20the%20Clinical%20Language%20Understanding%0AEvaluation%20%28CLUE%29%2C%20a%20benchmark%20tailored%20to%20evaluate%20LLMs%20on%20real-world%20clinical%0Atasks.%20CLUE%20includes%20two%20novel%20datasets%20derived%20from%20MIMIC%20IV%20discharge%20letters%0Aand%20four%20existing%20tasks%20designed%20to%20test%20the%20practical%20applicability%20of%20LLMs%20in%0Ahealthcare%20settings.%20Our%20evaluation%20covers%20several%20biomedical%20and%20general%0Adomain%20LLMs%2C%20providing%20insights%20into%20their%20clinical%20performance%20and%0Aapplicability.%20CLUE%20represents%20a%20step%20towards%20a%20standardized%20approach%20to%0Aevaluating%20and%20developing%20LLMs%20in%20healthcare%20to%20align%20future%20model%20development%0Awith%20the%20real-world%20needs%20of%20clinical%20application.%20We%20publish%20our%20evaluation%0Aand%20data%20generation%20scripts%3A%20https%3A//github.com/dadaamin/CLUE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04067v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs&entry.906535625=Amin%20Dada%20and%20Marie%20Bauer%20and%20Amanda%20Butler%20Contreras%20and%20Osman%20Alperen%20Kora%C5%9F%20and%20Constantin%20Marc%20Seibold%20and%20Kaleb%20E%20Smith%20and%20Jens%20Kleesiek&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20the%20potential%20to%20significantly%0Acontribute%20to%20patient%20care%2C%20diagnostics%2C%20and%20administrative%20processes.%20Emerging%0Abiomedical%20LLMs%20address%20healthcare-specific%20challenges%2C%20including%20privacy%0Ademands%20and%20computational%20constraints.%20However%2C%20evaluation%20of%20these%20models%20has%0Aprimarily%20been%20limited%20to%20non-clinical%20tasks%2C%20which%20do%20not%20reflect%20the%0Acomplexity%20of%20practical%20clinical%20applications.%20Additionally%2C%20there%20has%20been%20no%0Athorough%20comparison%20between%20biomedical%20and%20general-domain%20LLMs%20for%20clinical%0Atasks.%20To%20fill%20this%20gap%2C%20we%20present%20the%20Clinical%20Language%20Understanding%0AEvaluation%20%28CLUE%29%2C%20a%20benchmark%20tailored%20to%20evaluate%20LLMs%20on%20real-world%20clinical%0Atasks.%20CLUE%20includes%20two%20novel%20datasets%20derived%20from%20MIMIC%20IV%20discharge%20letters%0Aand%20four%20existing%20tasks%20designed%20to%20test%20the%20practical%20applicability%20of%20LLMs%20in%0Ahealthcare%20settings.%20Our%20evaluation%20covers%20several%20biomedical%20and%20general%0Adomain%20LLMs%2C%20providing%20insights%20into%20their%20clinical%20performance%20and%0Aapplicability.%20CLUE%20represents%20a%20step%20towards%20a%20standardized%20approach%20to%0Aevaluating%20and%20developing%20LLMs%20in%20healthcare%20to%20align%20future%20model%20development%0Awith%20the%20real-world%20needs%20of%20clinical%20application.%20We%20publish%20our%20evaluation%0Aand%20data%20generation%20scripts%3A%20https%3A//github.com/dadaamin/CLUE%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04067v1&entry.124074799=Read"},
{"title": "Which Augmentation Should I Use? An Empirical Investigation of\n  Augmentations for Self-Supervised Phonocardiogram Representation Learning", "author": "Aristotelis Ballas and Vasileios Papapanagiotou and Christos Diou", "abstract": "  Despite the recent increase in research activity, deep-learning models have\nnot yet been widely accepted in several real-world settings, such as medicine.\nThe shortage of high-quality annotated data often hinders the development of\nrobust and generalizable models, which do not suffer from degraded\neffectiveness when presented with out-of-distribution (OOD) datasets.\nContrastive Self-Supervised Learning (SSL) offers a potential solution to\nlabeled data scarcity, as it takes advantage of unlabeled data to increase\nmodel effectiveness and robustness. However, the selection of appropriate\ntransformations during the learning process is not a trivial task and even\nbreaks down the ability of the network to extract meaningful information. In\nthis research, we propose uncovering the optimal augmentations for applying\ncontrastive learning in 1D phonocardiogram (PCG) classification. We perform an\nextensive comparative evaluation of a wide range of audio-based augmentations,\nevaluate models on multiple datasets across downstream tasks, and report on the\nimpact of each augmentation. We demonstrate that depending on its training\ndistribution, the effectiveness of a fully-supervised model can degrade up to\n32%, while SSL models only lose up to 10% or even improve in some cases. We\nargue and experimentally demonstrate that, contrastive SSL pretraining can\nassist in providing robust classifiers which can generalize to unseen, OOD\ndata, without relying on time- and labor-intensive annotation processes by\nmedical experts. Furthermore, the proposed evaluation protocol sheds light on\nthe most promising and appropriate augmentations for robust PCG signal\nprocessing, by calculating their effect size on model training. Finally, we\nprovide researchers and practitioners with a roadmap towards producing robust\nmodels for PCG classification, in addition to an open-source codebase for\ndeveloping novel approaches.\n", "link": "http://arxiv.org/abs/2312.00502v3", "date": "2024-04-05", "relevancy": 1.9544, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5082}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.475}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4744}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Which%20Augmentation%20Should%20I%20Use%3F%20An%20Empirical%20Investigation%20of%0A%20%20Augmentations%20for%20Self-Supervised%20Phonocardiogram%20Representation%20Learning&body=Title%3A%20Which%20Augmentation%20Should%20I%20Use%3F%20An%20Empirical%20Investigation%20of%0A%20%20Augmentations%20for%20Self-Supervised%20Phonocardiogram%20Representation%20Learning%0AAuthor%3A%20Aristotelis%20Ballas%20and%20Vasileios%20Papapanagiotou%20and%20Christos%20Diou%0AAbstract%3A%20%20%20Despite%20the%20recent%20increase%20in%20research%20activity%2C%20deep-learning%20models%20have%0Anot%20yet%20been%20widely%20accepted%20in%20several%20real-world%20settings%2C%20such%20as%20medicine.%0AThe%20shortage%20of%20high-quality%20annotated%20data%20often%20hinders%20the%20development%20of%0Arobust%20and%20generalizable%20models%2C%20which%20do%20not%20suffer%20from%20degraded%0Aeffectiveness%20when%20presented%20with%20out-of-distribution%20%28OOD%29%20datasets.%0AContrastive%20Self-Supervised%20Learning%20%28SSL%29%20offers%20a%20potential%20solution%20to%0Alabeled%20data%20scarcity%2C%20as%20it%20takes%20advantage%20of%20unlabeled%20data%20to%20increase%0Amodel%20effectiveness%20and%20robustness.%20However%2C%20the%20selection%20of%20appropriate%0Atransformations%20during%20the%20learning%20process%20is%20not%20a%20trivial%20task%20and%20even%0Abreaks%20down%20the%20ability%20of%20the%20network%20to%20extract%20meaningful%20information.%20In%0Athis%20research%2C%20we%20propose%20uncovering%20the%20optimal%20augmentations%20for%20applying%0Acontrastive%20learning%20in%201D%20phonocardiogram%20%28PCG%29%20classification.%20We%20perform%20an%0Aextensive%20comparative%20evaluation%20of%20a%20wide%20range%20of%20audio-based%20augmentations%2C%0Aevaluate%20models%20on%20multiple%20datasets%20across%20downstream%20tasks%2C%20and%20report%20on%20the%0Aimpact%20of%20each%20augmentation.%20We%20demonstrate%20that%20depending%20on%20its%20training%0Adistribution%2C%20the%20effectiveness%20of%20a%20fully-supervised%20model%20can%20degrade%20up%20to%0A32%25%2C%20while%20SSL%20models%20only%20lose%20up%20to%2010%25%20or%20even%20improve%20in%20some%20cases.%20We%0Aargue%20and%20experimentally%20demonstrate%20that%2C%20contrastive%20SSL%20pretraining%20can%0Aassist%20in%20providing%20robust%20classifiers%20which%20can%20generalize%20to%20unseen%2C%20OOD%0Adata%2C%20without%20relying%20on%20time-%20and%20labor-intensive%20annotation%20processes%20by%0Amedical%20experts.%20Furthermore%2C%20the%20proposed%20evaluation%20protocol%20sheds%20light%20on%0Athe%20most%20promising%20and%20appropriate%20augmentations%20for%20robust%20PCG%20signal%0Aprocessing%2C%20by%20calculating%20their%20effect%20size%20on%20model%20training.%20Finally%2C%20we%0Aprovide%20researchers%20and%20practitioners%20with%20a%20roadmap%20towards%20producing%20robust%0Amodels%20for%20PCG%20classification%2C%20in%20addition%20to%20an%20open-source%20codebase%20for%0Adeveloping%20novel%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00502v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Which%20Augmentation%20Should%20I%20Use%3F%20An%20Empirical%20Investigation%20of%0A%20%20Augmentations%20for%20Self-Supervised%20Phonocardiogram%20Representation%20Learning&entry.906535625=Aristotelis%20Ballas%20and%20Vasileios%20Papapanagiotou%20and%20Christos%20Diou&entry.1292438233=%20%20Despite%20the%20recent%20increase%20in%20research%20activity%2C%20deep-learning%20models%20have%0Anot%20yet%20been%20widely%20accepted%20in%20several%20real-world%20settings%2C%20such%20as%20medicine.%0AThe%20shortage%20of%20high-quality%20annotated%20data%20often%20hinders%20the%20development%20of%0Arobust%20and%20generalizable%20models%2C%20which%20do%20not%20suffer%20from%20degraded%0Aeffectiveness%20when%20presented%20with%20out-of-distribution%20%28OOD%29%20datasets.%0AContrastive%20Self-Supervised%20Learning%20%28SSL%29%20offers%20a%20potential%20solution%20to%0Alabeled%20data%20scarcity%2C%20as%20it%20takes%20advantage%20of%20unlabeled%20data%20to%20increase%0Amodel%20effectiveness%20and%20robustness.%20However%2C%20the%20selection%20of%20appropriate%0Atransformations%20during%20the%20learning%20process%20is%20not%20a%20trivial%20task%20and%20even%0Abreaks%20down%20the%20ability%20of%20the%20network%20to%20extract%20meaningful%20information.%20In%0Athis%20research%2C%20we%20propose%20uncovering%20the%20optimal%20augmentations%20for%20applying%0Acontrastive%20learning%20in%201D%20phonocardiogram%20%28PCG%29%20classification.%20We%20perform%20an%0Aextensive%20comparative%20evaluation%20of%20a%20wide%20range%20of%20audio-based%20augmentations%2C%0Aevaluate%20models%20on%20multiple%20datasets%20across%20downstream%20tasks%2C%20and%20report%20on%20the%0Aimpact%20of%20each%20augmentation.%20We%20demonstrate%20that%20depending%20on%20its%20training%0Adistribution%2C%20the%20effectiveness%20of%20a%20fully-supervised%20model%20can%20degrade%20up%20to%0A32%25%2C%20while%20SSL%20models%20only%20lose%20up%20to%2010%25%20or%20even%20improve%20in%20some%20cases.%20We%0Aargue%20and%20experimentally%20demonstrate%20that%2C%20contrastive%20SSL%20pretraining%20can%0Aassist%20in%20providing%20robust%20classifiers%20which%20can%20generalize%20to%20unseen%2C%20OOD%0Adata%2C%20without%20relying%20on%20time-%20and%20labor-intensive%20annotation%20processes%20by%0Amedical%20experts.%20Furthermore%2C%20the%20proposed%20evaluation%20protocol%20sheds%20light%20on%0Athe%20most%20promising%20and%20appropriate%20augmentations%20for%20robust%20PCG%20signal%0Aprocessing%2C%20by%20calculating%20their%20effect%20size%20on%20model%20training.%20Finally%2C%20we%0Aprovide%20researchers%20and%20practitioners%20with%20a%20roadmap%20towards%20producing%20robust%0Amodels%20for%20PCG%20classification%2C%20in%20addition%20to%20an%20open-source%20codebase%20for%0Adeveloping%20novel%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00502v3&entry.124074799=Read"},
{"title": "Derivative-free tree optimization for complex systems", "author": "Ye Wei and Bo Peng and Ruiwen Xie and Yangtao Chen and Yu Qin and Peng Wen and Stefan Bauer and Po-Yen Tung", "abstract": "  A tremendous range of design tasks in materials, physics, and biology can be\nformulated as finding the optimum of an objective function depending on many\nparameters without knowing its closed-form expression or the derivative.\nTraditional derivative-free optimization techniques often rely on strong\nassumptions about objective functions, thereby failing at optimizing non-convex\nsystems beyond 100 dimensions. Here, we present a tree search method for\nderivative-free optimization that enables accelerated optimal design of\nhigh-dimensional complex systems. Specifically, we introduce stochastic tree\nexpansion, dynamic upper confidence bound, and short-range backpropagation\nmechanism to evade local optimum, iteratively approximating the global optimum\nusing machine learning models. This development effectively confronts the\ndimensionally challenging problems, achieving convergence to global optima\nacross various benchmark functions up to 2,000 dimensions, surpassing the\nexisting methods by 10- to 20-fold. Our method demonstrates wide applicability\nto a wide range of real-world complex systems spanning materials, physics, and\nbiology, considerably outperforming state-of-the-art algorithms. This enables\nefficient autonomous knowledge discovery and facilitates self-driving virtual\nlaboratories. Although we focus on problems within the realm of natural\nscience, the advancements in optimization techniques achieved herein are\napplicable to a broader spectrum of challenges across all quantitative\ndisciplines.\n", "link": "http://arxiv.org/abs/2404.04062v1", "date": "2024-04-05", "relevancy": 1.9481, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4942}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4909}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4783}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Derivative-free%20tree%20optimization%20for%20complex%20systems&body=Title%3A%20Derivative-free%20tree%20optimization%20for%20complex%20systems%0AAuthor%3A%20Ye%20Wei%20and%20Bo%20Peng%20and%20Ruiwen%20Xie%20and%20Yangtao%20Chen%20and%20Yu%20Qin%20and%20Peng%20Wen%20and%20Stefan%20Bauer%20and%20Po-Yen%20Tung%0AAbstract%3A%20%20%20A%20tremendous%20range%20of%20design%20tasks%20in%20materials%2C%20physics%2C%20and%20biology%20can%20be%0Aformulated%20as%20finding%20the%20optimum%20of%20an%20objective%20function%20depending%20on%20many%0Aparameters%20without%20knowing%20its%20closed-form%20expression%20or%20the%20derivative.%0ATraditional%20derivative-free%20optimization%20techniques%20often%20rely%20on%20strong%0Aassumptions%20about%20objective%20functions%2C%20thereby%20failing%20at%20optimizing%20non-convex%0Asystems%20beyond%20100%20dimensions.%20Here%2C%20we%20present%20a%20tree%20search%20method%20for%0Aderivative-free%20optimization%20that%20enables%20accelerated%20optimal%20design%20of%0Ahigh-dimensional%20complex%20systems.%20Specifically%2C%20we%20introduce%20stochastic%20tree%0Aexpansion%2C%20dynamic%20upper%20confidence%20bound%2C%20and%20short-range%20backpropagation%0Amechanism%20to%20evade%20local%20optimum%2C%20iteratively%20approximating%20the%20global%20optimum%0Ausing%20machine%20learning%20models.%20This%20development%20effectively%20confronts%20the%0Adimensionally%20challenging%20problems%2C%20achieving%20convergence%20to%20global%20optima%0Aacross%20various%20benchmark%20functions%20up%20to%202%2C000%20dimensions%2C%20surpassing%20the%0Aexisting%20methods%20by%2010-%20to%2020-fold.%20Our%20method%20demonstrates%20wide%20applicability%0Ato%20a%20wide%20range%20of%20real-world%20complex%20systems%20spanning%20materials%2C%20physics%2C%20and%0Abiology%2C%20considerably%20outperforming%20state-of-the-art%20algorithms.%20This%20enables%0Aefficient%20autonomous%20knowledge%20discovery%20and%20facilitates%20self-driving%20virtual%0Alaboratories.%20Although%20we%20focus%20on%20problems%20within%20the%20realm%20of%20natural%0Ascience%2C%20the%20advancements%20in%20optimization%20techniques%20achieved%20herein%20are%0Aapplicable%20to%20a%20broader%20spectrum%20of%20challenges%20across%20all%20quantitative%0Adisciplines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04062v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Derivative-free%20tree%20optimization%20for%20complex%20systems&entry.906535625=Ye%20Wei%20and%20Bo%20Peng%20and%20Ruiwen%20Xie%20and%20Yangtao%20Chen%20and%20Yu%20Qin%20and%20Peng%20Wen%20and%20Stefan%20Bauer%20and%20Po-Yen%20Tung&entry.1292438233=%20%20A%20tremendous%20range%20of%20design%20tasks%20in%20materials%2C%20physics%2C%20and%20biology%20can%20be%0Aformulated%20as%20finding%20the%20optimum%20of%20an%20objective%20function%20depending%20on%20many%0Aparameters%20without%20knowing%20its%20closed-form%20expression%20or%20the%20derivative.%0ATraditional%20derivative-free%20optimization%20techniques%20often%20rely%20on%20strong%0Aassumptions%20about%20objective%20functions%2C%20thereby%20failing%20at%20optimizing%20non-convex%0Asystems%20beyond%20100%20dimensions.%20Here%2C%20we%20present%20a%20tree%20search%20method%20for%0Aderivative-free%20optimization%20that%20enables%20accelerated%20optimal%20design%20of%0Ahigh-dimensional%20complex%20systems.%20Specifically%2C%20we%20introduce%20stochastic%20tree%0Aexpansion%2C%20dynamic%20upper%20confidence%20bound%2C%20and%20short-range%20backpropagation%0Amechanism%20to%20evade%20local%20optimum%2C%20iteratively%20approximating%20the%20global%20optimum%0Ausing%20machine%20learning%20models.%20This%20development%20effectively%20confronts%20the%0Adimensionally%20challenging%20problems%2C%20achieving%20convergence%20to%20global%20optima%0Aacross%20various%20benchmark%20functions%20up%20to%202%2C000%20dimensions%2C%20surpassing%20the%0Aexisting%20methods%20by%2010-%20to%2020-fold.%20Our%20method%20demonstrates%20wide%20applicability%0Ato%20a%20wide%20range%20of%20real-world%20complex%20systems%20spanning%20materials%2C%20physics%2C%20and%0Abiology%2C%20considerably%20outperforming%20state-of-the-art%20algorithms.%20This%20enables%0Aefficient%20autonomous%20knowledge%20discovery%20and%20facilitates%20self-driving%20virtual%0Alaboratories.%20Although%20we%20focus%20on%20problems%20within%20the%20realm%20of%20natural%0Ascience%2C%20the%20advancements%20in%20optimization%20techniques%20achieved%20herein%20are%0Aapplicable%20to%20a%20broader%20spectrum%20of%20challenges%20across%20all%20quantitative%0Adisciplines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04062v1&entry.124074799=Read"},
{"title": "Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models", "author": "Sangwon Jang and Jaehyeong Jo and Kimin Lee and Sung Ju Hwang", "abstract": "  Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.\n", "link": "http://arxiv.org/abs/2404.04243v1", "date": "2024-04-05", "relevancy": 1.9471, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6863}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6104}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Identity%20Decoupling%20for%20Multi-Subject%20Personalization%20of%20Text-to-Image%0A%20%20Models&body=Title%3A%20Identity%20Decoupling%20for%20Multi-Subject%20Personalization%20of%20Text-to-Image%0A%20%20Models%0AAuthor%3A%20Sangwon%20Jang%20and%20Jaehyeong%20Jo%20and%20Kimin%20Lee%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20shown%20remarkable%20success%20in%20generating%20a%0Apersonalized%20subject%20based%20on%20a%20few%20reference%20images.%20However%2C%20current%20methods%0Astruggle%20with%20handling%20multiple%20subjects%20simultaneously%2C%20often%20resulting%20in%0Amixed%20identities%20with%20combined%20attributes%20from%20different%20subjects.%20In%20this%0Awork%2C%20we%20present%20MuDI%2C%20a%20novel%20framework%20that%20enables%20multi-subject%0Apersonalization%20by%20effectively%20decoupling%20identities%20from%20multiple%20subjects.%0AOur%20main%20idea%20is%20to%20utilize%20segmented%20subjects%20generated%20by%20the%20Segment%0AAnything%20Model%20for%20both%20training%20and%20inference%2C%20as%20a%20form%20of%20data%20augmentation%0Afor%20training%20and%20initialization%20for%20the%20generation%20process.%20Our%20experiments%0Ademonstrate%20that%20MuDI%20can%20produce%20high-quality%20personalized%20images%20without%0Aidentity%20mixing%2C%20even%20for%20highly%20similar%20subjects%20as%20shown%20in%20Figure%201.%20In%0Ahuman%20evaluation%2C%20MuDI%20shows%20twice%20as%20many%20successes%20for%20personalizing%20multiple%0Asubjects%20without%20identity%20mixing%20over%20existing%20baselines%20and%20is%20preferred%20over%0A70%25%20compared%20to%20the%20strongest%20baseline.%20More%20results%20are%20available%20at%0Ahttps%3A//mudi-t2i.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04243v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity%20Decoupling%20for%20Multi-Subject%20Personalization%20of%20Text-to-Image%0A%20%20Models&entry.906535625=Sangwon%20Jang%20and%20Jaehyeong%20Jo%20and%20Kimin%20Lee%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20shown%20remarkable%20success%20in%20generating%20a%0Apersonalized%20subject%20based%20on%20a%20few%20reference%20images.%20However%2C%20current%20methods%0Astruggle%20with%20handling%20multiple%20subjects%20simultaneously%2C%20often%20resulting%20in%0Amixed%20identities%20with%20combined%20attributes%20from%20different%20subjects.%20In%20this%0Awork%2C%20we%20present%20MuDI%2C%20a%20novel%20framework%20that%20enables%20multi-subject%0Apersonalization%20by%20effectively%20decoupling%20identities%20from%20multiple%20subjects.%0AOur%20main%20idea%20is%20to%20utilize%20segmented%20subjects%20generated%20by%20the%20Segment%0AAnything%20Model%20for%20both%20training%20and%20inference%2C%20as%20a%20form%20of%20data%20augmentation%0Afor%20training%20and%20initialization%20for%20the%20generation%20process.%20Our%20experiments%0Ademonstrate%20that%20MuDI%20can%20produce%20high-quality%20personalized%20images%20without%0Aidentity%20mixing%2C%20even%20for%20highly%20similar%20subjects%20as%20shown%20in%20Figure%201.%20In%0Ahuman%20evaluation%2C%20MuDI%20shows%20twice%20as%20many%20successes%20for%20personalizing%20multiple%0Asubjects%20without%20identity%20mixing%20over%20existing%20baselines%20and%20is%20preferred%20over%0A70%25%20compared%20to%20the%20strongest%20baseline.%20More%20results%20are%20available%20at%0Ahttps%3A//mudi-t2i.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04243v1&entry.124074799=Read"},
{"title": "Pretraining Codomain Attention Neural Operators for Solving Multiphysics\n  PDEs", "author": "Md Ashiqur Rahman and Robert Joseph George and Mogab Elleithy and Daniel Leibovici and Zongyi Li and Boris Bonev and Colin White and Julius Berner and Raymond A. Yeh and Jean Kossaifi and Kamyar Azizzadenesheli and Anima Anandkumar", "abstract": "  Existing neural operator architectures face challenges when solving\nmultiphysics problems with coupled partial differential equations (PDEs), due\nto complex geometries, interactions between physical variables, and the lack of\nlarge amounts of high-resolution training data. To address these issues, we\npropose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions\nalong the codomain or channel space, enabling self-supervised learning or\npretraining of multiple PDE systems. Specifically, we extend positional\nencoding, self-attention, and normalization layers to the function space.\nCoDA-NO can learn representations of different PDE systems with a single model.\nWe evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs\nover multiple systems by considering few-shot learning settings. On complex\ndownstream tasks with limited data, such as fluid flow simulations and\nfluid-structure interactions, we found CoDA-NO to outperform existing methods\non the few-shot learning task by over $36\\%$. The code is available at\nhttps://github.com/ashiq24/CoDA-NO.\n", "link": "http://arxiv.org/abs/2403.12553v2", "date": "2024-04-05", "relevancy": 1.9451, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4986}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4848}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4828}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pretraining%20Codomain%20Attention%20Neural%20Operators%20for%20Solving%20Multiphysics%0A%20%20PDEs&body=Title%3A%20Pretraining%20Codomain%20Attention%20Neural%20Operators%20for%20Solving%20Multiphysics%0A%20%20PDEs%0AAuthor%3A%20Md%20Ashiqur%20Rahman%20and%20Robert%20Joseph%20George%20and%20Mogab%20Elleithy%20and%20Daniel%20Leibovici%20and%20Zongyi%20Li%20and%20Boris%20Bonev%20and%20Colin%20White%20and%20Julius%20Berner%20and%20Raymond%20A.%20Yeh%20and%20Jean%20Kossaifi%20and%20Kamyar%20Azizzadenesheli%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20Existing%20neural%20operator%20architectures%20face%20challenges%20when%20solving%0Amultiphysics%20problems%20with%20coupled%20partial%20differential%20equations%20%28PDEs%29%2C%20due%0Ato%20complex%20geometries%2C%20interactions%20between%20physical%20variables%2C%20and%20the%20lack%20of%0Alarge%20amounts%20of%20high-resolution%20training%20data.%20To%20address%20these%20issues%2C%20we%0Apropose%20Codomain%20Attention%20Neural%20Operator%20%28CoDA-NO%29%2C%20which%20tokenizes%20functions%0Aalong%20the%20codomain%20or%20channel%20space%2C%20enabling%20self-supervised%20learning%20or%0Apretraining%20of%20multiple%20PDE%20systems.%20Specifically%2C%20we%20extend%20positional%0Aencoding%2C%20self-attention%2C%20and%20normalization%20layers%20to%20the%20function%20space.%0ACoDA-NO%20can%20learn%20representations%20of%20different%20PDE%20systems%20with%20a%20single%20model.%0AWe%20evaluate%20CoDA-NO%27s%20potential%20as%20a%20backbone%20for%20learning%20multiphysics%20PDEs%0Aover%20multiple%20systems%20by%20considering%20few-shot%20learning%20settings.%20On%20complex%0Adownstream%20tasks%20with%20limited%20data%2C%20such%20as%20fluid%20flow%20simulations%20and%0Afluid-structure%20interactions%2C%20we%20found%20CoDA-NO%20to%20outperform%20existing%20methods%0Aon%20the%20few-shot%20learning%20task%20by%20over%20%2436%5C%25%24.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ashiq24/CoDA-NO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12553v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretraining%20Codomain%20Attention%20Neural%20Operators%20for%20Solving%20Multiphysics%0A%20%20PDEs&entry.906535625=Md%20Ashiqur%20Rahman%20and%20Robert%20Joseph%20George%20and%20Mogab%20Elleithy%20and%20Daniel%20Leibovici%20and%20Zongyi%20Li%20and%20Boris%20Bonev%20and%20Colin%20White%20and%20Julius%20Berner%20and%20Raymond%20A.%20Yeh%20and%20Jean%20Kossaifi%20and%20Kamyar%20Azizzadenesheli%20and%20Anima%20Anandkumar&entry.1292438233=%20%20Existing%20neural%20operator%20architectures%20face%20challenges%20when%20solving%0Amultiphysics%20problems%20with%20coupled%20partial%20differential%20equations%20%28PDEs%29%2C%20due%0Ato%20complex%20geometries%2C%20interactions%20between%20physical%20variables%2C%20and%20the%20lack%20of%0Alarge%20amounts%20of%20high-resolution%20training%20data.%20To%20address%20these%20issues%2C%20we%0Apropose%20Codomain%20Attention%20Neural%20Operator%20%28CoDA-NO%29%2C%20which%20tokenizes%20functions%0Aalong%20the%20codomain%20or%20channel%20space%2C%20enabling%20self-supervised%20learning%20or%0Apretraining%20of%20multiple%20PDE%20systems.%20Specifically%2C%20we%20extend%20positional%0Aencoding%2C%20self-attention%2C%20and%20normalization%20layers%20to%20the%20function%20space.%0ACoDA-NO%20can%20learn%20representations%20of%20different%20PDE%20systems%20with%20a%20single%20model.%0AWe%20evaluate%20CoDA-NO%27s%20potential%20as%20a%20backbone%20for%20learning%20multiphysics%20PDEs%0Aover%20multiple%20systems%20by%20considering%20few-shot%20learning%20settings.%20On%20complex%0Adownstream%20tasks%20with%20limited%20data%2C%20such%20as%20fluid%20flow%20simulations%20and%0Afluid-structure%20interactions%2C%20we%20found%20CoDA-NO%20to%20outperform%20existing%20methods%0Aon%20the%20few-shot%20learning%20task%20by%20over%20%2436%5C%25%24.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ashiq24/CoDA-NO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12553v2&entry.124074799=Read"},
{"title": "Intervention-Assisted Policy Gradient Methods for Online Stochastic\n  Queuing Network Optimization: Technical Report", "author": "Jerrod Wigmore and Brooke Shrader and Eytan Modiano", "abstract": "  Deep Reinforcement Learning (DRL) offers a powerful approach to training\nneural network control policies for stochastic queuing networks (SQN). However,\ntraditional DRL methods rely on offline simulations or static datasets,\nlimiting their real-world application in SQN control. This work proposes Online\nDeep Reinforcement Learning-based Controls (ODRLC) as an alternative, where an\nintelligent agent interacts directly with a real environment and learns an\noptimal control policy from these online interactions. SQNs present a challenge\nfor ODRLC due to the unbounded nature of the queues within the network\nresulting in an unbounded state-space. An unbounded state-space is particularly\nchallenging for neural network policies as neural networks are notoriously poor\nat extrapolating to unseen states. To address this challenge, we propose an\nintervention-assisted framework that leverages strategic interventions from\nknown stable policies to ensure the queue sizes remain bounded. This framework\ncombines the learning power of neural networks with the guaranteed stability of\nclassical control policies for SQNs. We introduce a method to design these\nintervention-assisted policies to ensure strong stability of the network.\nFurthermore, we extend foundational DRL theorems for intervention-assisted\npolicies and develop two practical algorithms specifically for ODRLC of SQNs.\nFinally, we demonstrate through experiments that our proposed algorithms\noutperform both classical control approaches and prior ODRLC algorithms.\n", "link": "http://arxiv.org/abs/2404.04106v1", "date": "2024-04-05", "relevancy": 1.9405, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4989}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Intervention-Assisted%20Policy%20Gradient%20Methods%20for%20Online%20Stochastic%0A%20%20Queuing%20Network%20Optimization%3A%20Technical%20Report&body=Title%3A%20Intervention-Assisted%20Policy%20Gradient%20Methods%20for%20Online%20Stochastic%0A%20%20Queuing%20Network%20Optimization%3A%20Technical%20Report%0AAuthor%3A%20Jerrod%20Wigmore%20and%20Brooke%20Shrader%20and%20Eytan%20Modiano%0AAbstract%3A%20%20%20Deep%20Reinforcement%20Learning%20%28DRL%29%20offers%20a%20powerful%20approach%20to%20training%0Aneural%20network%20control%20policies%20for%20stochastic%20queuing%20networks%20%28SQN%29.%20However%2C%0Atraditional%20DRL%20methods%20rely%20on%20offline%20simulations%20or%20static%20datasets%2C%0Alimiting%20their%20real-world%20application%20in%20SQN%20control.%20This%20work%20proposes%20Online%0ADeep%20Reinforcement%20Learning-based%20Controls%20%28ODRLC%29%20as%20an%20alternative%2C%20where%20an%0Aintelligent%20agent%20interacts%20directly%20with%20a%20real%20environment%20and%20learns%20an%0Aoptimal%20control%20policy%20from%20these%20online%20interactions.%20SQNs%20present%20a%20challenge%0Afor%20ODRLC%20due%20to%20the%20unbounded%20nature%20of%20the%20queues%20within%20the%20network%0Aresulting%20in%20an%20unbounded%20state-space.%20An%20unbounded%20state-space%20is%20particularly%0Achallenging%20for%20neural%20network%20policies%20as%20neural%20networks%20are%20notoriously%20poor%0Aat%20extrapolating%20to%20unseen%20states.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0Aintervention-assisted%20framework%20that%20leverages%20strategic%20interventions%20from%0Aknown%20stable%20policies%20to%20ensure%20the%20queue%20sizes%20remain%20bounded.%20This%20framework%0Acombines%20the%20learning%20power%20of%20neural%20networks%20with%20the%20guaranteed%20stability%20of%0Aclassical%20control%20policies%20for%20SQNs.%20We%20introduce%20a%20method%20to%20design%20these%0Aintervention-assisted%20policies%20to%20ensure%20strong%20stability%20of%20the%20network.%0AFurthermore%2C%20we%20extend%20foundational%20DRL%20theorems%20for%20intervention-assisted%0Apolicies%20and%20develop%20two%20practical%20algorithms%20specifically%20for%20ODRLC%20of%20SQNs.%0AFinally%2C%20we%20demonstrate%20through%20experiments%20that%20our%20proposed%20algorithms%0Aoutperform%20both%20classical%20control%20approaches%20and%20prior%20ODRLC%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04106v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intervention-Assisted%20Policy%20Gradient%20Methods%20for%20Online%20Stochastic%0A%20%20Queuing%20Network%20Optimization%3A%20Technical%20Report&entry.906535625=Jerrod%20Wigmore%20and%20Brooke%20Shrader%20and%20Eytan%20Modiano&entry.1292438233=%20%20Deep%20Reinforcement%20Learning%20%28DRL%29%20offers%20a%20powerful%20approach%20to%20training%0Aneural%20network%20control%20policies%20for%20stochastic%20queuing%20networks%20%28SQN%29.%20However%2C%0Atraditional%20DRL%20methods%20rely%20on%20offline%20simulations%20or%20static%20datasets%2C%0Alimiting%20their%20real-world%20application%20in%20SQN%20control.%20This%20work%20proposes%20Online%0ADeep%20Reinforcement%20Learning-based%20Controls%20%28ODRLC%29%20as%20an%20alternative%2C%20where%20an%0Aintelligent%20agent%20interacts%20directly%20with%20a%20real%20environment%20and%20learns%20an%0Aoptimal%20control%20policy%20from%20these%20online%20interactions.%20SQNs%20present%20a%20challenge%0Afor%20ODRLC%20due%20to%20the%20unbounded%20nature%20of%20the%20queues%20within%20the%20network%0Aresulting%20in%20an%20unbounded%20state-space.%20An%20unbounded%20state-space%20is%20particularly%0Achallenging%20for%20neural%20network%20policies%20as%20neural%20networks%20are%20notoriously%20poor%0Aat%20extrapolating%20to%20unseen%20states.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0Aintervention-assisted%20framework%20that%20leverages%20strategic%20interventions%20from%0Aknown%20stable%20policies%20to%20ensure%20the%20queue%20sizes%20remain%20bounded.%20This%20framework%0Acombines%20the%20learning%20power%20of%20neural%20networks%20with%20the%20guaranteed%20stability%20of%0Aclassical%20control%20policies%20for%20SQNs.%20We%20introduce%20a%20method%20to%20design%20these%0Aintervention-assisted%20policies%20to%20ensure%20strong%20stability%20of%20the%20network.%0AFurthermore%2C%20we%20extend%20foundational%20DRL%20theorems%20for%20intervention-assisted%0Apolicies%20and%20develop%20two%20practical%20algorithms%20specifically%20for%20ODRLC%20of%20SQNs.%0AFinally%2C%20we%20demonstrate%20through%20experiments%20that%20our%20proposed%20algorithms%0Aoutperform%20both%20classical%20control%20approaches%20and%20prior%20ODRLC%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04106v1&entry.124074799=Read"},
{"title": "Rolling the dice for better deep learning performance: A study of\n  randomness techniques in deep neural networks", "author": "Mohammed Ghaith Altarabichi and S\u0142awomir Nowaczyk and Sepideh Pashami and Peyman Sheikholharam Mashhadi and Julia Handl", "abstract": "  This paper investigates how various randomization techniques impact Deep\nNeural Networks (DNNs). Randomization, like weight noise and dropout, aids in\nreducing overfitting and enhancing generalization, but their interactions are\npoorly understood. The study categorizes randomness techniques into four types\nand proposes new methods: adding noise to the loss function and random masking\nof gradient updates. Using Particle Swarm Optimizer (PSO) for hyperparameter\noptimization, it explores optimal configurations across MNIST, FASHION-MNIST,\nCIFAR10, and CIFAR100 datasets. Over 30,000 configurations are evaluated,\nrevealing data augmentation and weight initialization randomness as main\nperformance contributors. Correlation analysis shows different optimizers\nprefer distinct randomization types. The complete implementation and dataset\nare available on GitHub.\n", "link": "http://arxiv.org/abs/2404.03992v1", "date": "2024-04-05", "relevancy": 1.9379, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4986}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4763}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4694}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rolling%20the%20dice%20for%20better%20deep%20learning%20performance%3A%20A%20study%20of%0A%20%20randomness%20techniques%20in%20deep%20neural%20networks&body=Title%3A%20Rolling%20the%20dice%20for%20better%20deep%20learning%20performance%3A%20A%20study%20of%0A%20%20randomness%20techniques%20in%20deep%20neural%20networks%0AAuthor%3A%20Mohammed%20Ghaith%20Altarabichi%20and%20S%C5%82awomir%20Nowaczyk%20and%20Sepideh%20Pashami%20and%20Peyman%20Sheikholharam%20Mashhadi%20and%20Julia%20Handl%0AAbstract%3A%20%20%20This%20paper%20investigates%20how%20various%20randomization%20techniques%20impact%20Deep%0ANeural%20Networks%20%28DNNs%29.%20Randomization%2C%20like%20weight%20noise%20and%20dropout%2C%20aids%20in%0Areducing%20overfitting%20and%20enhancing%20generalization%2C%20but%20their%20interactions%20are%0Apoorly%20understood.%20The%20study%20categorizes%20randomness%20techniques%20into%20four%20types%0Aand%20proposes%20new%20methods%3A%20adding%20noise%20to%20the%20loss%20function%20and%20random%20masking%0Aof%20gradient%20updates.%20Using%20Particle%20Swarm%20Optimizer%20%28PSO%29%20for%20hyperparameter%0Aoptimization%2C%20it%20explores%20optimal%20configurations%20across%20MNIST%2C%20FASHION-MNIST%2C%0ACIFAR10%2C%20and%20CIFAR100%20datasets.%20Over%2030%2C000%20configurations%20are%20evaluated%2C%0Arevealing%20data%20augmentation%20and%20weight%20initialization%20randomness%20as%20main%0Aperformance%20contributors.%20Correlation%20analysis%20shows%20different%20optimizers%0Aprefer%20distinct%20randomization%20types.%20The%20complete%20implementation%20and%20dataset%0Aare%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03992v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rolling%20the%20dice%20for%20better%20deep%20learning%20performance%3A%20A%20study%20of%0A%20%20randomness%20techniques%20in%20deep%20neural%20networks&entry.906535625=Mohammed%20Ghaith%20Altarabichi%20and%20S%C5%82awomir%20Nowaczyk%20and%20Sepideh%20Pashami%20and%20Peyman%20Sheikholharam%20Mashhadi%20and%20Julia%20Handl&entry.1292438233=%20%20This%20paper%20investigates%20how%20various%20randomization%20techniques%20impact%20Deep%0ANeural%20Networks%20%28DNNs%29.%20Randomization%2C%20like%20weight%20noise%20and%20dropout%2C%20aids%20in%0Areducing%20overfitting%20and%20enhancing%20generalization%2C%20but%20their%20interactions%20are%0Apoorly%20understood.%20The%20study%20categorizes%20randomness%20techniques%20into%20four%20types%0Aand%20proposes%20new%20methods%3A%20adding%20noise%20to%20the%20loss%20function%20and%20random%20masking%0Aof%20gradient%20updates.%20Using%20Particle%20Swarm%20Optimizer%20%28PSO%29%20for%20hyperparameter%0Aoptimization%2C%20it%20explores%20optimal%20configurations%20across%20MNIST%2C%20FASHION-MNIST%2C%0ACIFAR10%2C%20and%20CIFAR100%20datasets.%20Over%2030%2C000%20configurations%20are%20evaluated%2C%0Arevealing%20data%20augmentation%20and%20weight%20initialization%20randomness%20as%20main%0Aperformance%20contributors.%20Correlation%20analysis%20shows%20different%20optimizers%0Aprefer%20distinct%20randomization%20types.%20The%20complete%20implementation%20and%20dataset%0Aare%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03992v1&entry.124074799=Read"},
{"title": "Physics-Inspired Synthesized Underwater Image Dataset", "author": "Reina Kaneko and Hiroshi Higashi and Yuichi Tanaka", "abstract": "  This paper introduces the physics-inspired synthesized underwater image\ndataset (PHISWID), a dataset tailored for enhancing underwater image processing\nthrough physics-inspired image synthesis. Deep learning approaches to\nunderwater image enhancement typically demand extensive datasets, yet acquiring\npaired clean and degraded underwater ones poses significant challenges. While\nseveral underwater image datasets have been proposed using physics-based\nsynthesis, a publicly accessible collection has been lacking. Additionally,\nmost underwater image synthesis approaches do not intend to reproduce\natmospheric scenes, resulting in incomplete enhancement. PHISWID addresses this\ngap by offering a set of paired ground-truth (atmospheric) and synthetically\ndegraded underwater images, showcasing not only color degradation but also the\noften-neglected effects of marine snow, a composite of organic matter and sand\nparticles that considerably impairs underwater image clarity. The dataset\napplies these degradations to atmospheric RGB-D images, enhancing the dataset's\nrealism and applicability. PHISWID is particularly valuable for training deep\nneural networks in a supervised learning setting and for objectively assessing\nimage quality in benchmark analyses. Our results reveal that even a basic U-Net\narchitecture, when trained with PHISWID, substantially outperforms existing\nmethods in underwater image enhancement. We intend to release PHISWID publicly,\ncontributing a significant resource to the advancement of underwater imaging\ntechnology.\n", "link": "http://arxiv.org/abs/2404.03998v1", "date": "2024-04-05", "relevancy": 1.9226, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4939}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4933}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4627}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Physics-Inspired%20Synthesized%20Underwater%20Image%20Dataset&body=Title%3A%20Physics-Inspired%20Synthesized%20Underwater%20Image%20Dataset%0AAuthor%3A%20Reina%20Kaneko%20and%20Hiroshi%20Higashi%20and%20Yuichi%20Tanaka%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20physics-inspired%20synthesized%20underwater%20image%0Adataset%20%28PHISWID%29%2C%20a%20dataset%20tailored%20for%20enhancing%20underwater%20image%20processing%0Athrough%20physics-inspired%20image%20synthesis.%20Deep%20learning%20approaches%20to%0Aunderwater%20image%20enhancement%20typically%20demand%20extensive%20datasets%2C%20yet%20acquiring%0Apaired%20clean%20and%20degraded%20underwater%20ones%20poses%20significant%20challenges.%20While%0Aseveral%20underwater%20image%20datasets%20have%20been%20proposed%20using%20physics-based%0Asynthesis%2C%20a%20publicly%20accessible%20collection%20has%20been%20lacking.%20Additionally%2C%0Amost%20underwater%20image%20synthesis%20approaches%20do%20not%20intend%20to%20reproduce%0Aatmospheric%20scenes%2C%20resulting%20in%20incomplete%20enhancement.%20PHISWID%20addresses%20this%0Agap%20by%20offering%20a%20set%20of%20paired%20ground-truth%20%28atmospheric%29%20and%20synthetically%0Adegraded%20underwater%20images%2C%20showcasing%20not%20only%20color%20degradation%20but%20also%20the%0Aoften-neglected%20effects%20of%20marine%20snow%2C%20a%20composite%20of%20organic%20matter%20and%20sand%0Aparticles%20that%20considerably%20impairs%20underwater%20image%20clarity.%20The%20dataset%0Aapplies%20these%20degradations%20to%20atmospheric%20RGB-D%20images%2C%20enhancing%20the%20dataset%27s%0Arealism%20and%20applicability.%20PHISWID%20is%20particularly%20valuable%20for%20training%20deep%0Aneural%20networks%20in%20a%20supervised%20learning%20setting%20and%20for%20objectively%20assessing%0Aimage%20quality%20in%20benchmark%20analyses.%20Our%20results%20reveal%20that%20even%20a%20basic%20U-Net%0Aarchitecture%2C%20when%20trained%20with%20PHISWID%2C%20substantially%20outperforms%20existing%0Amethods%20in%20underwater%20image%20enhancement.%20We%20intend%20to%20release%20PHISWID%20publicly%2C%0Acontributing%20a%20significant%20resource%20to%20the%20advancement%20of%20underwater%20imaging%0Atechnology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03998v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Inspired%20Synthesized%20Underwater%20Image%20Dataset&entry.906535625=Reina%20Kaneko%20and%20Hiroshi%20Higashi%20and%20Yuichi%20Tanaka&entry.1292438233=%20%20This%20paper%20introduces%20the%20physics-inspired%20synthesized%20underwater%20image%0Adataset%20%28PHISWID%29%2C%20a%20dataset%20tailored%20for%20enhancing%20underwater%20image%20processing%0Athrough%20physics-inspired%20image%20synthesis.%20Deep%20learning%20approaches%20to%0Aunderwater%20image%20enhancement%20typically%20demand%20extensive%20datasets%2C%20yet%20acquiring%0Apaired%20clean%20and%20degraded%20underwater%20ones%20poses%20significant%20challenges.%20While%0Aseveral%20underwater%20image%20datasets%20have%20been%20proposed%20using%20physics-based%0Asynthesis%2C%20a%20publicly%20accessible%20collection%20has%20been%20lacking.%20Additionally%2C%0Amost%20underwater%20image%20synthesis%20approaches%20do%20not%20intend%20to%20reproduce%0Aatmospheric%20scenes%2C%20resulting%20in%20incomplete%20enhancement.%20PHISWID%20addresses%20this%0Agap%20by%20offering%20a%20set%20of%20paired%20ground-truth%20%28atmospheric%29%20and%20synthetically%0Adegraded%20underwater%20images%2C%20showcasing%20not%20only%20color%20degradation%20but%20also%20the%0Aoften-neglected%20effects%20of%20marine%20snow%2C%20a%20composite%20of%20organic%20matter%20and%20sand%0Aparticles%20that%20considerably%20impairs%20underwater%20image%20clarity.%20The%20dataset%0Aapplies%20these%20degradations%20to%20atmospheric%20RGB-D%20images%2C%20enhancing%20the%20dataset%27s%0Arealism%20and%20applicability.%20PHISWID%20is%20particularly%20valuable%20for%20training%20deep%0Aneural%20networks%20in%20a%20supervised%20learning%20setting%20and%20for%20objectively%20assessing%0Aimage%20quality%20in%20benchmark%20analyses.%20Our%20results%20reveal%20that%20even%20a%20basic%20U-Net%0Aarchitecture%2C%20when%20trained%20with%20PHISWID%2C%20substantially%20outperforms%20existing%0Amethods%20in%20underwater%20image%20enhancement.%20We%20intend%20to%20release%20PHISWID%20publicly%2C%0Acontributing%20a%20significant%20resource%20to%20the%20advancement%20of%20underwater%20imaging%0Atechnology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03998v1&entry.124074799=Read"},
{"title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt\n  Coherence Metrics with T2IScoreScore (TS2)", "author": "Michael Saxon and Fatima Jahara and Mahsa Khoshnoodi and Yujie Lu and Aditya Sharma and William Yang Wang", "abstract": "  With advances in the quality of text-to-image (T2I) models has come interest\nin benchmarking their prompt faithfulness-the semantic coherence of generated\nimages to the prompts they were conditioned on. A variety of T2I faithfulness\nmetrics have been proposed, leveraging advances in cross-modal embeddings and\nvision-language models (VLMs). However, these metrics are not rigorously\ncompared and benchmarked, instead presented against few weak baselines by\ncorrelation to human Likert scores over a set of easy-to-discriminate images.\n  We introduce T2IScoreScore (TS2), a curated set of semantic error graphs\ncontaining a prompt and a set increasingly erroneous images. These allow us to\nrigorously judge whether a given prompt faithfulness metric can correctly order\nimages with respect to their objective error count and significantly\ndiscriminate between different error nodes, using meta-metric scores derived\nfrom established statistical tests. Surprisingly, we find that the\nstate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we\ntested fail to significantly outperform simple feature-based metrics like\nCLIPScore, particularly on a hard subset of naturally-occurring T2I model\nerrors. TS2 will enable the development of better T2I prompt faithfulness\nmetrics through more rigorous comparison of their conformity to expected\norderings and separations under objective criteria.\n", "link": "http://arxiv.org/abs/2404.04251v1", "date": "2024-04-05", "relevancy": 1.9107, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4907}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4718}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4597}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Who%20Evaluates%20the%20Evaluations%3F%20Objectively%20Scoring%20Text-to-Image%20Prompt%0A%20%20Coherence%20Metrics%20with%20T2IScoreScore%20%28TS2%29&body=Title%3A%20Who%20Evaluates%20the%20Evaluations%3F%20Objectively%20Scoring%20Text-to-Image%20Prompt%0A%20%20Coherence%20Metrics%20with%20T2IScoreScore%20%28TS2%29%0AAuthor%3A%20Michael%20Saxon%20and%20Fatima%20Jahara%20and%20Mahsa%20Khoshnoodi%20and%20Yujie%20Lu%20and%20Aditya%20Sharma%20and%20William%20Yang%20Wang%0AAbstract%3A%20%20%20With%20advances%20in%20the%20quality%20of%20text-to-image%20%28T2I%29%20models%20has%20come%20interest%0Ain%20benchmarking%20their%20prompt%20faithfulness-the%20semantic%20coherence%20of%20generated%0Aimages%20to%20the%20prompts%20they%20were%20conditioned%20on.%20A%20variety%20of%20T2I%20faithfulness%0Ametrics%20have%20been%20proposed%2C%20leveraging%20advances%20in%20cross-modal%20embeddings%20and%0Avision-language%20models%20%28VLMs%29.%20However%2C%20these%20metrics%20are%20not%20rigorously%0Acompared%20and%20benchmarked%2C%20instead%20presented%20against%20few%20weak%20baselines%20by%0Acorrelation%20to%20human%20Likert%20scores%20over%20a%20set%20of%20easy-to-discriminate%20images.%0A%20%20We%20introduce%20T2IScoreScore%20%28TS2%29%2C%20a%20curated%20set%20of%20semantic%20error%20graphs%0Acontaining%20a%20prompt%20and%20a%20set%20increasingly%20erroneous%20images.%20These%20allow%20us%20to%0Arigorously%20judge%20whether%20a%20given%20prompt%20faithfulness%20metric%20can%20correctly%20order%0Aimages%20with%20respect%20to%20their%20objective%20error%20count%20and%20significantly%0Adiscriminate%20between%20different%20error%20nodes%2C%20using%20meta-metric%20scores%20derived%0Afrom%20established%20statistical%20tests.%20Surprisingly%2C%20we%20find%20that%20the%0Astate-of-the-art%20VLM-based%20metrics%20%28e.g.%2C%20TIFA%2C%20DSG%2C%20LLMScore%2C%20VIEScore%29%20we%0Atested%20fail%20to%20significantly%20outperform%20simple%20feature-based%20metrics%20like%0ACLIPScore%2C%20particularly%20on%20a%20hard%20subset%20of%20naturally-occurring%20T2I%20model%0Aerrors.%20TS2%20will%20enable%20the%20development%20of%20better%20T2I%20prompt%20faithfulness%0Ametrics%20through%20more%20rigorous%20comparison%20of%20their%20conformity%20to%20expected%0Aorderings%20and%20separations%20under%20objective%20criteria.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04251v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Evaluates%20the%20Evaluations%3F%20Objectively%20Scoring%20Text-to-Image%20Prompt%0A%20%20Coherence%20Metrics%20with%20T2IScoreScore%20%28TS2%29&entry.906535625=Michael%20Saxon%20and%20Fatima%20Jahara%20and%20Mahsa%20Khoshnoodi%20and%20Yujie%20Lu%20and%20Aditya%20Sharma%20and%20William%20Yang%20Wang&entry.1292438233=%20%20With%20advances%20in%20the%20quality%20of%20text-to-image%20%28T2I%29%20models%20has%20come%20interest%0Ain%20benchmarking%20their%20prompt%20faithfulness-the%20semantic%20coherence%20of%20generated%0Aimages%20to%20the%20prompts%20they%20were%20conditioned%20on.%20A%20variety%20of%20T2I%20faithfulness%0Ametrics%20have%20been%20proposed%2C%20leveraging%20advances%20in%20cross-modal%20embeddings%20and%0Avision-language%20models%20%28VLMs%29.%20However%2C%20these%20metrics%20are%20not%20rigorously%0Acompared%20and%20benchmarked%2C%20instead%20presented%20against%20few%20weak%20baselines%20by%0Acorrelation%20to%20human%20Likert%20scores%20over%20a%20set%20of%20easy-to-discriminate%20images.%0A%20%20We%20introduce%20T2IScoreScore%20%28TS2%29%2C%20a%20curated%20set%20of%20semantic%20error%20graphs%0Acontaining%20a%20prompt%20and%20a%20set%20increasingly%20erroneous%20images.%20These%20allow%20us%20to%0Arigorously%20judge%20whether%20a%20given%20prompt%20faithfulness%20metric%20can%20correctly%20order%0Aimages%20with%20respect%20to%20their%20objective%20error%20count%20and%20significantly%0Adiscriminate%20between%20different%20error%20nodes%2C%20using%20meta-metric%20scores%20derived%0Afrom%20established%20statistical%20tests.%20Surprisingly%2C%20we%20find%20that%20the%0Astate-of-the-art%20VLM-based%20metrics%20%28e.g.%2C%20TIFA%2C%20DSG%2C%20LLMScore%2C%20VIEScore%29%20we%0Atested%20fail%20to%20significantly%20outperform%20simple%20feature-based%20metrics%20like%0ACLIPScore%2C%20particularly%20on%20a%20hard%20subset%20of%20naturally-occurring%20T2I%20model%0Aerrors.%20TS2%20will%20enable%20the%20development%20of%20better%20T2I%20prompt%20faithfulness%0Ametrics%20through%20more%20rigorous%20comparison%20of%20their%20conformity%20to%20expected%0Aorderings%20and%20separations%20under%20objective%20criteria.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04251v1&entry.124074799=Read"},
{"title": "Demystifying Chains, Trees, and Graphs of Thoughts", "author": "Maciej Besta and Florim Memedi and Zhenyu Zhang and Robert Gerstenberger and Guangyuan Piao and Nils Blach and Piotr Nyczyk and Marcin Copik and Grzegorz Kwa\u015bniewski and J\u00fcrgen M\u00fcller and Lukas Gianinazzi and Ales Kubicek and Hubert Niewiadomski and Aidan O'Mahony and Onur Mutlu and Torsten Hoefler", "abstract": "  The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.\n", "link": "http://arxiv.org/abs/2401.14295v3", "date": "2024-04-05", "relevancy": 1.8923, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.492}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4554}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Demystifying%20Chains%2C%20Trees%2C%20and%20Graphs%20of%20Thoughts&body=Title%3A%20Demystifying%20Chains%2C%20Trees%2C%20and%20Graphs%20of%20Thoughts%0AAuthor%3A%20Maciej%20Besta%20and%20Florim%20Memedi%20and%20Zhenyu%20Zhang%20and%20Robert%20Gerstenberger%20and%20Guangyuan%20Piao%20and%20Nils%20Blach%20and%20Piotr%20Nyczyk%20and%20Marcin%20Copik%20and%20Grzegorz%20Kwa%C5%9Bniewski%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20Lukas%20Gianinazzi%20and%20Ales%20Kubicek%20and%20Hubert%20Niewiadomski%20and%20Aidan%20O%27Mahony%20and%20Onur%20Mutlu%20and%20Torsten%20Hoefler%0AAbstract%3A%20%20%20The%20field%20of%20natural%20language%20processing%20%28NLP%29%20has%20witnessed%20significant%0Aprogress%20in%20recent%20years%2C%20with%20a%20notable%20focus%20on%20improving%20large%20language%0Amodels%27%20%28LLM%29%20performance%20through%20innovative%20prompting%20techniques.%20Among%20these%2C%0Aprompt%20engineering%20coupled%20with%20structures%20has%20emerged%20as%20a%20promising%20paradigm%2C%0Awith%20designs%20such%20as%20Chain-of-Thought%2C%20Tree%20of%20Thoughts%2C%20or%20Graph%20of%20Thoughts%2C%0Ain%20which%20the%20overall%20LLM%20reasoning%20is%20guided%20by%20a%20structure%20such%20as%20a%20graph.%20As%0Aillustrated%20with%20numerous%20examples%2C%20this%20paradigm%20significantly%20enhances%20the%0ALLM%27s%20capability%20to%20solve%20numerous%20tasks%2C%20ranging%20from%20logical%20or%20mathematical%0Areasoning%20to%20planning%20or%20creative%20writing.%20To%20facilitate%20the%20understanding%20of%0Athis%20growing%20field%20and%20pave%20the%20way%20for%20future%20developments%2C%20we%20devise%20a%0Ageneral%20blueprint%20for%20effective%20and%20efficient%20LLM%20reasoning%20schemes.%20For%20this%2C%0Awe%20conduct%20an%20in-depth%20analysis%20of%20the%20prompt%20execution%20pipeline%2C%20clarifying%0Aand%20clearly%20defining%20different%20concepts.%20We%20then%20build%20the%20first%20taxonomy%20of%0Astructure-enhanced%20LLM%20reasoning%20schemes.%20We%20focus%20on%20identifying%20fundamental%0Aclasses%20of%20harnessed%20structures%2C%20and%20we%20analyze%20the%20representations%20of%20these%0Astructures%2C%20algorithms%20executed%20with%20these%20structures%2C%20and%20many%20others.%20We%0Arefer%20to%20these%20structures%20as%20reasoning%20topologies%2C%20because%20their%20representation%0Abecomes%20to%20a%20degree%20spatial%2C%20as%20they%20are%20contained%20within%20the%20LLM%20context.%20Our%0Astudy%20compares%20existing%20prompting%20schemes%20using%20the%20proposed%20taxonomy%2C%0Adiscussing%20how%20certain%20design%20choices%20lead%20to%20different%20patterns%20in%20performance%0Aand%20cost.%20We%20also%20outline%20theoretical%20underpinnings%2C%20relationships%20between%0Aprompting%20and%20other%20parts%20of%20the%20LLM%20ecosystem%20such%20as%20knowledge%20bases%2C%20and%20the%0Aassociated%20research%20challenges.%20Our%20work%20will%20help%20to%20advance%20future%20prompt%0Aengineering%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14295v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20Chains%2C%20Trees%2C%20and%20Graphs%20of%20Thoughts&entry.906535625=Maciej%20Besta%20and%20Florim%20Memedi%20and%20Zhenyu%20Zhang%20and%20Robert%20Gerstenberger%20and%20Guangyuan%20Piao%20and%20Nils%20Blach%20and%20Piotr%20Nyczyk%20and%20Marcin%20Copik%20and%20Grzegorz%20Kwa%C5%9Bniewski%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20Lukas%20Gianinazzi%20and%20Ales%20Kubicek%20and%20Hubert%20Niewiadomski%20and%20Aidan%20O%27Mahony%20and%20Onur%20Mutlu%20and%20Torsten%20Hoefler&entry.1292438233=%20%20The%20field%20of%20natural%20language%20processing%20%28NLP%29%20has%20witnessed%20significant%0Aprogress%20in%20recent%20years%2C%20with%20a%20notable%20focus%20on%20improving%20large%20language%0Amodels%27%20%28LLM%29%20performance%20through%20innovative%20prompting%20techniques.%20Among%20these%2C%0Aprompt%20engineering%20coupled%20with%20structures%20has%20emerged%20as%20a%20promising%20paradigm%2C%0Awith%20designs%20such%20as%20Chain-of-Thought%2C%20Tree%20of%20Thoughts%2C%20or%20Graph%20of%20Thoughts%2C%0Ain%20which%20the%20overall%20LLM%20reasoning%20is%20guided%20by%20a%20structure%20such%20as%20a%20graph.%20As%0Aillustrated%20with%20numerous%20examples%2C%20this%20paradigm%20significantly%20enhances%20the%0ALLM%27s%20capability%20to%20solve%20numerous%20tasks%2C%20ranging%20from%20logical%20or%20mathematical%0Areasoning%20to%20planning%20or%20creative%20writing.%20To%20facilitate%20the%20understanding%20of%0Athis%20growing%20field%20and%20pave%20the%20way%20for%20future%20developments%2C%20we%20devise%20a%0Ageneral%20blueprint%20for%20effective%20and%20efficient%20LLM%20reasoning%20schemes.%20For%20this%2C%0Awe%20conduct%20an%20in-depth%20analysis%20of%20the%20prompt%20execution%20pipeline%2C%20clarifying%0Aand%20clearly%20defining%20different%20concepts.%20We%20then%20build%20the%20first%20taxonomy%20of%0Astructure-enhanced%20LLM%20reasoning%20schemes.%20We%20focus%20on%20identifying%20fundamental%0Aclasses%20of%20harnessed%20structures%2C%20and%20we%20analyze%20the%20representations%20of%20these%0Astructures%2C%20algorithms%20executed%20with%20these%20structures%2C%20and%20many%20others.%20We%0Arefer%20to%20these%20structures%20as%20reasoning%20topologies%2C%20because%20their%20representation%0Abecomes%20to%20a%20degree%20spatial%2C%20as%20they%20are%20contained%20within%20the%20LLM%20context.%20Our%0Astudy%20compares%20existing%20prompting%20schemes%20using%20the%20proposed%20taxonomy%2C%0Adiscussing%20how%20certain%20design%20choices%20lead%20to%20different%20patterns%20in%20performance%0Aand%20cost.%20We%20also%20outline%20theoretical%20underpinnings%2C%20relationships%20between%0Aprompting%20and%20other%20parts%20of%20the%20LLM%20ecosystem%20such%20as%20knowledge%20bases%2C%20and%20the%0Aassociated%20research%20challenges.%20Our%20work%20will%20help%20to%20advance%20future%20prompt%0Aengineering%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14295v3&entry.124074799=Read"},
{"title": "H3DFact: Heterogeneous 3D Integrated CIM for Factorization with\n  Holographic Perceptual Representations", "author": "Zishen Wan and Che-Kai Liu and Mohamed Ibrahim and Hanchen Yang and Samuel Spetalnick and Tushar Krishna and Arijit Raychowdhury", "abstract": "  Disentangling attributes of various sensory signals is central to human-like\nperception and reasoning and a critical task for higher-order cognitive and\nneuro-symbolic AI systems. An elegant approach to represent this intricate\nfactorization is via high-dimensional holographic vectors drawing on\nbrain-inspired vector symbolic architectures. However, holographic\nfactorization involves iterative computation with high-dimensional\nmatrix-vector multiplications and suffers from non-convergence problems.\n  In this paper, we present H3DFact, a heterogeneous 3D integrated in-memory\ncompute engine capable of efficiently factorizing high-dimensional holographic\nrepresentations. H3DFact exploits the computation-in-superposition capability\nof holographic vectors and the intrinsic stochasticity associated with\nmemristive-based 3D compute-in-memory. Evaluated on large-scale factorization\nand perceptual problems, H3DFact demonstrates superior capability in\nfactorization accuracy and operational capacity by up to five orders of\nmagnitude, with 5.5x compute density, 1.2x energy efficiency improvements, and\n5.9x less silicon footprint compared to iso-capacity 2D designs.\n", "link": "http://arxiv.org/abs/2404.04173v1", "date": "2024-04-05", "relevancy": 1.8812, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4818}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4731}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4629}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20H3DFact%3A%20Heterogeneous%203D%20Integrated%20CIM%20for%20Factorization%20with%0A%20%20Holographic%20Perceptual%20Representations&body=Title%3A%20H3DFact%3A%20Heterogeneous%203D%20Integrated%20CIM%20for%20Factorization%20with%0A%20%20Holographic%20Perceptual%20Representations%0AAuthor%3A%20Zishen%20Wan%20and%20Che-Kai%20Liu%20and%20Mohamed%20Ibrahim%20and%20Hanchen%20Yang%20and%20Samuel%20Spetalnick%20and%20Tushar%20Krishna%20and%20Arijit%20Raychowdhury%0AAbstract%3A%20%20%20Disentangling%20attributes%20of%20various%20sensory%20signals%20is%20central%20to%20human-like%0Aperception%20and%20reasoning%20and%20a%20critical%20task%20for%20higher-order%20cognitive%20and%0Aneuro-symbolic%20AI%20systems.%20An%20elegant%20approach%20to%20represent%20this%20intricate%0Afactorization%20is%20via%20high-dimensional%20holographic%20vectors%20drawing%20on%0Abrain-inspired%20vector%20symbolic%20architectures.%20However%2C%20holographic%0Afactorization%20involves%20iterative%20computation%20with%20high-dimensional%0Amatrix-vector%20multiplications%20and%20suffers%20from%20non-convergence%20problems.%0A%20%20In%20this%20paper%2C%20we%20present%20H3DFact%2C%20a%20heterogeneous%203D%20integrated%20in-memory%0Acompute%20engine%20capable%20of%20efficiently%20factorizing%20high-dimensional%20holographic%0Arepresentations.%20H3DFact%20exploits%20the%20computation-in-superposition%20capability%0Aof%20holographic%20vectors%20and%20the%20intrinsic%20stochasticity%20associated%20with%0Amemristive-based%203D%20compute-in-memory.%20Evaluated%20on%20large-scale%20factorization%0Aand%20perceptual%20problems%2C%20H3DFact%20demonstrates%20superior%20capability%20in%0Afactorization%20accuracy%20and%20operational%20capacity%20by%20up%20to%20five%20orders%20of%0Amagnitude%2C%20with%205.5x%20compute%20density%2C%201.2x%20energy%20efficiency%20improvements%2C%20and%0A5.9x%20less%20silicon%20footprint%20compared%20to%20iso-capacity%202D%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04173v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H3DFact%3A%20Heterogeneous%203D%20Integrated%20CIM%20for%20Factorization%20with%0A%20%20Holographic%20Perceptual%20Representations&entry.906535625=Zishen%20Wan%20and%20Che-Kai%20Liu%20and%20Mohamed%20Ibrahim%20and%20Hanchen%20Yang%20and%20Samuel%20Spetalnick%20and%20Tushar%20Krishna%20and%20Arijit%20Raychowdhury&entry.1292438233=%20%20Disentangling%20attributes%20of%20various%20sensory%20signals%20is%20central%20to%20human-like%0Aperception%20and%20reasoning%20and%20a%20critical%20task%20for%20higher-order%20cognitive%20and%0Aneuro-symbolic%20AI%20systems.%20An%20elegant%20approach%20to%20represent%20this%20intricate%0Afactorization%20is%20via%20high-dimensional%20holographic%20vectors%20drawing%20on%0Abrain-inspired%20vector%20symbolic%20architectures.%20However%2C%20holographic%0Afactorization%20involves%20iterative%20computation%20with%20high-dimensional%0Amatrix-vector%20multiplications%20and%20suffers%20from%20non-convergence%20problems.%0A%20%20In%20this%20paper%2C%20we%20present%20H3DFact%2C%20a%20heterogeneous%203D%20integrated%20in-memory%0Acompute%20engine%20capable%20of%20efficiently%20factorizing%20high-dimensional%20holographic%0Arepresentations.%20H3DFact%20exploits%20the%20computation-in-superposition%20capability%0Aof%20holographic%20vectors%20and%20the%20intrinsic%20stochasticity%20associated%20with%0Amemristive-based%203D%20compute-in-memory.%20Evaluated%20on%20large-scale%20factorization%0Aand%20perceptual%20problems%2C%20H3DFact%20demonstrates%20superior%20capability%20in%0Afactorization%20accuracy%20and%20operational%20capacity%20by%20up%20to%20five%20orders%20of%0Amagnitude%2C%20with%205.5x%20compute%20density%2C%201.2x%20energy%20efficiency%20improvements%2C%20and%0A5.9x%20less%20silicon%20footprint%20compared%20to%20iso-capacity%202D%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04173v1&entry.124074799=Read"},
{"title": "Reliable Feature Selection for Adversarially Robust Cyber-Attack\n  Detection", "author": "Jo\u00e3o Vitorino and Miguel Silva and Eva Maia and Isabel Pra\u00e7a", "abstract": "  The growing cybersecurity threats make it essential to use high-quality data\nto train Machine Learning (ML) models for network traffic analysis, without\nnoisy or missing data. By selecting the most relevant features for cyber-attack\ndetection, it is possible to improve both the robustness and computational\nefficiency of the models used in a cybersecurity system. This work presents a\nfeature selection and consensus process that combines multiple methods and\napplies them to several network datasets. Two different feature sets were\nselected and were used to train multiple ML models with regular and adversarial\ntraining. Finally, an adversarial evasion robustness benchmark was performed to\nanalyze the reliability of the different feature sets and their impact on the\nsusceptibility of the models to adversarial examples. By using an improved\ndataset with more data diversity, selecting the best time-related features and\na more specific feature set, and performing adversarial training, the ML models\nwere able to achieve a better adversarially robust generalization. The\nrobustness of the models was significantly improved without their\ngeneralization to regular traffic flows being affected, without increases of\nfalse alarms, and without requiring too many computational resources, which\nenables a reliable detection of suspicious activity and perturbed traffic flows\nin enterprise computer networks.\n", "link": "http://arxiv.org/abs/2404.04188v1", "date": "2024-04-05", "relevancy": 1.8807, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5008}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4581}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4444}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reliable%20Feature%20Selection%20for%20Adversarially%20Robust%20Cyber-Attack%0A%20%20Detection&body=Title%3A%20Reliable%20Feature%20Selection%20for%20Adversarially%20Robust%20Cyber-Attack%0A%20%20Detection%0AAuthor%3A%20Jo%C3%A3o%20Vitorino%20and%20Miguel%20Silva%20and%20Eva%20Maia%20and%20Isabel%20Pra%C3%A7a%0AAbstract%3A%20%20%20The%20growing%20cybersecurity%20threats%20make%20it%20essential%20to%20use%20high-quality%20data%0Ato%20train%20Machine%20Learning%20%28ML%29%20models%20for%20network%20traffic%20analysis%2C%20without%0Anoisy%20or%20missing%20data.%20By%20selecting%20the%20most%20relevant%20features%20for%20cyber-attack%0Adetection%2C%20it%20is%20possible%20to%20improve%20both%20the%20robustness%20and%20computational%0Aefficiency%20of%20the%20models%20used%20in%20a%20cybersecurity%20system.%20This%20work%20presents%20a%0Afeature%20selection%20and%20consensus%20process%20that%20combines%20multiple%20methods%20and%0Aapplies%20them%20to%20several%20network%20datasets.%20Two%20different%20feature%20sets%20were%0Aselected%20and%20were%20used%20to%20train%20multiple%20ML%20models%20with%20regular%20and%20adversarial%0Atraining.%20Finally%2C%20an%20adversarial%20evasion%20robustness%20benchmark%20was%20performed%20to%0Aanalyze%20the%20reliability%20of%20the%20different%20feature%20sets%20and%20their%20impact%20on%20the%0Asusceptibility%20of%20the%20models%20to%20adversarial%20examples.%20By%20using%20an%20improved%0Adataset%20with%20more%20data%20diversity%2C%20selecting%20the%20best%20time-related%20features%20and%0Aa%20more%20specific%20feature%20set%2C%20and%20performing%20adversarial%20training%2C%20the%20ML%20models%0Awere%20able%20to%20achieve%20a%20better%20adversarially%20robust%20generalization.%20The%0Arobustness%20of%20the%20models%20was%20significantly%20improved%20without%20their%0Ageneralization%20to%20regular%20traffic%20flows%20being%20affected%2C%20without%20increases%20of%0Afalse%20alarms%2C%20and%20without%20requiring%20too%20many%20computational%20resources%2C%20which%0Aenables%20a%20reliable%20detection%20of%20suspicious%20activity%20and%20perturbed%20traffic%20flows%0Ain%20enterprise%20computer%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04188v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Feature%20Selection%20for%20Adversarially%20Robust%20Cyber-Attack%0A%20%20Detection&entry.906535625=Jo%C3%A3o%20Vitorino%20and%20Miguel%20Silva%20and%20Eva%20Maia%20and%20Isabel%20Pra%C3%A7a&entry.1292438233=%20%20The%20growing%20cybersecurity%20threats%20make%20it%20essential%20to%20use%20high-quality%20data%0Ato%20train%20Machine%20Learning%20%28ML%29%20models%20for%20network%20traffic%20analysis%2C%20without%0Anoisy%20or%20missing%20data.%20By%20selecting%20the%20most%20relevant%20features%20for%20cyber-attack%0Adetection%2C%20it%20is%20possible%20to%20improve%20both%20the%20robustness%20and%20computational%0Aefficiency%20of%20the%20models%20used%20in%20a%20cybersecurity%20system.%20This%20work%20presents%20a%0Afeature%20selection%20and%20consensus%20process%20that%20combines%20multiple%20methods%20and%0Aapplies%20them%20to%20several%20network%20datasets.%20Two%20different%20feature%20sets%20were%0Aselected%20and%20were%20used%20to%20train%20multiple%20ML%20models%20with%20regular%20and%20adversarial%0Atraining.%20Finally%2C%20an%20adversarial%20evasion%20robustness%20benchmark%20was%20performed%20to%0Aanalyze%20the%20reliability%20of%20the%20different%20feature%20sets%20and%20their%20impact%20on%20the%0Asusceptibility%20of%20the%20models%20to%20adversarial%20examples.%20By%20using%20an%20improved%0Adataset%20with%20more%20data%20diversity%2C%20selecting%20the%20best%20time-related%20features%20and%0Aa%20more%20specific%20feature%20set%2C%20and%20performing%20adversarial%20training%2C%20the%20ML%20models%0Awere%20able%20to%20achieve%20a%20better%20adversarially%20robust%20generalization.%20The%0Arobustness%20of%20the%20models%20was%20significantly%20improved%20without%20their%0Ageneralization%20to%20regular%20traffic%20flows%20being%20affected%2C%20without%20increases%20of%0Afalse%20alarms%2C%20and%20without%20requiring%20too%20many%20computational%20resources%2C%20which%0Aenables%20a%20reliable%20detection%20of%20suspicious%20activity%20and%20perturbed%20traffic%20flows%0Ain%20enterprise%20computer%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04188v1&entry.124074799=Read"},
{"title": "Precision Guided Approach to Mitigate Data Poisoning Attacks in\n  Federated Learning", "author": "K Naveen Kumar and C Krishna Mohan and Aravind Machiry", "abstract": "  Federated Learning (FL) is a collaborative learning paradigm enabling\nparticipants to collectively train a shared machine learning model while\npreserving the privacy of their sensitive data. Nevertheless, the inherent\ndecentralized and data-opaque characteristics of FL render its susceptibility\nto data poisoning attacks. These attacks introduce malformed or malicious\ninputs during local model training, subsequently influencing the global model\nand resulting in erroneous predictions. Current FL defense strategies against\ndata poisoning attacks either involve a trade-off between accuracy and\nrobustness or necessitate the presence of a uniformly distributed root dataset\nat the server. To overcome these limitations, we present FedZZ, which harnesses\na zone-based deviating update (ZBDU) mechanism to effectively counter data\npoisoning attacks in FL. Further, we introduce a precision-guided methodology\nthat actively characterizes these client clusters (zones), which in turn aids\nin recognizing and discarding malicious updates at the server. Our evaluation\nof FedZZ across two widely recognized datasets: CIFAR10 and EMNIST, demonstrate\nits efficacy in mitigating data poisoning attacks, surpassing the performance\nof prevailing state-of-the-art methodologies in both single and multi-client\nattack scenarios and varying attack volumes. Notably, FedZZ also functions as a\nrobust client selection strategy, even in highly non-IID and attack-free\nscenarios. Moreover, in the face of escalating poisoning rates, the model\naccuracy attained by FedZZ displays superior resilience compared to existing\ntechniques. For instance, when confronted with a 50% presence of malicious\nclients, FedZZ sustains an accuracy of 67.43%, while the accuracy of the\nsecond-best solution, FL-Defender, diminishes to 43.36%.\n", "link": "http://arxiv.org/abs/2404.04139v1", "date": "2024-04-05", "relevancy": 1.8671, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4929}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4655}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4576}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Precision%20Guided%20Approach%20to%20Mitigate%20Data%20Poisoning%20Attacks%20in%0A%20%20Federated%20Learning&body=Title%3A%20Precision%20Guided%20Approach%20to%20Mitigate%20Data%20Poisoning%20Attacks%20in%0A%20%20Federated%20Learning%0AAuthor%3A%20K%20Naveen%20Kumar%20and%20C%20Krishna%20Mohan%20and%20Aravind%20Machiry%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20collaborative%20learning%20paradigm%20enabling%0Aparticipants%20to%20collectively%20train%20a%20shared%20machine%20learning%20model%20while%0Apreserving%20the%20privacy%20of%20their%20sensitive%20data.%20Nevertheless%2C%20the%20inherent%0Adecentralized%20and%20data-opaque%20characteristics%20of%20FL%20render%20its%20susceptibility%0Ato%20data%20poisoning%20attacks.%20These%20attacks%20introduce%20malformed%20or%20malicious%0Ainputs%20during%20local%20model%20training%2C%20subsequently%20influencing%20the%20global%20model%0Aand%20resulting%20in%20erroneous%20predictions.%20Current%20FL%20defense%20strategies%20against%0Adata%20poisoning%20attacks%20either%20involve%20a%20trade-off%20between%20accuracy%20and%0Arobustness%20or%20necessitate%20the%20presence%20of%20a%20uniformly%20distributed%20root%20dataset%0Aat%20the%20server.%20To%20overcome%20these%20limitations%2C%20we%20present%20FedZZ%2C%20which%20harnesses%0Aa%20zone-based%20deviating%20update%20%28ZBDU%29%20mechanism%20to%20effectively%20counter%20data%0Apoisoning%20attacks%20in%20FL.%20Further%2C%20we%20introduce%20a%20precision-guided%20methodology%0Athat%20actively%20characterizes%20these%20client%20clusters%20%28zones%29%2C%20which%20in%20turn%20aids%0Ain%20recognizing%20and%20discarding%20malicious%20updates%20at%20the%20server.%20Our%20evaluation%0Aof%20FedZZ%20across%20two%20widely%20recognized%20datasets%3A%20CIFAR10%20and%20EMNIST%2C%20demonstrate%0Aits%20efficacy%20in%20mitigating%20data%20poisoning%20attacks%2C%20surpassing%20the%20performance%0Aof%20prevailing%20state-of-the-art%20methodologies%20in%20both%20single%20and%20multi-client%0Aattack%20scenarios%20and%20varying%20attack%20volumes.%20Notably%2C%20FedZZ%20also%20functions%20as%20a%0Arobust%20client%20selection%20strategy%2C%20even%20in%20highly%20non-IID%20and%20attack-free%0Ascenarios.%20Moreover%2C%20in%20the%20face%20of%20escalating%20poisoning%20rates%2C%20the%20model%0Aaccuracy%20attained%20by%20FedZZ%20displays%20superior%20resilience%20compared%20to%20existing%0Atechniques.%20For%20instance%2C%20when%20confronted%20with%20a%2050%25%20presence%20of%20malicious%0Aclients%2C%20FedZZ%20sustains%20an%20accuracy%20of%2067.43%25%2C%20while%20the%20accuracy%20of%20the%0Asecond-best%20solution%2C%20FL-Defender%2C%20diminishes%20to%2043.36%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04139v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Precision%20Guided%20Approach%20to%20Mitigate%20Data%20Poisoning%20Attacks%20in%0A%20%20Federated%20Learning&entry.906535625=K%20Naveen%20Kumar%20and%20C%20Krishna%20Mohan%20and%20Aravind%20Machiry&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20collaborative%20learning%20paradigm%20enabling%0Aparticipants%20to%20collectively%20train%20a%20shared%20machine%20learning%20model%20while%0Apreserving%20the%20privacy%20of%20their%20sensitive%20data.%20Nevertheless%2C%20the%20inherent%0Adecentralized%20and%20data-opaque%20characteristics%20of%20FL%20render%20its%20susceptibility%0Ato%20data%20poisoning%20attacks.%20These%20attacks%20introduce%20malformed%20or%20malicious%0Ainputs%20during%20local%20model%20training%2C%20subsequently%20influencing%20the%20global%20model%0Aand%20resulting%20in%20erroneous%20predictions.%20Current%20FL%20defense%20strategies%20against%0Adata%20poisoning%20attacks%20either%20involve%20a%20trade-off%20between%20accuracy%20and%0Arobustness%20or%20necessitate%20the%20presence%20of%20a%20uniformly%20distributed%20root%20dataset%0Aat%20the%20server.%20To%20overcome%20these%20limitations%2C%20we%20present%20FedZZ%2C%20which%20harnesses%0Aa%20zone-based%20deviating%20update%20%28ZBDU%29%20mechanism%20to%20effectively%20counter%20data%0Apoisoning%20attacks%20in%20FL.%20Further%2C%20we%20introduce%20a%20precision-guided%20methodology%0Athat%20actively%20characterizes%20these%20client%20clusters%20%28zones%29%2C%20which%20in%20turn%20aids%0Ain%20recognizing%20and%20discarding%20malicious%20updates%20at%20the%20server.%20Our%20evaluation%0Aof%20FedZZ%20across%20two%20widely%20recognized%20datasets%3A%20CIFAR10%20and%20EMNIST%2C%20demonstrate%0Aits%20efficacy%20in%20mitigating%20data%20poisoning%20attacks%2C%20surpassing%20the%20performance%0Aof%20prevailing%20state-of-the-art%20methodologies%20in%20both%20single%20and%20multi-client%0Aattack%20scenarios%20and%20varying%20attack%20volumes.%20Notably%2C%20FedZZ%20also%20functions%20as%20a%0Arobust%20client%20selection%20strategy%2C%20even%20in%20highly%20non-IID%20and%20attack-free%0Ascenarios.%20Moreover%2C%20in%20the%20face%20of%20escalating%20poisoning%20rates%2C%20the%20model%0Aaccuracy%20attained%20by%20FedZZ%20displays%20superior%20resilience%20compared%20to%20existing%0Atechniques.%20For%20instance%2C%20when%20confronted%20with%20a%2050%25%20presence%20of%20malicious%0Aclients%2C%20FedZZ%20sustains%20an%20accuracy%20of%2067.43%25%2C%20while%20the%20accuracy%20of%20the%0Asecond-best%20solution%2C%20FL-Defender%2C%20diminishes%20to%2043.36%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04139v1&entry.124074799=Read"},
{"title": "Multi-modal perception for soft robotic interactions using generative\n  models", "author": "Enrico Donato and Egidio Falotico and Thomas George Thuruthel", "abstract": "  Perception is essential for the active interaction of physical agents with\nthe external environment. The integration of multiple sensory modalities, such\nas touch and vision, enhances this perceptual process, creating a more\ncomprehensive and robust understanding of the world. Such fusion is\nparticularly useful for highly deformable bodies such as soft robots.\nDeveloping a compact, yet comprehensive state representation from multi-sensory\ninputs can pave the way for the development of complex control strategies. This\npaper introduces a perception model that harmonizes data from diverse\nmodalities to build a holistic state representation and assimilate essential\ninformation. The model relies on the causality between sensory input and\nrobotic actions, employing a generative model to efficiently compress fused\ninformation and predict the next observation. We present, for the first time, a\nstudy on how touch can be predicted from vision and proprioception on soft\nrobots, the importance of the cross-modal generation and why this is essential\nfor soft robotic interactions in unstructured environments.\n", "link": "http://arxiv.org/abs/2404.04220v1", "date": "2024-04-05", "relevancy": 1.8524, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.7015}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5961}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5866}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20perception%20for%20soft%20robotic%20interactions%20using%20generative%0A%20%20models&body=Title%3A%20Multi-modal%20perception%20for%20soft%20robotic%20interactions%20using%20generative%0A%20%20models%0AAuthor%3A%20Enrico%20Donato%20and%20Egidio%20Falotico%20and%20Thomas%20George%20Thuruthel%0AAbstract%3A%20%20%20Perception%20is%20essential%20for%20the%20active%20interaction%20of%20physical%20agents%20with%0Athe%20external%20environment.%20The%20integration%20of%20multiple%20sensory%20modalities%2C%20such%0Aas%20touch%20and%20vision%2C%20enhances%20this%20perceptual%20process%2C%20creating%20a%20more%0Acomprehensive%20and%20robust%20understanding%20of%20the%20world.%20Such%20fusion%20is%0Aparticularly%20useful%20for%20highly%20deformable%20bodies%20such%20as%20soft%20robots.%0ADeveloping%20a%20compact%2C%20yet%20comprehensive%20state%20representation%20from%20multi-sensory%0Ainputs%20can%20pave%20the%20way%20for%20the%20development%20of%20complex%20control%20strategies.%20This%0Apaper%20introduces%20a%20perception%20model%20that%20harmonizes%20data%20from%20diverse%0Amodalities%20to%20build%20a%20holistic%20state%20representation%20and%20assimilate%20essential%0Ainformation.%20The%20model%20relies%20on%20the%20causality%20between%20sensory%20input%20and%0Arobotic%20actions%2C%20employing%20a%20generative%20model%20to%20efficiently%20compress%20fused%0Ainformation%20and%20predict%20the%20next%20observation.%20We%20present%2C%20for%20the%20first%20time%2C%20a%0Astudy%20on%20how%20touch%20can%20be%20predicted%20from%20vision%20and%20proprioception%20on%20soft%0Arobots%2C%20the%20importance%20of%20the%20cross-modal%20generation%20and%20why%20this%20is%20essential%0Afor%20soft%20robotic%20interactions%20in%20unstructured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04220v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20perception%20for%20soft%20robotic%20interactions%20using%20generative%0A%20%20models&entry.906535625=Enrico%20Donato%20and%20Egidio%20Falotico%20and%20Thomas%20George%20Thuruthel&entry.1292438233=%20%20Perception%20is%20essential%20for%20the%20active%20interaction%20of%20physical%20agents%20with%0Athe%20external%20environment.%20The%20integration%20of%20multiple%20sensory%20modalities%2C%20such%0Aas%20touch%20and%20vision%2C%20enhances%20this%20perceptual%20process%2C%20creating%20a%20more%0Acomprehensive%20and%20robust%20understanding%20of%20the%20world.%20Such%20fusion%20is%0Aparticularly%20useful%20for%20highly%20deformable%20bodies%20such%20as%20soft%20robots.%0ADeveloping%20a%20compact%2C%20yet%20comprehensive%20state%20representation%20from%20multi-sensory%0Ainputs%20can%20pave%20the%20way%20for%20the%20development%20of%20complex%20control%20strategies.%20This%0Apaper%20introduces%20a%20perception%20model%20that%20harmonizes%20data%20from%20diverse%0Amodalities%20to%20build%20a%20holistic%20state%20representation%20and%20assimilate%20essential%0Ainformation.%20The%20model%20relies%20on%20the%20causality%20between%20sensory%20input%20and%0Arobotic%20actions%2C%20employing%20a%20generative%20model%20to%20efficiently%20compress%20fused%0Ainformation%20and%20predict%20the%20next%20observation.%20We%20present%2C%20for%20the%20first%20time%2C%20a%0Astudy%20on%20how%20touch%20can%20be%20predicted%20from%20vision%20and%20proprioception%20on%20soft%0Arobots%2C%20the%20importance%20of%20the%20cross-modal%20generation%20and%20why%20this%20is%20essential%0Afor%20soft%20robotic%20interactions%20in%20unstructured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04220v1&entry.124074799=Read"},
{"title": "The Missing U for Efficient Diffusion Models", "author": "Sergio Calvo-Ordonez and Chun-Wun Cheng and Jiahao Huang and Lipei Zhang and Guang Yang and Carola-Bibiane Schonlieb and Angelica I Aviles-Rivero", "abstract": "  Diffusion Probabilistic Models stand as a critical tool in generative\nmodelling, enabling the generation of complex data distributions. This family\nof generative models yields record-breaking performance in tasks such as image\nsynthesis, video generation, and molecule design. Despite their capabilities,\ntheir efficiency, especially in the reverse process, remains a challenge due to\nslow convergence rates and high computational costs. In this paper, we\nintroduce an approach that leverages continuous dynamical systems to design a\nnovel denoising network for diffusion models that is more parameter-efficient,\nexhibits faster convergence, and demonstrates increased noise robustness.\nExperimenting with Denoising Diffusion Probabilistic Models (DDPMs), our\nframework operates with approximately a quarter of the parameters, and $\\sim$\n30\\% of the Floating Point Operations (FLOPs) compared to standard U-Nets in\nDDPMs. Furthermore, our model is notably faster in inference than the baseline\nwhen measured in fair and equal conditions. We also provide a mathematical\nintuition as to why our proposed reverse process is faster as well as a\nmathematical discussion of the empirical tradeoffs in the denoising downstream\ntask. Finally, we argue that our method is compatible with existing performance\nenhancement techniques, enabling further improvements in efficiency, quality,\nand speed.\n", "link": "http://arxiv.org/abs/2310.20092v4", "date": "2024-04-05", "relevancy": 1.8449, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6799}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.602}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5825}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Missing%20U%20for%20Efficient%20Diffusion%20Models&body=Title%3A%20The%20Missing%20U%20for%20Efficient%20Diffusion%20Models%0AAuthor%3A%20Sergio%20Calvo-Ordonez%20and%20Chun-Wun%20Cheng%20and%20Jiahao%20Huang%20and%20Lipei%20Zhang%20and%20Guang%20Yang%20and%20Carola-Bibiane%20Schonlieb%20and%20Angelica%20I%20Aviles-Rivero%0AAbstract%3A%20%20%20Diffusion%20Probabilistic%20Models%20stand%20as%20a%20critical%20tool%20in%20generative%0Amodelling%2C%20enabling%20the%20generation%20of%20complex%20data%20distributions.%20This%20family%0Aof%20generative%20models%20yields%20record-breaking%20performance%20in%20tasks%20such%20as%20image%0Asynthesis%2C%20video%20generation%2C%20and%20molecule%20design.%20Despite%20their%20capabilities%2C%0Atheir%20efficiency%2C%20especially%20in%20the%20reverse%20process%2C%20remains%20a%20challenge%20due%20to%0Aslow%20convergence%20rates%20and%20high%20computational%20costs.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20approach%20that%20leverages%20continuous%20dynamical%20systems%20to%20design%20a%0Anovel%20denoising%20network%20for%20diffusion%20models%20that%20is%20more%20parameter-efficient%2C%0Aexhibits%20faster%20convergence%2C%20and%20demonstrates%20increased%20noise%20robustness.%0AExperimenting%20with%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%2C%20our%0Aframework%20operates%20with%20approximately%20a%20quarter%20of%20the%20parameters%2C%20and%20%24%5Csim%24%0A30%5C%25%20of%20the%20Floating%20Point%20Operations%20%28FLOPs%29%20compared%20to%20standard%20U-Nets%20in%0ADDPMs.%20Furthermore%2C%20our%20model%20is%20notably%20faster%20in%20inference%20than%20the%20baseline%0Awhen%20measured%20in%20fair%20and%20equal%20conditions.%20We%20also%20provide%20a%20mathematical%0Aintuition%20as%20to%20why%20our%20proposed%20reverse%20process%20is%20faster%20as%20well%20as%20a%0Amathematical%20discussion%20of%20the%20empirical%20tradeoffs%20in%20the%20denoising%20downstream%0Atask.%20Finally%2C%20we%20argue%20that%20our%20method%20is%20compatible%20with%20existing%20performance%0Aenhancement%20techniques%2C%20enabling%20further%20improvements%20in%20efficiency%2C%20quality%2C%0Aand%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20092v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Missing%20U%20for%20Efficient%20Diffusion%20Models&entry.906535625=Sergio%20Calvo-Ordonez%20and%20Chun-Wun%20Cheng%20and%20Jiahao%20Huang%20and%20Lipei%20Zhang%20and%20Guang%20Yang%20and%20Carola-Bibiane%20Schonlieb%20and%20Angelica%20I%20Aviles-Rivero&entry.1292438233=%20%20Diffusion%20Probabilistic%20Models%20stand%20as%20a%20critical%20tool%20in%20generative%0Amodelling%2C%20enabling%20the%20generation%20of%20complex%20data%20distributions.%20This%20family%0Aof%20generative%20models%20yields%20record-breaking%20performance%20in%20tasks%20such%20as%20image%0Asynthesis%2C%20video%20generation%2C%20and%20molecule%20design.%20Despite%20their%20capabilities%2C%0Atheir%20efficiency%2C%20especially%20in%20the%20reverse%20process%2C%20remains%20a%20challenge%20due%20to%0Aslow%20convergence%20rates%20and%20high%20computational%20costs.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20approach%20that%20leverages%20continuous%20dynamical%20systems%20to%20design%20a%0Anovel%20denoising%20network%20for%20diffusion%20models%20that%20is%20more%20parameter-efficient%2C%0Aexhibits%20faster%20convergence%2C%20and%20demonstrates%20increased%20noise%20robustness.%0AExperimenting%20with%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%2C%20our%0Aframework%20operates%20with%20approximately%20a%20quarter%20of%20the%20parameters%2C%20and%20%24%5Csim%24%0A30%5C%25%20of%20the%20Floating%20Point%20Operations%20%28FLOPs%29%20compared%20to%20standard%20U-Nets%20in%0ADDPMs.%20Furthermore%2C%20our%20model%20is%20notably%20faster%20in%20inference%20than%20the%20baseline%0Awhen%20measured%20in%20fair%20and%20equal%20conditions.%20We%20also%20provide%20a%20mathematical%0Aintuition%20as%20to%20why%20our%20proposed%20reverse%20process%20is%20faster%20as%20well%20as%20a%0Amathematical%20discussion%20of%20the%20empirical%20tradeoffs%20in%20the%20denoising%20downstream%0Atask.%20Finally%2C%20we%20argue%20that%20our%20method%20is%20compatible%20with%20existing%20performance%0Aenhancement%20techniques%2C%20enabling%20further%20improvements%20in%20efficiency%2C%20quality%2C%0Aand%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20092v4&entry.124074799=Read"},
{"title": "Bidirectional Human Interactive AI Framework for Social Robot Navigation", "author": "Tuba Girgin and Emre Girgin and Yigit Yildirim and Emre Ugur and Mehmet Haklidir", "abstract": "  Trustworthiness is a crucial concept in the context of human-robot\ninteraction. Cooperative robots must be transparent regarding their\ndecision-making process, especially when operating in a human-oriented\nenvironment. This paper presents a comprehensive end-to-end framework aimed at\nfostering trustworthy bidirectional human-robot interaction in collaborative\nenvironments for the social navigation of mobile robots. Our method enables a\nmobile robot to predict the trajectory of people and adjust its route in a\nsocially-aware manner. In case of conflict between human and robot decisions,\ndetected through visual examination, the route is dynamically modified based on\nhuman preference while verbal communication is maintained. We present our\npipeline, framework design, and preliminary experiments that form the\nfoundation of our proposition.\n", "link": "http://arxiv.org/abs/2404.04069v1", "date": "2024-04-05", "relevancy": 1.8206, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.644}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5792}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5417}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bidirectional%20Human%20Interactive%20AI%20Framework%20for%20Social%20Robot%20Navigation&body=Title%3A%20Bidirectional%20Human%20Interactive%20AI%20Framework%20for%20Social%20Robot%20Navigation%0AAuthor%3A%20Tuba%20Girgin%20and%20Emre%20Girgin%20and%20Yigit%20Yildirim%20and%20Emre%20Ugur%20and%20Mehmet%20Haklidir%0AAbstract%3A%20%20%20Trustworthiness%20is%20a%20crucial%20concept%20in%20the%20context%20of%20human-robot%0Ainteraction.%20Cooperative%20robots%20must%20be%20transparent%20regarding%20their%0Adecision-making%20process%2C%20especially%20when%20operating%20in%20a%20human-oriented%0Aenvironment.%20This%20paper%20presents%20a%20comprehensive%20end-to-end%20framework%20aimed%20at%0Afostering%20trustworthy%20bidirectional%20human-robot%20interaction%20in%20collaborative%0Aenvironments%20for%20the%20social%20navigation%20of%20mobile%20robots.%20Our%20method%20enables%20a%0Amobile%20robot%20to%20predict%20the%20trajectory%20of%20people%20and%20adjust%20its%20route%20in%20a%0Asocially-aware%20manner.%20In%20case%20of%20conflict%20between%20human%20and%20robot%20decisions%2C%0Adetected%20through%20visual%20examination%2C%20the%20route%20is%20dynamically%20modified%20based%20on%0Ahuman%20preference%20while%20verbal%20communication%20is%20maintained.%20We%20present%20our%0Apipeline%2C%20framework%20design%2C%20and%20preliminary%20experiments%20that%20form%20the%0Afoundation%20of%20our%20proposition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04069v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidirectional%20Human%20Interactive%20AI%20Framework%20for%20Social%20Robot%20Navigation&entry.906535625=Tuba%20Girgin%20and%20Emre%20Girgin%20and%20Yigit%20Yildirim%20and%20Emre%20Ugur%20and%20Mehmet%20Haklidir&entry.1292438233=%20%20Trustworthiness%20is%20a%20crucial%20concept%20in%20the%20context%20of%20human-robot%0Ainteraction.%20Cooperative%20robots%20must%20be%20transparent%20regarding%20their%0Adecision-making%20process%2C%20especially%20when%20operating%20in%20a%20human-oriented%0Aenvironment.%20This%20paper%20presents%20a%20comprehensive%20end-to-end%20framework%20aimed%20at%0Afostering%20trustworthy%20bidirectional%20human-robot%20interaction%20in%20collaborative%0Aenvironments%20for%20the%20social%20navigation%20of%20mobile%20robots.%20Our%20method%20enables%20a%0Amobile%20robot%20to%20predict%20the%20trajectory%20of%20people%20and%20adjust%20its%20route%20in%20a%0Asocially-aware%20manner.%20In%20case%20of%20conflict%20between%20human%20and%20robot%20decisions%2C%0Adetected%20through%20visual%20examination%2C%20the%20route%20is%20dynamically%20modified%20based%20on%0Ahuman%20preference%20while%20verbal%20communication%20is%20maintained.%20We%20present%20our%0Apipeline%2C%20framework%20design%2C%20and%20preliminary%20experiments%20that%20form%20the%0Afoundation%20of%20our%20proposition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04069v1&entry.124074799=Read"},
{"title": "QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick\n  Damaged-building Detection", "author": "Yao Sun and Yi Wang and Michael Eineder", "abstract": "  Quick and automated earthquake-damaged building detection from post-event\nsatellite imagery is crucial, yet it is challenging due to the scarcity of\ntraining data required to develop robust algorithms. This letter presents the\nfirst dataset dedicated to detecting earthquake-damaged buildings from\npost-event very high resolution (VHR) Synthetic Aperture Radar (SAR) and\noptical imagery. Utilizing open satellite imagery and annotations acquired\nafter the 2023 Turkey-Syria earthquakes, we deliver a dataset of coregistered\nbuilding footprints and satellite image patches of both SAR and optical data,\nencompassing more than four thousand buildings. The task of damaged building\ndetection is formulated as a binary image classification problem, that can also\nbe treated as an anomaly detection problem due to extreme class imbalance. We\nprovide baseline methods and results to serve as references for comparison.\nResearchers can utilize this dataset to expedite algorithm development,\nfacilitating the rapid detection of damaged buildings in response to future\nevents. The dataset and codes together with detailed explanations and\nvisualization are made publicly available at\n\\url{https://github.com/ya0-sun/PostEQ-SARopt-BuildingDamage}.\n", "link": "http://arxiv.org/abs/2312.06587v2", "date": "2024-04-05", "relevancy": 1.8061, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4644}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4439}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4385}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20QuickQuakeBuildings%3A%20Post-earthquake%20SAR-Optical%20Dataset%20for%20Quick%0A%20%20Damaged-building%20Detection&body=Title%3A%20QuickQuakeBuildings%3A%20Post-earthquake%20SAR-Optical%20Dataset%20for%20Quick%0A%20%20Damaged-building%20Detection%0AAuthor%3A%20Yao%20Sun%20and%20Yi%20Wang%20and%20Michael%20Eineder%0AAbstract%3A%20%20%20Quick%20and%20automated%20earthquake-damaged%20building%20detection%20from%20post-event%0Asatellite%20imagery%20is%20crucial%2C%20yet%20it%20is%20challenging%20due%20to%20the%20scarcity%20of%0Atraining%20data%20required%20to%20develop%20robust%20algorithms.%20This%20letter%20presents%20the%0Afirst%20dataset%20dedicated%20to%20detecting%20earthquake-damaged%20buildings%20from%0Apost-event%20very%20high%20resolution%20%28VHR%29%20Synthetic%20Aperture%20Radar%20%28SAR%29%20and%0Aoptical%20imagery.%20Utilizing%20open%20satellite%20imagery%20and%20annotations%20acquired%0Aafter%20the%202023%20Turkey-Syria%20earthquakes%2C%20we%20deliver%20a%20dataset%20of%20coregistered%0Abuilding%20footprints%20and%20satellite%20image%20patches%20of%20both%20SAR%20and%20optical%20data%2C%0Aencompassing%20more%20than%20four%20thousand%20buildings.%20The%20task%20of%20damaged%20building%0Adetection%20is%20formulated%20as%20a%20binary%20image%20classification%20problem%2C%20that%20can%20also%0Abe%20treated%20as%20an%20anomaly%20detection%20problem%20due%20to%20extreme%20class%20imbalance.%20We%0Aprovide%20baseline%20methods%20and%20results%20to%20serve%20as%20references%20for%20comparison.%0AResearchers%20can%20utilize%20this%20dataset%20to%20expedite%20algorithm%20development%2C%0Afacilitating%20the%20rapid%20detection%20of%20damaged%20buildings%20in%20response%20to%20future%0Aevents.%20The%20dataset%20and%20codes%20together%20with%20detailed%20explanations%20and%0Avisualization%20are%20made%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ya0-sun/PostEQ-SARopt-BuildingDamage%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06587v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuickQuakeBuildings%3A%20Post-earthquake%20SAR-Optical%20Dataset%20for%20Quick%0A%20%20Damaged-building%20Detection&entry.906535625=Yao%20Sun%20and%20Yi%20Wang%20and%20Michael%20Eineder&entry.1292438233=%20%20Quick%20and%20automated%20earthquake-damaged%20building%20detection%20from%20post-event%0Asatellite%20imagery%20is%20crucial%2C%20yet%20it%20is%20challenging%20due%20to%20the%20scarcity%20of%0Atraining%20data%20required%20to%20develop%20robust%20algorithms.%20This%20letter%20presents%20the%0Afirst%20dataset%20dedicated%20to%20detecting%20earthquake-damaged%20buildings%20from%0Apost-event%20very%20high%20resolution%20%28VHR%29%20Synthetic%20Aperture%20Radar%20%28SAR%29%20and%0Aoptical%20imagery.%20Utilizing%20open%20satellite%20imagery%20and%20annotations%20acquired%0Aafter%20the%202023%20Turkey-Syria%20earthquakes%2C%20we%20deliver%20a%20dataset%20of%20coregistered%0Abuilding%20footprints%20and%20satellite%20image%20patches%20of%20both%20SAR%20and%20optical%20data%2C%0Aencompassing%20more%20than%20four%20thousand%20buildings.%20The%20task%20of%20damaged%20building%0Adetection%20is%20formulated%20as%20a%20binary%20image%20classification%20problem%2C%20that%20can%20also%0Abe%20treated%20as%20an%20anomaly%20detection%20problem%20due%20to%20extreme%20class%20imbalance.%20We%0Aprovide%20baseline%20methods%20and%20results%20to%20serve%20as%20references%20for%20comparison.%0AResearchers%20can%20utilize%20this%20dataset%20to%20expedite%20algorithm%20development%2C%0Afacilitating%20the%20rapid%20detection%20of%20damaged%20buildings%20in%20response%20to%20future%0Aevents.%20The%20dataset%20and%20codes%20together%20with%20detailed%20explanations%20and%0Avisualization%20are%20made%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ya0-sun/PostEQ-SARopt-BuildingDamage%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06587v2&entry.124074799=Read"},
{"title": "Probabilistically Informed Robot Object Search with Multiple Regions", "author": "Matthew Collins and Jared J. Beard and Nicholas Ohi and Yu Gu", "abstract": "  The increasing use of autonomous robot systems in hazardous environments\nunderscores the need for efficient search and rescue operations. Despite\nsignificant advancements, existing literature on object search often falls\nshort in overcoming the difficulty of long planning horizons and dealing with\nsensor limitations, such as noise. This study introduces a novel approach that\nformulates the search problem as a belief Markov decision processes with\noptions (BMDP-O) to make Monte Carlo tree search (MCTS) a viable tool for\novercoming these challenges in large scale environments. The proposed\nformulation incorporates sequences of actions (options) to move between regions\nof interest, enabling the algorithm to efficiently scale to large environments.\nThis approach also enables the use of customizable fields of view, for use with\nmultiple types of sensors. Experimental results demonstrate the superiority of\nthis approach in large environments when compared to the problem without\noptions and alternative tools such as receding horizon planners. Given compute\ntime for the proposed formulation is relatively high, a further approximated\n\"lite\" formulation is proposed. The lite formulation finds objects in a\ncomparable number of steps with faster computation.\n", "link": "http://arxiv.org/abs/2404.04186v1", "date": "2024-04-05", "relevancy": 1.7943, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6498}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6323}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5637}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probabilistically%20Informed%20Robot%20Object%20Search%20with%20Multiple%20Regions&body=Title%3A%20Probabilistically%20Informed%20Robot%20Object%20Search%20with%20Multiple%20Regions%0AAuthor%3A%20Matthew%20Collins%20and%20Jared%20J.%20Beard%20and%20Nicholas%20Ohi%20and%20Yu%20Gu%0AAbstract%3A%20%20%20The%20increasing%20use%20of%20autonomous%20robot%20systems%20in%20hazardous%20environments%0Aunderscores%20the%20need%20for%20efficient%20search%20and%20rescue%20operations.%20Despite%0Asignificant%20advancements%2C%20existing%20literature%20on%20object%20search%20often%20falls%0Ashort%20in%20overcoming%20the%20difficulty%20of%20long%20planning%20horizons%20and%20dealing%20with%0Asensor%20limitations%2C%20such%20as%20noise.%20This%20study%20introduces%20a%20novel%20approach%20that%0Aformulates%20the%20search%20problem%20as%20a%20belief%20Markov%20decision%20processes%20with%0Aoptions%20%28BMDP-O%29%20to%20make%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20a%20viable%20tool%20for%0Aovercoming%20these%20challenges%20in%20large%20scale%20environments.%20The%20proposed%0Aformulation%20incorporates%20sequences%20of%20actions%20%28options%29%20to%20move%20between%20regions%0Aof%20interest%2C%20enabling%20the%20algorithm%20to%20efficiently%20scale%20to%20large%20environments.%0AThis%20approach%20also%20enables%20the%20use%20of%20customizable%20fields%20of%20view%2C%20for%20use%20with%0Amultiple%20types%20of%20sensors.%20Experimental%20results%20demonstrate%20the%20superiority%20of%0Athis%20approach%20in%20large%20environments%20when%20compared%20to%20the%20problem%20without%0Aoptions%20and%20alternative%20tools%20such%20as%20receding%20horizon%20planners.%20Given%20compute%0Atime%20for%20the%20proposed%20formulation%20is%20relatively%20high%2C%20a%20further%20approximated%0A%22lite%22%20formulation%20is%20proposed.%20The%20lite%20formulation%20finds%20objects%20in%20a%0Acomparable%20number%20of%20steps%20with%20faster%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04186v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistically%20Informed%20Robot%20Object%20Search%20with%20Multiple%20Regions&entry.906535625=Matthew%20Collins%20and%20Jared%20J.%20Beard%20and%20Nicholas%20Ohi%20and%20Yu%20Gu&entry.1292438233=%20%20The%20increasing%20use%20of%20autonomous%20robot%20systems%20in%20hazardous%20environments%0Aunderscores%20the%20need%20for%20efficient%20search%20and%20rescue%20operations.%20Despite%0Asignificant%20advancements%2C%20existing%20literature%20on%20object%20search%20often%20falls%0Ashort%20in%20overcoming%20the%20difficulty%20of%20long%20planning%20horizons%20and%20dealing%20with%0Asensor%20limitations%2C%20such%20as%20noise.%20This%20study%20introduces%20a%20novel%20approach%20that%0Aformulates%20the%20search%20problem%20as%20a%20belief%20Markov%20decision%20processes%20with%0Aoptions%20%28BMDP-O%29%20to%20make%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20a%20viable%20tool%20for%0Aovercoming%20these%20challenges%20in%20large%20scale%20environments.%20The%20proposed%0Aformulation%20incorporates%20sequences%20of%20actions%20%28options%29%20to%20move%20between%20regions%0Aof%20interest%2C%20enabling%20the%20algorithm%20to%20efficiently%20scale%20to%20large%20environments.%0AThis%20approach%20also%20enables%20the%20use%20of%20customizable%20fields%20of%20view%2C%20for%20use%20with%0Amultiple%20types%20of%20sensors.%20Experimental%20results%20demonstrate%20the%20superiority%20of%0Athis%20approach%20in%20large%20environments%20when%20compared%20to%20the%20problem%20without%0Aoptions%20and%20alternative%20tools%20such%20as%20receding%20horizon%20planners.%20Given%20compute%0Atime%20for%20the%20proposed%20formulation%20is%20relatively%20high%2C%20a%20further%20approximated%0A%22lite%22%20formulation%20is%20proposed.%20The%20lite%20formulation%20finds%20objects%20in%20a%0Acomparable%20number%20of%20steps%20with%20faster%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04186v1&entry.124074799=Read"},
{"title": "Multimodal Learning for Materials", "author": "Viggo Moro and Charlotte Loh and Rumen Dangovski and Ali Ghorashi and Andrew Ma and Zhuo Chen and Peter Y. Lu and Thomas Christensen and Marin Solja\u010di\u0107", "abstract": "  Artificial intelligence is transforming computational materials science,\nimproving the prediction of material properties, and accelerating the discovery\nof novel materials. Recently, publicly available material data repositories\nhave grown rapidly. This growth encompasses not only more materials, but also a\ngreater variety and quantity of their associated properties. Existing machine\nlearning efforts in materials science focus primarily on single-modality tasks,\ni.e., relationships between materials and a single physical property, thus not\ntaking advantage of the rich and multimodal set of material properties. Here,\nwe introduce Multimodal Learning for Materials (MultiMat), which enables\nself-supervised multi-modality training of foundation models for materials. We\ndemonstrate our framework's potential using data from the Materials Project\ndatabase on multiple axes: (i) MultiMat achieves state-of-the-art performance\nfor challenging material property prediction tasks; (ii) MultiMat enables novel\nand accurate material discovery via latent space similarity, enabling screening\nfor stable materials with desired properties; and (iii) MultiMat encodes\ninterpretable emergent features that may provide novel scientific insights.\n", "link": "http://arxiv.org/abs/2312.00111v2", "date": "2024-04-05", "relevancy": 1.7921, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5923}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5848}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Learning%20for%20Materials&body=Title%3A%20Multimodal%20Learning%20for%20Materials%0AAuthor%3A%20Viggo%20Moro%20and%20Charlotte%20Loh%20and%20Rumen%20Dangovski%20and%20Ali%20Ghorashi%20and%20Andrew%20Ma%20and%20Zhuo%20Chen%20and%20Peter%20Y.%20Lu%20and%20Thomas%20Christensen%20and%20Marin%20Solja%C4%8Di%C4%87%0AAbstract%3A%20%20%20Artificial%20intelligence%20is%20transforming%20computational%20materials%20science%2C%0Aimproving%20the%20prediction%20of%20material%20properties%2C%20and%20accelerating%20the%20discovery%0Aof%20novel%20materials.%20Recently%2C%20publicly%20available%20material%20data%20repositories%0Ahave%20grown%20rapidly.%20This%20growth%20encompasses%20not%20only%20more%20materials%2C%20but%20also%20a%0Agreater%20variety%20and%20quantity%20of%20their%20associated%20properties.%20Existing%20machine%0Alearning%20efforts%20in%20materials%20science%20focus%20primarily%20on%20single-modality%20tasks%2C%0Ai.e.%2C%20relationships%20between%20materials%20and%20a%20single%20physical%20property%2C%20thus%20not%0Ataking%20advantage%20of%20the%20rich%20and%20multimodal%20set%20of%20material%20properties.%20Here%2C%0Awe%20introduce%20Multimodal%20Learning%20for%20Materials%20%28MultiMat%29%2C%20which%20enables%0Aself-supervised%20multi-modality%20training%20of%20foundation%20models%20for%20materials.%20We%0Ademonstrate%20our%20framework%27s%20potential%20using%20data%20from%20the%20Materials%20Project%0Adatabase%20on%20multiple%20axes%3A%20%28i%29%20MultiMat%20achieves%20state-of-the-art%20performance%0Afor%20challenging%20material%20property%20prediction%20tasks%3B%20%28ii%29%20MultiMat%20enables%20novel%0Aand%20accurate%20material%20discovery%20via%20latent%20space%20similarity%2C%20enabling%20screening%0Afor%20stable%20materials%20with%20desired%20properties%3B%20and%20%28iii%29%20MultiMat%20encodes%0Ainterpretable%20emergent%20features%20that%20may%20provide%20novel%20scientific%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00111v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Learning%20for%20Materials&entry.906535625=Viggo%20Moro%20and%20Charlotte%20Loh%20and%20Rumen%20Dangovski%20and%20Ali%20Ghorashi%20and%20Andrew%20Ma%20and%20Zhuo%20Chen%20and%20Peter%20Y.%20Lu%20and%20Thomas%20Christensen%20and%20Marin%20Solja%C4%8Di%C4%87&entry.1292438233=%20%20Artificial%20intelligence%20is%20transforming%20computational%20materials%20science%2C%0Aimproving%20the%20prediction%20of%20material%20properties%2C%20and%20accelerating%20the%20discovery%0Aof%20novel%20materials.%20Recently%2C%20publicly%20available%20material%20data%20repositories%0Ahave%20grown%20rapidly.%20This%20growth%20encompasses%20not%20only%20more%20materials%2C%20but%20also%20a%0Agreater%20variety%20and%20quantity%20of%20their%20associated%20properties.%20Existing%20machine%0Alearning%20efforts%20in%20materials%20science%20focus%20primarily%20on%20single-modality%20tasks%2C%0Ai.e.%2C%20relationships%20between%20materials%20and%20a%20single%20physical%20property%2C%20thus%20not%0Ataking%20advantage%20of%20the%20rich%20and%20multimodal%20set%20of%20material%20properties.%20Here%2C%0Awe%20introduce%20Multimodal%20Learning%20for%20Materials%20%28MultiMat%29%2C%20which%20enables%0Aself-supervised%20multi-modality%20training%20of%20foundation%20models%20for%20materials.%20We%0Ademonstrate%20our%20framework%27s%20potential%20using%20data%20from%20the%20Materials%20Project%0Adatabase%20on%20multiple%20axes%3A%20%28i%29%20MultiMat%20achieves%20state-of-the-art%20performance%0Afor%20challenging%20material%20property%20prediction%20tasks%3B%20%28ii%29%20MultiMat%20enables%20novel%0Aand%20accurate%20material%20discovery%20via%20latent%20space%20similarity%2C%20enabling%20screening%0Afor%20stable%20materials%20with%20desired%20properties%3B%20and%20%28iii%29%20MultiMat%20encodes%0Ainterpretable%20emergent%20features%20that%20may%20provide%20novel%20scientific%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00111v2&entry.124074799=Read"},
{"title": "Towards Efficient and Accurate CT Segmentation via Edge-Preserving\n  Probabilistic Downsampling", "author": "Shahzad Ali and Yu Rim Lee and Soo Young Park and Won Young Tak and Soon Ki Jung", "abstract": "  Downsampling images and labels, often necessitated by limited resources or to\nexpedite network training, leads to the loss of small objects and thin\nboundaries. This undermines the segmentation network's capacity to interpret\nimages accurately and predict detailed labels, resulting in diminished\nperformance compared to processing at original resolutions. This situation\nexemplifies the trade-off between efficiency and accuracy, with higher\ndownsampling factors further impairing segmentation outcomes. Preserving\ninformation during downsampling is especially critical for medical image\nsegmentation tasks. To tackle this challenge, we introduce a novel method named\nEdge-preserving Probabilistic Downsampling (EPD). It utilizes class uncertainty\nwithin a local window to produce soft labels, with the window size dictating\nthe downsampling factor. This enables a network to produce quality predictions\nat low resolutions. Beyond preserving edge details more effectively than\nconventional nearest-neighbor downsampling, employing a similar algorithm for\nimages, it surpasses bilinear interpolation in image downsampling, enhancing\noverall performance. Our method significantly improved Intersection over Union\n(IoU) to 2.85%, 8.65%, and 11.89% when downsampling data to 1/2, 1/4, and 1/8,\nrespectively, compared to conventional interpolation methods.\n", "link": "http://arxiv.org/abs/2404.03991v1", "date": "2024-04-05", "relevancy": 1.7881, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6251}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5687}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5507}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20and%20Accurate%20CT%20Segmentation%20via%20Edge-Preserving%0A%20%20Probabilistic%20Downsampling&body=Title%3A%20Towards%20Efficient%20and%20Accurate%20CT%20Segmentation%20via%20Edge-Preserving%0A%20%20Probabilistic%20Downsampling%0AAuthor%3A%20Shahzad%20Ali%20and%20Yu%20Rim%20Lee%20and%20Soo%20Young%20Park%20and%20Won%20Young%20Tak%20and%20Soon%20Ki%20Jung%0AAbstract%3A%20%20%20Downsampling%20images%20and%20labels%2C%20often%20necessitated%20by%20limited%20resources%20or%20to%0Aexpedite%20network%20training%2C%20leads%20to%20the%20loss%20of%20small%20objects%20and%20thin%0Aboundaries.%20This%20undermines%20the%20segmentation%20network%27s%20capacity%20to%20interpret%0Aimages%20accurately%20and%20predict%20detailed%20labels%2C%20resulting%20in%20diminished%0Aperformance%20compared%20to%20processing%20at%20original%20resolutions.%20This%20situation%0Aexemplifies%20the%20trade-off%20between%20efficiency%20and%20accuracy%2C%20with%20higher%0Adownsampling%20factors%20further%20impairing%20segmentation%20outcomes.%20Preserving%0Ainformation%20during%20downsampling%20is%20especially%20critical%20for%20medical%20image%0Asegmentation%20tasks.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%20novel%20method%20named%0AEdge-preserving%20Probabilistic%20Downsampling%20%28EPD%29.%20It%20utilizes%20class%20uncertainty%0Awithin%20a%20local%20window%20to%20produce%20soft%20labels%2C%20with%20the%20window%20size%20dictating%0Athe%20downsampling%20factor.%20This%20enables%20a%20network%20to%20produce%20quality%20predictions%0Aat%20low%20resolutions.%20Beyond%20preserving%20edge%20details%20more%20effectively%20than%0Aconventional%20nearest-neighbor%20downsampling%2C%20employing%20a%20similar%20algorithm%20for%0Aimages%2C%20it%20surpasses%20bilinear%20interpolation%20in%20image%20downsampling%2C%20enhancing%0Aoverall%20performance.%20Our%20method%20significantly%20improved%20Intersection%20over%20Union%0A%28IoU%29%20to%202.85%25%2C%208.65%25%2C%20and%2011.89%25%20when%20downsampling%20data%20to%201/2%2C%201/4%2C%20and%201/8%2C%0Arespectively%2C%20compared%20to%20conventional%20interpolation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03991v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20and%20Accurate%20CT%20Segmentation%20via%20Edge-Preserving%0A%20%20Probabilistic%20Downsampling&entry.906535625=Shahzad%20Ali%20and%20Yu%20Rim%20Lee%20and%20Soo%20Young%20Park%20and%20Won%20Young%20Tak%20and%20Soon%20Ki%20Jung&entry.1292438233=%20%20Downsampling%20images%20and%20labels%2C%20often%20necessitated%20by%20limited%20resources%20or%20to%0Aexpedite%20network%20training%2C%20leads%20to%20the%20loss%20of%20small%20objects%20and%20thin%0Aboundaries.%20This%20undermines%20the%20segmentation%20network%27s%20capacity%20to%20interpret%0Aimages%20accurately%20and%20predict%20detailed%20labels%2C%20resulting%20in%20diminished%0Aperformance%20compared%20to%20processing%20at%20original%20resolutions.%20This%20situation%0Aexemplifies%20the%20trade-off%20between%20efficiency%20and%20accuracy%2C%20with%20higher%0Adownsampling%20factors%20further%20impairing%20segmentation%20outcomes.%20Preserving%0Ainformation%20during%20downsampling%20is%20especially%20critical%20for%20medical%20image%0Asegmentation%20tasks.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%20novel%20method%20named%0AEdge-preserving%20Probabilistic%20Downsampling%20%28EPD%29.%20It%20utilizes%20class%20uncertainty%0Awithin%20a%20local%20window%20to%20produce%20soft%20labels%2C%20with%20the%20window%20size%20dictating%0Athe%20downsampling%20factor.%20This%20enables%20a%20network%20to%20produce%20quality%20predictions%0Aat%20low%20resolutions.%20Beyond%20preserving%20edge%20details%20more%20effectively%20than%0Aconventional%20nearest-neighbor%20downsampling%2C%20employing%20a%20similar%20algorithm%20for%0Aimages%2C%20it%20surpasses%20bilinear%20interpolation%20in%20image%20downsampling%2C%20enhancing%0Aoverall%20performance.%20Our%20method%20significantly%20improved%20Intersection%20over%20Union%0A%28IoU%29%20to%202.85%25%2C%208.65%25%2C%20and%2011.89%25%20when%20downsampling%20data%20to%201/2%2C%201/4%2C%20and%201/8%2C%0Arespectively%2C%20compared%20to%20conventional%20interpolation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03991v1&entry.124074799=Read"},
{"title": "WorDepth: Variational Language Prior for Monocular Depth Estimation", "author": "Ziyao Zeng and Daniel Wang and Fengyu Yang and Hyoungseob Park and Yangchao Wu and Stefano Soatto and Byung-Woo Hong and Dong Lao and Alex Wong", "abstract": "  Three-dimensional (3D) reconstruction from a single image is an ill-posed\nproblem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text\ndescription(s) is similarly ill-posed, i.e. spatial arrangements of objects\ndescribed. We investigate the question of whether two inherently ambiguous\nmodalities can be used in conjunction to produce metric-scaled reconstructions.\nTo test this, we focus on monocular depth estimation, the problem of predicting\na dense depth map from a single image, but with an additional text caption\ndescribing the scene. To this end, we begin by encoding the text caption as a\nmean and standard deviation; using a variational framework, we learn the\ndistribution of the plausible metric reconstructions of 3D scenes corresponding\nto the text captions as a prior. To \"select\" a specific reconstruction or depth\nmap, we encode the given image through a conditional sampler that samples from\nthe latent space of the variational text encoder, which is then decoded to the\noutput depth map. Our approach is trained alternatingly between the text and\nimage branches: in one optimization step, we predict the mean and standard\ndeviation from the text description and sample from a standard Gaussian, and in\nthe other, we sample using a (image) conditional sampler. Once trained, we\ndirectly predict depth from the encoded text using the conditional sampler. We\ndemonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where\nwe show that language can consistently improve performance in both.\n", "link": "http://arxiv.org/abs/2404.03635v2", "date": "2024-04-05", "relevancy": 1.7833, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5916}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5856}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WorDepth%3A%20Variational%20Language%20Prior%20for%20Monocular%20Depth%20Estimation&body=Title%3A%20WorDepth%3A%20Variational%20Language%20Prior%20for%20Monocular%20Depth%20Estimation%0AAuthor%3A%20Ziyao%20Zeng%20and%20Daniel%20Wang%20and%20Fengyu%20Yang%20and%20Hyoungseob%20Park%20and%20Yangchao%20Wu%20and%20Stefano%20Soatto%20and%20Byung-Woo%20Hong%20and%20Dong%20Lao%20and%20Alex%20Wong%0AAbstract%3A%20%20%20Three-dimensional%20%283D%29%20reconstruction%20from%20a%20single%20image%20is%20an%20ill-posed%0Aproblem%20with%20inherent%20ambiguities%2C%20i.e.%20scale.%20Predicting%20a%203D%20scene%20from%20text%0Adescription%28s%29%20is%20similarly%20ill-posed%2C%20i.e.%20spatial%20arrangements%20of%20objects%0Adescribed.%20We%20investigate%20the%20question%20of%20whether%20two%20inherently%20ambiguous%0Amodalities%20can%20be%20used%20in%20conjunction%20to%20produce%20metric-scaled%20reconstructions.%0ATo%20test%20this%2C%20we%20focus%20on%20monocular%20depth%20estimation%2C%20the%20problem%20of%20predicting%0Aa%20dense%20depth%20map%20from%20a%20single%20image%2C%20but%20with%20an%20additional%20text%20caption%0Adescribing%20the%20scene.%20To%20this%20end%2C%20we%20begin%20by%20encoding%20the%20text%20caption%20as%20a%0Amean%20and%20standard%20deviation%3B%20using%20a%20variational%20framework%2C%20we%20learn%20the%0Adistribution%20of%20the%20plausible%20metric%20reconstructions%20of%203D%20scenes%20corresponding%0Ato%20the%20text%20captions%20as%20a%20prior.%20To%20%22select%22%20a%20specific%20reconstruction%20or%20depth%0Amap%2C%20we%20encode%20the%20given%20image%20through%20a%20conditional%20sampler%20that%20samples%20from%0Athe%20latent%20space%20of%20the%20variational%20text%20encoder%2C%20which%20is%20then%20decoded%20to%20the%0Aoutput%20depth%20map.%20Our%20approach%20is%20trained%20alternatingly%20between%20the%20text%20and%0Aimage%20branches%3A%20in%20one%20optimization%20step%2C%20we%20predict%20the%20mean%20and%20standard%0Adeviation%20from%20the%20text%20description%20and%20sample%20from%20a%20standard%20Gaussian%2C%20and%20in%0Athe%20other%2C%20we%20sample%20using%20a%20%28image%29%20conditional%20sampler.%20Once%20trained%2C%20we%0Adirectly%20predict%20depth%20from%20the%20encoded%20text%20using%20the%20conditional%20sampler.%20We%0Ademonstrate%20our%20approach%20on%20indoor%20%28NYUv2%29%20and%20outdoor%20%28KITTI%29%20scenarios%2C%20where%0Awe%20show%20that%20language%20can%20consistently%20improve%20performance%20in%20both.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03635v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorDepth%3A%20Variational%20Language%20Prior%20for%20Monocular%20Depth%20Estimation&entry.906535625=Ziyao%20Zeng%20and%20Daniel%20Wang%20and%20Fengyu%20Yang%20and%20Hyoungseob%20Park%20and%20Yangchao%20Wu%20and%20Stefano%20Soatto%20and%20Byung-Woo%20Hong%20and%20Dong%20Lao%20and%20Alex%20Wong&entry.1292438233=%20%20Three-dimensional%20%283D%29%20reconstruction%20from%20a%20single%20image%20is%20an%20ill-posed%0Aproblem%20with%20inherent%20ambiguities%2C%20i.e.%20scale.%20Predicting%20a%203D%20scene%20from%20text%0Adescription%28s%29%20is%20similarly%20ill-posed%2C%20i.e.%20spatial%20arrangements%20of%20objects%0Adescribed.%20We%20investigate%20the%20question%20of%20whether%20two%20inherently%20ambiguous%0Amodalities%20can%20be%20used%20in%20conjunction%20to%20produce%20metric-scaled%20reconstructions.%0ATo%20test%20this%2C%20we%20focus%20on%20monocular%20depth%20estimation%2C%20the%20problem%20of%20predicting%0Aa%20dense%20depth%20map%20from%20a%20single%20image%2C%20but%20with%20an%20additional%20text%20caption%0Adescribing%20the%20scene.%20To%20this%20end%2C%20we%20begin%20by%20encoding%20the%20text%20caption%20as%20a%0Amean%20and%20standard%20deviation%3B%20using%20a%20variational%20framework%2C%20we%20learn%20the%0Adistribution%20of%20the%20plausible%20metric%20reconstructions%20of%203D%20scenes%20corresponding%0Ato%20the%20text%20captions%20as%20a%20prior.%20To%20%22select%22%20a%20specific%20reconstruction%20or%20depth%0Amap%2C%20we%20encode%20the%20given%20image%20through%20a%20conditional%20sampler%20that%20samples%20from%0Athe%20latent%20space%20of%20the%20variational%20text%20encoder%2C%20which%20is%20then%20decoded%20to%20the%0Aoutput%20depth%20map.%20Our%20approach%20is%20trained%20alternatingly%20between%20the%20text%20and%0Aimage%20branches%3A%20in%20one%20optimization%20step%2C%20we%20predict%20the%20mean%20and%20standard%0Adeviation%20from%20the%20text%20description%20and%20sample%20from%20a%20standard%20Gaussian%2C%20and%20in%0Athe%20other%2C%20we%20sample%20using%20a%20%28image%29%20conditional%20sampler.%20Once%20trained%2C%20we%0Adirectly%20predict%20depth%20from%20the%20encoded%20text%20using%20the%20conditional%20sampler.%20We%0Ademonstrate%20our%20approach%20on%20indoor%20%28NYUv2%29%20and%20outdoor%20%28KITTI%29%20scenarios%2C%20where%0Awe%20show%20that%20language%20can%20consistently%20improve%20performance%20in%20both.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03635v2&entry.124074799=Read"},
{"title": "Score identity Distillation: Exponentially Fast Distillation of\n  Pretrained Diffusion Models for One-Step Generation", "author": "Mingyuan Zhou and Huangjie Zheng and Zhendong Wang and Mingzhang Yin and Hai Huang", "abstract": "  We introduce Score identity Distillation (SiD), an innovative data-free\nmethod that distills the generative capabilities of pretrained diffusion models\ninto a single-step generator. SiD not only facilitates an exponentially fast\nreduction in Fr\\'echet inception distance (FID) during distillation but also\napproaches or even exceeds the FID performance of the original teacher\ndiffusion models. By reformulating forward diffusion processes as semi-implicit\ndistributions, we leverage three score-related identities to create an\ninnovative loss mechanism. This mechanism achieves rapid FID reduction by\ntraining the generator using its own synthesized images, eliminating the need\nfor real data or reverse-diffusion-based generation, all accomplished within\nsignificantly shortened generation time. Upon evaluation across four benchmark\ndatasets, the SiD algorithm demonstrates high iteration efficiency during\ndistillation and surpasses competing distillation approaches, whether they are\none-step or few-step, data-free, or dependent on training data, in terms of\ngeneration quality. This achievement not only redefines the benchmarks for\nefficiency and effectiveness in diffusion distillation but also in the broader\nfield of diffusion-based generation. Our PyTorch implementation will be\npublicly accessible on GitHub.\n", "link": "http://arxiv.org/abs/2404.04057v1", "date": "2024-04-05", "relevancy": 1.7432, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5929}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.58}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5768}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Score%20identity%20Distillation%3A%20Exponentially%20Fast%20Distillation%20of%0A%20%20Pretrained%20Diffusion%20Models%20for%20One-Step%20Generation&body=Title%3A%20Score%20identity%20Distillation%3A%20Exponentially%20Fast%20Distillation%20of%0A%20%20Pretrained%20Diffusion%20Models%20for%20One-Step%20Generation%0AAuthor%3A%20Mingyuan%20Zhou%20and%20Huangjie%20Zheng%20and%20Zhendong%20Wang%20and%20Mingzhang%20Yin%20and%20Hai%20Huang%0AAbstract%3A%20%20%20We%20introduce%20Score%20identity%20Distillation%20%28SiD%29%2C%20an%20innovative%20data-free%0Amethod%20that%20distills%20the%20generative%20capabilities%20of%20pretrained%20diffusion%20models%0Ainto%20a%20single-step%20generator.%20SiD%20not%20only%20facilitates%20an%20exponentially%20fast%0Areduction%20in%20Fr%5C%27echet%20inception%20distance%20%28FID%29%20during%20distillation%20but%20also%0Aapproaches%20or%20even%20exceeds%20the%20FID%20performance%20of%20the%20original%20teacher%0Adiffusion%20models.%20By%20reformulating%20forward%20diffusion%20processes%20as%20semi-implicit%0Adistributions%2C%20we%20leverage%20three%20score-related%20identities%20to%20create%20an%0Ainnovative%20loss%20mechanism.%20This%20mechanism%20achieves%20rapid%20FID%20reduction%20by%0Atraining%20the%20generator%20using%20its%20own%20synthesized%20images%2C%20eliminating%20the%20need%0Afor%20real%20data%20or%20reverse-diffusion-based%20generation%2C%20all%20accomplished%20within%0Asignificantly%20shortened%20generation%20time.%20Upon%20evaluation%20across%20four%20benchmark%0Adatasets%2C%20the%20SiD%20algorithm%20demonstrates%20high%20iteration%20efficiency%20during%0Adistillation%20and%20surpasses%20competing%20distillation%20approaches%2C%20whether%20they%20are%0Aone-step%20or%20few-step%2C%20data-free%2C%20or%20dependent%20on%20training%20data%2C%20in%20terms%20of%0Ageneration%20quality.%20This%20achievement%20not%20only%20redefines%20the%20benchmarks%20for%0Aefficiency%20and%20effectiveness%20in%20diffusion%20distillation%20but%20also%20in%20the%20broader%0Afield%20of%20diffusion-based%20generation.%20Our%20PyTorch%20implementation%20will%20be%0Apublicly%20accessible%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04057v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Score%20identity%20Distillation%3A%20Exponentially%20Fast%20Distillation%20of%0A%20%20Pretrained%20Diffusion%20Models%20for%20One-Step%20Generation&entry.906535625=Mingyuan%20Zhou%20and%20Huangjie%20Zheng%20and%20Zhendong%20Wang%20and%20Mingzhang%20Yin%20and%20Hai%20Huang&entry.1292438233=%20%20We%20introduce%20Score%20identity%20Distillation%20%28SiD%29%2C%20an%20innovative%20data-free%0Amethod%20that%20distills%20the%20generative%20capabilities%20of%20pretrained%20diffusion%20models%0Ainto%20a%20single-step%20generator.%20SiD%20not%20only%20facilitates%20an%20exponentially%20fast%0Areduction%20in%20Fr%5C%27echet%20inception%20distance%20%28FID%29%20during%20distillation%20but%20also%0Aapproaches%20or%20even%20exceeds%20the%20FID%20performance%20of%20the%20original%20teacher%0Adiffusion%20models.%20By%20reformulating%20forward%20diffusion%20processes%20as%20semi-implicit%0Adistributions%2C%20we%20leverage%20three%20score-related%20identities%20to%20create%20an%0Ainnovative%20loss%20mechanism.%20This%20mechanism%20achieves%20rapid%20FID%20reduction%20by%0Atraining%20the%20generator%20using%20its%20own%20synthesized%20images%2C%20eliminating%20the%20need%0Afor%20real%20data%20or%20reverse-diffusion-based%20generation%2C%20all%20accomplished%20within%0Asignificantly%20shortened%20generation%20time.%20Upon%20evaluation%20across%20four%20benchmark%0Adatasets%2C%20the%20SiD%20algorithm%20demonstrates%20high%20iteration%20efficiency%20during%0Adistillation%20and%20surpasses%20competing%20distillation%20approaches%2C%20whether%20they%20are%0Aone-step%20or%20few-step%2C%20data-free%2C%20or%20dependent%20on%20training%20data%2C%20in%20terms%20of%0Ageneration%20quality.%20This%20achievement%20not%20only%20redefines%20the%20benchmarks%20for%0Aefficiency%20and%20effectiveness%20in%20diffusion%20distillation%20but%20also%20in%20the%20broader%0Afield%20of%20diffusion-based%20generation.%20Our%20PyTorch%20implementation%20will%20be%0Apublicly%20accessible%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04057v1&entry.124074799=Read"},
{"title": "DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative\n  Epipolar Sampling and Refinement Toward Equilibrium", "author": "Antyanta Bangunharcana and Ahmed Magd and Kyung-Soo Kim", "abstract": "  Self-supervised multi-frame depth estimation achieves high accuracy by\ncomputing matching costs of pixel correspondences between adjacent frames,\ninjecting geometric information into the network. These pixel-correspondence\ncandidates are computed based on the relative pose estimates between the\nframes. Accurate pose predictions are essential for precise matching cost\ncomputation as they influence the epipolar geometry. Furthermore, improved\ndepth estimates can, in turn, be used to align pose estimates.\n  Inspired by traditional structure-from-motion (SfM) principles, we propose\nthe DualRefine model, which tightly couples depth and pose estimation through a\nfeedback loop. Our novel update pipeline uses a deep equilibrium model\nframework to iteratively refine depth estimates and a hidden state of feature\nmaps by computing local matching costs based on epipolar geometry. Importantly,\nwe used the refined depth estimates and feature maps to compute pose updates at\neach step. This update in the pose estimates slowly alters the epipolar\ngeometry during the refinement process. Experimental results on the KITTI\ndataset demonstrate competitive depth prediction and odometry prediction\nperformance surpassing published self-supervised baselines.\n", "link": "http://arxiv.org/abs/2304.03560v2", "date": "2024-04-05", "relevancy": 1.7333, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5907}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5641}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.559}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DualRefine%3A%20Self-Supervised%20Depth%20and%20Pose%20Estimation%20Through%20Iterative%0A%20%20Epipolar%20Sampling%20and%20Refinement%20Toward%20Equilibrium&body=Title%3A%20DualRefine%3A%20Self-Supervised%20Depth%20and%20Pose%20Estimation%20Through%20Iterative%0A%20%20Epipolar%20Sampling%20and%20Refinement%20Toward%20Equilibrium%0AAuthor%3A%20Antyanta%20Bangunharcana%20and%20Ahmed%20Magd%20and%20Kyung-Soo%20Kim%0AAbstract%3A%20%20%20Self-supervised%20multi-frame%20depth%20estimation%20achieves%20high%20accuracy%20by%0Acomputing%20matching%20costs%20of%20pixel%20correspondences%20between%20adjacent%20frames%2C%0Ainjecting%20geometric%20information%20into%20the%20network.%20These%20pixel-correspondence%0Acandidates%20are%20computed%20based%20on%20the%20relative%20pose%20estimates%20between%20the%0Aframes.%20Accurate%20pose%20predictions%20are%20essential%20for%20precise%20matching%20cost%0Acomputation%20as%20they%20influence%20the%20epipolar%20geometry.%20Furthermore%2C%20improved%0Adepth%20estimates%20can%2C%20in%20turn%2C%20be%20used%20to%20align%20pose%20estimates.%0A%20%20Inspired%20by%20traditional%20structure-from-motion%20%28SfM%29%20principles%2C%20we%20propose%0Athe%20DualRefine%20model%2C%20which%20tightly%20couples%20depth%20and%20pose%20estimation%20through%20a%0Afeedback%20loop.%20Our%20novel%20update%20pipeline%20uses%20a%20deep%20equilibrium%20model%0Aframework%20to%20iteratively%20refine%20depth%20estimates%20and%20a%20hidden%20state%20of%20feature%0Amaps%20by%20computing%20local%20matching%20costs%20based%20on%20epipolar%20geometry.%20Importantly%2C%0Awe%20used%20the%20refined%20depth%20estimates%20and%20feature%20maps%20to%20compute%20pose%20updates%20at%0Aeach%20step.%20This%20update%20in%20the%20pose%20estimates%20slowly%20alters%20the%20epipolar%0Ageometry%20during%20the%20refinement%20process.%20Experimental%20results%20on%20the%20KITTI%0Adataset%20demonstrate%20competitive%20depth%20prediction%20and%20odometry%20prediction%0Aperformance%20surpassing%20published%20self-supervised%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03560v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualRefine%3A%20Self-Supervised%20Depth%20and%20Pose%20Estimation%20Through%20Iterative%0A%20%20Epipolar%20Sampling%20and%20Refinement%20Toward%20Equilibrium&entry.906535625=Antyanta%20Bangunharcana%20and%20Ahmed%20Magd%20and%20Kyung-Soo%20Kim&entry.1292438233=%20%20Self-supervised%20multi-frame%20depth%20estimation%20achieves%20high%20accuracy%20by%0Acomputing%20matching%20costs%20of%20pixel%20correspondences%20between%20adjacent%20frames%2C%0Ainjecting%20geometric%20information%20into%20the%20network.%20These%20pixel-correspondence%0Acandidates%20are%20computed%20based%20on%20the%20relative%20pose%20estimates%20between%20the%0Aframes.%20Accurate%20pose%20predictions%20are%20essential%20for%20precise%20matching%20cost%0Acomputation%20as%20they%20influence%20the%20epipolar%20geometry.%20Furthermore%2C%20improved%0Adepth%20estimates%20can%2C%20in%20turn%2C%20be%20used%20to%20align%20pose%20estimates.%0A%20%20Inspired%20by%20traditional%20structure-from-motion%20%28SfM%29%20principles%2C%20we%20propose%0Athe%20DualRefine%20model%2C%20which%20tightly%20couples%20depth%20and%20pose%20estimation%20through%20a%0Afeedback%20loop.%20Our%20novel%20update%20pipeline%20uses%20a%20deep%20equilibrium%20model%0Aframework%20to%20iteratively%20refine%20depth%20estimates%20and%20a%20hidden%20state%20of%20feature%0Amaps%20by%20computing%20local%20matching%20costs%20based%20on%20epipolar%20geometry.%20Importantly%2C%0Awe%20used%20the%20refined%20depth%20estimates%20and%20feature%20maps%20to%20compute%20pose%20updates%20at%0Aeach%20step.%20This%20update%20in%20the%20pose%20estimates%20slowly%20alters%20the%20epipolar%0Ageometry%20during%20the%20refinement%20process.%20Experimental%20results%20on%20the%20KITTI%0Adataset%20demonstrate%20competitive%20depth%20prediction%20and%20odometry%20prediction%0Aperformance%20surpassing%20published%20self-supervised%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03560v2&entry.124074799=Read"},
{"title": "Dynamic Prompt Optimizing for Text-to-Image Generation", "author": "Wenyi Mo and Tianyu Zhang and Yalong Bai and Bing Su and Ji-Rong Wen and Qing Yang", "abstract": "  Text-to-image generative models, specifically those based on diffusion models\nlike Imagen and Stable Diffusion, have made substantial advancements. Recently,\nthere has been a surge of interest in the delicate refinement of text prompts.\nUsers assign weights or alter the injection time steps of certain words in the\ntext prompts to improve the quality of generated images. However, the success\nof fine-control prompts depends on the accuracy of the text prompts and the\ncareful selection of weights and time steps, which requires significant manual\nintervention. To address this, we introduce the \\textbf{P}rompt\n\\textbf{A}uto-\\textbf{E}diting (PAE) method. Besides refining the original\nprompts for image generation, we further employ an online reinforcement\nlearning strategy to explore the weights and injection time steps of each word,\nleading to the dynamic fine-control prompts. The reward function during\ntraining encourages the model to consider aesthetic score, semantic\nconsistency, and user preferences. Experimental results demonstrate that our\nproposed method effectively improves the original prompts, generating visually\nmore appealing images while maintaining semantic alignment. Code is available\nat https://github.com/Mowenyii/PAE.\n", "link": "http://arxiv.org/abs/2404.04095v1", "date": "2024-04-05", "relevancy": 1.7033, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5856}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5627}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5626}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Prompt%20Optimizing%20for%20Text-to-Image%20Generation&body=Title%3A%20Dynamic%20Prompt%20Optimizing%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Wenyi%20Mo%20and%20Tianyu%20Zhang%20and%20Yalong%20Bai%20and%20Bing%20Su%20and%20Ji-Rong%20Wen%20and%20Qing%20Yang%0AAbstract%3A%20%20%20Text-to-image%20generative%20models%2C%20specifically%20those%20based%20on%20diffusion%20models%0Alike%20Imagen%20and%20Stable%20Diffusion%2C%20have%20made%20substantial%20advancements.%20Recently%2C%0Athere%20has%20been%20a%20surge%20of%20interest%20in%20the%20delicate%20refinement%20of%20text%20prompts.%0AUsers%20assign%20weights%20or%20alter%20the%20injection%20time%20steps%20of%20certain%20words%20in%20the%0Atext%20prompts%20to%20improve%20the%20quality%20of%20generated%20images.%20However%2C%20the%20success%0Aof%20fine-control%20prompts%20depends%20on%20the%20accuracy%20of%20the%20text%20prompts%20and%20the%0Acareful%20selection%20of%20weights%20and%20time%20steps%2C%20which%20requires%20significant%20manual%0Aintervention.%20To%20address%20this%2C%20we%20introduce%20the%20%5Ctextbf%7BP%7Drompt%0A%5Ctextbf%7BA%7Duto-%5Ctextbf%7BE%7Dditing%20%28PAE%29%20method.%20Besides%20refining%20the%20original%0Aprompts%20for%20image%20generation%2C%20we%20further%20employ%20an%20online%20reinforcement%0Alearning%20strategy%20to%20explore%20the%20weights%20and%20injection%20time%20steps%20of%20each%20word%2C%0Aleading%20to%20the%20dynamic%20fine-control%20prompts.%20The%20reward%20function%20during%0Atraining%20encourages%20the%20model%20to%20consider%20aesthetic%20score%2C%20semantic%0Aconsistency%2C%20and%20user%20preferences.%20Experimental%20results%20demonstrate%20that%20our%0Aproposed%20method%20effectively%20improves%20the%20original%20prompts%2C%20generating%20visually%0Amore%20appealing%20images%20while%20maintaining%20semantic%20alignment.%20Code%20is%20available%0Aat%20https%3A//github.com/Mowenyii/PAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04095v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Prompt%20Optimizing%20for%20Text-to-Image%20Generation&entry.906535625=Wenyi%20Mo%20and%20Tianyu%20Zhang%20and%20Yalong%20Bai%20and%20Bing%20Su%20and%20Ji-Rong%20Wen%20and%20Qing%20Yang&entry.1292438233=%20%20Text-to-image%20generative%20models%2C%20specifically%20those%20based%20on%20diffusion%20models%0Alike%20Imagen%20and%20Stable%20Diffusion%2C%20have%20made%20substantial%20advancements.%20Recently%2C%0Athere%20has%20been%20a%20surge%20of%20interest%20in%20the%20delicate%20refinement%20of%20text%20prompts.%0AUsers%20assign%20weights%20or%20alter%20the%20injection%20time%20steps%20of%20certain%20words%20in%20the%0Atext%20prompts%20to%20improve%20the%20quality%20of%20generated%20images.%20However%2C%20the%20success%0Aof%20fine-control%20prompts%20depends%20on%20the%20accuracy%20of%20the%20text%20prompts%20and%20the%0Acareful%20selection%20of%20weights%20and%20time%20steps%2C%20which%20requires%20significant%20manual%0Aintervention.%20To%20address%20this%2C%20we%20introduce%20the%20%5Ctextbf%7BP%7Drompt%0A%5Ctextbf%7BA%7Duto-%5Ctextbf%7BE%7Dditing%20%28PAE%29%20method.%20Besides%20refining%20the%20original%0Aprompts%20for%20image%20generation%2C%20we%20further%20employ%20an%20online%20reinforcement%0Alearning%20strategy%20to%20explore%20the%20weights%20and%20injection%20time%20steps%20of%20each%20word%2C%0Aleading%20to%20the%20dynamic%20fine-control%20prompts.%20The%20reward%20function%20during%0Atraining%20encourages%20the%20model%20to%20consider%20aesthetic%20score%2C%20semantic%0Aconsistency%2C%20and%20user%20preferences.%20Experimental%20results%20demonstrate%20that%20our%0Aproposed%20method%20effectively%20improves%20the%20original%20prompts%2C%20generating%20visually%0Amore%20appealing%20images%20while%20maintaining%20semantic%20alignment.%20Code%20is%20available%0Aat%20https%3A//github.com/Mowenyii/PAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04095v1&entry.124074799=Read"},
{"title": "Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation", "author": "Zifu Wan and Yuhao Wang and Silong Yong and Pingping Zhang and Simon Stepputtis and Katia Sycara and Yaqi Xie", "abstract": "  Multi-modal semantic segmentation significantly enhances AI agents'\nperception and scene understanding, especially under adverse conditions like\nlow-light or overexposed environments. Leveraging additional modalities\n(X-modality) like thermal and depth alongside traditional RGB provides\ncomplementary information, enabling more robust and reliable segmentation. In\nthis work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic\nsegmentation, utilizing the Selective Structured State Space Model, Mamba.\nUnlike conventional methods that rely on CNNs, with their limited local\nreceptive fields, or Vision Transformers (ViTs), which offer global receptive\nfields at the cost of quadratic complexity, our model achieves global receptive\nfields coverage with linear complexity. By employing a Siamese encoder and\ninnovating a Mamba fusion mechanism, we effectively select essential\ninformation from different modalities. A decoder is then developed to enhance\nthe channel-wise modeling ability of the model. Our method, Sigma, is\nrigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks,\ndemonstrating its superiority and marking the first successful application of\nState Space Models (SSMs) in multi-modal perception tasks. Code is available at\nhttps://github.com/zifuwan/Sigma.\n", "link": "http://arxiv.org/abs/2404.04256v1", "date": "2024-04-05", "relevancy": 1.7007, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5704}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5357}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sigma%3A%20Siamese%20Mamba%20Network%20for%20Multi-Modal%20Semantic%20Segmentation&body=Title%3A%20Sigma%3A%20Siamese%20Mamba%20Network%20for%20Multi-Modal%20Semantic%20Segmentation%0AAuthor%3A%20Zifu%20Wan%20and%20Yuhao%20Wang%20and%20Silong%20Yong%20and%20Pingping%20Zhang%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie%0AAbstract%3A%20%20%20Multi-modal%20semantic%20segmentation%20significantly%20enhances%20AI%20agents%27%0Aperception%20and%20scene%20understanding%2C%20especially%20under%20adverse%20conditions%20like%0Alow-light%20or%20overexposed%20environments.%20Leveraging%20additional%20modalities%0A%28X-modality%29%20like%20thermal%20and%20depth%20alongside%20traditional%20RGB%20provides%0Acomplementary%20information%2C%20enabling%20more%20robust%20and%20reliable%20segmentation.%20In%0Athis%20work%2C%20we%20introduce%20Sigma%2C%20a%20Siamese%20Mamba%20network%20for%20multi-modal%20semantic%0Asegmentation%2C%20utilizing%20the%20Selective%20Structured%20State%20Space%20Model%2C%20Mamba.%0AUnlike%20conventional%20methods%20that%20rely%20on%20CNNs%2C%20with%20their%20limited%20local%0Areceptive%20fields%2C%20or%20Vision%20Transformers%20%28ViTs%29%2C%20which%20offer%20global%20receptive%0Afields%20at%20the%20cost%20of%20quadratic%20complexity%2C%20our%20model%20achieves%20global%20receptive%0Afields%20coverage%20with%20linear%20complexity.%20By%20employing%20a%20Siamese%20encoder%20and%0Ainnovating%20a%20Mamba%20fusion%20mechanism%2C%20we%20effectively%20select%20essential%0Ainformation%20from%20different%20modalities.%20A%20decoder%20is%20then%20developed%20to%20enhance%0Athe%20channel-wise%20modeling%20ability%20of%20the%20model.%20Our%20method%2C%20Sigma%2C%20is%0Arigorously%20evaluated%20on%20both%20RGB-Thermal%20and%20RGB-Depth%20segmentation%20tasks%2C%0Ademonstrating%20its%20superiority%20and%20marking%20the%20first%20successful%20application%20of%0AState%20Space%20Models%20%28SSMs%29%20in%20multi-modal%20perception%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zifuwan/Sigma.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04256v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sigma%3A%20Siamese%20Mamba%20Network%20for%20Multi-Modal%20Semantic%20Segmentation&entry.906535625=Zifu%20Wan%20and%20Yuhao%20Wang%20and%20Silong%20Yong%20and%20Pingping%20Zhang%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie&entry.1292438233=%20%20Multi-modal%20semantic%20segmentation%20significantly%20enhances%20AI%20agents%27%0Aperception%20and%20scene%20understanding%2C%20especially%20under%20adverse%20conditions%20like%0Alow-light%20or%20overexposed%20environments.%20Leveraging%20additional%20modalities%0A%28X-modality%29%20like%20thermal%20and%20depth%20alongside%20traditional%20RGB%20provides%0Acomplementary%20information%2C%20enabling%20more%20robust%20and%20reliable%20segmentation.%20In%0Athis%20work%2C%20we%20introduce%20Sigma%2C%20a%20Siamese%20Mamba%20network%20for%20multi-modal%20semantic%0Asegmentation%2C%20utilizing%20the%20Selective%20Structured%20State%20Space%20Model%2C%20Mamba.%0AUnlike%20conventional%20methods%20that%20rely%20on%20CNNs%2C%20with%20their%20limited%20local%0Areceptive%20fields%2C%20or%20Vision%20Transformers%20%28ViTs%29%2C%20which%20offer%20global%20receptive%0Afields%20at%20the%20cost%20of%20quadratic%20complexity%2C%20our%20model%20achieves%20global%20receptive%0Afields%20coverage%20with%20linear%20complexity.%20By%20employing%20a%20Siamese%20encoder%20and%0Ainnovating%20a%20Mamba%20fusion%20mechanism%2C%20we%20effectively%20select%20essential%0Ainformation%20from%20different%20modalities.%20A%20decoder%20is%20then%20developed%20to%20enhance%0Athe%20channel-wise%20modeling%20ability%20of%20the%20model.%20Our%20method%2C%20Sigma%2C%20is%0Arigorously%20evaluated%20on%20both%20RGB-Thermal%20and%20RGB-Depth%20segmentation%20tasks%2C%0Ademonstrating%20its%20superiority%20and%20marking%20the%20first%20successful%20application%20of%0AState%20Space%20Models%20%28SSMs%29%20in%20multi-modal%20perception%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zifuwan/Sigma.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04256v1&entry.124074799=Read"},
{"title": "Fast Genetic Algorithm for feature selection -- A qualitative\n  approximation approach", "author": "Mohammed Ghaith Altarabichi and S\u0142awomir Nowaczyk and Sepideh Pashami and Peyman Sheikholharam Mashhadi", "abstract": "  Evolutionary Algorithms (EAs) are often challenging to apply in real-world\nsettings since evolutionary computations involve a large number of evaluations\nof a typically expensive fitness function. For example, an evaluation could\ninvolve training a new machine learning model. An approximation (also known as\nmeta-model or a surrogate) of the true function can be used in such\napplications to alleviate the computation cost. In this paper, we propose a\ntwo-stage surrogate-assisted evolutionary approach to address the computational\nissues arising from using Genetic Algorithm (GA) for feature selection in a\nwrapper setting for large datasets. We define 'Approximation Usefulness' to\ncapture the necessary conditions to ensure correctness of the EA computations\nwhen an approximation is used. Based on this definition, we propose a procedure\nto construct a lightweight qualitative meta-model by the active selection of\ndata instances. We then use a meta-model to carry out the feature selection\ntask. We apply this procedure to the GA-based algorithm CHC (Cross generational\nelitist selection, Heterogeneous recombination and Cataclysmic mutation) to\ncreate a Qualitative approXimations variant, CHCQX. We show that CHCQX\nconverges faster to feature subset solutions of significantly higher accuracy\n(as compared to CHC), particularly for large datasets with over 100K instances.\nWe also demonstrate the applicability of the thinking behind our approach more\nbroadly to Swarm Intelligence (SI), another branch of the Evolutionary\nComputation (EC) paradigm with results of PSOQX, a qualitative approximation\nadaptation of the Particle Swarm Optimization (PSO) method. A GitHub repository\nwith the complete implementation is available.\n", "link": "http://arxiv.org/abs/2404.03996v1", "date": "2024-04-05", "relevancy": 1.6878, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4274}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4191}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4155}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20Genetic%20Algorithm%20for%20feature%20selection%20--%20A%20qualitative%0A%20%20approximation%20approach&body=Title%3A%20Fast%20Genetic%20Algorithm%20for%20feature%20selection%20--%20A%20qualitative%0A%20%20approximation%20approach%0AAuthor%3A%20Mohammed%20Ghaith%20Altarabichi%20and%20S%C5%82awomir%20Nowaczyk%20and%20Sepideh%20Pashami%20and%20Peyman%20Sheikholharam%20Mashhadi%0AAbstract%3A%20%20%20Evolutionary%20Algorithms%20%28EAs%29%20are%20often%20challenging%20to%20apply%20in%20real-world%0Asettings%20since%20evolutionary%20computations%20involve%20a%20large%20number%20of%20evaluations%0Aof%20a%20typically%20expensive%20fitness%20function.%20For%20example%2C%20an%20evaluation%20could%0Ainvolve%20training%20a%20new%20machine%20learning%20model.%20An%20approximation%20%28also%20known%20as%0Ameta-model%20or%20a%20surrogate%29%20of%20the%20true%20function%20can%20be%20used%20in%20such%0Aapplications%20to%20alleviate%20the%20computation%20cost.%20In%20this%20paper%2C%20we%20propose%20a%0Atwo-stage%20surrogate-assisted%20evolutionary%20approach%20to%20address%20the%20computational%0Aissues%20arising%20from%20using%20Genetic%20Algorithm%20%28GA%29%20for%20feature%20selection%20in%20a%0Awrapper%20setting%20for%20large%20datasets.%20We%20define%20%27Approximation%20Usefulness%27%20to%0Acapture%20the%20necessary%20conditions%20to%20ensure%20correctness%20of%20the%20EA%20computations%0Awhen%20an%20approximation%20is%20used.%20Based%20on%20this%20definition%2C%20we%20propose%20a%20procedure%0Ato%20construct%20a%20lightweight%20qualitative%20meta-model%20by%20the%20active%20selection%20of%0Adata%20instances.%20We%20then%20use%20a%20meta-model%20to%20carry%20out%20the%20feature%20selection%0Atask.%20We%20apply%20this%20procedure%20to%20the%20GA-based%20algorithm%20CHC%20%28Cross%20generational%0Aelitist%20selection%2C%20Heterogeneous%20recombination%20and%20Cataclysmic%20mutation%29%20to%0Acreate%20a%20Qualitative%20approXimations%20variant%2C%20CHCQX.%20We%20show%20that%20CHCQX%0Aconverges%20faster%20to%20feature%20subset%20solutions%20of%20significantly%20higher%20accuracy%0A%28as%20compared%20to%20CHC%29%2C%20particularly%20for%20large%20datasets%20with%20over%20100K%20instances.%0AWe%20also%20demonstrate%20the%20applicability%20of%20the%20thinking%20behind%20our%20approach%20more%0Abroadly%20to%20Swarm%20Intelligence%20%28SI%29%2C%20another%20branch%20of%20the%20Evolutionary%0AComputation%20%28EC%29%20paradigm%20with%20results%20of%20PSOQX%2C%20a%20qualitative%20approximation%0Aadaptation%20of%20the%20Particle%20Swarm%20Optimization%20%28PSO%29%20method.%20A%20GitHub%20repository%0Awith%20the%20complete%20implementation%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03996v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Genetic%20Algorithm%20for%20feature%20selection%20--%20A%20qualitative%0A%20%20approximation%20approach&entry.906535625=Mohammed%20Ghaith%20Altarabichi%20and%20S%C5%82awomir%20Nowaczyk%20and%20Sepideh%20Pashami%20and%20Peyman%20Sheikholharam%20Mashhadi&entry.1292438233=%20%20Evolutionary%20Algorithms%20%28EAs%29%20are%20often%20challenging%20to%20apply%20in%20real-world%0Asettings%20since%20evolutionary%20computations%20involve%20a%20large%20number%20of%20evaluations%0Aof%20a%20typically%20expensive%20fitness%20function.%20For%20example%2C%20an%20evaluation%20could%0Ainvolve%20training%20a%20new%20machine%20learning%20model.%20An%20approximation%20%28also%20known%20as%0Ameta-model%20or%20a%20surrogate%29%20of%20the%20true%20function%20can%20be%20used%20in%20such%0Aapplications%20to%20alleviate%20the%20computation%20cost.%20In%20this%20paper%2C%20we%20propose%20a%0Atwo-stage%20surrogate-assisted%20evolutionary%20approach%20to%20address%20the%20computational%0Aissues%20arising%20from%20using%20Genetic%20Algorithm%20%28GA%29%20for%20feature%20selection%20in%20a%0Awrapper%20setting%20for%20large%20datasets.%20We%20define%20%27Approximation%20Usefulness%27%20to%0Acapture%20the%20necessary%20conditions%20to%20ensure%20correctness%20of%20the%20EA%20computations%0Awhen%20an%20approximation%20is%20used.%20Based%20on%20this%20definition%2C%20we%20propose%20a%20procedure%0Ato%20construct%20a%20lightweight%20qualitative%20meta-model%20by%20the%20active%20selection%20of%0Adata%20instances.%20We%20then%20use%20a%20meta-model%20to%20carry%20out%20the%20feature%20selection%0Atask.%20We%20apply%20this%20procedure%20to%20the%20GA-based%20algorithm%20CHC%20%28Cross%20generational%0Aelitist%20selection%2C%20Heterogeneous%20recombination%20and%20Cataclysmic%20mutation%29%20to%0Acreate%20a%20Qualitative%20approXimations%20variant%2C%20CHCQX.%20We%20show%20that%20CHCQX%0Aconverges%20faster%20to%20feature%20subset%20solutions%20of%20significantly%20higher%20accuracy%0A%28as%20compared%20to%20CHC%29%2C%20particularly%20for%20large%20datasets%20with%20over%20100K%20instances.%0AWe%20also%20demonstrate%20the%20applicability%20of%20the%20thinking%20behind%20our%20approach%20more%0Abroadly%20to%20Swarm%20Intelligence%20%28SI%29%2C%20another%20branch%20of%20the%20Evolutionary%0AComputation%20%28EC%29%20paradigm%20with%20results%20of%20PSOQX%2C%20a%20qualitative%20approximation%0Aadaptation%20of%20the%20Particle%20Swarm%20Optimization%20%28PSO%29%20method.%20A%20GitHub%20repository%0Awith%20the%20complete%20implementation%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03996v1&entry.124074799=Read"},
{"title": "Cycle Life Prediction for Lithium-ion Batteries: Machine Learning and\n  More", "author": "Joachim Schaeffer and Giacomo Galuppini and Jinwook Rhyu and Patrick A. Asinger and Robin Droop and Rolf Findeisen and Richard D. Braatz", "abstract": "  Batteries are dynamic systems with complicated nonlinear aging, highly\ndependent on cell design, chemistry, manufacturing, and operational conditions.\nPrediction of battery cycle life and estimation of aging states is important to\naccelerate battery R&D, testing, and to further the understanding of how\nbatteries degrade. Beyond testing, battery management systems rely on real-time\nmodels and onboard diagnostics and prognostics for safe operation. Estimating\nthe state of health and remaining useful life of a battery is important to\noptimize performance and use resources optimally.\n  This tutorial begins with an overview of first-principles, machine learning,\nand hybrid battery models. Then, a typical pipeline for the development of\ninterpretable machine learning models is explained and showcased for cycle life\nprediction from laboratory testing data. We highlight the challenges of machine\nlearning models, motivating the incorporation of physics in hybrid modeling\napproaches, which are needed to decipher the aging trajectory of batteries but\nrequire more data and further work on the physics of battery degradation. The\ntutorial closes with a discussion on generalization and further research\ndirections.\n", "link": "http://arxiv.org/abs/2404.04049v1", "date": "2024-04-05", "relevancy": 1.6876, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4772}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4266}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.395}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cycle%20Life%20Prediction%20for%20Lithium-ion%20Batteries%3A%20Machine%20Learning%20and%0A%20%20More&body=Title%3A%20Cycle%20Life%20Prediction%20for%20Lithium-ion%20Batteries%3A%20Machine%20Learning%20and%0A%20%20More%0AAuthor%3A%20Joachim%20Schaeffer%20and%20Giacomo%20Galuppini%20and%20Jinwook%20Rhyu%20and%20Patrick%20A.%20Asinger%20and%20Robin%20Droop%20and%20Rolf%20Findeisen%20and%20Richard%20D.%20Braatz%0AAbstract%3A%20%20%20Batteries%20are%20dynamic%20systems%20with%20complicated%20nonlinear%20aging%2C%20highly%0Adependent%20on%20cell%20design%2C%20chemistry%2C%20manufacturing%2C%20and%20operational%20conditions.%0APrediction%20of%20battery%20cycle%20life%20and%20estimation%20of%20aging%20states%20is%20important%20to%0Aaccelerate%20battery%20R%26D%2C%20testing%2C%20and%20to%20further%20the%20understanding%20of%20how%0Abatteries%20degrade.%20Beyond%20testing%2C%20battery%20management%20systems%20rely%20on%20real-time%0Amodels%20and%20onboard%20diagnostics%20and%20prognostics%20for%20safe%20operation.%20Estimating%0Athe%20state%20of%20health%20and%20remaining%20useful%20life%20of%20a%20battery%20is%20important%20to%0Aoptimize%20performance%20and%20use%20resources%20optimally.%0A%20%20This%20tutorial%20begins%20with%20an%20overview%20of%20first-principles%2C%20machine%20learning%2C%0Aand%20hybrid%20battery%20models.%20Then%2C%20a%20typical%20pipeline%20for%20the%20development%20of%0Ainterpretable%20machine%20learning%20models%20is%20explained%20and%20showcased%20for%20cycle%20life%0Aprediction%20from%20laboratory%20testing%20data.%20We%20highlight%20the%20challenges%20of%20machine%0Alearning%20models%2C%20motivating%20the%20incorporation%20of%20physics%20in%20hybrid%20modeling%0Aapproaches%2C%20which%20are%20needed%20to%20decipher%20the%20aging%20trajectory%20of%20batteries%20but%0Arequire%20more%20data%20and%20further%20work%20on%20the%20physics%20of%20battery%20degradation.%20The%0Atutorial%20closes%20with%20a%20discussion%20on%20generalization%20and%20further%20research%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04049v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cycle%20Life%20Prediction%20for%20Lithium-ion%20Batteries%3A%20Machine%20Learning%20and%0A%20%20More&entry.906535625=Joachim%20Schaeffer%20and%20Giacomo%20Galuppini%20and%20Jinwook%20Rhyu%20and%20Patrick%20A.%20Asinger%20and%20Robin%20Droop%20and%20Rolf%20Findeisen%20and%20Richard%20D.%20Braatz&entry.1292438233=%20%20Batteries%20are%20dynamic%20systems%20with%20complicated%20nonlinear%20aging%2C%20highly%0Adependent%20on%20cell%20design%2C%20chemistry%2C%20manufacturing%2C%20and%20operational%20conditions.%0APrediction%20of%20battery%20cycle%20life%20and%20estimation%20of%20aging%20states%20is%20important%20to%0Aaccelerate%20battery%20R%26D%2C%20testing%2C%20and%20to%20further%20the%20understanding%20of%20how%0Abatteries%20degrade.%20Beyond%20testing%2C%20battery%20management%20systems%20rely%20on%20real-time%0Amodels%20and%20onboard%20diagnostics%20and%20prognostics%20for%20safe%20operation.%20Estimating%0Athe%20state%20of%20health%20and%20remaining%20useful%20life%20of%20a%20battery%20is%20important%20to%0Aoptimize%20performance%20and%20use%20resources%20optimally.%0A%20%20This%20tutorial%20begins%20with%20an%20overview%20of%20first-principles%2C%20machine%20learning%2C%0Aand%20hybrid%20battery%20models.%20Then%2C%20a%20typical%20pipeline%20for%20the%20development%20of%0Ainterpretable%20machine%20learning%20models%20is%20explained%20and%20showcased%20for%20cycle%20life%0Aprediction%20from%20laboratory%20testing%20data.%20We%20highlight%20the%20challenges%20of%20machine%0Alearning%20models%2C%20motivating%20the%20incorporation%20of%20physics%20in%20hybrid%20modeling%0Aapproaches%2C%20which%20are%20needed%20to%20decipher%20the%20aging%20trajectory%20of%20batteries%20but%0Arequire%20more%20data%20and%20further%20work%20on%20the%20physics%20of%20battery%20degradation.%20The%0Atutorial%20closes%20with%20a%20discussion%20on%20generalization%20and%20further%20research%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04049v1&entry.124074799=Read"},
{"title": "Plug-and-Play image restoration with Stochastic deNOising REgularization", "author": "Marien Renaud and Jean Prost and Arthur Leclaire and Nicolas Papadakis", "abstract": "  Plug-and-Play (PnP) algorithms are a class of iterative algorithms that\naddress image inverse problems by combining a physical model and a deep neural\nnetwork for regularization. Even if they produce impressive image restoration\nresults, these algorithms rely on a non-standard use of a denoiser on images\nthat are less and less noisy along the iterations, which contrasts with recent\nalgorithms based on Diffusion Models (DM), where the denoiser is applied only\non re-noised images. We propose a new PnP framework, called Stochastic\ndeNOising REgularization (SNORE), which applies the denoiser only on images\nwith noise of the adequate level. It is based on an explicit stochastic\nregularization, which leads to a stochastic gradient descent algorithm to solve\nill-posed inverse problems. A convergence analysis of this algorithm and its\nannealing extension is provided. Experimentally, we prove that SNORE is\ncompetitive with respect to state-of-the-art methods on deblurring and\ninpainting tasks, both quantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2402.01779v2", "date": "2024-04-05", "relevancy": 1.6766, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5862}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5314}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5179}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Plug-and-Play%20image%20restoration%20with%20Stochastic%20deNOising%20REgularization&body=Title%3A%20Plug-and-Play%20image%20restoration%20with%20Stochastic%20deNOising%20REgularization%0AAuthor%3A%20Marien%20Renaud%20and%20Jean%20Prost%20and%20Arthur%20Leclaire%20and%20Nicolas%20Papadakis%0AAbstract%3A%20%20%20Plug-and-Play%20%28PnP%29%20algorithms%20are%20a%20class%20of%20iterative%20algorithms%20that%0Aaddress%20image%20inverse%20problems%20by%20combining%20a%20physical%20model%20and%20a%20deep%20neural%0Anetwork%20for%20regularization.%20Even%20if%20they%20produce%20impressive%20image%20restoration%0Aresults%2C%20these%20algorithms%20rely%20on%20a%20non-standard%20use%20of%20a%20denoiser%20on%20images%0Athat%20are%20less%20and%20less%20noisy%20along%20the%20iterations%2C%20which%20contrasts%20with%20recent%0Aalgorithms%20based%20on%20Diffusion%20Models%20%28DM%29%2C%20where%20the%20denoiser%20is%20applied%20only%0Aon%20re-noised%20images.%20We%20propose%20a%20new%20PnP%20framework%2C%20called%20Stochastic%0AdeNOising%20REgularization%20%28SNORE%29%2C%20which%20applies%20the%20denoiser%20only%20on%20images%0Awith%20noise%20of%20the%20adequate%20level.%20It%20is%20based%20on%20an%20explicit%20stochastic%0Aregularization%2C%20which%20leads%20to%20a%20stochastic%20gradient%20descent%20algorithm%20to%20solve%0Aill-posed%20inverse%20problems.%20A%20convergence%20analysis%20of%20this%20algorithm%20and%20its%0Aannealing%20extension%20is%20provided.%20Experimentally%2C%20we%20prove%20that%20SNORE%20is%0Acompetitive%20with%20respect%20to%20state-of-the-art%20methods%20on%20deblurring%20and%0Ainpainting%20tasks%2C%20both%20quantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01779v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plug-and-Play%20image%20restoration%20with%20Stochastic%20deNOising%20REgularization&entry.906535625=Marien%20Renaud%20and%20Jean%20Prost%20and%20Arthur%20Leclaire%20and%20Nicolas%20Papadakis&entry.1292438233=%20%20Plug-and-Play%20%28PnP%29%20algorithms%20are%20a%20class%20of%20iterative%20algorithms%20that%0Aaddress%20image%20inverse%20problems%20by%20combining%20a%20physical%20model%20and%20a%20deep%20neural%0Anetwork%20for%20regularization.%20Even%20if%20they%20produce%20impressive%20image%20restoration%0Aresults%2C%20these%20algorithms%20rely%20on%20a%20non-standard%20use%20of%20a%20denoiser%20on%20images%0Athat%20are%20less%20and%20less%20noisy%20along%20the%20iterations%2C%20which%20contrasts%20with%20recent%0Aalgorithms%20based%20on%20Diffusion%20Models%20%28DM%29%2C%20where%20the%20denoiser%20is%20applied%20only%0Aon%20re-noised%20images.%20We%20propose%20a%20new%20PnP%20framework%2C%20called%20Stochastic%0AdeNOising%20REgularization%20%28SNORE%29%2C%20which%20applies%20the%20denoiser%20only%20on%20images%0Awith%20noise%20of%20the%20adequate%20level.%20It%20is%20based%20on%20an%20explicit%20stochastic%0Aregularization%2C%20which%20leads%20to%20a%20stochastic%20gradient%20descent%20algorithm%20to%20solve%0Aill-posed%20inverse%20problems.%20A%20convergence%20analysis%20of%20this%20algorithm%20and%20its%0Aannealing%20extension%20is%20provided.%20Experimentally%2C%20we%20prove%20that%20SNORE%20is%0Acompetitive%20with%20respect%20to%20state-of-the-art%20methods%20on%20deblurring%20and%0Ainpainting%20tasks%2C%20both%20quantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01779v2&entry.124074799=Read"},
{"title": "Contextual Encoder-Decoder Network for Visual Saliency Prediction", "author": "Alexander Kroner and Mario Senden and Kurt Driessens and Rainer Goebel", "abstract": "  Predicting salient regions in natural images requires the detection of\nobjects that are present in a scene. To develop robust representations for this\nchallenging task, high-level visual features at multiple spatial scales must be\nextracted and augmented with contextual information. However, existing models\naimed at explaining human fixation maps do not incorporate such a mechanism\nexplicitly. Here we propose an approach based on a convolutional neural network\npre-trained on a large-scale image classification task. The architecture forms\nan encoder-decoder structure and includes a module with multiple convolutional\nlayers at different dilation rates to capture multi-scale features in parallel.\nMoreover, we combine the resulting representations with global scene\ninformation for accurately predicting visual saliency. Our model achieves\ncompetitive and consistent results across multiple evaluation metrics on two\npublic saliency benchmarks and we demonstrate the effectiveness of the\nsuggested approach on five datasets and selected examples. Compared to state of\nthe art approaches, the network is based on a lightweight image classification\nbackbone and hence presents a suitable choice for applications with limited\ncomputational resources, such as (virtual) robotic systems, to estimate human\nfixations across complex natural scenes.\n", "link": "http://arxiv.org/abs/1902.06634v4", "date": "2024-04-05", "relevancy": 1.6671, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5602}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5545}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contextual%20Encoder-Decoder%20Network%20for%20Visual%20Saliency%20Prediction&body=Title%3A%20Contextual%20Encoder-Decoder%20Network%20for%20Visual%20Saliency%20Prediction%0AAuthor%3A%20Alexander%20Kroner%20and%20Mario%20Senden%20and%20Kurt%20Driessens%20and%20Rainer%20Goebel%0AAbstract%3A%20%20%20Predicting%20salient%20regions%20in%20natural%20images%20requires%20the%20detection%20of%0Aobjects%20that%20are%20present%20in%20a%20scene.%20To%20develop%20robust%20representations%20for%20this%0Achallenging%20task%2C%20high-level%20visual%20features%20at%20multiple%20spatial%20scales%20must%20be%0Aextracted%20and%20augmented%20with%20contextual%20information.%20However%2C%20existing%20models%0Aaimed%20at%20explaining%20human%20fixation%20maps%20do%20not%20incorporate%20such%20a%20mechanism%0Aexplicitly.%20Here%20we%20propose%20an%20approach%20based%20on%20a%20convolutional%20neural%20network%0Apre-trained%20on%20a%20large-scale%20image%20classification%20task.%20The%20architecture%20forms%0Aan%20encoder-decoder%20structure%20and%20includes%20a%20module%20with%20multiple%20convolutional%0Alayers%20at%20different%20dilation%20rates%20to%20capture%20multi-scale%20features%20in%20parallel.%0AMoreover%2C%20we%20combine%20the%20resulting%20representations%20with%20global%20scene%0Ainformation%20for%20accurately%20predicting%20visual%20saliency.%20Our%20model%20achieves%0Acompetitive%20and%20consistent%20results%20across%20multiple%20evaluation%20metrics%20on%20two%0Apublic%20saliency%20benchmarks%20and%20we%20demonstrate%20the%20effectiveness%20of%20the%0Asuggested%20approach%20on%20five%20datasets%20and%20selected%20examples.%20Compared%20to%20state%20of%0Athe%20art%20approaches%2C%20the%20network%20is%20based%20on%20a%20lightweight%20image%20classification%0Abackbone%20and%20hence%20presents%20a%20suitable%20choice%20for%20applications%20with%20limited%0Acomputational%20resources%2C%20such%20as%20%28virtual%29%20robotic%20systems%2C%20to%20estimate%20human%0Afixations%20across%20complex%20natural%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1902.06634v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Encoder-Decoder%20Network%20for%20Visual%20Saliency%20Prediction&entry.906535625=Alexander%20Kroner%20and%20Mario%20Senden%20and%20Kurt%20Driessens%20and%20Rainer%20Goebel&entry.1292438233=%20%20Predicting%20salient%20regions%20in%20natural%20images%20requires%20the%20detection%20of%0Aobjects%20that%20are%20present%20in%20a%20scene.%20To%20develop%20robust%20representations%20for%20this%0Achallenging%20task%2C%20high-level%20visual%20features%20at%20multiple%20spatial%20scales%20must%20be%0Aextracted%20and%20augmented%20with%20contextual%20information.%20However%2C%20existing%20models%0Aaimed%20at%20explaining%20human%20fixation%20maps%20do%20not%20incorporate%20such%20a%20mechanism%0Aexplicitly.%20Here%20we%20propose%20an%20approach%20based%20on%20a%20convolutional%20neural%20network%0Apre-trained%20on%20a%20large-scale%20image%20classification%20task.%20The%20architecture%20forms%0Aan%20encoder-decoder%20structure%20and%20includes%20a%20module%20with%20multiple%20convolutional%0Alayers%20at%20different%20dilation%20rates%20to%20capture%20multi-scale%20features%20in%20parallel.%0AMoreover%2C%20we%20combine%20the%20resulting%20representations%20with%20global%20scene%0Ainformation%20for%20accurately%20predicting%20visual%20saliency.%20Our%20model%20achieves%0Acompetitive%20and%20consistent%20results%20across%20multiple%20evaluation%20metrics%20on%20two%0Apublic%20saliency%20benchmarks%20and%20we%20demonstrate%20the%20effectiveness%20of%20the%0Asuggested%20approach%20on%20five%20datasets%20and%20selected%20examples.%20Compared%20to%20state%20of%0Athe%20art%20approaches%2C%20the%20network%20is%20based%20on%20a%20lightweight%20image%20classification%0Abackbone%20and%20hence%20presents%20a%20suitable%20choice%20for%20applications%20with%20limited%0Acomputational%20resources%2C%20such%20as%20%28virtual%29%20robotic%20systems%2C%20to%20estimate%20human%0Afixations%20across%20complex%20natural%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/1902.06634v4&entry.124074799=Read"},
{"title": "One model to use them all: Training a segmentation model with\n  complementary datasets", "author": "Alexander C. Jenke and Sebastian Bodenstedt and Fiona R. Kolbinger and Marius Distler and J\u00fcrgen Weitz and Stefanie Speidel", "abstract": "  Understanding a surgical scene is crucial for computer-assisted surgery\nsystems to provide any intelligent assistance functionality. One way of\nachieving this scene understanding is via scene segmentation, where every pixel\nof a frame is classified and therefore identifies the visible structures and\ntissues. Progress on fully segmenting surgical scenes has been made using\nmachine learning. However, such models require large amounts of annotated\ntraining data, containing examples of all relevant object classes. Such fully\nannotated datasets are hard to create, as every pixel in a frame needs to be\nannotated by medical experts and, therefore, are rarely available. In this\nwork, we propose a method to combine multiple partially annotated datasets,\nwhich provide complementary annotations, into one model, enabling better scene\nsegmentation and the use of multiple readily available datasets. Our method\naims to combine available data with complementary labels by leveraging mutual\nexclusive properties to maximize information. Specifically, we propose to use\npositive annotations of other classes as negative samples and to exclude\nbackground pixels of binary annotations, as we cannot tell if they contain a\nclass not annotated but predicted by the model. We evaluate our method by\ntraining a DeepLabV3 on the publicly available Dresden Surgical Anatomy\nDataset, which provides multiple subsets of binary segmented anatomical\nstructures. Our approach successfully combines 6 classes into one model,\nincreasing the overall Dice Score by 4.4% compared to an ensemble of models\ntrained on the classes individually. By including information on multiple\nclasses, we were able to reduce confusion between stomach and colon by 24%. Our\nresults demonstrate the feasibility of training a model on multiple datasets.\nThis paves the way for future work further alleviating the need for one large,\nfully segmented datasets.\n", "link": "http://arxiv.org/abs/2402.19340v2", "date": "2024-04-05", "relevancy": 1.6663, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.57}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5593}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5152}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One%20model%20to%20use%20them%20all%3A%20Training%20a%20segmentation%20model%20with%0A%20%20complementary%20datasets&body=Title%3A%20One%20model%20to%20use%20them%20all%3A%20Training%20a%20segmentation%20model%20with%0A%20%20complementary%20datasets%0AAuthor%3A%20Alexander%20C.%20Jenke%20and%20Sebastian%20Bodenstedt%20and%20Fiona%20R.%20Kolbinger%20and%20Marius%20Distler%20and%20J%C3%BCrgen%20Weitz%20and%20Stefanie%20Speidel%0AAbstract%3A%20%20%20Understanding%20a%20surgical%20scene%20is%20crucial%20for%20computer-assisted%20surgery%0Asystems%20to%20provide%20any%20intelligent%20assistance%20functionality.%20One%20way%20of%0Aachieving%20this%20scene%20understanding%20is%20via%20scene%20segmentation%2C%20where%20every%20pixel%0Aof%20a%20frame%20is%20classified%20and%20therefore%20identifies%20the%20visible%20structures%20and%0Atissues.%20Progress%20on%20fully%20segmenting%20surgical%20scenes%20has%20been%20made%20using%0Amachine%20learning.%20However%2C%20such%20models%20require%20large%20amounts%20of%20annotated%0Atraining%20data%2C%20containing%20examples%20of%20all%20relevant%20object%20classes.%20Such%20fully%0Aannotated%20datasets%20are%20hard%20to%20create%2C%20as%20every%20pixel%20in%20a%20frame%20needs%20to%20be%0Aannotated%20by%20medical%20experts%20and%2C%20therefore%2C%20are%20rarely%20available.%20In%20this%0Awork%2C%20we%20propose%20a%20method%20to%20combine%20multiple%20partially%20annotated%20datasets%2C%0Awhich%20provide%20complementary%20annotations%2C%20into%20one%20model%2C%20enabling%20better%20scene%0Asegmentation%20and%20the%20use%20of%20multiple%20readily%20available%20datasets.%20Our%20method%0Aaims%20to%20combine%20available%20data%20with%20complementary%20labels%20by%20leveraging%20mutual%0Aexclusive%20properties%20to%20maximize%20information.%20Specifically%2C%20we%20propose%20to%20use%0Apositive%20annotations%20of%20other%20classes%20as%20negative%20samples%20and%20to%20exclude%0Abackground%20pixels%20of%20binary%20annotations%2C%20as%20we%20cannot%20tell%20if%20they%20contain%20a%0Aclass%20not%20annotated%20but%20predicted%20by%20the%20model.%20We%20evaluate%20our%20method%20by%0Atraining%20a%20DeepLabV3%20on%20the%20publicly%20available%20Dresden%20Surgical%20Anatomy%0ADataset%2C%20which%20provides%20multiple%20subsets%20of%20binary%20segmented%20anatomical%0Astructures.%20Our%20approach%20successfully%20combines%206%20classes%20into%20one%20model%2C%0Aincreasing%20the%20overall%20Dice%20Score%20by%204.4%25%20compared%20to%20an%20ensemble%20of%20models%0Atrained%20on%20the%20classes%20individually.%20By%20including%20information%20on%20multiple%0Aclasses%2C%20we%20were%20able%20to%20reduce%20confusion%20between%20stomach%20and%20colon%20by%2024%25.%20Our%0Aresults%20demonstrate%20the%20feasibility%20of%20training%20a%20model%20on%20multiple%20datasets.%0AThis%20paves%20the%20way%20for%20future%20work%20further%20alleviating%20the%20need%20for%20one%20large%2C%0Afully%20segmented%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19340v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20model%20to%20use%20them%20all%3A%20Training%20a%20segmentation%20model%20with%0A%20%20complementary%20datasets&entry.906535625=Alexander%20C.%20Jenke%20and%20Sebastian%20Bodenstedt%20and%20Fiona%20R.%20Kolbinger%20and%20Marius%20Distler%20and%20J%C3%BCrgen%20Weitz%20and%20Stefanie%20Speidel&entry.1292438233=%20%20Understanding%20a%20surgical%20scene%20is%20crucial%20for%20computer-assisted%20surgery%0Asystems%20to%20provide%20any%20intelligent%20assistance%20functionality.%20One%20way%20of%0Aachieving%20this%20scene%20understanding%20is%20via%20scene%20segmentation%2C%20where%20every%20pixel%0Aof%20a%20frame%20is%20classified%20and%20therefore%20identifies%20the%20visible%20structures%20and%0Atissues.%20Progress%20on%20fully%20segmenting%20surgical%20scenes%20has%20been%20made%20using%0Amachine%20learning.%20However%2C%20such%20models%20require%20large%20amounts%20of%20annotated%0Atraining%20data%2C%20containing%20examples%20of%20all%20relevant%20object%20classes.%20Such%20fully%0Aannotated%20datasets%20are%20hard%20to%20create%2C%20as%20every%20pixel%20in%20a%20frame%20needs%20to%20be%0Aannotated%20by%20medical%20experts%20and%2C%20therefore%2C%20are%20rarely%20available.%20In%20this%0Awork%2C%20we%20propose%20a%20method%20to%20combine%20multiple%20partially%20annotated%20datasets%2C%0Awhich%20provide%20complementary%20annotations%2C%20into%20one%20model%2C%20enabling%20better%20scene%0Asegmentation%20and%20the%20use%20of%20multiple%20readily%20available%20datasets.%20Our%20method%0Aaims%20to%20combine%20available%20data%20with%20complementary%20labels%20by%20leveraging%20mutual%0Aexclusive%20properties%20to%20maximize%20information.%20Specifically%2C%20we%20propose%20to%20use%0Apositive%20annotations%20of%20other%20classes%20as%20negative%20samples%20and%20to%20exclude%0Abackground%20pixels%20of%20binary%20annotations%2C%20as%20we%20cannot%20tell%20if%20they%20contain%20a%0Aclass%20not%20annotated%20but%20predicted%20by%20the%20model.%20We%20evaluate%20our%20method%20by%0Atraining%20a%20DeepLabV3%20on%20the%20publicly%20available%20Dresden%20Surgical%20Anatomy%0ADataset%2C%20which%20provides%20multiple%20subsets%20of%20binary%20segmented%20anatomical%0Astructures.%20Our%20approach%20successfully%20combines%206%20classes%20into%20one%20model%2C%0Aincreasing%20the%20overall%20Dice%20Score%20by%204.4%25%20compared%20to%20an%20ensemble%20of%20models%0Atrained%20on%20the%20classes%20individually.%20By%20including%20information%20on%20multiple%0Aclasses%2C%20we%20were%20able%20to%20reduce%20confusion%20between%20stomach%20and%20colon%20by%2024%25.%20Our%0Aresults%20demonstrate%20the%20feasibility%20of%20training%20a%20model%20on%20multiple%20datasets.%0AThis%20paves%20the%20way%20for%20future%20work%20further%20alleviating%20the%20need%20for%20one%20large%2C%0Afully%20segmented%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19340v2&entry.124074799=Read"},
{"title": "VoicePilot: Harnessing LLMs as Speech Interfaces for Physically\n  Assistive Robots", "author": "Akhil Padmanabha and Jessie Yuan and Janavi Gupta and Zulekha Karachiwalla and Carmel Majidi and Henny Admoni and Zackory Erickson", "abstract": "  Physically assistive robots present an opportunity to significantly increase\nthe well-being and independence of individuals with motor impairments or other\nforms of disability who are unable to complete activities of daily living.\nSpeech interfaces, especially ones that utilize Large Language Models (LLMs),\ncan enable individuals to effectively and naturally communicate high-level\ncommands and nuanced preferences to robots. Frameworks for integrating LLMs as\ninterfaces to robots for high level task planning and code generation have been\nproposed, but fail to incorporate human-centric considerations which are\nessential while developing assistive interfaces. In this work, we present a\nframework for incorporating LLMs as speech interfaces for physically assistive\nrobots, constructed iteratively with 3 stages of testing involving a feeding\nrobot, culminating in an evaluation with 11 older adults at an independent\nliving facility. We use both quantitative and qualitative data from the final\nstudy to validate our framework and additionally provide design guidelines for\nusing LLMs as speech interfaces for assistive robots. Videos and supporting\nfiles are located on our project website:\nhttps://sites.google.com/andrew.cmu.edu/voicepilot/\n", "link": "http://arxiv.org/abs/2404.04066v1", "date": "2024-04-05", "relevancy": 1.6662, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5602}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5579}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.541}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VoicePilot%3A%20Harnessing%20LLMs%20as%20Speech%20Interfaces%20for%20Physically%0A%20%20Assistive%20Robots&body=Title%3A%20VoicePilot%3A%20Harnessing%20LLMs%20as%20Speech%20Interfaces%20for%20Physically%0A%20%20Assistive%20Robots%0AAuthor%3A%20Akhil%20Padmanabha%20and%20Jessie%20Yuan%20and%20Janavi%20Gupta%20and%20Zulekha%20Karachiwalla%20and%20Carmel%20Majidi%20and%20Henny%20Admoni%20and%20Zackory%20Erickson%0AAbstract%3A%20%20%20Physically%20assistive%20robots%20present%20an%20opportunity%20to%20significantly%20increase%0Athe%20well-being%20and%20independence%20of%20individuals%20with%20motor%20impairments%20or%20other%0Aforms%20of%20disability%20who%20are%20unable%20to%20complete%20activities%20of%20daily%20living.%0ASpeech%20interfaces%2C%20especially%20ones%20that%20utilize%20Large%20Language%20Models%20%28LLMs%29%2C%0Acan%20enable%20individuals%20to%20effectively%20and%20naturally%20communicate%20high-level%0Acommands%20and%20nuanced%20preferences%20to%20robots.%20Frameworks%20for%20integrating%20LLMs%20as%0Ainterfaces%20to%20robots%20for%20high%20level%20task%20planning%20and%20code%20generation%20have%20been%0Aproposed%2C%20but%20fail%20to%20incorporate%20human-centric%20considerations%20which%20are%0Aessential%20while%20developing%20assistive%20interfaces.%20In%20this%20work%2C%20we%20present%20a%0Aframework%20for%20incorporating%20LLMs%20as%20speech%20interfaces%20for%20physically%20assistive%0Arobots%2C%20constructed%20iteratively%20with%203%20stages%20of%20testing%20involving%20a%20feeding%0Arobot%2C%20culminating%20in%20an%20evaluation%20with%2011%20older%20adults%20at%20an%20independent%0Aliving%20facility.%20We%20use%20both%20quantitative%20and%20qualitative%20data%20from%20the%20final%0Astudy%20to%20validate%20our%20framework%20and%20additionally%20provide%20design%20guidelines%20for%0Ausing%20LLMs%20as%20speech%20interfaces%20for%20assistive%20robots.%20Videos%20and%20supporting%0Afiles%20are%20located%20on%20our%20project%20website%3A%0Ahttps%3A//sites.google.com/andrew.cmu.edu/voicepilot/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04066v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoicePilot%3A%20Harnessing%20LLMs%20as%20Speech%20Interfaces%20for%20Physically%0A%20%20Assistive%20Robots&entry.906535625=Akhil%20Padmanabha%20and%20Jessie%20Yuan%20and%20Janavi%20Gupta%20and%20Zulekha%20Karachiwalla%20and%20Carmel%20Majidi%20and%20Henny%20Admoni%20and%20Zackory%20Erickson&entry.1292438233=%20%20Physically%20assistive%20robots%20present%20an%20opportunity%20to%20significantly%20increase%0Athe%20well-being%20and%20independence%20of%20individuals%20with%20motor%20impairments%20or%20other%0Aforms%20of%20disability%20who%20are%20unable%20to%20complete%20activities%20of%20daily%20living.%0ASpeech%20interfaces%2C%20especially%20ones%20that%20utilize%20Large%20Language%20Models%20%28LLMs%29%2C%0Acan%20enable%20individuals%20to%20effectively%20and%20naturally%20communicate%20high-level%0Acommands%20and%20nuanced%20preferences%20to%20robots.%20Frameworks%20for%20integrating%20LLMs%20as%0Ainterfaces%20to%20robots%20for%20high%20level%20task%20planning%20and%20code%20generation%20have%20been%0Aproposed%2C%20but%20fail%20to%20incorporate%20human-centric%20considerations%20which%20are%0Aessential%20while%20developing%20assistive%20interfaces.%20In%20this%20work%2C%20we%20present%20a%0Aframework%20for%20incorporating%20LLMs%20as%20speech%20interfaces%20for%20physically%20assistive%0Arobots%2C%20constructed%20iteratively%20with%203%20stages%20of%20testing%20involving%20a%20feeding%0Arobot%2C%20culminating%20in%20an%20evaluation%20with%2011%20older%20adults%20at%20an%20independent%0Aliving%20facility.%20We%20use%20both%20quantitative%20and%20qualitative%20data%20from%20the%20final%0Astudy%20to%20validate%20our%20framework%20and%20additionally%20provide%20design%20guidelines%20for%0Ausing%20LLMs%20as%20speech%20interfaces%20for%20assistive%20robots.%20Videos%20and%20supporting%0Afiles%20are%20located%20on%20our%20project%20website%3A%0Ahttps%3A//sites.google.com/andrew.cmu.edu/voicepilot/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04066v1&entry.124074799=Read"},
{"title": "Learning Enriched Features via Selective State Spaces Model for\n  Efficient Image Deblurring", "author": "Hu Gao and Depeng Dang", "abstract": "  Image deblurring aims to restore a high-quality image from its corresponding\nblurred. The emergence of CNNs and Transformers has enabled significant\nprogress. However, these methods often face the dilemma between eliminating\nlong-range degradation perturbations and maintaining computational efficiency.\nWhile the selective state space model (SSM) shows promise in modeling\nlong-range dependencies with linear complexity, it also encounters challenges\nsuch as local pixel forgetting and channel redundancy. To address this issue,\nwe propose an efficient image deblurring network that leverages selective state\nspaces model to aggregate enriched and accurate features. Specifically, we\nintroduce an aggregate local and global information block (ALGBlock) designed\nto effectively capture and integrate both local invariant properties and\nnon-local information. The ALGBlock comprises two primary modules: a module for\ncapturing local and global features (CLGF), and a feature aggregation module\n(FA). The CLGF module is composed of two branches: the global branch captures\nlong-range dependency features via a selective state spaces model, while the\nlocal branch employs simplified channel attention to model local connectivity,\nthereby reducing local pixel forgetting and channel redundancy. In addition, we\ndesign a FA module to accentuate the local part by recalibrating the weight\nduring the aggregation of the two branches for restoration. Experimental\nresults demonstrate that the proposed method outperforms state-of-the-art\napproaches on widely used benchmarks.\n", "link": "http://arxiv.org/abs/2403.20106v2", "date": "2024-04-05", "relevancy": 1.6622, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5803}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5486}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5414}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Enriched%20Features%20via%20Selective%20State%20Spaces%20Model%20for%0A%20%20Efficient%20Image%20Deblurring&body=Title%3A%20Learning%20Enriched%20Features%20via%20Selective%20State%20Spaces%20Model%20for%0A%20%20Efficient%20Image%20Deblurring%0AAuthor%3A%20Hu%20Gao%20and%20Depeng%20Dang%0AAbstract%3A%20%20%20Image%20deblurring%20aims%20to%20restore%20a%20high-quality%20image%20from%20its%20corresponding%0Ablurred.%20The%20emergence%20of%20CNNs%20and%20Transformers%20has%20enabled%20significant%0Aprogress.%20However%2C%20these%20methods%20often%20face%20the%20dilemma%20between%20eliminating%0Along-range%20degradation%20perturbations%20and%20maintaining%20computational%20efficiency.%0AWhile%20the%20selective%20state%20space%20model%20%28SSM%29%20shows%20promise%20in%20modeling%0Along-range%20dependencies%20with%20linear%20complexity%2C%20it%20also%20encounters%20challenges%0Asuch%20as%20local%20pixel%20forgetting%20and%20channel%20redundancy.%20To%20address%20this%20issue%2C%0Awe%20propose%20an%20efficient%20image%20deblurring%20network%20that%20leverages%20selective%20state%0Aspaces%20model%20to%20aggregate%20enriched%20and%20accurate%20features.%20Specifically%2C%20we%0Aintroduce%20an%20aggregate%20local%20and%20global%20information%20block%20%28ALGBlock%29%20designed%0Ato%20effectively%20capture%20and%20integrate%20both%20local%20invariant%20properties%20and%0Anon-local%20information.%20The%20ALGBlock%20comprises%20two%20primary%20modules%3A%20a%20module%20for%0Acapturing%20local%20and%20global%20features%20%28CLGF%29%2C%20and%20a%20feature%20aggregation%20module%0A%28FA%29.%20The%20CLGF%20module%20is%20composed%20of%20two%20branches%3A%20the%20global%20branch%20captures%0Along-range%20dependency%20features%20via%20a%20selective%20state%20spaces%20model%2C%20while%20the%0Alocal%20branch%20employs%20simplified%20channel%20attention%20to%20model%20local%20connectivity%2C%0Athereby%20reducing%20local%20pixel%20forgetting%20and%20channel%20redundancy.%20In%20addition%2C%20we%0Adesign%20a%20FA%20module%20to%20accentuate%20the%20local%20part%20by%20recalibrating%20the%20weight%0Aduring%20the%20aggregation%20of%20the%20two%20branches%20for%20restoration.%20Experimental%0Aresults%20demonstrate%20that%20the%20proposed%20method%20outperforms%20state-of-the-art%0Aapproaches%20on%20widely%20used%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20106v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Enriched%20Features%20via%20Selective%20State%20Spaces%20Model%20for%0A%20%20Efficient%20Image%20Deblurring&entry.906535625=Hu%20Gao%20and%20Depeng%20Dang&entry.1292438233=%20%20Image%20deblurring%20aims%20to%20restore%20a%20high-quality%20image%20from%20its%20corresponding%0Ablurred.%20The%20emergence%20of%20CNNs%20and%20Transformers%20has%20enabled%20significant%0Aprogress.%20However%2C%20these%20methods%20often%20face%20the%20dilemma%20between%20eliminating%0Along-range%20degradation%20perturbations%20and%20maintaining%20computational%20efficiency.%0AWhile%20the%20selective%20state%20space%20model%20%28SSM%29%20shows%20promise%20in%20modeling%0Along-range%20dependencies%20with%20linear%20complexity%2C%20it%20also%20encounters%20challenges%0Asuch%20as%20local%20pixel%20forgetting%20and%20channel%20redundancy.%20To%20address%20this%20issue%2C%0Awe%20propose%20an%20efficient%20image%20deblurring%20network%20that%20leverages%20selective%20state%0Aspaces%20model%20to%20aggregate%20enriched%20and%20accurate%20features.%20Specifically%2C%20we%0Aintroduce%20an%20aggregate%20local%20and%20global%20information%20block%20%28ALGBlock%29%20designed%0Ato%20effectively%20capture%20and%20integrate%20both%20local%20invariant%20properties%20and%0Anon-local%20information.%20The%20ALGBlock%20comprises%20two%20primary%20modules%3A%20a%20module%20for%0Acapturing%20local%20and%20global%20features%20%28CLGF%29%2C%20and%20a%20feature%20aggregation%20module%0A%28FA%29.%20The%20CLGF%20module%20is%20composed%20of%20two%20branches%3A%20the%20global%20branch%20captures%0Along-range%20dependency%20features%20via%20a%20selective%20state%20spaces%20model%2C%20while%20the%0Alocal%20branch%20employs%20simplified%20channel%20attention%20to%20model%20local%20connectivity%2C%0Athereby%20reducing%20local%20pixel%20forgetting%20and%20channel%20redundancy.%20In%20addition%2C%20we%0Adesign%20a%20FA%20module%20to%20accentuate%20the%20local%20part%20by%20recalibrating%20the%20weight%0Aduring%20the%20aggregation%20of%20the%20two%20branches%20for%20restoration.%20Experimental%0Aresults%20demonstrate%20that%20the%20proposed%20method%20outperforms%20state-of-the-art%0Aapproaches%20on%20widely%20used%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20106v2&entry.124074799=Read"},
{"title": "Growing Q-Networks: Solving Continuous Control Tasks with Adaptive\n  Control Resolution", "author": "Tim Seyde and Peter Werner and Wilko Schwarting and Markus Wulfmeier and Daniela Rus", "abstract": "  Recent reinforcement learning approaches have shown surprisingly strong\ncapabilities of bang-bang policies for solving continuous control benchmarks.\nThe underlying coarse action space discretizations often yield favourable\nexploration characteristics while final performance does not visibly suffer in\nthe absence of action penalization in line with optimal control theory. In\nrobotics applications, smooth control signals are commonly preferred to reduce\nsystem wear and energy efficiency, but action costs can be detrimental to\nexploration during early training. In this work, we aim to bridge this\nperformance gap by growing discrete action spaces from coarse to fine control\nresolution, taking advantage of recent results in decoupled Q-learning to scale\nour approach to high-dimensional action spaces up to dim(A) = 38. Our work\nindicates that an adaptive control resolution in combination with value\ndecomposition yields simple critic-only algorithms that yield surprisingly\nstrong performance on continuous control tasks.\n", "link": "http://arxiv.org/abs/2404.04253v1", "date": "2024-04-05", "relevancy": 1.6612, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5754}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5686}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4948}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Growing%20Q-Networks%3A%20Solving%20Continuous%20Control%20Tasks%20with%20Adaptive%0A%20%20Control%20Resolution&body=Title%3A%20Growing%20Q-Networks%3A%20Solving%20Continuous%20Control%20Tasks%20with%20Adaptive%0A%20%20Control%20Resolution%0AAuthor%3A%20Tim%20Seyde%20and%20Peter%20Werner%20and%20Wilko%20Schwarting%20and%20Markus%20Wulfmeier%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20Recent%20reinforcement%20learning%20approaches%20have%20shown%20surprisingly%20strong%0Acapabilities%20of%20bang-bang%20policies%20for%20solving%20continuous%20control%20benchmarks.%0AThe%20underlying%20coarse%20action%20space%20discretizations%20often%20yield%20favourable%0Aexploration%20characteristics%20while%20final%20performance%20does%20not%20visibly%20suffer%20in%0Athe%20absence%20of%20action%20penalization%20in%20line%20with%20optimal%20control%20theory.%20In%0Arobotics%20applications%2C%20smooth%20control%20signals%20are%20commonly%20preferred%20to%20reduce%0Asystem%20wear%20and%20energy%20efficiency%2C%20but%20action%20costs%20can%20be%20detrimental%20to%0Aexploration%20during%20early%20training.%20In%20this%20work%2C%20we%20aim%20to%20bridge%20this%0Aperformance%20gap%20by%20growing%20discrete%20action%20spaces%20from%20coarse%20to%20fine%20control%0Aresolution%2C%20taking%20advantage%20of%20recent%20results%20in%20decoupled%20Q-learning%20to%20scale%0Aour%20approach%20to%20high-dimensional%20action%20spaces%20up%20to%20dim%28A%29%20%3D%2038.%20Our%20work%0Aindicates%20that%20an%20adaptive%20control%20resolution%20in%20combination%20with%20value%0Adecomposition%20yields%20simple%20critic-only%20algorithms%20that%20yield%20surprisingly%0Astrong%20performance%20on%20continuous%20control%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04253v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Growing%20Q-Networks%3A%20Solving%20Continuous%20Control%20Tasks%20with%20Adaptive%0A%20%20Control%20Resolution&entry.906535625=Tim%20Seyde%20and%20Peter%20Werner%20and%20Wilko%20Schwarting%20and%20Markus%20Wulfmeier%20and%20Daniela%20Rus&entry.1292438233=%20%20Recent%20reinforcement%20learning%20approaches%20have%20shown%20surprisingly%20strong%0Acapabilities%20of%20bang-bang%20policies%20for%20solving%20continuous%20control%20benchmarks.%0AThe%20underlying%20coarse%20action%20space%20discretizations%20often%20yield%20favourable%0Aexploration%20characteristics%20while%20final%20performance%20does%20not%20visibly%20suffer%20in%0Athe%20absence%20of%20action%20penalization%20in%20line%20with%20optimal%20control%20theory.%20In%0Arobotics%20applications%2C%20smooth%20control%20signals%20are%20commonly%20preferred%20to%20reduce%0Asystem%20wear%20and%20energy%20efficiency%2C%20but%20action%20costs%20can%20be%20detrimental%20to%0Aexploration%20during%20early%20training.%20In%20this%20work%2C%20we%20aim%20to%20bridge%20this%0Aperformance%20gap%20by%20growing%20discrete%20action%20spaces%20from%20coarse%20to%20fine%20control%0Aresolution%2C%20taking%20advantage%20of%20recent%20results%20in%20decoupled%20Q-learning%20to%20scale%0Aour%20approach%20to%20high-dimensional%20action%20spaces%20up%20to%20dim%28A%29%20%3D%2038.%20Our%20work%0Aindicates%20that%20an%20adaptive%20control%20resolution%20in%20combination%20with%20value%0Adecomposition%20yields%20simple%20critic-only%20algorithms%20that%20yield%20surprisingly%0Astrong%20performance%20on%20continuous%20control%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04253v1&entry.124074799=Read"},
{"title": "Efficient Detection of Exchangeable Factors in Factor Graphs", "author": "Malte Luttermann and Johann Machemer and Marcel Gehrke", "abstract": "  To allow for tractable probabilistic inference with respect to domain sizes,\nlifted probabilistic inference exploits symmetries in probabilistic graphical\nmodels. However, checking whether two factors encode equivalent semantics and\nhence are exchangeable is computationally expensive. In this paper, we\nefficiently solve the problem of detecting exchangeable factors in a factor\ngraph. In particular, we introduce the detection of exchangeable factors (DEFT)\nalgorithm, which allows us to drastically reduce the computational effort for\nchecking whether two factors are exchangeable in practice. While previous\napproaches iterate all $O(n!)$ permutations of a factor's argument list in the\nworst case (where $n$ is the number of arguments of the factor), we prove that\nDEFT efficiently identifies restrictions to drastically reduce the number of\npermutations and validate the efficiency of DEFT in our empirical evaluation.\n", "link": "http://arxiv.org/abs/2403.10167v2", "date": "2024-04-05", "relevancy": 1.6438, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4315}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3992}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Detection%20of%20Exchangeable%20Factors%20in%20Factor%20Graphs&body=Title%3A%20Efficient%20Detection%20of%20Exchangeable%20Factors%20in%20Factor%20Graphs%0AAuthor%3A%20Malte%20Luttermann%20and%20Johann%20Machemer%20and%20Marcel%20Gehrke%0AAbstract%3A%20%20%20To%20allow%20for%20tractable%20probabilistic%20inference%20with%20respect%20to%20domain%20sizes%2C%0Alifted%20probabilistic%20inference%20exploits%20symmetries%20in%20probabilistic%20graphical%0Amodels.%20However%2C%20checking%20whether%20two%20factors%20encode%20equivalent%20semantics%20and%0Ahence%20are%20exchangeable%20is%20computationally%20expensive.%20In%20this%20paper%2C%20we%0Aefficiently%20solve%20the%20problem%20of%20detecting%20exchangeable%20factors%20in%20a%20factor%0Agraph.%20In%20particular%2C%20we%20introduce%20the%20detection%20of%20exchangeable%20factors%20%28DEFT%29%0Aalgorithm%2C%20which%20allows%20us%20to%20drastically%20reduce%20the%20computational%20effort%20for%0Achecking%20whether%20two%20factors%20are%20exchangeable%20in%20practice.%20While%20previous%0Aapproaches%20iterate%20all%20%24O%28n%21%29%24%20permutations%20of%20a%20factor%27s%20argument%20list%20in%20the%0Aworst%20case%20%28where%20%24n%24%20is%20the%20number%20of%20arguments%20of%20the%20factor%29%2C%20we%20prove%20that%0ADEFT%20efficiently%20identifies%20restrictions%20to%20drastically%20reduce%20the%20number%20of%0Apermutations%20and%20validate%20the%20efficiency%20of%20DEFT%20in%20our%20empirical%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10167v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Detection%20of%20Exchangeable%20Factors%20in%20Factor%20Graphs&entry.906535625=Malte%20Luttermann%20and%20Johann%20Machemer%20and%20Marcel%20Gehrke&entry.1292438233=%20%20To%20allow%20for%20tractable%20probabilistic%20inference%20with%20respect%20to%20domain%20sizes%2C%0Alifted%20probabilistic%20inference%20exploits%20symmetries%20in%20probabilistic%20graphical%0Amodels.%20However%2C%20checking%20whether%20two%20factors%20encode%20equivalent%20semantics%20and%0Ahence%20are%20exchangeable%20is%20computationally%20expensive.%20In%20this%20paper%2C%20we%0Aefficiently%20solve%20the%20problem%20of%20detecting%20exchangeable%20factors%20in%20a%20factor%0Agraph.%20In%20particular%2C%20we%20introduce%20the%20detection%20of%20exchangeable%20factors%20%28DEFT%29%0Aalgorithm%2C%20which%20allows%20us%20to%20drastically%20reduce%20the%20computational%20effort%20for%0Achecking%20whether%20two%20factors%20are%20exchangeable%20in%20practice.%20While%20previous%0Aapproaches%20iterate%20all%20%24O%28n%21%29%24%20permutations%20of%20a%20factor%27s%20argument%20list%20in%20the%0Aworst%20case%20%28where%20%24n%24%20is%20the%20number%20of%20arguments%20of%20the%20factor%29%2C%20we%20prove%20that%0ADEFT%20efficiently%20identifies%20restrictions%20to%20drastically%20reduce%20the%20number%20of%0Apermutations%20and%20validate%20the%20efficiency%20of%20DEFT%20in%20our%20empirical%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10167v2&entry.124074799=Read"},
{"title": "EGTR: Extracting Graph from Transformer for Scene Graph Generation", "author": "Jinbae Im and JeongYeon Nam and Nokyung Park and Hyungmin Lee and Seunghyun Park", "abstract": "  Scene Graph Generation (SGG) is a challenging task of detecting objects and\npredicting relationships between objects. After DETR was developed, one-stage\nSGG models based on a one-stage object detector have been actively studied.\nHowever, complex modeling is used to predict the relationship between objects,\nand the inherent relationship between object queries learned in the multi-head\nself-attention of the object detector has been neglected. We propose a\nlightweight one-stage SGG model that extracts the relation graph from the\nvarious relationships learned in the multi-head self-attention layers of the\nDETR decoder. By fully utilizing the self-attention by-products, the relation\ngraph can be extracted effectively with a shallow relation extraction head.\nConsidering the dependency of the relation extraction task on the object\ndetection task, we propose a novel relation smoothing technique that adjusts\nthe relation label adaptively according to the quality of the detected objects.\nBy the relation smoothing, the model is trained according to the continuous\ncurriculum that focuses on object detection task at the beginning of training\nand performs multi-task learning as the object detection performance gradually\nimproves. Furthermore, we propose a connectivity prediction task that predicts\nwhether a relation exists between object pairs as an auxiliary task of the\nrelation extraction. We demonstrate the effectiveness and efficiency of our\nmethod for the Visual Genome and Open Image V6 datasets. Our code is publicly\navailable at https://github.com/naver-ai/egtr.\n", "link": "http://arxiv.org/abs/2404.02072v3", "date": "2024-04-05", "relevancy": 1.6265, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5633}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5242}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5072}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation&body=Title%3A%20EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation%0AAuthor%3A%20Jinbae%20Im%20and%20JeongYeon%20Nam%20and%20Nokyung%20Park%20and%20Hyungmin%20Lee%20and%20Seunghyun%20Park%0AAbstract%3A%20%20%20Scene%20Graph%20Generation%20%28SGG%29%20is%20a%20challenging%20task%20of%20detecting%20objects%20and%0Apredicting%20relationships%20between%20objects.%20After%20DETR%20was%20developed%2C%20one-stage%0ASGG%20models%20based%20on%20a%20one-stage%20object%20detector%20have%20been%20actively%20studied.%0AHowever%2C%20complex%20modeling%20is%20used%20to%20predict%20the%20relationship%20between%20objects%2C%0Aand%20the%20inherent%20relationship%20between%20object%20queries%20learned%20in%20the%20multi-head%0Aself-attention%20of%20the%20object%20detector%20has%20been%20neglected.%20We%20propose%20a%0Alightweight%20one-stage%20SGG%20model%20that%20extracts%20the%20relation%20graph%20from%20the%0Avarious%20relationships%20learned%20in%20the%20multi-head%20self-attention%20layers%20of%20the%0ADETR%20decoder.%20By%20fully%20utilizing%20the%20self-attention%20by-products%2C%20the%20relation%0Agraph%20can%20be%20extracted%20effectively%20with%20a%20shallow%20relation%20extraction%20head.%0AConsidering%20the%20dependency%20of%20the%20relation%20extraction%20task%20on%20the%20object%0Adetection%20task%2C%20we%20propose%20a%20novel%20relation%20smoothing%20technique%20that%20adjusts%0Athe%20relation%20label%20adaptively%20according%20to%20the%20quality%20of%20the%20detected%20objects.%0ABy%20the%20relation%20smoothing%2C%20the%20model%20is%20trained%20according%20to%20the%20continuous%0Acurriculum%20that%20focuses%20on%20object%20detection%20task%20at%20the%20beginning%20of%20training%0Aand%20performs%20multi-task%20learning%20as%20the%20object%20detection%20performance%20gradually%0Aimproves.%20Furthermore%2C%20we%20propose%20a%20connectivity%20prediction%20task%20that%20predicts%0Awhether%20a%20relation%20exists%20between%20object%20pairs%20as%20an%20auxiliary%20task%20of%20the%0Arelation%20extraction.%20We%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%0Amethod%20for%20the%20Visual%20Genome%20and%20Open%20Image%20V6%20datasets.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/naver-ai/egtr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02072v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation&entry.906535625=Jinbae%20Im%20and%20JeongYeon%20Nam%20and%20Nokyung%20Park%20and%20Hyungmin%20Lee%20and%20Seunghyun%20Park&entry.1292438233=%20%20Scene%20Graph%20Generation%20%28SGG%29%20is%20a%20challenging%20task%20of%20detecting%20objects%20and%0Apredicting%20relationships%20between%20objects.%20After%20DETR%20was%20developed%2C%20one-stage%0ASGG%20models%20based%20on%20a%20one-stage%20object%20detector%20have%20been%20actively%20studied.%0AHowever%2C%20complex%20modeling%20is%20used%20to%20predict%20the%20relationship%20between%20objects%2C%0Aand%20the%20inherent%20relationship%20between%20object%20queries%20learned%20in%20the%20multi-head%0Aself-attention%20of%20the%20object%20detector%20has%20been%20neglected.%20We%20propose%20a%0Alightweight%20one-stage%20SGG%20model%20that%20extracts%20the%20relation%20graph%20from%20the%0Avarious%20relationships%20learned%20in%20the%20multi-head%20self-attention%20layers%20of%20the%0ADETR%20decoder.%20By%20fully%20utilizing%20the%20self-attention%20by-products%2C%20the%20relation%0Agraph%20can%20be%20extracted%20effectively%20with%20a%20shallow%20relation%20extraction%20head.%0AConsidering%20the%20dependency%20of%20the%20relation%20extraction%20task%20on%20the%20object%0Adetection%20task%2C%20we%20propose%20a%20novel%20relation%20smoothing%20technique%20that%20adjusts%0Athe%20relation%20label%20adaptively%20according%20to%20the%20quality%20of%20the%20detected%20objects.%0ABy%20the%20relation%20smoothing%2C%20the%20model%20is%20trained%20according%20to%20the%20continuous%0Acurriculum%20that%20focuses%20on%20object%20detection%20task%20at%20the%20beginning%20of%20training%0Aand%20performs%20multi-task%20learning%20as%20the%20object%20detection%20performance%20gradually%0Aimproves.%20Furthermore%2C%20we%20propose%20a%20connectivity%20prediction%20task%20that%20predicts%0Awhether%20a%20relation%20exists%20between%20object%20pairs%20as%20an%20auxiliary%20task%20of%20the%0Arelation%20extraction.%20We%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%0Amethod%20for%20the%20Visual%20Genome%20and%20Open%20Image%20V6%20datasets.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/naver-ai/egtr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02072v3&entry.124074799=Read"},
{"title": "Convex MPC and Thrust Allocation with Deadband for Spacecraft Rendezvous", "author": "Pedro Taborda and Hugo Matias and Daniel Silvestre and Pedro Louren\u00e7o", "abstract": "  This paper delves into a rendezvous scenario involving a chaser and a target\nspacecraft, focusing on the application of Model Predictive Control (MPC) to\ndesign a controller capable of guiding the chaser toward the target. The\noperational principle of spacecraft thrusters, requiring a minimum activation\ntime that leads to the existence of a control deadband, introduces\nmixed-integer constraints into the optimization, posing a considerable\ncomputational challenge due to the exponential complexity on the number of\ninteger constraints. We address this complexity by presenting two solver\nalgorithms that efficiently approximate the optimal solution in significantly\nless time than standard solvers, making them well-suited for real-time\napplications.\n", "link": "http://arxiv.org/abs/2404.04197v1", "date": "2024-04-05", "relevancy": 1.6137, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.436}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4028}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.391}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Convex%20MPC%20and%20Thrust%20Allocation%20with%20Deadband%20for%20Spacecraft%20Rendezvous&body=Title%3A%20Convex%20MPC%20and%20Thrust%20Allocation%20with%20Deadband%20for%20Spacecraft%20Rendezvous%0AAuthor%3A%20Pedro%20Taborda%20and%20Hugo%20Matias%20and%20Daniel%20Silvestre%20and%20Pedro%20Louren%C3%A7o%0AAbstract%3A%20%20%20This%20paper%20delves%20into%20a%20rendezvous%20scenario%20involving%20a%20chaser%20and%20a%20target%0Aspacecraft%2C%20focusing%20on%20the%20application%20of%20Model%20Predictive%20Control%20%28MPC%29%20to%0Adesign%20a%20controller%20capable%20of%20guiding%20the%20chaser%20toward%20the%20target.%20The%0Aoperational%20principle%20of%20spacecraft%20thrusters%2C%20requiring%20a%20minimum%20activation%0Atime%20that%20leads%20to%20the%20existence%20of%20a%20control%20deadband%2C%20introduces%0Amixed-integer%20constraints%20into%20the%20optimization%2C%20posing%20a%20considerable%0Acomputational%20challenge%20due%20to%20the%20exponential%20complexity%20on%20the%20number%20of%0Ainteger%20constraints.%20We%20address%20this%20complexity%20by%20presenting%20two%20solver%0Aalgorithms%20that%20efficiently%20approximate%20the%20optimal%20solution%20in%20significantly%0Aless%20time%20than%20standard%20solvers%2C%20making%20them%20well-suited%20for%20real-time%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04197v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convex%20MPC%20and%20Thrust%20Allocation%20with%20Deadband%20for%20Spacecraft%20Rendezvous&entry.906535625=Pedro%20Taborda%20and%20Hugo%20Matias%20and%20Daniel%20Silvestre%20and%20Pedro%20Louren%C3%A7o&entry.1292438233=%20%20This%20paper%20delves%20into%20a%20rendezvous%20scenario%20involving%20a%20chaser%20and%20a%20target%0Aspacecraft%2C%20focusing%20on%20the%20application%20of%20Model%20Predictive%20Control%20%28MPC%29%20to%0Adesign%20a%20controller%20capable%20of%20guiding%20the%20chaser%20toward%20the%20target.%20The%0Aoperational%20principle%20of%20spacecraft%20thrusters%2C%20requiring%20a%20minimum%20activation%0Atime%20that%20leads%20to%20the%20existence%20of%20a%20control%20deadband%2C%20introduces%0Amixed-integer%20constraints%20into%20the%20optimization%2C%20posing%20a%20considerable%0Acomputational%20challenge%20due%20to%20the%20exponential%20complexity%20on%20the%20number%20of%0Ainteger%20constraints.%20We%20address%20this%20complexity%20by%20presenting%20two%20solver%0Aalgorithms%20that%20efficiently%20approximate%20the%20optimal%20solution%20in%20significantly%0Aless%20time%20than%20standard%20solvers%2C%20making%20them%20well-suited%20for%20real-time%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04197v1&entry.124074799=Read"},
{"title": "Estimation of Concept Explanations Should be Uncertainty Aware", "author": "Vihari Piratla and Juyeon Heo and Katherine M. Collins and Sukriti Singh and Adrian Weller", "abstract": "  Model explanations can be valuable for interpreting and debugging predictive\nmodels. We study a specific kind called Concept Explanations, where the goal is\nto interpret a model using human-understandable concepts. Although popular for\ntheir easy interpretation, concept explanations are known to be noisy. We begin\nour work by identifying various sources of uncertainty in the estimation\npipeline that lead to such noise. We then propose an uncertainty-aware Bayesian\nestimation method to address these issues, which readily improved the quality\nof explanations. We demonstrate with theoretical analysis and empirical\nevaluation that explanations computed by our method are robust to train-time\nchoices while also being label-efficient. Further, our method proved capable of\nrecovering relevant concepts amongst a bank of thousands, in an evaluation with\nreal-datasets and off-the-shelf models, demonstrating its scalability. We\nbelieve the improved quality of uncertainty-aware concept explanations make\nthem a strong candidate for more reliable model interpretation. We release our\ncode at https://github.com/vps-anonconfs/uace.\n", "link": "http://arxiv.org/abs/2312.08063v2", "date": "2024-04-05", "relevancy": 1.6106, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5448}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Estimation%20of%20Concept%20Explanations%20Should%20be%20Uncertainty%20Aware&body=Title%3A%20Estimation%20of%20Concept%20Explanations%20Should%20be%20Uncertainty%20Aware%0AAuthor%3A%20Vihari%20Piratla%20and%20Juyeon%20Heo%20and%20Katherine%20M.%20Collins%20and%20Sukriti%20Singh%20and%20Adrian%20Weller%0AAbstract%3A%20%20%20Model%20explanations%20can%20be%20valuable%20for%20interpreting%20and%20debugging%20predictive%0Amodels.%20We%20study%20a%20specific%20kind%20called%20Concept%20Explanations%2C%20where%20the%20goal%20is%0Ato%20interpret%20a%20model%20using%20human-understandable%20concepts.%20Although%20popular%20for%0Atheir%20easy%20interpretation%2C%20concept%20explanations%20are%20known%20to%20be%20noisy.%20We%20begin%0Aour%20work%20by%20identifying%20various%20sources%20of%20uncertainty%20in%20the%20estimation%0Apipeline%20that%20lead%20to%20such%20noise.%20We%20then%20propose%20an%20uncertainty-aware%20Bayesian%0Aestimation%20method%20to%20address%20these%20issues%2C%20which%20readily%20improved%20the%20quality%0Aof%20explanations.%20We%20demonstrate%20with%20theoretical%20analysis%20and%20empirical%0Aevaluation%20that%20explanations%20computed%20by%20our%20method%20are%20robust%20to%20train-time%0Achoices%20while%20also%20being%20label-efficient.%20Further%2C%20our%20method%20proved%20capable%20of%0Arecovering%20relevant%20concepts%20amongst%20a%20bank%20of%20thousands%2C%20in%20an%20evaluation%20with%0Areal-datasets%20and%20off-the-shelf%20models%2C%20demonstrating%20its%20scalability.%20We%0Abelieve%20the%20improved%20quality%20of%20uncertainty-aware%20concept%20explanations%20make%0Athem%20a%20strong%20candidate%20for%20more%20reliable%20model%20interpretation.%20We%20release%20our%0Acode%20at%20https%3A//github.com/vps-anonconfs/uace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08063v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimation%20of%20Concept%20Explanations%20Should%20be%20Uncertainty%20Aware&entry.906535625=Vihari%20Piratla%20and%20Juyeon%20Heo%20and%20Katherine%20M.%20Collins%20and%20Sukriti%20Singh%20and%20Adrian%20Weller&entry.1292438233=%20%20Model%20explanations%20can%20be%20valuable%20for%20interpreting%20and%20debugging%20predictive%0Amodels.%20We%20study%20a%20specific%20kind%20called%20Concept%20Explanations%2C%20where%20the%20goal%20is%0Ato%20interpret%20a%20model%20using%20human-understandable%20concepts.%20Although%20popular%20for%0Atheir%20easy%20interpretation%2C%20concept%20explanations%20are%20known%20to%20be%20noisy.%20We%20begin%0Aour%20work%20by%20identifying%20various%20sources%20of%20uncertainty%20in%20the%20estimation%0Apipeline%20that%20lead%20to%20such%20noise.%20We%20then%20propose%20an%20uncertainty-aware%20Bayesian%0Aestimation%20method%20to%20address%20these%20issues%2C%20which%20readily%20improved%20the%20quality%0Aof%20explanations.%20We%20demonstrate%20with%20theoretical%20analysis%20and%20empirical%0Aevaluation%20that%20explanations%20computed%20by%20our%20method%20are%20robust%20to%20train-time%0Achoices%20while%20also%20being%20label-efficient.%20Further%2C%20our%20method%20proved%20capable%20of%0Arecovering%20relevant%20concepts%20amongst%20a%20bank%20of%20thousands%2C%20in%20an%20evaluation%20with%0Areal-datasets%20and%20off-the-shelf%20models%2C%20demonstrating%20its%20scalability.%20We%0Abelieve%20the%20improved%20quality%20of%20uncertainty-aware%20concept%20explanations%20make%0Athem%20a%20strong%20candidate%20for%20more%20reliable%20model%20interpretation.%20We%20release%20our%0Acode%20at%20https%3A//github.com/vps-anonconfs/uace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08063v2&entry.124074799=Read"},
{"title": "InstructHumans: Editing Animated 3D Human Textures with Instructions", "author": "Jiayin Zhu and Linlin Yang and Angela Yao", "abstract": "  We present InstructHumans, a novel framework for instruction-driven 3D human\ntexture editing. Existing text-based editing methods use Score Distillation\nSampling (SDS) to distill guidance from generative models. This work shows that\nnaively using such scores is harmful to editing as they destroy consistency\nwith the source avatar. Instead, we propose an alternate SDS for Editing\n(SDS-E) that selectively incorporates subterms of SDS across diffusion\ntimesteps. We further enhance SDS-E with spatial smoothness regularization and\ngradient-based viewpoint sampling to achieve high-quality edits with sharp and\nhigh-fidelity detailing. InstructHumans significantly outperforms existing 3D\nediting methods, consistent with the initial avatar while faithful to the\ntextual instructions. Project page: https://jyzhu.top/instruct-humans .\n", "link": "http://arxiv.org/abs/2404.04037v1", "date": "2024-04-05", "relevancy": 1.6066, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5693}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5313}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5237}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InstructHumans%3A%20Editing%20Animated%203D%20Human%20Textures%20with%20Instructions&body=Title%3A%20InstructHumans%3A%20Editing%20Animated%203D%20Human%20Textures%20with%20Instructions%0AAuthor%3A%20Jiayin%20Zhu%20and%20Linlin%20Yang%20and%20Angela%20Yao%0AAbstract%3A%20%20%20We%20present%20InstructHumans%2C%20a%20novel%20framework%20for%20instruction-driven%203D%20human%0Atexture%20editing.%20Existing%20text-based%20editing%20methods%20use%20Score%20Distillation%0ASampling%20%28SDS%29%20to%20distill%20guidance%20from%20generative%20models.%20This%20work%20shows%20that%0Anaively%20using%20such%20scores%20is%20harmful%20to%20editing%20as%20they%20destroy%20consistency%0Awith%20the%20source%20avatar.%20Instead%2C%20we%20propose%20an%20alternate%20SDS%20for%20Editing%0A%28SDS-E%29%20that%20selectively%20incorporates%20subterms%20of%20SDS%20across%20diffusion%0Atimesteps.%20We%20further%20enhance%20SDS-E%20with%20spatial%20smoothness%20regularization%20and%0Agradient-based%20viewpoint%20sampling%20to%20achieve%20high-quality%20edits%20with%20sharp%20and%0Ahigh-fidelity%20detailing.%20InstructHumans%20significantly%20outperforms%20existing%203D%0Aediting%20methods%2C%20consistent%20with%20the%20initial%20avatar%20while%20faithful%20to%20the%0Atextual%20instructions.%20Project%20page%3A%20https%3A//jyzhu.top/instruct-humans%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04037v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructHumans%3A%20Editing%20Animated%203D%20Human%20Textures%20with%20Instructions&entry.906535625=Jiayin%20Zhu%20and%20Linlin%20Yang%20and%20Angela%20Yao&entry.1292438233=%20%20We%20present%20InstructHumans%2C%20a%20novel%20framework%20for%20instruction-driven%203D%20human%0Atexture%20editing.%20Existing%20text-based%20editing%20methods%20use%20Score%20Distillation%0ASampling%20%28SDS%29%20to%20distill%20guidance%20from%20generative%20models.%20This%20work%20shows%20that%0Anaively%20using%20such%20scores%20is%20harmful%20to%20editing%20as%20they%20destroy%20consistency%0Awith%20the%20source%20avatar.%20Instead%2C%20we%20propose%20an%20alternate%20SDS%20for%20Editing%0A%28SDS-E%29%20that%20selectively%20incorporates%20subterms%20of%20SDS%20across%20diffusion%0Atimesteps.%20We%20further%20enhance%20SDS-E%20with%20spatial%20smoothness%20regularization%20and%0Agradient-based%20viewpoint%20sampling%20to%20achieve%20high-quality%20edits%20with%20sharp%20and%0Ahigh-fidelity%20detailing.%20InstructHumans%20significantly%20outperforms%20existing%203D%0Aediting%20methods%2C%20consistent%20with%20the%20initial%20avatar%20while%20faithful%20to%20the%0Atextual%20instructions.%20Project%20page%3A%20https%3A//jyzhu.top/instruct-humans%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04037v1&entry.124074799=Read"},
{"title": "Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner\n  Attacks, And The Role of Distillation as Defense Mechanism", "author": "Trilokesh Ranjan Sarkar and Nilanjan Das and Pralay Sankar Maitra and Bijoy Some and Ritwik Saha and Orijita Adhikary and Bishal Bose and Jaydip Sen", "abstract": "  This technical report delves into an in-depth exploration of adversarial\nattacks specifically targeted at Deep Neural Networks (DNNs) utilized for image\nclassification. The study also investigates defense mechanisms aimed at\nbolstering the robustness of machine learning models. The research focuses on\ncomprehending the ramifications of two prominent attack methodologies: the Fast\nGradient Sign Method (FGSM) and the Carlini-Wagner (CW) approach. These attacks\nare examined concerning three pre-trained image classifiers: Resnext50_32x4d,\nDenseNet-201, and VGG-19, utilizing the Tiny-ImageNet dataset. Furthermore, the\nstudy proposes the robustness of defensive distillation as a defense mechanism\nto counter FGSM and CW attacks. This defense mechanism is evaluated using the\nCIFAR-10 dataset, where CNN models, specifically resnet101 and Resnext50_32x4d,\nserve as the teacher and student models, respectively. The proposed defensive\ndistillation model exhibits effectiveness in thwarting attacks such as FGSM.\nHowever, it is noted to remain susceptible to more sophisticated techniques\nlike the CW attack. The document presents a meticulous validation of the\nproposed scheme. It provides detailed and comprehensive results, elucidating\nthe efficacy and limitations of the defense mechanisms employed. Through\nrigorous experimentation and analysis, the study offers insights into the\ndynamics of adversarial attacks on DNNs, as well as the effectiveness of\ndefensive strategies in mitigating their impact.\n", "link": "http://arxiv.org/abs/2404.04245v1", "date": "2024-04-05", "relevancy": 1.4276, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4813}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4793}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4723}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Adversarial%20Robustness%3A%20A%20Comparison%20Of%20FGSM%2C%20Carlini-Wagner%0A%20%20Attacks%2C%20And%20The%20Role%20of%20Distillation%20as%20Defense%20Mechanism&body=Title%3A%20Evaluating%20Adversarial%20Robustness%3A%20A%20Comparison%20Of%20FGSM%2C%20Carlini-Wagner%0A%20%20Attacks%2C%20And%20The%20Role%20of%20Distillation%20as%20Defense%20Mechanism%0AAuthor%3A%20Trilokesh%20Ranjan%20Sarkar%20and%20Nilanjan%20Das%20and%20Pralay%20Sankar%20Maitra%20and%20Bijoy%20Some%20and%20Ritwik%20Saha%20and%20Orijita%20Adhikary%20and%20Bishal%20Bose%20and%20Jaydip%20Sen%0AAbstract%3A%20%20%20This%20technical%20report%20delves%20into%20an%20in-depth%20exploration%20of%20adversarial%0Aattacks%20specifically%20targeted%20at%20Deep%20Neural%20Networks%20%28DNNs%29%20utilized%20for%20image%0Aclassification.%20The%20study%20also%20investigates%20defense%20mechanisms%20aimed%20at%0Abolstering%20the%20robustness%20of%20machine%20learning%20models.%20The%20research%20focuses%20on%0Acomprehending%20the%20ramifications%20of%20two%20prominent%20attack%20methodologies%3A%20the%20Fast%0AGradient%20Sign%20Method%20%28FGSM%29%20and%20the%20Carlini-Wagner%20%28CW%29%20approach.%20These%20attacks%0Aare%20examined%20concerning%20three%20pre-trained%20image%20classifiers%3A%20Resnext50_32x4d%2C%0ADenseNet-201%2C%20and%20VGG-19%2C%20utilizing%20the%20Tiny-ImageNet%20dataset.%20Furthermore%2C%20the%0Astudy%20proposes%20the%20robustness%20of%20defensive%20distillation%20as%20a%20defense%20mechanism%0Ato%20counter%20FGSM%20and%20CW%20attacks.%20This%20defense%20mechanism%20is%20evaluated%20using%20the%0ACIFAR-10%20dataset%2C%20where%20CNN%20models%2C%20specifically%20resnet101%20and%20Resnext50_32x4d%2C%0Aserve%20as%20the%20teacher%20and%20student%20models%2C%20respectively.%20The%20proposed%20defensive%0Adistillation%20model%20exhibits%20effectiveness%20in%20thwarting%20attacks%20such%20as%20FGSM.%0AHowever%2C%20it%20is%20noted%20to%20remain%20susceptible%20to%20more%20sophisticated%20techniques%0Alike%20the%20CW%20attack.%20The%20document%20presents%20a%20meticulous%20validation%20of%20the%0Aproposed%20scheme.%20It%20provides%20detailed%20and%20comprehensive%20results%2C%20elucidating%0Athe%20efficacy%20and%20limitations%20of%20the%20defense%20mechanisms%20employed.%20Through%0Arigorous%20experimentation%20and%20analysis%2C%20the%20study%20offers%20insights%20into%20the%0Adynamics%20of%20adversarial%20attacks%20on%20DNNs%2C%20as%20well%20as%20the%20effectiveness%20of%0Adefensive%20strategies%20in%20mitigating%20their%20impact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04245v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Adversarial%20Robustness%3A%20A%20Comparison%20Of%20FGSM%2C%20Carlini-Wagner%0A%20%20Attacks%2C%20And%20The%20Role%20of%20Distillation%20as%20Defense%20Mechanism&entry.906535625=Trilokesh%20Ranjan%20Sarkar%20and%20Nilanjan%20Das%20and%20Pralay%20Sankar%20Maitra%20and%20Bijoy%20Some%20and%20Ritwik%20Saha%20and%20Orijita%20Adhikary%20and%20Bishal%20Bose%20and%20Jaydip%20Sen&entry.1292438233=%20%20This%20technical%20report%20delves%20into%20an%20in-depth%20exploration%20of%20adversarial%0Aattacks%20specifically%20targeted%20at%20Deep%20Neural%20Networks%20%28DNNs%29%20utilized%20for%20image%0Aclassification.%20The%20study%20also%20investigates%20defense%20mechanisms%20aimed%20at%0Abolstering%20the%20robustness%20of%20machine%20learning%20models.%20The%20research%20focuses%20on%0Acomprehending%20the%20ramifications%20of%20two%20prominent%20attack%20methodologies%3A%20the%20Fast%0AGradient%20Sign%20Method%20%28FGSM%29%20and%20the%20Carlini-Wagner%20%28CW%29%20approach.%20These%20attacks%0Aare%20examined%20concerning%20three%20pre-trained%20image%20classifiers%3A%20Resnext50_32x4d%2C%0ADenseNet-201%2C%20and%20VGG-19%2C%20utilizing%20the%20Tiny-ImageNet%20dataset.%20Furthermore%2C%20the%0Astudy%20proposes%20the%20robustness%20of%20defensive%20distillation%20as%20a%20defense%20mechanism%0Ato%20counter%20FGSM%20and%20CW%20attacks.%20This%20defense%20mechanism%20is%20evaluated%20using%20the%0ACIFAR-10%20dataset%2C%20where%20CNN%20models%2C%20specifically%20resnet101%20and%20Resnext50_32x4d%2C%0Aserve%20as%20the%20teacher%20and%20student%20models%2C%20respectively.%20The%20proposed%20defensive%0Adistillation%20model%20exhibits%20effectiveness%20in%20thwarting%20attacks%20such%20as%20FGSM.%0AHowever%2C%20it%20is%20noted%20to%20remain%20susceptible%20to%20more%20sophisticated%20techniques%0Alike%20the%20CW%20attack.%20The%20document%20presents%20a%20meticulous%20validation%20of%20the%0Aproposed%20scheme.%20It%20provides%20detailed%20and%20comprehensive%20results%2C%20elucidating%0Athe%20efficacy%20and%20limitations%20of%20the%20defense%20mechanisms%20employed.%20Through%0Arigorous%20experimentation%20and%20analysis%2C%20the%20study%20offers%20insights%20into%20the%0Adynamics%20of%20adversarial%20attacks%20on%20DNNs%2C%20as%20well%20as%20the%20effectiveness%20of%0Adefensive%20strategies%20in%20mitigating%20their%20impact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04245v1&entry.124074799=Read"},
{"title": "Twins in rotational spectroscopy: Does a rotational spectrum uniquely\n  identify a molecule?", "author": "Marcus Schwarting and Nathan A. Seifert and Michael J. Davis and Ben Blaiszik and Ian Foster and Kirill Prozument", "abstract": "  Rotational spectroscopy is the most accurate method for determining\nstructures of molecules in the gas phase. It is often assumed that a rotational\nspectrum is a unique \"fingerprint\" of a molecule. The availability of large\nmolecular databases and the development of artificial intelligence methods for\nspectroscopy makes the testing of this assumption timely. In this paper, we\npose the determination of molecular structures from rotational spectra as an\ninverse problem. Within this framework, we adopt a funnel-based approach to\nsearch for molecular twins, which are two or more molecules, which have similar\nrotational spectra but distinctly different molecular structures. We\ndemonstrate that there are twins within standard levels of computational\naccuracy by generating rotational constants for many molecules from several\nlarge molecular databases, indicating the inverse problem is ill-posed.\nHowever, some twins can be distinguished by increasing the accuracy of the\ntheoretical methods or by performing additional experiments.\n", "link": "http://arxiv.org/abs/2404.04225v1", "date": "2024-04-05", "relevancy": 1.3113, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3439}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3291}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3202}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Twins%20in%20rotational%20spectroscopy%3A%20Does%20a%20rotational%20spectrum%20uniquely%0A%20%20identify%20a%20molecule%3F&body=Title%3A%20Twins%20in%20rotational%20spectroscopy%3A%20Does%20a%20rotational%20spectrum%20uniquely%0A%20%20identify%20a%20molecule%3F%0AAuthor%3A%20Marcus%20Schwarting%20and%20Nathan%20A.%20Seifert%20and%20Michael%20J.%20Davis%20and%20Ben%20Blaiszik%20and%20Ian%20Foster%20and%20Kirill%20Prozument%0AAbstract%3A%20%20%20Rotational%20spectroscopy%20is%20the%20most%20accurate%20method%20for%20determining%0Astructures%20of%20molecules%20in%20the%20gas%20phase.%20It%20is%20often%20assumed%20that%20a%20rotational%0Aspectrum%20is%20a%20unique%20%22fingerprint%22%20of%20a%20molecule.%20The%20availability%20of%20large%0Amolecular%20databases%20and%20the%20development%20of%20artificial%20intelligence%20methods%20for%0Aspectroscopy%20makes%20the%20testing%20of%20this%20assumption%20timely.%20In%20this%20paper%2C%20we%0Apose%20the%20determination%20of%20molecular%20structures%20from%20rotational%20spectra%20as%20an%0Ainverse%20problem.%20Within%20this%20framework%2C%20we%20adopt%20a%20funnel-based%20approach%20to%0Asearch%20for%20molecular%20twins%2C%20which%20are%20two%20or%20more%20molecules%2C%20which%20have%20similar%0Arotational%20spectra%20but%20distinctly%20different%20molecular%20structures.%20We%0Ademonstrate%20that%20there%20are%20twins%20within%20standard%20levels%20of%20computational%0Aaccuracy%20by%20generating%20rotational%20constants%20for%20many%20molecules%20from%20several%0Alarge%20molecular%20databases%2C%20indicating%20the%20inverse%20problem%20is%20ill-posed.%0AHowever%2C%20some%20twins%20can%20be%20distinguished%20by%20increasing%20the%20accuracy%20of%20the%0Atheoretical%20methods%20or%20by%20performing%20additional%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04225v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Twins%20in%20rotational%20spectroscopy%3A%20Does%20a%20rotational%20spectrum%20uniquely%0A%20%20identify%20a%20molecule%3F&entry.906535625=Marcus%20Schwarting%20and%20Nathan%20A.%20Seifert%20and%20Michael%20J.%20Davis%20and%20Ben%20Blaiszik%20and%20Ian%20Foster%20and%20Kirill%20Prozument&entry.1292438233=%20%20Rotational%20spectroscopy%20is%20the%20most%20accurate%20method%20for%20determining%0Astructures%20of%20molecules%20in%20the%20gas%20phase.%20It%20is%20often%20assumed%20that%20a%20rotational%0Aspectrum%20is%20a%20unique%20%22fingerprint%22%20of%20a%20molecule.%20The%20availability%20of%20large%0Amolecular%20databases%20and%20the%20development%20of%20artificial%20intelligence%20methods%20for%0Aspectroscopy%20makes%20the%20testing%20of%20this%20assumption%20timely.%20In%20this%20paper%2C%20we%0Apose%20the%20determination%20of%20molecular%20structures%20from%20rotational%20spectra%20as%20an%0Ainverse%20problem.%20Within%20this%20framework%2C%20we%20adopt%20a%20funnel-based%20approach%20to%0Asearch%20for%20molecular%20twins%2C%20which%20are%20two%20or%20more%20molecules%2C%20which%20have%20similar%0Arotational%20spectra%20but%20distinctly%20different%20molecular%20structures.%20We%0Ademonstrate%20that%20there%20are%20twins%20within%20standard%20levels%20of%20computational%0Aaccuracy%20by%20generating%20rotational%20constants%20for%20many%20molecules%20from%20several%0Alarge%20molecular%20databases%2C%20indicating%20the%20inverse%20problem%20is%20ill-posed.%0AHowever%2C%20some%20twins%20can%20be%20distinguished%20by%20increasing%20the%20accuracy%20of%20the%0Atheoretical%20methods%20or%20by%20performing%20additional%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04225v1&entry.124074799=Read"},
{"title": "Enhancing Programming Education with ChatGPT: A Case Study on Student\n  Perceptions and Interactions in a Python Course", "author": "Boxaun Ma and Li Chen and Shin'ichi Konomi", "abstract": "  The integration of ChatGPT as a supportive tool in education, notably in\nprogramming courses, addresses the unique challenges of programming education\nby providing assistance with debugging, code generation, and explanations.\nDespite existing research validating ChatGPT's effectiveness, its application\nin university-level programming education and a detailed understanding of\nstudent interactions and perspectives remain limited. This paper explores\nChatGPT's impact on learning in a Python programming course tailored for\nfirst-year students over eight weeks. By analyzing responses from surveys,\nopen-ended questions, and student-ChatGPT dialog data, we aim to provide a\ncomprehensive view of ChatGPT's utility and identify both its advantages and\nlimitations as perceived by students. Our study uncovers a generally positive\nreception toward ChatGPT and offers insights into its role in enhancing the\nprogramming education experience. These findings contribute to the broader\ndiscourse on AI's potential in education, suggesting paths for future research\nand application.\n", "link": "http://arxiv.org/abs/2403.15472v3", "date": "2024-04-05", "relevancy": 1.2778, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4383}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4156}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4054}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Programming%20Education%20with%20ChatGPT%3A%20A%20Case%20Study%20on%20Student%0A%20%20Perceptions%20and%20Interactions%20in%20a%20Python%20Course&body=Title%3A%20Enhancing%20Programming%20Education%20with%20ChatGPT%3A%20A%20Case%20Study%20on%20Student%0A%20%20Perceptions%20and%20Interactions%20in%20a%20Python%20Course%0AAuthor%3A%20Boxaun%20Ma%20and%20Li%20Chen%20and%20Shin%27ichi%20Konomi%0AAbstract%3A%20%20%20The%20integration%20of%20ChatGPT%20as%20a%20supportive%20tool%20in%20education%2C%20notably%20in%0Aprogramming%20courses%2C%20addresses%20the%20unique%20challenges%20of%20programming%20education%0Aby%20providing%20assistance%20with%20debugging%2C%20code%20generation%2C%20and%20explanations.%0ADespite%20existing%20research%20validating%20ChatGPT%27s%20effectiveness%2C%20its%20application%0Ain%20university-level%20programming%20education%20and%20a%20detailed%20understanding%20of%0Astudent%20interactions%20and%20perspectives%20remain%20limited.%20This%20paper%20explores%0AChatGPT%27s%20impact%20on%20learning%20in%20a%20Python%20programming%20course%20tailored%20for%0Afirst-year%20students%20over%20eight%20weeks.%20By%20analyzing%20responses%20from%20surveys%2C%0Aopen-ended%20questions%2C%20and%20student-ChatGPT%20dialog%20data%2C%20we%20aim%20to%20provide%20a%0Acomprehensive%20view%20of%20ChatGPT%27s%20utility%20and%20identify%20both%20its%20advantages%20and%0Alimitations%20as%20perceived%20by%20students.%20Our%20study%20uncovers%20a%20generally%20positive%0Areception%20toward%20ChatGPT%20and%20offers%20insights%20into%20its%20role%20in%20enhancing%20the%0Aprogramming%20education%20experience.%20These%20findings%20contribute%20to%20the%20broader%0Adiscourse%20on%20AI%27s%20potential%20in%20education%2C%20suggesting%20paths%20for%20future%20research%0Aand%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15472v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Programming%20Education%20with%20ChatGPT%3A%20A%20Case%20Study%20on%20Student%0A%20%20Perceptions%20and%20Interactions%20in%20a%20Python%20Course&entry.906535625=Boxaun%20Ma%20and%20Li%20Chen%20and%20Shin%27ichi%20Konomi&entry.1292438233=%20%20The%20integration%20of%20ChatGPT%20as%20a%20supportive%20tool%20in%20education%2C%20notably%20in%0Aprogramming%20courses%2C%20addresses%20the%20unique%20challenges%20of%20programming%20education%0Aby%20providing%20assistance%20with%20debugging%2C%20code%20generation%2C%20and%20explanations.%0ADespite%20existing%20research%20validating%20ChatGPT%27s%20effectiveness%2C%20its%20application%0Ain%20university-level%20programming%20education%20and%20a%20detailed%20understanding%20of%0Astudent%20interactions%20and%20perspectives%20remain%20limited.%20This%20paper%20explores%0AChatGPT%27s%20impact%20on%20learning%20in%20a%20Python%20programming%20course%20tailored%20for%0Afirst-year%20students%20over%20eight%20weeks.%20By%20analyzing%20responses%20from%20surveys%2C%0Aopen-ended%20questions%2C%20and%20student-ChatGPT%20dialog%20data%2C%20we%20aim%20to%20provide%20a%0Acomprehensive%20view%20of%20ChatGPT%27s%20utility%20and%20identify%20both%20its%20advantages%20and%0Alimitations%20as%20perceived%20by%20students.%20Our%20study%20uncovers%20a%20generally%20positive%0Areception%20toward%20ChatGPT%20and%20offers%20insights%20into%20its%20role%20in%20enhancing%20the%0Aprogramming%20education%20experience.%20These%20findings%20contribute%20to%20the%20broader%0Adiscourse%20on%20AI%27s%20potential%20in%20education%2C%20suggesting%20paths%20for%20future%20research%0Aand%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15472v3&entry.124074799=Read"},
{"title": "Continual Learning with Weight Interpolation", "author": "J\u0119drzej Kozal and Jan Wasilewski and Bartosz Krawczyk and Micha\u0142 Wo\u017aniak", "abstract": "  Continual learning poses a fundamental challenge for modern machine learning\nsystems, requiring models to adapt to new tasks while retaining knowledge from\nprevious ones. Addressing this challenge necessitates the development of\nefficient algorithms capable of learning from data streams and accumulating\nknowledge over time. This paper proposes a novel approach to continual learning\nutilizing the weight consolidation method. Our method, a simple yet powerful\ntechnique, enhances robustness against catastrophic forgetting by interpolating\nbetween old and new model weights after each novel task, effectively merging\ntwo models to facilitate exploration of local minima emerging after arrival of\nnew concepts. Moreover, we demonstrate that our approach can complement\nexisting rehearsal-based replay approaches, improving their accuracy and\nfurther mitigating the forgetting phenomenon. Additionally, our method provides\nan intuitive mechanism for controlling the stability-plasticity trade-off.\nExperimental results showcase the significant performance enhancement to\nstate-of-the-art experience replay algorithms the proposed weight consolidation\napproach offers. Our algorithm can be downloaded from\nhttps://github.com/jedrzejkozal/weight-interpolation-cl.\n", "link": "http://arxiv.org/abs/2404.04002v1", "date": "2024-04-05", "relevancy": 1.4825, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5001}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4983}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4778}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20with%20Weight%20Interpolation&body=Title%3A%20Continual%20Learning%20with%20Weight%20Interpolation%0AAuthor%3A%20J%C4%99drzej%20Kozal%20and%20Jan%20Wasilewski%20and%20Bartosz%20Krawczyk%20and%20Micha%C5%82%20Wo%C5%BAniak%0AAbstract%3A%20%20%20Continual%20learning%20poses%20a%20fundamental%20challenge%20for%20modern%20machine%20learning%0Asystems%2C%20requiring%20models%20to%20adapt%20to%20new%20tasks%20while%20retaining%20knowledge%20from%0Aprevious%20ones.%20Addressing%20this%20challenge%20necessitates%20the%20development%20of%0Aefficient%20algorithms%20capable%20of%20learning%20from%20data%20streams%20and%20accumulating%0Aknowledge%20over%20time.%20This%20paper%20proposes%20a%20novel%20approach%20to%20continual%20learning%0Autilizing%20the%20weight%20consolidation%20method.%20Our%20method%2C%20a%20simple%20yet%20powerful%0Atechnique%2C%20enhances%20robustness%20against%20catastrophic%20forgetting%20by%20interpolating%0Abetween%20old%20and%20new%20model%20weights%20after%20each%20novel%20task%2C%20effectively%20merging%0Atwo%20models%20to%20facilitate%20exploration%20of%20local%20minima%20emerging%20after%20arrival%20of%0Anew%20concepts.%20Moreover%2C%20we%20demonstrate%20that%20our%20approach%20can%20complement%0Aexisting%20rehearsal-based%20replay%20approaches%2C%20improving%20their%20accuracy%20and%0Afurther%20mitigating%20the%20forgetting%20phenomenon.%20Additionally%2C%20our%20method%20provides%0Aan%20intuitive%20mechanism%20for%20controlling%20the%20stability-plasticity%20trade-off.%0AExperimental%20results%20showcase%20the%20significant%20performance%20enhancement%20to%0Astate-of-the-art%20experience%20replay%20algorithms%20the%20proposed%20weight%20consolidation%0Aapproach%20offers.%20Our%20algorithm%20can%20be%20downloaded%20from%0Ahttps%3A//github.com/jedrzejkozal/weight-interpolation-cl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04002v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20with%20Weight%20Interpolation&entry.906535625=J%C4%99drzej%20Kozal%20and%20Jan%20Wasilewski%20and%20Bartosz%20Krawczyk%20and%20Micha%C5%82%20Wo%C5%BAniak&entry.1292438233=%20%20Continual%20learning%20poses%20a%20fundamental%20challenge%20for%20modern%20machine%20learning%0Asystems%2C%20requiring%20models%20to%20adapt%20to%20new%20tasks%20while%20retaining%20knowledge%20from%0Aprevious%20ones.%20Addressing%20this%20challenge%20necessitates%20the%20development%20of%0Aefficient%20algorithms%20capable%20of%20learning%20from%20data%20streams%20and%20accumulating%0Aknowledge%20over%20time.%20This%20paper%20proposes%20a%20novel%20approach%20to%20continual%20learning%0Autilizing%20the%20weight%20consolidation%20method.%20Our%20method%2C%20a%20simple%20yet%20powerful%0Atechnique%2C%20enhances%20robustness%20against%20catastrophic%20forgetting%20by%20interpolating%0Abetween%20old%20and%20new%20model%20weights%20after%20each%20novel%20task%2C%20effectively%20merging%0Atwo%20models%20to%20facilitate%20exploration%20of%20local%20minima%20emerging%20after%20arrival%20of%0Anew%20concepts.%20Moreover%2C%20we%20demonstrate%20that%20our%20approach%20can%20complement%0Aexisting%20rehearsal-based%20replay%20approaches%2C%20improving%20their%20accuracy%20and%0Afurther%20mitigating%20the%20forgetting%20phenomenon.%20Additionally%2C%20our%20method%20provides%0Aan%20intuitive%20mechanism%20for%20controlling%20the%20stability-plasticity%20trade-off.%0AExperimental%20results%20showcase%20the%20significant%20performance%20enhancement%20to%0Astate-of-the-art%20experience%20replay%20algorithms%20the%20proposed%20weight%20consolidation%0Aapproach%20offers.%20Our%20algorithm%20can%20be%20downloaded%20from%0Ahttps%3A//github.com/jedrzejkozal/weight-interpolation-cl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04002v1&entry.124074799=Read"},
{"title": "Controlling the Cascade: Kinematic Planning for N-ball Toss Juggling", "author": "Kai Ploeger and Jan Peters", "abstract": "  Dynamic movements are ubiquitous in human motor behavior as they tend to be\nmore efficient and can solve a broader range of skill domains than their\nquasi-static counterparts. For decades, robotic juggling tasks have been among\nthe most frequently studied dynamic manipulation problems since the required\ndynamic dexterity can be scaled to arbitrarily high difficulty. However,\nsuccessful approaches have been limited to basic juggling skills, indicating a\nlack of understanding of the required constraints for dexterous toss juggling.\nWe present a detailed analysis of the toss juggling task, identifying the key\nchallenges and formalizing it as a trajectory optimization problem. Building on\nour state-of-the-art, real-world toss juggling platform, we reach the\ntheoretical limits of toss juggling in simulation, evaluate a resulting\nreal-time controller in environments of varying difficulty and achieve robust\ntoss juggling of up to 17 balls on two anthropomorphic manipulators.\n", "link": "http://arxiv.org/abs/2207.01414v2", "date": "2024-04-05", "relevancy": 1.4942, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5261}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4778}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Controlling%20the%20Cascade%3A%20Kinematic%20Planning%20for%20N-ball%20Toss%20Juggling&body=Title%3A%20Controlling%20the%20Cascade%3A%20Kinematic%20Planning%20for%20N-ball%20Toss%20Juggling%0AAuthor%3A%20Kai%20Ploeger%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Dynamic%20movements%20are%20ubiquitous%20in%20human%20motor%20behavior%20as%20they%20tend%20to%20be%0Amore%20efficient%20and%20can%20solve%20a%20broader%20range%20of%20skill%20domains%20than%20their%0Aquasi-static%20counterparts.%20For%20decades%2C%20robotic%20juggling%20tasks%20have%20been%20among%0Athe%20most%20frequently%20studied%20dynamic%20manipulation%20problems%20since%20the%20required%0Adynamic%20dexterity%20can%20be%20scaled%20to%20arbitrarily%20high%20difficulty.%20However%2C%0Asuccessful%20approaches%20have%20been%20limited%20to%20basic%20juggling%20skills%2C%20indicating%20a%0Alack%20of%20understanding%20of%20the%20required%20constraints%20for%20dexterous%20toss%20juggling.%0AWe%20present%20a%20detailed%20analysis%20of%20the%20toss%20juggling%20task%2C%20identifying%20the%20key%0Achallenges%20and%20formalizing%20it%20as%20a%20trajectory%20optimization%20problem.%20Building%20on%0Aour%20state-of-the-art%2C%20real-world%20toss%20juggling%20platform%2C%20we%20reach%20the%0Atheoretical%20limits%20of%20toss%20juggling%20in%20simulation%2C%20evaluate%20a%20resulting%0Areal-time%20controller%20in%20environments%20of%20varying%20difficulty%20and%20achieve%20robust%0Atoss%20juggling%20of%20up%20to%2017%20balls%20on%20two%20anthropomorphic%20manipulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.01414v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20the%20Cascade%3A%20Kinematic%20Planning%20for%20N-ball%20Toss%20Juggling&entry.906535625=Kai%20Ploeger%20and%20Jan%20Peters&entry.1292438233=%20%20Dynamic%20movements%20are%20ubiquitous%20in%20human%20motor%20behavior%20as%20they%20tend%20to%20be%0Amore%20efficient%20and%20can%20solve%20a%20broader%20range%20of%20skill%20domains%20than%20their%0Aquasi-static%20counterparts.%20For%20decades%2C%20robotic%20juggling%20tasks%20have%20been%20among%0Athe%20most%20frequently%20studied%20dynamic%20manipulation%20problems%20since%20the%20required%0Adynamic%20dexterity%20can%20be%20scaled%20to%20arbitrarily%20high%20difficulty.%20However%2C%0Asuccessful%20approaches%20have%20been%20limited%20to%20basic%20juggling%20skills%2C%20indicating%20a%0Alack%20of%20understanding%20of%20the%20required%20constraints%20for%20dexterous%20toss%20juggling.%0AWe%20present%20a%20detailed%20analysis%20of%20the%20toss%20juggling%20task%2C%20identifying%20the%20key%0Achallenges%20and%20formalizing%20it%20as%20a%20trajectory%20optimization%20problem.%20Building%20on%0Aour%20state-of-the-art%2C%20real-world%20toss%20juggling%20platform%2C%20we%20reach%20the%0Atheoretical%20limits%20of%20toss%20juggling%20in%20simulation%2C%20evaluate%20a%20resulting%0Areal-time%20controller%20in%20environments%20of%20varying%20difficulty%20and%20achieve%20robust%0Atoss%20juggling%20of%20up%20to%2017%20balls%20on%20two%20anthropomorphic%20manipulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.01414v2&entry.124074799=Read"},
{"title": "Robust Preference Optimization with Provable Noise Tolerance for LLMs", "author": "Xize Liang and Chao Chen and Jie Wang and Yue Wu and Zhihang Fu and Zhihao Shi and Feng Wu and Jieping Ye", "abstract": "  The preference alignment aims to enable large language models (LLMs) to\ngenerate responses that conform to human values, which is essential for\ndeveloping general AI systems. Ranking-based methods -- a promising class of\nalignment approaches -- learn human preferences from datasets containing\nresponse pairs by optimizing the log-likelihood margins between preferred and\ndis-preferred responses. However, due to the inherent differences in\nannotators' preferences, ranking labels of comparisons for response pairs are\nunavoidably noisy. This seriously hurts the reliability of existing\nranking-based methods. To address this problem, we propose a provably\nnoise-tolerant preference alignment method, namely RObust Preference\nOptimization (ROPO). To the best of our knowledge, ROPO is the first preference\nalignment method with noise-tolerance guarantees. The key idea of ROPO is to\ndynamically assign conservative gradient weights to response pairs with high\nlabel uncertainty, based on the log-likelihood margins between the responses.\nBy effectively suppressing the gradients of noisy samples, our weighting\nstrategy ensures that the expected risk has the same gradient direction\nindependent of the presence and proportion of noise. Experiments on three\nopen-ended text generation tasks with four base models ranging in size from\n2.8B to 13B demonstrate that ROPO significantly outperforms existing\nranking-based methods.\n", "link": "http://arxiv.org/abs/2404.04102v1", "date": "2024-04-05", "relevancy": 1.4888, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4909}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4908}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Preference%20Optimization%20with%20Provable%20Noise%20Tolerance%20for%20LLMs&body=Title%3A%20Robust%20Preference%20Optimization%20with%20Provable%20Noise%20Tolerance%20for%20LLMs%0AAuthor%3A%20Xize%20Liang%20and%20Chao%20Chen%20and%20Jie%20Wang%20and%20Yue%20Wu%20and%20Zhihang%20Fu%20and%20Zhihao%20Shi%20and%20Feng%20Wu%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20The%20preference%20alignment%20aims%20to%20enable%20large%20language%20models%20%28LLMs%29%20to%0Agenerate%20responses%20that%20conform%20to%20human%20values%2C%20which%20is%20essential%20for%0Adeveloping%20general%20AI%20systems.%20Ranking-based%20methods%20--%20a%20promising%20class%20of%0Aalignment%20approaches%20--%20learn%20human%20preferences%20from%20datasets%20containing%0Aresponse%20pairs%20by%20optimizing%20the%20log-likelihood%20margins%20between%20preferred%20and%0Adis-preferred%20responses.%20However%2C%20due%20to%20the%20inherent%20differences%20in%0Aannotators%27%20preferences%2C%20ranking%20labels%20of%20comparisons%20for%20response%20pairs%20are%0Aunavoidably%20noisy.%20This%20seriously%20hurts%20the%20reliability%20of%20existing%0Aranking-based%20methods.%20To%20address%20this%20problem%2C%20we%20propose%20a%20provably%0Anoise-tolerant%20preference%20alignment%20method%2C%20namely%20RObust%20Preference%0AOptimization%20%28ROPO%29.%20To%20the%20best%20of%20our%20knowledge%2C%20ROPO%20is%20the%20first%20preference%0Aalignment%20method%20with%20noise-tolerance%20guarantees.%20The%20key%20idea%20of%20ROPO%20is%20to%0Adynamically%20assign%20conservative%20gradient%20weights%20to%20response%20pairs%20with%20high%0Alabel%20uncertainty%2C%20based%20on%20the%20log-likelihood%20margins%20between%20the%20responses.%0ABy%20effectively%20suppressing%20the%20gradients%20of%20noisy%20samples%2C%20our%20weighting%0Astrategy%20ensures%20that%20the%20expected%20risk%20has%20the%20same%20gradient%20direction%0Aindependent%20of%20the%20presence%20and%20proportion%20of%20noise.%20Experiments%20on%20three%0Aopen-ended%20text%20generation%20tasks%20with%20four%20base%20models%20ranging%20in%20size%20from%0A2.8B%20to%2013B%20demonstrate%20that%20ROPO%20significantly%20outperforms%20existing%0Aranking-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04102v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Preference%20Optimization%20with%20Provable%20Noise%20Tolerance%20for%20LLMs&entry.906535625=Xize%20Liang%20and%20Chao%20Chen%20and%20Jie%20Wang%20and%20Yue%20Wu%20and%20Zhihang%20Fu%20and%20Zhihao%20Shi%20and%20Feng%20Wu%20and%20Jieping%20Ye&entry.1292438233=%20%20The%20preference%20alignment%20aims%20to%20enable%20large%20language%20models%20%28LLMs%29%20to%0Agenerate%20responses%20that%20conform%20to%20human%20values%2C%20which%20is%20essential%20for%0Adeveloping%20general%20AI%20systems.%20Ranking-based%20methods%20--%20a%20promising%20class%20of%0Aalignment%20approaches%20--%20learn%20human%20preferences%20from%20datasets%20containing%0Aresponse%20pairs%20by%20optimizing%20the%20log-likelihood%20margins%20between%20preferred%20and%0Adis-preferred%20responses.%20However%2C%20due%20to%20the%20inherent%20differences%20in%0Aannotators%27%20preferences%2C%20ranking%20labels%20of%20comparisons%20for%20response%20pairs%20are%0Aunavoidably%20noisy.%20This%20seriously%20hurts%20the%20reliability%20of%20existing%0Aranking-based%20methods.%20To%20address%20this%20problem%2C%20we%20propose%20a%20provably%0Anoise-tolerant%20preference%20alignment%20method%2C%20namely%20RObust%20Preference%0AOptimization%20%28ROPO%29.%20To%20the%20best%20of%20our%20knowledge%2C%20ROPO%20is%20the%20first%20preference%0Aalignment%20method%20with%20noise-tolerance%20guarantees.%20The%20key%20idea%20of%20ROPO%20is%20to%0Adynamically%20assign%20conservative%20gradient%20weights%20to%20response%20pairs%20with%20high%0Alabel%20uncertainty%2C%20based%20on%20the%20log-likelihood%20margins%20between%20the%20responses.%0ABy%20effectively%20suppressing%20the%20gradients%20of%20noisy%20samples%2C%20our%20weighting%0Astrategy%20ensures%20that%20the%20expected%20risk%20has%20the%20same%20gradient%20direction%0Aindependent%20of%20the%20presence%20and%20proportion%20of%20noise.%20Experiments%20on%20three%0Aopen-ended%20text%20generation%20tasks%20with%20four%20base%20models%20ranging%20in%20size%20from%0A2.8B%20to%2013B%20demonstrate%20that%20ROPO%20significantly%20outperforms%20existing%0Aranking-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04102v1&entry.124074799=Read"},
{"title": "CSM-H-R: A Context Modeling Framework in Supporting Reasoning Automation\n  for Interoperable Intelligent Systems and Privacy Protection", "author": "Songhui Yue and Xiaoyan Hong and Randy K. Smith", "abstract": "  The automation of High-Level Context (HLC) reasoning across intelligent\nsystems at scale is imperative because of the unceasing accumulation of\ncontextual data, the trend of the fusion of data from multiple sources (e.g.,\nsensors, intelligent systems), and the intrinsic complexity and dynamism of\ncontext-based decision-making processes. To mitigate the challenges posed by\nthese issues, we propose a novel Hierarchical Ontology-State Modeling (HOSM)\nframework CSM-H-R, which programmatically combines ontologies and states at the\nmodeling phase and runtime phase for attaining the ability to recognize\nmeaningful HLC. It builds on the model of our prior work on the Context State\nMachine (CSM) engine by incorporating the H (Hierarchy) and R (Relationship and\ntRansition) dimensions to take care of the dynamic aspects of context. The\ndesign of the framework supports the sharing and interoperation of context\namong intelligent systems and the components for handling CSMs and the\nmanagement of hierarchy, relationship, and transition. Case studies are\ndeveloped for IntellElevator and IntellRestaurant, two intelligent applications\nin a smart campus setting. The prototype implementation of the framework\nexperiments on translating the HLC reasoning into vector and matrix computing\nand presents the potential of using advanced probabilistic models to reach the\nnext level of automation in integrating intelligent systems; meanwhile, privacy\nprotection support is achieved in the application domain by anonymization\nthrough indexing and reducing information correlation. An implementation of the\nframework is available at https://github.com/songhui01/CSM-H-R.\n", "link": "http://arxiv.org/abs/2308.11066v3", "date": "2024-04-05", "relevancy": 1.4272, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5065}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4464}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CSM-H-R%3A%20A%20Context%20Modeling%20Framework%20in%20Supporting%20Reasoning%20Automation%0A%20%20for%20Interoperable%20Intelligent%20Systems%20and%20Privacy%20Protection&body=Title%3A%20CSM-H-R%3A%20A%20Context%20Modeling%20Framework%20in%20Supporting%20Reasoning%20Automation%0A%20%20for%20Interoperable%20Intelligent%20Systems%20and%20Privacy%20Protection%0AAuthor%3A%20Songhui%20Yue%20and%20Xiaoyan%20Hong%20and%20Randy%20K.%20Smith%0AAbstract%3A%20%20%20The%20automation%20of%20High-Level%20Context%20%28HLC%29%20reasoning%20across%20intelligent%0Asystems%20at%20scale%20is%20imperative%20because%20of%20the%20unceasing%20accumulation%20of%0Acontextual%20data%2C%20the%20trend%20of%20the%20fusion%20of%20data%20from%20multiple%20sources%20%28e.g.%2C%0Asensors%2C%20intelligent%20systems%29%2C%20and%20the%20intrinsic%20complexity%20and%20dynamism%20of%0Acontext-based%20decision-making%20processes.%20To%20mitigate%20the%20challenges%20posed%20by%0Athese%20issues%2C%20we%20propose%20a%20novel%20Hierarchical%20Ontology-State%20Modeling%20%28HOSM%29%0Aframework%20CSM-H-R%2C%20which%20programmatically%20combines%20ontologies%20and%20states%20at%20the%0Amodeling%20phase%20and%20runtime%20phase%20for%20attaining%20the%20ability%20to%20recognize%0Ameaningful%20HLC.%20It%20builds%20on%20the%20model%20of%20our%20prior%20work%20on%20the%20Context%20State%0AMachine%20%28CSM%29%20engine%20by%20incorporating%20the%20H%20%28Hierarchy%29%20and%20R%20%28Relationship%20and%0AtRansition%29%20dimensions%20to%20take%20care%20of%20the%20dynamic%20aspects%20of%20context.%20The%0Adesign%20of%20the%20framework%20supports%20the%20sharing%20and%20interoperation%20of%20context%0Aamong%20intelligent%20systems%20and%20the%20components%20for%20handling%20CSMs%20and%20the%0Amanagement%20of%20hierarchy%2C%20relationship%2C%20and%20transition.%20Case%20studies%20are%0Adeveloped%20for%20IntellElevator%20and%20IntellRestaurant%2C%20two%20intelligent%20applications%0Ain%20a%20smart%20campus%20setting.%20The%20prototype%20implementation%20of%20the%20framework%0Aexperiments%20on%20translating%20the%20HLC%20reasoning%20into%20vector%20and%20matrix%20computing%0Aand%20presents%20the%20potential%20of%20using%20advanced%20probabilistic%20models%20to%20reach%20the%0Anext%20level%20of%20automation%20in%20integrating%20intelligent%20systems%3B%20meanwhile%2C%20privacy%0Aprotection%20support%20is%20achieved%20in%20the%20application%20domain%20by%20anonymization%0Athrough%20indexing%20and%20reducing%20information%20correlation.%20An%20implementation%20of%20the%0Aframework%20is%20available%20at%20https%3A//github.com/songhui01/CSM-H-R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.11066v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSM-H-R%3A%20A%20Context%20Modeling%20Framework%20in%20Supporting%20Reasoning%20Automation%0A%20%20for%20Interoperable%20Intelligent%20Systems%20and%20Privacy%20Protection&entry.906535625=Songhui%20Yue%20and%20Xiaoyan%20Hong%20and%20Randy%20K.%20Smith&entry.1292438233=%20%20The%20automation%20of%20High-Level%20Context%20%28HLC%29%20reasoning%20across%20intelligent%0Asystems%20at%20scale%20is%20imperative%20because%20of%20the%20unceasing%20accumulation%20of%0Acontextual%20data%2C%20the%20trend%20of%20the%20fusion%20of%20data%20from%20multiple%20sources%20%28e.g.%2C%0Asensors%2C%20intelligent%20systems%29%2C%20and%20the%20intrinsic%20complexity%20and%20dynamism%20of%0Acontext-based%20decision-making%20processes.%20To%20mitigate%20the%20challenges%20posed%20by%0Athese%20issues%2C%20we%20propose%20a%20novel%20Hierarchical%20Ontology-State%20Modeling%20%28HOSM%29%0Aframework%20CSM-H-R%2C%20which%20programmatically%20combines%20ontologies%20and%20states%20at%20the%0Amodeling%20phase%20and%20runtime%20phase%20for%20attaining%20the%20ability%20to%20recognize%0Ameaningful%20HLC.%20It%20builds%20on%20the%20model%20of%20our%20prior%20work%20on%20the%20Context%20State%0AMachine%20%28CSM%29%20engine%20by%20incorporating%20the%20H%20%28Hierarchy%29%20and%20R%20%28Relationship%20and%0AtRansition%29%20dimensions%20to%20take%20care%20of%20the%20dynamic%20aspects%20of%20context.%20The%0Adesign%20of%20the%20framework%20supports%20the%20sharing%20and%20interoperation%20of%20context%0Aamong%20intelligent%20systems%20and%20the%20components%20for%20handling%20CSMs%20and%20the%0Amanagement%20of%20hierarchy%2C%20relationship%2C%20and%20transition.%20Case%20studies%20are%0Adeveloped%20for%20IntellElevator%20and%20IntellRestaurant%2C%20two%20intelligent%20applications%0Ain%20a%20smart%20campus%20setting.%20The%20prototype%20implementation%20of%20the%20framework%0Aexperiments%20on%20translating%20the%20HLC%20reasoning%20into%20vector%20and%20matrix%20computing%0Aand%20presents%20the%20potential%20of%20using%20advanced%20probabilistic%20models%20to%20reach%20the%0Anext%20level%20of%20automation%20in%20integrating%20intelligent%20systems%3B%20meanwhile%2C%20privacy%0Aprotection%20support%20is%20achieved%20in%20the%20application%20domain%20by%20anonymization%0Athrough%20indexing%20and%20reducing%20information%20correlation.%20An%20implementation%20of%20the%0Aframework%20is%20available%20at%20https%3A//github.com/songhui01/CSM-H-R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.11066v3&entry.124074799=Read"},
{"title": "Humanoid Robots at work: where are we ?", "author": "Fabrice R. Noreils", "abstract": "  Launched by Elon Musk and its Optimus, we are witnessing a new race in which\nmany companies have already engaged. The objective it to put at work a new\ngeneration of humanoid robots in demanding industrial environments within 2 or\n3 years. Is this objective realistic ? The aim of this document and its main\ncontributions is to provide some hints by covering the following topics: First\nan analysis of 12 companies based on eight criteria that will help us to\ndistinguish companies based on their maturity and approach to the market;\nsecond as these humanoids are very complex systems we will provide an overview\nof the technological challenges to be addressed; third when humanoids are\ndeployed at scale, Operation and Maintenance become critical and the we will\nexplore what is new with these complex machines; Finally Pilots are the last\nstep to test the feasibility of a new system before mass deployment. This is an\nimportant step to test the maturity of a product and the strategy of the\nhumanoid supplier to address a market and two pragmatic approaches will be\ndiscussed.\n", "link": "http://arxiv.org/abs/2404.04249v1", "date": "2024-04-05", "relevancy": 1.4402, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4973}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4657}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Humanoid%20Robots%20at%20work%3A%20where%20are%20we%20%3F&body=Title%3A%20Humanoid%20Robots%20at%20work%3A%20where%20are%20we%20%3F%0AAuthor%3A%20Fabrice%20R.%20Noreils%0AAbstract%3A%20%20%20Launched%20by%20Elon%20Musk%20and%20its%20Optimus%2C%20we%20are%20witnessing%20a%20new%20race%20in%20which%0Amany%20companies%20have%20already%20engaged.%20The%20objective%20it%20to%20put%20at%20work%20a%20new%0Ageneration%20of%20humanoid%20robots%20in%20demanding%20industrial%20environments%20within%202%20or%0A3%20years.%20Is%20this%20objective%20realistic%20%3F%20The%20aim%20of%20this%20document%20and%20its%20main%0Acontributions%20is%20to%20provide%20some%20hints%20by%20covering%20the%20following%20topics%3A%20First%0Aan%20analysis%20of%2012%20companies%20based%20on%20eight%20criteria%20that%20will%20help%20us%20to%0Adistinguish%20companies%20based%20on%20their%20maturity%20and%20approach%20to%20the%20market%3B%0Asecond%20as%20these%20humanoids%20are%20very%20complex%20systems%20we%20will%20provide%20an%20overview%0Aof%20the%20technological%20challenges%20to%20be%20addressed%3B%20third%20when%20humanoids%20are%0Adeployed%20at%20scale%2C%20Operation%20and%20Maintenance%20become%20critical%20and%20the%20we%20will%0Aexplore%20what%20is%20new%20with%20these%20complex%20machines%3B%20Finally%20Pilots%20are%20the%20last%0Astep%20to%20test%20the%20feasibility%20of%20a%20new%20system%20before%20mass%20deployment.%20This%20is%20an%0Aimportant%20step%20to%20test%20the%20maturity%20of%20a%20product%20and%20the%20strategy%20of%20the%0Ahumanoid%20supplier%20to%20address%20a%20market%20and%20two%20pragmatic%20approaches%20will%20be%0Adiscussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04249v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Humanoid%20Robots%20at%20work%3A%20where%20are%20we%20%3F&entry.906535625=Fabrice%20R.%20Noreils&entry.1292438233=%20%20Launched%20by%20Elon%20Musk%20and%20its%20Optimus%2C%20we%20are%20witnessing%20a%20new%20race%20in%20which%0Amany%20companies%20have%20already%20engaged.%20The%20objective%20it%20to%20put%20at%20work%20a%20new%0Ageneration%20of%20humanoid%20robots%20in%20demanding%20industrial%20environments%20within%202%20or%0A3%20years.%20Is%20this%20objective%20realistic%20%3F%20The%20aim%20of%20this%20document%20and%20its%20main%0Acontributions%20is%20to%20provide%20some%20hints%20by%20covering%20the%20following%20topics%3A%20First%0Aan%20analysis%20of%2012%20companies%20based%20on%20eight%20criteria%20that%20will%20help%20us%20to%0Adistinguish%20companies%20based%20on%20their%20maturity%20and%20approach%20to%20the%20market%3B%0Asecond%20as%20these%20humanoids%20are%20very%20complex%20systems%20we%20will%20provide%20an%20overview%0Aof%20the%20technological%20challenges%20to%20be%20addressed%3B%20third%20when%20humanoids%20are%0Adeployed%20at%20scale%2C%20Operation%20and%20Maintenance%20become%20critical%20and%20the%20we%20will%0Aexplore%20what%20is%20new%20with%20these%20complex%20machines%3B%20Finally%20Pilots%20are%20the%20last%0Astep%20to%20test%20the%20feasibility%20of%20a%20new%20system%20before%20mass%20deployment.%20This%20is%20an%0Aimportant%20step%20to%20test%20the%20maturity%20of%20a%20product%20and%20the%20strategy%20of%20the%0Ahumanoid%20supplier%20to%20address%20a%20market%20and%20two%20pragmatic%20approaches%20will%20be%0Adiscussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04249v1&entry.124074799=Read"},
{"title": "Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning\n  Methodology", "author": "Gaith Rjoub and Saidul Islam and Jamal Bentahar and Mohammed Amin Almaiah and Rana Alrawashdeh", "abstract": "  The proliferation of the Internet of Things (IoT) has led to an explosion of\ndata generated by interconnected devices, presenting both opportunities and\nchallenges for intelligent decision-making in complex environments. Traditional\nReinforcement Learning (RL) approaches often struggle to fully harness this\ndata due to their limited ability to process and interpret the intricate\npatterns and dependencies inherent in IoT applications. This paper introduces a\nnovel framework that integrates transformer architectures with Proximal Policy\nOptimization (PPO) to address these challenges. By leveraging the\nself-attention mechanism of transformers, our approach enhances RL agents'\ncapacity for understanding and acting within dynamic IoT environments, leading\nto improved decision-making processes. We demonstrate the effectiveness of our\nmethod across various IoT scenarios, from smart home automation to industrial\ncontrol systems, showing marked improvements in decision-making efficiency and\nadaptability. Our contributions include a detailed exploration of the\ntransformer's role in processing heterogeneous IoT data, a comprehensive\nevaluation of the framework's performance in diverse environments, and a\nbenchmark against traditional RL methods. The results indicate significant\nadvancements in enabling RL agents to navigate the complexities of IoT\necosystems, highlighting the potential of our approach to revolutionize\nintelligent automation and decision-making in the IoT landscape.\n", "link": "http://arxiv.org/abs/2404.04205v1", "date": "2024-04-05", "relevancy": 1.0245, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.541}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4994}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4964}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20IoT%20Intelligence%3A%20A%20Transformer-based%20Reinforcement%20Learning%0A%20%20Methodology&body=Title%3A%20Enhancing%20IoT%20Intelligence%3A%20A%20Transformer-based%20Reinforcement%20Learning%0A%20%20Methodology%0AAuthor%3A%20Gaith%20Rjoub%20and%20Saidul%20Islam%20and%20Jamal%20Bentahar%20and%20Mohammed%20Amin%20Almaiah%20and%20Rana%20Alrawashdeh%0AAbstract%3A%20%20%20The%20proliferation%20of%20the%20Internet%20of%20Things%20%28IoT%29%20has%20led%20to%20an%20explosion%20of%0Adata%20generated%20by%20interconnected%20devices%2C%20presenting%20both%20opportunities%20and%0Achallenges%20for%20intelligent%20decision-making%20in%20complex%20environments.%20Traditional%0AReinforcement%20Learning%20%28RL%29%20approaches%20often%20struggle%20to%20fully%20harness%20this%0Adata%20due%20to%20their%20limited%20ability%20to%20process%20and%20interpret%20the%20intricate%0Apatterns%20and%20dependencies%20inherent%20in%20IoT%20applications.%20This%20paper%20introduces%20a%0Anovel%20framework%20that%20integrates%20transformer%20architectures%20with%20Proximal%20Policy%0AOptimization%20%28PPO%29%20to%20address%20these%20challenges.%20By%20leveraging%20the%0Aself-attention%20mechanism%20of%20transformers%2C%20our%20approach%20enhances%20RL%20agents%27%0Acapacity%20for%20understanding%20and%20acting%20within%20dynamic%20IoT%20environments%2C%20leading%0Ato%20improved%20decision-making%20processes.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20across%20various%20IoT%20scenarios%2C%20from%20smart%20home%20automation%20to%20industrial%0Acontrol%20systems%2C%20showing%20marked%20improvements%20in%20decision-making%20efficiency%20and%0Aadaptability.%20Our%20contributions%20include%20a%20detailed%20exploration%20of%20the%0Atransformer%27s%20role%20in%20processing%20heterogeneous%20IoT%20data%2C%20a%20comprehensive%0Aevaluation%20of%20the%20framework%27s%20performance%20in%20diverse%20environments%2C%20and%20a%0Abenchmark%20against%20traditional%20RL%20methods.%20The%20results%20indicate%20significant%0Aadvancements%20in%20enabling%20RL%20agents%20to%20navigate%20the%20complexities%20of%20IoT%0Aecosystems%2C%20highlighting%20the%20potential%20of%20our%20approach%20to%20revolutionize%0Aintelligent%20automation%20and%20decision-making%20in%20the%20IoT%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04205v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20IoT%20Intelligence%3A%20A%20Transformer-based%20Reinforcement%20Learning%0A%20%20Methodology&entry.906535625=Gaith%20Rjoub%20and%20Saidul%20Islam%20and%20Jamal%20Bentahar%20and%20Mohammed%20Amin%20Almaiah%20and%20Rana%20Alrawashdeh&entry.1292438233=%20%20The%20proliferation%20of%20the%20Internet%20of%20Things%20%28IoT%29%20has%20led%20to%20an%20explosion%20of%0Adata%20generated%20by%20interconnected%20devices%2C%20presenting%20both%20opportunities%20and%0Achallenges%20for%20intelligent%20decision-making%20in%20complex%20environments.%20Traditional%0AReinforcement%20Learning%20%28RL%29%20approaches%20often%20struggle%20to%20fully%20harness%20this%0Adata%20due%20to%20their%20limited%20ability%20to%20process%20and%20interpret%20the%20intricate%0Apatterns%20and%20dependencies%20inherent%20in%20IoT%20applications.%20This%20paper%20introduces%20a%0Anovel%20framework%20that%20integrates%20transformer%20architectures%20with%20Proximal%20Policy%0AOptimization%20%28PPO%29%20to%20address%20these%20challenges.%20By%20leveraging%20the%0Aself-attention%20mechanism%20of%20transformers%2C%20our%20approach%20enhances%20RL%20agents%27%0Acapacity%20for%20understanding%20and%20acting%20within%20dynamic%20IoT%20environments%2C%20leading%0Ato%20improved%20decision-making%20processes.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20across%20various%20IoT%20scenarios%2C%20from%20smart%20home%20automation%20to%20industrial%0Acontrol%20systems%2C%20showing%20marked%20improvements%20in%20decision-making%20efficiency%20and%0Aadaptability.%20Our%20contributions%20include%20a%20detailed%20exploration%20of%20the%0Atransformer%27s%20role%20in%20processing%20heterogeneous%20IoT%20data%2C%20a%20comprehensive%0Aevaluation%20of%20the%20framework%27s%20performance%20in%20diverse%20environments%2C%20and%20a%0Abenchmark%20against%20traditional%20RL%20methods.%20The%20results%20indicate%20significant%0Aadvancements%20in%20enabling%20RL%20agents%20to%20navigate%20the%20complexities%20of%20IoT%0Aecosystems%2C%20highlighting%20the%20potential%20of%20our%20approach%20to%20revolutionize%0Aintelligent%20automation%20and%20decision-making%20in%20the%20IoT%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04205v1&entry.124074799=Read"},
{"title": "Noisy Label Processing for Classification: A Survey", "author": "Mengting Li and Chuang Zhu", "abstract": "  In recent years, deep neural networks (DNNs) have gained remarkable\nachievement in computer vision tasks, and the success of DNNs often depends\ngreatly on the richness of data. However, the acquisition process of data and\nhigh-quality ground truth requires a lot of manpower and money. In the long,\ntedious process of data annotation, annotators are prone to make mistakes,\nresulting in incorrect labels of images, i.e., noisy labels. The emergence of\nnoisy labels is inevitable. Moreover, since research shows that DNNs can easily\nfit noisy labels, the existence of noisy labels will cause significant damage\nto the model training process. Therefore, it is crucial to combat noisy labels\nfor computer vision tasks, especially for classification tasks. In this survey,\nwe first comprehensively review the evolution of different deep learning\napproaches for noisy label combating in the image classification task. In\naddition, we also review different noise patterns that have been proposed to\ndesign robust algorithms. Furthermore, we explore the inner pattern of\nreal-world label noise and propose an algorithm to generate a synthetic label\nnoise pattern guided by real-world data. We test the algorithm on the\nwell-known real-world dataset CIFAR-10N to form a new real-world data-guided\nsynthetic benchmark and evaluate some typical noise-robust methods on the\nbenchmark.\n", "link": "http://arxiv.org/abs/2404.04159v1", "date": "2024-04-05", "relevancy": 1.55, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5241}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5105}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Noisy%20Label%20Processing%20for%20Classification%3A%20A%20Survey&body=Title%3A%20Noisy%20Label%20Processing%20for%20Classification%3A%20A%20Survey%0AAuthor%3A%20Mengting%20Li%20and%20Chuang%20Zhu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20neural%20networks%20%28DNNs%29%20have%20gained%20remarkable%0Aachievement%20in%20computer%20vision%20tasks%2C%20and%20the%20success%20of%20DNNs%20often%20depends%0Agreatly%20on%20the%20richness%20of%20data.%20However%2C%20the%20acquisition%20process%20of%20data%20and%0Ahigh-quality%20ground%20truth%20requires%20a%20lot%20of%20manpower%20and%20money.%20In%20the%20long%2C%0Atedious%20process%20of%20data%20annotation%2C%20annotators%20are%20prone%20to%20make%20mistakes%2C%0Aresulting%20in%20incorrect%20labels%20of%20images%2C%20i.e.%2C%20noisy%20labels.%20The%20emergence%20of%0Anoisy%20labels%20is%20inevitable.%20Moreover%2C%20since%20research%20shows%20that%20DNNs%20can%20easily%0Afit%20noisy%20labels%2C%20the%20existence%20of%20noisy%20labels%20will%20cause%20significant%20damage%0Ato%20the%20model%20training%20process.%20Therefore%2C%20it%20is%20crucial%20to%20combat%20noisy%20labels%0Afor%20computer%20vision%20tasks%2C%20especially%20for%20classification%20tasks.%20In%20this%20survey%2C%0Awe%20first%20comprehensively%20review%20the%20evolution%20of%20different%20deep%20learning%0Aapproaches%20for%20noisy%20label%20combating%20in%20the%20image%20classification%20task.%20In%0Aaddition%2C%20we%20also%20review%20different%20noise%20patterns%20that%20have%20been%20proposed%20to%0Adesign%20robust%20algorithms.%20Furthermore%2C%20we%20explore%20the%20inner%20pattern%20of%0Areal-world%20label%20noise%20and%20propose%20an%20algorithm%20to%20generate%20a%20synthetic%20label%0Anoise%20pattern%20guided%20by%20real-world%20data.%20We%20test%20the%20algorithm%20on%20the%0Awell-known%20real-world%20dataset%20CIFAR-10N%20to%20form%20a%20new%20real-world%20data-guided%0Asynthetic%20benchmark%20and%20evaluate%20some%20typical%20noise-robust%20methods%20on%20the%0Abenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04159v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noisy%20Label%20Processing%20for%20Classification%3A%20A%20Survey&entry.906535625=Mengting%20Li%20and%20Chuang%20Zhu&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20neural%20networks%20%28DNNs%29%20have%20gained%20remarkable%0Aachievement%20in%20computer%20vision%20tasks%2C%20and%20the%20success%20of%20DNNs%20often%20depends%0Agreatly%20on%20the%20richness%20of%20data.%20However%2C%20the%20acquisition%20process%20of%20data%20and%0Ahigh-quality%20ground%20truth%20requires%20a%20lot%20of%20manpower%20and%20money.%20In%20the%20long%2C%0Atedious%20process%20of%20data%20annotation%2C%20annotators%20are%20prone%20to%20make%20mistakes%2C%0Aresulting%20in%20incorrect%20labels%20of%20images%2C%20i.e.%2C%20noisy%20labels.%20The%20emergence%20of%0Anoisy%20labels%20is%20inevitable.%20Moreover%2C%20since%20research%20shows%20that%20DNNs%20can%20easily%0Afit%20noisy%20labels%2C%20the%20existence%20of%20noisy%20labels%20will%20cause%20significant%20damage%0Ato%20the%20model%20training%20process.%20Therefore%2C%20it%20is%20crucial%20to%20combat%20noisy%20labels%0Afor%20computer%20vision%20tasks%2C%20especially%20for%20classification%20tasks.%20In%20this%20survey%2C%0Awe%20first%20comprehensively%20review%20the%20evolution%20of%20different%20deep%20learning%0Aapproaches%20for%20noisy%20label%20combating%20in%20the%20image%20classification%20task.%20In%0Aaddition%2C%20we%20also%20review%20different%20noise%20patterns%20that%20have%20been%20proposed%20to%0Adesign%20robust%20algorithms.%20Furthermore%2C%20we%20explore%20the%20inner%20pattern%20of%0Areal-world%20label%20noise%20and%20propose%20an%20algorithm%20to%20generate%20a%20synthetic%20label%0Anoise%20pattern%20guided%20by%20real-world%20data.%20We%20test%20the%20algorithm%20on%20the%0Awell-known%20real-world%20dataset%20CIFAR-10N%20to%20form%20a%20new%20real-world%20data-guided%0Asynthetic%20benchmark%20and%20evaluate%20some%20typical%20noise-robust%20methods%20on%20the%0Abenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04159v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


