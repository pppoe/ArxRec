<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251007.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss\n  Guided Depth and Bidirectional Warping", "author": "Yu Ma and Guoliang Wei and Yue Cheng", "abstract": "  Novel View Synthesis (NVS) from sparse views presents a formidable challenge\nin 3D reconstruction, where limited multi-view constraints lead to severe\noverfitting, geometric distortion, and fragmented scenes. While 3D Gaussian\nSplatting (3DGS) delivers real-time, high-fidelity rendering, its performance\ndrastically deteriorates under sparse inputs, plagued by floating artifacts and\nstructural failures. To address these challenges, we introduce HBSplat, a\nunified framework that elevates 3DGS by seamlessly integrating robust\nstructural cues, virtual view constraints, and occluded region completion. Our\ncore contributions are threefold: a Hybrid-Loss Depth Estimation module that\nensures multi-view consistency by leveraging dense matching priors and\nintegrating reprojection, point propagation, and smoothness constraints; a\nBidirectional Warping Virtual View Synthesis method that enforces substantially\nstronger constraints by creating high-fidelity virtual views through\nbidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware\nReconstruction component that recovers occluded areas using a depth-difference\nmask and a learning-based inpainting model. Extensive evaluations on LLFF,\nBlender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art,\nachieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time\ninference. Code is available at: https://github.com/eternalland/HBSplat.\n", "link": "http://arxiv.org/abs/2509.24893v2", "date": "2025-10-07", "relevancy": 3.4694, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.722}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6927}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HBSplat%3A%20Robust%20Sparse-View%20Gaussian%20Reconstruction%20with%20Hybrid-Loss%0A%20%20Guided%20Depth%20and%20Bidirectional%20Warping&body=Title%3A%20HBSplat%3A%20Robust%20Sparse-View%20Gaussian%20Reconstruction%20with%20Hybrid-Loss%0A%20%20Guided%20Depth%20and%20Bidirectional%20Warping%0AAuthor%3A%20Yu%20Ma%20and%20Guoliang%20Wei%20and%20Yue%20Cheng%0AAbstract%3A%20%20%20Novel%20View%20Synthesis%20%28NVS%29%20from%20sparse%20views%20presents%20a%20formidable%20challenge%0Ain%203D%20reconstruction%2C%20where%20limited%20multi-view%20constraints%20lead%20to%20severe%0Aoverfitting%2C%20geometric%20distortion%2C%20and%20fragmented%20scenes.%20While%203D%20Gaussian%0ASplatting%20%283DGS%29%20delivers%20real-time%2C%20high-fidelity%20rendering%2C%20its%20performance%0Adrastically%20deteriorates%20under%20sparse%20inputs%2C%20plagued%20by%20floating%20artifacts%20and%0Astructural%20failures.%20To%20address%20these%20challenges%2C%20we%20introduce%20HBSplat%2C%20a%0Aunified%20framework%20that%20elevates%203DGS%20by%20seamlessly%20integrating%20robust%0Astructural%20cues%2C%20virtual%20view%20constraints%2C%20and%20occluded%20region%20completion.%20Our%0Acore%20contributions%20are%20threefold%3A%20a%20Hybrid-Loss%20Depth%20Estimation%20module%20that%0Aensures%20multi-view%20consistency%20by%20leveraging%20dense%20matching%20priors%20and%0Aintegrating%20reprojection%2C%20point%20propagation%2C%20and%20smoothness%20constraints%3B%20a%0ABidirectional%20Warping%20Virtual%20View%20Synthesis%20method%20that%20enforces%20substantially%0Astronger%20constraints%20by%20creating%20high-fidelity%20virtual%20views%20through%0Abidirectional%20depth-image%20warping%20and%20multi-view%20fusion%3B%20and%20an%20Occlusion-Aware%0AReconstruction%20component%20that%20recovers%20occluded%20areas%20using%20a%20depth-difference%0Amask%20and%20a%20learning-based%20inpainting%20model.%20Extensive%20evaluations%20on%20LLFF%2C%0ABlender%2C%20and%20DTU%20benchmarks%20validate%20that%20HBSplat%20sets%20a%20new%20state-of-the-art%2C%0Aachieving%20up%20to%2021.13%20dB%20PSNR%20and%200.189%20LPIPS%2C%20while%20maintaining%20real-time%0Ainference.%20Code%20is%20available%20at%3A%20https%3A//github.com/eternalland/HBSplat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24893v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHBSplat%253A%2520Robust%2520Sparse-View%2520Gaussian%2520Reconstruction%2520with%2520Hybrid-Loss%250A%2520%2520Guided%2520Depth%2520and%2520Bidirectional%2520Warping%26entry.906535625%3DYu%2520Ma%2520and%2520Guoliang%2520Wei%2520and%2520Yue%2520Cheng%26entry.1292438233%3D%2520%2520Novel%2520View%2520Synthesis%2520%2528NVS%2529%2520from%2520sparse%2520views%2520presents%2520a%2520formidable%2520challenge%250Ain%25203D%2520reconstruction%252C%2520where%2520limited%2520multi-view%2520constraints%2520lead%2520to%2520severe%250Aoverfitting%252C%2520geometric%2520distortion%252C%2520and%2520fragmented%2520scenes.%2520While%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520delivers%2520real-time%252C%2520high-fidelity%2520rendering%252C%2520its%2520performance%250Adrastically%2520deteriorates%2520under%2520sparse%2520inputs%252C%2520plagued%2520by%2520floating%2520artifacts%2520and%250Astructural%2520failures.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520HBSplat%252C%2520a%250Aunified%2520framework%2520that%2520elevates%25203DGS%2520by%2520seamlessly%2520integrating%2520robust%250Astructural%2520cues%252C%2520virtual%2520view%2520constraints%252C%2520and%2520occluded%2520region%2520completion.%2520Our%250Acore%2520contributions%2520are%2520threefold%253A%2520a%2520Hybrid-Loss%2520Depth%2520Estimation%2520module%2520that%250Aensures%2520multi-view%2520consistency%2520by%2520leveraging%2520dense%2520matching%2520priors%2520and%250Aintegrating%2520reprojection%252C%2520point%2520propagation%252C%2520and%2520smoothness%2520constraints%253B%2520a%250ABidirectional%2520Warping%2520Virtual%2520View%2520Synthesis%2520method%2520that%2520enforces%2520substantially%250Astronger%2520constraints%2520by%2520creating%2520high-fidelity%2520virtual%2520views%2520through%250Abidirectional%2520depth-image%2520warping%2520and%2520multi-view%2520fusion%253B%2520and%2520an%2520Occlusion-Aware%250AReconstruction%2520component%2520that%2520recovers%2520occluded%2520areas%2520using%2520a%2520depth-difference%250Amask%2520and%2520a%2520learning-based%2520inpainting%2520model.%2520Extensive%2520evaluations%2520on%2520LLFF%252C%250ABlender%252C%2520and%2520DTU%2520benchmarks%2520validate%2520that%2520HBSplat%2520sets%2520a%2520new%2520state-of-the-art%252C%250Aachieving%2520up%2520to%252021.13%2520dB%2520PSNR%2520and%25200.189%2520LPIPS%252C%2520while%2520maintaining%2520real-time%250Ainference.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/eternalland/HBSplat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24893v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HBSplat%3A%20Robust%20Sparse-View%20Gaussian%20Reconstruction%20with%20Hybrid-Loss%0A%20%20Guided%20Depth%20and%20Bidirectional%20Warping&entry.906535625=Yu%20Ma%20and%20Guoliang%20Wei%20and%20Yue%20Cheng&entry.1292438233=%20%20Novel%20View%20Synthesis%20%28NVS%29%20from%20sparse%20views%20presents%20a%20formidable%20challenge%0Ain%203D%20reconstruction%2C%20where%20limited%20multi-view%20constraints%20lead%20to%20severe%0Aoverfitting%2C%20geometric%20distortion%2C%20and%20fragmented%20scenes.%20While%203D%20Gaussian%0ASplatting%20%283DGS%29%20delivers%20real-time%2C%20high-fidelity%20rendering%2C%20its%20performance%0Adrastically%20deteriorates%20under%20sparse%20inputs%2C%20plagued%20by%20floating%20artifacts%20and%0Astructural%20failures.%20To%20address%20these%20challenges%2C%20we%20introduce%20HBSplat%2C%20a%0Aunified%20framework%20that%20elevates%203DGS%20by%20seamlessly%20integrating%20robust%0Astructural%20cues%2C%20virtual%20view%20constraints%2C%20and%20occluded%20region%20completion.%20Our%0Acore%20contributions%20are%20threefold%3A%20a%20Hybrid-Loss%20Depth%20Estimation%20module%20that%0Aensures%20multi-view%20consistency%20by%20leveraging%20dense%20matching%20priors%20and%0Aintegrating%20reprojection%2C%20point%20propagation%2C%20and%20smoothness%20constraints%3B%20a%0ABidirectional%20Warping%20Virtual%20View%20Synthesis%20method%20that%20enforces%20substantially%0Astronger%20constraints%20by%20creating%20high-fidelity%20virtual%20views%20through%0Abidirectional%20depth-image%20warping%20and%20multi-view%20fusion%3B%20and%20an%20Occlusion-Aware%0AReconstruction%20component%20that%20recovers%20occluded%20areas%20using%20a%20depth-difference%0Amask%20and%20a%20learning-based%20inpainting%20model.%20Extensive%20evaluations%20on%20LLFF%2C%0ABlender%2C%20and%20DTU%20benchmarks%20validate%20that%20HBSplat%20sets%20a%20new%20state-of-the-art%2C%0Aachieving%20up%20to%2021.13%20dB%20PSNR%20and%200.189%20LPIPS%2C%20while%20maintaining%20real-time%0Ainference.%20Code%20is%20available%20at%3A%20https%3A//github.com/eternalland/HBSplat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24893v2&entry.124074799=Read"},
{"title": "Human3R: Everyone Everywhere All at Once", "author": "Yue Chen and Xingyu Chen and Yuxuan Xue and Anpei Chen and Yuliang Xiu and Gerard Pons-Moll", "abstract": "  We present Human3R, a unified, feed-forward framework for online 4D\nhuman-scene reconstruction, in the world frame, from casually captured\nmonocular videos. Unlike previous approaches that rely on multi-stage\npipelines, iterative contact-aware refinement between humans and scenes, and\nheavy dependencies, e.g., human detection, depth estimation, and SLAM\npre-processing, Human3R jointly recovers global multi-person SMPL-X bodies\n(\"everyone\"), dense 3D scene (\"everywhere\"), and camera trajectories in a\nsingle forward pass (\"all-at-once\"). Our method builds upon the 4D online\nreconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,\nto strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct\nreadout of multiple SMPL-X bodies. Human3R is a unified model that eliminates\nheavy dependencies and iterative refinement. After being trained on the\nrelatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it\nachieves superior performance with remarkable efficiency: it reconstructs\nmultiple humans in a one-shot manner, along with 3D scenes, in one stage, at\nreal-time speed (15 FPS) with a low memory footprint (8 GB). Extensive\nexperiments demonstrate that Human3R delivers state-of-the-art or competitive\nperformance across tasks, including global human motion estimation, local human\nmesh recovery, video depth estimation, and camera pose estimation, with a\nsingle unified model. We hope that Human3R will serve as a simple yet strong\nbaseline, be easily extended for downstream applications.Code available in\nhttps://fanegg.github.io/Human3R\n", "link": "http://arxiv.org/abs/2510.06219v1", "date": "2025-10-07", "relevancy": 3.3897, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7039}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6716}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human3R%3A%20Everyone%20Everywhere%20All%20at%20Once&body=Title%3A%20Human3R%3A%20Everyone%20Everywhere%20All%20at%20Once%0AAuthor%3A%20Yue%20Chen%20and%20Xingyu%20Chen%20and%20Yuxuan%20Xue%20and%20Anpei%20Chen%20and%20Yuliang%20Xiu%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20We%20present%20Human3R%2C%20a%20unified%2C%20feed-forward%20framework%20for%20online%204D%0Ahuman-scene%20reconstruction%2C%20in%20the%20world%20frame%2C%20from%20casually%20captured%0Amonocular%20videos.%20Unlike%20previous%20approaches%20that%20rely%20on%20multi-stage%0Apipelines%2C%20iterative%20contact-aware%20refinement%20between%20humans%20and%20scenes%2C%20and%0Aheavy%20dependencies%2C%20e.g.%2C%20human%20detection%2C%20depth%20estimation%2C%20and%20SLAM%0Apre-processing%2C%20Human3R%20jointly%20recovers%20global%20multi-person%20SMPL-X%20bodies%0A%28%22everyone%22%29%2C%20dense%203D%20scene%20%28%22everywhere%22%29%2C%20and%20camera%20trajectories%20in%20a%0Asingle%20forward%20pass%20%28%22all-at-once%22%29.%20Our%20method%20builds%20upon%20the%204D%20online%0Areconstruction%20model%20CUT3R%2C%20and%20uses%20parameter-efficient%20visual%20prompt%20tuning%2C%0Ato%20strive%20to%20preserve%20CUT3R%27s%20rich%20spatiotemporal%20priors%2C%20while%20enabling%20direct%0Areadout%20of%20multiple%20SMPL-X%20bodies.%20Human3R%20is%20a%20unified%20model%20that%20eliminates%0Aheavy%20dependencies%20and%20iterative%20refinement.%20After%20being%20trained%20on%20the%0Arelatively%20small-scale%20synthetic%20dataset%20BEDLAM%20for%20just%20one%20day%20on%20one%20GPU%2C%20it%0Aachieves%20superior%20performance%20with%20remarkable%20efficiency%3A%20it%20reconstructs%0Amultiple%20humans%20in%20a%20one-shot%20manner%2C%20along%20with%203D%20scenes%2C%20in%20one%20stage%2C%20at%0Areal-time%20speed%20%2815%20FPS%29%20with%20a%20low%20memory%20footprint%20%288%20GB%29.%20Extensive%0Aexperiments%20demonstrate%20that%20Human3R%20delivers%20state-of-the-art%20or%20competitive%0Aperformance%20across%20tasks%2C%20including%20global%20human%20motion%20estimation%2C%20local%20human%0Amesh%20recovery%2C%20video%20depth%20estimation%2C%20and%20camera%20pose%20estimation%2C%20with%20a%0Asingle%20unified%20model.%20We%20hope%20that%20Human3R%20will%20serve%20as%20a%20simple%20yet%20strong%0Abaseline%2C%20be%20easily%20extended%20for%20downstream%20applications.Code%20available%20in%0Ahttps%3A//fanegg.github.io/Human3R%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman3R%253A%2520Everyone%2520Everywhere%2520All%2520at%2520Once%26entry.906535625%3DYue%2520Chen%2520and%2520Xingyu%2520Chen%2520and%2520Yuxuan%2520Xue%2520and%2520Anpei%2520Chen%2520and%2520Yuliang%2520Xiu%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520We%2520present%2520Human3R%252C%2520a%2520unified%252C%2520feed-forward%2520framework%2520for%2520online%25204D%250Ahuman-scene%2520reconstruction%252C%2520in%2520the%2520world%2520frame%252C%2520from%2520casually%2520captured%250Amonocular%2520videos.%2520Unlike%2520previous%2520approaches%2520that%2520rely%2520on%2520multi-stage%250Apipelines%252C%2520iterative%2520contact-aware%2520refinement%2520between%2520humans%2520and%2520scenes%252C%2520and%250Aheavy%2520dependencies%252C%2520e.g.%252C%2520human%2520detection%252C%2520depth%2520estimation%252C%2520and%2520SLAM%250Apre-processing%252C%2520Human3R%2520jointly%2520recovers%2520global%2520multi-person%2520SMPL-X%2520bodies%250A%2528%2522everyone%2522%2529%252C%2520dense%25203D%2520scene%2520%2528%2522everywhere%2522%2529%252C%2520and%2520camera%2520trajectories%2520in%2520a%250Asingle%2520forward%2520pass%2520%2528%2522all-at-once%2522%2529.%2520Our%2520method%2520builds%2520upon%2520the%25204D%2520online%250Areconstruction%2520model%2520CUT3R%252C%2520and%2520uses%2520parameter-efficient%2520visual%2520prompt%2520tuning%252C%250Ato%2520strive%2520to%2520preserve%2520CUT3R%2527s%2520rich%2520spatiotemporal%2520priors%252C%2520while%2520enabling%2520direct%250Areadout%2520of%2520multiple%2520SMPL-X%2520bodies.%2520Human3R%2520is%2520a%2520unified%2520model%2520that%2520eliminates%250Aheavy%2520dependencies%2520and%2520iterative%2520refinement.%2520After%2520being%2520trained%2520on%2520the%250Arelatively%2520small-scale%2520synthetic%2520dataset%2520BEDLAM%2520for%2520just%2520one%2520day%2520on%2520one%2520GPU%252C%2520it%250Aachieves%2520superior%2520performance%2520with%2520remarkable%2520efficiency%253A%2520it%2520reconstructs%250Amultiple%2520humans%2520in%2520a%2520one-shot%2520manner%252C%2520along%2520with%25203D%2520scenes%252C%2520in%2520one%2520stage%252C%2520at%250Areal-time%2520speed%2520%252815%2520FPS%2529%2520with%2520a%2520low%2520memory%2520footprint%2520%25288%2520GB%2529.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520Human3R%2520delivers%2520state-of-the-art%2520or%2520competitive%250Aperformance%2520across%2520tasks%252C%2520including%2520global%2520human%2520motion%2520estimation%252C%2520local%2520human%250Amesh%2520recovery%252C%2520video%2520depth%2520estimation%252C%2520and%2520camera%2520pose%2520estimation%252C%2520with%2520a%250Asingle%2520unified%2520model.%2520We%2520hope%2520that%2520Human3R%2520will%2520serve%2520as%2520a%2520simple%2520yet%2520strong%250Abaseline%252C%2520be%2520easily%2520extended%2520for%2520downstream%2520applications.Code%2520available%2520in%250Ahttps%253A//fanegg.github.io/Human3R%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human3R%3A%20Everyone%20Everywhere%20All%20at%20Once&entry.906535625=Yue%20Chen%20and%20Xingyu%20Chen%20and%20Yuxuan%20Xue%20and%20Anpei%20Chen%20and%20Yuliang%20Xiu%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20We%20present%20Human3R%2C%20a%20unified%2C%20feed-forward%20framework%20for%20online%204D%0Ahuman-scene%20reconstruction%2C%20in%20the%20world%20frame%2C%20from%20casually%20captured%0Amonocular%20videos.%20Unlike%20previous%20approaches%20that%20rely%20on%20multi-stage%0Apipelines%2C%20iterative%20contact-aware%20refinement%20between%20humans%20and%20scenes%2C%20and%0Aheavy%20dependencies%2C%20e.g.%2C%20human%20detection%2C%20depth%20estimation%2C%20and%20SLAM%0Apre-processing%2C%20Human3R%20jointly%20recovers%20global%20multi-person%20SMPL-X%20bodies%0A%28%22everyone%22%29%2C%20dense%203D%20scene%20%28%22everywhere%22%29%2C%20and%20camera%20trajectories%20in%20a%0Asingle%20forward%20pass%20%28%22all-at-once%22%29.%20Our%20method%20builds%20upon%20the%204D%20online%0Areconstruction%20model%20CUT3R%2C%20and%20uses%20parameter-efficient%20visual%20prompt%20tuning%2C%0Ato%20strive%20to%20preserve%20CUT3R%27s%20rich%20spatiotemporal%20priors%2C%20while%20enabling%20direct%0Areadout%20of%20multiple%20SMPL-X%20bodies.%20Human3R%20is%20a%20unified%20model%20that%20eliminates%0Aheavy%20dependencies%20and%20iterative%20refinement.%20After%20being%20trained%20on%20the%0Arelatively%20small-scale%20synthetic%20dataset%20BEDLAM%20for%20just%20one%20day%20on%20one%20GPU%2C%20it%0Aachieves%20superior%20performance%20with%20remarkable%20efficiency%3A%20it%20reconstructs%0Amultiple%20humans%20in%20a%20one-shot%20manner%2C%20along%20with%203D%20scenes%2C%20in%20one%20stage%2C%20at%0Areal-time%20speed%20%2815%20FPS%29%20with%20a%20low%20memory%20footprint%20%288%20GB%29.%20Extensive%0Aexperiments%20demonstrate%20that%20Human3R%20delivers%20state-of-the-art%20or%20competitive%0Aperformance%20across%20tasks%2C%20including%20global%20human%20motion%20estimation%2C%20local%20human%0Amesh%20recovery%2C%20video%20depth%20estimation%2C%20and%20camera%20pose%20estimation%2C%20with%20a%0Asingle%20unified%20model.%20We%20hope%20that%20Human3R%20will%20serve%20as%20a%20simple%20yet%20strong%0Abaseline%2C%20be%20easily%20extended%20for%20downstream%20applications.Code%20available%20in%0Ahttps%3A//fanegg.github.io/Human3R%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06219v1&entry.124074799=Read"},
{"title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via\n  Semantic-Aware Permutation", "author": "Shuo Yang and Haocheng Xi and Yilong Zhao and Muyang Li and Jintao Zhang and Han Cai and Yujun Lin and Xiuyu Li and Chenfeng Xu and Kelly Peng and Jianfei Chen and Song Han and Kurt Keutzer and Ion Stoica", "abstract": "  Diffusion Transformers (DiTs) are essential for video generation but suffer\nfrom significant latency due to the quadratic complexity of attention. By\ncomputing only critical tokens, sparse attention reduces computational costs\nand offers a promising acceleration approach. However, we identify that\nexisting methods fail to approach optimal generation quality under the same\ncomputation budget for two reasons: (1) Inaccurate critical token\nidentification: current methods cluster tokens based on position rather than\nsemantics, leading to imprecise aggregated representations. (2) Excessive\ncomputation waste: critical tokens are scattered among non-critical ones,\nleading to wasted computation on GPUs, which are optimized for processing\ncontiguous tokens. In this paper, we propose SVG2, a training-free framework\nthat maximizes identification accuracy and minimizes computation waste,\nachieving a Pareto frontier trade-off between generation quality and\nefficiency. The core of SVG2 is semantic-aware permutation, which clusters and\nreorders tokens based on semantic similarity using k-means. This approach\nensures both a precise cluster representation, improving identification\naccuracy, and a densified layout of critical tokens, enabling efficient\ncomputation without padding. Additionally, SVG2 integrates top-p dynamic budget\ncontrol and customized kernel implementations, achieving up to 2.30x and 1.89x\nspeedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan\n2.1, respectively. Our code is open-sourced at\n\\href{https://github.com/svg-project/Sparse-VideoGen}{https://github.com/svg-project/Sparse-VideoGen}.\n", "link": "http://arxiv.org/abs/2505.18875v3", "date": "2025-10-07", "relevancy": 3.1846, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6603}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6405}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.61}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20VideoGen2%3A%20Accelerate%20Video%20Generation%20with%20Sparse%20Attention%20via%0A%20%20Semantic-Aware%20Permutation&body=Title%3A%20Sparse%20VideoGen2%3A%20Accelerate%20Video%20Generation%20with%20Sparse%20Attention%20via%0A%20%20Semantic-Aware%20Permutation%0AAuthor%3A%20Shuo%20Yang%20and%20Haocheng%20Xi%20and%20Yilong%20Zhao%20and%20Muyang%20Li%20and%20Jintao%20Zhang%20and%20Han%20Cai%20and%20Yujun%20Lin%20and%20Xiuyu%20Li%20and%20Chenfeng%20Xu%20and%20Kelly%20Peng%20and%20Jianfei%20Chen%20and%20Song%20Han%20and%20Kurt%20Keutzer%20and%20Ion%20Stoica%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiTs%29%20are%20essential%20for%20video%20generation%20but%20suffer%0Afrom%20significant%20latency%20due%20to%20the%20quadratic%20complexity%20of%20attention.%20By%0Acomputing%20only%20critical%20tokens%2C%20sparse%20attention%20reduces%20computational%20costs%0Aand%20offers%20a%20promising%20acceleration%20approach.%20However%2C%20we%20identify%20that%0Aexisting%20methods%20fail%20to%20approach%20optimal%20generation%20quality%20under%20the%20same%0Acomputation%20budget%20for%20two%20reasons%3A%20%281%29%20Inaccurate%20critical%20token%0Aidentification%3A%20current%20methods%20cluster%20tokens%20based%20on%20position%20rather%20than%0Asemantics%2C%20leading%20to%20imprecise%20aggregated%20representations.%20%282%29%20Excessive%0Acomputation%20waste%3A%20critical%20tokens%20are%20scattered%20among%20non-critical%20ones%2C%0Aleading%20to%20wasted%20computation%20on%20GPUs%2C%20which%20are%20optimized%20for%20processing%0Acontiguous%20tokens.%20In%20this%20paper%2C%20we%20propose%20SVG2%2C%20a%20training-free%20framework%0Athat%20maximizes%20identification%20accuracy%20and%20minimizes%20computation%20waste%2C%0Aachieving%20a%20Pareto%20frontier%20trade-off%20between%20generation%20quality%20and%0Aefficiency.%20The%20core%20of%20SVG2%20is%20semantic-aware%20permutation%2C%20which%20clusters%20and%0Areorders%20tokens%20based%20on%20semantic%20similarity%20using%20k-means.%20This%20approach%0Aensures%20both%20a%20precise%20cluster%20representation%2C%20improving%20identification%0Aaccuracy%2C%20and%20a%20densified%20layout%20of%20critical%20tokens%2C%20enabling%20efficient%0Acomputation%20without%20padding.%20Additionally%2C%20SVG2%20integrates%20top-p%20dynamic%20budget%0Acontrol%20and%20customized%20kernel%20implementations%2C%20achieving%20up%20to%202.30x%20and%201.89x%0Aspeedup%20while%20maintaining%20a%20PSNR%20of%20up%20to%2030%20and%2026%20on%20HunyuanVideo%20and%20Wan%0A2.1%2C%20respectively.%20Our%20code%20is%20open-sourced%20at%0A%5Chref%7Bhttps%3A//github.com/svg-project/Sparse-VideoGen%7D%7Bhttps%3A//github.com/svg-project/Sparse-VideoGen%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18875v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520VideoGen2%253A%2520Accelerate%2520Video%2520Generation%2520with%2520Sparse%2520Attention%2520via%250A%2520%2520Semantic-Aware%2520Permutation%26entry.906535625%3DShuo%2520Yang%2520and%2520Haocheng%2520Xi%2520and%2520Yilong%2520Zhao%2520and%2520Muyang%2520Li%2520and%2520Jintao%2520Zhang%2520and%2520Han%2520Cai%2520and%2520Yujun%2520Lin%2520and%2520Xiuyu%2520Li%2520and%2520Chenfeng%2520Xu%2520and%2520Kelly%2520Peng%2520and%2520Jianfei%2520Chen%2520and%2520Song%2520Han%2520and%2520Kurt%2520Keutzer%2520and%2520Ion%2520Stoica%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520are%2520essential%2520for%2520video%2520generation%2520but%2520suffer%250Afrom%2520significant%2520latency%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520attention.%2520By%250Acomputing%2520only%2520critical%2520tokens%252C%2520sparse%2520attention%2520reduces%2520computational%2520costs%250Aand%2520offers%2520a%2520promising%2520acceleration%2520approach.%2520However%252C%2520we%2520identify%2520that%250Aexisting%2520methods%2520fail%2520to%2520approach%2520optimal%2520generation%2520quality%2520under%2520the%2520same%250Acomputation%2520budget%2520for%2520two%2520reasons%253A%2520%25281%2529%2520Inaccurate%2520critical%2520token%250Aidentification%253A%2520current%2520methods%2520cluster%2520tokens%2520based%2520on%2520position%2520rather%2520than%250Asemantics%252C%2520leading%2520to%2520imprecise%2520aggregated%2520representations.%2520%25282%2529%2520Excessive%250Acomputation%2520waste%253A%2520critical%2520tokens%2520are%2520scattered%2520among%2520non-critical%2520ones%252C%250Aleading%2520to%2520wasted%2520computation%2520on%2520GPUs%252C%2520which%2520are%2520optimized%2520for%2520processing%250Acontiguous%2520tokens.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SVG2%252C%2520a%2520training-free%2520framework%250Athat%2520maximizes%2520identification%2520accuracy%2520and%2520minimizes%2520computation%2520waste%252C%250Aachieving%2520a%2520Pareto%2520frontier%2520trade-off%2520between%2520generation%2520quality%2520and%250Aefficiency.%2520The%2520core%2520of%2520SVG2%2520is%2520semantic-aware%2520permutation%252C%2520which%2520clusters%2520and%250Areorders%2520tokens%2520based%2520on%2520semantic%2520similarity%2520using%2520k-means.%2520This%2520approach%250Aensures%2520both%2520a%2520precise%2520cluster%2520representation%252C%2520improving%2520identification%250Aaccuracy%252C%2520and%2520a%2520densified%2520layout%2520of%2520critical%2520tokens%252C%2520enabling%2520efficient%250Acomputation%2520without%2520padding.%2520Additionally%252C%2520SVG2%2520integrates%2520top-p%2520dynamic%2520budget%250Acontrol%2520and%2520customized%2520kernel%2520implementations%252C%2520achieving%2520up%2520to%25202.30x%2520and%25201.89x%250Aspeedup%2520while%2520maintaining%2520a%2520PSNR%2520of%2520up%2520to%252030%2520and%252026%2520on%2520HunyuanVideo%2520and%2520Wan%250A2.1%252C%2520respectively.%2520Our%2520code%2520is%2520open-sourced%2520at%250A%255Chref%257Bhttps%253A//github.com/svg-project/Sparse-VideoGen%257D%257Bhttps%253A//github.com/svg-project/Sparse-VideoGen%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18875v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20VideoGen2%3A%20Accelerate%20Video%20Generation%20with%20Sparse%20Attention%20via%0A%20%20Semantic-Aware%20Permutation&entry.906535625=Shuo%20Yang%20and%20Haocheng%20Xi%20and%20Yilong%20Zhao%20and%20Muyang%20Li%20and%20Jintao%20Zhang%20and%20Han%20Cai%20and%20Yujun%20Lin%20and%20Xiuyu%20Li%20and%20Chenfeng%20Xu%20and%20Kelly%20Peng%20and%20Jianfei%20Chen%20and%20Song%20Han%20and%20Kurt%20Keutzer%20and%20Ion%20Stoica&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiTs%29%20are%20essential%20for%20video%20generation%20but%20suffer%0Afrom%20significant%20latency%20due%20to%20the%20quadratic%20complexity%20of%20attention.%20By%0Acomputing%20only%20critical%20tokens%2C%20sparse%20attention%20reduces%20computational%20costs%0Aand%20offers%20a%20promising%20acceleration%20approach.%20However%2C%20we%20identify%20that%0Aexisting%20methods%20fail%20to%20approach%20optimal%20generation%20quality%20under%20the%20same%0Acomputation%20budget%20for%20two%20reasons%3A%20%281%29%20Inaccurate%20critical%20token%0Aidentification%3A%20current%20methods%20cluster%20tokens%20based%20on%20position%20rather%20than%0Asemantics%2C%20leading%20to%20imprecise%20aggregated%20representations.%20%282%29%20Excessive%0Acomputation%20waste%3A%20critical%20tokens%20are%20scattered%20among%20non-critical%20ones%2C%0Aleading%20to%20wasted%20computation%20on%20GPUs%2C%20which%20are%20optimized%20for%20processing%0Acontiguous%20tokens.%20In%20this%20paper%2C%20we%20propose%20SVG2%2C%20a%20training-free%20framework%0Athat%20maximizes%20identification%20accuracy%20and%20minimizes%20computation%20waste%2C%0Aachieving%20a%20Pareto%20frontier%20trade-off%20between%20generation%20quality%20and%0Aefficiency.%20The%20core%20of%20SVG2%20is%20semantic-aware%20permutation%2C%20which%20clusters%20and%0Areorders%20tokens%20based%20on%20semantic%20similarity%20using%20k-means.%20This%20approach%0Aensures%20both%20a%20precise%20cluster%20representation%2C%20improving%20identification%0Aaccuracy%2C%20and%20a%20densified%20layout%20of%20critical%20tokens%2C%20enabling%20efficient%0Acomputation%20without%20padding.%20Additionally%2C%20SVG2%20integrates%20top-p%20dynamic%20budget%0Acontrol%20and%20customized%20kernel%20implementations%2C%20achieving%20up%20to%202.30x%20and%201.89x%0Aspeedup%20while%20maintaining%20a%20PSNR%20of%20up%20to%2030%20and%2026%20on%20HunyuanVideo%20and%20Wan%0A2.1%2C%20respectively.%20Our%20code%20is%20open-sourced%20at%0A%5Chref%7Bhttps%3A//github.com/svg-project/Sparse-VideoGen%7D%7Bhttps%3A//github.com/svg-project/Sparse-VideoGen%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18875v3&entry.124074799=Read"},
{"title": "ShapeGen4D: Towards High Quality 4D Shape Generation from Videos", "author": "Jiraphon Yenphraphai and Ashkan Mirzaei and Jianqi Chen and Jiaxu Zou and Sergey Tulyakov and Raymond A. Yeh and Peter Wonka and Chaoyang Wang", "abstract": "  Video-conditioned 4D shape generation aims to recover time-varying 3D\ngeometry and view-consistent appearance directly from an input video. In this\nwork, we introduce a native video-to-4D shape generation framework that\nsynthesizes a single dynamic 3D representation end-to-end from the video. Our\nframework introduces three key components based on large-scale pre-trained 3D\nmodels: (i) a temporal attention that conditions generation on all frames while\nproducing a time-indexed dynamic representation; (ii) a time-aware point\nsampling and 4D latent anchoring that promote temporally consistent geometry\nand texture; and (iii) noise sharing across frames to enhance temporal\nstability. Our method accurately captures non-rigid motion, volume changes, and\neven topological transitions without per-frame optimization. Across diverse\nin-the-wild videos, our method improves robustness and perceptual fidelity and\nreduces failure modes compared with the baselines.\n", "link": "http://arxiv.org/abs/2510.06208v1", "date": "2025-10-07", "relevancy": 3.1123, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6502}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6094}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapeGen4D%3A%20Towards%20High%20Quality%204D%20Shape%20Generation%20from%20Videos&body=Title%3A%20ShapeGen4D%3A%20Towards%20High%20Quality%204D%20Shape%20Generation%20from%20Videos%0AAuthor%3A%20Jiraphon%20Yenphraphai%20and%20Ashkan%20Mirzaei%20and%20Jianqi%20Chen%20and%20Jiaxu%20Zou%20and%20Sergey%20Tulyakov%20and%20Raymond%20A.%20Yeh%20and%20Peter%20Wonka%20and%20Chaoyang%20Wang%0AAbstract%3A%20%20%20Video-conditioned%204D%20shape%20generation%20aims%20to%20recover%20time-varying%203D%0Ageometry%20and%20view-consistent%20appearance%20directly%20from%20an%20input%20video.%20In%20this%0Awork%2C%20we%20introduce%20a%20native%20video-to-4D%20shape%20generation%20framework%20that%0Asynthesizes%20a%20single%20dynamic%203D%20representation%20end-to-end%20from%20the%20video.%20Our%0Aframework%20introduces%20three%20key%20components%20based%20on%20large-scale%20pre-trained%203D%0Amodels%3A%20%28i%29%20a%20temporal%20attention%20that%20conditions%20generation%20on%20all%20frames%20while%0Aproducing%20a%20time-indexed%20dynamic%20representation%3B%20%28ii%29%20a%20time-aware%20point%0Asampling%20and%204D%20latent%20anchoring%20that%20promote%20temporally%20consistent%20geometry%0Aand%20texture%3B%20and%20%28iii%29%20noise%20sharing%20across%20frames%20to%20enhance%20temporal%0Astability.%20Our%20method%20accurately%20captures%20non-rigid%20motion%2C%20volume%20changes%2C%20and%0Aeven%20topological%20transitions%20without%20per-frame%20optimization.%20Across%20diverse%0Ain-the-wild%20videos%2C%20our%20method%20improves%20robustness%20and%20perceptual%20fidelity%20and%0Areduces%20failure%20modes%20compared%20with%20the%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapeGen4D%253A%2520Towards%2520High%2520Quality%25204D%2520Shape%2520Generation%2520from%2520Videos%26entry.906535625%3DJiraphon%2520Yenphraphai%2520and%2520Ashkan%2520Mirzaei%2520and%2520Jianqi%2520Chen%2520and%2520Jiaxu%2520Zou%2520and%2520Sergey%2520Tulyakov%2520and%2520Raymond%2520A.%2520Yeh%2520and%2520Peter%2520Wonka%2520and%2520Chaoyang%2520Wang%26entry.1292438233%3D%2520%2520Video-conditioned%25204D%2520shape%2520generation%2520aims%2520to%2520recover%2520time-varying%25203D%250Ageometry%2520and%2520view-consistent%2520appearance%2520directly%2520from%2520an%2520input%2520video.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520native%2520video-to-4D%2520shape%2520generation%2520framework%2520that%250Asynthesizes%2520a%2520single%2520dynamic%25203D%2520representation%2520end-to-end%2520from%2520the%2520video.%2520Our%250Aframework%2520introduces%2520three%2520key%2520components%2520based%2520on%2520large-scale%2520pre-trained%25203D%250Amodels%253A%2520%2528i%2529%2520a%2520temporal%2520attention%2520that%2520conditions%2520generation%2520on%2520all%2520frames%2520while%250Aproducing%2520a%2520time-indexed%2520dynamic%2520representation%253B%2520%2528ii%2529%2520a%2520time-aware%2520point%250Asampling%2520and%25204D%2520latent%2520anchoring%2520that%2520promote%2520temporally%2520consistent%2520geometry%250Aand%2520texture%253B%2520and%2520%2528iii%2529%2520noise%2520sharing%2520across%2520frames%2520to%2520enhance%2520temporal%250Astability.%2520Our%2520method%2520accurately%2520captures%2520non-rigid%2520motion%252C%2520volume%2520changes%252C%2520and%250Aeven%2520topological%2520transitions%2520without%2520per-frame%2520optimization.%2520Across%2520diverse%250Ain-the-wild%2520videos%252C%2520our%2520method%2520improves%2520robustness%2520and%2520perceptual%2520fidelity%2520and%250Areduces%2520failure%2520modes%2520compared%2520with%2520the%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeGen4D%3A%20Towards%20High%20Quality%204D%20Shape%20Generation%20from%20Videos&entry.906535625=Jiraphon%20Yenphraphai%20and%20Ashkan%20Mirzaei%20and%20Jianqi%20Chen%20and%20Jiaxu%20Zou%20and%20Sergey%20Tulyakov%20and%20Raymond%20A.%20Yeh%20and%20Peter%20Wonka%20and%20Chaoyang%20Wang&entry.1292438233=%20%20Video-conditioned%204D%20shape%20generation%20aims%20to%20recover%20time-varying%203D%0Ageometry%20and%20view-consistent%20appearance%20directly%20from%20an%20input%20video.%20In%20this%0Awork%2C%20we%20introduce%20a%20native%20video-to-4D%20shape%20generation%20framework%20that%0Asynthesizes%20a%20single%20dynamic%203D%20representation%20end-to-end%20from%20the%20video.%20Our%0Aframework%20introduces%20three%20key%20components%20based%20on%20large-scale%20pre-trained%203D%0Amodels%3A%20%28i%29%20a%20temporal%20attention%20that%20conditions%20generation%20on%20all%20frames%20while%0Aproducing%20a%20time-indexed%20dynamic%20representation%3B%20%28ii%29%20a%20time-aware%20point%0Asampling%20and%204D%20latent%20anchoring%20that%20promote%20temporally%20consistent%20geometry%0Aand%20texture%3B%20and%20%28iii%29%20noise%20sharing%20across%20frames%20to%20enhance%20temporal%0Astability.%20Our%20method%20accurately%20captures%20non-rigid%20motion%2C%20volume%20changes%2C%20and%0Aeven%20topological%20transitions%20without%20per-frame%20optimization.%20Across%20diverse%0Ain-the-wild%20videos%2C%20our%20method%20improves%20robustness%20and%20perceptual%20fidelity%20and%0Areduces%20failure%20modes%20compared%20with%20the%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06208v1&entry.124074799=Read"},
{"title": "Cross-Embodiment Dexterous Hand Articulation Generation via\n  Morphology-Aware Learning", "author": "Heng Zhang and Kevin Yuchen Ma and Mike Zheng Shou and Weisi Lin and Yan Wu", "abstract": "  Dexterous grasping with multi-fingered hands remains challenging due to\nhigh-dimensional articulations and the cost of optimization-based pipelines.\nExisting end-to-end methods require training on large-scale datasets for\nspecific hands, limiting their ability to generalize across different\nembodiments. We propose an eigengrasp-based, end-to-end framework for\ncross-embodiment grasp generation. From a hand's morphology description, we\nderive a morphology embedding and an eigengrasp set. Conditioned on these,\ntogether with the object point cloud and wrist pose, an amplitude predictor\nregresses articulation coefficients in a low-dimensional space, which are\ndecoded into full joint articulations. Articulation learning is supervised with\na Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant\nmotions and injects morphology-specific structure. In simulation on unseen\nobjects across three dexterous hands, our model attains a 91.9% average grasp\nsuccess rate with less than 0.4 seconds inference per grasp. With few-shot\nadaptation to an unseen hand, it achieves 85.6% success on unseen objects in\nsimulation, and real-world experiments on this few-shot generalized hand\nachieve an 87% success rate. The code and additional materials will be made\navailable upon publication on our project website\nhttps://connor-zh.github.io/cross_embodiment_dexterous_grasping.\n", "link": "http://arxiv.org/abs/2510.06068v1", "date": "2025-10-07", "relevancy": 3.0285, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.724}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Embodiment%20Dexterous%20Hand%20Articulation%20Generation%20via%0A%20%20Morphology-Aware%20Learning&body=Title%3A%20Cross-Embodiment%20Dexterous%20Hand%20Articulation%20Generation%20via%0A%20%20Morphology-Aware%20Learning%0AAuthor%3A%20Heng%20Zhang%20and%20Kevin%20Yuchen%20Ma%20and%20Mike%20Zheng%20Shou%20and%20Weisi%20Lin%20and%20Yan%20Wu%0AAbstract%3A%20%20%20Dexterous%20grasping%20with%20multi-fingered%20hands%20remains%20challenging%20due%20to%0Ahigh-dimensional%20articulations%20and%20the%20cost%20of%20optimization-based%20pipelines.%0AExisting%20end-to-end%20methods%20require%20training%20on%20large-scale%20datasets%20for%0Aspecific%20hands%2C%20limiting%20their%20ability%20to%20generalize%20across%20different%0Aembodiments.%20We%20propose%20an%20eigengrasp-based%2C%20end-to-end%20framework%20for%0Across-embodiment%20grasp%20generation.%20From%20a%20hand%27s%20morphology%20description%2C%20we%0Aderive%20a%20morphology%20embedding%20and%20an%20eigengrasp%20set.%20Conditioned%20on%20these%2C%0Atogether%20with%20the%20object%20point%20cloud%20and%20wrist%20pose%2C%20an%20amplitude%20predictor%0Aregresses%20articulation%20coefficients%20in%20a%20low-dimensional%20space%2C%20which%20are%0Adecoded%20into%20full%20joint%20articulations.%20Articulation%20learning%20is%20supervised%20with%0Aa%20Kinematic-Aware%20Articulation%20Loss%20%28KAL%29%20that%20emphasizes%20fingertip-relevant%0Amotions%20and%20injects%20morphology-specific%20structure.%20In%20simulation%20on%20unseen%0Aobjects%20across%20three%20dexterous%20hands%2C%20our%20model%20attains%20a%2091.9%25%20average%20grasp%0Asuccess%20rate%20with%20less%20than%200.4%20seconds%20inference%20per%20grasp.%20With%20few-shot%0Aadaptation%20to%20an%20unseen%20hand%2C%20it%20achieves%2085.6%25%20success%20on%20unseen%20objects%20in%0Asimulation%2C%20and%20real-world%20experiments%20on%20this%20few-shot%20generalized%20hand%0Aachieve%20an%2087%25%20success%20rate.%20The%20code%20and%20additional%20materials%20will%20be%20made%0Aavailable%20upon%20publication%20on%20our%20project%20website%0Ahttps%3A//connor-zh.github.io/cross_embodiment_dexterous_grasping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Embodiment%2520Dexterous%2520Hand%2520Articulation%2520Generation%2520via%250A%2520%2520Morphology-Aware%2520Learning%26entry.906535625%3DHeng%2520Zhang%2520and%2520Kevin%2520Yuchen%2520Ma%2520and%2520Mike%2520Zheng%2520Shou%2520and%2520Weisi%2520Lin%2520and%2520Yan%2520Wu%26entry.1292438233%3D%2520%2520Dexterous%2520grasping%2520with%2520multi-fingered%2520hands%2520remains%2520challenging%2520due%2520to%250Ahigh-dimensional%2520articulations%2520and%2520the%2520cost%2520of%2520optimization-based%2520pipelines.%250AExisting%2520end-to-end%2520methods%2520require%2520training%2520on%2520large-scale%2520datasets%2520for%250Aspecific%2520hands%252C%2520limiting%2520their%2520ability%2520to%2520generalize%2520across%2520different%250Aembodiments.%2520We%2520propose%2520an%2520eigengrasp-based%252C%2520end-to-end%2520framework%2520for%250Across-embodiment%2520grasp%2520generation.%2520From%2520a%2520hand%2527s%2520morphology%2520description%252C%2520we%250Aderive%2520a%2520morphology%2520embedding%2520and%2520an%2520eigengrasp%2520set.%2520Conditioned%2520on%2520these%252C%250Atogether%2520with%2520the%2520object%2520point%2520cloud%2520and%2520wrist%2520pose%252C%2520an%2520amplitude%2520predictor%250Aregresses%2520articulation%2520coefficients%2520in%2520a%2520low-dimensional%2520space%252C%2520which%2520are%250Adecoded%2520into%2520full%2520joint%2520articulations.%2520Articulation%2520learning%2520is%2520supervised%2520with%250Aa%2520Kinematic-Aware%2520Articulation%2520Loss%2520%2528KAL%2529%2520that%2520emphasizes%2520fingertip-relevant%250Amotions%2520and%2520injects%2520morphology-specific%2520structure.%2520In%2520simulation%2520on%2520unseen%250Aobjects%2520across%2520three%2520dexterous%2520hands%252C%2520our%2520model%2520attains%2520a%252091.9%2525%2520average%2520grasp%250Asuccess%2520rate%2520with%2520less%2520than%25200.4%2520seconds%2520inference%2520per%2520grasp.%2520With%2520few-shot%250Aadaptation%2520to%2520an%2520unseen%2520hand%252C%2520it%2520achieves%252085.6%2525%2520success%2520on%2520unseen%2520objects%2520in%250Asimulation%252C%2520and%2520real-world%2520experiments%2520on%2520this%2520few-shot%2520generalized%2520hand%250Aachieve%2520an%252087%2525%2520success%2520rate.%2520The%2520code%2520and%2520additional%2520materials%2520will%2520be%2520made%250Aavailable%2520upon%2520publication%2520on%2520our%2520project%2520website%250Ahttps%253A//connor-zh.github.io/cross_embodiment_dexterous_grasping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Embodiment%20Dexterous%20Hand%20Articulation%20Generation%20via%0A%20%20Morphology-Aware%20Learning&entry.906535625=Heng%20Zhang%20and%20Kevin%20Yuchen%20Ma%20and%20Mike%20Zheng%20Shou%20and%20Weisi%20Lin%20and%20Yan%20Wu&entry.1292438233=%20%20Dexterous%20grasping%20with%20multi-fingered%20hands%20remains%20challenging%20due%20to%0Ahigh-dimensional%20articulations%20and%20the%20cost%20of%20optimization-based%20pipelines.%0AExisting%20end-to-end%20methods%20require%20training%20on%20large-scale%20datasets%20for%0Aspecific%20hands%2C%20limiting%20their%20ability%20to%20generalize%20across%20different%0Aembodiments.%20We%20propose%20an%20eigengrasp-based%2C%20end-to-end%20framework%20for%0Across-embodiment%20grasp%20generation.%20From%20a%20hand%27s%20morphology%20description%2C%20we%0Aderive%20a%20morphology%20embedding%20and%20an%20eigengrasp%20set.%20Conditioned%20on%20these%2C%0Atogether%20with%20the%20object%20point%20cloud%20and%20wrist%20pose%2C%20an%20amplitude%20predictor%0Aregresses%20articulation%20coefficients%20in%20a%20low-dimensional%20space%2C%20which%20are%0Adecoded%20into%20full%20joint%20articulations.%20Articulation%20learning%20is%20supervised%20with%0Aa%20Kinematic-Aware%20Articulation%20Loss%20%28KAL%29%20that%20emphasizes%20fingertip-relevant%0Amotions%20and%20injects%20morphology-specific%20structure.%20In%20simulation%20on%20unseen%0Aobjects%20across%20three%20dexterous%20hands%2C%20our%20model%20attains%20a%2091.9%25%20average%20grasp%0Asuccess%20rate%20with%20less%20than%200.4%20seconds%20inference%20per%20grasp.%20With%20few-shot%0Aadaptation%20to%20an%20unseen%20hand%2C%20it%20achieves%2085.6%25%20success%20on%20unseen%20objects%20in%0Asimulation%2C%20and%20real-world%20experiments%20on%20this%20few-shot%20generalized%20hand%0Aachieve%20an%2087%25%20success%20rate.%20The%20code%20and%20additional%20materials%20will%20be%20made%0Aavailable%20upon%20publication%20on%20our%20project%20website%0Ahttps%3A//connor-zh.github.io/cross_embodiment_dexterous_grasping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06068v1&entry.124074799=Read"},
{"title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models\n  Hallucinations in Scene Text Spotting and Understanding", "author": "Yan Shu and Hangui Lin and Yexin Liu and Yan Zhang and Gangyan Zeng and Yan Li and Yu Zhou and Ser-Nam Lim and Harry Yang and Nicu Sebe", "abstract": "  Large Multimodal Models (LMMs) have achieved impressive progress in visual\nperception and reasoning. However, when confronted with visually ambiguous or\nnon-semantic scene text, they often struggle to accurately spot and understand\nthe content, frequently generating semantically plausible yet visually\nincorrect answers, which we refer to as semantic hallucination. In this work,\nwe investigate the underlying causes of semantic hallucination and identify a\nkey finding: Transformer layers in LLM with stronger attention focus on scene\ntext regions are less prone to producing semantic hallucinations. Thus, we\npropose a training-free semantic hallucination mitigation framework comprising\ntwo key components: (1) ZoomText, a coarse-to-fine strategy that identifies\npotential text regions without external detectors; and (2) Grounded Layer\nCorrection, which adaptively leverages the internal representations from layers\nless prone to hallucination to guide decoding, correcting hallucinated outputs\nfor non-semantic samples while preserving the semantics of meaningful ones. To\nenable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of 1,740\nsamples spanning both semantic and non-semantic cases, with manually curated\nquestion answer pairs designed to probe model hallucinations. Extensive\nexperiments demonstrate that our method not only effectively mitigates semantic\nhallucination but also achieves strong performance on public benchmarks for\nscene text spotting and understanding.\n", "link": "http://arxiv.org/abs/2506.05551v2", "date": "2025-10-07", "relevancy": 3.0167, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Semantics%20Mislead%20Vision%3A%20Mitigating%20Large%20Multimodal%20Models%0A%20%20Hallucinations%20in%20Scene%20Text%20Spotting%20and%20Understanding&body=Title%3A%20When%20Semantics%20Mislead%20Vision%3A%20Mitigating%20Large%20Multimodal%20Models%0A%20%20Hallucinations%20in%20Scene%20Text%20Spotting%20and%20Understanding%0AAuthor%3A%20Yan%20Shu%20and%20Hangui%20Lin%20and%20Yexin%20Liu%20and%20Yan%20Zhang%20and%20Gangyan%20Zeng%20and%20Yan%20Li%20and%20Yu%20Zhou%20and%20Ser-Nam%20Lim%20and%20Harry%20Yang%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20achieved%20impressive%20progress%20in%20visual%0Aperception%20and%20reasoning.%20However%2C%20when%20confronted%20with%20visually%20ambiguous%20or%0Anon-semantic%20scene%20text%2C%20they%20often%20struggle%20to%20accurately%20spot%20and%20understand%0Athe%20content%2C%20frequently%20generating%20semantically%20plausible%20yet%20visually%0Aincorrect%20answers%2C%20which%20we%20refer%20to%20as%20semantic%20hallucination.%20In%20this%20work%2C%0Awe%20investigate%20the%20underlying%20causes%20of%20semantic%20hallucination%20and%20identify%20a%0Akey%20finding%3A%20Transformer%20layers%20in%20LLM%20with%20stronger%20attention%20focus%20on%20scene%0Atext%20regions%20are%20less%20prone%20to%20producing%20semantic%20hallucinations.%20Thus%2C%20we%0Apropose%20a%20training-free%20semantic%20hallucination%20mitigation%20framework%20comprising%0Atwo%20key%20components%3A%20%281%29%20ZoomText%2C%20a%20coarse-to-fine%20strategy%20that%20identifies%0Apotential%20text%20regions%20without%20external%20detectors%3B%20and%20%282%29%20Grounded%20Layer%0ACorrection%2C%20which%20adaptively%20leverages%20the%20internal%20representations%20from%20layers%0Aless%20prone%20to%20hallucination%20to%20guide%20decoding%2C%20correcting%20hallucinated%20outputs%0Afor%20non-semantic%20samples%20while%20preserving%20the%20semantics%20of%20meaningful%20ones.%20To%0Aenable%20rigorous%20evaluation%2C%20we%20introduce%20TextHalu-Bench%2C%20a%20benchmark%20of%201%2C740%0Asamples%20spanning%20both%20semantic%20and%20non-semantic%20cases%2C%20with%20manually%20curated%0Aquestion%20answer%20pairs%20designed%20to%20probe%20model%20hallucinations.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20not%20only%20effectively%20mitigates%20semantic%0Ahallucination%20but%20also%20achieves%20strong%20performance%20on%20public%20benchmarks%20for%0Ascene%20text%20spotting%20and%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Semantics%2520Mislead%2520Vision%253A%2520Mitigating%2520Large%2520Multimodal%2520Models%250A%2520%2520Hallucinations%2520in%2520Scene%2520Text%2520Spotting%2520and%2520Understanding%26entry.906535625%3DYan%2520Shu%2520and%2520Hangui%2520Lin%2520and%2520Yexin%2520Liu%2520and%2520Yan%2520Zhang%2520and%2520Gangyan%2520Zeng%2520and%2520Yan%2520Li%2520and%2520Yu%2520Zhou%2520and%2520Ser-Nam%2520Lim%2520and%2520Harry%2520Yang%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520achieved%2520impressive%2520progress%2520in%2520visual%250Aperception%2520and%2520reasoning.%2520However%252C%2520when%2520confronted%2520with%2520visually%2520ambiguous%2520or%250Anon-semantic%2520scene%2520text%252C%2520they%2520often%2520struggle%2520to%2520accurately%2520spot%2520and%2520understand%250Athe%2520content%252C%2520frequently%2520generating%2520semantically%2520plausible%2520yet%2520visually%250Aincorrect%2520answers%252C%2520which%2520we%2520refer%2520to%2520as%2520semantic%2520hallucination.%2520In%2520this%2520work%252C%250Awe%2520investigate%2520the%2520underlying%2520causes%2520of%2520semantic%2520hallucination%2520and%2520identify%2520a%250Akey%2520finding%253A%2520Transformer%2520layers%2520in%2520LLM%2520with%2520stronger%2520attention%2520focus%2520on%2520scene%250Atext%2520regions%2520are%2520less%2520prone%2520to%2520producing%2520semantic%2520hallucinations.%2520Thus%252C%2520we%250Apropose%2520a%2520training-free%2520semantic%2520hallucination%2520mitigation%2520framework%2520comprising%250Atwo%2520key%2520components%253A%2520%25281%2529%2520ZoomText%252C%2520a%2520coarse-to-fine%2520strategy%2520that%2520identifies%250Apotential%2520text%2520regions%2520without%2520external%2520detectors%253B%2520and%2520%25282%2529%2520Grounded%2520Layer%250ACorrection%252C%2520which%2520adaptively%2520leverages%2520the%2520internal%2520representations%2520from%2520layers%250Aless%2520prone%2520to%2520hallucination%2520to%2520guide%2520decoding%252C%2520correcting%2520hallucinated%2520outputs%250Afor%2520non-semantic%2520samples%2520while%2520preserving%2520the%2520semantics%2520of%2520meaningful%2520ones.%2520To%250Aenable%2520rigorous%2520evaluation%252C%2520we%2520introduce%2520TextHalu-Bench%252C%2520a%2520benchmark%2520of%25201%252C740%250Asamples%2520spanning%2520both%2520semantic%2520and%2520non-semantic%2520cases%252C%2520with%2520manually%2520curated%250Aquestion%2520answer%2520pairs%2520designed%2520to%2520probe%2520model%2520hallucinations.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520not%2520only%2520effectively%2520mitigates%2520semantic%250Ahallucination%2520but%2520also%2520achieves%2520strong%2520performance%2520on%2520public%2520benchmarks%2520for%250Ascene%2520text%2520spotting%2520and%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Semantics%20Mislead%20Vision%3A%20Mitigating%20Large%20Multimodal%20Models%0A%20%20Hallucinations%20in%20Scene%20Text%20Spotting%20and%20Understanding&entry.906535625=Yan%20Shu%20and%20Hangui%20Lin%20and%20Yexin%20Liu%20and%20Yan%20Zhang%20and%20Gangyan%20Zeng%20and%20Yan%20Li%20and%20Yu%20Zhou%20and%20Ser-Nam%20Lim%20and%20Harry%20Yang%20and%20Nicu%20Sebe&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20achieved%20impressive%20progress%20in%20visual%0Aperception%20and%20reasoning.%20However%2C%20when%20confronted%20with%20visually%20ambiguous%20or%0Anon-semantic%20scene%20text%2C%20they%20often%20struggle%20to%20accurately%20spot%20and%20understand%0Athe%20content%2C%20frequently%20generating%20semantically%20plausible%20yet%20visually%0Aincorrect%20answers%2C%20which%20we%20refer%20to%20as%20semantic%20hallucination.%20In%20this%20work%2C%0Awe%20investigate%20the%20underlying%20causes%20of%20semantic%20hallucination%20and%20identify%20a%0Akey%20finding%3A%20Transformer%20layers%20in%20LLM%20with%20stronger%20attention%20focus%20on%20scene%0Atext%20regions%20are%20less%20prone%20to%20producing%20semantic%20hallucinations.%20Thus%2C%20we%0Apropose%20a%20training-free%20semantic%20hallucination%20mitigation%20framework%20comprising%0Atwo%20key%20components%3A%20%281%29%20ZoomText%2C%20a%20coarse-to-fine%20strategy%20that%20identifies%0Apotential%20text%20regions%20without%20external%20detectors%3B%20and%20%282%29%20Grounded%20Layer%0ACorrection%2C%20which%20adaptively%20leverages%20the%20internal%20representations%20from%20layers%0Aless%20prone%20to%20hallucination%20to%20guide%20decoding%2C%20correcting%20hallucinated%20outputs%0Afor%20non-semantic%20samples%20while%20preserving%20the%20semantics%20of%20meaningful%20ones.%20To%0Aenable%20rigorous%20evaluation%2C%20we%20introduce%20TextHalu-Bench%2C%20a%20benchmark%20of%201%2C740%0Asamples%20spanning%20both%20semantic%20and%20non-semantic%20cases%2C%20with%20manually%20curated%0Aquestion%20answer%20pairs%20designed%20to%20probe%20model%20hallucinations.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20not%20only%20effectively%20mitigates%20semantic%0Ahallucination%20but%20also%20achieves%20strong%20performance%20on%20public%20benchmarks%20for%0Ascene%20text%20spotting%20and%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05551v2&entry.124074799=Read"},
{"title": "Can foundation models actively gather information in interactive\n  environments to test hypotheses?", "author": "Danny P. Sawyer and Nan Rosemary Ke and Hubert Soyer and Martin Engelcke and David P Reichert and Drew A. Hudson and John Reid and Alexander Lerchner and Danilo Jimenez Rezende and Timothy P Lillicrap and Michael Mozer and Jane X Wang", "abstract": "  Foundation models excel at single-turn reasoning but struggle with multi-turn\nexploration in dynamic environments, a requirement for many real-world\nchallenges. We evaluated these models on their ability to learn from\nexperience, adapt, and gather information. First, in \"Feature World,\" a simple\nsetting for testing information gathering, models performed near-optimally.\nHowever, to test more complex, multi-trial learning, we implemented a\ntext-based version of the \"Alchemy\" environment, a benchmark for meta-learning.\nHere, agents must deduce a latent causal structure by integrating information\nacross many trials. In this setting, recent foundation models initially failed\nto improve their performance over time. Crucially, we found that prompting the\nmodels to summarize their observations at regular intervals enabled an emergent\nmeta-learning process. This allowed them to improve across trials and even\nadaptively re-learn when the environment's rules changed unexpectedly. While\nmost models handled the simple task, Alchemy revealed stark differences in\nrobustness: Gemini 2.5 performed best, followed by Claude 3.7, while ChatGPT-4o\nand o4-mini struggled. This underscores Alchemy's value as a benchmark. Our\nfindings demonstrate that the biggest challenge for foundation models is not\nselecting informative actions in the moment, but integrating knowledge through\nadaptive strategies over time. Encouragingly, there appears to be no intrinsic\nbarrier to future models mastering these abilities.\n", "link": "http://arxiv.org/abs/2412.06438v2", "date": "2025-10-07", "relevancy": 2.95, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5988}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20foundation%20models%20actively%20gather%20information%20in%20interactive%0A%20%20environments%20to%20test%20hypotheses%3F&body=Title%3A%20Can%20foundation%20models%20actively%20gather%20information%20in%20interactive%0A%20%20environments%20to%20test%20hypotheses%3F%0AAuthor%3A%20Danny%20P.%20Sawyer%20and%20Nan%20Rosemary%20Ke%20and%20Hubert%20Soyer%20and%20Martin%20Engelcke%20and%20David%20P%20Reichert%20and%20Drew%20A.%20Hudson%20and%20John%20Reid%20and%20Alexander%20Lerchner%20and%20Danilo%20Jimenez%20Rezende%20and%20Timothy%20P%20Lillicrap%20and%20Michael%20Mozer%20and%20Jane%20X%20Wang%0AAbstract%3A%20%20%20Foundation%20models%20excel%20at%20single-turn%20reasoning%20but%20struggle%20with%20multi-turn%0Aexploration%20in%20dynamic%20environments%2C%20a%20requirement%20for%20many%20real-world%0Achallenges.%20We%20evaluated%20these%20models%20on%20their%20ability%20to%20learn%20from%0Aexperience%2C%20adapt%2C%20and%20gather%20information.%20First%2C%20in%20%22Feature%20World%2C%22%20a%20simple%0Asetting%20for%20testing%20information%20gathering%2C%20models%20performed%20near-optimally.%0AHowever%2C%20to%20test%20more%20complex%2C%20multi-trial%20learning%2C%20we%20implemented%20a%0Atext-based%20version%20of%20the%20%22Alchemy%22%20environment%2C%20a%20benchmark%20for%20meta-learning.%0AHere%2C%20agents%20must%20deduce%20a%20latent%20causal%20structure%20by%20integrating%20information%0Aacross%20many%20trials.%20In%20this%20setting%2C%20recent%20foundation%20models%20initially%20failed%0Ato%20improve%20their%20performance%20over%20time.%20Crucially%2C%20we%20found%20that%20prompting%20the%0Amodels%20to%20summarize%20their%20observations%20at%20regular%20intervals%20enabled%20an%20emergent%0Ameta-learning%20process.%20This%20allowed%20them%20to%20improve%20across%20trials%20and%20even%0Aadaptively%20re-learn%20when%20the%20environment%27s%20rules%20changed%20unexpectedly.%20While%0Amost%20models%20handled%20the%20simple%20task%2C%20Alchemy%20revealed%20stark%20differences%20in%0Arobustness%3A%20Gemini%202.5%20performed%20best%2C%20followed%20by%20Claude%203.7%2C%20while%20ChatGPT-4o%0Aand%20o4-mini%20struggled.%20This%20underscores%20Alchemy%27s%20value%20as%20a%20benchmark.%20Our%0Afindings%20demonstrate%20that%20the%20biggest%20challenge%20for%20foundation%20models%20is%20not%0Aselecting%20informative%20actions%20in%20the%20moment%2C%20but%20integrating%20knowledge%20through%0Aadaptive%20strategies%20over%20time.%20Encouragingly%2C%20there%20appears%20to%20be%20no%20intrinsic%0Abarrier%20to%20future%20models%20mastering%20these%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520foundation%2520models%2520actively%2520gather%2520information%2520in%2520interactive%250A%2520%2520environments%2520to%2520test%2520hypotheses%253F%26entry.906535625%3DDanny%2520P.%2520Sawyer%2520and%2520Nan%2520Rosemary%2520Ke%2520and%2520Hubert%2520Soyer%2520and%2520Martin%2520Engelcke%2520and%2520David%2520P%2520Reichert%2520and%2520Drew%2520A.%2520Hudson%2520and%2520John%2520Reid%2520and%2520Alexander%2520Lerchner%2520and%2520Danilo%2520Jimenez%2520Rezende%2520and%2520Timothy%2520P%2520Lillicrap%2520and%2520Michael%2520Mozer%2520and%2520Jane%2520X%2520Wang%26entry.1292438233%3D%2520%2520Foundation%2520models%2520excel%2520at%2520single-turn%2520reasoning%2520but%2520struggle%2520with%2520multi-turn%250Aexploration%2520in%2520dynamic%2520environments%252C%2520a%2520requirement%2520for%2520many%2520real-world%250Achallenges.%2520We%2520evaluated%2520these%2520models%2520on%2520their%2520ability%2520to%2520learn%2520from%250Aexperience%252C%2520adapt%252C%2520and%2520gather%2520information.%2520First%252C%2520in%2520%2522Feature%2520World%252C%2522%2520a%2520simple%250Asetting%2520for%2520testing%2520information%2520gathering%252C%2520models%2520performed%2520near-optimally.%250AHowever%252C%2520to%2520test%2520more%2520complex%252C%2520multi-trial%2520learning%252C%2520we%2520implemented%2520a%250Atext-based%2520version%2520of%2520the%2520%2522Alchemy%2522%2520environment%252C%2520a%2520benchmark%2520for%2520meta-learning.%250AHere%252C%2520agents%2520must%2520deduce%2520a%2520latent%2520causal%2520structure%2520by%2520integrating%2520information%250Aacross%2520many%2520trials.%2520In%2520this%2520setting%252C%2520recent%2520foundation%2520models%2520initially%2520failed%250Ato%2520improve%2520their%2520performance%2520over%2520time.%2520Crucially%252C%2520we%2520found%2520that%2520prompting%2520the%250Amodels%2520to%2520summarize%2520their%2520observations%2520at%2520regular%2520intervals%2520enabled%2520an%2520emergent%250Ameta-learning%2520process.%2520This%2520allowed%2520them%2520to%2520improve%2520across%2520trials%2520and%2520even%250Aadaptively%2520re-learn%2520when%2520the%2520environment%2527s%2520rules%2520changed%2520unexpectedly.%2520While%250Amost%2520models%2520handled%2520the%2520simple%2520task%252C%2520Alchemy%2520revealed%2520stark%2520differences%2520in%250Arobustness%253A%2520Gemini%25202.5%2520performed%2520best%252C%2520followed%2520by%2520Claude%25203.7%252C%2520while%2520ChatGPT-4o%250Aand%2520o4-mini%2520struggled.%2520This%2520underscores%2520Alchemy%2527s%2520value%2520as%2520a%2520benchmark.%2520Our%250Afindings%2520demonstrate%2520that%2520the%2520biggest%2520challenge%2520for%2520foundation%2520models%2520is%2520not%250Aselecting%2520informative%2520actions%2520in%2520the%2520moment%252C%2520but%2520integrating%2520knowledge%2520through%250Aadaptive%2520strategies%2520over%2520time.%2520Encouragingly%252C%2520there%2520appears%2520to%2520be%2520no%2520intrinsic%250Abarrier%2520to%2520future%2520models%2520mastering%2520these%2520abilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20foundation%20models%20actively%20gather%20information%20in%20interactive%0A%20%20environments%20to%20test%20hypotheses%3F&entry.906535625=Danny%20P.%20Sawyer%20and%20Nan%20Rosemary%20Ke%20and%20Hubert%20Soyer%20and%20Martin%20Engelcke%20and%20David%20P%20Reichert%20and%20Drew%20A.%20Hudson%20and%20John%20Reid%20and%20Alexander%20Lerchner%20and%20Danilo%20Jimenez%20Rezende%20and%20Timothy%20P%20Lillicrap%20and%20Michael%20Mozer%20and%20Jane%20X%20Wang&entry.1292438233=%20%20Foundation%20models%20excel%20at%20single-turn%20reasoning%20but%20struggle%20with%20multi-turn%0Aexploration%20in%20dynamic%20environments%2C%20a%20requirement%20for%20many%20real-world%0Achallenges.%20We%20evaluated%20these%20models%20on%20their%20ability%20to%20learn%20from%0Aexperience%2C%20adapt%2C%20and%20gather%20information.%20First%2C%20in%20%22Feature%20World%2C%22%20a%20simple%0Asetting%20for%20testing%20information%20gathering%2C%20models%20performed%20near-optimally.%0AHowever%2C%20to%20test%20more%20complex%2C%20multi-trial%20learning%2C%20we%20implemented%20a%0Atext-based%20version%20of%20the%20%22Alchemy%22%20environment%2C%20a%20benchmark%20for%20meta-learning.%0AHere%2C%20agents%20must%20deduce%20a%20latent%20causal%20structure%20by%20integrating%20information%0Aacross%20many%20trials.%20In%20this%20setting%2C%20recent%20foundation%20models%20initially%20failed%0Ato%20improve%20their%20performance%20over%20time.%20Crucially%2C%20we%20found%20that%20prompting%20the%0Amodels%20to%20summarize%20their%20observations%20at%20regular%20intervals%20enabled%20an%20emergent%0Ameta-learning%20process.%20This%20allowed%20them%20to%20improve%20across%20trials%20and%20even%0Aadaptively%20re-learn%20when%20the%20environment%27s%20rules%20changed%20unexpectedly.%20While%0Amost%20models%20handled%20the%20simple%20task%2C%20Alchemy%20revealed%20stark%20differences%20in%0Arobustness%3A%20Gemini%202.5%20performed%20best%2C%20followed%20by%20Claude%203.7%2C%20while%20ChatGPT-4o%0Aand%20o4-mini%20struggled.%20This%20underscores%20Alchemy%27s%20value%20as%20a%20benchmark.%20Our%0Afindings%20demonstrate%20that%20the%20biggest%20challenge%20for%20foundation%20models%20is%20not%0Aselecting%20informative%20actions%20in%20the%20moment%2C%20but%20integrating%20knowledge%20through%0Aadaptive%20strategies%20over%20time.%20Encouragingly%2C%20there%20appears%20to%20be%20no%20intrinsic%0Abarrier%20to%20future%20models%20mastering%20these%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06438v2&entry.124074799=Read"},
{"title": "OneVision: An End-to-End Generative Framework for Multi-view E-commerce\n  Vision Search", "author": "Zexin Zheng and Huangyu Dai and Lingtao Mao and Xinyu Sun and Zihan Liang and Ben Chen and Yuqing Ding and Chenyi Lei and Wenwu Ou and Han Li and Kun Gai", "abstract": "  Traditional vision search, similar to search and recommendation systems,\nfollows the multi-stage cascading architecture (MCA) paradigm to balance\nefficiency and conversion. Specifically, the query image undergoes feature\nextraction, recall, pre-ranking, and ranking stages, ultimately presenting the\nuser with semantically similar products that meet their preferences. This\nmulti-view representation discrepancy of the same object in the query and the\noptimization objective collide across these stages, making it difficult to\nachieve Pareto optimality in both user experience and conversion. In this\npaper, an end-to-end generative framework, OneVision, is proposed to address\nthese problems. OneVision builds on VRQ, a vision-aligned residual quantization\nencoding, which can align the vastly different representations of an object\nacross multiple viewpoints while preserving the distinctive features of each\nproduct as much as possible. Then a multi-stage semantic alignment scheme is\nadopted to maintain strong visual similarity priors while effectively\nincorporating user-specific information for personalized preference generation.\nIn offline evaluations, OneVision performs on par with online MCA, while\nimproving inference efficiency by 21% through dynamic pruning. In A/B tests, it\nachieves significant online improvements: +2.15% item CTR, +2.27% CVR, and\n+3.12% order volume. These results demonstrate that a semantic ID centric,\ngenerative architecture can unify retrieval and personalization while\nsimplifying the serving pathway.\n", "link": "http://arxiv.org/abs/2510.05759v1", "date": "2025-10-07", "relevancy": 2.9256, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneVision%3A%20An%20End-to-End%20Generative%20Framework%20for%20Multi-view%20E-commerce%0A%20%20Vision%20Search&body=Title%3A%20OneVision%3A%20An%20End-to-End%20Generative%20Framework%20for%20Multi-view%20E-commerce%0A%20%20Vision%20Search%0AAuthor%3A%20Zexin%20Zheng%20and%20Huangyu%20Dai%20and%20Lingtao%20Mao%20and%20Xinyu%20Sun%20and%20Zihan%20Liang%20and%20Ben%20Chen%20and%20Yuqing%20Ding%20and%20Chenyi%20Lei%20and%20Wenwu%20Ou%20and%20Han%20Li%20and%20Kun%20Gai%0AAbstract%3A%20%20%20Traditional%20vision%20search%2C%20similar%20to%20search%20and%20recommendation%20systems%2C%0Afollows%20the%20multi-stage%20cascading%20architecture%20%28MCA%29%20paradigm%20to%20balance%0Aefficiency%20and%20conversion.%20Specifically%2C%20the%20query%20image%20undergoes%20feature%0Aextraction%2C%20recall%2C%20pre-ranking%2C%20and%20ranking%20stages%2C%20ultimately%20presenting%20the%0Auser%20with%20semantically%20similar%20products%20that%20meet%20their%20preferences.%20This%0Amulti-view%20representation%20discrepancy%20of%20the%20same%20object%20in%20the%20query%20and%20the%0Aoptimization%20objective%20collide%20across%20these%20stages%2C%20making%20it%20difficult%20to%0Aachieve%20Pareto%20optimality%20in%20both%20user%20experience%20and%20conversion.%20In%20this%0Apaper%2C%20an%20end-to-end%20generative%20framework%2C%20OneVision%2C%20is%20proposed%20to%20address%0Athese%20problems.%20OneVision%20builds%20on%20VRQ%2C%20a%20vision-aligned%20residual%20quantization%0Aencoding%2C%20which%20can%20align%20the%20vastly%20different%20representations%20of%20an%20object%0Aacross%20multiple%20viewpoints%20while%20preserving%20the%20distinctive%20features%20of%20each%0Aproduct%20as%20much%20as%20possible.%20Then%20a%20multi-stage%20semantic%20alignment%20scheme%20is%0Aadopted%20to%20maintain%20strong%20visual%20similarity%20priors%20while%20effectively%0Aincorporating%20user-specific%20information%20for%20personalized%20preference%20generation.%0AIn%20offline%20evaluations%2C%20OneVision%20performs%20on%20par%20with%20online%20MCA%2C%20while%0Aimproving%20inference%20efficiency%20by%2021%25%20through%20dynamic%20pruning.%20In%20A/B%20tests%2C%20it%0Aachieves%20significant%20online%20improvements%3A%20%2B2.15%25%20item%20CTR%2C%20%2B2.27%25%20CVR%2C%20and%0A%2B3.12%25%20order%20volume.%20These%20results%20demonstrate%20that%20a%20semantic%20ID%20centric%2C%0Agenerative%20architecture%20can%20unify%20retrieval%20and%20personalization%20while%0Asimplifying%20the%20serving%20pathway.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneVision%253A%2520An%2520End-to-End%2520Generative%2520Framework%2520for%2520Multi-view%2520E-commerce%250A%2520%2520Vision%2520Search%26entry.906535625%3DZexin%2520Zheng%2520and%2520Huangyu%2520Dai%2520and%2520Lingtao%2520Mao%2520and%2520Xinyu%2520Sun%2520and%2520Zihan%2520Liang%2520and%2520Ben%2520Chen%2520and%2520Yuqing%2520Ding%2520and%2520Chenyi%2520Lei%2520and%2520Wenwu%2520Ou%2520and%2520Han%2520Li%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520Traditional%2520vision%2520search%252C%2520similar%2520to%2520search%2520and%2520recommendation%2520systems%252C%250Afollows%2520the%2520multi-stage%2520cascading%2520architecture%2520%2528MCA%2529%2520paradigm%2520to%2520balance%250Aefficiency%2520and%2520conversion.%2520Specifically%252C%2520the%2520query%2520image%2520undergoes%2520feature%250Aextraction%252C%2520recall%252C%2520pre-ranking%252C%2520and%2520ranking%2520stages%252C%2520ultimately%2520presenting%2520the%250Auser%2520with%2520semantically%2520similar%2520products%2520that%2520meet%2520their%2520preferences.%2520This%250Amulti-view%2520representation%2520discrepancy%2520of%2520the%2520same%2520object%2520in%2520the%2520query%2520and%2520the%250Aoptimization%2520objective%2520collide%2520across%2520these%2520stages%252C%2520making%2520it%2520difficult%2520to%250Aachieve%2520Pareto%2520optimality%2520in%2520both%2520user%2520experience%2520and%2520conversion.%2520In%2520this%250Apaper%252C%2520an%2520end-to-end%2520generative%2520framework%252C%2520OneVision%252C%2520is%2520proposed%2520to%2520address%250Athese%2520problems.%2520OneVision%2520builds%2520on%2520VRQ%252C%2520a%2520vision-aligned%2520residual%2520quantization%250Aencoding%252C%2520which%2520can%2520align%2520the%2520vastly%2520different%2520representations%2520of%2520an%2520object%250Aacross%2520multiple%2520viewpoints%2520while%2520preserving%2520the%2520distinctive%2520features%2520of%2520each%250Aproduct%2520as%2520much%2520as%2520possible.%2520Then%2520a%2520multi-stage%2520semantic%2520alignment%2520scheme%2520is%250Aadopted%2520to%2520maintain%2520strong%2520visual%2520similarity%2520priors%2520while%2520effectively%250Aincorporating%2520user-specific%2520information%2520for%2520personalized%2520preference%2520generation.%250AIn%2520offline%2520evaluations%252C%2520OneVision%2520performs%2520on%2520par%2520with%2520online%2520MCA%252C%2520while%250Aimproving%2520inference%2520efficiency%2520by%252021%2525%2520through%2520dynamic%2520pruning.%2520In%2520A/B%2520tests%252C%2520it%250Aachieves%2520significant%2520online%2520improvements%253A%2520%252B2.15%2525%2520item%2520CTR%252C%2520%252B2.27%2525%2520CVR%252C%2520and%250A%252B3.12%2525%2520order%2520volume.%2520These%2520results%2520demonstrate%2520that%2520a%2520semantic%2520ID%2520centric%252C%250Agenerative%2520architecture%2520can%2520unify%2520retrieval%2520and%2520personalization%2520while%250Asimplifying%2520the%2520serving%2520pathway.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneVision%3A%20An%20End-to-End%20Generative%20Framework%20for%20Multi-view%20E-commerce%0A%20%20Vision%20Search&entry.906535625=Zexin%20Zheng%20and%20Huangyu%20Dai%20and%20Lingtao%20Mao%20and%20Xinyu%20Sun%20and%20Zihan%20Liang%20and%20Ben%20Chen%20and%20Yuqing%20Ding%20and%20Chenyi%20Lei%20and%20Wenwu%20Ou%20and%20Han%20Li%20and%20Kun%20Gai&entry.1292438233=%20%20Traditional%20vision%20search%2C%20similar%20to%20search%20and%20recommendation%20systems%2C%0Afollows%20the%20multi-stage%20cascading%20architecture%20%28MCA%29%20paradigm%20to%20balance%0Aefficiency%20and%20conversion.%20Specifically%2C%20the%20query%20image%20undergoes%20feature%0Aextraction%2C%20recall%2C%20pre-ranking%2C%20and%20ranking%20stages%2C%20ultimately%20presenting%20the%0Auser%20with%20semantically%20similar%20products%20that%20meet%20their%20preferences.%20This%0Amulti-view%20representation%20discrepancy%20of%20the%20same%20object%20in%20the%20query%20and%20the%0Aoptimization%20objective%20collide%20across%20these%20stages%2C%20making%20it%20difficult%20to%0Aachieve%20Pareto%20optimality%20in%20both%20user%20experience%20and%20conversion.%20In%20this%0Apaper%2C%20an%20end-to-end%20generative%20framework%2C%20OneVision%2C%20is%20proposed%20to%20address%0Athese%20problems.%20OneVision%20builds%20on%20VRQ%2C%20a%20vision-aligned%20residual%20quantization%0Aencoding%2C%20which%20can%20align%20the%20vastly%20different%20representations%20of%20an%20object%0Aacross%20multiple%20viewpoints%20while%20preserving%20the%20distinctive%20features%20of%20each%0Aproduct%20as%20much%20as%20possible.%20Then%20a%20multi-stage%20semantic%20alignment%20scheme%20is%0Aadopted%20to%20maintain%20strong%20visual%20similarity%20priors%20while%20effectively%0Aincorporating%20user-specific%20information%20for%20personalized%20preference%20generation.%0AIn%20offline%20evaluations%2C%20OneVision%20performs%20on%20par%20with%20online%20MCA%2C%20while%0Aimproving%20inference%20efficiency%20by%2021%25%20through%20dynamic%20pruning.%20In%20A/B%20tests%2C%20it%0Aachieves%20significant%20online%20improvements%3A%20%2B2.15%25%20item%20CTR%2C%20%2B2.27%25%20CVR%2C%20and%0A%2B3.12%25%20order%20volume.%20These%20results%20demonstrate%20that%20a%20semantic%20ID%20centric%2C%0Agenerative%20architecture%20can%20unify%20retrieval%20and%20personalization%20while%0Asimplifying%20the%20serving%20pathway.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05759v1&entry.124074799=Read"},
{"title": "GLVD: Guided Learned Vertex Descent", "author": "Pol Caselles Rico and Francesc Moreno Noguer", "abstract": "  Existing 3D face modeling methods usually depend on 3D Morphable Models,\nwhich inherently constrain the representation capacity to fixed shape priors.\nOptimization-based approaches offer high-quality reconstructions but tend to be\ncomputationally expensive. In this work, we introduce GLVD, a hybrid method for\n3D face reconstruction from few-shot images that extends Learned Vertex Descent\n(LVD) by integrating per-vertex neural field optimization with global\nstructural guidance from dynamically predicted 3D keypoints. By incorporating\nrelative spatial encoding, GLVD iteratively refines mesh vertices without\nrequiring dense 3D supervision. This enables expressive and adaptable geometry\nreconstruction while maintaining computational efficiency. GLVD achieves\nstate-of-the-art performance in single-view settings and remains highly\ncompetitive in multi-view scenarios, all while substantially reducing inference\ntime.\n", "link": "http://arxiv.org/abs/2510.06046v1", "date": "2025-10-07", "relevancy": 2.9196, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6165}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5676}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLVD%3A%20Guided%20Learned%20Vertex%20Descent&body=Title%3A%20GLVD%3A%20Guided%20Learned%20Vertex%20Descent%0AAuthor%3A%20Pol%20Caselles%20Rico%20and%20Francesc%20Moreno%20Noguer%0AAbstract%3A%20%20%20Existing%203D%20face%20modeling%20methods%20usually%20depend%20on%203D%20Morphable%20Models%2C%0Awhich%20inherently%20constrain%20the%20representation%20capacity%20to%20fixed%20shape%20priors.%0AOptimization-based%20approaches%20offer%20high-quality%20reconstructions%20but%20tend%20to%20be%0Acomputationally%20expensive.%20In%20this%20work%2C%20we%20introduce%20GLVD%2C%20a%20hybrid%20method%20for%0A3D%20face%20reconstruction%20from%20few-shot%20images%20that%20extends%20Learned%20Vertex%20Descent%0A%28LVD%29%20by%20integrating%20per-vertex%20neural%20field%20optimization%20with%20global%0Astructural%20guidance%20from%20dynamically%20predicted%203D%20keypoints.%20By%20incorporating%0Arelative%20spatial%20encoding%2C%20GLVD%20iteratively%20refines%20mesh%20vertices%20without%0Arequiring%20dense%203D%20supervision.%20This%20enables%20expressive%20and%20adaptable%20geometry%0Areconstruction%20while%20maintaining%20computational%20efficiency.%20GLVD%20achieves%0Astate-of-the-art%20performance%20in%20single-view%20settings%20and%20remains%20highly%0Acompetitive%20in%20multi-view%20scenarios%2C%20all%20while%20substantially%20reducing%20inference%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLVD%253A%2520Guided%2520Learned%2520Vertex%2520Descent%26entry.906535625%3DPol%2520Caselles%2520Rico%2520and%2520Francesc%2520Moreno%2520Noguer%26entry.1292438233%3D%2520%2520Existing%25203D%2520face%2520modeling%2520methods%2520usually%2520depend%2520on%25203D%2520Morphable%2520Models%252C%250Awhich%2520inherently%2520constrain%2520the%2520representation%2520capacity%2520to%2520fixed%2520shape%2520priors.%250AOptimization-based%2520approaches%2520offer%2520high-quality%2520reconstructions%2520but%2520tend%2520to%2520be%250Acomputationally%2520expensive.%2520In%2520this%2520work%252C%2520we%2520introduce%2520GLVD%252C%2520a%2520hybrid%2520method%2520for%250A3D%2520face%2520reconstruction%2520from%2520few-shot%2520images%2520that%2520extends%2520Learned%2520Vertex%2520Descent%250A%2528LVD%2529%2520by%2520integrating%2520per-vertex%2520neural%2520field%2520optimization%2520with%2520global%250Astructural%2520guidance%2520from%2520dynamically%2520predicted%25203D%2520keypoints.%2520By%2520incorporating%250Arelative%2520spatial%2520encoding%252C%2520GLVD%2520iteratively%2520refines%2520mesh%2520vertices%2520without%250Arequiring%2520dense%25203D%2520supervision.%2520This%2520enables%2520expressive%2520and%2520adaptable%2520geometry%250Areconstruction%2520while%2520maintaining%2520computational%2520efficiency.%2520GLVD%2520achieves%250Astate-of-the-art%2520performance%2520in%2520single-view%2520settings%2520and%2520remains%2520highly%250Acompetitive%2520in%2520multi-view%2520scenarios%252C%2520all%2520while%2520substantially%2520reducing%2520inference%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLVD%3A%20Guided%20Learned%20Vertex%20Descent&entry.906535625=Pol%20Caselles%20Rico%20and%20Francesc%20Moreno%20Noguer&entry.1292438233=%20%20Existing%203D%20face%20modeling%20methods%20usually%20depend%20on%203D%20Morphable%20Models%2C%0Awhich%20inherently%20constrain%20the%20representation%20capacity%20to%20fixed%20shape%20priors.%0AOptimization-based%20approaches%20offer%20high-quality%20reconstructions%20but%20tend%20to%20be%0Acomputationally%20expensive.%20In%20this%20work%2C%20we%20introduce%20GLVD%2C%20a%20hybrid%20method%20for%0A3D%20face%20reconstruction%20from%20few-shot%20images%20that%20extends%20Learned%20Vertex%20Descent%0A%28LVD%29%20by%20integrating%20per-vertex%20neural%20field%20optimization%20with%20global%0Astructural%20guidance%20from%20dynamically%20predicted%203D%20keypoints.%20By%20incorporating%0Arelative%20spatial%20encoding%2C%20GLVD%20iteratively%20refines%20mesh%20vertices%20without%0Arequiring%20dense%203D%20supervision.%20This%20enables%20expressive%20and%20adaptable%20geometry%0Areconstruction%20while%20maintaining%20computational%20efficiency.%20GLVD%20achieves%0Astate-of-the-art%20performance%20in%20single-view%20settings%20and%20remains%20highly%0Acompetitive%20in%20multi-view%20scenarios%2C%20all%20while%20substantially%20reducing%20inference%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06046v1&entry.124074799=Read"},
{"title": "Coordinate-Consistent Localization via Continuous-Time Calibration and\n  Fusion of UWB and SLAM Observations", "author": "Tien-Dat Nguyen and Thien-Minh Nguyen and Vinh-Hao Nguyen", "abstract": "  Onboard simultaneous localization and mapping (SLAM) methods are commonly\nused to provide accurate localization information for autonomous robots.\nHowever, the coordinate origin of SLAM estimate often resets for each run. On\nthe other hand, UWB-based localization with fixed anchors can ensure a\nconsistent coordinate reference across sessions; however, it requires an\naccurate assignment of the anchor nodes' coordinates. To this end, we propose a\ntwo-stage approach that calibrates and fuses UWB data and SLAM data to achieve\ncoordinate-wise consistent and accurate localization in the same environment.\nIn the first stage, we solve a continuous-time batch optimization problem by\nusing the range and odometry data from one full run, incorporating height\npriors and anchor-to-anchor distance factors to recover the anchors' 3D\npositions. For the subsequent runs in the second stage, a sliding-window\noptimization scheme fuses the UWB and SLAM data, which facilitates accurate\nlocalization in the same coordinate system. Experiments are carried out on the\nNTU VIRAL dataset with six scenarios of UAV flight, and we show that\ncalibration using data in one run is sufficient to enable accurate localization\nin the remaining runs. We release our source code to benefit the community at\nhttps://github.com/ntdathp/slam-uwb-calibration.\n", "link": "http://arxiv.org/abs/2510.05992v1", "date": "2025-10-07", "relevancy": 2.8619, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6473}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5419}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coordinate-Consistent%20Localization%20via%20Continuous-Time%20Calibration%20and%0A%20%20Fusion%20of%20UWB%20and%20SLAM%20Observations&body=Title%3A%20Coordinate-Consistent%20Localization%20via%20Continuous-Time%20Calibration%20and%0A%20%20Fusion%20of%20UWB%20and%20SLAM%20Observations%0AAuthor%3A%20Tien-Dat%20Nguyen%20and%20Thien-Minh%20Nguyen%20and%20Vinh-Hao%20Nguyen%0AAbstract%3A%20%20%20Onboard%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20methods%20are%20commonly%0Aused%20to%20provide%20accurate%20localization%20information%20for%20autonomous%20robots.%0AHowever%2C%20the%20coordinate%20origin%20of%20SLAM%20estimate%20often%20resets%20for%20each%20run.%20On%0Athe%20other%20hand%2C%20UWB-based%20localization%20with%20fixed%20anchors%20can%20ensure%20a%0Aconsistent%20coordinate%20reference%20across%20sessions%3B%20however%2C%20it%20requires%20an%0Aaccurate%20assignment%20of%20the%20anchor%20nodes%27%20coordinates.%20To%20this%20end%2C%20we%20propose%20a%0Atwo-stage%20approach%20that%20calibrates%20and%20fuses%20UWB%20data%20and%20SLAM%20data%20to%20achieve%0Acoordinate-wise%20consistent%20and%20accurate%20localization%20in%20the%20same%20environment.%0AIn%20the%20first%20stage%2C%20we%20solve%20a%20continuous-time%20batch%20optimization%20problem%20by%0Ausing%20the%20range%20and%20odometry%20data%20from%20one%20full%20run%2C%20incorporating%20height%0Apriors%20and%20anchor-to-anchor%20distance%20factors%20to%20recover%20the%20anchors%27%203D%0Apositions.%20For%20the%20subsequent%20runs%20in%20the%20second%20stage%2C%20a%20sliding-window%0Aoptimization%20scheme%20fuses%20the%20UWB%20and%20SLAM%20data%2C%20which%20facilitates%20accurate%0Alocalization%20in%20the%20same%20coordinate%20system.%20Experiments%20are%20carried%20out%20on%20the%0ANTU%20VIRAL%20dataset%20with%20six%20scenarios%20of%20UAV%20flight%2C%20and%20we%20show%20that%0Acalibration%20using%20data%20in%20one%20run%20is%20sufficient%20to%20enable%20accurate%20localization%0Ain%20the%20remaining%20runs.%20We%20release%20our%20source%20code%20to%20benefit%20the%20community%20at%0Ahttps%3A//github.com/ntdathp/slam-uwb-calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoordinate-Consistent%2520Localization%2520via%2520Continuous-Time%2520Calibration%2520and%250A%2520%2520Fusion%2520of%2520UWB%2520and%2520SLAM%2520Observations%26entry.906535625%3DTien-Dat%2520Nguyen%2520and%2520Thien-Minh%2520Nguyen%2520and%2520Vinh-Hao%2520Nguyen%26entry.1292438233%3D%2520%2520Onboard%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520methods%2520are%2520commonly%250Aused%2520to%2520provide%2520accurate%2520localization%2520information%2520for%2520autonomous%2520robots.%250AHowever%252C%2520the%2520coordinate%2520origin%2520of%2520SLAM%2520estimate%2520often%2520resets%2520for%2520each%2520run.%2520On%250Athe%2520other%2520hand%252C%2520UWB-based%2520localization%2520with%2520fixed%2520anchors%2520can%2520ensure%2520a%250Aconsistent%2520coordinate%2520reference%2520across%2520sessions%253B%2520however%252C%2520it%2520requires%2520an%250Aaccurate%2520assignment%2520of%2520the%2520anchor%2520nodes%2527%2520coordinates.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Atwo-stage%2520approach%2520that%2520calibrates%2520and%2520fuses%2520UWB%2520data%2520and%2520SLAM%2520data%2520to%2520achieve%250Acoordinate-wise%2520consistent%2520and%2520accurate%2520localization%2520in%2520the%2520same%2520environment.%250AIn%2520the%2520first%2520stage%252C%2520we%2520solve%2520a%2520continuous-time%2520batch%2520optimization%2520problem%2520by%250Ausing%2520the%2520range%2520and%2520odometry%2520data%2520from%2520one%2520full%2520run%252C%2520incorporating%2520height%250Apriors%2520and%2520anchor-to-anchor%2520distance%2520factors%2520to%2520recover%2520the%2520anchors%2527%25203D%250Apositions.%2520For%2520the%2520subsequent%2520runs%2520in%2520the%2520second%2520stage%252C%2520a%2520sliding-window%250Aoptimization%2520scheme%2520fuses%2520the%2520UWB%2520and%2520SLAM%2520data%252C%2520which%2520facilitates%2520accurate%250Alocalization%2520in%2520the%2520same%2520coordinate%2520system.%2520Experiments%2520are%2520carried%2520out%2520on%2520the%250ANTU%2520VIRAL%2520dataset%2520with%2520six%2520scenarios%2520of%2520UAV%2520flight%252C%2520and%2520we%2520show%2520that%250Acalibration%2520using%2520data%2520in%2520one%2520run%2520is%2520sufficient%2520to%2520enable%2520accurate%2520localization%250Ain%2520the%2520remaining%2520runs.%2520We%2520release%2520our%2520source%2520code%2520to%2520benefit%2520the%2520community%2520at%250Ahttps%253A//github.com/ntdathp/slam-uwb-calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coordinate-Consistent%20Localization%20via%20Continuous-Time%20Calibration%20and%0A%20%20Fusion%20of%20UWB%20and%20SLAM%20Observations&entry.906535625=Tien-Dat%20Nguyen%20and%20Thien-Minh%20Nguyen%20and%20Vinh-Hao%20Nguyen&entry.1292438233=%20%20Onboard%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20methods%20are%20commonly%0Aused%20to%20provide%20accurate%20localization%20information%20for%20autonomous%20robots.%0AHowever%2C%20the%20coordinate%20origin%20of%20SLAM%20estimate%20often%20resets%20for%20each%20run.%20On%0Athe%20other%20hand%2C%20UWB-based%20localization%20with%20fixed%20anchors%20can%20ensure%20a%0Aconsistent%20coordinate%20reference%20across%20sessions%3B%20however%2C%20it%20requires%20an%0Aaccurate%20assignment%20of%20the%20anchor%20nodes%27%20coordinates.%20To%20this%20end%2C%20we%20propose%20a%0Atwo-stage%20approach%20that%20calibrates%20and%20fuses%20UWB%20data%20and%20SLAM%20data%20to%20achieve%0Acoordinate-wise%20consistent%20and%20accurate%20localization%20in%20the%20same%20environment.%0AIn%20the%20first%20stage%2C%20we%20solve%20a%20continuous-time%20batch%20optimization%20problem%20by%0Ausing%20the%20range%20and%20odometry%20data%20from%20one%20full%20run%2C%20incorporating%20height%0Apriors%20and%20anchor-to-anchor%20distance%20factors%20to%20recover%20the%20anchors%27%203D%0Apositions.%20For%20the%20subsequent%20runs%20in%20the%20second%20stage%2C%20a%20sliding-window%0Aoptimization%20scheme%20fuses%20the%20UWB%20and%20SLAM%20data%2C%20which%20facilitates%20accurate%0Alocalization%20in%20the%20same%20coordinate%20system.%20Experiments%20are%20carried%20out%20on%20the%0ANTU%20VIRAL%20dataset%20with%20six%20scenarios%20of%20UAV%20flight%2C%20and%20we%20show%20that%0Acalibration%20using%20data%20in%20one%20run%20is%20sufficient%20to%20enable%20accurate%20localization%0Ain%20the%20remaining%20runs.%20We%20release%20our%20source%20code%20to%20benefit%20the%20community%20at%0Ahttps%3A//github.com/ntdathp/slam-uwb-calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05992v1&entry.124074799=Read"},
{"title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs", "author": "Can Li and Ying Liu and Ting Zhang and Mei Wang and Hua Huang", "abstract": "  Large Multimodal Models have achieved remarkable progress in integrating\nvision and language, enabling strong performance across perception, reasoning,\nand domain-specific tasks. However, their capacity to reason over multiple,\nvisually similar inputs remains insufficiently explored. Such fine-grained\ncomparative reasoning is central to real-world tasks, especially in mathematics\nand education, where learners must often distinguish between nearly identical\ndiagrams to identify correct solutions. To address this gap, we present\nVisioMath, a curated benchmark of 1,800 high-quality K-12 mathematics problems\nin which all candidate answers are diagrams with subtle visual similarities. A\ncomprehensive evaluation of state-of-the-art LMMs, covering both leading\nclosed-source systems and widely adopted open-source models, reveals a\nconsistent decline in accuracy as inter-image similarity increases. Analysis\nindicates that the dominant failure mode stems from image-text misalignment:\nrather than grounding reasoning in textual cues, models often resort to shallow\npositional heuristics, resulting in systematic errors. We further explore three\nalignment-oriented strategies, spanning training-free approaches and\nfinetuning, and achieve substantial accuracy gains. We hope that VisioMath will\nserve as a rigorous benchmark and catalyst for developing LMMs toward deeper\ndiagram understanding, precise comparative reasoning, and grounded\nmulti-image-text integration.\n", "link": "http://arxiv.org/abs/2506.06727v3", "date": "2025-10-07", "relevancy": 2.8461, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5806}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisioMath%3A%20Benchmarking%20Figure-based%20Mathematical%20Reasoning%20in%20LMMs&body=Title%3A%20VisioMath%3A%20Benchmarking%20Figure-based%20Mathematical%20Reasoning%20in%20LMMs%0AAuthor%3A%20Can%20Li%20and%20Ying%20Liu%20and%20Ting%20Zhang%20and%20Mei%20Wang%20and%20Hua%20Huang%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20have%20achieved%20remarkable%20progress%20in%20integrating%0Avision%20and%20language%2C%20enabling%20strong%20performance%20across%20perception%2C%20reasoning%2C%0Aand%20domain-specific%20tasks.%20However%2C%20their%20capacity%20to%20reason%20over%20multiple%2C%0Avisually%20similar%20inputs%20remains%20insufficiently%20explored.%20Such%20fine-grained%0Acomparative%20reasoning%20is%20central%20to%20real-world%20tasks%2C%20especially%20in%20mathematics%0Aand%20education%2C%20where%20learners%20must%20often%20distinguish%20between%20nearly%20identical%0Adiagrams%20to%20identify%20correct%20solutions.%20To%20address%20this%20gap%2C%20we%20present%0AVisioMath%2C%20a%20curated%20benchmark%20of%201%2C800%20high-quality%20K-12%20mathematics%20problems%0Ain%20which%20all%20candidate%20answers%20are%20diagrams%20with%20subtle%20visual%20similarities.%20A%0Acomprehensive%20evaluation%20of%20state-of-the-art%20LMMs%2C%20covering%20both%20leading%0Aclosed-source%20systems%20and%20widely%20adopted%20open-source%20models%2C%20reveals%20a%0Aconsistent%20decline%20in%20accuracy%20as%20inter-image%20similarity%20increases.%20Analysis%0Aindicates%20that%20the%20dominant%20failure%20mode%20stems%20from%20image-text%20misalignment%3A%0Arather%20than%20grounding%20reasoning%20in%20textual%20cues%2C%20models%20often%20resort%20to%20shallow%0Apositional%20heuristics%2C%20resulting%20in%20systematic%20errors.%20We%20further%20explore%20three%0Aalignment-oriented%20strategies%2C%20spanning%20training-free%20approaches%20and%0Afinetuning%2C%20and%20achieve%20substantial%20accuracy%20gains.%20We%20hope%20that%20VisioMath%20will%0Aserve%20as%20a%20rigorous%20benchmark%20and%20catalyst%20for%20developing%20LMMs%20toward%20deeper%0Adiagram%20understanding%2C%20precise%20comparative%20reasoning%2C%20and%20grounded%0Amulti-image-text%20integration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06727v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisioMath%253A%2520Benchmarking%2520Figure-based%2520Mathematical%2520Reasoning%2520in%2520LMMs%26entry.906535625%3DCan%2520Li%2520and%2520Ying%2520Liu%2520and%2520Ting%2520Zhang%2520and%2520Mei%2520Wang%2520and%2520Hua%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520have%2520achieved%2520remarkable%2520progress%2520in%2520integrating%250Avision%2520and%2520language%252C%2520enabling%2520strong%2520performance%2520across%2520perception%252C%2520reasoning%252C%250Aand%2520domain-specific%2520tasks.%2520However%252C%2520their%2520capacity%2520to%2520reason%2520over%2520multiple%252C%250Avisually%2520similar%2520inputs%2520remains%2520insufficiently%2520explored.%2520Such%2520fine-grained%250Acomparative%2520reasoning%2520is%2520central%2520to%2520real-world%2520tasks%252C%2520especially%2520in%2520mathematics%250Aand%2520education%252C%2520where%2520learners%2520must%2520often%2520distinguish%2520between%2520nearly%2520identical%250Adiagrams%2520to%2520identify%2520correct%2520solutions.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%250AVisioMath%252C%2520a%2520curated%2520benchmark%2520of%25201%252C800%2520high-quality%2520K-12%2520mathematics%2520problems%250Ain%2520which%2520all%2520candidate%2520answers%2520are%2520diagrams%2520with%2520subtle%2520visual%2520similarities.%2520A%250Acomprehensive%2520evaluation%2520of%2520state-of-the-art%2520LMMs%252C%2520covering%2520both%2520leading%250Aclosed-source%2520systems%2520and%2520widely%2520adopted%2520open-source%2520models%252C%2520reveals%2520a%250Aconsistent%2520decline%2520in%2520accuracy%2520as%2520inter-image%2520similarity%2520increases.%2520Analysis%250Aindicates%2520that%2520the%2520dominant%2520failure%2520mode%2520stems%2520from%2520image-text%2520misalignment%253A%250Arather%2520than%2520grounding%2520reasoning%2520in%2520textual%2520cues%252C%2520models%2520often%2520resort%2520to%2520shallow%250Apositional%2520heuristics%252C%2520resulting%2520in%2520systematic%2520errors.%2520We%2520further%2520explore%2520three%250Aalignment-oriented%2520strategies%252C%2520spanning%2520training-free%2520approaches%2520and%250Afinetuning%252C%2520and%2520achieve%2520substantial%2520accuracy%2520gains.%2520We%2520hope%2520that%2520VisioMath%2520will%250Aserve%2520as%2520a%2520rigorous%2520benchmark%2520and%2520catalyst%2520for%2520developing%2520LMMs%2520toward%2520deeper%250Adiagram%2520understanding%252C%2520precise%2520comparative%2520reasoning%252C%2520and%2520grounded%250Amulti-image-text%2520integration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06727v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisioMath%3A%20Benchmarking%20Figure-based%20Mathematical%20Reasoning%20in%20LMMs&entry.906535625=Can%20Li%20and%20Ying%20Liu%20and%20Ting%20Zhang%20and%20Mei%20Wang%20and%20Hua%20Huang&entry.1292438233=%20%20Large%20Multimodal%20Models%20have%20achieved%20remarkable%20progress%20in%20integrating%0Avision%20and%20language%2C%20enabling%20strong%20performance%20across%20perception%2C%20reasoning%2C%0Aand%20domain-specific%20tasks.%20However%2C%20their%20capacity%20to%20reason%20over%20multiple%2C%0Avisually%20similar%20inputs%20remains%20insufficiently%20explored.%20Such%20fine-grained%0Acomparative%20reasoning%20is%20central%20to%20real-world%20tasks%2C%20especially%20in%20mathematics%0Aand%20education%2C%20where%20learners%20must%20often%20distinguish%20between%20nearly%20identical%0Adiagrams%20to%20identify%20correct%20solutions.%20To%20address%20this%20gap%2C%20we%20present%0AVisioMath%2C%20a%20curated%20benchmark%20of%201%2C800%20high-quality%20K-12%20mathematics%20problems%0Ain%20which%20all%20candidate%20answers%20are%20diagrams%20with%20subtle%20visual%20similarities.%20A%0Acomprehensive%20evaluation%20of%20state-of-the-art%20LMMs%2C%20covering%20both%20leading%0Aclosed-source%20systems%20and%20widely%20adopted%20open-source%20models%2C%20reveals%20a%0Aconsistent%20decline%20in%20accuracy%20as%20inter-image%20similarity%20increases.%20Analysis%0Aindicates%20that%20the%20dominant%20failure%20mode%20stems%20from%20image-text%20misalignment%3A%0Arather%20than%20grounding%20reasoning%20in%20textual%20cues%2C%20models%20often%20resort%20to%20shallow%0Apositional%20heuristics%2C%20resulting%20in%20systematic%20errors.%20We%20further%20explore%20three%0Aalignment-oriented%20strategies%2C%20spanning%20training-free%20approaches%20and%0Afinetuning%2C%20and%20achieve%20substantial%20accuracy%20gains.%20We%20hope%20that%20VisioMath%20will%0Aserve%20as%20a%20rigorous%20benchmark%20and%20catalyst%20for%20developing%20LMMs%20toward%20deeper%0Adiagram%20understanding%2C%20precise%20comparative%20reasoning%2C%20and%20grounded%0Amulti-image-text%20integration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06727v3&entry.124074799=Read"},
{"title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation", "author": "Han Li and Xinyu Peng and Yaoming Wang and Zelin Peng and Xin Chen and Rongxiang Weng and Jingang Wang and Xunliang Cai and Wenrui Dai and Hongkai Xiong", "abstract": "  We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.\n", "link": "http://arxiv.org/abs/2509.03498v3", "date": "2025-10-07", "relevancy": 2.8213, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.565}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneCAT%3A%20Decoder-Only%20Auto-Regressive%20Model%20for%20Unified%20Understanding%20and%0A%20%20Generation&body=Title%3A%20OneCAT%3A%20Decoder-Only%20Auto-Regressive%20Model%20for%20Unified%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Han%20Li%20and%20Xinyu%20Peng%20and%20Yaoming%20Wang%20and%20Zelin%20Peng%20and%20Xin%20Chen%20and%20Rongxiang%20Weng%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%20and%20Wenrui%20Dai%20and%20Hongkai%20Xiong%0AAbstract%3A%20%20%20We%20introduce%20OneCAT%2C%20a%20unified%20multimodal%20model%20that%20seamlessly%20integrates%0Aunderstanding%2C%20generation%2C%20and%20editing%20within%20a%20novel%2C%20pure%20decoder-only%0Atransformer%20architecture.%20Our%20framework%20uniquely%20eliminates%20the%20need%20for%0Aexternal%20components%20such%20as%20Vision%20Transformers%20%28ViT%29%20or%20vision%20tokenizer%0Aduring%20inference%2C%20leading%20to%20significant%20efficiency%20gains%2C%20especially%20for%0Ahigh-resolution%20inputs.%20This%20is%20achieved%20through%20a%20modality-specific%0AMixture-of-Experts%20%28MoE%29%20structure%20trained%20with%20a%20single%20autoregressive%20%28AR%29%0Aobjective%2C%20which%20also%20natively%20supports%20dynamic%20resolutions.%20Furthermore%2C%20we%0Apioneer%20a%20multi-scale%20visual%20autoregressive%20mechanism%20within%20the%20Large%20Language%0AModel%20%28LLM%29%20that%20drastically%20reduces%20decoding%20steps%20compared%20to%20diffusion-based%0Amethods%20while%20maintaining%20state-of-the-art%20performance.%20Our%20findings%0Ademonstrate%20the%20powerful%20potential%20of%20pure%20autoregressive%20modeling%20as%20a%0Asufficient%20and%20elegant%20foundation%20for%20unified%20multimodal%20intelligence.%20As%20a%0Aresult%2C%20OneCAT%20sets%20a%20new%20performance%20standard%2C%20outperforming%20existing%0Aopen-source%20unified%20multimodal%20models%20across%20benchmarks%20for%20multimodal%0Ageneration%2C%20editing%2C%20and%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03498v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneCAT%253A%2520Decoder-Only%2520Auto-Regressive%2520Model%2520for%2520Unified%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DHan%2520Li%2520and%2520Xinyu%2520Peng%2520and%2520Yaoming%2520Wang%2520and%2520Zelin%2520Peng%2520and%2520Xin%2520Chen%2520and%2520Rongxiang%2520Weng%2520and%2520Jingang%2520Wang%2520and%2520Xunliang%2520Cai%2520and%2520Wenrui%2520Dai%2520and%2520Hongkai%2520Xiong%26entry.1292438233%3D%2520%2520We%2520introduce%2520OneCAT%252C%2520a%2520unified%2520multimodal%2520model%2520that%2520seamlessly%2520integrates%250Aunderstanding%252C%2520generation%252C%2520and%2520editing%2520within%2520a%2520novel%252C%2520pure%2520decoder-only%250Atransformer%2520architecture.%2520Our%2520framework%2520uniquely%2520eliminates%2520the%2520need%2520for%250Aexternal%2520components%2520such%2520as%2520Vision%2520Transformers%2520%2528ViT%2529%2520or%2520vision%2520tokenizer%250Aduring%2520inference%252C%2520leading%2520to%2520significant%2520efficiency%2520gains%252C%2520especially%2520for%250Ahigh-resolution%2520inputs.%2520This%2520is%2520achieved%2520through%2520a%2520modality-specific%250AMixture-of-Experts%2520%2528MoE%2529%2520structure%2520trained%2520with%2520a%2520single%2520autoregressive%2520%2528AR%2529%250Aobjective%252C%2520which%2520also%2520natively%2520supports%2520dynamic%2520resolutions.%2520Furthermore%252C%2520we%250Apioneer%2520a%2520multi-scale%2520visual%2520autoregressive%2520mechanism%2520within%2520the%2520Large%2520Language%250AModel%2520%2528LLM%2529%2520that%2520drastically%2520reduces%2520decoding%2520steps%2520compared%2520to%2520diffusion-based%250Amethods%2520while%2520maintaining%2520state-of-the-art%2520performance.%2520Our%2520findings%250Ademonstrate%2520the%2520powerful%2520potential%2520of%2520pure%2520autoregressive%2520modeling%2520as%2520a%250Asufficient%2520and%2520elegant%2520foundation%2520for%2520unified%2520multimodal%2520intelligence.%2520As%2520a%250Aresult%252C%2520OneCAT%2520sets%2520a%2520new%2520performance%2520standard%252C%2520outperforming%2520existing%250Aopen-source%2520unified%2520multimodal%2520models%2520across%2520benchmarks%2520for%2520multimodal%250Ageneration%252C%2520editing%252C%2520and%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03498v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneCAT%3A%20Decoder-Only%20Auto-Regressive%20Model%20for%20Unified%20Understanding%20and%0A%20%20Generation&entry.906535625=Han%20Li%20and%20Xinyu%20Peng%20and%20Yaoming%20Wang%20and%20Zelin%20Peng%20and%20Xin%20Chen%20and%20Rongxiang%20Weng%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%20and%20Wenrui%20Dai%20and%20Hongkai%20Xiong&entry.1292438233=%20%20We%20introduce%20OneCAT%2C%20a%20unified%20multimodal%20model%20that%20seamlessly%20integrates%0Aunderstanding%2C%20generation%2C%20and%20editing%20within%20a%20novel%2C%20pure%20decoder-only%0Atransformer%20architecture.%20Our%20framework%20uniquely%20eliminates%20the%20need%20for%0Aexternal%20components%20such%20as%20Vision%20Transformers%20%28ViT%29%20or%20vision%20tokenizer%0Aduring%20inference%2C%20leading%20to%20significant%20efficiency%20gains%2C%20especially%20for%0Ahigh-resolution%20inputs.%20This%20is%20achieved%20through%20a%20modality-specific%0AMixture-of-Experts%20%28MoE%29%20structure%20trained%20with%20a%20single%20autoregressive%20%28AR%29%0Aobjective%2C%20which%20also%20natively%20supports%20dynamic%20resolutions.%20Furthermore%2C%20we%0Apioneer%20a%20multi-scale%20visual%20autoregressive%20mechanism%20within%20the%20Large%20Language%0AModel%20%28LLM%29%20that%20drastically%20reduces%20decoding%20steps%20compared%20to%20diffusion-based%0Amethods%20while%20maintaining%20state-of-the-art%20performance.%20Our%20findings%0Ademonstrate%20the%20powerful%20potential%20of%20pure%20autoregressive%20modeling%20as%20a%0Asufficient%20and%20elegant%20foundation%20for%20unified%20multimodal%20intelligence.%20As%20a%0Aresult%2C%20OneCAT%20sets%20a%20new%20performance%20standard%2C%20outperforming%20existing%0Aopen-source%20unified%20multimodal%20models%20across%20benchmarks%20for%20multimodal%0Ageneration%2C%20editing%2C%20and%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03498v3&entry.124074799=Read"},
{"title": "Continual Learning for Image Captioning through Improved Image-Text\n  Alignment", "author": "Bertram Taetz and Gal Bordelius", "abstract": "  Generating accurate and coherent image captions in a continual learning\nsetting remains a major challenge due to catastrophic forgetting and the\ndifficulty of aligning evolving visual concepts with language over time. In\nthis work, we propose a novel multi-loss framework for continual image\ncaptioning that integrates semantic guidance through prompt-based continual\nlearning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,\nour approach combines standard cross-entropy loss with three additional\ncomponents: (1) a prompt-based cosine similarity loss that aligns image\nembeddings with synthetically constructed prompts encoding objects, attributes,\nand actions; (2) a CLIP-style loss that promotes alignment between image\nembeddings and target caption embedding; and (3) a language-guided contrastive\nloss that employs a triplet loss to enhance class-level discriminability\nbetween tasks. Notably, our approach introduces no additional overhead at\ninference time and requires no prompts during caption generation. We find that\nthis approach mitigates catastrophic forgetting, while achieving better\nsemantic caption alignment compared to state-of-the-art methods. The code can\nbe found via the following link https://github.com/\nGepardius/Taetz_Bordelius_Continual_ImageCaptioning.\n", "link": "http://arxiv.org/abs/2510.06009v1", "date": "2025-10-07", "relevancy": 2.803, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5945}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5574}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20for%20Image%20Captioning%20through%20Improved%20Image-Text%0A%20%20Alignment&body=Title%3A%20Continual%20Learning%20for%20Image%20Captioning%20through%20Improved%20Image-Text%0A%20%20Alignment%0AAuthor%3A%20Bertram%20Taetz%20and%20Gal%20Bordelius%0AAbstract%3A%20%20%20Generating%20accurate%20and%20coherent%20image%20captions%20in%20a%20continual%20learning%0Asetting%20remains%20a%20major%20challenge%20due%20to%20catastrophic%20forgetting%20and%20the%0Adifficulty%20of%20aligning%20evolving%20visual%20concepts%20with%20language%20over%20time.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20multi-loss%20framework%20for%20continual%20image%0Acaptioning%20that%20integrates%20semantic%20guidance%20through%20prompt-based%20continual%0Alearning%20and%20contrastive%20alignment.%20Built%20upon%20a%20pretrained%20ViT-GPT-2%20backbone%2C%0Aour%20approach%20combines%20standard%20cross-entropy%20loss%20with%20three%20additional%0Acomponents%3A%20%281%29%20a%20prompt-based%20cosine%20similarity%20loss%20that%20aligns%20image%0Aembeddings%20with%20synthetically%20constructed%20prompts%20encoding%20objects%2C%20attributes%2C%0Aand%20actions%3B%20%282%29%20a%20CLIP-style%20loss%20that%20promotes%20alignment%20between%20image%0Aembeddings%20and%20target%20caption%20embedding%3B%20and%20%283%29%20a%20language-guided%20contrastive%0Aloss%20that%20employs%20a%20triplet%20loss%20to%20enhance%20class-level%20discriminability%0Abetween%20tasks.%20Notably%2C%20our%20approach%20introduces%20no%20additional%20overhead%20at%0Ainference%20time%20and%20requires%20no%20prompts%20during%20caption%20generation.%20We%20find%20that%0Athis%20approach%20mitigates%20catastrophic%20forgetting%2C%20while%20achieving%20better%0Asemantic%20caption%20alignment%20compared%20to%20state-of-the-art%20methods.%20The%20code%20can%0Abe%20found%20via%20the%20following%20link%20https%3A//github.com/%0AGepardius/Taetz_Bordelius_Continual_ImageCaptioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520for%2520Image%2520Captioning%2520through%2520Improved%2520Image-Text%250A%2520%2520Alignment%26entry.906535625%3DBertram%2520Taetz%2520and%2520Gal%2520Bordelius%26entry.1292438233%3D%2520%2520Generating%2520accurate%2520and%2520coherent%2520image%2520captions%2520in%2520a%2520continual%2520learning%250Asetting%2520remains%2520a%2520major%2520challenge%2520due%2520to%2520catastrophic%2520forgetting%2520and%2520the%250Adifficulty%2520of%2520aligning%2520evolving%2520visual%2520concepts%2520with%2520language%2520over%2520time.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520novel%2520multi-loss%2520framework%2520for%2520continual%2520image%250Acaptioning%2520that%2520integrates%2520semantic%2520guidance%2520through%2520prompt-based%2520continual%250Alearning%2520and%2520contrastive%2520alignment.%2520Built%2520upon%2520a%2520pretrained%2520ViT-GPT-2%2520backbone%252C%250Aour%2520approach%2520combines%2520standard%2520cross-entropy%2520loss%2520with%2520three%2520additional%250Acomponents%253A%2520%25281%2529%2520a%2520prompt-based%2520cosine%2520similarity%2520loss%2520that%2520aligns%2520image%250Aembeddings%2520with%2520synthetically%2520constructed%2520prompts%2520encoding%2520objects%252C%2520attributes%252C%250Aand%2520actions%253B%2520%25282%2529%2520a%2520CLIP-style%2520loss%2520that%2520promotes%2520alignment%2520between%2520image%250Aembeddings%2520and%2520target%2520caption%2520embedding%253B%2520and%2520%25283%2529%2520a%2520language-guided%2520contrastive%250Aloss%2520that%2520employs%2520a%2520triplet%2520loss%2520to%2520enhance%2520class-level%2520discriminability%250Abetween%2520tasks.%2520Notably%252C%2520our%2520approach%2520introduces%2520no%2520additional%2520overhead%2520at%250Ainference%2520time%2520and%2520requires%2520no%2520prompts%2520during%2520caption%2520generation.%2520We%2520find%2520that%250Athis%2520approach%2520mitigates%2520catastrophic%2520forgetting%252C%2520while%2520achieving%2520better%250Asemantic%2520caption%2520alignment%2520compared%2520to%2520state-of-the-art%2520methods.%2520The%2520code%2520can%250Abe%2520found%2520via%2520the%2520following%2520link%2520https%253A//github.com/%250AGepardius/Taetz_Bordelius_Continual_ImageCaptioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20for%20Image%20Captioning%20through%20Improved%20Image-Text%0A%20%20Alignment&entry.906535625=Bertram%20Taetz%20and%20Gal%20Bordelius&entry.1292438233=%20%20Generating%20accurate%20and%20coherent%20image%20captions%20in%20a%20continual%20learning%0Asetting%20remains%20a%20major%20challenge%20due%20to%20catastrophic%20forgetting%20and%20the%0Adifficulty%20of%20aligning%20evolving%20visual%20concepts%20with%20language%20over%20time.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20multi-loss%20framework%20for%20continual%20image%0Acaptioning%20that%20integrates%20semantic%20guidance%20through%20prompt-based%20continual%0Alearning%20and%20contrastive%20alignment.%20Built%20upon%20a%20pretrained%20ViT-GPT-2%20backbone%2C%0Aour%20approach%20combines%20standard%20cross-entropy%20loss%20with%20three%20additional%0Acomponents%3A%20%281%29%20a%20prompt-based%20cosine%20similarity%20loss%20that%20aligns%20image%0Aembeddings%20with%20synthetically%20constructed%20prompts%20encoding%20objects%2C%20attributes%2C%0Aand%20actions%3B%20%282%29%20a%20CLIP-style%20loss%20that%20promotes%20alignment%20between%20image%0Aembeddings%20and%20target%20caption%20embedding%3B%20and%20%283%29%20a%20language-guided%20contrastive%0Aloss%20that%20employs%20a%20triplet%20loss%20to%20enhance%20class-level%20discriminability%0Abetween%20tasks.%20Notably%2C%20our%20approach%20introduces%20no%20additional%20overhead%20at%0Ainference%20time%20and%20requires%20no%20prompts%20during%20caption%20generation.%20We%20find%20that%0Athis%20approach%20mitigates%20catastrophic%20forgetting%2C%20while%20achieving%20better%0Asemantic%20caption%20alignment%20compared%20to%20state-of-the-art%20methods.%20The%20code%20can%0Abe%20found%20via%20the%20following%20link%20https%3A//github.com/%0AGepardius/Taetz_Bordelius_Continual_ImageCaptioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06009v1&entry.124074799=Read"},
{"title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via\n  Tree-based Group Relative Policy Optimization", "author": "Xinye Cao and Hongcan Guo and Jiawen Qian and Guoshun Nan and Chao Wang and Yuqi Pan and Tianhao Hou and Xiaojuan Wang and Yutong Gao", "abstract": "  Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner.\n", "link": "http://arxiv.org/abs/2510.06040v1", "date": "2025-10-07", "relevancy": 2.8013, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoMiner%3A%20Iteratively%20Grounding%20Key%20Frames%20of%20Hour-Long%20Videos%20via%0A%20%20Tree-based%20Group%20Relative%20Policy%20Optimization&body=Title%3A%20VideoMiner%3A%20Iteratively%20Grounding%20Key%20Frames%20of%20Hour-Long%20Videos%20via%0A%20%20Tree-based%20Group%20Relative%20Policy%20Optimization%0AAuthor%3A%20Xinye%20Cao%20and%20Hongcan%20Guo%20and%20Jiawen%20Qian%20and%20Guoshun%20Nan%20and%20Chao%20Wang%20and%20Yuqi%20Pan%20and%20Tianhao%20Hou%20and%20Xiaojuan%20Wang%20and%20Yutong%20Gao%0AAbstract%3A%20%20%20Understanding%20hour-long%20videos%20with%20multi-modal%20large%20language%20models%0A%28MM-LLMs%29%20enriches%20the%20landscape%20of%20human-centered%20AI%20applications.%20However%2C%0Afor%20end-to-end%20video%20understanding%20with%20LLMs%2C%20uniformly%20sampling%20video%20frames%0Aresults%20in%20LLMs%20being%20overwhelmed%20by%20a%20vast%20amount%20of%20irrelevant%20information%20as%0Avideo%20length%20increases.%20Existing%20hierarchical%20key%20frame%20extraction%20methods%0Aimprove%20the%20accuracy%20of%20video%20understanding%20but%20still%20face%20two%20critical%0Achallenges.%201%29%20How%20can%20the%20interference%20of%20extensive%20redundant%20information%20in%0Along%20videos%20be%20mitigated%3F%202%29%20How%20can%20a%20model%20dynamically%20adapt%20to%20complex%0Ahierarchical%20structures%20while%20accurately%20identifying%20key%20frames%3F%20To%20address%0Athese%20issues%2C%20we%20propose%20VideoMiner%2C%20which%20iteratively%20segments%2C%20captions%2C%20and%0Aclusters%20long%20videos%2C%20forming%20a%20hierarchical%20tree%20structure.%20The%20proposed%0AVideoMiner%20progresses%20from%20long%20videos%20to%20events%20to%20frames%20while%20preserving%0Atemporal%20coherence%2C%20effectively%20addressing%20the%20first%20challenge.%20To%20precisely%0Alocate%20key%20frames%2C%20we%20introduce%20T-GRPO%2C%20a%20tree-based%20group%20relative%20policy%0Aoptimization%20in%20reinforcement%20learning%20method%20that%20guides%20the%20exploration%20of%0Athe%20VideoMiner.%20The%20proposed%20T-GRPO%20is%20specifically%20designed%20for%20tree%0Astructures%2C%20integrating%20spatiotemporal%20information%20at%20the%20event%20level%20while%0Abeing%20guided%20by%20the%20question%2C%20thus%20solving%20the%20second%20challenge.%20We%20achieve%0Asuperior%20performance%20in%20all%20long-video%20understanding%20tasks%20and%20uncover%20several%0Ainteresting%20insights.%20Our%20proposed%20T-GRPO%20surprisingly%20incentivizes%20the%20model%0Ato%20spontaneously%20generate%20a%20reasoning%20chain.%20Additionally%2C%20the%20designed%20tree%0Agrowth%20auxin%20dynamically%20adjusts%20the%20expansion%20depth%2C%20obtaining%20accuracy%20and%0Aefficiency%20gains.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/caoxinye/VideoMiner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoMiner%253A%2520Iteratively%2520Grounding%2520Key%2520Frames%2520of%2520Hour-Long%2520Videos%2520via%250A%2520%2520Tree-based%2520Group%2520Relative%2520Policy%2520Optimization%26entry.906535625%3DXinye%2520Cao%2520and%2520Hongcan%2520Guo%2520and%2520Jiawen%2520Qian%2520and%2520Guoshun%2520Nan%2520and%2520Chao%2520Wang%2520and%2520Yuqi%2520Pan%2520and%2520Tianhao%2520Hou%2520and%2520Xiaojuan%2520Wang%2520and%2520Yutong%2520Gao%26entry.1292438233%3D%2520%2520Understanding%2520hour-long%2520videos%2520with%2520multi-modal%2520large%2520language%2520models%250A%2528MM-LLMs%2529%2520enriches%2520the%2520landscape%2520of%2520human-centered%2520AI%2520applications.%2520However%252C%250Afor%2520end-to-end%2520video%2520understanding%2520with%2520LLMs%252C%2520uniformly%2520sampling%2520video%2520frames%250Aresults%2520in%2520LLMs%2520being%2520overwhelmed%2520by%2520a%2520vast%2520amount%2520of%2520irrelevant%2520information%2520as%250Avideo%2520length%2520increases.%2520Existing%2520hierarchical%2520key%2520frame%2520extraction%2520methods%250Aimprove%2520the%2520accuracy%2520of%2520video%2520understanding%2520but%2520still%2520face%2520two%2520critical%250Achallenges.%25201%2529%2520How%2520can%2520the%2520interference%2520of%2520extensive%2520redundant%2520information%2520in%250Along%2520videos%2520be%2520mitigated%253F%25202%2529%2520How%2520can%2520a%2520model%2520dynamically%2520adapt%2520to%2520complex%250Ahierarchical%2520structures%2520while%2520accurately%2520identifying%2520key%2520frames%253F%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520VideoMiner%252C%2520which%2520iteratively%2520segments%252C%2520captions%252C%2520and%250Aclusters%2520long%2520videos%252C%2520forming%2520a%2520hierarchical%2520tree%2520structure.%2520The%2520proposed%250AVideoMiner%2520progresses%2520from%2520long%2520videos%2520to%2520events%2520to%2520frames%2520while%2520preserving%250Atemporal%2520coherence%252C%2520effectively%2520addressing%2520the%2520first%2520challenge.%2520To%2520precisely%250Alocate%2520key%2520frames%252C%2520we%2520introduce%2520T-GRPO%252C%2520a%2520tree-based%2520group%2520relative%2520policy%250Aoptimization%2520in%2520reinforcement%2520learning%2520method%2520that%2520guides%2520the%2520exploration%2520of%250Athe%2520VideoMiner.%2520The%2520proposed%2520T-GRPO%2520is%2520specifically%2520designed%2520for%2520tree%250Astructures%252C%2520integrating%2520spatiotemporal%2520information%2520at%2520the%2520event%2520level%2520while%250Abeing%2520guided%2520by%2520the%2520question%252C%2520thus%2520solving%2520the%2520second%2520challenge.%2520We%2520achieve%250Asuperior%2520performance%2520in%2520all%2520long-video%2520understanding%2520tasks%2520and%2520uncover%2520several%250Ainteresting%2520insights.%2520Our%2520proposed%2520T-GRPO%2520surprisingly%2520incentivizes%2520the%2520model%250Ato%2520spontaneously%2520generate%2520a%2520reasoning%2520chain.%2520Additionally%252C%2520the%2520designed%2520tree%250Agrowth%2520auxin%2520dynamically%2520adjusts%2520the%2520expansion%2520depth%252C%2520obtaining%2520accuracy%2520and%250Aefficiency%2520gains.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/caoxinye/VideoMiner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoMiner%3A%20Iteratively%20Grounding%20Key%20Frames%20of%20Hour-Long%20Videos%20via%0A%20%20Tree-based%20Group%20Relative%20Policy%20Optimization&entry.906535625=Xinye%20Cao%20and%20Hongcan%20Guo%20and%20Jiawen%20Qian%20and%20Guoshun%20Nan%20and%20Chao%20Wang%20and%20Yuqi%20Pan%20and%20Tianhao%20Hou%20and%20Xiaojuan%20Wang%20and%20Yutong%20Gao&entry.1292438233=%20%20Understanding%20hour-long%20videos%20with%20multi-modal%20large%20language%20models%0A%28MM-LLMs%29%20enriches%20the%20landscape%20of%20human-centered%20AI%20applications.%20However%2C%0Afor%20end-to-end%20video%20understanding%20with%20LLMs%2C%20uniformly%20sampling%20video%20frames%0Aresults%20in%20LLMs%20being%20overwhelmed%20by%20a%20vast%20amount%20of%20irrelevant%20information%20as%0Avideo%20length%20increases.%20Existing%20hierarchical%20key%20frame%20extraction%20methods%0Aimprove%20the%20accuracy%20of%20video%20understanding%20but%20still%20face%20two%20critical%0Achallenges.%201%29%20How%20can%20the%20interference%20of%20extensive%20redundant%20information%20in%0Along%20videos%20be%20mitigated%3F%202%29%20How%20can%20a%20model%20dynamically%20adapt%20to%20complex%0Ahierarchical%20structures%20while%20accurately%20identifying%20key%20frames%3F%20To%20address%0Athese%20issues%2C%20we%20propose%20VideoMiner%2C%20which%20iteratively%20segments%2C%20captions%2C%20and%0Aclusters%20long%20videos%2C%20forming%20a%20hierarchical%20tree%20structure.%20The%20proposed%0AVideoMiner%20progresses%20from%20long%20videos%20to%20events%20to%20frames%20while%20preserving%0Atemporal%20coherence%2C%20effectively%20addressing%20the%20first%20challenge.%20To%20precisely%0Alocate%20key%20frames%2C%20we%20introduce%20T-GRPO%2C%20a%20tree-based%20group%20relative%20policy%0Aoptimization%20in%20reinforcement%20learning%20method%20that%20guides%20the%20exploration%20of%0Athe%20VideoMiner.%20The%20proposed%20T-GRPO%20is%20specifically%20designed%20for%20tree%0Astructures%2C%20integrating%20spatiotemporal%20information%20at%20the%20event%20level%20while%0Abeing%20guided%20by%20the%20question%2C%20thus%20solving%20the%20second%20challenge.%20We%20achieve%0Asuperior%20performance%20in%20all%20long-video%20understanding%20tasks%20and%20uncover%20several%0Ainteresting%20insights.%20Our%20proposed%20T-GRPO%20surprisingly%20incentivizes%20the%20model%0Ato%20spontaneously%20generate%20a%20reasoning%20chain.%20Additionally%2C%20the%20designed%20tree%0Agrowth%20auxin%20dynamically%20adjusts%20the%20expansion%20depth%2C%20obtaining%20accuracy%20and%0Aefficiency%20gains.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/caoxinye/VideoMiner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06040v1&entry.124074799=Read"},
{"title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning", "author": "Mi Luo and Zihui Xue and Alex Dimakis and Kristen Grauman", "abstract": "  Video reasoning, the task of enabling machines to infer from dynamic visual\ncontent through multi-step logic, is crucial for advanced AI. While the\nChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,\nits application to video understanding remains underexplored. This paper\npresents a systematic analysis revealing that CoT often degrades performance in\nvideo reasoning, generating verbose but misleading internal monologues, and\nleading to hallucinated visual details and overridden correct intuitions - a\nphenomenon we term \"visual thinking drift\". We explain this drift through a\nBayesian lens, positing that CoT traces often diverge from actual visual\nevidence, instead amplifying internal biases or language priors, causing models\nto storytell rather than engage in grounded reasoning. To counteract this, we\nintroduce Visual Evidence Reward (VER), a novel reinforcement learning\nframework that explicitly rewards the generation of reasoning traces that are\nverifiably grounded in visual evidence. Comprehensive evaluation across 10\ndiverse video understanding benchmarks demonstrates that our Video-VER\nconsistently achieves top performance. Our work sheds light on the distinct\nchallenges of video-centric reasoning and encourages the development of AI that\nrobustly grounds its inferences in visual evidence - for large multimodal\nmodels that not only \"think before answering\", but also \"see while thinking\".\n", "link": "http://arxiv.org/abs/2510.06077v1", "date": "2025-10-07", "relevancy": 2.7938, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Thinking%20Drifts%3A%20Evidential%20Grounding%20for%20Robust%20Video%20Reasoning&body=Title%3A%20When%20Thinking%20Drifts%3A%20Evidential%20Grounding%20for%20Robust%20Video%20Reasoning%0AAuthor%3A%20Mi%20Luo%20and%20Zihui%20Xue%20and%20Alex%20Dimakis%20and%20Kristen%20Grauman%0AAbstract%3A%20%20%20Video%20reasoning%2C%20the%20task%20of%20enabling%20machines%20to%20infer%20from%20dynamic%20visual%0Acontent%20through%20multi-step%20logic%2C%20is%20crucial%20for%20advanced%20AI.%20While%20the%0AChain-of-Thought%20%28CoT%29%20mechanism%20has%20enhanced%20reasoning%20in%20text-based%20tasks%2C%0Aits%20application%20to%20video%20understanding%20remains%20underexplored.%20This%20paper%0Apresents%20a%20systematic%20analysis%20revealing%20that%20CoT%20often%20degrades%20performance%20in%0Avideo%20reasoning%2C%20generating%20verbose%20but%20misleading%20internal%20monologues%2C%20and%0Aleading%20to%20hallucinated%20visual%20details%20and%20overridden%20correct%20intuitions%20-%20a%0Aphenomenon%20we%20term%20%22visual%20thinking%20drift%22.%20We%20explain%20this%20drift%20through%20a%0ABayesian%20lens%2C%20positing%20that%20CoT%20traces%20often%20diverge%20from%20actual%20visual%0Aevidence%2C%20instead%20amplifying%20internal%20biases%20or%20language%20priors%2C%20causing%20models%0Ato%20storytell%20rather%20than%20engage%20in%20grounded%20reasoning.%20To%20counteract%20this%2C%20we%0Aintroduce%20Visual%20Evidence%20Reward%20%28VER%29%2C%20a%20novel%20reinforcement%20learning%0Aframework%20that%20explicitly%20rewards%20the%20generation%20of%20reasoning%20traces%20that%20are%0Averifiably%20grounded%20in%20visual%20evidence.%20Comprehensive%20evaluation%20across%2010%0Adiverse%20video%20understanding%20benchmarks%20demonstrates%20that%20our%20Video-VER%0Aconsistently%20achieves%20top%20performance.%20Our%20work%20sheds%20light%20on%20the%20distinct%0Achallenges%20of%20video-centric%20reasoning%20and%20encourages%20the%20development%20of%20AI%20that%0Arobustly%20grounds%20its%20inferences%20in%20visual%20evidence%20-%20for%20large%20multimodal%0Amodels%20that%20not%20only%20%22think%20before%20answering%22%2C%20but%20also%20%22see%20while%20thinking%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Thinking%2520Drifts%253A%2520Evidential%2520Grounding%2520for%2520Robust%2520Video%2520Reasoning%26entry.906535625%3DMi%2520Luo%2520and%2520Zihui%2520Xue%2520and%2520Alex%2520Dimakis%2520and%2520Kristen%2520Grauman%26entry.1292438233%3D%2520%2520Video%2520reasoning%252C%2520the%2520task%2520of%2520enabling%2520machines%2520to%2520infer%2520from%2520dynamic%2520visual%250Acontent%2520through%2520multi-step%2520logic%252C%2520is%2520crucial%2520for%2520advanced%2520AI.%2520While%2520the%250AChain-of-Thought%2520%2528CoT%2529%2520mechanism%2520has%2520enhanced%2520reasoning%2520in%2520text-based%2520tasks%252C%250Aits%2520application%2520to%2520video%2520understanding%2520remains%2520underexplored.%2520This%2520paper%250Apresents%2520a%2520systematic%2520analysis%2520revealing%2520that%2520CoT%2520often%2520degrades%2520performance%2520in%250Avideo%2520reasoning%252C%2520generating%2520verbose%2520but%2520misleading%2520internal%2520monologues%252C%2520and%250Aleading%2520to%2520hallucinated%2520visual%2520details%2520and%2520overridden%2520correct%2520intuitions%2520-%2520a%250Aphenomenon%2520we%2520term%2520%2522visual%2520thinking%2520drift%2522.%2520We%2520explain%2520this%2520drift%2520through%2520a%250ABayesian%2520lens%252C%2520positing%2520that%2520CoT%2520traces%2520often%2520diverge%2520from%2520actual%2520visual%250Aevidence%252C%2520instead%2520amplifying%2520internal%2520biases%2520or%2520language%2520priors%252C%2520causing%2520models%250Ato%2520storytell%2520rather%2520than%2520engage%2520in%2520grounded%2520reasoning.%2520To%2520counteract%2520this%252C%2520we%250Aintroduce%2520Visual%2520Evidence%2520Reward%2520%2528VER%2529%252C%2520a%2520novel%2520reinforcement%2520learning%250Aframework%2520that%2520explicitly%2520rewards%2520the%2520generation%2520of%2520reasoning%2520traces%2520that%2520are%250Averifiably%2520grounded%2520in%2520visual%2520evidence.%2520Comprehensive%2520evaluation%2520across%252010%250Adiverse%2520video%2520understanding%2520benchmarks%2520demonstrates%2520that%2520our%2520Video-VER%250Aconsistently%2520achieves%2520top%2520performance.%2520Our%2520work%2520sheds%2520light%2520on%2520the%2520distinct%250Achallenges%2520of%2520video-centric%2520reasoning%2520and%2520encourages%2520the%2520development%2520of%2520AI%2520that%250Arobustly%2520grounds%2520its%2520inferences%2520in%2520visual%2520evidence%2520-%2520for%2520large%2520multimodal%250Amodels%2520that%2520not%2520only%2520%2522think%2520before%2520answering%2522%252C%2520but%2520also%2520%2522see%2520while%2520thinking%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Thinking%20Drifts%3A%20Evidential%20Grounding%20for%20Robust%20Video%20Reasoning&entry.906535625=Mi%20Luo%20and%20Zihui%20Xue%20and%20Alex%20Dimakis%20and%20Kristen%20Grauman&entry.1292438233=%20%20Video%20reasoning%2C%20the%20task%20of%20enabling%20machines%20to%20infer%20from%20dynamic%20visual%0Acontent%20through%20multi-step%20logic%2C%20is%20crucial%20for%20advanced%20AI.%20While%20the%0AChain-of-Thought%20%28CoT%29%20mechanism%20has%20enhanced%20reasoning%20in%20text-based%20tasks%2C%0Aits%20application%20to%20video%20understanding%20remains%20underexplored.%20This%20paper%0Apresents%20a%20systematic%20analysis%20revealing%20that%20CoT%20often%20degrades%20performance%20in%0Avideo%20reasoning%2C%20generating%20verbose%20but%20misleading%20internal%20monologues%2C%20and%0Aleading%20to%20hallucinated%20visual%20details%20and%20overridden%20correct%20intuitions%20-%20a%0Aphenomenon%20we%20term%20%22visual%20thinking%20drift%22.%20We%20explain%20this%20drift%20through%20a%0ABayesian%20lens%2C%20positing%20that%20CoT%20traces%20often%20diverge%20from%20actual%20visual%0Aevidence%2C%20instead%20amplifying%20internal%20biases%20or%20language%20priors%2C%20causing%20models%0Ato%20storytell%20rather%20than%20engage%20in%20grounded%20reasoning.%20To%20counteract%20this%2C%20we%0Aintroduce%20Visual%20Evidence%20Reward%20%28VER%29%2C%20a%20novel%20reinforcement%20learning%0Aframework%20that%20explicitly%20rewards%20the%20generation%20of%20reasoning%20traces%20that%20are%0Averifiably%20grounded%20in%20visual%20evidence.%20Comprehensive%20evaluation%20across%2010%0Adiverse%20video%20understanding%20benchmarks%20demonstrates%20that%20our%20Video-VER%0Aconsistently%20achieves%20top%20performance.%20Our%20work%20sheds%20light%20on%20the%20distinct%0Achallenges%20of%20video-centric%20reasoning%20and%20encourages%20the%20development%20of%20AI%20that%0Arobustly%20grounds%20its%20inferences%20in%20visual%20evidence%20-%20for%20large%20multimodal%0Amodels%20that%20not%20only%20%22think%20before%20answering%22%2C%20but%20also%20%22see%20while%20thinking%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06077v1&entry.124074799=Read"},
{"title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive\n  Architectures", "author": "Hai Huang and Yann LeCun and Randall Balestriero", "abstract": "  Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa.\n", "link": "http://arxiv.org/abs/2509.14252v2", "date": "2025-10-07", "relevancy": 2.7875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-JEPA%3A%20Large%20Language%20Models%20Meet%20Joint%20Embedding%20Predictive%0A%20%20Architectures&body=Title%3A%20LLM-JEPA%3A%20Large%20Language%20Models%20Meet%20Joint%20Embedding%20Predictive%0A%20%20Architectures%0AAuthor%3A%20Hai%20Huang%20and%20Yann%20LeCun%20and%20Randall%20Balestriero%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20pretraining%2C%20finetuning%2C%20and%20evaluation%20rely%20on%0Ainput-space%20reconstruction%20and%20generative%20capabilities.%20Yet%2C%20it%20has%20been%0Aobserved%20in%20vision%20that%20embedding-space%20training%20objectives%2C%20e.g.%2C%20with%20Joint%0AEmbedding%20Predictive%20Architectures%20%28JEPAs%29%2C%20are%20far%20superior%20to%20their%0Ainput-space%20counterpart.%20That%20mismatch%20in%20how%20training%20is%20achieved%20between%0Alanguage%20and%20vision%20opens%20up%20a%20natural%20question%3A%20%7B%5Cem%20can%20language%20training%0Amethods%20learn%20a%20few%20tricks%20from%20the%20vision%20ones%3F%7D%20The%20lack%20of%20JEPA-style%20LLM%20is%0Aa%20testimony%20of%20the%20challenge%20in%20designing%20such%20objectives%20for%20language.%20In%20this%0Awork%2C%20we%20propose%20a%20first%20step%20in%20that%20direction%20where%20we%20develop%20LLM-JEPA%2C%20a%0AJEPA%20based%20solution%20for%20LLMs%20applicable%20both%20to%20finetuning%20and%20pretraining.%0AThus%20far%2C%20LLM-JEPA%20is%20able%20to%20outperform%20the%20standard%20LLM%20training%20objectives%0Aby%20a%20significant%20margin%20across%20models%2C%20all%20while%20being%20robust%20to%20overfiting.%0AThose%20findings%20are%20observed%20across%20numerous%20datasets%20%28NL-RX%2C%20GSM8K%2C%20Spider%2C%0ARottenTomatoes%29%20and%20various%20models%20from%20the%20Llama3%2C%20OpenELM%2C%20Gemma2%20and%20Olmo%0Afamilies.%20Code%3A%20https%3A//github.com/rbalestr-lab/llm-jepa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-JEPA%253A%2520Large%2520Language%2520Models%2520Meet%2520Joint%2520Embedding%2520Predictive%250A%2520%2520Architectures%26entry.906535625%3DHai%2520Huang%2520and%2520Yann%2520LeCun%2520and%2520Randall%2520Balestriero%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520pretraining%252C%2520finetuning%252C%2520and%2520evaluation%2520rely%2520on%250Ainput-space%2520reconstruction%2520and%2520generative%2520capabilities.%2520Yet%252C%2520it%2520has%2520been%250Aobserved%2520in%2520vision%2520that%2520embedding-space%2520training%2520objectives%252C%2520e.g.%252C%2520with%2520Joint%250AEmbedding%2520Predictive%2520Architectures%2520%2528JEPAs%2529%252C%2520are%2520far%2520superior%2520to%2520their%250Ainput-space%2520counterpart.%2520That%2520mismatch%2520in%2520how%2520training%2520is%2520achieved%2520between%250Alanguage%2520and%2520vision%2520opens%2520up%2520a%2520natural%2520question%253A%2520%257B%255Cem%2520can%2520language%2520training%250Amethods%2520learn%2520a%2520few%2520tricks%2520from%2520the%2520vision%2520ones%253F%257D%2520The%2520lack%2520of%2520JEPA-style%2520LLM%2520is%250Aa%2520testimony%2520of%2520the%2520challenge%2520in%2520designing%2520such%2520objectives%2520for%2520language.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520first%2520step%2520in%2520that%2520direction%2520where%2520we%2520develop%2520LLM-JEPA%252C%2520a%250AJEPA%2520based%2520solution%2520for%2520LLMs%2520applicable%2520both%2520to%2520finetuning%2520and%2520pretraining.%250AThus%2520far%252C%2520LLM-JEPA%2520is%2520able%2520to%2520outperform%2520the%2520standard%2520LLM%2520training%2520objectives%250Aby%2520a%2520significant%2520margin%2520across%2520models%252C%2520all%2520while%2520being%2520robust%2520to%2520overfiting.%250AThose%2520findings%2520are%2520observed%2520across%2520numerous%2520datasets%2520%2528NL-RX%252C%2520GSM8K%252C%2520Spider%252C%250ARottenTomatoes%2529%2520and%2520various%2520models%2520from%2520the%2520Llama3%252C%2520OpenELM%252C%2520Gemma2%2520and%2520Olmo%250Afamilies.%2520Code%253A%2520https%253A//github.com/rbalestr-lab/llm-jepa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-JEPA%3A%20Large%20Language%20Models%20Meet%20Joint%20Embedding%20Predictive%0A%20%20Architectures&entry.906535625=Hai%20Huang%20and%20Yann%20LeCun%20and%20Randall%20Balestriero&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20pretraining%2C%20finetuning%2C%20and%20evaluation%20rely%20on%0Ainput-space%20reconstruction%20and%20generative%20capabilities.%20Yet%2C%20it%20has%20been%0Aobserved%20in%20vision%20that%20embedding-space%20training%20objectives%2C%20e.g.%2C%20with%20Joint%0AEmbedding%20Predictive%20Architectures%20%28JEPAs%29%2C%20are%20far%20superior%20to%20their%0Ainput-space%20counterpart.%20That%20mismatch%20in%20how%20training%20is%20achieved%20between%0Alanguage%20and%20vision%20opens%20up%20a%20natural%20question%3A%20%7B%5Cem%20can%20language%20training%0Amethods%20learn%20a%20few%20tricks%20from%20the%20vision%20ones%3F%7D%20The%20lack%20of%20JEPA-style%20LLM%20is%0Aa%20testimony%20of%20the%20challenge%20in%20designing%20such%20objectives%20for%20language.%20In%20this%0Awork%2C%20we%20propose%20a%20first%20step%20in%20that%20direction%20where%20we%20develop%20LLM-JEPA%2C%20a%0AJEPA%20based%20solution%20for%20LLMs%20applicable%20both%20to%20finetuning%20and%20pretraining.%0AThus%20far%2C%20LLM-JEPA%20is%20able%20to%20outperform%20the%20standard%20LLM%20training%20objectives%0Aby%20a%20significant%20margin%20across%20models%2C%20all%20while%20being%20robust%20to%20overfiting.%0AThose%20findings%20are%20observed%20across%20numerous%20datasets%20%28NL-RX%2C%20GSM8K%2C%20Spider%2C%0ARottenTomatoes%29%20and%20various%20models%20from%20the%20Llama3%2C%20OpenELM%2C%20Gemma2%20and%20Olmo%0Afamilies.%20Code%3A%20https%3A//github.com/rbalestr-lab/llm-jepa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14252v2&entry.124074799=Read"},
{"title": "Generative Interfaces for Language Models", "author": "Jiaqi Chen and Yanzhe Zhang and Yutong Zhang and Yijia Shao and Diyi Yang", "abstract": "  Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with up to a 72% improvement in\nhuman preference. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.\n", "link": "http://arxiv.org/abs/2508.19227v2", "date": "2025-10-07", "relevancy": 2.7718, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6114}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5294}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Interfaces%20for%20Language%20Models&body=Title%3A%20Generative%20Interfaces%20for%20Language%20Models%0AAuthor%3A%20Jiaqi%20Chen%20and%20Yanzhe%20Zhang%20and%20Yutong%20Zhang%20and%20Yijia%20Shao%20and%20Diyi%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20seen%20as%20assistants%2C%20copilots%2C%0Aand%20consultants%2C%20capable%20of%20supporting%20a%20wide%20range%20of%20tasks%20through%20natural%0Aconversation.%20However%2C%20most%20systems%20remain%20constrained%20by%20a%20linear%0Arequest-response%20format%20that%20often%20makes%20interactions%20inefficient%20in%0Amulti-turn%2C%20information-dense%2C%20and%20exploratory%20tasks.%20To%20address%20these%0Alimitations%2C%20we%20propose%20Generative%20Interfaces%20for%20Language%20Models%2C%20a%20paradigm%0Ain%20which%20LLMs%20respond%20to%20user%20queries%20by%20proactively%20generating%20user%20interfaces%0A%28UIs%29%20that%20enable%20more%20adaptive%20and%20interactive%20engagement.%20Our%20framework%0Aleverages%20structured%20interface-specific%20representations%20and%20iterative%0Arefinements%20to%20translate%20user%20queries%20into%20task-specific%20UIs.%20For%20systematic%0Aevaluation%2C%20we%20introduce%20a%20multidimensional%20assessment%20framework%20that%20compares%0Agenerative%20interfaces%20with%20traditional%20chat-based%20ones%20across%20diverse%20tasks%2C%0Ainteraction%20patterns%2C%20and%20query%20types%2C%20capturing%20functional%2C%20interactive%2C%20and%0Aemotional%20aspects%20of%20user%20experience.%20Results%20show%20that%20generative%20interfaces%0Aconsistently%20outperform%20conversational%20ones%2C%20with%20up%20to%20a%2072%25%20improvement%20in%0Ahuman%20preference.%20These%20findings%20clarify%20when%20and%20why%20users%20favor%20generative%0Ainterfaces%2C%20paving%20the%20way%20for%20future%20advancements%20in%20human-AI%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19227v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Interfaces%2520for%2520Language%2520Models%26entry.906535625%3DJiaqi%2520Chen%2520and%2520Yanzhe%2520Zhang%2520and%2520Yutong%2520Zhang%2520and%2520Yijia%2520Shao%2520and%2520Diyi%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520seen%2520as%2520assistants%252C%2520copilots%252C%250Aand%2520consultants%252C%2520capable%2520of%2520supporting%2520a%2520wide%2520range%2520of%2520tasks%2520through%2520natural%250Aconversation.%2520However%252C%2520most%2520systems%2520remain%2520constrained%2520by%2520a%2520linear%250Arequest-response%2520format%2520that%2520often%2520makes%2520interactions%2520inefficient%2520in%250Amulti-turn%252C%2520information-dense%252C%2520and%2520exploratory%2520tasks.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520Generative%2520Interfaces%2520for%2520Language%2520Models%252C%2520a%2520paradigm%250Ain%2520which%2520LLMs%2520respond%2520to%2520user%2520queries%2520by%2520proactively%2520generating%2520user%2520interfaces%250A%2528UIs%2529%2520that%2520enable%2520more%2520adaptive%2520and%2520interactive%2520engagement.%2520Our%2520framework%250Aleverages%2520structured%2520interface-specific%2520representations%2520and%2520iterative%250Arefinements%2520to%2520translate%2520user%2520queries%2520into%2520task-specific%2520UIs.%2520For%2520systematic%250Aevaluation%252C%2520we%2520introduce%2520a%2520multidimensional%2520assessment%2520framework%2520that%2520compares%250Agenerative%2520interfaces%2520with%2520traditional%2520chat-based%2520ones%2520across%2520diverse%2520tasks%252C%250Ainteraction%2520patterns%252C%2520and%2520query%2520types%252C%2520capturing%2520functional%252C%2520interactive%252C%2520and%250Aemotional%2520aspects%2520of%2520user%2520experience.%2520Results%2520show%2520that%2520generative%2520interfaces%250Aconsistently%2520outperform%2520conversational%2520ones%252C%2520with%2520up%2520to%2520a%252072%2525%2520improvement%2520in%250Ahuman%2520preference.%2520These%2520findings%2520clarify%2520when%2520and%2520why%2520users%2520favor%2520generative%250Ainterfaces%252C%2520paving%2520the%2520way%2520for%2520future%2520advancements%2520in%2520human-AI%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19227v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Interfaces%20for%20Language%20Models&entry.906535625=Jiaqi%20Chen%20and%20Yanzhe%20Zhang%20and%20Yutong%20Zhang%20and%20Yijia%20Shao%20and%20Diyi%20Yang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20seen%20as%20assistants%2C%20copilots%2C%0Aand%20consultants%2C%20capable%20of%20supporting%20a%20wide%20range%20of%20tasks%20through%20natural%0Aconversation.%20However%2C%20most%20systems%20remain%20constrained%20by%20a%20linear%0Arequest-response%20format%20that%20often%20makes%20interactions%20inefficient%20in%0Amulti-turn%2C%20information-dense%2C%20and%20exploratory%20tasks.%20To%20address%20these%0Alimitations%2C%20we%20propose%20Generative%20Interfaces%20for%20Language%20Models%2C%20a%20paradigm%0Ain%20which%20LLMs%20respond%20to%20user%20queries%20by%20proactively%20generating%20user%20interfaces%0A%28UIs%29%20that%20enable%20more%20adaptive%20and%20interactive%20engagement.%20Our%20framework%0Aleverages%20structured%20interface-specific%20representations%20and%20iterative%0Arefinements%20to%20translate%20user%20queries%20into%20task-specific%20UIs.%20For%20systematic%0Aevaluation%2C%20we%20introduce%20a%20multidimensional%20assessment%20framework%20that%20compares%0Agenerative%20interfaces%20with%20traditional%20chat-based%20ones%20across%20diverse%20tasks%2C%0Ainteraction%20patterns%2C%20and%20query%20types%2C%20capturing%20functional%2C%20interactive%2C%20and%0Aemotional%20aspects%20of%20user%20experience.%20Results%20show%20that%20generative%20interfaces%0Aconsistently%20outperform%20conversational%20ones%2C%20with%20up%20to%20a%2072%25%20improvement%20in%0Ahuman%20preference.%20These%20findings%20clarify%20when%20and%20why%20users%20favor%20generative%0Ainterfaces%2C%20paving%20the%20way%20for%20future%20advancements%20in%20human-AI%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19227v2&entry.124074799=Read"},
{"title": "Noise2Score3D: Tweedie's Approach for Unsupervised Point Cloud Denoising", "author": "Xiangbin Wei and Yuanfeng Wang and Ao XU and Lingyu Zhu and Dongyong Sun and Keren Li and Yang Li and Qi Qin", "abstract": "  Building on recent advances in Bayesian statistics and image denoising, we\npropose Noise2Score3D, a fully unsupervised framework for point cloud\ndenoising. Noise2Score3D learns the score function of the underlying point\ncloud distribution directly from noisy data, eliminating the need for clean\ndata during training. Using Tweedie's formula, our method performs denoising in\na single step, avoiding the iterative processes used in existing unsupervised\nmethods, thus improving both accuracy and efficiency. Additionally, we\nintroduce Total Variation for Point Clouds as a denoising quality metric, which\nallows for the estimation of unknown noise parameters. Experimental results\ndemonstrate that Noise2Score3D achieves state-of-the-art performance on\nstandard benchmarks among unsupervised learning methods in Chamfer distance and\npoint-to-mesh metrics. Noise2Score3D also demonstrates strong generalization\nability beyond training datasets. Our method, by addressing the generalization\nissue and challenge of the absence of clean data in learning-based methods,\npaves the way for learning-based point cloud denoising methods in real-world\napplications.\n", "link": "http://arxiv.org/abs/2503.09283v3", "date": "2025-10-07", "relevancy": 2.754, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5771}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5463}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise2Score3D%3A%20Tweedie%27s%20Approach%20for%20Unsupervised%20Point%20Cloud%20Denoising&body=Title%3A%20Noise2Score3D%3A%20Tweedie%27s%20Approach%20for%20Unsupervised%20Point%20Cloud%20Denoising%0AAuthor%3A%20Xiangbin%20Wei%20and%20Yuanfeng%20Wang%20and%20Ao%20XU%20and%20Lingyu%20Zhu%20and%20Dongyong%20Sun%20and%20Keren%20Li%20and%20Yang%20Li%20and%20Qi%20Qin%0AAbstract%3A%20%20%20Building%20on%20recent%20advances%20in%20Bayesian%20statistics%20and%20image%20denoising%2C%20we%0Apropose%20Noise2Score3D%2C%20a%20fully%20unsupervised%20framework%20for%20point%20cloud%0Adenoising.%20Noise2Score3D%20learns%20the%20score%20function%20of%20the%20underlying%20point%0Acloud%20distribution%20directly%20from%20noisy%20data%2C%20eliminating%20the%20need%20for%20clean%0Adata%20during%20training.%20Using%20Tweedie%27s%20formula%2C%20our%20method%20performs%20denoising%20in%0Aa%20single%20step%2C%20avoiding%20the%20iterative%20processes%20used%20in%20existing%20unsupervised%0Amethods%2C%20thus%20improving%20both%20accuracy%20and%20efficiency.%20Additionally%2C%20we%0Aintroduce%20Total%20Variation%20for%20Point%20Clouds%20as%20a%20denoising%20quality%20metric%2C%20which%0Aallows%20for%20the%20estimation%20of%20unknown%20noise%20parameters.%20Experimental%20results%0Ademonstrate%20that%20Noise2Score3D%20achieves%20state-of-the-art%20performance%20on%0Astandard%20benchmarks%20among%20unsupervised%20learning%20methods%20in%20Chamfer%20distance%20and%0Apoint-to-mesh%20metrics.%20Noise2Score3D%20also%20demonstrates%20strong%20generalization%0Aability%20beyond%20training%20datasets.%20Our%20method%2C%20by%20addressing%20the%20generalization%0Aissue%20and%20challenge%20of%20the%20absence%20of%20clean%20data%20in%20learning-based%20methods%2C%0Apaves%20the%20way%20for%20learning-based%20point%20cloud%20denoising%20methods%20in%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09283v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise2Score3D%253A%2520Tweedie%2527s%2520Approach%2520for%2520Unsupervised%2520Point%2520Cloud%2520Denoising%26entry.906535625%3DXiangbin%2520Wei%2520and%2520Yuanfeng%2520Wang%2520and%2520Ao%2520XU%2520and%2520Lingyu%2520Zhu%2520and%2520Dongyong%2520Sun%2520and%2520Keren%2520Li%2520and%2520Yang%2520Li%2520and%2520Qi%2520Qin%26entry.1292438233%3D%2520%2520Building%2520on%2520recent%2520advances%2520in%2520Bayesian%2520statistics%2520and%2520image%2520denoising%252C%2520we%250Apropose%2520Noise2Score3D%252C%2520a%2520fully%2520unsupervised%2520framework%2520for%2520point%2520cloud%250Adenoising.%2520Noise2Score3D%2520learns%2520the%2520score%2520function%2520of%2520the%2520underlying%2520point%250Acloud%2520distribution%2520directly%2520from%2520noisy%2520data%252C%2520eliminating%2520the%2520need%2520for%2520clean%250Adata%2520during%2520training.%2520Using%2520Tweedie%2527s%2520formula%252C%2520our%2520method%2520performs%2520denoising%2520in%250Aa%2520single%2520step%252C%2520avoiding%2520the%2520iterative%2520processes%2520used%2520in%2520existing%2520unsupervised%250Amethods%252C%2520thus%2520improving%2520both%2520accuracy%2520and%2520efficiency.%2520Additionally%252C%2520we%250Aintroduce%2520Total%2520Variation%2520for%2520Point%2520Clouds%2520as%2520a%2520denoising%2520quality%2520metric%252C%2520which%250Aallows%2520for%2520the%2520estimation%2520of%2520unknown%2520noise%2520parameters.%2520Experimental%2520results%250Ademonstrate%2520that%2520Noise2Score3D%2520achieves%2520state-of-the-art%2520performance%2520on%250Astandard%2520benchmarks%2520among%2520unsupervised%2520learning%2520methods%2520in%2520Chamfer%2520distance%2520and%250Apoint-to-mesh%2520metrics.%2520Noise2Score3D%2520also%2520demonstrates%2520strong%2520generalization%250Aability%2520beyond%2520training%2520datasets.%2520Our%2520method%252C%2520by%2520addressing%2520the%2520generalization%250Aissue%2520and%2520challenge%2520of%2520the%2520absence%2520of%2520clean%2520data%2520in%2520learning-based%2520methods%252C%250Apaves%2520the%2520way%2520for%2520learning-based%2520point%2520cloud%2520denoising%2520methods%2520in%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09283v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise2Score3D%3A%20Tweedie%27s%20Approach%20for%20Unsupervised%20Point%20Cloud%20Denoising&entry.906535625=Xiangbin%20Wei%20and%20Yuanfeng%20Wang%20and%20Ao%20XU%20and%20Lingyu%20Zhu%20and%20Dongyong%20Sun%20and%20Keren%20Li%20and%20Yang%20Li%20and%20Qi%20Qin&entry.1292438233=%20%20Building%20on%20recent%20advances%20in%20Bayesian%20statistics%20and%20image%20denoising%2C%20we%0Apropose%20Noise2Score3D%2C%20a%20fully%20unsupervised%20framework%20for%20point%20cloud%0Adenoising.%20Noise2Score3D%20learns%20the%20score%20function%20of%20the%20underlying%20point%0Acloud%20distribution%20directly%20from%20noisy%20data%2C%20eliminating%20the%20need%20for%20clean%0Adata%20during%20training.%20Using%20Tweedie%27s%20formula%2C%20our%20method%20performs%20denoising%20in%0Aa%20single%20step%2C%20avoiding%20the%20iterative%20processes%20used%20in%20existing%20unsupervised%0Amethods%2C%20thus%20improving%20both%20accuracy%20and%20efficiency.%20Additionally%2C%20we%0Aintroduce%20Total%20Variation%20for%20Point%20Clouds%20as%20a%20denoising%20quality%20metric%2C%20which%0Aallows%20for%20the%20estimation%20of%20unknown%20noise%20parameters.%20Experimental%20results%0Ademonstrate%20that%20Noise2Score3D%20achieves%20state-of-the-art%20performance%20on%0Astandard%20benchmarks%20among%20unsupervised%20learning%20methods%20in%20Chamfer%20distance%20and%0Apoint-to-mesh%20metrics.%20Noise2Score3D%20also%20demonstrates%20strong%20generalization%0Aability%20beyond%20training%20datasets.%20Our%20method%2C%20by%20addressing%20the%20generalization%0Aissue%20and%20challenge%20of%20the%20absence%20of%20clean%20data%20in%20learning-based%20methods%2C%0Apaves%20the%20way%20for%20learning-based%20point%20cloud%20denoising%20methods%20in%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09283v3&entry.124074799=Read"},
{"title": "Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved\n  Reasoning", "author": "Chendong Wang and Donglin Bai and Yifan Yang and Xiao Jin and Anlan Zhang and Rui Wang and Shiqi Jiang and Yuqing Yang and Hao Wu and Qi Dai and Chong Luo and Ting Cao and Lili Qiu and Suman Banerjee", "abstract": "  We present \\emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA\nframework that preserves a fixed token budget by first \\emph{localizing}\nquestion-relevant interval(s) with a low-fps skim and then \\emph{answering} via\nspan-aware reallocation of visual tokens at higher effective frame rate,\nemitting an interleaved output with both spans and the final option for direct\nattribution. We also introduce \\dataname{}, which converts description based\nevent graphs into \\emph{span-grounded} multiple-choice QA by pairing each\nquestion with \\emph{ground-truth} time span(s) and related reasoning. ViTL is\ntrained end-to-end with an interleaved group-relative objective that couples\ntemporal IoU for localization with answer correctness, allowing credit to flow\nfrom answers back to spans without increasing compute. Under fixed token\nbudgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and\ntemporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations\nshow that span-aware token reallocation consistently surpasses uniform\nsampling. Together, \\dataname{} and ViTL provide an interpretable,\ncompute-efficient recipe for scalable long-video QA.\n", "link": "http://arxiv.org/abs/2510.04022v2", "date": "2025-10-07", "relevancy": 2.7259, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-in-the-Loop%3A%20Span-Grounded%20Long%20Video%20QA%20with%20Interleaved%0A%20%20Reasoning&body=Title%3A%20Video-in-the-Loop%3A%20Span-Grounded%20Long%20Video%20QA%20with%20Interleaved%0A%20%20Reasoning%0AAuthor%3A%20Chendong%20Wang%20and%20Donglin%20Bai%20and%20Yifan%20Yang%20and%20Xiao%20Jin%20and%20Anlan%20Zhang%20and%20Rui%20Wang%20and%20Shiqi%20Jiang%20and%20Yuqing%20Yang%20and%20Hao%20Wu%20and%20Qi%20Dai%20and%20Chong%20Luo%20and%20Ting%20Cao%20and%20Lili%20Qiu%20and%20Suman%20Banerjee%0AAbstract%3A%20%20%20We%20present%20%5Cemph%7BVideo-in-the-Loop%7D%20%28ViTL%29%2C%20a%20two-stage%20long-video%20QA%0Aframework%20that%20preserves%20a%20fixed%20token%20budget%20by%20first%20%5Cemph%7Blocalizing%7D%0Aquestion-relevant%20interval%28s%29%20with%20a%20low-fps%20skim%20and%20then%20%5Cemph%7Banswering%7D%20via%0Aspan-aware%20reallocation%20of%20visual%20tokens%20at%20higher%20effective%20frame%20rate%2C%0Aemitting%20an%20interleaved%20output%20with%20both%20spans%20and%20the%20final%20option%20for%20direct%0Aattribution.%20We%20also%20introduce%20%5Cdataname%7B%7D%2C%20which%20converts%20description%20based%0Aevent%20graphs%20into%20%5Cemph%7Bspan-grounded%7D%20multiple-choice%20QA%20by%20pairing%20each%0Aquestion%20with%20%5Cemph%7Bground-truth%7D%20time%20span%28s%29%20and%20related%20reasoning.%20ViTL%20is%0Atrained%20end-to-end%20with%20an%20interleaved%20group-relative%20objective%20that%20couples%0Atemporal%20IoU%20for%20localization%20with%20answer%20correctness%2C%20allowing%20credit%20to%20flow%0Afrom%20answers%20back%20to%20spans%20without%20increasing%20compute.%20Under%20fixed%20token%0Abudgets%2C%20ViTL%20attains%20up%20to%208.6%25%20with%2050%25%20less%20frame%20input%20on%20long-video%20QA%20and%0Atemporal%20grounding%20%28e.g.%2C%20Charades-STA%2C%20ActivityNet-Captions%29%20and%20ablations%0Ashow%20that%20span-aware%20token%20reallocation%20consistently%20surpasses%20uniform%0Asampling.%20Together%2C%20%5Cdataname%7B%7D%20and%20ViTL%20provide%20an%20interpretable%2C%0Acompute-efficient%20recipe%20for%20scalable%20long-video%20QA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-in-the-Loop%253A%2520Span-Grounded%2520Long%2520Video%2520QA%2520with%2520Interleaved%250A%2520%2520Reasoning%26entry.906535625%3DChendong%2520Wang%2520and%2520Donglin%2520Bai%2520and%2520Yifan%2520Yang%2520and%2520Xiao%2520Jin%2520and%2520Anlan%2520Zhang%2520and%2520Rui%2520Wang%2520and%2520Shiqi%2520Jiang%2520and%2520Yuqing%2520Yang%2520and%2520Hao%2520Wu%2520and%2520Qi%2520Dai%2520and%2520Chong%2520Luo%2520and%2520Ting%2520Cao%2520and%2520Lili%2520Qiu%2520and%2520Suman%2520Banerjee%26entry.1292438233%3D%2520%2520We%2520present%2520%255Cemph%257BVideo-in-the-Loop%257D%2520%2528ViTL%2529%252C%2520a%2520two-stage%2520long-video%2520QA%250Aframework%2520that%2520preserves%2520a%2520fixed%2520token%2520budget%2520by%2520first%2520%255Cemph%257Blocalizing%257D%250Aquestion-relevant%2520interval%2528s%2529%2520with%2520a%2520low-fps%2520skim%2520and%2520then%2520%255Cemph%257Banswering%257D%2520via%250Aspan-aware%2520reallocation%2520of%2520visual%2520tokens%2520at%2520higher%2520effective%2520frame%2520rate%252C%250Aemitting%2520an%2520interleaved%2520output%2520with%2520both%2520spans%2520and%2520the%2520final%2520option%2520for%2520direct%250Aattribution.%2520We%2520also%2520introduce%2520%255Cdataname%257B%257D%252C%2520which%2520converts%2520description%2520based%250Aevent%2520graphs%2520into%2520%255Cemph%257Bspan-grounded%257D%2520multiple-choice%2520QA%2520by%2520pairing%2520each%250Aquestion%2520with%2520%255Cemph%257Bground-truth%257D%2520time%2520span%2528s%2529%2520and%2520related%2520reasoning.%2520ViTL%2520is%250Atrained%2520end-to-end%2520with%2520an%2520interleaved%2520group-relative%2520objective%2520that%2520couples%250Atemporal%2520IoU%2520for%2520localization%2520with%2520answer%2520correctness%252C%2520allowing%2520credit%2520to%2520flow%250Afrom%2520answers%2520back%2520to%2520spans%2520without%2520increasing%2520compute.%2520Under%2520fixed%2520token%250Abudgets%252C%2520ViTL%2520attains%2520up%2520to%25208.6%2525%2520with%252050%2525%2520less%2520frame%2520input%2520on%2520long-video%2520QA%2520and%250Atemporal%2520grounding%2520%2528e.g.%252C%2520Charades-STA%252C%2520ActivityNet-Captions%2529%2520and%2520ablations%250Ashow%2520that%2520span-aware%2520token%2520reallocation%2520consistently%2520surpasses%2520uniform%250Asampling.%2520Together%252C%2520%255Cdataname%257B%257D%2520and%2520ViTL%2520provide%2520an%2520interpretable%252C%250Acompute-efficient%2520recipe%2520for%2520scalable%2520long-video%2520QA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-in-the-Loop%3A%20Span-Grounded%20Long%20Video%20QA%20with%20Interleaved%0A%20%20Reasoning&entry.906535625=Chendong%20Wang%20and%20Donglin%20Bai%20and%20Yifan%20Yang%20and%20Xiao%20Jin%20and%20Anlan%20Zhang%20and%20Rui%20Wang%20and%20Shiqi%20Jiang%20and%20Yuqing%20Yang%20and%20Hao%20Wu%20and%20Qi%20Dai%20and%20Chong%20Luo%20and%20Ting%20Cao%20and%20Lili%20Qiu%20and%20Suman%20Banerjee&entry.1292438233=%20%20We%20present%20%5Cemph%7BVideo-in-the-Loop%7D%20%28ViTL%29%2C%20a%20two-stage%20long-video%20QA%0Aframework%20that%20preserves%20a%20fixed%20token%20budget%20by%20first%20%5Cemph%7Blocalizing%7D%0Aquestion-relevant%20interval%28s%29%20with%20a%20low-fps%20skim%20and%20then%20%5Cemph%7Banswering%7D%20via%0Aspan-aware%20reallocation%20of%20visual%20tokens%20at%20higher%20effective%20frame%20rate%2C%0Aemitting%20an%20interleaved%20output%20with%20both%20spans%20and%20the%20final%20option%20for%20direct%0Aattribution.%20We%20also%20introduce%20%5Cdataname%7B%7D%2C%20which%20converts%20description%20based%0Aevent%20graphs%20into%20%5Cemph%7Bspan-grounded%7D%20multiple-choice%20QA%20by%20pairing%20each%0Aquestion%20with%20%5Cemph%7Bground-truth%7D%20time%20span%28s%29%20and%20related%20reasoning.%20ViTL%20is%0Atrained%20end-to-end%20with%20an%20interleaved%20group-relative%20objective%20that%20couples%0Atemporal%20IoU%20for%20localization%20with%20answer%20correctness%2C%20allowing%20credit%20to%20flow%0Afrom%20answers%20back%20to%20spans%20without%20increasing%20compute.%20Under%20fixed%20token%0Abudgets%2C%20ViTL%20attains%20up%20to%208.6%25%20with%2050%25%20less%20frame%20input%20on%20long-video%20QA%20and%0Atemporal%20grounding%20%28e.g.%2C%20Charades-STA%2C%20ActivityNet-Captions%29%20and%20ablations%0Ashow%20that%20span-aware%20token%20reallocation%20consistently%20surpasses%20uniform%0Asampling.%20Together%2C%20%5Cdataname%7B%7D%20and%20ViTL%20provide%20an%20interpretable%2C%0Acompute-efficient%20recipe%20for%20scalable%20long-video%20QA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04022v2&entry.124074799=Read"},
{"title": "DACP: Domain-Adaptive Continual Pre-Training of Large Language Models\n  for Phone Conversation Summarization", "author": "Xue-Yong Fu and Elena Khasanova and Md Tahmid Rahman Laskar and Harsh Saini and Shashi Bhushan TN", "abstract": "  Large language models (LLMs) have achieved impressive performance in text\nsummarization, yet their performance often falls short when applied to\nspecialized domains %or conversational data that differ from their original\npre-training distribution. While fine-tuning can improve summarization quality,\nit typically relies on costly and scarce high-quality labeled data. In this\nwork, we explore continual pre-training as a scalable, self-supervised approach\nto adapt LLMs for downstream summarization tasks, particularly in the context\nof noisy real-world conversation transcripts. We conduct extensive experiments\nusing large-scale, unlabeled business conversation data to investigate whether\ncontinual pre-training enhances model capabilities in conversational\nsummarization. Our results demonstrate that continual pre-training yields\nsubstantial gains in both in-domain and out-of-domain summarization benchmarks,\nwhile maintaining strong generalization and robustness. We also analyze the\neffects of data selection strategies, providing practical guidelines for\napplying continual pre-training in summarization-focused industrial\napplications.\n", "link": "http://arxiv.org/abs/2510.05858v1", "date": "2025-10-07", "relevancy": 2.6227, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DACP%3A%20Domain-Adaptive%20Continual%20Pre-Training%20of%20Large%20Language%20Models%0A%20%20for%20Phone%20Conversation%20Summarization&body=Title%3A%20DACP%3A%20Domain-Adaptive%20Continual%20Pre-Training%20of%20Large%20Language%20Models%0A%20%20for%20Phone%20Conversation%20Summarization%0AAuthor%3A%20Xue-Yong%20Fu%20and%20Elena%20Khasanova%20and%20Md%20Tahmid%20Rahman%20Laskar%20and%20Harsh%20Saini%20and%20Shashi%20Bhushan%20TN%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20in%20text%0Asummarization%2C%20yet%20their%20performance%20often%20falls%20short%20when%20applied%20to%0Aspecialized%20domains%20%25or%20conversational%20data%20that%20differ%20from%20their%20original%0Apre-training%20distribution.%20While%20fine-tuning%20can%20improve%20summarization%20quality%2C%0Ait%20typically%20relies%20on%20costly%20and%20scarce%20high-quality%20labeled%20data.%20In%20this%0Awork%2C%20we%20explore%20continual%20pre-training%20as%20a%20scalable%2C%20self-supervised%20approach%0Ato%20adapt%20LLMs%20for%20downstream%20summarization%20tasks%2C%20particularly%20in%20the%20context%0Aof%20noisy%20real-world%20conversation%20transcripts.%20We%20conduct%20extensive%20experiments%0Ausing%20large-scale%2C%20unlabeled%20business%20conversation%20data%20to%20investigate%20whether%0Acontinual%20pre-training%20enhances%20model%20capabilities%20in%20conversational%0Asummarization.%20Our%20results%20demonstrate%20that%20continual%20pre-training%20yields%0Asubstantial%20gains%20in%20both%20in-domain%20and%20out-of-domain%20summarization%20benchmarks%2C%0Awhile%20maintaining%20strong%20generalization%20and%20robustness.%20We%20also%20analyze%20the%0Aeffects%20of%20data%20selection%20strategies%2C%20providing%20practical%20guidelines%20for%0Aapplying%20continual%20pre-training%20in%20summarization-focused%20industrial%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDACP%253A%2520Domain-Adaptive%2520Continual%2520Pre-Training%2520of%2520Large%2520Language%2520Models%250A%2520%2520for%2520Phone%2520Conversation%2520Summarization%26entry.906535625%3DXue-Yong%2520Fu%2520and%2520Elena%2520Khasanova%2520and%2520Md%2520Tahmid%2520Rahman%2520Laskar%2520and%2520Harsh%2520Saini%2520and%2520Shashi%2520Bhushan%2520TN%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520in%2520text%250Asummarization%252C%2520yet%2520their%2520performance%2520often%2520falls%2520short%2520when%2520applied%2520to%250Aspecialized%2520domains%2520%2525or%2520conversational%2520data%2520that%2520differ%2520from%2520their%2520original%250Apre-training%2520distribution.%2520While%2520fine-tuning%2520can%2520improve%2520summarization%2520quality%252C%250Ait%2520typically%2520relies%2520on%2520costly%2520and%2520scarce%2520high-quality%2520labeled%2520data.%2520In%2520this%250Awork%252C%2520we%2520explore%2520continual%2520pre-training%2520as%2520a%2520scalable%252C%2520self-supervised%2520approach%250Ato%2520adapt%2520LLMs%2520for%2520downstream%2520summarization%2520tasks%252C%2520particularly%2520in%2520the%2520context%250Aof%2520noisy%2520real-world%2520conversation%2520transcripts.%2520We%2520conduct%2520extensive%2520experiments%250Ausing%2520large-scale%252C%2520unlabeled%2520business%2520conversation%2520data%2520to%2520investigate%2520whether%250Acontinual%2520pre-training%2520enhances%2520model%2520capabilities%2520in%2520conversational%250Asummarization.%2520Our%2520results%2520demonstrate%2520that%2520continual%2520pre-training%2520yields%250Asubstantial%2520gains%2520in%2520both%2520in-domain%2520and%2520out-of-domain%2520summarization%2520benchmarks%252C%250Awhile%2520maintaining%2520strong%2520generalization%2520and%2520robustness.%2520We%2520also%2520analyze%2520the%250Aeffects%2520of%2520data%2520selection%2520strategies%252C%2520providing%2520practical%2520guidelines%2520for%250Aapplying%2520continual%2520pre-training%2520in%2520summarization-focused%2520industrial%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DACP%3A%20Domain-Adaptive%20Continual%20Pre-Training%20of%20Large%20Language%20Models%0A%20%20for%20Phone%20Conversation%20Summarization&entry.906535625=Xue-Yong%20Fu%20and%20Elena%20Khasanova%20and%20Md%20Tahmid%20Rahman%20Laskar%20and%20Harsh%20Saini%20and%20Shashi%20Bhushan%20TN&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20in%20text%0Asummarization%2C%20yet%20their%20performance%20often%20falls%20short%20when%20applied%20to%0Aspecialized%20domains%20%25or%20conversational%20data%20that%20differ%20from%20their%20original%0Apre-training%20distribution.%20While%20fine-tuning%20can%20improve%20summarization%20quality%2C%0Ait%20typically%20relies%20on%20costly%20and%20scarce%20high-quality%20labeled%20data.%20In%20this%0Awork%2C%20we%20explore%20continual%20pre-training%20as%20a%20scalable%2C%20self-supervised%20approach%0Ato%20adapt%20LLMs%20for%20downstream%20summarization%20tasks%2C%20particularly%20in%20the%20context%0Aof%20noisy%20real-world%20conversation%20transcripts.%20We%20conduct%20extensive%20experiments%0Ausing%20large-scale%2C%20unlabeled%20business%20conversation%20data%20to%20investigate%20whether%0Acontinual%20pre-training%20enhances%20model%20capabilities%20in%20conversational%0Asummarization.%20Our%20results%20demonstrate%20that%20continual%20pre-training%20yields%0Asubstantial%20gains%20in%20both%20in-domain%20and%20out-of-domain%20summarization%20benchmarks%2C%0Awhile%20maintaining%20strong%20generalization%20and%20robustness.%20We%20also%20analyze%20the%0Aeffects%20of%20data%20selection%20strategies%2C%20providing%20practical%20guidelines%20for%0Aapplying%20continual%20pre-training%20in%20summarization-focused%20industrial%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05858v1&entry.124074799=Read"},
{"title": "OBSR: Open Benchmark for Spatial Representations", "author": "Julia Moska and Oleksii Furman and Kacper Kozaczko and Szymon Leszkiewicz and Jakub Polczyk and Piotr Gramacki and Piotr Szyma\u0144ski", "abstract": "  GeoAI is evolving rapidly, fueled by diverse geospatial datasets like traffic\npatterns, environmental data, and crowdsourced OpenStreetMap (OSM) information.\nWhile sophisticated AI models are being developed, existing benchmarks are\noften concentrated on single tasks and restricted to a single modality. As\nsuch, progress in GeoAI is limited by the lack of a standardized, multi-task,\nmodality-agnostic benchmark for their systematic evaluation. This paper\nintroduces a novel benchmark designed to assess the performance, accuracy, and\nefficiency of geospatial embedders. Our benchmark is modality-agnostic and\ncomprises 7 distinct datasets from diverse cities across three continents,\nensuring generalizability and mitigating demographic biases. It allows for the\nevaluation of GeoAI embedders on various phenomena that exhibit underlying\ngeographic processes. Furthermore, we establish a simple and intuitive\ntask-oriented model baselines, providing a crucial reference point for\ncomparing more complex solutions.\n", "link": "http://arxiv.org/abs/2510.05879v1", "date": "2025-10-07", "relevancy": 2.6159, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OBSR%3A%20Open%20Benchmark%20for%20Spatial%20Representations&body=Title%3A%20OBSR%3A%20Open%20Benchmark%20for%20Spatial%20Representations%0AAuthor%3A%20Julia%20Moska%20and%20Oleksii%20Furman%20and%20Kacper%20Kozaczko%20and%20Szymon%20Leszkiewicz%20and%20Jakub%20Polczyk%20and%20Piotr%20Gramacki%20and%20Piotr%20Szyma%C5%84ski%0AAbstract%3A%20%20%20GeoAI%20is%20evolving%20rapidly%2C%20fueled%20by%20diverse%20geospatial%20datasets%20like%20traffic%0Apatterns%2C%20environmental%20data%2C%20and%20crowdsourced%20OpenStreetMap%20%28OSM%29%20information.%0AWhile%20sophisticated%20AI%20models%20are%20being%20developed%2C%20existing%20benchmarks%20are%0Aoften%20concentrated%20on%20single%20tasks%20and%20restricted%20to%20a%20single%20modality.%20As%0Asuch%2C%20progress%20in%20GeoAI%20is%20limited%20by%20the%20lack%20of%20a%20standardized%2C%20multi-task%2C%0Amodality-agnostic%20benchmark%20for%20their%20systematic%20evaluation.%20This%20paper%0Aintroduces%20a%20novel%20benchmark%20designed%20to%20assess%20the%20performance%2C%20accuracy%2C%20and%0Aefficiency%20of%20geospatial%20embedders.%20Our%20benchmark%20is%20modality-agnostic%20and%0Acomprises%207%20distinct%20datasets%20from%20diverse%20cities%20across%20three%20continents%2C%0Aensuring%20generalizability%20and%20mitigating%20demographic%20biases.%20It%20allows%20for%20the%0Aevaluation%20of%20GeoAI%20embedders%20on%20various%20phenomena%20that%20exhibit%20underlying%0Ageographic%20processes.%20Furthermore%2C%20we%20establish%20a%20simple%20and%20intuitive%0Atask-oriented%20model%20baselines%2C%20providing%20a%20crucial%20reference%20point%20for%0Acomparing%20more%20complex%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOBSR%253A%2520Open%2520Benchmark%2520for%2520Spatial%2520Representations%26entry.906535625%3DJulia%2520Moska%2520and%2520Oleksii%2520Furman%2520and%2520Kacper%2520Kozaczko%2520and%2520Szymon%2520Leszkiewicz%2520and%2520Jakub%2520Polczyk%2520and%2520Piotr%2520Gramacki%2520and%2520Piotr%2520Szyma%25C5%2584ski%26entry.1292438233%3D%2520%2520GeoAI%2520is%2520evolving%2520rapidly%252C%2520fueled%2520by%2520diverse%2520geospatial%2520datasets%2520like%2520traffic%250Apatterns%252C%2520environmental%2520data%252C%2520and%2520crowdsourced%2520OpenStreetMap%2520%2528OSM%2529%2520information.%250AWhile%2520sophisticated%2520AI%2520models%2520are%2520being%2520developed%252C%2520existing%2520benchmarks%2520are%250Aoften%2520concentrated%2520on%2520single%2520tasks%2520and%2520restricted%2520to%2520a%2520single%2520modality.%2520As%250Asuch%252C%2520progress%2520in%2520GeoAI%2520is%2520limited%2520by%2520the%2520lack%2520of%2520a%2520standardized%252C%2520multi-task%252C%250Amodality-agnostic%2520benchmark%2520for%2520their%2520systematic%2520evaluation.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520benchmark%2520designed%2520to%2520assess%2520the%2520performance%252C%2520accuracy%252C%2520and%250Aefficiency%2520of%2520geospatial%2520embedders.%2520Our%2520benchmark%2520is%2520modality-agnostic%2520and%250Acomprises%25207%2520distinct%2520datasets%2520from%2520diverse%2520cities%2520across%2520three%2520continents%252C%250Aensuring%2520generalizability%2520and%2520mitigating%2520demographic%2520biases.%2520It%2520allows%2520for%2520the%250Aevaluation%2520of%2520GeoAI%2520embedders%2520on%2520various%2520phenomena%2520that%2520exhibit%2520underlying%250Ageographic%2520processes.%2520Furthermore%252C%2520we%2520establish%2520a%2520simple%2520and%2520intuitive%250Atask-oriented%2520model%2520baselines%252C%2520providing%2520a%2520crucial%2520reference%2520point%2520for%250Acomparing%2520more%2520complex%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OBSR%3A%20Open%20Benchmark%20for%20Spatial%20Representations&entry.906535625=Julia%20Moska%20and%20Oleksii%20Furman%20and%20Kacper%20Kozaczko%20and%20Szymon%20Leszkiewicz%20and%20Jakub%20Polczyk%20and%20Piotr%20Gramacki%20and%20Piotr%20Szyma%C5%84ski&entry.1292438233=%20%20GeoAI%20is%20evolving%20rapidly%2C%20fueled%20by%20diverse%20geospatial%20datasets%20like%20traffic%0Apatterns%2C%20environmental%20data%2C%20and%20crowdsourced%20OpenStreetMap%20%28OSM%29%20information.%0AWhile%20sophisticated%20AI%20models%20are%20being%20developed%2C%20existing%20benchmarks%20are%0Aoften%20concentrated%20on%20single%20tasks%20and%20restricted%20to%20a%20single%20modality.%20As%0Asuch%2C%20progress%20in%20GeoAI%20is%20limited%20by%20the%20lack%20of%20a%20standardized%2C%20multi-task%2C%0Amodality-agnostic%20benchmark%20for%20their%20systematic%20evaluation.%20This%20paper%0Aintroduces%20a%20novel%20benchmark%20designed%20to%20assess%20the%20performance%2C%20accuracy%2C%20and%0Aefficiency%20of%20geospatial%20embedders.%20Our%20benchmark%20is%20modality-agnostic%20and%0Acomprises%207%20distinct%20datasets%20from%20diverse%20cities%20across%20three%20continents%2C%0Aensuring%20generalizability%20and%20mitigating%20demographic%20biases.%20It%20allows%20for%20the%0Aevaluation%20of%20GeoAI%20embedders%20on%20various%20phenomena%20that%20exhibit%20underlying%0Ageographic%20processes.%20Furthermore%2C%20we%20establish%20a%20simple%20and%20intuitive%0Atask-oriented%20model%20baselines%2C%20providing%20a%20crucial%20reference%20point%20for%0Acomparing%20more%20complex%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05879v1&entry.124074799=Read"},
{"title": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal\n  Encoders", "author": "Riccardo Fosco Gramaccioni and Christian Marinoni and Eleonora Grassucci and Giordano Cicchetti and Aurelio Uncini and Danilo Comminiello", "abstract": "  In this work, we present FoleyGRAM, a novel approach to video-to-audio\ngeneration that emphasizes semantic conditioning through the use of aligned\nmultimodal encoders. Building on prior advancements in video-to-audio\ngeneration, FoleyGRAM leverages the Gramian Representation Alignment Measure\n(GRAM) to align embeddings across video, text, and audio modalities, enabling\nprecise semantic control over the audio generation process. The core of\nFoleyGRAM is a diffusion-based audio synthesis model conditioned on\nGRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness\nand temporal alignment with the corresponding input video. We evaluate\nFoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio\nmodels. Our experiments demonstrate that aligning multimodal encoders using\nGRAM enhances the system's ability to semantically align generated audio with\nvideo content, advancing the state of the art in video-to-audio synthesis.\n", "link": "http://arxiv.org/abs/2510.05829v1", "date": "2025-10-07", "relevancy": 2.6013, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5493}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5227}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoleyGRAM%3A%20Video-to-Audio%20Generation%20with%20GRAM-Aligned%20Multimodal%0A%20%20Encoders&body=Title%3A%20FoleyGRAM%3A%20Video-to-Audio%20Generation%20with%20GRAM-Aligned%20Multimodal%0A%20%20Encoders%0AAuthor%3A%20Riccardo%20Fosco%20Gramaccioni%20and%20Christian%20Marinoni%20and%20Eleonora%20Grassucci%20and%20Giordano%20Cicchetti%20and%20Aurelio%20Uncini%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20FoleyGRAM%2C%20a%20novel%20approach%20to%20video-to-audio%0Ageneration%20that%20emphasizes%20semantic%20conditioning%20through%20the%20use%20of%20aligned%0Amultimodal%20encoders.%20Building%20on%20prior%20advancements%20in%20video-to-audio%0Ageneration%2C%20FoleyGRAM%20leverages%20the%20Gramian%20Representation%20Alignment%20Measure%0A%28GRAM%29%20to%20align%20embeddings%20across%20video%2C%20text%2C%20and%20audio%20modalities%2C%20enabling%0Aprecise%20semantic%20control%20over%20the%20audio%20generation%20process.%20The%20core%20of%0AFoleyGRAM%20is%20a%20diffusion-based%20audio%20synthesis%20model%20conditioned%20on%0AGRAM-aligned%20embeddings%20and%20waveform%20envelopes%2C%20ensuring%20both%20semantic%20richness%0Aand%20temporal%20alignment%20with%20the%20corresponding%20input%20video.%20We%20evaluate%0AFoleyGRAM%20on%20the%20Greatest%20Hits%20dataset%2C%20a%20standard%20benchmark%20for%20video-to-audio%0Amodels.%20Our%20experiments%20demonstrate%20that%20aligning%20multimodal%20encoders%20using%0AGRAM%20enhances%20the%20system%27s%20ability%20to%20semantically%20align%20generated%20audio%20with%0Avideo%20content%2C%20advancing%20the%20state%20of%20the%20art%20in%20video-to-audio%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoleyGRAM%253A%2520Video-to-Audio%2520Generation%2520with%2520GRAM-Aligned%2520Multimodal%250A%2520%2520Encoders%26entry.906535625%3DRiccardo%2520Fosco%2520Gramaccioni%2520and%2520Christian%2520Marinoni%2520and%2520Eleonora%2520Grassucci%2520and%2520Giordano%2520Cicchetti%2520and%2520Aurelio%2520Uncini%2520and%2520Danilo%2520Comminiello%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520FoleyGRAM%252C%2520a%2520novel%2520approach%2520to%2520video-to-audio%250Ageneration%2520that%2520emphasizes%2520semantic%2520conditioning%2520through%2520the%2520use%2520of%2520aligned%250Amultimodal%2520encoders.%2520Building%2520on%2520prior%2520advancements%2520in%2520video-to-audio%250Ageneration%252C%2520FoleyGRAM%2520leverages%2520the%2520Gramian%2520Representation%2520Alignment%2520Measure%250A%2528GRAM%2529%2520to%2520align%2520embeddings%2520across%2520video%252C%2520text%252C%2520and%2520audio%2520modalities%252C%2520enabling%250Aprecise%2520semantic%2520control%2520over%2520the%2520audio%2520generation%2520process.%2520The%2520core%2520of%250AFoleyGRAM%2520is%2520a%2520diffusion-based%2520audio%2520synthesis%2520model%2520conditioned%2520on%250AGRAM-aligned%2520embeddings%2520and%2520waveform%2520envelopes%252C%2520ensuring%2520both%2520semantic%2520richness%250Aand%2520temporal%2520alignment%2520with%2520the%2520corresponding%2520input%2520video.%2520We%2520evaluate%250AFoleyGRAM%2520on%2520the%2520Greatest%2520Hits%2520dataset%252C%2520a%2520standard%2520benchmark%2520for%2520video-to-audio%250Amodels.%2520Our%2520experiments%2520demonstrate%2520that%2520aligning%2520multimodal%2520encoders%2520using%250AGRAM%2520enhances%2520the%2520system%2527s%2520ability%2520to%2520semantically%2520align%2520generated%2520audio%2520with%250Avideo%2520content%252C%2520advancing%2520the%2520state%2520of%2520the%2520art%2520in%2520video-to-audio%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoleyGRAM%3A%20Video-to-Audio%20Generation%20with%20GRAM-Aligned%20Multimodal%0A%20%20Encoders&entry.906535625=Riccardo%20Fosco%20Gramaccioni%20and%20Christian%20Marinoni%20and%20Eleonora%20Grassucci%20and%20Giordano%20Cicchetti%20and%20Aurelio%20Uncini%20and%20Danilo%20Comminiello&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20FoleyGRAM%2C%20a%20novel%20approach%20to%20video-to-audio%0Ageneration%20that%20emphasizes%20semantic%20conditioning%20through%20the%20use%20of%20aligned%0Amultimodal%20encoders.%20Building%20on%20prior%20advancements%20in%20video-to-audio%0Ageneration%2C%20FoleyGRAM%20leverages%20the%20Gramian%20Representation%20Alignment%20Measure%0A%28GRAM%29%20to%20align%20embeddings%20across%20video%2C%20text%2C%20and%20audio%20modalities%2C%20enabling%0Aprecise%20semantic%20control%20over%20the%20audio%20generation%20process.%20The%20core%20of%0AFoleyGRAM%20is%20a%20diffusion-based%20audio%20synthesis%20model%20conditioned%20on%0AGRAM-aligned%20embeddings%20and%20waveform%20envelopes%2C%20ensuring%20both%20semantic%20richness%0Aand%20temporal%20alignment%20with%20the%20corresponding%20input%20video.%20We%20evaluate%0AFoleyGRAM%20on%20the%20Greatest%20Hits%20dataset%2C%20a%20standard%20benchmark%20for%20video-to-audio%0Amodels.%20Our%20experiments%20demonstrate%20that%20aligning%20multimodal%20encoders%20using%0AGRAM%20enhances%20the%20system%27s%20ability%20to%20semantically%20align%20generated%20audio%20with%0Avideo%20content%2C%20advancing%20the%20state%20of%20the%20art%20in%20video-to-audio%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05829v1&entry.124074799=Read"},
{"title": "Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and\n  Interpretability Prediction", "author": "Mengying Yuan and Wenhao Wang and Zixuan Wang and Yujie Huang and Kangli Wei and Fei Li and Chong Teng and Donghong Ji", "abstract": "  Natural Language Inference (NLI) is a fundamental task in natural language\nprocessing. While NLI has developed many sub-directions such as sentence-level\nNLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI\n(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel\nparadigm: CDCL-NLI, which extends traditional NLI capabilities to\nmulti-document, multilingual scenarios. To support this task, we construct a\nhigh-quality CDCL-NLI dataset including 25,410 instances and spanning 26\nlanguages. To address the limitations of previous methods on CDCL-NLI task, we\nfurther propose an innovative method that integrates RST-enhanced graph fusion\nwith interpretability-aware prediction. Our approach leverages RST (Rhetorical\nStructure Theory) within heterogeneous graph neural networks for cross-document\ncontext modeling, and employs a structure-aware semantic alignment based on\nlexical chains for cross-lingual understanding. For NLI interpretability, we\ndevelop an EDU (Elementary Discourse Unit)-level attribution framework that\nproduces extractive explanations. Extensive experiments demonstrate our\napproach's superior performance, achieving significant improvements over both\nconventional NLI models as well as large language models. Our work sheds light\non the study of NLI and will bring research interest on cross-document\ncross-lingual context understanding, hallucination elimination and\ninterpretability inference. Our code and datasets are available at\n\"https://github.com/Leonardo123-ui/CDCL_NLI\" for peer review.\n", "link": "http://arxiv.org/abs/2504.12324v3", "date": "2025-10-07", "relevancy": 2.6004, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Document%20Cross-Lingual%20NLI%20via%20RST-Enhanced%20Graph%20Fusion%20and%0A%20%20Interpretability%20Prediction&body=Title%3A%20Cross-Document%20Cross-Lingual%20NLI%20via%20RST-Enhanced%20Graph%20Fusion%20and%0A%20%20Interpretability%20Prediction%0AAuthor%3A%20Mengying%20Yuan%20and%20Wenhao%20Wang%20and%20Zixuan%20Wang%20and%20Yujie%20Huang%20and%20Kangli%20Wei%20and%20Fei%20Li%20and%20Chong%20Teng%20and%20Donghong%20Ji%0AAbstract%3A%20%20%20Natural%20Language%20Inference%20%28NLI%29%20is%20a%20fundamental%20task%20in%20natural%20language%0Aprocessing.%20While%20NLI%20has%20developed%20many%20sub-directions%20such%20as%20sentence-level%0ANLI%2C%20document-level%20NLI%20and%20cross-lingual%20NLI%2C%20Cross-Document%20Cross-Lingual%20NLI%0A%28CDCL-NLI%29%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aparadigm%3A%20CDCL-NLI%2C%20which%20extends%20traditional%20NLI%20capabilities%20to%0Amulti-document%2C%20multilingual%20scenarios.%20To%20support%20this%20task%2C%20we%20construct%20a%0Ahigh-quality%20CDCL-NLI%20dataset%20including%2025%2C410%20instances%20and%20spanning%2026%0Alanguages.%20To%20address%20the%20limitations%20of%20previous%20methods%20on%20CDCL-NLI%20task%2C%20we%0Afurther%20propose%20an%20innovative%20method%20that%20integrates%20RST-enhanced%20graph%20fusion%0Awith%20interpretability-aware%20prediction.%20Our%20approach%20leverages%20RST%20%28Rhetorical%0AStructure%20Theory%29%20within%20heterogeneous%20graph%20neural%20networks%20for%20cross-document%0Acontext%20modeling%2C%20and%20employs%20a%20structure-aware%20semantic%20alignment%20based%20on%0Alexical%20chains%20for%20cross-lingual%20understanding.%20For%20NLI%20interpretability%2C%20we%0Adevelop%20an%20EDU%20%28Elementary%20Discourse%20Unit%29-level%20attribution%20framework%20that%0Aproduces%20extractive%20explanations.%20Extensive%20experiments%20demonstrate%20our%0Aapproach%27s%20superior%20performance%2C%20achieving%20significant%20improvements%20over%20both%0Aconventional%20NLI%20models%20as%20well%20as%20large%20language%20models.%20Our%20work%20sheds%20light%0Aon%20the%20study%20of%20NLI%20and%20will%20bring%20research%20interest%20on%20cross-document%0Across-lingual%20context%20understanding%2C%20hallucination%20elimination%20and%0Ainterpretability%20inference.%20Our%20code%20and%20datasets%20are%20available%20at%0A%22https%3A//github.com/Leonardo123-ui/CDCL_NLI%22%20for%20peer%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12324v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Document%2520Cross-Lingual%2520NLI%2520via%2520RST-Enhanced%2520Graph%2520Fusion%2520and%250A%2520%2520Interpretability%2520Prediction%26entry.906535625%3DMengying%2520Yuan%2520and%2520Wenhao%2520Wang%2520and%2520Zixuan%2520Wang%2520and%2520Yujie%2520Huang%2520and%2520Kangli%2520Wei%2520and%2520Fei%2520Li%2520and%2520Chong%2520Teng%2520and%2520Donghong%2520Ji%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Inference%2520%2528NLI%2529%2520is%2520a%2520fundamental%2520task%2520in%2520natural%2520language%250Aprocessing.%2520While%2520NLI%2520has%2520developed%2520many%2520sub-directions%2520such%2520as%2520sentence-level%250ANLI%252C%2520document-level%2520NLI%2520and%2520cross-lingual%2520NLI%252C%2520Cross-Document%2520Cross-Lingual%2520NLI%250A%2528CDCL-NLI%2529%2520remains%2520largely%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aparadigm%253A%2520CDCL-NLI%252C%2520which%2520extends%2520traditional%2520NLI%2520capabilities%2520to%250Amulti-document%252C%2520multilingual%2520scenarios.%2520To%2520support%2520this%2520task%252C%2520we%2520construct%2520a%250Ahigh-quality%2520CDCL-NLI%2520dataset%2520including%252025%252C410%2520instances%2520and%2520spanning%252026%250Alanguages.%2520To%2520address%2520the%2520limitations%2520of%2520previous%2520methods%2520on%2520CDCL-NLI%2520task%252C%2520we%250Afurther%2520propose%2520an%2520innovative%2520method%2520that%2520integrates%2520RST-enhanced%2520graph%2520fusion%250Awith%2520interpretability-aware%2520prediction.%2520Our%2520approach%2520leverages%2520RST%2520%2528Rhetorical%250AStructure%2520Theory%2529%2520within%2520heterogeneous%2520graph%2520neural%2520networks%2520for%2520cross-document%250Acontext%2520modeling%252C%2520and%2520employs%2520a%2520structure-aware%2520semantic%2520alignment%2520based%2520on%250Alexical%2520chains%2520for%2520cross-lingual%2520understanding.%2520For%2520NLI%2520interpretability%252C%2520we%250Adevelop%2520an%2520EDU%2520%2528Elementary%2520Discourse%2520Unit%2529-level%2520attribution%2520framework%2520that%250Aproduces%2520extractive%2520explanations.%2520Extensive%2520experiments%2520demonstrate%2520our%250Aapproach%2527s%2520superior%2520performance%252C%2520achieving%2520significant%2520improvements%2520over%2520both%250Aconventional%2520NLI%2520models%2520as%2520well%2520as%2520large%2520language%2520models.%2520Our%2520work%2520sheds%2520light%250Aon%2520the%2520study%2520of%2520NLI%2520and%2520will%2520bring%2520research%2520interest%2520on%2520cross-document%250Across-lingual%2520context%2520understanding%252C%2520hallucination%2520elimination%2520and%250Ainterpretability%2520inference.%2520Our%2520code%2520and%2520datasets%2520are%2520available%2520at%250A%2522https%253A//github.com/Leonardo123-ui/CDCL_NLI%2522%2520for%2520peer%2520review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12324v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Document%20Cross-Lingual%20NLI%20via%20RST-Enhanced%20Graph%20Fusion%20and%0A%20%20Interpretability%20Prediction&entry.906535625=Mengying%20Yuan%20and%20Wenhao%20Wang%20and%20Zixuan%20Wang%20and%20Yujie%20Huang%20and%20Kangli%20Wei%20and%20Fei%20Li%20and%20Chong%20Teng%20and%20Donghong%20Ji&entry.1292438233=%20%20Natural%20Language%20Inference%20%28NLI%29%20is%20a%20fundamental%20task%20in%20natural%20language%0Aprocessing.%20While%20NLI%20has%20developed%20many%20sub-directions%20such%20as%20sentence-level%0ANLI%2C%20document-level%20NLI%20and%20cross-lingual%20NLI%2C%20Cross-Document%20Cross-Lingual%20NLI%0A%28CDCL-NLI%29%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aparadigm%3A%20CDCL-NLI%2C%20which%20extends%20traditional%20NLI%20capabilities%20to%0Amulti-document%2C%20multilingual%20scenarios.%20To%20support%20this%20task%2C%20we%20construct%20a%0Ahigh-quality%20CDCL-NLI%20dataset%20including%2025%2C410%20instances%20and%20spanning%2026%0Alanguages.%20To%20address%20the%20limitations%20of%20previous%20methods%20on%20CDCL-NLI%20task%2C%20we%0Afurther%20propose%20an%20innovative%20method%20that%20integrates%20RST-enhanced%20graph%20fusion%0Awith%20interpretability-aware%20prediction.%20Our%20approach%20leverages%20RST%20%28Rhetorical%0AStructure%20Theory%29%20within%20heterogeneous%20graph%20neural%20networks%20for%20cross-document%0Acontext%20modeling%2C%20and%20employs%20a%20structure-aware%20semantic%20alignment%20based%20on%0Alexical%20chains%20for%20cross-lingual%20understanding.%20For%20NLI%20interpretability%2C%20we%0Adevelop%20an%20EDU%20%28Elementary%20Discourse%20Unit%29-level%20attribution%20framework%20that%0Aproduces%20extractive%20explanations.%20Extensive%20experiments%20demonstrate%20our%0Aapproach%27s%20superior%20performance%2C%20achieving%20significant%20improvements%20over%20both%0Aconventional%20NLI%20models%20as%20well%20as%20large%20language%20models.%20Our%20work%20sheds%20light%0Aon%20the%20study%20of%20NLI%20and%20will%20bring%20research%20interest%20on%20cross-document%0Across-lingual%20context%20understanding%2C%20hallucination%20elimination%20and%0Ainterpretability%20inference.%20Our%20code%20and%20datasets%20are%20available%20at%0A%22https%3A//github.com/Leonardo123-ui/CDCL_NLI%22%20for%20peer%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12324v3&entry.124074799=Read"},
{"title": "Low-Rank Tensor Recovery via Variational Schatten-p Quasi-Norm and\n  Jacobian Regularization", "author": "Zhengyun Cheng and Ruizhe Zhang and Guanwen Zhang and Yi Xu and Xiangyang Ji and Wei Zhou", "abstract": "  Higher-order tensors are well-suited for representing multi-dimensional data,\nsuch as images and videos, which typically characterize low-rank structures.\nLow-rank tensor decomposition has become essential in machine learning and\ncomputer vision, but existing methods like Tucker decomposition offer\nflexibility at the expense of interpretability. The CANDECOMP/PARAFAC (CP)\ndecomposition provides a natural and interpretable structure, while obtaining a\nsparse solutions remains challenging. Leveraging the rich properties of CP\ndecomposition, we propose a CP-based low-rank tensor function parameterized by\nneural networks (NN) for implicit neural representation. This approach can\nmodel the tensor both on-grid and beyond grid, fully utilizing the\nnon-linearity of NN with theoretical guarantees on excess risk bounds. To\nachieve sparser CP decomposition, we introduce a variational Schatten-p\nquasi-norm to prune redundant rank-1 components and prove that it serves as a\ncommon upper bound for the Schatten-p quasi-norms of arbitrary unfolding\nmatrices. For smoothness, we propose a regularization term based on the\nspectral norm of the Jacobian and Hutchinson's trace estimator. The proposed\nsmoothness regularization is SVD-free and avoids explicit chain rule\nderivations. It can serve as an alternative to Total Variation (TV)\nregularization in image denoising tasks and is naturally applicable to implicit\nneural representation. Extensive experiments on multi-dimensional data recovery\ntasks, including image inpainting, denoising, and point cloud upsampling,\ndemonstrate the superiority and versatility of our method compared to\nstate-of-the-art approaches. The code is available at\nhttps://github.com/CZY-Code/CP-Pruner.\n", "link": "http://arxiv.org/abs/2506.22134v2", "date": "2025-10-07", "relevancy": 2.5975, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5245}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5232}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Tensor%20Recovery%20via%20Variational%20Schatten-p%20Quasi-Norm%20and%0A%20%20Jacobian%20Regularization&body=Title%3A%20Low-Rank%20Tensor%20Recovery%20via%20Variational%20Schatten-p%20Quasi-Norm%20and%0A%20%20Jacobian%20Regularization%0AAuthor%3A%20Zhengyun%20Cheng%20and%20Ruizhe%20Zhang%20and%20Guanwen%20Zhang%20and%20Yi%20Xu%20and%20Xiangyang%20Ji%20and%20Wei%20Zhou%0AAbstract%3A%20%20%20Higher-order%20tensors%20are%20well-suited%20for%20representing%20multi-dimensional%20data%2C%0Asuch%20as%20images%20and%20videos%2C%20which%20typically%20characterize%20low-rank%20structures.%0ALow-rank%20tensor%20decomposition%20has%20become%20essential%20in%20machine%20learning%20and%0Acomputer%20vision%2C%20but%20existing%20methods%20like%20Tucker%20decomposition%20offer%0Aflexibility%20at%20the%20expense%20of%20interpretability.%20The%20CANDECOMP/PARAFAC%20%28CP%29%0Adecomposition%20provides%20a%20natural%20and%20interpretable%20structure%2C%20while%20obtaining%20a%0Asparse%20solutions%20remains%20challenging.%20Leveraging%20the%20rich%20properties%20of%20CP%0Adecomposition%2C%20we%20propose%20a%20CP-based%20low-rank%20tensor%20function%20parameterized%20by%0Aneural%20networks%20%28NN%29%20for%20implicit%20neural%20representation.%20This%20approach%20can%0Amodel%20the%20tensor%20both%20on-grid%20and%20beyond%20grid%2C%20fully%20utilizing%20the%0Anon-linearity%20of%20NN%20with%20theoretical%20guarantees%20on%20excess%20risk%20bounds.%20To%0Aachieve%20sparser%20CP%20decomposition%2C%20we%20introduce%20a%20variational%20Schatten-p%0Aquasi-norm%20to%20prune%20redundant%20rank-1%20components%20and%20prove%20that%20it%20serves%20as%20a%0Acommon%20upper%20bound%20for%20the%20Schatten-p%20quasi-norms%20of%20arbitrary%20unfolding%0Amatrices.%20For%20smoothness%2C%20we%20propose%20a%20regularization%20term%20based%20on%20the%0Aspectral%20norm%20of%20the%20Jacobian%20and%20Hutchinson%27s%20trace%20estimator.%20The%20proposed%0Asmoothness%20regularization%20is%20SVD-free%20and%20avoids%20explicit%20chain%20rule%0Aderivations.%20It%20can%20serve%20as%20an%20alternative%20to%20Total%20Variation%20%28TV%29%0Aregularization%20in%20image%20denoising%20tasks%20and%20is%20naturally%20applicable%20to%20implicit%0Aneural%20representation.%20Extensive%20experiments%20on%20multi-dimensional%20data%20recovery%0Atasks%2C%20including%20image%20inpainting%2C%20denoising%2C%20and%20point%20cloud%20upsampling%2C%0Ademonstrate%20the%20superiority%20and%20versatility%20of%20our%20method%20compared%20to%0Astate-of-the-art%20approaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/CZY-Code/CP-Pruner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Tensor%2520Recovery%2520via%2520Variational%2520Schatten-p%2520Quasi-Norm%2520and%250A%2520%2520Jacobian%2520Regularization%26entry.906535625%3DZhengyun%2520Cheng%2520and%2520Ruizhe%2520Zhang%2520and%2520Guanwen%2520Zhang%2520and%2520Yi%2520Xu%2520and%2520Xiangyang%2520Ji%2520and%2520Wei%2520Zhou%26entry.1292438233%3D%2520%2520Higher-order%2520tensors%2520are%2520well-suited%2520for%2520representing%2520multi-dimensional%2520data%252C%250Asuch%2520as%2520images%2520and%2520videos%252C%2520which%2520typically%2520characterize%2520low-rank%2520structures.%250ALow-rank%2520tensor%2520decomposition%2520has%2520become%2520essential%2520in%2520machine%2520learning%2520and%250Acomputer%2520vision%252C%2520but%2520existing%2520methods%2520like%2520Tucker%2520decomposition%2520offer%250Aflexibility%2520at%2520the%2520expense%2520of%2520interpretability.%2520The%2520CANDECOMP/PARAFAC%2520%2528CP%2529%250Adecomposition%2520provides%2520a%2520natural%2520and%2520interpretable%2520structure%252C%2520while%2520obtaining%2520a%250Asparse%2520solutions%2520remains%2520challenging.%2520Leveraging%2520the%2520rich%2520properties%2520of%2520CP%250Adecomposition%252C%2520we%2520propose%2520a%2520CP-based%2520low-rank%2520tensor%2520function%2520parameterized%2520by%250Aneural%2520networks%2520%2528NN%2529%2520for%2520implicit%2520neural%2520representation.%2520This%2520approach%2520can%250Amodel%2520the%2520tensor%2520both%2520on-grid%2520and%2520beyond%2520grid%252C%2520fully%2520utilizing%2520the%250Anon-linearity%2520of%2520NN%2520with%2520theoretical%2520guarantees%2520on%2520excess%2520risk%2520bounds.%2520To%250Aachieve%2520sparser%2520CP%2520decomposition%252C%2520we%2520introduce%2520a%2520variational%2520Schatten-p%250Aquasi-norm%2520to%2520prune%2520redundant%2520rank-1%2520components%2520and%2520prove%2520that%2520it%2520serves%2520as%2520a%250Acommon%2520upper%2520bound%2520for%2520the%2520Schatten-p%2520quasi-norms%2520of%2520arbitrary%2520unfolding%250Amatrices.%2520For%2520smoothness%252C%2520we%2520propose%2520a%2520regularization%2520term%2520based%2520on%2520the%250Aspectral%2520norm%2520of%2520the%2520Jacobian%2520and%2520Hutchinson%2527s%2520trace%2520estimator.%2520The%2520proposed%250Asmoothness%2520regularization%2520is%2520SVD-free%2520and%2520avoids%2520explicit%2520chain%2520rule%250Aderivations.%2520It%2520can%2520serve%2520as%2520an%2520alternative%2520to%2520Total%2520Variation%2520%2528TV%2529%250Aregularization%2520in%2520image%2520denoising%2520tasks%2520and%2520is%2520naturally%2520applicable%2520to%2520implicit%250Aneural%2520representation.%2520Extensive%2520experiments%2520on%2520multi-dimensional%2520data%2520recovery%250Atasks%252C%2520including%2520image%2520inpainting%252C%2520denoising%252C%2520and%2520point%2520cloud%2520upsampling%252C%250Ademonstrate%2520the%2520superiority%2520and%2520versatility%2520of%2520our%2520method%2520compared%2520to%250Astate-of-the-art%2520approaches.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/CZY-Code/CP-Pruner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Tensor%20Recovery%20via%20Variational%20Schatten-p%20Quasi-Norm%20and%0A%20%20Jacobian%20Regularization&entry.906535625=Zhengyun%20Cheng%20and%20Ruizhe%20Zhang%20and%20Guanwen%20Zhang%20and%20Yi%20Xu%20and%20Xiangyang%20Ji%20and%20Wei%20Zhou&entry.1292438233=%20%20Higher-order%20tensors%20are%20well-suited%20for%20representing%20multi-dimensional%20data%2C%0Asuch%20as%20images%20and%20videos%2C%20which%20typically%20characterize%20low-rank%20structures.%0ALow-rank%20tensor%20decomposition%20has%20become%20essential%20in%20machine%20learning%20and%0Acomputer%20vision%2C%20but%20existing%20methods%20like%20Tucker%20decomposition%20offer%0Aflexibility%20at%20the%20expense%20of%20interpretability.%20The%20CANDECOMP/PARAFAC%20%28CP%29%0Adecomposition%20provides%20a%20natural%20and%20interpretable%20structure%2C%20while%20obtaining%20a%0Asparse%20solutions%20remains%20challenging.%20Leveraging%20the%20rich%20properties%20of%20CP%0Adecomposition%2C%20we%20propose%20a%20CP-based%20low-rank%20tensor%20function%20parameterized%20by%0Aneural%20networks%20%28NN%29%20for%20implicit%20neural%20representation.%20This%20approach%20can%0Amodel%20the%20tensor%20both%20on-grid%20and%20beyond%20grid%2C%20fully%20utilizing%20the%0Anon-linearity%20of%20NN%20with%20theoretical%20guarantees%20on%20excess%20risk%20bounds.%20To%0Aachieve%20sparser%20CP%20decomposition%2C%20we%20introduce%20a%20variational%20Schatten-p%0Aquasi-norm%20to%20prune%20redundant%20rank-1%20components%20and%20prove%20that%20it%20serves%20as%20a%0Acommon%20upper%20bound%20for%20the%20Schatten-p%20quasi-norms%20of%20arbitrary%20unfolding%0Amatrices.%20For%20smoothness%2C%20we%20propose%20a%20regularization%20term%20based%20on%20the%0Aspectral%20norm%20of%20the%20Jacobian%20and%20Hutchinson%27s%20trace%20estimator.%20The%20proposed%0Asmoothness%20regularization%20is%20SVD-free%20and%20avoids%20explicit%20chain%20rule%0Aderivations.%20It%20can%20serve%20as%20an%20alternative%20to%20Total%20Variation%20%28TV%29%0Aregularization%20in%20image%20denoising%20tasks%20and%20is%20naturally%20applicable%20to%20implicit%0Aneural%20representation.%20Extensive%20experiments%20on%20multi-dimensional%20data%20recovery%0Atasks%2C%20including%20image%20inpainting%2C%20denoising%2C%20and%20point%20cloud%20upsampling%2C%0Ademonstrate%20the%20superiority%20and%20versatility%20of%20our%20method%20compared%20to%0Astate-of-the-art%20approaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/CZY-Code/CP-Pruner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22134v2&entry.124074799=Read"},
{"title": "RICO: Two Realistic Benchmarks and an In-Depth Analysis for Incremental\n  Learning in Object Detection", "author": "Matthias Neuwirth-Trapp and Maarten Bieshaar and Danda Pani Paudel and Luc Van Gool", "abstract": "  Incremental Learning (IL) trains models sequentially on new data without full\nretraining, offering privacy, efficiency, and scalability. IL must balance\nadaptability to new data with retention of old knowledge. However, evaluations\noften rely on synthetic, simplified benchmarks, obscuring real-world IL\nperformance. To address this, we introduce two Realistic Incremental Object\nDetection Benchmarks (RICO): Domain RICO (D-RICO) features domain shifts with a\nfixed class set, and Expanding-Classes RICO (EC-RICO) integrates new domains\nand classes per IL step. Built from 14 diverse datasets covering real and\nsynthetic domains, varying conditions (e.g., weather, time of day), camera\nsensors, perspectives, and labeling policies, both benchmarks capture\nchallenges absent in existing evaluations. Our experiments show that all IL\nmethods underperform in adaptability and retention, while replaying a small\namount of previous data already outperforms all methods. However, individual\ntraining on the data remains superior. We heuristically attribute this gap to\nweak teachers in distillation, single models' inability to manage diverse\ntasks, and insufficient plasticity. Our code will be made publicly available.\n", "link": "http://arxiv.org/abs/2508.13878v2", "date": "2025-10-07", "relevancy": 2.5917, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RICO%3A%20Two%20Realistic%20Benchmarks%20and%20an%20In-Depth%20Analysis%20for%20Incremental%0A%20%20Learning%20in%20Object%20Detection&body=Title%3A%20RICO%3A%20Two%20Realistic%20Benchmarks%20and%20an%20In-Depth%20Analysis%20for%20Incremental%0A%20%20Learning%20in%20Object%20Detection%0AAuthor%3A%20Matthias%20Neuwirth-Trapp%20and%20Maarten%20Bieshaar%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Incremental%20Learning%20%28IL%29%20trains%20models%20sequentially%20on%20new%20data%20without%20full%0Aretraining%2C%20offering%20privacy%2C%20efficiency%2C%20and%20scalability.%20IL%20must%20balance%0Aadaptability%20to%20new%20data%20with%20retention%20of%20old%20knowledge.%20However%2C%20evaluations%0Aoften%20rely%20on%20synthetic%2C%20simplified%20benchmarks%2C%20obscuring%20real-world%20IL%0Aperformance.%20To%20address%20this%2C%20we%20introduce%20two%20Realistic%20Incremental%20Object%0ADetection%20Benchmarks%20%28RICO%29%3A%20Domain%20RICO%20%28D-RICO%29%20features%20domain%20shifts%20with%20a%0Afixed%20class%20set%2C%20and%20Expanding-Classes%20RICO%20%28EC-RICO%29%20integrates%20new%20domains%0Aand%20classes%20per%20IL%20step.%20Built%20from%2014%20diverse%20datasets%20covering%20real%20and%0Asynthetic%20domains%2C%20varying%20conditions%20%28e.g.%2C%20weather%2C%20time%20of%20day%29%2C%20camera%0Asensors%2C%20perspectives%2C%20and%20labeling%20policies%2C%20both%20benchmarks%20capture%0Achallenges%20absent%20in%20existing%20evaluations.%20Our%20experiments%20show%20that%20all%20IL%0Amethods%20underperform%20in%20adaptability%20and%20retention%2C%20while%20replaying%20a%20small%0Aamount%20of%20previous%20data%20already%20outperforms%20all%20methods.%20However%2C%20individual%0Atraining%20on%20the%20data%20remains%20superior.%20We%20heuristically%20attribute%20this%20gap%20to%0Aweak%20teachers%20in%20distillation%2C%20single%20models%27%20inability%20to%20manage%20diverse%0Atasks%2C%20and%20insufficient%20plasticity.%20Our%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13878v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRICO%253A%2520Two%2520Realistic%2520Benchmarks%2520and%2520an%2520In-Depth%2520Analysis%2520for%2520Incremental%250A%2520%2520Learning%2520in%2520Object%2520Detection%26entry.906535625%3DMatthias%2520Neuwirth-Trapp%2520and%2520Maarten%2520Bieshaar%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Incremental%2520Learning%2520%2528IL%2529%2520trains%2520models%2520sequentially%2520on%2520new%2520data%2520without%2520full%250Aretraining%252C%2520offering%2520privacy%252C%2520efficiency%252C%2520and%2520scalability.%2520IL%2520must%2520balance%250Aadaptability%2520to%2520new%2520data%2520with%2520retention%2520of%2520old%2520knowledge.%2520However%252C%2520evaluations%250Aoften%2520rely%2520on%2520synthetic%252C%2520simplified%2520benchmarks%252C%2520obscuring%2520real-world%2520IL%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520introduce%2520two%2520Realistic%2520Incremental%2520Object%250ADetection%2520Benchmarks%2520%2528RICO%2529%253A%2520Domain%2520RICO%2520%2528D-RICO%2529%2520features%2520domain%2520shifts%2520with%2520a%250Afixed%2520class%2520set%252C%2520and%2520Expanding-Classes%2520RICO%2520%2528EC-RICO%2529%2520integrates%2520new%2520domains%250Aand%2520classes%2520per%2520IL%2520step.%2520Built%2520from%252014%2520diverse%2520datasets%2520covering%2520real%2520and%250Asynthetic%2520domains%252C%2520varying%2520conditions%2520%2528e.g.%252C%2520weather%252C%2520time%2520of%2520day%2529%252C%2520camera%250Asensors%252C%2520perspectives%252C%2520and%2520labeling%2520policies%252C%2520both%2520benchmarks%2520capture%250Achallenges%2520absent%2520in%2520existing%2520evaluations.%2520Our%2520experiments%2520show%2520that%2520all%2520IL%250Amethods%2520underperform%2520in%2520adaptability%2520and%2520retention%252C%2520while%2520replaying%2520a%2520small%250Aamount%2520of%2520previous%2520data%2520already%2520outperforms%2520all%2520methods.%2520However%252C%2520individual%250Atraining%2520on%2520the%2520data%2520remains%2520superior.%2520We%2520heuristically%2520attribute%2520this%2520gap%2520to%250Aweak%2520teachers%2520in%2520distillation%252C%2520single%2520models%2527%2520inability%2520to%2520manage%2520diverse%250Atasks%252C%2520and%2520insufficient%2520plasticity.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13878v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RICO%3A%20Two%20Realistic%20Benchmarks%20and%20an%20In-Depth%20Analysis%20for%20Incremental%0A%20%20Learning%20in%20Object%20Detection&entry.906535625=Matthias%20Neuwirth-Trapp%20and%20Maarten%20Bieshaar%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Incremental%20Learning%20%28IL%29%20trains%20models%20sequentially%20on%20new%20data%20without%20full%0Aretraining%2C%20offering%20privacy%2C%20efficiency%2C%20and%20scalability.%20IL%20must%20balance%0Aadaptability%20to%20new%20data%20with%20retention%20of%20old%20knowledge.%20However%2C%20evaluations%0Aoften%20rely%20on%20synthetic%2C%20simplified%20benchmarks%2C%20obscuring%20real-world%20IL%0Aperformance.%20To%20address%20this%2C%20we%20introduce%20two%20Realistic%20Incremental%20Object%0ADetection%20Benchmarks%20%28RICO%29%3A%20Domain%20RICO%20%28D-RICO%29%20features%20domain%20shifts%20with%20a%0Afixed%20class%20set%2C%20and%20Expanding-Classes%20RICO%20%28EC-RICO%29%20integrates%20new%20domains%0Aand%20classes%20per%20IL%20step.%20Built%20from%2014%20diverse%20datasets%20covering%20real%20and%0Asynthetic%20domains%2C%20varying%20conditions%20%28e.g.%2C%20weather%2C%20time%20of%20day%29%2C%20camera%0Asensors%2C%20perspectives%2C%20and%20labeling%20policies%2C%20both%20benchmarks%20capture%0Achallenges%20absent%20in%20existing%20evaluations.%20Our%20experiments%20show%20that%20all%20IL%0Amethods%20underperform%20in%20adaptability%20and%20retention%2C%20while%20replaying%20a%20small%0Aamount%20of%20previous%20data%20already%20outperforms%20all%20methods.%20However%2C%20individual%0Atraining%20on%20the%20data%20remains%20superior.%20We%20heuristically%20attribute%20this%20gap%20to%0Aweak%20teachers%20in%20distillation%2C%20single%20models%27%20inability%20to%20manage%20diverse%0Atasks%2C%20and%20insufficient%20plasticity.%20Our%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13878v2&entry.124074799=Read"},
{"title": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive\n  Evaluation of Chinese LLMs", "author": "Chengwei Wu and Jiapu Wang and Mingyang Gao and Xingrui Zhuo and Jipeng Guo and Runlin Lei and Haoran Luo and Tianyu Chen and Haoyi Zhou and Shirui Pan and Zechao Li", "abstract": "  Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language processing tasks. However, Chinese LLMs face unique\nchallenges, primarily due to the dominance of unstructured free text and the\nlack of structured representations in Chinese corpora. While existing\nbenchmarks for LLMs partially assess Chinese LLMs, they are still predominantly\nEnglish-centric and fail to address the unique linguistic characteristics of\nChinese, lacking structured datasets essential for robust evaluation. To\naddress these challenges, we present a Comprehensive Benchmark for Evaluating\nChinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese\nData-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million\naligned text pairs, each consisting of unstructured text coupled with one or\nmore corresponding triples, alongside a total of 15 million triples spanning\nfour critical domains. The core contributions of CDTP are threefold: (i)\nenriching Chinese corpora with high-quality structured information; (ii)\nenabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)\nsupporting multi-task fine-tuning to assess generalization and robustness\nacross scenarios, including Knowledge Graph Completion, Triple-to-Text\ngeneration, and Question Answering. Furthermore, we conduct rigorous\nevaluations through extensive experiments and ablation studies to assess the\neffectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.\nTo support reproducible research, we offer an open-source codebase and outline\npotential directions for future investigations based on our insights.\n", "link": "http://arxiv.org/abs/2510.06039v1", "date": "2025-10-07", "relevancy": 2.5888, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDTP%3A%20A%20Large-Scale%20Chinese%20Data-Text%20Pair%20Dataset%20for%20Comprehensive%0A%20%20Evaluation%20of%20Chinese%20LLMs&body=Title%3A%20CDTP%3A%20A%20Large-Scale%20Chinese%20Data-Text%20Pair%20Dataset%20for%20Comprehensive%0A%20%20Evaluation%20of%20Chinese%20LLMs%0AAuthor%3A%20Chengwei%20Wu%20and%20Jiapu%20Wang%20and%20Mingyang%20Gao%20and%20Xingrui%20Zhuo%20and%20Jipeng%20Guo%20and%20Runlin%20Lei%20and%20Haoran%20Luo%20and%20Tianyu%20Chen%20and%20Haoyi%20Zhou%20and%20Shirui%20Pan%20and%20Zechao%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20a%20wide%0Arange%20of%20natural%20language%20processing%20tasks.%20However%2C%20Chinese%20LLMs%20face%20unique%0Achallenges%2C%20primarily%20due%20to%20the%20dominance%20of%20unstructured%20free%20text%20and%20the%0Alack%20of%20structured%20representations%20in%20Chinese%20corpora.%20While%20existing%0Abenchmarks%20for%20LLMs%20partially%20assess%20Chinese%20LLMs%2C%20they%20are%20still%20predominantly%0AEnglish-centric%20and%20fail%20to%20address%20the%20unique%20linguistic%20characteristics%20of%0AChinese%2C%20lacking%20structured%20datasets%20essential%20for%20robust%20evaluation.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20a%20Comprehensive%20Benchmark%20for%20Evaluating%0AChinese%20Large%20Language%20Models%20%28CB-ECLLM%29%20based%20on%20the%20newly%20constructed%20Chinese%0AData-Text%20Pair%20%28CDTP%29%20dataset.%20Specifically%2C%20CDTP%20comprises%20over%207%20million%0Aaligned%20text%20pairs%2C%20each%20consisting%20of%20unstructured%20text%20coupled%20with%20one%20or%0Amore%20corresponding%20triples%2C%20alongside%20a%20total%20of%2015%20million%20triples%20spanning%0Afour%20critical%20domains.%20The%20core%20contributions%20of%20CDTP%20are%20threefold%3A%20%28i%29%0Aenriching%20Chinese%20corpora%20with%20high-quality%20structured%20information%3B%20%28ii%29%0Aenabling%20fine-grained%20evaluation%20tailored%20to%20knowledge-driven%20tasks%3B%20and%20%28iii%29%0Asupporting%20multi-task%20fine-tuning%20to%20assess%20generalization%20and%20robustness%0Aacross%20scenarios%2C%20including%20Knowledge%20Graph%20Completion%2C%20Triple-to-Text%0Ageneration%2C%20and%20Question%20Answering.%20Furthermore%2C%20we%20conduct%20rigorous%0Aevaluations%20through%20extensive%20experiments%20and%20ablation%20studies%20to%20assess%20the%0Aeffectiveness%2C%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20and%20robustness%20of%20the%20benchmark.%0ATo%20support%20reproducible%20research%2C%20we%20offer%20an%20open-source%20codebase%20and%20outline%0Apotential%20directions%20for%20future%20investigations%20based%20on%20our%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDTP%253A%2520A%2520Large-Scale%2520Chinese%2520Data-Text%2520Pair%2520Dataset%2520for%2520Comprehensive%250A%2520%2520Evaluation%2520of%2520Chinese%2520LLMs%26entry.906535625%3DChengwei%2520Wu%2520and%2520Jiapu%2520Wang%2520and%2520Mingyang%2520Gao%2520and%2520Xingrui%2520Zhuo%2520and%2520Jipeng%2520Guo%2520and%2520Runlin%2520Lei%2520and%2520Haoran%2520Luo%2520and%2520Tianyu%2520Chen%2520and%2520Haoyi%2520Zhou%2520and%2520Shirui%2520Pan%2520and%2520Zechao%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520a%2520wide%250Arange%2520of%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520Chinese%2520LLMs%2520face%2520unique%250Achallenges%252C%2520primarily%2520due%2520to%2520the%2520dominance%2520of%2520unstructured%2520free%2520text%2520and%2520the%250Alack%2520of%2520structured%2520representations%2520in%2520Chinese%2520corpora.%2520While%2520existing%250Abenchmarks%2520for%2520LLMs%2520partially%2520assess%2520Chinese%2520LLMs%252C%2520they%2520are%2520still%2520predominantly%250AEnglish-centric%2520and%2520fail%2520to%2520address%2520the%2520unique%2520linguistic%2520characteristics%2520of%250AChinese%252C%2520lacking%2520structured%2520datasets%2520essential%2520for%2520robust%2520evaluation.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520present%2520a%2520Comprehensive%2520Benchmark%2520for%2520Evaluating%250AChinese%2520Large%2520Language%2520Models%2520%2528CB-ECLLM%2529%2520based%2520on%2520the%2520newly%2520constructed%2520Chinese%250AData-Text%2520Pair%2520%2528CDTP%2529%2520dataset.%2520Specifically%252C%2520CDTP%2520comprises%2520over%25207%2520million%250Aaligned%2520text%2520pairs%252C%2520each%2520consisting%2520of%2520unstructured%2520text%2520coupled%2520with%2520one%2520or%250Amore%2520corresponding%2520triples%252C%2520alongside%2520a%2520total%2520of%252015%2520million%2520triples%2520spanning%250Afour%2520critical%2520domains.%2520The%2520core%2520contributions%2520of%2520CDTP%2520are%2520threefold%253A%2520%2528i%2529%250Aenriching%2520Chinese%2520corpora%2520with%2520high-quality%2520structured%2520information%253B%2520%2528ii%2529%250Aenabling%2520fine-grained%2520evaluation%2520tailored%2520to%2520knowledge-driven%2520tasks%253B%2520and%2520%2528iii%2529%250Asupporting%2520multi-task%2520fine-tuning%2520to%2520assess%2520generalization%2520and%2520robustness%250Aacross%2520scenarios%252C%2520including%2520Knowledge%2520Graph%2520Completion%252C%2520Triple-to-Text%250Ageneration%252C%2520and%2520Question%2520Answering.%2520Furthermore%252C%2520we%2520conduct%2520rigorous%250Aevaluations%2520through%2520extensive%2520experiments%2520and%2520ablation%2520studies%2520to%2520assess%2520the%250Aeffectiveness%252C%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%252C%2520and%2520robustness%2520of%2520the%2520benchmark.%250ATo%2520support%2520reproducible%2520research%252C%2520we%2520offer%2520an%2520open-source%2520codebase%2520and%2520outline%250Apotential%2520directions%2520for%2520future%2520investigations%2520based%2520on%2520our%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDTP%3A%20A%20Large-Scale%20Chinese%20Data-Text%20Pair%20Dataset%20for%20Comprehensive%0A%20%20Evaluation%20of%20Chinese%20LLMs&entry.906535625=Chengwei%20Wu%20and%20Jiapu%20Wang%20and%20Mingyang%20Gao%20and%20Xingrui%20Zhuo%20and%20Jipeng%20Guo%20and%20Runlin%20Lei%20and%20Haoran%20Luo%20and%20Tianyu%20Chen%20and%20Haoyi%20Zhou%20and%20Shirui%20Pan%20and%20Zechao%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20a%20wide%0Arange%20of%20natural%20language%20processing%20tasks.%20However%2C%20Chinese%20LLMs%20face%20unique%0Achallenges%2C%20primarily%20due%20to%20the%20dominance%20of%20unstructured%20free%20text%20and%20the%0Alack%20of%20structured%20representations%20in%20Chinese%20corpora.%20While%20existing%0Abenchmarks%20for%20LLMs%20partially%20assess%20Chinese%20LLMs%2C%20they%20are%20still%20predominantly%0AEnglish-centric%20and%20fail%20to%20address%20the%20unique%20linguistic%20characteristics%20of%0AChinese%2C%20lacking%20structured%20datasets%20essential%20for%20robust%20evaluation.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20a%20Comprehensive%20Benchmark%20for%20Evaluating%0AChinese%20Large%20Language%20Models%20%28CB-ECLLM%29%20based%20on%20the%20newly%20constructed%20Chinese%0AData-Text%20Pair%20%28CDTP%29%20dataset.%20Specifically%2C%20CDTP%20comprises%20over%207%20million%0Aaligned%20text%20pairs%2C%20each%20consisting%20of%20unstructured%20text%20coupled%20with%20one%20or%0Amore%20corresponding%20triples%2C%20alongside%20a%20total%20of%2015%20million%20triples%20spanning%0Afour%20critical%20domains.%20The%20core%20contributions%20of%20CDTP%20are%20threefold%3A%20%28i%29%0Aenriching%20Chinese%20corpora%20with%20high-quality%20structured%20information%3B%20%28ii%29%0Aenabling%20fine-grained%20evaluation%20tailored%20to%20knowledge-driven%20tasks%3B%20and%20%28iii%29%0Asupporting%20multi-task%20fine-tuning%20to%20assess%20generalization%20and%20robustness%0Aacross%20scenarios%2C%20including%20Knowledge%20Graph%20Completion%2C%20Triple-to-Text%0Ageneration%2C%20and%20Question%20Answering.%20Furthermore%2C%20we%20conduct%20rigorous%0Aevaluations%20through%20extensive%20experiments%20and%20ablation%20studies%20to%20assess%20the%0Aeffectiveness%2C%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20and%20robustness%20of%20the%20benchmark.%0ATo%20support%20reproducible%20research%2C%20we%20offer%20an%20open-source%20codebase%20and%20outline%0Apotential%20directions%20for%20future%20investigations%20based%20on%20our%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06039v1&entry.124074799=Read"},
{"title": "A Warm-basis Method for Bridging Learning and Iteration: a Case Study in\n  Fluorescence Molecular Tomography", "author": "Ruchi Guo and Jiahua Jiang and Bangti Jin and Wuwei Ren and Jianru Zhang", "abstract": "  Fluorescence Molecular Tomography (FMT) is a widely used non-invasive optical\nimaging technology in biomedical research. It usually faces significant\naccuracy challenges in depth reconstruction, and conventional iterative methods\nstruggle with poor $z$-resolution even with advanced regularization. Supervised\nlearning approaches can improve recovery accuracy but rely on large,\nhigh-quality paired training dataset that is often impractical to acquire in\npractice. This naturally raises the question of how learning-based approaches\ncan be effectively combined with iterative schemes to yield more accurate and\nstable algorithms. In this work, we present a novel warm-basis iterative\nprojection method (WB-IPM) and establish its theoretical underpinnings. The\nmethod is able to achieve significantly more accurate reconstructions than the\nlearning-based and iterative-based methods. In addition, it allows a weaker\nloss function depending solely on the directional component of the difference\nbetween ground truth and neural network output, thereby substantially reducing\nthe training effort. These features are justified by our error analysis as well\nas simulated and real-data experiments.\n", "link": "http://arxiv.org/abs/2510.05926v1", "date": "2025-10-07", "relevancy": 2.5843, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5106}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Warm-basis%20Method%20for%20Bridging%20Learning%20and%20Iteration%3A%20a%20Case%20Study%20in%0A%20%20Fluorescence%20Molecular%20Tomography&body=Title%3A%20A%20Warm-basis%20Method%20for%20Bridging%20Learning%20and%20Iteration%3A%20a%20Case%20Study%20in%0A%20%20Fluorescence%20Molecular%20Tomography%0AAuthor%3A%20Ruchi%20Guo%20and%20Jiahua%20Jiang%20and%20Bangti%20Jin%20and%20Wuwei%20Ren%20and%20Jianru%20Zhang%0AAbstract%3A%20%20%20Fluorescence%20Molecular%20Tomography%20%28FMT%29%20is%20a%20widely%20used%20non-invasive%20optical%0Aimaging%20technology%20in%20biomedical%20research.%20It%20usually%20faces%20significant%0Aaccuracy%20challenges%20in%20depth%20reconstruction%2C%20and%20conventional%20iterative%20methods%0Astruggle%20with%20poor%20%24z%24-resolution%20even%20with%20advanced%20regularization.%20Supervised%0Alearning%20approaches%20can%20improve%20recovery%20accuracy%20but%20rely%20on%20large%2C%0Ahigh-quality%20paired%20training%20dataset%20that%20is%20often%20impractical%20to%20acquire%20in%0Apractice.%20This%20naturally%20raises%20the%20question%20of%20how%20learning-based%20approaches%0Acan%20be%20effectively%20combined%20with%20iterative%20schemes%20to%20yield%20more%20accurate%20and%0Astable%20algorithms.%20In%20this%20work%2C%20we%20present%20a%20novel%20warm-basis%20iterative%0Aprojection%20method%20%28WB-IPM%29%20and%20establish%20its%20theoretical%20underpinnings.%20The%0Amethod%20is%20able%20to%20achieve%20significantly%20more%20accurate%20reconstructions%20than%20the%0Alearning-based%20and%20iterative-based%20methods.%20In%20addition%2C%20it%20allows%20a%20weaker%0Aloss%20function%20depending%20solely%20on%20the%20directional%20component%20of%20the%20difference%0Abetween%20ground%20truth%20and%20neural%20network%20output%2C%20thereby%20substantially%20reducing%0Athe%20training%20effort.%20These%20features%20are%20justified%20by%20our%20error%20analysis%20as%20well%0Aas%20simulated%20and%20real-data%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Warm-basis%2520Method%2520for%2520Bridging%2520Learning%2520and%2520Iteration%253A%2520a%2520Case%2520Study%2520in%250A%2520%2520Fluorescence%2520Molecular%2520Tomography%26entry.906535625%3DRuchi%2520Guo%2520and%2520Jiahua%2520Jiang%2520and%2520Bangti%2520Jin%2520and%2520Wuwei%2520Ren%2520and%2520Jianru%2520Zhang%26entry.1292438233%3D%2520%2520Fluorescence%2520Molecular%2520Tomography%2520%2528FMT%2529%2520is%2520a%2520widely%2520used%2520non-invasive%2520optical%250Aimaging%2520technology%2520in%2520biomedical%2520research.%2520It%2520usually%2520faces%2520significant%250Aaccuracy%2520challenges%2520in%2520depth%2520reconstruction%252C%2520and%2520conventional%2520iterative%2520methods%250Astruggle%2520with%2520poor%2520%2524z%2524-resolution%2520even%2520with%2520advanced%2520regularization.%2520Supervised%250Alearning%2520approaches%2520can%2520improve%2520recovery%2520accuracy%2520but%2520rely%2520on%2520large%252C%250Ahigh-quality%2520paired%2520training%2520dataset%2520that%2520is%2520often%2520impractical%2520to%2520acquire%2520in%250Apractice.%2520This%2520naturally%2520raises%2520the%2520question%2520of%2520how%2520learning-based%2520approaches%250Acan%2520be%2520effectively%2520combined%2520with%2520iterative%2520schemes%2520to%2520yield%2520more%2520accurate%2520and%250Astable%2520algorithms.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520warm-basis%2520iterative%250Aprojection%2520method%2520%2528WB-IPM%2529%2520and%2520establish%2520its%2520theoretical%2520underpinnings.%2520The%250Amethod%2520is%2520able%2520to%2520achieve%2520significantly%2520more%2520accurate%2520reconstructions%2520than%2520the%250Alearning-based%2520and%2520iterative-based%2520methods.%2520In%2520addition%252C%2520it%2520allows%2520a%2520weaker%250Aloss%2520function%2520depending%2520solely%2520on%2520the%2520directional%2520component%2520of%2520the%2520difference%250Abetween%2520ground%2520truth%2520and%2520neural%2520network%2520output%252C%2520thereby%2520substantially%2520reducing%250Athe%2520training%2520effort.%2520These%2520features%2520are%2520justified%2520by%2520our%2520error%2520analysis%2520as%2520well%250Aas%2520simulated%2520and%2520real-data%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Warm-basis%20Method%20for%20Bridging%20Learning%20and%20Iteration%3A%20a%20Case%20Study%20in%0A%20%20Fluorescence%20Molecular%20Tomography&entry.906535625=Ruchi%20Guo%20and%20Jiahua%20Jiang%20and%20Bangti%20Jin%20and%20Wuwei%20Ren%20and%20Jianru%20Zhang&entry.1292438233=%20%20Fluorescence%20Molecular%20Tomography%20%28FMT%29%20is%20a%20widely%20used%20non-invasive%20optical%0Aimaging%20technology%20in%20biomedical%20research.%20It%20usually%20faces%20significant%0Aaccuracy%20challenges%20in%20depth%20reconstruction%2C%20and%20conventional%20iterative%20methods%0Astruggle%20with%20poor%20%24z%24-resolution%20even%20with%20advanced%20regularization.%20Supervised%0Alearning%20approaches%20can%20improve%20recovery%20accuracy%20but%20rely%20on%20large%2C%0Ahigh-quality%20paired%20training%20dataset%20that%20is%20often%20impractical%20to%20acquire%20in%0Apractice.%20This%20naturally%20raises%20the%20question%20of%20how%20learning-based%20approaches%0Acan%20be%20effectively%20combined%20with%20iterative%20schemes%20to%20yield%20more%20accurate%20and%0Astable%20algorithms.%20In%20this%20work%2C%20we%20present%20a%20novel%20warm-basis%20iterative%0Aprojection%20method%20%28WB-IPM%29%20and%20establish%20its%20theoretical%20underpinnings.%20The%0Amethod%20is%20able%20to%20achieve%20significantly%20more%20accurate%20reconstructions%20than%20the%0Alearning-based%20and%20iterative-based%20methods.%20In%20addition%2C%20it%20allows%20a%20weaker%0Aloss%20function%20depending%20solely%20on%20the%20directional%20component%20of%20the%20difference%0Abetween%20ground%20truth%20and%20neural%20network%20output%2C%20thereby%20substantially%20reducing%0Athe%20training%20effort.%20These%20features%20are%20justified%20by%20our%20error%20analysis%20as%20well%0Aas%20simulated%20and%20real-data%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05926v1&entry.124074799=Read"},
{"title": "Conformalized Gaussian processes for online uncertainty quantification\n  over graphs", "author": "Jinwen Xu and Qin Lu and Georgios B. Giannakis", "abstract": "  Uncertainty quantification (UQ) over graphs arises in a number of\nsafety-critical applications in network science. The Gaussian process (GP), as\na classical Bayesian framework for UQ, has been developed to handle\ngraph-structured data by devising topology-aware kernel functions. However,\nsuch GP-based approaches are limited not only by the prohibitive computational\ncomplexity, but also the strict modeling assumptions that might yield poor\ncoverage, especially with labels arriving on the fly. To effect scalability, we\ndevise a novel graph-aware parametric GP model by leveraging the random feature\n(RF)-based kernel approximation, which is amenable to efficient recursive\nBayesian model updates. To further allow for adaptivity, an ensemble of\ngraph-aware RF-based scalable GPs have been leveraged, with per-GP weight\nadapted to data arriving incrementally. To ensure valid coverage with\nrobustness to model mis-specification, we wed the GP-based set predictors with\nthe online conformal prediction framework, which post-processes the prediction\nsets using adaptive thresholds. Experimental results the proposed method yields\nimproved coverage and efficient prediction sets over existing baselines by\nadaptively ensembling the GP models and setting the key threshold parameters in\nCP.\n", "link": "http://arxiv.org/abs/2510.06181v1", "date": "2025-10-07", "relevancy": 2.5699, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5368}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5127}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformalized%20Gaussian%20processes%20for%20online%20uncertainty%20quantification%0A%20%20over%20graphs&body=Title%3A%20Conformalized%20Gaussian%20processes%20for%20online%20uncertainty%20quantification%0A%20%20over%20graphs%0AAuthor%3A%20Jinwen%20Xu%20and%20Qin%20Lu%20and%20Georgios%20B.%20Giannakis%0AAbstract%3A%20%20%20Uncertainty%20quantification%20%28UQ%29%20over%20graphs%20arises%20in%20a%20number%20of%0Asafety-critical%20applications%20in%20network%20science.%20The%20Gaussian%20process%20%28GP%29%2C%20as%0Aa%20classical%20Bayesian%20framework%20for%20UQ%2C%20has%20been%20developed%20to%20handle%0Agraph-structured%20data%20by%20devising%20topology-aware%20kernel%20functions.%20However%2C%0Asuch%20GP-based%20approaches%20are%20limited%20not%20only%20by%20the%20prohibitive%20computational%0Acomplexity%2C%20but%20also%20the%20strict%20modeling%20assumptions%20that%20might%20yield%20poor%0Acoverage%2C%20especially%20with%20labels%20arriving%20on%20the%20fly.%20To%20effect%20scalability%2C%20we%0Adevise%20a%20novel%20graph-aware%20parametric%20GP%20model%20by%20leveraging%20the%20random%20feature%0A%28RF%29-based%20kernel%20approximation%2C%20which%20is%20amenable%20to%20efficient%20recursive%0ABayesian%20model%20updates.%20To%20further%20allow%20for%20adaptivity%2C%20an%20ensemble%20of%0Agraph-aware%20RF-based%20scalable%20GPs%20have%20been%20leveraged%2C%20with%20per-GP%20weight%0Aadapted%20to%20data%20arriving%20incrementally.%20To%20ensure%20valid%20coverage%20with%0Arobustness%20to%20model%20mis-specification%2C%20we%20wed%20the%20GP-based%20set%20predictors%20with%0Athe%20online%20conformal%20prediction%20framework%2C%20which%20post-processes%20the%20prediction%0Asets%20using%20adaptive%20thresholds.%20Experimental%20results%20the%20proposed%20method%20yields%0Aimproved%20coverage%20and%20efficient%20prediction%20sets%20over%20existing%20baselines%20by%0Aadaptively%20ensembling%20the%20GP%20models%20and%20setting%20the%20key%20threshold%20parameters%20in%0ACP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformalized%2520Gaussian%2520processes%2520for%2520online%2520uncertainty%2520quantification%250A%2520%2520over%2520graphs%26entry.906535625%3DJinwen%2520Xu%2520and%2520Qin%2520Lu%2520and%2520Georgios%2520B.%2520Giannakis%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520%2528UQ%2529%2520over%2520graphs%2520arises%2520in%2520a%2520number%2520of%250Asafety-critical%2520applications%2520in%2520network%2520science.%2520The%2520Gaussian%2520process%2520%2528GP%2529%252C%2520as%250Aa%2520classical%2520Bayesian%2520framework%2520for%2520UQ%252C%2520has%2520been%2520developed%2520to%2520handle%250Agraph-structured%2520data%2520by%2520devising%2520topology-aware%2520kernel%2520functions.%2520However%252C%250Asuch%2520GP-based%2520approaches%2520are%2520limited%2520not%2520only%2520by%2520the%2520prohibitive%2520computational%250Acomplexity%252C%2520but%2520also%2520the%2520strict%2520modeling%2520assumptions%2520that%2520might%2520yield%2520poor%250Acoverage%252C%2520especially%2520with%2520labels%2520arriving%2520on%2520the%2520fly.%2520To%2520effect%2520scalability%252C%2520we%250Adevise%2520a%2520novel%2520graph-aware%2520parametric%2520GP%2520model%2520by%2520leveraging%2520the%2520random%2520feature%250A%2528RF%2529-based%2520kernel%2520approximation%252C%2520which%2520is%2520amenable%2520to%2520efficient%2520recursive%250ABayesian%2520model%2520updates.%2520To%2520further%2520allow%2520for%2520adaptivity%252C%2520an%2520ensemble%2520of%250Agraph-aware%2520RF-based%2520scalable%2520GPs%2520have%2520been%2520leveraged%252C%2520with%2520per-GP%2520weight%250Aadapted%2520to%2520data%2520arriving%2520incrementally.%2520To%2520ensure%2520valid%2520coverage%2520with%250Arobustness%2520to%2520model%2520mis-specification%252C%2520we%2520wed%2520the%2520GP-based%2520set%2520predictors%2520with%250Athe%2520online%2520conformal%2520prediction%2520framework%252C%2520which%2520post-processes%2520the%2520prediction%250Asets%2520using%2520adaptive%2520thresholds.%2520Experimental%2520results%2520the%2520proposed%2520method%2520yields%250Aimproved%2520coverage%2520and%2520efficient%2520prediction%2520sets%2520over%2520existing%2520baselines%2520by%250Aadaptively%2520ensembling%2520the%2520GP%2520models%2520and%2520setting%2520the%2520key%2520threshold%2520parameters%2520in%250ACP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformalized%20Gaussian%20processes%20for%20online%20uncertainty%20quantification%0A%20%20over%20graphs&entry.906535625=Jinwen%20Xu%20and%20Qin%20Lu%20and%20Georgios%20B.%20Giannakis&entry.1292438233=%20%20Uncertainty%20quantification%20%28UQ%29%20over%20graphs%20arises%20in%20a%20number%20of%0Asafety-critical%20applications%20in%20network%20science.%20The%20Gaussian%20process%20%28GP%29%2C%20as%0Aa%20classical%20Bayesian%20framework%20for%20UQ%2C%20has%20been%20developed%20to%20handle%0Agraph-structured%20data%20by%20devising%20topology-aware%20kernel%20functions.%20However%2C%0Asuch%20GP-based%20approaches%20are%20limited%20not%20only%20by%20the%20prohibitive%20computational%0Acomplexity%2C%20but%20also%20the%20strict%20modeling%20assumptions%20that%20might%20yield%20poor%0Acoverage%2C%20especially%20with%20labels%20arriving%20on%20the%20fly.%20To%20effect%20scalability%2C%20we%0Adevise%20a%20novel%20graph-aware%20parametric%20GP%20model%20by%20leveraging%20the%20random%20feature%0A%28RF%29-based%20kernel%20approximation%2C%20which%20is%20amenable%20to%20efficient%20recursive%0ABayesian%20model%20updates.%20To%20further%20allow%20for%20adaptivity%2C%20an%20ensemble%20of%0Agraph-aware%20RF-based%20scalable%20GPs%20have%20been%20leveraged%2C%20with%20per-GP%20weight%0Aadapted%20to%20data%20arriving%20incrementally.%20To%20ensure%20valid%20coverage%20with%0Arobustness%20to%20model%20mis-specification%2C%20we%20wed%20the%20GP-based%20set%20predictors%20with%0Athe%20online%20conformal%20prediction%20framework%2C%20which%20post-processes%20the%20prediction%0Asets%20using%20adaptive%20thresholds.%20Experimental%20results%20the%20proposed%20method%20yields%0Aimproved%20coverage%20and%20efficient%20prediction%20sets%20over%20existing%20baselines%20by%0Aadaptively%20ensembling%20the%20GP%20models%20and%20setting%20the%20key%20threshold%20parameters%20in%0ACP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06181v1&entry.124074799=Read"},
{"title": "Incremental Object Detection with Prompt-based Methods", "author": "Matthias Neuwirth-Trapp and Maarten Bieshaar and Danda Pani Paudel and Luc Van Gool", "abstract": "  Visual prompt-based methods have seen growing interest in incremental\nlearning (IL) for image classification. These approaches learn additional\nembedding vectors while keeping the model frozen, making them efficient to\ntrain. However, no prior work has applied such methods to incremental object\ndetection (IOD), leaving their generalizability unclear. In this paper, we\nanalyze three different prompt-based methods under a complex domain-incremental\nlearning setting. We additionally provide a wide range of reference baselines\nfor comparison. Empirically, we show that the prompt-based approaches we tested\nunderperform in this setting. However, a strong yet practical method, combining\nvisual prompts with replaying a small portion of previous data, achieves the\nbest results. Together with additional experiments on prompt length and\ninitialization, our findings offer valuable insights for advancing prompt-based\nIL in IOD.\n", "link": "http://arxiv.org/abs/2508.14599v2", "date": "2025-10-07", "relevancy": 2.5531, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Object%20Detection%20with%20Prompt-based%20Methods&body=Title%3A%20Incremental%20Object%20Detection%20with%20Prompt-based%20Methods%0AAuthor%3A%20Matthias%20Neuwirth-Trapp%20and%20Maarten%20Bieshaar%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Visual%20prompt-based%20methods%20have%20seen%20growing%20interest%20in%20incremental%0Alearning%20%28IL%29%20for%20image%20classification.%20These%20approaches%20learn%20additional%0Aembedding%20vectors%20while%20keeping%20the%20model%20frozen%2C%20making%20them%20efficient%20to%0Atrain.%20However%2C%20no%20prior%20work%20has%20applied%20such%20methods%20to%20incremental%20object%0Adetection%20%28IOD%29%2C%20leaving%20their%20generalizability%20unclear.%20In%20this%20paper%2C%20we%0Aanalyze%20three%20different%20prompt-based%20methods%20under%20a%20complex%20domain-incremental%0Alearning%20setting.%20We%20additionally%20provide%20a%20wide%20range%20of%20reference%20baselines%0Afor%20comparison.%20Empirically%2C%20we%20show%20that%20the%20prompt-based%20approaches%20we%20tested%0Aunderperform%20in%20this%20setting.%20However%2C%20a%20strong%20yet%20practical%20method%2C%20combining%0Avisual%20prompts%20with%20replaying%20a%20small%20portion%20of%20previous%20data%2C%20achieves%20the%0Abest%20results.%20Together%20with%20additional%20experiments%20on%20prompt%20length%20and%0Ainitialization%2C%20our%20findings%20offer%20valuable%20insights%20for%20advancing%20prompt-based%0AIL%20in%20IOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14599v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Object%2520Detection%2520with%2520Prompt-based%2520Methods%26entry.906535625%3DMatthias%2520Neuwirth-Trapp%2520and%2520Maarten%2520Bieshaar%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Visual%2520prompt-based%2520methods%2520have%2520seen%2520growing%2520interest%2520in%2520incremental%250Alearning%2520%2528IL%2529%2520for%2520image%2520classification.%2520These%2520approaches%2520learn%2520additional%250Aembedding%2520vectors%2520while%2520keeping%2520the%2520model%2520frozen%252C%2520making%2520them%2520efficient%2520to%250Atrain.%2520However%252C%2520no%2520prior%2520work%2520has%2520applied%2520such%2520methods%2520to%2520incremental%2520object%250Adetection%2520%2528IOD%2529%252C%2520leaving%2520their%2520generalizability%2520unclear.%2520In%2520this%2520paper%252C%2520we%250Aanalyze%2520three%2520different%2520prompt-based%2520methods%2520under%2520a%2520complex%2520domain-incremental%250Alearning%2520setting.%2520We%2520additionally%2520provide%2520a%2520wide%2520range%2520of%2520reference%2520baselines%250Afor%2520comparison.%2520Empirically%252C%2520we%2520show%2520that%2520the%2520prompt-based%2520approaches%2520we%2520tested%250Aunderperform%2520in%2520this%2520setting.%2520However%252C%2520a%2520strong%2520yet%2520practical%2520method%252C%2520combining%250Avisual%2520prompts%2520with%2520replaying%2520a%2520small%2520portion%2520of%2520previous%2520data%252C%2520achieves%2520the%250Abest%2520results.%2520Together%2520with%2520additional%2520experiments%2520on%2520prompt%2520length%2520and%250Ainitialization%252C%2520our%2520findings%2520offer%2520valuable%2520insights%2520for%2520advancing%2520prompt-based%250AIL%2520in%2520IOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14599v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Object%20Detection%20with%20Prompt-based%20Methods&entry.906535625=Matthias%20Neuwirth-Trapp%20and%20Maarten%20Bieshaar%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Visual%20prompt-based%20methods%20have%20seen%20growing%20interest%20in%20incremental%0Alearning%20%28IL%29%20for%20image%20classification.%20These%20approaches%20learn%20additional%0Aembedding%20vectors%20while%20keeping%20the%20model%20frozen%2C%20making%20them%20efficient%20to%0Atrain.%20However%2C%20no%20prior%20work%20has%20applied%20such%20methods%20to%20incremental%20object%0Adetection%20%28IOD%29%2C%20leaving%20their%20generalizability%20unclear.%20In%20this%20paper%2C%20we%0Aanalyze%20three%20different%20prompt-based%20methods%20under%20a%20complex%20domain-incremental%0Alearning%20setting.%20We%20additionally%20provide%20a%20wide%20range%20of%20reference%20baselines%0Afor%20comparison.%20Empirically%2C%20we%20show%20that%20the%20prompt-based%20approaches%20we%20tested%0Aunderperform%20in%20this%20setting.%20However%2C%20a%20strong%20yet%20practical%20method%2C%20combining%0Avisual%20prompts%20with%20replaying%20a%20small%20portion%20of%20previous%20data%2C%20achieves%20the%0Abest%20results.%20Together%20with%20additional%20experiments%20on%20prompt%20length%20and%0Ainitialization%2C%20our%20findings%20offer%20valuable%20insights%20for%20advancing%20prompt-based%0AIL%20in%20IOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14599v2&entry.124074799=Read"},
{"title": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search", "author": "Yuta Oshima and Masahiro Suzuki and Yutaka Matsuo and Hiroki Furuta", "abstract": "  The remarkable progress in text-to-video diffusion models enables the\ngeneration of photorealistic videos, although the content of these generated\nvideos often includes unnatural movement or deformation, reverse playback, and\nmotionless scenes. Recently, an alignment problem has attracted huge attention,\nwhere we steer the output of diffusion models based on some measure of the\ncontent's goodness. Because there is a large room for improvement of perceptual\nquality along the frame direction, we should address which metrics we should\noptimize and how we can optimize them in the video generation. In this paper,\nwe propose diffusion latent beam search with lookahead estimator, which can\nselect a better diffusion latent to maximize a given alignment reward at\ninference time. We then point out that improving perceptual video quality with\nrespect to alignment to prompts requires reward calibration by weighting\nexisting metrics. This is because when humans or vision language models\nevaluate outputs, many previous metrics to quantify the naturalness of video do\nnot always correlate with the evaluation. We demonstrate that our method\nimproves the perceptual quality evaluated on the calibrated reward, VLMs, and\nhuman assessment, without model parameter update, and outputs the best\ngeneration compared to greedy search and best-of-N sampling under much more\nefficient computational cost. The experiments highlight that our method is\nbeneficial to many capable generative models, and provide a practical\nguideline: we should prioritize the inference-time compute allocation into\nenabling the lookahead estimator and increasing the search budget, rather than\nexpanding the denoising steps.\n", "link": "http://arxiv.org/abs/2501.19252v3", "date": "2025-10-07", "relevancy": 2.5405, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6668}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6343}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Text-to-Video%20Alignment%20with%20Diffusion%20Latent%20Beam%20Search&body=Title%3A%20Inference-Time%20Text-to-Video%20Alignment%20with%20Diffusion%20Latent%20Beam%20Search%0AAuthor%3A%20Yuta%20Oshima%20and%20Masahiro%20Suzuki%20and%20Yutaka%20Matsuo%20and%20Hiroki%20Furuta%0AAbstract%3A%20%20%20The%20remarkable%20progress%20in%20text-to-video%20diffusion%20models%20enables%20the%0Ageneration%20of%20photorealistic%20videos%2C%20although%20the%20content%20of%20these%20generated%0Avideos%20often%20includes%20unnatural%20movement%20or%20deformation%2C%20reverse%20playback%2C%20and%0Amotionless%20scenes.%20Recently%2C%20an%20alignment%20problem%20has%20attracted%20huge%20attention%2C%0Awhere%20we%20steer%20the%20output%20of%20diffusion%20models%20based%20on%20some%20measure%20of%20the%0Acontent%27s%20goodness.%20Because%20there%20is%20a%20large%20room%20for%20improvement%20of%20perceptual%0Aquality%20along%20the%20frame%20direction%2C%20we%20should%20address%20which%20metrics%20we%20should%0Aoptimize%20and%20how%20we%20can%20optimize%20them%20in%20the%20video%20generation.%20In%20this%20paper%2C%0Awe%20propose%20diffusion%20latent%20beam%20search%20with%20lookahead%20estimator%2C%20which%20can%0Aselect%20a%20better%20diffusion%20latent%20to%20maximize%20a%20given%20alignment%20reward%20at%0Ainference%20time.%20We%20then%20point%20out%20that%20improving%20perceptual%20video%20quality%20with%0Arespect%20to%20alignment%20to%20prompts%20requires%20reward%20calibration%20by%20weighting%0Aexisting%20metrics.%20This%20is%20because%20when%20humans%20or%20vision%20language%20models%0Aevaluate%20outputs%2C%20many%20previous%20metrics%20to%20quantify%20the%20naturalness%20of%20video%20do%0Anot%20always%20correlate%20with%20the%20evaluation.%20We%20demonstrate%20that%20our%20method%0Aimproves%20the%20perceptual%20quality%20evaluated%20on%20the%20calibrated%20reward%2C%20VLMs%2C%20and%0Ahuman%20assessment%2C%20without%20model%20parameter%20update%2C%20and%20outputs%20the%20best%0Ageneration%20compared%20to%20greedy%20search%20and%20best-of-N%20sampling%20under%20much%20more%0Aefficient%20computational%20cost.%20The%20experiments%20highlight%20that%20our%20method%20is%0Abeneficial%20to%20many%20capable%20generative%20models%2C%20and%20provide%20a%20practical%0Aguideline%3A%20we%20should%20prioritize%20the%20inference-time%20compute%20allocation%20into%0Aenabling%20the%20lookahead%20estimator%20and%20increasing%20the%20search%20budget%2C%20rather%20than%0Aexpanding%20the%20denoising%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19252v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Text-to-Video%2520Alignment%2520with%2520Diffusion%2520Latent%2520Beam%2520Search%26entry.906535625%3DYuta%2520Oshima%2520and%2520Masahiro%2520Suzuki%2520and%2520Yutaka%2520Matsuo%2520and%2520Hiroki%2520Furuta%26entry.1292438233%3D%2520%2520The%2520remarkable%2520progress%2520in%2520text-to-video%2520diffusion%2520models%2520enables%2520the%250Ageneration%2520of%2520photorealistic%2520videos%252C%2520although%2520the%2520content%2520of%2520these%2520generated%250Avideos%2520often%2520includes%2520unnatural%2520movement%2520or%2520deformation%252C%2520reverse%2520playback%252C%2520and%250Amotionless%2520scenes.%2520Recently%252C%2520an%2520alignment%2520problem%2520has%2520attracted%2520huge%2520attention%252C%250Awhere%2520we%2520steer%2520the%2520output%2520of%2520diffusion%2520models%2520based%2520on%2520some%2520measure%2520of%2520the%250Acontent%2527s%2520goodness.%2520Because%2520there%2520is%2520a%2520large%2520room%2520for%2520improvement%2520of%2520perceptual%250Aquality%2520along%2520the%2520frame%2520direction%252C%2520we%2520should%2520address%2520which%2520metrics%2520we%2520should%250Aoptimize%2520and%2520how%2520we%2520can%2520optimize%2520them%2520in%2520the%2520video%2520generation.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520diffusion%2520latent%2520beam%2520search%2520with%2520lookahead%2520estimator%252C%2520which%2520can%250Aselect%2520a%2520better%2520diffusion%2520latent%2520to%2520maximize%2520a%2520given%2520alignment%2520reward%2520at%250Ainference%2520time.%2520We%2520then%2520point%2520out%2520that%2520improving%2520perceptual%2520video%2520quality%2520with%250Arespect%2520to%2520alignment%2520to%2520prompts%2520requires%2520reward%2520calibration%2520by%2520weighting%250Aexisting%2520metrics.%2520This%2520is%2520because%2520when%2520humans%2520or%2520vision%2520language%2520models%250Aevaluate%2520outputs%252C%2520many%2520previous%2520metrics%2520to%2520quantify%2520the%2520naturalness%2520of%2520video%2520do%250Anot%2520always%2520correlate%2520with%2520the%2520evaluation.%2520We%2520demonstrate%2520that%2520our%2520method%250Aimproves%2520the%2520perceptual%2520quality%2520evaluated%2520on%2520the%2520calibrated%2520reward%252C%2520VLMs%252C%2520and%250Ahuman%2520assessment%252C%2520without%2520model%2520parameter%2520update%252C%2520and%2520outputs%2520the%2520best%250Ageneration%2520compared%2520to%2520greedy%2520search%2520and%2520best-of-N%2520sampling%2520under%2520much%2520more%250Aefficient%2520computational%2520cost.%2520The%2520experiments%2520highlight%2520that%2520our%2520method%2520is%250Abeneficial%2520to%2520many%2520capable%2520generative%2520models%252C%2520and%2520provide%2520a%2520practical%250Aguideline%253A%2520we%2520should%2520prioritize%2520the%2520inference-time%2520compute%2520allocation%2520into%250Aenabling%2520the%2520lookahead%2520estimator%2520and%2520increasing%2520the%2520search%2520budget%252C%2520rather%2520than%250Aexpanding%2520the%2520denoising%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19252v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Text-to-Video%20Alignment%20with%20Diffusion%20Latent%20Beam%20Search&entry.906535625=Yuta%20Oshima%20and%20Masahiro%20Suzuki%20and%20Yutaka%20Matsuo%20and%20Hiroki%20Furuta&entry.1292438233=%20%20The%20remarkable%20progress%20in%20text-to-video%20diffusion%20models%20enables%20the%0Ageneration%20of%20photorealistic%20videos%2C%20although%20the%20content%20of%20these%20generated%0Avideos%20often%20includes%20unnatural%20movement%20or%20deformation%2C%20reverse%20playback%2C%20and%0Amotionless%20scenes.%20Recently%2C%20an%20alignment%20problem%20has%20attracted%20huge%20attention%2C%0Awhere%20we%20steer%20the%20output%20of%20diffusion%20models%20based%20on%20some%20measure%20of%20the%0Acontent%27s%20goodness.%20Because%20there%20is%20a%20large%20room%20for%20improvement%20of%20perceptual%0Aquality%20along%20the%20frame%20direction%2C%20we%20should%20address%20which%20metrics%20we%20should%0Aoptimize%20and%20how%20we%20can%20optimize%20them%20in%20the%20video%20generation.%20In%20this%20paper%2C%0Awe%20propose%20diffusion%20latent%20beam%20search%20with%20lookahead%20estimator%2C%20which%20can%0Aselect%20a%20better%20diffusion%20latent%20to%20maximize%20a%20given%20alignment%20reward%20at%0Ainference%20time.%20We%20then%20point%20out%20that%20improving%20perceptual%20video%20quality%20with%0Arespect%20to%20alignment%20to%20prompts%20requires%20reward%20calibration%20by%20weighting%0Aexisting%20metrics.%20This%20is%20because%20when%20humans%20or%20vision%20language%20models%0Aevaluate%20outputs%2C%20many%20previous%20metrics%20to%20quantify%20the%20naturalness%20of%20video%20do%0Anot%20always%20correlate%20with%20the%20evaluation.%20We%20demonstrate%20that%20our%20method%0Aimproves%20the%20perceptual%20quality%20evaluated%20on%20the%20calibrated%20reward%2C%20VLMs%2C%20and%0Ahuman%20assessment%2C%20without%20model%20parameter%20update%2C%20and%20outputs%20the%20best%0Ageneration%20compared%20to%20greedy%20search%20and%20best-of-N%20sampling%20under%20much%20more%0Aefficient%20computational%20cost.%20The%20experiments%20highlight%20that%20our%20method%20is%0Abeneficial%20to%20many%20capable%20generative%20models%2C%20and%20provide%20a%20practical%0Aguideline%3A%20we%20should%20prioritize%20the%20inference-time%20compute%20allocation%20into%0Aenabling%20the%20lookahead%20estimator%20and%20increasing%20the%20search%20budget%2C%20rather%20than%0Aexpanding%20the%20denoising%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19252v3&entry.124074799=Read"},
{"title": "MoSA: Motion-Coherent Human Video Generation via Structure-Appearance\n  Decoupling", "author": "Haoyu Wang and Hao Tang and Donglin Di and Zhilu Zhang and Wangmeng Zuo and Feng Gao and Siwei Ma and Shiliang Zhang", "abstract": "  Existing video generation models predominantly emphasize appearance fidelity\nwhile exhibiting limited ability to synthesize complex human motions, such as\nwhole-body movements, long-range dynamics, and fine-grained human-environment\ninteractions. This often leads to unrealistic or physically implausible\nmovements with inadequate structural coherence. To conquer these challenges, we\npropose MoSA, which decouples the process of human video generation into two\ncomponents, i.e., structure generation and appearance generation. MoSA first\nemploys a 3D structure transformer to generate a human motion sequence from the\ntext prompt. The remaining video appearance is then synthesized under the\nguidance of this structural sequence. We achieve fine-grained control over the\nsparse human structures by introducing Human-Aware Dynamic Control modules with\na dense tracking constraint during training. The modeling of human-environment\ninteractions is improved through the proposed contact constraint. Those two\ncomponents work comprehensively to ensure the structural and appearance\nfidelity across the generated videos. This paper also contributes a large-scale\nhuman video dataset, which features more complex and diverse motions than\nexisting human video datasets. We conduct comprehensive comparisons between\nMoSA and a variety of approaches, including general video generation models,\nhuman video generation models, and human animation models. Experiments\ndemonstrate that MoSA substantially outperforms existing approaches across the\nmajority of evaluation metrics.\n", "link": "http://arxiv.org/abs/2508.17404v2", "date": "2025-10-07", "relevancy": 2.5329, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6627}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6283}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoSA%3A%20Motion-Coherent%20Human%20Video%20Generation%20via%20Structure-Appearance%0A%20%20Decoupling&body=Title%3A%20MoSA%3A%20Motion-Coherent%20Human%20Video%20Generation%20via%20Structure-Appearance%0A%20%20Decoupling%0AAuthor%3A%20Haoyu%20Wang%20and%20Hao%20Tang%20and%20Donglin%20Di%20and%20Zhilu%20Zhang%20and%20Wangmeng%20Zuo%20and%20Feng%20Gao%20and%20Siwei%20Ma%20and%20Shiliang%20Zhang%0AAbstract%3A%20%20%20Existing%20video%20generation%20models%20predominantly%20emphasize%20appearance%20fidelity%0Awhile%20exhibiting%20limited%20ability%20to%20synthesize%20complex%20human%20motions%2C%20such%20as%0Awhole-body%20movements%2C%20long-range%20dynamics%2C%20and%20fine-grained%20human-environment%0Ainteractions.%20This%20often%20leads%20to%20unrealistic%20or%20physically%20implausible%0Amovements%20with%20inadequate%20structural%20coherence.%20To%20conquer%20these%20challenges%2C%20we%0Apropose%20MoSA%2C%20which%20decouples%20the%20process%20of%20human%20video%20generation%20into%20two%0Acomponents%2C%20i.e.%2C%20structure%20generation%20and%20appearance%20generation.%20MoSA%20first%0Aemploys%20a%203D%20structure%20transformer%20to%20generate%20a%20human%20motion%20sequence%20from%20the%0Atext%20prompt.%20The%20remaining%20video%20appearance%20is%20then%20synthesized%20under%20the%0Aguidance%20of%20this%20structural%20sequence.%20We%20achieve%20fine-grained%20control%20over%20the%0Asparse%20human%20structures%20by%20introducing%20Human-Aware%20Dynamic%20Control%20modules%20with%0Aa%20dense%20tracking%20constraint%20during%20training.%20The%20modeling%20of%20human-environment%0Ainteractions%20is%20improved%20through%20the%20proposed%20contact%20constraint.%20Those%20two%0Acomponents%20work%20comprehensively%20to%20ensure%20the%20structural%20and%20appearance%0Afidelity%20across%20the%20generated%20videos.%20This%20paper%20also%20contributes%20a%20large-scale%0Ahuman%20video%20dataset%2C%20which%20features%20more%20complex%20and%20diverse%20motions%20than%0Aexisting%20human%20video%20datasets.%20We%20conduct%20comprehensive%20comparisons%20between%0AMoSA%20and%20a%20variety%20of%20approaches%2C%20including%20general%20video%20generation%20models%2C%0Ahuman%20video%20generation%20models%2C%20and%20human%20animation%20models.%20Experiments%0Ademonstrate%20that%20MoSA%20substantially%20outperforms%20existing%20approaches%20across%20the%0Amajority%20of%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoSA%253A%2520Motion-Coherent%2520Human%2520Video%2520Generation%2520via%2520Structure-Appearance%250A%2520%2520Decoupling%26entry.906535625%3DHaoyu%2520Wang%2520and%2520Hao%2520Tang%2520and%2520Donglin%2520Di%2520and%2520Zhilu%2520Zhang%2520and%2520Wangmeng%2520Zuo%2520and%2520Feng%2520Gao%2520and%2520Siwei%2520Ma%2520and%2520Shiliang%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520video%2520generation%2520models%2520predominantly%2520emphasize%2520appearance%2520fidelity%250Awhile%2520exhibiting%2520limited%2520ability%2520to%2520synthesize%2520complex%2520human%2520motions%252C%2520such%2520as%250Awhole-body%2520movements%252C%2520long-range%2520dynamics%252C%2520and%2520fine-grained%2520human-environment%250Ainteractions.%2520This%2520often%2520leads%2520to%2520unrealistic%2520or%2520physically%2520implausible%250Amovements%2520with%2520inadequate%2520structural%2520coherence.%2520To%2520conquer%2520these%2520challenges%252C%2520we%250Apropose%2520MoSA%252C%2520which%2520decouples%2520the%2520process%2520of%2520human%2520video%2520generation%2520into%2520two%250Acomponents%252C%2520i.e.%252C%2520structure%2520generation%2520and%2520appearance%2520generation.%2520MoSA%2520first%250Aemploys%2520a%25203D%2520structure%2520transformer%2520to%2520generate%2520a%2520human%2520motion%2520sequence%2520from%2520the%250Atext%2520prompt.%2520The%2520remaining%2520video%2520appearance%2520is%2520then%2520synthesized%2520under%2520the%250Aguidance%2520of%2520this%2520structural%2520sequence.%2520We%2520achieve%2520fine-grained%2520control%2520over%2520the%250Asparse%2520human%2520structures%2520by%2520introducing%2520Human-Aware%2520Dynamic%2520Control%2520modules%2520with%250Aa%2520dense%2520tracking%2520constraint%2520during%2520training.%2520The%2520modeling%2520of%2520human-environment%250Ainteractions%2520is%2520improved%2520through%2520the%2520proposed%2520contact%2520constraint.%2520Those%2520two%250Acomponents%2520work%2520comprehensively%2520to%2520ensure%2520the%2520structural%2520and%2520appearance%250Afidelity%2520across%2520the%2520generated%2520videos.%2520This%2520paper%2520also%2520contributes%2520a%2520large-scale%250Ahuman%2520video%2520dataset%252C%2520which%2520features%2520more%2520complex%2520and%2520diverse%2520motions%2520than%250Aexisting%2520human%2520video%2520datasets.%2520We%2520conduct%2520comprehensive%2520comparisons%2520between%250AMoSA%2520and%2520a%2520variety%2520of%2520approaches%252C%2520including%2520general%2520video%2520generation%2520models%252C%250Ahuman%2520video%2520generation%2520models%252C%2520and%2520human%2520animation%2520models.%2520Experiments%250Ademonstrate%2520that%2520MoSA%2520substantially%2520outperforms%2520existing%2520approaches%2520across%2520the%250Amajority%2520of%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoSA%3A%20Motion-Coherent%20Human%20Video%20Generation%20via%20Structure-Appearance%0A%20%20Decoupling&entry.906535625=Haoyu%20Wang%20and%20Hao%20Tang%20and%20Donglin%20Di%20and%20Zhilu%20Zhang%20and%20Wangmeng%20Zuo%20and%20Feng%20Gao%20and%20Siwei%20Ma%20and%20Shiliang%20Zhang&entry.1292438233=%20%20Existing%20video%20generation%20models%20predominantly%20emphasize%20appearance%20fidelity%0Awhile%20exhibiting%20limited%20ability%20to%20synthesize%20complex%20human%20motions%2C%20such%20as%0Awhole-body%20movements%2C%20long-range%20dynamics%2C%20and%20fine-grained%20human-environment%0Ainteractions.%20This%20often%20leads%20to%20unrealistic%20or%20physically%20implausible%0Amovements%20with%20inadequate%20structural%20coherence.%20To%20conquer%20these%20challenges%2C%20we%0Apropose%20MoSA%2C%20which%20decouples%20the%20process%20of%20human%20video%20generation%20into%20two%0Acomponents%2C%20i.e.%2C%20structure%20generation%20and%20appearance%20generation.%20MoSA%20first%0Aemploys%20a%203D%20structure%20transformer%20to%20generate%20a%20human%20motion%20sequence%20from%20the%0Atext%20prompt.%20The%20remaining%20video%20appearance%20is%20then%20synthesized%20under%20the%0Aguidance%20of%20this%20structural%20sequence.%20We%20achieve%20fine-grained%20control%20over%20the%0Asparse%20human%20structures%20by%20introducing%20Human-Aware%20Dynamic%20Control%20modules%20with%0Aa%20dense%20tracking%20constraint%20during%20training.%20The%20modeling%20of%20human-environment%0Ainteractions%20is%20improved%20through%20the%20proposed%20contact%20constraint.%20Those%20two%0Acomponents%20work%20comprehensively%20to%20ensure%20the%20structural%20and%20appearance%0Afidelity%20across%20the%20generated%20videos.%20This%20paper%20also%20contributes%20a%20large-scale%0Ahuman%20video%20dataset%2C%20which%20features%20more%20complex%20and%20diverse%20motions%20than%0Aexisting%20human%20video%20datasets.%20We%20conduct%20comprehensive%20comparisons%20between%0AMoSA%20and%20a%20variety%20of%20approaches%2C%20including%20general%20video%20generation%20models%2C%0Ahuman%20video%20generation%20models%2C%20and%20human%20animation%20models.%20Experiments%0Ademonstrate%20that%20MoSA%20substantially%20outperforms%20existing%20approaches%20across%20the%0Amajority%20of%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17404v2&entry.124074799=Read"},
{"title": "Adaptive Pruning for Increased Robustness and Reduced Computational\n  Overhead in Gaussian Process Accelerated Saddle Point Searches", "author": "Rohit Goswami and Hannes J\u00f3nsson", "abstract": "  Gaussian process (GP) regression provides a strategy for accelerating saddle\npoint searches on high-dimensional energy surfaces by reducing the number of\ntimes the energy and its derivatives with respect to atomic coordinates need to\nbe evaluated. The computational overhead in the hyperparameter optimization\ncan, however, be large and make the approach inefficient. Failures can also\noccur if the search ventures too far into regions that are not represented well\nenough by the GP model. Here, these challenges are resolved by using\ngeometry-aware optimal transport measures and an active pruning strategy using\na summation over Wasserstein-1 distances for each atom-type in farthest-point\nsampling, selecting a fixed-size subset of geometrically diverse configurations\nto avoid rapidly increasing cost of GP updates as more observations are made.\nStability is enhanced by permutation-invariant metric that provides a reliable\ntrust radius for early-stopping and a logarithmic barrier penalty for the\ngrowth of the signal variance. These physically motivated algorithmic changes\nprove their efficacy by reducing to less than a half the mean computational\ntime on a set of 238 challenging configurations from a previously published\ndata set of chemical reactions. With these improvements, the GP approach is\nestablished as, a robust and scalable algorithm for accelerating saddle point\nsearches when the evaluation of the energy and atomic forces requires\nsignificant computational effort.\n", "link": "http://arxiv.org/abs/2510.06030v1", "date": "2025-10-07", "relevancy": 2.5312, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5149}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5127}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Pruning%20for%20Increased%20Robustness%20and%20Reduced%20Computational%0A%20%20Overhead%20in%20Gaussian%20Process%20Accelerated%20Saddle%20Point%20Searches&body=Title%3A%20Adaptive%20Pruning%20for%20Increased%20Robustness%20and%20Reduced%20Computational%0A%20%20Overhead%20in%20Gaussian%20Process%20Accelerated%20Saddle%20Point%20Searches%0AAuthor%3A%20Rohit%20Goswami%20and%20Hannes%20J%C3%B3nsson%0AAbstract%3A%20%20%20Gaussian%20process%20%28GP%29%20regression%20provides%20a%20strategy%20for%20accelerating%20saddle%0Apoint%20searches%20on%20high-dimensional%20energy%20surfaces%20by%20reducing%20the%20number%20of%0Atimes%20the%20energy%20and%20its%20derivatives%20with%20respect%20to%20atomic%20coordinates%20need%20to%0Abe%20evaluated.%20The%20computational%20overhead%20in%20the%20hyperparameter%20optimization%0Acan%2C%20however%2C%20be%20large%20and%20make%20the%20approach%20inefficient.%20Failures%20can%20also%0Aoccur%20if%20the%20search%20ventures%20too%20far%20into%20regions%20that%20are%20not%20represented%20well%0Aenough%20by%20the%20GP%20model.%20Here%2C%20these%20challenges%20are%20resolved%20by%20using%0Ageometry-aware%20optimal%20transport%20measures%20and%20an%20active%20pruning%20strategy%20using%0Aa%20summation%20over%20Wasserstein-1%20distances%20for%20each%20atom-type%20in%20farthest-point%0Asampling%2C%20selecting%20a%20fixed-size%20subset%20of%20geometrically%20diverse%20configurations%0Ato%20avoid%20rapidly%20increasing%20cost%20of%20GP%20updates%20as%20more%20observations%20are%20made.%0AStability%20is%20enhanced%20by%20permutation-invariant%20metric%20that%20provides%20a%20reliable%0Atrust%20radius%20for%20early-stopping%20and%20a%20logarithmic%20barrier%20penalty%20for%20the%0Agrowth%20of%20the%20signal%20variance.%20These%20physically%20motivated%20algorithmic%20changes%0Aprove%20their%20efficacy%20by%20reducing%20to%20less%20than%20a%20half%20the%20mean%20computational%0Atime%20on%20a%20set%20of%20238%20challenging%20configurations%20from%20a%20previously%20published%0Adata%20set%20of%20chemical%20reactions.%20With%20these%20improvements%2C%20the%20GP%20approach%20is%0Aestablished%20as%2C%20a%20robust%20and%20scalable%20algorithm%20for%20accelerating%20saddle%20point%0Asearches%20when%20the%20evaluation%20of%20the%20energy%20and%20atomic%20forces%20requires%0Asignificant%20computational%20effort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Pruning%2520for%2520Increased%2520Robustness%2520and%2520Reduced%2520Computational%250A%2520%2520Overhead%2520in%2520Gaussian%2520Process%2520Accelerated%2520Saddle%2520Point%2520Searches%26entry.906535625%3DRohit%2520Goswami%2520and%2520Hannes%2520J%25C3%25B3nsson%26entry.1292438233%3D%2520%2520Gaussian%2520process%2520%2528GP%2529%2520regression%2520provides%2520a%2520strategy%2520for%2520accelerating%2520saddle%250Apoint%2520searches%2520on%2520high-dimensional%2520energy%2520surfaces%2520by%2520reducing%2520the%2520number%2520of%250Atimes%2520the%2520energy%2520and%2520its%2520derivatives%2520with%2520respect%2520to%2520atomic%2520coordinates%2520need%2520to%250Abe%2520evaluated.%2520The%2520computational%2520overhead%2520in%2520the%2520hyperparameter%2520optimization%250Acan%252C%2520however%252C%2520be%2520large%2520and%2520make%2520the%2520approach%2520inefficient.%2520Failures%2520can%2520also%250Aoccur%2520if%2520the%2520search%2520ventures%2520too%2520far%2520into%2520regions%2520that%2520are%2520not%2520represented%2520well%250Aenough%2520by%2520the%2520GP%2520model.%2520Here%252C%2520these%2520challenges%2520are%2520resolved%2520by%2520using%250Ageometry-aware%2520optimal%2520transport%2520measures%2520and%2520an%2520active%2520pruning%2520strategy%2520using%250Aa%2520summation%2520over%2520Wasserstein-1%2520distances%2520for%2520each%2520atom-type%2520in%2520farthest-point%250Asampling%252C%2520selecting%2520a%2520fixed-size%2520subset%2520of%2520geometrically%2520diverse%2520configurations%250Ato%2520avoid%2520rapidly%2520increasing%2520cost%2520of%2520GP%2520updates%2520as%2520more%2520observations%2520are%2520made.%250AStability%2520is%2520enhanced%2520by%2520permutation-invariant%2520metric%2520that%2520provides%2520a%2520reliable%250Atrust%2520radius%2520for%2520early-stopping%2520and%2520a%2520logarithmic%2520barrier%2520penalty%2520for%2520the%250Agrowth%2520of%2520the%2520signal%2520variance.%2520These%2520physically%2520motivated%2520algorithmic%2520changes%250Aprove%2520their%2520efficacy%2520by%2520reducing%2520to%2520less%2520than%2520a%2520half%2520the%2520mean%2520computational%250Atime%2520on%2520a%2520set%2520of%2520238%2520challenging%2520configurations%2520from%2520a%2520previously%2520published%250Adata%2520set%2520of%2520chemical%2520reactions.%2520With%2520these%2520improvements%252C%2520the%2520GP%2520approach%2520is%250Aestablished%2520as%252C%2520a%2520robust%2520and%2520scalable%2520algorithm%2520for%2520accelerating%2520saddle%2520point%250Asearches%2520when%2520the%2520evaluation%2520of%2520the%2520energy%2520and%2520atomic%2520forces%2520requires%250Asignificant%2520computational%2520effort.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Pruning%20for%20Increased%20Robustness%20and%20Reduced%20Computational%0A%20%20Overhead%20in%20Gaussian%20Process%20Accelerated%20Saddle%20Point%20Searches&entry.906535625=Rohit%20Goswami%20and%20Hannes%20J%C3%B3nsson&entry.1292438233=%20%20Gaussian%20process%20%28GP%29%20regression%20provides%20a%20strategy%20for%20accelerating%20saddle%0Apoint%20searches%20on%20high-dimensional%20energy%20surfaces%20by%20reducing%20the%20number%20of%0Atimes%20the%20energy%20and%20its%20derivatives%20with%20respect%20to%20atomic%20coordinates%20need%20to%0Abe%20evaluated.%20The%20computational%20overhead%20in%20the%20hyperparameter%20optimization%0Acan%2C%20however%2C%20be%20large%20and%20make%20the%20approach%20inefficient.%20Failures%20can%20also%0Aoccur%20if%20the%20search%20ventures%20too%20far%20into%20regions%20that%20are%20not%20represented%20well%0Aenough%20by%20the%20GP%20model.%20Here%2C%20these%20challenges%20are%20resolved%20by%20using%0Ageometry-aware%20optimal%20transport%20measures%20and%20an%20active%20pruning%20strategy%20using%0Aa%20summation%20over%20Wasserstein-1%20distances%20for%20each%20atom-type%20in%20farthest-point%0Asampling%2C%20selecting%20a%20fixed-size%20subset%20of%20geometrically%20diverse%20configurations%0Ato%20avoid%20rapidly%20increasing%20cost%20of%20GP%20updates%20as%20more%20observations%20are%20made.%0AStability%20is%20enhanced%20by%20permutation-invariant%20metric%20that%20provides%20a%20reliable%0Atrust%20radius%20for%20early-stopping%20and%20a%20logarithmic%20barrier%20penalty%20for%20the%0Agrowth%20of%20the%20signal%20variance.%20These%20physically%20motivated%20algorithmic%20changes%0Aprove%20their%20efficacy%20by%20reducing%20to%20less%20than%20a%20half%20the%20mean%20computational%0Atime%20on%20a%20set%20of%20238%20challenging%20configurations%20from%20a%20previously%20published%0Adata%20set%20of%20chemical%20reactions.%20With%20these%20improvements%2C%20the%20GP%20approach%20is%0Aestablished%20as%2C%20a%20robust%20and%20scalable%20algorithm%20for%20accelerating%20saddle%20point%0Asearches%20when%20the%20evaluation%20of%20the%20energy%20and%20atomic%20forces%20requires%0Asignificant%20computational%20effort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06030v1&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Mamba Architectures for Medical Image\n  Analysis: Classification, Segmentation, Restoration and Beyond", "author": "Shubhi Bansal and Sreeharish A and Madhava Prasath J and Manikandan S and Sreekanth Madisetty and Mohammad Zia Ur Rehman and Chandravardhan Singh Raghaw and Gaurav Duggal and Nagendra Kumar", "abstract": "  Mamba, a special case of the State Space Model, is gaining popularity as an\nalternative to template-based deep learning approaches in medical image\nanalysis. While transformers are powerful architectures, they have drawbacks,\nincluding quadratic computational complexity and an inability to address\nlong-range dependencies efficiently. This limitation affects the analysis of\nlarge and complex datasets in medical imaging, where there are many spatial and\ntemporal relationships. In contrast, Mamba offers benefits that make it\nwell-suited for medical image analysis. It has linear time complexity, which is\na significant improvement over transformers. Mamba processes longer sequences\nwithout attention mechanisms, enabling faster inference and requiring less\nmemory. Mamba also demonstrates strong performance in merging multimodal data,\nimproving diagnosis accuracy and patient outcomes. The organization of this\npaper allows readers to appreciate the capabilities of Mamba in medical imaging\nstep by step. We begin by defining core concepts of SSMs and models, including\nS4, S5, and S6, followed by an exploration of Mamba architectures such as pure\nMamba, U-Net variants, and hybrid models with convolutional neural networks,\ntransformers, and Graph Neural Networks. We also cover Mamba optimizations,\ntechniques and adaptations, scanning, datasets, applications, experimental\nresults, and conclude with its challenges and future directions in medical\nimaging. This review aims to demonstrate the transformative potential of Mamba\nin overcoming existing barriers within medical imaging while paving the way for\ninnovative advancements in the field. A comprehensive list of Mamba\narchitectures applied in the medical field, reviewed in this work, is available\nat Github.\n", "link": "http://arxiv.org/abs/2410.02362v2", "date": "2025-10-07", "relevancy": 2.527, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5252}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20of%20Mamba%20Architectures%20for%20Medical%20Image%0A%20%20Analysis%3A%20Classification%2C%20Segmentation%2C%20Restoration%20and%20Beyond&body=Title%3A%20A%20Comprehensive%20Survey%20of%20Mamba%20Architectures%20for%20Medical%20Image%0A%20%20Analysis%3A%20Classification%2C%20Segmentation%2C%20Restoration%20and%20Beyond%0AAuthor%3A%20Shubhi%20Bansal%20and%20Sreeharish%20A%20and%20Madhava%20Prasath%20J%20and%20Manikandan%20S%20and%20Sreekanth%20Madisetty%20and%20Mohammad%20Zia%20Ur%20Rehman%20and%20Chandravardhan%20Singh%20Raghaw%20and%20Gaurav%20Duggal%20and%20Nagendra%20Kumar%0AAbstract%3A%20%20%20Mamba%2C%20a%20special%20case%20of%20the%20State%20Space%20Model%2C%20is%20gaining%20popularity%20as%20an%0Aalternative%20to%20template-based%20deep%20learning%20approaches%20in%20medical%20image%0Aanalysis.%20While%20transformers%20are%20powerful%20architectures%2C%20they%20have%20drawbacks%2C%0Aincluding%20quadratic%20computational%20complexity%20and%20an%20inability%20to%20address%0Along-range%20dependencies%20efficiently.%20This%20limitation%20affects%20the%20analysis%20of%0Alarge%20and%20complex%20datasets%20in%20medical%20imaging%2C%20where%20there%20are%20many%20spatial%20and%0Atemporal%20relationships.%20In%20contrast%2C%20Mamba%20offers%20benefits%20that%20make%20it%0Awell-suited%20for%20medical%20image%20analysis.%20It%20has%20linear%20time%20complexity%2C%20which%20is%0Aa%20significant%20improvement%20over%20transformers.%20Mamba%20processes%20longer%20sequences%0Awithout%20attention%20mechanisms%2C%20enabling%20faster%20inference%20and%20requiring%20less%0Amemory.%20Mamba%20also%20demonstrates%20strong%20performance%20in%20merging%20multimodal%20data%2C%0Aimproving%20diagnosis%20accuracy%20and%20patient%20outcomes.%20The%20organization%20of%20this%0Apaper%20allows%20readers%20to%20appreciate%20the%20capabilities%20of%20Mamba%20in%20medical%20imaging%0Astep%20by%20step.%20We%20begin%20by%20defining%20core%20concepts%20of%20SSMs%20and%20models%2C%20including%0AS4%2C%20S5%2C%20and%20S6%2C%20followed%20by%20an%20exploration%20of%20Mamba%20architectures%20such%20as%20pure%0AMamba%2C%20U-Net%20variants%2C%20and%20hybrid%20models%20with%20convolutional%20neural%20networks%2C%0Atransformers%2C%20and%20Graph%20Neural%20Networks.%20We%20also%20cover%20Mamba%20optimizations%2C%0Atechniques%20and%20adaptations%2C%20scanning%2C%20datasets%2C%20applications%2C%20experimental%0Aresults%2C%20and%20conclude%20with%20its%20challenges%20and%20future%20directions%20in%20medical%0Aimaging.%20This%20review%20aims%20to%20demonstrate%20the%20transformative%20potential%20of%20Mamba%0Ain%20overcoming%20existing%20barriers%20within%20medical%20imaging%20while%20paving%20the%20way%20for%0Ainnovative%20advancements%20in%20the%20field.%20A%20comprehensive%20list%20of%20Mamba%0Aarchitectures%20applied%20in%20the%20medical%20field%2C%20reviewed%20in%20this%20work%2C%20is%20available%0Aat%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520of%2520Mamba%2520Architectures%2520for%2520Medical%2520Image%250A%2520%2520Analysis%253A%2520Classification%252C%2520Segmentation%252C%2520Restoration%2520and%2520Beyond%26entry.906535625%3DShubhi%2520Bansal%2520and%2520Sreeharish%2520A%2520and%2520Madhava%2520Prasath%2520J%2520and%2520Manikandan%2520S%2520and%2520Sreekanth%2520Madisetty%2520and%2520Mohammad%2520Zia%2520Ur%2520Rehman%2520and%2520Chandravardhan%2520Singh%2520Raghaw%2520and%2520Gaurav%2520Duggal%2520and%2520Nagendra%2520Kumar%26entry.1292438233%3D%2520%2520Mamba%252C%2520a%2520special%2520case%2520of%2520the%2520State%2520Space%2520Model%252C%2520is%2520gaining%2520popularity%2520as%2520an%250Aalternative%2520to%2520template-based%2520deep%2520learning%2520approaches%2520in%2520medical%2520image%250Aanalysis.%2520While%2520transformers%2520are%2520powerful%2520architectures%252C%2520they%2520have%2520drawbacks%252C%250Aincluding%2520quadratic%2520computational%2520complexity%2520and%2520an%2520inability%2520to%2520address%250Along-range%2520dependencies%2520efficiently.%2520This%2520limitation%2520affects%2520the%2520analysis%2520of%250Alarge%2520and%2520complex%2520datasets%2520in%2520medical%2520imaging%252C%2520where%2520there%2520are%2520many%2520spatial%2520and%250Atemporal%2520relationships.%2520In%2520contrast%252C%2520Mamba%2520offers%2520benefits%2520that%2520make%2520it%250Awell-suited%2520for%2520medical%2520image%2520analysis.%2520It%2520has%2520linear%2520time%2520complexity%252C%2520which%2520is%250Aa%2520significant%2520improvement%2520over%2520transformers.%2520Mamba%2520processes%2520longer%2520sequences%250Awithout%2520attention%2520mechanisms%252C%2520enabling%2520faster%2520inference%2520and%2520requiring%2520less%250Amemory.%2520Mamba%2520also%2520demonstrates%2520strong%2520performance%2520in%2520merging%2520multimodal%2520data%252C%250Aimproving%2520diagnosis%2520accuracy%2520and%2520patient%2520outcomes.%2520The%2520organization%2520of%2520this%250Apaper%2520allows%2520readers%2520to%2520appreciate%2520the%2520capabilities%2520of%2520Mamba%2520in%2520medical%2520imaging%250Astep%2520by%2520step.%2520We%2520begin%2520by%2520defining%2520core%2520concepts%2520of%2520SSMs%2520and%2520models%252C%2520including%250AS4%252C%2520S5%252C%2520and%2520S6%252C%2520followed%2520by%2520an%2520exploration%2520of%2520Mamba%2520architectures%2520such%2520as%2520pure%250AMamba%252C%2520U-Net%2520variants%252C%2520and%2520hybrid%2520models%2520with%2520convolutional%2520neural%2520networks%252C%250Atransformers%252C%2520and%2520Graph%2520Neural%2520Networks.%2520We%2520also%2520cover%2520Mamba%2520optimizations%252C%250Atechniques%2520and%2520adaptations%252C%2520scanning%252C%2520datasets%252C%2520applications%252C%2520experimental%250Aresults%252C%2520and%2520conclude%2520with%2520its%2520challenges%2520and%2520future%2520directions%2520in%2520medical%250Aimaging.%2520This%2520review%2520aims%2520to%2520demonstrate%2520the%2520transformative%2520potential%2520of%2520Mamba%250Ain%2520overcoming%2520existing%2520barriers%2520within%2520medical%2520imaging%2520while%2520paving%2520the%2520way%2520for%250Ainnovative%2520advancements%2520in%2520the%2520field.%2520A%2520comprehensive%2520list%2520of%2520Mamba%250Aarchitectures%2520applied%2520in%2520the%2520medical%2520field%252C%2520reviewed%2520in%2520this%2520work%252C%2520is%2520available%250Aat%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Mamba%20Architectures%20for%20Medical%20Image%0A%20%20Analysis%3A%20Classification%2C%20Segmentation%2C%20Restoration%20and%20Beyond&entry.906535625=Shubhi%20Bansal%20and%20Sreeharish%20A%20and%20Madhava%20Prasath%20J%20and%20Manikandan%20S%20and%20Sreekanth%20Madisetty%20and%20Mohammad%20Zia%20Ur%20Rehman%20and%20Chandravardhan%20Singh%20Raghaw%20and%20Gaurav%20Duggal%20and%20Nagendra%20Kumar&entry.1292438233=%20%20Mamba%2C%20a%20special%20case%20of%20the%20State%20Space%20Model%2C%20is%20gaining%20popularity%20as%20an%0Aalternative%20to%20template-based%20deep%20learning%20approaches%20in%20medical%20image%0Aanalysis.%20While%20transformers%20are%20powerful%20architectures%2C%20they%20have%20drawbacks%2C%0Aincluding%20quadratic%20computational%20complexity%20and%20an%20inability%20to%20address%0Along-range%20dependencies%20efficiently.%20This%20limitation%20affects%20the%20analysis%20of%0Alarge%20and%20complex%20datasets%20in%20medical%20imaging%2C%20where%20there%20are%20many%20spatial%20and%0Atemporal%20relationships.%20In%20contrast%2C%20Mamba%20offers%20benefits%20that%20make%20it%0Awell-suited%20for%20medical%20image%20analysis.%20It%20has%20linear%20time%20complexity%2C%20which%20is%0Aa%20significant%20improvement%20over%20transformers.%20Mamba%20processes%20longer%20sequences%0Awithout%20attention%20mechanisms%2C%20enabling%20faster%20inference%20and%20requiring%20less%0Amemory.%20Mamba%20also%20demonstrates%20strong%20performance%20in%20merging%20multimodal%20data%2C%0Aimproving%20diagnosis%20accuracy%20and%20patient%20outcomes.%20The%20organization%20of%20this%0Apaper%20allows%20readers%20to%20appreciate%20the%20capabilities%20of%20Mamba%20in%20medical%20imaging%0Astep%20by%20step.%20We%20begin%20by%20defining%20core%20concepts%20of%20SSMs%20and%20models%2C%20including%0AS4%2C%20S5%2C%20and%20S6%2C%20followed%20by%20an%20exploration%20of%20Mamba%20architectures%20such%20as%20pure%0AMamba%2C%20U-Net%20variants%2C%20and%20hybrid%20models%20with%20convolutional%20neural%20networks%2C%0Atransformers%2C%20and%20Graph%20Neural%20Networks.%20We%20also%20cover%20Mamba%20optimizations%2C%0Atechniques%20and%20adaptations%2C%20scanning%2C%20datasets%2C%20applications%2C%20experimental%0Aresults%2C%20and%20conclude%20with%20its%20challenges%20and%20future%20directions%20in%20medical%0Aimaging.%20This%20review%20aims%20to%20demonstrate%20the%20transformative%20potential%20of%20Mamba%0Ain%20overcoming%20existing%20barriers%20within%20medical%20imaging%20while%20paving%20the%20way%20for%0Ainnovative%20advancements%20in%20the%20field.%20A%20comprehensive%20list%20of%20Mamba%0Aarchitectures%20applied%20in%20the%20medical%20field%2C%20reviewed%20in%20this%20work%2C%20is%20available%0Aat%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02362v2&entry.124074799=Read"},
{"title": "Training-Free Time Series Classification via In-Context Reasoning with\n  LLM Agents", "author": "Songyuan Sui and Zihang Xu and Yu-Neng Chuang and Kwei-Herng Lai and Xia Hu", "abstract": "  Time series classification (TSC) spans diverse application scenarios, yet\nlabeled data are often scarce, making task-specific training costly and\ninflexible. Recent reasoning-oriented large language models (LLMs) show promise\nin understanding temporal patterns, but purely zero-shot usage remains\nsuboptimal. We propose FETA, a multi-agent framework for training-free TSC via\nexemplar-based in-context reasoning. FETA decomposes a multivariate series into\nchannel-wise subproblems, retrieves a few structurally similar labeled examples\nfor each channel, and leverages a reasoning LLM to compare the query against\nthese exemplars, producing channel-level labels with self-assessed confidences;\na confidence-weighted aggregator then fuses all channel decisions. This design\neliminates the need for pretraining or fine-tuning, improves efficiency by\npruning irrelevant channels and controlling input length, and enhances\ninterpretability through exemplar grounding and confidence estimation. On nine\nchallenging UEA datasets, FETA achieves strong accuracy under a fully\ntraining-free setting, surpassing multiple trained baselines. These results\ndemonstrate that a multi-agent in-context reasoning framework can transform\nLLMs into competitive, plug-and-play TSC solvers without any parameter\ntraining. The code is available at https://github.com/SongyuanSui/FETATSC.\n", "link": "http://arxiv.org/abs/2510.05950v1", "date": "2025-10-07", "relevancy": 2.516, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.52}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4968}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Time%20Series%20Classification%20via%20In-Context%20Reasoning%20with%0A%20%20LLM%20Agents&body=Title%3A%20Training-Free%20Time%20Series%20Classification%20via%20In-Context%20Reasoning%20with%0A%20%20LLM%20Agents%0AAuthor%3A%20Songyuan%20Sui%20and%20Zihang%20Xu%20and%20Yu-Neng%20Chuang%20and%20Kwei-Herng%20Lai%20and%20Xia%20Hu%0AAbstract%3A%20%20%20Time%20series%20classification%20%28TSC%29%20spans%20diverse%20application%20scenarios%2C%20yet%0Alabeled%20data%20are%20often%20scarce%2C%20making%20task-specific%20training%20costly%20and%0Ainflexible.%20Recent%20reasoning-oriented%20large%20language%20models%20%28LLMs%29%20show%20promise%0Ain%20understanding%20temporal%20patterns%2C%20but%20purely%20zero-shot%20usage%20remains%0Asuboptimal.%20We%20propose%20FETA%2C%20a%20multi-agent%20framework%20for%20training-free%20TSC%20via%0Aexemplar-based%20in-context%20reasoning.%20FETA%20decomposes%20a%20multivariate%20series%20into%0Achannel-wise%20subproblems%2C%20retrieves%20a%20few%20structurally%20similar%20labeled%20examples%0Afor%20each%20channel%2C%20and%20leverages%20a%20reasoning%20LLM%20to%20compare%20the%20query%20against%0Athese%20exemplars%2C%20producing%20channel-level%20labels%20with%20self-assessed%20confidences%3B%0Aa%20confidence-weighted%20aggregator%20then%20fuses%20all%20channel%20decisions.%20This%20design%0Aeliminates%20the%20need%20for%20pretraining%20or%20fine-tuning%2C%20improves%20efficiency%20by%0Apruning%20irrelevant%20channels%20and%20controlling%20input%20length%2C%20and%20enhances%0Ainterpretability%20through%20exemplar%20grounding%20and%20confidence%20estimation.%20On%20nine%0Achallenging%20UEA%20datasets%2C%20FETA%20achieves%20strong%20accuracy%20under%20a%20fully%0Atraining-free%20setting%2C%20surpassing%20multiple%20trained%20baselines.%20These%20results%0Ademonstrate%20that%20a%20multi-agent%20in-context%20reasoning%20framework%20can%20transform%0ALLMs%20into%20competitive%2C%20plug-and-play%20TSC%20solvers%20without%20any%20parameter%0Atraining.%20The%20code%20is%20available%20at%20https%3A//github.com/SongyuanSui/FETATSC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Time%2520Series%2520Classification%2520via%2520In-Context%2520Reasoning%2520with%250A%2520%2520LLM%2520Agents%26entry.906535625%3DSongyuan%2520Sui%2520and%2520Zihang%2520Xu%2520and%2520Yu-Neng%2520Chuang%2520and%2520Kwei-Herng%2520Lai%2520and%2520Xia%2520Hu%26entry.1292438233%3D%2520%2520Time%2520series%2520classification%2520%2528TSC%2529%2520spans%2520diverse%2520application%2520scenarios%252C%2520yet%250Alabeled%2520data%2520are%2520often%2520scarce%252C%2520making%2520task-specific%2520training%2520costly%2520and%250Ainflexible.%2520Recent%2520reasoning-oriented%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%2520promise%250Ain%2520understanding%2520temporal%2520patterns%252C%2520but%2520purely%2520zero-shot%2520usage%2520remains%250Asuboptimal.%2520We%2520propose%2520FETA%252C%2520a%2520multi-agent%2520framework%2520for%2520training-free%2520TSC%2520via%250Aexemplar-based%2520in-context%2520reasoning.%2520FETA%2520decomposes%2520a%2520multivariate%2520series%2520into%250Achannel-wise%2520subproblems%252C%2520retrieves%2520a%2520few%2520structurally%2520similar%2520labeled%2520examples%250Afor%2520each%2520channel%252C%2520and%2520leverages%2520a%2520reasoning%2520LLM%2520to%2520compare%2520the%2520query%2520against%250Athese%2520exemplars%252C%2520producing%2520channel-level%2520labels%2520with%2520self-assessed%2520confidences%253B%250Aa%2520confidence-weighted%2520aggregator%2520then%2520fuses%2520all%2520channel%2520decisions.%2520This%2520design%250Aeliminates%2520the%2520need%2520for%2520pretraining%2520or%2520fine-tuning%252C%2520improves%2520efficiency%2520by%250Apruning%2520irrelevant%2520channels%2520and%2520controlling%2520input%2520length%252C%2520and%2520enhances%250Ainterpretability%2520through%2520exemplar%2520grounding%2520and%2520confidence%2520estimation.%2520On%2520nine%250Achallenging%2520UEA%2520datasets%252C%2520FETA%2520achieves%2520strong%2520accuracy%2520under%2520a%2520fully%250Atraining-free%2520setting%252C%2520surpassing%2520multiple%2520trained%2520baselines.%2520These%2520results%250Ademonstrate%2520that%2520a%2520multi-agent%2520in-context%2520reasoning%2520framework%2520can%2520transform%250ALLMs%2520into%2520competitive%252C%2520plug-and-play%2520TSC%2520solvers%2520without%2520any%2520parameter%250Atraining.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/SongyuanSui/FETATSC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Time%20Series%20Classification%20via%20In-Context%20Reasoning%20with%0A%20%20LLM%20Agents&entry.906535625=Songyuan%20Sui%20and%20Zihang%20Xu%20and%20Yu-Neng%20Chuang%20and%20Kwei-Herng%20Lai%20and%20Xia%20Hu&entry.1292438233=%20%20Time%20series%20classification%20%28TSC%29%20spans%20diverse%20application%20scenarios%2C%20yet%0Alabeled%20data%20are%20often%20scarce%2C%20making%20task-specific%20training%20costly%20and%0Ainflexible.%20Recent%20reasoning-oriented%20large%20language%20models%20%28LLMs%29%20show%20promise%0Ain%20understanding%20temporal%20patterns%2C%20but%20purely%20zero-shot%20usage%20remains%0Asuboptimal.%20We%20propose%20FETA%2C%20a%20multi-agent%20framework%20for%20training-free%20TSC%20via%0Aexemplar-based%20in-context%20reasoning.%20FETA%20decomposes%20a%20multivariate%20series%20into%0Achannel-wise%20subproblems%2C%20retrieves%20a%20few%20structurally%20similar%20labeled%20examples%0Afor%20each%20channel%2C%20and%20leverages%20a%20reasoning%20LLM%20to%20compare%20the%20query%20against%0Athese%20exemplars%2C%20producing%20channel-level%20labels%20with%20self-assessed%20confidences%3B%0Aa%20confidence-weighted%20aggregator%20then%20fuses%20all%20channel%20decisions.%20This%20design%0Aeliminates%20the%20need%20for%20pretraining%20or%20fine-tuning%2C%20improves%20efficiency%20by%0Apruning%20irrelevant%20channels%20and%20controlling%20input%20length%2C%20and%20enhances%0Ainterpretability%20through%20exemplar%20grounding%20and%20confidence%20estimation.%20On%20nine%0Achallenging%20UEA%20datasets%2C%20FETA%20achieves%20strong%20accuracy%20under%20a%20fully%0Atraining-free%20setting%2C%20surpassing%20multiple%20trained%20baselines.%20These%20results%0Ademonstrate%20that%20a%20multi-agent%20in-context%20reasoning%20framework%20can%20transform%0ALLMs%20into%20competitive%2C%20plug-and-play%20TSC%20solvers%20without%20any%20parameter%0Atraining.%20The%20code%20is%20available%20at%20https%3A//github.com/SongyuanSui/FETATSC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05950v1&entry.124074799=Read"},
{"title": "Spectrum Tuning: Post-Training for Distributional Coverage and\n  In-Context Steerability", "author": "Taylor Sorensen and Benjamin Newman and Jared Moore and Chan Park and Jillian Fisher and Niloofar Mireshghallah and Liwei Jiang and Yejin Choi", "abstract": "  Language model post-training has enhanced instruction-following and\nperformance on many downstream tasks, but also comes with an often-overlooked\ncost on tasks with many possible valid answers. We characterize three\ndesiderata for conditional distributional modeling: in-context steerability,\nvalid output space coverage, and distributional alignment, and document across\nthree model families how current post-training can reduce these properties. In\nparticular, we disambiguate between two kinds of in-context learning: ICL for\neliciting existing underlying knowledge or capabilities, and in-context\nsteerability, where a model must use in-context information to override its\npriors and steer to a novel data generating distribution. To better evaluate\nand improve these desiderata, we introduce Spectrum Suite, a large-scale\nresource compiled from >40 data sources and spanning >90 tasks requiring models\nto steer to and match diverse distributions ranging from varied human\npreferences to numerical distributions and more. We find that while current\npost-training techniques help elicit underlying capabilities and knowledge,\nthey hurt models' ability to flexibly steer in-context. To mitigate these\nissues, we propose Spectrum Tuning, a post-training method using Spectrum Suite\nto improve steerability and distributional coverage. We find that Spectrum\nTuning often improves over pretrained models and their instruction-tuned\ncounterparts, enhancing steerability, spanning more of the output space, and\nimproving distributional alignment on held-out datasets.\n", "link": "http://arxiv.org/abs/2510.06084v1", "date": "2025-10-07", "relevancy": 2.4994, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectrum%20Tuning%3A%20Post-Training%20for%20Distributional%20Coverage%20and%0A%20%20In-Context%20Steerability&body=Title%3A%20Spectrum%20Tuning%3A%20Post-Training%20for%20Distributional%20Coverage%20and%0A%20%20In-Context%20Steerability%0AAuthor%3A%20Taylor%20Sorensen%20and%20Benjamin%20Newman%20and%20Jared%20Moore%20and%20Chan%20Park%20and%20Jillian%20Fisher%20and%20Niloofar%20Mireshghallah%20and%20Liwei%20Jiang%20and%20Yejin%20Choi%0AAbstract%3A%20%20%20Language%20model%20post-training%20has%20enhanced%20instruction-following%20and%0Aperformance%20on%20many%20downstream%20tasks%2C%20but%20also%20comes%20with%20an%20often-overlooked%0Acost%20on%20tasks%20with%20many%20possible%20valid%20answers.%20We%20characterize%20three%0Adesiderata%20for%20conditional%20distributional%20modeling%3A%20in-context%20steerability%2C%0Avalid%20output%20space%20coverage%2C%20and%20distributional%20alignment%2C%20and%20document%20across%0Athree%20model%20families%20how%20current%20post-training%20can%20reduce%20these%20properties.%20In%0Aparticular%2C%20we%20disambiguate%20between%20two%20kinds%20of%20in-context%20learning%3A%20ICL%20for%0Aeliciting%20existing%20underlying%20knowledge%20or%20capabilities%2C%20and%20in-context%0Asteerability%2C%20where%20a%20model%20must%20use%20in-context%20information%20to%20override%20its%0Apriors%20and%20steer%20to%20a%20novel%20data%20generating%20distribution.%20To%20better%20evaluate%0Aand%20improve%20these%20desiderata%2C%20we%20introduce%20Spectrum%20Suite%2C%20a%20large-scale%0Aresource%20compiled%20from%20%3E40%20data%20sources%20and%20spanning%20%3E90%20tasks%20requiring%20models%0Ato%20steer%20to%20and%20match%20diverse%20distributions%20ranging%20from%20varied%20human%0Apreferences%20to%20numerical%20distributions%20and%20more.%20We%20find%20that%20while%20current%0Apost-training%20techniques%20help%20elicit%20underlying%20capabilities%20and%20knowledge%2C%0Athey%20hurt%20models%27%20ability%20to%20flexibly%20steer%20in-context.%20To%20mitigate%20these%0Aissues%2C%20we%20propose%20Spectrum%20Tuning%2C%20a%20post-training%20method%20using%20Spectrum%20Suite%0Ato%20improve%20steerability%20and%20distributional%20coverage.%20We%20find%20that%20Spectrum%0ATuning%20often%20improves%20over%20pretrained%20models%20and%20their%20instruction-tuned%0Acounterparts%2C%20enhancing%20steerability%2C%20spanning%20more%20of%20the%20output%20space%2C%20and%0Aimproving%20distributional%20alignment%20on%20held-out%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectrum%2520Tuning%253A%2520Post-Training%2520for%2520Distributional%2520Coverage%2520and%250A%2520%2520In-Context%2520Steerability%26entry.906535625%3DTaylor%2520Sorensen%2520and%2520Benjamin%2520Newman%2520and%2520Jared%2520Moore%2520and%2520Chan%2520Park%2520and%2520Jillian%2520Fisher%2520and%2520Niloofar%2520Mireshghallah%2520and%2520Liwei%2520Jiang%2520and%2520Yejin%2520Choi%26entry.1292438233%3D%2520%2520Language%2520model%2520post-training%2520has%2520enhanced%2520instruction-following%2520and%250Aperformance%2520on%2520many%2520downstream%2520tasks%252C%2520but%2520also%2520comes%2520with%2520an%2520often-overlooked%250Acost%2520on%2520tasks%2520with%2520many%2520possible%2520valid%2520answers.%2520We%2520characterize%2520three%250Adesiderata%2520for%2520conditional%2520distributional%2520modeling%253A%2520in-context%2520steerability%252C%250Avalid%2520output%2520space%2520coverage%252C%2520and%2520distributional%2520alignment%252C%2520and%2520document%2520across%250Athree%2520model%2520families%2520how%2520current%2520post-training%2520can%2520reduce%2520these%2520properties.%2520In%250Aparticular%252C%2520we%2520disambiguate%2520between%2520two%2520kinds%2520of%2520in-context%2520learning%253A%2520ICL%2520for%250Aeliciting%2520existing%2520underlying%2520knowledge%2520or%2520capabilities%252C%2520and%2520in-context%250Asteerability%252C%2520where%2520a%2520model%2520must%2520use%2520in-context%2520information%2520to%2520override%2520its%250Apriors%2520and%2520steer%2520to%2520a%2520novel%2520data%2520generating%2520distribution.%2520To%2520better%2520evaluate%250Aand%2520improve%2520these%2520desiderata%252C%2520we%2520introduce%2520Spectrum%2520Suite%252C%2520a%2520large-scale%250Aresource%2520compiled%2520from%2520%253E40%2520data%2520sources%2520and%2520spanning%2520%253E90%2520tasks%2520requiring%2520models%250Ato%2520steer%2520to%2520and%2520match%2520diverse%2520distributions%2520ranging%2520from%2520varied%2520human%250Apreferences%2520to%2520numerical%2520distributions%2520and%2520more.%2520We%2520find%2520that%2520while%2520current%250Apost-training%2520techniques%2520help%2520elicit%2520underlying%2520capabilities%2520and%2520knowledge%252C%250Athey%2520hurt%2520models%2527%2520ability%2520to%2520flexibly%2520steer%2520in-context.%2520To%2520mitigate%2520these%250Aissues%252C%2520we%2520propose%2520Spectrum%2520Tuning%252C%2520a%2520post-training%2520method%2520using%2520Spectrum%2520Suite%250Ato%2520improve%2520steerability%2520and%2520distributional%2520coverage.%2520We%2520find%2520that%2520Spectrum%250ATuning%2520often%2520improves%2520over%2520pretrained%2520models%2520and%2520their%2520instruction-tuned%250Acounterparts%252C%2520enhancing%2520steerability%252C%2520spanning%2520more%2520of%2520the%2520output%2520space%252C%2520and%250Aimproving%2520distributional%2520alignment%2520on%2520held-out%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectrum%20Tuning%3A%20Post-Training%20for%20Distributional%20Coverage%20and%0A%20%20In-Context%20Steerability&entry.906535625=Taylor%20Sorensen%20and%20Benjamin%20Newman%20and%20Jared%20Moore%20and%20Chan%20Park%20and%20Jillian%20Fisher%20and%20Niloofar%20Mireshghallah%20and%20Liwei%20Jiang%20and%20Yejin%20Choi&entry.1292438233=%20%20Language%20model%20post-training%20has%20enhanced%20instruction-following%20and%0Aperformance%20on%20many%20downstream%20tasks%2C%20but%20also%20comes%20with%20an%20often-overlooked%0Acost%20on%20tasks%20with%20many%20possible%20valid%20answers.%20We%20characterize%20three%0Adesiderata%20for%20conditional%20distributional%20modeling%3A%20in-context%20steerability%2C%0Avalid%20output%20space%20coverage%2C%20and%20distributional%20alignment%2C%20and%20document%20across%0Athree%20model%20families%20how%20current%20post-training%20can%20reduce%20these%20properties.%20In%0Aparticular%2C%20we%20disambiguate%20between%20two%20kinds%20of%20in-context%20learning%3A%20ICL%20for%0Aeliciting%20existing%20underlying%20knowledge%20or%20capabilities%2C%20and%20in-context%0Asteerability%2C%20where%20a%20model%20must%20use%20in-context%20information%20to%20override%20its%0Apriors%20and%20steer%20to%20a%20novel%20data%20generating%20distribution.%20To%20better%20evaluate%0Aand%20improve%20these%20desiderata%2C%20we%20introduce%20Spectrum%20Suite%2C%20a%20large-scale%0Aresource%20compiled%20from%20%3E40%20data%20sources%20and%20spanning%20%3E90%20tasks%20requiring%20models%0Ato%20steer%20to%20and%20match%20diverse%20distributions%20ranging%20from%20varied%20human%0Apreferences%20to%20numerical%20distributions%20and%20more.%20We%20find%20that%20while%20current%0Apost-training%20techniques%20help%20elicit%20underlying%20capabilities%20and%20knowledge%2C%0Athey%20hurt%20models%27%20ability%20to%20flexibly%20steer%20in-context.%20To%20mitigate%20these%0Aissues%2C%20we%20propose%20Spectrum%20Tuning%2C%20a%20post-training%20method%20using%20Spectrum%20Suite%0Ato%20improve%20steerability%20and%20distributional%20coverage.%20We%20find%20that%20Spectrum%0ATuning%20often%20improves%20over%20pretrained%20models%20and%20their%20instruction-tuned%0Acounterparts%2C%20enhancing%20steerability%2C%20spanning%20more%20of%20the%20output%20space%2C%20and%0Aimproving%20distributional%20alignment%20on%20held-out%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06084v1&entry.124074799=Read"},
{"title": "Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning\n  in LLMs", "author": "Xueyan Li and Guinan Su and Mrinmaya Sachan and Jonas Geiping", "abstract": "  Large Language Models (LLMs) are increasingly applied to complex tasks that\nrequire extended reasoning. In such settings, models often benefit from diverse\nchains-of-thought to arrive at multiple candidate solutions. This requires two\ncompeting objectives: to inject enough stochasticity to explore multiple\nreasoning chains, and to ensure sufficient accuracy and quality in each path.\nExisting works pursue the first objective by increasing exploration at highly\nuncertain steps with higher temperature or larger candidate token sets, while\nothers improve reliability by rejecting samples with low confidence\npost-generation, implying that low confidence correlates with low answer\nquality. These two lines of thought are in conflict, as they conflate different\nsources of uncertainty. To resolve this, we argue that the decoding rule should\nbe calibrated by correctness, not confidence alone. We should sample from\ntokens with higher estimated correctness, and reduce sampling where expected\ncorrectness is low. We propose simple strategies that achieve this goal:\nGreedy-Threshold makes sampling greedy at very low confidence steps.\nCalibrated-TopK and Calibrated-epsilon set truncation threshold based on\nestimated rank-wise correctness. Together, our findings challenge prevailing\nheuristics about decoding under uncertainty and show gains across math and\ngeneral reasoning benchmarks.\n", "link": "http://arxiv.org/abs/2510.05987v1", "date": "2025-10-07", "relevancy": 2.4951, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5009}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5009}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample%20Smart%2C%20Not%20Hard%3A%20Correctness-First%20Decoding%20for%20Better%20Reasoning%0A%20%20in%20LLMs&body=Title%3A%20Sample%20Smart%2C%20Not%20Hard%3A%20Correctness-First%20Decoding%20for%20Better%20Reasoning%0A%20%20in%20LLMs%0AAuthor%3A%20Xueyan%20Li%20and%20Guinan%20Su%20and%20Mrinmaya%20Sachan%20and%20Jonas%20Geiping%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20applied%20to%20complex%20tasks%20that%0Arequire%20extended%20reasoning.%20In%20such%20settings%2C%20models%20often%20benefit%20from%20diverse%0Achains-of-thought%20to%20arrive%20at%20multiple%20candidate%20solutions.%20This%20requires%20two%0Acompeting%20objectives%3A%20to%20inject%20enough%20stochasticity%20to%20explore%20multiple%0Areasoning%20chains%2C%20and%20to%20ensure%20sufficient%20accuracy%20and%20quality%20in%20each%20path.%0AExisting%20works%20pursue%20the%20first%20objective%20by%20increasing%20exploration%20at%20highly%0Auncertain%20steps%20with%20higher%20temperature%20or%20larger%20candidate%20token%20sets%2C%20while%0Aothers%20improve%20reliability%20by%20rejecting%20samples%20with%20low%20confidence%0Apost-generation%2C%20implying%20that%20low%20confidence%20correlates%20with%20low%20answer%0Aquality.%20These%20two%20lines%20of%20thought%20are%20in%20conflict%2C%20as%20they%20conflate%20different%0Asources%20of%20uncertainty.%20To%20resolve%20this%2C%20we%20argue%20that%20the%20decoding%20rule%20should%0Abe%20calibrated%20by%20correctness%2C%20not%20confidence%20alone.%20We%20should%20sample%20from%0Atokens%20with%20higher%20estimated%20correctness%2C%20and%20reduce%20sampling%20where%20expected%0Acorrectness%20is%20low.%20We%20propose%20simple%20strategies%20that%20achieve%20this%20goal%3A%0AGreedy-Threshold%20makes%20sampling%20greedy%20at%20very%20low%20confidence%20steps.%0ACalibrated-TopK%20and%20Calibrated-epsilon%20set%20truncation%20threshold%20based%20on%0Aestimated%20rank-wise%20correctness.%20Together%2C%20our%20findings%20challenge%20prevailing%0Aheuristics%20about%20decoding%20under%20uncertainty%20and%20show%20gains%20across%20math%20and%0Ageneral%20reasoning%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample%2520Smart%252C%2520Not%2520Hard%253A%2520Correctness-First%2520Decoding%2520for%2520Better%2520Reasoning%250A%2520%2520in%2520LLMs%26entry.906535625%3DXueyan%2520Li%2520and%2520Guinan%2520Su%2520and%2520Mrinmaya%2520Sachan%2520and%2520Jonas%2520Geiping%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520applied%2520to%2520complex%2520tasks%2520that%250Arequire%2520extended%2520reasoning.%2520In%2520such%2520settings%252C%2520models%2520often%2520benefit%2520from%2520diverse%250Achains-of-thought%2520to%2520arrive%2520at%2520multiple%2520candidate%2520solutions.%2520This%2520requires%2520two%250Acompeting%2520objectives%253A%2520to%2520inject%2520enough%2520stochasticity%2520to%2520explore%2520multiple%250Areasoning%2520chains%252C%2520and%2520to%2520ensure%2520sufficient%2520accuracy%2520and%2520quality%2520in%2520each%2520path.%250AExisting%2520works%2520pursue%2520the%2520first%2520objective%2520by%2520increasing%2520exploration%2520at%2520highly%250Auncertain%2520steps%2520with%2520higher%2520temperature%2520or%2520larger%2520candidate%2520token%2520sets%252C%2520while%250Aothers%2520improve%2520reliability%2520by%2520rejecting%2520samples%2520with%2520low%2520confidence%250Apost-generation%252C%2520implying%2520that%2520low%2520confidence%2520correlates%2520with%2520low%2520answer%250Aquality.%2520These%2520two%2520lines%2520of%2520thought%2520are%2520in%2520conflict%252C%2520as%2520they%2520conflate%2520different%250Asources%2520of%2520uncertainty.%2520To%2520resolve%2520this%252C%2520we%2520argue%2520that%2520the%2520decoding%2520rule%2520should%250Abe%2520calibrated%2520by%2520correctness%252C%2520not%2520confidence%2520alone.%2520We%2520should%2520sample%2520from%250Atokens%2520with%2520higher%2520estimated%2520correctness%252C%2520and%2520reduce%2520sampling%2520where%2520expected%250Acorrectness%2520is%2520low.%2520We%2520propose%2520simple%2520strategies%2520that%2520achieve%2520this%2520goal%253A%250AGreedy-Threshold%2520makes%2520sampling%2520greedy%2520at%2520very%2520low%2520confidence%2520steps.%250ACalibrated-TopK%2520and%2520Calibrated-epsilon%2520set%2520truncation%2520threshold%2520based%2520on%250Aestimated%2520rank-wise%2520correctness.%2520Together%252C%2520our%2520findings%2520challenge%2520prevailing%250Aheuristics%2520about%2520decoding%2520under%2520uncertainty%2520and%2520show%2520gains%2520across%2520math%2520and%250Ageneral%2520reasoning%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%20Smart%2C%20Not%20Hard%3A%20Correctness-First%20Decoding%20for%20Better%20Reasoning%0A%20%20in%20LLMs&entry.906535625=Xueyan%20Li%20and%20Guinan%20Su%20and%20Mrinmaya%20Sachan%20and%20Jonas%20Geiping&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20applied%20to%20complex%20tasks%20that%0Arequire%20extended%20reasoning.%20In%20such%20settings%2C%20models%20often%20benefit%20from%20diverse%0Achains-of-thought%20to%20arrive%20at%20multiple%20candidate%20solutions.%20This%20requires%20two%0Acompeting%20objectives%3A%20to%20inject%20enough%20stochasticity%20to%20explore%20multiple%0Areasoning%20chains%2C%20and%20to%20ensure%20sufficient%20accuracy%20and%20quality%20in%20each%20path.%0AExisting%20works%20pursue%20the%20first%20objective%20by%20increasing%20exploration%20at%20highly%0Auncertain%20steps%20with%20higher%20temperature%20or%20larger%20candidate%20token%20sets%2C%20while%0Aothers%20improve%20reliability%20by%20rejecting%20samples%20with%20low%20confidence%0Apost-generation%2C%20implying%20that%20low%20confidence%20correlates%20with%20low%20answer%0Aquality.%20These%20two%20lines%20of%20thought%20are%20in%20conflict%2C%20as%20they%20conflate%20different%0Asources%20of%20uncertainty.%20To%20resolve%20this%2C%20we%20argue%20that%20the%20decoding%20rule%20should%0Abe%20calibrated%20by%20correctness%2C%20not%20confidence%20alone.%20We%20should%20sample%20from%0Atokens%20with%20higher%20estimated%20correctness%2C%20and%20reduce%20sampling%20where%20expected%0Acorrectness%20is%20low.%20We%20propose%20simple%20strategies%20that%20achieve%20this%20goal%3A%0AGreedy-Threshold%20makes%20sampling%20greedy%20at%20very%20low%20confidence%20steps.%0ACalibrated-TopK%20and%20Calibrated-epsilon%20set%20truncation%20threshold%20based%20on%0Aestimated%20rank-wise%20correctness.%20Together%2C%20our%20findings%20challenge%20prevailing%0Aheuristics%20about%20decoding%20under%20uncertainty%20and%20show%20gains%20across%20math%20and%0Ageneral%20reasoning%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05987v1&entry.124074799=Read"},
{"title": "How Foundational are Foundation Models for Time Series Forecasting?", "author": "Nouha Karaouli and Denis Coquenet and Elisa Fromont and Martial Mermillod and Marina Reyboz", "abstract": "  Foundation Models are designed to serve as versatile embedding machines, with\nstrong zero shot capabilities and superior generalization performance when\nfine-tuned on diverse downstream tasks. While this is largely true for language\nand vision foundation models, we argue that the inherent diversity of time\nseries data makes them less suited for building effective foundation models. We\ndemonstrate this using forecasting as our downstream task. We show that the\nzero-shot capabilities of a time series foundation model are significantly\ninfluenced and tied to the specific domains it has been pretrained on.\nFurthermore, when applied to unseen real-world time series data, fine-tuned\nfoundation models do not consistently yield substantially better results,\nrelative to their increased parameter count and memory footprint, than smaller,\ndedicated models tailored to the specific forecasting task at hand.\n", "link": "http://arxiv.org/abs/2510.00742v3", "date": "2025-10-07", "relevancy": 2.4752, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Foundational%20are%20Foundation%20Models%20for%20Time%20Series%20Forecasting%3F&body=Title%3A%20How%20Foundational%20are%20Foundation%20Models%20for%20Time%20Series%20Forecasting%3F%0AAuthor%3A%20Nouha%20Karaouli%20and%20Denis%20Coquenet%20and%20Elisa%20Fromont%20and%20Martial%20Mermillod%20and%20Marina%20Reyboz%0AAbstract%3A%20%20%20Foundation%20Models%20are%20designed%20to%20serve%20as%20versatile%20embedding%20machines%2C%20with%0Astrong%20zero%20shot%20capabilities%20and%20superior%20generalization%20performance%20when%0Afine-tuned%20on%20diverse%20downstream%20tasks.%20While%20this%20is%20largely%20true%20for%20language%0Aand%20vision%20foundation%20models%2C%20we%20argue%20that%20the%20inherent%20diversity%20of%20time%0Aseries%20data%20makes%20them%20less%20suited%20for%20building%20effective%20foundation%20models.%20We%0Ademonstrate%20this%20using%20forecasting%20as%20our%20downstream%20task.%20We%20show%20that%20the%0Azero-shot%20capabilities%20of%20a%20time%20series%20foundation%20model%20are%20significantly%0Ainfluenced%20and%20tied%20to%20the%20specific%20domains%20it%20has%20been%20pretrained%20on.%0AFurthermore%2C%20when%20applied%20to%20unseen%20real-world%20time%20series%20data%2C%20fine-tuned%0Afoundation%20models%20do%20not%20consistently%20yield%20substantially%20better%20results%2C%0Arelative%20to%20their%20increased%20parameter%20count%20and%20memory%20footprint%2C%20than%20smaller%2C%0Adedicated%20models%20tailored%20to%20the%20specific%20forecasting%20task%20at%20hand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00742v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Foundational%2520are%2520Foundation%2520Models%2520for%2520Time%2520Series%2520Forecasting%253F%26entry.906535625%3DNouha%2520Karaouli%2520and%2520Denis%2520Coquenet%2520and%2520Elisa%2520Fromont%2520and%2520Martial%2520Mermillod%2520and%2520Marina%2520Reyboz%26entry.1292438233%3D%2520%2520Foundation%2520Models%2520are%2520designed%2520to%2520serve%2520as%2520versatile%2520embedding%2520machines%252C%2520with%250Astrong%2520zero%2520shot%2520capabilities%2520and%2520superior%2520generalization%2520performance%2520when%250Afine-tuned%2520on%2520diverse%2520downstream%2520tasks.%2520While%2520this%2520is%2520largely%2520true%2520for%2520language%250Aand%2520vision%2520foundation%2520models%252C%2520we%2520argue%2520that%2520the%2520inherent%2520diversity%2520of%2520time%250Aseries%2520data%2520makes%2520them%2520less%2520suited%2520for%2520building%2520effective%2520foundation%2520models.%2520We%250Ademonstrate%2520this%2520using%2520forecasting%2520as%2520our%2520downstream%2520task.%2520We%2520show%2520that%2520the%250Azero-shot%2520capabilities%2520of%2520a%2520time%2520series%2520foundation%2520model%2520are%2520significantly%250Ainfluenced%2520and%2520tied%2520to%2520the%2520specific%2520domains%2520it%2520has%2520been%2520pretrained%2520on.%250AFurthermore%252C%2520when%2520applied%2520to%2520unseen%2520real-world%2520time%2520series%2520data%252C%2520fine-tuned%250Afoundation%2520models%2520do%2520not%2520consistently%2520yield%2520substantially%2520better%2520results%252C%250Arelative%2520to%2520their%2520increased%2520parameter%2520count%2520and%2520memory%2520footprint%252C%2520than%2520smaller%252C%250Adedicated%2520models%2520tailored%2520to%2520the%2520specific%2520forecasting%2520task%2520at%2520hand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00742v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Foundational%20are%20Foundation%20Models%20for%20Time%20Series%20Forecasting%3F&entry.906535625=Nouha%20Karaouli%20and%20Denis%20Coquenet%20and%20Elisa%20Fromont%20and%20Martial%20Mermillod%20and%20Marina%20Reyboz&entry.1292438233=%20%20Foundation%20Models%20are%20designed%20to%20serve%20as%20versatile%20embedding%20machines%2C%20with%0Astrong%20zero%20shot%20capabilities%20and%20superior%20generalization%20performance%20when%0Afine-tuned%20on%20diverse%20downstream%20tasks.%20While%20this%20is%20largely%20true%20for%20language%0Aand%20vision%20foundation%20models%2C%20we%20argue%20that%20the%20inherent%20diversity%20of%20time%0Aseries%20data%20makes%20them%20less%20suited%20for%20building%20effective%20foundation%20models.%20We%0Ademonstrate%20this%20using%20forecasting%20as%20our%20downstream%20task.%20We%20show%20that%20the%0Azero-shot%20capabilities%20of%20a%20time%20series%20foundation%20model%20are%20significantly%0Ainfluenced%20and%20tied%20to%20the%20specific%20domains%20it%20has%20been%20pretrained%20on.%0AFurthermore%2C%20when%20applied%20to%20unseen%20real-world%20time%20series%20data%2C%20fine-tuned%0Afoundation%20models%20do%20not%20consistently%20yield%20substantially%20better%20results%2C%0Arelative%20to%20their%20increased%20parameter%20count%20and%20memory%20footprint%2C%20than%20smaller%2C%0Adedicated%20models%20tailored%20to%20the%20specific%20forecasting%20task%20at%20hand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00742v3&entry.124074799=Read"},
{"title": "Understanding Catastrophic Interference: On the Identifibility of Latent\n  Representations", "author": "Yuke Li and Yujia Zheng and Tianyi Xiong and Zhenyi Wang and Heng Huang", "abstract": "  Catastrophic interference, also known as catastrophic forgetting, is a\nfundamental challenge in machine learning, where a trained learning model\nprogressively loses performance on previously learned tasks when adapting to\nnew ones. In this paper, we aim to better understand and model the catastrophic\ninterference problem from a latent representation learning point of view, and\npropose a novel theoretical framework that formulates catastrophic interference\nas an identification problem. Our analysis demonstrates that the forgetting\nphenomenon can be quantified by the distance between partial-task aware (PTA)\nand all-task aware (ATA) setups. Building upon recent advances in\nidentifiability theory, we prove that this distance can be minimized through\nidentification of shared latent variables between these setups. When learning,\nwe propose our method \\ourmeos with two-stage training strategy: First, we\nemploy maximum likelihood estimation to learn the latent representations from\nboth PTA and ATA configurations. Subsequently, we optimize the KL divergence to\nidentify and learn the shared latent variables. Through theoretical guarantee\nand empirical validations, we establish that identifying and learning these\nshared representations can effectively mitigate catastrophic interference in\nmachine learning systems. Our approach provides both theoretical guarantees and\npractical performance improvements across both synthetic and benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2509.23027v3", "date": "2025-10-07", "relevancy": 2.4747, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.503}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4919}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Catastrophic%20Interference%3A%20On%20the%20Identifibility%20of%20Latent%0A%20%20Representations&body=Title%3A%20Understanding%20Catastrophic%20Interference%3A%20On%20the%20Identifibility%20of%20Latent%0A%20%20Representations%0AAuthor%3A%20Yuke%20Li%20and%20Yujia%20Zheng%20and%20Tianyi%20Xiong%20and%20Zhenyi%20Wang%20and%20Heng%20Huang%0AAbstract%3A%20%20%20Catastrophic%20interference%2C%20also%20known%20as%20catastrophic%20forgetting%2C%20is%20a%0Afundamental%20challenge%20in%20machine%20learning%2C%20where%20a%20trained%20learning%20model%0Aprogressively%20loses%20performance%20on%20previously%20learned%20tasks%20when%20adapting%20to%0Anew%20ones.%20In%20this%20paper%2C%20we%20aim%20to%20better%20understand%20and%20model%20the%20catastrophic%0Ainterference%20problem%20from%20a%20latent%20representation%20learning%20point%20of%20view%2C%20and%0Apropose%20a%20novel%20theoretical%20framework%20that%20formulates%20catastrophic%20interference%0Aas%20an%20identification%20problem.%20Our%20analysis%20demonstrates%20that%20the%20forgetting%0Aphenomenon%20can%20be%20quantified%20by%20the%20distance%20between%20partial-task%20aware%20%28PTA%29%0Aand%20all-task%20aware%20%28ATA%29%20setups.%20Building%20upon%20recent%20advances%20in%0Aidentifiability%20theory%2C%20we%20prove%20that%20this%20distance%20can%20be%20minimized%20through%0Aidentification%20of%20shared%20latent%20variables%20between%20these%20setups.%20When%20learning%2C%0Awe%20propose%20our%20method%20%5Courmeos%20with%20two-stage%20training%20strategy%3A%20First%2C%20we%0Aemploy%20maximum%20likelihood%20estimation%20to%20learn%20the%20latent%20representations%20from%0Aboth%20PTA%20and%20ATA%20configurations.%20Subsequently%2C%20we%20optimize%20the%20KL%20divergence%20to%0Aidentify%20and%20learn%20the%20shared%20latent%20variables.%20Through%20theoretical%20guarantee%0Aand%20empirical%20validations%2C%20we%20establish%20that%20identifying%20and%20learning%20these%0Ashared%20representations%20can%20effectively%20mitigate%20catastrophic%20interference%20in%0Amachine%20learning%20systems.%20Our%20approach%20provides%20both%20theoretical%20guarantees%20and%0Apractical%20performance%20improvements%20across%20both%20synthetic%20and%20benchmark%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23027v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Catastrophic%2520Interference%253A%2520On%2520the%2520Identifibility%2520of%2520Latent%250A%2520%2520Representations%26entry.906535625%3DYuke%2520Li%2520and%2520Yujia%2520Zheng%2520and%2520Tianyi%2520Xiong%2520and%2520Zhenyi%2520Wang%2520and%2520Heng%2520Huang%26entry.1292438233%3D%2520%2520Catastrophic%2520interference%252C%2520also%2520known%2520as%2520catastrophic%2520forgetting%252C%2520is%2520a%250Afundamental%2520challenge%2520in%2520machine%2520learning%252C%2520where%2520a%2520trained%2520learning%2520model%250Aprogressively%2520loses%2520performance%2520on%2520previously%2520learned%2520tasks%2520when%2520adapting%2520to%250Anew%2520ones.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520better%2520understand%2520and%2520model%2520the%2520catastrophic%250Ainterference%2520problem%2520from%2520a%2520latent%2520representation%2520learning%2520point%2520of%2520view%252C%2520and%250Apropose%2520a%2520novel%2520theoretical%2520framework%2520that%2520formulates%2520catastrophic%2520interference%250Aas%2520an%2520identification%2520problem.%2520Our%2520analysis%2520demonstrates%2520that%2520the%2520forgetting%250Aphenomenon%2520can%2520be%2520quantified%2520by%2520the%2520distance%2520between%2520partial-task%2520aware%2520%2528PTA%2529%250Aand%2520all-task%2520aware%2520%2528ATA%2529%2520setups.%2520Building%2520upon%2520recent%2520advances%2520in%250Aidentifiability%2520theory%252C%2520we%2520prove%2520that%2520this%2520distance%2520can%2520be%2520minimized%2520through%250Aidentification%2520of%2520shared%2520latent%2520variables%2520between%2520these%2520setups.%2520When%2520learning%252C%250Awe%2520propose%2520our%2520method%2520%255Courmeos%2520with%2520two-stage%2520training%2520strategy%253A%2520First%252C%2520we%250Aemploy%2520maximum%2520likelihood%2520estimation%2520to%2520learn%2520the%2520latent%2520representations%2520from%250Aboth%2520PTA%2520and%2520ATA%2520configurations.%2520Subsequently%252C%2520we%2520optimize%2520the%2520KL%2520divergence%2520to%250Aidentify%2520and%2520learn%2520the%2520shared%2520latent%2520variables.%2520Through%2520theoretical%2520guarantee%250Aand%2520empirical%2520validations%252C%2520we%2520establish%2520that%2520identifying%2520and%2520learning%2520these%250Ashared%2520representations%2520can%2520effectively%2520mitigate%2520catastrophic%2520interference%2520in%250Amachine%2520learning%2520systems.%2520Our%2520approach%2520provides%2520both%2520theoretical%2520guarantees%2520and%250Apractical%2520performance%2520improvements%2520across%2520both%2520synthetic%2520and%2520benchmark%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23027v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Catastrophic%20Interference%3A%20On%20the%20Identifibility%20of%20Latent%0A%20%20Representations&entry.906535625=Yuke%20Li%20and%20Yujia%20Zheng%20and%20Tianyi%20Xiong%20and%20Zhenyi%20Wang%20and%20Heng%20Huang&entry.1292438233=%20%20Catastrophic%20interference%2C%20also%20known%20as%20catastrophic%20forgetting%2C%20is%20a%0Afundamental%20challenge%20in%20machine%20learning%2C%20where%20a%20trained%20learning%20model%0Aprogressively%20loses%20performance%20on%20previously%20learned%20tasks%20when%20adapting%20to%0Anew%20ones.%20In%20this%20paper%2C%20we%20aim%20to%20better%20understand%20and%20model%20the%20catastrophic%0Ainterference%20problem%20from%20a%20latent%20representation%20learning%20point%20of%20view%2C%20and%0Apropose%20a%20novel%20theoretical%20framework%20that%20formulates%20catastrophic%20interference%0Aas%20an%20identification%20problem.%20Our%20analysis%20demonstrates%20that%20the%20forgetting%0Aphenomenon%20can%20be%20quantified%20by%20the%20distance%20between%20partial-task%20aware%20%28PTA%29%0Aand%20all-task%20aware%20%28ATA%29%20setups.%20Building%20upon%20recent%20advances%20in%0Aidentifiability%20theory%2C%20we%20prove%20that%20this%20distance%20can%20be%20minimized%20through%0Aidentification%20of%20shared%20latent%20variables%20between%20these%20setups.%20When%20learning%2C%0Awe%20propose%20our%20method%20%5Courmeos%20with%20two-stage%20training%20strategy%3A%20First%2C%20we%0Aemploy%20maximum%20likelihood%20estimation%20to%20learn%20the%20latent%20representations%20from%0Aboth%20PTA%20and%20ATA%20configurations.%20Subsequently%2C%20we%20optimize%20the%20KL%20divergence%20to%0Aidentify%20and%20learn%20the%20shared%20latent%20variables.%20Through%20theoretical%20guarantee%0Aand%20empirical%20validations%2C%20we%20establish%20that%20identifying%20and%20learning%20these%0Ashared%20representations%20can%20effectively%20mitigate%20catastrophic%20interference%20in%0Amachine%20learning%20systems.%20Our%20approach%20provides%20both%20theoretical%20guarantees%20and%0Apractical%20performance%20improvements%20across%20both%20synthetic%20and%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23027v3&entry.124074799=Read"},
{"title": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World\n  Literature", "author": "Alisha Srivastava and Emir Korukluoglu and Minh Nhat Le and Duyen Tran and Chau Minh Pham and Marzena Karpinska and Mohit Iyyer", "abstract": "  Large language models (LLMs) are known to memorize and recall English text\nfrom their pretraining data. However, the extent to which this ability\ngeneralizes to non-English languages or transfers across languages remains\nunclear. This paper investigates multilingual and cross-lingual memorization in\nLLMs, probing if memorized content in one language (e.g., English) can be\nrecalled when presented in translation. To do so, we introduce OWL, a dataset\nof 31.5K aligned excerpts from 20 books in ten languages, including English\noriginals, official translations (Vietnamese, Spanish, Turkish), and new\ntranslations in six low-resource languages (Sesotho, Yoruba, Maithili,\nMalagasy, Setswana, Tahitian). We evaluate memorization across model families\nand sizes through three tasks: (1) direct probing, which asks the model to\nidentify a book's title and author; (2) name cloze, which requires predicting\nmasked character names; and (3) prefix probing, which involves generating\ncontinuations. We find that LLMs consistently recall content across languages,\neven for texts without direct translation in pretraining data. GPT-4o, for\nexample, identifies authors and titles 69% of the time and masked entities 6%\nof the time in newly translated excerpts. Perturbations (e.g., masking\ncharacters, shuffling words) modestly reduce direct probing accuracy (7% drop\nfor shuffled official translations). Our results highlight the extent of\ncross-lingual memorization and provide insights on the differences between the\nmodels.\n", "link": "http://arxiv.org/abs/2505.22945v2", "date": "2025-10-07", "relevancy": 2.4741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OWL%3A%20Probing%20Cross-Lingual%20Recall%20of%20Memorized%20Texts%20via%20World%0A%20%20Literature&body=Title%3A%20OWL%3A%20Probing%20Cross-Lingual%20Recall%20of%20Memorized%20Texts%20via%20World%0A%20%20Literature%0AAuthor%3A%20Alisha%20Srivastava%20and%20Emir%20Korukluoglu%20and%20Minh%20Nhat%20Le%20and%20Duyen%20Tran%20and%20Chau%20Minh%20Pham%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20known%20to%20memorize%20and%20recall%20English%20text%0Afrom%20their%20pretraining%20data.%20However%2C%20the%20extent%20to%20which%20this%20ability%0Ageneralizes%20to%20non-English%20languages%20or%20transfers%20across%20languages%20remains%0Aunclear.%20This%20paper%20investigates%20multilingual%20and%20cross-lingual%20memorization%20in%0ALLMs%2C%20probing%20if%20memorized%20content%20in%20one%20language%20%28e.g.%2C%20English%29%20can%20be%0Arecalled%20when%20presented%20in%20translation.%20To%20do%20so%2C%20we%20introduce%20OWL%2C%20a%20dataset%0Aof%2031.5K%20aligned%20excerpts%20from%2020%20books%20in%20ten%20languages%2C%20including%20English%0Aoriginals%2C%20official%20translations%20%28Vietnamese%2C%20Spanish%2C%20Turkish%29%2C%20and%20new%0Atranslations%20in%20six%20low-resource%20languages%20%28Sesotho%2C%20Yoruba%2C%20Maithili%2C%0AMalagasy%2C%20Setswana%2C%20Tahitian%29.%20We%20evaluate%20memorization%20across%20model%20families%0Aand%20sizes%20through%20three%20tasks%3A%20%281%29%20direct%20probing%2C%20which%20asks%20the%20model%20to%0Aidentify%20a%20book%27s%20title%20and%20author%3B%20%282%29%20name%20cloze%2C%20which%20requires%20predicting%0Amasked%20character%20names%3B%20and%20%283%29%20prefix%20probing%2C%20which%20involves%20generating%0Acontinuations.%20We%20find%20that%20LLMs%20consistently%20recall%20content%20across%20languages%2C%0Aeven%20for%20texts%20without%20direct%20translation%20in%20pretraining%20data.%20GPT-4o%2C%20for%0Aexample%2C%20identifies%20authors%20and%20titles%2069%25%20of%20the%20time%20and%20masked%20entities%206%25%0Aof%20the%20time%20in%20newly%20translated%20excerpts.%20Perturbations%20%28e.g.%2C%20masking%0Acharacters%2C%20shuffling%20words%29%20modestly%20reduce%20direct%20probing%20accuracy%20%287%25%20drop%0Afor%20shuffled%20official%20translations%29.%20Our%20results%20highlight%20the%20extent%20of%0Across-lingual%20memorization%20and%20provide%20insights%20on%20the%20differences%20between%20the%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOWL%253A%2520Probing%2520Cross-Lingual%2520Recall%2520of%2520Memorized%2520Texts%2520via%2520World%250A%2520%2520Literature%26entry.906535625%3DAlisha%2520Srivastava%2520and%2520Emir%2520Korukluoglu%2520and%2520Minh%2520Nhat%2520Le%2520and%2520Duyen%2520Tran%2520and%2520Chau%2520Minh%2520Pham%2520and%2520Marzena%2520Karpinska%2520and%2520Mohit%2520Iyyer%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520known%2520to%2520memorize%2520and%2520recall%2520English%2520text%250Afrom%2520their%2520pretraining%2520data.%2520However%252C%2520the%2520extent%2520to%2520which%2520this%2520ability%250Ageneralizes%2520to%2520non-English%2520languages%2520or%2520transfers%2520across%2520languages%2520remains%250Aunclear.%2520This%2520paper%2520investigates%2520multilingual%2520and%2520cross-lingual%2520memorization%2520in%250ALLMs%252C%2520probing%2520if%2520memorized%2520content%2520in%2520one%2520language%2520%2528e.g.%252C%2520English%2529%2520can%2520be%250Arecalled%2520when%2520presented%2520in%2520translation.%2520To%2520do%2520so%252C%2520we%2520introduce%2520OWL%252C%2520a%2520dataset%250Aof%252031.5K%2520aligned%2520excerpts%2520from%252020%2520books%2520in%2520ten%2520languages%252C%2520including%2520English%250Aoriginals%252C%2520official%2520translations%2520%2528Vietnamese%252C%2520Spanish%252C%2520Turkish%2529%252C%2520and%2520new%250Atranslations%2520in%2520six%2520low-resource%2520languages%2520%2528Sesotho%252C%2520Yoruba%252C%2520Maithili%252C%250AMalagasy%252C%2520Setswana%252C%2520Tahitian%2529.%2520We%2520evaluate%2520memorization%2520across%2520model%2520families%250Aand%2520sizes%2520through%2520three%2520tasks%253A%2520%25281%2529%2520direct%2520probing%252C%2520which%2520asks%2520the%2520model%2520to%250Aidentify%2520a%2520book%2527s%2520title%2520and%2520author%253B%2520%25282%2529%2520name%2520cloze%252C%2520which%2520requires%2520predicting%250Amasked%2520character%2520names%253B%2520and%2520%25283%2529%2520prefix%2520probing%252C%2520which%2520involves%2520generating%250Acontinuations.%2520We%2520find%2520that%2520LLMs%2520consistently%2520recall%2520content%2520across%2520languages%252C%250Aeven%2520for%2520texts%2520without%2520direct%2520translation%2520in%2520pretraining%2520data.%2520GPT-4o%252C%2520for%250Aexample%252C%2520identifies%2520authors%2520and%2520titles%252069%2525%2520of%2520the%2520time%2520and%2520masked%2520entities%25206%2525%250Aof%2520the%2520time%2520in%2520newly%2520translated%2520excerpts.%2520Perturbations%2520%2528e.g.%252C%2520masking%250Acharacters%252C%2520shuffling%2520words%2529%2520modestly%2520reduce%2520direct%2520probing%2520accuracy%2520%25287%2525%2520drop%250Afor%2520shuffled%2520official%2520translations%2529.%2520Our%2520results%2520highlight%2520the%2520extent%2520of%250Across-lingual%2520memorization%2520and%2520provide%2520insights%2520on%2520the%2520differences%2520between%2520the%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OWL%3A%20Probing%20Cross-Lingual%20Recall%20of%20Memorized%20Texts%20via%20World%0A%20%20Literature&entry.906535625=Alisha%20Srivastava%20and%20Emir%20Korukluoglu%20and%20Minh%20Nhat%20Le%20and%20Duyen%20Tran%20and%20Chau%20Minh%20Pham%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20known%20to%20memorize%20and%20recall%20English%20text%0Afrom%20their%20pretraining%20data.%20However%2C%20the%20extent%20to%20which%20this%20ability%0Ageneralizes%20to%20non-English%20languages%20or%20transfers%20across%20languages%20remains%0Aunclear.%20This%20paper%20investigates%20multilingual%20and%20cross-lingual%20memorization%20in%0ALLMs%2C%20probing%20if%20memorized%20content%20in%20one%20language%20%28e.g.%2C%20English%29%20can%20be%0Arecalled%20when%20presented%20in%20translation.%20To%20do%20so%2C%20we%20introduce%20OWL%2C%20a%20dataset%0Aof%2031.5K%20aligned%20excerpts%20from%2020%20books%20in%20ten%20languages%2C%20including%20English%0Aoriginals%2C%20official%20translations%20%28Vietnamese%2C%20Spanish%2C%20Turkish%29%2C%20and%20new%0Atranslations%20in%20six%20low-resource%20languages%20%28Sesotho%2C%20Yoruba%2C%20Maithili%2C%0AMalagasy%2C%20Setswana%2C%20Tahitian%29.%20We%20evaluate%20memorization%20across%20model%20families%0Aand%20sizes%20through%20three%20tasks%3A%20%281%29%20direct%20probing%2C%20which%20asks%20the%20model%20to%0Aidentify%20a%20book%27s%20title%20and%20author%3B%20%282%29%20name%20cloze%2C%20which%20requires%20predicting%0Amasked%20character%20names%3B%20and%20%283%29%20prefix%20probing%2C%20which%20involves%20generating%0Acontinuations.%20We%20find%20that%20LLMs%20consistently%20recall%20content%20across%20languages%2C%0Aeven%20for%20texts%20without%20direct%20translation%20in%20pretraining%20data.%20GPT-4o%2C%20for%0Aexample%2C%20identifies%20authors%20and%20titles%2069%25%20of%20the%20time%20and%20masked%20entities%206%25%0Aof%20the%20time%20in%20newly%20translated%20excerpts.%20Perturbations%20%28e.g.%2C%20masking%0Acharacters%2C%20shuffling%20words%29%20modestly%20reduce%20direct%20probing%20accuracy%20%287%25%20drop%0Afor%20shuffled%20official%20translations%29.%20Our%20results%20highlight%20the%20extent%20of%0Across-lingual%20memorization%20and%20provide%20insights%20on%20the%20differences%20between%20the%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22945v2&entry.124074799=Read"},
{"title": "Dropping the D: RGB-D SLAM Without the Depth Sensor", "author": "Mert Kiray and Alican Karaomer and Benjamin Busam", "abstract": "  We present DropD-SLAM, a real-time monocular SLAM system that achieves\nRGB-D-level accuracy without relying on depth sensors. The system replaces\nactive depth input with three pretrained vision modules: a monocular metric\ndepth estimator, a learned keypoint detector, and an instance segmentation\nnetwork. Dynamic objects are suppressed using dilated instance masks, while\nstatic keypoints are assigned predicted depth values and backprojected into 3D\nto form metrically scaled features. These are processed by an unmodified RGB-D\nSLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM\nattains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences,\nmatching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS\non a single GPU. These results suggest that modern pretrained vision models can\nreplace active depth sensors as reliable, real-time sources of metric scale,\nmarking a step toward simpler and more cost-effective SLAM systems.\n", "link": "http://arxiv.org/abs/2510.06216v1", "date": "2025-10-07", "relevancy": 2.4595, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6878}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5636}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dropping%20the%20D%3A%20RGB-D%20SLAM%20Without%20the%20Depth%20Sensor&body=Title%3A%20Dropping%20the%20D%3A%20RGB-D%20SLAM%20Without%20the%20Depth%20Sensor%0AAuthor%3A%20Mert%20Kiray%20and%20Alican%20Karaomer%20and%20Benjamin%20Busam%0AAbstract%3A%20%20%20We%20present%20DropD-SLAM%2C%20a%20real-time%20monocular%20SLAM%20system%20that%20achieves%0ARGB-D-level%20accuracy%20without%20relying%20on%20depth%20sensors.%20The%20system%20replaces%0Aactive%20depth%20input%20with%20three%20pretrained%20vision%20modules%3A%20a%20monocular%20metric%0Adepth%20estimator%2C%20a%20learned%20keypoint%20detector%2C%20and%20an%20instance%20segmentation%0Anetwork.%20Dynamic%20objects%20are%20suppressed%20using%20dilated%20instance%20masks%2C%20while%0Astatic%20keypoints%20are%20assigned%20predicted%20depth%20values%20and%20backprojected%20into%203D%0Ato%20form%20metrically%20scaled%20features.%20These%20are%20processed%20by%20an%20unmodified%20RGB-D%0ASLAM%20back%20end%20for%20tracking%20and%20mapping.%20On%20the%20TUM%20RGB-D%20benchmark%2C%20DropD-SLAM%0Aattains%207.4%20cm%20mean%20ATE%20on%20static%20sequences%20and%201.8%20cm%20on%20dynamic%20sequences%2C%0Amatching%20or%20surpassing%20state-of-the-art%20RGB-D%20methods%20while%20operating%20at%2022%20FPS%0Aon%20a%20single%20GPU.%20These%20results%20suggest%20that%20modern%20pretrained%20vision%20models%20can%0Areplace%20active%20depth%20sensors%20as%20reliable%2C%20real-time%20sources%20of%20metric%20scale%2C%0Amarking%20a%20step%20toward%20simpler%20and%20more%20cost-effective%20SLAM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDropping%2520the%2520D%253A%2520RGB-D%2520SLAM%2520Without%2520the%2520Depth%2520Sensor%26entry.906535625%3DMert%2520Kiray%2520and%2520Alican%2520Karaomer%2520and%2520Benjamin%2520Busam%26entry.1292438233%3D%2520%2520We%2520present%2520DropD-SLAM%252C%2520a%2520real-time%2520monocular%2520SLAM%2520system%2520that%2520achieves%250ARGB-D-level%2520accuracy%2520without%2520relying%2520on%2520depth%2520sensors.%2520The%2520system%2520replaces%250Aactive%2520depth%2520input%2520with%2520three%2520pretrained%2520vision%2520modules%253A%2520a%2520monocular%2520metric%250Adepth%2520estimator%252C%2520a%2520learned%2520keypoint%2520detector%252C%2520and%2520an%2520instance%2520segmentation%250Anetwork.%2520Dynamic%2520objects%2520are%2520suppressed%2520using%2520dilated%2520instance%2520masks%252C%2520while%250Astatic%2520keypoints%2520are%2520assigned%2520predicted%2520depth%2520values%2520and%2520backprojected%2520into%25203D%250Ato%2520form%2520metrically%2520scaled%2520features.%2520These%2520are%2520processed%2520by%2520an%2520unmodified%2520RGB-D%250ASLAM%2520back%2520end%2520for%2520tracking%2520and%2520mapping.%2520On%2520the%2520TUM%2520RGB-D%2520benchmark%252C%2520DropD-SLAM%250Aattains%25207.4%2520cm%2520mean%2520ATE%2520on%2520static%2520sequences%2520and%25201.8%2520cm%2520on%2520dynamic%2520sequences%252C%250Amatching%2520or%2520surpassing%2520state-of-the-art%2520RGB-D%2520methods%2520while%2520operating%2520at%252022%2520FPS%250Aon%2520a%2520single%2520GPU.%2520These%2520results%2520suggest%2520that%2520modern%2520pretrained%2520vision%2520models%2520can%250Areplace%2520active%2520depth%2520sensors%2520as%2520reliable%252C%2520real-time%2520sources%2520of%2520metric%2520scale%252C%250Amarking%2520a%2520step%2520toward%2520simpler%2520and%2520more%2520cost-effective%2520SLAM%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dropping%20the%20D%3A%20RGB-D%20SLAM%20Without%20the%20Depth%20Sensor&entry.906535625=Mert%20Kiray%20and%20Alican%20Karaomer%20and%20Benjamin%20Busam&entry.1292438233=%20%20We%20present%20DropD-SLAM%2C%20a%20real-time%20monocular%20SLAM%20system%20that%20achieves%0ARGB-D-level%20accuracy%20without%20relying%20on%20depth%20sensors.%20The%20system%20replaces%0Aactive%20depth%20input%20with%20three%20pretrained%20vision%20modules%3A%20a%20monocular%20metric%0Adepth%20estimator%2C%20a%20learned%20keypoint%20detector%2C%20and%20an%20instance%20segmentation%0Anetwork.%20Dynamic%20objects%20are%20suppressed%20using%20dilated%20instance%20masks%2C%20while%0Astatic%20keypoints%20are%20assigned%20predicted%20depth%20values%20and%20backprojected%20into%203D%0Ato%20form%20metrically%20scaled%20features.%20These%20are%20processed%20by%20an%20unmodified%20RGB-D%0ASLAM%20back%20end%20for%20tracking%20and%20mapping.%20On%20the%20TUM%20RGB-D%20benchmark%2C%20DropD-SLAM%0Aattains%207.4%20cm%20mean%20ATE%20on%20static%20sequences%20and%201.8%20cm%20on%20dynamic%20sequences%2C%0Amatching%20or%20surpassing%20state-of-the-art%20RGB-D%20methods%20while%20operating%20at%2022%20FPS%0Aon%20a%20single%20GPU.%20These%20results%20suggest%20that%20modern%20pretrained%20vision%20models%20can%0Areplace%20active%20depth%20sensors%20as%20reliable%2C%20real-time%20sources%20of%20metric%20scale%2C%0Amarking%20a%20step%20toward%20simpler%20and%20more%20cost-effective%20SLAM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06216v1&entry.124074799=Read"},
{"title": "Geometry-Guided Adversarial Prompt Detection via Curvature and Local\n  Intrinsic Dimension", "author": "Canaan Yung and Hanxun Huang and Christopher Leckie and Sarah Erfani", "abstract": "  Adversarial prompts are capable of jailbreaking frontier large language\nmodels (LLMs) and inducing undesirable behaviours, posing a significant\nobstacle to their safe deployment. Current mitigation strategies primarily rely\non activating built-in defence mechanisms or fine-tuning LLMs, both of which\nare computationally expensive and can sacrifice model utility. In contrast,\ndetection-based approaches are more efficient and practical for deployment in\nreal-world applications. However, the fundamental distinctions between\nadversarial and benign prompts remain poorly understood. In this work, we\nintroduce CurvaLID, a novel defence framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. To further enhance our solution, we leverage Local Intrinsic\nDimensionality (LID) to capture complementary geometric features of text\nprompts within adversarial subspaces. Our findings show that adversarial\nprompts exhibit distinct geometric signatures from benign prompts, enabling\nCurvaLID to achieve near-perfect classification and outperform state-of-the-art\ndetectors in adversarial prompt detection. CurvaLID provides a reliable and\nefficient safeguard against malicious queries as a model-agnostic method that\ngeneralises across multiple LLMs and attack families.\n", "link": "http://arxiv.org/abs/2503.03502v2", "date": "2025-10-07", "relevancy": 2.4528, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4854}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Guided%20Adversarial%20Prompt%20Detection%20via%20Curvature%20and%20Local%0A%20%20Intrinsic%20Dimension&body=Title%3A%20Geometry-Guided%20Adversarial%20Prompt%20Detection%20via%20Curvature%20and%20Local%0A%20%20Intrinsic%20Dimension%0AAuthor%3A%20Canaan%20Yung%20and%20Hanxun%20Huang%20and%20Christopher%20Leckie%20and%20Sarah%20Erfani%0AAbstract%3A%20%20%20Adversarial%20prompts%20are%20capable%20of%20jailbreaking%20frontier%20large%20language%0Amodels%20%28LLMs%29%20and%20inducing%20undesirable%20behaviours%2C%20posing%20a%20significant%0Aobstacle%20to%20their%20safe%20deployment.%20Current%20mitigation%20strategies%20primarily%20rely%0Aon%20activating%20built-in%20defence%20mechanisms%20or%20fine-tuning%20LLMs%2C%20both%20of%20which%0Aare%20computationally%20expensive%20and%20can%20sacrifice%20model%20utility.%20In%20contrast%2C%0Adetection-based%20approaches%20are%20more%20efficient%20and%20practical%20for%20deployment%20in%0Areal-world%20applications.%20However%2C%20the%20fundamental%20distinctions%20between%0Aadversarial%20and%20benign%20prompts%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%0Aintroduce%20CurvaLID%2C%20a%20novel%20defence%20framework%20that%20efficiently%20detects%0Aadversarial%20prompts%20by%20leveraging%20their%20geometric%20properties.%20It%20is%20agnostic%20to%0Athe%20type%20of%20LLM%2C%20offering%20a%20unified%20detection%20framework%20across%20diverse%0Aadversarial%20prompts%20and%20LLM%20architectures.%20CurvaLID%20builds%20on%20the%20geometric%0Aanalysis%20of%20text%20prompts%20to%20uncover%20their%20underlying%20differences.%20We%0Atheoretically%20extend%20the%20concept%20of%20curvature%20via%20the%20Whewell%20equation%20into%20an%0A%24n%24-dimensional%20word%20embedding%20space%2C%20enabling%20us%20to%20quantify%20local%20geometric%0Aproperties%2C%20including%20semantic%20shifts%20and%20curvature%20in%20the%20underlying%0Amanifolds.%20To%20further%20enhance%20our%20solution%2C%20we%20leverage%20Local%20Intrinsic%0ADimensionality%20%28LID%29%20to%20capture%20complementary%20geometric%20features%20of%20text%0Aprompts%20within%20adversarial%20subspaces.%20Our%20findings%20show%20that%20adversarial%0Aprompts%20exhibit%20distinct%20geometric%20signatures%20from%20benign%20prompts%2C%20enabling%0ACurvaLID%20to%20achieve%20near-perfect%20classification%20and%20outperform%20state-of-the-art%0Adetectors%20in%20adversarial%20prompt%20detection.%20CurvaLID%20provides%20a%20reliable%20and%0Aefficient%20safeguard%20against%20malicious%20queries%20as%20a%20model-agnostic%20method%20that%0Ageneralises%20across%20multiple%20LLMs%20and%20attack%20families.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Guided%2520Adversarial%2520Prompt%2520Detection%2520via%2520Curvature%2520and%2520Local%250A%2520%2520Intrinsic%2520Dimension%26entry.906535625%3DCanaan%2520Yung%2520and%2520Hanxun%2520Huang%2520and%2520Christopher%2520Leckie%2520and%2520Sarah%2520Erfani%26entry.1292438233%3D%2520%2520Adversarial%2520prompts%2520are%2520capable%2520of%2520jailbreaking%2520frontier%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520and%2520inducing%2520undesirable%2520behaviours%252C%2520posing%2520a%2520significant%250Aobstacle%2520to%2520their%2520safe%2520deployment.%2520Current%2520mitigation%2520strategies%2520primarily%2520rely%250Aon%2520activating%2520built-in%2520defence%2520mechanisms%2520or%2520fine-tuning%2520LLMs%252C%2520both%2520of%2520which%250Aare%2520computationally%2520expensive%2520and%2520can%2520sacrifice%2520model%2520utility.%2520In%2520contrast%252C%250Adetection-based%2520approaches%2520are%2520more%2520efficient%2520and%2520practical%2520for%2520deployment%2520in%250Areal-world%2520applications.%2520However%252C%2520the%2520fundamental%2520distinctions%2520between%250Aadversarial%2520and%2520benign%2520prompts%2520remain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520CurvaLID%252C%2520a%2520novel%2520defence%2520framework%2520that%2520efficiently%2520detects%250Aadversarial%2520prompts%2520by%2520leveraging%2520their%2520geometric%2520properties.%2520It%2520is%2520agnostic%2520to%250Athe%2520type%2520of%2520LLM%252C%2520offering%2520a%2520unified%2520detection%2520framework%2520across%2520diverse%250Aadversarial%2520prompts%2520and%2520LLM%2520architectures.%2520CurvaLID%2520builds%2520on%2520the%2520geometric%250Aanalysis%2520of%2520text%2520prompts%2520to%2520uncover%2520their%2520underlying%2520differences.%2520We%250Atheoretically%2520extend%2520the%2520concept%2520of%2520curvature%2520via%2520the%2520Whewell%2520equation%2520into%2520an%250A%2524n%2524-dimensional%2520word%2520embedding%2520space%252C%2520enabling%2520us%2520to%2520quantify%2520local%2520geometric%250Aproperties%252C%2520including%2520semantic%2520shifts%2520and%2520curvature%2520in%2520the%2520underlying%250Amanifolds.%2520To%2520further%2520enhance%2520our%2520solution%252C%2520we%2520leverage%2520Local%2520Intrinsic%250ADimensionality%2520%2528LID%2529%2520to%2520capture%2520complementary%2520geometric%2520features%2520of%2520text%250Aprompts%2520within%2520adversarial%2520subspaces.%2520Our%2520findings%2520show%2520that%2520adversarial%250Aprompts%2520exhibit%2520distinct%2520geometric%2520signatures%2520from%2520benign%2520prompts%252C%2520enabling%250ACurvaLID%2520to%2520achieve%2520near-perfect%2520classification%2520and%2520outperform%2520state-of-the-art%250Adetectors%2520in%2520adversarial%2520prompt%2520detection.%2520CurvaLID%2520provides%2520a%2520reliable%2520and%250Aefficient%2520safeguard%2520against%2520malicious%2520queries%2520as%2520a%2520model-agnostic%2520method%2520that%250Ageneralises%2520across%2520multiple%2520LLMs%2520and%2520attack%2520families.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Guided%20Adversarial%20Prompt%20Detection%20via%20Curvature%20and%20Local%0A%20%20Intrinsic%20Dimension&entry.906535625=Canaan%20Yung%20and%20Hanxun%20Huang%20and%20Christopher%20Leckie%20and%20Sarah%20Erfani&entry.1292438233=%20%20Adversarial%20prompts%20are%20capable%20of%20jailbreaking%20frontier%20large%20language%0Amodels%20%28LLMs%29%20and%20inducing%20undesirable%20behaviours%2C%20posing%20a%20significant%0Aobstacle%20to%20their%20safe%20deployment.%20Current%20mitigation%20strategies%20primarily%20rely%0Aon%20activating%20built-in%20defence%20mechanisms%20or%20fine-tuning%20LLMs%2C%20both%20of%20which%0Aare%20computationally%20expensive%20and%20can%20sacrifice%20model%20utility.%20In%20contrast%2C%0Adetection-based%20approaches%20are%20more%20efficient%20and%20practical%20for%20deployment%20in%0Areal-world%20applications.%20However%2C%20the%20fundamental%20distinctions%20between%0Aadversarial%20and%20benign%20prompts%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%0Aintroduce%20CurvaLID%2C%20a%20novel%20defence%20framework%20that%20efficiently%20detects%0Aadversarial%20prompts%20by%20leveraging%20their%20geometric%20properties.%20It%20is%20agnostic%20to%0Athe%20type%20of%20LLM%2C%20offering%20a%20unified%20detection%20framework%20across%20diverse%0Aadversarial%20prompts%20and%20LLM%20architectures.%20CurvaLID%20builds%20on%20the%20geometric%0Aanalysis%20of%20text%20prompts%20to%20uncover%20their%20underlying%20differences.%20We%0Atheoretically%20extend%20the%20concept%20of%20curvature%20via%20the%20Whewell%20equation%20into%20an%0A%24n%24-dimensional%20word%20embedding%20space%2C%20enabling%20us%20to%20quantify%20local%20geometric%0Aproperties%2C%20including%20semantic%20shifts%20and%20curvature%20in%20the%20underlying%0Amanifolds.%20To%20further%20enhance%20our%20solution%2C%20we%20leverage%20Local%20Intrinsic%0ADimensionality%20%28LID%29%20to%20capture%20complementary%20geometric%20features%20of%20text%0Aprompts%20within%20adversarial%20subspaces.%20Our%20findings%20show%20that%20adversarial%0Aprompts%20exhibit%20distinct%20geometric%20signatures%20from%20benign%20prompts%2C%20enabling%0ACurvaLID%20to%20achieve%20near-perfect%20classification%20and%20outperform%20state-of-the-art%0Adetectors%20in%20adversarial%20prompt%20detection.%20CurvaLID%20provides%20a%20reliable%20and%0Aefficient%20safeguard%20against%20malicious%20queries%20as%20a%20model-agnostic%20method%20that%0Ageneralises%20across%20multiple%20LLMs%20and%20attack%20families.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03502v2&entry.124074799=Read"},
{"title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models", "author": "Haoran Ye and Tianze Zhang and Yuhang Xie and Liyuan Zhang and Yuanyi Ren and Xin Zhang and Guojie Song", "abstract": "  Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.\n", "link": "http://arxiv.org/abs/2502.02444v6", "date": "2025-10-07", "relevancy": 2.4456, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Haoran%20Ye%20and%20Tianze%20Zhang%20and%20Yuhang%20Xie%20and%20Liyuan%20Zhang%20and%20Yuanyi%20Ren%20and%20Xin%20Zhang%20and%20Guojie%20Song%0AAbstract%3A%20%20%20Values%20are%20core%20drivers%20of%20individual%20and%20collective%20perception%2C%20cognition%2C%0Aand%20behavior.%20Value%20systems%2C%20such%20as%20Schwartz%27s%20Theory%20of%20Basic%20Human%20Values%2C%0Adelineate%20the%20hierarchy%20and%20interplay%20among%20these%20values%2C%20enabling%0Across-disciplinary%20investigations%20into%20decision-making%20and%20societal%20dynamics.%0ARecently%2C%20the%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20raised%20concerns%0Aregarding%20their%20elusive%20intrinsic%20values.%20Despite%20growing%20efforts%20in%0Aevaluating%2C%20understanding%2C%20and%20aligning%20LLM%20values%2C%20a%20psychologically%20grounded%0ALLM%20value%20system%20remains%20underexplored.%20This%20study%20addresses%20the%20gap%20by%0Aintroducing%20the%20Generative%20Psycho-Lexical%20Approach%20%28GPLA%29%2C%20a%20scalable%2C%0Aadaptable%2C%20and%20theoretically%20informed%20method%20for%20constructing%20value%20systems.%0ALeveraging%20GPLA%2C%20we%20propose%20a%20psychologically%20grounded%20five-factor%20value%20system%0Atailored%20for%20LLMs.%20For%20systematic%20validation%2C%20we%20present%20three%20benchmarking%0Atasks%20that%20integrate%20psychological%20principles%20with%20cutting-edge%20AI%20priorities.%0AOur%20results%20reveal%20that%20the%20proposed%20value%20system%20meets%20standard%20psychological%0Acriteria%2C%20better%20captures%20LLM%20values%2C%20improves%20LLM%20safety%20prediction%2C%20and%0Aenhances%20LLM%20alignment%2C%20when%20compared%20to%20the%20canonical%20Schwartz%27s%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02444v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Psycho-Lexical%2520Approach%2520for%2520Constructing%2520Value%2520Systems%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DHaoran%2520Ye%2520and%2520Tianze%2520Zhang%2520and%2520Yuhang%2520Xie%2520and%2520Liyuan%2520Zhang%2520and%2520Yuanyi%2520Ren%2520and%2520Xin%2520Zhang%2520and%2520Guojie%2520Song%26entry.1292438233%3D%2520%2520Values%2520are%2520core%2520drivers%2520of%2520individual%2520and%2520collective%2520perception%252C%2520cognition%252C%250Aand%2520behavior.%2520Value%2520systems%252C%2520such%2520as%2520Schwartz%2527s%2520Theory%2520of%2520Basic%2520Human%2520Values%252C%250Adelineate%2520the%2520hierarchy%2520and%2520interplay%2520among%2520these%2520values%252C%2520enabling%250Across-disciplinary%2520investigations%2520into%2520decision-making%2520and%2520societal%2520dynamics.%250ARecently%252C%2520the%2520rise%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520raised%2520concerns%250Aregarding%2520their%2520elusive%2520intrinsic%2520values.%2520Despite%2520growing%2520efforts%2520in%250Aevaluating%252C%2520understanding%252C%2520and%2520aligning%2520LLM%2520values%252C%2520a%2520psychologically%2520grounded%250ALLM%2520value%2520system%2520remains%2520underexplored.%2520This%2520study%2520addresses%2520the%2520gap%2520by%250Aintroducing%2520the%2520Generative%2520Psycho-Lexical%2520Approach%2520%2528GPLA%2529%252C%2520a%2520scalable%252C%250Aadaptable%252C%2520and%2520theoretically%2520informed%2520method%2520for%2520constructing%2520value%2520systems.%250ALeveraging%2520GPLA%252C%2520we%2520propose%2520a%2520psychologically%2520grounded%2520five-factor%2520value%2520system%250Atailored%2520for%2520LLMs.%2520For%2520systematic%2520validation%252C%2520we%2520present%2520three%2520benchmarking%250Atasks%2520that%2520integrate%2520psychological%2520principles%2520with%2520cutting-edge%2520AI%2520priorities.%250AOur%2520results%2520reveal%2520that%2520the%2520proposed%2520value%2520system%2520meets%2520standard%2520psychological%250Acriteria%252C%2520better%2520captures%2520LLM%2520values%252C%2520improves%2520LLM%2520safety%2520prediction%252C%2520and%250Aenhances%2520LLM%2520alignment%252C%2520when%2520compared%2520to%2520the%2520canonical%2520Schwartz%2527s%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02444v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models&entry.906535625=Haoran%20Ye%20and%20Tianze%20Zhang%20and%20Yuhang%20Xie%20and%20Liyuan%20Zhang%20and%20Yuanyi%20Ren%20and%20Xin%20Zhang%20and%20Guojie%20Song&entry.1292438233=%20%20Values%20are%20core%20drivers%20of%20individual%20and%20collective%20perception%2C%20cognition%2C%0Aand%20behavior.%20Value%20systems%2C%20such%20as%20Schwartz%27s%20Theory%20of%20Basic%20Human%20Values%2C%0Adelineate%20the%20hierarchy%20and%20interplay%20among%20these%20values%2C%20enabling%0Across-disciplinary%20investigations%20into%20decision-making%20and%20societal%20dynamics.%0ARecently%2C%20the%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20raised%20concerns%0Aregarding%20their%20elusive%20intrinsic%20values.%20Despite%20growing%20efforts%20in%0Aevaluating%2C%20understanding%2C%20and%20aligning%20LLM%20values%2C%20a%20psychologically%20grounded%0ALLM%20value%20system%20remains%20underexplored.%20This%20study%20addresses%20the%20gap%20by%0Aintroducing%20the%20Generative%20Psycho-Lexical%20Approach%20%28GPLA%29%2C%20a%20scalable%2C%0Aadaptable%2C%20and%20theoretically%20informed%20method%20for%20constructing%20value%20systems.%0ALeveraging%20GPLA%2C%20we%20propose%20a%20psychologically%20grounded%20five-factor%20value%20system%0Atailored%20for%20LLMs.%20For%20systematic%20validation%2C%20we%20present%20three%20benchmarking%0Atasks%20that%20integrate%20psychological%20principles%20with%20cutting-edge%20AI%20priorities.%0AOur%20results%20reveal%20that%20the%20proposed%20value%20system%20meets%20standard%20psychological%0Acriteria%2C%20better%20captures%20LLM%20values%2C%20improves%20LLM%20safety%20prediction%2C%20and%0Aenhances%20LLM%20alignment%2C%20when%20compared%20to%20the%20canonical%20Schwartz%27s%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02444v6&entry.124074799=Read"},
{"title": "VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought\n  Reasoning for Language-driven Grasp Generation", "author": "Haoran Zhang and Shuanghao Bai and Wanqi Zhou and Yuedi Zhang and Qi Zhang and Pengxiang Ding and Cheng Chi and Donglin Wang and Badong Chen", "abstract": "  Robotic grasping is one of the most fundamental tasks in robotic\nmanipulation, and grasp detection/generation has long been the subject of\nextensive research. Recently, language-driven grasp generation has emerged as a\npromising direction due to its practical interaction capabilities. However,\nmost existing approaches either lack sufficient reasoning and generalization\ncapabilities or depend on complex modular pipelines. Moreover, current grasp\nfoundation models tend to overemphasize dialog and object semantics, resulting\nin inferior performance and restriction to single-object grasping. To maintain\nstrong reasoning ability and generalization in cluttered environments, we\npropose VCoT-Grasp, an end-to-end grasp foundation model that incorporates\nvisual chain-of-thought reasoning to enhance visual understanding for grasp\ngeneration. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically\nfocuses on visual inputs while providing interpretable reasoning traces. For\ntraining, we refine and introduce a large-scale dataset, VCoT-GraspSet,\ncomprising 167K synthetic images with over 1.36M grasps, as well as 400+\nreal-world images with more than 1.2K grasps, annotated with intermediate\nbounding boxes. Extensive experiments on both VCoT-GraspSet and real robot\ndemonstrate that our method significantly improves grasp success rates and\ngeneralizes effectively to unseen objects, backgrounds, and distractors. More\ndetails can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.\n", "link": "http://arxiv.org/abs/2510.05827v1", "date": "2025-10-07", "relevancy": 2.4399, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6638}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCoT-Grasp%3A%20Grasp%20Foundation%20Models%20with%20Visual%20Chain-of-Thought%0A%20%20Reasoning%20for%20Language-driven%20Grasp%20Generation&body=Title%3A%20VCoT-Grasp%3A%20Grasp%20Foundation%20Models%20with%20Visual%20Chain-of-Thought%0A%20%20Reasoning%20for%20Language-driven%20Grasp%20Generation%0AAuthor%3A%20Haoran%20Zhang%20and%20Shuanghao%20Bai%20and%20Wanqi%20Zhou%20and%20Yuedi%20Zhang%20and%20Qi%20Zhang%20and%20Pengxiang%20Ding%20and%20Cheng%20Chi%20and%20Donglin%20Wang%20and%20Badong%20Chen%0AAbstract%3A%20%20%20Robotic%20grasping%20is%20one%20of%20the%20most%20fundamental%20tasks%20in%20robotic%0Amanipulation%2C%20and%20grasp%20detection/generation%20has%20long%20been%20the%20subject%20of%0Aextensive%20research.%20Recently%2C%20language-driven%20grasp%20generation%20has%20emerged%20as%20a%0Apromising%20direction%20due%20to%20its%20practical%20interaction%20capabilities.%20However%2C%0Amost%20existing%20approaches%20either%20lack%20sufficient%20reasoning%20and%20generalization%0Acapabilities%20or%20depend%20on%20complex%20modular%20pipelines.%20Moreover%2C%20current%20grasp%0Afoundation%20models%20tend%20to%20overemphasize%20dialog%20and%20object%20semantics%2C%20resulting%0Ain%20inferior%20performance%20and%20restriction%20to%20single-object%20grasping.%20To%20maintain%0Astrong%20reasoning%20ability%20and%20generalization%20in%20cluttered%20environments%2C%20we%0Apropose%20VCoT-Grasp%2C%20an%20end-to-end%20grasp%20foundation%20model%20that%20incorporates%0Avisual%20chain-of-thought%20reasoning%20to%20enhance%20visual%20understanding%20for%20grasp%0Ageneration.%20VCoT-Grasp%20adopts%20a%20multi-turn%20processing%20paradigm%20that%20dynamically%0Afocuses%20on%20visual%20inputs%20while%20providing%20interpretable%20reasoning%20traces.%20For%0Atraining%2C%20we%20refine%20and%20introduce%20a%20large-scale%20dataset%2C%20VCoT-GraspSet%2C%0Acomprising%20167K%20synthetic%20images%20with%20over%201.36M%20grasps%2C%20as%20well%20as%20400%2B%0Areal-world%20images%20with%20more%20than%201.2K%20grasps%2C%20annotated%20with%20intermediate%0Abounding%20boxes.%20Extensive%20experiments%20on%20both%20VCoT-GraspSet%20and%20real%20robot%0Ademonstrate%20that%20our%20method%20significantly%20improves%20grasp%20success%20rates%20and%0Ageneralizes%20effectively%20to%20unseen%20objects%2C%20backgrounds%2C%20and%20distractors.%20More%0Adetails%20can%20be%20found%20at%20https%3A//zhanghr2001.github.io/VCoT-Grasp.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCoT-Grasp%253A%2520Grasp%2520Foundation%2520Models%2520with%2520Visual%2520Chain-of-Thought%250A%2520%2520Reasoning%2520for%2520Language-driven%2520Grasp%2520Generation%26entry.906535625%3DHaoran%2520Zhang%2520and%2520Shuanghao%2520Bai%2520and%2520Wanqi%2520Zhou%2520and%2520Yuedi%2520Zhang%2520and%2520Qi%2520Zhang%2520and%2520Pengxiang%2520Ding%2520and%2520Cheng%2520Chi%2520and%2520Donglin%2520Wang%2520and%2520Badong%2520Chen%26entry.1292438233%3D%2520%2520Robotic%2520grasping%2520is%2520one%2520of%2520the%2520most%2520fundamental%2520tasks%2520in%2520robotic%250Amanipulation%252C%2520and%2520grasp%2520detection/generation%2520has%2520long%2520been%2520the%2520subject%2520of%250Aextensive%2520research.%2520Recently%252C%2520language-driven%2520grasp%2520generation%2520has%2520emerged%2520as%2520a%250Apromising%2520direction%2520due%2520to%2520its%2520practical%2520interaction%2520capabilities.%2520However%252C%250Amost%2520existing%2520approaches%2520either%2520lack%2520sufficient%2520reasoning%2520and%2520generalization%250Acapabilities%2520or%2520depend%2520on%2520complex%2520modular%2520pipelines.%2520Moreover%252C%2520current%2520grasp%250Afoundation%2520models%2520tend%2520to%2520overemphasize%2520dialog%2520and%2520object%2520semantics%252C%2520resulting%250Ain%2520inferior%2520performance%2520and%2520restriction%2520to%2520single-object%2520grasping.%2520To%2520maintain%250Astrong%2520reasoning%2520ability%2520and%2520generalization%2520in%2520cluttered%2520environments%252C%2520we%250Apropose%2520VCoT-Grasp%252C%2520an%2520end-to-end%2520grasp%2520foundation%2520model%2520that%2520incorporates%250Avisual%2520chain-of-thought%2520reasoning%2520to%2520enhance%2520visual%2520understanding%2520for%2520grasp%250Ageneration.%2520VCoT-Grasp%2520adopts%2520a%2520multi-turn%2520processing%2520paradigm%2520that%2520dynamically%250Afocuses%2520on%2520visual%2520inputs%2520while%2520providing%2520interpretable%2520reasoning%2520traces.%2520For%250Atraining%252C%2520we%2520refine%2520and%2520introduce%2520a%2520large-scale%2520dataset%252C%2520VCoT-GraspSet%252C%250Acomprising%2520167K%2520synthetic%2520images%2520with%2520over%25201.36M%2520grasps%252C%2520as%2520well%2520as%2520400%252B%250Areal-world%2520images%2520with%2520more%2520than%25201.2K%2520grasps%252C%2520annotated%2520with%2520intermediate%250Abounding%2520boxes.%2520Extensive%2520experiments%2520on%2520both%2520VCoT-GraspSet%2520and%2520real%2520robot%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520grasp%2520success%2520rates%2520and%250Ageneralizes%2520effectively%2520to%2520unseen%2520objects%252C%2520backgrounds%252C%2520and%2520distractors.%2520More%250Adetails%2520can%2520be%2520found%2520at%2520https%253A//zhanghr2001.github.io/VCoT-Grasp.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCoT-Grasp%3A%20Grasp%20Foundation%20Models%20with%20Visual%20Chain-of-Thought%0A%20%20Reasoning%20for%20Language-driven%20Grasp%20Generation&entry.906535625=Haoran%20Zhang%20and%20Shuanghao%20Bai%20and%20Wanqi%20Zhou%20and%20Yuedi%20Zhang%20and%20Qi%20Zhang%20and%20Pengxiang%20Ding%20and%20Cheng%20Chi%20and%20Donglin%20Wang%20and%20Badong%20Chen&entry.1292438233=%20%20Robotic%20grasping%20is%20one%20of%20the%20most%20fundamental%20tasks%20in%20robotic%0Amanipulation%2C%20and%20grasp%20detection/generation%20has%20long%20been%20the%20subject%20of%0Aextensive%20research.%20Recently%2C%20language-driven%20grasp%20generation%20has%20emerged%20as%20a%0Apromising%20direction%20due%20to%20its%20practical%20interaction%20capabilities.%20However%2C%0Amost%20existing%20approaches%20either%20lack%20sufficient%20reasoning%20and%20generalization%0Acapabilities%20or%20depend%20on%20complex%20modular%20pipelines.%20Moreover%2C%20current%20grasp%0Afoundation%20models%20tend%20to%20overemphasize%20dialog%20and%20object%20semantics%2C%20resulting%0Ain%20inferior%20performance%20and%20restriction%20to%20single-object%20grasping.%20To%20maintain%0Astrong%20reasoning%20ability%20and%20generalization%20in%20cluttered%20environments%2C%20we%0Apropose%20VCoT-Grasp%2C%20an%20end-to-end%20grasp%20foundation%20model%20that%20incorporates%0Avisual%20chain-of-thought%20reasoning%20to%20enhance%20visual%20understanding%20for%20grasp%0Ageneration.%20VCoT-Grasp%20adopts%20a%20multi-turn%20processing%20paradigm%20that%20dynamically%0Afocuses%20on%20visual%20inputs%20while%20providing%20interpretable%20reasoning%20traces.%20For%0Atraining%2C%20we%20refine%20and%20introduce%20a%20large-scale%20dataset%2C%20VCoT-GraspSet%2C%0Acomprising%20167K%20synthetic%20images%20with%20over%201.36M%20grasps%2C%20as%20well%20as%20400%2B%0Areal-world%20images%20with%20more%20than%201.2K%20grasps%2C%20annotated%20with%20intermediate%0Abounding%20boxes.%20Extensive%20experiments%20on%20both%20VCoT-GraspSet%20and%20real%20robot%0Ademonstrate%20that%20our%20method%20significantly%20improves%20grasp%20success%20rates%20and%0Ageneralizes%20effectively%20to%20unseen%20objects%2C%20backgrounds%2C%20and%20distractors.%20More%0Adetails%20can%20be%20found%20at%20https%3A//zhanghr2001.github.io/VCoT-Grasp.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05827v1&entry.124074799=Read"},
{"title": "Evaluation of Deformable Image Registration under Alignment-Regularity\n  Trade-of", "author": "Vasiliki Sideri-Lampretsa and Daniel Rueckert and Huaqi Qiu", "abstract": "  Evaluating deformable image registration (DIR) is challenging due to the\ninherent trade-off between achieving high alignment accuracy and maintaining\ndeformation regularity. However, most existing DIR works either address this\ntrade-off inadequately or overlook it altogether. In this paper, we highlight\nthe issues with existing practices and propose an evaluation scheme that\ncaptures the trade-off continuously to holistically evaluate DIR methods. We\nfirst introduce the alignment regularity characteristic (ARC) curves, which\ndescribe the performance of a given registration method as a spectrum under\nvarious degrees of regularity. We demonstrate that the ARC curves reveal unique\ninsights that are not evident from existing evaluation practices, using\nexperiments on representative deep learning DIR methods with various network\narchitectures and transformation models. We further adopt a HyperNetwork based\napproach that learns to continuously interpolate across the full regularization\nrange, accelerating the construction and improving the sample density of ARC\ncurves. Finally, we provide general guidelines for a nuanced model evaluation\nand selection using our evaluation scheme for both practitioners and\nregistration researchers.\n", "link": "http://arxiv.org/abs/2503.07185v3", "date": "2025-10-07", "relevancy": 2.4385, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4789}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Deformable%20Image%20Registration%20under%20Alignment-Regularity%0A%20%20Trade-of&body=Title%3A%20Evaluation%20of%20Deformable%20Image%20Registration%20under%20Alignment-Regularity%0A%20%20Trade-of%0AAuthor%3A%20Vasiliki%20Sideri-Lampretsa%20and%20Daniel%20Rueckert%20and%20Huaqi%20Qiu%0AAbstract%3A%20%20%20Evaluating%20deformable%20image%20registration%20%28DIR%29%20is%20challenging%20due%20to%20the%0Ainherent%20trade-off%20between%20achieving%20high%20alignment%20accuracy%20and%20maintaining%0Adeformation%20regularity.%20However%2C%20most%20existing%20DIR%20works%20either%20address%20this%0Atrade-off%20inadequately%20or%20overlook%20it%20altogether.%20In%20this%20paper%2C%20we%20highlight%0Athe%20issues%20with%20existing%20practices%20and%20propose%20an%20evaluation%20scheme%20that%0Acaptures%20the%20trade-off%20continuously%20to%20holistically%20evaluate%20DIR%20methods.%20We%0Afirst%20introduce%20the%20alignment%20regularity%20characteristic%20%28ARC%29%20curves%2C%20which%0Adescribe%20the%20performance%20of%20a%20given%20registration%20method%20as%20a%20spectrum%20under%0Avarious%20degrees%20of%20regularity.%20We%20demonstrate%20that%20the%20ARC%20curves%20reveal%20unique%0Ainsights%20that%20are%20not%20evident%20from%20existing%20evaluation%20practices%2C%20using%0Aexperiments%20on%20representative%20deep%20learning%20DIR%20methods%20with%20various%20network%0Aarchitectures%20and%20transformation%20models.%20We%20further%20adopt%20a%20HyperNetwork%20based%0Aapproach%20that%20learns%20to%20continuously%20interpolate%20across%20the%20full%20regularization%0Arange%2C%20accelerating%20the%20construction%20and%20improving%20the%20sample%20density%20of%20ARC%0Acurves.%20Finally%2C%20we%20provide%20general%20guidelines%20for%20a%20nuanced%20model%20evaluation%0Aand%20selection%20using%20our%20evaluation%20scheme%20for%20both%20practitioners%20and%0Aregistration%20researchers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07185v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Deformable%2520Image%2520Registration%2520under%2520Alignment-Regularity%250A%2520%2520Trade-of%26entry.906535625%3DVasiliki%2520Sideri-Lampretsa%2520and%2520Daniel%2520Rueckert%2520and%2520Huaqi%2520Qiu%26entry.1292438233%3D%2520%2520Evaluating%2520deformable%2520image%2520registration%2520%2528DIR%2529%2520is%2520challenging%2520due%2520to%2520the%250Ainherent%2520trade-off%2520between%2520achieving%2520high%2520alignment%2520accuracy%2520and%2520maintaining%250Adeformation%2520regularity.%2520However%252C%2520most%2520existing%2520DIR%2520works%2520either%2520address%2520this%250Atrade-off%2520inadequately%2520or%2520overlook%2520it%2520altogether.%2520In%2520this%2520paper%252C%2520we%2520highlight%250Athe%2520issues%2520with%2520existing%2520practices%2520and%2520propose%2520an%2520evaluation%2520scheme%2520that%250Acaptures%2520the%2520trade-off%2520continuously%2520to%2520holistically%2520evaluate%2520DIR%2520methods.%2520We%250Afirst%2520introduce%2520the%2520alignment%2520regularity%2520characteristic%2520%2528ARC%2529%2520curves%252C%2520which%250Adescribe%2520the%2520performance%2520of%2520a%2520given%2520registration%2520method%2520as%2520a%2520spectrum%2520under%250Avarious%2520degrees%2520of%2520regularity.%2520We%2520demonstrate%2520that%2520the%2520ARC%2520curves%2520reveal%2520unique%250Ainsights%2520that%2520are%2520not%2520evident%2520from%2520existing%2520evaluation%2520practices%252C%2520using%250Aexperiments%2520on%2520representative%2520deep%2520learning%2520DIR%2520methods%2520with%2520various%2520network%250Aarchitectures%2520and%2520transformation%2520models.%2520We%2520further%2520adopt%2520a%2520HyperNetwork%2520based%250Aapproach%2520that%2520learns%2520to%2520continuously%2520interpolate%2520across%2520the%2520full%2520regularization%250Arange%252C%2520accelerating%2520the%2520construction%2520and%2520improving%2520the%2520sample%2520density%2520of%2520ARC%250Acurves.%2520Finally%252C%2520we%2520provide%2520general%2520guidelines%2520for%2520a%2520nuanced%2520model%2520evaluation%250Aand%2520selection%2520using%2520our%2520evaluation%2520scheme%2520for%2520both%2520practitioners%2520and%250Aregistration%2520researchers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07185v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Deformable%20Image%20Registration%20under%20Alignment-Regularity%0A%20%20Trade-of&entry.906535625=Vasiliki%20Sideri-Lampretsa%20and%20Daniel%20Rueckert%20and%20Huaqi%20Qiu&entry.1292438233=%20%20Evaluating%20deformable%20image%20registration%20%28DIR%29%20is%20challenging%20due%20to%20the%0Ainherent%20trade-off%20between%20achieving%20high%20alignment%20accuracy%20and%20maintaining%0Adeformation%20regularity.%20However%2C%20most%20existing%20DIR%20works%20either%20address%20this%0Atrade-off%20inadequately%20or%20overlook%20it%20altogether.%20In%20this%20paper%2C%20we%20highlight%0Athe%20issues%20with%20existing%20practices%20and%20propose%20an%20evaluation%20scheme%20that%0Acaptures%20the%20trade-off%20continuously%20to%20holistically%20evaluate%20DIR%20methods.%20We%0Afirst%20introduce%20the%20alignment%20regularity%20characteristic%20%28ARC%29%20curves%2C%20which%0Adescribe%20the%20performance%20of%20a%20given%20registration%20method%20as%20a%20spectrum%20under%0Avarious%20degrees%20of%20regularity.%20We%20demonstrate%20that%20the%20ARC%20curves%20reveal%20unique%0Ainsights%20that%20are%20not%20evident%20from%20existing%20evaluation%20practices%2C%20using%0Aexperiments%20on%20representative%20deep%20learning%20DIR%20methods%20with%20various%20network%0Aarchitectures%20and%20transformation%20models.%20We%20further%20adopt%20a%20HyperNetwork%20based%0Aapproach%20that%20learns%20to%20continuously%20interpolate%20across%20the%20full%20regularization%0Arange%2C%20accelerating%20the%20construction%20and%20improving%20the%20sample%20density%20of%20ARC%0Acurves.%20Finally%2C%20we%20provide%20general%20guidelines%20for%20a%20nuanced%20model%20evaluation%0Aand%20selection%20using%20our%20evaluation%20scheme%20for%20both%20practitioners%20and%0Aregistration%20researchers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07185v3&entry.124074799=Read"},
{"title": "Epistemic Diversity and Knowledge Collapse in Large Language Models", "author": "Dustin Wright and Sarah Masud and Jared Moore and Srishti Yadav and Maria Antoniak and Chan Young Park and Isabelle Augenstein", "abstract": "  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n", "link": "http://arxiv.org/abs/2510.04226v2", "date": "2025-10-07", "relevancy": 2.4261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Epistemic%20Diversity%20and%20Knowledge%20Collapse%20in%20Large%20Language%20Models&body=Title%3A%20Epistemic%20Diversity%20and%20Knowledge%20Collapse%20in%20Large%20Language%20Models%0AAuthor%3A%20Dustin%20Wright%20and%20Sarah%20Masud%20and%20Jared%20Moore%20and%20Srishti%20Yadav%20and%20Maria%20Antoniak%20and%20Chan%20Young%20Park%20and%20Isabelle%20Augenstein%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20tend%20to%20generate%20lexically%2C%20semantically%2C%20and%0Astylistically%20homogenous%20texts.%20This%20poses%20a%20risk%20of%20knowledge%20collapse%2C%20where%0Ahomogenous%20LLMs%20mediate%20a%20shrinking%20in%20the%20range%20of%20accessible%20information%20over%0Atime.%20Existing%20works%20on%20homogenization%20are%20limited%20by%20a%20focus%20on%20closed-ended%0Amultiple-choice%20setups%20or%20fuzzy%20semantic%20features%2C%20and%20do%20not%20look%20at%20trends%0Aacross%20time%20and%20cultural%20contexts.%20To%20overcome%20this%2C%20we%20present%20a%20new%0Amethodology%20to%20measure%20epistemic%20diversity%2C%20i.e.%2C%20variation%20in%20real-world%0Aclaims%20in%20LLM%20outputs%2C%20which%20we%20use%20to%20perform%20a%20broad%20empirical%20study%20of%20LLM%0Aknowledge%20collapse.%20We%20test%2027%20LLMs%2C%20155%20topics%20covering%2012%20countries%2C%20and%20200%0Aprompt%20variations%20sourced%20from%20real%20user%20chats.%20For%20the%20topics%20in%20our%20study%2C%20we%0Ashow%20that%20while%20newer%20models%20tend%20to%20generate%20more%20diverse%20claims%2C%20nearly%20all%0Amodels%20are%20less%20epistemically%20diverse%20than%20a%20basic%20web%20search.%20We%20find%20that%0Amodel%20size%20has%20a%20negative%20impact%20on%20epistemic%20diversity%2C%20while%0Aretrieval-augmented%20generation%20%28RAG%29%20has%20a%20positive%20impact%2C%20though%20the%0Aimprovement%20from%20RAG%20varies%20by%20the%20cultural%20context.%20Finally%2C%20compared%20to%20a%0Atraditional%20knowledge%20source%20%28Wikipedia%29%2C%20we%20find%20that%20country-specific%20claims%0Areflect%20the%20English%20language%20more%20than%20the%20local%20one%2C%20highlighting%20a%20gap%20in%0Aepistemic%20representation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04226v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpistemic%2520Diversity%2520and%2520Knowledge%2520Collapse%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DDustin%2520Wright%2520and%2520Sarah%2520Masud%2520and%2520Jared%2520Moore%2520and%2520Srishti%2520Yadav%2520and%2520Maria%2520Antoniak%2520and%2520Chan%2520Young%2520Park%2520and%2520Isabelle%2520Augenstein%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520tend%2520to%2520generate%2520lexically%252C%2520semantically%252C%2520and%250Astylistically%2520homogenous%2520texts.%2520This%2520poses%2520a%2520risk%2520of%2520knowledge%2520collapse%252C%2520where%250Ahomogenous%2520LLMs%2520mediate%2520a%2520shrinking%2520in%2520the%2520range%2520of%2520accessible%2520information%2520over%250Atime.%2520Existing%2520works%2520on%2520homogenization%2520are%2520limited%2520by%2520a%2520focus%2520on%2520closed-ended%250Amultiple-choice%2520setups%2520or%2520fuzzy%2520semantic%2520features%252C%2520and%2520do%2520not%2520look%2520at%2520trends%250Aacross%2520time%2520and%2520cultural%2520contexts.%2520To%2520overcome%2520this%252C%2520we%2520present%2520a%2520new%250Amethodology%2520to%2520measure%2520epistemic%2520diversity%252C%2520i.e.%252C%2520variation%2520in%2520real-world%250Aclaims%2520in%2520LLM%2520outputs%252C%2520which%2520we%2520use%2520to%2520perform%2520a%2520broad%2520empirical%2520study%2520of%2520LLM%250Aknowledge%2520collapse.%2520We%2520test%252027%2520LLMs%252C%2520155%2520topics%2520covering%252012%2520countries%252C%2520and%2520200%250Aprompt%2520variations%2520sourced%2520from%2520real%2520user%2520chats.%2520For%2520the%2520topics%2520in%2520our%2520study%252C%2520we%250Ashow%2520that%2520while%2520newer%2520models%2520tend%2520to%2520generate%2520more%2520diverse%2520claims%252C%2520nearly%2520all%250Amodels%2520are%2520less%2520epistemically%2520diverse%2520than%2520a%2520basic%2520web%2520search.%2520We%2520find%2520that%250Amodel%2520size%2520has%2520a%2520negative%2520impact%2520on%2520epistemic%2520diversity%252C%2520while%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520a%2520positive%2520impact%252C%2520though%2520the%250Aimprovement%2520from%2520RAG%2520varies%2520by%2520the%2520cultural%2520context.%2520Finally%252C%2520compared%2520to%2520a%250Atraditional%2520knowledge%2520source%2520%2528Wikipedia%2529%252C%2520we%2520find%2520that%2520country-specific%2520claims%250Areflect%2520the%2520English%2520language%2520more%2520than%2520the%2520local%2520one%252C%2520highlighting%2520a%2520gap%2520in%250Aepistemic%2520representation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04226v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Epistemic%20Diversity%20and%20Knowledge%20Collapse%20in%20Large%20Language%20Models&entry.906535625=Dustin%20Wright%20and%20Sarah%20Masud%20and%20Jared%20Moore%20and%20Srishti%20Yadav%20and%20Maria%20Antoniak%20and%20Chan%20Young%20Park%20and%20Isabelle%20Augenstein&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20tend%20to%20generate%20lexically%2C%20semantically%2C%20and%0Astylistically%20homogenous%20texts.%20This%20poses%20a%20risk%20of%20knowledge%20collapse%2C%20where%0Ahomogenous%20LLMs%20mediate%20a%20shrinking%20in%20the%20range%20of%20accessible%20information%20over%0Atime.%20Existing%20works%20on%20homogenization%20are%20limited%20by%20a%20focus%20on%20closed-ended%0Amultiple-choice%20setups%20or%20fuzzy%20semantic%20features%2C%20and%20do%20not%20look%20at%20trends%0Aacross%20time%20and%20cultural%20contexts.%20To%20overcome%20this%2C%20we%20present%20a%20new%0Amethodology%20to%20measure%20epistemic%20diversity%2C%20i.e.%2C%20variation%20in%20real-world%0Aclaims%20in%20LLM%20outputs%2C%20which%20we%20use%20to%20perform%20a%20broad%20empirical%20study%20of%20LLM%0Aknowledge%20collapse.%20We%20test%2027%20LLMs%2C%20155%20topics%20covering%2012%20countries%2C%20and%20200%0Aprompt%20variations%20sourced%20from%20real%20user%20chats.%20For%20the%20topics%20in%20our%20study%2C%20we%0Ashow%20that%20while%20newer%20models%20tend%20to%20generate%20more%20diverse%20claims%2C%20nearly%20all%0Amodels%20are%20less%20epistemically%20diverse%20than%20a%20basic%20web%20search.%20We%20find%20that%0Amodel%20size%20has%20a%20negative%20impact%20on%20epistemic%20diversity%2C%20while%0Aretrieval-augmented%20generation%20%28RAG%29%20has%20a%20positive%20impact%2C%20though%20the%0Aimprovement%20from%20RAG%20varies%20by%20the%20cultural%20context.%20Finally%2C%20compared%20to%20a%0Atraditional%20knowledge%20source%20%28Wikipedia%29%2C%20we%20find%20that%20country-specific%20claims%0Areflect%20the%20English%20language%20more%20than%20the%20local%20one%2C%20highlighting%20a%20gap%20in%0Aepistemic%20representation%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04226v2&entry.124074799=Read"},
{"title": "A public cardiac CT dataset featuring the left atrial appendage", "author": "Bjoern Hansen and Jonas Pedersen and Klaus F. Kofoed and Oscar Camara and Rasmus R. Paulsen and Kristine Soerensen", "abstract": "  Despite the success of advanced segmentation frameworks such as\nTotalSegmentator (TS), accurate segmentations of the left atrial appendage\n(LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant\nchallenge in medical imaging. In this work, we present the first open-source,\nanatomically coherent dataset of curated, high-resolution segmentations for\nthese structures, supplemented with whole-heart labels produced by TS on the\npublicly available ImageCAS dataset consisting of 1000 cardiac computed\ntomography angiography (CCTA) scans. One purpose of the data set is to foster\nnovel approaches to the analysis of LAA morphology.\n  LAA segmentations on ImageCAS were generated using a state-of-the-art\nsegmentation framework developed specifically for high resolution LAA\nsegmentation. We trained the network on a large private dataset with manual\nannotations provided by medical readers guided by a trained cardiologist and\ntransferred the model to ImageCAS data. CA labels were improved from the\noriginal ImageCAS annotations, while PV segmentations were refined from TS\noutputs. In addition, we provide a list of scans from ImageCAS that contains\ncommon data flaws such as step artefacts, LAAs extending beyond the scanner's\nfield of view, and other types of data defects.\n", "link": "http://arxiv.org/abs/2510.06090v1", "date": "2025-10-07", "relevancy": 2.4036, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4793}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20public%20cardiac%20CT%20dataset%20featuring%20the%20left%20atrial%20appendage&body=Title%3A%20A%20public%20cardiac%20CT%20dataset%20featuring%20the%20left%20atrial%20appendage%0AAuthor%3A%20Bjoern%20Hansen%20and%20Jonas%20Pedersen%20and%20Klaus%20F.%20Kofoed%20and%20Oscar%20Camara%20and%20Rasmus%20R.%20Paulsen%20and%20Kristine%20Soerensen%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20advanced%20segmentation%20frameworks%20such%20as%0ATotalSegmentator%20%28TS%29%2C%20accurate%20segmentations%20of%20the%20left%20atrial%20appendage%0A%28LAA%29%2C%20coronary%20arteries%20%28CAs%29%2C%20and%20pulmonary%20veins%20%28PVs%29%20remain%20a%20significant%0Achallenge%20in%20medical%20imaging.%20In%20this%20work%2C%20we%20present%20the%20first%20open-source%2C%0Aanatomically%20coherent%20dataset%20of%20curated%2C%20high-resolution%20segmentations%20for%0Athese%20structures%2C%20supplemented%20with%20whole-heart%20labels%20produced%20by%20TS%20on%20the%0Apublicly%20available%20ImageCAS%20dataset%20consisting%20of%201000%20cardiac%20computed%0Atomography%20angiography%20%28CCTA%29%20scans.%20One%20purpose%20of%20the%20data%20set%20is%20to%20foster%0Anovel%20approaches%20to%20the%20analysis%20of%20LAA%20morphology.%0A%20%20LAA%20segmentations%20on%20ImageCAS%20were%20generated%20using%20a%20state-of-the-art%0Asegmentation%20framework%20developed%20specifically%20for%20high%20resolution%20LAA%0Asegmentation.%20We%20trained%20the%20network%20on%20a%20large%20private%20dataset%20with%20manual%0Aannotations%20provided%20by%20medical%20readers%20guided%20by%20a%20trained%20cardiologist%20and%0Atransferred%20the%20model%20to%20ImageCAS%20data.%20CA%20labels%20were%20improved%20from%20the%0Aoriginal%20ImageCAS%20annotations%2C%20while%20PV%20segmentations%20were%20refined%20from%20TS%0Aoutputs.%20In%20addition%2C%20we%20provide%20a%20list%20of%20scans%20from%20ImageCAS%20that%20contains%0Acommon%20data%20flaws%20such%20as%20step%20artefacts%2C%20LAAs%20extending%20beyond%20the%20scanner%27s%0Afield%20of%20view%2C%20and%20other%20types%20of%20data%20defects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520public%2520cardiac%2520CT%2520dataset%2520featuring%2520the%2520left%2520atrial%2520appendage%26entry.906535625%3DBjoern%2520Hansen%2520and%2520Jonas%2520Pedersen%2520and%2520Klaus%2520F.%2520Kofoed%2520and%2520Oscar%2520Camara%2520and%2520Rasmus%2520R.%2520Paulsen%2520and%2520Kristine%2520Soerensen%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520of%2520advanced%2520segmentation%2520frameworks%2520such%2520as%250ATotalSegmentator%2520%2528TS%2529%252C%2520accurate%2520segmentations%2520of%2520the%2520left%2520atrial%2520appendage%250A%2528LAA%2529%252C%2520coronary%2520arteries%2520%2528CAs%2529%252C%2520and%2520pulmonary%2520veins%2520%2528PVs%2529%2520remain%2520a%2520significant%250Achallenge%2520in%2520medical%2520imaging.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520open-source%252C%250Aanatomically%2520coherent%2520dataset%2520of%2520curated%252C%2520high-resolution%2520segmentations%2520for%250Athese%2520structures%252C%2520supplemented%2520with%2520whole-heart%2520labels%2520produced%2520by%2520TS%2520on%2520the%250Apublicly%2520available%2520ImageCAS%2520dataset%2520consisting%2520of%25201000%2520cardiac%2520computed%250Atomography%2520angiography%2520%2528CCTA%2529%2520scans.%2520One%2520purpose%2520of%2520the%2520data%2520set%2520is%2520to%2520foster%250Anovel%2520approaches%2520to%2520the%2520analysis%2520of%2520LAA%2520morphology.%250A%2520%2520LAA%2520segmentations%2520on%2520ImageCAS%2520were%2520generated%2520using%2520a%2520state-of-the-art%250Asegmentation%2520framework%2520developed%2520specifically%2520for%2520high%2520resolution%2520LAA%250Asegmentation.%2520We%2520trained%2520the%2520network%2520on%2520a%2520large%2520private%2520dataset%2520with%2520manual%250Aannotations%2520provided%2520by%2520medical%2520readers%2520guided%2520by%2520a%2520trained%2520cardiologist%2520and%250Atransferred%2520the%2520model%2520to%2520ImageCAS%2520data.%2520CA%2520labels%2520were%2520improved%2520from%2520the%250Aoriginal%2520ImageCAS%2520annotations%252C%2520while%2520PV%2520segmentations%2520were%2520refined%2520from%2520TS%250Aoutputs.%2520In%2520addition%252C%2520we%2520provide%2520a%2520list%2520of%2520scans%2520from%2520ImageCAS%2520that%2520contains%250Acommon%2520data%2520flaws%2520such%2520as%2520step%2520artefacts%252C%2520LAAs%2520extending%2520beyond%2520the%2520scanner%2527s%250Afield%2520of%2520view%252C%2520and%2520other%2520types%2520of%2520data%2520defects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20public%20cardiac%20CT%20dataset%20featuring%20the%20left%20atrial%20appendage&entry.906535625=Bjoern%20Hansen%20and%20Jonas%20Pedersen%20and%20Klaus%20F.%20Kofoed%20and%20Oscar%20Camara%20and%20Rasmus%20R.%20Paulsen%20and%20Kristine%20Soerensen&entry.1292438233=%20%20Despite%20the%20success%20of%20advanced%20segmentation%20frameworks%20such%20as%0ATotalSegmentator%20%28TS%29%2C%20accurate%20segmentations%20of%20the%20left%20atrial%20appendage%0A%28LAA%29%2C%20coronary%20arteries%20%28CAs%29%2C%20and%20pulmonary%20veins%20%28PVs%29%20remain%20a%20significant%0Achallenge%20in%20medical%20imaging.%20In%20this%20work%2C%20we%20present%20the%20first%20open-source%2C%0Aanatomically%20coherent%20dataset%20of%20curated%2C%20high-resolution%20segmentations%20for%0Athese%20structures%2C%20supplemented%20with%20whole-heart%20labels%20produced%20by%20TS%20on%20the%0Apublicly%20available%20ImageCAS%20dataset%20consisting%20of%201000%20cardiac%20computed%0Atomography%20angiography%20%28CCTA%29%20scans.%20One%20purpose%20of%20the%20data%20set%20is%20to%20foster%0Anovel%20approaches%20to%20the%20analysis%20of%20LAA%20morphology.%0A%20%20LAA%20segmentations%20on%20ImageCAS%20were%20generated%20using%20a%20state-of-the-art%0Asegmentation%20framework%20developed%20specifically%20for%20high%20resolution%20LAA%0Asegmentation.%20We%20trained%20the%20network%20on%20a%20large%20private%20dataset%20with%20manual%0Aannotations%20provided%20by%20medical%20readers%20guided%20by%20a%20trained%20cardiologist%20and%0Atransferred%20the%20model%20to%20ImageCAS%20data.%20CA%20labels%20were%20improved%20from%20the%0Aoriginal%20ImageCAS%20annotations%2C%20while%20PV%20segmentations%20were%20refined%20from%20TS%0Aoutputs.%20In%20addition%2C%20we%20provide%20a%20list%20of%20scans%20from%20ImageCAS%20that%20contains%0Acommon%20data%20flaws%20such%20as%20step%20artefacts%2C%20LAAs%20extending%20beyond%20the%20scanner%27s%0Afield%20of%20view%2C%20and%20other%20types%20of%20data%20defects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06090v1&entry.124074799=Read"},
{"title": "StarEmbed: Benchmarking Time Series Foundation Models on Astronomical\n  Observations of Variable Stars", "author": "Weijian Li and Hong-Yu Chen and Qinjie Lin and Nabeel Rehemtulla and Ved G. Shah and Dennis Wu and Adam A. Miller and Han Liu", "abstract": "  Time series foundation models (TSFMs) are increasingly being adopted as\nhighly-capable general-purpose time series representation learners. Although\ntheir training corpora are vast, they exclude astronomical time series data.\nObservations of stars produce peta-scale time series with unique challenges\nincluding irregular sampling and heteroskedasticity. We introduce StarEmbed,\nthe first public benchmark for rigorous and standardized evaluation of\nstate-of-the-art TSFMs on stellar time series observations (``light curves'').\nWe benchmark on three scientifically-motivated downstream tasks: unsupervised\nclustering, supervised classification, and out-of-distribution source\ndetection. StarEmbed integrates a catalog of expert-vetted labels with\nmulti-variate light curves from the Zwicky Transient Facility, yielding ~40k\nhand-labeled light curves spread across seven astrophysical classes. We\nevaluate the zero-shot representation capabilities of three TSFMs (MOIRAI,\nChronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against\nhandcrafted feature extraction, the long-standing baseline in the astrophysics\nliterature. Our results demonstrate that these TSFMs, especially the Chronos\nmodels, which are trained on data completely unlike the astronomical\nobservations, can outperform established astrophysics-specific baselines in\nsome tasks and effectively generalize to entirely new data. In particular,\nTSFMs deliver state-of-the-art performance on our out-of-distribution source\ndetection benchmark. With the first benchmark of TSFMs on astronomical time\nseries data, we test the limits of their generalization and motivate a paradigm\nshift in time-domain astronomy from using task-specific, fully supervised\npipelines toward adopting generic foundation model representations for the\nanalysis of peta-scale datasets from forthcoming observatories.\n", "link": "http://arxiv.org/abs/2510.06200v1", "date": "2025-10-07", "relevancy": 2.4022, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StarEmbed%3A%20Benchmarking%20Time%20Series%20Foundation%20Models%20on%20Astronomical%0A%20%20Observations%20of%20Variable%20Stars&body=Title%3A%20StarEmbed%3A%20Benchmarking%20Time%20Series%20Foundation%20Models%20on%20Astronomical%0A%20%20Observations%20of%20Variable%20Stars%0AAuthor%3A%20Weijian%20Li%20and%20Hong-Yu%20Chen%20and%20Qinjie%20Lin%20and%20Nabeel%20Rehemtulla%20and%20Ved%20G.%20Shah%20and%20Dennis%20Wu%20and%20Adam%20A.%20Miller%20and%20Han%20Liu%0AAbstract%3A%20%20%20Time%20series%20foundation%20models%20%28TSFMs%29%20are%20increasingly%20being%20adopted%20as%0Ahighly-capable%20general-purpose%20time%20series%20representation%20learners.%20Although%0Atheir%20training%20corpora%20are%20vast%2C%20they%20exclude%20astronomical%20time%20series%20data.%0AObservations%20of%20stars%20produce%20peta-scale%20time%20series%20with%20unique%20challenges%0Aincluding%20irregular%20sampling%20and%20heteroskedasticity.%20We%20introduce%20StarEmbed%2C%0Athe%20first%20public%20benchmark%20for%20rigorous%20and%20standardized%20evaluation%20of%0Astate-of-the-art%20TSFMs%20on%20stellar%20time%20series%20observations%20%28%60%60light%20curves%27%27%29.%0AWe%20benchmark%20on%20three%20scientifically-motivated%20downstream%20tasks%3A%20unsupervised%0Aclustering%2C%20supervised%20classification%2C%20and%20out-of-distribution%20source%0Adetection.%20StarEmbed%20integrates%20a%20catalog%20of%20expert-vetted%20labels%20with%0Amulti-variate%20light%20curves%20from%20the%20Zwicky%20Transient%20Facility%2C%20yielding%20~40k%0Ahand-labeled%20light%20curves%20spread%20across%20seven%20astrophysical%20classes.%20We%0Aevaluate%20the%20zero-shot%20representation%20capabilities%20of%20three%20TSFMs%20%28MOIRAI%2C%0AChronos%2C%20Chronos-Bolt%29%20and%20a%20domain-specific%20transformer%20%28Astromer%29%20against%0Ahandcrafted%20feature%20extraction%2C%20the%20long-standing%20baseline%20in%20the%20astrophysics%0Aliterature.%20Our%20results%20demonstrate%20that%20these%20TSFMs%2C%20especially%20the%20Chronos%0Amodels%2C%20which%20are%20trained%20on%20data%20completely%20unlike%20the%20astronomical%0Aobservations%2C%20can%20outperform%20established%20astrophysics-specific%20baselines%20in%0Asome%20tasks%20and%20effectively%20generalize%20to%20entirely%20new%20data.%20In%20particular%2C%0ATSFMs%20deliver%20state-of-the-art%20performance%20on%20our%20out-of-distribution%20source%0Adetection%20benchmark.%20With%20the%20first%20benchmark%20of%20TSFMs%20on%20astronomical%20time%0Aseries%20data%2C%20we%20test%20the%20limits%20of%20their%20generalization%20and%20motivate%20a%20paradigm%0Ashift%20in%20time-domain%20astronomy%20from%20using%20task-specific%2C%20fully%20supervised%0Apipelines%20toward%20adopting%20generic%20foundation%20model%20representations%20for%20the%0Aanalysis%20of%20peta-scale%20datasets%20from%20forthcoming%20observatories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStarEmbed%253A%2520Benchmarking%2520Time%2520Series%2520Foundation%2520Models%2520on%2520Astronomical%250A%2520%2520Observations%2520of%2520Variable%2520Stars%26entry.906535625%3DWeijian%2520Li%2520and%2520Hong-Yu%2520Chen%2520and%2520Qinjie%2520Lin%2520and%2520Nabeel%2520Rehemtulla%2520and%2520Ved%2520G.%2520Shah%2520and%2520Dennis%2520Wu%2520and%2520Adam%2520A.%2520Miller%2520and%2520Han%2520Liu%26entry.1292438233%3D%2520%2520Time%2520series%2520foundation%2520models%2520%2528TSFMs%2529%2520are%2520increasingly%2520being%2520adopted%2520as%250Ahighly-capable%2520general-purpose%2520time%2520series%2520representation%2520learners.%2520Although%250Atheir%2520training%2520corpora%2520are%2520vast%252C%2520they%2520exclude%2520astronomical%2520time%2520series%2520data.%250AObservations%2520of%2520stars%2520produce%2520peta-scale%2520time%2520series%2520with%2520unique%2520challenges%250Aincluding%2520irregular%2520sampling%2520and%2520heteroskedasticity.%2520We%2520introduce%2520StarEmbed%252C%250Athe%2520first%2520public%2520benchmark%2520for%2520rigorous%2520and%2520standardized%2520evaluation%2520of%250Astate-of-the-art%2520TSFMs%2520on%2520stellar%2520time%2520series%2520observations%2520%2528%2560%2560light%2520curves%2527%2527%2529.%250AWe%2520benchmark%2520on%2520three%2520scientifically-motivated%2520downstream%2520tasks%253A%2520unsupervised%250Aclustering%252C%2520supervised%2520classification%252C%2520and%2520out-of-distribution%2520source%250Adetection.%2520StarEmbed%2520integrates%2520a%2520catalog%2520of%2520expert-vetted%2520labels%2520with%250Amulti-variate%2520light%2520curves%2520from%2520the%2520Zwicky%2520Transient%2520Facility%252C%2520yielding%2520~40k%250Ahand-labeled%2520light%2520curves%2520spread%2520across%2520seven%2520astrophysical%2520classes.%2520We%250Aevaluate%2520the%2520zero-shot%2520representation%2520capabilities%2520of%2520three%2520TSFMs%2520%2528MOIRAI%252C%250AChronos%252C%2520Chronos-Bolt%2529%2520and%2520a%2520domain-specific%2520transformer%2520%2528Astromer%2529%2520against%250Ahandcrafted%2520feature%2520extraction%252C%2520the%2520long-standing%2520baseline%2520in%2520the%2520astrophysics%250Aliterature.%2520Our%2520results%2520demonstrate%2520that%2520these%2520TSFMs%252C%2520especially%2520the%2520Chronos%250Amodels%252C%2520which%2520are%2520trained%2520on%2520data%2520completely%2520unlike%2520the%2520astronomical%250Aobservations%252C%2520can%2520outperform%2520established%2520astrophysics-specific%2520baselines%2520in%250Asome%2520tasks%2520and%2520effectively%2520generalize%2520to%2520entirely%2520new%2520data.%2520In%2520particular%252C%250ATSFMs%2520deliver%2520state-of-the-art%2520performance%2520on%2520our%2520out-of-distribution%2520source%250Adetection%2520benchmark.%2520With%2520the%2520first%2520benchmark%2520of%2520TSFMs%2520on%2520astronomical%2520time%250Aseries%2520data%252C%2520we%2520test%2520the%2520limits%2520of%2520their%2520generalization%2520and%2520motivate%2520a%2520paradigm%250Ashift%2520in%2520time-domain%2520astronomy%2520from%2520using%2520task-specific%252C%2520fully%2520supervised%250Apipelines%2520toward%2520adopting%2520generic%2520foundation%2520model%2520representations%2520for%2520the%250Aanalysis%2520of%2520peta-scale%2520datasets%2520from%2520forthcoming%2520observatories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StarEmbed%3A%20Benchmarking%20Time%20Series%20Foundation%20Models%20on%20Astronomical%0A%20%20Observations%20of%20Variable%20Stars&entry.906535625=Weijian%20Li%20and%20Hong-Yu%20Chen%20and%20Qinjie%20Lin%20and%20Nabeel%20Rehemtulla%20and%20Ved%20G.%20Shah%20and%20Dennis%20Wu%20and%20Adam%20A.%20Miller%20and%20Han%20Liu&entry.1292438233=%20%20Time%20series%20foundation%20models%20%28TSFMs%29%20are%20increasingly%20being%20adopted%20as%0Ahighly-capable%20general-purpose%20time%20series%20representation%20learners.%20Although%0Atheir%20training%20corpora%20are%20vast%2C%20they%20exclude%20astronomical%20time%20series%20data.%0AObservations%20of%20stars%20produce%20peta-scale%20time%20series%20with%20unique%20challenges%0Aincluding%20irregular%20sampling%20and%20heteroskedasticity.%20We%20introduce%20StarEmbed%2C%0Athe%20first%20public%20benchmark%20for%20rigorous%20and%20standardized%20evaluation%20of%0Astate-of-the-art%20TSFMs%20on%20stellar%20time%20series%20observations%20%28%60%60light%20curves%27%27%29.%0AWe%20benchmark%20on%20three%20scientifically-motivated%20downstream%20tasks%3A%20unsupervised%0Aclustering%2C%20supervised%20classification%2C%20and%20out-of-distribution%20source%0Adetection.%20StarEmbed%20integrates%20a%20catalog%20of%20expert-vetted%20labels%20with%0Amulti-variate%20light%20curves%20from%20the%20Zwicky%20Transient%20Facility%2C%20yielding%20~40k%0Ahand-labeled%20light%20curves%20spread%20across%20seven%20astrophysical%20classes.%20We%0Aevaluate%20the%20zero-shot%20representation%20capabilities%20of%20three%20TSFMs%20%28MOIRAI%2C%0AChronos%2C%20Chronos-Bolt%29%20and%20a%20domain-specific%20transformer%20%28Astromer%29%20against%0Ahandcrafted%20feature%20extraction%2C%20the%20long-standing%20baseline%20in%20the%20astrophysics%0Aliterature.%20Our%20results%20demonstrate%20that%20these%20TSFMs%2C%20especially%20the%20Chronos%0Amodels%2C%20which%20are%20trained%20on%20data%20completely%20unlike%20the%20astronomical%0Aobservations%2C%20can%20outperform%20established%20astrophysics-specific%20baselines%20in%0Asome%20tasks%20and%20effectively%20generalize%20to%20entirely%20new%20data.%20In%20particular%2C%0ATSFMs%20deliver%20state-of-the-art%20performance%20on%20our%20out-of-distribution%20source%0Adetection%20benchmark.%20With%20the%20first%20benchmark%20of%20TSFMs%20on%20astronomical%20time%0Aseries%20data%2C%20we%20test%20the%20limits%20of%20their%20generalization%20and%20motivate%20a%20paradigm%0Ashift%20in%20time-domain%20astronomy%20from%20using%20task-specific%2C%20fully%20supervised%0Apipelines%20toward%20adopting%20generic%20foundation%20model%20representations%20for%20the%0Aanalysis%20of%20peta-scale%20datasets%20from%20forthcoming%20observatories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06200v1&entry.124074799=Read"},
{"title": "Mellum: Production-Grade in-IDE Contextual Code Completion with\n  Multi-File Project Understanding", "author": "Nikita Pavlichenko and Iurii Nazarov and Ivan Dolgov and Ekaterina Garanina and Dmitry Ustalov and Ivan Bondyrev and Kseniia Lysaniuk and Evgeniia Vu and Kirill Chekmenev and Joseph Shtok and Yaroslav Golubev and Anton Semenkin and Uladzislau Sazanovich", "abstract": "  We present the Mellum models family, open-weight code completion models\ndesigned for interactive use in JetBrains IDEs. Mellums have 4B parameters,\nadopt a Llama-style architecture, and are pre-trained on ~4T tokens of\npermissively licensed, multi-language code. Our studies show that (i) careful\ndata curation and staged training significantly improve the model's quality,\n(ii) editor-critical capabilities such as context packing are necessary for\nhigh-quality suggestions, and (iii) a compact, task-focused model can meet the\ncost and latency constraints of interactive completion.\n  In the paper, we describe an end-to-end industrial pipeline for producing\ncontextualized in-editor completion: disciplined data governance, multi-stage\ntraining that includes fill-in-the-middle and project context via supervised\nfine-tuning, and alignment via direct preference optimization using feedback\nfrom real-world scenarios. Our quality evaluations include both large-scale\noffline benchmarks and online telemetry from production deployments in\nJetBrains IDEs. Mellums are released under the Apache-2.0 license on\nHuggingFace, with a public model card providing a reproducible reference for\npractitioners. Our experience offers a pragmatic blueprint for taking a\nfocused, open model from a research prototype to at scale production for\nhundreds of thousands of users.\n", "link": "http://arxiv.org/abs/2510.05788v1", "date": "2025-10-07", "relevancy": 2.4022, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mellum%3A%20Production-Grade%20in-IDE%20Contextual%20Code%20Completion%20with%0A%20%20Multi-File%20Project%20Understanding&body=Title%3A%20Mellum%3A%20Production-Grade%20in-IDE%20Contextual%20Code%20Completion%20with%0A%20%20Multi-File%20Project%20Understanding%0AAuthor%3A%20Nikita%20Pavlichenko%20and%20Iurii%20Nazarov%20and%20Ivan%20Dolgov%20and%20Ekaterina%20Garanina%20and%20Dmitry%20Ustalov%20and%20Ivan%20Bondyrev%20and%20Kseniia%20Lysaniuk%20and%20Evgeniia%20Vu%20and%20Kirill%20Chekmenev%20and%20Joseph%20Shtok%20and%20Yaroslav%20Golubev%20and%20Anton%20Semenkin%20and%20Uladzislau%20Sazanovich%0AAbstract%3A%20%20%20We%20present%20the%20Mellum%20models%20family%2C%20open-weight%20code%20completion%20models%0Adesigned%20for%20interactive%20use%20in%20JetBrains%20IDEs.%20Mellums%20have%204B%20parameters%2C%0Aadopt%20a%20Llama-style%20architecture%2C%20and%20are%20pre-trained%20on%20~4T%20tokens%20of%0Apermissively%20licensed%2C%20multi-language%20code.%20Our%20studies%20show%20that%20%28i%29%20careful%0Adata%20curation%20and%20staged%20training%20significantly%20improve%20the%20model%27s%20quality%2C%0A%28ii%29%20editor-critical%20capabilities%20such%20as%20context%20packing%20are%20necessary%20for%0Ahigh-quality%20suggestions%2C%20and%20%28iii%29%20a%20compact%2C%20task-focused%20model%20can%20meet%20the%0Acost%20and%20latency%20constraints%20of%20interactive%20completion.%0A%20%20In%20the%20paper%2C%20we%20describe%20an%20end-to-end%20industrial%20pipeline%20for%20producing%0Acontextualized%20in-editor%20completion%3A%20disciplined%20data%20governance%2C%20multi-stage%0Atraining%20that%20includes%20fill-in-the-middle%20and%20project%20context%20via%20supervised%0Afine-tuning%2C%20and%20alignment%20via%20direct%20preference%20optimization%20using%20feedback%0Afrom%20real-world%20scenarios.%20Our%20quality%20evaluations%20include%20both%20large-scale%0Aoffline%20benchmarks%20and%20online%20telemetry%20from%20production%20deployments%20in%0AJetBrains%20IDEs.%20Mellums%20are%20released%20under%20the%20Apache-2.0%20license%20on%0AHuggingFace%2C%20with%20a%20public%20model%20card%20providing%20a%20reproducible%20reference%20for%0Apractitioners.%20Our%20experience%20offers%20a%20pragmatic%20blueprint%20for%20taking%20a%0Afocused%2C%20open%20model%20from%20a%20research%20prototype%20to%20at%20scale%20production%20for%0Ahundreds%20of%20thousands%20of%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMellum%253A%2520Production-Grade%2520in-IDE%2520Contextual%2520Code%2520Completion%2520with%250A%2520%2520Multi-File%2520Project%2520Understanding%26entry.906535625%3DNikita%2520Pavlichenko%2520and%2520Iurii%2520Nazarov%2520and%2520Ivan%2520Dolgov%2520and%2520Ekaterina%2520Garanina%2520and%2520Dmitry%2520Ustalov%2520and%2520Ivan%2520Bondyrev%2520and%2520Kseniia%2520Lysaniuk%2520and%2520Evgeniia%2520Vu%2520and%2520Kirill%2520Chekmenev%2520and%2520Joseph%2520Shtok%2520and%2520Yaroslav%2520Golubev%2520and%2520Anton%2520Semenkin%2520and%2520Uladzislau%2520Sazanovich%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Mellum%2520models%2520family%252C%2520open-weight%2520code%2520completion%2520models%250Adesigned%2520for%2520interactive%2520use%2520in%2520JetBrains%2520IDEs.%2520Mellums%2520have%25204B%2520parameters%252C%250Aadopt%2520a%2520Llama-style%2520architecture%252C%2520and%2520are%2520pre-trained%2520on%2520~4T%2520tokens%2520of%250Apermissively%2520licensed%252C%2520multi-language%2520code.%2520Our%2520studies%2520show%2520that%2520%2528i%2529%2520careful%250Adata%2520curation%2520and%2520staged%2520training%2520significantly%2520improve%2520the%2520model%2527s%2520quality%252C%250A%2528ii%2529%2520editor-critical%2520capabilities%2520such%2520as%2520context%2520packing%2520are%2520necessary%2520for%250Ahigh-quality%2520suggestions%252C%2520and%2520%2528iii%2529%2520a%2520compact%252C%2520task-focused%2520model%2520can%2520meet%2520the%250Acost%2520and%2520latency%2520constraints%2520of%2520interactive%2520completion.%250A%2520%2520In%2520the%2520paper%252C%2520we%2520describe%2520an%2520end-to-end%2520industrial%2520pipeline%2520for%2520producing%250Acontextualized%2520in-editor%2520completion%253A%2520disciplined%2520data%2520governance%252C%2520multi-stage%250Atraining%2520that%2520includes%2520fill-in-the-middle%2520and%2520project%2520context%2520via%2520supervised%250Afine-tuning%252C%2520and%2520alignment%2520via%2520direct%2520preference%2520optimization%2520using%2520feedback%250Afrom%2520real-world%2520scenarios.%2520Our%2520quality%2520evaluations%2520include%2520both%2520large-scale%250Aoffline%2520benchmarks%2520and%2520online%2520telemetry%2520from%2520production%2520deployments%2520in%250AJetBrains%2520IDEs.%2520Mellums%2520are%2520released%2520under%2520the%2520Apache-2.0%2520license%2520on%250AHuggingFace%252C%2520with%2520a%2520public%2520model%2520card%2520providing%2520a%2520reproducible%2520reference%2520for%250Apractitioners.%2520Our%2520experience%2520offers%2520a%2520pragmatic%2520blueprint%2520for%2520taking%2520a%250Afocused%252C%2520open%2520model%2520from%2520a%2520research%2520prototype%2520to%2520at%2520scale%2520production%2520for%250Ahundreds%2520of%2520thousands%2520of%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mellum%3A%20Production-Grade%20in-IDE%20Contextual%20Code%20Completion%20with%0A%20%20Multi-File%20Project%20Understanding&entry.906535625=Nikita%20Pavlichenko%20and%20Iurii%20Nazarov%20and%20Ivan%20Dolgov%20and%20Ekaterina%20Garanina%20and%20Dmitry%20Ustalov%20and%20Ivan%20Bondyrev%20and%20Kseniia%20Lysaniuk%20and%20Evgeniia%20Vu%20and%20Kirill%20Chekmenev%20and%20Joseph%20Shtok%20and%20Yaroslav%20Golubev%20and%20Anton%20Semenkin%20and%20Uladzislau%20Sazanovich&entry.1292438233=%20%20We%20present%20the%20Mellum%20models%20family%2C%20open-weight%20code%20completion%20models%0Adesigned%20for%20interactive%20use%20in%20JetBrains%20IDEs.%20Mellums%20have%204B%20parameters%2C%0Aadopt%20a%20Llama-style%20architecture%2C%20and%20are%20pre-trained%20on%20~4T%20tokens%20of%0Apermissively%20licensed%2C%20multi-language%20code.%20Our%20studies%20show%20that%20%28i%29%20careful%0Adata%20curation%20and%20staged%20training%20significantly%20improve%20the%20model%27s%20quality%2C%0A%28ii%29%20editor-critical%20capabilities%20such%20as%20context%20packing%20are%20necessary%20for%0Ahigh-quality%20suggestions%2C%20and%20%28iii%29%20a%20compact%2C%20task-focused%20model%20can%20meet%20the%0Acost%20and%20latency%20constraints%20of%20interactive%20completion.%0A%20%20In%20the%20paper%2C%20we%20describe%20an%20end-to-end%20industrial%20pipeline%20for%20producing%0Acontextualized%20in-editor%20completion%3A%20disciplined%20data%20governance%2C%20multi-stage%0Atraining%20that%20includes%20fill-in-the-middle%20and%20project%20context%20via%20supervised%0Afine-tuning%2C%20and%20alignment%20via%20direct%20preference%20optimization%20using%20feedback%0Afrom%20real-world%20scenarios.%20Our%20quality%20evaluations%20include%20both%20large-scale%0Aoffline%20benchmarks%20and%20online%20telemetry%20from%20production%20deployments%20in%0AJetBrains%20IDEs.%20Mellums%20are%20released%20under%20the%20Apache-2.0%20license%20on%0AHuggingFace%2C%20with%20a%20public%20model%20card%20providing%20a%20reproducible%20reference%20for%0Apractitioners.%20Our%20experience%20offers%20a%20pragmatic%20blueprint%20for%20taking%20a%0Afocused%2C%20open%20model%20from%20a%20research%20prototype%20to%20at%20scale%20production%20for%0Ahundreds%20of%20thousands%20of%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05788v1&entry.124074799=Read"},
{"title": "The Physics of Data and Tasks: Theories of Locality and Compositionality\n  in Deep Learning", "author": "Alessandro Favero", "abstract": "  Deep neural networks have achieved remarkable success, yet our understanding\nof how they learn remains limited. These models can learn high-dimensional\ntasks, which is generally statistically intractable due to the curse of\ndimensionality. This apparent paradox suggests that learnable data must have an\nunderlying latent structure. What is the nature of this structure? How do\nneural networks encode and exploit it, and how does it quantitatively impact\nperformance - for instance, how does generalization improve with the number of\ntraining examples? This thesis addresses these questions by studying the roles\nof locality and compositionality in data, tasks, and deep learning\nrepresentations.\n", "link": "http://arxiv.org/abs/2510.06106v1", "date": "2025-10-07", "relevancy": 2.3952, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Physics%20of%20Data%20and%20Tasks%3A%20Theories%20of%20Locality%20and%20Compositionality%0A%20%20in%20Deep%20Learning&body=Title%3A%20The%20Physics%20of%20Data%20and%20Tasks%3A%20Theories%20of%20Locality%20and%20Compositionality%0A%20%20in%20Deep%20Learning%0AAuthor%3A%20Alessandro%20Favero%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20achieved%20remarkable%20success%2C%20yet%20our%20understanding%0Aof%20how%20they%20learn%20remains%20limited.%20These%20models%20can%20learn%20high-dimensional%0Atasks%2C%20which%20is%20generally%20statistically%20intractable%20due%20to%20the%20curse%20of%0Adimensionality.%20This%20apparent%20paradox%20suggests%20that%20learnable%20data%20must%20have%20an%0Aunderlying%20latent%20structure.%20What%20is%20the%20nature%20of%20this%20structure%3F%20How%20do%0Aneural%20networks%20encode%20and%20exploit%20it%2C%20and%20how%20does%20it%20quantitatively%20impact%0Aperformance%20-%20for%20instance%2C%20how%20does%20generalization%20improve%20with%20the%20number%20of%0Atraining%20examples%3F%20This%20thesis%20addresses%20these%20questions%20by%20studying%20the%20roles%0Aof%20locality%20and%20compositionality%20in%20data%2C%20tasks%2C%20and%20deep%20learning%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Physics%2520of%2520Data%2520and%2520Tasks%253A%2520Theories%2520of%2520Locality%2520and%2520Compositionality%250A%2520%2520in%2520Deep%2520Learning%26entry.906535625%3DAlessandro%2520Favero%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520achieved%2520remarkable%2520success%252C%2520yet%2520our%2520understanding%250Aof%2520how%2520they%2520learn%2520remains%2520limited.%2520These%2520models%2520can%2520learn%2520high-dimensional%250Atasks%252C%2520which%2520is%2520generally%2520statistically%2520intractable%2520due%2520to%2520the%2520curse%2520of%250Adimensionality.%2520This%2520apparent%2520paradox%2520suggests%2520that%2520learnable%2520data%2520must%2520have%2520an%250Aunderlying%2520latent%2520structure.%2520What%2520is%2520the%2520nature%2520of%2520this%2520structure%253F%2520How%2520do%250Aneural%2520networks%2520encode%2520and%2520exploit%2520it%252C%2520and%2520how%2520does%2520it%2520quantitatively%2520impact%250Aperformance%2520-%2520for%2520instance%252C%2520how%2520does%2520generalization%2520improve%2520with%2520the%2520number%2520of%250Atraining%2520examples%253F%2520This%2520thesis%2520addresses%2520these%2520questions%2520by%2520studying%2520the%2520roles%250Aof%2520locality%2520and%2520compositionality%2520in%2520data%252C%2520tasks%252C%2520and%2520deep%2520learning%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Physics%20of%20Data%20and%20Tasks%3A%20Theories%20of%20Locality%20and%20Compositionality%0A%20%20in%20Deep%20Learning&entry.906535625=Alessandro%20Favero&entry.1292438233=%20%20Deep%20neural%20networks%20have%20achieved%20remarkable%20success%2C%20yet%20our%20understanding%0Aof%20how%20they%20learn%20remains%20limited.%20These%20models%20can%20learn%20high-dimensional%0Atasks%2C%20which%20is%20generally%20statistically%20intractable%20due%20to%20the%20curse%20of%0Adimensionality.%20This%20apparent%20paradox%20suggests%20that%20learnable%20data%20must%20have%20an%0Aunderlying%20latent%20structure.%20What%20is%20the%20nature%20of%20this%20structure%3F%20How%20do%0Aneural%20networks%20encode%20and%20exploit%20it%2C%20and%20how%20does%20it%20quantitatively%20impact%0Aperformance%20-%20for%20instance%2C%20how%20does%20generalization%20improve%20with%20the%20number%20of%0Atraining%20examples%3F%20This%20thesis%20addresses%20these%20questions%20by%20studying%20the%20roles%0Aof%20locality%20and%20compositionality%20in%20data%2C%20tasks%2C%20and%20deep%20learning%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06106v1&entry.124074799=Read"},
{"title": "CAPO: Towards Enhancing LLM Reasoning through Generative Credit\n  Assignment", "author": "Guofu Xie and Yunsheng Shi and Hongtao Tian and Ting Yao and Xiao Zhang", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback. However, current RLVR methods typically assign the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies. Methods like PPO provide\ncredit assignment by value estimation, but yield inaccurate and unverifiable\nsignals due to limited sampling. On the other hand, methods using Process\nReward Models can provide step-wise rewards but suffer from several key\nlimitations: they require high-quality process supervision labels, the feedback\nis unreliable due to probabilistic reward modeling, and their application in\nonline reinforcement learning (RL) is time-consuming. To overcome these\nlimitations, we introduce a simple but efficient method-Credit Assignment\nPolicy Optimization (CAPO). Instead of training auxiliary models, CAPO directly\nleverages an off-the-shelf, general-purpose LLM as a Generative Process Reward\nModel (LLM-as-GenPRM) to generate all step-wise critique by one pass only based\non the correctness of the step itself, providing deterministic token-level\ncredits to refine the tokens that were originally assigned identical rule-based\nrewards. To further enhance the accuracy and robustness, we employ voting\nmechanisms that scale with the number of generated critiques. Extensive\nexperiments on various backbones like Llama and Qwen models show that CAPO\nconsistently outperforms supervised learning-based and RL-based fine-tuning\nmethods across four challenging mathematical benchmarks and three out-of-domain\nbenchmarks. Further analysis shows that CAPO can help the model to foster the\nlearning of correct reasoning pathways leading to correct answers.\n", "link": "http://arxiv.org/abs/2508.02298v2", "date": "2025-10-07", "relevancy": 2.3922, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPO%3A%20Towards%20Enhancing%20LLM%20Reasoning%20through%20Generative%20Credit%0A%20%20Assignment&body=Title%3A%20CAPO%3A%20Towards%20Enhancing%20LLM%20Reasoning%20through%20Generative%20Credit%0A%20%20Assignment%0AAuthor%3A%20Guofu%20Xie%20and%20Yunsheng%20Shi%20and%20Hongtao%20Tian%20and%20Ting%20Yao%20and%20Xiao%20Zhang%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20improved%20the%0Areasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29%20by%20using%20rule-based%20binary%0Afeedback.%20However%2C%20current%20RLVR%20methods%20typically%20assign%20the%20same%20reward%20to%0Aevery%20token.%20This%20coarse-grained%20feedback%20hampers%20precise%20credit%20assignment%2C%0Amaking%20it%20hard%20for%20models%20to%20identify%20which%20reasoning%20steps%20lead%20to%20success%20or%0Afailure%2C%20and%20often%20results%20in%20suboptimal%20policies.%20Methods%20like%20PPO%20provide%0Acredit%20assignment%20by%20value%20estimation%2C%20but%20yield%20inaccurate%20and%20unverifiable%0Asignals%20due%20to%20limited%20sampling.%20On%20the%20other%20hand%2C%20methods%20using%20Process%0AReward%20Models%20can%20provide%20step-wise%20rewards%20but%20suffer%20from%20several%20key%0Alimitations%3A%20they%20require%20high-quality%20process%20supervision%20labels%2C%20the%20feedback%0Ais%20unreliable%20due%20to%20probabilistic%20reward%20modeling%2C%20and%20their%20application%20in%0Aonline%20reinforcement%20learning%20%28RL%29%20is%20time-consuming.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20a%20simple%20but%20efficient%20method-Credit%20Assignment%0APolicy%20Optimization%20%28CAPO%29.%20Instead%20of%20training%20auxiliary%20models%2C%20CAPO%20directly%0Aleverages%20an%20off-the-shelf%2C%20general-purpose%20LLM%20as%20a%20Generative%20Process%20Reward%0AModel%20%28LLM-as-GenPRM%29%20to%20generate%20all%20step-wise%20critique%20by%20one%20pass%20only%20based%0Aon%20the%20correctness%20of%20the%20step%20itself%2C%20providing%20deterministic%20token-level%0Acredits%20to%20refine%20the%20tokens%20that%20were%20originally%20assigned%20identical%20rule-based%0Arewards.%20To%20further%20enhance%20the%20accuracy%20and%20robustness%2C%20we%20employ%20voting%0Amechanisms%20that%20scale%20with%20the%20number%20of%20generated%20critiques.%20Extensive%0Aexperiments%20on%20various%20backbones%20like%20Llama%20and%20Qwen%20models%20show%20that%20CAPO%0Aconsistently%20outperforms%20supervised%20learning-based%20and%20RL-based%20fine-tuning%0Amethods%20across%20four%20challenging%20mathematical%20benchmarks%20and%20three%20out-of-domain%0Abenchmarks.%20Further%20analysis%20shows%20that%20CAPO%20can%20help%20the%20model%20to%20foster%20the%0Alearning%20of%20correct%20reasoning%20pathways%20leading%20to%20correct%20answers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPO%253A%2520Towards%2520Enhancing%2520LLM%2520Reasoning%2520through%2520Generative%2520Credit%250A%2520%2520Assignment%26entry.906535625%3DGuofu%2520Xie%2520and%2520Yunsheng%2520Shi%2520and%2520Hongtao%2520Tian%2520and%2520Ting%2520Yao%2520and%2520Xiao%2520Zhang%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520improved%2520the%250Areasoning%2520abilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520by%2520using%2520rule-based%2520binary%250Afeedback.%2520However%252C%2520current%2520RLVR%2520methods%2520typically%2520assign%2520the%2520same%2520reward%2520to%250Aevery%2520token.%2520This%2520coarse-grained%2520feedback%2520hampers%2520precise%2520credit%2520assignment%252C%250Amaking%2520it%2520hard%2520for%2520models%2520to%2520identify%2520which%2520reasoning%2520steps%2520lead%2520to%2520success%2520or%250Afailure%252C%2520and%2520often%2520results%2520in%2520suboptimal%2520policies.%2520Methods%2520like%2520PPO%2520provide%250Acredit%2520assignment%2520by%2520value%2520estimation%252C%2520but%2520yield%2520inaccurate%2520and%2520unverifiable%250Asignals%2520due%2520to%2520limited%2520sampling.%2520On%2520the%2520other%2520hand%252C%2520methods%2520using%2520Process%250AReward%2520Models%2520can%2520provide%2520step-wise%2520rewards%2520but%2520suffer%2520from%2520several%2520key%250Alimitations%253A%2520they%2520require%2520high-quality%2520process%2520supervision%2520labels%252C%2520the%2520feedback%250Ais%2520unreliable%2520due%2520to%2520probabilistic%2520reward%2520modeling%252C%2520and%2520their%2520application%2520in%250Aonline%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520time-consuming.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520simple%2520but%2520efficient%2520method-Credit%2520Assignment%250APolicy%2520Optimization%2520%2528CAPO%2529.%2520Instead%2520of%2520training%2520auxiliary%2520models%252C%2520CAPO%2520directly%250Aleverages%2520an%2520off-the-shelf%252C%2520general-purpose%2520LLM%2520as%2520a%2520Generative%2520Process%2520Reward%250AModel%2520%2528LLM-as-GenPRM%2529%2520to%2520generate%2520all%2520step-wise%2520critique%2520by%2520one%2520pass%2520only%2520based%250Aon%2520the%2520correctness%2520of%2520the%2520step%2520itself%252C%2520providing%2520deterministic%2520token-level%250Acredits%2520to%2520refine%2520the%2520tokens%2520that%2520were%2520originally%2520assigned%2520identical%2520rule-based%250Arewards.%2520To%2520further%2520enhance%2520the%2520accuracy%2520and%2520robustness%252C%2520we%2520employ%2520voting%250Amechanisms%2520that%2520scale%2520with%2520the%2520number%2520of%2520generated%2520critiques.%2520Extensive%250Aexperiments%2520on%2520various%2520backbones%2520like%2520Llama%2520and%2520Qwen%2520models%2520show%2520that%2520CAPO%250Aconsistently%2520outperforms%2520supervised%2520learning-based%2520and%2520RL-based%2520fine-tuning%250Amethods%2520across%2520four%2520challenging%2520mathematical%2520benchmarks%2520and%2520three%2520out-of-domain%250Abenchmarks.%2520Further%2520analysis%2520shows%2520that%2520CAPO%2520can%2520help%2520the%2520model%2520to%2520foster%2520the%250Alearning%2520of%2520correct%2520reasoning%2520pathways%2520leading%2520to%2520correct%2520answers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPO%3A%20Towards%20Enhancing%20LLM%20Reasoning%20through%20Generative%20Credit%0A%20%20Assignment&entry.906535625=Guofu%20Xie%20and%20Yunsheng%20Shi%20and%20Hongtao%20Tian%20and%20Ting%20Yao%20and%20Xiao%20Zhang&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20improved%20the%0Areasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29%20by%20using%20rule-based%20binary%0Afeedback.%20However%2C%20current%20RLVR%20methods%20typically%20assign%20the%20same%20reward%20to%0Aevery%20token.%20This%20coarse-grained%20feedback%20hampers%20precise%20credit%20assignment%2C%0Amaking%20it%20hard%20for%20models%20to%20identify%20which%20reasoning%20steps%20lead%20to%20success%20or%0Afailure%2C%20and%20often%20results%20in%20suboptimal%20policies.%20Methods%20like%20PPO%20provide%0Acredit%20assignment%20by%20value%20estimation%2C%20but%20yield%20inaccurate%20and%20unverifiable%0Asignals%20due%20to%20limited%20sampling.%20On%20the%20other%20hand%2C%20methods%20using%20Process%0AReward%20Models%20can%20provide%20step-wise%20rewards%20but%20suffer%20from%20several%20key%0Alimitations%3A%20they%20require%20high-quality%20process%20supervision%20labels%2C%20the%20feedback%0Ais%20unreliable%20due%20to%20probabilistic%20reward%20modeling%2C%20and%20their%20application%20in%0Aonline%20reinforcement%20learning%20%28RL%29%20is%20time-consuming.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20a%20simple%20but%20efficient%20method-Credit%20Assignment%0APolicy%20Optimization%20%28CAPO%29.%20Instead%20of%20training%20auxiliary%20models%2C%20CAPO%20directly%0Aleverages%20an%20off-the-shelf%2C%20general-purpose%20LLM%20as%20a%20Generative%20Process%20Reward%0AModel%20%28LLM-as-GenPRM%29%20to%20generate%20all%20step-wise%20critique%20by%20one%20pass%20only%20based%0Aon%20the%20correctness%20of%20the%20step%20itself%2C%20providing%20deterministic%20token-level%0Acredits%20to%20refine%20the%20tokens%20that%20were%20originally%20assigned%20identical%20rule-based%0Arewards.%20To%20further%20enhance%20the%20accuracy%20and%20robustness%2C%20we%20employ%20voting%0Amechanisms%20that%20scale%20with%20the%20number%20of%20generated%20critiques.%20Extensive%0Aexperiments%20on%20various%20backbones%20like%20Llama%20and%20Qwen%20models%20show%20that%20CAPO%0Aconsistently%20outperforms%20supervised%20learning-based%20and%20RL-based%20fine-tuning%0Amethods%20across%20four%20challenging%20mathematical%20benchmarks%20and%20three%20out-of-domain%0Abenchmarks.%20Further%20analysis%20shows%20that%20CAPO%20can%20help%20the%20model%20to%20foster%20the%0Alearning%20of%20correct%20reasoning%20pathways%20leading%20to%20correct%20answers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02298v2&entry.124074799=Read"},
{"title": "Shortcuts Everywhere and Nowhere: Exploring Multi-Trigger Backdoor\n  Attacks", "author": "Yige Li and Jiabo He and Hanxun Huang and Jun Sun and Xingjun Ma and Yu-Gang Jiang", "abstract": "  Backdoor attacks have become a significant threat to the pre-training and\ndeployment of deep neural networks (DNNs). Although numerous methods for\ndetecting and mitigating backdoor attacks have been proposed, most rely on\nidentifying and eliminating the ``shortcut\" created by the backdoor, which\nlinks a specific source class to a target class. However, these approaches can\nbe easily circumvented by designing multiple backdoor triggers that create\nshortcuts everywhere and therefore nowhere specific. In this study, we explore\nthe concept of Multi-Trigger Backdoor Attacks (MTBAs), where multiple\nadversaries leverage different types of triggers to poison the same dataset. By\nproposing and investigating three types of multi-trigger attacks including\n\\textit{parallel}, \\textit{sequential}, and \\textit{hybrid} attacks, we\ndemonstrate that 1) multiple triggers can coexist, overwrite, or cross-activate\none another, and 2) MTBAs easily break the prevalent shortcut assumption\nunderlying most existing backdoor detection/removal methods, rendering them\nineffective. Given the security risk posed by MTBAs, we have created a\nmulti-trigger backdoor poisoning dataset to facilitate future research on\ndetecting and mitigating these attacks, and we also discuss potential defense\nstrategies against MTBAs. Our code is available at\nhttps://github.com/bboylyg/Multi-Trigger-Backdoor-Attacks.\n", "link": "http://arxiv.org/abs/2401.15295v4", "date": "2025-10-07", "relevancy": 2.363, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4856}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4681}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shortcuts%20Everywhere%20and%20Nowhere%3A%20Exploring%20Multi-Trigger%20Backdoor%0A%20%20Attacks&body=Title%3A%20Shortcuts%20Everywhere%20and%20Nowhere%3A%20Exploring%20Multi-Trigger%20Backdoor%0A%20%20Attacks%0AAuthor%3A%20Yige%20Li%20and%20Jiabo%20He%20and%20Hanxun%20Huang%20and%20Jun%20Sun%20and%20Xingjun%20Ma%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Backdoor%20attacks%20have%20become%20a%20significant%20threat%20to%20the%20pre-training%20and%0Adeployment%20of%20deep%20neural%20networks%20%28DNNs%29.%20Although%20numerous%20methods%20for%0Adetecting%20and%20mitigating%20backdoor%20attacks%20have%20been%20proposed%2C%20most%20rely%20on%0Aidentifying%20and%20eliminating%20the%20%60%60shortcut%22%20created%20by%20the%20backdoor%2C%20which%0Alinks%20a%20specific%20source%20class%20to%20a%20target%20class.%20However%2C%20these%20approaches%20can%0Abe%20easily%20circumvented%20by%20designing%20multiple%20backdoor%20triggers%20that%20create%0Ashortcuts%20everywhere%20and%20therefore%20nowhere%20specific.%20In%20this%20study%2C%20we%20explore%0Athe%20concept%20of%20Multi-Trigger%20Backdoor%20Attacks%20%28MTBAs%29%2C%20where%20multiple%0Aadversaries%20leverage%20different%20types%20of%20triggers%20to%20poison%20the%20same%20dataset.%20By%0Aproposing%20and%20investigating%20three%20types%20of%20multi-trigger%20attacks%20including%0A%5Ctextit%7Bparallel%7D%2C%20%5Ctextit%7Bsequential%7D%2C%20and%20%5Ctextit%7Bhybrid%7D%20attacks%2C%20we%0Ademonstrate%20that%201%29%20multiple%20triggers%20can%20coexist%2C%20overwrite%2C%20or%20cross-activate%0Aone%20another%2C%20and%202%29%20MTBAs%20easily%20break%20the%20prevalent%20shortcut%20assumption%0Aunderlying%20most%20existing%20backdoor%20detection/removal%20methods%2C%20rendering%20them%0Aineffective.%20Given%20the%20security%20risk%20posed%20by%20MTBAs%2C%20we%20have%20created%20a%0Amulti-trigger%20backdoor%20poisoning%20dataset%20to%20facilitate%20future%20research%20on%0Adetecting%20and%20mitigating%20these%20attacks%2C%20and%20we%20also%20discuss%20potential%20defense%0Astrategies%20against%20MTBAs.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/bboylyg/Multi-Trigger-Backdoor-Attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15295v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShortcuts%2520Everywhere%2520and%2520Nowhere%253A%2520Exploring%2520Multi-Trigger%2520Backdoor%250A%2520%2520Attacks%26entry.906535625%3DYige%2520Li%2520and%2520Jiabo%2520He%2520and%2520Hanxun%2520Huang%2520and%2520Jun%2520Sun%2520and%2520Xingjun%2520Ma%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Backdoor%2520attacks%2520have%2520become%2520a%2520significant%2520threat%2520to%2520the%2520pre-training%2520and%250Adeployment%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520Although%2520numerous%2520methods%2520for%250Adetecting%2520and%2520mitigating%2520backdoor%2520attacks%2520have%2520been%2520proposed%252C%2520most%2520rely%2520on%250Aidentifying%2520and%2520eliminating%2520the%2520%2560%2560shortcut%2522%2520created%2520by%2520the%2520backdoor%252C%2520which%250Alinks%2520a%2520specific%2520source%2520class%2520to%2520a%2520target%2520class.%2520However%252C%2520these%2520approaches%2520can%250Abe%2520easily%2520circumvented%2520by%2520designing%2520multiple%2520backdoor%2520triggers%2520that%2520create%250Ashortcuts%2520everywhere%2520and%2520therefore%2520nowhere%2520specific.%2520In%2520this%2520study%252C%2520we%2520explore%250Athe%2520concept%2520of%2520Multi-Trigger%2520Backdoor%2520Attacks%2520%2528MTBAs%2529%252C%2520where%2520multiple%250Aadversaries%2520leverage%2520different%2520types%2520of%2520triggers%2520to%2520poison%2520the%2520same%2520dataset.%2520By%250Aproposing%2520and%2520investigating%2520three%2520types%2520of%2520multi-trigger%2520attacks%2520including%250A%255Ctextit%257Bparallel%257D%252C%2520%255Ctextit%257Bsequential%257D%252C%2520and%2520%255Ctextit%257Bhybrid%257D%2520attacks%252C%2520we%250Ademonstrate%2520that%25201%2529%2520multiple%2520triggers%2520can%2520coexist%252C%2520overwrite%252C%2520or%2520cross-activate%250Aone%2520another%252C%2520and%25202%2529%2520MTBAs%2520easily%2520break%2520the%2520prevalent%2520shortcut%2520assumption%250Aunderlying%2520most%2520existing%2520backdoor%2520detection/removal%2520methods%252C%2520rendering%2520them%250Aineffective.%2520Given%2520the%2520security%2520risk%2520posed%2520by%2520MTBAs%252C%2520we%2520have%2520created%2520a%250Amulti-trigger%2520backdoor%2520poisoning%2520dataset%2520to%2520facilitate%2520future%2520research%2520on%250Adetecting%2520and%2520mitigating%2520these%2520attacks%252C%2520and%2520we%2520also%2520discuss%2520potential%2520defense%250Astrategies%2520against%2520MTBAs.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/bboylyg/Multi-Trigger-Backdoor-Attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15295v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shortcuts%20Everywhere%20and%20Nowhere%3A%20Exploring%20Multi-Trigger%20Backdoor%0A%20%20Attacks&entry.906535625=Yige%20Li%20and%20Jiabo%20He%20and%20Hanxun%20Huang%20and%20Jun%20Sun%20and%20Xingjun%20Ma%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Backdoor%20attacks%20have%20become%20a%20significant%20threat%20to%20the%20pre-training%20and%0Adeployment%20of%20deep%20neural%20networks%20%28DNNs%29.%20Although%20numerous%20methods%20for%0Adetecting%20and%20mitigating%20backdoor%20attacks%20have%20been%20proposed%2C%20most%20rely%20on%0Aidentifying%20and%20eliminating%20the%20%60%60shortcut%22%20created%20by%20the%20backdoor%2C%20which%0Alinks%20a%20specific%20source%20class%20to%20a%20target%20class.%20However%2C%20these%20approaches%20can%0Abe%20easily%20circumvented%20by%20designing%20multiple%20backdoor%20triggers%20that%20create%0Ashortcuts%20everywhere%20and%20therefore%20nowhere%20specific.%20In%20this%20study%2C%20we%20explore%0Athe%20concept%20of%20Multi-Trigger%20Backdoor%20Attacks%20%28MTBAs%29%2C%20where%20multiple%0Aadversaries%20leverage%20different%20types%20of%20triggers%20to%20poison%20the%20same%20dataset.%20By%0Aproposing%20and%20investigating%20three%20types%20of%20multi-trigger%20attacks%20including%0A%5Ctextit%7Bparallel%7D%2C%20%5Ctextit%7Bsequential%7D%2C%20and%20%5Ctextit%7Bhybrid%7D%20attacks%2C%20we%0Ademonstrate%20that%201%29%20multiple%20triggers%20can%20coexist%2C%20overwrite%2C%20or%20cross-activate%0Aone%20another%2C%20and%202%29%20MTBAs%20easily%20break%20the%20prevalent%20shortcut%20assumption%0Aunderlying%20most%20existing%20backdoor%20detection/removal%20methods%2C%20rendering%20them%0Aineffective.%20Given%20the%20security%20risk%20posed%20by%20MTBAs%2C%20we%20have%20created%20a%0Amulti-trigger%20backdoor%20poisoning%20dataset%20to%20facilitate%20future%20research%20on%0Adetecting%20and%20mitigating%20these%20attacks%2C%20and%20we%20also%20discuss%20potential%20defense%0Astrategies%20against%20MTBAs.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/bboylyg/Multi-Trigger-Backdoor-Attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15295v4&entry.124074799=Read"},
{"title": "Analyzing the Effect of Embedding Norms and Singular Values to\n  Oversmoothing in Graph Neural Networks", "author": "Dimitrios Kelesis and Dimitris Fotakis and Georgios Paliouras", "abstract": "  In this paper, we study the factors that contribute to the effect of\noversmoothing in deep Graph Neural Networks (GNNs). Specifically, our analysis\nis based on a new metric (Mean Average Squared Distance - $MASED$) to quantify\nthe extent of oversmoothing. We derive layer-wise bounds on $MASED$, which\naggregate to yield global upper and lower distance bounds. Based on this\nquantification of oversmoothing, we further analyze the importance of two\ndifferent properties of the model; namely the norms of the generated node\nembeddings, along with the largest and smallest singular values of the weight\nmatrices. Building on the insights drawn from the theoretical analysis, we show\nthat oversmoothing increases as the number of trainable weight matrices and the\nnumber of adjacency matrices increases. We also use the derived layer-wise\nbounds on $MASED$ to form a proposal for decoupling the number of hops (i.e.,\nadjacency depth) from the number of weight matrices. In particular, we\nintroduce G-Reg, a regularization scheme that increases the bounds, and\ndemonstrate through extensive experiments that by doing so node classification\naccuracy increases, achieving robustness at large depths. We further show that\nby reducing oversmoothing in deep networks, we can achieve better results in\nsome tasks than using shallow ones. Specifically, we experiment with a ``cold\nstart\" scenario, i.e., when there is no feature information for the unlabeled\nnodes. Finally, we show empirically the trade-off between receptive field size\n(i.e., number of weight matrices) and performance, using the $MASED$ bounds.\nThis is achieved by distributing adjacency hops across a small number of\ntrainable layers, avoiding the extremes of under- or over-parameterization of\nthe GNN.\n", "link": "http://arxiv.org/abs/2510.06066v1", "date": "2025-10-07", "relevancy": 2.3612, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4934}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4656}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20the%20Effect%20of%20Embedding%20Norms%20and%20Singular%20Values%20to%0A%20%20Oversmoothing%20in%20Graph%20Neural%20Networks&body=Title%3A%20Analyzing%20the%20Effect%20of%20Embedding%20Norms%20and%20Singular%20Values%20to%0A%20%20Oversmoothing%20in%20Graph%20Neural%20Networks%0AAuthor%3A%20Dimitrios%20Kelesis%20and%20Dimitris%20Fotakis%20and%20Georgios%20Paliouras%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20factors%20that%20contribute%20to%20the%20effect%20of%0Aoversmoothing%20in%20deep%20Graph%20Neural%20Networks%20%28GNNs%29.%20Specifically%2C%20our%20analysis%0Ais%20based%20on%20a%20new%20metric%20%28Mean%20Average%20Squared%20Distance%20-%20%24MASED%24%29%20to%20quantify%0Athe%20extent%20of%20oversmoothing.%20We%20derive%20layer-wise%20bounds%20on%20%24MASED%24%2C%20which%0Aaggregate%20to%20yield%20global%20upper%20and%20lower%20distance%20bounds.%20Based%20on%20this%0Aquantification%20of%20oversmoothing%2C%20we%20further%20analyze%20the%20importance%20of%20two%0Adifferent%20properties%20of%20the%20model%3B%20namely%20the%20norms%20of%20the%20generated%20node%0Aembeddings%2C%20along%20with%20the%20largest%20and%20smallest%20singular%20values%20of%20the%20weight%0Amatrices.%20Building%20on%20the%20insights%20drawn%20from%20the%20theoretical%20analysis%2C%20we%20show%0Athat%20oversmoothing%20increases%20as%20the%20number%20of%20trainable%20weight%20matrices%20and%20the%0Anumber%20of%20adjacency%20matrices%20increases.%20We%20also%20use%20the%20derived%20layer-wise%0Abounds%20on%20%24MASED%24%20to%20form%20a%20proposal%20for%20decoupling%20the%20number%20of%20hops%20%28i.e.%2C%0Aadjacency%20depth%29%20from%20the%20number%20of%20weight%20matrices.%20In%20particular%2C%20we%0Aintroduce%20G-Reg%2C%20a%20regularization%20scheme%20that%20increases%20the%20bounds%2C%20and%0Ademonstrate%20through%20extensive%20experiments%20that%20by%20doing%20so%20node%20classification%0Aaccuracy%20increases%2C%20achieving%20robustness%20at%20large%20depths.%20We%20further%20show%20that%0Aby%20reducing%20oversmoothing%20in%20deep%20networks%2C%20we%20can%20achieve%20better%20results%20in%0Asome%20tasks%20than%20using%20shallow%20ones.%20Specifically%2C%20we%20experiment%20with%20a%20%60%60cold%0Astart%22%20scenario%2C%20i.e.%2C%20when%20there%20is%20no%20feature%20information%20for%20the%20unlabeled%0Anodes.%20Finally%2C%20we%20show%20empirically%20the%20trade-off%20between%20receptive%20field%20size%0A%28i.e.%2C%20number%20of%20weight%20matrices%29%20and%20performance%2C%20using%20the%20%24MASED%24%20bounds.%0AThis%20is%20achieved%20by%20distributing%20adjacency%20hops%20across%20a%20small%20number%20of%0Atrainable%20layers%2C%20avoiding%20the%20extremes%20of%20under-%20or%20over-parameterization%20of%0Athe%20GNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520the%2520Effect%2520of%2520Embedding%2520Norms%2520and%2520Singular%2520Values%2520to%250A%2520%2520Oversmoothing%2520in%2520Graph%2520Neural%2520Networks%26entry.906535625%3DDimitrios%2520Kelesis%2520and%2520Dimitris%2520Fotakis%2520and%2520Georgios%2520Paliouras%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520factors%2520that%2520contribute%2520to%2520the%2520effect%2520of%250Aoversmoothing%2520in%2520deep%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520Specifically%252C%2520our%2520analysis%250Ais%2520based%2520on%2520a%2520new%2520metric%2520%2528Mean%2520Average%2520Squared%2520Distance%2520-%2520%2524MASED%2524%2529%2520to%2520quantify%250Athe%2520extent%2520of%2520oversmoothing.%2520We%2520derive%2520layer-wise%2520bounds%2520on%2520%2524MASED%2524%252C%2520which%250Aaggregate%2520to%2520yield%2520global%2520upper%2520and%2520lower%2520distance%2520bounds.%2520Based%2520on%2520this%250Aquantification%2520of%2520oversmoothing%252C%2520we%2520further%2520analyze%2520the%2520importance%2520of%2520two%250Adifferent%2520properties%2520of%2520the%2520model%253B%2520namely%2520the%2520norms%2520of%2520the%2520generated%2520node%250Aembeddings%252C%2520along%2520with%2520the%2520largest%2520and%2520smallest%2520singular%2520values%2520of%2520the%2520weight%250Amatrices.%2520Building%2520on%2520the%2520insights%2520drawn%2520from%2520the%2520theoretical%2520analysis%252C%2520we%2520show%250Athat%2520oversmoothing%2520increases%2520as%2520the%2520number%2520of%2520trainable%2520weight%2520matrices%2520and%2520the%250Anumber%2520of%2520adjacency%2520matrices%2520increases.%2520We%2520also%2520use%2520the%2520derived%2520layer-wise%250Abounds%2520on%2520%2524MASED%2524%2520to%2520form%2520a%2520proposal%2520for%2520decoupling%2520the%2520number%2520of%2520hops%2520%2528i.e.%252C%250Aadjacency%2520depth%2529%2520from%2520the%2520number%2520of%2520weight%2520matrices.%2520In%2520particular%252C%2520we%250Aintroduce%2520G-Reg%252C%2520a%2520regularization%2520scheme%2520that%2520increases%2520the%2520bounds%252C%2520and%250Ademonstrate%2520through%2520extensive%2520experiments%2520that%2520by%2520doing%2520so%2520node%2520classification%250Aaccuracy%2520increases%252C%2520achieving%2520robustness%2520at%2520large%2520depths.%2520We%2520further%2520show%2520that%250Aby%2520reducing%2520oversmoothing%2520in%2520deep%2520networks%252C%2520we%2520can%2520achieve%2520better%2520results%2520in%250Asome%2520tasks%2520than%2520using%2520shallow%2520ones.%2520Specifically%252C%2520we%2520experiment%2520with%2520a%2520%2560%2560cold%250Astart%2522%2520scenario%252C%2520i.e.%252C%2520when%2520there%2520is%2520no%2520feature%2520information%2520for%2520the%2520unlabeled%250Anodes.%2520Finally%252C%2520we%2520show%2520empirically%2520the%2520trade-off%2520between%2520receptive%2520field%2520size%250A%2528i.e.%252C%2520number%2520of%2520weight%2520matrices%2529%2520and%2520performance%252C%2520using%2520the%2520%2524MASED%2524%2520bounds.%250AThis%2520is%2520achieved%2520by%2520distributing%2520adjacency%2520hops%2520across%2520a%2520small%2520number%2520of%250Atrainable%2520layers%252C%2520avoiding%2520the%2520extremes%2520of%2520under-%2520or%2520over-parameterization%2520of%250Athe%2520GNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20the%20Effect%20of%20Embedding%20Norms%20and%20Singular%20Values%20to%0A%20%20Oversmoothing%20in%20Graph%20Neural%20Networks&entry.906535625=Dimitrios%20Kelesis%20and%20Dimitris%20Fotakis%20and%20Georgios%20Paliouras&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20factors%20that%20contribute%20to%20the%20effect%20of%0Aoversmoothing%20in%20deep%20Graph%20Neural%20Networks%20%28GNNs%29.%20Specifically%2C%20our%20analysis%0Ais%20based%20on%20a%20new%20metric%20%28Mean%20Average%20Squared%20Distance%20-%20%24MASED%24%29%20to%20quantify%0Athe%20extent%20of%20oversmoothing.%20We%20derive%20layer-wise%20bounds%20on%20%24MASED%24%2C%20which%0Aaggregate%20to%20yield%20global%20upper%20and%20lower%20distance%20bounds.%20Based%20on%20this%0Aquantification%20of%20oversmoothing%2C%20we%20further%20analyze%20the%20importance%20of%20two%0Adifferent%20properties%20of%20the%20model%3B%20namely%20the%20norms%20of%20the%20generated%20node%0Aembeddings%2C%20along%20with%20the%20largest%20and%20smallest%20singular%20values%20of%20the%20weight%0Amatrices.%20Building%20on%20the%20insights%20drawn%20from%20the%20theoretical%20analysis%2C%20we%20show%0Athat%20oversmoothing%20increases%20as%20the%20number%20of%20trainable%20weight%20matrices%20and%20the%0Anumber%20of%20adjacency%20matrices%20increases.%20We%20also%20use%20the%20derived%20layer-wise%0Abounds%20on%20%24MASED%24%20to%20form%20a%20proposal%20for%20decoupling%20the%20number%20of%20hops%20%28i.e.%2C%0Aadjacency%20depth%29%20from%20the%20number%20of%20weight%20matrices.%20In%20particular%2C%20we%0Aintroduce%20G-Reg%2C%20a%20regularization%20scheme%20that%20increases%20the%20bounds%2C%20and%0Ademonstrate%20through%20extensive%20experiments%20that%20by%20doing%20so%20node%20classification%0Aaccuracy%20increases%2C%20achieving%20robustness%20at%20large%20depths.%20We%20further%20show%20that%0Aby%20reducing%20oversmoothing%20in%20deep%20networks%2C%20we%20can%20achieve%20better%20results%20in%0Asome%20tasks%20than%20using%20shallow%20ones.%20Specifically%2C%20we%20experiment%20with%20a%20%60%60cold%0Astart%22%20scenario%2C%20i.e.%2C%20when%20there%20is%20no%20feature%20information%20for%20the%20unlabeled%0Anodes.%20Finally%2C%20we%20show%20empirically%20the%20trade-off%20between%20receptive%20field%20size%0A%28i.e.%2C%20number%20of%20weight%20matrices%29%20and%20performance%2C%20using%20the%20%24MASED%24%20bounds.%0AThis%20is%20achieved%20by%20distributing%20adjacency%20hops%20across%20a%20small%20number%20of%0Atrainable%20layers%2C%20avoiding%20the%20extremes%20of%20under-%20or%20over-parameterization%20of%0Athe%20GNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06066v1&entry.124074799=Read"},
{"title": "DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair\n  Manipulation", "author": "Chengyang Zhao and Uksang Yoo and Arkadeep Narayan Chaudhury and Giljoo Nam and Jonathan Francis and Jeffrey Ichnowski and Jean Oh", "abstract": "  Hair care is an essential daily activity, yet it remains inaccessible to\nindividuals with limited mobility and challenging for autonomous robot systems\ndue to the fine-grained physical structure and complex dynamics of hair. In\nthis work, we present DYMO-Hair, a model-based robot hair care system. We\nintroduce a novel dynamics learning paradigm that is suited for volumetric\nquantities such as hair, relying on an action-conditioned latent state editing\nmechanism, coupled with a compact 3D latent space of diverse hairstyles to\nimprove generalizability. This latent space is pre-trained at scale using a\nnovel hair physics simulator, enabling generalization across previously unseen\nhairstyles. Using the dynamics model with a Model Predictive Path Integral\n(MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair\nstyling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model\noutperforms baselines on capturing local deformation for diverse, unseen\nhairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling\ntasks on unseen hairstyles, with an average of 22% lower final geometric error\nand 42% higher success rate than the state-of-the-art system. Real-world\nexperiments exhibit zero-shot transferability of our system to wigs, achieving\nconsistent success on challenging unseen hairstyles where the state-of-the-art\nsystem fails. Together, these results introduce a foundation for model-based\nrobot hair care, advancing toward more generalizable, flexible, and accessible\nrobot hair styling in unconstrained physical environments. More details are\navailable on our project page: https://chengyzhao.github.io/DYMOHair-web/.\n", "link": "http://arxiv.org/abs/2510.06199v1", "date": "2025-10-07", "relevancy": 2.3578, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.638}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DYMO-Hair%3A%20Generalizable%20Volumetric%20Dynamics%20Modeling%20for%20Robot%20Hair%0A%20%20Manipulation&body=Title%3A%20DYMO-Hair%3A%20Generalizable%20Volumetric%20Dynamics%20Modeling%20for%20Robot%20Hair%0A%20%20Manipulation%0AAuthor%3A%20Chengyang%20Zhao%20and%20Uksang%20Yoo%20and%20Arkadeep%20Narayan%20Chaudhury%20and%20Giljoo%20Nam%20and%20Jonathan%20Francis%20and%20Jeffrey%20Ichnowski%20and%20Jean%20Oh%0AAbstract%3A%20%20%20Hair%20care%20is%20an%20essential%20daily%20activity%2C%20yet%20it%20remains%20inaccessible%20to%0Aindividuals%20with%20limited%20mobility%20and%20challenging%20for%20autonomous%20robot%20systems%0Adue%20to%20the%20fine-grained%20physical%20structure%20and%20complex%20dynamics%20of%20hair.%20In%0Athis%20work%2C%20we%20present%20DYMO-Hair%2C%20a%20model-based%20robot%20hair%20care%20system.%20We%0Aintroduce%20a%20novel%20dynamics%20learning%20paradigm%20that%20is%20suited%20for%20volumetric%0Aquantities%20such%20as%20hair%2C%20relying%20on%20an%20action-conditioned%20latent%20state%20editing%0Amechanism%2C%20coupled%20with%20a%20compact%203D%20latent%20space%20of%20diverse%20hairstyles%20to%0Aimprove%20generalizability.%20This%20latent%20space%20is%20pre-trained%20at%20scale%20using%20a%0Anovel%20hair%20physics%20simulator%2C%20enabling%20generalization%20across%20previously%20unseen%0Ahairstyles.%20Using%20the%20dynamics%20model%20with%20a%20Model%20Predictive%20Path%20Integral%0A%28MPPI%29%20planner%2C%20DYMO-Hair%20is%20able%20to%20perform%20visual%20goal-conditioned%20hair%0Astyling.%20Experiments%20in%20simulation%20demonstrate%20that%20DYMO-Hair%27s%20dynamics%20model%0Aoutperforms%20baselines%20on%20capturing%20local%20deformation%20for%20diverse%2C%20unseen%0Ahairstyles.%20DYMO-Hair%20further%20outperforms%20baselines%20in%20closed-loop%20hair%20styling%0Atasks%20on%20unseen%20hairstyles%2C%20with%20an%20average%20of%2022%25%20lower%20final%20geometric%20error%0Aand%2042%25%20higher%20success%20rate%20than%20the%20state-of-the-art%20system.%20Real-world%0Aexperiments%20exhibit%20zero-shot%20transferability%20of%20our%20system%20to%20wigs%2C%20achieving%0Aconsistent%20success%20on%20challenging%20unseen%20hairstyles%20where%20the%20state-of-the-art%0Asystem%20fails.%20Together%2C%20these%20results%20introduce%20a%20foundation%20for%20model-based%0Arobot%20hair%20care%2C%20advancing%20toward%20more%20generalizable%2C%20flexible%2C%20and%20accessible%0Arobot%20hair%20styling%20in%20unconstrained%20physical%20environments.%20More%20details%20are%0Aavailable%20on%20our%20project%20page%3A%20https%3A//chengyzhao.github.io/DYMOHair-web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDYMO-Hair%253A%2520Generalizable%2520Volumetric%2520Dynamics%2520Modeling%2520for%2520Robot%2520Hair%250A%2520%2520Manipulation%26entry.906535625%3DChengyang%2520Zhao%2520and%2520Uksang%2520Yoo%2520and%2520Arkadeep%2520Narayan%2520Chaudhury%2520and%2520Giljoo%2520Nam%2520and%2520Jonathan%2520Francis%2520and%2520Jeffrey%2520Ichnowski%2520and%2520Jean%2520Oh%26entry.1292438233%3D%2520%2520Hair%2520care%2520is%2520an%2520essential%2520daily%2520activity%252C%2520yet%2520it%2520remains%2520inaccessible%2520to%250Aindividuals%2520with%2520limited%2520mobility%2520and%2520challenging%2520for%2520autonomous%2520robot%2520systems%250Adue%2520to%2520the%2520fine-grained%2520physical%2520structure%2520and%2520complex%2520dynamics%2520of%2520hair.%2520In%250Athis%2520work%252C%2520we%2520present%2520DYMO-Hair%252C%2520a%2520model-based%2520robot%2520hair%2520care%2520system.%2520We%250Aintroduce%2520a%2520novel%2520dynamics%2520learning%2520paradigm%2520that%2520is%2520suited%2520for%2520volumetric%250Aquantities%2520such%2520as%2520hair%252C%2520relying%2520on%2520an%2520action-conditioned%2520latent%2520state%2520editing%250Amechanism%252C%2520coupled%2520with%2520a%2520compact%25203D%2520latent%2520space%2520of%2520diverse%2520hairstyles%2520to%250Aimprove%2520generalizability.%2520This%2520latent%2520space%2520is%2520pre-trained%2520at%2520scale%2520using%2520a%250Anovel%2520hair%2520physics%2520simulator%252C%2520enabling%2520generalization%2520across%2520previously%2520unseen%250Ahairstyles.%2520Using%2520the%2520dynamics%2520model%2520with%2520a%2520Model%2520Predictive%2520Path%2520Integral%250A%2528MPPI%2529%2520planner%252C%2520DYMO-Hair%2520is%2520able%2520to%2520perform%2520visual%2520goal-conditioned%2520hair%250Astyling.%2520Experiments%2520in%2520simulation%2520demonstrate%2520that%2520DYMO-Hair%2527s%2520dynamics%2520model%250Aoutperforms%2520baselines%2520on%2520capturing%2520local%2520deformation%2520for%2520diverse%252C%2520unseen%250Ahairstyles.%2520DYMO-Hair%2520further%2520outperforms%2520baselines%2520in%2520closed-loop%2520hair%2520styling%250Atasks%2520on%2520unseen%2520hairstyles%252C%2520with%2520an%2520average%2520of%252022%2525%2520lower%2520final%2520geometric%2520error%250Aand%252042%2525%2520higher%2520success%2520rate%2520than%2520the%2520state-of-the-art%2520system.%2520Real-world%250Aexperiments%2520exhibit%2520zero-shot%2520transferability%2520of%2520our%2520system%2520to%2520wigs%252C%2520achieving%250Aconsistent%2520success%2520on%2520challenging%2520unseen%2520hairstyles%2520where%2520the%2520state-of-the-art%250Asystem%2520fails.%2520Together%252C%2520these%2520results%2520introduce%2520a%2520foundation%2520for%2520model-based%250Arobot%2520hair%2520care%252C%2520advancing%2520toward%2520more%2520generalizable%252C%2520flexible%252C%2520and%2520accessible%250Arobot%2520hair%2520styling%2520in%2520unconstrained%2520physical%2520environments.%2520More%2520details%2520are%250Aavailable%2520on%2520our%2520project%2520page%253A%2520https%253A//chengyzhao.github.io/DYMOHair-web/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DYMO-Hair%3A%20Generalizable%20Volumetric%20Dynamics%20Modeling%20for%20Robot%20Hair%0A%20%20Manipulation&entry.906535625=Chengyang%20Zhao%20and%20Uksang%20Yoo%20and%20Arkadeep%20Narayan%20Chaudhury%20and%20Giljoo%20Nam%20and%20Jonathan%20Francis%20and%20Jeffrey%20Ichnowski%20and%20Jean%20Oh&entry.1292438233=%20%20Hair%20care%20is%20an%20essential%20daily%20activity%2C%20yet%20it%20remains%20inaccessible%20to%0Aindividuals%20with%20limited%20mobility%20and%20challenging%20for%20autonomous%20robot%20systems%0Adue%20to%20the%20fine-grained%20physical%20structure%20and%20complex%20dynamics%20of%20hair.%20In%0Athis%20work%2C%20we%20present%20DYMO-Hair%2C%20a%20model-based%20robot%20hair%20care%20system.%20We%0Aintroduce%20a%20novel%20dynamics%20learning%20paradigm%20that%20is%20suited%20for%20volumetric%0Aquantities%20such%20as%20hair%2C%20relying%20on%20an%20action-conditioned%20latent%20state%20editing%0Amechanism%2C%20coupled%20with%20a%20compact%203D%20latent%20space%20of%20diverse%20hairstyles%20to%0Aimprove%20generalizability.%20This%20latent%20space%20is%20pre-trained%20at%20scale%20using%20a%0Anovel%20hair%20physics%20simulator%2C%20enabling%20generalization%20across%20previously%20unseen%0Ahairstyles.%20Using%20the%20dynamics%20model%20with%20a%20Model%20Predictive%20Path%20Integral%0A%28MPPI%29%20planner%2C%20DYMO-Hair%20is%20able%20to%20perform%20visual%20goal-conditioned%20hair%0Astyling.%20Experiments%20in%20simulation%20demonstrate%20that%20DYMO-Hair%27s%20dynamics%20model%0Aoutperforms%20baselines%20on%20capturing%20local%20deformation%20for%20diverse%2C%20unseen%0Ahairstyles.%20DYMO-Hair%20further%20outperforms%20baselines%20in%20closed-loop%20hair%20styling%0Atasks%20on%20unseen%20hairstyles%2C%20with%20an%20average%20of%2022%25%20lower%20final%20geometric%20error%0Aand%2042%25%20higher%20success%20rate%20than%20the%20state-of-the-art%20system.%20Real-world%0Aexperiments%20exhibit%20zero-shot%20transferability%20of%20our%20system%20to%20wigs%2C%20achieving%0Aconsistent%20success%20on%20challenging%20unseen%20hairstyles%20where%20the%20state-of-the-art%0Asystem%20fails.%20Together%2C%20these%20results%20introduce%20a%20foundation%20for%20model-based%0Arobot%20hair%20care%2C%20advancing%20toward%20more%20generalizable%2C%20flexible%2C%20and%20accessible%0Arobot%20hair%20styling%20in%20unconstrained%20physical%20environments.%20More%20details%20are%0Aavailable%20on%20our%20project%20page%3A%20https%3A//chengyzhao.github.io/DYMOHair-web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06199v1&entry.124074799=Read"},
{"title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models", "author": "Jiahao Wang and Zhenpei Yang and Yijing Bai and Yingwei Li and Yuliang Zou and Bo Sun and Abhijit Kundu and Jose Lezama and Luna Yue Huang and Zehao Zhu and Jyh-Jing Hwang and Dragomir Anguelov and Mingxing Tan and Chiyu Max Jiang", "abstract": "  Recent advances in generative models have sparked exciting new possibilities\nin the field of autonomous vehicles. Specifically, video generation models are\nnow being explored as controllable virtual testing environments.\nSimultaneously, end-to-end (E2E) driving models have emerged as a streamlined\nalternative to conventional modular autonomous driving systems, gaining\npopularity for their simplicity and scalability. However, the application of\nthese techniques to simulation and planning raises important questions. First,\nwhile video generation models can generate increasingly realistic videos, can\nthese videos faithfully adhere to the specified conditions and be realistic\nenough for E2E autonomous planner evaluation? Second, given that data is\ncrucial for understanding and controlling E2E planners, how can we gain deeper\ninsights into their biases and improve their ability to generalize to\nout-of-distribution scenarios? In this work, we bridge the gap between the\ndriving models and generative world models (Drive&Gen) to address these\nquestions. We propose novel statistical measures leveraging E2E drivers to\nevaluate the realism of generated videos. By exploiting the controllability of\nthe video generation model, we conduct targeted experiments to investigate\ndistribution gaps affecting E2E planner performance. Finally, we show that\nsynthetic data produced by the video generation model offers a cost-effective\nalternative to real-world data collection. This synthetic data effectively\nimproves E2E model generalization beyond existing Operational Design Domains,\nfacilitating the expansion of autonomous vehicle services into new operational\ncontexts.\n", "link": "http://arxiv.org/abs/2510.06209v1", "date": "2025-10-07", "relevancy": 2.3514, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5958}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5851}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drive%26Gen%3A%20Co-Evaluating%20End-to-End%20Driving%20and%20Video%20Generation%20Models&body=Title%3A%20Drive%26Gen%3A%20Co-Evaluating%20End-to-End%20Driving%20and%20Video%20Generation%20Models%0AAuthor%3A%20Jiahao%20Wang%20and%20Zhenpei%20Yang%20and%20Yijing%20Bai%20and%20Yingwei%20Li%20and%20Yuliang%20Zou%20and%20Bo%20Sun%20and%20Abhijit%20Kundu%20and%20Jose%20Lezama%20and%20Luna%20Yue%20Huang%20and%20Zehao%20Zhu%20and%20Jyh-Jing%20Hwang%20and%20Dragomir%20Anguelov%20and%20Mingxing%20Tan%20and%20Chiyu%20Max%20Jiang%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20models%20have%20sparked%20exciting%20new%20possibilities%0Ain%20the%20field%20of%20autonomous%20vehicles.%20Specifically%2C%20video%20generation%20models%20are%0Anow%20being%20explored%20as%20controllable%20virtual%20testing%20environments.%0ASimultaneously%2C%20end-to-end%20%28E2E%29%20driving%20models%20have%20emerged%20as%20a%20streamlined%0Aalternative%20to%20conventional%20modular%20autonomous%20driving%20systems%2C%20gaining%0Apopularity%20for%20their%20simplicity%20and%20scalability.%20However%2C%20the%20application%20of%0Athese%20techniques%20to%20simulation%20and%20planning%20raises%20important%20questions.%20First%2C%0Awhile%20video%20generation%20models%20can%20generate%20increasingly%20realistic%20videos%2C%20can%0Athese%20videos%20faithfully%20adhere%20to%20the%20specified%20conditions%20and%20be%20realistic%0Aenough%20for%20E2E%20autonomous%20planner%20evaluation%3F%20Second%2C%20given%20that%20data%20is%0Acrucial%20for%20understanding%20and%20controlling%20E2E%20planners%2C%20how%20can%20we%20gain%20deeper%0Ainsights%20into%20their%20biases%20and%20improve%20their%20ability%20to%20generalize%20to%0Aout-of-distribution%20scenarios%3F%20In%20this%20work%2C%20we%20bridge%20the%20gap%20between%20the%0Adriving%20models%20and%20generative%20world%20models%20%28Drive%26Gen%29%20to%20address%20these%0Aquestions.%20We%20propose%20novel%20statistical%20measures%20leveraging%20E2E%20drivers%20to%0Aevaluate%20the%20realism%20of%20generated%20videos.%20By%20exploiting%20the%20controllability%20of%0Athe%20video%20generation%20model%2C%20we%20conduct%20targeted%20experiments%20to%20investigate%0Adistribution%20gaps%20affecting%20E2E%20planner%20performance.%20Finally%2C%20we%20show%20that%0Asynthetic%20data%20produced%20by%20the%20video%20generation%20model%20offers%20a%20cost-effective%0Aalternative%20to%20real-world%20data%20collection.%20This%20synthetic%20data%20effectively%0Aimproves%20E2E%20model%20generalization%20beyond%20existing%20Operational%20Design%20Domains%2C%0Afacilitating%20the%20expansion%20of%20autonomous%20vehicle%20services%20into%20new%20operational%0Acontexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrive%2526Gen%253A%2520Co-Evaluating%2520End-to-End%2520Driving%2520and%2520Video%2520Generation%2520Models%26entry.906535625%3DJiahao%2520Wang%2520and%2520Zhenpei%2520Yang%2520and%2520Yijing%2520Bai%2520and%2520Yingwei%2520Li%2520and%2520Yuliang%2520Zou%2520and%2520Bo%2520Sun%2520and%2520Abhijit%2520Kundu%2520and%2520Jose%2520Lezama%2520and%2520Luna%2520Yue%2520Huang%2520and%2520Zehao%2520Zhu%2520and%2520Jyh-Jing%2520Hwang%2520and%2520Dragomir%2520Anguelov%2520and%2520Mingxing%2520Tan%2520and%2520Chiyu%2520Max%2520Jiang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520models%2520have%2520sparked%2520exciting%2520new%2520possibilities%250Ain%2520the%2520field%2520of%2520autonomous%2520vehicles.%2520Specifically%252C%2520video%2520generation%2520models%2520are%250Anow%2520being%2520explored%2520as%2520controllable%2520virtual%2520testing%2520environments.%250ASimultaneously%252C%2520end-to-end%2520%2528E2E%2529%2520driving%2520models%2520have%2520emerged%2520as%2520a%2520streamlined%250Aalternative%2520to%2520conventional%2520modular%2520autonomous%2520driving%2520systems%252C%2520gaining%250Apopularity%2520for%2520their%2520simplicity%2520and%2520scalability.%2520However%252C%2520the%2520application%2520of%250Athese%2520techniques%2520to%2520simulation%2520and%2520planning%2520raises%2520important%2520questions.%2520First%252C%250Awhile%2520video%2520generation%2520models%2520can%2520generate%2520increasingly%2520realistic%2520videos%252C%2520can%250Athese%2520videos%2520faithfully%2520adhere%2520to%2520the%2520specified%2520conditions%2520and%2520be%2520realistic%250Aenough%2520for%2520E2E%2520autonomous%2520planner%2520evaluation%253F%2520Second%252C%2520given%2520that%2520data%2520is%250Acrucial%2520for%2520understanding%2520and%2520controlling%2520E2E%2520planners%252C%2520how%2520can%2520we%2520gain%2520deeper%250Ainsights%2520into%2520their%2520biases%2520and%2520improve%2520their%2520ability%2520to%2520generalize%2520to%250Aout-of-distribution%2520scenarios%253F%2520In%2520this%2520work%252C%2520we%2520bridge%2520the%2520gap%2520between%2520the%250Adriving%2520models%2520and%2520generative%2520world%2520models%2520%2528Drive%2526Gen%2529%2520to%2520address%2520these%250Aquestions.%2520We%2520propose%2520novel%2520statistical%2520measures%2520leveraging%2520E2E%2520drivers%2520to%250Aevaluate%2520the%2520realism%2520of%2520generated%2520videos.%2520By%2520exploiting%2520the%2520controllability%2520of%250Athe%2520video%2520generation%2520model%252C%2520we%2520conduct%2520targeted%2520experiments%2520to%2520investigate%250Adistribution%2520gaps%2520affecting%2520E2E%2520planner%2520performance.%2520Finally%252C%2520we%2520show%2520that%250Asynthetic%2520data%2520produced%2520by%2520the%2520video%2520generation%2520model%2520offers%2520a%2520cost-effective%250Aalternative%2520to%2520real-world%2520data%2520collection.%2520This%2520synthetic%2520data%2520effectively%250Aimproves%2520E2E%2520model%2520generalization%2520beyond%2520existing%2520Operational%2520Design%2520Domains%252C%250Afacilitating%2520the%2520expansion%2520of%2520autonomous%2520vehicle%2520services%2520into%2520new%2520operational%250Acontexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drive%26Gen%3A%20Co-Evaluating%20End-to-End%20Driving%20and%20Video%20Generation%20Models&entry.906535625=Jiahao%20Wang%20and%20Zhenpei%20Yang%20and%20Yijing%20Bai%20and%20Yingwei%20Li%20and%20Yuliang%20Zou%20and%20Bo%20Sun%20and%20Abhijit%20Kundu%20and%20Jose%20Lezama%20and%20Luna%20Yue%20Huang%20and%20Zehao%20Zhu%20and%20Jyh-Jing%20Hwang%20and%20Dragomir%20Anguelov%20and%20Mingxing%20Tan%20and%20Chiyu%20Max%20Jiang&entry.1292438233=%20%20Recent%20advances%20in%20generative%20models%20have%20sparked%20exciting%20new%20possibilities%0Ain%20the%20field%20of%20autonomous%20vehicles.%20Specifically%2C%20video%20generation%20models%20are%0Anow%20being%20explored%20as%20controllable%20virtual%20testing%20environments.%0ASimultaneously%2C%20end-to-end%20%28E2E%29%20driving%20models%20have%20emerged%20as%20a%20streamlined%0Aalternative%20to%20conventional%20modular%20autonomous%20driving%20systems%2C%20gaining%0Apopularity%20for%20their%20simplicity%20and%20scalability.%20However%2C%20the%20application%20of%0Athese%20techniques%20to%20simulation%20and%20planning%20raises%20important%20questions.%20First%2C%0Awhile%20video%20generation%20models%20can%20generate%20increasingly%20realistic%20videos%2C%20can%0Athese%20videos%20faithfully%20adhere%20to%20the%20specified%20conditions%20and%20be%20realistic%0Aenough%20for%20E2E%20autonomous%20planner%20evaluation%3F%20Second%2C%20given%20that%20data%20is%0Acrucial%20for%20understanding%20and%20controlling%20E2E%20planners%2C%20how%20can%20we%20gain%20deeper%0Ainsights%20into%20their%20biases%20and%20improve%20their%20ability%20to%20generalize%20to%0Aout-of-distribution%20scenarios%3F%20In%20this%20work%2C%20we%20bridge%20the%20gap%20between%20the%0Adriving%20models%20and%20generative%20world%20models%20%28Drive%26Gen%29%20to%20address%20these%0Aquestions.%20We%20propose%20novel%20statistical%20measures%20leveraging%20E2E%20drivers%20to%0Aevaluate%20the%20realism%20of%20generated%20videos.%20By%20exploiting%20the%20controllability%20of%0Athe%20video%20generation%20model%2C%20we%20conduct%20targeted%20experiments%20to%20investigate%0Adistribution%20gaps%20affecting%20E2E%20planner%20performance.%20Finally%2C%20we%20show%20that%0Asynthetic%20data%20produced%20by%20the%20video%20generation%20model%20offers%20a%20cost-effective%0Aalternative%20to%20real-world%20data%20collection.%20This%20synthetic%20data%20effectively%0Aimproves%20E2E%20model%20generalization%20beyond%20existing%20Operational%20Design%20Domains%2C%0Afacilitating%20the%20expansion%20of%20autonomous%20vehicle%20services%20into%20new%20operational%0Acontexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06209v1&entry.124074799=Read"},
{"title": "Influence Functions for Efficient Data Selection in Reasoning", "author": "Prateek Humane and Paolo Cudrano and Daniel Z. Kaplan and Matteo Matteucci and Supriyo Chakraborty and Irina Rish", "abstract": "  Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows\nthat a small amount of high-quality data can outperform massive datasets. Yet,\nwhat constitutes \"quality\" remains ill-defined. Existing reasoning methods rely\non indirect heuristics such as problem difficulty or trace length, while\ninstruction-tuning has explored a broader range of automated selection\nstrategies, but rarely in the context of reasoning. We propose to define\nreasoning data quality using influence functions, which measure the causal\neffect of individual CoT examples on downstream accuracy, and introduce\ninfluence-based pruning, which consistently outperforms perplexity and\nembedding-based baselines on math reasoning within a model family.\n", "link": "http://arxiv.org/abs/2510.06108v1", "date": "2025-10-07", "relevancy": 2.3365, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Influence%20Functions%20for%20Efficient%20Data%20Selection%20in%20Reasoning&body=Title%3A%20Influence%20Functions%20for%20Efficient%20Data%20Selection%20in%20Reasoning%0AAuthor%3A%20Prateek%20Humane%20and%20Paolo%20Cudrano%20and%20Daniel%20Z.%20Kaplan%20and%20Matteo%20Matteucci%20and%20Supriyo%20Chakraborty%20and%20Irina%20Rish%0AAbstract%3A%20%20%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20on%20chain-of-thought%20%28CoT%29%20data%20shows%0Athat%20a%20small%20amount%20of%20high-quality%20data%20can%20outperform%20massive%20datasets.%20Yet%2C%0Awhat%20constitutes%20%22quality%22%20remains%20ill-defined.%20Existing%20reasoning%20methods%20rely%0Aon%20indirect%20heuristics%20such%20as%20problem%20difficulty%20or%20trace%20length%2C%20while%0Ainstruction-tuning%20has%20explored%20a%20broader%20range%20of%20automated%20selection%0Astrategies%2C%20but%20rarely%20in%20the%20context%20of%20reasoning.%20We%20propose%20to%20define%0Areasoning%20data%20quality%20using%20influence%20functions%2C%20which%20measure%20the%20causal%0Aeffect%20of%20individual%20CoT%20examples%20on%20downstream%20accuracy%2C%20and%20introduce%0Ainfluence-based%20pruning%2C%20which%20consistently%20outperforms%20perplexity%20and%0Aembedding-based%20baselines%20on%20math%20reasoning%20within%20a%20model%20family.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfluence%2520Functions%2520for%2520Efficient%2520Data%2520Selection%2520in%2520Reasoning%26entry.906535625%3DPrateek%2520Humane%2520and%2520Paolo%2520Cudrano%2520and%2520Daniel%2520Z.%2520Kaplan%2520and%2520Matteo%2520Matteucci%2520and%2520Supriyo%2520Chakraborty%2520and%2520Irina%2520Rish%26entry.1292438233%3D%2520%2520Fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520chain-of-thought%2520%2528CoT%2529%2520data%2520shows%250Athat%2520a%2520small%2520amount%2520of%2520high-quality%2520data%2520can%2520outperform%2520massive%2520datasets.%2520Yet%252C%250Awhat%2520constitutes%2520%2522quality%2522%2520remains%2520ill-defined.%2520Existing%2520reasoning%2520methods%2520rely%250Aon%2520indirect%2520heuristics%2520such%2520as%2520problem%2520difficulty%2520or%2520trace%2520length%252C%2520while%250Ainstruction-tuning%2520has%2520explored%2520a%2520broader%2520range%2520of%2520automated%2520selection%250Astrategies%252C%2520but%2520rarely%2520in%2520the%2520context%2520of%2520reasoning.%2520We%2520propose%2520to%2520define%250Areasoning%2520data%2520quality%2520using%2520influence%2520functions%252C%2520which%2520measure%2520the%2520causal%250Aeffect%2520of%2520individual%2520CoT%2520examples%2520on%2520downstream%2520accuracy%252C%2520and%2520introduce%250Ainfluence-based%2520pruning%252C%2520which%2520consistently%2520outperforms%2520perplexity%2520and%250Aembedding-based%2520baselines%2520on%2520math%2520reasoning%2520within%2520a%2520model%2520family.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influence%20Functions%20for%20Efficient%20Data%20Selection%20in%20Reasoning&entry.906535625=Prateek%20Humane%20and%20Paolo%20Cudrano%20and%20Daniel%20Z.%20Kaplan%20and%20Matteo%20Matteucci%20and%20Supriyo%20Chakraborty%20and%20Irina%20Rish&entry.1292438233=%20%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20on%20chain-of-thought%20%28CoT%29%20data%20shows%0Athat%20a%20small%20amount%20of%20high-quality%20data%20can%20outperform%20massive%20datasets.%20Yet%2C%0Awhat%20constitutes%20%22quality%22%20remains%20ill-defined.%20Existing%20reasoning%20methods%20rely%0Aon%20indirect%20heuristics%20such%20as%20problem%20difficulty%20or%20trace%20length%2C%20while%0Ainstruction-tuning%20has%20explored%20a%20broader%20range%20of%20automated%20selection%0Astrategies%2C%20but%20rarely%20in%20the%20context%20of%20reasoning.%20We%20propose%20to%20define%0Areasoning%20data%20quality%20using%20influence%20functions%2C%20which%20measure%20the%20causal%0Aeffect%20of%20individual%20CoT%20examples%20on%20downstream%20accuracy%2C%20and%20introduce%0Ainfluence-based%20pruning%2C%20which%20consistently%20outperforms%20perplexity%20and%0Aembedding-based%20baselines%20on%20math%20reasoning%20within%20a%20model%20family.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06108v1&entry.124074799=Read"},
{"title": "Are Heterogeneous Graph Neural Networks Truly Effective? A Causal\n  Perspective", "author": "Xiao Yang and Xuejiao Zhao and Zhiqi Shen", "abstract": "  Graph neural networks (GNNs) have achieved remarkable success in node\nclassification. Building on this progress, heterogeneous graph neural networks\n(HGNNs) integrate relation types and node and edge semantics to leverage\nheterogeneous information. Causal analysis for HGNNs is advancing rapidly,\naiming to separate genuine causal effects from spurious correlations. However,\nwhether HGNNs are intrinsically effective remains underexamined, and most\nstudies implicitly assume rather than establish this effectiveness. In this\nwork, we examine HGNNs from two perspectives: model architecture and\nheterogeneous information. We conduct a systematic reproduction across 21\ndatasets and 20 baselines, complemented by comprehensive hyperparameter\nretuning. To further disentangle the source of performance gains, we develop a\ncausal effect estimation framework that constructs and evaluates candidate\nfactors under standard assumptions through factual and counterfactual analyses,\nwith robustness validated via minimal sufficient adjustment sets, cross-method\nconsistency checks, and sensitivity analyses. Our results lead to two\nconclusions. First, model architecture and complexity have no causal effect on\nperformance. Second, heterogeneous information exerts a positive causal effect\nby increasing homophily and local-global distribution discrepancy, which makes\nnode classes more distinguishable. The implementation is publicly available at\nhttps://github.com/YXNTU/CausalHGNN.\n", "link": "http://arxiv.org/abs/2510.05750v1", "date": "2025-10-07", "relevancy": 2.3345, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4821}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4672}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Heterogeneous%20Graph%20Neural%20Networks%20Truly%20Effective%3F%20A%20Causal%0A%20%20Perspective&body=Title%3A%20Are%20Heterogeneous%20Graph%20Neural%20Networks%20Truly%20Effective%3F%20A%20Causal%0A%20%20Perspective%0AAuthor%3A%20Xiao%20Yang%20and%20Xuejiao%20Zhao%20and%20Zhiqi%20Shen%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20remarkable%20success%20in%20node%0Aclassification.%20Building%20on%20this%20progress%2C%20heterogeneous%20graph%20neural%20networks%0A%28HGNNs%29%20integrate%20relation%20types%20and%20node%20and%20edge%20semantics%20to%20leverage%0Aheterogeneous%20information.%20Causal%20analysis%20for%20HGNNs%20is%20advancing%20rapidly%2C%0Aaiming%20to%20separate%20genuine%20causal%20effects%20from%20spurious%20correlations.%20However%2C%0Awhether%20HGNNs%20are%20intrinsically%20effective%20remains%20underexamined%2C%20and%20most%0Astudies%20implicitly%20assume%20rather%20than%20establish%20this%20effectiveness.%20In%20this%0Awork%2C%20we%20examine%20HGNNs%20from%20two%20perspectives%3A%20model%20architecture%20and%0Aheterogeneous%20information.%20We%20conduct%20a%20systematic%20reproduction%20across%2021%0Adatasets%20and%2020%20baselines%2C%20complemented%20by%20comprehensive%20hyperparameter%0Aretuning.%20To%20further%20disentangle%20the%20source%20of%20performance%20gains%2C%20we%20develop%20a%0Acausal%20effect%20estimation%20framework%20that%20constructs%20and%20evaluates%20candidate%0Afactors%20under%20standard%20assumptions%20through%20factual%20and%20counterfactual%20analyses%2C%0Awith%20robustness%20validated%20via%20minimal%20sufficient%20adjustment%20sets%2C%20cross-method%0Aconsistency%20checks%2C%20and%20sensitivity%20analyses.%20Our%20results%20lead%20to%20two%0Aconclusions.%20First%2C%20model%20architecture%20and%20complexity%20have%20no%20causal%20effect%20on%0Aperformance.%20Second%2C%20heterogeneous%20information%20exerts%20a%20positive%20causal%20effect%0Aby%20increasing%20homophily%20and%20local-global%20distribution%20discrepancy%2C%20which%20makes%0Anode%20classes%20more%20distinguishable.%20The%20implementation%20is%20publicly%20available%20at%0Ahttps%3A//github.com/YXNTU/CausalHGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Heterogeneous%2520Graph%2520Neural%2520Networks%2520Truly%2520Effective%253F%2520A%2520Causal%250A%2520%2520Perspective%26entry.906535625%3DXiao%2520Yang%2520and%2520Xuejiao%2520Zhao%2520and%2520Zhiqi%2520Shen%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520node%250Aclassification.%2520Building%2520on%2520this%2520progress%252C%2520heterogeneous%2520graph%2520neural%2520networks%250A%2528HGNNs%2529%2520integrate%2520relation%2520types%2520and%2520node%2520and%2520edge%2520semantics%2520to%2520leverage%250Aheterogeneous%2520information.%2520Causal%2520analysis%2520for%2520HGNNs%2520is%2520advancing%2520rapidly%252C%250Aaiming%2520to%2520separate%2520genuine%2520causal%2520effects%2520from%2520spurious%2520correlations.%2520However%252C%250Awhether%2520HGNNs%2520are%2520intrinsically%2520effective%2520remains%2520underexamined%252C%2520and%2520most%250Astudies%2520implicitly%2520assume%2520rather%2520than%2520establish%2520this%2520effectiveness.%2520In%2520this%250Awork%252C%2520we%2520examine%2520HGNNs%2520from%2520two%2520perspectives%253A%2520model%2520architecture%2520and%250Aheterogeneous%2520information.%2520We%2520conduct%2520a%2520systematic%2520reproduction%2520across%252021%250Adatasets%2520and%252020%2520baselines%252C%2520complemented%2520by%2520comprehensive%2520hyperparameter%250Aretuning.%2520To%2520further%2520disentangle%2520the%2520source%2520of%2520performance%2520gains%252C%2520we%2520develop%2520a%250Acausal%2520effect%2520estimation%2520framework%2520that%2520constructs%2520and%2520evaluates%2520candidate%250Afactors%2520under%2520standard%2520assumptions%2520through%2520factual%2520and%2520counterfactual%2520analyses%252C%250Awith%2520robustness%2520validated%2520via%2520minimal%2520sufficient%2520adjustment%2520sets%252C%2520cross-method%250Aconsistency%2520checks%252C%2520and%2520sensitivity%2520analyses.%2520Our%2520results%2520lead%2520to%2520two%250Aconclusions.%2520First%252C%2520model%2520architecture%2520and%2520complexity%2520have%2520no%2520causal%2520effect%2520on%250Aperformance.%2520Second%252C%2520heterogeneous%2520information%2520exerts%2520a%2520positive%2520causal%2520effect%250Aby%2520increasing%2520homophily%2520and%2520local-global%2520distribution%2520discrepancy%252C%2520which%2520makes%250Anode%2520classes%2520more%2520distinguishable.%2520The%2520implementation%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/YXNTU/CausalHGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Heterogeneous%20Graph%20Neural%20Networks%20Truly%20Effective%3F%20A%20Causal%0A%20%20Perspective&entry.906535625=Xiao%20Yang%20and%20Xuejiao%20Zhao%20and%20Zhiqi%20Shen&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20remarkable%20success%20in%20node%0Aclassification.%20Building%20on%20this%20progress%2C%20heterogeneous%20graph%20neural%20networks%0A%28HGNNs%29%20integrate%20relation%20types%20and%20node%20and%20edge%20semantics%20to%20leverage%0Aheterogeneous%20information.%20Causal%20analysis%20for%20HGNNs%20is%20advancing%20rapidly%2C%0Aaiming%20to%20separate%20genuine%20causal%20effects%20from%20spurious%20correlations.%20However%2C%0Awhether%20HGNNs%20are%20intrinsically%20effective%20remains%20underexamined%2C%20and%20most%0Astudies%20implicitly%20assume%20rather%20than%20establish%20this%20effectiveness.%20In%20this%0Awork%2C%20we%20examine%20HGNNs%20from%20two%20perspectives%3A%20model%20architecture%20and%0Aheterogeneous%20information.%20We%20conduct%20a%20systematic%20reproduction%20across%2021%0Adatasets%20and%2020%20baselines%2C%20complemented%20by%20comprehensive%20hyperparameter%0Aretuning.%20To%20further%20disentangle%20the%20source%20of%20performance%20gains%2C%20we%20develop%20a%0Acausal%20effect%20estimation%20framework%20that%20constructs%20and%20evaluates%20candidate%0Afactors%20under%20standard%20assumptions%20through%20factual%20and%20counterfactual%20analyses%2C%0Awith%20robustness%20validated%20via%20minimal%20sufficient%20adjustment%20sets%2C%20cross-method%0Aconsistency%20checks%2C%20and%20sensitivity%20analyses.%20Our%20results%20lead%20to%20two%0Aconclusions.%20First%2C%20model%20architecture%20and%20complexity%20have%20no%20causal%20effect%20on%0Aperformance.%20Second%2C%20heterogeneous%20information%20exerts%20a%20positive%20causal%20effect%0Aby%20increasing%20homophily%20and%20local-global%20distribution%20discrepancy%2C%20which%20makes%0Anode%20classes%20more%20distinguishable.%20The%20implementation%20is%20publicly%20available%20at%0Ahttps%3A//github.com/YXNTU/CausalHGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05750v1&entry.124074799=Read"},
{"title": "ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous\n  Driving", "author": "Yongxuan Lyu and Guangfeng Jiang and Hongsi Liu and Jun Liu", "abstract": "  The manual annotation of outdoor LiDAR point clouds for instance segmentation\nis extremely costly and time-consuming. Current methods attempt to reduce this\nburden but still rely on some form of human labeling. To completely eliminate\nthis dependency, we introduce ALISE, a novel framework that performs LiDAR\ninstance segmentation without any annotations. The central challenge is to\ngenerate high-quality pseudo-labels in a fully unsupervised manner. Our\napproach starts by employing Vision Foundation Models (VFMs), guided by text\nand images, to produce initial pseudo-labels. We then refine these labels\nthrough a dedicated spatio-temporal voting module, which combines 2D and 3D\nsemantics for both offline and online optimization. To achieve superior feature\nlearning, we further introduce two forms of semantic supervision: a set of 2D\nprior-based losses that inject visual knowledge into the 3D network, and a\nnovel prototype-based contrastive loss that builds a discriminative feature\nspace by exploiting 3D semantic consistency. This comprehensive design results\nin significant performance gains, establishing a new state-of-the-art for\nunsupervised 3D instance segmentation. Remarkably, our approach even\noutperforms MWSIS, a method that operates with supervision from ground-truth\n(GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).\n", "link": "http://arxiv.org/abs/2510.05752v1", "date": "2025-10-07", "relevancy": 2.3334, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6135}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5778}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALISE%3A%20Annotation-Free%20LiDAR%20Instance%20Segmentation%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20ALISE%3A%20Annotation-Free%20LiDAR%20Instance%20Segmentation%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Yongxuan%20Lyu%20and%20Guangfeng%20Jiang%20and%20Hongsi%20Liu%20and%20Jun%20Liu%0AAbstract%3A%20%20%20The%20manual%20annotation%20of%20outdoor%20LiDAR%20point%20clouds%20for%20instance%20segmentation%0Ais%20extremely%20costly%20and%20time-consuming.%20Current%20methods%20attempt%20to%20reduce%20this%0Aburden%20but%20still%20rely%20on%20some%20form%20of%20human%20labeling.%20To%20completely%20eliminate%0Athis%20dependency%2C%20we%20introduce%20ALISE%2C%20a%20novel%20framework%20that%20performs%20LiDAR%0Ainstance%20segmentation%20without%20any%20annotations.%20The%20central%20challenge%20is%20to%0Agenerate%20high-quality%20pseudo-labels%20in%20a%20fully%20unsupervised%20manner.%20Our%0Aapproach%20starts%20by%20employing%20Vision%20Foundation%20Models%20%28VFMs%29%2C%20guided%20by%20text%0Aand%20images%2C%20to%20produce%20initial%20pseudo-labels.%20We%20then%20refine%20these%20labels%0Athrough%20a%20dedicated%20spatio-temporal%20voting%20module%2C%20which%20combines%202D%20and%203D%0Asemantics%20for%20both%20offline%20and%20online%20optimization.%20To%20achieve%20superior%20feature%0Alearning%2C%20we%20further%20introduce%20two%20forms%20of%20semantic%20supervision%3A%20a%20set%20of%202D%0Aprior-based%20losses%20that%20inject%20visual%20knowledge%20into%20the%203D%20network%2C%20and%20a%0Anovel%20prototype-based%20contrastive%20loss%20that%20builds%20a%20discriminative%20feature%0Aspace%20by%20exploiting%203D%20semantic%20consistency.%20This%20comprehensive%20design%20results%0Ain%20significant%20performance%20gains%2C%20establishing%20a%20new%20state-of-the-art%20for%0Aunsupervised%203D%20instance%20segmentation.%20Remarkably%2C%20our%20approach%20even%0Aoutperforms%20MWSIS%2C%20a%20method%20that%20operates%20with%20supervision%20from%20ground-truth%0A%28GT%29%202D%20bounding%20boxes%20by%20a%20margin%20of%202.53%25%20in%20mAP%20%2850.95%25%20vs.%2048.42%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALISE%253A%2520Annotation-Free%2520LiDAR%2520Instance%2520Segmentation%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DYongxuan%2520Lyu%2520and%2520Guangfeng%2520Jiang%2520and%2520Hongsi%2520Liu%2520and%2520Jun%2520Liu%26entry.1292438233%3D%2520%2520The%2520manual%2520annotation%2520of%2520outdoor%2520LiDAR%2520point%2520clouds%2520for%2520instance%2520segmentation%250Ais%2520extremely%2520costly%2520and%2520time-consuming.%2520Current%2520methods%2520attempt%2520to%2520reduce%2520this%250Aburden%2520but%2520still%2520rely%2520on%2520some%2520form%2520of%2520human%2520labeling.%2520To%2520completely%2520eliminate%250Athis%2520dependency%252C%2520we%2520introduce%2520ALISE%252C%2520a%2520novel%2520framework%2520that%2520performs%2520LiDAR%250Ainstance%2520segmentation%2520without%2520any%2520annotations.%2520The%2520central%2520challenge%2520is%2520to%250Agenerate%2520high-quality%2520pseudo-labels%2520in%2520a%2520fully%2520unsupervised%2520manner.%2520Our%250Aapproach%2520starts%2520by%2520employing%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%252C%2520guided%2520by%2520text%250Aand%2520images%252C%2520to%2520produce%2520initial%2520pseudo-labels.%2520We%2520then%2520refine%2520these%2520labels%250Athrough%2520a%2520dedicated%2520spatio-temporal%2520voting%2520module%252C%2520which%2520combines%25202D%2520and%25203D%250Asemantics%2520for%2520both%2520offline%2520and%2520online%2520optimization.%2520To%2520achieve%2520superior%2520feature%250Alearning%252C%2520we%2520further%2520introduce%2520two%2520forms%2520of%2520semantic%2520supervision%253A%2520a%2520set%2520of%25202D%250Aprior-based%2520losses%2520that%2520inject%2520visual%2520knowledge%2520into%2520the%25203D%2520network%252C%2520and%2520a%250Anovel%2520prototype-based%2520contrastive%2520loss%2520that%2520builds%2520a%2520discriminative%2520feature%250Aspace%2520by%2520exploiting%25203D%2520semantic%2520consistency.%2520This%2520comprehensive%2520design%2520results%250Ain%2520significant%2520performance%2520gains%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520for%250Aunsupervised%25203D%2520instance%2520segmentation.%2520Remarkably%252C%2520our%2520approach%2520even%250Aoutperforms%2520MWSIS%252C%2520a%2520method%2520that%2520operates%2520with%2520supervision%2520from%2520ground-truth%250A%2528GT%2529%25202D%2520bounding%2520boxes%2520by%2520a%2520margin%2520of%25202.53%2525%2520in%2520mAP%2520%252850.95%2525%2520vs.%252048.42%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALISE%3A%20Annotation-Free%20LiDAR%20Instance%20Segmentation%20for%20Autonomous%0A%20%20Driving&entry.906535625=Yongxuan%20Lyu%20and%20Guangfeng%20Jiang%20and%20Hongsi%20Liu%20and%20Jun%20Liu&entry.1292438233=%20%20The%20manual%20annotation%20of%20outdoor%20LiDAR%20point%20clouds%20for%20instance%20segmentation%0Ais%20extremely%20costly%20and%20time-consuming.%20Current%20methods%20attempt%20to%20reduce%20this%0Aburden%20but%20still%20rely%20on%20some%20form%20of%20human%20labeling.%20To%20completely%20eliminate%0Athis%20dependency%2C%20we%20introduce%20ALISE%2C%20a%20novel%20framework%20that%20performs%20LiDAR%0Ainstance%20segmentation%20without%20any%20annotations.%20The%20central%20challenge%20is%20to%0Agenerate%20high-quality%20pseudo-labels%20in%20a%20fully%20unsupervised%20manner.%20Our%0Aapproach%20starts%20by%20employing%20Vision%20Foundation%20Models%20%28VFMs%29%2C%20guided%20by%20text%0Aand%20images%2C%20to%20produce%20initial%20pseudo-labels.%20We%20then%20refine%20these%20labels%0Athrough%20a%20dedicated%20spatio-temporal%20voting%20module%2C%20which%20combines%202D%20and%203D%0Asemantics%20for%20both%20offline%20and%20online%20optimization.%20To%20achieve%20superior%20feature%0Alearning%2C%20we%20further%20introduce%20two%20forms%20of%20semantic%20supervision%3A%20a%20set%20of%202D%0Aprior-based%20losses%20that%20inject%20visual%20knowledge%20into%20the%203D%20network%2C%20and%20a%0Anovel%20prototype-based%20contrastive%20loss%20that%20builds%20a%20discriminative%20feature%0Aspace%20by%20exploiting%203D%20semantic%20consistency.%20This%20comprehensive%20design%20results%0Ain%20significant%20performance%20gains%2C%20establishing%20a%20new%20state-of-the-art%20for%0Aunsupervised%203D%20instance%20segmentation.%20Remarkably%2C%20our%20approach%20even%0Aoutperforms%20MWSIS%2C%20a%20method%20that%20operates%20with%20supervision%20from%20ground-truth%0A%28GT%29%202D%20bounding%20boxes%20by%20a%20margin%20of%202.53%25%20in%20mAP%20%2850.95%25%20vs.%2048.42%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05752v1&entry.124074799=Read"},
{"title": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining", "author": "Jie Hao and Rui Yu and Wei Zhang and Huixia Wang and Jie Xu and Mingrui Liu", "abstract": "  Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks.\n", "link": "http://arxiv.org/abs/2510.06048v1", "date": "2025-10-07", "relevancy": 2.3326, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLISS%3A%20A%20Lightweight%20Bilevel%20Influence%20Scoring%20Method%20for%20Data%20Selection%0A%20%20in%20Language%20Model%20Pretraining&body=Title%3A%20BLISS%3A%20A%20Lightweight%20Bilevel%20Influence%20Scoring%20Method%20for%20Data%20Selection%0A%20%20in%20Language%20Model%20Pretraining%0AAuthor%3A%20Jie%20Hao%20and%20Rui%20Yu%20and%20Wei%20Zhang%20and%20Huixia%20Wang%20and%20Jie%20Xu%20and%20Mingrui%20Liu%0AAbstract%3A%20%20%20Effective%20data%20selection%20is%20essential%20for%20pretraining%20large%20language%20models%0A%28LLMs%29%2C%20enhancing%20efficiency%20and%20improving%20generalization%20to%20downstream%20tasks.%0AHowever%2C%20existing%20approaches%20often%20require%20leveraging%20external%20pretrained%0Amodels%2C%20making%20it%20difficult%20to%20disentangle%20the%20effects%20of%20data%20selection%20from%0Athose%20of%20the%20external%20pretrained%20models.%20In%20addition%2C%20they%20often%20overlook%20the%0Along-term%20impact%20of%20selected%20data%20if%20the%20model%20is%20trained%20to%20convergence%2C%0Aprimarily%20due%20to%20the%20prohibitive%20cost%20of%20full-scale%20LLM%20pretraining.%20In%20this%0Apaper%2C%20we%20introduce%20BLISS%20%28%5Ctextbf%7BB%7Dileve%5Ctextbf%7BL%7D%20%5Ctextbf%7BI%7Dnfluence%0A%5Ctextbf%7BS%7Dcoring%20method%20for%20data%20%5Ctextbf%7BS%7Delection%29%3A%20a%20lightweight%20data%0Aselection%20method%20that%20operates%20entirely%20%5Cemph%7Bfrom%20scratch%7D%2C%20without%20relying%20on%0Aany%20external%20pretrained%20oracle%20models%2C%20while%20explicitly%20accounting%20for%20the%0Along-term%20impact%20of%20selected%20data.%20BLISS%20leverages%20a%20small%20proxy%20model%20as%20a%0Asurrogate%20for%20the%20LLM%20and%20employs%20a%20score%20model%20to%20estimate%20the%20long-term%0Ainfluence%20of%20training%20samples%20if%20the%20proxy%20model%20is%20trained%20to%20convergence.%20We%0Aformulate%20data%20selection%20as%20a%20bilevel%20optimization%20problem%2C%20where%20the%0Aupper-level%20objective%20optimizes%20the%20score%20model%20to%20assign%20importance%20weights%20to%0Atraining%20samples%2C%20ensuring%20that%20minimizing%20the%20lower-level%20objective%20%28i.e.%2C%0Atraining%20the%20proxy%20model%20over%20the%20weighted%20training%20loss%20until%20convergence%29%0Aleads%20to%20best%20validation%20performance.%20Once%20optimized%2C%20the%20trained%20score%20model%0Apredicts%20influence%20scores%20for%20the%20dataset%2C%20enabling%20efficient%20selection%20of%0Ahigh-quality%20samples%20for%20LLM%20pretraining.%20We%20validate%20BLISS%20by%20pretraining%0A410M/1B/2.8B%20Pythia%20and%20LLaMA-0.5B%20models%20on%20selected%20subsets%20of%20the%20C4%0Adataset.%20Notably%2C%20under%20the%201B%20model%20setting%2C%20BLISS%20achieves%20%241.7%5Ctimes%24%0Aspeedup%20in%20reaching%20the%20same%20performance%20as%20the%20state-of-the-art%20method%2C%0Ademonstrating%20superior%20performance%20across%20multiple%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLISS%253A%2520A%2520Lightweight%2520Bilevel%2520Influence%2520Scoring%2520Method%2520for%2520Data%2520Selection%250A%2520%2520in%2520Language%2520Model%2520Pretraining%26entry.906535625%3DJie%2520Hao%2520and%2520Rui%2520Yu%2520and%2520Wei%2520Zhang%2520and%2520Huixia%2520Wang%2520and%2520Jie%2520Xu%2520and%2520Mingrui%2520Liu%26entry.1292438233%3D%2520%2520Effective%2520data%2520selection%2520is%2520essential%2520for%2520pretraining%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520enhancing%2520efficiency%2520and%2520improving%2520generalization%2520to%2520downstream%2520tasks.%250AHowever%252C%2520existing%2520approaches%2520often%2520require%2520leveraging%2520external%2520pretrained%250Amodels%252C%2520making%2520it%2520difficult%2520to%2520disentangle%2520the%2520effects%2520of%2520data%2520selection%2520from%250Athose%2520of%2520the%2520external%2520pretrained%2520models.%2520In%2520addition%252C%2520they%2520often%2520overlook%2520the%250Along-term%2520impact%2520of%2520selected%2520data%2520if%2520the%2520model%2520is%2520trained%2520to%2520convergence%252C%250Aprimarily%2520due%2520to%2520the%2520prohibitive%2520cost%2520of%2520full-scale%2520LLM%2520pretraining.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520BLISS%2520%2528%255Ctextbf%257BB%257Dileve%255Ctextbf%257BL%257D%2520%255Ctextbf%257BI%257Dnfluence%250A%255Ctextbf%257BS%257Dcoring%2520method%2520for%2520data%2520%255Ctextbf%257BS%257Delection%2529%253A%2520a%2520lightweight%2520data%250Aselection%2520method%2520that%2520operates%2520entirely%2520%255Cemph%257Bfrom%2520scratch%257D%252C%2520without%2520relying%2520on%250Aany%2520external%2520pretrained%2520oracle%2520models%252C%2520while%2520explicitly%2520accounting%2520for%2520the%250Along-term%2520impact%2520of%2520selected%2520data.%2520BLISS%2520leverages%2520a%2520small%2520proxy%2520model%2520as%2520a%250Asurrogate%2520for%2520the%2520LLM%2520and%2520employs%2520a%2520score%2520model%2520to%2520estimate%2520the%2520long-term%250Ainfluence%2520of%2520training%2520samples%2520if%2520the%2520proxy%2520model%2520is%2520trained%2520to%2520convergence.%2520We%250Aformulate%2520data%2520selection%2520as%2520a%2520bilevel%2520optimization%2520problem%252C%2520where%2520the%250Aupper-level%2520objective%2520optimizes%2520the%2520score%2520model%2520to%2520assign%2520importance%2520weights%2520to%250Atraining%2520samples%252C%2520ensuring%2520that%2520minimizing%2520the%2520lower-level%2520objective%2520%2528i.e.%252C%250Atraining%2520the%2520proxy%2520model%2520over%2520the%2520weighted%2520training%2520loss%2520until%2520convergence%2529%250Aleads%2520to%2520best%2520validation%2520performance.%2520Once%2520optimized%252C%2520the%2520trained%2520score%2520model%250Apredicts%2520influence%2520scores%2520for%2520the%2520dataset%252C%2520enabling%2520efficient%2520selection%2520of%250Ahigh-quality%2520samples%2520for%2520LLM%2520pretraining.%2520We%2520validate%2520BLISS%2520by%2520pretraining%250A410M/1B/2.8B%2520Pythia%2520and%2520LLaMA-0.5B%2520models%2520on%2520selected%2520subsets%2520of%2520the%2520C4%250Adataset.%2520Notably%252C%2520under%2520the%25201B%2520model%2520setting%252C%2520BLISS%2520achieves%2520%25241.7%255Ctimes%2524%250Aspeedup%2520in%2520reaching%2520the%2520same%2520performance%2520as%2520the%2520state-of-the-art%2520method%252C%250Ademonstrating%2520superior%2520performance%2520across%2520multiple%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLISS%3A%20A%20Lightweight%20Bilevel%20Influence%20Scoring%20Method%20for%20Data%20Selection%0A%20%20in%20Language%20Model%20Pretraining&entry.906535625=Jie%20Hao%20and%20Rui%20Yu%20and%20Wei%20Zhang%20and%20Huixia%20Wang%20and%20Jie%20Xu%20and%20Mingrui%20Liu&entry.1292438233=%20%20Effective%20data%20selection%20is%20essential%20for%20pretraining%20large%20language%20models%0A%28LLMs%29%2C%20enhancing%20efficiency%20and%20improving%20generalization%20to%20downstream%20tasks.%0AHowever%2C%20existing%20approaches%20often%20require%20leveraging%20external%20pretrained%0Amodels%2C%20making%20it%20difficult%20to%20disentangle%20the%20effects%20of%20data%20selection%20from%0Athose%20of%20the%20external%20pretrained%20models.%20In%20addition%2C%20they%20often%20overlook%20the%0Along-term%20impact%20of%20selected%20data%20if%20the%20model%20is%20trained%20to%20convergence%2C%0Aprimarily%20due%20to%20the%20prohibitive%20cost%20of%20full-scale%20LLM%20pretraining.%20In%20this%0Apaper%2C%20we%20introduce%20BLISS%20%28%5Ctextbf%7BB%7Dileve%5Ctextbf%7BL%7D%20%5Ctextbf%7BI%7Dnfluence%0A%5Ctextbf%7BS%7Dcoring%20method%20for%20data%20%5Ctextbf%7BS%7Delection%29%3A%20a%20lightweight%20data%0Aselection%20method%20that%20operates%20entirely%20%5Cemph%7Bfrom%20scratch%7D%2C%20without%20relying%20on%0Aany%20external%20pretrained%20oracle%20models%2C%20while%20explicitly%20accounting%20for%20the%0Along-term%20impact%20of%20selected%20data.%20BLISS%20leverages%20a%20small%20proxy%20model%20as%20a%0Asurrogate%20for%20the%20LLM%20and%20employs%20a%20score%20model%20to%20estimate%20the%20long-term%0Ainfluence%20of%20training%20samples%20if%20the%20proxy%20model%20is%20trained%20to%20convergence.%20We%0Aformulate%20data%20selection%20as%20a%20bilevel%20optimization%20problem%2C%20where%20the%0Aupper-level%20objective%20optimizes%20the%20score%20model%20to%20assign%20importance%20weights%20to%0Atraining%20samples%2C%20ensuring%20that%20minimizing%20the%20lower-level%20objective%20%28i.e.%2C%0Atraining%20the%20proxy%20model%20over%20the%20weighted%20training%20loss%20until%20convergence%29%0Aleads%20to%20best%20validation%20performance.%20Once%20optimized%2C%20the%20trained%20score%20model%0Apredicts%20influence%20scores%20for%20the%20dataset%2C%20enabling%20efficient%20selection%20of%0Ahigh-quality%20samples%20for%20LLM%20pretraining.%20We%20validate%20BLISS%20by%20pretraining%0A410M/1B/2.8B%20Pythia%20and%20LLaMA-0.5B%20models%20on%20selected%20subsets%20of%20the%20C4%0Adataset.%20Notably%2C%20under%20the%201B%20model%20setting%2C%20BLISS%20achieves%20%241.7%5Ctimes%24%0Aspeedup%20in%20reaching%20the%20same%20performance%20as%20the%20state-of-the-art%20method%2C%0Ademonstrating%20superior%20performance%20across%20multiple%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06048v1&entry.124074799=Read"},
{"title": "Reasoning under Vision: Understanding Visual-Spatial Cognition in\n  Vision-Language Models for CAPTCHA", "author": "Python Song and Luke Tenyi Chang and Yun-Yun Tsai and Penghui Li and Junfeng Yang", "abstract": "  CAPTCHA, originally designed to distinguish humans from robots, has evolved\ninto a real-world benchmark for assessing the spatial reasoning capabilities of\nvision-language models. In this work, we first show that step-by-step reasoning\nis crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent\nhigh-difficulty spatial reasoning tasks, and that current commercial\nvision-language models still struggle with such reasoning. In particular, we\nobserve that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to\neffectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).\nHowever, our findings indicate that requiring the model to perform step-by-step\nreasoning before generating the final coordinates can significantly enhance its\nsolving accuracy, underscoring the severity of the gap. To systematically study\nthis issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with\nreasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,\netc.) with step-by-step action solutions and grounding annotations. We further\ndefine five reasoning-oriented metrics that enable a comprehensive evaluation\nof models reasoning capabilities. To validate the effectiveness of reasoning,\nwe also propose a general agentic VLM-based framework that incorporates the\nmodels inherent reasoning abilities. Our method achieves state-of-the-art\nperformance across five high-difficulty CAPTCHA types, with an average solving\naccuracy of 83.9 percent, substantially surpassing existing baselines. These\nresults reveal the limitations of current models and highlight the importance\nof reasoning in advancing visual-spatial challenges in the future.\n", "link": "http://arxiv.org/abs/2510.06067v1", "date": "2025-10-07", "relevancy": 2.3314, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5958}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20under%20Vision%3A%20Understanding%20Visual-Spatial%20Cognition%20in%0A%20%20Vision-Language%20Models%20for%20CAPTCHA&body=Title%3A%20Reasoning%20under%20Vision%3A%20Understanding%20Visual-Spatial%20Cognition%20in%0A%20%20Vision-Language%20Models%20for%20CAPTCHA%0AAuthor%3A%20Python%20Song%20and%20Luke%20Tenyi%20Chang%20and%20Yun-Yun%20Tsai%20and%20Penghui%20Li%20and%20Junfeng%20Yang%0AAbstract%3A%20%20%20CAPTCHA%2C%20originally%20designed%20to%20distinguish%20humans%20from%20robots%2C%20has%20evolved%0Ainto%20a%20real-world%20benchmark%20for%20assessing%20the%20spatial%20reasoning%20capabilities%20of%0Avision-language%20models.%20In%20this%20work%2C%20we%20first%20show%20that%20step-by-step%20reasoning%0Ais%20crucial%20for%20vision-language%20models%20%28VLMs%29%20to%20solve%20CAPTCHAs%2C%20which%20represent%0Ahigh-difficulty%20spatial%20reasoning%20tasks%2C%20and%20that%20current%20commercial%0Avision-language%20models%20still%20struggle%20with%20such%20reasoning.%20In%20particular%2C%20we%0Aobserve%20that%20most%20commercial%20VLMs%20%28e.g.%2C%20Gemini%2C%20Claude%2C%20GPT%2C%20etc.%29%20fail%20to%0Aeffectively%20solve%20CAPTCHAs%20and%20thus%20achieve%20low%20accuracy%20%28around%2021.9%20percent%29.%0AHowever%2C%20our%20findings%20indicate%20that%20requiring%20the%20model%20to%20perform%20step-by-step%0Areasoning%20before%20generating%20the%20final%20coordinates%20can%20significantly%20enhance%20its%0Asolving%20accuracy%2C%20underscoring%20the%20severity%20of%20the%20gap.%20To%20systematically%20study%0Athis%20issue%2C%20we%20introduce%20CAPTCHA-X%2C%20the%20first%20real-world%20CAPTCHA%20benchmark%20with%0Areasoning%2C%20covering%20seven%20categories%20of%20CAPTCHAs%20%28such%20as%20Gobang%2C%20hCaptcha%2C%0Aetc.%29%20with%20step-by-step%20action%20solutions%20and%20grounding%20annotations.%20We%20further%0Adefine%20five%20reasoning-oriented%20metrics%20that%20enable%20a%20comprehensive%20evaluation%0Aof%20models%20reasoning%20capabilities.%20To%20validate%20the%20effectiveness%20of%20reasoning%2C%0Awe%20also%20propose%20a%20general%20agentic%20VLM-based%20framework%20that%20incorporates%20the%0Amodels%20inherent%20reasoning%20abilities.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%20across%20five%20high-difficulty%20CAPTCHA%20types%2C%20with%20an%20average%20solving%0Aaccuracy%20of%2083.9%20percent%2C%20substantially%20surpassing%20existing%20baselines.%20These%0Aresults%20reveal%20the%20limitations%20of%20current%20models%20and%20highlight%20the%20importance%0Aof%20reasoning%20in%20advancing%20visual-spatial%20challenges%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520under%2520Vision%253A%2520Understanding%2520Visual-Spatial%2520Cognition%2520in%250A%2520%2520Vision-Language%2520Models%2520for%2520CAPTCHA%26entry.906535625%3DPython%2520Song%2520and%2520Luke%2520Tenyi%2520Chang%2520and%2520Yun-Yun%2520Tsai%2520and%2520Penghui%2520Li%2520and%2520Junfeng%2520Yang%26entry.1292438233%3D%2520%2520CAPTCHA%252C%2520originally%2520designed%2520to%2520distinguish%2520humans%2520from%2520robots%252C%2520has%2520evolved%250Ainto%2520a%2520real-world%2520benchmark%2520for%2520assessing%2520the%2520spatial%2520reasoning%2520capabilities%2520of%250Avision-language%2520models.%2520In%2520this%2520work%252C%2520we%2520first%2520show%2520that%2520step-by-step%2520reasoning%250Ais%2520crucial%2520for%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520solve%2520CAPTCHAs%252C%2520which%2520represent%250Ahigh-difficulty%2520spatial%2520reasoning%2520tasks%252C%2520and%2520that%2520current%2520commercial%250Avision-language%2520models%2520still%2520struggle%2520with%2520such%2520reasoning.%2520In%2520particular%252C%2520we%250Aobserve%2520that%2520most%2520commercial%2520VLMs%2520%2528e.g.%252C%2520Gemini%252C%2520Claude%252C%2520GPT%252C%2520etc.%2529%2520fail%2520to%250Aeffectively%2520solve%2520CAPTCHAs%2520and%2520thus%2520achieve%2520low%2520accuracy%2520%2528around%252021.9%2520percent%2529.%250AHowever%252C%2520our%2520findings%2520indicate%2520that%2520requiring%2520the%2520model%2520to%2520perform%2520step-by-step%250Areasoning%2520before%2520generating%2520the%2520final%2520coordinates%2520can%2520significantly%2520enhance%2520its%250Asolving%2520accuracy%252C%2520underscoring%2520the%2520severity%2520of%2520the%2520gap.%2520To%2520systematically%2520study%250Athis%2520issue%252C%2520we%2520introduce%2520CAPTCHA-X%252C%2520the%2520first%2520real-world%2520CAPTCHA%2520benchmark%2520with%250Areasoning%252C%2520covering%2520seven%2520categories%2520of%2520CAPTCHAs%2520%2528such%2520as%2520Gobang%252C%2520hCaptcha%252C%250Aetc.%2529%2520with%2520step-by-step%2520action%2520solutions%2520and%2520grounding%2520annotations.%2520We%2520further%250Adefine%2520five%2520reasoning-oriented%2520metrics%2520that%2520enable%2520a%2520comprehensive%2520evaluation%250Aof%2520models%2520reasoning%2520capabilities.%2520To%2520validate%2520the%2520effectiveness%2520of%2520reasoning%252C%250Awe%2520also%2520propose%2520a%2520general%2520agentic%2520VLM-based%2520framework%2520that%2520incorporates%2520the%250Amodels%2520inherent%2520reasoning%2520abilities.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520five%2520high-difficulty%2520CAPTCHA%2520types%252C%2520with%2520an%2520average%2520solving%250Aaccuracy%2520of%252083.9%2520percent%252C%2520substantially%2520surpassing%2520existing%2520baselines.%2520These%250Aresults%2520reveal%2520the%2520limitations%2520of%2520current%2520models%2520and%2520highlight%2520the%2520importance%250Aof%2520reasoning%2520in%2520advancing%2520visual-spatial%2520challenges%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20under%20Vision%3A%20Understanding%20Visual-Spatial%20Cognition%20in%0A%20%20Vision-Language%20Models%20for%20CAPTCHA&entry.906535625=Python%20Song%20and%20Luke%20Tenyi%20Chang%20and%20Yun-Yun%20Tsai%20and%20Penghui%20Li%20and%20Junfeng%20Yang&entry.1292438233=%20%20CAPTCHA%2C%20originally%20designed%20to%20distinguish%20humans%20from%20robots%2C%20has%20evolved%0Ainto%20a%20real-world%20benchmark%20for%20assessing%20the%20spatial%20reasoning%20capabilities%20of%0Avision-language%20models.%20In%20this%20work%2C%20we%20first%20show%20that%20step-by-step%20reasoning%0Ais%20crucial%20for%20vision-language%20models%20%28VLMs%29%20to%20solve%20CAPTCHAs%2C%20which%20represent%0Ahigh-difficulty%20spatial%20reasoning%20tasks%2C%20and%20that%20current%20commercial%0Avision-language%20models%20still%20struggle%20with%20such%20reasoning.%20In%20particular%2C%20we%0Aobserve%20that%20most%20commercial%20VLMs%20%28e.g.%2C%20Gemini%2C%20Claude%2C%20GPT%2C%20etc.%29%20fail%20to%0Aeffectively%20solve%20CAPTCHAs%20and%20thus%20achieve%20low%20accuracy%20%28around%2021.9%20percent%29.%0AHowever%2C%20our%20findings%20indicate%20that%20requiring%20the%20model%20to%20perform%20step-by-step%0Areasoning%20before%20generating%20the%20final%20coordinates%20can%20significantly%20enhance%20its%0Asolving%20accuracy%2C%20underscoring%20the%20severity%20of%20the%20gap.%20To%20systematically%20study%0Athis%20issue%2C%20we%20introduce%20CAPTCHA-X%2C%20the%20first%20real-world%20CAPTCHA%20benchmark%20with%0Areasoning%2C%20covering%20seven%20categories%20of%20CAPTCHAs%20%28such%20as%20Gobang%2C%20hCaptcha%2C%0Aetc.%29%20with%20step-by-step%20action%20solutions%20and%20grounding%20annotations.%20We%20further%0Adefine%20five%20reasoning-oriented%20metrics%20that%20enable%20a%20comprehensive%20evaluation%0Aof%20models%20reasoning%20capabilities.%20To%20validate%20the%20effectiveness%20of%20reasoning%2C%0Awe%20also%20propose%20a%20general%20agentic%20VLM-based%20framework%20that%20incorporates%20the%0Amodels%20inherent%20reasoning%20abilities.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%20across%20five%20high-difficulty%20CAPTCHA%20types%2C%20with%20an%20average%20solving%0Aaccuracy%20of%2083.9%20percent%2C%20substantially%20surpassing%20existing%20baselines.%20These%0Aresults%20reveal%20the%20limitations%20of%20current%20models%20and%20highlight%20the%20importance%0Aof%20reasoning%20in%20advancing%20visual-spatial%20challenges%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06067v1&entry.124074799=Read"},
{"title": "EntryPrune: Neural Network Feature Selection using First Impressions", "author": "Felix Zimmer and Patrik Okanovic and Torsten Hoefler", "abstract": "  There is an ongoing effort to develop feature selection algorithms to improve\ninterpretability, reduce computational resources, and minimize overfitting in\npredictive models. Neural networks stand out as architectures on which to build\nfeature selection methods, and recently, neuron pruning and regrowth have\nemerged from the sparse neural network literature as promising new tools. We\nintroduce EntryPrune, a novel supervised feature selection algorithm using a\ndense neural network with a dynamic sparse input layer. It employs entry-based\npruning, a novel approach that compares neurons based on their relative change\ninduced when they have entered the network. Extensive experiments on 13\ndifferent datasets show that our approach generally outperforms the current\nstate-of-the-art methods, and in particular improves the average accuracy on\nlow-dimensional datasets. Furthermore, we show that EntryPruning surpasses\ntraditional techniques such as magnitude pruning within the EntryPrune\nframework and that EntryPrune achieves lower runtime than competing approaches.\nOur code is available at https://github.com/flxzimmer/entryprune.\n", "link": "http://arxiv.org/abs/2410.02344v4", "date": "2025-10-07", "relevancy": 2.3314, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5092}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4483}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EntryPrune%3A%20Neural%20Network%20Feature%20Selection%20using%20First%20Impressions&body=Title%3A%20EntryPrune%3A%20Neural%20Network%20Feature%20Selection%20using%20First%20Impressions%0AAuthor%3A%20Felix%20Zimmer%20and%20Patrik%20Okanovic%20and%20Torsten%20Hoefler%0AAbstract%3A%20%20%20There%20is%20an%20ongoing%20effort%20to%20develop%20feature%20selection%20algorithms%20to%20improve%0Ainterpretability%2C%20reduce%20computational%20resources%2C%20and%20minimize%20overfitting%20in%0Apredictive%20models.%20Neural%20networks%20stand%20out%20as%20architectures%20on%20which%20to%20build%0Afeature%20selection%20methods%2C%20and%20recently%2C%20neuron%20pruning%20and%20regrowth%20have%0Aemerged%20from%20the%20sparse%20neural%20network%20literature%20as%20promising%20new%20tools.%20We%0Aintroduce%20EntryPrune%2C%20a%20novel%20supervised%20feature%20selection%20algorithm%20using%20a%0Adense%20neural%20network%20with%20a%20dynamic%20sparse%20input%20layer.%20It%20employs%20entry-based%0Apruning%2C%20a%20novel%20approach%20that%20compares%20neurons%20based%20on%20their%20relative%20change%0Ainduced%20when%20they%20have%20entered%20the%20network.%20Extensive%20experiments%20on%2013%0Adifferent%20datasets%20show%20that%20our%20approach%20generally%20outperforms%20the%20current%0Astate-of-the-art%20methods%2C%20and%20in%20particular%20improves%20the%20average%20accuracy%20on%0Alow-dimensional%20datasets.%20Furthermore%2C%20we%20show%20that%20EntryPruning%20surpasses%0Atraditional%20techniques%20such%20as%20magnitude%20pruning%20within%20the%20EntryPrune%0Aframework%20and%20that%20EntryPrune%20achieves%20lower%20runtime%20than%20competing%20approaches.%0AOur%20code%20is%20available%20at%20https%3A//github.com/flxzimmer/entryprune.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02344v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntryPrune%253A%2520Neural%2520Network%2520Feature%2520Selection%2520using%2520First%2520Impressions%26entry.906535625%3DFelix%2520Zimmer%2520and%2520Patrik%2520Okanovic%2520and%2520Torsten%2520Hoefler%26entry.1292438233%3D%2520%2520There%2520is%2520an%2520ongoing%2520effort%2520to%2520develop%2520feature%2520selection%2520algorithms%2520to%2520improve%250Ainterpretability%252C%2520reduce%2520computational%2520resources%252C%2520and%2520minimize%2520overfitting%2520in%250Apredictive%2520models.%2520Neural%2520networks%2520stand%2520out%2520as%2520architectures%2520on%2520which%2520to%2520build%250Afeature%2520selection%2520methods%252C%2520and%2520recently%252C%2520neuron%2520pruning%2520and%2520regrowth%2520have%250Aemerged%2520from%2520the%2520sparse%2520neural%2520network%2520literature%2520as%2520promising%2520new%2520tools.%2520We%250Aintroduce%2520EntryPrune%252C%2520a%2520novel%2520supervised%2520feature%2520selection%2520algorithm%2520using%2520a%250Adense%2520neural%2520network%2520with%2520a%2520dynamic%2520sparse%2520input%2520layer.%2520It%2520employs%2520entry-based%250Apruning%252C%2520a%2520novel%2520approach%2520that%2520compares%2520neurons%2520based%2520on%2520their%2520relative%2520change%250Ainduced%2520when%2520they%2520have%2520entered%2520the%2520network.%2520Extensive%2520experiments%2520on%252013%250Adifferent%2520datasets%2520show%2520that%2520our%2520approach%2520generally%2520outperforms%2520the%2520current%250Astate-of-the-art%2520methods%252C%2520and%2520in%2520particular%2520improves%2520the%2520average%2520accuracy%2520on%250Alow-dimensional%2520datasets.%2520Furthermore%252C%2520we%2520show%2520that%2520EntryPruning%2520surpasses%250Atraditional%2520techniques%2520such%2520as%2520magnitude%2520pruning%2520within%2520the%2520EntryPrune%250Aframework%2520and%2520that%2520EntryPrune%2520achieves%2520lower%2520runtime%2520than%2520competing%2520approaches.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/flxzimmer/entryprune.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02344v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EntryPrune%3A%20Neural%20Network%20Feature%20Selection%20using%20First%20Impressions&entry.906535625=Felix%20Zimmer%20and%20Patrik%20Okanovic%20and%20Torsten%20Hoefler&entry.1292438233=%20%20There%20is%20an%20ongoing%20effort%20to%20develop%20feature%20selection%20algorithms%20to%20improve%0Ainterpretability%2C%20reduce%20computational%20resources%2C%20and%20minimize%20overfitting%20in%0Apredictive%20models.%20Neural%20networks%20stand%20out%20as%20architectures%20on%20which%20to%20build%0Afeature%20selection%20methods%2C%20and%20recently%2C%20neuron%20pruning%20and%20regrowth%20have%0Aemerged%20from%20the%20sparse%20neural%20network%20literature%20as%20promising%20new%20tools.%20We%0Aintroduce%20EntryPrune%2C%20a%20novel%20supervised%20feature%20selection%20algorithm%20using%20a%0Adense%20neural%20network%20with%20a%20dynamic%20sparse%20input%20layer.%20It%20employs%20entry-based%0Apruning%2C%20a%20novel%20approach%20that%20compares%20neurons%20based%20on%20their%20relative%20change%0Ainduced%20when%20they%20have%20entered%20the%20network.%20Extensive%20experiments%20on%2013%0Adifferent%20datasets%20show%20that%20our%20approach%20generally%20outperforms%20the%20current%0Astate-of-the-art%20methods%2C%20and%20in%20particular%20improves%20the%20average%20accuracy%20on%0Alow-dimensional%20datasets.%20Furthermore%2C%20we%20show%20that%20EntryPruning%20surpasses%0Atraditional%20techniques%20such%20as%20magnitude%20pruning%20within%20the%20EntryPrune%0Aframework%20and%20that%20EntryPrune%20achieves%20lower%20runtime%20than%20competing%20approaches.%0AOur%20code%20is%20available%20at%20https%3A//github.com/flxzimmer/entryprune.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02344v4&entry.124074799=Read"},
{"title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression", "author": "Yi-Hsin Li and Thomas Sikora and Sebastian Knorr and M\u00e5rten Sj\u00f6str\u00f6m", "abstract": "  The Steered Mixture of Experts regression framework has demonstrated strong\nperformance in image reconstruction, compression, denoising, and\nsuper-resolution. However, its high computational cost limits practical\napplications. This work introduces a rasterization-based optimization strategy\nthat combines the efficiency of rasterized Gaussian kernel rendering with the\nedge-aware gating mechanism of the Steered Mixture of Experts. The proposed\nmethod is designed to accelerate two-dimensional image regression while\nmaintaining the model's inherent sparsity and reconstruction quality. By\nreplacing global iterative optimization with a rasterized formulation, the\nmethod achieves significantly faster parameter updates and more\nmemory-efficient model representations. In addition, the proposed framework\nsupports applications such as native super-resolution and image denoising,\nwhich are not directly achievable with standard rasterized Gaussian kernel\napproaches. The combination of fast rasterized optimization with the edge-aware\nstructure of the Steered Mixture of Experts provides a new balance between\ncomputational efficiency and reconstruction fidelity for two-dimensional image\nprocessing tasks.\n", "link": "http://arxiv.org/abs/2510.05814v1", "date": "2025-10-07", "relevancy": 2.3298, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5916}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.585}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rasterized%20Steered%20Mixture%20of%20Experts%20for%20Efficient%202D%20Image%20Regression&body=Title%3A%20Rasterized%20Steered%20Mixture%20of%20Experts%20for%20Efficient%202D%20Image%20Regression%0AAuthor%3A%20Yi-Hsin%20Li%20and%20Thomas%20Sikora%20and%20Sebastian%20Knorr%20and%20M%C3%A5rten%20Sj%C3%B6str%C3%B6m%0AAbstract%3A%20%20%20The%20Steered%20Mixture%20of%20Experts%20regression%20framework%20has%20demonstrated%20strong%0Aperformance%20in%20image%20reconstruction%2C%20compression%2C%20denoising%2C%20and%0Asuper-resolution.%20However%2C%20its%20high%20computational%20cost%20limits%20practical%0Aapplications.%20This%20work%20introduces%20a%20rasterization-based%20optimization%20strategy%0Athat%20combines%20the%20efficiency%20of%20rasterized%20Gaussian%20kernel%20rendering%20with%20the%0Aedge-aware%20gating%20mechanism%20of%20the%20Steered%20Mixture%20of%20Experts.%20The%20proposed%0Amethod%20is%20designed%20to%20accelerate%20two-dimensional%20image%20regression%20while%0Amaintaining%20the%20model%27s%20inherent%20sparsity%20and%20reconstruction%20quality.%20By%0Areplacing%20global%20iterative%20optimization%20with%20a%20rasterized%20formulation%2C%20the%0Amethod%20achieves%20significantly%20faster%20parameter%20updates%20and%20more%0Amemory-efficient%20model%20representations.%20In%20addition%2C%20the%20proposed%20framework%0Asupports%20applications%20such%20as%20native%20super-resolution%20and%20image%20denoising%2C%0Awhich%20are%20not%20directly%20achievable%20with%20standard%20rasterized%20Gaussian%20kernel%0Aapproaches.%20The%20combination%20of%20fast%20rasterized%20optimization%20with%20the%20edge-aware%0Astructure%20of%20the%20Steered%20Mixture%20of%20Experts%20provides%20a%20new%20balance%20between%0Acomputational%20efficiency%20and%20reconstruction%20fidelity%20for%20two-dimensional%20image%0Aprocessing%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRasterized%2520Steered%2520Mixture%2520of%2520Experts%2520for%2520Efficient%25202D%2520Image%2520Regression%26entry.906535625%3DYi-Hsin%2520Li%2520and%2520Thomas%2520Sikora%2520and%2520Sebastian%2520Knorr%2520and%2520M%25C3%25A5rten%2520Sj%25C3%25B6str%25C3%25B6m%26entry.1292438233%3D%2520%2520The%2520Steered%2520Mixture%2520of%2520Experts%2520regression%2520framework%2520has%2520demonstrated%2520strong%250Aperformance%2520in%2520image%2520reconstruction%252C%2520compression%252C%2520denoising%252C%2520and%250Asuper-resolution.%2520However%252C%2520its%2520high%2520computational%2520cost%2520limits%2520practical%250Aapplications.%2520This%2520work%2520introduces%2520a%2520rasterization-based%2520optimization%2520strategy%250Athat%2520combines%2520the%2520efficiency%2520of%2520rasterized%2520Gaussian%2520kernel%2520rendering%2520with%2520the%250Aedge-aware%2520gating%2520mechanism%2520of%2520the%2520Steered%2520Mixture%2520of%2520Experts.%2520The%2520proposed%250Amethod%2520is%2520designed%2520to%2520accelerate%2520two-dimensional%2520image%2520regression%2520while%250Amaintaining%2520the%2520model%2527s%2520inherent%2520sparsity%2520and%2520reconstruction%2520quality.%2520By%250Areplacing%2520global%2520iterative%2520optimization%2520with%2520a%2520rasterized%2520formulation%252C%2520the%250Amethod%2520achieves%2520significantly%2520faster%2520parameter%2520updates%2520and%2520more%250Amemory-efficient%2520model%2520representations.%2520In%2520addition%252C%2520the%2520proposed%2520framework%250Asupports%2520applications%2520such%2520as%2520native%2520super-resolution%2520and%2520image%2520denoising%252C%250Awhich%2520are%2520not%2520directly%2520achievable%2520with%2520standard%2520rasterized%2520Gaussian%2520kernel%250Aapproaches.%2520The%2520combination%2520of%2520fast%2520rasterized%2520optimization%2520with%2520the%2520edge-aware%250Astructure%2520of%2520the%2520Steered%2520Mixture%2520of%2520Experts%2520provides%2520a%2520new%2520balance%2520between%250Acomputational%2520efficiency%2520and%2520reconstruction%2520fidelity%2520for%2520two-dimensional%2520image%250Aprocessing%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rasterized%20Steered%20Mixture%20of%20Experts%20for%20Efficient%202D%20Image%20Regression&entry.906535625=Yi-Hsin%20Li%20and%20Thomas%20Sikora%20and%20Sebastian%20Knorr%20and%20M%C3%A5rten%20Sj%C3%B6str%C3%B6m&entry.1292438233=%20%20The%20Steered%20Mixture%20of%20Experts%20regression%20framework%20has%20demonstrated%20strong%0Aperformance%20in%20image%20reconstruction%2C%20compression%2C%20denoising%2C%20and%0Asuper-resolution.%20However%2C%20its%20high%20computational%20cost%20limits%20practical%0Aapplications.%20This%20work%20introduces%20a%20rasterization-based%20optimization%20strategy%0Athat%20combines%20the%20efficiency%20of%20rasterized%20Gaussian%20kernel%20rendering%20with%20the%0Aedge-aware%20gating%20mechanism%20of%20the%20Steered%20Mixture%20of%20Experts.%20The%20proposed%0Amethod%20is%20designed%20to%20accelerate%20two-dimensional%20image%20regression%20while%0Amaintaining%20the%20model%27s%20inherent%20sparsity%20and%20reconstruction%20quality.%20By%0Areplacing%20global%20iterative%20optimization%20with%20a%20rasterized%20formulation%2C%20the%0Amethod%20achieves%20significantly%20faster%20parameter%20updates%20and%20more%0Amemory-efficient%20model%20representations.%20In%20addition%2C%20the%20proposed%20framework%0Asupports%20applications%20such%20as%20native%20super-resolution%20and%20image%20denoising%2C%0Awhich%20are%20not%20directly%20achievable%20with%20standard%20rasterized%20Gaussian%20kernel%0Aapproaches.%20The%20combination%20of%20fast%20rasterized%20optimization%20with%20the%20edge-aware%0Astructure%20of%20the%20Steered%20Mixture%20of%20Experts%20provides%20a%20new%20balance%20between%0Acomputational%20efficiency%20and%20reconstruction%20fidelity%20for%20two-dimensional%20image%0Aprocessing%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05814v1&entry.124074799=Read"},
{"title": "Revisiting Long-context Modeling from Context Denoising Perspective", "author": "Zecheng Tang and Baibei Ji and Juntao Li and Lijun Wu and Haijia Gui and Min Zhang", "abstract": "  Long-context models (LCMs) have demonstrated great potential in processing\nlong sequences, facilitating many real-world applications. The success of LCMs\ncan be attributed to their ability to locate implicit critical information\nwithin the context for further prediction. However, recent research reveals\nthat LCMs are often susceptible to contextual noise, i.e., irrelevant tokens,\nthat can mislead model attention. In this paper, we conduct a fine-grained\nanalysis of the context noise and propose an effective metric, the Integrated\nGradient (IG) score, to detect and quantify the noise information within the\ncontext. Our findings reveal that even simple mitigation of detected context\nnoise can substantially boost the model's attention on critical tokens and\nbenefit subsequent predictions. Building on this insight, we propose Context\nDenoising Training (CDT), a straightforward yet effective training strategy\nthat improves attention on critical tokens while reinforcing their influence on\nmodel predictions. Extensive experiments across four tasks, under both context\nwindow scaling and long-context alignment settings, demonstrate the superiority\nof CDT. Notably, when trained with CDT, an open-source 8B model can achieve\nperformance (50.92) comparable to GPT-4o (51.00).\n", "link": "http://arxiv.org/abs/2510.05862v1", "date": "2025-10-07", "relevancy": 2.2763, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5769}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5769}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Long-context%20Modeling%20from%20Context%20Denoising%20Perspective&body=Title%3A%20Revisiting%20Long-context%20Modeling%20from%20Context%20Denoising%20Perspective%0AAuthor%3A%20Zecheng%20Tang%20and%20Baibei%20Ji%20and%20Juntao%20Li%20and%20Lijun%20Wu%20and%20Haijia%20Gui%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Long-context%20models%20%28LCMs%29%20have%20demonstrated%20great%20potential%20in%20processing%0Along%20sequences%2C%20facilitating%20many%20real-world%20applications.%20The%20success%20of%20LCMs%0Acan%20be%20attributed%20to%20their%20ability%20to%20locate%20implicit%20critical%20information%0Awithin%20the%20context%20for%20further%20prediction.%20However%2C%20recent%20research%20reveals%0Athat%20LCMs%20are%20often%20susceptible%20to%20contextual%20noise%2C%20i.e.%2C%20irrelevant%20tokens%2C%0Athat%20can%20mislead%20model%20attention.%20In%20this%20paper%2C%20we%20conduct%20a%20fine-grained%0Aanalysis%20of%20the%20context%20noise%20and%20propose%20an%20effective%20metric%2C%20the%20Integrated%0AGradient%20%28IG%29%20score%2C%20to%20detect%20and%20quantify%20the%20noise%20information%20within%20the%0Acontext.%20Our%20findings%20reveal%20that%20even%20simple%20mitigation%20of%20detected%20context%0Anoise%20can%20substantially%20boost%20the%20model%27s%20attention%20on%20critical%20tokens%20and%0Abenefit%20subsequent%20predictions.%20Building%20on%20this%20insight%2C%20we%20propose%20Context%0ADenoising%20Training%20%28CDT%29%2C%20a%20straightforward%20yet%20effective%20training%20strategy%0Athat%20improves%20attention%20on%20critical%20tokens%20while%20reinforcing%20their%20influence%20on%0Amodel%20predictions.%20Extensive%20experiments%20across%20four%20tasks%2C%20under%20both%20context%0Awindow%20scaling%20and%20long-context%20alignment%20settings%2C%20demonstrate%20the%20superiority%0Aof%20CDT.%20Notably%2C%20when%20trained%20with%20CDT%2C%20an%20open-source%208B%20model%20can%20achieve%0Aperformance%20%2850.92%29%20comparable%20to%20GPT-4o%20%2851.00%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Long-context%2520Modeling%2520from%2520Context%2520Denoising%2520Perspective%26entry.906535625%3DZecheng%2520Tang%2520and%2520Baibei%2520Ji%2520and%2520Juntao%2520Li%2520and%2520Lijun%2520Wu%2520and%2520Haijia%2520Gui%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Long-context%2520models%2520%2528LCMs%2529%2520have%2520demonstrated%2520great%2520potential%2520in%2520processing%250Along%2520sequences%252C%2520facilitating%2520many%2520real-world%2520applications.%2520The%2520success%2520of%2520LCMs%250Acan%2520be%2520attributed%2520to%2520their%2520ability%2520to%2520locate%2520implicit%2520critical%2520information%250Awithin%2520the%2520context%2520for%2520further%2520prediction.%2520However%252C%2520recent%2520research%2520reveals%250Athat%2520LCMs%2520are%2520often%2520susceptible%2520to%2520contextual%2520noise%252C%2520i.e.%252C%2520irrelevant%2520tokens%252C%250Athat%2520can%2520mislead%2520model%2520attention.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520fine-grained%250Aanalysis%2520of%2520the%2520context%2520noise%2520and%2520propose%2520an%2520effective%2520metric%252C%2520the%2520Integrated%250AGradient%2520%2528IG%2529%2520score%252C%2520to%2520detect%2520and%2520quantify%2520the%2520noise%2520information%2520within%2520the%250Acontext.%2520Our%2520findings%2520reveal%2520that%2520even%2520simple%2520mitigation%2520of%2520detected%2520context%250Anoise%2520can%2520substantially%2520boost%2520the%2520model%2527s%2520attention%2520on%2520critical%2520tokens%2520and%250Abenefit%2520subsequent%2520predictions.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520Context%250ADenoising%2520Training%2520%2528CDT%2529%252C%2520a%2520straightforward%2520yet%2520effective%2520training%2520strategy%250Athat%2520improves%2520attention%2520on%2520critical%2520tokens%2520while%2520reinforcing%2520their%2520influence%2520on%250Amodel%2520predictions.%2520Extensive%2520experiments%2520across%2520four%2520tasks%252C%2520under%2520both%2520context%250Awindow%2520scaling%2520and%2520long-context%2520alignment%2520settings%252C%2520demonstrate%2520the%2520superiority%250Aof%2520CDT.%2520Notably%252C%2520when%2520trained%2520with%2520CDT%252C%2520an%2520open-source%25208B%2520model%2520can%2520achieve%250Aperformance%2520%252850.92%2529%2520comparable%2520to%2520GPT-4o%2520%252851.00%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Long-context%20Modeling%20from%20Context%20Denoising%20Perspective&entry.906535625=Zecheng%20Tang%20and%20Baibei%20Ji%20and%20Juntao%20Li%20and%20Lijun%20Wu%20and%20Haijia%20Gui%20and%20Min%20Zhang&entry.1292438233=%20%20Long-context%20models%20%28LCMs%29%20have%20demonstrated%20great%20potential%20in%20processing%0Along%20sequences%2C%20facilitating%20many%20real-world%20applications.%20The%20success%20of%20LCMs%0Acan%20be%20attributed%20to%20their%20ability%20to%20locate%20implicit%20critical%20information%0Awithin%20the%20context%20for%20further%20prediction.%20However%2C%20recent%20research%20reveals%0Athat%20LCMs%20are%20often%20susceptible%20to%20contextual%20noise%2C%20i.e.%2C%20irrelevant%20tokens%2C%0Athat%20can%20mislead%20model%20attention.%20In%20this%20paper%2C%20we%20conduct%20a%20fine-grained%0Aanalysis%20of%20the%20context%20noise%20and%20propose%20an%20effective%20metric%2C%20the%20Integrated%0AGradient%20%28IG%29%20score%2C%20to%20detect%20and%20quantify%20the%20noise%20information%20within%20the%0Acontext.%20Our%20findings%20reveal%20that%20even%20simple%20mitigation%20of%20detected%20context%0Anoise%20can%20substantially%20boost%20the%20model%27s%20attention%20on%20critical%20tokens%20and%0Abenefit%20subsequent%20predictions.%20Building%20on%20this%20insight%2C%20we%20propose%20Context%0ADenoising%20Training%20%28CDT%29%2C%20a%20straightforward%20yet%20effective%20training%20strategy%0Athat%20improves%20attention%20on%20critical%20tokens%20while%20reinforcing%20their%20influence%20on%0Amodel%20predictions.%20Extensive%20experiments%20across%20four%20tasks%2C%20under%20both%20context%0Awindow%20scaling%20and%20long-context%20alignment%20settings%2C%20demonstrate%20the%20superiority%0Aof%20CDT.%20Notably%2C%20when%20trained%20with%20CDT%2C%20an%20open-source%208B%20model%20can%20achieve%0Aperformance%20%2850.92%29%20comparable%20to%20GPT-4o%20%2851.00%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05862v1&entry.124074799=Read"},
{"title": "On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond", "author": "Chenxiao Yang and Cai Zhou and David Wipf and Zhiyuan Li", "abstract": "  This paper formally studies generation processes, including auto-regressive\nnext-token prediction and masked diffusion, that abstract beyond architectural\nspecifics. At this level of abstraction, we quantify their benefits and\nlimitations through measurable criteria such as computational hardness and\nlearnability. In particular, we demonstrate that allowing generation to proceed\nbeyond autoregression and current masked diffusion, with capabilities to\nrewrite and length-variable edit, can bring significant theoretical and\nempirical advantages, with important implications for frontier LLMs that aspire\nto tackle increasingly hard problems and work universally across domains beyond\nnatural language, such as coding and science.\n", "link": "http://arxiv.org/abs/2510.06190v1", "date": "2025-10-07", "relevancy": 2.2733, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5793}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5768}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Powerful%20Ways%20to%20Generate%3A%20Autoregression%2C%20Diffusion%2C%20and%20Beyond&body=Title%3A%20On%20Powerful%20Ways%20to%20Generate%3A%20Autoregression%2C%20Diffusion%2C%20and%20Beyond%0AAuthor%3A%20Chenxiao%20Yang%20and%20Cai%20Zhou%20and%20David%20Wipf%20and%20Zhiyuan%20Li%0AAbstract%3A%20%20%20This%20paper%20formally%20studies%20generation%20processes%2C%20including%20auto-regressive%0Anext-token%20prediction%20and%20masked%20diffusion%2C%20that%20abstract%20beyond%20architectural%0Aspecifics.%20At%20this%20level%20of%20abstraction%2C%20we%20quantify%20their%20benefits%20and%0Alimitations%20through%20measurable%20criteria%20such%20as%20computational%20hardness%20and%0Alearnability.%20In%20particular%2C%20we%20demonstrate%20that%20allowing%20generation%20to%20proceed%0Abeyond%20autoregression%20and%20current%20masked%20diffusion%2C%20with%20capabilities%20to%0Arewrite%20and%20length-variable%20edit%2C%20can%20bring%20significant%20theoretical%20and%0Aempirical%20advantages%2C%20with%20important%20implications%20for%20frontier%20LLMs%20that%20aspire%0Ato%20tackle%20increasingly%20hard%20problems%20and%20work%20universally%20across%20domains%20beyond%0Anatural%20language%2C%20such%20as%20coding%20and%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Powerful%2520Ways%2520to%2520Generate%253A%2520Autoregression%252C%2520Diffusion%252C%2520and%2520Beyond%26entry.906535625%3DChenxiao%2520Yang%2520and%2520Cai%2520Zhou%2520and%2520David%2520Wipf%2520and%2520Zhiyuan%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520formally%2520studies%2520generation%2520processes%252C%2520including%2520auto-regressive%250Anext-token%2520prediction%2520and%2520masked%2520diffusion%252C%2520that%2520abstract%2520beyond%2520architectural%250Aspecifics.%2520At%2520this%2520level%2520of%2520abstraction%252C%2520we%2520quantify%2520their%2520benefits%2520and%250Alimitations%2520through%2520measurable%2520criteria%2520such%2520as%2520computational%2520hardness%2520and%250Alearnability.%2520In%2520particular%252C%2520we%2520demonstrate%2520that%2520allowing%2520generation%2520to%2520proceed%250Abeyond%2520autoregression%2520and%2520current%2520masked%2520diffusion%252C%2520with%2520capabilities%2520to%250Arewrite%2520and%2520length-variable%2520edit%252C%2520can%2520bring%2520significant%2520theoretical%2520and%250Aempirical%2520advantages%252C%2520with%2520important%2520implications%2520for%2520frontier%2520LLMs%2520that%2520aspire%250Ato%2520tackle%2520increasingly%2520hard%2520problems%2520and%2520work%2520universally%2520across%2520domains%2520beyond%250Anatural%2520language%252C%2520such%2520as%2520coding%2520and%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Powerful%20Ways%20to%20Generate%3A%20Autoregression%2C%20Diffusion%2C%20and%20Beyond&entry.906535625=Chenxiao%20Yang%20and%20Cai%20Zhou%20and%20David%20Wipf%20and%20Zhiyuan%20Li&entry.1292438233=%20%20This%20paper%20formally%20studies%20generation%20processes%2C%20including%20auto-regressive%0Anext-token%20prediction%20and%20masked%20diffusion%2C%20that%20abstract%20beyond%20architectural%0Aspecifics.%20At%20this%20level%20of%20abstraction%2C%20we%20quantify%20their%20benefits%20and%0Alimitations%20through%20measurable%20criteria%20such%20as%20computational%20hardness%20and%0Alearnability.%20In%20particular%2C%20we%20demonstrate%20that%20allowing%20generation%20to%20proceed%0Abeyond%20autoregression%20and%20current%20masked%20diffusion%2C%20with%20capabilities%20to%0Arewrite%20and%20length-variable%20edit%2C%20can%20bring%20significant%20theoretical%20and%0Aempirical%20advantages%2C%20with%20important%20implications%20for%20frontier%20LLMs%20that%20aspire%0Ato%20tackle%20increasingly%20hard%20problems%20and%20work%20universally%20across%20domains%20beyond%0Anatural%20language%2C%20such%20as%20coding%20and%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06190v1&entry.124074799=Read"},
{"title": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "author": "Jiwan Chung and Junhyeok Kim and Siyeol Kim and Jaeyoung Lee and Min Soo Kim and Youngjae Yu", "abstract": "  When thinking with images, humans rarely rely on a single glance: they\nrevisit visual information repeatedly during reasoning. However, existing\nmodels typically process images only once and thereafter generate reasoning\nentirely in text, lacking mechanisms to re-access or ground inference in visual\nrepresentations. We empirically confirm this: as reasoning chains lengthen,\nmodels progressively lose focus on relevant regions. In response, we introduce\nv1, a lightweight extension that enables active visual referencing through a\nsimple point-and-copy approach. This allows the model to identify relevant\nimage patches and copy their embeddings back into the reasoning stream,\nensuring that evolving hypotheses remain grounded in perceptual evidence.\nCrucially, our pointing strategy lets the MLLM directly select image patches\nusing their semantic representations as keys, keeping perceptual evidence\nembedded in the same space as the model's reasoning. To train this capability,\nwe construct v1g, a dataset of 300K multimodal reasoning traces with\ninterleaved visual grounding annotations. Across various multimodal\nmathematical reasoning benchmarks, v1 consistently outperforms comparable\nbaselines, establishing point-and-copy as a practical mechanism for grounded\nreasoning. The model checkpoint and dataset are available at\ngithub.com/jun297/v1.\n", "link": "http://arxiv.org/abs/2505.18842v4", "date": "2025-10-07", "relevancy": 2.2587, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20v1%3A%20Learning%20to%20Point%20Visual%20Tokens%20for%20Multimodal%20Grounded%20Reasoning&body=Title%3A%20v1%3A%20Learning%20to%20Point%20Visual%20Tokens%20for%20Multimodal%20Grounded%20Reasoning%0AAuthor%3A%20Jiwan%20Chung%20and%20Junhyeok%20Kim%20and%20Siyeol%20Kim%20and%20Jaeyoung%20Lee%20and%20Min%20Soo%20Kim%20and%20Youngjae%20Yu%0AAbstract%3A%20%20%20When%20thinking%20with%20images%2C%20humans%20rarely%20rely%20on%20a%20single%20glance%3A%20they%0Arevisit%20visual%20information%20repeatedly%20during%20reasoning.%20However%2C%20existing%0Amodels%20typically%20process%20images%20only%20once%20and%20thereafter%20generate%20reasoning%0Aentirely%20in%20text%2C%20lacking%20mechanisms%20to%20re-access%20or%20ground%20inference%20in%20visual%0Arepresentations.%20We%20empirically%20confirm%20this%3A%20as%20reasoning%20chains%20lengthen%2C%0Amodels%20progressively%20lose%20focus%20on%20relevant%20regions.%20In%20response%2C%20we%20introduce%0Av1%2C%20a%20lightweight%20extension%20that%20enables%20active%20visual%20referencing%20through%20a%0Asimple%20point-and-copy%20approach.%20This%20allows%20the%20model%20to%20identify%20relevant%0Aimage%20patches%20and%20copy%20their%20embeddings%20back%20into%20the%20reasoning%20stream%2C%0Aensuring%20that%20evolving%20hypotheses%20remain%20grounded%20in%20perceptual%20evidence.%0ACrucially%2C%20our%20pointing%20strategy%20lets%20the%20MLLM%20directly%20select%20image%20patches%0Ausing%20their%20semantic%20representations%20as%20keys%2C%20keeping%20perceptual%20evidence%0Aembedded%20in%20the%20same%20space%20as%20the%20model%27s%20reasoning.%20To%20train%20this%20capability%2C%0Awe%20construct%20v1g%2C%20a%20dataset%20of%20300K%20multimodal%20reasoning%20traces%20with%0Ainterleaved%20visual%20grounding%20annotations.%20Across%20various%20multimodal%0Amathematical%20reasoning%20benchmarks%2C%20v1%20consistently%20outperforms%20comparable%0Abaselines%2C%20establishing%20point-and-copy%20as%20a%20practical%20mechanism%20for%20grounded%0Areasoning.%20The%20model%20checkpoint%20and%20dataset%20are%20available%20at%0Agithub.com/jun297/v1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18842v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dv1%253A%2520Learning%2520to%2520Point%2520Visual%2520Tokens%2520for%2520Multimodal%2520Grounded%2520Reasoning%26entry.906535625%3DJiwan%2520Chung%2520and%2520Junhyeok%2520Kim%2520and%2520Siyeol%2520Kim%2520and%2520Jaeyoung%2520Lee%2520and%2520Min%2520Soo%2520Kim%2520and%2520Youngjae%2520Yu%26entry.1292438233%3D%2520%2520When%2520thinking%2520with%2520images%252C%2520humans%2520rarely%2520rely%2520on%2520a%2520single%2520glance%253A%2520they%250Arevisit%2520visual%2520information%2520repeatedly%2520during%2520reasoning.%2520However%252C%2520existing%250Amodels%2520typically%2520process%2520images%2520only%2520once%2520and%2520thereafter%2520generate%2520reasoning%250Aentirely%2520in%2520text%252C%2520lacking%2520mechanisms%2520to%2520re-access%2520or%2520ground%2520inference%2520in%2520visual%250Arepresentations.%2520We%2520empirically%2520confirm%2520this%253A%2520as%2520reasoning%2520chains%2520lengthen%252C%250Amodels%2520progressively%2520lose%2520focus%2520on%2520relevant%2520regions.%2520In%2520response%252C%2520we%2520introduce%250Av1%252C%2520a%2520lightweight%2520extension%2520that%2520enables%2520active%2520visual%2520referencing%2520through%2520a%250Asimple%2520point-and-copy%2520approach.%2520This%2520allows%2520the%2520model%2520to%2520identify%2520relevant%250Aimage%2520patches%2520and%2520copy%2520their%2520embeddings%2520back%2520into%2520the%2520reasoning%2520stream%252C%250Aensuring%2520that%2520evolving%2520hypotheses%2520remain%2520grounded%2520in%2520perceptual%2520evidence.%250ACrucially%252C%2520our%2520pointing%2520strategy%2520lets%2520the%2520MLLM%2520directly%2520select%2520image%2520patches%250Ausing%2520their%2520semantic%2520representations%2520as%2520keys%252C%2520keeping%2520perceptual%2520evidence%250Aembedded%2520in%2520the%2520same%2520space%2520as%2520the%2520model%2527s%2520reasoning.%2520To%2520train%2520this%2520capability%252C%250Awe%2520construct%2520v1g%252C%2520a%2520dataset%2520of%2520300K%2520multimodal%2520reasoning%2520traces%2520with%250Ainterleaved%2520visual%2520grounding%2520annotations.%2520Across%2520various%2520multimodal%250Amathematical%2520reasoning%2520benchmarks%252C%2520v1%2520consistently%2520outperforms%2520comparable%250Abaselines%252C%2520establishing%2520point-and-copy%2520as%2520a%2520practical%2520mechanism%2520for%2520grounded%250Areasoning.%2520The%2520model%2520checkpoint%2520and%2520dataset%2520are%2520available%2520at%250Agithub.com/jun297/v1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18842v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=v1%3A%20Learning%20to%20Point%20Visual%20Tokens%20for%20Multimodal%20Grounded%20Reasoning&entry.906535625=Jiwan%20Chung%20and%20Junhyeok%20Kim%20and%20Siyeol%20Kim%20and%20Jaeyoung%20Lee%20and%20Min%20Soo%20Kim%20and%20Youngjae%20Yu&entry.1292438233=%20%20When%20thinking%20with%20images%2C%20humans%20rarely%20rely%20on%20a%20single%20glance%3A%20they%0Arevisit%20visual%20information%20repeatedly%20during%20reasoning.%20However%2C%20existing%0Amodels%20typically%20process%20images%20only%20once%20and%20thereafter%20generate%20reasoning%0Aentirely%20in%20text%2C%20lacking%20mechanisms%20to%20re-access%20or%20ground%20inference%20in%20visual%0Arepresentations.%20We%20empirically%20confirm%20this%3A%20as%20reasoning%20chains%20lengthen%2C%0Amodels%20progressively%20lose%20focus%20on%20relevant%20regions.%20In%20response%2C%20we%20introduce%0Av1%2C%20a%20lightweight%20extension%20that%20enables%20active%20visual%20referencing%20through%20a%0Asimple%20point-and-copy%20approach.%20This%20allows%20the%20model%20to%20identify%20relevant%0Aimage%20patches%20and%20copy%20their%20embeddings%20back%20into%20the%20reasoning%20stream%2C%0Aensuring%20that%20evolving%20hypotheses%20remain%20grounded%20in%20perceptual%20evidence.%0ACrucially%2C%20our%20pointing%20strategy%20lets%20the%20MLLM%20directly%20select%20image%20patches%0Ausing%20their%20semantic%20representations%20as%20keys%2C%20keeping%20perceptual%20evidence%0Aembedded%20in%20the%20same%20space%20as%20the%20model%27s%20reasoning.%20To%20train%20this%20capability%2C%0Awe%20construct%20v1g%2C%20a%20dataset%20of%20300K%20multimodal%20reasoning%20traces%20with%0Ainterleaved%20visual%20grounding%20annotations.%20Across%20various%20multimodal%0Amathematical%20reasoning%20benchmarks%2C%20v1%20consistently%20outperforms%20comparable%0Abaselines%2C%20establishing%20point-and-copy%20as%20a%20practical%20mechanism%20for%20grounded%0Areasoning.%20The%20model%20checkpoint%20and%20dataset%20are%20available%20at%0Agithub.com/jun297/v1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18842v4&entry.124074799=Read"},
{"title": "Redefining Generalization in Visual Domains: A Two-Axis Framework for\n  Fake Image Detection with FusionDetect", "author": "Amirtaha Amanzadi and Zahra Dehghanian and Hamid Beigy and Hamid R. Rabiee", "abstract": "  The rapid development of generative models has made it increasingly crucial\nto develop detectors that can reliably detect synthetic images. Although most\nof the work has now focused on cross-generator generalization, we argue that\nthis viewpoint is too limited. Detecting synthetic images involves another\nequally important challenge: generalization across visual domains. To bridge\nthis gap,we present the OmniGen Benchmark. This comprehensive evaluation\ndataset incorporates 12 state-of-the-art generators, providing a more realistic\nway of evaluating detector performance under realistic conditions. In addition,\nwe introduce a new method, FusionDetect, aimed at addressing both vectors of\ngeneralization. FusionDetect draws on the benefits of two frozen foundation\nmodels: CLIP & Dinov2. By deriving features from both complementary models,we\ndevelop a cohesive feature space that naturally adapts to changes in both\nthecontent and design of the generator. Our extensive experiments demonstrate\nthat FusionDetect delivers not only a new state-of-the-art, which is 3.87% more\naccurate than its closest competitor and 6.13% more precise on average on\nestablished benchmarks, but also achieves a 4.48% increase in accuracy on\nOmniGen,along with exceptional robustness to common image perturbations. We\nintroduce not only a top-performing detector, but also a new benchmark and\nframework for furthering universal AI image detection. The code and dataset are\navailable at http://github.com/amir-aman/FusionDetect\n", "link": "http://arxiv.org/abs/2510.05740v1", "date": "2025-10-07", "relevancy": 2.2464, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5776}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5645}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redefining%20Generalization%20in%20Visual%20Domains%3A%20A%20Two-Axis%20Framework%20for%0A%20%20Fake%20Image%20Detection%20with%20FusionDetect&body=Title%3A%20Redefining%20Generalization%20in%20Visual%20Domains%3A%20A%20Two-Axis%20Framework%20for%0A%20%20Fake%20Image%20Detection%20with%20FusionDetect%0AAuthor%3A%20Amirtaha%20Amanzadi%20and%20Zahra%20Dehghanian%20and%20Hamid%20Beigy%20and%20Hamid%20R.%20Rabiee%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20generative%20models%20has%20made%20it%20increasingly%20crucial%0Ato%20develop%20detectors%20that%20can%20reliably%20detect%20synthetic%20images.%20Although%20most%0Aof%20the%20work%20has%20now%20focused%20on%20cross-generator%20generalization%2C%20we%20argue%20that%0Athis%20viewpoint%20is%20too%20limited.%20Detecting%20synthetic%20images%20involves%20another%0Aequally%20important%20challenge%3A%20generalization%20across%20visual%20domains.%20To%20bridge%0Athis%20gap%2Cwe%20present%20the%20OmniGen%20Benchmark.%20This%20comprehensive%20evaluation%0Adataset%20incorporates%2012%20state-of-the-art%20generators%2C%20providing%20a%20more%20realistic%0Away%20of%20evaluating%20detector%20performance%20under%20realistic%20conditions.%20In%20addition%2C%0Awe%20introduce%20a%20new%20method%2C%20FusionDetect%2C%20aimed%20at%20addressing%20both%20vectors%20of%0Ageneralization.%20FusionDetect%20draws%20on%20the%20benefits%20of%20two%20frozen%20foundation%0Amodels%3A%20CLIP%20%26%20Dinov2.%20By%20deriving%20features%20from%20both%20complementary%20models%2Cwe%0Adevelop%20a%20cohesive%20feature%20space%20that%20naturally%20adapts%20to%20changes%20in%20both%0Athecontent%20and%20design%20of%20the%20generator.%20Our%20extensive%20experiments%20demonstrate%0Athat%20FusionDetect%20delivers%20not%20only%20a%20new%20state-of-the-art%2C%20which%20is%203.87%25%20more%0Aaccurate%20than%20its%20closest%20competitor%20and%206.13%25%20more%20precise%20on%20average%20on%0Aestablished%20benchmarks%2C%20but%20also%20achieves%20a%204.48%25%20increase%20in%20accuracy%20on%0AOmniGen%2Calong%20with%20exceptional%20robustness%20to%20common%20image%20perturbations.%20We%0Aintroduce%20not%20only%20a%20top-performing%20detector%2C%20but%20also%20a%20new%20benchmark%20and%0Aframework%20for%20furthering%20universal%20AI%20image%20detection.%20The%20code%20and%20dataset%20are%0Aavailable%20at%20http%3A//github.com/amir-aman/FusionDetect%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedefining%2520Generalization%2520in%2520Visual%2520Domains%253A%2520A%2520Two-Axis%2520Framework%2520for%250A%2520%2520Fake%2520Image%2520Detection%2520with%2520FusionDetect%26entry.906535625%3DAmirtaha%2520Amanzadi%2520and%2520Zahra%2520Dehghanian%2520and%2520Hamid%2520Beigy%2520and%2520Hamid%2520R.%2520Rabiee%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520generative%2520models%2520has%2520made%2520it%2520increasingly%2520crucial%250Ato%2520develop%2520detectors%2520that%2520can%2520reliably%2520detect%2520synthetic%2520images.%2520Although%2520most%250Aof%2520the%2520work%2520has%2520now%2520focused%2520on%2520cross-generator%2520generalization%252C%2520we%2520argue%2520that%250Athis%2520viewpoint%2520is%2520too%2520limited.%2520Detecting%2520synthetic%2520images%2520involves%2520another%250Aequally%2520important%2520challenge%253A%2520generalization%2520across%2520visual%2520domains.%2520To%2520bridge%250Athis%2520gap%252Cwe%2520present%2520the%2520OmniGen%2520Benchmark.%2520This%2520comprehensive%2520evaluation%250Adataset%2520incorporates%252012%2520state-of-the-art%2520generators%252C%2520providing%2520a%2520more%2520realistic%250Away%2520of%2520evaluating%2520detector%2520performance%2520under%2520realistic%2520conditions.%2520In%2520addition%252C%250Awe%2520introduce%2520a%2520new%2520method%252C%2520FusionDetect%252C%2520aimed%2520at%2520addressing%2520both%2520vectors%2520of%250Ageneralization.%2520FusionDetect%2520draws%2520on%2520the%2520benefits%2520of%2520two%2520frozen%2520foundation%250Amodels%253A%2520CLIP%2520%2526%2520Dinov2.%2520By%2520deriving%2520features%2520from%2520both%2520complementary%2520models%252Cwe%250Adevelop%2520a%2520cohesive%2520feature%2520space%2520that%2520naturally%2520adapts%2520to%2520changes%2520in%2520both%250Athecontent%2520and%2520design%2520of%2520the%2520generator.%2520Our%2520extensive%2520experiments%2520demonstrate%250Athat%2520FusionDetect%2520delivers%2520not%2520only%2520a%2520new%2520state-of-the-art%252C%2520which%2520is%25203.87%2525%2520more%250Aaccurate%2520than%2520its%2520closest%2520competitor%2520and%25206.13%2525%2520more%2520precise%2520on%2520average%2520on%250Aestablished%2520benchmarks%252C%2520but%2520also%2520achieves%2520a%25204.48%2525%2520increase%2520in%2520accuracy%2520on%250AOmniGen%252Calong%2520with%2520exceptional%2520robustness%2520to%2520common%2520image%2520perturbations.%2520We%250Aintroduce%2520not%2520only%2520a%2520top-performing%2520detector%252C%2520but%2520also%2520a%2520new%2520benchmark%2520and%250Aframework%2520for%2520furthering%2520universal%2520AI%2520image%2520detection.%2520The%2520code%2520and%2520dataset%2520are%250Aavailable%2520at%2520http%253A//github.com/amir-aman/FusionDetect%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redefining%20Generalization%20in%20Visual%20Domains%3A%20A%20Two-Axis%20Framework%20for%0A%20%20Fake%20Image%20Detection%20with%20FusionDetect&entry.906535625=Amirtaha%20Amanzadi%20and%20Zahra%20Dehghanian%20and%20Hamid%20Beigy%20and%20Hamid%20R.%20Rabiee&entry.1292438233=%20%20The%20rapid%20development%20of%20generative%20models%20has%20made%20it%20increasingly%20crucial%0Ato%20develop%20detectors%20that%20can%20reliably%20detect%20synthetic%20images.%20Although%20most%0Aof%20the%20work%20has%20now%20focused%20on%20cross-generator%20generalization%2C%20we%20argue%20that%0Athis%20viewpoint%20is%20too%20limited.%20Detecting%20synthetic%20images%20involves%20another%0Aequally%20important%20challenge%3A%20generalization%20across%20visual%20domains.%20To%20bridge%0Athis%20gap%2Cwe%20present%20the%20OmniGen%20Benchmark.%20This%20comprehensive%20evaluation%0Adataset%20incorporates%2012%20state-of-the-art%20generators%2C%20providing%20a%20more%20realistic%0Away%20of%20evaluating%20detector%20performance%20under%20realistic%20conditions.%20In%20addition%2C%0Awe%20introduce%20a%20new%20method%2C%20FusionDetect%2C%20aimed%20at%20addressing%20both%20vectors%20of%0Ageneralization.%20FusionDetect%20draws%20on%20the%20benefits%20of%20two%20frozen%20foundation%0Amodels%3A%20CLIP%20%26%20Dinov2.%20By%20deriving%20features%20from%20both%20complementary%20models%2Cwe%0Adevelop%20a%20cohesive%20feature%20space%20that%20naturally%20adapts%20to%20changes%20in%20both%0Athecontent%20and%20design%20of%20the%20generator.%20Our%20extensive%20experiments%20demonstrate%0Athat%20FusionDetect%20delivers%20not%20only%20a%20new%20state-of-the-art%2C%20which%20is%203.87%25%20more%0Aaccurate%20than%20its%20closest%20competitor%20and%206.13%25%20more%20precise%20on%20average%20on%0Aestablished%20benchmarks%2C%20but%20also%20achieves%20a%204.48%25%20increase%20in%20accuracy%20on%0AOmniGen%2Calong%20with%20exceptional%20robustness%20to%20common%20image%20perturbations.%20We%0Aintroduce%20not%20only%20a%20top-performing%20detector%2C%20but%20also%20a%20new%20benchmark%20and%0Aframework%20for%20furthering%20universal%20AI%20image%20detection.%20The%20code%20and%20dataset%20are%0Aavailable%20at%20http%3A//github.com/amir-aman/FusionDetect%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05740v1&entry.124074799=Read"},
{"title": "Probing the Difficulty Perception Mechanism of Large Language Models", "author": "Sunbowen Lee and Qingyu Yin and Chak Tou Leong and Jialiang Zhang and Yicheng Gong and Xiaoyu Shen", "abstract": "  Large language models (LLMs) are increasingly deployed on complex reasoning\ntasks, yet little is known about their ability to internally evaluate problem\ndifficulty, which is an essential capability for adaptive reasoning and\nefficient resource allocation. In this work, we investigate whether LLMs\nimplicitly encode problem difficulty in their internal representations. Using a\nlinear probe on the final-token representations of LLMs, we demonstrate that\nthe difficulty level of math problems can be linearly modeled. We further\nlocate the specific attention heads of the final Transformer layer: these\nattention heads have opposite activation patterns for simple and difficult\nproblems, thus achieving perception of difficulty. Our ablation experiments\nprove the accuracy of the location. Crucially, our experiments provide\npractical support for using LLMs as automatic difficulty annotators,\npotentially substantially reducing reliance on costly human labeling in\nbenchmark construction and curriculum learning. We also uncover that there is a\nsignificant difference in entropy and difficulty perception at the token level.\nOur study reveals that difficulty perception in LLMs is not only present but\nalso structurally organized, offering new theoretical insights and practical\ndirections for future research.\n", "link": "http://arxiv.org/abs/2510.05969v1", "date": "2025-10-07", "relevancy": 2.2418, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20the%20Difficulty%20Perception%20Mechanism%20of%20Large%20Language%20Models&body=Title%3A%20Probing%20the%20Difficulty%20Perception%20Mechanism%20of%20Large%20Language%20Models%0AAuthor%3A%20Sunbowen%20Lee%20and%20Qingyu%20Yin%20and%20Chak%20Tou%20Leong%20and%20Jialiang%20Zhang%20and%20Yicheng%20Gong%20and%20Xiaoyu%20Shen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20on%20complex%20reasoning%0Atasks%2C%20yet%20little%20is%20known%20about%20their%20ability%20to%20internally%20evaluate%20problem%0Adifficulty%2C%20which%20is%20an%20essential%20capability%20for%20adaptive%20reasoning%20and%0Aefficient%20resource%20allocation.%20In%20this%20work%2C%20we%20investigate%20whether%20LLMs%0Aimplicitly%20encode%20problem%20difficulty%20in%20their%20internal%20representations.%20Using%20a%0Alinear%20probe%20on%20the%20final-token%20representations%20of%20LLMs%2C%20we%20demonstrate%20that%0Athe%20difficulty%20level%20of%20math%20problems%20can%20be%20linearly%20modeled.%20We%20further%0Alocate%20the%20specific%20attention%20heads%20of%20the%20final%20Transformer%20layer%3A%20these%0Aattention%20heads%20have%20opposite%20activation%20patterns%20for%20simple%20and%20difficult%0Aproblems%2C%20thus%20achieving%20perception%20of%20difficulty.%20Our%20ablation%20experiments%0Aprove%20the%20accuracy%20of%20the%20location.%20Crucially%2C%20our%20experiments%20provide%0Apractical%20support%20for%20using%20LLMs%20as%20automatic%20difficulty%20annotators%2C%0Apotentially%20substantially%20reducing%20reliance%20on%20costly%20human%20labeling%20in%0Abenchmark%20construction%20and%20curriculum%20learning.%20We%20also%20uncover%20that%20there%20is%20a%0Asignificant%20difference%20in%20entropy%20and%20difficulty%20perception%20at%20the%20token%20level.%0AOur%20study%20reveals%20that%20difficulty%20perception%20in%20LLMs%20is%20not%20only%20present%20but%0Aalso%20structurally%20organized%2C%20offering%20new%20theoretical%20insights%20and%20practical%0Adirections%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520the%2520Difficulty%2520Perception%2520Mechanism%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DSunbowen%2520Lee%2520and%2520Qingyu%2520Yin%2520and%2520Chak%2520Tou%2520Leong%2520and%2520Jialiang%2520Zhang%2520and%2520Yicheng%2520Gong%2520and%2520Xiaoyu%2520Shen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520on%2520complex%2520reasoning%250Atasks%252C%2520yet%2520little%2520is%2520known%2520about%2520their%2520ability%2520to%2520internally%2520evaluate%2520problem%250Adifficulty%252C%2520which%2520is%2520an%2520essential%2520capability%2520for%2520adaptive%2520reasoning%2520and%250Aefficient%2520resource%2520allocation.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%2520LLMs%250Aimplicitly%2520encode%2520problem%2520difficulty%2520in%2520their%2520internal%2520representations.%2520Using%2520a%250Alinear%2520probe%2520on%2520the%2520final-token%2520representations%2520of%2520LLMs%252C%2520we%2520demonstrate%2520that%250Athe%2520difficulty%2520level%2520of%2520math%2520problems%2520can%2520be%2520linearly%2520modeled.%2520We%2520further%250Alocate%2520the%2520specific%2520attention%2520heads%2520of%2520the%2520final%2520Transformer%2520layer%253A%2520these%250Aattention%2520heads%2520have%2520opposite%2520activation%2520patterns%2520for%2520simple%2520and%2520difficult%250Aproblems%252C%2520thus%2520achieving%2520perception%2520of%2520difficulty.%2520Our%2520ablation%2520experiments%250Aprove%2520the%2520accuracy%2520of%2520the%2520location.%2520Crucially%252C%2520our%2520experiments%2520provide%250Apractical%2520support%2520for%2520using%2520LLMs%2520as%2520automatic%2520difficulty%2520annotators%252C%250Apotentially%2520substantially%2520reducing%2520reliance%2520on%2520costly%2520human%2520labeling%2520in%250Abenchmark%2520construction%2520and%2520curriculum%2520learning.%2520We%2520also%2520uncover%2520that%2520there%2520is%2520a%250Asignificant%2520difference%2520in%2520entropy%2520and%2520difficulty%2520perception%2520at%2520the%2520token%2520level.%250AOur%2520study%2520reveals%2520that%2520difficulty%2520perception%2520in%2520LLMs%2520is%2520not%2520only%2520present%2520but%250Aalso%2520structurally%2520organized%252C%2520offering%2520new%2520theoretical%2520insights%2520and%2520practical%250Adirections%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20the%20Difficulty%20Perception%20Mechanism%20of%20Large%20Language%20Models&entry.906535625=Sunbowen%20Lee%20and%20Qingyu%20Yin%20and%20Chak%20Tou%20Leong%20and%20Jialiang%20Zhang%20and%20Yicheng%20Gong%20and%20Xiaoyu%20Shen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20on%20complex%20reasoning%0Atasks%2C%20yet%20little%20is%20known%20about%20their%20ability%20to%20internally%20evaluate%20problem%0Adifficulty%2C%20which%20is%20an%20essential%20capability%20for%20adaptive%20reasoning%20and%0Aefficient%20resource%20allocation.%20In%20this%20work%2C%20we%20investigate%20whether%20LLMs%0Aimplicitly%20encode%20problem%20difficulty%20in%20their%20internal%20representations.%20Using%20a%0Alinear%20probe%20on%20the%20final-token%20representations%20of%20LLMs%2C%20we%20demonstrate%20that%0Athe%20difficulty%20level%20of%20math%20problems%20can%20be%20linearly%20modeled.%20We%20further%0Alocate%20the%20specific%20attention%20heads%20of%20the%20final%20Transformer%20layer%3A%20these%0Aattention%20heads%20have%20opposite%20activation%20patterns%20for%20simple%20and%20difficult%0Aproblems%2C%20thus%20achieving%20perception%20of%20difficulty.%20Our%20ablation%20experiments%0Aprove%20the%20accuracy%20of%20the%20location.%20Crucially%2C%20our%20experiments%20provide%0Apractical%20support%20for%20using%20LLMs%20as%20automatic%20difficulty%20annotators%2C%0Apotentially%20substantially%20reducing%20reliance%20on%20costly%20human%20labeling%20in%0Abenchmark%20construction%20and%20curriculum%20learning.%20We%20also%20uncover%20that%20there%20is%20a%0Asignificant%20difference%20in%20entropy%20and%20difficulty%20perception%20at%20the%20token%20level.%0AOur%20study%20reveals%20that%20difficulty%20perception%20in%20LLMs%20is%20not%20only%20present%20but%0Aalso%20structurally%20organized%2C%20offering%20new%20theoretical%20insights%20and%20practical%0Adirections%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05969v1&entry.124074799=Read"},
{"title": "Hierarchical Reasoning Models: Perspectives and Misconceptions", "author": "Renee Ge and Qianli Liao and Tomaso Poggio", "abstract": "  Transformers have demonstrated remarkable performance in natural language\nprocessing and related domains, as they largely focus on sequential,\nautoregressive next-token prediction tasks. Yet, they struggle in logical\nreasoning, not necessarily because of a fundamental limitation of these models,\nbut possibly due to the lack of exploration of more creative uses, such as\nlatent space and recurrent reasoning. An emerging exploration in this direction\nis the Hierarchical Reasoning Model (Wang et. al., 2025), which introduces a\nnovel type of recurrent reasoning in the latent space of transformers,\nachieving remarkable performance on a wide range of 2D reasoning tasks. Despite\nthe promising results, this line of models is still at an early stage and calls\nfor in-depth investigation. In this work, we review this class of models,\nexamine key design choices, test alternative variants and clarify common\nmisconceptions.\n", "link": "http://arxiv.org/abs/2510.00355v2", "date": "2025-10-07", "relevancy": 2.2366, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Reasoning%20Models%3A%20Perspectives%20and%20Misconceptions&body=Title%3A%20Hierarchical%20Reasoning%20Models%3A%20Perspectives%20and%20Misconceptions%0AAuthor%3A%20Renee%20Ge%20and%20Qianli%20Liao%20and%20Tomaso%20Poggio%0AAbstract%3A%20%20%20Transformers%20have%20demonstrated%20remarkable%20performance%20in%20natural%20language%0Aprocessing%20and%20related%20domains%2C%20as%20they%20largely%20focus%20on%20sequential%2C%0Aautoregressive%20next-token%20prediction%20tasks.%20Yet%2C%20they%20struggle%20in%20logical%0Areasoning%2C%20not%20necessarily%20because%20of%20a%20fundamental%20limitation%20of%20these%20models%2C%0Abut%20possibly%20due%20to%20the%20lack%20of%20exploration%20of%20more%20creative%20uses%2C%20such%20as%0Alatent%20space%20and%20recurrent%20reasoning.%20An%20emerging%20exploration%20in%20this%20direction%0Ais%20the%20Hierarchical%20Reasoning%20Model%20%28Wang%20et.%20al.%2C%202025%29%2C%20which%20introduces%20a%0Anovel%20type%20of%20recurrent%20reasoning%20in%20the%20latent%20space%20of%20transformers%2C%0Aachieving%20remarkable%20performance%20on%20a%20wide%20range%20of%202D%20reasoning%20tasks.%20Despite%0Athe%20promising%20results%2C%20this%20line%20of%20models%20is%20still%20at%20an%20early%20stage%20and%20calls%0Afor%20in-depth%20investigation.%20In%20this%20work%2C%20we%20review%20this%20class%20of%20models%2C%0Aexamine%20key%20design%20choices%2C%20test%20alternative%20variants%20and%20clarify%20common%0Amisconceptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Reasoning%2520Models%253A%2520Perspectives%2520and%2520Misconceptions%26entry.906535625%3DRenee%2520Ge%2520and%2520Qianli%2520Liao%2520and%2520Tomaso%2520Poggio%26entry.1292438233%3D%2520%2520Transformers%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520natural%2520language%250Aprocessing%2520and%2520related%2520domains%252C%2520as%2520they%2520largely%2520focus%2520on%2520sequential%252C%250Aautoregressive%2520next-token%2520prediction%2520tasks.%2520Yet%252C%2520they%2520struggle%2520in%2520logical%250Areasoning%252C%2520not%2520necessarily%2520because%2520of%2520a%2520fundamental%2520limitation%2520of%2520these%2520models%252C%250Abut%2520possibly%2520due%2520to%2520the%2520lack%2520of%2520exploration%2520of%2520more%2520creative%2520uses%252C%2520such%2520as%250Alatent%2520space%2520and%2520recurrent%2520reasoning.%2520An%2520emerging%2520exploration%2520in%2520this%2520direction%250Ais%2520the%2520Hierarchical%2520Reasoning%2520Model%2520%2528Wang%2520et.%2520al.%252C%25202025%2529%252C%2520which%2520introduces%2520a%250Anovel%2520type%2520of%2520recurrent%2520reasoning%2520in%2520the%2520latent%2520space%2520of%2520transformers%252C%250Aachieving%2520remarkable%2520performance%2520on%2520a%2520wide%2520range%2520of%25202D%2520reasoning%2520tasks.%2520Despite%250Athe%2520promising%2520results%252C%2520this%2520line%2520of%2520models%2520is%2520still%2520at%2520an%2520early%2520stage%2520and%2520calls%250Afor%2520in-depth%2520investigation.%2520In%2520this%2520work%252C%2520we%2520review%2520this%2520class%2520of%2520models%252C%250Aexamine%2520key%2520design%2520choices%252C%2520test%2520alternative%2520variants%2520and%2520clarify%2520common%250Amisconceptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Reasoning%20Models%3A%20Perspectives%20and%20Misconceptions&entry.906535625=Renee%20Ge%20and%20Qianli%20Liao%20and%20Tomaso%20Poggio&entry.1292438233=%20%20Transformers%20have%20demonstrated%20remarkable%20performance%20in%20natural%20language%0Aprocessing%20and%20related%20domains%2C%20as%20they%20largely%20focus%20on%20sequential%2C%0Aautoregressive%20next-token%20prediction%20tasks.%20Yet%2C%20they%20struggle%20in%20logical%0Areasoning%2C%20not%20necessarily%20because%20of%20a%20fundamental%20limitation%20of%20these%20models%2C%0Abut%20possibly%20due%20to%20the%20lack%20of%20exploration%20of%20more%20creative%20uses%2C%20such%20as%0Alatent%20space%20and%20recurrent%20reasoning.%20An%20emerging%20exploration%20in%20this%20direction%0Ais%20the%20Hierarchical%20Reasoning%20Model%20%28Wang%20et.%20al.%2C%202025%29%2C%20which%20introduces%20a%0Anovel%20type%20of%20recurrent%20reasoning%20in%20the%20latent%20space%20of%20transformers%2C%0Aachieving%20remarkable%20performance%20on%20a%20wide%20range%20of%202D%20reasoning%20tasks.%20Despite%0Athe%20promising%20results%2C%20this%20line%20of%20models%20is%20still%20at%20an%20early%20stage%20and%20calls%0Afor%20in-depth%20investigation.%20In%20this%20work%2C%20we%20review%20this%20class%20of%20models%2C%0Aexamine%20key%20design%20choices%2C%20test%20alternative%20variants%20and%20clarify%20common%0Amisconceptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00355v2&entry.124074799=Read"},
{"title": "Human-in-the-loop Optimisation in Robot-assisted Gait Training", "author": "Andreas Christou and Andreas Sochopoulos and Elliot Lister and Sethu Vijayakumar", "abstract": "  Wearable robots offer a promising solution for quantitatively monitoring gait\nand providing systematic, adaptive assistance to promote patient independence\nand improve gait. However, due to significant interpersonal and intrapersonal\nvariability in walking patterns, it is important to design robot controllers\nthat can adapt to the unique characteristics of each individual. This paper\ninvestigates the potential of human-in-the-loop optimisation (HILO) to deliver\npersonalised assistance in gait training. The Covariance Matrix Adaptation\nEvolution Strategy (CMA-ES) was employed to continuously optimise an\nassist-as-needed controller of a lower-limb exoskeleton. Six healthy\nindividuals participated over a two-day experiment. Our results suggest that\nwhile the CMA-ES appears to converge to a unique set of stiffnesses for each\nindividual, no measurable impact on the subjects' performance was observed\nduring the validation trials. These findings highlight the impact of\nhuman-robot co-adaptation and human behaviour variability, whose effect may be\ngreater than potential benefits of personalising rule-based assistive\ncontrollers. Our work contributes to understanding the limitations of current\npersonalisation approaches in exoskeleton-assisted gait rehabilitation and\nidentifies key challenges for effective implementation of human-in-the-loop\noptimisation in this domain.\n", "link": "http://arxiv.org/abs/2510.05780v1", "date": "2025-10-07", "relevancy": 2.2356, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6105}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5584}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-in-the-loop%20Optimisation%20in%20Robot-assisted%20Gait%20Training&body=Title%3A%20Human-in-the-loop%20Optimisation%20in%20Robot-assisted%20Gait%20Training%0AAuthor%3A%20Andreas%20Christou%20and%20Andreas%20Sochopoulos%20and%20Elliot%20Lister%20and%20Sethu%20Vijayakumar%0AAbstract%3A%20%20%20Wearable%20robots%20offer%20a%20promising%20solution%20for%20quantitatively%20monitoring%20gait%0Aand%20providing%20systematic%2C%20adaptive%20assistance%20to%20promote%20patient%20independence%0Aand%20improve%20gait.%20However%2C%20due%20to%20significant%20interpersonal%20and%20intrapersonal%0Avariability%20in%20walking%20patterns%2C%20it%20is%20important%20to%20design%20robot%20controllers%0Athat%20can%20adapt%20to%20the%20unique%20characteristics%20of%20each%20individual.%20This%20paper%0Ainvestigates%20the%20potential%20of%20human-in-the-loop%20optimisation%20%28HILO%29%20to%20deliver%0Apersonalised%20assistance%20in%20gait%20training.%20The%20Covariance%20Matrix%20Adaptation%0AEvolution%20Strategy%20%28CMA-ES%29%20was%20employed%20to%20continuously%20optimise%20an%0Aassist-as-needed%20controller%20of%20a%20lower-limb%20exoskeleton.%20Six%20healthy%0Aindividuals%20participated%20over%20a%20two-day%20experiment.%20Our%20results%20suggest%20that%0Awhile%20the%20CMA-ES%20appears%20to%20converge%20to%20a%20unique%20set%20of%20stiffnesses%20for%20each%0Aindividual%2C%20no%20measurable%20impact%20on%20the%20subjects%27%20performance%20was%20observed%0Aduring%20the%20validation%20trials.%20These%20findings%20highlight%20the%20impact%20of%0Ahuman-robot%20co-adaptation%20and%20human%20behaviour%20variability%2C%20whose%20effect%20may%20be%0Agreater%20than%20potential%20benefits%20of%20personalising%20rule-based%20assistive%0Acontrollers.%20Our%20work%20contributes%20to%20understanding%20the%20limitations%20of%20current%0Apersonalisation%20approaches%20in%20exoskeleton-assisted%20gait%20rehabilitation%20and%0Aidentifies%20key%20challenges%20for%20effective%20implementation%20of%20human-in-the-loop%0Aoptimisation%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-in-the-loop%2520Optimisation%2520in%2520Robot-assisted%2520Gait%2520Training%26entry.906535625%3DAndreas%2520Christou%2520and%2520Andreas%2520Sochopoulos%2520and%2520Elliot%2520Lister%2520and%2520Sethu%2520Vijayakumar%26entry.1292438233%3D%2520%2520Wearable%2520robots%2520offer%2520a%2520promising%2520solution%2520for%2520quantitatively%2520monitoring%2520gait%250Aand%2520providing%2520systematic%252C%2520adaptive%2520assistance%2520to%2520promote%2520patient%2520independence%250Aand%2520improve%2520gait.%2520However%252C%2520due%2520to%2520significant%2520interpersonal%2520and%2520intrapersonal%250Avariability%2520in%2520walking%2520patterns%252C%2520it%2520is%2520important%2520to%2520design%2520robot%2520controllers%250Athat%2520can%2520adapt%2520to%2520the%2520unique%2520characteristics%2520of%2520each%2520individual.%2520This%2520paper%250Ainvestigates%2520the%2520potential%2520of%2520human-in-the-loop%2520optimisation%2520%2528HILO%2529%2520to%2520deliver%250Apersonalised%2520assistance%2520in%2520gait%2520training.%2520The%2520Covariance%2520Matrix%2520Adaptation%250AEvolution%2520Strategy%2520%2528CMA-ES%2529%2520was%2520employed%2520to%2520continuously%2520optimise%2520an%250Aassist-as-needed%2520controller%2520of%2520a%2520lower-limb%2520exoskeleton.%2520Six%2520healthy%250Aindividuals%2520participated%2520over%2520a%2520two-day%2520experiment.%2520Our%2520results%2520suggest%2520that%250Awhile%2520the%2520CMA-ES%2520appears%2520to%2520converge%2520to%2520a%2520unique%2520set%2520of%2520stiffnesses%2520for%2520each%250Aindividual%252C%2520no%2520measurable%2520impact%2520on%2520the%2520subjects%2527%2520performance%2520was%2520observed%250Aduring%2520the%2520validation%2520trials.%2520These%2520findings%2520highlight%2520the%2520impact%2520of%250Ahuman-robot%2520co-adaptation%2520and%2520human%2520behaviour%2520variability%252C%2520whose%2520effect%2520may%2520be%250Agreater%2520than%2520potential%2520benefits%2520of%2520personalising%2520rule-based%2520assistive%250Acontrollers.%2520Our%2520work%2520contributes%2520to%2520understanding%2520the%2520limitations%2520of%2520current%250Apersonalisation%2520approaches%2520in%2520exoskeleton-assisted%2520gait%2520rehabilitation%2520and%250Aidentifies%2520key%2520challenges%2520for%2520effective%2520implementation%2520of%2520human-in-the-loop%250Aoptimisation%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-in-the-loop%20Optimisation%20in%20Robot-assisted%20Gait%20Training&entry.906535625=Andreas%20Christou%20and%20Andreas%20Sochopoulos%20and%20Elliot%20Lister%20and%20Sethu%20Vijayakumar&entry.1292438233=%20%20Wearable%20robots%20offer%20a%20promising%20solution%20for%20quantitatively%20monitoring%20gait%0Aand%20providing%20systematic%2C%20adaptive%20assistance%20to%20promote%20patient%20independence%0Aand%20improve%20gait.%20However%2C%20due%20to%20significant%20interpersonal%20and%20intrapersonal%0Avariability%20in%20walking%20patterns%2C%20it%20is%20important%20to%20design%20robot%20controllers%0Athat%20can%20adapt%20to%20the%20unique%20characteristics%20of%20each%20individual.%20This%20paper%0Ainvestigates%20the%20potential%20of%20human-in-the-loop%20optimisation%20%28HILO%29%20to%20deliver%0Apersonalised%20assistance%20in%20gait%20training.%20The%20Covariance%20Matrix%20Adaptation%0AEvolution%20Strategy%20%28CMA-ES%29%20was%20employed%20to%20continuously%20optimise%20an%0Aassist-as-needed%20controller%20of%20a%20lower-limb%20exoskeleton.%20Six%20healthy%0Aindividuals%20participated%20over%20a%20two-day%20experiment.%20Our%20results%20suggest%20that%0Awhile%20the%20CMA-ES%20appears%20to%20converge%20to%20a%20unique%20set%20of%20stiffnesses%20for%20each%0Aindividual%2C%20no%20measurable%20impact%20on%20the%20subjects%27%20performance%20was%20observed%0Aduring%20the%20validation%20trials.%20These%20findings%20highlight%20the%20impact%20of%0Ahuman-robot%20co-adaptation%20and%20human%20behaviour%20variability%2C%20whose%20effect%20may%20be%0Agreater%20than%20potential%20benefits%20of%20personalising%20rule-based%20assistive%0Acontrollers.%20Our%20work%20contributes%20to%20understanding%20the%20limitations%20of%20current%0Apersonalisation%20approaches%20in%20exoskeleton-assisted%20gait%20rehabilitation%20and%0Aidentifies%20key%20challenges%20for%20effective%20implementation%20of%20human-in-the-loop%0Aoptimisation%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05780v1&entry.124074799=Read"},
{"title": "Multimodal Trajectory Representation Learning for Travel Time Estimation", "author": "Zhi Liu and Xuyuan Hu and Xiao Han and Zhehao Dai and Zhaolin Deng and Guojiang Shen and Xiangjie Kong", "abstract": "  Accurate travel time estimation (TTE) plays a crucial role in intelligent\ntransportation systems. However, it remains challenging due to heterogeneous\ndata sources and complex traffic dynamics. Moreover, conventional approaches\ntypically convert trajectories into fixed-length representations, neglecting\nthe inherent variability of real-world trajectories, which often leads to\ninformation loss or feature redundancy. To address these challenges, this paper\nintroduces the Multimodal Dynamic Trajectory Integration (MDTI) framework--a\nnovel multimodal trajectory representation learning approach that integrates\nGPS sequences, grid trajectories, and road network constraints to enhance TTE\naccuracy. MDTI employs modality-specific encoders and a cross-modal interaction\nmodule to capture complementary spatial, temporal, and topological semantics,\nwhile a dynamic trajectory modeling mechanism adaptively regulates information\ndensity for trajectories of varying lengths. Two self-supervised pretraining\nobjectives, named contrastive alignment and masked language modeling, further\nstrengthen multimodal consistency and contextual understanding. Extensive\nexperiments on three real-world datasets demonstrate that MDTI consistently\noutperforms state-of-the-art baselines, confirming its robustness and strong\ngeneralization abilities. The code is publicly available at:\nhttps://github.com/freshhxy/MDTI/\n", "link": "http://arxiv.org/abs/2510.05840v1", "date": "2025-10-07", "relevancy": 2.2293, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5807}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5447}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Trajectory%20Representation%20Learning%20for%20Travel%20Time%20Estimation&body=Title%3A%20Multimodal%20Trajectory%20Representation%20Learning%20for%20Travel%20Time%20Estimation%0AAuthor%3A%20Zhi%20Liu%20and%20Xuyuan%20Hu%20and%20Xiao%20Han%20and%20Zhehao%20Dai%20and%20Zhaolin%20Deng%20and%20Guojiang%20Shen%20and%20Xiangjie%20Kong%0AAbstract%3A%20%20%20Accurate%20travel%20time%20estimation%20%28TTE%29%20plays%20a%20crucial%20role%20in%20intelligent%0Atransportation%20systems.%20However%2C%20it%20remains%20challenging%20due%20to%20heterogeneous%0Adata%20sources%20and%20complex%20traffic%20dynamics.%20Moreover%2C%20conventional%20approaches%0Atypically%20convert%20trajectories%20into%20fixed-length%20representations%2C%20neglecting%0Athe%20inherent%20variability%20of%20real-world%20trajectories%2C%20which%20often%20leads%20to%0Ainformation%20loss%20or%20feature%20redundancy.%20To%20address%20these%20challenges%2C%20this%20paper%0Aintroduces%20the%20Multimodal%20Dynamic%20Trajectory%20Integration%20%28MDTI%29%20framework--a%0Anovel%20multimodal%20trajectory%20representation%20learning%20approach%20that%20integrates%0AGPS%20sequences%2C%20grid%20trajectories%2C%20and%20road%20network%20constraints%20to%20enhance%20TTE%0Aaccuracy.%20MDTI%20employs%20modality-specific%20encoders%20and%20a%20cross-modal%20interaction%0Amodule%20to%20capture%20complementary%20spatial%2C%20temporal%2C%20and%20topological%20semantics%2C%0Awhile%20a%20dynamic%20trajectory%20modeling%20mechanism%20adaptively%20regulates%20information%0Adensity%20for%20trajectories%20of%20varying%20lengths.%20Two%20self-supervised%20pretraining%0Aobjectives%2C%20named%20contrastive%20alignment%20and%20masked%20language%20modeling%2C%20further%0Astrengthen%20multimodal%20consistency%20and%20contextual%20understanding.%20Extensive%0Aexperiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20MDTI%20consistently%0Aoutperforms%20state-of-the-art%20baselines%2C%20confirming%20its%20robustness%20and%20strong%0Ageneralization%20abilities.%20The%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/freshhxy/MDTI/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Trajectory%2520Representation%2520Learning%2520for%2520Travel%2520Time%2520Estimation%26entry.906535625%3DZhi%2520Liu%2520and%2520Xuyuan%2520Hu%2520and%2520Xiao%2520Han%2520and%2520Zhehao%2520Dai%2520and%2520Zhaolin%2520Deng%2520and%2520Guojiang%2520Shen%2520and%2520Xiangjie%2520Kong%26entry.1292438233%3D%2520%2520Accurate%2520travel%2520time%2520estimation%2520%2528TTE%2529%2520plays%2520a%2520crucial%2520role%2520in%2520intelligent%250Atransportation%2520systems.%2520However%252C%2520it%2520remains%2520challenging%2520due%2520to%2520heterogeneous%250Adata%2520sources%2520and%2520complex%2520traffic%2520dynamics.%2520Moreover%252C%2520conventional%2520approaches%250Atypically%2520convert%2520trajectories%2520into%2520fixed-length%2520representations%252C%2520neglecting%250Athe%2520inherent%2520variability%2520of%2520real-world%2520trajectories%252C%2520which%2520often%2520leads%2520to%250Ainformation%2520loss%2520or%2520feature%2520redundancy.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%250Aintroduces%2520the%2520Multimodal%2520Dynamic%2520Trajectory%2520Integration%2520%2528MDTI%2529%2520framework--a%250Anovel%2520multimodal%2520trajectory%2520representation%2520learning%2520approach%2520that%2520integrates%250AGPS%2520sequences%252C%2520grid%2520trajectories%252C%2520and%2520road%2520network%2520constraints%2520to%2520enhance%2520TTE%250Aaccuracy.%2520MDTI%2520employs%2520modality-specific%2520encoders%2520and%2520a%2520cross-modal%2520interaction%250Amodule%2520to%2520capture%2520complementary%2520spatial%252C%2520temporal%252C%2520and%2520topological%2520semantics%252C%250Awhile%2520a%2520dynamic%2520trajectory%2520modeling%2520mechanism%2520adaptively%2520regulates%2520information%250Adensity%2520for%2520trajectories%2520of%2520varying%2520lengths.%2520Two%2520self-supervised%2520pretraining%250Aobjectives%252C%2520named%2520contrastive%2520alignment%2520and%2520masked%2520language%2520modeling%252C%2520further%250Astrengthen%2520multimodal%2520consistency%2520and%2520contextual%2520understanding.%2520Extensive%250Aexperiments%2520on%2520three%2520real-world%2520datasets%2520demonstrate%2520that%2520MDTI%2520consistently%250Aoutperforms%2520state-of-the-art%2520baselines%252C%2520confirming%2520its%2520robustness%2520and%2520strong%250Ageneralization%2520abilities.%2520The%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/freshhxy/MDTI/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Trajectory%20Representation%20Learning%20for%20Travel%20Time%20Estimation&entry.906535625=Zhi%20Liu%20and%20Xuyuan%20Hu%20and%20Xiao%20Han%20and%20Zhehao%20Dai%20and%20Zhaolin%20Deng%20and%20Guojiang%20Shen%20and%20Xiangjie%20Kong&entry.1292438233=%20%20Accurate%20travel%20time%20estimation%20%28TTE%29%20plays%20a%20crucial%20role%20in%20intelligent%0Atransportation%20systems.%20However%2C%20it%20remains%20challenging%20due%20to%20heterogeneous%0Adata%20sources%20and%20complex%20traffic%20dynamics.%20Moreover%2C%20conventional%20approaches%0Atypically%20convert%20trajectories%20into%20fixed-length%20representations%2C%20neglecting%0Athe%20inherent%20variability%20of%20real-world%20trajectories%2C%20which%20often%20leads%20to%0Ainformation%20loss%20or%20feature%20redundancy.%20To%20address%20these%20challenges%2C%20this%20paper%0Aintroduces%20the%20Multimodal%20Dynamic%20Trajectory%20Integration%20%28MDTI%29%20framework--a%0Anovel%20multimodal%20trajectory%20representation%20learning%20approach%20that%20integrates%0AGPS%20sequences%2C%20grid%20trajectories%2C%20and%20road%20network%20constraints%20to%20enhance%20TTE%0Aaccuracy.%20MDTI%20employs%20modality-specific%20encoders%20and%20a%20cross-modal%20interaction%0Amodule%20to%20capture%20complementary%20spatial%2C%20temporal%2C%20and%20topological%20semantics%2C%0Awhile%20a%20dynamic%20trajectory%20modeling%20mechanism%20adaptively%20regulates%20information%0Adensity%20for%20trajectories%20of%20varying%20lengths.%20Two%20self-supervised%20pretraining%0Aobjectives%2C%20named%20contrastive%20alignment%20and%20masked%20language%20modeling%2C%20further%0Astrengthen%20multimodal%20consistency%20and%20contextual%20understanding.%20Extensive%0Aexperiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20MDTI%20consistently%0Aoutperforms%20state-of-the-art%20baselines%2C%20confirming%20its%20robustness%20and%20strong%0Ageneralization%20abilities.%20The%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/freshhxy/MDTI/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05840v1&entry.124074799=Read"},
{"title": "HOG-Diff: Higher-Order Guided Diffusion for Graph Generation", "author": "Yiming Huang and Tolga Birdal", "abstract": "  Graph generation is a critical yet challenging task as empirical analyses\nrequire a deep understanding of complex, non-Euclidean structures. Diffusion\nmodels have recently made significant achievements in graph generation, but\nthese models are typically adapted from image generation frameworks and\noverlook inherent higher-order topology, leaving them ill-suited for capturing\nthe topological properties of graphs. In this work, we propose Higher-order\nGuided Diffusion (HOG-Diff), a principled framework that progressively\ngenerates plausible graphs with inherent topological structures. HOG-Diff\nfollows a coarse-to-fine generation curriculum guided by higher-order topology\nand implemented via diffusion bridges. We further prove that our model exhibits\na stronger theoretical guarantee than classical diffusion frameworks. Extensive\nexperiments on both molecular and generic graph generation tasks demonstrate\nthat our method consistently outperforms or remains competitive with\nstate-of-the-art baselines. Our code is available at\nhttps://github.com/Yiminghh/HOG-Diff.\n", "link": "http://arxiv.org/abs/2502.04308v2", "date": "2025-10-07", "relevancy": 2.2283, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5756}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5457}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOG-Diff%3A%20Higher-Order%20Guided%20Diffusion%20for%20Graph%20Generation&body=Title%3A%20HOG-Diff%3A%20Higher-Order%20Guided%20Diffusion%20for%20Graph%20Generation%0AAuthor%3A%20Yiming%20Huang%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Graph%20generation%20is%20a%20critical%20yet%20challenging%20task%20as%20empirical%20analyses%0Arequire%20a%20deep%20understanding%20of%20complex%2C%20non-Euclidean%20structures.%20Diffusion%0Amodels%20have%20recently%20made%20significant%20achievements%20in%20graph%20generation%2C%20but%0Athese%20models%20are%20typically%20adapted%20from%20image%20generation%20frameworks%20and%0Aoverlook%20inherent%20higher-order%20topology%2C%20leaving%20them%20ill-suited%20for%20capturing%0Athe%20topological%20properties%20of%20graphs.%20In%20this%20work%2C%20we%20propose%20Higher-order%0AGuided%20Diffusion%20%28HOG-Diff%29%2C%20a%20principled%20framework%20that%20progressively%0Agenerates%20plausible%20graphs%20with%20inherent%20topological%20structures.%20HOG-Diff%0Afollows%20a%20coarse-to-fine%20generation%20curriculum%20guided%20by%20higher-order%20topology%0Aand%20implemented%20via%20diffusion%20bridges.%20We%20further%20prove%20that%20our%20model%20exhibits%0Aa%20stronger%20theoretical%20guarantee%20than%20classical%20diffusion%20frameworks.%20Extensive%0Aexperiments%20on%20both%20molecular%20and%20generic%20graph%20generation%20tasks%20demonstrate%0Athat%20our%20method%20consistently%20outperforms%20or%20remains%20competitive%20with%0Astate-of-the-art%20baselines.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Yiminghh/HOG-Diff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOG-Diff%253A%2520Higher-Order%2520Guided%2520Diffusion%2520for%2520Graph%2520Generation%26entry.906535625%3DYiming%2520Huang%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Graph%2520generation%2520is%2520a%2520critical%2520yet%2520challenging%2520task%2520as%2520empirical%2520analyses%250Arequire%2520a%2520deep%2520understanding%2520of%2520complex%252C%2520non-Euclidean%2520structures.%2520Diffusion%250Amodels%2520have%2520recently%2520made%2520significant%2520achievements%2520in%2520graph%2520generation%252C%2520but%250Athese%2520models%2520are%2520typically%2520adapted%2520from%2520image%2520generation%2520frameworks%2520and%250Aoverlook%2520inherent%2520higher-order%2520topology%252C%2520leaving%2520them%2520ill-suited%2520for%2520capturing%250Athe%2520topological%2520properties%2520of%2520graphs.%2520In%2520this%2520work%252C%2520we%2520propose%2520Higher-order%250AGuided%2520Diffusion%2520%2528HOG-Diff%2529%252C%2520a%2520principled%2520framework%2520that%2520progressively%250Agenerates%2520plausible%2520graphs%2520with%2520inherent%2520topological%2520structures.%2520HOG-Diff%250Afollows%2520a%2520coarse-to-fine%2520generation%2520curriculum%2520guided%2520by%2520higher-order%2520topology%250Aand%2520implemented%2520via%2520diffusion%2520bridges.%2520We%2520further%2520prove%2520that%2520our%2520model%2520exhibits%250Aa%2520stronger%2520theoretical%2520guarantee%2520than%2520classical%2520diffusion%2520frameworks.%2520Extensive%250Aexperiments%2520on%2520both%2520molecular%2520and%2520generic%2520graph%2520generation%2520tasks%2520demonstrate%250Athat%2520our%2520method%2520consistently%2520outperforms%2520or%2520remains%2520competitive%2520with%250Astate-of-the-art%2520baselines.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Yiminghh/HOG-Diff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOG-Diff%3A%20Higher-Order%20Guided%20Diffusion%20for%20Graph%20Generation&entry.906535625=Yiming%20Huang%20and%20Tolga%20Birdal&entry.1292438233=%20%20Graph%20generation%20is%20a%20critical%20yet%20challenging%20task%20as%20empirical%20analyses%0Arequire%20a%20deep%20understanding%20of%20complex%2C%20non-Euclidean%20structures.%20Diffusion%0Amodels%20have%20recently%20made%20significant%20achievements%20in%20graph%20generation%2C%20but%0Athese%20models%20are%20typically%20adapted%20from%20image%20generation%20frameworks%20and%0Aoverlook%20inherent%20higher-order%20topology%2C%20leaving%20them%20ill-suited%20for%20capturing%0Athe%20topological%20properties%20of%20graphs.%20In%20this%20work%2C%20we%20propose%20Higher-order%0AGuided%20Diffusion%20%28HOG-Diff%29%2C%20a%20principled%20framework%20that%20progressively%0Agenerates%20plausible%20graphs%20with%20inherent%20topological%20structures.%20HOG-Diff%0Afollows%20a%20coarse-to-fine%20generation%20curriculum%20guided%20by%20higher-order%20topology%0Aand%20implemented%20via%20diffusion%20bridges.%20We%20further%20prove%20that%20our%20model%20exhibits%0Aa%20stronger%20theoretical%20guarantee%20than%20classical%20diffusion%20frameworks.%20Extensive%0Aexperiments%20on%20both%20molecular%20and%20generic%20graph%20generation%20tasks%20demonstrate%0Athat%20our%20method%20consistently%20outperforms%20or%20remains%20competitive%20with%0Astate-of-the-art%20baselines.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Yiminghh/HOG-Diff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04308v2&entry.124074799=Read"},
{"title": "Enhancing Fitness Movement Recognition with Attention Mechanism and\n  Pre-Trained Feature Extractors", "author": "Shanjid Hasan Nishat and Srabonti Deb and Mohiuddin Ahmed", "abstract": "  Fitness movement recognition, a focused subdomain of human activity\nrecognition (HAR), plays a vital role in health monitoring, rehabilitation, and\npersonalized fitness training by enabling automated exercise classification\nfrom video data. However, many existing deep learning approaches rely on\ncomputationally intensive 3D models, limiting their feasibility in real-time or\nresource-constrained settings. In this paper, we present a lightweight and\neffective framework that integrates pre-trained 2D Convolutional Neural\nNetworks (CNNs) such as ResNet50, EfficientNet, and Vision Transformers (ViT)\nwith a Long Short-Term Memory (LSTM) network enhanced by spatial attention.\nThese models efficiently extract spatial features while the LSTM captures\ntemporal dependencies, and the attention mechanism emphasizes informative\nsegments. We evaluate the framework on a curated subset of the UCF101 dataset,\nachieving a peak accuracy of 93.34\\% with the ResNet50-based configuration.\nComparative results demonstrate the superiority of our approach over several\nstate-of-the-art HAR systems. The proposed method offers a scalable and\nreal-time-capable solution for fitness activity recognition with broader\napplications in vision-based health and activity monitoring.\n", "link": "http://arxiv.org/abs/2509.02511v2", "date": "2025-10-07", "relevancy": 2.2152, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6017}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5197}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Fitness%20Movement%20Recognition%20with%20Attention%20Mechanism%20and%0A%20%20Pre-Trained%20Feature%20Extractors&body=Title%3A%20Enhancing%20Fitness%20Movement%20Recognition%20with%20Attention%20Mechanism%20and%0A%20%20Pre-Trained%20Feature%20Extractors%0AAuthor%3A%20Shanjid%20Hasan%20Nishat%20and%20Srabonti%20Deb%20and%20Mohiuddin%20Ahmed%0AAbstract%3A%20%20%20Fitness%20movement%20recognition%2C%20a%20focused%20subdomain%20of%20human%20activity%0Arecognition%20%28HAR%29%2C%20plays%20a%20vital%20role%20in%20health%20monitoring%2C%20rehabilitation%2C%20and%0Apersonalized%20fitness%20training%20by%20enabling%20automated%20exercise%20classification%0Afrom%20video%20data.%20However%2C%20many%20existing%20deep%20learning%20approaches%20rely%20on%0Acomputationally%20intensive%203D%20models%2C%20limiting%20their%20feasibility%20in%20real-time%20or%0Aresource-constrained%20settings.%20In%20this%20paper%2C%20we%20present%20a%20lightweight%20and%0Aeffective%20framework%20that%20integrates%20pre-trained%202D%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20such%20as%20ResNet50%2C%20EfficientNet%2C%20and%20Vision%20Transformers%20%28ViT%29%0Awith%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20network%20enhanced%20by%20spatial%20attention.%0AThese%20models%20efficiently%20extract%20spatial%20features%20while%20the%20LSTM%20captures%0Atemporal%20dependencies%2C%20and%20the%20attention%20mechanism%20emphasizes%20informative%0Asegments.%20We%20evaluate%20the%20framework%20on%20a%20curated%20subset%20of%20the%20UCF101%20dataset%2C%0Aachieving%20a%20peak%20accuracy%20of%2093.34%5C%25%20with%20the%20ResNet50-based%20configuration.%0AComparative%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20several%0Astate-of-the-art%20HAR%20systems.%20The%20proposed%20method%20offers%20a%20scalable%20and%0Areal-time-capable%20solution%20for%20fitness%20activity%20recognition%20with%20broader%0Aapplications%20in%20vision-based%20health%20and%20activity%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Fitness%2520Movement%2520Recognition%2520with%2520Attention%2520Mechanism%2520and%250A%2520%2520Pre-Trained%2520Feature%2520Extractors%26entry.906535625%3DShanjid%2520Hasan%2520Nishat%2520and%2520Srabonti%2520Deb%2520and%2520Mohiuddin%2520Ahmed%26entry.1292438233%3D%2520%2520Fitness%2520movement%2520recognition%252C%2520a%2520focused%2520subdomain%2520of%2520human%2520activity%250Arecognition%2520%2528HAR%2529%252C%2520plays%2520a%2520vital%2520role%2520in%2520health%2520monitoring%252C%2520rehabilitation%252C%2520and%250Apersonalized%2520fitness%2520training%2520by%2520enabling%2520automated%2520exercise%2520classification%250Afrom%2520video%2520data.%2520However%252C%2520many%2520existing%2520deep%2520learning%2520approaches%2520rely%2520on%250Acomputationally%2520intensive%25203D%2520models%252C%2520limiting%2520their%2520feasibility%2520in%2520real-time%2520or%250Aresource-constrained%2520settings.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520lightweight%2520and%250Aeffective%2520framework%2520that%2520integrates%2520pre-trained%25202D%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%2520such%2520as%2520ResNet50%252C%2520EfficientNet%252C%2520and%2520Vision%2520Transformers%2520%2528ViT%2529%250Awith%2520a%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520network%2520enhanced%2520by%2520spatial%2520attention.%250AThese%2520models%2520efficiently%2520extract%2520spatial%2520features%2520while%2520the%2520LSTM%2520captures%250Atemporal%2520dependencies%252C%2520and%2520the%2520attention%2520mechanism%2520emphasizes%2520informative%250Asegments.%2520We%2520evaluate%2520the%2520framework%2520on%2520a%2520curated%2520subset%2520of%2520the%2520UCF101%2520dataset%252C%250Aachieving%2520a%2520peak%2520accuracy%2520of%252093.34%255C%2525%2520with%2520the%2520ResNet50-based%2520configuration.%250AComparative%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520several%250Astate-of-the-art%2520HAR%2520systems.%2520The%2520proposed%2520method%2520offers%2520a%2520scalable%2520and%250Areal-time-capable%2520solution%2520for%2520fitness%2520activity%2520recognition%2520with%2520broader%250Aapplications%2520in%2520vision-based%2520health%2520and%2520activity%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Fitness%20Movement%20Recognition%20with%20Attention%20Mechanism%20and%0A%20%20Pre-Trained%20Feature%20Extractors&entry.906535625=Shanjid%20Hasan%20Nishat%20and%20Srabonti%20Deb%20and%20Mohiuddin%20Ahmed&entry.1292438233=%20%20Fitness%20movement%20recognition%2C%20a%20focused%20subdomain%20of%20human%20activity%0Arecognition%20%28HAR%29%2C%20plays%20a%20vital%20role%20in%20health%20monitoring%2C%20rehabilitation%2C%20and%0Apersonalized%20fitness%20training%20by%20enabling%20automated%20exercise%20classification%0Afrom%20video%20data.%20However%2C%20many%20existing%20deep%20learning%20approaches%20rely%20on%0Acomputationally%20intensive%203D%20models%2C%20limiting%20their%20feasibility%20in%20real-time%20or%0Aresource-constrained%20settings.%20In%20this%20paper%2C%20we%20present%20a%20lightweight%20and%0Aeffective%20framework%20that%20integrates%20pre-trained%202D%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20such%20as%20ResNet50%2C%20EfficientNet%2C%20and%20Vision%20Transformers%20%28ViT%29%0Awith%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20network%20enhanced%20by%20spatial%20attention.%0AThese%20models%20efficiently%20extract%20spatial%20features%20while%20the%20LSTM%20captures%0Atemporal%20dependencies%2C%20and%20the%20attention%20mechanism%20emphasizes%20informative%0Asegments.%20We%20evaluate%20the%20framework%20on%20a%20curated%20subset%20of%20the%20UCF101%20dataset%2C%0Aachieving%20a%20peak%20accuracy%20of%2093.34%5C%25%20with%20the%20ResNet50-based%20configuration.%0AComparative%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20several%0Astate-of-the-art%20HAR%20systems.%20The%20proposed%20method%20offers%20a%20scalable%20and%0Areal-time-capable%20solution%20for%20fitness%20activity%20recognition%20with%20broader%0Aapplications%20in%20vision-based%20health%20and%20activity%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02511v2&entry.124074799=Read"},
{"title": "EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark", "author": "Deheng Zhang and Yuqian Fu and Runyi Yang and Yang Miao and Tianwen Qian and Xu Zheng and Guolei Sun and Ajad Chhatkuli and Xuanjing Huang and Yu-Gang Jiang and Luc Van Gool and Danda Pani Paudel", "abstract": "  Most existing benchmarks for egocentric vision understanding focus primarily\non daytime scenarios, overlooking the low-light conditions that are inevitable\nin real-world applications. To investigate this gap, we present EgoNight, the\nfirst comprehensive benchmark for nighttime egocentric vision, with visual\nquestion answering (VQA) as the core task. A key feature of EgoNight is the\nintroduction of day-night aligned videos, which enhance night annotation\nquality using the daytime data and reveal clear performance gaps between\nlighting conditions. To achieve this, we collect both synthetic videos rendered\nby Blender and real-world recordings, ensuring that scenes and actions are\nvisually and temporally aligned. Leveraging these paired videos, we construct\nEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and\nrefinement through extensive human verification. Each QA pair is double-checked\nby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs\nacross 90 videos, spanning 12 diverse QA types, with more than 300 hours of\nhuman work. Evaluations of state-of-the-art multimodal large language models\n(MLLMs) reveal substantial performance drops when transferring from day to\nnight, underscoring the challenges of reasoning under low-light conditions.\nBeyond VQA, EgoNight also introduces two auxiliary tasks, day-night\ncorrespondence retrieval and egocentric depth estimation at night, that further\nexplore the boundaries of existing models. We believe EgoNight-VQA provides a\nstrong foundation for advancing application-driven egocentric vision research\nand for developing models that generalize across illumination domains. All the\ndata and code will be made available upon acceptance.\n", "link": "http://arxiv.org/abs/2510.06218v1", "date": "2025-10-07", "relevancy": 2.2147, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoNight%3A%20Towards%20Egocentric%20Vision%20Understanding%20at%20Night%20with%20a%0A%20%20Challenging%20Benchmark&body=Title%3A%20EgoNight%3A%20Towards%20Egocentric%20Vision%20Understanding%20at%20Night%20with%20a%0A%20%20Challenging%20Benchmark%0AAuthor%3A%20Deheng%20Zhang%20and%20Yuqian%20Fu%20and%20Runyi%20Yang%20and%20Yang%20Miao%20and%20Tianwen%20Qian%20and%20Xu%20Zheng%20and%20Guolei%20Sun%20and%20Ajad%20Chhatkuli%20and%20Xuanjing%20Huang%20and%20Yu-Gang%20Jiang%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%20%20%20Most%20existing%20benchmarks%20for%20egocentric%20vision%20understanding%20focus%20primarily%0Aon%20daytime%20scenarios%2C%20overlooking%20the%20low-light%20conditions%20that%20are%20inevitable%0Ain%20real-world%20applications.%20To%20investigate%20this%20gap%2C%20we%20present%20EgoNight%2C%20the%0Afirst%20comprehensive%20benchmark%20for%20nighttime%20egocentric%20vision%2C%20with%20visual%0Aquestion%20answering%20%28VQA%29%20as%20the%20core%20task.%20A%20key%20feature%20of%20EgoNight%20is%20the%0Aintroduction%20of%20day-night%20aligned%20videos%2C%20which%20enhance%20night%20annotation%0Aquality%20using%20the%20daytime%20data%20and%20reveal%20clear%20performance%20gaps%20between%0Alighting%20conditions.%20To%20achieve%20this%2C%20we%20collect%20both%20synthetic%20videos%20rendered%0Aby%20Blender%20and%20real-world%20recordings%2C%20ensuring%20that%20scenes%20and%20actions%20are%0Avisually%20and%20temporally%20aligned.%20Leveraging%20these%20paired%20videos%2C%20we%20construct%0AEgoNight-VQA%2C%20supported%20by%20a%20novel%20day-augmented%20night%20auto-labeling%20engine%20and%0Arefinement%20through%20extensive%20human%20verification.%20Each%20QA%20pair%20is%20double-checked%0Aby%20annotators%20for%20reliability.%20In%20total%2C%20EgoNight-VQA%20contains%203658%20QA%20pairs%0Aacross%2090%20videos%2C%20spanning%2012%20diverse%20QA%20types%2C%20with%20more%20than%20300%20hours%20of%0Ahuman%20work.%20Evaluations%20of%20state-of-the-art%20multimodal%20large%20language%20models%0A%28MLLMs%29%20reveal%20substantial%20performance%20drops%20when%20transferring%20from%20day%20to%0Anight%2C%20underscoring%20the%20challenges%20of%20reasoning%20under%20low-light%20conditions.%0ABeyond%20VQA%2C%20EgoNight%20also%20introduces%20two%20auxiliary%20tasks%2C%20day-night%0Acorrespondence%20retrieval%20and%20egocentric%20depth%20estimation%20at%20night%2C%20that%20further%0Aexplore%20the%20boundaries%20of%20existing%20models.%20We%20believe%20EgoNight-VQA%20provides%20a%0Astrong%20foundation%20for%20advancing%20application-driven%20egocentric%20vision%20research%0Aand%20for%20developing%20models%20that%20generalize%20across%20illumination%20domains.%20All%20the%0Adata%20and%20code%20will%20be%20made%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoNight%253A%2520Towards%2520Egocentric%2520Vision%2520Understanding%2520at%2520Night%2520with%2520a%250A%2520%2520Challenging%2520Benchmark%26entry.906535625%3DDeheng%2520Zhang%2520and%2520Yuqian%2520Fu%2520and%2520Runyi%2520Yang%2520and%2520Yang%2520Miao%2520and%2520Tianwen%2520Qian%2520and%2520Xu%2520Zheng%2520and%2520Guolei%2520Sun%2520and%2520Ajad%2520Chhatkuli%2520and%2520Xuanjing%2520Huang%2520and%2520Yu-Gang%2520Jiang%2520and%2520Luc%2520Van%2520Gool%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3D%2520%2520Most%2520existing%2520benchmarks%2520for%2520egocentric%2520vision%2520understanding%2520focus%2520primarily%250Aon%2520daytime%2520scenarios%252C%2520overlooking%2520the%2520low-light%2520conditions%2520that%2520are%2520inevitable%250Ain%2520real-world%2520applications.%2520To%2520investigate%2520this%2520gap%252C%2520we%2520present%2520EgoNight%252C%2520the%250Afirst%2520comprehensive%2520benchmark%2520for%2520nighttime%2520egocentric%2520vision%252C%2520with%2520visual%250Aquestion%2520answering%2520%2528VQA%2529%2520as%2520the%2520core%2520task.%2520A%2520key%2520feature%2520of%2520EgoNight%2520is%2520the%250Aintroduction%2520of%2520day-night%2520aligned%2520videos%252C%2520which%2520enhance%2520night%2520annotation%250Aquality%2520using%2520the%2520daytime%2520data%2520and%2520reveal%2520clear%2520performance%2520gaps%2520between%250Alighting%2520conditions.%2520To%2520achieve%2520this%252C%2520we%2520collect%2520both%2520synthetic%2520videos%2520rendered%250Aby%2520Blender%2520and%2520real-world%2520recordings%252C%2520ensuring%2520that%2520scenes%2520and%2520actions%2520are%250Avisually%2520and%2520temporally%2520aligned.%2520Leveraging%2520these%2520paired%2520videos%252C%2520we%2520construct%250AEgoNight-VQA%252C%2520supported%2520by%2520a%2520novel%2520day-augmented%2520night%2520auto-labeling%2520engine%2520and%250Arefinement%2520through%2520extensive%2520human%2520verification.%2520Each%2520QA%2520pair%2520is%2520double-checked%250Aby%2520annotators%2520for%2520reliability.%2520In%2520total%252C%2520EgoNight-VQA%2520contains%25203658%2520QA%2520pairs%250Aacross%252090%2520videos%252C%2520spanning%252012%2520diverse%2520QA%2520types%252C%2520with%2520more%2520than%2520300%2520hours%2520of%250Ahuman%2520work.%2520Evaluations%2520of%2520state-of-the-art%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520reveal%2520substantial%2520performance%2520drops%2520when%2520transferring%2520from%2520day%2520to%250Anight%252C%2520underscoring%2520the%2520challenges%2520of%2520reasoning%2520under%2520low-light%2520conditions.%250ABeyond%2520VQA%252C%2520EgoNight%2520also%2520introduces%2520two%2520auxiliary%2520tasks%252C%2520day-night%250Acorrespondence%2520retrieval%2520and%2520egocentric%2520depth%2520estimation%2520at%2520night%252C%2520that%2520further%250Aexplore%2520the%2520boundaries%2520of%2520existing%2520models.%2520We%2520believe%2520EgoNight-VQA%2520provides%2520a%250Astrong%2520foundation%2520for%2520advancing%2520application-driven%2520egocentric%2520vision%2520research%250Aand%2520for%2520developing%2520models%2520that%2520generalize%2520across%2520illumination%2520domains.%2520All%2520the%250Adata%2520and%2520code%2520will%2520be%2520made%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoNight%3A%20Towards%20Egocentric%20Vision%20Understanding%20at%20Night%20with%20a%0A%20%20Challenging%20Benchmark&entry.906535625=Deheng%20Zhang%20and%20Yuqian%20Fu%20and%20Runyi%20Yang%20and%20Yang%20Miao%20and%20Tianwen%20Qian%20and%20Xu%20Zheng%20and%20Guolei%20Sun%20and%20Ajad%20Chhatkuli%20and%20Xuanjing%20Huang%20and%20Yu-Gang%20Jiang%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel&entry.1292438233=%20%20Most%20existing%20benchmarks%20for%20egocentric%20vision%20understanding%20focus%20primarily%0Aon%20daytime%20scenarios%2C%20overlooking%20the%20low-light%20conditions%20that%20are%20inevitable%0Ain%20real-world%20applications.%20To%20investigate%20this%20gap%2C%20we%20present%20EgoNight%2C%20the%0Afirst%20comprehensive%20benchmark%20for%20nighttime%20egocentric%20vision%2C%20with%20visual%0Aquestion%20answering%20%28VQA%29%20as%20the%20core%20task.%20A%20key%20feature%20of%20EgoNight%20is%20the%0Aintroduction%20of%20day-night%20aligned%20videos%2C%20which%20enhance%20night%20annotation%0Aquality%20using%20the%20daytime%20data%20and%20reveal%20clear%20performance%20gaps%20between%0Alighting%20conditions.%20To%20achieve%20this%2C%20we%20collect%20both%20synthetic%20videos%20rendered%0Aby%20Blender%20and%20real-world%20recordings%2C%20ensuring%20that%20scenes%20and%20actions%20are%0Avisually%20and%20temporally%20aligned.%20Leveraging%20these%20paired%20videos%2C%20we%20construct%0AEgoNight-VQA%2C%20supported%20by%20a%20novel%20day-augmented%20night%20auto-labeling%20engine%20and%0Arefinement%20through%20extensive%20human%20verification.%20Each%20QA%20pair%20is%20double-checked%0Aby%20annotators%20for%20reliability.%20In%20total%2C%20EgoNight-VQA%20contains%203658%20QA%20pairs%0Aacross%2090%20videos%2C%20spanning%2012%20diverse%20QA%20types%2C%20with%20more%20than%20300%20hours%20of%0Ahuman%20work.%20Evaluations%20of%20state-of-the-art%20multimodal%20large%20language%20models%0A%28MLLMs%29%20reveal%20substantial%20performance%20drops%20when%20transferring%20from%20day%20to%0Anight%2C%20underscoring%20the%20challenges%20of%20reasoning%20under%20low-light%20conditions.%0ABeyond%20VQA%2C%20EgoNight%20also%20introduces%20two%20auxiliary%20tasks%2C%20day-night%0Acorrespondence%20retrieval%20and%20egocentric%20depth%20estimation%20at%20night%2C%20that%20further%0Aexplore%20the%20boundaries%20of%20existing%20models.%20We%20believe%20EgoNight-VQA%20provides%20a%0Astrong%20foundation%20for%20advancing%20application-driven%20egocentric%20vision%20research%0Aand%20for%20developing%20models%20that%20generalize%20across%20illumination%20domains.%20All%20the%0Adata%20and%20code%20will%20be%20made%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06218v1&entry.124074799=Read"},
{"title": "A Dynamic Mode Decomposition Approach to Morphological Component\n  Analysis", "author": "Owen T. Huber and Raghu G. Raj and Tianyu Chen and Zacharie I. Idriss", "abstract": "  This paper introduces a novel methodology of adapting the representation of\nvideos based on the dynamics of their scene content variation. In particular,\nwe demonstrate how the clustering of dynamic mode decomposition eigenvalues can\nbe leveraged to learn an adaptive video representation for separating\nstructurally distinct morphologies of a video. We extend the morphological\ncomponent analysis (MCA) algorithm, which uses multiple predefined incoherent\ndictionaries and a sparsity prior to separate distinct sources in signals, by\nintroducing our novel eigenspace clustering technique to obtain data-driven MCA\ndictionaries, which we call dynamic morphological component analysis (DMCA).\nAfter deriving our novel algorithm, we offer a motivational example of DMCA\napplied to a still image, then demonstrate DMCA's effectiveness in denoising\napplications on videos from the Adobe 240fps dataset. Afterwards, we provide an\nexample of DMCA enhancing the signal-to-noise ratio of a faint target summed\nwith a sea state, and conclude the paper by applying DMCA to separate a bicycle\nfrom wind clutter in inverse synthetic aperture radar images.\n", "link": "http://arxiv.org/abs/2510.05977v1", "date": "2025-10-07", "relevancy": 2.2143, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5665}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5456}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dynamic%20Mode%20Decomposition%20Approach%20to%20Morphological%20Component%0A%20%20Analysis&body=Title%3A%20A%20Dynamic%20Mode%20Decomposition%20Approach%20to%20Morphological%20Component%0A%20%20Analysis%0AAuthor%3A%20Owen%20T.%20Huber%20and%20Raghu%20G.%20Raj%20and%20Tianyu%20Chen%20and%20Zacharie%20I.%20Idriss%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20methodology%20of%20adapting%20the%20representation%20of%0Avideos%20based%20on%20the%20dynamics%20of%20their%20scene%20content%20variation.%20In%20particular%2C%0Awe%20demonstrate%20how%20the%20clustering%20of%20dynamic%20mode%20decomposition%20eigenvalues%20can%0Abe%20leveraged%20to%20learn%20an%20adaptive%20video%20representation%20for%20separating%0Astructurally%20distinct%20morphologies%20of%20a%20video.%20We%20extend%20the%20morphological%0Acomponent%20analysis%20%28MCA%29%20algorithm%2C%20which%20uses%20multiple%20predefined%20incoherent%0Adictionaries%20and%20a%20sparsity%20prior%20to%20separate%20distinct%20sources%20in%20signals%2C%20by%0Aintroducing%20our%20novel%20eigenspace%20clustering%20technique%20to%20obtain%20data-driven%20MCA%0Adictionaries%2C%20which%20we%20call%20dynamic%20morphological%20component%20analysis%20%28DMCA%29.%0AAfter%20deriving%20our%20novel%20algorithm%2C%20we%20offer%20a%20motivational%20example%20of%20DMCA%0Aapplied%20to%20a%20still%20image%2C%20then%20demonstrate%20DMCA%27s%20effectiveness%20in%20denoising%0Aapplications%20on%20videos%20from%20the%20Adobe%20240fps%20dataset.%20Afterwards%2C%20we%20provide%20an%0Aexample%20of%20DMCA%20enhancing%20the%20signal-to-noise%20ratio%20of%20a%20faint%20target%20summed%0Awith%20a%20sea%20state%2C%20and%20conclude%20the%20paper%20by%20applying%20DMCA%20to%20separate%20a%20bicycle%0Afrom%20wind%20clutter%20in%20inverse%20synthetic%20aperture%20radar%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dynamic%2520Mode%2520Decomposition%2520Approach%2520to%2520Morphological%2520Component%250A%2520%2520Analysis%26entry.906535625%3DOwen%2520T.%2520Huber%2520and%2520Raghu%2520G.%2520Raj%2520and%2520Tianyu%2520Chen%2520and%2520Zacharie%2520I.%2520Idriss%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520methodology%2520of%2520adapting%2520the%2520representation%2520of%250Avideos%2520based%2520on%2520the%2520dynamics%2520of%2520their%2520scene%2520content%2520variation.%2520In%2520particular%252C%250Awe%2520demonstrate%2520how%2520the%2520clustering%2520of%2520dynamic%2520mode%2520decomposition%2520eigenvalues%2520can%250Abe%2520leveraged%2520to%2520learn%2520an%2520adaptive%2520video%2520representation%2520for%2520separating%250Astructurally%2520distinct%2520morphologies%2520of%2520a%2520video.%2520We%2520extend%2520the%2520morphological%250Acomponent%2520analysis%2520%2528MCA%2529%2520algorithm%252C%2520which%2520uses%2520multiple%2520predefined%2520incoherent%250Adictionaries%2520and%2520a%2520sparsity%2520prior%2520to%2520separate%2520distinct%2520sources%2520in%2520signals%252C%2520by%250Aintroducing%2520our%2520novel%2520eigenspace%2520clustering%2520technique%2520to%2520obtain%2520data-driven%2520MCA%250Adictionaries%252C%2520which%2520we%2520call%2520dynamic%2520morphological%2520component%2520analysis%2520%2528DMCA%2529.%250AAfter%2520deriving%2520our%2520novel%2520algorithm%252C%2520we%2520offer%2520a%2520motivational%2520example%2520of%2520DMCA%250Aapplied%2520to%2520a%2520still%2520image%252C%2520then%2520demonstrate%2520DMCA%2527s%2520effectiveness%2520in%2520denoising%250Aapplications%2520on%2520videos%2520from%2520the%2520Adobe%2520240fps%2520dataset.%2520Afterwards%252C%2520we%2520provide%2520an%250Aexample%2520of%2520DMCA%2520enhancing%2520the%2520signal-to-noise%2520ratio%2520of%2520a%2520faint%2520target%2520summed%250Awith%2520a%2520sea%2520state%252C%2520and%2520conclude%2520the%2520paper%2520by%2520applying%2520DMCA%2520to%2520separate%2520a%2520bicycle%250Afrom%2520wind%2520clutter%2520in%2520inverse%2520synthetic%2520aperture%2520radar%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dynamic%20Mode%20Decomposition%20Approach%20to%20Morphological%20Component%0A%20%20Analysis&entry.906535625=Owen%20T.%20Huber%20and%20Raghu%20G.%20Raj%20and%20Tianyu%20Chen%20and%20Zacharie%20I.%20Idriss&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20methodology%20of%20adapting%20the%20representation%20of%0Avideos%20based%20on%20the%20dynamics%20of%20their%20scene%20content%20variation.%20In%20particular%2C%0Awe%20demonstrate%20how%20the%20clustering%20of%20dynamic%20mode%20decomposition%20eigenvalues%20can%0Abe%20leveraged%20to%20learn%20an%20adaptive%20video%20representation%20for%20separating%0Astructurally%20distinct%20morphologies%20of%20a%20video.%20We%20extend%20the%20morphological%0Acomponent%20analysis%20%28MCA%29%20algorithm%2C%20which%20uses%20multiple%20predefined%20incoherent%0Adictionaries%20and%20a%20sparsity%20prior%20to%20separate%20distinct%20sources%20in%20signals%2C%20by%0Aintroducing%20our%20novel%20eigenspace%20clustering%20technique%20to%20obtain%20data-driven%20MCA%0Adictionaries%2C%20which%20we%20call%20dynamic%20morphological%20component%20analysis%20%28DMCA%29.%0AAfter%20deriving%20our%20novel%20algorithm%2C%20we%20offer%20a%20motivational%20example%20of%20DMCA%0Aapplied%20to%20a%20still%20image%2C%20then%20demonstrate%20DMCA%27s%20effectiveness%20in%20denoising%0Aapplications%20on%20videos%20from%20the%20Adobe%20240fps%20dataset.%20Afterwards%2C%20we%20provide%20an%0Aexample%20of%20DMCA%20enhancing%20the%20signal-to-noise%20ratio%20of%20a%20faint%20target%20summed%0Awith%20a%20sea%20state%2C%20and%20conclude%20the%20paper%20by%20applying%20DMCA%20to%20separate%20a%20bicycle%0Afrom%20wind%20clutter%20in%20inverse%20synthetic%20aperture%20radar%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05977v1&entry.124074799=Read"},
{"title": "Segment-Factorized Full-Song Generation on Symbolic Piano Music", "author": "Ping-Yi Chen and Chih-Pin Tan and Yi-Hsuan Yang", "abstract": "  We propose the Segmented Full-Song Model (SFS) for symbolic full-song\ngeneration. The model accepts a user-provided song structure and an optional\nshort seed segment that anchors the main idea around which the song is\ndeveloped. By factorizing a song into segments and generating each one through\nselective attention to related segments, the model achieves higher quality and\nefficiency compared to prior work. To demonstrate its suitability for human-AI\ninteraction, we further wrap SFS into a web application that enables users to\niteratively co-create music on a piano roll with customizable structures and\nflexible ordering.\n", "link": "http://arxiv.org/abs/2510.05881v1", "date": "2025-10-07", "relevancy": 2.207, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4585}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4388}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment-Factorized%20Full-Song%20Generation%20on%20Symbolic%20Piano%20Music&body=Title%3A%20Segment-Factorized%20Full-Song%20Generation%20on%20Symbolic%20Piano%20Music%0AAuthor%3A%20Ping-Yi%20Chen%20and%20Chih-Pin%20Tan%20and%20Yi-Hsuan%20Yang%0AAbstract%3A%20%20%20We%20propose%20the%20Segmented%20Full-Song%20Model%20%28SFS%29%20for%20symbolic%20full-song%0Ageneration.%20The%20model%20accepts%20a%20user-provided%20song%20structure%20and%20an%20optional%0Ashort%20seed%20segment%20that%20anchors%20the%20main%20idea%20around%20which%20the%20song%20is%0Adeveloped.%20By%20factorizing%20a%20song%20into%20segments%20and%20generating%20each%20one%20through%0Aselective%20attention%20to%20related%20segments%2C%20the%20model%20achieves%20higher%20quality%20and%0Aefficiency%20compared%20to%20prior%20work.%20To%20demonstrate%20its%20suitability%20for%20human-AI%0Ainteraction%2C%20we%20further%20wrap%20SFS%20into%20a%20web%20application%20that%20enables%20users%20to%0Aiteratively%20co-create%20music%20on%20a%20piano%20roll%20with%20customizable%20structures%20and%0Aflexible%20ordering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment-Factorized%2520Full-Song%2520Generation%2520on%2520Symbolic%2520Piano%2520Music%26entry.906535625%3DPing-Yi%2520Chen%2520and%2520Chih-Pin%2520Tan%2520and%2520Yi-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520Segmented%2520Full-Song%2520Model%2520%2528SFS%2529%2520for%2520symbolic%2520full-song%250Ageneration.%2520The%2520model%2520accepts%2520a%2520user-provided%2520song%2520structure%2520and%2520an%2520optional%250Ashort%2520seed%2520segment%2520that%2520anchors%2520the%2520main%2520idea%2520around%2520which%2520the%2520song%2520is%250Adeveloped.%2520By%2520factorizing%2520a%2520song%2520into%2520segments%2520and%2520generating%2520each%2520one%2520through%250Aselective%2520attention%2520to%2520related%2520segments%252C%2520the%2520model%2520achieves%2520higher%2520quality%2520and%250Aefficiency%2520compared%2520to%2520prior%2520work.%2520To%2520demonstrate%2520its%2520suitability%2520for%2520human-AI%250Ainteraction%252C%2520we%2520further%2520wrap%2520SFS%2520into%2520a%2520web%2520application%2520that%2520enables%2520users%2520to%250Aiteratively%2520co-create%2520music%2520on%2520a%2520piano%2520roll%2520with%2520customizable%2520structures%2520and%250Aflexible%2520ordering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment-Factorized%20Full-Song%20Generation%20on%20Symbolic%20Piano%20Music&entry.906535625=Ping-Yi%20Chen%20and%20Chih-Pin%20Tan%20and%20Yi-Hsuan%20Yang&entry.1292438233=%20%20We%20propose%20the%20Segmented%20Full-Song%20Model%20%28SFS%29%20for%20symbolic%20full-song%0Ageneration.%20The%20model%20accepts%20a%20user-provided%20song%20structure%20and%20an%20optional%0Ashort%20seed%20segment%20that%20anchors%20the%20main%20idea%20around%20which%20the%20song%20is%0Adeveloped.%20By%20factorizing%20a%20song%20into%20segments%20and%20generating%20each%20one%20through%0Aselective%20attention%20to%20related%20segments%2C%20the%20model%20achieves%20higher%20quality%20and%0Aefficiency%20compared%20to%20prior%20work.%20To%20demonstrate%20its%20suitability%20for%20human-AI%0Ainteraction%2C%20we%20further%20wrap%20SFS%20into%20a%20web%20application%20that%20enables%20users%20to%0Aiteratively%20co-create%20music%20on%20a%20piano%20roll%20with%20customizable%20structures%20and%0Aflexible%20ordering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05881v1&entry.124074799=Read"},
{"title": "MixReasoning: Switching Modes to Think", "author": "Haiquan Lu and Gongfan Fang and Xinyin Ma and Qi Li and Xinchao Wang", "abstract": "  Reasoning models enhance performance by tackling problems in a step-by-step\nmanner, decomposing them into sub-problems and exploring long chains of thought\nbefore producing an answer. However, applying extended reasoning to every step\nintroduces substantial redundancy, as sub-problems vary widely in difficulty\nand complexity: a small number of pivotal steps are genuinely challenging and\ndecisive for the final answer, while many others only involve straightforward\nrevisions or simple computations. Therefore, a natural idea is to endow\nreasoning models with the ability to adaptively respond to this variation,\nrather than treating all steps with the same level of elaboration. To this end,\nwe propose MixReasoning, a framework that dynamically adjusts the depth of\nreasoning within a single response. The resulting chain of thought then becomes\na mixture of detailed reasoning on difficult steps and concise inference on\nsimpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning\nshortens reasoning length and substantially improves efficiency without\ncompromising accuracy.\n", "link": "http://arxiv.org/abs/2510.06052v1", "date": "2025-10-07", "relevancy": 2.2063, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4423}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MixReasoning%3A%20Switching%20Modes%20to%20Think&body=Title%3A%20MixReasoning%3A%20Switching%20Modes%20to%20Think%0AAuthor%3A%20Haiquan%20Lu%20and%20Gongfan%20Fang%20and%20Xinyin%20Ma%20and%20Qi%20Li%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Reasoning%20models%20enhance%20performance%20by%20tackling%20problems%20in%20a%20step-by-step%0Amanner%2C%20decomposing%20them%20into%20sub-problems%20and%20exploring%20long%20chains%20of%20thought%0Abefore%20producing%20an%20answer.%20However%2C%20applying%20extended%20reasoning%20to%20every%20step%0Aintroduces%20substantial%20redundancy%2C%20as%20sub-problems%20vary%20widely%20in%20difficulty%0Aand%20complexity%3A%20a%20small%20number%20of%20pivotal%20steps%20are%20genuinely%20challenging%20and%0Adecisive%20for%20the%20final%20answer%2C%20while%20many%20others%20only%20involve%20straightforward%0Arevisions%20or%20simple%20computations.%20Therefore%2C%20a%20natural%20idea%20is%20to%20endow%0Areasoning%20models%20with%20the%20ability%20to%20adaptively%20respond%20to%20this%20variation%2C%0Arather%20than%20treating%20all%20steps%20with%20the%20same%20level%20of%20elaboration.%20To%20this%20end%2C%0Awe%20propose%20MixReasoning%2C%20a%20framework%20that%20dynamically%20adjusts%20the%20depth%20of%0Areasoning%20within%20a%20single%20response.%20The%20resulting%20chain%20of%20thought%20then%20becomes%0Aa%20mixture%20of%20detailed%20reasoning%20on%20difficult%20steps%20and%20concise%20inference%20on%0Asimpler%20ones.%20Experiments%20on%20GSM8K%2C%20MATH-500%2C%20and%20AIME%20show%20that%20MixReasoning%0Ashortens%20reasoning%20length%20and%20substantially%20improves%20efficiency%20without%0Acompromising%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixReasoning%253A%2520Switching%2520Modes%2520to%2520Think%26entry.906535625%3DHaiquan%2520Lu%2520and%2520Gongfan%2520Fang%2520and%2520Xinyin%2520Ma%2520and%2520Qi%2520Li%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Reasoning%2520models%2520enhance%2520performance%2520by%2520tackling%2520problems%2520in%2520a%2520step-by-step%250Amanner%252C%2520decomposing%2520them%2520into%2520sub-problems%2520and%2520exploring%2520long%2520chains%2520of%2520thought%250Abefore%2520producing%2520an%2520answer.%2520However%252C%2520applying%2520extended%2520reasoning%2520to%2520every%2520step%250Aintroduces%2520substantial%2520redundancy%252C%2520as%2520sub-problems%2520vary%2520widely%2520in%2520difficulty%250Aand%2520complexity%253A%2520a%2520small%2520number%2520of%2520pivotal%2520steps%2520are%2520genuinely%2520challenging%2520and%250Adecisive%2520for%2520the%2520final%2520answer%252C%2520while%2520many%2520others%2520only%2520involve%2520straightforward%250Arevisions%2520or%2520simple%2520computations.%2520Therefore%252C%2520a%2520natural%2520idea%2520is%2520to%2520endow%250Areasoning%2520models%2520with%2520the%2520ability%2520to%2520adaptively%2520respond%2520to%2520this%2520variation%252C%250Arather%2520than%2520treating%2520all%2520steps%2520with%2520the%2520same%2520level%2520of%2520elaboration.%2520To%2520this%2520end%252C%250Awe%2520propose%2520MixReasoning%252C%2520a%2520framework%2520that%2520dynamically%2520adjusts%2520the%2520depth%2520of%250Areasoning%2520within%2520a%2520single%2520response.%2520The%2520resulting%2520chain%2520of%2520thought%2520then%2520becomes%250Aa%2520mixture%2520of%2520detailed%2520reasoning%2520on%2520difficult%2520steps%2520and%2520concise%2520inference%2520on%250Asimpler%2520ones.%2520Experiments%2520on%2520GSM8K%252C%2520MATH-500%252C%2520and%2520AIME%2520show%2520that%2520MixReasoning%250Ashortens%2520reasoning%2520length%2520and%2520substantially%2520improves%2520efficiency%2520without%250Acompromising%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MixReasoning%3A%20Switching%20Modes%20to%20Think&entry.906535625=Haiquan%20Lu%20and%20Gongfan%20Fang%20and%20Xinyin%20Ma%20and%20Qi%20Li%20and%20Xinchao%20Wang&entry.1292438233=%20%20Reasoning%20models%20enhance%20performance%20by%20tackling%20problems%20in%20a%20step-by-step%0Amanner%2C%20decomposing%20them%20into%20sub-problems%20and%20exploring%20long%20chains%20of%20thought%0Abefore%20producing%20an%20answer.%20However%2C%20applying%20extended%20reasoning%20to%20every%20step%0Aintroduces%20substantial%20redundancy%2C%20as%20sub-problems%20vary%20widely%20in%20difficulty%0Aand%20complexity%3A%20a%20small%20number%20of%20pivotal%20steps%20are%20genuinely%20challenging%20and%0Adecisive%20for%20the%20final%20answer%2C%20while%20many%20others%20only%20involve%20straightforward%0Arevisions%20or%20simple%20computations.%20Therefore%2C%20a%20natural%20idea%20is%20to%20endow%0Areasoning%20models%20with%20the%20ability%20to%20adaptively%20respond%20to%20this%20variation%2C%0Arather%20than%20treating%20all%20steps%20with%20the%20same%20level%20of%20elaboration.%20To%20this%20end%2C%0Awe%20propose%20MixReasoning%2C%20a%20framework%20that%20dynamically%20adjusts%20the%20depth%20of%0Areasoning%20within%20a%20single%20response.%20The%20resulting%20chain%20of%20thought%20then%20becomes%0Aa%20mixture%20of%20detailed%20reasoning%20on%20difficult%20steps%20and%20concise%20inference%20on%0Asimpler%20ones.%20Experiments%20on%20GSM8K%2C%20MATH-500%2C%20and%20AIME%20show%20that%20MixReasoning%0Ashortens%20reasoning%20length%20and%20substantially%20improves%20efficiency%20without%0Acompromising%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06052v1&entry.124074799=Read"},
{"title": "AuToMATo: An Out-Of-The-Box Persistence-Based Clustering Algorithm", "author": "Marius Huber and Sara Kalisnik and Patrick Schnider", "abstract": "  We present AuToMATo, a novel clustering algorithm based on persistent\nhomology. While AuToMATo is not parameter-free per se, we provide default\nchoices for its parameters that make it into an out-of-the-box clustering\nalgorithm that performs well across the board. AuToMATo combines the existing\nToMATo clustering algorithm with a bootstrapping procedure in order to separate\nsignificant peaks of an estimated density function from non-significant ones.\nWe perform a thorough comparison of AuToMATo (with its parameters fixed to\ntheir defaults) against many other state-of-the-art clustering algorithms. We\nfind not only that AuToMATo compares favorably against parameter-free\nclustering algorithms, but in many instances also significantly outperforms\neven the best selection of parameters for other algorithms. AuToMATo is\nmotivated by applications in topological data analysis, in particular the\nMapper algorithm, where it is desirable to work with a clustering algorithm\nthat does not need tuning of its parameters. Indeed, we provide evidence that\nAuToMATo performs well when used with Mapper. Finally, we provide an\nopen-source implementation of AuToMATo in Python that is fully compatible with\nthe standard scikit-learn architecture.\n", "link": "http://arxiv.org/abs/2408.06958v3", "date": "2025-10-07", "relevancy": 2.1951, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4593}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4345}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuToMATo%3A%20An%20Out-Of-The-Box%20Persistence-Based%20Clustering%20Algorithm&body=Title%3A%20AuToMATo%3A%20An%20Out-Of-The-Box%20Persistence-Based%20Clustering%20Algorithm%0AAuthor%3A%20Marius%20Huber%20and%20Sara%20Kalisnik%20and%20Patrick%20Schnider%0AAbstract%3A%20%20%20We%20present%20AuToMATo%2C%20a%20novel%20clustering%20algorithm%20based%20on%20persistent%0Ahomology.%20While%20AuToMATo%20is%20not%20parameter-free%20per%20se%2C%20we%20provide%20default%0Achoices%20for%20its%20parameters%20that%20make%20it%20into%20an%20out-of-the-box%20clustering%0Aalgorithm%20that%20performs%20well%20across%20the%20board.%20AuToMATo%20combines%20the%20existing%0AToMATo%20clustering%20algorithm%20with%20a%20bootstrapping%20procedure%20in%20order%20to%20separate%0Asignificant%20peaks%20of%20an%20estimated%20density%20function%20from%20non-significant%20ones.%0AWe%20perform%20a%20thorough%20comparison%20of%20AuToMATo%20%28with%20its%20parameters%20fixed%20to%0Atheir%20defaults%29%20against%20many%20other%20state-of-the-art%20clustering%20algorithms.%20We%0Afind%20not%20only%20that%20AuToMATo%20compares%20favorably%20against%20parameter-free%0Aclustering%20algorithms%2C%20but%20in%20many%20instances%20also%20significantly%20outperforms%0Aeven%20the%20best%20selection%20of%20parameters%20for%20other%20algorithms.%20AuToMATo%20is%0Amotivated%20by%20applications%20in%20topological%20data%20analysis%2C%20in%20particular%20the%0AMapper%20algorithm%2C%20where%20it%20is%20desirable%20to%20work%20with%20a%20clustering%20algorithm%0Athat%20does%20not%20need%20tuning%20of%20its%20parameters.%20Indeed%2C%20we%20provide%20evidence%20that%0AAuToMATo%20performs%20well%20when%20used%20with%20Mapper.%20Finally%2C%20we%20provide%20an%0Aopen-source%20implementation%20of%20AuToMATo%20in%20Python%20that%20is%20fully%20compatible%20with%0Athe%20standard%20scikit-learn%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06958v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuToMATo%253A%2520An%2520Out-Of-The-Box%2520Persistence-Based%2520Clustering%2520Algorithm%26entry.906535625%3DMarius%2520Huber%2520and%2520Sara%2520Kalisnik%2520and%2520Patrick%2520Schnider%26entry.1292438233%3D%2520%2520We%2520present%2520AuToMATo%252C%2520a%2520novel%2520clustering%2520algorithm%2520based%2520on%2520persistent%250Ahomology.%2520While%2520AuToMATo%2520is%2520not%2520parameter-free%2520per%2520se%252C%2520we%2520provide%2520default%250Achoices%2520for%2520its%2520parameters%2520that%2520make%2520it%2520into%2520an%2520out-of-the-box%2520clustering%250Aalgorithm%2520that%2520performs%2520well%2520across%2520the%2520board.%2520AuToMATo%2520combines%2520the%2520existing%250AToMATo%2520clustering%2520algorithm%2520with%2520a%2520bootstrapping%2520procedure%2520in%2520order%2520to%2520separate%250Asignificant%2520peaks%2520of%2520an%2520estimated%2520density%2520function%2520from%2520non-significant%2520ones.%250AWe%2520perform%2520a%2520thorough%2520comparison%2520of%2520AuToMATo%2520%2528with%2520its%2520parameters%2520fixed%2520to%250Atheir%2520defaults%2529%2520against%2520many%2520other%2520state-of-the-art%2520clustering%2520algorithms.%2520We%250Afind%2520not%2520only%2520that%2520AuToMATo%2520compares%2520favorably%2520against%2520parameter-free%250Aclustering%2520algorithms%252C%2520but%2520in%2520many%2520instances%2520also%2520significantly%2520outperforms%250Aeven%2520the%2520best%2520selection%2520of%2520parameters%2520for%2520other%2520algorithms.%2520AuToMATo%2520is%250Amotivated%2520by%2520applications%2520in%2520topological%2520data%2520analysis%252C%2520in%2520particular%2520the%250AMapper%2520algorithm%252C%2520where%2520it%2520is%2520desirable%2520to%2520work%2520with%2520a%2520clustering%2520algorithm%250Athat%2520does%2520not%2520need%2520tuning%2520of%2520its%2520parameters.%2520Indeed%252C%2520we%2520provide%2520evidence%2520that%250AAuToMATo%2520performs%2520well%2520when%2520used%2520with%2520Mapper.%2520Finally%252C%2520we%2520provide%2520an%250Aopen-source%2520implementation%2520of%2520AuToMATo%2520in%2520Python%2520that%2520is%2520fully%2520compatible%2520with%250Athe%2520standard%2520scikit-learn%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06958v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuToMATo%3A%20An%20Out-Of-The-Box%20Persistence-Based%20Clustering%20Algorithm&entry.906535625=Marius%20Huber%20and%20Sara%20Kalisnik%20and%20Patrick%20Schnider&entry.1292438233=%20%20We%20present%20AuToMATo%2C%20a%20novel%20clustering%20algorithm%20based%20on%20persistent%0Ahomology.%20While%20AuToMATo%20is%20not%20parameter-free%20per%20se%2C%20we%20provide%20default%0Achoices%20for%20its%20parameters%20that%20make%20it%20into%20an%20out-of-the-box%20clustering%0Aalgorithm%20that%20performs%20well%20across%20the%20board.%20AuToMATo%20combines%20the%20existing%0AToMATo%20clustering%20algorithm%20with%20a%20bootstrapping%20procedure%20in%20order%20to%20separate%0Asignificant%20peaks%20of%20an%20estimated%20density%20function%20from%20non-significant%20ones.%0AWe%20perform%20a%20thorough%20comparison%20of%20AuToMATo%20%28with%20its%20parameters%20fixed%20to%0Atheir%20defaults%29%20against%20many%20other%20state-of-the-art%20clustering%20algorithms.%20We%0Afind%20not%20only%20that%20AuToMATo%20compares%20favorably%20against%20parameter-free%0Aclustering%20algorithms%2C%20but%20in%20many%20instances%20also%20significantly%20outperforms%0Aeven%20the%20best%20selection%20of%20parameters%20for%20other%20algorithms.%20AuToMATo%20is%0Amotivated%20by%20applications%20in%20topological%20data%20analysis%2C%20in%20particular%20the%0AMapper%20algorithm%2C%20where%20it%20is%20desirable%20to%20work%20with%20a%20clustering%20algorithm%0Athat%20does%20not%20need%20tuning%20of%20its%20parameters.%20Indeed%2C%20we%20provide%20evidence%20that%0AAuToMATo%20performs%20well%20when%20used%20with%20Mapper.%20Finally%2C%20we%20provide%20an%0Aopen-source%20implementation%20of%20AuToMATo%20in%20Python%20that%20is%20fully%20compatible%20with%0Athe%20standard%20scikit-learn%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06958v3&entry.124074799=Read"},
{"title": "Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical\n  Imaging", "author": "Ron Keuth and Paul Kaftan and Mattias P. Heinrich", "abstract": "  The generalization of the Transformer architecture via MetaFormer has\nreshaped our understanding of its success in computer vision. By replacing\nself-attention with simpler token mixers, MetaFormer provides strong baselines\nfor vision tasks. However, while extensively studied on natural image datasets,\nits use in medical imaging remains scarce, and existing works rarely compare\ndifferent token mixers, potentially overlooking more suitable designs choices.\nIn this work, we present the first comprehensive study of token mixers for\nmedical imaging. We systematically analyze pooling-, convolution-, and\nattention-based token mixers within the MetaFormer architecture on image\nclassification (global prediction task) and semantic segmentation (dense\nprediction task). Our evaluation spans eight datasets covering diverse\nmodalities and common challenges in the medical domain. Given the prevalence of\npretraining from natural images to mitigate medical data scarcity, we also\nexamine transferring pretrained weights to new token mixers. Our results show\nthat, for classification, low-complexity token mixers (e.g. grouped convolution\nor pooling) are sufficient, aligning with findings on natural images.\nPretrained weights remain useful despite the domain gap introduced by the new\ntoken mixer. For segmentation, we find that the local inductive bias of\nconvolutional token mixers is essential. Grouped convolutions emerge as the\npreferred choice, as they reduce runtime and parameter count compared to\nstandard convolutions, while the MetaFormer's channel-MLPs already provide the\nnecessary cross-channel interactions. Our code is available on GitHub.\n", "link": "http://arxiv.org/abs/2510.05971v1", "date": "2025-10-07", "relevancy": 2.1899, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5711}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5407}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shaken%20or%20Stirred%3F%20An%20Analysis%20of%20MetaFormer%27s%20Token%20Mixing%20for%20Medical%0A%20%20Imaging&body=Title%3A%20Shaken%20or%20Stirred%3F%20An%20Analysis%20of%20MetaFormer%27s%20Token%20Mixing%20for%20Medical%0A%20%20Imaging%0AAuthor%3A%20Ron%20Keuth%20and%20Paul%20Kaftan%20and%20Mattias%20P.%20Heinrich%0AAbstract%3A%20%20%20The%20generalization%20of%20the%20Transformer%20architecture%20via%20MetaFormer%20has%0Areshaped%20our%20understanding%20of%20its%20success%20in%20computer%20vision.%20By%20replacing%0Aself-attention%20with%20simpler%20token%20mixers%2C%20MetaFormer%20provides%20strong%20baselines%0Afor%20vision%20tasks.%20However%2C%20while%20extensively%20studied%20on%20natural%20image%20datasets%2C%0Aits%20use%20in%20medical%20imaging%20remains%20scarce%2C%20and%20existing%20works%20rarely%20compare%0Adifferent%20token%20mixers%2C%20potentially%20overlooking%20more%20suitable%20designs%20choices.%0AIn%20this%20work%2C%20we%20present%20the%20first%20comprehensive%20study%20of%20token%20mixers%20for%0Amedical%20imaging.%20We%20systematically%20analyze%20pooling-%2C%20convolution-%2C%20and%0Aattention-based%20token%20mixers%20within%20the%20MetaFormer%20architecture%20on%20image%0Aclassification%20%28global%20prediction%20task%29%20and%20semantic%20segmentation%20%28dense%0Aprediction%20task%29.%20Our%20evaluation%20spans%20eight%20datasets%20covering%20diverse%0Amodalities%20and%20common%20challenges%20in%20the%20medical%20domain.%20Given%20the%20prevalence%20of%0Apretraining%20from%20natural%20images%20to%20mitigate%20medical%20data%20scarcity%2C%20we%20also%0Aexamine%20transferring%20pretrained%20weights%20to%20new%20token%20mixers.%20Our%20results%20show%0Athat%2C%20for%20classification%2C%20low-complexity%20token%20mixers%20%28e.g.%20grouped%20convolution%0Aor%20pooling%29%20are%20sufficient%2C%20aligning%20with%20findings%20on%20natural%20images.%0APretrained%20weights%20remain%20useful%20despite%20the%20domain%20gap%20introduced%20by%20the%20new%0Atoken%20mixer.%20For%20segmentation%2C%20we%20find%20that%20the%20local%20inductive%20bias%20of%0Aconvolutional%20token%20mixers%20is%20essential.%20Grouped%20convolutions%20emerge%20as%20the%0Apreferred%20choice%2C%20as%20they%20reduce%20runtime%20and%20parameter%20count%20compared%20to%0Astandard%20convolutions%2C%20while%20the%20MetaFormer%27s%20channel-MLPs%20already%20provide%20the%0Anecessary%20cross-channel%20interactions.%20Our%20code%20is%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShaken%2520or%2520Stirred%253F%2520An%2520Analysis%2520of%2520MetaFormer%2527s%2520Token%2520Mixing%2520for%2520Medical%250A%2520%2520Imaging%26entry.906535625%3DRon%2520Keuth%2520and%2520Paul%2520Kaftan%2520and%2520Mattias%2520P.%2520Heinrich%26entry.1292438233%3D%2520%2520The%2520generalization%2520of%2520the%2520Transformer%2520architecture%2520via%2520MetaFormer%2520has%250Areshaped%2520our%2520understanding%2520of%2520its%2520success%2520in%2520computer%2520vision.%2520By%2520replacing%250Aself-attention%2520with%2520simpler%2520token%2520mixers%252C%2520MetaFormer%2520provides%2520strong%2520baselines%250Afor%2520vision%2520tasks.%2520However%252C%2520while%2520extensively%2520studied%2520on%2520natural%2520image%2520datasets%252C%250Aits%2520use%2520in%2520medical%2520imaging%2520remains%2520scarce%252C%2520and%2520existing%2520works%2520rarely%2520compare%250Adifferent%2520token%2520mixers%252C%2520potentially%2520overlooking%2520more%2520suitable%2520designs%2520choices.%250AIn%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520comprehensive%2520study%2520of%2520token%2520mixers%2520for%250Amedical%2520imaging.%2520We%2520systematically%2520analyze%2520pooling-%252C%2520convolution-%252C%2520and%250Aattention-based%2520token%2520mixers%2520within%2520the%2520MetaFormer%2520architecture%2520on%2520image%250Aclassification%2520%2528global%2520prediction%2520task%2529%2520and%2520semantic%2520segmentation%2520%2528dense%250Aprediction%2520task%2529.%2520Our%2520evaluation%2520spans%2520eight%2520datasets%2520covering%2520diverse%250Amodalities%2520and%2520common%2520challenges%2520in%2520the%2520medical%2520domain.%2520Given%2520the%2520prevalence%2520of%250Apretraining%2520from%2520natural%2520images%2520to%2520mitigate%2520medical%2520data%2520scarcity%252C%2520we%2520also%250Aexamine%2520transferring%2520pretrained%2520weights%2520to%2520new%2520token%2520mixers.%2520Our%2520results%2520show%250Athat%252C%2520for%2520classification%252C%2520low-complexity%2520token%2520mixers%2520%2528e.g.%2520grouped%2520convolution%250Aor%2520pooling%2529%2520are%2520sufficient%252C%2520aligning%2520with%2520findings%2520on%2520natural%2520images.%250APretrained%2520weights%2520remain%2520useful%2520despite%2520the%2520domain%2520gap%2520introduced%2520by%2520the%2520new%250Atoken%2520mixer.%2520For%2520segmentation%252C%2520we%2520find%2520that%2520the%2520local%2520inductive%2520bias%2520of%250Aconvolutional%2520token%2520mixers%2520is%2520essential.%2520Grouped%2520convolutions%2520emerge%2520as%2520the%250Apreferred%2520choice%252C%2520as%2520they%2520reduce%2520runtime%2520and%2520parameter%2520count%2520compared%2520to%250Astandard%2520convolutions%252C%2520while%2520the%2520MetaFormer%2527s%2520channel-MLPs%2520already%2520provide%2520the%250Anecessary%2520cross-channel%2520interactions.%2520Our%2520code%2520is%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shaken%20or%20Stirred%3F%20An%20Analysis%20of%20MetaFormer%27s%20Token%20Mixing%20for%20Medical%0A%20%20Imaging&entry.906535625=Ron%20Keuth%20and%20Paul%20Kaftan%20and%20Mattias%20P.%20Heinrich&entry.1292438233=%20%20The%20generalization%20of%20the%20Transformer%20architecture%20via%20MetaFormer%20has%0Areshaped%20our%20understanding%20of%20its%20success%20in%20computer%20vision.%20By%20replacing%0Aself-attention%20with%20simpler%20token%20mixers%2C%20MetaFormer%20provides%20strong%20baselines%0Afor%20vision%20tasks.%20However%2C%20while%20extensively%20studied%20on%20natural%20image%20datasets%2C%0Aits%20use%20in%20medical%20imaging%20remains%20scarce%2C%20and%20existing%20works%20rarely%20compare%0Adifferent%20token%20mixers%2C%20potentially%20overlooking%20more%20suitable%20designs%20choices.%0AIn%20this%20work%2C%20we%20present%20the%20first%20comprehensive%20study%20of%20token%20mixers%20for%0Amedical%20imaging.%20We%20systematically%20analyze%20pooling-%2C%20convolution-%2C%20and%0Aattention-based%20token%20mixers%20within%20the%20MetaFormer%20architecture%20on%20image%0Aclassification%20%28global%20prediction%20task%29%20and%20semantic%20segmentation%20%28dense%0Aprediction%20task%29.%20Our%20evaluation%20spans%20eight%20datasets%20covering%20diverse%0Amodalities%20and%20common%20challenges%20in%20the%20medical%20domain.%20Given%20the%20prevalence%20of%0Apretraining%20from%20natural%20images%20to%20mitigate%20medical%20data%20scarcity%2C%20we%20also%0Aexamine%20transferring%20pretrained%20weights%20to%20new%20token%20mixers.%20Our%20results%20show%0Athat%2C%20for%20classification%2C%20low-complexity%20token%20mixers%20%28e.g.%20grouped%20convolution%0Aor%20pooling%29%20are%20sufficient%2C%20aligning%20with%20findings%20on%20natural%20images.%0APretrained%20weights%20remain%20useful%20despite%20the%20domain%20gap%20introduced%20by%20the%20new%0Atoken%20mixer.%20For%20segmentation%2C%20we%20find%20that%20the%20local%20inductive%20bias%20of%0Aconvolutional%20token%20mixers%20is%20essential.%20Grouped%20convolutions%20emerge%20as%20the%0Apreferred%20choice%2C%20as%20they%20reduce%20runtime%20and%20parameter%20count%20compared%20to%0Astandard%20convolutions%2C%20while%20the%20MetaFormer%27s%20channel-MLPs%20already%20provide%20the%0Anecessary%20cross-channel%20interactions.%20Our%20code%20is%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05971v1&entry.124074799=Read"},
{"title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by\n  Measuring Reasoning Effort", "author": "Xinpeng Wang and Nitish Joshi and Barbara Plank and Rico Angell and He He", "abstract": "  Reward hacking, where a reasoning model exploits loopholes in a reward\nfunction to achieve high rewards without solving the intended task, poses a\nsignificant threat. This behavior may be explicit, i.e. verbalized in the\nmodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thus\nbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE\n(Truncated Reasoning AUC Evaluation). Our key observation is that hacking\noccurs when exploiting the loophole is easier than solving the actual task.\nThis means that the model is using less 'effort' than required to achieve high\nreward. TRACE quantifies effort by measuring how early a model's reasoning\nbecomes sufficient to obtain the reward. We progressively truncate a model's\nCoT at various lengths, force the model to answer, and estimate the expected\nreward at each cutoff. A hacking model, which takes a shortcut, will achieve a\nhigh expected reward with only a small fraction of its CoT, yielding a large\narea under the accuracy-vs-length curve. TRACE achieves over 65% gains over our\nstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B\nmonitor in coding. We further show that TRACE can discover unknown loopholes\nduring training. Overall, TRACE offers a scalable unsupervised approach for\noversight where current monitoring methods prove ineffective.\n", "link": "http://arxiv.org/abs/2510.01367v3", "date": "2025-10-07", "relevancy": 2.1869, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4385}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20It%20Thinking%20or%20Cheating%3F%20Detecting%20Implicit%20Reward%20Hacking%20by%0A%20%20Measuring%20Reasoning%20Effort&body=Title%3A%20Is%20It%20Thinking%20or%20Cheating%3F%20Detecting%20Implicit%20Reward%20Hacking%20by%0A%20%20Measuring%20Reasoning%20Effort%0AAuthor%3A%20Xinpeng%20Wang%20and%20Nitish%20Joshi%20and%20Barbara%20Plank%20and%20Rico%20Angell%20and%20He%20He%0AAbstract%3A%20%20%20Reward%20hacking%2C%20where%20a%20reasoning%20model%20exploits%20loopholes%20in%20a%20reward%0Afunction%20to%20achieve%20high%20rewards%20without%20solving%20the%20intended%20task%2C%20poses%20a%0Asignificant%20threat.%20This%20behavior%20may%20be%20explicit%2C%20i.e.%20verbalized%20in%20the%0Amodel%27s%20chain-of-thought%20%28CoT%29%2C%20or%20implicit%2C%20where%20the%20CoT%20appears%20benign%20thus%0Abypasses%20CoT%20monitors.%20To%20detect%20implicit%20reward%20hacking%2C%20we%20propose%20TRACE%0A%28Truncated%20Reasoning%20AUC%20Evaluation%29.%20Our%20key%20observation%20is%20that%20hacking%0Aoccurs%20when%20exploiting%20the%20loophole%20is%20easier%20than%20solving%20the%20actual%20task.%0AThis%20means%20that%20the%20model%20is%20using%20less%20%27effort%27%20than%20required%20to%20achieve%20high%0Areward.%20TRACE%20quantifies%20effort%20by%20measuring%20how%20early%20a%20model%27s%20reasoning%0Abecomes%20sufficient%20to%20obtain%20the%20reward.%20We%20progressively%20truncate%20a%20model%27s%0ACoT%20at%20various%20lengths%2C%20force%20the%20model%20to%20answer%2C%20and%20estimate%20the%20expected%0Areward%20at%20each%20cutoff.%20A%20hacking%20model%2C%20which%20takes%20a%20shortcut%2C%20will%20achieve%20a%0Ahigh%20expected%20reward%20with%20only%20a%20small%20fraction%20of%20its%20CoT%2C%20yielding%20a%20large%0Aarea%20under%20the%20accuracy-vs-length%20curve.%20TRACE%20achieves%20over%2065%25%20gains%20over%20our%0Astrongest%2072B%20CoT%20monitor%20in%20math%20reasoning%2C%20and%20over%2030%25%20gains%20over%20a%2032B%0Amonitor%20in%20coding.%20We%20further%20show%20that%20TRACE%20can%20discover%20unknown%20loopholes%0Aduring%20training.%20Overall%2C%20TRACE%20offers%20a%20scalable%20unsupervised%20approach%20for%0Aoversight%20where%20current%20monitoring%20methods%20prove%20ineffective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01367v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520It%2520Thinking%2520or%2520Cheating%253F%2520Detecting%2520Implicit%2520Reward%2520Hacking%2520by%250A%2520%2520Measuring%2520Reasoning%2520Effort%26entry.906535625%3DXinpeng%2520Wang%2520and%2520Nitish%2520Joshi%2520and%2520Barbara%2520Plank%2520and%2520Rico%2520Angell%2520and%2520He%2520He%26entry.1292438233%3D%2520%2520Reward%2520hacking%252C%2520where%2520a%2520reasoning%2520model%2520exploits%2520loopholes%2520in%2520a%2520reward%250Afunction%2520to%2520achieve%2520high%2520rewards%2520without%2520solving%2520the%2520intended%2520task%252C%2520poses%2520a%250Asignificant%2520threat.%2520This%2520behavior%2520may%2520be%2520explicit%252C%2520i.e.%2520verbalized%2520in%2520the%250Amodel%2527s%2520chain-of-thought%2520%2528CoT%2529%252C%2520or%2520implicit%252C%2520where%2520the%2520CoT%2520appears%2520benign%2520thus%250Abypasses%2520CoT%2520monitors.%2520To%2520detect%2520implicit%2520reward%2520hacking%252C%2520we%2520propose%2520TRACE%250A%2528Truncated%2520Reasoning%2520AUC%2520Evaluation%2529.%2520Our%2520key%2520observation%2520is%2520that%2520hacking%250Aoccurs%2520when%2520exploiting%2520the%2520loophole%2520is%2520easier%2520than%2520solving%2520the%2520actual%2520task.%250AThis%2520means%2520that%2520the%2520model%2520is%2520using%2520less%2520%2527effort%2527%2520than%2520required%2520to%2520achieve%2520high%250Areward.%2520TRACE%2520quantifies%2520effort%2520by%2520measuring%2520how%2520early%2520a%2520model%2527s%2520reasoning%250Abecomes%2520sufficient%2520to%2520obtain%2520the%2520reward.%2520We%2520progressively%2520truncate%2520a%2520model%2527s%250ACoT%2520at%2520various%2520lengths%252C%2520force%2520the%2520model%2520to%2520answer%252C%2520and%2520estimate%2520the%2520expected%250Areward%2520at%2520each%2520cutoff.%2520A%2520hacking%2520model%252C%2520which%2520takes%2520a%2520shortcut%252C%2520will%2520achieve%2520a%250Ahigh%2520expected%2520reward%2520with%2520only%2520a%2520small%2520fraction%2520of%2520its%2520CoT%252C%2520yielding%2520a%2520large%250Aarea%2520under%2520the%2520accuracy-vs-length%2520curve.%2520TRACE%2520achieves%2520over%252065%2525%2520gains%2520over%2520our%250Astrongest%252072B%2520CoT%2520monitor%2520in%2520math%2520reasoning%252C%2520and%2520over%252030%2525%2520gains%2520over%2520a%252032B%250Amonitor%2520in%2520coding.%2520We%2520further%2520show%2520that%2520TRACE%2520can%2520discover%2520unknown%2520loopholes%250Aduring%2520training.%2520Overall%252C%2520TRACE%2520offers%2520a%2520scalable%2520unsupervised%2520approach%2520for%250Aoversight%2520where%2520current%2520monitoring%2520methods%2520prove%2520ineffective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01367v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20It%20Thinking%20or%20Cheating%3F%20Detecting%20Implicit%20Reward%20Hacking%20by%0A%20%20Measuring%20Reasoning%20Effort&entry.906535625=Xinpeng%20Wang%20and%20Nitish%20Joshi%20and%20Barbara%20Plank%20and%20Rico%20Angell%20and%20He%20He&entry.1292438233=%20%20Reward%20hacking%2C%20where%20a%20reasoning%20model%20exploits%20loopholes%20in%20a%20reward%0Afunction%20to%20achieve%20high%20rewards%20without%20solving%20the%20intended%20task%2C%20poses%20a%0Asignificant%20threat.%20This%20behavior%20may%20be%20explicit%2C%20i.e.%20verbalized%20in%20the%0Amodel%27s%20chain-of-thought%20%28CoT%29%2C%20or%20implicit%2C%20where%20the%20CoT%20appears%20benign%20thus%0Abypasses%20CoT%20monitors.%20To%20detect%20implicit%20reward%20hacking%2C%20we%20propose%20TRACE%0A%28Truncated%20Reasoning%20AUC%20Evaluation%29.%20Our%20key%20observation%20is%20that%20hacking%0Aoccurs%20when%20exploiting%20the%20loophole%20is%20easier%20than%20solving%20the%20actual%20task.%0AThis%20means%20that%20the%20model%20is%20using%20less%20%27effort%27%20than%20required%20to%20achieve%20high%0Areward.%20TRACE%20quantifies%20effort%20by%20measuring%20how%20early%20a%20model%27s%20reasoning%0Abecomes%20sufficient%20to%20obtain%20the%20reward.%20We%20progressively%20truncate%20a%20model%27s%0ACoT%20at%20various%20lengths%2C%20force%20the%20model%20to%20answer%2C%20and%20estimate%20the%20expected%0Areward%20at%20each%20cutoff.%20A%20hacking%20model%2C%20which%20takes%20a%20shortcut%2C%20will%20achieve%20a%0Ahigh%20expected%20reward%20with%20only%20a%20small%20fraction%20of%20its%20CoT%2C%20yielding%20a%20large%0Aarea%20under%20the%20accuracy-vs-length%20curve.%20TRACE%20achieves%20over%2065%25%20gains%20over%20our%0Astrongest%2072B%20CoT%20monitor%20in%20math%20reasoning%2C%20and%20over%2030%25%20gains%20over%20a%2032B%0Amonitor%20in%20coding.%20We%20further%20show%20that%20TRACE%20can%20discover%20unknown%20loopholes%0Aduring%20training.%20Overall%2C%20TRACE%20offers%20a%20scalable%20unsupervised%20approach%20for%0Aoversight%20where%20current%20monitoring%20methods%20prove%20ineffective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01367v3&entry.124074799=Read"},
{"title": "CreditDecoding: Accelerating Parallel Decoding in Diffusion Large\n  Language Models with Trace Credits", "author": "Kangyu Wang and Zhiyun Jiang and Haibo Feng and Weijia Zhao and Lin Liu and Jianguo Li and Zhenzhong Lan and Weiyao Lin", "abstract": "  Diffusion large language models (dLLMs) generate text through iterative\ndenoising steps, achieving parallel decoding by denoising only high-confidence\npositions at each step. However, existing approaches often repetitively remask\ntokens due to initially low confidence scores, leading to redundant iterations\nand limiting overall acceleration. Through the analysis of dLLM decoding\ntraces, we observe that the model often determines the final prediction for a\ntoken several steps before the decoding step. To leverage this historical\ninformation and avoid redundant steps, we introduce the concept of Trace\nCredit, which quantifies each token's convergence potential by accumulating\nhistorical logits. Furthermore, we propose CreditDecoding, a training-free\nparallel decoding algorithm that accelerates the confidence convergence of\ncorrect but underconfident tokens by fusing current logits with Trace Credit.\nThis process significantly reduces redundant iterations and enhances decoding\nrobustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup\nand a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times\nspeedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.\nImportantly, CreditDecoding scales effectively to long sequences and is\northogonal to mainstream inference optimizations, making it a readily\nintegrable and versatile solution.\n", "link": "http://arxiv.org/abs/2510.06133v1", "date": "2025-10-07", "relevancy": 2.1786, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6049}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CreditDecoding%3A%20Accelerating%20Parallel%20Decoding%20in%20Diffusion%20Large%0A%20%20Language%20Models%20with%20Trace%20Credits&body=Title%3A%20CreditDecoding%3A%20Accelerating%20Parallel%20Decoding%20in%20Diffusion%20Large%0A%20%20Language%20Models%20with%20Trace%20Credits%0AAuthor%3A%20Kangyu%20Wang%20and%20Zhiyun%20Jiang%20and%20Haibo%20Feng%20and%20Weijia%20Zhao%20and%20Lin%20Liu%20and%20Jianguo%20Li%20and%20Zhenzhong%20Lan%20and%20Weiyao%20Lin%0AAbstract%3A%20%20%20Diffusion%20large%20language%20models%20%28dLLMs%29%20generate%20text%20through%20iterative%0Adenoising%20steps%2C%20achieving%20parallel%20decoding%20by%20denoising%20only%20high-confidence%0Apositions%20at%20each%20step.%20However%2C%20existing%20approaches%20often%20repetitively%20remask%0Atokens%20due%20to%20initially%20low%20confidence%20scores%2C%20leading%20to%20redundant%20iterations%0Aand%20limiting%20overall%20acceleration.%20Through%20the%20analysis%20of%20dLLM%20decoding%0Atraces%2C%20we%20observe%20that%20the%20model%20often%20determines%20the%20final%20prediction%20for%20a%0Atoken%20several%20steps%20before%20the%20decoding%20step.%20To%20leverage%20this%20historical%0Ainformation%20and%20avoid%20redundant%20steps%2C%20we%20introduce%20the%20concept%20of%20Trace%0ACredit%2C%20which%20quantifies%20each%20token%27s%20convergence%20potential%20by%20accumulating%0Ahistorical%20logits.%20Furthermore%2C%20we%20propose%20CreditDecoding%2C%20a%20training-free%0Aparallel%20decoding%20algorithm%20that%20accelerates%20the%20confidence%20convergence%20of%0Acorrect%20but%20underconfident%20tokens%20by%20fusing%20current%20logits%20with%20Trace%20Credit.%0AThis%20process%20significantly%20reduces%20redundant%20iterations%20and%20enhances%20decoding%0Arobustness.%20On%20eight%20benchmarks%2C%20CreditDecoding%20achieves%20a%205.48%20times%20speedup%0Aand%20a%200.48%20performance%20improvement%20over%20LLaDA-8B-Instruct%2C%20and%20a%204.11%20times%0Aspeedup%20with%20a%200.15%20performance%20improvement%20over%20LLaDA-MoE-Instruct.%0AImportantly%2C%20CreditDecoding%20scales%20effectively%20to%20long%20sequences%20and%20is%0Aorthogonal%20to%20mainstream%20inference%20optimizations%2C%20making%20it%20a%20readily%0Aintegrable%20and%20versatile%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreditDecoding%253A%2520Accelerating%2520Parallel%2520Decoding%2520in%2520Diffusion%2520Large%250A%2520%2520Language%2520Models%2520with%2520Trace%2520Credits%26entry.906535625%3DKangyu%2520Wang%2520and%2520Zhiyun%2520Jiang%2520and%2520Haibo%2520Feng%2520and%2520Weijia%2520Zhao%2520and%2520Lin%2520Liu%2520and%2520Jianguo%2520Li%2520and%2520Zhenzhong%2520Lan%2520and%2520Weiyao%2520Lin%26entry.1292438233%3D%2520%2520Diffusion%2520large%2520language%2520models%2520%2528dLLMs%2529%2520generate%2520text%2520through%2520iterative%250Adenoising%2520steps%252C%2520achieving%2520parallel%2520decoding%2520by%2520denoising%2520only%2520high-confidence%250Apositions%2520at%2520each%2520step.%2520However%252C%2520existing%2520approaches%2520often%2520repetitively%2520remask%250Atokens%2520due%2520to%2520initially%2520low%2520confidence%2520scores%252C%2520leading%2520to%2520redundant%2520iterations%250Aand%2520limiting%2520overall%2520acceleration.%2520Through%2520the%2520analysis%2520of%2520dLLM%2520decoding%250Atraces%252C%2520we%2520observe%2520that%2520the%2520model%2520often%2520determines%2520the%2520final%2520prediction%2520for%2520a%250Atoken%2520several%2520steps%2520before%2520the%2520decoding%2520step.%2520To%2520leverage%2520this%2520historical%250Ainformation%2520and%2520avoid%2520redundant%2520steps%252C%2520we%2520introduce%2520the%2520concept%2520of%2520Trace%250ACredit%252C%2520which%2520quantifies%2520each%2520token%2527s%2520convergence%2520potential%2520by%2520accumulating%250Ahistorical%2520logits.%2520Furthermore%252C%2520we%2520propose%2520CreditDecoding%252C%2520a%2520training-free%250Aparallel%2520decoding%2520algorithm%2520that%2520accelerates%2520the%2520confidence%2520convergence%2520of%250Acorrect%2520but%2520underconfident%2520tokens%2520by%2520fusing%2520current%2520logits%2520with%2520Trace%2520Credit.%250AThis%2520process%2520significantly%2520reduces%2520redundant%2520iterations%2520and%2520enhances%2520decoding%250Arobustness.%2520On%2520eight%2520benchmarks%252C%2520CreditDecoding%2520achieves%2520a%25205.48%2520times%2520speedup%250Aand%2520a%25200.48%2520performance%2520improvement%2520over%2520LLaDA-8B-Instruct%252C%2520and%2520a%25204.11%2520times%250Aspeedup%2520with%2520a%25200.15%2520performance%2520improvement%2520over%2520LLaDA-MoE-Instruct.%250AImportantly%252C%2520CreditDecoding%2520scales%2520effectively%2520to%2520long%2520sequences%2520and%2520is%250Aorthogonal%2520to%2520mainstream%2520inference%2520optimizations%252C%2520making%2520it%2520a%2520readily%250Aintegrable%2520and%2520versatile%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CreditDecoding%3A%20Accelerating%20Parallel%20Decoding%20in%20Diffusion%20Large%0A%20%20Language%20Models%20with%20Trace%20Credits&entry.906535625=Kangyu%20Wang%20and%20Zhiyun%20Jiang%20and%20Haibo%20Feng%20and%20Weijia%20Zhao%20and%20Lin%20Liu%20and%20Jianguo%20Li%20and%20Zhenzhong%20Lan%20and%20Weiyao%20Lin&entry.1292438233=%20%20Diffusion%20large%20language%20models%20%28dLLMs%29%20generate%20text%20through%20iterative%0Adenoising%20steps%2C%20achieving%20parallel%20decoding%20by%20denoising%20only%20high-confidence%0Apositions%20at%20each%20step.%20However%2C%20existing%20approaches%20often%20repetitively%20remask%0Atokens%20due%20to%20initially%20low%20confidence%20scores%2C%20leading%20to%20redundant%20iterations%0Aand%20limiting%20overall%20acceleration.%20Through%20the%20analysis%20of%20dLLM%20decoding%0Atraces%2C%20we%20observe%20that%20the%20model%20often%20determines%20the%20final%20prediction%20for%20a%0Atoken%20several%20steps%20before%20the%20decoding%20step.%20To%20leverage%20this%20historical%0Ainformation%20and%20avoid%20redundant%20steps%2C%20we%20introduce%20the%20concept%20of%20Trace%0ACredit%2C%20which%20quantifies%20each%20token%27s%20convergence%20potential%20by%20accumulating%0Ahistorical%20logits.%20Furthermore%2C%20we%20propose%20CreditDecoding%2C%20a%20training-free%0Aparallel%20decoding%20algorithm%20that%20accelerates%20the%20confidence%20convergence%20of%0Acorrect%20but%20underconfident%20tokens%20by%20fusing%20current%20logits%20with%20Trace%20Credit.%0AThis%20process%20significantly%20reduces%20redundant%20iterations%20and%20enhances%20decoding%0Arobustness.%20On%20eight%20benchmarks%2C%20CreditDecoding%20achieves%20a%205.48%20times%20speedup%0Aand%20a%200.48%20performance%20improvement%20over%20LLaDA-8B-Instruct%2C%20and%20a%204.11%20times%0Aspeedup%20with%20a%200.15%20performance%20improvement%20over%20LLaDA-MoE-Instruct.%0AImportantly%2C%20CreditDecoding%20scales%20effectively%20to%20long%20sequences%20and%20is%0Aorthogonal%20to%20mainstream%20inference%20optimizations%2C%20making%20it%20a%20readily%0Aintegrable%20and%20versatile%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06133v1&entry.124074799=Read"},
{"title": "EmoHRNet: High-Resolution Neural Network Based Speech Emotion\n  Recognition", "author": "Akshay Muppidi and Martin Radfar", "abstract": "  Speech emotion recognition (SER) is pivotal for enhancing human-machine\ninteractions. This paper introduces \"EmoHRNet\", a novel adaptation of\nHigh-Resolution Networks (HRNet) tailored for SER. The HRNet structure is\ndesigned to maintain high-resolution representations from the initial to the\nfinal layers. By transforming audio samples into spectrograms, EmoHRNet\nleverages the HRNet architecture to extract high-level features. EmoHRNet's\nunique architecture maintains high-resolution representations throughout,\ncapturing both granular and overarching emotional cues from speech signals. The\nmodel outperforms leading models, achieving accuracies of 92.45% on RAVDESS,\n80.06% on IEMOCAP, and 92.77% on EMOVO. Thus, we show that EmoHRNet sets a new\nbenchmark in the SER domain.\n", "link": "http://arxiv.org/abs/2510.06072v1", "date": "2025-10-07", "relevancy": 2.1706, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoHRNet%3A%20High-Resolution%20Neural%20Network%20Based%20Speech%20Emotion%0A%20%20Recognition&body=Title%3A%20EmoHRNet%3A%20High-Resolution%20Neural%20Network%20Based%20Speech%20Emotion%0A%20%20Recognition%0AAuthor%3A%20Akshay%20Muppidi%20and%20Martin%20Radfar%0AAbstract%3A%20%20%20Speech%20emotion%20recognition%20%28SER%29%20is%20pivotal%20for%20enhancing%20human-machine%0Ainteractions.%20This%20paper%20introduces%20%22EmoHRNet%22%2C%20a%20novel%20adaptation%20of%0AHigh-Resolution%20Networks%20%28HRNet%29%20tailored%20for%20SER.%20The%20HRNet%20structure%20is%0Adesigned%20to%20maintain%20high-resolution%20representations%20from%20the%20initial%20to%20the%0Afinal%20layers.%20By%20transforming%20audio%20samples%20into%20spectrograms%2C%20EmoHRNet%0Aleverages%20the%20HRNet%20architecture%20to%20extract%20high-level%20features.%20EmoHRNet%27s%0Aunique%20architecture%20maintains%20high-resolution%20representations%20throughout%2C%0Acapturing%20both%20granular%20and%20overarching%20emotional%20cues%20from%20speech%20signals.%20The%0Amodel%20outperforms%20leading%20models%2C%20achieving%20accuracies%20of%2092.45%25%20on%20RAVDESS%2C%0A80.06%25%20on%20IEMOCAP%2C%20and%2092.77%25%20on%20EMOVO.%20Thus%2C%20we%20show%20that%20EmoHRNet%20sets%20a%20new%0Abenchmark%20in%20the%20SER%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoHRNet%253A%2520High-Resolution%2520Neural%2520Network%2520Based%2520Speech%2520Emotion%250A%2520%2520Recognition%26entry.906535625%3DAkshay%2520Muppidi%2520and%2520Martin%2520Radfar%26entry.1292438233%3D%2520%2520Speech%2520emotion%2520recognition%2520%2528SER%2529%2520is%2520pivotal%2520for%2520enhancing%2520human-machine%250Ainteractions.%2520This%2520paper%2520introduces%2520%2522EmoHRNet%2522%252C%2520a%2520novel%2520adaptation%2520of%250AHigh-Resolution%2520Networks%2520%2528HRNet%2529%2520tailored%2520for%2520SER.%2520The%2520HRNet%2520structure%2520is%250Adesigned%2520to%2520maintain%2520high-resolution%2520representations%2520from%2520the%2520initial%2520to%2520the%250Afinal%2520layers.%2520By%2520transforming%2520audio%2520samples%2520into%2520spectrograms%252C%2520EmoHRNet%250Aleverages%2520the%2520HRNet%2520architecture%2520to%2520extract%2520high-level%2520features.%2520EmoHRNet%2527s%250Aunique%2520architecture%2520maintains%2520high-resolution%2520representations%2520throughout%252C%250Acapturing%2520both%2520granular%2520and%2520overarching%2520emotional%2520cues%2520from%2520speech%2520signals.%2520The%250Amodel%2520outperforms%2520leading%2520models%252C%2520achieving%2520accuracies%2520of%252092.45%2525%2520on%2520RAVDESS%252C%250A80.06%2525%2520on%2520IEMOCAP%252C%2520and%252092.77%2525%2520on%2520EMOVO.%2520Thus%252C%2520we%2520show%2520that%2520EmoHRNet%2520sets%2520a%2520new%250Abenchmark%2520in%2520the%2520SER%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoHRNet%3A%20High-Resolution%20Neural%20Network%20Based%20Speech%20Emotion%0A%20%20Recognition&entry.906535625=Akshay%20Muppidi%20and%20Martin%20Radfar&entry.1292438233=%20%20Speech%20emotion%20recognition%20%28SER%29%20is%20pivotal%20for%20enhancing%20human-machine%0Ainteractions.%20This%20paper%20introduces%20%22EmoHRNet%22%2C%20a%20novel%20adaptation%20of%0AHigh-Resolution%20Networks%20%28HRNet%29%20tailored%20for%20SER.%20The%20HRNet%20structure%20is%0Adesigned%20to%20maintain%20high-resolution%20representations%20from%20the%20initial%20to%20the%0Afinal%20layers.%20By%20transforming%20audio%20samples%20into%20spectrograms%2C%20EmoHRNet%0Aleverages%20the%20HRNet%20architecture%20to%20extract%20high-level%20features.%20EmoHRNet%27s%0Aunique%20architecture%20maintains%20high-resolution%20representations%20throughout%2C%0Acapturing%20both%20granular%20and%20overarching%20emotional%20cues%20from%20speech%20signals.%20The%0Amodel%20outperforms%20leading%20models%2C%20achieving%20accuracies%20of%2092.45%25%20on%20RAVDESS%2C%0A80.06%25%20on%20IEMOCAP%2C%20and%2092.77%25%20on%20EMOVO.%20Thus%2C%20we%20show%20that%20EmoHRNet%20sets%20a%20new%0Abenchmark%20in%20the%20SER%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06072v1&entry.124074799=Read"},
{"title": "A Novel Technique for Robust Training of Deep Networks With Multisource\n  Weak Labeled Remote Sensing Data", "author": "Gianmarco Perantoni and Lorenzo Bruzzone", "abstract": "  Deep learning has gained broad interest in remote sensing image scene\nclassification thanks to the effectiveness of deep neural networks in\nextracting the semantics from complex data. However, deep networks require\nlarge amounts of training samples to obtain good generalization capabilities\nand are sensitive to errors in the training labels. This is a problem in remote\nsensing since highly reliable labels can be obtained at high costs and in\nlimited amount. However, many sources of less reliable labeled data are\navailable, e.g., obsolete digital maps. In order to train deep networks with\nlarger datasets, we propose both the combination of single or multiple weak\nsources of labeled data with a small but reliable dataset to generate\nmultisource labeled datasets and a novel training strategy where the\nreliability of each source is taken in consideration. This is done by\nexploiting the transition matrices describing the statistics of the errors of\neach source. The transition matrices are embedded into the labels and used\nduring the training process to weigh each label according to the related\nsource. The proposed method acts as a weighting scheme at gradient level, where\neach instance contributes with different weights to the optimization of\ndifferent classes. The effectiveness of the proposed method is validated by\nexperiments on different datasets. The results proved the robustness and\ncapability of leveraging on unreliable source of labels of the proposed method.\n", "link": "http://arxiv.org/abs/2510.05760v1", "date": "2025-10-07", "relevancy": 2.1696, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Technique%20for%20Robust%20Training%20of%20Deep%20Networks%20With%20Multisource%0A%20%20Weak%20Labeled%20Remote%20Sensing%20Data&body=Title%3A%20A%20Novel%20Technique%20for%20Robust%20Training%20of%20Deep%20Networks%20With%20Multisource%0A%20%20Weak%20Labeled%20Remote%20Sensing%20Data%0AAuthor%3A%20Gianmarco%20Perantoni%20and%20Lorenzo%20Bruzzone%0AAbstract%3A%20%20%20Deep%20learning%20has%20gained%20broad%20interest%20in%20remote%20sensing%20image%20scene%0Aclassification%20thanks%20to%20the%20effectiveness%20of%20deep%20neural%20networks%20in%0Aextracting%20the%20semantics%20from%20complex%20data.%20However%2C%20deep%20networks%20require%0Alarge%20amounts%20of%20training%20samples%20to%20obtain%20good%20generalization%20capabilities%0Aand%20are%20sensitive%20to%20errors%20in%20the%20training%20labels.%20This%20is%20a%20problem%20in%20remote%0Asensing%20since%20highly%20reliable%20labels%20can%20be%20obtained%20at%20high%20costs%20and%20in%0Alimited%20amount.%20However%2C%20many%20sources%20of%20less%20reliable%20labeled%20data%20are%0Aavailable%2C%20e.g.%2C%20obsolete%20digital%20maps.%20In%20order%20to%20train%20deep%20networks%20with%0Alarger%20datasets%2C%20we%20propose%20both%20the%20combination%20of%20single%20or%20multiple%20weak%0Asources%20of%20labeled%20data%20with%20a%20small%20but%20reliable%20dataset%20to%20generate%0Amultisource%20labeled%20datasets%20and%20a%20novel%20training%20strategy%20where%20the%0Areliability%20of%20each%20source%20is%20taken%20in%20consideration.%20This%20is%20done%20by%0Aexploiting%20the%20transition%20matrices%20describing%20the%20statistics%20of%20the%20errors%20of%0Aeach%20source.%20The%20transition%20matrices%20are%20embedded%20into%20the%20labels%20and%20used%0Aduring%20the%20training%20process%20to%20weigh%20each%20label%20according%20to%20the%20related%0Asource.%20The%20proposed%20method%20acts%20as%20a%20weighting%20scheme%20at%20gradient%20level%2C%20where%0Aeach%20instance%20contributes%20with%20different%20weights%20to%20the%20optimization%20of%0Adifferent%20classes.%20The%20effectiveness%20of%20the%20proposed%20method%20is%20validated%20by%0Aexperiments%20on%20different%20datasets.%20The%20results%20proved%20the%20robustness%20and%0Acapability%20of%20leveraging%20on%20unreliable%20source%20of%20labels%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Technique%2520for%2520Robust%2520Training%2520of%2520Deep%2520Networks%2520With%2520Multisource%250A%2520%2520Weak%2520Labeled%2520Remote%2520Sensing%2520Data%26entry.906535625%3DGianmarco%2520Perantoni%2520and%2520Lorenzo%2520Bruzzone%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520gained%2520broad%2520interest%2520in%2520remote%2520sensing%2520image%2520scene%250Aclassification%2520thanks%2520to%2520the%2520effectiveness%2520of%2520deep%2520neural%2520networks%2520in%250Aextracting%2520the%2520semantics%2520from%2520complex%2520data.%2520However%252C%2520deep%2520networks%2520require%250Alarge%2520amounts%2520of%2520training%2520samples%2520to%2520obtain%2520good%2520generalization%2520capabilities%250Aand%2520are%2520sensitive%2520to%2520errors%2520in%2520the%2520training%2520labels.%2520This%2520is%2520a%2520problem%2520in%2520remote%250Asensing%2520since%2520highly%2520reliable%2520labels%2520can%2520be%2520obtained%2520at%2520high%2520costs%2520and%2520in%250Alimited%2520amount.%2520However%252C%2520many%2520sources%2520of%2520less%2520reliable%2520labeled%2520data%2520are%250Aavailable%252C%2520e.g.%252C%2520obsolete%2520digital%2520maps.%2520In%2520order%2520to%2520train%2520deep%2520networks%2520with%250Alarger%2520datasets%252C%2520we%2520propose%2520both%2520the%2520combination%2520of%2520single%2520or%2520multiple%2520weak%250Asources%2520of%2520labeled%2520data%2520with%2520a%2520small%2520but%2520reliable%2520dataset%2520to%2520generate%250Amultisource%2520labeled%2520datasets%2520and%2520a%2520novel%2520training%2520strategy%2520where%2520the%250Areliability%2520of%2520each%2520source%2520is%2520taken%2520in%2520consideration.%2520This%2520is%2520done%2520by%250Aexploiting%2520the%2520transition%2520matrices%2520describing%2520the%2520statistics%2520of%2520the%2520errors%2520of%250Aeach%2520source.%2520The%2520transition%2520matrices%2520are%2520embedded%2520into%2520the%2520labels%2520and%2520used%250Aduring%2520the%2520training%2520process%2520to%2520weigh%2520each%2520label%2520according%2520to%2520the%2520related%250Asource.%2520The%2520proposed%2520method%2520acts%2520as%2520a%2520weighting%2520scheme%2520at%2520gradient%2520level%252C%2520where%250Aeach%2520instance%2520contributes%2520with%2520different%2520weights%2520to%2520the%2520optimization%2520of%250Adifferent%2520classes.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520method%2520is%2520validated%2520by%250Aexperiments%2520on%2520different%2520datasets.%2520The%2520results%2520proved%2520the%2520robustness%2520and%250Acapability%2520of%2520leveraging%2520on%2520unreliable%2520source%2520of%2520labels%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Technique%20for%20Robust%20Training%20of%20Deep%20Networks%20With%20Multisource%0A%20%20Weak%20Labeled%20Remote%20Sensing%20Data&entry.906535625=Gianmarco%20Perantoni%20and%20Lorenzo%20Bruzzone&entry.1292438233=%20%20Deep%20learning%20has%20gained%20broad%20interest%20in%20remote%20sensing%20image%20scene%0Aclassification%20thanks%20to%20the%20effectiveness%20of%20deep%20neural%20networks%20in%0Aextracting%20the%20semantics%20from%20complex%20data.%20However%2C%20deep%20networks%20require%0Alarge%20amounts%20of%20training%20samples%20to%20obtain%20good%20generalization%20capabilities%0Aand%20are%20sensitive%20to%20errors%20in%20the%20training%20labels.%20This%20is%20a%20problem%20in%20remote%0Asensing%20since%20highly%20reliable%20labels%20can%20be%20obtained%20at%20high%20costs%20and%20in%0Alimited%20amount.%20However%2C%20many%20sources%20of%20less%20reliable%20labeled%20data%20are%0Aavailable%2C%20e.g.%2C%20obsolete%20digital%20maps.%20In%20order%20to%20train%20deep%20networks%20with%0Alarger%20datasets%2C%20we%20propose%20both%20the%20combination%20of%20single%20or%20multiple%20weak%0Asources%20of%20labeled%20data%20with%20a%20small%20but%20reliable%20dataset%20to%20generate%0Amultisource%20labeled%20datasets%20and%20a%20novel%20training%20strategy%20where%20the%0Areliability%20of%20each%20source%20is%20taken%20in%20consideration.%20This%20is%20done%20by%0Aexploiting%20the%20transition%20matrices%20describing%20the%20statistics%20of%20the%20errors%20of%0Aeach%20source.%20The%20transition%20matrices%20are%20embedded%20into%20the%20labels%20and%20used%0Aduring%20the%20training%20process%20to%20weigh%20each%20label%20according%20to%20the%20related%0Asource.%20The%20proposed%20method%20acts%20as%20a%20weighting%20scheme%20at%20gradient%20level%2C%20where%0Aeach%20instance%20contributes%20with%20different%20weights%20to%20the%20optimization%20of%0Adifferent%20classes.%20The%20effectiveness%20of%20the%20proposed%20method%20is%20validated%20by%0Aexperiments%20on%20different%20datasets.%20The%20results%20proved%20the%20robustness%20and%0Acapability%20of%20leveraging%20on%20unreliable%20source%20of%20labels%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05760v1&entry.124074799=Read"},
{"title": "AuxDet: Auxiliary Metadata Matters for Omni-Domain Infrared Small Target\n  Detection", "author": "Yangting Shi and Yinfei Zhu and Renjie He and Le Hui and Meng Cai and Ming-Ming Cheng and Yimian Dai", "abstract": "  Omni-domain infrared small target detection (Omni-IRSTD) poses formidable\nchallenges, as a single model must seamlessly adapt to diverse imaging systems,\nvarying resolutions, and multiple spectral bands simultaneously. Current\napproaches predominantly rely on visual-only modeling paradigms that not only\nstruggle with complex background interference and inherently scarce target\nfeatures, but also exhibit limited generalization capabilities across complex\nomni-scene environments where significant domain shifts and appearance\nvariations occur. In this work, we reveal a critical oversight in existing\nparadigms: the neglect of readily available auxiliary metadata describing\nimaging parameters and acquisition conditions, such as spectral bands, sensor\nplatforms, resolution, and observation perspectives. To address this\nlimitation, we propose the Auxiliary Metadata Driven Infrared Small Target\nDetector (AuxDet), a novel multimodal framework that is the first to\nincorporate metadata into the IRSTD paradigm for scene-aware optimization.\nThrough a high-dimensional fusion module based on multi-layer perceptrons\n(MLPs), AuxDet dynamically integrates metadata semantics with visual features,\nguiding adaptive representation learning for each individual sample.\nAdditionally, we design a lightweight prior-initialized enhancement module\nusing 1D convolutional blocks to further refine fused features and recover\nfine-grained target cues. Extensive experiments on the challenging\nWideIRSTD-Full benchmark demonstrate that AuxDet consistently outperforms\nstate-of-the-art methods, validating the critical role of auxiliary information\nin improving robustness and accuracy in omni-domain IRSTD tasks. Code is\navailable at https://github.com/GrokCV/AuxDet.\n", "link": "http://arxiv.org/abs/2505.15184v2", "date": "2025-10-07", "relevancy": 2.164, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuxDet%3A%20Auxiliary%20Metadata%20Matters%20for%20Omni-Domain%20Infrared%20Small%20Target%0A%20%20Detection&body=Title%3A%20AuxDet%3A%20Auxiliary%20Metadata%20Matters%20for%20Omni-Domain%20Infrared%20Small%20Target%0A%20%20Detection%0AAuthor%3A%20Yangting%20Shi%20and%20Yinfei%20Zhu%20and%20Renjie%20He%20and%20Le%20Hui%20and%20Meng%20Cai%20and%20Ming-Ming%20Cheng%20and%20Yimian%20Dai%0AAbstract%3A%20%20%20Omni-domain%20infrared%20small%20target%20detection%20%28Omni-IRSTD%29%20poses%20formidable%0Achallenges%2C%20as%20a%20single%20model%20must%20seamlessly%20adapt%20to%20diverse%20imaging%20systems%2C%0Avarying%20resolutions%2C%20and%20multiple%20spectral%20bands%20simultaneously.%20Current%0Aapproaches%20predominantly%20rely%20on%20visual-only%20modeling%20paradigms%20that%20not%20only%0Astruggle%20with%20complex%20background%20interference%20and%20inherently%20scarce%20target%0Afeatures%2C%20but%20also%20exhibit%20limited%20generalization%20capabilities%20across%20complex%0Aomni-scene%20environments%20where%20significant%20domain%20shifts%20and%20appearance%0Avariations%20occur.%20In%20this%20work%2C%20we%20reveal%20a%20critical%20oversight%20in%20existing%0Aparadigms%3A%20the%20neglect%20of%20readily%20available%20auxiliary%20metadata%20describing%0Aimaging%20parameters%20and%20acquisition%20conditions%2C%20such%20as%20spectral%20bands%2C%20sensor%0Aplatforms%2C%20resolution%2C%20and%20observation%20perspectives.%20To%20address%20this%0Alimitation%2C%20we%20propose%20the%20Auxiliary%20Metadata%20Driven%20Infrared%20Small%20Target%0ADetector%20%28AuxDet%29%2C%20a%20novel%20multimodal%20framework%20that%20is%20the%20first%20to%0Aincorporate%20metadata%20into%20the%20IRSTD%20paradigm%20for%20scene-aware%20optimization.%0AThrough%20a%20high-dimensional%20fusion%20module%20based%20on%20multi-layer%20perceptrons%0A%28MLPs%29%2C%20AuxDet%20dynamically%20integrates%20metadata%20semantics%20with%20visual%20features%2C%0Aguiding%20adaptive%20representation%20learning%20for%20each%20individual%20sample.%0AAdditionally%2C%20we%20design%20a%20lightweight%20prior-initialized%20enhancement%20module%0Ausing%201D%20convolutional%20blocks%20to%20further%20refine%20fused%20features%20and%20recover%0Afine-grained%20target%20cues.%20Extensive%20experiments%20on%20the%20challenging%0AWideIRSTD-Full%20benchmark%20demonstrate%20that%20AuxDet%20consistently%20outperforms%0Astate-of-the-art%20methods%2C%20validating%20the%20critical%20role%20of%20auxiliary%20information%0Ain%20improving%20robustness%20and%20accuracy%20in%20omni-domain%20IRSTD%20tasks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/GrokCV/AuxDet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuxDet%253A%2520Auxiliary%2520Metadata%2520Matters%2520for%2520Omni-Domain%2520Infrared%2520Small%2520Target%250A%2520%2520Detection%26entry.906535625%3DYangting%2520Shi%2520and%2520Yinfei%2520Zhu%2520and%2520Renjie%2520He%2520and%2520Le%2520Hui%2520and%2520Meng%2520Cai%2520and%2520Ming-Ming%2520Cheng%2520and%2520Yimian%2520Dai%26entry.1292438233%3D%2520%2520Omni-domain%2520infrared%2520small%2520target%2520detection%2520%2528Omni-IRSTD%2529%2520poses%2520formidable%250Achallenges%252C%2520as%2520a%2520single%2520model%2520must%2520seamlessly%2520adapt%2520to%2520diverse%2520imaging%2520systems%252C%250Avarying%2520resolutions%252C%2520and%2520multiple%2520spectral%2520bands%2520simultaneously.%2520Current%250Aapproaches%2520predominantly%2520rely%2520on%2520visual-only%2520modeling%2520paradigms%2520that%2520not%2520only%250Astruggle%2520with%2520complex%2520background%2520interference%2520and%2520inherently%2520scarce%2520target%250Afeatures%252C%2520but%2520also%2520exhibit%2520limited%2520generalization%2520capabilities%2520across%2520complex%250Aomni-scene%2520environments%2520where%2520significant%2520domain%2520shifts%2520and%2520appearance%250Avariations%2520occur.%2520In%2520this%2520work%252C%2520we%2520reveal%2520a%2520critical%2520oversight%2520in%2520existing%250Aparadigms%253A%2520the%2520neglect%2520of%2520readily%2520available%2520auxiliary%2520metadata%2520describing%250Aimaging%2520parameters%2520and%2520acquisition%2520conditions%252C%2520such%2520as%2520spectral%2520bands%252C%2520sensor%250Aplatforms%252C%2520resolution%252C%2520and%2520observation%2520perspectives.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520the%2520Auxiliary%2520Metadata%2520Driven%2520Infrared%2520Small%2520Target%250ADetector%2520%2528AuxDet%2529%252C%2520a%2520novel%2520multimodal%2520framework%2520that%2520is%2520the%2520first%2520to%250Aincorporate%2520metadata%2520into%2520the%2520IRSTD%2520paradigm%2520for%2520scene-aware%2520optimization.%250AThrough%2520a%2520high-dimensional%2520fusion%2520module%2520based%2520on%2520multi-layer%2520perceptrons%250A%2528MLPs%2529%252C%2520AuxDet%2520dynamically%2520integrates%2520metadata%2520semantics%2520with%2520visual%2520features%252C%250Aguiding%2520adaptive%2520representation%2520learning%2520for%2520each%2520individual%2520sample.%250AAdditionally%252C%2520we%2520design%2520a%2520lightweight%2520prior-initialized%2520enhancement%2520module%250Ausing%25201D%2520convolutional%2520blocks%2520to%2520further%2520refine%2520fused%2520features%2520and%2520recover%250Afine-grained%2520target%2520cues.%2520Extensive%2520experiments%2520on%2520the%2520challenging%250AWideIRSTD-Full%2520benchmark%2520demonstrate%2520that%2520AuxDet%2520consistently%2520outperforms%250Astate-of-the-art%2520methods%252C%2520validating%2520the%2520critical%2520role%2520of%2520auxiliary%2520information%250Ain%2520improving%2520robustness%2520and%2520accuracy%2520in%2520omni-domain%2520IRSTD%2520tasks.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/GrokCV/AuxDet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuxDet%3A%20Auxiliary%20Metadata%20Matters%20for%20Omni-Domain%20Infrared%20Small%20Target%0A%20%20Detection&entry.906535625=Yangting%20Shi%20and%20Yinfei%20Zhu%20and%20Renjie%20He%20and%20Le%20Hui%20and%20Meng%20Cai%20and%20Ming-Ming%20Cheng%20and%20Yimian%20Dai&entry.1292438233=%20%20Omni-domain%20infrared%20small%20target%20detection%20%28Omni-IRSTD%29%20poses%20formidable%0Achallenges%2C%20as%20a%20single%20model%20must%20seamlessly%20adapt%20to%20diverse%20imaging%20systems%2C%0Avarying%20resolutions%2C%20and%20multiple%20spectral%20bands%20simultaneously.%20Current%0Aapproaches%20predominantly%20rely%20on%20visual-only%20modeling%20paradigms%20that%20not%20only%0Astruggle%20with%20complex%20background%20interference%20and%20inherently%20scarce%20target%0Afeatures%2C%20but%20also%20exhibit%20limited%20generalization%20capabilities%20across%20complex%0Aomni-scene%20environments%20where%20significant%20domain%20shifts%20and%20appearance%0Avariations%20occur.%20In%20this%20work%2C%20we%20reveal%20a%20critical%20oversight%20in%20existing%0Aparadigms%3A%20the%20neglect%20of%20readily%20available%20auxiliary%20metadata%20describing%0Aimaging%20parameters%20and%20acquisition%20conditions%2C%20such%20as%20spectral%20bands%2C%20sensor%0Aplatforms%2C%20resolution%2C%20and%20observation%20perspectives.%20To%20address%20this%0Alimitation%2C%20we%20propose%20the%20Auxiliary%20Metadata%20Driven%20Infrared%20Small%20Target%0ADetector%20%28AuxDet%29%2C%20a%20novel%20multimodal%20framework%20that%20is%20the%20first%20to%0Aincorporate%20metadata%20into%20the%20IRSTD%20paradigm%20for%20scene-aware%20optimization.%0AThrough%20a%20high-dimensional%20fusion%20module%20based%20on%20multi-layer%20perceptrons%0A%28MLPs%29%2C%20AuxDet%20dynamically%20integrates%20metadata%20semantics%20with%20visual%20features%2C%0Aguiding%20adaptive%20representation%20learning%20for%20each%20individual%20sample.%0AAdditionally%2C%20we%20design%20a%20lightweight%20prior-initialized%20enhancement%20module%0Ausing%201D%20convolutional%20blocks%20to%20further%20refine%20fused%20features%20and%20recover%0Afine-grained%20target%20cues.%20Extensive%20experiments%20on%20the%20challenging%0AWideIRSTD-Full%20benchmark%20demonstrate%20that%20AuxDet%20consistently%20outperforms%0Astate-of-the-art%20methods%2C%20validating%20the%20critical%20role%20of%20auxiliary%20information%0Ain%20improving%20robustness%20and%20accuracy%20in%20omni-domain%20IRSTD%20tasks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/GrokCV/AuxDet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15184v2&entry.124074799=Read"},
{"title": "Spatiotemporal Graph Learning with Direct Volumetric Information Passing\n  and Feature Enhancement", "author": "Yuan Mi and Qi Wang and Xueqin Hu and Yike Guo and Ji-Rong Wen and Yang Liu and Hao Sun", "abstract": "  Data-driven learning of physical systems has kindled significant attention,\nwhere many neural models have been developed. In particular, mesh-based graph\nneural networks (GNNs) have demonstrated significant potential in modeling\nspatiotemporal dynamics across arbitrary geometric domains. However, the\nexisting node-edge message-passing and aggregation mechanism in GNNs limits the\nrepresentation learning ability. In this paper, we proposed a dual-module\nframework, Cell-embedded and Feature-enhanced Graph Neural Network (aka,\nCeFeGNN), for learning spatiotemporal dynamics. Specifically, we embed\nlearnable cell attributions to the common node-edge message passing process,\nwhich better captures the spatial dependency of regional features. Such a\nstrategy essentially upgrades the local aggregation scheme from first order\n(e.g., from edge to node) to a higher order (e.g., from volume and edge to\nnode), which takes advantage of volumetric information in message passing.\nMeanwhile, a novel feature-enhanced block is designed to further improve the\nmodel's performance and alleviate the over-smoothness problem. Extensive\nexperiments on various PDE systems and one real-world dataset demonstrate that\nCeFeGNN achieves superior performance compared with other baselines.\n", "link": "http://arxiv.org/abs/2409.18013v2", "date": "2025-10-07", "relevancy": 2.1628, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5651}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5256}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatiotemporal%20Graph%20Learning%20with%20Direct%20Volumetric%20Information%20Passing%0A%20%20and%20Feature%20Enhancement&body=Title%3A%20Spatiotemporal%20Graph%20Learning%20with%20Direct%20Volumetric%20Information%20Passing%0A%20%20and%20Feature%20Enhancement%0AAuthor%3A%20Yuan%20Mi%20and%20Qi%20Wang%20and%20Xueqin%20Hu%20and%20Yike%20Guo%20and%20Ji-Rong%20Wen%20and%20Yang%20Liu%20and%20Hao%20Sun%0AAbstract%3A%20%20%20Data-driven%20learning%20of%20physical%20systems%20has%20kindled%20significant%20attention%2C%0Awhere%20many%20neural%20models%20have%20been%20developed.%20In%20particular%2C%20mesh-based%20graph%0Aneural%20networks%20%28GNNs%29%20have%20demonstrated%20significant%20potential%20in%20modeling%0Aspatiotemporal%20dynamics%20across%20arbitrary%20geometric%20domains.%20However%2C%20the%0Aexisting%20node-edge%20message-passing%20and%20aggregation%20mechanism%20in%20GNNs%20limits%20the%0Arepresentation%20learning%20ability.%20In%20this%20paper%2C%20we%20proposed%20a%20dual-module%0Aframework%2C%20Cell-embedded%20and%20Feature-enhanced%20Graph%20Neural%20Network%20%28aka%2C%0ACeFeGNN%29%2C%20for%20learning%20spatiotemporal%20dynamics.%20Specifically%2C%20we%20embed%0Alearnable%20cell%20attributions%20to%20the%20common%20node-edge%20message%20passing%20process%2C%0Awhich%20better%20captures%20the%20spatial%20dependency%20of%20regional%20features.%20Such%20a%0Astrategy%20essentially%20upgrades%20the%20local%20aggregation%20scheme%20from%20first%20order%0A%28e.g.%2C%20from%20edge%20to%20node%29%20to%20a%20higher%20order%20%28e.g.%2C%20from%20volume%20and%20edge%20to%0Anode%29%2C%20which%20takes%20advantage%20of%20volumetric%20information%20in%20message%20passing.%0AMeanwhile%2C%20a%20novel%20feature-enhanced%20block%20is%20designed%20to%20further%20improve%20the%0Amodel%27s%20performance%20and%20alleviate%20the%20over-smoothness%20problem.%20Extensive%0Aexperiments%20on%20various%20PDE%20systems%20and%20one%20real-world%20dataset%20demonstrate%20that%0ACeFeGNN%20achieves%20superior%20performance%20compared%20with%20other%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18013v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatiotemporal%2520Graph%2520Learning%2520with%2520Direct%2520Volumetric%2520Information%2520Passing%250A%2520%2520and%2520Feature%2520Enhancement%26entry.906535625%3DYuan%2520Mi%2520and%2520Qi%2520Wang%2520and%2520Xueqin%2520Hu%2520and%2520Yike%2520Guo%2520and%2520Ji-Rong%2520Wen%2520and%2520Yang%2520Liu%2520and%2520Hao%2520Sun%26entry.1292438233%3D%2520%2520Data-driven%2520learning%2520of%2520physical%2520systems%2520has%2520kindled%2520significant%2520attention%252C%250Awhere%2520many%2520neural%2520models%2520have%2520been%2520developed.%2520In%2520particular%252C%2520mesh-based%2520graph%250Aneural%2520networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520significant%2520potential%2520in%2520modeling%250Aspatiotemporal%2520dynamics%2520across%2520arbitrary%2520geometric%2520domains.%2520However%252C%2520the%250Aexisting%2520node-edge%2520message-passing%2520and%2520aggregation%2520mechanism%2520in%2520GNNs%2520limits%2520the%250Arepresentation%2520learning%2520ability.%2520In%2520this%2520paper%252C%2520we%2520proposed%2520a%2520dual-module%250Aframework%252C%2520Cell-embedded%2520and%2520Feature-enhanced%2520Graph%2520Neural%2520Network%2520%2528aka%252C%250ACeFeGNN%2529%252C%2520for%2520learning%2520spatiotemporal%2520dynamics.%2520Specifically%252C%2520we%2520embed%250Alearnable%2520cell%2520attributions%2520to%2520the%2520common%2520node-edge%2520message%2520passing%2520process%252C%250Awhich%2520better%2520captures%2520the%2520spatial%2520dependency%2520of%2520regional%2520features.%2520Such%2520a%250Astrategy%2520essentially%2520upgrades%2520the%2520local%2520aggregation%2520scheme%2520from%2520first%2520order%250A%2528e.g.%252C%2520from%2520edge%2520to%2520node%2529%2520to%2520a%2520higher%2520order%2520%2528e.g.%252C%2520from%2520volume%2520and%2520edge%2520to%250Anode%2529%252C%2520which%2520takes%2520advantage%2520of%2520volumetric%2520information%2520in%2520message%2520passing.%250AMeanwhile%252C%2520a%2520novel%2520feature-enhanced%2520block%2520is%2520designed%2520to%2520further%2520improve%2520the%250Amodel%2527s%2520performance%2520and%2520alleviate%2520the%2520over-smoothness%2520problem.%2520Extensive%250Aexperiments%2520on%2520various%2520PDE%2520systems%2520and%2520one%2520real-world%2520dataset%2520demonstrate%2520that%250ACeFeGNN%2520achieves%2520superior%2520performance%2520compared%2520with%2520other%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18013v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatiotemporal%20Graph%20Learning%20with%20Direct%20Volumetric%20Information%20Passing%0A%20%20and%20Feature%20Enhancement&entry.906535625=Yuan%20Mi%20and%20Qi%20Wang%20and%20Xueqin%20Hu%20and%20Yike%20Guo%20and%20Ji-Rong%20Wen%20and%20Yang%20Liu%20and%20Hao%20Sun&entry.1292438233=%20%20Data-driven%20learning%20of%20physical%20systems%20has%20kindled%20significant%20attention%2C%0Awhere%20many%20neural%20models%20have%20been%20developed.%20In%20particular%2C%20mesh-based%20graph%0Aneural%20networks%20%28GNNs%29%20have%20demonstrated%20significant%20potential%20in%20modeling%0Aspatiotemporal%20dynamics%20across%20arbitrary%20geometric%20domains.%20However%2C%20the%0Aexisting%20node-edge%20message-passing%20and%20aggregation%20mechanism%20in%20GNNs%20limits%20the%0Arepresentation%20learning%20ability.%20In%20this%20paper%2C%20we%20proposed%20a%20dual-module%0Aframework%2C%20Cell-embedded%20and%20Feature-enhanced%20Graph%20Neural%20Network%20%28aka%2C%0ACeFeGNN%29%2C%20for%20learning%20spatiotemporal%20dynamics.%20Specifically%2C%20we%20embed%0Alearnable%20cell%20attributions%20to%20the%20common%20node-edge%20message%20passing%20process%2C%0Awhich%20better%20captures%20the%20spatial%20dependency%20of%20regional%20features.%20Such%20a%0Astrategy%20essentially%20upgrades%20the%20local%20aggregation%20scheme%20from%20first%20order%0A%28e.g.%2C%20from%20edge%20to%20node%29%20to%20a%20higher%20order%20%28e.g.%2C%20from%20volume%20and%20edge%20to%0Anode%29%2C%20which%20takes%20advantage%20of%20volumetric%20information%20in%20message%20passing.%0AMeanwhile%2C%20a%20novel%20feature-enhanced%20block%20is%20designed%20to%20further%20improve%20the%0Amodel%27s%20performance%20and%20alleviate%20the%20over-smoothness%20problem.%20Extensive%0Aexperiments%20on%20various%20PDE%20systems%20and%20one%20real-world%20dataset%20demonstrate%20that%0ACeFeGNN%20achieves%20superior%20performance%20compared%20with%20other%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18013v2&entry.124074799=Read"},
{"title": "Emergent AI Surveillance: Overlearned Person Re-Identification and Its\n  Mitigation in Law Enforcement Context", "author": "An Thi Nguyen and Radina Stoykova and Eric Arazo", "abstract": "  Generic instance search models can dramatically reduce the manual effort\nrequired to analyze vast surveillance footage during criminal investigations by\nretrieving specific objects of interest to law enforcement. However, our\nresearch reveals an unintended emergent capability: through overlearning, these\nmodels can single out specific individuals even when trained on datasets\nwithout human subjects. This capability raises concerns regarding\nidentification and profiling of individuals based on their personal data, while\nthere is currently no clear standard on how de-identification can be achieved.\nWe evaluate two technical safeguards to curtail a model's person\nre-identification capacity: index exclusion and confusion loss. Our experiments\ndemonstrate that combining these approaches can reduce person re-identification\naccuracy to below 2% while maintaining 82% of retrieval performance for\nnon-person objects. However, we identify critical vulnerabilities in these\nmitigations, including potential circumvention using partial person images.\nThese findings highlight urgent regulatory questions at the intersection of AI\ngovernance and data protection: How should we classify and regulate systems\nwith emergent identification capabilities? And what technical standards should\nbe required to prevent identification capabilities from developing in seemingly\nbenign applications?\n", "link": "http://arxiv.org/abs/2510.06026v1", "date": "2025-10-07", "relevancy": 2.1565, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5521}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5418}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20AI%20Surveillance%3A%20Overlearned%20Person%20Re-Identification%20and%20Its%0A%20%20Mitigation%20in%20Law%20Enforcement%20Context&body=Title%3A%20Emergent%20AI%20Surveillance%3A%20Overlearned%20Person%20Re-Identification%20and%20Its%0A%20%20Mitigation%20in%20Law%20Enforcement%20Context%0AAuthor%3A%20An%20Thi%20Nguyen%20and%20Radina%20Stoykova%20and%20Eric%20Arazo%0AAbstract%3A%20%20%20Generic%20instance%20search%20models%20can%20dramatically%20reduce%20the%20manual%20effort%0Arequired%20to%20analyze%20vast%20surveillance%20footage%20during%20criminal%20investigations%20by%0Aretrieving%20specific%20objects%20of%20interest%20to%20law%20enforcement.%20However%2C%20our%0Aresearch%20reveals%20an%20unintended%20emergent%20capability%3A%20through%20overlearning%2C%20these%0Amodels%20can%20single%20out%20specific%20individuals%20even%20when%20trained%20on%20datasets%0Awithout%20human%20subjects.%20This%20capability%20raises%20concerns%20regarding%0Aidentification%20and%20profiling%20of%20individuals%20based%20on%20their%20personal%20data%2C%20while%0Athere%20is%20currently%20no%20clear%20standard%20on%20how%20de-identification%20can%20be%20achieved.%0AWe%20evaluate%20two%20technical%20safeguards%20to%20curtail%20a%20model%27s%20person%0Are-identification%20capacity%3A%20index%20exclusion%20and%20confusion%20loss.%20Our%20experiments%0Ademonstrate%20that%20combining%20these%20approaches%20can%20reduce%20person%20re-identification%0Aaccuracy%20to%20below%202%25%20while%20maintaining%2082%25%20of%20retrieval%20performance%20for%0Anon-person%20objects.%20However%2C%20we%20identify%20critical%20vulnerabilities%20in%20these%0Amitigations%2C%20including%20potential%20circumvention%20using%20partial%20person%20images.%0AThese%20findings%20highlight%20urgent%20regulatory%20questions%20at%20the%20intersection%20of%20AI%0Agovernance%20and%20data%20protection%3A%20How%20should%20we%20classify%20and%20regulate%20systems%0Awith%20emergent%20identification%20capabilities%3F%20And%20what%20technical%20standards%20should%0Abe%20required%20to%20prevent%20identification%20capabilities%20from%20developing%20in%20seemingly%0Abenign%20applications%3F%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520AI%2520Surveillance%253A%2520Overlearned%2520Person%2520Re-Identification%2520and%2520Its%250A%2520%2520Mitigation%2520in%2520Law%2520Enforcement%2520Context%26entry.906535625%3DAn%2520Thi%2520Nguyen%2520and%2520Radina%2520Stoykova%2520and%2520Eric%2520Arazo%26entry.1292438233%3D%2520%2520Generic%2520instance%2520search%2520models%2520can%2520dramatically%2520reduce%2520the%2520manual%2520effort%250Arequired%2520to%2520analyze%2520vast%2520surveillance%2520footage%2520during%2520criminal%2520investigations%2520by%250Aretrieving%2520specific%2520objects%2520of%2520interest%2520to%2520law%2520enforcement.%2520However%252C%2520our%250Aresearch%2520reveals%2520an%2520unintended%2520emergent%2520capability%253A%2520through%2520overlearning%252C%2520these%250Amodels%2520can%2520single%2520out%2520specific%2520individuals%2520even%2520when%2520trained%2520on%2520datasets%250Awithout%2520human%2520subjects.%2520This%2520capability%2520raises%2520concerns%2520regarding%250Aidentification%2520and%2520profiling%2520of%2520individuals%2520based%2520on%2520their%2520personal%2520data%252C%2520while%250Athere%2520is%2520currently%2520no%2520clear%2520standard%2520on%2520how%2520de-identification%2520can%2520be%2520achieved.%250AWe%2520evaluate%2520two%2520technical%2520safeguards%2520to%2520curtail%2520a%2520model%2527s%2520person%250Are-identification%2520capacity%253A%2520index%2520exclusion%2520and%2520confusion%2520loss.%2520Our%2520experiments%250Ademonstrate%2520that%2520combining%2520these%2520approaches%2520can%2520reduce%2520person%2520re-identification%250Aaccuracy%2520to%2520below%25202%2525%2520while%2520maintaining%252082%2525%2520of%2520retrieval%2520performance%2520for%250Anon-person%2520objects.%2520However%252C%2520we%2520identify%2520critical%2520vulnerabilities%2520in%2520these%250Amitigations%252C%2520including%2520potential%2520circumvention%2520using%2520partial%2520person%2520images.%250AThese%2520findings%2520highlight%2520urgent%2520regulatory%2520questions%2520at%2520the%2520intersection%2520of%2520AI%250Agovernance%2520and%2520data%2520protection%253A%2520How%2520should%2520we%2520classify%2520and%2520regulate%2520systems%250Awith%2520emergent%2520identification%2520capabilities%253F%2520And%2520what%2520technical%2520standards%2520should%250Abe%2520required%2520to%2520prevent%2520identification%2520capabilities%2520from%2520developing%2520in%2520seemingly%250Abenign%2520applications%253F%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20AI%20Surveillance%3A%20Overlearned%20Person%20Re-Identification%20and%20Its%0A%20%20Mitigation%20in%20Law%20Enforcement%20Context&entry.906535625=An%20Thi%20Nguyen%20and%20Radina%20Stoykova%20and%20Eric%20Arazo&entry.1292438233=%20%20Generic%20instance%20search%20models%20can%20dramatically%20reduce%20the%20manual%20effort%0Arequired%20to%20analyze%20vast%20surveillance%20footage%20during%20criminal%20investigations%20by%0Aretrieving%20specific%20objects%20of%20interest%20to%20law%20enforcement.%20However%2C%20our%0Aresearch%20reveals%20an%20unintended%20emergent%20capability%3A%20through%20overlearning%2C%20these%0Amodels%20can%20single%20out%20specific%20individuals%20even%20when%20trained%20on%20datasets%0Awithout%20human%20subjects.%20This%20capability%20raises%20concerns%20regarding%0Aidentification%20and%20profiling%20of%20individuals%20based%20on%20their%20personal%20data%2C%20while%0Athere%20is%20currently%20no%20clear%20standard%20on%20how%20de-identification%20can%20be%20achieved.%0AWe%20evaluate%20two%20technical%20safeguards%20to%20curtail%20a%20model%27s%20person%0Are-identification%20capacity%3A%20index%20exclusion%20and%20confusion%20loss.%20Our%20experiments%0Ademonstrate%20that%20combining%20these%20approaches%20can%20reduce%20person%20re-identification%0Aaccuracy%20to%20below%202%25%20while%20maintaining%2082%25%20of%20retrieval%20performance%20for%0Anon-person%20objects.%20However%2C%20we%20identify%20critical%20vulnerabilities%20in%20these%0Amitigations%2C%20including%20potential%20circumvention%20using%20partial%20person%20images.%0AThese%20findings%20highlight%20urgent%20regulatory%20questions%20at%20the%20intersection%20of%20AI%0Agovernance%20and%20data%20protection%3A%20How%20should%20we%20classify%20and%20regulate%20systems%0Awith%20emergent%20identification%20capabilities%3F%20And%20what%20technical%20standards%20should%0Abe%20required%20to%20prevent%20identification%20capabilities%20from%20developing%20in%20seemingly%0Abenign%20applications%3F%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06026v1&entry.124074799=Read"},
{"title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis", "author": "Yuzhuang Xu and Xu Han and Yuanchi Zhang and Yixuan Wang and Yijun Liu and Shiyu Ji and Qingfu Zhu and Wanxiang Che", "abstract": "  Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.\n", "link": "http://arxiv.org/abs/2508.02322v2", "date": "2025-10-07", "relevancy": 2.1519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMERA%3A%20Multi-Matrix%20Joint%20Compression%20for%20MoE%20Models%20via%20Micro-Expert%0A%20%20Redundancy%20Analysis&body=Title%3A%20CAMERA%3A%20Multi-Matrix%20Joint%20Compression%20for%20MoE%20Models%20via%20Micro-Expert%0A%20%20Redundancy%20Analysis%0AAuthor%3A%20Yuzhuang%20Xu%20and%20Xu%20Han%20and%20Yuanchi%20Zhang%20and%20Yixuan%20Wang%20and%20Yijun%20Liu%20and%20Shiyu%20Ji%20and%20Qingfu%20Zhu%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20with%20Mixture-of-Experts%20%28MoE%29%20architectures%20are%0Adistinguished%20by%20their%20strong%20performance%20scaling%20with%20increasing%20parameters%0Aacross%20a%20wide%20range%20of%20tasks%2C%20yet%20they%20also%20suffer%20from%20substantial%0Acomputational%20and%20storage%20overheads.%20Notably%2C%20the%20performance%20gains%20of%20MoE%0Amodels%20do%20not%20scale%20proportionally%20with%20the%20growth%20in%20expert%20parameters.%20While%0Aprior%20works%20attempt%20to%20reduce%20parameters%20via%20expert-level%20pruning%2C%20merging%2C%20or%0Adecomposition%2C%20they%20still%20suffer%20from%20challenges%20in%20both%20performance%20and%0Acomputational%20efficiency.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%0Aintroducing%20micro-expert%20as%20a%20finer-grained%20compression%20unit%20that%20spans%20across%0Amatrices.%20We%20first%20establish%20a%20more%20fundamental%20perspective%2C%20viewing%20MoE%20layers%0Aas%20mixtures%20of%20micro-experts%2C%20and%20present%20CAMERA%2C%20a%20lightweight%20and%0Atraining-free%20framework%20for%20identifying%20micro-expert%20redundancy.%20Our%20analysis%0Auncovers%20significant%20variance%20in%20micro-expert%20contributions%20during%20decoding.%0ABased%20on%20this%20insight%2C%20we%20further%20propose%20CAMERA-P%2C%20a%20structured%20micro-expert%0Apruning%20framework%2C%20and%20CAMERA-Q%2C%20a%20mixed-precision%20quantization%20idea%20designed%0Afor%20micro-experts.%20Extensive%20experiments%20on%20nine%20downstream%20tasks%20show%20that%0ACAMERA-P%20consistently%20outperforms%20strong%20baselines%20under%20pruning%20ratios%20ranging%0Afrom%2020%25%20to%2060%25.%20Furthermore%2C%20CAMERA-Q%20achieves%20superior%20results%20under%0Aaggressive%202-bit%20quantization%2C%20surpassing%20existing%20matrix-%20and%20channel-level%0Aideas.%20Notably%2C%20our%20method%20enables%20complete%20micro-expert%20analysis%20of%0AQwen2-57B-A14B%20in%20less%20than%205%20minutes%20on%20a%20single%20NVIDIA%20A100-40GB%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMERA%253A%2520Multi-Matrix%2520Joint%2520Compression%2520for%2520MoE%2520Models%2520via%2520Micro-Expert%250A%2520%2520Redundancy%2520Analysis%26entry.906535625%3DYuzhuang%2520Xu%2520and%2520Xu%2520Han%2520and%2520Yuanchi%2520Zhang%2520and%2520Yixuan%2520Wang%2520and%2520Yijun%2520Liu%2520and%2520Shiyu%2520Ji%2520and%2520Qingfu%2520Zhu%2520and%2520Wanxiang%2520Che%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures%2520are%250Adistinguished%2520by%2520their%2520strong%2520performance%2520scaling%2520with%2520increasing%2520parameters%250Aacross%2520a%2520wide%2520range%2520of%2520tasks%252C%2520yet%2520they%2520also%2520suffer%2520from%2520substantial%250Acomputational%2520and%2520storage%2520overheads.%2520Notably%252C%2520the%2520performance%2520gains%2520of%2520MoE%250Amodels%2520do%2520not%2520scale%2520proportionally%2520with%2520the%2520growth%2520in%2520expert%2520parameters.%2520While%250Aprior%2520works%2520attempt%2520to%2520reduce%2520parameters%2520via%2520expert-level%2520pruning%252C%2520merging%252C%2520or%250Adecomposition%252C%2520they%2520still%2520suffer%2520from%2520challenges%2520in%2520both%2520performance%2520and%250Acomputational%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520challenges%2520by%250Aintroducing%2520micro-expert%2520as%2520a%2520finer-grained%2520compression%2520unit%2520that%2520spans%2520across%250Amatrices.%2520We%2520first%2520establish%2520a%2520more%2520fundamental%2520perspective%252C%2520viewing%2520MoE%2520layers%250Aas%2520mixtures%2520of%2520micro-experts%252C%2520and%2520present%2520CAMERA%252C%2520a%2520lightweight%2520and%250Atraining-free%2520framework%2520for%2520identifying%2520micro-expert%2520redundancy.%2520Our%2520analysis%250Auncovers%2520significant%2520variance%2520in%2520micro-expert%2520contributions%2520during%2520decoding.%250ABased%2520on%2520this%2520insight%252C%2520we%2520further%2520propose%2520CAMERA-P%252C%2520a%2520structured%2520micro-expert%250Apruning%2520framework%252C%2520and%2520CAMERA-Q%252C%2520a%2520mixed-precision%2520quantization%2520idea%2520designed%250Afor%2520micro-experts.%2520Extensive%2520experiments%2520on%2520nine%2520downstream%2520tasks%2520show%2520that%250ACAMERA-P%2520consistently%2520outperforms%2520strong%2520baselines%2520under%2520pruning%2520ratios%2520ranging%250Afrom%252020%2525%2520to%252060%2525.%2520Furthermore%252C%2520CAMERA-Q%2520achieves%2520superior%2520results%2520under%250Aaggressive%25202-bit%2520quantization%252C%2520surpassing%2520existing%2520matrix-%2520and%2520channel-level%250Aideas.%2520Notably%252C%2520our%2520method%2520enables%2520complete%2520micro-expert%2520analysis%2520of%250AQwen2-57B-A14B%2520in%2520less%2520than%25205%2520minutes%2520on%2520a%2520single%2520NVIDIA%2520A100-40GB%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMERA%3A%20Multi-Matrix%20Joint%20Compression%20for%20MoE%20Models%20via%20Micro-Expert%0A%20%20Redundancy%20Analysis&entry.906535625=Yuzhuang%20Xu%20and%20Xu%20Han%20and%20Yuanchi%20Zhang%20and%20Yixuan%20Wang%20and%20Yijun%20Liu%20and%20Shiyu%20Ji%20and%20Qingfu%20Zhu%20and%20Wanxiang%20Che&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20with%20Mixture-of-Experts%20%28MoE%29%20architectures%20are%0Adistinguished%20by%20their%20strong%20performance%20scaling%20with%20increasing%20parameters%0Aacross%20a%20wide%20range%20of%20tasks%2C%20yet%20they%20also%20suffer%20from%20substantial%0Acomputational%20and%20storage%20overheads.%20Notably%2C%20the%20performance%20gains%20of%20MoE%0Amodels%20do%20not%20scale%20proportionally%20with%20the%20growth%20in%20expert%20parameters.%20While%0Aprior%20works%20attempt%20to%20reduce%20parameters%20via%20expert-level%20pruning%2C%20merging%2C%20or%0Adecomposition%2C%20they%20still%20suffer%20from%20challenges%20in%20both%20performance%20and%0Acomputational%20efficiency.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%0Aintroducing%20micro-expert%20as%20a%20finer-grained%20compression%20unit%20that%20spans%20across%0Amatrices.%20We%20first%20establish%20a%20more%20fundamental%20perspective%2C%20viewing%20MoE%20layers%0Aas%20mixtures%20of%20micro-experts%2C%20and%20present%20CAMERA%2C%20a%20lightweight%20and%0Atraining-free%20framework%20for%20identifying%20micro-expert%20redundancy.%20Our%20analysis%0Auncovers%20significant%20variance%20in%20micro-expert%20contributions%20during%20decoding.%0ABased%20on%20this%20insight%2C%20we%20further%20propose%20CAMERA-P%2C%20a%20structured%20micro-expert%0Apruning%20framework%2C%20and%20CAMERA-Q%2C%20a%20mixed-precision%20quantization%20idea%20designed%0Afor%20micro-experts.%20Extensive%20experiments%20on%20nine%20downstream%20tasks%20show%20that%0ACAMERA-P%20consistently%20outperforms%20strong%20baselines%20under%20pruning%20ratios%20ranging%0Afrom%2020%25%20to%2060%25.%20Furthermore%2C%20CAMERA-Q%20achieves%20superior%20results%20under%0Aaggressive%202-bit%20quantization%2C%20surpassing%20existing%20matrix-%20and%20channel-level%0Aideas.%20Notably%2C%20our%20method%20enables%20complete%20micro-expert%20analysis%20of%0AQwen2-57B-A14B%20in%20less%20than%205%20minutes%20on%20a%20single%20NVIDIA%20A100-40GB%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02322v2&entry.124074799=Read"},
{"title": "Paying Attention to Hybrid Attention: Untangling the Issues with\n  Conversion Methods", "author": "Martin Benfeghoul and Teresa Delgado and Adnan Oomerjee and Haitham Bou Ammar and Jun Wang and Zafeirios Fountas", "abstract": "  Transformers' quadratic computational complexity limits their scalability\ndespite remarkable performance. While linear attention reduces this to linear\ncomplexity, pre-training such models from scratch remains, in most cases,\nprohibitively expensive. Recent post-training linearisation methods convert\npre-trained Transformers to linear models efficiently, often using hybrid\napproaches that combine linear attention with sliding-window softmax. We\nidentify a critical flaw: existing hybrid methods inadvertently bypass the\nlinear component, relying almost entirely on SWA. Component-level diagnostics\nreveal this previously undetected behaviour stems from overlooked evaluation\npractices on common-sense benchmarks. We propose three solutions to ensure\nbalanced component usage: (i) inference-time hybridisation of linear-only\nconversions with sliding-window softmax; (ii) HedgeCATs, combining\nattention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled\nSliding-window Dropout (SSD), which stochastically suppresses the softmax\nbranch during training to prevent component collapse. Our methods maintain\ncomputational efficiency while recovering most base model performance and\nensuring genuine linear attention adoption, restoring the validity of\nperformance attributions in hybrid conversions.\n", "link": "http://arxiv.org/abs/2510.05901v1", "date": "2025-10-07", "relevancy": 2.147, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5667}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5419}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paying%20Attention%20to%20Hybrid%20Attention%3A%20Untangling%20the%20Issues%20with%0A%20%20Conversion%20Methods&body=Title%3A%20Paying%20Attention%20to%20Hybrid%20Attention%3A%20Untangling%20the%20Issues%20with%0A%20%20Conversion%20Methods%0AAuthor%3A%20Martin%20Benfeghoul%20and%20Teresa%20Delgado%20and%20Adnan%20Oomerjee%20and%20Haitham%20Bou%20Ammar%20and%20Jun%20Wang%20and%20Zafeirios%20Fountas%0AAbstract%3A%20%20%20Transformers%27%20quadratic%20computational%20complexity%20limits%20their%20scalability%0Adespite%20remarkable%20performance.%20While%20linear%20attention%20reduces%20this%20to%20linear%0Acomplexity%2C%20pre-training%20such%20models%20from%20scratch%20remains%2C%20in%20most%20cases%2C%0Aprohibitively%20expensive.%20Recent%20post-training%20linearisation%20methods%20convert%0Apre-trained%20Transformers%20to%20linear%20models%20efficiently%2C%20often%20using%20hybrid%0Aapproaches%20that%20combine%20linear%20attention%20with%20sliding-window%20softmax.%20We%0Aidentify%20a%20critical%20flaw%3A%20existing%20hybrid%20methods%20inadvertently%20bypass%20the%0Alinear%20component%2C%20relying%20almost%20entirely%20on%20SWA.%20Component-level%20diagnostics%0Areveal%20this%20previously%20undetected%20behaviour%20stems%20from%20overlooked%20evaluation%0Apractices%20on%20common-sense%20benchmarks.%20We%20propose%20three%20solutions%20to%20ensure%0Abalanced%20component%20usage%3A%20%28i%29%20inference-time%20hybridisation%20of%20linear-only%0Aconversions%20with%20sliding-window%20softmax%3B%20%28ii%29%20HedgeCATs%2C%20combining%0Aattention-weight%20transfer%20with%20targeted%20LoRA%20fine-tuning%3B%20and%20%28iii%29%20Scheduled%0ASliding-window%20Dropout%20%28SSD%29%2C%20which%20stochastically%20suppresses%20the%20softmax%0Abranch%20during%20training%20to%20prevent%20component%20collapse.%20Our%20methods%20maintain%0Acomputational%20efficiency%20while%20recovering%20most%20base%20model%20performance%20and%0Aensuring%20genuine%20linear%20attention%20adoption%2C%20restoring%20the%20validity%20of%0Aperformance%20attributions%20in%20hybrid%20conversions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaying%2520Attention%2520to%2520Hybrid%2520Attention%253A%2520Untangling%2520the%2520Issues%2520with%250A%2520%2520Conversion%2520Methods%26entry.906535625%3DMartin%2520Benfeghoul%2520and%2520Teresa%2520Delgado%2520and%2520Adnan%2520Oomerjee%2520and%2520Haitham%2520Bou%2520Ammar%2520and%2520Jun%2520Wang%2520and%2520Zafeirios%2520Fountas%26entry.1292438233%3D%2520%2520Transformers%2527%2520quadratic%2520computational%2520complexity%2520limits%2520their%2520scalability%250Adespite%2520remarkable%2520performance.%2520While%2520linear%2520attention%2520reduces%2520this%2520to%2520linear%250Acomplexity%252C%2520pre-training%2520such%2520models%2520from%2520scratch%2520remains%252C%2520in%2520most%2520cases%252C%250Aprohibitively%2520expensive.%2520Recent%2520post-training%2520linearisation%2520methods%2520convert%250Apre-trained%2520Transformers%2520to%2520linear%2520models%2520efficiently%252C%2520often%2520using%2520hybrid%250Aapproaches%2520that%2520combine%2520linear%2520attention%2520with%2520sliding-window%2520softmax.%2520We%250Aidentify%2520a%2520critical%2520flaw%253A%2520existing%2520hybrid%2520methods%2520inadvertently%2520bypass%2520the%250Alinear%2520component%252C%2520relying%2520almost%2520entirely%2520on%2520SWA.%2520Component-level%2520diagnostics%250Areveal%2520this%2520previously%2520undetected%2520behaviour%2520stems%2520from%2520overlooked%2520evaluation%250Apractices%2520on%2520common-sense%2520benchmarks.%2520We%2520propose%2520three%2520solutions%2520to%2520ensure%250Abalanced%2520component%2520usage%253A%2520%2528i%2529%2520inference-time%2520hybridisation%2520of%2520linear-only%250Aconversions%2520with%2520sliding-window%2520softmax%253B%2520%2528ii%2529%2520HedgeCATs%252C%2520combining%250Aattention-weight%2520transfer%2520with%2520targeted%2520LoRA%2520fine-tuning%253B%2520and%2520%2528iii%2529%2520Scheduled%250ASliding-window%2520Dropout%2520%2528SSD%2529%252C%2520which%2520stochastically%2520suppresses%2520the%2520softmax%250Abranch%2520during%2520training%2520to%2520prevent%2520component%2520collapse.%2520Our%2520methods%2520maintain%250Acomputational%2520efficiency%2520while%2520recovering%2520most%2520base%2520model%2520performance%2520and%250Aensuring%2520genuine%2520linear%2520attention%2520adoption%252C%2520restoring%2520the%2520validity%2520of%250Aperformance%2520attributions%2520in%2520hybrid%2520conversions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paying%20Attention%20to%20Hybrid%20Attention%3A%20Untangling%20the%20Issues%20with%0A%20%20Conversion%20Methods&entry.906535625=Martin%20Benfeghoul%20and%20Teresa%20Delgado%20and%20Adnan%20Oomerjee%20and%20Haitham%20Bou%20Ammar%20and%20Jun%20Wang%20and%20Zafeirios%20Fountas&entry.1292438233=%20%20Transformers%27%20quadratic%20computational%20complexity%20limits%20their%20scalability%0Adespite%20remarkable%20performance.%20While%20linear%20attention%20reduces%20this%20to%20linear%0Acomplexity%2C%20pre-training%20such%20models%20from%20scratch%20remains%2C%20in%20most%20cases%2C%0Aprohibitively%20expensive.%20Recent%20post-training%20linearisation%20methods%20convert%0Apre-trained%20Transformers%20to%20linear%20models%20efficiently%2C%20often%20using%20hybrid%0Aapproaches%20that%20combine%20linear%20attention%20with%20sliding-window%20softmax.%20We%0Aidentify%20a%20critical%20flaw%3A%20existing%20hybrid%20methods%20inadvertently%20bypass%20the%0Alinear%20component%2C%20relying%20almost%20entirely%20on%20SWA.%20Component-level%20diagnostics%0Areveal%20this%20previously%20undetected%20behaviour%20stems%20from%20overlooked%20evaluation%0Apractices%20on%20common-sense%20benchmarks.%20We%20propose%20three%20solutions%20to%20ensure%0Abalanced%20component%20usage%3A%20%28i%29%20inference-time%20hybridisation%20of%20linear-only%0Aconversions%20with%20sliding-window%20softmax%3B%20%28ii%29%20HedgeCATs%2C%20combining%0Aattention-weight%20transfer%20with%20targeted%20LoRA%20fine-tuning%3B%20and%20%28iii%29%20Scheduled%0ASliding-window%20Dropout%20%28SSD%29%2C%20which%20stochastically%20suppresses%20the%20softmax%0Abranch%20during%20training%20to%20prevent%20component%20collapse.%20Our%20methods%20maintain%0Acomputational%20efficiency%20while%20recovering%20most%20base%20model%20performance%20and%0Aensuring%20genuine%20linear%20attention%20adoption%2C%20restoring%20the%20validity%20of%0Aperformance%20attributions%20in%20hybrid%20conversions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05901v1&entry.124074799=Read"},
{"title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural\n  Language", "author": "Periklis Mantenoglou and Rishi Hazra and Pedro Zuidberg Dos Martires and Luc De Raedt", "abstract": "  Owing to their reasoning capabilities, large language models (LLMs) have been\nevaluated on planning tasks described in natural language. However, LLMs have\nlargely been tested on planning domains without constraints. In order to deploy\nthem in real-world settings where adherence to constraints, in particular\nsafety constraints, is critical, we need to evaluate their performance on\nconstrained planning tasks. We introduce LexiCon -- a natural language-based\n(Lexi) constrained (Con) planning benchmark, consisting of a suite of\nenvironments, that can be used to evaluate the planning capabilities of LLMs in\na principled fashion. The core idea behind LexiCon is to take existing planning\nenvironments and impose temporal constraints on the states. These constrained\nproblems are then translated into natural language and given to an LLM to\nsolve. A key feature of LexiCon is its extensibility. That is, the set of\nsupported environments can be extended with new (unconstrained) environment\ngenerators, for which temporal constraints are constructed automatically. This\nrenders LexiCon future-proof: the hardness of the generated planning problems\ncan be increased as the planning capabilities of LLMs improve. Our experiments\nreveal that the performance of state-of-the-art LLMs, including reasoning\nmodels like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of\nthe planning tasks increases.\n", "link": "http://arxiv.org/abs/2510.05972v1", "date": "2025-10-07", "relevancy": 2.1428, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LexiCon%3A%20a%20Benchmark%20for%20Planning%20under%20Temporal%20Constraints%20in%20Natural%0A%20%20Language&body=Title%3A%20LexiCon%3A%20a%20Benchmark%20for%20Planning%20under%20Temporal%20Constraints%20in%20Natural%0A%20%20Language%0AAuthor%3A%20Periklis%20Mantenoglou%20and%20Rishi%20Hazra%20and%20Pedro%20Zuidberg%20Dos%20Martires%20and%20Luc%20De%20Raedt%0AAbstract%3A%20%20%20Owing%20to%20their%20reasoning%20capabilities%2C%20large%20language%20models%20%28LLMs%29%20have%20been%0Aevaluated%20on%20planning%20tasks%20described%20in%20natural%20language.%20However%2C%20LLMs%20have%0Alargely%20been%20tested%20on%20planning%20domains%20without%20constraints.%20In%20order%20to%20deploy%0Athem%20in%20real-world%20settings%20where%20adherence%20to%20constraints%2C%20in%20particular%0Asafety%20constraints%2C%20is%20critical%2C%20we%20need%20to%20evaluate%20their%20performance%20on%0Aconstrained%20planning%20tasks.%20We%20introduce%20LexiCon%20--%20a%20natural%20language-based%0A%28Lexi%29%20constrained%20%28Con%29%20planning%20benchmark%2C%20consisting%20of%20a%20suite%20of%0Aenvironments%2C%20that%20can%20be%20used%20to%20evaluate%20the%20planning%20capabilities%20of%20LLMs%20in%0Aa%20principled%20fashion.%20The%20core%20idea%20behind%20LexiCon%20is%20to%20take%20existing%20planning%0Aenvironments%20and%20impose%20temporal%20constraints%20on%20the%20states.%20These%20constrained%0Aproblems%20are%20then%20translated%20into%20natural%20language%20and%20given%20to%20an%20LLM%20to%0Asolve.%20A%20key%20feature%20of%20LexiCon%20is%20its%20extensibility.%20That%20is%2C%20the%20set%20of%0Asupported%20environments%20can%20be%20extended%20with%20new%20%28unconstrained%29%20environment%0Agenerators%2C%20for%20which%20temporal%20constraints%20are%20constructed%20automatically.%20This%0Arenders%20LexiCon%20future-proof%3A%20the%20hardness%20of%20the%20generated%20planning%20problems%0Acan%20be%20increased%20as%20the%20planning%20capabilities%20of%20LLMs%20improve.%20Our%20experiments%0Areveal%20that%20the%20performance%20of%20state-of-the-art%20LLMs%2C%20including%20reasoning%0Amodels%20like%20GPT-5%2C%20o3%2C%20and%20R1%2C%20deteriorates%20as%20the%20degree%20of%20constrainedness%20of%0Athe%20planning%20tasks%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLexiCon%253A%2520a%2520Benchmark%2520for%2520Planning%2520under%2520Temporal%2520Constraints%2520in%2520Natural%250A%2520%2520Language%26entry.906535625%3DPeriklis%2520Mantenoglou%2520and%2520Rishi%2520Hazra%2520and%2520Pedro%2520Zuidberg%2520Dos%2520Martires%2520and%2520Luc%2520De%2520Raedt%26entry.1292438233%3D%2520%2520Owing%2520to%2520their%2520reasoning%2520capabilities%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%250Aevaluated%2520on%2520planning%2520tasks%2520described%2520in%2520natural%2520language.%2520However%252C%2520LLMs%2520have%250Alargely%2520been%2520tested%2520on%2520planning%2520domains%2520without%2520constraints.%2520In%2520order%2520to%2520deploy%250Athem%2520in%2520real-world%2520settings%2520where%2520adherence%2520to%2520constraints%252C%2520in%2520particular%250Asafety%2520constraints%252C%2520is%2520critical%252C%2520we%2520need%2520to%2520evaluate%2520their%2520performance%2520on%250Aconstrained%2520planning%2520tasks.%2520We%2520introduce%2520LexiCon%2520--%2520a%2520natural%2520language-based%250A%2528Lexi%2529%2520constrained%2520%2528Con%2529%2520planning%2520benchmark%252C%2520consisting%2520of%2520a%2520suite%2520of%250Aenvironments%252C%2520that%2520can%2520be%2520used%2520to%2520evaluate%2520the%2520planning%2520capabilities%2520of%2520LLMs%2520in%250Aa%2520principled%2520fashion.%2520The%2520core%2520idea%2520behind%2520LexiCon%2520is%2520to%2520take%2520existing%2520planning%250Aenvironments%2520and%2520impose%2520temporal%2520constraints%2520on%2520the%2520states.%2520These%2520constrained%250Aproblems%2520are%2520then%2520translated%2520into%2520natural%2520language%2520and%2520given%2520to%2520an%2520LLM%2520to%250Asolve.%2520A%2520key%2520feature%2520of%2520LexiCon%2520is%2520its%2520extensibility.%2520That%2520is%252C%2520the%2520set%2520of%250Asupported%2520environments%2520can%2520be%2520extended%2520with%2520new%2520%2528unconstrained%2529%2520environment%250Agenerators%252C%2520for%2520which%2520temporal%2520constraints%2520are%2520constructed%2520automatically.%2520This%250Arenders%2520LexiCon%2520future-proof%253A%2520the%2520hardness%2520of%2520the%2520generated%2520planning%2520problems%250Acan%2520be%2520increased%2520as%2520the%2520planning%2520capabilities%2520of%2520LLMs%2520improve.%2520Our%2520experiments%250Areveal%2520that%2520the%2520performance%2520of%2520state-of-the-art%2520LLMs%252C%2520including%2520reasoning%250Amodels%2520like%2520GPT-5%252C%2520o3%252C%2520and%2520R1%252C%2520deteriorates%2520as%2520the%2520degree%2520of%2520constrainedness%2520of%250Athe%2520planning%2520tasks%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LexiCon%3A%20a%20Benchmark%20for%20Planning%20under%20Temporal%20Constraints%20in%20Natural%0A%20%20Language&entry.906535625=Periklis%20Mantenoglou%20and%20Rishi%20Hazra%20and%20Pedro%20Zuidberg%20Dos%20Martires%20and%20Luc%20De%20Raedt&entry.1292438233=%20%20Owing%20to%20their%20reasoning%20capabilities%2C%20large%20language%20models%20%28LLMs%29%20have%20been%0Aevaluated%20on%20planning%20tasks%20described%20in%20natural%20language.%20However%2C%20LLMs%20have%0Alargely%20been%20tested%20on%20planning%20domains%20without%20constraints.%20In%20order%20to%20deploy%0Athem%20in%20real-world%20settings%20where%20adherence%20to%20constraints%2C%20in%20particular%0Asafety%20constraints%2C%20is%20critical%2C%20we%20need%20to%20evaluate%20their%20performance%20on%0Aconstrained%20planning%20tasks.%20We%20introduce%20LexiCon%20--%20a%20natural%20language-based%0A%28Lexi%29%20constrained%20%28Con%29%20planning%20benchmark%2C%20consisting%20of%20a%20suite%20of%0Aenvironments%2C%20that%20can%20be%20used%20to%20evaluate%20the%20planning%20capabilities%20of%20LLMs%20in%0Aa%20principled%20fashion.%20The%20core%20idea%20behind%20LexiCon%20is%20to%20take%20existing%20planning%0Aenvironments%20and%20impose%20temporal%20constraints%20on%20the%20states.%20These%20constrained%0Aproblems%20are%20then%20translated%20into%20natural%20language%20and%20given%20to%20an%20LLM%20to%0Asolve.%20A%20key%20feature%20of%20LexiCon%20is%20its%20extensibility.%20That%20is%2C%20the%20set%20of%0Asupported%20environments%20can%20be%20extended%20with%20new%20%28unconstrained%29%20environment%0Agenerators%2C%20for%20which%20temporal%20constraints%20are%20constructed%20automatically.%20This%0Arenders%20LexiCon%20future-proof%3A%20the%20hardness%20of%20the%20generated%20planning%20problems%0Acan%20be%20increased%20as%20the%20planning%20capabilities%20of%20LLMs%20improve.%20Our%20experiments%0Areveal%20that%20the%20performance%20of%20state-of-the-art%20LLMs%2C%20including%20reasoning%0Amodels%20like%20GPT-5%2C%20o3%2C%20and%20R1%2C%20deteriorates%20as%20the%20degree%20of%20constrainedness%20of%0Athe%20planning%20tasks%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05972v1&entry.124074799=Read"},
{"title": "Mitigating Exponential Mixed Frequency Growth through Frequency\n  Selection", "author": "Michael Poppel and David Bucher and Maximilian Zorn and Nico Kraus and Philipp Altmann and Jonas Stein and Claudia Linnhoff-Popien", "abstract": "  Quantum machine learning research has expanded rapidly due to potential\ncomputational advantages over classical methods. Angle encoding has emerged as\na popular choice as feature map (FM) for embedding classical data into quantum\nmodels due to its simplicity and natural generation of truncated Fourier\nseries, providing universal function approximation capabilities. Efficient FMs\nwithin quantum circuits can exploit exponential scaling of Fourier frequencies,\nwith multi-dimensional inputs introducing additional exponential growth through\nmixed-frequency terms. Despite this promising expressive capability, practical\nimplementation faces significant challenges. Through controlled experiments\nwith white-box target functions, we demonstrate that training failures can\noccur even when all relevant frequencies are theoretically accessible. We\nillustrate how two primary known causes lead to unsuccessful optimization:\ninsufficient trainable parameters relative to the model's frequency content,\nand limitations imposed by the ansatz's dynamic lie algebra dimension, but also\nuncover an additional parameter burden: the necessity of controlling non-unique\nfrequencies within the model. To address this, we propose near-zero weight\ninitialization to suppress unnecessary duplicate frequencies. For target\nfunctions with a priori frequency knowledge, we introduce frequency selection\nas a practical solution that reduces parameter requirements and mitigates the\nexponential growth that would otherwise render problems intractable due to\nparameter insufficiency. Our frequency selection approach achieved near-optimal\nperformance (median $R^2 \\approx 0.95$) with 78\\% of the parameters needed by\nthe best standard approach in 10 randomly chosen target functions.\n", "link": "http://arxiv.org/abs/2508.10533v3", "date": "2025-10-07", "relevancy": 2.1403, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4329}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4295}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Exponential%20Mixed%20Frequency%20Growth%20through%20Frequency%0A%20%20Selection&body=Title%3A%20Mitigating%20Exponential%20Mixed%20Frequency%20Growth%20through%20Frequency%0A%20%20Selection%0AAuthor%3A%20Michael%20Poppel%20and%20David%20Bucher%20and%20Maximilian%20Zorn%20and%20Nico%20Kraus%20and%20Philipp%20Altmann%20and%20Jonas%20Stein%20and%20Claudia%20Linnhoff-Popien%0AAbstract%3A%20%20%20Quantum%20machine%20learning%20research%20has%20expanded%20rapidly%20due%20to%20potential%0Acomputational%20advantages%20over%20classical%20methods.%20Angle%20encoding%20has%20emerged%20as%0Aa%20popular%20choice%20as%20feature%20map%20%28FM%29%20for%20embedding%20classical%20data%20into%20quantum%0Amodels%20due%20to%20its%20simplicity%20and%20natural%20generation%20of%20truncated%20Fourier%0Aseries%2C%20providing%20universal%20function%20approximation%20capabilities.%20Efficient%20FMs%0Awithin%20quantum%20circuits%20can%20exploit%20exponential%20scaling%20of%20Fourier%20frequencies%2C%0Awith%20multi-dimensional%20inputs%20introducing%20additional%20exponential%20growth%20through%0Amixed-frequency%20terms.%20Despite%20this%20promising%20expressive%20capability%2C%20practical%0Aimplementation%20faces%20significant%20challenges.%20Through%20controlled%20experiments%0Awith%20white-box%20target%20functions%2C%20we%20demonstrate%20that%20training%20failures%20can%0Aoccur%20even%20when%20all%20relevant%20frequencies%20are%20theoretically%20accessible.%20We%0Aillustrate%20how%20two%20primary%20known%20causes%20lead%20to%20unsuccessful%20optimization%3A%0Ainsufficient%20trainable%20parameters%20relative%20to%20the%20model%27s%20frequency%20content%2C%0Aand%20limitations%20imposed%20by%20the%20ansatz%27s%20dynamic%20lie%20algebra%20dimension%2C%20but%20also%0Auncover%20an%20additional%20parameter%20burden%3A%20the%20necessity%20of%20controlling%20non-unique%0Afrequencies%20within%20the%20model.%20To%20address%20this%2C%20we%20propose%20near-zero%20weight%0Ainitialization%20to%20suppress%20unnecessary%20duplicate%20frequencies.%20For%20target%0Afunctions%20with%20a%20priori%20frequency%20knowledge%2C%20we%20introduce%20frequency%20selection%0Aas%20a%20practical%20solution%20that%20reduces%20parameter%20requirements%20and%20mitigates%20the%0Aexponential%20growth%20that%20would%20otherwise%20render%20problems%20intractable%20due%20to%0Aparameter%20insufficiency.%20Our%20frequency%20selection%20approach%20achieved%20near-optimal%0Aperformance%20%28median%20%24R%5E2%20%5Capprox%200.95%24%29%20with%2078%5C%25%20of%20the%20parameters%20needed%20by%0Athe%20best%20standard%20approach%20in%2010%20randomly%20chosen%20target%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10533v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Exponential%2520Mixed%2520Frequency%2520Growth%2520through%2520Frequency%250A%2520%2520Selection%26entry.906535625%3DMichael%2520Poppel%2520and%2520David%2520Bucher%2520and%2520Maximilian%2520Zorn%2520and%2520Nico%2520Kraus%2520and%2520Philipp%2520Altmann%2520and%2520Jonas%2520Stein%2520and%2520Claudia%2520Linnhoff-Popien%26entry.1292438233%3D%2520%2520Quantum%2520machine%2520learning%2520research%2520has%2520expanded%2520rapidly%2520due%2520to%2520potential%250Acomputational%2520advantages%2520over%2520classical%2520methods.%2520Angle%2520encoding%2520has%2520emerged%2520as%250Aa%2520popular%2520choice%2520as%2520feature%2520map%2520%2528FM%2529%2520for%2520embedding%2520classical%2520data%2520into%2520quantum%250Amodels%2520due%2520to%2520its%2520simplicity%2520and%2520natural%2520generation%2520of%2520truncated%2520Fourier%250Aseries%252C%2520providing%2520universal%2520function%2520approximation%2520capabilities.%2520Efficient%2520FMs%250Awithin%2520quantum%2520circuits%2520can%2520exploit%2520exponential%2520scaling%2520of%2520Fourier%2520frequencies%252C%250Awith%2520multi-dimensional%2520inputs%2520introducing%2520additional%2520exponential%2520growth%2520through%250Amixed-frequency%2520terms.%2520Despite%2520this%2520promising%2520expressive%2520capability%252C%2520practical%250Aimplementation%2520faces%2520significant%2520challenges.%2520Through%2520controlled%2520experiments%250Awith%2520white-box%2520target%2520functions%252C%2520we%2520demonstrate%2520that%2520training%2520failures%2520can%250Aoccur%2520even%2520when%2520all%2520relevant%2520frequencies%2520are%2520theoretically%2520accessible.%2520We%250Aillustrate%2520how%2520two%2520primary%2520known%2520causes%2520lead%2520to%2520unsuccessful%2520optimization%253A%250Ainsufficient%2520trainable%2520parameters%2520relative%2520to%2520the%2520model%2527s%2520frequency%2520content%252C%250Aand%2520limitations%2520imposed%2520by%2520the%2520ansatz%2527s%2520dynamic%2520lie%2520algebra%2520dimension%252C%2520but%2520also%250Auncover%2520an%2520additional%2520parameter%2520burden%253A%2520the%2520necessity%2520of%2520controlling%2520non-unique%250Afrequencies%2520within%2520the%2520model.%2520To%2520address%2520this%252C%2520we%2520propose%2520near-zero%2520weight%250Ainitialization%2520to%2520suppress%2520unnecessary%2520duplicate%2520frequencies.%2520For%2520target%250Afunctions%2520with%2520a%2520priori%2520frequency%2520knowledge%252C%2520we%2520introduce%2520frequency%2520selection%250Aas%2520a%2520practical%2520solution%2520that%2520reduces%2520parameter%2520requirements%2520and%2520mitigates%2520the%250Aexponential%2520growth%2520that%2520would%2520otherwise%2520render%2520problems%2520intractable%2520due%2520to%250Aparameter%2520insufficiency.%2520Our%2520frequency%2520selection%2520approach%2520achieved%2520near-optimal%250Aperformance%2520%2528median%2520%2524R%255E2%2520%255Capprox%25200.95%2524%2529%2520with%252078%255C%2525%2520of%2520the%2520parameters%2520needed%2520by%250Athe%2520best%2520standard%2520approach%2520in%252010%2520randomly%2520chosen%2520target%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10533v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Exponential%20Mixed%20Frequency%20Growth%20through%20Frequency%0A%20%20Selection&entry.906535625=Michael%20Poppel%20and%20David%20Bucher%20and%20Maximilian%20Zorn%20and%20Nico%20Kraus%20and%20Philipp%20Altmann%20and%20Jonas%20Stein%20and%20Claudia%20Linnhoff-Popien&entry.1292438233=%20%20Quantum%20machine%20learning%20research%20has%20expanded%20rapidly%20due%20to%20potential%0Acomputational%20advantages%20over%20classical%20methods.%20Angle%20encoding%20has%20emerged%20as%0Aa%20popular%20choice%20as%20feature%20map%20%28FM%29%20for%20embedding%20classical%20data%20into%20quantum%0Amodels%20due%20to%20its%20simplicity%20and%20natural%20generation%20of%20truncated%20Fourier%0Aseries%2C%20providing%20universal%20function%20approximation%20capabilities.%20Efficient%20FMs%0Awithin%20quantum%20circuits%20can%20exploit%20exponential%20scaling%20of%20Fourier%20frequencies%2C%0Awith%20multi-dimensional%20inputs%20introducing%20additional%20exponential%20growth%20through%0Amixed-frequency%20terms.%20Despite%20this%20promising%20expressive%20capability%2C%20practical%0Aimplementation%20faces%20significant%20challenges.%20Through%20controlled%20experiments%0Awith%20white-box%20target%20functions%2C%20we%20demonstrate%20that%20training%20failures%20can%0Aoccur%20even%20when%20all%20relevant%20frequencies%20are%20theoretically%20accessible.%20We%0Aillustrate%20how%20two%20primary%20known%20causes%20lead%20to%20unsuccessful%20optimization%3A%0Ainsufficient%20trainable%20parameters%20relative%20to%20the%20model%27s%20frequency%20content%2C%0Aand%20limitations%20imposed%20by%20the%20ansatz%27s%20dynamic%20lie%20algebra%20dimension%2C%20but%20also%0Auncover%20an%20additional%20parameter%20burden%3A%20the%20necessity%20of%20controlling%20non-unique%0Afrequencies%20within%20the%20model.%20To%20address%20this%2C%20we%20propose%20near-zero%20weight%0Ainitialization%20to%20suppress%20unnecessary%20duplicate%20frequencies.%20For%20target%0Afunctions%20with%20a%20priori%20frequency%20knowledge%2C%20we%20introduce%20frequency%20selection%0Aas%20a%20practical%20solution%20that%20reduces%20parameter%20requirements%20and%20mitigates%20the%0Aexponential%20growth%20that%20would%20otherwise%20render%20problems%20intractable%20due%20to%0Aparameter%20insufficiency.%20Our%20frequency%20selection%20approach%20achieved%20near-optimal%0Aperformance%20%28median%20%24R%5E2%20%5Capprox%200.95%24%29%20with%2078%5C%25%20of%20the%20parameters%20needed%20by%0Athe%20best%20standard%20approach%20in%2010%20randomly%20chosen%20target%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10533v3&entry.124074799=Read"},
{"title": "Interpretable Clustering: A Survey", "author": "Lianyu Hu and Mudi Jiang and Junjie Dong and Xinying Liu and Zengyou He", "abstract": "  In recent years, much of the research on clustering algorithms has primarily\nfocused on enhancing their accuracy and efficiency, frequently at the expense\nof interpretability. However, as these methods are increasingly being applied\nin high-stakes domains such as healthcare, finance, and autonomous systems, the\nneed for transparent and interpretable clustering outcomes has become a\ncritical concern. This is not only necessary for gaining user trust but also\nfor satisfying the growing ethical and regulatory demands in these fields.\nEnsuring that decisions derived from clustering algorithms can be clearly\nunderstood and justified is now a fundamental requirement. To address this\nneed, this paper provides a comprehensive and structured review of the current\nstate of explainable clustering algorithms, identifying key criteria to\ndistinguish between various methods. These insights can effectively assist\nresearchers in making informed decisions about the most suitable explainable\nclustering methods for specific application contexts, while also promoting the\ndevelopment and adoption of clustering algorithms that are both efficient and\ntransparent. For convenient access and reference, an open repository organizes\nrepresentative and emerging interpretable clustering methods under the taxonomy\nproposed in this survey, available at\nhttps://github.com/hulianyu/Awesome-Interpretable-Clustering\n", "link": "http://arxiv.org/abs/2409.00743v2", "date": "2025-10-07", "relevancy": 2.1156, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4237}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Clustering%3A%20A%20Survey&body=Title%3A%20Interpretable%20Clustering%3A%20A%20Survey%0AAuthor%3A%20Lianyu%20Hu%20and%20Mudi%20Jiang%20and%20Junjie%20Dong%20and%20Xinying%20Liu%20and%20Zengyou%20He%0AAbstract%3A%20%20%20In%20recent%20years%2C%20much%20of%20the%20research%20on%20clustering%20algorithms%20has%20primarily%0Afocused%20on%20enhancing%20their%20accuracy%20and%20efficiency%2C%20frequently%20at%20the%20expense%0Aof%20interpretability.%20However%2C%20as%20these%20methods%20are%20increasingly%20being%20applied%0Ain%20high-stakes%20domains%20such%20as%20healthcare%2C%20finance%2C%20and%20autonomous%20systems%2C%20the%0Aneed%20for%20transparent%20and%20interpretable%20clustering%20outcomes%20has%20become%20a%0Acritical%20concern.%20This%20is%20not%20only%20necessary%20for%20gaining%20user%20trust%20but%20also%0Afor%20satisfying%20the%20growing%20ethical%20and%20regulatory%20demands%20in%20these%20fields.%0AEnsuring%20that%20decisions%20derived%20from%20clustering%20algorithms%20can%20be%20clearly%0Aunderstood%20and%20justified%20is%20now%20a%20fundamental%20requirement.%20To%20address%20this%0Aneed%2C%20this%20paper%20provides%20a%20comprehensive%20and%20structured%20review%20of%20the%20current%0Astate%20of%20explainable%20clustering%20algorithms%2C%20identifying%20key%20criteria%20to%0Adistinguish%20between%20various%20methods.%20These%20insights%20can%20effectively%20assist%0Aresearchers%20in%20making%20informed%20decisions%20about%20the%20most%20suitable%20explainable%0Aclustering%20methods%20for%20specific%20application%20contexts%2C%20while%20also%20promoting%20the%0Adevelopment%20and%20adoption%20of%20clustering%20algorithms%20that%20are%20both%20efficient%20and%0Atransparent.%20For%20convenient%20access%20and%20reference%2C%20an%20open%20repository%20organizes%0Arepresentative%20and%20emerging%20interpretable%20clustering%20methods%20under%20the%20taxonomy%0Aproposed%20in%20this%20survey%2C%20available%20at%0Ahttps%3A//github.com/hulianyu/Awesome-Interpretable-Clustering%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00743v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Clustering%253A%2520A%2520Survey%26entry.906535625%3DLianyu%2520Hu%2520and%2520Mudi%2520Jiang%2520and%2520Junjie%2520Dong%2520and%2520Xinying%2520Liu%2520and%2520Zengyou%2520He%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520much%2520of%2520the%2520research%2520on%2520clustering%2520algorithms%2520has%2520primarily%250Afocused%2520on%2520enhancing%2520their%2520accuracy%2520and%2520efficiency%252C%2520frequently%2520at%2520the%2520expense%250Aof%2520interpretability.%2520However%252C%2520as%2520these%2520methods%2520are%2520increasingly%2520being%2520applied%250Ain%2520high-stakes%2520domains%2520such%2520as%2520healthcare%252C%2520finance%252C%2520and%2520autonomous%2520systems%252C%2520the%250Aneed%2520for%2520transparent%2520and%2520interpretable%2520clustering%2520outcomes%2520has%2520become%2520a%250Acritical%2520concern.%2520This%2520is%2520not%2520only%2520necessary%2520for%2520gaining%2520user%2520trust%2520but%2520also%250Afor%2520satisfying%2520the%2520growing%2520ethical%2520and%2520regulatory%2520demands%2520in%2520these%2520fields.%250AEnsuring%2520that%2520decisions%2520derived%2520from%2520clustering%2520algorithms%2520can%2520be%2520clearly%250Aunderstood%2520and%2520justified%2520is%2520now%2520a%2520fundamental%2520requirement.%2520To%2520address%2520this%250Aneed%252C%2520this%2520paper%2520provides%2520a%2520comprehensive%2520and%2520structured%2520review%2520of%2520the%2520current%250Astate%2520of%2520explainable%2520clustering%2520algorithms%252C%2520identifying%2520key%2520criteria%2520to%250Adistinguish%2520between%2520various%2520methods.%2520These%2520insights%2520can%2520effectively%2520assist%250Aresearchers%2520in%2520making%2520informed%2520decisions%2520about%2520the%2520most%2520suitable%2520explainable%250Aclustering%2520methods%2520for%2520specific%2520application%2520contexts%252C%2520while%2520also%2520promoting%2520the%250Adevelopment%2520and%2520adoption%2520of%2520clustering%2520algorithms%2520that%2520are%2520both%2520efficient%2520and%250Atransparent.%2520For%2520convenient%2520access%2520and%2520reference%252C%2520an%2520open%2520repository%2520organizes%250Arepresentative%2520and%2520emerging%2520interpretable%2520clustering%2520methods%2520under%2520the%2520taxonomy%250Aproposed%2520in%2520this%2520survey%252C%2520available%2520at%250Ahttps%253A//github.com/hulianyu/Awesome-Interpretable-Clustering%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00743v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Clustering%3A%20A%20Survey&entry.906535625=Lianyu%20Hu%20and%20Mudi%20Jiang%20and%20Junjie%20Dong%20and%20Xinying%20Liu%20and%20Zengyou%20He&entry.1292438233=%20%20In%20recent%20years%2C%20much%20of%20the%20research%20on%20clustering%20algorithms%20has%20primarily%0Afocused%20on%20enhancing%20their%20accuracy%20and%20efficiency%2C%20frequently%20at%20the%20expense%0Aof%20interpretability.%20However%2C%20as%20these%20methods%20are%20increasingly%20being%20applied%0Ain%20high-stakes%20domains%20such%20as%20healthcare%2C%20finance%2C%20and%20autonomous%20systems%2C%20the%0Aneed%20for%20transparent%20and%20interpretable%20clustering%20outcomes%20has%20become%20a%0Acritical%20concern.%20This%20is%20not%20only%20necessary%20for%20gaining%20user%20trust%20but%20also%0Afor%20satisfying%20the%20growing%20ethical%20and%20regulatory%20demands%20in%20these%20fields.%0AEnsuring%20that%20decisions%20derived%20from%20clustering%20algorithms%20can%20be%20clearly%0Aunderstood%20and%20justified%20is%20now%20a%20fundamental%20requirement.%20To%20address%20this%0Aneed%2C%20this%20paper%20provides%20a%20comprehensive%20and%20structured%20review%20of%20the%20current%0Astate%20of%20explainable%20clustering%20algorithms%2C%20identifying%20key%20criteria%20to%0Adistinguish%20between%20various%20methods.%20These%20insights%20can%20effectively%20assist%0Aresearchers%20in%20making%20informed%20decisions%20about%20the%20most%20suitable%20explainable%0Aclustering%20methods%20for%20specific%20application%20contexts%2C%20while%20also%20promoting%20the%0Adevelopment%20and%20adoption%20of%20clustering%20algorithms%20that%20are%20both%20efficient%20and%0Atransparent.%20For%20convenient%20access%20and%20reference%2C%20an%20open%20repository%20organizes%0Arepresentative%20and%20emerging%20interpretable%20clustering%20methods%20under%20the%20taxonomy%0Aproposed%20in%20this%20survey%2C%20available%20at%0Ahttps%3A//github.com/hulianyu/Awesome-Interpretable-Clustering%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00743v2&entry.124074799=Read"},
{"title": "Deformable Image Registration for Self-supervised Cardiac Phase\n  Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images", "author": "Sven Koehler and Sarah Kaye Mueller and Jonathan Kiekenap and Gerald Greil and Tarique Hussain and Samir Sarikouch and Florian Andr\u00e9 and Norbert Frey and Sandy Engelhardt", "abstract": "  Cardiovascular magnetic resonance (CMR) is the gold standard for assessing\ncardiac function, but individual cardiac cycles complicate automatic temporal\ncomparison or sub-phase analysis. Accurate cardiac keyframe detection can\neliminate this problem. However, automatic methods solely derive end-systole\n(ES) and end-diastole (ED) frames from left ventricular volume curves, which do\nnot provide a deeper insight into myocardial motion. We propose a\nself-supervised deep learning method detecting five keyframes in short-axis\n(SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable\nregistration fields are derived from the images and used to compute a 1D motion\ndescriptor, which provides valuable insights into global cardiac contraction\nand relaxation patterns. From these characteristic curves, keyframes are\ndetermined using a simple set of rules. The method was independently evaluated\nfor both views using three public, multicentre, multidisease datasets. M&Ms-2\n(n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC\n(n=100) datasets for repeatability control. Furthermore, generalisability to\npatients with rare congenital heart defects was tested using the German\nCompetence Network (GCN) dataset. Our self-supervised approach achieved\nimproved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED\nand ES, as measured by cyclic frame difference (cFD), compared with the\nvolume-based approach. We can detect ED and ES, as well as three additional\nkeyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for\nSAX and 1.73 for LAX. Our approach enables temporally aligned inter- and\nintra-patient analysis of cardiac dynamics, irrespective of cycle or phase\nlengths. GitHub repository:\nhttps://github.com/Cardio-AI/cmr-multi-view-phase-detection.git\n", "link": "http://arxiv.org/abs/2510.05819v1", "date": "2025-10-07", "relevancy": 2.1091, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5405}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5298}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deformable%20Image%20Registration%20for%20Self-supervised%20Cardiac%20Phase%0A%20%20Detection%20in%20Multi-View%20Multi-Disease%20Cardiac%20Magnetic%20Resonance%20Images&body=Title%3A%20Deformable%20Image%20Registration%20for%20Self-supervised%20Cardiac%20Phase%0A%20%20Detection%20in%20Multi-View%20Multi-Disease%20Cardiac%20Magnetic%20Resonance%20Images%0AAuthor%3A%20Sven%20Koehler%20and%20Sarah%20Kaye%20Mueller%20and%20Jonathan%20Kiekenap%20and%20Gerald%20Greil%20and%20Tarique%20Hussain%20and%20Samir%20Sarikouch%20and%20Florian%20Andr%C3%A9%20and%20Norbert%20Frey%20and%20Sandy%20Engelhardt%0AAbstract%3A%20%20%20Cardiovascular%20magnetic%20resonance%20%28CMR%29%20is%20the%20gold%20standard%20for%20assessing%0Acardiac%20function%2C%20but%20individual%20cardiac%20cycles%20complicate%20automatic%20temporal%0Acomparison%20or%20sub-phase%20analysis.%20Accurate%20cardiac%20keyframe%20detection%20can%0Aeliminate%20this%20problem.%20However%2C%20automatic%20methods%20solely%20derive%20end-systole%0A%28ES%29%20and%20end-diastole%20%28ED%29%20frames%20from%20left%20ventricular%20volume%20curves%2C%20which%20do%0Anot%20provide%20a%20deeper%20insight%20into%20myocardial%20motion.%20We%20propose%20a%0Aself-supervised%20deep%20learning%20method%20detecting%20five%20keyframes%20in%20short-axis%0A%28SAX%29%20and%20four-chamber%20long-axis%20%284CH%29%20cine%20CMR.%20Initially%2C%20dense%20deformable%0Aregistration%20fields%20are%20derived%20from%20the%20images%20and%20used%20to%20compute%20a%201D%20motion%0Adescriptor%2C%20which%20provides%20valuable%20insights%20into%20global%20cardiac%20contraction%0Aand%20relaxation%20patterns.%20From%20these%20characteristic%20curves%2C%20keyframes%20are%0Adetermined%20using%20a%20simple%20set%20of%20rules.%20The%20method%20was%20independently%20evaluated%0Afor%20both%20views%20using%20three%20public%2C%20multicentre%2C%20multidisease%20datasets.%20M%26Ms-2%0A%28n%3D360%29%20dataset%20was%20used%20for%20training%20and%20evaluation%2C%20and%20M%26Ms%20%28n%3D345%29%20and%20ACDC%0A%28n%3D100%29%20datasets%20for%20repeatability%20control.%20Furthermore%2C%20generalisability%20to%0Apatients%20with%20rare%20congenital%20heart%20defects%20was%20tested%20using%20the%20German%0ACompetence%20Network%20%28GCN%29%20dataset.%20Our%20self-supervised%20approach%20achieved%0Aimproved%20detection%20accuracy%20by%2030%25%20-%2051%25%20for%20SAX%20and%2011%25%20-%2047%25%20for%204CH%20in%20ED%0Aand%20ES%2C%20as%20measured%20by%20cyclic%20frame%20difference%20%28cFD%29%2C%20compared%20with%20the%0Avolume-based%20approach.%20We%20can%20detect%20ED%20and%20ES%2C%20as%20well%20as%20three%20additional%0Akeyframes%20throughout%20the%20cardiac%20cycle%20with%20a%20mean%20cFD%20below%201.31%20frames%20for%0ASAX%20and%201.73%20for%20LAX.%20Our%20approach%20enables%20temporally%20aligned%20inter-%20and%0Aintra-patient%20analysis%20of%20cardiac%20dynamics%2C%20irrespective%20of%20cycle%20or%20phase%0Alengths.%20GitHub%20repository%3A%0Ahttps%3A//github.com/Cardio-AI/cmr-multi-view-phase-detection.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeformable%2520Image%2520Registration%2520for%2520Self-supervised%2520Cardiac%2520Phase%250A%2520%2520Detection%2520in%2520Multi-View%2520Multi-Disease%2520Cardiac%2520Magnetic%2520Resonance%2520Images%26entry.906535625%3DSven%2520Koehler%2520and%2520Sarah%2520Kaye%2520Mueller%2520and%2520Jonathan%2520Kiekenap%2520and%2520Gerald%2520Greil%2520and%2520Tarique%2520Hussain%2520and%2520Samir%2520Sarikouch%2520and%2520Florian%2520Andr%25C3%25A9%2520and%2520Norbert%2520Frey%2520and%2520Sandy%2520Engelhardt%26entry.1292438233%3D%2520%2520Cardiovascular%2520magnetic%2520resonance%2520%2528CMR%2529%2520is%2520the%2520gold%2520standard%2520for%2520assessing%250Acardiac%2520function%252C%2520but%2520individual%2520cardiac%2520cycles%2520complicate%2520automatic%2520temporal%250Acomparison%2520or%2520sub-phase%2520analysis.%2520Accurate%2520cardiac%2520keyframe%2520detection%2520can%250Aeliminate%2520this%2520problem.%2520However%252C%2520automatic%2520methods%2520solely%2520derive%2520end-systole%250A%2528ES%2529%2520and%2520end-diastole%2520%2528ED%2529%2520frames%2520from%2520left%2520ventricular%2520volume%2520curves%252C%2520which%2520do%250Anot%2520provide%2520a%2520deeper%2520insight%2520into%2520myocardial%2520motion.%2520We%2520propose%2520a%250Aself-supervised%2520deep%2520learning%2520method%2520detecting%2520five%2520keyframes%2520in%2520short-axis%250A%2528SAX%2529%2520and%2520four-chamber%2520long-axis%2520%25284CH%2529%2520cine%2520CMR.%2520Initially%252C%2520dense%2520deformable%250Aregistration%2520fields%2520are%2520derived%2520from%2520the%2520images%2520and%2520used%2520to%2520compute%2520a%25201D%2520motion%250Adescriptor%252C%2520which%2520provides%2520valuable%2520insights%2520into%2520global%2520cardiac%2520contraction%250Aand%2520relaxation%2520patterns.%2520From%2520these%2520characteristic%2520curves%252C%2520keyframes%2520are%250Adetermined%2520using%2520a%2520simple%2520set%2520of%2520rules.%2520The%2520method%2520was%2520independently%2520evaluated%250Afor%2520both%2520views%2520using%2520three%2520public%252C%2520multicentre%252C%2520multidisease%2520datasets.%2520M%2526Ms-2%250A%2528n%253D360%2529%2520dataset%2520was%2520used%2520for%2520training%2520and%2520evaluation%252C%2520and%2520M%2526Ms%2520%2528n%253D345%2529%2520and%2520ACDC%250A%2528n%253D100%2529%2520datasets%2520for%2520repeatability%2520control.%2520Furthermore%252C%2520generalisability%2520to%250Apatients%2520with%2520rare%2520congenital%2520heart%2520defects%2520was%2520tested%2520using%2520the%2520German%250ACompetence%2520Network%2520%2528GCN%2529%2520dataset.%2520Our%2520self-supervised%2520approach%2520achieved%250Aimproved%2520detection%2520accuracy%2520by%252030%2525%2520-%252051%2525%2520for%2520SAX%2520and%252011%2525%2520-%252047%2525%2520for%25204CH%2520in%2520ED%250Aand%2520ES%252C%2520as%2520measured%2520by%2520cyclic%2520frame%2520difference%2520%2528cFD%2529%252C%2520compared%2520with%2520the%250Avolume-based%2520approach.%2520We%2520can%2520detect%2520ED%2520and%2520ES%252C%2520as%2520well%2520as%2520three%2520additional%250Akeyframes%2520throughout%2520the%2520cardiac%2520cycle%2520with%2520a%2520mean%2520cFD%2520below%25201.31%2520frames%2520for%250ASAX%2520and%25201.73%2520for%2520LAX.%2520Our%2520approach%2520enables%2520temporally%2520aligned%2520inter-%2520and%250Aintra-patient%2520analysis%2520of%2520cardiac%2520dynamics%252C%2520irrespective%2520of%2520cycle%2520or%2520phase%250Alengths.%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/Cardio-AI/cmr-multi-view-phase-detection.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deformable%20Image%20Registration%20for%20Self-supervised%20Cardiac%20Phase%0A%20%20Detection%20in%20Multi-View%20Multi-Disease%20Cardiac%20Magnetic%20Resonance%20Images&entry.906535625=Sven%20Koehler%20and%20Sarah%20Kaye%20Mueller%20and%20Jonathan%20Kiekenap%20and%20Gerald%20Greil%20and%20Tarique%20Hussain%20and%20Samir%20Sarikouch%20and%20Florian%20Andr%C3%A9%20and%20Norbert%20Frey%20and%20Sandy%20Engelhardt&entry.1292438233=%20%20Cardiovascular%20magnetic%20resonance%20%28CMR%29%20is%20the%20gold%20standard%20for%20assessing%0Acardiac%20function%2C%20but%20individual%20cardiac%20cycles%20complicate%20automatic%20temporal%0Acomparison%20or%20sub-phase%20analysis.%20Accurate%20cardiac%20keyframe%20detection%20can%0Aeliminate%20this%20problem.%20However%2C%20automatic%20methods%20solely%20derive%20end-systole%0A%28ES%29%20and%20end-diastole%20%28ED%29%20frames%20from%20left%20ventricular%20volume%20curves%2C%20which%20do%0Anot%20provide%20a%20deeper%20insight%20into%20myocardial%20motion.%20We%20propose%20a%0Aself-supervised%20deep%20learning%20method%20detecting%20five%20keyframes%20in%20short-axis%0A%28SAX%29%20and%20four-chamber%20long-axis%20%284CH%29%20cine%20CMR.%20Initially%2C%20dense%20deformable%0Aregistration%20fields%20are%20derived%20from%20the%20images%20and%20used%20to%20compute%20a%201D%20motion%0Adescriptor%2C%20which%20provides%20valuable%20insights%20into%20global%20cardiac%20contraction%0Aand%20relaxation%20patterns.%20From%20these%20characteristic%20curves%2C%20keyframes%20are%0Adetermined%20using%20a%20simple%20set%20of%20rules.%20The%20method%20was%20independently%20evaluated%0Afor%20both%20views%20using%20three%20public%2C%20multicentre%2C%20multidisease%20datasets.%20M%26Ms-2%0A%28n%3D360%29%20dataset%20was%20used%20for%20training%20and%20evaluation%2C%20and%20M%26Ms%20%28n%3D345%29%20and%20ACDC%0A%28n%3D100%29%20datasets%20for%20repeatability%20control.%20Furthermore%2C%20generalisability%20to%0Apatients%20with%20rare%20congenital%20heart%20defects%20was%20tested%20using%20the%20German%0ACompetence%20Network%20%28GCN%29%20dataset.%20Our%20self-supervised%20approach%20achieved%0Aimproved%20detection%20accuracy%20by%2030%25%20-%2051%25%20for%20SAX%20and%2011%25%20-%2047%25%20for%204CH%20in%20ED%0Aand%20ES%2C%20as%20measured%20by%20cyclic%20frame%20difference%20%28cFD%29%2C%20compared%20with%20the%0Avolume-based%20approach.%20We%20can%20detect%20ED%20and%20ES%2C%20as%20well%20as%20three%20additional%0Akeyframes%20throughout%20the%20cardiac%20cycle%20with%20a%20mean%20cFD%20below%201.31%20frames%20for%0ASAX%20and%201.73%20for%20LAX.%20Our%20approach%20enables%20temporally%20aligned%20inter-%20and%0Aintra-patient%20analysis%20of%20cardiac%20dynamics%2C%20irrespective%20of%20cycle%20or%20phase%0Alengths.%20GitHub%20repository%3A%0Ahttps%3A//github.com/Cardio-AI/cmr-multi-view-phase-detection.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05819v1&entry.124074799=Read"},
{"title": "Building Resource-Constrained Language Agents: A Korean Case Study on\n  Chemical Toxicity Information", "author": "Hojun Cho and Donghu Kim and Soyoung Yang and Chan Lee and Hunjoo Lee and Jaegul Choo", "abstract": "  Language agents powered by large language models (LLMs) face significant\ndeployment challenges in resource-constrained environments, particularly for\nspecialized domains and less-common languages. This paper presents Tox-chat, a\nKorean chemical toxicity information agent devised within these limitations. We\npropose two key innovations: a context-efficient architecture that reduces\ntoken consumption through hierarchical section search, and a scenario-based\ndialogue generation methodology that effectively distills tool-using\ncapabilities from larger models. Experimental evaluations demonstrate that our\nfine-tuned 8B parameter model substantially outperforms both untuned models and\nbaseline approaches, in terms of DB faithfulness and preference. Our work\noffers valuable insights for researchers developing domain-specific language\nagents under practical constraints.\n", "link": "http://arxiv.org/abs/2503.17753v2", "date": "2025-10-07", "relevancy": 1.9031, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4987}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Resource-Constrained%20Language%20Agents%3A%20A%20Korean%20Case%20Study%20on%0A%20%20Chemical%20Toxicity%20Information&body=Title%3A%20Building%20Resource-Constrained%20Language%20Agents%3A%20A%20Korean%20Case%20Study%20on%0A%20%20Chemical%20Toxicity%20Information%0AAuthor%3A%20Hojun%20Cho%20and%20Donghu%20Kim%20and%20Soyoung%20Yang%20and%20Chan%20Lee%20and%20Hunjoo%20Lee%20and%20Jaegul%20Choo%0AAbstract%3A%20%20%20Language%20agents%20powered%20by%20large%20language%20models%20%28LLMs%29%20face%20significant%0Adeployment%20challenges%20in%20resource-constrained%20environments%2C%20particularly%20for%0Aspecialized%20domains%20and%20less-common%20languages.%20This%20paper%20presents%20Tox-chat%2C%20a%0AKorean%20chemical%20toxicity%20information%20agent%20devised%20within%20these%20limitations.%20We%0Apropose%20two%20key%20innovations%3A%20a%20context-efficient%20architecture%20that%20reduces%0Atoken%20consumption%20through%20hierarchical%20section%20search%2C%20and%20a%20scenario-based%0Adialogue%20generation%20methodology%20that%20effectively%20distills%20tool-using%0Acapabilities%20from%20larger%20models.%20Experimental%20evaluations%20demonstrate%20that%20our%0Afine-tuned%208B%20parameter%20model%20substantially%20outperforms%20both%20untuned%20models%20and%0Abaseline%20approaches%2C%20in%20terms%20of%20DB%20faithfulness%20and%20preference.%20Our%20work%0Aoffers%20valuable%20insights%20for%20researchers%20developing%20domain-specific%20language%0Aagents%20under%20practical%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Resource-Constrained%2520Language%2520Agents%253A%2520A%2520Korean%2520Case%2520Study%2520on%250A%2520%2520Chemical%2520Toxicity%2520Information%26entry.906535625%3DHojun%2520Cho%2520and%2520Donghu%2520Kim%2520and%2520Soyoung%2520Yang%2520and%2520Chan%2520Lee%2520and%2520Hunjoo%2520Lee%2520and%2520Jaegul%2520Choo%26entry.1292438233%3D%2520%2520Language%2520agents%2520powered%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520face%2520significant%250Adeployment%2520challenges%2520in%2520resource-constrained%2520environments%252C%2520particularly%2520for%250Aspecialized%2520domains%2520and%2520less-common%2520languages.%2520This%2520paper%2520presents%2520Tox-chat%252C%2520a%250AKorean%2520chemical%2520toxicity%2520information%2520agent%2520devised%2520within%2520these%2520limitations.%2520We%250Apropose%2520two%2520key%2520innovations%253A%2520a%2520context-efficient%2520architecture%2520that%2520reduces%250Atoken%2520consumption%2520through%2520hierarchical%2520section%2520search%252C%2520and%2520a%2520scenario-based%250Adialogue%2520generation%2520methodology%2520that%2520effectively%2520distills%2520tool-using%250Acapabilities%2520from%2520larger%2520models.%2520Experimental%2520evaluations%2520demonstrate%2520that%2520our%250Afine-tuned%25208B%2520parameter%2520model%2520substantially%2520outperforms%2520both%2520untuned%2520models%2520and%250Abaseline%2520approaches%252C%2520in%2520terms%2520of%2520DB%2520faithfulness%2520and%2520preference.%2520Our%2520work%250Aoffers%2520valuable%2520insights%2520for%2520researchers%2520developing%2520domain-specific%2520language%250Aagents%2520under%2520practical%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Resource-Constrained%20Language%20Agents%3A%20A%20Korean%20Case%20Study%20on%0A%20%20Chemical%20Toxicity%20Information&entry.906535625=Hojun%20Cho%20and%20Donghu%20Kim%20and%20Soyoung%20Yang%20and%20Chan%20Lee%20and%20Hunjoo%20Lee%20and%20Jaegul%20Choo&entry.1292438233=%20%20Language%20agents%20powered%20by%20large%20language%20models%20%28LLMs%29%20face%20significant%0Adeployment%20challenges%20in%20resource-constrained%20environments%2C%20particularly%20for%0Aspecialized%20domains%20and%20less-common%20languages.%20This%20paper%20presents%20Tox-chat%2C%20a%0AKorean%20chemical%20toxicity%20information%20agent%20devised%20within%20these%20limitations.%20We%0Apropose%20two%20key%20innovations%3A%20a%20context-efficient%20architecture%20that%20reduces%0Atoken%20consumption%20through%20hierarchical%20section%20search%2C%20and%20a%20scenario-based%0Adialogue%20generation%20methodology%20that%20effectively%20distills%20tool-using%0Acapabilities%20from%20larger%20models.%20Experimental%20evaluations%20demonstrate%20that%20our%0Afine-tuned%208B%20parameter%20model%20substantially%20outperforms%20both%20untuned%20models%20and%0Abaseline%20approaches%2C%20in%20terms%20of%20DB%20faithfulness%20and%20preference.%20Our%20work%0Aoffers%20valuable%20insights%20for%20researchers%20developing%20domain-specific%20language%0Aagents%20under%20practical%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17753v2&entry.124074799=Read"},
{"title": "Flow4Agent: Long-form Video Understanding via Motion Prior from Optical\n  Flow", "author": "Ruyang Liu and Shangkun Sun and Haoran Tang and Ge Li and Wei Gao", "abstract": "  Long-form video understanding has always been a challenging problem due to\nthe significant redundancy in both temporal and spatial contents. This\nchallenge is further exacerbated by the limited context length of Multimodal\nLarge Language Models (MLLMs). To address this issue, many previous works have\nattempted to extract key video information, where the \"key\" is typically\nsemantic-aware and heavily dependent on the CLIP model as prior. In this paper,\nwe propose Flow4Agent, a novel framework that pioneeringly incorporates motion\npriors from optical flow to facilitate LLM-based long video understanding.\nFlow4Agent mitigates the redundancy in long videos at both temporal and spatial\nlevels through two core modules: Temporal Granularity Optimization (TGO)\nadaptively refines framelevel hierarchies, which first leverages coarse flow\npriors to group similar visual contents and then applies semantic priors to\nfilter out highly irrelevant scene information. Motion Token Pruning (MTP)\nfurther refines the intra-frame visual representations, pruning high-redundancy\nvideo tokens using fine-grained optical flow information. Extensive experiments\ndemonstrate that our Flow4Agent outperforms existing methods across a wide\nrange of video MLLM benchmarks, especially for hour-level video understanding\ntasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.\n", "link": "http://arxiv.org/abs/2510.05836v1", "date": "2025-10-07", "relevancy": 1.65, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.57}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow4Agent%3A%20Long-form%20Video%20Understanding%20via%20Motion%20Prior%20from%20Optical%0A%20%20Flow&body=Title%3A%20Flow4Agent%3A%20Long-form%20Video%20Understanding%20via%20Motion%20Prior%20from%20Optical%0A%20%20Flow%0AAuthor%3A%20Ruyang%20Liu%20and%20Shangkun%20Sun%20and%20Haoran%20Tang%20and%20Ge%20Li%20and%20Wei%20Gao%0AAbstract%3A%20%20%20Long-form%20video%20understanding%20has%20always%20been%20a%20challenging%20problem%20due%20to%0Athe%20significant%20redundancy%20in%20both%20temporal%20and%20spatial%20contents.%20This%0Achallenge%20is%20further%20exacerbated%20by%20the%20limited%20context%20length%20of%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29.%20To%20address%20this%20issue%2C%20many%20previous%20works%20have%0Aattempted%20to%20extract%20key%20video%20information%2C%20where%20the%20%22key%22%20is%20typically%0Asemantic-aware%20and%20heavily%20dependent%20on%20the%20CLIP%20model%20as%20prior.%20In%20this%20paper%2C%0Awe%20propose%20Flow4Agent%2C%20a%20novel%20framework%20that%20pioneeringly%20incorporates%20motion%0Apriors%20from%20optical%20flow%20to%20facilitate%20LLM-based%20long%20video%20understanding.%0AFlow4Agent%20mitigates%20the%20redundancy%20in%20long%20videos%20at%20both%20temporal%20and%20spatial%0Alevels%20through%20two%20core%20modules%3A%20Temporal%20Granularity%20Optimization%20%28TGO%29%0Aadaptively%20refines%20framelevel%20hierarchies%2C%20which%20first%20leverages%20coarse%20flow%0Apriors%20to%20group%20similar%20visual%20contents%20and%20then%20applies%20semantic%20priors%20to%0Afilter%20out%20highly%20irrelevant%20scene%20information.%20Motion%20Token%20Pruning%20%28MTP%29%0Afurther%20refines%20the%20intra-frame%20visual%20representations%2C%20pruning%20high-redundancy%0Avideo%20tokens%20using%20fine-grained%20optical%20flow%20information.%20Extensive%20experiments%0Ademonstrate%20that%20our%20Flow4Agent%20outperforms%20existing%20methods%20across%20a%20wide%0Arange%20of%20video%20MLLM%20benchmarks%2C%20especially%20for%20hour-level%20video%20understanding%0Atasks%2C%20achieving%2064.7%25%20on%20Video-MME%2C%2071.4%25%20on%20MLVU%20and%2060.4%25%20on%20LongVideoBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow4Agent%253A%2520Long-form%2520Video%2520Understanding%2520via%2520Motion%2520Prior%2520from%2520Optical%250A%2520%2520Flow%26entry.906535625%3DRuyang%2520Liu%2520and%2520Shangkun%2520Sun%2520and%2520Haoran%2520Tang%2520and%2520Ge%2520Li%2520and%2520Wei%2520Gao%26entry.1292438233%3D%2520%2520Long-form%2520video%2520understanding%2520has%2520always%2520been%2520a%2520challenging%2520problem%2520due%2520to%250Athe%2520significant%2520redundancy%2520in%2520both%2520temporal%2520and%2520spatial%2520contents.%2520This%250Achallenge%2520is%2520further%2520exacerbated%2520by%2520the%2520limited%2520context%2520length%2520of%2520Multimodal%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529.%2520To%2520address%2520this%2520issue%252C%2520many%2520previous%2520works%2520have%250Aattempted%2520to%2520extract%2520key%2520video%2520information%252C%2520where%2520the%2520%2522key%2522%2520is%2520typically%250Asemantic-aware%2520and%2520heavily%2520dependent%2520on%2520the%2520CLIP%2520model%2520as%2520prior.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520Flow4Agent%252C%2520a%2520novel%2520framework%2520that%2520pioneeringly%2520incorporates%2520motion%250Apriors%2520from%2520optical%2520flow%2520to%2520facilitate%2520LLM-based%2520long%2520video%2520understanding.%250AFlow4Agent%2520mitigates%2520the%2520redundancy%2520in%2520long%2520videos%2520at%2520both%2520temporal%2520and%2520spatial%250Alevels%2520through%2520two%2520core%2520modules%253A%2520Temporal%2520Granularity%2520Optimization%2520%2528TGO%2529%250Aadaptively%2520refines%2520framelevel%2520hierarchies%252C%2520which%2520first%2520leverages%2520coarse%2520flow%250Apriors%2520to%2520group%2520similar%2520visual%2520contents%2520and%2520then%2520applies%2520semantic%2520priors%2520to%250Afilter%2520out%2520highly%2520irrelevant%2520scene%2520information.%2520Motion%2520Token%2520Pruning%2520%2528MTP%2529%250Afurther%2520refines%2520the%2520intra-frame%2520visual%2520representations%252C%2520pruning%2520high-redundancy%250Avideo%2520tokens%2520using%2520fine-grained%2520optical%2520flow%2520information.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520Flow4Agent%2520outperforms%2520existing%2520methods%2520across%2520a%2520wide%250Arange%2520of%2520video%2520MLLM%2520benchmarks%252C%2520especially%2520for%2520hour-level%2520video%2520understanding%250Atasks%252C%2520achieving%252064.7%2525%2520on%2520Video-MME%252C%252071.4%2525%2520on%2520MLVU%2520and%252060.4%2525%2520on%2520LongVideoBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow4Agent%3A%20Long-form%20Video%20Understanding%20via%20Motion%20Prior%20from%20Optical%0A%20%20Flow&entry.906535625=Ruyang%20Liu%20and%20Shangkun%20Sun%20and%20Haoran%20Tang%20and%20Ge%20Li%20and%20Wei%20Gao&entry.1292438233=%20%20Long-form%20video%20understanding%20has%20always%20been%20a%20challenging%20problem%20due%20to%0Athe%20significant%20redundancy%20in%20both%20temporal%20and%20spatial%20contents.%20This%0Achallenge%20is%20further%20exacerbated%20by%20the%20limited%20context%20length%20of%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29.%20To%20address%20this%20issue%2C%20many%20previous%20works%20have%0Aattempted%20to%20extract%20key%20video%20information%2C%20where%20the%20%22key%22%20is%20typically%0Asemantic-aware%20and%20heavily%20dependent%20on%20the%20CLIP%20model%20as%20prior.%20In%20this%20paper%2C%0Awe%20propose%20Flow4Agent%2C%20a%20novel%20framework%20that%20pioneeringly%20incorporates%20motion%0Apriors%20from%20optical%20flow%20to%20facilitate%20LLM-based%20long%20video%20understanding.%0AFlow4Agent%20mitigates%20the%20redundancy%20in%20long%20videos%20at%20both%20temporal%20and%20spatial%0Alevels%20through%20two%20core%20modules%3A%20Temporal%20Granularity%20Optimization%20%28TGO%29%0Aadaptively%20refines%20framelevel%20hierarchies%2C%20which%20first%20leverages%20coarse%20flow%0Apriors%20to%20group%20similar%20visual%20contents%20and%20then%20applies%20semantic%20priors%20to%0Afilter%20out%20highly%20irrelevant%20scene%20information.%20Motion%20Token%20Pruning%20%28MTP%29%0Afurther%20refines%20the%20intra-frame%20visual%20representations%2C%20pruning%20high-redundancy%0Avideo%20tokens%20using%20fine-grained%20optical%20flow%20information.%20Extensive%20experiments%0Ademonstrate%20that%20our%20Flow4Agent%20outperforms%20existing%20methods%20across%20a%20wide%0Arange%20of%20video%20MLLM%20benchmarks%2C%20especially%20for%20hour-level%20video%20understanding%0Atasks%2C%20achieving%2064.7%25%20on%20Video-MME%2C%2071.4%25%20on%20MLVU%20and%2060.4%25%20on%20LongVideoBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05836v1&entry.124074799=Read"},
{"title": "Hybrid Quantum-Classical Policy Gradient for Adaptive Control of\n  Cyber-Physical Systems: A Comparative Study of VQC vs. MLP", "author": "Aueaphum Aueawatthanaphisut and Nyi Wunna Tun", "abstract": "  The comparative evaluation between classical and quantum reinforcement\nlearning (QRL) paradigms was conducted to investigate their convergence\nbehavior, robustness under observational noise, and computational efficiency in\na benchmark control environment. The study employed a multilayer perceptron\n(MLP) agent as a classical baseline and a parameterized variational quantum\ncircuit (VQC) as a quantum counterpart, both trained on the CartPole-v1\nenvironment over 500 episodes. Empirical results demonstrated that the\nclassical MLP achieved near-optimal policy convergence with a mean return of\n498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast,\nthe VQC exhibited limited learning capability, with an average return of 14.6\n+/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise\nrobustness analysis further revealed that the MLP policy deteriorated\ngracefully under Gaussian perturbations, while the VQC displayed higher\nsensitivity at equivalent noise levels. Despite the lower asymptotic\nperformance, the VQC exhibited significantly lower parameter count and\nmarginally increased training time, highlighting its potential scalability for\nlow-resource quantum processors. The results suggest that while classical\nneural policies remain dominant in current control benchmarks, quantum-enhanced\narchitectures could offer promising efficiency advantages once hardware noise\nand expressivity limitations are mitigated.\n", "link": "http://arxiv.org/abs/2510.06010v1", "date": "2025-10-07", "relevancy": 1.4173, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5112}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4821}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Quantum-Classical%20Policy%20Gradient%20for%20Adaptive%20Control%20of%0A%20%20Cyber-Physical%20Systems%3A%20A%20Comparative%20Study%20of%20VQC%20vs.%20MLP&body=Title%3A%20Hybrid%20Quantum-Classical%20Policy%20Gradient%20for%20Adaptive%20Control%20of%0A%20%20Cyber-Physical%20Systems%3A%20A%20Comparative%20Study%20of%20VQC%20vs.%20MLP%0AAuthor%3A%20Aueaphum%20Aueawatthanaphisut%20and%20Nyi%20Wunna%20Tun%0AAbstract%3A%20%20%20The%20comparative%20evaluation%20between%20classical%20and%20quantum%20reinforcement%0Alearning%20%28QRL%29%20paradigms%20was%20conducted%20to%20investigate%20their%20convergence%0Abehavior%2C%20robustness%20under%20observational%20noise%2C%20and%20computational%20efficiency%20in%0Aa%20benchmark%20control%20environment.%20The%20study%20employed%20a%20multilayer%20perceptron%0A%28MLP%29%20agent%20as%20a%20classical%20baseline%20and%20a%20parameterized%20variational%20quantum%0Acircuit%20%28VQC%29%20as%20a%20quantum%20counterpart%2C%20both%20trained%20on%20the%20CartPole-v1%0Aenvironment%20over%20500%20episodes.%20Empirical%20results%20demonstrated%20that%20the%0Aclassical%20MLP%20achieved%20near-optimal%20policy%20convergence%20with%20a%20mean%20return%20of%0A498.7%20%2B/-%203.2%2C%20maintaining%20stable%20equilibrium%20throughout%20training.%20In%20contrast%2C%0Athe%20VQC%20exhibited%20limited%20learning%20capability%2C%20with%20an%20average%20return%20of%2014.6%0A%2B/-%204.8%2C%20primarily%20constrained%20by%20circuit%20depth%20and%20qubit%20connectivity.%20Noise%0Arobustness%20analysis%20further%20revealed%20that%20the%20MLP%20policy%20deteriorated%0Agracefully%20under%20Gaussian%20perturbations%2C%20while%20the%20VQC%20displayed%20higher%0Asensitivity%20at%20equivalent%20noise%20levels.%20Despite%20the%20lower%20asymptotic%0Aperformance%2C%20the%20VQC%20exhibited%20significantly%20lower%20parameter%20count%20and%0Amarginally%20increased%20training%20time%2C%20highlighting%20its%20potential%20scalability%20for%0Alow-resource%20quantum%20processors.%20The%20results%20suggest%20that%20while%20classical%0Aneural%20policies%20remain%20dominant%20in%20current%20control%20benchmarks%2C%20quantum-enhanced%0Aarchitectures%20could%20offer%20promising%20efficiency%20advantages%20once%20hardware%20noise%0Aand%20expressivity%20limitations%20are%20mitigated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Quantum-Classical%2520Policy%2520Gradient%2520for%2520Adaptive%2520Control%2520of%250A%2520%2520Cyber-Physical%2520Systems%253A%2520A%2520Comparative%2520Study%2520of%2520VQC%2520vs.%2520MLP%26entry.906535625%3DAueaphum%2520Aueawatthanaphisut%2520and%2520Nyi%2520Wunna%2520Tun%26entry.1292438233%3D%2520%2520The%2520comparative%2520evaluation%2520between%2520classical%2520and%2520quantum%2520reinforcement%250Alearning%2520%2528QRL%2529%2520paradigms%2520was%2520conducted%2520to%2520investigate%2520their%2520convergence%250Abehavior%252C%2520robustness%2520under%2520observational%2520noise%252C%2520and%2520computational%2520efficiency%2520in%250Aa%2520benchmark%2520control%2520environment.%2520The%2520study%2520employed%2520a%2520multilayer%2520perceptron%250A%2528MLP%2529%2520agent%2520as%2520a%2520classical%2520baseline%2520and%2520a%2520parameterized%2520variational%2520quantum%250Acircuit%2520%2528VQC%2529%2520as%2520a%2520quantum%2520counterpart%252C%2520both%2520trained%2520on%2520the%2520CartPole-v1%250Aenvironment%2520over%2520500%2520episodes.%2520Empirical%2520results%2520demonstrated%2520that%2520the%250Aclassical%2520MLP%2520achieved%2520near-optimal%2520policy%2520convergence%2520with%2520a%2520mean%2520return%2520of%250A498.7%2520%252B/-%25203.2%252C%2520maintaining%2520stable%2520equilibrium%2520throughout%2520training.%2520In%2520contrast%252C%250Athe%2520VQC%2520exhibited%2520limited%2520learning%2520capability%252C%2520with%2520an%2520average%2520return%2520of%252014.6%250A%252B/-%25204.8%252C%2520primarily%2520constrained%2520by%2520circuit%2520depth%2520and%2520qubit%2520connectivity.%2520Noise%250Arobustness%2520analysis%2520further%2520revealed%2520that%2520the%2520MLP%2520policy%2520deteriorated%250Agracefully%2520under%2520Gaussian%2520perturbations%252C%2520while%2520the%2520VQC%2520displayed%2520higher%250Asensitivity%2520at%2520equivalent%2520noise%2520levels.%2520Despite%2520the%2520lower%2520asymptotic%250Aperformance%252C%2520the%2520VQC%2520exhibited%2520significantly%2520lower%2520parameter%2520count%2520and%250Amarginally%2520increased%2520training%2520time%252C%2520highlighting%2520its%2520potential%2520scalability%2520for%250Alow-resource%2520quantum%2520processors.%2520The%2520results%2520suggest%2520that%2520while%2520classical%250Aneural%2520policies%2520remain%2520dominant%2520in%2520current%2520control%2520benchmarks%252C%2520quantum-enhanced%250Aarchitectures%2520could%2520offer%2520promising%2520efficiency%2520advantages%2520once%2520hardware%2520noise%250Aand%2520expressivity%2520limitations%2520are%2520mitigated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Quantum-Classical%20Policy%20Gradient%20for%20Adaptive%20Control%20of%0A%20%20Cyber-Physical%20Systems%3A%20A%20Comparative%20Study%20of%20VQC%20vs.%20MLP&entry.906535625=Aueaphum%20Aueawatthanaphisut%20and%20Nyi%20Wunna%20Tun&entry.1292438233=%20%20The%20comparative%20evaluation%20between%20classical%20and%20quantum%20reinforcement%0Alearning%20%28QRL%29%20paradigms%20was%20conducted%20to%20investigate%20their%20convergence%0Abehavior%2C%20robustness%20under%20observational%20noise%2C%20and%20computational%20efficiency%20in%0Aa%20benchmark%20control%20environment.%20The%20study%20employed%20a%20multilayer%20perceptron%0A%28MLP%29%20agent%20as%20a%20classical%20baseline%20and%20a%20parameterized%20variational%20quantum%0Acircuit%20%28VQC%29%20as%20a%20quantum%20counterpart%2C%20both%20trained%20on%20the%20CartPole-v1%0Aenvironment%20over%20500%20episodes.%20Empirical%20results%20demonstrated%20that%20the%0Aclassical%20MLP%20achieved%20near-optimal%20policy%20convergence%20with%20a%20mean%20return%20of%0A498.7%20%2B/-%203.2%2C%20maintaining%20stable%20equilibrium%20throughout%20training.%20In%20contrast%2C%0Athe%20VQC%20exhibited%20limited%20learning%20capability%2C%20with%20an%20average%20return%20of%2014.6%0A%2B/-%204.8%2C%20primarily%20constrained%20by%20circuit%20depth%20and%20qubit%20connectivity.%20Noise%0Arobustness%20analysis%20further%20revealed%20that%20the%20MLP%20policy%20deteriorated%0Agracefully%20under%20Gaussian%20perturbations%2C%20while%20the%20VQC%20displayed%20higher%0Asensitivity%20at%20equivalent%20noise%20levels.%20Despite%20the%20lower%20asymptotic%0Aperformance%2C%20the%20VQC%20exhibited%20significantly%20lower%20parameter%20count%20and%0Amarginally%20increased%20training%20time%2C%20highlighting%20its%20potential%20scalability%20for%0Alow-resource%20quantum%20processors.%20The%20results%20suggest%20that%20while%20classical%0Aneural%20policies%20remain%20dominant%20in%20current%20control%20benchmarks%2C%20quantum-enhanced%0Aarchitectures%20could%20offer%20promising%20efficiency%20advantages%20once%20hardware%20noise%0Aand%20expressivity%20limitations%20are%20mitigated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06010v1&entry.124074799=Read"},
{"title": "RamPINN: Recovering Raman Spectra From Coherent Anti-Stokes Spectra\n  Using Embedded Physics", "author": "Sai Karthikeya Vemuri and Adithya Ashok Chalain Valapil and Tim B\u00fcchner and Joachim Denzler", "abstract": "  Transferring the recent advancements in deep learning into scientific\ndisciplines is hindered by the lack of the required large-scale datasets for\ntraining. We argue that in these knowledge-rich domains, the established body\nof scientific theory provides reliable inductive biases in the form of\ngoverning physical laws. We address the ill-posed inverse problem of recovering\nRaman spectra from noisy Coherent Anti-Stokes Raman Scattering (CARS)\nmeasurements, as the true Raman signal here is suppressed by a dominating\nnon-resonant background. We propose RamPINN, a model that learns to recover\nRaman spectra from given CARS spectra. Our core methodological contribution is\na physics-informed neural network that utilizes a dual-decoder architecture to\ndisentangle resonant and non-resonant signals. This is done by enforcing the\nKramers-Kronig causality relations via a differentiable Hilbert transform loss\non the resonant and a smoothness prior on the non-resonant part of the signal.\nTrained entirely on synthetic data, RamPINN demonstrates strong zero-shot\ngeneralization to real-world experimental data, explicitly closing this gap and\nsignificantly outperforming existing baselines. Furthermore, we show that\ntraining with these physics-based losses alone, without access to any\nground-truth Raman spectra, still yields competitive results. This work\nhighlights a broader concept: formal scientific rules can act as a potent\ninductive bias, enabling robust, self-supervised learning in data-limited\nscientific domains.\n", "link": "http://arxiv.org/abs/2510.06020v1", "date": "2025-10-07", "relevancy": 1.9651, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5046}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5008}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RamPINN%3A%20Recovering%20Raman%20Spectra%20From%20Coherent%20Anti-Stokes%20Spectra%0A%20%20Using%20Embedded%20Physics&body=Title%3A%20RamPINN%3A%20Recovering%20Raman%20Spectra%20From%20Coherent%20Anti-Stokes%20Spectra%0A%20%20Using%20Embedded%20Physics%0AAuthor%3A%20Sai%20Karthikeya%20Vemuri%20and%20Adithya%20Ashok%20Chalain%20Valapil%20and%20Tim%20B%C3%BCchner%20and%20Joachim%20Denzler%0AAbstract%3A%20%20%20Transferring%20the%20recent%20advancements%20in%20deep%20learning%20into%20scientific%0Adisciplines%20is%20hindered%20by%20the%20lack%20of%20the%20required%20large-scale%20datasets%20for%0Atraining.%20We%20argue%20that%20in%20these%20knowledge-rich%20domains%2C%20the%20established%20body%0Aof%20scientific%20theory%20provides%20reliable%20inductive%20biases%20in%20the%20form%20of%0Agoverning%20physical%20laws.%20We%20address%20the%20ill-posed%20inverse%20problem%20of%20recovering%0ARaman%20spectra%20from%20noisy%20Coherent%20Anti-Stokes%20Raman%20Scattering%20%28CARS%29%0Ameasurements%2C%20as%20the%20true%20Raman%20signal%20here%20is%20suppressed%20by%20a%20dominating%0Anon-resonant%20background.%20We%20propose%20RamPINN%2C%20a%20model%20that%20learns%20to%20recover%0ARaman%20spectra%20from%20given%20CARS%20spectra.%20Our%20core%20methodological%20contribution%20is%0Aa%20physics-informed%20neural%20network%20that%20utilizes%20a%20dual-decoder%20architecture%20to%0Adisentangle%20resonant%20and%20non-resonant%20signals.%20This%20is%20done%20by%20enforcing%20the%0AKramers-Kronig%20causality%20relations%20via%20a%20differentiable%20Hilbert%20transform%20loss%0Aon%20the%20resonant%20and%20a%20smoothness%20prior%20on%20the%20non-resonant%20part%20of%20the%20signal.%0ATrained%20entirely%20on%20synthetic%20data%2C%20RamPINN%20demonstrates%20strong%20zero-shot%0Ageneralization%20to%20real-world%20experimental%20data%2C%20explicitly%20closing%20this%20gap%20and%0Asignificantly%20outperforming%20existing%20baselines.%20Furthermore%2C%20we%20show%20that%0Atraining%20with%20these%20physics-based%20losses%20alone%2C%20without%20access%20to%20any%0Aground-truth%20Raman%20spectra%2C%20still%20yields%20competitive%20results.%20This%20work%0Ahighlights%20a%20broader%20concept%3A%20formal%20scientific%20rules%20can%20act%20as%20a%20potent%0Ainductive%20bias%2C%20enabling%20robust%2C%20self-supervised%20learning%20in%20data-limited%0Ascientific%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRamPINN%253A%2520Recovering%2520Raman%2520Spectra%2520From%2520Coherent%2520Anti-Stokes%2520Spectra%250A%2520%2520Using%2520Embedded%2520Physics%26entry.906535625%3DSai%2520Karthikeya%2520Vemuri%2520and%2520Adithya%2520Ashok%2520Chalain%2520Valapil%2520and%2520Tim%2520B%25C3%25BCchner%2520and%2520Joachim%2520Denzler%26entry.1292438233%3D%2520%2520Transferring%2520the%2520recent%2520advancements%2520in%2520deep%2520learning%2520into%2520scientific%250Adisciplines%2520is%2520hindered%2520by%2520the%2520lack%2520of%2520the%2520required%2520large-scale%2520datasets%2520for%250Atraining.%2520We%2520argue%2520that%2520in%2520these%2520knowledge-rich%2520domains%252C%2520the%2520established%2520body%250Aof%2520scientific%2520theory%2520provides%2520reliable%2520inductive%2520biases%2520in%2520the%2520form%2520of%250Agoverning%2520physical%2520laws.%2520We%2520address%2520the%2520ill-posed%2520inverse%2520problem%2520of%2520recovering%250ARaman%2520spectra%2520from%2520noisy%2520Coherent%2520Anti-Stokes%2520Raman%2520Scattering%2520%2528CARS%2529%250Ameasurements%252C%2520as%2520the%2520true%2520Raman%2520signal%2520here%2520is%2520suppressed%2520by%2520a%2520dominating%250Anon-resonant%2520background.%2520We%2520propose%2520RamPINN%252C%2520a%2520model%2520that%2520learns%2520to%2520recover%250ARaman%2520spectra%2520from%2520given%2520CARS%2520spectra.%2520Our%2520core%2520methodological%2520contribution%2520is%250Aa%2520physics-informed%2520neural%2520network%2520that%2520utilizes%2520a%2520dual-decoder%2520architecture%2520to%250Adisentangle%2520resonant%2520and%2520non-resonant%2520signals.%2520This%2520is%2520done%2520by%2520enforcing%2520the%250AKramers-Kronig%2520causality%2520relations%2520via%2520a%2520differentiable%2520Hilbert%2520transform%2520loss%250Aon%2520the%2520resonant%2520and%2520a%2520smoothness%2520prior%2520on%2520the%2520non-resonant%2520part%2520of%2520the%2520signal.%250ATrained%2520entirely%2520on%2520synthetic%2520data%252C%2520RamPINN%2520demonstrates%2520strong%2520zero-shot%250Ageneralization%2520to%2520real-world%2520experimental%2520data%252C%2520explicitly%2520closing%2520this%2520gap%2520and%250Asignificantly%2520outperforming%2520existing%2520baselines.%2520Furthermore%252C%2520we%2520show%2520that%250Atraining%2520with%2520these%2520physics-based%2520losses%2520alone%252C%2520without%2520access%2520to%2520any%250Aground-truth%2520Raman%2520spectra%252C%2520still%2520yields%2520competitive%2520results.%2520This%2520work%250Ahighlights%2520a%2520broader%2520concept%253A%2520formal%2520scientific%2520rules%2520can%2520act%2520as%2520a%2520potent%250Ainductive%2520bias%252C%2520enabling%2520robust%252C%2520self-supervised%2520learning%2520in%2520data-limited%250Ascientific%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RamPINN%3A%20Recovering%20Raman%20Spectra%20From%20Coherent%20Anti-Stokes%20Spectra%0A%20%20Using%20Embedded%20Physics&entry.906535625=Sai%20Karthikeya%20Vemuri%20and%20Adithya%20Ashok%20Chalain%20Valapil%20and%20Tim%20B%C3%BCchner%20and%20Joachim%20Denzler&entry.1292438233=%20%20Transferring%20the%20recent%20advancements%20in%20deep%20learning%20into%20scientific%0Adisciplines%20is%20hindered%20by%20the%20lack%20of%20the%20required%20large-scale%20datasets%20for%0Atraining.%20We%20argue%20that%20in%20these%20knowledge-rich%20domains%2C%20the%20established%20body%0Aof%20scientific%20theory%20provides%20reliable%20inductive%20biases%20in%20the%20form%20of%0Agoverning%20physical%20laws.%20We%20address%20the%20ill-posed%20inverse%20problem%20of%20recovering%0ARaman%20spectra%20from%20noisy%20Coherent%20Anti-Stokes%20Raman%20Scattering%20%28CARS%29%0Ameasurements%2C%20as%20the%20true%20Raman%20signal%20here%20is%20suppressed%20by%20a%20dominating%0Anon-resonant%20background.%20We%20propose%20RamPINN%2C%20a%20model%20that%20learns%20to%20recover%0ARaman%20spectra%20from%20given%20CARS%20spectra.%20Our%20core%20methodological%20contribution%20is%0Aa%20physics-informed%20neural%20network%20that%20utilizes%20a%20dual-decoder%20architecture%20to%0Adisentangle%20resonant%20and%20non-resonant%20signals.%20This%20is%20done%20by%20enforcing%20the%0AKramers-Kronig%20causality%20relations%20via%20a%20differentiable%20Hilbert%20transform%20loss%0Aon%20the%20resonant%20and%20a%20smoothness%20prior%20on%20the%20non-resonant%20part%20of%20the%20signal.%0ATrained%20entirely%20on%20synthetic%20data%2C%20RamPINN%20demonstrates%20strong%20zero-shot%0Ageneralization%20to%20real-world%20experimental%20data%2C%20explicitly%20closing%20this%20gap%20and%0Asignificantly%20outperforming%20existing%20baselines.%20Furthermore%2C%20we%20show%20that%0Atraining%20with%20these%20physics-based%20losses%20alone%2C%20without%20access%20to%20any%0Aground-truth%20Raman%20spectra%2C%20still%20yields%20competitive%20results.%20This%20work%0Ahighlights%20a%20broader%20concept%3A%20formal%20scientific%20rules%20can%20act%20as%20a%20potent%0Ainductive%20bias%2C%20enabling%20robust%2C%20self-supervised%20learning%20in%20data-limited%0Ascientific%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06020v1&entry.124074799=Read"},
{"title": "Training Dynamics Impact Post-Training Quantization Robustness", "author": "Albert Catalan-Tatjer and Niccol\u00f2 Ajroldi and Jonas Geiping", "abstract": "  While post-training quantization is widely adopted for efficient deployment\nof large language models, the mechanisms underlying quantization robustness\nremain unclear. We conduct a comprehensive analysis of quantization degradation\nacross open-source language model training trajectories up to 32B parameters\nand 15T training tokens to accurately assess the relationship between training\ndynamics and quantization performance. Our key finding is that quantization\nerrors in large-scale training runs are driven by a complex interplay between\nlearning rate and other training hyperparameters. Specifically, once learning\nrates decay, validation loss and quantization error diverge, largely\nindependent of training data scale. To investigate interventions on the\ntraining dynamics and identify specific configurations that can modulate\nquantization robustness favorably, we train our own models in controlled\nexperiments up to 100B tokens. Our results challenge the assumption that\nincreasing dataset scale inherently compromises quantization effectiveness,\ndemonstrating instead that strategic training hyperparameter interventions can\nimprove quantization quality at scale.\n", "link": "http://arxiv.org/abs/2510.06213v1", "date": "2025-10-07", "relevancy": 1.5127, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5123}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.507}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Dynamics%20Impact%20Post-Training%20Quantization%20Robustness&body=Title%3A%20Training%20Dynamics%20Impact%20Post-Training%20Quantization%20Robustness%0AAuthor%3A%20Albert%20Catalan-Tatjer%20and%20Niccol%C3%B2%20Ajroldi%20and%20Jonas%20Geiping%0AAbstract%3A%20%20%20While%20post-training%20quantization%20is%20widely%20adopted%20for%20efficient%20deployment%0Aof%20large%20language%20models%2C%20the%20mechanisms%20underlying%20quantization%20robustness%0Aremain%20unclear.%20We%20conduct%20a%20comprehensive%20analysis%20of%20quantization%20degradation%0Aacross%20open-source%20language%20model%20training%20trajectories%20up%20to%2032B%20parameters%0Aand%2015T%20training%20tokens%20to%20accurately%20assess%20the%20relationship%20between%20training%0Adynamics%20and%20quantization%20performance.%20Our%20key%20finding%20is%20that%20quantization%0Aerrors%20in%20large-scale%20training%20runs%20are%20driven%20by%20a%20complex%20interplay%20between%0Alearning%20rate%20and%20other%20training%20hyperparameters.%20Specifically%2C%20once%20learning%0Arates%20decay%2C%20validation%20loss%20and%20quantization%20error%20diverge%2C%20largely%0Aindependent%20of%20training%20data%20scale.%20To%20investigate%20interventions%20on%20the%0Atraining%20dynamics%20and%20identify%20specific%20configurations%20that%20can%20modulate%0Aquantization%20robustness%20favorably%2C%20we%20train%20our%20own%20models%20in%20controlled%0Aexperiments%20up%20to%20100B%20tokens.%20Our%20results%20challenge%20the%20assumption%20that%0Aincreasing%20dataset%20scale%20inherently%20compromises%20quantization%20effectiveness%2C%0Ademonstrating%20instead%20that%20strategic%20training%20hyperparameter%20interventions%20can%0Aimprove%20quantization%20quality%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Dynamics%2520Impact%2520Post-Training%2520Quantization%2520Robustness%26entry.906535625%3DAlbert%2520Catalan-Tatjer%2520and%2520Niccol%25C3%25B2%2520Ajroldi%2520and%2520Jonas%2520Geiping%26entry.1292438233%3D%2520%2520While%2520post-training%2520quantization%2520is%2520widely%2520adopted%2520for%2520efficient%2520deployment%250Aof%2520large%2520language%2520models%252C%2520the%2520mechanisms%2520underlying%2520quantization%2520robustness%250Aremain%2520unclear.%2520We%2520conduct%2520a%2520comprehensive%2520analysis%2520of%2520quantization%2520degradation%250Aacross%2520open-source%2520language%2520model%2520training%2520trajectories%2520up%2520to%252032B%2520parameters%250Aand%252015T%2520training%2520tokens%2520to%2520accurately%2520assess%2520the%2520relationship%2520between%2520training%250Adynamics%2520and%2520quantization%2520performance.%2520Our%2520key%2520finding%2520is%2520that%2520quantization%250Aerrors%2520in%2520large-scale%2520training%2520runs%2520are%2520driven%2520by%2520a%2520complex%2520interplay%2520between%250Alearning%2520rate%2520and%2520other%2520training%2520hyperparameters.%2520Specifically%252C%2520once%2520learning%250Arates%2520decay%252C%2520validation%2520loss%2520and%2520quantization%2520error%2520diverge%252C%2520largely%250Aindependent%2520of%2520training%2520data%2520scale.%2520To%2520investigate%2520interventions%2520on%2520the%250Atraining%2520dynamics%2520and%2520identify%2520specific%2520configurations%2520that%2520can%2520modulate%250Aquantization%2520robustness%2520favorably%252C%2520we%2520train%2520our%2520own%2520models%2520in%2520controlled%250Aexperiments%2520up%2520to%2520100B%2520tokens.%2520Our%2520results%2520challenge%2520the%2520assumption%2520that%250Aincreasing%2520dataset%2520scale%2520inherently%2520compromises%2520quantization%2520effectiveness%252C%250Ademonstrating%2520instead%2520that%2520strategic%2520training%2520hyperparameter%2520interventions%2520can%250Aimprove%2520quantization%2520quality%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Dynamics%20Impact%20Post-Training%20Quantization%20Robustness&entry.906535625=Albert%20Catalan-Tatjer%20and%20Niccol%C3%B2%20Ajroldi%20and%20Jonas%20Geiping&entry.1292438233=%20%20While%20post-training%20quantization%20is%20widely%20adopted%20for%20efficient%20deployment%0Aof%20large%20language%20models%2C%20the%20mechanisms%20underlying%20quantization%20robustness%0Aremain%20unclear.%20We%20conduct%20a%20comprehensive%20analysis%20of%20quantization%20degradation%0Aacross%20open-source%20language%20model%20training%20trajectories%20up%20to%2032B%20parameters%0Aand%2015T%20training%20tokens%20to%20accurately%20assess%20the%20relationship%20between%20training%0Adynamics%20and%20quantization%20performance.%20Our%20key%20finding%20is%20that%20quantization%0Aerrors%20in%20large-scale%20training%20runs%20are%20driven%20by%20a%20complex%20interplay%20between%0Alearning%20rate%20and%20other%20training%20hyperparameters.%20Specifically%2C%20once%20learning%0Arates%20decay%2C%20validation%20loss%20and%20quantization%20error%20diverge%2C%20largely%0Aindependent%20of%20training%20data%20scale.%20To%20investigate%20interventions%20on%20the%0Atraining%20dynamics%20and%20identify%20specific%20configurations%20that%20can%20modulate%0Aquantization%20robustness%20favorably%2C%20we%20train%20our%20own%20models%20in%20controlled%0Aexperiments%20up%20to%20100B%20tokens.%20Our%20results%20challenge%20the%20assumption%20that%0Aincreasing%20dataset%20scale%20inherently%20compromises%20quantization%20effectiveness%2C%0Ademonstrating%20instead%20that%20strategic%20training%20hyperparameter%20interventions%20can%0Aimprove%20quantization%20quality%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06213v1&entry.124074799=Read"},
{"title": "InforME: Improving Informativeness of Abstractive Text Summarization\n  With Informative Attention Guided by Named Entity Salience", "author": "Jianbin Shen and Christy Jie Liang and Junyu Xuan", "abstract": "  Abstractive text summarization is integral to the Big Data era, which demands\nadvanced methods to turn voluminous and often long text data into concise but\ncoherent and informative summaries for efficient human consumption. Despite\nsignificant progress, there is still room for improvement in various aspects.\nOne such aspect is to improve informativeness. Hence, this paper proposes a\nnovel learning approach consisting of two methods: an optimal transport-based\ninformative attention method to improve learning focal information in reference\nsummaries and an accumulative joint entropy reduction method on named entities\nto enhance informative salience. Experiment results show that our approach\nachieves better ROUGE scores compared to prior work on CNN/Daily Mail while\nhaving competitive results on XSum. Human evaluation of informativeness also\ndemonstrates the better performance of our approach over a strong baseline.\nFurther analysis gives insight into the plausible reasons underlying the\nevaluation results.\n", "link": "http://arxiv.org/abs/2510.05769v1", "date": "2025-10-07", "relevancy": 1.9833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InforME%3A%20Improving%20Informativeness%20of%20Abstractive%20Text%20Summarization%0A%20%20With%20Informative%20Attention%20Guided%20by%20Named%20Entity%20Salience&body=Title%3A%20InforME%3A%20Improving%20Informativeness%20of%20Abstractive%20Text%20Summarization%0A%20%20With%20Informative%20Attention%20Guided%20by%20Named%20Entity%20Salience%0AAuthor%3A%20Jianbin%20Shen%20and%20Christy%20Jie%20Liang%20and%20Junyu%20Xuan%0AAbstract%3A%20%20%20Abstractive%20text%20summarization%20is%20integral%20to%20the%20Big%20Data%20era%2C%20which%20demands%0Aadvanced%20methods%20to%20turn%20voluminous%20and%20often%20long%20text%20data%20into%20concise%20but%0Acoherent%20and%20informative%20summaries%20for%20efficient%20human%20consumption.%20Despite%0Asignificant%20progress%2C%20there%20is%20still%20room%20for%20improvement%20in%20various%20aspects.%0AOne%20such%20aspect%20is%20to%20improve%20informativeness.%20Hence%2C%20this%20paper%20proposes%20a%0Anovel%20learning%20approach%20consisting%20of%20two%20methods%3A%20an%20optimal%20transport-based%0Ainformative%20attention%20method%20to%20improve%20learning%20focal%20information%20in%20reference%0Asummaries%20and%20an%20accumulative%20joint%20entropy%20reduction%20method%20on%20named%20entities%0Ato%20enhance%20informative%20salience.%20Experiment%20results%20show%20that%20our%20approach%0Aachieves%20better%20ROUGE%20scores%20compared%20to%20prior%20work%20on%20CNN/Daily%20Mail%20while%0Ahaving%20competitive%20results%20on%20XSum.%20Human%20evaluation%20of%20informativeness%20also%0Ademonstrates%20the%20better%20performance%20of%20our%20approach%20over%20a%20strong%20baseline.%0AFurther%20analysis%20gives%20insight%20into%20the%20plausible%20reasons%20underlying%20the%0Aevaluation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInforME%253A%2520Improving%2520Informativeness%2520of%2520Abstractive%2520Text%2520Summarization%250A%2520%2520With%2520Informative%2520Attention%2520Guided%2520by%2520Named%2520Entity%2520Salience%26entry.906535625%3DJianbin%2520Shen%2520and%2520Christy%2520Jie%2520Liang%2520and%2520Junyu%2520Xuan%26entry.1292438233%3D%2520%2520Abstractive%2520text%2520summarization%2520is%2520integral%2520to%2520the%2520Big%2520Data%2520era%252C%2520which%2520demands%250Aadvanced%2520methods%2520to%2520turn%2520voluminous%2520and%2520often%2520long%2520text%2520data%2520into%2520concise%2520but%250Acoherent%2520and%2520informative%2520summaries%2520for%2520efficient%2520human%2520consumption.%2520Despite%250Asignificant%2520progress%252C%2520there%2520is%2520still%2520room%2520for%2520improvement%2520in%2520various%2520aspects.%250AOne%2520such%2520aspect%2520is%2520to%2520improve%2520informativeness.%2520Hence%252C%2520this%2520paper%2520proposes%2520a%250Anovel%2520learning%2520approach%2520consisting%2520of%2520two%2520methods%253A%2520an%2520optimal%2520transport-based%250Ainformative%2520attention%2520method%2520to%2520improve%2520learning%2520focal%2520information%2520in%2520reference%250Asummaries%2520and%2520an%2520accumulative%2520joint%2520entropy%2520reduction%2520method%2520on%2520named%2520entities%250Ato%2520enhance%2520informative%2520salience.%2520Experiment%2520results%2520show%2520that%2520our%2520approach%250Aachieves%2520better%2520ROUGE%2520scores%2520compared%2520to%2520prior%2520work%2520on%2520CNN/Daily%2520Mail%2520while%250Ahaving%2520competitive%2520results%2520on%2520XSum.%2520Human%2520evaluation%2520of%2520informativeness%2520also%250Ademonstrates%2520the%2520better%2520performance%2520of%2520our%2520approach%2520over%2520a%2520strong%2520baseline.%250AFurther%2520analysis%2520gives%2520insight%2520into%2520the%2520plausible%2520reasons%2520underlying%2520the%250Aevaluation%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InforME%3A%20Improving%20Informativeness%20of%20Abstractive%20Text%20Summarization%0A%20%20With%20Informative%20Attention%20Guided%20by%20Named%20Entity%20Salience&entry.906535625=Jianbin%20Shen%20and%20Christy%20Jie%20Liang%20and%20Junyu%20Xuan&entry.1292438233=%20%20Abstractive%20text%20summarization%20is%20integral%20to%20the%20Big%20Data%20era%2C%20which%20demands%0Aadvanced%20methods%20to%20turn%20voluminous%20and%20often%20long%20text%20data%20into%20concise%20but%0Acoherent%20and%20informative%20summaries%20for%20efficient%20human%20consumption.%20Despite%0Asignificant%20progress%2C%20there%20is%20still%20room%20for%20improvement%20in%20various%20aspects.%0AOne%20such%20aspect%20is%20to%20improve%20informativeness.%20Hence%2C%20this%20paper%20proposes%20a%0Anovel%20learning%20approach%20consisting%20of%20two%20methods%3A%20an%20optimal%20transport-based%0Ainformative%20attention%20method%20to%20improve%20learning%20focal%20information%20in%20reference%0Asummaries%20and%20an%20accumulative%20joint%20entropy%20reduction%20method%20on%20named%20entities%0Ato%20enhance%20informative%20salience.%20Experiment%20results%20show%20that%20our%20approach%0Aachieves%20better%20ROUGE%20scores%20compared%20to%20prior%20work%20on%20CNN/Daily%20Mail%20while%0Ahaving%20competitive%20results%20on%20XSum.%20Human%20evaluation%20of%20informativeness%20also%0Ademonstrates%20the%20better%20performance%20of%20our%20approach%20over%20a%20strong%20baseline.%0AFurther%20analysis%20gives%20insight%20into%20the%20plausible%20reasons%20underlying%20the%0Aevaluation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05769v1&entry.124074799=Read"},
{"title": "Constrained free energy minimization for the design of thermal states\n  and stabilizer thermodynamic systems", "author": "Michele Minervini and Madison Chin and Jacob Kupperman and Nana Liu and Ivy Luo and Meghan Ly and Soorya Rethinasamy and Kathie Wang and Mark M. Wilde", "abstract": "  A quantum thermodynamic system is described by a Hamiltonian and a list of\nconserved, non-commuting charges, and a fundamental goal is to determine the\nminimum energy of the system subject to constraints on the charges. Recently,\n[Liu et al., arXiv:2505.04514] proposed first- and second-order classical and\nhybrid quantum-classical algorithms for solving a dual chemical potential\nmaximization problem, and they proved that these algorithms converge to global\noptima by means of gradient-ascent approaches. In this paper, we benchmark\nthese algorithms on several problems of interest in thermodynamics, including\none- and two-dimensional quantum Heisenberg models with nearest and\nnext-to-nearest neighbor interactions and with the charges set to the total x,\ny, and z magnetizations. We also offer an alternative compelling interpretation\nof these algorithms as methods for designing ground and thermal states of\ncontrollable Hamiltonians, with potential applications in molecular and\nmaterial design. Furthermore, we introduce stabilizer thermodynamic systems as\nthermodynamic systems based on stabilizer codes, with the Hamiltonian\nconstructed from a given code's stabilizer operators and the charges\nconstructed from the code's logical operators. We benchmark the aforementioned\nalgorithms on several examples of stabilizer thermodynamic systems, including\nthose constructed from the one-to-three-qubit repetition code, the perfect\none-to-five-qubit code, and the two-to-four-qubit error-detecting code.\nFinally, we observe that the aforementioned hybrid quantum-classical\nalgorithms, when applied to stabilizer thermodynamic systems, can serve as\nalternative methods for encoding qubits into stabilizer codes at a fixed\ntemperature, and we provide an effective method for warm-starting these\nencoding algorithms whenever a single qubit is encoded into multiple physical\nqubits.\n", "link": "http://arxiv.org/abs/2508.09103v2", "date": "2025-10-07", "relevancy": 0.7808, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3924}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3908}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20free%20energy%20minimization%20for%20the%20design%20of%20thermal%20states%0A%20%20and%20stabilizer%20thermodynamic%20systems&body=Title%3A%20Constrained%20free%20energy%20minimization%20for%20the%20design%20of%20thermal%20states%0A%20%20and%20stabilizer%20thermodynamic%20systems%0AAuthor%3A%20Michele%20Minervini%20and%20Madison%20Chin%20and%20Jacob%20Kupperman%20and%20Nana%20Liu%20and%20Ivy%20Luo%20and%20Meghan%20Ly%20and%20Soorya%20Rethinasamy%20and%20Kathie%20Wang%20and%20Mark%20M.%20Wilde%0AAbstract%3A%20%20%20A%20quantum%20thermodynamic%20system%20is%20described%20by%20a%20Hamiltonian%20and%20a%20list%20of%0Aconserved%2C%20non-commuting%20charges%2C%20and%20a%20fundamental%20goal%20is%20to%20determine%20the%0Aminimum%20energy%20of%20the%20system%20subject%20to%20constraints%20on%20the%20charges.%20Recently%2C%0A%5BLiu%20et%20al.%2C%20arXiv%3A2505.04514%5D%20proposed%20first-%20and%20second-order%20classical%20and%0Ahybrid%20quantum-classical%20algorithms%20for%20solving%20a%20dual%20chemical%20potential%0Amaximization%20problem%2C%20and%20they%20proved%20that%20these%20algorithms%20converge%20to%20global%0Aoptima%20by%20means%20of%20gradient-ascent%20approaches.%20In%20this%20paper%2C%20we%20benchmark%0Athese%20algorithms%20on%20several%20problems%20of%20interest%20in%20thermodynamics%2C%20including%0Aone-%20and%20two-dimensional%20quantum%20Heisenberg%20models%20with%20nearest%20and%0Anext-to-nearest%20neighbor%20interactions%20and%20with%20the%20charges%20set%20to%20the%20total%20x%2C%0Ay%2C%20and%20z%20magnetizations.%20We%20also%20offer%20an%20alternative%20compelling%20interpretation%0Aof%20these%20algorithms%20as%20methods%20for%20designing%20ground%20and%20thermal%20states%20of%0Acontrollable%20Hamiltonians%2C%20with%20potential%20applications%20in%20molecular%20and%0Amaterial%20design.%20Furthermore%2C%20we%20introduce%20stabilizer%20thermodynamic%20systems%20as%0Athermodynamic%20systems%20based%20on%20stabilizer%20codes%2C%20with%20the%20Hamiltonian%0Aconstructed%20from%20a%20given%20code%27s%20stabilizer%20operators%20and%20the%20charges%0Aconstructed%20from%20the%20code%27s%20logical%20operators.%20We%20benchmark%20the%20aforementioned%0Aalgorithms%20on%20several%20examples%20of%20stabilizer%20thermodynamic%20systems%2C%20including%0Athose%20constructed%20from%20the%20one-to-three-qubit%20repetition%20code%2C%20the%20perfect%0Aone-to-five-qubit%20code%2C%20and%20the%20two-to-four-qubit%20error-detecting%20code.%0AFinally%2C%20we%20observe%20that%20the%20aforementioned%20hybrid%20quantum-classical%0Aalgorithms%2C%20when%20applied%20to%20stabilizer%20thermodynamic%20systems%2C%20can%20serve%20as%0Aalternative%20methods%20for%20encoding%20qubits%20into%20stabilizer%20codes%20at%20a%20fixed%0Atemperature%2C%20and%20we%20provide%20an%20effective%20method%20for%20warm-starting%20these%0Aencoding%20algorithms%20whenever%20a%20single%20qubit%20is%20encoded%20into%20multiple%20physical%0Aqubits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520free%2520energy%2520minimization%2520for%2520the%2520design%2520of%2520thermal%2520states%250A%2520%2520and%2520stabilizer%2520thermodynamic%2520systems%26entry.906535625%3DMichele%2520Minervini%2520and%2520Madison%2520Chin%2520and%2520Jacob%2520Kupperman%2520and%2520Nana%2520Liu%2520and%2520Ivy%2520Luo%2520and%2520Meghan%2520Ly%2520and%2520Soorya%2520Rethinasamy%2520and%2520Kathie%2520Wang%2520and%2520Mark%2520M.%2520Wilde%26entry.1292438233%3D%2520%2520A%2520quantum%2520thermodynamic%2520system%2520is%2520described%2520by%2520a%2520Hamiltonian%2520and%2520a%2520list%2520of%250Aconserved%252C%2520non-commuting%2520charges%252C%2520and%2520a%2520fundamental%2520goal%2520is%2520to%2520determine%2520the%250Aminimum%2520energy%2520of%2520the%2520system%2520subject%2520to%2520constraints%2520on%2520the%2520charges.%2520Recently%252C%250A%255BLiu%2520et%2520al.%252C%2520arXiv%253A2505.04514%255D%2520proposed%2520first-%2520and%2520second-order%2520classical%2520and%250Ahybrid%2520quantum-classical%2520algorithms%2520for%2520solving%2520a%2520dual%2520chemical%2520potential%250Amaximization%2520problem%252C%2520and%2520they%2520proved%2520that%2520these%2520algorithms%2520converge%2520to%2520global%250Aoptima%2520by%2520means%2520of%2520gradient-ascent%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520benchmark%250Athese%2520algorithms%2520on%2520several%2520problems%2520of%2520interest%2520in%2520thermodynamics%252C%2520including%250Aone-%2520and%2520two-dimensional%2520quantum%2520Heisenberg%2520models%2520with%2520nearest%2520and%250Anext-to-nearest%2520neighbor%2520interactions%2520and%2520with%2520the%2520charges%2520set%2520to%2520the%2520total%2520x%252C%250Ay%252C%2520and%2520z%2520magnetizations.%2520We%2520also%2520offer%2520an%2520alternative%2520compelling%2520interpretation%250Aof%2520these%2520algorithms%2520as%2520methods%2520for%2520designing%2520ground%2520and%2520thermal%2520states%2520of%250Acontrollable%2520Hamiltonians%252C%2520with%2520potential%2520applications%2520in%2520molecular%2520and%250Amaterial%2520design.%2520Furthermore%252C%2520we%2520introduce%2520stabilizer%2520thermodynamic%2520systems%2520as%250Athermodynamic%2520systems%2520based%2520on%2520stabilizer%2520codes%252C%2520with%2520the%2520Hamiltonian%250Aconstructed%2520from%2520a%2520given%2520code%2527s%2520stabilizer%2520operators%2520and%2520the%2520charges%250Aconstructed%2520from%2520the%2520code%2527s%2520logical%2520operators.%2520We%2520benchmark%2520the%2520aforementioned%250Aalgorithms%2520on%2520several%2520examples%2520of%2520stabilizer%2520thermodynamic%2520systems%252C%2520including%250Athose%2520constructed%2520from%2520the%2520one-to-three-qubit%2520repetition%2520code%252C%2520the%2520perfect%250Aone-to-five-qubit%2520code%252C%2520and%2520the%2520two-to-four-qubit%2520error-detecting%2520code.%250AFinally%252C%2520we%2520observe%2520that%2520the%2520aforementioned%2520hybrid%2520quantum-classical%250Aalgorithms%252C%2520when%2520applied%2520to%2520stabilizer%2520thermodynamic%2520systems%252C%2520can%2520serve%2520as%250Aalternative%2520methods%2520for%2520encoding%2520qubits%2520into%2520stabilizer%2520codes%2520at%2520a%2520fixed%250Atemperature%252C%2520and%2520we%2520provide%2520an%2520effective%2520method%2520for%2520warm-starting%2520these%250Aencoding%2520algorithms%2520whenever%2520a%2520single%2520qubit%2520is%2520encoded%2520into%2520multiple%2520physical%250Aqubits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20free%20energy%20minimization%20for%20the%20design%20of%20thermal%20states%0A%20%20and%20stabilizer%20thermodynamic%20systems&entry.906535625=Michele%20Minervini%20and%20Madison%20Chin%20and%20Jacob%20Kupperman%20and%20Nana%20Liu%20and%20Ivy%20Luo%20and%20Meghan%20Ly%20and%20Soorya%20Rethinasamy%20and%20Kathie%20Wang%20and%20Mark%20M.%20Wilde&entry.1292438233=%20%20A%20quantum%20thermodynamic%20system%20is%20described%20by%20a%20Hamiltonian%20and%20a%20list%20of%0Aconserved%2C%20non-commuting%20charges%2C%20and%20a%20fundamental%20goal%20is%20to%20determine%20the%0Aminimum%20energy%20of%20the%20system%20subject%20to%20constraints%20on%20the%20charges.%20Recently%2C%0A%5BLiu%20et%20al.%2C%20arXiv%3A2505.04514%5D%20proposed%20first-%20and%20second-order%20classical%20and%0Ahybrid%20quantum-classical%20algorithms%20for%20solving%20a%20dual%20chemical%20potential%0Amaximization%20problem%2C%20and%20they%20proved%20that%20these%20algorithms%20converge%20to%20global%0Aoptima%20by%20means%20of%20gradient-ascent%20approaches.%20In%20this%20paper%2C%20we%20benchmark%0Athese%20algorithms%20on%20several%20problems%20of%20interest%20in%20thermodynamics%2C%20including%0Aone-%20and%20two-dimensional%20quantum%20Heisenberg%20models%20with%20nearest%20and%0Anext-to-nearest%20neighbor%20interactions%20and%20with%20the%20charges%20set%20to%20the%20total%20x%2C%0Ay%2C%20and%20z%20magnetizations.%20We%20also%20offer%20an%20alternative%20compelling%20interpretation%0Aof%20these%20algorithms%20as%20methods%20for%20designing%20ground%20and%20thermal%20states%20of%0Acontrollable%20Hamiltonians%2C%20with%20potential%20applications%20in%20molecular%20and%0Amaterial%20design.%20Furthermore%2C%20we%20introduce%20stabilizer%20thermodynamic%20systems%20as%0Athermodynamic%20systems%20based%20on%20stabilizer%20codes%2C%20with%20the%20Hamiltonian%0Aconstructed%20from%20a%20given%20code%27s%20stabilizer%20operators%20and%20the%20charges%0Aconstructed%20from%20the%20code%27s%20logical%20operators.%20We%20benchmark%20the%20aforementioned%0Aalgorithms%20on%20several%20examples%20of%20stabilizer%20thermodynamic%20systems%2C%20including%0Athose%20constructed%20from%20the%20one-to-three-qubit%20repetition%20code%2C%20the%20perfect%0Aone-to-five-qubit%20code%2C%20and%20the%20two-to-four-qubit%20error-detecting%20code.%0AFinally%2C%20we%20observe%20that%20the%20aforementioned%20hybrid%20quantum-classical%0Aalgorithms%2C%20when%20applied%20to%20stabilizer%20thermodynamic%20systems%2C%20can%20serve%20as%0Aalternative%20methods%20for%20encoding%20qubits%20into%20stabilizer%20codes%20at%20a%20fixed%0Atemperature%2C%20and%20we%20provide%20an%20effective%20method%20for%20warm-starting%20these%0Aencoding%20algorithms%20whenever%20a%20single%20qubit%20is%20encoded%20into%20multiple%20physical%0Aqubits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09103v2&entry.124074799=Read"},
{"title": "ESS-Flow: Training-free guidance of flow-based models as inference in\n  source space", "author": "Adhithyan Kalaivanan and Zheng Zhao and Jens Sj\u00f6lund and Fredrik Lindsten", "abstract": "  Guiding pretrained flow-based generative models for conditional generation or\nto produce samples with desired target properties enables solving diverse tasks\nwithout retraining on paired data. We present ESS-Flow, a gradient-free method\nthat leverages the typically Gaussian prior of the source distribution in\nflow-based models to perform Bayesian inference directly in the source space\nusing Elliptical Slice Sampling. ESS-Flow only requires forward passes through\nthe generative model and observation process, no gradient or Jacobian\ncomputations, and is applicable even when gradients are unreliable or\nunavailable, such as with simulation-based observations or quantization in the\ngeneration or observation process. We demonstrate its effectiveness on\ndesigning materials with desired target properties and predicting protein\nstructures from sparse inter-residue distance measurements.\n", "link": "http://arxiv.org/abs/2510.05849v1", "date": "2025-10-07", "relevancy": 1.5876, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5908}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5183}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESS-Flow%3A%20Training-free%20guidance%20of%20flow-based%20models%20as%20inference%20in%0A%20%20source%20space&body=Title%3A%20ESS-Flow%3A%20Training-free%20guidance%20of%20flow-based%20models%20as%20inference%20in%0A%20%20source%20space%0AAuthor%3A%20Adhithyan%20Kalaivanan%20and%20Zheng%20Zhao%20and%20Jens%20Sj%C3%B6lund%20and%20Fredrik%20Lindsten%0AAbstract%3A%20%20%20Guiding%20pretrained%20flow-based%20generative%20models%20for%20conditional%20generation%20or%0Ato%20produce%20samples%20with%20desired%20target%20properties%20enables%20solving%20diverse%20tasks%0Awithout%20retraining%20on%20paired%20data.%20We%20present%20ESS-Flow%2C%20a%20gradient-free%20method%0Athat%20leverages%20the%20typically%20Gaussian%20prior%20of%20the%20source%20distribution%20in%0Aflow-based%20models%20to%20perform%20Bayesian%20inference%20directly%20in%20the%20source%20space%0Ausing%20Elliptical%20Slice%20Sampling.%20ESS-Flow%20only%20requires%20forward%20passes%20through%0Athe%20generative%20model%20and%20observation%20process%2C%20no%20gradient%20or%20Jacobian%0Acomputations%2C%20and%20is%20applicable%20even%20when%20gradients%20are%20unreliable%20or%0Aunavailable%2C%20such%20as%20with%20simulation-based%20observations%20or%20quantization%20in%20the%0Ageneration%20or%20observation%20process.%20We%20demonstrate%20its%20effectiveness%20on%0Adesigning%20materials%20with%20desired%20target%20properties%20and%20predicting%20protein%0Astructures%20from%20sparse%20inter-residue%20distance%20measurements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESS-Flow%253A%2520Training-free%2520guidance%2520of%2520flow-based%2520models%2520as%2520inference%2520in%250A%2520%2520source%2520space%26entry.906535625%3DAdhithyan%2520Kalaivanan%2520and%2520Zheng%2520Zhao%2520and%2520Jens%2520Sj%25C3%25B6lund%2520and%2520Fredrik%2520Lindsten%26entry.1292438233%3D%2520%2520Guiding%2520pretrained%2520flow-based%2520generative%2520models%2520for%2520conditional%2520generation%2520or%250Ato%2520produce%2520samples%2520with%2520desired%2520target%2520properties%2520enables%2520solving%2520diverse%2520tasks%250Awithout%2520retraining%2520on%2520paired%2520data.%2520We%2520present%2520ESS-Flow%252C%2520a%2520gradient-free%2520method%250Athat%2520leverages%2520the%2520typically%2520Gaussian%2520prior%2520of%2520the%2520source%2520distribution%2520in%250Aflow-based%2520models%2520to%2520perform%2520Bayesian%2520inference%2520directly%2520in%2520the%2520source%2520space%250Ausing%2520Elliptical%2520Slice%2520Sampling.%2520ESS-Flow%2520only%2520requires%2520forward%2520passes%2520through%250Athe%2520generative%2520model%2520and%2520observation%2520process%252C%2520no%2520gradient%2520or%2520Jacobian%250Acomputations%252C%2520and%2520is%2520applicable%2520even%2520when%2520gradients%2520are%2520unreliable%2520or%250Aunavailable%252C%2520such%2520as%2520with%2520simulation-based%2520observations%2520or%2520quantization%2520in%2520the%250Ageneration%2520or%2520observation%2520process.%2520We%2520demonstrate%2520its%2520effectiveness%2520on%250Adesigning%2520materials%2520with%2520desired%2520target%2520properties%2520and%2520predicting%2520protein%250Astructures%2520from%2520sparse%2520inter-residue%2520distance%2520measurements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESS-Flow%3A%20Training-free%20guidance%20of%20flow-based%20models%20as%20inference%20in%0A%20%20source%20space&entry.906535625=Adhithyan%20Kalaivanan%20and%20Zheng%20Zhao%20and%20Jens%20Sj%C3%B6lund%20and%20Fredrik%20Lindsten&entry.1292438233=%20%20Guiding%20pretrained%20flow-based%20generative%20models%20for%20conditional%20generation%20or%0Ato%20produce%20samples%20with%20desired%20target%20properties%20enables%20solving%20diverse%20tasks%0Awithout%20retraining%20on%20paired%20data.%20We%20present%20ESS-Flow%2C%20a%20gradient-free%20method%0Athat%20leverages%20the%20typically%20Gaussian%20prior%20of%20the%20source%20distribution%20in%0Aflow-based%20models%20to%20perform%20Bayesian%20inference%20directly%20in%20the%20source%20space%0Ausing%20Elliptical%20Slice%20Sampling.%20ESS-Flow%20only%20requires%20forward%20passes%20through%0Athe%20generative%20model%20and%20observation%20process%2C%20no%20gradient%20or%20Jacobian%0Acomputations%2C%20and%20is%20applicable%20even%20when%20gradients%20are%20unreliable%20or%0Aunavailable%2C%20such%20as%20with%20simulation-based%20observations%20or%20quantization%20in%20the%0Ageneration%20or%20observation%20process.%20We%20demonstrate%20its%20effectiveness%20on%0Adesigning%20materials%20with%20desired%20target%20properties%20and%20predicting%20protein%0Astructures%20from%20sparse%20inter-residue%20distance%20measurements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05849v1&entry.124074799=Read"},
{"title": "How to model Human Actions distribution with Event Sequence Data", "author": "Egor Surkov and Dmitry Osin and Evgeny Burnaev and Egor Shvetsov", "abstract": "  This paper studies forecasting of the future distribution of events in human\naction sequences, a task essential in domains like retail, finance, healthcare,\nand recommendation systems where the precise temporal order is often less\ncritical than the set of outcomes. We challenge the dominant autoregressive\nparadigm and investigate whether explicitly modeling the future distribution or\norder-invariant multi-token approaches outperform order-preserving methods. We\nanalyze local order invariance and introduce a KL-based metric to quantify\ntemporal drift. We find that a simple explicit distribution forecasting\nobjective consistently surpasses complex implicit baselines. We further\ndemonstrate that mode collapse of predicted categories is primarily driven by\ndistributional imbalance. This work provides a principled framework for\nselecting modeling strategies and offers practical guidance for building more\naccurate and robust forecasting systems.\n", "link": "http://arxiv.org/abs/2510.05856v1", "date": "2025-10-07", "relevancy": 1.5571, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5244}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20model%20Human%20Actions%20distribution%20with%20Event%20Sequence%20Data&body=Title%3A%20How%20to%20model%20Human%20Actions%20distribution%20with%20Event%20Sequence%20Data%0AAuthor%3A%20Egor%20Surkov%20and%20Dmitry%20Osin%20and%20Evgeny%20Burnaev%20and%20Egor%20Shvetsov%0AAbstract%3A%20%20%20This%20paper%20studies%20forecasting%20of%20the%20future%20distribution%20of%20events%20in%20human%0Aaction%20sequences%2C%20a%20task%20essential%20in%20domains%20like%20retail%2C%20finance%2C%20healthcare%2C%0Aand%20recommendation%20systems%20where%20the%20precise%20temporal%20order%20is%20often%20less%0Acritical%20than%20the%20set%20of%20outcomes.%20We%20challenge%20the%20dominant%20autoregressive%0Aparadigm%20and%20investigate%20whether%20explicitly%20modeling%20the%20future%20distribution%20or%0Aorder-invariant%20multi-token%20approaches%20outperform%20order-preserving%20methods.%20We%0Aanalyze%20local%20order%20invariance%20and%20introduce%20a%20KL-based%20metric%20to%20quantify%0Atemporal%20drift.%20We%20find%20that%20a%20simple%20explicit%20distribution%20forecasting%0Aobjective%20consistently%20surpasses%20complex%20implicit%20baselines.%20We%20further%0Ademonstrate%20that%20mode%20collapse%20of%20predicted%20categories%20is%20primarily%20driven%20by%0Adistributional%20imbalance.%20This%20work%20provides%20a%20principled%20framework%20for%0Aselecting%20modeling%20strategies%20and%20offers%20practical%20guidance%20for%20building%20more%0Aaccurate%20and%20robust%20forecasting%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520model%2520Human%2520Actions%2520distribution%2520with%2520Event%2520Sequence%2520Data%26entry.906535625%3DEgor%2520Surkov%2520and%2520Dmitry%2520Osin%2520and%2520Evgeny%2520Burnaev%2520and%2520Egor%2520Shvetsov%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520forecasting%2520of%2520the%2520future%2520distribution%2520of%2520events%2520in%2520human%250Aaction%2520sequences%252C%2520a%2520task%2520essential%2520in%2520domains%2520like%2520retail%252C%2520finance%252C%2520healthcare%252C%250Aand%2520recommendation%2520systems%2520where%2520the%2520precise%2520temporal%2520order%2520is%2520often%2520less%250Acritical%2520than%2520the%2520set%2520of%2520outcomes.%2520We%2520challenge%2520the%2520dominant%2520autoregressive%250Aparadigm%2520and%2520investigate%2520whether%2520explicitly%2520modeling%2520the%2520future%2520distribution%2520or%250Aorder-invariant%2520multi-token%2520approaches%2520outperform%2520order-preserving%2520methods.%2520We%250Aanalyze%2520local%2520order%2520invariance%2520and%2520introduce%2520a%2520KL-based%2520metric%2520to%2520quantify%250Atemporal%2520drift.%2520We%2520find%2520that%2520a%2520simple%2520explicit%2520distribution%2520forecasting%250Aobjective%2520consistently%2520surpasses%2520complex%2520implicit%2520baselines.%2520We%2520further%250Ademonstrate%2520that%2520mode%2520collapse%2520of%2520predicted%2520categories%2520is%2520primarily%2520driven%2520by%250Adistributional%2520imbalance.%2520This%2520work%2520provides%2520a%2520principled%2520framework%2520for%250Aselecting%2520modeling%2520strategies%2520and%2520offers%2520practical%2520guidance%2520for%2520building%2520more%250Aaccurate%2520and%2520robust%2520forecasting%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20model%20Human%20Actions%20distribution%20with%20Event%20Sequence%20Data&entry.906535625=Egor%20Surkov%20and%20Dmitry%20Osin%20and%20Evgeny%20Burnaev%20and%20Egor%20Shvetsov&entry.1292438233=%20%20This%20paper%20studies%20forecasting%20of%20the%20future%20distribution%20of%20events%20in%20human%0Aaction%20sequences%2C%20a%20task%20essential%20in%20domains%20like%20retail%2C%20finance%2C%20healthcare%2C%0Aand%20recommendation%20systems%20where%20the%20precise%20temporal%20order%20is%20often%20less%0Acritical%20than%20the%20set%20of%20outcomes.%20We%20challenge%20the%20dominant%20autoregressive%0Aparadigm%20and%20investigate%20whether%20explicitly%20modeling%20the%20future%20distribution%20or%0Aorder-invariant%20multi-token%20approaches%20outperform%20order-preserving%20methods.%20We%0Aanalyze%20local%20order%20invariance%20and%20introduce%20a%20KL-based%20metric%20to%20quantify%0Atemporal%20drift.%20We%20find%20that%20a%20simple%20explicit%20distribution%20forecasting%0Aobjective%20consistently%20surpasses%20complex%20implicit%20baselines.%20We%20further%0Ademonstrate%20that%20mode%20collapse%20of%20predicted%20categories%20is%20primarily%20driven%20by%0Adistributional%20imbalance.%20This%20work%20provides%20a%20principled%20framework%20for%0Aselecting%20modeling%20strategies%20and%20offers%20practical%20guidance%20for%20building%20more%0Aaccurate%20and%20robust%20forecasting%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05856v1&entry.124074799=Read"},
{"title": "Improving Clinical Dataset Condensation with Mode Connectivity-based\n  Trajectory Surrogates", "author": "Pafue Christy Nganjimi and Andrew Soltan and Danielle Belgrave and Lei Clifton and David A. Clifton and Anshul Thakur", "abstract": "  Dataset condensation (DC) enables the creation of compact, privacy-preserving\nsynthetic datasets that can match the utility of real patient records,\nsupporting democratised access to highly regulated clinical data for developing\ndownstream clinical models. State-of-the-art DC methods supervise synthetic\ndata by aligning the training dynamics of models trained on real and those\ntrained on synthetic data, typically using full stochastic gradient descent\n(SGD) trajectories as alignment targets; however, these trajectories are often\nnoisy, high-curvature, and storage-intensive, leading to unstable gradients,\nslow convergence, and substantial memory overhead. We address these limitations\nby replacing full SGD trajectories with smooth, low-loss parametric surrogates,\nspecifically quadratic B\\'ezier curves that connect the initial and final model\nstates from real training trajectories. These mode-connected paths provide\nnoise-free, low-curvature supervision signals that stabilise gradients,\naccelerate convergence, and eliminate the need for dense trajectory storage. We\ntheoretically justify B\\'ezier-mode connections as effective surrogates for SGD\npaths and empirically show that the proposed method outperforms\nstate-of-the-art condensation approaches across five clinical datasets,\nyielding condensed datasets that enable clinically effective model development.\n", "link": "http://arxiv.org/abs/2510.05805v1", "date": "2025-10-07", "relevancy": 1.5492, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5217}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.512}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Clinical%20Dataset%20Condensation%20with%20Mode%20Connectivity-based%0A%20%20Trajectory%20Surrogates&body=Title%3A%20Improving%20Clinical%20Dataset%20Condensation%20with%20Mode%20Connectivity-based%0A%20%20Trajectory%20Surrogates%0AAuthor%3A%20Pafue%20Christy%20Nganjimi%20and%20Andrew%20Soltan%20and%20Danielle%20Belgrave%20and%20Lei%20Clifton%20and%20David%20A.%20Clifton%20and%20Anshul%20Thakur%0AAbstract%3A%20%20%20Dataset%20condensation%20%28DC%29%20enables%20the%20creation%20of%20compact%2C%20privacy-preserving%0Asynthetic%20datasets%20that%20can%20match%20the%20utility%20of%20real%20patient%20records%2C%0Asupporting%20democratised%20access%20to%20highly%20regulated%20clinical%20data%20for%20developing%0Adownstream%20clinical%20models.%20State-of-the-art%20DC%20methods%20supervise%20synthetic%0Adata%20by%20aligning%20the%20training%20dynamics%20of%20models%20trained%20on%20real%20and%20those%0Atrained%20on%20synthetic%20data%2C%20typically%20using%20full%20stochastic%20gradient%20descent%0A%28SGD%29%20trajectories%20as%20alignment%20targets%3B%20however%2C%20these%20trajectories%20are%20often%0Anoisy%2C%20high-curvature%2C%20and%20storage-intensive%2C%20leading%20to%20unstable%20gradients%2C%0Aslow%20convergence%2C%20and%20substantial%20memory%20overhead.%20We%20address%20these%20limitations%0Aby%20replacing%20full%20SGD%20trajectories%20with%20smooth%2C%20low-loss%20parametric%20surrogates%2C%0Aspecifically%20quadratic%20B%5C%27ezier%20curves%20that%20connect%20the%20initial%20and%20final%20model%0Astates%20from%20real%20training%20trajectories.%20These%20mode-connected%20paths%20provide%0Anoise-free%2C%20low-curvature%20supervision%20signals%20that%20stabilise%20gradients%2C%0Aaccelerate%20convergence%2C%20and%20eliminate%20the%20need%20for%20dense%20trajectory%20storage.%20We%0Atheoretically%20justify%20B%5C%27ezier-mode%20connections%20as%20effective%20surrogates%20for%20SGD%0Apaths%20and%20empirically%20show%20that%20the%20proposed%20method%20outperforms%0Astate-of-the-art%20condensation%20approaches%20across%20five%20clinical%20datasets%2C%0Ayielding%20condensed%20datasets%20that%20enable%20clinically%20effective%20model%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Clinical%2520Dataset%2520Condensation%2520with%2520Mode%2520Connectivity-based%250A%2520%2520Trajectory%2520Surrogates%26entry.906535625%3DPafue%2520Christy%2520Nganjimi%2520and%2520Andrew%2520Soltan%2520and%2520Danielle%2520Belgrave%2520and%2520Lei%2520Clifton%2520and%2520David%2520A.%2520Clifton%2520and%2520Anshul%2520Thakur%26entry.1292438233%3D%2520%2520Dataset%2520condensation%2520%2528DC%2529%2520enables%2520the%2520creation%2520of%2520compact%252C%2520privacy-preserving%250Asynthetic%2520datasets%2520that%2520can%2520match%2520the%2520utility%2520of%2520real%2520patient%2520records%252C%250Asupporting%2520democratised%2520access%2520to%2520highly%2520regulated%2520clinical%2520data%2520for%2520developing%250Adownstream%2520clinical%2520models.%2520State-of-the-art%2520DC%2520methods%2520supervise%2520synthetic%250Adata%2520by%2520aligning%2520the%2520training%2520dynamics%2520of%2520models%2520trained%2520on%2520real%2520and%2520those%250Atrained%2520on%2520synthetic%2520data%252C%2520typically%2520using%2520full%2520stochastic%2520gradient%2520descent%250A%2528SGD%2529%2520trajectories%2520as%2520alignment%2520targets%253B%2520however%252C%2520these%2520trajectories%2520are%2520often%250Anoisy%252C%2520high-curvature%252C%2520and%2520storage-intensive%252C%2520leading%2520to%2520unstable%2520gradients%252C%250Aslow%2520convergence%252C%2520and%2520substantial%2520memory%2520overhead.%2520We%2520address%2520these%2520limitations%250Aby%2520replacing%2520full%2520SGD%2520trajectories%2520with%2520smooth%252C%2520low-loss%2520parametric%2520surrogates%252C%250Aspecifically%2520quadratic%2520B%255C%2527ezier%2520curves%2520that%2520connect%2520the%2520initial%2520and%2520final%2520model%250Astates%2520from%2520real%2520training%2520trajectories.%2520These%2520mode-connected%2520paths%2520provide%250Anoise-free%252C%2520low-curvature%2520supervision%2520signals%2520that%2520stabilise%2520gradients%252C%250Aaccelerate%2520convergence%252C%2520and%2520eliminate%2520the%2520need%2520for%2520dense%2520trajectory%2520storage.%2520We%250Atheoretically%2520justify%2520B%255C%2527ezier-mode%2520connections%2520as%2520effective%2520surrogates%2520for%2520SGD%250Apaths%2520and%2520empirically%2520show%2520that%2520the%2520proposed%2520method%2520outperforms%250Astate-of-the-art%2520condensation%2520approaches%2520across%2520five%2520clinical%2520datasets%252C%250Ayielding%2520condensed%2520datasets%2520that%2520enable%2520clinically%2520effective%2520model%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Clinical%20Dataset%20Condensation%20with%20Mode%20Connectivity-based%0A%20%20Trajectory%20Surrogates&entry.906535625=Pafue%20Christy%20Nganjimi%20and%20Andrew%20Soltan%20and%20Danielle%20Belgrave%20and%20Lei%20Clifton%20and%20David%20A.%20Clifton%20and%20Anshul%20Thakur&entry.1292438233=%20%20Dataset%20condensation%20%28DC%29%20enables%20the%20creation%20of%20compact%2C%20privacy-preserving%0Asynthetic%20datasets%20that%20can%20match%20the%20utility%20of%20real%20patient%20records%2C%0Asupporting%20democratised%20access%20to%20highly%20regulated%20clinical%20data%20for%20developing%0Adownstream%20clinical%20models.%20State-of-the-art%20DC%20methods%20supervise%20synthetic%0Adata%20by%20aligning%20the%20training%20dynamics%20of%20models%20trained%20on%20real%20and%20those%0Atrained%20on%20synthetic%20data%2C%20typically%20using%20full%20stochastic%20gradient%20descent%0A%28SGD%29%20trajectories%20as%20alignment%20targets%3B%20however%2C%20these%20trajectories%20are%20often%0Anoisy%2C%20high-curvature%2C%20and%20storage-intensive%2C%20leading%20to%20unstable%20gradients%2C%0Aslow%20convergence%2C%20and%20substantial%20memory%20overhead.%20We%20address%20these%20limitations%0Aby%20replacing%20full%20SGD%20trajectories%20with%20smooth%2C%20low-loss%20parametric%20surrogates%2C%0Aspecifically%20quadratic%20B%5C%27ezier%20curves%20that%20connect%20the%20initial%20and%20final%20model%0Astates%20from%20real%20training%20trajectories.%20These%20mode-connected%20paths%20provide%0Anoise-free%2C%20low-curvature%20supervision%20signals%20that%20stabilise%20gradients%2C%0Aaccelerate%20convergence%2C%20and%20eliminate%20the%20need%20for%20dense%20trajectory%20storage.%20We%0Atheoretically%20justify%20B%5C%27ezier-mode%20connections%20as%20effective%20surrogates%20for%20SGD%0Apaths%20and%20empirically%20show%20that%20the%20proposed%20method%20outperforms%0Astate-of-the-art%20condensation%20approaches%20across%20five%20clinical%20datasets%2C%0Ayielding%20condensed%20datasets%20that%20enable%20clinically%20effective%20model%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05805v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


