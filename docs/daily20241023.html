<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241021.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Fully Explicit Dynamic Gaussian Splatting", "author": "Junoh Lee and Chang-Yeon Won and Hyunjun Jung and Inhwan Bae and Hae-Gon Jeon", "abstract": "  3D Gaussian Splatting has shown fast and high-quality rendering results in\nstatic scenes by leveraging dense 3D prior and explicit representations.\nUnfortunately, the benefits of the prior and representation do not involve\nnovel view synthesis for dynamic motions. Ironically, this is because the main\nbarrier is the reliance on them, which requires increasing training and\nrendering times to account for dynamic motions. In this paper, we design a\nExplicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate\nstatic and dynamic Gaussians during training, and to explicitly sample\npositions and rotations of the dynamic Gaussians at sparse timestamps. The\nsampled positions and rotations are then interpolated to represent both\nspatially and temporally continuous motions of objects in dynamic scenes as\nwell as reducing computational cost. Additionally, we introduce a progressive\ntraining scheme and a point-backtracking technique that improves Ex4DGS's\nconvergence. We initially train Ex4DGS using short timestamps and progressively\nextend timestamps, which makes it work well with a few point clouds. The\npoint-backtracking is used to quantify the cumulative error of each Gaussian\nover time, enabling the detection and removal of erroneous Gaussians in dynamic\nscenes. Comprehensive experiments on various scenes demonstrate the\nstate-of-the-art rendering quality from our method, achieving fast rendering of\n62 fps on a single 2080Ti GPU.\n", "link": "http://arxiv.org/abs/2410.15629v2", "date": "2024-10-22", "relevancy": 3.3535, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6915}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6677}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Explicit%20Dynamic%20Gaussian%20Splatting&body=Title%3A%20Fully%20Explicit%20Dynamic%20Gaussian%20Splatting%0AAuthor%3A%20Junoh%20Lee%20and%20Chang-Yeon%20Won%20and%20Hyunjun%20Jung%20and%20Inhwan%20Bae%20and%20Hae-Gon%20Jeon%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20shown%20fast%20and%20high-quality%20rendering%20results%20in%0Astatic%20scenes%20by%20leveraging%20dense%203D%20prior%20and%20explicit%20representations.%0AUnfortunately%2C%20the%20benefits%20of%20the%20prior%20and%20representation%20do%20not%20involve%0Anovel%20view%20synthesis%20for%20dynamic%20motions.%20Ironically%2C%20this%20is%20because%20the%20main%0Abarrier%20is%20the%20reliance%20on%20them%2C%20which%20requires%20increasing%20training%20and%0Arendering%20times%20to%20account%20for%20dynamic%20motions.%20In%20this%20paper%2C%20we%20design%20a%0AExplicit%204D%20Gaussian%20Splatting%28Ex4DGS%29.%20Our%20key%20idea%20is%20to%20firstly%20separate%0Astatic%20and%20dynamic%20Gaussians%20during%20training%2C%20and%20to%20explicitly%20sample%0Apositions%20and%20rotations%20of%20the%20dynamic%20Gaussians%20at%20sparse%20timestamps.%20The%0Asampled%20positions%20and%20rotations%20are%20then%20interpolated%20to%20represent%20both%0Aspatially%20and%20temporally%20continuous%20motions%20of%20objects%20in%20dynamic%20scenes%20as%0Awell%20as%20reducing%20computational%20cost.%20Additionally%2C%20we%20introduce%20a%20progressive%0Atraining%20scheme%20and%20a%20point-backtracking%20technique%20that%20improves%20Ex4DGS%27s%0Aconvergence.%20We%20initially%20train%20Ex4DGS%20using%20short%20timestamps%20and%20progressively%0Aextend%20timestamps%2C%20which%20makes%20it%20work%20well%20with%20a%20few%20point%20clouds.%20The%0Apoint-backtracking%20is%20used%20to%20quantify%20the%20cumulative%20error%20of%20each%20Gaussian%0Aover%20time%2C%20enabling%20the%20detection%20and%20removal%20of%20erroneous%20Gaussians%20in%20dynamic%0Ascenes.%20Comprehensive%20experiments%20on%20various%20scenes%20demonstrate%20the%0Astate-of-the-art%20rendering%20quality%20from%20our%20method%2C%20achieving%20fast%20rendering%20of%0A62%20fps%20on%20a%20single%202080Ti%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Explicit%2520Dynamic%2520Gaussian%2520Splatting%26entry.906535625%3DJunoh%2520Lee%2520and%2520Chang-Yeon%2520Won%2520and%2520Hyunjun%2520Jung%2520and%2520Inhwan%2520Bae%2520and%2520Hae-Gon%2520Jeon%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520shown%2520fast%2520and%2520high-quality%2520rendering%2520results%2520in%250Astatic%2520scenes%2520by%2520leveraging%2520dense%25203D%2520prior%2520and%2520explicit%2520representations.%250AUnfortunately%252C%2520the%2520benefits%2520of%2520the%2520prior%2520and%2520representation%2520do%2520not%2520involve%250Anovel%2520view%2520synthesis%2520for%2520dynamic%2520motions.%2520Ironically%252C%2520this%2520is%2520because%2520the%2520main%250Abarrier%2520is%2520the%2520reliance%2520on%2520them%252C%2520which%2520requires%2520increasing%2520training%2520and%250Arendering%2520times%2520to%2520account%2520for%2520dynamic%2520motions.%2520In%2520this%2520paper%252C%2520we%2520design%2520a%250AExplicit%25204D%2520Gaussian%2520Splatting%2528Ex4DGS%2529.%2520Our%2520key%2520idea%2520is%2520to%2520firstly%2520separate%250Astatic%2520and%2520dynamic%2520Gaussians%2520during%2520training%252C%2520and%2520to%2520explicitly%2520sample%250Apositions%2520and%2520rotations%2520of%2520the%2520dynamic%2520Gaussians%2520at%2520sparse%2520timestamps.%2520The%250Asampled%2520positions%2520and%2520rotations%2520are%2520then%2520interpolated%2520to%2520represent%2520both%250Aspatially%2520and%2520temporally%2520continuous%2520motions%2520of%2520objects%2520in%2520dynamic%2520scenes%2520as%250Awell%2520as%2520reducing%2520computational%2520cost.%2520Additionally%252C%2520we%2520introduce%2520a%2520progressive%250Atraining%2520scheme%2520and%2520a%2520point-backtracking%2520technique%2520that%2520improves%2520Ex4DGS%2527s%250Aconvergence.%2520We%2520initially%2520train%2520Ex4DGS%2520using%2520short%2520timestamps%2520and%2520progressively%250Aextend%2520timestamps%252C%2520which%2520makes%2520it%2520work%2520well%2520with%2520a%2520few%2520point%2520clouds.%2520The%250Apoint-backtracking%2520is%2520used%2520to%2520quantify%2520the%2520cumulative%2520error%2520of%2520each%2520Gaussian%250Aover%2520time%252C%2520enabling%2520the%2520detection%2520and%2520removal%2520of%2520erroneous%2520Gaussians%2520in%2520dynamic%250Ascenes.%2520Comprehensive%2520experiments%2520on%2520various%2520scenes%2520demonstrate%2520the%250Astate-of-the-art%2520rendering%2520quality%2520from%2520our%2520method%252C%2520achieving%2520fast%2520rendering%2520of%250A62%2520fps%2520on%2520a%2520single%25202080Ti%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Explicit%20Dynamic%20Gaussian%20Splatting&entry.906535625=Junoh%20Lee%20and%20Chang-Yeon%20Won%20and%20Hyunjun%20Jung%20and%20Inhwan%20Bae%20and%20Hae-Gon%20Jeon&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20shown%20fast%20and%20high-quality%20rendering%20results%20in%0Astatic%20scenes%20by%20leveraging%20dense%203D%20prior%20and%20explicit%20representations.%0AUnfortunately%2C%20the%20benefits%20of%20the%20prior%20and%20representation%20do%20not%20involve%0Anovel%20view%20synthesis%20for%20dynamic%20motions.%20Ironically%2C%20this%20is%20because%20the%20main%0Abarrier%20is%20the%20reliance%20on%20them%2C%20which%20requires%20increasing%20training%20and%0Arendering%20times%20to%20account%20for%20dynamic%20motions.%20In%20this%20paper%2C%20we%20design%20a%0AExplicit%204D%20Gaussian%20Splatting%28Ex4DGS%29.%20Our%20key%20idea%20is%20to%20firstly%20separate%0Astatic%20and%20dynamic%20Gaussians%20during%20training%2C%20and%20to%20explicitly%20sample%0Apositions%20and%20rotations%20of%20the%20dynamic%20Gaussians%20at%20sparse%20timestamps.%20The%0Asampled%20positions%20and%20rotations%20are%20then%20interpolated%20to%20represent%20both%0Aspatially%20and%20temporally%20continuous%20motions%20of%20objects%20in%20dynamic%20scenes%20as%0Awell%20as%20reducing%20computational%20cost.%20Additionally%2C%20we%20introduce%20a%20progressive%0Atraining%20scheme%20and%20a%20point-backtracking%20technique%20that%20improves%20Ex4DGS%27s%0Aconvergence.%20We%20initially%20train%20Ex4DGS%20using%20short%20timestamps%20and%20progressively%0Aextend%20timestamps%2C%20which%20makes%20it%20work%20well%20with%20a%20few%20point%20clouds.%20The%0Apoint-backtracking%20is%20used%20to%20quantify%20the%20cumulative%20error%20of%20each%20Gaussian%0Aover%20time%2C%20enabling%20the%20detection%20and%20removal%20of%20erroneous%20Gaussians%20in%20dynamic%0Ascenes.%20Comprehensive%20experiments%20on%20various%20scenes%20demonstrate%20the%0Astate-of-the-art%20rendering%20quality%20from%20our%20method%2C%20achieving%20fast%20rendering%20of%0A62%20fps%20on%20a%20single%202080Ti%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15629v2&entry.124074799=Read"},
{"title": "VistaDream: Sampling multiview consistent images for single-view scene\n  reconstruction", "author": "Haiping Wang and Yuan Liu and Ziwei Liu and Wenping Wang and Zhen Dong and Bisheng Yang", "abstract": "  In this paper, we propose VistaDream a novel framework to reconstruct a 3D\nscene from a single-view image. Recent diffusion models enable generating\nhigh-quality novel-view images from a single-view input image. Most existing\nmethods only concentrate on building the consistency between the input image\nand the generated images while losing the consistency between the generated\nimages. VistaDream addresses this problem by a two-stage pipeline. In the first\nstage, VistaDream begins with building a global coarse 3D scaffold by zooming\nout a little step with inpainted boundaries and an estimated depth map. Then,\non this global scaffold, we use iterative diffusion-based RGB-D inpainting to\ngenerate novel-view images to inpaint the holes of the scaffold. In the second\nstage, we further enhance the consistency between the generated novel-view\nimages by a novel training-free Multiview Consistency Sampling (MCS) that\nintroduces multi-view consistency constraints in the reverse sampling process\nof diffusion models. Experimental results demonstrate that without training or\nfine-tuning existing diffusion models, VistaDream achieves consistent and\nhigh-quality novel view synthesis using just single-view images and outperforms\nbaseline methods by a large margin. The code, videos, and interactive demos are\navailable at https://vistadream-project-page.github.io/.\n", "link": "http://arxiv.org/abs/2410.16892v1", "date": "2024-10-22", "relevancy": 3.3233, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7149}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7149}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VistaDream%3A%20Sampling%20multiview%20consistent%20images%20for%20single-view%20scene%0A%20%20reconstruction&body=Title%3A%20VistaDream%3A%20Sampling%20multiview%20consistent%20images%20for%20single-view%20scene%0A%20%20reconstruction%0AAuthor%3A%20Haiping%20Wang%20and%20Yuan%20Liu%20and%20Ziwei%20Liu%20and%20Wenping%20Wang%20and%20Zhen%20Dong%20and%20Bisheng%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20VistaDream%20a%20novel%20framework%20to%20reconstruct%20a%203D%0Ascene%20from%20a%20single-view%20image.%20Recent%20diffusion%20models%20enable%20generating%0Ahigh-quality%20novel-view%20images%20from%20a%20single-view%20input%20image.%20Most%20existing%0Amethods%20only%20concentrate%20on%20building%20the%20consistency%20between%20the%20input%20image%0Aand%20the%20generated%20images%20while%20losing%20the%20consistency%20between%20the%20generated%0Aimages.%20VistaDream%20addresses%20this%20problem%20by%20a%20two-stage%20pipeline.%20In%20the%20first%0Astage%2C%20VistaDream%20begins%20with%20building%20a%20global%20coarse%203D%20scaffold%20by%20zooming%0Aout%20a%20little%20step%20with%20inpainted%20boundaries%20and%20an%20estimated%20depth%20map.%20Then%2C%0Aon%20this%20global%20scaffold%2C%20we%20use%20iterative%20diffusion-based%20RGB-D%20inpainting%20to%0Agenerate%20novel-view%20images%20to%20inpaint%20the%20holes%20of%20the%20scaffold.%20In%20the%20second%0Astage%2C%20we%20further%20enhance%20the%20consistency%20between%20the%20generated%20novel-view%0Aimages%20by%20a%20novel%20training-free%20Multiview%20Consistency%20Sampling%20%28MCS%29%20that%0Aintroduces%20multi-view%20consistency%20constraints%20in%20the%20reverse%20sampling%20process%0Aof%20diffusion%20models.%20Experimental%20results%20demonstrate%20that%20without%20training%20or%0Afine-tuning%20existing%20diffusion%20models%2C%20VistaDream%20achieves%20consistent%20and%0Ahigh-quality%20novel%20view%20synthesis%20using%20just%20single-view%20images%20and%20outperforms%0Abaseline%20methods%20by%20a%20large%20margin.%20The%20code%2C%20videos%2C%20and%20interactive%20demos%20are%0Aavailable%20at%20https%3A//vistadream-project-page.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVistaDream%253A%2520Sampling%2520multiview%2520consistent%2520images%2520for%2520single-view%2520scene%250A%2520%2520reconstruction%26entry.906535625%3DHaiping%2520Wang%2520and%2520Yuan%2520Liu%2520and%2520Ziwei%2520Liu%2520and%2520Wenping%2520Wang%2520and%2520Zhen%2520Dong%2520and%2520Bisheng%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520VistaDream%2520a%2520novel%2520framework%2520to%2520reconstruct%2520a%25203D%250Ascene%2520from%2520a%2520single-view%2520image.%2520Recent%2520diffusion%2520models%2520enable%2520generating%250Ahigh-quality%2520novel-view%2520images%2520from%2520a%2520single-view%2520input%2520image.%2520Most%2520existing%250Amethods%2520only%2520concentrate%2520on%2520building%2520the%2520consistency%2520between%2520the%2520input%2520image%250Aand%2520the%2520generated%2520images%2520while%2520losing%2520the%2520consistency%2520between%2520the%2520generated%250Aimages.%2520VistaDream%2520addresses%2520this%2520problem%2520by%2520a%2520two-stage%2520pipeline.%2520In%2520the%2520first%250Astage%252C%2520VistaDream%2520begins%2520with%2520building%2520a%2520global%2520coarse%25203D%2520scaffold%2520by%2520zooming%250Aout%2520a%2520little%2520step%2520with%2520inpainted%2520boundaries%2520and%2520an%2520estimated%2520depth%2520map.%2520Then%252C%250Aon%2520this%2520global%2520scaffold%252C%2520we%2520use%2520iterative%2520diffusion-based%2520RGB-D%2520inpainting%2520to%250Agenerate%2520novel-view%2520images%2520to%2520inpaint%2520the%2520holes%2520of%2520the%2520scaffold.%2520In%2520the%2520second%250Astage%252C%2520we%2520further%2520enhance%2520the%2520consistency%2520between%2520the%2520generated%2520novel-view%250Aimages%2520by%2520a%2520novel%2520training-free%2520Multiview%2520Consistency%2520Sampling%2520%2528MCS%2529%2520that%250Aintroduces%2520multi-view%2520consistency%2520constraints%2520in%2520the%2520reverse%2520sampling%2520process%250Aof%2520diffusion%2520models.%2520Experimental%2520results%2520demonstrate%2520that%2520without%2520training%2520or%250Afine-tuning%2520existing%2520diffusion%2520models%252C%2520VistaDream%2520achieves%2520consistent%2520and%250Ahigh-quality%2520novel%2520view%2520synthesis%2520using%2520just%2520single-view%2520images%2520and%2520outperforms%250Abaseline%2520methods%2520by%2520a%2520large%2520margin.%2520The%2520code%252C%2520videos%252C%2520and%2520interactive%2520demos%2520are%250Aavailable%2520at%2520https%253A//vistadream-project-page.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VistaDream%3A%20Sampling%20multiview%20consistent%20images%20for%20single-view%20scene%0A%20%20reconstruction&entry.906535625=Haiping%20Wang%20and%20Yuan%20Liu%20and%20Ziwei%20Liu%20and%20Wenping%20Wang%20and%20Zhen%20Dong%20and%20Bisheng%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20VistaDream%20a%20novel%20framework%20to%20reconstruct%20a%203D%0Ascene%20from%20a%20single-view%20image.%20Recent%20diffusion%20models%20enable%20generating%0Ahigh-quality%20novel-view%20images%20from%20a%20single-view%20input%20image.%20Most%20existing%0Amethods%20only%20concentrate%20on%20building%20the%20consistency%20between%20the%20input%20image%0Aand%20the%20generated%20images%20while%20losing%20the%20consistency%20between%20the%20generated%0Aimages.%20VistaDream%20addresses%20this%20problem%20by%20a%20two-stage%20pipeline.%20In%20the%20first%0Astage%2C%20VistaDream%20begins%20with%20building%20a%20global%20coarse%203D%20scaffold%20by%20zooming%0Aout%20a%20little%20step%20with%20inpainted%20boundaries%20and%20an%20estimated%20depth%20map.%20Then%2C%0Aon%20this%20global%20scaffold%2C%20we%20use%20iterative%20diffusion-based%20RGB-D%20inpainting%20to%0Agenerate%20novel-view%20images%20to%20inpaint%20the%20holes%20of%20the%20scaffold.%20In%20the%20second%0Astage%2C%20we%20further%20enhance%20the%20consistency%20between%20the%20generated%20novel-view%0Aimages%20by%20a%20novel%20training-free%20Multiview%20Consistency%20Sampling%20%28MCS%29%20that%0Aintroduces%20multi-view%20consistency%20constraints%20in%20the%20reverse%20sampling%20process%0Aof%20diffusion%20models.%20Experimental%20results%20demonstrate%20that%20without%20training%20or%0Afine-tuning%20existing%20diffusion%20models%2C%20VistaDream%20achieves%20consistent%20and%0Ahigh-quality%20novel%20view%20synthesis%20using%20just%20single-view%20images%20and%20outperforms%0Abaseline%20methods%20by%20a%20large%20margin.%20The%20code%2C%20videos%2C%20and%20interactive%20demos%20are%0Aavailable%20at%20https%3A//vistadream-project-page.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16892v1&entry.124074799=Read"},
{"title": "Multi-Layer Gaussian Splatting for Immersive Anatomy Visualization", "author": "Constantin Kleinbeck and Hannah Schieber and Klaus Engel and Ralf Gutjahr and Daniel Roth", "abstract": "  In medical image visualization, path tracing of volumetric medical data like\nCT scans produces lifelike three-dimensional visualizations. Immersive VR\ndisplays can further enhance the understanding of complex anatomies. Going\nbeyond the diagnostic quality of traditional 2D slices, they enable interactive\n3D evaluation of anatomies, supporting medical education and planning.\nRendering high-quality visualizations in real-time, however, is computationally\nintensive and impractical for compute-constrained devices like mobile headsets.\n  We propose a novel approach utilizing GS to create an efficient but static\nintermediate representation of CT scans. We introduce a layered GS\nrepresentation, incrementally including different anatomical structures while\nminimizing overlap and extending the GS training to remove inactive Gaussians.\nWe further compress the created model with clustering across layers.\n  Our approach achieves interactive frame rates while preserving anatomical\nstructures, with quality adjustable to the target hardware. Compared to\nstandard GS, our representation retains some of the explorative qualities\ninitially enabled by immersive path tracing. Selective activation and clipping\nof layers are possible at rendering time, adding a degree of interactivity to\notherwise static GS models. This could enable scenarios where high\ncomputational demands would otherwise prohibit using path-traced medical\nvolumes.\n", "link": "http://arxiv.org/abs/2410.16978v1", "date": "2024-10-22", "relevancy": 3.3138, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6908}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6663}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Layer%20Gaussian%20Splatting%20for%20Immersive%20Anatomy%20Visualization&body=Title%3A%20Multi-Layer%20Gaussian%20Splatting%20for%20Immersive%20Anatomy%20Visualization%0AAuthor%3A%20Constantin%20Kleinbeck%20and%20Hannah%20Schieber%20and%20Klaus%20Engel%20and%20Ralf%20Gutjahr%20and%20Daniel%20Roth%0AAbstract%3A%20%20%20In%20medical%20image%20visualization%2C%20path%20tracing%20of%20volumetric%20medical%20data%20like%0ACT%20scans%20produces%20lifelike%20three-dimensional%20visualizations.%20Immersive%20VR%0Adisplays%20can%20further%20enhance%20the%20understanding%20of%20complex%20anatomies.%20Going%0Abeyond%20the%20diagnostic%20quality%20of%20traditional%202D%20slices%2C%20they%20enable%20interactive%0A3D%20evaluation%20of%20anatomies%2C%20supporting%20medical%20education%20and%20planning.%0ARendering%20high-quality%20visualizations%20in%20real-time%2C%20however%2C%20is%20computationally%0Aintensive%20and%20impractical%20for%20compute-constrained%20devices%20like%20mobile%20headsets.%0A%20%20We%20propose%20a%20novel%20approach%20utilizing%20GS%20to%20create%20an%20efficient%20but%20static%0Aintermediate%20representation%20of%20CT%20scans.%20We%20introduce%20a%20layered%20GS%0Arepresentation%2C%20incrementally%20including%20different%20anatomical%20structures%20while%0Aminimizing%20overlap%20and%20extending%20the%20GS%20training%20to%20remove%20inactive%20Gaussians.%0AWe%20further%20compress%20the%20created%20model%20with%20clustering%20across%20layers.%0A%20%20Our%20approach%20achieves%20interactive%20frame%20rates%20while%20preserving%20anatomical%0Astructures%2C%20with%20quality%20adjustable%20to%20the%20target%20hardware.%20Compared%20to%0Astandard%20GS%2C%20our%20representation%20retains%20some%20of%20the%20explorative%20qualities%0Ainitially%20enabled%20by%20immersive%20path%20tracing.%20Selective%20activation%20and%20clipping%0Aof%20layers%20are%20possible%20at%20rendering%20time%2C%20adding%20a%20degree%20of%20interactivity%20to%0Aotherwise%20static%20GS%20models.%20This%20could%20enable%20scenarios%20where%20high%0Acomputational%20demands%20would%20otherwise%20prohibit%20using%20path-traced%20medical%0Avolumes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Layer%2520Gaussian%2520Splatting%2520for%2520Immersive%2520Anatomy%2520Visualization%26entry.906535625%3DConstantin%2520Kleinbeck%2520and%2520Hannah%2520Schieber%2520and%2520Klaus%2520Engel%2520and%2520Ralf%2520Gutjahr%2520and%2520Daniel%2520Roth%26entry.1292438233%3D%2520%2520In%2520medical%2520image%2520visualization%252C%2520path%2520tracing%2520of%2520volumetric%2520medical%2520data%2520like%250ACT%2520scans%2520produces%2520lifelike%2520three-dimensional%2520visualizations.%2520Immersive%2520VR%250Adisplays%2520can%2520further%2520enhance%2520the%2520understanding%2520of%2520complex%2520anatomies.%2520Going%250Abeyond%2520the%2520diagnostic%2520quality%2520of%2520traditional%25202D%2520slices%252C%2520they%2520enable%2520interactive%250A3D%2520evaluation%2520of%2520anatomies%252C%2520supporting%2520medical%2520education%2520and%2520planning.%250ARendering%2520high-quality%2520visualizations%2520in%2520real-time%252C%2520however%252C%2520is%2520computationally%250Aintensive%2520and%2520impractical%2520for%2520compute-constrained%2520devices%2520like%2520mobile%2520headsets.%250A%2520%2520We%2520propose%2520a%2520novel%2520approach%2520utilizing%2520GS%2520to%2520create%2520an%2520efficient%2520but%2520static%250Aintermediate%2520representation%2520of%2520CT%2520scans.%2520We%2520introduce%2520a%2520layered%2520GS%250Arepresentation%252C%2520incrementally%2520including%2520different%2520anatomical%2520structures%2520while%250Aminimizing%2520overlap%2520and%2520extending%2520the%2520GS%2520training%2520to%2520remove%2520inactive%2520Gaussians.%250AWe%2520further%2520compress%2520the%2520created%2520model%2520with%2520clustering%2520across%2520layers.%250A%2520%2520Our%2520approach%2520achieves%2520interactive%2520frame%2520rates%2520while%2520preserving%2520anatomical%250Astructures%252C%2520with%2520quality%2520adjustable%2520to%2520the%2520target%2520hardware.%2520Compared%2520to%250Astandard%2520GS%252C%2520our%2520representation%2520retains%2520some%2520of%2520the%2520explorative%2520qualities%250Ainitially%2520enabled%2520by%2520immersive%2520path%2520tracing.%2520Selective%2520activation%2520and%2520clipping%250Aof%2520layers%2520are%2520possible%2520at%2520rendering%2520time%252C%2520adding%2520a%2520degree%2520of%2520interactivity%2520to%250Aotherwise%2520static%2520GS%2520models.%2520This%2520could%2520enable%2520scenarios%2520where%2520high%250Acomputational%2520demands%2520would%2520otherwise%2520prohibit%2520using%2520path-traced%2520medical%250Avolumes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Layer%20Gaussian%20Splatting%20for%20Immersive%20Anatomy%20Visualization&entry.906535625=Constantin%20Kleinbeck%20and%20Hannah%20Schieber%20and%20Klaus%20Engel%20and%20Ralf%20Gutjahr%20and%20Daniel%20Roth&entry.1292438233=%20%20In%20medical%20image%20visualization%2C%20path%20tracing%20of%20volumetric%20medical%20data%20like%0ACT%20scans%20produces%20lifelike%20three-dimensional%20visualizations.%20Immersive%20VR%0Adisplays%20can%20further%20enhance%20the%20understanding%20of%20complex%20anatomies.%20Going%0Abeyond%20the%20diagnostic%20quality%20of%20traditional%202D%20slices%2C%20they%20enable%20interactive%0A3D%20evaluation%20of%20anatomies%2C%20supporting%20medical%20education%20and%20planning.%0ARendering%20high-quality%20visualizations%20in%20real-time%2C%20however%2C%20is%20computationally%0Aintensive%20and%20impractical%20for%20compute-constrained%20devices%20like%20mobile%20headsets.%0A%20%20We%20propose%20a%20novel%20approach%20utilizing%20GS%20to%20create%20an%20efficient%20but%20static%0Aintermediate%20representation%20of%20CT%20scans.%20We%20introduce%20a%20layered%20GS%0Arepresentation%2C%20incrementally%20including%20different%20anatomical%20structures%20while%0Aminimizing%20overlap%20and%20extending%20the%20GS%20training%20to%20remove%20inactive%20Gaussians.%0AWe%20further%20compress%20the%20created%20model%20with%20clustering%20across%20layers.%0A%20%20Our%20approach%20achieves%20interactive%20frame%20rates%20while%20preserving%20anatomical%0Astructures%2C%20with%20quality%20adjustable%20to%20the%20target%20hardware.%20Compared%20to%0Astandard%20GS%2C%20our%20representation%20retains%20some%20of%20the%20explorative%20qualities%0Ainitially%20enabled%20by%20immersive%20path%20tracing.%20Selective%20activation%20and%20clipping%0Aof%20layers%20are%20possible%20at%20rendering%20time%2C%20adding%20a%20degree%20of%20interactivity%20to%0Aotherwise%20static%20GS%20models.%20This%20could%20enable%20scenarios%20where%20high%0Acomputational%20demands%20would%20otherwise%20prohibit%20using%20path-traced%20medical%0Avolumes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16978v1&entry.124074799=Read"},
{"title": "E-3DGS: Gaussian Splatting with Exposure and Motion Events", "author": "Xiaoting Yin and Hao Shi and Yuhan Bao and Zhenshan Bing and Yiyi Liao and Kailun Yang and Kaiwei Wang", "abstract": "  Estimating Neural Radiance Fields (NeRFs) from images captured under optimal\nconditions has been extensively explored in the vision community. However,\nrobotic applications often face challenges such as motion blur, insufficient\nillumination, and high computational overhead, which adversely affect\ndownstream tasks like navigation, inspection, and scene visualization. To\naddress these challenges, we propose E-3DGS, a novel event-based approach that\npartitions events into motion (from camera or object movement) and exposure\n(from camera exposure), using the former to handle fast-motion scenes and using\nthe latter to reconstruct grayscale images for high-quality training and\noptimization of event-based 3D Gaussian Splatting (3DGS). We introduce a novel\nintegration of 3DGS with exposure events for high-quality reconstruction of\nexplicit scene representations. Our versatile framework can operate on motion\nevents alone for 3D reconstruction, enhance quality using exposure events, or\nadopt a hybrid mode that balances quality and effectiveness by optimizing with\ninitial exposure events followed by high-speed motion events. We also introduce\nEME-3D, a real-world 3D dataset with exposure events, motion events, camera\ncalibration parameters, and sparse point clouds. Our method is faster and\ndelivers better reconstruction quality than event-based NeRF while being more\ncost-effective than NeRF methods that combine event and RGB data by using a\nsingle event sensor. By combining motion and exposure events, E-3DGS sets a new\nbenchmark for event-based 3D reconstruction with robust performance in\nchallenging conditions and lower hardware demands. The source code and dataset\nwill be available at https://github.com/MasterHow/E-3DGS.\n", "link": "http://arxiv.org/abs/2410.16995v1", "date": "2024-10-22", "relevancy": 3.2918, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7079}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6337}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-3DGS%3A%20Gaussian%20Splatting%20with%20Exposure%20and%20Motion%20Events&body=Title%3A%20E-3DGS%3A%20Gaussian%20Splatting%20with%20Exposure%20and%20Motion%20Events%0AAuthor%3A%20Xiaoting%20Yin%20and%20Hao%20Shi%20and%20Yuhan%20Bao%20and%20Zhenshan%20Bing%20and%20Yiyi%20Liao%20and%20Kailun%20Yang%20and%20Kaiwei%20Wang%0AAbstract%3A%20%20%20Estimating%20Neural%20Radiance%20Fields%20%28NeRFs%29%20from%20images%20captured%20under%20optimal%0Aconditions%20has%20been%20extensively%20explored%20in%20the%20vision%20community.%20However%2C%0Arobotic%20applications%20often%20face%20challenges%20such%20as%20motion%20blur%2C%20insufficient%0Aillumination%2C%20and%20high%20computational%20overhead%2C%20which%20adversely%20affect%0Adownstream%20tasks%20like%20navigation%2C%20inspection%2C%20and%20scene%20visualization.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20E-3DGS%2C%20a%20novel%20event-based%20approach%20that%0Apartitions%20events%20into%20motion%20%28from%20camera%20or%20object%20movement%29%20and%20exposure%0A%28from%20camera%20exposure%29%2C%20using%20the%20former%20to%20handle%20fast-motion%20scenes%20and%20using%0Athe%20latter%20to%20reconstruct%20grayscale%20images%20for%20high-quality%20training%20and%0Aoptimization%20of%20event-based%203D%20Gaussian%20Splatting%20%283DGS%29.%20We%20introduce%20a%20novel%0Aintegration%20of%203DGS%20with%20exposure%20events%20for%20high-quality%20reconstruction%20of%0Aexplicit%20scene%20representations.%20Our%20versatile%20framework%20can%20operate%20on%20motion%0Aevents%20alone%20for%203D%20reconstruction%2C%20enhance%20quality%20using%20exposure%20events%2C%20or%0Aadopt%20a%20hybrid%20mode%20that%20balances%20quality%20and%20effectiveness%20by%20optimizing%20with%0Ainitial%20exposure%20events%20followed%20by%20high-speed%20motion%20events.%20We%20also%20introduce%0AEME-3D%2C%20a%20real-world%203D%20dataset%20with%20exposure%20events%2C%20motion%20events%2C%20camera%0Acalibration%20parameters%2C%20and%20sparse%20point%20clouds.%20Our%20method%20is%20faster%20and%0Adelivers%20better%20reconstruction%20quality%20than%20event-based%20NeRF%20while%20being%20more%0Acost-effective%20than%20NeRF%20methods%20that%20combine%20event%20and%20RGB%20data%20by%20using%20a%0Asingle%20event%20sensor.%20By%20combining%20motion%20and%20exposure%20events%2C%20E-3DGS%20sets%20a%20new%0Abenchmark%20for%20event-based%203D%20reconstruction%20with%20robust%20performance%20in%0Achallenging%20conditions%20and%20lower%20hardware%20demands.%20The%20source%20code%20and%20dataset%0Awill%20be%20available%20at%20https%3A//github.com/MasterHow/E-3DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-3DGS%253A%2520Gaussian%2520Splatting%2520with%2520Exposure%2520and%2520Motion%2520Events%26entry.906535625%3DXiaoting%2520Yin%2520and%2520Hao%2520Shi%2520and%2520Yuhan%2520Bao%2520and%2520Zhenshan%2520Bing%2520and%2520Yiyi%2520Liao%2520and%2520Kailun%2520Yang%2520and%2520Kaiwei%2520Wang%26entry.1292438233%3D%2520%2520Estimating%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520from%2520images%2520captured%2520under%2520optimal%250Aconditions%2520has%2520been%2520extensively%2520explored%2520in%2520the%2520vision%2520community.%2520However%252C%250Arobotic%2520applications%2520often%2520face%2520challenges%2520such%2520as%2520motion%2520blur%252C%2520insufficient%250Aillumination%252C%2520and%2520high%2520computational%2520overhead%252C%2520which%2520adversely%2520affect%250Adownstream%2520tasks%2520like%2520navigation%252C%2520inspection%252C%2520and%2520scene%2520visualization.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520E-3DGS%252C%2520a%2520novel%2520event-based%2520approach%2520that%250Apartitions%2520events%2520into%2520motion%2520%2528from%2520camera%2520or%2520object%2520movement%2529%2520and%2520exposure%250A%2528from%2520camera%2520exposure%2529%252C%2520using%2520the%2520former%2520to%2520handle%2520fast-motion%2520scenes%2520and%2520using%250Athe%2520latter%2520to%2520reconstruct%2520grayscale%2520images%2520for%2520high-quality%2520training%2520and%250Aoptimization%2520of%2520event-based%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520We%2520introduce%2520a%2520novel%250Aintegration%2520of%25203DGS%2520with%2520exposure%2520events%2520for%2520high-quality%2520reconstruction%2520of%250Aexplicit%2520scene%2520representations.%2520Our%2520versatile%2520framework%2520can%2520operate%2520on%2520motion%250Aevents%2520alone%2520for%25203D%2520reconstruction%252C%2520enhance%2520quality%2520using%2520exposure%2520events%252C%2520or%250Aadopt%2520a%2520hybrid%2520mode%2520that%2520balances%2520quality%2520and%2520effectiveness%2520by%2520optimizing%2520with%250Ainitial%2520exposure%2520events%2520followed%2520by%2520high-speed%2520motion%2520events.%2520We%2520also%2520introduce%250AEME-3D%252C%2520a%2520real-world%25203D%2520dataset%2520with%2520exposure%2520events%252C%2520motion%2520events%252C%2520camera%250Acalibration%2520parameters%252C%2520and%2520sparse%2520point%2520clouds.%2520Our%2520method%2520is%2520faster%2520and%250Adelivers%2520better%2520reconstruction%2520quality%2520than%2520event-based%2520NeRF%2520while%2520being%2520more%250Acost-effective%2520than%2520NeRF%2520methods%2520that%2520combine%2520event%2520and%2520RGB%2520data%2520by%2520using%2520a%250Asingle%2520event%2520sensor.%2520By%2520combining%2520motion%2520and%2520exposure%2520events%252C%2520E-3DGS%2520sets%2520a%2520new%250Abenchmark%2520for%2520event-based%25203D%2520reconstruction%2520with%2520robust%2520performance%2520in%250Achallenging%2520conditions%2520and%2520lower%2520hardware%2520demands.%2520The%2520source%2520code%2520and%2520dataset%250Awill%2520be%2520available%2520at%2520https%253A//github.com/MasterHow/E-3DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-3DGS%3A%20Gaussian%20Splatting%20with%20Exposure%20and%20Motion%20Events&entry.906535625=Xiaoting%20Yin%20and%20Hao%20Shi%20and%20Yuhan%20Bao%20and%20Zhenshan%20Bing%20and%20Yiyi%20Liao%20and%20Kailun%20Yang%20and%20Kaiwei%20Wang&entry.1292438233=%20%20Estimating%20Neural%20Radiance%20Fields%20%28NeRFs%29%20from%20images%20captured%20under%20optimal%0Aconditions%20has%20been%20extensively%20explored%20in%20the%20vision%20community.%20However%2C%0Arobotic%20applications%20often%20face%20challenges%20such%20as%20motion%20blur%2C%20insufficient%0Aillumination%2C%20and%20high%20computational%20overhead%2C%20which%20adversely%20affect%0Adownstream%20tasks%20like%20navigation%2C%20inspection%2C%20and%20scene%20visualization.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20E-3DGS%2C%20a%20novel%20event-based%20approach%20that%0Apartitions%20events%20into%20motion%20%28from%20camera%20or%20object%20movement%29%20and%20exposure%0A%28from%20camera%20exposure%29%2C%20using%20the%20former%20to%20handle%20fast-motion%20scenes%20and%20using%0Athe%20latter%20to%20reconstruct%20grayscale%20images%20for%20high-quality%20training%20and%0Aoptimization%20of%20event-based%203D%20Gaussian%20Splatting%20%283DGS%29.%20We%20introduce%20a%20novel%0Aintegration%20of%203DGS%20with%20exposure%20events%20for%20high-quality%20reconstruction%20of%0Aexplicit%20scene%20representations.%20Our%20versatile%20framework%20can%20operate%20on%20motion%0Aevents%20alone%20for%203D%20reconstruction%2C%20enhance%20quality%20using%20exposure%20events%2C%20or%0Aadopt%20a%20hybrid%20mode%20that%20balances%20quality%20and%20effectiveness%20by%20optimizing%20with%0Ainitial%20exposure%20events%20followed%20by%20high-speed%20motion%20events.%20We%20also%20introduce%0AEME-3D%2C%20a%20real-world%203D%20dataset%20with%20exposure%20events%2C%20motion%20events%2C%20camera%0Acalibration%20parameters%2C%20and%20sparse%20point%20clouds.%20Our%20method%20is%20faster%20and%0Adelivers%20better%20reconstruction%20quality%20than%20event-based%20NeRF%20while%20being%20more%0Acost-effective%20than%20NeRF%20methods%20that%20combine%20event%20and%20RGB%20data%20by%20using%20a%0Asingle%20event%20sensor.%20By%20combining%20motion%20and%20exposure%20events%2C%20E-3DGS%20sets%20a%20new%0Abenchmark%20for%20event-based%203D%20reconstruction%20with%20robust%20performance%20in%0Achallenging%20conditions%20and%20lower%20hardware%20demands.%20The%20source%20code%20and%20dataset%0Awill%20be%20available%20at%20https%3A//github.com/MasterHow/E-3DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16995v1&entry.124074799=Read"},
{"title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes", "author": "Cheng-De Fan and Chen-Wei Chang and Yi-Ruei Liu and Jie-Ying Lee and Jiun-Long Huang and Yu-Chee Tseng and Yu-Lun Liu", "abstract": "  We present SpectroMotion, a novel approach that combines 3D Gaussian\nSplatting (3DGS) with physically-based rendering (PBR) and deformation fields\nto reconstruct dynamic specular scenes. Previous methods extending 3DGS to\nmodel dynamic scenes have struggled to accurately represent specular surfaces.\nOur method addresses this limitation by introducing a residual correction\ntechnique for accurate surface normal computation during deformation,\ncomplemented by a deformable environment map that adapts to time-varying\nlighting conditions. We implement a coarse-to-fine training strategy that\nsignificantly enhances both scene geometry and specular color prediction. We\ndemonstrate that our model outperforms prior methods for view synthesis of\nscenes containing dynamic specular objects and that it is the only existing\n3DGS method capable of synthesizing photorealistic real-world dynamic specular\nscenes, outperforming state-of-the-art methods in rendering complex, dynamic,\nand specular scenes.\n", "link": "http://arxiv.org/abs/2410.17249v1", "date": "2024-10-22", "relevancy": 3.2387, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6697}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6456}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectroMotion%3A%20Dynamic%203D%20Reconstruction%20of%20Specular%20Scenes&body=Title%3A%20SpectroMotion%3A%20Dynamic%203D%20Reconstruction%20of%20Specular%20Scenes%0AAuthor%3A%20Cheng-De%20Fan%20and%20Chen-Wei%20Chang%20and%20Yi-Ruei%20Liu%20and%20Jie-Ying%20Lee%20and%20Jiun-Long%20Huang%20and%20Yu-Chee%20Tseng%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20We%20present%20SpectroMotion%2C%20a%20novel%20approach%20that%20combines%203D%20Gaussian%0ASplatting%20%283DGS%29%20with%20physically-based%20rendering%20%28PBR%29%20and%20deformation%20fields%0Ato%20reconstruct%20dynamic%20specular%20scenes.%20Previous%20methods%20extending%203DGS%20to%0Amodel%20dynamic%20scenes%20have%20struggled%20to%20accurately%20represent%20specular%20surfaces.%0AOur%20method%20addresses%20this%20limitation%20by%20introducing%20a%20residual%20correction%0Atechnique%20for%20accurate%20surface%20normal%20computation%20during%20deformation%2C%0Acomplemented%20by%20a%20deformable%20environment%20map%20that%20adapts%20to%20time-varying%0Alighting%20conditions.%20We%20implement%20a%20coarse-to-fine%20training%20strategy%20that%0Asignificantly%20enhances%20both%20scene%20geometry%20and%20specular%20color%20prediction.%20We%0Ademonstrate%20that%20our%20model%20outperforms%20prior%20methods%20for%20view%20synthesis%20of%0Ascenes%20containing%20dynamic%20specular%20objects%20and%20that%20it%20is%20the%20only%20existing%0A3DGS%20method%20capable%20of%20synthesizing%20photorealistic%20real-world%20dynamic%20specular%0Ascenes%2C%20outperforming%20state-of-the-art%20methods%20in%20rendering%20complex%2C%20dynamic%2C%0Aand%20specular%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectroMotion%253A%2520Dynamic%25203D%2520Reconstruction%2520of%2520Specular%2520Scenes%26entry.906535625%3DCheng-De%2520Fan%2520and%2520Chen-Wei%2520Chang%2520and%2520Yi-Ruei%2520Liu%2520and%2520Jie-Ying%2520Lee%2520and%2520Jiun-Long%2520Huang%2520and%2520Yu-Chee%2520Tseng%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520SpectroMotion%252C%2520a%2520novel%2520approach%2520that%2520combines%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520with%2520physically-based%2520rendering%2520%2528PBR%2529%2520and%2520deformation%2520fields%250Ato%2520reconstruct%2520dynamic%2520specular%2520scenes.%2520Previous%2520methods%2520extending%25203DGS%2520to%250Amodel%2520dynamic%2520scenes%2520have%2520struggled%2520to%2520accurately%2520represent%2520specular%2520surfaces.%250AOur%2520method%2520addresses%2520this%2520limitation%2520by%2520introducing%2520a%2520residual%2520correction%250Atechnique%2520for%2520accurate%2520surface%2520normal%2520computation%2520during%2520deformation%252C%250Acomplemented%2520by%2520a%2520deformable%2520environment%2520map%2520that%2520adapts%2520to%2520time-varying%250Alighting%2520conditions.%2520We%2520implement%2520a%2520coarse-to-fine%2520training%2520strategy%2520that%250Asignificantly%2520enhances%2520both%2520scene%2520geometry%2520and%2520specular%2520color%2520prediction.%2520We%250Ademonstrate%2520that%2520our%2520model%2520outperforms%2520prior%2520methods%2520for%2520view%2520synthesis%2520of%250Ascenes%2520containing%2520dynamic%2520specular%2520objects%2520and%2520that%2520it%2520is%2520the%2520only%2520existing%250A3DGS%2520method%2520capable%2520of%2520synthesizing%2520photorealistic%2520real-world%2520dynamic%2520specular%250Ascenes%252C%2520outperforming%2520state-of-the-art%2520methods%2520in%2520rendering%2520complex%252C%2520dynamic%252C%250Aand%2520specular%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectroMotion%3A%20Dynamic%203D%20Reconstruction%20of%20Specular%20Scenes&entry.906535625=Cheng-De%20Fan%20and%20Chen-Wei%20Chang%20and%20Yi-Ruei%20Liu%20and%20Jie-Ying%20Lee%20and%20Jiun-Long%20Huang%20and%20Yu-Chee%20Tseng%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20We%20present%20SpectroMotion%2C%20a%20novel%20approach%20that%20combines%203D%20Gaussian%0ASplatting%20%283DGS%29%20with%20physically-based%20rendering%20%28PBR%29%20and%20deformation%20fields%0Ato%20reconstruct%20dynamic%20specular%20scenes.%20Previous%20methods%20extending%203DGS%20to%0Amodel%20dynamic%20scenes%20have%20struggled%20to%20accurately%20represent%20specular%20surfaces.%0AOur%20method%20addresses%20this%20limitation%20by%20introducing%20a%20residual%20correction%0Atechnique%20for%20accurate%20surface%20normal%20computation%20during%20deformation%2C%0Acomplemented%20by%20a%20deformable%20environment%20map%20that%20adapts%20to%20time-varying%0Alighting%20conditions.%20We%20implement%20a%20coarse-to-fine%20training%20strategy%20that%0Asignificantly%20enhances%20both%20scene%20geometry%20and%20specular%20color%20prediction.%20We%0Ademonstrate%20that%20our%20model%20outperforms%20prior%20methods%20for%20view%20synthesis%20of%0Ascenes%20containing%20dynamic%20specular%20objects%20and%20that%20it%20is%20the%20only%20existing%0A3DGS%20method%20capable%20of%20synthesizing%20photorealistic%20real-world%20dynamic%20specular%0Ascenes%2C%20outperforming%20state-of-the-art%20methods%20in%20rendering%20complex%2C%20dynamic%2C%0Aand%20specular%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17249v1&entry.124074799=Read"},
{"title": "Are Visual-Language Models Effective in Action Recognition? A\n  Comparative Study", "author": "Mahmoud Ali and Di Yang and Fran\u00e7ois Br\u00e9mond", "abstract": "  Current vision-language foundation models, such as CLIP, have recently shown\nsignificant improvement in performance across various downstream tasks.\nHowever, whether such foundation models significantly improve more complex\nfine-grained action recognition tasks is still an open question. To answer this\nquestion and better find out the future research direction on human behavior\nanalysis in-the-wild, this paper provides a large-scale study and insight on\ncurrent state-of-the-art vision foundation models by comparing their transfer\nability onto zero-shot and frame-wise action recognition tasks. Extensive\nexperiments are conducted on recent fine-grained, human-centric action\nrecognition datasets (e.g., Toyota Smarthome, Penn Action, UAV-Human, TSU,\nCharades) including action classification and segmentation.\n", "link": "http://arxiv.org/abs/2410.17149v1", "date": "2024-10-22", "relevancy": 3.0971, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Visual-Language%20Models%20Effective%20in%20Action%20Recognition%3F%20A%0A%20%20Comparative%20Study&body=Title%3A%20Are%20Visual-Language%20Models%20Effective%20in%20Action%20Recognition%3F%20A%0A%20%20Comparative%20Study%0AAuthor%3A%20Mahmoud%20Ali%20and%20Di%20Yang%20and%20Fran%C3%A7ois%20Br%C3%A9mond%0AAbstract%3A%20%20%20Current%20vision-language%20foundation%20models%2C%20such%20as%20CLIP%2C%20have%20recently%20shown%0Asignificant%20improvement%20in%20performance%20across%20various%20downstream%20tasks.%0AHowever%2C%20whether%20such%20foundation%20models%20significantly%20improve%20more%20complex%0Afine-grained%20action%20recognition%20tasks%20is%20still%20an%20open%20question.%20To%20answer%20this%0Aquestion%20and%20better%20find%20out%20the%20future%20research%20direction%20on%20human%20behavior%0Aanalysis%20in-the-wild%2C%20this%20paper%20provides%20a%20large-scale%20study%20and%20insight%20on%0Acurrent%20state-of-the-art%20vision%20foundation%20models%20by%20comparing%20their%20transfer%0Aability%20onto%20zero-shot%20and%20frame-wise%20action%20recognition%20tasks.%20Extensive%0Aexperiments%20are%20conducted%20on%20recent%20fine-grained%2C%20human-centric%20action%0Arecognition%20datasets%20%28e.g.%2C%20Toyota%20Smarthome%2C%20Penn%20Action%2C%20UAV-Human%2C%20TSU%2C%0ACharades%29%20including%20action%20classification%20and%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Visual-Language%2520Models%2520Effective%2520in%2520Action%2520Recognition%253F%2520A%250A%2520%2520Comparative%2520Study%26entry.906535625%3DMahmoud%2520Ali%2520and%2520Di%2520Yang%2520and%2520Fran%25C3%25A7ois%2520Br%25C3%25A9mond%26entry.1292438233%3D%2520%2520Current%2520vision-language%2520foundation%2520models%252C%2520such%2520as%2520CLIP%252C%2520have%2520recently%2520shown%250Asignificant%2520improvement%2520in%2520performance%2520across%2520various%2520downstream%2520tasks.%250AHowever%252C%2520whether%2520such%2520foundation%2520models%2520significantly%2520improve%2520more%2520complex%250Afine-grained%2520action%2520recognition%2520tasks%2520is%2520still%2520an%2520open%2520question.%2520To%2520answer%2520this%250Aquestion%2520and%2520better%2520find%2520out%2520the%2520future%2520research%2520direction%2520on%2520human%2520behavior%250Aanalysis%2520in-the-wild%252C%2520this%2520paper%2520provides%2520a%2520large-scale%2520study%2520and%2520insight%2520on%250Acurrent%2520state-of-the-art%2520vision%2520foundation%2520models%2520by%2520comparing%2520their%2520transfer%250Aability%2520onto%2520zero-shot%2520and%2520frame-wise%2520action%2520recognition%2520tasks.%2520Extensive%250Aexperiments%2520are%2520conducted%2520on%2520recent%2520fine-grained%252C%2520human-centric%2520action%250Arecognition%2520datasets%2520%2528e.g.%252C%2520Toyota%2520Smarthome%252C%2520Penn%2520Action%252C%2520UAV-Human%252C%2520TSU%252C%250ACharades%2529%2520including%2520action%2520classification%2520and%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Visual-Language%20Models%20Effective%20in%20Action%20Recognition%3F%20A%0A%20%20Comparative%20Study&entry.906535625=Mahmoud%20Ali%20and%20Di%20Yang%20and%20Fran%C3%A7ois%20Br%C3%A9mond&entry.1292438233=%20%20Current%20vision-language%20foundation%20models%2C%20such%20as%20CLIP%2C%20have%20recently%20shown%0Asignificant%20improvement%20in%20performance%20across%20various%20downstream%20tasks.%0AHowever%2C%20whether%20such%20foundation%20models%20significantly%20improve%20more%20complex%0Afine-grained%20action%20recognition%20tasks%20is%20still%20an%20open%20question.%20To%20answer%20this%0Aquestion%20and%20better%20find%20out%20the%20future%20research%20direction%20on%20human%20behavior%0Aanalysis%20in-the-wild%2C%20this%20paper%20provides%20a%20large-scale%20study%20and%20insight%20on%0Acurrent%20state-of-the-art%20vision%20foundation%20models%20by%20comparing%20their%20transfer%0Aability%20onto%20zero-shot%20and%20frame-wise%20action%20recognition%20tasks.%20Extensive%0Aexperiments%20are%20conducted%20on%20recent%20fine-grained%2C%20human-centric%20action%0Arecognition%20datasets%20%28e.g.%2C%20Toyota%20Smarthome%2C%20Penn%20Action%2C%20UAV-Human%2C%20TSU%2C%0ACharades%29%20including%20action%20classification%20and%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17149v1&entry.124074799=Read"},
{"title": "3D-TAFS: A Training-free Framework for 3D Affordance Segmentation", "author": "Meng Chu and Xuan Zhang and Zhedong Zheng and Tat-Seng Chua", "abstract": "  Translating high-level linguistic instructions into precise robotic actions\nin the physical world remains challenging, particularly when considering the\nfeasibility of interacting with 3D objects. In this paper, we introduce\n3D-TAFS, a novel training-free multimodal framework for 3D affordance\nsegmentation, alongside a benchmark for evaluating interactive language-guided\naffordance in everyday environments. In particular, our framework integrates a\nlarge multimodal model with a specialized 3D vision network, enabling seamless\nfusion of 2D and 3D visual understanding with language comprehension. To\nfacilitate evaluation, we present a dataset of ten typical indoor environments,\neach with 50 images annotated for object actions and 3D affordance\nsegmentation. Extensive experiments validate the proposed 3D-TAFS's capability\nin handling interactive 3D affordance segmentation tasks across diverse\nsettings, showcasing competitive performance across various metrics. Our\nresults highlight 3D-TAFS's potential for enhancing human-robot interaction\nbased on affordance understanding in complex indoor environments, advancing the\ndevelopment of more intuitive and efficient robotic frameworks for real-world\napplications.\n", "link": "http://arxiv.org/abs/2409.10078v4", "date": "2024-10-22", "relevancy": 3.0896, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-TAFS%3A%20A%20Training-free%20Framework%20for%203D%20Affordance%20Segmentation&body=Title%3A%203D-TAFS%3A%20A%20Training-free%20Framework%20for%203D%20Affordance%20Segmentation%0AAuthor%3A%20Meng%20Chu%20and%20Xuan%20Zhang%20and%20Zhedong%20Zheng%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Translating%20high-level%20linguistic%20instructions%20into%20precise%20robotic%20actions%0Ain%20the%20physical%20world%20remains%20challenging%2C%20particularly%20when%20considering%20the%0Afeasibility%20of%20interacting%20with%203D%20objects.%20In%20this%20paper%2C%20we%20introduce%0A3D-TAFS%2C%20a%20novel%20training-free%20multimodal%20framework%20for%203D%20affordance%0Asegmentation%2C%20alongside%20a%20benchmark%20for%20evaluating%20interactive%20language-guided%0Aaffordance%20in%20everyday%20environments.%20In%20particular%2C%20our%20framework%20integrates%20a%0Alarge%20multimodal%20model%20with%20a%20specialized%203D%20vision%20network%2C%20enabling%20seamless%0Afusion%20of%202D%20and%203D%20visual%20understanding%20with%20language%20comprehension.%20To%0Afacilitate%20evaluation%2C%20we%20present%20a%20dataset%20of%20ten%20typical%20indoor%20environments%2C%0Aeach%20with%2050%20images%20annotated%20for%20object%20actions%20and%203D%20affordance%0Asegmentation.%20Extensive%20experiments%20validate%20the%20proposed%203D-TAFS%27s%20capability%0Ain%20handling%20interactive%203D%20affordance%20segmentation%20tasks%20across%20diverse%0Asettings%2C%20showcasing%20competitive%20performance%20across%20various%20metrics.%20Our%0Aresults%20highlight%203D-TAFS%27s%20potential%20for%20enhancing%20human-robot%20interaction%0Abased%20on%20affordance%20understanding%20in%20complex%20indoor%20environments%2C%20advancing%20the%0Adevelopment%20of%20more%20intuitive%20and%20efficient%20robotic%20frameworks%20for%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10078v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-TAFS%253A%2520A%2520Training-free%2520Framework%2520for%25203D%2520Affordance%2520Segmentation%26entry.906535625%3DMeng%2520Chu%2520and%2520Xuan%2520Zhang%2520and%2520Zhedong%2520Zheng%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Translating%2520high-level%2520linguistic%2520instructions%2520into%2520precise%2520robotic%2520actions%250Ain%2520the%2520physical%2520world%2520remains%2520challenging%252C%2520particularly%2520when%2520considering%2520the%250Afeasibility%2520of%2520interacting%2520with%25203D%2520objects.%2520In%2520this%2520paper%252C%2520we%2520introduce%250A3D-TAFS%252C%2520a%2520novel%2520training-free%2520multimodal%2520framework%2520for%25203D%2520affordance%250Asegmentation%252C%2520alongside%2520a%2520benchmark%2520for%2520evaluating%2520interactive%2520language-guided%250Aaffordance%2520in%2520everyday%2520environments.%2520In%2520particular%252C%2520our%2520framework%2520integrates%2520a%250Alarge%2520multimodal%2520model%2520with%2520a%2520specialized%25203D%2520vision%2520network%252C%2520enabling%2520seamless%250Afusion%2520of%25202D%2520and%25203D%2520visual%2520understanding%2520with%2520language%2520comprehension.%2520To%250Afacilitate%2520evaluation%252C%2520we%2520present%2520a%2520dataset%2520of%2520ten%2520typical%2520indoor%2520environments%252C%250Aeach%2520with%252050%2520images%2520annotated%2520for%2520object%2520actions%2520and%25203D%2520affordance%250Asegmentation.%2520Extensive%2520experiments%2520validate%2520the%2520proposed%25203D-TAFS%2527s%2520capability%250Ain%2520handling%2520interactive%25203D%2520affordance%2520segmentation%2520tasks%2520across%2520diverse%250Asettings%252C%2520showcasing%2520competitive%2520performance%2520across%2520various%2520metrics.%2520Our%250Aresults%2520highlight%25203D-TAFS%2527s%2520potential%2520for%2520enhancing%2520human-robot%2520interaction%250Abased%2520on%2520affordance%2520understanding%2520in%2520complex%2520indoor%2520environments%252C%2520advancing%2520the%250Adevelopment%2520of%2520more%2520intuitive%2520and%2520efficient%2520robotic%2520frameworks%2520for%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10078v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-TAFS%3A%20A%20Training-free%20Framework%20for%203D%20Affordance%20Segmentation&entry.906535625=Meng%20Chu%20and%20Xuan%20Zhang%20and%20Zhedong%20Zheng%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Translating%20high-level%20linguistic%20instructions%20into%20precise%20robotic%20actions%0Ain%20the%20physical%20world%20remains%20challenging%2C%20particularly%20when%20considering%20the%0Afeasibility%20of%20interacting%20with%203D%20objects.%20In%20this%20paper%2C%20we%20introduce%0A3D-TAFS%2C%20a%20novel%20training-free%20multimodal%20framework%20for%203D%20affordance%0Asegmentation%2C%20alongside%20a%20benchmark%20for%20evaluating%20interactive%20language-guided%0Aaffordance%20in%20everyday%20environments.%20In%20particular%2C%20our%20framework%20integrates%20a%0Alarge%20multimodal%20model%20with%20a%20specialized%203D%20vision%20network%2C%20enabling%20seamless%0Afusion%20of%202D%20and%203D%20visual%20understanding%20with%20language%20comprehension.%20To%0Afacilitate%20evaluation%2C%20we%20present%20a%20dataset%20of%20ten%20typical%20indoor%20environments%2C%0Aeach%20with%2050%20images%20annotated%20for%20object%20actions%20and%203D%20affordance%0Asegmentation.%20Extensive%20experiments%20validate%20the%20proposed%203D-TAFS%27s%20capability%0Ain%20handling%20interactive%203D%20affordance%20segmentation%20tasks%20across%20diverse%0Asettings%2C%20showcasing%20competitive%20performance%20across%20various%20metrics.%20Our%0Aresults%20highlight%203D-TAFS%27s%20potential%20for%20enhancing%20human-robot%20interaction%0Abased%20on%20affordance%20understanding%20in%20complex%20indoor%20environments%2C%20advancing%20the%0Adevelopment%20of%20more%20intuitive%20and%20efficient%20robotic%20frameworks%20for%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10078v4&entry.124074799=Read"},
{"title": "Incremental Joint Learning of Depth, Pose and Implicit Scene\n  Representation on Monocular Camera in Large-scale Scenes", "author": "Tianchen Deng and Nailin Wang and Chongdi Wang and Shenghai Yuan and Jingchuan Wang and Danwei Wang and Weidong Chen", "abstract": "  Dense scene reconstruction for photo-realistic view synthesis has various\napplications, such as VR/AR, autonomous vehicles. However, most existing\nmethods have difficulties in large-scale scenes due to three core challenges:\n\\textit{(a) inaccurate depth input.} Accurate depth input is impossible to get\nin real-world large-scale scenes. \\textit{(b) inaccurate pose estimation.} Most\nexisting approaches rely on accurate pre-estimated camera poses. \\textit{(c)\ninsufficient scene representation capability.} A single global radiance field\nlacks the capacity to effectively scale to large-scale scenes. To this end, we\npropose an incremental joint learning framework, which can achieve accurate\ndepth, pose estimation, and large-scale scene reconstruction. A vision\ntransformer-based network is adopted as the backbone to enhance performance in\nscale information estimation. For pose estimation, a feature-metric bundle\nadjustment (FBA) method is designed for accurate and robust camera tracking in\nlarge-scale scenes. In terms of implicit scene representation, we propose an\nincremental scene representation method to construct the entire large-scale\nscene as multiple local radiance fields to enhance the scalability of 3D scene\nrepresentation. Extended experiments have been conducted to demonstrate the\neffectiveness and accuracy of our method in depth estimation, pose estimation,\nand large-scale scene reconstruction.\n", "link": "http://arxiv.org/abs/2404.06050v2", "date": "2024-10-22", "relevancy": 3.075, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6164}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Joint%20Learning%20of%20Depth%2C%20Pose%20and%20Implicit%20Scene%0A%20%20Representation%20on%20Monocular%20Camera%20in%20Large-scale%20Scenes&body=Title%3A%20Incremental%20Joint%20Learning%20of%20Depth%2C%20Pose%20and%20Implicit%20Scene%0A%20%20Representation%20on%20Monocular%20Camera%20in%20Large-scale%20Scenes%0AAuthor%3A%20Tianchen%20Deng%20and%20Nailin%20Wang%20and%20Chongdi%20Wang%20and%20Shenghai%20Yuan%20and%20Jingchuan%20Wang%20and%20Danwei%20Wang%20and%20Weidong%20Chen%0AAbstract%3A%20%20%20Dense%20scene%20reconstruction%20for%20photo-realistic%20view%20synthesis%20has%20various%0Aapplications%2C%20such%20as%20VR/AR%2C%20autonomous%20vehicles.%20However%2C%20most%20existing%0Amethods%20have%20difficulties%20in%20large-scale%20scenes%20due%20to%20three%20core%20challenges%3A%0A%5Ctextit%7B%28a%29%20inaccurate%20depth%20input.%7D%20Accurate%20depth%20input%20is%20impossible%20to%20get%0Ain%20real-world%20large-scale%20scenes.%20%5Ctextit%7B%28b%29%20inaccurate%20pose%20estimation.%7D%20Most%0Aexisting%20approaches%20rely%20on%20accurate%20pre-estimated%20camera%20poses.%20%5Ctextit%7B%28c%29%0Ainsufficient%20scene%20representation%20capability.%7D%20A%20single%20global%20radiance%20field%0Alacks%20the%20capacity%20to%20effectively%20scale%20to%20large-scale%20scenes.%20To%20this%20end%2C%20we%0Apropose%20an%20incremental%20joint%20learning%20framework%2C%20which%20can%20achieve%20accurate%0Adepth%2C%20pose%20estimation%2C%20and%20large-scale%20scene%20reconstruction.%20A%20vision%0Atransformer-based%20network%20is%20adopted%20as%20the%20backbone%20to%20enhance%20performance%20in%0Ascale%20information%20estimation.%20For%20pose%20estimation%2C%20a%20feature-metric%20bundle%0Aadjustment%20%28FBA%29%20method%20is%20designed%20for%20accurate%20and%20robust%20camera%20tracking%20in%0Alarge-scale%20scenes.%20In%20terms%20of%20implicit%20scene%20representation%2C%20we%20propose%20an%0Aincremental%20scene%20representation%20method%20to%20construct%20the%20entire%20large-scale%0Ascene%20as%20multiple%20local%20radiance%20fields%20to%20enhance%20the%20scalability%20of%203D%20scene%0Arepresentation.%20Extended%20experiments%20have%20been%20conducted%20to%20demonstrate%20the%0Aeffectiveness%20and%20accuracy%20of%20our%20method%20in%20depth%20estimation%2C%20pose%20estimation%2C%0Aand%20large-scale%20scene%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Joint%2520Learning%2520of%2520Depth%252C%2520Pose%2520and%2520Implicit%2520Scene%250A%2520%2520Representation%2520on%2520Monocular%2520Camera%2520in%2520Large-scale%2520Scenes%26entry.906535625%3DTianchen%2520Deng%2520and%2520Nailin%2520Wang%2520and%2520Chongdi%2520Wang%2520and%2520Shenghai%2520Yuan%2520and%2520Jingchuan%2520Wang%2520and%2520Danwei%2520Wang%2520and%2520Weidong%2520Chen%26entry.1292438233%3D%2520%2520Dense%2520scene%2520reconstruction%2520for%2520photo-realistic%2520view%2520synthesis%2520has%2520various%250Aapplications%252C%2520such%2520as%2520VR/AR%252C%2520autonomous%2520vehicles.%2520However%252C%2520most%2520existing%250Amethods%2520have%2520difficulties%2520in%2520large-scale%2520scenes%2520due%2520to%2520three%2520core%2520challenges%253A%250A%255Ctextit%257B%2528a%2529%2520inaccurate%2520depth%2520input.%257D%2520Accurate%2520depth%2520input%2520is%2520impossible%2520to%2520get%250Ain%2520real-world%2520large-scale%2520scenes.%2520%255Ctextit%257B%2528b%2529%2520inaccurate%2520pose%2520estimation.%257D%2520Most%250Aexisting%2520approaches%2520rely%2520on%2520accurate%2520pre-estimated%2520camera%2520poses.%2520%255Ctextit%257B%2528c%2529%250Ainsufficient%2520scene%2520representation%2520capability.%257D%2520A%2520single%2520global%2520radiance%2520field%250Alacks%2520the%2520capacity%2520to%2520effectively%2520scale%2520to%2520large-scale%2520scenes.%2520To%2520this%2520end%252C%2520we%250Apropose%2520an%2520incremental%2520joint%2520learning%2520framework%252C%2520which%2520can%2520achieve%2520accurate%250Adepth%252C%2520pose%2520estimation%252C%2520and%2520large-scale%2520scene%2520reconstruction.%2520A%2520vision%250Atransformer-based%2520network%2520is%2520adopted%2520as%2520the%2520backbone%2520to%2520enhance%2520performance%2520in%250Ascale%2520information%2520estimation.%2520For%2520pose%2520estimation%252C%2520a%2520feature-metric%2520bundle%250Aadjustment%2520%2528FBA%2529%2520method%2520is%2520designed%2520for%2520accurate%2520and%2520robust%2520camera%2520tracking%2520in%250Alarge-scale%2520scenes.%2520In%2520terms%2520of%2520implicit%2520scene%2520representation%252C%2520we%2520propose%2520an%250Aincremental%2520scene%2520representation%2520method%2520to%2520construct%2520the%2520entire%2520large-scale%250Ascene%2520as%2520multiple%2520local%2520radiance%2520fields%2520to%2520enhance%2520the%2520scalability%2520of%25203D%2520scene%250Arepresentation.%2520Extended%2520experiments%2520have%2520been%2520conducted%2520to%2520demonstrate%2520the%250Aeffectiveness%2520and%2520accuracy%2520of%2520our%2520method%2520in%2520depth%2520estimation%252C%2520pose%2520estimation%252C%250Aand%2520large-scale%2520scene%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Joint%20Learning%20of%20Depth%2C%20Pose%20and%20Implicit%20Scene%0A%20%20Representation%20on%20Monocular%20Camera%20in%20Large-scale%20Scenes&entry.906535625=Tianchen%20Deng%20and%20Nailin%20Wang%20and%20Chongdi%20Wang%20and%20Shenghai%20Yuan%20and%20Jingchuan%20Wang%20and%20Danwei%20Wang%20and%20Weidong%20Chen&entry.1292438233=%20%20Dense%20scene%20reconstruction%20for%20photo-realistic%20view%20synthesis%20has%20various%0Aapplications%2C%20such%20as%20VR/AR%2C%20autonomous%20vehicles.%20However%2C%20most%20existing%0Amethods%20have%20difficulties%20in%20large-scale%20scenes%20due%20to%20three%20core%20challenges%3A%0A%5Ctextit%7B%28a%29%20inaccurate%20depth%20input.%7D%20Accurate%20depth%20input%20is%20impossible%20to%20get%0Ain%20real-world%20large-scale%20scenes.%20%5Ctextit%7B%28b%29%20inaccurate%20pose%20estimation.%7D%20Most%0Aexisting%20approaches%20rely%20on%20accurate%20pre-estimated%20camera%20poses.%20%5Ctextit%7B%28c%29%0Ainsufficient%20scene%20representation%20capability.%7D%20A%20single%20global%20radiance%20field%0Alacks%20the%20capacity%20to%20effectively%20scale%20to%20large-scale%20scenes.%20To%20this%20end%2C%20we%0Apropose%20an%20incremental%20joint%20learning%20framework%2C%20which%20can%20achieve%20accurate%0Adepth%2C%20pose%20estimation%2C%20and%20large-scale%20scene%20reconstruction.%20A%20vision%0Atransformer-based%20network%20is%20adopted%20as%20the%20backbone%20to%20enhance%20performance%20in%0Ascale%20information%20estimation.%20For%20pose%20estimation%2C%20a%20feature-metric%20bundle%0Aadjustment%20%28FBA%29%20method%20is%20designed%20for%20accurate%20and%20robust%20camera%20tracking%20in%0Alarge-scale%20scenes.%20In%20terms%20of%20implicit%20scene%20representation%2C%20we%20propose%20an%0Aincremental%20scene%20representation%20method%20to%20construct%20the%20entire%20large-scale%0Ascene%20as%20multiple%20local%20radiance%20fields%20to%20enhance%20the%20scalability%20of%203D%20scene%0Arepresentation.%20Extended%20experiments%20have%20been%20conducted%20to%20demonstrate%20the%0Aeffectiveness%20and%20accuracy%20of%20our%20method%20in%20depth%20estimation%2C%20pose%20estimation%2C%0Aand%20large-scale%20scene%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06050v2&entry.124074799=Read"},
{"title": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary\n  Resolution", "author": "Zuyan Liu and Yuhao Dong and Ziwei Liu and Winston Hu and Jiwen Lu and Yongming Rao", "abstract": "  Visual data comes in various forms, ranging from small icons of just a few\npixels to long videos spanning hours. Existing multi-modal LLMs usually\nstandardize these diverse visual inputs to a fixed resolution for visual\nencoders and yield similar numbers of tokens for LLMs. This approach is\nnon-optimal for multimodal understanding and inefficient for processing inputs\nwith long and short visual contents. To solve the problem, we propose Oryx, a\nunified multimodal architecture for the spatial-temporal understanding of\nimages, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to\nseamlessly and efficiently process visual inputs with arbitrary spatial sizes\nand temporal lengths through two core innovations: 1) a pre-trained OryxViT\nmodel that can encode images at any resolution into LLM-friendly visual\nrepresentations; 2) a dynamic compressor module that supports 1x to 16x\ncompression on visual tokens by request. These design features enable Oryx to\naccommodate extremely long visual contexts, such as videos, with lower\nresolution and high compression while maintaining high recognition precision\nfor tasks like document understanding with native resolution and no\ncompression. Beyond the architectural improvements, enhanced data curation and\nspecialized training on long-context retrieval and spatial-aware data help Oryx\nachieve strong capabilities in image, video, and 3D multimodal understanding\nsimultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.\n", "link": "http://arxiv.org/abs/2409.12961v2", "date": "2024-10-22", "relevancy": 3.0425, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6149}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Oryx%20MLLM%3A%20On-Demand%20Spatial-Temporal%20Understanding%20at%20Arbitrary%0A%20%20Resolution&body=Title%3A%20Oryx%20MLLM%3A%20On-Demand%20Spatial-Temporal%20Understanding%20at%20Arbitrary%0A%20%20Resolution%0AAuthor%3A%20Zuyan%20Liu%20and%20Yuhao%20Dong%20and%20Ziwei%20Liu%20and%20Winston%20Hu%20and%20Jiwen%20Lu%20and%20Yongming%20Rao%0AAbstract%3A%20%20%20Visual%20data%20comes%20in%20various%20forms%2C%20ranging%20from%20small%20icons%20of%20just%20a%20few%0Apixels%20to%20long%20videos%20spanning%20hours.%20Existing%20multi-modal%20LLMs%20usually%0Astandardize%20these%20diverse%20visual%20inputs%20to%20a%20fixed%20resolution%20for%20visual%0Aencoders%20and%20yield%20similar%20numbers%20of%20tokens%20for%20LLMs.%20This%20approach%20is%0Anon-optimal%20for%20multimodal%20understanding%20and%20inefficient%20for%20processing%20inputs%0Awith%20long%20and%20short%20visual%20contents.%20To%20solve%20the%20problem%2C%20we%20propose%20Oryx%2C%20a%0Aunified%20multimodal%20architecture%20for%20the%20spatial-temporal%20understanding%20of%0Aimages%2C%20videos%2C%20and%20multi-view%203D%20scenes.%20Oryx%20offers%20an%20on-demand%20solution%20to%0Aseamlessly%20and%20efficiently%20process%20visual%20inputs%20with%20arbitrary%20spatial%20sizes%0Aand%20temporal%20lengths%20through%20two%20core%20innovations%3A%201%29%20a%20pre-trained%20OryxViT%0Amodel%20that%20can%20encode%20images%20at%20any%20resolution%20into%20LLM-friendly%20visual%0Arepresentations%3B%202%29%20a%20dynamic%20compressor%20module%20that%20supports%201x%20to%2016x%0Acompression%20on%20visual%20tokens%20by%20request.%20These%20design%20features%20enable%20Oryx%20to%0Aaccommodate%20extremely%20long%20visual%20contexts%2C%20such%20as%20videos%2C%20with%20lower%0Aresolution%20and%20high%20compression%20while%20maintaining%20high%20recognition%20precision%0Afor%20tasks%20like%20document%20understanding%20with%20native%20resolution%20and%20no%0Acompression.%20Beyond%20the%20architectural%20improvements%2C%20enhanced%20data%20curation%20and%0Aspecialized%20training%20on%20long-context%20retrieval%20and%20spatial-aware%20data%20help%20Oryx%0Aachieve%20strong%20capabilities%20in%20image%2C%20video%2C%20and%203D%20multimodal%20understanding%0Asimultaneously.%20Our%20work%20is%20open-sourced%20at%20https%3A//github.com/Oryx-mllm/Oryx.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOryx%2520MLLM%253A%2520On-Demand%2520Spatial-Temporal%2520Understanding%2520at%2520Arbitrary%250A%2520%2520Resolution%26entry.906535625%3DZuyan%2520Liu%2520and%2520Yuhao%2520Dong%2520and%2520Ziwei%2520Liu%2520and%2520Winston%2520Hu%2520and%2520Jiwen%2520Lu%2520and%2520Yongming%2520Rao%26entry.1292438233%3D%2520%2520Visual%2520data%2520comes%2520in%2520various%2520forms%252C%2520ranging%2520from%2520small%2520icons%2520of%2520just%2520a%2520few%250Apixels%2520to%2520long%2520videos%2520spanning%2520hours.%2520Existing%2520multi-modal%2520LLMs%2520usually%250Astandardize%2520these%2520diverse%2520visual%2520inputs%2520to%2520a%2520fixed%2520resolution%2520for%2520visual%250Aencoders%2520and%2520yield%2520similar%2520numbers%2520of%2520tokens%2520for%2520LLMs.%2520This%2520approach%2520is%250Anon-optimal%2520for%2520multimodal%2520understanding%2520and%2520inefficient%2520for%2520processing%2520inputs%250Awith%2520long%2520and%2520short%2520visual%2520contents.%2520To%2520solve%2520the%2520problem%252C%2520we%2520propose%2520Oryx%252C%2520a%250Aunified%2520multimodal%2520architecture%2520for%2520the%2520spatial-temporal%2520understanding%2520of%250Aimages%252C%2520videos%252C%2520and%2520multi-view%25203D%2520scenes.%2520Oryx%2520offers%2520an%2520on-demand%2520solution%2520to%250Aseamlessly%2520and%2520efficiently%2520process%2520visual%2520inputs%2520with%2520arbitrary%2520spatial%2520sizes%250Aand%2520temporal%2520lengths%2520through%2520two%2520core%2520innovations%253A%25201%2529%2520a%2520pre-trained%2520OryxViT%250Amodel%2520that%2520can%2520encode%2520images%2520at%2520any%2520resolution%2520into%2520LLM-friendly%2520visual%250Arepresentations%253B%25202%2529%2520a%2520dynamic%2520compressor%2520module%2520that%2520supports%25201x%2520to%252016x%250Acompression%2520on%2520visual%2520tokens%2520by%2520request.%2520These%2520design%2520features%2520enable%2520Oryx%2520to%250Aaccommodate%2520extremely%2520long%2520visual%2520contexts%252C%2520such%2520as%2520videos%252C%2520with%2520lower%250Aresolution%2520and%2520high%2520compression%2520while%2520maintaining%2520high%2520recognition%2520precision%250Afor%2520tasks%2520like%2520document%2520understanding%2520with%2520native%2520resolution%2520and%2520no%250Acompression.%2520Beyond%2520the%2520architectural%2520improvements%252C%2520enhanced%2520data%2520curation%2520and%250Aspecialized%2520training%2520on%2520long-context%2520retrieval%2520and%2520spatial-aware%2520data%2520help%2520Oryx%250Aachieve%2520strong%2520capabilities%2520in%2520image%252C%2520video%252C%2520and%25203D%2520multimodal%2520understanding%250Asimultaneously.%2520Our%2520work%2520is%2520open-sourced%2520at%2520https%253A//github.com/Oryx-mllm/Oryx.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Oryx%20MLLM%3A%20On-Demand%20Spatial-Temporal%20Understanding%20at%20Arbitrary%0A%20%20Resolution&entry.906535625=Zuyan%20Liu%20and%20Yuhao%20Dong%20and%20Ziwei%20Liu%20and%20Winston%20Hu%20and%20Jiwen%20Lu%20and%20Yongming%20Rao&entry.1292438233=%20%20Visual%20data%20comes%20in%20various%20forms%2C%20ranging%20from%20small%20icons%20of%20just%20a%20few%0Apixels%20to%20long%20videos%20spanning%20hours.%20Existing%20multi-modal%20LLMs%20usually%0Astandardize%20these%20diverse%20visual%20inputs%20to%20a%20fixed%20resolution%20for%20visual%0Aencoders%20and%20yield%20similar%20numbers%20of%20tokens%20for%20LLMs.%20This%20approach%20is%0Anon-optimal%20for%20multimodal%20understanding%20and%20inefficient%20for%20processing%20inputs%0Awith%20long%20and%20short%20visual%20contents.%20To%20solve%20the%20problem%2C%20we%20propose%20Oryx%2C%20a%0Aunified%20multimodal%20architecture%20for%20the%20spatial-temporal%20understanding%20of%0Aimages%2C%20videos%2C%20and%20multi-view%203D%20scenes.%20Oryx%20offers%20an%20on-demand%20solution%20to%0Aseamlessly%20and%20efficiently%20process%20visual%20inputs%20with%20arbitrary%20spatial%20sizes%0Aand%20temporal%20lengths%20through%20two%20core%20innovations%3A%201%29%20a%20pre-trained%20OryxViT%0Amodel%20that%20can%20encode%20images%20at%20any%20resolution%20into%20LLM-friendly%20visual%0Arepresentations%3B%202%29%20a%20dynamic%20compressor%20module%20that%20supports%201x%20to%2016x%0Acompression%20on%20visual%20tokens%20by%20request.%20These%20design%20features%20enable%20Oryx%20to%0Aaccommodate%20extremely%20long%20visual%20contexts%2C%20such%20as%20videos%2C%20with%20lower%0Aresolution%20and%20high%20compression%20while%20maintaining%20high%20recognition%20precision%0Afor%20tasks%20like%20document%20understanding%20with%20native%20resolution%20and%20no%0Acompression.%20Beyond%20the%20architectural%20improvements%2C%20enhanced%20data%20curation%20and%0Aspecialized%20training%20on%20long-context%20retrieval%20and%20spatial-aware%20data%20help%20Oryx%0Aachieve%20strong%20capabilities%20in%20image%2C%20video%2C%20and%203D%20multimodal%20understanding%0Asimultaneously.%20Our%20work%20is%20open-sourced%20at%20https%3A//github.com/Oryx-mllm/Oryx.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12961v2&entry.124074799=Read"},
{"title": "Context and Geometry Aware Voxel Transformer for Semantic Scene\n  Completion", "author": "Zhu Yu and Runmin Zhang and Jiacheng Ying and Junchen Yu and Xiaohai Hu and Lun Luo and Si-Yuan Cao and Hui-Liang Shen", "abstract": "  Vision-based Semantic Scene Completion (SSC) has gained much attention due to\nits widespread applications in various 3D perception tasks. Existing\nsparse-to-dense approaches typically employ shared context-independent queries\nacross various input images, which fails to capture distinctions among them as\nthe focal regions of different inputs vary and may result in undirected feature\naggregation of cross-attention. Additionally, the absence of depth information\nmay lead to points projected onto the image plane sharing the same 2D position\nor similar sampling points in the feature map, resulting in depth ambiguity. In\nthis paper, we present a novel context and geometry aware voxel transformer. It\nutilizes a context aware query generator to initialize context-dependent\nqueries tailored to individual input images, effectively capturing their unique\ncharacteristics and aggregating information within the region of interest.\nFurthermore, it extend deformable cross-attention from 2D to 3D pixel space,\nenabling the differentiation of points with similar image coordinates based on\ntheir depth coordinates. Building upon this module, we introduce a neural\nnetwork named CGFormer to achieve semantic scene completion. Simultaneously,\nCGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost\nthe semantic and geometric representation abilities of the transformed 3D\nvolume from both local and global perspectives. Experimental results\ndemonstrate that CGFormer achieves state-of-the-art performance on the\nSemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and\n20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer\neven outperforms approaches employing temporal images as inputs or much larger\nimage backbone networks.\n", "link": "http://arxiv.org/abs/2405.13675v4", "date": "2024-10-22", "relevancy": 2.9833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5986}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20and%20Geometry%20Aware%20Voxel%20Transformer%20for%20Semantic%20Scene%0A%20%20Completion&body=Title%3A%20Context%20and%20Geometry%20Aware%20Voxel%20Transformer%20for%20Semantic%20Scene%0A%20%20Completion%0AAuthor%3A%20Zhu%20Yu%20and%20Runmin%20Zhang%20and%20Jiacheng%20Ying%20and%20Junchen%20Yu%20and%20Xiaohai%20Hu%20and%20Lun%20Luo%20and%20Si-Yuan%20Cao%20and%20Hui-Liang%20Shen%0AAbstract%3A%20%20%20Vision-based%20Semantic%20Scene%20Completion%20%28SSC%29%20has%20gained%20much%20attention%20due%20to%0Aits%20widespread%20applications%20in%20various%203D%20perception%20tasks.%20Existing%0Asparse-to-dense%20approaches%20typically%20employ%20shared%20context-independent%20queries%0Aacross%20various%20input%20images%2C%20which%20fails%20to%20capture%20distinctions%20among%20them%20as%0Athe%20focal%20regions%20of%20different%20inputs%20vary%20and%20may%20result%20in%20undirected%20feature%0Aaggregation%20of%20cross-attention.%20Additionally%2C%20the%20absence%20of%20depth%20information%0Amay%20lead%20to%20points%20projected%20onto%20the%20image%20plane%20sharing%20the%20same%202D%20position%0Aor%20similar%20sampling%20points%20in%20the%20feature%20map%2C%20resulting%20in%20depth%20ambiguity.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20context%20and%20geometry%20aware%20voxel%20transformer.%20It%0Autilizes%20a%20context%20aware%20query%20generator%20to%20initialize%20context-dependent%0Aqueries%20tailored%20to%20individual%20input%20images%2C%20effectively%20capturing%20their%20unique%0Acharacteristics%20and%20aggregating%20information%20within%20the%20region%20of%20interest.%0AFurthermore%2C%20it%20extend%20deformable%20cross-attention%20from%202D%20to%203D%20pixel%20space%2C%0Aenabling%20the%20differentiation%20of%20points%20with%20similar%20image%20coordinates%20based%20on%0Atheir%20depth%20coordinates.%20Building%20upon%20this%20module%2C%20we%20introduce%20a%20neural%0Anetwork%20named%20CGFormer%20to%20achieve%20semantic%20scene%20completion.%20Simultaneously%2C%0ACGFormer%20leverages%20multiple%203D%20representations%20%28i.e.%2C%20voxel%20and%20TPV%29%20to%20boost%0Athe%20semantic%20and%20geometric%20representation%20abilities%20of%20the%20transformed%203D%0Avolume%20from%20both%20local%20and%20global%20perspectives.%20Experimental%20results%0Ademonstrate%20that%20CGFormer%20achieves%20state-of-the-art%20performance%20on%20the%0ASemanticKITTI%20and%20SSCBench-KITTI-360%20benchmarks%2C%20attaining%20a%20mIoU%20of%2016.87%20and%0A20.05%2C%20as%20well%20as%20an%20IoU%20of%2045.99%20and%2048.07%2C%20respectively.%20Remarkably%2C%20CGFormer%0Aeven%20outperforms%20approaches%20employing%20temporal%20images%20as%20inputs%20or%20much%20larger%0Aimage%20backbone%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13675v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520and%2520Geometry%2520Aware%2520Voxel%2520Transformer%2520for%2520Semantic%2520Scene%250A%2520%2520Completion%26entry.906535625%3DZhu%2520Yu%2520and%2520Runmin%2520Zhang%2520and%2520Jiacheng%2520Ying%2520and%2520Junchen%2520Yu%2520and%2520Xiaohai%2520Hu%2520and%2520Lun%2520Luo%2520and%2520Si-Yuan%2520Cao%2520and%2520Hui-Liang%2520Shen%26entry.1292438233%3D%2520%2520Vision-based%2520Semantic%2520Scene%2520Completion%2520%2528SSC%2529%2520has%2520gained%2520much%2520attention%2520due%2520to%250Aits%2520widespread%2520applications%2520in%2520various%25203D%2520perception%2520tasks.%2520Existing%250Asparse-to-dense%2520approaches%2520typically%2520employ%2520shared%2520context-independent%2520queries%250Aacross%2520various%2520input%2520images%252C%2520which%2520fails%2520to%2520capture%2520distinctions%2520among%2520them%2520as%250Athe%2520focal%2520regions%2520of%2520different%2520inputs%2520vary%2520and%2520may%2520result%2520in%2520undirected%2520feature%250Aaggregation%2520of%2520cross-attention.%2520Additionally%252C%2520the%2520absence%2520of%2520depth%2520information%250Amay%2520lead%2520to%2520points%2520projected%2520onto%2520the%2520image%2520plane%2520sharing%2520the%2520same%25202D%2520position%250Aor%2520similar%2520sampling%2520points%2520in%2520the%2520feature%2520map%252C%2520resulting%2520in%2520depth%2520ambiguity.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520novel%2520context%2520and%2520geometry%2520aware%2520voxel%2520transformer.%2520It%250Autilizes%2520a%2520context%2520aware%2520query%2520generator%2520to%2520initialize%2520context-dependent%250Aqueries%2520tailored%2520to%2520individual%2520input%2520images%252C%2520effectively%2520capturing%2520their%2520unique%250Acharacteristics%2520and%2520aggregating%2520information%2520within%2520the%2520region%2520of%2520interest.%250AFurthermore%252C%2520it%2520extend%2520deformable%2520cross-attention%2520from%25202D%2520to%25203D%2520pixel%2520space%252C%250Aenabling%2520the%2520differentiation%2520of%2520points%2520with%2520similar%2520image%2520coordinates%2520based%2520on%250Atheir%2520depth%2520coordinates.%2520Building%2520upon%2520this%2520module%252C%2520we%2520introduce%2520a%2520neural%250Anetwork%2520named%2520CGFormer%2520to%2520achieve%2520semantic%2520scene%2520completion.%2520Simultaneously%252C%250ACGFormer%2520leverages%2520multiple%25203D%2520representations%2520%2528i.e.%252C%2520voxel%2520and%2520TPV%2529%2520to%2520boost%250Athe%2520semantic%2520and%2520geometric%2520representation%2520abilities%2520of%2520the%2520transformed%25203D%250Avolume%2520from%2520both%2520local%2520and%2520global%2520perspectives.%2520Experimental%2520results%250Ademonstrate%2520that%2520CGFormer%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250ASemanticKITTI%2520and%2520SSCBench-KITTI-360%2520benchmarks%252C%2520attaining%2520a%2520mIoU%2520of%252016.87%2520and%250A20.05%252C%2520as%2520well%2520as%2520an%2520IoU%2520of%252045.99%2520and%252048.07%252C%2520respectively.%2520Remarkably%252C%2520CGFormer%250Aeven%2520outperforms%2520approaches%2520employing%2520temporal%2520images%2520as%2520inputs%2520or%2520much%2520larger%250Aimage%2520backbone%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13675v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20and%20Geometry%20Aware%20Voxel%20Transformer%20for%20Semantic%20Scene%0A%20%20Completion&entry.906535625=Zhu%20Yu%20and%20Runmin%20Zhang%20and%20Jiacheng%20Ying%20and%20Junchen%20Yu%20and%20Xiaohai%20Hu%20and%20Lun%20Luo%20and%20Si-Yuan%20Cao%20and%20Hui-Liang%20Shen&entry.1292438233=%20%20Vision-based%20Semantic%20Scene%20Completion%20%28SSC%29%20has%20gained%20much%20attention%20due%20to%0Aits%20widespread%20applications%20in%20various%203D%20perception%20tasks.%20Existing%0Asparse-to-dense%20approaches%20typically%20employ%20shared%20context-independent%20queries%0Aacross%20various%20input%20images%2C%20which%20fails%20to%20capture%20distinctions%20among%20them%20as%0Athe%20focal%20regions%20of%20different%20inputs%20vary%20and%20may%20result%20in%20undirected%20feature%0Aaggregation%20of%20cross-attention.%20Additionally%2C%20the%20absence%20of%20depth%20information%0Amay%20lead%20to%20points%20projected%20onto%20the%20image%20plane%20sharing%20the%20same%202D%20position%0Aor%20similar%20sampling%20points%20in%20the%20feature%20map%2C%20resulting%20in%20depth%20ambiguity.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20context%20and%20geometry%20aware%20voxel%20transformer.%20It%0Autilizes%20a%20context%20aware%20query%20generator%20to%20initialize%20context-dependent%0Aqueries%20tailored%20to%20individual%20input%20images%2C%20effectively%20capturing%20their%20unique%0Acharacteristics%20and%20aggregating%20information%20within%20the%20region%20of%20interest.%0AFurthermore%2C%20it%20extend%20deformable%20cross-attention%20from%202D%20to%203D%20pixel%20space%2C%0Aenabling%20the%20differentiation%20of%20points%20with%20similar%20image%20coordinates%20based%20on%0Atheir%20depth%20coordinates.%20Building%20upon%20this%20module%2C%20we%20introduce%20a%20neural%0Anetwork%20named%20CGFormer%20to%20achieve%20semantic%20scene%20completion.%20Simultaneously%2C%0ACGFormer%20leverages%20multiple%203D%20representations%20%28i.e.%2C%20voxel%20and%20TPV%29%20to%20boost%0Athe%20semantic%20and%20geometric%20representation%20abilities%20of%20the%20transformed%203D%0Avolume%20from%20both%20local%20and%20global%20perspectives.%20Experimental%20results%0Ademonstrate%20that%20CGFormer%20achieves%20state-of-the-art%20performance%20on%20the%0ASemanticKITTI%20and%20SSCBench-KITTI-360%20benchmarks%2C%20attaining%20a%20mIoU%20of%2016.87%20and%0A20.05%2C%20as%20well%20as%20an%20IoU%20of%2045.99%20and%2048.07%2C%20respectively.%20Remarkably%2C%20CGFormer%0Aeven%20outperforms%20approaches%20employing%20temporal%20images%20as%20inputs%20or%20much%20larger%0Aimage%20backbone%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13675v4&entry.124074799=Read"},
{"title": "Impact of 3D LiDAR Resolution in Graph-based SLAM Approaches: A\n  Comparative Study", "author": "J. Jorge and T. Barros and C. Premebida and M. Aleksandrov and D. Goehring and U. J. Nunes", "abstract": "  Simultaneous Localization and Mapping (SLAM) is a key component of autonomous\nsystems operating in environments that require a consistent map for reliable\nlocalization. SLAM has been a widely studied topic for decades with most of the\nsolutions being camera or LiDAR based. Early LiDAR-based approaches primarily\nrelied on 2D data, whereas more recent frameworks use 3D data. In this work, we\nsurvey recent 3D LiDAR-based Graph-SLAM methods in urban environments, aiming\nto compare their strengths, weaknesses, and limitations. Additionally, we\nevaluate their robustness regarding the LiDAR resolution namely 64 $vs$ 128\nchannels. Regarding SLAM methods, we evaluate SC-LeGO-LOAM, SC-LIO-SAM,\nCartographer, and HDL-Graph on real-world urban environments using the KITTI\nodometry dataset (a LiDAR with 64-channels only) and a new dataset\n(AUTONOMOS-LABS). The latter dataset, collected using instrumented vehicles\ndriving in Berlin suburban area, comprises both 64 and 128 LiDARs. The\nexperimental results are reported in terms of quantitative `metrics' and\ncomplemented by qualitative maps.\n", "link": "http://arxiv.org/abs/2410.17171v1", "date": "2024-10-22", "relevancy": 2.9371, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6115}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%203D%20LiDAR%20Resolution%20in%20Graph-based%20SLAM%20Approaches%3A%20A%0A%20%20Comparative%20Study&body=Title%3A%20Impact%20of%203D%20LiDAR%20Resolution%20in%20Graph-based%20SLAM%20Approaches%3A%20A%0A%20%20Comparative%20Study%0AAuthor%3A%20J.%20Jorge%20and%20T.%20Barros%20and%20C.%20Premebida%20and%20M.%20Aleksandrov%20and%20D.%20Goehring%20and%20U.%20J.%20Nunes%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20key%20component%20of%20autonomous%0Asystems%20operating%20in%20environments%20that%20require%20a%20consistent%20map%20for%20reliable%0Alocalization.%20SLAM%20has%20been%20a%20widely%20studied%20topic%20for%20decades%20with%20most%20of%20the%0Asolutions%20being%20camera%20or%20LiDAR%20based.%20Early%20LiDAR-based%20approaches%20primarily%0Arelied%20on%202D%20data%2C%20whereas%20more%20recent%20frameworks%20use%203D%20data.%20In%20this%20work%2C%20we%0Asurvey%20recent%203D%20LiDAR-based%20Graph-SLAM%20methods%20in%20urban%20environments%2C%20aiming%0Ato%20compare%20their%20strengths%2C%20weaknesses%2C%20and%20limitations.%20Additionally%2C%20we%0Aevaluate%20their%20robustness%20regarding%20the%20LiDAR%20resolution%20namely%2064%20%24vs%24%20128%0Achannels.%20Regarding%20SLAM%20methods%2C%20we%20evaluate%20SC-LeGO-LOAM%2C%20SC-LIO-SAM%2C%0ACartographer%2C%20and%20HDL-Graph%20on%20real-world%20urban%20environments%20using%20the%20KITTI%0Aodometry%20dataset%20%28a%20LiDAR%20with%2064-channels%20only%29%20and%20a%20new%20dataset%0A%28AUTONOMOS-LABS%29.%20The%20latter%20dataset%2C%20collected%20using%20instrumented%20vehicles%0Adriving%20in%20Berlin%20suburban%20area%2C%20comprises%20both%2064%20and%20128%20LiDARs.%20The%0Aexperimental%20results%20are%20reported%20in%20terms%20of%20quantitative%20%60metrics%27%20and%0Acomplemented%20by%20qualitative%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%25203D%2520LiDAR%2520Resolution%2520in%2520Graph-based%2520SLAM%2520Approaches%253A%2520A%250A%2520%2520Comparative%2520Study%26entry.906535625%3DJ.%2520Jorge%2520and%2520T.%2520Barros%2520and%2520C.%2520Premebida%2520and%2520M.%2520Aleksandrov%2520and%2520D.%2520Goehring%2520and%2520U.%2520J.%2520Nunes%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520is%2520a%2520key%2520component%2520of%2520autonomous%250Asystems%2520operating%2520in%2520environments%2520that%2520require%2520a%2520consistent%2520map%2520for%2520reliable%250Alocalization.%2520SLAM%2520has%2520been%2520a%2520widely%2520studied%2520topic%2520for%2520decades%2520with%2520most%2520of%2520the%250Asolutions%2520being%2520camera%2520or%2520LiDAR%2520based.%2520Early%2520LiDAR-based%2520approaches%2520primarily%250Arelied%2520on%25202D%2520data%252C%2520whereas%2520more%2520recent%2520frameworks%2520use%25203D%2520data.%2520In%2520this%2520work%252C%2520we%250Asurvey%2520recent%25203D%2520LiDAR-based%2520Graph-SLAM%2520methods%2520in%2520urban%2520environments%252C%2520aiming%250Ato%2520compare%2520their%2520strengths%252C%2520weaknesses%252C%2520and%2520limitations.%2520Additionally%252C%2520we%250Aevaluate%2520their%2520robustness%2520regarding%2520the%2520LiDAR%2520resolution%2520namely%252064%2520%2524vs%2524%2520128%250Achannels.%2520Regarding%2520SLAM%2520methods%252C%2520we%2520evaluate%2520SC-LeGO-LOAM%252C%2520SC-LIO-SAM%252C%250ACartographer%252C%2520and%2520HDL-Graph%2520on%2520real-world%2520urban%2520environments%2520using%2520the%2520KITTI%250Aodometry%2520dataset%2520%2528a%2520LiDAR%2520with%252064-channels%2520only%2529%2520and%2520a%2520new%2520dataset%250A%2528AUTONOMOS-LABS%2529.%2520The%2520latter%2520dataset%252C%2520collected%2520using%2520instrumented%2520vehicles%250Adriving%2520in%2520Berlin%2520suburban%2520area%252C%2520comprises%2520both%252064%2520and%2520128%2520LiDARs.%2520The%250Aexperimental%2520results%2520are%2520reported%2520in%2520terms%2520of%2520quantitative%2520%2560metrics%2527%2520and%250Acomplemented%2520by%2520qualitative%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%203D%20LiDAR%20Resolution%20in%20Graph-based%20SLAM%20Approaches%3A%20A%0A%20%20Comparative%20Study&entry.906535625=J.%20Jorge%20and%20T.%20Barros%20and%20C.%20Premebida%20and%20M.%20Aleksandrov%20and%20D.%20Goehring%20and%20U.%20J.%20Nunes&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20key%20component%20of%20autonomous%0Asystems%20operating%20in%20environments%20that%20require%20a%20consistent%20map%20for%20reliable%0Alocalization.%20SLAM%20has%20been%20a%20widely%20studied%20topic%20for%20decades%20with%20most%20of%20the%0Asolutions%20being%20camera%20or%20LiDAR%20based.%20Early%20LiDAR-based%20approaches%20primarily%0Arelied%20on%202D%20data%2C%20whereas%20more%20recent%20frameworks%20use%203D%20data.%20In%20this%20work%2C%20we%0Asurvey%20recent%203D%20LiDAR-based%20Graph-SLAM%20methods%20in%20urban%20environments%2C%20aiming%0Ato%20compare%20their%20strengths%2C%20weaknesses%2C%20and%20limitations.%20Additionally%2C%20we%0Aevaluate%20their%20robustness%20regarding%20the%20LiDAR%20resolution%20namely%2064%20%24vs%24%20128%0Achannels.%20Regarding%20SLAM%20methods%2C%20we%20evaluate%20SC-LeGO-LOAM%2C%20SC-LIO-SAM%2C%0ACartographer%2C%20and%20HDL-Graph%20on%20real-world%20urban%20environments%20using%20the%20KITTI%0Aodometry%20dataset%20%28a%20LiDAR%20with%2064-channels%20only%29%20and%20a%20new%20dataset%0A%28AUTONOMOS-LABS%29.%20The%20latter%20dataset%2C%20collected%20using%20instrumented%20vehicles%0Adriving%20in%20Berlin%20suburban%20area%2C%20comprises%20both%2064%20and%20128%20LiDARs.%20The%0Aexperimental%20results%20are%20reported%20in%20terms%20of%20quantitative%20%60metrics%27%20and%0Acomplemented%20by%20qualitative%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17171v1&entry.124074799=Read"},
{"title": "DCDepth: Progressive Monocular Depth Estimation in Discrete Cosine\n  Domain", "author": "Kun Wang and Zhiqiang Yan and Junkai Fan and Wanlu Zhu and Xiang Li and Jun Li and Jian Yang", "abstract": "  In this paper, we introduce DCDepth, a novel framework for the long-standing\nmonocular depth estimation task. Moving beyond conventional pixel-wise depth\nestimation in the spatial domain, our approach estimates the frequency\ncoefficients of depth patches after transforming them into the discrete cosine\ndomain. This unique formulation allows for the modeling of local depth\ncorrelations within each patch. Crucially, the frequency transformation\nsegregates the depth information into various frequency components, with\nlow-frequency components encapsulating the core scene structure and\nhigh-frequency components detailing the finer aspects. This decomposition forms\nthe basis of our progressive strategy, which begins with the prediction of\nlow-frequency components to establish a global scene context, followed by\nsuccessive refinement of local details through the prediction of\nhigher-frequency components. We conduct comprehensive experiments on\nNYU-Depth-V2, TOFDC, and KITTI datasets, and demonstrate the state-of-the-art\nperformance of DCDepth. Code is available at https://github.com/w2kun/DCDepth.\n", "link": "http://arxiv.org/abs/2410.14980v2", "date": "2024-10-22", "relevancy": 2.8332, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCDepth%3A%20Progressive%20Monocular%20Depth%20Estimation%20in%20Discrete%20Cosine%0A%20%20Domain&body=Title%3A%20DCDepth%3A%20Progressive%20Monocular%20Depth%20Estimation%20in%20Discrete%20Cosine%0A%20%20Domain%0AAuthor%3A%20Kun%20Wang%20and%20Zhiqiang%20Yan%20and%20Junkai%20Fan%20and%20Wanlu%20Zhu%20and%20Xiang%20Li%20and%20Jun%20Li%20and%20Jian%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20DCDepth%2C%20a%20novel%20framework%20for%20the%20long-standing%0Amonocular%20depth%20estimation%20task.%20Moving%20beyond%20conventional%20pixel-wise%20depth%0Aestimation%20in%20the%20spatial%20domain%2C%20our%20approach%20estimates%20the%20frequency%0Acoefficients%20of%20depth%20patches%20after%20transforming%20them%20into%20the%20discrete%20cosine%0Adomain.%20This%20unique%20formulation%20allows%20for%20the%20modeling%20of%20local%20depth%0Acorrelations%20within%20each%20patch.%20Crucially%2C%20the%20frequency%20transformation%0Asegregates%20the%20depth%20information%20into%20various%20frequency%20components%2C%20with%0Alow-frequency%20components%20encapsulating%20the%20core%20scene%20structure%20and%0Ahigh-frequency%20components%20detailing%20the%20finer%20aspects.%20This%20decomposition%20forms%0Athe%20basis%20of%20our%20progressive%20strategy%2C%20which%20begins%20with%20the%20prediction%20of%0Alow-frequency%20components%20to%20establish%20a%20global%20scene%20context%2C%20followed%20by%0Asuccessive%20refinement%20of%20local%20details%20through%20the%20prediction%20of%0Ahigher-frequency%20components.%20We%20conduct%20comprehensive%20experiments%20on%0ANYU-Depth-V2%2C%20TOFDC%2C%20and%20KITTI%20datasets%2C%20and%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20DCDepth.%20Code%20is%20available%20at%20https%3A//github.com/w2kun/DCDepth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCDepth%253A%2520Progressive%2520Monocular%2520Depth%2520Estimation%2520in%2520Discrete%2520Cosine%250A%2520%2520Domain%26entry.906535625%3DKun%2520Wang%2520and%2520Zhiqiang%2520Yan%2520and%2520Junkai%2520Fan%2520and%2520Wanlu%2520Zhu%2520and%2520Xiang%2520Li%2520and%2520Jun%2520Li%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DCDepth%252C%2520a%2520novel%2520framework%2520for%2520the%2520long-standing%250Amonocular%2520depth%2520estimation%2520task.%2520Moving%2520beyond%2520conventional%2520pixel-wise%2520depth%250Aestimation%2520in%2520the%2520spatial%2520domain%252C%2520our%2520approach%2520estimates%2520the%2520frequency%250Acoefficients%2520of%2520depth%2520patches%2520after%2520transforming%2520them%2520into%2520the%2520discrete%2520cosine%250Adomain.%2520This%2520unique%2520formulation%2520allows%2520for%2520the%2520modeling%2520of%2520local%2520depth%250Acorrelations%2520within%2520each%2520patch.%2520Crucially%252C%2520the%2520frequency%2520transformation%250Asegregates%2520the%2520depth%2520information%2520into%2520various%2520frequency%2520components%252C%2520with%250Alow-frequency%2520components%2520encapsulating%2520the%2520core%2520scene%2520structure%2520and%250Ahigh-frequency%2520components%2520detailing%2520the%2520finer%2520aspects.%2520This%2520decomposition%2520forms%250Athe%2520basis%2520of%2520our%2520progressive%2520strategy%252C%2520which%2520begins%2520with%2520the%2520prediction%2520of%250Alow-frequency%2520components%2520to%2520establish%2520a%2520global%2520scene%2520context%252C%2520followed%2520by%250Asuccessive%2520refinement%2520of%2520local%2520details%2520through%2520the%2520prediction%2520of%250Ahigher-frequency%2520components.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%250ANYU-Depth-V2%252C%2520TOFDC%252C%2520and%2520KITTI%2520datasets%252C%2520and%2520demonstrate%2520the%2520state-of-the-art%250Aperformance%2520of%2520DCDepth.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/w2kun/DCDepth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCDepth%3A%20Progressive%20Monocular%20Depth%20Estimation%20in%20Discrete%20Cosine%0A%20%20Domain&entry.906535625=Kun%20Wang%20and%20Zhiqiang%20Yan%20and%20Junkai%20Fan%20and%20Wanlu%20Zhu%20and%20Xiang%20Li%20and%20Jun%20Li%20and%20Jian%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20DCDepth%2C%20a%20novel%20framework%20for%20the%20long-standing%0Amonocular%20depth%20estimation%20task.%20Moving%20beyond%20conventional%20pixel-wise%20depth%0Aestimation%20in%20the%20spatial%20domain%2C%20our%20approach%20estimates%20the%20frequency%0Acoefficients%20of%20depth%20patches%20after%20transforming%20them%20into%20the%20discrete%20cosine%0Adomain.%20This%20unique%20formulation%20allows%20for%20the%20modeling%20of%20local%20depth%0Acorrelations%20within%20each%20patch.%20Crucially%2C%20the%20frequency%20transformation%0Asegregates%20the%20depth%20information%20into%20various%20frequency%20components%2C%20with%0Alow-frequency%20components%20encapsulating%20the%20core%20scene%20structure%20and%0Ahigh-frequency%20components%20detailing%20the%20finer%20aspects.%20This%20decomposition%20forms%0Athe%20basis%20of%20our%20progressive%20strategy%2C%20which%20begins%20with%20the%20prediction%20of%0Alow-frequency%20components%20to%20establish%20a%20global%20scene%20context%2C%20followed%20by%0Asuccessive%20refinement%20of%20local%20details%20through%20the%20prediction%20of%0Ahigher-frequency%20components.%20We%20conduct%20comprehensive%20experiments%20on%0ANYU-Depth-V2%2C%20TOFDC%2C%20and%20KITTI%20datasets%2C%20and%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20DCDepth.%20Code%20is%20available%20at%20https%3A//github.com/w2kun/DCDepth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14980v2&entry.124074799=Read"},
{"title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples", "author": "Baiqi Li and Zhiqiu Lin and Wenxuan Peng and Jean de Dieu Nyandwi and Daniel Jiang and Zixian Ma and Simran Khanuja and Ranjay Krishna and Graham Neubig and Deva Ramanan", "abstract": "  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n", "link": "http://arxiv.org/abs/2410.14669v2", "date": "2024-10-22", "relevancy": 2.8212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NaturalBench%3A%20Evaluating%20Vision-Language%20Models%20on%20Natural%20Adversarial%0A%20%20Samples&body=Title%3A%20NaturalBench%3A%20Evaluating%20Vision-Language%20Models%20on%20Natural%20Adversarial%0A%20%20Samples%0AAuthor%3A%20Baiqi%20Li%20and%20Zhiqiu%20Lin%20and%20Wenxuan%20Peng%20and%20Jean%20de%20Dieu%20Nyandwi%20and%20Daniel%20Jiang%20and%20Zixian%20Ma%20and%20Simran%20Khanuja%20and%20Ranjay%20Krishna%20and%20Graham%20Neubig%20and%20Deva%20Ramanan%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20recent%0Avisual-question-answering%20%28VQA%29%20benchmarks%20that%20evaluate%20complex%0Avisio-linguistic%20reasoning.%20However%2C%20are%20these%20models%20truly%20effective%3F%20In%20this%0Awork%2C%20we%20show%20that%20VLMs%20still%20struggle%20with%20natural%20images%20and%20questions%20that%0Ahumans%20can%20easily%20answer%2C%20which%20we%20term%20natural%20adversarial%20samples.%20We%20also%0Afind%20it%20surprisingly%20easy%20to%20generate%20these%20VQA%20samples%20from%20natural%20image-text%0Acorpora%20using%20off-the-shelf%20models%20like%20CLIP%20and%20ChatGPT.%20We%20propose%20a%0Asemi-automated%20approach%20to%20collect%20a%20new%20benchmark%2C%20NaturalBench%2C%20for%20reliably%0Aevaluating%20VLMs%20with%2010%2C000%20human-verified%20VQA%20samples.%20Crucially%2C%20we%20adopt%20a%0A%24%5Ctextbf%7Bvision-centric%7D%24%20design%20by%20pairing%20each%20question%20with%20two%20images%20that%0Ayield%20different%20answers%2C%20preventing%20blind%20solutions%20from%20answering%20without%0Ausing%20the%20images.%20This%20makes%20NaturalBench%20more%20challenging%20than%20previous%0Abenchmarks%20that%20can%20be%20solved%20with%20commonsense%20priors.%20We%20evaluate%2053%0Astate-of-the-art%20VLMs%20on%20NaturalBench%2C%20showing%20that%20models%20like%0ALLaVA-OneVision%2C%20Cambrian-1%2C%20Llama3.2-Vision%2C%20Molmo%2C%20Qwen2-VL%2C%20and%20even%20GPT-4o%0Alag%2050%25-70%25%20behind%20human%20performance%20%28over%2090%25%29.%20We%20analyze%20why%20NaturalBench%20is%0Ahard%20from%20two%20angles%3A%20%281%29%20Compositionality%3A%20Solving%20NaturalBench%20requires%0Adiverse%20visio-linguistic%20skills%2C%20including%20understanding%20attribute%20bindings%2C%0Aobject%20relationships%2C%20and%20advanced%20reasoning%20like%20logic%20and%20counting.%20To%20this%0Aend%2C%20unlike%20prior%20work%20that%20uses%20a%20single%20tag%20per%20sample%2C%20we%20tag%20each%0ANaturalBench%20sample%20with%201%20to%208%20skill%20tags%20for%20fine-grained%20evaluation.%20%282%29%0ABiases%3A%20NaturalBench%20exposes%20severe%20biases%20in%20VLMs%2C%20as%20models%20often%20choose%20the%0Asame%20answer%20regardless%20of%20the%20image.%20Lastly%2C%20we%20apply%20our%20benchmark%20curation%0Amethod%20to%20diverse%20data%20sources%2C%20including%20long%20captions%20%28over%20100%20words%29%20and%0Anon-English%20languages%20like%20Chinese%20and%20Hindi%2C%20highlighting%20its%20potential%20for%0Adynamic%20evaluations%20of%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14669v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNaturalBench%253A%2520Evaluating%2520Vision-Language%2520Models%2520on%2520Natural%2520Adversarial%250A%2520%2520Samples%26entry.906535625%3DBaiqi%2520Li%2520and%2520Zhiqiu%2520Lin%2520and%2520Wenxuan%2520Peng%2520and%2520Jean%2520de%2520Dieu%2520Nyandwi%2520and%2520Daniel%2520Jiang%2520and%2520Zixian%2520Ma%2520and%2520Simran%2520Khanuja%2520and%2520Ranjay%2520Krishna%2520and%2520Graham%2520Neubig%2520and%2520Deva%2520Ramanan%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520recent%250Avisual-question-answering%2520%2528VQA%2529%2520benchmarks%2520that%2520evaluate%2520complex%250Avisio-linguistic%2520reasoning.%2520However%252C%2520are%2520these%2520models%2520truly%2520effective%253F%2520In%2520this%250Awork%252C%2520we%2520show%2520that%2520VLMs%2520still%2520struggle%2520with%2520natural%2520images%2520and%2520questions%2520that%250Ahumans%2520can%2520easily%2520answer%252C%2520which%2520we%2520term%2520natural%2520adversarial%2520samples.%2520We%2520also%250Afind%2520it%2520surprisingly%2520easy%2520to%2520generate%2520these%2520VQA%2520samples%2520from%2520natural%2520image-text%250Acorpora%2520using%2520off-the-shelf%2520models%2520like%2520CLIP%2520and%2520ChatGPT.%2520We%2520propose%2520a%250Asemi-automated%2520approach%2520to%2520collect%2520a%2520new%2520benchmark%252C%2520NaturalBench%252C%2520for%2520reliably%250Aevaluating%2520VLMs%2520with%252010%252C000%2520human-verified%2520VQA%2520samples.%2520Crucially%252C%2520we%2520adopt%2520a%250A%2524%255Ctextbf%257Bvision-centric%257D%2524%2520design%2520by%2520pairing%2520each%2520question%2520with%2520two%2520images%2520that%250Ayield%2520different%2520answers%252C%2520preventing%2520blind%2520solutions%2520from%2520answering%2520without%250Ausing%2520the%2520images.%2520This%2520makes%2520NaturalBench%2520more%2520challenging%2520than%2520previous%250Abenchmarks%2520that%2520can%2520be%2520solved%2520with%2520commonsense%2520priors.%2520We%2520evaluate%252053%250Astate-of-the-art%2520VLMs%2520on%2520NaturalBench%252C%2520showing%2520that%2520models%2520like%250ALLaVA-OneVision%252C%2520Cambrian-1%252C%2520Llama3.2-Vision%252C%2520Molmo%252C%2520Qwen2-VL%252C%2520and%2520even%2520GPT-4o%250Alag%252050%2525-70%2525%2520behind%2520human%2520performance%2520%2528over%252090%2525%2529.%2520We%2520analyze%2520why%2520NaturalBench%2520is%250Ahard%2520from%2520two%2520angles%253A%2520%25281%2529%2520Compositionality%253A%2520Solving%2520NaturalBench%2520requires%250Adiverse%2520visio-linguistic%2520skills%252C%2520including%2520understanding%2520attribute%2520bindings%252C%250Aobject%2520relationships%252C%2520and%2520advanced%2520reasoning%2520like%2520logic%2520and%2520counting.%2520To%2520this%250Aend%252C%2520unlike%2520prior%2520work%2520that%2520uses%2520a%2520single%2520tag%2520per%2520sample%252C%2520we%2520tag%2520each%250ANaturalBench%2520sample%2520with%25201%2520to%25208%2520skill%2520tags%2520for%2520fine-grained%2520evaluation.%2520%25282%2529%250ABiases%253A%2520NaturalBench%2520exposes%2520severe%2520biases%2520in%2520VLMs%252C%2520as%2520models%2520often%2520choose%2520the%250Asame%2520answer%2520regardless%2520of%2520the%2520image.%2520Lastly%252C%2520we%2520apply%2520our%2520benchmark%2520curation%250Amethod%2520to%2520diverse%2520data%2520sources%252C%2520including%2520long%2520captions%2520%2528over%2520100%2520words%2529%2520and%250Anon-English%2520languages%2520like%2520Chinese%2520and%2520Hindi%252C%2520highlighting%2520its%2520potential%2520for%250Adynamic%2520evaluations%2520of%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14669v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NaturalBench%3A%20Evaluating%20Vision-Language%20Models%20on%20Natural%20Adversarial%0A%20%20Samples&entry.906535625=Baiqi%20Li%20and%20Zhiqiu%20Lin%20and%20Wenxuan%20Peng%20and%20Jean%20de%20Dieu%20Nyandwi%20and%20Daniel%20Jiang%20and%20Zixian%20Ma%20and%20Simran%20Khanuja%20and%20Ranjay%20Krishna%20and%20Graham%20Neubig%20and%20Deva%20Ramanan&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20recent%0Avisual-question-answering%20%28VQA%29%20benchmarks%20that%20evaluate%20complex%0Avisio-linguistic%20reasoning.%20However%2C%20are%20these%20models%20truly%20effective%3F%20In%20this%0Awork%2C%20we%20show%20that%20VLMs%20still%20struggle%20with%20natural%20images%20and%20questions%20that%0Ahumans%20can%20easily%20answer%2C%20which%20we%20term%20natural%20adversarial%20samples.%20We%20also%0Afind%20it%20surprisingly%20easy%20to%20generate%20these%20VQA%20samples%20from%20natural%20image-text%0Acorpora%20using%20off-the-shelf%20models%20like%20CLIP%20and%20ChatGPT.%20We%20propose%20a%0Asemi-automated%20approach%20to%20collect%20a%20new%20benchmark%2C%20NaturalBench%2C%20for%20reliably%0Aevaluating%20VLMs%20with%2010%2C000%20human-verified%20VQA%20samples.%20Crucially%2C%20we%20adopt%20a%0A%24%5Ctextbf%7Bvision-centric%7D%24%20design%20by%20pairing%20each%20question%20with%20two%20images%20that%0Ayield%20different%20answers%2C%20preventing%20blind%20solutions%20from%20answering%20without%0Ausing%20the%20images.%20This%20makes%20NaturalBench%20more%20challenging%20than%20previous%0Abenchmarks%20that%20can%20be%20solved%20with%20commonsense%20priors.%20We%20evaluate%2053%0Astate-of-the-art%20VLMs%20on%20NaturalBench%2C%20showing%20that%20models%20like%0ALLaVA-OneVision%2C%20Cambrian-1%2C%20Llama3.2-Vision%2C%20Molmo%2C%20Qwen2-VL%2C%20and%20even%20GPT-4o%0Alag%2050%25-70%25%20behind%20human%20performance%20%28over%2090%25%29.%20We%20analyze%20why%20NaturalBench%20is%0Ahard%20from%20two%20angles%3A%20%281%29%20Compositionality%3A%20Solving%20NaturalBench%20requires%0Adiverse%20visio-linguistic%20skills%2C%20including%20understanding%20attribute%20bindings%2C%0Aobject%20relationships%2C%20and%20advanced%20reasoning%20like%20logic%20and%20counting.%20To%20this%0Aend%2C%20unlike%20prior%20work%20that%20uses%20a%20single%20tag%20per%20sample%2C%20we%20tag%20each%0ANaturalBench%20sample%20with%201%20to%208%20skill%20tags%20for%20fine-grained%20evaluation.%20%282%29%0ABiases%3A%20NaturalBench%20exposes%20severe%20biases%20in%20VLMs%2C%20as%20models%20often%20choose%20the%0Asame%20answer%20regardless%20of%20the%20image.%20Lastly%2C%20we%20apply%20our%20benchmark%20curation%0Amethod%20to%20diverse%20data%20sources%2C%20including%20long%20captions%20%28over%20100%20words%29%20and%0Anon-English%20languages%20like%20Chinese%20and%20Hindi%2C%20highlighting%20its%20potential%20for%0Adynamic%20evaluations%20of%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14669v2&entry.124074799=Read"},
{"title": "EPContrast: Effective Point-level Contrastive Learning for Large-scale\n  Point Cloud Understanding", "author": "Zhiyi Pan and Guoqing Liu and Wei Gao and Thomas H. Li", "abstract": "  The acquisition of inductive bias through point-level contrastive learning\nholds paramount significance in point cloud pre-training. However, the square\ngrowth in computational requirements with the scale of the point cloud poses a\nsubstantial impediment to the practical deployment and execution. To address\nthis challenge, this paper proposes an Effective Point-level Contrastive\nLearning method for large-scale point cloud understanding dubbed\n\\textbf{EPContrast}, which consists of AGContrast and ChannelContrast. In\npractice, AGContrast constructs positive and negative pairs based on asymmetric\ngranularity embedding, while ChannelContrast imposes contrastive supervision\nbetween channel feature maps. EPContrast offers point-level contrastive loss\nwhile concurrently mitigating the computational resource burden. The efficacy\nof EPContrast is substantiated through comprehensive validation on S3DIS and\nScanNetV2, encompassing tasks such as semantic segmentation, instance\nsegmentation, and object detection. In addition, rich ablation experiments\ndemonstrate remarkable bias induction capabilities under label-efficient and\none-epoch training settings.\n", "link": "http://arxiv.org/abs/2410.17207v1", "date": "2024-10-22", "relevancy": 2.7627, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5677}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EPContrast%3A%20Effective%20Point-level%20Contrastive%20Learning%20for%20Large-scale%0A%20%20Point%20Cloud%20Understanding&body=Title%3A%20EPContrast%3A%20Effective%20Point-level%20Contrastive%20Learning%20for%20Large-scale%0A%20%20Point%20Cloud%20Understanding%0AAuthor%3A%20Zhiyi%20Pan%20and%20Guoqing%20Liu%20and%20Wei%20Gao%20and%20Thomas%20H.%20Li%0AAbstract%3A%20%20%20The%20acquisition%20of%20inductive%20bias%20through%20point-level%20contrastive%20learning%0Aholds%20paramount%20significance%20in%20point%20cloud%20pre-training.%20However%2C%20the%20square%0Agrowth%20in%20computational%20requirements%20with%20the%20scale%20of%20the%20point%20cloud%20poses%20a%0Asubstantial%20impediment%20to%20the%20practical%20deployment%20and%20execution.%20To%20address%0Athis%20challenge%2C%20this%20paper%20proposes%20an%20Effective%20Point-level%20Contrastive%0ALearning%20method%20for%20large-scale%20point%20cloud%20understanding%20dubbed%0A%5Ctextbf%7BEPContrast%7D%2C%20which%20consists%20of%20AGContrast%20and%20ChannelContrast.%20In%0Apractice%2C%20AGContrast%20constructs%20positive%20and%20negative%20pairs%20based%20on%20asymmetric%0Agranularity%20embedding%2C%20while%20ChannelContrast%20imposes%20contrastive%20supervision%0Abetween%20channel%20feature%20maps.%20EPContrast%20offers%20point-level%20contrastive%20loss%0Awhile%20concurrently%20mitigating%20the%20computational%20resource%20burden.%20The%20efficacy%0Aof%20EPContrast%20is%20substantiated%20through%20comprehensive%20validation%20on%20S3DIS%20and%0AScanNetV2%2C%20encompassing%20tasks%20such%20as%20semantic%20segmentation%2C%20instance%0Asegmentation%2C%20and%20object%20detection.%20In%20addition%2C%20rich%20ablation%20experiments%0Ademonstrate%20remarkable%20bias%20induction%20capabilities%20under%20label-efficient%20and%0Aone-epoch%20training%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEPContrast%253A%2520Effective%2520Point-level%2520Contrastive%2520Learning%2520for%2520Large-scale%250A%2520%2520Point%2520Cloud%2520Understanding%26entry.906535625%3DZhiyi%2520Pan%2520and%2520Guoqing%2520Liu%2520and%2520Wei%2520Gao%2520and%2520Thomas%2520H.%2520Li%26entry.1292438233%3D%2520%2520The%2520acquisition%2520of%2520inductive%2520bias%2520through%2520point-level%2520contrastive%2520learning%250Aholds%2520paramount%2520significance%2520in%2520point%2520cloud%2520pre-training.%2520However%252C%2520the%2520square%250Agrowth%2520in%2520computational%2520requirements%2520with%2520the%2520scale%2520of%2520the%2520point%2520cloud%2520poses%2520a%250Asubstantial%2520impediment%2520to%2520the%2520practical%2520deployment%2520and%2520execution.%2520To%2520address%250Athis%2520challenge%252C%2520this%2520paper%2520proposes%2520an%2520Effective%2520Point-level%2520Contrastive%250ALearning%2520method%2520for%2520large-scale%2520point%2520cloud%2520understanding%2520dubbed%250A%255Ctextbf%257BEPContrast%257D%252C%2520which%2520consists%2520of%2520AGContrast%2520and%2520ChannelContrast.%2520In%250Apractice%252C%2520AGContrast%2520constructs%2520positive%2520and%2520negative%2520pairs%2520based%2520on%2520asymmetric%250Agranularity%2520embedding%252C%2520while%2520ChannelContrast%2520imposes%2520contrastive%2520supervision%250Abetween%2520channel%2520feature%2520maps.%2520EPContrast%2520offers%2520point-level%2520contrastive%2520loss%250Awhile%2520concurrently%2520mitigating%2520the%2520computational%2520resource%2520burden.%2520The%2520efficacy%250Aof%2520EPContrast%2520is%2520substantiated%2520through%2520comprehensive%2520validation%2520on%2520S3DIS%2520and%250AScanNetV2%252C%2520encompassing%2520tasks%2520such%2520as%2520semantic%2520segmentation%252C%2520instance%250Asegmentation%252C%2520and%2520object%2520detection.%2520In%2520addition%252C%2520rich%2520ablation%2520experiments%250Ademonstrate%2520remarkable%2520bias%2520induction%2520capabilities%2520under%2520label-efficient%2520and%250Aone-epoch%2520training%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPContrast%3A%20Effective%20Point-level%20Contrastive%20Learning%20for%20Large-scale%0A%20%20Point%20Cloud%20Understanding&entry.906535625=Zhiyi%20Pan%20and%20Guoqing%20Liu%20and%20Wei%20Gao%20and%20Thomas%20H.%20Li&entry.1292438233=%20%20The%20acquisition%20of%20inductive%20bias%20through%20point-level%20contrastive%20learning%0Aholds%20paramount%20significance%20in%20point%20cloud%20pre-training.%20However%2C%20the%20square%0Agrowth%20in%20computational%20requirements%20with%20the%20scale%20of%20the%20point%20cloud%20poses%20a%0Asubstantial%20impediment%20to%20the%20practical%20deployment%20and%20execution.%20To%20address%0Athis%20challenge%2C%20this%20paper%20proposes%20an%20Effective%20Point-level%20Contrastive%0ALearning%20method%20for%20large-scale%20point%20cloud%20understanding%20dubbed%0A%5Ctextbf%7BEPContrast%7D%2C%20which%20consists%20of%20AGContrast%20and%20ChannelContrast.%20In%0Apractice%2C%20AGContrast%20constructs%20positive%20and%20negative%20pairs%20based%20on%20asymmetric%0Agranularity%20embedding%2C%20while%20ChannelContrast%20imposes%20contrastive%20supervision%0Abetween%20channel%20feature%20maps.%20EPContrast%20offers%20point-level%20contrastive%20loss%0Awhile%20concurrently%20mitigating%20the%20computational%20resource%20burden.%20The%20efficacy%0Aof%20EPContrast%20is%20substantiated%20through%20comprehensive%20validation%20on%20S3DIS%20and%0AScanNetV2%2C%20encompassing%20tasks%20such%20as%20semantic%20segmentation%2C%20instance%0Asegmentation%2C%20and%20object%20detection.%20In%20addition%2C%20rich%20ablation%20experiments%0Ademonstrate%20remarkable%20bias%20induction%20capabilities%20under%20label-efficient%20and%0Aone-epoch%20training%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17207v1&entry.124074799=Read"},
{"title": "ISImed: A Framework for Self-Supervised Learning using Intrinsic Spatial\n  Information in Medical Images", "author": "Nabil Jabareen and Dongsheng Yuan and S\u00f6ren Lukassen", "abstract": "  This paper demonstrates that spatial information can be used to learn\ninterpretable representations in medical images using Self-Supervised Learning\n(SSL). Our proposed method, ISImed, is based on the observation that medical\nimages exhibit a much lower variability among different images compared to\nclassic data vision benchmarks. By leveraging this resemblance of human body\nstructures across multiple images, we establish a self-supervised objective\nthat creates a latent representation capable of capturing its location in the\nphysical realm. More specifically, our method involves sampling image crops and\ncreating a distance matrix that compares the learned representation vectors of\nall possible combinations of these crops to the true distance between them. The\nintuition is, that the learned latent space is a positional encoding for a\ngiven image crop. We hypothesize, that by learning these positional encodings,\ncomprehensive image representations have to be generated. To test this\nhypothesis and evaluate our method, we compare our learned representation with\ntwo state-of-the-art SSL benchmarking methods on two publicly available medical\nimaging datasets. We show that our method can efficiently learn representations\nthat capture the underlying structure of the data and can be used to transfer\nto a downstream classification task.\n", "link": "http://arxiv.org/abs/2410.16947v1", "date": "2024-10-22", "relevancy": 2.7518, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5861}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5329}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ISImed%3A%20A%20Framework%20for%20Self-Supervised%20Learning%20using%20Intrinsic%20Spatial%0A%20%20Information%20in%20Medical%20Images&body=Title%3A%20ISImed%3A%20A%20Framework%20for%20Self-Supervised%20Learning%20using%20Intrinsic%20Spatial%0A%20%20Information%20in%20Medical%20Images%0AAuthor%3A%20Nabil%20Jabareen%20and%20Dongsheng%20Yuan%20and%20S%C3%B6ren%20Lukassen%0AAbstract%3A%20%20%20This%20paper%20demonstrates%20that%20spatial%20information%20can%20be%20used%20to%20learn%0Ainterpretable%20representations%20in%20medical%20images%20using%20Self-Supervised%20Learning%0A%28SSL%29.%20Our%20proposed%20method%2C%20ISImed%2C%20is%20based%20on%20the%20observation%20that%20medical%0Aimages%20exhibit%20a%20much%20lower%20variability%20among%20different%20images%20compared%20to%0Aclassic%20data%20vision%20benchmarks.%20By%20leveraging%20this%20resemblance%20of%20human%20body%0Astructures%20across%20multiple%20images%2C%20we%20establish%20a%20self-supervised%20objective%0Athat%20creates%20a%20latent%20representation%20capable%20of%20capturing%20its%20location%20in%20the%0Aphysical%20realm.%20More%20specifically%2C%20our%20method%20involves%20sampling%20image%20crops%20and%0Acreating%20a%20distance%20matrix%20that%20compares%20the%20learned%20representation%20vectors%20of%0Aall%20possible%20combinations%20of%20these%20crops%20to%20the%20true%20distance%20between%20them.%20The%0Aintuition%20is%2C%20that%20the%20learned%20latent%20space%20is%20a%20positional%20encoding%20for%20a%0Agiven%20image%20crop.%20We%20hypothesize%2C%20that%20by%20learning%20these%20positional%20encodings%2C%0Acomprehensive%20image%20representations%20have%20to%20be%20generated.%20To%20test%20this%0Ahypothesis%20and%20evaluate%20our%20method%2C%20we%20compare%20our%20learned%20representation%20with%0Atwo%20state-of-the-art%20SSL%20benchmarking%20methods%20on%20two%20publicly%20available%20medical%0Aimaging%20datasets.%20We%20show%20that%20our%20method%20can%20efficiently%20learn%20representations%0Athat%20capture%20the%20underlying%20structure%20of%20the%20data%20and%20can%20be%20used%20to%20transfer%0Ato%20a%20downstream%20classification%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DISImed%253A%2520A%2520Framework%2520for%2520Self-Supervised%2520Learning%2520using%2520Intrinsic%2520Spatial%250A%2520%2520Information%2520in%2520Medical%2520Images%26entry.906535625%3DNabil%2520Jabareen%2520and%2520Dongsheng%2520Yuan%2520and%2520S%25C3%25B6ren%2520Lukassen%26entry.1292438233%3D%2520%2520This%2520paper%2520demonstrates%2520that%2520spatial%2520information%2520can%2520be%2520used%2520to%2520learn%250Ainterpretable%2520representations%2520in%2520medical%2520images%2520using%2520Self-Supervised%2520Learning%250A%2528SSL%2529.%2520Our%2520proposed%2520method%252C%2520ISImed%252C%2520is%2520based%2520on%2520the%2520observation%2520that%2520medical%250Aimages%2520exhibit%2520a%2520much%2520lower%2520variability%2520among%2520different%2520images%2520compared%2520to%250Aclassic%2520data%2520vision%2520benchmarks.%2520By%2520leveraging%2520this%2520resemblance%2520of%2520human%2520body%250Astructures%2520across%2520multiple%2520images%252C%2520we%2520establish%2520a%2520self-supervised%2520objective%250Athat%2520creates%2520a%2520latent%2520representation%2520capable%2520of%2520capturing%2520its%2520location%2520in%2520the%250Aphysical%2520realm.%2520More%2520specifically%252C%2520our%2520method%2520involves%2520sampling%2520image%2520crops%2520and%250Acreating%2520a%2520distance%2520matrix%2520that%2520compares%2520the%2520learned%2520representation%2520vectors%2520of%250Aall%2520possible%2520combinations%2520of%2520these%2520crops%2520to%2520the%2520true%2520distance%2520between%2520them.%2520The%250Aintuition%2520is%252C%2520that%2520the%2520learned%2520latent%2520space%2520is%2520a%2520positional%2520encoding%2520for%2520a%250Agiven%2520image%2520crop.%2520We%2520hypothesize%252C%2520that%2520by%2520learning%2520these%2520positional%2520encodings%252C%250Acomprehensive%2520image%2520representations%2520have%2520to%2520be%2520generated.%2520To%2520test%2520this%250Ahypothesis%2520and%2520evaluate%2520our%2520method%252C%2520we%2520compare%2520our%2520learned%2520representation%2520with%250Atwo%2520state-of-the-art%2520SSL%2520benchmarking%2520methods%2520on%2520two%2520publicly%2520available%2520medical%250Aimaging%2520datasets.%2520We%2520show%2520that%2520our%2520method%2520can%2520efficiently%2520learn%2520representations%250Athat%2520capture%2520the%2520underlying%2520structure%2520of%2520the%2520data%2520and%2520can%2520be%2520used%2520to%2520transfer%250Ato%2520a%2520downstream%2520classification%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ISImed%3A%20A%20Framework%20for%20Self-Supervised%20Learning%20using%20Intrinsic%20Spatial%0A%20%20Information%20in%20Medical%20Images&entry.906535625=Nabil%20Jabareen%20and%20Dongsheng%20Yuan%20and%20S%C3%B6ren%20Lukassen&entry.1292438233=%20%20This%20paper%20demonstrates%20that%20spatial%20information%20can%20be%20used%20to%20learn%0Ainterpretable%20representations%20in%20medical%20images%20using%20Self-Supervised%20Learning%0A%28SSL%29.%20Our%20proposed%20method%2C%20ISImed%2C%20is%20based%20on%20the%20observation%20that%20medical%0Aimages%20exhibit%20a%20much%20lower%20variability%20among%20different%20images%20compared%20to%0Aclassic%20data%20vision%20benchmarks.%20By%20leveraging%20this%20resemblance%20of%20human%20body%0Astructures%20across%20multiple%20images%2C%20we%20establish%20a%20self-supervised%20objective%0Athat%20creates%20a%20latent%20representation%20capable%20of%20capturing%20its%20location%20in%20the%0Aphysical%20realm.%20More%20specifically%2C%20our%20method%20involves%20sampling%20image%20crops%20and%0Acreating%20a%20distance%20matrix%20that%20compares%20the%20learned%20representation%20vectors%20of%0Aall%20possible%20combinations%20of%20these%20crops%20to%20the%20true%20distance%20between%20them.%20The%0Aintuition%20is%2C%20that%20the%20learned%20latent%20space%20is%20a%20positional%20encoding%20for%20a%0Agiven%20image%20crop.%20We%20hypothesize%2C%20that%20by%20learning%20these%20positional%20encodings%2C%0Acomprehensive%20image%20representations%20have%20to%20be%20generated.%20To%20test%20this%0Ahypothesis%20and%20evaluate%20our%20method%2C%20we%20compare%20our%20learned%20representation%20with%0Atwo%20state-of-the-art%20SSL%20benchmarking%20methods%20on%20two%20publicly%20available%20medical%0Aimaging%20datasets.%20We%20show%20that%20our%20method%20can%20efficiently%20learn%20representations%0Athat%20capture%20the%20underlying%20structure%20of%20the%20data%20and%20can%20be%20used%20to%20transfer%0Ato%20a%20downstream%20classification%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16947v1&entry.124074799=Read"},
{"title": "Frontiers in Intelligent Colonoscopy", "author": "Ge-Peng Ji and Jingyi Liu and Peng Xu and Nick Barnes and Fahad Shahbaz Khan and Salman Khan and Deng-Ping Fan", "abstract": "  Colonoscopy is currently one of the most sensitive screening methods for\ncolorectal cancer. This study investigates the frontiers of intelligent\ncolonoscopy techniques and their prospective implications for multimodal\nmedical applications. With this goal, we begin by assessing the current\ndata-centric and model-centric landscapes through four tasks for colonoscopic\nscene perception, including classification, detection, segmentation, and\nvision-language understanding. This assessment enables us to identify\ndomain-specific challenges and reveals that multimodal research in colonoscopy\nremains open for further exploration. To embrace the coming multimodal era, we\nestablish three foundational initiatives: a large-scale multimodal instruction\ntuning dataset ColonINST, a colonoscopy-designed multimodal language model\nColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this\nrapidly evolving field, we provide a public website for the latest updates:\nhttps://github.com/ai4colonoscopy/IntelliScope.\n", "link": "http://arxiv.org/abs/2410.17241v1", "date": "2024-10-22", "relevancy": 2.7367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frontiers%20in%20Intelligent%20Colonoscopy&body=Title%3A%20Frontiers%20in%20Intelligent%20Colonoscopy%0AAuthor%3A%20Ge-Peng%20Ji%20and%20Jingyi%20Liu%20and%20Peng%20Xu%20and%20Nick%20Barnes%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%20and%20Deng-Ping%20Fan%0AAbstract%3A%20%20%20Colonoscopy%20is%20currently%20one%20of%20the%20most%20sensitive%20screening%20methods%20for%0Acolorectal%20cancer.%20This%20study%20investigates%20the%20frontiers%20of%20intelligent%0Acolonoscopy%20techniques%20and%20their%20prospective%20implications%20for%20multimodal%0Amedical%20applications.%20With%20this%20goal%2C%20we%20begin%20by%20assessing%20the%20current%0Adata-centric%20and%20model-centric%20landscapes%20through%20four%20tasks%20for%20colonoscopic%0Ascene%20perception%2C%20including%20classification%2C%20detection%2C%20segmentation%2C%20and%0Avision-language%20understanding.%20This%20assessment%20enables%20us%20to%20identify%0Adomain-specific%20challenges%20and%20reveals%20that%20multimodal%20research%20in%20colonoscopy%0Aremains%20open%20for%20further%20exploration.%20To%20embrace%20the%20coming%20multimodal%20era%2C%20we%0Aestablish%20three%20foundational%20initiatives%3A%20a%20large-scale%20multimodal%20instruction%0Atuning%20dataset%20ColonINST%2C%20a%20colonoscopy-designed%20multimodal%20language%20model%0AColonGPT%2C%20and%20a%20multimodal%20benchmark.%20To%20facilitate%20ongoing%20monitoring%20of%20this%0Arapidly%20evolving%20field%2C%20we%20provide%20a%20public%20website%20for%20the%20latest%20updates%3A%0Ahttps%3A//github.com/ai4colonoscopy/IntelliScope.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrontiers%2520in%2520Intelligent%2520Colonoscopy%26entry.906535625%3DGe-Peng%2520Ji%2520and%2520Jingyi%2520Liu%2520and%2520Peng%2520Xu%2520and%2520Nick%2520Barnes%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%2520and%2520Deng-Ping%2520Fan%26entry.1292438233%3D%2520%2520Colonoscopy%2520is%2520currently%2520one%2520of%2520the%2520most%2520sensitive%2520screening%2520methods%2520for%250Acolorectal%2520cancer.%2520This%2520study%2520investigates%2520the%2520frontiers%2520of%2520intelligent%250Acolonoscopy%2520techniques%2520and%2520their%2520prospective%2520implications%2520for%2520multimodal%250Amedical%2520applications.%2520With%2520this%2520goal%252C%2520we%2520begin%2520by%2520assessing%2520the%2520current%250Adata-centric%2520and%2520model-centric%2520landscapes%2520through%2520four%2520tasks%2520for%2520colonoscopic%250Ascene%2520perception%252C%2520including%2520classification%252C%2520detection%252C%2520segmentation%252C%2520and%250Avision-language%2520understanding.%2520This%2520assessment%2520enables%2520us%2520to%2520identify%250Adomain-specific%2520challenges%2520and%2520reveals%2520that%2520multimodal%2520research%2520in%2520colonoscopy%250Aremains%2520open%2520for%2520further%2520exploration.%2520To%2520embrace%2520the%2520coming%2520multimodal%2520era%252C%2520we%250Aestablish%2520three%2520foundational%2520initiatives%253A%2520a%2520large-scale%2520multimodal%2520instruction%250Atuning%2520dataset%2520ColonINST%252C%2520a%2520colonoscopy-designed%2520multimodal%2520language%2520model%250AColonGPT%252C%2520and%2520a%2520multimodal%2520benchmark.%2520To%2520facilitate%2520ongoing%2520monitoring%2520of%2520this%250Arapidly%2520evolving%2520field%252C%2520we%2520provide%2520a%2520public%2520website%2520for%2520the%2520latest%2520updates%253A%250Ahttps%253A//github.com/ai4colonoscopy/IntelliScope.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frontiers%20in%20Intelligent%20Colonoscopy&entry.906535625=Ge-Peng%20Ji%20and%20Jingyi%20Liu%20and%20Peng%20Xu%20and%20Nick%20Barnes%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%20and%20Deng-Ping%20Fan&entry.1292438233=%20%20Colonoscopy%20is%20currently%20one%20of%20the%20most%20sensitive%20screening%20methods%20for%0Acolorectal%20cancer.%20This%20study%20investigates%20the%20frontiers%20of%20intelligent%0Acolonoscopy%20techniques%20and%20their%20prospective%20implications%20for%20multimodal%0Amedical%20applications.%20With%20this%20goal%2C%20we%20begin%20by%20assessing%20the%20current%0Adata-centric%20and%20model-centric%20landscapes%20through%20four%20tasks%20for%20colonoscopic%0Ascene%20perception%2C%20including%20classification%2C%20detection%2C%20segmentation%2C%20and%0Avision-language%20understanding.%20This%20assessment%20enables%20us%20to%20identify%0Adomain-specific%20challenges%20and%20reveals%20that%20multimodal%20research%20in%20colonoscopy%0Aremains%20open%20for%20further%20exploration.%20To%20embrace%20the%20coming%20multimodal%20era%2C%20we%0Aestablish%20three%20foundational%20initiatives%3A%20a%20large-scale%20multimodal%20instruction%0Atuning%20dataset%20ColonINST%2C%20a%20colonoscopy-designed%20multimodal%20language%20model%0AColonGPT%2C%20and%20a%20multimodal%20benchmark.%20To%20facilitate%20ongoing%20monitoring%20of%20this%0Arapidly%20evolving%20field%2C%20we%20provide%20a%20public%20website%20for%20the%20latest%20updates%3A%0Ahttps%3A//github.com/ai4colonoscopy/IntelliScope.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17241v1&entry.124074799=Read"},
{"title": "SPVSoAP3D: A Second-order Average Pooling Approach to enhance 3D Place\n  Recognition in Horticultural Environments", "author": "T. Barros and C. Premebida and S. Aravecchia and C. Pradalier and U. J. Nunes", "abstract": "  3D LiDAR-based place recognition has been extensively researched in urban\nenvironments, yet it remains underexplored in agricultural settings. Unlike\nurban contexts, horticultural environments, characterized by their permeability\nto laser beams, result in sparse and overlapping LiDAR scans with suboptimal\ngeometries. This phenomenon leads to intra- and inter-row descriptor ambiguity.\nIn this work, we address this challenge by introducing SPVSoAP3D, a novel\nmodeling approach that combines a voxel-based feature extraction network with\nan aggregation technique based on a second-order average pooling operator,\ncomplemented by a descriptor enhancement stage. Furthermore, we augment the\nexisting HORTO-3DLM dataset by introducing two new sequences derived from\nhorticultural environments. We evaluate the performance of SPVSoAP3D against\nstate-of-the-art (SOTA) models, including OverlapTransformer, PointNetVLAD, and\nLOGG3D-Net, utilizing a cross-validation protocol on both the newly introduced\nsequences and the existing HORTO-3DLM dataset. The findings indicate that the\naverage operator is more suitable for horticultural environments compared to\nthe max operator and other first-order pooling techniques. Additionally, the\nresults highlight the improvements brought by the descriptor enhancement stage.\n", "link": "http://arxiv.org/abs/2410.17017v1", "date": "2024-10-22", "relevancy": 2.7331, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPVSoAP3D%3A%20A%20Second-order%20Average%20Pooling%20Approach%20to%20enhance%203D%20Place%0A%20%20Recognition%20in%20Horticultural%20Environments&body=Title%3A%20SPVSoAP3D%3A%20A%20Second-order%20Average%20Pooling%20Approach%20to%20enhance%203D%20Place%0A%20%20Recognition%20in%20Horticultural%20Environments%0AAuthor%3A%20T.%20Barros%20and%20C.%20Premebida%20and%20S.%20Aravecchia%20and%20C.%20Pradalier%20and%20U.%20J.%20Nunes%0AAbstract%3A%20%20%203D%20LiDAR-based%20place%20recognition%20has%20been%20extensively%20researched%20in%20urban%0Aenvironments%2C%20yet%20it%20remains%20underexplored%20in%20agricultural%20settings.%20Unlike%0Aurban%20contexts%2C%20horticultural%20environments%2C%20characterized%20by%20their%20permeability%0Ato%20laser%20beams%2C%20result%20in%20sparse%20and%20overlapping%20LiDAR%20scans%20with%20suboptimal%0Ageometries.%20This%20phenomenon%20leads%20to%20intra-%20and%20inter-row%20descriptor%20ambiguity.%0AIn%20this%20work%2C%20we%20address%20this%20challenge%20by%20introducing%20SPVSoAP3D%2C%20a%20novel%0Amodeling%20approach%20that%20combines%20a%20voxel-based%20feature%20extraction%20network%20with%0Aan%20aggregation%20technique%20based%20on%20a%20second-order%20average%20pooling%20operator%2C%0Acomplemented%20by%20a%20descriptor%20enhancement%20stage.%20Furthermore%2C%20we%20augment%20the%0Aexisting%20HORTO-3DLM%20dataset%20by%20introducing%20two%20new%20sequences%20derived%20from%0Ahorticultural%20environments.%20We%20evaluate%20the%20performance%20of%20SPVSoAP3D%20against%0Astate-of-the-art%20%28SOTA%29%20models%2C%20including%20OverlapTransformer%2C%20PointNetVLAD%2C%20and%0ALOGG3D-Net%2C%20utilizing%20a%20cross-validation%20protocol%20on%20both%20the%20newly%20introduced%0Asequences%20and%20the%20existing%20HORTO-3DLM%20dataset.%20The%20findings%20indicate%20that%20the%0Aaverage%20operator%20is%20more%20suitable%20for%20horticultural%20environments%20compared%20to%0Athe%20max%20operator%20and%20other%20first-order%20pooling%20techniques.%20Additionally%2C%20the%0Aresults%20highlight%20the%20improvements%20brought%20by%20the%20descriptor%20enhancement%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPVSoAP3D%253A%2520A%2520Second-order%2520Average%2520Pooling%2520Approach%2520to%2520enhance%25203D%2520Place%250A%2520%2520Recognition%2520in%2520Horticultural%2520Environments%26entry.906535625%3DT.%2520Barros%2520and%2520C.%2520Premebida%2520and%2520S.%2520Aravecchia%2520and%2520C.%2520Pradalier%2520and%2520U.%2520J.%2520Nunes%26entry.1292438233%3D%2520%25203D%2520LiDAR-based%2520place%2520recognition%2520has%2520been%2520extensively%2520researched%2520in%2520urban%250Aenvironments%252C%2520yet%2520it%2520remains%2520underexplored%2520in%2520agricultural%2520settings.%2520Unlike%250Aurban%2520contexts%252C%2520horticultural%2520environments%252C%2520characterized%2520by%2520their%2520permeability%250Ato%2520laser%2520beams%252C%2520result%2520in%2520sparse%2520and%2520overlapping%2520LiDAR%2520scans%2520with%2520suboptimal%250Ageometries.%2520This%2520phenomenon%2520leads%2520to%2520intra-%2520and%2520inter-row%2520descriptor%2520ambiguity.%250AIn%2520this%2520work%252C%2520we%2520address%2520this%2520challenge%2520by%2520introducing%2520SPVSoAP3D%252C%2520a%2520novel%250Amodeling%2520approach%2520that%2520combines%2520a%2520voxel-based%2520feature%2520extraction%2520network%2520with%250Aan%2520aggregation%2520technique%2520based%2520on%2520a%2520second-order%2520average%2520pooling%2520operator%252C%250Acomplemented%2520by%2520a%2520descriptor%2520enhancement%2520stage.%2520Furthermore%252C%2520we%2520augment%2520the%250Aexisting%2520HORTO-3DLM%2520dataset%2520by%2520introducing%2520two%2520new%2520sequences%2520derived%2520from%250Ahorticultural%2520environments.%2520We%2520evaluate%2520the%2520performance%2520of%2520SPVSoAP3D%2520against%250Astate-of-the-art%2520%2528SOTA%2529%2520models%252C%2520including%2520OverlapTransformer%252C%2520PointNetVLAD%252C%2520and%250ALOGG3D-Net%252C%2520utilizing%2520a%2520cross-validation%2520protocol%2520on%2520both%2520the%2520newly%2520introduced%250Asequences%2520and%2520the%2520existing%2520HORTO-3DLM%2520dataset.%2520The%2520findings%2520indicate%2520that%2520the%250Aaverage%2520operator%2520is%2520more%2520suitable%2520for%2520horticultural%2520environments%2520compared%2520to%250Athe%2520max%2520operator%2520and%2520other%2520first-order%2520pooling%2520techniques.%2520Additionally%252C%2520the%250Aresults%2520highlight%2520the%2520improvements%2520brought%2520by%2520the%2520descriptor%2520enhancement%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPVSoAP3D%3A%20A%20Second-order%20Average%20Pooling%20Approach%20to%20enhance%203D%20Place%0A%20%20Recognition%20in%20Horticultural%20Environments&entry.906535625=T.%20Barros%20and%20C.%20Premebida%20and%20S.%20Aravecchia%20and%20C.%20Pradalier%20and%20U.%20J.%20Nunes&entry.1292438233=%20%203D%20LiDAR-based%20place%20recognition%20has%20been%20extensively%20researched%20in%20urban%0Aenvironments%2C%20yet%20it%20remains%20underexplored%20in%20agricultural%20settings.%20Unlike%0Aurban%20contexts%2C%20horticultural%20environments%2C%20characterized%20by%20their%20permeability%0Ato%20laser%20beams%2C%20result%20in%20sparse%20and%20overlapping%20LiDAR%20scans%20with%20suboptimal%0Ageometries.%20This%20phenomenon%20leads%20to%20intra-%20and%20inter-row%20descriptor%20ambiguity.%0AIn%20this%20work%2C%20we%20address%20this%20challenge%20by%20introducing%20SPVSoAP3D%2C%20a%20novel%0Amodeling%20approach%20that%20combines%20a%20voxel-based%20feature%20extraction%20network%20with%0Aan%20aggregation%20technique%20based%20on%20a%20second-order%20average%20pooling%20operator%2C%0Acomplemented%20by%20a%20descriptor%20enhancement%20stage.%20Furthermore%2C%20we%20augment%20the%0Aexisting%20HORTO-3DLM%20dataset%20by%20introducing%20two%20new%20sequences%20derived%20from%0Ahorticultural%20environments.%20We%20evaluate%20the%20performance%20of%20SPVSoAP3D%20against%0Astate-of-the-art%20%28SOTA%29%20models%2C%20including%20OverlapTransformer%2C%20PointNetVLAD%2C%20and%0ALOGG3D-Net%2C%20utilizing%20a%20cross-validation%20protocol%20on%20both%20the%20newly%20introduced%0Asequences%20and%20the%20existing%20HORTO-3DLM%20dataset.%20The%20findings%20indicate%20that%20the%0Aaverage%20operator%20is%20more%20suitable%20for%20horticultural%20environments%20compared%20to%0Athe%20max%20operator%20and%20other%20first-order%20pooling%20techniques.%20Additionally%2C%20the%0Aresults%20highlight%20the%20improvements%20brought%20by%20the%20descriptor%20enhancement%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17017v1&entry.124074799=Read"},
{"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK\n  Perspective", "author": "Akiyoshi Tomihari and Issei Sato", "abstract": "  The two-stage fine-tuning (FT) method, linear probing (LP) then fine-tuning\n(LP-FT), outperforms linear probing and FT alone. This holds true for both\nin-distribution (ID) and out-of-distribution (OOD) data. One key reason for its\nsuccess is the preservation of pre-trained features, achieved by obtaining a\nnear-optimal linear head during LP. However, despite the widespread use of\nlarge language models, there has been limited exploration of more complex\narchitectures such as Transformers. In this paper, we analyze the training\ndynamics of LP-FT for classification tasks on the basis of the neural tangent\nkernel (NTK) theory. Our analysis decomposes the NTK matrix into two\ncomponents. This decomposition highlights the importance of the linear head\nnorm alongside the prediction accuracy at the start of the FT stage. We also\nobserve a significant increase in the linear head norm during LP, which stems\nfrom training with the cross-entropy (CE) loss. This increase in the linear\nhead norm effectively reduces changes in learned features. Furthermore, we find\nthat this increased norm can adversely affect model calibration, which can be\ncorrected using temperature scaling. Additionally, we extend our analysis with\nthe NTK to the low-rank adaptation (LoRA) method and validate its\neffectiveness. Our experiments using a Transformer-based model on multiple\nnatural language processing datasets confirm our theoretical analysis. Our\nstudy demonstrates the effectiveness of LP-FT for fine-tuning language models.\nCode is available at https://github.com/tom4649/lp-ft_ntk.\n", "link": "http://arxiv.org/abs/2405.16747v2", "date": "2024-10-22", "relevancy": 2.7054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Linear%20Probing%20then%20Fine-tuning%20Language%20Models%20from%20NTK%0A%20%20Perspective&body=Title%3A%20Understanding%20Linear%20Probing%20then%20Fine-tuning%20Language%20Models%20from%20NTK%0A%20%20Perspective%0AAuthor%3A%20Akiyoshi%20Tomihari%20and%20Issei%20Sato%0AAbstract%3A%20%20%20The%20two-stage%20fine-tuning%20%28FT%29%20method%2C%20linear%20probing%20%28LP%29%20then%20fine-tuning%0A%28LP-FT%29%2C%20outperforms%20linear%20probing%20and%20FT%20alone.%20This%20holds%20true%20for%20both%0Ain-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%20data.%20One%20key%20reason%20for%20its%0Asuccess%20is%20the%20preservation%20of%20pre-trained%20features%2C%20achieved%20by%20obtaining%20a%0Anear-optimal%20linear%20head%20during%20LP.%20However%2C%20despite%20the%20widespread%20use%20of%0Alarge%20language%20models%2C%20there%20has%20been%20limited%20exploration%20of%20more%20complex%0Aarchitectures%20such%20as%20Transformers.%20In%20this%20paper%2C%20we%20analyze%20the%20training%0Adynamics%20of%20LP-FT%20for%20classification%20tasks%20on%20the%20basis%20of%20the%20neural%20tangent%0Akernel%20%28NTK%29%20theory.%20Our%20analysis%20decomposes%20the%20NTK%20matrix%20into%20two%0Acomponents.%20This%20decomposition%20highlights%20the%20importance%20of%20the%20linear%20head%0Anorm%20alongside%20the%20prediction%20accuracy%20at%20the%20start%20of%20the%20FT%20stage.%20We%20also%0Aobserve%20a%20significant%20increase%20in%20the%20linear%20head%20norm%20during%20LP%2C%20which%20stems%0Afrom%20training%20with%20the%20cross-entropy%20%28CE%29%20loss.%20This%20increase%20in%20the%20linear%0Ahead%20norm%20effectively%20reduces%20changes%20in%20learned%20features.%20Furthermore%2C%20we%20find%0Athat%20this%20increased%20norm%20can%20adversely%20affect%20model%20calibration%2C%20which%20can%20be%0Acorrected%20using%20temperature%20scaling.%20Additionally%2C%20we%20extend%20our%20analysis%20with%0Athe%20NTK%20to%20the%20low-rank%20adaptation%20%28LoRA%29%20method%20and%20validate%20its%0Aeffectiveness.%20Our%20experiments%20using%20a%20Transformer-based%20model%20on%20multiple%0Anatural%20language%20processing%20datasets%20confirm%20our%20theoretical%20analysis.%20Our%0Astudy%20demonstrates%20the%20effectiveness%20of%20LP-FT%20for%20fine-tuning%20language%20models.%0ACode%20is%20available%20at%20https%3A//github.com/tom4649/lp-ft_ntk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Linear%2520Probing%2520then%2520Fine-tuning%2520Language%2520Models%2520from%2520NTK%250A%2520%2520Perspective%26entry.906535625%3DAkiyoshi%2520Tomihari%2520and%2520Issei%2520Sato%26entry.1292438233%3D%2520%2520The%2520two-stage%2520fine-tuning%2520%2528FT%2529%2520method%252C%2520linear%2520probing%2520%2528LP%2529%2520then%2520fine-tuning%250A%2528LP-FT%2529%252C%2520outperforms%2520linear%2520probing%2520and%2520FT%2520alone.%2520This%2520holds%2520true%2520for%2520both%250Ain-distribution%2520%2528ID%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%2520data.%2520One%2520key%2520reason%2520for%2520its%250Asuccess%2520is%2520the%2520preservation%2520of%2520pre-trained%2520features%252C%2520achieved%2520by%2520obtaining%2520a%250Anear-optimal%2520linear%2520head%2520during%2520LP.%2520However%252C%2520despite%2520the%2520widespread%2520use%2520of%250Alarge%2520language%2520models%252C%2520there%2520has%2520been%2520limited%2520exploration%2520of%2520more%2520complex%250Aarchitectures%2520such%2520as%2520Transformers.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520training%250Adynamics%2520of%2520LP-FT%2520for%2520classification%2520tasks%2520on%2520the%2520basis%2520of%2520the%2520neural%2520tangent%250Akernel%2520%2528NTK%2529%2520theory.%2520Our%2520analysis%2520decomposes%2520the%2520NTK%2520matrix%2520into%2520two%250Acomponents.%2520This%2520decomposition%2520highlights%2520the%2520importance%2520of%2520the%2520linear%2520head%250Anorm%2520alongside%2520the%2520prediction%2520accuracy%2520at%2520the%2520start%2520of%2520the%2520FT%2520stage.%2520We%2520also%250Aobserve%2520a%2520significant%2520increase%2520in%2520the%2520linear%2520head%2520norm%2520during%2520LP%252C%2520which%2520stems%250Afrom%2520training%2520with%2520the%2520cross-entropy%2520%2528CE%2529%2520loss.%2520This%2520increase%2520in%2520the%2520linear%250Ahead%2520norm%2520effectively%2520reduces%2520changes%2520in%2520learned%2520features.%2520Furthermore%252C%2520we%2520find%250Athat%2520this%2520increased%2520norm%2520can%2520adversely%2520affect%2520model%2520calibration%252C%2520which%2520can%2520be%250Acorrected%2520using%2520temperature%2520scaling.%2520Additionally%252C%2520we%2520extend%2520our%2520analysis%2520with%250Athe%2520NTK%2520to%2520the%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520method%2520and%2520validate%2520its%250Aeffectiveness.%2520Our%2520experiments%2520using%2520a%2520Transformer-based%2520model%2520on%2520multiple%250Anatural%2520language%2520processing%2520datasets%2520confirm%2520our%2520theoretical%2520analysis.%2520Our%250Astudy%2520demonstrates%2520the%2520effectiveness%2520of%2520LP-FT%2520for%2520fine-tuning%2520language%2520models.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/tom4649/lp-ft_ntk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Linear%20Probing%20then%20Fine-tuning%20Language%20Models%20from%20NTK%0A%20%20Perspective&entry.906535625=Akiyoshi%20Tomihari%20and%20Issei%20Sato&entry.1292438233=%20%20The%20two-stage%20fine-tuning%20%28FT%29%20method%2C%20linear%20probing%20%28LP%29%20then%20fine-tuning%0A%28LP-FT%29%2C%20outperforms%20linear%20probing%20and%20FT%20alone.%20This%20holds%20true%20for%20both%0Ain-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%20data.%20One%20key%20reason%20for%20its%0Asuccess%20is%20the%20preservation%20of%20pre-trained%20features%2C%20achieved%20by%20obtaining%20a%0Anear-optimal%20linear%20head%20during%20LP.%20However%2C%20despite%20the%20widespread%20use%20of%0Alarge%20language%20models%2C%20there%20has%20been%20limited%20exploration%20of%20more%20complex%0Aarchitectures%20such%20as%20Transformers.%20In%20this%20paper%2C%20we%20analyze%20the%20training%0Adynamics%20of%20LP-FT%20for%20classification%20tasks%20on%20the%20basis%20of%20the%20neural%20tangent%0Akernel%20%28NTK%29%20theory.%20Our%20analysis%20decomposes%20the%20NTK%20matrix%20into%20two%0Acomponents.%20This%20decomposition%20highlights%20the%20importance%20of%20the%20linear%20head%0Anorm%20alongside%20the%20prediction%20accuracy%20at%20the%20start%20of%20the%20FT%20stage.%20We%20also%0Aobserve%20a%20significant%20increase%20in%20the%20linear%20head%20norm%20during%20LP%2C%20which%20stems%0Afrom%20training%20with%20the%20cross-entropy%20%28CE%29%20loss.%20This%20increase%20in%20the%20linear%0Ahead%20norm%20effectively%20reduces%20changes%20in%20learned%20features.%20Furthermore%2C%20we%20find%0Athat%20this%20increased%20norm%20can%20adversely%20affect%20model%20calibration%2C%20which%20can%20be%0Acorrected%20using%20temperature%20scaling.%20Additionally%2C%20we%20extend%20our%20analysis%20with%0Athe%20NTK%20to%20the%20low-rank%20adaptation%20%28LoRA%29%20method%20and%20validate%20its%0Aeffectiveness.%20Our%20experiments%20using%20a%20Transformer-based%20model%20on%20multiple%0Anatural%20language%20processing%20datasets%20confirm%20our%20theoretical%20analysis.%20Our%0Astudy%20demonstrates%20the%20effectiveness%20of%20LP-FT%20for%20fine-tuning%20language%20models.%0ACode%20is%20available%20at%20https%3A//github.com/tom4649/lp-ft_ntk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16747v2&entry.124074799=Read"},
{"title": "Joint Point Cloud Upsampling and Cleaning with Octree-based CNNs", "author": "Jihe Li and Bo Pang and Peng-Shuai Wang", "abstract": "  Recovering dense and uniformly distributed point clouds from sparse or noisy\ndata remains a significant challenge. Recently, great progress has been made on\nthese tasks, but usually at the cost of increasingly intricate modules or\ncomplicated network architectures, leading to long inference time and huge\nresource consumption. Instead, we embrace simplicity and present a simple yet\nefficient method for jointly upsampling and cleaning point clouds. Our method\nleverages an off-the-shelf octree-based 3D U-Net (OUNet) with minor\nmodifications, enabling the upsampling and cleaning tasks within a single\nnetwork. Our network directly processes each input point cloud as a whole\ninstead of processing each point cloud patch as in previous works, which\nsignificantly eases the implementation and brings at least 47 times faster\ninference. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performances under huge efficiency advantages on a series of\nbenchmarks. We expect our method to serve simple baselines and inspire\nresearchers to rethink the method design on point cloud upsampling and\ncleaning.\n", "link": "http://arxiv.org/abs/2410.17001v1", "date": "2024-10-22", "relevancy": 2.6861, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5503}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5347}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Point%20Cloud%20Upsampling%20and%20Cleaning%20with%20Octree-based%20CNNs&body=Title%3A%20Joint%20Point%20Cloud%20Upsampling%20and%20Cleaning%20with%20Octree-based%20CNNs%0AAuthor%3A%20Jihe%20Li%20and%20Bo%20Pang%20and%20Peng-Shuai%20Wang%0AAbstract%3A%20%20%20Recovering%20dense%20and%20uniformly%20distributed%20point%20clouds%20from%20sparse%20or%20noisy%0Adata%20remains%20a%20significant%20challenge.%20Recently%2C%20great%20progress%20has%20been%20made%20on%0Athese%20tasks%2C%20but%20usually%20at%20the%20cost%20of%20increasingly%20intricate%20modules%20or%0Acomplicated%20network%20architectures%2C%20leading%20to%20long%20inference%20time%20and%20huge%0Aresource%20consumption.%20Instead%2C%20we%20embrace%20simplicity%20and%20present%20a%20simple%20yet%0Aefficient%20method%20for%20jointly%20upsampling%20and%20cleaning%20point%20clouds.%20Our%20method%0Aleverages%20an%20off-the-shelf%20octree-based%203D%20U-Net%20%28OUNet%29%20with%20minor%0Amodifications%2C%20enabling%20the%20upsampling%20and%20cleaning%20tasks%20within%20a%20single%0Anetwork.%20Our%20network%20directly%20processes%20each%20input%20point%20cloud%20as%20a%20whole%0Ainstead%20of%20processing%20each%20point%20cloud%20patch%20as%20in%20previous%20works%2C%20which%0Asignificantly%20eases%20the%20implementation%20and%20brings%20at%20least%2047%20times%20faster%0Ainference.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performances%20under%20huge%20efficiency%20advantages%20on%20a%20series%20of%0Abenchmarks.%20We%20expect%20our%20method%20to%20serve%20simple%20baselines%20and%20inspire%0Aresearchers%20to%20rethink%20the%20method%20design%20on%20point%20cloud%20upsampling%20and%0Acleaning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Point%2520Cloud%2520Upsampling%2520and%2520Cleaning%2520with%2520Octree-based%2520CNNs%26entry.906535625%3DJihe%2520Li%2520and%2520Bo%2520Pang%2520and%2520Peng-Shuai%2520Wang%26entry.1292438233%3D%2520%2520Recovering%2520dense%2520and%2520uniformly%2520distributed%2520point%2520clouds%2520from%2520sparse%2520or%2520noisy%250Adata%2520remains%2520a%2520significant%2520challenge.%2520Recently%252C%2520great%2520progress%2520has%2520been%2520made%2520on%250Athese%2520tasks%252C%2520but%2520usually%2520at%2520the%2520cost%2520of%2520increasingly%2520intricate%2520modules%2520or%250Acomplicated%2520network%2520architectures%252C%2520leading%2520to%2520long%2520inference%2520time%2520and%2520huge%250Aresource%2520consumption.%2520Instead%252C%2520we%2520embrace%2520simplicity%2520and%2520present%2520a%2520simple%2520yet%250Aefficient%2520method%2520for%2520jointly%2520upsampling%2520and%2520cleaning%2520point%2520clouds.%2520Our%2520method%250Aleverages%2520an%2520off-the-shelf%2520octree-based%25203D%2520U-Net%2520%2528OUNet%2529%2520with%2520minor%250Amodifications%252C%2520enabling%2520the%2520upsampling%2520and%2520cleaning%2520tasks%2520within%2520a%2520single%250Anetwork.%2520Our%2520network%2520directly%2520processes%2520each%2520input%2520point%2520cloud%2520as%2520a%2520whole%250Ainstead%2520of%2520processing%2520each%2520point%2520cloud%2520patch%2520as%2520in%2520previous%2520works%252C%2520which%250Asignificantly%2520eases%2520the%2520implementation%2520and%2520brings%2520at%2520least%252047%2520times%2520faster%250Ainference.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performances%2520under%2520huge%2520efficiency%2520advantages%2520on%2520a%2520series%2520of%250Abenchmarks.%2520We%2520expect%2520our%2520method%2520to%2520serve%2520simple%2520baselines%2520and%2520inspire%250Aresearchers%2520to%2520rethink%2520the%2520method%2520design%2520on%2520point%2520cloud%2520upsampling%2520and%250Acleaning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Point%20Cloud%20Upsampling%20and%20Cleaning%20with%20Octree-based%20CNNs&entry.906535625=Jihe%20Li%20and%20Bo%20Pang%20and%20Peng-Shuai%20Wang&entry.1292438233=%20%20Recovering%20dense%20and%20uniformly%20distributed%20point%20clouds%20from%20sparse%20or%20noisy%0Adata%20remains%20a%20significant%20challenge.%20Recently%2C%20great%20progress%20has%20been%20made%20on%0Athese%20tasks%2C%20but%20usually%20at%20the%20cost%20of%20increasingly%20intricate%20modules%20or%0Acomplicated%20network%20architectures%2C%20leading%20to%20long%20inference%20time%20and%20huge%0Aresource%20consumption.%20Instead%2C%20we%20embrace%20simplicity%20and%20present%20a%20simple%20yet%0Aefficient%20method%20for%20jointly%20upsampling%20and%20cleaning%20point%20clouds.%20Our%20method%0Aleverages%20an%20off-the-shelf%20octree-based%203D%20U-Net%20%28OUNet%29%20with%20minor%0Amodifications%2C%20enabling%20the%20upsampling%20and%20cleaning%20tasks%20within%20a%20single%0Anetwork.%20Our%20network%20directly%20processes%20each%20input%20point%20cloud%20as%20a%20whole%0Ainstead%20of%20processing%20each%20point%20cloud%20patch%20as%20in%20previous%20works%2C%20which%0Asignificantly%20eases%20the%20implementation%20and%20brings%20at%20least%2047%20times%20faster%0Ainference.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performances%20under%20huge%20efficiency%20advantages%20on%20a%20series%20of%0Abenchmarks.%20We%20expect%20our%20method%20to%20serve%20simple%20baselines%20and%20inspire%0Aresearchers%20to%20rethink%20the%20method%20design%20on%20point%20cloud%20upsampling%20and%0Acleaning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17001v1&entry.124074799=Read"},
{"title": "Sample-Efficient Geometry Reconstruction from Euclidean Distances using\n  Non-Convex Optimization", "author": "Ipsita Ghosh and Abiy Tasissa and Christian K\u00fcmmerle", "abstract": "  The problem of finding suitable point embedding or geometric configurations\ngiven only Euclidean distance information of point pairs arises both as a core\ntask and as a sub-problem in a variety of machine learning applications. In\nthis paper, we aim to solve this problem given a minimal number of distance\nsamples. To this end, we leverage continuous and non-convex rank minimization\nformulations of the problem and establish a local convergence guarantee for a\nvariant of iteratively reweighted least squares (IRLS), which applies if a\nminimal random set of observed distances is provided. As a technical tool, we\nestablish a restricted isometry property (RIP) restricted to a tangent space of\nthe manifold of symmetric rank-$r$ matrices given random Euclidean distance\nmeasurements, which might be of independent interest for the analysis of other\nnon-convex approaches. Furthermore, we assess data efficiency, scalability and\ngeneralizability of different reconstruction algorithms through numerical\nexperiments with simulated data as well as real-world data, demonstrating the\nproposed algorithm's ability to identify the underlying geometry from fewer\ndistance samples compared to the state-of-the-art.\n", "link": "http://arxiv.org/abs/2410.16982v1", "date": "2024-10-22", "relevancy": 2.6736, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5558}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5397}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-Efficient%20Geometry%20Reconstruction%20from%20Euclidean%20Distances%20using%0A%20%20Non-Convex%20Optimization&body=Title%3A%20Sample-Efficient%20Geometry%20Reconstruction%20from%20Euclidean%20Distances%20using%0A%20%20Non-Convex%20Optimization%0AAuthor%3A%20Ipsita%20Ghosh%20and%20Abiy%20Tasissa%20and%20Christian%20K%C3%BCmmerle%0AAbstract%3A%20%20%20The%20problem%20of%20finding%20suitable%20point%20embedding%20or%20geometric%20configurations%0Agiven%20only%20Euclidean%20distance%20information%20of%20point%20pairs%20arises%20both%20as%20a%20core%0Atask%20and%20as%20a%20sub-problem%20in%20a%20variety%20of%20machine%20learning%20applications.%20In%0Athis%20paper%2C%20we%20aim%20to%20solve%20this%20problem%20given%20a%20minimal%20number%20of%20distance%0Asamples.%20To%20this%20end%2C%20we%20leverage%20continuous%20and%20non-convex%20rank%20minimization%0Aformulations%20of%20the%20problem%20and%20establish%20a%20local%20convergence%20guarantee%20for%20a%0Avariant%20of%20iteratively%20reweighted%20least%20squares%20%28IRLS%29%2C%20which%20applies%20if%20a%0Aminimal%20random%20set%20of%20observed%20distances%20is%20provided.%20As%20a%20technical%20tool%2C%20we%0Aestablish%20a%20restricted%20isometry%20property%20%28RIP%29%20restricted%20to%20a%20tangent%20space%20of%0Athe%20manifold%20of%20symmetric%20rank-%24r%24%20matrices%20given%20random%20Euclidean%20distance%0Ameasurements%2C%20which%20might%20be%20of%20independent%20interest%20for%20the%20analysis%20of%20other%0Anon-convex%20approaches.%20Furthermore%2C%20we%20assess%20data%20efficiency%2C%20scalability%20and%0Ageneralizability%20of%20different%20reconstruction%20algorithms%20through%20numerical%0Aexperiments%20with%20simulated%20data%20as%20well%20as%20real-world%20data%2C%20demonstrating%20the%0Aproposed%20algorithm%27s%20ability%20to%20identify%20the%20underlying%20geometry%20from%20fewer%0Adistance%20samples%20compared%20to%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-Efficient%2520Geometry%2520Reconstruction%2520from%2520Euclidean%2520Distances%2520using%250A%2520%2520Non-Convex%2520Optimization%26entry.906535625%3DIpsita%2520Ghosh%2520and%2520Abiy%2520Tasissa%2520and%2520Christian%2520K%25C3%25BCmmerle%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520finding%2520suitable%2520point%2520embedding%2520or%2520geometric%2520configurations%250Agiven%2520only%2520Euclidean%2520distance%2520information%2520of%2520point%2520pairs%2520arises%2520both%2520as%2520a%2520core%250Atask%2520and%2520as%2520a%2520sub-problem%2520in%2520a%2520variety%2520of%2520machine%2520learning%2520applications.%2520In%250Athis%2520paper%252C%2520we%2520aim%2520to%2520solve%2520this%2520problem%2520given%2520a%2520minimal%2520number%2520of%2520distance%250Asamples.%2520To%2520this%2520end%252C%2520we%2520leverage%2520continuous%2520and%2520non-convex%2520rank%2520minimization%250Aformulations%2520of%2520the%2520problem%2520and%2520establish%2520a%2520local%2520convergence%2520guarantee%2520for%2520a%250Avariant%2520of%2520iteratively%2520reweighted%2520least%2520squares%2520%2528IRLS%2529%252C%2520which%2520applies%2520if%2520a%250Aminimal%2520random%2520set%2520of%2520observed%2520distances%2520is%2520provided.%2520As%2520a%2520technical%2520tool%252C%2520we%250Aestablish%2520a%2520restricted%2520isometry%2520property%2520%2528RIP%2529%2520restricted%2520to%2520a%2520tangent%2520space%2520of%250Athe%2520manifold%2520of%2520symmetric%2520rank-%2524r%2524%2520matrices%2520given%2520random%2520Euclidean%2520distance%250Ameasurements%252C%2520which%2520might%2520be%2520of%2520independent%2520interest%2520for%2520the%2520analysis%2520of%2520other%250Anon-convex%2520approaches.%2520Furthermore%252C%2520we%2520assess%2520data%2520efficiency%252C%2520scalability%2520and%250Ageneralizability%2520of%2520different%2520reconstruction%2520algorithms%2520through%2520numerical%250Aexperiments%2520with%2520simulated%2520data%2520as%2520well%2520as%2520real-world%2520data%252C%2520demonstrating%2520the%250Aproposed%2520algorithm%2527s%2520ability%2520to%2520identify%2520the%2520underlying%2520geometry%2520from%2520fewer%250Adistance%2520samples%2520compared%2520to%2520the%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-Efficient%20Geometry%20Reconstruction%20from%20Euclidean%20Distances%20using%0A%20%20Non-Convex%20Optimization&entry.906535625=Ipsita%20Ghosh%20and%20Abiy%20Tasissa%20and%20Christian%20K%C3%BCmmerle&entry.1292438233=%20%20The%20problem%20of%20finding%20suitable%20point%20embedding%20or%20geometric%20configurations%0Agiven%20only%20Euclidean%20distance%20information%20of%20point%20pairs%20arises%20both%20as%20a%20core%0Atask%20and%20as%20a%20sub-problem%20in%20a%20variety%20of%20machine%20learning%20applications.%20In%0Athis%20paper%2C%20we%20aim%20to%20solve%20this%20problem%20given%20a%20minimal%20number%20of%20distance%0Asamples.%20To%20this%20end%2C%20we%20leverage%20continuous%20and%20non-convex%20rank%20minimization%0Aformulations%20of%20the%20problem%20and%20establish%20a%20local%20convergence%20guarantee%20for%20a%0Avariant%20of%20iteratively%20reweighted%20least%20squares%20%28IRLS%29%2C%20which%20applies%20if%20a%0Aminimal%20random%20set%20of%20observed%20distances%20is%20provided.%20As%20a%20technical%20tool%2C%20we%0Aestablish%20a%20restricted%20isometry%20property%20%28RIP%29%20restricted%20to%20a%20tangent%20space%20of%0Athe%20manifold%20of%20symmetric%20rank-%24r%24%20matrices%20given%20random%20Euclidean%20distance%0Ameasurements%2C%20which%20might%20be%20of%20independent%20interest%20for%20the%20analysis%20of%20other%0Anon-convex%20approaches.%20Furthermore%2C%20we%20assess%20data%20efficiency%2C%20scalability%20and%0Ageneralizability%20of%20different%20reconstruction%20algorithms%20through%20numerical%0Aexperiments%20with%20simulated%20data%20as%20well%20as%20real-world%20data%2C%20demonstrating%20the%0Aproposed%20algorithm%27s%20ability%20to%20identify%20the%20underlying%20geometry%20from%20fewer%0Adistance%20samples%20compared%20to%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16982v1&entry.124074799=Read"},
{"title": "YOLO-TS: Real-Time Traffic Sign Detection with Enhanced Accuracy Using\n  Optimized Receptive Fields and Anchor-Free Fusion", "author": "Junzhou Chen and Heqiang Huang and Ronghui Zhang and Nengchao Lyu and Yanyong Guo and Hong-Ning Dai and Hong Yan", "abstract": "  Ensuring safety in both autonomous driving and advanced driver-assistance\nsystems (ADAS) depends critically on the efficient deployment of traffic sign\nrecognition technology. While current methods show effectiveness, they often\ncompromise between speed and accuracy. To address this issue, we present a\nnovel real-time and efficient road sign detection network, YOLO-TS. This\nnetwork significantly improves performance by optimizing the receptive fields\nof multi-scale feature maps to align more closely with the size distribution of\ntraffic signs in various datasets. Moreover, our innovative feature-fusion\nstrategy, leveraging the flexibility of Anchor-Free methods, allows for\nmulti-scale object detection on a high-resolution feature map abundant in\ncontextual information, achieving remarkable enhancements in both accuracy and\nspeed. To mitigate the adverse effects of the grid pattern caused by dilated\nconvolutions on the detection of smaller objects, we have devised a unique\nmodule that not only mitigates this grid effect but also widens the receptive\nfield to encompass an extensive range of spatial contextual information, thus\nboosting the efficiency of information usage. Evaluation on challenging public\ndatasets, TT100K and CCTSDB2021, demonstrates that YOLO-TS surpasses existing\nstate-of-the-art methods in terms of both accuracy and speed. The code for our\nmethod will be available.\n", "link": "http://arxiv.org/abs/2410.17144v1", "date": "2024-10-22", "relevancy": 2.6044, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5374}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5146}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLO-TS%3A%20Real-Time%20Traffic%20Sign%20Detection%20with%20Enhanced%20Accuracy%20Using%0A%20%20Optimized%20Receptive%20Fields%20and%20Anchor-Free%20Fusion&body=Title%3A%20YOLO-TS%3A%20Real-Time%20Traffic%20Sign%20Detection%20with%20Enhanced%20Accuracy%20Using%0A%20%20Optimized%20Receptive%20Fields%20and%20Anchor-Free%20Fusion%0AAuthor%3A%20Junzhou%20Chen%20and%20Heqiang%20Huang%20and%20Ronghui%20Zhang%20and%20Nengchao%20Lyu%20and%20Yanyong%20Guo%20and%20Hong-Ning%20Dai%20and%20Hong%20Yan%0AAbstract%3A%20%20%20Ensuring%20safety%20in%20both%20autonomous%20driving%20and%20advanced%20driver-assistance%0Asystems%20%28ADAS%29%20depends%20critically%20on%20the%20efficient%20deployment%20of%20traffic%20sign%0Arecognition%20technology.%20While%20current%20methods%20show%20effectiveness%2C%20they%20often%0Acompromise%20between%20speed%20and%20accuracy.%20To%20address%20this%20issue%2C%20we%20present%20a%0Anovel%20real-time%20and%20efficient%20road%20sign%20detection%20network%2C%20YOLO-TS.%20This%0Anetwork%20significantly%20improves%20performance%20by%20optimizing%20the%20receptive%20fields%0Aof%20multi-scale%20feature%20maps%20to%20align%20more%20closely%20with%20the%20size%20distribution%20of%0Atraffic%20signs%20in%20various%20datasets.%20Moreover%2C%20our%20innovative%20feature-fusion%0Astrategy%2C%20leveraging%20the%20flexibility%20of%20Anchor-Free%20methods%2C%20allows%20for%0Amulti-scale%20object%20detection%20on%20a%20high-resolution%20feature%20map%20abundant%20in%0Acontextual%20information%2C%20achieving%20remarkable%20enhancements%20in%20both%20accuracy%20and%0Aspeed.%20To%20mitigate%20the%20adverse%20effects%20of%20the%20grid%20pattern%20caused%20by%20dilated%0Aconvolutions%20on%20the%20detection%20of%20smaller%20objects%2C%20we%20have%20devised%20a%20unique%0Amodule%20that%20not%20only%20mitigates%20this%20grid%20effect%20but%20also%20widens%20the%20receptive%0Afield%20to%20encompass%20an%20extensive%20range%20of%20spatial%20contextual%20information%2C%20thus%0Aboosting%20the%20efficiency%20of%20information%20usage.%20Evaluation%20on%20challenging%20public%0Adatasets%2C%20TT100K%20and%20CCTSDB2021%2C%20demonstrates%20that%20YOLO-TS%20surpasses%20existing%0Astate-of-the-art%20methods%20in%20terms%20of%20both%20accuracy%20and%20speed.%20The%20code%20for%20our%0Amethod%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLO-TS%253A%2520Real-Time%2520Traffic%2520Sign%2520Detection%2520with%2520Enhanced%2520Accuracy%2520Using%250A%2520%2520Optimized%2520Receptive%2520Fields%2520and%2520Anchor-Free%2520Fusion%26entry.906535625%3DJunzhou%2520Chen%2520and%2520Heqiang%2520Huang%2520and%2520Ronghui%2520Zhang%2520and%2520Nengchao%2520Lyu%2520and%2520Yanyong%2520Guo%2520and%2520Hong-Ning%2520Dai%2520and%2520Hong%2520Yan%26entry.1292438233%3D%2520%2520Ensuring%2520safety%2520in%2520both%2520autonomous%2520driving%2520and%2520advanced%2520driver-assistance%250Asystems%2520%2528ADAS%2529%2520depends%2520critically%2520on%2520the%2520efficient%2520deployment%2520of%2520traffic%2520sign%250Arecognition%2520technology.%2520While%2520current%2520methods%2520show%2520effectiveness%252C%2520they%2520often%250Acompromise%2520between%2520speed%2520and%2520accuracy.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520a%250Anovel%2520real-time%2520and%2520efficient%2520road%2520sign%2520detection%2520network%252C%2520YOLO-TS.%2520This%250Anetwork%2520significantly%2520improves%2520performance%2520by%2520optimizing%2520the%2520receptive%2520fields%250Aof%2520multi-scale%2520feature%2520maps%2520to%2520align%2520more%2520closely%2520with%2520the%2520size%2520distribution%2520of%250Atraffic%2520signs%2520in%2520various%2520datasets.%2520Moreover%252C%2520our%2520innovative%2520feature-fusion%250Astrategy%252C%2520leveraging%2520the%2520flexibility%2520of%2520Anchor-Free%2520methods%252C%2520allows%2520for%250Amulti-scale%2520object%2520detection%2520on%2520a%2520high-resolution%2520feature%2520map%2520abundant%2520in%250Acontextual%2520information%252C%2520achieving%2520remarkable%2520enhancements%2520in%2520both%2520accuracy%2520and%250Aspeed.%2520To%2520mitigate%2520the%2520adverse%2520effects%2520of%2520the%2520grid%2520pattern%2520caused%2520by%2520dilated%250Aconvolutions%2520on%2520the%2520detection%2520of%2520smaller%2520objects%252C%2520we%2520have%2520devised%2520a%2520unique%250Amodule%2520that%2520not%2520only%2520mitigates%2520this%2520grid%2520effect%2520but%2520also%2520widens%2520the%2520receptive%250Afield%2520to%2520encompass%2520an%2520extensive%2520range%2520of%2520spatial%2520contextual%2520information%252C%2520thus%250Aboosting%2520the%2520efficiency%2520of%2520information%2520usage.%2520Evaluation%2520on%2520challenging%2520public%250Adatasets%252C%2520TT100K%2520and%2520CCTSDB2021%252C%2520demonstrates%2520that%2520YOLO-TS%2520surpasses%2520existing%250Astate-of-the-art%2520methods%2520in%2520terms%2520of%2520both%2520accuracy%2520and%2520speed.%2520The%2520code%2520for%2520our%250Amethod%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLO-TS%3A%20Real-Time%20Traffic%20Sign%20Detection%20with%20Enhanced%20Accuracy%20Using%0A%20%20Optimized%20Receptive%20Fields%20and%20Anchor-Free%20Fusion&entry.906535625=Junzhou%20Chen%20and%20Heqiang%20Huang%20and%20Ronghui%20Zhang%20and%20Nengchao%20Lyu%20and%20Yanyong%20Guo%20and%20Hong-Ning%20Dai%20and%20Hong%20Yan&entry.1292438233=%20%20Ensuring%20safety%20in%20both%20autonomous%20driving%20and%20advanced%20driver-assistance%0Asystems%20%28ADAS%29%20depends%20critically%20on%20the%20efficient%20deployment%20of%20traffic%20sign%0Arecognition%20technology.%20While%20current%20methods%20show%20effectiveness%2C%20they%20often%0Acompromise%20between%20speed%20and%20accuracy.%20To%20address%20this%20issue%2C%20we%20present%20a%0Anovel%20real-time%20and%20efficient%20road%20sign%20detection%20network%2C%20YOLO-TS.%20This%0Anetwork%20significantly%20improves%20performance%20by%20optimizing%20the%20receptive%20fields%0Aof%20multi-scale%20feature%20maps%20to%20align%20more%20closely%20with%20the%20size%20distribution%20of%0Atraffic%20signs%20in%20various%20datasets.%20Moreover%2C%20our%20innovative%20feature-fusion%0Astrategy%2C%20leveraging%20the%20flexibility%20of%20Anchor-Free%20methods%2C%20allows%20for%0Amulti-scale%20object%20detection%20on%20a%20high-resolution%20feature%20map%20abundant%20in%0Acontextual%20information%2C%20achieving%20remarkable%20enhancements%20in%20both%20accuracy%20and%0Aspeed.%20To%20mitigate%20the%20adverse%20effects%20of%20the%20grid%20pattern%20caused%20by%20dilated%0Aconvolutions%20on%20the%20detection%20of%20smaller%20objects%2C%20we%20have%20devised%20a%20unique%0Amodule%20that%20not%20only%20mitigates%20this%20grid%20effect%20but%20also%20widens%20the%20receptive%0Afield%20to%20encompass%20an%20extensive%20range%20of%20spatial%20contextual%20information%2C%20thus%0Aboosting%20the%20efficiency%20of%20information%20usage.%20Evaluation%20on%20challenging%20public%0Adatasets%2C%20TT100K%20and%20CCTSDB2021%2C%20demonstrates%20that%20YOLO-TS%20surpasses%20existing%0Astate-of-the-art%20methods%20in%20terms%20of%20both%20accuracy%20and%20speed.%20The%20code%20for%20our%0Amethod%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17144v1&entry.124074799=Read"},
{"title": "Towards Understanding the Working Mechanism of Text-to-Image Diffusion\n  Model", "author": "Mingyang Yi and Aoxue Li and Yi Xin and Zhenguo Li", "abstract": "  Recently, the strong latent Diffusion Probabilistic Model (DPM) has been\napplied to high-quality Text-to-Image (T2I) generation (e.g., Stable\nDiffusion), by injecting the encoded target text prompt into the gradually\ndenoised diffusion image generator. Despite the success of DPM in practice, the\nmechanism behind it remains to be explored. To fill this blank, we begin by\nexamining the intermediate statuses during the gradual denoising generation\nprocess in DPM. The empirical observations indicate, the shape of image is\nreconstructed after the first few denoising steps, and then the image is filled\nwith details (e.g., texture). The phenomenon is because the low-frequency\nsignal (shape relevant) of the noisy image is not corrupted until the final\nstage in the forward process (initial stage of generation) of adding noise in\nDPM. Inspired by the observations, we proceed to explore the influence of each\ntoken in the text prompt during the two stages. After a series of experiments\nof T2I generations conditioned on a set of text prompts. We conclude that in\nthe earlier generation stage, the image is mostly decided by the special token\n[\\texttt{EOS}] in the text prompt, and the information in the text prompt is\nalready conveyed in this stage. After that, the diffusion model completes the\ndetails of generated images by information from themselves. Finally, we propose\nto apply this observation to accelerate the process of T2I generation by\nproperly removing text guidance, which finally accelerates the sampling up to\n25\\%+.\n", "link": "http://arxiv.org/abs/2405.15330v2", "date": "2024-10-22", "relevancy": 2.5666, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6629}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6416}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20the%20Working%20Mechanism%20of%20Text-to-Image%20Diffusion%0A%20%20Model&body=Title%3A%20Towards%20Understanding%20the%20Working%20Mechanism%20of%20Text-to-Image%20Diffusion%0A%20%20Model%0AAuthor%3A%20Mingyang%20Yi%20and%20Aoxue%20Li%20and%20Yi%20Xin%20and%20Zhenguo%20Li%0AAbstract%3A%20%20%20Recently%2C%20the%20strong%20latent%20Diffusion%20Probabilistic%20Model%20%28DPM%29%20has%20been%0Aapplied%20to%20high-quality%20Text-to-Image%20%28T2I%29%20generation%20%28e.g.%2C%20Stable%0ADiffusion%29%2C%20by%20injecting%20the%20encoded%20target%20text%20prompt%20into%20the%20gradually%0Adenoised%20diffusion%20image%20generator.%20Despite%20the%20success%20of%20DPM%20in%20practice%2C%20the%0Amechanism%20behind%20it%20remains%20to%20be%20explored.%20To%20fill%20this%20blank%2C%20we%20begin%20by%0Aexamining%20the%20intermediate%20statuses%20during%20the%20gradual%20denoising%20generation%0Aprocess%20in%20DPM.%20The%20empirical%20observations%20indicate%2C%20the%20shape%20of%20image%20is%0Areconstructed%20after%20the%20first%20few%20denoising%20steps%2C%20and%20then%20the%20image%20is%20filled%0Awith%20details%20%28e.g.%2C%20texture%29.%20The%20phenomenon%20is%20because%20the%20low-frequency%0Asignal%20%28shape%20relevant%29%20of%20the%20noisy%20image%20is%20not%20corrupted%20until%20the%20final%0Astage%20in%20the%20forward%20process%20%28initial%20stage%20of%20generation%29%20of%20adding%20noise%20in%0ADPM.%20Inspired%20by%20the%20observations%2C%20we%20proceed%20to%20explore%20the%20influence%20of%20each%0Atoken%20in%20the%20text%20prompt%20during%20the%20two%20stages.%20After%20a%20series%20of%20experiments%0Aof%20T2I%20generations%20conditioned%20on%20a%20set%20of%20text%20prompts.%20We%20conclude%20that%20in%0Athe%20earlier%20generation%20stage%2C%20the%20image%20is%20mostly%20decided%20by%20the%20special%20token%0A%5B%5Ctexttt%7BEOS%7D%5D%20in%20the%20text%20prompt%2C%20and%20the%20information%20in%20the%20text%20prompt%20is%0Aalready%20conveyed%20in%20this%20stage.%20After%20that%2C%20the%20diffusion%20model%20completes%20the%0Adetails%20of%20generated%20images%20by%20information%20from%20themselves.%20Finally%2C%20we%20propose%0Ato%20apply%20this%20observation%20to%20accelerate%20the%20process%20of%20T2I%20generation%20by%0Aproperly%20removing%20text%20guidance%2C%20which%20finally%20accelerates%20the%20sampling%20up%20to%0A25%5C%25%2B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520the%2520Working%2520Mechanism%2520of%2520Text-to-Image%2520Diffusion%250A%2520%2520Model%26entry.906535625%3DMingyang%2520Yi%2520and%2520Aoxue%2520Li%2520and%2520Yi%2520Xin%2520and%2520Zhenguo%2520Li%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520strong%2520latent%2520Diffusion%2520Probabilistic%2520Model%2520%2528DPM%2529%2520has%2520been%250Aapplied%2520to%2520high-quality%2520Text-to-Image%2520%2528T2I%2529%2520generation%2520%2528e.g.%252C%2520Stable%250ADiffusion%2529%252C%2520by%2520injecting%2520the%2520encoded%2520target%2520text%2520prompt%2520into%2520the%2520gradually%250Adenoised%2520diffusion%2520image%2520generator.%2520Despite%2520the%2520success%2520of%2520DPM%2520in%2520practice%252C%2520the%250Amechanism%2520behind%2520it%2520remains%2520to%2520be%2520explored.%2520To%2520fill%2520this%2520blank%252C%2520we%2520begin%2520by%250Aexamining%2520the%2520intermediate%2520statuses%2520during%2520the%2520gradual%2520denoising%2520generation%250Aprocess%2520in%2520DPM.%2520The%2520empirical%2520observations%2520indicate%252C%2520the%2520shape%2520of%2520image%2520is%250Areconstructed%2520after%2520the%2520first%2520few%2520denoising%2520steps%252C%2520and%2520then%2520the%2520image%2520is%2520filled%250Awith%2520details%2520%2528e.g.%252C%2520texture%2529.%2520The%2520phenomenon%2520is%2520because%2520the%2520low-frequency%250Asignal%2520%2528shape%2520relevant%2529%2520of%2520the%2520noisy%2520image%2520is%2520not%2520corrupted%2520until%2520the%2520final%250Astage%2520in%2520the%2520forward%2520process%2520%2528initial%2520stage%2520of%2520generation%2529%2520of%2520adding%2520noise%2520in%250ADPM.%2520Inspired%2520by%2520the%2520observations%252C%2520we%2520proceed%2520to%2520explore%2520the%2520influence%2520of%2520each%250Atoken%2520in%2520the%2520text%2520prompt%2520during%2520the%2520two%2520stages.%2520After%2520a%2520series%2520of%2520experiments%250Aof%2520T2I%2520generations%2520conditioned%2520on%2520a%2520set%2520of%2520text%2520prompts.%2520We%2520conclude%2520that%2520in%250Athe%2520earlier%2520generation%2520stage%252C%2520the%2520image%2520is%2520mostly%2520decided%2520by%2520the%2520special%2520token%250A%255B%255Ctexttt%257BEOS%257D%255D%2520in%2520the%2520text%2520prompt%252C%2520and%2520the%2520information%2520in%2520the%2520text%2520prompt%2520is%250Aalready%2520conveyed%2520in%2520this%2520stage.%2520After%2520that%252C%2520the%2520diffusion%2520model%2520completes%2520the%250Adetails%2520of%2520generated%2520images%2520by%2520information%2520from%2520themselves.%2520Finally%252C%2520we%2520propose%250Ato%2520apply%2520this%2520observation%2520to%2520accelerate%2520the%2520process%2520of%2520T2I%2520generation%2520by%250Aproperly%2520removing%2520text%2520guidance%252C%2520which%2520finally%2520accelerates%2520the%2520sampling%2520up%2520to%250A25%255C%2525%252B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20the%20Working%20Mechanism%20of%20Text-to-Image%20Diffusion%0A%20%20Model&entry.906535625=Mingyang%20Yi%20and%20Aoxue%20Li%20and%20Yi%20Xin%20and%20Zhenguo%20Li&entry.1292438233=%20%20Recently%2C%20the%20strong%20latent%20Diffusion%20Probabilistic%20Model%20%28DPM%29%20has%20been%0Aapplied%20to%20high-quality%20Text-to-Image%20%28T2I%29%20generation%20%28e.g.%2C%20Stable%0ADiffusion%29%2C%20by%20injecting%20the%20encoded%20target%20text%20prompt%20into%20the%20gradually%0Adenoised%20diffusion%20image%20generator.%20Despite%20the%20success%20of%20DPM%20in%20practice%2C%20the%0Amechanism%20behind%20it%20remains%20to%20be%20explored.%20To%20fill%20this%20blank%2C%20we%20begin%20by%0Aexamining%20the%20intermediate%20statuses%20during%20the%20gradual%20denoising%20generation%0Aprocess%20in%20DPM.%20The%20empirical%20observations%20indicate%2C%20the%20shape%20of%20image%20is%0Areconstructed%20after%20the%20first%20few%20denoising%20steps%2C%20and%20then%20the%20image%20is%20filled%0Awith%20details%20%28e.g.%2C%20texture%29.%20The%20phenomenon%20is%20because%20the%20low-frequency%0Asignal%20%28shape%20relevant%29%20of%20the%20noisy%20image%20is%20not%20corrupted%20until%20the%20final%0Astage%20in%20the%20forward%20process%20%28initial%20stage%20of%20generation%29%20of%20adding%20noise%20in%0ADPM.%20Inspired%20by%20the%20observations%2C%20we%20proceed%20to%20explore%20the%20influence%20of%20each%0Atoken%20in%20the%20text%20prompt%20during%20the%20two%20stages.%20After%20a%20series%20of%20experiments%0Aof%20T2I%20generations%20conditioned%20on%20a%20set%20of%20text%20prompts.%20We%20conclude%20that%20in%0Athe%20earlier%20generation%20stage%2C%20the%20image%20is%20mostly%20decided%20by%20the%20special%20token%0A%5B%5Ctexttt%7BEOS%7D%5D%20in%20the%20text%20prompt%2C%20and%20the%20information%20in%20the%20text%20prompt%20is%0Aalready%20conveyed%20in%20this%20stage.%20After%20that%2C%20the%20diffusion%20model%20completes%20the%0Adetails%20of%20generated%20images%20by%20information%20from%20themselves.%20Finally%2C%20we%20propose%0Ato%20apply%20this%20observation%20to%20accelerate%20the%20process%20of%20T2I%20generation%20by%0Aproperly%20removing%20text%20guidance%2C%20which%20finally%20accelerates%20the%20sampling%20up%20to%0A25%5C%25%2B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15330v2&entry.124074799=Read"},
{"title": "GLBench: A Comprehensive Benchmark for Graph with Large Language Models", "author": "Yuhan Li and Peisong Wang and Xiao Zhu and Aochuan Chen and Haiyun Jiang and Deng Cai and Victor Wai Kin Chan and Jia Li", "abstract": "  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n", "link": "http://arxiv.org/abs/2407.07457v3", "date": "2024-10-22", "relevancy": 2.5665, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLBench%3A%20A%20Comprehensive%20Benchmark%20for%20Graph%20with%20Large%20Language%20Models&body=Title%3A%20GLBench%3A%20A%20Comprehensive%20Benchmark%20for%20Graph%20with%20Large%20Language%20Models%0AAuthor%3A%20Yuhan%20Li%20and%20Peisong%20Wang%20and%20Xiao%20Zhu%20and%20Aochuan%20Chen%20and%20Haiyun%20Jiang%20and%20Deng%20Cai%20and%20Victor%20Wai%20Kin%20Chan%20and%20Jia%20Li%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20revolutionized%20the%20way%20we%0Ainteract%20with%20graphs%2C%20leading%20to%20a%20new%20paradigm%20called%20GraphLLM.%20Despite%20the%0Arapid%20development%20of%20GraphLLM%20methods%20in%20recent%20years%2C%20the%20progress%20and%0Aunderstanding%20of%20this%20field%20remain%20unclear%20due%20to%20the%20lack%20of%20a%20benchmark%20with%0Aconsistent%20experimental%20protocols.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GLBench%2C%0Athe%20first%20comprehensive%20benchmark%20for%20evaluating%20GraphLLM%20methods%20in%20both%0Asupervised%20and%20zero-shot%20scenarios.%20GLBench%20provides%20a%20fair%20and%20thorough%0Aevaluation%20of%20different%20categories%20of%20GraphLLM%20methods%2C%20along%20with%20traditional%0Abaselines%20such%20as%20graph%20neural%20networks.%20Through%20extensive%20experiments%20on%20a%0Acollection%20of%20real-world%20datasets%20with%20consistent%20data%20processing%20and%20splitting%0Astrategies%2C%20we%20have%20uncovered%20several%20key%20findings.%20Firstly%2C%20GraphLLM%20methods%0Aoutperform%20traditional%20baselines%20in%20supervised%20settings%2C%20with%20LLM-as-enhancers%0Ashowing%20the%20most%20robust%20performance.%20However%2C%20using%20LLMs%20as%20predictors%20is%20less%0Aeffective%20and%20often%20leads%20to%20uncontrollable%20output%20issues.%20We%20also%20notice%20that%0Ano%20clear%20scaling%20laws%20exist%20for%20current%20GraphLLM%20methods.%20In%20addition%2C%20both%0Astructures%20and%20semantics%20are%20crucial%20for%20effective%20zero-shot%20transfer%2C%20and%20our%0Aproposed%20simple%20baseline%20can%20even%20outperform%20several%20models%20tailored%20for%0Azero-shot%20scenarios.%20The%20data%20and%20code%20of%20the%20benchmark%20can%20be%20found%20at%0Ahttps%3A//github.com/NineAbyss/GLBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07457v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Graph%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DYuhan%2520Li%2520and%2520Peisong%2520Wang%2520and%2520Xiao%2520Zhu%2520and%2520Aochuan%2520Chen%2520and%2520Haiyun%2520Jiang%2520and%2520Deng%2520Cai%2520and%2520Victor%2520Wai%2520Kin%2520Chan%2520and%2520Jia%2520Li%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520revolutionized%2520the%2520way%2520we%250Ainteract%2520with%2520graphs%252C%2520leading%2520to%2520a%2520new%2520paradigm%2520called%2520GraphLLM.%2520Despite%2520the%250Arapid%2520development%2520of%2520GraphLLM%2520methods%2520in%2520recent%2520years%252C%2520the%2520progress%2520and%250Aunderstanding%2520of%2520this%2520field%2520remain%2520unclear%2520due%2520to%2520the%2520lack%2520of%2520a%2520benchmark%2520with%250Aconsistent%2520experimental%2520protocols.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520GLBench%252C%250Athe%2520first%2520comprehensive%2520benchmark%2520for%2520evaluating%2520GraphLLM%2520methods%2520in%2520both%250Asupervised%2520and%2520zero-shot%2520scenarios.%2520GLBench%2520provides%2520a%2520fair%2520and%2520thorough%250Aevaluation%2520of%2520different%2520categories%2520of%2520GraphLLM%2520methods%252C%2520along%2520with%2520traditional%250Abaselines%2520such%2520as%2520graph%2520neural%2520networks.%2520Through%2520extensive%2520experiments%2520on%2520a%250Acollection%2520of%2520real-world%2520datasets%2520with%2520consistent%2520data%2520processing%2520and%2520splitting%250Astrategies%252C%2520we%2520have%2520uncovered%2520several%2520key%2520findings.%2520Firstly%252C%2520GraphLLM%2520methods%250Aoutperform%2520traditional%2520baselines%2520in%2520supervised%2520settings%252C%2520with%2520LLM-as-enhancers%250Ashowing%2520the%2520most%2520robust%2520performance.%2520However%252C%2520using%2520LLMs%2520as%2520predictors%2520is%2520less%250Aeffective%2520and%2520often%2520leads%2520to%2520uncontrollable%2520output%2520issues.%2520We%2520also%2520notice%2520that%250Ano%2520clear%2520scaling%2520laws%2520exist%2520for%2520current%2520GraphLLM%2520methods.%2520In%2520addition%252C%2520both%250Astructures%2520and%2520semantics%2520are%2520crucial%2520for%2520effective%2520zero-shot%2520transfer%252C%2520and%2520our%250Aproposed%2520simple%2520baseline%2520can%2520even%2520outperform%2520several%2520models%2520tailored%2520for%250Azero-shot%2520scenarios.%2520The%2520data%2520and%2520code%2520of%2520the%2520benchmark%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/NineAbyss/GLBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07457v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLBench%3A%20A%20Comprehensive%20Benchmark%20for%20Graph%20with%20Large%20Language%20Models&entry.906535625=Yuhan%20Li%20and%20Peisong%20Wang%20and%20Xiao%20Zhu%20and%20Aochuan%20Chen%20and%20Haiyun%20Jiang%20and%20Deng%20Cai%20and%20Victor%20Wai%20Kin%20Chan%20and%20Jia%20Li&entry.1292438233=%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20revolutionized%20the%20way%20we%0Ainteract%20with%20graphs%2C%20leading%20to%20a%20new%20paradigm%20called%20GraphLLM.%20Despite%20the%0Arapid%20development%20of%20GraphLLM%20methods%20in%20recent%20years%2C%20the%20progress%20and%0Aunderstanding%20of%20this%20field%20remain%20unclear%20due%20to%20the%20lack%20of%20a%20benchmark%20with%0Aconsistent%20experimental%20protocols.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GLBench%2C%0Athe%20first%20comprehensive%20benchmark%20for%20evaluating%20GraphLLM%20methods%20in%20both%0Asupervised%20and%20zero-shot%20scenarios.%20GLBench%20provides%20a%20fair%20and%20thorough%0Aevaluation%20of%20different%20categories%20of%20GraphLLM%20methods%2C%20along%20with%20traditional%0Abaselines%20such%20as%20graph%20neural%20networks.%20Through%20extensive%20experiments%20on%20a%0Acollection%20of%20real-world%20datasets%20with%20consistent%20data%20processing%20and%20splitting%0Astrategies%2C%20we%20have%20uncovered%20several%20key%20findings.%20Firstly%2C%20GraphLLM%20methods%0Aoutperform%20traditional%20baselines%20in%20supervised%20settings%2C%20with%20LLM-as-enhancers%0Ashowing%20the%20most%20robust%20performance.%20However%2C%20using%20LLMs%20as%20predictors%20is%20less%0Aeffective%20and%20often%20leads%20to%20uncontrollable%20output%20issues.%20We%20also%20notice%20that%0Ano%20clear%20scaling%20laws%20exist%20for%20current%20GraphLLM%20methods.%20In%20addition%2C%20both%0Astructures%20and%20semantics%20are%20crucial%20for%20effective%20zero-shot%20transfer%2C%20and%20our%0Aproposed%20simple%20baseline%20can%20even%20outperform%20several%20models%20tailored%20for%0Azero-shot%20scenarios.%20The%20data%20and%20code%20of%20the%20benchmark%20can%20be%20found%20at%0Ahttps%3A//github.com/NineAbyss/GLBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07457v3&entry.124074799=Read"},
{"title": "DNAHLM -- DNA sequence and Human Language mixed large language Model", "author": "Wang Liang", "abstract": "  There are already many DNA large language models, but most of them still\nfollow traditional uses, such as extracting sequence features for\nclassification tasks. More innovative applications of large language models,\nsuch as prompt engineering, RAG, and zero-shot or few-shot prediction, remain\nchallenging for DNA-based models. The key issue lies in the fact that DNA\nmodels and human natural language models are entirely separate; however,\ntechniques like prompt engineering require the use of natural language, thereby\nsignificantly limiting the application of DNA large language models. This paper\nintroduces a hybrid model trained on the GPT-2 network, combining DNA sequences\nand English text to explore the potential of using prompts and fine-tuning in\nDNA models. The model has demonstrated its effectiveness in DNA related\nzero-shot prediction and multitask application.\n", "link": "http://arxiv.org/abs/2410.16917v1", "date": "2024-10-22", "relevancy": 2.5574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNAHLM%20--%20DNA%20sequence%20and%20Human%20Language%20mixed%20large%20language%20Model&body=Title%3A%20DNAHLM%20--%20DNA%20sequence%20and%20Human%20Language%20mixed%20large%20language%20Model%0AAuthor%3A%20Wang%20Liang%0AAbstract%3A%20%20%20There%20are%20already%20many%20DNA%20large%20language%20models%2C%20but%20most%20of%20them%20still%0Afollow%20traditional%20uses%2C%20such%20as%20extracting%20sequence%20features%20for%0Aclassification%20tasks.%20More%20innovative%20applications%20of%20large%20language%20models%2C%0Asuch%20as%20prompt%20engineering%2C%20RAG%2C%20and%20zero-shot%20or%20few-shot%20prediction%2C%20remain%0Achallenging%20for%20DNA-based%20models.%20The%20key%20issue%20lies%20in%20the%20fact%20that%20DNA%0Amodels%20and%20human%20natural%20language%20models%20are%20entirely%20separate%3B%20however%2C%0Atechniques%20like%20prompt%20engineering%20require%20the%20use%20of%20natural%20language%2C%20thereby%0Asignificantly%20limiting%20the%20application%20of%20DNA%20large%20language%20models.%20This%20paper%0Aintroduces%20a%20hybrid%20model%20trained%20on%20the%20GPT-2%20network%2C%20combining%20DNA%20sequences%0Aand%20English%20text%20to%20explore%20the%20potential%20of%20using%20prompts%20and%20fine-tuning%20in%0ADNA%20models.%20The%20model%20has%20demonstrated%20its%20effectiveness%20in%20DNA%20related%0Azero-shot%20prediction%20and%20multitask%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNAHLM%2520--%2520DNA%2520sequence%2520and%2520Human%2520Language%2520mixed%2520large%2520language%2520Model%26entry.906535625%3DWang%2520Liang%26entry.1292438233%3D%2520%2520There%2520are%2520already%2520many%2520DNA%2520large%2520language%2520models%252C%2520but%2520most%2520of%2520them%2520still%250Afollow%2520traditional%2520uses%252C%2520such%2520as%2520extracting%2520sequence%2520features%2520for%250Aclassification%2520tasks.%2520More%2520innovative%2520applications%2520of%2520large%2520language%2520models%252C%250Asuch%2520as%2520prompt%2520engineering%252C%2520RAG%252C%2520and%2520zero-shot%2520or%2520few-shot%2520prediction%252C%2520remain%250Achallenging%2520for%2520DNA-based%2520models.%2520The%2520key%2520issue%2520lies%2520in%2520the%2520fact%2520that%2520DNA%250Amodels%2520and%2520human%2520natural%2520language%2520models%2520are%2520entirely%2520separate%253B%2520however%252C%250Atechniques%2520like%2520prompt%2520engineering%2520require%2520the%2520use%2520of%2520natural%2520language%252C%2520thereby%250Asignificantly%2520limiting%2520the%2520application%2520of%2520DNA%2520large%2520language%2520models.%2520This%2520paper%250Aintroduces%2520a%2520hybrid%2520model%2520trained%2520on%2520the%2520GPT-2%2520network%252C%2520combining%2520DNA%2520sequences%250Aand%2520English%2520text%2520to%2520explore%2520the%2520potential%2520of%2520using%2520prompts%2520and%2520fine-tuning%2520in%250ADNA%2520models.%2520The%2520model%2520has%2520demonstrated%2520its%2520effectiveness%2520in%2520DNA%2520related%250Azero-shot%2520prediction%2520and%2520multitask%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNAHLM%20--%20DNA%20sequence%20and%20Human%20Language%20mixed%20large%20language%20Model&entry.906535625=Wang%20Liang&entry.1292438233=%20%20There%20are%20already%20many%20DNA%20large%20language%20models%2C%20but%20most%20of%20them%20still%0Afollow%20traditional%20uses%2C%20such%20as%20extracting%20sequence%20features%20for%0Aclassification%20tasks.%20More%20innovative%20applications%20of%20large%20language%20models%2C%0Asuch%20as%20prompt%20engineering%2C%20RAG%2C%20and%20zero-shot%20or%20few-shot%20prediction%2C%20remain%0Achallenging%20for%20DNA-based%20models.%20The%20key%20issue%20lies%20in%20the%20fact%20that%20DNA%0Amodels%20and%20human%20natural%20language%20models%20are%20entirely%20separate%3B%20however%2C%0Atechniques%20like%20prompt%20engineering%20require%20the%20use%20of%20natural%20language%2C%20thereby%0Asignificantly%20limiting%20the%20application%20of%20DNA%20large%20language%20models.%20This%20paper%0Aintroduces%20a%20hybrid%20model%20trained%20on%20the%20GPT-2%20network%2C%20combining%20DNA%20sequences%0Aand%20English%20text%20to%20explore%20the%20potential%20of%20using%20prompts%20and%20fine-tuning%20in%0ADNA%20models.%20The%20model%20has%20demonstrated%20its%20effectiveness%20in%20DNA%20related%0Azero-shot%20prediction%20and%20multitask%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16917v1&entry.124074799=Read"},
{"title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling", "author": "Shengpeng Ji and Ziyue Jiang and Wen Wang and Yifu Chen and Minghui Fang and Jialong Zuo and Qian Yang and Xize Cheng and Zehan Wang and Ruiqi Li and Ziang Zhang and Xiaoda Yang and Rongjie Huang and Yidi Jiang and Qian Chen and Siqi Zheng and Wen Wang and Zhou Zhao", "abstract": "  Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.\n", "link": "http://arxiv.org/abs/2408.16532v2", "date": "2024-10-22", "relevancy": 2.5376, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5313}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling&body=Title%3A%20WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling%0AAuthor%3A%20Shengpeng%20Ji%20and%20Ziyue%20Jiang%20and%20Wen%20Wang%20and%20Yifu%20Chen%20and%20Minghui%20Fang%20and%20Jialong%20Zuo%20and%20Qian%20Yang%20and%20Xize%20Cheng%20and%20Zehan%20Wang%20and%20Ruiqi%20Li%20and%20Ziang%20Zhang%20and%20Xiaoda%20Yang%20and%20Rongjie%20Huang%20and%20Yidi%20Jiang%20and%20Qian%20Chen%20and%20Siqi%20Zheng%20and%20Wen%20Wang%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Language%20models%20have%20been%20effectively%20applied%20to%20modeling%20natural%20signals%2C%0Asuch%20as%20images%2C%20video%2C%20speech%2C%20and%20audio.%20A%20crucial%20component%20of%20these%20models%0Ais%20the%20codec%20tokenizer%2C%20which%20compresses%20high-dimensional%20natural%20signals%20into%0Alower-dimensional%20discrete%20tokens.%20In%20this%20paper%2C%20we%20introduce%20WavTokenizer%2C%0Awhich%20offers%20several%20advantages%20over%20previous%20SOTA%20acoustic%20codec%20models%20in%20the%0Aaudio%20domain%3A%201%29extreme%20compression.%20By%20compressing%20the%20layers%20of%20quantizers%0Aand%20the%20temporal%20dimension%20of%20the%20discrete%20codec%2C%20one-second%20audio%20of%2024kHz%0Asampling%20rate%20requires%20only%20a%20single%20quantizer%20with%2040%20or%2075%20tokens.%202%29improved%0Asubjective%20quality.%20Despite%20the%20reduced%20number%20of%20tokens%2C%20WavTokenizer%20achieves%0Astate-of-the-art%20reconstruction%20quality%20with%20outstanding%20UTMOS%20scores%20and%0Ainherently%20contains%20richer%20semantic%20information.%20Specifically%2C%20we%20achieve%20these%0Aresults%20by%20designing%20a%20broader%20VQ%20space%2C%20extended%20contextual%20windows%2C%20and%0Aimproved%20attention%20networks%2C%20as%20well%20as%20introducing%20a%20powerful%20multi-scale%0Adiscriminator%20and%20an%20inverse%20Fourier%20transform%20structure.%20We%20conducted%0Aextensive%20reconstruction%20experiments%20in%20the%20domains%20of%20speech%2C%20audio%2C%20and%0Amusic.%20WavTokenizer%20exhibited%20strong%20performance%20across%20various%20objective%20and%0Asubjective%20metrics%20compared%20to%20state-of-the-art%20models.%20We%20also%20tested%20semantic%0Ainformation%2C%20VQ%20utilization%2C%20and%20adaptability%20to%20generative%20models.%0AComprehensive%20ablation%20studies%20confirm%20the%20necessity%20of%20each%20module%20in%0AWavTokenizer.%20The%20related%20code%2C%20demos%2C%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/jishengpeng/WavTokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16532v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavTokenizer%253A%2520an%2520Efficient%2520Acoustic%2520Discrete%2520Codec%2520Tokenizer%2520for%2520Audio%250A%2520%2520Language%2520Modeling%26entry.906535625%3DShengpeng%2520Ji%2520and%2520Ziyue%2520Jiang%2520and%2520Wen%2520Wang%2520and%2520Yifu%2520Chen%2520and%2520Minghui%2520Fang%2520and%2520Jialong%2520Zuo%2520and%2520Qian%2520Yang%2520and%2520Xize%2520Cheng%2520and%2520Zehan%2520Wang%2520and%2520Ruiqi%2520Li%2520and%2520Ziang%2520Zhang%2520and%2520Xiaoda%2520Yang%2520and%2520Rongjie%2520Huang%2520and%2520Yidi%2520Jiang%2520and%2520Qian%2520Chen%2520and%2520Siqi%2520Zheng%2520and%2520Wen%2520Wang%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Language%2520models%2520have%2520been%2520effectively%2520applied%2520to%2520modeling%2520natural%2520signals%252C%250Asuch%2520as%2520images%252C%2520video%252C%2520speech%252C%2520and%2520audio.%2520A%2520crucial%2520component%2520of%2520these%2520models%250Ais%2520the%2520codec%2520tokenizer%252C%2520which%2520compresses%2520high-dimensional%2520natural%2520signals%2520into%250Alower-dimensional%2520discrete%2520tokens.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520WavTokenizer%252C%250Awhich%2520offers%2520several%2520advantages%2520over%2520previous%2520SOTA%2520acoustic%2520codec%2520models%2520in%2520the%250Aaudio%2520domain%253A%25201%2529extreme%2520compression.%2520By%2520compressing%2520the%2520layers%2520of%2520quantizers%250Aand%2520the%2520temporal%2520dimension%2520of%2520the%2520discrete%2520codec%252C%2520one-second%2520audio%2520of%252024kHz%250Asampling%2520rate%2520requires%2520only%2520a%2520single%2520quantizer%2520with%252040%2520or%252075%2520tokens.%25202%2529improved%250Asubjective%2520quality.%2520Despite%2520the%2520reduced%2520number%2520of%2520tokens%252C%2520WavTokenizer%2520achieves%250Astate-of-the-art%2520reconstruction%2520quality%2520with%2520outstanding%2520UTMOS%2520scores%2520and%250Ainherently%2520contains%2520richer%2520semantic%2520information.%2520Specifically%252C%2520we%2520achieve%2520these%250Aresults%2520by%2520designing%2520a%2520broader%2520VQ%2520space%252C%2520extended%2520contextual%2520windows%252C%2520and%250Aimproved%2520attention%2520networks%252C%2520as%2520well%2520as%2520introducing%2520a%2520powerful%2520multi-scale%250Adiscriminator%2520and%2520an%2520inverse%2520Fourier%2520transform%2520structure.%2520We%2520conducted%250Aextensive%2520reconstruction%2520experiments%2520in%2520the%2520domains%2520of%2520speech%252C%2520audio%252C%2520and%250Amusic.%2520WavTokenizer%2520exhibited%2520strong%2520performance%2520across%2520various%2520objective%2520and%250Asubjective%2520metrics%2520compared%2520to%2520state-of-the-art%2520models.%2520We%2520also%2520tested%2520semantic%250Ainformation%252C%2520VQ%2520utilization%252C%2520and%2520adaptability%2520to%2520generative%2520models.%250AComprehensive%2520ablation%2520studies%2520confirm%2520the%2520necessity%2520of%2520each%2520module%2520in%250AWavTokenizer.%2520The%2520related%2520code%252C%2520demos%252C%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/jishengpeng/WavTokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16532v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling&entry.906535625=Shengpeng%20Ji%20and%20Ziyue%20Jiang%20and%20Wen%20Wang%20and%20Yifu%20Chen%20and%20Minghui%20Fang%20and%20Jialong%20Zuo%20and%20Qian%20Yang%20and%20Xize%20Cheng%20and%20Zehan%20Wang%20and%20Ruiqi%20Li%20and%20Ziang%20Zhang%20and%20Xiaoda%20Yang%20and%20Rongjie%20Huang%20and%20Yidi%20Jiang%20and%20Qian%20Chen%20and%20Siqi%20Zheng%20and%20Wen%20Wang%20and%20Zhou%20Zhao&entry.1292438233=%20%20Language%20models%20have%20been%20effectively%20applied%20to%20modeling%20natural%20signals%2C%0Asuch%20as%20images%2C%20video%2C%20speech%2C%20and%20audio.%20A%20crucial%20component%20of%20these%20models%0Ais%20the%20codec%20tokenizer%2C%20which%20compresses%20high-dimensional%20natural%20signals%20into%0Alower-dimensional%20discrete%20tokens.%20In%20this%20paper%2C%20we%20introduce%20WavTokenizer%2C%0Awhich%20offers%20several%20advantages%20over%20previous%20SOTA%20acoustic%20codec%20models%20in%20the%0Aaudio%20domain%3A%201%29extreme%20compression.%20By%20compressing%20the%20layers%20of%20quantizers%0Aand%20the%20temporal%20dimension%20of%20the%20discrete%20codec%2C%20one-second%20audio%20of%2024kHz%0Asampling%20rate%20requires%20only%20a%20single%20quantizer%20with%2040%20or%2075%20tokens.%202%29improved%0Asubjective%20quality.%20Despite%20the%20reduced%20number%20of%20tokens%2C%20WavTokenizer%20achieves%0Astate-of-the-art%20reconstruction%20quality%20with%20outstanding%20UTMOS%20scores%20and%0Ainherently%20contains%20richer%20semantic%20information.%20Specifically%2C%20we%20achieve%20these%0Aresults%20by%20designing%20a%20broader%20VQ%20space%2C%20extended%20contextual%20windows%2C%20and%0Aimproved%20attention%20networks%2C%20as%20well%20as%20introducing%20a%20powerful%20multi-scale%0Adiscriminator%20and%20an%20inverse%20Fourier%20transform%20structure.%20We%20conducted%0Aextensive%20reconstruction%20experiments%20in%20the%20domains%20of%20speech%2C%20audio%2C%20and%0Amusic.%20WavTokenizer%20exhibited%20strong%20performance%20across%20various%20objective%20and%0Asubjective%20metrics%20compared%20to%20state-of-the-art%20models.%20We%20also%20tested%20semantic%0Ainformation%2C%20VQ%20utilization%2C%20and%20adaptability%20to%20generative%20models.%0AComprehensive%20ablation%20studies%20confirm%20the%20necessity%20of%20each%20module%20in%0AWavTokenizer.%20The%20related%20code%2C%20demos%2C%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/jishengpeng/WavTokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16532v2&entry.124074799=Read"},
{"title": "CLAP: Concave Linear APproximation for Quadratic Graph Matching", "author": "Yongqing Liang and Huijun Han and Xin Li", "abstract": "  Solving point-wise feature correspondence in visual data is a fundamental\nproblem in computer vision. A powerful model that addresses this challenge is\nto formulate it as graph matching, which entails solving a Quadratic Assignment\nProblem (QAP) with node-wise and edge-wise constraints. However, solving such a\nQAP can be both expensive and difficult due to numerous local extreme points.\nIn this work, we introduce a novel linear model and solver designed to\naccelerate the computation of graph matching. Specifically, we employ a\npositive semi-definite matrix approximation to establish the structural\nattribute constraint.We then transform the original QAP into a linear model\nthat is concave for maximization. This model can subsequently be solved using\nthe Sinkhorn optimal transport algorithm, known for its enhanced efficiency and\nnumerical stability compared to existing approaches. Experimental results on\nthe widely used benchmark PascalVOC showcase that our algorithm achieves\nstate-of-the-art performance with significantly improved efficiency. Source\ncode: https://github.com/xmlyqing00/clap\n", "link": "http://arxiv.org/abs/2410.17101v1", "date": "2024-10-22", "relevancy": 2.5369, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.521}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAP%3A%20Concave%20Linear%20APproximation%20for%20Quadratic%20Graph%20Matching&body=Title%3A%20CLAP%3A%20Concave%20Linear%20APproximation%20for%20Quadratic%20Graph%20Matching%0AAuthor%3A%20Yongqing%20Liang%20and%20Huijun%20Han%20and%20Xin%20Li%0AAbstract%3A%20%20%20Solving%20point-wise%20feature%20correspondence%20in%20visual%20data%20is%20a%20fundamental%0Aproblem%20in%20computer%20vision.%20A%20powerful%20model%20that%20addresses%20this%20challenge%20is%0Ato%20formulate%20it%20as%20graph%20matching%2C%20which%20entails%20solving%20a%20Quadratic%20Assignment%0AProblem%20%28QAP%29%20with%20node-wise%20and%20edge-wise%20constraints.%20However%2C%20solving%20such%20a%0AQAP%20can%20be%20both%20expensive%20and%20difficult%20due%20to%20numerous%20local%20extreme%20points.%0AIn%20this%20work%2C%20we%20introduce%20a%20novel%20linear%20model%20and%20solver%20designed%20to%0Aaccelerate%20the%20computation%20of%20graph%20matching.%20Specifically%2C%20we%20employ%20a%0Apositive%20semi-definite%20matrix%20approximation%20to%20establish%20the%20structural%0Aattribute%20constraint.We%20then%20transform%20the%20original%20QAP%20into%20a%20linear%20model%0Athat%20is%20concave%20for%20maximization.%20This%20model%20can%20subsequently%20be%20solved%20using%0Athe%20Sinkhorn%20optimal%20transport%20algorithm%2C%20known%20for%20its%20enhanced%20efficiency%20and%0Anumerical%20stability%20compared%20to%20existing%20approaches.%20Experimental%20results%20on%0Athe%20widely%20used%20benchmark%20PascalVOC%20showcase%20that%20our%20algorithm%20achieves%0Astate-of-the-art%20performance%20with%20significantly%20improved%20efficiency.%20Source%0Acode%3A%20https%3A//github.com/xmlyqing00/clap%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAP%253A%2520Concave%2520Linear%2520APproximation%2520for%2520Quadratic%2520Graph%2520Matching%26entry.906535625%3DYongqing%2520Liang%2520and%2520Huijun%2520Han%2520and%2520Xin%2520Li%26entry.1292438233%3D%2520%2520Solving%2520point-wise%2520feature%2520correspondence%2520in%2520visual%2520data%2520is%2520a%2520fundamental%250Aproblem%2520in%2520computer%2520vision.%2520A%2520powerful%2520model%2520that%2520addresses%2520this%2520challenge%2520is%250Ato%2520formulate%2520it%2520as%2520graph%2520matching%252C%2520which%2520entails%2520solving%2520a%2520Quadratic%2520Assignment%250AProblem%2520%2528QAP%2529%2520with%2520node-wise%2520and%2520edge-wise%2520constraints.%2520However%252C%2520solving%2520such%2520a%250AQAP%2520can%2520be%2520both%2520expensive%2520and%2520difficult%2520due%2520to%2520numerous%2520local%2520extreme%2520points.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520linear%2520model%2520and%2520solver%2520designed%2520to%250Aaccelerate%2520the%2520computation%2520of%2520graph%2520matching.%2520Specifically%252C%2520we%2520employ%2520a%250Apositive%2520semi-definite%2520matrix%2520approximation%2520to%2520establish%2520the%2520structural%250Aattribute%2520constraint.We%2520then%2520transform%2520the%2520original%2520QAP%2520into%2520a%2520linear%2520model%250Athat%2520is%2520concave%2520for%2520maximization.%2520This%2520model%2520can%2520subsequently%2520be%2520solved%2520using%250Athe%2520Sinkhorn%2520optimal%2520transport%2520algorithm%252C%2520known%2520for%2520its%2520enhanced%2520efficiency%2520and%250Anumerical%2520stability%2520compared%2520to%2520existing%2520approaches.%2520Experimental%2520results%2520on%250Athe%2520widely%2520used%2520benchmark%2520PascalVOC%2520showcase%2520that%2520our%2520algorithm%2520achieves%250Astate-of-the-art%2520performance%2520with%2520significantly%2520improved%2520efficiency.%2520Source%250Acode%253A%2520https%253A//github.com/xmlyqing00/clap%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAP%3A%20Concave%20Linear%20APproximation%20for%20Quadratic%20Graph%20Matching&entry.906535625=Yongqing%20Liang%20and%20Huijun%20Han%20and%20Xin%20Li&entry.1292438233=%20%20Solving%20point-wise%20feature%20correspondence%20in%20visual%20data%20is%20a%20fundamental%0Aproblem%20in%20computer%20vision.%20A%20powerful%20model%20that%20addresses%20this%20challenge%20is%0Ato%20formulate%20it%20as%20graph%20matching%2C%20which%20entails%20solving%20a%20Quadratic%20Assignment%0AProblem%20%28QAP%29%20with%20node-wise%20and%20edge-wise%20constraints.%20However%2C%20solving%20such%20a%0AQAP%20can%20be%20both%20expensive%20and%20difficult%20due%20to%20numerous%20local%20extreme%20points.%0AIn%20this%20work%2C%20we%20introduce%20a%20novel%20linear%20model%20and%20solver%20designed%20to%0Aaccelerate%20the%20computation%20of%20graph%20matching.%20Specifically%2C%20we%20employ%20a%0Apositive%20semi-definite%20matrix%20approximation%20to%20establish%20the%20structural%0Aattribute%20constraint.We%20then%20transform%20the%20original%20QAP%20into%20a%20linear%20model%0Athat%20is%20concave%20for%20maximization.%20This%20model%20can%20subsequently%20be%20solved%20using%0Athe%20Sinkhorn%20optimal%20transport%20algorithm%2C%20known%20for%20its%20enhanced%20efficiency%20and%0Anumerical%20stability%20compared%20to%20existing%20approaches.%20Experimental%20results%20on%0Athe%20widely%20used%20benchmark%20PascalVOC%20showcase%20that%20our%20algorithm%20achieves%0Astate-of-the-art%20performance%20with%20significantly%20improved%20efficiency.%20Source%0Acode%3A%20https%3A//github.com/xmlyqing00/clap%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17101v1&entry.124074799=Read"},
{"title": "IdenBAT: Disentangled Representation Learning for Identity-Preserved\n  Brain Age Transformation", "author": "Junyeong Maeng and Kwanseok Oh and Wonsik Jung and Heung-Il Suk", "abstract": "  Brain age transformation aims to convert reference brain images into\nsynthesized images that accurately reflect the age-specific features of a\ntarget age group. The primary objective of this task is to modify only the\nage-related attributes of the reference image while preserving all other\nage-irrelevant attributes. However, achieving this goal poses substantial\nchallenges due to the inherent entanglement of various image attributes within\nfeatures extracted from a backbone encoder, resulting in simultaneous\nalterations during the image generation. To address this challenge, we propose\na novel architecture that employs disentangled representation learning for\nidentity-preserved brain age transformation called IdenBAT. This approach\nfacilitates the decomposition of image features, ensuring the preservation of\nindividual traits while selectively transforming age-related characteristics to\nmatch those of the target age group. Through comprehensive experiments\nconducted on both 2D and full-size 3D brain datasets, our method adeptly\nconverts input images to target age while retaining individual characteristics\naccurately. Furthermore, our approach demonstrates superiority over existing\nstate-of-the-art regarding performance fidelity.\n", "link": "http://arxiv.org/abs/2410.16945v1", "date": "2024-10-22", "relevancy": 2.532, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5382}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4922}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IdenBAT%3A%20Disentangled%20Representation%20Learning%20for%20Identity-Preserved%0A%20%20Brain%20Age%20Transformation&body=Title%3A%20IdenBAT%3A%20Disentangled%20Representation%20Learning%20for%20Identity-Preserved%0A%20%20Brain%20Age%20Transformation%0AAuthor%3A%20Junyeong%20Maeng%20and%20Kwanseok%20Oh%20and%20Wonsik%20Jung%20and%20Heung-Il%20Suk%0AAbstract%3A%20%20%20Brain%20age%20transformation%20aims%20to%20convert%20reference%20brain%20images%20into%0Asynthesized%20images%20that%20accurately%20reflect%20the%20age-specific%20features%20of%20a%0Atarget%20age%20group.%20The%20primary%20objective%20of%20this%20task%20is%20to%20modify%20only%20the%0Aage-related%20attributes%20of%20the%20reference%20image%20while%20preserving%20all%20other%0Aage-irrelevant%20attributes.%20However%2C%20achieving%20this%20goal%20poses%20substantial%0Achallenges%20due%20to%20the%20inherent%20entanglement%20of%20various%20image%20attributes%20within%0Afeatures%20extracted%20from%20a%20backbone%20encoder%2C%20resulting%20in%20simultaneous%0Aalterations%20during%20the%20image%20generation.%20To%20address%20this%20challenge%2C%20we%20propose%0Aa%20novel%20architecture%20that%20employs%20disentangled%20representation%20learning%20for%0Aidentity-preserved%20brain%20age%20transformation%20called%20IdenBAT.%20This%20approach%0Afacilitates%20the%20decomposition%20of%20image%20features%2C%20ensuring%20the%20preservation%20of%0Aindividual%20traits%20while%20selectively%20transforming%20age-related%20characteristics%20to%0Amatch%20those%20of%20the%20target%20age%20group.%20Through%20comprehensive%20experiments%0Aconducted%20on%20both%202D%20and%20full-size%203D%20brain%20datasets%2C%20our%20method%20adeptly%0Aconverts%20input%20images%20to%20target%20age%20while%20retaining%20individual%20characteristics%0Aaccurately.%20Furthermore%2C%20our%20approach%20demonstrates%20superiority%20over%20existing%0Astate-of-the-art%20regarding%20performance%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdenBAT%253A%2520Disentangled%2520Representation%2520Learning%2520for%2520Identity-Preserved%250A%2520%2520Brain%2520Age%2520Transformation%26entry.906535625%3DJunyeong%2520Maeng%2520and%2520Kwanseok%2520Oh%2520and%2520Wonsik%2520Jung%2520and%2520Heung-Il%2520Suk%26entry.1292438233%3D%2520%2520Brain%2520age%2520transformation%2520aims%2520to%2520convert%2520reference%2520brain%2520images%2520into%250Asynthesized%2520images%2520that%2520accurately%2520reflect%2520the%2520age-specific%2520features%2520of%2520a%250Atarget%2520age%2520group.%2520The%2520primary%2520objective%2520of%2520this%2520task%2520is%2520to%2520modify%2520only%2520the%250Aage-related%2520attributes%2520of%2520the%2520reference%2520image%2520while%2520preserving%2520all%2520other%250Aage-irrelevant%2520attributes.%2520However%252C%2520achieving%2520this%2520goal%2520poses%2520substantial%250Achallenges%2520due%2520to%2520the%2520inherent%2520entanglement%2520of%2520various%2520image%2520attributes%2520within%250Afeatures%2520extracted%2520from%2520a%2520backbone%2520encoder%252C%2520resulting%2520in%2520simultaneous%250Aalterations%2520during%2520the%2520image%2520generation.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250Aa%2520novel%2520architecture%2520that%2520employs%2520disentangled%2520representation%2520learning%2520for%250Aidentity-preserved%2520brain%2520age%2520transformation%2520called%2520IdenBAT.%2520This%2520approach%250Afacilitates%2520the%2520decomposition%2520of%2520image%2520features%252C%2520ensuring%2520the%2520preservation%2520of%250Aindividual%2520traits%2520while%2520selectively%2520transforming%2520age-related%2520characteristics%2520to%250Amatch%2520those%2520of%2520the%2520target%2520age%2520group.%2520Through%2520comprehensive%2520experiments%250Aconducted%2520on%2520both%25202D%2520and%2520full-size%25203D%2520brain%2520datasets%252C%2520our%2520method%2520adeptly%250Aconverts%2520input%2520images%2520to%2520target%2520age%2520while%2520retaining%2520individual%2520characteristics%250Aaccurately.%2520Furthermore%252C%2520our%2520approach%2520demonstrates%2520superiority%2520over%2520existing%250Astate-of-the-art%2520regarding%2520performance%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IdenBAT%3A%20Disentangled%20Representation%20Learning%20for%20Identity-Preserved%0A%20%20Brain%20Age%20Transformation&entry.906535625=Junyeong%20Maeng%20and%20Kwanseok%20Oh%20and%20Wonsik%20Jung%20and%20Heung-Il%20Suk&entry.1292438233=%20%20Brain%20age%20transformation%20aims%20to%20convert%20reference%20brain%20images%20into%0Asynthesized%20images%20that%20accurately%20reflect%20the%20age-specific%20features%20of%20a%0Atarget%20age%20group.%20The%20primary%20objective%20of%20this%20task%20is%20to%20modify%20only%20the%0Aage-related%20attributes%20of%20the%20reference%20image%20while%20preserving%20all%20other%0Aage-irrelevant%20attributes.%20However%2C%20achieving%20this%20goal%20poses%20substantial%0Achallenges%20due%20to%20the%20inherent%20entanglement%20of%20various%20image%20attributes%20within%0Afeatures%20extracted%20from%20a%20backbone%20encoder%2C%20resulting%20in%20simultaneous%0Aalterations%20during%20the%20image%20generation.%20To%20address%20this%20challenge%2C%20we%20propose%0Aa%20novel%20architecture%20that%20employs%20disentangled%20representation%20learning%20for%0Aidentity-preserved%20brain%20age%20transformation%20called%20IdenBAT.%20This%20approach%0Afacilitates%20the%20decomposition%20of%20image%20features%2C%20ensuring%20the%20preservation%20of%0Aindividual%20traits%20while%20selectively%20transforming%20age-related%20characteristics%20to%0Amatch%20those%20of%20the%20target%20age%20group.%20Through%20comprehensive%20experiments%0Aconducted%20on%20both%202D%20and%20full-size%203D%20brain%20datasets%2C%20our%20method%20adeptly%0Aconverts%20input%20images%20to%20target%20age%20while%20retaining%20individual%20characteristics%0Aaccurately.%20Furthermore%2C%20our%20approach%20demonstrates%20superiority%20over%20existing%0Astate-of-the-art%20regarding%20performance%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16945v1&entry.124074799=Read"},
{"title": "Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence", "author": "\u0130lker I\u015f\u0131k and Ramazan Gokberk Cinbis and Ebru Aydin Gol", "abstract": "  We propose a novel approach for learning interchangeable tokens in language\nmodels to obtain an extendable vocabulary that can generalize to new tokens.\nOur method is designed to address alpha-equivalence, the principle that\nrenaming bound variables in a syntactic expression preserves semantics. This\nproperty arises in many formal languages such as temporal logics, in which all\nproposition symbols represent the same concept but are distinguishable from\neach other. To handle such tokens, we develop a dual-part embedding approach.\nThe first part is shared across all interchangeable tokens, thereby enforcing\nthat they represent the same core concept. The second part is randomly\ngenerated for each token, which enables distinguishability. We evaluate our\nmethod in a Transformer encoder-decoder model on two tasks: solving linear\ntemporal logic formulae and copying with extendable vocabulary. Our method\ndemonstrates promising generalization capabilities in addition to introducing a\nfavorable inductive bias for alpha-equivalence.\n", "link": "http://arxiv.org/abs/2410.17161v1", "date": "2024-10-22", "relevancy": 2.5157, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interchangeable%20Token%20Embeddings%20for%20Extendable%20Vocabulary%20and%0A%20%20Alpha-Equivalence&body=Title%3A%20Interchangeable%20Token%20Embeddings%20for%20Extendable%20Vocabulary%20and%0A%20%20Alpha-Equivalence%0AAuthor%3A%20%C4%B0lker%20I%C5%9F%C4%B1k%20and%20Ramazan%20Gokberk%20Cinbis%20and%20Ebru%20Aydin%20Gol%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20for%20learning%20interchangeable%20tokens%20in%20language%0Amodels%20to%20obtain%20an%20extendable%20vocabulary%20that%20can%20generalize%20to%20new%20tokens.%0AOur%20method%20is%20designed%20to%20address%20alpha-equivalence%2C%20the%20principle%20that%0Arenaming%20bound%20variables%20in%20a%20syntactic%20expression%20preserves%20semantics.%20This%0Aproperty%20arises%20in%20many%20formal%20languages%20such%20as%20temporal%20logics%2C%20in%20which%20all%0Aproposition%20symbols%20represent%20the%20same%20concept%20but%20are%20distinguishable%20from%0Aeach%20other.%20To%20handle%20such%20tokens%2C%20we%20develop%20a%20dual-part%20embedding%20approach.%0AThe%20first%20part%20is%20shared%20across%20all%20interchangeable%20tokens%2C%20thereby%20enforcing%0Athat%20they%20represent%20the%20same%20core%20concept.%20The%20second%20part%20is%20randomly%0Agenerated%20for%20each%20token%2C%20which%20enables%20distinguishability.%20We%20evaluate%20our%0Amethod%20in%20a%20Transformer%20encoder-decoder%20model%20on%20two%20tasks%3A%20solving%20linear%0Atemporal%20logic%20formulae%20and%20copying%20with%20extendable%20vocabulary.%20Our%20method%0Ademonstrates%20promising%20generalization%20capabilities%20in%20addition%20to%20introducing%20a%0Afavorable%20inductive%20bias%20for%20alpha-equivalence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterchangeable%2520Token%2520Embeddings%2520for%2520Extendable%2520Vocabulary%2520and%250A%2520%2520Alpha-Equivalence%26entry.906535625%3D%25C4%25B0lker%2520I%25C5%259F%25C4%25B1k%2520and%2520Ramazan%2520Gokberk%2520Cinbis%2520and%2520Ebru%2520Aydin%2520Gol%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520for%2520learning%2520interchangeable%2520tokens%2520in%2520language%250Amodels%2520to%2520obtain%2520an%2520extendable%2520vocabulary%2520that%2520can%2520generalize%2520to%2520new%2520tokens.%250AOur%2520method%2520is%2520designed%2520to%2520address%2520alpha-equivalence%252C%2520the%2520principle%2520that%250Arenaming%2520bound%2520variables%2520in%2520a%2520syntactic%2520expression%2520preserves%2520semantics.%2520This%250Aproperty%2520arises%2520in%2520many%2520formal%2520languages%2520such%2520as%2520temporal%2520logics%252C%2520in%2520which%2520all%250Aproposition%2520symbols%2520represent%2520the%2520same%2520concept%2520but%2520are%2520distinguishable%2520from%250Aeach%2520other.%2520To%2520handle%2520such%2520tokens%252C%2520we%2520develop%2520a%2520dual-part%2520embedding%2520approach.%250AThe%2520first%2520part%2520is%2520shared%2520across%2520all%2520interchangeable%2520tokens%252C%2520thereby%2520enforcing%250Athat%2520they%2520represent%2520the%2520same%2520core%2520concept.%2520The%2520second%2520part%2520is%2520randomly%250Agenerated%2520for%2520each%2520token%252C%2520which%2520enables%2520distinguishability.%2520We%2520evaluate%2520our%250Amethod%2520in%2520a%2520Transformer%2520encoder-decoder%2520model%2520on%2520two%2520tasks%253A%2520solving%2520linear%250Atemporal%2520logic%2520formulae%2520and%2520copying%2520with%2520extendable%2520vocabulary.%2520Our%2520method%250Ademonstrates%2520promising%2520generalization%2520capabilities%2520in%2520addition%2520to%2520introducing%2520a%250Afavorable%2520inductive%2520bias%2520for%2520alpha-equivalence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interchangeable%20Token%20Embeddings%20for%20Extendable%20Vocabulary%20and%0A%20%20Alpha-Equivalence&entry.906535625=%C4%B0lker%20I%C5%9F%C4%B1k%20and%20Ramazan%20Gokberk%20Cinbis%20and%20Ebru%20Aydin%20Gol&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20for%20learning%20interchangeable%20tokens%20in%20language%0Amodels%20to%20obtain%20an%20extendable%20vocabulary%20that%20can%20generalize%20to%20new%20tokens.%0AOur%20method%20is%20designed%20to%20address%20alpha-equivalence%2C%20the%20principle%20that%0Arenaming%20bound%20variables%20in%20a%20syntactic%20expression%20preserves%20semantics.%20This%0Aproperty%20arises%20in%20many%20formal%20languages%20such%20as%20temporal%20logics%2C%20in%20which%20all%0Aproposition%20symbols%20represent%20the%20same%20concept%20but%20are%20distinguishable%20from%0Aeach%20other.%20To%20handle%20such%20tokens%2C%20we%20develop%20a%20dual-part%20embedding%20approach.%0AThe%20first%20part%20is%20shared%20across%20all%20interchangeable%20tokens%2C%20thereby%20enforcing%0Athat%20they%20represent%20the%20same%20core%20concept.%20The%20second%20part%20is%20randomly%0Agenerated%20for%20each%20token%2C%20which%20enables%20distinguishability.%20We%20evaluate%20our%0Amethod%20in%20a%20Transformer%20encoder-decoder%20model%20on%20two%20tasks%3A%20solving%20linear%0Atemporal%20logic%20formulae%20and%20copying%20with%20extendable%20vocabulary.%20Our%20method%0Ademonstrates%20promising%20generalization%20capabilities%20in%20addition%20to%20introducing%20a%0Afavorable%20inductive%20bias%20for%20alpha-equivalence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17161v1&entry.124074799=Read"},
{"title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias", "author": "Haian Jin and Hanwen Jiang and Hao Tan and Kai Zhang and Sai Bi and Tianyuan Zhang and Fujun Luan and Noah Snavely and Zexiang Xu", "abstract": "  We propose the Large View Synthesis Model (LVSM), a novel transformer-based\napproach for scalable and generalizable novel view synthesis from sparse-view\ninputs. We introduce two architectures: (1) an encoder-decoder LVSM, which\nencodes input image tokens into a fixed number of 1D latent tokens, functioning\nas a fully learned scene representation, and decodes novel-view images from\nthem; and (2) a decoder-only LVSM, which directly maps input images to\nnovel-view outputs, completely eliminating intermediate scene representations.\nBoth models bypass the 3D inductive biases used in previous methods -- from 3D\nrepresentations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar\nprojections, plane sweeps) -- addressing novel view synthesis with a fully\ndata-driven approach. While the encoder-decoder model offers faster inference\ndue to its independent latent representation, the decoder-only LVSM achieves\nsuperior quality, scalability, and zero-shot generalization, outperforming\nprevious state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive\nevaluations across multiple datasets demonstrate that both LVSM variants\nachieve state-of-the-art novel view synthesis quality. Notably, our models\nsurpass all previous methods even with reduced computational resources (1-2\nGPUs). Please see our website for more details:\nhttps://haian-jin.github.io/projects/LVSM/ .\n", "link": "http://arxiv.org/abs/2410.17242v1", "date": "2024-10-22", "relevancy": 2.5092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6325}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVSM%3A%20A%20Large%20View%20Synthesis%20Model%20with%20Minimal%203D%20Inductive%20Bias&body=Title%3A%20LVSM%3A%20A%20Large%20View%20Synthesis%20Model%20with%20Minimal%203D%20Inductive%20Bias%0AAuthor%3A%20Haian%20Jin%20and%20Hanwen%20Jiang%20and%20Hao%20Tan%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Tianyuan%20Zhang%20and%20Fujun%20Luan%20and%20Noah%20Snavely%20and%20Zexiang%20Xu%0AAbstract%3A%20%20%20We%20propose%20the%20Large%20View%20Synthesis%20Model%20%28LVSM%29%2C%20a%20novel%20transformer-based%0Aapproach%20for%20scalable%20and%20generalizable%20novel%20view%20synthesis%20from%20sparse-view%0Ainputs.%20We%20introduce%20two%20architectures%3A%20%281%29%20an%20encoder-decoder%20LVSM%2C%20which%0Aencodes%20input%20image%20tokens%20into%20a%20fixed%20number%20of%201D%20latent%20tokens%2C%20functioning%0Aas%20a%20fully%20learned%20scene%20representation%2C%20and%20decodes%20novel-view%20images%20from%0Athem%3B%20and%20%282%29%20a%20decoder-only%20LVSM%2C%20which%20directly%20maps%20input%20images%20to%0Anovel-view%20outputs%2C%20completely%20eliminating%20intermediate%20scene%20representations.%0ABoth%20models%20bypass%20the%203D%20inductive%20biases%20used%20in%20previous%20methods%20--%20from%203D%0Arepresentations%20%28e.g.%2C%20NeRF%2C%203DGS%29%20to%20network%20designs%20%28e.g.%2C%20epipolar%0Aprojections%2C%20plane%20sweeps%29%20--%20addressing%20novel%20view%20synthesis%20with%20a%20fully%0Adata-driven%20approach.%20While%20the%20encoder-decoder%20model%20offers%20faster%20inference%0Adue%20to%20its%20independent%20latent%20representation%2C%20the%20decoder-only%20LVSM%20achieves%0Asuperior%20quality%2C%20scalability%2C%20and%20zero-shot%20generalization%2C%20outperforming%0Aprevious%20state-of-the-art%20methods%20by%201.5%20to%203.5%20dB%20PSNR.%20Comprehensive%0Aevaluations%20across%20multiple%20datasets%20demonstrate%20that%20both%20LVSM%20variants%0Aachieve%20state-of-the-art%20novel%20view%20synthesis%20quality.%20Notably%2C%20our%20models%0Asurpass%20all%20previous%20methods%20even%20with%20reduced%20computational%20resources%20%281-2%0AGPUs%29.%20Please%20see%20our%20website%20for%20more%20details%3A%0Ahttps%3A//haian-jin.github.io/projects/LVSM/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVSM%253A%2520A%2520Large%2520View%2520Synthesis%2520Model%2520with%2520Minimal%25203D%2520Inductive%2520Bias%26entry.906535625%3DHaian%2520Jin%2520and%2520Hanwen%2520Jiang%2520and%2520Hao%2520Tan%2520and%2520Kai%2520Zhang%2520and%2520Sai%2520Bi%2520and%2520Tianyuan%2520Zhang%2520and%2520Fujun%2520Luan%2520and%2520Noah%2520Snavely%2520and%2520Zexiang%2520Xu%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520Large%2520View%2520Synthesis%2520Model%2520%2528LVSM%2529%252C%2520a%2520novel%2520transformer-based%250Aapproach%2520for%2520scalable%2520and%2520generalizable%2520novel%2520view%2520synthesis%2520from%2520sparse-view%250Ainputs.%2520We%2520introduce%2520two%2520architectures%253A%2520%25281%2529%2520an%2520encoder-decoder%2520LVSM%252C%2520which%250Aencodes%2520input%2520image%2520tokens%2520into%2520a%2520fixed%2520number%2520of%25201D%2520latent%2520tokens%252C%2520functioning%250Aas%2520a%2520fully%2520learned%2520scene%2520representation%252C%2520and%2520decodes%2520novel-view%2520images%2520from%250Athem%253B%2520and%2520%25282%2529%2520a%2520decoder-only%2520LVSM%252C%2520which%2520directly%2520maps%2520input%2520images%2520to%250Anovel-view%2520outputs%252C%2520completely%2520eliminating%2520intermediate%2520scene%2520representations.%250ABoth%2520models%2520bypass%2520the%25203D%2520inductive%2520biases%2520used%2520in%2520previous%2520methods%2520--%2520from%25203D%250Arepresentations%2520%2528e.g.%252C%2520NeRF%252C%25203DGS%2529%2520to%2520network%2520designs%2520%2528e.g.%252C%2520epipolar%250Aprojections%252C%2520plane%2520sweeps%2529%2520--%2520addressing%2520novel%2520view%2520synthesis%2520with%2520a%2520fully%250Adata-driven%2520approach.%2520While%2520the%2520encoder-decoder%2520model%2520offers%2520faster%2520inference%250Adue%2520to%2520its%2520independent%2520latent%2520representation%252C%2520the%2520decoder-only%2520LVSM%2520achieves%250Asuperior%2520quality%252C%2520scalability%252C%2520and%2520zero-shot%2520generalization%252C%2520outperforming%250Aprevious%2520state-of-the-art%2520methods%2520by%25201.5%2520to%25203.5%2520dB%2520PSNR.%2520Comprehensive%250Aevaluations%2520across%2520multiple%2520datasets%2520demonstrate%2520that%2520both%2520LVSM%2520variants%250Aachieve%2520state-of-the-art%2520novel%2520view%2520synthesis%2520quality.%2520Notably%252C%2520our%2520models%250Asurpass%2520all%2520previous%2520methods%2520even%2520with%2520reduced%2520computational%2520resources%2520%25281-2%250AGPUs%2529.%2520Please%2520see%2520our%2520website%2520for%2520more%2520details%253A%250Ahttps%253A//haian-jin.github.io/projects/LVSM/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVSM%3A%20A%20Large%20View%20Synthesis%20Model%20with%20Minimal%203D%20Inductive%20Bias&entry.906535625=Haian%20Jin%20and%20Hanwen%20Jiang%20and%20Hao%20Tan%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Tianyuan%20Zhang%20and%20Fujun%20Luan%20and%20Noah%20Snavely%20and%20Zexiang%20Xu&entry.1292438233=%20%20We%20propose%20the%20Large%20View%20Synthesis%20Model%20%28LVSM%29%2C%20a%20novel%20transformer-based%0Aapproach%20for%20scalable%20and%20generalizable%20novel%20view%20synthesis%20from%20sparse-view%0Ainputs.%20We%20introduce%20two%20architectures%3A%20%281%29%20an%20encoder-decoder%20LVSM%2C%20which%0Aencodes%20input%20image%20tokens%20into%20a%20fixed%20number%20of%201D%20latent%20tokens%2C%20functioning%0Aas%20a%20fully%20learned%20scene%20representation%2C%20and%20decodes%20novel-view%20images%20from%0Athem%3B%20and%20%282%29%20a%20decoder-only%20LVSM%2C%20which%20directly%20maps%20input%20images%20to%0Anovel-view%20outputs%2C%20completely%20eliminating%20intermediate%20scene%20representations.%0ABoth%20models%20bypass%20the%203D%20inductive%20biases%20used%20in%20previous%20methods%20--%20from%203D%0Arepresentations%20%28e.g.%2C%20NeRF%2C%203DGS%29%20to%20network%20designs%20%28e.g.%2C%20epipolar%0Aprojections%2C%20plane%20sweeps%29%20--%20addressing%20novel%20view%20synthesis%20with%20a%20fully%0Adata-driven%20approach.%20While%20the%20encoder-decoder%20model%20offers%20faster%20inference%0Adue%20to%20its%20independent%20latent%20representation%2C%20the%20decoder-only%20LVSM%20achieves%0Asuperior%20quality%2C%20scalability%2C%20and%20zero-shot%20generalization%2C%20outperforming%0Aprevious%20state-of-the-art%20methods%20by%201.5%20to%203.5%20dB%20PSNR.%20Comprehensive%0Aevaluations%20across%20multiple%20datasets%20demonstrate%20that%20both%20LVSM%20variants%0Aachieve%20state-of-the-art%20novel%20view%20synthesis%20quality.%20Notably%2C%20our%20models%0Asurpass%20all%20previous%20methods%20even%20with%20reduced%20computational%20resources%20%281-2%0AGPUs%29.%20Please%20see%20our%20website%20for%20more%20details%3A%0Ahttps%3A//haian-jin.github.io/projects/LVSM/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17242v1&entry.124074799=Read"},
{"title": "IPDreamer: Appearance-Controllable 3D Object Generation with Complex\n  Image Prompts", "author": "Bohan Zeng and Shanglin Li and Yutang Feng and Ling Yang and Hong Li and Sicheng Gao and Jiaming Liu and Conghui He and Wentao Zhang and Jianzhuang Liu and Baochang Zhang and Shuicheng Yan", "abstract": "  Recent advances in 3D generation have been remarkable, with methods such as\nDreamFusion leveraging large-scale text-to-image diffusion-based models to\nguide 3D object generation. These methods enable the synthesis of detailed and\nphotorealistic textured objects. However, the appearance of 3D objects produced\nby such text-to-3D models is often unpredictable, and it is hard for\nsingle-image-to-3D methods to deal with images lacking a clear subject,\ncomplicating the generation of appearance-controllable 3D objects from complex\nimages. To address these challenges, we present IPDreamer, a novel method that\ncaptures intricate appearance features from complex $\\textbf{I}$mage\n$\\textbf{P}$rompts and aligns the synthesized 3D object with these extracted\nfeatures, enabling high-fidelity, appearance-controllable 3D object generation.\nOur experiments demonstrate that IPDreamer consistently generates high-quality\n3D objects that align with both the textual and complex image prompts,\nhighlighting its promising capability in appearance-controlled, complex 3D\nobject generation. Our code is available at\nhttps://github.com/zengbohan0217/IPDreamer.\n", "link": "http://arxiv.org/abs/2310.05375v6", "date": "2024-10-22", "relevancy": 2.5038, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6346}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6316}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPDreamer%3A%20Appearance-Controllable%203D%20Object%20Generation%20with%20Complex%0A%20%20Image%20Prompts&body=Title%3A%20IPDreamer%3A%20Appearance-Controllable%203D%20Object%20Generation%20with%20Complex%0A%20%20Image%20Prompts%0AAuthor%3A%20Bohan%20Zeng%20and%20Shanglin%20Li%20and%20Yutang%20Feng%20and%20Ling%20Yang%20and%20Hong%20Li%20and%20Sicheng%20Gao%20and%20Jiaming%20Liu%20and%20Conghui%20He%20and%20Wentao%20Zhang%20and%20Jianzhuang%20Liu%20and%20Baochang%20Zhang%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20generation%20have%20been%20remarkable%2C%20with%20methods%20such%20as%0ADreamFusion%20leveraging%20large-scale%20text-to-image%20diffusion-based%20models%20to%0Aguide%203D%20object%20generation.%20These%20methods%20enable%20the%20synthesis%20of%20detailed%20and%0Aphotorealistic%20textured%20objects.%20However%2C%20the%20appearance%20of%203D%20objects%20produced%0Aby%20such%20text-to-3D%20models%20is%20often%20unpredictable%2C%20and%20it%20is%20hard%20for%0Asingle-image-to-3D%20methods%20to%20deal%20with%20images%20lacking%20a%20clear%20subject%2C%0Acomplicating%20the%20generation%20of%20appearance-controllable%203D%20objects%20from%20complex%0Aimages.%20To%20address%20these%20challenges%2C%20we%20present%20IPDreamer%2C%20a%20novel%20method%20that%0Acaptures%20intricate%20appearance%20features%20from%20complex%20%24%5Ctextbf%7BI%7D%24mage%0A%24%5Ctextbf%7BP%7D%24rompts%20and%20aligns%20the%20synthesized%203D%20object%20with%20these%20extracted%0Afeatures%2C%20enabling%20high-fidelity%2C%20appearance-controllable%203D%20object%20generation.%0AOur%20experiments%20demonstrate%20that%20IPDreamer%20consistently%20generates%20high-quality%0A3D%20objects%20that%20align%20with%20both%20the%20textual%20and%20complex%20image%20prompts%2C%0Ahighlighting%20its%20promising%20capability%20in%20appearance-controlled%2C%20complex%203D%0Aobject%20generation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zengbohan0217/IPDreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05375v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPDreamer%253A%2520Appearance-Controllable%25203D%2520Object%2520Generation%2520with%2520Complex%250A%2520%2520Image%2520Prompts%26entry.906535625%3DBohan%2520Zeng%2520and%2520Shanglin%2520Li%2520and%2520Yutang%2520Feng%2520and%2520Ling%2520Yang%2520and%2520Hong%2520Li%2520and%2520Sicheng%2520Gao%2520and%2520Jiaming%2520Liu%2520and%2520Conghui%2520He%2520and%2520Wentao%2520Zhang%2520and%2520Jianzhuang%2520Liu%2520and%2520Baochang%2520Zhang%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520generation%2520have%2520been%2520remarkable%252C%2520with%2520methods%2520such%2520as%250ADreamFusion%2520leveraging%2520large-scale%2520text-to-image%2520diffusion-based%2520models%2520to%250Aguide%25203D%2520object%2520generation.%2520These%2520methods%2520enable%2520the%2520synthesis%2520of%2520detailed%2520and%250Aphotorealistic%2520textured%2520objects.%2520However%252C%2520the%2520appearance%2520of%25203D%2520objects%2520produced%250Aby%2520such%2520text-to-3D%2520models%2520is%2520often%2520unpredictable%252C%2520and%2520it%2520is%2520hard%2520for%250Asingle-image-to-3D%2520methods%2520to%2520deal%2520with%2520images%2520lacking%2520a%2520clear%2520subject%252C%250Acomplicating%2520the%2520generation%2520of%2520appearance-controllable%25203D%2520objects%2520from%2520complex%250Aimages.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520IPDreamer%252C%2520a%2520novel%2520method%2520that%250Acaptures%2520intricate%2520appearance%2520features%2520from%2520complex%2520%2524%255Ctextbf%257BI%257D%2524mage%250A%2524%255Ctextbf%257BP%257D%2524rompts%2520and%2520aligns%2520the%2520synthesized%25203D%2520object%2520with%2520these%2520extracted%250Afeatures%252C%2520enabling%2520high-fidelity%252C%2520appearance-controllable%25203D%2520object%2520generation.%250AOur%2520experiments%2520demonstrate%2520that%2520IPDreamer%2520consistently%2520generates%2520high-quality%250A3D%2520objects%2520that%2520align%2520with%2520both%2520the%2520textual%2520and%2520complex%2520image%2520prompts%252C%250Ahighlighting%2520its%2520promising%2520capability%2520in%2520appearance-controlled%252C%2520complex%25203D%250Aobject%2520generation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zengbohan0217/IPDreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05375v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPDreamer%3A%20Appearance-Controllable%203D%20Object%20Generation%20with%20Complex%0A%20%20Image%20Prompts&entry.906535625=Bohan%20Zeng%20and%20Shanglin%20Li%20and%20Yutang%20Feng%20and%20Ling%20Yang%20and%20Hong%20Li%20and%20Sicheng%20Gao%20and%20Jiaming%20Liu%20and%20Conghui%20He%20and%20Wentao%20Zhang%20and%20Jianzhuang%20Liu%20and%20Baochang%20Zhang%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Recent%20advances%20in%203D%20generation%20have%20been%20remarkable%2C%20with%20methods%20such%20as%0ADreamFusion%20leveraging%20large-scale%20text-to-image%20diffusion-based%20models%20to%0Aguide%203D%20object%20generation.%20These%20methods%20enable%20the%20synthesis%20of%20detailed%20and%0Aphotorealistic%20textured%20objects.%20However%2C%20the%20appearance%20of%203D%20objects%20produced%0Aby%20such%20text-to-3D%20models%20is%20often%20unpredictable%2C%20and%20it%20is%20hard%20for%0Asingle-image-to-3D%20methods%20to%20deal%20with%20images%20lacking%20a%20clear%20subject%2C%0Acomplicating%20the%20generation%20of%20appearance-controllable%203D%20objects%20from%20complex%0Aimages.%20To%20address%20these%20challenges%2C%20we%20present%20IPDreamer%2C%20a%20novel%20method%20that%0Acaptures%20intricate%20appearance%20features%20from%20complex%20%24%5Ctextbf%7BI%7D%24mage%0A%24%5Ctextbf%7BP%7D%24rompts%20and%20aligns%20the%20synthesized%203D%20object%20with%20these%20extracted%0Afeatures%2C%20enabling%20high-fidelity%2C%20appearance-controllable%203D%20object%20generation.%0AOur%20experiments%20demonstrate%20that%20IPDreamer%20consistently%20generates%20high-quality%0A3D%20objects%20that%20align%20with%20both%20the%20textual%20and%20complex%20image%20prompts%2C%0Ahighlighting%20its%20promising%20capability%20in%20appearance-controlled%2C%20complex%203D%0Aobject%20generation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zengbohan0217/IPDreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05375v6&entry.124074799=Read"},
{"title": "AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and\n  Results", "author": "Maksim Smirnov and Aleksandr Gushchin and Anastasia Antsiferova and Dmitry Vatolin and Radu Timofte and Ziheng Jia and Zicheng Zhang and Wei Sun and Jiaying Qian and Yuqin Cao and Yinan Sun and Yuxin Zhu and Xiongkuo Min and Guangtao Zhai and Kanjar De and Qing Luo and Ao-Xiang Zhang and Peng Zhang and Haibo Lei and Linyan Jiang and Yaqing Li and Wenhui Meng and Zhenzhong Chen and Zhengxue Cheng and Jiahao Xiao and Jun Xu and Chenlong He and Qi Zheng and Ruoxi Zhu and Min Li and Yibo Fan and Zhengzhong Tu", "abstract": "  Video quality assessment (VQA) is a crucial task in the development of video\ncompression standards, as it directly impacts the viewer experience. This paper\npresents the results of the Compressed Video Quality Assessment challenge, held\nin conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV\n2024. The challenge aimed to evaluate the performance of VQA methods on a\ndiverse dataset of 459 videos, encoded with 14 codecs of various compression\nstandards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a\ncomprehensive collection of compression artifacts. To measure the methods\nperformance, we employed traditional correlation coefficients between their\npredictions and subjective scores, which were collected via large-scale\ncrowdsourced pairwise human comparisons. For training purposes, participants\nwere provided with the Compressed Video Quality Assessment Dataset (CVQAD), a\npreviously developed dataset of 1022 videos. Up to 30 participating teams\nregistered for the challenge, while we report the results of 6 teams, which\nsubmitted valid final solutions and code for reproducing the results. Moreover,\nwe calculated and present the performance of state-of-the-art VQA methods on\nthe developed dataset, providing a comprehensive benchmark for future research.\nThe dataset, results, and online leaderboard are publicly available at\nhttps://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.\n", "link": "http://arxiv.org/abs/2408.11982v3", "date": "2024-10-22", "relevancy": 2.5003, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5037}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIM%202024%20Challenge%20on%20Compressed%20Video%20Quality%20Assessment%3A%20Methods%20and%0A%20%20Results&body=Title%3A%20AIM%202024%20Challenge%20on%20Compressed%20Video%20Quality%20Assessment%3A%20Methods%20and%0A%20%20Results%0AAuthor%3A%20Maksim%20Smirnov%20and%20Aleksandr%20Gushchin%20and%20Anastasia%20Antsiferova%20and%20Dmitry%20Vatolin%20and%20Radu%20Timofte%20and%20Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Wei%20Sun%20and%20Jiaying%20Qian%20and%20Yuqin%20Cao%20and%20Yinan%20Sun%20and%20Yuxin%20Zhu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Kanjar%20De%20and%20Qing%20Luo%20and%20Ao-Xiang%20Zhang%20and%20Peng%20Zhang%20and%20Haibo%20Lei%20and%20Linyan%20Jiang%20and%20Yaqing%20Li%20and%20Wenhui%20Meng%20and%20Zhenzhong%20Chen%20and%20Zhengxue%20Cheng%20and%20Jiahao%20Xiao%20and%20Jun%20Xu%20and%20Chenlong%20He%20and%20Qi%20Zheng%20and%20Ruoxi%20Zhu%20and%20Min%20Li%20and%20Yibo%20Fan%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20Video%20quality%20assessment%20%28VQA%29%20is%20a%20crucial%20task%20in%20the%20development%20of%20video%0Acompression%20standards%2C%20as%20it%20directly%20impacts%20the%20viewer%20experience.%20This%20paper%0Apresents%20the%20results%20of%20the%20Compressed%20Video%20Quality%20Assessment%20challenge%2C%20held%0Ain%20conjunction%20with%20the%20Advances%20in%20Image%20Manipulation%20%28AIM%29%20workshop%20at%20ECCV%0A2024.%20The%20challenge%20aimed%20to%20evaluate%20the%20performance%20of%20VQA%20methods%20on%20a%0Adiverse%20dataset%20of%20459%20videos%2C%20encoded%20with%2014%20codecs%20of%20various%20compression%0Astandards%20%28AVC/H.264%2C%20HEVC/H.265%2C%20AV1%2C%20and%20VVC/H.266%29%20and%20containing%20a%0Acomprehensive%20collection%20of%20compression%20artifacts.%20To%20measure%20the%20methods%0Aperformance%2C%20we%20employed%20traditional%20correlation%20coefficients%20between%20their%0Apredictions%20and%20subjective%20scores%2C%20which%20were%20collected%20via%20large-scale%0Acrowdsourced%20pairwise%20human%20comparisons.%20For%20training%20purposes%2C%20participants%0Awere%20provided%20with%20the%20Compressed%20Video%20Quality%20Assessment%20Dataset%20%28CVQAD%29%2C%20a%0Apreviously%20developed%20dataset%20of%201022%20videos.%20Up%20to%2030%20participating%20teams%0Aregistered%20for%20the%20challenge%2C%20while%20we%20report%20the%20results%20of%206%20teams%2C%20which%0Asubmitted%20valid%20final%20solutions%20and%20code%20for%20reproducing%20the%20results.%20Moreover%2C%0Awe%20calculated%20and%20present%20the%20performance%20of%20state-of-the-art%20VQA%20methods%20on%0Athe%20developed%20dataset%2C%20providing%20a%20comprehensive%20benchmark%20for%20future%20research.%0AThe%20dataset%2C%20results%2C%20and%20online%20leaderboard%20are%20publicly%20available%20at%0Ahttps%3A//challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11982v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIM%25202024%2520Challenge%2520on%2520Compressed%2520Video%2520Quality%2520Assessment%253A%2520Methods%2520and%250A%2520%2520Results%26entry.906535625%3DMaksim%2520Smirnov%2520and%2520Aleksandr%2520Gushchin%2520and%2520Anastasia%2520Antsiferova%2520and%2520Dmitry%2520Vatolin%2520and%2520Radu%2520Timofte%2520and%2520Ziheng%2520Jia%2520and%2520Zicheng%2520Zhang%2520and%2520Wei%2520Sun%2520and%2520Jiaying%2520Qian%2520and%2520Yuqin%2520Cao%2520and%2520Yinan%2520Sun%2520and%2520Yuxin%2520Zhu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%2520and%2520Kanjar%2520De%2520and%2520Qing%2520Luo%2520and%2520Ao-Xiang%2520Zhang%2520and%2520Peng%2520Zhang%2520and%2520Haibo%2520Lei%2520and%2520Linyan%2520Jiang%2520and%2520Yaqing%2520Li%2520and%2520Wenhui%2520Meng%2520and%2520Zhenzhong%2520Chen%2520and%2520Zhengxue%2520Cheng%2520and%2520Jiahao%2520Xiao%2520and%2520Jun%2520Xu%2520and%2520Chenlong%2520He%2520and%2520Qi%2520Zheng%2520and%2520Ruoxi%2520Zhu%2520and%2520Min%2520Li%2520and%2520Yibo%2520Fan%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520Video%2520quality%2520assessment%2520%2528VQA%2529%2520is%2520a%2520crucial%2520task%2520in%2520the%2520development%2520of%2520video%250Acompression%2520standards%252C%2520as%2520it%2520directly%2520impacts%2520the%2520viewer%2520experience.%2520This%2520paper%250Apresents%2520the%2520results%2520of%2520the%2520Compressed%2520Video%2520Quality%2520Assessment%2520challenge%252C%2520held%250Ain%2520conjunction%2520with%2520the%2520Advances%2520in%2520Image%2520Manipulation%2520%2528AIM%2529%2520workshop%2520at%2520ECCV%250A2024.%2520The%2520challenge%2520aimed%2520to%2520evaluate%2520the%2520performance%2520of%2520VQA%2520methods%2520on%2520a%250Adiverse%2520dataset%2520of%2520459%2520videos%252C%2520encoded%2520with%252014%2520codecs%2520of%2520various%2520compression%250Astandards%2520%2528AVC/H.264%252C%2520HEVC/H.265%252C%2520AV1%252C%2520and%2520VVC/H.266%2529%2520and%2520containing%2520a%250Acomprehensive%2520collection%2520of%2520compression%2520artifacts.%2520To%2520measure%2520the%2520methods%250Aperformance%252C%2520we%2520employed%2520traditional%2520correlation%2520coefficients%2520between%2520their%250Apredictions%2520and%2520subjective%2520scores%252C%2520which%2520were%2520collected%2520via%2520large-scale%250Acrowdsourced%2520pairwise%2520human%2520comparisons.%2520For%2520training%2520purposes%252C%2520participants%250Awere%2520provided%2520with%2520the%2520Compressed%2520Video%2520Quality%2520Assessment%2520Dataset%2520%2528CVQAD%2529%252C%2520a%250Apreviously%2520developed%2520dataset%2520of%25201022%2520videos.%2520Up%2520to%252030%2520participating%2520teams%250Aregistered%2520for%2520the%2520challenge%252C%2520while%2520we%2520report%2520the%2520results%2520of%25206%2520teams%252C%2520which%250Asubmitted%2520valid%2520final%2520solutions%2520and%2520code%2520for%2520reproducing%2520the%2520results.%2520Moreover%252C%250Awe%2520calculated%2520and%2520present%2520the%2520performance%2520of%2520state-of-the-art%2520VQA%2520methods%2520on%250Athe%2520developed%2520dataset%252C%2520providing%2520a%2520comprehensive%2520benchmark%2520for%2520future%2520research.%250AThe%2520dataset%252C%2520results%252C%2520and%2520online%2520leaderboard%2520are%2520publicly%2520available%2520at%250Ahttps%253A//challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11982v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIM%202024%20Challenge%20on%20Compressed%20Video%20Quality%20Assessment%3A%20Methods%20and%0A%20%20Results&entry.906535625=Maksim%20Smirnov%20and%20Aleksandr%20Gushchin%20and%20Anastasia%20Antsiferova%20and%20Dmitry%20Vatolin%20and%20Radu%20Timofte%20and%20Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Wei%20Sun%20and%20Jiaying%20Qian%20and%20Yuqin%20Cao%20and%20Yinan%20Sun%20and%20Yuxin%20Zhu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Kanjar%20De%20and%20Qing%20Luo%20and%20Ao-Xiang%20Zhang%20and%20Peng%20Zhang%20and%20Haibo%20Lei%20and%20Linyan%20Jiang%20and%20Yaqing%20Li%20and%20Wenhui%20Meng%20and%20Zhenzhong%20Chen%20and%20Zhengxue%20Cheng%20and%20Jiahao%20Xiao%20and%20Jun%20Xu%20and%20Chenlong%20He%20and%20Qi%20Zheng%20and%20Ruoxi%20Zhu%20and%20Min%20Li%20and%20Yibo%20Fan%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20Video%20quality%20assessment%20%28VQA%29%20is%20a%20crucial%20task%20in%20the%20development%20of%20video%0Acompression%20standards%2C%20as%20it%20directly%20impacts%20the%20viewer%20experience.%20This%20paper%0Apresents%20the%20results%20of%20the%20Compressed%20Video%20Quality%20Assessment%20challenge%2C%20held%0Ain%20conjunction%20with%20the%20Advances%20in%20Image%20Manipulation%20%28AIM%29%20workshop%20at%20ECCV%0A2024.%20The%20challenge%20aimed%20to%20evaluate%20the%20performance%20of%20VQA%20methods%20on%20a%0Adiverse%20dataset%20of%20459%20videos%2C%20encoded%20with%2014%20codecs%20of%20various%20compression%0Astandards%20%28AVC/H.264%2C%20HEVC/H.265%2C%20AV1%2C%20and%20VVC/H.266%29%20and%20containing%20a%0Acomprehensive%20collection%20of%20compression%20artifacts.%20To%20measure%20the%20methods%0Aperformance%2C%20we%20employed%20traditional%20correlation%20coefficients%20between%20their%0Apredictions%20and%20subjective%20scores%2C%20which%20were%20collected%20via%20large-scale%0Acrowdsourced%20pairwise%20human%20comparisons.%20For%20training%20purposes%2C%20participants%0Awere%20provided%20with%20the%20Compressed%20Video%20Quality%20Assessment%20Dataset%20%28CVQAD%29%2C%20a%0Apreviously%20developed%20dataset%20of%201022%20videos.%20Up%20to%2030%20participating%20teams%0Aregistered%20for%20the%20challenge%2C%20while%20we%20report%20the%20results%20of%206%20teams%2C%20which%0Asubmitted%20valid%20final%20solutions%20and%20code%20for%20reproducing%20the%20results.%20Moreover%2C%0Awe%20calculated%20and%20present%20the%20performance%20of%20state-of-the-art%20VQA%20methods%20on%0Athe%20developed%20dataset%2C%20providing%20a%20comprehensive%20benchmark%20for%20future%20research.%0AThe%20dataset%2C%20results%2C%20and%20online%20leaderboard%20are%20publicly%20available%20at%0Ahttps%3A//challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11982v3&entry.124074799=Read"},
{"title": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks", "author": "Shuyang Hou and Zhangxiao Shen and Anqi Zhao and Jianyuan Liang and Zhipeng Gui and Xuefeng Guan and Rui Li and Huayi Wu", "abstract": "  The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation.\n", "link": "http://arxiv.org/abs/2410.17031v1", "date": "2024-10-22", "relevancy": 2.4855, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoCode-GPT%3A%20A%20Large%20Language%20Model%20for%20Geospatial%20Code%20Generation%20Tasks&body=Title%3A%20GeoCode-GPT%3A%20A%20Large%20Language%20Model%20for%20Geospatial%20Code%20Generation%20Tasks%0AAuthor%3A%20Shuyang%20Hou%20and%20Zhangxiao%20Shen%20and%20Anqi%20Zhao%20and%20Jianyuan%20Liang%20and%20Zhipeng%20Gui%20and%20Xuefeng%20Guan%20and%20Rui%20Li%20and%20Huayi%20Wu%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20spatiotemporal%20data%20and%20modeling%20tasks%20in%0Ageosciences%20has%20made%20geospatial%20code%20generation%20technology%20a%20critical%20factor%20in%0Aenhancing%20productivity.%20Although%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Apotential%20in%20code%20generation%20tasks%2C%20they%20often%20encounter%20issues%20such%20as%20refusal%0Ato%20code%20or%20hallucination%20in%20geospatial%20code%20generation%20due%20to%20a%20lack%20of%0Adomain-specific%20knowledge%20and%20code%20corpora.%20To%20address%20these%20challenges%2C%20this%0Apaper%20presents%20and%20open-sources%20the%20GeoCode-PT%20and%20GeoCode-SFT%20corpora%2C%20along%0Awith%20the%20GeoCode-Eval%20evaluation%20dataset.%20Additionally%2C%20by%20leveraging%20QLoRA%20and%0ALoRA%20for%20pretraining%20and%20fine-tuning%2C%20we%20introduce%20GeoCode-GPT-7B%2C%20the%20first%0ALLM%20focused%20on%20geospatial%20code%20generation%2C%20fine-tuned%20from%20Code%20Llama-7B.%0AFurthermore%2C%20we%20establish%20a%20comprehensive%20geospatial%20code%20evaluation%20framework%2C%0Aincorporating%20option%20matching%2C%20expert%20validation%2C%20and%20prompt%20engineering%0Ascoring%20for%20LLMs%2C%20and%20systematically%20evaluate%20GeoCode-GPT-7B%20using%20the%0AGeoCode-Eval%20dataset.%20Experimental%20results%20show%20that%20GeoCode-GPT%20outperforms%0Aother%20models%20in%20multiple-choice%20accuracy%20by%209.1%25%20to%2032.1%25%2C%20in%20code%0Asummarization%20ability%20by%201.7%25%20to%2025.4%25%2C%20and%20in%20code%20generation%20capability%20by%0A1.2%25%20to%2025.1%25.%20This%20paper%20provides%20a%20solution%20and%20empirical%20validation%20for%0Aenhancing%20LLMs%27%20performance%20in%20geospatial%20code%20generation%2C%20extends%20the%0Aboundaries%20of%20domain-specific%20model%20applications%2C%20and%20offers%20valuable%20insights%0Ainto%20unlocking%20their%20potential%20in%20geospatial%20code%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoCode-GPT%253A%2520A%2520Large%2520Language%2520Model%2520for%2520Geospatial%2520Code%2520Generation%2520Tasks%26entry.906535625%3DShuyang%2520Hou%2520and%2520Zhangxiao%2520Shen%2520and%2520Anqi%2520Zhao%2520and%2520Jianyuan%2520Liang%2520and%2520Zhipeng%2520Gui%2520and%2520Xuefeng%2520Guan%2520and%2520Rui%2520Li%2520and%2520Huayi%2520Wu%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520spatiotemporal%2520data%2520and%2520modeling%2520tasks%2520in%250Ageosciences%2520has%2520made%2520geospatial%2520code%2520generation%2520technology%2520a%2520critical%2520factor%2520in%250Aenhancing%2520productivity.%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%250Apotential%2520in%2520code%2520generation%2520tasks%252C%2520they%2520often%2520encounter%2520issues%2520such%2520as%2520refusal%250Ato%2520code%2520or%2520hallucination%2520in%2520geospatial%2520code%2520generation%2520due%2520to%2520a%2520lack%2520of%250Adomain-specific%2520knowledge%2520and%2520code%2520corpora.%2520To%2520address%2520these%2520challenges%252C%2520this%250Apaper%2520presents%2520and%2520open-sources%2520the%2520GeoCode-PT%2520and%2520GeoCode-SFT%2520corpora%252C%2520along%250Awith%2520the%2520GeoCode-Eval%2520evaluation%2520dataset.%2520Additionally%252C%2520by%2520leveraging%2520QLoRA%2520and%250ALoRA%2520for%2520pretraining%2520and%2520fine-tuning%252C%2520we%2520introduce%2520GeoCode-GPT-7B%252C%2520the%2520first%250ALLM%2520focused%2520on%2520geospatial%2520code%2520generation%252C%2520fine-tuned%2520from%2520Code%2520Llama-7B.%250AFurthermore%252C%2520we%2520establish%2520a%2520comprehensive%2520geospatial%2520code%2520evaluation%2520framework%252C%250Aincorporating%2520option%2520matching%252C%2520expert%2520validation%252C%2520and%2520prompt%2520engineering%250Ascoring%2520for%2520LLMs%252C%2520and%2520systematically%2520evaluate%2520GeoCode-GPT-7B%2520using%2520the%250AGeoCode-Eval%2520dataset.%2520Experimental%2520results%2520show%2520that%2520GeoCode-GPT%2520outperforms%250Aother%2520models%2520in%2520multiple-choice%2520accuracy%2520by%25209.1%2525%2520to%252032.1%2525%252C%2520in%2520code%250Asummarization%2520ability%2520by%25201.7%2525%2520to%252025.4%2525%252C%2520and%2520in%2520code%2520generation%2520capability%2520by%250A1.2%2525%2520to%252025.1%2525.%2520This%2520paper%2520provides%2520a%2520solution%2520and%2520empirical%2520validation%2520for%250Aenhancing%2520LLMs%2527%2520performance%2520in%2520geospatial%2520code%2520generation%252C%2520extends%2520the%250Aboundaries%2520of%2520domain-specific%2520model%2520applications%252C%2520and%2520offers%2520valuable%2520insights%250Ainto%2520unlocking%2520their%2520potential%2520in%2520geospatial%2520code%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoCode-GPT%3A%20A%20Large%20Language%20Model%20for%20Geospatial%20Code%20Generation%20Tasks&entry.906535625=Shuyang%20Hou%20and%20Zhangxiao%20Shen%20and%20Anqi%20Zhao%20and%20Jianyuan%20Liang%20and%20Zhipeng%20Gui%20and%20Xuefeng%20Guan%20and%20Rui%20Li%20and%20Huayi%20Wu&entry.1292438233=%20%20The%20increasing%20demand%20for%20spatiotemporal%20data%20and%20modeling%20tasks%20in%0Ageosciences%20has%20made%20geospatial%20code%20generation%20technology%20a%20critical%20factor%20in%0Aenhancing%20productivity.%20Although%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Apotential%20in%20code%20generation%20tasks%2C%20they%20often%20encounter%20issues%20such%20as%20refusal%0Ato%20code%20or%20hallucination%20in%20geospatial%20code%20generation%20due%20to%20a%20lack%20of%0Adomain-specific%20knowledge%20and%20code%20corpora.%20To%20address%20these%20challenges%2C%20this%0Apaper%20presents%20and%20open-sources%20the%20GeoCode-PT%20and%20GeoCode-SFT%20corpora%2C%20along%0Awith%20the%20GeoCode-Eval%20evaluation%20dataset.%20Additionally%2C%20by%20leveraging%20QLoRA%20and%0ALoRA%20for%20pretraining%20and%20fine-tuning%2C%20we%20introduce%20GeoCode-GPT-7B%2C%20the%20first%0ALLM%20focused%20on%20geospatial%20code%20generation%2C%20fine-tuned%20from%20Code%20Llama-7B.%0AFurthermore%2C%20we%20establish%20a%20comprehensive%20geospatial%20code%20evaluation%20framework%2C%0Aincorporating%20option%20matching%2C%20expert%20validation%2C%20and%20prompt%20engineering%0Ascoring%20for%20LLMs%2C%20and%20systematically%20evaluate%20GeoCode-GPT-7B%20using%20the%0AGeoCode-Eval%20dataset.%20Experimental%20results%20show%20that%20GeoCode-GPT%20outperforms%0Aother%20models%20in%20multiple-choice%20accuracy%20by%209.1%25%20to%2032.1%25%2C%20in%20code%0Asummarization%20ability%20by%201.7%25%20to%2025.4%25%2C%20and%20in%20code%20generation%20capability%20by%0A1.2%25%20to%2025.1%25.%20This%20paper%20provides%20a%20solution%20and%20empirical%20validation%20for%0Aenhancing%20LLMs%27%20performance%20in%20geospatial%20code%20generation%2C%20extends%20the%0Aboundaries%20of%20domain-specific%20model%20applications%2C%20and%20offers%20valuable%20insights%0Ainto%20unlocking%20their%20potential%20in%20geospatial%20code%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17031v1&entry.124074799=Read"},
{"title": "AlphaChimp: Tracking and Behavior Recognition of Chimpanzees", "author": "Xiaoxuan Ma and Yutang Lin and Yuan Xu and Stephan P. Kaufhold and Jack Terwilliger and Andres Meza and Yixin Zhu and Federico Rossano and Yizhou Wang", "abstract": "  Understanding non-human primate behavior is crucial for improving animal\nwelfare, modeling social behavior, and gaining insights into both distinctly\nhuman and shared behaviors. Despite recent advances in computer vision,\nautomated analysis of primate behavior remains challenging due to the\ncomplexity of their social interactions and the lack of specialized algorithms.\nExisting methods often struggle with the nuanced behaviors and frequent\nocclusions characteristic of primate social dynamics. This study aims to\ndevelop an effective method for automated detection, tracking, and recognition\nof chimpanzee behaviors in video footage. Here we show that our proposed\nmethod, AlphaChimp, an end-to-end approach that simultaneously detects\nchimpanzee positions and estimates behavior categories from videos,\nsignificantly outperforms existing methods in behavior recognition. AlphaChimp\nachieves approximately 10% higher tracking accuracy and a 20% improvement in\nbehavior recognition compared to state-of-the-art methods, particularly\nexcelling in the recognition of social behaviors. This superior performance\nstems from AlphaChimp's innovative architecture, which integrates temporal\nfeature fusion with a Transformer-based self-attention mechanism, enabling more\neffective capture and interpretation of complex social interactions among\nchimpanzees. Our approach bridges the gap between computer vision and\nprimatology, enhancing technical capabilities and deepening our understanding\nof primate communication and sociality. We release our code and models and hope\nthis will facilitate future research in animal social dynamics. This work\ncontributes to ethology, cognitive science, and artificial intelligence,\noffering new perspectives on social intelligence.\n", "link": "http://arxiv.org/abs/2410.17136v1", "date": "2024-10-22", "relevancy": 2.4811, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaChimp%3A%20Tracking%20and%20Behavior%20Recognition%20of%20Chimpanzees&body=Title%3A%20AlphaChimp%3A%20Tracking%20and%20Behavior%20Recognition%20of%20Chimpanzees%0AAuthor%3A%20Xiaoxuan%20Ma%20and%20Yutang%20Lin%20and%20Yuan%20Xu%20and%20Stephan%20P.%20Kaufhold%20and%20Jack%20Terwilliger%20and%20Andres%20Meza%20and%20Yixin%20Zhu%20and%20Federico%20Rossano%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20Understanding%20non-human%20primate%20behavior%20is%20crucial%20for%20improving%20animal%0Awelfare%2C%20modeling%20social%20behavior%2C%20and%20gaining%20insights%20into%20both%20distinctly%0Ahuman%20and%20shared%20behaviors.%20Despite%20recent%20advances%20in%20computer%20vision%2C%0Aautomated%20analysis%20of%20primate%20behavior%20remains%20challenging%20due%20to%20the%0Acomplexity%20of%20their%20social%20interactions%20and%20the%20lack%20of%20specialized%20algorithms.%0AExisting%20methods%20often%20struggle%20with%20the%20nuanced%20behaviors%20and%20frequent%0Aocclusions%20characteristic%20of%20primate%20social%20dynamics.%20This%20study%20aims%20to%0Adevelop%20an%20effective%20method%20for%20automated%20detection%2C%20tracking%2C%20and%20recognition%0Aof%20chimpanzee%20behaviors%20in%20video%20footage.%20Here%20we%20show%20that%20our%20proposed%0Amethod%2C%20AlphaChimp%2C%20an%20end-to-end%20approach%20that%20simultaneously%20detects%0Achimpanzee%20positions%20and%20estimates%20behavior%20categories%20from%20videos%2C%0Asignificantly%20outperforms%20existing%20methods%20in%20behavior%20recognition.%20AlphaChimp%0Aachieves%20approximately%2010%25%20higher%20tracking%20accuracy%20and%20a%2020%25%20improvement%20in%0Abehavior%20recognition%20compared%20to%20state-of-the-art%20methods%2C%20particularly%0Aexcelling%20in%20the%20recognition%20of%20social%20behaviors.%20This%20superior%20performance%0Astems%20from%20AlphaChimp%27s%20innovative%20architecture%2C%20which%20integrates%20temporal%0Afeature%20fusion%20with%20a%20Transformer-based%20self-attention%20mechanism%2C%20enabling%20more%0Aeffective%20capture%20and%20interpretation%20of%20complex%20social%20interactions%20among%0Achimpanzees.%20Our%20approach%20bridges%20the%20gap%20between%20computer%20vision%20and%0Aprimatology%2C%20enhancing%20technical%20capabilities%20and%20deepening%20our%20understanding%0Aof%20primate%20communication%20and%20sociality.%20We%20release%20our%20code%20and%20models%20and%20hope%0Athis%20will%20facilitate%20future%20research%20in%20animal%20social%20dynamics.%20This%20work%0Acontributes%20to%20ethology%2C%20cognitive%20science%2C%20and%20artificial%20intelligence%2C%0Aoffering%20new%20perspectives%20on%20social%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaChimp%253A%2520Tracking%2520and%2520Behavior%2520Recognition%2520of%2520Chimpanzees%26entry.906535625%3DXiaoxuan%2520Ma%2520and%2520Yutang%2520Lin%2520and%2520Yuan%2520Xu%2520and%2520Stephan%2520P.%2520Kaufhold%2520and%2520Jack%2520Terwilliger%2520and%2520Andres%2520Meza%2520and%2520Yixin%2520Zhu%2520and%2520Federico%2520Rossano%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520Understanding%2520non-human%2520primate%2520behavior%2520is%2520crucial%2520for%2520improving%2520animal%250Awelfare%252C%2520modeling%2520social%2520behavior%252C%2520and%2520gaining%2520insights%2520into%2520both%2520distinctly%250Ahuman%2520and%2520shared%2520behaviors.%2520Despite%2520recent%2520advances%2520in%2520computer%2520vision%252C%250Aautomated%2520analysis%2520of%2520primate%2520behavior%2520remains%2520challenging%2520due%2520to%2520the%250Acomplexity%2520of%2520their%2520social%2520interactions%2520and%2520the%2520lack%2520of%2520specialized%2520algorithms.%250AExisting%2520methods%2520often%2520struggle%2520with%2520the%2520nuanced%2520behaviors%2520and%2520frequent%250Aocclusions%2520characteristic%2520of%2520primate%2520social%2520dynamics.%2520This%2520study%2520aims%2520to%250Adevelop%2520an%2520effective%2520method%2520for%2520automated%2520detection%252C%2520tracking%252C%2520and%2520recognition%250Aof%2520chimpanzee%2520behaviors%2520in%2520video%2520footage.%2520Here%2520we%2520show%2520that%2520our%2520proposed%250Amethod%252C%2520AlphaChimp%252C%2520an%2520end-to-end%2520approach%2520that%2520simultaneously%2520detects%250Achimpanzee%2520positions%2520and%2520estimates%2520behavior%2520categories%2520from%2520videos%252C%250Asignificantly%2520outperforms%2520existing%2520methods%2520in%2520behavior%2520recognition.%2520AlphaChimp%250Aachieves%2520approximately%252010%2525%2520higher%2520tracking%2520accuracy%2520and%2520a%252020%2525%2520improvement%2520in%250Abehavior%2520recognition%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520particularly%250Aexcelling%2520in%2520the%2520recognition%2520of%2520social%2520behaviors.%2520This%2520superior%2520performance%250Astems%2520from%2520AlphaChimp%2527s%2520innovative%2520architecture%252C%2520which%2520integrates%2520temporal%250Afeature%2520fusion%2520with%2520a%2520Transformer-based%2520self-attention%2520mechanism%252C%2520enabling%2520more%250Aeffective%2520capture%2520and%2520interpretation%2520of%2520complex%2520social%2520interactions%2520among%250Achimpanzees.%2520Our%2520approach%2520bridges%2520the%2520gap%2520between%2520computer%2520vision%2520and%250Aprimatology%252C%2520enhancing%2520technical%2520capabilities%2520and%2520deepening%2520our%2520understanding%250Aof%2520primate%2520communication%2520and%2520sociality.%2520We%2520release%2520our%2520code%2520and%2520models%2520and%2520hope%250Athis%2520will%2520facilitate%2520future%2520research%2520in%2520animal%2520social%2520dynamics.%2520This%2520work%250Acontributes%2520to%2520ethology%252C%2520cognitive%2520science%252C%2520and%2520artificial%2520intelligence%252C%250Aoffering%2520new%2520perspectives%2520on%2520social%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaChimp%3A%20Tracking%20and%20Behavior%20Recognition%20of%20Chimpanzees&entry.906535625=Xiaoxuan%20Ma%20and%20Yutang%20Lin%20and%20Yuan%20Xu%20and%20Stephan%20P.%20Kaufhold%20and%20Jack%20Terwilliger%20and%20Andres%20Meza%20and%20Yixin%20Zhu%20and%20Federico%20Rossano%20and%20Yizhou%20Wang&entry.1292438233=%20%20Understanding%20non-human%20primate%20behavior%20is%20crucial%20for%20improving%20animal%0Awelfare%2C%20modeling%20social%20behavior%2C%20and%20gaining%20insights%20into%20both%20distinctly%0Ahuman%20and%20shared%20behaviors.%20Despite%20recent%20advances%20in%20computer%20vision%2C%0Aautomated%20analysis%20of%20primate%20behavior%20remains%20challenging%20due%20to%20the%0Acomplexity%20of%20their%20social%20interactions%20and%20the%20lack%20of%20specialized%20algorithms.%0AExisting%20methods%20often%20struggle%20with%20the%20nuanced%20behaviors%20and%20frequent%0Aocclusions%20characteristic%20of%20primate%20social%20dynamics.%20This%20study%20aims%20to%0Adevelop%20an%20effective%20method%20for%20automated%20detection%2C%20tracking%2C%20and%20recognition%0Aof%20chimpanzee%20behaviors%20in%20video%20footage.%20Here%20we%20show%20that%20our%20proposed%0Amethod%2C%20AlphaChimp%2C%20an%20end-to-end%20approach%20that%20simultaneously%20detects%0Achimpanzee%20positions%20and%20estimates%20behavior%20categories%20from%20videos%2C%0Asignificantly%20outperforms%20existing%20methods%20in%20behavior%20recognition.%20AlphaChimp%0Aachieves%20approximately%2010%25%20higher%20tracking%20accuracy%20and%20a%2020%25%20improvement%20in%0Abehavior%20recognition%20compared%20to%20state-of-the-art%20methods%2C%20particularly%0Aexcelling%20in%20the%20recognition%20of%20social%20behaviors.%20This%20superior%20performance%0Astems%20from%20AlphaChimp%27s%20innovative%20architecture%2C%20which%20integrates%20temporal%0Afeature%20fusion%20with%20a%20Transformer-based%20self-attention%20mechanism%2C%20enabling%20more%0Aeffective%20capture%20and%20interpretation%20of%20complex%20social%20interactions%20among%0Achimpanzees.%20Our%20approach%20bridges%20the%20gap%20between%20computer%20vision%20and%0Aprimatology%2C%20enhancing%20technical%20capabilities%20and%20deepening%20our%20understanding%0Aof%20primate%20communication%20and%20sociality.%20We%20release%20our%20code%20and%20models%20and%20hope%0Athis%20will%20facilitate%20future%20research%20in%20animal%20social%20dynamics.%20This%20work%0Acontributes%20to%20ethology%2C%20cognitive%20science%2C%20and%20artificial%20intelligence%2C%0Aoffering%20new%20perspectives%20on%20social%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17136v1&entry.124074799=Read"},
{"title": "RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance", "author": "Zhicheng Sun and Zhenhao Yang and Yang Jin and Haozhe Chi and Kun Xu and Kun Xu and Liwei Chen and Hao Jiang and Yang Song and Kun Gai and Yadong Mu", "abstract": "  Customizing diffusion models to generate identity-preserving images from\nuser-provided reference images is an intriguing new problem. The prevalent\napproaches typically require training on extensive domain-specific images to\nachieve identity preservation, which lacks flexibility across different use\ncases. To address this issue, we exploit classifier guidance, a training-free\ntechnique that steers diffusion models using an existing classifier, for\npersonalized image generation. Our study shows that based on a recent rectified\nflow framework, the major limitation of vanilla classifier guidance in\nrequiring a special classifier can be resolved with a simple fixed-point\nsolution, allowing flexible personalization with off-the-shelf image\ndiscriminators. Moreover, its solving procedure proves to be stable when\nanchored to a reference flow trajectory, with a convergence guarantee. The\nderived method is implemented on rectified flow with different off-the-shelf\nimage discriminators, delivering advantageous personalization results for human\nfaces, live subjects, and certain objects. Code is available at\nhttps://github.com/feifeiobama/RectifID.\n", "link": "http://arxiv.org/abs/2405.14677v2", "date": "2024-10-22", "relevancy": 2.4158, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6671}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6132}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RectifID%3A%20Personalizing%20Rectified%20Flow%20with%20Anchored%20Classifier%20Guidance&body=Title%3A%20RectifID%3A%20Personalizing%20Rectified%20Flow%20with%20Anchored%20Classifier%20Guidance%0AAuthor%3A%20Zhicheng%20Sun%20and%20Zhenhao%20Yang%20and%20Yang%20Jin%20and%20Haozhe%20Chi%20and%20Kun%20Xu%20and%20Kun%20Xu%20and%20Liwei%20Chen%20and%20Hao%20Jiang%20and%20Yang%20Song%20and%20Kun%20Gai%20and%20Yadong%20Mu%0AAbstract%3A%20%20%20Customizing%20diffusion%20models%20to%20generate%20identity-preserving%20images%20from%0Auser-provided%20reference%20images%20is%20an%20intriguing%20new%20problem.%20The%20prevalent%0Aapproaches%20typically%20require%20training%20on%20extensive%20domain-specific%20images%20to%0Aachieve%20identity%20preservation%2C%20which%20lacks%20flexibility%20across%20different%20use%0Acases.%20To%20address%20this%20issue%2C%20we%20exploit%20classifier%20guidance%2C%20a%20training-free%0Atechnique%20that%20steers%20diffusion%20models%20using%20an%20existing%20classifier%2C%20for%0Apersonalized%20image%20generation.%20Our%20study%20shows%20that%20based%20on%20a%20recent%20rectified%0Aflow%20framework%2C%20the%20major%20limitation%20of%20vanilla%20classifier%20guidance%20in%0Arequiring%20a%20special%20classifier%20can%20be%20resolved%20with%20a%20simple%20fixed-point%0Asolution%2C%20allowing%20flexible%20personalization%20with%20off-the-shelf%20image%0Adiscriminators.%20Moreover%2C%20its%20solving%20procedure%20proves%20to%20be%20stable%20when%0Aanchored%20to%20a%20reference%20flow%20trajectory%2C%20with%20a%20convergence%20guarantee.%20The%0Aderived%20method%20is%20implemented%20on%20rectified%20flow%20with%20different%20off-the-shelf%0Aimage%20discriminators%2C%20delivering%20advantageous%20personalization%20results%20for%20human%0Afaces%2C%20live%20subjects%2C%20and%20certain%20objects.%20Code%20is%20available%20at%0Ahttps%3A//github.com/feifeiobama/RectifID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRectifID%253A%2520Personalizing%2520Rectified%2520Flow%2520with%2520Anchored%2520Classifier%2520Guidance%26entry.906535625%3DZhicheng%2520Sun%2520and%2520Zhenhao%2520Yang%2520and%2520Yang%2520Jin%2520and%2520Haozhe%2520Chi%2520and%2520Kun%2520Xu%2520and%2520Kun%2520Xu%2520and%2520Liwei%2520Chen%2520and%2520Hao%2520Jiang%2520and%2520Yang%2520Song%2520and%2520Kun%2520Gai%2520and%2520Yadong%2520Mu%26entry.1292438233%3D%2520%2520Customizing%2520diffusion%2520models%2520to%2520generate%2520identity-preserving%2520images%2520from%250Auser-provided%2520reference%2520images%2520is%2520an%2520intriguing%2520new%2520problem.%2520The%2520prevalent%250Aapproaches%2520typically%2520require%2520training%2520on%2520extensive%2520domain-specific%2520images%2520to%250Aachieve%2520identity%2520preservation%252C%2520which%2520lacks%2520flexibility%2520across%2520different%2520use%250Acases.%2520To%2520address%2520this%2520issue%252C%2520we%2520exploit%2520classifier%2520guidance%252C%2520a%2520training-free%250Atechnique%2520that%2520steers%2520diffusion%2520models%2520using%2520an%2520existing%2520classifier%252C%2520for%250Apersonalized%2520image%2520generation.%2520Our%2520study%2520shows%2520that%2520based%2520on%2520a%2520recent%2520rectified%250Aflow%2520framework%252C%2520the%2520major%2520limitation%2520of%2520vanilla%2520classifier%2520guidance%2520in%250Arequiring%2520a%2520special%2520classifier%2520can%2520be%2520resolved%2520with%2520a%2520simple%2520fixed-point%250Asolution%252C%2520allowing%2520flexible%2520personalization%2520with%2520off-the-shelf%2520image%250Adiscriminators.%2520Moreover%252C%2520its%2520solving%2520procedure%2520proves%2520to%2520be%2520stable%2520when%250Aanchored%2520to%2520a%2520reference%2520flow%2520trajectory%252C%2520with%2520a%2520convergence%2520guarantee.%2520The%250Aderived%2520method%2520is%2520implemented%2520on%2520rectified%2520flow%2520with%2520different%2520off-the-shelf%250Aimage%2520discriminators%252C%2520delivering%2520advantageous%2520personalization%2520results%2520for%2520human%250Afaces%252C%2520live%2520subjects%252C%2520and%2520certain%2520objects.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/feifeiobama/RectifID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RectifID%3A%20Personalizing%20Rectified%20Flow%20with%20Anchored%20Classifier%20Guidance&entry.906535625=Zhicheng%20Sun%20and%20Zhenhao%20Yang%20and%20Yang%20Jin%20and%20Haozhe%20Chi%20and%20Kun%20Xu%20and%20Kun%20Xu%20and%20Liwei%20Chen%20and%20Hao%20Jiang%20and%20Yang%20Song%20and%20Kun%20Gai%20and%20Yadong%20Mu&entry.1292438233=%20%20Customizing%20diffusion%20models%20to%20generate%20identity-preserving%20images%20from%0Auser-provided%20reference%20images%20is%20an%20intriguing%20new%20problem.%20The%20prevalent%0Aapproaches%20typically%20require%20training%20on%20extensive%20domain-specific%20images%20to%0Aachieve%20identity%20preservation%2C%20which%20lacks%20flexibility%20across%20different%20use%0Acases.%20To%20address%20this%20issue%2C%20we%20exploit%20classifier%20guidance%2C%20a%20training-free%0Atechnique%20that%20steers%20diffusion%20models%20using%20an%20existing%20classifier%2C%20for%0Apersonalized%20image%20generation.%20Our%20study%20shows%20that%20based%20on%20a%20recent%20rectified%0Aflow%20framework%2C%20the%20major%20limitation%20of%20vanilla%20classifier%20guidance%20in%0Arequiring%20a%20special%20classifier%20can%20be%20resolved%20with%20a%20simple%20fixed-point%0Asolution%2C%20allowing%20flexible%20personalization%20with%20off-the-shelf%20image%0Adiscriminators.%20Moreover%2C%20its%20solving%20procedure%20proves%20to%20be%20stable%20when%0Aanchored%20to%20a%20reference%20flow%20trajectory%2C%20with%20a%20convergence%20guarantee.%20The%0Aderived%20method%20is%20implemented%20on%20rectified%20flow%20with%20different%20off-the-shelf%0Aimage%20discriminators%2C%20delivering%20advantageous%20personalization%20results%20for%20human%0Afaces%2C%20live%20subjects%2C%20and%20certain%20objects.%20Code%20is%20available%20at%0Ahttps%3A//github.com/feifeiobama/RectifID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14677v2&entry.124074799=Read"},
{"title": "A Survey on Deep Learning-based Gaze Direction Regression: Searching for\n  the State-of-the-art", "author": "Franko \u0160iki\u0107 and Donik Vr\u0161nak and Sven Lon\u010dari\u0107", "abstract": "  In this paper, we present a survey of deep learning-based methods for the\nregression of gaze direction vector from head and eye images. We describe in\ndetail numerous published methods with a focus on the input data, architecture\nof the model, and loss function used to supervise the model. Additionally, we\npresent a list of datasets that can be used to train and evaluate gaze\ndirection regression methods. Furthermore, we noticed that the results reported\nin the literature are often not comparable one to another due to differences in\nthe validation or even test subsets used. To address this problem, we\nre-evaluated several methods on the commonly used in-the-wild Gaze360 dataset\nusing the same validation setup. The experimental results show that the latest\nmethods, although claiming state-of-the-art results, significantly underperform\ncompared with some older methods. Finally, we show that the temporal models\noutperform the static models under static test conditions.\n", "link": "http://arxiv.org/abs/2410.17082v1", "date": "2024-10-22", "relevancy": 2.4081, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4889}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4785}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Deep%20Learning-based%20Gaze%20Direction%20Regression%3A%20Searching%20for%0A%20%20the%20State-of-the-art&body=Title%3A%20A%20Survey%20on%20Deep%20Learning-based%20Gaze%20Direction%20Regression%3A%20Searching%20for%0A%20%20the%20State-of-the-art%0AAuthor%3A%20Franko%20%C5%A0iki%C4%87%20and%20Donik%20Vr%C5%A1nak%20and%20Sven%20Lon%C4%8Dari%C4%87%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20survey%20of%20deep%20learning-based%20methods%20for%20the%0Aregression%20of%20gaze%20direction%20vector%20from%20head%20and%20eye%20images.%20We%20describe%20in%0Adetail%20numerous%20published%20methods%20with%20a%20focus%20on%20the%20input%20data%2C%20architecture%0Aof%20the%20model%2C%20and%20loss%20function%20used%20to%20supervise%20the%20model.%20Additionally%2C%20we%0Apresent%20a%20list%20of%20datasets%20that%20can%20be%20used%20to%20train%20and%20evaluate%20gaze%0Adirection%20regression%20methods.%20Furthermore%2C%20we%20noticed%20that%20the%20results%20reported%0Ain%20the%20literature%20are%20often%20not%20comparable%20one%20to%20another%20due%20to%20differences%20in%0Athe%20validation%20or%20even%20test%20subsets%20used.%20To%20address%20this%20problem%2C%20we%0Are-evaluated%20several%20methods%20on%20the%20commonly%20used%20in-the-wild%20Gaze360%20dataset%0Ausing%20the%20same%20validation%20setup.%20The%20experimental%20results%20show%20that%20the%20latest%0Amethods%2C%20although%20claiming%20state-of-the-art%20results%2C%20significantly%20underperform%0Acompared%20with%20some%20older%20methods.%20Finally%2C%20we%20show%20that%20the%20temporal%20models%0Aoutperform%20the%20static%20models%20under%20static%20test%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Deep%2520Learning-based%2520Gaze%2520Direction%2520Regression%253A%2520Searching%2520for%250A%2520%2520the%2520State-of-the-art%26entry.906535625%3DFranko%2520%25C5%25A0iki%25C4%2587%2520and%2520Donik%2520Vr%25C5%25A1nak%2520and%2520Sven%2520Lon%25C4%258Dari%25C4%2587%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520survey%2520of%2520deep%2520learning-based%2520methods%2520for%2520the%250Aregression%2520of%2520gaze%2520direction%2520vector%2520from%2520head%2520and%2520eye%2520images.%2520We%2520describe%2520in%250Adetail%2520numerous%2520published%2520methods%2520with%2520a%2520focus%2520on%2520the%2520input%2520data%252C%2520architecture%250Aof%2520the%2520model%252C%2520and%2520loss%2520function%2520used%2520to%2520supervise%2520the%2520model.%2520Additionally%252C%2520we%250Apresent%2520a%2520list%2520of%2520datasets%2520that%2520can%2520be%2520used%2520to%2520train%2520and%2520evaluate%2520gaze%250Adirection%2520regression%2520methods.%2520Furthermore%252C%2520we%2520noticed%2520that%2520the%2520results%2520reported%250Ain%2520the%2520literature%2520are%2520often%2520not%2520comparable%2520one%2520to%2520another%2520due%2520to%2520differences%2520in%250Athe%2520validation%2520or%2520even%2520test%2520subsets%2520used.%2520To%2520address%2520this%2520problem%252C%2520we%250Are-evaluated%2520several%2520methods%2520on%2520the%2520commonly%2520used%2520in-the-wild%2520Gaze360%2520dataset%250Ausing%2520the%2520same%2520validation%2520setup.%2520The%2520experimental%2520results%2520show%2520that%2520the%2520latest%250Amethods%252C%2520although%2520claiming%2520state-of-the-art%2520results%252C%2520significantly%2520underperform%250Acompared%2520with%2520some%2520older%2520methods.%2520Finally%252C%2520we%2520show%2520that%2520the%2520temporal%2520models%250Aoutperform%2520the%2520static%2520models%2520under%2520static%2520test%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Deep%20Learning-based%20Gaze%20Direction%20Regression%3A%20Searching%20for%0A%20%20the%20State-of-the-art&entry.906535625=Franko%20%C5%A0iki%C4%87%20and%20Donik%20Vr%C5%A1nak%20and%20Sven%20Lon%C4%8Dari%C4%87&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20survey%20of%20deep%20learning-based%20methods%20for%20the%0Aregression%20of%20gaze%20direction%20vector%20from%20head%20and%20eye%20images.%20We%20describe%20in%0Adetail%20numerous%20published%20methods%20with%20a%20focus%20on%20the%20input%20data%2C%20architecture%0Aof%20the%20model%2C%20and%20loss%20function%20used%20to%20supervise%20the%20model.%20Additionally%2C%20we%0Apresent%20a%20list%20of%20datasets%20that%20can%20be%20used%20to%20train%20and%20evaluate%20gaze%0Adirection%20regression%20methods.%20Furthermore%2C%20we%20noticed%20that%20the%20results%20reported%0Ain%20the%20literature%20are%20often%20not%20comparable%20one%20to%20another%20due%20to%20differences%20in%0Athe%20validation%20or%20even%20test%20subsets%20used.%20To%20address%20this%20problem%2C%20we%0Are-evaluated%20several%20methods%20on%20the%20commonly%20used%20in-the-wild%20Gaze360%20dataset%0Ausing%20the%20same%20validation%20setup.%20The%20experimental%20results%20show%20that%20the%20latest%0Amethods%2C%20although%20claiming%20state-of-the-art%20results%2C%20significantly%20underperform%0Acompared%20with%20some%20older%20methods.%20Finally%2C%20we%20show%20that%20the%20temporal%20models%0Aoutperform%20the%20static%20models%20under%20static%20test%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17082v1&entry.124074799=Read"},
{"title": "LiNo: Advancing Recursive Residual Decomposition of Linear and Nonlinear\n  Patterns for Robust Time Series Forecasting", "author": "Guoqi Yu and Yaoming Li and Xiaoyu Guo and Dayu Wang and Zirui Liu and Shujun Wang and Tong Yang", "abstract": "  Forecasting models are pivotal in a data-driven world with vast volumes of\ntime series data that appear as a compound of vast Linear and Nonlinear\npatterns. Recent deep time series forecasting models struggle to utilize\nseasonal and trend decomposition to separate the entangled components. Such a\nstrategy only explicitly extracts simple linear patterns like trends, leaving\nthe other linear modes and vast unexplored nonlinear patterns to the residual.\nTheir flawed linear and nonlinear feature extraction models and shallow-level\ndecomposition limit their adaptation to the diverse patterns present in\nreal-world scenarios. Given this, we innovate Recursive Residual Decomposition\nby introducing explicit extraction of both linear and nonlinear patterns. This\ndeeper-level decomposition framework, which is named LiNo, captures linear\npatterns using a Li block which can be a moving average kernel, and models\nnonlinear patterns using a No block which can be a Transformer encoder. The\nextraction of these two patterns is performed alternatively and recursively. To\nachieve the full potential of LiNo, we develop the current simple linear\npattern extractor to a general learnable autoregressive model, and design a\nnovel No block that can handle all essential nonlinear patterns. Remarkably,\nthe proposed LiNo achieves state-of-the-art on thirteen real-world benchmarks\nunder univariate and multivariate forecasting scenarios. Experiments show that\ncurrent forecasting models can deliver more robust and precise results through\nthis advanced Recursive Residual Decomposition. We hope this work could offer\ninsight into designing more effective forecasting models. Code is available at\nthis Repository: https://github.com/Levi-Ackman/LiNo.\n", "link": "http://arxiv.org/abs/2410.17159v1", "date": "2024-10-22", "relevancy": 2.3855, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiNo%3A%20Advancing%20Recursive%20Residual%20Decomposition%20of%20Linear%20and%20Nonlinear%0A%20%20Patterns%20for%20Robust%20Time%20Series%20Forecasting&body=Title%3A%20LiNo%3A%20Advancing%20Recursive%20Residual%20Decomposition%20of%20Linear%20and%20Nonlinear%0A%20%20Patterns%20for%20Robust%20Time%20Series%20Forecasting%0AAuthor%3A%20Guoqi%20Yu%20and%20Yaoming%20Li%20and%20Xiaoyu%20Guo%20and%20Dayu%20Wang%20and%20Zirui%20Liu%20and%20Shujun%20Wang%20and%20Tong%20Yang%0AAbstract%3A%20%20%20Forecasting%20models%20are%20pivotal%20in%20a%20data-driven%20world%20with%20vast%20volumes%20of%0Atime%20series%20data%20that%20appear%20as%20a%20compound%20of%20vast%20Linear%20and%20Nonlinear%0Apatterns.%20Recent%20deep%20time%20series%20forecasting%20models%20struggle%20to%20utilize%0Aseasonal%20and%20trend%20decomposition%20to%20separate%20the%20entangled%20components.%20Such%20a%0Astrategy%20only%20explicitly%20extracts%20simple%20linear%20patterns%20like%20trends%2C%20leaving%0Athe%20other%20linear%20modes%20and%20vast%20unexplored%20nonlinear%20patterns%20to%20the%20residual.%0ATheir%20flawed%20linear%20and%20nonlinear%20feature%20extraction%20models%20and%20shallow-level%0Adecomposition%20limit%20their%20adaptation%20to%20the%20diverse%20patterns%20present%20in%0Areal-world%20scenarios.%20Given%20this%2C%20we%20innovate%20Recursive%20Residual%20Decomposition%0Aby%20introducing%20explicit%20extraction%20of%20both%20linear%20and%20nonlinear%20patterns.%20This%0Adeeper-level%20decomposition%20framework%2C%20which%20is%20named%20LiNo%2C%20captures%20linear%0Apatterns%20using%20a%20Li%20block%20which%20can%20be%20a%20moving%20average%20kernel%2C%20and%20models%0Anonlinear%20patterns%20using%20a%20No%20block%20which%20can%20be%20a%20Transformer%20encoder.%20The%0Aextraction%20of%20these%20two%20patterns%20is%20performed%20alternatively%20and%20recursively.%20To%0Aachieve%20the%20full%20potential%20of%20LiNo%2C%20we%20develop%20the%20current%20simple%20linear%0Apattern%20extractor%20to%20a%20general%20learnable%20autoregressive%20model%2C%20and%20design%20a%0Anovel%20No%20block%20that%20can%20handle%20all%20essential%20nonlinear%20patterns.%20Remarkably%2C%0Athe%20proposed%20LiNo%20achieves%20state-of-the-art%20on%20thirteen%20real-world%20benchmarks%0Aunder%20univariate%20and%20multivariate%20forecasting%20scenarios.%20Experiments%20show%20that%0Acurrent%20forecasting%20models%20can%20deliver%20more%20robust%20and%20precise%20results%20through%0Athis%20advanced%20Recursive%20Residual%20Decomposition.%20We%20hope%20this%20work%20could%20offer%0Ainsight%20into%20designing%20more%20effective%20forecasting%20models.%20Code%20is%20available%20at%0Athis%20Repository%3A%20https%3A//github.com/Levi-Ackman/LiNo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiNo%253A%2520Advancing%2520Recursive%2520Residual%2520Decomposition%2520of%2520Linear%2520and%2520Nonlinear%250A%2520%2520Patterns%2520for%2520Robust%2520Time%2520Series%2520Forecasting%26entry.906535625%3DGuoqi%2520Yu%2520and%2520Yaoming%2520Li%2520and%2520Xiaoyu%2520Guo%2520and%2520Dayu%2520Wang%2520and%2520Zirui%2520Liu%2520and%2520Shujun%2520Wang%2520and%2520Tong%2520Yang%26entry.1292438233%3D%2520%2520Forecasting%2520models%2520are%2520pivotal%2520in%2520a%2520data-driven%2520world%2520with%2520vast%2520volumes%2520of%250Atime%2520series%2520data%2520that%2520appear%2520as%2520a%2520compound%2520of%2520vast%2520Linear%2520and%2520Nonlinear%250Apatterns.%2520Recent%2520deep%2520time%2520series%2520forecasting%2520models%2520struggle%2520to%2520utilize%250Aseasonal%2520and%2520trend%2520decomposition%2520to%2520separate%2520the%2520entangled%2520components.%2520Such%2520a%250Astrategy%2520only%2520explicitly%2520extracts%2520simple%2520linear%2520patterns%2520like%2520trends%252C%2520leaving%250Athe%2520other%2520linear%2520modes%2520and%2520vast%2520unexplored%2520nonlinear%2520patterns%2520to%2520the%2520residual.%250ATheir%2520flawed%2520linear%2520and%2520nonlinear%2520feature%2520extraction%2520models%2520and%2520shallow-level%250Adecomposition%2520limit%2520their%2520adaptation%2520to%2520the%2520diverse%2520patterns%2520present%2520in%250Areal-world%2520scenarios.%2520Given%2520this%252C%2520we%2520innovate%2520Recursive%2520Residual%2520Decomposition%250Aby%2520introducing%2520explicit%2520extraction%2520of%2520both%2520linear%2520and%2520nonlinear%2520patterns.%2520This%250Adeeper-level%2520decomposition%2520framework%252C%2520which%2520is%2520named%2520LiNo%252C%2520captures%2520linear%250Apatterns%2520using%2520a%2520Li%2520block%2520which%2520can%2520be%2520a%2520moving%2520average%2520kernel%252C%2520and%2520models%250Anonlinear%2520patterns%2520using%2520a%2520No%2520block%2520which%2520can%2520be%2520a%2520Transformer%2520encoder.%2520The%250Aextraction%2520of%2520these%2520two%2520patterns%2520is%2520performed%2520alternatively%2520and%2520recursively.%2520To%250Aachieve%2520the%2520full%2520potential%2520of%2520LiNo%252C%2520we%2520develop%2520the%2520current%2520simple%2520linear%250Apattern%2520extractor%2520to%2520a%2520general%2520learnable%2520autoregressive%2520model%252C%2520and%2520design%2520a%250Anovel%2520No%2520block%2520that%2520can%2520handle%2520all%2520essential%2520nonlinear%2520patterns.%2520Remarkably%252C%250Athe%2520proposed%2520LiNo%2520achieves%2520state-of-the-art%2520on%2520thirteen%2520real-world%2520benchmarks%250Aunder%2520univariate%2520and%2520multivariate%2520forecasting%2520scenarios.%2520Experiments%2520show%2520that%250Acurrent%2520forecasting%2520models%2520can%2520deliver%2520more%2520robust%2520and%2520precise%2520results%2520through%250Athis%2520advanced%2520Recursive%2520Residual%2520Decomposition.%2520We%2520hope%2520this%2520work%2520could%2520offer%250Ainsight%2520into%2520designing%2520more%2520effective%2520forecasting%2520models.%2520Code%2520is%2520available%2520at%250Athis%2520Repository%253A%2520https%253A//github.com/Levi-Ackman/LiNo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiNo%3A%20Advancing%20Recursive%20Residual%20Decomposition%20of%20Linear%20and%20Nonlinear%0A%20%20Patterns%20for%20Robust%20Time%20Series%20Forecasting&entry.906535625=Guoqi%20Yu%20and%20Yaoming%20Li%20and%20Xiaoyu%20Guo%20and%20Dayu%20Wang%20and%20Zirui%20Liu%20and%20Shujun%20Wang%20and%20Tong%20Yang&entry.1292438233=%20%20Forecasting%20models%20are%20pivotal%20in%20a%20data-driven%20world%20with%20vast%20volumes%20of%0Atime%20series%20data%20that%20appear%20as%20a%20compound%20of%20vast%20Linear%20and%20Nonlinear%0Apatterns.%20Recent%20deep%20time%20series%20forecasting%20models%20struggle%20to%20utilize%0Aseasonal%20and%20trend%20decomposition%20to%20separate%20the%20entangled%20components.%20Such%20a%0Astrategy%20only%20explicitly%20extracts%20simple%20linear%20patterns%20like%20trends%2C%20leaving%0Athe%20other%20linear%20modes%20and%20vast%20unexplored%20nonlinear%20patterns%20to%20the%20residual.%0ATheir%20flawed%20linear%20and%20nonlinear%20feature%20extraction%20models%20and%20shallow-level%0Adecomposition%20limit%20their%20adaptation%20to%20the%20diverse%20patterns%20present%20in%0Areal-world%20scenarios.%20Given%20this%2C%20we%20innovate%20Recursive%20Residual%20Decomposition%0Aby%20introducing%20explicit%20extraction%20of%20both%20linear%20and%20nonlinear%20patterns.%20This%0Adeeper-level%20decomposition%20framework%2C%20which%20is%20named%20LiNo%2C%20captures%20linear%0Apatterns%20using%20a%20Li%20block%20which%20can%20be%20a%20moving%20average%20kernel%2C%20and%20models%0Anonlinear%20patterns%20using%20a%20No%20block%20which%20can%20be%20a%20Transformer%20encoder.%20The%0Aextraction%20of%20these%20two%20patterns%20is%20performed%20alternatively%20and%20recursively.%20To%0Aachieve%20the%20full%20potential%20of%20LiNo%2C%20we%20develop%20the%20current%20simple%20linear%0Apattern%20extractor%20to%20a%20general%20learnable%20autoregressive%20model%2C%20and%20design%20a%0Anovel%20No%20block%20that%20can%20handle%20all%20essential%20nonlinear%20patterns.%20Remarkably%2C%0Athe%20proposed%20LiNo%20achieves%20state-of-the-art%20on%20thirteen%20real-world%20benchmarks%0Aunder%20univariate%20and%20multivariate%20forecasting%20scenarios.%20Experiments%20show%20that%0Acurrent%20forecasting%20models%20can%20deliver%20more%20robust%20and%20precise%20results%20through%0Athis%20advanced%20Recursive%20Residual%20Decomposition.%20We%20hope%20this%20work%20could%20offer%0Ainsight%20into%20designing%20more%20effective%20forecasting%20models.%20Code%20is%20available%20at%0Athis%20Repository%3A%20https%3A//github.com/Levi-Ackman/LiNo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17159v1&entry.124074799=Read"},
{"title": "Do LLMs \"know\" internally when they follow instructions?", "author": "Juyeon Heo and Christina Heinze-Deml and Oussama Elachqar and Shirley Ren and Udhay Nallasamy and Andy Miller and Kwan Ho Ryan Chan and Jaya Narain", "abstract": "  Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.\n", "link": "http://arxiv.org/abs/2410.14516v2", "date": "2024-10-22", "relevancy": 2.3755, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20%22know%22%20internally%20when%20they%20follow%20instructions%3F&body=Title%3A%20Do%20LLMs%20%22know%22%20internally%20when%20they%20follow%20instructions%3F%0AAuthor%3A%20Juyeon%20Heo%20and%20Christina%20Heinze-Deml%20and%20Oussama%20Elachqar%20and%20Shirley%20Ren%20and%20Udhay%20Nallasamy%20and%20Andy%20Miller%20and%20Kwan%20Ho%20Ryan%20Chan%20and%20Jaya%20Narain%0AAbstract%3A%20%20%20Instruction-following%20is%20crucial%20for%20building%20AI%20agents%20with%20large%20language%0Amodels%20%28LLMs%29%2C%20as%20these%20models%20must%20adhere%20strictly%20to%20user-provided%0Aconstraints%20and%20guidelines.%20However%2C%20LLMs%20often%20fail%20to%20follow%20even%20simple%20and%0Aclear%20instructions.%20To%20improve%20instruction-following%20behavior%20and%20prevent%0Aundesirable%20outputs%2C%20a%20deeper%20understanding%20of%20how%20LLMs%27%20internal%20states%20relate%0Ato%20these%20outcomes%20is%20required.%20Our%20analysis%20of%20LLM%20internal%20states%20reveal%20a%0Adimension%20in%20the%20input%20embedding%20space%20linked%20to%20successful%0Ainstruction-following.%20We%20demonstrate%20that%20modifying%20representations%20along%20this%0Adimension%20improves%20instruction-following%20success%20rates%20compared%20to%20random%0Achanges%2C%20without%20compromising%20response%20quality.%20Further%20investigation%20reveals%0Athat%20this%20dimension%20is%20more%20closely%20related%20to%20the%20phrasing%20of%20prompts%20rather%0Athan%20the%20inherent%20difficulty%20of%20the%20task%20or%20instructions.%20This%20discovery%20also%0Asuggests%20explanations%20for%20why%20LLMs%20sometimes%20fail%20to%20follow%20clear%20instructions%0Aand%20why%20prompt%20engineering%20is%20often%20effective%2C%20even%20when%20the%20content%20remains%0Alargely%20unchanged.%20This%20work%20provides%20insight%20into%20the%20internal%20workings%20of%0ALLMs%27%20instruction-following%2C%20paving%20the%20way%20for%20reliable%20LLM%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520%2522know%2522%2520internally%2520when%2520they%2520follow%2520instructions%253F%26entry.906535625%3DJuyeon%2520Heo%2520and%2520Christina%2520Heinze-Deml%2520and%2520Oussama%2520Elachqar%2520and%2520Shirley%2520Ren%2520and%2520Udhay%2520Nallasamy%2520and%2520Andy%2520Miller%2520and%2520Kwan%2520Ho%2520Ryan%2520Chan%2520and%2520Jaya%2520Narain%26entry.1292438233%3D%2520%2520Instruction-following%2520is%2520crucial%2520for%2520building%2520AI%2520agents%2520with%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520as%2520these%2520models%2520must%2520adhere%2520strictly%2520to%2520user-provided%250Aconstraints%2520and%2520guidelines.%2520However%252C%2520LLMs%2520often%2520fail%2520to%2520follow%2520even%2520simple%2520and%250Aclear%2520instructions.%2520To%2520improve%2520instruction-following%2520behavior%2520and%2520prevent%250Aundesirable%2520outputs%252C%2520a%2520deeper%2520understanding%2520of%2520how%2520LLMs%2527%2520internal%2520states%2520relate%250Ato%2520these%2520outcomes%2520is%2520required.%2520Our%2520analysis%2520of%2520LLM%2520internal%2520states%2520reveal%2520a%250Adimension%2520in%2520the%2520input%2520embedding%2520space%2520linked%2520to%2520successful%250Ainstruction-following.%2520We%2520demonstrate%2520that%2520modifying%2520representations%2520along%2520this%250Adimension%2520improves%2520instruction-following%2520success%2520rates%2520compared%2520to%2520random%250Achanges%252C%2520without%2520compromising%2520response%2520quality.%2520Further%2520investigation%2520reveals%250Athat%2520this%2520dimension%2520is%2520more%2520closely%2520related%2520to%2520the%2520phrasing%2520of%2520prompts%2520rather%250Athan%2520the%2520inherent%2520difficulty%2520of%2520the%2520task%2520or%2520instructions.%2520This%2520discovery%2520also%250Asuggests%2520explanations%2520for%2520why%2520LLMs%2520sometimes%2520fail%2520to%2520follow%2520clear%2520instructions%250Aand%2520why%2520prompt%2520engineering%2520is%2520often%2520effective%252C%2520even%2520when%2520the%2520content%2520remains%250Alargely%2520unchanged.%2520This%2520work%2520provides%2520insight%2520into%2520the%2520internal%2520workings%2520of%250ALLMs%2527%2520instruction-following%252C%2520paving%2520the%2520way%2520for%2520reliable%2520LLM%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20%22know%22%20internally%20when%20they%20follow%20instructions%3F&entry.906535625=Juyeon%20Heo%20and%20Christina%20Heinze-Deml%20and%20Oussama%20Elachqar%20and%20Shirley%20Ren%20and%20Udhay%20Nallasamy%20and%20Andy%20Miller%20and%20Kwan%20Ho%20Ryan%20Chan%20and%20Jaya%20Narain&entry.1292438233=%20%20Instruction-following%20is%20crucial%20for%20building%20AI%20agents%20with%20large%20language%0Amodels%20%28LLMs%29%2C%20as%20these%20models%20must%20adhere%20strictly%20to%20user-provided%0Aconstraints%20and%20guidelines.%20However%2C%20LLMs%20often%20fail%20to%20follow%20even%20simple%20and%0Aclear%20instructions.%20To%20improve%20instruction-following%20behavior%20and%20prevent%0Aundesirable%20outputs%2C%20a%20deeper%20understanding%20of%20how%20LLMs%27%20internal%20states%20relate%0Ato%20these%20outcomes%20is%20required.%20Our%20analysis%20of%20LLM%20internal%20states%20reveal%20a%0Adimension%20in%20the%20input%20embedding%20space%20linked%20to%20successful%0Ainstruction-following.%20We%20demonstrate%20that%20modifying%20representations%20along%20this%0Adimension%20improves%20instruction-following%20success%20rates%20compared%20to%20random%0Achanges%2C%20without%20compromising%20response%20quality.%20Further%20investigation%20reveals%0Athat%20this%20dimension%20is%20more%20closely%20related%20to%20the%20phrasing%20of%20prompts%20rather%0Athan%20the%20inherent%20difficulty%20of%20the%20task%20or%20instructions.%20This%20discovery%20also%0Asuggests%20explanations%20for%20why%20LLMs%20sometimes%20fail%20to%20follow%20clear%20instructions%0Aand%20why%20prompt%20engineering%20is%20often%20effective%2C%20even%20when%20the%20content%20remains%0Alargely%20unchanged.%20This%20work%20provides%20insight%20into%20the%20internal%20workings%20of%0ALLMs%27%20instruction-following%2C%20paving%20the%20way%20for%20reliable%20LLM%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14516v2&entry.124074799=Read"},
{"title": "Higher-Order Message Passing for Glycan Representation Learning", "author": "Roman Joeres and Daniel Bojar", "abstract": "  Glycans are the most complex biological sequence, with monosaccharides\nforming extended, non-linear sequences. As post-translational modifications,\nthey modulate protein structure, function, and interactions. Due to their\ndiversity and complexity, predictive models of glycan properties and functions\nare still insufficient. Graph Neural Networks (GNNs) are deep learning models\ndesigned to process and analyze graph-structured data. These architectures\nleverage the connectivity and relational information in graphs to learn\neffective representations of nodes, edges, and entire graphs. Iteratively\naggregating information from neighboring nodes, GNNs capture complex patterns\nwithin graph data, making them particularly well-suited for tasks such as link\nprediction or graph classification across domains. This work presents a new\nmodel architecture based on combinatorial complexes and higher-order message\npassing to extract features from glycan structures into a latent space\nrepresentation. The architecture is evaluated on an improved GlycanML benchmark\nsuite, establishing a new state-of-the-art performance. We envision that these\nimprovements will spur further advances in computational glycosciences and\nreveal the roles of glycans in biology.\n", "link": "http://arxiv.org/abs/2409.13467v2", "date": "2024-10-22", "relevancy": 2.3737, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4857}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4712}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Message%20Passing%20for%20Glycan%20Representation%20Learning&body=Title%3A%20Higher-Order%20Message%20Passing%20for%20Glycan%20Representation%20Learning%0AAuthor%3A%20Roman%20Joeres%20and%20Daniel%20Bojar%0AAbstract%3A%20%20%20Glycans%20are%20the%20most%20complex%20biological%20sequence%2C%20with%20monosaccharides%0Aforming%20extended%2C%20non-linear%20sequences.%20As%20post-translational%20modifications%2C%0Athey%20modulate%20protein%20structure%2C%20function%2C%20and%20interactions.%20Due%20to%20their%0Adiversity%20and%20complexity%2C%20predictive%20models%20of%20glycan%20properties%20and%20functions%0Aare%20still%20insufficient.%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20deep%20learning%20models%0Adesigned%20to%20process%20and%20analyze%20graph-structured%20data.%20These%20architectures%0Aleverage%20the%20connectivity%20and%20relational%20information%20in%20graphs%20to%20learn%0Aeffective%20representations%20of%20nodes%2C%20edges%2C%20and%20entire%20graphs.%20Iteratively%0Aaggregating%20information%20from%20neighboring%20nodes%2C%20GNNs%20capture%20complex%20patterns%0Awithin%20graph%20data%2C%20making%20them%20particularly%20well-suited%20for%20tasks%20such%20as%20link%0Aprediction%20or%20graph%20classification%20across%20domains.%20This%20work%20presents%20a%20new%0Amodel%20architecture%20based%20on%20combinatorial%20complexes%20and%20higher-order%20message%0Apassing%20to%20extract%20features%20from%20glycan%20structures%20into%20a%20latent%20space%0Arepresentation.%20The%20architecture%20is%20evaluated%20on%20an%20improved%20GlycanML%20benchmark%0Asuite%2C%20establishing%20a%20new%20state-of-the-art%20performance.%20We%20envision%20that%20these%0Aimprovements%20will%20spur%20further%20advances%20in%20computational%20glycosciences%20and%0Areveal%20the%20roles%20of%20glycans%20in%20biology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Message%2520Passing%2520for%2520Glycan%2520Representation%2520Learning%26entry.906535625%3DRoman%2520Joeres%2520and%2520Daniel%2520Bojar%26entry.1292438233%3D%2520%2520Glycans%2520are%2520the%2520most%2520complex%2520biological%2520sequence%252C%2520with%2520monosaccharides%250Aforming%2520extended%252C%2520non-linear%2520sequences.%2520As%2520post-translational%2520modifications%252C%250Athey%2520modulate%2520protein%2520structure%252C%2520function%252C%2520and%2520interactions.%2520Due%2520to%2520their%250Adiversity%2520and%2520complexity%252C%2520predictive%2520models%2520of%2520glycan%2520properties%2520and%2520functions%250Aare%2520still%2520insufficient.%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520deep%2520learning%2520models%250Adesigned%2520to%2520process%2520and%2520analyze%2520graph-structured%2520data.%2520These%2520architectures%250Aleverage%2520the%2520connectivity%2520and%2520relational%2520information%2520in%2520graphs%2520to%2520learn%250Aeffective%2520representations%2520of%2520nodes%252C%2520edges%252C%2520and%2520entire%2520graphs.%2520Iteratively%250Aaggregating%2520information%2520from%2520neighboring%2520nodes%252C%2520GNNs%2520capture%2520complex%2520patterns%250Awithin%2520graph%2520data%252C%2520making%2520them%2520particularly%2520well-suited%2520for%2520tasks%2520such%2520as%2520link%250Aprediction%2520or%2520graph%2520classification%2520across%2520domains.%2520This%2520work%2520presents%2520a%2520new%250Amodel%2520architecture%2520based%2520on%2520combinatorial%2520complexes%2520and%2520higher-order%2520message%250Apassing%2520to%2520extract%2520features%2520from%2520glycan%2520structures%2520into%2520a%2520latent%2520space%250Arepresentation.%2520The%2520architecture%2520is%2520evaluated%2520on%2520an%2520improved%2520GlycanML%2520benchmark%250Asuite%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520performance.%2520We%2520envision%2520that%2520these%250Aimprovements%2520will%2520spur%2520further%2520advances%2520in%2520computational%2520glycosciences%2520and%250Areveal%2520the%2520roles%2520of%2520glycans%2520in%2520biology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Message%20Passing%20for%20Glycan%20Representation%20Learning&entry.906535625=Roman%20Joeres%20and%20Daniel%20Bojar&entry.1292438233=%20%20Glycans%20are%20the%20most%20complex%20biological%20sequence%2C%20with%20monosaccharides%0Aforming%20extended%2C%20non-linear%20sequences.%20As%20post-translational%20modifications%2C%0Athey%20modulate%20protein%20structure%2C%20function%2C%20and%20interactions.%20Due%20to%20their%0Adiversity%20and%20complexity%2C%20predictive%20models%20of%20glycan%20properties%20and%20functions%0Aare%20still%20insufficient.%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20deep%20learning%20models%0Adesigned%20to%20process%20and%20analyze%20graph-structured%20data.%20These%20architectures%0Aleverage%20the%20connectivity%20and%20relational%20information%20in%20graphs%20to%20learn%0Aeffective%20representations%20of%20nodes%2C%20edges%2C%20and%20entire%20graphs.%20Iteratively%0Aaggregating%20information%20from%20neighboring%20nodes%2C%20GNNs%20capture%20complex%20patterns%0Awithin%20graph%20data%2C%20making%20them%20particularly%20well-suited%20for%20tasks%20such%20as%20link%0Aprediction%20or%20graph%20classification%20across%20domains.%20This%20work%20presents%20a%20new%0Amodel%20architecture%20based%20on%20combinatorial%20complexes%20and%20higher-order%20message%0Apassing%20to%20extract%20features%20from%20glycan%20structures%20into%20a%20latent%20space%0Arepresentation.%20The%20architecture%20is%20evaluated%20on%20an%20improved%20GlycanML%20benchmark%0Asuite%2C%20establishing%20a%20new%20state-of-the-art%20performance.%20We%20envision%20that%20these%0Aimprovements%20will%20spur%20further%20advances%20in%20computational%20glycosciences%20and%0Areveal%20the%20roles%20of%20glycans%20in%20biology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13467v2&entry.124074799=Read"},
{"title": "Online Tensor Learning: Computational and Statistical Trade-offs,\n  Adaptivity and Optimal Regret", "author": "Jingyang Li and Jian-Feng Cai and Yang Chen and Dong Xia", "abstract": "  Large tensor learning algorithms are typically computationally expensive and\nrequire storing a vast amount of data. In this paper, we propose a unified\nonline Riemannian gradient descent (oRGrad) algorithm for tensor learning,\nwhich is computationally efficient, consumes much less memory, and can handle\nsequentially arriving data while making timely predictions. The algorithm is\napplicable to both linear and generalized linear models. If the time horizon T\nis known, oRGrad achieves statistical optimality by choosing an appropriate\nfixed step size. We find that noisy tensor completion particularly benefits\nfrom online algorithms by avoiding the trimming procedure and ensuring sharp\nentry-wise statistical error, which is often technically challenging for\noffline methods. The regret of oRGrad is analyzed, revealing a fascinating\ntrilemma concerning the computational convergence rate, statistical error, and\nregret bound. By selecting an appropriate constant step size, oRGrad achieves\nan $O(T^{1/2})$ regret. We then introduce the adaptive-oRGrad algorithm, which\ncan achieve the optimal $O(\\log T)$ regret by adaptively selecting step sizes,\nregardless of whether the time horizon is known. The adaptive-oRGrad algorithm\ncan attain a statistically optimal error rate without knowing the horizon.\nComprehensive numerical simulations corroborate our theoretical findings. We\nshow that oRGrad significantly outperforms its offline counterpart in\npredicting the solar F10.7 index with tensor predictors that monitor space\nweather impacts.\n", "link": "http://arxiv.org/abs/2306.03372v3", "date": "2024-10-22", "relevancy": 2.3545, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4761}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Tensor%20Learning%3A%20Computational%20and%20Statistical%20Trade-offs%2C%0A%20%20Adaptivity%20and%20Optimal%20Regret&body=Title%3A%20Online%20Tensor%20Learning%3A%20Computational%20and%20Statistical%20Trade-offs%2C%0A%20%20Adaptivity%20and%20Optimal%20Regret%0AAuthor%3A%20Jingyang%20Li%20and%20Jian-Feng%20Cai%20and%20Yang%20Chen%20and%20Dong%20Xia%0AAbstract%3A%20%20%20Large%20tensor%20learning%20algorithms%20are%20typically%20computationally%20expensive%20and%0Arequire%20storing%20a%20vast%20amount%20of%20data.%20In%20this%20paper%2C%20we%20propose%20a%20unified%0Aonline%20Riemannian%20gradient%20descent%20%28oRGrad%29%20algorithm%20for%20tensor%20learning%2C%0Awhich%20is%20computationally%20efficient%2C%20consumes%20much%20less%20memory%2C%20and%20can%20handle%0Asequentially%20arriving%20data%20while%20making%20timely%20predictions.%20The%20algorithm%20is%0Aapplicable%20to%20both%20linear%20and%20generalized%20linear%20models.%20If%20the%20time%20horizon%20T%0Ais%20known%2C%20oRGrad%20achieves%20statistical%20optimality%20by%20choosing%20an%20appropriate%0Afixed%20step%20size.%20We%20find%20that%20noisy%20tensor%20completion%20particularly%20benefits%0Afrom%20online%20algorithms%20by%20avoiding%20the%20trimming%20procedure%20and%20ensuring%20sharp%0Aentry-wise%20statistical%20error%2C%20which%20is%20often%20technically%20challenging%20for%0Aoffline%20methods.%20The%20regret%20of%20oRGrad%20is%20analyzed%2C%20revealing%20a%20fascinating%0Atrilemma%20concerning%20the%20computational%20convergence%20rate%2C%20statistical%20error%2C%20and%0Aregret%20bound.%20By%20selecting%20an%20appropriate%20constant%20step%20size%2C%20oRGrad%20achieves%0Aan%20%24O%28T%5E%7B1/2%7D%29%24%20regret.%20We%20then%20introduce%20the%20adaptive-oRGrad%20algorithm%2C%20which%0Acan%20achieve%20the%20optimal%20%24O%28%5Clog%20T%29%24%20regret%20by%20adaptively%20selecting%20step%20sizes%2C%0Aregardless%20of%20whether%20the%20time%20horizon%20is%20known.%20The%20adaptive-oRGrad%20algorithm%0Acan%20attain%20a%20statistically%20optimal%20error%20rate%20without%20knowing%20the%20horizon.%0AComprehensive%20numerical%20simulations%20corroborate%20our%20theoretical%20findings.%20We%0Ashow%20that%20oRGrad%20significantly%20outperforms%20its%20offline%20counterpart%20in%0Apredicting%20the%20solar%20F10.7%20index%20with%20tensor%20predictors%20that%20monitor%20space%0Aweather%20impacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.03372v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Tensor%2520Learning%253A%2520Computational%2520and%2520Statistical%2520Trade-offs%252C%250A%2520%2520Adaptivity%2520and%2520Optimal%2520Regret%26entry.906535625%3DJingyang%2520Li%2520and%2520Jian-Feng%2520Cai%2520and%2520Yang%2520Chen%2520and%2520Dong%2520Xia%26entry.1292438233%3D%2520%2520Large%2520tensor%2520learning%2520algorithms%2520are%2520typically%2520computationally%2520expensive%2520and%250Arequire%2520storing%2520a%2520vast%2520amount%2520of%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520unified%250Aonline%2520Riemannian%2520gradient%2520descent%2520%2528oRGrad%2529%2520algorithm%2520for%2520tensor%2520learning%252C%250Awhich%2520is%2520computationally%2520efficient%252C%2520consumes%2520much%2520less%2520memory%252C%2520and%2520can%2520handle%250Asequentially%2520arriving%2520data%2520while%2520making%2520timely%2520predictions.%2520The%2520algorithm%2520is%250Aapplicable%2520to%2520both%2520linear%2520and%2520generalized%2520linear%2520models.%2520If%2520the%2520time%2520horizon%2520T%250Ais%2520known%252C%2520oRGrad%2520achieves%2520statistical%2520optimality%2520by%2520choosing%2520an%2520appropriate%250Afixed%2520step%2520size.%2520We%2520find%2520that%2520noisy%2520tensor%2520completion%2520particularly%2520benefits%250Afrom%2520online%2520algorithms%2520by%2520avoiding%2520the%2520trimming%2520procedure%2520and%2520ensuring%2520sharp%250Aentry-wise%2520statistical%2520error%252C%2520which%2520is%2520often%2520technically%2520challenging%2520for%250Aoffline%2520methods.%2520The%2520regret%2520of%2520oRGrad%2520is%2520analyzed%252C%2520revealing%2520a%2520fascinating%250Atrilemma%2520concerning%2520the%2520computational%2520convergence%2520rate%252C%2520statistical%2520error%252C%2520and%250Aregret%2520bound.%2520By%2520selecting%2520an%2520appropriate%2520constant%2520step%2520size%252C%2520oRGrad%2520achieves%250Aan%2520%2524O%2528T%255E%257B1/2%257D%2529%2524%2520regret.%2520We%2520then%2520introduce%2520the%2520adaptive-oRGrad%2520algorithm%252C%2520which%250Acan%2520achieve%2520the%2520optimal%2520%2524O%2528%255Clog%2520T%2529%2524%2520regret%2520by%2520adaptively%2520selecting%2520step%2520sizes%252C%250Aregardless%2520of%2520whether%2520the%2520time%2520horizon%2520is%2520known.%2520The%2520adaptive-oRGrad%2520algorithm%250Acan%2520attain%2520a%2520statistically%2520optimal%2520error%2520rate%2520without%2520knowing%2520the%2520horizon.%250AComprehensive%2520numerical%2520simulations%2520corroborate%2520our%2520theoretical%2520findings.%2520We%250Ashow%2520that%2520oRGrad%2520significantly%2520outperforms%2520its%2520offline%2520counterpart%2520in%250Apredicting%2520the%2520solar%2520F10.7%2520index%2520with%2520tensor%2520predictors%2520that%2520monitor%2520space%250Aweather%2520impacts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.03372v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Tensor%20Learning%3A%20Computational%20and%20Statistical%20Trade-offs%2C%0A%20%20Adaptivity%20and%20Optimal%20Regret&entry.906535625=Jingyang%20Li%20and%20Jian-Feng%20Cai%20and%20Yang%20Chen%20and%20Dong%20Xia&entry.1292438233=%20%20Large%20tensor%20learning%20algorithms%20are%20typically%20computationally%20expensive%20and%0Arequire%20storing%20a%20vast%20amount%20of%20data.%20In%20this%20paper%2C%20we%20propose%20a%20unified%0Aonline%20Riemannian%20gradient%20descent%20%28oRGrad%29%20algorithm%20for%20tensor%20learning%2C%0Awhich%20is%20computationally%20efficient%2C%20consumes%20much%20less%20memory%2C%20and%20can%20handle%0Asequentially%20arriving%20data%20while%20making%20timely%20predictions.%20The%20algorithm%20is%0Aapplicable%20to%20both%20linear%20and%20generalized%20linear%20models.%20If%20the%20time%20horizon%20T%0Ais%20known%2C%20oRGrad%20achieves%20statistical%20optimality%20by%20choosing%20an%20appropriate%0Afixed%20step%20size.%20We%20find%20that%20noisy%20tensor%20completion%20particularly%20benefits%0Afrom%20online%20algorithms%20by%20avoiding%20the%20trimming%20procedure%20and%20ensuring%20sharp%0Aentry-wise%20statistical%20error%2C%20which%20is%20often%20technically%20challenging%20for%0Aoffline%20methods.%20The%20regret%20of%20oRGrad%20is%20analyzed%2C%20revealing%20a%20fascinating%0Atrilemma%20concerning%20the%20computational%20convergence%20rate%2C%20statistical%20error%2C%20and%0Aregret%20bound.%20By%20selecting%20an%20appropriate%20constant%20step%20size%2C%20oRGrad%20achieves%0Aan%20%24O%28T%5E%7B1/2%7D%29%24%20regret.%20We%20then%20introduce%20the%20adaptive-oRGrad%20algorithm%2C%20which%0Acan%20achieve%20the%20optimal%20%24O%28%5Clog%20T%29%24%20regret%20by%20adaptively%20selecting%20step%20sizes%2C%0Aregardless%20of%20whether%20the%20time%20horizon%20is%20known.%20The%20adaptive-oRGrad%20algorithm%0Acan%20attain%20a%20statistically%20optimal%20error%20rate%20without%20knowing%20the%20horizon.%0AComprehensive%20numerical%20simulations%20corroborate%20our%20theoretical%20findings.%20We%0Ashow%20that%20oRGrad%20significantly%20outperforms%20its%20offline%20counterpart%20in%0Apredicting%20the%20solar%20F10.7%20index%20with%20tensor%20predictors%20that%20monitor%20space%0Aweather%20impacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.03372v3&entry.124074799=Read"},
{"title": "On high-dimensional modifications of the nearest neighbor classifier", "author": "Annesha Ghosh and Bilol Banerjee and Anil K. Ghosh", "abstract": "  Nearest neighbor classifier is arguably the most simple and popular\nnonparametric classifier available in the literature. However, due to the\nconcentration of pairwise distances and the violation of the neighborhood\nstructure, this classifier often suffers in high-dimension, low-sample size\n(HDLSS) situations, especially when the scale difference between the competing\nclasses dominates their location difference. Several attempts have been made in\nthe literature to take care of this problem. In this article, we discuss some\nof these existing methods and propose some new ones. We carry out some\ntheoretical investigations in this regard and analyze several simulated and\nbenchmark datasets to compare the empirical performances of proposed methods\nwith some of the existing ones.\n", "link": "http://arxiv.org/abs/2407.05145v2", "date": "2024-10-22", "relevancy": 2.3433, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4718}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4677}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20high-dimensional%20modifications%20of%20the%20nearest%20neighbor%20classifier&body=Title%3A%20On%20high-dimensional%20modifications%20of%20the%20nearest%20neighbor%20classifier%0AAuthor%3A%20Annesha%20Ghosh%20and%20Bilol%20Banerjee%20and%20Anil%20K.%20Ghosh%0AAbstract%3A%20%20%20Nearest%20neighbor%20classifier%20is%20arguably%20the%20most%20simple%20and%20popular%0Anonparametric%20classifier%20available%20in%20the%20literature.%20However%2C%20due%20to%20the%0Aconcentration%20of%20pairwise%20distances%20and%20the%20violation%20of%20the%20neighborhood%0Astructure%2C%20this%20classifier%20often%20suffers%20in%20high-dimension%2C%20low-sample%20size%0A%28HDLSS%29%20situations%2C%20especially%20when%20the%20scale%20difference%20between%20the%20competing%0Aclasses%20dominates%20their%20location%20difference.%20Several%20attempts%20have%20been%20made%20in%0Athe%20literature%20to%20take%20care%20of%20this%20problem.%20In%20this%20article%2C%20we%20discuss%20some%0Aof%20these%20existing%20methods%20and%20propose%20some%20new%20ones.%20We%20carry%20out%20some%0Atheoretical%20investigations%20in%20this%20regard%20and%20analyze%20several%20simulated%20and%0Abenchmark%20datasets%20to%20compare%20the%20empirical%20performances%20of%20proposed%20methods%0Awith%20some%20of%20the%20existing%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05145v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520high-dimensional%2520modifications%2520of%2520the%2520nearest%2520neighbor%2520classifier%26entry.906535625%3DAnnesha%2520Ghosh%2520and%2520Bilol%2520Banerjee%2520and%2520Anil%2520K.%2520Ghosh%26entry.1292438233%3D%2520%2520Nearest%2520neighbor%2520classifier%2520is%2520arguably%2520the%2520most%2520simple%2520and%2520popular%250Anonparametric%2520classifier%2520available%2520in%2520the%2520literature.%2520However%252C%2520due%2520to%2520the%250Aconcentration%2520of%2520pairwise%2520distances%2520and%2520the%2520violation%2520of%2520the%2520neighborhood%250Astructure%252C%2520this%2520classifier%2520often%2520suffers%2520in%2520high-dimension%252C%2520low-sample%2520size%250A%2528HDLSS%2529%2520situations%252C%2520especially%2520when%2520the%2520scale%2520difference%2520between%2520the%2520competing%250Aclasses%2520dominates%2520their%2520location%2520difference.%2520Several%2520attempts%2520have%2520been%2520made%2520in%250Athe%2520literature%2520to%2520take%2520care%2520of%2520this%2520problem.%2520In%2520this%2520article%252C%2520we%2520discuss%2520some%250Aof%2520these%2520existing%2520methods%2520and%2520propose%2520some%2520new%2520ones.%2520We%2520carry%2520out%2520some%250Atheoretical%2520investigations%2520in%2520this%2520regard%2520and%2520analyze%2520several%2520simulated%2520and%250Abenchmark%2520datasets%2520to%2520compare%2520the%2520empirical%2520performances%2520of%2520proposed%2520methods%250Awith%2520some%2520of%2520the%2520existing%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05145v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20high-dimensional%20modifications%20of%20the%20nearest%20neighbor%20classifier&entry.906535625=Annesha%20Ghosh%20and%20Bilol%20Banerjee%20and%20Anil%20K.%20Ghosh&entry.1292438233=%20%20Nearest%20neighbor%20classifier%20is%20arguably%20the%20most%20simple%20and%20popular%0Anonparametric%20classifier%20available%20in%20the%20literature.%20However%2C%20due%20to%20the%0Aconcentration%20of%20pairwise%20distances%20and%20the%20violation%20of%20the%20neighborhood%0Astructure%2C%20this%20classifier%20often%20suffers%20in%20high-dimension%2C%20low-sample%20size%0A%28HDLSS%29%20situations%2C%20especially%20when%20the%20scale%20difference%20between%20the%20competing%0Aclasses%20dominates%20their%20location%20difference.%20Several%20attempts%20have%20been%20made%20in%0Athe%20literature%20to%20take%20care%20of%20this%20problem.%20In%20this%20article%2C%20we%20discuss%20some%0Aof%20these%20existing%20methods%20and%20propose%20some%20new%20ones.%20We%20carry%20out%20some%0Atheoretical%20investigations%20in%20this%20regard%20and%20analyze%20several%20simulated%20and%0Abenchmark%20datasets%20to%20compare%20the%20empirical%20performances%20of%20proposed%20methods%0Awith%20some%20of%20the%20existing%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05145v2&entry.124074799=Read"},
{"title": "Reinforcement Learning for Data-Driven Workflows in Radio\n  Interferometry. I. Principal Demonstration in Calibration", "author": "Brian M. Kirk and Urvashi Rau and Ramyaa Ramyaa", "abstract": "  Radio interferometry is an observational technique used to study\nastrophysical phenomena. Data gathered by an interferometer requires\nsubstantial processing before astronomers can extract the scientific\ninformation from it. Data processing consists of a sequence of calibration and\nanalysis procedures where choices must be made about the sequence of procedures\nas well as the specific configuration of the procedure itself. These choices\nare typically based on a combination of measurable data characteristics, an\nunderstanding of the instrument itself, an appreciation of the trade-offs\nbetween compute cost and accuracy, and a learned understanding of what is\nconsidered \"best practice\". A metric of absolute correctness is not always\navailable and validity is often subject to human judgment. The underlying\nprinciples and software configurations to discern a reasonable workflow for a\ngiven dataset is the subject of training workshops for students and scientists.\nOur goal is to use objective metrics that quantify best practice, and\nnumerically map out the decision space with respect to our metrics. With these\nobjective metrics we demonstrate an automated, data-driven, decision system\nthat is capable of sequencing the optimal action(s) for processing\ninterferometric data. This paper introduces a simplified description of the\nprinciples behind interferometry and the procedures required for data\nprocessing. We highlight the issues with current automation approaches and\npropose our ideas for solving these bottlenecks. A prototype is demonstrated\nand the results are discussed.\n", "link": "http://arxiv.org/abs/2410.17135v1", "date": "2024-10-22", "relevancy": 2.3235, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4687}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4661}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20for%20Data-Driven%20Workflows%20in%20Radio%0A%20%20Interferometry.%20I.%20Principal%20Demonstration%20in%20Calibration&body=Title%3A%20Reinforcement%20Learning%20for%20Data-Driven%20Workflows%20in%20Radio%0A%20%20Interferometry.%20I.%20Principal%20Demonstration%20in%20Calibration%0AAuthor%3A%20Brian%20M.%20Kirk%20and%20Urvashi%20Rau%20and%20Ramyaa%20Ramyaa%0AAbstract%3A%20%20%20Radio%20interferometry%20is%20an%20observational%20technique%20used%20to%20study%0Aastrophysical%20phenomena.%20Data%20gathered%20by%20an%20interferometer%20requires%0Asubstantial%20processing%20before%20astronomers%20can%20extract%20the%20scientific%0Ainformation%20from%20it.%20Data%20processing%20consists%20of%20a%20sequence%20of%20calibration%20and%0Aanalysis%20procedures%20where%20choices%20must%20be%20made%20about%20the%20sequence%20of%20procedures%0Aas%20well%20as%20the%20specific%20configuration%20of%20the%20procedure%20itself.%20These%20choices%0Aare%20typically%20based%20on%20a%20combination%20of%20measurable%20data%20characteristics%2C%20an%0Aunderstanding%20of%20the%20instrument%20itself%2C%20an%20appreciation%20of%20the%20trade-offs%0Abetween%20compute%20cost%20and%20accuracy%2C%20and%20a%20learned%20understanding%20of%20what%20is%0Aconsidered%20%22best%20practice%22.%20A%20metric%20of%20absolute%20correctness%20is%20not%20always%0Aavailable%20and%20validity%20is%20often%20subject%20to%20human%20judgment.%20The%20underlying%0Aprinciples%20and%20software%20configurations%20to%20discern%20a%20reasonable%20workflow%20for%20a%0Agiven%20dataset%20is%20the%20subject%20of%20training%20workshops%20for%20students%20and%20scientists.%0AOur%20goal%20is%20to%20use%20objective%20metrics%20that%20quantify%20best%20practice%2C%20and%0Anumerically%20map%20out%20the%20decision%20space%20with%20respect%20to%20our%20metrics.%20With%20these%0Aobjective%20metrics%20we%20demonstrate%20an%20automated%2C%20data-driven%2C%20decision%20system%0Athat%20is%20capable%20of%20sequencing%20the%20optimal%20action%28s%29%20for%20processing%0Ainterferometric%20data.%20This%20paper%20introduces%20a%20simplified%20description%20of%20the%0Aprinciples%20behind%20interferometry%20and%20the%20procedures%20required%20for%20data%0Aprocessing.%20We%20highlight%20the%20issues%20with%20current%20automation%20approaches%20and%0Apropose%20our%20ideas%20for%20solving%20these%20bottlenecks.%20A%20prototype%20is%20demonstrated%0Aand%20the%20results%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520for%2520Data-Driven%2520Workflows%2520in%2520Radio%250A%2520%2520Interferometry.%2520I.%2520Principal%2520Demonstration%2520in%2520Calibration%26entry.906535625%3DBrian%2520M.%2520Kirk%2520and%2520Urvashi%2520Rau%2520and%2520Ramyaa%2520Ramyaa%26entry.1292438233%3D%2520%2520Radio%2520interferometry%2520is%2520an%2520observational%2520technique%2520used%2520to%2520study%250Aastrophysical%2520phenomena.%2520Data%2520gathered%2520by%2520an%2520interferometer%2520requires%250Asubstantial%2520processing%2520before%2520astronomers%2520can%2520extract%2520the%2520scientific%250Ainformation%2520from%2520it.%2520Data%2520processing%2520consists%2520of%2520a%2520sequence%2520of%2520calibration%2520and%250Aanalysis%2520procedures%2520where%2520choices%2520must%2520be%2520made%2520about%2520the%2520sequence%2520of%2520procedures%250Aas%2520well%2520as%2520the%2520specific%2520configuration%2520of%2520the%2520procedure%2520itself.%2520These%2520choices%250Aare%2520typically%2520based%2520on%2520a%2520combination%2520of%2520measurable%2520data%2520characteristics%252C%2520an%250Aunderstanding%2520of%2520the%2520instrument%2520itself%252C%2520an%2520appreciation%2520of%2520the%2520trade-offs%250Abetween%2520compute%2520cost%2520and%2520accuracy%252C%2520and%2520a%2520learned%2520understanding%2520of%2520what%2520is%250Aconsidered%2520%2522best%2520practice%2522.%2520A%2520metric%2520of%2520absolute%2520correctness%2520is%2520not%2520always%250Aavailable%2520and%2520validity%2520is%2520often%2520subject%2520to%2520human%2520judgment.%2520The%2520underlying%250Aprinciples%2520and%2520software%2520configurations%2520to%2520discern%2520a%2520reasonable%2520workflow%2520for%2520a%250Agiven%2520dataset%2520is%2520the%2520subject%2520of%2520training%2520workshops%2520for%2520students%2520and%2520scientists.%250AOur%2520goal%2520is%2520to%2520use%2520objective%2520metrics%2520that%2520quantify%2520best%2520practice%252C%2520and%250Anumerically%2520map%2520out%2520the%2520decision%2520space%2520with%2520respect%2520to%2520our%2520metrics.%2520With%2520these%250Aobjective%2520metrics%2520we%2520demonstrate%2520an%2520automated%252C%2520data-driven%252C%2520decision%2520system%250Athat%2520is%2520capable%2520of%2520sequencing%2520the%2520optimal%2520action%2528s%2529%2520for%2520processing%250Ainterferometric%2520data.%2520This%2520paper%2520introduces%2520a%2520simplified%2520description%2520of%2520the%250Aprinciples%2520behind%2520interferometry%2520and%2520the%2520procedures%2520required%2520for%2520data%250Aprocessing.%2520We%2520highlight%2520the%2520issues%2520with%2520current%2520automation%2520approaches%2520and%250Apropose%2520our%2520ideas%2520for%2520solving%2520these%2520bottlenecks.%2520A%2520prototype%2520is%2520demonstrated%250Aand%2520the%2520results%2520are%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20for%20Data-Driven%20Workflows%20in%20Radio%0A%20%20Interferometry.%20I.%20Principal%20Demonstration%20in%20Calibration&entry.906535625=Brian%20M.%20Kirk%20and%20Urvashi%20Rau%20and%20Ramyaa%20Ramyaa&entry.1292438233=%20%20Radio%20interferometry%20is%20an%20observational%20technique%20used%20to%20study%0Aastrophysical%20phenomena.%20Data%20gathered%20by%20an%20interferometer%20requires%0Asubstantial%20processing%20before%20astronomers%20can%20extract%20the%20scientific%0Ainformation%20from%20it.%20Data%20processing%20consists%20of%20a%20sequence%20of%20calibration%20and%0Aanalysis%20procedures%20where%20choices%20must%20be%20made%20about%20the%20sequence%20of%20procedures%0Aas%20well%20as%20the%20specific%20configuration%20of%20the%20procedure%20itself.%20These%20choices%0Aare%20typically%20based%20on%20a%20combination%20of%20measurable%20data%20characteristics%2C%20an%0Aunderstanding%20of%20the%20instrument%20itself%2C%20an%20appreciation%20of%20the%20trade-offs%0Abetween%20compute%20cost%20and%20accuracy%2C%20and%20a%20learned%20understanding%20of%20what%20is%0Aconsidered%20%22best%20practice%22.%20A%20metric%20of%20absolute%20correctness%20is%20not%20always%0Aavailable%20and%20validity%20is%20often%20subject%20to%20human%20judgment.%20The%20underlying%0Aprinciples%20and%20software%20configurations%20to%20discern%20a%20reasonable%20workflow%20for%20a%0Agiven%20dataset%20is%20the%20subject%20of%20training%20workshops%20for%20students%20and%20scientists.%0AOur%20goal%20is%20to%20use%20objective%20metrics%20that%20quantify%20best%20practice%2C%20and%0Anumerically%20map%20out%20the%20decision%20space%20with%20respect%20to%20our%20metrics.%20With%20these%0Aobjective%20metrics%20we%20demonstrate%20an%20automated%2C%20data-driven%2C%20decision%20system%0Athat%20is%20capable%20of%20sequencing%20the%20optimal%20action%28s%29%20for%20processing%0Ainterferometric%20data.%20This%20paper%20introduces%20a%20simplified%20description%20of%20the%0Aprinciples%20behind%20interferometry%20and%20the%20procedures%20required%20for%20data%0Aprocessing.%20We%20highlight%20the%20issues%20with%20current%20automation%20approaches%20and%0Apropose%20our%20ideas%20for%20solving%20these%20bottlenecks.%20A%20prototype%20is%20demonstrated%0Aand%20the%20results%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17135v1&entry.124074799=Read"},
{"title": "On Functional Dimension and Persistent Pseudodimension", "author": "J. Elisenda Grigsby and Kathryn Lindsey", "abstract": "  For any fixed feedforward ReLU neural network architecture, it is well-known\nthat many different parameter settings can determine the same function. It is\nless well-known that the degree of this redundancy is inhomogeneous across\nparameter space. In this work, we discuss two locally applicable complexity\nmeasures for ReLU network classes and what we know about the relationship\nbetween them: (1) the local functional dimension [14, 18], and (2) a local\nversion of VC dimension that we call persistent pseudodimension. The former is\neasy to compute on finite batches of points; the latter should give local\nbounds on the generalization gap, which would inform an understanding of the\nmechanics of the double descent phenomenon [7].\n", "link": "http://arxiv.org/abs/2410.17191v1", "date": "2024-10-22", "relevancy": 2.3034, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4645}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.46}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Functional%20Dimension%20and%20Persistent%20Pseudodimension&body=Title%3A%20On%20Functional%20Dimension%20and%20Persistent%20Pseudodimension%0AAuthor%3A%20J.%20Elisenda%20Grigsby%20and%20Kathryn%20Lindsey%0AAbstract%3A%20%20%20For%20any%20fixed%20feedforward%20ReLU%20neural%20network%20architecture%2C%20it%20is%20well-known%0Athat%20many%20different%20parameter%20settings%20can%20determine%20the%20same%20function.%20It%20is%0Aless%20well-known%20that%20the%20degree%20of%20this%20redundancy%20is%20inhomogeneous%20across%0Aparameter%20space.%20In%20this%20work%2C%20we%20discuss%20two%20locally%20applicable%20complexity%0Ameasures%20for%20ReLU%20network%20classes%20and%20what%20we%20know%20about%20the%20relationship%0Abetween%20them%3A%20%281%29%20the%20local%20functional%20dimension%20%5B14%2C%2018%5D%2C%20and%20%282%29%20a%20local%0Aversion%20of%20VC%20dimension%20that%20we%20call%20persistent%20pseudodimension.%20The%20former%20is%0Aeasy%20to%20compute%20on%20finite%20batches%20of%20points%3B%20the%20latter%20should%20give%20local%0Abounds%20on%20the%20generalization%20gap%2C%20which%20would%20inform%20an%20understanding%20of%20the%0Amechanics%20of%20the%20double%20descent%20phenomenon%20%5B7%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Functional%2520Dimension%2520and%2520Persistent%2520Pseudodimension%26entry.906535625%3DJ.%2520Elisenda%2520Grigsby%2520and%2520Kathryn%2520Lindsey%26entry.1292438233%3D%2520%2520For%2520any%2520fixed%2520feedforward%2520ReLU%2520neural%2520network%2520architecture%252C%2520it%2520is%2520well-known%250Athat%2520many%2520different%2520parameter%2520settings%2520can%2520determine%2520the%2520same%2520function.%2520It%2520is%250Aless%2520well-known%2520that%2520the%2520degree%2520of%2520this%2520redundancy%2520is%2520inhomogeneous%2520across%250Aparameter%2520space.%2520In%2520this%2520work%252C%2520we%2520discuss%2520two%2520locally%2520applicable%2520complexity%250Ameasures%2520for%2520ReLU%2520network%2520classes%2520and%2520what%2520we%2520know%2520about%2520the%2520relationship%250Abetween%2520them%253A%2520%25281%2529%2520the%2520local%2520functional%2520dimension%2520%255B14%252C%252018%255D%252C%2520and%2520%25282%2529%2520a%2520local%250Aversion%2520of%2520VC%2520dimension%2520that%2520we%2520call%2520persistent%2520pseudodimension.%2520The%2520former%2520is%250Aeasy%2520to%2520compute%2520on%2520finite%2520batches%2520of%2520points%253B%2520the%2520latter%2520should%2520give%2520local%250Abounds%2520on%2520the%2520generalization%2520gap%252C%2520which%2520would%2520inform%2520an%2520understanding%2520of%2520the%250Amechanics%2520of%2520the%2520double%2520descent%2520phenomenon%2520%255B7%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Functional%20Dimension%20and%20Persistent%20Pseudodimension&entry.906535625=J.%20Elisenda%20Grigsby%20and%20Kathryn%20Lindsey&entry.1292438233=%20%20For%20any%20fixed%20feedforward%20ReLU%20neural%20network%20architecture%2C%20it%20is%20well-known%0Athat%20many%20different%20parameter%20settings%20can%20determine%20the%20same%20function.%20It%20is%0Aless%20well-known%20that%20the%20degree%20of%20this%20redundancy%20is%20inhomogeneous%20across%0Aparameter%20space.%20In%20this%20work%2C%20we%20discuss%20two%20locally%20applicable%20complexity%0Ameasures%20for%20ReLU%20network%20classes%20and%20what%20we%20know%20about%20the%20relationship%0Abetween%20them%3A%20%281%29%20the%20local%20functional%20dimension%20%5B14%2C%2018%5D%2C%20and%20%282%29%20a%20local%0Aversion%20of%20VC%20dimension%20that%20we%20call%20persistent%20pseudodimension.%20The%20former%20is%0Aeasy%20to%20compute%20on%20finite%20batches%20of%20points%3B%20the%20latter%20should%20give%20local%0Abounds%20on%20the%20generalization%20gap%2C%20which%20would%20inform%20an%20understanding%20of%20the%0Amechanics%20of%20the%20double%20descent%20phenomenon%20%5B7%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17191v1&entry.124074799=Read"},
{"title": "Revealing Hidden Bias in AI: Lessons from Large Language Models", "author": "Django Beatty and Kritsada Masanthia and Teepakorn Kaphol and Niphan Sethi", "abstract": "  As large language models (LLMs) become integral to recruitment processes,\nconcerns about AI-induced bias have intensified. This study examines biases in\ncandidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5,\nand Llama 3.1 405B, focusing on characteristics such as gender, race, and age.\nWe evaluate the effectiveness of LLM-based anonymization in reducing these\nbiases. Findings indicate that while anonymization reduces certain biases,\nparticularly gender bias, the degree of effectiveness varies across models and\nbias types. Notably, Llama 3.1 405B exhibited the lowest overall bias.\nMoreover, our methodology of comparing anonymized and non-anonymized data\nreveals a novel approach to assessing inherent biases in LLMs beyond\nrecruitment applications. This study underscores the importance of careful LLM\nselection and suggests best practices for minimizing bias in AI applications,\npromoting fairness and inclusivity.\n", "link": "http://arxiv.org/abs/2410.16927v1", "date": "2024-10-22", "relevancy": 2.3012, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20Hidden%20Bias%20in%20AI%3A%20Lessons%20from%20Large%20Language%20Models&body=Title%3A%20Revealing%20Hidden%20Bias%20in%20AI%3A%20Lessons%20from%20Large%20Language%20Models%0AAuthor%3A%20Django%20Beatty%20and%20Kritsada%20Masanthia%20and%20Teepakorn%20Kaphol%20and%20Niphan%20Sethi%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20become%20integral%20to%20recruitment%20processes%2C%0Aconcerns%20about%20AI-induced%20bias%20have%20intensified.%20This%20study%20examines%20biases%20in%0Acandidate%20interview%20reports%20generated%20by%20Claude%203.5%20Sonnet%2C%20GPT-4o%2C%20Gemini%201.5%2C%0Aand%20Llama%203.1%20405B%2C%20focusing%20on%20characteristics%20such%20as%20gender%2C%20race%2C%20and%20age.%0AWe%20evaluate%20the%20effectiveness%20of%20LLM-based%20anonymization%20in%20reducing%20these%0Abiases.%20Findings%20indicate%20that%20while%20anonymization%20reduces%20certain%20biases%2C%0Aparticularly%20gender%20bias%2C%20the%20degree%20of%20effectiveness%20varies%20across%20models%20and%0Abias%20types.%20Notably%2C%20Llama%203.1%20405B%20exhibited%20the%20lowest%20overall%20bias.%0AMoreover%2C%20our%20methodology%20of%20comparing%20anonymized%20and%20non-anonymized%20data%0Areveals%20a%20novel%20approach%20to%20assessing%20inherent%20biases%20in%20LLMs%20beyond%0Arecruitment%20applications.%20This%20study%20underscores%20the%20importance%20of%20careful%20LLM%0Aselection%20and%20suggests%20best%20practices%20for%20minimizing%20bias%20in%20AI%20applications%2C%0Apromoting%20fairness%20and%20inclusivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520Hidden%2520Bias%2520in%2520AI%253A%2520Lessons%2520from%2520Large%2520Language%2520Models%26entry.906535625%3DDjango%2520Beatty%2520and%2520Kritsada%2520Masanthia%2520and%2520Teepakorn%2520Kaphol%2520and%2520Niphan%2520Sethi%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%2520integral%2520to%2520recruitment%2520processes%252C%250Aconcerns%2520about%2520AI-induced%2520bias%2520have%2520intensified.%2520This%2520study%2520examines%2520biases%2520in%250Acandidate%2520interview%2520reports%2520generated%2520by%2520Claude%25203.5%2520Sonnet%252C%2520GPT-4o%252C%2520Gemini%25201.5%252C%250Aand%2520Llama%25203.1%2520405B%252C%2520focusing%2520on%2520characteristics%2520such%2520as%2520gender%252C%2520race%252C%2520and%2520age.%250AWe%2520evaluate%2520the%2520effectiveness%2520of%2520LLM-based%2520anonymization%2520in%2520reducing%2520these%250Abiases.%2520Findings%2520indicate%2520that%2520while%2520anonymization%2520reduces%2520certain%2520biases%252C%250Aparticularly%2520gender%2520bias%252C%2520the%2520degree%2520of%2520effectiveness%2520varies%2520across%2520models%2520and%250Abias%2520types.%2520Notably%252C%2520Llama%25203.1%2520405B%2520exhibited%2520the%2520lowest%2520overall%2520bias.%250AMoreover%252C%2520our%2520methodology%2520of%2520comparing%2520anonymized%2520and%2520non-anonymized%2520data%250Areveals%2520a%2520novel%2520approach%2520to%2520assessing%2520inherent%2520biases%2520in%2520LLMs%2520beyond%250Arecruitment%2520applications.%2520This%2520study%2520underscores%2520the%2520importance%2520of%2520careful%2520LLM%250Aselection%2520and%2520suggests%2520best%2520practices%2520for%2520minimizing%2520bias%2520in%2520AI%2520applications%252C%250Apromoting%2520fairness%2520and%2520inclusivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20Hidden%20Bias%20in%20AI%3A%20Lessons%20from%20Large%20Language%20Models&entry.906535625=Django%20Beatty%20and%20Kritsada%20Masanthia%20and%20Teepakorn%20Kaphol%20and%20Niphan%20Sethi&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20become%20integral%20to%20recruitment%20processes%2C%0Aconcerns%20about%20AI-induced%20bias%20have%20intensified.%20This%20study%20examines%20biases%20in%0Acandidate%20interview%20reports%20generated%20by%20Claude%203.5%20Sonnet%2C%20GPT-4o%2C%20Gemini%201.5%2C%0Aand%20Llama%203.1%20405B%2C%20focusing%20on%20characteristics%20such%20as%20gender%2C%20race%2C%20and%20age.%0AWe%20evaluate%20the%20effectiveness%20of%20LLM-based%20anonymization%20in%20reducing%20these%0Abiases.%20Findings%20indicate%20that%20while%20anonymization%20reduces%20certain%20biases%2C%0Aparticularly%20gender%20bias%2C%20the%20degree%20of%20effectiveness%20varies%20across%20models%20and%0Abias%20types.%20Notably%2C%20Llama%203.1%20405B%20exhibited%20the%20lowest%20overall%20bias.%0AMoreover%2C%20our%20methodology%20of%20comparing%20anonymized%20and%20non-anonymized%20data%0Areveals%20a%20novel%20approach%20to%20assessing%20inherent%20biases%20in%20LLMs%20beyond%0Arecruitment%20applications.%20This%20study%20underscores%20the%20importance%20of%20careful%20LLM%0Aselection%20and%20suggests%20best%20practices%20for%20minimizing%20bias%20in%20AI%20applications%2C%0Apromoting%20fairness%20and%20inclusivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16927v1&entry.124074799=Read"},
{"title": "Optimal Robust Estimation under Local and Global Corruptions: Stronger\n  Adversary and Smaller Error", "author": "Thanasis Pittas and Ankit Pensia", "abstract": "  Algorithmic robust statistics has traditionally focused on the contamination\nmodel where a small fraction of the samples are arbitrarily corrupted. We\nconsider a recent contamination model that combines two kinds of corruptions:\n(i) small fraction of arbitrary outliers, as in classical robust statistics,\nand (ii) local perturbations, where samples may undergo bounded shifts on\naverage. While each noise model is well understood individually, the combined\ncontamination model poses new algorithmic challenges, with only partial results\nknown. Existing efficient algorithms are limited in two ways: (i) they work\nonly for a weak notion of local perturbations, and (ii) they obtain suboptimal\nerror for isotropic subgaussian distributions (among others). The latter\nlimitation led [NGS24, COLT'24] to hypothesize that improving the error might,\nin fact, be computationally hard. Perhaps surprisingly, we show that\ninformation theoretically optimal error can indeed be achieved in polynomial\ntime, under an even \\emph{stronger} local perturbation model (the\nsliced-Wasserstein metric as opposed to the Wasserstein metric). Notably, our\nanalysis reveals that the entire family of stability-based robust mean\nestimators continues to work optimally in a black-box manner for the combined\ncontamination model. This generalization is particularly useful in real-world\nscenarios where the specific form of data corruption is not known in advance.\nWe also present efficient algorithms for distribution learning and principal\ncomponent analysis in the combined contamination model.\n", "link": "http://arxiv.org/abs/2410.17230v1", "date": "2024-10-22", "relevancy": 2.2941, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4707}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4572}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Robust%20Estimation%20under%20Local%20and%20Global%20Corruptions%3A%20Stronger%0A%20%20Adversary%20and%20Smaller%20Error&body=Title%3A%20Optimal%20Robust%20Estimation%20under%20Local%20and%20Global%20Corruptions%3A%20Stronger%0A%20%20Adversary%20and%20Smaller%20Error%0AAuthor%3A%20Thanasis%20Pittas%20and%20Ankit%20Pensia%0AAbstract%3A%20%20%20Algorithmic%20robust%20statistics%20has%20traditionally%20focused%20on%20the%20contamination%0Amodel%20where%20a%20small%20fraction%20of%20the%20samples%20are%20arbitrarily%20corrupted.%20We%0Aconsider%20a%20recent%20contamination%20model%20that%20combines%20two%20kinds%20of%20corruptions%3A%0A%28i%29%20small%20fraction%20of%20arbitrary%20outliers%2C%20as%20in%20classical%20robust%20statistics%2C%0Aand%20%28ii%29%20local%20perturbations%2C%20where%20samples%20may%20undergo%20bounded%20shifts%20on%0Aaverage.%20While%20each%20noise%20model%20is%20well%20understood%20individually%2C%20the%20combined%0Acontamination%20model%20poses%20new%20algorithmic%20challenges%2C%20with%20only%20partial%20results%0Aknown.%20Existing%20efficient%20algorithms%20are%20limited%20in%20two%20ways%3A%20%28i%29%20they%20work%0Aonly%20for%20a%20weak%20notion%20of%20local%20perturbations%2C%20and%20%28ii%29%20they%20obtain%20suboptimal%0Aerror%20for%20isotropic%20subgaussian%20distributions%20%28among%20others%29.%20The%20latter%0Alimitation%20led%20%5BNGS24%2C%20COLT%2724%5D%20to%20hypothesize%20that%20improving%20the%20error%20might%2C%0Ain%20fact%2C%20be%20computationally%20hard.%20Perhaps%20surprisingly%2C%20we%20show%20that%0Ainformation%20theoretically%20optimal%20error%20can%20indeed%20be%20achieved%20in%20polynomial%0Atime%2C%20under%20an%20even%20%5Cemph%7Bstronger%7D%20local%20perturbation%20model%20%28the%0Asliced-Wasserstein%20metric%20as%20opposed%20to%20the%20Wasserstein%20metric%29.%20Notably%2C%20our%0Aanalysis%20reveals%20that%20the%20entire%20family%20of%20stability-based%20robust%20mean%0Aestimators%20continues%20to%20work%20optimally%20in%20a%20black-box%20manner%20for%20the%20combined%0Acontamination%20model.%20This%20generalization%20is%20particularly%20useful%20in%20real-world%0Ascenarios%20where%20the%20specific%20form%20of%20data%20corruption%20is%20not%20known%20in%20advance.%0AWe%20also%20present%20efficient%20algorithms%20for%20distribution%20learning%20and%20principal%0Acomponent%20analysis%20in%20the%20combined%20contamination%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Robust%2520Estimation%2520under%2520Local%2520and%2520Global%2520Corruptions%253A%2520Stronger%250A%2520%2520Adversary%2520and%2520Smaller%2520Error%26entry.906535625%3DThanasis%2520Pittas%2520and%2520Ankit%2520Pensia%26entry.1292438233%3D%2520%2520Algorithmic%2520robust%2520statistics%2520has%2520traditionally%2520focused%2520on%2520the%2520contamination%250Amodel%2520where%2520a%2520small%2520fraction%2520of%2520the%2520samples%2520are%2520arbitrarily%2520corrupted.%2520We%250Aconsider%2520a%2520recent%2520contamination%2520model%2520that%2520combines%2520two%2520kinds%2520of%2520corruptions%253A%250A%2528i%2529%2520small%2520fraction%2520of%2520arbitrary%2520outliers%252C%2520as%2520in%2520classical%2520robust%2520statistics%252C%250Aand%2520%2528ii%2529%2520local%2520perturbations%252C%2520where%2520samples%2520may%2520undergo%2520bounded%2520shifts%2520on%250Aaverage.%2520While%2520each%2520noise%2520model%2520is%2520well%2520understood%2520individually%252C%2520the%2520combined%250Acontamination%2520model%2520poses%2520new%2520algorithmic%2520challenges%252C%2520with%2520only%2520partial%2520results%250Aknown.%2520Existing%2520efficient%2520algorithms%2520are%2520limited%2520in%2520two%2520ways%253A%2520%2528i%2529%2520they%2520work%250Aonly%2520for%2520a%2520weak%2520notion%2520of%2520local%2520perturbations%252C%2520and%2520%2528ii%2529%2520they%2520obtain%2520suboptimal%250Aerror%2520for%2520isotropic%2520subgaussian%2520distributions%2520%2528among%2520others%2529.%2520The%2520latter%250Alimitation%2520led%2520%255BNGS24%252C%2520COLT%252724%255D%2520to%2520hypothesize%2520that%2520improving%2520the%2520error%2520might%252C%250Ain%2520fact%252C%2520be%2520computationally%2520hard.%2520Perhaps%2520surprisingly%252C%2520we%2520show%2520that%250Ainformation%2520theoretically%2520optimal%2520error%2520can%2520indeed%2520be%2520achieved%2520in%2520polynomial%250Atime%252C%2520under%2520an%2520even%2520%255Cemph%257Bstronger%257D%2520local%2520perturbation%2520model%2520%2528the%250Asliced-Wasserstein%2520metric%2520as%2520opposed%2520to%2520the%2520Wasserstein%2520metric%2529.%2520Notably%252C%2520our%250Aanalysis%2520reveals%2520that%2520the%2520entire%2520family%2520of%2520stability-based%2520robust%2520mean%250Aestimators%2520continues%2520to%2520work%2520optimally%2520in%2520a%2520black-box%2520manner%2520for%2520the%2520combined%250Acontamination%2520model.%2520This%2520generalization%2520is%2520particularly%2520useful%2520in%2520real-world%250Ascenarios%2520where%2520the%2520specific%2520form%2520of%2520data%2520corruption%2520is%2520not%2520known%2520in%2520advance.%250AWe%2520also%2520present%2520efficient%2520algorithms%2520for%2520distribution%2520learning%2520and%2520principal%250Acomponent%2520analysis%2520in%2520the%2520combined%2520contamination%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Robust%20Estimation%20under%20Local%20and%20Global%20Corruptions%3A%20Stronger%0A%20%20Adversary%20and%20Smaller%20Error&entry.906535625=Thanasis%20Pittas%20and%20Ankit%20Pensia&entry.1292438233=%20%20Algorithmic%20robust%20statistics%20has%20traditionally%20focused%20on%20the%20contamination%0Amodel%20where%20a%20small%20fraction%20of%20the%20samples%20are%20arbitrarily%20corrupted.%20We%0Aconsider%20a%20recent%20contamination%20model%20that%20combines%20two%20kinds%20of%20corruptions%3A%0A%28i%29%20small%20fraction%20of%20arbitrary%20outliers%2C%20as%20in%20classical%20robust%20statistics%2C%0Aand%20%28ii%29%20local%20perturbations%2C%20where%20samples%20may%20undergo%20bounded%20shifts%20on%0Aaverage.%20While%20each%20noise%20model%20is%20well%20understood%20individually%2C%20the%20combined%0Acontamination%20model%20poses%20new%20algorithmic%20challenges%2C%20with%20only%20partial%20results%0Aknown.%20Existing%20efficient%20algorithms%20are%20limited%20in%20two%20ways%3A%20%28i%29%20they%20work%0Aonly%20for%20a%20weak%20notion%20of%20local%20perturbations%2C%20and%20%28ii%29%20they%20obtain%20suboptimal%0Aerror%20for%20isotropic%20subgaussian%20distributions%20%28among%20others%29.%20The%20latter%0Alimitation%20led%20%5BNGS24%2C%20COLT%2724%5D%20to%20hypothesize%20that%20improving%20the%20error%20might%2C%0Ain%20fact%2C%20be%20computationally%20hard.%20Perhaps%20surprisingly%2C%20we%20show%20that%0Ainformation%20theoretically%20optimal%20error%20can%20indeed%20be%20achieved%20in%20polynomial%0Atime%2C%20under%20an%20even%20%5Cemph%7Bstronger%7D%20local%20perturbation%20model%20%28the%0Asliced-Wasserstein%20metric%20as%20opposed%20to%20the%20Wasserstein%20metric%29.%20Notably%2C%20our%0Aanalysis%20reveals%20that%20the%20entire%20family%20of%20stability-based%20robust%20mean%0Aestimators%20continues%20to%20work%20optimally%20in%20a%20black-box%20manner%20for%20the%20combined%0Acontamination%20model.%20This%20generalization%20is%20particularly%20useful%20in%20real-world%0Ascenarios%20where%20the%20specific%20form%20of%20data%20corruption%20is%20not%20known%20in%20advance.%0AWe%20also%20present%20efficient%20algorithms%20for%20distribution%20learning%20and%20principal%0Acomponent%20analysis%20in%20the%20combined%20contamination%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17230v1&entry.124074799=Read"},
{"title": "Emphasizing Discriminative Features for Dataset Distillation in Complex\n  Scenarios", "author": "Kai Wang and Zekai Li and Zhi-Qi Cheng and Samir Khaki and Ahmad Sajedi and Ramakrishna Vedantam and Konstantinos N Plataniotis and Alexander Hauptmann and Yang You", "abstract": "  Dataset distillation has demonstrated strong performance on simple datasets\nlike CIFAR, MNIST, and TinyImageNet but struggles to achieve similar results in\nmore complex scenarios. In this paper, we propose EDF (emphasizes the\ndiscriminative features), a dataset distillation method that enhances key\ndiscriminative regions in synthetic images using Grad-CAM activation maps. Our\napproach is inspired by a key observation: in simple datasets, high-activation\nareas typically occupy most of the image, whereas in complex scenarios, the\nsize of these areas is much smaller. Unlike previous methods that treat all\npixels equally when synthesizing images, EDF uses Grad-CAM activation maps to\nenhance high-activation areas. From a supervision perspective, we downplay\nsupervision signals that have lower losses, as they contain common patterns.\nAdditionally, to help the DD community better explore complex scenarios, we\nbuild the Complex Dataset Distillation (Comp-DD) benchmark by meticulously\nselecting sixteen subsets, eight easy and eight hard, from ImageNet-1K. In\nparticular, EDF consistently outperforms SOTA results in complex scenarios,\nsuch as ImageNet-1K subsets. Hopefully, more researchers will be inspired and\nencouraged to improve the practicality and efficacy of DD. Our code and\nbenchmark will be made public at https://github.com/NUS-HPC-AI-Lab/EDF.\n", "link": "http://arxiv.org/abs/2410.17193v1", "date": "2024-10-22", "relevancy": 2.2909, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5838}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5782}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emphasizing%20Discriminative%20Features%20for%20Dataset%20Distillation%20in%20Complex%0A%20%20Scenarios&body=Title%3A%20Emphasizing%20Discriminative%20Features%20for%20Dataset%20Distillation%20in%20Complex%0A%20%20Scenarios%0AAuthor%3A%20Kai%20Wang%20and%20Zekai%20Li%20and%20Zhi-Qi%20Cheng%20and%20Samir%20Khaki%20and%20Ahmad%20Sajedi%20and%20Ramakrishna%20Vedantam%20and%20Konstantinos%20N%20Plataniotis%20and%20Alexander%20Hauptmann%20and%20Yang%20You%0AAbstract%3A%20%20%20Dataset%20distillation%20has%20demonstrated%20strong%20performance%20on%20simple%20datasets%0Alike%20CIFAR%2C%20MNIST%2C%20and%20TinyImageNet%20but%20struggles%20to%20achieve%20similar%20results%20in%0Amore%20complex%20scenarios.%20In%20this%20paper%2C%20we%20propose%20EDF%20%28emphasizes%20the%0Adiscriminative%20features%29%2C%20a%20dataset%20distillation%20method%20that%20enhances%20key%0Adiscriminative%20regions%20in%20synthetic%20images%20using%20Grad-CAM%20activation%20maps.%20Our%0Aapproach%20is%20inspired%20by%20a%20key%20observation%3A%20in%20simple%20datasets%2C%20high-activation%0Aareas%20typically%20occupy%20most%20of%20the%20image%2C%20whereas%20in%20complex%20scenarios%2C%20the%0Asize%20of%20these%20areas%20is%20much%20smaller.%20Unlike%20previous%20methods%20that%20treat%20all%0Apixels%20equally%20when%20synthesizing%20images%2C%20EDF%20uses%20Grad-CAM%20activation%20maps%20to%0Aenhance%20high-activation%20areas.%20From%20a%20supervision%20perspective%2C%20we%20downplay%0Asupervision%20signals%20that%20have%20lower%20losses%2C%20as%20they%20contain%20common%20patterns.%0AAdditionally%2C%20to%20help%20the%20DD%20community%20better%20explore%20complex%20scenarios%2C%20we%0Abuild%20the%20Complex%20Dataset%20Distillation%20%28Comp-DD%29%20benchmark%20by%20meticulously%0Aselecting%20sixteen%20subsets%2C%20eight%20easy%20and%20eight%20hard%2C%20from%20ImageNet-1K.%20In%0Aparticular%2C%20EDF%20consistently%20outperforms%20SOTA%20results%20in%20complex%20scenarios%2C%0Asuch%20as%20ImageNet-1K%20subsets.%20Hopefully%2C%20more%20researchers%20will%20be%20inspired%20and%0Aencouraged%20to%20improve%20the%20practicality%20and%20efficacy%20of%20DD.%20Our%20code%20and%0Abenchmark%20will%20be%20made%20public%20at%20https%3A//github.com/NUS-HPC-AI-Lab/EDF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmphasizing%2520Discriminative%2520Features%2520for%2520Dataset%2520Distillation%2520in%2520Complex%250A%2520%2520Scenarios%26entry.906535625%3DKai%2520Wang%2520and%2520Zekai%2520Li%2520and%2520Zhi-Qi%2520Cheng%2520and%2520Samir%2520Khaki%2520and%2520Ahmad%2520Sajedi%2520and%2520Ramakrishna%2520Vedantam%2520and%2520Konstantinos%2520N%2520Plataniotis%2520and%2520Alexander%2520Hauptmann%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Dataset%2520distillation%2520has%2520demonstrated%2520strong%2520performance%2520on%2520simple%2520datasets%250Alike%2520CIFAR%252C%2520MNIST%252C%2520and%2520TinyImageNet%2520but%2520struggles%2520to%2520achieve%2520similar%2520results%2520in%250Amore%2520complex%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520EDF%2520%2528emphasizes%2520the%250Adiscriminative%2520features%2529%252C%2520a%2520dataset%2520distillation%2520method%2520that%2520enhances%2520key%250Adiscriminative%2520regions%2520in%2520synthetic%2520images%2520using%2520Grad-CAM%2520activation%2520maps.%2520Our%250Aapproach%2520is%2520inspired%2520by%2520a%2520key%2520observation%253A%2520in%2520simple%2520datasets%252C%2520high-activation%250Aareas%2520typically%2520occupy%2520most%2520of%2520the%2520image%252C%2520whereas%2520in%2520complex%2520scenarios%252C%2520the%250Asize%2520of%2520these%2520areas%2520is%2520much%2520smaller.%2520Unlike%2520previous%2520methods%2520that%2520treat%2520all%250Apixels%2520equally%2520when%2520synthesizing%2520images%252C%2520EDF%2520uses%2520Grad-CAM%2520activation%2520maps%2520to%250Aenhance%2520high-activation%2520areas.%2520From%2520a%2520supervision%2520perspective%252C%2520we%2520downplay%250Asupervision%2520signals%2520that%2520have%2520lower%2520losses%252C%2520as%2520they%2520contain%2520common%2520patterns.%250AAdditionally%252C%2520to%2520help%2520the%2520DD%2520community%2520better%2520explore%2520complex%2520scenarios%252C%2520we%250Abuild%2520the%2520Complex%2520Dataset%2520Distillation%2520%2528Comp-DD%2529%2520benchmark%2520by%2520meticulously%250Aselecting%2520sixteen%2520subsets%252C%2520eight%2520easy%2520and%2520eight%2520hard%252C%2520from%2520ImageNet-1K.%2520In%250Aparticular%252C%2520EDF%2520consistently%2520outperforms%2520SOTA%2520results%2520in%2520complex%2520scenarios%252C%250Asuch%2520as%2520ImageNet-1K%2520subsets.%2520Hopefully%252C%2520more%2520researchers%2520will%2520be%2520inspired%2520and%250Aencouraged%2520to%2520improve%2520the%2520practicality%2520and%2520efficacy%2520of%2520DD.%2520Our%2520code%2520and%250Abenchmark%2520will%2520be%2520made%2520public%2520at%2520https%253A//github.com/NUS-HPC-AI-Lab/EDF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emphasizing%20Discriminative%20Features%20for%20Dataset%20Distillation%20in%20Complex%0A%20%20Scenarios&entry.906535625=Kai%20Wang%20and%20Zekai%20Li%20and%20Zhi-Qi%20Cheng%20and%20Samir%20Khaki%20and%20Ahmad%20Sajedi%20and%20Ramakrishna%20Vedantam%20and%20Konstantinos%20N%20Plataniotis%20and%20Alexander%20Hauptmann%20and%20Yang%20You&entry.1292438233=%20%20Dataset%20distillation%20has%20demonstrated%20strong%20performance%20on%20simple%20datasets%0Alike%20CIFAR%2C%20MNIST%2C%20and%20TinyImageNet%20but%20struggles%20to%20achieve%20similar%20results%20in%0Amore%20complex%20scenarios.%20In%20this%20paper%2C%20we%20propose%20EDF%20%28emphasizes%20the%0Adiscriminative%20features%29%2C%20a%20dataset%20distillation%20method%20that%20enhances%20key%0Adiscriminative%20regions%20in%20synthetic%20images%20using%20Grad-CAM%20activation%20maps.%20Our%0Aapproach%20is%20inspired%20by%20a%20key%20observation%3A%20in%20simple%20datasets%2C%20high-activation%0Aareas%20typically%20occupy%20most%20of%20the%20image%2C%20whereas%20in%20complex%20scenarios%2C%20the%0Asize%20of%20these%20areas%20is%20much%20smaller.%20Unlike%20previous%20methods%20that%20treat%20all%0Apixels%20equally%20when%20synthesizing%20images%2C%20EDF%20uses%20Grad-CAM%20activation%20maps%20to%0Aenhance%20high-activation%20areas.%20From%20a%20supervision%20perspective%2C%20we%20downplay%0Asupervision%20signals%20that%20have%20lower%20losses%2C%20as%20they%20contain%20common%20patterns.%0AAdditionally%2C%20to%20help%20the%20DD%20community%20better%20explore%20complex%20scenarios%2C%20we%0Abuild%20the%20Complex%20Dataset%20Distillation%20%28Comp-DD%29%20benchmark%20by%20meticulously%0Aselecting%20sixteen%20subsets%2C%20eight%20easy%20and%20eight%20hard%2C%20from%20ImageNet-1K.%20In%0Aparticular%2C%20EDF%20consistently%20outperforms%20SOTA%20results%20in%20complex%20scenarios%2C%0Asuch%20as%20ImageNet-1K%20subsets.%20Hopefully%2C%20more%20researchers%20will%20be%20inspired%20and%0Aencouraged%20to%20improve%20the%20practicality%20and%20efficacy%20of%20DD.%20Our%20code%20and%0Abenchmark%20will%20be%20made%20public%20at%20https%3A//github.com/NUS-HPC-AI-Lab/EDF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17193v1&entry.124074799=Read"},
{"title": "The Impact of Large Language Models in Academia: from Writing to\n  Speaking", "author": "Mingmeng Geng and Caixi Chen and Yanru Wu and Dongping Chen and Yao Wan and Pan Zhou", "abstract": "  Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.\n", "link": "http://arxiv.org/abs/2409.13686v2", "date": "2024-10-22", "relevancy": 2.2833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Large%20Language%20Models%20in%20Academia%3A%20from%20Writing%20to%0A%20%20Speaking&body=Title%3A%20The%20Impact%20of%20Large%20Language%20Models%20in%20Academia%3A%20from%20Writing%20to%0A%20%20Speaking%0AAuthor%3A%20Mingmeng%20Geng%20and%20Caixi%20Chen%20and%20Yanru%20Wu%20and%20Dongping%20Chen%20and%20Yao%20Wan%20and%20Pan%20Zhou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20impacting%20human%20society%2C%0Aparticularly%20in%20textual%20information.%20Based%20on%20more%20than%2030%2C000%20papers%20and%201%2C000%0Apresentations%20from%20machine%20learning%20conferences%2C%20we%20examined%20and%20compared%20the%0Awords%20used%20in%20writing%20and%20speaking%2C%20representing%20the%20first%20large-scale%20study%20of%0Ahow%20LLMs%20influence%20the%20two%20main%20modes%20of%20verbal%20communication%20and%20expression%0Awithin%20the%20same%20group%20of%20people.%20Our%20empirical%20results%20show%20that%20LLM-style%0Awords%20such%20as%20%22significant%22%20have%20been%20used%20more%20frequently%20in%20abstracts%20and%0Aoral%20presentations.%20The%20impact%20on%20speaking%20is%20beginning%20to%20emerge%20and%20is%20likely%0Ato%20grow%20in%20the%20future%2C%20calling%20attention%20to%20the%20implicit%20influence%20and%20ripple%0Aeffect%20of%20LLMs%20on%20human%20society.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Large%2520Language%2520Models%2520in%2520Academia%253A%2520from%2520Writing%2520to%250A%2520%2520Speaking%26entry.906535625%3DMingmeng%2520Geng%2520and%2520Caixi%2520Chen%2520and%2520Yanru%2520Wu%2520and%2520Dongping%2520Chen%2520and%2520Yao%2520Wan%2520and%2520Pan%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520impacting%2520human%2520society%252C%250Aparticularly%2520in%2520textual%2520information.%2520Based%2520on%2520more%2520than%252030%252C000%2520papers%2520and%25201%252C000%250Apresentations%2520from%2520machine%2520learning%2520conferences%252C%2520we%2520examined%2520and%2520compared%2520the%250Awords%2520used%2520in%2520writing%2520and%2520speaking%252C%2520representing%2520the%2520first%2520large-scale%2520study%2520of%250Ahow%2520LLMs%2520influence%2520the%2520two%2520main%2520modes%2520of%2520verbal%2520communication%2520and%2520expression%250Awithin%2520the%2520same%2520group%2520of%2520people.%2520Our%2520empirical%2520results%2520show%2520that%2520LLM-style%250Awords%2520such%2520as%2520%2522significant%2522%2520have%2520been%2520used%2520more%2520frequently%2520in%2520abstracts%2520and%250Aoral%2520presentations.%2520The%2520impact%2520on%2520speaking%2520is%2520beginning%2520to%2520emerge%2520and%2520is%2520likely%250Ato%2520grow%2520in%2520the%2520future%252C%2520calling%2520attention%2520to%2520the%2520implicit%2520influence%2520and%2520ripple%250Aeffect%2520of%2520LLMs%2520on%2520human%2520society.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Large%20Language%20Models%20in%20Academia%3A%20from%20Writing%20to%0A%20%20Speaking&entry.906535625=Mingmeng%20Geng%20and%20Caixi%20Chen%20and%20Yanru%20Wu%20and%20Dongping%20Chen%20and%20Yao%20Wan%20and%20Pan%20Zhou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20impacting%20human%20society%2C%0Aparticularly%20in%20textual%20information.%20Based%20on%20more%20than%2030%2C000%20papers%20and%201%2C000%0Apresentations%20from%20machine%20learning%20conferences%2C%20we%20examined%20and%20compared%20the%0Awords%20used%20in%20writing%20and%20speaking%2C%20representing%20the%20first%20large-scale%20study%20of%0Ahow%20LLMs%20influence%20the%20two%20main%20modes%20of%20verbal%20communication%20and%20expression%0Awithin%20the%20same%20group%20of%20people.%20Our%20empirical%20results%20show%20that%20LLM-style%0Awords%20such%20as%20%22significant%22%20have%20been%20used%20more%20frequently%20in%20abstracts%20and%0Aoral%20presentations.%20The%20impact%20on%20speaking%20is%20beginning%20to%20emerge%20and%20is%20likely%0Ato%20grow%20in%20the%20future%2C%20calling%20attention%20to%20the%20implicit%20influence%20and%20ripple%0Aeffect%20of%20LLMs%20on%20human%20society.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13686v2&entry.124074799=Read"},
{"title": "Exploring Diversity-based Active Learning for 3D Object Detection in\n  Autonomous Driving", "author": "Jinpeng Lin and Zhihao Liang and Shengheng Deng and Lile Cai and Tao Jiang and Tianrui Li and Kui Jia and Xun Xu", "abstract": "  3D object detection has recently received much attention due to its great\npotential in autonomous vehicle (AV). The success of deep learning based object\ndetectors relies on the availability of large-scale annotated datasets, which\nis time-consuming and expensive to compile, especially for 3D bounding box\nannotation. In this work, we investigate diversity-based active learning (AL)\nas a potential solution to alleviate the annotation burden. Given limited\nannotation budget, only the most informative frames and objects are\nautomatically selected for human to annotate. Technically, we take the\nadvantage of the multimodal information provided in an AV dataset, and propose\na novel acquisition function that enforces spatial and temporal diversity in\nthe selected samples. We benchmark the proposed method against other AL\nstrategies under realistic annotation cost measurement, where the realistic\ncosts for annotating a frame and a 3D bounding box are both taken into\nconsideration. We demonstrate the effectiveness of the proposed method on the\nnuScenes dataset and show that it outperforms existing AL strategies\nsignificantly. Code is available at\nhttps://github.com/Linkon87/Exploring-Diversity-based-Active-Learning-for-3D-Object-Detection-in-Autonomous-Driving\n", "link": "http://arxiv.org/abs/2205.07708v3", "date": "2024-10-22", "relevancy": 2.2808, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6097}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Diversity-based%20Active%20Learning%20for%203D%20Object%20Detection%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Exploring%20Diversity-based%20Active%20Learning%20for%203D%20Object%20Detection%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Jinpeng%20Lin%20and%20Zhihao%20Liang%20and%20Shengheng%20Deng%20and%20Lile%20Cai%20and%20Tao%20Jiang%20and%20Tianrui%20Li%20and%20Kui%20Jia%20and%20Xun%20Xu%0AAbstract%3A%20%20%203D%20object%20detection%20has%20recently%20received%20much%20attention%20due%20to%20its%20great%0Apotential%20in%20autonomous%20vehicle%20%28AV%29.%20The%20success%20of%20deep%20learning%20based%20object%0Adetectors%20relies%20on%20the%20availability%20of%20large-scale%20annotated%20datasets%2C%20which%0Ais%20time-consuming%20and%20expensive%20to%20compile%2C%20especially%20for%203D%20bounding%20box%0Aannotation.%20In%20this%20work%2C%20we%20investigate%20diversity-based%20active%20learning%20%28AL%29%0Aas%20a%20potential%20solution%20to%20alleviate%20the%20annotation%20burden.%20Given%20limited%0Aannotation%20budget%2C%20only%20the%20most%20informative%20frames%20and%20objects%20are%0Aautomatically%20selected%20for%20human%20to%20annotate.%20Technically%2C%20we%20take%20the%0Aadvantage%20of%20the%20multimodal%20information%20provided%20in%20an%20AV%20dataset%2C%20and%20propose%0Aa%20novel%20acquisition%20function%20that%20enforces%20spatial%20and%20temporal%20diversity%20in%0Athe%20selected%20samples.%20We%20benchmark%20the%20proposed%20method%20against%20other%20AL%0Astrategies%20under%20realistic%20annotation%20cost%20measurement%2C%20where%20the%20realistic%0Acosts%20for%20annotating%20a%20frame%20and%20a%203D%20bounding%20box%20are%20both%20taken%20into%0Aconsideration.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20on%20the%0AnuScenes%20dataset%20and%20show%20that%20it%20outperforms%20existing%20AL%20strategies%0Asignificantly.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Linkon87/Exploring-Diversity-based-Active-Learning-for-3D-Object-Detection-in-Autonomous-Driving%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.07708v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Diversity-based%2520Active%2520Learning%2520for%25203D%2520Object%2520Detection%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DJinpeng%2520Lin%2520and%2520Zhihao%2520Liang%2520and%2520Shengheng%2520Deng%2520and%2520Lile%2520Cai%2520and%2520Tao%2520Jiang%2520and%2520Tianrui%2520Li%2520and%2520Kui%2520Jia%2520and%2520Xun%2520Xu%26entry.1292438233%3D%2520%25203D%2520object%2520detection%2520has%2520recently%2520received%2520much%2520attention%2520due%2520to%2520its%2520great%250Apotential%2520in%2520autonomous%2520vehicle%2520%2528AV%2529.%2520The%2520success%2520of%2520deep%2520learning%2520based%2520object%250Adetectors%2520relies%2520on%2520the%2520availability%2520of%2520large-scale%2520annotated%2520datasets%252C%2520which%250Ais%2520time-consuming%2520and%2520expensive%2520to%2520compile%252C%2520especially%2520for%25203D%2520bounding%2520box%250Aannotation.%2520In%2520this%2520work%252C%2520we%2520investigate%2520diversity-based%2520active%2520learning%2520%2528AL%2529%250Aas%2520a%2520potential%2520solution%2520to%2520alleviate%2520the%2520annotation%2520burden.%2520Given%2520limited%250Aannotation%2520budget%252C%2520only%2520the%2520most%2520informative%2520frames%2520and%2520objects%2520are%250Aautomatically%2520selected%2520for%2520human%2520to%2520annotate.%2520Technically%252C%2520we%2520take%2520the%250Aadvantage%2520of%2520the%2520multimodal%2520information%2520provided%2520in%2520an%2520AV%2520dataset%252C%2520and%2520propose%250Aa%2520novel%2520acquisition%2520function%2520that%2520enforces%2520spatial%2520and%2520temporal%2520diversity%2520in%250Athe%2520selected%2520samples.%2520We%2520benchmark%2520the%2520proposed%2520method%2520against%2520other%2520AL%250Astrategies%2520under%2520realistic%2520annotation%2520cost%2520measurement%252C%2520where%2520the%2520realistic%250Acosts%2520for%2520annotating%2520a%2520frame%2520and%2520a%25203D%2520bounding%2520box%2520are%2520both%2520taken%2520into%250Aconsideration.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520on%2520the%250AnuScenes%2520dataset%2520and%2520show%2520that%2520it%2520outperforms%2520existing%2520AL%2520strategies%250Asignificantly.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Linkon87/Exploring-Diversity-based-Active-Learning-for-3D-Object-Detection-in-Autonomous-Driving%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.07708v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Diversity-based%20Active%20Learning%20for%203D%20Object%20Detection%20in%0A%20%20Autonomous%20Driving&entry.906535625=Jinpeng%20Lin%20and%20Zhihao%20Liang%20and%20Shengheng%20Deng%20and%20Lile%20Cai%20and%20Tao%20Jiang%20and%20Tianrui%20Li%20and%20Kui%20Jia%20and%20Xun%20Xu&entry.1292438233=%20%203D%20object%20detection%20has%20recently%20received%20much%20attention%20due%20to%20its%20great%0Apotential%20in%20autonomous%20vehicle%20%28AV%29.%20The%20success%20of%20deep%20learning%20based%20object%0Adetectors%20relies%20on%20the%20availability%20of%20large-scale%20annotated%20datasets%2C%20which%0Ais%20time-consuming%20and%20expensive%20to%20compile%2C%20especially%20for%203D%20bounding%20box%0Aannotation.%20In%20this%20work%2C%20we%20investigate%20diversity-based%20active%20learning%20%28AL%29%0Aas%20a%20potential%20solution%20to%20alleviate%20the%20annotation%20burden.%20Given%20limited%0Aannotation%20budget%2C%20only%20the%20most%20informative%20frames%20and%20objects%20are%0Aautomatically%20selected%20for%20human%20to%20annotate.%20Technically%2C%20we%20take%20the%0Aadvantage%20of%20the%20multimodal%20information%20provided%20in%20an%20AV%20dataset%2C%20and%20propose%0Aa%20novel%20acquisition%20function%20that%20enforces%20spatial%20and%20temporal%20diversity%20in%0Athe%20selected%20samples.%20We%20benchmark%20the%20proposed%20method%20against%20other%20AL%0Astrategies%20under%20realistic%20annotation%20cost%20measurement%2C%20where%20the%20realistic%0Acosts%20for%20annotating%20a%20frame%20and%20a%203D%20bounding%20box%20are%20both%20taken%20into%0Aconsideration.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20on%20the%0AnuScenes%20dataset%20and%20show%20that%20it%20outperforms%20existing%20AL%20strategies%0Asignificantly.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Linkon87/Exploring-Diversity-based-Active-Learning-for-3D-Object-Detection-in-Autonomous-Driving%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.07708v3&entry.124074799=Read"},
{"title": "PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction\n  in Off-road Driving", "author": "Zhipeng Zhao and Bowen Li and Yi Du and Taimeng Fu and Chen Wang", "abstract": "  Motion prediction is critical for autonomous off-road driving, however, it\npresents significantly more challenges than on-road driving because of the\ncomplex interaction between the vehicle and the terrain. Traditional\nphysics-based approaches encounter difficulties in accurately modeling dynamic\nsystems and external disturbance. In contrast, data-driven neural networks\nrequire extensive datasets and struggle with explicitly capturing the\nfundamental physical laws, which can easily lead to poor generalization. By\nmerging the advantages of both methods, neuro-symbolic approaches present a\npromising direction. These methods embed physical laws into neural models,\npotentially significantly improving generalization capabilities. However, no\nprior works were evaluated in real-world settings for off-road driving. To\nbridge this gap, we present PhysORD, a neural-symbolic approach integrating the\nconservation law, i.e., the Euler-Lagrange equation, into data-driven neural\nmodels for motion prediction in off-road driving. Our experiments showed that\nPhysORD can accurately predict vehicle motion and tolerate external disturbance\nby modeling uncertainties. The learned dynamics model achieves 46.7% higher\naccuracy using only 3.1% of the parameters compared to data-driven methods,\ndemonstrating the data efficiency and superior generalization ability of our\nneural-symbolic method.\n", "link": "http://arxiv.org/abs/2404.01596v3", "date": "2024-10-22", "relevancy": 2.2776, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6424}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5669}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysORD%3A%20A%20Neuro-Symbolic%20Approach%20for%20Physics-infused%20Motion%20Prediction%0A%20%20in%20Off-road%20Driving&body=Title%3A%20PhysORD%3A%20A%20Neuro-Symbolic%20Approach%20for%20Physics-infused%20Motion%20Prediction%0A%20%20in%20Off-road%20Driving%0AAuthor%3A%20Zhipeng%20Zhao%20and%20Bowen%20Li%20and%20Yi%20Du%20and%20Taimeng%20Fu%20and%20Chen%20Wang%0AAbstract%3A%20%20%20Motion%20prediction%20is%20critical%20for%20autonomous%20off-road%20driving%2C%20however%2C%20it%0Apresents%20significantly%20more%20challenges%20than%20on-road%20driving%20because%20of%20the%0Acomplex%20interaction%20between%20the%20vehicle%20and%20the%20terrain.%20Traditional%0Aphysics-based%20approaches%20encounter%20difficulties%20in%20accurately%20modeling%20dynamic%0Asystems%20and%20external%20disturbance.%20In%20contrast%2C%20data-driven%20neural%20networks%0Arequire%20extensive%20datasets%20and%20struggle%20with%20explicitly%20capturing%20the%0Afundamental%20physical%20laws%2C%20which%20can%20easily%20lead%20to%20poor%20generalization.%20By%0Amerging%20the%20advantages%20of%20both%20methods%2C%20neuro-symbolic%20approaches%20present%20a%0Apromising%20direction.%20These%20methods%20embed%20physical%20laws%20into%20neural%20models%2C%0Apotentially%20significantly%20improving%20generalization%20capabilities.%20However%2C%20no%0Aprior%20works%20were%20evaluated%20in%20real-world%20settings%20for%20off-road%20driving.%20To%0Abridge%20this%20gap%2C%20we%20present%20PhysORD%2C%20a%20neural-symbolic%20approach%20integrating%20the%0Aconservation%20law%2C%20i.e.%2C%20the%20Euler-Lagrange%20equation%2C%20into%20data-driven%20neural%0Amodels%20for%20motion%20prediction%20in%20off-road%20driving.%20Our%20experiments%20showed%20that%0APhysORD%20can%20accurately%20predict%20vehicle%20motion%20and%20tolerate%20external%20disturbance%0Aby%20modeling%20uncertainties.%20The%20learned%20dynamics%20model%20achieves%2046.7%25%20higher%0Aaccuracy%20using%20only%203.1%25%20of%20the%20parameters%20compared%20to%20data-driven%20methods%2C%0Ademonstrating%20the%20data%20efficiency%20and%20superior%20generalization%20ability%20of%20our%0Aneural-symbolic%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01596v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysORD%253A%2520A%2520Neuro-Symbolic%2520Approach%2520for%2520Physics-infused%2520Motion%2520Prediction%250A%2520%2520in%2520Off-road%2520Driving%26entry.906535625%3DZhipeng%2520Zhao%2520and%2520Bowen%2520Li%2520and%2520Yi%2520Du%2520and%2520Taimeng%2520Fu%2520and%2520Chen%2520Wang%26entry.1292438233%3D%2520%2520Motion%2520prediction%2520is%2520critical%2520for%2520autonomous%2520off-road%2520driving%252C%2520however%252C%2520it%250Apresents%2520significantly%2520more%2520challenges%2520than%2520on-road%2520driving%2520because%2520of%2520the%250Acomplex%2520interaction%2520between%2520the%2520vehicle%2520and%2520the%2520terrain.%2520Traditional%250Aphysics-based%2520approaches%2520encounter%2520difficulties%2520in%2520accurately%2520modeling%2520dynamic%250Asystems%2520and%2520external%2520disturbance.%2520In%2520contrast%252C%2520data-driven%2520neural%2520networks%250Arequire%2520extensive%2520datasets%2520and%2520struggle%2520with%2520explicitly%2520capturing%2520the%250Afundamental%2520physical%2520laws%252C%2520which%2520can%2520easily%2520lead%2520to%2520poor%2520generalization.%2520By%250Amerging%2520the%2520advantages%2520of%2520both%2520methods%252C%2520neuro-symbolic%2520approaches%2520present%2520a%250Apromising%2520direction.%2520These%2520methods%2520embed%2520physical%2520laws%2520into%2520neural%2520models%252C%250Apotentially%2520significantly%2520improving%2520generalization%2520capabilities.%2520However%252C%2520no%250Aprior%2520works%2520were%2520evaluated%2520in%2520real-world%2520settings%2520for%2520off-road%2520driving.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520present%2520PhysORD%252C%2520a%2520neural-symbolic%2520approach%2520integrating%2520the%250Aconservation%2520law%252C%2520i.e.%252C%2520the%2520Euler-Lagrange%2520equation%252C%2520into%2520data-driven%2520neural%250Amodels%2520for%2520motion%2520prediction%2520in%2520off-road%2520driving.%2520Our%2520experiments%2520showed%2520that%250APhysORD%2520can%2520accurately%2520predict%2520vehicle%2520motion%2520and%2520tolerate%2520external%2520disturbance%250Aby%2520modeling%2520uncertainties.%2520The%2520learned%2520dynamics%2520model%2520achieves%252046.7%2525%2520higher%250Aaccuracy%2520using%2520only%25203.1%2525%2520of%2520the%2520parameters%2520compared%2520to%2520data-driven%2520methods%252C%250Ademonstrating%2520the%2520data%2520efficiency%2520and%2520superior%2520generalization%2520ability%2520of%2520our%250Aneural-symbolic%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01596v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysORD%3A%20A%20Neuro-Symbolic%20Approach%20for%20Physics-infused%20Motion%20Prediction%0A%20%20in%20Off-road%20Driving&entry.906535625=Zhipeng%20Zhao%20and%20Bowen%20Li%20and%20Yi%20Du%20and%20Taimeng%20Fu%20and%20Chen%20Wang&entry.1292438233=%20%20Motion%20prediction%20is%20critical%20for%20autonomous%20off-road%20driving%2C%20however%2C%20it%0Apresents%20significantly%20more%20challenges%20than%20on-road%20driving%20because%20of%20the%0Acomplex%20interaction%20between%20the%20vehicle%20and%20the%20terrain.%20Traditional%0Aphysics-based%20approaches%20encounter%20difficulties%20in%20accurately%20modeling%20dynamic%0Asystems%20and%20external%20disturbance.%20In%20contrast%2C%20data-driven%20neural%20networks%0Arequire%20extensive%20datasets%20and%20struggle%20with%20explicitly%20capturing%20the%0Afundamental%20physical%20laws%2C%20which%20can%20easily%20lead%20to%20poor%20generalization.%20By%0Amerging%20the%20advantages%20of%20both%20methods%2C%20neuro-symbolic%20approaches%20present%20a%0Apromising%20direction.%20These%20methods%20embed%20physical%20laws%20into%20neural%20models%2C%0Apotentially%20significantly%20improving%20generalization%20capabilities.%20However%2C%20no%0Aprior%20works%20were%20evaluated%20in%20real-world%20settings%20for%20off-road%20driving.%20To%0Abridge%20this%20gap%2C%20we%20present%20PhysORD%2C%20a%20neural-symbolic%20approach%20integrating%20the%0Aconservation%20law%2C%20i.e.%2C%20the%20Euler-Lagrange%20equation%2C%20into%20data-driven%20neural%0Amodels%20for%20motion%20prediction%20in%20off-road%20driving.%20Our%20experiments%20showed%20that%0APhysORD%20can%20accurately%20predict%20vehicle%20motion%20and%20tolerate%20external%20disturbance%0Aby%20modeling%20uncertainties.%20The%20learned%20dynamics%20model%20achieves%2046.7%25%20higher%0Aaccuracy%20using%20only%203.1%25%20of%20the%20parameters%20compared%20to%20data-driven%20methods%2C%0Ademonstrating%20the%20data%20efficiency%20and%20superior%20generalization%20ability%20of%20our%0Aneural-symbolic%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01596v3&entry.124074799=Read"},
{"title": "PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner", "author": "Kota Kondo and Claudius T. Tewari and Andrea Tagliabue and Jesus Tordesillas and Parker C. Lusk and Jonathan P. How", "abstract": "  In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.\n", "link": "http://arxiv.org/abs/2406.10060v2", "date": "2024-10-22", "relevancy": 2.2751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5829}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5775}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner&body=Title%3A%20PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner%0AAuthor%3A%20Kota%20Kondo%20and%20Claudius%20T.%20Tewari%20and%20Andrea%20Tagliabue%20and%20Jesus%20Tordesillas%20and%20Parker%20C.%20Lusk%20and%20Jonathan%20P.%20How%0AAbstract%3A%20%20%20In%20decentralized%20multiagent%20trajectory%20planners%2C%20agents%20need%20to%20communicate%0Aand%20exchange%20their%20positions%20to%20generate%20collision-free%20trajectories.%20However%2C%0Adue%20to%20localization%20errors/uncertainties%2C%20trajectory%20deconfliction%20can%20fail%0Aeven%20if%20trajectories%20are%20perfectly%20shared%20between%20agents.%20To%20address%20this%0Aissue%2C%20we%20first%20present%20PARM%20and%20PARM%2A%2C%20perception-aware%2C%20decentralized%2C%0Aasynchronous%20multiagent%20trajectory%20planners%20that%20enable%20a%20team%20of%20agents%20to%0Anavigate%20uncertain%20environments%20while%20deconflicting%20trajectories%20and%20avoiding%0Aobstacles%20using%20perception%20information.%20PARM%2A%20differs%20from%20PARM%20as%20it%20is%20less%0Aconservative%2C%20using%20more%20computation%20to%20find%20closer-to-optimal%20solutions.%20While%0Athese%20methods%20achieve%20state-of-the-art%20performance%2C%20they%20suffer%20from%20high%0Acomputational%20costs%20as%20they%20need%20to%20solve%20large%20optimization%20problems%20onboard%2C%0Amaking%20it%20difficult%20for%20agents%20to%20replan%20at%20high%20rates.%20To%20overcome%20this%0Achallenge%2C%20we%20present%20our%20second%20key%20contribution%2C%20PRIMER%2C%20a%20learning-based%0Aplanner%20trained%20with%20imitation%20learning%20%28IL%29%20using%20PARM%2A%20as%20the%20expert%0Ademonstrator.%20PRIMER%20leverages%20the%20low%20computational%20requirements%20at%20deployment%0Aof%20neural%20networks%20and%20achieves%20a%20computation%20speed%20up%20to%205500%20times%20faster%0Athan%20optimization-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIMER%253A%2520Perception-Aware%2520Robust%2520Learning-based%2520Multiagent%2520Trajectory%250A%2520%2520Planner%26entry.906535625%3DKota%2520Kondo%2520and%2520Claudius%2520T.%2520Tewari%2520and%2520Andrea%2520Tagliabue%2520and%2520Jesus%2520Tordesillas%2520and%2520Parker%2520C.%2520Lusk%2520and%2520Jonathan%2520P.%2520How%26entry.1292438233%3D%2520%2520In%2520decentralized%2520multiagent%2520trajectory%2520planners%252C%2520agents%2520need%2520to%2520communicate%250Aand%2520exchange%2520their%2520positions%2520to%2520generate%2520collision-free%2520trajectories.%2520However%252C%250Adue%2520to%2520localization%2520errors/uncertainties%252C%2520trajectory%2520deconfliction%2520can%2520fail%250Aeven%2520if%2520trajectories%2520are%2520perfectly%2520shared%2520between%2520agents.%2520To%2520address%2520this%250Aissue%252C%2520we%2520first%2520present%2520PARM%2520and%2520PARM%252A%252C%2520perception-aware%252C%2520decentralized%252C%250Aasynchronous%2520multiagent%2520trajectory%2520planners%2520that%2520enable%2520a%2520team%2520of%2520agents%2520to%250Anavigate%2520uncertain%2520environments%2520while%2520deconflicting%2520trajectories%2520and%2520avoiding%250Aobstacles%2520using%2520perception%2520information.%2520PARM%252A%2520differs%2520from%2520PARM%2520as%2520it%2520is%2520less%250Aconservative%252C%2520using%2520more%2520computation%2520to%2520find%2520closer-to-optimal%2520solutions.%2520While%250Athese%2520methods%2520achieve%2520state-of-the-art%2520performance%252C%2520they%2520suffer%2520from%2520high%250Acomputational%2520costs%2520as%2520they%2520need%2520to%2520solve%2520large%2520optimization%2520problems%2520onboard%252C%250Amaking%2520it%2520difficult%2520for%2520agents%2520to%2520replan%2520at%2520high%2520rates.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520present%2520our%2520second%2520key%2520contribution%252C%2520PRIMER%252C%2520a%2520learning-based%250Aplanner%2520trained%2520with%2520imitation%2520learning%2520%2528IL%2529%2520using%2520PARM%252A%2520as%2520the%2520expert%250Ademonstrator.%2520PRIMER%2520leverages%2520the%2520low%2520computational%2520requirements%2520at%2520deployment%250Aof%2520neural%2520networks%2520and%2520achieves%2520a%2520computation%2520speed%2520up%2520to%25205500%2520times%2520faster%250Athan%2520optimization-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner&entry.906535625=Kota%20Kondo%20and%20Claudius%20T.%20Tewari%20and%20Andrea%20Tagliabue%20and%20Jesus%20Tordesillas%20and%20Parker%20C.%20Lusk%20and%20Jonathan%20P.%20How&entry.1292438233=%20%20In%20decentralized%20multiagent%20trajectory%20planners%2C%20agents%20need%20to%20communicate%0Aand%20exchange%20their%20positions%20to%20generate%20collision-free%20trajectories.%20However%2C%0Adue%20to%20localization%20errors/uncertainties%2C%20trajectory%20deconfliction%20can%20fail%0Aeven%20if%20trajectories%20are%20perfectly%20shared%20between%20agents.%20To%20address%20this%0Aissue%2C%20we%20first%20present%20PARM%20and%20PARM%2A%2C%20perception-aware%2C%20decentralized%2C%0Aasynchronous%20multiagent%20trajectory%20planners%20that%20enable%20a%20team%20of%20agents%20to%0Anavigate%20uncertain%20environments%20while%20deconflicting%20trajectories%20and%20avoiding%0Aobstacles%20using%20perception%20information.%20PARM%2A%20differs%20from%20PARM%20as%20it%20is%20less%0Aconservative%2C%20using%20more%20computation%20to%20find%20closer-to-optimal%20solutions.%20While%0Athese%20methods%20achieve%20state-of-the-art%20performance%2C%20they%20suffer%20from%20high%0Acomputational%20costs%20as%20they%20need%20to%20solve%20large%20optimization%20problems%20onboard%2C%0Amaking%20it%20difficult%20for%20agents%20to%20replan%20at%20high%20rates.%20To%20overcome%20this%0Achallenge%2C%20we%20present%20our%20second%20key%20contribution%2C%20PRIMER%2C%20a%20learning-based%0Aplanner%20trained%20with%20imitation%20learning%20%28IL%29%20using%20PARM%2A%20as%20the%20expert%0Ademonstrator.%20PRIMER%20leverages%20the%20low%20computational%20requirements%20at%20deployment%0Aof%20neural%20networks%20and%20achieves%20a%20computation%20speed%20up%20to%205500%20times%20faster%0Athan%20optimization-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10060v2&entry.124074799=Read"},
{"title": "Order Matters: Exploring Order Sensitivity in Multimodal Large Language\n  Models", "author": "Zhijie Tan and Xu Chu and Weiping Li and Tong Mo", "abstract": "  Multimodal Large Language Models (MLLMs) utilize multimodal contexts\nconsisting of text, images, or videos to solve various multimodal tasks.\nHowever, we find that changing the order of multimodal input can cause the\nmodel's performance to fluctuate between advanced performance and random\nguessing. This phenomenon exists in both single-modality (text-only or\nimage-only) and mixed-modality (image-text-pair) contexts. Furthermore, we\ndemonstrate that popular MLLMs pay special attention to certain multimodal\ncontext positions, particularly the beginning and end. Leveraging this special\nattention, we place key video frames and important image/text content in\nspecial positions within the context and submit them to the MLLM for inference.\nThis method results in average performance gains of 14.7% for video-caption\nmatching and 17.8% for visual question answering tasks. Additionally, we\npropose a new metric, Position-Invariant Accuracy (PIA), to address order bias\nin MLLM evaluation. Our research findings contribute to a better understanding\nof Multi-Modal In-Context Learning (MMICL) and provide practical strategies for\nenhancing MLLM performance without increasing computational costs.\n", "link": "http://arxiv.org/abs/2410.16983v1", "date": "2024-10-22", "relevancy": 2.2625, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5736}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Order%20Matters%3A%20Exploring%20Order%20Sensitivity%20in%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20Order%20Matters%3A%20Exploring%20Order%20Sensitivity%20in%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Zhijie%20Tan%20and%20Xu%20Chu%20and%20Weiping%20Li%20and%20Tong%20Mo%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20utilize%20multimodal%20contexts%0Aconsisting%20of%20text%2C%20images%2C%20or%20videos%20to%20solve%20various%20multimodal%20tasks.%0AHowever%2C%20we%20find%20that%20changing%20the%20order%20of%20multimodal%20input%20can%20cause%20the%0Amodel%27s%20performance%20to%20fluctuate%20between%20advanced%20performance%20and%20random%0Aguessing.%20This%20phenomenon%20exists%20in%20both%20single-modality%20%28text-only%20or%0Aimage-only%29%20and%20mixed-modality%20%28image-text-pair%29%20contexts.%20Furthermore%2C%20we%0Ademonstrate%20that%20popular%20MLLMs%20pay%20special%20attention%20to%20certain%20multimodal%0Acontext%20positions%2C%20particularly%20the%20beginning%20and%20end.%20Leveraging%20this%20special%0Aattention%2C%20we%20place%20key%20video%20frames%20and%20important%20image/text%20content%20in%0Aspecial%20positions%20within%20the%20context%20and%20submit%20them%20to%20the%20MLLM%20for%20inference.%0AThis%20method%20results%20in%20average%20performance%20gains%20of%2014.7%25%20for%20video-caption%0Amatching%20and%2017.8%25%20for%20visual%20question%20answering%20tasks.%20Additionally%2C%20we%0Apropose%20a%20new%20metric%2C%20Position-Invariant%20Accuracy%20%28PIA%29%2C%20to%20address%20order%20bias%0Ain%20MLLM%20evaluation.%20Our%20research%20findings%20contribute%20to%20a%20better%20understanding%0Aof%20Multi-Modal%20In-Context%20Learning%20%28MMICL%29%20and%20provide%20practical%20strategies%20for%0Aenhancing%20MLLM%20performance%20without%20increasing%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrder%2520Matters%253A%2520Exploring%2520Order%2520Sensitivity%2520in%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DZhijie%2520Tan%2520and%2520Xu%2520Chu%2520and%2520Weiping%2520Li%2520and%2520Tong%2520Mo%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520utilize%2520multimodal%2520contexts%250Aconsisting%2520of%2520text%252C%2520images%252C%2520or%2520videos%2520to%2520solve%2520various%2520multimodal%2520tasks.%250AHowever%252C%2520we%2520find%2520that%2520changing%2520the%2520order%2520of%2520multimodal%2520input%2520can%2520cause%2520the%250Amodel%2527s%2520performance%2520to%2520fluctuate%2520between%2520advanced%2520performance%2520and%2520random%250Aguessing.%2520This%2520phenomenon%2520exists%2520in%2520both%2520single-modality%2520%2528text-only%2520or%250Aimage-only%2529%2520and%2520mixed-modality%2520%2528image-text-pair%2529%2520contexts.%2520Furthermore%252C%2520we%250Ademonstrate%2520that%2520popular%2520MLLMs%2520pay%2520special%2520attention%2520to%2520certain%2520multimodal%250Acontext%2520positions%252C%2520particularly%2520the%2520beginning%2520and%2520end.%2520Leveraging%2520this%2520special%250Aattention%252C%2520we%2520place%2520key%2520video%2520frames%2520and%2520important%2520image/text%2520content%2520in%250Aspecial%2520positions%2520within%2520the%2520context%2520and%2520submit%2520them%2520to%2520the%2520MLLM%2520for%2520inference.%250AThis%2520method%2520results%2520in%2520average%2520performance%2520gains%2520of%252014.7%2525%2520for%2520video-caption%250Amatching%2520and%252017.8%2525%2520for%2520visual%2520question%2520answering%2520tasks.%2520Additionally%252C%2520we%250Apropose%2520a%2520new%2520metric%252C%2520Position-Invariant%2520Accuracy%2520%2528PIA%2529%252C%2520to%2520address%2520order%2520bias%250Ain%2520MLLM%2520evaluation.%2520Our%2520research%2520findings%2520contribute%2520to%2520a%2520better%2520understanding%250Aof%2520Multi-Modal%2520In-Context%2520Learning%2520%2528MMICL%2529%2520and%2520provide%2520practical%2520strategies%2520for%250Aenhancing%2520MLLM%2520performance%2520without%2520increasing%2520computational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Order%20Matters%3A%20Exploring%20Order%20Sensitivity%20in%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Zhijie%20Tan%20and%20Xu%20Chu%20and%20Weiping%20Li%20and%20Tong%20Mo&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20utilize%20multimodal%20contexts%0Aconsisting%20of%20text%2C%20images%2C%20or%20videos%20to%20solve%20various%20multimodal%20tasks.%0AHowever%2C%20we%20find%20that%20changing%20the%20order%20of%20multimodal%20input%20can%20cause%20the%0Amodel%27s%20performance%20to%20fluctuate%20between%20advanced%20performance%20and%20random%0Aguessing.%20This%20phenomenon%20exists%20in%20both%20single-modality%20%28text-only%20or%0Aimage-only%29%20and%20mixed-modality%20%28image-text-pair%29%20contexts.%20Furthermore%2C%20we%0Ademonstrate%20that%20popular%20MLLMs%20pay%20special%20attention%20to%20certain%20multimodal%0Acontext%20positions%2C%20particularly%20the%20beginning%20and%20end.%20Leveraging%20this%20special%0Aattention%2C%20we%20place%20key%20video%20frames%20and%20important%20image/text%20content%20in%0Aspecial%20positions%20within%20the%20context%20and%20submit%20them%20to%20the%20MLLM%20for%20inference.%0AThis%20method%20results%20in%20average%20performance%20gains%20of%2014.7%25%20for%20video-caption%0Amatching%20and%2017.8%25%20for%20visual%20question%20answering%20tasks.%20Additionally%2C%20we%0Apropose%20a%20new%20metric%2C%20Position-Invariant%20Accuracy%20%28PIA%29%2C%20to%20address%20order%20bias%0Ain%20MLLM%20evaluation.%20Our%20research%20findings%20contribute%20to%20a%20better%20understanding%0Aof%20Multi-Modal%20In-Context%20Learning%20%28MMICL%29%20and%20provide%20practical%20strategies%20for%0Aenhancing%20MLLM%20performance%20without%20increasing%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16983v1&entry.124074799=Read"},
{"title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid\n  Visual Redundancy Reduction", "author": "Long Xing and Qidong Huang and Xiaoyi Dong and Jiajie Lu and Pan Zhang and Yuhang Zang and Yuhang Cao and Conghui He and Jiaqi Wang and Feng Wu and Dahua Lin", "abstract": "  In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. We hope that the insights and approach\nintroduced by PyramidDrop will inspire future research to further investigate\nthe role of image tokens in LVLMs.\n", "link": "http://arxiv.org/abs/2410.17247v1", "date": "2024-10-22", "relevancy": 2.2561, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5915}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PyramidDrop%3A%20Accelerating%20Your%20Large%20Vision-Language%20Models%20via%20Pyramid%0A%20%20Visual%20Redundancy%20Reduction&body=Title%3A%20PyramidDrop%3A%20Accelerating%20Your%20Large%20Vision-Language%20Models%20via%20Pyramid%0A%20%20Visual%20Redundancy%20Reduction%0AAuthor%3A%20Long%20Xing%20and%20Qidong%20Huang%20and%20Xiaoyi%20Dong%20and%20Jiajie%20Lu%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Conghui%20He%20and%20Jiaqi%20Wang%20and%20Feng%20Wu%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20In%20large%20vision-language%20models%20%28LVLMs%29%2C%20images%20serve%20as%20inputs%20that%20carry%20a%0Awealth%20of%20information.%20As%20the%20idiom%20%22A%20picture%20is%20worth%20a%20thousand%20words%22%0Aimplies%2C%20representing%20a%20single%20image%20in%20current%20LVLMs%20can%20require%20hundreds%20or%0Aeven%20thousands%20of%20tokens.%20This%20results%20in%20significant%20computational%20costs%2C%0Awhich%20grow%20quadratically%20as%20input%20image%20resolution%20increases%2C%20thereby%20severely%0Aimpacting%20the%20efficiency%20of%20both%20training%20and%20inference.%20Previous%20approaches%0Ahave%20attempted%20to%20reduce%20the%20number%20of%20image%20tokens%20either%20before%20or%20within%20the%0Aearly%20layers%20of%20LVLMs.%20However%2C%20these%20strategies%20inevitably%20result%20in%20the%20loss%0Aof%20crucial%20image%20information%2C%20ultimately%20diminishing%20model%20performance.%20To%0Aaddress%20this%20challenge%2C%20we%20conduct%20an%20empirical%20study%20revealing%20that%20all%20visual%0Atokens%20are%20necessary%20for%20LVLMs%20in%20the%20shallow%20layers%2C%20and%20token%20redundancy%0Aprogressively%20increases%20in%20the%20deeper%20layers%20of%20the%20model.%20To%20this%20end%2C%20we%0Apropose%20PyramidDrop%2C%20a%20visual%20redundancy%20reduction%20strategy%20for%20LVLMs%20to%20boost%0Atheir%20efficiency%20in%20both%20training%20and%20inference%20with%20neglectable%20performance%0Aloss.%20Specifically%2C%20we%20partition%20the%20LVLM%20into%20several%20stages%20and%20drop%20part%20of%0Athe%20image%20tokens%20at%20the%20end%20of%20each%20stage%20with%20a%20pre-defined%20ratio%2C%20creating%0Apyramid-like%20visual%20tokens%20across%20model%20layers.%20The%20dropping%20is%20based%20on%20a%0Alightweight%20similarity%20calculation%20with%20a%20negligible%20time%20overhead.%20Extensive%0Aexperiments%20demonstrate%20that%20PyramidDrop%20can%20achieve%20a%2040%25%20training%20time%20and%0A55%25%20inference%20FLOPs%20acceleration%20of%20LLaVA-NeXT%20with%20comparable%20performance.%0ABesides%2C%20the%20PyramidDrop%20could%20also%20serve%20as%20a%20plug-and-play%20strategy%20for%0Ainference%20acceleration%20without%20training%2C%20with%20better%20performance%20and%20lower%0Ainference%20cost%20than%20counterparts.%20We%20hope%20that%20the%20insights%20and%20approach%0Aintroduced%20by%20PyramidDrop%20will%20inspire%20future%20research%20to%20further%20investigate%0Athe%20role%20of%20image%20tokens%20in%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyramidDrop%253A%2520Accelerating%2520Your%2520Large%2520Vision-Language%2520Models%2520via%2520Pyramid%250A%2520%2520Visual%2520Redundancy%2520Reduction%26entry.906535625%3DLong%2520Xing%2520and%2520Qidong%2520Huang%2520and%2520Xiaoyi%2520Dong%2520and%2520Jiajie%2520Lu%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Conghui%2520He%2520and%2520Jiaqi%2520Wang%2520and%2520Feng%2520Wu%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520In%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%252C%2520images%2520serve%2520as%2520inputs%2520that%2520carry%2520a%250Awealth%2520of%2520information.%2520As%2520the%2520idiom%2520%2522A%2520picture%2520is%2520worth%2520a%2520thousand%2520words%2522%250Aimplies%252C%2520representing%2520a%2520single%2520image%2520in%2520current%2520LVLMs%2520can%2520require%2520hundreds%2520or%250Aeven%2520thousands%2520of%2520tokens.%2520This%2520results%2520in%2520significant%2520computational%2520costs%252C%250Awhich%2520grow%2520quadratically%2520as%2520input%2520image%2520resolution%2520increases%252C%2520thereby%2520severely%250Aimpacting%2520the%2520efficiency%2520of%2520both%2520training%2520and%2520inference.%2520Previous%2520approaches%250Ahave%2520attempted%2520to%2520reduce%2520the%2520number%2520of%2520image%2520tokens%2520either%2520before%2520or%2520within%2520the%250Aearly%2520layers%2520of%2520LVLMs.%2520However%252C%2520these%2520strategies%2520inevitably%2520result%2520in%2520the%2520loss%250Aof%2520crucial%2520image%2520information%252C%2520ultimately%2520diminishing%2520model%2520performance.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520conduct%2520an%2520empirical%2520study%2520revealing%2520that%2520all%2520visual%250Atokens%2520are%2520necessary%2520for%2520LVLMs%2520in%2520the%2520shallow%2520layers%252C%2520and%2520token%2520redundancy%250Aprogressively%2520increases%2520in%2520the%2520deeper%2520layers%2520of%2520the%2520model.%2520To%2520this%2520end%252C%2520we%250Apropose%2520PyramidDrop%252C%2520a%2520visual%2520redundancy%2520reduction%2520strategy%2520for%2520LVLMs%2520to%2520boost%250Atheir%2520efficiency%2520in%2520both%2520training%2520and%2520inference%2520with%2520neglectable%2520performance%250Aloss.%2520Specifically%252C%2520we%2520partition%2520the%2520LVLM%2520into%2520several%2520stages%2520and%2520drop%2520part%2520of%250Athe%2520image%2520tokens%2520at%2520the%2520end%2520of%2520each%2520stage%2520with%2520a%2520pre-defined%2520ratio%252C%2520creating%250Apyramid-like%2520visual%2520tokens%2520across%2520model%2520layers.%2520The%2520dropping%2520is%2520based%2520on%2520a%250Alightweight%2520similarity%2520calculation%2520with%2520a%2520negligible%2520time%2520overhead.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520PyramidDrop%2520can%2520achieve%2520a%252040%2525%2520training%2520time%2520and%250A55%2525%2520inference%2520FLOPs%2520acceleration%2520of%2520LLaVA-NeXT%2520with%2520comparable%2520performance.%250ABesides%252C%2520the%2520PyramidDrop%2520could%2520also%2520serve%2520as%2520a%2520plug-and-play%2520strategy%2520for%250Ainference%2520acceleration%2520without%2520training%252C%2520with%2520better%2520performance%2520and%2520lower%250Ainference%2520cost%2520than%2520counterparts.%2520We%2520hope%2520that%2520the%2520insights%2520and%2520approach%250Aintroduced%2520by%2520PyramidDrop%2520will%2520inspire%2520future%2520research%2520to%2520further%2520investigate%250Athe%2520role%2520of%2520image%2520tokens%2520in%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PyramidDrop%3A%20Accelerating%20Your%20Large%20Vision-Language%20Models%20via%20Pyramid%0A%20%20Visual%20Redundancy%20Reduction&entry.906535625=Long%20Xing%20and%20Qidong%20Huang%20and%20Xiaoyi%20Dong%20and%20Jiajie%20Lu%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Conghui%20He%20and%20Jiaqi%20Wang%20and%20Feng%20Wu%20and%20Dahua%20Lin&entry.1292438233=%20%20In%20large%20vision-language%20models%20%28LVLMs%29%2C%20images%20serve%20as%20inputs%20that%20carry%20a%0Awealth%20of%20information.%20As%20the%20idiom%20%22A%20picture%20is%20worth%20a%20thousand%20words%22%0Aimplies%2C%20representing%20a%20single%20image%20in%20current%20LVLMs%20can%20require%20hundreds%20or%0Aeven%20thousands%20of%20tokens.%20This%20results%20in%20significant%20computational%20costs%2C%0Awhich%20grow%20quadratically%20as%20input%20image%20resolution%20increases%2C%20thereby%20severely%0Aimpacting%20the%20efficiency%20of%20both%20training%20and%20inference.%20Previous%20approaches%0Ahave%20attempted%20to%20reduce%20the%20number%20of%20image%20tokens%20either%20before%20or%20within%20the%0Aearly%20layers%20of%20LVLMs.%20However%2C%20these%20strategies%20inevitably%20result%20in%20the%20loss%0Aof%20crucial%20image%20information%2C%20ultimately%20diminishing%20model%20performance.%20To%0Aaddress%20this%20challenge%2C%20we%20conduct%20an%20empirical%20study%20revealing%20that%20all%20visual%0Atokens%20are%20necessary%20for%20LVLMs%20in%20the%20shallow%20layers%2C%20and%20token%20redundancy%0Aprogressively%20increases%20in%20the%20deeper%20layers%20of%20the%20model.%20To%20this%20end%2C%20we%0Apropose%20PyramidDrop%2C%20a%20visual%20redundancy%20reduction%20strategy%20for%20LVLMs%20to%20boost%0Atheir%20efficiency%20in%20both%20training%20and%20inference%20with%20neglectable%20performance%0Aloss.%20Specifically%2C%20we%20partition%20the%20LVLM%20into%20several%20stages%20and%20drop%20part%20of%0Athe%20image%20tokens%20at%20the%20end%20of%20each%20stage%20with%20a%20pre-defined%20ratio%2C%20creating%0Apyramid-like%20visual%20tokens%20across%20model%20layers.%20The%20dropping%20is%20based%20on%20a%0Alightweight%20similarity%20calculation%20with%20a%20negligible%20time%20overhead.%20Extensive%0Aexperiments%20demonstrate%20that%20PyramidDrop%20can%20achieve%20a%2040%25%20training%20time%20and%0A55%25%20inference%20FLOPs%20acceleration%20of%20LLaVA-NeXT%20with%20comparable%20performance.%0ABesides%2C%20the%20PyramidDrop%20could%20also%20serve%20as%20a%20plug-and-play%20strategy%20for%0Ainference%20acceleration%20without%20training%2C%20with%20better%20performance%20and%20lower%0Ainference%20cost%20than%20counterparts.%20We%20hope%20that%20the%20insights%20and%20approach%0Aintroduced%20by%20PyramidDrop%20will%20inspire%20future%20research%20to%20further%20investigate%0Athe%20role%20of%20image%20tokens%20in%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17247v1&entry.124074799=Read"},
{"title": "DPEC: Dual-Path Error Compensation Method for Enhanced Low-Light Image\n  Clarity", "author": "Shuang Wang and Qianwen Lu and Yihe Nie and Qingchuan Tao and Yanmei Yu", "abstract": "  For the task of low-light image enhancement, deep learning-based algorithms\nhave demonstrated superiority and effectiveness compared to traditional\nmethods. Existing deep learning algorithms are proposed mainly based on the\nRetinex theory but overlook the noise and color distortion present in the\ninput, which frequently results in significant noise amplification and local\ncolor distortion in the final results. To address this, we propose a Dual-Path\nError Compensation method (DPEC), which aims to improve image quality in\nlow-light conditions. DPEC performs precise pixel-level error estimation, which\naccurately captures subtle pixels differences, and independent denoising, which\neffectively removes unnecessary noise. This method restores image brightness\nwhile preserving local texture details and avoiding noise amplification.\nFurthermore, to compensate for the traditional CNN's limited ability to capture\nlong-range semantic information and considering both computational speed and\nresource efficiency, we integrated the VMamba architecture into the backbone of\nDPEC. In addition, we introduced the HIS-Retinex loss to constrain the training\nof DPEC, ensuring that the overall brightness distribution of the images more\nclosely aligns with real-world conditions. Comprehensive quantitative and\nqualitative experimental results demonstrate that our algorithm significantly\noutperforms state-of-the-art methods across six benchmark tests.\n", "link": "http://arxiv.org/abs/2407.09553v3", "date": "2024-10-22", "relevancy": 2.2514, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.592}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5607}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPEC%3A%20Dual-Path%20Error%20Compensation%20Method%20for%20Enhanced%20Low-Light%20Image%0A%20%20Clarity&body=Title%3A%20DPEC%3A%20Dual-Path%20Error%20Compensation%20Method%20for%20Enhanced%20Low-Light%20Image%0A%20%20Clarity%0AAuthor%3A%20Shuang%20Wang%20and%20Qianwen%20Lu%20and%20Yihe%20Nie%20and%20Qingchuan%20Tao%20and%20Yanmei%20Yu%0AAbstract%3A%20%20%20For%20the%20task%20of%20low-light%20image%20enhancement%2C%20deep%20learning-based%20algorithms%0Ahave%20demonstrated%20superiority%20and%20effectiveness%20compared%20to%20traditional%0Amethods.%20Existing%20deep%20learning%20algorithms%20are%20proposed%20mainly%20based%20on%20the%0ARetinex%20theory%20but%20overlook%20the%20noise%20and%20color%20distortion%20present%20in%20the%0Ainput%2C%20which%20frequently%20results%20in%20significant%20noise%20amplification%20and%20local%0Acolor%20distortion%20in%20the%20final%20results.%20To%20address%20this%2C%20we%20propose%20a%20Dual-Path%0AError%20Compensation%20method%20%28DPEC%29%2C%20which%20aims%20to%20improve%20image%20quality%20in%0Alow-light%20conditions.%20DPEC%20performs%20precise%20pixel-level%20error%20estimation%2C%20which%0Aaccurately%20captures%20subtle%20pixels%20differences%2C%20and%20independent%20denoising%2C%20which%0Aeffectively%20removes%20unnecessary%20noise.%20This%20method%20restores%20image%20brightness%0Awhile%20preserving%20local%20texture%20details%20and%20avoiding%20noise%20amplification.%0AFurthermore%2C%20to%20compensate%20for%20the%20traditional%20CNN%27s%20limited%20ability%20to%20capture%0Along-range%20semantic%20information%20and%20considering%20both%20computational%20speed%20and%0Aresource%20efficiency%2C%20we%20integrated%20the%20VMamba%20architecture%20into%20the%20backbone%20of%0ADPEC.%20In%20addition%2C%20we%20introduced%20the%20HIS-Retinex%20loss%20to%20constrain%20the%20training%0Aof%20DPEC%2C%20ensuring%20that%20the%20overall%20brightness%20distribution%20of%20the%20images%20more%0Aclosely%20aligns%20with%20real-world%20conditions.%20Comprehensive%20quantitative%20and%0Aqualitative%20experimental%20results%20demonstrate%20that%20our%20algorithm%20significantly%0Aoutperforms%20state-of-the-art%20methods%20across%20six%20benchmark%20tests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09553v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPEC%253A%2520Dual-Path%2520Error%2520Compensation%2520Method%2520for%2520Enhanced%2520Low-Light%2520Image%250A%2520%2520Clarity%26entry.906535625%3DShuang%2520Wang%2520and%2520Qianwen%2520Lu%2520and%2520Yihe%2520Nie%2520and%2520Qingchuan%2520Tao%2520and%2520Yanmei%2520Yu%26entry.1292438233%3D%2520%2520For%2520the%2520task%2520of%2520low-light%2520image%2520enhancement%252C%2520deep%2520learning-based%2520algorithms%250Ahave%2520demonstrated%2520superiority%2520and%2520effectiveness%2520compared%2520to%2520traditional%250Amethods.%2520Existing%2520deep%2520learning%2520algorithms%2520are%2520proposed%2520mainly%2520based%2520on%2520the%250ARetinex%2520theory%2520but%2520overlook%2520the%2520noise%2520and%2520color%2520distortion%2520present%2520in%2520the%250Ainput%252C%2520which%2520frequently%2520results%2520in%2520significant%2520noise%2520amplification%2520and%2520local%250Acolor%2520distortion%2520in%2520the%2520final%2520results.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Dual-Path%250AError%2520Compensation%2520method%2520%2528DPEC%2529%252C%2520which%2520aims%2520to%2520improve%2520image%2520quality%2520in%250Alow-light%2520conditions.%2520DPEC%2520performs%2520precise%2520pixel-level%2520error%2520estimation%252C%2520which%250Aaccurately%2520captures%2520subtle%2520pixels%2520differences%252C%2520and%2520independent%2520denoising%252C%2520which%250Aeffectively%2520removes%2520unnecessary%2520noise.%2520This%2520method%2520restores%2520image%2520brightness%250Awhile%2520preserving%2520local%2520texture%2520details%2520and%2520avoiding%2520noise%2520amplification.%250AFurthermore%252C%2520to%2520compensate%2520for%2520the%2520traditional%2520CNN%2527s%2520limited%2520ability%2520to%2520capture%250Along-range%2520semantic%2520information%2520and%2520considering%2520both%2520computational%2520speed%2520and%250Aresource%2520efficiency%252C%2520we%2520integrated%2520the%2520VMamba%2520architecture%2520into%2520the%2520backbone%2520of%250ADPEC.%2520In%2520addition%252C%2520we%2520introduced%2520the%2520HIS-Retinex%2520loss%2520to%2520constrain%2520the%2520training%250Aof%2520DPEC%252C%2520ensuring%2520that%2520the%2520overall%2520brightness%2520distribution%2520of%2520the%2520images%2520more%250Aclosely%2520aligns%2520with%2520real-world%2520conditions.%2520Comprehensive%2520quantitative%2520and%250Aqualitative%2520experimental%2520results%2520demonstrate%2520that%2520our%2520algorithm%2520significantly%250Aoutperforms%2520state-of-the-art%2520methods%2520across%2520six%2520benchmark%2520tests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09553v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPEC%3A%20Dual-Path%20Error%20Compensation%20Method%20for%20Enhanced%20Low-Light%20Image%0A%20%20Clarity&entry.906535625=Shuang%20Wang%20and%20Qianwen%20Lu%20and%20Yihe%20Nie%20and%20Qingchuan%20Tao%20and%20Yanmei%20Yu&entry.1292438233=%20%20For%20the%20task%20of%20low-light%20image%20enhancement%2C%20deep%20learning-based%20algorithms%0Ahave%20demonstrated%20superiority%20and%20effectiveness%20compared%20to%20traditional%0Amethods.%20Existing%20deep%20learning%20algorithms%20are%20proposed%20mainly%20based%20on%20the%0ARetinex%20theory%20but%20overlook%20the%20noise%20and%20color%20distortion%20present%20in%20the%0Ainput%2C%20which%20frequently%20results%20in%20significant%20noise%20amplification%20and%20local%0Acolor%20distortion%20in%20the%20final%20results.%20To%20address%20this%2C%20we%20propose%20a%20Dual-Path%0AError%20Compensation%20method%20%28DPEC%29%2C%20which%20aims%20to%20improve%20image%20quality%20in%0Alow-light%20conditions.%20DPEC%20performs%20precise%20pixel-level%20error%20estimation%2C%20which%0Aaccurately%20captures%20subtle%20pixels%20differences%2C%20and%20independent%20denoising%2C%20which%0Aeffectively%20removes%20unnecessary%20noise.%20This%20method%20restores%20image%20brightness%0Awhile%20preserving%20local%20texture%20details%20and%20avoiding%20noise%20amplification.%0AFurthermore%2C%20to%20compensate%20for%20the%20traditional%20CNN%27s%20limited%20ability%20to%20capture%0Along-range%20semantic%20information%20and%20considering%20both%20computational%20speed%20and%0Aresource%20efficiency%2C%20we%20integrated%20the%20VMamba%20architecture%20into%20the%20backbone%20of%0ADPEC.%20In%20addition%2C%20we%20introduced%20the%20HIS-Retinex%20loss%20to%20constrain%20the%20training%0Aof%20DPEC%2C%20ensuring%20that%20the%20overall%20brightness%20distribution%20of%20the%20images%20more%0Aclosely%20aligns%20with%20real-world%20conditions.%20Comprehensive%20quantitative%20and%0Aqualitative%20experimental%20results%20demonstrate%20that%20our%20algorithm%20significantly%0Aoutperforms%20state-of-the-art%20methods%20across%20six%20benchmark%20tests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09553v3&entry.124074799=Read"},
{"title": "Pedestrian motion prediction evaluation for urban autonomous driving", "author": "Dmytro Zabolotnii and Yar Muhammad and Naveed Muhammad", "abstract": "  Pedestrian motion prediction is a key part of the modular-based autonomous\ndriving pipeline, ensuring safe, accurate, and timely awareness of human\nagents' possible future trajectories. The autonomous vehicle can use this\ninformation to prevent any possible accidents and create a comfortable and\npleasant driving experience for the passengers and pedestrians. A wealth of\nresearch was done on the topic from the authors of robotics, computer vision,\nintelligent transportation systems, and other fields. However, a relatively\nunexplored angle is the integration of the state-of-art solutions into existing\nautonomous driving stacks and evaluating them in real-life conditions rather\nthan sanitized datasets. We analyze selected publications with provided\nopen-source solutions and provide a perspective obtained by integrating them\ninto existing Autonomous Driving framework - Autoware Mini and performing\nexperiments in natural urban conditions in Tartu, Estonia to determine\nvaluability of traditional motion prediction metrics. This perspective should\nbe valuable to any potential autonomous driving or robotics engineer looking\nfor the real-world performance of the existing state-of-art pedestrian motion\nprediction problem. The code with instructions on accessing the dataset is\navailable at https://github.com/dmytrozabolotnii/autoware_mini.\n", "link": "http://arxiv.org/abs/2410.16864v1", "date": "2024-10-22", "relevancy": 2.2468, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6117}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5747}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pedestrian%20motion%20prediction%20evaluation%20for%20urban%20autonomous%20driving&body=Title%3A%20Pedestrian%20motion%20prediction%20evaluation%20for%20urban%20autonomous%20driving%0AAuthor%3A%20Dmytro%20Zabolotnii%20and%20Yar%20Muhammad%20and%20Naveed%20Muhammad%0AAbstract%3A%20%20%20Pedestrian%20motion%20prediction%20is%20a%20key%20part%20of%20the%20modular-based%20autonomous%0Adriving%20pipeline%2C%20ensuring%20safe%2C%20accurate%2C%20and%20timely%20awareness%20of%20human%0Aagents%27%20possible%20future%20trajectories.%20The%20autonomous%20vehicle%20can%20use%20this%0Ainformation%20to%20prevent%20any%20possible%20accidents%20and%20create%20a%20comfortable%20and%0Apleasant%20driving%20experience%20for%20the%20passengers%20and%20pedestrians.%20A%20wealth%20of%0Aresearch%20was%20done%20on%20the%20topic%20from%20the%20authors%20of%20robotics%2C%20computer%20vision%2C%0Aintelligent%20transportation%20systems%2C%20and%20other%20fields.%20However%2C%20a%20relatively%0Aunexplored%20angle%20is%20the%20integration%20of%20the%20state-of-art%20solutions%20into%20existing%0Aautonomous%20driving%20stacks%20and%20evaluating%20them%20in%20real-life%20conditions%20rather%0Athan%20sanitized%20datasets.%20We%20analyze%20selected%20publications%20with%20provided%0Aopen-source%20solutions%20and%20provide%20a%20perspective%20obtained%20by%20integrating%20them%0Ainto%20existing%20Autonomous%20Driving%20framework%20-%20Autoware%20Mini%20and%20performing%0Aexperiments%20in%20natural%20urban%20conditions%20in%20Tartu%2C%20Estonia%20to%20determine%0Avaluability%20of%20traditional%20motion%20prediction%20metrics.%20This%20perspective%20should%0Abe%20valuable%20to%20any%20potential%20autonomous%20driving%20or%20robotics%20engineer%20looking%0Afor%20the%20real-world%20performance%20of%20the%20existing%20state-of-art%20pedestrian%20motion%0Aprediction%20problem.%20The%20code%20with%20instructions%20on%20accessing%20the%20dataset%20is%0Aavailable%20at%20https%3A//github.com/dmytrozabolotnii/autoware_mini.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPedestrian%2520motion%2520prediction%2520evaluation%2520for%2520urban%2520autonomous%2520driving%26entry.906535625%3DDmytro%2520Zabolotnii%2520and%2520Yar%2520Muhammad%2520and%2520Naveed%2520Muhammad%26entry.1292438233%3D%2520%2520Pedestrian%2520motion%2520prediction%2520is%2520a%2520key%2520part%2520of%2520the%2520modular-based%2520autonomous%250Adriving%2520pipeline%252C%2520ensuring%2520safe%252C%2520accurate%252C%2520and%2520timely%2520awareness%2520of%2520human%250Aagents%2527%2520possible%2520future%2520trajectories.%2520The%2520autonomous%2520vehicle%2520can%2520use%2520this%250Ainformation%2520to%2520prevent%2520any%2520possible%2520accidents%2520and%2520create%2520a%2520comfortable%2520and%250Apleasant%2520driving%2520experience%2520for%2520the%2520passengers%2520and%2520pedestrians.%2520A%2520wealth%2520of%250Aresearch%2520was%2520done%2520on%2520the%2520topic%2520from%2520the%2520authors%2520of%2520robotics%252C%2520computer%2520vision%252C%250Aintelligent%2520transportation%2520systems%252C%2520and%2520other%2520fields.%2520However%252C%2520a%2520relatively%250Aunexplored%2520angle%2520is%2520the%2520integration%2520of%2520the%2520state-of-art%2520solutions%2520into%2520existing%250Aautonomous%2520driving%2520stacks%2520and%2520evaluating%2520them%2520in%2520real-life%2520conditions%2520rather%250Athan%2520sanitized%2520datasets.%2520We%2520analyze%2520selected%2520publications%2520with%2520provided%250Aopen-source%2520solutions%2520and%2520provide%2520a%2520perspective%2520obtained%2520by%2520integrating%2520them%250Ainto%2520existing%2520Autonomous%2520Driving%2520framework%2520-%2520Autoware%2520Mini%2520and%2520performing%250Aexperiments%2520in%2520natural%2520urban%2520conditions%2520in%2520Tartu%252C%2520Estonia%2520to%2520determine%250Avaluability%2520of%2520traditional%2520motion%2520prediction%2520metrics.%2520This%2520perspective%2520should%250Abe%2520valuable%2520to%2520any%2520potential%2520autonomous%2520driving%2520or%2520robotics%2520engineer%2520looking%250Afor%2520the%2520real-world%2520performance%2520of%2520the%2520existing%2520state-of-art%2520pedestrian%2520motion%250Aprediction%2520problem.%2520The%2520code%2520with%2520instructions%2520on%2520accessing%2520the%2520dataset%2520is%250Aavailable%2520at%2520https%253A//github.com/dmytrozabolotnii/autoware_mini.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pedestrian%20motion%20prediction%20evaluation%20for%20urban%20autonomous%20driving&entry.906535625=Dmytro%20Zabolotnii%20and%20Yar%20Muhammad%20and%20Naveed%20Muhammad&entry.1292438233=%20%20Pedestrian%20motion%20prediction%20is%20a%20key%20part%20of%20the%20modular-based%20autonomous%0Adriving%20pipeline%2C%20ensuring%20safe%2C%20accurate%2C%20and%20timely%20awareness%20of%20human%0Aagents%27%20possible%20future%20trajectories.%20The%20autonomous%20vehicle%20can%20use%20this%0Ainformation%20to%20prevent%20any%20possible%20accidents%20and%20create%20a%20comfortable%20and%0Apleasant%20driving%20experience%20for%20the%20passengers%20and%20pedestrians.%20A%20wealth%20of%0Aresearch%20was%20done%20on%20the%20topic%20from%20the%20authors%20of%20robotics%2C%20computer%20vision%2C%0Aintelligent%20transportation%20systems%2C%20and%20other%20fields.%20However%2C%20a%20relatively%0Aunexplored%20angle%20is%20the%20integration%20of%20the%20state-of-art%20solutions%20into%20existing%0Aautonomous%20driving%20stacks%20and%20evaluating%20them%20in%20real-life%20conditions%20rather%0Athan%20sanitized%20datasets.%20We%20analyze%20selected%20publications%20with%20provided%0Aopen-source%20solutions%20and%20provide%20a%20perspective%20obtained%20by%20integrating%20them%0Ainto%20existing%20Autonomous%20Driving%20framework%20-%20Autoware%20Mini%20and%20performing%0Aexperiments%20in%20natural%20urban%20conditions%20in%20Tartu%2C%20Estonia%20to%20determine%0Avaluability%20of%20traditional%20motion%20prediction%20metrics.%20This%20perspective%20should%0Abe%20valuable%20to%20any%20potential%20autonomous%20driving%20or%20robotics%20engineer%20looking%0Afor%20the%20real-world%20performance%20of%20the%20existing%20state-of-art%20pedestrian%20motion%0Aprediction%20problem.%20The%20code%20with%20instructions%20on%20accessing%20the%20dataset%20is%0Aavailable%20at%20https%3A//github.com/dmytrozabolotnii/autoware_mini.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16864v1&entry.124074799=Read"},
{"title": "Can General-Purpose Large Language Models Generalize to English-Thai\n  Machine Translation ?", "author": "Jirat Chiaranaipanich and Naiyarat Hanmatheekuna and Jitkapat Sawatphol and Krittamate Tiankanon and Jiramet Kinchagawat and Amrest Chinkamol and Parinthapat Pengpun and Piyalitt Ittichaiwong and Peerat Limkonchotiwat", "abstract": "  Large language models (LLMs) perform well on common tasks but struggle with\ngeneralization in low-resource and low-computation settings. We examine this\nlimitation by testing various LLMs and specialized translation models on\nEnglish-Thai machine translation and code-switching datasets. Our findings\nreveal that under more strict computational constraints, such as 4-bit\nquantization, LLMs fail to translate effectively. In contrast, specialized\nmodels, with comparable or lower computational requirements, consistently\noutperform LLMs. This underscores the importance of specialized models for\nmaintaining performance under resource constraints.\n", "link": "http://arxiv.org/abs/2410.17145v1", "date": "2024-10-22", "relevancy": 2.2322, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20General-Purpose%20Large%20Language%20Models%20Generalize%20to%20English-Thai%0A%20%20Machine%20Translation%20%3F&body=Title%3A%20Can%20General-Purpose%20Large%20Language%20Models%20Generalize%20to%20English-Thai%0A%20%20Machine%20Translation%20%3F%0AAuthor%3A%20Jirat%20Chiaranaipanich%20and%20Naiyarat%20Hanmatheekuna%20and%20Jitkapat%20Sawatphol%20and%20Krittamate%20Tiankanon%20and%20Jiramet%20Kinchagawat%20and%20Amrest%20Chinkamol%20and%20Parinthapat%20Pengpun%20and%20Piyalitt%20Ittichaiwong%20and%20Peerat%20Limkonchotiwat%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20perform%20well%20on%20common%20tasks%20but%20struggle%20with%0Ageneralization%20in%20low-resource%20and%20low-computation%20settings.%20We%20examine%20this%0Alimitation%20by%20testing%20various%20LLMs%20and%20specialized%20translation%20models%20on%0AEnglish-Thai%20machine%20translation%20and%20code-switching%20datasets.%20Our%20findings%0Areveal%20that%20under%20more%20strict%20computational%20constraints%2C%20such%20as%204-bit%0Aquantization%2C%20LLMs%20fail%20to%20translate%20effectively.%20In%20contrast%2C%20specialized%0Amodels%2C%20with%20comparable%20or%20lower%20computational%20requirements%2C%20consistently%0Aoutperform%20LLMs.%20This%20underscores%20the%20importance%20of%20specialized%20models%20for%0Amaintaining%20performance%20under%20resource%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520General-Purpose%2520Large%2520Language%2520Models%2520Generalize%2520to%2520English-Thai%250A%2520%2520Machine%2520Translation%2520%253F%26entry.906535625%3DJirat%2520Chiaranaipanich%2520and%2520Naiyarat%2520Hanmatheekuna%2520and%2520Jitkapat%2520Sawatphol%2520and%2520Krittamate%2520Tiankanon%2520and%2520Jiramet%2520Kinchagawat%2520and%2520Amrest%2520Chinkamol%2520and%2520Parinthapat%2520Pengpun%2520and%2520Piyalitt%2520Ittichaiwong%2520and%2520Peerat%2520Limkonchotiwat%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520perform%2520well%2520on%2520common%2520tasks%2520but%2520struggle%2520with%250Ageneralization%2520in%2520low-resource%2520and%2520low-computation%2520settings.%2520We%2520examine%2520this%250Alimitation%2520by%2520testing%2520various%2520LLMs%2520and%2520specialized%2520translation%2520models%2520on%250AEnglish-Thai%2520machine%2520translation%2520and%2520code-switching%2520datasets.%2520Our%2520findings%250Areveal%2520that%2520under%2520more%2520strict%2520computational%2520constraints%252C%2520such%2520as%25204-bit%250Aquantization%252C%2520LLMs%2520fail%2520to%2520translate%2520effectively.%2520In%2520contrast%252C%2520specialized%250Amodels%252C%2520with%2520comparable%2520or%2520lower%2520computational%2520requirements%252C%2520consistently%250Aoutperform%2520LLMs.%2520This%2520underscores%2520the%2520importance%2520of%2520specialized%2520models%2520for%250Amaintaining%2520performance%2520under%2520resource%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20General-Purpose%20Large%20Language%20Models%20Generalize%20to%20English-Thai%0A%20%20Machine%20Translation%20%3F&entry.906535625=Jirat%20Chiaranaipanich%20and%20Naiyarat%20Hanmatheekuna%20and%20Jitkapat%20Sawatphol%20and%20Krittamate%20Tiankanon%20and%20Jiramet%20Kinchagawat%20and%20Amrest%20Chinkamol%20and%20Parinthapat%20Pengpun%20and%20Piyalitt%20Ittichaiwong%20and%20Peerat%20Limkonchotiwat&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20perform%20well%20on%20common%20tasks%20but%20struggle%20with%0Ageneralization%20in%20low-resource%20and%20low-computation%20settings.%20We%20examine%20this%0Alimitation%20by%20testing%20various%20LLMs%20and%20specialized%20translation%20models%20on%0AEnglish-Thai%20machine%20translation%20and%20code-switching%20datasets.%20Our%20findings%0Areveal%20that%20under%20more%20strict%20computational%20constraints%2C%20such%20as%204-bit%0Aquantization%2C%20LLMs%20fail%20to%20translate%20effectively.%20In%20contrast%2C%20specialized%0Amodels%2C%20with%20comparable%20or%20lower%20computational%20requirements%2C%20consistently%0Aoutperform%20LLMs.%20This%20underscores%20the%20importance%20of%20specialized%20models%20for%0Amaintaining%20performance%20under%20resource%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17145v1&entry.124074799=Read"},
{"title": "Consistent Distributed Cooperative Localization: A Coordinate\n  Transformation Approach", "author": "Chungeng Tian and Ning Hao and Fenghua He and Haodi Yao", "abstract": "  This paper addresses the consistency issue of multi-robot distributed\ncooperative localization. We introduce a consistent distributed cooperative\nlocalization algorithm conducting state estimation in a transformed coordinate.\nThe core idea involves a linear time-varying coordinated transformation to\nrender the propagation Jacobian independent of the state and make it suitable\nfor a distributed manner. This transformation is seamlessly integrated into a\nserver-based distributed cooperative localization framework, in which each\nrobot estimates its own state while the server maintains the\ncross-correlations. The transformation ensures the correct observability\nproperty of the entire framework. Moreover, the algorithm accommodates various\ntypes of robot-to-robot relative measurements, broadening its applicability.\nThrough simulations and real-world dataset experiments, the proposed algorithm\nhas demonstrated better performance in terms of both consistency and accuracy\ncompared to existing algorithms.\n", "link": "http://arxiv.org/abs/2303.01205v2", "date": "2024-10-22", "relevancy": 2.2211, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5671}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5482}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Distributed%20Cooperative%20Localization%3A%20A%20Coordinate%0A%20%20Transformation%20Approach&body=Title%3A%20Consistent%20Distributed%20Cooperative%20Localization%3A%20A%20Coordinate%0A%20%20Transformation%20Approach%0AAuthor%3A%20Chungeng%20Tian%20and%20Ning%20Hao%20and%20Fenghua%20He%20and%20Haodi%20Yao%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20consistency%20issue%20of%20multi-robot%20distributed%0Acooperative%20localization.%20We%20introduce%20a%20consistent%20distributed%20cooperative%0Alocalization%20algorithm%20conducting%20state%20estimation%20in%20a%20transformed%20coordinate.%0AThe%20core%20idea%20involves%20a%20linear%20time-varying%20coordinated%20transformation%20to%0Arender%20the%20propagation%20Jacobian%20independent%20of%20the%20state%20and%20make%20it%20suitable%0Afor%20a%20distributed%20manner.%20This%20transformation%20is%20seamlessly%20integrated%20into%20a%0Aserver-based%20distributed%20cooperative%20localization%20framework%2C%20in%20which%20each%0Arobot%20estimates%20its%20own%20state%20while%20the%20server%20maintains%20the%0Across-correlations.%20The%20transformation%20ensures%20the%20correct%20observability%0Aproperty%20of%20the%20entire%20framework.%20Moreover%2C%20the%20algorithm%20accommodates%20various%0Atypes%20of%20robot-to-robot%20relative%20measurements%2C%20broadening%20its%20applicability.%0AThrough%20simulations%20and%20real-world%20dataset%20experiments%2C%20the%20proposed%20algorithm%0Ahas%20demonstrated%20better%20performance%20in%20terms%20of%20both%20consistency%20and%20accuracy%0Acompared%20to%20existing%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.01205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Distributed%2520Cooperative%2520Localization%253A%2520A%2520Coordinate%250A%2520%2520Transformation%2520Approach%26entry.906535625%3DChungeng%2520Tian%2520and%2520Ning%2520Hao%2520and%2520Fenghua%2520He%2520and%2520Haodi%2520Yao%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520consistency%2520issue%2520of%2520multi-robot%2520distributed%250Acooperative%2520localization.%2520We%2520introduce%2520a%2520consistent%2520distributed%2520cooperative%250Alocalization%2520algorithm%2520conducting%2520state%2520estimation%2520in%2520a%2520transformed%2520coordinate.%250AThe%2520core%2520idea%2520involves%2520a%2520linear%2520time-varying%2520coordinated%2520transformation%2520to%250Arender%2520the%2520propagation%2520Jacobian%2520independent%2520of%2520the%2520state%2520and%2520make%2520it%2520suitable%250Afor%2520a%2520distributed%2520manner.%2520This%2520transformation%2520is%2520seamlessly%2520integrated%2520into%2520a%250Aserver-based%2520distributed%2520cooperative%2520localization%2520framework%252C%2520in%2520which%2520each%250Arobot%2520estimates%2520its%2520own%2520state%2520while%2520the%2520server%2520maintains%2520the%250Across-correlations.%2520The%2520transformation%2520ensures%2520the%2520correct%2520observability%250Aproperty%2520of%2520the%2520entire%2520framework.%2520Moreover%252C%2520the%2520algorithm%2520accommodates%2520various%250Atypes%2520of%2520robot-to-robot%2520relative%2520measurements%252C%2520broadening%2520its%2520applicability.%250AThrough%2520simulations%2520and%2520real-world%2520dataset%2520experiments%252C%2520the%2520proposed%2520algorithm%250Ahas%2520demonstrated%2520better%2520performance%2520in%2520terms%2520of%2520both%2520consistency%2520and%2520accuracy%250Acompared%2520to%2520existing%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.01205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Distributed%20Cooperative%20Localization%3A%20A%20Coordinate%0A%20%20Transformation%20Approach&entry.906535625=Chungeng%20Tian%20and%20Ning%20Hao%20and%20Fenghua%20He%20and%20Haodi%20Yao&entry.1292438233=%20%20This%20paper%20addresses%20the%20consistency%20issue%20of%20multi-robot%20distributed%0Acooperative%20localization.%20We%20introduce%20a%20consistent%20distributed%20cooperative%0Alocalization%20algorithm%20conducting%20state%20estimation%20in%20a%20transformed%20coordinate.%0AThe%20core%20idea%20involves%20a%20linear%20time-varying%20coordinated%20transformation%20to%0Arender%20the%20propagation%20Jacobian%20independent%20of%20the%20state%20and%20make%20it%20suitable%0Afor%20a%20distributed%20manner.%20This%20transformation%20is%20seamlessly%20integrated%20into%20a%0Aserver-based%20distributed%20cooperative%20localization%20framework%2C%20in%20which%20each%0Arobot%20estimates%20its%20own%20state%20while%20the%20server%20maintains%20the%0Across-correlations.%20The%20transformation%20ensures%20the%20correct%20observability%0Aproperty%20of%20the%20entire%20framework.%20Moreover%2C%20the%20algorithm%20accommodates%20various%0Atypes%20of%20robot-to-robot%20relative%20measurements%2C%20broadening%20its%20applicability.%0AThrough%20simulations%20and%20real-world%20dataset%20experiments%2C%20the%20proposed%20algorithm%0Ahas%20demonstrated%20better%20performance%20in%20terms%20of%20both%20consistency%20and%20accuracy%0Acompared%20to%20existing%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.01205v2&entry.124074799=Read"},
{"title": "Towards Real Zero-Shot Camouflaged Object Segmentation without\n  Camouflaged Annotations", "author": "Cheng Lei and Jie Fan and Xinran Li and Tianzhu Xiang and Ao Li and Ce Zhu and Le Zhang", "abstract": "  Camouflaged Object Segmentation (COS) faces significant challenges due to the\nscarcity of annotated data, where meticulous pixel-level annotation is both\nlabor-intensive and costly, primarily due to the intricate object-background\nboundaries. Addressing the core question, \"Can COS be effectively achieved in a\nzero-shot manner without manual annotations for any camouflaged object?\" we\naffirmatively respond and introduce a robust zero-shot COS framework. This\nframework leverages the inherent local pattern bias of COS and employs a broad\nsemantic feature space derived from salient object segmentation (SOS) for\nefficient zero-shot transfer. We incorporate an Masked Image Modeling (MIM)\nbased image encoder optimized for Parameter-Efficient Fine-Tuning (PEFT), a\nMultimodal Large Language Model (M-LLM), and a Multi-scale Fine-grained\nAlignment (MFA) mechanism. The MIM pre-trained image encoder focuses on\ncapturing essential low-level features, while the M-LLM generates caption\nembeddings processed alongside these visual cues. These embeddings are\nprecisely aligned using MFA, enabling our framework to accurately interpret and\nnavigate complex semantic contexts. To optimize operational efficiency, we\nintroduce a learnable codebook that represents the M-LLM during inference,\nsignificantly reducing computational overhead. Our framework demonstrates its\nversatility and efficacy through rigorous experimentation, achieving\nstate-of-the-art performance in zero-shot COS with $F_{\\beta}^w$ scores of\n72.9\\% on CAMO and 71.7\\% on COD10K. By removing the M-LLM during inference, we\nachieve an inference speed comparable to that of traditional end-to-end models,\nreaching 18.1 FPS. Code: https://github.com/R-LEI360725/ZSCOS-CaMF\n", "link": "http://arxiv.org/abs/2410.16953v1", "date": "2024-10-22", "relevancy": 2.2207, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5588}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5581}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Real%20Zero-Shot%20Camouflaged%20Object%20Segmentation%20without%0A%20%20Camouflaged%20Annotations&body=Title%3A%20Towards%20Real%20Zero-Shot%20Camouflaged%20Object%20Segmentation%20without%0A%20%20Camouflaged%20Annotations%0AAuthor%3A%20Cheng%20Lei%20and%20Jie%20Fan%20and%20Xinran%20Li%20and%20Tianzhu%20Xiang%20and%20Ao%20Li%20and%20Ce%20Zhu%20and%20Le%20Zhang%0AAbstract%3A%20%20%20Camouflaged%20Object%20Segmentation%20%28COS%29%20faces%20significant%20challenges%20due%20to%20the%0Ascarcity%20of%20annotated%20data%2C%20where%20meticulous%20pixel-level%20annotation%20is%20both%0Alabor-intensive%20and%20costly%2C%20primarily%20due%20to%20the%20intricate%20object-background%0Aboundaries.%20Addressing%20the%20core%20question%2C%20%22Can%20COS%20be%20effectively%20achieved%20in%20a%0Azero-shot%20manner%20without%20manual%20annotations%20for%20any%20camouflaged%20object%3F%22%20we%0Aaffirmatively%20respond%20and%20introduce%20a%20robust%20zero-shot%20COS%20framework.%20This%0Aframework%20leverages%20the%20inherent%20local%20pattern%20bias%20of%20COS%20and%20employs%20a%20broad%0Asemantic%20feature%20space%20derived%20from%20salient%20object%20segmentation%20%28SOS%29%20for%0Aefficient%20zero-shot%20transfer.%20We%20incorporate%20an%20Masked%20Image%20Modeling%20%28MIM%29%0Abased%20image%20encoder%20optimized%20for%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%2C%20a%0AMultimodal%20Large%20Language%20Model%20%28M-LLM%29%2C%20and%20a%20Multi-scale%20Fine-grained%0AAlignment%20%28MFA%29%20mechanism.%20The%20MIM%20pre-trained%20image%20encoder%20focuses%20on%0Acapturing%20essential%20low-level%20features%2C%20while%20the%20M-LLM%20generates%20caption%0Aembeddings%20processed%20alongside%20these%20visual%20cues.%20These%20embeddings%20are%0Aprecisely%20aligned%20using%20MFA%2C%20enabling%20our%20framework%20to%20accurately%20interpret%20and%0Anavigate%20complex%20semantic%20contexts.%20To%20optimize%20operational%20efficiency%2C%20we%0Aintroduce%20a%20learnable%20codebook%20that%20represents%20the%20M-LLM%20during%20inference%2C%0Asignificantly%20reducing%20computational%20overhead.%20Our%20framework%20demonstrates%20its%0Aversatility%20and%20efficacy%20through%20rigorous%20experimentation%2C%20achieving%0Astate-of-the-art%20performance%20in%20zero-shot%20COS%20with%20%24F_%7B%5Cbeta%7D%5Ew%24%20scores%20of%0A72.9%5C%25%20on%20CAMO%20and%2071.7%5C%25%20on%20COD10K.%20By%20removing%20the%20M-LLM%20during%20inference%2C%20we%0Aachieve%20an%20inference%20speed%20comparable%20to%20that%20of%20traditional%20end-to-end%20models%2C%0Areaching%2018.1%20FPS.%20Code%3A%20https%3A//github.com/R-LEI360725/ZSCOS-CaMF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Real%2520Zero-Shot%2520Camouflaged%2520Object%2520Segmentation%2520without%250A%2520%2520Camouflaged%2520Annotations%26entry.906535625%3DCheng%2520Lei%2520and%2520Jie%2520Fan%2520and%2520Xinran%2520Li%2520and%2520Tianzhu%2520Xiang%2520and%2520Ao%2520Li%2520and%2520Ce%2520Zhu%2520and%2520Le%2520Zhang%26entry.1292438233%3D%2520%2520Camouflaged%2520Object%2520Segmentation%2520%2528COS%2529%2520faces%2520significant%2520challenges%2520due%2520to%2520the%250Ascarcity%2520of%2520annotated%2520data%252C%2520where%2520meticulous%2520pixel-level%2520annotation%2520is%2520both%250Alabor-intensive%2520and%2520costly%252C%2520primarily%2520due%2520to%2520the%2520intricate%2520object-background%250Aboundaries.%2520Addressing%2520the%2520core%2520question%252C%2520%2522Can%2520COS%2520be%2520effectively%2520achieved%2520in%2520a%250Azero-shot%2520manner%2520without%2520manual%2520annotations%2520for%2520any%2520camouflaged%2520object%253F%2522%2520we%250Aaffirmatively%2520respond%2520and%2520introduce%2520a%2520robust%2520zero-shot%2520COS%2520framework.%2520This%250Aframework%2520leverages%2520the%2520inherent%2520local%2520pattern%2520bias%2520of%2520COS%2520and%2520employs%2520a%2520broad%250Asemantic%2520feature%2520space%2520derived%2520from%2520salient%2520object%2520segmentation%2520%2528SOS%2529%2520for%250Aefficient%2520zero-shot%2520transfer.%2520We%2520incorporate%2520an%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%250Abased%2520image%2520encoder%2520optimized%2520for%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%252C%2520a%250AMultimodal%2520Large%2520Language%2520Model%2520%2528M-LLM%2529%252C%2520and%2520a%2520Multi-scale%2520Fine-grained%250AAlignment%2520%2528MFA%2529%2520mechanism.%2520The%2520MIM%2520pre-trained%2520image%2520encoder%2520focuses%2520on%250Acapturing%2520essential%2520low-level%2520features%252C%2520while%2520the%2520M-LLM%2520generates%2520caption%250Aembeddings%2520processed%2520alongside%2520these%2520visual%2520cues.%2520These%2520embeddings%2520are%250Aprecisely%2520aligned%2520using%2520MFA%252C%2520enabling%2520our%2520framework%2520to%2520accurately%2520interpret%2520and%250Anavigate%2520complex%2520semantic%2520contexts.%2520To%2520optimize%2520operational%2520efficiency%252C%2520we%250Aintroduce%2520a%2520learnable%2520codebook%2520that%2520represents%2520the%2520M-LLM%2520during%2520inference%252C%250Asignificantly%2520reducing%2520computational%2520overhead.%2520Our%2520framework%2520demonstrates%2520its%250Aversatility%2520and%2520efficacy%2520through%2520rigorous%2520experimentation%252C%2520achieving%250Astate-of-the-art%2520performance%2520in%2520zero-shot%2520COS%2520with%2520%2524F_%257B%255Cbeta%257D%255Ew%2524%2520scores%2520of%250A72.9%255C%2525%2520on%2520CAMO%2520and%252071.7%255C%2525%2520on%2520COD10K.%2520By%2520removing%2520the%2520M-LLM%2520during%2520inference%252C%2520we%250Aachieve%2520an%2520inference%2520speed%2520comparable%2520to%2520that%2520of%2520traditional%2520end-to-end%2520models%252C%250Areaching%252018.1%2520FPS.%2520Code%253A%2520https%253A//github.com/R-LEI360725/ZSCOS-CaMF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Real%20Zero-Shot%20Camouflaged%20Object%20Segmentation%20without%0A%20%20Camouflaged%20Annotations&entry.906535625=Cheng%20Lei%20and%20Jie%20Fan%20and%20Xinran%20Li%20and%20Tianzhu%20Xiang%20and%20Ao%20Li%20and%20Ce%20Zhu%20and%20Le%20Zhang&entry.1292438233=%20%20Camouflaged%20Object%20Segmentation%20%28COS%29%20faces%20significant%20challenges%20due%20to%20the%0Ascarcity%20of%20annotated%20data%2C%20where%20meticulous%20pixel-level%20annotation%20is%20both%0Alabor-intensive%20and%20costly%2C%20primarily%20due%20to%20the%20intricate%20object-background%0Aboundaries.%20Addressing%20the%20core%20question%2C%20%22Can%20COS%20be%20effectively%20achieved%20in%20a%0Azero-shot%20manner%20without%20manual%20annotations%20for%20any%20camouflaged%20object%3F%22%20we%0Aaffirmatively%20respond%20and%20introduce%20a%20robust%20zero-shot%20COS%20framework.%20This%0Aframework%20leverages%20the%20inherent%20local%20pattern%20bias%20of%20COS%20and%20employs%20a%20broad%0Asemantic%20feature%20space%20derived%20from%20salient%20object%20segmentation%20%28SOS%29%20for%0Aefficient%20zero-shot%20transfer.%20We%20incorporate%20an%20Masked%20Image%20Modeling%20%28MIM%29%0Abased%20image%20encoder%20optimized%20for%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%2C%20a%0AMultimodal%20Large%20Language%20Model%20%28M-LLM%29%2C%20and%20a%20Multi-scale%20Fine-grained%0AAlignment%20%28MFA%29%20mechanism.%20The%20MIM%20pre-trained%20image%20encoder%20focuses%20on%0Acapturing%20essential%20low-level%20features%2C%20while%20the%20M-LLM%20generates%20caption%0Aembeddings%20processed%20alongside%20these%20visual%20cues.%20These%20embeddings%20are%0Aprecisely%20aligned%20using%20MFA%2C%20enabling%20our%20framework%20to%20accurately%20interpret%20and%0Anavigate%20complex%20semantic%20contexts.%20To%20optimize%20operational%20efficiency%2C%20we%0Aintroduce%20a%20learnable%20codebook%20that%20represents%20the%20M-LLM%20during%20inference%2C%0Asignificantly%20reducing%20computational%20overhead.%20Our%20framework%20demonstrates%20its%0Aversatility%20and%20efficacy%20through%20rigorous%20experimentation%2C%20achieving%0Astate-of-the-art%20performance%20in%20zero-shot%20COS%20with%20%24F_%7B%5Cbeta%7D%5Ew%24%20scores%20of%0A72.9%5C%25%20on%20CAMO%20and%2071.7%5C%25%20on%20COD10K.%20By%20removing%20the%20M-LLM%20during%20inference%2C%20we%0Aachieve%20an%20inference%20speed%20comparable%20to%20that%20of%20traditional%20end-to-end%20models%2C%0Areaching%2018.1%20FPS.%20Code%3A%20https%3A//github.com/R-LEI360725/ZSCOS-CaMF%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16953v1&entry.124074799=Read"},
{"title": "Direction-Constrained Control for Efficient Physical Human-Robot\n  Interaction under Hierarchical Tasks", "author": "Mengxin Xu and Weiwei Wan and Hesheng Wang and Kensuke Harada", "abstract": "  This paper proposes a control method to address the physical Human-Robot\nInteraction (pHRI) challenge in the context of hierarchical tasks. A common\napproach to managing hierarchical tasks is Hierarchical Quadratic Programming\n(HQP), which, however, cannot be directly applied to human interaction due to\nits allowance of arbitrary velocity direction adjustments. To resolve this\nlimitation, we introduce the concept of directional constraints and develop a\ndirection-constrained optimization algorithm to handle the nonlinearities\ninduced by these constraints. The algorithm solves two sub-problems, minimizing\nthe error and minimizing the deviation angle, in parallel, and combines the\nresults of the two sub-problems to produce a final optimal outcome. The mutual\ninfluence between these two sub-problems is analyzed to determine the best\nparameter for combination. Additionally, the velocity objective in our control\nframework is computed using a variable admittance controller. Traditional\nadmittance control does not account for constraints. To address this issue, we\npropose a variable admittance control method to adjust control objectives\ndynamically. The method helps reduce the deviation between robot velocity and\nhuman intention at the constraint boundaries, thereby enhancing interaction\nefficiency. We evaluate the proposed method in scenarios where a human operator\nphysically interacts with a 7-degree-of-freedom robotic arm. The results\nhighlight the importance of incorporating directional constraints in pHRI for\nhierarchical tasks. Compared to existing methods, our approach generates\nsmoother robotic trajectories during interaction while avoiding interaction\ndelays at the constraint boundaries.\n", "link": "http://arxiv.org/abs/2410.16922v1", "date": "2024-10-22", "relevancy": 2.211, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5649}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5509}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direction-Constrained%20Control%20for%20Efficient%20Physical%20Human-Robot%0A%20%20Interaction%20under%20Hierarchical%20Tasks&body=Title%3A%20Direction-Constrained%20Control%20for%20Efficient%20Physical%20Human-Robot%0A%20%20Interaction%20under%20Hierarchical%20Tasks%0AAuthor%3A%20Mengxin%20Xu%20and%20Weiwei%20Wan%20and%20Hesheng%20Wang%20and%20Kensuke%20Harada%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20control%20method%20to%20address%20the%20physical%20Human-Robot%0AInteraction%20%28pHRI%29%20challenge%20in%20the%20context%20of%20hierarchical%20tasks.%20A%20common%0Aapproach%20to%20managing%20hierarchical%20tasks%20is%20Hierarchical%20Quadratic%20Programming%0A%28HQP%29%2C%20which%2C%20however%2C%20cannot%20be%20directly%20applied%20to%20human%20interaction%20due%20to%0Aits%20allowance%20of%20arbitrary%20velocity%20direction%20adjustments.%20To%20resolve%20this%0Alimitation%2C%20we%20introduce%20the%20concept%20of%20directional%20constraints%20and%20develop%20a%0Adirection-constrained%20optimization%20algorithm%20to%20handle%20the%20nonlinearities%0Ainduced%20by%20these%20constraints.%20The%20algorithm%20solves%20two%20sub-problems%2C%20minimizing%0Athe%20error%20and%20minimizing%20the%20deviation%20angle%2C%20in%20parallel%2C%20and%20combines%20the%0Aresults%20of%20the%20two%20sub-problems%20to%20produce%20a%20final%20optimal%20outcome.%20The%20mutual%0Ainfluence%20between%20these%20two%20sub-problems%20is%20analyzed%20to%20determine%20the%20best%0Aparameter%20for%20combination.%20Additionally%2C%20the%20velocity%20objective%20in%20our%20control%0Aframework%20is%20computed%20using%20a%20variable%20admittance%20controller.%20Traditional%0Aadmittance%20control%20does%20not%20account%20for%20constraints.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20variable%20admittance%20control%20method%20to%20adjust%20control%20objectives%0Adynamically.%20The%20method%20helps%20reduce%20the%20deviation%20between%20robot%20velocity%20and%0Ahuman%20intention%20at%20the%20constraint%20boundaries%2C%20thereby%20enhancing%20interaction%0Aefficiency.%20We%20evaluate%20the%20proposed%20method%20in%20scenarios%20where%20a%20human%20operator%0Aphysically%20interacts%20with%20a%207-degree-of-freedom%20robotic%20arm.%20The%20results%0Ahighlight%20the%20importance%20of%20incorporating%20directional%20constraints%20in%20pHRI%20for%0Ahierarchical%20tasks.%20Compared%20to%20existing%20methods%2C%20our%20approach%20generates%0Asmoother%20robotic%20trajectories%20during%20interaction%20while%20avoiding%20interaction%0Adelays%20at%20the%20constraint%20boundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirection-Constrained%2520Control%2520for%2520Efficient%2520Physical%2520Human-Robot%250A%2520%2520Interaction%2520under%2520Hierarchical%2520Tasks%26entry.906535625%3DMengxin%2520Xu%2520and%2520Weiwei%2520Wan%2520and%2520Hesheng%2520Wang%2520and%2520Kensuke%2520Harada%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520control%2520method%2520to%2520address%2520the%2520physical%2520Human-Robot%250AInteraction%2520%2528pHRI%2529%2520challenge%2520in%2520the%2520context%2520of%2520hierarchical%2520tasks.%2520A%2520common%250Aapproach%2520to%2520managing%2520hierarchical%2520tasks%2520is%2520Hierarchical%2520Quadratic%2520Programming%250A%2528HQP%2529%252C%2520which%252C%2520however%252C%2520cannot%2520be%2520directly%2520applied%2520to%2520human%2520interaction%2520due%2520to%250Aits%2520allowance%2520of%2520arbitrary%2520velocity%2520direction%2520adjustments.%2520To%2520resolve%2520this%250Alimitation%252C%2520we%2520introduce%2520the%2520concept%2520of%2520directional%2520constraints%2520and%2520develop%2520a%250Adirection-constrained%2520optimization%2520algorithm%2520to%2520handle%2520the%2520nonlinearities%250Ainduced%2520by%2520these%2520constraints.%2520The%2520algorithm%2520solves%2520two%2520sub-problems%252C%2520minimizing%250Athe%2520error%2520and%2520minimizing%2520the%2520deviation%2520angle%252C%2520in%2520parallel%252C%2520and%2520combines%2520the%250Aresults%2520of%2520the%2520two%2520sub-problems%2520to%2520produce%2520a%2520final%2520optimal%2520outcome.%2520The%2520mutual%250Ainfluence%2520between%2520these%2520two%2520sub-problems%2520is%2520analyzed%2520to%2520determine%2520the%2520best%250Aparameter%2520for%2520combination.%2520Additionally%252C%2520the%2520velocity%2520objective%2520in%2520our%2520control%250Aframework%2520is%2520computed%2520using%2520a%2520variable%2520admittance%2520controller.%2520Traditional%250Aadmittance%2520control%2520does%2520not%2520account%2520for%2520constraints.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520variable%2520admittance%2520control%2520method%2520to%2520adjust%2520control%2520objectives%250Adynamically.%2520The%2520method%2520helps%2520reduce%2520the%2520deviation%2520between%2520robot%2520velocity%2520and%250Ahuman%2520intention%2520at%2520the%2520constraint%2520boundaries%252C%2520thereby%2520enhancing%2520interaction%250Aefficiency.%2520We%2520evaluate%2520the%2520proposed%2520method%2520in%2520scenarios%2520where%2520a%2520human%2520operator%250Aphysically%2520interacts%2520with%2520a%25207-degree-of-freedom%2520robotic%2520arm.%2520The%2520results%250Ahighlight%2520the%2520importance%2520of%2520incorporating%2520directional%2520constraints%2520in%2520pHRI%2520for%250Ahierarchical%2520tasks.%2520Compared%2520to%2520existing%2520methods%252C%2520our%2520approach%2520generates%250Asmoother%2520robotic%2520trajectories%2520during%2520interaction%2520while%2520avoiding%2520interaction%250Adelays%2520at%2520the%2520constraint%2520boundaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direction-Constrained%20Control%20for%20Efficient%20Physical%20Human-Robot%0A%20%20Interaction%20under%20Hierarchical%20Tasks&entry.906535625=Mengxin%20Xu%20and%20Weiwei%20Wan%20and%20Hesheng%20Wang%20and%20Kensuke%20Harada&entry.1292438233=%20%20This%20paper%20proposes%20a%20control%20method%20to%20address%20the%20physical%20Human-Robot%0AInteraction%20%28pHRI%29%20challenge%20in%20the%20context%20of%20hierarchical%20tasks.%20A%20common%0Aapproach%20to%20managing%20hierarchical%20tasks%20is%20Hierarchical%20Quadratic%20Programming%0A%28HQP%29%2C%20which%2C%20however%2C%20cannot%20be%20directly%20applied%20to%20human%20interaction%20due%20to%0Aits%20allowance%20of%20arbitrary%20velocity%20direction%20adjustments.%20To%20resolve%20this%0Alimitation%2C%20we%20introduce%20the%20concept%20of%20directional%20constraints%20and%20develop%20a%0Adirection-constrained%20optimization%20algorithm%20to%20handle%20the%20nonlinearities%0Ainduced%20by%20these%20constraints.%20The%20algorithm%20solves%20two%20sub-problems%2C%20minimizing%0Athe%20error%20and%20minimizing%20the%20deviation%20angle%2C%20in%20parallel%2C%20and%20combines%20the%0Aresults%20of%20the%20two%20sub-problems%20to%20produce%20a%20final%20optimal%20outcome.%20The%20mutual%0Ainfluence%20between%20these%20two%20sub-problems%20is%20analyzed%20to%20determine%20the%20best%0Aparameter%20for%20combination.%20Additionally%2C%20the%20velocity%20objective%20in%20our%20control%0Aframework%20is%20computed%20using%20a%20variable%20admittance%20controller.%20Traditional%0Aadmittance%20control%20does%20not%20account%20for%20constraints.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20variable%20admittance%20control%20method%20to%20adjust%20control%20objectives%0Adynamically.%20The%20method%20helps%20reduce%20the%20deviation%20between%20robot%20velocity%20and%0Ahuman%20intention%20at%20the%20constraint%20boundaries%2C%20thereby%20enhancing%20interaction%0Aefficiency.%20We%20evaluate%20the%20proposed%20method%20in%20scenarios%20where%20a%20human%20operator%0Aphysically%20interacts%20with%20a%207-degree-of-freedom%20robotic%20arm.%20The%20results%0Ahighlight%20the%20importance%20of%20incorporating%20directional%20constraints%20in%20pHRI%20for%0Ahierarchical%20tasks.%20Compared%20to%20existing%20methods%2C%20our%20approach%20generates%0Asmoother%20robotic%20trajectories%20during%20interaction%20while%20avoiding%20interaction%0Adelays%20at%20the%20constraint%20boundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16922v1&entry.124074799=Read"},
{"title": "Agent-driven Generative Semantic Communication with Cross-Modality and\n  Prediction", "author": "Wanting Yang and Zehui Xiong and Yanli Yuan and Wenchao Jiang and Tony Q. S. Quek and Merouane Debbah", "abstract": "  In the era of 6G, with compelling visions of intelligent transportation\nsystems and digital twins, remote surveillance is poised to become a ubiquitous\npractice. Substantial data volume and frequent updates present challenges in\nwireless networks. To address these challenges, we propose a novel agent-driven\ngenerative semantic communication (A-GSC) framework based on reinforcement\nlearning. In contrast to the existing research on semantic communication\n(SemCom), which mainly focuses on either semantic extraction or semantic\nsampling, we seamlessly integrate both by jointly considering the intrinsic\nattributes of source information and the contextual information regarding the\ntask. Notably, the introduction of generative artificial intelligence (GAI)\nenables the independent design of semantic encoders and decoders. In this work,\nwe develop an agent-assisted semantic encoder with cross-modality capability,\nwhich can track the semantic changes, channel condition, to perform adaptive\nsemantic extraction and sampling. Accordingly, we design a semantic decoder\nwith both predictive and generative capabilities, consisting of two tailored\nmodules. Moreover, the effectiveness of the designed models has been verified\nusing the UA-DETRAC dataset, demonstrating the performance gains of the overall\nA-GSC framework in both energy saving and reconstruction accuracy.\n", "link": "http://arxiv.org/abs/2404.06997v3", "date": "2024-10-22", "relevancy": 2.2073, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5767}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5483}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent-driven%20Generative%20Semantic%20Communication%20with%20Cross-Modality%20and%0A%20%20Prediction&body=Title%3A%20Agent-driven%20Generative%20Semantic%20Communication%20with%20Cross-Modality%20and%0A%20%20Prediction%0AAuthor%3A%20Wanting%20Yang%20and%20Zehui%20Xiong%20and%20Yanli%20Yuan%20and%20Wenchao%20Jiang%20and%20Tony%20Q.%20S.%20Quek%20and%20Merouane%20Debbah%0AAbstract%3A%20%20%20In%20the%20era%20of%206G%2C%20with%20compelling%20visions%20of%20intelligent%20transportation%0Asystems%20and%20digital%20twins%2C%20remote%20surveillance%20is%20poised%20to%20become%20a%20ubiquitous%0Apractice.%20Substantial%20data%20volume%20and%20frequent%20updates%20present%20challenges%20in%0Awireless%20networks.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20agent-driven%0Agenerative%20semantic%20communication%20%28A-GSC%29%20framework%20based%20on%20reinforcement%0Alearning.%20In%20contrast%20to%20the%20existing%20research%20on%20semantic%20communication%0A%28SemCom%29%2C%20which%20mainly%20focuses%20on%20either%20semantic%20extraction%20or%20semantic%0Asampling%2C%20we%20seamlessly%20integrate%20both%20by%20jointly%20considering%20the%20intrinsic%0Aattributes%20of%20source%20information%20and%20the%20contextual%20information%20regarding%20the%0Atask.%20Notably%2C%20the%20introduction%20of%20generative%20artificial%20intelligence%20%28GAI%29%0Aenables%20the%20independent%20design%20of%20semantic%20encoders%20and%20decoders.%20In%20this%20work%2C%0Awe%20develop%20an%20agent-assisted%20semantic%20encoder%20with%20cross-modality%20capability%2C%0Awhich%20can%20track%20the%20semantic%20changes%2C%20channel%20condition%2C%20to%20perform%20adaptive%0Asemantic%20extraction%20and%20sampling.%20Accordingly%2C%20we%20design%20a%20semantic%20decoder%0Awith%20both%20predictive%20and%20generative%20capabilities%2C%20consisting%20of%20two%20tailored%0Amodules.%20Moreover%2C%20the%20effectiveness%20of%20the%20designed%20models%20has%20been%20verified%0Ausing%20the%20UA-DETRAC%20dataset%2C%20demonstrating%20the%20performance%20gains%20of%20the%20overall%0AA-GSC%20framework%20in%20both%20energy%20saving%20and%20reconstruction%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06997v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent-driven%2520Generative%2520Semantic%2520Communication%2520with%2520Cross-Modality%2520and%250A%2520%2520Prediction%26entry.906535625%3DWanting%2520Yang%2520and%2520Zehui%2520Xiong%2520and%2520Yanli%2520Yuan%2520and%2520Wenchao%2520Jiang%2520and%2520Tony%2520Q.%2520S.%2520Quek%2520and%2520Merouane%2520Debbah%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%25206G%252C%2520with%2520compelling%2520visions%2520of%2520intelligent%2520transportation%250Asystems%2520and%2520digital%2520twins%252C%2520remote%2520surveillance%2520is%2520poised%2520to%2520become%2520a%2520ubiquitous%250Apractice.%2520Substantial%2520data%2520volume%2520and%2520frequent%2520updates%2520present%2520challenges%2520in%250Awireless%2520networks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520agent-driven%250Agenerative%2520semantic%2520communication%2520%2528A-GSC%2529%2520framework%2520based%2520on%2520reinforcement%250Alearning.%2520In%2520contrast%2520to%2520the%2520existing%2520research%2520on%2520semantic%2520communication%250A%2528SemCom%2529%252C%2520which%2520mainly%2520focuses%2520on%2520either%2520semantic%2520extraction%2520or%2520semantic%250Asampling%252C%2520we%2520seamlessly%2520integrate%2520both%2520by%2520jointly%2520considering%2520the%2520intrinsic%250Aattributes%2520of%2520source%2520information%2520and%2520the%2520contextual%2520information%2520regarding%2520the%250Atask.%2520Notably%252C%2520the%2520introduction%2520of%2520generative%2520artificial%2520intelligence%2520%2528GAI%2529%250Aenables%2520the%2520independent%2520design%2520of%2520semantic%2520encoders%2520and%2520decoders.%2520In%2520this%2520work%252C%250Awe%2520develop%2520an%2520agent-assisted%2520semantic%2520encoder%2520with%2520cross-modality%2520capability%252C%250Awhich%2520can%2520track%2520the%2520semantic%2520changes%252C%2520channel%2520condition%252C%2520to%2520perform%2520adaptive%250Asemantic%2520extraction%2520and%2520sampling.%2520Accordingly%252C%2520we%2520design%2520a%2520semantic%2520decoder%250Awith%2520both%2520predictive%2520and%2520generative%2520capabilities%252C%2520consisting%2520of%2520two%2520tailored%250Amodules.%2520Moreover%252C%2520the%2520effectiveness%2520of%2520the%2520designed%2520models%2520has%2520been%2520verified%250Ausing%2520the%2520UA-DETRAC%2520dataset%252C%2520demonstrating%2520the%2520performance%2520gains%2520of%2520the%2520overall%250AA-GSC%2520framework%2520in%2520both%2520energy%2520saving%2520and%2520reconstruction%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06997v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent-driven%20Generative%20Semantic%20Communication%20with%20Cross-Modality%20and%0A%20%20Prediction&entry.906535625=Wanting%20Yang%20and%20Zehui%20Xiong%20and%20Yanli%20Yuan%20and%20Wenchao%20Jiang%20and%20Tony%20Q.%20S.%20Quek%20and%20Merouane%20Debbah&entry.1292438233=%20%20In%20the%20era%20of%206G%2C%20with%20compelling%20visions%20of%20intelligent%20transportation%0Asystems%20and%20digital%20twins%2C%20remote%20surveillance%20is%20poised%20to%20become%20a%20ubiquitous%0Apractice.%20Substantial%20data%20volume%20and%20frequent%20updates%20present%20challenges%20in%0Awireless%20networks.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20agent-driven%0Agenerative%20semantic%20communication%20%28A-GSC%29%20framework%20based%20on%20reinforcement%0Alearning.%20In%20contrast%20to%20the%20existing%20research%20on%20semantic%20communication%0A%28SemCom%29%2C%20which%20mainly%20focuses%20on%20either%20semantic%20extraction%20or%20semantic%0Asampling%2C%20we%20seamlessly%20integrate%20both%20by%20jointly%20considering%20the%20intrinsic%0Aattributes%20of%20source%20information%20and%20the%20contextual%20information%20regarding%20the%0Atask.%20Notably%2C%20the%20introduction%20of%20generative%20artificial%20intelligence%20%28GAI%29%0Aenables%20the%20independent%20design%20of%20semantic%20encoders%20and%20decoders.%20In%20this%20work%2C%0Awe%20develop%20an%20agent-assisted%20semantic%20encoder%20with%20cross-modality%20capability%2C%0Awhich%20can%20track%20the%20semantic%20changes%2C%20channel%20condition%2C%20to%20perform%20adaptive%0Asemantic%20extraction%20and%20sampling.%20Accordingly%2C%20we%20design%20a%20semantic%20decoder%0Awith%20both%20predictive%20and%20generative%20capabilities%2C%20consisting%20of%20two%20tailored%0Amodules.%20Moreover%2C%20the%20effectiveness%20of%20the%20designed%20models%20has%20been%20verified%0Ausing%20the%20UA-DETRAC%20dataset%2C%20demonstrating%20the%20performance%20gains%20of%20the%20overall%0AA-GSC%20framework%20in%20both%20energy%20saving%20and%20reconstruction%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06997v3&entry.124074799=Read"},
{"title": "An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and\n  Geometric Reasoning Skills Using Computer Graphics Questions", "author": "Tony Haoran Feng and Paul Denny and Burkhard C. W\u00fcnsche and Andrew Luxton-Reilly and Jacqueline Whalley", "abstract": "  CG (Computer Graphics) is a popular field of CS (Computer Science), but many\nstudents find this topic difficult due to it requiring a large number of\nskills, such as mathematics, programming, geometric reasoning, and creativity.\nOver the past few years, researchers have investigated ways to harness the\npower of GenAI (Generative Artificial Intelligence) to improve teaching. In CS,\nmuch of the research has focused on introductory computing. A recent study\nevaluating the performance of an LLM (Large Language Model), GPT-4 (text-only),\non CG questions, indicated poor performance and reliance on detailed\ndescriptions of image content, which often required considerable insight from\nthe user to return reasonable results. So far, no studies have investigated the\nabilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG\nquestions and how these abilities can be used to improve teaching.\n  In this study, we construct two datasets of CG questions requiring varying\ndegrees of visual perception skills and geometric reasoning skills, and\nevaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find\nthat although GPT-4o exhibits great potential in solving questions with visual\ninformation independently, major limitations still exist to the accuracy and\nquality of the generated results. We propose several novel approaches for CG\neducators to incorporate GenAI into CG teaching despite these limitations. We\nhope that our guidelines further encourage learning and engagement in CG\nclassrooms.\n", "link": "http://arxiv.org/abs/2410.16991v1", "date": "2024-10-22", "relevancy": 2.1969, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5551}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.552}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Eye%20for%20an%20AI%3A%20Evaluating%20GPT-4o%27s%20Visual%20Perception%20Skills%20and%0A%20%20Geometric%20Reasoning%20Skills%20Using%20Computer%20Graphics%20Questions&body=Title%3A%20An%20Eye%20for%20an%20AI%3A%20Evaluating%20GPT-4o%27s%20Visual%20Perception%20Skills%20and%0A%20%20Geometric%20Reasoning%20Skills%20Using%20Computer%20Graphics%20Questions%0AAuthor%3A%20Tony%20Haoran%20Feng%20and%20Paul%20Denny%20and%20Burkhard%20C.%20W%C3%BCnsche%20and%20Andrew%20Luxton-Reilly%20and%20Jacqueline%20Whalley%0AAbstract%3A%20%20%20CG%20%28Computer%20Graphics%29%20is%20a%20popular%20field%20of%20CS%20%28Computer%20Science%29%2C%20but%20many%0Astudents%20find%20this%20topic%20difficult%20due%20to%20it%20requiring%20a%20large%20number%20of%0Askills%2C%20such%20as%20mathematics%2C%20programming%2C%20geometric%20reasoning%2C%20and%20creativity.%0AOver%20the%20past%20few%20years%2C%20researchers%20have%20investigated%20ways%20to%20harness%20the%0Apower%20of%20GenAI%20%28Generative%20Artificial%20Intelligence%29%20to%20improve%20teaching.%20In%20CS%2C%0Amuch%20of%20the%20research%20has%20focused%20on%20introductory%20computing.%20A%20recent%20study%0Aevaluating%20the%20performance%20of%20an%20LLM%20%28Large%20Language%20Model%29%2C%20GPT-4%20%28text-only%29%2C%0Aon%20CG%20questions%2C%20indicated%20poor%20performance%20and%20reliance%20on%20detailed%0Adescriptions%20of%20image%20content%2C%20which%20often%20required%20considerable%20insight%20from%0Athe%20user%20to%20return%20reasonable%20results.%20So%20far%2C%20no%20studies%20have%20investigated%20the%0Aabilities%20of%20LMMs%20%28Large%20Multimodal%20Models%29%2C%20or%20multimodal%20LLMs%2C%20to%20solve%20CG%0Aquestions%20and%20how%20these%20abilities%20can%20be%20used%20to%20improve%20teaching.%0A%20%20In%20this%20study%2C%20we%20construct%20two%20datasets%20of%20CG%20questions%20requiring%20varying%0Adegrees%20of%20visual%20perception%20skills%20and%20geometric%20reasoning%20skills%2C%20and%0Aevaluate%20the%20current%20state-of-the-art%20LMM%2C%20GPT-4o%2C%20on%20the%20two%20datasets.%20We%20find%0Athat%20although%20GPT-4o%20exhibits%20great%20potential%20in%20solving%20questions%20with%20visual%0Ainformation%20independently%2C%20major%20limitations%20still%20exist%20to%20the%20accuracy%20and%0Aquality%20of%20the%20generated%20results.%20We%20propose%20several%20novel%20approaches%20for%20CG%0Aeducators%20to%20incorporate%20GenAI%20into%20CG%20teaching%20despite%20these%20limitations.%20We%0Ahope%20that%20our%20guidelines%20further%20encourage%20learning%20and%20engagement%20in%20CG%0Aclassrooms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Eye%2520for%2520an%2520AI%253A%2520Evaluating%2520GPT-4o%2527s%2520Visual%2520Perception%2520Skills%2520and%250A%2520%2520Geometric%2520Reasoning%2520Skills%2520Using%2520Computer%2520Graphics%2520Questions%26entry.906535625%3DTony%2520Haoran%2520Feng%2520and%2520Paul%2520Denny%2520and%2520Burkhard%2520C.%2520W%25C3%25BCnsche%2520and%2520Andrew%2520Luxton-Reilly%2520and%2520Jacqueline%2520Whalley%26entry.1292438233%3D%2520%2520CG%2520%2528Computer%2520Graphics%2529%2520is%2520a%2520popular%2520field%2520of%2520CS%2520%2528Computer%2520Science%2529%252C%2520but%2520many%250Astudents%2520find%2520this%2520topic%2520difficult%2520due%2520to%2520it%2520requiring%2520a%2520large%2520number%2520of%250Askills%252C%2520such%2520as%2520mathematics%252C%2520programming%252C%2520geometric%2520reasoning%252C%2520and%2520creativity.%250AOver%2520the%2520past%2520few%2520years%252C%2520researchers%2520have%2520investigated%2520ways%2520to%2520harness%2520the%250Apower%2520of%2520GenAI%2520%2528Generative%2520Artificial%2520Intelligence%2529%2520to%2520improve%2520teaching.%2520In%2520CS%252C%250Amuch%2520of%2520the%2520research%2520has%2520focused%2520on%2520introductory%2520computing.%2520A%2520recent%2520study%250Aevaluating%2520the%2520performance%2520of%2520an%2520LLM%2520%2528Large%2520Language%2520Model%2529%252C%2520GPT-4%2520%2528text-only%2529%252C%250Aon%2520CG%2520questions%252C%2520indicated%2520poor%2520performance%2520and%2520reliance%2520on%2520detailed%250Adescriptions%2520of%2520image%2520content%252C%2520which%2520often%2520required%2520considerable%2520insight%2520from%250Athe%2520user%2520to%2520return%2520reasonable%2520results.%2520So%2520far%252C%2520no%2520studies%2520have%2520investigated%2520the%250Aabilities%2520of%2520LMMs%2520%2528Large%2520Multimodal%2520Models%2529%252C%2520or%2520multimodal%2520LLMs%252C%2520to%2520solve%2520CG%250Aquestions%2520and%2520how%2520these%2520abilities%2520can%2520be%2520used%2520to%2520improve%2520teaching.%250A%2520%2520In%2520this%2520study%252C%2520we%2520construct%2520two%2520datasets%2520of%2520CG%2520questions%2520requiring%2520varying%250Adegrees%2520of%2520visual%2520perception%2520skills%2520and%2520geometric%2520reasoning%2520skills%252C%2520and%250Aevaluate%2520the%2520current%2520state-of-the-art%2520LMM%252C%2520GPT-4o%252C%2520on%2520the%2520two%2520datasets.%2520We%2520find%250Athat%2520although%2520GPT-4o%2520exhibits%2520great%2520potential%2520in%2520solving%2520questions%2520with%2520visual%250Ainformation%2520independently%252C%2520major%2520limitations%2520still%2520exist%2520to%2520the%2520accuracy%2520and%250Aquality%2520of%2520the%2520generated%2520results.%2520We%2520propose%2520several%2520novel%2520approaches%2520for%2520CG%250Aeducators%2520to%2520incorporate%2520GenAI%2520into%2520CG%2520teaching%2520despite%2520these%2520limitations.%2520We%250Ahope%2520that%2520our%2520guidelines%2520further%2520encourage%2520learning%2520and%2520engagement%2520in%2520CG%250Aclassrooms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Eye%20for%20an%20AI%3A%20Evaluating%20GPT-4o%27s%20Visual%20Perception%20Skills%20and%0A%20%20Geometric%20Reasoning%20Skills%20Using%20Computer%20Graphics%20Questions&entry.906535625=Tony%20Haoran%20Feng%20and%20Paul%20Denny%20and%20Burkhard%20C.%20W%C3%BCnsche%20and%20Andrew%20Luxton-Reilly%20and%20Jacqueline%20Whalley&entry.1292438233=%20%20CG%20%28Computer%20Graphics%29%20is%20a%20popular%20field%20of%20CS%20%28Computer%20Science%29%2C%20but%20many%0Astudents%20find%20this%20topic%20difficult%20due%20to%20it%20requiring%20a%20large%20number%20of%0Askills%2C%20such%20as%20mathematics%2C%20programming%2C%20geometric%20reasoning%2C%20and%20creativity.%0AOver%20the%20past%20few%20years%2C%20researchers%20have%20investigated%20ways%20to%20harness%20the%0Apower%20of%20GenAI%20%28Generative%20Artificial%20Intelligence%29%20to%20improve%20teaching.%20In%20CS%2C%0Amuch%20of%20the%20research%20has%20focused%20on%20introductory%20computing.%20A%20recent%20study%0Aevaluating%20the%20performance%20of%20an%20LLM%20%28Large%20Language%20Model%29%2C%20GPT-4%20%28text-only%29%2C%0Aon%20CG%20questions%2C%20indicated%20poor%20performance%20and%20reliance%20on%20detailed%0Adescriptions%20of%20image%20content%2C%20which%20often%20required%20considerable%20insight%20from%0Athe%20user%20to%20return%20reasonable%20results.%20So%20far%2C%20no%20studies%20have%20investigated%20the%0Aabilities%20of%20LMMs%20%28Large%20Multimodal%20Models%29%2C%20or%20multimodal%20LLMs%2C%20to%20solve%20CG%0Aquestions%20and%20how%20these%20abilities%20can%20be%20used%20to%20improve%20teaching.%0A%20%20In%20this%20study%2C%20we%20construct%20two%20datasets%20of%20CG%20questions%20requiring%20varying%0Adegrees%20of%20visual%20perception%20skills%20and%20geometric%20reasoning%20skills%2C%20and%0Aevaluate%20the%20current%20state-of-the-art%20LMM%2C%20GPT-4o%2C%20on%20the%20two%20datasets.%20We%20find%0Athat%20although%20GPT-4o%20exhibits%20great%20potential%20in%20solving%20questions%20with%20visual%0Ainformation%20independently%2C%20major%20limitations%20still%20exist%20to%20the%20accuracy%20and%0Aquality%20of%20the%20generated%20results.%20We%20propose%20several%20novel%20approaches%20for%20CG%0Aeducators%20to%20incorporate%20GenAI%20into%20CG%20teaching%20despite%20these%20limitations.%20We%0Ahope%20that%20our%20guidelines%20further%20encourage%20learning%20and%20engagement%20in%20CG%0Aclassrooms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16991v1&entry.124074799=Read"},
{"title": "PGCS: Physical Law embedded Generative Cloud Synthesis in Remote Sensing\n  Images", "author": "Liying Xu and Huifang Li and Huanfeng Shen and Mingyang Lei and Tao Jiang", "abstract": "  Data quantity and quality are both critical for information extraction and\nanalyzation in remote sensing. However, the current remote sensing datasets\noften fail to meet these two requirements, for which cloud is a primary factor\ndegrading the data quantity and quality. This limitation affects the precision\nof results in remote sensing application, particularly those derived from\ndata-driven techniques. In this paper, a physical law embedded generative cloud\nsynthesis method (PGCS) is proposed to generate diverse realistic cloud images\nto enhance real data and promote the development of algorithms for subsequent\ntasks, such as cloud correction, cloud detection, and data augmentation for\nclassification, recognition, and segmentation. The PGCS method involves two key\nphases: spatial synthesis and spectral synthesis. In the spatial synthesis\nphase, a style-based generative adversarial network is utilized to simulate the\nspatial characteristics, generating an infinite number of single-channel\nclouds. In the spectral synthesis phase, the atmospheric scattering law is\nembedded through a local statistics and global fitting method, converting the\nsingle-channel clouds into multi-spectral clouds. The experimental results\ndemonstrate that PGCS achieves a high accuracy in both phases and performs\nbetter than three other existing cloud synthesis methods. Two cloud correction\nmethods are developed from PGCS and exhibits a superior performance compared to\nstate-of-the-art methods in the cloud correction task. Furthermore, the\napplication of PGCS with data from various sensors was investigated and\nsuccessfully extended. Code will be provided at\nhttps://github.com/Liying-Xu/PGCS.\n", "link": "http://arxiv.org/abs/2410.16955v1", "date": "2024-10-22", "relevancy": 2.191, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5744}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5676}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PGCS%3A%20Physical%20Law%20embedded%20Generative%20Cloud%20Synthesis%20in%20Remote%20Sensing%0A%20%20Images&body=Title%3A%20PGCS%3A%20Physical%20Law%20embedded%20Generative%20Cloud%20Synthesis%20in%20Remote%20Sensing%0A%20%20Images%0AAuthor%3A%20Liying%20Xu%20and%20Huifang%20Li%20and%20Huanfeng%20Shen%20and%20Mingyang%20Lei%20and%20Tao%20Jiang%0AAbstract%3A%20%20%20Data%20quantity%20and%20quality%20are%20both%20critical%20for%20information%20extraction%20and%0Aanalyzation%20in%20remote%20sensing.%20However%2C%20the%20current%20remote%20sensing%20datasets%0Aoften%20fail%20to%20meet%20these%20two%20requirements%2C%20for%20which%20cloud%20is%20a%20primary%20factor%0Adegrading%20the%20data%20quantity%20and%20quality.%20This%20limitation%20affects%20the%20precision%0Aof%20results%20in%20remote%20sensing%20application%2C%20particularly%20those%20derived%20from%0Adata-driven%20techniques.%20In%20this%20paper%2C%20a%20physical%20law%20embedded%20generative%20cloud%0Asynthesis%20method%20%28PGCS%29%20is%20proposed%20to%20generate%20diverse%20realistic%20cloud%20images%0Ato%20enhance%20real%20data%20and%20promote%20the%20development%20of%20algorithms%20for%20subsequent%0Atasks%2C%20such%20as%20cloud%20correction%2C%20cloud%20detection%2C%20and%20data%20augmentation%20for%0Aclassification%2C%20recognition%2C%20and%20segmentation.%20The%20PGCS%20method%20involves%20two%20key%0Aphases%3A%20spatial%20synthesis%20and%20spectral%20synthesis.%20In%20the%20spatial%20synthesis%0Aphase%2C%20a%20style-based%20generative%20adversarial%20network%20is%20utilized%20to%20simulate%20the%0Aspatial%20characteristics%2C%20generating%20an%20infinite%20number%20of%20single-channel%0Aclouds.%20In%20the%20spectral%20synthesis%20phase%2C%20the%20atmospheric%20scattering%20law%20is%0Aembedded%20through%20a%20local%20statistics%20and%20global%20fitting%20method%2C%20converting%20the%0Asingle-channel%20clouds%20into%20multi-spectral%20clouds.%20The%20experimental%20results%0Ademonstrate%20that%20PGCS%20achieves%20a%20high%20accuracy%20in%20both%20phases%20and%20performs%0Abetter%20than%20three%20other%20existing%20cloud%20synthesis%20methods.%20Two%20cloud%20correction%0Amethods%20are%20developed%20from%20PGCS%20and%20exhibits%20a%20superior%20performance%20compared%20to%0Astate-of-the-art%20methods%20in%20the%20cloud%20correction%20task.%20Furthermore%2C%20the%0Aapplication%20of%20PGCS%20with%20data%20from%20various%20sensors%20was%20investigated%20and%0Asuccessfully%20extended.%20Code%20will%20be%20provided%20at%0Ahttps%3A//github.com/Liying-Xu/PGCS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPGCS%253A%2520Physical%2520Law%2520embedded%2520Generative%2520Cloud%2520Synthesis%2520in%2520Remote%2520Sensing%250A%2520%2520Images%26entry.906535625%3DLiying%2520Xu%2520and%2520Huifang%2520Li%2520and%2520Huanfeng%2520Shen%2520and%2520Mingyang%2520Lei%2520and%2520Tao%2520Jiang%26entry.1292438233%3D%2520%2520Data%2520quantity%2520and%2520quality%2520are%2520both%2520critical%2520for%2520information%2520extraction%2520and%250Aanalyzation%2520in%2520remote%2520sensing.%2520However%252C%2520the%2520current%2520remote%2520sensing%2520datasets%250Aoften%2520fail%2520to%2520meet%2520these%2520two%2520requirements%252C%2520for%2520which%2520cloud%2520is%2520a%2520primary%2520factor%250Adegrading%2520the%2520data%2520quantity%2520and%2520quality.%2520This%2520limitation%2520affects%2520the%2520precision%250Aof%2520results%2520in%2520remote%2520sensing%2520application%252C%2520particularly%2520those%2520derived%2520from%250Adata-driven%2520techniques.%2520In%2520this%2520paper%252C%2520a%2520physical%2520law%2520embedded%2520generative%2520cloud%250Asynthesis%2520method%2520%2528PGCS%2529%2520is%2520proposed%2520to%2520generate%2520diverse%2520realistic%2520cloud%2520images%250Ato%2520enhance%2520real%2520data%2520and%2520promote%2520the%2520development%2520of%2520algorithms%2520for%2520subsequent%250Atasks%252C%2520such%2520as%2520cloud%2520correction%252C%2520cloud%2520detection%252C%2520and%2520data%2520augmentation%2520for%250Aclassification%252C%2520recognition%252C%2520and%2520segmentation.%2520The%2520PGCS%2520method%2520involves%2520two%2520key%250Aphases%253A%2520spatial%2520synthesis%2520and%2520spectral%2520synthesis.%2520In%2520the%2520spatial%2520synthesis%250Aphase%252C%2520a%2520style-based%2520generative%2520adversarial%2520network%2520is%2520utilized%2520to%2520simulate%2520the%250Aspatial%2520characteristics%252C%2520generating%2520an%2520infinite%2520number%2520of%2520single-channel%250Aclouds.%2520In%2520the%2520spectral%2520synthesis%2520phase%252C%2520the%2520atmospheric%2520scattering%2520law%2520is%250Aembedded%2520through%2520a%2520local%2520statistics%2520and%2520global%2520fitting%2520method%252C%2520converting%2520the%250Asingle-channel%2520clouds%2520into%2520multi-spectral%2520clouds.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520PGCS%2520achieves%2520a%2520high%2520accuracy%2520in%2520both%2520phases%2520and%2520performs%250Abetter%2520than%2520three%2520other%2520existing%2520cloud%2520synthesis%2520methods.%2520Two%2520cloud%2520correction%250Amethods%2520are%2520developed%2520from%2520PGCS%2520and%2520exhibits%2520a%2520superior%2520performance%2520compared%2520to%250Astate-of-the-art%2520methods%2520in%2520the%2520cloud%2520correction%2520task.%2520Furthermore%252C%2520the%250Aapplication%2520of%2520PGCS%2520with%2520data%2520from%2520various%2520sensors%2520was%2520investigated%2520and%250Asuccessfully%2520extended.%2520Code%2520will%2520be%2520provided%2520at%250Ahttps%253A//github.com/Liying-Xu/PGCS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PGCS%3A%20Physical%20Law%20embedded%20Generative%20Cloud%20Synthesis%20in%20Remote%20Sensing%0A%20%20Images&entry.906535625=Liying%20Xu%20and%20Huifang%20Li%20and%20Huanfeng%20Shen%20and%20Mingyang%20Lei%20and%20Tao%20Jiang&entry.1292438233=%20%20Data%20quantity%20and%20quality%20are%20both%20critical%20for%20information%20extraction%20and%0Aanalyzation%20in%20remote%20sensing.%20However%2C%20the%20current%20remote%20sensing%20datasets%0Aoften%20fail%20to%20meet%20these%20two%20requirements%2C%20for%20which%20cloud%20is%20a%20primary%20factor%0Adegrading%20the%20data%20quantity%20and%20quality.%20This%20limitation%20affects%20the%20precision%0Aof%20results%20in%20remote%20sensing%20application%2C%20particularly%20those%20derived%20from%0Adata-driven%20techniques.%20In%20this%20paper%2C%20a%20physical%20law%20embedded%20generative%20cloud%0Asynthesis%20method%20%28PGCS%29%20is%20proposed%20to%20generate%20diverse%20realistic%20cloud%20images%0Ato%20enhance%20real%20data%20and%20promote%20the%20development%20of%20algorithms%20for%20subsequent%0Atasks%2C%20such%20as%20cloud%20correction%2C%20cloud%20detection%2C%20and%20data%20augmentation%20for%0Aclassification%2C%20recognition%2C%20and%20segmentation.%20The%20PGCS%20method%20involves%20two%20key%0Aphases%3A%20spatial%20synthesis%20and%20spectral%20synthesis.%20In%20the%20spatial%20synthesis%0Aphase%2C%20a%20style-based%20generative%20adversarial%20network%20is%20utilized%20to%20simulate%20the%0Aspatial%20characteristics%2C%20generating%20an%20infinite%20number%20of%20single-channel%0Aclouds.%20In%20the%20spectral%20synthesis%20phase%2C%20the%20atmospheric%20scattering%20law%20is%0Aembedded%20through%20a%20local%20statistics%20and%20global%20fitting%20method%2C%20converting%20the%0Asingle-channel%20clouds%20into%20multi-spectral%20clouds.%20The%20experimental%20results%0Ademonstrate%20that%20PGCS%20achieves%20a%20high%20accuracy%20in%20both%20phases%20and%20performs%0Abetter%20than%20three%20other%20existing%20cloud%20synthesis%20methods.%20Two%20cloud%20correction%0Amethods%20are%20developed%20from%20PGCS%20and%20exhibits%20a%20superior%20performance%20compared%20to%0Astate-of-the-art%20methods%20in%20the%20cloud%20correction%20task.%20Furthermore%2C%20the%0Aapplication%20of%20PGCS%20with%20data%20from%20various%20sensors%20was%20investigated%20and%0Asuccessfully%20extended.%20Code%20will%20be%20provided%20at%0Ahttps%3A//github.com/Liying-Xu/PGCS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16955v1&entry.124074799=Read"},
{"title": "Language Model Non-myopic Generation for Reasoning and Planning", "author": "Chang Ma and Haiteng Zhao and Junlei Zhang and Junxian He and Lingpeng Kong", "abstract": "  Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.\n", "link": "http://arxiv.org/abs/2410.17195v1", "date": "2024-10-22", "relevancy": 2.1651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Model%20Non-myopic%20Generation%20for%20Reasoning%20and%20Planning&body=Title%3A%20Language%20Model%20Non-myopic%20Generation%20for%20Reasoning%20and%20Planning%0AAuthor%3A%20Chang%20Ma%20and%20Haiteng%20Zhao%20and%20Junlei%20Zhang%20and%20Junxian%20He%20and%20Lingpeng%20Kong%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20demonstrated%20remarkable%20abilities%20in%20reasoning%20and%0Aplanning%20by%20breaking%20down%20complex%20problems%20into%20sequential%20steps.%20Despite%20their%0Asuccess%20in%20various%20domains%20like%20mathematical%20problem-solving%20and%20coding%2C%20LLMs%0Aface%20challenges%20in%20ensuring%20reliable%20and%20optimal%20planning%20due%20to%20their%20inherent%0Amyopic%20nature%20of%20autoregressive%20decoding.%20This%20paper%20revisits%20LLM%20reasoning%0Afrom%20an%20optimal-control%20perspective%2C%20proposing%20a%20novel%20method%2C%0APredictive-Decoding%2C%20that%20leverages%20Model%20Predictive%20Control%20to%20enhance%0Aplanning%20accuracy.%20By%20re-weighting%20LLM%20distributions%20based%20on%20foresight%0Atrajectories%2C%20Predictive-Decoding%20aims%20to%20mitigate%20early%20errors%20and%20promote%0Anon-myopic%20planning.%20Our%20experiments%20show%20significant%20improvements%20in%20a%20wide%0Arange%20of%20tasks%20for%20math%2C%20coding%2C%20and%20agents.%20Furthermore%2C%20Predictive-Decoding%0Ademonstrates%20computational%20efficiency%2C%20outperforming%20search%20baselines%20with%0Areduced%20computational%20resources.%20This%20study%20provides%20insights%20into%20optimizing%0ALLM%20planning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Model%2520Non-myopic%2520Generation%2520for%2520Reasoning%2520and%2520Planning%26entry.906535625%3DChang%2520Ma%2520and%2520Haiteng%2520Zhao%2520and%2520Junlei%2520Zhang%2520and%2520Junxian%2520He%2520and%2520Lingpeng%2520Kong%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520demonstrated%2520remarkable%2520abilities%2520in%2520reasoning%2520and%250Aplanning%2520by%2520breaking%2520down%2520complex%2520problems%2520into%2520sequential%2520steps.%2520Despite%2520their%250Asuccess%2520in%2520various%2520domains%2520like%2520mathematical%2520problem-solving%2520and%2520coding%252C%2520LLMs%250Aface%2520challenges%2520in%2520ensuring%2520reliable%2520and%2520optimal%2520planning%2520due%2520to%2520their%2520inherent%250Amyopic%2520nature%2520of%2520autoregressive%2520decoding.%2520This%2520paper%2520revisits%2520LLM%2520reasoning%250Afrom%2520an%2520optimal-control%2520perspective%252C%2520proposing%2520a%2520novel%2520method%252C%250APredictive-Decoding%252C%2520that%2520leverages%2520Model%2520Predictive%2520Control%2520to%2520enhance%250Aplanning%2520accuracy.%2520By%2520re-weighting%2520LLM%2520distributions%2520based%2520on%2520foresight%250Atrajectories%252C%2520Predictive-Decoding%2520aims%2520to%2520mitigate%2520early%2520errors%2520and%2520promote%250Anon-myopic%2520planning.%2520Our%2520experiments%2520show%2520significant%2520improvements%2520in%2520a%2520wide%250Arange%2520of%2520tasks%2520for%2520math%252C%2520coding%252C%2520and%2520agents.%2520Furthermore%252C%2520Predictive-Decoding%250Ademonstrates%2520computational%2520efficiency%252C%2520outperforming%2520search%2520baselines%2520with%250Areduced%2520computational%2520resources.%2520This%2520study%2520provides%2520insights%2520into%2520optimizing%250ALLM%2520planning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Model%20Non-myopic%20Generation%20for%20Reasoning%20and%20Planning&entry.906535625=Chang%20Ma%20and%20Haiteng%20Zhao%20and%20Junlei%20Zhang%20and%20Junxian%20He%20and%20Lingpeng%20Kong&entry.1292438233=%20%20Large%20Language%20Models%20have%20demonstrated%20remarkable%20abilities%20in%20reasoning%20and%0Aplanning%20by%20breaking%20down%20complex%20problems%20into%20sequential%20steps.%20Despite%20their%0Asuccess%20in%20various%20domains%20like%20mathematical%20problem-solving%20and%20coding%2C%20LLMs%0Aface%20challenges%20in%20ensuring%20reliable%20and%20optimal%20planning%20due%20to%20their%20inherent%0Amyopic%20nature%20of%20autoregressive%20decoding.%20This%20paper%20revisits%20LLM%20reasoning%0Afrom%20an%20optimal-control%20perspective%2C%20proposing%20a%20novel%20method%2C%0APredictive-Decoding%2C%20that%20leverages%20Model%20Predictive%20Control%20to%20enhance%0Aplanning%20accuracy.%20By%20re-weighting%20LLM%20distributions%20based%20on%20foresight%0Atrajectories%2C%20Predictive-Decoding%20aims%20to%20mitigate%20early%20errors%20and%20promote%0Anon-myopic%20planning.%20Our%20experiments%20show%20significant%20improvements%20in%20a%20wide%0Arange%20of%20tasks%20for%20math%2C%20coding%2C%20and%20agents.%20Furthermore%2C%20Predictive-Decoding%0Ademonstrates%20computational%20efficiency%2C%20outperforming%20search%20baselines%20with%0Areduced%20computational%20resources.%20This%20study%20provides%20insights%20into%20optimizing%0ALLM%20planning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17195v1&entry.124074799=Read"},
{"title": "The Persian Rug: solving toy models of superposition using large-scale\n  symmetries", "author": "Aditya Cowsik and Kfir Dolev and Alex Infanger", "abstract": "  We present a complete mechanistic description of the algorithm learned by a\nminimal non-linear sparse data autoencoder in the limit of large input\ndimension. The model, originally presented in arXiv:2209.10652, compresses\nsparse data vectors through a linear layer and decompresses using another\nlinear layer followed by a ReLU activation. We notice that when the data is\npermutation symmetric (no input feature is privileged) large models reliably\nlearn an algorithm that is sensitive to individual weights only through their\nlarge-scale statistics. For these models, the loss function becomes\nanalytically tractable. Using this understanding, we give the explicit scalings\nof the loss at high sparsity, and show that the model is near-optimal among\nrecently proposed architectures. In particular, changing or adding to the\nactivation function any elementwise or filtering operation can at best improve\nthe model's performance by a constant factor. Finally, we forward-engineer a\nmodel with the requisite symmetries and show that its loss precisely matches\nthat of the trained models. Unlike the trained model weights, the low\nrandomness in the artificial weights results in miraculous fractal structures\nresembling a Persian rug, to which the algorithm is oblivious. Our work\ncontributes to neural network interpretability by introducing techniques for\nunderstanding the structure of autoencoders. Code to reproduce our results can\nbe found at https://github.com/KfirD/PersianRug .\n", "link": "http://arxiv.org/abs/2410.12101v2", "date": "2024-10-22", "relevancy": 2.1451, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5603}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5436}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Persian%20Rug%3A%20solving%20toy%20models%20of%20superposition%20using%20large-scale%0A%20%20symmetries&body=Title%3A%20The%20Persian%20Rug%3A%20solving%20toy%20models%20of%20superposition%20using%20large-scale%0A%20%20symmetries%0AAuthor%3A%20Aditya%20Cowsik%20and%20Kfir%20Dolev%20and%20Alex%20Infanger%0AAbstract%3A%20%20%20We%20present%20a%20complete%20mechanistic%20description%20of%20the%20algorithm%20learned%20by%20a%0Aminimal%20non-linear%20sparse%20data%20autoencoder%20in%20the%20limit%20of%20large%20input%0Adimension.%20The%20model%2C%20originally%20presented%20in%20arXiv%3A2209.10652%2C%20compresses%0Asparse%20data%20vectors%20through%20a%20linear%20layer%20and%20decompresses%20using%20another%0Alinear%20layer%20followed%20by%20a%20ReLU%20activation.%20We%20notice%20that%20when%20the%20data%20is%0Apermutation%20symmetric%20%28no%20input%20feature%20is%20privileged%29%20large%20models%20reliably%0Alearn%20an%20algorithm%20that%20is%20sensitive%20to%20individual%20weights%20only%20through%20their%0Alarge-scale%20statistics.%20For%20these%20models%2C%20the%20loss%20function%20becomes%0Aanalytically%20tractable.%20Using%20this%20understanding%2C%20we%20give%20the%20explicit%20scalings%0Aof%20the%20loss%20at%20high%20sparsity%2C%20and%20show%20that%20the%20model%20is%20near-optimal%20among%0Arecently%20proposed%20architectures.%20In%20particular%2C%20changing%20or%20adding%20to%20the%0Aactivation%20function%20any%20elementwise%20or%20filtering%20operation%20can%20at%20best%20improve%0Athe%20model%27s%20performance%20by%20a%20constant%20factor.%20Finally%2C%20we%20forward-engineer%20a%0Amodel%20with%20the%20requisite%20symmetries%20and%20show%20that%20its%20loss%20precisely%20matches%0Athat%20of%20the%20trained%20models.%20Unlike%20the%20trained%20model%20weights%2C%20the%20low%0Arandomness%20in%20the%20artificial%20weights%20results%20in%20miraculous%20fractal%20structures%0Aresembling%20a%20Persian%20rug%2C%20to%20which%20the%20algorithm%20is%20oblivious.%20Our%20work%0Acontributes%20to%20neural%20network%20interpretability%20by%20introducing%20techniques%20for%0Aunderstanding%20the%20structure%20of%20autoencoders.%20Code%20to%20reproduce%20our%20results%20can%0Abe%20found%20at%20https%3A//github.com/KfirD/PersianRug%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Persian%2520Rug%253A%2520solving%2520toy%2520models%2520of%2520superposition%2520using%2520large-scale%250A%2520%2520symmetries%26entry.906535625%3DAditya%2520Cowsik%2520and%2520Kfir%2520Dolev%2520and%2520Alex%2520Infanger%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520complete%2520mechanistic%2520description%2520of%2520the%2520algorithm%2520learned%2520by%2520a%250Aminimal%2520non-linear%2520sparse%2520data%2520autoencoder%2520in%2520the%2520limit%2520of%2520large%2520input%250Adimension.%2520The%2520model%252C%2520originally%2520presented%2520in%2520arXiv%253A2209.10652%252C%2520compresses%250Asparse%2520data%2520vectors%2520through%2520a%2520linear%2520layer%2520and%2520decompresses%2520using%2520another%250Alinear%2520layer%2520followed%2520by%2520a%2520ReLU%2520activation.%2520We%2520notice%2520that%2520when%2520the%2520data%2520is%250Apermutation%2520symmetric%2520%2528no%2520input%2520feature%2520is%2520privileged%2529%2520large%2520models%2520reliably%250Alearn%2520an%2520algorithm%2520that%2520is%2520sensitive%2520to%2520individual%2520weights%2520only%2520through%2520their%250Alarge-scale%2520statistics.%2520For%2520these%2520models%252C%2520the%2520loss%2520function%2520becomes%250Aanalytically%2520tractable.%2520Using%2520this%2520understanding%252C%2520we%2520give%2520the%2520explicit%2520scalings%250Aof%2520the%2520loss%2520at%2520high%2520sparsity%252C%2520and%2520show%2520that%2520the%2520model%2520is%2520near-optimal%2520among%250Arecently%2520proposed%2520architectures.%2520In%2520particular%252C%2520changing%2520or%2520adding%2520to%2520the%250Aactivation%2520function%2520any%2520elementwise%2520or%2520filtering%2520operation%2520can%2520at%2520best%2520improve%250Athe%2520model%2527s%2520performance%2520by%2520a%2520constant%2520factor.%2520Finally%252C%2520we%2520forward-engineer%2520a%250Amodel%2520with%2520the%2520requisite%2520symmetries%2520and%2520show%2520that%2520its%2520loss%2520precisely%2520matches%250Athat%2520of%2520the%2520trained%2520models.%2520Unlike%2520the%2520trained%2520model%2520weights%252C%2520the%2520low%250Arandomness%2520in%2520the%2520artificial%2520weights%2520results%2520in%2520miraculous%2520fractal%2520structures%250Aresembling%2520a%2520Persian%2520rug%252C%2520to%2520which%2520the%2520algorithm%2520is%2520oblivious.%2520Our%2520work%250Acontributes%2520to%2520neural%2520network%2520interpretability%2520by%2520introducing%2520techniques%2520for%250Aunderstanding%2520the%2520structure%2520of%2520autoencoders.%2520Code%2520to%2520reproduce%2520our%2520results%2520can%250Abe%2520found%2520at%2520https%253A//github.com/KfirD/PersianRug%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Persian%20Rug%3A%20solving%20toy%20models%20of%20superposition%20using%20large-scale%0A%20%20symmetries&entry.906535625=Aditya%20Cowsik%20and%20Kfir%20Dolev%20and%20Alex%20Infanger&entry.1292438233=%20%20We%20present%20a%20complete%20mechanistic%20description%20of%20the%20algorithm%20learned%20by%20a%0Aminimal%20non-linear%20sparse%20data%20autoencoder%20in%20the%20limit%20of%20large%20input%0Adimension.%20The%20model%2C%20originally%20presented%20in%20arXiv%3A2209.10652%2C%20compresses%0Asparse%20data%20vectors%20through%20a%20linear%20layer%20and%20decompresses%20using%20another%0Alinear%20layer%20followed%20by%20a%20ReLU%20activation.%20We%20notice%20that%20when%20the%20data%20is%0Apermutation%20symmetric%20%28no%20input%20feature%20is%20privileged%29%20large%20models%20reliably%0Alearn%20an%20algorithm%20that%20is%20sensitive%20to%20individual%20weights%20only%20through%20their%0Alarge-scale%20statistics.%20For%20these%20models%2C%20the%20loss%20function%20becomes%0Aanalytically%20tractable.%20Using%20this%20understanding%2C%20we%20give%20the%20explicit%20scalings%0Aof%20the%20loss%20at%20high%20sparsity%2C%20and%20show%20that%20the%20model%20is%20near-optimal%20among%0Arecently%20proposed%20architectures.%20In%20particular%2C%20changing%20or%20adding%20to%20the%0Aactivation%20function%20any%20elementwise%20or%20filtering%20operation%20can%20at%20best%20improve%0Athe%20model%27s%20performance%20by%20a%20constant%20factor.%20Finally%2C%20we%20forward-engineer%20a%0Amodel%20with%20the%20requisite%20symmetries%20and%20show%20that%20its%20loss%20precisely%20matches%0Athat%20of%20the%20trained%20models.%20Unlike%20the%20trained%20model%20weights%2C%20the%20low%0Arandomness%20in%20the%20artificial%20weights%20results%20in%20miraculous%20fractal%20structures%0Aresembling%20a%20Persian%20rug%2C%20to%20which%20the%20algorithm%20is%20oblivious.%20Our%20work%0Acontributes%20to%20neural%20network%20interpretability%20by%20introducing%20techniques%20for%0Aunderstanding%20the%20structure%20of%20autoencoders.%20Code%20to%20reproduce%20our%20results%20can%0Abe%20found%20at%20https%3A//github.com/KfirD/PersianRug%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12101v2&entry.124074799=Read"},
{"title": "AGSENet: A Robust Road Ponding Detection Method for Proactive Traffic\n  Safety", "author": "Ronghui Zhang and Shangyu Yang and Dakang Lyu and Zihan Wang and Junzhou Chen and Yilong Ren and Bolin Gao and Zhihan Lv", "abstract": "  Road ponding, a prevalent traffic hazard, poses a serious threat to road\nsafety by causing vehicles to lose control and leading to accidents ranging\nfrom minor fender benders to severe collisions. Existing technologies struggle\nto accurately identify road ponding due to complex road textures and variable\nponding coloration influenced by reflection characteristics. To address this\nchallenge, we propose a novel approach called Self-Attention-based Global\nSaliency-Enhanced Network (AGSENet) for proactive road ponding detection and\ntraffic safety improvement. AGSENet incorporates saliency detection techniques\nthrough the Channel Saliency Information Focus (CSIF) and Spatial Saliency\nInformation Enhancement (SSIE) modules. The CSIF module, integrated into the\nencoder, employs self-attention to highlight similar features by fusing spatial\nand channel information. The SSIE module, embedded in the decoder, refines edge\nfeatures and reduces noise by leveraging correlations across different feature\nlevels. To ensure accurate and reliable evaluation, we corrected significant\nmislabeling and missing annotations in the Puddle-1000 dataset. Additionally,\nwe constructed the Foggy-Puddle and Night-Puddle datasets for road ponding\ndetection in low-light and foggy conditions, respectively. Experimental results\ndemonstrate that AGSENet outperforms existing methods, achieving IoU\nimprovements of 2.03\\%, 0.62\\%, and 1.06\\% on the Puddle-1000, Foggy-Puddle,\nand Night-Puddle datasets, respectively, setting a new state-of-the-art in this\nfield. Finally, we verified the algorithm's reliability on edge computing\ndevices. This work provides a valuable reference for proactive warning research\nin road traffic safety.\n", "link": "http://arxiv.org/abs/2410.16999v1", "date": "2024-10-22", "relevancy": 2.1281, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5702}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5089}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGSENet%3A%20A%20Robust%20Road%20Ponding%20Detection%20Method%20for%20Proactive%20Traffic%0A%20%20Safety&body=Title%3A%20AGSENet%3A%20A%20Robust%20Road%20Ponding%20Detection%20Method%20for%20Proactive%20Traffic%0A%20%20Safety%0AAuthor%3A%20Ronghui%20Zhang%20and%20Shangyu%20Yang%20and%20Dakang%20Lyu%20and%20Zihan%20Wang%20and%20Junzhou%20Chen%20and%20Yilong%20Ren%20and%20Bolin%20Gao%20and%20Zhihan%20Lv%0AAbstract%3A%20%20%20Road%20ponding%2C%20a%20prevalent%20traffic%20hazard%2C%20poses%20a%20serious%20threat%20to%20road%0Asafety%20by%20causing%20vehicles%20to%20lose%20control%20and%20leading%20to%20accidents%20ranging%0Afrom%20minor%20fender%20benders%20to%20severe%20collisions.%20Existing%20technologies%20struggle%0Ato%20accurately%20identify%20road%20ponding%20due%20to%20complex%20road%20textures%20and%20variable%0Aponding%20coloration%20influenced%20by%20reflection%20characteristics.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20novel%20approach%20called%20Self-Attention-based%20Global%0ASaliency-Enhanced%20Network%20%28AGSENet%29%20for%20proactive%20road%20ponding%20detection%20and%0Atraffic%20safety%20improvement.%20AGSENet%20incorporates%20saliency%20detection%20techniques%0Athrough%20the%20Channel%20Saliency%20Information%20Focus%20%28CSIF%29%20and%20Spatial%20Saliency%0AInformation%20Enhancement%20%28SSIE%29%20modules.%20The%20CSIF%20module%2C%20integrated%20into%20the%0Aencoder%2C%20employs%20self-attention%20to%20highlight%20similar%20features%20by%20fusing%20spatial%0Aand%20channel%20information.%20The%20SSIE%20module%2C%20embedded%20in%20the%20decoder%2C%20refines%20edge%0Afeatures%20and%20reduces%20noise%20by%20leveraging%20correlations%20across%20different%20feature%0Alevels.%20To%20ensure%20accurate%20and%20reliable%20evaluation%2C%20we%20corrected%20significant%0Amislabeling%20and%20missing%20annotations%20in%20the%20Puddle-1000%20dataset.%20Additionally%2C%0Awe%20constructed%20the%20Foggy-Puddle%20and%20Night-Puddle%20datasets%20for%20road%20ponding%0Adetection%20in%20low-light%20and%20foggy%20conditions%2C%20respectively.%20Experimental%20results%0Ademonstrate%20that%20AGSENet%20outperforms%20existing%20methods%2C%20achieving%20IoU%0Aimprovements%20of%202.03%5C%25%2C%200.62%5C%25%2C%20and%201.06%5C%25%20on%20the%20Puddle-1000%2C%20Foggy-Puddle%2C%0Aand%20Night-Puddle%20datasets%2C%20respectively%2C%20setting%20a%20new%20state-of-the-art%20in%20this%0Afield.%20Finally%2C%20we%20verified%20the%20algorithm%27s%20reliability%20on%20edge%20computing%0Adevices.%20This%20work%20provides%20a%20valuable%20reference%20for%20proactive%20warning%20research%0Ain%20road%20traffic%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGSENet%253A%2520A%2520Robust%2520Road%2520Ponding%2520Detection%2520Method%2520for%2520Proactive%2520Traffic%250A%2520%2520Safety%26entry.906535625%3DRonghui%2520Zhang%2520and%2520Shangyu%2520Yang%2520and%2520Dakang%2520Lyu%2520and%2520Zihan%2520Wang%2520and%2520Junzhou%2520Chen%2520and%2520Yilong%2520Ren%2520and%2520Bolin%2520Gao%2520and%2520Zhihan%2520Lv%26entry.1292438233%3D%2520%2520Road%2520ponding%252C%2520a%2520prevalent%2520traffic%2520hazard%252C%2520poses%2520a%2520serious%2520threat%2520to%2520road%250Asafety%2520by%2520causing%2520vehicles%2520to%2520lose%2520control%2520and%2520leading%2520to%2520accidents%2520ranging%250Afrom%2520minor%2520fender%2520benders%2520to%2520severe%2520collisions.%2520Existing%2520technologies%2520struggle%250Ato%2520accurately%2520identify%2520road%2520ponding%2520due%2520to%2520complex%2520road%2520textures%2520and%2520variable%250Aponding%2520coloration%2520influenced%2520by%2520reflection%2520characteristics.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520Self-Attention-based%2520Global%250ASaliency-Enhanced%2520Network%2520%2528AGSENet%2529%2520for%2520proactive%2520road%2520ponding%2520detection%2520and%250Atraffic%2520safety%2520improvement.%2520AGSENet%2520incorporates%2520saliency%2520detection%2520techniques%250Athrough%2520the%2520Channel%2520Saliency%2520Information%2520Focus%2520%2528CSIF%2529%2520and%2520Spatial%2520Saliency%250AInformation%2520Enhancement%2520%2528SSIE%2529%2520modules.%2520The%2520CSIF%2520module%252C%2520integrated%2520into%2520the%250Aencoder%252C%2520employs%2520self-attention%2520to%2520highlight%2520similar%2520features%2520by%2520fusing%2520spatial%250Aand%2520channel%2520information.%2520The%2520SSIE%2520module%252C%2520embedded%2520in%2520the%2520decoder%252C%2520refines%2520edge%250Afeatures%2520and%2520reduces%2520noise%2520by%2520leveraging%2520correlations%2520across%2520different%2520feature%250Alevels.%2520To%2520ensure%2520accurate%2520and%2520reliable%2520evaluation%252C%2520we%2520corrected%2520significant%250Amislabeling%2520and%2520missing%2520annotations%2520in%2520the%2520Puddle-1000%2520dataset.%2520Additionally%252C%250Awe%2520constructed%2520the%2520Foggy-Puddle%2520and%2520Night-Puddle%2520datasets%2520for%2520road%2520ponding%250Adetection%2520in%2520low-light%2520and%2520foggy%2520conditions%252C%2520respectively.%2520Experimental%2520results%250Ademonstrate%2520that%2520AGSENet%2520outperforms%2520existing%2520methods%252C%2520achieving%2520IoU%250Aimprovements%2520of%25202.03%255C%2525%252C%25200.62%255C%2525%252C%2520and%25201.06%255C%2525%2520on%2520the%2520Puddle-1000%252C%2520Foggy-Puddle%252C%250Aand%2520Night-Puddle%2520datasets%252C%2520respectively%252C%2520setting%2520a%2520new%2520state-of-the-art%2520in%2520this%250Afield.%2520Finally%252C%2520we%2520verified%2520the%2520algorithm%2527s%2520reliability%2520on%2520edge%2520computing%250Adevices.%2520This%2520work%2520provides%2520a%2520valuable%2520reference%2520for%2520proactive%2520warning%2520research%250Ain%2520road%2520traffic%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGSENet%3A%20A%20Robust%20Road%20Ponding%20Detection%20Method%20for%20Proactive%20Traffic%0A%20%20Safety&entry.906535625=Ronghui%20Zhang%20and%20Shangyu%20Yang%20and%20Dakang%20Lyu%20and%20Zihan%20Wang%20and%20Junzhou%20Chen%20and%20Yilong%20Ren%20and%20Bolin%20Gao%20and%20Zhihan%20Lv&entry.1292438233=%20%20Road%20ponding%2C%20a%20prevalent%20traffic%20hazard%2C%20poses%20a%20serious%20threat%20to%20road%0Asafety%20by%20causing%20vehicles%20to%20lose%20control%20and%20leading%20to%20accidents%20ranging%0Afrom%20minor%20fender%20benders%20to%20severe%20collisions.%20Existing%20technologies%20struggle%0Ato%20accurately%20identify%20road%20ponding%20due%20to%20complex%20road%20textures%20and%20variable%0Aponding%20coloration%20influenced%20by%20reflection%20characteristics.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20novel%20approach%20called%20Self-Attention-based%20Global%0ASaliency-Enhanced%20Network%20%28AGSENet%29%20for%20proactive%20road%20ponding%20detection%20and%0Atraffic%20safety%20improvement.%20AGSENet%20incorporates%20saliency%20detection%20techniques%0Athrough%20the%20Channel%20Saliency%20Information%20Focus%20%28CSIF%29%20and%20Spatial%20Saliency%0AInformation%20Enhancement%20%28SSIE%29%20modules.%20The%20CSIF%20module%2C%20integrated%20into%20the%0Aencoder%2C%20employs%20self-attention%20to%20highlight%20similar%20features%20by%20fusing%20spatial%0Aand%20channel%20information.%20The%20SSIE%20module%2C%20embedded%20in%20the%20decoder%2C%20refines%20edge%0Afeatures%20and%20reduces%20noise%20by%20leveraging%20correlations%20across%20different%20feature%0Alevels.%20To%20ensure%20accurate%20and%20reliable%20evaluation%2C%20we%20corrected%20significant%0Amislabeling%20and%20missing%20annotations%20in%20the%20Puddle-1000%20dataset.%20Additionally%2C%0Awe%20constructed%20the%20Foggy-Puddle%20and%20Night-Puddle%20datasets%20for%20road%20ponding%0Adetection%20in%20low-light%20and%20foggy%20conditions%2C%20respectively.%20Experimental%20results%0Ademonstrate%20that%20AGSENet%20outperforms%20existing%20methods%2C%20achieving%20IoU%0Aimprovements%20of%202.03%5C%25%2C%200.62%5C%25%2C%20and%201.06%5C%25%20on%20the%20Puddle-1000%2C%20Foggy-Puddle%2C%0Aand%20Night-Puddle%20datasets%2C%20respectively%2C%20setting%20a%20new%20state-of-the-art%20in%20this%0Afield.%20Finally%2C%20we%20verified%20the%20algorithm%27s%20reliability%20on%20edge%20computing%0Adevices.%20This%20work%20provides%20a%20valuable%20reference%20for%20proactive%20warning%20research%0Ain%20road%20traffic%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16999v1&entry.124074799=Read"},
{"title": "ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in\n  Robotic Surgical Skill Assessment", "author": "Julien Quarez and Matthew Elliot and Oscar Maccormac and Marc Modat and Sebastien Ourselin and Jonathan Shapey and Alejandro Granados", "abstract": "  In surgical skill assessment, Objective Structured Assessments of Technical\nSkills (OSATS scores) and the Global Rating Scale (GRS) are established tools\nfor evaluating the performance of surgeons during training. These metrics,\ncoupled with feedback on their performance, enable surgeons to improve and\nachieve standards of practice. Recent studies on the open-source dataset\nJIGSAW, which contains both GRS and OSATS labels, have focused on regressing\nGRS scores from kinematic signals, video data, or a combination of both. In\nthis paper, we argue that regressing the GRS score, a unitless value, by itself\nis too restrictive, and variations throughout the surgical trial do not hold\nsignificant clinical meaning. To address this gap, we developed a recurrent\ntransformer model that outputs the surgeon's performance throughout their\ntraining session by relating the model's hidden states to five OSATS scores\nderived from kinematic signals. These scores are averaged and aggregated to\nproduce a GRS prediction, enabling assessment of the model's performance\nagainst the state-of-the-art (SOTA). We report Spearman's Correlation\nCoefficient (SCC), demonstrating that our model outperforms SOTA models for all\ntasks, except for Suturing under the leave-one-subject-out (LOSO) scheme (SCC\n0.68-0.89), while achieving comparable performance for suturing and across\ntasks under the leave-one-user-out (LOUO) scheme (SCC 0.45-0.68) and beating\nSOTA for Needle Passing (0.69). We argue that relating final OSATS scores to\nshort instances throughout a surgeon's procedure is more clinically meaningful\nthan a single GRS score. This approach also allows us to translate quantitative\npredictions into qualitative feedback, which is crucial for any automated\nsurgical skill assessment pipeline. A senior surgeon validated our model's\nbehaviour and agreed with the semi-supervised predictions 77 \\% (p = 0.006) of\nthe time.\n", "link": "http://arxiv.org/abs/2407.05180v2", "date": "2024-10-22", "relevancy": 2.1154, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5381}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5344}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReCAP%3A%20Recursive%20Cross%20Attention%20Network%20for%20Pseudo-Label%20Generation%20in%0A%20%20Robotic%20Surgical%20Skill%20Assessment&body=Title%3A%20ReCAP%3A%20Recursive%20Cross%20Attention%20Network%20for%20Pseudo-Label%20Generation%20in%0A%20%20Robotic%20Surgical%20Skill%20Assessment%0AAuthor%3A%20Julien%20Quarez%20and%20Matthew%20Elliot%20and%20Oscar%20Maccormac%20and%20Marc%20Modat%20and%20Sebastien%20Ourselin%20and%20Jonathan%20Shapey%20and%20Alejandro%20Granados%0AAbstract%3A%20%20%20In%20surgical%20skill%20assessment%2C%20Objective%20Structured%20Assessments%20of%20Technical%0ASkills%20%28OSATS%20scores%29%20and%20the%20Global%20Rating%20Scale%20%28GRS%29%20are%20established%20tools%0Afor%20evaluating%20the%20performance%20of%20surgeons%20during%20training.%20These%20metrics%2C%0Acoupled%20with%20feedback%20on%20their%20performance%2C%20enable%20surgeons%20to%20improve%20and%0Aachieve%20standards%20of%20practice.%20Recent%20studies%20on%20the%20open-source%20dataset%0AJIGSAW%2C%20which%20contains%20both%20GRS%20and%20OSATS%20labels%2C%20have%20focused%20on%20regressing%0AGRS%20scores%20from%20kinematic%20signals%2C%20video%20data%2C%20or%20a%20combination%20of%20both.%20In%0Athis%20paper%2C%20we%20argue%20that%20regressing%20the%20GRS%20score%2C%20a%20unitless%20value%2C%20by%20itself%0Ais%20too%20restrictive%2C%20and%20variations%20throughout%20the%20surgical%20trial%20do%20not%20hold%0Asignificant%20clinical%20meaning.%20To%20address%20this%20gap%2C%20we%20developed%20a%20recurrent%0Atransformer%20model%20that%20outputs%20the%20surgeon%27s%20performance%20throughout%20their%0Atraining%20session%20by%20relating%20the%20model%27s%20hidden%20states%20to%20five%20OSATS%20scores%0Aderived%20from%20kinematic%20signals.%20These%20scores%20are%20averaged%20and%20aggregated%20to%0Aproduce%20a%20GRS%20prediction%2C%20enabling%20assessment%20of%20the%20model%27s%20performance%0Aagainst%20the%20state-of-the-art%20%28SOTA%29.%20We%20report%20Spearman%27s%20Correlation%0ACoefficient%20%28SCC%29%2C%20demonstrating%20that%20our%20model%20outperforms%20SOTA%20models%20for%20all%0Atasks%2C%20except%20for%20Suturing%20under%20the%20leave-one-subject-out%20%28LOSO%29%20scheme%20%28SCC%0A0.68-0.89%29%2C%20while%20achieving%20comparable%20performance%20for%20suturing%20and%20across%0Atasks%20under%20the%20leave-one-user-out%20%28LOUO%29%20scheme%20%28SCC%200.45-0.68%29%20and%20beating%0ASOTA%20for%20Needle%20Passing%20%280.69%29.%20We%20argue%20that%20relating%20final%20OSATS%20scores%20to%0Ashort%20instances%20throughout%20a%20surgeon%27s%20procedure%20is%20more%20clinically%20meaningful%0Athan%20a%20single%20GRS%20score.%20This%20approach%20also%20allows%20us%20to%20translate%20quantitative%0Apredictions%20into%20qualitative%20feedback%2C%20which%20is%20crucial%20for%20any%20automated%0Asurgical%20skill%20assessment%20pipeline.%20A%20senior%20surgeon%20validated%20our%20model%27s%0Abehaviour%20and%20agreed%20with%20the%20semi-supervised%20predictions%2077%20%5C%25%20%28p%20%3D%200.006%29%20of%0Athe%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReCAP%253A%2520Recursive%2520Cross%2520Attention%2520Network%2520for%2520Pseudo-Label%2520Generation%2520in%250A%2520%2520Robotic%2520Surgical%2520Skill%2520Assessment%26entry.906535625%3DJulien%2520Quarez%2520and%2520Matthew%2520Elliot%2520and%2520Oscar%2520Maccormac%2520and%2520Marc%2520Modat%2520and%2520Sebastien%2520Ourselin%2520and%2520Jonathan%2520Shapey%2520and%2520Alejandro%2520Granados%26entry.1292438233%3D%2520%2520In%2520surgical%2520skill%2520assessment%252C%2520Objective%2520Structured%2520Assessments%2520of%2520Technical%250ASkills%2520%2528OSATS%2520scores%2529%2520and%2520the%2520Global%2520Rating%2520Scale%2520%2528GRS%2529%2520are%2520established%2520tools%250Afor%2520evaluating%2520the%2520performance%2520of%2520surgeons%2520during%2520training.%2520These%2520metrics%252C%250Acoupled%2520with%2520feedback%2520on%2520their%2520performance%252C%2520enable%2520surgeons%2520to%2520improve%2520and%250Aachieve%2520standards%2520of%2520practice.%2520Recent%2520studies%2520on%2520the%2520open-source%2520dataset%250AJIGSAW%252C%2520which%2520contains%2520both%2520GRS%2520and%2520OSATS%2520labels%252C%2520have%2520focused%2520on%2520regressing%250AGRS%2520scores%2520from%2520kinematic%2520signals%252C%2520video%2520data%252C%2520or%2520a%2520combination%2520of%2520both.%2520In%250Athis%2520paper%252C%2520we%2520argue%2520that%2520regressing%2520the%2520GRS%2520score%252C%2520a%2520unitless%2520value%252C%2520by%2520itself%250Ais%2520too%2520restrictive%252C%2520and%2520variations%2520throughout%2520the%2520surgical%2520trial%2520do%2520not%2520hold%250Asignificant%2520clinical%2520meaning.%2520To%2520address%2520this%2520gap%252C%2520we%2520developed%2520a%2520recurrent%250Atransformer%2520model%2520that%2520outputs%2520the%2520surgeon%2527s%2520performance%2520throughout%2520their%250Atraining%2520session%2520by%2520relating%2520the%2520model%2527s%2520hidden%2520states%2520to%2520five%2520OSATS%2520scores%250Aderived%2520from%2520kinematic%2520signals.%2520These%2520scores%2520are%2520averaged%2520and%2520aggregated%2520to%250Aproduce%2520a%2520GRS%2520prediction%252C%2520enabling%2520assessment%2520of%2520the%2520model%2527s%2520performance%250Aagainst%2520the%2520state-of-the-art%2520%2528SOTA%2529.%2520We%2520report%2520Spearman%2527s%2520Correlation%250ACoefficient%2520%2528SCC%2529%252C%2520demonstrating%2520that%2520our%2520model%2520outperforms%2520SOTA%2520models%2520for%2520all%250Atasks%252C%2520except%2520for%2520Suturing%2520under%2520the%2520leave-one-subject-out%2520%2528LOSO%2529%2520scheme%2520%2528SCC%250A0.68-0.89%2529%252C%2520while%2520achieving%2520comparable%2520performance%2520for%2520suturing%2520and%2520across%250Atasks%2520under%2520the%2520leave-one-user-out%2520%2528LOUO%2529%2520scheme%2520%2528SCC%25200.45-0.68%2529%2520and%2520beating%250ASOTA%2520for%2520Needle%2520Passing%2520%25280.69%2529.%2520We%2520argue%2520that%2520relating%2520final%2520OSATS%2520scores%2520to%250Ashort%2520instances%2520throughout%2520a%2520surgeon%2527s%2520procedure%2520is%2520more%2520clinically%2520meaningful%250Athan%2520a%2520single%2520GRS%2520score.%2520This%2520approach%2520also%2520allows%2520us%2520to%2520translate%2520quantitative%250Apredictions%2520into%2520qualitative%2520feedback%252C%2520which%2520is%2520crucial%2520for%2520any%2520automated%250Asurgical%2520skill%2520assessment%2520pipeline.%2520A%2520senior%2520surgeon%2520validated%2520our%2520model%2527s%250Abehaviour%2520and%2520agreed%2520with%2520the%2520semi-supervised%2520predictions%252077%2520%255C%2525%2520%2528p%2520%253D%25200.006%2529%2520of%250Athe%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReCAP%3A%20Recursive%20Cross%20Attention%20Network%20for%20Pseudo-Label%20Generation%20in%0A%20%20Robotic%20Surgical%20Skill%20Assessment&entry.906535625=Julien%20Quarez%20and%20Matthew%20Elliot%20and%20Oscar%20Maccormac%20and%20Marc%20Modat%20and%20Sebastien%20Ourselin%20and%20Jonathan%20Shapey%20and%20Alejandro%20Granados&entry.1292438233=%20%20In%20surgical%20skill%20assessment%2C%20Objective%20Structured%20Assessments%20of%20Technical%0ASkills%20%28OSATS%20scores%29%20and%20the%20Global%20Rating%20Scale%20%28GRS%29%20are%20established%20tools%0Afor%20evaluating%20the%20performance%20of%20surgeons%20during%20training.%20These%20metrics%2C%0Acoupled%20with%20feedback%20on%20their%20performance%2C%20enable%20surgeons%20to%20improve%20and%0Aachieve%20standards%20of%20practice.%20Recent%20studies%20on%20the%20open-source%20dataset%0AJIGSAW%2C%20which%20contains%20both%20GRS%20and%20OSATS%20labels%2C%20have%20focused%20on%20regressing%0AGRS%20scores%20from%20kinematic%20signals%2C%20video%20data%2C%20or%20a%20combination%20of%20both.%20In%0Athis%20paper%2C%20we%20argue%20that%20regressing%20the%20GRS%20score%2C%20a%20unitless%20value%2C%20by%20itself%0Ais%20too%20restrictive%2C%20and%20variations%20throughout%20the%20surgical%20trial%20do%20not%20hold%0Asignificant%20clinical%20meaning.%20To%20address%20this%20gap%2C%20we%20developed%20a%20recurrent%0Atransformer%20model%20that%20outputs%20the%20surgeon%27s%20performance%20throughout%20their%0Atraining%20session%20by%20relating%20the%20model%27s%20hidden%20states%20to%20five%20OSATS%20scores%0Aderived%20from%20kinematic%20signals.%20These%20scores%20are%20averaged%20and%20aggregated%20to%0Aproduce%20a%20GRS%20prediction%2C%20enabling%20assessment%20of%20the%20model%27s%20performance%0Aagainst%20the%20state-of-the-art%20%28SOTA%29.%20We%20report%20Spearman%27s%20Correlation%0ACoefficient%20%28SCC%29%2C%20demonstrating%20that%20our%20model%20outperforms%20SOTA%20models%20for%20all%0Atasks%2C%20except%20for%20Suturing%20under%20the%20leave-one-subject-out%20%28LOSO%29%20scheme%20%28SCC%0A0.68-0.89%29%2C%20while%20achieving%20comparable%20performance%20for%20suturing%20and%20across%0Atasks%20under%20the%20leave-one-user-out%20%28LOUO%29%20scheme%20%28SCC%200.45-0.68%29%20and%20beating%0ASOTA%20for%20Needle%20Passing%20%280.69%29.%20We%20argue%20that%20relating%20final%20OSATS%20scores%20to%0Ashort%20instances%20throughout%20a%20surgeon%27s%20procedure%20is%20more%20clinically%20meaningful%0Athan%20a%20single%20GRS%20score.%20This%20approach%20also%20allows%20us%20to%20translate%20quantitative%0Apredictions%20into%20qualitative%20feedback%2C%20which%20is%20crucial%20for%20any%20automated%0Asurgical%20skill%20assessment%20pipeline.%20A%20senior%20surgeon%20validated%20our%20model%27s%0Abehaviour%20and%20agreed%20with%20the%20semi-supervised%20predictions%2077%20%5C%25%20%28p%20%3D%200.006%29%20of%0Athe%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05180v2&entry.124074799=Read"},
{"title": "Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from\n  Duplex to Monoplex IHC Images", "author": "Nicolas Brieu and Nicolas Triltsch and Philipp Wortmann and Dominik Winter and Shashank Saran and Marlon Rebelatto and G\u00fcnter Schmidt", "abstract": "  Generative models enable the translation from a source image domain where\nreadily trained models are available to a target domain unseen during training.\nWhile Cycle Generative Adversarial Networks (GANs) are well established, the\nassociated cycle consistency constrain relies on that an invertible mapping\nexists between the two domains. This is, however, not the case for the\ntranslation between images stained with chromogenic monoplex and duplex\nimmunohistochemistry (IHC) assays. Focusing on the translation from the latter\nto the first, we propose - through the introduction of a novel training design,\nan alternative constrain leveraging a set of immunofluorescence (IF) images as\nan auxiliary unpaired image domain. Quantitative and qualitative results on a\ndownstream segmentation task show the benefit of the proposed method in\ncomparison to baseline approaches.\n", "link": "http://arxiv.org/abs/2403.07389v2", "date": "2024-10-22", "relevancy": 2.1112, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5473}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5239}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auxiliary%20CycleGAN-guidance%20for%20Task-Aware%20Domain%20Translation%20from%0A%20%20Duplex%20to%20Monoplex%20IHC%20Images&body=Title%3A%20Auxiliary%20CycleGAN-guidance%20for%20Task-Aware%20Domain%20Translation%20from%0A%20%20Duplex%20to%20Monoplex%20IHC%20Images%0AAuthor%3A%20Nicolas%20Brieu%20and%20Nicolas%20Triltsch%20and%20Philipp%20Wortmann%20and%20Dominik%20Winter%20and%20Shashank%20Saran%20and%20Marlon%20Rebelatto%20and%20G%C3%BCnter%20Schmidt%0AAbstract%3A%20%20%20Generative%20models%20enable%20the%20translation%20from%20a%20source%20image%20domain%20where%0Areadily%20trained%20models%20are%20available%20to%20a%20target%20domain%20unseen%20during%20training.%0AWhile%20Cycle%20Generative%20Adversarial%20Networks%20%28GANs%29%20are%20well%20established%2C%20the%0Aassociated%20cycle%20consistency%20constrain%20relies%20on%20that%20an%20invertible%20mapping%0Aexists%20between%20the%20two%20domains.%20This%20is%2C%20however%2C%20not%20the%20case%20for%20the%0Atranslation%20between%20images%20stained%20with%20chromogenic%20monoplex%20and%20duplex%0Aimmunohistochemistry%20%28IHC%29%20assays.%20Focusing%20on%20the%20translation%20from%20the%20latter%0Ato%20the%20first%2C%20we%20propose%20-%20through%20the%20introduction%20of%20a%20novel%20training%20design%2C%0Aan%20alternative%20constrain%20leveraging%20a%20set%20of%20immunofluorescence%20%28IF%29%20images%20as%0Aan%20auxiliary%20unpaired%20image%20domain.%20Quantitative%20and%20qualitative%20results%20on%20a%0Adownstream%20segmentation%20task%20show%20the%20benefit%20of%20the%20proposed%20method%20in%0Acomparison%20to%20baseline%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuxiliary%2520CycleGAN-guidance%2520for%2520Task-Aware%2520Domain%2520Translation%2520from%250A%2520%2520Duplex%2520to%2520Monoplex%2520IHC%2520Images%26entry.906535625%3DNicolas%2520Brieu%2520and%2520Nicolas%2520Triltsch%2520and%2520Philipp%2520Wortmann%2520and%2520Dominik%2520Winter%2520and%2520Shashank%2520Saran%2520and%2520Marlon%2520Rebelatto%2520and%2520G%25C3%25BCnter%2520Schmidt%26entry.1292438233%3D%2520%2520Generative%2520models%2520enable%2520the%2520translation%2520from%2520a%2520source%2520image%2520domain%2520where%250Areadily%2520trained%2520models%2520are%2520available%2520to%2520a%2520target%2520domain%2520unseen%2520during%2520training.%250AWhile%2520Cycle%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520are%2520well%2520established%252C%2520the%250Aassociated%2520cycle%2520consistency%2520constrain%2520relies%2520on%2520that%2520an%2520invertible%2520mapping%250Aexists%2520between%2520the%2520two%2520domains.%2520This%2520is%252C%2520however%252C%2520not%2520the%2520case%2520for%2520the%250Atranslation%2520between%2520images%2520stained%2520with%2520chromogenic%2520monoplex%2520and%2520duplex%250Aimmunohistochemistry%2520%2528IHC%2529%2520assays.%2520Focusing%2520on%2520the%2520translation%2520from%2520the%2520latter%250Ato%2520the%2520first%252C%2520we%2520propose%2520-%2520through%2520the%2520introduction%2520of%2520a%2520novel%2520training%2520design%252C%250Aan%2520alternative%2520constrain%2520leveraging%2520a%2520set%2520of%2520immunofluorescence%2520%2528IF%2529%2520images%2520as%250Aan%2520auxiliary%2520unpaired%2520image%2520domain.%2520Quantitative%2520and%2520qualitative%2520results%2520on%2520a%250Adownstream%2520segmentation%2520task%2520show%2520the%2520benefit%2520of%2520the%2520proposed%2520method%2520in%250Acomparison%2520to%2520baseline%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auxiliary%20CycleGAN-guidance%20for%20Task-Aware%20Domain%20Translation%20from%0A%20%20Duplex%20to%20Monoplex%20IHC%20Images&entry.906535625=Nicolas%20Brieu%20and%20Nicolas%20Triltsch%20and%20Philipp%20Wortmann%20and%20Dominik%20Winter%20and%20Shashank%20Saran%20and%20Marlon%20Rebelatto%20and%20G%C3%BCnter%20Schmidt&entry.1292438233=%20%20Generative%20models%20enable%20the%20translation%20from%20a%20source%20image%20domain%20where%0Areadily%20trained%20models%20are%20available%20to%20a%20target%20domain%20unseen%20during%20training.%0AWhile%20Cycle%20Generative%20Adversarial%20Networks%20%28GANs%29%20are%20well%20established%2C%20the%0Aassociated%20cycle%20consistency%20constrain%20relies%20on%20that%20an%20invertible%20mapping%0Aexists%20between%20the%20two%20domains.%20This%20is%2C%20however%2C%20not%20the%20case%20for%20the%0Atranslation%20between%20images%20stained%20with%20chromogenic%20monoplex%20and%20duplex%0Aimmunohistochemistry%20%28IHC%29%20assays.%20Focusing%20on%20the%20translation%20from%20the%20latter%0Ato%20the%20first%2C%20we%20propose%20-%20through%20the%20introduction%20of%20a%20novel%20training%20design%2C%0Aan%20alternative%20constrain%20leveraging%20a%20set%20of%20immunofluorescence%20%28IF%29%20images%20as%0Aan%20auxiliary%20unpaired%20image%20domain.%20Quantitative%20and%20qualitative%20results%20on%20a%0Adownstream%20segmentation%20task%20show%20the%20benefit%20of%20the%20proposed%20method%20in%0Acomparison%20to%20baseline%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07389v2&entry.124074799=Read"},
{"title": "MBD: Multi b-value Denoising of Diffusion Magnetic Resonance Images", "author": "Jakub Jurek and Andrzej Materka and Kamil Ludwisiak and Agata Majos and Filip Szczepankiewicz", "abstract": "  We propose a novel approach to denoising diffusion magnetic resonance images\n(dMRI) using convolutional neural networks, that exploits the benefits of data\nacquired at multiple b-values to offset the need for many redundant\nobservations. Denoising is especially relevant in dMRI since noise can have a\ndeleterious impact on both quantification accuracy and image preprocessing. The\nmost successful methods proposed to date, like Marchenko-Pastur Principal\nComponent Analysis (MPPCA) denoising, are tailored to diffusion-weighting\nrepeated for many encoding directions. They exploit high redundancy of the\ndataset that oversamples the diffusion-encoding direction space, since many\ndirections have collinear components.\n  However, there are many dMRI techniques that do not entail a large number of\nencoding directions or repetitions, and are therefore less suited to this\napproach. For example, clinical dMRI exams may include as few as three encoding\ndirections, with low or negligible data redundancy across directions. Moreover,\npromising new dMRI approaches, like spherical b-tensor encoding (STE), benefit\nfrom high b-values while sensitizing the signal to diffusion along all\ndirections in just a single shot.\n  We introduce a convolutional neural network approach that we call\nmulti-b-value-based denoising (MBD). MBD exploits the similarity in\ndiffusion-weighted images (DWI) across different b-values but along the same\ndiffusion encoding direction. It allows denoising of diffusion images with high\nnoise variance while avoiding blurring, and using just a small number input\nimages.\n", "link": "http://arxiv.org/abs/2410.16898v1", "date": "2024-10-22", "relevancy": 2.109, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5586}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5233}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MBD%3A%20Multi%20b-value%20Denoising%20of%20Diffusion%20Magnetic%20Resonance%20Images&body=Title%3A%20MBD%3A%20Multi%20b-value%20Denoising%20of%20Diffusion%20Magnetic%20Resonance%20Images%0AAuthor%3A%20Jakub%20Jurek%20and%20Andrzej%20Materka%20and%20Kamil%20Ludwisiak%20and%20Agata%20Majos%20and%20Filip%20Szczepankiewicz%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20to%20denoising%20diffusion%20magnetic%20resonance%20images%0A%28dMRI%29%20using%20convolutional%20neural%20networks%2C%20that%20exploits%20the%20benefits%20of%20data%0Aacquired%20at%20multiple%20b-values%20to%20offset%20the%20need%20for%20many%20redundant%0Aobservations.%20Denoising%20is%20especially%20relevant%20in%20dMRI%20since%20noise%20can%20have%20a%0Adeleterious%20impact%20on%20both%20quantification%20accuracy%20and%20image%20preprocessing.%20The%0Amost%20successful%20methods%20proposed%20to%20date%2C%20like%20Marchenko-Pastur%20Principal%0AComponent%20Analysis%20%28MPPCA%29%20denoising%2C%20are%20tailored%20to%20diffusion-weighting%0Arepeated%20for%20many%20encoding%20directions.%20They%20exploit%20high%20redundancy%20of%20the%0Adataset%20that%20oversamples%20the%20diffusion-encoding%20direction%20space%2C%20since%20many%0Adirections%20have%20collinear%20components.%0A%20%20However%2C%20there%20are%20many%20dMRI%20techniques%20that%20do%20not%20entail%20a%20large%20number%20of%0Aencoding%20directions%20or%20repetitions%2C%20and%20are%20therefore%20less%20suited%20to%20this%0Aapproach.%20For%20example%2C%20clinical%20dMRI%20exams%20may%20include%20as%20few%20as%20three%20encoding%0Adirections%2C%20with%20low%20or%20negligible%20data%20redundancy%20across%20directions.%20Moreover%2C%0Apromising%20new%20dMRI%20approaches%2C%20like%20spherical%20b-tensor%20encoding%20%28STE%29%2C%20benefit%0Afrom%20high%20b-values%20while%20sensitizing%20the%20signal%20to%20diffusion%20along%20all%0Adirections%20in%20just%20a%20single%20shot.%0A%20%20We%20introduce%20a%20convolutional%20neural%20network%20approach%20that%20we%20call%0Amulti-b-value-based%20denoising%20%28MBD%29.%20MBD%20exploits%20the%20similarity%20in%0Adiffusion-weighted%20images%20%28DWI%29%20across%20different%20b-values%20but%20along%20the%20same%0Adiffusion%20encoding%20direction.%20It%20allows%20denoising%20of%20diffusion%20images%20with%20high%0Anoise%20variance%20while%20avoiding%20blurring%2C%20and%20using%20just%20a%20small%20number%20input%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMBD%253A%2520Multi%2520b-value%2520Denoising%2520of%2520Diffusion%2520Magnetic%2520Resonance%2520Images%26entry.906535625%3DJakub%2520Jurek%2520and%2520Andrzej%2520Materka%2520and%2520Kamil%2520Ludwisiak%2520and%2520Agata%2520Majos%2520and%2520Filip%2520Szczepankiewicz%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520to%2520denoising%2520diffusion%2520magnetic%2520resonance%2520images%250A%2528dMRI%2529%2520using%2520convolutional%2520neural%2520networks%252C%2520that%2520exploits%2520the%2520benefits%2520of%2520data%250Aacquired%2520at%2520multiple%2520b-values%2520to%2520offset%2520the%2520need%2520for%2520many%2520redundant%250Aobservations.%2520Denoising%2520is%2520especially%2520relevant%2520in%2520dMRI%2520since%2520noise%2520can%2520have%2520a%250Adeleterious%2520impact%2520on%2520both%2520quantification%2520accuracy%2520and%2520image%2520preprocessing.%2520The%250Amost%2520successful%2520methods%2520proposed%2520to%2520date%252C%2520like%2520Marchenko-Pastur%2520Principal%250AComponent%2520Analysis%2520%2528MPPCA%2529%2520denoising%252C%2520are%2520tailored%2520to%2520diffusion-weighting%250Arepeated%2520for%2520many%2520encoding%2520directions.%2520They%2520exploit%2520high%2520redundancy%2520of%2520the%250Adataset%2520that%2520oversamples%2520the%2520diffusion-encoding%2520direction%2520space%252C%2520since%2520many%250Adirections%2520have%2520collinear%2520components.%250A%2520%2520However%252C%2520there%2520are%2520many%2520dMRI%2520techniques%2520that%2520do%2520not%2520entail%2520a%2520large%2520number%2520of%250Aencoding%2520directions%2520or%2520repetitions%252C%2520and%2520are%2520therefore%2520less%2520suited%2520to%2520this%250Aapproach.%2520For%2520example%252C%2520clinical%2520dMRI%2520exams%2520may%2520include%2520as%2520few%2520as%2520three%2520encoding%250Adirections%252C%2520with%2520low%2520or%2520negligible%2520data%2520redundancy%2520across%2520directions.%2520Moreover%252C%250Apromising%2520new%2520dMRI%2520approaches%252C%2520like%2520spherical%2520b-tensor%2520encoding%2520%2528STE%2529%252C%2520benefit%250Afrom%2520high%2520b-values%2520while%2520sensitizing%2520the%2520signal%2520to%2520diffusion%2520along%2520all%250Adirections%2520in%2520just%2520a%2520single%2520shot.%250A%2520%2520We%2520introduce%2520a%2520convolutional%2520neural%2520network%2520approach%2520that%2520we%2520call%250Amulti-b-value-based%2520denoising%2520%2528MBD%2529.%2520MBD%2520exploits%2520the%2520similarity%2520in%250Adiffusion-weighted%2520images%2520%2528DWI%2529%2520across%2520different%2520b-values%2520but%2520along%2520the%2520same%250Adiffusion%2520encoding%2520direction.%2520It%2520allows%2520denoising%2520of%2520diffusion%2520images%2520with%2520high%250Anoise%2520variance%2520while%2520avoiding%2520blurring%252C%2520and%2520using%2520just%2520a%2520small%2520number%2520input%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MBD%3A%20Multi%20b-value%20Denoising%20of%20Diffusion%20Magnetic%20Resonance%20Images&entry.906535625=Jakub%20Jurek%20and%20Andrzej%20Materka%20and%20Kamil%20Ludwisiak%20and%20Agata%20Majos%20and%20Filip%20Szczepankiewicz&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20to%20denoising%20diffusion%20magnetic%20resonance%20images%0A%28dMRI%29%20using%20convolutional%20neural%20networks%2C%20that%20exploits%20the%20benefits%20of%20data%0Aacquired%20at%20multiple%20b-values%20to%20offset%20the%20need%20for%20many%20redundant%0Aobservations.%20Denoising%20is%20especially%20relevant%20in%20dMRI%20since%20noise%20can%20have%20a%0Adeleterious%20impact%20on%20both%20quantification%20accuracy%20and%20image%20preprocessing.%20The%0Amost%20successful%20methods%20proposed%20to%20date%2C%20like%20Marchenko-Pastur%20Principal%0AComponent%20Analysis%20%28MPPCA%29%20denoising%2C%20are%20tailored%20to%20diffusion-weighting%0Arepeated%20for%20many%20encoding%20directions.%20They%20exploit%20high%20redundancy%20of%20the%0Adataset%20that%20oversamples%20the%20diffusion-encoding%20direction%20space%2C%20since%20many%0Adirections%20have%20collinear%20components.%0A%20%20However%2C%20there%20are%20many%20dMRI%20techniques%20that%20do%20not%20entail%20a%20large%20number%20of%0Aencoding%20directions%20or%20repetitions%2C%20and%20are%20therefore%20less%20suited%20to%20this%0Aapproach.%20For%20example%2C%20clinical%20dMRI%20exams%20may%20include%20as%20few%20as%20three%20encoding%0Adirections%2C%20with%20low%20or%20negligible%20data%20redundancy%20across%20directions.%20Moreover%2C%0Apromising%20new%20dMRI%20approaches%2C%20like%20spherical%20b-tensor%20encoding%20%28STE%29%2C%20benefit%0Afrom%20high%20b-values%20while%20sensitizing%20the%20signal%20to%20diffusion%20along%20all%0Adirections%20in%20just%20a%20single%20shot.%0A%20%20We%20introduce%20a%20convolutional%20neural%20network%20approach%20that%20we%20call%0Amulti-b-value-based%20denoising%20%28MBD%29.%20MBD%20exploits%20the%20similarity%20in%0Adiffusion-weighted%20images%20%28DWI%29%20across%20different%20b-values%20but%20along%20the%20same%0Adiffusion%20encoding%20direction.%20It%20allows%20denoising%20of%20diffusion%20images%20with%20high%0Anoise%20variance%20while%20avoiding%20blurring%2C%20and%20using%20just%20a%20small%20number%20input%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16898v1&entry.124074799=Read"},
{"title": "Advancing Training Efficiency of Deep Spiking Neural Networks through\n  Rate-based Backpropagation", "author": "Chengting Yu and Lei Liu and Gaoang Wang and Erping Li and Aili Wang", "abstract": "  Recent insights have revealed that rate-coding is a primary form of\ninformation representation captured by surrogate-gradient-based Backpropagation\nThrough Time (BPTT) in training deep Spiking Neural Networks (SNNs). Motivated\nby these findings, we propose rate-based backpropagation, a training strategy\nspecifically designed to exploit rate-based representations to reduce the\ncomplexity of BPTT. Our method minimizes reliance on detailed temporal\nderivatives by focusing on averaged dynamics, streamlining the computational\ngraph to reduce memory and computational demands of SNNs training. We\nsubstantiate the rationality of the gradient approximation between BPTT and the\nproposed method through both theoretical analysis and empirical observations.\nComprehensive experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS\nvalidate that our method achieves comparable performance to BPTT counterparts,\nand surpasses state-of-the-art efficient training techniques. By leveraging the\ninherent benefits of rate-coding, this work sets the stage for more scalable\nand efficient SNNs training within resource-constrained environments. Our code\nis available at https://github.com/Tab-ct/rate-based-backpropagation.\n", "link": "http://arxiv.org/abs/2410.11488v2", "date": "2024-10-22", "relevancy": 2.1015, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5339}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5219}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Training%20Efficiency%20of%20Deep%20Spiking%20Neural%20Networks%20through%0A%20%20Rate-based%20Backpropagation&body=Title%3A%20Advancing%20Training%20Efficiency%20of%20Deep%20Spiking%20Neural%20Networks%20through%0A%20%20Rate-based%20Backpropagation%0AAuthor%3A%20Chengting%20Yu%20and%20Lei%20Liu%20and%20Gaoang%20Wang%20and%20Erping%20Li%20and%20Aili%20Wang%0AAbstract%3A%20%20%20Recent%20insights%20have%20revealed%20that%20rate-coding%20is%20a%20primary%20form%20of%0Ainformation%20representation%20captured%20by%20surrogate-gradient-based%20Backpropagation%0AThrough%20Time%20%28BPTT%29%20in%20training%20deep%20Spiking%20Neural%20Networks%20%28SNNs%29.%20Motivated%0Aby%20these%20findings%2C%20we%20propose%20rate-based%20backpropagation%2C%20a%20training%20strategy%0Aspecifically%20designed%20to%20exploit%20rate-based%20representations%20to%20reduce%20the%0Acomplexity%20of%20BPTT.%20Our%20method%20minimizes%20reliance%20on%20detailed%20temporal%0Aderivatives%20by%20focusing%20on%20averaged%20dynamics%2C%20streamlining%20the%20computational%0Agraph%20to%20reduce%20memory%20and%20computational%20demands%20of%20SNNs%20training.%20We%0Asubstantiate%20the%20rationality%20of%20the%20gradient%20approximation%20between%20BPTT%20and%20the%0Aproposed%20method%20through%20both%20theoretical%20analysis%20and%20empirical%20observations.%0AComprehensive%20experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20ImageNet%2C%20and%20CIFAR10-DVS%0Avalidate%20that%20our%20method%20achieves%20comparable%20performance%20to%20BPTT%20counterparts%2C%0Aand%20surpasses%20state-of-the-art%20efficient%20training%20techniques.%20By%20leveraging%20the%0Ainherent%20benefits%20of%20rate-coding%2C%20this%20work%20sets%20the%20stage%20for%20more%20scalable%0Aand%20efficient%20SNNs%20training%20within%20resource-constrained%20environments.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/Tab-ct/rate-based-backpropagation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Training%2520Efficiency%2520of%2520Deep%2520Spiking%2520Neural%2520Networks%2520through%250A%2520%2520Rate-based%2520Backpropagation%26entry.906535625%3DChengting%2520Yu%2520and%2520Lei%2520Liu%2520and%2520Gaoang%2520Wang%2520and%2520Erping%2520Li%2520and%2520Aili%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520insights%2520have%2520revealed%2520that%2520rate-coding%2520is%2520a%2520primary%2520form%2520of%250Ainformation%2520representation%2520captured%2520by%2520surrogate-gradient-based%2520Backpropagation%250AThrough%2520Time%2520%2528BPTT%2529%2520in%2520training%2520deep%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529.%2520Motivated%250Aby%2520these%2520findings%252C%2520we%2520propose%2520rate-based%2520backpropagation%252C%2520a%2520training%2520strategy%250Aspecifically%2520designed%2520to%2520exploit%2520rate-based%2520representations%2520to%2520reduce%2520the%250Acomplexity%2520of%2520BPTT.%2520Our%2520method%2520minimizes%2520reliance%2520on%2520detailed%2520temporal%250Aderivatives%2520by%2520focusing%2520on%2520averaged%2520dynamics%252C%2520streamlining%2520the%2520computational%250Agraph%2520to%2520reduce%2520memory%2520and%2520computational%2520demands%2520of%2520SNNs%2520training.%2520We%250Asubstantiate%2520the%2520rationality%2520of%2520the%2520gradient%2520approximation%2520between%2520BPTT%2520and%2520the%250Aproposed%2520method%2520through%2520both%2520theoretical%2520analysis%2520and%2520empirical%2520observations.%250AComprehensive%2520experiments%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520ImageNet%252C%2520and%2520CIFAR10-DVS%250Avalidate%2520that%2520our%2520method%2520achieves%2520comparable%2520performance%2520to%2520BPTT%2520counterparts%252C%250Aand%2520surpasses%2520state-of-the-art%2520efficient%2520training%2520techniques.%2520By%2520leveraging%2520the%250Ainherent%2520benefits%2520of%2520rate-coding%252C%2520this%2520work%2520sets%2520the%2520stage%2520for%2520more%2520scalable%250Aand%2520efficient%2520SNNs%2520training%2520within%2520resource-constrained%2520environments.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/Tab-ct/rate-based-backpropagation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Training%20Efficiency%20of%20Deep%20Spiking%20Neural%20Networks%20through%0A%20%20Rate-based%20Backpropagation&entry.906535625=Chengting%20Yu%20and%20Lei%20Liu%20and%20Gaoang%20Wang%20and%20Erping%20Li%20and%20Aili%20Wang&entry.1292438233=%20%20Recent%20insights%20have%20revealed%20that%20rate-coding%20is%20a%20primary%20form%20of%0Ainformation%20representation%20captured%20by%20surrogate-gradient-based%20Backpropagation%0AThrough%20Time%20%28BPTT%29%20in%20training%20deep%20Spiking%20Neural%20Networks%20%28SNNs%29.%20Motivated%0Aby%20these%20findings%2C%20we%20propose%20rate-based%20backpropagation%2C%20a%20training%20strategy%0Aspecifically%20designed%20to%20exploit%20rate-based%20representations%20to%20reduce%20the%0Acomplexity%20of%20BPTT.%20Our%20method%20minimizes%20reliance%20on%20detailed%20temporal%0Aderivatives%20by%20focusing%20on%20averaged%20dynamics%2C%20streamlining%20the%20computational%0Agraph%20to%20reduce%20memory%20and%20computational%20demands%20of%20SNNs%20training.%20We%0Asubstantiate%20the%20rationality%20of%20the%20gradient%20approximation%20between%20BPTT%20and%20the%0Aproposed%20method%20through%20both%20theoretical%20analysis%20and%20empirical%20observations.%0AComprehensive%20experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20ImageNet%2C%20and%20CIFAR10-DVS%0Avalidate%20that%20our%20method%20achieves%20comparable%20performance%20to%20BPTT%20counterparts%2C%0Aand%20surpasses%20state-of-the-art%20efficient%20training%20techniques.%20By%20leveraging%20the%0Ainherent%20benefits%20of%20rate-coding%2C%20this%20work%20sets%20the%20stage%20for%20more%20scalable%0Aand%20efficient%20SNNs%20training%20within%20resource-constrained%20environments.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/Tab-ct/rate-based-backpropagation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11488v2&entry.124074799=Read"},
{"title": "LFME: A Simple Framework for Learning from Multiple Experts in Domain\n  Generalization", "author": "Liang Chen and Yong Zhang and Yibing Song and Zhiqiang Shen and Lingqiao Liu", "abstract": "  Domain generalization (DG) methods aim to maintain good performance in an\nunseen target domain by using training data from multiple source domains. While\nsuccess on certain occasions are observed, enhancing the baseline across most\nscenarios remains challenging. This work introduces a simple yet effective\nframework, dubbed learning from multiple experts (LFME), that aims to make the\ntarget model an expert in all source domains to improve DG. Specifically,\nbesides learning the target model used in inference, LFME will also train\nmultiple experts specialized in different domains, whose output probabilities\nprovide professional guidance by simply regularizing the logit of the target\nmodel. Delving deep into the framework, we reveal that the introduced logit\nregularization term implicitly provides effects of enabling the target model to\nharness more information, and mining hard samples from the experts during\ntraining. Extensive experiments on benchmarks from different DG tasks\ndemonstrate that LFME is consistently beneficial to the baseline and can\nachieve comparable performance to existing arts. Code is available\nat~\\url{https://github.com/liangchen527/LFME}.\n", "link": "http://arxiv.org/abs/2410.17020v1", "date": "2024-10-22", "relevancy": 2.0945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LFME%3A%20A%20Simple%20Framework%20for%20Learning%20from%20Multiple%20Experts%20in%20Domain%0A%20%20Generalization&body=Title%3A%20LFME%3A%20A%20Simple%20Framework%20for%20Learning%20from%20Multiple%20Experts%20in%20Domain%0A%20%20Generalization%0AAuthor%3A%20Liang%20Chen%20and%20Yong%20Zhang%20and%20Yibing%20Song%20and%20Zhiqiang%20Shen%20and%20Lingqiao%20Liu%0AAbstract%3A%20%20%20Domain%20generalization%20%28DG%29%20methods%20aim%20to%20maintain%20good%20performance%20in%20an%0Aunseen%20target%20domain%20by%20using%20training%20data%20from%20multiple%20source%20domains.%20While%0Asuccess%20on%20certain%20occasions%20are%20observed%2C%20enhancing%20the%20baseline%20across%20most%0Ascenarios%20remains%20challenging.%20This%20work%20introduces%20a%20simple%20yet%20effective%0Aframework%2C%20dubbed%20learning%20from%20multiple%20experts%20%28LFME%29%2C%20that%20aims%20to%20make%20the%0Atarget%20model%20an%20expert%20in%20all%20source%20domains%20to%20improve%20DG.%20Specifically%2C%0Abesides%20learning%20the%20target%20model%20used%20in%20inference%2C%20LFME%20will%20also%20train%0Amultiple%20experts%20specialized%20in%20different%20domains%2C%20whose%20output%20probabilities%0Aprovide%20professional%20guidance%20by%20simply%20regularizing%20the%20logit%20of%20the%20target%0Amodel.%20Delving%20deep%20into%20the%20framework%2C%20we%20reveal%20that%20the%20introduced%20logit%0Aregularization%20term%20implicitly%20provides%20effects%20of%20enabling%20the%20target%20model%20to%0Aharness%20more%20information%2C%20and%20mining%20hard%20samples%20from%20the%20experts%20during%0Atraining.%20Extensive%20experiments%20on%20benchmarks%20from%20different%20DG%20tasks%0Ademonstrate%20that%20LFME%20is%20consistently%20beneficial%20to%20the%20baseline%20and%20can%0Aachieve%20comparable%20performance%20to%20existing%20arts.%20Code%20is%20available%0Aat~%5Curl%7Bhttps%3A//github.com/liangchen527/LFME%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLFME%253A%2520A%2520Simple%2520Framework%2520for%2520Learning%2520from%2520Multiple%2520Experts%2520in%2520Domain%250A%2520%2520Generalization%26entry.906535625%3DLiang%2520Chen%2520and%2520Yong%2520Zhang%2520and%2520Yibing%2520Song%2520and%2520Zhiqiang%2520Shen%2520and%2520Lingqiao%2520Liu%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520%2528DG%2529%2520methods%2520aim%2520to%2520maintain%2520good%2520performance%2520in%2520an%250Aunseen%2520target%2520domain%2520by%2520using%2520training%2520data%2520from%2520multiple%2520source%2520domains.%2520While%250Asuccess%2520on%2520certain%2520occasions%2520are%2520observed%252C%2520enhancing%2520the%2520baseline%2520across%2520most%250Ascenarios%2520remains%2520challenging.%2520This%2520work%2520introduces%2520a%2520simple%2520yet%2520effective%250Aframework%252C%2520dubbed%2520learning%2520from%2520multiple%2520experts%2520%2528LFME%2529%252C%2520that%2520aims%2520to%2520make%2520the%250Atarget%2520model%2520an%2520expert%2520in%2520all%2520source%2520domains%2520to%2520improve%2520DG.%2520Specifically%252C%250Abesides%2520learning%2520the%2520target%2520model%2520used%2520in%2520inference%252C%2520LFME%2520will%2520also%2520train%250Amultiple%2520experts%2520specialized%2520in%2520different%2520domains%252C%2520whose%2520output%2520probabilities%250Aprovide%2520professional%2520guidance%2520by%2520simply%2520regularizing%2520the%2520logit%2520of%2520the%2520target%250Amodel.%2520Delving%2520deep%2520into%2520the%2520framework%252C%2520we%2520reveal%2520that%2520the%2520introduced%2520logit%250Aregularization%2520term%2520implicitly%2520provides%2520effects%2520of%2520enabling%2520the%2520target%2520model%2520to%250Aharness%2520more%2520information%252C%2520and%2520mining%2520hard%2520samples%2520from%2520the%2520experts%2520during%250Atraining.%2520Extensive%2520experiments%2520on%2520benchmarks%2520from%2520different%2520DG%2520tasks%250Ademonstrate%2520that%2520LFME%2520is%2520consistently%2520beneficial%2520to%2520the%2520baseline%2520and%2520can%250Aachieve%2520comparable%2520performance%2520to%2520existing%2520arts.%2520Code%2520is%2520available%250Aat~%255Curl%257Bhttps%253A//github.com/liangchen527/LFME%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LFME%3A%20A%20Simple%20Framework%20for%20Learning%20from%20Multiple%20Experts%20in%20Domain%0A%20%20Generalization&entry.906535625=Liang%20Chen%20and%20Yong%20Zhang%20and%20Yibing%20Song%20and%20Zhiqiang%20Shen%20and%20Lingqiao%20Liu&entry.1292438233=%20%20Domain%20generalization%20%28DG%29%20methods%20aim%20to%20maintain%20good%20performance%20in%20an%0Aunseen%20target%20domain%20by%20using%20training%20data%20from%20multiple%20source%20domains.%20While%0Asuccess%20on%20certain%20occasions%20are%20observed%2C%20enhancing%20the%20baseline%20across%20most%0Ascenarios%20remains%20challenging.%20This%20work%20introduces%20a%20simple%20yet%20effective%0Aframework%2C%20dubbed%20learning%20from%20multiple%20experts%20%28LFME%29%2C%20that%20aims%20to%20make%20the%0Atarget%20model%20an%20expert%20in%20all%20source%20domains%20to%20improve%20DG.%20Specifically%2C%0Abesides%20learning%20the%20target%20model%20used%20in%20inference%2C%20LFME%20will%20also%20train%0Amultiple%20experts%20specialized%20in%20different%20domains%2C%20whose%20output%20probabilities%0Aprovide%20professional%20guidance%20by%20simply%20regularizing%20the%20logit%20of%20the%20target%0Amodel.%20Delving%20deep%20into%20the%20framework%2C%20we%20reveal%20that%20the%20introduced%20logit%0Aregularization%20term%20implicitly%20provides%20effects%20of%20enabling%20the%20target%20model%20to%0Aharness%20more%20information%2C%20and%20mining%20hard%20samples%20from%20the%20experts%20during%0Atraining.%20Extensive%20experiments%20on%20benchmarks%20from%20different%20DG%20tasks%0Ademonstrate%20that%20LFME%20is%20consistently%20beneficial%20to%20the%20baseline%20and%20can%0Aachieve%20comparable%20performance%20to%20existing%20arts.%20Code%20is%20available%0Aat~%5Curl%7Bhttps%3A//github.com/liangchen527/LFME%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17020v1&entry.124074799=Read"},
{"title": "FlightAR: AR Flight Assistance Interface with Multiple Video Streams and\n  Object Detection Aimed at Immersive Drone Control", "author": "Oleg Sautenkov and Selamawit Asfaw and Yasheerah Yaqoot and Muhammad Ahsan Mustafa and Aleksey Fedoseev and Daria Trinitatova and Dzmitry Tsetserukou", "abstract": "  The swift advancement of unmanned aerial vehicle (UAV) technologies\nnecessitates new standards for developing human-drone interaction (HDI)\ninterfaces. Most interfaces for HDI, especially first-person view (FPV)\ngoggles, limit the operator's ability to obtain information from the\nenvironment. This paper presents a novel interface, FlightAR, that integrates\naugmented reality (AR) overlays of UAV first-person view (FPV) and bottom\ncamera feeds with head-mounted display (HMD) to enhance the pilot's situational\nawareness. Using FlightAR, the system provides pilots not only with a video\nstream from several UAV cameras simultaneously, but also the ability to observe\ntheir surroundings in real time. User evaluation with NASA-TLX and UEQ surveys\nshowed low physical demand ($\\mu=1.8$, $SD = 0.8$) and good performance\n($\\mu=3.4$, $SD = 0.8$), proving better user assessments in comparison with\nbaseline FPV goggles. Participants also rated the system highly for stimulation\n($\\mu=2.35$, $SD = 0.9$), novelty ($\\mu=2.1$, $SD = 0.9$) and attractiveness\n($\\mu=1.97$, $SD = 1$), indicating positive user experiences. These results\ndemonstrate the potential of the system to improve UAV piloting experience\nthrough enhanced situational awareness and intuitive control. The code is\navailable here: https://github.com/Sautenich/FlightAR\n", "link": "http://arxiv.org/abs/2410.16943v1", "date": "2024-10-22", "relevancy": 2.086, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5354}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5157}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlightAR%3A%20AR%20Flight%20Assistance%20Interface%20with%20Multiple%20Video%20Streams%20and%0A%20%20Object%20Detection%20Aimed%20at%20Immersive%20Drone%20Control&body=Title%3A%20FlightAR%3A%20AR%20Flight%20Assistance%20Interface%20with%20Multiple%20Video%20Streams%20and%0A%20%20Object%20Detection%20Aimed%20at%20Immersive%20Drone%20Control%0AAuthor%3A%20Oleg%20Sautenkov%20and%20Selamawit%20Asfaw%20and%20Yasheerah%20Yaqoot%20and%20Muhammad%20Ahsan%20Mustafa%20and%20Aleksey%20Fedoseev%20and%20Daria%20Trinitatova%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20The%20swift%20advancement%20of%20unmanned%20aerial%20vehicle%20%28UAV%29%20technologies%0Anecessitates%20new%20standards%20for%20developing%20human-drone%20interaction%20%28HDI%29%0Ainterfaces.%20Most%20interfaces%20for%20HDI%2C%20especially%20first-person%20view%20%28FPV%29%0Agoggles%2C%20limit%20the%20operator%27s%20ability%20to%20obtain%20information%20from%20the%0Aenvironment.%20This%20paper%20presents%20a%20novel%20interface%2C%20FlightAR%2C%20that%20integrates%0Aaugmented%20reality%20%28AR%29%20overlays%20of%20UAV%20first-person%20view%20%28FPV%29%20and%20bottom%0Acamera%20feeds%20with%20head-mounted%20display%20%28HMD%29%20to%20enhance%20the%20pilot%27s%20situational%0Aawareness.%20Using%20FlightAR%2C%20the%20system%20provides%20pilots%20not%20only%20with%20a%20video%0Astream%20from%20several%20UAV%20cameras%20simultaneously%2C%20but%20also%20the%20ability%20to%20observe%0Atheir%20surroundings%20in%20real%20time.%20User%20evaluation%20with%20NASA-TLX%20and%20UEQ%20surveys%0Ashowed%20low%20physical%20demand%20%28%24%5Cmu%3D1.8%24%2C%20%24SD%20%3D%200.8%24%29%20and%20good%20performance%0A%28%24%5Cmu%3D3.4%24%2C%20%24SD%20%3D%200.8%24%29%2C%20proving%20better%20user%20assessments%20in%20comparison%20with%0Abaseline%20FPV%20goggles.%20Participants%20also%20rated%20the%20system%20highly%20for%20stimulation%0A%28%24%5Cmu%3D2.35%24%2C%20%24SD%20%3D%200.9%24%29%2C%20novelty%20%28%24%5Cmu%3D2.1%24%2C%20%24SD%20%3D%200.9%24%29%20and%20attractiveness%0A%28%24%5Cmu%3D1.97%24%2C%20%24SD%20%3D%201%24%29%2C%20indicating%20positive%20user%20experiences.%20These%20results%0Ademonstrate%20the%20potential%20of%20the%20system%20to%20improve%20UAV%20piloting%20experience%0Athrough%20enhanced%20situational%20awareness%20and%20intuitive%20control.%20The%20code%20is%0Aavailable%20here%3A%20https%3A//github.com/Sautenich/FlightAR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlightAR%253A%2520AR%2520Flight%2520Assistance%2520Interface%2520with%2520Multiple%2520Video%2520Streams%2520and%250A%2520%2520Object%2520Detection%2520Aimed%2520at%2520Immersive%2520Drone%2520Control%26entry.906535625%3DOleg%2520Sautenkov%2520and%2520Selamawit%2520Asfaw%2520and%2520Yasheerah%2520Yaqoot%2520and%2520Muhammad%2520Ahsan%2520Mustafa%2520and%2520Aleksey%2520Fedoseev%2520and%2520Daria%2520Trinitatova%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520The%2520swift%2520advancement%2520of%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%2520technologies%250Anecessitates%2520new%2520standards%2520for%2520developing%2520human-drone%2520interaction%2520%2528HDI%2529%250Ainterfaces.%2520Most%2520interfaces%2520for%2520HDI%252C%2520especially%2520first-person%2520view%2520%2528FPV%2529%250Agoggles%252C%2520limit%2520the%2520operator%2527s%2520ability%2520to%2520obtain%2520information%2520from%2520the%250Aenvironment.%2520This%2520paper%2520presents%2520a%2520novel%2520interface%252C%2520FlightAR%252C%2520that%2520integrates%250Aaugmented%2520reality%2520%2528AR%2529%2520overlays%2520of%2520UAV%2520first-person%2520view%2520%2528FPV%2529%2520and%2520bottom%250Acamera%2520feeds%2520with%2520head-mounted%2520display%2520%2528HMD%2529%2520to%2520enhance%2520the%2520pilot%2527s%2520situational%250Aawareness.%2520Using%2520FlightAR%252C%2520the%2520system%2520provides%2520pilots%2520not%2520only%2520with%2520a%2520video%250Astream%2520from%2520several%2520UAV%2520cameras%2520simultaneously%252C%2520but%2520also%2520the%2520ability%2520to%2520observe%250Atheir%2520surroundings%2520in%2520real%2520time.%2520User%2520evaluation%2520with%2520NASA-TLX%2520and%2520UEQ%2520surveys%250Ashowed%2520low%2520physical%2520demand%2520%2528%2524%255Cmu%253D1.8%2524%252C%2520%2524SD%2520%253D%25200.8%2524%2529%2520and%2520good%2520performance%250A%2528%2524%255Cmu%253D3.4%2524%252C%2520%2524SD%2520%253D%25200.8%2524%2529%252C%2520proving%2520better%2520user%2520assessments%2520in%2520comparison%2520with%250Abaseline%2520FPV%2520goggles.%2520Participants%2520also%2520rated%2520the%2520system%2520highly%2520for%2520stimulation%250A%2528%2524%255Cmu%253D2.35%2524%252C%2520%2524SD%2520%253D%25200.9%2524%2529%252C%2520novelty%2520%2528%2524%255Cmu%253D2.1%2524%252C%2520%2524SD%2520%253D%25200.9%2524%2529%2520and%2520attractiveness%250A%2528%2524%255Cmu%253D1.97%2524%252C%2520%2524SD%2520%253D%25201%2524%2529%252C%2520indicating%2520positive%2520user%2520experiences.%2520These%2520results%250Ademonstrate%2520the%2520potential%2520of%2520the%2520system%2520to%2520improve%2520UAV%2520piloting%2520experience%250Athrough%2520enhanced%2520situational%2520awareness%2520and%2520intuitive%2520control.%2520The%2520code%2520is%250Aavailable%2520here%253A%2520https%253A//github.com/Sautenich/FlightAR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlightAR%3A%20AR%20Flight%20Assistance%20Interface%20with%20Multiple%20Video%20Streams%20and%0A%20%20Object%20Detection%20Aimed%20at%20Immersive%20Drone%20Control&entry.906535625=Oleg%20Sautenkov%20and%20Selamawit%20Asfaw%20and%20Yasheerah%20Yaqoot%20and%20Muhammad%20Ahsan%20Mustafa%20and%20Aleksey%20Fedoseev%20and%20Daria%20Trinitatova%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20The%20swift%20advancement%20of%20unmanned%20aerial%20vehicle%20%28UAV%29%20technologies%0Anecessitates%20new%20standards%20for%20developing%20human-drone%20interaction%20%28HDI%29%0Ainterfaces.%20Most%20interfaces%20for%20HDI%2C%20especially%20first-person%20view%20%28FPV%29%0Agoggles%2C%20limit%20the%20operator%27s%20ability%20to%20obtain%20information%20from%20the%0Aenvironment.%20This%20paper%20presents%20a%20novel%20interface%2C%20FlightAR%2C%20that%20integrates%0Aaugmented%20reality%20%28AR%29%20overlays%20of%20UAV%20first-person%20view%20%28FPV%29%20and%20bottom%0Acamera%20feeds%20with%20head-mounted%20display%20%28HMD%29%20to%20enhance%20the%20pilot%27s%20situational%0Aawareness.%20Using%20FlightAR%2C%20the%20system%20provides%20pilots%20not%20only%20with%20a%20video%0Astream%20from%20several%20UAV%20cameras%20simultaneously%2C%20but%20also%20the%20ability%20to%20observe%0Atheir%20surroundings%20in%20real%20time.%20User%20evaluation%20with%20NASA-TLX%20and%20UEQ%20surveys%0Ashowed%20low%20physical%20demand%20%28%24%5Cmu%3D1.8%24%2C%20%24SD%20%3D%200.8%24%29%20and%20good%20performance%0A%28%24%5Cmu%3D3.4%24%2C%20%24SD%20%3D%200.8%24%29%2C%20proving%20better%20user%20assessments%20in%20comparison%20with%0Abaseline%20FPV%20goggles.%20Participants%20also%20rated%20the%20system%20highly%20for%20stimulation%0A%28%24%5Cmu%3D2.35%24%2C%20%24SD%20%3D%200.9%24%29%2C%20novelty%20%28%24%5Cmu%3D2.1%24%2C%20%24SD%20%3D%200.9%24%29%20and%20attractiveness%0A%28%24%5Cmu%3D1.97%24%2C%20%24SD%20%3D%201%24%29%2C%20indicating%20positive%20user%20experiences.%20These%20results%0Ademonstrate%20the%20potential%20of%20the%20system%20to%20improve%20UAV%20piloting%20experience%0Athrough%20enhanced%20situational%20awareness%20and%20intuitive%20control.%20The%20code%20is%0Aavailable%20here%3A%20https%3A//github.com/Sautenich/FlightAR%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16943v1&entry.124074799=Read"},
{"title": "Do LLMs estimate uncertainty well in instruction-following?", "author": "Juyeon Heo and Miao Xiong and Christina Heinze-Deml and Jaya Narain", "abstract": "  Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.\n", "link": "http://arxiv.org/abs/2410.14582v2", "date": "2024-10-22", "relevancy": 2.0834, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6023}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.505}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20estimate%20uncertainty%20well%20in%20instruction-following%3F&body=Title%3A%20Do%20LLMs%20estimate%20uncertainty%20well%20in%20instruction-following%3F%0AAuthor%3A%20Juyeon%20Heo%20and%20Miao%20Xiong%20and%20Christina%20Heinze-Deml%20and%20Jaya%20Narain%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20could%20be%20valuable%20personal%20AI%20agents%20across%0Avarious%20domains%2C%20provided%20they%20can%20precisely%20follow%20user%20instructions.%20However%2C%0Arecent%20studies%20have%20shown%20significant%20limitations%20in%20LLMs%27%0Ainstruction-following%20capabilities%2C%20raising%20concerns%20about%20their%20reliability%20in%0Ahigh-stakes%20applications.%20Accurately%20estimating%20LLMs%27%20uncertainty%20in%20adhering%0Ato%20instructions%20is%20critical%20to%20mitigating%20deployment%20risks.%20We%20present%2C%20to%20our%0Aknowledge%2C%20the%20first%20systematic%20evaluation%20of%20the%20uncertainty%20estimation%0Aabilities%20of%20LLMs%20in%20the%20context%20of%20instruction-following.%20Our%20study%20identifies%0Akey%20challenges%20with%20existing%20instruction-following%20benchmarks%2C%20where%20multiple%0Afactors%20are%20entangled%20with%20uncertainty%20stems%20from%20instruction-following%2C%0Acomplicating%20the%20isolation%20and%20comparison%20across%20methods%20and%20models.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20controlled%20evaluation%20setup%20with%20two%20benchmark%0Aversions%20of%20data%2C%20enabling%20a%20comprehensive%20comparison%20of%20uncertainty%20estimation%0Amethods%20under%20various%20conditions.%20Our%20findings%20show%20that%20existing%20uncertainty%0Amethods%20struggle%2C%20particularly%20when%20models%20make%20subtle%20errors%20in%20instruction%0Afollowing.%20While%20internal%20model%20states%20provide%20some%20improvement%2C%20they%20remain%0Ainadequate%20in%20more%20complex%20scenarios.%20The%20insights%20from%20our%20controlled%0Aevaluation%20setups%20provide%20a%20crucial%20understanding%20of%20LLMs%27%20limitations%20and%0Apotential%20for%20uncertainty%20estimation%20in%20instruction-following%20tasks%2C%20paving%20the%0Away%20for%20more%20trustworthy%20AI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520estimate%2520uncertainty%2520well%2520in%2520instruction-following%253F%26entry.906535625%3DJuyeon%2520Heo%2520and%2520Miao%2520Xiong%2520and%2520Christina%2520Heinze-Deml%2520and%2520Jaya%2520Narain%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520could%2520be%2520valuable%2520personal%2520AI%2520agents%2520across%250Avarious%2520domains%252C%2520provided%2520they%2520can%2520precisely%2520follow%2520user%2520instructions.%2520However%252C%250Arecent%2520studies%2520have%2520shown%2520significant%2520limitations%2520in%2520LLMs%2527%250Ainstruction-following%2520capabilities%252C%2520raising%2520concerns%2520about%2520their%2520reliability%2520in%250Ahigh-stakes%2520applications.%2520Accurately%2520estimating%2520LLMs%2527%2520uncertainty%2520in%2520adhering%250Ato%2520instructions%2520is%2520critical%2520to%2520mitigating%2520deployment%2520risks.%2520We%2520present%252C%2520to%2520our%250Aknowledge%252C%2520the%2520first%2520systematic%2520evaluation%2520of%2520the%2520uncertainty%2520estimation%250Aabilities%2520of%2520LLMs%2520in%2520the%2520context%2520of%2520instruction-following.%2520Our%2520study%2520identifies%250Akey%2520challenges%2520with%2520existing%2520instruction-following%2520benchmarks%252C%2520where%2520multiple%250Afactors%2520are%2520entangled%2520with%2520uncertainty%2520stems%2520from%2520instruction-following%252C%250Acomplicating%2520the%2520isolation%2520and%2520comparison%2520across%2520methods%2520and%2520models.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520a%2520controlled%2520evaluation%2520setup%2520with%2520two%2520benchmark%250Aversions%2520of%2520data%252C%2520enabling%2520a%2520comprehensive%2520comparison%2520of%2520uncertainty%2520estimation%250Amethods%2520under%2520various%2520conditions.%2520Our%2520findings%2520show%2520that%2520existing%2520uncertainty%250Amethods%2520struggle%252C%2520particularly%2520when%2520models%2520make%2520subtle%2520errors%2520in%2520instruction%250Afollowing.%2520While%2520internal%2520model%2520states%2520provide%2520some%2520improvement%252C%2520they%2520remain%250Ainadequate%2520in%2520more%2520complex%2520scenarios.%2520The%2520insights%2520from%2520our%2520controlled%250Aevaluation%2520setups%2520provide%2520a%2520crucial%2520understanding%2520of%2520LLMs%2527%2520limitations%2520and%250Apotential%2520for%2520uncertainty%2520estimation%2520in%2520instruction-following%2520tasks%252C%2520paving%2520the%250Away%2520for%2520more%2520trustworthy%2520AI%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20estimate%20uncertainty%20well%20in%20instruction-following%3F&entry.906535625=Juyeon%20Heo%20and%20Miao%20Xiong%20and%20Christina%20Heinze-Deml%20and%20Jaya%20Narain&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20could%20be%20valuable%20personal%20AI%20agents%20across%0Avarious%20domains%2C%20provided%20they%20can%20precisely%20follow%20user%20instructions.%20However%2C%0Arecent%20studies%20have%20shown%20significant%20limitations%20in%20LLMs%27%0Ainstruction-following%20capabilities%2C%20raising%20concerns%20about%20their%20reliability%20in%0Ahigh-stakes%20applications.%20Accurately%20estimating%20LLMs%27%20uncertainty%20in%20adhering%0Ato%20instructions%20is%20critical%20to%20mitigating%20deployment%20risks.%20We%20present%2C%20to%20our%0Aknowledge%2C%20the%20first%20systematic%20evaluation%20of%20the%20uncertainty%20estimation%0Aabilities%20of%20LLMs%20in%20the%20context%20of%20instruction-following.%20Our%20study%20identifies%0Akey%20challenges%20with%20existing%20instruction-following%20benchmarks%2C%20where%20multiple%0Afactors%20are%20entangled%20with%20uncertainty%20stems%20from%20instruction-following%2C%0Acomplicating%20the%20isolation%20and%20comparison%20across%20methods%20and%20models.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20controlled%20evaluation%20setup%20with%20two%20benchmark%0Aversions%20of%20data%2C%20enabling%20a%20comprehensive%20comparison%20of%20uncertainty%20estimation%0Amethods%20under%20various%20conditions.%20Our%20findings%20show%20that%20existing%20uncertainty%0Amethods%20struggle%2C%20particularly%20when%20models%20make%20subtle%20errors%20in%20instruction%0Afollowing.%20While%20internal%20model%20states%20provide%20some%20improvement%2C%20they%20remain%0Ainadequate%20in%20more%20complex%20scenarios.%20The%20insights%20from%20our%20controlled%0Aevaluation%20setups%20provide%20a%20crucial%20understanding%20of%20LLMs%27%20limitations%20and%0Apotential%20for%20uncertainty%20estimation%20in%20instruction-following%20tasks%2C%20paving%20the%0Away%20for%20more%20trustworthy%20AI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14582v2&entry.124074799=Read"},
{"title": "Unsupervised Time Series Anomaly Prediction with Importance-based\n  Generative Contrastive Learning", "author": "Kai Zhao and Zhihao Zhuang and Chenjuan Guo and Hao Miao and Yunyao Cheng and Bin Yang", "abstract": "  Time series anomaly prediction plays an essential role in many real-world\nscenarios, such as environmental prevention and prompt maintenance of\ncyber-physical systems. However, existing time series anomaly prediction\nmethods mainly require supervised training with plenty of manually labeled\ndata, which are difficult to obtain in practice. Besides, unseen anomalies can\noccur during inference, which could differ from the labeled training data and\nmake these models fail to predict such new anomalies. In this paper, we study a\nnovel problem of unsupervised time series anomaly prediction. We provide a\ntheoretical analysis and propose Importance-based Generative Contrastive\nLearning (IGCL) to address the aforementioned problems. IGCL distinguishes\nbetween normal and anomaly precursors, which are generated by our anomaly\nprecursor pattern generation module. To address the efficiency issues caused by\nthe potential complex anomaly precursor combinations, we propose a memory bank\nwith importance-based scores to adaptively store representative anomaly\nprecursors and generate more complicated anomaly precursors. Extensive\nexperiments on seven benchmark datasets show our method outperforms\nstate-of-the-art baselines on unsupervised time series anomaly prediction\nproblems.\n", "link": "http://arxiv.org/abs/2410.16888v1", "date": "2024-10-22", "relevancy": 2.078, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5236}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5191}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Time%20Series%20Anomaly%20Prediction%20with%20Importance-based%0A%20%20Generative%20Contrastive%20Learning&body=Title%3A%20Unsupervised%20Time%20Series%20Anomaly%20Prediction%20with%20Importance-based%0A%20%20Generative%20Contrastive%20Learning%0AAuthor%3A%20Kai%20Zhao%20and%20Zhihao%20Zhuang%20and%20Chenjuan%20Guo%20and%20Hao%20Miao%20and%20Yunyao%20Cheng%20and%20Bin%20Yang%0AAbstract%3A%20%20%20Time%20series%20anomaly%20prediction%20plays%20an%20essential%20role%20in%20many%20real-world%0Ascenarios%2C%20such%20as%20environmental%20prevention%20and%20prompt%20maintenance%20of%0Acyber-physical%20systems.%20However%2C%20existing%20time%20series%20anomaly%20prediction%0Amethods%20mainly%20require%20supervised%20training%20with%20plenty%20of%20manually%20labeled%0Adata%2C%20which%20are%20difficult%20to%20obtain%20in%20practice.%20Besides%2C%20unseen%20anomalies%20can%0Aoccur%20during%20inference%2C%20which%20could%20differ%20from%20the%20labeled%20training%20data%20and%0Amake%20these%20models%20fail%20to%20predict%20such%20new%20anomalies.%20In%20this%20paper%2C%20we%20study%20a%0Anovel%20problem%20of%20unsupervised%20time%20series%20anomaly%20prediction.%20We%20provide%20a%0Atheoretical%20analysis%20and%20propose%20Importance-based%20Generative%20Contrastive%0ALearning%20%28IGCL%29%20to%20address%20the%20aforementioned%20problems.%20IGCL%20distinguishes%0Abetween%20normal%20and%20anomaly%20precursors%2C%20which%20are%20generated%20by%20our%20anomaly%0Aprecursor%20pattern%20generation%20module.%20To%20address%20the%20efficiency%20issues%20caused%20by%0Athe%20potential%20complex%20anomaly%20precursor%20combinations%2C%20we%20propose%20a%20memory%20bank%0Awith%20importance-based%20scores%20to%20adaptively%20store%20representative%20anomaly%0Aprecursors%20and%20generate%20more%20complicated%20anomaly%20precursors.%20Extensive%0Aexperiments%20on%20seven%20benchmark%20datasets%20show%20our%20method%20outperforms%0Astate-of-the-art%20baselines%20on%20unsupervised%20time%20series%20anomaly%20prediction%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Time%2520Series%2520Anomaly%2520Prediction%2520with%2520Importance-based%250A%2520%2520Generative%2520Contrastive%2520Learning%26entry.906535625%3DKai%2520Zhao%2520and%2520Zhihao%2520Zhuang%2520and%2520Chenjuan%2520Guo%2520and%2520Hao%2520Miao%2520and%2520Yunyao%2520Cheng%2520and%2520Bin%2520Yang%26entry.1292438233%3D%2520%2520Time%2520series%2520anomaly%2520prediction%2520plays%2520an%2520essential%2520role%2520in%2520many%2520real-world%250Ascenarios%252C%2520such%2520as%2520environmental%2520prevention%2520and%2520prompt%2520maintenance%2520of%250Acyber-physical%2520systems.%2520However%252C%2520existing%2520time%2520series%2520anomaly%2520prediction%250Amethods%2520mainly%2520require%2520supervised%2520training%2520with%2520plenty%2520of%2520manually%2520labeled%250Adata%252C%2520which%2520are%2520difficult%2520to%2520obtain%2520in%2520practice.%2520Besides%252C%2520unseen%2520anomalies%2520can%250Aoccur%2520during%2520inference%252C%2520which%2520could%2520differ%2520from%2520the%2520labeled%2520training%2520data%2520and%250Amake%2520these%2520models%2520fail%2520to%2520predict%2520such%2520new%2520anomalies.%2520In%2520this%2520paper%252C%2520we%2520study%2520a%250Anovel%2520problem%2520of%2520unsupervised%2520time%2520series%2520anomaly%2520prediction.%2520We%2520provide%2520a%250Atheoretical%2520analysis%2520and%2520propose%2520Importance-based%2520Generative%2520Contrastive%250ALearning%2520%2528IGCL%2529%2520to%2520address%2520the%2520aforementioned%2520problems.%2520IGCL%2520distinguishes%250Abetween%2520normal%2520and%2520anomaly%2520precursors%252C%2520which%2520are%2520generated%2520by%2520our%2520anomaly%250Aprecursor%2520pattern%2520generation%2520module.%2520To%2520address%2520the%2520efficiency%2520issues%2520caused%2520by%250Athe%2520potential%2520complex%2520anomaly%2520precursor%2520combinations%252C%2520we%2520propose%2520a%2520memory%2520bank%250Awith%2520importance-based%2520scores%2520to%2520adaptively%2520store%2520representative%2520anomaly%250Aprecursors%2520and%2520generate%2520more%2520complicated%2520anomaly%2520precursors.%2520Extensive%250Aexperiments%2520on%2520seven%2520benchmark%2520datasets%2520show%2520our%2520method%2520outperforms%250Astate-of-the-art%2520baselines%2520on%2520unsupervised%2520time%2520series%2520anomaly%2520prediction%250Aproblems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Time%20Series%20Anomaly%20Prediction%20with%20Importance-based%0A%20%20Generative%20Contrastive%20Learning&entry.906535625=Kai%20Zhao%20and%20Zhihao%20Zhuang%20and%20Chenjuan%20Guo%20and%20Hao%20Miao%20and%20Yunyao%20Cheng%20and%20Bin%20Yang&entry.1292438233=%20%20Time%20series%20anomaly%20prediction%20plays%20an%20essential%20role%20in%20many%20real-world%0Ascenarios%2C%20such%20as%20environmental%20prevention%20and%20prompt%20maintenance%20of%0Acyber-physical%20systems.%20However%2C%20existing%20time%20series%20anomaly%20prediction%0Amethods%20mainly%20require%20supervised%20training%20with%20plenty%20of%20manually%20labeled%0Adata%2C%20which%20are%20difficult%20to%20obtain%20in%20practice.%20Besides%2C%20unseen%20anomalies%20can%0Aoccur%20during%20inference%2C%20which%20could%20differ%20from%20the%20labeled%20training%20data%20and%0Amake%20these%20models%20fail%20to%20predict%20such%20new%20anomalies.%20In%20this%20paper%2C%20we%20study%20a%0Anovel%20problem%20of%20unsupervised%20time%20series%20anomaly%20prediction.%20We%20provide%20a%0Atheoretical%20analysis%20and%20propose%20Importance-based%20Generative%20Contrastive%0ALearning%20%28IGCL%29%20to%20address%20the%20aforementioned%20problems.%20IGCL%20distinguishes%0Abetween%20normal%20and%20anomaly%20precursors%2C%20which%20are%20generated%20by%20our%20anomaly%0Aprecursor%20pattern%20generation%20module.%20To%20address%20the%20efficiency%20issues%20caused%20by%0Athe%20potential%20complex%20anomaly%20precursor%20combinations%2C%20we%20propose%20a%20memory%20bank%0Awith%20importance-based%20scores%20to%20adaptively%20store%20representative%20anomaly%0Aprecursors%20and%20generate%20more%20complicated%20anomaly%20precursors.%20Extensive%0Aexperiments%20on%20seven%20benchmark%20datasets%20show%20our%20method%20outperforms%0Astate-of-the-art%20baselines%20on%20unsupervised%20time%20series%20anomaly%20prediction%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16888v1&entry.124074799=Read"},
{"title": "Multi Kernel Estimation based Object Segmentation", "author": "Haim Goldfisher and Asaf Yekutiel", "abstract": "  This paper presents a novel approach for multi-kernel estimation by enhancing\nthe KernelGAN algorithm, which traditionally estimates a single kernel for the\nentire image. We introduce Multi-KernelGAN, which extends KernelGAN's\ncapabilities by estimating two distinct kernels based on object segmentation\nmasks. Our approach is validated through three distinct methods: texture-based\npatch Fast Fourier Transform (FFT) calculation, detail-based segmentation, and\ndeep learning-based object segmentation using YOLOv8 and the Segment Anything\nModel (SAM). Among these methods, the combination of YOLO and SAM yields the\nbest results for kernel estimation. Experimental results demonstrate that our\nmulti-kernel estimation technique outperforms conventional single-kernel\nmethods in super-resolution tasks.\n", "link": "http://arxiv.org/abs/2410.17064v1", "date": "2024-10-22", "relevancy": 2.0586, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5195}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.516}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi%20Kernel%20Estimation%20based%20Object%20Segmentation&body=Title%3A%20Multi%20Kernel%20Estimation%20based%20Object%20Segmentation%0AAuthor%3A%20Haim%20Goldfisher%20and%20Asaf%20Yekutiel%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20multi-kernel%20estimation%20by%20enhancing%0Athe%20KernelGAN%20algorithm%2C%20which%20traditionally%20estimates%20a%20single%20kernel%20for%20the%0Aentire%20image.%20We%20introduce%20Multi-KernelGAN%2C%20which%20extends%20KernelGAN%27s%0Acapabilities%20by%20estimating%20two%20distinct%20kernels%20based%20on%20object%20segmentation%0Amasks.%20Our%20approach%20is%20validated%20through%20three%20distinct%20methods%3A%20texture-based%0Apatch%20Fast%20Fourier%20Transform%20%28FFT%29%20calculation%2C%20detail-based%20segmentation%2C%20and%0Adeep%20learning-based%20object%20segmentation%20using%20YOLOv8%20and%20the%20Segment%20Anything%0AModel%20%28SAM%29.%20Among%20these%20methods%2C%20the%20combination%20of%20YOLO%20and%20SAM%20yields%20the%0Abest%20results%20for%20kernel%20estimation.%20Experimental%20results%20demonstrate%20that%20our%0Amulti-kernel%20estimation%20technique%20outperforms%20conventional%20single-kernel%0Amethods%20in%20super-resolution%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti%2520Kernel%2520Estimation%2520based%2520Object%2520Segmentation%26entry.906535625%3DHaim%2520Goldfisher%2520and%2520Asaf%2520Yekutiel%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520multi-kernel%2520estimation%2520by%2520enhancing%250Athe%2520KernelGAN%2520algorithm%252C%2520which%2520traditionally%2520estimates%2520a%2520single%2520kernel%2520for%2520the%250Aentire%2520image.%2520We%2520introduce%2520Multi-KernelGAN%252C%2520which%2520extends%2520KernelGAN%2527s%250Acapabilities%2520by%2520estimating%2520two%2520distinct%2520kernels%2520based%2520on%2520object%2520segmentation%250Amasks.%2520Our%2520approach%2520is%2520validated%2520through%2520three%2520distinct%2520methods%253A%2520texture-based%250Apatch%2520Fast%2520Fourier%2520Transform%2520%2528FFT%2529%2520calculation%252C%2520detail-based%2520segmentation%252C%2520and%250Adeep%2520learning-based%2520object%2520segmentation%2520using%2520YOLOv8%2520and%2520the%2520Segment%2520Anything%250AModel%2520%2528SAM%2529.%2520Among%2520these%2520methods%252C%2520the%2520combination%2520of%2520YOLO%2520and%2520SAM%2520yields%2520the%250Abest%2520results%2520for%2520kernel%2520estimation.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amulti-kernel%2520estimation%2520technique%2520outperforms%2520conventional%2520single-kernel%250Amethods%2520in%2520super-resolution%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi%20Kernel%20Estimation%20based%20Object%20Segmentation&entry.906535625=Haim%20Goldfisher%20and%20Asaf%20Yekutiel&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20multi-kernel%20estimation%20by%20enhancing%0Athe%20KernelGAN%20algorithm%2C%20which%20traditionally%20estimates%20a%20single%20kernel%20for%20the%0Aentire%20image.%20We%20introduce%20Multi-KernelGAN%2C%20which%20extends%20KernelGAN%27s%0Acapabilities%20by%20estimating%20two%20distinct%20kernels%20based%20on%20object%20segmentation%0Amasks.%20Our%20approach%20is%20validated%20through%20three%20distinct%20methods%3A%20texture-based%0Apatch%20Fast%20Fourier%20Transform%20%28FFT%29%20calculation%2C%20detail-based%20segmentation%2C%20and%0Adeep%20learning-based%20object%20segmentation%20using%20YOLOv8%20and%20the%20Segment%20Anything%0AModel%20%28SAM%29.%20Among%20these%20methods%2C%20the%20combination%20of%20YOLO%20and%20SAM%20yields%20the%0Abest%20results%20for%20kernel%20estimation.%20Experimental%20results%20demonstrate%20that%20our%0Amulti-kernel%20estimation%20technique%20outperforms%20conventional%20single-kernel%0Amethods%20in%20super-resolution%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17064v1&entry.124074799=Read"},
{"title": "Leaky ReLUs That Differ in Forward and Backward Pass Facilitate\n  Activation Maximization in Deep Neural Networks", "author": "Christoph Linse and Erhardt Barth and Thomas Martinetz", "abstract": "  Activation maximization (AM) strives to generate optimal input stimuli,\nrevealing features that trigger high responses in trained deep neural networks.\nAM is an important method of explainable AI. We demonstrate that AM fails to\nproduce optimal input stimuli for simple functions containing ReLUs or Leaky\nReLUs, casting doubt on the practical usefulness of AM and the visual\ninterpretation of the generated images. This paper proposes a solution based on\nusing Leaky ReLUs with a high negative slope in the backward pass while keeping\nthe original, usually zero, slope in the forward pass. The approach\nsignificantly increases the maxima found by AM. The resulting ProxyGrad\nalgorithm implements a novel optimization technique for neural networks that\nemploys a secondary network as a proxy for gradient computation. This proxy\nnetwork is designed to have a simpler loss landscape with fewer local maxima\nthan the original network. Our chosen proxy network is an identical copy of the\noriginal network, including its weights, with distinct negative slopes in the\nLeaky ReLUs. Moreover, we show that ProxyGrad can be used to train the weights\nof Convolutional Neural Networks for classification such that, on some of the\ntested benchmarks, they outperform traditional networks.\n", "link": "http://arxiv.org/abs/2410.16958v1", "date": "2024-10-22", "relevancy": 2.0557, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5368}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5103}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leaky%20ReLUs%20That%20Differ%20in%20Forward%20and%20Backward%20Pass%20Facilitate%0A%20%20Activation%20Maximization%20in%20Deep%20Neural%20Networks&body=Title%3A%20Leaky%20ReLUs%20That%20Differ%20in%20Forward%20and%20Backward%20Pass%20Facilitate%0A%20%20Activation%20Maximization%20in%20Deep%20Neural%20Networks%0AAuthor%3A%20Christoph%20Linse%20and%20Erhardt%20Barth%20and%20Thomas%20Martinetz%0AAbstract%3A%20%20%20Activation%20maximization%20%28AM%29%20strives%20to%20generate%20optimal%20input%20stimuli%2C%0Arevealing%20features%20that%20trigger%20high%20responses%20in%20trained%20deep%20neural%20networks.%0AAM%20is%20an%20important%20method%20of%20explainable%20AI.%20We%20demonstrate%20that%20AM%20fails%20to%0Aproduce%20optimal%20input%20stimuli%20for%20simple%20functions%20containing%20ReLUs%20or%20Leaky%0AReLUs%2C%20casting%20doubt%20on%20the%20practical%20usefulness%20of%20AM%20and%20the%20visual%0Ainterpretation%20of%20the%20generated%20images.%20This%20paper%20proposes%20a%20solution%20based%20on%0Ausing%20Leaky%20ReLUs%20with%20a%20high%20negative%20slope%20in%20the%20backward%20pass%20while%20keeping%0Athe%20original%2C%20usually%20zero%2C%20slope%20in%20the%20forward%20pass.%20The%20approach%0Asignificantly%20increases%20the%20maxima%20found%20by%20AM.%20The%20resulting%20ProxyGrad%0Aalgorithm%20implements%20a%20novel%20optimization%20technique%20for%20neural%20networks%20that%0Aemploys%20a%20secondary%20network%20as%20a%20proxy%20for%20gradient%20computation.%20This%20proxy%0Anetwork%20is%20designed%20to%20have%20a%20simpler%20loss%20landscape%20with%20fewer%20local%20maxima%0Athan%20the%20original%20network.%20Our%20chosen%20proxy%20network%20is%20an%20identical%20copy%20of%20the%0Aoriginal%20network%2C%20including%20its%20weights%2C%20with%20distinct%20negative%20slopes%20in%20the%0ALeaky%20ReLUs.%20Moreover%2C%20we%20show%20that%20ProxyGrad%20can%20be%20used%20to%20train%20the%20weights%0Aof%20Convolutional%20Neural%20Networks%20for%20classification%20such%20that%2C%20on%20some%20of%20the%0Atested%20benchmarks%2C%20they%20outperform%20traditional%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeaky%2520ReLUs%2520That%2520Differ%2520in%2520Forward%2520and%2520Backward%2520Pass%2520Facilitate%250A%2520%2520Activation%2520Maximization%2520in%2520Deep%2520Neural%2520Networks%26entry.906535625%3DChristoph%2520Linse%2520and%2520Erhardt%2520Barth%2520and%2520Thomas%2520Martinetz%26entry.1292438233%3D%2520%2520Activation%2520maximization%2520%2528AM%2529%2520strives%2520to%2520generate%2520optimal%2520input%2520stimuli%252C%250Arevealing%2520features%2520that%2520trigger%2520high%2520responses%2520in%2520trained%2520deep%2520neural%2520networks.%250AAM%2520is%2520an%2520important%2520method%2520of%2520explainable%2520AI.%2520We%2520demonstrate%2520that%2520AM%2520fails%2520to%250Aproduce%2520optimal%2520input%2520stimuli%2520for%2520simple%2520functions%2520containing%2520ReLUs%2520or%2520Leaky%250AReLUs%252C%2520casting%2520doubt%2520on%2520the%2520practical%2520usefulness%2520of%2520AM%2520and%2520the%2520visual%250Ainterpretation%2520of%2520the%2520generated%2520images.%2520This%2520paper%2520proposes%2520a%2520solution%2520based%2520on%250Ausing%2520Leaky%2520ReLUs%2520with%2520a%2520high%2520negative%2520slope%2520in%2520the%2520backward%2520pass%2520while%2520keeping%250Athe%2520original%252C%2520usually%2520zero%252C%2520slope%2520in%2520the%2520forward%2520pass.%2520The%2520approach%250Asignificantly%2520increases%2520the%2520maxima%2520found%2520by%2520AM.%2520The%2520resulting%2520ProxyGrad%250Aalgorithm%2520implements%2520a%2520novel%2520optimization%2520technique%2520for%2520neural%2520networks%2520that%250Aemploys%2520a%2520secondary%2520network%2520as%2520a%2520proxy%2520for%2520gradient%2520computation.%2520This%2520proxy%250Anetwork%2520is%2520designed%2520to%2520have%2520a%2520simpler%2520loss%2520landscape%2520with%2520fewer%2520local%2520maxima%250Athan%2520the%2520original%2520network.%2520Our%2520chosen%2520proxy%2520network%2520is%2520an%2520identical%2520copy%2520of%2520the%250Aoriginal%2520network%252C%2520including%2520its%2520weights%252C%2520with%2520distinct%2520negative%2520slopes%2520in%2520the%250ALeaky%2520ReLUs.%2520Moreover%252C%2520we%2520show%2520that%2520ProxyGrad%2520can%2520be%2520used%2520to%2520train%2520the%2520weights%250Aof%2520Convolutional%2520Neural%2520Networks%2520for%2520classification%2520such%2520that%252C%2520on%2520some%2520of%2520the%250Atested%2520benchmarks%252C%2520they%2520outperform%2520traditional%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leaky%20ReLUs%20That%20Differ%20in%20Forward%20and%20Backward%20Pass%20Facilitate%0A%20%20Activation%20Maximization%20in%20Deep%20Neural%20Networks&entry.906535625=Christoph%20Linse%20and%20Erhardt%20Barth%20and%20Thomas%20Martinetz&entry.1292438233=%20%20Activation%20maximization%20%28AM%29%20strives%20to%20generate%20optimal%20input%20stimuli%2C%0Arevealing%20features%20that%20trigger%20high%20responses%20in%20trained%20deep%20neural%20networks.%0AAM%20is%20an%20important%20method%20of%20explainable%20AI.%20We%20demonstrate%20that%20AM%20fails%20to%0Aproduce%20optimal%20input%20stimuli%20for%20simple%20functions%20containing%20ReLUs%20or%20Leaky%0AReLUs%2C%20casting%20doubt%20on%20the%20practical%20usefulness%20of%20AM%20and%20the%20visual%0Ainterpretation%20of%20the%20generated%20images.%20This%20paper%20proposes%20a%20solution%20based%20on%0Ausing%20Leaky%20ReLUs%20with%20a%20high%20negative%20slope%20in%20the%20backward%20pass%20while%20keeping%0Athe%20original%2C%20usually%20zero%2C%20slope%20in%20the%20forward%20pass.%20The%20approach%0Asignificantly%20increases%20the%20maxima%20found%20by%20AM.%20The%20resulting%20ProxyGrad%0Aalgorithm%20implements%20a%20novel%20optimization%20technique%20for%20neural%20networks%20that%0Aemploys%20a%20secondary%20network%20as%20a%20proxy%20for%20gradient%20computation.%20This%20proxy%0Anetwork%20is%20designed%20to%20have%20a%20simpler%20loss%20landscape%20with%20fewer%20local%20maxima%0Athan%20the%20original%20network.%20Our%20chosen%20proxy%20network%20is%20an%20identical%20copy%20of%20the%0Aoriginal%20network%2C%20including%20its%20weights%2C%20with%20distinct%20negative%20slopes%20in%20the%0ALeaky%20ReLUs.%20Moreover%2C%20we%20show%20that%20ProxyGrad%20can%20be%20used%20to%20train%20the%20weights%0Aof%20Convolutional%20Neural%20Networks%20for%20classification%20such%20that%2C%20on%20some%20of%20the%0Atested%20benchmarks%2C%20they%20outperform%20traditional%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16958v1&entry.124074799=Read"},
{"title": "One Thousand and One Pairs: A \"novel\" challenge for long-context\n  language models", "author": "Marzena Karpinska and Katherine Thai and Kyle Lo and Tanya Goyal and Mohit Iyyer", "abstract": "  Synthetic long-context LLM benchmarks (e.g., \"needle-in-the-haystack\") test\nonly surface-level retrieval capabilities, but how well can long-context LLMs\nretrieve, synthesize, and reason over information across book-length inputs? We\naddress this question by creating NoCha, a dataset of 1,001 minimally different\npairs of true and false claims about 67 recently-published English fictional\nbooks, written by human readers of those books. In contrast to existing\nlong-context benchmarks, our annotators confirm that the largest share of pairs\nin NoCha require global reasoning over the entire book to verify. Our\nexperiments show that while human readers easily perform this task, it is\nenormously challenging for all ten long-context LLMs that we evaluate: no\nopen-weight model performs above random chance (despite their strong\nperformance on synthetic benchmarks), while GPT-4o achieves the highest\naccuracy at 55.8%. Further analysis reveals that (1) on average, models perform\nmuch better on pairs that require only sentence-level retrieval vs. global\nreasoning; (2) model-generated explanations for their decisions are often\ninaccurate even for correctly-labeled claims; and (3) models perform\nsubstantially worse on speculative fiction books that contain extensive\nworld-building. The methodology proposed in NoCha allows for the evolution of\nthe benchmark dataset and the easy analysis of future models.\n", "link": "http://arxiv.org/abs/2406.16264v3", "date": "2024-10-22", "relevancy": 2.0503, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Thousand%20and%20One%20Pairs%3A%20A%20%22novel%22%20challenge%20for%20long-context%0A%20%20language%20models&body=Title%3A%20One%20Thousand%20and%20One%20Pairs%3A%20A%20%22novel%22%20challenge%20for%20long-context%0A%20%20language%20models%0AAuthor%3A%20Marzena%20Karpinska%20and%20Katherine%20Thai%20and%20Kyle%20Lo%20and%20Tanya%20Goyal%20and%20Mohit%20Iyyer%0AAbstract%3A%20%20%20Synthetic%20long-context%20LLM%20benchmarks%20%28e.g.%2C%20%22needle-in-the-haystack%22%29%20test%0Aonly%20surface-level%20retrieval%20capabilities%2C%20but%20how%20well%20can%20long-context%20LLMs%0Aretrieve%2C%20synthesize%2C%20and%20reason%20over%20information%20across%20book-length%20inputs%3F%20We%0Aaddress%20this%20question%20by%20creating%20NoCha%2C%20a%20dataset%20of%201%2C001%20minimally%20different%0Apairs%20of%20true%20and%20false%20claims%20about%2067%20recently-published%20English%20fictional%0Abooks%2C%20written%20by%20human%20readers%20of%20those%20books.%20In%20contrast%20to%20existing%0Along-context%20benchmarks%2C%20our%20annotators%20confirm%20that%20the%20largest%20share%20of%20pairs%0Ain%20NoCha%20require%20global%20reasoning%20over%20the%20entire%20book%20to%20verify.%20Our%0Aexperiments%20show%20that%20while%20human%20readers%20easily%20perform%20this%20task%2C%20it%20is%0Aenormously%20challenging%20for%20all%20ten%20long-context%20LLMs%20that%20we%20evaluate%3A%20no%0Aopen-weight%20model%20performs%20above%20random%20chance%20%28despite%20their%20strong%0Aperformance%20on%20synthetic%20benchmarks%29%2C%20while%20GPT-4o%20achieves%20the%20highest%0Aaccuracy%20at%2055.8%25.%20Further%20analysis%20reveals%20that%20%281%29%20on%20average%2C%20models%20perform%0Amuch%20better%20on%20pairs%20that%20require%20only%20sentence-level%20retrieval%20vs.%20global%0Areasoning%3B%20%282%29%20model-generated%20explanations%20for%20their%20decisions%20are%20often%0Ainaccurate%20even%20for%20correctly-labeled%20claims%3B%20and%20%283%29%20models%20perform%0Asubstantially%20worse%20on%20speculative%20fiction%20books%20that%20contain%20extensive%0Aworld-building.%20The%20methodology%20proposed%20in%20NoCha%20allows%20for%20the%20evolution%20of%0Athe%20benchmark%20dataset%20and%20the%20easy%20analysis%20of%20future%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16264v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Thousand%2520and%2520One%2520Pairs%253A%2520A%2520%2522novel%2522%2520challenge%2520for%2520long-context%250A%2520%2520language%2520models%26entry.906535625%3DMarzena%2520Karpinska%2520and%2520Katherine%2520Thai%2520and%2520Kyle%2520Lo%2520and%2520Tanya%2520Goyal%2520and%2520Mohit%2520Iyyer%26entry.1292438233%3D%2520%2520Synthetic%2520long-context%2520LLM%2520benchmarks%2520%2528e.g.%252C%2520%2522needle-in-the-haystack%2522%2529%2520test%250Aonly%2520surface-level%2520retrieval%2520capabilities%252C%2520but%2520how%2520well%2520can%2520long-context%2520LLMs%250Aretrieve%252C%2520synthesize%252C%2520and%2520reason%2520over%2520information%2520across%2520book-length%2520inputs%253F%2520We%250Aaddress%2520this%2520question%2520by%2520creating%2520NoCha%252C%2520a%2520dataset%2520of%25201%252C001%2520minimally%2520different%250Apairs%2520of%2520true%2520and%2520false%2520claims%2520about%252067%2520recently-published%2520English%2520fictional%250Abooks%252C%2520written%2520by%2520human%2520readers%2520of%2520those%2520books.%2520In%2520contrast%2520to%2520existing%250Along-context%2520benchmarks%252C%2520our%2520annotators%2520confirm%2520that%2520the%2520largest%2520share%2520of%2520pairs%250Ain%2520NoCha%2520require%2520global%2520reasoning%2520over%2520the%2520entire%2520book%2520to%2520verify.%2520Our%250Aexperiments%2520show%2520that%2520while%2520human%2520readers%2520easily%2520perform%2520this%2520task%252C%2520it%2520is%250Aenormously%2520challenging%2520for%2520all%2520ten%2520long-context%2520LLMs%2520that%2520we%2520evaluate%253A%2520no%250Aopen-weight%2520model%2520performs%2520above%2520random%2520chance%2520%2528despite%2520their%2520strong%250Aperformance%2520on%2520synthetic%2520benchmarks%2529%252C%2520while%2520GPT-4o%2520achieves%2520the%2520highest%250Aaccuracy%2520at%252055.8%2525.%2520Further%2520analysis%2520reveals%2520that%2520%25281%2529%2520on%2520average%252C%2520models%2520perform%250Amuch%2520better%2520on%2520pairs%2520that%2520require%2520only%2520sentence-level%2520retrieval%2520vs.%2520global%250Areasoning%253B%2520%25282%2529%2520model-generated%2520explanations%2520for%2520their%2520decisions%2520are%2520often%250Ainaccurate%2520even%2520for%2520correctly-labeled%2520claims%253B%2520and%2520%25283%2529%2520models%2520perform%250Asubstantially%2520worse%2520on%2520speculative%2520fiction%2520books%2520that%2520contain%2520extensive%250Aworld-building.%2520The%2520methodology%2520proposed%2520in%2520NoCha%2520allows%2520for%2520the%2520evolution%2520of%250Athe%2520benchmark%2520dataset%2520and%2520the%2520easy%2520analysis%2520of%2520future%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16264v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Thousand%20and%20One%20Pairs%3A%20A%20%22novel%22%20challenge%20for%20long-context%0A%20%20language%20models&entry.906535625=Marzena%20Karpinska%20and%20Katherine%20Thai%20and%20Kyle%20Lo%20and%20Tanya%20Goyal%20and%20Mohit%20Iyyer&entry.1292438233=%20%20Synthetic%20long-context%20LLM%20benchmarks%20%28e.g.%2C%20%22needle-in-the-haystack%22%29%20test%0Aonly%20surface-level%20retrieval%20capabilities%2C%20but%20how%20well%20can%20long-context%20LLMs%0Aretrieve%2C%20synthesize%2C%20and%20reason%20over%20information%20across%20book-length%20inputs%3F%20We%0Aaddress%20this%20question%20by%20creating%20NoCha%2C%20a%20dataset%20of%201%2C001%20minimally%20different%0Apairs%20of%20true%20and%20false%20claims%20about%2067%20recently-published%20English%20fictional%0Abooks%2C%20written%20by%20human%20readers%20of%20those%20books.%20In%20contrast%20to%20existing%0Along-context%20benchmarks%2C%20our%20annotators%20confirm%20that%20the%20largest%20share%20of%20pairs%0Ain%20NoCha%20require%20global%20reasoning%20over%20the%20entire%20book%20to%20verify.%20Our%0Aexperiments%20show%20that%20while%20human%20readers%20easily%20perform%20this%20task%2C%20it%20is%0Aenormously%20challenging%20for%20all%20ten%20long-context%20LLMs%20that%20we%20evaluate%3A%20no%0Aopen-weight%20model%20performs%20above%20random%20chance%20%28despite%20their%20strong%0Aperformance%20on%20synthetic%20benchmarks%29%2C%20while%20GPT-4o%20achieves%20the%20highest%0Aaccuracy%20at%2055.8%25.%20Further%20analysis%20reveals%20that%20%281%29%20on%20average%2C%20models%20perform%0Amuch%20better%20on%20pairs%20that%20require%20only%20sentence-level%20retrieval%20vs.%20global%0Areasoning%3B%20%282%29%20model-generated%20explanations%20for%20their%20decisions%20are%20often%0Ainaccurate%20even%20for%20correctly-labeled%20claims%3B%20and%20%283%29%20models%20perform%0Asubstantially%20worse%20on%20speculative%20fiction%20books%20that%20contain%20extensive%0Aworld-building.%20The%20methodology%20proposed%20in%20NoCha%20allows%20for%20the%20evolution%20of%0Athe%20benchmark%20dataset%20and%20the%20easy%20analysis%20of%20future%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16264v3&entry.124074799=Read"},
{"title": "Large Language Model-based Augmentation for Imbalanced Node\n  Classification on Text-Attributed Graphs", "author": "Leyao Wang and Yu Wang and Bo Ni and Yuying Zhao and Tyler Derr", "abstract": "  Node classification on graphs frequently encounters the challenge of class\nimbalance, leading to biased performance and posing significant risks in\nreal-world applications. Although several data-centric solutions have been\nproposed, none of them focus on Text-Attributed Graphs (TAGs), and therefore\noverlook the potential of leveraging the rich semantics encoded in textual\nfeatures for boosting the classification of minority nodes. Given this crucial\ngap, we investigate the possibility of augmenting graph data in the text space,\nleveraging the textual generation power of Large Language Models (LLMs) to\nhandle imbalanced node classification on TAGs. Specifically, we propose a novel\napproach called LA-TAG (LLM-based Augmentation on Text-Attributed Graphs),\nwhich prompts LLMs to generate synthetic texts based on existing node texts in\nthe graph. Furthermore, to integrate these synthetic text-attributed nodes into\nthe graph, we introduce a text-based link predictor to connect the synthesized\nnodes with the existing nodes. Our experiments across multiple datasets and\nevaluation metrics show that our framework significantly outperforms\ntraditional non-textual-based data augmentation strategies and specific node\nimbalance solutions. This highlights the promise of using LLMs to resolve\nimbalance issues on TAGs.\n", "link": "http://arxiv.org/abs/2410.16882v1", "date": "2024-10-22", "relevancy": 2.044, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5061}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-based%20Augmentation%20for%20Imbalanced%20Node%0A%20%20Classification%20on%20Text-Attributed%20Graphs&body=Title%3A%20Large%20Language%20Model-based%20Augmentation%20for%20Imbalanced%20Node%0A%20%20Classification%20on%20Text-Attributed%20Graphs%0AAuthor%3A%20Leyao%20Wang%20and%20Yu%20Wang%20and%20Bo%20Ni%20and%20Yuying%20Zhao%20and%20Tyler%20Derr%0AAbstract%3A%20%20%20Node%20classification%20on%20graphs%20frequently%20encounters%20the%20challenge%20of%20class%0Aimbalance%2C%20leading%20to%20biased%20performance%20and%20posing%20significant%20risks%20in%0Areal-world%20applications.%20Although%20several%20data-centric%20solutions%20have%20been%0Aproposed%2C%20none%20of%20them%20focus%20on%20Text-Attributed%20Graphs%20%28TAGs%29%2C%20and%20therefore%0Aoverlook%20the%20potential%20of%20leveraging%20the%20rich%20semantics%20encoded%20in%20textual%0Afeatures%20for%20boosting%20the%20classification%20of%20minority%20nodes.%20Given%20this%20crucial%0Agap%2C%20we%20investigate%20the%20possibility%20of%20augmenting%20graph%20data%20in%20the%20text%20space%2C%0Aleveraging%20the%20textual%20generation%20power%20of%20Large%20Language%20Models%20%28LLMs%29%20to%0Ahandle%20imbalanced%20node%20classification%20on%20TAGs.%20Specifically%2C%20we%20propose%20a%20novel%0Aapproach%20called%20LA-TAG%20%28LLM-based%20Augmentation%20on%20Text-Attributed%20Graphs%29%2C%0Awhich%20prompts%20LLMs%20to%20generate%20synthetic%20texts%20based%20on%20existing%20node%20texts%20in%0Athe%20graph.%20Furthermore%2C%20to%20integrate%20these%20synthetic%20text-attributed%20nodes%20into%0Athe%20graph%2C%20we%20introduce%20a%20text-based%20link%20predictor%20to%20connect%20the%20synthesized%0Anodes%20with%20the%20existing%20nodes.%20Our%20experiments%20across%20multiple%20datasets%20and%0Aevaluation%20metrics%20show%20that%20our%20framework%20significantly%20outperforms%0Atraditional%20non-textual-based%20data%20augmentation%20strategies%20and%20specific%20node%0Aimbalance%20solutions.%20This%20highlights%20the%20promise%20of%20using%20LLMs%20to%20resolve%0Aimbalance%20issues%20on%20TAGs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-based%2520Augmentation%2520for%2520Imbalanced%2520Node%250A%2520%2520Classification%2520on%2520Text-Attributed%2520Graphs%26entry.906535625%3DLeyao%2520Wang%2520and%2520Yu%2520Wang%2520and%2520Bo%2520Ni%2520and%2520Yuying%2520Zhao%2520and%2520Tyler%2520Derr%26entry.1292438233%3D%2520%2520Node%2520classification%2520on%2520graphs%2520frequently%2520encounters%2520the%2520challenge%2520of%2520class%250Aimbalance%252C%2520leading%2520to%2520biased%2520performance%2520and%2520posing%2520significant%2520risks%2520in%250Areal-world%2520applications.%2520Although%2520several%2520data-centric%2520solutions%2520have%2520been%250Aproposed%252C%2520none%2520of%2520them%2520focus%2520on%2520Text-Attributed%2520Graphs%2520%2528TAGs%2529%252C%2520and%2520therefore%250Aoverlook%2520the%2520potential%2520of%2520leveraging%2520the%2520rich%2520semantics%2520encoded%2520in%2520textual%250Afeatures%2520for%2520boosting%2520the%2520classification%2520of%2520minority%2520nodes.%2520Given%2520this%2520crucial%250Agap%252C%2520we%2520investigate%2520the%2520possibility%2520of%2520augmenting%2520graph%2520data%2520in%2520the%2520text%2520space%252C%250Aleveraging%2520the%2520textual%2520generation%2520power%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Ahandle%2520imbalanced%2520node%2520classification%2520on%2520TAGs.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520called%2520LA-TAG%2520%2528LLM-based%2520Augmentation%2520on%2520Text-Attributed%2520Graphs%2529%252C%250Awhich%2520prompts%2520LLMs%2520to%2520generate%2520synthetic%2520texts%2520based%2520on%2520existing%2520node%2520texts%2520in%250Athe%2520graph.%2520Furthermore%252C%2520to%2520integrate%2520these%2520synthetic%2520text-attributed%2520nodes%2520into%250Athe%2520graph%252C%2520we%2520introduce%2520a%2520text-based%2520link%2520predictor%2520to%2520connect%2520the%2520synthesized%250Anodes%2520with%2520the%2520existing%2520nodes.%2520Our%2520experiments%2520across%2520multiple%2520datasets%2520and%250Aevaluation%2520metrics%2520show%2520that%2520our%2520framework%2520significantly%2520outperforms%250Atraditional%2520non-textual-based%2520data%2520augmentation%2520strategies%2520and%2520specific%2520node%250Aimbalance%2520solutions.%2520This%2520highlights%2520the%2520promise%2520of%2520using%2520LLMs%2520to%2520resolve%250Aimbalance%2520issues%2520on%2520TAGs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-based%20Augmentation%20for%20Imbalanced%20Node%0A%20%20Classification%20on%20Text-Attributed%20Graphs&entry.906535625=Leyao%20Wang%20and%20Yu%20Wang%20and%20Bo%20Ni%20and%20Yuying%20Zhao%20and%20Tyler%20Derr&entry.1292438233=%20%20Node%20classification%20on%20graphs%20frequently%20encounters%20the%20challenge%20of%20class%0Aimbalance%2C%20leading%20to%20biased%20performance%20and%20posing%20significant%20risks%20in%0Areal-world%20applications.%20Although%20several%20data-centric%20solutions%20have%20been%0Aproposed%2C%20none%20of%20them%20focus%20on%20Text-Attributed%20Graphs%20%28TAGs%29%2C%20and%20therefore%0Aoverlook%20the%20potential%20of%20leveraging%20the%20rich%20semantics%20encoded%20in%20textual%0Afeatures%20for%20boosting%20the%20classification%20of%20minority%20nodes.%20Given%20this%20crucial%0Agap%2C%20we%20investigate%20the%20possibility%20of%20augmenting%20graph%20data%20in%20the%20text%20space%2C%0Aleveraging%20the%20textual%20generation%20power%20of%20Large%20Language%20Models%20%28LLMs%29%20to%0Ahandle%20imbalanced%20node%20classification%20on%20TAGs.%20Specifically%2C%20we%20propose%20a%20novel%0Aapproach%20called%20LA-TAG%20%28LLM-based%20Augmentation%20on%20Text-Attributed%20Graphs%29%2C%0Awhich%20prompts%20LLMs%20to%20generate%20synthetic%20texts%20based%20on%20existing%20node%20texts%20in%0Athe%20graph.%20Furthermore%2C%20to%20integrate%20these%20synthetic%20text-attributed%20nodes%20into%0Athe%20graph%2C%20we%20introduce%20a%20text-based%20link%20predictor%20to%20connect%20the%20synthesized%0Anodes%20with%20the%20existing%20nodes.%20Our%20experiments%20across%20multiple%20datasets%20and%0Aevaluation%20metrics%20show%20that%20our%20framework%20significantly%20outperforms%0Atraditional%20non-textual-based%20data%20augmentation%20strategies%20and%20specific%20node%0Aimbalance%20solutions.%20This%20highlights%20the%20promise%20of%20using%20LLMs%20to%20resolve%0Aimbalance%20issues%20on%20TAGs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16882v1&entry.124074799=Read"},
{"title": "Scalable spectral representations for network multiagent control", "author": "Zhaolin Ren and  Runyu and  Zhang and Bo Dai and Na Li", "abstract": "  Network Markov Decision Processes (MDPs), a popular model for multi-agent\ncontrol, pose a significant challenge to efficient learning due to the\nexponential growth of the global state-action space with the number of agents.\nIn this work, utilizing the exponential decay property of network dynamics, we\nfirst derive scalable spectral local representations for network MDPs, which\ninduces a network linear subspace for the local $Q$-function of each agent.\nBuilding on these local spectral representations, we design a scalable\nalgorithmic framework for continuous state-action network MDPs, and provide\nend-to-end guarantees for the convergence of our algorithm. Empirically, we\nvalidate the effectiveness of our scalable representation-based approach on two\nbenchmark problems, and demonstrate the advantages of our approach over generic\nfunction approximation approaches to representing the local $Q$-functions.\n", "link": "http://arxiv.org/abs/2410.17221v1", "date": "2024-10-22", "relevancy": 2.0403, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5326}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5297}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20spectral%20representations%20for%20network%20multiagent%20control&body=Title%3A%20Scalable%20spectral%20representations%20for%20network%20multiagent%20control%0AAuthor%3A%20Zhaolin%20Ren%20and%20%20Runyu%20and%20%20Zhang%20and%20Bo%20Dai%20and%20Na%20Li%0AAbstract%3A%20%20%20Network%20Markov%20Decision%20Processes%20%28MDPs%29%2C%20a%20popular%20model%20for%20multi-agent%0Acontrol%2C%20pose%20a%20significant%20challenge%20to%20efficient%20learning%20due%20to%20the%0Aexponential%20growth%20of%20the%20global%20state-action%20space%20with%20the%20number%20of%20agents.%0AIn%20this%20work%2C%20utilizing%20the%20exponential%20decay%20property%20of%20network%20dynamics%2C%20we%0Afirst%20derive%20scalable%20spectral%20local%20representations%20for%20network%20MDPs%2C%20which%0Ainduces%20a%20network%20linear%20subspace%20for%20the%20local%20%24Q%24-function%20of%20each%20agent.%0ABuilding%20on%20these%20local%20spectral%20representations%2C%20we%20design%20a%20scalable%0Aalgorithmic%20framework%20for%20continuous%20state-action%20network%20MDPs%2C%20and%20provide%0Aend-to-end%20guarantees%20for%20the%20convergence%20of%20our%20algorithm.%20Empirically%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20scalable%20representation-based%20approach%20on%20two%0Abenchmark%20problems%2C%20and%20demonstrate%20the%20advantages%20of%20our%20approach%20over%20generic%0Afunction%20approximation%20approaches%20to%20representing%20the%20local%20%24Q%24-functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520spectral%2520representations%2520for%2520network%2520multiagent%2520control%26entry.906535625%3DZhaolin%2520Ren%2520and%2520%2520Runyu%2520and%2520%2520Zhang%2520and%2520Bo%2520Dai%2520and%2520Na%2520Li%26entry.1292438233%3D%2520%2520Network%2520Markov%2520Decision%2520Processes%2520%2528MDPs%2529%252C%2520a%2520popular%2520model%2520for%2520multi-agent%250Acontrol%252C%2520pose%2520a%2520significant%2520challenge%2520to%2520efficient%2520learning%2520due%2520to%2520the%250Aexponential%2520growth%2520of%2520the%2520global%2520state-action%2520space%2520with%2520the%2520number%2520of%2520agents.%250AIn%2520this%2520work%252C%2520utilizing%2520the%2520exponential%2520decay%2520property%2520of%2520network%2520dynamics%252C%2520we%250Afirst%2520derive%2520scalable%2520spectral%2520local%2520representations%2520for%2520network%2520MDPs%252C%2520which%250Ainduces%2520a%2520network%2520linear%2520subspace%2520for%2520the%2520local%2520%2524Q%2524-function%2520of%2520each%2520agent.%250ABuilding%2520on%2520these%2520local%2520spectral%2520representations%252C%2520we%2520design%2520a%2520scalable%250Aalgorithmic%2520framework%2520for%2520continuous%2520state-action%2520network%2520MDPs%252C%2520and%2520provide%250Aend-to-end%2520guarantees%2520for%2520the%2520convergence%2520of%2520our%2520algorithm.%2520Empirically%252C%2520we%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520scalable%2520representation-based%2520approach%2520on%2520two%250Abenchmark%2520problems%252C%2520and%2520demonstrate%2520the%2520advantages%2520of%2520our%2520approach%2520over%2520generic%250Afunction%2520approximation%2520approaches%2520to%2520representing%2520the%2520local%2520%2524Q%2524-functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20spectral%20representations%20for%20network%20multiagent%20control&entry.906535625=Zhaolin%20Ren%20and%20%20Runyu%20and%20%20Zhang%20and%20Bo%20Dai%20and%20Na%20Li&entry.1292438233=%20%20Network%20Markov%20Decision%20Processes%20%28MDPs%29%2C%20a%20popular%20model%20for%20multi-agent%0Acontrol%2C%20pose%20a%20significant%20challenge%20to%20efficient%20learning%20due%20to%20the%0Aexponential%20growth%20of%20the%20global%20state-action%20space%20with%20the%20number%20of%20agents.%0AIn%20this%20work%2C%20utilizing%20the%20exponential%20decay%20property%20of%20network%20dynamics%2C%20we%0Afirst%20derive%20scalable%20spectral%20local%20representations%20for%20network%20MDPs%2C%20which%0Ainduces%20a%20network%20linear%20subspace%20for%20the%20local%20%24Q%24-function%20of%20each%20agent.%0ABuilding%20on%20these%20local%20spectral%20representations%2C%20we%20design%20a%20scalable%0Aalgorithmic%20framework%20for%20continuous%20state-action%20network%20MDPs%2C%20and%20provide%0Aend-to-end%20guarantees%20for%20the%20convergence%20of%20our%20algorithm.%20Empirically%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20scalable%20representation-based%20approach%20on%20two%0Abenchmark%20problems%2C%20and%20demonstrate%20the%20advantages%20of%20our%20approach%20over%20generic%0Afunction%20approximation%20approaches%20to%20representing%20the%20local%20%24Q%24-functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17221v1&entry.124074799=Read"},
{"title": "LLM Gesticulator: Leveraging Large Language Models for Scalable and\n  Controllable Co-Speech Gesture Synthesis", "author": "Haozhou Pang and Tianwei Ding and Lanshan He and Ming Tao and Lu Zhang and Qi Gan", "abstract": "  In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.\n", "link": "http://arxiv.org/abs/2410.10851v2", "date": "2024-10-22", "relevancy": 2.0383, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5288}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5261}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Gesticulator%3A%20Leveraging%20Large%20Language%20Models%20for%20Scalable%20and%0A%20%20Controllable%20Co-Speech%20Gesture%20Synthesis&body=Title%3A%20LLM%20Gesticulator%3A%20Leveraging%20Large%20Language%20Models%20for%20Scalable%20and%0A%20%20Controllable%20Co-Speech%20Gesture%20Synthesis%0AAuthor%3A%20Haozhou%20Pang%20and%20Tianwei%20Ding%20and%20Lanshan%20He%20and%20Ming%20Tao%20and%20Lu%20Zhang%20and%20Qi%20Gan%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20LLM%20Gesticulator%2C%20an%20LLM-based%20audio-driven%0Aco-speech%20gesture%20generation%20framework%20that%20synthesizes%20full-body%20animations%0Athat%20are%20rhythmically%20aligned%20with%20the%20input%20audio%20while%20exhibiting%20natural%0Amovements%20and%20editability.%20Compared%20to%20previous%20work%2C%20our%20model%20demonstrates%0Asubstantial%20scalability.%20As%20the%20size%20of%20the%20backbone%20LLM%20model%20increases%2C%20our%0Aframework%20shows%20proportional%20improvements%20in%20evaluation%20metrics%20%28a.k.a.%20scaling%0Alaw%29.%20Our%20method%20also%20exhibits%20strong%20controllability%20where%20the%20content%2C%20style%0Aof%20the%20generated%20gestures%20can%20be%20controlled%20by%20text%20prompt.%20To%20the%20best%20of%20our%0Aknowledge%2C%20LLM%20gesticulator%20is%20the%20first%20work%20that%20use%20LLM%20on%20the%20co-speech%0Ageneration%20task.%20Evaluation%20with%20existing%20objective%20metrics%20and%20user%20studies%0Aindicate%20that%20our%20framework%20outperforms%20prior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Gesticulator%253A%2520Leveraging%2520Large%2520Language%2520Models%2520for%2520Scalable%2520and%250A%2520%2520Controllable%2520Co-Speech%2520Gesture%2520Synthesis%26entry.906535625%3DHaozhou%2520Pang%2520and%2520Tianwei%2520Ding%2520and%2520Lanshan%2520He%2520and%2520Ming%2520Tao%2520and%2520Lu%2520Zhang%2520and%2520Qi%2520Gan%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520LLM%2520Gesticulator%252C%2520an%2520LLM-based%2520audio-driven%250Aco-speech%2520gesture%2520generation%2520framework%2520that%2520synthesizes%2520full-body%2520animations%250Athat%2520are%2520rhythmically%2520aligned%2520with%2520the%2520input%2520audio%2520while%2520exhibiting%2520natural%250Amovements%2520and%2520editability.%2520Compared%2520to%2520previous%2520work%252C%2520our%2520model%2520demonstrates%250Asubstantial%2520scalability.%2520As%2520the%2520size%2520of%2520the%2520backbone%2520LLM%2520model%2520increases%252C%2520our%250Aframework%2520shows%2520proportional%2520improvements%2520in%2520evaluation%2520metrics%2520%2528a.k.a.%2520scaling%250Alaw%2529.%2520Our%2520method%2520also%2520exhibits%2520strong%2520controllability%2520where%2520the%2520content%252C%2520style%250Aof%2520the%2520generated%2520gestures%2520can%2520be%2520controlled%2520by%2520text%2520prompt.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520LLM%2520gesticulator%2520is%2520the%2520first%2520work%2520that%2520use%2520LLM%2520on%2520the%2520co-speech%250Ageneration%2520task.%2520Evaluation%2520with%2520existing%2520objective%2520metrics%2520and%2520user%2520studies%250Aindicate%2520that%2520our%2520framework%2520outperforms%2520prior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Gesticulator%3A%20Leveraging%20Large%20Language%20Models%20for%20Scalable%20and%0A%20%20Controllable%20Co-Speech%20Gesture%20Synthesis&entry.906535625=Haozhou%20Pang%20and%20Tianwei%20Ding%20and%20Lanshan%20He%20and%20Ming%20Tao%20and%20Lu%20Zhang%20and%20Qi%20Gan&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20LLM%20Gesticulator%2C%20an%20LLM-based%20audio-driven%0Aco-speech%20gesture%20generation%20framework%20that%20synthesizes%20full-body%20animations%0Athat%20are%20rhythmically%20aligned%20with%20the%20input%20audio%20while%20exhibiting%20natural%0Amovements%20and%20editability.%20Compared%20to%20previous%20work%2C%20our%20model%20demonstrates%0Asubstantial%20scalability.%20As%20the%20size%20of%20the%20backbone%20LLM%20model%20increases%2C%20our%0Aframework%20shows%20proportional%20improvements%20in%20evaluation%20metrics%20%28a.k.a.%20scaling%0Alaw%29.%20Our%20method%20also%20exhibits%20strong%20controllability%20where%20the%20content%2C%20style%0Aof%20the%20generated%20gestures%20can%20be%20controlled%20by%20text%20prompt.%20To%20the%20best%20of%20our%0Aknowledge%2C%20LLM%20gesticulator%20is%20the%20first%20work%20that%20use%20LLM%20on%20the%20co-speech%0Ageneration%20task.%20Evaluation%20with%20existing%20objective%20metrics%20and%20user%20studies%0Aindicate%20that%20our%20framework%20outperforms%20prior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10851v2&entry.124074799=Read"},
{"title": "Research on Travel Route Planing Problems Based on Greedy Algorithm", "author": "Yiquan Wang", "abstract": "  The route planning problem based on the greedy algorithm represents a method\nof identifying the optimal or near-optimal route between a given start point\nand end point. In this paper, the PCA method is employed initially to downscale\nthe city evaluation indexes, extract the key principal components, and then\ndownscale the data using the KMO and TOPSIS algorithms, all of which are based\non the MindSpore framework. Secondly, for the dataset that does not pass the\nKMO test, the entropy weight method and TOPSIS method will be employed for\ncomprehensive evaluation. Finally, a route planning algorithm is proposed and\noptimised based on the greedy algorithm, which provides personalised route\ncustomisation according to the different needs of tourists. In addition, the\nlocal travelling efficiency, the time required to visit tourist attractions and\nthe necessary daily breaks are considered in order to reduce the cost and avoid\nfalling into the locally optimal solution.\n", "link": "http://arxiv.org/abs/2410.13226v2", "date": "2024-10-22", "relevancy": 2.0377, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4371}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3939}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20Travel%20Route%20Planing%20Problems%20Based%20on%20Greedy%20Algorithm&body=Title%3A%20Research%20on%20Travel%20Route%20Planing%20Problems%20Based%20on%20Greedy%20Algorithm%0AAuthor%3A%20Yiquan%20Wang%0AAbstract%3A%20%20%20The%20route%20planning%20problem%20based%20on%20the%20greedy%20algorithm%20represents%20a%20method%0Aof%20identifying%20the%20optimal%20or%20near-optimal%20route%20between%20a%20given%20start%20point%0Aand%20end%20point.%20In%20this%20paper%2C%20the%20PCA%20method%20is%20employed%20initially%20to%20downscale%0Athe%20city%20evaluation%20indexes%2C%20extract%20the%20key%20principal%20components%2C%20and%20then%0Adownscale%20the%20data%20using%20the%20KMO%20and%20TOPSIS%20algorithms%2C%20all%20of%20which%20are%20based%0Aon%20the%20MindSpore%20framework.%20Secondly%2C%20for%20the%20dataset%20that%20does%20not%20pass%20the%0AKMO%20test%2C%20the%20entropy%20weight%20method%20and%20TOPSIS%20method%20will%20be%20employed%20for%0Acomprehensive%20evaluation.%20Finally%2C%20a%20route%20planning%20algorithm%20is%20proposed%20and%0Aoptimised%20based%20on%20the%20greedy%20algorithm%2C%20which%20provides%20personalised%20route%0Acustomisation%20according%20to%20the%20different%20needs%20of%20tourists.%20In%20addition%2C%20the%0Alocal%20travelling%20efficiency%2C%20the%20time%20required%20to%20visit%20tourist%20attractions%20and%0Athe%20necessary%20daily%20breaks%20are%20considered%20in%20order%20to%20reduce%20the%20cost%20and%20avoid%0Afalling%20into%20the%20locally%20optimal%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13226v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520Travel%2520Route%2520Planing%2520Problems%2520Based%2520on%2520Greedy%2520Algorithm%26entry.906535625%3DYiquan%2520Wang%26entry.1292438233%3D%2520%2520The%2520route%2520planning%2520problem%2520based%2520on%2520the%2520greedy%2520algorithm%2520represents%2520a%2520method%250Aof%2520identifying%2520the%2520optimal%2520or%2520near-optimal%2520route%2520between%2520a%2520given%2520start%2520point%250Aand%2520end%2520point.%2520In%2520this%2520paper%252C%2520the%2520PCA%2520method%2520is%2520employed%2520initially%2520to%2520downscale%250Athe%2520city%2520evaluation%2520indexes%252C%2520extract%2520the%2520key%2520principal%2520components%252C%2520and%2520then%250Adownscale%2520the%2520data%2520using%2520the%2520KMO%2520and%2520TOPSIS%2520algorithms%252C%2520all%2520of%2520which%2520are%2520based%250Aon%2520the%2520MindSpore%2520framework.%2520Secondly%252C%2520for%2520the%2520dataset%2520that%2520does%2520not%2520pass%2520the%250AKMO%2520test%252C%2520the%2520entropy%2520weight%2520method%2520and%2520TOPSIS%2520method%2520will%2520be%2520employed%2520for%250Acomprehensive%2520evaluation.%2520Finally%252C%2520a%2520route%2520planning%2520algorithm%2520is%2520proposed%2520and%250Aoptimised%2520based%2520on%2520the%2520greedy%2520algorithm%252C%2520which%2520provides%2520personalised%2520route%250Acustomisation%2520according%2520to%2520the%2520different%2520needs%2520of%2520tourists.%2520In%2520addition%252C%2520the%250Alocal%2520travelling%2520efficiency%252C%2520the%2520time%2520required%2520to%2520visit%2520tourist%2520attractions%2520and%250Athe%2520necessary%2520daily%2520breaks%2520are%2520considered%2520in%2520order%2520to%2520reduce%2520the%2520cost%2520and%2520avoid%250Afalling%2520into%2520the%2520locally%2520optimal%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13226v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20Travel%20Route%20Planing%20Problems%20Based%20on%20Greedy%20Algorithm&entry.906535625=Yiquan%20Wang&entry.1292438233=%20%20The%20route%20planning%20problem%20based%20on%20the%20greedy%20algorithm%20represents%20a%20method%0Aof%20identifying%20the%20optimal%20or%20near-optimal%20route%20between%20a%20given%20start%20point%0Aand%20end%20point.%20In%20this%20paper%2C%20the%20PCA%20method%20is%20employed%20initially%20to%20downscale%0Athe%20city%20evaluation%20indexes%2C%20extract%20the%20key%20principal%20components%2C%20and%20then%0Adownscale%20the%20data%20using%20the%20KMO%20and%20TOPSIS%20algorithms%2C%20all%20of%20which%20are%20based%0Aon%20the%20MindSpore%20framework.%20Secondly%2C%20for%20the%20dataset%20that%20does%20not%20pass%20the%0AKMO%20test%2C%20the%20entropy%20weight%20method%20and%20TOPSIS%20method%20will%20be%20employed%20for%0Acomprehensive%20evaluation.%20Finally%2C%20a%20route%20planning%20algorithm%20is%20proposed%20and%0Aoptimised%20based%20on%20the%20greedy%20algorithm%2C%20which%20provides%20personalised%20route%0Acustomisation%20according%20to%20the%20different%20needs%20of%20tourists.%20In%20addition%2C%20the%0Alocal%20travelling%20efficiency%2C%20the%20time%20required%20to%20visit%20tourist%20attractions%20and%0Athe%20necessary%20daily%20breaks%20are%20considered%20in%20order%20to%20reduce%20the%20cost%20and%20avoid%0Afalling%20into%20the%20locally%20optimal%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13226v2&entry.124074799=Read"},
{"title": "LIMIS: Towards Language-based Interactive Medical Image Segmentation", "author": "Lena Heinemann and Alexander Jaus and Zdravko Marinov and Moon Kim and Maria Francesca Spadea and Jens Kleesiek and Rainer Stiefelhagen", "abstract": "  Within this work, we introduce LIMIS: The first purely language-based\ninteractive medical image segmentation model. We achieve this by adapting\nGrounded SAM to the medical domain and designing a language-based model\ninteraction strategy that allows radiologists to incorporate their knowledge\ninto the segmentation process. LIMIS produces high-quality initial segmentation\nmasks by leveraging medical foundation models and allows users to adapt\nsegmentation masks using only language, opening up interactive segmentation to\nscenarios where physicians require using their hands for other tasks. We\nevaluate LIMIS on three publicly available medical datasets in terms of\nperformance and usability with experts from the medical domain confirming its\nhigh-quality segmentation masks and its interactive usability.\n", "link": "http://arxiv.org/abs/2410.16939v1", "date": "2024-10-22", "relevancy": 2.0362, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIMIS%3A%20Towards%20Language-based%20Interactive%20Medical%20Image%20Segmentation&body=Title%3A%20LIMIS%3A%20Towards%20Language-based%20Interactive%20Medical%20Image%20Segmentation%0AAuthor%3A%20Lena%20Heinemann%20and%20Alexander%20Jaus%20and%20Zdravko%20Marinov%20and%20Moon%20Kim%20and%20Maria%20Francesca%20Spadea%20and%20Jens%20Kleesiek%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20Within%20this%20work%2C%20we%20introduce%20LIMIS%3A%20The%20first%20purely%20language-based%0Ainteractive%20medical%20image%20segmentation%20model.%20We%20achieve%20this%20by%20adapting%0AGrounded%20SAM%20to%20the%20medical%20domain%20and%20designing%20a%20language-based%20model%0Ainteraction%20strategy%20that%20allows%20radiologists%20to%20incorporate%20their%20knowledge%0Ainto%20the%20segmentation%20process.%20LIMIS%20produces%20high-quality%20initial%20segmentation%0Amasks%20by%20leveraging%20medical%20foundation%20models%20and%20allows%20users%20to%20adapt%0Asegmentation%20masks%20using%20only%20language%2C%20opening%20up%20interactive%20segmentation%20to%0Ascenarios%20where%20physicians%20require%20using%20their%20hands%20for%20other%20tasks.%20We%0Aevaluate%20LIMIS%20on%20three%20publicly%20available%20medical%20datasets%20in%20terms%20of%0Aperformance%20and%20usability%20with%20experts%20from%20the%20medical%20domain%20confirming%20its%0Ahigh-quality%20segmentation%20masks%20and%20its%20interactive%20usability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIMIS%253A%2520Towards%2520Language-based%2520Interactive%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DLena%2520Heinemann%2520and%2520Alexander%2520Jaus%2520and%2520Zdravko%2520Marinov%2520and%2520Moon%2520Kim%2520and%2520Maria%2520Francesca%2520Spadea%2520and%2520Jens%2520Kleesiek%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520Within%2520this%2520work%252C%2520we%2520introduce%2520LIMIS%253A%2520The%2520first%2520purely%2520language-based%250Ainteractive%2520medical%2520image%2520segmentation%2520model.%2520We%2520achieve%2520this%2520by%2520adapting%250AGrounded%2520SAM%2520to%2520the%2520medical%2520domain%2520and%2520designing%2520a%2520language-based%2520model%250Ainteraction%2520strategy%2520that%2520allows%2520radiologists%2520to%2520incorporate%2520their%2520knowledge%250Ainto%2520the%2520segmentation%2520process.%2520LIMIS%2520produces%2520high-quality%2520initial%2520segmentation%250Amasks%2520by%2520leveraging%2520medical%2520foundation%2520models%2520and%2520allows%2520users%2520to%2520adapt%250Asegmentation%2520masks%2520using%2520only%2520language%252C%2520opening%2520up%2520interactive%2520segmentation%2520to%250Ascenarios%2520where%2520physicians%2520require%2520using%2520their%2520hands%2520for%2520other%2520tasks.%2520We%250Aevaluate%2520LIMIS%2520on%2520three%2520publicly%2520available%2520medical%2520datasets%2520in%2520terms%2520of%250Aperformance%2520and%2520usability%2520with%2520experts%2520from%2520the%2520medical%2520domain%2520confirming%2520its%250Ahigh-quality%2520segmentation%2520masks%2520and%2520its%2520interactive%2520usability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIMIS%3A%20Towards%20Language-based%20Interactive%20Medical%20Image%20Segmentation&entry.906535625=Lena%20Heinemann%20and%20Alexander%20Jaus%20and%20Zdravko%20Marinov%20and%20Moon%20Kim%20and%20Maria%20Francesca%20Spadea%20and%20Jens%20Kleesiek%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20Within%20this%20work%2C%20we%20introduce%20LIMIS%3A%20The%20first%20purely%20language-based%0Ainteractive%20medical%20image%20segmentation%20model.%20We%20achieve%20this%20by%20adapting%0AGrounded%20SAM%20to%20the%20medical%20domain%20and%20designing%20a%20language-based%20model%0Ainteraction%20strategy%20that%20allows%20radiologists%20to%20incorporate%20their%20knowledge%0Ainto%20the%20segmentation%20process.%20LIMIS%20produces%20high-quality%20initial%20segmentation%0Amasks%20by%20leveraging%20medical%20foundation%20models%20and%20allows%20users%20to%20adapt%0Asegmentation%20masks%20using%20only%20language%2C%20opening%20up%20interactive%20segmentation%20to%0Ascenarios%20where%20physicians%20require%20using%20their%20hands%20for%20other%20tasks.%20We%0Aevaluate%20LIMIS%20on%20three%20publicly%20available%20medical%20datasets%20in%20terms%20of%0Aperformance%20and%20usability%20with%20experts%20from%20the%20medical%20domain%20confirming%20its%0Ahigh-quality%20segmentation%20masks%20and%20its%20interactive%20usability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16939v1&entry.124074799=Read"},
{"title": "Rethinking Complex Queries on Knowledge Graphs with Neural Link\n  Predictors", "author": "Hang Yin and Zihao Wang and Yangqiu Song", "abstract": "  Reasoning on knowledge graphs is a challenging task because it utilizes\nobserved information to predict the missing one. Particularly, answering\ncomplex queries based on first-order logic is one of the crucial tasks to\nverify learning to reason abilities for generalization and composition.\nRecently, the prevailing method is query embedding which learns the embedding\nof a set of entities and treats logic operations as set operations and has\nshown great empirical success. Though there has been much research following\nthe same formulation, many of its claims lack a formal and systematic\ninspection. In this paper, we rethink this formulation and justify many of the\nprevious claims by characterizing the scope of queries investigated previously\nand precisely identifying the gap between its formulation and its goal, as well\nas providing complexity analysis for the currently investigated queries.\nMoreover, we develop a new dataset containing ten new types of queries with\nfeatures that have never been considered and therefore can provide a thorough\ninvestigation of complex queries. Finally, we propose a new neural-symbolic\nmethod, Fuzzy Inference with Truth value (FIT), where we equip the neural link\npredictors with fuzzy logic theory to support end-to-end learning using complex\nqueries with provable reasoning capability. Empirical results show that our\nmethod outperforms previous methods significantly in the new dataset and also\nsurpasses previous methods in the existing dataset at the same time.\n", "link": "http://arxiv.org/abs/2304.07063v4", "date": "2024-10-22", "relevancy": 2.028, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Complex%20Queries%20on%20Knowledge%20Graphs%20with%20Neural%20Link%0A%20%20Predictors&body=Title%3A%20Rethinking%20Complex%20Queries%20on%20Knowledge%20Graphs%20with%20Neural%20Link%0A%20%20Predictors%0AAuthor%3A%20Hang%20Yin%20and%20Zihao%20Wang%20and%20Yangqiu%20Song%0AAbstract%3A%20%20%20Reasoning%20on%20knowledge%20graphs%20is%20a%20challenging%20task%20because%20it%20utilizes%0Aobserved%20information%20to%20predict%20the%20missing%20one.%20Particularly%2C%20answering%0Acomplex%20queries%20based%20on%20first-order%20logic%20is%20one%20of%20the%20crucial%20tasks%20to%0Averify%20learning%20to%20reason%20abilities%20for%20generalization%20and%20composition.%0ARecently%2C%20the%20prevailing%20method%20is%20query%20embedding%20which%20learns%20the%20embedding%0Aof%20a%20set%20of%20entities%20and%20treats%20logic%20operations%20as%20set%20operations%20and%20has%0Ashown%20great%20empirical%20success.%20Though%20there%20has%20been%20much%20research%20following%0Athe%20same%20formulation%2C%20many%20of%20its%20claims%20lack%20a%20formal%20and%20systematic%0Ainspection.%20In%20this%20paper%2C%20we%20rethink%20this%20formulation%20and%20justify%20many%20of%20the%0Aprevious%20claims%20by%20characterizing%20the%20scope%20of%20queries%20investigated%20previously%0Aand%20precisely%20identifying%20the%20gap%20between%20its%20formulation%20and%20its%20goal%2C%20as%20well%0Aas%20providing%20complexity%20analysis%20for%20the%20currently%20investigated%20queries.%0AMoreover%2C%20we%20develop%20a%20new%20dataset%20containing%20ten%20new%20types%20of%20queries%20with%0Afeatures%20that%20have%20never%20been%20considered%20and%20therefore%20can%20provide%20a%20thorough%0Ainvestigation%20of%20complex%20queries.%20Finally%2C%20we%20propose%20a%20new%20neural-symbolic%0Amethod%2C%20Fuzzy%20Inference%20with%20Truth%20value%20%28FIT%29%2C%20where%20we%20equip%20the%20neural%20link%0Apredictors%20with%20fuzzy%20logic%20theory%20to%20support%20end-to-end%20learning%20using%20complex%0Aqueries%20with%20provable%20reasoning%20capability.%20Empirical%20results%20show%20that%20our%0Amethod%20outperforms%20previous%20methods%20significantly%20in%20the%20new%20dataset%20and%20also%0Asurpasses%20previous%20methods%20in%20the%20existing%20dataset%20at%20the%20same%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.07063v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Complex%2520Queries%2520on%2520Knowledge%2520Graphs%2520with%2520Neural%2520Link%250A%2520%2520Predictors%26entry.906535625%3DHang%2520Yin%2520and%2520Zihao%2520Wang%2520and%2520Yangqiu%2520Song%26entry.1292438233%3D%2520%2520Reasoning%2520on%2520knowledge%2520graphs%2520is%2520a%2520challenging%2520task%2520because%2520it%2520utilizes%250Aobserved%2520information%2520to%2520predict%2520the%2520missing%2520one.%2520Particularly%252C%2520answering%250Acomplex%2520queries%2520based%2520on%2520first-order%2520logic%2520is%2520one%2520of%2520the%2520crucial%2520tasks%2520to%250Averify%2520learning%2520to%2520reason%2520abilities%2520for%2520generalization%2520and%2520composition.%250ARecently%252C%2520the%2520prevailing%2520method%2520is%2520query%2520embedding%2520which%2520learns%2520the%2520embedding%250Aof%2520a%2520set%2520of%2520entities%2520and%2520treats%2520logic%2520operations%2520as%2520set%2520operations%2520and%2520has%250Ashown%2520great%2520empirical%2520success.%2520Though%2520there%2520has%2520been%2520much%2520research%2520following%250Athe%2520same%2520formulation%252C%2520many%2520of%2520its%2520claims%2520lack%2520a%2520formal%2520and%2520systematic%250Ainspection.%2520In%2520this%2520paper%252C%2520we%2520rethink%2520this%2520formulation%2520and%2520justify%2520many%2520of%2520the%250Aprevious%2520claims%2520by%2520characterizing%2520the%2520scope%2520of%2520queries%2520investigated%2520previously%250Aand%2520precisely%2520identifying%2520the%2520gap%2520between%2520its%2520formulation%2520and%2520its%2520goal%252C%2520as%2520well%250Aas%2520providing%2520complexity%2520analysis%2520for%2520the%2520currently%2520investigated%2520queries.%250AMoreover%252C%2520we%2520develop%2520a%2520new%2520dataset%2520containing%2520ten%2520new%2520types%2520of%2520queries%2520with%250Afeatures%2520that%2520have%2520never%2520been%2520considered%2520and%2520therefore%2520can%2520provide%2520a%2520thorough%250Ainvestigation%2520of%2520complex%2520queries.%2520Finally%252C%2520we%2520propose%2520a%2520new%2520neural-symbolic%250Amethod%252C%2520Fuzzy%2520Inference%2520with%2520Truth%2520value%2520%2528FIT%2529%252C%2520where%2520we%2520equip%2520the%2520neural%2520link%250Apredictors%2520with%2520fuzzy%2520logic%2520theory%2520to%2520support%2520end-to-end%2520learning%2520using%2520complex%250Aqueries%2520with%2520provable%2520reasoning%2520capability.%2520Empirical%2520results%2520show%2520that%2520our%250Amethod%2520outperforms%2520previous%2520methods%2520significantly%2520in%2520the%2520new%2520dataset%2520and%2520also%250Asurpasses%2520previous%2520methods%2520in%2520the%2520existing%2520dataset%2520at%2520the%2520same%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.07063v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Complex%20Queries%20on%20Knowledge%20Graphs%20with%20Neural%20Link%0A%20%20Predictors&entry.906535625=Hang%20Yin%20and%20Zihao%20Wang%20and%20Yangqiu%20Song&entry.1292438233=%20%20Reasoning%20on%20knowledge%20graphs%20is%20a%20challenging%20task%20because%20it%20utilizes%0Aobserved%20information%20to%20predict%20the%20missing%20one.%20Particularly%2C%20answering%0Acomplex%20queries%20based%20on%20first-order%20logic%20is%20one%20of%20the%20crucial%20tasks%20to%0Averify%20learning%20to%20reason%20abilities%20for%20generalization%20and%20composition.%0ARecently%2C%20the%20prevailing%20method%20is%20query%20embedding%20which%20learns%20the%20embedding%0Aof%20a%20set%20of%20entities%20and%20treats%20logic%20operations%20as%20set%20operations%20and%20has%0Ashown%20great%20empirical%20success.%20Though%20there%20has%20been%20much%20research%20following%0Athe%20same%20formulation%2C%20many%20of%20its%20claims%20lack%20a%20formal%20and%20systematic%0Ainspection.%20In%20this%20paper%2C%20we%20rethink%20this%20formulation%20and%20justify%20many%20of%20the%0Aprevious%20claims%20by%20characterizing%20the%20scope%20of%20queries%20investigated%20previously%0Aand%20precisely%20identifying%20the%20gap%20between%20its%20formulation%20and%20its%20goal%2C%20as%20well%0Aas%20providing%20complexity%20analysis%20for%20the%20currently%20investigated%20queries.%0AMoreover%2C%20we%20develop%20a%20new%20dataset%20containing%20ten%20new%20types%20of%20queries%20with%0Afeatures%20that%20have%20never%20been%20considered%20and%20therefore%20can%20provide%20a%20thorough%0Ainvestigation%20of%20complex%20queries.%20Finally%2C%20we%20propose%20a%20new%20neural-symbolic%0Amethod%2C%20Fuzzy%20Inference%20with%20Truth%20value%20%28FIT%29%2C%20where%20we%20equip%20the%20neural%20link%0Apredictors%20with%20fuzzy%20logic%20theory%20to%20support%20end-to-end%20learning%20using%20complex%0Aqueries%20with%20provable%20reasoning%20capability.%20Empirical%20results%20show%20that%20our%0Amethod%20outperforms%20previous%20methods%20significantly%20in%20the%20new%20dataset%20and%20also%0Asurpasses%20previous%20methods%20in%20the%20existing%20dataset%20at%20the%20same%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.07063v4&entry.124074799=Read"},
{"title": "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances\n  Model Merging", "author": "Ke Wang and Nikolaos Dimitriadis and Alessandro Favero and Guillermo Ortiz-Jimenez and Francois Fleuret and Pascal Frossard", "abstract": "  Large pre-trained models exhibit impressive zero-shot performance across\ndiverse tasks, but fine-tuning often leads to catastrophic forgetting, where\nimprovements on a target domain degrade generalization on other tasks. To\naddress this challenge, we introduce LiNeS, Layer-increasing Network Scaling, a\npost-training editing technique designed to preserve pre-trained generalization\nwhile enhancing fine-tuned task performance. LiNeS scales parameter updates\nlinearly based on their layer depth within the network, maintaining shallow\nlayers close to their pre-trained values to preserve general features while\nallowing deeper layers to retain task-specific representations. We further\nextend this approach to multi-task model merging scenarios, where layer-wise\nscaling of merged parameters reduces negative task interference. LiNeS\ndemonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nIt mitigates forgetting, enhances out-of-distribution generalization,\nintegrates seamlessly with existing multi-task model merging baselines\nimproving their performance across benchmarks and model sizes, and can boost\ngeneralization when merging LLM policies aligned with different rewards via\nRLHF. Importantly, our method is simple to implement and complementary to many\nexisting techniques.\n", "link": "http://arxiv.org/abs/2410.17146v1", "date": "2024-10-22", "relevancy": 2.0275, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.517}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5139}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiNeS%3A%20Post-training%20Layer%20Scaling%20Prevents%20Forgetting%20and%20Enhances%0A%20%20Model%20Merging&body=Title%3A%20LiNeS%3A%20Post-training%20Layer%20Scaling%20Prevents%20Forgetting%20and%20Enhances%0A%20%20Model%20Merging%0AAuthor%3A%20Ke%20Wang%20and%20Nikolaos%20Dimitriadis%20and%20Alessandro%20Favero%20and%20Guillermo%20Ortiz-Jimenez%20and%20Francois%20Fleuret%20and%20Pascal%20Frossard%0AAbstract%3A%20%20%20Large%20pre-trained%20models%20exhibit%20impressive%20zero-shot%20performance%20across%0Adiverse%20tasks%2C%20but%20fine-tuning%20often%20leads%20to%20catastrophic%20forgetting%2C%20where%0Aimprovements%20on%20a%20target%20domain%20degrade%20generalization%20on%20other%20tasks.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20LiNeS%2C%20Layer-increasing%20Network%20Scaling%2C%20a%0Apost-training%20editing%20technique%20designed%20to%20preserve%20pre-trained%20generalization%0Awhile%20enhancing%20fine-tuned%20task%20performance.%20LiNeS%20scales%20parameter%20updates%0Alinearly%20based%20on%20their%20layer%20depth%20within%20the%20network%2C%20maintaining%20shallow%0Alayers%20close%20to%20their%20pre-trained%20values%20to%20preserve%20general%20features%20while%0Aallowing%20deeper%20layers%20to%20retain%20task-specific%20representations.%20We%20further%0Aextend%20this%20approach%20to%20multi-task%20model%20merging%20scenarios%2C%20where%20layer-wise%0Ascaling%20of%20merged%20parameters%20reduces%20negative%20task%20interference.%20LiNeS%0Ademonstrates%20significant%20improvements%20in%20both%20single-task%20and%20multi-task%0Asettings%20across%20various%20benchmarks%20in%20vision%20and%20natural%20language%20processing.%0AIt%20mitigates%20forgetting%2C%20enhances%20out-of-distribution%20generalization%2C%0Aintegrates%20seamlessly%20with%20existing%20multi-task%20model%20merging%20baselines%0Aimproving%20their%20performance%20across%20benchmarks%20and%20model%20sizes%2C%20and%20can%20boost%0Ageneralization%20when%20merging%20LLM%20policies%20aligned%20with%20different%20rewards%20via%0ARLHF.%20Importantly%2C%20our%20method%20is%20simple%20to%20implement%20and%20complementary%20to%20many%0Aexisting%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiNeS%253A%2520Post-training%2520Layer%2520Scaling%2520Prevents%2520Forgetting%2520and%2520Enhances%250A%2520%2520Model%2520Merging%26entry.906535625%3DKe%2520Wang%2520and%2520Nikolaos%2520Dimitriadis%2520and%2520Alessandro%2520Favero%2520and%2520Guillermo%2520Ortiz-Jimenez%2520and%2520Francois%2520Fleuret%2520and%2520Pascal%2520Frossard%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520models%2520exhibit%2520impressive%2520zero-shot%2520performance%2520across%250Adiverse%2520tasks%252C%2520but%2520fine-tuning%2520often%2520leads%2520to%2520catastrophic%2520forgetting%252C%2520where%250Aimprovements%2520on%2520a%2520target%2520domain%2520degrade%2520generalization%2520on%2520other%2520tasks.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520LiNeS%252C%2520Layer-increasing%2520Network%2520Scaling%252C%2520a%250Apost-training%2520editing%2520technique%2520designed%2520to%2520preserve%2520pre-trained%2520generalization%250Awhile%2520enhancing%2520fine-tuned%2520task%2520performance.%2520LiNeS%2520scales%2520parameter%2520updates%250Alinearly%2520based%2520on%2520their%2520layer%2520depth%2520within%2520the%2520network%252C%2520maintaining%2520shallow%250Alayers%2520close%2520to%2520their%2520pre-trained%2520values%2520to%2520preserve%2520general%2520features%2520while%250Aallowing%2520deeper%2520layers%2520to%2520retain%2520task-specific%2520representations.%2520We%2520further%250Aextend%2520this%2520approach%2520to%2520multi-task%2520model%2520merging%2520scenarios%252C%2520where%2520layer-wise%250Ascaling%2520of%2520merged%2520parameters%2520reduces%2520negative%2520task%2520interference.%2520LiNeS%250Ademonstrates%2520significant%2520improvements%2520in%2520both%2520single-task%2520and%2520multi-task%250Asettings%2520across%2520various%2520benchmarks%2520in%2520vision%2520and%2520natural%2520language%2520processing.%250AIt%2520mitigates%2520forgetting%252C%2520enhances%2520out-of-distribution%2520generalization%252C%250Aintegrates%2520seamlessly%2520with%2520existing%2520multi-task%2520model%2520merging%2520baselines%250Aimproving%2520their%2520performance%2520across%2520benchmarks%2520and%2520model%2520sizes%252C%2520and%2520can%2520boost%250Ageneralization%2520when%2520merging%2520LLM%2520policies%2520aligned%2520with%2520different%2520rewards%2520via%250ARLHF.%2520Importantly%252C%2520our%2520method%2520is%2520simple%2520to%2520implement%2520and%2520complementary%2520to%2520many%250Aexisting%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiNeS%3A%20Post-training%20Layer%20Scaling%20Prevents%20Forgetting%20and%20Enhances%0A%20%20Model%20Merging&entry.906535625=Ke%20Wang%20and%20Nikolaos%20Dimitriadis%20and%20Alessandro%20Favero%20and%20Guillermo%20Ortiz-Jimenez%20and%20Francois%20Fleuret%20and%20Pascal%20Frossard&entry.1292438233=%20%20Large%20pre-trained%20models%20exhibit%20impressive%20zero-shot%20performance%20across%0Adiverse%20tasks%2C%20but%20fine-tuning%20often%20leads%20to%20catastrophic%20forgetting%2C%20where%0Aimprovements%20on%20a%20target%20domain%20degrade%20generalization%20on%20other%20tasks.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20LiNeS%2C%20Layer-increasing%20Network%20Scaling%2C%20a%0Apost-training%20editing%20technique%20designed%20to%20preserve%20pre-trained%20generalization%0Awhile%20enhancing%20fine-tuned%20task%20performance.%20LiNeS%20scales%20parameter%20updates%0Alinearly%20based%20on%20their%20layer%20depth%20within%20the%20network%2C%20maintaining%20shallow%0Alayers%20close%20to%20their%20pre-trained%20values%20to%20preserve%20general%20features%20while%0Aallowing%20deeper%20layers%20to%20retain%20task-specific%20representations.%20We%20further%0Aextend%20this%20approach%20to%20multi-task%20model%20merging%20scenarios%2C%20where%20layer-wise%0Ascaling%20of%20merged%20parameters%20reduces%20negative%20task%20interference.%20LiNeS%0Ademonstrates%20significant%20improvements%20in%20both%20single-task%20and%20multi-task%0Asettings%20across%20various%20benchmarks%20in%20vision%20and%20natural%20language%20processing.%0AIt%20mitigates%20forgetting%2C%20enhances%20out-of-distribution%20generalization%2C%0Aintegrates%20seamlessly%20with%20existing%20multi-task%20model%20merging%20baselines%0Aimproving%20their%20performance%20across%20benchmarks%20and%20model%20sizes%2C%20and%20can%20boost%0Ageneralization%20when%20merging%20LLM%20policies%20aligned%20with%20different%20rewards%20via%0ARLHF.%20Importantly%2C%20our%20method%20is%20simple%20to%20implement%20and%20complementary%20to%20many%0Aexisting%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17146v1&entry.124074799=Read"},
{"title": "CK4Gen: A Knowledge Distillation Framework for Generating High-Utility\n  Synthetic Survival Datasets in Healthcare", "author": "Nicholas I-Hsien Kuo and Blanca Gallego and Louisa Jorm", "abstract": "  Access to real clinical data is heavily restricted by privacy regulations,\nhindering both healthcare research and education. These constraints slow\nprogress in developing new treatments and data-driven healthcare solutions,\nwhile also limiting students' access to real-world datasets, leaving them\nwithout essential practical skills. High-utility synthetic datasets are\ntherefore critical for advancing research and providing meaningful training\nmaterial. However, current generative models -- such as Variational\nAutoencoders (VAEs) and Generative Adversarial Networks (GANs) -- produce\nsurface-level realism at the expense of healthcare utility, blending distinct\npatient profiles and producing synthetic data of limited practical relevance.\nTo overcome these limitations, we introduce CK4Gen (Cox Knowledge for\nGeneration), a novel framework that leverages knowledge distillation from Cox\nProportional Hazards (CoxPH) models to create synthetic survival datasets that\npreserve key clinical characteristics, including hazard ratios and survival\ncurves. CK4Gen avoids the interpolation issues seen in VAEs and GANs by\nmaintaining distinct patient risk profiles, ensuring realistic and reliable\noutputs for research and educational use. Validated across four benchmark\ndatasets -- GBSG2, ACTG320, WHAS500, and FLChain -- CK4Gen outperforms\ncompeting techniques by better aligning real and synthetic data, enhancing\nsurvival model performance in both discrimination and calibration via data\naugmentation. As CK4Gen is scalable across clinical conditions, and with code\nto be made publicly available, future researchers can apply it to their own\ndatasets to generate synthetic versions suitable for open sharing.\n", "link": "http://arxiv.org/abs/2410.16872v1", "date": "2024-10-22", "relevancy": 2.0261, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5112}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5076}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CK4Gen%3A%20A%20Knowledge%20Distillation%20Framework%20for%20Generating%20High-Utility%0A%20%20Synthetic%20Survival%20Datasets%20in%20Healthcare&body=Title%3A%20CK4Gen%3A%20A%20Knowledge%20Distillation%20Framework%20for%20Generating%20High-Utility%0A%20%20Synthetic%20Survival%20Datasets%20in%20Healthcare%0AAuthor%3A%20Nicholas%20I-Hsien%20Kuo%20and%20Blanca%20Gallego%20and%20Louisa%20Jorm%0AAbstract%3A%20%20%20Access%20to%20real%20clinical%20data%20is%20heavily%20restricted%20by%20privacy%20regulations%2C%0Ahindering%20both%20healthcare%20research%20and%20education.%20These%20constraints%20slow%0Aprogress%20in%20developing%20new%20treatments%20and%20data-driven%20healthcare%20solutions%2C%0Awhile%20also%20limiting%20students%27%20access%20to%20real-world%20datasets%2C%20leaving%20them%0Awithout%20essential%20practical%20skills.%20High-utility%20synthetic%20datasets%20are%0Atherefore%20critical%20for%20advancing%20research%20and%20providing%20meaningful%20training%0Amaterial.%20However%2C%20current%20generative%20models%20--%20such%20as%20Variational%0AAutoencoders%20%28VAEs%29%20and%20Generative%20Adversarial%20Networks%20%28GANs%29%20--%20produce%0Asurface-level%20realism%20at%20the%20expense%20of%20healthcare%20utility%2C%20blending%20distinct%0Apatient%20profiles%20and%20producing%20synthetic%20data%20of%20limited%20practical%20relevance.%0ATo%20overcome%20these%20limitations%2C%20we%20introduce%20CK4Gen%20%28Cox%20Knowledge%20for%0AGeneration%29%2C%20a%20novel%20framework%20that%20leverages%20knowledge%20distillation%20from%20Cox%0AProportional%20Hazards%20%28CoxPH%29%20models%20to%20create%20synthetic%20survival%20datasets%20that%0Apreserve%20key%20clinical%20characteristics%2C%20including%20hazard%20ratios%20and%20survival%0Acurves.%20CK4Gen%20avoids%20the%20interpolation%20issues%20seen%20in%20VAEs%20and%20GANs%20by%0Amaintaining%20distinct%20patient%20risk%20profiles%2C%20ensuring%20realistic%20and%20reliable%0Aoutputs%20for%20research%20and%20educational%20use.%20Validated%20across%20four%20benchmark%0Adatasets%20--%20GBSG2%2C%20ACTG320%2C%20WHAS500%2C%20and%20FLChain%20--%20CK4Gen%20outperforms%0Acompeting%20techniques%20by%20better%20aligning%20real%20and%20synthetic%20data%2C%20enhancing%0Asurvival%20model%20performance%20in%20both%20discrimination%20and%20calibration%20via%20data%0Aaugmentation.%20As%20CK4Gen%20is%20scalable%20across%20clinical%20conditions%2C%20and%20with%20code%0Ato%20be%20made%20publicly%20available%2C%20future%20researchers%20can%20apply%20it%20to%20their%20own%0Adatasets%20to%20generate%20synthetic%20versions%20suitable%20for%20open%20sharing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCK4Gen%253A%2520A%2520Knowledge%2520Distillation%2520Framework%2520for%2520Generating%2520High-Utility%250A%2520%2520Synthetic%2520Survival%2520Datasets%2520in%2520Healthcare%26entry.906535625%3DNicholas%2520I-Hsien%2520Kuo%2520and%2520Blanca%2520Gallego%2520and%2520Louisa%2520Jorm%26entry.1292438233%3D%2520%2520Access%2520to%2520real%2520clinical%2520data%2520is%2520heavily%2520restricted%2520by%2520privacy%2520regulations%252C%250Ahindering%2520both%2520healthcare%2520research%2520and%2520education.%2520These%2520constraints%2520slow%250Aprogress%2520in%2520developing%2520new%2520treatments%2520and%2520data-driven%2520healthcare%2520solutions%252C%250Awhile%2520also%2520limiting%2520students%2527%2520access%2520to%2520real-world%2520datasets%252C%2520leaving%2520them%250Awithout%2520essential%2520practical%2520skills.%2520High-utility%2520synthetic%2520datasets%2520are%250Atherefore%2520critical%2520for%2520advancing%2520research%2520and%2520providing%2520meaningful%2520training%250Amaterial.%2520However%252C%2520current%2520generative%2520models%2520--%2520such%2520as%2520Variational%250AAutoencoders%2520%2528VAEs%2529%2520and%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520--%2520produce%250Asurface-level%2520realism%2520at%2520the%2520expense%2520of%2520healthcare%2520utility%252C%2520blending%2520distinct%250Apatient%2520profiles%2520and%2520producing%2520synthetic%2520data%2520of%2520limited%2520practical%2520relevance.%250ATo%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520CK4Gen%2520%2528Cox%2520Knowledge%2520for%250AGeneration%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520knowledge%2520distillation%2520from%2520Cox%250AProportional%2520Hazards%2520%2528CoxPH%2529%2520models%2520to%2520create%2520synthetic%2520survival%2520datasets%2520that%250Apreserve%2520key%2520clinical%2520characteristics%252C%2520including%2520hazard%2520ratios%2520and%2520survival%250Acurves.%2520CK4Gen%2520avoids%2520the%2520interpolation%2520issues%2520seen%2520in%2520VAEs%2520and%2520GANs%2520by%250Amaintaining%2520distinct%2520patient%2520risk%2520profiles%252C%2520ensuring%2520realistic%2520and%2520reliable%250Aoutputs%2520for%2520research%2520and%2520educational%2520use.%2520Validated%2520across%2520four%2520benchmark%250Adatasets%2520--%2520GBSG2%252C%2520ACTG320%252C%2520WHAS500%252C%2520and%2520FLChain%2520--%2520CK4Gen%2520outperforms%250Acompeting%2520techniques%2520by%2520better%2520aligning%2520real%2520and%2520synthetic%2520data%252C%2520enhancing%250Asurvival%2520model%2520performance%2520in%2520both%2520discrimination%2520and%2520calibration%2520via%2520data%250Aaugmentation.%2520As%2520CK4Gen%2520is%2520scalable%2520across%2520clinical%2520conditions%252C%2520and%2520with%2520code%250Ato%2520be%2520made%2520publicly%2520available%252C%2520future%2520researchers%2520can%2520apply%2520it%2520to%2520their%2520own%250Adatasets%2520to%2520generate%2520synthetic%2520versions%2520suitable%2520for%2520open%2520sharing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CK4Gen%3A%20A%20Knowledge%20Distillation%20Framework%20for%20Generating%20High-Utility%0A%20%20Synthetic%20Survival%20Datasets%20in%20Healthcare&entry.906535625=Nicholas%20I-Hsien%20Kuo%20and%20Blanca%20Gallego%20and%20Louisa%20Jorm&entry.1292438233=%20%20Access%20to%20real%20clinical%20data%20is%20heavily%20restricted%20by%20privacy%20regulations%2C%0Ahindering%20both%20healthcare%20research%20and%20education.%20These%20constraints%20slow%0Aprogress%20in%20developing%20new%20treatments%20and%20data-driven%20healthcare%20solutions%2C%0Awhile%20also%20limiting%20students%27%20access%20to%20real-world%20datasets%2C%20leaving%20them%0Awithout%20essential%20practical%20skills.%20High-utility%20synthetic%20datasets%20are%0Atherefore%20critical%20for%20advancing%20research%20and%20providing%20meaningful%20training%0Amaterial.%20However%2C%20current%20generative%20models%20--%20such%20as%20Variational%0AAutoencoders%20%28VAEs%29%20and%20Generative%20Adversarial%20Networks%20%28GANs%29%20--%20produce%0Asurface-level%20realism%20at%20the%20expense%20of%20healthcare%20utility%2C%20blending%20distinct%0Apatient%20profiles%20and%20producing%20synthetic%20data%20of%20limited%20practical%20relevance.%0ATo%20overcome%20these%20limitations%2C%20we%20introduce%20CK4Gen%20%28Cox%20Knowledge%20for%0AGeneration%29%2C%20a%20novel%20framework%20that%20leverages%20knowledge%20distillation%20from%20Cox%0AProportional%20Hazards%20%28CoxPH%29%20models%20to%20create%20synthetic%20survival%20datasets%20that%0Apreserve%20key%20clinical%20characteristics%2C%20including%20hazard%20ratios%20and%20survival%0Acurves.%20CK4Gen%20avoids%20the%20interpolation%20issues%20seen%20in%20VAEs%20and%20GANs%20by%0Amaintaining%20distinct%20patient%20risk%20profiles%2C%20ensuring%20realistic%20and%20reliable%0Aoutputs%20for%20research%20and%20educational%20use.%20Validated%20across%20four%20benchmark%0Adatasets%20--%20GBSG2%2C%20ACTG320%2C%20WHAS500%2C%20and%20FLChain%20--%20CK4Gen%20outperforms%0Acompeting%20techniques%20by%20better%20aligning%20real%20and%20synthetic%20data%2C%20enhancing%0Asurvival%20model%20performance%20in%20both%20discrimination%20and%20calibration%20via%20data%0Aaugmentation.%20As%20CK4Gen%20is%20scalable%20across%20clinical%20conditions%2C%20and%20with%20code%0Ato%20be%20made%20publicly%20available%2C%20future%20researchers%20can%20apply%20it%20to%20their%20own%0Adatasets%20to%20generate%20synthetic%20versions%20suitable%20for%20open%20sharing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16872v1&entry.124074799=Read"},
{"title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic\n  Entropy", "author": "Benedict Aaron Tjandra and Muhammed Razzak and Jannik Kossen and Kunal Handa and Yarin Gal", "abstract": "  Large Language Models (LLMs) are known to hallucinate, whereby they generate\nplausible but inaccurate text. This phenomenon poses significant risks in\ncritical applications, such as medicine or law, necessitating robust\nhallucination mitigation strategies. While recent works have proposed\nfine-tuning methods to teach LLMs to abstain from answering questions beyond\ntheir knowledge or capabilities, these methods rely on the existence of\nground-truth labels or are limited to short-form responses. To address these\nlimitations, we propose fine-tuning using semantic entropy, an uncertainty\nmeasure derived from introspection into the model which does not require\nexternal labels. We demonstrate that our approach matches or outperforms models\nfine-tuned using prior work and achieves strong performance for both short and\nlong-form generations on a range of datasets.\n", "link": "http://arxiv.org/abs/2410.17234v1", "date": "2024-10-22", "relevancy": 2.0177, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20Large%20Language%20Models%20to%20Appropriately%20Abstain%20with%20Semantic%0A%20%20Entropy&body=Title%3A%20Fine-Tuning%20Large%20Language%20Models%20to%20Appropriately%20Abstain%20with%20Semantic%0A%20%20Entropy%0AAuthor%3A%20Benedict%20Aaron%20Tjandra%20and%20Muhammed%20Razzak%20and%20Jannik%20Kossen%20and%20Kunal%20Handa%20and%20Yarin%20Gal%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20known%20to%20hallucinate%2C%20whereby%20they%20generate%0Aplausible%20but%20inaccurate%20text.%20This%20phenomenon%20poses%20significant%20risks%20in%0Acritical%20applications%2C%20such%20as%20medicine%20or%20law%2C%20necessitating%20robust%0Ahallucination%20mitigation%20strategies.%20While%20recent%20works%20have%20proposed%0Afine-tuning%20methods%20to%20teach%20LLMs%20to%20abstain%20from%20answering%20questions%20beyond%0Atheir%20knowledge%20or%20capabilities%2C%20these%20methods%20rely%20on%20the%20existence%20of%0Aground-truth%20labels%20or%20are%20limited%20to%20short-form%20responses.%20To%20address%20these%0Alimitations%2C%20we%20propose%20fine-tuning%20using%20semantic%20entropy%2C%20an%20uncertainty%0Ameasure%20derived%20from%20introspection%20into%20the%20model%20which%20does%20not%20require%0Aexternal%20labels.%20We%20demonstrate%20that%20our%20approach%20matches%20or%20outperforms%20models%0Afine-tuned%20using%20prior%20work%20and%20achieves%20strong%20performance%20for%20both%20short%20and%0Along-form%20generations%20on%20a%20range%20of%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520Large%2520Language%2520Models%2520to%2520Appropriately%2520Abstain%2520with%2520Semantic%250A%2520%2520Entropy%26entry.906535625%3DBenedict%2520Aaron%2520Tjandra%2520and%2520Muhammed%2520Razzak%2520and%2520Jannik%2520Kossen%2520and%2520Kunal%2520Handa%2520and%2520Yarin%2520Gal%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520known%2520to%2520hallucinate%252C%2520whereby%2520they%2520generate%250Aplausible%2520but%2520inaccurate%2520text.%2520This%2520phenomenon%2520poses%2520significant%2520risks%2520in%250Acritical%2520applications%252C%2520such%2520as%2520medicine%2520or%2520law%252C%2520necessitating%2520robust%250Ahallucination%2520mitigation%2520strategies.%2520While%2520recent%2520works%2520have%2520proposed%250Afine-tuning%2520methods%2520to%2520teach%2520LLMs%2520to%2520abstain%2520from%2520answering%2520questions%2520beyond%250Atheir%2520knowledge%2520or%2520capabilities%252C%2520these%2520methods%2520rely%2520on%2520the%2520existence%2520of%250Aground-truth%2520labels%2520or%2520are%2520limited%2520to%2520short-form%2520responses.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520fine-tuning%2520using%2520semantic%2520entropy%252C%2520an%2520uncertainty%250Ameasure%2520derived%2520from%2520introspection%2520into%2520the%2520model%2520which%2520does%2520not%2520require%250Aexternal%2520labels.%2520We%2520demonstrate%2520that%2520our%2520approach%2520matches%2520or%2520outperforms%2520models%250Afine-tuned%2520using%2520prior%2520work%2520and%2520achieves%2520strong%2520performance%2520for%2520both%2520short%2520and%250Along-form%2520generations%2520on%2520a%2520range%2520of%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20Large%20Language%20Models%20to%20Appropriately%20Abstain%20with%20Semantic%0A%20%20Entropy&entry.906535625=Benedict%20Aaron%20Tjandra%20and%20Muhammed%20Razzak%20and%20Jannik%20Kossen%20and%20Kunal%20Handa%20and%20Yarin%20Gal&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20known%20to%20hallucinate%2C%20whereby%20they%20generate%0Aplausible%20but%20inaccurate%20text.%20This%20phenomenon%20poses%20significant%20risks%20in%0Acritical%20applications%2C%20such%20as%20medicine%20or%20law%2C%20necessitating%20robust%0Ahallucination%20mitigation%20strategies.%20While%20recent%20works%20have%20proposed%0Afine-tuning%20methods%20to%20teach%20LLMs%20to%20abstain%20from%20answering%20questions%20beyond%0Atheir%20knowledge%20or%20capabilities%2C%20these%20methods%20rely%20on%20the%20existence%20of%0Aground-truth%20labels%20or%20are%20limited%20to%20short-form%20responses.%20To%20address%20these%0Alimitations%2C%20we%20propose%20fine-tuning%20using%20semantic%20entropy%2C%20an%20uncertainty%0Ameasure%20derived%20from%20introspection%20into%20the%20model%20which%20does%20not%20require%0Aexternal%20labels.%20We%20demonstrate%20that%20our%20approach%20matches%20or%20outperforms%20models%0Afine-tuned%20using%20prior%20work%20and%20achieves%20strong%20performance%20for%20both%20short%20and%0Along-form%20generations%20on%20a%20range%20of%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17234v1&entry.124074799=Read"},
{"title": "Mitral Regurgitation Recognition based on Unsupervised\n  Out-of-Distribution Detection with Residual Diffusion Amplification", "author": "Zhe Liu and Xiliang Zhu and Tong Han and Yuhao Huang and Jian Wang and Lian Liu and Fang Wang and Dong Ni and Zhongshan Gou and Xin Yang", "abstract": "  Mitral regurgitation (MR) is a serious heart valve disease. Early and\naccurate diagnosis of MR via ultrasound video is critical for timely clinical\ndecision-making and surgical intervention. However, manual MR diagnosis heavily\nrelies on the operator's experience, which may cause misdiagnosis and\ninter-observer variability. Since MR data is limited and has large intra-class\nvariability, we propose an unsupervised out-of-distribution (OOD) detection\nmethod to identify MR rather than building a deep classifier. To our knowledge,\nwe are the first to explore OOD in MR ultrasound videos. Our method consists of\na feature extractor, a feature reconstruction model, and a residual\naccumulation amplification algorithm. The feature extractor obtains features\nfrom the video clips and feeds them into the feature reconstruction model to\nrestore the original features. The residual accumulation amplification\nalgorithm then iteratively performs noise feature reconstruction, amplifying\nthe reconstructed error of OOD features. This algorithm is straightforward yet\nefficient and can seamlessly integrate as a plug-and-play component in\nreconstruction-based OOD detection methods. We validated the proposed method on\na large ultrasound dataset containing 893 non-MR and 267 MR videos.\nExperimental results show that our OOD detection method can effectively\nidentify MR samples.\n", "link": "http://arxiv.org/abs/2407.21497v3", "date": "2024-10-22", "relevancy": 2.0162, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5535}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5025}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitral%20Regurgitation%20Recognition%20based%20on%20Unsupervised%0A%20%20Out-of-Distribution%20Detection%20with%20Residual%20Diffusion%20Amplification&body=Title%3A%20Mitral%20Regurgitation%20Recognition%20based%20on%20Unsupervised%0A%20%20Out-of-Distribution%20Detection%20with%20Residual%20Diffusion%20Amplification%0AAuthor%3A%20Zhe%20Liu%20and%20Xiliang%20Zhu%20and%20Tong%20Han%20and%20Yuhao%20Huang%20and%20Jian%20Wang%20and%20Lian%20Liu%20and%20Fang%20Wang%20and%20Dong%20Ni%20and%20Zhongshan%20Gou%20and%20Xin%20Yang%0AAbstract%3A%20%20%20Mitral%20regurgitation%20%28MR%29%20is%20a%20serious%20heart%20valve%20disease.%20Early%20and%0Aaccurate%20diagnosis%20of%20MR%20via%20ultrasound%20video%20is%20critical%20for%20timely%20clinical%0Adecision-making%20and%20surgical%20intervention.%20However%2C%20manual%20MR%20diagnosis%20heavily%0Arelies%20on%20the%20operator%27s%20experience%2C%20which%20may%20cause%20misdiagnosis%20and%0Ainter-observer%20variability.%20Since%20MR%20data%20is%20limited%20and%20has%20large%20intra-class%0Avariability%2C%20we%20propose%20an%20unsupervised%20out-of-distribution%20%28OOD%29%20detection%0Amethod%20to%20identify%20MR%20rather%20than%20building%20a%20deep%20classifier.%20To%20our%20knowledge%2C%0Awe%20are%20the%20first%20to%20explore%20OOD%20in%20MR%20ultrasound%20videos.%20Our%20method%20consists%20of%0Aa%20feature%20extractor%2C%20a%20feature%20reconstruction%20model%2C%20and%20a%20residual%0Aaccumulation%20amplification%20algorithm.%20The%20feature%20extractor%20obtains%20features%0Afrom%20the%20video%20clips%20and%20feeds%20them%20into%20the%20feature%20reconstruction%20model%20to%0Arestore%20the%20original%20features.%20The%20residual%20accumulation%20amplification%0Aalgorithm%20then%20iteratively%20performs%20noise%20feature%20reconstruction%2C%20amplifying%0Athe%20reconstructed%20error%20of%20OOD%20features.%20This%20algorithm%20is%20straightforward%20yet%0Aefficient%20and%20can%20seamlessly%20integrate%20as%20a%20plug-and-play%20component%20in%0Areconstruction-based%20OOD%20detection%20methods.%20We%20validated%20the%20proposed%20method%20on%0Aa%20large%20ultrasound%20dataset%20containing%20893%20non-MR%20and%20267%20MR%20videos.%0AExperimental%20results%20show%20that%20our%20OOD%20detection%20method%20can%20effectively%0Aidentify%20MR%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21497v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitral%2520Regurgitation%2520Recognition%2520based%2520on%2520Unsupervised%250A%2520%2520Out-of-Distribution%2520Detection%2520with%2520Residual%2520Diffusion%2520Amplification%26entry.906535625%3DZhe%2520Liu%2520and%2520Xiliang%2520Zhu%2520and%2520Tong%2520Han%2520and%2520Yuhao%2520Huang%2520and%2520Jian%2520Wang%2520and%2520Lian%2520Liu%2520and%2520Fang%2520Wang%2520and%2520Dong%2520Ni%2520and%2520Zhongshan%2520Gou%2520and%2520Xin%2520Yang%26entry.1292438233%3D%2520%2520Mitral%2520regurgitation%2520%2528MR%2529%2520is%2520a%2520serious%2520heart%2520valve%2520disease.%2520Early%2520and%250Aaccurate%2520diagnosis%2520of%2520MR%2520via%2520ultrasound%2520video%2520is%2520critical%2520for%2520timely%2520clinical%250Adecision-making%2520and%2520surgical%2520intervention.%2520However%252C%2520manual%2520MR%2520diagnosis%2520heavily%250Arelies%2520on%2520the%2520operator%2527s%2520experience%252C%2520which%2520may%2520cause%2520misdiagnosis%2520and%250Ainter-observer%2520variability.%2520Since%2520MR%2520data%2520is%2520limited%2520and%2520has%2520large%2520intra-class%250Avariability%252C%2520we%2520propose%2520an%2520unsupervised%2520out-of-distribution%2520%2528OOD%2529%2520detection%250Amethod%2520to%2520identify%2520MR%2520rather%2520than%2520building%2520a%2520deep%2520classifier.%2520To%2520our%2520knowledge%252C%250Awe%2520are%2520the%2520first%2520to%2520explore%2520OOD%2520in%2520MR%2520ultrasound%2520videos.%2520Our%2520method%2520consists%2520of%250Aa%2520feature%2520extractor%252C%2520a%2520feature%2520reconstruction%2520model%252C%2520and%2520a%2520residual%250Aaccumulation%2520amplification%2520algorithm.%2520The%2520feature%2520extractor%2520obtains%2520features%250Afrom%2520the%2520video%2520clips%2520and%2520feeds%2520them%2520into%2520the%2520feature%2520reconstruction%2520model%2520to%250Arestore%2520the%2520original%2520features.%2520The%2520residual%2520accumulation%2520amplification%250Aalgorithm%2520then%2520iteratively%2520performs%2520noise%2520feature%2520reconstruction%252C%2520amplifying%250Athe%2520reconstructed%2520error%2520of%2520OOD%2520features.%2520This%2520algorithm%2520is%2520straightforward%2520yet%250Aefficient%2520and%2520can%2520seamlessly%2520integrate%2520as%2520a%2520plug-and-play%2520component%2520in%250Areconstruction-based%2520OOD%2520detection%2520methods.%2520We%2520validated%2520the%2520proposed%2520method%2520on%250Aa%2520large%2520ultrasound%2520dataset%2520containing%2520893%2520non-MR%2520and%2520267%2520MR%2520videos.%250AExperimental%2520results%2520show%2520that%2520our%2520OOD%2520detection%2520method%2520can%2520effectively%250Aidentify%2520MR%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21497v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitral%20Regurgitation%20Recognition%20based%20on%20Unsupervised%0A%20%20Out-of-Distribution%20Detection%20with%20Residual%20Diffusion%20Amplification&entry.906535625=Zhe%20Liu%20and%20Xiliang%20Zhu%20and%20Tong%20Han%20and%20Yuhao%20Huang%20and%20Jian%20Wang%20and%20Lian%20Liu%20and%20Fang%20Wang%20and%20Dong%20Ni%20and%20Zhongshan%20Gou%20and%20Xin%20Yang&entry.1292438233=%20%20Mitral%20regurgitation%20%28MR%29%20is%20a%20serious%20heart%20valve%20disease.%20Early%20and%0Aaccurate%20diagnosis%20of%20MR%20via%20ultrasound%20video%20is%20critical%20for%20timely%20clinical%0Adecision-making%20and%20surgical%20intervention.%20However%2C%20manual%20MR%20diagnosis%20heavily%0Arelies%20on%20the%20operator%27s%20experience%2C%20which%20may%20cause%20misdiagnosis%20and%0Ainter-observer%20variability.%20Since%20MR%20data%20is%20limited%20and%20has%20large%20intra-class%0Avariability%2C%20we%20propose%20an%20unsupervised%20out-of-distribution%20%28OOD%29%20detection%0Amethod%20to%20identify%20MR%20rather%20than%20building%20a%20deep%20classifier.%20To%20our%20knowledge%2C%0Awe%20are%20the%20first%20to%20explore%20OOD%20in%20MR%20ultrasound%20videos.%20Our%20method%20consists%20of%0Aa%20feature%20extractor%2C%20a%20feature%20reconstruction%20model%2C%20and%20a%20residual%0Aaccumulation%20amplification%20algorithm.%20The%20feature%20extractor%20obtains%20features%0Afrom%20the%20video%20clips%20and%20feeds%20them%20into%20the%20feature%20reconstruction%20model%20to%0Arestore%20the%20original%20features.%20The%20residual%20accumulation%20amplification%0Aalgorithm%20then%20iteratively%20performs%20noise%20feature%20reconstruction%2C%20amplifying%0Athe%20reconstructed%20error%20of%20OOD%20features.%20This%20algorithm%20is%20straightforward%20yet%0Aefficient%20and%20can%20seamlessly%20integrate%20as%20a%20plug-and-play%20component%20in%0Areconstruction-based%20OOD%20detection%20methods.%20We%20validated%20the%20proposed%20method%20on%0Aa%20large%20ultrasound%20dataset%20containing%20893%20non-MR%20and%20267%20MR%20videos.%0AExperimental%20results%20show%20that%20our%20OOD%20detection%20method%20can%20effectively%0Aidentify%20MR%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21497v3&entry.124074799=Read"},
{"title": "Enhancing Generalization in Convolutional Neural Networks through\n  Regularization with Edge and Line Features", "author": "Christoph Linse and Beatrice Br\u00fcckner and Thomas Martinetz", "abstract": "  This paper proposes a novel regularization approach to bias Convolutional\nNeural Networks (CNNs) toward utilizing edge and line features in their hidden\nlayers. Rather than learning arbitrary kernels, we constrain the convolution\nlayers to edge and line detection kernels. This intentional bias regularizes\nthe models, improving generalization performance, especially on small datasets.\nAs a result, test accuracies improve by margins of 5-11 percentage points\nacross four challenging fine-grained classification datasets with limited\ntraining data and an identical number of trainable parameters. Instead of\ntraditional convolutional layers, we use Pre-defined Filter Modules, which\nconvolve input data using a fixed set of 3x3 pre-defined edge and line filters.\nA subsequent ReLU erases information that did not trigger any positive\nresponse. Next, a 1x1 convolutional layer generates linear combinations.\nNotably, the pre-defined filters are a fixed component of the architecture,\nremaining unchanged during the training phase. Our findings reveal that the\nnumber of dimensions spanned by the set of pre-defined filters has a low impact\non recognition performance. However, the size of the set of filters matters,\nwith nine or more filters providing optimal results.\n", "link": "http://arxiv.org/abs/2410.16897v1", "date": "2024-10-22", "relevancy": 2.0162, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5355}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4861}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Generalization%20in%20Convolutional%20Neural%20Networks%20through%0A%20%20Regularization%20with%20Edge%20and%20Line%20Features&body=Title%3A%20Enhancing%20Generalization%20in%20Convolutional%20Neural%20Networks%20through%0A%20%20Regularization%20with%20Edge%20and%20Line%20Features%0AAuthor%3A%20Christoph%20Linse%20and%20Beatrice%20Br%C3%BCckner%20and%20Thomas%20Martinetz%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20regularization%20approach%20to%20bias%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20toward%20utilizing%20edge%20and%20line%20features%20in%20their%20hidden%0Alayers.%20Rather%20than%20learning%20arbitrary%20kernels%2C%20we%20constrain%20the%20convolution%0Alayers%20to%20edge%20and%20line%20detection%20kernels.%20This%20intentional%20bias%20regularizes%0Athe%20models%2C%20improving%20generalization%20performance%2C%20especially%20on%20small%20datasets.%0AAs%20a%20result%2C%20test%20accuracies%20improve%20by%20margins%20of%205-11%20percentage%20points%0Aacross%20four%20challenging%20fine-grained%20classification%20datasets%20with%20limited%0Atraining%20data%20and%20an%20identical%20number%20of%20trainable%20parameters.%20Instead%20of%0Atraditional%20convolutional%20layers%2C%20we%20use%20Pre-defined%20Filter%20Modules%2C%20which%0Aconvolve%20input%20data%20using%20a%20fixed%20set%20of%203x3%20pre-defined%20edge%20and%20line%20filters.%0AA%20subsequent%20ReLU%20erases%20information%20that%20did%20not%20trigger%20any%20positive%0Aresponse.%20Next%2C%20a%201x1%20convolutional%20layer%20generates%20linear%20combinations.%0ANotably%2C%20the%20pre-defined%20filters%20are%20a%20fixed%20component%20of%20the%20architecture%2C%0Aremaining%20unchanged%20during%20the%20training%20phase.%20Our%20findings%20reveal%20that%20the%0Anumber%20of%20dimensions%20spanned%20by%20the%20set%20of%20pre-defined%20filters%20has%20a%20low%20impact%0Aon%20recognition%20performance.%20However%2C%20the%20size%20of%20the%20set%20of%20filters%20matters%2C%0Awith%20nine%20or%20more%20filters%20providing%20optimal%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Generalization%2520in%2520Convolutional%2520Neural%2520Networks%2520through%250A%2520%2520Regularization%2520with%2520Edge%2520and%2520Line%2520Features%26entry.906535625%3DChristoph%2520Linse%2520and%2520Beatrice%2520Br%25C3%25BCckner%2520and%2520Thomas%2520Martinetz%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520regularization%2520approach%2520to%2520bias%2520Convolutional%250ANeural%2520Networks%2520%2528CNNs%2529%2520toward%2520utilizing%2520edge%2520and%2520line%2520features%2520in%2520their%2520hidden%250Alayers.%2520Rather%2520than%2520learning%2520arbitrary%2520kernels%252C%2520we%2520constrain%2520the%2520convolution%250Alayers%2520to%2520edge%2520and%2520line%2520detection%2520kernels.%2520This%2520intentional%2520bias%2520regularizes%250Athe%2520models%252C%2520improving%2520generalization%2520performance%252C%2520especially%2520on%2520small%2520datasets.%250AAs%2520a%2520result%252C%2520test%2520accuracies%2520improve%2520by%2520margins%2520of%25205-11%2520percentage%2520points%250Aacross%2520four%2520challenging%2520fine-grained%2520classification%2520datasets%2520with%2520limited%250Atraining%2520data%2520and%2520an%2520identical%2520number%2520of%2520trainable%2520parameters.%2520Instead%2520of%250Atraditional%2520convolutional%2520layers%252C%2520we%2520use%2520Pre-defined%2520Filter%2520Modules%252C%2520which%250Aconvolve%2520input%2520data%2520using%2520a%2520fixed%2520set%2520of%25203x3%2520pre-defined%2520edge%2520and%2520line%2520filters.%250AA%2520subsequent%2520ReLU%2520erases%2520information%2520that%2520did%2520not%2520trigger%2520any%2520positive%250Aresponse.%2520Next%252C%2520a%25201x1%2520convolutional%2520layer%2520generates%2520linear%2520combinations.%250ANotably%252C%2520the%2520pre-defined%2520filters%2520are%2520a%2520fixed%2520component%2520of%2520the%2520architecture%252C%250Aremaining%2520unchanged%2520during%2520the%2520training%2520phase.%2520Our%2520findings%2520reveal%2520that%2520the%250Anumber%2520of%2520dimensions%2520spanned%2520by%2520the%2520set%2520of%2520pre-defined%2520filters%2520has%2520a%2520low%2520impact%250Aon%2520recognition%2520performance.%2520However%252C%2520the%2520size%2520of%2520the%2520set%2520of%2520filters%2520matters%252C%250Awith%2520nine%2520or%2520more%2520filters%2520providing%2520optimal%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Generalization%20in%20Convolutional%20Neural%20Networks%20through%0A%20%20Regularization%20with%20Edge%20and%20Line%20Features&entry.906535625=Christoph%20Linse%20and%20Beatrice%20Br%C3%BCckner%20and%20Thomas%20Martinetz&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20regularization%20approach%20to%20bias%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20toward%20utilizing%20edge%20and%20line%20features%20in%20their%20hidden%0Alayers.%20Rather%20than%20learning%20arbitrary%20kernels%2C%20we%20constrain%20the%20convolution%0Alayers%20to%20edge%20and%20line%20detection%20kernels.%20This%20intentional%20bias%20regularizes%0Athe%20models%2C%20improving%20generalization%20performance%2C%20especially%20on%20small%20datasets.%0AAs%20a%20result%2C%20test%20accuracies%20improve%20by%20margins%20of%205-11%20percentage%20points%0Aacross%20four%20challenging%20fine-grained%20classification%20datasets%20with%20limited%0Atraining%20data%20and%20an%20identical%20number%20of%20trainable%20parameters.%20Instead%20of%0Atraditional%20convolutional%20layers%2C%20we%20use%20Pre-defined%20Filter%20Modules%2C%20which%0Aconvolve%20input%20data%20using%20a%20fixed%20set%20of%203x3%20pre-defined%20edge%20and%20line%20filters.%0AA%20subsequent%20ReLU%20erases%20information%20that%20did%20not%20trigger%20any%20positive%0Aresponse.%20Next%2C%20a%201x1%20convolutional%20layer%20generates%20linear%20combinations.%0ANotably%2C%20the%20pre-defined%20filters%20are%20a%20fixed%20component%20of%20the%20architecture%2C%0Aremaining%20unchanged%20during%20the%20training%20phase.%20Our%20findings%20reveal%20that%20the%0Anumber%20of%20dimensions%20spanned%20by%20the%20set%20of%20pre-defined%20filters%20has%20a%20low%20impact%0Aon%20recognition%20performance.%20However%2C%20the%20size%20of%20the%20set%20of%20filters%20matters%2C%0Awith%20nine%20or%20more%20filters%20providing%20optimal%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16897v1&entry.124074799=Read"},
{"title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs", "author": "Yash Sinha and Murari Mandal and Mohan Kankanhalli", "abstract": "  The key components of machine learning are data samples for training, model\nfor learning patterns, and loss function for optimizing accuracy. Analogously,\nunlearning can potentially be achieved through anti-data samples (or\nanti-samples), unlearning method, and reversed loss function. While prior\nresearch has explored unlearning methods and reversed loss functions, the\npotential of anti-samples remains largely untapped. In this paper, we introduce\nUnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language\nmodels (LLMs). Our contributions are threefold; first, we propose a novel\nconcept of anti-sample-induced unlearning; second, we generate anti-samples by\nleveraging misleading rationales, which help reverse learned associations and\naccelerate the unlearning process; and third, we enable fine-grained targeted\nunlearning, allowing for the selective removal of specific associations without\nimpacting related knowledge - something not achievable by previous works.\nResults demonstrate that anti-samples offer an efficient, targeted unlearning\nstrategy for LLMs, opening new avenues for privacy-preserving machine learning\nand model modification.\n", "link": "http://arxiv.org/abs/2410.17050v1", "date": "2024-10-22", "relevancy": 2.0097, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5152}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.499}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnStar%3A%20Unlearning%20with%20Self-Taught%20Anti-Sample%20Reasoning%20for%20LLMs&body=Title%3A%20UnStar%3A%20Unlearning%20with%20Self-Taught%20Anti-Sample%20Reasoning%20for%20LLMs%0AAuthor%3A%20Yash%20Sinha%20and%20Murari%20Mandal%20and%20Mohan%20Kankanhalli%0AAbstract%3A%20%20%20The%20key%20components%20of%20machine%20learning%20are%20data%20samples%20for%20training%2C%20model%0Afor%20learning%20patterns%2C%20and%20loss%20function%20for%20optimizing%20accuracy.%20Analogously%2C%0Aunlearning%20can%20potentially%20be%20achieved%20through%20anti-data%20samples%20%28or%0Aanti-samples%29%2C%20unlearning%20method%2C%20and%20reversed%20loss%20function.%20While%20prior%0Aresearch%20has%20explored%20unlearning%20methods%20and%20reversed%20loss%20functions%2C%20the%0Apotential%20of%20anti-samples%20remains%20largely%20untapped.%20In%20this%20paper%2C%20we%20introduce%0AUnSTAR%3A%20Unlearning%20with%20Self-Taught%20Anti-Sample%20Reasoning%20for%20large%20language%0Amodels%20%28LLMs%29.%20Our%20contributions%20are%20threefold%3B%20first%2C%20we%20propose%20a%20novel%0Aconcept%20of%20anti-sample-induced%20unlearning%3B%20second%2C%20we%20generate%20anti-samples%20by%0Aleveraging%20misleading%20rationales%2C%20which%20help%20reverse%20learned%20associations%20and%0Aaccelerate%20the%20unlearning%20process%3B%20and%20third%2C%20we%20enable%20fine-grained%20targeted%0Aunlearning%2C%20allowing%20for%20the%20selective%20removal%20of%20specific%20associations%20without%0Aimpacting%20related%20knowledge%20-%20something%20not%20achievable%20by%20previous%20works.%0AResults%20demonstrate%20that%20anti-samples%20offer%20an%20efficient%2C%20targeted%20unlearning%0Astrategy%20for%20LLMs%2C%20opening%20new%20avenues%20for%20privacy-preserving%20machine%20learning%0Aand%20model%20modification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnStar%253A%2520Unlearning%2520with%2520Self-Taught%2520Anti-Sample%2520Reasoning%2520for%2520LLMs%26entry.906535625%3DYash%2520Sinha%2520and%2520Murari%2520Mandal%2520and%2520Mohan%2520Kankanhalli%26entry.1292438233%3D%2520%2520The%2520key%2520components%2520of%2520machine%2520learning%2520are%2520data%2520samples%2520for%2520training%252C%2520model%250Afor%2520learning%2520patterns%252C%2520and%2520loss%2520function%2520for%2520optimizing%2520accuracy.%2520Analogously%252C%250Aunlearning%2520can%2520potentially%2520be%2520achieved%2520through%2520anti-data%2520samples%2520%2528or%250Aanti-samples%2529%252C%2520unlearning%2520method%252C%2520and%2520reversed%2520loss%2520function.%2520While%2520prior%250Aresearch%2520has%2520explored%2520unlearning%2520methods%2520and%2520reversed%2520loss%2520functions%252C%2520the%250Apotential%2520of%2520anti-samples%2520remains%2520largely%2520untapped.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AUnSTAR%253A%2520Unlearning%2520with%2520Self-Taught%2520Anti-Sample%2520Reasoning%2520for%2520large%2520language%250Amodels%2520%2528LLMs%2529.%2520Our%2520contributions%2520are%2520threefold%253B%2520first%252C%2520we%2520propose%2520a%2520novel%250Aconcept%2520of%2520anti-sample-induced%2520unlearning%253B%2520second%252C%2520we%2520generate%2520anti-samples%2520by%250Aleveraging%2520misleading%2520rationales%252C%2520which%2520help%2520reverse%2520learned%2520associations%2520and%250Aaccelerate%2520the%2520unlearning%2520process%253B%2520and%2520third%252C%2520we%2520enable%2520fine-grained%2520targeted%250Aunlearning%252C%2520allowing%2520for%2520the%2520selective%2520removal%2520of%2520specific%2520associations%2520without%250Aimpacting%2520related%2520knowledge%2520-%2520something%2520not%2520achievable%2520by%2520previous%2520works.%250AResults%2520demonstrate%2520that%2520anti-samples%2520offer%2520an%2520efficient%252C%2520targeted%2520unlearning%250Astrategy%2520for%2520LLMs%252C%2520opening%2520new%2520avenues%2520for%2520privacy-preserving%2520machine%2520learning%250Aand%2520model%2520modification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnStar%3A%20Unlearning%20with%20Self-Taught%20Anti-Sample%20Reasoning%20for%20LLMs&entry.906535625=Yash%20Sinha%20and%20Murari%20Mandal%20and%20Mohan%20Kankanhalli&entry.1292438233=%20%20The%20key%20components%20of%20machine%20learning%20are%20data%20samples%20for%20training%2C%20model%0Afor%20learning%20patterns%2C%20and%20loss%20function%20for%20optimizing%20accuracy.%20Analogously%2C%0Aunlearning%20can%20potentially%20be%20achieved%20through%20anti-data%20samples%20%28or%0Aanti-samples%29%2C%20unlearning%20method%2C%20and%20reversed%20loss%20function.%20While%20prior%0Aresearch%20has%20explored%20unlearning%20methods%20and%20reversed%20loss%20functions%2C%20the%0Apotential%20of%20anti-samples%20remains%20largely%20untapped.%20In%20this%20paper%2C%20we%20introduce%0AUnSTAR%3A%20Unlearning%20with%20Self-Taught%20Anti-Sample%20Reasoning%20for%20large%20language%0Amodels%20%28LLMs%29.%20Our%20contributions%20are%20threefold%3B%20first%2C%20we%20propose%20a%20novel%0Aconcept%20of%20anti-sample-induced%20unlearning%3B%20second%2C%20we%20generate%20anti-samples%20by%0Aleveraging%20misleading%20rationales%2C%20which%20help%20reverse%20learned%20associations%20and%0Aaccelerate%20the%20unlearning%20process%3B%20and%20third%2C%20we%20enable%20fine-grained%20targeted%0Aunlearning%2C%20allowing%20for%20the%20selective%20removal%20of%20specific%20associations%20without%0Aimpacting%20related%20knowledge%20-%20something%20not%20achievable%20by%20previous%20works.%0AResults%20demonstrate%20that%20anti-samples%20offer%20an%20efficient%2C%20targeted%20unlearning%0Astrategy%20for%20LLMs%2C%20opening%20new%20avenues%20for%20privacy-preserving%20machine%20learning%0Aand%20model%20modification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17050v1&entry.124074799=Read"},
{"title": "Few-shot In-Context Preference Learning Using Large Language Models", "author": "Chao Yu and Hong Lu and Jiaxuan Gao and Qixin Tan and Xinting Yang and Yu Wang and Yi Wu and Eugene Vinitsky", "abstract": "  Designing reward functions is a core component of reinforcement learning but\ncan be challenging for truly complex behavior. Reinforcement Learning from\nHuman Feedback (RLHF) has been used to alleviate this challenge by replacing a\nhand-coded reward function with a reward function learned from preferences.\nHowever, it can be exceedingly inefficient to learn these rewards as they are\noften learned tabula rasa. We investigate whether Large Language Models (LLMs)\ncan reduce this query inefficiency by converting an iterative series of human\npreferences into code representing the rewards. We propose In-Context\nPreference Learning (ICPL), a method that uses the grounding of an LLM to\naccelerate learning reward functions from preferences. ICPL takes the\nenvironment context and task description, synthesizes a set of reward\nfunctions, and then repeatedly updates the reward functions using human\nrankings of videos of the resultant policies. Using synthetic preferences, we\ndemonstrate that ICPL is orders of magnitude more efficient than RLHF and is\neven competitive with methods that use ground-truth reward functions instead of\npreferences. Finally, we perform a series of human preference-learning trials\nand observe that ICPL extends beyond synthetic settings and can work\neffectively with humans-in-the-loop. Additional information and videos are\nprovided at https://sites.google.com/view/few-shot-icpl/home.\n", "link": "http://arxiv.org/abs/2410.17233v1", "date": "2024-10-22", "relevancy": 2.0039, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20In-Context%20Preference%20Learning%20Using%20Large%20Language%20Models&body=Title%3A%20Few-shot%20In-Context%20Preference%20Learning%20Using%20Large%20Language%20Models%0AAuthor%3A%20Chao%20Yu%20and%20Hong%20Lu%20and%20Jiaxuan%20Gao%20and%20Qixin%20Tan%20and%20Xinting%20Yang%20and%20Yu%20Wang%20and%20Yi%20Wu%20and%20Eugene%20Vinitsky%0AAbstract%3A%20%20%20Designing%20reward%20functions%20is%20a%20core%20component%20of%20reinforcement%20learning%20but%0Acan%20be%20challenging%20for%20truly%20complex%20behavior.%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20has%20been%20used%20to%20alleviate%20this%20challenge%20by%20replacing%20a%0Ahand-coded%20reward%20function%20with%20a%20reward%20function%20learned%20from%20preferences.%0AHowever%2C%20it%20can%20be%20exceedingly%20inefficient%20to%20learn%20these%20rewards%20as%20they%20are%0Aoften%20learned%20tabula%20rasa.%20We%20investigate%20whether%20Large%20Language%20Models%20%28LLMs%29%0Acan%20reduce%20this%20query%20inefficiency%20by%20converting%20an%20iterative%20series%20of%20human%0Apreferences%20into%20code%20representing%20the%20rewards.%20We%20propose%20In-Context%0APreference%20Learning%20%28ICPL%29%2C%20a%20method%20that%20uses%20the%20grounding%20of%20an%20LLM%20to%0Aaccelerate%20learning%20reward%20functions%20from%20preferences.%20ICPL%20takes%20the%0Aenvironment%20context%20and%20task%20description%2C%20synthesizes%20a%20set%20of%20reward%0Afunctions%2C%20and%20then%20repeatedly%20updates%20the%20reward%20functions%20using%20human%0Arankings%20of%20videos%20of%20the%20resultant%20policies.%20Using%20synthetic%20preferences%2C%20we%0Ademonstrate%20that%20ICPL%20is%20orders%20of%20magnitude%20more%20efficient%20than%20RLHF%20and%20is%0Aeven%20competitive%20with%20methods%20that%20use%20ground-truth%20reward%20functions%20instead%20of%0Apreferences.%20Finally%2C%20we%20perform%20a%20series%20of%20human%20preference-learning%20trials%0Aand%20observe%20that%20ICPL%20extends%20beyond%20synthetic%20settings%20and%20can%20work%0Aeffectively%20with%20humans-in-the-loop.%20Additional%20information%20and%20videos%20are%0Aprovided%20at%20https%3A//sites.google.com/view/few-shot-icpl/home.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520In-Context%2520Preference%2520Learning%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DChao%2520Yu%2520and%2520Hong%2520Lu%2520and%2520Jiaxuan%2520Gao%2520and%2520Qixin%2520Tan%2520and%2520Xinting%2520Yang%2520and%2520Yu%2520Wang%2520and%2520Yi%2520Wu%2520and%2520Eugene%2520Vinitsky%26entry.1292438233%3D%2520%2520Designing%2520reward%2520functions%2520is%2520a%2520core%2520component%2520of%2520reinforcement%2520learning%2520but%250Acan%2520be%2520challenging%2520for%2520truly%2520complex%2520behavior.%2520Reinforcement%2520Learning%2520from%250AHuman%2520Feedback%2520%2528RLHF%2529%2520has%2520been%2520used%2520to%2520alleviate%2520this%2520challenge%2520by%2520replacing%2520a%250Ahand-coded%2520reward%2520function%2520with%2520a%2520reward%2520function%2520learned%2520from%2520preferences.%250AHowever%252C%2520it%2520can%2520be%2520exceedingly%2520inefficient%2520to%2520learn%2520these%2520rewards%2520as%2520they%2520are%250Aoften%2520learned%2520tabula%2520rasa.%2520We%2520investigate%2520whether%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Acan%2520reduce%2520this%2520query%2520inefficiency%2520by%2520converting%2520an%2520iterative%2520series%2520of%2520human%250Apreferences%2520into%2520code%2520representing%2520the%2520rewards.%2520We%2520propose%2520In-Context%250APreference%2520Learning%2520%2528ICPL%2529%252C%2520a%2520method%2520that%2520uses%2520the%2520grounding%2520of%2520an%2520LLM%2520to%250Aaccelerate%2520learning%2520reward%2520functions%2520from%2520preferences.%2520ICPL%2520takes%2520the%250Aenvironment%2520context%2520and%2520task%2520description%252C%2520synthesizes%2520a%2520set%2520of%2520reward%250Afunctions%252C%2520and%2520then%2520repeatedly%2520updates%2520the%2520reward%2520functions%2520using%2520human%250Arankings%2520of%2520videos%2520of%2520the%2520resultant%2520policies.%2520Using%2520synthetic%2520preferences%252C%2520we%250Ademonstrate%2520that%2520ICPL%2520is%2520orders%2520of%2520magnitude%2520more%2520efficient%2520than%2520RLHF%2520and%2520is%250Aeven%2520competitive%2520with%2520methods%2520that%2520use%2520ground-truth%2520reward%2520functions%2520instead%2520of%250Apreferences.%2520Finally%252C%2520we%2520perform%2520a%2520series%2520of%2520human%2520preference-learning%2520trials%250Aand%2520observe%2520that%2520ICPL%2520extends%2520beyond%2520synthetic%2520settings%2520and%2520can%2520work%250Aeffectively%2520with%2520humans-in-the-loop.%2520Additional%2520information%2520and%2520videos%2520are%250Aprovided%2520at%2520https%253A//sites.google.com/view/few-shot-icpl/home.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20In-Context%20Preference%20Learning%20Using%20Large%20Language%20Models&entry.906535625=Chao%20Yu%20and%20Hong%20Lu%20and%20Jiaxuan%20Gao%20and%20Qixin%20Tan%20and%20Xinting%20Yang%20and%20Yu%20Wang%20and%20Yi%20Wu%20and%20Eugene%20Vinitsky&entry.1292438233=%20%20Designing%20reward%20functions%20is%20a%20core%20component%20of%20reinforcement%20learning%20but%0Acan%20be%20challenging%20for%20truly%20complex%20behavior.%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20has%20been%20used%20to%20alleviate%20this%20challenge%20by%20replacing%20a%0Ahand-coded%20reward%20function%20with%20a%20reward%20function%20learned%20from%20preferences.%0AHowever%2C%20it%20can%20be%20exceedingly%20inefficient%20to%20learn%20these%20rewards%20as%20they%20are%0Aoften%20learned%20tabula%20rasa.%20We%20investigate%20whether%20Large%20Language%20Models%20%28LLMs%29%0Acan%20reduce%20this%20query%20inefficiency%20by%20converting%20an%20iterative%20series%20of%20human%0Apreferences%20into%20code%20representing%20the%20rewards.%20We%20propose%20In-Context%0APreference%20Learning%20%28ICPL%29%2C%20a%20method%20that%20uses%20the%20grounding%20of%20an%20LLM%20to%0Aaccelerate%20learning%20reward%20functions%20from%20preferences.%20ICPL%20takes%20the%0Aenvironment%20context%20and%20task%20description%2C%20synthesizes%20a%20set%20of%20reward%0Afunctions%2C%20and%20then%20repeatedly%20updates%20the%20reward%20functions%20using%20human%0Arankings%20of%20videos%20of%20the%20resultant%20policies.%20Using%20synthetic%20preferences%2C%20we%0Ademonstrate%20that%20ICPL%20is%20orders%20of%20magnitude%20more%20efficient%20than%20RLHF%20and%20is%0Aeven%20competitive%20with%20methods%20that%20use%20ground-truth%20reward%20functions%20instead%20of%0Apreferences.%20Finally%2C%20we%20perform%20a%20series%20of%20human%20preference-learning%20trials%0Aand%20observe%20that%20ICPL%20extends%20beyond%20synthetic%20settings%20and%20can%20work%0Aeffectively%20with%20humans-in-the-loop.%20Additional%20information%20and%20videos%20are%0Aprovided%20at%20https%3A//sites.google.com/view/few-shot-icpl/home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17233v1&entry.124074799=Read"},
{"title": "Dynamic graph neural networks for enhanced volatility prediction in\n  financial markets", "author": "Pulikandala Nithish Kumar and Nneka Umeorah and Alex Alochukwu", "abstract": "  Volatility forecasting is essential for risk management and decision-making\nin financial markets. Traditional models like Generalized Autoregressive\nConditional Heteroskedasticity (GARCH) effectively capture volatility\nclustering but often fail to model complex, non-linear interdependencies\nbetween multiple indices. This paper proposes a novel approach using Graph\nNeural Networks (GNNs) to represent global financial markets as dynamic graphs.\nThe Temporal Graph Attention Network (Temporal GAT) combines Graph\nConvolutional Networks (GCNs) and Graph Attention Networks (GATs) to capture\nthe temporal and structural dynamics of volatility spillovers. By utilizing\ncorrelation-based and volatility spillover indices, the Temporal GAT constructs\ndirected graphs that enhance the accuracy of volatility predictions. Empirical\nresults from a 15-year study of eight major global indices show that the\nTemporal GAT outperforms traditional GARCH models and other machine learning\nmethods, particularly in short- to mid-term forecasts. The sensitivity and\nscenario-based analysis over a range of parameters and hyperparameters further\ndemonstrate the significance of the proposed technique. Hence, this work\nhighlights the potential of GNNs in modeling complex market behaviors,\nproviding valuable insights for financial analysts and investors.\n", "link": "http://arxiv.org/abs/2410.16858v1", "date": "2024-10-22", "relevancy": 1.9966, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.544}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4776}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20graph%20neural%20networks%20for%20enhanced%20volatility%20prediction%20in%0A%20%20financial%20markets&body=Title%3A%20Dynamic%20graph%20neural%20networks%20for%20enhanced%20volatility%20prediction%20in%0A%20%20financial%20markets%0AAuthor%3A%20Pulikandala%20Nithish%20Kumar%20and%20Nneka%20Umeorah%20and%20Alex%20Alochukwu%0AAbstract%3A%20%20%20Volatility%20forecasting%20is%20essential%20for%20risk%20management%20and%20decision-making%0Ain%20financial%20markets.%20Traditional%20models%20like%20Generalized%20Autoregressive%0AConditional%20Heteroskedasticity%20%28GARCH%29%20effectively%20capture%20volatility%0Aclustering%20but%20often%20fail%20to%20model%20complex%2C%20non-linear%20interdependencies%0Abetween%20multiple%20indices.%20This%20paper%20proposes%20a%20novel%20approach%20using%20Graph%0ANeural%20Networks%20%28GNNs%29%20to%20represent%20global%20financial%20markets%20as%20dynamic%20graphs.%0AThe%20Temporal%20Graph%20Attention%20Network%20%28Temporal%20GAT%29%20combines%20Graph%0AConvolutional%20Networks%20%28GCNs%29%20and%20Graph%20Attention%20Networks%20%28GATs%29%20to%20capture%0Athe%20temporal%20and%20structural%20dynamics%20of%20volatility%20spillovers.%20By%20utilizing%0Acorrelation-based%20and%20volatility%20spillover%20indices%2C%20the%20Temporal%20GAT%20constructs%0Adirected%20graphs%20that%20enhance%20the%20accuracy%20of%20volatility%20predictions.%20Empirical%0Aresults%20from%20a%2015-year%20study%20of%20eight%20major%20global%20indices%20show%20that%20the%0ATemporal%20GAT%20outperforms%20traditional%20GARCH%20models%20and%20other%20machine%20learning%0Amethods%2C%20particularly%20in%20short-%20to%20mid-term%20forecasts.%20The%20sensitivity%20and%0Ascenario-based%20analysis%20over%20a%20range%20of%20parameters%20and%20hyperparameters%20further%0Ademonstrate%20the%20significance%20of%20the%20proposed%20technique.%20Hence%2C%20this%20work%0Ahighlights%20the%20potential%20of%20GNNs%20in%20modeling%20complex%20market%20behaviors%2C%0Aproviding%20valuable%20insights%20for%20financial%20analysts%20and%20investors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520graph%2520neural%2520networks%2520for%2520enhanced%2520volatility%2520prediction%2520in%250A%2520%2520financial%2520markets%26entry.906535625%3DPulikandala%2520Nithish%2520Kumar%2520and%2520Nneka%2520Umeorah%2520and%2520Alex%2520Alochukwu%26entry.1292438233%3D%2520%2520Volatility%2520forecasting%2520is%2520essential%2520for%2520risk%2520management%2520and%2520decision-making%250Ain%2520financial%2520markets.%2520Traditional%2520models%2520like%2520Generalized%2520Autoregressive%250AConditional%2520Heteroskedasticity%2520%2528GARCH%2529%2520effectively%2520capture%2520volatility%250Aclustering%2520but%2520often%2520fail%2520to%2520model%2520complex%252C%2520non-linear%2520interdependencies%250Abetween%2520multiple%2520indices.%2520This%2520paper%2520proposes%2520a%2520novel%2520approach%2520using%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%2520to%2520represent%2520global%2520financial%2520markets%2520as%2520dynamic%2520graphs.%250AThe%2520Temporal%2520Graph%2520Attention%2520Network%2520%2528Temporal%2520GAT%2529%2520combines%2520Graph%250AConvolutional%2520Networks%2520%2528GCNs%2529%2520and%2520Graph%2520Attention%2520Networks%2520%2528GATs%2529%2520to%2520capture%250Athe%2520temporal%2520and%2520structural%2520dynamics%2520of%2520volatility%2520spillovers.%2520By%2520utilizing%250Acorrelation-based%2520and%2520volatility%2520spillover%2520indices%252C%2520the%2520Temporal%2520GAT%2520constructs%250Adirected%2520graphs%2520that%2520enhance%2520the%2520accuracy%2520of%2520volatility%2520predictions.%2520Empirical%250Aresults%2520from%2520a%252015-year%2520study%2520of%2520eight%2520major%2520global%2520indices%2520show%2520that%2520the%250ATemporal%2520GAT%2520outperforms%2520traditional%2520GARCH%2520models%2520and%2520other%2520machine%2520learning%250Amethods%252C%2520particularly%2520in%2520short-%2520to%2520mid-term%2520forecasts.%2520The%2520sensitivity%2520and%250Ascenario-based%2520analysis%2520over%2520a%2520range%2520of%2520parameters%2520and%2520hyperparameters%2520further%250Ademonstrate%2520the%2520significance%2520of%2520the%2520proposed%2520technique.%2520Hence%252C%2520this%2520work%250Ahighlights%2520the%2520potential%2520of%2520GNNs%2520in%2520modeling%2520complex%2520market%2520behaviors%252C%250Aproviding%2520valuable%2520insights%2520for%2520financial%2520analysts%2520and%2520investors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20graph%20neural%20networks%20for%20enhanced%20volatility%20prediction%20in%0A%20%20financial%20markets&entry.906535625=Pulikandala%20Nithish%20Kumar%20and%20Nneka%20Umeorah%20and%20Alex%20Alochukwu&entry.1292438233=%20%20Volatility%20forecasting%20is%20essential%20for%20risk%20management%20and%20decision-making%0Ain%20financial%20markets.%20Traditional%20models%20like%20Generalized%20Autoregressive%0AConditional%20Heteroskedasticity%20%28GARCH%29%20effectively%20capture%20volatility%0Aclustering%20but%20often%20fail%20to%20model%20complex%2C%20non-linear%20interdependencies%0Abetween%20multiple%20indices.%20This%20paper%20proposes%20a%20novel%20approach%20using%20Graph%0ANeural%20Networks%20%28GNNs%29%20to%20represent%20global%20financial%20markets%20as%20dynamic%20graphs.%0AThe%20Temporal%20Graph%20Attention%20Network%20%28Temporal%20GAT%29%20combines%20Graph%0AConvolutional%20Networks%20%28GCNs%29%20and%20Graph%20Attention%20Networks%20%28GATs%29%20to%20capture%0Athe%20temporal%20and%20structural%20dynamics%20of%20volatility%20spillovers.%20By%20utilizing%0Acorrelation-based%20and%20volatility%20spillover%20indices%2C%20the%20Temporal%20GAT%20constructs%0Adirected%20graphs%20that%20enhance%20the%20accuracy%20of%20volatility%20predictions.%20Empirical%0Aresults%20from%20a%2015-year%20study%20of%20eight%20major%20global%20indices%20show%20that%20the%0ATemporal%20GAT%20outperforms%20traditional%20GARCH%20models%20and%20other%20machine%20learning%0Amethods%2C%20particularly%20in%20short-%20to%20mid-term%20forecasts.%20The%20sensitivity%20and%0Ascenario-based%20analysis%20over%20a%20range%20of%20parameters%20and%20hyperparameters%20further%0Ademonstrate%20the%20significance%20of%20the%20proposed%20technique.%20Hence%2C%20this%20work%0Ahighlights%20the%20potential%20of%20GNNs%20in%20modeling%20complex%20market%20behaviors%2C%0Aproviding%20valuable%20insights%20for%20financial%20analysts%20and%20investors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16858v1&entry.124074799=Read"},
{"title": "Stacking Your Transformers: A Closer Look at Model Growth for Efficient\n  LLM Pre-Training", "author": "Wenyu Du and Tongxu Luo and Zihan Qiu and Zeyu Huang and Yikang Shen and Reynold Cheng and Yike Guo and Jie Fu", "abstract": "  LLMs are computationally expensive to pre-train due to their large scale.\nModel growth emerges as a promising approach by leveraging smaller models to\naccelerate the training of larger ones. However, the viability of these model\ngrowth methods in efficient LLM pre-training remains underexplored. This work\nidentifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1)\nlack of comprehensive evaluation, ($\\textit{O}$2) untested viability for\nscaling, and ($\\textit{O}$3) lack of empirical guidelines. To tackle\n$\\textit{O}$1, we summarize existing approaches into four atomic growth\noperators and systematically evaluate them in a standardized LLM pre-training\nsetting. Our findings reveal that a depthwise stacking operator, called\n$G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to\ndecreased loss and improved overall performance on eight standard NLP\nbenchmarks compared to strong baselines. Motivated by these promising results,\nwe conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to\naddress $\\textit{O}$2 and $\\textit{O}$3. For $\\textit{O}$2 (untested\nscalability), our study shows that $G_{\\text{stack}}$ is scalable and\nconsistently performs well, with experiments up to 7B LLMs after growth and\npre-training LLMs with 750B tokens. For example, compared to a conventionally\ntrained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to\nthe same loss with 194B tokens, resulting in a 54.6\\% speedup. We further\naddress $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines\nto determine growth timing and growth factor for $G_{\\text{stack}}$, making it\npractical in general LLM pre-training. We also provide in-depth discussions and\ncomprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained\nmodel are available at https://llm-stacking.github.io.\n", "link": "http://arxiv.org/abs/2405.15319v2", "date": "2024-10-22", "relevancy": 1.9905, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5193}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stacking%20Your%20Transformers%3A%20A%20Closer%20Look%20at%20Model%20Growth%20for%20Efficient%0A%20%20LLM%20Pre-Training&body=Title%3A%20Stacking%20Your%20Transformers%3A%20A%20Closer%20Look%20at%20Model%20Growth%20for%20Efficient%0A%20%20LLM%20Pre-Training%0AAuthor%3A%20Wenyu%20Du%20and%20Tongxu%20Luo%20and%20Zihan%20Qiu%20and%20Zeyu%20Huang%20and%20Yikang%20Shen%20and%20Reynold%20Cheng%20and%20Yike%20Guo%20and%20Jie%20Fu%0AAbstract%3A%20%20%20LLMs%20are%20computationally%20expensive%20to%20pre-train%20due%20to%20their%20large%20scale.%0AModel%20growth%20emerges%20as%20a%20promising%20approach%20by%20leveraging%20smaller%20models%20to%0Aaccelerate%20the%20training%20of%20larger%20ones.%20However%2C%20the%20viability%20of%20these%20model%0Agrowth%20methods%20in%20efficient%20LLM%20pre-training%20remains%20underexplored.%20This%20work%0Aidentifies%20three%20critical%20%24%5Cunderline%7B%5Ctextit%7BO%7D%7D%24bstacles%3A%20%28%24%5Ctextit%7BO%7D%241%29%0Alack%20of%20comprehensive%20evaluation%2C%20%28%24%5Ctextit%7BO%7D%242%29%20untested%20viability%20for%0Ascaling%2C%20and%20%28%24%5Ctextit%7BO%7D%243%29%20lack%20of%20empirical%20guidelines.%20To%20tackle%0A%24%5Ctextit%7BO%7D%241%2C%20we%20summarize%20existing%20approaches%20into%20four%20atomic%20growth%0Aoperators%20and%20systematically%20evaluate%20them%20in%20a%20standardized%20LLM%20pre-training%0Asetting.%20Our%20findings%20reveal%20that%20a%20depthwise%20stacking%20operator%2C%20called%0A%24G_%7B%5Ctext%7Bstack%7D%7D%24%2C%20exhibits%20remarkable%20acceleration%20in%20training%2C%20leading%20to%0Adecreased%20loss%20and%20improved%20overall%20performance%20on%20eight%20standard%20NLP%0Abenchmarks%20compared%20to%20strong%20baselines.%20Motivated%20by%20these%20promising%20results%2C%0Awe%20conduct%20extensive%20experiments%20to%20delve%20deeper%20into%20%24G_%7B%5Ctext%7Bstack%7D%7D%24%20to%0Aaddress%20%24%5Ctextit%7BO%7D%242%20and%20%24%5Ctextit%7BO%7D%243.%20For%20%24%5Ctextit%7BO%7D%242%20%28untested%0Ascalability%29%2C%20our%20study%20shows%20that%20%24G_%7B%5Ctext%7Bstack%7D%7D%24%20is%20scalable%20and%0Aconsistently%20performs%20well%2C%20with%20experiments%20up%20to%207B%20LLMs%20after%20growth%20and%0Apre-training%20LLMs%20with%20750B%20tokens.%20For%20example%2C%20compared%20to%20a%20conventionally%0Atrained%207B%20model%20using%20300B%20tokens%2C%20our%20%24G_%7B%5Ctext%7Bstack%7D%7D%24%20model%20converges%20to%0Athe%20same%20loss%20with%20194B%20tokens%2C%20resulting%20in%20a%2054.6%5C%25%20speedup.%20We%20further%0Aaddress%20%24%5Ctextit%7BO%7D%243%20%28lack%20of%20empirical%20guidelines%29%20by%20formalizing%20guidelines%0Ato%20determine%20growth%20timing%20and%20growth%20factor%20for%20%24G_%7B%5Ctext%7Bstack%7D%7D%24%2C%20making%20it%0Apractical%20in%20general%20LLM%20pre-training.%20We%20also%20provide%20in-depth%20discussions%20and%0Acomprehensive%20ablation%20studies%20of%20%24G_%7B%5Ctext%7Bstack%7D%7D%24.%20Our%20code%20and%20pre-trained%0Amodel%20are%20available%20at%20https%3A//llm-stacking.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStacking%2520Your%2520Transformers%253A%2520A%2520Closer%2520Look%2520at%2520Model%2520Growth%2520for%2520Efficient%250A%2520%2520LLM%2520Pre-Training%26entry.906535625%3DWenyu%2520Du%2520and%2520Tongxu%2520Luo%2520and%2520Zihan%2520Qiu%2520and%2520Zeyu%2520Huang%2520and%2520Yikang%2520Shen%2520and%2520Reynold%2520Cheng%2520and%2520Yike%2520Guo%2520and%2520Jie%2520Fu%26entry.1292438233%3D%2520%2520LLMs%2520are%2520computationally%2520expensive%2520to%2520pre-train%2520due%2520to%2520their%2520large%2520scale.%250AModel%2520growth%2520emerges%2520as%2520a%2520promising%2520approach%2520by%2520leveraging%2520smaller%2520models%2520to%250Aaccelerate%2520the%2520training%2520of%2520larger%2520ones.%2520However%252C%2520the%2520viability%2520of%2520these%2520model%250Agrowth%2520methods%2520in%2520efficient%2520LLM%2520pre-training%2520remains%2520underexplored.%2520This%2520work%250Aidentifies%2520three%2520critical%2520%2524%255Cunderline%257B%255Ctextit%257BO%257D%257D%2524bstacles%253A%2520%2528%2524%255Ctextit%257BO%257D%25241%2529%250Alack%2520of%2520comprehensive%2520evaluation%252C%2520%2528%2524%255Ctextit%257BO%257D%25242%2529%2520untested%2520viability%2520for%250Ascaling%252C%2520and%2520%2528%2524%255Ctextit%257BO%257D%25243%2529%2520lack%2520of%2520empirical%2520guidelines.%2520To%2520tackle%250A%2524%255Ctextit%257BO%257D%25241%252C%2520we%2520summarize%2520existing%2520approaches%2520into%2520four%2520atomic%2520growth%250Aoperators%2520and%2520systematically%2520evaluate%2520them%2520in%2520a%2520standardized%2520LLM%2520pre-training%250Asetting.%2520Our%2520findings%2520reveal%2520that%2520a%2520depthwise%2520stacking%2520operator%252C%2520called%250A%2524G_%257B%255Ctext%257Bstack%257D%257D%2524%252C%2520exhibits%2520remarkable%2520acceleration%2520in%2520training%252C%2520leading%2520to%250Adecreased%2520loss%2520and%2520improved%2520overall%2520performance%2520on%2520eight%2520standard%2520NLP%250Abenchmarks%2520compared%2520to%2520strong%2520baselines.%2520Motivated%2520by%2520these%2520promising%2520results%252C%250Awe%2520conduct%2520extensive%2520experiments%2520to%2520delve%2520deeper%2520into%2520%2524G_%257B%255Ctext%257Bstack%257D%257D%2524%2520to%250Aaddress%2520%2524%255Ctextit%257BO%257D%25242%2520and%2520%2524%255Ctextit%257BO%257D%25243.%2520For%2520%2524%255Ctextit%257BO%257D%25242%2520%2528untested%250Ascalability%2529%252C%2520our%2520study%2520shows%2520that%2520%2524G_%257B%255Ctext%257Bstack%257D%257D%2524%2520is%2520scalable%2520and%250Aconsistently%2520performs%2520well%252C%2520with%2520experiments%2520up%2520to%25207B%2520LLMs%2520after%2520growth%2520and%250Apre-training%2520LLMs%2520with%2520750B%2520tokens.%2520For%2520example%252C%2520compared%2520to%2520a%2520conventionally%250Atrained%25207B%2520model%2520using%2520300B%2520tokens%252C%2520our%2520%2524G_%257B%255Ctext%257Bstack%257D%257D%2524%2520model%2520converges%2520to%250Athe%2520same%2520loss%2520with%2520194B%2520tokens%252C%2520resulting%2520in%2520a%252054.6%255C%2525%2520speedup.%2520We%2520further%250Aaddress%2520%2524%255Ctextit%257BO%257D%25243%2520%2528lack%2520of%2520empirical%2520guidelines%2529%2520by%2520formalizing%2520guidelines%250Ato%2520determine%2520growth%2520timing%2520and%2520growth%2520factor%2520for%2520%2524G_%257B%255Ctext%257Bstack%257D%257D%2524%252C%2520making%2520it%250Apractical%2520in%2520general%2520LLM%2520pre-training.%2520We%2520also%2520provide%2520in-depth%2520discussions%2520and%250Acomprehensive%2520ablation%2520studies%2520of%2520%2524G_%257B%255Ctext%257Bstack%257D%257D%2524.%2520Our%2520code%2520and%2520pre-trained%250Amodel%2520are%2520available%2520at%2520https%253A//llm-stacking.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stacking%20Your%20Transformers%3A%20A%20Closer%20Look%20at%20Model%20Growth%20for%20Efficient%0A%20%20LLM%20Pre-Training&entry.906535625=Wenyu%20Du%20and%20Tongxu%20Luo%20and%20Zihan%20Qiu%20and%20Zeyu%20Huang%20and%20Yikang%20Shen%20and%20Reynold%20Cheng%20and%20Yike%20Guo%20and%20Jie%20Fu&entry.1292438233=%20%20LLMs%20are%20computationally%20expensive%20to%20pre-train%20due%20to%20their%20large%20scale.%0AModel%20growth%20emerges%20as%20a%20promising%20approach%20by%20leveraging%20smaller%20models%20to%0Aaccelerate%20the%20training%20of%20larger%20ones.%20However%2C%20the%20viability%20of%20these%20model%0Agrowth%20methods%20in%20efficient%20LLM%20pre-training%20remains%20underexplored.%20This%20work%0Aidentifies%20three%20critical%20%24%5Cunderline%7B%5Ctextit%7BO%7D%7D%24bstacles%3A%20%28%24%5Ctextit%7BO%7D%241%29%0Alack%20of%20comprehensive%20evaluation%2C%20%28%24%5Ctextit%7BO%7D%242%29%20untested%20viability%20for%0Ascaling%2C%20and%20%28%24%5Ctextit%7BO%7D%243%29%20lack%20of%20empirical%20guidelines.%20To%20tackle%0A%24%5Ctextit%7BO%7D%241%2C%20we%20summarize%20existing%20approaches%20into%20four%20atomic%20growth%0Aoperators%20and%20systematically%20evaluate%20them%20in%20a%20standardized%20LLM%20pre-training%0Asetting.%20Our%20findings%20reveal%20that%20a%20depthwise%20stacking%20operator%2C%20called%0A%24G_%7B%5Ctext%7Bstack%7D%7D%24%2C%20exhibits%20remarkable%20acceleration%20in%20training%2C%20leading%20to%0Adecreased%20loss%20and%20improved%20overall%20performance%20on%20eight%20standard%20NLP%0Abenchmarks%20compared%20to%20strong%20baselines.%20Motivated%20by%20these%20promising%20results%2C%0Awe%20conduct%20extensive%20experiments%20to%20delve%20deeper%20into%20%24G_%7B%5Ctext%7Bstack%7D%7D%24%20to%0Aaddress%20%24%5Ctextit%7BO%7D%242%20and%20%24%5Ctextit%7BO%7D%243.%20For%20%24%5Ctextit%7BO%7D%242%20%28untested%0Ascalability%29%2C%20our%20study%20shows%20that%20%24G_%7B%5Ctext%7Bstack%7D%7D%24%20is%20scalable%20and%0Aconsistently%20performs%20well%2C%20with%20experiments%20up%20to%207B%20LLMs%20after%20growth%20and%0Apre-training%20LLMs%20with%20750B%20tokens.%20For%20example%2C%20compared%20to%20a%20conventionally%0Atrained%207B%20model%20using%20300B%20tokens%2C%20our%20%24G_%7B%5Ctext%7Bstack%7D%7D%24%20model%20converges%20to%0Athe%20same%20loss%20with%20194B%20tokens%2C%20resulting%20in%20a%2054.6%5C%25%20speedup.%20We%20further%0Aaddress%20%24%5Ctextit%7BO%7D%243%20%28lack%20of%20empirical%20guidelines%29%20by%20formalizing%20guidelines%0Ato%20determine%20growth%20timing%20and%20growth%20factor%20for%20%24G_%7B%5Ctext%7Bstack%7D%7D%24%2C%20making%20it%0Apractical%20in%20general%20LLM%20pre-training.%20We%20also%20provide%20in-depth%20discussions%20and%0Acomprehensive%20ablation%20studies%20of%20%24G_%7B%5Ctext%7Bstack%7D%7D%24.%20Our%20code%20and%20pre-trained%0Amodel%20are%20available%20at%20https%3A//llm-stacking.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15319v2&entry.124074799=Read"},
{"title": "Pyramid Vector Quantization for LLMs", "author": "Tycho F. A. van der Ouderaa and Maximilian L. Croci and Agrin Hilmkil and James Hensman", "abstract": "  Recent works on compression of large language models (LLM) using quantization\nconsidered reparameterizing the architecture such that weights are distributed\non the sphere. This demonstratively improves the ability to quantize by\nincreasing the mathematical notion of coherence, resulting in fewer weight\noutliers without affecting the network output. In this work, we aim to further\nexploit this spherical geometry of the weights when performing quantization by\nconsidering Pyramid Vector Quantization (PVQ) for large language models.\nArranging points evenly on the sphere is notoriously difficult, especially in\nhigh dimensions, and in case approximate solutions exists, representing points\nexplicitly in a codebook is typically not feasible due to its additional memory\ncost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting\npoints onto the 1-sphere, which allows for efficient encoding and decoding\nwithout requiring an explicit codebook in memory. To obtain a practical\nalgorithm, we propose to combine PVQ with scale quantization for which we\nderive theoretically optimal quantizations, under empirically verified\nassumptions. Further, we extend pyramid vector quantization to use Hessian\ninformation to minimize quantization error under expected feature activations,\ninstead of only relying on weight magnitudes. Experimentally, we achieves\nstate-of-the-art quantization performance with pareto-optimal trade-off between\nperformance and bits per weight and bits per activation, compared to compared\nmethods. On weight-only, we find that we can quantize a Llama-3 70B model to\n3.25 bits per weight and retain 98\\% accuracy on downstream tasks.\n", "link": "http://arxiv.org/abs/2410.16926v1", "date": "2024-10-22", "relevancy": 1.9872, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pyramid%20Vector%20Quantization%20for%20LLMs&body=Title%3A%20Pyramid%20Vector%20Quantization%20for%20LLMs%0AAuthor%3A%20Tycho%20F.%20A.%20van%20der%20Ouderaa%20and%20Maximilian%20L.%20Croci%20and%20Agrin%20Hilmkil%20and%20James%20Hensman%0AAbstract%3A%20%20%20Recent%20works%20on%20compression%20of%20large%20language%20models%20%28LLM%29%20using%20quantization%0Aconsidered%20reparameterizing%20the%20architecture%20such%20that%20weights%20are%20distributed%0Aon%20the%20sphere.%20This%20demonstratively%20improves%20the%20ability%20to%20quantize%20by%0Aincreasing%20the%20mathematical%20notion%20of%20coherence%2C%20resulting%20in%20fewer%20weight%0Aoutliers%20without%20affecting%20the%20network%20output.%20In%20this%20work%2C%20we%20aim%20to%20further%0Aexploit%20this%20spherical%20geometry%20of%20the%20weights%20when%20performing%20quantization%20by%0Aconsidering%20Pyramid%20Vector%20Quantization%20%28PVQ%29%20for%20large%20language%20models.%0AArranging%20points%20evenly%20on%20the%20sphere%20is%20notoriously%20difficult%2C%20especially%20in%0Ahigh%20dimensions%2C%20and%20in%20case%20approximate%20solutions%20exists%2C%20representing%20points%0Aexplicitly%20in%20a%20codebook%20is%20typically%20not%20feasible%20due%20to%20its%20additional%20memory%0Acost.%20Instead%2C%20PVQ%20uses%20a%20fixed%20integer%20lattice%20on%20the%20sphere%20by%20projecting%0Apoints%20onto%20the%201-sphere%2C%20which%20allows%20for%20efficient%20encoding%20and%20decoding%0Awithout%20requiring%20an%20explicit%20codebook%20in%20memory.%20To%20obtain%20a%20practical%0Aalgorithm%2C%20we%20propose%20to%20combine%20PVQ%20with%20scale%20quantization%20for%20which%20we%0Aderive%20theoretically%20optimal%20quantizations%2C%20under%20empirically%20verified%0Aassumptions.%20Further%2C%20we%20extend%20pyramid%20vector%20quantization%20to%20use%20Hessian%0Ainformation%20to%20minimize%20quantization%20error%20under%20expected%20feature%20activations%2C%0Ainstead%20of%20only%20relying%20on%20weight%20magnitudes.%20Experimentally%2C%20we%20achieves%0Astate-of-the-art%20quantization%20performance%20with%20pareto-optimal%20trade-off%20between%0Aperformance%20and%20bits%20per%20weight%20and%20bits%20per%20activation%2C%20compared%20to%20compared%0Amethods.%20On%20weight-only%2C%20we%20find%20that%20we%20can%20quantize%20a%20Llama-3%2070B%20model%20to%0A3.25%20bits%20per%20weight%20and%20retain%2098%5C%25%20accuracy%20on%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyramid%2520Vector%2520Quantization%2520for%2520LLMs%26entry.906535625%3DTycho%2520F.%2520A.%2520van%2520der%2520Ouderaa%2520and%2520Maximilian%2520L.%2520Croci%2520and%2520Agrin%2520Hilmkil%2520and%2520James%2520Hensman%26entry.1292438233%3D%2520%2520Recent%2520works%2520on%2520compression%2520of%2520large%2520language%2520models%2520%2528LLM%2529%2520using%2520quantization%250Aconsidered%2520reparameterizing%2520the%2520architecture%2520such%2520that%2520weights%2520are%2520distributed%250Aon%2520the%2520sphere.%2520This%2520demonstratively%2520improves%2520the%2520ability%2520to%2520quantize%2520by%250Aincreasing%2520the%2520mathematical%2520notion%2520of%2520coherence%252C%2520resulting%2520in%2520fewer%2520weight%250Aoutliers%2520without%2520affecting%2520the%2520network%2520output.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520further%250Aexploit%2520this%2520spherical%2520geometry%2520of%2520the%2520weights%2520when%2520performing%2520quantization%2520by%250Aconsidering%2520Pyramid%2520Vector%2520Quantization%2520%2528PVQ%2529%2520for%2520large%2520language%2520models.%250AArranging%2520points%2520evenly%2520on%2520the%2520sphere%2520is%2520notoriously%2520difficult%252C%2520especially%2520in%250Ahigh%2520dimensions%252C%2520and%2520in%2520case%2520approximate%2520solutions%2520exists%252C%2520representing%2520points%250Aexplicitly%2520in%2520a%2520codebook%2520is%2520typically%2520not%2520feasible%2520due%2520to%2520its%2520additional%2520memory%250Acost.%2520Instead%252C%2520PVQ%2520uses%2520a%2520fixed%2520integer%2520lattice%2520on%2520the%2520sphere%2520by%2520projecting%250Apoints%2520onto%2520the%25201-sphere%252C%2520which%2520allows%2520for%2520efficient%2520encoding%2520and%2520decoding%250Awithout%2520requiring%2520an%2520explicit%2520codebook%2520in%2520memory.%2520To%2520obtain%2520a%2520practical%250Aalgorithm%252C%2520we%2520propose%2520to%2520combine%2520PVQ%2520with%2520scale%2520quantization%2520for%2520which%2520we%250Aderive%2520theoretically%2520optimal%2520quantizations%252C%2520under%2520empirically%2520verified%250Aassumptions.%2520Further%252C%2520we%2520extend%2520pyramid%2520vector%2520quantization%2520to%2520use%2520Hessian%250Ainformation%2520to%2520minimize%2520quantization%2520error%2520under%2520expected%2520feature%2520activations%252C%250Ainstead%2520of%2520only%2520relying%2520on%2520weight%2520magnitudes.%2520Experimentally%252C%2520we%2520achieves%250Astate-of-the-art%2520quantization%2520performance%2520with%2520pareto-optimal%2520trade-off%2520between%250Aperformance%2520and%2520bits%2520per%2520weight%2520and%2520bits%2520per%2520activation%252C%2520compared%2520to%2520compared%250Amethods.%2520On%2520weight-only%252C%2520we%2520find%2520that%2520we%2520can%2520quantize%2520a%2520Llama-3%252070B%2520model%2520to%250A3.25%2520bits%2520per%2520weight%2520and%2520retain%252098%255C%2525%2520accuracy%2520on%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pyramid%20Vector%20Quantization%20for%20LLMs&entry.906535625=Tycho%20F.%20A.%20van%20der%20Ouderaa%20and%20Maximilian%20L.%20Croci%20and%20Agrin%20Hilmkil%20and%20James%20Hensman&entry.1292438233=%20%20Recent%20works%20on%20compression%20of%20large%20language%20models%20%28LLM%29%20using%20quantization%0Aconsidered%20reparameterizing%20the%20architecture%20such%20that%20weights%20are%20distributed%0Aon%20the%20sphere.%20This%20demonstratively%20improves%20the%20ability%20to%20quantize%20by%0Aincreasing%20the%20mathematical%20notion%20of%20coherence%2C%20resulting%20in%20fewer%20weight%0Aoutliers%20without%20affecting%20the%20network%20output.%20In%20this%20work%2C%20we%20aim%20to%20further%0Aexploit%20this%20spherical%20geometry%20of%20the%20weights%20when%20performing%20quantization%20by%0Aconsidering%20Pyramid%20Vector%20Quantization%20%28PVQ%29%20for%20large%20language%20models.%0AArranging%20points%20evenly%20on%20the%20sphere%20is%20notoriously%20difficult%2C%20especially%20in%0Ahigh%20dimensions%2C%20and%20in%20case%20approximate%20solutions%20exists%2C%20representing%20points%0Aexplicitly%20in%20a%20codebook%20is%20typically%20not%20feasible%20due%20to%20its%20additional%20memory%0Acost.%20Instead%2C%20PVQ%20uses%20a%20fixed%20integer%20lattice%20on%20the%20sphere%20by%20projecting%0Apoints%20onto%20the%201-sphere%2C%20which%20allows%20for%20efficient%20encoding%20and%20decoding%0Awithout%20requiring%20an%20explicit%20codebook%20in%20memory.%20To%20obtain%20a%20practical%0Aalgorithm%2C%20we%20propose%20to%20combine%20PVQ%20with%20scale%20quantization%20for%20which%20we%0Aderive%20theoretically%20optimal%20quantizations%2C%20under%20empirically%20verified%0Aassumptions.%20Further%2C%20we%20extend%20pyramid%20vector%20quantization%20to%20use%20Hessian%0Ainformation%20to%20minimize%20quantization%20error%20under%20expected%20feature%20activations%2C%0Ainstead%20of%20only%20relying%20on%20weight%20magnitudes.%20Experimentally%2C%20we%20achieves%0Astate-of-the-art%20quantization%20performance%20with%20pareto-optimal%20trade-off%20between%0Aperformance%20and%20bits%20per%20weight%20and%20bits%20per%20activation%2C%20compared%20to%20compared%0Amethods.%20On%20weight-only%2C%20we%20find%20that%20we%20can%20quantize%20a%20Llama-3%2070B%20model%20to%0A3.25%20bits%20per%20weight%20and%20retain%2098%5C%25%20accuracy%20on%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16926v1&entry.124074799=Read"},
{"title": "Adversarial Online Collaborative Filtering", "author": "Stephen Pasteris and Fabio Vitale and Mark Herbster and Claudio Gentile and Andre' Panisson", "abstract": "  We investigate the problem of online collaborative filtering under\nno-repetition constraints, whereby users need to be served content in an online\nfashion and a given user cannot be recommended the same content item more than\nonce. We start by designing and analyzing an algorithm that works under\nbiclustering assumptions on the user-item preference matrix, and show that this\nalgorithm exhibits an optimal regret guarantee, while being fully adaptive, in\nthat it is oblivious to any prior knowledge about the sequence of users, the\nuniverse of items, as well as the biclustering parameters of the preference\nmatrix. We then propose a more robust version of this algorithm which operates\nwith general matrices. Also this algorithm is parameter free, and we prove\nregret guarantees that scale with the amount by which the preference matrix\ndeviates from a biclustered structure. To our knowledge, these are the first\nresults on online collaborative filtering that hold at this level of generality\nand adaptivity under no-repetition constraints. Finally, we complement our\ntheoretical findings with simple experiments on real-world datasets aimed at\nboth validating the theory and empirically comparing to standard baselines.\nThis comparison shows the competitive advantage of our approach over these\nbaselines.\n", "link": "http://arxiv.org/abs/2302.05765v3", "date": "2024-10-22", "relevancy": 1.3053, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4407}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4336}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Online%20Collaborative%20Filtering&body=Title%3A%20Adversarial%20Online%20Collaborative%20Filtering%0AAuthor%3A%20Stephen%20Pasteris%20and%20Fabio%20Vitale%20and%20Mark%20Herbster%20and%20Claudio%20Gentile%20and%20Andre%27%20Panisson%0AAbstract%3A%20%20%20We%20investigate%20the%20problem%20of%20online%20collaborative%20filtering%20under%0Ano-repetition%20constraints%2C%20whereby%20users%20need%20to%20be%20served%20content%20in%20an%20online%0Afashion%20and%20a%20given%20user%20cannot%20be%20recommended%20the%20same%20content%20item%20more%20than%0Aonce.%20We%20start%20by%20designing%20and%20analyzing%20an%20algorithm%20that%20works%20under%0Abiclustering%20assumptions%20on%20the%20user-item%20preference%20matrix%2C%20and%20show%20that%20this%0Aalgorithm%20exhibits%20an%20optimal%20regret%20guarantee%2C%20while%20being%20fully%20adaptive%2C%20in%0Athat%20it%20is%20oblivious%20to%20any%20prior%20knowledge%20about%20the%20sequence%20of%20users%2C%20the%0Auniverse%20of%20items%2C%20as%20well%20as%20the%20biclustering%20parameters%20of%20the%20preference%0Amatrix.%20We%20then%20propose%20a%20more%20robust%20version%20of%20this%20algorithm%20which%20operates%0Awith%20general%20matrices.%20Also%20this%20algorithm%20is%20parameter%20free%2C%20and%20we%20prove%0Aregret%20guarantees%20that%20scale%20with%20the%20amount%20by%20which%20the%20preference%20matrix%0Adeviates%20from%20a%20biclustered%20structure.%20To%20our%20knowledge%2C%20these%20are%20the%20first%0Aresults%20on%20online%20collaborative%20filtering%20that%20hold%20at%20this%20level%20of%20generality%0Aand%20adaptivity%20under%20no-repetition%20constraints.%20Finally%2C%20we%20complement%20our%0Atheoretical%20findings%20with%20simple%20experiments%20on%20real-world%20datasets%20aimed%20at%0Aboth%20validating%20the%20theory%20and%20empirically%20comparing%20to%20standard%20baselines.%0AThis%20comparison%20shows%20the%20competitive%20advantage%20of%20our%20approach%20over%20these%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05765v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Online%2520Collaborative%2520Filtering%26entry.906535625%3DStephen%2520Pasteris%2520and%2520Fabio%2520Vitale%2520and%2520Mark%2520Herbster%2520and%2520Claudio%2520Gentile%2520and%2520Andre%2527%2520Panisson%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520problem%2520of%2520online%2520collaborative%2520filtering%2520under%250Ano-repetition%2520constraints%252C%2520whereby%2520users%2520need%2520to%2520be%2520served%2520content%2520in%2520an%2520online%250Afashion%2520and%2520a%2520given%2520user%2520cannot%2520be%2520recommended%2520the%2520same%2520content%2520item%2520more%2520than%250Aonce.%2520We%2520start%2520by%2520designing%2520and%2520analyzing%2520an%2520algorithm%2520that%2520works%2520under%250Abiclustering%2520assumptions%2520on%2520the%2520user-item%2520preference%2520matrix%252C%2520and%2520show%2520that%2520this%250Aalgorithm%2520exhibits%2520an%2520optimal%2520regret%2520guarantee%252C%2520while%2520being%2520fully%2520adaptive%252C%2520in%250Athat%2520it%2520is%2520oblivious%2520to%2520any%2520prior%2520knowledge%2520about%2520the%2520sequence%2520of%2520users%252C%2520the%250Auniverse%2520of%2520items%252C%2520as%2520well%2520as%2520the%2520biclustering%2520parameters%2520of%2520the%2520preference%250Amatrix.%2520We%2520then%2520propose%2520a%2520more%2520robust%2520version%2520of%2520this%2520algorithm%2520which%2520operates%250Awith%2520general%2520matrices.%2520Also%2520this%2520algorithm%2520is%2520parameter%2520free%252C%2520and%2520we%2520prove%250Aregret%2520guarantees%2520that%2520scale%2520with%2520the%2520amount%2520by%2520which%2520the%2520preference%2520matrix%250Adeviates%2520from%2520a%2520biclustered%2520structure.%2520To%2520our%2520knowledge%252C%2520these%2520are%2520the%2520first%250Aresults%2520on%2520online%2520collaborative%2520filtering%2520that%2520hold%2520at%2520this%2520level%2520of%2520generality%250Aand%2520adaptivity%2520under%2520no-repetition%2520constraints.%2520Finally%252C%2520we%2520complement%2520our%250Atheoretical%2520findings%2520with%2520simple%2520experiments%2520on%2520real-world%2520datasets%2520aimed%2520at%250Aboth%2520validating%2520the%2520theory%2520and%2520empirically%2520comparing%2520to%2520standard%2520baselines.%250AThis%2520comparison%2520shows%2520the%2520competitive%2520advantage%2520of%2520our%2520approach%2520over%2520these%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.05765v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Online%20Collaborative%20Filtering&entry.906535625=Stephen%20Pasteris%20and%20Fabio%20Vitale%20and%20Mark%20Herbster%20and%20Claudio%20Gentile%20and%20Andre%27%20Panisson&entry.1292438233=%20%20We%20investigate%20the%20problem%20of%20online%20collaborative%20filtering%20under%0Ano-repetition%20constraints%2C%20whereby%20users%20need%20to%20be%20served%20content%20in%20an%20online%0Afashion%20and%20a%20given%20user%20cannot%20be%20recommended%20the%20same%20content%20item%20more%20than%0Aonce.%20We%20start%20by%20designing%20and%20analyzing%20an%20algorithm%20that%20works%20under%0Abiclustering%20assumptions%20on%20the%20user-item%20preference%20matrix%2C%20and%20show%20that%20this%0Aalgorithm%20exhibits%20an%20optimal%20regret%20guarantee%2C%20while%20being%20fully%20adaptive%2C%20in%0Athat%20it%20is%20oblivious%20to%20any%20prior%20knowledge%20about%20the%20sequence%20of%20users%2C%20the%0Auniverse%20of%20items%2C%20as%20well%20as%20the%20biclustering%20parameters%20of%20the%20preference%0Amatrix.%20We%20then%20propose%20a%20more%20robust%20version%20of%20this%20algorithm%20which%20operates%0Awith%20general%20matrices.%20Also%20this%20algorithm%20is%20parameter%20free%2C%20and%20we%20prove%0Aregret%20guarantees%20that%20scale%20with%20the%20amount%20by%20which%20the%20preference%20matrix%0Adeviates%20from%20a%20biclustered%20structure.%20To%20our%20knowledge%2C%20these%20are%20the%20first%0Aresults%20on%20online%20collaborative%20filtering%20that%20hold%20at%20this%20level%20of%20generality%0Aand%20adaptivity%20under%20no-repetition%20constraints.%20Finally%2C%20we%20complement%20our%0Atheoretical%20findings%20with%20simple%20experiments%20on%20real-world%20datasets%20aimed%20at%0Aboth%20validating%20the%20theory%20and%20empirically%20comparing%20to%20standard%20baselines.%0AThis%20comparison%20shows%20the%20competitive%20advantage%20of%20our%20approach%20over%20these%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05765v3&entry.124074799=Read"},
{"title": "SleepCoT: A Lightweight Personalized Sleep Health Model via\n  Chain-of-Thought Distillation", "author": "Huimin Zheng and Xiaofeng Xing and Xiangmin Xu", "abstract": "  We present a novel approach to personalized sleep health management using\nfew-shot Chain-of-Thought (CoT) distillation, enabling small-scale language\nmodels (> 2B parameters) to rival the performance of large language models\n(LLMs) in specialized health domains. Our method simultaneously distills\nproblem-solving strategies, long-tail expert knowledge, and personalized\nrecommendation capabilities from larger models into more efficient, compact\nmodels. Unlike existing systems, our approach offers three key functionalities:\ngenerating personalized sleep health recommendations, supporting user-specific\nfollow-up inquiries, and providing responses to domain-specific knowledge\nquestions. We focus on sleep health due to its measurability via wearable\ndevices and its impact on overall well-being. Our experimental setup, involving\nGPT-4o for data synthesis, Qwen-max for instruction set creation, and Qwen2.5\n1.5B for model distillation, demonstrates significant improvements over\nbaseline small-scale models in penalization, reasoning, and knowledge\napplication. Experiments using 100 simulated sleep reports and 1,000\ndomain-specific questions shows our model achieves comparable performance to\nlarger models while maintaining efficiency for real-world deployment. This\nresearch not only advances AI-driven health management but also provides a\nnovel approach to leveraging LLM capabilities in resource-constrained\nenvironments, potentially enhancing the accessibility of personalized\nhealthcare solutions.\n", "link": "http://arxiv.org/abs/2410.16924v1", "date": "2024-10-22", "relevancy": 1.4945, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5103}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4836}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SleepCoT%3A%20A%20Lightweight%20Personalized%20Sleep%20Health%20Model%20via%0A%20%20Chain-of-Thought%20Distillation&body=Title%3A%20SleepCoT%3A%20A%20Lightweight%20Personalized%20Sleep%20Health%20Model%20via%0A%20%20Chain-of-Thought%20Distillation%0AAuthor%3A%20Huimin%20Zheng%20and%20Xiaofeng%20Xing%20and%20Xiangmin%20Xu%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20personalized%20sleep%20health%20management%20using%0Afew-shot%20Chain-of-Thought%20%28CoT%29%20distillation%2C%20enabling%20small-scale%20language%0Amodels%20%28%3E%202B%20parameters%29%20to%20rival%20the%20performance%20of%20large%20language%20models%0A%28LLMs%29%20in%20specialized%20health%20domains.%20Our%20method%20simultaneously%20distills%0Aproblem-solving%20strategies%2C%20long-tail%20expert%20knowledge%2C%20and%20personalized%0Arecommendation%20capabilities%20from%20larger%20models%20into%20more%20efficient%2C%20compact%0Amodels.%20Unlike%20existing%20systems%2C%20our%20approach%20offers%20three%20key%20functionalities%3A%0Agenerating%20personalized%20sleep%20health%20recommendations%2C%20supporting%20user-specific%0Afollow-up%20inquiries%2C%20and%20providing%20responses%20to%20domain-specific%20knowledge%0Aquestions.%20We%20focus%20on%20sleep%20health%20due%20to%20its%20measurability%20via%20wearable%0Adevices%20and%20its%20impact%20on%20overall%20well-being.%20Our%20experimental%20setup%2C%20involving%0AGPT-4o%20for%20data%20synthesis%2C%20Qwen-max%20for%20instruction%20set%20creation%2C%20and%20Qwen2.5%0A1.5B%20for%20model%20distillation%2C%20demonstrates%20significant%20improvements%20over%0Abaseline%20small-scale%20models%20in%20penalization%2C%20reasoning%2C%20and%20knowledge%0Aapplication.%20Experiments%20using%20100%20simulated%20sleep%20reports%20and%201%2C000%0Adomain-specific%20questions%20shows%20our%20model%20achieves%20comparable%20performance%20to%0Alarger%20models%20while%20maintaining%20efficiency%20for%20real-world%20deployment.%20This%0Aresearch%20not%20only%20advances%20AI-driven%20health%20management%20but%20also%20provides%20a%0Anovel%20approach%20to%20leveraging%20LLM%20capabilities%20in%20resource-constrained%0Aenvironments%2C%20potentially%20enhancing%20the%20accessibility%20of%20personalized%0Ahealthcare%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSleepCoT%253A%2520A%2520Lightweight%2520Personalized%2520Sleep%2520Health%2520Model%2520via%250A%2520%2520Chain-of-Thought%2520Distillation%26entry.906535625%3DHuimin%2520Zheng%2520and%2520Xiaofeng%2520Xing%2520and%2520Xiangmin%2520Xu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520personalized%2520sleep%2520health%2520management%2520using%250Afew-shot%2520Chain-of-Thought%2520%2528CoT%2529%2520distillation%252C%2520enabling%2520small-scale%2520language%250Amodels%2520%2528%253E%25202B%2520parameters%2529%2520to%2520rival%2520the%2520performance%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520in%2520specialized%2520health%2520domains.%2520Our%2520method%2520simultaneously%2520distills%250Aproblem-solving%2520strategies%252C%2520long-tail%2520expert%2520knowledge%252C%2520and%2520personalized%250Arecommendation%2520capabilities%2520from%2520larger%2520models%2520into%2520more%2520efficient%252C%2520compact%250Amodels.%2520Unlike%2520existing%2520systems%252C%2520our%2520approach%2520offers%2520three%2520key%2520functionalities%253A%250Agenerating%2520personalized%2520sleep%2520health%2520recommendations%252C%2520supporting%2520user-specific%250Afollow-up%2520inquiries%252C%2520and%2520providing%2520responses%2520to%2520domain-specific%2520knowledge%250Aquestions.%2520We%2520focus%2520on%2520sleep%2520health%2520due%2520to%2520its%2520measurability%2520via%2520wearable%250Adevices%2520and%2520its%2520impact%2520on%2520overall%2520well-being.%2520Our%2520experimental%2520setup%252C%2520involving%250AGPT-4o%2520for%2520data%2520synthesis%252C%2520Qwen-max%2520for%2520instruction%2520set%2520creation%252C%2520and%2520Qwen2.5%250A1.5B%2520for%2520model%2520distillation%252C%2520demonstrates%2520significant%2520improvements%2520over%250Abaseline%2520small-scale%2520models%2520in%2520penalization%252C%2520reasoning%252C%2520and%2520knowledge%250Aapplication.%2520Experiments%2520using%2520100%2520simulated%2520sleep%2520reports%2520and%25201%252C000%250Adomain-specific%2520questions%2520shows%2520our%2520model%2520achieves%2520comparable%2520performance%2520to%250Alarger%2520models%2520while%2520maintaining%2520efficiency%2520for%2520real-world%2520deployment.%2520This%250Aresearch%2520not%2520only%2520advances%2520AI-driven%2520health%2520management%2520but%2520also%2520provides%2520a%250Anovel%2520approach%2520to%2520leveraging%2520LLM%2520capabilities%2520in%2520resource-constrained%250Aenvironments%252C%2520potentially%2520enhancing%2520the%2520accessibility%2520of%2520personalized%250Ahealthcare%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SleepCoT%3A%20A%20Lightweight%20Personalized%20Sleep%20Health%20Model%20via%0A%20%20Chain-of-Thought%20Distillation&entry.906535625=Huimin%20Zheng%20and%20Xiaofeng%20Xing%20and%20Xiangmin%20Xu&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20personalized%20sleep%20health%20management%20using%0Afew-shot%20Chain-of-Thought%20%28CoT%29%20distillation%2C%20enabling%20small-scale%20language%0Amodels%20%28%3E%202B%20parameters%29%20to%20rival%20the%20performance%20of%20large%20language%20models%0A%28LLMs%29%20in%20specialized%20health%20domains.%20Our%20method%20simultaneously%20distills%0Aproblem-solving%20strategies%2C%20long-tail%20expert%20knowledge%2C%20and%20personalized%0Arecommendation%20capabilities%20from%20larger%20models%20into%20more%20efficient%2C%20compact%0Amodels.%20Unlike%20existing%20systems%2C%20our%20approach%20offers%20three%20key%20functionalities%3A%0Agenerating%20personalized%20sleep%20health%20recommendations%2C%20supporting%20user-specific%0Afollow-up%20inquiries%2C%20and%20providing%20responses%20to%20domain-specific%20knowledge%0Aquestions.%20We%20focus%20on%20sleep%20health%20due%20to%20its%20measurability%20via%20wearable%0Adevices%20and%20its%20impact%20on%20overall%20well-being.%20Our%20experimental%20setup%2C%20involving%0AGPT-4o%20for%20data%20synthesis%2C%20Qwen-max%20for%20instruction%20set%20creation%2C%20and%20Qwen2.5%0A1.5B%20for%20model%20distillation%2C%20demonstrates%20significant%20improvements%20over%0Abaseline%20small-scale%20models%20in%20penalization%2C%20reasoning%2C%20and%20knowledge%0Aapplication.%20Experiments%20using%20100%20simulated%20sleep%20reports%20and%201%2C000%0Adomain-specific%20questions%20shows%20our%20model%20achieves%20comparable%20performance%20to%0Alarger%20models%20while%20maintaining%20efficiency%20for%20real-world%20deployment.%20This%0Aresearch%20not%20only%20advances%20AI-driven%20health%20management%20but%20also%20provides%20a%0Anovel%20approach%20to%20leveraging%20LLM%20capabilities%20in%20resource-constrained%0Aenvironments%2C%20potentially%20enhancing%20the%20accessibility%20of%20personalized%0Ahealthcare%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16924v1&entry.124074799=Read"},
{"title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs", "author": "Jo\u00e3o Pedro Fernandes Torres and Catherine Mulligan and Joaquim Jorge and Catarina Moreira", "abstract": "  The growing volume of academic publications poses significant challenges for\nresearchers conducting timely and accurate Systematic Literature Reviews,\nparticularly in fast-evolving fields like artificial intelligence. This growth\nof academic literature also makes it increasingly difficult for lay people to\naccess scientific knowledge effectively, meaning academic literature is often\nmisrepresented in the popular press and, more broadly, in society. Traditional\nSLR methods are labor-intensive and error-prone, and they struggle to keep up\nwith the rapid pace of new research. To address these issues, we developed\n\\textit{PROMPTHEUS}: an AI-driven pipeline solution that automates the SLR\nprocess using Large Language Models. We aimed to enhance efficiency by reducing\nthe manual workload while maintaining the precision and coherence required for\ncomprehensive literature synthesis. PROMPTHEUS automates key stages of the SLR\nprocess, including systematic search, data extraction, topic modeling using\nBERTopic, and summarization with transformer models. Evaluations conducted\nacross five research domains demonstrate that PROMPTHEUS reduces review time,\nachieves high precision, and provides coherent topic organization, offering a\nscalable and effective solution for conducting literature reviews in an\nincreasingly crowded research landscape. In addition, such tools may reduce the\nincreasing mistrust in science by making summarization more accessible to\nlaypeople.\n  The code for this project can be found on the GitHub repository at\nhttps://github.com/joaopftorres/PROMPTHEUS.git\n", "link": "http://arxiv.org/abs/2410.15978v2", "date": "2024-10-22", "relevancy": 1.3636, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4618}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROMPTHEUS%3A%20A%20Human-Centered%20Pipeline%20to%20Streamline%20SLRs%20with%20LLMs&body=Title%3A%20PROMPTHEUS%3A%20A%20Human-Centered%20Pipeline%20to%20Streamline%20SLRs%20with%20LLMs%0AAuthor%3A%20Jo%C3%A3o%20Pedro%20Fernandes%20Torres%20and%20Catherine%20Mulligan%20and%20Joaquim%20Jorge%20and%20Catarina%20Moreira%0AAbstract%3A%20%20%20The%20growing%20volume%20of%20academic%20publications%20poses%20significant%20challenges%20for%0Aresearchers%20conducting%20timely%20and%20accurate%20Systematic%20Literature%20Reviews%2C%0Aparticularly%20in%20fast-evolving%20fields%20like%20artificial%20intelligence.%20This%20growth%0Aof%20academic%20literature%20also%20makes%20it%20increasingly%20difficult%20for%20lay%20people%20to%0Aaccess%20scientific%20knowledge%20effectively%2C%20meaning%20academic%20literature%20is%20often%0Amisrepresented%20in%20the%20popular%20press%20and%2C%20more%20broadly%2C%20in%20society.%20Traditional%0ASLR%20methods%20are%20labor-intensive%20and%20error-prone%2C%20and%20they%20struggle%20to%20keep%20up%0Awith%20the%20rapid%20pace%20of%20new%20research.%20To%20address%20these%20issues%2C%20we%20developed%0A%5Ctextit%7BPROMPTHEUS%7D%3A%20an%20AI-driven%20pipeline%20solution%20that%20automates%20the%20SLR%0Aprocess%20using%20Large%20Language%20Models.%20We%20aimed%20to%20enhance%20efficiency%20by%20reducing%0Athe%20manual%20workload%20while%20maintaining%20the%20precision%20and%20coherence%20required%20for%0Acomprehensive%20literature%20synthesis.%20PROMPTHEUS%20automates%20key%20stages%20of%20the%20SLR%0Aprocess%2C%20including%20systematic%20search%2C%20data%20extraction%2C%20topic%20modeling%20using%0ABERTopic%2C%20and%20summarization%20with%20transformer%20models.%20Evaluations%20conducted%0Aacross%20five%20research%20domains%20demonstrate%20that%20PROMPTHEUS%20reduces%20review%20time%2C%0Aachieves%20high%20precision%2C%20and%20provides%20coherent%20topic%20organization%2C%20offering%20a%0Ascalable%20and%20effective%20solution%20for%20conducting%20literature%20reviews%20in%20an%0Aincreasingly%20crowded%20research%20landscape.%20In%20addition%2C%20such%20tools%20may%20reduce%20the%0Aincreasing%20mistrust%20in%20science%20by%20making%20summarization%20more%20accessible%20to%0Alaypeople.%0A%20%20The%20code%20for%20this%20project%20can%20be%20found%20on%20the%20GitHub%20repository%20at%0Ahttps%3A//github.com/joaopftorres/PROMPTHEUS.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROMPTHEUS%253A%2520A%2520Human-Centered%2520Pipeline%2520to%2520Streamline%2520SLRs%2520with%2520LLMs%26entry.906535625%3DJo%25C3%25A3o%2520Pedro%2520Fernandes%2520Torres%2520and%2520Catherine%2520Mulligan%2520and%2520Joaquim%2520Jorge%2520and%2520Catarina%2520Moreira%26entry.1292438233%3D%2520%2520The%2520growing%2520volume%2520of%2520academic%2520publications%2520poses%2520significant%2520challenges%2520for%250Aresearchers%2520conducting%2520timely%2520and%2520accurate%2520Systematic%2520Literature%2520Reviews%252C%250Aparticularly%2520in%2520fast-evolving%2520fields%2520like%2520artificial%2520intelligence.%2520This%2520growth%250Aof%2520academic%2520literature%2520also%2520makes%2520it%2520increasingly%2520difficult%2520for%2520lay%2520people%2520to%250Aaccess%2520scientific%2520knowledge%2520effectively%252C%2520meaning%2520academic%2520literature%2520is%2520often%250Amisrepresented%2520in%2520the%2520popular%2520press%2520and%252C%2520more%2520broadly%252C%2520in%2520society.%2520Traditional%250ASLR%2520methods%2520are%2520labor-intensive%2520and%2520error-prone%252C%2520and%2520they%2520struggle%2520to%2520keep%2520up%250Awith%2520the%2520rapid%2520pace%2520of%2520new%2520research.%2520To%2520address%2520these%2520issues%252C%2520we%2520developed%250A%255Ctextit%257BPROMPTHEUS%257D%253A%2520an%2520AI-driven%2520pipeline%2520solution%2520that%2520automates%2520the%2520SLR%250Aprocess%2520using%2520Large%2520Language%2520Models.%2520We%2520aimed%2520to%2520enhance%2520efficiency%2520by%2520reducing%250Athe%2520manual%2520workload%2520while%2520maintaining%2520the%2520precision%2520and%2520coherence%2520required%2520for%250Acomprehensive%2520literature%2520synthesis.%2520PROMPTHEUS%2520automates%2520key%2520stages%2520of%2520the%2520SLR%250Aprocess%252C%2520including%2520systematic%2520search%252C%2520data%2520extraction%252C%2520topic%2520modeling%2520using%250ABERTopic%252C%2520and%2520summarization%2520with%2520transformer%2520models.%2520Evaluations%2520conducted%250Aacross%2520five%2520research%2520domains%2520demonstrate%2520that%2520PROMPTHEUS%2520reduces%2520review%2520time%252C%250Aachieves%2520high%2520precision%252C%2520and%2520provides%2520coherent%2520topic%2520organization%252C%2520offering%2520a%250Ascalable%2520and%2520effective%2520solution%2520for%2520conducting%2520literature%2520reviews%2520in%2520an%250Aincreasingly%2520crowded%2520research%2520landscape.%2520In%2520addition%252C%2520such%2520tools%2520may%2520reduce%2520the%250Aincreasing%2520mistrust%2520in%2520science%2520by%2520making%2520summarization%2520more%2520accessible%2520to%250Alaypeople.%250A%2520%2520The%2520code%2520for%2520this%2520project%2520can%2520be%2520found%2520on%2520the%2520GitHub%2520repository%2520at%250Ahttps%253A//github.com/joaopftorres/PROMPTHEUS.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROMPTHEUS%3A%20A%20Human-Centered%20Pipeline%20to%20Streamline%20SLRs%20with%20LLMs&entry.906535625=Jo%C3%A3o%20Pedro%20Fernandes%20Torres%20and%20Catherine%20Mulligan%20and%20Joaquim%20Jorge%20and%20Catarina%20Moreira&entry.1292438233=%20%20The%20growing%20volume%20of%20academic%20publications%20poses%20significant%20challenges%20for%0Aresearchers%20conducting%20timely%20and%20accurate%20Systematic%20Literature%20Reviews%2C%0Aparticularly%20in%20fast-evolving%20fields%20like%20artificial%20intelligence.%20This%20growth%0Aof%20academic%20literature%20also%20makes%20it%20increasingly%20difficult%20for%20lay%20people%20to%0Aaccess%20scientific%20knowledge%20effectively%2C%20meaning%20academic%20literature%20is%20often%0Amisrepresented%20in%20the%20popular%20press%20and%2C%20more%20broadly%2C%20in%20society.%20Traditional%0ASLR%20methods%20are%20labor-intensive%20and%20error-prone%2C%20and%20they%20struggle%20to%20keep%20up%0Awith%20the%20rapid%20pace%20of%20new%20research.%20To%20address%20these%20issues%2C%20we%20developed%0A%5Ctextit%7BPROMPTHEUS%7D%3A%20an%20AI-driven%20pipeline%20solution%20that%20automates%20the%20SLR%0Aprocess%20using%20Large%20Language%20Models.%20We%20aimed%20to%20enhance%20efficiency%20by%20reducing%0Athe%20manual%20workload%20while%20maintaining%20the%20precision%20and%20coherence%20required%20for%0Acomprehensive%20literature%20synthesis.%20PROMPTHEUS%20automates%20key%20stages%20of%20the%20SLR%0Aprocess%2C%20including%20systematic%20search%2C%20data%20extraction%2C%20topic%20modeling%20using%0ABERTopic%2C%20and%20summarization%20with%20transformer%20models.%20Evaluations%20conducted%0Aacross%20five%20research%20domains%20demonstrate%20that%20PROMPTHEUS%20reduces%20review%20time%2C%0Aachieves%20high%20precision%2C%20and%20provides%20coherent%20topic%20organization%2C%20offering%20a%0Ascalable%20and%20effective%20solution%20for%20conducting%20literature%20reviews%20in%20an%0Aincreasingly%20crowded%20research%20landscape.%20In%20addition%2C%20such%20tools%20may%20reduce%20the%0Aincreasing%20mistrust%20in%20science%20by%20making%20summarization%20more%20accessible%20to%0Alaypeople.%0A%20%20The%20code%20for%20this%20project%20can%20be%20found%20on%20the%20GitHub%20repository%20at%0Ahttps%3A//github.com/joaopftorres/PROMPTHEUS.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15978v2&entry.124074799=Read"},
{"title": "Towards Enhancing the Reproducibility of Deep Learning Bugs: An\n  Empirical Study", "author": "Mehil B. Shah and Mohammad Masudur Rahman and Foutse Khomh", "abstract": "  Context: Deep learning has achieved remarkable progress in various domains.\nHowever, like any software system, deep learning systems contain bugs, some of\nwhich can have severe impacts, as evidenced by crashes involving autonomous\nvehicles. Despite substantial advancements in deep learning techniques, little\nresearch has focused on reproducing deep learning bugs, which is an essential\nstep for their resolution. Existing literature suggests that only 3% of deep\nlearning bugs are reproducible, underscoring the need for further research.\n  Objective: This paper examines the reproducibility of deep learning bugs. We\nidentify edit actions and useful information that could improve the\nreproducibility of deep learning bugs.\n  Method: First, we construct a dataset of 668 deep-learning bugs from Stack\nOverflow and GitHub across three frameworks and 22 architectures. Second, out\nof the 668 bugs, we select 165 bugs using stratified sampling and attempt to\ndetermine their reproducibility. While reproducing these bugs, we identify edit\nactions and useful information for their reproduction. Third, we used the\nApriori algorithm to identify useful information and edit actions required to\nreproduce specific types of bugs. Finally, we conducted a user study involving\n22 developers to assess the effectiveness of our findings in real-life\nsettings.\n  Results: We successfully reproduced 148 out of 165 bugs attempted. We\nidentified ten edit actions and five useful types of component information that\ncan help us reproduce the deep learning bugs. With the help of our findings,\nthe developers were able to reproduce 22.92% more bugs and reduce their\nreproduction time by 24.35%.\n  Conclusions: Our research addresses the critical issue of deep learning bug\nreproducibility. Practitioners and researchers can leverage our findings to\nimprove deep learning bug reproducibility.\n", "link": "http://arxiv.org/abs/2401.03069v4", "date": "2024-10-22", "relevancy": 1.6773, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.453}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4175}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Enhancing%20the%20Reproducibility%20of%20Deep%20Learning%20Bugs%3A%20An%0A%20%20Empirical%20Study&body=Title%3A%20Towards%20Enhancing%20the%20Reproducibility%20of%20Deep%20Learning%20Bugs%3A%20An%0A%20%20Empirical%20Study%0AAuthor%3A%20Mehil%20B.%20Shah%20and%20Mohammad%20Masudur%20Rahman%20and%20Foutse%20Khomh%0AAbstract%3A%20%20%20Context%3A%20Deep%20learning%20has%20achieved%20remarkable%20progress%20in%20various%20domains.%0AHowever%2C%20like%20any%20software%20system%2C%20deep%20learning%20systems%20contain%20bugs%2C%20some%20of%0Awhich%20can%20have%20severe%20impacts%2C%20as%20evidenced%20by%20crashes%20involving%20autonomous%0Avehicles.%20Despite%20substantial%20advancements%20in%20deep%20learning%20techniques%2C%20little%0Aresearch%20has%20focused%20on%20reproducing%20deep%20learning%20bugs%2C%20which%20is%20an%20essential%0Astep%20for%20their%20resolution.%20Existing%20literature%20suggests%20that%20only%203%25%20of%20deep%0Alearning%20bugs%20are%20reproducible%2C%20underscoring%20the%20need%20for%20further%20research.%0A%20%20Objective%3A%20This%20paper%20examines%20the%20reproducibility%20of%20deep%20learning%20bugs.%20We%0Aidentify%20edit%20actions%20and%20useful%20information%20that%20could%20improve%20the%0Areproducibility%20of%20deep%20learning%20bugs.%0A%20%20Method%3A%20First%2C%20we%20construct%20a%20dataset%20of%20668%20deep-learning%20bugs%20from%20Stack%0AOverflow%20and%20GitHub%20across%20three%20frameworks%20and%2022%20architectures.%20Second%2C%20out%0Aof%20the%20668%20bugs%2C%20we%20select%20165%20bugs%20using%20stratified%20sampling%20and%20attempt%20to%0Adetermine%20their%20reproducibility.%20While%20reproducing%20these%20bugs%2C%20we%20identify%20edit%0Aactions%20and%20useful%20information%20for%20their%20reproduction.%20Third%2C%20we%20used%20the%0AApriori%20algorithm%20to%20identify%20useful%20information%20and%20edit%20actions%20required%20to%0Areproduce%20specific%20types%20of%20bugs.%20Finally%2C%20we%20conducted%20a%20user%20study%20involving%0A22%20developers%20to%20assess%20the%20effectiveness%20of%20our%20findings%20in%20real-life%0Asettings.%0A%20%20Results%3A%20We%20successfully%20reproduced%20148%20out%20of%20165%20bugs%20attempted.%20We%0Aidentified%20ten%20edit%20actions%20and%20five%20useful%20types%20of%20component%20information%20that%0Acan%20help%20us%20reproduce%20the%20deep%20learning%20bugs.%20With%20the%20help%20of%20our%20findings%2C%0Athe%20developers%20were%20able%20to%20reproduce%2022.92%25%20more%20bugs%20and%20reduce%20their%0Areproduction%20time%20by%2024.35%25.%0A%20%20Conclusions%3A%20Our%20research%20addresses%20the%20critical%20issue%20of%20deep%20learning%20bug%0Areproducibility.%20Practitioners%20and%20researchers%20can%20leverage%20our%20findings%20to%0Aimprove%20deep%20learning%20bug%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03069v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Enhancing%2520the%2520Reproducibility%2520of%2520Deep%2520Learning%2520Bugs%253A%2520An%250A%2520%2520Empirical%2520Study%26entry.906535625%3DMehil%2520B.%2520Shah%2520and%2520Mohammad%2520Masudur%2520Rahman%2520and%2520Foutse%2520Khomh%26entry.1292438233%3D%2520%2520Context%253A%2520Deep%2520learning%2520has%2520achieved%2520remarkable%2520progress%2520in%2520various%2520domains.%250AHowever%252C%2520like%2520any%2520software%2520system%252C%2520deep%2520learning%2520systems%2520contain%2520bugs%252C%2520some%2520of%250Awhich%2520can%2520have%2520severe%2520impacts%252C%2520as%2520evidenced%2520by%2520crashes%2520involving%2520autonomous%250Avehicles.%2520Despite%2520substantial%2520advancements%2520in%2520deep%2520learning%2520techniques%252C%2520little%250Aresearch%2520has%2520focused%2520on%2520reproducing%2520deep%2520learning%2520bugs%252C%2520which%2520is%2520an%2520essential%250Astep%2520for%2520their%2520resolution.%2520Existing%2520literature%2520suggests%2520that%2520only%25203%2525%2520of%2520deep%250Alearning%2520bugs%2520are%2520reproducible%252C%2520underscoring%2520the%2520need%2520for%2520further%2520research.%250A%2520%2520Objective%253A%2520This%2520paper%2520examines%2520the%2520reproducibility%2520of%2520deep%2520learning%2520bugs.%2520We%250Aidentify%2520edit%2520actions%2520and%2520useful%2520information%2520that%2520could%2520improve%2520the%250Areproducibility%2520of%2520deep%2520learning%2520bugs.%250A%2520%2520Method%253A%2520First%252C%2520we%2520construct%2520a%2520dataset%2520of%2520668%2520deep-learning%2520bugs%2520from%2520Stack%250AOverflow%2520and%2520GitHub%2520across%2520three%2520frameworks%2520and%252022%2520architectures.%2520Second%252C%2520out%250Aof%2520the%2520668%2520bugs%252C%2520we%2520select%2520165%2520bugs%2520using%2520stratified%2520sampling%2520and%2520attempt%2520to%250Adetermine%2520their%2520reproducibility.%2520While%2520reproducing%2520these%2520bugs%252C%2520we%2520identify%2520edit%250Aactions%2520and%2520useful%2520information%2520for%2520their%2520reproduction.%2520Third%252C%2520we%2520used%2520the%250AApriori%2520algorithm%2520to%2520identify%2520useful%2520information%2520and%2520edit%2520actions%2520required%2520to%250Areproduce%2520specific%2520types%2520of%2520bugs.%2520Finally%252C%2520we%2520conducted%2520a%2520user%2520study%2520involving%250A22%2520developers%2520to%2520assess%2520the%2520effectiveness%2520of%2520our%2520findings%2520in%2520real-life%250Asettings.%250A%2520%2520Results%253A%2520We%2520successfully%2520reproduced%2520148%2520out%2520of%2520165%2520bugs%2520attempted.%2520We%250Aidentified%2520ten%2520edit%2520actions%2520and%2520five%2520useful%2520types%2520of%2520component%2520information%2520that%250Acan%2520help%2520us%2520reproduce%2520the%2520deep%2520learning%2520bugs.%2520With%2520the%2520help%2520of%2520our%2520findings%252C%250Athe%2520developers%2520were%2520able%2520to%2520reproduce%252022.92%2525%2520more%2520bugs%2520and%2520reduce%2520their%250Areproduction%2520time%2520by%252024.35%2525.%250A%2520%2520Conclusions%253A%2520Our%2520research%2520addresses%2520the%2520critical%2520issue%2520of%2520deep%2520learning%2520bug%250Areproducibility.%2520Practitioners%2520and%2520researchers%2520can%2520leverage%2520our%2520findings%2520to%250Aimprove%2520deep%2520learning%2520bug%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03069v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Enhancing%20the%20Reproducibility%20of%20Deep%20Learning%20Bugs%3A%20An%0A%20%20Empirical%20Study&entry.906535625=Mehil%20B.%20Shah%20and%20Mohammad%20Masudur%20Rahman%20and%20Foutse%20Khomh&entry.1292438233=%20%20Context%3A%20Deep%20learning%20has%20achieved%20remarkable%20progress%20in%20various%20domains.%0AHowever%2C%20like%20any%20software%20system%2C%20deep%20learning%20systems%20contain%20bugs%2C%20some%20of%0Awhich%20can%20have%20severe%20impacts%2C%20as%20evidenced%20by%20crashes%20involving%20autonomous%0Avehicles.%20Despite%20substantial%20advancements%20in%20deep%20learning%20techniques%2C%20little%0Aresearch%20has%20focused%20on%20reproducing%20deep%20learning%20bugs%2C%20which%20is%20an%20essential%0Astep%20for%20their%20resolution.%20Existing%20literature%20suggests%20that%20only%203%25%20of%20deep%0Alearning%20bugs%20are%20reproducible%2C%20underscoring%20the%20need%20for%20further%20research.%0A%20%20Objective%3A%20This%20paper%20examines%20the%20reproducibility%20of%20deep%20learning%20bugs.%20We%0Aidentify%20edit%20actions%20and%20useful%20information%20that%20could%20improve%20the%0Areproducibility%20of%20deep%20learning%20bugs.%0A%20%20Method%3A%20First%2C%20we%20construct%20a%20dataset%20of%20668%20deep-learning%20bugs%20from%20Stack%0AOverflow%20and%20GitHub%20across%20three%20frameworks%20and%2022%20architectures.%20Second%2C%20out%0Aof%20the%20668%20bugs%2C%20we%20select%20165%20bugs%20using%20stratified%20sampling%20and%20attempt%20to%0Adetermine%20their%20reproducibility.%20While%20reproducing%20these%20bugs%2C%20we%20identify%20edit%0Aactions%20and%20useful%20information%20for%20their%20reproduction.%20Third%2C%20we%20used%20the%0AApriori%20algorithm%20to%20identify%20useful%20information%20and%20edit%20actions%20required%20to%0Areproduce%20specific%20types%20of%20bugs.%20Finally%2C%20we%20conducted%20a%20user%20study%20involving%0A22%20developers%20to%20assess%20the%20effectiveness%20of%20our%20findings%20in%20real-life%0Asettings.%0A%20%20Results%3A%20We%20successfully%20reproduced%20148%20out%20of%20165%20bugs%20attempted.%20We%0Aidentified%20ten%20edit%20actions%20and%20five%20useful%20types%20of%20component%20information%20that%0Acan%20help%20us%20reproduce%20the%20deep%20learning%20bugs.%20With%20the%20help%20of%20our%20findings%2C%0Athe%20developers%20were%20able%20to%20reproduce%2022.92%25%20more%20bugs%20and%20reduce%20their%0Areproduction%20time%20by%2024.35%25.%0A%20%20Conclusions%3A%20Our%20research%20addresses%20the%20critical%20issue%20of%20deep%20learning%20bug%0Areproducibility.%20Practitioners%20and%20researchers%20can%20leverage%20our%20findings%20to%0Aimprove%20deep%20learning%20bug%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03069v4&entry.124074799=Read"},
{"title": "Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile\n  Skins", "author": "Venkatesh Pattabiraman and Yifeng Cao and Siddhant Haldar and Lerrel Pinto and Raunaq Bhirangi", "abstract": "  While visuomotor policy learning has advanced robotic manipulation, precisely\nexecuting contact-rich tasks remains challenging due to the limitations of\nvision in reasoning about physical interactions. To address this, recent work\nhas sought to integrate tactile sensing into policy learning. However, many\nexisting approaches rely on optical tactile sensors that are either restricted\nto recognition tasks or require complex dimensionality reduction steps for\npolicy learning. In this work, we explore learning policies with magnetic skin\nsensors, which are inherently low-dimensional, highly sensitive, and\ninexpensive to integrate with robotic platforms. To leverage these sensors\neffectively, we present the Visuo-Skin (ViSk) framework, a simple approach that\nuses a transformer-based policy and treats skin sensor data as additional\ntokens alongside visual information. Evaluated on four complex real-world tasks\ninvolving credit card swiping, plug insertion, USB insertion, and bookshelf\nretrieval, ViSk significantly outperforms both vision-only and optical tactile\nsensing based policies. Further analysis reveals that combining tactile and\nvisual modalities enhances policy performance and spatial generalization,\nachieving an average improvement of 27.5% across tasks.\nhttps://visuoskin.github.io/\n", "link": "http://arxiv.org/abs/2410.17246v1", "date": "2024-10-22", "relevancy": 1.6963, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6262}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5492}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Precise%2C%20Contact-Rich%20Manipulation%20through%20Uncalibrated%20Tactile%0A%20%20Skins&body=Title%3A%20Learning%20Precise%2C%20Contact-Rich%20Manipulation%20through%20Uncalibrated%20Tactile%0A%20%20Skins%0AAuthor%3A%20Venkatesh%20Pattabiraman%20and%20Yifeng%20Cao%20and%20Siddhant%20Haldar%20and%20Lerrel%20Pinto%20and%20Raunaq%20Bhirangi%0AAbstract%3A%20%20%20While%20visuomotor%20policy%20learning%20has%20advanced%20robotic%20manipulation%2C%20precisely%0Aexecuting%20contact-rich%20tasks%20remains%20challenging%20due%20to%20the%20limitations%20of%0Avision%20in%20reasoning%20about%20physical%20interactions.%20To%20address%20this%2C%20recent%20work%0Ahas%20sought%20to%20integrate%20tactile%20sensing%20into%20policy%20learning.%20However%2C%20many%0Aexisting%20approaches%20rely%20on%20optical%20tactile%20sensors%20that%20are%20either%20restricted%0Ato%20recognition%20tasks%20or%20require%20complex%20dimensionality%20reduction%20steps%20for%0Apolicy%20learning.%20In%20this%20work%2C%20we%20explore%20learning%20policies%20with%20magnetic%20skin%0Asensors%2C%20which%20are%20inherently%20low-dimensional%2C%20highly%20sensitive%2C%20and%0Ainexpensive%20to%20integrate%20with%20robotic%20platforms.%20To%20leverage%20these%20sensors%0Aeffectively%2C%20we%20present%20the%20Visuo-Skin%20%28ViSk%29%20framework%2C%20a%20simple%20approach%20that%0Auses%20a%20transformer-based%20policy%20and%20treats%20skin%20sensor%20data%20as%20additional%0Atokens%20alongside%20visual%20information.%20Evaluated%20on%20four%20complex%20real-world%20tasks%0Ainvolving%20credit%20card%20swiping%2C%20plug%20insertion%2C%20USB%20insertion%2C%20and%20bookshelf%0Aretrieval%2C%20ViSk%20significantly%20outperforms%20both%20vision-only%20and%20optical%20tactile%0Asensing%20based%20policies.%20Further%20analysis%20reveals%20that%20combining%20tactile%20and%0Avisual%20modalities%20enhances%20policy%20performance%20and%20spatial%20generalization%2C%0Aachieving%20an%20average%20improvement%20of%2027.5%25%20across%20tasks.%0Ahttps%3A//visuoskin.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Precise%252C%2520Contact-Rich%2520Manipulation%2520through%2520Uncalibrated%2520Tactile%250A%2520%2520Skins%26entry.906535625%3DVenkatesh%2520Pattabiraman%2520and%2520Yifeng%2520Cao%2520and%2520Siddhant%2520Haldar%2520and%2520Lerrel%2520Pinto%2520and%2520Raunaq%2520Bhirangi%26entry.1292438233%3D%2520%2520While%2520visuomotor%2520policy%2520learning%2520has%2520advanced%2520robotic%2520manipulation%252C%2520precisely%250Aexecuting%2520contact-rich%2520tasks%2520remains%2520challenging%2520due%2520to%2520the%2520limitations%2520of%250Avision%2520in%2520reasoning%2520about%2520physical%2520interactions.%2520To%2520address%2520this%252C%2520recent%2520work%250Ahas%2520sought%2520to%2520integrate%2520tactile%2520sensing%2520into%2520policy%2520learning.%2520However%252C%2520many%250Aexisting%2520approaches%2520rely%2520on%2520optical%2520tactile%2520sensors%2520that%2520are%2520either%2520restricted%250Ato%2520recognition%2520tasks%2520or%2520require%2520complex%2520dimensionality%2520reduction%2520steps%2520for%250Apolicy%2520learning.%2520In%2520this%2520work%252C%2520we%2520explore%2520learning%2520policies%2520with%2520magnetic%2520skin%250Asensors%252C%2520which%2520are%2520inherently%2520low-dimensional%252C%2520highly%2520sensitive%252C%2520and%250Ainexpensive%2520to%2520integrate%2520with%2520robotic%2520platforms.%2520To%2520leverage%2520these%2520sensors%250Aeffectively%252C%2520we%2520present%2520the%2520Visuo-Skin%2520%2528ViSk%2529%2520framework%252C%2520a%2520simple%2520approach%2520that%250Auses%2520a%2520transformer-based%2520policy%2520and%2520treats%2520skin%2520sensor%2520data%2520as%2520additional%250Atokens%2520alongside%2520visual%2520information.%2520Evaluated%2520on%2520four%2520complex%2520real-world%2520tasks%250Ainvolving%2520credit%2520card%2520swiping%252C%2520plug%2520insertion%252C%2520USB%2520insertion%252C%2520and%2520bookshelf%250Aretrieval%252C%2520ViSk%2520significantly%2520outperforms%2520both%2520vision-only%2520and%2520optical%2520tactile%250Asensing%2520based%2520policies.%2520Further%2520analysis%2520reveals%2520that%2520combining%2520tactile%2520and%250Avisual%2520modalities%2520enhances%2520policy%2520performance%2520and%2520spatial%2520generalization%252C%250Aachieving%2520an%2520average%2520improvement%2520of%252027.5%2525%2520across%2520tasks.%250Ahttps%253A//visuoskin.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Precise%2C%20Contact-Rich%20Manipulation%20through%20Uncalibrated%20Tactile%0A%20%20Skins&entry.906535625=Venkatesh%20Pattabiraman%20and%20Yifeng%20Cao%20and%20Siddhant%20Haldar%20and%20Lerrel%20Pinto%20and%20Raunaq%20Bhirangi&entry.1292438233=%20%20While%20visuomotor%20policy%20learning%20has%20advanced%20robotic%20manipulation%2C%20precisely%0Aexecuting%20contact-rich%20tasks%20remains%20challenging%20due%20to%20the%20limitations%20of%0Avision%20in%20reasoning%20about%20physical%20interactions.%20To%20address%20this%2C%20recent%20work%0Ahas%20sought%20to%20integrate%20tactile%20sensing%20into%20policy%20learning.%20However%2C%20many%0Aexisting%20approaches%20rely%20on%20optical%20tactile%20sensors%20that%20are%20either%20restricted%0Ato%20recognition%20tasks%20or%20require%20complex%20dimensionality%20reduction%20steps%20for%0Apolicy%20learning.%20In%20this%20work%2C%20we%20explore%20learning%20policies%20with%20magnetic%20skin%0Asensors%2C%20which%20are%20inherently%20low-dimensional%2C%20highly%20sensitive%2C%20and%0Ainexpensive%20to%20integrate%20with%20robotic%20platforms.%20To%20leverage%20these%20sensors%0Aeffectively%2C%20we%20present%20the%20Visuo-Skin%20%28ViSk%29%20framework%2C%20a%20simple%20approach%20that%0Auses%20a%20transformer-based%20policy%20and%20treats%20skin%20sensor%20data%20as%20additional%0Atokens%20alongside%20visual%20information.%20Evaluated%20on%20four%20complex%20real-world%20tasks%0Ainvolving%20credit%20card%20swiping%2C%20plug%20insertion%2C%20USB%20insertion%2C%20and%20bookshelf%0Aretrieval%2C%20ViSk%20significantly%20outperforms%20both%20vision-only%20and%20optical%20tactile%0Asensing%20based%20policies.%20Further%20analysis%20reveals%20that%20combining%20tactile%20and%0Avisual%20modalities%20enhances%20policy%20performance%20and%20spatial%20generalization%2C%0Aachieving%20an%20average%20improvement%20of%2027.5%25%20across%20tasks.%0Ahttps%3A//visuoskin.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17246v1&entry.124074799=Read"},
{"title": "Minimum-Violation Temporal Logic Planning for Heterogeneous Robots under\n  Robot Skill Failures", "author": "Samarth Kalluraya and Beichen Zhou and Yiannis Kantaros", "abstract": "  In this paper, we consider teams of robots with heterogeneous skills (e.g.,\nsensing and manipulation) tasked with collaborative missions described by\nLinear Temporal Logic (LTL) formulas. These LTL-encoded tasks require robots to\napply their skills to specific regions and objects in a temporal and logical\norder. While existing temporal logic planning algorithms can synthesize\ncorrect-by-construction paths, they typically lack reactivity to unexpected\nfailures of robot skills, which can compromise mission performance. This paper\naddresses this challenge by proposing a reactive LTL planning algorithm that\nadapts to unexpected failures during deployment. Specifically, the proposed\nalgorithm reassigns sub-tasks to robots based on their functioning skills and\nlocally revises team plans to accommodate these new assignments and ensure\nmission completion. The main novelty of the proposed algorithm is its ability\nto handle cases where mission completion becomes impossible due to limited\nfunctioning robots. Instead of reporting mission failure, the algorithm\nstrategically prioritizes the most crucial sub-tasks and locally revises the\nteam's plans, as per user-specified priorities, to minimize mission violations.\nWe provide theoretical conditions under which the proposed framework computes\nthe minimum violation task reassignments and team plans. We provide numerical\nand hardware experiments to demonstrate the efficiency of the proposed method.\n", "link": "http://arxiv.org/abs/2410.17188v1", "date": "2024-10-22", "relevancy": 1.4803, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5241}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimum-Violation%20Temporal%20Logic%20Planning%20for%20Heterogeneous%20Robots%20under%0A%20%20Robot%20Skill%20Failures&body=Title%3A%20Minimum-Violation%20Temporal%20Logic%20Planning%20for%20Heterogeneous%20Robots%20under%0A%20%20Robot%20Skill%20Failures%0AAuthor%3A%20Samarth%20Kalluraya%20and%20Beichen%20Zhou%20and%20Yiannis%20Kantaros%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20teams%20of%20robots%20with%20heterogeneous%20skills%20%28e.g.%2C%0Asensing%20and%20manipulation%29%20tasked%20with%20collaborative%20missions%20described%20by%0ALinear%20Temporal%20Logic%20%28LTL%29%20formulas.%20These%20LTL-encoded%20tasks%20require%20robots%20to%0Aapply%20their%20skills%20to%20specific%20regions%20and%20objects%20in%20a%20temporal%20and%20logical%0Aorder.%20While%20existing%20temporal%20logic%20planning%20algorithms%20can%20synthesize%0Acorrect-by-construction%20paths%2C%20they%20typically%20lack%20reactivity%20to%20unexpected%0Afailures%20of%20robot%20skills%2C%20which%20can%20compromise%20mission%20performance.%20This%20paper%0Aaddresses%20this%20challenge%20by%20proposing%20a%20reactive%20LTL%20planning%20algorithm%20that%0Aadapts%20to%20unexpected%20failures%20during%20deployment.%20Specifically%2C%20the%20proposed%0Aalgorithm%20reassigns%20sub-tasks%20to%20robots%20based%20on%20their%20functioning%20skills%20and%0Alocally%20revises%20team%20plans%20to%20accommodate%20these%20new%20assignments%20and%20ensure%0Amission%20completion.%20The%20main%20novelty%20of%20the%20proposed%20algorithm%20is%20its%20ability%0Ato%20handle%20cases%20where%20mission%20completion%20becomes%20impossible%20due%20to%20limited%0Afunctioning%20robots.%20Instead%20of%20reporting%20mission%20failure%2C%20the%20algorithm%0Astrategically%20prioritizes%20the%20most%20crucial%20sub-tasks%20and%20locally%20revises%20the%0Ateam%27s%20plans%2C%20as%20per%20user-specified%20priorities%2C%20to%20minimize%20mission%20violations.%0AWe%20provide%20theoretical%20conditions%20under%20which%20the%20proposed%20framework%20computes%0Athe%20minimum%20violation%20task%20reassignments%20and%20team%20plans.%20We%20provide%20numerical%0Aand%20hardware%20experiments%20to%20demonstrate%20the%20efficiency%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimum-Violation%2520Temporal%2520Logic%2520Planning%2520for%2520Heterogeneous%2520Robots%2520under%250A%2520%2520Robot%2520Skill%2520Failures%26entry.906535625%3DSamarth%2520Kalluraya%2520and%2520Beichen%2520Zhou%2520and%2520Yiannis%2520Kantaros%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520teams%2520of%2520robots%2520with%2520heterogeneous%2520skills%2520%2528e.g.%252C%250Asensing%2520and%2520manipulation%2529%2520tasked%2520with%2520collaborative%2520missions%2520described%2520by%250ALinear%2520Temporal%2520Logic%2520%2528LTL%2529%2520formulas.%2520These%2520LTL-encoded%2520tasks%2520require%2520robots%2520to%250Aapply%2520their%2520skills%2520to%2520specific%2520regions%2520and%2520objects%2520in%2520a%2520temporal%2520and%2520logical%250Aorder.%2520While%2520existing%2520temporal%2520logic%2520planning%2520algorithms%2520can%2520synthesize%250Acorrect-by-construction%2520paths%252C%2520they%2520typically%2520lack%2520reactivity%2520to%2520unexpected%250Afailures%2520of%2520robot%2520skills%252C%2520which%2520can%2520compromise%2520mission%2520performance.%2520This%2520paper%250Aaddresses%2520this%2520challenge%2520by%2520proposing%2520a%2520reactive%2520LTL%2520planning%2520algorithm%2520that%250Aadapts%2520to%2520unexpected%2520failures%2520during%2520deployment.%2520Specifically%252C%2520the%2520proposed%250Aalgorithm%2520reassigns%2520sub-tasks%2520to%2520robots%2520based%2520on%2520their%2520functioning%2520skills%2520and%250Alocally%2520revises%2520team%2520plans%2520to%2520accommodate%2520these%2520new%2520assignments%2520and%2520ensure%250Amission%2520completion.%2520The%2520main%2520novelty%2520of%2520the%2520proposed%2520algorithm%2520is%2520its%2520ability%250Ato%2520handle%2520cases%2520where%2520mission%2520completion%2520becomes%2520impossible%2520due%2520to%2520limited%250Afunctioning%2520robots.%2520Instead%2520of%2520reporting%2520mission%2520failure%252C%2520the%2520algorithm%250Astrategically%2520prioritizes%2520the%2520most%2520crucial%2520sub-tasks%2520and%2520locally%2520revises%2520the%250Ateam%2527s%2520plans%252C%2520as%2520per%2520user-specified%2520priorities%252C%2520to%2520minimize%2520mission%2520violations.%250AWe%2520provide%2520theoretical%2520conditions%2520under%2520which%2520the%2520proposed%2520framework%2520computes%250Athe%2520minimum%2520violation%2520task%2520reassignments%2520and%2520team%2520plans.%2520We%2520provide%2520numerical%250Aand%2520hardware%2520experiments%2520to%2520demonstrate%2520the%2520efficiency%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimum-Violation%20Temporal%20Logic%20Planning%20for%20Heterogeneous%20Robots%20under%0A%20%20Robot%20Skill%20Failures&entry.906535625=Samarth%20Kalluraya%20and%20Beichen%20Zhou%20and%20Yiannis%20Kantaros&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20teams%20of%20robots%20with%20heterogeneous%20skills%20%28e.g.%2C%0Asensing%20and%20manipulation%29%20tasked%20with%20collaborative%20missions%20described%20by%0ALinear%20Temporal%20Logic%20%28LTL%29%20formulas.%20These%20LTL-encoded%20tasks%20require%20robots%20to%0Aapply%20their%20skills%20to%20specific%20regions%20and%20objects%20in%20a%20temporal%20and%20logical%0Aorder.%20While%20existing%20temporal%20logic%20planning%20algorithms%20can%20synthesize%0Acorrect-by-construction%20paths%2C%20they%20typically%20lack%20reactivity%20to%20unexpected%0Afailures%20of%20robot%20skills%2C%20which%20can%20compromise%20mission%20performance.%20This%20paper%0Aaddresses%20this%20challenge%20by%20proposing%20a%20reactive%20LTL%20planning%20algorithm%20that%0Aadapts%20to%20unexpected%20failures%20during%20deployment.%20Specifically%2C%20the%20proposed%0Aalgorithm%20reassigns%20sub-tasks%20to%20robots%20based%20on%20their%20functioning%20skills%20and%0Alocally%20revises%20team%20plans%20to%20accommodate%20these%20new%20assignments%20and%20ensure%0Amission%20completion.%20The%20main%20novelty%20of%20the%20proposed%20algorithm%20is%20its%20ability%0Ato%20handle%20cases%20where%20mission%20completion%20becomes%20impossible%20due%20to%20limited%0Afunctioning%20robots.%20Instead%20of%20reporting%20mission%20failure%2C%20the%20algorithm%0Astrategically%20prioritizes%20the%20most%20crucial%20sub-tasks%20and%20locally%20revises%20the%0Ateam%27s%20plans%2C%20as%20per%20user-specified%20priorities%2C%20to%20minimize%20mission%20violations.%0AWe%20provide%20theoretical%20conditions%20under%20which%20the%20proposed%20framework%20computes%0Athe%20minimum%20violation%20task%20reassignments%20and%20team%20plans.%20We%20provide%20numerical%0Aand%20hardware%20experiments%20to%20demonstrate%20the%20efficiency%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17188v1&entry.124074799=Read"},
{"title": "A Historical Trajectory Assisted Optimization Method for Zeroth-Order\n  Federated Learning", "author": "Chenlin Wu and Xiaoyu He and Zike Li and Jing Gong and Zibin Zheng", "abstract": "  Federated learning heavily relies on distributed gradient descent techniques.\nIn the situation where gradient information is not available, the gradients\nneed to be estimated from zeroth-order information, which typically involves\ncomputing finite-differences along isotropic random directions. This method\nsuffers from high estimation errors, as the geometric features of the objective\nlandscape may be overlooked during the isotropic sampling. In this work, we\npropose a non-isotropic sampling method to improve the gradient estimation\nprocedure. Gradients in our method are estimated in a subspace spanned by\nhistorical trajectories of solutions, aiming to encourage the exploration of\npromising regions and hence improve the convergence. The proposed method uses a\ncovariance matrix for sampling which is a convex combination of two parts. The\nfirst part is a thin projection matrix containing the basis of the subspace\nwhich is designed to improve the exploitation ability. The second part is the\nhistorical trajectories. We implement this method in zeroth-order federated\nsettings, and show that the convergence rate aligns with existing ones while\nintroducing no significant overheads in communication or local computation. The\neffectiveness of our proposal is verified on several numerical experiments in\ncomparison to several commonly-used zeroth-order federated optimization\nalgorithms.\n", "link": "http://arxiv.org/abs/2409.15955v4", "date": "2024-10-22", "relevancy": 1.8901, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.479}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4694}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Historical%20Trajectory%20Assisted%20Optimization%20Method%20for%20Zeroth-Order%0A%20%20Federated%20Learning&body=Title%3A%20A%20Historical%20Trajectory%20Assisted%20Optimization%20Method%20for%20Zeroth-Order%0A%20%20Federated%20Learning%0AAuthor%3A%20Chenlin%20Wu%20and%20Xiaoyu%20He%20and%20Zike%20Li%20and%20Jing%20Gong%20and%20Zibin%20Zheng%0AAbstract%3A%20%20%20Federated%20learning%20heavily%20relies%20on%20distributed%20gradient%20descent%20techniques.%0AIn%20the%20situation%20where%20gradient%20information%20is%20not%20available%2C%20the%20gradients%0Aneed%20to%20be%20estimated%20from%20zeroth-order%20information%2C%20which%20typically%20involves%0Acomputing%20finite-differences%20along%20isotropic%20random%20directions.%20This%20method%0Asuffers%20from%20high%20estimation%20errors%2C%20as%20the%20geometric%20features%20of%20the%20objective%0Alandscape%20may%20be%20overlooked%20during%20the%20isotropic%20sampling.%20In%20this%20work%2C%20we%0Apropose%20a%20non-isotropic%20sampling%20method%20to%20improve%20the%20gradient%20estimation%0Aprocedure.%20Gradients%20in%20our%20method%20are%20estimated%20in%20a%20subspace%20spanned%20by%0Ahistorical%20trajectories%20of%20solutions%2C%20aiming%20to%20encourage%20the%20exploration%20of%0Apromising%20regions%20and%20hence%20improve%20the%20convergence.%20The%20proposed%20method%20uses%20a%0Acovariance%20matrix%20for%20sampling%20which%20is%20a%20convex%20combination%20of%20two%20parts.%20The%0Afirst%20part%20is%20a%20thin%20projection%20matrix%20containing%20the%20basis%20of%20the%20subspace%0Awhich%20is%20designed%20to%20improve%20the%20exploitation%20ability.%20The%20second%20part%20is%20the%0Ahistorical%20trajectories.%20We%20implement%20this%20method%20in%20zeroth-order%20federated%0Asettings%2C%20and%20show%20that%20the%20convergence%20rate%20aligns%20with%20existing%20ones%20while%0Aintroducing%20no%20significant%20overheads%20in%20communication%20or%20local%20computation.%20The%0Aeffectiveness%20of%20our%20proposal%20is%20verified%20on%20several%20numerical%20experiments%20in%0Acomparison%20to%20several%20commonly-used%20zeroth-order%20federated%20optimization%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15955v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Historical%2520Trajectory%2520Assisted%2520Optimization%2520Method%2520for%2520Zeroth-Order%250A%2520%2520Federated%2520Learning%26entry.906535625%3DChenlin%2520Wu%2520and%2520Xiaoyu%2520He%2520and%2520Zike%2520Li%2520and%2520Jing%2520Gong%2520and%2520Zibin%2520Zheng%26entry.1292438233%3D%2520%2520Federated%2520learning%2520heavily%2520relies%2520on%2520distributed%2520gradient%2520descent%2520techniques.%250AIn%2520the%2520situation%2520where%2520gradient%2520information%2520is%2520not%2520available%252C%2520the%2520gradients%250Aneed%2520to%2520be%2520estimated%2520from%2520zeroth-order%2520information%252C%2520which%2520typically%2520involves%250Acomputing%2520finite-differences%2520along%2520isotropic%2520random%2520directions.%2520This%2520method%250Asuffers%2520from%2520high%2520estimation%2520errors%252C%2520as%2520the%2520geometric%2520features%2520of%2520the%2520objective%250Alandscape%2520may%2520be%2520overlooked%2520during%2520the%2520isotropic%2520sampling.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520non-isotropic%2520sampling%2520method%2520to%2520improve%2520the%2520gradient%2520estimation%250Aprocedure.%2520Gradients%2520in%2520our%2520method%2520are%2520estimated%2520in%2520a%2520subspace%2520spanned%2520by%250Ahistorical%2520trajectories%2520of%2520solutions%252C%2520aiming%2520to%2520encourage%2520the%2520exploration%2520of%250Apromising%2520regions%2520and%2520hence%2520improve%2520the%2520convergence.%2520The%2520proposed%2520method%2520uses%2520a%250Acovariance%2520matrix%2520for%2520sampling%2520which%2520is%2520a%2520convex%2520combination%2520of%2520two%2520parts.%2520The%250Afirst%2520part%2520is%2520a%2520thin%2520projection%2520matrix%2520containing%2520the%2520basis%2520of%2520the%2520subspace%250Awhich%2520is%2520designed%2520to%2520improve%2520the%2520exploitation%2520ability.%2520The%2520second%2520part%2520is%2520the%250Ahistorical%2520trajectories.%2520We%2520implement%2520this%2520method%2520in%2520zeroth-order%2520federated%250Asettings%252C%2520and%2520show%2520that%2520the%2520convergence%2520rate%2520aligns%2520with%2520existing%2520ones%2520while%250Aintroducing%2520no%2520significant%2520overheads%2520in%2520communication%2520or%2520local%2520computation.%2520The%250Aeffectiveness%2520of%2520our%2520proposal%2520is%2520verified%2520on%2520several%2520numerical%2520experiments%2520in%250Acomparison%2520to%2520several%2520commonly-used%2520zeroth-order%2520federated%2520optimization%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15955v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Historical%20Trajectory%20Assisted%20Optimization%20Method%20for%20Zeroth-Order%0A%20%20Federated%20Learning&entry.906535625=Chenlin%20Wu%20and%20Xiaoyu%20He%20and%20Zike%20Li%20and%20Jing%20Gong%20and%20Zibin%20Zheng&entry.1292438233=%20%20Federated%20learning%20heavily%20relies%20on%20distributed%20gradient%20descent%20techniques.%0AIn%20the%20situation%20where%20gradient%20information%20is%20not%20available%2C%20the%20gradients%0Aneed%20to%20be%20estimated%20from%20zeroth-order%20information%2C%20which%20typically%20involves%0Acomputing%20finite-differences%20along%20isotropic%20random%20directions.%20This%20method%0Asuffers%20from%20high%20estimation%20errors%2C%20as%20the%20geometric%20features%20of%20the%20objective%0Alandscape%20may%20be%20overlooked%20during%20the%20isotropic%20sampling.%20In%20this%20work%2C%20we%0Apropose%20a%20non-isotropic%20sampling%20method%20to%20improve%20the%20gradient%20estimation%0Aprocedure.%20Gradients%20in%20our%20method%20are%20estimated%20in%20a%20subspace%20spanned%20by%0Ahistorical%20trajectories%20of%20solutions%2C%20aiming%20to%20encourage%20the%20exploration%20of%0Apromising%20regions%20and%20hence%20improve%20the%20convergence.%20The%20proposed%20method%20uses%20a%0Acovariance%20matrix%20for%20sampling%20which%20is%20a%20convex%20combination%20of%20two%20parts.%20The%0Afirst%20part%20is%20a%20thin%20projection%20matrix%20containing%20the%20basis%20of%20the%20subspace%0Awhich%20is%20designed%20to%20improve%20the%20exploitation%20ability.%20The%20second%20part%20is%20the%0Ahistorical%20trajectories.%20We%20implement%20this%20method%20in%20zeroth-order%20federated%0Asettings%2C%20and%20show%20that%20the%20convergence%20rate%20aligns%20with%20existing%20ones%20while%0Aintroducing%20no%20significant%20overheads%20in%20communication%20or%20local%20computation.%20The%0Aeffectiveness%20of%20our%20proposal%20is%20verified%20on%20several%20numerical%20experiments%20in%0Acomparison%20to%20several%20commonly-used%20zeroth-order%20federated%20optimization%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15955v4&entry.124074799=Read"},
{"title": "The Complexity of Optimizing Atomic Congestion", "author": "Cornelius Brand and Robert Ganian and Subrahmanyam Kalyanasundaram and Fionn Mc Inerney", "abstract": "  Atomic congestion games are a classic topic in network design, routing, and\nalgorithmic game theory, and are capable of modeling congestion and flow\noptimization tasks in various application areas. While both the price of\nanarchy for such games as well as the computational complexity of computing\ntheir Nash equilibria are by now well-understood, the computational complexity\nof computing a system-optimal set of strategies -- that is, a centrally planned\nrouting that minimizes the average cost of agents -- is severely understudied\nin the literature. We close this gap by identifying the exact boundaries of\ntractability for the problem through the lens of the parameterized complexity\nparadigm. After showing that the problem remains highly intractable even on\nextremely simple networks, we obtain a set of results which demonstrate that\nthe structural parameters which control the computational (in)tractability of\nthe problem are not vertex-separator based in nature (such as, e.g.,\ntreewidth), but rather based on edge separators. We conclude by extending our\nanalysis towards the (even more challenging) min-max variant of the problem.\n", "link": "http://arxiv.org/abs/2312.10219v2", "date": "2024-10-22", "relevancy": 1.139, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3872}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Complexity%20of%20Optimizing%20Atomic%20Congestion&body=Title%3A%20The%20Complexity%20of%20Optimizing%20Atomic%20Congestion%0AAuthor%3A%20Cornelius%20Brand%20and%20Robert%20Ganian%20and%20Subrahmanyam%20Kalyanasundaram%20and%20Fionn%20Mc%20Inerney%0AAbstract%3A%20%20%20Atomic%20congestion%20games%20are%20a%20classic%20topic%20in%20network%20design%2C%20routing%2C%20and%0Aalgorithmic%20game%20theory%2C%20and%20are%20capable%20of%20modeling%20congestion%20and%20flow%0Aoptimization%20tasks%20in%20various%20application%20areas.%20While%20both%20the%20price%20of%0Aanarchy%20for%20such%20games%20as%20well%20as%20the%20computational%20complexity%20of%20computing%0Atheir%20Nash%20equilibria%20are%20by%20now%20well-understood%2C%20the%20computational%20complexity%0Aof%20computing%20a%20system-optimal%20set%20of%20strategies%20--%20that%20is%2C%20a%20centrally%20planned%0Arouting%20that%20minimizes%20the%20average%20cost%20of%20agents%20--%20is%20severely%20understudied%0Ain%20the%20literature.%20We%20close%20this%20gap%20by%20identifying%20the%20exact%20boundaries%20of%0Atractability%20for%20the%20problem%20through%20the%20lens%20of%20the%20parameterized%20complexity%0Aparadigm.%20After%20showing%20that%20the%20problem%20remains%20highly%20intractable%20even%20on%0Aextremely%20simple%20networks%2C%20we%20obtain%20a%20set%20of%20results%20which%20demonstrate%20that%0Athe%20structural%20parameters%20which%20control%20the%20computational%20%28in%29tractability%20of%0Athe%20problem%20are%20not%20vertex-separator%20based%20in%20nature%20%28such%20as%2C%20e.g.%2C%0Atreewidth%29%2C%20but%20rather%20based%20on%20edge%20separators.%20We%20conclude%20by%20extending%20our%0Aanalysis%20towards%20the%20%28even%20more%20challenging%29%20min-max%20variant%20of%20the%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10219v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Complexity%2520of%2520Optimizing%2520Atomic%2520Congestion%26entry.906535625%3DCornelius%2520Brand%2520and%2520Robert%2520Ganian%2520and%2520Subrahmanyam%2520Kalyanasundaram%2520and%2520Fionn%2520Mc%2520Inerney%26entry.1292438233%3D%2520%2520Atomic%2520congestion%2520games%2520are%2520a%2520classic%2520topic%2520in%2520network%2520design%252C%2520routing%252C%2520and%250Aalgorithmic%2520game%2520theory%252C%2520and%2520are%2520capable%2520of%2520modeling%2520congestion%2520and%2520flow%250Aoptimization%2520tasks%2520in%2520various%2520application%2520areas.%2520While%2520both%2520the%2520price%2520of%250Aanarchy%2520for%2520such%2520games%2520as%2520well%2520as%2520the%2520computational%2520complexity%2520of%2520computing%250Atheir%2520Nash%2520equilibria%2520are%2520by%2520now%2520well-understood%252C%2520the%2520computational%2520complexity%250Aof%2520computing%2520a%2520system-optimal%2520set%2520of%2520strategies%2520--%2520that%2520is%252C%2520a%2520centrally%2520planned%250Arouting%2520that%2520minimizes%2520the%2520average%2520cost%2520of%2520agents%2520--%2520is%2520severely%2520understudied%250Ain%2520the%2520literature.%2520We%2520close%2520this%2520gap%2520by%2520identifying%2520the%2520exact%2520boundaries%2520of%250Atractability%2520for%2520the%2520problem%2520through%2520the%2520lens%2520of%2520the%2520parameterized%2520complexity%250Aparadigm.%2520After%2520showing%2520that%2520the%2520problem%2520remains%2520highly%2520intractable%2520even%2520on%250Aextremely%2520simple%2520networks%252C%2520we%2520obtain%2520a%2520set%2520of%2520results%2520which%2520demonstrate%2520that%250Athe%2520structural%2520parameters%2520which%2520control%2520the%2520computational%2520%2528in%2529tractability%2520of%250Athe%2520problem%2520are%2520not%2520vertex-separator%2520based%2520in%2520nature%2520%2528such%2520as%252C%2520e.g.%252C%250Atreewidth%2529%252C%2520but%2520rather%2520based%2520on%2520edge%2520separators.%2520We%2520conclude%2520by%2520extending%2520our%250Aanalysis%2520towards%2520the%2520%2528even%2520more%2520challenging%2529%2520min-max%2520variant%2520of%2520the%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10219v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Complexity%20of%20Optimizing%20Atomic%20Congestion&entry.906535625=Cornelius%20Brand%20and%20Robert%20Ganian%20and%20Subrahmanyam%20Kalyanasundaram%20and%20Fionn%20Mc%20Inerney&entry.1292438233=%20%20Atomic%20congestion%20games%20are%20a%20classic%20topic%20in%20network%20design%2C%20routing%2C%20and%0Aalgorithmic%20game%20theory%2C%20and%20are%20capable%20of%20modeling%20congestion%20and%20flow%0Aoptimization%20tasks%20in%20various%20application%20areas.%20While%20both%20the%20price%20of%0Aanarchy%20for%20such%20games%20as%20well%20as%20the%20computational%20complexity%20of%20computing%0Atheir%20Nash%20equilibria%20are%20by%20now%20well-understood%2C%20the%20computational%20complexity%0Aof%20computing%20a%20system-optimal%20set%20of%20strategies%20--%20that%20is%2C%20a%20centrally%20planned%0Arouting%20that%20minimizes%20the%20average%20cost%20of%20agents%20--%20is%20severely%20understudied%0Ain%20the%20literature.%20We%20close%20this%20gap%20by%20identifying%20the%20exact%20boundaries%20of%0Atractability%20for%20the%20problem%20through%20the%20lens%20of%20the%20parameterized%20complexity%0Aparadigm.%20After%20showing%20that%20the%20problem%20remains%20highly%20intractable%20even%20on%0Aextremely%20simple%20networks%2C%20we%20obtain%20a%20set%20of%20results%20which%20demonstrate%20that%0Athe%20structural%20parameters%20which%20control%20the%20computational%20%28in%29tractability%20of%0Athe%20problem%20are%20not%20vertex-separator%20based%20in%20nature%20%28such%20as%2C%20e.g.%2C%0Atreewidth%29%2C%20but%20rather%20based%20on%20edge%20separators.%20We%20conclude%20by%20extending%20our%0Aanalysis%20towards%20the%20%28even%20more%20challenging%29%20min-max%20variant%20of%20the%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10219v2&entry.124074799=Read"},
{"title": "Breaking the Memory Barrier: Near Infinite Batch Size Scaling for\n  Contrastive Loss", "author": "Zesen Cheng and Hang Zhang and Kehan Li and Sicong Leng and Zhiqiang Hu and Fei Wu and Deli Zhao and Xin Li and Lidong Bing", "abstract": "  Contrastive loss is a powerful approach for representation learning, where\nlarger batch sizes enhance performance by providing more negative samples to\nbetter distinguish between similar and dissimilar data. However, scaling batch\nsizes is constrained by the quadratic growth in GPU memory consumption,\nprimarily due to the full instantiation of the similarity matrix. To address\nthis, we propose a tile-based computation strategy that partitions the\ncontrastive loss calculation into arbitrary small blocks, avoiding full\nmaterialization of the similarity matrix. Furthermore, we introduce a\nmulti-level tiling strategy to leverage the hierarchical structure of\ndistributed systems, employing ring-based communication at the GPU level to\noptimize synchronization and fused kernels at the CUDA core level to reduce I/O\noverhead. Experimental results show that the proposed method scales batch sizes\nto unprecedented levels. For instance, it enables contrastive training of a\nCLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB\nwithout sacrificing any accuracy. Compared to SOTA memory-efficient solutions,\nit achieves a two-order-of-magnitude reduction in memory while maintaining\ncomparable speed. The code will be made publicly available.\n", "link": "http://arxiv.org/abs/2410.17243v1", "date": "2024-10-22", "relevancy": 1.6342, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5516}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5439}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Memory%20Barrier%3A%20Near%20Infinite%20Batch%20Size%20Scaling%20for%0A%20%20Contrastive%20Loss&body=Title%3A%20Breaking%20the%20Memory%20Barrier%3A%20Near%20Infinite%20Batch%20Size%20Scaling%20for%0A%20%20Contrastive%20Loss%0AAuthor%3A%20Zesen%20Cheng%20and%20Hang%20Zhang%20and%20Kehan%20Li%20and%20Sicong%20Leng%20and%20Zhiqiang%20Hu%20and%20Fei%20Wu%20and%20Deli%20Zhao%20and%20Xin%20Li%20and%20Lidong%20Bing%0AAbstract%3A%20%20%20Contrastive%20loss%20is%20a%20powerful%20approach%20for%20representation%20learning%2C%20where%0Alarger%20batch%20sizes%20enhance%20performance%20by%20providing%20more%20negative%20samples%20to%0Abetter%20distinguish%20between%20similar%20and%20dissimilar%20data.%20However%2C%20scaling%20batch%0Asizes%20is%20constrained%20by%20the%20quadratic%20growth%20in%20GPU%20memory%20consumption%2C%0Aprimarily%20due%20to%20the%20full%20instantiation%20of%20the%20similarity%20matrix.%20To%20address%0Athis%2C%20we%20propose%20a%20tile-based%20computation%20strategy%20that%20partitions%20the%0Acontrastive%20loss%20calculation%20into%20arbitrary%20small%20blocks%2C%20avoiding%20full%0Amaterialization%20of%20the%20similarity%20matrix.%20Furthermore%2C%20we%20introduce%20a%0Amulti-level%20tiling%20strategy%20to%20leverage%20the%20hierarchical%20structure%20of%0Adistributed%20systems%2C%20employing%20ring-based%20communication%20at%20the%20GPU%20level%20to%0Aoptimize%20synchronization%20and%20fused%20kernels%20at%20the%20CUDA%20core%20level%20to%20reduce%20I/O%0Aoverhead.%20Experimental%20results%20show%20that%20the%20proposed%20method%20scales%20batch%20sizes%0Ato%20unprecedented%20levels.%20For%20instance%2C%20it%20enables%20contrastive%20training%20of%20a%0ACLIP-ViT-L/14%20model%20with%20a%20batch%20size%20of%204M%20or%2012M%20using%208%20or%2032%20A800%2080GB%0Awithout%20sacrificing%20any%20accuracy.%20Compared%20to%20SOTA%20memory-efficient%20solutions%2C%0Ait%20achieves%20a%20two-order-of-magnitude%20reduction%20in%20memory%20while%20maintaining%0Acomparable%20speed.%20The%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Memory%2520Barrier%253A%2520Near%2520Infinite%2520Batch%2520Size%2520Scaling%2520for%250A%2520%2520Contrastive%2520Loss%26entry.906535625%3DZesen%2520Cheng%2520and%2520Hang%2520Zhang%2520and%2520Kehan%2520Li%2520and%2520Sicong%2520Leng%2520and%2520Zhiqiang%2520Hu%2520and%2520Fei%2520Wu%2520and%2520Deli%2520Zhao%2520and%2520Xin%2520Li%2520and%2520Lidong%2520Bing%26entry.1292438233%3D%2520%2520Contrastive%2520loss%2520is%2520a%2520powerful%2520approach%2520for%2520representation%2520learning%252C%2520where%250Alarger%2520batch%2520sizes%2520enhance%2520performance%2520by%2520providing%2520more%2520negative%2520samples%2520to%250Abetter%2520distinguish%2520between%2520similar%2520and%2520dissimilar%2520data.%2520However%252C%2520scaling%2520batch%250Asizes%2520is%2520constrained%2520by%2520the%2520quadratic%2520growth%2520in%2520GPU%2520memory%2520consumption%252C%250Aprimarily%2520due%2520to%2520the%2520full%2520instantiation%2520of%2520the%2520similarity%2520matrix.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520tile-based%2520computation%2520strategy%2520that%2520partitions%2520the%250Acontrastive%2520loss%2520calculation%2520into%2520arbitrary%2520small%2520blocks%252C%2520avoiding%2520full%250Amaterialization%2520of%2520the%2520similarity%2520matrix.%2520Furthermore%252C%2520we%2520introduce%2520a%250Amulti-level%2520tiling%2520strategy%2520to%2520leverage%2520the%2520hierarchical%2520structure%2520of%250Adistributed%2520systems%252C%2520employing%2520ring-based%2520communication%2520at%2520the%2520GPU%2520level%2520to%250Aoptimize%2520synchronization%2520and%2520fused%2520kernels%2520at%2520the%2520CUDA%2520core%2520level%2520to%2520reduce%2520I/O%250Aoverhead.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520method%2520scales%2520batch%2520sizes%250Ato%2520unprecedented%2520levels.%2520For%2520instance%252C%2520it%2520enables%2520contrastive%2520training%2520of%2520a%250ACLIP-ViT-L/14%2520model%2520with%2520a%2520batch%2520size%2520of%25204M%2520or%252012M%2520using%25208%2520or%252032%2520A800%252080GB%250Awithout%2520sacrificing%2520any%2520accuracy.%2520Compared%2520to%2520SOTA%2520memory-efficient%2520solutions%252C%250Ait%2520achieves%2520a%2520two-order-of-magnitude%2520reduction%2520in%2520memory%2520while%2520maintaining%250Acomparable%2520speed.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Memory%20Barrier%3A%20Near%20Infinite%20Batch%20Size%20Scaling%20for%0A%20%20Contrastive%20Loss&entry.906535625=Zesen%20Cheng%20and%20Hang%20Zhang%20and%20Kehan%20Li%20and%20Sicong%20Leng%20and%20Zhiqiang%20Hu%20and%20Fei%20Wu%20and%20Deli%20Zhao%20and%20Xin%20Li%20and%20Lidong%20Bing&entry.1292438233=%20%20Contrastive%20loss%20is%20a%20powerful%20approach%20for%20representation%20learning%2C%20where%0Alarger%20batch%20sizes%20enhance%20performance%20by%20providing%20more%20negative%20samples%20to%0Abetter%20distinguish%20between%20similar%20and%20dissimilar%20data.%20However%2C%20scaling%20batch%0Asizes%20is%20constrained%20by%20the%20quadratic%20growth%20in%20GPU%20memory%20consumption%2C%0Aprimarily%20due%20to%20the%20full%20instantiation%20of%20the%20similarity%20matrix.%20To%20address%0Athis%2C%20we%20propose%20a%20tile-based%20computation%20strategy%20that%20partitions%20the%0Acontrastive%20loss%20calculation%20into%20arbitrary%20small%20blocks%2C%20avoiding%20full%0Amaterialization%20of%20the%20similarity%20matrix.%20Furthermore%2C%20we%20introduce%20a%0Amulti-level%20tiling%20strategy%20to%20leverage%20the%20hierarchical%20structure%20of%0Adistributed%20systems%2C%20employing%20ring-based%20communication%20at%20the%20GPU%20level%20to%0Aoptimize%20synchronization%20and%20fused%20kernels%20at%20the%20CUDA%20core%20level%20to%20reduce%20I/O%0Aoverhead.%20Experimental%20results%20show%20that%20the%20proposed%20method%20scales%20batch%20sizes%0Ato%20unprecedented%20levels.%20For%20instance%2C%20it%20enables%20contrastive%20training%20of%20a%0ACLIP-ViT-L/14%20model%20with%20a%20batch%20size%20of%204M%20or%2012M%20using%208%20or%2032%20A800%2080GB%0Awithout%20sacrificing%20any%20accuracy.%20Compared%20to%20SOTA%20memory-efficient%20solutions%2C%0Ait%20achieves%20a%20two-order-of-magnitude%20reduction%20in%20memory%20while%20maintaining%0Acomparable%20speed.%20The%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17243v1&entry.124074799=Read"},
{"title": "A Novel Reinforcement Learning Model for Post-Incident Malware\n  Investigations", "author": "Dipo Dunsin and Mohamed Chahine Ghanem and Karim Ouazzane and Vassil Vassilev", "abstract": "  This Research proposes a Novel Reinforcement Learning (RL) model to optimise\nmalware forensics investigation during cyber incident response. It aims to\nimprove forensic investigation efficiency by reducing false negatives and\nadapting current practices to evolving malware signatures. The proposed RL\nframework leverages techniques such as Q-learning and the Markov Decision\nProcess (MDP) to train the system to identify malware patterns in live memory\ndumps, thereby automating forensic tasks. The RL model is based on a detailed\nmalware workflow diagram that guides the analysis of malware artefacts using\nstatic and behavioural techniques as well as machine learning algorithms.\nFurthermore, it seeks to address challenges in the UK justice system by\nensuring the accuracy of forensic evidence. We conduct testing and evaluation\nin controlled environments, using datasets created with Windows operating\nsystems to simulate malware infections. The experimental results demonstrate\nthat RL improves malware detection rates compared to conventional methods, with\nthe RL model's performance varying depending on the complexity and learning\nrate of the environment. The study concludes that while RL offers promising\npotential for automating malware forensics, its efficacy across diverse malware\ntypes requires ongoing refinement of reward systems and feature extraction\nmethods.\n", "link": "http://arxiv.org/abs/2410.15028v2", "date": "2024-10-22", "relevancy": 1.3634, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4685}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Reinforcement%20Learning%20Model%20for%20Post-Incident%20Malware%0A%20%20Investigations&body=Title%3A%20A%20Novel%20Reinforcement%20Learning%20Model%20for%20Post-Incident%20Malware%0A%20%20Investigations%0AAuthor%3A%20Dipo%20Dunsin%20and%20Mohamed%20Chahine%20Ghanem%20and%20Karim%20Ouazzane%20and%20Vassil%20Vassilev%0AAbstract%3A%20%20%20This%20Research%20proposes%20a%20Novel%20Reinforcement%20Learning%20%28RL%29%20model%20to%20optimise%0Amalware%20forensics%20investigation%20during%20cyber%20incident%20response.%20It%20aims%20to%0Aimprove%20forensic%20investigation%20efficiency%20by%20reducing%20false%20negatives%20and%0Aadapting%20current%20practices%20to%20evolving%20malware%20signatures.%20The%20proposed%20RL%0Aframework%20leverages%20techniques%20such%20as%20Q-learning%20and%20the%20Markov%20Decision%0AProcess%20%28MDP%29%20to%20train%20the%20system%20to%20identify%20malware%20patterns%20in%20live%20memory%0Adumps%2C%20thereby%20automating%20forensic%20tasks.%20The%20RL%20model%20is%20based%20on%20a%20detailed%0Amalware%20workflow%20diagram%20that%20guides%20the%20analysis%20of%20malware%20artefacts%20using%0Astatic%20and%20behavioural%20techniques%20as%20well%20as%20machine%20learning%20algorithms.%0AFurthermore%2C%20it%20seeks%20to%20address%20challenges%20in%20the%20UK%20justice%20system%20by%0Aensuring%20the%20accuracy%20of%20forensic%20evidence.%20We%20conduct%20testing%20and%20evaluation%0Ain%20controlled%20environments%2C%20using%20datasets%20created%20with%20Windows%20operating%0Asystems%20to%20simulate%20malware%20infections.%20The%20experimental%20results%20demonstrate%0Athat%20RL%20improves%20malware%20detection%20rates%20compared%20to%20conventional%20methods%2C%20with%0Athe%20RL%20model%27s%20performance%20varying%20depending%20on%20the%20complexity%20and%20learning%0Arate%20of%20the%20environment.%20The%20study%20concludes%20that%20while%20RL%20offers%20promising%0Apotential%20for%20automating%20malware%20forensics%2C%20its%20efficacy%20across%20diverse%20malware%0Atypes%20requires%20ongoing%20refinement%20of%20reward%20systems%20and%20feature%20extraction%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Reinforcement%2520Learning%2520Model%2520for%2520Post-Incident%2520Malware%250A%2520%2520Investigations%26entry.906535625%3DDipo%2520Dunsin%2520and%2520Mohamed%2520Chahine%2520Ghanem%2520and%2520Karim%2520Ouazzane%2520and%2520Vassil%2520Vassilev%26entry.1292438233%3D%2520%2520This%2520Research%2520proposes%2520a%2520Novel%2520Reinforcement%2520Learning%2520%2528RL%2529%2520model%2520to%2520optimise%250Amalware%2520forensics%2520investigation%2520during%2520cyber%2520incident%2520response.%2520It%2520aims%2520to%250Aimprove%2520forensic%2520investigation%2520efficiency%2520by%2520reducing%2520false%2520negatives%2520and%250Aadapting%2520current%2520practices%2520to%2520evolving%2520malware%2520signatures.%2520The%2520proposed%2520RL%250Aframework%2520leverages%2520techniques%2520such%2520as%2520Q-learning%2520and%2520the%2520Markov%2520Decision%250AProcess%2520%2528MDP%2529%2520to%2520train%2520the%2520system%2520to%2520identify%2520malware%2520patterns%2520in%2520live%2520memory%250Adumps%252C%2520thereby%2520automating%2520forensic%2520tasks.%2520The%2520RL%2520model%2520is%2520based%2520on%2520a%2520detailed%250Amalware%2520workflow%2520diagram%2520that%2520guides%2520the%2520analysis%2520of%2520malware%2520artefacts%2520using%250Astatic%2520and%2520behavioural%2520techniques%2520as%2520well%2520as%2520machine%2520learning%2520algorithms.%250AFurthermore%252C%2520it%2520seeks%2520to%2520address%2520challenges%2520in%2520the%2520UK%2520justice%2520system%2520by%250Aensuring%2520the%2520accuracy%2520of%2520forensic%2520evidence.%2520We%2520conduct%2520testing%2520and%2520evaluation%250Ain%2520controlled%2520environments%252C%2520using%2520datasets%2520created%2520with%2520Windows%2520operating%250Asystems%2520to%2520simulate%2520malware%2520infections.%2520The%2520experimental%2520results%2520demonstrate%250Athat%2520RL%2520improves%2520malware%2520detection%2520rates%2520compared%2520to%2520conventional%2520methods%252C%2520with%250Athe%2520RL%2520model%2527s%2520performance%2520varying%2520depending%2520on%2520the%2520complexity%2520and%2520learning%250Arate%2520of%2520the%2520environment.%2520The%2520study%2520concludes%2520that%2520while%2520RL%2520offers%2520promising%250Apotential%2520for%2520automating%2520malware%2520forensics%252C%2520its%2520efficacy%2520across%2520diverse%2520malware%250Atypes%2520requires%2520ongoing%2520refinement%2520of%2520reward%2520systems%2520and%2520feature%2520extraction%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Reinforcement%20Learning%20Model%20for%20Post-Incident%20Malware%0A%20%20Investigations&entry.906535625=Dipo%20Dunsin%20and%20Mohamed%20Chahine%20Ghanem%20and%20Karim%20Ouazzane%20and%20Vassil%20Vassilev&entry.1292438233=%20%20This%20Research%20proposes%20a%20Novel%20Reinforcement%20Learning%20%28RL%29%20model%20to%20optimise%0Amalware%20forensics%20investigation%20during%20cyber%20incident%20response.%20It%20aims%20to%0Aimprove%20forensic%20investigation%20efficiency%20by%20reducing%20false%20negatives%20and%0Aadapting%20current%20practices%20to%20evolving%20malware%20signatures.%20The%20proposed%20RL%0Aframework%20leverages%20techniques%20such%20as%20Q-learning%20and%20the%20Markov%20Decision%0AProcess%20%28MDP%29%20to%20train%20the%20system%20to%20identify%20malware%20patterns%20in%20live%20memory%0Adumps%2C%20thereby%20automating%20forensic%20tasks.%20The%20RL%20model%20is%20based%20on%20a%20detailed%0Amalware%20workflow%20diagram%20that%20guides%20the%20analysis%20of%20malware%20artefacts%20using%0Astatic%20and%20behavioural%20techniques%20as%20well%20as%20machine%20learning%20algorithms.%0AFurthermore%2C%20it%20seeks%20to%20address%20challenges%20in%20the%20UK%20justice%20system%20by%0Aensuring%20the%20accuracy%20of%20forensic%20evidence.%20We%20conduct%20testing%20and%20evaluation%0Ain%20controlled%20environments%2C%20using%20datasets%20created%20with%20Windows%20operating%0Asystems%20to%20simulate%20malware%20infections.%20The%20experimental%20results%20demonstrate%0Athat%20RL%20improves%20malware%20detection%20rates%20compared%20to%20conventional%20methods%2C%20with%0Athe%20RL%20model%27s%20performance%20varying%20depending%20on%20the%20complexity%20and%20learning%0Arate%20of%20the%20environment.%20The%20study%20concludes%20that%20while%20RL%20offers%20promising%0Apotential%20for%20automating%20malware%20forensics%2C%20its%20efficacy%20across%20diverse%20malware%0Atypes%20requires%20ongoing%20refinement%20of%20reward%20systems%20and%20feature%20extraction%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15028v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


