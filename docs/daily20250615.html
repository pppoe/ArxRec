<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250612.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied\n  Intelligence", "author": "Wang Xinjie and Liu Liu and Cao Yu and Wu Ruiqi and Qin Wenkang and Wang Dehui and Sui Wei and Su Zhizhong", "abstract": "  Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.\n", "link": "http://arxiv.org/abs/2506.10600v1", "date": "2025-06-12", "relevancy": 3.252, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6697}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6686}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmbodiedGen%3A%20Towards%20a%20Generative%203D%20World%20Engine%20for%20Embodied%0A%20%20Intelligence&body=Title%3A%20EmbodiedGen%3A%20Towards%20a%20Generative%203D%20World%20Engine%20for%20Embodied%0A%20%20Intelligence%0AAuthor%3A%20Wang%20Xinjie%20and%20Liu%20Liu%20and%20Cao%20Yu%20and%20Wu%20Ruiqi%20and%20Qin%20Wenkang%20and%20Wang%20Dehui%20and%20Sui%20Wei%20and%20Su%20Zhizhong%0AAbstract%3A%20%20%20Constructing%20a%20physically%20realistic%20and%20accurately%20scaled%20simulated%203D%20world%0Ais%20crucial%20for%20the%20training%20and%20evaluation%20of%20embodied%20intelligence%20tasks.%20The%0Adiversity%2C%20realism%2C%20low%20cost%20accessibility%20and%20affordability%20of%203D%20data%20assets%0Aare%20critical%20for%20achieving%20generalization%20and%20scalability%20in%20embodied%20AI.%0AHowever%2C%20most%20current%20embodied%20intelligence%20tasks%20still%20rely%20heavily%20on%0Atraditional%203D%20computer%20graphics%20assets%20manually%20created%20and%20annotated%2C%20which%0Asuffer%20from%20high%20production%20costs%20and%20limited%20realism.%20These%20limitations%0Asignificantly%20hinder%20the%20scalability%20of%20data%20driven%20approaches.%20We%20present%0AEmbodiedGen%2C%20a%20foundational%20platform%20for%20interactive%203D%20world%20generation.%20It%0Aenables%20the%20scalable%20generation%20of%20high-quality%2C%20controllable%20and%0Aphotorealistic%203D%20assets%20with%20accurate%20physical%20properties%20and%20real-world%20scale%0Ain%20the%20Unified%20Robotics%20Description%20Format%20%28URDF%29%20at%20low%20cost.%20These%20assets%20can%0Abe%20directly%20imported%20into%20various%20physics%20simulation%20engines%20for%20fine-grained%0Aphysical%20control%2C%20supporting%20downstream%20tasks%20in%20training%20and%20evaluation.%0AEmbodiedGen%20is%20an%20easy-to-use%2C%20full-featured%20toolkit%20composed%20of%20six%20key%0Amodules%3A%20Image-to-3D%2C%20Text-to-3D%2C%20Texture%20Generation%2C%20Articulated%20Object%0AGeneration%2C%20Scene%20Generation%20and%20Layout%20Generation.%20EmbodiedGen%20generates%0Adiverse%20and%20interactive%203D%20worlds%20composed%20of%20generative%203D%20assets%2C%20leveraging%0Agenerative%20AI%20to%20address%20the%20challenges%20of%20generalization%20and%20evaluation%20to%20the%0Aneeds%20of%20embodied%20intelligence%20related%20research.%20Code%20is%20available%20at%0Ahttps%3A//horizonrobotics.github.io/robot_lab/embodied_gen/index.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodiedGen%253A%2520Towards%2520a%2520Generative%25203D%2520World%2520Engine%2520for%2520Embodied%250A%2520%2520Intelligence%26entry.906535625%3DWang%2520Xinjie%2520and%2520Liu%2520Liu%2520and%2520Cao%2520Yu%2520and%2520Wu%2520Ruiqi%2520and%2520Qin%2520Wenkang%2520and%2520Wang%2520Dehui%2520and%2520Sui%2520Wei%2520and%2520Su%2520Zhizhong%26entry.1292438233%3D%2520%2520Constructing%2520a%2520physically%2520realistic%2520and%2520accurately%2520scaled%2520simulated%25203D%2520world%250Ais%2520crucial%2520for%2520the%2520training%2520and%2520evaluation%2520of%2520embodied%2520intelligence%2520tasks.%2520The%250Adiversity%252C%2520realism%252C%2520low%2520cost%2520accessibility%2520and%2520affordability%2520of%25203D%2520data%2520assets%250Aare%2520critical%2520for%2520achieving%2520generalization%2520and%2520scalability%2520in%2520embodied%2520AI.%250AHowever%252C%2520most%2520current%2520embodied%2520intelligence%2520tasks%2520still%2520rely%2520heavily%2520on%250Atraditional%25203D%2520computer%2520graphics%2520assets%2520manually%2520created%2520and%2520annotated%252C%2520which%250Asuffer%2520from%2520high%2520production%2520costs%2520and%2520limited%2520realism.%2520These%2520limitations%250Asignificantly%2520hinder%2520the%2520scalability%2520of%2520data%2520driven%2520approaches.%2520We%2520present%250AEmbodiedGen%252C%2520a%2520foundational%2520platform%2520for%2520interactive%25203D%2520world%2520generation.%2520It%250Aenables%2520the%2520scalable%2520generation%2520of%2520high-quality%252C%2520controllable%2520and%250Aphotorealistic%25203D%2520assets%2520with%2520accurate%2520physical%2520properties%2520and%2520real-world%2520scale%250Ain%2520the%2520Unified%2520Robotics%2520Description%2520Format%2520%2528URDF%2529%2520at%2520low%2520cost.%2520These%2520assets%2520can%250Abe%2520directly%2520imported%2520into%2520various%2520physics%2520simulation%2520engines%2520for%2520fine-grained%250Aphysical%2520control%252C%2520supporting%2520downstream%2520tasks%2520in%2520training%2520and%2520evaluation.%250AEmbodiedGen%2520is%2520an%2520easy-to-use%252C%2520full-featured%2520toolkit%2520composed%2520of%2520six%2520key%250Amodules%253A%2520Image-to-3D%252C%2520Text-to-3D%252C%2520Texture%2520Generation%252C%2520Articulated%2520Object%250AGeneration%252C%2520Scene%2520Generation%2520and%2520Layout%2520Generation.%2520EmbodiedGen%2520generates%250Adiverse%2520and%2520interactive%25203D%2520worlds%2520composed%2520of%2520generative%25203D%2520assets%252C%2520leveraging%250Agenerative%2520AI%2520to%2520address%2520the%2520challenges%2520of%2520generalization%2520and%2520evaluation%2520to%2520the%250Aneeds%2520of%2520embodied%2520intelligence%2520related%2520research.%2520Code%2520is%2520available%2520at%250Ahttps%253A//horizonrobotics.github.io/robot_lab/embodied_gen/index.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmbodiedGen%3A%20Towards%20a%20Generative%203D%20World%20Engine%20for%20Embodied%0A%20%20Intelligence&entry.906535625=Wang%20Xinjie%20and%20Liu%20Liu%20and%20Cao%20Yu%20and%20Wu%20Ruiqi%20and%20Qin%20Wenkang%20and%20Wang%20Dehui%20and%20Sui%20Wei%20and%20Su%20Zhizhong&entry.1292438233=%20%20Constructing%20a%20physically%20realistic%20and%20accurately%20scaled%20simulated%203D%20world%0Ais%20crucial%20for%20the%20training%20and%20evaluation%20of%20embodied%20intelligence%20tasks.%20The%0Adiversity%2C%20realism%2C%20low%20cost%20accessibility%20and%20affordability%20of%203D%20data%20assets%0Aare%20critical%20for%20achieving%20generalization%20and%20scalability%20in%20embodied%20AI.%0AHowever%2C%20most%20current%20embodied%20intelligence%20tasks%20still%20rely%20heavily%20on%0Atraditional%203D%20computer%20graphics%20assets%20manually%20created%20and%20annotated%2C%20which%0Asuffer%20from%20high%20production%20costs%20and%20limited%20realism.%20These%20limitations%0Asignificantly%20hinder%20the%20scalability%20of%20data%20driven%20approaches.%20We%20present%0AEmbodiedGen%2C%20a%20foundational%20platform%20for%20interactive%203D%20world%20generation.%20It%0Aenables%20the%20scalable%20generation%20of%20high-quality%2C%20controllable%20and%0Aphotorealistic%203D%20assets%20with%20accurate%20physical%20properties%20and%20real-world%20scale%0Ain%20the%20Unified%20Robotics%20Description%20Format%20%28URDF%29%20at%20low%20cost.%20These%20assets%20can%0Abe%20directly%20imported%20into%20various%20physics%20simulation%20engines%20for%20fine-grained%0Aphysical%20control%2C%20supporting%20downstream%20tasks%20in%20training%20and%20evaluation.%0AEmbodiedGen%20is%20an%20easy-to-use%2C%20full-featured%20toolkit%20composed%20of%20six%20key%0Amodules%3A%20Image-to-3D%2C%20Text-to-3D%2C%20Texture%20Generation%2C%20Articulated%20Object%0AGeneration%2C%20Scene%20Generation%20and%20Layout%20Generation.%20EmbodiedGen%20generates%0Adiverse%20and%20interactive%203D%20worlds%20composed%20of%20generative%203D%20assets%2C%20leveraging%0Agenerative%20AI%20to%20address%20the%20challenges%20of%20generalization%20and%20evaluation%20to%20the%0Aneeds%20of%20embodied%20intelligence%20related%20research.%20Code%20is%20available%20at%0Ahttps%3A//horizonrobotics.github.io/robot_lab/embodied_gen/index.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10600v1&entry.124074799=Read"},
{"title": "LRSLAM: Low-rank Representation of Signed Distance Fields in Dense\n  Visual SLAM System", "author": "Hongbeen Park and Minjeong Park and Giljoo Nam and Jinkyu Kim", "abstract": "  Simultaneous Localization and Mapping (SLAM) has been crucial across various\ndomains, including autonomous driving, mobile robotics, and mixed reality.\nDense visual SLAM, leveraging RGB-D camera systems, offers advantages but faces\nchallenges in achieving real-time performance, robustness, and scalability for\nlarge-scale scenes. Recent approaches utilizing neural implicit scene\nrepresentations show promise but suffer from high computational costs and\nmemory requirements. ESLAM introduced a plane-based tensor decomposition but\nstill struggled with memory growth. Addressing these challenges, we propose a\nmore efficient visual SLAM model, called LRSLAM, utilizing low-rank tensor\ndecomposition methods. Our approach, leveraging the Six-axis and CP\ndecompositions, achieves better convergence rates, memory efficiency, and\nreconstruction/localization quality than existing state-of-the-art approaches.\nEvaluation across diverse indoor RGB-D datasets demonstrates LRSLAM's superior\nperformance in terms of parameter efficiency, processing time, and accuracy,\nretaining reconstruction and localization quality. Our code will be publicly\navailable upon publication.\n", "link": "http://arxiv.org/abs/2506.10567v1", "date": "2025-06-12", "relevancy": 3.2113, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7272}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6222}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LRSLAM%3A%20Low-rank%20Representation%20of%20Signed%20Distance%20Fields%20in%20Dense%0A%20%20Visual%20SLAM%20System&body=Title%3A%20LRSLAM%3A%20Low-rank%20Representation%20of%20Signed%20Distance%20Fields%20in%20Dense%0A%20%20Visual%20SLAM%20System%0AAuthor%3A%20Hongbeen%20Park%20and%20Minjeong%20Park%20and%20Giljoo%20Nam%20and%20Jinkyu%20Kim%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20has%20been%20crucial%20across%20various%0Adomains%2C%20including%20autonomous%20driving%2C%20mobile%20robotics%2C%20and%20mixed%20reality.%0ADense%20visual%20SLAM%2C%20leveraging%20RGB-D%20camera%20systems%2C%20offers%20advantages%20but%20faces%0Achallenges%20in%20achieving%20real-time%20performance%2C%20robustness%2C%20and%20scalability%20for%0Alarge-scale%20scenes.%20Recent%20approaches%20utilizing%20neural%20implicit%20scene%0Arepresentations%20show%20promise%20but%20suffer%20from%20high%20computational%20costs%20and%0Amemory%20requirements.%20ESLAM%20introduced%20a%20plane-based%20tensor%20decomposition%20but%0Astill%20struggled%20with%20memory%20growth.%20Addressing%20these%20challenges%2C%20we%20propose%20a%0Amore%20efficient%20visual%20SLAM%20model%2C%20called%20LRSLAM%2C%20utilizing%20low-rank%20tensor%0Adecomposition%20methods.%20Our%20approach%2C%20leveraging%20the%20Six-axis%20and%20CP%0Adecompositions%2C%20achieves%20better%20convergence%20rates%2C%20memory%20efficiency%2C%20and%0Areconstruction/localization%20quality%20than%20existing%20state-of-the-art%20approaches.%0AEvaluation%20across%20diverse%20indoor%20RGB-D%20datasets%20demonstrates%20LRSLAM%27s%20superior%0Aperformance%20in%20terms%20of%20parameter%20efficiency%2C%20processing%20time%2C%20and%20accuracy%2C%0Aretaining%20reconstruction%20and%20localization%20quality.%20Our%20code%20will%20be%20publicly%0Aavailable%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLRSLAM%253A%2520Low-rank%2520Representation%2520of%2520Signed%2520Distance%2520Fields%2520in%2520Dense%250A%2520%2520Visual%2520SLAM%2520System%26entry.906535625%3DHongbeen%2520Park%2520and%2520Minjeong%2520Park%2520and%2520Giljoo%2520Nam%2520and%2520Jinkyu%2520Kim%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520has%2520been%2520crucial%2520across%2520various%250Adomains%252C%2520including%2520autonomous%2520driving%252C%2520mobile%2520robotics%252C%2520and%2520mixed%2520reality.%250ADense%2520visual%2520SLAM%252C%2520leveraging%2520RGB-D%2520camera%2520systems%252C%2520offers%2520advantages%2520but%2520faces%250Achallenges%2520in%2520achieving%2520real-time%2520performance%252C%2520robustness%252C%2520and%2520scalability%2520for%250Alarge-scale%2520scenes.%2520Recent%2520approaches%2520utilizing%2520neural%2520implicit%2520scene%250Arepresentations%2520show%2520promise%2520but%2520suffer%2520from%2520high%2520computational%2520costs%2520and%250Amemory%2520requirements.%2520ESLAM%2520introduced%2520a%2520plane-based%2520tensor%2520decomposition%2520but%250Astill%2520struggled%2520with%2520memory%2520growth.%2520Addressing%2520these%2520challenges%252C%2520we%2520propose%2520a%250Amore%2520efficient%2520visual%2520SLAM%2520model%252C%2520called%2520LRSLAM%252C%2520utilizing%2520low-rank%2520tensor%250Adecomposition%2520methods.%2520Our%2520approach%252C%2520leveraging%2520the%2520Six-axis%2520and%2520CP%250Adecompositions%252C%2520achieves%2520better%2520convergence%2520rates%252C%2520memory%2520efficiency%252C%2520and%250Areconstruction/localization%2520quality%2520than%2520existing%2520state-of-the-art%2520approaches.%250AEvaluation%2520across%2520diverse%2520indoor%2520RGB-D%2520datasets%2520demonstrates%2520LRSLAM%2527s%2520superior%250Aperformance%2520in%2520terms%2520of%2520parameter%2520efficiency%252C%2520processing%2520time%252C%2520and%2520accuracy%252C%250Aretaining%2520reconstruction%2520and%2520localization%2520quality.%2520Our%2520code%2520will%2520be%2520publicly%250Aavailable%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LRSLAM%3A%20Low-rank%20Representation%20of%20Signed%20Distance%20Fields%20in%20Dense%0A%20%20Visual%20SLAM%20System&entry.906535625=Hongbeen%20Park%20and%20Minjeong%20Park%20and%20Giljoo%20Nam%20and%20Jinkyu%20Kim&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20has%20been%20crucial%20across%20various%0Adomains%2C%20including%20autonomous%20driving%2C%20mobile%20robotics%2C%20and%20mixed%20reality.%0ADense%20visual%20SLAM%2C%20leveraging%20RGB-D%20camera%20systems%2C%20offers%20advantages%20but%20faces%0Achallenges%20in%20achieving%20real-time%20performance%2C%20robustness%2C%20and%20scalability%20for%0Alarge-scale%20scenes.%20Recent%20approaches%20utilizing%20neural%20implicit%20scene%0Arepresentations%20show%20promise%20but%20suffer%20from%20high%20computational%20costs%20and%0Amemory%20requirements.%20ESLAM%20introduced%20a%20plane-based%20tensor%20decomposition%20but%0Astill%20struggled%20with%20memory%20growth.%20Addressing%20these%20challenges%2C%20we%20propose%20a%0Amore%20efficient%20visual%20SLAM%20model%2C%20called%20LRSLAM%2C%20utilizing%20low-rank%20tensor%0Adecomposition%20methods.%20Our%20approach%2C%20leveraging%20the%20Six-axis%20and%20CP%0Adecompositions%2C%20achieves%20better%20convergence%20rates%2C%20memory%20efficiency%2C%20and%0Areconstruction/localization%20quality%20than%20existing%20state-of-the-art%20approaches.%0AEvaluation%20across%20diverse%20indoor%20RGB-D%20datasets%20demonstrates%20LRSLAM%27s%20superior%0Aperformance%20in%20terms%20of%20parameter%20efficiency%2C%20processing%20time%2C%20and%20accuracy%2C%0Aretaining%20reconstruction%20and%20localization%20quality.%20Our%20code%20will%20be%20publicly%0Aavailable%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10567v1&entry.124074799=Read"},
{"title": "ODG: Occupancy Prediction Using Dual Gaussians", "author": "Yunxiao Shi and Yinhao Zhu and Shizhong Han and Jisoo Jeong and Amin Ansari and Hong Cai and Fatih Porikli", "abstract": "  Occupancy prediction infers fine-grained 3D geometry and semantics from\ncamera images of the surrounding environment, making it a critical perception\ntask for autonomous driving. Existing methods either adopt dense grids as scene\nrepresentation, which is difficult to scale to high resolution, or learn the\nentire scene using a single set of sparse queries, which is insufficient to\nhandle the various object characteristics. In this paper, we present ODG, a\nhierarchical dual sparse Gaussian representation to effectively capture complex\nscene dynamics. Building upon the observation that driving scenes can be\nuniversally decomposed into static and dynamic counterparts, we define dual\nGaussian queries to better model the diverse scene objects. We utilize a\nhierarchical Gaussian transformer to predict the occupied voxel centers and\nsemantic classes along with the Gaussian parameters. Leveraging the real-time\nrendering capability of 3D Gaussian Splatting, we also impose rendering\nsupervision with available depth and semantic map annotations injecting\npixel-level alignment to boost occupancy learning. Extensive experiments on the\nOcc3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets\nnew state-of-the-art results while maintaining low inference cost.\n", "link": "http://arxiv.org/abs/2506.09417v2", "date": "2025-06-12", "relevancy": 3.21, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6561}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6409}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ODG%3A%20Occupancy%20Prediction%20Using%20Dual%20Gaussians&body=Title%3A%20ODG%3A%20Occupancy%20Prediction%20Using%20Dual%20Gaussians%0AAuthor%3A%20Yunxiao%20Shi%20and%20Yinhao%20Zhu%20and%20Shizhong%20Han%20and%20Jisoo%20Jeong%20and%20Amin%20Ansari%20and%20Hong%20Cai%20and%20Fatih%20Porikli%0AAbstract%3A%20%20%20Occupancy%20prediction%20infers%20fine-grained%203D%20geometry%20and%20semantics%20from%0Acamera%20images%20of%20the%20surrounding%20environment%2C%20making%20it%20a%20critical%20perception%0Atask%20for%20autonomous%20driving.%20Existing%20methods%20either%20adopt%20dense%20grids%20as%20scene%0Arepresentation%2C%20which%20is%20difficult%20to%20scale%20to%20high%20resolution%2C%20or%20learn%20the%0Aentire%20scene%20using%20a%20single%20set%20of%20sparse%20queries%2C%20which%20is%20insufficient%20to%0Ahandle%20the%20various%20object%20characteristics.%20In%20this%20paper%2C%20we%20present%20ODG%2C%20a%0Ahierarchical%20dual%20sparse%20Gaussian%20representation%20to%20effectively%20capture%20complex%0Ascene%20dynamics.%20Building%20upon%20the%20observation%20that%20driving%20scenes%20can%20be%0Auniversally%20decomposed%20into%20static%20and%20dynamic%20counterparts%2C%20we%20define%20dual%0AGaussian%20queries%20to%20better%20model%20the%20diverse%20scene%20objects.%20We%20utilize%20a%0Ahierarchical%20Gaussian%20transformer%20to%20predict%20the%20occupied%20voxel%20centers%20and%0Asemantic%20classes%20along%20with%20the%20Gaussian%20parameters.%20Leveraging%20the%20real-time%0Arendering%20capability%20of%203D%20Gaussian%20Splatting%2C%20we%20also%20impose%20rendering%0Asupervision%20with%20available%20depth%20and%20semantic%20map%20annotations%20injecting%0Apixel-level%20alignment%20to%20boost%20occupancy%20learning.%20Extensive%20experiments%20on%20the%0AOcc3D-nuScenes%20and%20Occ3D-Waymo%20benchmarks%20demonstrate%20our%20proposed%20method%20sets%0Anew%20state-of-the-art%20results%20while%20maintaining%20low%20inference%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DODG%253A%2520Occupancy%2520Prediction%2520Using%2520Dual%2520Gaussians%26entry.906535625%3DYunxiao%2520Shi%2520and%2520Yinhao%2520Zhu%2520and%2520Shizhong%2520Han%2520and%2520Jisoo%2520Jeong%2520and%2520Amin%2520Ansari%2520and%2520Hong%2520Cai%2520and%2520Fatih%2520Porikli%26entry.1292438233%3D%2520%2520Occupancy%2520prediction%2520infers%2520fine-grained%25203D%2520geometry%2520and%2520semantics%2520from%250Acamera%2520images%2520of%2520the%2520surrounding%2520environment%252C%2520making%2520it%2520a%2520critical%2520perception%250Atask%2520for%2520autonomous%2520driving.%2520Existing%2520methods%2520either%2520adopt%2520dense%2520grids%2520as%2520scene%250Arepresentation%252C%2520which%2520is%2520difficult%2520to%2520scale%2520to%2520high%2520resolution%252C%2520or%2520learn%2520the%250Aentire%2520scene%2520using%2520a%2520single%2520set%2520of%2520sparse%2520queries%252C%2520which%2520is%2520insufficient%2520to%250Ahandle%2520the%2520various%2520object%2520characteristics.%2520In%2520this%2520paper%252C%2520we%2520present%2520ODG%252C%2520a%250Ahierarchical%2520dual%2520sparse%2520Gaussian%2520representation%2520to%2520effectively%2520capture%2520complex%250Ascene%2520dynamics.%2520Building%2520upon%2520the%2520observation%2520that%2520driving%2520scenes%2520can%2520be%250Auniversally%2520decomposed%2520into%2520static%2520and%2520dynamic%2520counterparts%252C%2520we%2520define%2520dual%250AGaussian%2520queries%2520to%2520better%2520model%2520the%2520diverse%2520scene%2520objects.%2520We%2520utilize%2520a%250Ahierarchical%2520Gaussian%2520transformer%2520to%2520predict%2520the%2520occupied%2520voxel%2520centers%2520and%250Asemantic%2520classes%2520along%2520with%2520the%2520Gaussian%2520parameters.%2520Leveraging%2520the%2520real-time%250Arendering%2520capability%2520of%25203D%2520Gaussian%2520Splatting%252C%2520we%2520also%2520impose%2520rendering%250Asupervision%2520with%2520available%2520depth%2520and%2520semantic%2520map%2520annotations%2520injecting%250Apixel-level%2520alignment%2520to%2520boost%2520occupancy%2520learning.%2520Extensive%2520experiments%2520on%2520the%250AOcc3D-nuScenes%2520and%2520Occ3D-Waymo%2520benchmarks%2520demonstrate%2520our%2520proposed%2520method%2520sets%250Anew%2520state-of-the-art%2520results%2520while%2520maintaining%2520low%2520inference%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODG%3A%20Occupancy%20Prediction%20Using%20Dual%20Gaussians&entry.906535625=Yunxiao%20Shi%20and%20Yinhao%20Zhu%20and%20Shizhong%20Han%20and%20Jisoo%20Jeong%20and%20Amin%20Ansari%20and%20Hong%20Cai%20and%20Fatih%20Porikli&entry.1292438233=%20%20Occupancy%20prediction%20infers%20fine-grained%203D%20geometry%20and%20semantics%20from%0Acamera%20images%20of%20the%20surrounding%20environment%2C%20making%20it%20a%20critical%20perception%0Atask%20for%20autonomous%20driving.%20Existing%20methods%20either%20adopt%20dense%20grids%20as%20scene%0Arepresentation%2C%20which%20is%20difficult%20to%20scale%20to%20high%20resolution%2C%20or%20learn%20the%0Aentire%20scene%20using%20a%20single%20set%20of%20sparse%20queries%2C%20which%20is%20insufficient%20to%0Ahandle%20the%20various%20object%20characteristics.%20In%20this%20paper%2C%20we%20present%20ODG%2C%20a%0Ahierarchical%20dual%20sparse%20Gaussian%20representation%20to%20effectively%20capture%20complex%0Ascene%20dynamics.%20Building%20upon%20the%20observation%20that%20driving%20scenes%20can%20be%0Auniversally%20decomposed%20into%20static%20and%20dynamic%20counterparts%2C%20we%20define%20dual%0AGaussian%20queries%20to%20better%20model%20the%20diverse%20scene%20objects.%20We%20utilize%20a%0Ahierarchical%20Gaussian%20transformer%20to%20predict%20the%20occupied%20voxel%20centers%20and%0Asemantic%20classes%20along%20with%20the%20Gaussian%20parameters.%20Leveraging%20the%20real-time%0Arendering%20capability%20of%203D%20Gaussian%20Splatting%2C%20we%20also%20impose%20rendering%0Asupervision%20with%20available%20depth%20and%20semantic%20map%20annotations%20injecting%0Apixel-level%20alignment%20to%20boost%20occupancy%20learning.%20Extensive%20experiments%20on%20the%0AOcc3D-nuScenes%20and%20Occ3D-Waymo%20benchmarks%20demonstrate%20our%20proposed%20method%20sets%0Anew%20state-of-the-art%20results%20while%20maintaining%20low%20inference%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09417v2&entry.124074799=Read"},
{"title": "Visually Descriptive Language Model for Vector Graphics Reasoning", "author": "Zhenhailong Wang and Joy Hsu and Xingyao Wang and Kuan-Hao Huang and Manling Li and Jiajun Wu and Heng Ji", "abstract": "  Despite significant advancements, large multimodal models (LMMs) still\nstruggle to bridge the gap between low-level visual perception -- focusing on\nshapes, sizes, and layouts -- and high-level language reasoning, such as\nsemantics and logic. This limitation is evident in tasks that require precise\nvisual perception, like comparing geometric properties or solving visual\nreasoning problems. To study this failure mode, we focus on vector graphics --\nimages composed of 2D objects and shapes, prevalent in LMM-based tasks in web,\ndesign, and OS environments. We identify two key research questions: how can we\nenable precise visual perception, and how can we facilitate high-level\nreasoning based on such low-level perceptions? To capture fine visual details,\nwe use Scalable Vector Graphics (SVG) for accurate encoding of visual scenes.\nHowever, SVGs are not readily interpretable by LMMs in a zero-shot manner. To\ntackle this, we propose the Visually Descriptive Language Model (VDLM), which\nintroduces a Primal Visual Description (PVD) as an intermediate textual\nrepresentation. PVD translates SVGs into a text-based abstraction consisting of\nprimitive attributes (e.g., shape, position, measurement) and their\ncorresponding values. PVD can be learned using task-agnostic synthesized data\nand represents visual primitives that are universal across vector graphics.\nThis abstraction is more structured, allowing for direct interpretation by\nfoundation models for zero-shot generalization. Without human-annotated data,\nempirical results show that VDLM significantly improves state-of-the-art LMMs\nlike GPT-4o on various multimodal perception and reasoning tasks. Extensive\nanalyses of VDLM show improved interpretability due to its disentangled\nperception and reasoning. We also demonstrate a positive correlation between\nPVD quality and task performance. Project page:\nhttps://mikewangwzhl.github.io/VDLM/\n", "link": "http://arxiv.org/abs/2404.06479v5", "date": "2025-06-12", "relevancy": 3.0634, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6301}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6301}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visually%20Descriptive%20Language%20Model%20for%20Vector%20Graphics%20Reasoning&body=Title%3A%20Visually%20Descriptive%20Language%20Model%20for%20Vector%20Graphics%20Reasoning%0AAuthor%3A%20Zhenhailong%20Wang%20and%20Joy%20Hsu%20and%20Xingyao%20Wang%20and%20Kuan-Hao%20Huang%20and%20Manling%20Li%20and%20Jiajun%20Wu%20and%20Heng%20Ji%0AAbstract%3A%20%20%20Despite%20significant%20advancements%2C%20large%20multimodal%20models%20%28LMMs%29%20still%0Astruggle%20to%20bridge%20the%20gap%20between%20low-level%20visual%20perception%20--%20focusing%20on%0Ashapes%2C%20sizes%2C%20and%20layouts%20--%20and%20high-level%20language%20reasoning%2C%20such%20as%0Asemantics%20and%20logic.%20This%20limitation%20is%20evident%20in%20tasks%20that%20require%20precise%0Avisual%20perception%2C%20like%20comparing%20geometric%20properties%20or%20solving%20visual%0Areasoning%20problems.%20To%20study%20this%20failure%20mode%2C%20we%20focus%20on%20vector%20graphics%20--%0Aimages%20composed%20of%202D%20objects%20and%20shapes%2C%20prevalent%20in%20LMM-based%20tasks%20in%20web%2C%0Adesign%2C%20and%20OS%20environments.%20We%20identify%20two%20key%20research%20questions%3A%20how%20can%20we%0Aenable%20precise%20visual%20perception%2C%20and%20how%20can%20we%20facilitate%20high-level%0Areasoning%20based%20on%20such%20low-level%20perceptions%3F%20To%20capture%20fine%20visual%20details%2C%0Awe%20use%20Scalable%20Vector%20Graphics%20%28SVG%29%20for%20accurate%20encoding%20of%20visual%20scenes.%0AHowever%2C%20SVGs%20are%20not%20readily%20interpretable%20by%20LMMs%20in%20a%20zero-shot%20manner.%20To%0Atackle%20this%2C%20we%20propose%20the%20Visually%20Descriptive%20Language%20Model%20%28VDLM%29%2C%20which%0Aintroduces%20a%20Primal%20Visual%20Description%20%28PVD%29%20as%20an%20intermediate%20textual%0Arepresentation.%20PVD%20translates%20SVGs%20into%20a%20text-based%20abstraction%20consisting%20of%0Aprimitive%20attributes%20%28e.g.%2C%20shape%2C%20position%2C%20measurement%29%20and%20their%0Acorresponding%20values.%20PVD%20can%20be%20learned%20using%20task-agnostic%20synthesized%20data%0Aand%20represents%20visual%20primitives%20that%20are%20universal%20across%20vector%20graphics.%0AThis%20abstraction%20is%20more%20structured%2C%20allowing%20for%20direct%20interpretation%20by%0Afoundation%20models%20for%20zero-shot%20generalization.%20Without%20human-annotated%20data%2C%0Aempirical%20results%20show%20that%20VDLM%20significantly%20improves%20state-of-the-art%20LMMs%0Alike%20GPT-4o%20on%20various%20multimodal%20perception%20and%20reasoning%20tasks.%20Extensive%0Aanalyses%20of%20VDLM%20show%20improved%20interpretability%20due%20to%20its%20disentangled%0Aperception%20and%20reasoning.%20We%20also%20demonstrate%20a%20positive%20correlation%20between%0APVD%20quality%20and%20task%20performance.%20Project%20page%3A%0Ahttps%3A//mikewangwzhl.github.io/VDLM/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06479v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisually%2520Descriptive%2520Language%2520Model%2520for%2520Vector%2520Graphics%2520Reasoning%26entry.906535625%3DZhenhailong%2520Wang%2520and%2520Joy%2520Hsu%2520and%2520Xingyao%2520Wang%2520and%2520Kuan-Hao%2520Huang%2520and%2520Manling%2520Li%2520and%2520Jiajun%2520Wu%2520and%2520Heng%2520Ji%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%252C%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520still%250Astruggle%2520to%2520bridge%2520the%2520gap%2520between%2520low-level%2520visual%2520perception%2520--%2520focusing%2520on%250Ashapes%252C%2520sizes%252C%2520and%2520layouts%2520--%2520and%2520high-level%2520language%2520reasoning%252C%2520such%2520as%250Asemantics%2520and%2520logic.%2520This%2520limitation%2520is%2520evident%2520in%2520tasks%2520that%2520require%2520precise%250Avisual%2520perception%252C%2520like%2520comparing%2520geometric%2520properties%2520or%2520solving%2520visual%250Areasoning%2520problems.%2520To%2520study%2520this%2520failure%2520mode%252C%2520we%2520focus%2520on%2520vector%2520graphics%2520--%250Aimages%2520composed%2520of%25202D%2520objects%2520and%2520shapes%252C%2520prevalent%2520in%2520LMM-based%2520tasks%2520in%2520web%252C%250Adesign%252C%2520and%2520OS%2520environments.%2520We%2520identify%2520two%2520key%2520research%2520questions%253A%2520how%2520can%2520we%250Aenable%2520precise%2520visual%2520perception%252C%2520and%2520how%2520can%2520we%2520facilitate%2520high-level%250Areasoning%2520based%2520on%2520such%2520low-level%2520perceptions%253F%2520To%2520capture%2520fine%2520visual%2520details%252C%250Awe%2520use%2520Scalable%2520Vector%2520Graphics%2520%2528SVG%2529%2520for%2520accurate%2520encoding%2520of%2520visual%2520scenes.%250AHowever%252C%2520SVGs%2520are%2520not%2520readily%2520interpretable%2520by%2520LMMs%2520in%2520a%2520zero-shot%2520manner.%2520To%250Atackle%2520this%252C%2520we%2520propose%2520the%2520Visually%2520Descriptive%2520Language%2520Model%2520%2528VDLM%2529%252C%2520which%250Aintroduces%2520a%2520Primal%2520Visual%2520Description%2520%2528PVD%2529%2520as%2520an%2520intermediate%2520textual%250Arepresentation.%2520PVD%2520translates%2520SVGs%2520into%2520a%2520text-based%2520abstraction%2520consisting%2520of%250Aprimitive%2520attributes%2520%2528e.g.%252C%2520shape%252C%2520position%252C%2520measurement%2529%2520and%2520their%250Acorresponding%2520values.%2520PVD%2520can%2520be%2520learned%2520using%2520task-agnostic%2520synthesized%2520data%250Aand%2520represents%2520visual%2520primitives%2520that%2520are%2520universal%2520across%2520vector%2520graphics.%250AThis%2520abstraction%2520is%2520more%2520structured%252C%2520allowing%2520for%2520direct%2520interpretation%2520by%250Afoundation%2520models%2520for%2520zero-shot%2520generalization.%2520Without%2520human-annotated%2520data%252C%250Aempirical%2520results%2520show%2520that%2520VDLM%2520significantly%2520improves%2520state-of-the-art%2520LMMs%250Alike%2520GPT-4o%2520on%2520various%2520multimodal%2520perception%2520and%2520reasoning%2520tasks.%2520Extensive%250Aanalyses%2520of%2520VDLM%2520show%2520improved%2520interpretability%2520due%2520to%2520its%2520disentangled%250Aperception%2520and%2520reasoning.%2520We%2520also%2520demonstrate%2520a%2520positive%2520correlation%2520between%250APVD%2520quality%2520and%2520task%2520performance.%2520Project%2520page%253A%250Ahttps%253A//mikewangwzhl.github.io/VDLM/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06479v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visually%20Descriptive%20Language%20Model%20for%20Vector%20Graphics%20Reasoning&entry.906535625=Zhenhailong%20Wang%20and%20Joy%20Hsu%20and%20Xingyao%20Wang%20and%20Kuan-Hao%20Huang%20and%20Manling%20Li%20and%20Jiajun%20Wu%20and%20Heng%20Ji&entry.1292438233=%20%20Despite%20significant%20advancements%2C%20large%20multimodal%20models%20%28LMMs%29%20still%0Astruggle%20to%20bridge%20the%20gap%20between%20low-level%20visual%20perception%20--%20focusing%20on%0Ashapes%2C%20sizes%2C%20and%20layouts%20--%20and%20high-level%20language%20reasoning%2C%20such%20as%0Asemantics%20and%20logic.%20This%20limitation%20is%20evident%20in%20tasks%20that%20require%20precise%0Avisual%20perception%2C%20like%20comparing%20geometric%20properties%20or%20solving%20visual%0Areasoning%20problems.%20To%20study%20this%20failure%20mode%2C%20we%20focus%20on%20vector%20graphics%20--%0Aimages%20composed%20of%202D%20objects%20and%20shapes%2C%20prevalent%20in%20LMM-based%20tasks%20in%20web%2C%0Adesign%2C%20and%20OS%20environments.%20We%20identify%20two%20key%20research%20questions%3A%20how%20can%20we%0Aenable%20precise%20visual%20perception%2C%20and%20how%20can%20we%20facilitate%20high-level%0Areasoning%20based%20on%20such%20low-level%20perceptions%3F%20To%20capture%20fine%20visual%20details%2C%0Awe%20use%20Scalable%20Vector%20Graphics%20%28SVG%29%20for%20accurate%20encoding%20of%20visual%20scenes.%0AHowever%2C%20SVGs%20are%20not%20readily%20interpretable%20by%20LMMs%20in%20a%20zero-shot%20manner.%20To%0Atackle%20this%2C%20we%20propose%20the%20Visually%20Descriptive%20Language%20Model%20%28VDLM%29%2C%20which%0Aintroduces%20a%20Primal%20Visual%20Description%20%28PVD%29%20as%20an%20intermediate%20textual%0Arepresentation.%20PVD%20translates%20SVGs%20into%20a%20text-based%20abstraction%20consisting%20of%0Aprimitive%20attributes%20%28e.g.%2C%20shape%2C%20position%2C%20measurement%29%20and%20their%0Acorresponding%20values.%20PVD%20can%20be%20learned%20using%20task-agnostic%20synthesized%20data%0Aand%20represents%20visual%20primitives%20that%20are%20universal%20across%20vector%20graphics.%0AThis%20abstraction%20is%20more%20structured%2C%20allowing%20for%20direct%20interpretation%20by%0Afoundation%20models%20for%20zero-shot%20generalization.%20Without%20human-annotated%20data%2C%0Aempirical%20results%20show%20that%20VDLM%20significantly%20improves%20state-of-the-art%20LMMs%0Alike%20GPT-4o%20on%20various%20multimodal%20perception%20and%20reasoning%20tasks.%20Extensive%0Aanalyses%20of%20VDLM%20show%20improved%20interpretability%20due%20to%20its%20disentangled%0Aperception%20and%20reasoning.%20We%20also%20demonstrate%20a%20positive%20correlation%20between%0APVD%20quality%20and%20task%20performance.%20Project%20page%3A%0Ahttps%3A//mikewangwzhl.github.io/VDLM/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06479v5&entry.124074799=Read"},
{"title": "GenWorld: Towards Detecting AI-generated Real-world Simulation Videos", "author": "Weiliang Chen and Wenzhao Zheng and Yu Zheng and Lei Chen and Jie Zhou and Jiwen Lu and Yueqi Duan", "abstract": "  The flourishing of video generation technologies has endangered the\ncredibility of real-world information and intensified the demand for\nAI-generated video detectors. Despite some progress, the lack of high-quality\nreal-world datasets hinders the development of trustworthy detectors. In this\npaper, we propose GenWorld, a large-scale, high-quality, and real-world\nsimulation dataset for AI-generated video detection. GenWorld features the\nfollowing characteristics: (1) Real-world Simulation: GenWorld focuses on\nvideos that replicate real-world scenarios, which have a significant impact due\nto their realism and potential influence; (2) High Quality: GenWorld employs\nmultiple state-of-the-art video generation models to provide realistic and\nhigh-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes\nvideos generated from diverse generators and various prompt modalities (e.g.,\ntext, image, video), offering the potential to learn more generalizable\nforensic features. We analyze existing methods and find they fail to detect\nhigh-quality videos generated by world models (i.e., Cosmos), revealing\npotential drawbacks of ignoring real-world clues. To address this, we propose a\nsimple yet effective model, SpannDetector, to leverage multi-view consistency\nas a strong criterion for real-world AI-generated video detection. Experiments\nshow that our method achieves superior results, highlighting a promising\ndirection for explainable AI-generated video detection based on physical\nplausibility. We believe that GenWorld will advance the field of AI-generated\nvideo detection. Project Page: https://chen-wl20.github.io/GenWorld\n", "link": "http://arxiv.org/abs/2506.10975v1", "date": "2025-06-12", "relevancy": 2.9975, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6411}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6037}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenWorld%3A%20Towards%20Detecting%20AI-generated%20Real-world%20Simulation%20Videos&body=Title%3A%20GenWorld%3A%20Towards%20Detecting%20AI-generated%20Real-world%20Simulation%20Videos%0AAuthor%3A%20Weiliang%20Chen%20and%20Wenzhao%20Zheng%20and%20Yu%20Zheng%20and%20Lei%20Chen%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20The%20flourishing%20of%20video%20generation%20technologies%20has%20endangered%20the%0Acredibility%20of%20real-world%20information%20and%20intensified%20the%20demand%20for%0AAI-generated%20video%20detectors.%20Despite%20some%20progress%2C%20the%20lack%20of%20high-quality%0Areal-world%20datasets%20hinders%20the%20development%20of%20trustworthy%20detectors.%20In%20this%0Apaper%2C%20we%20propose%20GenWorld%2C%20a%20large-scale%2C%20high-quality%2C%20and%20real-world%0Asimulation%20dataset%20for%20AI-generated%20video%20detection.%20GenWorld%20features%20the%0Afollowing%20characteristics%3A%20%281%29%20Real-world%20Simulation%3A%20GenWorld%20focuses%20on%0Avideos%20that%20replicate%20real-world%20scenarios%2C%20which%20have%20a%20significant%20impact%20due%0Ato%20their%20realism%20and%20potential%20influence%3B%20%282%29%20High%20Quality%3A%20GenWorld%20employs%0Amultiple%20state-of-the-art%20video%20generation%20models%20to%20provide%20realistic%20and%0Ahigh-quality%20forged%20videos%3B%20%283%29%20Cross-prompt%20Diversity%3A%20GenWorld%20includes%0Avideos%20generated%20from%20diverse%20generators%20and%20various%20prompt%20modalities%20%28e.g.%2C%0Atext%2C%20image%2C%20video%29%2C%20offering%20the%20potential%20to%20learn%20more%20generalizable%0Aforensic%20features.%20We%20analyze%20existing%20methods%20and%20find%20they%20fail%20to%20detect%0Ahigh-quality%20videos%20generated%20by%20world%20models%20%28i.e.%2C%20Cosmos%29%2C%20revealing%0Apotential%20drawbacks%20of%20ignoring%20real-world%20clues.%20To%20address%20this%2C%20we%20propose%20a%0Asimple%20yet%20effective%20model%2C%20SpannDetector%2C%20to%20leverage%20multi-view%20consistency%0Aas%20a%20strong%20criterion%20for%20real-world%20AI-generated%20video%20detection.%20Experiments%0Ashow%20that%20our%20method%20achieves%20superior%20results%2C%20highlighting%20a%20promising%0Adirection%20for%20explainable%20AI-generated%20video%20detection%20based%20on%20physical%0Aplausibility.%20We%20believe%20that%20GenWorld%20will%20advance%20the%20field%20of%20AI-generated%0Avideo%20detection.%20Project%20Page%3A%20https%3A//chen-wl20.github.io/GenWorld%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenWorld%253A%2520Towards%2520Detecting%2520AI-generated%2520Real-world%2520Simulation%2520Videos%26entry.906535625%3DWeiliang%2520Chen%2520and%2520Wenzhao%2520Zheng%2520and%2520Yu%2520Zheng%2520and%2520Lei%2520Chen%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520The%2520flourishing%2520of%2520video%2520generation%2520technologies%2520has%2520endangered%2520the%250Acredibility%2520of%2520real-world%2520information%2520and%2520intensified%2520the%2520demand%2520for%250AAI-generated%2520video%2520detectors.%2520Despite%2520some%2520progress%252C%2520the%2520lack%2520of%2520high-quality%250Areal-world%2520datasets%2520hinders%2520the%2520development%2520of%2520trustworthy%2520detectors.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520GenWorld%252C%2520a%2520large-scale%252C%2520high-quality%252C%2520and%2520real-world%250Asimulation%2520dataset%2520for%2520AI-generated%2520video%2520detection.%2520GenWorld%2520features%2520the%250Afollowing%2520characteristics%253A%2520%25281%2529%2520Real-world%2520Simulation%253A%2520GenWorld%2520focuses%2520on%250Avideos%2520that%2520replicate%2520real-world%2520scenarios%252C%2520which%2520have%2520a%2520significant%2520impact%2520due%250Ato%2520their%2520realism%2520and%2520potential%2520influence%253B%2520%25282%2529%2520High%2520Quality%253A%2520GenWorld%2520employs%250Amultiple%2520state-of-the-art%2520video%2520generation%2520models%2520to%2520provide%2520realistic%2520and%250Ahigh-quality%2520forged%2520videos%253B%2520%25283%2529%2520Cross-prompt%2520Diversity%253A%2520GenWorld%2520includes%250Avideos%2520generated%2520from%2520diverse%2520generators%2520and%2520various%2520prompt%2520modalities%2520%2528e.g.%252C%250Atext%252C%2520image%252C%2520video%2529%252C%2520offering%2520the%2520potential%2520to%2520learn%2520more%2520generalizable%250Aforensic%2520features.%2520We%2520analyze%2520existing%2520methods%2520and%2520find%2520they%2520fail%2520to%2520detect%250Ahigh-quality%2520videos%2520generated%2520by%2520world%2520models%2520%2528i.e.%252C%2520Cosmos%2529%252C%2520revealing%250Apotential%2520drawbacks%2520of%2520ignoring%2520real-world%2520clues.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520model%252C%2520SpannDetector%252C%2520to%2520leverage%2520multi-view%2520consistency%250Aas%2520a%2520strong%2520criterion%2520for%2520real-world%2520AI-generated%2520video%2520detection.%2520Experiments%250Ashow%2520that%2520our%2520method%2520achieves%2520superior%2520results%252C%2520highlighting%2520a%2520promising%250Adirection%2520for%2520explainable%2520AI-generated%2520video%2520detection%2520based%2520on%2520physical%250Aplausibility.%2520We%2520believe%2520that%2520GenWorld%2520will%2520advance%2520the%2520field%2520of%2520AI-generated%250Avideo%2520detection.%2520Project%2520Page%253A%2520https%253A//chen-wl20.github.io/GenWorld%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenWorld%3A%20Towards%20Detecting%20AI-generated%20Real-world%20Simulation%20Videos&entry.906535625=Weiliang%20Chen%20and%20Wenzhao%20Zheng%20and%20Yu%20Zheng%20and%20Lei%20Chen%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%20and%20Yueqi%20Duan&entry.1292438233=%20%20The%20flourishing%20of%20video%20generation%20technologies%20has%20endangered%20the%0Acredibility%20of%20real-world%20information%20and%20intensified%20the%20demand%20for%0AAI-generated%20video%20detectors.%20Despite%20some%20progress%2C%20the%20lack%20of%20high-quality%0Areal-world%20datasets%20hinders%20the%20development%20of%20trustworthy%20detectors.%20In%20this%0Apaper%2C%20we%20propose%20GenWorld%2C%20a%20large-scale%2C%20high-quality%2C%20and%20real-world%0Asimulation%20dataset%20for%20AI-generated%20video%20detection.%20GenWorld%20features%20the%0Afollowing%20characteristics%3A%20%281%29%20Real-world%20Simulation%3A%20GenWorld%20focuses%20on%0Avideos%20that%20replicate%20real-world%20scenarios%2C%20which%20have%20a%20significant%20impact%20due%0Ato%20their%20realism%20and%20potential%20influence%3B%20%282%29%20High%20Quality%3A%20GenWorld%20employs%0Amultiple%20state-of-the-art%20video%20generation%20models%20to%20provide%20realistic%20and%0Ahigh-quality%20forged%20videos%3B%20%283%29%20Cross-prompt%20Diversity%3A%20GenWorld%20includes%0Avideos%20generated%20from%20diverse%20generators%20and%20various%20prompt%20modalities%20%28e.g.%2C%0Atext%2C%20image%2C%20video%29%2C%20offering%20the%20potential%20to%20learn%20more%20generalizable%0Aforensic%20features.%20We%20analyze%20existing%20methods%20and%20find%20they%20fail%20to%20detect%0Ahigh-quality%20videos%20generated%20by%20world%20models%20%28i.e.%2C%20Cosmos%29%2C%20revealing%0Apotential%20drawbacks%20of%20ignoring%20real-world%20clues.%20To%20address%20this%2C%20we%20propose%20a%0Asimple%20yet%20effective%20model%2C%20SpannDetector%2C%20to%20leverage%20multi-view%20consistency%0Aas%20a%20strong%20criterion%20for%20real-world%20AI-generated%20video%20detection.%20Experiments%0Ashow%20that%20our%20method%20achieves%20superior%20results%2C%20highlighting%20a%20promising%0Adirection%20for%20explainable%20AI-generated%20video%20detection%20based%20on%20physical%0Aplausibility.%20We%20believe%20that%20GenWorld%20will%20advance%20the%20field%20of%20AI-generated%0Avideo%20detection.%20Project%20Page%3A%20https%3A//chen-wl20.github.io/GenWorld%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10975v1&entry.124074799=Read"},
{"title": "InstaInpaint: Instant 3D-Scene Inpainting with Masked Large\n  Reconstruction Model", "author": "Junqi You and Chieh Hubert Lin and Weijie Lyu and Zhengbo Zhang and Ming-Hsuan Yang", "abstract": "  Recent advances in 3D scene reconstruction enable real-time viewing in\nvirtual and augmented reality. To support interactive operations for better\nimmersiveness, such as moving or editing objects, 3D scene inpainting methods\nare proposed to repair or complete the altered geometry. However, current\napproaches rely on lengthy and computationally intensive optimization, making\nthem impractical for real-time or online applications. We propose InstaInpaint,\na reference-based feed-forward framework that produces 3D-scene inpainting from\na 2D inpainting proposal within 0.4 seconds. We develop a self-supervised\nmasked-finetuning strategy to enable training of our custom large\nreconstruction model (LRM) on the large-scale dataset. Through extensive\nexperiments, we analyze and identify several key designs that improve\ngeneralization, textural consistency, and geometric correctness. InstaInpaint\nachieves a 1000x speed-up from prior methods while maintaining a\nstate-of-the-art performance across two standard benchmarks. Moreover, we show\nthat InstaInpaint generalizes well to flexible downstream applications such as\nobject insertion and multi-region inpainting. More video results are available\nat our project page: https://dhmbb2.github.io/InstaInpaint_page/.\n", "link": "http://arxiv.org/abs/2506.10980v1", "date": "2025-06-12", "relevancy": 2.9421, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5963}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5845}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstaInpaint%3A%20Instant%203D-Scene%20Inpainting%20with%20Masked%20Large%0A%20%20Reconstruction%20Model&body=Title%3A%20InstaInpaint%3A%20Instant%203D-Scene%20Inpainting%20with%20Masked%20Large%0A%20%20Reconstruction%20Model%0AAuthor%3A%20Junqi%20You%20and%20Chieh%20Hubert%20Lin%20and%20Weijie%20Lyu%20and%20Zhengbo%20Zhang%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20scene%20reconstruction%20enable%20real-time%20viewing%20in%0Avirtual%20and%20augmented%20reality.%20To%20support%20interactive%20operations%20for%20better%0Aimmersiveness%2C%20such%20as%20moving%20or%20editing%20objects%2C%203D%20scene%20inpainting%20methods%0Aare%20proposed%20to%20repair%20or%20complete%20the%20altered%20geometry.%20However%2C%20current%0Aapproaches%20rely%20on%20lengthy%20and%20computationally%20intensive%20optimization%2C%20making%0Athem%20impractical%20for%20real-time%20or%20online%20applications.%20We%20propose%20InstaInpaint%2C%0Aa%20reference-based%20feed-forward%20framework%20that%20produces%203D-scene%20inpainting%20from%0Aa%202D%20inpainting%20proposal%20within%200.4%20seconds.%20We%20develop%20a%20self-supervised%0Amasked-finetuning%20strategy%20to%20enable%20training%20of%20our%20custom%20large%0Areconstruction%20model%20%28LRM%29%20on%20the%20large-scale%20dataset.%20Through%20extensive%0Aexperiments%2C%20we%20analyze%20and%20identify%20several%20key%20designs%20that%20improve%0Ageneralization%2C%20textural%20consistency%2C%20and%20geometric%20correctness.%20InstaInpaint%0Aachieves%20a%201000x%20speed-up%20from%20prior%20methods%20while%20maintaining%20a%0Astate-of-the-art%20performance%20across%20two%20standard%20benchmarks.%20Moreover%2C%20we%20show%0Athat%20InstaInpaint%20generalizes%20well%20to%20flexible%20downstream%20applications%20such%20as%0Aobject%20insertion%20and%20multi-region%20inpainting.%20More%20video%20results%20are%20available%0Aat%20our%20project%20page%3A%20https%3A//dhmbb2.github.io/InstaInpaint_page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstaInpaint%253A%2520Instant%25203D-Scene%2520Inpainting%2520with%2520Masked%2520Large%250A%2520%2520Reconstruction%2520Model%26entry.906535625%3DJunqi%2520You%2520and%2520Chieh%2520Hubert%2520Lin%2520and%2520Weijie%2520Lyu%2520and%2520Zhengbo%2520Zhang%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520scene%2520reconstruction%2520enable%2520real-time%2520viewing%2520in%250Avirtual%2520and%2520augmented%2520reality.%2520To%2520support%2520interactive%2520operations%2520for%2520better%250Aimmersiveness%252C%2520such%2520as%2520moving%2520or%2520editing%2520objects%252C%25203D%2520scene%2520inpainting%2520methods%250Aare%2520proposed%2520to%2520repair%2520or%2520complete%2520the%2520altered%2520geometry.%2520However%252C%2520current%250Aapproaches%2520rely%2520on%2520lengthy%2520and%2520computationally%2520intensive%2520optimization%252C%2520making%250Athem%2520impractical%2520for%2520real-time%2520or%2520online%2520applications.%2520We%2520propose%2520InstaInpaint%252C%250Aa%2520reference-based%2520feed-forward%2520framework%2520that%2520produces%25203D-scene%2520inpainting%2520from%250Aa%25202D%2520inpainting%2520proposal%2520within%25200.4%2520seconds.%2520We%2520develop%2520a%2520self-supervised%250Amasked-finetuning%2520strategy%2520to%2520enable%2520training%2520of%2520our%2520custom%2520large%250Areconstruction%2520model%2520%2528LRM%2529%2520on%2520the%2520large-scale%2520dataset.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520analyze%2520and%2520identify%2520several%2520key%2520designs%2520that%2520improve%250Ageneralization%252C%2520textural%2520consistency%252C%2520and%2520geometric%2520correctness.%2520InstaInpaint%250Aachieves%2520a%25201000x%2520speed-up%2520from%2520prior%2520methods%2520while%2520maintaining%2520a%250Astate-of-the-art%2520performance%2520across%2520two%2520standard%2520benchmarks.%2520Moreover%252C%2520we%2520show%250Athat%2520InstaInpaint%2520generalizes%2520well%2520to%2520flexible%2520downstream%2520applications%2520such%2520as%250Aobject%2520insertion%2520and%2520multi-region%2520inpainting.%2520More%2520video%2520results%2520are%2520available%250Aat%2520our%2520project%2520page%253A%2520https%253A//dhmbb2.github.io/InstaInpaint_page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstaInpaint%3A%20Instant%203D-Scene%20Inpainting%20with%20Masked%20Large%0A%20%20Reconstruction%20Model&entry.906535625=Junqi%20You%20and%20Chieh%20Hubert%20Lin%20and%20Weijie%20Lyu%20and%20Zhengbo%20Zhang%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Recent%20advances%20in%203D%20scene%20reconstruction%20enable%20real-time%20viewing%20in%0Avirtual%20and%20augmented%20reality.%20To%20support%20interactive%20operations%20for%20better%0Aimmersiveness%2C%20such%20as%20moving%20or%20editing%20objects%2C%203D%20scene%20inpainting%20methods%0Aare%20proposed%20to%20repair%20or%20complete%20the%20altered%20geometry.%20However%2C%20current%0Aapproaches%20rely%20on%20lengthy%20and%20computationally%20intensive%20optimization%2C%20making%0Athem%20impractical%20for%20real-time%20or%20online%20applications.%20We%20propose%20InstaInpaint%2C%0Aa%20reference-based%20feed-forward%20framework%20that%20produces%203D-scene%20inpainting%20from%0Aa%202D%20inpainting%20proposal%20within%200.4%20seconds.%20We%20develop%20a%20self-supervised%0Amasked-finetuning%20strategy%20to%20enable%20training%20of%20our%20custom%20large%0Areconstruction%20model%20%28LRM%29%20on%20the%20large-scale%20dataset.%20Through%20extensive%0Aexperiments%2C%20we%20analyze%20and%20identify%20several%20key%20designs%20that%20improve%0Ageneralization%2C%20textural%20consistency%2C%20and%20geometric%20correctness.%20InstaInpaint%0Aachieves%20a%201000x%20speed-up%20from%20prior%20methods%20while%20maintaining%20a%0Astate-of-the-art%20performance%20across%20two%20standard%20benchmarks.%20Moreover%2C%20we%20show%0Athat%20InstaInpaint%20generalizes%20well%20to%20flexible%20downstream%20applications%20such%20as%0Aobject%20insertion%20and%20multi-region%20inpainting.%20More%20video%20results%20are%20available%0Aat%20our%20project%20page%3A%20https%3A//dhmbb2.github.io/InstaInpaint_page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10980v1&entry.124074799=Read"},
{"title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal\n  Understanding", "author": "Yuhang Zhang and Haosheng Yu and Jiaping Xiao and Mir Feroskhan", "abstract": "  Vision-and-language navigation (VLN) is a long-standing challenge in\nautonomous robotics, aiming to empower agents with the ability to follow human\ninstructions while navigating complex environments. Two key bottlenecks remain\nin this field: generalization to out-of-distribution environments and reliance\non fixed discrete action spaces. To address these challenges, we propose\nVision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles\n(UAVs) to execute language-guided flight. Without the requirement for\nlocalization or active ranging sensors, VLFly outputs continuous velocity\ncommands purely from egocentric observations captured by an onboard monocular\ncamera. The VLFly integrates three modules: an instruction encoder based on a\nlarge language model (LLM) that reformulates high-level language into\nstructured prompts, a goal retriever powered by a vision-language model (VLM)\nthat matches these prompts to goal images via vision-language similarity, and a\nwaypoint planner that generates executable trajectories for real-time UAV\ncontrol. VLFly is evaluated across diverse simulation environments without\nadditional fine-tuning and consistently outperforms all baselines. Moreover,\nreal-world VLN tasks in indoor and outdoor environments under direct and\nindirect instructions demonstrate that VLFly achieves robust open-vocabulary\ngoal understanding and generalized navigation capabilities, even in the\npresence of abstract language input.\n", "link": "http://arxiv.org/abs/2506.10756v1", "date": "2025-06-12", "relevancy": 2.8758, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5847}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounded%20Vision-Language%20Navigation%20for%20UAVs%20with%20Open-Vocabulary%20Goal%0A%20%20Understanding&body=Title%3A%20Grounded%20Vision-Language%20Navigation%20for%20UAVs%20with%20Open-Vocabulary%20Goal%0A%20%20Understanding%0AAuthor%3A%20Yuhang%20Zhang%20and%20Haosheng%20Yu%20and%20Jiaping%20Xiao%20and%20Mir%20Feroskhan%0AAbstract%3A%20%20%20Vision-and-language%20navigation%20%28VLN%29%20is%20a%20long-standing%20challenge%20in%0Aautonomous%20robotics%2C%20aiming%20to%20empower%20agents%20with%20the%20ability%20to%20follow%20human%0Ainstructions%20while%20navigating%20complex%20environments.%20Two%20key%20bottlenecks%20remain%0Ain%20this%20field%3A%20generalization%20to%20out-of-distribution%20environments%20and%20reliance%0Aon%20fixed%20discrete%20action%20spaces.%20To%20address%20these%20challenges%2C%20we%20propose%0AVision-Language%20Fly%20%28VLFly%29%2C%20a%20framework%20tailored%20for%20Unmanned%20Aerial%20Vehicles%0A%28UAVs%29%20to%20execute%20language-guided%20flight.%20Without%20the%20requirement%20for%0Alocalization%20or%20active%20ranging%20sensors%2C%20VLFly%20outputs%20continuous%20velocity%0Acommands%20purely%20from%20egocentric%20observations%20captured%20by%20an%20onboard%20monocular%0Acamera.%20The%20VLFly%20integrates%20three%20modules%3A%20an%20instruction%20encoder%20based%20on%20a%0Alarge%20language%20model%20%28LLM%29%20that%20reformulates%20high-level%20language%20into%0Astructured%20prompts%2C%20a%20goal%20retriever%20powered%20by%20a%20vision-language%20model%20%28VLM%29%0Athat%20matches%20these%20prompts%20to%20goal%20images%20via%20vision-language%20similarity%2C%20and%20a%0Awaypoint%20planner%20that%20generates%20executable%20trajectories%20for%20real-time%20UAV%0Acontrol.%20VLFly%20is%20evaluated%20across%20diverse%20simulation%20environments%20without%0Aadditional%20fine-tuning%20and%20consistently%20outperforms%20all%20baselines.%20Moreover%2C%0Areal-world%20VLN%20tasks%20in%20indoor%20and%20outdoor%20environments%20under%20direct%20and%0Aindirect%20instructions%20demonstrate%20that%20VLFly%20achieves%20robust%20open-vocabulary%0Agoal%20understanding%20and%20generalized%20navigation%20capabilities%2C%20even%20in%20the%0Apresence%20of%20abstract%20language%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounded%2520Vision-Language%2520Navigation%2520for%2520UAVs%2520with%2520Open-Vocabulary%2520Goal%250A%2520%2520Understanding%26entry.906535625%3DYuhang%2520Zhang%2520and%2520Haosheng%2520Yu%2520and%2520Jiaping%2520Xiao%2520and%2520Mir%2520Feroskhan%26entry.1292438233%3D%2520%2520Vision-and-language%2520navigation%2520%2528VLN%2529%2520is%2520a%2520long-standing%2520challenge%2520in%250Aautonomous%2520robotics%252C%2520aiming%2520to%2520empower%2520agents%2520with%2520the%2520ability%2520to%2520follow%2520human%250Ainstructions%2520while%2520navigating%2520complex%2520environments.%2520Two%2520key%2520bottlenecks%2520remain%250Ain%2520this%2520field%253A%2520generalization%2520to%2520out-of-distribution%2520environments%2520and%2520reliance%250Aon%2520fixed%2520discrete%2520action%2520spaces.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AVision-Language%2520Fly%2520%2528VLFly%2529%252C%2520a%2520framework%2520tailored%2520for%2520Unmanned%2520Aerial%2520Vehicles%250A%2528UAVs%2529%2520to%2520execute%2520language-guided%2520flight.%2520Without%2520the%2520requirement%2520for%250Alocalization%2520or%2520active%2520ranging%2520sensors%252C%2520VLFly%2520outputs%2520continuous%2520velocity%250Acommands%2520purely%2520from%2520egocentric%2520observations%2520captured%2520by%2520an%2520onboard%2520monocular%250Acamera.%2520The%2520VLFly%2520integrates%2520three%2520modules%253A%2520an%2520instruction%2520encoder%2520based%2520on%2520a%250Alarge%2520language%2520model%2520%2528LLM%2529%2520that%2520reformulates%2520high-level%2520language%2520into%250Astructured%2520prompts%252C%2520a%2520goal%2520retriever%2520powered%2520by%2520a%2520vision-language%2520model%2520%2528VLM%2529%250Athat%2520matches%2520these%2520prompts%2520to%2520goal%2520images%2520via%2520vision-language%2520similarity%252C%2520and%2520a%250Awaypoint%2520planner%2520that%2520generates%2520executable%2520trajectories%2520for%2520real-time%2520UAV%250Acontrol.%2520VLFly%2520is%2520evaluated%2520across%2520diverse%2520simulation%2520environments%2520without%250Aadditional%2520fine-tuning%2520and%2520consistently%2520outperforms%2520all%2520baselines.%2520Moreover%252C%250Areal-world%2520VLN%2520tasks%2520in%2520indoor%2520and%2520outdoor%2520environments%2520under%2520direct%2520and%250Aindirect%2520instructions%2520demonstrate%2520that%2520VLFly%2520achieves%2520robust%2520open-vocabulary%250Agoal%2520understanding%2520and%2520generalized%2520navigation%2520capabilities%252C%2520even%2520in%2520the%250Apresence%2520of%2520abstract%2520language%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounded%20Vision-Language%20Navigation%20for%20UAVs%20with%20Open-Vocabulary%20Goal%0A%20%20Understanding&entry.906535625=Yuhang%20Zhang%20and%20Haosheng%20Yu%20and%20Jiaping%20Xiao%20and%20Mir%20Feroskhan&entry.1292438233=%20%20Vision-and-language%20navigation%20%28VLN%29%20is%20a%20long-standing%20challenge%20in%0Aautonomous%20robotics%2C%20aiming%20to%20empower%20agents%20with%20the%20ability%20to%20follow%20human%0Ainstructions%20while%20navigating%20complex%20environments.%20Two%20key%20bottlenecks%20remain%0Ain%20this%20field%3A%20generalization%20to%20out-of-distribution%20environments%20and%20reliance%0Aon%20fixed%20discrete%20action%20spaces.%20To%20address%20these%20challenges%2C%20we%20propose%0AVision-Language%20Fly%20%28VLFly%29%2C%20a%20framework%20tailored%20for%20Unmanned%20Aerial%20Vehicles%0A%28UAVs%29%20to%20execute%20language-guided%20flight.%20Without%20the%20requirement%20for%0Alocalization%20or%20active%20ranging%20sensors%2C%20VLFly%20outputs%20continuous%20velocity%0Acommands%20purely%20from%20egocentric%20observations%20captured%20by%20an%20onboard%20monocular%0Acamera.%20The%20VLFly%20integrates%20three%20modules%3A%20an%20instruction%20encoder%20based%20on%20a%0Alarge%20language%20model%20%28LLM%29%20that%20reformulates%20high-level%20language%20into%0Astructured%20prompts%2C%20a%20goal%20retriever%20powered%20by%20a%20vision-language%20model%20%28VLM%29%0Athat%20matches%20these%20prompts%20to%20goal%20images%20via%20vision-language%20similarity%2C%20and%20a%0Awaypoint%20planner%20that%20generates%20executable%20trajectories%20for%20real-time%20UAV%0Acontrol.%20VLFly%20is%20evaluated%20across%20diverse%20simulation%20environments%20without%0Aadditional%20fine-tuning%20and%20consistently%20outperforms%20all%20baselines.%20Moreover%2C%0Areal-world%20VLN%20tasks%20in%20indoor%20and%20outdoor%20environments%20under%20direct%20and%0Aindirect%20instructions%20demonstrate%20that%20VLFly%20achieves%20robust%20open-vocabulary%0Agoal%20understanding%20and%20generalized%20navigation%20capabilities%2C%20even%20in%20the%0Apresence%20of%20abstract%20language%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10756v1&entry.124074799=Read"},
{"title": "QuadricFormer: Scene as Superquadrics for 3D Semantic Occupancy\n  Prediction", "author": "Sicheng Zuo and Wenzhao Zheng and Xiaoyong Han and Longchao Yang and Yong Pan and Jiwen Lu", "abstract": "  3D occupancy prediction is crucial for robust autonomous driving systems as\nit enables comprehensive perception of environmental structures and semantics.\nMost existing methods employ dense voxel-based scene representations, ignoring\nthe sparsity of driving scenes and resulting in inefficiency. Recent works\nexplore object-centric representations based on sparse Gaussians, but their\nellipsoidal shape prior limits the modeling of diverse structures. In\nreal-world driving scenes, objects exhibit rich geometries (e.g., cuboids,\ncylinders, and irregular shapes), necessitating excessive ellipsoidal Gaussians\ndensely packed for accurate modeling, which leads to inefficient\nrepresentations. To address this, we propose to use geometrically expressive\nsuperquadrics as scene primitives, enabling efficient representation of complex\nstructures with fewer primitives through their inherent shape diversity. We\ndevelop a probabilistic superquadric mixture model, which interprets each\nsuperquadric as an occupancy probability distribution with a corresponding\ngeometry prior, and calculates semantics through probabilistic mixture.\nBuilding on this, we present QuadricFormer, a superquadric-based model for\nefficient 3D occupancy prediction, and introduce a pruning-and-splitting module\nto further enhance modeling efficiency by concentrating superquadrics in\noccupied regions. Extensive experiments on the nuScenes dataset demonstrate\nthat QuadricFormer achieves state-of-the-art performance while maintaining\nsuperior efficiency.\n", "link": "http://arxiv.org/abs/2506.10977v1", "date": "2025-06-12", "relevancy": 2.8622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuadricFormer%3A%20Scene%20as%20Superquadrics%20for%203D%20Semantic%20Occupancy%0A%20%20Prediction&body=Title%3A%20QuadricFormer%3A%20Scene%20as%20Superquadrics%20for%203D%20Semantic%20Occupancy%0A%20%20Prediction%0AAuthor%3A%20Sicheng%20Zuo%20and%20Wenzhao%20Zheng%20and%20Xiaoyong%20Han%20and%20Longchao%20Yang%20and%20Yong%20Pan%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%203D%20occupancy%20prediction%20is%20crucial%20for%20robust%20autonomous%20driving%20systems%20as%0Ait%20enables%20comprehensive%20perception%20of%20environmental%20structures%20and%20semantics.%0AMost%20existing%20methods%20employ%20dense%20voxel-based%20scene%20representations%2C%20ignoring%0Athe%20sparsity%20of%20driving%20scenes%20and%20resulting%20in%20inefficiency.%20Recent%20works%0Aexplore%20object-centric%20representations%20based%20on%20sparse%20Gaussians%2C%20but%20their%0Aellipsoidal%20shape%20prior%20limits%20the%20modeling%20of%20diverse%20structures.%20In%0Areal-world%20driving%20scenes%2C%20objects%20exhibit%20rich%20geometries%20%28e.g.%2C%20cuboids%2C%0Acylinders%2C%20and%20irregular%20shapes%29%2C%20necessitating%20excessive%20ellipsoidal%20Gaussians%0Adensely%20packed%20for%20accurate%20modeling%2C%20which%20leads%20to%20inefficient%0Arepresentations.%20To%20address%20this%2C%20we%20propose%20to%20use%20geometrically%20expressive%0Asuperquadrics%20as%20scene%20primitives%2C%20enabling%20efficient%20representation%20of%20complex%0Astructures%20with%20fewer%20primitives%20through%20their%20inherent%20shape%20diversity.%20We%0Adevelop%20a%20probabilistic%20superquadric%20mixture%20model%2C%20which%20interprets%20each%0Asuperquadric%20as%20an%20occupancy%20probability%20distribution%20with%20a%20corresponding%0Ageometry%20prior%2C%20and%20calculates%20semantics%20through%20probabilistic%20mixture.%0ABuilding%20on%20this%2C%20we%20present%20QuadricFormer%2C%20a%20superquadric-based%20model%20for%0Aefficient%203D%20occupancy%20prediction%2C%20and%20introduce%20a%20pruning-and-splitting%20module%0Ato%20further%20enhance%20modeling%20efficiency%20by%20concentrating%20superquadrics%20in%0Aoccupied%20regions.%20Extensive%20experiments%20on%20the%20nuScenes%20dataset%20demonstrate%0Athat%20QuadricFormer%20achieves%20state-of-the-art%20performance%20while%20maintaining%0Asuperior%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuadricFormer%253A%2520Scene%2520as%2520Superquadrics%2520for%25203D%2520Semantic%2520Occupancy%250A%2520%2520Prediction%26entry.906535625%3DSicheng%2520Zuo%2520and%2520Wenzhao%2520Zheng%2520and%2520Xiaoyong%2520Han%2520and%2520Longchao%2520Yang%2520and%2520Yong%2520Pan%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%25203D%2520occupancy%2520prediction%2520is%2520crucial%2520for%2520robust%2520autonomous%2520driving%2520systems%2520as%250Ait%2520enables%2520comprehensive%2520perception%2520of%2520environmental%2520structures%2520and%2520semantics.%250AMost%2520existing%2520methods%2520employ%2520dense%2520voxel-based%2520scene%2520representations%252C%2520ignoring%250Athe%2520sparsity%2520of%2520driving%2520scenes%2520and%2520resulting%2520in%2520inefficiency.%2520Recent%2520works%250Aexplore%2520object-centric%2520representations%2520based%2520on%2520sparse%2520Gaussians%252C%2520but%2520their%250Aellipsoidal%2520shape%2520prior%2520limits%2520the%2520modeling%2520of%2520diverse%2520structures.%2520In%250Areal-world%2520driving%2520scenes%252C%2520objects%2520exhibit%2520rich%2520geometries%2520%2528e.g.%252C%2520cuboids%252C%250Acylinders%252C%2520and%2520irregular%2520shapes%2529%252C%2520necessitating%2520excessive%2520ellipsoidal%2520Gaussians%250Adensely%2520packed%2520for%2520accurate%2520modeling%252C%2520which%2520leads%2520to%2520inefficient%250Arepresentations.%2520To%2520address%2520this%252C%2520we%2520propose%2520to%2520use%2520geometrically%2520expressive%250Asuperquadrics%2520as%2520scene%2520primitives%252C%2520enabling%2520efficient%2520representation%2520of%2520complex%250Astructures%2520with%2520fewer%2520primitives%2520through%2520their%2520inherent%2520shape%2520diversity.%2520We%250Adevelop%2520a%2520probabilistic%2520superquadric%2520mixture%2520model%252C%2520which%2520interprets%2520each%250Asuperquadric%2520as%2520an%2520occupancy%2520probability%2520distribution%2520with%2520a%2520corresponding%250Ageometry%2520prior%252C%2520and%2520calculates%2520semantics%2520through%2520probabilistic%2520mixture.%250ABuilding%2520on%2520this%252C%2520we%2520present%2520QuadricFormer%252C%2520a%2520superquadric-based%2520model%2520for%250Aefficient%25203D%2520occupancy%2520prediction%252C%2520and%2520introduce%2520a%2520pruning-and-splitting%2520module%250Ato%2520further%2520enhance%2520modeling%2520efficiency%2520by%2520concentrating%2520superquadrics%2520in%250Aoccupied%2520regions.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520dataset%2520demonstrate%250Athat%2520QuadricFormer%2520achieves%2520state-of-the-art%2520performance%2520while%2520maintaining%250Asuperior%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuadricFormer%3A%20Scene%20as%20Superquadrics%20for%203D%20Semantic%20Occupancy%0A%20%20Prediction&entry.906535625=Sicheng%20Zuo%20and%20Wenzhao%20Zheng%20and%20Xiaoyong%20Han%20and%20Longchao%20Yang%20and%20Yong%20Pan%20and%20Jiwen%20Lu&entry.1292438233=%20%203D%20occupancy%20prediction%20is%20crucial%20for%20robust%20autonomous%20driving%20systems%20as%0Ait%20enables%20comprehensive%20perception%20of%20environmental%20structures%20and%20semantics.%0AMost%20existing%20methods%20employ%20dense%20voxel-based%20scene%20representations%2C%20ignoring%0Athe%20sparsity%20of%20driving%20scenes%20and%20resulting%20in%20inefficiency.%20Recent%20works%0Aexplore%20object-centric%20representations%20based%20on%20sparse%20Gaussians%2C%20but%20their%0Aellipsoidal%20shape%20prior%20limits%20the%20modeling%20of%20diverse%20structures.%20In%0Areal-world%20driving%20scenes%2C%20objects%20exhibit%20rich%20geometries%20%28e.g.%2C%20cuboids%2C%0Acylinders%2C%20and%20irregular%20shapes%29%2C%20necessitating%20excessive%20ellipsoidal%20Gaussians%0Adensely%20packed%20for%20accurate%20modeling%2C%20which%20leads%20to%20inefficient%0Arepresentations.%20To%20address%20this%2C%20we%20propose%20to%20use%20geometrically%20expressive%0Asuperquadrics%20as%20scene%20primitives%2C%20enabling%20efficient%20representation%20of%20complex%0Astructures%20with%20fewer%20primitives%20through%20their%20inherent%20shape%20diversity.%20We%0Adevelop%20a%20probabilistic%20superquadric%20mixture%20model%2C%20which%20interprets%20each%0Asuperquadric%20as%20an%20occupancy%20probability%20distribution%20with%20a%20corresponding%0Ageometry%20prior%2C%20and%20calculates%20semantics%20through%20probabilistic%20mixture.%0ABuilding%20on%20this%2C%20we%20present%20QuadricFormer%2C%20a%20superquadric-based%20model%20for%0Aefficient%203D%20occupancy%20prediction%2C%20and%20introduce%20a%20pruning-and-splitting%20module%0Ato%20further%20enhance%20modeling%20efficiency%20by%20concentrating%20superquadrics%20in%0Aoccupied%20regions.%20Extensive%20experiments%20on%20the%20nuScenes%20dataset%20demonstrate%0Athat%20QuadricFormer%20achieves%20state-of-the-art%20performance%20while%20maintaining%0Asuperior%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10977v1&entry.124074799=Read"},
{"title": "Semantic-decoupled Spatial Partition Guided Point-supervised Oriented\n  Object Detection", "author": "Xinyuan Liu and Hang Xu and Yike Ma and Yucheng Zhang and Feng Dai", "abstract": "  Recent remote sensing tech advancements drive imagery growth, making oriented\nobject detection rapid development, yet hindered by labor-intensive annotation\nfor high-density scenes. Oriented object detection with point supervision\noffers a cost-effective solution for densely packed scenes in remote sensing,\nyet existing methods suffer from inadequate sample assignment and instance\nconfusion due to rigid rule-based designs. To address this, we propose SSP\n(Semantic-decoupled Spatial Partition), a unified framework that synergizes\nrule-driven prior injection and data-driven label purification. Specifically,\nSSP introduces two core innovations: 1) Pixel-level Spatial Partition-based\nSample Assignment, which compactly estimates the upper and lower bounds of\nobject scales and mines high-quality positive samples and hard negative samples\nthrough spatial partitioning of pixel maps. 2) Semantic Spatial Partition-based\nBox Extraction, which derives instances from spatial partitions modulated by\nsemantic maps and reliably converts them into bounding boxes to form\npseudo-labels for supervising the learning of downstream detectors. Experiments\non DOTA-v1.0 and others demonstrate SSP\\' s superiority: it achieves 45.78% mAP\nunder point supervision, outperforming SOTA method PointOBB-v2 by 4.10%.\nFurthermore, when integrated with ORCNN and ReDet architectures, the SSP\nframework achieves mAP values of 47.86% and 48.50%, respectively. The code is\navailable at https://github.com/antxinyuan/ssp.\n", "link": "http://arxiv.org/abs/2506.10601v1", "date": "2025-06-12", "relevancy": 2.8369, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5751}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5646}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-decoupled%20Spatial%20Partition%20Guided%20Point-supervised%20Oriented%0A%20%20Object%20Detection&body=Title%3A%20Semantic-decoupled%20Spatial%20Partition%20Guided%20Point-supervised%20Oriented%0A%20%20Object%20Detection%0AAuthor%3A%20Xinyuan%20Liu%20and%20Hang%20Xu%20and%20Yike%20Ma%20and%20Yucheng%20Zhang%20and%20Feng%20Dai%0AAbstract%3A%20%20%20Recent%20remote%20sensing%20tech%20advancements%20drive%20imagery%20growth%2C%20making%20oriented%0Aobject%20detection%20rapid%20development%2C%20yet%20hindered%20by%20labor-intensive%20annotation%0Afor%20high-density%20scenes.%20Oriented%20object%20detection%20with%20point%20supervision%0Aoffers%20a%20cost-effective%20solution%20for%20densely%20packed%20scenes%20in%20remote%20sensing%2C%0Ayet%20existing%20methods%20suffer%20from%20inadequate%20sample%20assignment%20and%20instance%0Aconfusion%20due%20to%20rigid%20rule-based%20designs.%20To%20address%20this%2C%20we%20propose%20SSP%0A%28Semantic-decoupled%20Spatial%20Partition%29%2C%20a%20unified%20framework%20that%20synergizes%0Arule-driven%20prior%20injection%20and%20data-driven%20label%20purification.%20Specifically%2C%0ASSP%20introduces%20two%20core%20innovations%3A%201%29%20Pixel-level%20Spatial%20Partition-based%0ASample%20Assignment%2C%20which%20compactly%20estimates%20the%20upper%20and%20lower%20bounds%20of%0Aobject%20scales%20and%20mines%20high-quality%20positive%20samples%20and%20hard%20negative%20samples%0Athrough%20spatial%20partitioning%20of%20pixel%20maps.%202%29%20Semantic%20Spatial%20Partition-based%0ABox%20Extraction%2C%20which%20derives%20instances%20from%20spatial%20partitions%20modulated%20by%0Asemantic%20maps%20and%20reliably%20converts%20them%20into%20bounding%20boxes%20to%20form%0Apseudo-labels%20for%20supervising%20the%20learning%20of%20downstream%20detectors.%20Experiments%0Aon%20DOTA-v1.0%20and%20others%20demonstrate%20SSP%5C%27%20s%20superiority%3A%20it%20achieves%2045.78%25%20mAP%0Aunder%20point%20supervision%2C%20outperforming%20SOTA%20method%20PointOBB-v2%20by%204.10%25.%0AFurthermore%2C%20when%20integrated%20with%20ORCNN%20and%20ReDet%20architectures%2C%20the%20SSP%0Aframework%20achieves%20mAP%20values%20of%2047.86%25%20and%2048.50%25%2C%20respectively.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/antxinyuan/ssp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-decoupled%2520Spatial%2520Partition%2520Guided%2520Point-supervised%2520Oriented%250A%2520%2520Object%2520Detection%26entry.906535625%3DXinyuan%2520Liu%2520and%2520Hang%2520Xu%2520and%2520Yike%2520Ma%2520and%2520Yucheng%2520Zhang%2520and%2520Feng%2520Dai%26entry.1292438233%3D%2520%2520Recent%2520remote%2520sensing%2520tech%2520advancements%2520drive%2520imagery%2520growth%252C%2520making%2520oriented%250Aobject%2520detection%2520rapid%2520development%252C%2520yet%2520hindered%2520by%2520labor-intensive%2520annotation%250Afor%2520high-density%2520scenes.%2520Oriented%2520object%2520detection%2520with%2520point%2520supervision%250Aoffers%2520a%2520cost-effective%2520solution%2520for%2520densely%2520packed%2520scenes%2520in%2520remote%2520sensing%252C%250Ayet%2520existing%2520methods%2520suffer%2520from%2520inadequate%2520sample%2520assignment%2520and%2520instance%250Aconfusion%2520due%2520to%2520rigid%2520rule-based%2520designs.%2520To%2520address%2520this%252C%2520we%2520propose%2520SSP%250A%2528Semantic-decoupled%2520Spatial%2520Partition%2529%252C%2520a%2520unified%2520framework%2520that%2520synergizes%250Arule-driven%2520prior%2520injection%2520and%2520data-driven%2520label%2520purification.%2520Specifically%252C%250ASSP%2520introduces%2520two%2520core%2520innovations%253A%25201%2529%2520Pixel-level%2520Spatial%2520Partition-based%250ASample%2520Assignment%252C%2520which%2520compactly%2520estimates%2520the%2520upper%2520and%2520lower%2520bounds%2520of%250Aobject%2520scales%2520and%2520mines%2520high-quality%2520positive%2520samples%2520and%2520hard%2520negative%2520samples%250Athrough%2520spatial%2520partitioning%2520of%2520pixel%2520maps.%25202%2529%2520Semantic%2520Spatial%2520Partition-based%250ABox%2520Extraction%252C%2520which%2520derives%2520instances%2520from%2520spatial%2520partitions%2520modulated%2520by%250Asemantic%2520maps%2520and%2520reliably%2520converts%2520them%2520into%2520bounding%2520boxes%2520to%2520form%250Apseudo-labels%2520for%2520supervising%2520the%2520learning%2520of%2520downstream%2520detectors.%2520Experiments%250Aon%2520DOTA-v1.0%2520and%2520others%2520demonstrate%2520SSP%255C%2527%2520s%2520superiority%253A%2520it%2520achieves%252045.78%2525%2520mAP%250Aunder%2520point%2520supervision%252C%2520outperforming%2520SOTA%2520method%2520PointOBB-v2%2520by%25204.10%2525.%250AFurthermore%252C%2520when%2520integrated%2520with%2520ORCNN%2520and%2520ReDet%2520architectures%252C%2520the%2520SSP%250Aframework%2520achieves%2520mAP%2520values%2520of%252047.86%2525%2520and%252048.50%2525%252C%2520respectively.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/antxinyuan/ssp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-decoupled%20Spatial%20Partition%20Guided%20Point-supervised%20Oriented%0A%20%20Object%20Detection&entry.906535625=Xinyuan%20Liu%20and%20Hang%20Xu%20and%20Yike%20Ma%20and%20Yucheng%20Zhang%20and%20Feng%20Dai&entry.1292438233=%20%20Recent%20remote%20sensing%20tech%20advancements%20drive%20imagery%20growth%2C%20making%20oriented%0Aobject%20detection%20rapid%20development%2C%20yet%20hindered%20by%20labor-intensive%20annotation%0Afor%20high-density%20scenes.%20Oriented%20object%20detection%20with%20point%20supervision%0Aoffers%20a%20cost-effective%20solution%20for%20densely%20packed%20scenes%20in%20remote%20sensing%2C%0Ayet%20existing%20methods%20suffer%20from%20inadequate%20sample%20assignment%20and%20instance%0Aconfusion%20due%20to%20rigid%20rule-based%20designs.%20To%20address%20this%2C%20we%20propose%20SSP%0A%28Semantic-decoupled%20Spatial%20Partition%29%2C%20a%20unified%20framework%20that%20synergizes%0Arule-driven%20prior%20injection%20and%20data-driven%20label%20purification.%20Specifically%2C%0ASSP%20introduces%20two%20core%20innovations%3A%201%29%20Pixel-level%20Spatial%20Partition-based%0ASample%20Assignment%2C%20which%20compactly%20estimates%20the%20upper%20and%20lower%20bounds%20of%0Aobject%20scales%20and%20mines%20high-quality%20positive%20samples%20and%20hard%20negative%20samples%0Athrough%20spatial%20partitioning%20of%20pixel%20maps.%202%29%20Semantic%20Spatial%20Partition-based%0ABox%20Extraction%2C%20which%20derives%20instances%20from%20spatial%20partitions%20modulated%20by%0Asemantic%20maps%20and%20reliably%20converts%20them%20into%20bounding%20boxes%20to%20form%0Apseudo-labels%20for%20supervising%20the%20learning%20of%20downstream%20detectors.%20Experiments%0Aon%20DOTA-v1.0%20and%20others%20demonstrate%20SSP%5C%27%20s%20superiority%3A%20it%20achieves%2045.78%25%20mAP%0Aunder%20point%20supervision%2C%20outperforming%20SOTA%20method%20PointOBB-v2%20by%204.10%25.%0AFurthermore%2C%20when%20integrated%20with%20ORCNN%20and%20ReDet%20architectures%2C%20the%20SSP%0Aframework%20achieves%20mAP%20values%20of%2047.86%25%20and%2048.50%25%2C%20respectively.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/antxinyuan/ssp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10601v1&entry.124074799=Read"},
{"title": "ContextRefine-CLIP for EPIC-KITCHENS-100 Multi-Instance Retrieval\n  Challenge 2025", "author": "Jing He and Yiqing Wang and Lingling Li and Kexin Zhang and Puhua Chen", "abstract": "  This report presents ContextRefine-CLIP (CR-CLIP), an efficient model for\nvisual-textual multi-instance retrieval tasks. The approach is based on the\ndual-encoder AVION, on which we introduce a cross-modal attention flow module\nto achieve bidirectional dynamic interaction and refinement between visual and\ntextual features to generate more context-aware joint representations. For\nsoft-label relevance matrices provided in tasks such as EPIC-KITCHENS-100,\nCR-CLIP can work with Symmetric Multi-Similarity Loss to achieve more accurate\nsemantic alignment and optimization using the refined features. Without using\nensemble learning, the CR-CLIP model achieves 66.78mAP and 82.08nDCG on the\nEPIC-KITCHENS-100 public leaderboard, which significantly outperforms the\nbaseline model and fully validates its effectiveness in cross-modal retrieval.\nThe code will be released open-source on\nhttps://github.com/delCayr/ContextRefine-Clip\n", "link": "http://arxiv.org/abs/2506.10550v1", "date": "2025-06-12", "relevancy": 2.8323, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextRefine-CLIP%20for%20EPIC-KITCHENS-100%20Multi-Instance%20Retrieval%0A%20%20Challenge%202025&body=Title%3A%20ContextRefine-CLIP%20for%20EPIC-KITCHENS-100%20Multi-Instance%20Retrieval%0A%20%20Challenge%202025%0AAuthor%3A%20Jing%20He%20and%20Yiqing%20Wang%20and%20Lingling%20Li%20and%20Kexin%20Zhang%20and%20Puhua%20Chen%0AAbstract%3A%20%20%20This%20report%20presents%20ContextRefine-CLIP%20%28CR-CLIP%29%2C%20an%20efficient%20model%20for%0Avisual-textual%20multi-instance%20retrieval%20tasks.%20The%20approach%20is%20based%20on%20the%0Adual-encoder%20AVION%2C%20on%20which%20we%20introduce%20a%20cross-modal%20attention%20flow%20module%0Ato%20achieve%20bidirectional%20dynamic%20interaction%20and%20refinement%20between%20visual%20and%0Atextual%20features%20to%20generate%20more%20context-aware%20joint%20representations.%20For%0Asoft-label%20relevance%20matrices%20provided%20in%20tasks%20such%20as%20EPIC-KITCHENS-100%2C%0ACR-CLIP%20can%20work%20with%20Symmetric%20Multi-Similarity%20Loss%20to%20achieve%20more%20accurate%0Asemantic%20alignment%20and%20optimization%20using%20the%20refined%20features.%20Without%20using%0Aensemble%20learning%2C%20the%20CR-CLIP%20model%20achieves%2066.78mAP%20and%2082.08nDCG%20on%20the%0AEPIC-KITCHENS-100%20public%20leaderboard%2C%20which%20significantly%20outperforms%20the%0Abaseline%20model%20and%20fully%20validates%20its%20effectiveness%20in%20cross-modal%20retrieval.%0AThe%20code%20will%20be%20released%20open-source%20on%0Ahttps%3A//github.com/delCayr/ContextRefine-Clip%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextRefine-CLIP%2520for%2520EPIC-KITCHENS-100%2520Multi-Instance%2520Retrieval%250A%2520%2520Challenge%25202025%26entry.906535625%3DJing%2520He%2520and%2520Yiqing%2520Wang%2520and%2520Lingling%2520Li%2520and%2520Kexin%2520Zhang%2520and%2520Puhua%2520Chen%26entry.1292438233%3D%2520%2520This%2520report%2520presents%2520ContextRefine-CLIP%2520%2528CR-CLIP%2529%252C%2520an%2520efficient%2520model%2520for%250Avisual-textual%2520multi-instance%2520retrieval%2520tasks.%2520The%2520approach%2520is%2520based%2520on%2520the%250Adual-encoder%2520AVION%252C%2520on%2520which%2520we%2520introduce%2520a%2520cross-modal%2520attention%2520flow%2520module%250Ato%2520achieve%2520bidirectional%2520dynamic%2520interaction%2520and%2520refinement%2520between%2520visual%2520and%250Atextual%2520features%2520to%2520generate%2520more%2520context-aware%2520joint%2520representations.%2520For%250Asoft-label%2520relevance%2520matrices%2520provided%2520in%2520tasks%2520such%2520as%2520EPIC-KITCHENS-100%252C%250ACR-CLIP%2520can%2520work%2520with%2520Symmetric%2520Multi-Similarity%2520Loss%2520to%2520achieve%2520more%2520accurate%250Asemantic%2520alignment%2520and%2520optimization%2520using%2520the%2520refined%2520features.%2520Without%2520using%250Aensemble%2520learning%252C%2520the%2520CR-CLIP%2520model%2520achieves%252066.78mAP%2520and%252082.08nDCG%2520on%2520the%250AEPIC-KITCHENS-100%2520public%2520leaderboard%252C%2520which%2520significantly%2520outperforms%2520the%250Abaseline%2520model%2520and%2520fully%2520validates%2520its%2520effectiveness%2520in%2520cross-modal%2520retrieval.%250AThe%2520code%2520will%2520be%2520released%2520open-source%2520on%250Ahttps%253A//github.com/delCayr/ContextRefine-Clip%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextRefine-CLIP%20for%20EPIC-KITCHENS-100%20Multi-Instance%20Retrieval%0A%20%20Challenge%202025&entry.906535625=Jing%20He%20and%20Yiqing%20Wang%20and%20Lingling%20Li%20and%20Kexin%20Zhang%20and%20Puhua%20Chen&entry.1292438233=%20%20This%20report%20presents%20ContextRefine-CLIP%20%28CR-CLIP%29%2C%20an%20efficient%20model%20for%0Avisual-textual%20multi-instance%20retrieval%20tasks.%20The%20approach%20is%20based%20on%20the%0Adual-encoder%20AVION%2C%20on%20which%20we%20introduce%20a%20cross-modal%20attention%20flow%20module%0Ato%20achieve%20bidirectional%20dynamic%20interaction%20and%20refinement%20between%20visual%20and%0Atextual%20features%20to%20generate%20more%20context-aware%20joint%20representations.%20For%0Asoft-label%20relevance%20matrices%20provided%20in%20tasks%20such%20as%20EPIC-KITCHENS-100%2C%0ACR-CLIP%20can%20work%20with%20Symmetric%20Multi-Similarity%20Loss%20to%20achieve%20more%20accurate%0Asemantic%20alignment%20and%20optimization%20using%20the%20refined%20features.%20Without%20using%0Aensemble%20learning%2C%20the%20CR-CLIP%20model%20achieves%2066.78mAP%20and%2082.08nDCG%20on%20the%0AEPIC-KITCHENS-100%20public%20leaderboard%2C%20which%20significantly%20outperforms%20the%0Abaseline%20model%20and%20fully%20validates%20its%20effectiveness%20in%20cross-modal%20retrieval.%0AThe%20code%20will%20be%20released%20open-source%20on%0Ahttps%3A//github.com/delCayr/ContextRefine-Clip%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10550v1&entry.124074799=Read"},
{"title": "Improving Medical Visual Representation Learning with Pathological-level\n  Cross-Modal Alignment and Correlation Exploration", "author": "Jun Wang and Lixing Zhu and Xiaohan Yu and Abhir Bhalerao and Yulan He", "abstract": "  Learning medical visual representations from image-report pairs through joint\nlearning has garnered increasing research attention due to its potential to\nalleviate the data scarcity problem in the medical domain. The primary\nchallenges stem from the lengthy reports that feature complex discourse\nrelations and semantic pathologies. Previous works have predominantly focused\non instance-wise or token-wise cross-modal alignment, often neglecting the\nimportance of pathological-level consistency. This paper presents a novel\nframework PLACE that promotes the Pathological-Level Alignment and enriches the\nfine-grained details via Correlation Exploration without additional human\nannotations. Specifically, we propose a novel pathological-level cross-modal\nalignment (PCMA) approach to maximize the consistency of pathology observations\nfrom both images and reports. To facilitate this, a Visual Pathology\nObservation Extractor is introduced to extract visual pathological observation\nrepresentations from localized tokens. The PCMA module operates independently\nof any external disease annotations, enhancing the generalizability and\nrobustness of our methods. Furthermore, we design a proxy task that enforces\nthe model to identify correlations among image patches, thereby enriching the\nfine-grained details crucial for various downstream tasks. Experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance on multiple downstream tasks, including classification,\nimage-to-text retrieval, semantic segmentation, object detection and report\ngeneration.\n", "link": "http://arxiv.org/abs/2506.10573v1", "date": "2025-06-12", "relevancy": 2.7867, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Medical%20Visual%20Representation%20Learning%20with%20Pathological-level%0A%20%20Cross-Modal%20Alignment%20and%20Correlation%20Exploration&body=Title%3A%20Improving%20Medical%20Visual%20Representation%20Learning%20with%20Pathological-level%0A%20%20Cross-Modal%20Alignment%20and%20Correlation%20Exploration%0AAuthor%3A%20Jun%20Wang%20and%20Lixing%20Zhu%20and%20Xiaohan%20Yu%20and%20Abhir%20Bhalerao%20and%20Yulan%20He%0AAbstract%3A%20%20%20Learning%20medical%20visual%20representations%20from%20image-report%20pairs%20through%20joint%0Alearning%20has%20garnered%20increasing%20research%20attention%20due%20to%20its%20potential%20to%0Aalleviate%20the%20data%20scarcity%20problem%20in%20the%20medical%20domain.%20The%20primary%0Achallenges%20stem%20from%20the%20lengthy%20reports%20that%20feature%20complex%20discourse%0Arelations%20and%20semantic%20pathologies.%20Previous%20works%20have%20predominantly%20focused%0Aon%20instance-wise%20or%20token-wise%20cross-modal%20alignment%2C%20often%20neglecting%20the%0Aimportance%20of%20pathological-level%20consistency.%20This%20paper%20presents%20a%20novel%0Aframework%20PLACE%20that%20promotes%20the%20Pathological-Level%20Alignment%20and%20enriches%20the%0Afine-grained%20details%20via%20Correlation%20Exploration%20without%20additional%20human%0Aannotations.%20Specifically%2C%20we%20propose%20a%20novel%20pathological-level%20cross-modal%0Aalignment%20%28PCMA%29%20approach%20to%20maximize%20the%20consistency%20of%20pathology%20observations%0Afrom%20both%20images%20and%20reports.%20To%20facilitate%20this%2C%20a%20Visual%20Pathology%0AObservation%20Extractor%20is%20introduced%20to%20extract%20visual%20pathological%20observation%0Arepresentations%20from%20localized%20tokens.%20The%20PCMA%20module%20operates%20independently%0Aof%20any%20external%20disease%20annotations%2C%20enhancing%20the%20generalizability%20and%0Arobustness%20of%20our%20methods.%20Furthermore%2C%20we%20design%20a%20proxy%20task%20that%20enforces%0Athe%20model%20to%20identify%20correlations%20among%20image%20patches%2C%20thereby%20enriching%20the%0Afine-grained%20details%20crucial%20for%20various%20downstream%20tasks.%20Experimental%20results%0Ademonstrate%20that%20our%20proposed%20framework%20achieves%20new%20state-of-the-art%0Aperformance%20on%20multiple%20downstream%20tasks%2C%20including%20classification%2C%0Aimage-to-text%20retrieval%2C%20semantic%20segmentation%2C%20object%20detection%20and%20report%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Medical%2520Visual%2520Representation%2520Learning%2520with%2520Pathological-level%250A%2520%2520Cross-Modal%2520Alignment%2520and%2520Correlation%2520Exploration%26entry.906535625%3DJun%2520Wang%2520and%2520Lixing%2520Zhu%2520and%2520Xiaohan%2520Yu%2520and%2520Abhir%2520Bhalerao%2520and%2520Yulan%2520He%26entry.1292438233%3D%2520%2520Learning%2520medical%2520visual%2520representations%2520from%2520image-report%2520pairs%2520through%2520joint%250Alearning%2520has%2520garnered%2520increasing%2520research%2520attention%2520due%2520to%2520its%2520potential%2520to%250Aalleviate%2520the%2520data%2520scarcity%2520problem%2520in%2520the%2520medical%2520domain.%2520The%2520primary%250Achallenges%2520stem%2520from%2520the%2520lengthy%2520reports%2520that%2520feature%2520complex%2520discourse%250Arelations%2520and%2520semantic%2520pathologies.%2520Previous%2520works%2520have%2520predominantly%2520focused%250Aon%2520instance-wise%2520or%2520token-wise%2520cross-modal%2520alignment%252C%2520often%2520neglecting%2520the%250Aimportance%2520of%2520pathological-level%2520consistency.%2520This%2520paper%2520presents%2520a%2520novel%250Aframework%2520PLACE%2520that%2520promotes%2520the%2520Pathological-Level%2520Alignment%2520and%2520enriches%2520the%250Afine-grained%2520details%2520via%2520Correlation%2520Exploration%2520without%2520additional%2520human%250Aannotations.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520pathological-level%2520cross-modal%250Aalignment%2520%2528PCMA%2529%2520approach%2520to%2520maximize%2520the%2520consistency%2520of%2520pathology%2520observations%250Afrom%2520both%2520images%2520and%2520reports.%2520To%2520facilitate%2520this%252C%2520a%2520Visual%2520Pathology%250AObservation%2520Extractor%2520is%2520introduced%2520to%2520extract%2520visual%2520pathological%2520observation%250Arepresentations%2520from%2520localized%2520tokens.%2520The%2520PCMA%2520module%2520operates%2520independently%250Aof%2520any%2520external%2520disease%2520annotations%252C%2520enhancing%2520the%2520generalizability%2520and%250Arobustness%2520of%2520our%2520methods.%2520Furthermore%252C%2520we%2520design%2520a%2520proxy%2520task%2520that%2520enforces%250Athe%2520model%2520to%2520identify%2520correlations%2520among%2520image%2520patches%252C%2520thereby%2520enriching%2520the%250Afine-grained%2520details%2520crucial%2520for%2520various%2520downstream%2520tasks.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520proposed%2520framework%2520achieves%2520new%2520state-of-the-art%250Aperformance%2520on%2520multiple%2520downstream%2520tasks%252C%2520including%2520classification%252C%250Aimage-to-text%2520retrieval%252C%2520semantic%2520segmentation%252C%2520object%2520detection%2520and%2520report%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Medical%20Visual%20Representation%20Learning%20with%20Pathological-level%0A%20%20Cross-Modal%20Alignment%20and%20Correlation%20Exploration&entry.906535625=Jun%20Wang%20and%20Lixing%20Zhu%20and%20Xiaohan%20Yu%20and%20Abhir%20Bhalerao%20and%20Yulan%20He&entry.1292438233=%20%20Learning%20medical%20visual%20representations%20from%20image-report%20pairs%20through%20joint%0Alearning%20has%20garnered%20increasing%20research%20attention%20due%20to%20its%20potential%20to%0Aalleviate%20the%20data%20scarcity%20problem%20in%20the%20medical%20domain.%20The%20primary%0Achallenges%20stem%20from%20the%20lengthy%20reports%20that%20feature%20complex%20discourse%0Arelations%20and%20semantic%20pathologies.%20Previous%20works%20have%20predominantly%20focused%0Aon%20instance-wise%20or%20token-wise%20cross-modal%20alignment%2C%20often%20neglecting%20the%0Aimportance%20of%20pathological-level%20consistency.%20This%20paper%20presents%20a%20novel%0Aframework%20PLACE%20that%20promotes%20the%20Pathological-Level%20Alignment%20and%20enriches%20the%0Afine-grained%20details%20via%20Correlation%20Exploration%20without%20additional%20human%0Aannotations.%20Specifically%2C%20we%20propose%20a%20novel%20pathological-level%20cross-modal%0Aalignment%20%28PCMA%29%20approach%20to%20maximize%20the%20consistency%20of%20pathology%20observations%0Afrom%20both%20images%20and%20reports.%20To%20facilitate%20this%2C%20a%20Visual%20Pathology%0AObservation%20Extractor%20is%20introduced%20to%20extract%20visual%20pathological%20observation%0Arepresentations%20from%20localized%20tokens.%20The%20PCMA%20module%20operates%20independently%0Aof%20any%20external%20disease%20annotations%2C%20enhancing%20the%20generalizability%20and%0Arobustness%20of%20our%20methods.%20Furthermore%2C%20we%20design%20a%20proxy%20task%20that%20enforces%0Athe%20model%20to%20identify%20correlations%20among%20image%20patches%2C%20thereby%20enriching%20the%0Afine-grained%20details%20crucial%20for%20various%20downstream%20tasks.%20Experimental%20results%0Ademonstrate%20that%20our%20proposed%20framework%20achieves%20new%20state-of-the-art%0Aperformance%20on%20multiple%20downstream%20tasks%2C%20including%20classification%2C%0Aimage-to-text%20retrieval%2C%20semantic%20segmentation%2C%20object%20detection%20and%20report%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10573v1&entry.124074799=Read"},
{"title": "Unsupervised Deformable Image Registration with Structural Nonparametric\n  Smoothing", "author": "Hang Zhang and Xiang Chen and Renjiu Hu and Rongguang Wang and Jinwei Zhang and Min Liu and Yaonan Wang and Gaolei Li and Xinxing Cheng and Jinming Duan", "abstract": "  Learning-based deformable image registration (DIR) accelerates alignment by\namortizing traditional optimization via neural networks. Label supervision\nfurther enhances accuracy, enabling efficient and precise nonlinear alignment\nof unseen scans. However, images with sparse features amid large smooth\nregions, such as retinal vessels, introduce aperture and large-displacement\nchallenges that unsupervised DIR methods struggle to address. This limitation\noccurs because neural networks predict deformation fields in a single forward\npass, leaving fields unconstrained post-training and shifting the\nregularization burden entirely to network weights. To address these issues, we\nintroduce SmoothProper, a plug-and-play neural module enforcing smoothness and\npromoting message passing within the network's forward pass. By integrating a\nduality-based optimization layer with tailored interaction terms, SmoothProper\nefficiently propagates flow signals across spatial locations, enforces\nsmoothness, and preserves structural consistency. It is model-agnostic,\nseamlessly integrates into existing registration frameworks with minimal\nparameter overhead, and eliminates regularizer hyperparameter tuning.\nPreliminary results on a retinal vessel dataset exhibiting aperture and\nlarge-displacement challenges demonstrate our method reduces registration error\nto 1.88 pixels on 2912x2912 images, marking the first unsupervised DIR approach\nto effectively address both challenges. The source code will be available at\nhttps://github.com/tinymilky/SmoothProper.\n", "link": "http://arxiv.org/abs/2506.10813v1", "date": "2025-06-12", "relevancy": 2.771, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.556}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Deformable%20Image%20Registration%20with%20Structural%20Nonparametric%0A%20%20Smoothing&body=Title%3A%20Unsupervised%20Deformable%20Image%20Registration%20with%20Structural%20Nonparametric%0A%20%20Smoothing%0AAuthor%3A%20Hang%20Zhang%20and%20Xiang%20Chen%20and%20Renjiu%20Hu%20and%20Rongguang%20Wang%20and%20Jinwei%20Zhang%20and%20Min%20Liu%20and%20Yaonan%20Wang%20and%20Gaolei%20Li%20and%20Xinxing%20Cheng%20and%20Jinming%20Duan%0AAbstract%3A%20%20%20Learning-based%20deformable%20image%20registration%20%28DIR%29%20accelerates%20alignment%20by%0Aamortizing%20traditional%20optimization%20via%20neural%20networks.%20Label%20supervision%0Afurther%20enhances%20accuracy%2C%20enabling%20efficient%20and%20precise%20nonlinear%20alignment%0Aof%20unseen%20scans.%20However%2C%20images%20with%20sparse%20features%20amid%20large%20smooth%0Aregions%2C%20such%20as%20retinal%20vessels%2C%20introduce%20aperture%20and%20large-displacement%0Achallenges%20that%20unsupervised%20DIR%20methods%20struggle%20to%20address.%20This%20limitation%0Aoccurs%20because%20neural%20networks%20predict%20deformation%20fields%20in%20a%20single%20forward%0Apass%2C%20leaving%20fields%20unconstrained%20post-training%20and%20shifting%20the%0Aregularization%20burden%20entirely%20to%20network%20weights.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20SmoothProper%2C%20a%20plug-and-play%20neural%20module%20enforcing%20smoothness%20and%0Apromoting%20message%20passing%20within%20the%20network%27s%20forward%20pass.%20By%20integrating%20a%0Aduality-based%20optimization%20layer%20with%20tailored%20interaction%20terms%2C%20SmoothProper%0Aefficiently%20propagates%20flow%20signals%20across%20spatial%20locations%2C%20enforces%0Asmoothness%2C%20and%20preserves%20structural%20consistency.%20It%20is%20model-agnostic%2C%0Aseamlessly%20integrates%20into%20existing%20registration%20frameworks%20with%20minimal%0Aparameter%20overhead%2C%20and%20eliminates%20regularizer%20hyperparameter%20tuning.%0APreliminary%20results%20on%20a%20retinal%20vessel%20dataset%20exhibiting%20aperture%20and%0Alarge-displacement%20challenges%20demonstrate%20our%20method%20reduces%20registration%20error%0Ato%201.88%20pixels%20on%202912x2912%20images%2C%20marking%20the%20first%20unsupervised%20DIR%20approach%0Ato%20effectively%20address%20both%20challenges.%20The%20source%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/tinymilky/SmoothProper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Deformable%2520Image%2520Registration%2520with%2520Structural%2520Nonparametric%250A%2520%2520Smoothing%26entry.906535625%3DHang%2520Zhang%2520and%2520Xiang%2520Chen%2520and%2520Renjiu%2520Hu%2520and%2520Rongguang%2520Wang%2520and%2520Jinwei%2520Zhang%2520and%2520Min%2520Liu%2520and%2520Yaonan%2520Wang%2520and%2520Gaolei%2520Li%2520and%2520Xinxing%2520Cheng%2520and%2520Jinming%2520Duan%26entry.1292438233%3D%2520%2520Learning-based%2520deformable%2520image%2520registration%2520%2528DIR%2529%2520accelerates%2520alignment%2520by%250Aamortizing%2520traditional%2520optimization%2520via%2520neural%2520networks.%2520Label%2520supervision%250Afurther%2520enhances%2520accuracy%252C%2520enabling%2520efficient%2520and%2520precise%2520nonlinear%2520alignment%250Aof%2520unseen%2520scans.%2520However%252C%2520images%2520with%2520sparse%2520features%2520amid%2520large%2520smooth%250Aregions%252C%2520such%2520as%2520retinal%2520vessels%252C%2520introduce%2520aperture%2520and%2520large-displacement%250Achallenges%2520that%2520unsupervised%2520DIR%2520methods%2520struggle%2520to%2520address.%2520This%2520limitation%250Aoccurs%2520because%2520neural%2520networks%2520predict%2520deformation%2520fields%2520in%2520a%2520single%2520forward%250Apass%252C%2520leaving%2520fields%2520unconstrained%2520post-training%2520and%2520shifting%2520the%250Aregularization%2520burden%2520entirely%2520to%2520network%2520weights.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520SmoothProper%252C%2520a%2520plug-and-play%2520neural%2520module%2520enforcing%2520smoothness%2520and%250Apromoting%2520message%2520passing%2520within%2520the%2520network%2527s%2520forward%2520pass.%2520By%2520integrating%2520a%250Aduality-based%2520optimization%2520layer%2520with%2520tailored%2520interaction%2520terms%252C%2520SmoothProper%250Aefficiently%2520propagates%2520flow%2520signals%2520across%2520spatial%2520locations%252C%2520enforces%250Asmoothness%252C%2520and%2520preserves%2520structural%2520consistency.%2520It%2520is%2520model-agnostic%252C%250Aseamlessly%2520integrates%2520into%2520existing%2520registration%2520frameworks%2520with%2520minimal%250Aparameter%2520overhead%252C%2520and%2520eliminates%2520regularizer%2520hyperparameter%2520tuning.%250APreliminary%2520results%2520on%2520a%2520retinal%2520vessel%2520dataset%2520exhibiting%2520aperture%2520and%250Alarge-displacement%2520challenges%2520demonstrate%2520our%2520method%2520reduces%2520registration%2520error%250Ato%25201.88%2520pixels%2520on%25202912x2912%2520images%252C%2520marking%2520the%2520first%2520unsupervised%2520DIR%2520approach%250Ato%2520effectively%2520address%2520both%2520challenges.%2520The%2520source%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/tinymilky/SmoothProper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Deformable%20Image%20Registration%20with%20Structural%20Nonparametric%0A%20%20Smoothing&entry.906535625=Hang%20Zhang%20and%20Xiang%20Chen%20and%20Renjiu%20Hu%20and%20Rongguang%20Wang%20and%20Jinwei%20Zhang%20and%20Min%20Liu%20and%20Yaonan%20Wang%20and%20Gaolei%20Li%20and%20Xinxing%20Cheng%20and%20Jinming%20Duan&entry.1292438233=%20%20Learning-based%20deformable%20image%20registration%20%28DIR%29%20accelerates%20alignment%20by%0Aamortizing%20traditional%20optimization%20via%20neural%20networks.%20Label%20supervision%0Afurther%20enhances%20accuracy%2C%20enabling%20efficient%20and%20precise%20nonlinear%20alignment%0Aof%20unseen%20scans.%20However%2C%20images%20with%20sparse%20features%20amid%20large%20smooth%0Aregions%2C%20such%20as%20retinal%20vessels%2C%20introduce%20aperture%20and%20large-displacement%0Achallenges%20that%20unsupervised%20DIR%20methods%20struggle%20to%20address.%20This%20limitation%0Aoccurs%20because%20neural%20networks%20predict%20deformation%20fields%20in%20a%20single%20forward%0Apass%2C%20leaving%20fields%20unconstrained%20post-training%20and%20shifting%20the%0Aregularization%20burden%20entirely%20to%20network%20weights.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20SmoothProper%2C%20a%20plug-and-play%20neural%20module%20enforcing%20smoothness%20and%0Apromoting%20message%20passing%20within%20the%20network%27s%20forward%20pass.%20By%20integrating%20a%0Aduality-based%20optimization%20layer%20with%20tailored%20interaction%20terms%2C%20SmoothProper%0Aefficiently%20propagates%20flow%20signals%20across%20spatial%20locations%2C%20enforces%0Asmoothness%2C%20and%20preserves%20structural%20consistency.%20It%20is%20model-agnostic%2C%0Aseamlessly%20integrates%20into%20existing%20registration%20frameworks%20with%20minimal%0Aparameter%20overhead%2C%20and%20eliminates%20regularizer%20hyperparameter%20tuning.%0APreliminary%20results%20on%20a%20retinal%20vessel%20dataset%20exhibiting%20aperture%20and%0Alarge-displacement%20challenges%20demonstrate%20our%20method%20reduces%20registration%20error%0Ato%201.88%20pixels%20on%202912x2912%20images%2C%20marking%20the%20first%20unsupervised%20DIR%20approach%0Ato%20effectively%20address%20both%20challenges.%20The%20source%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/tinymilky/SmoothProper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10813v1&entry.124074799=Read"},
{"title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation", "author": "Haoyuan Shi and Yunxin Li and Xinyu Chen and Longyue Wang and Baotian Hu and Min Zhang", "abstract": "  Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.\n", "link": "http://arxiv.org/abs/2506.10540v1", "date": "2025-06-12", "relevancy": 2.7709, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5636}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5556}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AniMaker%3A%20Automated%20Multi-Agent%20Animated%20Storytelling%20with%20MCTS-Driven%0A%20%20Clip%20Generation&body=Title%3A%20AniMaker%3A%20Automated%20Multi-Agent%20Animated%20Storytelling%20with%20MCTS-Driven%0A%20%20Clip%20Generation%0AAuthor%3A%20Haoyuan%20Shi%20and%20Yunxin%20Li%20and%20Xinyu%20Chen%20and%20Longyue%20Wang%20and%20Baotian%20Hu%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Despite%20rapid%20advancements%20in%20video%20generation%20models%2C%20generating%20coherent%0Astorytelling%20videos%20that%20span%20multiple%20scenes%20and%20characters%20remains%0Achallenging.%20Current%20methods%20often%20rigidly%20convert%20pre-generated%20keyframes%20into%0Afixed-length%20clips%2C%20resulting%20in%20disjointed%20narratives%20and%20pacing%20issues.%0AFurthermore%2C%20the%20inherent%20instability%20of%20video%20generation%20models%20means%20that%0Aeven%20a%20single%20low-quality%20clip%20can%20significantly%20degrade%20the%20entire%20output%0Aanimation%27s%20logical%20coherence%20and%20visual%20continuity.%20To%20overcome%20these%0Aobstacles%2C%20we%20introduce%20AniMaker%2C%20a%20multi-agent%20framework%20enabling%20efficient%0Amulti-candidate%20clip%20generation%20and%20storytelling-aware%20clip%20selection%2C%20thus%0Acreating%20globally%20consistent%20and%20story-coherent%20animation%20solely%20from%20text%0Ainput.%20The%20framework%20is%20structured%20around%20specialized%20agents%2C%20including%20the%0ADirector%20Agent%20for%20storyboard%20generation%2C%20the%20Photography%20Agent%20for%20video%20clip%0Ageneration%2C%20the%20Reviewer%20Agent%20for%20evaluation%2C%20and%20the%20Post-Production%20Agent%0Afor%20editing%20and%20voiceover.%20Central%20to%20AniMaker%27s%20approach%20are%20two%20key%20technical%0Acomponents%3A%20MCTS-Gen%20in%20Photography%20Agent%2C%20an%20efficient%20Monte%20Carlo%20Tree%20Search%0A%28MCTS%29-inspired%20strategy%20that%20intelligently%20navigates%20the%20candidate%20space%20to%0Agenerate%20high-potential%20clips%20while%20optimizing%20resource%20usage%3B%20and%20AniEval%20in%0AReviewer%20Agent%2C%20the%20first%20framework%20specifically%20designed%20for%20multi-shot%0Aanimation%20evaluation%2C%20which%20assesses%20critical%20aspects%20such%20as%20story-level%0Aconsistency%2C%20action%20completion%2C%20and%20animation-specific%20features%20by%20considering%0Aeach%20clip%20in%20the%20context%20of%20its%20preceding%20and%20succeeding%20clips.%20Experiments%0Ademonstrate%20that%20AniMaker%20achieves%20superior%20quality%20as%20measured%20by%20popular%0Ametrics%20including%20VBench%20and%20our%20proposed%20AniEval%20framework%2C%20while%0Asignificantly%20improving%20the%20efficiency%20of%20multi-candidate%20generation%2C%20pushing%0AAI-generated%20storytelling%20animation%20closer%20to%20production%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAniMaker%253A%2520Automated%2520Multi-Agent%2520Animated%2520Storytelling%2520with%2520MCTS-Driven%250A%2520%2520Clip%2520Generation%26entry.906535625%3DHaoyuan%2520Shi%2520and%2520Yunxin%2520Li%2520and%2520Xinyu%2520Chen%2520and%2520Longyue%2520Wang%2520and%2520Baotian%2520Hu%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520rapid%2520advancements%2520in%2520video%2520generation%2520models%252C%2520generating%2520coherent%250Astorytelling%2520videos%2520that%2520span%2520multiple%2520scenes%2520and%2520characters%2520remains%250Achallenging.%2520Current%2520methods%2520often%2520rigidly%2520convert%2520pre-generated%2520keyframes%2520into%250Afixed-length%2520clips%252C%2520resulting%2520in%2520disjointed%2520narratives%2520and%2520pacing%2520issues.%250AFurthermore%252C%2520the%2520inherent%2520instability%2520of%2520video%2520generation%2520models%2520means%2520that%250Aeven%2520a%2520single%2520low-quality%2520clip%2520can%2520significantly%2520degrade%2520the%2520entire%2520output%250Aanimation%2527s%2520logical%2520coherence%2520and%2520visual%2520continuity.%2520To%2520overcome%2520these%250Aobstacles%252C%2520we%2520introduce%2520AniMaker%252C%2520a%2520multi-agent%2520framework%2520enabling%2520efficient%250Amulti-candidate%2520clip%2520generation%2520and%2520storytelling-aware%2520clip%2520selection%252C%2520thus%250Acreating%2520globally%2520consistent%2520and%2520story-coherent%2520animation%2520solely%2520from%2520text%250Ainput.%2520The%2520framework%2520is%2520structured%2520around%2520specialized%2520agents%252C%2520including%2520the%250ADirector%2520Agent%2520for%2520storyboard%2520generation%252C%2520the%2520Photography%2520Agent%2520for%2520video%2520clip%250Ageneration%252C%2520the%2520Reviewer%2520Agent%2520for%2520evaluation%252C%2520and%2520the%2520Post-Production%2520Agent%250Afor%2520editing%2520and%2520voiceover.%2520Central%2520to%2520AniMaker%2527s%2520approach%2520are%2520two%2520key%2520technical%250Acomponents%253A%2520MCTS-Gen%2520in%2520Photography%2520Agent%252C%2520an%2520efficient%2520Monte%2520Carlo%2520Tree%2520Search%250A%2528MCTS%2529-inspired%2520strategy%2520that%2520intelligently%2520navigates%2520the%2520candidate%2520space%2520to%250Agenerate%2520high-potential%2520clips%2520while%2520optimizing%2520resource%2520usage%253B%2520and%2520AniEval%2520in%250AReviewer%2520Agent%252C%2520the%2520first%2520framework%2520specifically%2520designed%2520for%2520multi-shot%250Aanimation%2520evaluation%252C%2520which%2520assesses%2520critical%2520aspects%2520such%2520as%2520story-level%250Aconsistency%252C%2520action%2520completion%252C%2520and%2520animation-specific%2520features%2520by%2520considering%250Aeach%2520clip%2520in%2520the%2520context%2520of%2520its%2520preceding%2520and%2520succeeding%2520clips.%2520Experiments%250Ademonstrate%2520that%2520AniMaker%2520achieves%2520superior%2520quality%2520as%2520measured%2520by%2520popular%250Ametrics%2520including%2520VBench%2520and%2520our%2520proposed%2520AniEval%2520framework%252C%2520while%250Asignificantly%2520improving%2520the%2520efficiency%2520of%2520multi-candidate%2520generation%252C%2520pushing%250AAI-generated%2520storytelling%2520animation%2520closer%2520to%2520production%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AniMaker%3A%20Automated%20Multi-Agent%20Animated%20Storytelling%20with%20MCTS-Driven%0A%20%20Clip%20Generation&entry.906535625=Haoyuan%20Shi%20and%20Yunxin%20Li%20and%20Xinyu%20Chen%20and%20Longyue%20Wang%20and%20Baotian%20Hu%20and%20Min%20Zhang&entry.1292438233=%20%20Despite%20rapid%20advancements%20in%20video%20generation%20models%2C%20generating%20coherent%0Astorytelling%20videos%20that%20span%20multiple%20scenes%20and%20characters%20remains%0Achallenging.%20Current%20methods%20often%20rigidly%20convert%20pre-generated%20keyframes%20into%0Afixed-length%20clips%2C%20resulting%20in%20disjointed%20narratives%20and%20pacing%20issues.%0AFurthermore%2C%20the%20inherent%20instability%20of%20video%20generation%20models%20means%20that%0Aeven%20a%20single%20low-quality%20clip%20can%20significantly%20degrade%20the%20entire%20output%0Aanimation%27s%20logical%20coherence%20and%20visual%20continuity.%20To%20overcome%20these%0Aobstacles%2C%20we%20introduce%20AniMaker%2C%20a%20multi-agent%20framework%20enabling%20efficient%0Amulti-candidate%20clip%20generation%20and%20storytelling-aware%20clip%20selection%2C%20thus%0Acreating%20globally%20consistent%20and%20story-coherent%20animation%20solely%20from%20text%0Ainput.%20The%20framework%20is%20structured%20around%20specialized%20agents%2C%20including%20the%0ADirector%20Agent%20for%20storyboard%20generation%2C%20the%20Photography%20Agent%20for%20video%20clip%0Ageneration%2C%20the%20Reviewer%20Agent%20for%20evaluation%2C%20and%20the%20Post-Production%20Agent%0Afor%20editing%20and%20voiceover.%20Central%20to%20AniMaker%27s%20approach%20are%20two%20key%20technical%0Acomponents%3A%20MCTS-Gen%20in%20Photography%20Agent%2C%20an%20efficient%20Monte%20Carlo%20Tree%20Search%0A%28MCTS%29-inspired%20strategy%20that%20intelligently%20navigates%20the%20candidate%20space%20to%0Agenerate%20high-potential%20clips%20while%20optimizing%20resource%20usage%3B%20and%20AniEval%20in%0AReviewer%20Agent%2C%20the%20first%20framework%20specifically%20designed%20for%20multi-shot%0Aanimation%20evaluation%2C%20which%20assesses%20critical%20aspects%20such%20as%20story-level%0Aconsistency%2C%20action%20completion%2C%20and%20animation-specific%20features%20by%20considering%0Aeach%20clip%20in%20the%20context%20of%20its%20preceding%20and%20succeeding%20clips.%20Experiments%0Ademonstrate%20that%20AniMaker%20achieves%20superior%20quality%20as%20measured%20by%20popular%0Ametrics%20including%20VBench%20and%20our%20proposed%20AniEval%20framework%2C%20while%0Asignificantly%20improving%20the%20efficiency%20of%20multi-candidate%20generation%2C%20pushing%0AAI-generated%20storytelling%20animation%20closer%20to%20production%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10540v1&entry.124074799=Read"},
{"title": "Generalist Models in Medical Image Segmentation: A Survey and\n  Performance Comparison with Task-Specific Approaches", "author": "Andrea Moglia and Matteo Leccardi and Matteo Cavicchioli and Alice Maccarini and Marco Marcon and Luca Mainardi and Pietro Cerveri", "abstract": "  Following the successful paradigm shift of large language models, leveraging\npre-training on a massive corpus of data and fine-tuning on different\ndownstream tasks, generalist models have made their foray into computer vision.\nThe introduction of Segment Anything Model (SAM) set a milestone on\nsegmentation of natural images, inspiring the design of a multitude of\narchitectures for medical image segmentation. In this survey we offer a\ncomprehensive and in-depth investigation on generalist models for medical image\nsegmentation. We start with an introduction on the fundamentals concepts\nunderpinning their development. Then, we provide a taxonomy on the different\ndeclinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on\nthe recent SAM 2, on other innovative models trained on images alone, and\nothers trained on both text and images. We thoroughly analyze their\nperformances at the level of both primary research and best-in-literature,\nfollowed by a rigorous comparison with the state-of-the-art task-specific\nmodels. We emphasize the need to address challenges in terms of compliance with\nregulatory frameworks, privacy and security laws, budget, and trustworthy\nartificial intelligence (AI). Finally, we share our perspective on future\ndirections concerning synthetic data, early fusion, lessons learnt from\ngeneralist models in natural language processing, agentic AI and physical AI,\nand clinical translation.\n", "link": "http://arxiv.org/abs/2506.10825v1", "date": "2025-06-12", "relevancy": 2.7615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalist%20Models%20in%20Medical%20Image%20Segmentation%3A%20A%20Survey%20and%0A%20%20Performance%20Comparison%20with%20Task-Specific%20Approaches&body=Title%3A%20Generalist%20Models%20in%20Medical%20Image%20Segmentation%3A%20A%20Survey%20and%0A%20%20Performance%20Comparison%20with%20Task-Specific%20Approaches%0AAuthor%3A%20Andrea%20Moglia%20and%20Matteo%20Leccardi%20and%20Matteo%20Cavicchioli%20and%20Alice%20Maccarini%20and%20Marco%20Marcon%20and%20Luca%20Mainardi%20and%20Pietro%20Cerveri%0AAbstract%3A%20%20%20Following%20the%20successful%20paradigm%20shift%20of%20large%20language%20models%2C%20leveraging%0Apre-training%20on%20a%20massive%20corpus%20of%20data%20and%20fine-tuning%20on%20different%0Adownstream%20tasks%2C%20generalist%20models%20have%20made%20their%20foray%20into%20computer%20vision.%0AThe%20introduction%20of%20Segment%20Anything%20Model%20%28SAM%29%20set%20a%20milestone%20on%0Asegmentation%20of%20natural%20images%2C%20inspiring%20the%20design%20of%20a%20multitude%20of%0Aarchitectures%20for%20medical%20image%20segmentation.%20In%20this%20survey%20we%20offer%20a%0Acomprehensive%20and%20in-depth%20investigation%20on%20generalist%20models%20for%20medical%20image%0Asegmentation.%20We%20start%20with%20an%20introduction%20on%20the%20fundamentals%20concepts%0Aunderpinning%20their%20development.%20Then%2C%20we%20provide%20a%20taxonomy%20on%20the%20different%0Adeclinations%20of%20SAM%20in%20terms%20of%20zero-shot%2C%20few-shot%2C%20fine-tuning%2C%20adapters%2C%20on%0Athe%20recent%20SAM%202%2C%20on%20other%20innovative%20models%20trained%20on%20images%20alone%2C%20and%0Aothers%20trained%20on%20both%20text%20and%20images.%20We%20thoroughly%20analyze%20their%0Aperformances%20at%20the%20level%20of%20both%20primary%20research%20and%20best-in-literature%2C%0Afollowed%20by%20a%20rigorous%20comparison%20with%20the%20state-of-the-art%20task-specific%0Amodels.%20We%20emphasize%20the%20need%20to%20address%20challenges%20in%20terms%20of%20compliance%20with%0Aregulatory%20frameworks%2C%20privacy%20and%20security%20laws%2C%20budget%2C%20and%20trustworthy%0Aartificial%20intelligence%20%28AI%29.%20Finally%2C%20we%20share%20our%20perspective%20on%20future%0Adirections%20concerning%20synthetic%20data%2C%20early%20fusion%2C%20lessons%20learnt%20from%0Ageneralist%20models%20in%20natural%20language%20processing%2C%20agentic%20AI%20and%20physical%20AI%2C%0Aand%20clinical%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralist%2520Models%2520in%2520Medical%2520Image%2520Segmentation%253A%2520A%2520Survey%2520and%250A%2520%2520Performance%2520Comparison%2520with%2520Task-Specific%2520Approaches%26entry.906535625%3DAndrea%2520Moglia%2520and%2520Matteo%2520Leccardi%2520and%2520Matteo%2520Cavicchioli%2520and%2520Alice%2520Maccarini%2520and%2520Marco%2520Marcon%2520and%2520Luca%2520Mainardi%2520and%2520Pietro%2520Cerveri%26entry.1292438233%3D%2520%2520Following%2520the%2520successful%2520paradigm%2520shift%2520of%2520large%2520language%2520models%252C%2520leveraging%250Apre-training%2520on%2520a%2520massive%2520corpus%2520of%2520data%2520and%2520fine-tuning%2520on%2520different%250Adownstream%2520tasks%252C%2520generalist%2520models%2520have%2520made%2520their%2520foray%2520into%2520computer%2520vision.%250AThe%2520introduction%2520of%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520set%2520a%2520milestone%2520on%250Asegmentation%2520of%2520natural%2520images%252C%2520inspiring%2520the%2520design%2520of%2520a%2520multitude%2520of%250Aarchitectures%2520for%2520medical%2520image%2520segmentation.%2520In%2520this%2520survey%2520we%2520offer%2520a%250Acomprehensive%2520and%2520in-depth%2520investigation%2520on%2520generalist%2520models%2520for%2520medical%2520image%250Asegmentation.%2520We%2520start%2520with%2520an%2520introduction%2520on%2520the%2520fundamentals%2520concepts%250Aunderpinning%2520their%2520development.%2520Then%252C%2520we%2520provide%2520a%2520taxonomy%2520on%2520the%2520different%250Adeclinations%2520of%2520SAM%2520in%2520terms%2520of%2520zero-shot%252C%2520few-shot%252C%2520fine-tuning%252C%2520adapters%252C%2520on%250Athe%2520recent%2520SAM%25202%252C%2520on%2520other%2520innovative%2520models%2520trained%2520on%2520images%2520alone%252C%2520and%250Aothers%2520trained%2520on%2520both%2520text%2520and%2520images.%2520We%2520thoroughly%2520analyze%2520their%250Aperformances%2520at%2520the%2520level%2520of%2520both%2520primary%2520research%2520and%2520best-in-literature%252C%250Afollowed%2520by%2520a%2520rigorous%2520comparison%2520with%2520the%2520state-of-the-art%2520task-specific%250Amodels.%2520We%2520emphasize%2520the%2520need%2520to%2520address%2520challenges%2520in%2520terms%2520of%2520compliance%2520with%250Aregulatory%2520frameworks%252C%2520privacy%2520and%2520security%2520laws%252C%2520budget%252C%2520and%2520trustworthy%250Aartificial%2520intelligence%2520%2528AI%2529.%2520Finally%252C%2520we%2520share%2520our%2520perspective%2520on%2520future%250Adirections%2520concerning%2520synthetic%2520data%252C%2520early%2520fusion%252C%2520lessons%2520learnt%2520from%250Ageneralist%2520models%2520in%2520natural%2520language%2520processing%252C%2520agentic%2520AI%2520and%2520physical%2520AI%252C%250Aand%2520clinical%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalist%20Models%20in%20Medical%20Image%20Segmentation%3A%20A%20Survey%20and%0A%20%20Performance%20Comparison%20with%20Task-Specific%20Approaches&entry.906535625=Andrea%20Moglia%20and%20Matteo%20Leccardi%20and%20Matteo%20Cavicchioli%20and%20Alice%20Maccarini%20and%20Marco%20Marcon%20and%20Luca%20Mainardi%20and%20Pietro%20Cerveri&entry.1292438233=%20%20Following%20the%20successful%20paradigm%20shift%20of%20large%20language%20models%2C%20leveraging%0Apre-training%20on%20a%20massive%20corpus%20of%20data%20and%20fine-tuning%20on%20different%0Adownstream%20tasks%2C%20generalist%20models%20have%20made%20their%20foray%20into%20computer%20vision.%0AThe%20introduction%20of%20Segment%20Anything%20Model%20%28SAM%29%20set%20a%20milestone%20on%0Asegmentation%20of%20natural%20images%2C%20inspiring%20the%20design%20of%20a%20multitude%20of%0Aarchitectures%20for%20medical%20image%20segmentation.%20In%20this%20survey%20we%20offer%20a%0Acomprehensive%20and%20in-depth%20investigation%20on%20generalist%20models%20for%20medical%20image%0Asegmentation.%20We%20start%20with%20an%20introduction%20on%20the%20fundamentals%20concepts%0Aunderpinning%20their%20development.%20Then%2C%20we%20provide%20a%20taxonomy%20on%20the%20different%0Adeclinations%20of%20SAM%20in%20terms%20of%20zero-shot%2C%20few-shot%2C%20fine-tuning%2C%20adapters%2C%20on%0Athe%20recent%20SAM%202%2C%20on%20other%20innovative%20models%20trained%20on%20images%20alone%2C%20and%0Aothers%20trained%20on%20both%20text%20and%20images.%20We%20thoroughly%20analyze%20their%0Aperformances%20at%20the%20level%20of%20both%20primary%20research%20and%20best-in-literature%2C%0Afollowed%20by%20a%20rigorous%20comparison%20with%20the%20state-of-the-art%20task-specific%0Amodels.%20We%20emphasize%20the%20need%20to%20address%20challenges%20in%20terms%20of%20compliance%20with%0Aregulatory%20frameworks%2C%20privacy%20and%20security%20laws%2C%20budget%2C%20and%20trustworthy%0Aartificial%20intelligence%20%28AI%29.%20Finally%2C%20we%20share%20our%20perspective%20on%20future%0Adirections%20concerning%20synthetic%20data%2C%20early%20fusion%2C%20lessons%20learnt%20from%0Ageneralist%20models%20in%20natural%20language%20processing%2C%20agentic%20AI%20and%20physical%20AI%2C%0Aand%20clinical%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10825v1&entry.124074799=Read"},
{"title": "SpectralAR: Spectral Autoregressive Visual Generation", "author": "Yuanhui Huang and Weiliang Chen and Wenzhao Zheng and Yueqi Duan and Jie Zhou and Jiwen Lu", "abstract": "  Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.\n", "link": "http://arxiv.org/abs/2506.10962v1", "date": "2025-06-12", "relevancy": 2.7524, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5647}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.545}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectralAR%3A%20Spectral%20Autoregressive%20Visual%20Generation&body=Title%3A%20SpectralAR%3A%20Spectral%20Autoregressive%20Visual%20Generation%0AAuthor%3A%20Yuanhui%20Huang%20and%20Weiliang%20Chen%20and%20Wenzhao%20Zheng%20and%20Yueqi%20Duan%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Autoregressive%20visual%20generation%20has%20garnered%20increasing%20attention%20due%20to%20its%0Ascalability%20and%20compatibility%20with%20other%20modalities%20compared%20with%20diffusion%0Amodels.%20Most%20existing%20methods%20construct%20visual%20sequences%20as%20spatial%20patches%20for%0Aautoregressive%20generation.%20However%2C%20image%20patches%20are%20inherently%20parallel%2C%0Acontradicting%20the%20causal%20nature%20of%20autoregressive%20modeling.%20To%20address%20this%2C%20we%0Apropose%20a%20Spectral%20AutoRegressive%20%28SpectralAR%29%20visual%20generation%20framework%2C%0Awhich%20realizes%20causality%20for%20visual%20sequences%20from%20the%20spectral%20perspective.%0ASpecifically%2C%20we%20first%20transform%20an%20image%20into%20ordered%20spectral%20tokens%20with%0ANested%20Spectral%20Tokenization%2C%20representing%20lower%20to%20higher%20frequency%0Acomponents.%20We%20then%20perform%20autoregressive%20generation%20in%20a%20coarse-to-fine%0Amanner%20with%20the%20sequences%20of%20spectral%20tokens.%20By%20considering%20different%20levels%0Aof%20detail%20in%20images%2C%20our%20SpectralAR%20achieves%20both%20sequence%20causality%20and%20token%0Aefficiency%20without%20bells%20and%20whistles.%20We%20conduct%20extensive%20experiments%20on%0AImageNet-1K%20for%20image%20reconstruction%20and%20autoregressive%20generation%2C%20and%0ASpectralAR%20achieves%203.02%20gFID%20with%20only%2064%20tokens%20and%20310M%20parameters.%20Project%0Apage%3A%20https%3A//huang-yh.github.io/spectralar/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectralAR%253A%2520Spectral%2520Autoregressive%2520Visual%2520Generation%26entry.906535625%3DYuanhui%2520Huang%2520and%2520Weiliang%2520Chen%2520and%2520Wenzhao%2520Zheng%2520and%2520Yueqi%2520Duan%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Autoregressive%2520visual%2520generation%2520has%2520garnered%2520increasing%2520attention%2520due%2520to%2520its%250Ascalability%2520and%2520compatibility%2520with%2520other%2520modalities%2520compared%2520with%2520diffusion%250Amodels.%2520Most%2520existing%2520methods%2520construct%2520visual%2520sequences%2520as%2520spatial%2520patches%2520for%250Aautoregressive%2520generation.%2520However%252C%2520image%2520patches%2520are%2520inherently%2520parallel%252C%250Acontradicting%2520the%2520causal%2520nature%2520of%2520autoregressive%2520modeling.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520Spectral%2520AutoRegressive%2520%2528SpectralAR%2529%2520visual%2520generation%2520framework%252C%250Awhich%2520realizes%2520causality%2520for%2520visual%2520sequences%2520from%2520the%2520spectral%2520perspective.%250ASpecifically%252C%2520we%2520first%2520transform%2520an%2520image%2520into%2520ordered%2520spectral%2520tokens%2520with%250ANested%2520Spectral%2520Tokenization%252C%2520representing%2520lower%2520to%2520higher%2520frequency%250Acomponents.%2520We%2520then%2520perform%2520autoregressive%2520generation%2520in%2520a%2520coarse-to-fine%250Amanner%2520with%2520the%2520sequences%2520of%2520spectral%2520tokens.%2520By%2520considering%2520different%2520levels%250Aof%2520detail%2520in%2520images%252C%2520our%2520SpectralAR%2520achieves%2520both%2520sequence%2520causality%2520and%2520token%250Aefficiency%2520without%2520bells%2520and%2520whistles.%2520We%2520conduct%2520extensive%2520experiments%2520on%250AImageNet-1K%2520for%2520image%2520reconstruction%2520and%2520autoregressive%2520generation%252C%2520and%250ASpectralAR%2520achieves%25203.02%2520gFID%2520with%2520only%252064%2520tokens%2520and%2520310M%2520parameters.%2520Project%250Apage%253A%2520https%253A//huang-yh.github.io/spectralar/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectralAR%3A%20Spectral%20Autoregressive%20Visual%20Generation&entry.906535625=Yuanhui%20Huang%20and%20Weiliang%20Chen%20and%20Wenzhao%20Zheng%20and%20Yueqi%20Duan%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Autoregressive%20visual%20generation%20has%20garnered%20increasing%20attention%20due%20to%20its%0Ascalability%20and%20compatibility%20with%20other%20modalities%20compared%20with%20diffusion%0Amodels.%20Most%20existing%20methods%20construct%20visual%20sequences%20as%20spatial%20patches%20for%0Aautoregressive%20generation.%20However%2C%20image%20patches%20are%20inherently%20parallel%2C%0Acontradicting%20the%20causal%20nature%20of%20autoregressive%20modeling.%20To%20address%20this%2C%20we%0Apropose%20a%20Spectral%20AutoRegressive%20%28SpectralAR%29%20visual%20generation%20framework%2C%0Awhich%20realizes%20causality%20for%20visual%20sequences%20from%20the%20spectral%20perspective.%0ASpecifically%2C%20we%20first%20transform%20an%20image%20into%20ordered%20spectral%20tokens%20with%0ANested%20Spectral%20Tokenization%2C%20representing%20lower%20to%20higher%20frequency%0Acomponents.%20We%20then%20perform%20autoregressive%20generation%20in%20a%20coarse-to-fine%0Amanner%20with%20the%20sequences%20of%20spectral%20tokens.%20By%20considering%20different%20levels%0Aof%20detail%20in%20images%2C%20our%20SpectralAR%20achieves%20both%20sequence%20causality%20and%20token%0Aefficiency%20without%20bells%20and%20whistles.%20We%20conduct%20extensive%20experiments%20on%0AImageNet-1K%20for%20image%20reconstruction%20and%20autoregressive%20generation%2C%20and%0ASpectralAR%20achieves%203.02%20gFID%20with%20only%2064%20tokens%20and%20310M%20parameters.%20Project%0Apage%3A%20https%3A//huang-yh.github.io/spectralar/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10962v1&entry.124074799=Read"},
{"title": "DanceChat: Large Language Model-Guided Music-to-Dance Generation", "author": "Qing Wang and Xiaohang Yang and Yilan Dong and Naveen Raj Govindaraj and Gregory Slabaugh and Shanxin Yuan", "abstract": "  Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2506.10574v1", "date": "2025-06-12", "relevancy": 2.7524, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5773}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5429}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DanceChat%3A%20Large%20Language%20Model-Guided%20Music-to-Dance%20Generation&body=Title%3A%20DanceChat%3A%20Large%20Language%20Model-Guided%20Music-to-Dance%20Generation%0AAuthor%3A%20Qing%20Wang%20and%20Xiaohang%20Yang%20and%20Yilan%20Dong%20and%20Naveen%20Raj%20Govindaraj%20and%20Gregory%20Slabaugh%20and%20Shanxin%20Yuan%0AAbstract%3A%20%20%20Music-to-dance%20generation%20aims%20to%20synthesize%20human%20dance%20motion%20conditioned%0Aon%20musical%20input.%20Despite%20recent%20progress%2C%20significant%20challenges%20remain%20due%20to%0Athe%20semantic%20gap%20between%20music%20and%20dance%20motion%2C%20as%20music%20offers%20only%20abstract%0Acues%2C%20such%20as%20melody%2C%20groove%2C%20and%20emotion%2C%20without%20explicitly%20specifying%20the%0Aphysical%20movements.%20Moreover%2C%20a%20single%20piece%20of%20music%20can%20produce%20multiple%0Aplausible%20dance%20interpretations.%20This%20one-to-many%20mapping%20demands%20additional%0Aguidance%2C%20as%20music%20alone%20provides%20limited%20information%20for%20generating%20diverse%0Adance%20movements.%20The%20challenge%20is%20further%20amplified%20by%20the%20scarcity%20of%20paired%0Amusic%20and%20dance%20data%2C%20which%20restricts%20the%20model%5C%5Ea%5Cu%7BA%7D%5C%27Zs%20ability%20to%20learn%0Adiverse%20dance%20patterns.%20In%20this%20paper%2C%20we%20introduce%20DanceChat%2C%20a%20Large%20Language%0AModel%20%28LLM%29-guided%20music-to-dance%20generation%20approach.%20We%20use%20an%20LLM%20as%20a%0Achoreographer%20that%20provides%20textual%20motion%20instructions%2C%20offering%20explicit%2C%0Ahigh-level%20guidance%20for%20dance%20generation.%20This%20approach%20goes%20beyond%20implicit%0Alearning%20from%20music%20alone%2C%20enabling%20the%20model%20to%20generate%20dance%20that%20is%20both%0Amore%20diverse%20and%20better%20aligned%20with%20musical%20styles.%20Our%20approach%20consists%20of%0Athree%20components%3A%20%281%29%20an%20LLM-based%20pseudo%20instruction%20generation%20module%20that%0Aproduces%20textual%20dance%20guidance%20based%20on%20music%20style%20and%20structure%2C%20%282%29%20a%0Amulti-modal%20feature%20extraction%20and%20fusion%20module%20that%20integrates%20music%2C%20rhythm%2C%0Aand%20textual%20guidance%20into%20a%20shared%20representation%2C%20and%20%283%29%20a%20diffusion-based%0Amotion%20synthesis%20module%20together%20with%20a%20multi-modal%20alignment%20loss%2C%20which%0Aensures%20that%20the%20generated%20dance%20is%20aligned%20with%20both%20musical%20and%20textual%20cues.%0AExtensive%20experiments%20on%20AIST%2B%2B%20and%20human%20evaluations%20show%20that%20DanceChat%0Aoutperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDanceChat%253A%2520Large%2520Language%2520Model-Guided%2520Music-to-Dance%2520Generation%26entry.906535625%3DQing%2520Wang%2520and%2520Xiaohang%2520Yang%2520and%2520Yilan%2520Dong%2520and%2520Naveen%2520Raj%2520Govindaraj%2520and%2520Gregory%2520Slabaugh%2520and%2520Shanxin%2520Yuan%26entry.1292438233%3D%2520%2520Music-to-dance%2520generation%2520aims%2520to%2520synthesize%2520human%2520dance%2520motion%2520conditioned%250Aon%2520musical%2520input.%2520Despite%2520recent%2520progress%252C%2520significant%2520challenges%2520remain%2520due%2520to%250Athe%2520semantic%2520gap%2520between%2520music%2520and%2520dance%2520motion%252C%2520as%2520music%2520offers%2520only%2520abstract%250Acues%252C%2520such%2520as%2520melody%252C%2520groove%252C%2520and%2520emotion%252C%2520without%2520explicitly%2520specifying%2520the%250Aphysical%2520movements.%2520Moreover%252C%2520a%2520single%2520piece%2520of%2520music%2520can%2520produce%2520multiple%250Aplausible%2520dance%2520interpretations.%2520This%2520one-to-many%2520mapping%2520demands%2520additional%250Aguidance%252C%2520as%2520music%2520alone%2520provides%2520limited%2520information%2520for%2520generating%2520diverse%250Adance%2520movements.%2520The%2520challenge%2520is%2520further%2520amplified%2520by%2520the%2520scarcity%2520of%2520paired%250Amusic%2520and%2520dance%2520data%252C%2520which%2520restricts%2520the%2520model%255C%255Ea%255Cu%257BA%257D%255C%2527Zs%2520ability%2520to%2520learn%250Adiverse%2520dance%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DanceChat%252C%2520a%2520Large%2520Language%250AModel%2520%2528LLM%2529-guided%2520music-to-dance%2520generation%2520approach.%2520We%2520use%2520an%2520LLM%2520as%2520a%250Achoreographer%2520that%2520provides%2520textual%2520motion%2520instructions%252C%2520offering%2520explicit%252C%250Ahigh-level%2520guidance%2520for%2520dance%2520generation.%2520This%2520approach%2520goes%2520beyond%2520implicit%250Alearning%2520from%2520music%2520alone%252C%2520enabling%2520the%2520model%2520to%2520generate%2520dance%2520that%2520is%2520both%250Amore%2520diverse%2520and%2520better%2520aligned%2520with%2520musical%2520styles.%2520Our%2520approach%2520consists%2520of%250Athree%2520components%253A%2520%25281%2529%2520an%2520LLM-based%2520pseudo%2520instruction%2520generation%2520module%2520that%250Aproduces%2520textual%2520dance%2520guidance%2520based%2520on%2520music%2520style%2520and%2520structure%252C%2520%25282%2529%2520a%250Amulti-modal%2520feature%2520extraction%2520and%2520fusion%2520module%2520that%2520integrates%2520music%252C%2520rhythm%252C%250Aand%2520textual%2520guidance%2520into%2520a%2520shared%2520representation%252C%2520and%2520%25283%2529%2520a%2520diffusion-based%250Amotion%2520synthesis%2520module%2520together%2520with%2520a%2520multi-modal%2520alignment%2520loss%252C%2520which%250Aensures%2520that%2520the%2520generated%2520dance%2520is%2520aligned%2520with%2520both%2520musical%2520and%2520textual%2520cues.%250AExtensive%2520experiments%2520on%2520AIST%252B%252B%2520and%2520human%2520evaluations%2520show%2520that%2520DanceChat%250Aoutperforms%2520state-of-the-art%2520methods%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanceChat%3A%20Large%20Language%20Model-Guided%20Music-to-Dance%20Generation&entry.906535625=Qing%20Wang%20and%20Xiaohang%20Yang%20and%20Yilan%20Dong%20and%20Naveen%20Raj%20Govindaraj%20and%20Gregory%20Slabaugh%20and%20Shanxin%20Yuan&entry.1292438233=%20%20Music-to-dance%20generation%20aims%20to%20synthesize%20human%20dance%20motion%20conditioned%0Aon%20musical%20input.%20Despite%20recent%20progress%2C%20significant%20challenges%20remain%20due%20to%0Athe%20semantic%20gap%20between%20music%20and%20dance%20motion%2C%20as%20music%20offers%20only%20abstract%0Acues%2C%20such%20as%20melody%2C%20groove%2C%20and%20emotion%2C%20without%20explicitly%20specifying%20the%0Aphysical%20movements.%20Moreover%2C%20a%20single%20piece%20of%20music%20can%20produce%20multiple%0Aplausible%20dance%20interpretations.%20This%20one-to-many%20mapping%20demands%20additional%0Aguidance%2C%20as%20music%20alone%20provides%20limited%20information%20for%20generating%20diverse%0Adance%20movements.%20The%20challenge%20is%20further%20amplified%20by%20the%20scarcity%20of%20paired%0Amusic%20and%20dance%20data%2C%20which%20restricts%20the%20model%5C%5Ea%5Cu%7BA%7D%5C%27Zs%20ability%20to%20learn%0Adiverse%20dance%20patterns.%20In%20this%20paper%2C%20we%20introduce%20DanceChat%2C%20a%20Large%20Language%0AModel%20%28LLM%29-guided%20music-to-dance%20generation%20approach.%20We%20use%20an%20LLM%20as%20a%0Achoreographer%20that%20provides%20textual%20motion%20instructions%2C%20offering%20explicit%2C%0Ahigh-level%20guidance%20for%20dance%20generation.%20This%20approach%20goes%20beyond%20implicit%0Alearning%20from%20music%20alone%2C%20enabling%20the%20model%20to%20generate%20dance%20that%20is%20both%0Amore%20diverse%20and%20better%20aligned%20with%20musical%20styles.%20Our%20approach%20consists%20of%0Athree%20components%3A%20%281%29%20an%20LLM-based%20pseudo%20instruction%20generation%20module%20that%0Aproduces%20textual%20dance%20guidance%20based%20on%20music%20style%20and%20structure%2C%20%282%29%20a%0Amulti-modal%20feature%20extraction%20and%20fusion%20module%20that%20integrates%20music%2C%20rhythm%2C%0Aand%20textual%20guidance%20into%20a%20shared%20representation%2C%20and%20%283%29%20a%20diffusion-based%0Amotion%20synthesis%20module%20together%20with%20a%20multi-modal%20alignment%20loss%2C%20which%0Aensures%20that%20the%20generated%20dance%20is%20aligned%20with%20both%20musical%20and%20textual%20cues.%0AExtensive%20experiments%20on%20AIST%2B%2B%20and%20human%20evaluations%20show%20that%20DanceChat%0Aoutperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10574v1&entry.124074799=Read"},
{"title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video\n  Generation via Motion-designed Diffusion Transformers", "author": "Lizhen Wang and Zhurong Xia and Tianshu Hu and Pengrui Wang and Pengfei Wang and Zerong Zheng and Ming Zhou", "abstract": "  In e-commerce and digital marketing, generating high-fidelity human-product\ndemonstration videos is important for effective product presentation. However,\nmost existing frameworks either fail to preserve the identities of both humans\nand products or lack an understanding of human-product spatial relationships,\nleading to unrealistic representations and unnatural interactions. To address\nthese challenges, we propose a Diffusion Transformer (DiT)-based framework. Our\nmethod simultaneously preserves human identities and product-specific details,\nsuch as logos and textures, by injecting paired human-product reference\ninformation and utilizing an additional masked cross-attention mechanism. We\nemploy a 3D body mesh template and product bounding boxes to provide precise\nmotion guidance, enabling intuitive alignment of hand gestures with product\nplacements. Additionally, structured text encoding is used to incorporate\ncategory-level semantics, enhancing 3D consistency during small rotational\nchanges across frames. Trained on a hybrid dataset with extensive data\naugmentation strategies, our approach outperforms state-of-the-art techniques\nin maintaining the identity integrity of both humans and products and\ngenerating realistic demonstration motions. Project page:\nhttps://submit2025-dream.github.io/DreamActor-H1/.\n", "link": "http://arxiv.org/abs/2506.10568v1", "date": "2025-06-12", "relevancy": 2.7449, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7261}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6684}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamActor-H1%3A%20High-Fidelity%20Human-Product%20Demonstration%20Video%0A%20%20Generation%20via%20Motion-designed%20Diffusion%20Transformers&body=Title%3A%20DreamActor-H1%3A%20High-Fidelity%20Human-Product%20Demonstration%20Video%0A%20%20Generation%20via%20Motion-designed%20Diffusion%20Transformers%0AAuthor%3A%20Lizhen%20Wang%20and%20Zhurong%20Xia%20and%20Tianshu%20Hu%20and%20Pengrui%20Wang%20and%20Pengfei%20Wang%20and%20Zerong%20Zheng%20and%20Ming%20Zhou%0AAbstract%3A%20%20%20In%20e-commerce%20and%20digital%20marketing%2C%20generating%20high-fidelity%20human-product%0Ademonstration%20videos%20is%20important%20for%20effective%20product%20presentation.%20However%2C%0Amost%20existing%20frameworks%20either%20fail%20to%20preserve%20the%20identities%20of%20both%20humans%0Aand%20products%20or%20lack%20an%20understanding%20of%20human-product%20spatial%20relationships%2C%0Aleading%20to%20unrealistic%20representations%20and%20unnatural%20interactions.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20Diffusion%20Transformer%20%28DiT%29-based%20framework.%20Our%0Amethod%20simultaneously%20preserves%20human%20identities%20and%20product-specific%20details%2C%0Asuch%20as%20logos%20and%20textures%2C%20by%20injecting%20paired%20human-product%20reference%0Ainformation%20and%20utilizing%20an%20additional%20masked%20cross-attention%20mechanism.%20We%0Aemploy%20a%203D%20body%20mesh%20template%20and%20product%20bounding%20boxes%20to%20provide%20precise%0Amotion%20guidance%2C%20enabling%20intuitive%20alignment%20of%20hand%20gestures%20with%20product%0Aplacements.%20Additionally%2C%20structured%20text%20encoding%20is%20used%20to%20incorporate%0Acategory-level%20semantics%2C%20enhancing%203D%20consistency%20during%20small%20rotational%0Achanges%20across%20frames.%20Trained%20on%20a%20hybrid%20dataset%20with%20extensive%20data%0Aaugmentation%20strategies%2C%20our%20approach%20outperforms%20state-of-the-art%20techniques%0Ain%20maintaining%20the%20identity%20integrity%20of%20both%20humans%20and%20products%20and%0Agenerating%20realistic%20demonstration%20motions.%20Project%20page%3A%0Ahttps%3A//submit2025-dream.github.io/DreamActor-H1/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamActor-H1%253A%2520High-Fidelity%2520Human-Product%2520Demonstration%2520Video%250A%2520%2520Generation%2520via%2520Motion-designed%2520Diffusion%2520Transformers%26entry.906535625%3DLizhen%2520Wang%2520and%2520Zhurong%2520Xia%2520and%2520Tianshu%2520Hu%2520and%2520Pengrui%2520Wang%2520and%2520Pengfei%2520Wang%2520and%2520Zerong%2520Zheng%2520and%2520Ming%2520Zhou%26entry.1292438233%3D%2520%2520In%2520e-commerce%2520and%2520digital%2520marketing%252C%2520generating%2520high-fidelity%2520human-product%250Ademonstration%2520videos%2520is%2520important%2520for%2520effective%2520product%2520presentation.%2520However%252C%250Amost%2520existing%2520frameworks%2520either%2520fail%2520to%2520preserve%2520the%2520identities%2520of%2520both%2520humans%250Aand%2520products%2520or%2520lack%2520an%2520understanding%2520of%2520human-product%2520spatial%2520relationships%252C%250Aleading%2520to%2520unrealistic%2520representations%2520and%2520unnatural%2520interactions.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520Diffusion%2520Transformer%2520%2528DiT%2529-based%2520framework.%2520Our%250Amethod%2520simultaneously%2520preserves%2520human%2520identities%2520and%2520product-specific%2520details%252C%250Asuch%2520as%2520logos%2520and%2520textures%252C%2520by%2520injecting%2520paired%2520human-product%2520reference%250Ainformation%2520and%2520utilizing%2520an%2520additional%2520masked%2520cross-attention%2520mechanism.%2520We%250Aemploy%2520a%25203D%2520body%2520mesh%2520template%2520and%2520product%2520bounding%2520boxes%2520to%2520provide%2520precise%250Amotion%2520guidance%252C%2520enabling%2520intuitive%2520alignment%2520of%2520hand%2520gestures%2520with%2520product%250Aplacements.%2520Additionally%252C%2520structured%2520text%2520encoding%2520is%2520used%2520to%2520incorporate%250Acategory-level%2520semantics%252C%2520enhancing%25203D%2520consistency%2520during%2520small%2520rotational%250Achanges%2520across%2520frames.%2520Trained%2520on%2520a%2520hybrid%2520dataset%2520with%2520extensive%2520data%250Aaugmentation%2520strategies%252C%2520our%2520approach%2520outperforms%2520state-of-the-art%2520techniques%250Ain%2520maintaining%2520the%2520identity%2520integrity%2520of%2520both%2520humans%2520and%2520products%2520and%250Agenerating%2520realistic%2520demonstration%2520motions.%2520Project%2520page%253A%250Ahttps%253A//submit2025-dream.github.io/DreamActor-H1/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamActor-H1%3A%20High-Fidelity%20Human-Product%20Demonstration%20Video%0A%20%20Generation%20via%20Motion-designed%20Diffusion%20Transformers&entry.906535625=Lizhen%20Wang%20and%20Zhurong%20Xia%20and%20Tianshu%20Hu%20and%20Pengrui%20Wang%20and%20Pengfei%20Wang%20and%20Zerong%20Zheng%20and%20Ming%20Zhou&entry.1292438233=%20%20In%20e-commerce%20and%20digital%20marketing%2C%20generating%20high-fidelity%20human-product%0Ademonstration%20videos%20is%20important%20for%20effective%20product%20presentation.%20However%2C%0Amost%20existing%20frameworks%20either%20fail%20to%20preserve%20the%20identities%20of%20both%20humans%0Aand%20products%20or%20lack%20an%20understanding%20of%20human-product%20spatial%20relationships%2C%0Aleading%20to%20unrealistic%20representations%20and%20unnatural%20interactions.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20Diffusion%20Transformer%20%28DiT%29-based%20framework.%20Our%0Amethod%20simultaneously%20preserves%20human%20identities%20and%20product-specific%20details%2C%0Asuch%20as%20logos%20and%20textures%2C%20by%20injecting%20paired%20human-product%20reference%0Ainformation%20and%20utilizing%20an%20additional%20masked%20cross-attention%20mechanism.%20We%0Aemploy%20a%203D%20body%20mesh%20template%20and%20product%20bounding%20boxes%20to%20provide%20precise%0Amotion%20guidance%2C%20enabling%20intuitive%20alignment%20of%20hand%20gestures%20with%20product%0Aplacements.%20Additionally%2C%20structured%20text%20encoding%20is%20used%20to%20incorporate%0Acategory-level%20semantics%2C%20enhancing%203D%20consistency%20during%20small%20rotational%0Achanges%20across%20frames.%20Trained%20on%20a%20hybrid%20dataset%20with%20extensive%20data%0Aaugmentation%20strategies%2C%20our%20approach%20outperforms%20state-of-the-art%20techniques%0Ain%20maintaining%20the%20identity%20integrity%20of%20both%20humans%20and%20products%20and%0Agenerating%20realistic%20demonstration%20motions.%20Project%20page%3A%0Ahttps%3A//submit2025-dream.github.io/DreamActor-H1/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10568v1&entry.124074799=Read"},
{"title": "Execution Guided Line-by-Line Code Generation", "author": "Boaz Lavon and Shahar Katz and Lior Wolf", "abstract": "  We present a novel approach to neural code generation that incorporates\nreal-time execution signals into the language model generation process. While\nlarge language models (LLMs) have demonstrated impressive code generation\ncapabilities, they typically do not utilize execution feedback during\ninference, a critical signal that human programmers regularly leverage. Our\nmethod, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically\nincorporates execution signals as the model generates code, providing\nline-by-line feedback that guides the generation process toward executable\nsolutions. EG-CFG employs a multi-stage process: first, we conduct beam search\nto sample candidate program completions for each line; second, we extract\nexecution signals by executing these candidates against test cases; and\nfinally, we incorporate these signals into the prompt during generation. By\nmaintaining consistent signals across tokens within the same line and\nrefreshing signals at line boundaries, our approach provides coherent guidance\nwhile preserving syntactic structure. Moreover, the method naturally supports\nnative parallelism at the task level in which multiple agents operate in\nparallel, exploring diverse reasoning paths and collectively generating a broad\nset of candidate solutions. Our experiments across diverse coding tasks\ndemonstrate that EG-CFG significantly improves code generation performance\ncompared to standard approaches, achieving state-of-the-art results across\nvarious levels of complexity, from foundational problems to challenging\ncompetitive programming tasks. Our code is available at:\nhttps://github.com/boazlavon/eg_cfg\n", "link": "http://arxiv.org/abs/2506.10948v1", "date": "2025-06-12", "relevancy": 2.683, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5474}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5412}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Execution%20Guided%20Line-by-Line%20Code%20Generation&body=Title%3A%20Execution%20Guided%20Line-by-Line%20Code%20Generation%0AAuthor%3A%20Boaz%20Lavon%20and%20Shahar%20Katz%20and%20Lior%20Wolf%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20neural%20code%20generation%20that%20incorporates%0Areal-time%20execution%20signals%20into%20the%20language%20model%20generation%20process.%20While%0Alarge%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20code%20generation%0Acapabilities%2C%20they%20typically%20do%20not%20utilize%20execution%20feedback%20during%0Ainference%2C%20a%20critical%20signal%20that%20human%20programmers%20regularly%20leverage.%20Our%0Amethod%2C%20Execution-Guided%20Classifier-Free%20Guidance%20%28EG-CFG%29%2C%20dynamically%0Aincorporates%20execution%20signals%20as%20the%20model%20generates%20code%2C%20providing%0Aline-by-line%20feedback%20that%20guides%20the%20generation%20process%20toward%20executable%0Asolutions.%20EG-CFG%20employs%20a%20multi-stage%20process%3A%20first%2C%20we%20conduct%20beam%20search%0Ato%20sample%20candidate%20program%20completions%20for%20each%20line%3B%20second%2C%20we%20extract%0Aexecution%20signals%20by%20executing%20these%20candidates%20against%20test%20cases%3B%20and%0Afinally%2C%20we%20incorporate%20these%20signals%20into%20the%20prompt%20during%20generation.%20By%0Amaintaining%20consistent%20signals%20across%20tokens%20within%20the%20same%20line%20and%0Arefreshing%20signals%20at%20line%20boundaries%2C%20our%20approach%20provides%20coherent%20guidance%0Awhile%20preserving%20syntactic%20structure.%20Moreover%2C%20the%20method%20naturally%20supports%0Anative%20parallelism%20at%20the%20task%20level%20in%20which%20multiple%20agents%20operate%20in%0Aparallel%2C%20exploring%20diverse%20reasoning%20paths%20and%20collectively%20generating%20a%20broad%0Aset%20of%20candidate%20solutions.%20Our%20experiments%20across%20diverse%20coding%20tasks%0Ademonstrate%20that%20EG-CFG%20significantly%20improves%20code%20generation%20performance%0Acompared%20to%20standard%20approaches%2C%20achieving%20state-of-the-art%20results%20across%0Avarious%20levels%20of%20complexity%2C%20from%20foundational%20problems%20to%20challenging%0Acompetitive%20programming%20tasks.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/boazlavon/eg_cfg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExecution%2520Guided%2520Line-by-Line%2520Code%2520Generation%26entry.906535625%3DBoaz%2520Lavon%2520and%2520Shahar%2520Katz%2520and%2520Lior%2520Wolf%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520neural%2520code%2520generation%2520that%2520incorporates%250Areal-time%2520execution%2520signals%2520into%2520the%2520language%2520model%2520generation%2520process.%2520While%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520code%2520generation%250Acapabilities%252C%2520they%2520typically%2520do%2520not%2520utilize%2520execution%2520feedback%2520during%250Ainference%252C%2520a%2520critical%2520signal%2520that%2520human%2520programmers%2520regularly%2520leverage.%2520Our%250Amethod%252C%2520Execution-Guided%2520Classifier-Free%2520Guidance%2520%2528EG-CFG%2529%252C%2520dynamically%250Aincorporates%2520execution%2520signals%2520as%2520the%2520model%2520generates%2520code%252C%2520providing%250Aline-by-line%2520feedback%2520that%2520guides%2520the%2520generation%2520process%2520toward%2520executable%250Asolutions.%2520EG-CFG%2520employs%2520a%2520multi-stage%2520process%253A%2520first%252C%2520we%2520conduct%2520beam%2520search%250Ato%2520sample%2520candidate%2520program%2520completions%2520for%2520each%2520line%253B%2520second%252C%2520we%2520extract%250Aexecution%2520signals%2520by%2520executing%2520these%2520candidates%2520against%2520test%2520cases%253B%2520and%250Afinally%252C%2520we%2520incorporate%2520these%2520signals%2520into%2520the%2520prompt%2520during%2520generation.%2520By%250Amaintaining%2520consistent%2520signals%2520across%2520tokens%2520within%2520the%2520same%2520line%2520and%250Arefreshing%2520signals%2520at%2520line%2520boundaries%252C%2520our%2520approach%2520provides%2520coherent%2520guidance%250Awhile%2520preserving%2520syntactic%2520structure.%2520Moreover%252C%2520the%2520method%2520naturally%2520supports%250Anative%2520parallelism%2520at%2520the%2520task%2520level%2520in%2520which%2520multiple%2520agents%2520operate%2520in%250Aparallel%252C%2520exploring%2520diverse%2520reasoning%2520paths%2520and%2520collectively%2520generating%2520a%2520broad%250Aset%2520of%2520candidate%2520solutions.%2520Our%2520experiments%2520across%2520diverse%2520coding%2520tasks%250Ademonstrate%2520that%2520EG-CFG%2520significantly%2520improves%2520code%2520generation%2520performance%250Acompared%2520to%2520standard%2520approaches%252C%2520achieving%2520state-of-the-art%2520results%2520across%250Avarious%2520levels%2520of%2520complexity%252C%2520from%2520foundational%2520problems%2520to%2520challenging%250Acompetitive%2520programming%2520tasks.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/boazlavon/eg_cfg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Execution%20Guided%20Line-by-Line%20Code%20Generation&entry.906535625=Boaz%20Lavon%20and%20Shahar%20Katz%20and%20Lior%20Wolf&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20neural%20code%20generation%20that%20incorporates%0Areal-time%20execution%20signals%20into%20the%20language%20model%20generation%20process.%20While%0Alarge%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20code%20generation%0Acapabilities%2C%20they%20typically%20do%20not%20utilize%20execution%20feedback%20during%0Ainference%2C%20a%20critical%20signal%20that%20human%20programmers%20regularly%20leverage.%20Our%0Amethod%2C%20Execution-Guided%20Classifier-Free%20Guidance%20%28EG-CFG%29%2C%20dynamically%0Aincorporates%20execution%20signals%20as%20the%20model%20generates%20code%2C%20providing%0Aline-by-line%20feedback%20that%20guides%20the%20generation%20process%20toward%20executable%0Asolutions.%20EG-CFG%20employs%20a%20multi-stage%20process%3A%20first%2C%20we%20conduct%20beam%20search%0Ato%20sample%20candidate%20program%20completions%20for%20each%20line%3B%20second%2C%20we%20extract%0Aexecution%20signals%20by%20executing%20these%20candidates%20against%20test%20cases%3B%20and%0Afinally%2C%20we%20incorporate%20these%20signals%20into%20the%20prompt%20during%20generation.%20By%0Amaintaining%20consistent%20signals%20across%20tokens%20within%20the%20same%20line%20and%0Arefreshing%20signals%20at%20line%20boundaries%2C%20our%20approach%20provides%20coherent%20guidance%0Awhile%20preserving%20syntactic%20structure.%20Moreover%2C%20the%20method%20naturally%20supports%0Anative%20parallelism%20at%20the%20task%20level%20in%20which%20multiple%20agents%20operate%20in%0Aparallel%2C%20exploring%20diverse%20reasoning%20paths%20and%20collectively%20generating%20a%20broad%0Aset%20of%20candidate%20solutions.%20Our%20experiments%20across%20diverse%20coding%20tasks%0Ademonstrate%20that%20EG-CFG%20significantly%20improves%20code%20generation%20performance%0Acompared%20to%20standard%20approaches%2C%20achieving%20state-of-the-art%20results%20across%0Avarious%20levels%20of%20complexity%2C%20from%20foundational%20problems%20to%20challenging%0Acompetitive%20programming%20tasks.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/boazlavon/eg_cfg%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10948v1&entry.124074799=Read"},
{"title": "MSTAR: Box-free Multi-query Scene Text Retrieval with Attention\n  Recycling", "author": "Liang Yin and Xudong Xie and Zhang Li and Xiang Bai and Yuliang Liu", "abstract": "  Scene text retrieval has made significant progress with the assistance of\naccurate text localization. However, existing approaches typically require\ncostly bounding box annotations for training. Besides, they mostly adopt a\ncustomized retrieval strategy but struggle to unify various types of queries to\nmeet diverse retrieval needs. To address these issues, we introduce Muti-query\nScene Text retrieval with Attention Recycling (MSTAR), a box-free approach for\nscene text retrieval. It incorporates progressive vision embedding to\ndynamically capture the multi-grained representation of texts and harmonizes\nfree-style text queries with style-aware instructions. Additionally, a\nmulti-instance matching module is integrated to enhance vision-language\nalignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset,\nthe first benchmark designed to evaluate the multi-query scene text retrieval\ncapability of models, comprising four query types and 16k images. Extensive\nexperiments demonstrate the superiority of our method across seven public\ndatasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous\nstate-of-the-art model by 6.4% in MAP on Total-Text while eliminating box\nannotation costs. Moreover, on the MQTR benchmark, MSTAR significantly\noutperforms the previous models by an average of 8.5%. The code and datasets\nare available at https://github.com/yingift/MSTAR.\n", "link": "http://arxiv.org/abs/2506.10609v1", "date": "2025-06-12", "relevancy": 2.6823, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSTAR%3A%20Box-free%20Multi-query%20Scene%20Text%20Retrieval%20with%20Attention%0A%20%20Recycling&body=Title%3A%20MSTAR%3A%20Box-free%20Multi-query%20Scene%20Text%20Retrieval%20with%20Attention%0A%20%20Recycling%0AAuthor%3A%20Liang%20Yin%20and%20Xudong%20Xie%20and%20Zhang%20Li%20and%20Xiang%20Bai%20and%20Yuliang%20Liu%0AAbstract%3A%20%20%20Scene%20text%20retrieval%20has%20made%20significant%20progress%20with%20the%20assistance%20of%0Aaccurate%20text%20localization.%20However%2C%20existing%20approaches%20typically%20require%0Acostly%20bounding%20box%20annotations%20for%20training.%20Besides%2C%20they%20mostly%20adopt%20a%0Acustomized%20retrieval%20strategy%20but%20struggle%20to%20unify%20various%20types%20of%20queries%20to%0Ameet%20diverse%20retrieval%20needs.%20To%20address%20these%20issues%2C%20we%20introduce%20Muti-query%0AScene%20Text%20retrieval%20with%20Attention%20Recycling%20%28MSTAR%29%2C%20a%20box-free%20approach%20for%0Ascene%20text%20retrieval.%20It%20incorporates%20progressive%20vision%20embedding%20to%0Adynamically%20capture%20the%20multi-grained%20representation%20of%20texts%20and%20harmonizes%0Afree-style%20text%20queries%20with%20style-aware%20instructions.%20Additionally%2C%20a%0Amulti-instance%20matching%20module%20is%20integrated%20to%20enhance%20vision-language%0Aalignment.%20Furthermore%2C%20we%20build%20the%20Multi-Query%20Text%20Retrieval%20%28MQTR%29%20dataset%2C%0Athe%20first%20benchmark%20designed%20to%20evaluate%20the%20multi-query%20scene%20text%20retrieval%0Acapability%20of%20models%2C%20comprising%20four%20query%20types%20and%2016k%20images.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20across%20seven%20public%0Adatasets%20and%20the%20MQTR%20dataset.%20Notably%2C%20MSTAR%20marginally%20surpasses%20the%20previous%0Astate-of-the-art%20model%20by%206.4%25%20in%20MAP%20on%20Total-Text%20while%20eliminating%20box%0Aannotation%20costs.%20Moreover%2C%20on%20the%20MQTR%20benchmark%2C%20MSTAR%20significantly%0Aoutperforms%20the%20previous%20models%20by%20an%20average%20of%208.5%25.%20The%20code%20and%20datasets%0Aare%20available%20at%20https%3A//github.com/yingift/MSTAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSTAR%253A%2520Box-free%2520Multi-query%2520Scene%2520Text%2520Retrieval%2520with%2520Attention%250A%2520%2520Recycling%26entry.906535625%3DLiang%2520Yin%2520and%2520Xudong%2520Xie%2520and%2520Zhang%2520Li%2520and%2520Xiang%2520Bai%2520and%2520Yuliang%2520Liu%26entry.1292438233%3D%2520%2520Scene%2520text%2520retrieval%2520has%2520made%2520significant%2520progress%2520with%2520the%2520assistance%2520of%250Aaccurate%2520text%2520localization.%2520However%252C%2520existing%2520approaches%2520typically%2520require%250Acostly%2520bounding%2520box%2520annotations%2520for%2520training.%2520Besides%252C%2520they%2520mostly%2520adopt%2520a%250Acustomized%2520retrieval%2520strategy%2520but%2520struggle%2520to%2520unify%2520various%2520types%2520of%2520queries%2520to%250Ameet%2520diverse%2520retrieval%2520needs.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Muti-query%250AScene%2520Text%2520retrieval%2520with%2520Attention%2520Recycling%2520%2528MSTAR%2529%252C%2520a%2520box-free%2520approach%2520for%250Ascene%2520text%2520retrieval.%2520It%2520incorporates%2520progressive%2520vision%2520embedding%2520to%250Adynamically%2520capture%2520the%2520multi-grained%2520representation%2520of%2520texts%2520and%2520harmonizes%250Afree-style%2520text%2520queries%2520with%2520style-aware%2520instructions.%2520Additionally%252C%2520a%250Amulti-instance%2520matching%2520module%2520is%2520integrated%2520to%2520enhance%2520vision-language%250Aalignment.%2520Furthermore%252C%2520we%2520build%2520the%2520Multi-Query%2520Text%2520Retrieval%2520%2528MQTR%2529%2520dataset%252C%250Athe%2520first%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520multi-query%2520scene%2520text%2520retrieval%250Acapability%2520of%2520models%252C%2520comprising%2520four%2520query%2520types%2520and%252016k%2520images.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520across%2520seven%2520public%250Adatasets%2520and%2520the%2520MQTR%2520dataset.%2520Notably%252C%2520MSTAR%2520marginally%2520surpasses%2520the%2520previous%250Astate-of-the-art%2520model%2520by%25206.4%2525%2520in%2520MAP%2520on%2520Total-Text%2520while%2520eliminating%2520box%250Aannotation%2520costs.%2520Moreover%252C%2520on%2520the%2520MQTR%2520benchmark%252C%2520MSTAR%2520significantly%250Aoutperforms%2520the%2520previous%2520models%2520by%2520an%2520average%2520of%25208.5%2525.%2520The%2520code%2520and%2520datasets%250Aare%2520available%2520at%2520https%253A//github.com/yingift/MSTAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSTAR%3A%20Box-free%20Multi-query%20Scene%20Text%20Retrieval%20with%20Attention%0A%20%20Recycling&entry.906535625=Liang%20Yin%20and%20Xudong%20Xie%20and%20Zhang%20Li%20and%20Xiang%20Bai%20and%20Yuliang%20Liu&entry.1292438233=%20%20Scene%20text%20retrieval%20has%20made%20significant%20progress%20with%20the%20assistance%20of%0Aaccurate%20text%20localization.%20However%2C%20existing%20approaches%20typically%20require%0Acostly%20bounding%20box%20annotations%20for%20training.%20Besides%2C%20they%20mostly%20adopt%20a%0Acustomized%20retrieval%20strategy%20but%20struggle%20to%20unify%20various%20types%20of%20queries%20to%0Ameet%20diverse%20retrieval%20needs.%20To%20address%20these%20issues%2C%20we%20introduce%20Muti-query%0AScene%20Text%20retrieval%20with%20Attention%20Recycling%20%28MSTAR%29%2C%20a%20box-free%20approach%20for%0Ascene%20text%20retrieval.%20It%20incorporates%20progressive%20vision%20embedding%20to%0Adynamically%20capture%20the%20multi-grained%20representation%20of%20texts%20and%20harmonizes%0Afree-style%20text%20queries%20with%20style-aware%20instructions.%20Additionally%2C%20a%0Amulti-instance%20matching%20module%20is%20integrated%20to%20enhance%20vision-language%0Aalignment.%20Furthermore%2C%20we%20build%20the%20Multi-Query%20Text%20Retrieval%20%28MQTR%29%20dataset%2C%0Athe%20first%20benchmark%20designed%20to%20evaluate%20the%20multi-query%20scene%20text%20retrieval%0Acapability%20of%20models%2C%20comprising%20four%20query%20types%20and%2016k%20images.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20across%20seven%20public%0Adatasets%20and%20the%20MQTR%20dataset.%20Notably%2C%20MSTAR%20marginally%20surpasses%20the%20previous%0Astate-of-the-art%20model%20by%206.4%25%20in%20MAP%20on%20Total-Text%20while%20eliminating%20box%0Aannotation%20costs.%20Moreover%2C%20on%20the%20MQTR%20benchmark%2C%20MSTAR%20significantly%0Aoutperforms%20the%20previous%20models%20by%20an%20average%20of%208.5%25.%20The%20code%20and%20datasets%0Aare%20available%20at%20https%3A//github.com/yingift/MSTAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10609v1&entry.124074799=Read"},
{"title": "Understanding In-Context Learning on Structured Manifolds: Bridging\n  Attention to Kernel Methods", "author": "Zhaiming Shen and Alexander Hsu and Rongjie Lai and Wenjing Liao", "abstract": "  While in-context learning (ICL) has achieved remarkable success in natural\nlanguage and vision domains, its theoretical understanding--particularly in the\ncontext of structured geometric data--remains unexplored. In this work, we\ninitiate a theoretical study of ICL for regression of H\\\"older functions on\nmanifolds. By establishing a novel connection between the attention mechanism\nand classical kernel methods, we derive generalization error bounds in terms of\nthe prompt length and the number of training tasks. When a sufficient number of\ntraining tasks are observed, transformers give rise to the minimax regression\nrate of H\\\"older functions on manifolds, which scales exponentially with the\nintrinsic dimension of the manifold, rather than the ambient space dimension.\nOur result also characterizes how the generalization error scales with the\nnumber of training tasks, shedding light on the complexity of transformers as\nin-context algorithm learners. Our findings provide foundational insights into\nthe role of geometry in ICL and novels tools to study ICL of nonlinear models.\n", "link": "http://arxiv.org/abs/2506.10959v1", "date": "2025-06-12", "relevancy": 2.6336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5384}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods&body=Title%3A%20Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods%0AAuthor%3A%20Zhaiming%20Shen%20and%20Alexander%20Hsu%20and%20Rongjie%20Lai%20and%20Wenjing%20Liao%0AAbstract%3A%20%20%20While%20in-context%20learning%20%28ICL%29%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20and%20vision%20domains%2C%20its%20theoretical%20understanding--particularly%20in%20the%0Acontext%20of%20structured%20geometric%20data--remains%20unexplored.%20In%20this%20work%2C%20we%0Ainitiate%20a%20theoretical%20study%20of%20ICL%20for%20regression%20of%20H%5C%22older%20functions%20on%0Amanifolds.%20By%20establishing%20a%20novel%20connection%20between%20the%20attention%20mechanism%0Aand%20classical%20kernel%20methods%2C%20we%20derive%20generalization%20error%20bounds%20in%20terms%20of%0Athe%20prompt%20length%20and%20the%20number%20of%20training%20tasks.%20When%20a%20sufficient%20number%20of%0Atraining%20tasks%20are%20observed%2C%20transformers%20give%20rise%20to%20the%20minimax%20regression%0Arate%20of%20H%5C%22older%20functions%20on%20manifolds%2C%20which%20scales%20exponentially%20with%20the%0Aintrinsic%20dimension%20of%20the%20manifold%2C%20rather%20than%20the%20ambient%20space%20dimension.%0AOur%20result%20also%20characterizes%20how%20the%20generalization%20error%20scales%20with%20the%0Anumber%20of%20training%20tasks%2C%20shedding%20light%20on%20the%20complexity%20of%20transformers%20as%0Ain-context%20algorithm%20learners.%20Our%20findings%20provide%20foundational%20insights%20into%0Athe%20role%20of%20geometry%20in%20ICL%20and%20novels%20tools%20to%20study%20ICL%20of%20nonlinear%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520In-Context%2520Learning%2520on%2520Structured%2520Manifolds%253A%2520Bridging%250A%2520%2520Attention%2520to%2520Kernel%2520Methods%26entry.906535625%3DZhaiming%2520Shen%2520and%2520Alexander%2520Hsu%2520and%2520Rongjie%2520Lai%2520and%2520Wenjing%2520Liao%26entry.1292438233%3D%2520%2520While%2520in-context%2520learning%2520%2528ICL%2529%2520has%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520and%2520vision%2520domains%252C%2520its%2520theoretical%2520understanding--particularly%2520in%2520the%250Acontext%2520of%2520structured%2520geometric%2520data--remains%2520unexplored.%2520In%2520this%2520work%252C%2520we%250Ainitiate%2520a%2520theoretical%2520study%2520of%2520ICL%2520for%2520regression%2520of%2520H%255C%2522older%2520functions%2520on%250Amanifolds.%2520By%2520establishing%2520a%2520novel%2520connection%2520between%2520the%2520attention%2520mechanism%250Aand%2520classical%2520kernel%2520methods%252C%2520we%2520derive%2520generalization%2520error%2520bounds%2520in%2520terms%2520of%250Athe%2520prompt%2520length%2520and%2520the%2520number%2520of%2520training%2520tasks.%2520When%2520a%2520sufficient%2520number%2520of%250Atraining%2520tasks%2520are%2520observed%252C%2520transformers%2520give%2520rise%2520to%2520the%2520minimax%2520regression%250Arate%2520of%2520H%255C%2522older%2520functions%2520on%2520manifolds%252C%2520which%2520scales%2520exponentially%2520with%2520the%250Aintrinsic%2520dimension%2520of%2520the%2520manifold%252C%2520rather%2520than%2520the%2520ambient%2520space%2520dimension.%250AOur%2520result%2520also%2520characterizes%2520how%2520the%2520generalization%2520error%2520scales%2520with%2520the%250Anumber%2520of%2520training%2520tasks%252C%2520shedding%2520light%2520on%2520the%2520complexity%2520of%2520transformers%2520as%250Ain-context%2520algorithm%2520learners.%2520Our%2520findings%2520provide%2520foundational%2520insights%2520into%250Athe%2520role%2520of%2520geometry%2520in%2520ICL%2520and%2520novels%2520tools%2520to%2520study%2520ICL%2520of%2520nonlinear%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods&entry.906535625=Zhaiming%20Shen%20and%20Alexander%20Hsu%20and%20Rongjie%20Lai%20and%20Wenjing%20Liao&entry.1292438233=%20%20While%20in-context%20learning%20%28ICL%29%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20and%20vision%20domains%2C%20its%20theoretical%20understanding--particularly%20in%20the%0Acontext%20of%20structured%20geometric%20data--remains%20unexplored.%20In%20this%20work%2C%20we%0Ainitiate%20a%20theoretical%20study%20of%20ICL%20for%20regression%20of%20H%5C%22older%20functions%20on%0Amanifolds.%20By%20establishing%20a%20novel%20connection%20between%20the%20attention%20mechanism%0Aand%20classical%20kernel%20methods%2C%20we%20derive%20generalization%20error%20bounds%20in%20terms%20of%0Athe%20prompt%20length%20and%20the%20number%20of%20training%20tasks.%20When%20a%20sufficient%20number%20of%0Atraining%20tasks%20are%20observed%2C%20transformers%20give%20rise%20to%20the%20minimax%20regression%0Arate%20of%20H%5C%22older%20functions%20on%20manifolds%2C%20which%20scales%20exponentially%20with%20the%0Aintrinsic%20dimension%20of%20the%20manifold%2C%20rather%20than%20the%20ambient%20space%20dimension.%0AOur%20result%20also%20characterizes%20how%20the%20generalization%20error%20scales%20with%20the%0Anumber%20of%20training%20tasks%2C%20shedding%20light%20on%20the%20complexity%20of%20transformers%20as%0Ain-context%20algorithm%20learners.%20Our%20findings%20provide%20foundational%20insights%20into%0Athe%20role%20of%20geometry%20in%20ICL%20and%20novels%20tools%20to%20study%20ICL%20of%20nonlinear%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10959v1&entry.124074799=Read"},
{"title": "PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image\n  Analysis", "author": "Marzieh Oghbaie and Teresa Ara\u00fajoa and Hrvoje Bogunovi\u0107", "abstract": "  Background and Objective: Prototype-based methods improve interpretability by\nlearning fine-grained part-prototypes; however, their visualization in the\ninput pixel space is not always consistent with human-understandable\nbiomarkers. In addition, well-known prototype-based approaches typically learn\nextremely granular prototypes that are less interpretable in medical imaging,\nwhere both the presence and extent of biomarkers and lesions are critical.\n  Methods: To address these challenges, we propose PiPViT (Patch-based Visual\nInterpretable Prototypes), an inherently interpretable prototypical model for\nimage recognition. Leveraging a vision transformer (ViT), PiPViT captures\nlong-range dependencies among patches to learn robust, human-interpretable\nprototypes that approximate lesion extent only using image-level labels.\nAdditionally, PiPViT benefits from contrastive learning and multi-resolution\ninput processing, which enables effective localization of biomarkers across\nscales.\n  Results: We evaluated PiPViT on retinal OCT image classification across four\ndatasets, where it achieved competitive quantitative performance compared to\nstate-of-the-art methods while delivering more meaningful explanations.\nMoreover, quantitative evaluation on a hold-out test set confirms that the\nlearned prototypes are semantically and clinically relevant. We believe PiPViT\ncan transparently explain its decisions and assist clinicians in understanding\ndiagnostic outcomes. Github page: https://github.com/marziehoghbaie/PiPViT\n", "link": "http://arxiv.org/abs/2506.10669v1", "date": "2025-06-12", "relevancy": 2.6239, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5377}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5211}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PiPViT%3A%20Patch-based%20Visual%20Interpretable%20Prototypes%20for%20Retinal%20Image%0A%20%20Analysis&body=Title%3A%20PiPViT%3A%20Patch-based%20Visual%20Interpretable%20Prototypes%20for%20Retinal%20Image%0A%20%20Analysis%0AAuthor%3A%20Marzieh%20Oghbaie%20and%20Teresa%20Ara%C3%BAjoa%20and%20Hrvoje%20Bogunovi%C4%87%0AAbstract%3A%20%20%20Background%20and%20Objective%3A%20Prototype-based%20methods%20improve%20interpretability%20by%0Alearning%20fine-grained%20part-prototypes%3B%20however%2C%20their%20visualization%20in%20the%0Ainput%20pixel%20space%20is%20not%20always%20consistent%20with%20human-understandable%0Abiomarkers.%20In%20addition%2C%20well-known%20prototype-based%20approaches%20typically%20learn%0Aextremely%20granular%20prototypes%20that%20are%20less%20interpretable%20in%20medical%20imaging%2C%0Awhere%20both%20the%20presence%20and%20extent%20of%20biomarkers%20and%20lesions%20are%20critical.%0A%20%20Methods%3A%20To%20address%20these%20challenges%2C%20we%20propose%20PiPViT%20%28Patch-based%20Visual%0AInterpretable%20Prototypes%29%2C%20an%20inherently%20interpretable%20prototypical%20model%20for%0Aimage%20recognition.%20Leveraging%20a%20vision%20transformer%20%28ViT%29%2C%20PiPViT%20captures%0Along-range%20dependencies%20among%20patches%20to%20learn%20robust%2C%20human-interpretable%0Aprototypes%20that%20approximate%20lesion%20extent%20only%20using%20image-level%20labels.%0AAdditionally%2C%20PiPViT%20benefits%20from%20contrastive%20learning%20and%20multi-resolution%0Ainput%20processing%2C%20which%20enables%20effective%20localization%20of%20biomarkers%20across%0Ascales.%0A%20%20Results%3A%20We%20evaluated%20PiPViT%20on%20retinal%20OCT%20image%20classification%20across%20four%0Adatasets%2C%20where%20it%20achieved%20competitive%20quantitative%20performance%20compared%20to%0Astate-of-the-art%20methods%20while%20delivering%20more%20meaningful%20explanations.%0AMoreover%2C%20quantitative%20evaluation%20on%20a%20hold-out%20test%20set%20confirms%20that%20the%0Alearned%20prototypes%20are%20semantically%20and%20clinically%20relevant.%20We%20believe%20PiPViT%0Acan%20transparently%20explain%20its%20decisions%20and%20assist%20clinicians%20in%20understanding%0Adiagnostic%20outcomes.%20Github%20page%3A%20https%3A//github.com/marziehoghbaie/PiPViT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiPViT%253A%2520Patch-based%2520Visual%2520Interpretable%2520Prototypes%2520for%2520Retinal%2520Image%250A%2520%2520Analysis%26entry.906535625%3DMarzieh%2520Oghbaie%2520and%2520Teresa%2520Ara%25C3%25BAjoa%2520and%2520Hrvoje%2520Bogunovi%25C4%2587%26entry.1292438233%3D%2520%2520Background%2520and%2520Objective%253A%2520Prototype-based%2520methods%2520improve%2520interpretability%2520by%250Alearning%2520fine-grained%2520part-prototypes%253B%2520however%252C%2520their%2520visualization%2520in%2520the%250Ainput%2520pixel%2520space%2520is%2520not%2520always%2520consistent%2520with%2520human-understandable%250Abiomarkers.%2520In%2520addition%252C%2520well-known%2520prototype-based%2520approaches%2520typically%2520learn%250Aextremely%2520granular%2520prototypes%2520that%2520are%2520less%2520interpretable%2520in%2520medical%2520imaging%252C%250Awhere%2520both%2520the%2520presence%2520and%2520extent%2520of%2520biomarkers%2520and%2520lesions%2520are%2520critical.%250A%2520%2520Methods%253A%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520PiPViT%2520%2528Patch-based%2520Visual%250AInterpretable%2520Prototypes%2529%252C%2520an%2520inherently%2520interpretable%2520prototypical%2520model%2520for%250Aimage%2520recognition.%2520Leveraging%2520a%2520vision%2520transformer%2520%2528ViT%2529%252C%2520PiPViT%2520captures%250Along-range%2520dependencies%2520among%2520patches%2520to%2520learn%2520robust%252C%2520human-interpretable%250Aprototypes%2520that%2520approximate%2520lesion%2520extent%2520only%2520using%2520image-level%2520labels.%250AAdditionally%252C%2520PiPViT%2520benefits%2520from%2520contrastive%2520learning%2520and%2520multi-resolution%250Ainput%2520processing%252C%2520which%2520enables%2520effective%2520localization%2520of%2520biomarkers%2520across%250Ascales.%250A%2520%2520Results%253A%2520We%2520evaluated%2520PiPViT%2520on%2520retinal%2520OCT%2520image%2520classification%2520across%2520four%250Adatasets%252C%2520where%2520it%2520achieved%2520competitive%2520quantitative%2520performance%2520compared%2520to%250Astate-of-the-art%2520methods%2520while%2520delivering%2520more%2520meaningful%2520explanations.%250AMoreover%252C%2520quantitative%2520evaluation%2520on%2520a%2520hold-out%2520test%2520set%2520confirms%2520that%2520the%250Alearned%2520prototypes%2520are%2520semantically%2520and%2520clinically%2520relevant.%2520We%2520believe%2520PiPViT%250Acan%2520transparently%2520explain%2520its%2520decisions%2520and%2520assist%2520clinicians%2520in%2520understanding%250Adiagnostic%2520outcomes.%2520Github%2520page%253A%2520https%253A//github.com/marziehoghbaie/PiPViT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PiPViT%3A%20Patch-based%20Visual%20Interpretable%20Prototypes%20for%20Retinal%20Image%0A%20%20Analysis&entry.906535625=Marzieh%20Oghbaie%20and%20Teresa%20Ara%C3%BAjoa%20and%20Hrvoje%20Bogunovi%C4%87&entry.1292438233=%20%20Background%20and%20Objective%3A%20Prototype-based%20methods%20improve%20interpretability%20by%0Alearning%20fine-grained%20part-prototypes%3B%20however%2C%20their%20visualization%20in%20the%0Ainput%20pixel%20space%20is%20not%20always%20consistent%20with%20human-understandable%0Abiomarkers.%20In%20addition%2C%20well-known%20prototype-based%20approaches%20typically%20learn%0Aextremely%20granular%20prototypes%20that%20are%20less%20interpretable%20in%20medical%20imaging%2C%0Awhere%20both%20the%20presence%20and%20extent%20of%20biomarkers%20and%20lesions%20are%20critical.%0A%20%20Methods%3A%20To%20address%20these%20challenges%2C%20we%20propose%20PiPViT%20%28Patch-based%20Visual%0AInterpretable%20Prototypes%29%2C%20an%20inherently%20interpretable%20prototypical%20model%20for%0Aimage%20recognition.%20Leveraging%20a%20vision%20transformer%20%28ViT%29%2C%20PiPViT%20captures%0Along-range%20dependencies%20among%20patches%20to%20learn%20robust%2C%20human-interpretable%0Aprototypes%20that%20approximate%20lesion%20extent%20only%20using%20image-level%20labels.%0AAdditionally%2C%20PiPViT%20benefits%20from%20contrastive%20learning%20and%20multi-resolution%0Ainput%20processing%2C%20which%20enables%20effective%20localization%20of%20biomarkers%20across%0Ascales.%0A%20%20Results%3A%20We%20evaluated%20PiPViT%20on%20retinal%20OCT%20image%20classification%20across%20four%0Adatasets%2C%20where%20it%20achieved%20competitive%20quantitative%20performance%20compared%20to%0Astate-of-the-art%20methods%20while%20delivering%20more%20meaningful%20explanations.%0AMoreover%2C%20quantitative%20evaluation%20on%20a%20hold-out%20test%20set%20confirms%20that%20the%0Alearned%20prototypes%20are%20semantically%20and%20clinically%20relevant.%20We%20believe%20PiPViT%0Acan%20transparently%20explain%20its%20decisions%20and%20assist%20clinicians%20in%20understanding%0Adiagnostic%20outcomes.%20Github%20page%3A%20https%3A//github.com/marziehoghbaie/PiPViT%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10669v1&entry.124074799=Read"},
{"title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4\n  GPU-Hours Fine-Tuning", "author": "Xiaoyi Bao and Jindi Lv and Xiaofeng Wang and Zheng Zhu and Xinze Chen and YuKun Zhou and Jiancheng Lv and Xingang Wang and Guan Huang", "abstract": "  Recent progress in diffusion models has greatly enhanced video generation\nquality, yet these models still require fine-tuning to improve specific\ndimensions like instance preservation, motion rationality, composition, and\nphysical plausibility. Existing fine-tuning approaches often rely on human\nannotations and large-scale computational resources, limiting their\npracticality. In this work, we propose GigaVideo-1, an efficient fine-tuning\nframework that advances video generation without additional human supervision.\nRather than injecting large volumes of high-quality data from external sources,\nGigaVideo-1 unlocks the latent potential of pre-trained video diffusion models\nthrough automatic feedback. Specifically, we focus on two key aspects of the\nfine-tuning process: data and optimization. To improve fine-tuning data, we\ndesign a prompt-driven data engine that constructs diverse, weakness-oriented\ntraining samples. On the optimization side, we introduce a reward-guided\ntraining strategy, which adaptively weights samples using feedback from\npre-trained vision-language models with a realism constraint. We evaluate\nGigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17\nevaluation dimensions. Experiments show that GigaVideo-1 consistently improves\nperformance on almost all the dimensions with an average gain of about 4% using\nonly 4 GPU-hours. Requiring no manual annotations and minimal real data,\nGigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and\ndata will be publicly available.\n", "link": "http://arxiv.org/abs/2506.10639v1", "date": "2025-06-12", "relevancy": 2.6174, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6638}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6529}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GigaVideo-1%3A%20Advancing%20Video%20Generation%20via%20Automatic%20Feedback%20with%204%0A%20%20GPU-Hours%20Fine-Tuning&body=Title%3A%20GigaVideo-1%3A%20Advancing%20Video%20Generation%20via%20Automatic%20Feedback%20with%204%0A%20%20GPU-Hours%20Fine-Tuning%0AAuthor%3A%20Xiaoyi%20Bao%20and%20Jindi%20Lv%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Xinze%20Chen%20and%20YuKun%20Zhou%20and%20Jiancheng%20Lv%20and%20Xingang%20Wang%20and%20Guan%20Huang%0AAbstract%3A%20%20%20Recent%20progress%20in%20diffusion%20models%20has%20greatly%20enhanced%20video%20generation%0Aquality%2C%20yet%20these%20models%20still%20require%20fine-tuning%20to%20improve%20specific%0Adimensions%20like%20instance%20preservation%2C%20motion%20rationality%2C%20composition%2C%20and%0Aphysical%20plausibility.%20Existing%20fine-tuning%20approaches%20often%20rely%20on%20human%0Aannotations%20and%20large-scale%20computational%20resources%2C%20limiting%20their%0Apracticality.%20In%20this%20work%2C%20we%20propose%20GigaVideo-1%2C%20an%20efficient%20fine-tuning%0Aframework%20that%20advances%20video%20generation%20without%20additional%20human%20supervision.%0ARather%20than%20injecting%20large%20volumes%20of%20high-quality%20data%20from%20external%20sources%2C%0AGigaVideo-1%20unlocks%20the%20latent%20potential%20of%20pre-trained%20video%20diffusion%20models%0Athrough%20automatic%20feedback.%20Specifically%2C%20we%20focus%20on%20two%20key%20aspects%20of%20the%0Afine-tuning%20process%3A%20data%20and%20optimization.%20To%20improve%20fine-tuning%20data%2C%20we%0Adesign%20a%20prompt-driven%20data%20engine%20that%20constructs%20diverse%2C%20weakness-oriented%0Atraining%20samples.%20On%20the%20optimization%20side%2C%20we%20introduce%20a%20reward-guided%0Atraining%20strategy%2C%20which%20adaptively%20weights%20samples%20using%20feedback%20from%0Apre-trained%20vision-language%20models%20with%20a%20realism%20constraint.%20We%20evaluate%0AGigaVideo-1%20on%20the%20VBench-2.0%20benchmark%20using%20Wan2.1%20as%20the%20baseline%20across%2017%0Aevaluation%20dimensions.%20Experiments%20show%20that%20GigaVideo-1%20consistently%20improves%0Aperformance%20on%20almost%20all%20the%20dimensions%20with%20an%20average%20gain%20of%20about%204%25%20using%0Aonly%204%20GPU-hours.%20Requiring%20no%20manual%20annotations%20and%20minimal%20real%20data%2C%0AGigaVideo-1%20demonstrates%20both%20effectiveness%20and%20efficiency.%20Code%2C%20model%2C%20and%0Adata%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGigaVideo-1%253A%2520Advancing%2520Video%2520Generation%2520via%2520Automatic%2520Feedback%2520with%25204%250A%2520%2520GPU-Hours%2520Fine-Tuning%26entry.906535625%3DXiaoyi%2520Bao%2520and%2520Jindi%2520Lv%2520and%2520Xiaofeng%2520Wang%2520and%2520Zheng%2520Zhu%2520and%2520Xinze%2520Chen%2520and%2520YuKun%2520Zhou%2520and%2520Jiancheng%2520Lv%2520and%2520Xingang%2520Wang%2520and%2520Guan%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520diffusion%2520models%2520has%2520greatly%2520enhanced%2520video%2520generation%250Aquality%252C%2520yet%2520these%2520models%2520still%2520require%2520fine-tuning%2520to%2520improve%2520specific%250Adimensions%2520like%2520instance%2520preservation%252C%2520motion%2520rationality%252C%2520composition%252C%2520and%250Aphysical%2520plausibility.%2520Existing%2520fine-tuning%2520approaches%2520often%2520rely%2520on%2520human%250Aannotations%2520and%2520large-scale%2520computational%2520resources%252C%2520limiting%2520their%250Apracticality.%2520In%2520this%2520work%252C%2520we%2520propose%2520GigaVideo-1%252C%2520an%2520efficient%2520fine-tuning%250Aframework%2520that%2520advances%2520video%2520generation%2520without%2520additional%2520human%2520supervision.%250ARather%2520than%2520injecting%2520large%2520volumes%2520of%2520high-quality%2520data%2520from%2520external%2520sources%252C%250AGigaVideo-1%2520unlocks%2520the%2520latent%2520potential%2520of%2520pre-trained%2520video%2520diffusion%2520models%250Athrough%2520automatic%2520feedback.%2520Specifically%252C%2520we%2520focus%2520on%2520two%2520key%2520aspects%2520of%2520the%250Afine-tuning%2520process%253A%2520data%2520and%2520optimization.%2520To%2520improve%2520fine-tuning%2520data%252C%2520we%250Adesign%2520a%2520prompt-driven%2520data%2520engine%2520that%2520constructs%2520diverse%252C%2520weakness-oriented%250Atraining%2520samples.%2520On%2520the%2520optimization%2520side%252C%2520we%2520introduce%2520a%2520reward-guided%250Atraining%2520strategy%252C%2520which%2520adaptively%2520weights%2520samples%2520using%2520feedback%2520from%250Apre-trained%2520vision-language%2520models%2520with%2520a%2520realism%2520constraint.%2520We%2520evaluate%250AGigaVideo-1%2520on%2520the%2520VBench-2.0%2520benchmark%2520using%2520Wan2.1%2520as%2520the%2520baseline%2520across%252017%250Aevaluation%2520dimensions.%2520Experiments%2520show%2520that%2520GigaVideo-1%2520consistently%2520improves%250Aperformance%2520on%2520almost%2520all%2520the%2520dimensions%2520with%2520an%2520average%2520gain%2520of%2520about%25204%2525%2520using%250Aonly%25204%2520GPU-hours.%2520Requiring%2520no%2520manual%2520annotations%2520and%2520minimal%2520real%2520data%252C%250AGigaVideo-1%2520demonstrates%2520both%2520effectiveness%2520and%2520efficiency.%2520Code%252C%2520model%252C%2520and%250Adata%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GigaVideo-1%3A%20Advancing%20Video%20Generation%20via%20Automatic%20Feedback%20with%204%0A%20%20GPU-Hours%20Fine-Tuning&entry.906535625=Xiaoyi%20Bao%20and%20Jindi%20Lv%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Xinze%20Chen%20and%20YuKun%20Zhou%20and%20Jiancheng%20Lv%20and%20Xingang%20Wang%20and%20Guan%20Huang&entry.1292438233=%20%20Recent%20progress%20in%20diffusion%20models%20has%20greatly%20enhanced%20video%20generation%0Aquality%2C%20yet%20these%20models%20still%20require%20fine-tuning%20to%20improve%20specific%0Adimensions%20like%20instance%20preservation%2C%20motion%20rationality%2C%20composition%2C%20and%0Aphysical%20plausibility.%20Existing%20fine-tuning%20approaches%20often%20rely%20on%20human%0Aannotations%20and%20large-scale%20computational%20resources%2C%20limiting%20their%0Apracticality.%20In%20this%20work%2C%20we%20propose%20GigaVideo-1%2C%20an%20efficient%20fine-tuning%0Aframework%20that%20advances%20video%20generation%20without%20additional%20human%20supervision.%0ARather%20than%20injecting%20large%20volumes%20of%20high-quality%20data%20from%20external%20sources%2C%0AGigaVideo-1%20unlocks%20the%20latent%20potential%20of%20pre-trained%20video%20diffusion%20models%0Athrough%20automatic%20feedback.%20Specifically%2C%20we%20focus%20on%20two%20key%20aspects%20of%20the%0Afine-tuning%20process%3A%20data%20and%20optimization.%20To%20improve%20fine-tuning%20data%2C%20we%0Adesign%20a%20prompt-driven%20data%20engine%20that%20constructs%20diverse%2C%20weakness-oriented%0Atraining%20samples.%20On%20the%20optimization%20side%2C%20we%20introduce%20a%20reward-guided%0Atraining%20strategy%2C%20which%20adaptively%20weights%20samples%20using%20feedback%20from%0Apre-trained%20vision-language%20models%20with%20a%20realism%20constraint.%20We%20evaluate%0AGigaVideo-1%20on%20the%20VBench-2.0%20benchmark%20using%20Wan2.1%20as%20the%20baseline%20across%2017%0Aevaluation%20dimensions.%20Experiments%20show%20that%20GigaVideo-1%20consistently%20improves%0Aperformance%20on%20almost%20all%20the%20dimensions%20with%20an%20average%20gain%20of%20about%204%25%20using%0Aonly%204%20GPU-hours.%20Requiring%20no%20manual%20annotations%20and%20minimal%20real%20data%2C%0AGigaVideo-1%20demonstrates%20both%20effectiveness%20and%20efficiency.%20Code%2C%20model%2C%20and%0Adata%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10639v1&entry.124074799=Read"},
{"title": "Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations", "author": "Xili Yi and Nima Fazeli", "abstract": "  We introduce Vib2Move, a novel approach for in-hand object reconfiguration\nthat uses fingertip micro-vibrations and gravity to precisely reposition planar\nobjects. Our framework comprises three key innovations. First, we design a\nvibration-based actuator that dynamically modulates the effective finger-object\nfriction coefficient, effectively emulating changes in gripping force. Second,\nwe derive a sliding motion model for objects clamped in a parallel gripper with\ntwo symmetric, variable-friction contact patches. Third, we propose a motion\nplanner that coordinates end-effector finger trajectories and fingertip\nvibrations to achieve the desired object pose. In real-world trials, Vib2Move\nconsistently yields final positioning errors below 6 mm, demonstrating\nreliable, high-precision manipulation across a variety of planar objects. For\nmore results and information, please visit https://vib2move.github.io.\n", "link": "http://arxiv.org/abs/2506.10923v1", "date": "2025-06-12", "relevancy": 2.5908, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5225}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5179}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vib2Move%3A%20In-Hand%20Object%20Reconfiguration%20via%20Fingertip%20Micro-Vibrations&body=Title%3A%20Vib2Move%3A%20In-Hand%20Object%20Reconfiguration%20via%20Fingertip%20Micro-Vibrations%0AAuthor%3A%20Xili%20Yi%20and%20Nima%20Fazeli%0AAbstract%3A%20%20%20We%20introduce%20Vib2Move%2C%20a%20novel%20approach%20for%20in-hand%20object%20reconfiguration%0Athat%20uses%20fingertip%20micro-vibrations%20and%20gravity%20to%20precisely%20reposition%20planar%0Aobjects.%20Our%20framework%20comprises%20three%20key%20innovations.%20First%2C%20we%20design%20a%0Avibration-based%20actuator%20that%20dynamically%20modulates%20the%20effective%20finger-object%0Afriction%20coefficient%2C%20effectively%20emulating%20changes%20in%20gripping%20force.%20Second%2C%0Awe%20derive%20a%20sliding%20motion%20model%20for%20objects%20clamped%20in%20a%20parallel%20gripper%20with%0Atwo%20symmetric%2C%20variable-friction%20contact%20patches.%20Third%2C%20we%20propose%20a%20motion%0Aplanner%20that%20coordinates%20end-effector%20finger%20trajectories%20and%20fingertip%0Avibrations%20to%20achieve%20the%20desired%20object%20pose.%20In%20real-world%20trials%2C%20Vib2Move%0Aconsistently%20yields%20final%20positioning%20errors%20below%206%20mm%2C%20demonstrating%0Areliable%2C%20high-precision%20manipulation%20across%20a%20variety%20of%20planar%20objects.%20For%0Amore%20results%20and%20information%2C%20please%20visit%20https%3A//vib2move.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVib2Move%253A%2520In-Hand%2520Object%2520Reconfiguration%2520via%2520Fingertip%2520Micro-Vibrations%26entry.906535625%3DXili%2520Yi%2520and%2520Nima%2520Fazeli%26entry.1292438233%3D%2520%2520We%2520introduce%2520Vib2Move%252C%2520a%2520novel%2520approach%2520for%2520in-hand%2520object%2520reconfiguration%250Athat%2520uses%2520fingertip%2520micro-vibrations%2520and%2520gravity%2520to%2520precisely%2520reposition%2520planar%250Aobjects.%2520Our%2520framework%2520comprises%2520three%2520key%2520innovations.%2520First%252C%2520we%2520design%2520a%250Avibration-based%2520actuator%2520that%2520dynamically%2520modulates%2520the%2520effective%2520finger-object%250Afriction%2520coefficient%252C%2520effectively%2520emulating%2520changes%2520in%2520gripping%2520force.%2520Second%252C%250Awe%2520derive%2520a%2520sliding%2520motion%2520model%2520for%2520objects%2520clamped%2520in%2520a%2520parallel%2520gripper%2520with%250Atwo%2520symmetric%252C%2520variable-friction%2520contact%2520patches.%2520Third%252C%2520we%2520propose%2520a%2520motion%250Aplanner%2520that%2520coordinates%2520end-effector%2520finger%2520trajectories%2520and%2520fingertip%250Avibrations%2520to%2520achieve%2520the%2520desired%2520object%2520pose.%2520In%2520real-world%2520trials%252C%2520Vib2Move%250Aconsistently%2520yields%2520final%2520positioning%2520errors%2520below%25206%2520mm%252C%2520demonstrating%250Areliable%252C%2520high-precision%2520manipulation%2520across%2520a%2520variety%2520of%2520planar%2520objects.%2520For%250Amore%2520results%2520and%2520information%252C%2520please%2520visit%2520https%253A//vib2move.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vib2Move%3A%20In-Hand%20Object%20Reconfiguration%20via%20Fingertip%20Micro-Vibrations&entry.906535625=Xili%20Yi%20and%20Nima%20Fazeli&entry.1292438233=%20%20We%20introduce%20Vib2Move%2C%20a%20novel%20approach%20for%20in-hand%20object%20reconfiguration%0Athat%20uses%20fingertip%20micro-vibrations%20and%20gravity%20to%20precisely%20reposition%20planar%0Aobjects.%20Our%20framework%20comprises%20three%20key%20innovations.%20First%2C%20we%20design%20a%0Avibration-based%20actuator%20that%20dynamically%20modulates%20the%20effective%20finger-object%0Afriction%20coefficient%2C%20effectively%20emulating%20changes%20in%20gripping%20force.%20Second%2C%0Awe%20derive%20a%20sliding%20motion%20model%20for%20objects%20clamped%20in%20a%20parallel%20gripper%20with%0Atwo%20symmetric%2C%20variable-friction%20contact%20patches.%20Third%2C%20we%20propose%20a%20motion%0Aplanner%20that%20coordinates%20end-effector%20finger%20trajectories%20and%20fingertip%0Avibrations%20to%20achieve%20the%20desired%20object%20pose.%20In%20real-world%20trials%2C%20Vib2Move%0Aconsistently%20yields%20final%20positioning%20errors%20below%206%20mm%2C%20demonstrating%0Areliable%2C%20high-precision%20manipulation%20across%20a%20variety%20of%20planar%20objects.%20For%0Amore%20results%20and%20information%2C%20please%20visit%20https%3A//vib2move.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10923v1&entry.124074799=Read"},
{"title": "Continual Hyperbolic Learning of Instances and Classes", "author": "Melika Ayoughi and Mina Ghadimi Atigh and Mohammad Mahdi Derakhshani and Cees G. M. Snoek and Pascal Mettes and Paul Groth", "abstract": "  Continual learning has traditionally focused on classifying either instances\nor classes, but real-world applications, such as robotics and self-driving\ncars, require models to handle both simultaneously. To mirror real-life\nscenarios, we introduce the task of continual learning of instances and\nclasses, at the same time. This task challenges models to adapt to multiple\nlevels of granularity over time, which requires balancing fine-grained instance\nrecognition with coarse-grained class generalization. In this paper, we\nidentify that classes and instances naturally form a hierarchical structure. To\nmodel these hierarchical relationships, we propose HyperCLIC, a continual\nlearning algorithm that leverages hyperbolic space, which is uniquely suited\nfor hierarchical data due to its ability to represent tree-like structures with\nlow distortion and compact embeddings. Our framework incorporates hyperbolic\nclassification and distillation objectives, enabling the continual embedding of\nhierarchical relations. To evaluate performance across multiple granularities,\nwe introduce continual hierarchical metrics. We validate our approach on\nEgoObjects, the only dataset that captures the complexity of hierarchical\nobject recognition in dynamic real-world environments. Empirical results show\nthat HyperCLIC operates effectively at multiple granularities with improved\nhierarchical generalization.\n", "link": "http://arxiv.org/abs/2506.10710v1", "date": "2025-06-12", "relevancy": 2.5884, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5284}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5238}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Hyperbolic%20Learning%20of%20Instances%20and%20Classes&body=Title%3A%20Continual%20Hyperbolic%20Learning%20of%20Instances%20and%20Classes%0AAuthor%3A%20Melika%20Ayoughi%20and%20Mina%20Ghadimi%20Atigh%20and%20Mohammad%20Mahdi%20Derakhshani%20and%20Cees%20G.%20M.%20Snoek%20and%20Pascal%20Mettes%20and%20Paul%20Groth%0AAbstract%3A%20%20%20Continual%20learning%20has%20traditionally%20focused%20on%20classifying%20either%20instances%0Aor%20classes%2C%20but%20real-world%20applications%2C%20such%20as%20robotics%20and%20self-driving%0Acars%2C%20require%20models%20to%20handle%20both%20simultaneously.%20To%20mirror%20real-life%0Ascenarios%2C%20we%20introduce%20the%20task%20of%20continual%20learning%20of%20instances%20and%0Aclasses%2C%20at%20the%20same%20time.%20This%20task%20challenges%20models%20to%20adapt%20to%20multiple%0Alevels%20of%20granularity%20over%20time%2C%20which%20requires%20balancing%20fine-grained%20instance%0Arecognition%20with%20coarse-grained%20class%20generalization.%20In%20this%20paper%2C%20we%0Aidentify%20that%20classes%20and%20instances%20naturally%20form%20a%20hierarchical%20structure.%20To%0Amodel%20these%20hierarchical%20relationships%2C%20we%20propose%20HyperCLIC%2C%20a%20continual%0Alearning%20algorithm%20that%20leverages%20hyperbolic%20space%2C%20which%20is%20uniquely%20suited%0Afor%20hierarchical%20data%20due%20to%20its%20ability%20to%20represent%20tree-like%20structures%20with%0Alow%20distortion%20and%20compact%20embeddings.%20Our%20framework%20incorporates%20hyperbolic%0Aclassification%20and%20distillation%20objectives%2C%20enabling%20the%20continual%20embedding%20of%0Ahierarchical%20relations.%20To%20evaluate%20performance%20across%20multiple%20granularities%2C%0Awe%20introduce%20continual%20hierarchical%20metrics.%20We%20validate%20our%20approach%20on%0AEgoObjects%2C%20the%20only%20dataset%20that%20captures%20the%20complexity%20of%20hierarchical%0Aobject%20recognition%20in%20dynamic%20real-world%20environments.%20Empirical%20results%20show%0Athat%20HyperCLIC%20operates%20effectively%20at%20multiple%20granularities%20with%20improved%0Ahierarchical%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Hyperbolic%2520Learning%2520of%2520Instances%2520and%2520Classes%26entry.906535625%3DMelika%2520Ayoughi%2520and%2520Mina%2520Ghadimi%2520Atigh%2520and%2520Mohammad%2520Mahdi%2520Derakhshani%2520and%2520Cees%2520G.%2520M.%2520Snoek%2520and%2520Pascal%2520Mettes%2520and%2520Paul%2520Groth%26entry.1292438233%3D%2520%2520Continual%2520learning%2520has%2520traditionally%2520focused%2520on%2520classifying%2520either%2520instances%250Aor%2520classes%252C%2520but%2520real-world%2520applications%252C%2520such%2520as%2520robotics%2520and%2520self-driving%250Acars%252C%2520require%2520models%2520to%2520handle%2520both%2520simultaneously.%2520To%2520mirror%2520real-life%250Ascenarios%252C%2520we%2520introduce%2520the%2520task%2520of%2520continual%2520learning%2520of%2520instances%2520and%250Aclasses%252C%2520at%2520the%2520same%2520time.%2520This%2520task%2520challenges%2520models%2520to%2520adapt%2520to%2520multiple%250Alevels%2520of%2520granularity%2520over%2520time%252C%2520which%2520requires%2520balancing%2520fine-grained%2520instance%250Arecognition%2520with%2520coarse-grained%2520class%2520generalization.%2520In%2520this%2520paper%252C%2520we%250Aidentify%2520that%2520classes%2520and%2520instances%2520naturally%2520form%2520a%2520hierarchical%2520structure.%2520To%250Amodel%2520these%2520hierarchical%2520relationships%252C%2520we%2520propose%2520HyperCLIC%252C%2520a%2520continual%250Alearning%2520algorithm%2520that%2520leverages%2520hyperbolic%2520space%252C%2520which%2520is%2520uniquely%2520suited%250Afor%2520hierarchical%2520data%2520due%2520to%2520its%2520ability%2520to%2520represent%2520tree-like%2520structures%2520with%250Alow%2520distortion%2520and%2520compact%2520embeddings.%2520Our%2520framework%2520incorporates%2520hyperbolic%250Aclassification%2520and%2520distillation%2520objectives%252C%2520enabling%2520the%2520continual%2520embedding%2520of%250Ahierarchical%2520relations.%2520To%2520evaluate%2520performance%2520across%2520multiple%2520granularities%252C%250Awe%2520introduce%2520continual%2520hierarchical%2520metrics.%2520We%2520validate%2520our%2520approach%2520on%250AEgoObjects%252C%2520the%2520only%2520dataset%2520that%2520captures%2520the%2520complexity%2520of%2520hierarchical%250Aobject%2520recognition%2520in%2520dynamic%2520real-world%2520environments.%2520Empirical%2520results%2520show%250Athat%2520HyperCLIC%2520operates%2520effectively%2520at%2520multiple%2520granularities%2520with%2520improved%250Ahierarchical%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Hyperbolic%20Learning%20of%20Instances%20and%20Classes&entry.906535625=Melika%20Ayoughi%20and%20Mina%20Ghadimi%20Atigh%20and%20Mohammad%20Mahdi%20Derakhshani%20and%20Cees%20G.%20M.%20Snoek%20and%20Pascal%20Mettes%20and%20Paul%20Groth&entry.1292438233=%20%20Continual%20learning%20has%20traditionally%20focused%20on%20classifying%20either%20instances%0Aor%20classes%2C%20but%20real-world%20applications%2C%20such%20as%20robotics%20and%20self-driving%0Acars%2C%20require%20models%20to%20handle%20both%20simultaneously.%20To%20mirror%20real-life%0Ascenarios%2C%20we%20introduce%20the%20task%20of%20continual%20learning%20of%20instances%20and%0Aclasses%2C%20at%20the%20same%20time.%20This%20task%20challenges%20models%20to%20adapt%20to%20multiple%0Alevels%20of%20granularity%20over%20time%2C%20which%20requires%20balancing%20fine-grained%20instance%0Arecognition%20with%20coarse-grained%20class%20generalization.%20In%20this%20paper%2C%20we%0Aidentify%20that%20classes%20and%20instances%20naturally%20form%20a%20hierarchical%20structure.%20To%0Amodel%20these%20hierarchical%20relationships%2C%20we%20propose%20HyperCLIC%2C%20a%20continual%0Alearning%20algorithm%20that%20leverages%20hyperbolic%20space%2C%20which%20is%20uniquely%20suited%0Afor%20hierarchical%20data%20due%20to%20its%20ability%20to%20represent%20tree-like%20structures%20with%0Alow%20distortion%20and%20compact%20embeddings.%20Our%20framework%20incorporates%20hyperbolic%0Aclassification%20and%20distillation%20objectives%2C%20enabling%20the%20continual%20embedding%20of%0Ahierarchical%20relations.%20To%20evaluate%20performance%20across%20multiple%20granularities%2C%0Awe%20introduce%20continual%20hierarchical%20metrics.%20We%20validate%20our%20approach%20on%0AEgoObjects%2C%20the%20only%20dataset%20that%20captures%20the%20complexity%20of%20hierarchical%0Aobject%20recognition%20in%20dynamic%20real-world%20environments.%20Empirical%20results%20show%0Athat%20HyperCLIC%20operates%20effectively%20at%20multiple%20granularities%20with%20improved%0Ahierarchical%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10710v1&entry.124074799=Read"},
{"title": "Semi-Automated Quality Assurance in Digital Pathology: Tile\n  Classification Approach", "author": "Meredith VandeHaar and M. Clinch and I. Yilmaz and M. A. Rahman and Y. Xiao and F. Dogany and H. M. Alazab and A. Nassar and Z. Akkus and B. Dangott", "abstract": "  Quality assurance is a critical but underexplored area in digital pathology,\nwhere even minor artifacts can have significant effects. Artifacts have been\nshown to negatively impact the performance of AI diagnostic models. In current\npractice, trained staff manually review digitized images prior to release of\nthese slides to pathologists which are then used to render a diagnosis.\nConventional image processing approaches, provide a foundation for detecting\nartifacts on digital pathology slides. However, current tools do not leverage\ndeep learning, which has the potential to improve detection accuracy and\nscalability. Despite these advancements, methods for quality assurance in\ndigital pathology remain limited, presenting a gap for innovation.\n  We propose an AI algorithm designed to screen digital pathology slides by\nanalyzing tiles and categorizing them into one of 10 predefined artifact types\nor as background. This algorithm identifies and localizes artifacts, creating a\nmap that highlights regions of interest. By directing human operators to\nspecific tiles affected by artifacts, the algorithm minimizes the time and\neffort required to manually review entire slides for quality issues.\n  From internal archives and The Cancer Genome Atlas, 133 whole slide images\nwere selected and 10 artifacts were annotated using an internally developed\nsoftware ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple\nmodels at different tile sizes and magnification was performed. InceptionResNet\nwas selected. Single artifact models were trained and tested, followed by a\nlimited multiple instance model with artifacts that performed well together\n(chatter, fold, and pen). From the results of this study we suggest a hybrid\ndesign for artifact screening composed of both single artifact binary models as\nwell as multiple instance models to optimize detection of each artifact.\n", "link": "http://arxiv.org/abs/2506.10916v1", "date": "2025-06-12", "relevancy": 2.5504, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5395}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4992}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Automated%20Quality%20Assurance%20in%20Digital%20Pathology%3A%20Tile%0A%20%20Classification%20Approach&body=Title%3A%20Semi-Automated%20Quality%20Assurance%20in%20Digital%20Pathology%3A%20Tile%0A%20%20Classification%20Approach%0AAuthor%3A%20Meredith%20VandeHaar%20and%20M.%20Clinch%20and%20I.%20Yilmaz%20and%20M.%20A.%20Rahman%20and%20Y.%20Xiao%20and%20F.%20Dogany%20and%20H.%20M.%20Alazab%20and%20A.%20Nassar%20and%20Z.%20Akkus%20and%20B.%20Dangott%0AAbstract%3A%20%20%20Quality%20assurance%20is%20a%20critical%20but%20underexplored%20area%20in%20digital%20pathology%2C%0Awhere%20even%20minor%20artifacts%20can%20have%20significant%20effects.%20Artifacts%20have%20been%0Ashown%20to%20negatively%20impact%20the%20performance%20of%20AI%20diagnostic%20models.%20In%20current%0Apractice%2C%20trained%20staff%20manually%20review%20digitized%20images%20prior%20to%20release%20of%0Athese%20slides%20to%20pathologists%20which%20are%20then%20used%20to%20render%20a%20diagnosis.%0AConventional%20image%20processing%20approaches%2C%20provide%20a%20foundation%20for%20detecting%0Aartifacts%20on%20digital%20pathology%20slides.%20However%2C%20current%20tools%20do%20not%20leverage%0Adeep%20learning%2C%20which%20has%20the%20potential%20to%20improve%20detection%20accuracy%20and%0Ascalability.%20Despite%20these%20advancements%2C%20methods%20for%20quality%20assurance%20in%0Adigital%20pathology%20remain%20limited%2C%20presenting%20a%20gap%20for%20innovation.%0A%20%20We%20propose%20an%20AI%20algorithm%20designed%20to%20screen%20digital%20pathology%20slides%20by%0Aanalyzing%20tiles%20and%20categorizing%20them%20into%20one%20of%2010%20predefined%20artifact%20types%0Aor%20as%20background.%20This%20algorithm%20identifies%20and%20localizes%20artifacts%2C%20creating%20a%0Amap%20that%20highlights%20regions%20of%20interest.%20By%20directing%20human%20operators%20to%0Aspecific%20tiles%20affected%20by%20artifacts%2C%20the%20algorithm%20minimizes%20the%20time%20and%0Aeffort%20required%20to%20manually%20review%20entire%20slides%20for%20quality%20issues.%0A%20%20From%20internal%20archives%20and%20The%20Cancer%20Genome%20Atlas%2C%20133%20whole%20slide%20images%0Awere%20selected%20and%2010%20artifacts%20were%20annotated%20using%20an%20internally%20developed%0Asoftware%20ZAPP%20%28Mayo%20Clinic%2C%20Jacksonville%2C%20FL%29.%20Ablation%20study%20of%20multiple%0Amodels%20at%20different%20tile%20sizes%20and%20magnification%20was%20performed.%20InceptionResNet%0Awas%20selected.%20Single%20artifact%20models%20were%20trained%20and%20tested%2C%20followed%20by%20a%0Alimited%20multiple%20instance%20model%20with%20artifacts%20that%20performed%20well%20together%0A%28chatter%2C%20fold%2C%20and%20pen%29.%20From%20the%20results%20of%20this%20study%20we%20suggest%20a%20hybrid%0Adesign%20for%20artifact%20screening%20composed%20of%20both%20single%20artifact%20binary%20models%20as%0Awell%20as%20multiple%20instance%20models%20to%20optimize%20detection%20of%20each%20artifact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Automated%2520Quality%2520Assurance%2520in%2520Digital%2520Pathology%253A%2520Tile%250A%2520%2520Classification%2520Approach%26entry.906535625%3DMeredith%2520VandeHaar%2520and%2520M.%2520Clinch%2520and%2520I.%2520Yilmaz%2520and%2520M.%2520A.%2520Rahman%2520and%2520Y.%2520Xiao%2520and%2520F.%2520Dogany%2520and%2520H.%2520M.%2520Alazab%2520and%2520A.%2520Nassar%2520and%2520Z.%2520Akkus%2520and%2520B.%2520Dangott%26entry.1292438233%3D%2520%2520Quality%2520assurance%2520is%2520a%2520critical%2520but%2520underexplored%2520area%2520in%2520digital%2520pathology%252C%250Awhere%2520even%2520minor%2520artifacts%2520can%2520have%2520significant%2520effects.%2520Artifacts%2520have%2520been%250Ashown%2520to%2520negatively%2520impact%2520the%2520performance%2520of%2520AI%2520diagnostic%2520models.%2520In%2520current%250Apractice%252C%2520trained%2520staff%2520manually%2520review%2520digitized%2520images%2520prior%2520to%2520release%2520of%250Athese%2520slides%2520to%2520pathologists%2520which%2520are%2520then%2520used%2520to%2520render%2520a%2520diagnosis.%250AConventional%2520image%2520processing%2520approaches%252C%2520provide%2520a%2520foundation%2520for%2520detecting%250Aartifacts%2520on%2520digital%2520pathology%2520slides.%2520However%252C%2520current%2520tools%2520do%2520not%2520leverage%250Adeep%2520learning%252C%2520which%2520has%2520the%2520potential%2520to%2520improve%2520detection%2520accuracy%2520and%250Ascalability.%2520Despite%2520these%2520advancements%252C%2520methods%2520for%2520quality%2520assurance%2520in%250Adigital%2520pathology%2520remain%2520limited%252C%2520presenting%2520a%2520gap%2520for%2520innovation.%250A%2520%2520We%2520propose%2520an%2520AI%2520algorithm%2520designed%2520to%2520screen%2520digital%2520pathology%2520slides%2520by%250Aanalyzing%2520tiles%2520and%2520categorizing%2520them%2520into%2520one%2520of%252010%2520predefined%2520artifact%2520types%250Aor%2520as%2520background.%2520This%2520algorithm%2520identifies%2520and%2520localizes%2520artifacts%252C%2520creating%2520a%250Amap%2520that%2520highlights%2520regions%2520of%2520interest.%2520By%2520directing%2520human%2520operators%2520to%250Aspecific%2520tiles%2520affected%2520by%2520artifacts%252C%2520the%2520algorithm%2520minimizes%2520the%2520time%2520and%250Aeffort%2520required%2520to%2520manually%2520review%2520entire%2520slides%2520for%2520quality%2520issues.%250A%2520%2520From%2520internal%2520archives%2520and%2520The%2520Cancer%2520Genome%2520Atlas%252C%2520133%2520whole%2520slide%2520images%250Awere%2520selected%2520and%252010%2520artifacts%2520were%2520annotated%2520using%2520an%2520internally%2520developed%250Asoftware%2520ZAPP%2520%2528Mayo%2520Clinic%252C%2520Jacksonville%252C%2520FL%2529.%2520Ablation%2520study%2520of%2520multiple%250Amodels%2520at%2520different%2520tile%2520sizes%2520and%2520magnification%2520was%2520performed.%2520InceptionResNet%250Awas%2520selected.%2520Single%2520artifact%2520models%2520were%2520trained%2520and%2520tested%252C%2520followed%2520by%2520a%250Alimited%2520multiple%2520instance%2520model%2520with%2520artifacts%2520that%2520performed%2520well%2520together%250A%2528chatter%252C%2520fold%252C%2520and%2520pen%2529.%2520From%2520the%2520results%2520of%2520this%2520study%2520we%2520suggest%2520a%2520hybrid%250Adesign%2520for%2520artifact%2520screening%2520composed%2520of%2520both%2520single%2520artifact%2520binary%2520models%2520as%250Awell%2520as%2520multiple%2520instance%2520models%2520to%2520optimize%2520detection%2520of%2520each%2520artifact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Automated%20Quality%20Assurance%20in%20Digital%20Pathology%3A%20Tile%0A%20%20Classification%20Approach&entry.906535625=Meredith%20VandeHaar%20and%20M.%20Clinch%20and%20I.%20Yilmaz%20and%20M.%20A.%20Rahman%20and%20Y.%20Xiao%20and%20F.%20Dogany%20and%20H.%20M.%20Alazab%20and%20A.%20Nassar%20and%20Z.%20Akkus%20and%20B.%20Dangott&entry.1292438233=%20%20Quality%20assurance%20is%20a%20critical%20but%20underexplored%20area%20in%20digital%20pathology%2C%0Awhere%20even%20minor%20artifacts%20can%20have%20significant%20effects.%20Artifacts%20have%20been%0Ashown%20to%20negatively%20impact%20the%20performance%20of%20AI%20diagnostic%20models.%20In%20current%0Apractice%2C%20trained%20staff%20manually%20review%20digitized%20images%20prior%20to%20release%20of%0Athese%20slides%20to%20pathologists%20which%20are%20then%20used%20to%20render%20a%20diagnosis.%0AConventional%20image%20processing%20approaches%2C%20provide%20a%20foundation%20for%20detecting%0Aartifacts%20on%20digital%20pathology%20slides.%20However%2C%20current%20tools%20do%20not%20leverage%0Adeep%20learning%2C%20which%20has%20the%20potential%20to%20improve%20detection%20accuracy%20and%0Ascalability.%20Despite%20these%20advancements%2C%20methods%20for%20quality%20assurance%20in%0Adigital%20pathology%20remain%20limited%2C%20presenting%20a%20gap%20for%20innovation.%0A%20%20We%20propose%20an%20AI%20algorithm%20designed%20to%20screen%20digital%20pathology%20slides%20by%0Aanalyzing%20tiles%20and%20categorizing%20them%20into%20one%20of%2010%20predefined%20artifact%20types%0Aor%20as%20background.%20This%20algorithm%20identifies%20and%20localizes%20artifacts%2C%20creating%20a%0Amap%20that%20highlights%20regions%20of%20interest.%20By%20directing%20human%20operators%20to%0Aspecific%20tiles%20affected%20by%20artifacts%2C%20the%20algorithm%20minimizes%20the%20time%20and%0Aeffort%20required%20to%20manually%20review%20entire%20slides%20for%20quality%20issues.%0A%20%20From%20internal%20archives%20and%20The%20Cancer%20Genome%20Atlas%2C%20133%20whole%20slide%20images%0Awere%20selected%20and%2010%20artifacts%20were%20annotated%20using%20an%20internally%20developed%0Asoftware%20ZAPP%20%28Mayo%20Clinic%2C%20Jacksonville%2C%20FL%29.%20Ablation%20study%20of%20multiple%0Amodels%20at%20different%20tile%20sizes%20and%20magnification%20was%20performed.%20InceptionResNet%0Awas%20selected.%20Single%20artifact%20models%20were%20trained%20and%20tested%2C%20followed%20by%20a%0Alimited%20multiple%20instance%20model%20with%20artifacts%20that%20performed%20well%20together%0A%28chatter%2C%20fold%2C%20and%20pen%29.%20From%20the%20results%20of%20this%20study%20we%20suggest%20a%20hybrid%0Adesign%20for%20artifact%20screening%20composed%20of%20both%20single%20artifact%20binary%20models%20as%0Awell%20as%20multiple%20instance%20models%20to%20optimize%20detection%20of%20each%20artifact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10916v1&entry.124074799=Read"},
{"title": "Detecting High-Stakes Interactions with Activation Probes", "author": "Alex McKenzie and Urja Pawar and Phil Blandfort and William Bankes and David Krueger and Ekdeep Singh Lubana and Dmitrii Krasheninnikov", "abstract": "  Monitoring is an important aspect of safely deploying Large Language Models\n(LLMs). This paper examines activation probes for detecting \"high-stakes\"\ninteractions -- where the text indicates that the interaction might lead to\nsignificant harm -- as a critical, yet underexplored, target for such\nmonitoring. We evaluate several probe architectures trained on synthetic data,\nand find them to exhibit robust generalization to diverse, out-of-distribution,\nreal-world data. Probes' performance is comparable to that of prompted or\nfinetuned medium-sized LLM monitors, while offering computational savings of\nsix orders-of-magnitude. Our experiments also highlight the potential of\nbuilding resource-aware hierarchical monitoring systems, where probes serve as\nan efficient initial filter and flag cases for more expensive downstream\nanalysis. We release our novel synthetic dataset and codebase to encourage\nfurther study.\n", "link": "http://arxiv.org/abs/2506.10805v1", "date": "2025-06-12", "relevancy": 2.5309, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20High-Stakes%20Interactions%20with%20Activation%20Probes&body=Title%3A%20Detecting%20High-Stakes%20Interactions%20with%20Activation%20Probes%0AAuthor%3A%20Alex%20McKenzie%20and%20Urja%20Pawar%20and%20Phil%20Blandfort%20and%20William%20Bankes%20and%20David%20Krueger%20and%20Ekdeep%20Singh%20Lubana%20and%20Dmitrii%20Krasheninnikov%0AAbstract%3A%20%20%20Monitoring%20is%20an%20important%20aspect%20of%20safely%20deploying%20Large%20Language%20Models%0A%28LLMs%29.%20This%20paper%20examines%20activation%20probes%20for%20detecting%20%22high-stakes%22%0Ainteractions%20--%20where%20the%20text%20indicates%20that%20the%20interaction%20might%20lead%20to%0Asignificant%20harm%20--%20as%20a%20critical%2C%20yet%20underexplored%2C%20target%20for%20such%0Amonitoring.%20We%20evaluate%20several%20probe%20architectures%20trained%20on%20synthetic%20data%2C%0Aand%20find%20them%20to%20exhibit%20robust%20generalization%20to%20diverse%2C%20out-of-distribution%2C%0Areal-world%20data.%20Probes%27%20performance%20is%20comparable%20to%20that%20of%20prompted%20or%0Afinetuned%20medium-sized%20LLM%20monitors%2C%20while%20offering%20computational%20savings%20of%0Asix%20orders-of-magnitude.%20Our%20experiments%20also%20highlight%20the%20potential%20of%0Abuilding%20resource-aware%20hierarchical%20monitoring%20systems%2C%20where%20probes%20serve%20as%0Aan%20efficient%20initial%20filter%20and%20flag%20cases%20for%20more%20expensive%20downstream%0Aanalysis.%20We%20release%20our%20novel%20synthetic%20dataset%20and%20codebase%20to%20encourage%0Afurther%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520High-Stakes%2520Interactions%2520with%2520Activation%2520Probes%26entry.906535625%3DAlex%2520McKenzie%2520and%2520Urja%2520Pawar%2520and%2520Phil%2520Blandfort%2520and%2520William%2520Bankes%2520and%2520David%2520Krueger%2520and%2520Ekdeep%2520Singh%2520Lubana%2520and%2520Dmitrii%2520Krasheninnikov%26entry.1292438233%3D%2520%2520Monitoring%2520is%2520an%2520important%2520aspect%2520of%2520safely%2520deploying%2520Large%2520Language%2520Models%250A%2528LLMs%2529.%2520This%2520paper%2520examines%2520activation%2520probes%2520for%2520detecting%2520%2522high-stakes%2522%250Ainteractions%2520--%2520where%2520the%2520text%2520indicates%2520that%2520the%2520interaction%2520might%2520lead%2520to%250Asignificant%2520harm%2520--%2520as%2520a%2520critical%252C%2520yet%2520underexplored%252C%2520target%2520for%2520such%250Amonitoring.%2520We%2520evaluate%2520several%2520probe%2520architectures%2520trained%2520on%2520synthetic%2520data%252C%250Aand%2520find%2520them%2520to%2520exhibit%2520robust%2520generalization%2520to%2520diverse%252C%2520out-of-distribution%252C%250Areal-world%2520data.%2520Probes%2527%2520performance%2520is%2520comparable%2520to%2520that%2520of%2520prompted%2520or%250Afinetuned%2520medium-sized%2520LLM%2520monitors%252C%2520while%2520offering%2520computational%2520savings%2520of%250Asix%2520orders-of-magnitude.%2520Our%2520experiments%2520also%2520highlight%2520the%2520potential%2520of%250Abuilding%2520resource-aware%2520hierarchical%2520monitoring%2520systems%252C%2520where%2520probes%2520serve%2520as%250Aan%2520efficient%2520initial%2520filter%2520and%2520flag%2520cases%2520for%2520more%2520expensive%2520downstream%250Aanalysis.%2520We%2520release%2520our%2520novel%2520synthetic%2520dataset%2520and%2520codebase%2520to%2520encourage%250Afurther%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20High-Stakes%20Interactions%20with%20Activation%20Probes&entry.906535625=Alex%20McKenzie%20and%20Urja%20Pawar%20and%20Phil%20Blandfort%20and%20William%20Bankes%20and%20David%20Krueger%20and%20Ekdeep%20Singh%20Lubana%20and%20Dmitrii%20Krasheninnikov&entry.1292438233=%20%20Monitoring%20is%20an%20important%20aspect%20of%20safely%20deploying%20Large%20Language%20Models%0A%28LLMs%29.%20This%20paper%20examines%20activation%20probes%20for%20detecting%20%22high-stakes%22%0Ainteractions%20--%20where%20the%20text%20indicates%20that%20the%20interaction%20might%20lead%20to%0Asignificant%20harm%20--%20as%20a%20critical%2C%20yet%20underexplored%2C%20target%20for%20such%0Amonitoring.%20We%20evaluate%20several%20probe%20architectures%20trained%20on%20synthetic%20data%2C%0Aand%20find%20them%20to%20exhibit%20robust%20generalization%20to%20diverse%2C%20out-of-distribution%2C%0Areal-world%20data.%20Probes%27%20performance%20is%20comparable%20to%20that%20of%20prompted%20or%0Afinetuned%20medium-sized%20LLM%20monitors%2C%20while%20offering%20computational%20savings%20of%0Asix%20orders-of-magnitude.%20Our%20experiments%20also%20highlight%20the%20potential%20of%0Abuilding%20resource-aware%20hierarchical%20monitoring%20systems%2C%20where%20probes%20serve%20as%0Aan%20efficient%20initial%20filter%20and%20flag%20cases%20for%20more%20expensive%20downstream%0Aanalysis.%20We%20release%20our%20novel%20synthetic%20dataset%20and%20codebase%20to%20encourage%0Afurther%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10805v1&entry.124074799=Read"},
{"title": "Persistent Topological Features in Large Language Models", "author": "Yuri Gardinazzi and Karthik Viswanathan and Giada Panerai and Alessio Ansuini and Alberto Cazzaniga and Matteo Biagetti", "abstract": "  Understanding the decision-making processes of large language models is\ncritical given their widespread applications. To achieve this, we aim to\nconnect a formal mathematical framework -- zigzag persistence from topological\ndata analysis -- with practical and easily applicable algorithms. Zigzag\npersistence is particularly effective for characterizing data as it dynamically\ntransforms across model layers. Within this framework, we introduce topological\ndescriptors that measure how topological features, $p$-dimensional holes,\npersist and evolve throughout the layers. Unlike methods that assess each layer\nindividually and then aggregate the results, our approach directly tracks the\nfull evolutionary path of these features. This offers a statistical perspective\non how prompts are rearranged and their relative positions changed in the\nrepresentation space, providing insights into the system's operation as an\nintegrated whole. To demonstrate the expressivity and applicability of our\nframework, we highlight how sensitive these descriptors are to different models\nand a variety of datasets. As a showcase application to a downstream task, we\nuse zigzag persistence to establish a criterion for layer pruning, achieving\nresults comparable to state-of-the-art methods while preserving the\nsystem-level perspective.\n", "link": "http://arxiv.org/abs/2410.11042v2", "date": "2025-06-12", "relevancy": 2.5276, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Topological%20Features%20in%20Large%20Language%20Models&body=Title%3A%20Persistent%20Topological%20Features%20in%20Large%20Language%20Models%0AAuthor%3A%20Yuri%20Gardinazzi%20and%20Karthik%20Viswanathan%20and%20Giada%20Panerai%20and%20Alessio%20Ansuini%20and%20Alberto%20Cazzaniga%20and%20Matteo%20Biagetti%0AAbstract%3A%20%20%20Understanding%20the%20decision-making%20processes%20of%20large%20language%20models%20is%0Acritical%20given%20their%20widespread%20applications.%20To%20achieve%20this%2C%20we%20aim%20to%0Aconnect%20a%20formal%20mathematical%20framework%20--%20zigzag%20persistence%20from%20topological%0Adata%20analysis%20--%20with%20practical%20and%20easily%20applicable%20algorithms.%20Zigzag%0Apersistence%20is%20particularly%20effective%20for%20characterizing%20data%20as%20it%20dynamically%0Atransforms%20across%20model%20layers.%20Within%20this%20framework%2C%20we%20introduce%20topological%0Adescriptors%20that%20measure%20how%20topological%20features%2C%20%24p%24-dimensional%20holes%2C%0Apersist%20and%20evolve%20throughout%20the%20layers.%20Unlike%20methods%20that%20assess%20each%20layer%0Aindividually%20and%20then%20aggregate%20the%20results%2C%20our%20approach%20directly%20tracks%20the%0Afull%20evolutionary%20path%20of%20these%20features.%20This%20offers%20a%20statistical%20perspective%0Aon%20how%20prompts%20are%20rearranged%20and%20their%20relative%20positions%20changed%20in%20the%0Arepresentation%20space%2C%20providing%20insights%20into%20the%20system%27s%20operation%20as%20an%0Aintegrated%20whole.%20To%20demonstrate%20the%20expressivity%20and%20applicability%20of%20our%0Aframework%2C%20we%20highlight%20how%20sensitive%20these%20descriptors%20are%20to%20different%20models%0Aand%20a%20variety%20of%20datasets.%20As%20a%20showcase%20application%20to%20a%20downstream%20task%2C%20we%0Ause%20zigzag%20persistence%20to%20establish%20a%20criterion%20for%20layer%20pruning%2C%20achieving%0Aresults%20comparable%20to%20state-of-the-art%20methods%20while%20preserving%20the%0Asystem-level%20perspective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Topological%2520Features%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DYuri%2520Gardinazzi%2520and%2520Karthik%2520Viswanathan%2520and%2520Giada%2520Panerai%2520and%2520Alessio%2520Ansuini%2520and%2520Alberto%2520Cazzaniga%2520and%2520Matteo%2520Biagetti%26entry.1292438233%3D%2520%2520Understanding%2520the%2520decision-making%2520processes%2520of%2520large%2520language%2520models%2520is%250Acritical%2520given%2520their%2520widespread%2520applications.%2520To%2520achieve%2520this%252C%2520we%2520aim%2520to%250Aconnect%2520a%2520formal%2520mathematical%2520framework%2520--%2520zigzag%2520persistence%2520from%2520topological%250Adata%2520analysis%2520--%2520with%2520practical%2520and%2520easily%2520applicable%2520algorithms.%2520Zigzag%250Apersistence%2520is%2520particularly%2520effective%2520for%2520characterizing%2520data%2520as%2520it%2520dynamically%250Atransforms%2520across%2520model%2520layers.%2520Within%2520this%2520framework%252C%2520we%2520introduce%2520topological%250Adescriptors%2520that%2520measure%2520how%2520topological%2520features%252C%2520%2524p%2524-dimensional%2520holes%252C%250Apersist%2520and%2520evolve%2520throughout%2520the%2520layers.%2520Unlike%2520methods%2520that%2520assess%2520each%2520layer%250Aindividually%2520and%2520then%2520aggregate%2520the%2520results%252C%2520our%2520approach%2520directly%2520tracks%2520the%250Afull%2520evolutionary%2520path%2520of%2520these%2520features.%2520This%2520offers%2520a%2520statistical%2520perspective%250Aon%2520how%2520prompts%2520are%2520rearranged%2520and%2520their%2520relative%2520positions%2520changed%2520in%2520the%250Arepresentation%2520space%252C%2520providing%2520insights%2520into%2520the%2520system%2527s%2520operation%2520as%2520an%250Aintegrated%2520whole.%2520To%2520demonstrate%2520the%2520expressivity%2520and%2520applicability%2520of%2520our%250Aframework%252C%2520we%2520highlight%2520how%2520sensitive%2520these%2520descriptors%2520are%2520to%2520different%2520models%250Aand%2520a%2520variety%2520of%2520datasets.%2520As%2520a%2520showcase%2520application%2520to%2520a%2520downstream%2520task%252C%2520we%250Ause%2520zigzag%2520persistence%2520to%2520establish%2520a%2520criterion%2520for%2520layer%2520pruning%252C%2520achieving%250Aresults%2520comparable%2520to%2520state-of-the-art%2520methods%2520while%2520preserving%2520the%250Asystem-level%2520perspective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Topological%20Features%20in%20Large%20Language%20Models&entry.906535625=Yuri%20Gardinazzi%20and%20Karthik%20Viswanathan%20and%20Giada%20Panerai%20and%20Alessio%20Ansuini%20and%20Alberto%20Cazzaniga%20and%20Matteo%20Biagetti&entry.1292438233=%20%20Understanding%20the%20decision-making%20processes%20of%20large%20language%20models%20is%0Acritical%20given%20their%20widespread%20applications.%20To%20achieve%20this%2C%20we%20aim%20to%0Aconnect%20a%20formal%20mathematical%20framework%20--%20zigzag%20persistence%20from%20topological%0Adata%20analysis%20--%20with%20practical%20and%20easily%20applicable%20algorithms.%20Zigzag%0Apersistence%20is%20particularly%20effective%20for%20characterizing%20data%20as%20it%20dynamically%0Atransforms%20across%20model%20layers.%20Within%20this%20framework%2C%20we%20introduce%20topological%0Adescriptors%20that%20measure%20how%20topological%20features%2C%20%24p%24-dimensional%20holes%2C%0Apersist%20and%20evolve%20throughout%20the%20layers.%20Unlike%20methods%20that%20assess%20each%20layer%0Aindividually%20and%20then%20aggregate%20the%20results%2C%20our%20approach%20directly%20tracks%20the%0Afull%20evolutionary%20path%20of%20these%20features.%20This%20offers%20a%20statistical%20perspective%0Aon%20how%20prompts%20are%20rearranged%20and%20their%20relative%20positions%20changed%20in%20the%0Arepresentation%20space%2C%20providing%20insights%20into%20the%20system%27s%20operation%20as%20an%0Aintegrated%20whole.%20To%20demonstrate%20the%20expressivity%20and%20applicability%20of%20our%0Aframework%2C%20we%20highlight%20how%20sensitive%20these%20descriptors%20are%20to%20different%20models%0Aand%20a%20variety%20of%20datasets.%20As%20a%20showcase%20application%20to%20a%20downstream%20task%2C%20we%0Ause%20zigzag%20persistence%20to%20establish%20a%20criterion%20for%20layer%20pruning%2C%20achieving%0Aresults%20comparable%20to%20state-of-the-art%20methods%20while%20preserving%20the%0Asystem-level%20perspective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11042v2&entry.124074799=Read"},
{"title": "Slimming Down LLMs Without Losing Their Minds", "author": " Qingda and  Mai", "abstract": "  This paper investigates and validates the impact of fine-tuning on large\nlanguage model performance, focusing on parameter-efficient methods (LoRA and\nQLoRA). We evaluate model capabilities across three key domains: (1)\ncommonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3)\nmulti-domain knowledge (MMLU-CS).\n  Our findings demonstrate that: (1) LoRA-based methods effectively improve\ntask-specific performance while maintaining computational efficiency, and (2)\nperformance strongly depends on alignment between fine-tuning dataset and\nbenchmark tasks. The study provides both theoretical insights into\nparameter-efficient mechanisms and practical guidance for developers\nimplementing efficient LLM adaptation with limited resources.\n", "link": "http://arxiv.org/abs/2506.10885v1", "date": "2025-06-12", "relevancy": 2.4768, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4987}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slimming%20Down%20LLMs%20Without%20Losing%20Their%20Minds&body=Title%3A%20Slimming%20Down%20LLMs%20Without%20Losing%20Their%20Minds%0AAuthor%3A%20%20Qingda%20and%20%20Mai%0AAbstract%3A%20%20%20This%20paper%20investigates%20and%20validates%20the%20impact%20of%20fine-tuning%20on%20large%0Alanguage%20model%20performance%2C%20focusing%20on%20parameter-efficient%20methods%20%28LoRA%20and%0AQLoRA%29.%20We%20evaluate%20model%20capabilities%20across%20three%20key%20domains%3A%20%281%29%0Acommonsense%20reasoning%20%28HellaSwag%29%2C%20%282%29%20mathematical%20reasoning%20%28GSM8K%29%2C%20and%20%283%29%0Amulti-domain%20knowledge%20%28MMLU-CS%29.%0A%20%20Our%20findings%20demonstrate%20that%3A%20%281%29%20LoRA-based%20methods%20effectively%20improve%0Atask-specific%20performance%20while%20maintaining%20computational%20efficiency%2C%20and%20%282%29%0Aperformance%20strongly%20depends%20on%20alignment%20between%20fine-tuning%20dataset%20and%0Abenchmark%20tasks.%20The%20study%20provides%20both%20theoretical%20insights%20into%0Aparameter-efficient%20mechanisms%20and%20practical%20guidance%20for%20developers%0Aimplementing%20efficient%20LLM%20adaptation%20with%20limited%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlimming%2520Down%2520LLMs%2520Without%2520Losing%2520Their%2520Minds%26entry.906535625%3D%2520Qingda%2520and%2520%2520Mai%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520and%2520validates%2520the%2520impact%2520of%2520fine-tuning%2520on%2520large%250Alanguage%2520model%2520performance%252C%2520focusing%2520on%2520parameter-efficient%2520methods%2520%2528LoRA%2520and%250AQLoRA%2529.%2520We%2520evaluate%2520model%2520capabilities%2520across%2520three%2520key%2520domains%253A%2520%25281%2529%250Acommonsense%2520reasoning%2520%2528HellaSwag%2529%252C%2520%25282%2529%2520mathematical%2520reasoning%2520%2528GSM8K%2529%252C%2520and%2520%25283%2529%250Amulti-domain%2520knowledge%2520%2528MMLU-CS%2529.%250A%2520%2520Our%2520findings%2520demonstrate%2520that%253A%2520%25281%2529%2520LoRA-based%2520methods%2520effectively%2520improve%250Atask-specific%2520performance%2520while%2520maintaining%2520computational%2520efficiency%252C%2520and%2520%25282%2529%250Aperformance%2520strongly%2520depends%2520on%2520alignment%2520between%2520fine-tuning%2520dataset%2520and%250Abenchmark%2520tasks.%2520The%2520study%2520provides%2520both%2520theoretical%2520insights%2520into%250Aparameter-efficient%2520mechanisms%2520and%2520practical%2520guidance%2520for%2520developers%250Aimplementing%2520efficient%2520LLM%2520adaptation%2520with%2520limited%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slimming%20Down%20LLMs%20Without%20Losing%20Their%20Minds&entry.906535625=%20Qingda%20and%20%20Mai&entry.1292438233=%20%20This%20paper%20investigates%20and%20validates%20the%20impact%20of%20fine-tuning%20on%20large%0Alanguage%20model%20performance%2C%20focusing%20on%20parameter-efficient%20methods%20%28LoRA%20and%0AQLoRA%29.%20We%20evaluate%20model%20capabilities%20across%20three%20key%20domains%3A%20%281%29%0Acommonsense%20reasoning%20%28HellaSwag%29%2C%20%282%29%20mathematical%20reasoning%20%28GSM8K%29%2C%20and%20%283%29%0Amulti-domain%20knowledge%20%28MMLU-CS%29.%0A%20%20Our%20findings%20demonstrate%20that%3A%20%281%29%20LoRA-based%20methods%20effectively%20improve%0Atask-specific%20performance%20while%20maintaining%20computational%20efficiency%2C%20and%20%282%29%0Aperformance%20strongly%20depends%20on%20alignment%20between%20fine-tuning%20dataset%20and%0Abenchmark%20tasks.%20The%20study%20provides%20both%20theoretical%20insights%20into%0Aparameter-efficient%20mechanisms%20and%20practical%20guidance%20for%20developers%0Aimplementing%20efficient%20LLM%20adaptation%20with%20limited%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10885v1&entry.124074799=Read"},
{"title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using", "author": "Huaying Yuan and Zheng Liu and Junjie Zhou and Ji-Rong Wen and Zhicheng Dou", "abstract": "  Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.\n", "link": "http://arxiv.org/abs/2506.10821v1", "date": "2025-06-12", "relevancy": 2.4745, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.626}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoDeepResearch%3A%20Long%20Video%20Understanding%20With%20Agentic%20Tool%20Using&body=Title%3A%20VideoDeepResearch%3A%20Long%20Video%20Understanding%20With%20Agentic%20Tool%20Using%0AAuthor%3A%20Huaying%20Yuan%20and%20Zheng%20Liu%20and%20Junjie%20Zhou%20and%20Ji-Rong%20Wen%20and%20Zhicheng%20Dou%0AAbstract%3A%20%20%20Long%20video%20understanding%20%28LVU%29%20presents%20a%20significant%20challenge%20for%20current%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20due%20to%20the%20task%27s%20inherent%20complexity%0Aand%20context%20window%20constraint.%20It%20is%20widely%20assumed%20that%20addressing%20LVU%20tasks%0Arequires%20foundation%20MLLMs%20with%20extended%20context%20windows%2C%20strong%20visual%0Aperception%20capabilities%2C%20and%20proficient%20domain%20expertise.%20In%20this%20work%2C%20we%0Achallenge%20this%20common%20belief%20by%20introducing%20VideoDeepResearch%2C%20a%20novel%20agentic%0Aframework%20for%20long%20video%20understanding.%20Our%20approach%20relies%20solely%20on%20a%0Atext-only%20large%20reasoning%20model%20%28LRM%29%20combined%20with%20a%20modular%20multi-modal%0Atoolkit%2C%20including%20multimodal%20retrievers%20and%20visual%20perceivers%2C%20all%20of%20which%0Aare%20readily%20available%20in%20practice.%20For%20each%20LVU%20task%2C%20the%20system%20formulates%20a%0Aproblem-solving%20strategy%20through%20reasoning%2C%20while%20selectively%20accessing%20and%0Autilizing%20essential%20video%20content%20via%20tool%20using.%20We%20conduct%20extensive%0Aexperiments%20on%20popular%20LVU%20benchmarks%2C%20including%20MLVU%2C%20Video-MME%2C%20and%20LVBench.%0AOur%20results%20demonstrate%20that%20VideoDeepResearch%20achieves%20substantial%0Aimprovements%20over%20existing%20MLLM%20baselines%2C%20surpassing%20the%20previous%0Astate-of-the-art%20by%209.6%25%2C%206.6%25%2C%20and%203.9%25%20on%20MLVU%20%28test%29%2C%20LVBench%2C%20and%0ALongVideoBench%2C%20respectively.%20These%20findings%20highlight%20the%20promise%20of%20agentic%0Asystems%20in%20overcoming%20key%20challenges%20in%20LVU%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoDeepResearch%253A%2520Long%2520Video%2520Understanding%2520With%2520Agentic%2520Tool%2520Using%26entry.906535625%3DHuaying%2520Yuan%2520and%2520Zheng%2520Liu%2520and%2520Junjie%2520Zhou%2520and%2520Ji-Rong%2520Wen%2520and%2520Zhicheng%2520Dou%26entry.1292438233%3D%2520%2520Long%2520video%2520understanding%2520%2528LVU%2529%2520presents%2520a%2520significant%2520challenge%2520for%2520current%250Amulti-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520due%2520to%2520the%2520task%2527s%2520inherent%2520complexity%250Aand%2520context%2520window%2520constraint.%2520It%2520is%2520widely%2520assumed%2520that%2520addressing%2520LVU%2520tasks%250Arequires%2520foundation%2520MLLMs%2520with%2520extended%2520context%2520windows%252C%2520strong%2520visual%250Aperception%2520capabilities%252C%2520and%2520proficient%2520domain%2520expertise.%2520In%2520this%2520work%252C%2520we%250Achallenge%2520this%2520common%2520belief%2520by%2520introducing%2520VideoDeepResearch%252C%2520a%2520novel%2520agentic%250Aframework%2520for%2520long%2520video%2520understanding.%2520Our%2520approach%2520relies%2520solely%2520on%2520a%250Atext-only%2520large%2520reasoning%2520model%2520%2528LRM%2529%2520combined%2520with%2520a%2520modular%2520multi-modal%250Atoolkit%252C%2520including%2520multimodal%2520retrievers%2520and%2520visual%2520perceivers%252C%2520all%2520of%2520which%250Aare%2520readily%2520available%2520in%2520practice.%2520For%2520each%2520LVU%2520task%252C%2520the%2520system%2520formulates%2520a%250Aproblem-solving%2520strategy%2520through%2520reasoning%252C%2520while%2520selectively%2520accessing%2520and%250Autilizing%2520essential%2520video%2520content%2520via%2520tool%2520using.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520popular%2520LVU%2520benchmarks%252C%2520including%2520MLVU%252C%2520Video-MME%252C%2520and%2520LVBench.%250AOur%2520results%2520demonstrate%2520that%2520VideoDeepResearch%2520achieves%2520substantial%250Aimprovements%2520over%2520existing%2520MLLM%2520baselines%252C%2520surpassing%2520the%2520previous%250Astate-of-the-art%2520by%25209.6%2525%252C%25206.6%2525%252C%2520and%25203.9%2525%2520on%2520MLVU%2520%2528test%2529%252C%2520LVBench%252C%2520and%250ALongVideoBench%252C%2520respectively.%2520These%2520findings%2520highlight%2520the%2520promise%2520of%2520agentic%250Asystems%2520in%2520overcoming%2520key%2520challenges%2520in%2520LVU%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoDeepResearch%3A%20Long%20Video%20Understanding%20With%20Agentic%20Tool%20Using&entry.906535625=Huaying%20Yuan%20and%20Zheng%20Liu%20and%20Junjie%20Zhou%20and%20Ji-Rong%20Wen%20and%20Zhicheng%20Dou&entry.1292438233=%20%20Long%20video%20understanding%20%28LVU%29%20presents%20a%20significant%20challenge%20for%20current%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20due%20to%20the%20task%27s%20inherent%20complexity%0Aand%20context%20window%20constraint.%20It%20is%20widely%20assumed%20that%20addressing%20LVU%20tasks%0Arequires%20foundation%20MLLMs%20with%20extended%20context%20windows%2C%20strong%20visual%0Aperception%20capabilities%2C%20and%20proficient%20domain%20expertise.%20In%20this%20work%2C%20we%0Achallenge%20this%20common%20belief%20by%20introducing%20VideoDeepResearch%2C%20a%20novel%20agentic%0Aframework%20for%20long%20video%20understanding.%20Our%20approach%20relies%20solely%20on%20a%0Atext-only%20large%20reasoning%20model%20%28LRM%29%20combined%20with%20a%20modular%20multi-modal%0Atoolkit%2C%20including%20multimodal%20retrievers%20and%20visual%20perceivers%2C%20all%20of%20which%0Aare%20readily%20available%20in%20practice.%20For%20each%20LVU%20task%2C%20the%20system%20formulates%20a%0Aproblem-solving%20strategy%20through%20reasoning%2C%20while%20selectively%20accessing%20and%0Autilizing%20essential%20video%20content%20via%20tool%20using.%20We%20conduct%20extensive%0Aexperiments%20on%20popular%20LVU%20benchmarks%2C%20including%20MLVU%2C%20Video-MME%2C%20and%20LVBench.%0AOur%20results%20demonstrate%20that%20VideoDeepResearch%20achieves%20substantial%0Aimprovements%20over%20existing%20MLLM%20baselines%2C%20surpassing%20the%20previous%0Astate-of-the-art%20by%209.6%25%2C%206.6%25%2C%20and%203.9%25%20on%20MLVU%20%28test%29%2C%20LVBench%2C%20and%0ALongVideoBench%2C%20respectively.%20These%20findings%20highlight%20the%20promise%20of%20agentic%0Asystems%20in%20overcoming%20key%20challenges%20in%20LVU%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10821v1&entry.124074799=Read"},
{"title": "Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated\n  Global Context Information", "author": "Kehan Long and Shasha Li and Chen Xu and Jintao Tang and Ting Wang", "abstract": "  Recent advancements have successfully harnessed the power of Large Language\nModels (LLMs) for zero-shot document ranking, exploring a variety of prompting\nstrategies. Comparative approaches like pairwise and listwise achieve high\neffectiveness but are computationally intensive and thus less practical for\nlarger-scale applications. Scoring-based pointwise approaches exhibit superior\nefficiency by independently and simultaneously generating the relevance scores\nfor each candidate document. However, this independence ignores critical\ncomparative insights between documents, resulting in inconsistent scoring and\nsuboptimal performance. In this paper, we aim to improve the effectiveness of\npointwise methods while preserving their efficiency through two key\ninnovations: (1) We propose a novel Global-Consistent Comparative Pointwise\nRanking (GCCP) strategy that incorporates global reference comparisons between\neach candidate and an anchor document to generate contrastive relevance scores.\nWe strategically design the anchor document as a query-focused summary of\npseudo-relevant candidates, which serves as an effective reference point by\ncapturing the global context for document comparison. (2) These contrastive\nrelevance scores can be efficiently Post-Aggregated with existing pointwise\nmethods, seamlessly integrating essential Global Context information in a\ntraining-free manner (PAGC). Extensive experiments on the TREC DL and BEIR\nbenchmark demonstrate that our approach significantly outperforms previous\npointwise methods while maintaining comparable efficiency. Our method also\nachieves competitive performance against comparative methods that require\nsubstantially more computational resources. More analyses further validate the\nefficacy of our anchor construction strategy.\n", "link": "http://arxiv.org/abs/2506.10859v1", "date": "2025-06-12", "relevancy": 2.4684, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Precise%20Zero-Shot%20Pointwise%20Ranking%20with%20LLMs%20through%20Post-Aggregated%0A%20%20Global%20Context%20Information&body=Title%3A%20Precise%20Zero-Shot%20Pointwise%20Ranking%20with%20LLMs%20through%20Post-Aggregated%0A%20%20Global%20Context%20Information%0AAuthor%3A%20Kehan%20Long%20and%20Shasha%20Li%20and%20Chen%20Xu%20and%20Jintao%20Tang%20and%20Ting%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20have%20successfully%20harnessed%20the%20power%20of%20Large%20Language%0AModels%20%28LLMs%29%20for%20zero-shot%20document%20ranking%2C%20exploring%20a%20variety%20of%20prompting%0Astrategies.%20Comparative%20approaches%20like%20pairwise%20and%20listwise%20achieve%20high%0Aeffectiveness%20but%20are%20computationally%20intensive%20and%20thus%20less%20practical%20for%0Alarger-scale%20applications.%20Scoring-based%20pointwise%20approaches%20exhibit%20superior%0Aefficiency%20by%20independently%20and%20simultaneously%20generating%20the%20relevance%20scores%0Afor%20each%20candidate%20document.%20However%2C%20this%20independence%20ignores%20critical%0Acomparative%20insights%20between%20documents%2C%20resulting%20in%20inconsistent%20scoring%20and%0Asuboptimal%20performance.%20In%20this%20paper%2C%20we%20aim%20to%20improve%20the%20effectiveness%20of%0Apointwise%20methods%20while%20preserving%20their%20efficiency%20through%20two%20key%0Ainnovations%3A%20%281%29%20We%20propose%20a%20novel%20Global-Consistent%20Comparative%20Pointwise%0ARanking%20%28GCCP%29%20strategy%20that%20incorporates%20global%20reference%20comparisons%20between%0Aeach%20candidate%20and%20an%20anchor%20document%20to%20generate%20contrastive%20relevance%20scores.%0AWe%20strategically%20design%20the%20anchor%20document%20as%20a%20query-focused%20summary%20of%0Apseudo-relevant%20candidates%2C%20which%20serves%20as%20an%20effective%20reference%20point%20by%0Acapturing%20the%20global%20context%20for%20document%20comparison.%20%282%29%20These%20contrastive%0Arelevance%20scores%20can%20be%20efficiently%20Post-Aggregated%20with%20existing%20pointwise%0Amethods%2C%20seamlessly%20integrating%20essential%20Global%20Context%20information%20in%20a%0Atraining-free%20manner%20%28PAGC%29.%20Extensive%20experiments%20on%20the%20TREC%20DL%20and%20BEIR%0Abenchmark%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20previous%0Apointwise%20methods%20while%20maintaining%20comparable%20efficiency.%20Our%20method%20also%0Aachieves%20competitive%20performance%20against%20comparative%20methods%20that%20require%0Asubstantially%20more%20computational%20resources.%20More%20analyses%20further%20validate%20the%0Aefficacy%20of%20our%20anchor%20construction%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrecise%2520Zero-Shot%2520Pointwise%2520Ranking%2520with%2520LLMs%2520through%2520Post-Aggregated%250A%2520%2520Global%2520Context%2520Information%26entry.906535625%3DKehan%2520Long%2520and%2520Shasha%2520Li%2520and%2520Chen%2520Xu%2520and%2520Jintao%2520Tang%2520and%2520Ting%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520successfully%2520harnessed%2520the%2520power%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520for%2520zero-shot%2520document%2520ranking%252C%2520exploring%2520a%2520variety%2520of%2520prompting%250Astrategies.%2520Comparative%2520approaches%2520like%2520pairwise%2520and%2520listwise%2520achieve%2520high%250Aeffectiveness%2520but%2520are%2520computationally%2520intensive%2520and%2520thus%2520less%2520practical%2520for%250Alarger-scale%2520applications.%2520Scoring-based%2520pointwise%2520approaches%2520exhibit%2520superior%250Aefficiency%2520by%2520independently%2520and%2520simultaneously%2520generating%2520the%2520relevance%2520scores%250Afor%2520each%2520candidate%2520document.%2520However%252C%2520this%2520independence%2520ignores%2520critical%250Acomparative%2520insights%2520between%2520documents%252C%2520resulting%2520in%2520inconsistent%2520scoring%2520and%250Asuboptimal%2520performance.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520improve%2520the%2520effectiveness%2520of%250Apointwise%2520methods%2520while%2520preserving%2520their%2520efficiency%2520through%2520two%2520key%250Ainnovations%253A%2520%25281%2529%2520We%2520propose%2520a%2520novel%2520Global-Consistent%2520Comparative%2520Pointwise%250ARanking%2520%2528GCCP%2529%2520strategy%2520that%2520incorporates%2520global%2520reference%2520comparisons%2520between%250Aeach%2520candidate%2520and%2520an%2520anchor%2520document%2520to%2520generate%2520contrastive%2520relevance%2520scores.%250AWe%2520strategically%2520design%2520the%2520anchor%2520document%2520as%2520a%2520query-focused%2520summary%2520of%250Apseudo-relevant%2520candidates%252C%2520which%2520serves%2520as%2520an%2520effective%2520reference%2520point%2520by%250Acapturing%2520the%2520global%2520context%2520for%2520document%2520comparison.%2520%25282%2529%2520These%2520contrastive%250Arelevance%2520scores%2520can%2520be%2520efficiently%2520Post-Aggregated%2520with%2520existing%2520pointwise%250Amethods%252C%2520seamlessly%2520integrating%2520essential%2520Global%2520Context%2520information%2520in%2520a%250Atraining-free%2520manner%2520%2528PAGC%2529.%2520Extensive%2520experiments%2520on%2520the%2520TREC%2520DL%2520and%2520BEIR%250Abenchmark%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520previous%250Apointwise%2520methods%2520while%2520maintaining%2520comparable%2520efficiency.%2520Our%2520method%2520also%250Aachieves%2520competitive%2520performance%2520against%2520comparative%2520methods%2520that%2520require%250Asubstantially%2520more%2520computational%2520resources.%2520More%2520analyses%2520further%2520validate%2520the%250Aefficacy%2520of%2520our%2520anchor%2520construction%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Precise%20Zero-Shot%20Pointwise%20Ranking%20with%20LLMs%20through%20Post-Aggregated%0A%20%20Global%20Context%20Information&entry.906535625=Kehan%20Long%20and%20Shasha%20Li%20and%20Chen%20Xu%20and%20Jintao%20Tang%20and%20Ting%20Wang&entry.1292438233=%20%20Recent%20advancements%20have%20successfully%20harnessed%20the%20power%20of%20Large%20Language%0AModels%20%28LLMs%29%20for%20zero-shot%20document%20ranking%2C%20exploring%20a%20variety%20of%20prompting%0Astrategies.%20Comparative%20approaches%20like%20pairwise%20and%20listwise%20achieve%20high%0Aeffectiveness%20but%20are%20computationally%20intensive%20and%20thus%20less%20practical%20for%0Alarger-scale%20applications.%20Scoring-based%20pointwise%20approaches%20exhibit%20superior%0Aefficiency%20by%20independently%20and%20simultaneously%20generating%20the%20relevance%20scores%0Afor%20each%20candidate%20document.%20However%2C%20this%20independence%20ignores%20critical%0Acomparative%20insights%20between%20documents%2C%20resulting%20in%20inconsistent%20scoring%20and%0Asuboptimal%20performance.%20In%20this%20paper%2C%20we%20aim%20to%20improve%20the%20effectiveness%20of%0Apointwise%20methods%20while%20preserving%20their%20efficiency%20through%20two%20key%0Ainnovations%3A%20%281%29%20We%20propose%20a%20novel%20Global-Consistent%20Comparative%20Pointwise%0ARanking%20%28GCCP%29%20strategy%20that%20incorporates%20global%20reference%20comparisons%20between%0Aeach%20candidate%20and%20an%20anchor%20document%20to%20generate%20contrastive%20relevance%20scores.%0AWe%20strategically%20design%20the%20anchor%20document%20as%20a%20query-focused%20summary%20of%0Apseudo-relevant%20candidates%2C%20which%20serves%20as%20an%20effective%20reference%20point%20by%0Acapturing%20the%20global%20context%20for%20document%20comparison.%20%282%29%20These%20contrastive%0Arelevance%20scores%20can%20be%20efficiently%20Post-Aggregated%20with%20existing%20pointwise%0Amethods%2C%20seamlessly%20integrating%20essential%20Global%20Context%20information%20in%20a%0Atraining-free%20manner%20%28PAGC%29.%20Extensive%20experiments%20on%20the%20TREC%20DL%20and%20BEIR%0Abenchmark%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20previous%0Apointwise%20methods%20while%20maintaining%20comparable%20efficiency.%20Our%20method%20also%0Aachieves%20competitive%20performance%20against%20comparative%20methods%20that%20require%0Asubstantially%20more%20computational%20resources.%20More%20analyses%20further%20validate%20the%0Aefficacy%20of%20our%20anchor%20construction%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10859v1&entry.124074799=Read"},
{"title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training", "author": "Mozhi Zhang and Howe Tissue and Lu Wang and Xipeng Qiu", "abstract": "  We introduce~\\textsc{Domain2Vec}, a novel approach that decomposes any\ndataset into a linear combination of several \\emph{meta-domains}, a new concept\ndesigned to capture the key underlying features of datasets.\n\\textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\emph{\\textbf{D}istribution\n\\textbf{A}lignment \\textbf{A}ssumption} (DA$^{2}$), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, \\textsc{Domain2vec} can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\n\\textsc{Domain2Vec} helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\n\\textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only\n$51.5\\%$ of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, \\textsc{Domain2Vec} improves\ndownstream performance by an average of $2.83\\%$.\n", "link": "http://arxiv.org/abs/2506.10952v1", "date": "2025-06-12", "relevancy": 2.4637, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain2Vec%3A%20Vectorizing%20Datasets%20to%20Find%20the%20Optimal%20Data%20Mixture%0A%20%20without%20Training&body=Title%3A%20Domain2Vec%3A%20Vectorizing%20Datasets%20to%20Find%20the%20Optimal%20Data%20Mixture%0A%20%20without%20Training%0AAuthor%3A%20Mozhi%20Zhang%20and%20Howe%20Tissue%20and%20Lu%20Wang%20and%20Xipeng%20Qiu%0AAbstract%3A%20%20%20We%20introduce~%5Ctextsc%7BDomain2Vec%7D%2C%20a%20novel%20approach%20that%20decomposes%20any%0Adataset%20into%20a%20linear%20combination%20of%20several%20%5Cemph%7Bmeta-domains%7D%2C%20a%20new%20concept%0Adesigned%20to%20capture%20the%20key%20underlying%20features%20of%20datasets.%0A%5Ctextsc%7BDomain2Vec%7D%20maintains%20a%20vocabulary%20of%20meta-domains%20and%20uses%20a%0Aclassifier%20to%20decompose%20any%20given%20dataset%20into%20a%20domain%20vector%20that%20corresponds%0Ato%20a%20distribution%20over%20this%20vocabulary.%20These%20domain%20vectors%20enable%20the%0Aidentification%20of%20the%20optimal%20data%20mixture%20for%20language%20model%20%28LM%29%20pretraining%0Ain%20a%20training-free%20manner%20under%20the%20%5Cemph%7B%5Ctextbf%7BD%7Distribution%0A%5Ctextbf%7BA%7Dlignment%20%5Ctextbf%7BA%7Dssumption%7D%20%28DA%24%5E%7B2%7D%24%29%2C%20which%20suggests%20that%20when%0Athe%20data%20distributions%20of%20the%20training%20set%20and%20the%20validation%20set%20are%20better%0Aaligned%2C%20a%20lower%20validation%20loss%20is%20achieved.%20Moreover%2C%20%5Ctextsc%7BDomain2vec%7D%20can%0Abe%20seamlessly%20integrated%20into%20previous%20works%20to%20model%20the%20relationship%20between%0Adomain%20vectors%20and%20LM%20performance%2C%20greatly%20enhancing%20the%20efficiency%20and%0Ascalability%20of%20previous%20methods.%20Extensive%20experiments%20demonstrate%20that%0A%5Ctextsc%7BDomain2Vec%7D%20helps%20find%20the%20data%20mixture%20that%20enhances%20downstream%20task%0Aperformance%20with%20minimal%20computational%20overhead.%20Specifically%2C%0A%5Ctextsc%7BDomain2Vec%7D%20achieves%20the%20same%20validation%20loss%20on%20Pile-CC%20using%20only%0A%2451.5%5C%25%24%20of%20the%20computation%20required%20when%20training%20on%20the%20original%20mixture%20of%0AThe%20Pile%20dataset.%20Under%20equivalent%20compute%20budget%2C%20%5Ctextsc%7BDomain2Vec%7D%20improves%0Adownstream%20performance%20by%20an%20average%20of%20%242.83%5C%25%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain2Vec%253A%2520Vectorizing%2520Datasets%2520to%2520Find%2520the%2520Optimal%2520Data%2520Mixture%250A%2520%2520without%2520Training%26entry.906535625%3DMozhi%2520Zhang%2520and%2520Howe%2520Tissue%2520and%2520Lu%2520Wang%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3D%2520%2520We%2520introduce~%255Ctextsc%257BDomain2Vec%257D%252C%2520a%2520novel%2520approach%2520that%2520decomposes%2520any%250Adataset%2520into%2520a%2520linear%2520combination%2520of%2520several%2520%255Cemph%257Bmeta-domains%257D%252C%2520a%2520new%2520concept%250Adesigned%2520to%2520capture%2520the%2520key%2520underlying%2520features%2520of%2520datasets.%250A%255Ctextsc%257BDomain2Vec%257D%2520maintains%2520a%2520vocabulary%2520of%2520meta-domains%2520and%2520uses%2520a%250Aclassifier%2520to%2520decompose%2520any%2520given%2520dataset%2520into%2520a%2520domain%2520vector%2520that%2520corresponds%250Ato%2520a%2520distribution%2520over%2520this%2520vocabulary.%2520These%2520domain%2520vectors%2520enable%2520the%250Aidentification%2520of%2520the%2520optimal%2520data%2520mixture%2520for%2520language%2520model%2520%2528LM%2529%2520pretraining%250Ain%2520a%2520training-free%2520manner%2520under%2520the%2520%255Cemph%257B%255Ctextbf%257BD%257Distribution%250A%255Ctextbf%257BA%257Dlignment%2520%255Ctextbf%257BA%257Dssumption%257D%2520%2528DA%2524%255E%257B2%257D%2524%2529%252C%2520which%2520suggests%2520that%2520when%250Athe%2520data%2520distributions%2520of%2520the%2520training%2520set%2520and%2520the%2520validation%2520set%2520are%2520better%250Aaligned%252C%2520a%2520lower%2520validation%2520loss%2520is%2520achieved.%2520Moreover%252C%2520%255Ctextsc%257BDomain2vec%257D%2520can%250Abe%2520seamlessly%2520integrated%2520into%2520previous%2520works%2520to%2520model%2520the%2520relationship%2520between%250Adomain%2520vectors%2520and%2520LM%2520performance%252C%2520greatly%2520enhancing%2520the%2520efficiency%2520and%250Ascalability%2520of%2520previous%2520methods.%2520Extensive%2520experiments%2520demonstrate%2520that%250A%255Ctextsc%257BDomain2Vec%257D%2520helps%2520find%2520the%2520data%2520mixture%2520that%2520enhances%2520downstream%2520task%250Aperformance%2520with%2520minimal%2520computational%2520overhead.%2520Specifically%252C%250A%255Ctextsc%257BDomain2Vec%257D%2520achieves%2520the%2520same%2520validation%2520loss%2520on%2520Pile-CC%2520using%2520only%250A%252451.5%255C%2525%2524%2520of%2520the%2520computation%2520required%2520when%2520training%2520on%2520the%2520original%2520mixture%2520of%250AThe%2520Pile%2520dataset.%2520Under%2520equivalent%2520compute%2520budget%252C%2520%255Ctextsc%257BDomain2Vec%257D%2520improves%250Adownstream%2520performance%2520by%2520an%2520average%2520of%2520%25242.83%255C%2525%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain2Vec%3A%20Vectorizing%20Datasets%20to%20Find%20the%20Optimal%20Data%20Mixture%0A%20%20without%20Training&entry.906535625=Mozhi%20Zhang%20and%20Howe%20Tissue%20and%20Lu%20Wang%20and%20Xipeng%20Qiu&entry.1292438233=%20%20We%20introduce~%5Ctextsc%7BDomain2Vec%7D%2C%20a%20novel%20approach%20that%20decomposes%20any%0Adataset%20into%20a%20linear%20combination%20of%20several%20%5Cemph%7Bmeta-domains%7D%2C%20a%20new%20concept%0Adesigned%20to%20capture%20the%20key%20underlying%20features%20of%20datasets.%0A%5Ctextsc%7BDomain2Vec%7D%20maintains%20a%20vocabulary%20of%20meta-domains%20and%20uses%20a%0Aclassifier%20to%20decompose%20any%20given%20dataset%20into%20a%20domain%20vector%20that%20corresponds%0Ato%20a%20distribution%20over%20this%20vocabulary.%20These%20domain%20vectors%20enable%20the%0Aidentification%20of%20the%20optimal%20data%20mixture%20for%20language%20model%20%28LM%29%20pretraining%0Ain%20a%20training-free%20manner%20under%20the%20%5Cemph%7B%5Ctextbf%7BD%7Distribution%0A%5Ctextbf%7BA%7Dlignment%20%5Ctextbf%7BA%7Dssumption%7D%20%28DA%24%5E%7B2%7D%24%29%2C%20which%20suggests%20that%20when%0Athe%20data%20distributions%20of%20the%20training%20set%20and%20the%20validation%20set%20are%20better%0Aaligned%2C%20a%20lower%20validation%20loss%20is%20achieved.%20Moreover%2C%20%5Ctextsc%7BDomain2vec%7D%20can%0Abe%20seamlessly%20integrated%20into%20previous%20works%20to%20model%20the%20relationship%20between%0Adomain%20vectors%20and%20LM%20performance%2C%20greatly%20enhancing%20the%20efficiency%20and%0Ascalability%20of%20previous%20methods.%20Extensive%20experiments%20demonstrate%20that%0A%5Ctextsc%7BDomain2Vec%7D%20helps%20find%20the%20data%20mixture%20that%20enhances%20downstream%20task%0Aperformance%20with%20minimal%20computational%20overhead.%20Specifically%2C%0A%5Ctextsc%7BDomain2Vec%7D%20achieves%20the%20same%20validation%20loss%20on%20Pile-CC%20using%20only%0A%2451.5%5C%25%24%20of%20the%20computation%20required%20when%20training%20on%20the%20original%20mixture%20of%0AThe%20Pile%20dataset.%20Under%20equivalent%20compute%20budget%2C%20%5Ctextsc%7BDomain2Vec%7D%20improves%0Adownstream%20performance%20by%20an%20average%20of%20%242.83%5C%25%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10952v1&entry.124074799=Read"},
{"title": "TexTailor: Customized Text-aligned Texturing via Effective Resampling", "author": "Suin Lee and Dae-Shik Kim", "abstract": "  We present TexTailor, a novel method for generating consistent object\ntextures from textual descriptions. Existing text-to-texture synthesis\napproaches utilize depth-aware diffusion models to progressively generate\nimages and synthesize textures across predefined multiple viewpoints. However,\nthese approaches lead to a gradual shift in texture properties across\nviewpoints due to (1) insufficient integration of previously synthesized\ntextures at each viewpoint during the diffusion process and (2) the\nautoregressive nature of the texture synthesis process. Moreover, the\npredefined selection of camera positions, which does not account for the\nobject's geometry, limits the effective use of texture information synthesized\nfrom different viewpoints, ultimately degrading overall texture consistency. In\nTexTailor, we address these issues by (1) applying a resampling scheme that\nrepeatedly integrates information from previously synthesized textures within\nthe diffusion process, and (2) fine-tuning a depth-aware diffusion model on\nthese resampled textures. During this process, we observed that using only a\nfew training images restricts the model's original ability to generate\nhigh-fidelity images aligned with the conditioning, and therefore propose an\nperformance preservation loss to mitigate this issue. Additionally, we improve\nthe synthesis of view-consistent textures by adaptively adjusting camera\npositions based on the object's geometry. Experiments on a subset of the\nObjaverse dataset and the ShapeNet car dataset demonstrate that TexTailor\noutperforms state-of-the-art methods in synthesizing view-consistent textures.\nThe source code for TexTailor is available at\nhttps://github.com/Adios42/Textailor\n", "link": "http://arxiv.org/abs/2506.10612v1", "date": "2025-06-12", "relevancy": 2.4541, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6017}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TexTailor%3A%20Customized%20Text-aligned%20Texturing%20via%20Effective%20Resampling&body=Title%3A%20TexTailor%3A%20Customized%20Text-aligned%20Texturing%20via%20Effective%20Resampling%0AAuthor%3A%20Suin%20Lee%20and%20Dae-Shik%20Kim%0AAbstract%3A%20%20%20We%20present%20TexTailor%2C%20a%20novel%20method%20for%20generating%20consistent%20object%0Atextures%20from%20textual%20descriptions.%20Existing%20text-to-texture%20synthesis%0Aapproaches%20utilize%20depth-aware%20diffusion%20models%20to%20progressively%20generate%0Aimages%20and%20synthesize%20textures%20across%20predefined%20multiple%20viewpoints.%20However%2C%0Athese%20approaches%20lead%20to%20a%20gradual%20shift%20in%20texture%20properties%20across%0Aviewpoints%20due%20to%20%281%29%20insufficient%20integration%20of%20previously%20synthesized%0Atextures%20at%20each%20viewpoint%20during%20the%20diffusion%20process%20and%20%282%29%20the%0Aautoregressive%20nature%20of%20the%20texture%20synthesis%20process.%20Moreover%2C%20the%0Apredefined%20selection%20of%20camera%20positions%2C%20which%20does%20not%20account%20for%20the%0Aobject%27s%20geometry%2C%20limits%20the%20effective%20use%20of%20texture%20information%20synthesized%0Afrom%20different%20viewpoints%2C%20ultimately%20degrading%20overall%20texture%20consistency.%20In%0ATexTailor%2C%20we%20address%20these%20issues%20by%20%281%29%20applying%20a%20resampling%20scheme%20that%0Arepeatedly%20integrates%20information%20from%20previously%20synthesized%20textures%20within%0Athe%20diffusion%20process%2C%20and%20%282%29%20fine-tuning%20a%20depth-aware%20diffusion%20model%20on%0Athese%20resampled%20textures.%20During%20this%20process%2C%20we%20observed%20that%20using%20only%20a%0Afew%20training%20images%20restricts%20the%20model%27s%20original%20ability%20to%20generate%0Ahigh-fidelity%20images%20aligned%20with%20the%20conditioning%2C%20and%20therefore%20propose%20an%0Aperformance%20preservation%20loss%20to%20mitigate%20this%20issue.%20Additionally%2C%20we%20improve%0Athe%20synthesis%20of%20view-consistent%20textures%20by%20adaptively%20adjusting%20camera%0Apositions%20based%20on%20the%20object%27s%20geometry.%20Experiments%20on%20a%20subset%20of%20the%0AObjaverse%20dataset%20and%20the%20ShapeNet%20car%20dataset%20demonstrate%20that%20TexTailor%0Aoutperforms%20state-of-the-art%20methods%20in%20synthesizing%20view-consistent%20textures.%0AThe%20source%20code%20for%20TexTailor%20is%20available%20at%0Ahttps%3A//github.com/Adios42/Textailor%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTexTailor%253A%2520Customized%2520Text-aligned%2520Texturing%2520via%2520Effective%2520Resampling%26entry.906535625%3DSuin%2520Lee%2520and%2520Dae-Shik%2520Kim%26entry.1292438233%3D%2520%2520We%2520present%2520TexTailor%252C%2520a%2520novel%2520method%2520for%2520generating%2520consistent%2520object%250Atextures%2520from%2520textual%2520descriptions.%2520Existing%2520text-to-texture%2520synthesis%250Aapproaches%2520utilize%2520depth-aware%2520diffusion%2520models%2520to%2520progressively%2520generate%250Aimages%2520and%2520synthesize%2520textures%2520across%2520predefined%2520multiple%2520viewpoints.%2520However%252C%250Athese%2520approaches%2520lead%2520to%2520a%2520gradual%2520shift%2520in%2520texture%2520properties%2520across%250Aviewpoints%2520due%2520to%2520%25281%2529%2520insufficient%2520integration%2520of%2520previously%2520synthesized%250Atextures%2520at%2520each%2520viewpoint%2520during%2520the%2520diffusion%2520process%2520and%2520%25282%2529%2520the%250Aautoregressive%2520nature%2520of%2520the%2520texture%2520synthesis%2520process.%2520Moreover%252C%2520the%250Apredefined%2520selection%2520of%2520camera%2520positions%252C%2520which%2520does%2520not%2520account%2520for%2520the%250Aobject%2527s%2520geometry%252C%2520limits%2520the%2520effective%2520use%2520of%2520texture%2520information%2520synthesized%250Afrom%2520different%2520viewpoints%252C%2520ultimately%2520degrading%2520overall%2520texture%2520consistency.%2520In%250ATexTailor%252C%2520we%2520address%2520these%2520issues%2520by%2520%25281%2529%2520applying%2520a%2520resampling%2520scheme%2520that%250Arepeatedly%2520integrates%2520information%2520from%2520previously%2520synthesized%2520textures%2520within%250Athe%2520diffusion%2520process%252C%2520and%2520%25282%2529%2520fine-tuning%2520a%2520depth-aware%2520diffusion%2520model%2520on%250Athese%2520resampled%2520textures.%2520During%2520this%2520process%252C%2520we%2520observed%2520that%2520using%2520only%2520a%250Afew%2520training%2520images%2520restricts%2520the%2520model%2527s%2520original%2520ability%2520to%2520generate%250Ahigh-fidelity%2520images%2520aligned%2520with%2520the%2520conditioning%252C%2520and%2520therefore%2520propose%2520an%250Aperformance%2520preservation%2520loss%2520to%2520mitigate%2520this%2520issue.%2520Additionally%252C%2520we%2520improve%250Athe%2520synthesis%2520of%2520view-consistent%2520textures%2520by%2520adaptively%2520adjusting%2520camera%250Apositions%2520based%2520on%2520the%2520object%2527s%2520geometry.%2520Experiments%2520on%2520a%2520subset%2520of%2520the%250AObjaverse%2520dataset%2520and%2520the%2520ShapeNet%2520car%2520dataset%2520demonstrate%2520that%2520TexTailor%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520synthesizing%2520view-consistent%2520textures.%250AThe%2520source%2520code%2520for%2520TexTailor%2520is%2520available%2520at%250Ahttps%253A//github.com/Adios42/Textailor%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexTailor%3A%20Customized%20Text-aligned%20Texturing%20via%20Effective%20Resampling&entry.906535625=Suin%20Lee%20and%20Dae-Shik%20Kim&entry.1292438233=%20%20We%20present%20TexTailor%2C%20a%20novel%20method%20for%20generating%20consistent%20object%0Atextures%20from%20textual%20descriptions.%20Existing%20text-to-texture%20synthesis%0Aapproaches%20utilize%20depth-aware%20diffusion%20models%20to%20progressively%20generate%0Aimages%20and%20synthesize%20textures%20across%20predefined%20multiple%20viewpoints.%20However%2C%0Athese%20approaches%20lead%20to%20a%20gradual%20shift%20in%20texture%20properties%20across%0Aviewpoints%20due%20to%20%281%29%20insufficient%20integration%20of%20previously%20synthesized%0Atextures%20at%20each%20viewpoint%20during%20the%20diffusion%20process%20and%20%282%29%20the%0Aautoregressive%20nature%20of%20the%20texture%20synthesis%20process.%20Moreover%2C%20the%0Apredefined%20selection%20of%20camera%20positions%2C%20which%20does%20not%20account%20for%20the%0Aobject%27s%20geometry%2C%20limits%20the%20effective%20use%20of%20texture%20information%20synthesized%0Afrom%20different%20viewpoints%2C%20ultimately%20degrading%20overall%20texture%20consistency.%20In%0ATexTailor%2C%20we%20address%20these%20issues%20by%20%281%29%20applying%20a%20resampling%20scheme%20that%0Arepeatedly%20integrates%20information%20from%20previously%20synthesized%20textures%20within%0Athe%20diffusion%20process%2C%20and%20%282%29%20fine-tuning%20a%20depth-aware%20diffusion%20model%20on%0Athese%20resampled%20textures.%20During%20this%20process%2C%20we%20observed%20that%20using%20only%20a%0Afew%20training%20images%20restricts%20the%20model%27s%20original%20ability%20to%20generate%0Ahigh-fidelity%20images%20aligned%20with%20the%20conditioning%2C%20and%20therefore%20propose%20an%0Aperformance%20preservation%20loss%20to%20mitigate%20this%20issue.%20Additionally%2C%20we%20improve%0Athe%20synthesis%20of%20view-consistent%20textures%20by%20adaptively%20adjusting%20camera%0Apositions%20based%20on%20the%20object%27s%20geometry.%20Experiments%20on%20a%20subset%20of%20the%0AObjaverse%20dataset%20and%20the%20ShapeNet%20car%20dataset%20demonstrate%20that%20TexTailor%0Aoutperforms%20state-of-the-art%20methods%20in%20synthesizing%20view-consistent%20textures.%0AThe%20source%20code%20for%20TexTailor%20is%20available%20at%0Ahttps%3A//github.com/Adios42/Textailor%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10612v1&entry.124074799=Read"},
{"title": "Adaptive Federated LoRA in Heterogeneous Wireless Networks with\n  Independent Sampling", "author": "Yanzhao Hou and Jiaxiang Geng and Boyu Li and Xiaofeng Tao and Juncheng Wang and Xiaodong Xu and Bing Luo", "abstract": "  Federated LoRA has emerged as a promising technique for efficiently\nfine-tuning large language models (LLMs) on distributed devices by reducing the\nnumber of trainable parameters. However, existing approaches often inadequately\noverlook the theoretical and practical implications of system and data\nheterogeneity, thereby failing to optimize the overall training efficiency,\nparticularly in terms of wall-clock time. In this paper, we propose an adaptive\nfederated LoRA strategy with independent client sampling to minimize the\nconvergence wall-clock time of federated fine-tuning under both computation and\ncommunication heterogeneity. We first derive a new convergence bound for\nfederated LoRA with arbitrary and independent client sampling, notably without\nrequiring the stringent bounded gradient assumption. Then, we introduce an\nadaptive bandwidth allocation scheme that accounts for heterogeneous client\nresources and system bandwidth constraints. Based on the derived theory, we\nformulate and solve a non-convex optimization problem to jointly determine the\nLoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock\nconvergence time. An efficient and low-complexity algorithm is developed to\napproximate the solution. Finally, extensive experiments demonstrate that our\napproach significantly reduces wall-clock training time compared to\nstate-of-the-art methods across various models and datasets.\n", "link": "http://arxiv.org/abs/2505.23555v2", "date": "2025-06-12", "relevancy": 2.4437, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5135}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5001}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Federated%20LoRA%20in%20Heterogeneous%20Wireless%20Networks%20with%0A%20%20Independent%20Sampling&body=Title%3A%20Adaptive%20Federated%20LoRA%20in%20Heterogeneous%20Wireless%20Networks%20with%0A%20%20Independent%20Sampling%0AAuthor%3A%20Yanzhao%20Hou%20and%20Jiaxiang%20Geng%20and%20Boyu%20Li%20and%20Xiaofeng%20Tao%20and%20Juncheng%20Wang%20and%20Xiaodong%20Xu%20and%20Bing%20Luo%0AAbstract%3A%20%20%20Federated%20LoRA%20has%20emerged%20as%20a%20promising%20technique%20for%20efficiently%0Afine-tuning%20large%20language%20models%20%28LLMs%29%20on%20distributed%20devices%20by%20reducing%20the%0Anumber%20of%20trainable%20parameters.%20However%2C%20existing%20approaches%20often%20inadequately%0Aoverlook%20the%20theoretical%20and%20practical%20implications%20of%20system%20and%20data%0Aheterogeneity%2C%20thereby%20failing%20to%20optimize%20the%20overall%20training%20efficiency%2C%0Aparticularly%20in%20terms%20of%20wall-clock%20time.%20In%20this%20paper%2C%20we%20propose%20an%20adaptive%0Afederated%20LoRA%20strategy%20with%20independent%20client%20sampling%20to%20minimize%20the%0Aconvergence%20wall-clock%20time%20of%20federated%20fine-tuning%20under%20both%20computation%20and%0Acommunication%20heterogeneity.%20We%20first%20derive%20a%20new%20convergence%20bound%20for%0Afederated%20LoRA%20with%20arbitrary%20and%20independent%20client%20sampling%2C%20notably%20without%0Arequiring%20the%20stringent%20bounded%20gradient%20assumption.%20Then%2C%20we%20introduce%20an%0Aadaptive%20bandwidth%20allocation%20scheme%20that%20accounts%20for%20heterogeneous%20client%0Aresources%20and%20system%20bandwidth%20constraints.%20Based%20on%20the%20derived%20theory%2C%20we%0Aformulate%20and%20solve%20a%20non-convex%20optimization%20problem%20to%20jointly%20determine%20the%0ALoRA%20sketching%20ratios%20and%20sampling%20probabilities%2C%20aiming%20to%20minimize%20wall-clock%0Aconvergence%20time.%20An%20efficient%20and%20low-complexity%20algorithm%20is%20developed%20to%0Aapproximate%20the%20solution.%20Finally%2C%20extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20significantly%20reduces%20wall-clock%20training%20time%20compared%20to%0Astate-of-the-art%20methods%20across%20various%20models%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Federated%2520LoRA%2520in%2520Heterogeneous%2520Wireless%2520Networks%2520with%250A%2520%2520Independent%2520Sampling%26entry.906535625%3DYanzhao%2520Hou%2520and%2520Jiaxiang%2520Geng%2520and%2520Boyu%2520Li%2520and%2520Xiaofeng%2520Tao%2520and%2520Juncheng%2520Wang%2520and%2520Xiaodong%2520Xu%2520and%2520Bing%2520Luo%26entry.1292438233%3D%2520%2520Federated%2520LoRA%2520has%2520emerged%2520as%2520a%2520promising%2520technique%2520for%2520efficiently%250Afine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520distributed%2520devices%2520by%2520reducing%2520the%250Anumber%2520of%2520trainable%2520parameters.%2520However%252C%2520existing%2520approaches%2520often%2520inadequately%250Aoverlook%2520the%2520theoretical%2520and%2520practical%2520implications%2520of%2520system%2520and%2520data%250Aheterogeneity%252C%2520thereby%2520failing%2520to%2520optimize%2520the%2520overall%2520training%2520efficiency%252C%250Aparticularly%2520in%2520terms%2520of%2520wall-clock%2520time.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520adaptive%250Afederated%2520LoRA%2520strategy%2520with%2520independent%2520client%2520sampling%2520to%2520minimize%2520the%250Aconvergence%2520wall-clock%2520time%2520of%2520federated%2520fine-tuning%2520under%2520both%2520computation%2520and%250Acommunication%2520heterogeneity.%2520We%2520first%2520derive%2520a%2520new%2520convergence%2520bound%2520for%250Afederated%2520LoRA%2520with%2520arbitrary%2520and%2520independent%2520client%2520sampling%252C%2520notably%2520without%250Arequiring%2520the%2520stringent%2520bounded%2520gradient%2520assumption.%2520Then%252C%2520we%2520introduce%2520an%250Aadaptive%2520bandwidth%2520allocation%2520scheme%2520that%2520accounts%2520for%2520heterogeneous%2520client%250Aresources%2520and%2520system%2520bandwidth%2520constraints.%2520Based%2520on%2520the%2520derived%2520theory%252C%2520we%250Aformulate%2520and%2520solve%2520a%2520non-convex%2520optimization%2520problem%2520to%2520jointly%2520determine%2520the%250ALoRA%2520sketching%2520ratios%2520and%2520sampling%2520probabilities%252C%2520aiming%2520to%2520minimize%2520wall-clock%250Aconvergence%2520time.%2520An%2520efficient%2520and%2520low-complexity%2520algorithm%2520is%2520developed%2520to%250Aapproximate%2520the%2520solution.%2520Finally%252C%2520extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520significantly%2520reduces%2520wall-clock%2520training%2520time%2520compared%2520to%250Astate-of-the-art%2520methods%2520across%2520various%2520models%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Federated%20LoRA%20in%20Heterogeneous%20Wireless%20Networks%20with%0A%20%20Independent%20Sampling&entry.906535625=Yanzhao%20Hou%20and%20Jiaxiang%20Geng%20and%20Boyu%20Li%20and%20Xiaofeng%20Tao%20and%20Juncheng%20Wang%20and%20Xiaodong%20Xu%20and%20Bing%20Luo&entry.1292438233=%20%20Federated%20LoRA%20has%20emerged%20as%20a%20promising%20technique%20for%20efficiently%0Afine-tuning%20large%20language%20models%20%28LLMs%29%20on%20distributed%20devices%20by%20reducing%20the%0Anumber%20of%20trainable%20parameters.%20However%2C%20existing%20approaches%20often%20inadequately%0Aoverlook%20the%20theoretical%20and%20practical%20implications%20of%20system%20and%20data%0Aheterogeneity%2C%20thereby%20failing%20to%20optimize%20the%20overall%20training%20efficiency%2C%0Aparticularly%20in%20terms%20of%20wall-clock%20time.%20In%20this%20paper%2C%20we%20propose%20an%20adaptive%0Afederated%20LoRA%20strategy%20with%20independent%20client%20sampling%20to%20minimize%20the%0Aconvergence%20wall-clock%20time%20of%20federated%20fine-tuning%20under%20both%20computation%20and%0Acommunication%20heterogeneity.%20We%20first%20derive%20a%20new%20convergence%20bound%20for%0Afederated%20LoRA%20with%20arbitrary%20and%20independent%20client%20sampling%2C%20notably%20without%0Arequiring%20the%20stringent%20bounded%20gradient%20assumption.%20Then%2C%20we%20introduce%20an%0Aadaptive%20bandwidth%20allocation%20scheme%20that%20accounts%20for%20heterogeneous%20client%0Aresources%20and%20system%20bandwidth%20constraints.%20Based%20on%20the%20derived%20theory%2C%20we%0Aformulate%20and%20solve%20a%20non-convex%20optimization%20problem%20to%20jointly%20determine%20the%0ALoRA%20sketching%20ratios%20and%20sampling%20probabilities%2C%20aiming%20to%20minimize%20wall-clock%0Aconvergence%20time.%20An%20efficient%20and%20low-complexity%20algorithm%20is%20developed%20to%0Aapproximate%20the%20solution.%20Finally%2C%20extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20significantly%20reduces%20wall-clock%20training%20time%20compared%20to%0Astate-of-the-art%20methods%20across%20various%20models%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23555v2&entry.124074799=Read"},
{"title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner", "author": "Marco Spinaci and Marek Polewczyk and Maximilian Schambach and Sam Thelin", "abstract": "  Tabular in-context learning (ICL) has recently achieved state-of-the-art\n(SOTA) performance on several tabular prediction tasks. Previously restricted\nto classification problems on small tables, recent advances such as TabPFN and\nTabICL have extended its use to larger datasets. While being architecturally\nefficient and well-adapted to tabular data structures, current table-native ICL\narchitectures, being trained exclusively on synthetic data, do not fully\nleverage the rich semantics and world knowledge contained in real-world tabular\ndata. On another end of this spectrum, tabular ICL models based on pretrained\nlarge language models such as TabuLa-8B integrate deep semantic understanding\nand world knowledge but are only able to make use of a small amount of context\ndue to inherent architectural limitations. With the aim to combine the best of\nboth these worlds, we introduce ConTextTab, integrating semantic understanding\nand alignment into a table-native ICL framework. By employing specialized\nembeddings for different data modalities and by training on large-scale\nreal-world tabular data, our model is competitive with SOTA across a broad set\nof benchmarks while setting a new standard on the semantically rich CARTE\nbenchmark.\n", "link": "http://arxiv.org/abs/2506.10707v1", "date": "2025-06-12", "relevancy": 2.4433, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConTextTab%3A%20A%20Semantics-Aware%20Tabular%20In-Context%20Learner&body=Title%3A%20ConTextTab%3A%20A%20Semantics-Aware%20Tabular%20In-Context%20Learner%0AAuthor%3A%20Marco%20Spinaci%20and%20Marek%20Polewczyk%20and%20Maximilian%20Schambach%20and%20Sam%20Thelin%0AAbstract%3A%20%20%20Tabular%20in-context%20learning%20%28ICL%29%20has%20recently%20achieved%20state-of-the-art%0A%28SOTA%29%20performance%20on%20several%20tabular%20prediction%20tasks.%20Previously%20restricted%0Ato%20classification%20problems%20on%20small%20tables%2C%20recent%20advances%20such%20as%20TabPFN%20and%0ATabICL%20have%20extended%20its%20use%20to%20larger%20datasets.%20While%20being%20architecturally%0Aefficient%20and%20well-adapted%20to%20tabular%20data%20structures%2C%20current%20table-native%20ICL%0Aarchitectures%2C%20being%20trained%20exclusively%20on%20synthetic%20data%2C%20do%20not%20fully%0Aleverage%20the%20rich%20semantics%20and%20world%20knowledge%20contained%20in%20real-world%20tabular%0Adata.%20On%20another%20end%20of%20this%20spectrum%2C%20tabular%20ICL%20models%20based%20on%20pretrained%0Alarge%20language%20models%20such%20as%20TabuLa-8B%20integrate%20deep%20semantic%20understanding%0Aand%20world%20knowledge%20but%20are%20only%20able%20to%20make%20use%20of%20a%20small%20amount%20of%20context%0Adue%20to%20inherent%20architectural%20limitations.%20With%20the%20aim%20to%20combine%20the%20best%20of%0Aboth%20these%20worlds%2C%20we%20introduce%20ConTextTab%2C%20integrating%20semantic%20understanding%0Aand%20alignment%20into%20a%20table-native%20ICL%20framework.%20By%20employing%20specialized%0Aembeddings%20for%20different%20data%20modalities%20and%20by%20training%20on%20large-scale%0Areal-world%20tabular%20data%2C%20our%20model%20is%20competitive%20with%20SOTA%20across%20a%20broad%20set%0Aof%20benchmarks%20while%20setting%20a%20new%20standard%20on%20the%20semantically%20rich%20CARTE%0Abenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConTextTab%253A%2520A%2520Semantics-Aware%2520Tabular%2520In-Context%2520Learner%26entry.906535625%3DMarco%2520Spinaci%2520and%2520Marek%2520Polewczyk%2520and%2520Maximilian%2520Schambach%2520and%2520Sam%2520Thelin%26entry.1292438233%3D%2520%2520Tabular%2520in-context%2520learning%2520%2528ICL%2529%2520has%2520recently%2520achieved%2520state-of-the-art%250A%2528SOTA%2529%2520performance%2520on%2520several%2520tabular%2520prediction%2520tasks.%2520Previously%2520restricted%250Ato%2520classification%2520problems%2520on%2520small%2520tables%252C%2520recent%2520advances%2520such%2520as%2520TabPFN%2520and%250ATabICL%2520have%2520extended%2520its%2520use%2520to%2520larger%2520datasets.%2520While%2520being%2520architecturally%250Aefficient%2520and%2520well-adapted%2520to%2520tabular%2520data%2520structures%252C%2520current%2520table-native%2520ICL%250Aarchitectures%252C%2520being%2520trained%2520exclusively%2520on%2520synthetic%2520data%252C%2520do%2520not%2520fully%250Aleverage%2520the%2520rich%2520semantics%2520and%2520world%2520knowledge%2520contained%2520in%2520real-world%2520tabular%250Adata.%2520On%2520another%2520end%2520of%2520this%2520spectrum%252C%2520tabular%2520ICL%2520models%2520based%2520on%2520pretrained%250Alarge%2520language%2520models%2520such%2520as%2520TabuLa-8B%2520integrate%2520deep%2520semantic%2520understanding%250Aand%2520world%2520knowledge%2520but%2520are%2520only%2520able%2520to%2520make%2520use%2520of%2520a%2520small%2520amount%2520of%2520context%250Adue%2520to%2520inherent%2520architectural%2520limitations.%2520With%2520the%2520aim%2520to%2520combine%2520the%2520best%2520of%250Aboth%2520these%2520worlds%252C%2520we%2520introduce%2520ConTextTab%252C%2520integrating%2520semantic%2520understanding%250Aand%2520alignment%2520into%2520a%2520table-native%2520ICL%2520framework.%2520By%2520employing%2520specialized%250Aembeddings%2520for%2520different%2520data%2520modalities%2520and%2520by%2520training%2520on%2520large-scale%250Areal-world%2520tabular%2520data%252C%2520our%2520model%2520is%2520competitive%2520with%2520SOTA%2520across%2520a%2520broad%2520set%250Aof%2520benchmarks%2520while%2520setting%2520a%2520new%2520standard%2520on%2520the%2520semantically%2520rich%2520CARTE%250Abenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConTextTab%3A%20A%20Semantics-Aware%20Tabular%20In-Context%20Learner&entry.906535625=Marco%20Spinaci%20and%20Marek%20Polewczyk%20and%20Maximilian%20Schambach%20and%20Sam%20Thelin&entry.1292438233=%20%20Tabular%20in-context%20learning%20%28ICL%29%20has%20recently%20achieved%20state-of-the-art%0A%28SOTA%29%20performance%20on%20several%20tabular%20prediction%20tasks.%20Previously%20restricted%0Ato%20classification%20problems%20on%20small%20tables%2C%20recent%20advances%20such%20as%20TabPFN%20and%0ATabICL%20have%20extended%20its%20use%20to%20larger%20datasets.%20While%20being%20architecturally%0Aefficient%20and%20well-adapted%20to%20tabular%20data%20structures%2C%20current%20table-native%20ICL%0Aarchitectures%2C%20being%20trained%20exclusively%20on%20synthetic%20data%2C%20do%20not%20fully%0Aleverage%20the%20rich%20semantics%20and%20world%20knowledge%20contained%20in%20real-world%20tabular%0Adata.%20On%20another%20end%20of%20this%20spectrum%2C%20tabular%20ICL%20models%20based%20on%20pretrained%0Alarge%20language%20models%20such%20as%20TabuLa-8B%20integrate%20deep%20semantic%20understanding%0Aand%20world%20knowledge%20but%20are%20only%20able%20to%20make%20use%20of%20a%20small%20amount%20of%20context%0Adue%20to%20inherent%20architectural%20limitations.%20With%20the%20aim%20to%20combine%20the%20best%20of%0Aboth%20these%20worlds%2C%20we%20introduce%20ConTextTab%2C%20integrating%20semantic%20understanding%0Aand%20alignment%20into%20a%20table-native%20ICL%20framework.%20By%20employing%20specialized%0Aembeddings%20for%20different%20data%20modalities%20and%20by%20training%20on%20large-scale%0Areal-world%20tabular%20data%2C%20our%20model%20is%20competitive%20with%20SOTA%20across%20a%20broad%20set%0Aof%20benchmarks%20while%20setting%20a%20new%20standard%20on%20the%20semantically%20rich%20CARTE%0Abenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10707v1&entry.124074799=Read"},
{"title": "Saturation Self-Organizing Map", "author": "Igor Urbanik and Pawe\u0142 Gajewski", "abstract": "  Continual learning poses a fundamental challenge for neural systems, which\noften suffer from catastrophic forgetting when exposed to sequential tasks.\nSelf-Organizing Maps (SOMs), despite their interpretability and efficiency, are\nnot immune to this issue. In this paper, we introduce Saturation\nSelf-Organizing Maps (SatSOM)-an extension of SOMs designed to improve\nknowledge retention in continual learning scenarios. SatSOM incorporates a\nnovel saturation mechanism that gradually reduces the learning rate and\nneighborhood radius of neurons as they accumulate information. This effectively\nfreezes well-trained neurons and redirects learning to underutilized areas of\nthe map.\n", "link": "http://arxiv.org/abs/2506.10680v1", "date": "2025-06-12", "relevancy": 2.4416, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5012}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4906}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Saturation%20Self-Organizing%20Map&body=Title%3A%20Saturation%20Self-Organizing%20Map%0AAuthor%3A%20Igor%20Urbanik%20and%20Pawe%C5%82%20Gajewski%0AAbstract%3A%20%20%20Continual%20learning%20poses%20a%20fundamental%20challenge%20for%20neural%20systems%2C%20which%0Aoften%20suffer%20from%20catastrophic%20forgetting%20when%20exposed%20to%20sequential%20tasks.%0ASelf-Organizing%20Maps%20%28SOMs%29%2C%20despite%20their%20interpretability%20and%20efficiency%2C%20are%0Anot%20immune%20to%20this%20issue.%20In%20this%20paper%2C%20we%20introduce%20Saturation%0ASelf-Organizing%20Maps%20%28SatSOM%29-an%20extension%20of%20SOMs%20designed%20to%20improve%0Aknowledge%20retention%20in%20continual%20learning%20scenarios.%20SatSOM%20incorporates%20a%0Anovel%20saturation%20mechanism%20that%20gradually%20reduces%20the%20learning%20rate%20and%0Aneighborhood%20radius%20of%20neurons%20as%20they%20accumulate%20information.%20This%20effectively%0Afreezes%20well-trained%20neurons%20and%20redirects%20learning%20to%20underutilized%20areas%20of%0Athe%20map.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaturation%2520Self-Organizing%2520Map%26entry.906535625%3DIgor%2520Urbanik%2520and%2520Pawe%25C5%2582%2520Gajewski%26entry.1292438233%3D%2520%2520Continual%2520learning%2520poses%2520a%2520fundamental%2520challenge%2520for%2520neural%2520systems%252C%2520which%250Aoften%2520suffer%2520from%2520catastrophic%2520forgetting%2520when%2520exposed%2520to%2520sequential%2520tasks.%250ASelf-Organizing%2520Maps%2520%2528SOMs%2529%252C%2520despite%2520their%2520interpretability%2520and%2520efficiency%252C%2520are%250Anot%2520immune%2520to%2520this%2520issue.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Saturation%250ASelf-Organizing%2520Maps%2520%2528SatSOM%2529-an%2520extension%2520of%2520SOMs%2520designed%2520to%2520improve%250Aknowledge%2520retention%2520in%2520continual%2520learning%2520scenarios.%2520SatSOM%2520incorporates%2520a%250Anovel%2520saturation%2520mechanism%2520that%2520gradually%2520reduces%2520the%2520learning%2520rate%2520and%250Aneighborhood%2520radius%2520of%2520neurons%2520as%2520they%2520accumulate%2520information.%2520This%2520effectively%250Afreezes%2520well-trained%2520neurons%2520and%2520redirects%2520learning%2520to%2520underutilized%2520areas%2520of%250Athe%2520map.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saturation%20Self-Organizing%20Map&entry.906535625=Igor%20Urbanik%20and%20Pawe%C5%82%20Gajewski&entry.1292438233=%20%20Continual%20learning%20poses%20a%20fundamental%20challenge%20for%20neural%20systems%2C%20which%0Aoften%20suffer%20from%20catastrophic%20forgetting%20when%20exposed%20to%20sequential%20tasks.%0ASelf-Organizing%20Maps%20%28SOMs%29%2C%20despite%20their%20interpretability%20and%20efficiency%2C%20are%0Anot%20immune%20to%20this%20issue.%20In%20this%20paper%2C%20we%20introduce%20Saturation%0ASelf-Organizing%20Maps%20%28SatSOM%29-an%20extension%20of%20SOMs%20designed%20to%20improve%0Aknowledge%20retention%20in%20continual%20learning%20scenarios.%20SatSOM%20incorporates%20a%0Anovel%20saturation%20mechanism%20that%20gradually%20reduces%20the%20learning%20rate%20and%0Aneighborhood%20radius%20of%20neurons%20as%20they%20accumulate%20information.%20This%20effectively%0Afreezes%20well-trained%20neurons%20and%20redirects%20learning%20to%20underutilized%20areas%20of%0Athe%20map.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10680v1&entry.124074799=Read"},
{"title": "Fine-Grained Perturbation Guidance via Attention Head Selection", "author": "Donghoon Ahn and Jiwon Kang and Sanghyun Lee and Minjae Kim and Jaewon Min and Wooseok Jang and Saungwu Lee and Sayak Paul and Susung Hong and Seungryong Kim", "abstract": "  Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.\n", "link": "http://arxiv.org/abs/2506.10978v1", "date": "2025-06-12", "relevancy": 2.4207, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6291}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6232}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Perturbation%20Guidance%20via%20Attention%20Head%20Selection&body=Title%3A%20Fine-Grained%20Perturbation%20Guidance%20via%20Attention%20Head%20Selection%0AAuthor%3A%20Donghoon%20Ahn%20and%20Jiwon%20Kang%20and%20Sanghyun%20Lee%20and%20Minjae%20Kim%20and%20Jaewon%20Min%20and%20Wooseok%20Jang%20and%20Saungwu%20Lee%20and%20Sayak%20Paul%20and%20Susung%20Hong%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Recent%20guidance%20methods%20in%20diffusion%20models%20steer%20reverse%20sampling%20by%0Aperturbing%20the%20model%20to%20construct%20an%20implicit%20weak%20model%20and%20guide%20generation%0Aaway%20from%20it.%20Among%20these%20approaches%2C%20attention%20perturbation%20has%20demonstrated%0Astrong%20empirical%20performance%20in%20unconditional%20scenarios%20where%20classifier-free%0Aguidance%20is%20not%20applicable.%20However%2C%20existing%20attention%20perturbation%20methods%0Alack%20principled%20approaches%20for%20determining%20where%20perturbations%20should%20be%0Aapplied%2C%20particularly%20in%20Diffusion%20Transformer%20%28DiT%29%20architectures%20where%0Aquality-relevant%20computations%20are%20distributed%20across%20layers.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20granularity%20of%20attention%20perturbations%2C%20ranging%20from%20the%20layer%0Alevel%20down%20to%20individual%20attention%20heads%2C%20and%20discover%20that%20specific%20heads%0Agovern%20distinct%20visual%20concepts%20such%20as%20structure%2C%20style%2C%20and%20texture%20quality.%0ABuilding%20on%20this%20insight%2C%20we%20propose%20%22HeadHunter%22%2C%20a%20systematic%20framework%20for%0Aiteratively%20selecting%20attention%20heads%20that%20align%20with%20user-centric%20objectives%2C%0Aenabling%20fine-grained%20control%20over%20generation%20quality%20and%20visual%20attributes.%20In%0Aaddition%2C%20we%20introduce%20SoftPAG%2C%20which%20linearly%20interpolates%20each%20selected%0Ahead%27s%20attention%20map%20toward%20an%20identity%20matrix%2C%20providing%20a%20continuous%20knob%20to%0Atune%20perturbation%20strength%20and%20suppress%20artifacts.%20Our%20approach%20not%20only%0Amitigates%20the%20oversmoothing%20issues%20of%20existing%20layer-level%20perturbation%20but%0Aalso%20enables%20targeted%20manipulation%20of%20specific%20visual%20styles%20through%0Acompositional%20head%20selection.%20We%20validate%20our%20method%20on%20modern%20large-scale%0ADiT-based%20text-to-image%20models%20including%20Stable%20Diffusion%203%20and%20FLUX.1%2C%0Ademonstrating%20superior%20performance%20in%20both%20general%20quality%20enhancement%20and%0Astyle-specific%20guidance.%20Our%20work%20provides%20the%20first%20head-level%20analysis%20of%0Aattention%20perturbation%20in%20diffusion%20models%2C%20uncovering%20interpretable%0Aspecialization%20within%20attention%20layers%20and%20enabling%20practical%20design%20of%0Aeffective%20perturbation%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Perturbation%2520Guidance%2520via%2520Attention%2520Head%2520Selection%26entry.906535625%3DDonghoon%2520Ahn%2520and%2520Jiwon%2520Kang%2520and%2520Sanghyun%2520Lee%2520and%2520Minjae%2520Kim%2520and%2520Jaewon%2520Min%2520and%2520Wooseok%2520Jang%2520and%2520Saungwu%2520Lee%2520and%2520Sayak%2520Paul%2520and%2520Susung%2520Hong%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520guidance%2520methods%2520in%2520diffusion%2520models%2520steer%2520reverse%2520sampling%2520by%250Aperturbing%2520the%2520model%2520to%2520construct%2520an%2520implicit%2520weak%2520model%2520and%2520guide%2520generation%250Aaway%2520from%2520it.%2520Among%2520these%2520approaches%252C%2520attention%2520perturbation%2520has%2520demonstrated%250Astrong%2520empirical%2520performance%2520in%2520unconditional%2520scenarios%2520where%2520classifier-free%250Aguidance%2520is%2520not%2520applicable.%2520However%252C%2520existing%2520attention%2520perturbation%2520methods%250Alack%2520principled%2520approaches%2520for%2520determining%2520where%2520perturbations%2520should%2520be%250Aapplied%252C%2520particularly%2520in%2520Diffusion%2520Transformer%2520%2528DiT%2529%2520architectures%2520where%250Aquality-relevant%2520computations%2520are%2520distributed%2520across%2520layers.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520the%2520granularity%2520of%2520attention%2520perturbations%252C%2520ranging%2520from%2520the%2520layer%250Alevel%2520down%2520to%2520individual%2520attention%2520heads%252C%2520and%2520discover%2520that%2520specific%2520heads%250Agovern%2520distinct%2520visual%2520concepts%2520such%2520as%2520structure%252C%2520style%252C%2520and%2520texture%2520quality.%250ABuilding%2520on%2520this%2520insight%252C%2520we%2520propose%2520%2522HeadHunter%2522%252C%2520a%2520systematic%2520framework%2520for%250Aiteratively%2520selecting%2520attention%2520heads%2520that%2520align%2520with%2520user-centric%2520objectives%252C%250Aenabling%2520fine-grained%2520control%2520over%2520generation%2520quality%2520and%2520visual%2520attributes.%2520In%250Aaddition%252C%2520we%2520introduce%2520SoftPAG%252C%2520which%2520linearly%2520interpolates%2520each%2520selected%250Ahead%2527s%2520attention%2520map%2520toward%2520an%2520identity%2520matrix%252C%2520providing%2520a%2520continuous%2520knob%2520to%250Atune%2520perturbation%2520strength%2520and%2520suppress%2520artifacts.%2520Our%2520approach%2520not%2520only%250Amitigates%2520the%2520oversmoothing%2520issues%2520of%2520existing%2520layer-level%2520perturbation%2520but%250Aalso%2520enables%2520targeted%2520manipulation%2520of%2520specific%2520visual%2520styles%2520through%250Acompositional%2520head%2520selection.%2520We%2520validate%2520our%2520method%2520on%2520modern%2520large-scale%250ADiT-based%2520text-to-image%2520models%2520including%2520Stable%2520Diffusion%25203%2520and%2520FLUX.1%252C%250Ademonstrating%2520superior%2520performance%2520in%2520both%2520general%2520quality%2520enhancement%2520and%250Astyle-specific%2520guidance.%2520Our%2520work%2520provides%2520the%2520first%2520head-level%2520analysis%2520of%250Aattention%2520perturbation%2520in%2520diffusion%2520models%252C%2520uncovering%2520interpretable%250Aspecialization%2520within%2520attention%2520layers%2520and%2520enabling%2520practical%2520design%2520of%250Aeffective%2520perturbation%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Perturbation%20Guidance%20via%20Attention%20Head%20Selection&entry.906535625=Donghoon%20Ahn%20and%20Jiwon%20Kang%20and%20Sanghyun%20Lee%20and%20Minjae%20Kim%20and%20Jaewon%20Min%20and%20Wooseok%20Jang%20and%20Saungwu%20Lee%20and%20Sayak%20Paul%20and%20Susung%20Hong%20and%20Seungryong%20Kim&entry.1292438233=%20%20Recent%20guidance%20methods%20in%20diffusion%20models%20steer%20reverse%20sampling%20by%0Aperturbing%20the%20model%20to%20construct%20an%20implicit%20weak%20model%20and%20guide%20generation%0Aaway%20from%20it.%20Among%20these%20approaches%2C%20attention%20perturbation%20has%20demonstrated%0Astrong%20empirical%20performance%20in%20unconditional%20scenarios%20where%20classifier-free%0Aguidance%20is%20not%20applicable.%20However%2C%20existing%20attention%20perturbation%20methods%0Alack%20principled%20approaches%20for%20determining%20where%20perturbations%20should%20be%0Aapplied%2C%20particularly%20in%20Diffusion%20Transformer%20%28DiT%29%20architectures%20where%0Aquality-relevant%20computations%20are%20distributed%20across%20layers.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20granularity%20of%20attention%20perturbations%2C%20ranging%20from%20the%20layer%0Alevel%20down%20to%20individual%20attention%20heads%2C%20and%20discover%20that%20specific%20heads%0Agovern%20distinct%20visual%20concepts%20such%20as%20structure%2C%20style%2C%20and%20texture%20quality.%0ABuilding%20on%20this%20insight%2C%20we%20propose%20%22HeadHunter%22%2C%20a%20systematic%20framework%20for%0Aiteratively%20selecting%20attention%20heads%20that%20align%20with%20user-centric%20objectives%2C%0Aenabling%20fine-grained%20control%20over%20generation%20quality%20and%20visual%20attributes.%20In%0Aaddition%2C%20we%20introduce%20SoftPAG%2C%20which%20linearly%20interpolates%20each%20selected%0Ahead%27s%20attention%20map%20toward%20an%20identity%20matrix%2C%20providing%20a%20continuous%20knob%20to%0Atune%20perturbation%20strength%20and%20suppress%20artifacts.%20Our%20approach%20not%20only%0Amitigates%20the%20oversmoothing%20issues%20of%20existing%20layer-level%20perturbation%20but%0Aalso%20enables%20targeted%20manipulation%20of%20specific%20visual%20styles%20through%0Acompositional%20head%20selection.%20We%20validate%20our%20method%20on%20modern%20large-scale%0ADiT-based%20text-to-image%20models%20including%20Stable%20Diffusion%203%20and%20FLUX.1%2C%0Ademonstrating%20superior%20performance%20in%20both%20general%20quality%20enhancement%20and%0Astyle-specific%20guidance.%20Our%20work%20provides%20the%20first%20head-level%20analysis%20of%0Aattention%20perturbation%20in%20diffusion%20models%2C%20uncovering%20interpretable%0Aspecialization%20within%20attention%20layers%20and%20enabling%20practical%20design%20of%0Aeffective%20perturbation%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10978v1&entry.124074799=Read"},
{"title": "SceneCompleter: Dense 3D Scene Completion for Generative Novel View\n  Synthesis", "author": "Weiliang Chen and Jiayi Bi and Yuanhui Huang and Wenzhao Zheng and Yueqi Duan", "abstract": "  Generative models have gained significant attention in novel view synthesis\n(NVS) by alleviating the reliance on dense multi-view captures. However,\nexisting methods typically fall into a conventional paradigm, where generative\nmodels first complete missing areas in 2D, followed by 3D recovery techniques\nto reconstruct the scene, which often results in overly smooth surfaces and\ndistorted geometry, as generative models struggle to infer 3D structure solely\nfrom RGB data. In this paper, we propose SceneCompleter, a novel framework that\nachieves 3D-consistent generative novel view synthesis through dense 3D scene\ncompletion. SceneCompleter achieves both visual coherence and 3D-consistent\ngenerative scene completion through two key components: (1) a\ngeometry-appearance dual-stream diffusion model that jointly synthesizes novel\nviews in RGBD space; (2) a scene embedder that encodes a more holistic scene\nunderstanding from the reference image. By effectively fusing structural and\ntextural information, our method demonstrates superior coherence and\nplausibility in generative novel view synthesis across diverse datasets.\nProject Page: https://chen-wl20.github.io/SceneCompleter\n", "link": "http://arxiv.org/abs/2506.10981v1", "date": "2025-06-12", "relevancy": 2.418, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6051}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6051}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneCompleter%3A%20Dense%203D%20Scene%20Completion%20for%20Generative%20Novel%20View%0A%20%20Synthesis&body=Title%3A%20SceneCompleter%3A%20Dense%203D%20Scene%20Completion%20for%20Generative%20Novel%20View%0A%20%20Synthesis%0AAuthor%3A%20Weiliang%20Chen%20and%20Jiayi%20Bi%20and%20Yuanhui%20Huang%20and%20Wenzhao%20Zheng%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20Generative%20models%20have%20gained%20significant%20attention%20in%20novel%20view%20synthesis%0A%28NVS%29%20by%20alleviating%20the%20reliance%20on%20dense%20multi-view%20captures.%20However%2C%0Aexisting%20methods%20typically%20fall%20into%20a%20conventional%20paradigm%2C%20where%20generative%0Amodels%20first%20complete%20missing%20areas%20in%202D%2C%20followed%20by%203D%20recovery%20techniques%0Ato%20reconstruct%20the%20scene%2C%20which%20often%20results%20in%20overly%20smooth%20surfaces%20and%0Adistorted%20geometry%2C%20as%20generative%20models%20struggle%20to%20infer%203D%20structure%20solely%0Afrom%20RGB%20data.%20In%20this%20paper%2C%20we%20propose%20SceneCompleter%2C%20a%20novel%20framework%20that%0Aachieves%203D-consistent%20generative%20novel%20view%20synthesis%20through%20dense%203D%20scene%0Acompletion.%20SceneCompleter%20achieves%20both%20visual%20coherence%20and%203D-consistent%0Agenerative%20scene%20completion%20through%20two%20key%20components%3A%20%281%29%20a%0Ageometry-appearance%20dual-stream%20diffusion%20model%20that%20jointly%20synthesizes%20novel%0Aviews%20in%20RGBD%20space%3B%20%282%29%20a%20scene%20embedder%20that%20encodes%20a%20more%20holistic%20scene%0Aunderstanding%20from%20the%20reference%20image.%20By%20effectively%20fusing%20structural%20and%0Atextural%20information%2C%20our%20method%20demonstrates%20superior%20coherence%20and%0Aplausibility%20in%20generative%20novel%20view%20synthesis%20across%20diverse%20datasets.%0AProject%20Page%3A%20https%3A//chen-wl20.github.io/SceneCompleter%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneCompleter%253A%2520Dense%25203D%2520Scene%2520Completion%2520for%2520Generative%2520Novel%2520View%250A%2520%2520Synthesis%26entry.906535625%3DWeiliang%2520Chen%2520and%2520Jiayi%2520Bi%2520and%2520Yuanhui%2520Huang%2520and%2520Wenzhao%2520Zheng%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520gained%2520significant%2520attention%2520in%2520novel%2520view%2520synthesis%250A%2528NVS%2529%2520by%2520alleviating%2520the%2520reliance%2520on%2520dense%2520multi-view%2520captures.%2520However%252C%250Aexisting%2520methods%2520typically%2520fall%2520into%2520a%2520conventional%2520paradigm%252C%2520where%2520generative%250Amodels%2520first%2520complete%2520missing%2520areas%2520in%25202D%252C%2520followed%2520by%25203D%2520recovery%2520techniques%250Ato%2520reconstruct%2520the%2520scene%252C%2520which%2520often%2520results%2520in%2520overly%2520smooth%2520surfaces%2520and%250Adistorted%2520geometry%252C%2520as%2520generative%2520models%2520struggle%2520to%2520infer%25203D%2520structure%2520solely%250Afrom%2520RGB%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SceneCompleter%252C%2520a%2520novel%2520framework%2520that%250Aachieves%25203D-consistent%2520generative%2520novel%2520view%2520synthesis%2520through%2520dense%25203D%2520scene%250Acompletion.%2520SceneCompleter%2520achieves%2520both%2520visual%2520coherence%2520and%25203D-consistent%250Agenerative%2520scene%2520completion%2520through%2520two%2520key%2520components%253A%2520%25281%2529%2520a%250Ageometry-appearance%2520dual-stream%2520diffusion%2520model%2520that%2520jointly%2520synthesizes%2520novel%250Aviews%2520in%2520RGBD%2520space%253B%2520%25282%2529%2520a%2520scene%2520embedder%2520that%2520encodes%2520a%2520more%2520holistic%2520scene%250Aunderstanding%2520from%2520the%2520reference%2520image.%2520By%2520effectively%2520fusing%2520structural%2520and%250Atextural%2520information%252C%2520our%2520method%2520demonstrates%2520superior%2520coherence%2520and%250Aplausibility%2520in%2520generative%2520novel%2520view%2520synthesis%2520across%2520diverse%2520datasets.%250AProject%2520Page%253A%2520https%253A//chen-wl20.github.io/SceneCompleter%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneCompleter%3A%20Dense%203D%20Scene%20Completion%20for%20Generative%20Novel%20View%0A%20%20Synthesis&entry.906535625=Weiliang%20Chen%20and%20Jiayi%20Bi%20and%20Yuanhui%20Huang%20and%20Wenzhao%20Zheng%20and%20Yueqi%20Duan&entry.1292438233=%20%20Generative%20models%20have%20gained%20significant%20attention%20in%20novel%20view%20synthesis%0A%28NVS%29%20by%20alleviating%20the%20reliance%20on%20dense%20multi-view%20captures.%20However%2C%0Aexisting%20methods%20typically%20fall%20into%20a%20conventional%20paradigm%2C%20where%20generative%0Amodels%20first%20complete%20missing%20areas%20in%202D%2C%20followed%20by%203D%20recovery%20techniques%0Ato%20reconstruct%20the%20scene%2C%20which%20often%20results%20in%20overly%20smooth%20surfaces%20and%0Adistorted%20geometry%2C%20as%20generative%20models%20struggle%20to%20infer%203D%20structure%20solely%0Afrom%20RGB%20data.%20In%20this%20paper%2C%20we%20propose%20SceneCompleter%2C%20a%20novel%20framework%20that%0Aachieves%203D-consistent%20generative%20novel%20view%20synthesis%20through%20dense%203D%20scene%0Acompletion.%20SceneCompleter%20achieves%20both%20visual%20coherence%20and%203D-consistent%0Agenerative%20scene%20completion%20through%20two%20key%20components%3A%20%281%29%20a%0Ageometry-appearance%20dual-stream%20diffusion%20model%20that%20jointly%20synthesizes%20novel%0Aviews%20in%20RGBD%20space%3B%20%282%29%20a%20scene%20embedder%20that%20encodes%20a%20more%20holistic%20scene%0Aunderstanding%20from%20the%20reference%20image.%20By%20effectively%20fusing%20structural%20and%0Atextural%20information%2C%20our%20method%20demonstrates%20superior%20coherence%20and%0Aplausibility%20in%20generative%20novel%20view%20synthesis%20across%20diverse%20datasets.%0AProject%20Page%3A%20https%3A//chen-wl20.github.io/SceneCompleter%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10981v1&entry.124074799=Read"},
{"title": "Great Models Think Alike and this Undermines AI Oversight", "author": "Shashwat Goel and Joschka Struber and Ilze Amanda Auzina and Karuna K Chandra and Ponnurangam Kumaraguru and Douwe Kiela and Ameya Prabhu and Matthias Bethge and Jonas Geiping", "abstract": "  As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as ''AI Oversight''. We study\nhow model similarity affects both aspects of AI oversight by proposing Chance\nAdjusted Probabilistic Agreement (CAPA): a metric for LM similarity based on\noverlap in model mistakes. Using CAPA, we first show that LLM-as-a-judge scores\nfavor models similar to the judge, generalizing recent self-preference results.\nThen, we study training on LM annotations, and find complementary knowledge\nbetween the weak supervisor and strong student model plays a crucial role in\ngains from ''weak-to-strong generalization''. As model capabilities increase,\nit becomes harder to find their mistakes, and we might defer more to AI\noversight. However, we observe a concerning trend -- model mistakes are\nbecoming more similar with increasing capabilities, pointing to risks from\ncorrelated failures. Our work underscores the importance of reporting and\ncorrecting for model similarity, especially in the emerging paradigm of AI\noversight.\n", "link": "http://arxiv.org/abs/2502.04313v2", "date": "2025-06-12", "relevancy": 2.4104, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Great%20Models%20Think%20Alike%20and%20this%20Undermines%20AI%20Oversight&body=Title%3A%20Great%20Models%20Think%20Alike%20and%20this%20Undermines%20AI%20Oversight%0AAuthor%3A%20Shashwat%20Goel%20and%20Joschka%20Struber%20and%20Ilze%20Amanda%20Auzina%20and%20Karuna%20K%20Chandra%20and%20Ponnurangam%20Kumaraguru%20and%20Douwe%20Kiela%20and%20Ameya%20Prabhu%20and%20Matthias%20Bethge%20and%20Jonas%20Geiping%0AAbstract%3A%20%20%20As%20Language%20Model%20%28LM%29%20capabilities%20advance%2C%20evaluating%20and%20supervising%20them%0Aat%20scale%20is%20getting%20harder%20for%20humans.%20There%20is%20hope%20that%20other%20language%20models%0Acan%20automate%20both%20these%20tasks%2C%20which%20we%20refer%20to%20as%20%27%27AI%20Oversight%27%27.%20We%20study%0Ahow%20model%20similarity%20affects%20both%20aspects%20of%20AI%20oversight%20by%20proposing%20Chance%0AAdjusted%20Probabilistic%20Agreement%20%28CAPA%29%3A%20a%20metric%20for%20LM%20similarity%20based%20on%0Aoverlap%20in%20model%20mistakes.%20Using%20CAPA%2C%20we%20first%20show%20that%20LLM-as-a-judge%20scores%0Afavor%20models%20similar%20to%20the%20judge%2C%20generalizing%20recent%20self-preference%20results.%0AThen%2C%20we%20study%20training%20on%20LM%20annotations%2C%20and%20find%20complementary%20knowledge%0Abetween%20the%20weak%20supervisor%20and%20strong%20student%20model%20plays%20a%20crucial%20role%20in%0Agains%20from%20%27%27weak-to-strong%20generalization%27%27.%20As%20model%20capabilities%20increase%2C%0Ait%20becomes%20harder%20to%20find%20their%20mistakes%2C%20and%20we%20might%20defer%20more%20to%20AI%0Aoversight.%20However%2C%20we%20observe%20a%20concerning%20trend%20--%20model%20mistakes%20are%0Abecoming%20more%20similar%20with%20increasing%20capabilities%2C%20pointing%20to%20risks%20from%0Acorrelated%20failures.%20Our%20work%20underscores%20the%20importance%20of%20reporting%20and%0Acorrecting%20for%20model%20similarity%2C%20especially%20in%20the%20emerging%20paradigm%20of%20AI%0Aoversight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreat%2520Models%2520Think%2520Alike%2520and%2520this%2520Undermines%2520AI%2520Oversight%26entry.906535625%3DShashwat%2520Goel%2520and%2520Joschka%2520Struber%2520and%2520Ilze%2520Amanda%2520Auzina%2520and%2520Karuna%2520K%2520Chandra%2520and%2520Ponnurangam%2520Kumaraguru%2520and%2520Douwe%2520Kiela%2520and%2520Ameya%2520Prabhu%2520and%2520Matthias%2520Bethge%2520and%2520Jonas%2520Geiping%26entry.1292438233%3D%2520%2520As%2520Language%2520Model%2520%2528LM%2529%2520capabilities%2520advance%252C%2520evaluating%2520and%2520supervising%2520them%250Aat%2520scale%2520is%2520getting%2520harder%2520for%2520humans.%2520There%2520is%2520hope%2520that%2520other%2520language%2520models%250Acan%2520automate%2520both%2520these%2520tasks%252C%2520which%2520we%2520refer%2520to%2520as%2520%2527%2527AI%2520Oversight%2527%2527.%2520We%2520study%250Ahow%2520model%2520similarity%2520affects%2520both%2520aspects%2520of%2520AI%2520oversight%2520by%2520proposing%2520Chance%250AAdjusted%2520Probabilistic%2520Agreement%2520%2528CAPA%2529%253A%2520a%2520metric%2520for%2520LM%2520similarity%2520based%2520on%250Aoverlap%2520in%2520model%2520mistakes.%2520Using%2520CAPA%252C%2520we%2520first%2520show%2520that%2520LLM-as-a-judge%2520scores%250Afavor%2520models%2520similar%2520to%2520the%2520judge%252C%2520generalizing%2520recent%2520self-preference%2520results.%250AThen%252C%2520we%2520study%2520training%2520on%2520LM%2520annotations%252C%2520and%2520find%2520complementary%2520knowledge%250Abetween%2520the%2520weak%2520supervisor%2520and%2520strong%2520student%2520model%2520plays%2520a%2520crucial%2520role%2520in%250Agains%2520from%2520%2527%2527weak-to-strong%2520generalization%2527%2527.%2520As%2520model%2520capabilities%2520increase%252C%250Ait%2520becomes%2520harder%2520to%2520find%2520their%2520mistakes%252C%2520and%2520we%2520might%2520defer%2520more%2520to%2520AI%250Aoversight.%2520However%252C%2520we%2520observe%2520a%2520concerning%2520trend%2520--%2520model%2520mistakes%2520are%250Abecoming%2520more%2520similar%2520with%2520increasing%2520capabilities%252C%2520pointing%2520to%2520risks%2520from%250Acorrelated%2520failures.%2520Our%2520work%2520underscores%2520the%2520importance%2520of%2520reporting%2520and%250Acorrecting%2520for%2520model%2520similarity%252C%2520especially%2520in%2520the%2520emerging%2520paradigm%2520of%2520AI%250Aoversight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Great%20Models%20Think%20Alike%20and%20this%20Undermines%20AI%20Oversight&entry.906535625=Shashwat%20Goel%20and%20Joschka%20Struber%20and%20Ilze%20Amanda%20Auzina%20and%20Karuna%20K%20Chandra%20and%20Ponnurangam%20Kumaraguru%20and%20Douwe%20Kiela%20and%20Ameya%20Prabhu%20and%20Matthias%20Bethge%20and%20Jonas%20Geiping&entry.1292438233=%20%20As%20Language%20Model%20%28LM%29%20capabilities%20advance%2C%20evaluating%20and%20supervising%20them%0Aat%20scale%20is%20getting%20harder%20for%20humans.%20There%20is%20hope%20that%20other%20language%20models%0Acan%20automate%20both%20these%20tasks%2C%20which%20we%20refer%20to%20as%20%27%27AI%20Oversight%27%27.%20We%20study%0Ahow%20model%20similarity%20affects%20both%20aspects%20of%20AI%20oversight%20by%20proposing%20Chance%0AAdjusted%20Probabilistic%20Agreement%20%28CAPA%29%3A%20a%20metric%20for%20LM%20similarity%20based%20on%0Aoverlap%20in%20model%20mistakes.%20Using%20CAPA%2C%20we%20first%20show%20that%20LLM-as-a-judge%20scores%0Afavor%20models%20similar%20to%20the%20judge%2C%20generalizing%20recent%20self-preference%20results.%0AThen%2C%20we%20study%20training%20on%20LM%20annotations%2C%20and%20find%20complementary%20knowledge%0Abetween%20the%20weak%20supervisor%20and%20strong%20student%20model%20plays%20a%20crucial%20role%20in%0Agains%20from%20%27%27weak-to-strong%20generalization%27%27.%20As%20model%20capabilities%20increase%2C%0Ait%20becomes%20harder%20to%20find%20their%20mistakes%2C%20and%20we%20might%20defer%20more%20to%20AI%0Aoversight.%20However%2C%20we%20observe%20a%20concerning%20trend%20--%20model%20mistakes%20are%0Abecoming%20more%20similar%20with%20increasing%20capabilities%2C%20pointing%20to%20risks%20from%0Acorrelated%20failures.%20Our%20work%20underscores%20the%20importance%20of%20reporting%20and%0Acorrecting%20for%20model%20similarity%2C%20especially%20in%20the%20emerging%20paradigm%20of%20AI%0Aoversight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04313v2&entry.124074799=Read"},
{"title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views\n  for Robust Recommendation", "author": "Narges Nemati and Mostafa Haghir Chehreghani", "abstract": "  Matrix completion is a widely adopted framework in recommender systems, as\npredicting the missing entries in the user-item rating matrix enables a\ncomprehensive understanding of user preferences. However, current graph neural\nnetwork (GNN)-based approaches are highly sensitive to noisy or irrelevant\nedges--due to their inherent message-passing mechanisms--and are prone to\noverfitting, which limits their generalizability. To overcome these challenges,\nwe propose a novel method called Matrix Completion using Contrastive Learning\n(MCCL). Our approach begins by extracting local neighborhood subgraphs for each\ninteraction and subsequently generates two distinct graph representations. The\nfirst representation emphasizes denoising by integrating GNN layers with an\nattention mechanism, while the second is obtained via a graph variational\nautoencoder that aligns the feature distribution with a standard prior. A\nmutual learning loss function is employed during training to gradually\nharmonize these representations, enabling the model to capture common patterns\nand significantly enhance its generalizability. Extensive experiments on\nseveral real-world datasets demonstrate that our approach not only improves the\nnumerical accuracy of the predicted scores--achieving up to a 0.8% improvement\nin RMSE--but also produces superior rankings with improvements of up to 36% in\nranking metrics.\n", "link": "http://arxiv.org/abs/2506.10658v1", "date": "2025-06-12", "relevancy": 2.4073, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.506}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4707}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Matrix%20Completion%20with%20Denoising%20and%20Augmented%20Graph%20Views%0A%20%20for%20Robust%20Recommendation&body=Title%3A%20Contrastive%20Matrix%20Completion%20with%20Denoising%20and%20Augmented%20Graph%20Views%0A%20%20for%20Robust%20Recommendation%0AAuthor%3A%20Narges%20Nemati%20and%20Mostafa%20Haghir%20Chehreghani%0AAbstract%3A%20%20%20Matrix%20completion%20is%20a%20widely%20adopted%20framework%20in%20recommender%20systems%2C%20as%0Apredicting%20the%20missing%20entries%20in%20the%20user-item%20rating%20matrix%20enables%20a%0Acomprehensive%20understanding%20of%20user%20preferences.%20However%2C%20current%20graph%20neural%0Anetwork%20%28GNN%29-based%20approaches%20are%20highly%20sensitive%20to%20noisy%20or%20irrelevant%0Aedges--due%20to%20their%20inherent%20message-passing%20mechanisms--and%20are%20prone%20to%0Aoverfitting%2C%20which%20limits%20their%20generalizability.%20To%20overcome%20these%20challenges%2C%0Awe%20propose%20a%20novel%20method%20called%20Matrix%20Completion%20using%20Contrastive%20Learning%0A%28MCCL%29.%20Our%20approach%20begins%20by%20extracting%20local%20neighborhood%20subgraphs%20for%20each%0Ainteraction%20and%20subsequently%20generates%20two%20distinct%20graph%20representations.%20The%0Afirst%20representation%20emphasizes%20denoising%20by%20integrating%20GNN%20layers%20with%20an%0Aattention%20mechanism%2C%20while%20the%20second%20is%20obtained%20via%20a%20graph%20variational%0Aautoencoder%20that%20aligns%20the%20feature%20distribution%20with%20a%20standard%20prior.%20A%0Amutual%20learning%20loss%20function%20is%20employed%20during%20training%20to%20gradually%0Aharmonize%20these%20representations%2C%20enabling%20the%20model%20to%20capture%20common%20patterns%0Aand%20significantly%20enhance%20its%20generalizability.%20Extensive%20experiments%20on%0Aseveral%20real-world%20datasets%20demonstrate%20that%20our%20approach%20not%20only%20improves%20the%0Anumerical%20accuracy%20of%20the%20predicted%20scores--achieving%20up%20to%20a%200.8%25%20improvement%0Ain%20RMSE--but%20also%20produces%20superior%20rankings%20with%20improvements%20of%20up%20to%2036%25%20in%0Aranking%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Matrix%2520Completion%2520with%2520Denoising%2520and%2520Augmented%2520Graph%2520Views%250A%2520%2520for%2520Robust%2520Recommendation%26entry.906535625%3DNarges%2520Nemati%2520and%2520Mostafa%2520Haghir%2520Chehreghani%26entry.1292438233%3D%2520%2520Matrix%2520completion%2520is%2520a%2520widely%2520adopted%2520framework%2520in%2520recommender%2520systems%252C%2520as%250Apredicting%2520the%2520missing%2520entries%2520in%2520the%2520user-item%2520rating%2520matrix%2520enables%2520a%250Acomprehensive%2520understanding%2520of%2520user%2520preferences.%2520However%252C%2520current%2520graph%2520neural%250Anetwork%2520%2528GNN%2529-based%2520approaches%2520are%2520highly%2520sensitive%2520to%2520noisy%2520or%2520irrelevant%250Aedges--due%2520to%2520their%2520inherent%2520message-passing%2520mechanisms--and%2520are%2520prone%2520to%250Aoverfitting%252C%2520which%2520limits%2520their%2520generalizability.%2520To%2520overcome%2520these%2520challenges%252C%250Awe%2520propose%2520a%2520novel%2520method%2520called%2520Matrix%2520Completion%2520using%2520Contrastive%2520Learning%250A%2528MCCL%2529.%2520Our%2520approach%2520begins%2520by%2520extracting%2520local%2520neighborhood%2520subgraphs%2520for%2520each%250Ainteraction%2520and%2520subsequently%2520generates%2520two%2520distinct%2520graph%2520representations.%2520The%250Afirst%2520representation%2520emphasizes%2520denoising%2520by%2520integrating%2520GNN%2520layers%2520with%2520an%250Aattention%2520mechanism%252C%2520while%2520the%2520second%2520is%2520obtained%2520via%2520a%2520graph%2520variational%250Aautoencoder%2520that%2520aligns%2520the%2520feature%2520distribution%2520with%2520a%2520standard%2520prior.%2520A%250Amutual%2520learning%2520loss%2520function%2520is%2520employed%2520during%2520training%2520to%2520gradually%250Aharmonize%2520these%2520representations%252C%2520enabling%2520the%2520model%2520to%2520capture%2520common%2520patterns%250Aand%2520significantly%2520enhance%2520its%2520generalizability.%2520Extensive%2520experiments%2520on%250Aseveral%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520not%2520only%2520improves%2520the%250Anumerical%2520accuracy%2520of%2520the%2520predicted%2520scores--achieving%2520up%2520to%2520a%25200.8%2525%2520improvement%250Ain%2520RMSE--but%2520also%2520produces%2520superior%2520rankings%2520with%2520improvements%2520of%2520up%2520to%252036%2525%2520in%250Aranking%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Matrix%20Completion%20with%20Denoising%20and%20Augmented%20Graph%20Views%0A%20%20for%20Robust%20Recommendation&entry.906535625=Narges%20Nemati%20and%20Mostafa%20Haghir%20Chehreghani&entry.1292438233=%20%20Matrix%20completion%20is%20a%20widely%20adopted%20framework%20in%20recommender%20systems%2C%20as%0Apredicting%20the%20missing%20entries%20in%20the%20user-item%20rating%20matrix%20enables%20a%0Acomprehensive%20understanding%20of%20user%20preferences.%20However%2C%20current%20graph%20neural%0Anetwork%20%28GNN%29-based%20approaches%20are%20highly%20sensitive%20to%20noisy%20or%20irrelevant%0Aedges--due%20to%20their%20inherent%20message-passing%20mechanisms--and%20are%20prone%20to%0Aoverfitting%2C%20which%20limits%20their%20generalizability.%20To%20overcome%20these%20challenges%2C%0Awe%20propose%20a%20novel%20method%20called%20Matrix%20Completion%20using%20Contrastive%20Learning%0A%28MCCL%29.%20Our%20approach%20begins%20by%20extracting%20local%20neighborhood%20subgraphs%20for%20each%0Ainteraction%20and%20subsequently%20generates%20two%20distinct%20graph%20representations.%20The%0Afirst%20representation%20emphasizes%20denoising%20by%20integrating%20GNN%20layers%20with%20an%0Aattention%20mechanism%2C%20while%20the%20second%20is%20obtained%20via%20a%20graph%20variational%0Aautoencoder%20that%20aligns%20the%20feature%20distribution%20with%20a%20standard%20prior.%20A%0Amutual%20learning%20loss%20function%20is%20employed%20during%20training%20to%20gradually%0Aharmonize%20these%20representations%2C%20enabling%20the%20model%20to%20capture%20common%20patterns%0Aand%20significantly%20enhance%20its%20generalizability.%20Extensive%20experiments%20on%0Aseveral%20real-world%20datasets%20demonstrate%20that%20our%20approach%20not%20only%20improves%20the%0Anumerical%20accuracy%20of%20the%20predicted%20scores--achieving%20up%20to%20a%200.8%25%20improvement%0Ain%20RMSE--but%20also%20produces%20superior%20rankings%20with%20improvements%20of%20up%20to%2036%25%20in%0Aranking%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10658v1&entry.124074799=Read"},
{"title": "A Unit Enhancement and Guidance Framework for Audio-Driven Avatar Video\n  Generation", "author": "S. Z. Zhou and Y. B. Wang and J. F. Wu and T. Hu and J. N. Zhang", "abstract": "  Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose Parts-aware\nAudio-driven Human Animation, PAHA, a unit enhancement and guidance framework\nfor audio-driven upper-body animation. We introduce two key methods:\nParts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR\ndynamically adjusts regional training loss weights based on pose confidence\nscores, effectively improving visual quality. PCE constructs and trains\ndiffusion-based regional audio-visual classifiers to improve the consistency of\nmotion and co-speech audio. Afterwards, we design two novel inference guidance\nmethods for the foregoing classifiers, Sequential Guidance (SG) and\nDifferential Guidance (DG), to balance efficiency and quality respectively.\nAdditionally, we build CNAS, the first public Chinese News Anchor Speech\ndataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2505.03603v5", "date": "2025-06-12", "relevancy": 2.4019, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6056}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6022}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unit%20Enhancement%20and%20Guidance%20Framework%20for%20Audio-Driven%20Avatar%20Video%0A%20%20Generation&body=Title%3A%20A%20Unit%20Enhancement%20and%20Guidance%20Framework%20for%20Audio-Driven%20Avatar%20Video%0A%20%20Generation%0AAuthor%3A%20S.%20Z.%20Zhou%20and%20Y.%20B.%20Wang%20and%20J.%20F.%20Wu%20and%20T.%20Hu%20and%20J.%20N.%20Zhang%0AAbstract%3A%20%20%20Audio-driven%20human%20animation%20technology%20is%20widely%20used%20in%20human-computer%0Ainteraction%2C%20and%20the%20emergence%20of%20diffusion%20models%20has%20further%20advanced%20its%0Adevelopment.%20Currently%2C%20most%20methods%20rely%20on%20multi-stage%20generation%20and%0Aintermediate%20representations%2C%20resulting%20in%20long%20inference%20time%20and%20issues%20with%0Ageneration%20quality%20in%20specific%20foreground%20regions%20and%20audio-motion%20consistency.%0AThese%20shortcomings%20are%20primarily%20due%20to%20the%20lack%20of%20localized%20fine-grained%0Asupervised%20guidance.%20To%20address%20above%20challenges%2C%20we%20propose%20Parts-aware%0AAudio-driven%20Human%20Animation%2C%20PAHA%2C%20a%20unit%20enhancement%20and%20guidance%20framework%0Afor%20audio-driven%20upper-body%20animation.%20We%20introduce%20two%20key%20methods%3A%0AParts-Aware%20Re-weighting%20%28PAR%29%20and%20Parts%20Consistency%20Enhancement%20%28PCE%29.%20PAR%0Adynamically%20adjusts%20regional%20training%20loss%20weights%20based%20on%20pose%20confidence%0Ascores%2C%20effectively%20improving%20visual%20quality.%20PCE%20constructs%20and%20trains%0Adiffusion-based%20regional%20audio-visual%20classifiers%20to%20improve%20the%20consistency%20of%0Amotion%20and%20co-speech%20audio.%20Afterwards%2C%20we%20design%20two%20novel%20inference%20guidance%0Amethods%20for%20the%20foregoing%20classifiers%2C%20Sequential%20Guidance%20%28SG%29%20and%0ADifferential%20Guidance%20%28DG%29%2C%20to%20balance%20efficiency%20and%20quality%20respectively.%0AAdditionally%2C%20we%20build%20CNAS%2C%20the%20first%20public%20Chinese%20News%20Anchor%20Speech%0Adataset%2C%20to%20advance%20research%20and%20validation%20in%20this%20field.%20Extensive%0Aexperimental%20results%20and%20user%20studies%20demonstrate%20that%20PAHA%20significantly%0Aoutperforms%20existing%20methods%20in%20audio-motion%20alignment%20and%20video-related%0Aevaluations.%20The%20codes%20and%20CNAS%20dataset%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03603v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unit%2520Enhancement%2520and%2520Guidance%2520Framework%2520for%2520Audio-Driven%2520Avatar%2520Video%250A%2520%2520Generation%26entry.906535625%3DS.%2520Z.%2520Zhou%2520and%2520Y.%2520B.%2520Wang%2520and%2520J.%2520F.%2520Wu%2520and%2520T.%2520Hu%2520and%2520J.%2520N.%2520Zhang%26entry.1292438233%3D%2520%2520Audio-driven%2520human%2520animation%2520technology%2520is%2520widely%2520used%2520in%2520human-computer%250Ainteraction%252C%2520and%2520the%2520emergence%2520of%2520diffusion%2520models%2520has%2520further%2520advanced%2520its%250Adevelopment.%2520Currently%252C%2520most%2520methods%2520rely%2520on%2520multi-stage%2520generation%2520and%250Aintermediate%2520representations%252C%2520resulting%2520in%2520long%2520inference%2520time%2520and%2520issues%2520with%250Ageneration%2520quality%2520in%2520specific%2520foreground%2520regions%2520and%2520audio-motion%2520consistency.%250AThese%2520shortcomings%2520are%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520localized%2520fine-grained%250Asupervised%2520guidance.%2520To%2520address%2520above%2520challenges%252C%2520we%2520propose%2520Parts-aware%250AAudio-driven%2520Human%2520Animation%252C%2520PAHA%252C%2520a%2520unit%2520enhancement%2520and%2520guidance%2520framework%250Afor%2520audio-driven%2520upper-body%2520animation.%2520We%2520introduce%2520two%2520key%2520methods%253A%250AParts-Aware%2520Re-weighting%2520%2528PAR%2529%2520and%2520Parts%2520Consistency%2520Enhancement%2520%2528PCE%2529.%2520PAR%250Adynamically%2520adjusts%2520regional%2520training%2520loss%2520weights%2520based%2520on%2520pose%2520confidence%250Ascores%252C%2520effectively%2520improving%2520visual%2520quality.%2520PCE%2520constructs%2520and%2520trains%250Adiffusion-based%2520regional%2520audio-visual%2520classifiers%2520to%2520improve%2520the%2520consistency%2520of%250Amotion%2520and%2520co-speech%2520audio.%2520Afterwards%252C%2520we%2520design%2520two%2520novel%2520inference%2520guidance%250Amethods%2520for%2520the%2520foregoing%2520classifiers%252C%2520Sequential%2520Guidance%2520%2528SG%2529%2520and%250ADifferential%2520Guidance%2520%2528DG%2529%252C%2520to%2520balance%2520efficiency%2520and%2520quality%2520respectively.%250AAdditionally%252C%2520we%2520build%2520CNAS%252C%2520the%2520first%2520public%2520Chinese%2520News%2520Anchor%2520Speech%250Adataset%252C%2520to%2520advance%2520research%2520and%2520validation%2520in%2520this%2520field.%2520Extensive%250Aexperimental%2520results%2520and%2520user%2520studies%2520demonstrate%2520that%2520PAHA%2520significantly%250Aoutperforms%2520existing%2520methods%2520in%2520audio-motion%2520alignment%2520and%2520video-related%250Aevaluations.%2520The%2520codes%2520and%2520CNAS%2520dataset%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03603v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unit%20Enhancement%20and%20Guidance%20Framework%20for%20Audio-Driven%20Avatar%20Video%0A%20%20Generation&entry.906535625=S.%20Z.%20Zhou%20and%20Y.%20B.%20Wang%20and%20J.%20F.%20Wu%20and%20T.%20Hu%20and%20J.%20N.%20Zhang&entry.1292438233=%20%20Audio-driven%20human%20animation%20technology%20is%20widely%20used%20in%20human-computer%0Ainteraction%2C%20and%20the%20emergence%20of%20diffusion%20models%20has%20further%20advanced%20its%0Adevelopment.%20Currently%2C%20most%20methods%20rely%20on%20multi-stage%20generation%20and%0Aintermediate%20representations%2C%20resulting%20in%20long%20inference%20time%20and%20issues%20with%0Ageneration%20quality%20in%20specific%20foreground%20regions%20and%20audio-motion%20consistency.%0AThese%20shortcomings%20are%20primarily%20due%20to%20the%20lack%20of%20localized%20fine-grained%0Asupervised%20guidance.%20To%20address%20above%20challenges%2C%20we%20propose%20Parts-aware%0AAudio-driven%20Human%20Animation%2C%20PAHA%2C%20a%20unit%20enhancement%20and%20guidance%20framework%0Afor%20audio-driven%20upper-body%20animation.%20We%20introduce%20two%20key%20methods%3A%0AParts-Aware%20Re-weighting%20%28PAR%29%20and%20Parts%20Consistency%20Enhancement%20%28PCE%29.%20PAR%0Adynamically%20adjusts%20regional%20training%20loss%20weights%20based%20on%20pose%20confidence%0Ascores%2C%20effectively%20improving%20visual%20quality.%20PCE%20constructs%20and%20trains%0Adiffusion-based%20regional%20audio-visual%20classifiers%20to%20improve%20the%20consistency%20of%0Amotion%20and%20co-speech%20audio.%20Afterwards%2C%20we%20design%20two%20novel%20inference%20guidance%0Amethods%20for%20the%20foregoing%20classifiers%2C%20Sequential%20Guidance%20%28SG%29%20and%0ADifferential%20Guidance%20%28DG%29%2C%20to%20balance%20efficiency%20and%20quality%20respectively.%0AAdditionally%2C%20we%20build%20CNAS%2C%20the%20first%20public%20Chinese%20News%20Anchor%20Speech%0Adataset%2C%20to%20advance%20research%20and%20validation%20in%20this%20field.%20Extensive%0Aexperimental%20results%20and%20user%20studies%20demonstrate%20that%20PAHA%20significantly%0Aoutperforms%20existing%20methods%20in%20audio-motion%20alignment%20and%20video-related%0Aevaluations.%20The%20codes%20and%20CNAS%20dataset%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03603v5&entry.124074799=Read"},
{"title": "TDS-CLIP: Temporal Difference Side Network for Efficient VideoAction\n  Recognition", "author": "Bin Wang and Wentong Li and Wenqian Wang and Mingliang Gao and Runmin Cong and Wei Zhang", "abstract": "  Recently, large-scale pre-trained vision-language models (e.g., CLIP), have\ngarnered significant attention thanks to their powerful representative\ncapabilities. This inspires researchers in transferring the knowledge from\nthese large pre-trained models to other task-specific models, e.g., Video\nAction Recognition (VAR) models, via particularly leveraging side networks to\nenhance the efficiency of parameter-efficient fine-tuning (PEFT). However,\ncurrent transferring approaches in VAR tend to directly transfer the frozen\nknowledge from large pre-trained models to action recognition networks with\nminimal cost, instead of exploiting the temporal modeling capabilities of the\naction recognition models themselves. Therefore, in this paper, we propose a\nnovel memory-efficient Temporal Difference Side Network (TDS-CLIP) to balance\nknowledge transferring and temporal modeling, avoiding backpropagation in\nfrozen parameter models. Specifically, we introduce a Temporal Difference\nAdapter (TD-Adapter), which can effectively capture local temporal differences\nin motion features to strengthen the model's global temporal modeling\ncapabilities. Furthermore, we designed a Side Motion Enhancement Adapter\n(SME-Adapter) to guide the proposed side network in efficiently learning the\nrich motion information in videos, thereby improving the side network's ability\nto capture and learn motion information. Extensive experiments are conducted on\nthree benchmark datasets, including Something-Something V1&V2, and\nKinetics-400. Experimental results show that our method achieves competitive\nperformance in video action recognition tasks.\n", "link": "http://arxiv.org/abs/2408.10688v2", "date": "2025-06-12", "relevancy": 2.3996, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6258}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.601}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TDS-CLIP%3A%20Temporal%20Difference%20Side%20Network%20for%20Efficient%20VideoAction%0A%20%20Recognition&body=Title%3A%20TDS-CLIP%3A%20Temporal%20Difference%20Side%20Network%20for%20Efficient%20VideoAction%0A%20%20Recognition%0AAuthor%3A%20Bin%20Wang%20and%20Wentong%20Li%20and%20Wenqian%20Wang%20and%20Mingliang%20Gao%20and%20Runmin%20Cong%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20large-scale%20pre-trained%20vision-language%20models%20%28e.g.%2C%20CLIP%29%2C%20have%0Agarnered%20significant%20attention%20thanks%20to%20their%20powerful%20representative%0Acapabilities.%20This%20inspires%20researchers%20in%20transferring%20the%20knowledge%20from%0Athese%20large%20pre-trained%20models%20to%20other%20task-specific%20models%2C%20e.g.%2C%20Video%0AAction%20Recognition%20%28VAR%29%20models%2C%20via%20particularly%20leveraging%20side%20networks%20to%0Aenhance%20the%20efficiency%20of%20parameter-efficient%20fine-tuning%20%28PEFT%29.%20However%2C%0Acurrent%20transferring%20approaches%20in%20VAR%20tend%20to%20directly%20transfer%20the%20frozen%0Aknowledge%20from%20large%20pre-trained%20models%20to%20action%20recognition%20networks%20with%0Aminimal%20cost%2C%20instead%20of%20exploiting%20the%20temporal%20modeling%20capabilities%20of%20the%0Aaction%20recognition%20models%20themselves.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20a%0Anovel%20memory-efficient%20Temporal%20Difference%20Side%20Network%20%28TDS-CLIP%29%20to%20balance%0Aknowledge%20transferring%20and%20temporal%20modeling%2C%20avoiding%20backpropagation%20in%0Afrozen%20parameter%20models.%20Specifically%2C%20we%20introduce%20a%20Temporal%20Difference%0AAdapter%20%28TD-Adapter%29%2C%20which%20can%20effectively%20capture%20local%20temporal%20differences%0Ain%20motion%20features%20to%20strengthen%20the%20model%27s%20global%20temporal%20modeling%0Acapabilities.%20Furthermore%2C%20we%20designed%20a%20Side%20Motion%20Enhancement%20Adapter%0A%28SME-Adapter%29%20to%20guide%20the%20proposed%20side%20network%20in%20efficiently%20learning%20the%0Arich%20motion%20information%20in%20videos%2C%20thereby%20improving%20the%20side%20network%27s%20ability%0Ato%20capture%20and%20learn%20motion%20information.%20Extensive%20experiments%20are%20conducted%20on%0Athree%20benchmark%20datasets%2C%20including%20Something-Something%20V1%26V2%2C%20and%0AKinetics-400.%20Experimental%20results%20show%20that%20our%20method%20achieves%20competitive%0Aperformance%20in%20video%20action%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTDS-CLIP%253A%2520Temporal%2520Difference%2520Side%2520Network%2520for%2520Efficient%2520VideoAction%250A%2520%2520Recognition%26entry.906535625%3DBin%2520Wang%2520and%2520Wentong%2520Li%2520and%2520Wenqian%2520Wang%2520and%2520Mingliang%2520Gao%2520and%2520Runmin%2520Cong%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520large-scale%2520pre-trained%2520vision-language%2520models%2520%2528e.g.%252C%2520CLIP%2529%252C%2520have%250Agarnered%2520significant%2520attention%2520thanks%2520to%2520their%2520powerful%2520representative%250Acapabilities.%2520This%2520inspires%2520researchers%2520in%2520transferring%2520the%2520knowledge%2520from%250Athese%2520large%2520pre-trained%2520models%2520to%2520other%2520task-specific%2520models%252C%2520e.g.%252C%2520Video%250AAction%2520Recognition%2520%2528VAR%2529%2520models%252C%2520via%2520particularly%2520leveraging%2520side%2520networks%2520to%250Aenhance%2520the%2520efficiency%2520of%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529.%2520However%252C%250Acurrent%2520transferring%2520approaches%2520in%2520VAR%2520tend%2520to%2520directly%2520transfer%2520the%2520frozen%250Aknowledge%2520from%2520large%2520pre-trained%2520models%2520to%2520action%2520recognition%2520networks%2520with%250Aminimal%2520cost%252C%2520instead%2520of%2520exploiting%2520the%2520temporal%2520modeling%2520capabilities%2520of%2520the%250Aaction%2520recognition%2520models%2520themselves.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520memory-efficient%2520Temporal%2520Difference%2520Side%2520Network%2520%2528TDS-CLIP%2529%2520to%2520balance%250Aknowledge%2520transferring%2520and%2520temporal%2520modeling%252C%2520avoiding%2520backpropagation%2520in%250Afrozen%2520parameter%2520models.%2520Specifically%252C%2520we%2520introduce%2520a%2520Temporal%2520Difference%250AAdapter%2520%2528TD-Adapter%2529%252C%2520which%2520can%2520effectively%2520capture%2520local%2520temporal%2520differences%250Ain%2520motion%2520features%2520to%2520strengthen%2520the%2520model%2527s%2520global%2520temporal%2520modeling%250Acapabilities.%2520Furthermore%252C%2520we%2520designed%2520a%2520Side%2520Motion%2520Enhancement%2520Adapter%250A%2528SME-Adapter%2529%2520to%2520guide%2520the%2520proposed%2520side%2520network%2520in%2520efficiently%2520learning%2520the%250Arich%2520motion%2520information%2520in%2520videos%252C%2520thereby%2520improving%2520the%2520side%2520network%2527s%2520ability%250Ato%2520capture%2520and%2520learn%2520motion%2520information.%2520Extensive%2520experiments%2520are%2520conducted%2520on%250Athree%2520benchmark%2520datasets%252C%2520including%2520Something-Something%2520V1%2526V2%252C%2520and%250AKinetics-400.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520achieves%2520competitive%250Aperformance%2520in%2520video%2520action%2520recognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TDS-CLIP%3A%20Temporal%20Difference%20Side%20Network%20for%20Efficient%20VideoAction%0A%20%20Recognition&entry.906535625=Bin%20Wang%20and%20Wentong%20Li%20and%20Wenqian%20Wang%20and%20Mingliang%20Gao%20and%20Runmin%20Cong%20and%20Wei%20Zhang&entry.1292438233=%20%20Recently%2C%20large-scale%20pre-trained%20vision-language%20models%20%28e.g.%2C%20CLIP%29%2C%20have%0Agarnered%20significant%20attention%20thanks%20to%20their%20powerful%20representative%0Acapabilities.%20This%20inspires%20researchers%20in%20transferring%20the%20knowledge%20from%0Athese%20large%20pre-trained%20models%20to%20other%20task-specific%20models%2C%20e.g.%2C%20Video%0AAction%20Recognition%20%28VAR%29%20models%2C%20via%20particularly%20leveraging%20side%20networks%20to%0Aenhance%20the%20efficiency%20of%20parameter-efficient%20fine-tuning%20%28PEFT%29.%20However%2C%0Acurrent%20transferring%20approaches%20in%20VAR%20tend%20to%20directly%20transfer%20the%20frozen%0Aknowledge%20from%20large%20pre-trained%20models%20to%20action%20recognition%20networks%20with%0Aminimal%20cost%2C%20instead%20of%20exploiting%20the%20temporal%20modeling%20capabilities%20of%20the%0Aaction%20recognition%20models%20themselves.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20a%0Anovel%20memory-efficient%20Temporal%20Difference%20Side%20Network%20%28TDS-CLIP%29%20to%20balance%0Aknowledge%20transferring%20and%20temporal%20modeling%2C%20avoiding%20backpropagation%20in%0Afrozen%20parameter%20models.%20Specifically%2C%20we%20introduce%20a%20Temporal%20Difference%0AAdapter%20%28TD-Adapter%29%2C%20which%20can%20effectively%20capture%20local%20temporal%20differences%0Ain%20motion%20features%20to%20strengthen%20the%20model%27s%20global%20temporal%20modeling%0Acapabilities.%20Furthermore%2C%20we%20designed%20a%20Side%20Motion%20Enhancement%20Adapter%0A%28SME-Adapter%29%20to%20guide%20the%20proposed%20side%20network%20in%20efficiently%20learning%20the%0Arich%20motion%20information%20in%20videos%2C%20thereby%20improving%20the%20side%20network%27s%20ability%0Ato%20capture%20and%20learn%20motion%20information.%20Extensive%20experiments%20are%20conducted%20on%0Athree%20benchmark%20datasets%2C%20including%20Something-Something%20V1%26V2%2C%20and%0AKinetics-400.%20Experimental%20results%20show%20that%20our%20method%20achieves%20competitive%0Aperformance%20in%20video%20action%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10688v2&entry.124074799=Read"},
{"title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following\n  Manipulation", "author": "Ning Gao and Yilun Chen and Shuai Yang and Xinyi Chen and Yang Tian and Hao Li and Haifeng Huang and Hanqing Wang and Tai Wang and Jiangmiao Pang", "abstract": "  Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/.\n", "link": "http://arxiv.org/abs/2506.10966v1", "date": "2025-06-12", "relevancy": 2.3654, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6174}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5933}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GENMANIP%3A%20LLM-driven%20Simulation%20for%20Generalizable%20Instruction-Following%0A%20%20Manipulation&body=Title%3A%20GENMANIP%3A%20LLM-driven%20Simulation%20for%20Generalizable%20Instruction-Following%0A%20%20Manipulation%0AAuthor%3A%20Ning%20Gao%20and%20Yilun%20Chen%20and%20Shuai%20Yang%20and%20Xinyi%20Chen%20and%20Yang%20Tian%20and%20Hao%20Li%20and%20Haifeng%20Huang%20and%20Hanqing%20Wang%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Robotic%20manipulation%20in%20real-world%20settings%20remains%20challenging%2C%20especially%0Aregarding%20robust%20generalization.%20Existing%20simulation%20platforms%20lack%20sufficient%0Asupport%20for%20exploring%20how%20policies%20adapt%20to%20varied%20instructions%20and%20scenarios.%0AThus%2C%20they%20lag%20behind%20the%20growing%20interest%20in%20instruction-following%20foundation%0Amodels%20like%20LLMs%2C%20whose%20adaptability%20is%20crucial%20yet%20remains%20underexplored%20in%0Afair%20comparisons.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GenManip%2C%20a%20realistic%0Atabletop%20simulation%20platform%20tailored%20for%20policy%20generalization%20studies.%20It%0Afeatures%20an%20automatic%20pipeline%20via%20LLM-driven%20task-oriented%20scene%20graph%20to%0Asynthesize%20large-scale%2C%20diverse%20tasks%20using%2010K%20annotated%203D%20object%20assets.%20To%0Asystematically%20assess%20generalization%2C%20we%20present%20GenManip-Bench%2C%20a%20benchmark%20of%0A200%20scenarios%20refined%20via%20human-in-the-loop%20corrections.%20We%20evaluate%20two%20policy%0Atypes%3A%20%281%29%20modular%20manipulation%20systems%20integrating%20foundation%20models%20for%0Aperception%2C%20reasoning%2C%20and%20planning%2C%20and%20%282%29%20end-to-end%20policies%20trained%0Athrough%20scalable%20data%20collection.%20Results%20show%20that%20while%20data%20scaling%20benefits%0Aend-to-end%20methods%2C%20modular%20systems%20enhanced%20with%20foundation%20models%20generalize%0Amore%20effectively%20across%20diverse%20scenarios.%20We%20anticipate%20this%20platform%20to%0Afacilitate%20critical%20insights%20for%20advancing%20policy%20generalization%20in%20realistic%0Aconditions.%20Project%20Page%3A%20https%3A//genmanip.axi404.top/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGENMANIP%253A%2520LLM-driven%2520Simulation%2520for%2520Generalizable%2520Instruction-Following%250A%2520%2520Manipulation%26entry.906535625%3DNing%2520Gao%2520and%2520Yilun%2520Chen%2520and%2520Shuai%2520Yang%2520and%2520Xinyi%2520Chen%2520and%2520Yang%2520Tian%2520and%2520Hao%2520Li%2520and%2520Haifeng%2520Huang%2520and%2520Hanqing%2520Wang%2520and%2520Tai%2520Wang%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Robotic%2520manipulation%2520in%2520real-world%2520settings%2520remains%2520challenging%252C%2520especially%250Aregarding%2520robust%2520generalization.%2520Existing%2520simulation%2520platforms%2520lack%2520sufficient%250Asupport%2520for%2520exploring%2520how%2520policies%2520adapt%2520to%2520varied%2520instructions%2520and%2520scenarios.%250AThus%252C%2520they%2520lag%2520behind%2520the%2520growing%2520interest%2520in%2520instruction-following%2520foundation%250Amodels%2520like%2520LLMs%252C%2520whose%2520adaptability%2520is%2520crucial%2520yet%2520remains%2520underexplored%2520in%250Afair%2520comparisons.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520GenManip%252C%2520a%2520realistic%250Atabletop%2520simulation%2520platform%2520tailored%2520for%2520policy%2520generalization%2520studies.%2520It%250Afeatures%2520an%2520automatic%2520pipeline%2520via%2520LLM-driven%2520task-oriented%2520scene%2520graph%2520to%250Asynthesize%2520large-scale%252C%2520diverse%2520tasks%2520using%252010K%2520annotated%25203D%2520object%2520assets.%2520To%250Asystematically%2520assess%2520generalization%252C%2520we%2520present%2520GenManip-Bench%252C%2520a%2520benchmark%2520of%250A200%2520scenarios%2520refined%2520via%2520human-in-the-loop%2520corrections.%2520We%2520evaluate%2520two%2520policy%250Atypes%253A%2520%25281%2529%2520modular%2520manipulation%2520systems%2520integrating%2520foundation%2520models%2520for%250Aperception%252C%2520reasoning%252C%2520and%2520planning%252C%2520and%2520%25282%2529%2520end-to-end%2520policies%2520trained%250Athrough%2520scalable%2520data%2520collection.%2520Results%2520show%2520that%2520while%2520data%2520scaling%2520benefits%250Aend-to-end%2520methods%252C%2520modular%2520systems%2520enhanced%2520with%2520foundation%2520models%2520generalize%250Amore%2520effectively%2520across%2520diverse%2520scenarios.%2520We%2520anticipate%2520this%2520platform%2520to%250Afacilitate%2520critical%2520insights%2520for%2520advancing%2520policy%2520generalization%2520in%2520realistic%250Aconditions.%2520Project%2520Page%253A%2520https%253A//genmanip.axi404.top/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GENMANIP%3A%20LLM-driven%20Simulation%20for%20Generalizable%20Instruction-Following%0A%20%20Manipulation&entry.906535625=Ning%20Gao%20and%20Yilun%20Chen%20and%20Shuai%20Yang%20and%20Xinyi%20Chen%20and%20Yang%20Tian%20and%20Hao%20Li%20and%20Haifeng%20Huang%20and%20Hanqing%20Wang%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Robotic%20manipulation%20in%20real-world%20settings%20remains%20challenging%2C%20especially%0Aregarding%20robust%20generalization.%20Existing%20simulation%20platforms%20lack%20sufficient%0Asupport%20for%20exploring%20how%20policies%20adapt%20to%20varied%20instructions%20and%20scenarios.%0AThus%2C%20they%20lag%20behind%20the%20growing%20interest%20in%20instruction-following%20foundation%0Amodels%20like%20LLMs%2C%20whose%20adaptability%20is%20crucial%20yet%20remains%20underexplored%20in%0Afair%20comparisons.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GenManip%2C%20a%20realistic%0Atabletop%20simulation%20platform%20tailored%20for%20policy%20generalization%20studies.%20It%0Afeatures%20an%20automatic%20pipeline%20via%20LLM-driven%20task-oriented%20scene%20graph%20to%0Asynthesize%20large-scale%2C%20diverse%20tasks%20using%2010K%20annotated%203D%20object%20assets.%20To%0Asystematically%20assess%20generalization%2C%20we%20present%20GenManip-Bench%2C%20a%20benchmark%20of%0A200%20scenarios%20refined%20via%20human-in-the-loop%20corrections.%20We%20evaluate%20two%20policy%0Atypes%3A%20%281%29%20modular%20manipulation%20systems%20integrating%20foundation%20models%20for%0Aperception%2C%20reasoning%2C%20and%20planning%2C%20and%20%282%29%20end-to-end%20policies%20trained%0Athrough%20scalable%20data%20collection.%20Results%20show%20that%20while%20data%20scaling%20benefits%0Aend-to-end%20methods%2C%20modular%20systems%20enhanced%20with%20foundation%20models%20generalize%0Amore%20effectively%20across%20diverse%20scenarios.%20We%20anticipate%20this%20platform%20to%0Afacilitate%20critical%20insights%20for%20advancing%20policy%20generalization%20in%20realistic%0Aconditions.%20Project%20Page%3A%20https%3A//genmanip.axi404.top/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10966v1&entry.124074799=Read"},
{"title": "In-Hand Object Pose Estimation via Visual-Tactile Fusion", "author": "Felix Nonnengie\u00dfer and Alap Kshirsagar and Boris Belousov and Jan Peters", "abstract": "  Accurate in-hand pose estimation is crucial for robotic object manipulation,\nbut visual occlusion remains a major challenge for vision-based approaches.\nThis paper presents an approach to robotic in-hand object pose estimation,\ncombining visual and tactile information to accurately determine the position\nand orientation of objects grasped by a robotic hand. We address the challenge\nof visual occlusion by fusing visual information from a wrist-mounted RGB-D\ncamera with tactile information from vision-based tactile sensors mounted on\nthe fingertips of a robotic gripper. Our approach employs a weighting and\nsensor fusion module to combine point clouds from heterogeneous sensor types\nand control each modality's contribution to the pose estimation process. We use\nan augmented Iterative Closest Point (ICP) algorithm adapted for weighted point\nclouds to estimate the 6D object pose. Our experiments show that incorporating\ntactile information significantly improves pose estimation accuracy,\nparticularly when occlusion is high. Our method achieves an average pose\nestimation error of 7.5 mm and 16.7 degrees, outperforming vision-only\nbaselines by up to 20%. We also demonstrate the ability of our method to\nperform precise object manipulation in a real-world insertion task.\n", "link": "http://arxiv.org/abs/2506.10787v1", "date": "2025-06-12", "relevancy": 2.356, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5995}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5939}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Hand%20Object%20Pose%20Estimation%20via%20Visual-Tactile%20Fusion&body=Title%3A%20In-Hand%20Object%20Pose%20Estimation%20via%20Visual-Tactile%20Fusion%0AAuthor%3A%20Felix%20Nonnengie%C3%9Fer%20and%20Alap%20Kshirsagar%20and%20Boris%20Belousov%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Accurate%20in-hand%20pose%20estimation%20is%20crucial%20for%20robotic%20object%20manipulation%2C%0Abut%20visual%20occlusion%20remains%20a%20major%20challenge%20for%20vision-based%20approaches.%0AThis%20paper%20presents%20an%20approach%20to%20robotic%20in-hand%20object%20pose%20estimation%2C%0Acombining%20visual%20and%20tactile%20information%20to%20accurately%20determine%20the%20position%0Aand%20orientation%20of%20objects%20grasped%20by%20a%20robotic%20hand.%20We%20address%20the%20challenge%0Aof%20visual%20occlusion%20by%20fusing%20visual%20information%20from%20a%20wrist-mounted%20RGB-D%0Acamera%20with%20tactile%20information%20from%20vision-based%20tactile%20sensors%20mounted%20on%0Athe%20fingertips%20of%20a%20robotic%20gripper.%20Our%20approach%20employs%20a%20weighting%20and%0Asensor%20fusion%20module%20to%20combine%20point%20clouds%20from%20heterogeneous%20sensor%20types%0Aand%20control%20each%20modality%27s%20contribution%20to%20the%20pose%20estimation%20process.%20We%20use%0Aan%20augmented%20Iterative%20Closest%20Point%20%28ICP%29%20algorithm%20adapted%20for%20weighted%20point%0Aclouds%20to%20estimate%20the%206D%20object%20pose.%20Our%20experiments%20show%20that%20incorporating%0Atactile%20information%20significantly%20improves%20pose%20estimation%20accuracy%2C%0Aparticularly%20when%20occlusion%20is%20high.%20Our%20method%20achieves%20an%20average%20pose%0Aestimation%20error%20of%207.5%20mm%20and%2016.7%20degrees%2C%20outperforming%20vision-only%0Abaselines%20by%20up%20to%2020%25.%20We%20also%20demonstrate%20the%20ability%20of%20our%20method%20to%0Aperform%20precise%20object%20manipulation%20in%20a%20real-world%20insertion%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Hand%2520Object%2520Pose%2520Estimation%2520via%2520Visual-Tactile%2520Fusion%26entry.906535625%3DFelix%2520Nonnengie%25C3%259Fer%2520and%2520Alap%2520Kshirsagar%2520and%2520Boris%2520Belousov%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Accurate%2520in-hand%2520pose%2520estimation%2520is%2520crucial%2520for%2520robotic%2520object%2520manipulation%252C%250Abut%2520visual%2520occlusion%2520remains%2520a%2520major%2520challenge%2520for%2520vision-based%2520approaches.%250AThis%2520paper%2520presents%2520an%2520approach%2520to%2520robotic%2520in-hand%2520object%2520pose%2520estimation%252C%250Acombining%2520visual%2520and%2520tactile%2520information%2520to%2520accurately%2520determine%2520the%2520position%250Aand%2520orientation%2520of%2520objects%2520grasped%2520by%2520a%2520robotic%2520hand.%2520We%2520address%2520the%2520challenge%250Aof%2520visual%2520occlusion%2520by%2520fusing%2520visual%2520information%2520from%2520a%2520wrist-mounted%2520RGB-D%250Acamera%2520with%2520tactile%2520information%2520from%2520vision-based%2520tactile%2520sensors%2520mounted%2520on%250Athe%2520fingertips%2520of%2520a%2520robotic%2520gripper.%2520Our%2520approach%2520employs%2520a%2520weighting%2520and%250Asensor%2520fusion%2520module%2520to%2520combine%2520point%2520clouds%2520from%2520heterogeneous%2520sensor%2520types%250Aand%2520control%2520each%2520modality%2527s%2520contribution%2520to%2520the%2520pose%2520estimation%2520process.%2520We%2520use%250Aan%2520augmented%2520Iterative%2520Closest%2520Point%2520%2528ICP%2529%2520algorithm%2520adapted%2520for%2520weighted%2520point%250Aclouds%2520to%2520estimate%2520the%25206D%2520object%2520pose.%2520Our%2520experiments%2520show%2520that%2520incorporating%250Atactile%2520information%2520significantly%2520improves%2520pose%2520estimation%2520accuracy%252C%250Aparticularly%2520when%2520occlusion%2520is%2520high.%2520Our%2520method%2520achieves%2520an%2520average%2520pose%250Aestimation%2520error%2520of%25207.5%2520mm%2520and%252016.7%2520degrees%252C%2520outperforming%2520vision-only%250Abaselines%2520by%2520up%2520to%252020%2525.%2520We%2520also%2520demonstrate%2520the%2520ability%2520of%2520our%2520method%2520to%250Aperform%2520precise%2520object%2520manipulation%2520in%2520a%2520real-world%2520insertion%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Hand%20Object%20Pose%20Estimation%20via%20Visual-Tactile%20Fusion&entry.906535625=Felix%20Nonnengie%C3%9Fer%20and%20Alap%20Kshirsagar%20and%20Boris%20Belousov%20and%20Jan%20Peters&entry.1292438233=%20%20Accurate%20in-hand%20pose%20estimation%20is%20crucial%20for%20robotic%20object%20manipulation%2C%0Abut%20visual%20occlusion%20remains%20a%20major%20challenge%20for%20vision-based%20approaches.%0AThis%20paper%20presents%20an%20approach%20to%20robotic%20in-hand%20object%20pose%20estimation%2C%0Acombining%20visual%20and%20tactile%20information%20to%20accurately%20determine%20the%20position%0Aand%20orientation%20of%20objects%20grasped%20by%20a%20robotic%20hand.%20We%20address%20the%20challenge%0Aof%20visual%20occlusion%20by%20fusing%20visual%20information%20from%20a%20wrist-mounted%20RGB-D%0Acamera%20with%20tactile%20information%20from%20vision-based%20tactile%20sensors%20mounted%20on%0Athe%20fingertips%20of%20a%20robotic%20gripper.%20Our%20approach%20employs%20a%20weighting%20and%0Asensor%20fusion%20module%20to%20combine%20point%20clouds%20from%20heterogeneous%20sensor%20types%0Aand%20control%20each%20modality%27s%20contribution%20to%20the%20pose%20estimation%20process.%20We%20use%0Aan%20augmented%20Iterative%20Closest%20Point%20%28ICP%29%20algorithm%20adapted%20for%20weighted%20point%0Aclouds%20to%20estimate%20the%206D%20object%20pose.%20Our%20experiments%20show%20that%20incorporating%0Atactile%20information%20significantly%20improves%20pose%20estimation%20accuracy%2C%0Aparticularly%20when%20occlusion%20is%20high.%20Our%20method%20achieves%20an%20average%20pose%0Aestimation%20error%20of%207.5%20mm%20and%2016.7%20degrees%2C%20outperforming%20vision-only%0Abaselines%20by%20up%20to%2020%25.%20We%20also%20demonstrate%20the%20ability%20of%20our%20method%20to%0Aperform%20precise%20object%20manipulation%20in%20a%20real-world%20insertion%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10787v1&entry.124074799=Read"},
{"title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical\n  Problem Solving", "author": "Vincenzo Colle and Mohamed Sana and Nicola Piovesan and Antonio De Domenico and Fadhel Ayed and Merouane Debbah", "abstract": "  The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.\n", "link": "http://arxiv.org/abs/2506.10674v1", "date": "2025-06-12", "relevancy": 2.3494, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeleMath%3A%20A%20Benchmark%20for%20Large%20Language%20Models%20in%20Telecom%20Mathematical%0A%20%20Problem%20Solving&body=Title%3A%20TeleMath%3A%20A%20Benchmark%20for%20Large%20Language%20Models%20in%20Telecom%20Mathematical%0A%20%20Problem%20Solving%0AAuthor%3A%20Vincenzo%20Colle%20and%20Mohamed%20Sana%20and%20Nicola%20Piovesan%20and%20Antonio%20De%20Domenico%20and%20Fadhel%20Ayed%20and%20Merouane%20Debbah%0AAbstract%3A%20%20%20The%20increasing%20adoption%20of%20artificial%20intelligence%20in%20telecommunications%20has%0Araised%20interest%20in%20the%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20address%0Adomain-specific%2C%20mathematically%20intensive%20tasks.%20Although%20recent%20advancements%0Ahave%20improved%20the%20performance%20of%20LLMs%20in%20general%20mathematical%20reasoning%2C%20their%0Aeffectiveness%20within%20specialized%20domains%2C%20such%20as%20signal%20processing%2C%20network%0Aoptimization%2C%20and%20performance%20analysis%2C%20remains%20largely%20unexplored.%20To%20address%0Athis%20gap%2C%20we%20introduce%20TeleMath%2C%20the%20first%20benchmark%20dataset%20specifically%0Adesigned%20to%20evaluate%20LLM%20performance%20in%20solving%20mathematical%20problems%20with%0Anumerical%20solutions%20in%20the%20telecommunications%20domain.%20Comprising%20500%0Aquestion-answer%20%28QnA%29%20pairs%2C%20TeleMath%20covers%20a%20wide%20spectrum%20of%20topics%20in%20the%0Atelecommunications%20field.%20This%20paper%20outlines%20the%20proposed%20QnAs%20generation%0Apipeline%2C%20starting%20from%20a%20selected%20seed%20of%20problems%20crafted%20by%20Subject%20Matter%0AExperts.%20The%20evaluation%20of%20a%20wide%20range%20of%20open-source%20LLMs%20reveals%20that%20best%0Aperformance%20on%20TeleMath%20is%20achieved%20by%20recent%20models%20explicitly%20designed%20for%0Amathematical%20or%20logical%20reasoning.%20In%20contrast%2C%20general-purpose%20models%2C%20even%0Athose%20with%20a%20large%20number%20of%20parameters%2C%20often%20struggle%20with%20these%20challenges.%0AWe%20have%20released%20the%20dataset%20and%20the%20evaluation%20code%20to%20ease%20result%0Areproducibility%20and%20support%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeleMath%253A%2520A%2520Benchmark%2520for%2520Large%2520Language%2520Models%2520in%2520Telecom%2520Mathematical%250A%2520%2520Problem%2520Solving%26entry.906535625%3DVincenzo%2520Colle%2520and%2520Mohamed%2520Sana%2520and%2520Nicola%2520Piovesan%2520and%2520Antonio%2520De%2520Domenico%2520and%2520Fadhel%2520Ayed%2520and%2520Merouane%2520Debbah%26entry.1292438233%3D%2520%2520The%2520increasing%2520adoption%2520of%2520artificial%2520intelligence%2520in%2520telecommunications%2520has%250Araised%2520interest%2520in%2520the%2520capability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520address%250Adomain-specific%252C%2520mathematically%2520intensive%2520tasks.%2520Although%2520recent%2520advancements%250Ahave%2520improved%2520the%2520performance%2520of%2520LLMs%2520in%2520general%2520mathematical%2520reasoning%252C%2520their%250Aeffectiveness%2520within%2520specialized%2520domains%252C%2520such%2520as%2520signal%2520processing%252C%2520network%250Aoptimization%252C%2520and%2520performance%2520analysis%252C%2520remains%2520largely%2520unexplored.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520TeleMath%252C%2520the%2520first%2520benchmark%2520dataset%2520specifically%250Adesigned%2520to%2520evaluate%2520LLM%2520performance%2520in%2520solving%2520mathematical%2520problems%2520with%250Anumerical%2520solutions%2520in%2520the%2520telecommunications%2520domain.%2520Comprising%2520500%250Aquestion-answer%2520%2528QnA%2529%2520pairs%252C%2520TeleMath%2520covers%2520a%2520wide%2520spectrum%2520of%2520topics%2520in%2520the%250Atelecommunications%2520field.%2520This%2520paper%2520outlines%2520the%2520proposed%2520QnAs%2520generation%250Apipeline%252C%2520starting%2520from%2520a%2520selected%2520seed%2520of%2520problems%2520crafted%2520by%2520Subject%2520Matter%250AExperts.%2520The%2520evaluation%2520of%2520a%2520wide%2520range%2520of%2520open-source%2520LLMs%2520reveals%2520that%2520best%250Aperformance%2520on%2520TeleMath%2520is%2520achieved%2520by%2520recent%2520models%2520explicitly%2520designed%2520for%250Amathematical%2520or%2520logical%2520reasoning.%2520In%2520contrast%252C%2520general-purpose%2520models%252C%2520even%250Athose%2520with%2520a%2520large%2520number%2520of%2520parameters%252C%2520often%2520struggle%2520with%2520these%2520challenges.%250AWe%2520have%2520released%2520the%2520dataset%2520and%2520the%2520evaluation%2520code%2520to%2520ease%2520result%250Areproducibility%2520and%2520support%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeleMath%3A%20A%20Benchmark%20for%20Large%20Language%20Models%20in%20Telecom%20Mathematical%0A%20%20Problem%20Solving&entry.906535625=Vincenzo%20Colle%20and%20Mohamed%20Sana%20and%20Nicola%20Piovesan%20and%20Antonio%20De%20Domenico%20and%20Fadhel%20Ayed%20and%20Merouane%20Debbah&entry.1292438233=%20%20The%20increasing%20adoption%20of%20artificial%20intelligence%20in%20telecommunications%20has%0Araised%20interest%20in%20the%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20address%0Adomain-specific%2C%20mathematically%20intensive%20tasks.%20Although%20recent%20advancements%0Ahave%20improved%20the%20performance%20of%20LLMs%20in%20general%20mathematical%20reasoning%2C%20their%0Aeffectiveness%20within%20specialized%20domains%2C%20such%20as%20signal%20processing%2C%20network%0Aoptimization%2C%20and%20performance%20analysis%2C%20remains%20largely%20unexplored.%20To%20address%0Athis%20gap%2C%20we%20introduce%20TeleMath%2C%20the%20first%20benchmark%20dataset%20specifically%0Adesigned%20to%20evaluate%20LLM%20performance%20in%20solving%20mathematical%20problems%20with%0Anumerical%20solutions%20in%20the%20telecommunications%20domain.%20Comprising%20500%0Aquestion-answer%20%28QnA%29%20pairs%2C%20TeleMath%20covers%20a%20wide%20spectrum%20of%20topics%20in%20the%0Atelecommunications%20field.%20This%20paper%20outlines%20the%20proposed%20QnAs%20generation%0Apipeline%2C%20starting%20from%20a%20selected%20seed%20of%20problems%20crafted%20by%20Subject%20Matter%0AExperts.%20The%20evaluation%20of%20a%20wide%20range%20of%20open-source%20LLMs%20reveals%20that%20best%0Aperformance%20on%20TeleMath%20is%20achieved%20by%20recent%20models%20explicitly%20designed%20for%0Amathematical%20or%20logical%20reasoning.%20In%20contrast%2C%20general-purpose%20models%2C%20even%0Athose%20with%20a%20large%20number%20of%20parameters%2C%20often%20struggle%20with%20these%20challenges.%0AWe%20have%20released%20the%20dataset%20and%20the%20evaluation%20code%20to%20ease%20result%0Areproducibility%20and%20support%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10674v1&entry.124074799=Read"},
{"title": "Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality\n  Assessment", "author": "Zhuoxuan Cai and Jian Zhang and Xinbin Yuan and Peng-Tao Jiang and Wenxiang Chen and Bowen Tang and Lujian Yao and Qiyuan Wang and Jinwen Chen and Bo Li", "abstract": "  Recent studies demonstrate that multimodal large language models (MLLMs) can\nproficiently evaluate visual quality through interpretable assessments.\nHowever, existing approaches typically treat quality scoring and reasoning\ndescriptions as separate tasks with disjoint optimization objectives, leading\nto a trade-off: models adept at quality reasoning descriptions struggle with\nprecise score regression, while score-focused models lack interpretability.\nThis limitation hinders the full potential of MLLMs in visual quality\nassessment, where accuracy and interpretability should be mutually reinforcing.\nTo address this, we propose a unified two-stage training framework comprising a\ncold-start stage and a reinforcement learning-based fine-tuning stage.\nSpecifically, in the first stage, we distill high-quality data from a teacher\nmodel through expert-designed prompts, initializing reasoning capabilities via\ncross-entropy loss supervision. In the second stage, we introduce a novel\nreward with Group Relative Policy Optimization (GRPO) to jointly optimize\nscoring accuracy and reasoning consistency. We designate the models derived\nfrom these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show\nthat Q-Ponder achieves state-of-the-art (SOTA) performance on quality score\nregression benchmarks, delivering up to 6.5% higher SRCC on cross-domain\ndatasets. Furthermore, Q-Ponder significantly outperforms description-based\nSOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in\ndescription accuracy and reasonableness, demonstrating the generalization\npotential over diverse tasks.\n", "link": "http://arxiv.org/abs/2506.05384v2", "date": "2025-06-12", "relevancy": 2.3412, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-Ponder%3A%20A%20Unified%20Training%20Pipeline%20for%20Reasoning-based%20Visual%20Quality%0A%20%20Assessment&body=Title%3A%20Q-Ponder%3A%20A%20Unified%20Training%20Pipeline%20for%20Reasoning-based%20Visual%20Quality%0A%20%20Assessment%0AAuthor%3A%20Zhuoxuan%20Cai%20and%20Jian%20Zhang%20and%20Xinbin%20Yuan%20and%20Peng-Tao%20Jiang%20and%20Wenxiang%20Chen%20and%20Bowen%20Tang%20and%20Lujian%20Yao%20and%20Qiyuan%20Wang%20and%20Jinwen%20Chen%20and%20Bo%20Li%0AAbstract%3A%20%20%20Recent%20studies%20demonstrate%20that%20multimodal%20large%20language%20models%20%28MLLMs%29%20can%0Aproficiently%20evaluate%20visual%20quality%20through%20interpretable%20assessments.%0AHowever%2C%20existing%20approaches%20typically%20treat%20quality%20scoring%20and%20reasoning%0Adescriptions%20as%20separate%20tasks%20with%20disjoint%20optimization%20objectives%2C%20leading%0Ato%20a%20trade-off%3A%20models%20adept%20at%20quality%20reasoning%20descriptions%20struggle%20with%0Aprecise%20score%20regression%2C%20while%20score-focused%20models%20lack%20interpretability.%0AThis%20limitation%20hinders%20the%20full%20potential%20of%20MLLMs%20in%20visual%20quality%0Aassessment%2C%20where%20accuracy%20and%20interpretability%20should%20be%20mutually%20reinforcing.%0ATo%20address%20this%2C%20we%20propose%20a%20unified%20two-stage%20training%20framework%20comprising%20a%0Acold-start%20stage%20and%20a%20reinforcement%20learning-based%20fine-tuning%20stage.%0ASpecifically%2C%20in%20the%20first%20stage%2C%20we%20distill%20high-quality%20data%20from%20a%20teacher%0Amodel%20through%20expert-designed%20prompts%2C%20initializing%20reasoning%20capabilities%20via%0Across-entropy%20loss%20supervision.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20novel%0Areward%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20jointly%20optimize%0Ascoring%20accuracy%20and%20reasoning%20consistency.%20We%20designate%20the%20models%20derived%0Afrom%20these%20two%20stages%20as%20Q-Ponder-CI%20and%20Q-Ponder.%20Extensive%20experiments%20show%0Athat%20Q-Ponder%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20quality%20score%0Aregression%20benchmarks%2C%20delivering%20up%20to%206.5%25%20higher%20SRCC%20on%20cross-domain%0Adatasets.%20Furthermore%2C%20Q-Ponder%20significantly%20outperforms%20description-based%0ASOTA%20models%2C%20including%20its%20teacher%20model%20Qwen-2.5-VL-72B%2C%20particularly%20in%0Adescription%20accuracy%20and%20reasonableness%2C%20demonstrating%20the%20generalization%0Apotential%20over%20diverse%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05384v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-Ponder%253A%2520A%2520Unified%2520Training%2520Pipeline%2520for%2520Reasoning-based%2520Visual%2520Quality%250A%2520%2520Assessment%26entry.906535625%3DZhuoxuan%2520Cai%2520and%2520Jian%2520Zhang%2520and%2520Xinbin%2520Yuan%2520and%2520Peng-Tao%2520Jiang%2520and%2520Wenxiang%2520Chen%2520and%2520Bowen%2520Tang%2520and%2520Lujian%2520Yao%2520and%2520Qiyuan%2520Wang%2520and%2520Jinwen%2520Chen%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520Recent%2520studies%2520demonstrate%2520that%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520can%250Aproficiently%2520evaluate%2520visual%2520quality%2520through%2520interpretable%2520assessments.%250AHowever%252C%2520existing%2520approaches%2520typically%2520treat%2520quality%2520scoring%2520and%2520reasoning%250Adescriptions%2520as%2520separate%2520tasks%2520with%2520disjoint%2520optimization%2520objectives%252C%2520leading%250Ato%2520a%2520trade-off%253A%2520models%2520adept%2520at%2520quality%2520reasoning%2520descriptions%2520struggle%2520with%250Aprecise%2520score%2520regression%252C%2520while%2520score-focused%2520models%2520lack%2520interpretability.%250AThis%2520limitation%2520hinders%2520the%2520full%2520potential%2520of%2520MLLMs%2520in%2520visual%2520quality%250Aassessment%252C%2520where%2520accuracy%2520and%2520interpretability%2520should%2520be%2520mutually%2520reinforcing.%250ATo%2520address%2520this%252C%2520we%2520propose%2520a%2520unified%2520two-stage%2520training%2520framework%2520comprising%2520a%250Acold-start%2520stage%2520and%2520a%2520reinforcement%2520learning-based%2520fine-tuning%2520stage.%250ASpecifically%252C%2520in%2520the%2520first%2520stage%252C%2520we%2520distill%2520high-quality%2520data%2520from%2520a%2520teacher%250Amodel%2520through%2520expert-designed%2520prompts%252C%2520initializing%2520reasoning%2520capabilities%2520via%250Across-entropy%2520loss%2520supervision.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520a%2520novel%250Areward%2520with%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520jointly%2520optimize%250Ascoring%2520accuracy%2520and%2520reasoning%2520consistency.%2520We%2520designate%2520the%2520models%2520derived%250Afrom%2520these%2520two%2520stages%2520as%2520Q-Ponder-CI%2520and%2520Q-Ponder.%2520Extensive%2520experiments%2520show%250Athat%2520Q-Ponder%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520quality%2520score%250Aregression%2520benchmarks%252C%2520delivering%2520up%2520to%25206.5%2525%2520higher%2520SRCC%2520on%2520cross-domain%250Adatasets.%2520Furthermore%252C%2520Q-Ponder%2520significantly%2520outperforms%2520description-based%250ASOTA%2520models%252C%2520including%2520its%2520teacher%2520model%2520Qwen-2.5-VL-72B%252C%2520particularly%2520in%250Adescription%2520accuracy%2520and%2520reasonableness%252C%2520demonstrating%2520the%2520generalization%250Apotential%2520over%2520diverse%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05384v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Ponder%3A%20A%20Unified%20Training%20Pipeline%20for%20Reasoning-based%20Visual%20Quality%0A%20%20Assessment&entry.906535625=Zhuoxuan%20Cai%20and%20Jian%20Zhang%20and%20Xinbin%20Yuan%20and%20Peng-Tao%20Jiang%20and%20Wenxiang%20Chen%20and%20Bowen%20Tang%20and%20Lujian%20Yao%20and%20Qiyuan%20Wang%20and%20Jinwen%20Chen%20and%20Bo%20Li&entry.1292438233=%20%20Recent%20studies%20demonstrate%20that%20multimodal%20large%20language%20models%20%28MLLMs%29%20can%0Aproficiently%20evaluate%20visual%20quality%20through%20interpretable%20assessments.%0AHowever%2C%20existing%20approaches%20typically%20treat%20quality%20scoring%20and%20reasoning%0Adescriptions%20as%20separate%20tasks%20with%20disjoint%20optimization%20objectives%2C%20leading%0Ato%20a%20trade-off%3A%20models%20adept%20at%20quality%20reasoning%20descriptions%20struggle%20with%0Aprecise%20score%20regression%2C%20while%20score-focused%20models%20lack%20interpretability.%0AThis%20limitation%20hinders%20the%20full%20potential%20of%20MLLMs%20in%20visual%20quality%0Aassessment%2C%20where%20accuracy%20and%20interpretability%20should%20be%20mutually%20reinforcing.%0ATo%20address%20this%2C%20we%20propose%20a%20unified%20two-stage%20training%20framework%20comprising%20a%0Acold-start%20stage%20and%20a%20reinforcement%20learning-based%20fine-tuning%20stage.%0ASpecifically%2C%20in%20the%20first%20stage%2C%20we%20distill%20high-quality%20data%20from%20a%20teacher%0Amodel%20through%20expert-designed%20prompts%2C%20initializing%20reasoning%20capabilities%20via%0Across-entropy%20loss%20supervision.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20novel%0Areward%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20jointly%20optimize%0Ascoring%20accuracy%20and%20reasoning%20consistency.%20We%20designate%20the%20models%20derived%0Afrom%20these%20two%20stages%20as%20Q-Ponder-CI%20and%20Q-Ponder.%20Extensive%20experiments%20show%0Athat%20Q-Ponder%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20quality%20score%0Aregression%20benchmarks%2C%20delivering%20up%20to%206.5%25%20higher%20SRCC%20on%20cross-domain%0Adatasets.%20Furthermore%2C%20Q-Ponder%20significantly%20outperforms%20description-based%0ASOTA%20models%2C%20including%20its%20teacher%20model%20Qwen-2.5-VL-72B%2C%20particularly%20in%0Adescription%20accuracy%20and%20reasonableness%2C%20demonstrating%20the%20generalization%0Apotential%20over%20diverse%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05384v2&entry.124074799=Read"},
{"title": "Self-Adapting Language Models", "author": "Adam Zweiger and Jyothish Pari and Han Guo and Ekin Aky\u00fcrek and Yoon Kim and Pulkit Agrawal", "abstract": "  Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.\n", "link": "http://arxiv.org/abs/2506.10943v1", "date": "2025-06-12", "relevancy": 2.3316, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.459}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Adapting%20Language%20Models&body=Title%3A%20Self-Adapting%20Language%20Models%0AAuthor%3A%20Adam%20Zweiger%20and%20Jyothish%20Pari%20and%20Han%20Guo%20and%20Ekin%20Aky%C3%BCrek%20and%20Yoon%20Kim%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20powerful%20but%20static%3B%20they%20lack%20mechanisms%20to%0Aadapt%20their%20weights%20in%20response%20to%20new%20tasks%2C%20knowledge%2C%20or%20examples.%20We%0Aintroduce%20Self-Adapting%20LLMs%20%28SEAL%29%2C%20a%20framework%20that%20enables%20LLMs%20to%0Aself-adapt%20by%20generating%20their%20own%20finetuning%20data%20and%20update%20directives.%20Given%0Aa%20new%20input%2C%20the%20model%20produces%20a%20self-edit-a%20generation%20that%20may%20restructure%0Athe%20information%20in%20different%20ways%2C%20specify%20optimization%20hyperparameters%2C%20or%0Ainvoke%20tools%20for%20data%20augmentation%20and%20gradient-based%20updates.%20Through%0Asupervised%20finetuning%20%28SFT%29%2C%20these%20self-edits%20result%20in%20persistent%20weight%0Aupdates%2C%20enabling%20lasting%20adaptation.%20To%20train%20the%20model%20to%20produce%20effective%0Aself-edits%2C%20we%20use%20a%20reinforcement%20learning%20loop%20with%20the%20downstream%0Aperformance%20of%20the%20updated%20model%20as%20the%20reward%20signal.%20Unlike%20prior%20approaches%0Athat%20rely%20on%20separate%20adaptation%20modules%20or%20auxiliary%20networks%2C%20SEAL%20directly%0Auses%20the%20model%27s%20own%20generation%20to%20control%20its%20adaptation%20process.%20Experiments%0Aon%20knowledge%20incorporation%20and%20few-shot%20generalization%20show%20that%20SEAL%20is%20a%0Apromising%20step%20toward%20language%20models%20capable%20of%20self-directed%20adaptation.%20Our%0Awebsite%20and%20code%20is%20available%20at%20https%3A//jyopari.github.io/posts/seal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Adapting%2520Language%2520Models%26entry.906535625%3DAdam%2520Zweiger%2520and%2520Jyothish%2520Pari%2520and%2520Han%2520Guo%2520and%2520Ekin%2520Aky%25C3%25BCrek%2520and%2520Yoon%2520Kim%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520powerful%2520but%2520static%253B%2520they%2520lack%2520mechanisms%2520to%250Aadapt%2520their%2520weights%2520in%2520response%2520to%2520new%2520tasks%252C%2520knowledge%252C%2520or%2520examples.%2520We%250Aintroduce%2520Self-Adapting%2520LLMs%2520%2528SEAL%2529%252C%2520a%2520framework%2520that%2520enables%2520LLMs%2520to%250Aself-adapt%2520by%2520generating%2520their%2520own%2520finetuning%2520data%2520and%2520update%2520directives.%2520Given%250Aa%2520new%2520input%252C%2520the%2520model%2520produces%2520a%2520self-edit-a%2520generation%2520that%2520may%2520restructure%250Athe%2520information%2520in%2520different%2520ways%252C%2520specify%2520optimization%2520hyperparameters%252C%2520or%250Ainvoke%2520tools%2520for%2520data%2520augmentation%2520and%2520gradient-based%2520updates.%2520Through%250Asupervised%2520finetuning%2520%2528SFT%2529%252C%2520these%2520self-edits%2520result%2520in%2520persistent%2520weight%250Aupdates%252C%2520enabling%2520lasting%2520adaptation.%2520To%2520train%2520the%2520model%2520to%2520produce%2520effective%250Aself-edits%252C%2520we%2520use%2520a%2520reinforcement%2520learning%2520loop%2520with%2520the%2520downstream%250Aperformance%2520of%2520the%2520updated%2520model%2520as%2520the%2520reward%2520signal.%2520Unlike%2520prior%2520approaches%250Athat%2520rely%2520on%2520separate%2520adaptation%2520modules%2520or%2520auxiliary%2520networks%252C%2520SEAL%2520directly%250Auses%2520the%2520model%2527s%2520own%2520generation%2520to%2520control%2520its%2520adaptation%2520process.%2520Experiments%250Aon%2520knowledge%2520incorporation%2520and%2520few-shot%2520generalization%2520show%2520that%2520SEAL%2520is%2520a%250Apromising%2520step%2520toward%2520language%2520models%2520capable%2520of%2520self-directed%2520adaptation.%2520Our%250Awebsite%2520and%2520code%2520is%2520available%2520at%2520https%253A//jyopari.github.io/posts/seal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Adapting%20Language%20Models&entry.906535625=Adam%20Zweiger%20and%20Jyothish%20Pari%20and%20Han%20Guo%20and%20Ekin%20Aky%C3%BCrek%20and%20Yoon%20Kim%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20powerful%20but%20static%3B%20they%20lack%20mechanisms%20to%0Aadapt%20their%20weights%20in%20response%20to%20new%20tasks%2C%20knowledge%2C%20or%20examples.%20We%0Aintroduce%20Self-Adapting%20LLMs%20%28SEAL%29%2C%20a%20framework%20that%20enables%20LLMs%20to%0Aself-adapt%20by%20generating%20their%20own%20finetuning%20data%20and%20update%20directives.%20Given%0Aa%20new%20input%2C%20the%20model%20produces%20a%20self-edit-a%20generation%20that%20may%20restructure%0Athe%20information%20in%20different%20ways%2C%20specify%20optimization%20hyperparameters%2C%20or%0Ainvoke%20tools%20for%20data%20augmentation%20and%20gradient-based%20updates.%20Through%0Asupervised%20finetuning%20%28SFT%29%2C%20these%20self-edits%20result%20in%20persistent%20weight%0Aupdates%2C%20enabling%20lasting%20adaptation.%20To%20train%20the%20model%20to%20produce%20effective%0Aself-edits%2C%20we%20use%20a%20reinforcement%20learning%20loop%20with%20the%20downstream%0Aperformance%20of%20the%20updated%20model%20as%20the%20reward%20signal.%20Unlike%20prior%20approaches%0Athat%20rely%20on%20separate%20adaptation%20modules%20or%20auxiliary%20networks%2C%20SEAL%20directly%0Auses%20the%20model%27s%20own%20generation%20to%20control%20its%20adaptation%20process.%20Experiments%0Aon%20knowledge%20incorporation%20and%20few-shot%20generalization%20show%20that%20SEAL%20is%20a%0Apromising%20step%20toward%20language%20models%20capable%20of%20self-directed%20adaptation.%20Our%0Awebsite%20and%20code%20is%20available%20at%20https%3A//jyopari.github.io/posts/seal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10943v1&entry.124074799=Read"},
{"title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection\n  in Preference Optimization", "author": "Hee Suk Yoon and Eunseop Yoon and Mark Hasegawa-Johnson and Sungwoong Kim and Chang D. Yoo", "abstract": "  We introduce ConfPO, a method for preference learning in Large Language\nModels (LLMs) that identifies and optimizes preference-critical tokens based\nsolely on the training policy's confidence, without requiring any auxiliary\nmodels or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as\nDirect Preference Optimization (DPO), which uniformly adjust all token\nprobabilities regardless of their relevance to preference, ConfPO focuses\noptimization on the most impactful tokens. This targeted approach improves\nalignment quality while mitigating overoptimization (i.e., reward hacking) by\nusing the KL divergence budget more efficiently. In contrast to recent\ntoken-level methods that rely on credit-assignment models or AI annotators,\nraising concerns about scalability and reliability, ConfPO is simple,\nlightweight, and model-free. Experimental results on challenging alignment\nbenchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO\nconsistently outperforms uniform DAAs across various LLMs, delivering better\nalignment with zero additional computational overhead.\n", "link": "http://arxiv.org/abs/2506.08712v2", "date": "2025-06-12", "relevancy": 2.3271, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConfPO%3A%20Exploiting%20Policy%20Model%20Confidence%20for%20Critical%20Token%20Selection%0A%20%20in%20Preference%20Optimization&body=Title%3A%20ConfPO%3A%20Exploiting%20Policy%20Model%20Confidence%20for%20Critical%20Token%20Selection%0A%20%20in%20Preference%20Optimization%0AAuthor%3A%20Hee%20Suk%20Yoon%20and%20Eunseop%20Yoon%20and%20Mark%20Hasegawa-Johnson%20and%20Sungwoong%20Kim%20and%20Chang%20D.%20Yoo%0AAbstract%3A%20%20%20We%20introduce%20ConfPO%2C%20a%20method%20for%20preference%20learning%20in%20Large%20Language%0AModels%20%28LLMs%29%20that%20identifies%20and%20optimizes%20preference-critical%20tokens%20based%0Asolely%20on%20the%20training%20policy%27s%20confidence%2C%20without%20requiring%20any%20auxiliary%0Amodels%20or%20compute.%20Unlike%20prior%20Direct%20Alignment%20Algorithms%20%28DAAs%29%20such%20as%0ADirect%20Preference%20Optimization%20%28DPO%29%2C%20which%20uniformly%20adjust%20all%20token%0Aprobabilities%20regardless%20of%20their%20relevance%20to%20preference%2C%20ConfPO%20focuses%0Aoptimization%20on%20the%20most%20impactful%20tokens.%20This%20targeted%20approach%20improves%0Aalignment%20quality%20while%20mitigating%20overoptimization%20%28i.e.%2C%20reward%20hacking%29%20by%0Ausing%20the%20KL%20divergence%20budget%20more%20efficiently.%20In%20contrast%20to%20recent%0Atoken-level%20methods%20that%20rely%20on%20credit-assignment%20models%20or%20AI%20annotators%2C%0Araising%20concerns%20about%20scalability%20and%20reliability%2C%20ConfPO%20is%20simple%2C%0Alightweight%2C%20and%20model-free.%20Experimental%20results%20on%20challenging%20alignment%0Abenchmarks%2C%20including%20AlpacaEval%202%20and%20Arena-Hard%2C%20demonstrate%20that%20ConfPO%0Aconsistently%20outperforms%20uniform%20DAAs%20across%20various%20LLMs%2C%20delivering%20better%0Aalignment%20with%20zero%20additional%20computational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfPO%253A%2520Exploiting%2520Policy%2520Model%2520Confidence%2520for%2520Critical%2520Token%2520Selection%250A%2520%2520in%2520Preference%2520Optimization%26entry.906535625%3DHee%2520Suk%2520Yoon%2520and%2520Eunseop%2520Yoon%2520and%2520Mark%2520Hasegawa-Johnson%2520and%2520Sungwoong%2520Kim%2520and%2520Chang%2520D.%2520Yoo%26entry.1292438233%3D%2520%2520We%2520introduce%2520ConfPO%252C%2520a%2520method%2520for%2520preference%2520learning%2520in%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520that%2520identifies%2520and%2520optimizes%2520preference-critical%2520tokens%2520based%250Asolely%2520on%2520the%2520training%2520policy%2527s%2520confidence%252C%2520without%2520requiring%2520any%2520auxiliary%250Amodels%2520or%2520compute.%2520Unlike%2520prior%2520Direct%2520Alignment%2520Algorithms%2520%2528DAAs%2529%2520such%2520as%250ADirect%2520Preference%2520Optimization%2520%2528DPO%2529%252C%2520which%2520uniformly%2520adjust%2520all%2520token%250Aprobabilities%2520regardless%2520of%2520their%2520relevance%2520to%2520preference%252C%2520ConfPO%2520focuses%250Aoptimization%2520on%2520the%2520most%2520impactful%2520tokens.%2520This%2520targeted%2520approach%2520improves%250Aalignment%2520quality%2520while%2520mitigating%2520overoptimization%2520%2528i.e.%252C%2520reward%2520hacking%2529%2520by%250Ausing%2520the%2520KL%2520divergence%2520budget%2520more%2520efficiently.%2520In%2520contrast%2520to%2520recent%250Atoken-level%2520methods%2520that%2520rely%2520on%2520credit-assignment%2520models%2520or%2520AI%2520annotators%252C%250Araising%2520concerns%2520about%2520scalability%2520and%2520reliability%252C%2520ConfPO%2520is%2520simple%252C%250Alightweight%252C%2520and%2520model-free.%2520Experimental%2520results%2520on%2520challenging%2520alignment%250Abenchmarks%252C%2520including%2520AlpacaEval%25202%2520and%2520Arena-Hard%252C%2520demonstrate%2520that%2520ConfPO%250Aconsistently%2520outperforms%2520uniform%2520DAAs%2520across%2520various%2520LLMs%252C%2520delivering%2520better%250Aalignment%2520with%2520zero%2520additional%2520computational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConfPO%3A%20Exploiting%20Policy%20Model%20Confidence%20for%20Critical%20Token%20Selection%0A%20%20in%20Preference%20Optimization&entry.906535625=Hee%20Suk%20Yoon%20and%20Eunseop%20Yoon%20and%20Mark%20Hasegawa-Johnson%20and%20Sungwoong%20Kim%20and%20Chang%20D.%20Yoo&entry.1292438233=%20%20We%20introduce%20ConfPO%2C%20a%20method%20for%20preference%20learning%20in%20Large%20Language%0AModels%20%28LLMs%29%20that%20identifies%20and%20optimizes%20preference-critical%20tokens%20based%0Asolely%20on%20the%20training%20policy%27s%20confidence%2C%20without%20requiring%20any%20auxiliary%0Amodels%20or%20compute.%20Unlike%20prior%20Direct%20Alignment%20Algorithms%20%28DAAs%29%20such%20as%0ADirect%20Preference%20Optimization%20%28DPO%29%2C%20which%20uniformly%20adjust%20all%20token%0Aprobabilities%20regardless%20of%20their%20relevance%20to%20preference%2C%20ConfPO%20focuses%0Aoptimization%20on%20the%20most%20impactful%20tokens.%20This%20targeted%20approach%20improves%0Aalignment%20quality%20while%20mitigating%20overoptimization%20%28i.e.%2C%20reward%20hacking%29%20by%0Ausing%20the%20KL%20divergence%20budget%20more%20efficiently.%20In%20contrast%20to%20recent%0Atoken-level%20methods%20that%20rely%20on%20credit-assignment%20models%20or%20AI%20annotators%2C%0Araising%20concerns%20about%20scalability%20and%20reliability%2C%20ConfPO%20is%20simple%2C%0Alightweight%2C%20and%20model-free.%20Experimental%20results%20on%20challenging%20alignment%0Abenchmarks%2C%20including%20AlpacaEval%202%20and%20Arena-Hard%2C%20demonstrate%20that%20ConfPO%0Aconsistently%20outperforms%20uniform%20DAAs%20across%20various%20LLMs%2C%20delivering%20better%0Aalignment%20with%20zero%20additional%20computational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08712v2&entry.124074799=Read"},
{"title": "Balancing Tails when Comparing Distributions: Comprehensive Equity Index\n  (CEI) with Application to Bias Evaluation in Operational Face Biometrics", "author": "Imanol Solano and Julian Fierrez and Aythami Morales and Alejandro Pe\u00f1a and Ruben Tolosana and Francisco Zamora-Martinez and Javier San Agustin", "abstract": "  Demographic bias in high-performance face recognition (FR) systems often\neludes detection by existing metrics, especially with respect to subtle\ndisparities in the tails of the score distribution. We introduce the\nComprehensive Equity Index (CEI), a novel metric designed to address this\nlimitation. CEI uniquely analyzes genuine and impostor score distributions\nseparately, enabling a configurable focus on tail probabilities while also\nconsidering overall distribution shapes. Our extensive experiments (evaluating\nstate-of-the-art FR systems, intentionally biased models, and diverse datasets)\nconfirm CEI's superior ability to detect nuanced biases where previous methods\nfall short. Furthermore, we present CEI^A, an automated version of the metric\nthat enhances objectivity and simplifies practical application. CEI provides a\nrobust and sensitive tool for operational FR fairness assessment. The proposed\nmethods have been developed particularly for bias evaluation in face biometrics\nbut, in general, they are applicable for comparing statistical distributions in\nany problem where one is interested in analyzing the distribution tails.\n", "link": "http://arxiv.org/abs/2506.10564v1", "date": "2025-06-12", "relevancy": 2.3234, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4799}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4742}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20Tails%20when%20Comparing%20Distributions%3A%20Comprehensive%20Equity%20Index%0A%20%20%28CEI%29%20with%20Application%20to%20Bias%20Evaluation%20in%20Operational%20Face%20Biometrics&body=Title%3A%20Balancing%20Tails%20when%20Comparing%20Distributions%3A%20Comprehensive%20Equity%20Index%0A%20%20%28CEI%29%20with%20Application%20to%20Bias%20Evaluation%20in%20Operational%20Face%20Biometrics%0AAuthor%3A%20Imanol%20Solano%20and%20Julian%20Fierrez%20and%20Aythami%20Morales%20and%20Alejandro%20Pe%C3%B1a%20and%20Ruben%20Tolosana%20and%20Francisco%20Zamora-Martinez%20and%20Javier%20San%20Agustin%0AAbstract%3A%20%20%20Demographic%20bias%20in%20high-performance%20face%20recognition%20%28FR%29%20systems%20often%0Aeludes%20detection%20by%20existing%20metrics%2C%20especially%20with%20respect%20to%20subtle%0Adisparities%20in%20the%20tails%20of%20the%20score%20distribution.%20We%20introduce%20the%0AComprehensive%20Equity%20Index%20%28CEI%29%2C%20a%20novel%20metric%20designed%20to%20address%20this%0Alimitation.%20CEI%20uniquely%20analyzes%20genuine%20and%20impostor%20score%20distributions%0Aseparately%2C%20enabling%20a%20configurable%20focus%20on%20tail%20probabilities%20while%20also%0Aconsidering%20overall%20distribution%20shapes.%20Our%20extensive%20experiments%20%28evaluating%0Astate-of-the-art%20FR%20systems%2C%20intentionally%20biased%20models%2C%20and%20diverse%20datasets%29%0Aconfirm%20CEI%27s%20superior%20ability%20to%20detect%20nuanced%20biases%20where%20previous%20methods%0Afall%20short.%20Furthermore%2C%20we%20present%20CEI%5EA%2C%20an%20automated%20version%20of%20the%20metric%0Athat%20enhances%20objectivity%20and%20simplifies%20practical%20application.%20CEI%20provides%20a%0Arobust%20and%20sensitive%20tool%20for%20operational%20FR%20fairness%20assessment.%20The%20proposed%0Amethods%20have%20been%20developed%20particularly%20for%20bias%20evaluation%20in%20face%20biometrics%0Abut%2C%20in%20general%2C%20they%20are%20applicable%20for%20comparing%20statistical%20distributions%20in%0Aany%20problem%20where%20one%20is%20interested%20in%20analyzing%20the%20distribution%20tails.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520Tails%2520when%2520Comparing%2520Distributions%253A%2520Comprehensive%2520Equity%2520Index%250A%2520%2520%2528CEI%2529%2520with%2520Application%2520to%2520Bias%2520Evaluation%2520in%2520Operational%2520Face%2520Biometrics%26entry.906535625%3DImanol%2520Solano%2520and%2520Julian%2520Fierrez%2520and%2520Aythami%2520Morales%2520and%2520Alejandro%2520Pe%25C3%25B1a%2520and%2520Ruben%2520Tolosana%2520and%2520Francisco%2520Zamora-Martinez%2520and%2520Javier%2520San%2520Agustin%26entry.1292438233%3D%2520%2520Demographic%2520bias%2520in%2520high-performance%2520face%2520recognition%2520%2528FR%2529%2520systems%2520often%250Aeludes%2520detection%2520by%2520existing%2520metrics%252C%2520especially%2520with%2520respect%2520to%2520subtle%250Adisparities%2520in%2520the%2520tails%2520of%2520the%2520score%2520distribution.%2520We%2520introduce%2520the%250AComprehensive%2520Equity%2520Index%2520%2528CEI%2529%252C%2520a%2520novel%2520metric%2520designed%2520to%2520address%2520this%250Alimitation.%2520CEI%2520uniquely%2520analyzes%2520genuine%2520and%2520impostor%2520score%2520distributions%250Aseparately%252C%2520enabling%2520a%2520configurable%2520focus%2520on%2520tail%2520probabilities%2520while%2520also%250Aconsidering%2520overall%2520distribution%2520shapes.%2520Our%2520extensive%2520experiments%2520%2528evaluating%250Astate-of-the-art%2520FR%2520systems%252C%2520intentionally%2520biased%2520models%252C%2520and%2520diverse%2520datasets%2529%250Aconfirm%2520CEI%2527s%2520superior%2520ability%2520to%2520detect%2520nuanced%2520biases%2520where%2520previous%2520methods%250Afall%2520short.%2520Furthermore%252C%2520we%2520present%2520CEI%255EA%252C%2520an%2520automated%2520version%2520of%2520the%2520metric%250Athat%2520enhances%2520objectivity%2520and%2520simplifies%2520practical%2520application.%2520CEI%2520provides%2520a%250Arobust%2520and%2520sensitive%2520tool%2520for%2520operational%2520FR%2520fairness%2520assessment.%2520The%2520proposed%250Amethods%2520have%2520been%2520developed%2520particularly%2520for%2520bias%2520evaluation%2520in%2520face%2520biometrics%250Abut%252C%2520in%2520general%252C%2520they%2520are%2520applicable%2520for%2520comparing%2520statistical%2520distributions%2520in%250Aany%2520problem%2520where%2520one%2520is%2520interested%2520in%2520analyzing%2520the%2520distribution%2520tails.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Tails%20when%20Comparing%20Distributions%3A%20Comprehensive%20Equity%20Index%0A%20%20%28CEI%29%20with%20Application%20to%20Bias%20Evaluation%20in%20Operational%20Face%20Biometrics&entry.906535625=Imanol%20Solano%20and%20Julian%20Fierrez%20and%20Aythami%20Morales%20and%20Alejandro%20Pe%C3%B1a%20and%20Ruben%20Tolosana%20and%20Francisco%20Zamora-Martinez%20and%20Javier%20San%20Agustin&entry.1292438233=%20%20Demographic%20bias%20in%20high-performance%20face%20recognition%20%28FR%29%20systems%20often%0Aeludes%20detection%20by%20existing%20metrics%2C%20especially%20with%20respect%20to%20subtle%0Adisparities%20in%20the%20tails%20of%20the%20score%20distribution.%20We%20introduce%20the%0AComprehensive%20Equity%20Index%20%28CEI%29%2C%20a%20novel%20metric%20designed%20to%20address%20this%0Alimitation.%20CEI%20uniquely%20analyzes%20genuine%20and%20impostor%20score%20distributions%0Aseparately%2C%20enabling%20a%20configurable%20focus%20on%20tail%20probabilities%20while%20also%0Aconsidering%20overall%20distribution%20shapes.%20Our%20extensive%20experiments%20%28evaluating%0Astate-of-the-art%20FR%20systems%2C%20intentionally%20biased%20models%2C%20and%20diverse%20datasets%29%0Aconfirm%20CEI%27s%20superior%20ability%20to%20detect%20nuanced%20biases%20where%20previous%20methods%0Afall%20short.%20Furthermore%2C%20we%20present%20CEI%5EA%2C%20an%20automated%20version%20of%20the%20metric%0Athat%20enhances%20objectivity%20and%20simplifies%20practical%20application.%20CEI%20provides%20a%0Arobust%20and%20sensitive%20tool%20for%20operational%20FR%20fairness%20assessment.%20The%20proposed%0Amethods%20have%20been%20developed%20particularly%20for%20bias%20evaluation%20in%20face%20biometrics%0Abut%2C%20in%20general%2C%20they%20are%20applicable%20for%20comparing%20statistical%20distributions%20in%0Aany%20problem%20where%20one%20is%20interested%20in%20analyzing%20the%20distribution%20tails.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10564v1&entry.124074799=Read"},
{"title": "Dense Associative Memory with Epanechnikov Energy", "author": "Benjamin Hoover and Zhaoyang Shi and Krishnakumar Balasubramanian and Dmitry Krotov and Parikshit Ram", "abstract": "  We propose a novel energy function for Dense Associative Memory (DenseAM)\nnetworks, the log-sum-ReLU (LSR), inspired by optimal kernel density\nestimation. Unlike the common log-sum-exponential (LSE) function, LSR is based\non the Epanechnikov kernel and enables exact memory retrieval with exponential\ncapacity without requiring exponential separation functions. Moreover, it\nintroduces abundant additional \\emph{emergent} local minima while preserving\nperfect pattern recovery -- a characteristic previously unseen in DenseAM\nliterature. Empirical results show that LSR energy has significantly more local\nminima (memories) that have comparable log-likelihood to LSE-based models.\nAnalysis of LSR's emergent memories on image datasets reveals a degree of\ncreativity and novelty, hinting at this method's potential for both large-scale\nmemory storage and generative tasks.\n", "link": "http://arxiv.org/abs/2506.10801v1", "date": "2025-06-12", "relevancy": 2.3224, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4691}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4628}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Associative%20Memory%20with%20Epanechnikov%20Energy&body=Title%3A%20Dense%20Associative%20Memory%20with%20Epanechnikov%20Energy%0AAuthor%3A%20Benjamin%20Hoover%20and%20Zhaoyang%20Shi%20and%20Krishnakumar%20Balasubramanian%20and%20Dmitry%20Krotov%20and%20Parikshit%20Ram%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20energy%20function%20for%20Dense%20Associative%20Memory%20%28DenseAM%29%0Anetworks%2C%20the%20log-sum-ReLU%20%28LSR%29%2C%20inspired%20by%20optimal%20kernel%20density%0Aestimation.%20Unlike%20the%20common%20log-sum-exponential%20%28LSE%29%20function%2C%20LSR%20is%20based%0Aon%20the%20Epanechnikov%20kernel%20and%20enables%20exact%20memory%20retrieval%20with%20exponential%0Acapacity%20without%20requiring%20exponential%20separation%20functions.%20Moreover%2C%20it%0Aintroduces%20abundant%20additional%20%5Cemph%7Bemergent%7D%20local%20minima%20while%20preserving%0Aperfect%20pattern%20recovery%20--%20a%20characteristic%20previously%20unseen%20in%20DenseAM%0Aliterature.%20Empirical%20results%20show%20that%20LSR%20energy%20has%20significantly%20more%20local%0Aminima%20%28memories%29%20that%20have%20comparable%20log-likelihood%20to%20LSE-based%20models.%0AAnalysis%20of%20LSR%27s%20emergent%20memories%20on%20image%20datasets%20reveals%20a%20degree%20of%0Acreativity%20and%20novelty%2C%20hinting%20at%20this%20method%27s%20potential%20for%20both%20large-scale%0Amemory%20storage%20and%20generative%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Associative%2520Memory%2520with%2520Epanechnikov%2520Energy%26entry.906535625%3DBenjamin%2520Hoover%2520and%2520Zhaoyang%2520Shi%2520and%2520Krishnakumar%2520Balasubramanian%2520and%2520Dmitry%2520Krotov%2520and%2520Parikshit%2520Ram%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520energy%2520function%2520for%2520Dense%2520Associative%2520Memory%2520%2528DenseAM%2529%250Anetworks%252C%2520the%2520log-sum-ReLU%2520%2528LSR%2529%252C%2520inspired%2520by%2520optimal%2520kernel%2520density%250Aestimation.%2520Unlike%2520the%2520common%2520log-sum-exponential%2520%2528LSE%2529%2520function%252C%2520LSR%2520is%2520based%250Aon%2520the%2520Epanechnikov%2520kernel%2520and%2520enables%2520exact%2520memory%2520retrieval%2520with%2520exponential%250Acapacity%2520without%2520requiring%2520exponential%2520separation%2520functions.%2520Moreover%252C%2520it%250Aintroduces%2520abundant%2520additional%2520%255Cemph%257Bemergent%257D%2520local%2520minima%2520while%2520preserving%250Aperfect%2520pattern%2520recovery%2520--%2520a%2520characteristic%2520previously%2520unseen%2520in%2520DenseAM%250Aliterature.%2520Empirical%2520results%2520show%2520that%2520LSR%2520energy%2520has%2520significantly%2520more%2520local%250Aminima%2520%2528memories%2529%2520that%2520have%2520comparable%2520log-likelihood%2520to%2520LSE-based%2520models.%250AAnalysis%2520of%2520LSR%2527s%2520emergent%2520memories%2520on%2520image%2520datasets%2520reveals%2520a%2520degree%2520of%250Acreativity%2520and%2520novelty%252C%2520hinting%2520at%2520this%2520method%2527s%2520potential%2520for%2520both%2520large-scale%250Amemory%2520storage%2520and%2520generative%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Associative%20Memory%20with%20Epanechnikov%20Energy&entry.906535625=Benjamin%20Hoover%20and%20Zhaoyang%20Shi%20and%20Krishnakumar%20Balasubramanian%20and%20Dmitry%20Krotov%20and%20Parikshit%20Ram&entry.1292438233=%20%20We%20propose%20a%20novel%20energy%20function%20for%20Dense%20Associative%20Memory%20%28DenseAM%29%0Anetworks%2C%20the%20log-sum-ReLU%20%28LSR%29%2C%20inspired%20by%20optimal%20kernel%20density%0Aestimation.%20Unlike%20the%20common%20log-sum-exponential%20%28LSE%29%20function%2C%20LSR%20is%20based%0Aon%20the%20Epanechnikov%20kernel%20and%20enables%20exact%20memory%20retrieval%20with%20exponential%0Acapacity%20without%20requiring%20exponential%20separation%20functions.%20Moreover%2C%20it%0Aintroduces%20abundant%20additional%20%5Cemph%7Bemergent%7D%20local%20minima%20while%20preserving%0Aperfect%20pattern%20recovery%20--%20a%20characteristic%20previously%20unseen%20in%20DenseAM%0Aliterature.%20Empirical%20results%20show%20that%20LSR%20energy%20has%20significantly%20more%20local%0Aminima%20%28memories%29%20that%20have%20comparable%20log-likelihood%20to%20LSE-based%20models.%0AAnalysis%20of%20LSR%27s%20emergent%20memories%20on%20image%20datasets%20reveals%20a%20degree%20of%0Acreativity%20and%20novelty%2C%20hinting%20at%20this%20method%27s%20potential%20for%20both%20large-scale%0Amemory%20storage%20and%20generative%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10801v1&entry.124074799=Read"},
{"title": "Training-Free Safe Denoisers for Safe Use of Diffusion Models", "author": "Mingyu Kim and Dongjun Kim and Amman Yusuf and Stefano Ermon and Mijung Park", "abstract": "  There is growing concern over the safety of powerful diffusion models (DMs),\nas they are often misused to produce inappropriate, not-safe-for-work (NSFW)\ncontent or generate copyrighted material or data of individuals who wish to be\nforgotten. Many existing methods tackle these issues by heavily relying on\ntext-based negative prompts or extensively retraining DMs to eliminate certain\nfeatures or samples. In this paper, we take a radically different approach,\ndirectly modifying the sampling trajectory by leveraging a negation set (e.g.,\nunsafe images, copyrighted data, or datapoints needed to be excluded) to avoid\nspecific regions of data distribution, without needing to retrain or fine-tune\nDMs. We formally derive the relationship between the expected denoised samples\nthat are safe and those that are not safe, leading to our $\\textit{safe}$\ndenoiser which ensures its final samples are away from the area to be negated.\nInspired by the derivation, we develop a practical algorithm that successfully\nproduces high-quality samples while avoiding negation areas of the data\ndistribution in text-conditional, class-conditional, and unconditional image\ngeneration scenarios. These results hint at the great potential of our\ntraining-free safe denoiser for using DMs more safely.\n", "link": "http://arxiv.org/abs/2502.08011v3", "date": "2025-06-12", "relevancy": 2.3037, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6027}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5811}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Safe%20Denoisers%20for%20Safe%20Use%20of%20Diffusion%20Models&body=Title%3A%20Training-Free%20Safe%20Denoisers%20for%20Safe%20Use%20of%20Diffusion%20Models%0AAuthor%3A%20Mingyu%20Kim%20and%20Dongjun%20Kim%20and%20Amman%20Yusuf%20and%20Stefano%20Ermon%20and%20Mijung%20Park%0AAbstract%3A%20%20%20There%20is%20growing%20concern%20over%20the%20safety%20of%20powerful%20diffusion%20models%20%28DMs%29%2C%0Aas%20they%20are%20often%20misused%20to%20produce%20inappropriate%2C%20not-safe-for-work%20%28NSFW%29%0Acontent%20or%20generate%20copyrighted%20material%20or%20data%20of%20individuals%20who%20wish%20to%20be%0Aforgotten.%20Many%20existing%20methods%20tackle%20these%20issues%20by%20heavily%20relying%20on%0Atext-based%20negative%20prompts%20or%20extensively%20retraining%20DMs%20to%20eliminate%20certain%0Afeatures%20or%20samples.%20In%20this%20paper%2C%20we%20take%20a%20radically%20different%20approach%2C%0Adirectly%20modifying%20the%20sampling%20trajectory%20by%20leveraging%20a%20negation%20set%20%28e.g.%2C%0Aunsafe%20images%2C%20copyrighted%20data%2C%20or%20datapoints%20needed%20to%20be%20excluded%29%20to%20avoid%0Aspecific%20regions%20of%20data%20distribution%2C%20without%20needing%20to%20retrain%20or%20fine-tune%0ADMs.%20We%20formally%20derive%20the%20relationship%20between%20the%20expected%20denoised%20samples%0Athat%20are%20safe%20and%20those%20that%20are%20not%20safe%2C%20leading%20to%20our%20%24%5Ctextit%7Bsafe%7D%24%0Adenoiser%20which%20ensures%20its%20final%20samples%20are%20away%20from%20the%20area%20to%20be%20negated.%0AInspired%20by%20the%20derivation%2C%20we%20develop%20a%20practical%20algorithm%20that%20successfully%0Aproduces%20high-quality%20samples%20while%20avoiding%20negation%20areas%20of%20the%20data%0Adistribution%20in%20text-conditional%2C%20class-conditional%2C%20and%20unconditional%20image%0Ageneration%20scenarios.%20These%20results%20hint%20at%20the%20great%20potential%20of%20our%0Atraining-free%20safe%20denoiser%20for%20using%20DMs%20more%20safely.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08011v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Safe%2520Denoisers%2520for%2520Safe%2520Use%2520of%2520Diffusion%2520Models%26entry.906535625%3DMingyu%2520Kim%2520and%2520Dongjun%2520Kim%2520and%2520Amman%2520Yusuf%2520and%2520Stefano%2520Ermon%2520and%2520Mijung%2520Park%26entry.1292438233%3D%2520%2520There%2520is%2520growing%2520concern%2520over%2520the%2520safety%2520of%2520powerful%2520diffusion%2520models%2520%2528DMs%2529%252C%250Aas%2520they%2520are%2520often%2520misused%2520to%2520produce%2520inappropriate%252C%2520not-safe-for-work%2520%2528NSFW%2529%250Acontent%2520or%2520generate%2520copyrighted%2520material%2520or%2520data%2520of%2520individuals%2520who%2520wish%2520to%2520be%250Aforgotten.%2520Many%2520existing%2520methods%2520tackle%2520these%2520issues%2520by%2520heavily%2520relying%2520on%250Atext-based%2520negative%2520prompts%2520or%2520extensively%2520retraining%2520DMs%2520to%2520eliminate%2520certain%250Afeatures%2520or%2520samples.%2520In%2520this%2520paper%252C%2520we%2520take%2520a%2520radically%2520different%2520approach%252C%250Adirectly%2520modifying%2520the%2520sampling%2520trajectory%2520by%2520leveraging%2520a%2520negation%2520set%2520%2528e.g.%252C%250Aunsafe%2520images%252C%2520copyrighted%2520data%252C%2520or%2520datapoints%2520needed%2520to%2520be%2520excluded%2529%2520to%2520avoid%250Aspecific%2520regions%2520of%2520data%2520distribution%252C%2520without%2520needing%2520to%2520retrain%2520or%2520fine-tune%250ADMs.%2520We%2520formally%2520derive%2520the%2520relationship%2520between%2520the%2520expected%2520denoised%2520samples%250Athat%2520are%2520safe%2520and%2520those%2520that%2520are%2520not%2520safe%252C%2520leading%2520to%2520our%2520%2524%255Ctextit%257Bsafe%257D%2524%250Adenoiser%2520which%2520ensures%2520its%2520final%2520samples%2520are%2520away%2520from%2520the%2520area%2520to%2520be%2520negated.%250AInspired%2520by%2520the%2520derivation%252C%2520we%2520develop%2520a%2520practical%2520algorithm%2520that%2520successfully%250Aproduces%2520high-quality%2520samples%2520while%2520avoiding%2520negation%2520areas%2520of%2520the%2520data%250Adistribution%2520in%2520text-conditional%252C%2520class-conditional%252C%2520and%2520unconditional%2520image%250Ageneration%2520scenarios.%2520These%2520results%2520hint%2520at%2520the%2520great%2520potential%2520of%2520our%250Atraining-free%2520safe%2520denoiser%2520for%2520using%2520DMs%2520more%2520safely.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08011v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Safe%20Denoisers%20for%20Safe%20Use%20of%20Diffusion%20Models&entry.906535625=Mingyu%20Kim%20and%20Dongjun%20Kim%20and%20Amman%20Yusuf%20and%20Stefano%20Ermon%20and%20Mijung%20Park&entry.1292438233=%20%20There%20is%20growing%20concern%20over%20the%20safety%20of%20powerful%20diffusion%20models%20%28DMs%29%2C%0Aas%20they%20are%20often%20misused%20to%20produce%20inappropriate%2C%20not-safe-for-work%20%28NSFW%29%0Acontent%20or%20generate%20copyrighted%20material%20or%20data%20of%20individuals%20who%20wish%20to%20be%0Aforgotten.%20Many%20existing%20methods%20tackle%20these%20issues%20by%20heavily%20relying%20on%0Atext-based%20negative%20prompts%20or%20extensively%20retraining%20DMs%20to%20eliminate%20certain%0Afeatures%20or%20samples.%20In%20this%20paper%2C%20we%20take%20a%20radically%20different%20approach%2C%0Adirectly%20modifying%20the%20sampling%20trajectory%20by%20leveraging%20a%20negation%20set%20%28e.g.%2C%0Aunsafe%20images%2C%20copyrighted%20data%2C%20or%20datapoints%20needed%20to%20be%20excluded%29%20to%20avoid%0Aspecific%20regions%20of%20data%20distribution%2C%20without%20needing%20to%20retrain%20or%20fine-tune%0ADMs.%20We%20formally%20derive%20the%20relationship%20between%20the%20expected%20denoised%20samples%0Athat%20are%20safe%20and%20those%20that%20are%20not%20safe%2C%20leading%20to%20our%20%24%5Ctextit%7Bsafe%7D%24%0Adenoiser%20which%20ensures%20its%20final%20samples%20are%20away%20from%20the%20area%20to%20be%20negated.%0AInspired%20by%20the%20derivation%2C%20we%20develop%20a%20practical%20algorithm%20that%20successfully%0Aproduces%20high-quality%20samples%20while%20avoiding%20negation%20areas%20of%20the%20data%0Adistribution%20in%20text-conditional%2C%20class-conditional%2C%20and%20unconditional%20image%0Ageneration%20scenarios.%20These%20results%20hint%20at%20the%20great%20potential%20of%20our%0Atraining-free%20safe%20denoiser%20for%20using%20DMs%20more%20safely.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08011v3&entry.124074799=Read"},
{"title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation", "author": "Zhao Zhang and Yutao Cheng and Dexiang Hong and Maoke Yang and Gonglei Shi and Lei Ma and Hui Zhang and Jie Shao and Xinglong Wu", "abstract": "  Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter\n", "link": "http://arxiv.org/abs/2506.10890v1", "date": "2025-06-12", "relevancy": 2.2902, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5973}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5753}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CreatiPoster%3A%20Towards%20Editable%20and%20Controllable%20Multi-Layer%20Graphic%0A%20%20Design%20Generation&body=Title%3A%20CreatiPoster%3A%20Towards%20Editable%20and%20Controllable%20Multi-Layer%20Graphic%0A%20%20Design%20Generation%0AAuthor%3A%20Zhao%20Zhang%20and%20Yutao%20Cheng%20and%20Dexiang%20Hong%20and%20Maoke%20Yang%20and%20Gonglei%20Shi%20and%20Lei%20Ma%20and%20Hui%20Zhang%20and%20Jie%20Shao%20and%20Xinglong%20Wu%0AAbstract%3A%20%20%20Graphic%20design%20plays%20a%20crucial%20role%20in%20both%20commercial%20and%20personal%20contexts%2C%0Ayet%20creating%20high-quality%2C%20editable%2C%20and%20aesthetically%20pleasing%20graphic%0Acompositions%20remains%20a%20time-consuming%20and%20skill-intensive%20task%2C%20especially%20for%0Abeginners.%20Current%20AI%20tools%20automate%20parts%20of%20the%20workflow%2C%20but%20struggle%20to%0Aaccurately%20incorporate%20user-supplied%20assets%2C%20maintain%20editability%2C%20and%20achieve%0Aprofessional%20visual%20appeal.%20Commercial%20systems%2C%20like%20Canva%20Magic%20Design%2C%20rely%0Aon%20vast%20template%20libraries%2C%20which%20are%20impractical%20for%20replicate.%20In%20this%20paper%2C%0Awe%20introduce%20CreatiPoster%2C%20a%20framework%20that%20generates%20editable%2C%20multi-layer%0Acompositions%20from%20optional%20natural-language%20instructions%20or%20assets.%20A%20protocol%0Amodel%2C%20an%20RGBA%20large%20multimodal%20model%2C%20first%20produces%20a%20JSON%20specification%0Adetailing%20every%20layer%20%28text%20or%20asset%29%20with%20precise%20layout%2C%20hierarchy%2C%20content%0Aand%20style%2C%20plus%20a%20concise%20background%20prompt.%20A%20conditional%20background%20model%0Athen%20synthesizes%20a%20coherent%20background%20conditioned%20on%20this%20rendered%20foreground%0Alayers.%20We%20construct%20a%20benchmark%20with%20automated%20metrics%20for%20graphic-design%0Ageneration%20and%20show%20that%20CreatiPoster%20surpasses%20leading%20open-source%20approaches%0Aand%20proprietary%20commercial%20systems.%20To%20catalyze%20further%20research%2C%20we%20release%20a%0Acopyright-free%20corpus%20of%20100%2C000%20multi-layer%20designs.%20CreatiPoster%20supports%0Adiverse%20applications%20such%20as%20canvas%20editing%2C%20text%20overlay%2C%20responsive%20resizing%2C%0Amultilingual%20adaptation%2C%20and%20animated%20posters%2C%20advancing%20the%20democratization%20of%0AAI-assisted%20graphic%20design.%20Project%20homepage%3A%0Ahttps%3A//github.com/graphic-design-ai/creatiposter%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreatiPoster%253A%2520Towards%2520Editable%2520and%2520Controllable%2520Multi-Layer%2520Graphic%250A%2520%2520Design%2520Generation%26entry.906535625%3DZhao%2520Zhang%2520and%2520Yutao%2520Cheng%2520and%2520Dexiang%2520Hong%2520and%2520Maoke%2520Yang%2520and%2520Gonglei%2520Shi%2520and%2520Lei%2520Ma%2520and%2520Hui%2520Zhang%2520and%2520Jie%2520Shao%2520and%2520Xinglong%2520Wu%26entry.1292438233%3D%2520%2520Graphic%2520design%2520plays%2520a%2520crucial%2520role%2520in%2520both%2520commercial%2520and%2520personal%2520contexts%252C%250Ayet%2520creating%2520high-quality%252C%2520editable%252C%2520and%2520aesthetically%2520pleasing%2520graphic%250Acompositions%2520remains%2520a%2520time-consuming%2520and%2520skill-intensive%2520task%252C%2520especially%2520for%250Abeginners.%2520Current%2520AI%2520tools%2520automate%2520parts%2520of%2520the%2520workflow%252C%2520but%2520struggle%2520to%250Aaccurately%2520incorporate%2520user-supplied%2520assets%252C%2520maintain%2520editability%252C%2520and%2520achieve%250Aprofessional%2520visual%2520appeal.%2520Commercial%2520systems%252C%2520like%2520Canva%2520Magic%2520Design%252C%2520rely%250Aon%2520vast%2520template%2520libraries%252C%2520which%2520are%2520impractical%2520for%2520replicate.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520CreatiPoster%252C%2520a%2520framework%2520that%2520generates%2520editable%252C%2520multi-layer%250Acompositions%2520from%2520optional%2520natural-language%2520instructions%2520or%2520assets.%2520A%2520protocol%250Amodel%252C%2520an%2520RGBA%2520large%2520multimodal%2520model%252C%2520first%2520produces%2520a%2520JSON%2520specification%250Adetailing%2520every%2520layer%2520%2528text%2520or%2520asset%2529%2520with%2520precise%2520layout%252C%2520hierarchy%252C%2520content%250Aand%2520style%252C%2520plus%2520a%2520concise%2520background%2520prompt.%2520A%2520conditional%2520background%2520model%250Athen%2520synthesizes%2520a%2520coherent%2520background%2520conditioned%2520on%2520this%2520rendered%2520foreground%250Alayers.%2520We%2520construct%2520a%2520benchmark%2520with%2520automated%2520metrics%2520for%2520graphic-design%250Ageneration%2520and%2520show%2520that%2520CreatiPoster%2520surpasses%2520leading%2520open-source%2520approaches%250Aand%2520proprietary%2520commercial%2520systems.%2520To%2520catalyze%2520further%2520research%252C%2520we%2520release%2520a%250Acopyright-free%2520corpus%2520of%2520100%252C000%2520multi-layer%2520designs.%2520CreatiPoster%2520supports%250Adiverse%2520applications%2520such%2520as%2520canvas%2520editing%252C%2520text%2520overlay%252C%2520responsive%2520resizing%252C%250Amultilingual%2520adaptation%252C%2520and%2520animated%2520posters%252C%2520advancing%2520the%2520democratization%2520of%250AAI-assisted%2520graphic%2520design.%2520Project%2520homepage%253A%250Ahttps%253A//github.com/graphic-design-ai/creatiposter%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CreatiPoster%3A%20Towards%20Editable%20and%20Controllable%20Multi-Layer%20Graphic%0A%20%20Design%20Generation&entry.906535625=Zhao%20Zhang%20and%20Yutao%20Cheng%20and%20Dexiang%20Hong%20and%20Maoke%20Yang%20and%20Gonglei%20Shi%20and%20Lei%20Ma%20and%20Hui%20Zhang%20and%20Jie%20Shao%20and%20Xinglong%20Wu&entry.1292438233=%20%20Graphic%20design%20plays%20a%20crucial%20role%20in%20both%20commercial%20and%20personal%20contexts%2C%0Ayet%20creating%20high-quality%2C%20editable%2C%20and%20aesthetically%20pleasing%20graphic%0Acompositions%20remains%20a%20time-consuming%20and%20skill-intensive%20task%2C%20especially%20for%0Abeginners.%20Current%20AI%20tools%20automate%20parts%20of%20the%20workflow%2C%20but%20struggle%20to%0Aaccurately%20incorporate%20user-supplied%20assets%2C%20maintain%20editability%2C%20and%20achieve%0Aprofessional%20visual%20appeal.%20Commercial%20systems%2C%20like%20Canva%20Magic%20Design%2C%20rely%0Aon%20vast%20template%20libraries%2C%20which%20are%20impractical%20for%20replicate.%20In%20this%20paper%2C%0Awe%20introduce%20CreatiPoster%2C%20a%20framework%20that%20generates%20editable%2C%20multi-layer%0Acompositions%20from%20optional%20natural-language%20instructions%20or%20assets.%20A%20protocol%0Amodel%2C%20an%20RGBA%20large%20multimodal%20model%2C%20first%20produces%20a%20JSON%20specification%0Adetailing%20every%20layer%20%28text%20or%20asset%29%20with%20precise%20layout%2C%20hierarchy%2C%20content%0Aand%20style%2C%20plus%20a%20concise%20background%20prompt.%20A%20conditional%20background%20model%0Athen%20synthesizes%20a%20coherent%20background%20conditioned%20on%20this%20rendered%20foreground%0Alayers.%20We%20construct%20a%20benchmark%20with%20automated%20metrics%20for%20graphic-design%0Ageneration%20and%20show%20that%20CreatiPoster%20surpasses%20leading%20open-source%20approaches%0Aand%20proprietary%20commercial%20systems.%20To%20catalyze%20further%20research%2C%20we%20release%20a%0Acopyright-free%20corpus%20of%20100%2C000%20multi-layer%20designs.%20CreatiPoster%20supports%0Adiverse%20applications%20such%20as%20canvas%20editing%2C%20text%20overlay%2C%20responsive%20resizing%2C%0Amultilingual%20adaptation%2C%20and%20animated%20posters%2C%20advancing%20the%20democratization%20of%0AAI-assisted%20graphic%20design.%20Project%20homepage%3A%0Ahttps%3A//github.com/graphic-design-ai/creatiposter%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10890v1&entry.124074799=Read"},
{"title": "Post-Training Quantization for Video Matting", "author": "Tianrui Zhu and Houyuan Chen and Ruihao Gong and Michele Magno and Haotong Qin and Kai Zhang", "abstract": "  Video matting is crucial for applications such as film production and virtual\nreality, yet deploying its computationally intensive models on\nresource-constrained devices presents challenges. Quantization is a key\ntechnique for model compression and acceleration. As an efficient approach,\nPost-Training Quantization (PTQ) is still in its nascent stages for video\nmatting, facing significant hurdles in maintaining accuracy and temporal\ncoherence. To address these challenges, this paper proposes a novel and general\nPTQ framework specifically designed for video matting models, marking, to the\nbest of our knowledge, the first systematic attempt in this domain. Our\ncontributions include: (1) A two-stage PTQ strategy that combines\nblock-reconstruction-based optimization for fast, stable initial quantization\nand local dependency capture, followed by a global calibration of quantization\nparameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine\nCalibration (GAC) method that enables the network to compensate for cumulative\nstatistical distortions arising from factors such as neglected BN layer\neffects, even reducing the error of existing PTQ methods on video matting tasks\nup to 20%. (3) An Optical Flow Assistance (OFA) component that leverages\ntemporal and semantic priors from frames to guide the PTQ process, enhancing\nthe model's ability to distinguish moving foregrounds in complex scenes and\nultimately achieving near full-precision performance even under ultra-low-bit\nquantization. Comprehensive quantitative and visual results show that our\nPTQ4VM achieves the state-of-the-art accuracy performance across different\nbit-widths compared to the existing quantization methods. We highlight that the\n4-bit PTQ4VM even achieves performance close to the full-precision counterpart\nwhile enjoying 8x FLOP savings.\n", "link": "http://arxiv.org/abs/2506.10840v1", "date": "2025-06-12", "relevancy": 2.2812, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5884}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5609}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-Training%20Quantization%20for%20Video%20Matting&body=Title%3A%20Post-Training%20Quantization%20for%20Video%20Matting%0AAuthor%3A%20Tianrui%20Zhu%20and%20Houyuan%20Chen%20and%20Ruihao%20Gong%20and%20Michele%20Magno%20and%20Haotong%20Qin%20and%20Kai%20Zhang%0AAbstract%3A%20%20%20Video%20matting%20is%20crucial%20for%20applications%20such%20as%20film%20production%20and%20virtual%0Areality%2C%20yet%20deploying%20its%20computationally%20intensive%20models%20on%0Aresource-constrained%20devices%20presents%20challenges.%20Quantization%20is%20a%20key%0Atechnique%20for%20model%20compression%20and%20acceleration.%20As%20an%20efficient%20approach%2C%0APost-Training%20Quantization%20%28PTQ%29%20is%20still%20in%20its%20nascent%20stages%20for%20video%0Amatting%2C%20facing%20significant%20hurdles%20in%20maintaining%20accuracy%20and%20temporal%0Acoherence.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20novel%20and%20general%0APTQ%20framework%20specifically%20designed%20for%20video%20matting%20models%2C%20marking%2C%20to%20the%0Abest%20of%20our%20knowledge%2C%20the%20first%20systematic%20attempt%20in%20this%20domain.%20Our%0Acontributions%20include%3A%20%281%29%20A%20two-stage%20PTQ%20strategy%20that%20combines%0Ablock-reconstruction-based%20optimization%20for%20fast%2C%20stable%20initial%20quantization%0Aand%20local%20dependency%20capture%2C%20followed%20by%20a%20global%20calibration%20of%20quantization%0Aparameters%20to%20minimize%20accuracy%20loss.%20%282%29%20A%20Statistically-Driven%20Global%20Affine%0ACalibration%20%28GAC%29%20method%20that%20enables%20the%20network%20to%20compensate%20for%20cumulative%0Astatistical%20distortions%20arising%20from%20factors%20such%20as%20neglected%20BN%20layer%0Aeffects%2C%20even%20reducing%20the%20error%20of%20existing%20PTQ%20methods%20on%20video%20matting%20tasks%0Aup%20to%2020%25.%20%283%29%20An%20Optical%20Flow%20Assistance%20%28OFA%29%20component%20that%20leverages%0Atemporal%20and%20semantic%20priors%20from%20frames%20to%20guide%20the%20PTQ%20process%2C%20enhancing%0Athe%20model%27s%20ability%20to%20distinguish%20moving%20foregrounds%20in%20complex%20scenes%20and%0Aultimately%20achieving%20near%20full-precision%20performance%20even%20under%20ultra-low-bit%0Aquantization.%20Comprehensive%20quantitative%20and%20visual%20results%20show%20that%20our%0APTQ4VM%20achieves%20the%20state-of-the-art%20accuracy%20performance%20across%20different%0Abit-widths%20compared%20to%20the%20existing%20quantization%20methods.%20We%20highlight%20that%20the%0A4-bit%20PTQ4VM%20even%20achieves%20performance%20close%20to%20the%20full-precision%20counterpart%0Awhile%20enjoying%208x%20FLOP%20savings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-Training%2520Quantization%2520for%2520Video%2520Matting%26entry.906535625%3DTianrui%2520Zhu%2520and%2520Houyuan%2520Chen%2520and%2520Ruihao%2520Gong%2520and%2520Michele%2520Magno%2520and%2520Haotong%2520Qin%2520and%2520Kai%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520matting%2520is%2520crucial%2520for%2520applications%2520such%2520as%2520film%2520production%2520and%2520virtual%250Areality%252C%2520yet%2520deploying%2520its%2520computationally%2520intensive%2520models%2520on%250Aresource-constrained%2520devices%2520presents%2520challenges.%2520Quantization%2520is%2520a%2520key%250Atechnique%2520for%2520model%2520compression%2520and%2520acceleration.%2520As%2520an%2520efficient%2520approach%252C%250APost-Training%2520Quantization%2520%2528PTQ%2529%2520is%2520still%2520in%2520its%2520nascent%2520stages%2520for%2520video%250Amatting%252C%2520facing%2520significant%2520hurdles%2520in%2520maintaining%2520accuracy%2520and%2520temporal%250Acoherence.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520and%2520general%250APTQ%2520framework%2520specifically%2520designed%2520for%2520video%2520matting%2520models%252C%2520marking%252C%2520to%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520the%2520first%2520systematic%2520attempt%2520in%2520this%2520domain.%2520Our%250Acontributions%2520include%253A%2520%25281%2529%2520A%2520two-stage%2520PTQ%2520strategy%2520that%2520combines%250Ablock-reconstruction-based%2520optimization%2520for%2520fast%252C%2520stable%2520initial%2520quantization%250Aand%2520local%2520dependency%2520capture%252C%2520followed%2520by%2520a%2520global%2520calibration%2520of%2520quantization%250Aparameters%2520to%2520minimize%2520accuracy%2520loss.%2520%25282%2529%2520A%2520Statistically-Driven%2520Global%2520Affine%250ACalibration%2520%2528GAC%2529%2520method%2520that%2520enables%2520the%2520network%2520to%2520compensate%2520for%2520cumulative%250Astatistical%2520distortions%2520arising%2520from%2520factors%2520such%2520as%2520neglected%2520BN%2520layer%250Aeffects%252C%2520even%2520reducing%2520the%2520error%2520of%2520existing%2520PTQ%2520methods%2520on%2520video%2520matting%2520tasks%250Aup%2520to%252020%2525.%2520%25283%2529%2520An%2520Optical%2520Flow%2520Assistance%2520%2528OFA%2529%2520component%2520that%2520leverages%250Atemporal%2520and%2520semantic%2520priors%2520from%2520frames%2520to%2520guide%2520the%2520PTQ%2520process%252C%2520enhancing%250Athe%2520model%2527s%2520ability%2520to%2520distinguish%2520moving%2520foregrounds%2520in%2520complex%2520scenes%2520and%250Aultimately%2520achieving%2520near%2520full-precision%2520performance%2520even%2520under%2520ultra-low-bit%250Aquantization.%2520Comprehensive%2520quantitative%2520and%2520visual%2520results%2520show%2520that%2520our%250APTQ4VM%2520achieves%2520the%2520state-of-the-art%2520accuracy%2520performance%2520across%2520different%250Abit-widths%2520compared%2520to%2520the%2520existing%2520quantization%2520methods.%2520We%2520highlight%2520that%2520the%250A4-bit%2520PTQ4VM%2520even%2520achieves%2520performance%2520close%2520to%2520the%2520full-precision%2520counterpart%250Awhile%2520enjoying%25208x%2520FLOP%2520savings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-Training%20Quantization%20for%20Video%20Matting&entry.906535625=Tianrui%20Zhu%20and%20Houyuan%20Chen%20and%20Ruihao%20Gong%20and%20Michele%20Magno%20and%20Haotong%20Qin%20and%20Kai%20Zhang&entry.1292438233=%20%20Video%20matting%20is%20crucial%20for%20applications%20such%20as%20film%20production%20and%20virtual%0Areality%2C%20yet%20deploying%20its%20computationally%20intensive%20models%20on%0Aresource-constrained%20devices%20presents%20challenges.%20Quantization%20is%20a%20key%0Atechnique%20for%20model%20compression%20and%20acceleration.%20As%20an%20efficient%20approach%2C%0APost-Training%20Quantization%20%28PTQ%29%20is%20still%20in%20its%20nascent%20stages%20for%20video%0Amatting%2C%20facing%20significant%20hurdles%20in%20maintaining%20accuracy%20and%20temporal%0Acoherence.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20novel%20and%20general%0APTQ%20framework%20specifically%20designed%20for%20video%20matting%20models%2C%20marking%2C%20to%20the%0Abest%20of%20our%20knowledge%2C%20the%20first%20systematic%20attempt%20in%20this%20domain.%20Our%0Acontributions%20include%3A%20%281%29%20A%20two-stage%20PTQ%20strategy%20that%20combines%0Ablock-reconstruction-based%20optimization%20for%20fast%2C%20stable%20initial%20quantization%0Aand%20local%20dependency%20capture%2C%20followed%20by%20a%20global%20calibration%20of%20quantization%0Aparameters%20to%20minimize%20accuracy%20loss.%20%282%29%20A%20Statistically-Driven%20Global%20Affine%0ACalibration%20%28GAC%29%20method%20that%20enables%20the%20network%20to%20compensate%20for%20cumulative%0Astatistical%20distortions%20arising%20from%20factors%20such%20as%20neglected%20BN%20layer%0Aeffects%2C%20even%20reducing%20the%20error%20of%20existing%20PTQ%20methods%20on%20video%20matting%20tasks%0Aup%20to%2020%25.%20%283%29%20An%20Optical%20Flow%20Assistance%20%28OFA%29%20component%20that%20leverages%0Atemporal%20and%20semantic%20priors%20from%20frames%20to%20guide%20the%20PTQ%20process%2C%20enhancing%0Athe%20model%27s%20ability%20to%20distinguish%20moving%20foregrounds%20in%20complex%20scenes%20and%0Aultimately%20achieving%20near%20full-precision%20performance%20even%20under%20ultra-low-bit%0Aquantization.%20Comprehensive%20quantitative%20and%20visual%20results%20show%20that%20our%0APTQ4VM%20achieves%20the%20state-of-the-art%20accuracy%20performance%20across%20different%0Abit-widths%20compared%20to%20the%20existing%20quantization%20methods.%20We%20highlight%20that%20the%0A4-bit%20PTQ4VM%20even%20achieves%20performance%20close%20to%20the%20full-precision%20counterpart%0Awhile%20enjoying%208x%20FLOP%20savings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10840v1&entry.124074799=Read"},
{"title": "Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial\n  Motion Capture", "author": "Chengxu Zuo and Jiawei Huang and Xiao Jiang and Yuan Yao and Xiangren Shi and Rui Cao and Xinyu Yi and Feng Xu and Shihui Guo and Yipeng Qin", "abstract": "  In this paper, we propose a novel dynamic calibration method for sparse\ninertial motion capture systems, which is the first to break the restrictive\nabsolute static assumption in IMU calibration, i.e., the coordinate drift RG'G\nand measurement offset RBS remain constant during the entire motion, thereby\nsignificantly expanding their application scenarios. Specifically, we achieve\nreal-time estimation of RG'G and RBS under two relaxed assumptions: i) the\nmatrices change negligibly in a short time window; ii) the human movements/IMU\nreadings are diverse in such a time window. Intuitively, the first assumption\nreduces the number of candidate matrices, and the second assumption provides\ndiverse constraints, which greatly reduces the solution space and allows for\naccurate estimation of RG'G and RBS from a short history of IMU readings in\nreal time. To achieve this, we created synthetic datasets of paired RG'G, RBS\nmatrices and IMU readings, and learned their mappings using a Transformer-based\nmodel. We also designed a calibration trigger based on the diversity of IMU\nreadings to ensure that assumption ii) is met before applying our method. To\nour knowledge, we are the first to achieve implicit IMU calibration (i.e.,\nseamlessly putting IMUs into use without the need for an explicit calibration\nprocess), as well as the first to enable long-term and accurate motion capture\nusing sparse IMUs. The code and dataset are available at\nhttps://github.com/ZuoCX1996/TIC.\n", "link": "http://arxiv.org/abs/2506.10580v1", "date": "2025-06-12", "relevancy": 2.2636, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5705}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5645}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer%20IMU%20Calibrator%3A%20Dynamic%20On-body%20IMU%20Calibration%20for%20Inertial%0A%20%20Motion%20Capture&body=Title%3A%20Transformer%20IMU%20Calibrator%3A%20Dynamic%20On-body%20IMU%20Calibration%20for%20Inertial%0A%20%20Motion%20Capture%0AAuthor%3A%20Chengxu%20Zuo%20and%20Jiawei%20Huang%20and%20Xiao%20Jiang%20and%20Yuan%20Yao%20and%20Xiangren%20Shi%20and%20Rui%20Cao%20and%20Xinyu%20Yi%20and%20Feng%20Xu%20and%20Shihui%20Guo%20and%20Yipeng%20Qin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20dynamic%20calibration%20method%20for%20sparse%0Ainertial%20motion%20capture%20systems%2C%20which%20is%20the%20first%20to%20break%20the%20restrictive%0Aabsolute%20static%20assumption%20in%20IMU%20calibration%2C%20i.e.%2C%20the%20coordinate%20drift%20RG%27G%0Aand%20measurement%20offset%20RBS%20remain%20constant%20during%20the%20entire%20motion%2C%20thereby%0Asignificantly%20expanding%20their%20application%20scenarios.%20Specifically%2C%20we%20achieve%0Areal-time%20estimation%20of%20RG%27G%20and%20RBS%20under%20two%20relaxed%20assumptions%3A%20i%29%20the%0Amatrices%20change%20negligibly%20in%20a%20short%20time%20window%3B%20ii%29%20the%20human%20movements/IMU%0Areadings%20are%20diverse%20in%20such%20a%20time%20window.%20Intuitively%2C%20the%20first%20assumption%0Areduces%20the%20number%20of%20candidate%20matrices%2C%20and%20the%20second%20assumption%20provides%0Adiverse%20constraints%2C%20which%20greatly%20reduces%20the%20solution%20space%20and%20allows%20for%0Aaccurate%20estimation%20of%20RG%27G%20and%20RBS%20from%20a%20short%20history%20of%20IMU%20readings%20in%0Areal%20time.%20To%20achieve%20this%2C%20we%20created%20synthetic%20datasets%20of%20paired%20RG%27G%2C%20RBS%0Amatrices%20and%20IMU%20readings%2C%20and%20learned%20their%20mappings%20using%20a%20Transformer-based%0Amodel.%20We%20also%20designed%20a%20calibration%20trigger%20based%20on%20the%20diversity%20of%20IMU%0Areadings%20to%20ensure%20that%20assumption%20ii%29%20is%20met%20before%20applying%20our%20method.%20To%0Aour%20knowledge%2C%20we%20are%20the%20first%20to%20achieve%20implicit%20IMU%20calibration%20%28i.e.%2C%0Aseamlessly%20putting%20IMUs%20into%20use%20without%20the%20need%20for%20an%20explicit%20calibration%0Aprocess%29%2C%20as%20well%20as%20the%20first%20to%20enable%20long-term%20and%20accurate%20motion%20capture%0Ausing%20sparse%20IMUs.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ZuoCX1996/TIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer%2520IMU%2520Calibrator%253A%2520Dynamic%2520On-body%2520IMU%2520Calibration%2520for%2520Inertial%250A%2520%2520Motion%2520Capture%26entry.906535625%3DChengxu%2520Zuo%2520and%2520Jiawei%2520Huang%2520and%2520Xiao%2520Jiang%2520and%2520Yuan%2520Yao%2520and%2520Xiangren%2520Shi%2520and%2520Rui%2520Cao%2520and%2520Xinyu%2520Yi%2520and%2520Feng%2520Xu%2520and%2520Shihui%2520Guo%2520and%2520Yipeng%2520Qin%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520dynamic%2520calibration%2520method%2520for%2520sparse%250Ainertial%2520motion%2520capture%2520systems%252C%2520which%2520is%2520the%2520first%2520to%2520break%2520the%2520restrictive%250Aabsolute%2520static%2520assumption%2520in%2520IMU%2520calibration%252C%2520i.e.%252C%2520the%2520coordinate%2520drift%2520RG%2527G%250Aand%2520measurement%2520offset%2520RBS%2520remain%2520constant%2520during%2520the%2520entire%2520motion%252C%2520thereby%250Asignificantly%2520expanding%2520their%2520application%2520scenarios.%2520Specifically%252C%2520we%2520achieve%250Areal-time%2520estimation%2520of%2520RG%2527G%2520and%2520RBS%2520under%2520two%2520relaxed%2520assumptions%253A%2520i%2529%2520the%250Amatrices%2520change%2520negligibly%2520in%2520a%2520short%2520time%2520window%253B%2520ii%2529%2520the%2520human%2520movements/IMU%250Areadings%2520are%2520diverse%2520in%2520such%2520a%2520time%2520window.%2520Intuitively%252C%2520the%2520first%2520assumption%250Areduces%2520the%2520number%2520of%2520candidate%2520matrices%252C%2520and%2520the%2520second%2520assumption%2520provides%250Adiverse%2520constraints%252C%2520which%2520greatly%2520reduces%2520the%2520solution%2520space%2520and%2520allows%2520for%250Aaccurate%2520estimation%2520of%2520RG%2527G%2520and%2520RBS%2520from%2520a%2520short%2520history%2520of%2520IMU%2520readings%2520in%250Areal%2520time.%2520To%2520achieve%2520this%252C%2520we%2520created%2520synthetic%2520datasets%2520of%2520paired%2520RG%2527G%252C%2520RBS%250Amatrices%2520and%2520IMU%2520readings%252C%2520and%2520learned%2520their%2520mappings%2520using%2520a%2520Transformer-based%250Amodel.%2520We%2520also%2520designed%2520a%2520calibration%2520trigger%2520based%2520on%2520the%2520diversity%2520of%2520IMU%250Areadings%2520to%2520ensure%2520that%2520assumption%2520ii%2529%2520is%2520met%2520before%2520applying%2520our%2520method.%2520To%250Aour%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520achieve%2520implicit%2520IMU%2520calibration%2520%2528i.e.%252C%250Aseamlessly%2520putting%2520IMUs%2520into%2520use%2520without%2520the%2520need%2520for%2520an%2520explicit%2520calibration%250Aprocess%2529%252C%2520as%2520well%2520as%2520the%2520first%2520to%2520enable%2520long-term%2520and%2520accurate%2520motion%2520capture%250Ausing%2520sparse%2520IMUs.%2520The%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/ZuoCX1996/TIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20IMU%20Calibrator%3A%20Dynamic%20On-body%20IMU%20Calibration%20for%20Inertial%0A%20%20Motion%20Capture&entry.906535625=Chengxu%20Zuo%20and%20Jiawei%20Huang%20and%20Xiao%20Jiang%20and%20Yuan%20Yao%20and%20Xiangren%20Shi%20and%20Rui%20Cao%20and%20Xinyu%20Yi%20and%20Feng%20Xu%20and%20Shihui%20Guo%20and%20Yipeng%20Qin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20dynamic%20calibration%20method%20for%20sparse%0Ainertial%20motion%20capture%20systems%2C%20which%20is%20the%20first%20to%20break%20the%20restrictive%0Aabsolute%20static%20assumption%20in%20IMU%20calibration%2C%20i.e.%2C%20the%20coordinate%20drift%20RG%27G%0Aand%20measurement%20offset%20RBS%20remain%20constant%20during%20the%20entire%20motion%2C%20thereby%0Asignificantly%20expanding%20their%20application%20scenarios.%20Specifically%2C%20we%20achieve%0Areal-time%20estimation%20of%20RG%27G%20and%20RBS%20under%20two%20relaxed%20assumptions%3A%20i%29%20the%0Amatrices%20change%20negligibly%20in%20a%20short%20time%20window%3B%20ii%29%20the%20human%20movements/IMU%0Areadings%20are%20diverse%20in%20such%20a%20time%20window.%20Intuitively%2C%20the%20first%20assumption%0Areduces%20the%20number%20of%20candidate%20matrices%2C%20and%20the%20second%20assumption%20provides%0Adiverse%20constraints%2C%20which%20greatly%20reduces%20the%20solution%20space%20and%20allows%20for%0Aaccurate%20estimation%20of%20RG%27G%20and%20RBS%20from%20a%20short%20history%20of%20IMU%20readings%20in%0Areal%20time.%20To%20achieve%20this%2C%20we%20created%20synthetic%20datasets%20of%20paired%20RG%27G%2C%20RBS%0Amatrices%20and%20IMU%20readings%2C%20and%20learned%20their%20mappings%20using%20a%20Transformer-based%0Amodel.%20We%20also%20designed%20a%20calibration%20trigger%20based%20on%20the%20diversity%20of%20IMU%0Areadings%20to%20ensure%20that%20assumption%20ii%29%20is%20met%20before%20applying%20our%20method.%20To%0Aour%20knowledge%2C%20we%20are%20the%20first%20to%20achieve%20implicit%20IMU%20calibration%20%28i.e.%2C%0Aseamlessly%20putting%20IMUs%20into%20use%20without%20the%20need%20for%20an%20explicit%20calibration%0Aprocess%29%2C%20as%20well%20as%20the%20first%20to%20enable%20long-term%20and%20accurate%20motion%20capture%0Ausing%20sparse%20IMUs.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ZuoCX1996/TIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10580v1&entry.124074799=Read"},
{"title": "Rethinking Random Masking in Self Distillation on ViT", "author": "Jihyeon Seong and Hyunkyung Han", "abstract": "  Vision Transformers (ViTs) have demonstrated remarkable performance across a\nwide range of vision tasks. In particular, self-distillation frameworks such as\nDINO have contributed significantly to these advances. Within such frameworks,\nrandom masking is often utilized to improve training efficiency and introduce\nregularization. However, recent studies have raised concerns that\nindiscriminate random masking may inadvertently eliminate critical semantic\ninformation, motivating the development of more informed masking strategies. In\nthis study, we explore the role of random masking in the self-distillation\nsetting, focusing on the DINO framework. Specifically, we apply random masking\nexclusively to the student's global view, while preserving the student's local\nviews and the teacher's global view in their original, unmasked forms. This\ndesign leverages DINO's multi-view augmentation scheme to retain clean\nsupervision while inducing robustness through masked inputs. We evaluate our\napproach using DINO-Tiny on the mini-ImageNet dataset and show that random\nmasking under this asymmetric setup yields more robust and fine-grained\nattention maps, ultimately enhancing downstream performance.\n", "link": "http://arxiv.org/abs/2506.10582v1", "date": "2025-06-12", "relevancy": 2.2606, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5812}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5537}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Random%20Masking%20in%20Self%20Distillation%20on%20ViT&body=Title%3A%20Rethinking%20Random%20Masking%20in%20Self%20Distillation%20on%20ViT%0AAuthor%3A%20Jihyeon%20Seong%20and%20Hyunkyung%20Han%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20remarkable%20performance%20across%20a%0Awide%20range%20of%20vision%20tasks.%20In%20particular%2C%20self-distillation%20frameworks%20such%20as%0ADINO%20have%20contributed%20significantly%20to%20these%20advances.%20Within%20such%20frameworks%2C%0Arandom%20masking%20is%20often%20utilized%20to%20improve%20training%20efficiency%20and%20introduce%0Aregularization.%20However%2C%20recent%20studies%20have%20raised%20concerns%20that%0Aindiscriminate%20random%20masking%20may%20inadvertently%20eliminate%20critical%20semantic%0Ainformation%2C%20motivating%20the%20development%20of%20more%20informed%20masking%20strategies.%20In%0Athis%20study%2C%20we%20explore%20the%20role%20of%20random%20masking%20in%20the%20self-distillation%0Asetting%2C%20focusing%20on%20the%20DINO%20framework.%20Specifically%2C%20we%20apply%20random%20masking%0Aexclusively%20to%20the%20student%27s%20global%20view%2C%20while%20preserving%20the%20student%27s%20local%0Aviews%20and%20the%20teacher%27s%20global%20view%20in%20their%20original%2C%20unmasked%20forms.%20This%0Adesign%20leverages%20DINO%27s%20multi-view%20augmentation%20scheme%20to%20retain%20clean%0Asupervision%20while%20inducing%20robustness%20through%20masked%20inputs.%20We%20evaluate%20our%0Aapproach%20using%20DINO-Tiny%20on%20the%20mini-ImageNet%20dataset%20and%20show%20that%20random%0Amasking%20under%20this%20asymmetric%20setup%20yields%20more%20robust%20and%20fine-grained%0Aattention%20maps%2C%20ultimately%20enhancing%20downstream%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Random%2520Masking%2520in%2520Self%2520Distillation%2520on%2520ViT%26entry.906535625%3DJihyeon%2520Seong%2520and%2520Hyunkyung%2520Han%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520a%250Awide%2520range%2520of%2520vision%2520tasks.%2520In%2520particular%252C%2520self-distillation%2520frameworks%2520such%2520as%250ADINO%2520have%2520contributed%2520significantly%2520to%2520these%2520advances.%2520Within%2520such%2520frameworks%252C%250Arandom%2520masking%2520is%2520often%2520utilized%2520to%2520improve%2520training%2520efficiency%2520and%2520introduce%250Aregularization.%2520However%252C%2520recent%2520studies%2520have%2520raised%2520concerns%2520that%250Aindiscriminate%2520random%2520masking%2520may%2520inadvertently%2520eliminate%2520critical%2520semantic%250Ainformation%252C%2520motivating%2520the%2520development%2520of%2520more%2520informed%2520masking%2520strategies.%2520In%250Athis%2520study%252C%2520we%2520explore%2520the%2520role%2520of%2520random%2520masking%2520in%2520the%2520self-distillation%250Asetting%252C%2520focusing%2520on%2520the%2520DINO%2520framework.%2520Specifically%252C%2520we%2520apply%2520random%2520masking%250Aexclusively%2520to%2520the%2520student%2527s%2520global%2520view%252C%2520while%2520preserving%2520the%2520student%2527s%2520local%250Aviews%2520and%2520the%2520teacher%2527s%2520global%2520view%2520in%2520their%2520original%252C%2520unmasked%2520forms.%2520This%250Adesign%2520leverages%2520DINO%2527s%2520multi-view%2520augmentation%2520scheme%2520to%2520retain%2520clean%250Asupervision%2520while%2520inducing%2520robustness%2520through%2520masked%2520inputs.%2520We%2520evaluate%2520our%250Aapproach%2520using%2520DINO-Tiny%2520on%2520the%2520mini-ImageNet%2520dataset%2520and%2520show%2520that%2520random%250Amasking%2520under%2520this%2520asymmetric%2520setup%2520yields%2520more%2520robust%2520and%2520fine-grained%250Aattention%2520maps%252C%2520ultimately%2520enhancing%2520downstream%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Random%20Masking%20in%20Self%20Distillation%20on%20ViT&entry.906535625=Jihyeon%20Seong%20and%20Hyunkyung%20Han&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20remarkable%20performance%20across%20a%0Awide%20range%20of%20vision%20tasks.%20In%20particular%2C%20self-distillation%20frameworks%20such%20as%0ADINO%20have%20contributed%20significantly%20to%20these%20advances.%20Within%20such%20frameworks%2C%0Arandom%20masking%20is%20often%20utilized%20to%20improve%20training%20efficiency%20and%20introduce%0Aregularization.%20However%2C%20recent%20studies%20have%20raised%20concerns%20that%0Aindiscriminate%20random%20masking%20may%20inadvertently%20eliminate%20critical%20semantic%0Ainformation%2C%20motivating%20the%20development%20of%20more%20informed%20masking%20strategies.%20In%0Athis%20study%2C%20we%20explore%20the%20role%20of%20random%20masking%20in%20the%20self-distillation%0Asetting%2C%20focusing%20on%20the%20DINO%20framework.%20Specifically%2C%20we%20apply%20random%20masking%0Aexclusively%20to%20the%20student%27s%20global%20view%2C%20while%20preserving%20the%20student%27s%20local%0Aviews%20and%20the%20teacher%27s%20global%20view%20in%20their%20original%2C%20unmasked%20forms.%20This%0Adesign%20leverages%20DINO%27s%20multi-view%20augmentation%20scheme%20to%20retain%20clean%0Asupervision%20while%20inducing%20robustness%20through%20masked%20inputs.%20We%20evaluate%20our%0Aapproach%20using%20DINO-Tiny%20on%20the%20mini-ImageNet%20dataset%20and%20show%20that%20random%0Amasking%20under%20this%20asymmetric%20setup%20yields%20more%20robust%20and%20fine-grained%0Aattention%20maps%2C%20ultimately%20enhancing%20downstream%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10582v1&entry.124074799=Read"},
{"title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning\n  in Transformers", "author": "Yixiao Huang and Hanlin Zhu and Tianyu Guo and Jiantao Jiao and Somayeh Sojoudi and Michael I. Jordan and Stuart Russell and Song Mei", "abstract": "  Large language models (LLMs) can acquire new knowledge through fine-tuning,\nbut this process exhibits a puzzling duality: models can generalize remarkably\nfrom new facts, yet are also prone to hallucinating incorrect information.\nHowever, the reasons for this phenomenon remain poorly understood. In this\nwork, we argue that both behaviors stem from a single mechanism known as\nout-of-context reasoning (OCR): the ability to deduce implications by\nassociating concepts, even those without a causal link. Our experiments across\nfive prominent LLMs confirm that OCR indeed drives both generalization and\nhallucination, depending on whether the associated concepts are causally\nrelated. To build a rigorous theoretical understanding of this phenomenon, we\nthen formalize OCR as a synthetic factual recall task. We empirically show that\na one-layer single-head attention-only transformer with factorized output and\nvalue matrices can learn to solve this task, while a model with combined\nweights cannot, highlighting the crucial role of matrix factorization. Our\ntheoretical analysis shows that the OCR capability can be attributed to the\nimplicit bias of gradient descent, which favors solutions that minimize the\nnuclear norm of the combined output-value matrix. This mathematical structure\nexplains why the model learns to associate facts and implications with high\nsample efficiency, regardless of whether the correlation is causal or merely\nspurious. Ultimately, our work provides a theoretical foundation for\nunderstanding the OCR phenomenon, offering a new lens for analyzing and\nmitigating undesirable behaviors from knowledge injection.\n", "link": "http://arxiv.org/abs/2506.10887v1", "date": "2025-06-12", "relevancy": 2.2562, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20or%20Hallucination%3F%20Understanding%20Out-of-Context%20Reasoning%0A%20%20in%20Transformers&body=Title%3A%20Generalization%20or%20Hallucination%3F%20Understanding%20Out-of-Context%20Reasoning%0A%20%20in%20Transformers%0AAuthor%3A%20Yixiao%20Huang%20and%20Hanlin%20Zhu%20and%20Tianyu%20Guo%20and%20Jiantao%20Jiao%20and%20Somayeh%20Sojoudi%20and%20Michael%20I.%20Jordan%20and%20Stuart%20Russell%20and%20Song%20Mei%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20acquire%20new%20knowledge%20through%20fine-tuning%2C%0Abut%20this%20process%20exhibits%20a%20puzzling%20duality%3A%20models%20can%20generalize%20remarkably%0Afrom%20new%20facts%2C%20yet%20are%20also%20prone%20to%20hallucinating%20incorrect%20information.%0AHowever%2C%20the%20reasons%20for%20this%20phenomenon%20remain%20poorly%20understood.%20In%20this%0Awork%2C%20we%20argue%20that%20both%20behaviors%20stem%20from%20a%20single%20mechanism%20known%20as%0Aout-of-context%20reasoning%20%28OCR%29%3A%20the%20ability%20to%20deduce%20implications%20by%0Aassociating%20concepts%2C%20even%20those%20without%20a%20causal%20link.%20Our%20experiments%20across%0Afive%20prominent%20LLMs%20confirm%20that%20OCR%20indeed%20drives%20both%20generalization%20and%0Ahallucination%2C%20depending%20on%20whether%20the%20associated%20concepts%20are%20causally%0Arelated.%20To%20build%20a%20rigorous%20theoretical%20understanding%20of%20this%20phenomenon%2C%20we%0Athen%20formalize%20OCR%20as%20a%20synthetic%20factual%20recall%20task.%20We%20empirically%20show%20that%0Aa%20one-layer%20single-head%20attention-only%20transformer%20with%20factorized%20output%20and%0Avalue%20matrices%20can%20learn%20to%20solve%20this%20task%2C%20while%20a%20model%20with%20combined%0Aweights%20cannot%2C%20highlighting%20the%20crucial%20role%20of%20matrix%20factorization.%20Our%0Atheoretical%20analysis%20shows%20that%20the%20OCR%20capability%20can%20be%20attributed%20to%20the%0Aimplicit%20bias%20of%20gradient%20descent%2C%20which%20favors%20solutions%20that%20minimize%20the%0Anuclear%20norm%20of%20the%20combined%20output-value%20matrix.%20This%20mathematical%20structure%0Aexplains%20why%20the%20model%20learns%20to%20associate%20facts%20and%20implications%20with%20high%0Asample%20efficiency%2C%20regardless%20of%20whether%20the%20correlation%20is%20causal%20or%20merely%0Aspurious.%20Ultimately%2C%20our%20work%20provides%20a%20theoretical%20foundation%20for%0Aunderstanding%20the%20OCR%20phenomenon%2C%20offering%20a%20new%20lens%20for%20analyzing%20and%0Amitigating%20undesirable%20behaviors%20from%20knowledge%20injection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520or%2520Hallucination%253F%2520Understanding%2520Out-of-Context%2520Reasoning%250A%2520%2520in%2520Transformers%26entry.906535625%3DYixiao%2520Huang%2520and%2520Hanlin%2520Zhu%2520and%2520Tianyu%2520Guo%2520and%2520Jiantao%2520Jiao%2520and%2520Somayeh%2520Sojoudi%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Stuart%2520Russell%2520and%2520Song%2520Mei%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520acquire%2520new%2520knowledge%2520through%2520fine-tuning%252C%250Abut%2520this%2520process%2520exhibits%2520a%2520puzzling%2520duality%253A%2520models%2520can%2520generalize%2520remarkably%250Afrom%2520new%2520facts%252C%2520yet%2520are%2520also%2520prone%2520to%2520hallucinating%2520incorrect%2520information.%250AHowever%252C%2520the%2520reasons%2520for%2520this%2520phenomenon%2520remain%2520poorly%2520understood.%2520In%2520this%250Awork%252C%2520we%2520argue%2520that%2520both%2520behaviors%2520stem%2520from%2520a%2520single%2520mechanism%2520known%2520as%250Aout-of-context%2520reasoning%2520%2528OCR%2529%253A%2520the%2520ability%2520to%2520deduce%2520implications%2520by%250Aassociating%2520concepts%252C%2520even%2520those%2520without%2520a%2520causal%2520link.%2520Our%2520experiments%2520across%250Afive%2520prominent%2520LLMs%2520confirm%2520that%2520OCR%2520indeed%2520drives%2520both%2520generalization%2520and%250Ahallucination%252C%2520depending%2520on%2520whether%2520the%2520associated%2520concepts%2520are%2520causally%250Arelated.%2520To%2520build%2520a%2520rigorous%2520theoretical%2520understanding%2520of%2520this%2520phenomenon%252C%2520we%250Athen%2520formalize%2520OCR%2520as%2520a%2520synthetic%2520factual%2520recall%2520task.%2520We%2520empirically%2520show%2520that%250Aa%2520one-layer%2520single-head%2520attention-only%2520transformer%2520with%2520factorized%2520output%2520and%250Avalue%2520matrices%2520can%2520learn%2520to%2520solve%2520this%2520task%252C%2520while%2520a%2520model%2520with%2520combined%250Aweights%2520cannot%252C%2520highlighting%2520the%2520crucial%2520role%2520of%2520matrix%2520factorization.%2520Our%250Atheoretical%2520analysis%2520shows%2520that%2520the%2520OCR%2520capability%2520can%2520be%2520attributed%2520to%2520the%250Aimplicit%2520bias%2520of%2520gradient%2520descent%252C%2520which%2520favors%2520solutions%2520that%2520minimize%2520the%250Anuclear%2520norm%2520of%2520the%2520combined%2520output-value%2520matrix.%2520This%2520mathematical%2520structure%250Aexplains%2520why%2520the%2520model%2520learns%2520to%2520associate%2520facts%2520and%2520implications%2520with%2520high%250Asample%2520efficiency%252C%2520regardless%2520of%2520whether%2520the%2520correlation%2520is%2520causal%2520or%2520merely%250Aspurious.%2520Ultimately%252C%2520our%2520work%2520provides%2520a%2520theoretical%2520foundation%2520for%250Aunderstanding%2520the%2520OCR%2520phenomenon%252C%2520offering%2520a%2520new%2520lens%2520for%2520analyzing%2520and%250Amitigating%2520undesirable%2520behaviors%2520from%2520knowledge%2520injection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20or%20Hallucination%3F%20Understanding%20Out-of-Context%20Reasoning%0A%20%20in%20Transformers&entry.906535625=Yixiao%20Huang%20and%20Hanlin%20Zhu%20and%20Tianyu%20Guo%20and%20Jiantao%20Jiao%20and%20Somayeh%20Sojoudi%20and%20Michael%20I.%20Jordan%20and%20Stuart%20Russell%20and%20Song%20Mei&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20acquire%20new%20knowledge%20through%20fine-tuning%2C%0Abut%20this%20process%20exhibits%20a%20puzzling%20duality%3A%20models%20can%20generalize%20remarkably%0Afrom%20new%20facts%2C%20yet%20are%20also%20prone%20to%20hallucinating%20incorrect%20information.%0AHowever%2C%20the%20reasons%20for%20this%20phenomenon%20remain%20poorly%20understood.%20In%20this%0Awork%2C%20we%20argue%20that%20both%20behaviors%20stem%20from%20a%20single%20mechanism%20known%20as%0Aout-of-context%20reasoning%20%28OCR%29%3A%20the%20ability%20to%20deduce%20implications%20by%0Aassociating%20concepts%2C%20even%20those%20without%20a%20causal%20link.%20Our%20experiments%20across%0Afive%20prominent%20LLMs%20confirm%20that%20OCR%20indeed%20drives%20both%20generalization%20and%0Ahallucination%2C%20depending%20on%20whether%20the%20associated%20concepts%20are%20causally%0Arelated.%20To%20build%20a%20rigorous%20theoretical%20understanding%20of%20this%20phenomenon%2C%20we%0Athen%20formalize%20OCR%20as%20a%20synthetic%20factual%20recall%20task.%20We%20empirically%20show%20that%0Aa%20one-layer%20single-head%20attention-only%20transformer%20with%20factorized%20output%20and%0Avalue%20matrices%20can%20learn%20to%20solve%20this%20task%2C%20while%20a%20model%20with%20combined%0Aweights%20cannot%2C%20highlighting%20the%20crucial%20role%20of%20matrix%20factorization.%20Our%0Atheoretical%20analysis%20shows%20that%20the%20OCR%20capability%20can%20be%20attributed%20to%20the%0Aimplicit%20bias%20of%20gradient%20descent%2C%20which%20favors%20solutions%20that%20minimize%20the%0Anuclear%20norm%20of%20the%20combined%20output-value%20matrix.%20This%20mathematical%20structure%0Aexplains%20why%20the%20model%20learns%20to%20associate%20facts%20and%20implications%20with%20high%0Asample%20efficiency%2C%20regardless%20of%20whether%20the%20correlation%20is%20causal%20or%20merely%0Aspurious.%20Ultimately%2C%20our%20work%20provides%20a%20theoretical%20foundation%20for%0Aunderstanding%20the%20OCR%20phenomenon%2C%20offering%20a%20new%20lens%20for%20analyzing%20and%0Amitigating%20undesirable%20behaviors%20from%20knowledge%20injection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10887v1&entry.124074799=Read"},
{"title": "Object-Centric Latent Action Learning", "author": "Albina Klepach and Alexander Nikulin and Ilya Zisman and Denis Tarasov and Alexander Derevyagin and Andrei Polubarov and Nikita Lyubaykin and Vladislav Kurenkov", "abstract": "  Leveraging vast amounts of unlabeled internet video data for embodied AI is\ncurrently bottlenecked by the lack of action labels and the presence of\naction-correlated visual distractors. Although recent latent action policy\noptimization (LAPO) has shown promise in inferring proxy-action labels from\nvisual observations, its performance degrades significantly when distractors\nare present. To address this limitation, we propose a novel object-centric\nlatent action learning framework that centers on objects rather than pixels. We\nleverage self-supervised object-centric pretraining to disentangle\naction-related and distracting dynamics. This allows LAPO to focus on\ntask-relevant interactions, resulting in more robust proxy-action labels,\nenabling better imitation learning and efficient adaptation of the agent with\njust a few action-labeled trajectories. We evaluated our method in eight\nvisually complex tasks across the Distracting Control Suite (DCS) and\nDistracting MetaWorld (DMW). Our results show that object-centric pretraining\nmitigates the negative effects of distractors by 50%, as measured by downstream\ntask performance: average return (DCS) and success rate (DMW).\n", "link": "http://arxiv.org/abs/2502.09680v2", "date": "2025-06-12", "relevancy": 2.2498, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5756}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Centric%20Latent%20Action%20Learning&body=Title%3A%20Object-Centric%20Latent%20Action%20Learning%0AAuthor%3A%20Albina%20Klepach%20and%20Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Denis%20Tarasov%20and%20Alexander%20Derevyagin%20and%20Andrei%20Polubarov%20and%20Nikita%20Lyubaykin%20and%20Vladislav%20Kurenkov%0AAbstract%3A%20%20%20Leveraging%20vast%20amounts%20of%20unlabeled%20internet%20video%20data%20for%20embodied%20AI%20is%0Acurrently%20bottlenecked%20by%20the%20lack%20of%20action%20labels%20and%20the%20presence%20of%0Aaction-correlated%20visual%20distractors.%20Although%20recent%20latent%20action%20policy%0Aoptimization%20%28LAPO%29%20has%20shown%20promise%20in%20inferring%20proxy-action%20labels%20from%0Avisual%20observations%2C%20its%20performance%20degrades%20significantly%20when%20distractors%0Aare%20present.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20object-centric%0Alatent%20action%20learning%20framework%20that%20centers%20on%20objects%20rather%20than%20pixels.%20We%0Aleverage%20self-supervised%20object-centric%20pretraining%20to%20disentangle%0Aaction-related%20and%20distracting%20dynamics.%20This%20allows%20LAPO%20to%20focus%20on%0Atask-relevant%20interactions%2C%20resulting%20in%20more%20robust%20proxy-action%20labels%2C%0Aenabling%20better%20imitation%20learning%20and%20efficient%20adaptation%20of%20the%20agent%20with%0Ajust%20a%20few%20action-labeled%20trajectories.%20We%20evaluated%20our%20method%20in%20eight%0Avisually%20complex%20tasks%20across%20the%20Distracting%20Control%20Suite%20%28DCS%29%20and%0ADistracting%20MetaWorld%20%28DMW%29.%20Our%20results%20show%20that%20object-centric%20pretraining%0Amitigates%20the%20negative%20effects%20of%20distractors%20by%2050%25%2C%20as%20measured%20by%20downstream%0Atask%20performance%3A%20average%20return%20%28DCS%29%20and%20success%20rate%20%28DMW%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09680v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Centric%2520Latent%2520Action%2520Learning%26entry.906535625%3DAlbina%2520Klepach%2520and%2520Alexander%2520Nikulin%2520and%2520Ilya%2520Zisman%2520and%2520Denis%2520Tarasov%2520and%2520Alexander%2520Derevyagin%2520and%2520Andrei%2520Polubarov%2520and%2520Nikita%2520Lyubaykin%2520and%2520Vladislav%2520Kurenkov%26entry.1292438233%3D%2520%2520Leveraging%2520vast%2520amounts%2520of%2520unlabeled%2520internet%2520video%2520data%2520for%2520embodied%2520AI%2520is%250Acurrently%2520bottlenecked%2520by%2520the%2520lack%2520of%2520action%2520labels%2520and%2520the%2520presence%2520of%250Aaction-correlated%2520visual%2520distractors.%2520Although%2520recent%2520latent%2520action%2520policy%250Aoptimization%2520%2528LAPO%2529%2520has%2520shown%2520promise%2520in%2520inferring%2520proxy-action%2520labels%2520from%250Avisual%2520observations%252C%2520its%2520performance%2520degrades%2520significantly%2520when%2520distractors%250Aare%2520present.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520object-centric%250Alatent%2520action%2520learning%2520framework%2520that%2520centers%2520on%2520objects%2520rather%2520than%2520pixels.%2520We%250Aleverage%2520self-supervised%2520object-centric%2520pretraining%2520to%2520disentangle%250Aaction-related%2520and%2520distracting%2520dynamics.%2520This%2520allows%2520LAPO%2520to%2520focus%2520on%250Atask-relevant%2520interactions%252C%2520resulting%2520in%2520more%2520robust%2520proxy-action%2520labels%252C%250Aenabling%2520better%2520imitation%2520learning%2520and%2520efficient%2520adaptation%2520of%2520the%2520agent%2520with%250Ajust%2520a%2520few%2520action-labeled%2520trajectories.%2520We%2520evaluated%2520our%2520method%2520in%2520eight%250Avisually%2520complex%2520tasks%2520across%2520the%2520Distracting%2520Control%2520Suite%2520%2528DCS%2529%2520and%250ADistracting%2520MetaWorld%2520%2528DMW%2529.%2520Our%2520results%2520show%2520that%2520object-centric%2520pretraining%250Amitigates%2520the%2520negative%2520effects%2520of%2520distractors%2520by%252050%2525%252C%2520as%2520measured%2520by%2520downstream%250Atask%2520performance%253A%2520average%2520return%2520%2528DCS%2529%2520and%2520success%2520rate%2520%2528DMW%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09680v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Centric%20Latent%20Action%20Learning&entry.906535625=Albina%20Klepach%20and%20Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Denis%20Tarasov%20and%20Alexander%20Derevyagin%20and%20Andrei%20Polubarov%20and%20Nikita%20Lyubaykin%20and%20Vladislav%20Kurenkov&entry.1292438233=%20%20Leveraging%20vast%20amounts%20of%20unlabeled%20internet%20video%20data%20for%20embodied%20AI%20is%0Acurrently%20bottlenecked%20by%20the%20lack%20of%20action%20labels%20and%20the%20presence%20of%0Aaction-correlated%20visual%20distractors.%20Although%20recent%20latent%20action%20policy%0Aoptimization%20%28LAPO%29%20has%20shown%20promise%20in%20inferring%20proxy-action%20labels%20from%0Avisual%20observations%2C%20its%20performance%20degrades%20significantly%20when%20distractors%0Aare%20present.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20object-centric%0Alatent%20action%20learning%20framework%20that%20centers%20on%20objects%20rather%20than%20pixels.%20We%0Aleverage%20self-supervised%20object-centric%20pretraining%20to%20disentangle%0Aaction-related%20and%20distracting%20dynamics.%20This%20allows%20LAPO%20to%20focus%20on%0Atask-relevant%20interactions%2C%20resulting%20in%20more%20robust%20proxy-action%20labels%2C%0Aenabling%20better%20imitation%20learning%20and%20efficient%20adaptation%20of%20the%20agent%20with%0Ajust%20a%20few%20action-labeled%20trajectories.%20We%20evaluated%20our%20method%20in%20eight%0Avisually%20complex%20tasks%20across%20the%20Distracting%20Control%20Suite%20%28DCS%29%20and%0ADistracting%20MetaWorld%20%28DMW%29.%20Our%20results%20show%20that%20object-centric%20pretraining%0Amitigates%20the%20negative%20effects%20of%20distractors%20by%2050%25%2C%20as%20measured%20by%20downstream%0Atask%20performance%3A%20average%20return%20%28DCS%29%20and%20success%20rate%20%28DMW%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09680v2&entry.124074799=Read"},
{"title": "AIR: Zero-shot Generative Model Adaptation with Iterative Refinement", "author": "Guimeng Liu and Milad Abdollahzadeh and Ngai-Man Cheung", "abstract": "  Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained\ngenerator to a target domain using only text guidance and without any samples\nfrom the target domain. Central to recent ZSGM approaches are directional loss\nwhich use the text guidance in the form of aligning the image offset with text\noffset in the embedding space of a vision-language model like CLIP. This is\nsimilar to the analogical reasoning in NLP where the offset between one pair of\nwords is used to identify a missing element in another pair by aligning the\noffset between these two pairs. However, a major limitation of existing ZSGM\nmethods is that the learning objective assumes the complete alignment between\nimage offset and text offset in the CLIP embedding space, resulting in quality\ndegrade in generated images. Our work makes two main contributions. Inspired by\nthe offset misalignment studies in NLP, as our first contribution, we perform\nan empirical study to analyze the misalignment between text offset and image\noffset in CLIP embedding space for various large publicly available datasets.\nOur important finding is that offset misalignment in CLIP embedding space is\ncorrelated with concept distance, i.e., close concepts have a less offset\nmisalignment. To address the limitations of the current approaches, as our\nsecond contribution, we propose Adaptation with Iterative Refinement (AIR)\nwhich is the first ZSGM approach to focus on improving target domain image\nquality based on our new insight on offset misalignment.Qualitative,\nquantitative, and user study in 26 experiment setups consistently demonstrate\nthe proposed AIR approach achieves SOTA performance. Additional experiments are\nin Supp.\n", "link": "http://arxiv.org/abs/2506.10895v1", "date": "2025-06-12", "relevancy": 2.2484, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5542}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIR%3A%20Zero-shot%20Generative%20Model%20Adaptation%20with%20Iterative%20Refinement&body=Title%3A%20AIR%3A%20Zero-shot%20Generative%20Model%20Adaptation%20with%20Iterative%20Refinement%0AAuthor%3A%20Guimeng%20Liu%20and%20Milad%20Abdollahzadeh%20and%20Ngai-Man%20Cheung%0AAbstract%3A%20%20%20Zero-shot%20generative%20model%20adaptation%20%28ZSGM%29%20aims%20to%20adapt%20a%20pre-trained%0Agenerator%20to%20a%20target%20domain%20using%20only%20text%20guidance%20and%20without%20any%20samples%0Afrom%20the%20target%20domain.%20Central%20to%20recent%20ZSGM%20approaches%20are%20directional%20loss%0Awhich%20use%20the%20text%20guidance%20in%20the%20form%20of%20aligning%20the%20image%20offset%20with%20text%0Aoffset%20in%20the%20embedding%20space%20of%20a%20vision-language%20model%20like%20CLIP.%20This%20is%0Asimilar%20to%20the%20analogical%20reasoning%20in%20NLP%20where%20the%20offset%20between%20one%20pair%20of%0Awords%20is%20used%20to%20identify%20a%20missing%20element%20in%20another%20pair%20by%20aligning%20the%0Aoffset%20between%20these%20two%20pairs.%20However%2C%20a%20major%20limitation%20of%20existing%20ZSGM%0Amethods%20is%20that%20the%20learning%20objective%20assumes%20the%20complete%20alignment%20between%0Aimage%20offset%20and%20text%20offset%20in%20the%20CLIP%20embedding%20space%2C%20resulting%20in%20quality%0Adegrade%20in%20generated%20images.%20Our%20work%20makes%20two%20main%20contributions.%20Inspired%20by%0Athe%20offset%20misalignment%20studies%20in%20NLP%2C%20as%20our%20first%20contribution%2C%20we%20perform%0Aan%20empirical%20study%20to%20analyze%20the%20misalignment%20between%20text%20offset%20and%20image%0Aoffset%20in%20CLIP%20embedding%20space%20for%20various%20large%20publicly%20available%20datasets.%0AOur%20important%20finding%20is%20that%20offset%20misalignment%20in%20CLIP%20embedding%20space%20is%0Acorrelated%20with%20concept%20distance%2C%20i.e.%2C%20close%20concepts%20have%20a%20less%20offset%0Amisalignment.%20To%20address%20the%20limitations%20of%20the%20current%20approaches%2C%20as%20our%0Asecond%20contribution%2C%20we%20propose%20Adaptation%20with%20Iterative%20Refinement%20%28AIR%29%0Awhich%20is%20the%20first%20ZSGM%20approach%20to%20focus%20on%20improving%20target%20domain%20image%0Aquality%20based%20on%20our%20new%20insight%20on%20offset%20misalignment.Qualitative%2C%0Aquantitative%2C%20and%20user%20study%20in%2026%20experiment%20setups%20consistently%20demonstrate%0Athe%20proposed%20AIR%20approach%20achieves%20SOTA%20performance.%20Additional%20experiments%20are%0Ain%20Supp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIR%253A%2520Zero-shot%2520Generative%2520Model%2520Adaptation%2520with%2520Iterative%2520Refinement%26entry.906535625%3DGuimeng%2520Liu%2520and%2520Milad%2520Abdollahzadeh%2520and%2520Ngai-Man%2520Cheung%26entry.1292438233%3D%2520%2520Zero-shot%2520generative%2520model%2520adaptation%2520%2528ZSGM%2529%2520aims%2520to%2520adapt%2520a%2520pre-trained%250Agenerator%2520to%2520a%2520target%2520domain%2520using%2520only%2520text%2520guidance%2520and%2520without%2520any%2520samples%250Afrom%2520the%2520target%2520domain.%2520Central%2520to%2520recent%2520ZSGM%2520approaches%2520are%2520directional%2520loss%250Awhich%2520use%2520the%2520text%2520guidance%2520in%2520the%2520form%2520of%2520aligning%2520the%2520image%2520offset%2520with%2520text%250Aoffset%2520in%2520the%2520embedding%2520space%2520of%2520a%2520vision-language%2520model%2520like%2520CLIP.%2520This%2520is%250Asimilar%2520to%2520the%2520analogical%2520reasoning%2520in%2520NLP%2520where%2520the%2520offset%2520between%2520one%2520pair%2520of%250Awords%2520is%2520used%2520to%2520identify%2520a%2520missing%2520element%2520in%2520another%2520pair%2520by%2520aligning%2520the%250Aoffset%2520between%2520these%2520two%2520pairs.%2520However%252C%2520a%2520major%2520limitation%2520of%2520existing%2520ZSGM%250Amethods%2520is%2520that%2520the%2520learning%2520objective%2520assumes%2520the%2520complete%2520alignment%2520between%250Aimage%2520offset%2520and%2520text%2520offset%2520in%2520the%2520CLIP%2520embedding%2520space%252C%2520resulting%2520in%2520quality%250Adegrade%2520in%2520generated%2520images.%2520Our%2520work%2520makes%2520two%2520main%2520contributions.%2520Inspired%2520by%250Athe%2520offset%2520misalignment%2520studies%2520in%2520NLP%252C%2520as%2520our%2520first%2520contribution%252C%2520we%2520perform%250Aan%2520empirical%2520study%2520to%2520analyze%2520the%2520misalignment%2520between%2520text%2520offset%2520and%2520image%250Aoffset%2520in%2520CLIP%2520embedding%2520space%2520for%2520various%2520large%2520publicly%2520available%2520datasets.%250AOur%2520important%2520finding%2520is%2520that%2520offset%2520misalignment%2520in%2520CLIP%2520embedding%2520space%2520is%250Acorrelated%2520with%2520concept%2520distance%252C%2520i.e.%252C%2520close%2520concepts%2520have%2520a%2520less%2520offset%250Amisalignment.%2520To%2520address%2520the%2520limitations%2520of%2520the%2520current%2520approaches%252C%2520as%2520our%250Asecond%2520contribution%252C%2520we%2520propose%2520Adaptation%2520with%2520Iterative%2520Refinement%2520%2528AIR%2529%250Awhich%2520is%2520the%2520first%2520ZSGM%2520approach%2520to%2520focus%2520on%2520improving%2520target%2520domain%2520image%250Aquality%2520based%2520on%2520our%2520new%2520insight%2520on%2520offset%2520misalignment.Qualitative%252C%250Aquantitative%252C%2520and%2520user%2520study%2520in%252026%2520experiment%2520setups%2520consistently%2520demonstrate%250Athe%2520proposed%2520AIR%2520approach%2520achieves%2520SOTA%2520performance.%2520Additional%2520experiments%2520are%250Ain%2520Supp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIR%3A%20Zero-shot%20Generative%20Model%20Adaptation%20with%20Iterative%20Refinement&entry.906535625=Guimeng%20Liu%20and%20Milad%20Abdollahzadeh%20and%20Ngai-Man%20Cheung&entry.1292438233=%20%20Zero-shot%20generative%20model%20adaptation%20%28ZSGM%29%20aims%20to%20adapt%20a%20pre-trained%0Agenerator%20to%20a%20target%20domain%20using%20only%20text%20guidance%20and%20without%20any%20samples%0Afrom%20the%20target%20domain.%20Central%20to%20recent%20ZSGM%20approaches%20are%20directional%20loss%0Awhich%20use%20the%20text%20guidance%20in%20the%20form%20of%20aligning%20the%20image%20offset%20with%20text%0Aoffset%20in%20the%20embedding%20space%20of%20a%20vision-language%20model%20like%20CLIP.%20This%20is%0Asimilar%20to%20the%20analogical%20reasoning%20in%20NLP%20where%20the%20offset%20between%20one%20pair%20of%0Awords%20is%20used%20to%20identify%20a%20missing%20element%20in%20another%20pair%20by%20aligning%20the%0Aoffset%20between%20these%20two%20pairs.%20However%2C%20a%20major%20limitation%20of%20existing%20ZSGM%0Amethods%20is%20that%20the%20learning%20objective%20assumes%20the%20complete%20alignment%20between%0Aimage%20offset%20and%20text%20offset%20in%20the%20CLIP%20embedding%20space%2C%20resulting%20in%20quality%0Adegrade%20in%20generated%20images.%20Our%20work%20makes%20two%20main%20contributions.%20Inspired%20by%0Athe%20offset%20misalignment%20studies%20in%20NLP%2C%20as%20our%20first%20contribution%2C%20we%20perform%0Aan%20empirical%20study%20to%20analyze%20the%20misalignment%20between%20text%20offset%20and%20image%0Aoffset%20in%20CLIP%20embedding%20space%20for%20various%20large%20publicly%20available%20datasets.%0AOur%20important%20finding%20is%20that%20offset%20misalignment%20in%20CLIP%20embedding%20space%20is%0Acorrelated%20with%20concept%20distance%2C%20i.e.%2C%20close%20concepts%20have%20a%20less%20offset%0Amisalignment.%20To%20address%20the%20limitations%20of%20the%20current%20approaches%2C%20as%20our%0Asecond%20contribution%2C%20we%20propose%20Adaptation%20with%20Iterative%20Refinement%20%28AIR%29%0Awhich%20is%20the%20first%20ZSGM%20approach%20to%20focus%20on%20improving%20target%20domain%20image%0Aquality%20based%20on%20our%20new%20insight%20on%20offset%20misalignment.Qualitative%2C%0Aquantitative%2C%20and%20user%20study%20in%2026%20experiment%20setups%20consistently%20demonstrate%0Athe%20proposed%20AIR%20approach%20achieves%20SOTA%20performance.%20Additional%20experiments%20are%0Ain%20Supp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10895v1&entry.124074799=Read"},
{"title": "Prompts to Summaries: Zero-Shot Language-Guided Video Summarization", "author": "Mario Barbara and Alaa Maalouf", "abstract": "  The explosive growth of video data intensified the need for flexible\nuser-controllable summarization tools that can operate without domain-specific\ntraining data. Existing methods either rely on datasets, limiting\ngeneralization, or cannot incorporate user intent expressed in natural\nlanguage. We introduce Prompts-to-Summaries: the first zero-shot,\ntext-queryable video summarizer that converts off-the-shelf video-language\nmodels (VidLMs) captions into user-guided skims via large language models\n(LLMs) judging, without the use of training data at all, beating all\nunsupervised and matching supervised methods. Our pipeline (i) segments raw\nvideo footage into coherent scenes, (ii) generates rich scene-level\ndescriptions through a memory-efficient, batch-style VidLM prompting scheme\nthat scales to hours-long videos on a single GPU, (iii) leverages an LLM as a\njudge to assign scene-level importance scores under a carefully crafted prompt,\nand finally, (iv) propagates those scores to short segments level via two new\nmetrics: consistency (temporal coherency) and uniqueness (novelty), yielding\nfine-grained frame importance. On SumMe and TVSum, our data-free approach\nsurpasses all prior data-hungry unsupervised methods. It also performs\ncompetitively on the Query-Focused Video Summarization (QFVS) benchmark,\ndespite using no training data and the competing methods requiring supervised\nframe-level importance. To spur further research, we release VidSum-Reason, a\nnew query-driven dataset featuring long-tailed concepts and multi-step\nreasoning; our framework attains robust F1 scores and serves as the first\nchallenging baseline. Overall, our results demonstrate that pretrained\nmultimodal models, when orchestrated with principled prompting and score\npropagation, already provide a powerful foundation for universal,\ntext-queryable video summarization.\n", "link": "http://arxiv.org/abs/2506.10807v1", "date": "2025-06-12", "relevancy": 2.2396, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5633}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompts%20to%20Summaries%3A%20Zero-Shot%20Language-Guided%20Video%20Summarization&body=Title%3A%20Prompts%20to%20Summaries%3A%20Zero-Shot%20Language-Guided%20Video%20Summarization%0AAuthor%3A%20Mario%20Barbara%20and%20Alaa%20Maalouf%0AAbstract%3A%20%20%20The%20explosive%20growth%20of%20video%20data%20intensified%20the%20need%20for%20flexible%0Auser-controllable%20summarization%20tools%20that%20can%20operate%20without%20domain-specific%0Atraining%20data.%20Existing%20methods%20either%20rely%20on%20datasets%2C%20limiting%0Ageneralization%2C%20or%20cannot%20incorporate%20user%20intent%20expressed%20in%20natural%0Alanguage.%20We%20introduce%20Prompts-to-Summaries%3A%20the%20first%20zero-shot%2C%0Atext-queryable%20video%20summarizer%20that%20converts%20off-the-shelf%20video-language%0Amodels%20%28VidLMs%29%20captions%20into%20user-guided%20skims%20via%20large%20language%20models%0A%28LLMs%29%20judging%2C%20without%20the%20use%20of%20training%20data%20at%20all%2C%20beating%20all%0Aunsupervised%20and%20matching%20supervised%20methods.%20Our%20pipeline%20%28i%29%20segments%20raw%0Avideo%20footage%20into%20coherent%20scenes%2C%20%28ii%29%20generates%20rich%20scene-level%0Adescriptions%20through%20a%20memory-efficient%2C%20batch-style%20VidLM%20prompting%20scheme%0Athat%20scales%20to%20hours-long%20videos%20on%20a%20single%20GPU%2C%20%28iii%29%20leverages%20an%20LLM%20as%20a%0Ajudge%20to%20assign%20scene-level%20importance%20scores%20under%20a%20carefully%20crafted%20prompt%2C%0Aand%20finally%2C%20%28iv%29%20propagates%20those%20scores%20to%20short%20segments%20level%20via%20two%20new%0Ametrics%3A%20consistency%20%28temporal%20coherency%29%20and%20uniqueness%20%28novelty%29%2C%20yielding%0Afine-grained%20frame%20importance.%20On%20SumMe%20and%20TVSum%2C%20our%20data-free%20approach%0Asurpasses%20all%20prior%20data-hungry%20unsupervised%20methods.%20It%20also%20performs%0Acompetitively%20on%20the%20Query-Focused%20Video%20Summarization%20%28QFVS%29%20benchmark%2C%0Adespite%20using%20no%20training%20data%20and%20the%20competing%20methods%20requiring%20supervised%0Aframe-level%20importance.%20To%20spur%20further%20research%2C%20we%20release%20VidSum-Reason%2C%20a%0Anew%20query-driven%20dataset%20featuring%20long-tailed%20concepts%20and%20multi-step%0Areasoning%3B%20our%20framework%20attains%20robust%20F1%20scores%20and%20serves%20as%20the%20first%0Achallenging%20baseline.%20Overall%2C%20our%20results%20demonstrate%20that%20pretrained%0Amultimodal%20models%2C%20when%20orchestrated%20with%20principled%20prompting%20and%20score%0Apropagation%2C%20already%20provide%20a%20powerful%20foundation%20for%20universal%2C%0Atext-queryable%20video%20summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompts%2520to%2520Summaries%253A%2520Zero-Shot%2520Language-Guided%2520Video%2520Summarization%26entry.906535625%3DMario%2520Barbara%2520and%2520Alaa%2520Maalouf%26entry.1292438233%3D%2520%2520The%2520explosive%2520growth%2520of%2520video%2520data%2520intensified%2520the%2520need%2520for%2520flexible%250Auser-controllable%2520summarization%2520tools%2520that%2520can%2520operate%2520without%2520domain-specific%250Atraining%2520data.%2520Existing%2520methods%2520either%2520rely%2520on%2520datasets%252C%2520limiting%250Ageneralization%252C%2520or%2520cannot%2520incorporate%2520user%2520intent%2520expressed%2520in%2520natural%250Alanguage.%2520We%2520introduce%2520Prompts-to-Summaries%253A%2520the%2520first%2520zero-shot%252C%250Atext-queryable%2520video%2520summarizer%2520that%2520converts%2520off-the-shelf%2520video-language%250Amodels%2520%2528VidLMs%2529%2520captions%2520into%2520user-guided%2520skims%2520via%2520large%2520language%2520models%250A%2528LLMs%2529%2520judging%252C%2520without%2520the%2520use%2520of%2520training%2520data%2520at%2520all%252C%2520beating%2520all%250Aunsupervised%2520and%2520matching%2520supervised%2520methods.%2520Our%2520pipeline%2520%2528i%2529%2520segments%2520raw%250Avideo%2520footage%2520into%2520coherent%2520scenes%252C%2520%2528ii%2529%2520generates%2520rich%2520scene-level%250Adescriptions%2520through%2520a%2520memory-efficient%252C%2520batch-style%2520VidLM%2520prompting%2520scheme%250Athat%2520scales%2520to%2520hours-long%2520videos%2520on%2520a%2520single%2520GPU%252C%2520%2528iii%2529%2520leverages%2520an%2520LLM%2520as%2520a%250Ajudge%2520to%2520assign%2520scene-level%2520importance%2520scores%2520under%2520a%2520carefully%2520crafted%2520prompt%252C%250Aand%2520finally%252C%2520%2528iv%2529%2520propagates%2520those%2520scores%2520to%2520short%2520segments%2520level%2520via%2520two%2520new%250Ametrics%253A%2520consistency%2520%2528temporal%2520coherency%2529%2520and%2520uniqueness%2520%2528novelty%2529%252C%2520yielding%250Afine-grained%2520frame%2520importance.%2520On%2520SumMe%2520and%2520TVSum%252C%2520our%2520data-free%2520approach%250Asurpasses%2520all%2520prior%2520data-hungry%2520unsupervised%2520methods.%2520It%2520also%2520performs%250Acompetitively%2520on%2520the%2520Query-Focused%2520Video%2520Summarization%2520%2528QFVS%2529%2520benchmark%252C%250Adespite%2520using%2520no%2520training%2520data%2520and%2520the%2520competing%2520methods%2520requiring%2520supervised%250Aframe-level%2520importance.%2520To%2520spur%2520further%2520research%252C%2520we%2520release%2520VidSum-Reason%252C%2520a%250Anew%2520query-driven%2520dataset%2520featuring%2520long-tailed%2520concepts%2520and%2520multi-step%250Areasoning%253B%2520our%2520framework%2520attains%2520robust%2520F1%2520scores%2520and%2520serves%2520as%2520the%2520first%250Achallenging%2520baseline.%2520Overall%252C%2520our%2520results%2520demonstrate%2520that%2520pretrained%250Amultimodal%2520models%252C%2520when%2520orchestrated%2520with%2520principled%2520prompting%2520and%2520score%250Apropagation%252C%2520already%2520provide%2520a%2520powerful%2520foundation%2520for%2520universal%252C%250Atext-queryable%2520video%2520summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompts%20to%20Summaries%3A%20Zero-Shot%20Language-Guided%20Video%20Summarization&entry.906535625=Mario%20Barbara%20and%20Alaa%20Maalouf&entry.1292438233=%20%20The%20explosive%20growth%20of%20video%20data%20intensified%20the%20need%20for%20flexible%0Auser-controllable%20summarization%20tools%20that%20can%20operate%20without%20domain-specific%0Atraining%20data.%20Existing%20methods%20either%20rely%20on%20datasets%2C%20limiting%0Ageneralization%2C%20or%20cannot%20incorporate%20user%20intent%20expressed%20in%20natural%0Alanguage.%20We%20introduce%20Prompts-to-Summaries%3A%20the%20first%20zero-shot%2C%0Atext-queryable%20video%20summarizer%20that%20converts%20off-the-shelf%20video-language%0Amodels%20%28VidLMs%29%20captions%20into%20user-guided%20skims%20via%20large%20language%20models%0A%28LLMs%29%20judging%2C%20without%20the%20use%20of%20training%20data%20at%20all%2C%20beating%20all%0Aunsupervised%20and%20matching%20supervised%20methods.%20Our%20pipeline%20%28i%29%20segments%20raw%0Avideo%20footage%20into%20coherent%20scenes%2C%20%28ii%29%20generates%20rich%20scene-level%0Adescriptions%20through%20a%20memory-efficient%2C%20batch-style%20VidLM%20prompting%20scheme%0Athat%20scales%20to%20hours-long%20videos%20on%20a%20single%20GPU%2C%20%28iii%29%20leverages%20an%20LLM%20as%20a%0Ajudge%20to%20assign%20scene-level%20importance%20scores%20under%20a%20carefully%20crafted%20prompt%2C%0Aand%20finally%2C%20%28iv%29%20propagates%20those%20scores%20to%20short%20segments%20level%20via%20two%20new%0Ametrics%3A%20consistency%20%28temporal%20coherency%29%20and%20uniqueness%20%28novelty%29%2C%20yielding%0Afine-grained%20frame%20importance.%20On%20SumMe%20and%20TVSum%2C%20our%20data-free%20approach%0Asurpasses%20all%20prior%20data-hungry%20unsupervised%20methods.%20It%20also%20performs%0Acompetitively%20on%20the%20Query-Focused%20Video%20Summarization%20%28QFVS%29%20benchmark%2C%0Adespite%20using%20no%20training%20data%20and%20the%20competing%20methods%20requiring%20supervised%0Aframe-level%20importance.%20To%20spur%20further%20research%2C%20we%20release%20VidSum-Reason%2C%20a%0Anew%20query-driven%20dataset%20featuring%20long-tailed%20concepts%20and%20multi-step%0Areasoning%3B%20our%20framework%20attains%20robust%20F1%20scores%20and%20serves%20as%20the%20first%0Achallenging%20baseline.%20Overall%2C%20our%20results%20demonstrate%20that%20pretrained%0Amultimodal%20models%2C%20when%20orchestrated%20with%20principled%20prompting%20and%20score%0Apropagation%2C%20already%20provide%20a%20powerful%20foundation%20for%20universal%2C%0Atext-queryable%20video%20summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10807v1&entry.124074799=Read"},
{"title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective\n  Instance-Specific Pansharpening on Real-World Data", "author": "Tianyu Xin and Jin-Liang Xiao and Zeyu Xia and Shan Yin and Liang-Jian Deng", "abstract": "  Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU.\n", "link": "http://arxiv.org/abs/2504.10242v2", "date": "2025-06-12", "relevancy": 2.2351, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5617}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT%3A%20A%20Conditional%20Adaptation%20Tailor%20for%20Efficient%20and%20Effective%0A%20%20Instance-Specific%20Pansharpening%20on%20Real-World%20Data&body=Title%3A%20CAT%3A%20A%20Conditional%20Adaptation%20Tailor%20for%20Efficient%20and%20Effective%0A%20%20Instance-Specific%20Pansharpening%20on%20Real-World%20Data%0AAuthor%3A%20Tianyu%20Xin%20and%20Jin-Liang%20Xiao%20and%20Zeyu%20Xia%20and%20Shan%20Yin%20and%20Liang-Jian%20Deng%0AAbstract%3A%20%20%20Pansharpening%20is%20a%20crucial%20remote%20sensing%20technique%20that%20fuses%20low-resolution%0Amultispectral%20%28LRMS%29%20images%20with%20high-resolution%20panchromatic%20%28PAN%29%20images%20to%0Agenerate%20high-resolution%20multispectral%20%28HRMS%29%20imagery.%20Although%20deep%20learning%0Atechniques%20have%20significantly%20advanced%20pansharpening%2C%20many%20existing%20methods%0Asuffer%20from%20limited%20cross-sensor%20generalization%20and%20high%20computational%0Aoverhead%2C%20restricting%20their%20real-time%20applications.%20To%20address%20these%0Achallenges%2C%20we%20propose%20an%20efficient%20framework%20that%20quickly%20adapts%20to%20a%20specific%0Ainput%20instance%2C%20completing%20both%20training%20and%20inference%20in%20a%20short%20time.%20Our%0Aframework%20splits%20the%20input%20image%20into%20multiple%20patches%2C%20selects%20a%20subset%20for%0Aunsupervised%20CAT%20training%2C%20and%20then%20performs%20inference%20on%20all%20patches%2C%0Astitching%20them%20into%20the%20final%20output.%20The%20CAT%20module%2C%20integrated%20between%20the%0Afeature%20extraction%20and%20channel%20transformation%20stages%20of%20a%20pre-trained%20network%2C%0Atailors%20the%20fused%20features%20and%20fixes%20the%20parameters%20for%20efficient%20inference%2C%0Agenerating%20improved%20results.%20Our%20approach%20offers%20two%20key%20advantages%3A%20%281%29%0A%24%5Ctextit%7BImproved%20Generalization%20Ability%7D%24%3A%20by%20mitigating%20cross-sensor%0Adegradation%2C%20our%20model--although%20pre-trained%20on%20a%20specific%20dataset--achieves%0Asuperior%20performance%20on%20datasets%20captured%20by%20other%20sensors%3B%20%282%29%0A%24%5Ctextit%7BEnhanced%20Computational%20Efficiency%7D%24%3A%20the%20CAT-enhanced%20network%20can%0Aswiftly%20adapt%20to%20the%20test%20sample%20using%20the%20single%20LRMS-PAN%20pair%20input%2C%20without%0Arequiring%20extensive%20large-scale%20data%20retraining.%20Experiments%20on%20the%20real-world%0Adata%20from%20WorldView-3%20and%20WorldView-2%20datasets%20demonstrate%20that%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20cross-sensor%20real-world%20data%2C%20while%0Aachieving%20both%20training%20and%20inference%20of%20%24512%5Ctimes512%24%20image%20within%0A%24%5Ctextit%7B0.4%20seconds%7D%24%20and%20%244000%5Ctimes4000%24%20image%20within%20%24%5Ctextit%7B3%20seconds%7D%24%0Aat%20the%20fastest%20setting%20on%20a%20commonly%20used%20RTX%203090%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT%253A%2520A%2520Conditional%2520Adaptation%2520Tailor%2520for%2520Efficient%2520and%2520Effective%250A%2520%2520Instance-Specific%2520Pansharpening%2520on%2520Real-World%2520Data%26entry.906535625%3DTianyu%2520Xin%2520and%2520Jin-Liang%2520Xiao%2520and%2520Zeyu%2520Xia%2520and%2520Shan%2520Yin%2520and%2520Liang-Jian%2520Deng%26entry.1292438233%3D%2520%2520Pansharpening%2520is%2520a%2520crucial%2520remote%2520sensing%2520technique%2520that%2520fuses%2520low-resolution%250Amultispectral%2520%2528LRMS%2529%2520images%2520with%2520high-resolution%2520panchromatic%2520%2528PAN%2529%2520images%2520to%250Agenerate%2520high-resolution%2520multispectral%2520%2528HRMS%2529%2520imagery.%2520Although%2520deep%2520learning%250Atechniques%2520have%2520significantly%2520advanced%2520pansharpening%252C%2520many%2520existing%2520methods%250Asuffer%2520from%2520limited%2520cross-sensor%2520generalization%2520and%2520high%2520computational%250Aoverhead%252C%2520restricting%2520their%2520real-time%2520applications.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520an%2520efficient%2520framework%2520that%2520quickly%2520adapts%2520to%2520a%2520specific%250Ainput%2520instance%252C%2520completing%2520both%2520training%2520and%2520inference%2520in%2520a%2520short%2520time.%2520Our%250Aframework%2520splits%2520the%2520input%2520image%2520into%2520multiple%2520patches%252C%2520selects%2520a%2520subset%2520for%250Aunsupervised%2520CAT%2520training%252C%2520and%2520then%2520performs%2520inference%2520on%2520all%2520patches%252C%250Astitching%2520them%2520into%2520the%2520final%2520output.%2520The%2520CAT%2520module%252C%2520integrated%2520between%2520the%250Afeature%2520extraction%2520and%2520channel%2520transformation%2520stages%2520of%2520a%2520pre-trained%2520network%252C%250Atailors%2520the%2520fused%2520features%2520and%2520fixes%2520the%2520parameters%2520for%2520efficient%2520inference%252C%250Agenerating%2520improved%2520results.%2520Our%2520approach%2520offers%2520two%2520key%2520advantages%253A%2520%25281%2529%250A%2524%255Ctextit%257BImproved%2520Generalization%2520Ability%257D%2524%253A%2520by%2520mitigating%2520cross-sensor%250Adegradation%252C%2520our%2520model--although%2520pre-trained%2520on%2520a%2520specific%2520dataset--achieves%250Asuperior%2520performance%2520on%2520datasets%2520captured%2520by%2520other%2520sensors%253B%2520%25282%2529%250A%2524%255Ctextit%257BEnhanced%2520Computational%2520Efficiency%257D%2524%253A%2520the%2520CAT-enhanced%2520network%2520can%250Aswiftly%2520adapt%2520to%2520the%2520test%2520sample%2520using%2520the%2520single%2520LRMS-PAN%2520pair%2520input%252C%2520without%250Arequiring%2520extensive%2520large-scale%2520data%2520retraining.%2520Experiments%2520on%2520the%2520real-world%250Adata%2520from%2520WorldView-3%2520and%2520WorldView-2%2520datasets%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520on%2520cross-sensor%2520real-world%2520data%252C%2520while%250Aachieving%2520both%2520training%2520and%2520inference%2520of%2520%2524512%255Ctimes512%2524%2520image%2520within%250A%2524%255Ctextit%257B0.4%2520seconds%257D%2524%2520and%2520%25244000%255Ctimes4000%2524%2520image%2520within%2520%2524%255Ctextit%257B3%2520seconds%257D%2524%250Aat%2520the%2520fastest%2520setting%2520on%2520a%2520commonly%2520used%2520RTX%25203090%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT%3A%20A%20Conditional%20Adaptation%20Tailor%20for%20Efficient%20and%20Effective%0A%20%20Instance-Specific%20Pansharpening%20on%20Real-World%20Data&entry.906535625=Tianyu%20Xin%20and%20Jin-Liang%20Xiao%20and%20Zeyu%20Xia%20and%20Shan%20Yin%20and%20Liang-Jian%20Deng&entry.1292438233=%20%20Pansharpening%20is%20a%20crucial%20remote%20sensing%20technique%20that%20fuses%20low-resolution%0Amultispectral%20%28LRMS%29%20images%20with%20high-resolution%20panchromatic%20%28PAN%29%20images%20to%0Agenerate%20high-resolution%20multispectral%20%28HRMS%29%20imagery.%20Although%20deep%20learning%0Atechniques%20have%20significantly%20advanced%20pansharpening%2C%20many%20existing%20methods%0Asuffer%20from%20limited%20cross-sensor%20generalization%20and%20high%20computational%0Aoverhead%2C%20restricting%20their%20real-time%20applications.%20To%20address%20these%0Achallenges%2C%20we%20propose%20an%20efficient%20framework%20that%20quickly%20adapts%20to%20a%20specific%0Ainput%20instance%2C%20completing%20both%20training%20and%20inference%20in%20a%20short%20time.%20Our%0Aframework%20splits%20the%20input%20image%20into%20multiple%20patches%2C%20selects%20a%20subset%20for%0Aunsupervised%20CAT%20training%2C%20and%20then%20performs%20inference%20on%20all%20patches%2C%0Astitching%20them%20into%20the%20final%20output.%20The%20CAT%20module%2C%20integrated%20between%20the%0Afeature%20extraction%20and%20channel%20transformation%20stages%20of%20a%20pre-trained%20network%2C%0Atailors%20the%20fused%20features%20and%20fixes%20the%20parameters%20for%20efficient%20inference%2C%0Agenerating%20improved%20results.%20Our%20approach%20offers%20two%20key%20advantages%3A%20%281%29%0A%24%5Ctextit%7BImproved%20Generalization%20Ability%7D%24%3A%20by%20mitigating%20cross-sensor%0Adegradation%2C%20our%20model--although%20pre-trained%20on%20a%20specific%20dataset--achieves%0Asuperior%20performance%20on%20datasets%20captured%20by%20other%20sensors%3B%20%282%29%0A%24%5Ctextit%7BEnhanced%20Computational%20Efficiency%7D%24%3A%20the%20CAT-enhanced%20network%20can%0Aswiftly%20adapt%20to%20the%20test%20sample%20using%20the%20single%20LRMS-PAN%20pair%20input%2C%20without%0Arequiring%20extensive%20large-scale%20data%20retraining.%20Experiments%20on%20the%20real-world%0Adata%20from%20WorldView-3%20and%20WorldView-2%20datasets%20demonstrate%20that%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20cross-sensor%20real-world%20data%2C%20while%0Aachieving%20both%20training%20and%20inference%20of%20%24512%5Ctimes512%24%20image%20within%0A%24%5Ctextit%7B0.4%20seconds%7D%24%20and%20%244000%5Ctimes4000%24%20image%20within%20%24%5Ctextit%7B3%20seconds%7D%24%0Aat%20the%20fastest%20setting%20on%20a%20commonly%20used%20RTX%203090%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10242v2&entry.124074799=Read"},
{"title": "Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of\n  Videos Based on Chain-of-Thought", "author": "Shuyi Zhang and Xiaoshuai Hao and Yingbo Tang and Lingfeng Zhang and Pengwei Wang and Zhongyuan Wang and Hongxuan Ma and Shanghang Zhang", "abstract": "  Video content comprehension is essential for various applications, ranging\nfrom video analysis to interactive systems. Despite advancements in large-scale\nvision-language models (VLMs), these models often struggle to capture the\nnuanced, spatiotemporal details essential for thorough video analysis. To\naddress this gap, we introduce Video-CoT, a groundbreaking dataset designed to\nenhance spatiotemporal understanding using Chain-of-Thought (CoT)\nmethodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal\nquestion-answer pairs and 23,000 high-quality CoT-annotated samples, providing\na solid foundation for evaluating spatiotemporal understanding in video\ncomprehension. Additionally, we provide a comprehensive benchmark for assessing\nthese tasks, with each task featuring 750 images and tailored evaluation\nmetrics. Our extensive experiments reveal that current VLMs face significant\nchallenges in achieving satisfactory performance, high-lighting the\ndifficulties of effective spatiotemporal understanding. Overall, the Video-CoT\ndataset and benchmark open new avenues for research in multimedia understanding\nand support future innovations in intelligent systems requiring advanced video\nanalysis capabilities. By making these resources publicly available, we aim to\nencourage further exploration in this critical area. Project\nwebsite:https://video-cot.github.io/ .\n", "link": "http://arxiv.org/abs/2506.08817v3", "date": "2025-06-12", "relevancy": 2.227, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5576}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-CoT%3A%20A%20Comprehensive%20Dataset%20for%20Spatiotemporal%20Understanding%20of%0A%20%20Videos%20Based%20on%20Chain-of-Thought&body=Title%3A%20Video-CoT%3A%20A%20Comprehensive%20Dataset%20for%20Spatiotemporal%20Understanding%20of%0A%20%20Videos%20Based%20on%20Chain-of-Thought%0AAuthor%3A%20Shuyi%20Zhang%20and%20Xiaoshuai%20Hao%20and%20Yingbo%20Tang%20and%20Lingfeng%20Zhang%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Hongxuan%20Ma%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Video%20content%20comprehension%20is%20essential%20for%20various%20applications%2C%20ranging%0Afrom%20video%20analysis%20to%20interactive%20systems.%20Despite%20advancements%20in%20large-scale%0Avision-language%20models%20%28VLMs%29%2C%20these%20models%20often%20struggle%20to%20capture%20the%0Anuanced%2C%20spatiotemporal%20details%20essential%20for%20thorough%20video%20analysis.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20Video-CoT%2C%20a%20groundbreaking%20dataset%20designed%20to%0Aenhance%20spatiotemporal%20understanding%20using%20Chain-of-Thought%20%28CoT%29%0Amethodologies.%20Video-CoT%20contains%20192%2C000%20fine-grained%20spa-tiotemporal%0Aquestion-answer%20pairs%20and%2023%2C000%20high-quality%20CoT-annotated%20samples%2C%20providing%0Aa%20solid%20foundation%20for%20evaluating%20spatiotemporal%20understanding%20in%20video%0Acomprehension.%20Additionally%2C%20we%20provide%20a%20comprehensive%20benchmark%20for%20assessing%0Athese%20tasks%2C%20with%20each%20task%20featuring%20750%20images%20and%20tailored%20evaluation%0Ametrics.%20Our%20extensive%20experiments%20reveal%20that%20current%20VLMs%20face%20significant%0Achallenges%20in%20achieving%20satisfactory%20performance%2C%20high-lighting%20the%0Adifficulties%20of%20effective%20spatiotemporal%20understanding.%20Overall%2C%20the%20Video-CoT%0Adataset%20and%20benchmark%20open%20new%20avenues%20for%20research%20in%20multimedia%20understanding%0Aand%20support%20future%20innovations%20in%20intelligent%20systems%20requiring%20advanced%20video%0Aanalysis%20capabilities.%20By%20making%20these%20resources%20publicly%20available%2C%20we%20aim%20to%0Aencourage%20further%20exploration%20in%20this%20critical%20area.%20Project%0Awebsite%3Ahttps%3A//video-cot.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08817v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-CoT%253A%2520A%2520Comprehensive%2520Dataset%2520for%2520Spatiotemporal%2520Understanding%2520of%250A%2520%2520Videos%2520Based%2520on%2520Chain-of-Thought%26entry.906535625%3DShuyi%2520Zhang%2520and%2520Xiaoshuai%2520Hao%2520and%2520Yingbo%2520Tang%2520and%2520Lingfeng%2520Zhang%2520and%2520Pengwei%2520Wang%2520and%2520Zhongyuan%2520Wang%2520and%2520Hongxuan%2520Ma%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520content%2520comprehension%2520is%2520essential%2520for%2520various%2520applications%252C%2520ranging%250Afrom%2520video%2520analysis%2520to%2520interactive%2520systems.%2520Despite%2520advancements%2520in%2520large-scale%250Avision-language%2520models%2520%2528VLMs%2529%252C%2520these%2520models%2520often%2520struggle%2520to%2520capture%2520the%250Anuanced%252C%2520spatiotemporal%2520details%2520essential%2520for%2520thorough%2520video%2520analysis.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520Video-CoT%252C%2520a%2520groundbreaking%2520dataset%2520designed%2520to%250Aenhance%2520spatiotemporal%2520understanding%2520using%2520Chain-of-Thought%2520%2528CoT%2529%250Amethodologies.%2520Video-CoT%2520contains%2520192%252C000%2520fine-grained%2520spa-tiotemporal%250Aquestion-answer%2520pairs%2520and%252023%252C000%2520high-quality%2520CoT-annotated%2520samples%252C%2520providing%250Aa%2520solid%2520foundation%2520for%2520evaluating%2520spatiotemporal%2520understanding%2520in%2520video%250Acomprehension.%2520Additionally%252C%2520we%2520provide%2520a%2520comprehensive%2520benchmark%2520for%2520assessing%250Athese%2520tasks%252C%2520with%2520each%2520task%2520featuring%2520750%2520images%2520and%2520tailored%2520evaluation%250Ametrics.%2520Our%2520extensive%2520experiments%2520reveal%2520that%2520current%2520VLMs%2520face%2520significant%250Achallenges%2520in%2520achieving%2520satisfactory%2520performance%252C%2520high-lighting%2520the%250Adifficulties%2520of%2520effective%2520spatiotemporal%2520understanding.%2520Overall%252C%2520the%2520Video-CoT%250Adataset%2520and%2520benchmark%2520open%2520new%2520avenues%2520for%2520research%2520in%2520multimedia%2520understanding%250Aand%2520support%2520future%2520innovations%2520in%2520intelligent%2520systems%2520requiring%2520advanced%2520video%250Aanalysis%2520capabilities.%2520By%2520making%2520these%2520resources%2520publicly%2520available%252C%2520we%2520aim%2520to%250Aencourage%2520further%2520exploration%2520in%2520this%2520critical%2520area.%2520Project%250Awebsite%253Ahttps%253A//video-cot.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08817v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-CoT%3A%20A%20Comprehensive%20Dataset%20for%20Spatiotemporal%20Understanding%20of%0A%20%20Videos%20Based%20on%20Chain-of-Thought&entry.906535625=Shuyi%20Zhang%20and%20Xiaoshuai%20Hao%20and%20Yingbo%20Tang%20and%20Lingfeng%20Zhang%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Hongxuan%20Ma%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Video%20content%20comprehension%20is%20essential%20for%20various%20applications%2C%20ranging%0Afrom%20video%20analysis%20to%20interactive%20systems.%20Despite%20advancements%20in%20large-scale%0Avision-language%20models%20%28VLMs%29%2C%20these%20models%20often%20struggle%20to%20capture%20the%0Anuanced%2C%20spatiotemporal%20details%20essential%20for%20thorough%20video%20analysis.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20Video-CoT%2C%20a%20groundbreaking%20dataset%20designed%20to%0Aenhance%20spatiotemporal%20understanding%20using%20Chain-of-Thought%20%28CoT%29%0Amethodologies.%20Video-CoT%20contains%20192%2C000%20fine-grained%20spa-tiotemporal%0Aquestion-answer%20pairs%20and%2023%2C000%20high-quality%20CoT-annotated%20samples%2C%20providing%0Aa%20solid%20foundation%20for%20evaluating%20spatiotemporal%20understanding%20in%20video%0Acomprehension.%20Additionally%2C%20we%20provide%20a%20comprehensive%20benchmark%20for%20assessing%0Athese%20tasks%2C%20with%20each%20task%20featuring%20750%20images%20and%20tailored%20evaluation%0Ametrics.%20Our%20extensive%20experiments%20reveal%20that%20current%20VLMs%20face%20significant%0Achallenges%20in%20achieving%20satisfactory%20performance%2C%20high-lighting%20the%0Adifficulties%20of%20effective%20spatiotemporal%20understanding.%20Overall%2C%20the%20Video-CoT%0Adataset%20and%20benchmark%20open%20new%20avenues%20for%20research%20in%20multimedia%20understanding%0Aand%20support%20future%20innovations%20in%20intelligent%20systems%20requiring%20advanced%20video%0Aanalysis%20capabilities.%20By%20making%20these%20resources%20publicly%20available%2C%20we%20aim%20to%0Aencourage%20further%20exploration%20in%20this%20critical%20area.%20Project%0Awebsite%3Ahttps%3A//video-cot.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08817v3&entry.124074799=Read"},
{"title": "Three iterations of $(d-1)$-WL test distinguish non isometric clouds of\n  $d$-dimensional points", "author": "Valentino Delle Rose and Alexander Kozachinskiy and Crist\u00f3bal Rojas and Mircea Petrache and Pablo Barcel\u00f3", "abstract": "  The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for\nchecking isomorphism of graphs. It has also been observed that it underlies the\ndesign of several graph neural network architectures, whose capabilities and\nperformance can be understood in terms of the expressive power of this test.\nMotivated by recent developments in machine learning applications to datasets\ninvolving three-dimensional objects, we study when the WL test is {\\em\ncomplete} for clouds of euclidean points represented by complete distance\ngraphs, i.e., when it can distinguish, up to isometry, any arbitrary such\ncloud. %arbitrary clouds of euclidean points represented by complete distance\ngraphs. % How many dimensions of the Weisfeiler--Lehman test is enough to\ndistinguish any two non-isometric point clouds in $d$-dimensional Euclidean\nspace, assuming that these point clouds are given as complete graphs labeled by\ndistances between the points? This question is important for understanding,\nwhich architectures of graph neural networks are capable of fully exploiting\nthe spacial structure of a point cloud.\n  Our main result states that the $(d-1)$-dimensional WL test is complete for\npoint clouds in $d$-dimensional Euclidean space, for any $d\\ge 2$, and that\nonly three iterations of the test suffice. We also observe that the\n$d$-dimensional WL test only requires one iteration to achieve completeness.\n  Our paper thus provides complete understanding of the 3-dimensional case: it\nwas shown in previous works that 1-WL is not complete in $\\mathbb{R}^3$, and we\nshow that 2-WL is complete there. We also strengthen the lower bound for 1-WL\nby showing that it is unable to recognize planar point clouds in\n$\\mathbb{R}^3$. Finally, we show that 2-WL is not complete in $\\mathbb{R}^6$,\nleaving as an open question, whether it is complete in $\\mathbb{R}^{d}$ for $d\n= 4,5$.\n", "link": "http://arxiv.org/abs/2303.12853v4", "date": "2025-06-12", "relevancy": 2.2171, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4651}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4332}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three%20iterations%20of%20%24%28d-1%29%24-WL%20test%20distinguish%20non%20isometric%20clouds%20of%0A%20%20%24d%24-dimensional%20points&body=Title%3A%20Three%20iterations%20of%20%24%28d-1%29%24-WL%20test%20distinguish%20non%20isometric%20clouds%20of%0A%20%20%24d%24-dimensional%20points%0AAuthor%3A%20Valentino%20Delle%20Rose%20and%20Alexander%20Kozachinskiy%20and%20Crist%C3%B3bal%20Rojas%20and%20Mircea%20Petrache%20and%20Pablo%20Barcel%C3%B3%0AAbstract%3A%20%20%20The%20Weisfeiler--Lehman%20%28WL%29%20test%20is%20a%20fundamental%20iterative%20algorithm%20for%0Achecking%20isomorphism%20of%20graphs.%20It%20has%20also%20been%20observed%20that%20it%20underlies%20the%0Adesign%20of%20several%20graph%20neural%20network%20architectures%2C%20whose%20capabilities%20and%0Aperformance%20can%20be%20understood%20in%20terms%20of%20the%20expressive%20power%20of%20this%20test.%0AMotivated%20by%20recent%20developments%20in%20machine%20learning%20applications%20to%20datasets%0Ainvolving%20three-dimensional%20objects%2C%20we%20study%20when%20the%20WL%20test%20is%20%7B%5Cem%0Acomplete%7D%20for%20clouds%20of%20euclidean%20points%20represented%20by%20complete%20distance%0Agraphs%2C%20i.e.%2C%20when%20it%20can%20distinguish%2C%20up%20to%20isometry%2C%20any%20arbitrary%20such%0Acloud.%20%25arbitrary%20clouds%20of%20euclidean%20points%20represented%20by%20complete%20distance%0Agraphs.%20%25%20How%20many%20dimensions%20of%20the%20Weisfeiler--Lehman%20test%20is%20enough%20to%0Adistinguish%20any%20two%20non-isometric%20point%20clouds%20in%20%24d%24-dimensional%20Euclidean%0Aspace%2C%20assuming%20that%20these%20point%20clouds%20are%20given%20as%20complete%20graphs%20labeled%20by%0Adistances%20between%20the%20points%3F%20This%20question%20is%20important%20for%20understanding%2C%0Awhich%20architectures%20of%20graph%20neural%20networks%20are%20capable%20of%20fully%20exploiting%0Athe%20spacial%20structure%20of%20a%20point%20cloud.%0A%20%20Our%20main%20result%20states%20that%20the%20%24%28d-1%29%24-dimensional%20WL%20test%20is%20complete%20for%0Apoint%20clouds%20in%20%24d%24-dimensional%20Euclidean%20space%2C%20for%20any%20%24d%5Cge%202%24%2C%20and%20that%0Aonly%20three%20iterations%20of%20the%20test%20suffice.%20We%20also%20observe%20that%20the%0A%24d%24-dimensional%20WL%20test%20only%20requires%20one%20iteration%20to%20achieve%20completeness.%0A%20%20Our%20paper%20thus%20provides%20complete%20understanding%20of%20the%203-dimensional%20case%3A%20it%0Awas%20shown%20in%20previous%20works%20that%201-WL%20is%20not%20complete%20in%20%24%5Cmathbb%7BR%7D%5E3%24%2C%20and%20we%0Ashow%20that%202-WL%20is%20complete%20there.%20We%20also%20strengthen%20the%20lower%20bound%20for%201-WL%0Aby%20showing%20that%20it%20is%20unable%20to%20recognize%20planar%20point%20clouds%20in%0A%24%5Cmathbb%7BR%7D%5E3%24.%20Finally%2C%20we%20show%20that%202-WL%20is%20not%20complete%20in%20%24%5Cmathbb%7BR%7D%5E6%24%2C%0Aleaving%20as%20an%20open%20question%2C%20whether%20it%20is%20complete%20in%20%24%5Cmathbb%7BR%7D%5E%7Bd%7D%24%20for%20%24d%0A%3D%204%2C5%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.12853v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree%2520iterations%2520of%2520%2524%2528d-1%2529%2524-WL%2520test%2520distinguish%2520non%2520isometric%2520clouds%2520of%250A%2520%2520%2524d%2524-dimensional%2520points%26entry.906535625%3DValentino%2520Delle%2520Rose%2520and%2520Alexander%2520Kozachinskiy%2520and%2520Crist%25C3%25B3bal%2520Rojas%2520and%2520Mircea%2520Petrache%2520and%2520Pablo%2520Barcel%25C3%25B3%26entry.1292438233%3D%2520%2520The%2520Weisfeiler--Lehman%2520%2528WL%2529%2520test%2520is%2520a%2520fundamental%2520iterative%2520algorithm%2520for%250Achecking%2520isomorphism%2520of%2520graphs.%2520It%2520has%2520also%2520been%2520observed%2520that%2520it%2520underlies%2520the%250Adesign%2520of%2520several%2520graph%2520neural%2520network%2520architectures%252C%2520whose%2520capabilities%2520and%250Aperformance%2520can%2520be%2520understood%2520in%2520terms%2520of%2520the%2520expressive%2520power%2520of%2520this%2520test.%250AMotivated%2520by%2520recent%2520developments%2520in%2520machine%2520learning%2520applications%2520to%2520datasets%250Ainvolving%2520three-dimensional%2520objects%252C%2520we%2520study%2520when%2520the%2520WL%2520test%2520is%2520%257B%255Cem%250Acomplete%257D%2520for%2520clouds%2520of%2520euclidean%2520points%2520represented%2520by%2520complete%2520distance%250Agraphs%252C%2520i.e.%252C%2520when%2520it%2520can%2520distinguish%252C%2520up%2520to%2520isometry%252C%2520any%2520arbitrary%2520such%250Acloud.%2520%2525arbitrary%2520clouds%2520of%2520euclidean%2520points%2520represented%2520by%2520complete%2520distance%250Agraphs.%2520%2525%2520How%2520many%2520dimensions%2520of%2520the%2520Weisfeiler--Lehman%2520test%2520is%2520enough%2520to%250Adistinguish%2520any%2520two%2520non-isometric%2520point%2520clouds%2520in%2520%2524d%2524-dimensional%2520Euclidean%250Aspace%252C%2520assuming%2520that%2520these%2520point%2520clouds%2520are%2520given%2520as%2520complete%2520graphs%2520labeled%2520by%250Adistances%2520between%2520the%2520points%253F%2520This%2520question%2520is%2520important%2520for%2520understanding%252C%250Awhich%2520architectures%2520of%2520graph%2520neural%2520networks%2520are%2520capable%2520of%2520fully%2520exploiting%250Athe%2520spacial%2520structure%2520of%2520a%2520point%2520cloud.%250A%2520%2520Our%2520main%2520result%2520states%2520that%2520the%2520%2524%2528d-1%2529%2524-dimensional%2520WL%2520test%2520is%2520complete%2520for%250Apoint%2520clouds%2520in%2520%2524d%2524-dimensional%2520Euclidean%2520space%252C%2520for%2520any%2520%2524d%255Cge%25202%2524%252C%2520and%2520that%250Aonly%2520three%2520iterations%2520of%2520the%2520test%2520suffice.%2520We%2520also%2520observe%2520that%2520the%250A%2524d%2524-dimensional%2520WL%2520test%2520only%2520requires%2520one%2520iteration%2520to%2520achieve%2520completeness.%250A%2520%2520Our%2520paper%2520thus%2520provides%2520complete%2520understanding%2520of%2520the%25203-dimensional%2520case%253A%2520it%250Awas%2520shown%2520in%2520previous%2520works%2520that%25201-WL%2520is%2520not%2520complete%2520in%2520%2524%255Cmathbb%257BR%257D%255E3%2524%252C%2520and%2520we%250Ashow%2520that%25202-WL%2520is%2520complete%2520there.%2520We%2520also%2520strengthen%2520the%2520lower%2520bound%2520for%25201-WL%250Aby%2520showing%2520that%2520it%2520is%2520unable%2520to%2520recognize%2520planar%2520point%2520clouds%2520in%250A%2524%255Cmathbb%257BR%257D%255E3%2524.%2520Finally%252C%2520we%2520show%2520that%25202-WL%2520is%2520not%2520complete%2520in%2520%2524%255Cmathbb%257BR%257D%255E6%2524%252C%250Aleaving%2520as%2520an%2520open%2520question%252C%2520whether%2520it%2520is%2520complete%2520in%2520%2524%255Cmathbb%257BR%257D%255E%257Bd%257D%2524%2520for%2520%2524d%250A%253D%25204%252C5%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.12853v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three%20iterations%20of%20%24%28d-1%29%24-WL%20test%20distinguish%20non%20isometric%20clouds%20of%0A%20%20%24d%24-dimensional%20points&entry.906535625=Valentino%20Delle%20Rose%20and%20Alexander%20Kozachinskiy%20and%20Crist%C3%B3bal%20Rojas%20and%20Mircea%20Petrache%20and%20Pablo%20Barcel%C3%B3&entry.1292438233=%20%20The%20Weisfeiler--Lehman%20%28WL%29%20test%20is%20a%20fundamental%20iterative%20algorithm%20for%0Achecking%20isomorphism%20of%20graphs.%20It%20has%20also%20been%20observed%20that%20it%20underlies%20the%0Adesign%20of%20several%20graph%20neural%20network%20architectures%2C%20whose%20capabilities%20and%0Aperformance%20can%20be%20understood%20in%20terms%20of%20the%20expressive%20power%20of%20this%20test.%0AMotivated%20by%20recent%20developments%20in%20machine%20learning%20applications%20to%20datasets%0Ainvolving%20three-dimensional%20objects%2C%20we%20study%20when%20the%20WL%20test%20is%20%7B%5Cem%0Acomplete%7D%20for%20clouds%20of%20euclidean%20points%20represented%20by%20complete%20distance%0Agraphs%2C%20i.e.%2C%20when%20it%20can%20distinguish%2C%20up%20to%20isometry%2C%20any%20arbitrary%20such%0Acloud.%20%25arbitrary%20clouds%20of%20euclidean%20points%20represented%20by%20complete%20distance%0Agraphs.%20%25%20How%20many%20dimensions%20of%20the%20Weisfeiler--Lehman%20test%20is%20enough%20to%0Adistinguish%20any%20two%20non-isometric%20point%20clouds%20in%20%24d%24-dimensional%20Euclidean%0Aspace%2C%20assuming%20that%20these%20point%20clouds%20are%20given%20as%20complete%20graphs%20labeled%20by%0Adistances%20between%20the%20points%3F%20This%20question%20is%20important%20for%20understanding%2C%0Awhich%20architectures%20of%20graph%20neural%20networks%20are%20capable%20of%20fully%20exploiting%0Athe%20spacial%20structure%20of%20a%20point%20cloud.%0A%20%20Our%20main%20result%20states%20that%20the%20%24%28d-1%29%24-dimensional%20WL%20test%20is%20complete%20for%0Apoint%20clouds%20in%20%24d%24-dimensional%20Euclidean%20space%2C%20for%20any%20%24d%5Cge%202%24%2C%20and%20that%0Aonly%20three%20iterations%20of%20the%20test%20suffice.%20We%20also%20observe%20that%20the%0A%24d%24-dimensional%20WL%20test%20only%20requires%20one%20iteration%20to%20achieve%20completeness.%0A%20%20Our%20paper%20thus%20provides%20complete%20understanding%20of%20the%203-dimensional%20case%3A%20it%0Awas%20shown%20in%20previous%20works%20that%201-WL%20is%20not%20complete%20in%20%24%5Cmathbb%7BR%7D%5E3%24%2C%20and%20we%0Ashow%20that%202-WL%20is%20complete%20there.%20We%20also%20strengthen%20the%20lower%20bound%20for%201-WL%0Aby%20showing%20that%20it%20is%20unable%20to%20recognize%20planar%20point%20clouds%20in%0A%24%5Cmathbb%7BR%7D%5E3%24.%20Finally%2C%20we%20show%20that%202-WL%20is%20not%20complete%20in%20%24%5Cmathbb%7BR%7D%5E6%24%2C%0Aleaving%20as%20an%20open%20question%2C%20whether%20it%20is%20complete%20in%20%24%5Cmathbb%7BR%7D%5E%7Bd%7D%24%20for%20%24d%0A%3D%204%2C5%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.12853v4&entry.124074799=Read"},
{"title": "Reinforcing Multimodal Understanding and Generation with Dual\n  Self-rewards", "author": "Jixiang Hong and Yiran Zhang and Guanzhong Wang and Yi Liu and Ji-Rong Wen and Rui Yan", "abstract": "  Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks.\n", "link": "http://arxiv.org/abs/2506.07963v2", "date": "2025-06-12", "relevancy": 2.2134, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5699}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5454}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcing%20Multimodal%20Understanding%20and%20Generation%20with%20Dual%0A%20%20Self-rewards&body=Title%3A%20Reinforcing%20Multimodal%20Understanding%20and%20Generation%20with%20Dual%0A%20%20Self-rewards%0AAuthor%3A%20Jixiang%20Hong%20and%20Yiran%20Zhang%20and%20Guanzhong%20Wang%20and%20Yi%20Liu%20and%20Ji-Rong%20Wen%20and%20Rui%20Yan%0AAbstract%3A%20%20%20Building%20upon%20large%20language%20models%20%28LLMs%29%2C%20recent%20large%20multimodal%20models%0A%28LMMs%29%20unify%20cross-model%20understanding%20and%20generation%20into%20a%20single%20framework.%0AHowever%2C%20LMMs%20still%20struggle%20to%20achieve%20accurate%20image-text%20alignment%2C%20prone%20to%0Agenerating%20text%20responses%20contradicting%20the%20visual%20input%20or%20failing%20to%20follow%0Athe%20text-to-image%20prompts.%20Current%20solutions%20require%20external%20supervision%0A%28e.g.%2C%20human%20feedback%20or%20reward%20models%29%20and%20only%20address%20unidirectional%0Atasks-either%20understanding%20or%20generation.%20In%20this%20work%2C%20based%20on%20the%0Aobservation%20that%20understanding%20and%20generation%20are%20inverse%20dual%20tasks%2C%20we%0Aintroduce%20a%20self-supervised%20dual%20reward%20mechanism%20to%20reinforce%20the%0Aunderstanding%20and%20generation%20capabilities%20of%20LMMs.%20Specifically%2C%20we%20sample%0Amultiple%20outputs%20for%20a%20given%20input%20in%20one%20task%20domain%2C%20then%20reverse%20the%0Ainput-output%20pairs%20to%20compute%20the%20dual%20likelihood%20of%20the%20model%20as%20self-rewards%0Afor%20optimization.%20Extensive%20experimental%20results%20on%20visual%20understanding%20and%0Ageneration%20benchmarks%20demonstrate%20that%20our%20method%20can%20effectively%20enhance%20the%0Aperformance%20of%20the%20model%20without%20any%20external%20supervision%2C%20especially%20achieving%0Aremarkable%20improvements%20in%20text-to-image%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07963v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcing%2520Multimodal%2520Understanding%2520and%2520Generation%2520with%2520Dual%250A%2520%2520Self-rewards%26entry.906535625%3DJixiang%2520Hong%2520and%2520Yiran%2520Zhang%2520and%2520Guanzhong%2520Wang%2520and%2520Yi%2520Liu%2520and%2520Ji-Rong%2520Wen%2520and%2520Rui%2520Yan%26entry.1292438233%3D%2520%2520Building%2520upon%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520recent%2520large%2520multimodal%2520models%250A%2528LMMs%2529%2520unify%2520cross-model%2520understanding%2520and%2520generation%2520into%2520a%2520single%2520framework.%250AHowever%252C%2520LMMs%2520still%2520struggle%2520to%2520achieve%2520accurate%2520image-text%2520alignment%252C%2520prone%2520to%250Agenerating%2520text%2520responses%2520contradicting%2520the%2520visual%2520input%2520or%2520failing%2520to%2520follow%250Athe%2520text-to-image%2520prompts.%2520Current%2520solutions%2520require%2520external%2520supervision%250A%2528e.g.%252C%2520human%2520feedback%2520or%2520reward%2520models%2529%2520and%2520only%2520address%2520unidirectional%250Atasks-either%2520understanding%2520or%2520generation.%2520In%2520this%2520work%252C%2520based%2520on%2520the%250Aobservation%2520that%2520understanding%2520and%2520generation%2520are%2520inverse%2520dual%2520tasks%252C%2520we%250Aintroduce%2520a%2520self-supervised%2520dual%2520reward%2520mechanism%2520to%2520reinforce%2520the%250Aunderstanding%2520and%2520generation%2520capabilities%2520of%2520LMMs.%2520Specifically%252C%2520we%2520sample%250Amultiple%2520outputs%2520for%2520a%2520given%2520input%2520in%2520one%2520task%2520domain%252C%2520then%2520reverse%2520the%250Ainput-output%2520pairs%2520to%2520compute%2520the%2520dual%2520likelihood%2520of%2520the%2520model%2520as%2520self-rewards%250Afor%2520optimization.%2520Extensive%2520experimental%2520results%2520on%2520visual%2520understanding%2520and%250Ageneration%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520can%2520effectively%2520enhance%2520the%250Aperformance%2520of%2520the%2520model%2520without%2520any%2520external%2520supervision%252C%2520especially%2520achieving%250Aremarkable%2520improvements%2520in%2520text-to-image%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07963v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcing%20Multimodal%20Understanding%20and%20Generation%20with%20Dual%0A%20%20Self-rewards&entry.906535625=Jixiang%20Hong%20and%20Yiran%20Zhang%20and%20Guanzhong%20Wang%20and%20Yi%20Liu%20and%20Ji-Rong%20Wen%20and%20Rui%20Yan&entry.1292438233=%20%20Building%20upon%20large%20language%20models%20%28LLMs%29%2C%20recent%20large%20multimodal%20models%0A%28LMMs%29%20unify%20cross-model%20understanding%20and%20generation%20into%20a%20single%20framework.%0AHowever%2C%20LMMs%20still%20struggle%20to%20achieve%20accurate%20image-text%20alignment%2C%20prone%20to%0Agenerating%20text%20responses%20contradicting%20the%20visual%20input%20or%20failing%20to%20follow%0Athe%20text-to-image%20prompts.%20Current%20solutions%20require%20external%20supervision%0A%28e.g.%2C%20human%20feedback%20or%20reward%20models%29%20and%20only%20address%20unidirectional%0Atasks-either%20understanding%20or%20generation.%20In%20this%20work%2C%20based%20on%20the%0Aobservation%20that%20understanding%20and%20generation%20are%20inverse%20dual%20tasks%2C%20we%0Aintroduce%20a%20self-supervised%20dual%20reward%20mechanism%20to%20reinforce%20the%0Aunderstanding%20and%20generation%20capabilities%20of%20LMMs.%20Specifically%2C%20we%20sample%0Amultiple%20outputs%20for%20a%20given%20input%20in%20one%20task%20domain%2C%20then%20reverse%20the%0Ainput-output%20pairs%20to%20compute%20the%20dual%20likelihood%20of%20the%20model%20as%20self-rewards%0Afor%20optimization.%20Extensive%20experimental%20results%20on%20visual%20understanding%20and%0Ageneration%20benchmarks%20demonstrate%20that%20our%20method%20can%20effectively%20enhance%20the%0Aperformance%20of%20the%20model%20without%20any%20external%20supervision%2C%20especially%20achieving%0Aremarkable%20improvements%20in%20text-to-image%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07963v2&entry.124074799=Read"},
{"title": "ME: Trigger Element Combination Backdoor Attack on Copyright\n  Infringement", "author": "Feiyu Yang and Siyuan Liang and Aishan Liu and Dacheng Tao", "abstract": "  The capability of generative diffusion models (DMs) like Stable Diffusion\n(SD) in replicating training data could be taken advantage of by attackers to\nlaunch the Copyright Infringement Attack, with duplicated poisoned image-text\npairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew\noutstanding performance in attacking SD in text-to-image tasks. However, the\nfeasible data resources in this area are still limited, some of them are even\nconstrained or prohibited due to the issues like copyright ownership or\ninappropriate contents; And not all of the images in current datasets are\nsuitable for the proposed attacking methods; Besides, the state-of-the-art\n(SoTA) performance of SBD is far from ideal when few generated poisoning\nsamples could be adopted for attacks. In this paper, we raised new datasets\naccessible for researching in attacks like SBD, and proposed Multi-Element (ME)\nattack method based on SBD by increasing the number of poisonous visual-text\nelements per poisoned sample to enhance the ability of attacking, while\nimporting Discrete Cosine Transform (DCT) for the poisoned samples to maintain\nthe stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch\n(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,\nrespectively close to or even outperformed benchmark Pokemon and Mijourney\ndatasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI\nand DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than\noriginal SBD, which failed to attack at all.\n", "link": "http://arxiv.org/abs/2506.10776v1", "date": "2025-06-12", "relevancy": 2.2132, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5836}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5616}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ME%3A%20Trigger%20Element%20Combination%20Backdoor%20Attack%20on%20Copyright%0A%20%20Infringement&body=Title%3A%20ME%3A%20Trigger%20Element%20Combination%20Backdoor%20Attack%20on%20Copyright%0A%20%20Infringement%0AAuthor%3A%20Feiyu%20Yang%20and%20Siyuan%20Liang%20and%20Aishan%20Liu%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20The%20capability%20of%20generative%20diffusion%20models%20%28DMs%29%20like%20Stable%20Diffusion%0A%28SD%29%20in%20replicating%20training%20data%20could%20be%20taken%20advantage%20of%20by%20attackers%20to%0Alaunch%20the%20Copyright%20Infringement%20Attack%2C%20with%20duplicated%20poisoned%20image-text%0Apairs.%20SilentBadDiffusion%20%28SBD%29%20is%20a%20method%20proposed%20recently%2C%20which%20shew%0Aoutstanding%20performance%20in%20attacking%20SD%20in%20text-to-image%20tasks.%20However%2C%20the%0Afeasible%20data%20resources%20in%20this%20area%20are%20still%20limited%2C%20some%20of%20them%20are%20even%0Aconstrained%20or%20prohibited%20due%20to%20the%20issues%20like%20copyright%20ownership%20or%0Ainappropriate%20contents%3B%20And%20not%20all%20of%20the%20images%20in%20current%20datasets%20are%0Asuitable%20for%20the%20proposed%20attacking%20methods%3B%20Besides%2C%20the%20state-of-the-art%0A%28SoTA%29%20performance%20of%20SBD%20is%20far%20from%20ideal%20when%20few%20generated%20poisoning%0Asamples%20could%20be%20adopted%20for%20attacks.%20In%20this%20paper%2C%20we%20raised%20new%20datasets%0Aaccessible%20for%20researching%20in%20attacks%20like%20SBD%2C%20and%20proposed%20Multi-Element%20%28ME%29%0Aattack%20method%20based%20on%20SBD%20by%20increasing%20the%20number%20of%20poisonous%20visual-text%0Aelements%20per%20poisoned%20sample%20to%20enhance%20the%20ability%20of%20attacking%2C%20while%0Aimporting%20Discrete%20Cosine%20Transform%20%28DCT%29%20for%20the%20poisoned%20samples%20to%20maintain%0Athe%20stealthiness.%20The%20Copyright%20Infringement%20Rate%20%28CIR%29%20/%20First%20Attack%20Epoch%0A%28FAE%29%20we%20got%20on%20the%20two%20new%20datasets%20were%2016.78%25%20/%2039.50%20and%2051.20%25%20/%2023.60%2C%0Arespectively%20close%20to%20or%20even%20outperformed%20benchmark%20Pokemon%20and%20Mijourney%0Adatasets.%20In%20condition%20of%20low%20subsampling%20ratio%20%285%25%2C%206%20poisoned%20samples%29%2C%20MESI%0Aand%20DCT%20earned%20CIR%20/%20FAE%20of%200.23%25%20/%2084.00%20and%2012.73%25%20/%2065.50%2C%20both%20better%20than%0Aoriginal%20SBD%2C%20which%20failed%20to%20attack%20at%20all.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DME%253A%2520Trigger%2520Element%2520Combination%2520Backdoor%2520Attack%2520on%2520Copyright%250A%2520%2520Infringement%26entry.906535625%3DFeiyu%2520Yang%2520and%2520Siyuan%2520Liang%2520and%2520Aishan%2520Liu%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520The%2520capability%2520of%2520generative%2520diffusion%2520models%2520%2528DMs%2529%2520like%2520Stable%2520Diffusion%250A%2528SD%2529%2520in%2520replicating%2520training%2520data%2520could%2520be%2520taken%2520advantage%2520of%2520by%2520attackers%2520to%250Alaunch%2520the%2520Copyright%2520Infringement%2520Attack%252C%2520with%2520duplicated%2520poisoned%2520image-text%250Apairs.%2520SilentBadDiffusion%2520%2528SBD%2529%2520is%2520a%2520method%2520proposed%2520recently%252C%2520which%2520shew%250Aoutstanding%2520performance%2520in%2520attacking%2520SD%2520in%2520text-to-image%2520tasks.%2520However%252C%2520the%250Afeasible%2520data%2520resources%2520in%2520this%2520area%2520are%2520still%2520limited%252C%2520some%2520of%2520them%2520are%2520even%250Aconstrained%2520or%2520prohibited%2520due%2520to%2520the%2520issues%2520like%2520copyright%2520ownership%2520or%250Ainappropriate%2520contents%253B%2520And%2520not%2520all%2520of%2520the%2520images%2520in%2520current%2520datasets%2520are%250Asuitable%2520for%2520the%2520proposed%2520attacking%2520methods%253B%2520Besides%252C%2520the%2520state-of-the-art%250A%2528SoTA%2529%2520performance%2520of%2520SBD%2520is%2520far%2520from%2520ideal%2520when%2520few%2520generated%2520poisoning%250Asamples%2520could%2520be%2520adopted%2520for%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520raised%2520new%2520datasets%250Aaccessible%2520for%2520researching%2520in%2520attacks%2520like%2520SBD%252C%2520and%2520proposed%2520Multi-Element%2520%2528ME%2529%250Aattack%2520method%2520based%2520on%2520SBD%2520by%2520increasing%2520the%2520number%2520of%2520poisonous%2520visual-text%250Aelements%2520per%2520poisoned%2520sample%2520to%2520enhance%2520the%2520ability%2520of%2520attacking%252C%2520while%250Aimporting%2520Discrete%2520Cosine%2520Transform%2520%2528DCT%2529%2520for%2520the%2520poisoned%2520samples%2520to%2520maintain%250Athe%2520stealthiness.%2520The%2520Copyright%2520Infringement%2520Rate%2520%2528CIR%2529%2520/%2520First%2520Attack%2520Epoch%250A%2528FAE%2529%2520we%2520got%2520on%2520the%2520two%2520new%2520datasets%2520were%252016.78%2525%2520/%252039.50%2520and%252051.20%2525%2520/%252023.60%252C%250Arespectively%2520close%2520to%2520or%2520even%2520outperformed%2520benchmark%2520Pokemon%2520and%2520Mijourney%250Adatasets.%2520In%2520condition%2520of%2520low%2520subsampling%2520ratio%2520%25285%2525%252C%25206%2520poisoned%2520samples%2529%252C%2520MESI%250Aand%2520DCT%2520earned%2520CIR%2520/%2520FAE%2520of%25200.23%2525%2520/%252084.00%2520and%252012.73%2525%2520/%252065.50%252C%2520both%2520better%2520than%250Aoriginal%2520SBD%252C%2520which%2520failed%2520to%2520attack%2520at%2520all.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ME%3A%20Trigger%20Element%20Combination%20Backdoor%20Attack%20on%20Copyright%0A%20%20Infringement&entry.906535625=Feiyu%20Yang%20and%20Siyuan%20Liang%20and%20Aishan%20Liu%20and%20Dacheng%20Tao&entry.1292438233=%20%20The%20capability%20of%20generative%20diffusion%20models%20%28DMs%29%20like%20Stable%20Diffusion%0A%28SD%29%20in%20replicating%20training%20data%20could%20be%20taken%20advantage%20of%20by%20attackers%20to%0Alaunch%20the%20Copyright%20Infringement%20Attack%2C%20with%20duplicated%20poisoned%20image-text%0Apairs.%20SilentBadDiffusion%20%28SBD%29%20is%20a%20method%20proposed%20recently%2C%20which%20shew%0Aoutstanding%20performance%20in%20attacking%20SD%20in%20text-to-image%20tasks.%20However%2C%20the%0Afeasible%20data%20resources%20in%20this%20area%20are%20still%20limited%2C%20some%20of%20them%20are%20even%0Aconstrained%20or%20prohibited%20due%20to%20the%20issues%20like%20copyright%20ownership%20or%0Ainappropriate%20contents%3B%20And%20not%20all%20of%20the%20images%20in%20current%20datasets%20are%0Asuitable%20for%20the%20proposed%20attacking%20methods%3B%20Besides%2C%20the%20state-of-the-art%0A%28SoTA%29%20performance%20of%20SBD%20is%20far%20from%20ideal%20when%20few%20generated%20poisoning%0Asamples%20could%20be%20adopted%20for%20attacks.%20In%20this%20paper%2C%20we%20raised%20new%20datasets%0Aaccessible%20for%20researching%20in%20attacks%20like%20SBD%2C%20and%20proposed%20Multi-Element%20%28ME%29%0Aattack%20method%20based%20on%20SBD%20by%20increasing%20the%20number%20of%20poisonous%20visual-text%0Aelements%20per%20poisoned%20sample%20to%20enhance%20the%20ability%20of%20attacking%2C%20while%0Aimporting%20Discrete%20Cosine%20Transform%20%28DCT%29%20for%20the%20poisoned%20samples%20to%20maintain%0Athe%20stealthiness.%20The%20Copyright%20Infringement%20Rate%20%28CIR%29%20/%20First%20Attack%20Epoch%0A%28FAE%29%20we%20got%20on%20the%20two%20new%20datasets%20were%2016.78%25%20/%2039.50%20and%2051.20%25%20/%2023.60%2C%0Arespectively%20close%20to%20or%20even%20outperformed%20benchmark%20Pokemon%20and%20Mijourney%0Adatasets.%20In%20condition%20of%20low%20subsampling%20ratio%20%285%25%2C%206%20poisoned%20samples%29%2C%20MESI%0Aand%20DCT%20earned%20CIR%20/%20FAE%20of%200.23%25%20/%2084.00%20and%2012.73%25%20/%2065.50%2C%20both%20better%20than%0Aoriginal%20SBD%2C%20which%20failed%20to%20attack%20at%20all.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10776v1&entry.124074799=Read"},
{"title": "Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary\n  Ultra-Large Scales", "author": "Wenhao Guo and Peng Lu and Xujun Peng and Zhaoran Zhao and Sheng Li", "abstract": "  Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience\na significant performance decline when the upsampling factor exceeds the range\ncovered by the training data, introducing substantial blurring. To address this\nissue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for\nultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier,\nwhich decomposes the image into a series of strokes represented as vector\ngraphics for magnification. Then, the detail completion module also restores\nmissing details, ensuring high-fidelity image reconstruction. Our cyclic\nstrategy achieves ultra-large upsampling by iteratively refining details with\nthis unified SbCA model, trained only once for all, while keeping sub-scales\nwithin the training range. Our approach effectively addresses the distribution\ndrift issue and eliminates artifacts, noise and blurring, producing\nhigh-quality, high-resolution super-resolved images. Experimental validations\non both synthetic and real-world datasets demonstrate that our approach\nsignificantly outperforms existing methods in ultra-large upsampling tasks\n(e.g. $\\times100$), delivering visual quality far superior to state-of-the-art\ntechniques.\n", "link": "http://arxiv.org/abs/2506.10774v1", "date": "2025-06-12", "relevancy": 2.2126, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5628}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stroke-based%20Cyclic%20Amplifier%3A%20Image%20Super-Resolution%20at%20Arbitrary%0A%20%20Ultra-Large%20Scales&body=Title%3A%20Stroke-based%20Cyclic%20Amplifier%3A%20Image%20Super-Resolution%20at%20Arbitrary%0A%20%20Ultra-Large%20Scales%0AAuthor%3A%20Wenhao%20Guo%20and%20Peng%20Lu%20and%20Xujun%20Peng%20and%20Zhaoran%20Zhao%20and%20Sheng%20Li%0AAbstract%3A%20%20%20Prior%20Arbitrary-Scale%20Image%20Super-Resolution%20%28ASISR%29%20methods%20often%20experience%0Aa%20significant%20performance%20decline%20when%20the%20upsampling%20factor%20exceeds%20the%20range%0Acovered%20by%20the%20training%20data%2C%20introducing%20substantial%20blurring.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20unified%20model%2C%20Stroke-based%20Cyclic%20Amplifier%20%28SbCA%29%2C%20for%0Aultra-large%20upsampling%20tasks.%20The%20key%20of%20SbCA%20is%20the%20stroke%20vector%20amplifier%2C%0Awhich%20decomposes%20the%20image%20into%20a%20series%20of%20strokes%20represented%20as%20vector%0Agraphics%20for%20magnification.%20Then%2C%20the%20detail%20completion%20module%20also%20restores%0Amissing%20details%2C%20ensuring%20high-fidelity%20image%20reconstruction.%20Our%20cyclic%0Astrategy%20achieves%20ultra-large%20upsampling%20by%20iteratively%20refining%20details%20with%0Athis%20unified%20SbCA%20model%2C%20trained%20only%20once%20for%20all%2C%20while%20keeping%20sub-scales%0Awithin%20the%20training%20range.%20Our%20approach%20effectively%20addresses%20the%20distribution%0Adrift%20issue%20and%20eliminates%20artifacts%2C%20noise%20and%20blurring%2C%20producing%0Ahigh-quality%2C%20high-resolution%20super-resolved%20images.%20Experimental%20validations%0Aon%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20existing%20methods%20in%20ultra-large%20upsampling%20tasks%0A%28e.g.%20%24%5Ctimes100%24%29%2C%20delivering%20visual%20quality%20far%20superior%20to%20state-of-the-art%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStroke-based%2520Cyclic%2520Amplifier%253A%2520Image%2520Super-Resolution%2520at%2520Arbitrary%250A%2520%2520Ultra-Large%2520Scales%26entry.906535625%3DWenhao%2520Guo%2520and%2520Peng%2520Lu%2520and%2520Xujun%2520Peng%2520and%2520Zhaoran%2520Zhao%2520and%2520Sheng%2520Li%26entry.1292438233%3D%2520%2520Prior%2520Arbitrary-Scale%2520Image%2520Super-Resolution%2520%2528ASISR%2529%2520methods%2520often%2520experience%250Aa%2520significant%2520performance%2520decline%2520when%2520the%2520upsampling%2520factor%2520exceeds%2520the%2520range%250Acovered%2520by%2520the%2520training%2520data%252C%2520introducing%2520substantial%2520blurring.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520unified%2520model%252C%2520Stroke-based%2520Cyclic%2520Amplifier%2520%2528SbCA%2529%252C%2520for%250Aultra-large%2520upsampling%2520tasks.%2520The%2520key%2520of%2520SbCA%2520is%2520the%2520stroke%2520vector%2520amplifier%252C%250Awhich%2520decomposes%2520the%2520image%2520into%2520a%2520series%2520of%2520strokes%2520represented%2520as%2520vector%250Agraphics%2520for%2520magnification.%2520Then%252C%2520the%2520detail%2520completion%2520module%2520also%2520restores%250Amissing%2520details%252C%2520ensuring%2520high-fidelity%2520image%2520reconstruction.%2520Our%2520cyclic%250Astrategy%2520achieves%2520ultra-large%2520upsampling%2520by%2520iteratively%2520refining%2520details%2520with%250Athis%2520unified%2520SbCA%2520model%252C%2520trained%2520only%2520once%2520for%2520all%252C%2520while%2520keeping%2520sub-scales%250Awithin%2520the%2520training%2520range.%2520Our%2520approach%2520effectively%2520addresses%2520the%2520distribution%250Adrift%2520issue%2520and%2520eliminates%2520artifacts%252C%2520noise%2520and%2520blurring%252C%2520producing%250Ahigh-quality%252C%2520high-resolution%2520super-resolved%2520images.%2520Experimental%2520validations%250Aon%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520existing%2520methods%2520in%2520ultra-large%2520upsampling%2520tasks%250A%2528e.g.%2520%2524%255Ctimes100%2524%2529%252C%2520delivering%2520visual%2520quality%2520far%2520superior%2520to%2520state-of-the-art%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stroke-based%20Cyclic%20Amplifier%3A%20Image%20Super-Resolution%20at%20Arbitrary%0A%20%20Ultra-Large%20Scales&entry.906535625=Wenhao%20Guo%20and%20Peng%20Lu%20and%20Xujun%20Peng%20and%20Zhaoran%20Zhao%20and%20Sheng%20Li&entry.1292438233=%20%20Prior%20Arbitrary-Scale%20Image%20Super-Resolution%20%28ASISR%29%20methods%20often%20experience%0Aa%20significant%20performance%20decline%20when%20the%20upsampling%20factor%20exceeds%20the%20range%0Acovered%20by%20the%20training%20data%2C%20introducing%20substantial%20blurring.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20unified%20model%2C%20Stroke-based%20Cyclic%20Amplifier%20%28SbCA%29%2C%20for%0Aultra-large%20upsampling%20tasks.%20The%20key%20of%20SbCA%20is%20the%20stroke%20vector%20amplifier%2C%0Awhich%20decomposes%20the%20image%20into%20a%20series%20of%20strokes%20represented%20as%20vector%0Agraphics%20for%20magnification.%20Then%2C%20the%20detail%20completion%20module%20also%20restores%0Amissing%20details%2C%20ensuring%20high-fidelity%20image%20reconstruction.%20Our%20cyclic%0Astrategy%20achieves%20ultra-large%20upsampling%20by%20iteratively%20refining%20details%20with%0Athis%20unified%20SbCA%20model%2C%20trained%20only%20once%20for%20all%2C%20while%20keeping%20sub-scales%0Awithin%20the%20training%20range.%20Our%20approach%20effectively%20addresses%20the%20distribution%0Adrift%20issue%20and%20eliminates%20artifacts%2C%20noise%20and%20blurring%2C%20producing%0Ahigh-quality%2C%20high-resolution%20super-resolved%20images.%20Experimental%20validations%0Aon%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20existing%20methods%20in%20ultra-large%20upsampling%20tasks%0A%28e.g.%20%24%5Ctimes100%24%29%2C%20delivering%20visual%20quality%20far%20superior%20to%20state-of-the-art%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10774v1&entry.124074799=Read"},
{"title": "Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac\n  Substructure Segmentation", "author": "Nicholas Summerfield and Qisheng He and Alex Kuo and Ahmed I. Ghanem and Simeng Zhu and Chase Ruff and Joshua Pan and Anudeep Kumar and Prashant Nagpal and Jiwei Zhao and Ming Dong and Carri K. Glide-Hurst", "abstract": "  Cardiac substructures are essential in thoracic radiation therapy planning to\nminimize risk of radiation-induced heart disease. Deep learning (DL) offers\nefficient methods to reduce contouring burden but lacks generalizability across\ndifferent modalities and overlapping structures. This work introduces and\nvalidates a Modality-AGnostic Image Cascade (MAGIC) for comprehensive and\nmulti-modal cardiac substructure segmentation. MAGIC is implemented through\nreplicated encoding and decoding branches of an nnU-Net-based, U-shaped\nbackbone conserving the function of a single model. Twenty cardiac\nsubstructures (heart, chambers, great vessels (GVs), valves, coronary arteries\n(CAs), and conduction nodes) from simulation CT (Sim-CT), low-field MR-Linac,\nand cardiac CT angiography (CCTA) modalities were manually delineated and used\nto train (n=76), validate (n=15), and test (n=30) MAGIC. Twelve comparison\nmodels (four segmentation subgroups across three modalities) were equivalently\ntrained. All methods were compared for training efficiency and against\nreference contours using the Dice Similarity Coefficient (DSC) and two-tailed\nWilcoxon Signed-Rank test (threshold, p<0.05). Average DSC scores were\n0.75(0.16) for Sim-CT, 0.68(0.21) for MR-Linac, and 0.80(0.16) for CCTA. MAGIC\noutperforms the comparison in 57% of cases, with limited statistical\ndifferences. MAGIC offers an effective and accurate segmentation solution that\nis lightweight and capable of segmenting multiple modalities and overlapping\nstructures in a single model. MAGIC further enables clinical implementation by\nsimplifying the computational requirements and offering unparalleled\nflexibility for clinical settings.\n", "link": "http://arxiv.org/abs/2506.10797v1", "date": "2025-06-12", "relevancy": 2.2021, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5652}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-AGnostic%20Image%20Cascade%20%28MAGIC%29%20for%20Multi-Modality%20Cardiac%0A%20%20Substructure%20Segmentation&body=Title%3A%20Modality-AGnostic%20Image%20Cascade%20%28MAGIC%29%20for%20Multi-Modality%20Cardiac%0A%20%20Substructure%20Segmentation%0AAuthor%3A%20Nicholas%20Summerfield%20and%20Qisheng%20He%20and%20Alex%20Kuo%20and%20Ahmed%20I.%20Ghanem%20and%20Simeng%20Zhu%20and%20Chase%20Ruff%20and%20Joshua%20Pan%20and%20Anudeep%20Kumar%20and%20Prashant%20Nagpal%20and%20Jiwei%20Zhao%20and%20Ming%20Dong%20and%20Carri%20K.%20Glide-Hurst%0AAbstract%3A%20%20%20Cardiac%20substructures%20are%20essential%20in%20thoracic%20radiation%20therapy%20planning%20to%0Aminimize%20risk%20of%20radiation-induced%20heart%20disease.%20Deep%20learning%20%28DL%29%20offers%0Aefficient%20methods%20to%20reduce%20contouring%20burden%20but%20lacks%20generalizability%20across%0Adifferent%20modalities%20and%20overlapping%20structures.%20This%20work%20introduces%20and%0Avalidates%20a%20Modality-AGnostic%20Image%20Cascade%20%28MAGIC%29%20for%20comprehensive%20and%0Amulti-modal%20cardiac%20substructure%20segmentation.%20MAGIC%20is%20implemented%20through%0Areplicated%20encoding%20and%20decoding%20branches%20of%20an%20nnU-Net-based%2C%20U-shaped%0Abackbone%20conserving%20the%20function%20of%20a%20single%20model.%20Twenty%20cardiac%0Asubstructures%20%28heart%2C%20chambers%2C%20great%20vessels%20%28GVs%29%2C%20valves%2C%20coronary%20arteries%0A%28CAs%29%2C%20and%20conduction%20nodes%29%20from%20simulation%20CT%20%28Sim-CT%29%2C%20low-field%20MR-Linac%2C%0Aand%20cardiac%20CT%20angiography%20%28CCTA%29%20modalities%20were%20manually%20delineated%20and%20used%0Ato%20train%20%28n%3D76%29%2C%20validate%20%28n%3D15%29%2C%20and%20test%20%28n%3D30%29%20MAGIC.%20Twelve%20comparison%0Amodels%20%28four%20segmentation%20subgroups%20across%20three%20modalities%29%20were%20equivalently%0Atrained.%20All%20methods%20were%20compared%20for%20training%20efficiency%20and%20against%0Areference%20contours%20using%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29%20and%20two-tailed%0AWilcoxon%20Signed-Rank%20test%20%28threshold%2C%20p%3C0.05%29.%20Average%20DSC%20scores%20were%0A0.75%280.16%29%20for%20Sim-CT%2C%200.68%280.21%29%20for%20MR-Linac%2C%20and%200.80%280.16%29%20for%20CCTA.%20MAGIC%0Aoutperforms%20the%20comparison%20in%2057%25%20of%20cases%2C%20with%20limited%20statistical%0Adifferences.%20MAGIC%20offers%20an%20effective%20and%20accurate%20segmentation%20solution%20that%0Ais%20lightweight%20and%20capable%20of%20segmenting%20multiple%20modalities%20and%20overlapping%0Astructures%20in%20a%20single%20model.%20MAGIC%20further%20enables%20clinical%20implementation%20by%0Asimplifying%20the%20computational%20requirements%20and%20offering%20unparalleled%0Aflexibility%20for%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-AGnostic%2520Image%2520Cascade%2520%2528MAGIC%2529%2520for%2520Multi-Modality%2520Cardiac%250A%2520%2520Substructure%2520Segmentation%26entry.906535625%3DNicholas%2520Summerfield%2520and%2520Qisheng%2520He%2520and%2520Alex%2520Kuo%2520and%2520Ahmed%2520I.%2520Ghanem%2520and%2520Simeng%2520Zhu%2520and%2520Chase%2520Ruff%2520and%2520Joshua%2520Pan%2520and%2520Anudeep%2520Kumar%2520and%2520Prashant%2520Nagpal%2520and%2520Jiwei%2520Zhao%2520and%2520Ming%2520Dong%2520and%2520Carri%2520K.%2520Glide-Hurst%26entry.1292438233%3D%2520%2520Cardiac%2520substructures%2520are%2520essential%2520in%2520thoracic%2520radiation%2520therapy%2520planning%2520to%250Aminimize%2520risk%2520of%2520radiation-induced%2520heart%2520disease.%2520Deep%2520learning%2520%2528DL%2529%2520offers%250Aefficient%2520methods%2520to%2520reduce%2520contouring%2520burden%2520but%2520lacks%2520generalizability%2520across%250Adifferent%2520modalities%2520and%2520overlapping%2520structures.%2520This%2520work%2520introduces%2520and%250Avalidates%2520a%2520Modality-AGnostic%2520Image%2520Cascade%2520%2528MAGIC%2529%2520for%2520comprehensive%2520and%250Amulti-modal%2520cardiac%2520substructure%2520segmentation.%2520MAGIC%2520is%2520implemented%2520through%250Areplicated%2520encoding%2520and%2520decoding%2520branches%2520of%2520an%2520nnU-Net-based%252C%2520U-shaped%250Abackbone%2520conserving%2520the%2520function%2520of%2520a%2520single%2520model.%2520Twenty%2520cardiac%250Asubstructures%2520%2528heart%252C%2520chambers%252C%2520great%2520vessels%2520%2528GVs%2529%252C%2520valves%252C%2520coronary%2520arteries%250A%2528CAs%2529%252C%2520and%2520conduction%2520nodes%2529%2520from%2520simulation%2520CT%2520%2528Sim-CT%2529%252C%2520low-field%2520MR-Linac%252C%250Aand%2520cardiac%2520CT%2520angiography%2520%2528CCTA%2529%2520modalities%2520were%2520manually%2520delineated%2520and%2520used%250Ato%2520train%2520%2528n%253D76%2529%252C%2520validate%2520%2528n%253D15%2529%252C%2520and%2520test%2520%2528n%253D30%2529%2520MAGIC.%2520Twelve%2520comparison%250Amodels%2520%2528four%2520segmentation%2520subgroups%2520across%2520three%2520modalities%2529%2520were%2520equivalently%250Atrained.%2520All%2520methods%2520were%2520compared%2520for%2520training%2520efficiency%2520and%2520against%250Areference%2520contours%2520using%2520the%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520and%2520two-tailed%250AWilcoxon%2520Signed-Rank%2520test%2520%2528threshold%252C%2520p%253C0.05%2529.%2520Average%2520DSC%2520scores%2520were%250A0.75%25280.16%2529%2520for%2520Sim-CT%252C%25200.68%25280.21%2529%2520for%2520MR-Linac%252C%2520and%25200.80%25280.16%2529%2520for%2520CCTA.%2520MAGIC%250Aoutperforms%2520the%2520comparison%2520in%252057%2525%2520of%2520cases%252C%2520with%2520limited%2520statistical%250Adifferences.%2520MAGIC%2520offers%2520an%2520effective%2520and%2520accurate%2520segmentation%2520solution%2520that%250Ais%2520lightweight%2520and%2520capable%2520of%2520segmenting%2520multiple%2520modalities%2520and%2520overlapping%250Astructures%2520in%2520a%2520single%2520model.%2520MAGIC%2520further%2520enables%2520clinical%2520implementation%2520by%250Asimplifying%2520the%2520computational%2520requirements%2520and%2520offering%2520unparalleled%250Aflexibility%2520for%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-AGnostic%20Image%20Cascade%20%28MAGIC%29%20for%20Multi-Modality%20Cardiac%0A%20%20Substructure%20Segmentation&entry.906535625=Nicholas%20Summerfield%20and%20Qisheng%20He%20and%20Alex%20Kuo%20and%20Ahmed%20I.%20Ghanem%20and%20Simeng%20Zhu%20and%20Chase%20Ruff%20and%20Joshua%20Pan%20and%20Anudeep%20Kumar%20and%20Prashant%20Nagpal%20and%20Jiwei%20Zhao%20and%20Ming%20Dong%20and%20Carri%20K.%20Glide-Hurst&entry.1292438233=%20%20Cardiac%20substructures%20are%20essential%20in%20thoracic%20radiation%20therapy%20planning%20to%0Aminimize%20risk%20of%20radiation-induced%20heart%20disease.%20Deep%20learning%20%28DL%29%20offers%0Aefficient%20methods%20to%20reduce%20contouring%20burden%20but%20lacks%20generalizability%20across%0Adifferent%20modalities%20and%20overlapping%20structures.%20This%20work%20introduces%20and%0Avalidates%20a%20Modality-AGnostic%20Image%20Cascade%20%28MAGIC%29%20for%20comprehensive%20and%0Amulti-modal%20cardiac%20substructure%20segmentation.%20MAGIC%20is%20implemented%20through%0Areplicated%20encoding%20and%20decoding%20branches%20of%20an%20nnU-Net-based%2C%20U-shaped%0Abackbone%20conserving%20the%20function%20of%20a%20single%20model.%20Twenty%20cardiac%0Asubstructures%20%28heart%2C%20chambers%2C%20great%20vessels%20%28GVs%29%2C%20valves%2C%20coronary%20arteries%0A%28CAs%29%2C%20and%20conduction%20nodes%29%20from%20simulation%20CT%20%28Sim-CT%29%2C%20low-field%20MR-Linac%2C%0Aand%20cardiac%20CT%20angiography%20%28CCTA%29%20modalities%20were%20manually%20delineated%20and%20used%0Ato%20train%20%28n%3D76%29%2C%20validate%20%28n%3D15%29%2C%20and%20test%20%28n%3D30%29%20MAGIC.%20Twelve%20comparison%0Amodels%20%28four%20segmentation%20subgroups%20across%20three%20modalities%29%20were%20equivalently%0Atrained.%20All%20methods%20were%20compared%20for%20training%20efficiency%20and%20against%0Areference%20contours%20using%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29%20and%20two-tailed%0AWilcoxon%20Signed-Rank%20test%20%28threshold%2C%20p%3C0.05%29.%20Average%20DSC%20scores%20were%0A0.75%280.16%29%20for%20Sim-CT%2C%200.68%280.21%29%20for%20MR-Linac%2C%20and%200.80%280.16%29%20for%20CCTA.%20MAGIC%0Aoutperforms%20the%20comparison%20in%2057%25%20of%20cases%2C%20with%20limited%20statistical%0Adifferences.%20MAGIC%20offers%20an%20effective%20and%20accurate%20segmentation%20solution%20that%0Ais%20lightweight%20and%20capable%20of%20segmenting%20multiple%20modalities%20and%20overlapping%0Astructures%20in%20a%20single%20model.%20MAGIC%20further%20enables%20clinical%20implementation%20by%0Asimplifying%20the%20computational%20requirements%20and%20offering%20unparalleled%0Aflexibility%20for%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10797v1&entry.124074799=Read"},
{"title": "Divide-Fuse-Conquer: Eliciting \"Aha Moments\" in Multi-Scenario Games", "author": "Xiaoqing Zhang and Huabin Zheng and Ang Lv and Yuhan Liu and Zirui Song and Xiuying Chen and Rui Yan and Flood Sung", "abstract": "  Large language models (LLMs) have been observed to suddenly exhibit advanced\nreasoning abilities during reinforcement learning (RL), resembling an ``aha\nmoment'' triggered by simple outcome-based rewards. While RL has proven\neffective in eliciting such breakthroughs in tasks involving mathematics,\ncoding, and vision, it faces significant challenges in multi-scenario games.\nThe diversity of game rules, interaction modes, and environmental complexities\noften leads to policies that perform well in one scenario but fail to\ngeneralize to others. Simply combining multiple scenarios during training\nintroduces additional challenges, such as training instability and poor\nperformance. To overcome these challenges, we propose Divide-Fuse-Conquer, a\nframework designed to enhance generalization in multi-scenario RL. This\napproach starts by heuristically grouping games based on characteristics such\nas rules and difficulties. Specialized models are then trained for each group\nto excel at games in the group is what we refer to as the divide step. Next, we\nfuse model parameters from different groups as a new model, and continue\ntraining it for multiple groups, until the scenarios in all groups are\nconquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align\ntrained with the Divide-Fuse-Conquer strategy reaches a performance level\ncomparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can\ninspire future research on using reinforcement learning to improve the\ngeneralization of LLMs.\n", "link": "http://arxiv.org/abs/2505.16401v4", "date": "2025-06-12", "relevancy": 2.201, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide-Fuse-Conquer%3A%20Eliciting%20%22Aha%20Moments%22%20in%20Multi-Scenario%20Games&body=Title%3A%20Divide-Fuse-Conquer%3A%20Eliciting%20%22Aha%20Moments%22%20in%20Multi-Scenario%20Games%0AAuthor%3A%20Xiaoqing%20Zhang%20and%20Huabin%20Zheng%20and%20Ang%20Lv%20and%20Yuhan%20Liu%20and%20Zirui%20Song%20and%20Xiuying%20Chen%20and%20Rui%20Yan%20and%20Flood%20Sung%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20observed%20to%20suddenly%20exhibit%20advanced%0Areasoning%20abilities%20during%20reinforcement%20learning%20%28RL%29%2C%20resembling%20an%20%60%60aha%0Amoment%27%27%20triggered%20by%20simple%20outcome-based%20rewards.%20While%20RL%20has%20proven%0Aeffective%20in%20eliciting%20such%20breakthroughs%20in%20tasks%20involving%20mathematics%2C%0Acoding%2C%20and%20vision%2C%20it%20faces%20significant%20challenges%20in%20multi-scenario%20games.%0AThe%20diversity%20of%20game%20rules%2C%20interaction%20modes%2C%20and%20environmental%20complexities%0Aoften%20leads%20to%20policies%20that%20perform%20well%20in%20one%20scenario%20but%20fail%20to%0Ageneralize%20to%20others.%20Simply%20combining%20multiple%20scenarios%20during%20training%0Aintroduces%20additional%20challenges%2C%20such%20as%20training%20instability%20and%20poor%0Aperformance.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Divide-Fuse-Conquer%2C%20a%0Aframework%20designed%20to%20enhance%20generalization%20in%20multi-scenario%20RL.%20This%0Aapproach%20starts%20by%20heuristically%20grouping%20games%20based%20on%20characteristics%20such%0Aas%20rules%20and%20difficulties.%20Specialized%20models%20are%20then%20trained%20for%20each%20group%0Ato%20excel%20at%20games%20in%20the%20group%20is%20what%20we%20refer%20to%20as%20the%20divide%20step.%20Next%2C%20we%0Afuse%20model%20parameters%20from%20different%20groups%20as%20a%20new%20model%2C%20and%20continue%0Atraining%20it%20for%20multiple%20groups%2C%20until%20the%20scenarios%20in%20all%20groups%20are%0Aconquered.%20Experiments%20across%2018%20TextArena%20games%20show%20that%20Qwen2.5-32B-Align%0Atrained%20with%20the%20Divide-Fuse-Conquer%20strategy%20reaches%20a%20performance%20level%0Acomparable%20to%20Claude3.5%2C%20achieving%207%20wins%20and%204%20draws.%20We%20hope%20our%20approach%20can%0Ainspire%20future%20research%20on%20using%20reinforcement%20learning%20to%20improve%20the%0Ageneralization%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16401v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide-Fuse-Conquer%253A%2520Eliciting%2520%2522Aha%2520Moments%2522%2520in%2520Multi-Scenario%2520Games%26entry.906535625%3DXiaoqing%2520Zhang%2520and%2520Huabin%2520Zheng%2520and%2520Ang%2520Lv%2520and%2520Yuhan%2520Liu%2520and%2520Zirui%2520Song%2520and%2520Xiuying%2520Chen%2520and%2520Rui%2520Yan%2520and%2520Flood%2520Sung%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520observed%2520to%2520suddenly%2520exhibit%2520advanced%250Areasoning%2520abilities%2520during%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520resembling%2520an%2520%2560%2560aha%250Amoment%2527%2527%2520triggered%2520by%2520simple%2520outcome-based%2520rewards.%2520While%2520RL%2520has%2520proven%250Aeffective%2520in%2520eliciting%2520such%2520breakthroughs%2520in%2520tasks%2520involving%2520mathematics%252C%250Acoding%252C%2520and%2520vision%252C%2520it%2520faces%2520significant%2520challenges%2520in%2520multi-scenario%2520games.%250AThe%2520diversity%2520of%2520game%2520rules%252C%2520interaction%2520modes%252C%2520and%2520environmental%2520complexities%250Aoften%2520leads%2520to%2520policies%2520that%2520perform%2520well%2520in%2520one%2520scenario%2520but%2520fail%2520to%250Ageneralize%2520to%2520others.%2520Simply%2520combining%2520multiple%2520scenarios%2520during%2520training%250Aintroduces%2520additional%2520challenges%252C%2520such%2520as%2520training%2520instability%2520and%2520poor%250Aperformance.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520Divide-Fuse-Conquer%252C%2520a%250Aframework%2520designed%2520to%2520enhance%2520generalization%2520in%2520multi-scenario%2520RL.%2520This%250Aapproach%2520starts%2520by%2520heuristically%2520grouping%2520games%2520based%2520on%2520characteristics%2520such%250Aas%2520rules%2520and%2520difficulties.%2520Specialized%2520models%2520are%2520then%2520trained%2520for%2520each%2520group%250Ato%2520excel%2520at%2520games%2520in%2520the%2520group%2520is%2520what%2520we%2520refer%2520to%2520as%2520the%2520divide%2520step.%2520Next%252C%2520we%250Afuse%2520model%2520parameters%2520from%2520different%2520groups%2520as%2520a%2520new%2520model%252C%2520and%2520continue%250Atraining%2520it%2520for%2520multiple%2520groups%252C%2520until%2520the%2520scenarios%2520in%2520all%2520groups%2520are%250Aconquered.%2520Experiments%2520across%252018%2520TextArena%2520games%2520show%2520that%2520Qwen2.5-32B-Align%250Atrained%2520with%2520the%2520Divide-Fuse-Conquer%2520strategy%2520reaches%2520a%2520performance%2520level%250Acomparable%2520to%2520Claude3.5%252C%2520achieving%25207%2520wins%2520and%25204%2520draws.%2520We%2520hope%2520our%2520approach%2520can%250Ainspire%2520future%2520research%2520on%2520using%2520reinforcement%2520learning%2520to%2520improve%2520the%250Ageneralization%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16401v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide-Fuse-Conquer%3A%20Eliciting%20%22Aha%20Moments%22%20in%20Multi-Scenario%20Games&entry.906535625=Xiaoqing%20Zhang%20and%20Huabin%20Zheng%20and%20Ang%20Lv%20and%20Yuhan%20Liu%20and%20Zirui%20Song%20and%20Xiuying%20Chen%20and%20Rui%20Yan%20and%20Flood%20Sung&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20observed%20to%20suddenly%20exhibit%20advanced%0Areasoning%20abilities%20during%20reinforcement%20learning%20%28RL%29%2C%20resembling%20an%20%60%60aha%0Amoment%27%27%20triggered%20by%20simple%20outcome-based%20rewards.%20While%20RL%20has%20proven%0Aeffective%20in%20eliciting%20such%20breakthroughs%20in%20tasks%20involving%20mathematics%2C%0Acoding%2C%20and%20vision%2C%20it%20faces%20significant%20challenges%20in%20multi-scenario%20games.%0AThe%20diversity%20of%20game%20rules%2C%20interaction%20modes%2C%20and%20environmental%20complexities%0Aoften%20leads%20to%20policies%20that%20perform%20well%20in%20one%20scenario%20but%20fail%20to%0Ageneralize%20to%20others.%20Simply%20combining%20multiple%20scenarios%20during%20training%0Aintroduces%20additional%20challenges%2C%20such%20as%20training%20instability%20and%20poor%0Aperformance.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Divide-Fuse-Conquer%2C%20a%0Aframework%20designed%20to%20enhance%20generalization%20in%20multi-scenario%20RL.%20This%0Aapproach%20starts%20by%20heuristically%20grouping%20games%20based%20on%20characteristics%20such%0Aas%20rules%20and%20difficulties.%20Specialized%20models%20are%20then%20trained%20for%20each%20group%0Ato%20excel%20at%20games%20in%20the%20group%20is%20what%20we%20refer%20to%20as%20the%20divide%20step.%20Next%2C%20we%0Afuse%20model%20parameters%20from%20different%20groups%20as%20a%20new%20model%2C%20and%20continue%0Atraining%20it%20for%20multiple%20groups%2C%20until%20the%20scenarios%20in%20all%20groups%20are%0Aconquered.%20Experiments%20across%2018%20TextArena%20games%20show%20that%20Qwen2.5-32B-Align%0Atrained%20with%20the%20Divide-Fuse-Conquer%20strategy%20reaches%20a%20performance%20level%0Acomparable%20to%20Claude3.5%2C%20achieving%207%20wins%20and%204%20draws.%20We%20hope%20our%20approach%20can%0Ainspire%20future%20research%20on%20using%20reinforcement%20learning%20to%20improve%20the%0Ageneralization%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16401v4&entry.124074799=Read"},
{"title": "OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces\n  Optimization Problems", "author": "Xiaozhe Li and Jixuan Chen and Xinyu Fang and Shengyuan Ding and Haodong Duan and Qingwen Liu and Kai Chen", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in solving\ndiverse tasks. However, their proficiency in iteratively optimizing complex\nsolutions through learning from previous feedback remains insufficiently\nexplored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark\ndesigned to evaluate LLM agents on large-scale search space optimization\nproblems. OPT-BENCH includes 20 real-world machine learning tasks sourced from\nKaggle and 10 classical NP problems, offering a diverse and challenging\nenvironment for assessing LLM agents on iterative reasoning and solution\nrefinement. To enable rigorous evaluation, we introduce OPT-Agent, an\nend-to-end optimization framework that emulates human reasoning when tackling\ncomplex problems by generating, validating, and iteratively improving solutions\nthrough leveraging historical feedback. Through extensive experiments on 9\nstate-of-the-art LLMs from 6 model families, we analyze the effects of\noptimization iterations, temperature settings, and model architectures on\nsolution quality and convergence. Our results demonstrate that incorporating\nhistorical context significantly enhances optimization performance across both\nML and NP tasks. All datasets, code, and evaluation tools are open-sourced to\npromote further research in advancing LLM-driven optimization and iterative\nreasoning. Project page:\n\\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.\n", "link": "http://arxiv.org/abs/2506.10764v1", "date": "2025-06-12", "relevancy": 2.1833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPT-BENCH%3A%20Evaluating%20LLM%20Agent%20on%20Large-Scale%20Search%20Spaces%0A%20%20Optimization%20Problems&body=Title%3A%20OPT-BENCH%3A%20Evaluating%20LLM%20Agent%20on%20Large-Scale%20Search%20Spaces%0A%20%20Optimization%20Problems%0AAuthor%3A%20Xiaozhe%20Li%20and%20Jixuan%20Chen%20and%20Xinyu%20Fang%20and%20Shengyuan%20Ding%20and%20Haodong%20Duan%20and%20Qingwen%20Liu%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20solving%0Adiverse%20tasks.%20However%2C%20their%20proficiency%20in%20iteratively%20optimizing%20complex%0Asolutions%20through%20learning%20from%20previous%20feedback%20remains%20insufficiently%0Aexplored.%20To%20bridge%20this%20gap%2C%20we%20present%20OPT-BENCH%2C%20a%20comprehensive%20benchmark%0Adesigned%20to%20evaluate%20LLM%20agents%20on%20large-scale%20search%20space%20optimization%0Aproblems.%20OPT-BENCH%20includes%2020%20real-world%20machine%20learning%20tasks%20sourced%20from%0AKaggle%20and%2010%20classical%20NP%20problems%2C%20offering%20a%20diverse%20and%20challenging%0Aenvironment%20for%20assessing%20LLM%20agents%20on%20iterative%20reasoning%20and%20solution%0Arefinement.%20To%20enable%20rigorous%20evaluation%2C%20we%20introduce%20OPT-Agent%2C%20an%0Aend-to-end%20optimization%20framework%20that%20emulates%20human%20reasoning%20when%20tackling%0Acomplex%20problems%20by%20generating%2C%20validating%2C%20and%20iteratively%20improving%20solutions%0Athrough%20leveraging%20historical%20feedback.%20Through%20extensive%20experiments%20on%209%0Astate-of-the-art%20LLMs%20from%206%20model%20families%2C%20we%20analyze%20the%20effects%20of%0Aoptimization%20iterations%2C%20temperature%20settings%2C%20and%20model%20architectures%20on%0Asolution%20quality%20and%20convergence.%20Our%20results%20demonstrate%20that%20incorporating%0Ahistorical%20context%20significantly%20enhances%20optimization%20performance%20across%20both%0AML%20and%20NP%20tasks.%20All%20datasets%2C%20code%2C%20and%20evaluation%20tools%20are%20open-sourced%20to%0Apromote%20further%20research%20in%20advancing%20LLM-driven%20optimization%20and%20iterative%0Areasoning.%20Project%20page%3A%0A%5Chref%7Bhttps%3A//github.com/OliverLeeXZ/OPT-BENCH%7D%7Bhttps%3A//github.com/OliverLeeXZ/OPT-BENCH%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPT-BENCH%253A%2520Evaluating%2520LLM%2520Agent%2520on%2520Large-Scale%2520Search%2520Spaces%250A%2520%2520Optimization%2520Problems%26entry.906535625%3DXiaozhe%2520Li%2520and%2520Jixuan%2520Chen%2520and%2520Xinyu%2520Fang%2520and%2520Shengyuan%2520Ding%2520and%2520Haodong%2520Duan%2520and%2520Qingwen%2520Liu%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520solving%250Adiverse%2520tasks.%2520However%252C%2520their%2520proficiency%2520in%2520iteratively%2520optimizing%2520complex%250Asolutions%2520through%2520learning%2520from%2520previous%2520feedback%2520remains%2520insufficiently%250Aexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520OPT-BENCH%252C%2520a%2520comprehensive%2520benchmark%250Adesigned%2520to%2520evaluate%2520LLM%2520agents%2520on%2520large-scale%2520search%2520space%2520optimization%250Aproblems.%2520OPT-BENCH%2520includes%252020%2520real-world%2520machine%2520learning%2520tasks%2520sourced%2520from%250AKaggle%2520and%252010%2520classical%2520NP%2520problems%252C%2520offering%2520a%2520diverse%2520and%2520challenging%250Aenvironment%2520for%2520assessing%2520LLM%2520agents%2520on%2520iterative%2520reasoning%2520and%2520solution%250Arefinement.%2520To%2520enable%2520rigorous%2520evaluation%252C%2520we%2520introduce%2520OPT-Agent%252C%2520an%250Aend-to-end%2520optimization%2520framework%2520that%2520emulates%2520human%2520reasoning%2520when%2520tackling%250Acomplex%2520problems%2520by%2520generating%252C%2520validating%252C%2520and%2520iteratively%2520improving%2520solutions%250Athrough%2520leveraging%2520historical%2520feedback.%2520Through%2520extensive%2520experiments%2520on%25209%250Astate-of-the-art%2520LLMs%2520from%25206%2520model%2520families%252C%2520we%2520analyze%2520the%2520effects%2520of%250Aoptimization%2520iterations%252C%2520temperature%2520settings%252C%2520and%2520model%2520architectures%2520on%250Asolution%2520quality%2520and%2520convergence.%2520Our%2520results%2520demonstrate%2520that%2520incorporating%250Ahistorical%2520context%2520significantly%2520enhances%2520optimization%2520performance%2520across%2520both%250AML%2520and%2520NP%2520tasks.%2520All%2520datasets%252C%2520code%252C%2520and%2520evaluation%2520tools%2520are%2520open-sourced%2520to%250Apromote%2520further%2520research%2520in%2520advancing%2520LLM-driven%2520optimization%2520and%2520iterative%250Areasoning.%2520Project%2520page%253A%250A%255Chref%257Bhttps%253A//github.com/OliverLeeXZ/OPT-BENCH%257D%257Bhttps%253A//github.com/OliverLeeXZ/OPT-BENCH%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPT-BENCH%3A%20Evaluating%20LLM%20Agent%20on%20Large-Scale%20Search%20Spaces%0A%20%20Optimization%20Problems&entry.906535625=Xiaozhe%20Li%20and%20Jixuan%20Chen%20and%20Xinyu%20Fang%20and%20Shengyuan%20Ding%20and%20Haodong%20Duan%20and%20Qingwen%20Liu%20and%20Kai%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20solving%0Adiverse%20tasks.%20However%2C%20their%20proficiency%20in%20iteratively%20optimizing%20complex%0Asolutions%20through%20learning%20from%20previous%20feedback%20remains%20insufficiently%0Aexplored.%20To%20bridge%20this%20gap%2C%20we%20present%20OPT-BENCH%2C%20a%20comprehensive%20benchmark%0Adesigned%20to%20evaluate%20LLM%20agents%20on%20large-scale%20search%20space%20optimization%0Aproblems.%20OPT-BENCH%20includes%2020%20real-world%20machine%20learning%20tasks%20sourced%20from%0AKaggle%20and%2010%20classical%20NP%20problems%2C%20offering%20a%20diverse%20and%20challenging%0Aenvironment%20for%20assessing%20LLM%20agents%20on%20iterative%20reasoning%20and%20solution%0Arefinement.%20To%20enable%20rigorous%20evaluation%2C%20we%20introduce%20OPT-Agent%2C%20an%0Aend-to-end%20optimization%20framework%20that%20emulates%20human%20reasoning%20when%20tackling%0Acomplex%20problems%20by%20generating%2C%20validating%2C%20and%20iteratively%20improving%20solutions%0Athrough%20leveraging%20historical%20feedback.%20Through%20extensive%20experiments%20on%209%0Astate-of-the-art%20LLMs%20from%206%20model%20families%2C%20we%20analyze%20the%20effects%20of%0Aoptimization%20iterations%2C%20temperature%20settings%2C%20and%20model%20architectures%20on%0Asolution%20quality%20and%20convergence.%20Our%20results%20demonstrate%20that%20incorporating%0Ahistorical%20context%20significantly%20enhances%20optimization%20performance%20across%20both%0AML%20and%20NP%20tasks.%20All%20datasets%2C%20code%2C%20and%20evaluation%20tools%20are%20open-sourced%20to%0Apromote%20further%20research%20in%20advancing%20LLM-driven%20optimization%20and%20iterative%0Areasoning.%20Project%20page%3A%0A%5Chref%7Bhttps%3A//github.com/OliverLeeXZ/OPT-BENCH%7D%7Bhttps%3A//github.com/OliverLeeXZ/OPT-BENCH%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10764v1&entry.124074799=Read"},
{"title": "CoRT: Code-integrated Reasoning within Thinking", "author": "Chengpeng Li and Zhengyang Tang and Ziniu Li and Mingfeng Xue and Keqin Bao and Tian Ding and Ruoyu Sun and Benyou Wang and Xiang Wang and Junyang Lin and Dayiheng Liu", "abstract": "  Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.\n", "link": "http://arxiv.org/abs/2506.09820v2", "date": "2025-06-12", "relevancy": 2.1689, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoRT%3A%20Code-integrated%20Reasoning%20within%20Thinking&body=Title%3A%20CoRT%3A%20Code-integrated%20Reasoning%20within%20Thinking%0AAuthor%3A%20Chengpeng%20Li%20and%20Zhengyang%20Tang%20and%20Ziniu%20Li%20and%20Mingfeng%20Xue%20and%20Keqin%20Bao%20and%20Tian%20Ding%20and%20Ruoyu%20Sun%20and%20Benyou%20Wang%20and%20Xiang%20Wang%20and%20Junyang%20Lin%20and%20Dayiheng%20Liu%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20like%20o1%20and%20DeepSeek-R1%20have%20shown%20remarkable%0Aprogress%20in%20natural%20language%20reasoning%20with%20long%20chain-of-thought%20%28CoT%29%2C%20yet%0Athey%20remain%20inefficient%20or%20inaccurate%20when%20handling%20complex%20mathematical%0Aoperations.%20Addressing%20these%20limitations%20through%20computational%20tools%20%28e.g.%2C%0Acomputation%20libraries%20and%20symbolic%20solvers%29%20is%20promising%2C%20but%20it%20introduces%20a%0Atechnical%20challenge%3A%20Code%20Interpreter%20%28CI%29%20brings%20external%20knowledge%20beyond%20the%0Amodel%27s%20internal%20text%20representations%2C%20thus%20the%20direct%20combination%20is%20not%0Aefficient.%20This%20paper%20introduces%20CoRT%2C%20a%20post-training%20framework%20for%20teaching%0ALRMs%20to%20leverage%20CI%20effectively%20and%20efficiently.%20As%20a%20first%20step%2C%20we%20address%0Athe%20data%20scarcity%20issue%20by%20synthesizing%20code-integrated%20reasoning%20data%20through%0AHint-Engineering%2C%20which%20strategically%20inserts%20different%20hints%20at%20appropriate%0Apositions%20to%20optimize%20LRM-CI%20interaction.%20We%20manually%20create%2030%20high-quality%0Asamples%2C%20upon%20which%20we%20post-train%20models%20ranging%20from%201.5B%20to%2032B%20parameters%2C%0Awith%20supervised%20fine-tuning%2C%20rejection%20fine-tuning%20and%20reinforcement%20learning.%0AOur%20experimental%20results%20demonstrate%20that%20Hint-Engineering%20models%20achieve%204%5C%25%0Aand%208%5C%25%20absolute%20improvements%20on%20DeepSeek-R1-Distill-Qwen-32B%20and%0ADeepSeek-R1-Distill-Qwen-1.5B%20respectively%2C%20across%20five%20challenging%0Amathematical%20reasoning%20datasets.%20Furthermore%2C%20Hint-Engineering%20models%20use%20about%0A30%5C%25%20fewer%20tokens%20for%20the%2032B%20model%20and%2050%5C%25%20fewer%20tokens%20for%20the%201.5B%20model%0Acompared%20with%20the%20natural%20language%20models.%20The%20models%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ChengpengLi1003/CoRT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoRT%253A%2520Code-integrated%2520Reasoning%2520within%2520Thinking%26entry.906535625%3DChengpeng%2520Li%2520and%2520Zhengyang%2520Tang%2520and%2520Ziniu%2520Li%2520and%2520Mingfeng%2520Xue%2520and%2520Keqin%2520Bao%2520and%2520Tian%2520Ding%2520and%2520Ruoyu%2520Sun%2520and%2520Benyou%2520Wang%2520and%2520Xiang%2520Wang%2520and%2520Junyang%2520Lin%2520and%2520Dayiheng%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520like%2520o1%2520and%2520DeepSeek-R1%2520have%2520shown%2520remarkable%250Aprogress%2520in%2520natural%2520language%2520reasoning%2520with%2520long%2520chain-of-thought%2520%2528CoT%2529%252C%2520yet%250Athey%2520remain%2520inefficient%2520or%2520inaccurate%2520when%2520handling%2520complex%2520mathematical%250Aoperations.%2520Addressing%2520these%2520limitations%2520through%2520computational%2520tools%2520%2528e.g.%252C%250Acomputation%2520libraries%2520and%2520symbolic%2520solvers%2529%2520is%2520promising%252C%2520but%2520it%2520introduces%2520a%250Atechnical%2520challenge%253A%2520Code%2520Interpreter%2520%2528CI%2529%2520brings%2520external%2520knowledge%2520beyond%2520the%250Amodel%2527s%2520internal%2520text%2520representations%252C%2520thus%2520the%2520direct%2520combination%2520is%2520not%250Aefficient.%2520This%2520paper%2520introduces%2520CoRT%252C%2520a%2520post-training%2520framework%2520for%2520teaching%250ALRMs%2520to%2520leverage%2520CI%2520effectively%2520and%2520efficiently.%2520As%2520a%2520first%2520step%252C%2520we%2520address%250Athe%2520data%2520scarcity%2520issue%2520by%2520synthesizing%2520code-integrated%2520reasoning%2520data%2520through%250AHint-Engineering%252C%2520which%2520strategically%2520inserts%2520different%2520hints%2520at%2520appropriate%250Apositions%2520to%2520optimize%2520LRM-CI%2520interaction.%2520We%2520manually%2520create%252030%2520high-quality%250Asamples%252C%2520upon%2520which%2520we%2520post-train%2520models%2520ranging%2520from%25201.5B%2520to%252032B%2520parameters%252C%250Awith%2520supervised%2520fine-tuning%252C%2520rejection%2520fine-tuning%2520and%2520reinforcement%2520learning.%250AOur%2520experimental%2520results%2520demonstrate%2520that%2520Hint-Engineering%2520models%2520achieve%25204%255C%2525%250Aand%25208%255C%2525%2520absolute%2520improvements%2520on%2520DeepSeek-R1-Distill-Qwen-32B%2520and%250ADeepSeek-R1-Distill-Qwen-1.5B%2520respectively%252C%2520across%2520five%2520challenging%250Amathematical%2520reasoning%2520datasets.%2520Furthermore%252C%2520Hint-Engineering%2520models%2520use%2520about%250A30%255C%2525%2520fewer%2520tokens%2520for%2520the%252032B%2520model%2520and%252050%255C%2525%2520fewer%2520tokens%2520for%2520the%25201.5B%2520model%250Acompared%2520with%2520the%2520natural%2520language%2520models.%2520The%2520models%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/ChengpengLi1003/CoRT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRT%3A%20Code-integrated%20Reasoning%20within%20Thinking&entry.906535625=Chengpeng%20Li%20and%20Zhengyang%20Tang%20and%20Ziniu%20Li%20and%20Mingfeng%20Xue%20and%20Keqin%20Bao%20and%20Tian%20Ding%20and%20Ruoyu%20Sun%20and%20Benyou%20Wang%20and%20Xiang%20Wang%20and%20Junyang%20Lin%20and%20Dayiheng%20Liu&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20like%20o1%20and%20DeepSeek-R1%20have%20shown%20remarkable%0Aprogress%20in%20natural%20language%20reasoning%20with%20long%20chain-of-thought%20%28CoT%29%2C%20yet%0Athey%20remain%20inefficient%20or%20inaccurate%20when%20handling%20complex%20mathematical%0Aoperations.%20Addressing%20these%20limitations%20through%20computational%20tools%20%28e.g.%2C%0Acomputation%20libraries%20and%20symbolic%20solvers%29%20is%20promising%2C%20but%20it%20introduces%20a%0Atechnical%20challenge%3A%20Code%20Interpreter%20%28CI%29%20brings%20external%20knowledge%20beyond%20the%0Amodel%27s%20internal%20text%20representations%2C%20thus%20the%20direct%20combination%20is%20not%0Aefficient.%20This%20paper%20introduces%20CoRT%2C%20a%20post-training%20framework%20for%20teaching%0ALRMs%20to%20leverage%20CI%20effectively%20and%20efficiently.%20As%20a%20first%20step%2C%20we%20address%0Athe%20data%20scarcity%20issue%20by%20synthesizing%20code-integrated%20reasoning%20data%20through%0AHint-Engineering%2C%20which%20strategically%20inserts%20different%20hints%20at%20appropriate%0Apositions%20to%20optimize%20LRM-CI%20interaction.%20We%20manually%20create%2030%20high-quality%0Asamples%2C%20upon%20which%20we%20post-train%20models%20ranging%20from%201.5B%20to%2032B%20parameters%2C%0Awith%20supervised%20fine-tuning%2C%20rejection%20fine-tuning%20and%20reinforcement%20learning.%0AOur%20experimental%20results%20demonstrate%20that%20Hint-Engineering%20models%20achieve%204%5C%25%0Aand%208%5C%25%20absolute%20improvements%20on%20DeepSeek-R1-Distill-Qwen-32B%20and%0ADeepSeek-R1-Distill-Qwen-1.5B%20respectively%2C%20across%20five%20challenging%0Amathematical%20reasoning%20datasets.%20Furthermore%2C%20Hint-Engineering%20models%20use%20about%0A30%5C%25%20fewer%20tokens%20for%20the%2032B%20model%20and%2050%5C%25%20fewer%20tokens%20for%20the%201.5B%20model%0Acompared%20with%20the%20natural%20language%20models.%20The%20models%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ChengpengLi1003/CoRT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09820v2&entry.124074799=Read"},
{"title": "ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability\n  in MLLMs with Free-Style Intermediate State Representations", "author": "Xuecheng Wu and Jiaxing Liu and Danlei Huang and Xiaoyu Li and Yifan Wang and Chen Chen and Liya Ma and Xuezhi Cao and Junxiao Xue", "abstract": "  Visual-Interleaved Chain-of-Thought (VI-CoT) enables MLLMs to continually\nupdate their understanding and decisions based on step-wise intermediate visual\nstates (IVS), much like a human would, which demonstrates impressive success in\nvarious tasks, thereby leading to emerged advancements in related benchmarks.\nDespite promising progress, current benchmarks provide models with relatively\nfixed IVS, rather than free-style IVS, whch might forcibly distort the original\nthinking trajectories, failing to evaluate their intrinsic reasoning\ncapabilities. More importantly, existing benchmarks neglect to systematically\nexplore the impact factors that IVS would impart to untamed reasoning\nperformance. To tackle above gaps, we introduce a specialized benchmark termed\nViC-Bench, consisting of four representive tasks: maze navigation, jigsaw\npuzzle, embodied long-horizon planning, and complex counting, where each task\nhas dedicated free-style IVS generation pipeline supporting function calls. To\nsystematically examine VI-CoT capability, we propose a thorough evaluation\nsuite incorporating a progressive three-stage strategy with targeted new\nmetrics. Besides, we establish Incremental Prompting Information Injection\n(IPII) strategy to ablatively explore the prompting factors for VI-CoT. We\nextensively conduct evaluations for 18 advanced MLLMs, revealing key insights\ninto their VI-CoT capability. Our proposed benchmark is publicly open at\nHuggingface.\n", "link": "http://arxiv.org/abs/2505.14404v2", "date": "2025-06-12", "relevancy": 2.1613, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5505}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViC-Bench%3A%20Benchmarking%20Visual-Interleaved%20Chain-of-Thought%20Capability%0A%20%20in%20MLLMs%20with%20Free-Style%20Intermediate%20State%20Representations&body=Title%3A%20ViC-Bench%3A%20Benchmarking%20Visual-Interleaved%20Chain-of-Thought%20Capability%0A%20%20in%20MLLMs%20with%20Free-Style%20Intermediate%20State%20Representations%0AAuthor%3A%20Xuecheng%20Wu%20and%20Jiaxing%20Liu%20and%20Danlei%20Huang%20and%20Xiaoyu%20Li%20and%20Yifan%20Wang%20and%20Chen%20Chen%20and%20Liya%20Ma%20and%20Xuezhi%20Cao%20and%20Junxiao%20Xue%0AAbstract%3A%20%20%20Visual-Interleaved%20Chain-of-Thought%20%28VI-CoT%29%20enables%20MLLMs%20to%20continually%0Aupdate%20their%20understanding%20and%20decisions%20based%20on%20step-wise%20intermediate%20visual%0Astates%20%28IVS%29%2C%20much%20like%20a%20human%20would%2C%20which%20demonstrates%20impressive%20success%20in%0Avarious%20tasks%2C%20thereby%20leading%20to%20emerged%20advancements%20in%20related%20benchmarks.%0ADespite%20promising%20progress%2C%20current%20benchmarks%20provide%20models%20with%20relatively%0Afixed%20IVS%2C%20rather%20than%20free-style%20IVS%2C%20whch%20might%20forcibly%20distort%20the%20original%0Athinking%20trajectories%2C%20failing%20to%20evaluate%20their%20intrinsic%20reasoning%0Acapabilities.%20More%20importantly%2C%20existing%20benchmarks%20neglect%20to%20systematically%0Aexplore%20the%20impact%20factors%20that%20IVS%20would%20impart%20to%20untamed%20reasoning%0Aperformance.%20To%20tackle%20above%20gaps%2C%20we%20introduce%20a%20specialized%20benchmark%20termed%0AViC-Bench%2C%20consisting%20of%20four%20representive%20tasks%3A%20maze%20navigation%2C%20jigsaw%0Apuzzle%2C%20embodied%20long-horizon%20planning%2C%20and%20complex%20counting%2C%20where%20each%20task%0Ahas%20dedicated%20free-style%20IVS%20generation%20pipeline%20supporting%20function%20calls.%20To%0Asystematically%20examine%20VI-CoT%20capability%2C%20we%20propose%20a%20thorough%20evaluation%0Asuite%20incorporating%20a%20progressive%20three-stage%20strategy%20with%20targeted%20new%0Ametrics.%20Besides%2C%20we%20establish%20Incremental%20Prompting%20Information%20Injection%0A%28IPII%29%20strategy%20to%20ablatively%20explore%20the%20prompting%20factors%20for%20VI-CoT.%20We%0Aextensively%20conduct%20evaluations%20for%2018%20advanced%20MLLMs%2C%20revealing%20key%20insights%0Ainto%20their%20VI-CoT%20capability.%20Our%20proposed%20benchmark%20is%20publicly%20open%20at%0AHuggingface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViC-Bench%253A%2520Benchmarking%2520Visual-Interleaved%2520Chain-of-Thought%2520Capability%250A%2520%2520in%2520MLLMs%2520with%2520Free-Style%2520Intermediate%2520State%2520Representations%26entry.906535625%3DXuecheng%2520Wu%2520and%2520Jiaxing%2520Liu%2520and%2520Danlei%2520Huang%2520and%2520Xiaoyu%2520Li%2520and%2520Yifan%2520Wang%2520and%2520Chen%2520Chen%2520and%2520Liya%2520Ma%2520and%2520Xuezhi%2520Cao%2520and%2520Junxiao%2520Xue%26entry.1292438233%3D%2520%2520Visual-Interleaved%2520Chain-of-Thought%2520%2528VI-CoT%2529%2520enables%2520MLLMs%2520to%2520continually%250Aupdate%2520their%2520understanding%2520and%2520decisions%2520based%2520on%2520step-wise%2520intermediate%2520visual%250Astates%2520%2528IVS%2529%252C%2520much%2520like%2520a%2520human%2520would%252C%2520which%2520demonstrates%2520impressive%2520success%2520in%250Avarious%2520tasks%252C%2520thereby%2520leading%2520to%2520emerged%2520advancements%2520in%2520related%2520benchmarks.%250ADespite%2520promising%2520progress%252C%2520current%2520benchmarks%2520provide%2520models%2520with%2520relatively%250Afixed%2520IVS%252C%2520rather%2520than%2520free-style%2520IVS%252C%2520whch%2520might%2520forcibly%2520distort%2520the%2520original%250Athinking%2520trajectories%252C%2520failing%2520to%2520evaluate%2520their%2520intrinsic%2520reasoning%250Acapabilities.%2520More%2520importantly%252C%2520existing%2520benchmarks%2520neglect%2520to%2520systematically%250Aexplore%2520the%2520impact%2520factors%2520that%2520IVS%2520would%2520impart%2520to%2520untamed%2520reasoning%250Aperformance.%2520To%2520tackle%2520above%2520gaps%252C%2520we%2520introduce%2520a%2520specialized%2520benchmark%2520termed%250AViC-Bench%252C%2520consisting%2520of%2520four%2520representive%2520tasks%253A%2520maze%2520navigation%252C%2520jigsaw%250Apuzzle%252C%2520embodied%2520long-horizon%2520planning%252C%2520and%2520complex%2520counting%252C%2520where%2520each%2520task%250Ahas%2520dedicated%2520free-style%2520IVS%2520generation%2520pipeline%2520supporting%2520function%2520calls.%2520To%250Asystematically%2520examine%2520VI-CoT%2520capability%252C%2520we%2520propose%2520a%2520thorough%2520evaluation%250Asuite%2520incorporating%2520a%2520progressive%2520three-stage%2520strategy%2520with%2520targeted%2520new%250Ametrics.%2520Besides%252C%2520we%2520establish%2520Incremental%2520Prompting%2520Information%2520Injection%250A%2528IPII%2529%2520strategy%2520to%2520ablatively%2520explore%2520the%2520prompting%2520factors%2520for%2520VI-CoT.%2520We%250Aextensively%2520conduct%2520evaluations%2520for%252018%2520advanced%2520MLLMs%252C%2520revealing%2520key%2520insights%250Ainto%2520their%2520VI-CoT%2520capability.%2520Our%2520proposed%2520benchmark%2520is%2520publicly%2520open%2520at%250AHuggingface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViC-Bench%3A%20Benchmarking%20Visual-Interleaved%20Chain-of-Thought%20Capability%0A%20%20in%20MLLMs%20with%20Free-Style%20Intermediate%20State%20Representations&entry.906535625=Xuecheng%20Wu%20and%20Jiaxing%20Liu%20and%20Danlei%20Huang%20and%20Xiaoyu%20Li%20and%20Yifan%20Wang%20and%20Chen%20Chen%20and%20Liya%20Ma%20and%20Xuezhi%20Cao%20and%20Junxiao%20Xue&entry.1292438233=%20%20Visual-Interleaved%20Chain-of-Thought%20%28VI-CoT%29%20enables%20MLLMs%20to%20continually%0Aupdate%20their%20understanding%20and%20decisions%20based%20on%20step-wise%20intermediate%20visual%0Astates%20%28IVS%29%2C%20much%20like%20a%20human%20would%2C%20which%20demonstrates%20impressive%20success%20in%0Avarious%20tasks%2C%20thereby%20leading%20to%20emerged%20advancements%20in%20related%20benchmarks.%0ADespite%20promising%20progress%2C%20current%20benchmarks%20provide%20models%20with%20relatively%0Afixed%20IVS%2C%20rather%20than%20free-style%20IVS%2C%20whch%20might%20forcibly%20distort%20the%20original%0Athinking%20trajectories%2C%20failing%20to%20evaluate%20their%20intrinsic%20reasoning%0Acapabilities.%20More%20importantly%2C%20existing%20benchmarks%20neglect%20to%20systematically%0Aexplore%20the%20impact%20factors%20that%20IVS%20would%20impart%20to%20untamed%20reasoning%0Aperformance.%20To%20tackle%20above%20gaps%2C%20we%20introduce%20a%20specialized%20benchmark%20termed%0AViC-Bench%2C%20consisting%20of%20four%20representive%20tasks%3A%20maze%20navigation%2C%20jigsaw%0Apuzzle%2C%20embodied%20long-horizon%20planning%2C%20and%20complex%20counting%2C%20where%20each%20task%0Ahas%20dedicated%20free-style%20IVS%20generation%20pipeline%20supporting%20function%20calls.%20To%0Asystematically%20examine%20VI-CoT%20capability%2C%20we%20propose%20a%20thorough%20evaluation%0Asuite%20incorporating%20a%20progressive%20three-stage%20strategy%20with%20targeted%20new%0Ametrics.%20Besides%2C%20we%20establish%20Incremental%20Prompting%20Information%20Injection%0A%28IPII%29%20strategy%20to%20ablatively%20explore%20the%20prompting%20factors%20for%20VI-CoT.%20We%0Aextensively%20conduct%20evaluations%20for%2018%20advanced%20MLLMs%2C%20revealing%20key%20insights%0Ainto%20their%20VI-CoT%20capability.%20Our%20proposed%20benchmark%20is%20publicly%20open%20at%0AHuggingface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14404v2&entry.124074799=Read"},
{"title": "Consistent Story Generation with Asymmetry Zigzag Sampling", "author": "Mingxiao Li and Mang Ning and Marie-Francine Moens", "abstract": "  Text-to-image generation models have made significant progress in producing\nhigh-quality images from textual descriptions, yet they continue to struggle\nwith maintaining subject consistency across multiple images, a fundamental\nrequirement for visual storytelling. Existing methods attempt to address this\nby either fine-tuning models on large-scale story visualization datasets, which\nis resource-intensive, or by using training-free techniques that share\ninformation across generations, which still yield limited success. In this\npaper, we introduce a novel training-free sampling strategy called Zigzag\nSampling with Asymmetric Prompts and Visual Sharing to enhance subject\nconsistency in visual story generation. Our approach proposes a zigzag sampling\nmechanism that alternates between asymmetric prompting to retain subject\ncharacteristics, while a visual sharing module transfers visual cues across\ngenerated images to %further enforce consistency. Experimental results, based\non both quantitative metrics and qualitative evaluations, demonstrate that our\nmethod significantly outperforms previous approaches in generating coherent and\nconsistent visual stories. The code is available at\nhttps://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.\n", "link": "http://arxiv.org/abs/2506.09612v2", "date": "2025-06-12", "relevancy": 2.1547, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5577}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5328}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Story%20Generation%20with%20Asymmetry%20Zigzag%20Sampling&body=Title%3A%20Consistent%20Story%20Generation%20with%20Asymmetry%20Zigzag%20Sampling%0AAuthor%3A%20Mingxiao%20Li%20and%20Mang%20Ning%20and%20Marie-Francine%20Moens%0AAbstract%3A%20%20%20Text-to-image%20generation%20models%20have%20made%20significant%20progress%20in%20producing%0Ahigh-quality%20images%20from%20textual%20descriptions%2C%20yet%20they%20continue%20to%20struggle%0Awith%20maintaining%20subject%20consistency%20across%20multiple%20images%2C%20a%20fundamental%0Arequirement%20for%20visual%20storytelling.%20Existing%20methods%20attempt%20to%20address%20this%0Aby%20either%20fine-tuning%20models%20on%20large-scale%20story%20visualization%20datasets%2C%20which%0Ais%20resource-intensive%2C%20or%20by%20using%20training-free%20techniques%20that%20share%0Ainformation%20across%20generations%2C%20which%20still%20yield%20limited%20success.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20training-free%20sampling%20strategy%20called%20Zigzag%0ASampling%20with%20Asymmetric%20Prompts%20and%20Visual%20Sharing%20to%20enhance%20subject%0Aconsistency%20in%20visual%20story%20generation.%20Our%20approach%20proposes%20a%20zigzag%20sampling%0Amechanism%20that%20alternates%20between%20asymmetric%20prompting%20to%20retain%20subject%0Acharacteristics%2C%20while%20a%20visual%20sharing%20module%20transfers%20visual%20cues%20across%0Agenerated%20images%20to%20%25further%20enforce%20consistency.%20Experimental%20results%2C%20based%0Aon%20both%20quantitative%20metrics%20and%20qualitative%20evaluations%2C%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20previous%20approaches%20in%20generating%20coherent%20and%0Aconsistent%20visual%20stories.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Story%2520Generation%2520with%2520Asymmetry%2520Zigzag%2520Sampling%26entry.906535625%3DMingxiao%2520Li%2520and%2520Mang%2520Ning%2520and%2520Marie-Francine%2520Moens%26entry.1292438233%3D%2520%2520Text-to-image%2520generation%2520models%2520have%2520made%2520significant%2520progress%2520in%2520producing%250Ahigh-quality%2520images%2520from%2520textual%2520descriptions%252C%2520yet%2520they%2520continue%2520to%2520struggle%250Awith%2520maintaining%2520subject%2520consistency%2520across%2520multiple%2520images%252C%2520a%2520fundamental%250Arequirement%2520for%2520visual%2520storytelling.%2520Existing%2520methods%2520attempt%2520to%2520address%2520this%250Aby%2520either%2520fine-tuning%2520models%2520on%2520large-scale%2520story%2520visualization%2520datasets%252C%2520which%250Ais%2520resource-intensive%252C%2520or%2520by%2520using%2520training-free%2520techniques%2520that%2520share%250Ainformation%2520across%2520generations%252C%2520which%2520still%2520yield%2520limited%2520success.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520training-free%2520sampling%2520strategy%2520called%2520Zigzag%250ASampling%2520with%2520Asymmetric%2520Prompts%2520and%2520Visual%2520Sharing%2520to%2520enhance%2520subject%250Aconsistency%2520in%2520visual%2520story%2520generation.%2520Our%2520approach%2520proposes%2520a%2520zigzag%2520sampling%250Amechanism%2520that%2520alternates%2520between%2520asymmetric%2520prompting%2520to%2520retain%2520subject%250Acharacteristics%252C%2520while%2520a%2520visual%2520sharing%2520module%2520transfers%2520visual%2520cues%2520across%250Agenerated%2520images%2520to%2520%2525further%2520enforce%2520consistency.%2520Experimental%2520results%252C%2520based%250Aon%2520both%2520quantitative%2520metrics%2520and%2520qualitative%2520evaluations%252C%2520demonstrate%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520previous%2520approaches%2520in%2520generating%2520coherent%2520and%250Aconsistent%2520visual%2520stories.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Story%20Generation%20with%20Asymmetry%20Zigzag%20Sampling&entry.906535625=Mingxiao%20Li%20and%20Mang%20Ning%20and%20Marie-Francine%20Moens&entry.1292438233=%20%20Text-to-image%20generation%20models%20have%20made%20significant%20progress%20in%20producing%0Ahigh-quality%20images%20from%20textual%20descriptions%2C%20yet%20they%20continue%20to%20struggle%0Awith%20maintaining%20subject%20consistency%20across%20multiple%20images%2C%20a%20fundamental%0Arequirement%20for%20visual%20storytelling.%20Existing%20methods%20attempt%20to%20address%20this%0Aby%20either%20fine-tuning%20models%20on%20large-scale%20story%20visualization%20datasets%2C%20which%0Ais%20resource-intensive%2C%20or%20by%20using%20training-free%20techniques%20that%20share%0Ainformation%20across%20generations%2C%20which%20still%20yield%20limited%20success.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20training-free%20sampling%20strategy%20called%20Zigzag%0ASampling%20with%20Asymmetric%20Prompts%20and%20Visual%20Sharing%20to%20enhance%20subject%0Aconsistency%20in%20visual%20story%20generation.%20Our%20approach%20proposes%20a%20zigzag%20sampling%0Amechanism%20that%20alternates%20between%20asymmetric%20prompting%20to%20retain%20subject%0Acharacteristics%2C%20while%20a%20visual%20sharing%20module%20transfers%20visual%20cues%20across%0Agenerated%20images%20to%20%25further%20enforce%20consistency.%20Experimental%20results%2C%20based%0Aon%20both%20quantitative%20metrics%20and%20qualitative%20evaluations%2C%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20previous%20approaches%20in%20generating%20coherent%20and%0Aconsistent%20visual%20stories.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09612v2&entry.124074799=Read"},
{"title": "On feature selection in double-imbalanced data settings: a Random Forest\n  approach", "author": "Fabio Demaria", "abstract": "  Feature selection is a critical step in high-dimensional classification\ntasks, particularly under challenging conditions of double imbalance, namely\nsettings characterized by both class imbalance in the response variable and\ndimensional asymmetry in the data $(n \\gg p)$. In such scenarios, traditional\nfeature selection methods applied to Random Forests (RF) often yield unstable\nor misleading importance rankings. This paper proposes a novel thresholding\nscheme for feature selection based on minimal depth, which exploits the tree\ntopology to assess variable relevance. Extensive experiments on simulated and\nreal-world datasets demonstrate that the proposed approach produces more\nparsimonious and accurate subsets of variables compared to conventional minimal\ndepth-based selection. The method provides a practical and interpretable\nsolution for variable selection in RF under double imbalance conditions.\n", "link": "http://arxiv.org/abs/2506.10929v1", "date": "2025-06-12", "relevancy": 2.1505, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4756}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4166}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20feature%20selection%20in%20double-imbalanced%20data%20settings%3A%20a%20Random%20Forest%0A%20%20approach&body=Title%3A%20On%20feature%20selection%20in%20double-imbalanced%20data%20settings%3A%20a%20Random%20Forest%0A%20%20approach%0AAuthor%3A%20Fabio%20Demaria%0AAbstract%3A%20%20%20Feature%20selection%20is%20a%20critical%20step%20in%20high-dimensional%20classification%0Atasks%2C%20particularly%20under%20challenging%20conditions%20of%20double%20imbalance%2C%20namely%0Asettings%20characterized%20by%20both%20class%20imbalance%20in%20the%20response%20variable%20and%0Adimensional%20asymmetry%20in%20the%20data%20%24%28n%20%5Cgg%20p%29%24.%20In%20such%20scenarios%2C%20traditional%0Afeature%20selection%20methods%20applied%20to%20Random%20Forests%20%28RF%29%20often%20yield%20unstable%0Aor%20misleading%20importance%20rankings.%20This%20paper%20proposes%20a%20novel%20thresholding%0Ascheme%20for%20feature%20selection%20based%20on%20minimal%20depth%2C%20which%20exploits%20the%20tree%0Atopology%20to%20assess%20variable%20relevance.%20Extensive%20experiments%20on%20simulated%20and%0Areal-world%20datasets%20demonstrate%20that%20the%20proposed%20approach%20produces%20more%0Aparsimonious%20and%20accurate%20subsets%20of%20variables%20compared%20to%20conventional%20minimal%0Adepth-based%20selection.%20The%20method%20provides%20a%20practical%20and%20interpretable%0Asolution%20for%20variable%20selection%20in%20RF%20under%20double%20imbalance%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520feature%2520selection%2520in%2520double-imbalanced%2520data%2520settings%253A%2520a%2520Random%2520Forest%250A%2520%2520approach%26entry.906535625%3DFabio%2520Demaria%26entry.1292438233%3D%2520%2520Feature%2520selection%2520is%2520a%2520critical%2520step%2520in%2520high-dimensional%2520classification%250Atasks%252C%2520particularly%2520under%2520challenging%2520conditions%2520of%2520double%2520imbalance%252C%2520namely%250Asettings%2520characterized%2520by%2520both%2520class%2520imbalance%2520in%2520the%2520response%2520variable%2520and%250Adimensional%2520asymmetry%2520in%2520the%2520data%2520%2524%2528n%2520%255Cgg%2520p%2529%2524.%2520In%2520such%2520scenarios%252C%2520traditional%250Afeature%2520selection%2520methods%2520applied%2520to%2520Random%2520Forests%2520%2528RF%2529%2520often%2520yield%2520unstable%250Aor%2520misleading%2520importance%2520rankings.%2520This%2520paper%2520proposes%2520a%2520novel%2520thresholding%250Ascheme%2520for%2520feature%2520selection%2520based%2520on%2520minimal%2520depth%252C%2520which%2520exploits%2520the%2520tree%250Atopology%2520to%2520assess%2520variable%2520relevance.%2520Extensive%2520experiments%2520on%2520simulated%2520and%250Areal-world%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520produces%2520more%250Aparsimonious%2520and%2520accurate%2520subsets%2520of%2520variables%2520compared%2520to%2520conventional%2520minimal%250Adepth-based%2520selection.%2520The%2520method%2520provides%2520a%2520practical%2520and%2520interpretable%250Asolution%2520for%2520variable%2520selection%2520in%2520RF%2520under%2520double%2520imbalance%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20feature%20selection%20in%20double-imbalanced%20data%20settings%3A%20a%20Random%20Forest%0A%20%20approach&entry.906535625=Fabio%20Demaria&entry.1292438233=%20%20Feature%20selection%20is%20a%20critical%20step%20in%20high-dimensional%20classification%0Atasks%2C%20particularly%20under%20challenging%20conditions%20of%20double%20imbalance%2C%20namely%0Asettings%20characterized%20by%20both%20class%20imbalance%20in%20the%20response%20variable%20and%0Adimensional%20asymmetry%20in%20the%20data%20%24%28n%20%5Cgg%20p%29%24.%20In%20such%20scenarios%2C%20traditional%0Afeature%20selection%20methods%20applied%20to%20Random%20Forests%20%28RF%29%20often%20yield%20unstable%0Aor%20misleading%20importance%20rankings.%20This%20paper%20proposes%20a%20novel%20thresholding%0Ascheme%20for%20feature%20selection%20based%20on%20minimal%20depth%2C%20which%20exploits%20the%20tree%0Atopology%20to%20assess%20variable%20relevance.%20Extensive%20experiments%20on%20simulated%20and%0Areal-world%20datasets%20demonstrate%20that%20the%20proposed%20approach%20produces%20more%0Aparsimonious%20and%20accurate%20subsets%20of%20variables%20compared%20to%20conventional%20minimal%0Adepth-based%20selection.%20The%20method%20provides%20a%20practical%20and%20interpretable%0Asolution%20for%20variable%20selection%20in%20RF%20under%20double%20imbalance%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10929v1&entry.124074799=Read"},
{"title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for\n  Token Pruning in MLLMs", "author": "Qizhe Zhang and Mengzhen Liu and Lichen Li and Ming Lu and Yuan Zhang and Junwen Pan and Qi She and Shanghang Zhang", "abstract": "  In multimodal large language models (MLLMs), the length of input visual\ntokens is often significantly greater than that of their textual counterparts,\nleading to a high inference cost. Many works aim to address this issue by\nremoving redundant visual tokens. However, current approaches either rely on\nattention-based pruning, which retains numerous duplicate tokens, or use\nsimilarity-based pruning, overlooking the instruction relevance, consequently\ncausing suboptimal performance. In this paper, we go beyond attention or\nsimilarity by proposing a novel visual token pruning method named CDPruner,\nwhich maximizes the conditional diversity of retained tokens. We first define\nthe conditional similarity between visual tokens conditioned on the\ninstruction, and then reformulate the token pruning problem with determinantal\npoint process (DPP) to maximize the conditional diversity of the selected\nsubset. The proposed CDPruner is training-free and model-agnostic, allowing\neasy application to various MLLMs. Extensive experiments across diverse MLLMs\nshow that CDPruner establishes new state-of-the-art on various vision-language\nbenchmarks. By maximizing conditional diversity through DPP, the selected\nsubset better represents the input images while closely adhering to user\ninstructions, thereby preserving strong performance even with high reduction\nratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\\% and CUDA latency\nby 78\\%, while maintaining 94\\% of the original accuracy. Our code is available\nat https://github.com/Theia-4869/CDPruner.\n", "link": "http://arxiv.org/abs/2506.10967v1", "date": "2025-06-12", "relevancy": 2.1433, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Attention%20or%20Similarity%3A%20Maximizing%20Conditional%20Diversity%20for%0A%20%20Token%20Pruning%20in%20MLLMs&body=Title%3A%20Beyond%20Attention%20or%20Similarity%3A%20Maximizing%20Conditional%20Diversity%20for%0A%20%20Token%20Pruning%20in%20MLLMs%0AAuthor%3A%20Qizhe%20Zhang%20and%20Mengzhen%20Liu%20and%20Lichen%20Li%20and%20Ming%20Lu%20and%20Yuan%20Zhang%20and%20Junwen%20Pan%20and%20Qi%20She%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20In%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20the%20length%20of%20input%20visual%0Atokens%20is%20often%20significantly%20greater%20than%20that%20of%20their%20textual%20counterparts%2C%0Aleading%20to%20a%20high%20inference%20cost.%20Many%20works%20aim%20to%20address%20this%20issue%20by%0Aremoving%20redundant%20visual%20tokens.%20However%2C%20current%20approaches%20either%20rely%20on%0Aattention-based%20pruning%2C%20which%20retains%20numerous%20duplicate%20tokens%2C%20or%20use%0Asimilarity-based%20pruning%2C%20overlooking%20the%20instruction%20relevance%2C%20consequently%0Acausing%20suboptimal%20performance.%20In%20this%20paper%2C%20we%20go%20beyond%20attention%20or%0Asimilarity%20by%20proposing%20a%20novel%20visual%20token%20pruning%20method%20named%20CDPruner%2C%0Awhich%20maximizes%20the%20conditional%20diversity%20of%20retained%20tokens.%20We%20first%20define%0Athe%20conditional%20similarity%20between%20visual%20tokens%20conditioned%20on%20the%0Ainstruction%2C%20and%20then%20reformulate%20the%20token%20pruning%20problem%20with%20determinantal%0Apoint%20process%20%28DPP%29%20to%20maximize%20the%20conditional%20diversity%20of%20the%20selected%0Asubset.%20The%20proposed%20CDPruner%20is%20training-free%20and%20model-agnostic%2C%20allowing%0Aeasy%20application%20to%20various%20MLLMs.%20Extensive%20experiments%20across%20diverse%20MLLMs%0Ashow%20that%20CDPruner%20establishes%20new%20state-of-the-art%20on%20various%20vision-language%0Abenchmarks.%20By%20maximizing%20conditional%20diversity%20through%20DPP%2C%20the%20selected%0Asubset%20better%20represents%20the%20input%20images%20while%20closely%20adhering%20to%20user%0Ainstructions%2C%20thereby%20preserving%20strong%20performance%20even%20with%20high%20reduction%0Aratios.%20When%20applied%20to%20LLaVA%2C%20CDPruner%20reduces%20FLOPs%20by%2095%5C%25%20and%20CUDA%20latency%0Aby%2078%5C%25%2C%20while%20maintaining%2094%5C%25%20of%20the%20original%20accuracy.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/Theia-4869/CDPruner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Attention%2520or%2520Similarity%253A%2520Maximizing%2520Conditional%2520Diversity%2520for%250A%2520%2520Token%2520Pruning%2520in%2520MLLMs%26entry.906535625%3DQizhe%2520Zhang%2520and%2520Mengzhen%2520Liu%2520and%2520Lichen%2520Li%2520and%2520Ming%2520Lu%2520and%2520Yuan%2520Zhang%2520and%2520Junwen%2520Pan%2520and%2520Qi%2520She%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520In%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520the%2520length%2520of%2520input%2520visual%250Atokens%2520is%2520often%2520significantly%2520greater%2520than%2520that%2520of%2520their%2520textual%2520counterparts%252C%250Aleading%2520to%2520a%2520high%2520inference%2520cost.%2520Many%2520works%2520aim%2520to%2520address%2520this%2520issue%2520by%250Aremoving%2520redundant%2520visual%2520tokens.%2520However%252C%2520current%2520approaches%2520either%2520rely%2520on%250Aattention-based%2520pruning%252C%2520which%2520retains%2520numerous%2520duplicate%2520tokens%252C%2520or%2520use%250Asimilarity-based%2520pruning%252C%2520overlooking%2520the%2520instruction%2520relevance%252C%2520consequently%250Acausing%2520suboptimal%2520performance.%2520In%2520this%2520paper%252C%2520we%2520go%2520beyond%2520attention%2520or%250Asimilarity%2520by%2520proposing%2520a%2520novel%2520visual%2520token%2520pruning%2520method%2520named%2520CDPruner%252C%250Awhich%2520maximizes%2520the%2520conditional%2520diversity%2520of%2520retained%2520tokens.%2520We%2520first%2520define%250Athe%2520conditional%2520similarity%2520between%2520visual%2520tokens%2520conditioned%2520on%2520the%250Ainstruction%252C%2520and%2520then%2520reformulate%2520the%2520token%2520pruning%2520problem%2520with%2520determinantal%250Apoint%2520process%2520%2528DPP%2529%2520to%2520maximize%2520the%2520conditional%2520diversity%2520of%2520the%2520selected%250Asubset.%2520The%2520proposed%2520CDPruner%2520is%2520training-free%2520and%2520model-agnostic%252C%2520allowing%250Aeasy%2520application%2520to%2520various%2520MLLMs.%2520Extensive%2520experiments%2520across%2520diverse%2520MLLMs%250Ashow%2520that%2520CDPruner%2520establishes%2520new%2520state-of-the-art%2520on%2520various%2520vision-language%250Abenchmarks.%2520By%2520maximizing%2520conditional%2520diversity%2520through%2520DPP%252C%2520the%2520selected%250Asubset%2520better%2520represents%2520the%2520input%2520images%2520while%2520closely%2520adhering%2520to%2520user%250Ainstructions%252C%2520thereby%2520preserving%2520strong%2520performance%2520even%2520with%2520high%2520reduction%250Aratios.%2520When%2520applied%2520to%2520LLaVA%252C%2520CDPruner%2520reduces%2520FLOPs%2520by%252095%255C%2525%2520and%2520CUDA%2520latency%250Aby%252078%255C%2525%252C%2520while%2520maintaining%252094%255C%2525%2520of%2520the%2520original%2520accuracy.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/Theia-4869/CDPruner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Attention%20or%20Similarity%3A%20Maximizing%20Conditional%20Diversity%20for%0A%20%20Token%20Pruning%20in%20MLLMs&entry.906535625=Qizhe%20Zhang%20and%20Mengzhen%20Liu%20and%20Lichen%20Li%20and%20Ming%20Lu%20and%20Yuan%20Zhang%20and%20Junwen%20Pan%20and%20Qi%20She%20and%20Shanghang%20Zhang&entry.1292438233=%20%20In%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20the%20length%20of%20input%20visual%0Atokens%20is%20often%20significantly%20greater%20than%20that%20of%20their%20textual%20counterparts%2C%0Aleading%20to%20a%20high%20inference%20cost.%20Many%20works%20aim%20to%20address%20this%20issue%20by%0Aremoving%20redundant%20visual%20tokens.%20However%2C%20current%20approaches%20either%20rely%20on%0Aattention-based%20pruning%2C%20which%20retains%20numerous%20duplicate%20tokens%2C%20or%20use%0Asimilarity-based%20pruning%2C%20overlooking%20the%20instruction%20relevance%2C%20consequently%0Acausing%20suboptimal%20performance.%20In%20this%20paper%2C%20we%20go%20beyond%20attention%20or%0Asimilarity%20by%20proposing%20a%20novel%20visual%20token%20pruning%20method%20named%20CDPruner%2C%0Awhich%20maximizes%20the%20conditional%20diversity%20of%20retained%20tokens.%20We%20first%20define%0Athe%20conditional%20similarity%20between%20visual%20tokens%20conditioned%20on%20the%0Ainstruction%2C%20and%20then%20reformulate%20the%20token%20pruning%20problem%20with%20determinantal%0Apoint%20process%20%28DPP%29%20to%20maximize%20the%20conditional%20diversity%20of%20the%20selected%0Asubset.%20The%20proposed%20CDPruner%20is%20training-free%20and%20model-agnostic%2C%20allowing%0Aeasy%20application%20to%20various%20MLLMs.%20Extensive%20experiments%20across%20diverse%20MLLMs%0Ashow%20that%20CDPruner%20establishes%20new%20state-of-the-art%20on%20various%20vision-language%0Abenchmarks.%20By%20maximizing%20conditional%20diversity%20through%20DPP%2C%20the%20selected%0Asubset%20better%20represents%20the%20input%20images%20while%20closely%20adhering%20to%20user%0Ainstructions%2C%20thereby%20preserving%20strong%20performance%20even%20with%20high%20reduction%0Aratios.%20When%20applied%20to%20LLaVA%2C%20CDPruner%20reduces%20FLOPs%20by%2095%5C%25%20and%20CUDA%20latency%0Aby%2078%5C%25%2C%20while%20maintaining%2094%5C%25%20of%20the%20original%20accuracy.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/Theia-4869/CDPruner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10967v1&entry.124074799=Read"},
{"title": "Inference-Time Decomposition of Activations (ITDA): A Scalable Approach\n  to Interpreting Large Language Models", "author": "Patrick Leask and Neel Nanda and Noura Al Moubayed", "abstract": "  Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage\nModels (LLM) activations into interpretable latents. However, due to their\nsubstantial training cost, most academic research uses open-source SAEs which\nare only available for a restricted set of models of up to 27B parameters. SAE\nlatents are also learned from a dataset of activations, which means they do not\ntransfer between models. Motivated by relative representation similarity\nmeasures, we introduce Inference-Time Decomposition of Activations (ITDA)\nmodels, an alternative method for decomposing language model activations. To\ntrain an ITDA, we greedily construct a dictionary of language model activations\non a dataset of prompts, selecting those activations which were worst\napproximated by matching pursuit on the existing dictionary. ITDAs can be\ntrained in just 1% of the time required for SAEs, using 1% of the data. This\nallowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.\nITDAs can achieve similar reconstruction performance to SAEs on some target\nLLMs, but generally incur a performance penalty. However, ITDA dictionaries\nenable cross-model comparisons, and a simple Jaccard similarity index on ITDA\ndictionaries outperforms existing methods like CKA, SVCCA, and relative\nrepresentation similarity metrics. ITDAs provide a cheap alternative to SAEs\nwhere computational resources are limited, or when cross model comparisons are\nnecessary. Code available at https://github.com/pleask/itda.\n", "link": "http://arxiv.org/abs/2505.17769v2", "date": "2025-06-12", "relevancy": 2.1401, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Decomposition%20of%20Activations%20%28ITDA%29%3A%20A%20Scalable%20Approach%0A%20%20to%20Interpreting%20Large%20Language%20Models&body=Title%3A%20Inference-Time%20Decomposition%20of%20Activations%20%28ITDA%29%3A%20A%20Scalable%20Approach%0A%20%20to%20Interpreting%20Large%20Language%20Models%0AAuthor%3A%20Patrick%20Leask%20and%20Neel%20Nanda%20and%20Noura%20Al%20Moubayed%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20method%20for%20decomposing%20Large%20Langage%0AModels%20%28LLM%29%20activations%20into%20interpretable%20latents.%20However%2C%20due%20to%20their%0Asubstantial%20training%20cost%2C%20most%20academic%20research%20uses%20open-source%20SAEs%20which%0Aare%20only%20available%20for%20a%20restricted%20set%20of%20models%20of%20up%20to%2027B%20parameters.%20SAE%0Alatents%20are%20also%20learned%20from%20a%20dataset%20of%20activations%2C%20which%20means%20they%20do%20not%0Atransfer%20between%20models.%20Motivated%20by%20relative%20representation%20similarity%0Ameasures%2C%20we%20introduce%20Inference-Time%20Decomposition%20of%20Activations%20%28ITDA%29%0Amodels%2C%20an%20alternative%20method%20for%20decomposing%20language%20model%20activations.%20To%0Atrain%20an%20ITDA%2C%20we%20greedily%20construct%20a%20dictionary%20of%20language%20model%20activations%0Aon%20a%20dataset%20of%20prompts%2C%20selecting%20those%20activations%20which%20were%20worst%0Aapproximated%20by%20matching%20pursuit%20on%20the%20existing%20dictionary.%20ITDAs%20can%20be%0Atrained%20in%20just%201%25%20of%20the%20time%20required%20for%20SAEs%2C%20using%201%25%20of%20the%20data.%20This%0Aallowed%20us%20to%20train%20ITDAs%20on%20Llama-3.1%2070B%20and%20405B%20on%20a%20single%20consumer%20GPU.%0AITDAs%20can%20achieve%20similar%20reconstruction%20performance%20to%20SAEs%20on%20some%20target%0ALLMs%2C%20but%20generally%20incur%20a%20performance%20penalty.%20However%2C%20ITDA%20dictionaries%0Aenable%20cross-model%20comparisons%2C%20and%20a%20simple%20Jaccard%20similarity%20index%20on%20ITDA%0Adictionaries%20outperforms%20existing%20methods%20like%20CKA%2C%20SVCCA%2C%20and%20relative%0Arepresentation%20similarity%20metrics.%20ITDAs%20provide%20a%20cheap%20alternative%20to%20SAEs%0Awhere%20computational%20resources%20are%20limited%2C%20or%20when%20cross%20model%20comparisons%20are%0Anecessary.%20Code%20available%20at%20https%3A//github.com/pleask/itda.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Decomposition%2520of%2520Activations%2520%2528ITDA%2529%253A%2520A%2520Scalable%2520Approach%250A%2520%2520to%2520Interpreting%2520Large%2520Language%2520Models%26entry.906535625%3DPatrick%2520Leask%2520and%2520Neel%2520Nanda%2520and%2520Noura%2520Al%2520Moubayed%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520popular%2520method%2520for%2520decomposing%2520Large%2520Langage%250AModels%2520%2528LLM%2529%2520activations%2520into%2520interpretable%2520latents.%2520However%252C%2520due%2520to%2520their%250Asubstantial%2520training%2520cost%252C%2520most%2520academic%2520research%2520uses%2520open-source%2520SAEs%2520which%250Aare%2520only%2520available%2520for%2520a%2520restricted%2520set%2520of%2520models%2520of%2520up%2520to%252027B%2520parameters.%2520SAE%250Alatents%2520are%2520also%2520learned%2520from%2520a%2520dataset%2520of%2520activations%252C%2520which%2520means%2520they%2520do%2520not%250Atransfer%2520between%2520models.%2520Motivated%2520by%2520relative%2520representation%2520similarity%250Ameasures%252C%2520we%2520introduce%2520Inference-Time%2520Decomposition%2520of%2520Activations%2520%2528ITDA%2529%250Amodels%252C%2520an%2520alternative%2520method%2520for%2520decomposing%2520language%2520model%2520activations.%2520To%250Atrain%2520an%2520ITDA%252C%2520we%2520greedily%2520construct%2520a%2520dictionary%2520of%2520language%2520model%2520activations%250Aon%2520a%2520dataset%2520of%2520prompts%252C%2520selecting%2520those%2520activations%2520which%2520were%2520worst%250Aapproximated%2520by%2520matching%2520pursuit%2520on%2520the%2520existing%2520dictionary.%2520ITDAs%2520can%2520be%250Atrained%2520in%2520just%25201%2525%2520of%2520the%2520time%2520required%2520for%2520SAEs%252C%2520using%25201%2525%2520of%2520the%2520data.%2520This%250Aallowed%2520us%2520to%2520train%2520ITDAs%2520on%2520Llama-3.1%252070B%2520and%2520405B%2520on%2520a%2520single%2520consumer%2520GPU.%250AITDAs%2520can%2520achieve%2520similar%2520reconstruction%2520performance%2520to%2520SAEs%2520on%2520some%2520target%250ALLMs%252C%2520but%2520generally%2520incur%2520a%2520performance%2520penalty.%2520However%252C%2520ITDA%2520dictionaries%250Aenable%2520cross-model%2520comparisons%252C%2520and%2520a%2520simple%2520Jaccard%2520similarity%2520index%2520on%2520ITDA%250Adictionaries%2520outperforms%2520existing%2520methods%2520like%2520CKA%252C%2520SVCCA%252C%2520and%2520relative%250Arepresentation%2520similarity%2520metrics.%2520ITDAs%2520provide%2520a%2520cheap%2520alternative%2520to%2520SAEs%250Awhere%2520computational%2520resources%2520are%2520limited%252C%2520or%2520when%2520cross%2520model%2520comparisons%2520are%250Anecessary.%2520Code%2520available%2520at%2520https%253A//github.com/pleask/itda.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Decomposition%20of%20Activations%20%28ITDA%29%3A%20A%20Scalable%20Approach%0A%20%20to%20Interpreting%20Large%20Language%20Models&entry.906535625=Patrick%20Leask%20and%20Neel%20Nanda%20and%20Noura%20Al%20Moubayed&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20method%20for%20decomposing%20Large%20Langage%0AModels%20%28LLM%29%20activations%20into%20interpretable%20latents.%20However%2C%20due%20to%20their%0Asubstantial%20training%20cost%2C%20most%20academic%20research%20uses%20open-source%20SAEs%20which%0Aare%20only%20available%20for%20a%20restricted%20set%20of%20models%20of%20up%20to%2027B%20parameters.%20SAE%0Alatents%20are%20also%20learned%20from%20a%20dataset%20of%20activations%2C%20which%20means%20they%20do%20not%0Atransfer%20between%20models.%20Motivated%20by%20relative%20representation%20similarity%0Ameasures%2C%20we%20introduce%20Inference-Time%20Decomposition%20of%20Activations%20%28ITDA%29%0Amodels%2C%20an%20alternative%20method%20for%20decomposing%20language%20model%20activations.%20To%0Atrain%20an%20ITDA%2C%20we%20greedily%20construct%20a%20dictionary%20of%20language%20model%20activations%0Aon%20a%20dataset%20of%20prompts%2C%20selecting%20those%20activations%20which%20were%20worst%0Aapproximated%20by%20matching%20pursuit%20on%20the%20existing%20dictionary.%20ITDAs%20can%20be%0Atrained%20in%20just%201%25%20of%20the%20time%20required%20for%20SAEs%2C%20using%201%25%20of%20the%20data.%20This%0Aallowed%20us%20to%20train%20ITDAs%20on%20Llama-3.1%2070B%20and%20405B%20on%20a%20single%20consumer%20GPU.%0AITDAs%20can%20achieve%20similar%20reconstruction%20performance%20to%20SAEs%20on%20some%20target%0ALLMs%2C%20but%20generally%20incur%20a%20performance%20penalty.%20However%2C%20ITDA%20dictionaries%0Aenable%20cross-model%20comparisons%2C%20and%20a%20simple%20Jaccard%20similarity%20index%20on%20ITDA%0Adictionaries%20outperforms%20existing%20methods%20like%20CKA%2C%20SVCCA%2C%20and%20relative%0Arepresentation%20similarity%20metrics.%20ITDAs%20provide%20a%20cheap%20alternative%20to%20SAEs%0Awhere%20computational%20resources%20are%20limited%2C%20or%20when%20cross%20model%20comparisons%20are%0Anecessary.%20Code%20available%20at%20https%3A//github.com/pleask/itda.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17769v2&entry.124074799=Read"},
{"title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos", "author": "Jiashuo Yu and Yue Wu and Meng Chu and Zhifei Ren and Zizheng Huang and Pei Chu and Ruijie Zhang and Yinan He and Qirui Li and Songze Li and Zhenxiang Li and Zhongying Tu and Conghui He and Yu Qiao and Yali Wang and Yi Wang and Limin Wang", "abstract": "  We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.\n", "link": "http://arxiv.org/abs/2506.10857v1", "date": "2025-06-12", "relevancy": 2.1389, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRBench%3A%20A%20Benchmark%20for%20Multi-Step%20Reasoning%20in%20Long%20Narrative%20Videos&body=Title%3A%20VRBench%3A%20A%20Benchmark%20for%20Multi-Step%20Reasoning%20in%20Long%20Narrative%20Videos%0AAuthor%3A%20Jiashuo%20Yu%20and%20Yue%20Wu%20and%20Meng%20Chu%20and%20Zhifei%20Ren%20and%20Zizheng%20Huang%20and%20Pei%20Chu%20and%20Ruijie%20Zhang%20and%20Yinan%20He%20and%20Qirui%20Li%20and%20Songze%20Li%20and%20Zhenxiang%20Li%20and%20Zhongying%20Tu%20and%20Conghui%20He%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Yi%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20We%20present%20VRBench%2C%20the%20first%20long%20narrative%20video%20benchmark%20crafted%20for%0Aevaluating%20large%20models%27%20multi-step%20reasoning%20capabilities%2C%20addressing%0Alimitations%20in%20existing%20evaluations%20that%20overlook%20temporal%20reasoning%20and%0Aprocedural%20validity.%20It%20comprises%201%2C010%20long%20videos%20%28with%20an%20average%20duration%0Aof%201.6%20hours%29%2C%20along%20with%209%2C468%20human-labeled%20multi-step%20question-answering%0Apairs%20and%2030%2C292%20reasoning%20steps%20with%20timestamps.%20These%20videos%20are%20curated%20via%0Aa%20multi-stage%20filtering%20process%20including%20expert%20inter-rater%20reviewing%20to%0Aprioritize%20plot%20coherence.%20We%20develop%20a%20human-AI%20collaborative%20framework%20that%0Agenerates%20coherent%20reasoning%20chains%2C%20each%20requiring%20multiple%20temporally%0Agrounded%20steps%2C%20spanning%20seven%20types%20%28e.g.%2C%20event%20attribution%2C%20implicit%0Ainference%29.%20VRBench%20designs%20a%20multi-phase%20evaluation%20pipeline%20that%20assesses%0Amodels%20at%20both%20the%20outcome%20and%20process%20levels.%20Apart%20from%20the%20MCQs%20for%20the%0Afinal%20results%2C%20we%20propose%20a%20progress-level%20LLM-guided%20scoring%20metric%20to%0Aevaluate%20the%20quality%20of%20the%20reasoning%20chain%20from%20multiple%20dimensions%0Acomprehensively.%20Through%20extensive%20evaluations%20of%2012%20LLMs%20and%2016%20VLMs%20on%0AVRBench%2C%20we%20undertake%20a%20thorough%20analysis%20and%20provide%20valuable%20insights%20that%0Aadvance%20the%20field%20of%20multi-step%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRBench%253A%2520A%2520Benchmark%2520for%2520Multi-Step%2520Reasoning%2520in%2520Long%2520Narrative%2520Videos%26entry.906535625%3DJiashuo%2520Yu%2520and%2520Yue%2520Wu%2520and%2520Meng%2520Chu%2520and%2520Zhifei%2520Ren%2520and%2520Zizheng%2520Huang%2520and%2520Pei%2520Chu%2520and%2520Ruijie%2520Zhang%2520and%2520Yinan%2520He%2520and%2520Qirui%2520Li%2520and%2520Songze%2520Li%2520and%2520Zhenxiang%2520Li%2520and%2520Zhongying%2520Tu%2520and%2520Conghui%2520He%2520and%2520Yu%2520Qiao%2520and%2520Yali%2520Wang%2520and%2520Yi%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520VRBench%252C%2520the%2520first%2520long%2520narrative%2520video%2520benchmark%2520crafted%2520for%250Aevaluating%2520large%2520models%2527%2520multi-step%2520reasoning%2520capabilities%252C%2520addressing%250Alimitations%2520in%2520existing%2520evaluations%2520that%2520overlook%2520temporal%2520reasoning%2520and%250Aprocedural%2520validity.%2520It%2520comprises%25201%252C010%2520long%2520videos%2520%2528with%2520an%2520average%2520duration%250Aof%25201.6%2520hours%2529%252C%2520along%2520with%25209%252C468%2520human-labeled%2520multi-step%2520question-answering%250Apairs%2520and%252030%252C292%2520reasoning%2520steps%2520with%2520timestamps.%2520These%2520videos%2520are%2520curated%2520via%250Aa%2520multi-stage%2520filtering%2520process%2520including%2520expert%2520inter-rater%2520reviewing%2520to%250Aprioritize%2520plot%2520coherence.%2520We%2520develop%2520a%2520human-AI%2520collaborative%2520framework%2520that%250Agenerates%2520coherent%2520reasoning%2520chains%252C%2520each%2520requiring%2520multiple%2520temporally%250Agrounded%2520steps%252C%2520spanning%2520seven%2520types%2520%2528e.g.%252C%2520event%2520attribution%252C%2520implicit%250Ainference%2529.%2520VRBench%2520designs%2520a%2520multi-phase%2520evaluation%2520pipeline%2520that%2520assesses%250Amodels%2520at%2520both%2520the%2520outcome%2520and%2520process%2520levels.%2520Apart%2520from%2520the%2520MCQs%2520for%2520the%250Afinal%2520results%252C%2520we%2520propose%2520a%2520progress-level%2520LLM-guided%2520scoring%2520metric%2520to%250Aevaluate%2520the%2520quality%2520of%2520the%2520reasoning%2520chain%2520from%2520multiple%2520dimensions%250Acomprehensively.%2520Through%2520extensive%2520evaluations%2520of%252012%2520LLMs%2520and%252016%2520VLMs%2520on%250AVRBench%252C%2520we%2520undertake%2520a%2520thorough%2520analysis%2520and%2520provide%2520valuable%2520insights%2520that%250Aadvance%2520the%2520field%2520of%2520multi-step%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRBench%3A%20A%20Benchmark%20for%20Multi-Step%20Reasoning%20in%20Long%20Narrative%20Videos&entry.906535625=Jiashuo%20Yu%20and%20Yue%20Wu%20and%20Meng%20Chu%20and%20Zhifei%20Ren%20and%20Zizheng%20Huang%20and%20Pei%20Chu%20and%20Ruijie%20Zhang%20and%20Yinan%20He%20and%20Qirui%20Li%20and%20Songze%20Li%20and%20Zhenxiang%20Li%20and%20Zhongying%20Tu%20and%20Conghui%20He%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Yi%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20We%20present%20VRBench%2C%20the%20first%20long%20narrative%20video%20benchmark%20crafted%20for%0Aevaluating%20large%20models%27%20multi-step%20reasoning%20capabilities%2C%20addressing%0Alimitations%20in%20existing%20evaluations%20that%20overlook%20temporal%20reasoning%20and%0Aprocedural%20validity.%20It%20comprises%201%2C010%20long%20videos%20%28with%20an%20average%20duration%0Aof%201.6%20hours%29%2C%20along%20with%209%2C468%20human-labeled%20multi-step%20question-answering%0Apairs%20and%2030%2C292%20reasoning%20steps%20with%20timestamps.%20These%20videos%20are%20curated%20via%0Aa%20multi-stage%20filtering%20process%20including%20expert%20inter-rater%20reviewing%20to%0Aprioritize%20plot%20coherence.%20We%20develop%20a%20human-AI%20collaborative%20framework%20that%0Agenerates%20coherent%20reasoning%20chains%2C%20each%20requiring%20multiple%20temporally%0Agrounded%20steps%2C%20spanning%20seven%20types%20%28e.g.%2C%20event%20attribution%2C%20implicit%0Ainference%29.%20VRBench%20designs%20a%20multi-phase%20evaluation%20pipeline%20that%20assesses%0Amodels%20at%20both%20the%20outcome%20and%20process%20levels.%20Apart%20from%20the%20MCQs%20for%20the%0Afinal%20results%2C%20we%20propose%20a%20progress-level%20LLM-guided%20scoring%20metric%20to%0Aevaluate%20the%20quality%20of%20the%20reasoning%20chain%20from%20multiple%20dimensions%0Acomprehensively.%20Through%20extensive%20evaluations%20of%2012%20LLMs%20and%2016%20VLMs%20on%0AVRBench%2C%20we%20undertake%20a%20thorough%20analysis%20and%20provide%20valuable%20insights%20that%0Aadvance%20the%20field%20of%20multi-step%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10857v1&entry.124074799=Read"},
{"title": "Scalable unsupervised feature selection via weight stability", "author": "Xudong Zhang and Renato Cordeiro de Amorim", "abstract": "  Unsupervised feature selection is critical for improving clustering\nperformance in high-dimensional data, where irrelevant features can obscure\nmeaningful structure. In this work, we introduce the Minkowski weighted\n$k$-means++, a novel initialisation strategy for the Minkowski Weighted\n$k$-means. Our initialisation selects centroids probabilistically using feature\nrelevance estimates derived from the data itself. Building on this, we propose\ntwo new feature selection algorithms, FS-MWK++, which aggregates feature\nweights across a range of Minkowski exponents to identify stable and\ninformative features, and SFS-MWK++, a scalable variant based on subsampling.\nWe support our approach with a theoretical guarantee under mild assumptions and\nextensive experiments showing that our methods consistently outperform existing\nalternatives. Our software can be found at\nhttps://github.com/xzhang4-ops1/FSMWK.\n", "link": "http://arxiv.org/abs/2506.06114v2", "date": "2025-06-12", "relevancy": 2.1371, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4486}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4244}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability&body=Title%3A%20Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability%0AAuthor%3A%20Xudong%20Zhang%20and%20Renato%20Cordeiro%20de%20Amorim%0AAbstract%3A%20%20%20Unsupervised%20feature%20selection%20is%20critical%20for%20improving%20clustering%0Aperformance%20in%20high-dimensional%20data%2C%20where%20irrelevant%20features%20can%20obscure%0Ameaningful%20structure.%20In%20this%20work%2C%20we%20introduce%20the%20Minkowski%20weighted%0A%24k%24-means%2B%2B%2C%20a%20novel%20initialisation%20strategy%20for%20the%20Minkowski%20Weighted%0A%24k%24-means.%20Our%20initialisation%20selects%20centroids%20probabilistically%20using%20feature%0Arelevance%20estimates%20derived%20from%20the%20data%20itself.%20Building%20on%20this%2C%20we%20propose%0Atwo%20new%20feature%20selection%20algorithms%2C%20FS-MWK%2B%2B%2C%20which%20aggregates%20feature%0Aweights%20across%20a%20range%20of%20Minkowski%20exponents%20to%20identify%20stable%20and%0Ainformative%20features%2C%20and%20SFS-MWK%2B%2B%2C%20a%20scalable%20variant%20based%20on%20subsampling.%0AWe%20support%20our%20approach%20with%20a%20theoretical%20guarantee%20under%20mild%20assumptions%20and%0Aextensive%20experiments%20showing%20that%20our%20methods%20consistently%20outperform%20existing%0Aalternatives.%20Our%20software%20can%20be%20found%20at%0Ahttps%3A//github.com/xzhang4-ops1/FSMWK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520unsupervised%2520feature%2520selection%2520via%2520weight%2520stability%26entry.906535625%3DXudong%2520Zhang%2520and%2520Renato%2520Cordeiro%2520de%2520Amorim%26entry.1292438233%3D%2520%2520Unsupervised%2520feature%2520selection%2520is%2520critical%2520for%2520improving%2520clustering%250Aperformance%2520in%2520high-dimensional%2520data%252C%2520where%2520irrelevant%2520features%2520can%2520obscure%250Ameaningful%2520structure.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Minkowski%2520weighted%250A%2524k%2524-means%252B%252B%252C%2520a%2520novel%2520initialisation%2520strategy%2520for%2520the%2520Minkowski%2520Weighted%250A%2524k%2524-means.%2520Our%2520initialisation%2520selects%2520centroids%2520probabilistically%2520using%2520feature%250Arelevance%2520estimates%2520derived%2520from%2520the%2520data%2520itself.%2520Building%2520on%2520this%252C%2520we%2520propose%250Atwo%2520new%2520feature%2520selection%2520algorithms%252C%2520FS-MWK%252B%252B%252C%2520which%2520aggregates%2520feature%250Aweights%2520across%2520a%2520range%2520of%2520Minkowski%2520exponents%2520to%2520identify%2520stable%2520and%250Ainformative%2520features%252C%2520and%2520SFS-MWK%252B%252B%252C%2520a%2520scalable%2520variant%2520based%2520on%2520subsampling.%250AWe%2520support%2520our%2520approach%2520with%2520a%2520theoretical%2520guarantee%2520under%2520mild%2520assumptions%2520and%250Aextensive%2520experiments%2520showing%2520that%2520our%2520methods%2520consistently%2520outperform%2520existing%250Aalternatives.%2520Our%2520software%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/xzhang4-ops1/FSMWK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability&entry.906535625=Xudong%20Zhang%20and%20Renato%20Cordeiro%20de%20Amorim&entry.1292438233=%20%20Unsupervised%20feature%20selection%20is%20critical%20for%20improving%20clustering%0Aperformance%20in%20high-dimensional%20data%2C%20where%20irrelevant%20features%20can%20obscure%0Ameaningful%20structure.%20In%20this%20work%2C%20we%20introduce%20the%20Minkowski%20weighted%0A%24k%24-means%2B%2B%2C%20a%20novel%20initialisation%20strategy%20for%20the%20Minkowski%20Weighted%0A%24k%24-means.%20Our%20initialisation%20selects%20centroids%20probabilistically%20using%20feature%0Arelevance%20estimates%20derived%20from%20the%20data%20itself.%20Building%20on%20this%2C%20we%20propose%0Atwo%20new%20feature%20selection%20algorithms%2C%20FS-MWK%2B%2B%2C%20which%20aggregates%20feature%0Aweights%20across%20a%20range%20of%20Minkowski%20exponents%20to%20identify%20stable%20and%0Ainformative%20features%2C%20and%20SFS-MWK%2B%2B%2C%20a%20scalable%20variant%20based%20on%20subsampling.%0AWe%20support%20our%20approach%20with%20a%20theoretical%20guarantee%20under%20mild%20assumptions%20and%0Aextensive%20experiments%20showing%20that%20our%20methods%20consistently%20outperform%20existing%0Aalternatives.%20Our%20software%20can%20be%20found%20at%0Ahttps%3A//github.com/xzhang4-ops1/FSMWK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06114v2&entry.124074799=Read"},
{"title": "A User's Guide to Sampling Strategies for Sliced Optimal Transport", "author": "Keanu Sisouk and Julie Delon and Julien Tierny", "abstract": "  This paper serves as a user's guide to sampling strategies for sliced optimal\ntransport. We provide reminders and additional regularity results on the Sliced\nWasserstein distance. We detail the construction methods, generation time\ncomplexity, theoretical guarantees, and conditions for each strategy.\nAdditionally, we provide insights into their suitability for sliced optimal\ntransport in theory. Extensive experiments on both simulated and real-world\ndata offer a representative comparison of the strategies, culminating in\npractical recommendations for their best usage.\n", "link": "http://arxiv.org/abs/2502.02275v4", "date": "2025-06-12", "relevancy": 2.1314, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.443}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4226}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20User%27s%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport&body=Title%3A%20A%20User%27s%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport%0AAuthor%3A%20Keanu%20Sisouk%20and%20Julie%20Delon%20and%20Julien%20Tierny%0AAbstract%3A%20%20%20This%20paper%20serves%20as%20a%20user%27s%20guide%20to%20sampling%20strategies%20for%20sliced%20optimal%0Atransport.%20We%20provide%20reminders%20and%20additional%20regularity%20results%20on%20the%20Sliced%0AWasserstein%20distance.%20We%20detail%20the%20construction%20methods%2C%20generation%20time%0Acomplexity%2C%20theoretical%20guarantees%2C%20and%20conditions%20for%20each%20strategy.%0AAdditionally%2C%20we%20provide%20insights%20into%20their%20suitability%20for%20sliced%20optimal%0Atransport%20in%20theory.%20Extensive%20experiments%20on%20both%20simulated%20and%20real-world%0Adata%20offer%20a%20representative%20comparison%20of%20the%20strategies%2C%20culminating%20in%0Apractical%20recommendations%20for%20their%20best%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02275v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520User%2527s%2520Guide%2520to%2520Sampling%2520Strategies%2520for%2520Sliced%2520Optimal%2520Transport%26entry.906535625%3DKeanu%2520Sisouk%2520and%2520Julie%2520Delon%2520and%2520Julien%2520Tierny%26entry.1292438233%3D%2520%2520This%2520paper%2520serves%2520as%2520a%2520user%2527s%2520guide%2520to%2520sampling%2520strategies%2520for%2520sliced%2520optimal%250Atransport.%2520We%2520provide%2520reminders%2520and%2520additional%2520regularity%2520results%2520on%2520the%2520Sliced%250AWasserstein%2520distance.%2520We%2520detail%2520the%2520construction%2520methods%252C%2520generation%2520time%250Acomplexity%252C%2520theoretical%2520guarantees%252C%2520and%2520conditions%2520for%2520each%2520strategy.%250AAdditionally%252C%2520we%2520provide%2520insights%2520into%2520their%2520suitability%2520for%2520sliced%2520optimal%250Atransport%2520in%2520theory.%2520Extensive%2520experiments%2520on%2520both%2520simulated%2520and%2520real-world%250Adata%2520offer%2520a%2520representative%2520comparison%2520of%2520the%2520strategies%252C%2520culminating%2520in%250Apractical%2520recommendations%2520for%2520their%2520best%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02275v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20User%27s%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport&entry.906535625=Keanu%20Sisouk%20and%20Julie%20Delon%20and%20Julien%20Tierny&entry.1292438233=%20%20This%20paper%20serves%20as%20a%20user%27s%20guide%20to%20sampling%20strategies%20for%20sliced%20optimal%0Atransport.%20We%20provide%20reminders%20and%20additional%20regularity%20results%20on%20the%20Sliced%0AWasserstein%20distance.%20We%20detail%20the%20construction%20methods%2C%20generation%20time%0Acomplexity%2C%20theoretical%20guarantees%2C%20and%20conditions%20for%20each%20strategy.%0AAdditionally%2C%20we%20provide%20insights%20into%20their%20suitability%20for%20sliced%20optimal%0Atransport%20in%20theory.%20Extensive%20experiments%20on%20both%20simulated%20and%20real-world%0Adata%20offer%20a%20representative%20comparison%20of%20the%20strategies%2C%20culminating%20in%0Apractical%20recommendations%20for%20their%20best%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02275v4&entry.124074799=Read"},
{"title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for\n  Text-to-Image Reasoning", "author": "Yuxuan Luo and Yuhui Yuan and Junwen Chen and Haonan Cai and Ziyi Yue and Yuwei Yang and Fatima Zohra Daha and Ji Li and Zhouhui Lian", "abstract": "  In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning--a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits--low entity fidelity, weak\nrelations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,\nunderscoring the benchmark's difficulty. To spur further progress, we release\nFLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines\na reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.\n", "link": "http://arxiv.org/abs/2506.10963v1", "date": "2025-06-12", "relevancy": 2.1267, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5409}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5301}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMMG%3A%20A%20Massive%2C%20Multidisciplinary%2C%20Multi-Tier%20Generation%20Benchmark%20for%0A%20%20Text-to-Image%20Reasoning&body=Title%3A%20MMMG%3A%20A%20Massive%2C%20Multidisciplinary%2C%20Multi-Tier%20Generation%20Benchmark%20for%0A%20%20Text-to-Image%20Reasoning%0AAuthor%3A%20Yuxuan%20Luo%20and%20Yuhui%20Yuan%20and%20Junwen%20Chen%20and%20Haonan%20Cai%20and%20Ziyi%20Yue%20and%20Yuwei%20Yang%20and%20Fatima%20Zohra%20Daha%20and%20Ji%20Li%20and%20Zhouhui%20Lian%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20knowledge%20image%20generation%20as%20a%20new%20task%2C%0Aalongside%20the%20Massive%20Multi-Discipline%20Multi-Tier%20Knowledge-Image%20Generation%0ABenchmark%20%28MMMG%29%20to%20probe%20the%20reasoning%20capability%20of%20image%20generation%20models.%0AKnowledge%20images%20have%20been%20central%20to%20human%20civilization%20and%20to%20the%20mechanisms%0Aof%20human%20learning--a%20fact%20underscored%20by%20dual-coding%20theory%20and%20the%0Apicture-superiority%20effect.%20Generating%20such%20images%20is%20challenging%2C%20demanding%0Amultimodal%20reasoning%20that%20fuses%20world%20knowledge%20with%20pixel-level%20grounding%20into%0Aclear%20explanatory%20visuals.%20To%20enable%20comprehensive%20evaluation%2C%20MMMG%20offers%0A4%2C456%20expert-validated%20%28knowledge%29%20image-prompt%20pairs%20spanning%2010%20disciplines%2C%0A6%20educational%20levels%2C%20and%20diverse%20knowledge%20formats%20such%20as%20charts%2C%20diagrams%2C%0Aand%20mind%20maps.%20To%20eliminate%20confounding%20complexity%20during%20evaluation%2C%20we%20adopt%0Aa%20unified%20Knowledge%20Graph%20%28KG%29%20representation.%20Each%20KG%20explicitly%20delineates%20a%0Atarget%20image%27s%20core%20entities%20and%20their%20dependencies.%20We%20further%20introduce%0AMMMG-Score%20to%20evaluate%20generated%20knowledge%20images.%20This%20metric%20combines%20factual%0Afidelity%2C%20measured%20by%20graph-edit%20distance%20between%20KGs%2C%20with%20visual%20clarity%0Aassessment.%20Comprehensive%20evaluations%20of%2016%20state-of-the-art%20text-to-image%0Ageneration%20models%20expose%20serious%20reasoning%20deficits--low%20entity%20fidelity%2C%20weak%0Arelations%2C%20and%20clutter--with%20GPT-4o%20achieving%20an%20MMMG-Score%20of%20only%2050.20%2C%0Aunderscoring%20the%20benchmark%27s%20difficulty.%20To%20spur%20further%20progress%2C%20we%20release%0AFLUX-Reason%20%28MMMG-Score%20of%2034.45%29%2C%20an%20effective%20and%20open%20baseline%20that%20combines%0Aa%20reasoning%20LLM%20with%20diffusion%20models%20and%20is%20trained%20on%2016%2C000%20curated%0Aknowledge%20image-prompt%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMMG%253A%2520A%2520Massive%252C%2520Multidisciplinary%252C%2520Multi-Tier%2520Generation%2520Benchmark%2520for%250A%2520%2520Text-to-Image%2520Reasoning%26entry.906535625%3DYuxuan%2520Luo%2520and%2520Yuhui%2520Yuan%2520and%2520Junwen%2520Chen%2520and%2520Haonan%2520Cai%2520and%2520Ziyi%2520Yue%2520and%2520Yuwei%2520Yang%2520and%2520Fatima%2520Zohra%2520Daha%2520and%2520Ji%2520Li%2520and%2520Zhouhui%2520Lian%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520knowledge%2520image%2520generation%2520as%2520a%2520new%2520task%252C%250Aalongside%2520the%2520Massive%2520Multi-Discipline%2520Multi-Tier%2520Knowledge-Image%2520Generation%250ABenchmark%2520%2528MMMG%2529%2520to%2520probe%2520the%2520reasoning%2520capability%2520of%2520image%2520generation%2520models.%250AKnowledge%2520images%2520have%2520been%2520central%2520to%2520human%2520civilization%2520and%2520to%2520the%2520mechanisms%250Aof%2520human%2520learning--a%2520fact%2520underscored%2520by%2520dual-coding%2520theory%2520and%2520the%250Apicture-superiority%2520effect.%2520Generating%2520such%2520images%2520is%2520challenging%252C%2520demanding%250Amultimodal%2520reasoning%2520that%2520fuses%2520world%2520knowledge%2520with%2520pixel-level%2520grounding%2520into%250Aclear%2520explanatory%2520visuals.%2520To%2520enable%2520comprehensive%2520evaluation%252C%2520MMMG%2520offers%250A4%252C456%2520expert-validated%2520%2528knowledge%2529%2520image-prompt%2520pairs%2520spanning%252010%2520disciplines%252C%250A6%2520educational%2520levels%252C%2520and%2520diverse%2520knowledge%2520formats%2520such%2520as%2520charts%252C%2520diagrams%252C%250Aand%2520mind%2520maps.%2520To%2520eliminate%2520confounding%2520complexity%2520during%2520evaluation%252C%2520we%2520adopt%250Aa%2520unified%2520Knowledge%2520Graph%2520%2528KG%2529%2520representation.%2520Each%2520KG%2520explicitly%2520delineates%2520a%250Atarget%2520image%2527s%2520core%2520entities%2520and%2520their%2520dependencies.%2520We%2520further%2520introduce%250AMMMG-Score%2520to%2520evaluate%2520generated%2520knowledge%2520images.%2520This%2520metric%2520combines%2520factual%250Afidelity%252C%2520measured%2520by%2520graph-edit%2520distance%2520between%2520KGs%252C%2520with%2520visual%2520clarity%250Aassessment.%2520Comprehensive%2520evaluations%2520of%252016%2520state-of-the-art%2520text-to-image%250Ageneration%2520models%2520expose%2520serious%2520reasoning%2520deficits--low%2520entity%2520fidelity%252C%2520weak%250Arelations%252C%2520and%2520clutter--with%2520GPT-4o%2520achieving%2520an%2520MMMG-Score%2520of%2520only%252050.20%252C%250Aunderscoring%2520the%2520benchmark%2527s%2520difficulty.%2520To%2520spur%2520further%2520progress%252C%2520we%2520release%250AFLUX-Reason%2520%2528MMMG-Score%2520of%252034.45%2529%252C%2520an%2520effective%2520and%2520open%2520baseline%2520that%2520combines%250Aa%2520reasoning%2520LLM%2520with%2520diffusion%2520models%2520and%2520is%2520trained%2520on%252016%252C000%2520curated%250Aknowledge%2520image-prompt%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMMG%3A%20A%20Massive%2C%20Multidisciplinary%2C%20Multi-Tier%20Generation%20Benchmark%20for%0A%20%20Text-to-Image%20Reasoning&entry.906535625=Yuxuan%20Luo%20and%20Yuhui%20Yuan%20and%20Junwen%20Chen%20and%20Haonan%20Cai%20and%20Ziyi%20Yue%20and%20Yuwei%20Yang%20and%20Fatima%20Zohra%20Daha%20and%20Ji%20Li%20and%20Zhouhui%20Lian&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20knowledge%20image%20generation%20as%20a%20new%20task%2C%0Aalongside%20the%20Massive%20Multi-Discipline%20Multi-Tier%20Knowledge-Image%20Generation%0ABenchmark%20%28MMMG%29%20to%20probe%20the%20reasoning%20capability%20of%20image%20generation%20models.%0AKnowledge%20images%20have%20been%20central%20to%20human%20civilization%20and%20to%20the%20mechanisms%0Aof%20human%20learning--a%20fact%20underscored%20by%20dual-coding%20theory%20and%20the%0Apicture-superiority%20effect.%20Generating%20such%20images%20is%20challenging%2C%20demanding%0Amultimodal%20reasoning%20that%20fuses%20world%20knowledge%20with%20pixel-level%20grounding%20into%0Aclear%20explanatory%20visuals.%20To%20enable%20comprehensive%20evaluation%2C%20MMMG%20offers%0A4%2C456%20expert-validated%20%28knowledge%29%20image-prompt%20pairs%20spanning%2010%20disciplines%2C%0A6%20educational%20levels%2C%20and%20diverse%20knowledge%20formats%20such%20as%20charts%2C%20diagrams%2C%0Aand%20mind%20maps.%20To%20eliminate%20confounding%20complexity%20during%20evaluation%2C%20we%20adopt%0Aa%20unified%20Knowledge%20Graph%20%28KG%29%20representation.%20Each%20KG%20explicitly%20delineates%20a%0Atarget%20image%27s%20core%20entities%20and%20their%20dependencies.%20We%20further%20introduce%0AMMMG-Score%20to%20evaluate%20generated%20knowledge%20images.%20This%20metric%20combines%20factual%0Afidelity%2C%20measured%20by%20graph-edit%20distance%20between%20KGs%2C%20with%20visual%20clarity%0Aassessment.%20Comprehensive%20evaluations%20of%2016%20state-of-the-art%20text-to-image%0Ageneration%20models%20expose%20serious%20reasoning%20deficits--low%20entity%20fidelity%2C%20weak%0Arelations%2C%20and%20clutter--with%20GPT-4o%20achieving%20an%20MMMG-Score%20of%20only%2050.20%2C%0Aunderscoring%20the%20benchmark%27s%20difficulty.%20To%20spur%20further%20progress%2C%20we%20release%0AFLUX-Reason%20%28MMMG-Score%20of%2034.45%29%2C%20an%20effective%20and%20open%20baseline%20that%20combines%0Aa%20reasoning%20LLM%20with%20diffusion%20models%20and%20is%20trained%20on%2016%2C000%20curated%0Aknowledge%20image-prompt%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10963v1&entry.124074799=Read"},
{"title": "Invariant Extended Kalman Filter for Autonomous Surface Vessels with\n  Partial Orientation Measurements", "author": "Derek Benham and Easton Potokar and Joshua G. Mangelson", "abstract": "  Autonomous surface vessels (ASVs) are increasingly vital for marine science,\noffering robust platforms for underwater mapping and inspection. Accurate state\nestimation, particularly of vehicle pose, is paramount for precise seafloor\nmapping, as even small surface deviations can have significant consequences\nwhen sensing the seafloor below. To address this challenge, we propose an\nInvariant Extended Kalman Filter (InEKF) framework designed to integrate\npartial orientation measurements. While conventional estimation often relies on\nrelative position measurements to fixed landmarks, open ocean ASVs primarily\nobserve a receding horizon. We leverage forward-facing monocular cameras to\nestimate roll and pitch with respect to this horizon, which provides\nyaw-ambiguous partial orientation information. To effectively utilize these\nmeasurements within the InEKF, we introduce a novel framework for incorporating\nsuch partial orientation data. This approach contrasts with traditional InEKF\nimplementations that assume full orientation measurements and is particularly\nrelevant for planar vehicle motion constrained to a \"seafaring plane.\" This\npaper details the developed InEKF framework; its integration with horizon-based\nroll/pitch observations and dual-antenna GPS heading measurements for ASV state\nestimation; and provides a comparative analysis against the InEKF using full\norientation and a Multiplicative EKF (MEKF). Our results demonstrate the\nefficacy and robustness of the proposed partial orientation measurements for\naccurate ASV state estimation in open ocean environments.\n", "link": "http://arxiv.org/abs/2506.10850v1", "date": "2025-06-12", "relevancy": 2.1209, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5379}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariant%20Extended%20Kalman%20Filter%20for%20Autonomous%20Surface%20Vessels%20with%0A%20%20Partial%20Orientation%20Measurements&body=Title%3A%20Invariant%20Extended%20Kalman%20Filter%20for%20Autonomous%20Surface%20Vessels%20with%0A%20%20Partial%20Orientation%20Measurements%0AAuthor%3A%20Derek%20Benham%20and%20Easton%20Potokar%20and%20Joshua%20G.%20Mangelson%0AAbstract%3A%20%20%20Autonomous%20surface%20vessels%20%28ASVs%29%20are%20increasingly%20vital%20for%20marine%20science%2C%0Aoffering%20robust%20platforms%20for%20underwater%20mapping%20and%20inspection.%20Accurate%20state%0Aestimation%2C%20particularly%20of%20vehicle%20pose%2C%20is%20paramount%20for%20precise%20seafloor%0Amapping%2C%20as%20even%20small%20surface%20deviations%20can%20have%20significant%20consequences%0Awhen%20sensing%20the%20seafloor%20below.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0AInvariant%20Extended%20Kalman%20Filter%20%28InEKF%29%20framework%20designed%20to%20integrate%0Apartial%20orientation%20measurements.%20While%20conventional%20estimation%20often%20relies%20on%0Arelative%20position%20measurements%20to%20fixed%20landmarks%2C%20open%20ocean%20ASVs%20primarily%0Aobserve%20a%20receding%20horizon.%20We%20leverage%20forward-facing%20monocular%20cameras%20to%0Aestimate%20roll%20and%20pitch%20with%20respect%20to%20this%20horizon%2C%20which%20provides%0Ayaw-ambiguous%20partial%20orientation%20information.%20To%20effectively%20utilize%20these%0Ameasurements%20within%20the%20InEKF%2C%20we%20introduce%20a%20novel%20framework%20for%20incorporating%0Asuch%20partial%20orientation%20data.%20This%20approach%20contrasts%20with%20traditional%20InEKF%0Aimplementations%20that%20assume%20full%20orientation%20measurements%20and%20is%20particularly%0Arelevant%20for%20planar%20vehicle%20motion%20constrained%20to%20a%20%22seafaring%20plane.%22%20This%0Apaper%20details%20the%20developed%20InEKF%20framework%3B%20its%20integration%20with%20horizon-based%0Aroll/pitch%20observations%20and%20dual-antenna%20GPS%20heading%20measurements%20for%20ASV%20state%0Aestimation%3B%20and%20provides%20a%20comparative%20analysis%20against%20the%20InEKF%20using%20full%0Aorientation%20and%20a%20Multiplicative%20EKF%20%28MEKF%29.%20Our%20results%20demonstrate%20the%0Aefficacy%20and%20robustness%20of%20the%20proposed%20partial%20orientation%20measurements%20for%0Aaccurate%20ASV%20state%20estimation%20in%20open%20ocean%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariant%2520Extended%2520Kalman%2520Filter%2520for%2520Autonomous%2520Surface%2520Vessels%2520with%250A%2520%2520Partial%2520Orientation%2520Measurements%26entry.906535625%3DDerek%2520Benham%2520and%2520Easton%2520Potokar%2520and%2520Joshua%2520G.%2520Mangelson%26entry.1292438233%3D%2520%2520Autonomous%2520surface%2520vessels%2520%2528ASVs%2529%2520are%2520increasingly%2520vital%2520for%2520marine%2520science%252C%250Aoffering%2520robust%2520platforms%2520for%2520underwater%2520mapping%2520and%2520inspection.%2520Accurate%2520state%250Aestimation%252C%2520particularly%2520of%2520vehicle%2520pose%252C%2520is%2520paramount%2520for%2520precise%2520seafloor%250Amapping%252C%2520as%2520even%2520small%2520surface%2520deviations%2520can%2520have%2520significant%2520consequences%250Awhen%2520sensing%2520the%2520seafloor%2520below.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%250AInvariant%2520Extended%2520Kalman%2520Filter%2520%2528InEKF%2529%2520framework%2520designed%2520to%2520integrate%250Apartial%2520orientation%2520measurements.%2520While%2520conventional%2520estimation%2520often%2520relies%2520on%250Arelative%2520position%2520measurements%2520to%2520fixed%2520landmarks%252C%2520open%2520ocean%2520ASVs%2520primarily%250Aobserve%2520a%2520receding%2520horizon.%2520We%2520leverage%2520forward-facing%2520monocular%2520cameras%2520to%250Aestimate%2520roll%2520and%2520pitch%2520with%2520respect%2520to%2520this%2520horizon%252C%2520which%2520provides%250Ayaw-ambiguous%2520partial%2520orientation%2520information.%2520To%2520effectively%2520utilize%2520these%250Ameasurements%2520within%2520the%2520InEKF%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520for%2520incorporating%250Asuch%2520partial%2520orientation%2520data.%2520This%2520approach%2520contrasts%2520with%2520traditional%2520InEKF%250Aimplementations%2520that%2520assume%2520full%2520orientation%2520measurements%2520and%2520is%2520particularly%250Arelevant%2520for%2520planar%2520vehicle%2520motion%2520constrained%2520to%2520a%2520%2522seafaring%2520plane.%2522%2520This%250Apaper%2520details%2520the%2520developed%2520InEKF%2520framework%253B%2520its%2520integration%2520with%2520horizon-based%250Aroll/pitch%2520observations%2520and%2520dual-antenna%2520GPS%2520heading%2520measurements%2520for%2520ASV%2520state%250Aestimation%253B%2520and%2520provides%2520a%2520comparative%2520analysis%2520against%2520the%2520InEKF%2520using%2520full%250Aorientation%2520and%2520a%2520Multiplicative%2520EKF%2520%2528MEKF%2529.%2520Our%2520results%2520demonstrate%2520the%250Aefficacy%2520and%2520robustness%2520of%2520the%2520proposed%2520partial%2520orientation%2520measurements%2520for%250Aaccurate%2520ASV%2520state%2520estimation%2520in%2520open%2520ocean%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariant%20Extended%20Kalman%20Filter%20for%20Autonomous%20Surface%20Vessels%20with%0A%20%20Partial%20Orientation%20Measurements&entry.906535625=Derek%20Benham%20and%20Easton%20Potokar%20and%20Joshua%20G.%20Mangelson&entry.1292438233=%20%20Autonomous%20surface%20vessels%20%28ASVs%29%20are%20increasingly%20vital%20for%20marine%20science%2C%0Aoffering%20robust%20platforms%20for%20underwater%20mapping%20and%20inspection.%20Accurate%20state%0Aestimation%2C%20particularly%20of%20vehicle%20pose%2C%20is%20paramount%20for%20precise%20seafloor%0Amapping%2C%20as%20even%20small%20surface%20deviations%20can%20have%20significant%20consequences%0Awhen%20sensing%20the%20seafloor%20below.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0AInvariant%20Extended%20Kalman%20Filter%20%28InEKF%29%20framework%20designed%20to%20integrate%0Apartial%20orientation%20measurements.%20While%20conventional%20estimation%20often%20relies%20on%0Arelative%20position%20measurements%20to%20fixed%20landmarks%2C%20open%20ocean%20ASVs%20primarily%0Aobserve%20a%20receding%20horizon.%20We%20leverage%20forward-facing%20monocular%20cameras%20to%0Aestimate%20roll%20and%20pitch%20with%20respect%20to%20this%20horizon%2C%20which%20provides%0Ayaw-ambiguous%20partial%20orientation%20information.%20To%20effectively%20utilize%20these%0Ameasurements%20within%20the%20InEKF%2C%20we%20introduce%20a%20novel%20framework%20for%20incorporating%0Asuch%20partial%20orientation%20data.%20This%20approach%20contrasts%20with%20traditional%20InEKF%0Aimplementations%20that%20assume%20full%20orientation%20measurements%20and%20is%20particularly%0Arelevant%20for%20planar%20vehicle%20motion%20constrained%20to%20a%20%22seafaring%20plane.%22%20This%0Apaper%20details%20the%20developed%20InEKF%20framework%3B%20its%20integration%20with%20horizon-based%0Aroll/pitch%20observations%20and%20dual-antenna%20GPS%20heading%20measurements%20for%20ASV%20state%0Aestimation%3B%20and%20provides%20a%20comparative%20analysis%20against%20the%20InEKF%20using%20full%0Aorientation%20and%20a%20Multiplicative%20EKF%20%28MEKF%29.%20Our%20results%20demonstrate%20the%0Aefficacy%20and%20robustness%20of%20the%20proposed%20partial%20orientation%20measurements%20for%0Aaccurate%20ASV%20state%20estimation%20in%20open%20ocean%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10850v1&entry.124074799=Read"},
{"title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models", "author": "Yulei Qin and Gang Li and Zongyi Li and Zihan Xu and Yuchen Shi and Zhekai Lin and Xiao Cui and Ke Li and Xing Sun", "abstract": "  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.\n", "link": "http://arxiv.org/abs/2506.01413v2", "date": "2025-06-12", "relevancy": 2.1091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incentivizing%20Reasoning%20for%20Advanced%20Instruction-Following%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20Incentivizing%20Reasoning%20for%20Advanced%20Instruction-Following%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yulei%20Qin%20and%20Gang%20Li%20and%20Zongyi%20Li%20and%20Zihan%20Xu%20and%20Yuchen%20Shi%20and%20Zhekai%20Lin%20and%20Xiao%20Cui%20and%20Ke%20Li%20and%20Xing%20Sun%0AAbstract%3A%20%20%20Existing%20large%20language%20models%20%28LLMs%29%20face%20challenges%20of%20following%20complex%0Ainstructions%2C%20especially%20when%20multiple%20constraints%20are%20present%20and%20organized%20in%0Aparalleling%2C%20chaining%2C%20and%20branching%20structures.%20One%20intuitive%20solution%2C%20namely%0Achain-of-thought%20%28CoT%29%2C%20is%20expected%20to%20universally%20improve%20capabilities%20of%0ALLMs.%20However%2C%20we%20find%20that%20the%20vanilla%20CoT%20exerts%20a%20negative%20impact%20on%0Aperformance%20due%20to%20its%20superficial%20reasoning%20pattern%20of%20simply%20paraphrasing%20the%0Ainstructions.%20It%20fails%20to%20peel%20back%20the%20compositions%20of%20constraints%20for%0Aidentifying%20their%20relationship%20across%20hierarchies%20of%20types%20and%20dimensions.%20To%0Athis%20end%2C%20we%20propose%20a%20systematic%20method%20to%20boost%20LLMs%20in%20dealing%20with%20complex%0Ainstructions%20via%20incentivizing%20reasoning%20for%20test-time%20compute%20scaling.%20First%2C%0Awe%20stem%20from%20the%20decomposition%20of%20complex%20instructions%20under%20existing%0Ataxonomies%20and%20propose%20a%20reproducible%20data%20acquisition%20method.%20Second%2C%20we%0Aexploit%20reinforcement%20learning%20%28RL%29%20with%20verifiable%20rule-centric%20reward%20signals%0Ato%20cultivate%20reasoning%20specifically%20for%20instruction%20following.%20We%20address%20the%0Ashallow%2C%20non-essential%20nature%20of%20reasoning%20under%20complex%20instructions%20via%0Asample-wise%20contrast%20for%20superior%20CoT%20enforcement.%20We%20also%20exploit%20behavior%0Acloning%20of%20experts%20to%20facilitate%20steady%20distribution%20shift%20from%20fast-thinking%0ALLMs%20to%20skillful%20reasoners.%20Extensive%20evaluations%20on%20seven%20comprehensive%0Abenchmarks%20confirm%20the%20validity%20of%20the%20proposed%20method%2C%20where%20a%201.5B%20LLM%0Aachieves%2011.74%25%20gains%20with%20performance%20comparable%20to%20a%208B%20LLM.%20Codes%20and%20data%0Aare%20available%20at%20https%3A//github.com/yuleiqin/RAIF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncentivizing%2520Reasoning%2520for%2520Advanced%2520Instruction-Following%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYulei%2520Qin%2520and%2520Gang%2520Li%2520and%2520Zongyi%2520Li%2520and%2520Zihan%2520Xu%2520and%2520Yuchen%2520Shi%2520and%2520Zhekai%2520Lin%2520and%2520Xiao%2520Cui%2520and%2520Ke%2520Li%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520Existing%2520large%2520language%2520models%2520%2528LLMs%2529%2520face%2520challenges%2520of%2520following%2520complex%250Ainstructions%252C%2520especially%2520when%2520multiple%2520constraints%2520are%2520present%2520and%2520organized%2520in%250Aparalleling%252C%2520chaining%252C%2520and%2520branching%2520structures.%2520One%2520intuitive%2520solution%252C%2520namely%250Achain-of-thought%2520%2528CoT%2529%252C%2520is%2520expected%2520to%2520universally%2520improve%2520capabilities%2520of%250ALLMs.%2520However%252C%2520we%2520find%2520that%2520the%2520vanilla%2520CoT%2520exerts%2520a%2520negative%2520impact%2520on%250Aperformance%2520due%2520to%2520its%2520superficial%2520reasoning%2520pattern%2520of%2520simply%2520paraphrasing%2520the%250Ainstructions.%2520It%2520fails%2520to%2520peel%2520back%2520the%2520compositions%2520of%2520constraints%2520for%250Aidentifying%2520their%2520relationship%2520across%2520hierarchies%2520of%2520types%2520and%2520dimensions.%2520To%250Athis%2520end%252C%2520we%2520propose%2520a%2520systematic%2520method%2520to%2520boost%2520LLMs%2520in%2520dealing%2520with%2520complex%250Ainstructions%2520via%2520incentivizing%2520reasoning%2520for%2520test-time%2520compute%2520scaling.%2520First%252C%250Awe%2520stem%2520from%2520the%2520decomposition%2520of%2520complex%2520instructions%2520under%2520existing%250Ataxonomies%2520and%2520propose%2520a%2520reproducible%2520data%2520acquisition%2520method.%2520Second%252C%2520we%250Aexploit%2520reinforcement%2520learning%2520%2528RL%2529%2520with%2520verifiable%2520rule-centric%2520reward%2520signals%250Ato%2520cultivate%2520reasoning%2520specifically%2520for%2520instruction%2520following.%2520We%2520address%2520the%250Ashallow%252C%2520non-essential%2520nature%2520of%2520reasoning%2520under%2520complex%2520instructions%2520via%250Asample-wise%2520contrast%2520for%2520superior%2520CoT%2520enforcement.%2520We%2520also%2520exploit%2520behavior%250Acloning%2520of%2520experts%2520to%2520facilitate%2520steady%2520distribution%2520shift%2520from%2520fast-thinking%250ALLMs%2520to%2520skillful%2520reasoners.%2520Extensive%2520evaluations%2520on%2520seven%2520comprehensive%250Abenchmarks%2520confirm%2520the%2520validity%2520of%2520the%2520proposed%2520method%252C%2520where%2520a%25201.5B%2520LLM%250Aachieves%252011.74%2525%2520gains%2520with%2520performance%2520comparable%2520to%2520a%25208B%2520LLM.%2520Codes%2520and%2520data%250Aare%2520available%2520at%2520https%253A//github.com/yuleiqin/RAIF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incentivizing%20Reasoning%20for%20Advanced%20Instruction-Following%20of%20Large%0A%20%20Language%20Models&entry.906535625=Yulei%20Qin%20and%20Gang%20Li%20and%20Zongyi%20Li%20and%20Zihan%20Xu%20and%20Yuchen%20Shi%20and%20Zhekai%20Lin%20and%20Xiao%20Cui%20and%20Ke%20Li%20and%20Xing%20Sun&entry.1292438233=%20%20Existing%20large%20language%20models%20%28LLMs%29%20face%20challenges%20of%20following%20complex%0Ainstructions%2C%20especially%20when%20multiple%20constraints%20are%20present%20and%20organized%20in%0Aparalleling%2C%20chaining%2C%20and%20branching%20structures.%20One%20intuitive%20solution%2C%20namely%0Achain-of-thought%20%28CoT%29%2C%20is%20expected%20to%20universally%20improve%20capabilities%20of%0ALLMs.%20However%2C%20we%20find%20that%20the%20vanilla%20CoT%20exerts%20a%20negative%20impact%20on%0Aperformance%20due%20to%20its%20superficial%20reasoning%20pattern%20of%20simply%20paraphrasing%20the%0Ainstructions.%20It%20fails%20to%20peel%20back%20the%20compositions%20of%20constraints%20for%0Aidentifying%20their%20relationship%20across%20hierarchies%20of%20types%20and%20dimensions.%20To%0Athis%20end%2C%20we%20propose%20a%20systematic%20method%20to%20boost%20LLMs%20in%20dealing%20with%20complex%0Ainstructions%20via%20incentivizing%20reasoning%20for%20test-time%20compute%20scaling.%20First%2C%0Awe%20stem%20from%20the%20decomposition%20of%20complex%20instructions%20under%20existing%0Ataxonomies%20and%20propose%20a%20reproducible%20data%20acquisition%20method.%20Second%2C%20we%0Aexploit%20reinforcement%20learning%20%28RL%29%20with%20verifiable%20rule-centric%20reward%20signals%0Ato%20cultivate%20reasoning%20specifically%20for%20instruction%20following.%20We%20address%20the%0Ashallow%2C%20non-essential%20nature%20of%20reasoning%20under%20complex%20instructions%20via%0Asample-wise%20contrast%20for%20superior%20CoT%20enforcement.%20We%20also%20exploit%20behavior%0Acloning%20of%20experts%20to%20facilitate%20steady%20distribution%20shift%20from%20fast-thinking%0ALLMs%20to%20skillful%20reasoners.%20Extensive%20evaluations%20on%20seven%20comprehensive%0Abenchmarks%20confirm%20the%20validity%20of%20the%20proposed%20method%2C%20where%20a%201.5B%20LLM%0Aachieves%2011.74%25%20gains%20with%20performance%20comparable%20to%20a%208B%20LLM.%20Codes%20and%20data%0Aare%20available%20at%20https%3A//github.com/yuleiqin/RAIF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01413v2&entry.124074799=Read"},
{"title": "IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly\n  Detection in Medical Domain", "author": "Hong Huang and Weixiang Sun and Zhijian Wu and Jingwen Niu and Donghuan Lu and Xian Wu and Yefeng Zheng", "abstract": "  Recent advances in vision-language models, such as CLIP, have significantly\nimproved performance in zero- and few-shot anomaly detection (ZFSAD) tasks.\nHowever, most existing CLIP-based methods assume prior knowledge of categories\nand rely on carefully designed prompts tailored to specific scenarios. While\nthese text prompts capture semantic information in the textual space, they\noften fail to distinguish normal and anomalous instances in the joint embedding\nspace. Moreover, most ZFSAD approaches focus on industrial domains, with\nlimited exploration in medical tasks. To address these limitations, we propose\nIQE-CLIP, a novel framework for ZFSAD in the medical domain. We show that query\nembeddings integrating both textual and instance-aware visual information serve\nas more effective indicators of anomalies. Specifically, we introduce\nclass-based and learnable prompting tokens to better adapt CLIP to the medical\nsetting. Furthermore, we design an instance-aware query module that extracts\nregion-level contextual information from both modalities, enabling the\ngeneration of anomaly-sensitive embeddings. Extensive experiments on six\nmedical datasets demonstrate that IQE-CLIP achieves state-of-the-art\nperformance in both zero-shot and few-shot settings. Code and data are\navailable at \\href{https://github.com/hongh0/IQE-CLIP/}{this https URL}.\n", "link": "http://arxiv.org/abs/2506.10730v1", "date": "2025-06-12", "relevancy": 2.1071, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5316}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5278}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IQE-CLIP%3A%20Instance-aware%20Query%20Embedding%20for%20Zero-/Few-shot%20Anomaly%0A%20%20Detection%20in%20Medical%20Domain&body=Title%3A%20IQE-CLIP%3A%20Instance-aware%20Query%20Embedding%20for%20Zero-/Few-shot%20Anomaly%0A%20%20Detection%20in%20Medical%20Domain%0AAuthor%3A%20Hong%20Huang%20and%20Weixiang%20Sun%20and%20Zhijian%20Wu%20and%20Jingwen%20Niu%20and%20Donghuan%20Lu%20and%20Xian%20Wu%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20significantly%0Aimproved%20performance%20in%20zero-%20and%20few-shot%20anomaly%20detection%20%28ZFSAD%29%20tasks.%0AHowever%2C%20most%20existing%20CLIP-based%20methods%20assume%20prior%20knowledge%20of%20categories%0Aand%20rely%20on%20carefully%20designed%20prompts%20tailored%20to%20specific%20scenarios.%20While%0Athese%20text%20prompts%20capture%20semantic%20information%20in%20the%20textual%20space%2C%20they%0Aoften%20fail%20to%20distinguish%20normal%20and%20anomalous%20instances%20in%20the%20joint%20embedding%0Aspace.%20Moreover%2C%20most%20ZFSAD%20approaches%20focus%20on%20industrial%20domains%2C%20with%0Alimited%20exploration%20in%20medical%20tasks.%20To%20address%20these%20limitations%2C%20we%20propose%0AIQE-CLIP%2C%20a%20novel%20framework%20for%20ZFSAD%20in%20the%20medical%20domain.%20We%20show%20that%20query%0Aembeddings%20integrating%20both%20textual%20and%20instance-aware%20visual%20information%20serve%0Aas%20more%20effective%20indicators%20of%20anomalies.%20Specifically%2C%20we%20introduce%0Aclass-based%20and%20learnable%20prompting%20tokens%20to%20better%20adapt%20CLIP%20to%20the%20medical%0Asetting.%20Furthermore%2C%20we%20design%20an%20instance-aware%20query%20module%20that%20extracts%0Aregion-level%20contextual%20information%20from%20both%20modalities%2C%20enabling%20the%0Ageneration%20of%20anomaly-sensitive%20embeddings.%20Extensive%20experiments%20on%20six%0Amedical%20datasets%20demonstrate%20that%20IQE-CLIP%20achieves%20state-of-the-art%0Aperformance%20in%20both%20zero-shot%20and%20few-shot%20settings.%20Code%20and%20data%20are%0Aavailable%20at%20%5Chref%7Bhttps%3A//github.com/hongh0/IQE-CLIP/%7D%7Bthis%20https%20URL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIQE-CLIP%253A%2520Instance-aware%2520Query%2520Embedding%2520for%2520Zero-/Few-shot%2520Anomaly%250A%2520%2520Detection%2520in%2520Medical%2520Domain%26entry.906535625%3DHong%2520Huang%2520and%2520Weixiang%2520Sun%2520and%2520Zhijian%2520Wu%2520and%2520Jingwen%2520Niu%2520and%2520Donghuan%2520Lu%2520and%2520Xian%2520Wu%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520have%2520significantly%250Aimproved%2520performance%2520in%2520zero-%2520and%2520few-shot%2520anomaly%2520detection%2520%2528ZFSAD%2529%2520tasks.%250AHowever%252C%2520most%2520existing%2520CLIP-based%2520methods%2520assume%2520prior%2520knowledge%2520of%2520categories%250Aand%2520rely%2520on%2520carefully%2520designed%2520prompts%2520tailored%2520to%2520specific%2520scenarios.%2520While%250Athese%2520text%2520prompts%2520capture%2520semantic%2520information%2520in%2520the%2520textual%2520space%252C%2520they%250Aoften%2520fail%2520to%2520distinguish%2520normal%2520and%2520anomalous%2520instances%2520in%2520the%2520joint%2520embedding%250Aspace.%2520Moreover%252C%2520most%2520ZFSAD%2520approaches%2520focus%2520on%2520industrial%2520domains%252C%2520with%250Alimited%2520exploration%2520in%2520medical%2520tasks.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250AIQE-CLIP%252C%2520a%2520novel%2520framework%2520for%2520ZFSAD%2520in%2520the%2520medical%2520domain.%2520We%2520show%2520that%2520query%250Aembeddings%2520integrating%2520both%2520textual%2520and%2520instance-aware%2520visual%2520information%2520serve%250Aas%2520more%2520effective%2520indicators%2520of%2520anomalies.%2520Specifically%252C%2520we%2520introduce%250Aclass-based%2520and%2520learnable%2520prompting%2520tokens%2520to%2520better%2520adapt%2520CLIP%2520to%2520the%2520medical%250Asetting.%2520Furthermore%252C%2520we%2520design%2520an%2520instance-aware%2520query%2520module%2520that%2520extracts%250Aregion-level%2520contextual%2520information%2520from%2520both%2520modalities%252C%2520enabling%2520the%250Ageneration%2520of%2520anomaly-sensitive%2520embeddings.%2520Extensive%2520experiments%2520on%2520six%250Amedical%2520datasets%2520demonstrate%2520that%2520IQE-CLIP%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520both%2520zero-shot%2520and%2520few-shot%2520settings.%2520Code%2520and%2520data%2520are%250Aavailable%2520at%2520%255Chref%257Bhttps%253A//github.com/hongh0/IQE-CLIP/%257D%257Bthis%2520https%2520URL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IQE-CLIP%3A%20Instance-aware%20Query%20Embedding%20for%20Zero-/Few-shot%20Anomaly%0A%20%20Detection%20in%20Medical%20Domain&entry.906535625=Hong%20Huang%20and%20Weixiang%20Sun%20and%20Zhijian%20Wu%20and%20Jingwen%20Niu%20and%20Donghuan%20Lu%20and%20Xian%20Wu%20and%20Yefeng%20Zheng&entry.1292438233=%20%20Recent%20advances%20in%20vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20significantly%0Aimproved%20performance%20in%20zero-%20and%20few-shot%20anomaly%20detection%20%28ZFSAD%29%20tasks.%0AHowever%2C%20most%20existing%20CLIP-based%20methods%20assume%20prior%20knowledge%20of%20categories%0Aand%20rely%20on%20carefully%20designed%20prompts%20tailored%20to%20specific%20scenarios.%20While%0Athese%20text%20prompts%20capture%20semantic%20information%20in%20the%20textual%20space%2C%20they%0Aoften%20fail%20to%20distinguish%20normal%20and%20anomalous%20instances%20in%20the%20joint%20embedding%0Aspace.%20Moreover%2C%20most%20ZFSAD%20approaches%20focus%20on%20industrial%20domains%2C%20with%0Alimited%20exploration%20in%20medical%20tasks.%20To%20address%20these%20limitations%2C%20we%20propose%0AIQE-CLIP%2C%20a%20novel%20framework%20for%20ZFSAD%20in%20the%20medical%20domain.%20We%20show%20that%20query%0Aembeddings%20integrating%20both%20textual%20and%20instance-aware%20visual%20information%20serve%0Aas%20more%20effective%20indicators%20of%20anomalies.%20Specifically%2C%20we%20introduce%0Aclass-based%20and%20learnable%20prompting%20tokens%20to%20better%20adapt%20CLIP%20to%20the%20medical%0Asetting.%20Furthermore%2C%20we%20design%20an%20instance-aware%20query%20module%20that%20extracts%0Aregion-level%20contextual%20information%20from%20both%20modalities%2C%20enabling%20the%0Ageneration%20of%20anomaly-sensitive%20embeddings.%20Extensive%20experiments%20on%20six%0Amedical%20datasets%20demonstrate%20that%20IQE-CLIP%20achieves%20state-of-the-art%0Aperformance%20in%20both%20zero-shot%20and%20few-shot%20settings.%20Code%20and%20data%20are%0Aavailable%20at%20%5Chref%7Bhttps%3A//github.com/hongh0/IQE-CLIP/%7D%7Bthis%20https%20URL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10730v1&entry.124074799=Read"},
{"title": "Latent Action Learning Requires Supervision in the Presence of\n  Distractors", "author": "Alexander Nikulin and Ilya Zisman and Denis Tarasov and Nikita Lyubaykin and Andrei Polubarov and Igor Kiselev and Vladislav Kurenkov", "abstract": "  Recently, latent action learning, pioneered by Latent Action Policies (LAPO),\nhave shown remarkable pre-training efficiency on observation-only data,\noffering potential for leveraging vast amounts of video available on the web\nfor embodied AI. However, prior work has focused on distractor-free data, where\nchanges between observations are primarily explained by ground-truth actions.\nUnfortunately, real-world videos contain action-correlated distractors that may\nhinder latent action learning. Using Distracting Control Suite (DCS) we\nempirically investigate the effect of distractors on latent action learning and\ndemonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO\nmodification that improves the quality of latent actions by 8x, as measured by\nlinear probing. Importantly, we show that providing supervision with\nground-truth actions, as few as 2.5% of the full dataset, during latent action\nlearning improves downstream performance by 4.2x on average. Our findings\nsuggest that integrating supervision during Latent Action Models (LAM) training\nis critical in the presence of distractors, challenging the conventional\npipeline of first learning LAM and only then decoding from latent to\nground-truth actions.\n", "link": "http://arxiv.org/abs/2502.00379v5", "date": "2025-06-12", "relevancy": 2.1036, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5342}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5201}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Action%20Learning%20Requires%20Supervision%20in%20the%20Presence%20of%0A%20%20Distractors&body=Title%3A%20Latent%20Action%20Learning%20Requires%20Supervision%20in%20the%20Presence%20of%0A%20%20Distractors%0AAuthor%3A%20Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Denis%20Tarasov%20and%20Nikita%20Lyubaykin%20and%20Andrei%20Polubarov%20and%20Igor%20Kiselev%20and%20Vladislav%20Kurenkov%0AAbstract%3A%20%20%20Recently%2C%20latent%20action%20learning%2C%20pioneered%20by%20Latent%20Action%20Policies%20%28LAPO%29%2C%0Ahave%20shown%20remarkable%20pre-training%20efficiency%20on%20observation-only%20data%2C%0Aoffering%20potential%20for%20leveraging%20vast%20amounts%20of%20video%20available%20on%20the%20web%0Afor%20embodied%20AI.%20However%2C%20prior%20work%20has%20focused%20on%20distractor-free%20data%2C%20where%0Achanges%20between%20observations%20are%20primarily%20explained%20by%20ground-truth%20actions.%0AUnfortunately%2C%20real-world%20videos%20contain%20action-correlated%20distractors%20that%20may%0Ahinder%20latent%20action%20learning.%20Using%20Distracting%20Control%20Suite%20%28DCS%29%20we%0Aempirically%20investigate%20the%20effect%20of%20distractors%20on%20latent%20action%20learning%20and%0Ademonstrate%20that%20LAPO%20struggle%20in%20such%20scenario.%20We%20propose%20LAOM%2C%20a%20simple%20LAPO%0Amodification%20that%20improves%20the%20quality%20of%20latent%20actions%20by%208x%2C%20as%20measured%20by%0Alinear%20probing.%20Importantly%2C%20we%20show%20that%20providing%20supervision%20with%0Aground-truth%20actions%2C%20as%20few%20as%202.5%25%20of%20the%20full%20dataset%2C%20during%20latent%20action%0Alearning%20improves%20downstream%20performance%20by%204.2x%20on%20average.%20Our%20findings%0Asuggest%20that%20integrating%20supervision%20during%20Latent%20Action%20Models%20%28LAM%29%20training%0Ais%20critical%20in%20the%20presence%20of%20distractors%2C%20challenging%20the%20conventional%0Apipeline%20of%20first%20learning%20LAM%20and%20only%20then%20decoding%20from%20latent%20to%0Aground-truth%20actions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00379v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Action%2520Learning%2520Requires%2520Supervision%2520in%2520the%2520Presence%2520of%250A%2520%2520Distractors%26entry.906535625%3DAlexander%2520Nikulin%2520and%2520Ilya%2520Zisman%2520and%2520Denis%2520Tarasov%2520and%2520Nikita%2520Lyubaykin%2520and%2520Andrei%2520Polubarov%2520and%2520Igor%2520Kiselev%2520and%2520Vladislav%2520Kurenkov%26entry.1292438233%3D%2520%2520Recently%252C%2520latent%2520action%2520learning%252C%2520pioneered%2520by%2520Latent%2520Action%2520Policies%2520%2528LAPO%2529%252C%250Ahave%2520shown%2520remarkable%2520pre-training%2520efficiency%2520on%2520observation-only%2520data%252C%250Aoffering%2520potential%2520for%2520leveraging%2520vast%2520amounts%2520of%2520video%2520available%2520on%2520the%2520web%250Afor%2520embodied%2520AI.%2520However%252C%2520prior%2520work%2520has%2520focused%2520on%2520distractor-free%2520data%252C%2520where%250Achanges%2520between%2520observations%2520are%2520primarily%2520explained%2520by%2520ground-truth%2520actions.%250AUnfortunately%252C%2520real-world%2520videos%2520contain%2520action-correlated%2520distractors%2520that%2520may%250Ahinder%2520latent%2520action%2520learning.%2520Using%2520Distracting%2520Control%2520Suite%2520%2528DCS%2529%2520we%250Aempirically%2520investigate%2520the%2520effect%2520of%2520distractors%2520on%2520latent%2520action%2520learning%2520and%250Ademonstrate%2520that%2520LAPO%2520struggle%2520in%2520such%2520scenario.%2520We%2520propose%2520LAOM%252C%2520a%2520simple%2520LAPO%250Amodification%2520that%2520improves%2520the%2520quality%2520of%2520latent%2520actions%2520by%25208x%252C%2520as%2520measured%2520by%250Alinear%2520probing.%2520Importantly%252C%2520we%2520show%2520that%2520providing%2520supervision%2520with%250Aground-truth%2520actions%252C%2520as%2520few%2520as%25202.5%2525%2520of%2520the%2520full%2520dataset%252C%2520during%2520latent%2520action%250Alearning%2520improves%2520downstream%2520performance%2520by%25204.2x%2520on%2520average.%2520Our%2520findings%250Asuggest%2520that%2520integrating%2520supervision%2520during%2520Latent%2520Action%2520Models%2520%2528LAM%2529%2520training%250Ais%2520critical%2520in%2520the%2520presence%2520of%2520distractors%252C%2520challenging%2520the%2520conventional%250Apipeline%2520of%2520first%2520learning%2520LAM%2520and%2520only%2520then%2520decoding%2520from%2520latent%2520to%250Aground-truth%2520actions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00379v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Action%20Learning%20Requires%20Supervision%20in%20the%20Presence%20of%0A%20%20Distractors&entry.906535625=Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Denis%20Tarasov%20and%20Nikita%20Lyubaykin%20and%20Andrei%20Polubarov%20and%20Igor%20Kiselev%20and%20Vladislav%20Kurenkov&entry.1292438233=%20%20Recently%2C%20latent%20action%20learning%2C%20pioneered%20by%20Latent%20Action%20Policies%20%28LAPO%29%2C%0Ahave%20shown%20remarkable%20pre-training%20efficiency%20on%20observation-only%20data%2C%0Aoffering%20potential%20for%20leveraging%20vast%20amounts%20of%20video%20available%20on%20the%20web%0Afor%20embodied%20AI.%20However%2C%20prior%20work%20has%20focused%20on%20distractor-free%20data%2C%20where%0Achanges%20between%20observations%20are%20primarily%20explained%20by%20ground-truth%20actions.%0AUnfortunately%2C%20real-world%20videos%20contain%20action-correlated%20distractors%20that%20may%0Ahinder%20latent%20action%20learning.%20Using%20Distracting%20Control%20Suite%20%28DCS%29%20we%0Aempirically%20investigate%20the%20effect%20of%20distractors%20on%20latent%20action%20learning%20and%0Ademonstrate%20that%20LAPO%20struggle%20in%20such%20scenario.%20We%20propose%20LAOM%2C%20a%20simple%20LAPO%0Amodification%20that%20improves%20the%20quality%20of%20latent%20actions%20by%208x%2C%20as%20measured%20by%0Alinear%20probing.%20Importantly%2C%20we%20show%20that%20providing%20supervision%20with%0Aground-truth%20actions%2C%20as%20few%20as%202.5%25%20of%20the%20full%20dataset%2C%20during%20latent%20action%0Alearning%20improves%20downstream%20performance%20by%204.2x%20on%20average.%20Our%20findings%0Asuggest%20that%20integrating%20supervision%20during%20Latent%20Action%20Models%20%28LAM%29%20training%0Ais%20critical%20in%20the%20presence%20of%20distractors%2C%20challenging%20the%20conventional%0Apipeline%20of%20first%20learning%20LAM%20and%20only%20then%20decoding%20from%20latent%20to%0Aground-truth%20actions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00379v5&entry.124074799=Read"},
{"title": "SlotPi: Physics-informed Object-centric Reasoning Models", "author": "Jian Li and Wan Han and Ning Lin and Yu-Liang Zhan and Ruizhi Chengze and Haining Wang and Yi Zhang and Hongsheng Liu and Zidong Wang and Fan Yu and Hao Sun", "abstract": "  Understanding and reasoning about dynamics governed by physical laws through\nvisual observation, akin to human capabilities in the real world, poses\nsignificant challenges. Currently, object-centric dynamic simulation methods,\nwhich emulate human behavior, have achieved notable progress but overlook two\ncritical aspects: 1) the integration of physical knowledge into models. Humans\ngain physical insights by observing the world and apply this knowledge to\naccurately reason about various dynamic scenarios; 2) the validation of model\nadaptability across diverse scenarios. Real-world dynamics, especially those\ninvolving fluids and objects, demand models that not only capture object\ninteractions but also simulate fluid flow characteristics. To address these\ngaps, we introduce SlotPi, a slot-based physics-informed object-centric\nreasoning model. SlotPi integrates a physical module based on Hamiltonian\nprinciples with a spatio-temporal prediction module for dynamic forecasting.\nOur experiments highlight the model's strengths in tasks such as prediction and\nVisual Question Answering (VQA) on benchmark and fluid datasets. Furthermore,\nwe have created a real-world dataset encompassing object interactions, fluid\ndynamics, and fluid-object interactions, on which we validated our model's\ncapabilities. The model's robust performance across all datasets underscores\nits strong adaptability, laying a foundation for developing more advanced world\nmodels.\n", "link": "http://arxiv.org/abs/2506.10778v1", "date": "2025-06-12", "relevancy": 2.0958, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5756}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlotPi%3A%20Physics-informed%20Object-centric%20Reasoning%20Models&body=Title%3A%20SlotPi%3A%20Physics-informed%20Object-centric%20Reasoning%20Models%0AAuthor%3A%20Jian%20Li%20and%20Wan%20Han%20and%20Ning%20Lin%20and%20Yu-Liang%20Zhan%20and%20Ruizhi%20Chengze%20and%20Haining%20Wang%20and%20Yi%20Zhang%20and%20Hongsheng%20Liu%20and%20Zidong%20Wang%20and%20Fan%20Yu%20and%20Hao%20Sun%0AAbstract%3A%20%20%20Understanding%20and%20reasoning%20about%20dynamics%20governed%20by%20physical%20laws%20through%0Avisual%20observation%2C%20akin%20to%20human%20capabilities%20in%20the%20real%20world%2C%20poses%0Asignificant%20challenges.%20Currently%2C%20object-centric%20dynamic%20simulation%20methods%2C%0Awhich%20emulate%20human%20behavior%2C%20have%20achieved%20notable%20progress%20but%20overlook%20two%0Acritical%20aspects%3A%201%29%20the%20integration%20of%20physical%20knowledge%20into%20models.%20Humans%0Again%20physical%20insights%20by%20observing%20the%20world%20and%20apply%20this%20knowledge%20to%0Aaccurately%20reason%20about%20various%20dynamic%20scenarios%3B%202%29%20the%20validation%20of%20model%0Aadaptability%20across%20diverse%20scenarios.%20Real-world%20dynamics%2C%20especially%20those%0Ainvolving%20fluids%20and%20objects%2C%20demand%20models%20that%20not%20only%20capture%20object%0Ainteractions%20but%20also%20simulate%20fluid%20flow%20characteristics.%20To%20address%20these%0Agaps%2C%20we%20introduce%20SlotPi%2C%20a%20slot-based%20physics-informed%20object-centric%0Areasoning%20model.%20SlotPi%20integrates%20a%20physical%20module%20based%20on%20Hamiltonian%0Aprinciples%20with%20a%20spatio-temporal%20prediction%20module%20for%20dynamic%20forecasting.%0AOur%20experiments%20highlight%20the%20model%27s%20strengths%20in%20tasks%20such%20as%20prediction%20and%0AVisual%20Question%20Answering%20%28VQA%29%20on%20benchmark%20and%20fluid%20datasets.%20Furthermore%2C%0Awe%20have%20created%20a%20real-world%20dataset%20encompassing%20object%20interactions%2C%20fluid%0Adynamics%2C%20and%20fluid-object%20interactions%2C%20on%20which%20we%20validated%20our%20model%27s%0Acapabilities.%20The%20model%27s%20robust%20performance%20across%20all%20datasets%20underscores%0Aits%20strong%20adaptability%2C%20laying%20a%20foundation%20for%20developing%20more%20advanced%20world%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlotPi%253A%2520Physics-informed%2520Object-centric%2520Reasoning%2520Models%26entry.906535625%3DJian%2520Li%2520and%2520Wan%2520Han%2520and%2520Ning%2520Lin%2520and%2520Yu-Liang%2520Zhan%2520and%2520Ruizhi%2520Chengze%2520and%2520Haining%2520Wang%2520and%2520Yi%2520Zhang%2520and%2520Hongsheng%2520Liu%2520and%2520Zidong%2520Wang%2520and%2520Fan%2520Yu%2520and%2520Hao%2520Sun%26entry.1292438233%3D%2520%2520Understanding%2520and%2520reasoning%2520about%2520dynamics%2520governed%2520by%2520physical%2520laws%2520through%250Avisual%2520observation%252C%2520akin%2520to%2520human%2520capabilities%2520in%2520the%2520real%2520world%252C%2520poses%250Asignificant%2520challenges.%2520Currently%252C%2520object-centric%2520dynamic%2520simulation%2520methods%252C%250Awhich%2520emulate%2520human%2520behavior%252C%2520have%2520achieved%2520notable%2520progress%2520but%2520overlook%2520two%250Acritical%2520aspects%253A%25201%2529%2520the%2520integration%2520of%2520physical%2520knowledge%2520into%2520models.%2520Humans%250Again%2520physical%2520insights%2520by%2520observing%2520the%2520world%2520and%2520apply%2520this%2520knowledge%2520to%250Aaccurately%2520reason%2520about%2520various%2520dynamic%2520scenarios%253B%25202%2529%2520the%2520validation%2520of%2520model%250Aadaptability%2520across%2520diverse%2520scenarios.%2520Real-world%2520dynamics%252C%2520especially%2520those%250Ainvolving%2520fluids%2520and%2520objects%252C%2520demand%2520models%2520that%2520not%2520only%2520capture%2520object%250Ainteractions%2520but%2520also%2520simulate%2520fluid%2520flow%2520characteristics.%2520To%2520address%2520these%250Agaps%252C%2520we%2520introduce%2520SlotPi%252C%2520a%2520slot-based%2520physics-informed%2520object-centric%250Areasoning%2520model.%2520SlotPi%2520integrates%2520a%2520physical%2520module%2520based%2520on%2520Hamiltonian%250Aprinciples%2520with%2520a%2520spatio-temporal%2520prediction%2520module%2520for%2520dynamic%2520forecasting.%250AOur%2520experiments%2520highlight%2520the%2520model%2527s%2520strengths%2520in%2520tasks%2520such%2520as%2520prediction%2520and%250AVisual%2520Question%2520Answering%2520%2528VQA%2529%2520on%2520benchmark%2520and%2520fluid%2520datasets.%2520Furthermore%252C%250Awe%2520have%2520created%2520a%2520real-world%2520dataset%2520encompassing%2520object%2520interactions%252C%2520fluid%250Adynamics%252C%2520and%2520fluid-object%2520interactions%252C%2520on%2520which%2520we%2520validated%2520our%2520model%2527s%250Acapabilities.%2520The%2520model%2527s%2520robust%2520performance%2520across%2520all%2520datasets%2520underscores%250Aits%2520strong%2520adaptability%252C%2520laying%2520a%2520foundation%2520for%2520developing%2520more%2520advanced%2520world%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlotPi%3A%20Physics-informed%20Object-centric%20Reasoning%20Models&entry.906535625=Jian%20Li%20and%20Wan%20Han%20and%20Ning%20Lin%20and%20Yu-Liang%20Zhan%20and%20Ruizhi%20Chengze%20and%20Haining%20Wang%20and%20Yi%20Zhang%20and%20Hongsheng%20Liu%20and%20Zidong%20Wang%20and%20Fan%20Yu%20and%20Hao%20Sun&entry.1292438233=%20%20Understanding%20and%20reasoning%20about%20dynamics%20governed%20by%20physical%20laws%20through%0Avisual%20observation%2C%20akin%20to%20human%20capabilities%20in%20the%20real%20world%2C%20poses%0Asignificant%20challenges.%20Currently%2C%20object-centric%20dynamic%20simulation%20methods%2C%0Awhich%20emulate%20human%20behavior%2C%20have%20achieved%20notable%20progress%20but%20overlook%20two%0Acritical%20aspects%3A%201%29%20the%20integration%20of%20physical%20knowledge%20into%20models.%20Humans%0Again%20physical%20insights%20by%20observing%20the%20world%20and%20apply%20this%20knowledge%20to%0Aaccurately%20reason%20about%20various%20dynamic%20scenarios%3B%202%29%20the%20validation%20of%20model%0Aadaptability%20across%20diverse%20scenarios.%20Real-world%20dynamics%2C%20especially%20those%0Ainvolving%20fluids%20and%20objects%2C%20demand%20models%20that%20not%20only%20capture%20object%0Ainteractions%20but%20also%20simulate%20fluid%20flow%20characteristics.%20To%20address%20these%0Agaps%2C%20we%20introduce%20SlotPi%2C%20a%20slot-based%20physics-informed%20object-centric%0Areasoning%20model.%20SlotPi%20integrates%20a%20physical%20module%20based%20on%20Hamiltonian%0Aprinciples%20with%20a%20spatio-temporal%20prediction%20module%20for%20dynamic%20forecasting.%0AOur%20experiments%20highlight%20the%20model%27s%20strengths%20in%20tasks%20such%20as%20prediction%20and%0AVisual%20Question%20Answering%20%28VQA%29%20on%20benchmark%20and%20fluid%20datasets.%20Furthermore%2C%0Awe%20have%20created%20a%20real-world%20dataset%20encompassing%20object%20interactions%2C%20fluid%0Adynamics%2C%20and%20fluid-object%20interactions%2C%20on%20which%20we%20validated%20our%20model%27s%0Acapabilities.%20The%20model%27s%20robust%20performance%20across%20all%20datasets%20underscores%0Aits%20strong%20adaptability%2C%20laying%20a%20foundation%20for%20developing%20more%20advanced%20world%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10778v1&entry.124074799=Read"},
{"title": "Pretraining Generative Flow Networks with Inexpensive Rewards for\n  Molecular Graph Generation", "author": "Mohit Pandey and Gopeshh Subbaraj and Artem Cherkasov and Martin Ester and Emmanuel Bengio", "abstract": "  Generative Flow Networks (GFlowNets) have recently emerged as a suitable\nframework for generating diverse and high-quality molecular structures by\nlearning from rewards treated as unnormalized distributions. Previous works in\nthis framework often restrict exploration by using predefined molecular\nfragments as building blocks, limiting the chemical space that can be accessed.\nIn this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative\nmodel leveraging individual atoms as building blocks to explore drug-like\nchemical space more comprehensively. We propose an unsupervised pre-training\napproach using drug-like molecule datasets, which teaches A-GFNs about\ninexpensive yet informative molecular descriptors such as drug-likeliness,\ntopological polar surface area, and synthetic accessibility scores. These\nproperties serve as proxy rewards, guiding A-GFNs towards regions of chemical\nspace that exhibit desirable pharmacological properties. We further implement a\ngoal-conditioned finetuning process, which adapts A-GFNs to optimize for\nspecific target properties. In this work, we pretrain A-GFN on a subset of ZINC\ndataset, and by employing robust evaluation metrics we show the effectiveness\nof our approach when compared to other relevant baseline methods for a wide\nrange of drug design tasks. The code is accessible at\nhttps://github.com/diamondspark/AGFN.\n", "link": "http://arxiv.org/abs/2503.06337v4", "date": "2025-06-12", "relevancy": 2.0912, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5358}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5284}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretraining%20Generative%20Flow%20Networks%20with%20Inexpensive%20Rewards%20for%0A%20%20Molecular%20Graph%20Generation&body=Title%3A%20Pretraining%20Generative%20Flow%20Networks%20with%20Inexpensive%20Rewards%20for%0A%20%20Molecular%20Graph%20Generation%0AAuthor%3A%20Mohit%20Pandey%20and%20Gopeshh%20Subbaraj%20and%20Artem%20Cherkasov%20and%20Martin%20Ester%20and%20Emmanuel%20Bengio%0AAbstract%3A%20%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20have%20recently%20emerged%20as%20a%20suitable%0Aframework%20for%20generating%20diverse%20and%20high-quality%20molecular%20structures%20by%0Alearning%20from%20rewards%20treated%20as%20unnormalized%20distributions.%20Previous%20works%20in%0Athis%20framework%20often%20restrict%20exploration%20by%20using%20predefined%20molecular%0Afragments%20as%20building%20blocks%2C%20limiting%20the%20chemical%20space%20that%20can%20be%20accessed.%0AIn%20this%20work%2C%20we%20introduce%20Atomic%20GFlowNets%20%28A-GFNs%29%2C%20a%20foundational%20generative%0Amodel%20leveraging%20individual%20atoms%20as%20building%20blocks%20to%20explore%20drug-like%0Achemical%20space%20more%20comprehensively.%20We%20propose%20an%20unsupervised%20pre-training%0Aapproach%20using%20drug-like%20molecule%20datasets%2C%20which%20teaches%20A-GFNs%20about%0Ainexpensive%20yet%20informative%20molecular%20descriptors%20such%20as%20drug-likeliness%2C%0Atopological%20polar%20surface%20area%2C%20and%20synthetic%20accessibility%20scores.%20These%0Aproperties%20serve%20as%20proxy%20rewards%2C%20guiding%20A-GFNs%20towards%20regions%20of%20chemical%0Aspace%20that%20exhibit%20desirable%20pharmacological%20properties.%20We%20further%20implement%20a%0Agoal-conditioned%20finetuning%20process%2C%20which%20adapts%20A-GFNs%20to%20optimize%20for%0Aspecific%20target%20properties.%20In%20this%20work%2C%20we%20pretrain%20A-GFN%20on%20a%20subset%20of%20ZINC%0Adataset%2C%20and%20by%20employing%20robust%20evaluation%20metrics%20we%20show%20the%20effectiveness%0Aof%20our%20approach%20when%20compared%20to%20other%20relevant%20baseline%20methods%20for%20a%20wide%0Arange%20of%20drug%20design%20tasks.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/diamondspark/AGFN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06337v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretraining%2520Generative%2520Flow%2520Networks%2520with%2520Inexpensive%2520Rewards%2520for%250A%2520%2520Molecular%2520Graph%2520Generation%26entry.906535625%3DMohit%2520Pandey%2520and%2520Gopeshh%2520Subbaraj%2520and%2520Artem%2520Cherkasov%2520and%2520Martin%2520Ester%2520and%2520Emmanuel%2520Bengio%26entry.1292438233%3D%2520%2520Generative%2520Flow%2520Networks%2520%2528GFlowNets%2529%2520have%2520recently%2520emerged%2520as%2520a%2520suitable%250Aframework%2520for%2520generating%2520diverse%2520and%2520high-quality%2520molecular%2520structures%2520by%250Alearning%2520from%2520rewards%2520treated%2520as%2520unnormalized%2520distributions.%2520Previous%2520works%2520in%250Athis%2520framework%2520often%2520restrict%2520exploration%2520by%2520using%2520predefined%2520molecular%250Afragments%2520as%2520building%2520blocks%252C%2520limiting%2520the%2520chemical%2520space%2520that%2520can%2520be%2520accessed.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520Atomic%2520GFlowNets%2520%2528A-GFNs%2529%252C%2520a%2520foundational%2520generative%250Amodel%2520leveraging%2520individual%2520atoms%2520as%2520building%2520blocks%2520to%2520explore%2520drug-like%250Achemical%2520space%2520more%2520comprehensively.%2520We%2520propose%2520an%2520unsupervised%2520pre-training%250Aapproach%2520using%2520drug-like%2520molecule%2520datasets%252C%2520which%2520teaches%2520A-GFNs%2520about%250Ainexpensive%2520yet%2520informative%2520molecular%2520descriptors%2520such%2520as%2520drug-likeliness%252C%250Atopological%2520polar%2520surface%2520area%252C%2520and%2520synthetic%2520accessibility%2520scores.%2520These%250Aproperties%2520serve%2520as%2520proxy%2520rewards%252C%2520guiding%2520A-GFNs%2520towards%2520regions%2520of%2520chemical%250Aspace%2520that%2520exhibit%2520desirable%2520pharmacological%2520properties.%2520We%2520further%2520implement%2520a%250Agoal-conditioned%2520finetuning%2520process%252C%2520which%2520adapts%2520A-GFNs%2520to%2520optimize%2520for%250Aspecific%2520target%2520properties.%2520In%2520this%2520work%252C%2520we%2520pretrain%2520A-GFN%2520on%2520a%2520subset%2520of%2520ZINC%250Adataset%252C%2520and%2520by%2520employing%2520robust%2520evaluation%2520metrics%2520we%2520show%2520the%2520effectiveness%250Aof%2520our%2520approach%2520when%2520compared%2520to%2520other%2520relevant%2520baseline%2520methods%2520for%2520a%2520wide%250Arange%2520of%2520drug%2520design%2520tasks.%2520The%2520code%2520is%2520accessible%2520at%250Ahttps%253A//github.com/diamondspark/AGFN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06337v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretraining%20Generative%20Flow%20Networks%20with%20Inexpensive%20Rewards%20for%0A%20%20Molecular%20Graph%20Generation&entry.906535625=Mohit%20Pandey%20and%20Gopeshh%20Subbaraj%20and%20Artem%20Cherkasov%20and%20Martin%20Ester%20and%20Emmanuel%20Bengio&entry.1292438233=%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20have%20recently%20emerged%20as%20a%20suitable%0Aframework%20for%20generating%20diverse%20and%20high-quality%20molecular%20structures%20by%0Alearning%20from%20rewards%20treated%20as%20unnormalized%20distributions.%20Previous%20works%20in%0Athis%20framework%20often%20restrict%20exploration%20by%20using%20predefined%20molecular%0Afragments%20as%20building%20blocks%2C%20limiting%20the%20chemical%20space%20that%20can%20be%20accessed.%0AIn%20this%20work%2C%20we%20introduce%20Atomic%20GFlowNets%20%28A-GFNs%29%2C%20a%20foundational%20generative%0Amodel%20leveraging%20individual%20atoms%20as%20building%20blocks%20to%20explore%20drug-like%0Achemical%20space%20more%20comprehensively.%20We%20propose%20an%20unsupervised%20pre-training%0Aapproach%20using%20drug-like%20molecule%20datasets%2C%20which%20teaches%20A-GFNs%20about%0Ainexpensive%20yet%20informative%20molecular%20descriptors%20such%20as%20drug-likeliness%2C%0Atopological%20polar%20surface%20area%2C%20and%20synthetic%20accessibility%20scores.%20These%0Aproperties%20serve%20as%20proxy%20rewards%2C%20guiding%20A-GFNs%20towards%20regions%20of%20chemical%0Aspace%20that%20exhibit%20desirable%20pharmacological%20properties.%20We%20further%20implement%20a%0Agoal-conditioned%20finetuning%20process%2C%20which%20adapts%20A-GFNs%20to%20optimize%20for%0Aspecific%20target%20properties.%20In%20this%20work%2C%20we%20pretrain%20A-GFN%20on%20a%20subset%20of%20ZINC%0Adataset%2C%20and%20by%20employing%20robust%20evaluation%20metrics%20we%20show%20the%20effectiveness%0Aof%20our%20approach%20when%20compared%20to%20other%20relevant%20baseline%20methods%20for%20a%20wide%0Arange%20of%20drug%20design%20tasks.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/diamondspark/AGFN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06337v4&entry.124074799=Read"},
{"title": "Learning richness modulates equality reasoning in neural networks", "author": "William L. Tong and Cengiz Pehlevan", "abstract": "  Equality reasoning is ubiquitous and purely abstract: sameness or difference\nmay be evaluated no matter the nature of the underlying objects. As a result,\nsame-different (SD) tasks have been extensively studied as a starting point for\nunderstanding abstract reasoning in humans and across animal species. With the\nrise of neural networks that exhibit striking apparent proficiency for\nabstractions, equality reasoning in these models has also gained interest. Yet\ndespite extensive study, conclusions about equality reasoning vary widely and\nwith little consensus. To clarify the underlying principles in learning SD\ntasks, we develop a theory of equality reasoning in multi-layer perceptrons\n(MLP). Following observations in comparative psychology, we propose a spectrum\nof behavior that ranges from conceptual to perceptual outcomes. Conceptual\nbehavior is characterized by task-specific representations, efficient learning,\nand insensitivity to spurious perceptual details. Perceptual behavior is\ncharacterized by strong sensitivity to spurious perceptual details, accompanied\nby the need for exhaustive training to learn the task. We develop a\nmathematical theory to show that an MLP's behavior is driven by learning\nrichness. Rich-regime MLPs exhibit conceptual behavior, whereas lazy-regime\nMLPs exhibit perceptual behavior. We validate our theoretical findings in\nvision SD experiments, showing that rich feature learning promotes success by\nencouraging hallmarks of conceptual behavior. Overall, our work identifies\nfeature learning richness as a key parameter modulating equality reasoning, and\nsuggests that equality reasoning in humans and animals may similarly depend on\nlearning richness in neural circuits.\n", "link": "http://arxiv.org/abs/2503.09781v2", "date": "2025-06-12", "relevancy": 2.0904, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20richness%20modulates%20equality%20reasoning%20in%20neural%20networks&body=Title%3A%20Learning%20richness%20modulates%20equality%20reasoning%20in%20neural%20networks%0AAuthor%3A%20William%20L.%20Tong%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20Equality%20reasoning%20is%20ubiquitous%20and%20purely%20abstract%3A%20sameness%20or%20difference%0Amay%20be%20evaluated%20no%20matter%20the%20nature%20of%20the%20underlying%20objects.%20As%20a%20result%2C%0Asame-different%20%28SD%29%20tasks%20have%20been%20extensively%20studied%20as%20a%20starting%20point%20for%0Aunderstanding%20abstract%20reasoning%20in%20humans%20and%20across%20animal%20species.%20With%20the%0Arise%20of%20neural%20networks%20that%20exhibit%20striking%20apparent%20proficiency%20for%0Aabstractions%2C%20equality%20reasoning%20in%20these%20models%20has%20also%20gained%20interest.%20Yet%0Adespite%20extensive%20study%2C%20conclusions%20about%20equality%20reasoning%20vary%20widely%20and%0Awith%20little%20consensus.%20To%20clarify%20the%20underlying%20principles%20in%20learning%20SD%0Atasks%2C%20we%20develop%20a%20theory%20of%20equality%20reasoning%20in%20multi-layer%20perceptrons%0A%28MLP%29.%20Following%20observations%20in%20comparative%20psychology%2C%20we%20propose%20a%20spectrum%0Aof%20behavior%20that%20ranges%20from%20conceptual%20to%20perceptual%20outcomes.%20Conceptual%0Abehavior%20is%20characterized%20by%20task-specific%20representations%2C%20efficient%20learning%2C%0Aand%20insensitivity%20to%20spurious%20perceptual%20details.%20Perceptual%20behavior%20is%0Acharacterized%20by%20strong%20sensitivity%20to%20spurious%20perceptual%20details%2C%20accompanied%0Aby%20the%20need%20for%20exhaustive%20training%20to%20learn%20the%20task.%20We%20develop%20a%0Amathematical%20theory%20to%20show%20that%20an%20MLP%27s%20behavior%20is%20driven%20by%20learning%0Arichness.%20Rich-regime%20MLPs%20exhibit%20conceptual%20behavior%2C%20whereas%20lazy-regime%0AMLPs%20exhibit%20perceptual%20behavior.%20We%20validate%20our%20theoretical%20findings%20in%0Avision%20SD%20experiments%2C%20showing%20that%20rich%20feature%20learning%20promotes%20success%20by%0Aencouraging%20hallmarks%20of%20conceptual%20behavior.%20Overall%2C%20our%20work%20identifies%0Afeature%20learning%20richness%20as%20a%20key%20parameter%20modulating%20equality%20reasoning%2C%20and%0Asuggests%20that%20equality%20reasoning%20in%20humans%20and%20animals%20may%20similarly%20depend%20on%0Alearning%20richness%20in%20neural%20circuits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09781v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520richness%2520modulates%2520equality%2520reasoning%2520in%2520neural%2520networks%26entry.906535625%3DWilliam%2520L.%2520Tong%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520Equality%2520reasoning%2520is%2520ubiquitous%2520and%2520purely%2520abstract%253A%2520sameness%2520or%2520difference%250Amay%2520be%2520evaluated%2520no%2520matter%2520the%2520nature%2520of%2520the%2520underlying%2520objects.%2520As%2520a%2520result%252C%250Asame-different%2520%2528SD%2529%2520tasks%2520have%2520been%2520extensively%2520studied%2520as%2520a%2520starting%2520point%2520for%250Aunderstanding%2520abstract%2520reasoning%2520in%2520humans%2520and%2520across%2520animal%2520species.%2520With%2520the%250Arise%2520of%2520neural%2520networks%2520that%2520exhibit%2520striking%2520apparent%2520proficiency%2520for%250Aabstractions%252C%2520equality%2520reasoning%2520in%2520these%2520models%2520has%2520also%2520gained%2520interest.%2520Yet%250Adespite%2520extensive%2520study%252C%2520conclusions%2520about%2520equality%2520reasoning%2520vary%2520widely%2520and%250Awith%2520little%2520consensus.%2520To%2520clarify%2520the%2520underlying%2520principles%2520in%2520learning%2520SD%250Atasks%252C%2520we%2520develop%2520a%2520theory%2520of%2520equality%2520reasoning%2520in%2520multi-layer%2520perceptrons%250A%2528MLP%2529.%2520Following%2520observations%2520in%2520comparative%2520psychology%252C%2520we%2520propose%2520a%2520spectrum%250Aof%2520behavior%2520that%2520ranges%2520from%2520conceptual%2520to%2520perceptual%2520outcomes.%2520Conceptual%250Abehavior%2520is%2520characterized%2520by%2520task-specific%2520representations%252C%2520efficient%2520learning%252C%250Aand%2520insensitivity%2520to%2520spurious%2520perceptual%2520details.%2520Perceptual%2520behavior%2520is%250Acharacterized%2520by%2520strong%2520sensitivity%2520to%2520spurious%2520perceptual%2520details%252C%2520accompanied%250Aby%2520the%2520need%2520for%2520exhaustive%2520training%2520to%2520learn%2520the%2520task.%2520We%2520develop%2520a%250Amathematical%2520theory%2520to%2520show%2520that%2520an%2520MLP%2527s%2520behavior%2520is%2520driven%2520by%2520learning%250Arichness.%2520Rich-regime%2520MLPs%2520exhibit%2520conceptual%2520behavior%252C%2520whereas%2520lazy-regime%250AMLPs%2520exhibit%2520perceptual%2520behavior.%2520We%2520validate%2520our%2520theoretical%2520findings%2520in%250Avision%2520SD%2520experiments%252C%2520showing%2520that%2520rich%2520feature%2520learning%2520promotes%2520success%2520by%250Aencouraging%2520hallmarks%2520of%2520conceptual%2520behavior.%2520Overall%252C%2520our%2520work%2520identifies%250Afeature%2520learning%2520richness%2520as%2520a%2520key%2520parameter%2520modulating%2520equality%2520reasoning%252C%2520and%250Asuggests%2520that%2520equality%2520reasoning%2520in%2520humans%2520and%2520animals%2520may%2520similarly%2520depend%2520on%250Alearning%2520richness%2520in%2520neural%2520circuits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09781v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20richness%20modulates%20equality%20reasoning%20in%20neural%20networks&entry.906535625=William%20L.%20Tong%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20Equality%20reasoning%20is%20ubiquitous%20and%20purely%20abstract%3A%20sameness%20or%20difference%0Amay%20be%20evaluated%20no%20matter%20the%20nature%20of%20the%20underlying%20objects.%20As%20a%20result%2C%0Asame-different%20%28SD%29%20tasks%20have%20been%20extensively%20studied%20as%20a%20starting%20point%20for%0Aunderstanding%20abstract%20reasoning%20in%20humans%20and%20across%20animal%20species.%20With%20the%0Arise%20of%20neural%20networks%20that%20exhibit%20striking%20apparent%20proficiency%20for%0Aabstractions%2C%20equality%20reasoning%20in%20these%20models%20has%20also%20gained%20interest.%20Yet%0Adespite%20extensive%20study%2C%20conclusions%20about%20equality%20reasoning%20vary%20widely%20and%0Awith%20little%20consensus.%20To%20clarify%20the%20underlying%20principles%20in%20learning%20SD%0Atasks%2C%20we%20develop%20a%20theory%20of%20equality%20reasoning%20in%20multi-layer%20perceptrons%0A%28MLP%29.%20Following%20observations%20in%20comparative%20psychology%2C%20we%20propose%20a%20spectrum%0Aof%20behavior%20that%20ranges%20from%20conceptual%20to%20perceptual%20outcomes.%20Conceptual%0Abehavior%20is%20characterized%20by%20task-specific%20representations%2C%20efficient%20learning%2C%0Aand%20insensitivity%20to%20spurious%20perceptual%20details.%20Perceptual%20behavior%20is%0Acharacterized%20by%20strong%20sensitivity%20to%20spurious%20perceptual%20details%2C%20accompanied%0Aby%20the%20need%20for%20exhaustive%20training%20to%20learn%20the%20task.%20We%20develop%20a%0Amathematical%20theory%20to%20show%20that%20an%20MLP%27s%20behavior%20is%20driven%20by%20learning%0Arichness.%20Rich-regime%20MLPs%20exhibit%20conceptual%20behavior%2C%20whereas%20lazy-regime%0AMLPs%20exhibit%20perceptual%20behavior.%20We%20validate%20our%20theoretical%20findings%20in%0Avision%20SD%20experiments%2C%20showing%20that%20rich%20feature%20learning%20promotes%20success%20by%0Aencouraging%20hallmarks%20of%20conceptual%20behavior.%20Overall%2C%20our%20work%20identifies%0Afeature%20learning%20richness%20as%20a%20key%20parameter%20modulating%20equality%20reasoning%2C%20and%0Asuggests%20that%20equality%20reasoning%20in%20humans%20and%20animals%20may%20similarly%20depend%20on%0Alearning%20richness%20in%20neural%20circuits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09781v2&entry.124074799=Read"},
{"title": "GUARD: Guided Unlearning and Retention via Data Attribution for Large\n  Language Models", "author": "Evelyn Ma and Duo Zhou and Peizhi Niu and Huiting Zhou and Huan Zhang and Olgica Milenkovic and S. Rasoul Etesami", "abstract": "  Unlearning in large language models (LLMs) is becoming increasingly important\ndue to regulatory compliance, copyright protection, and privacy concerns.\nHowever, a key challenge in LLM unlearning is unintended forgetting, where the\nremoval of specific data inadvertently impairs the utility of the model and its\nretention of valuable, desired information. While prior work has primarily\nfocused on architectural innovations, the influence of data-level factors on\nunlearning performance remains underexplored. As a result, existing methods\noften suffer from degraded retention when forgetting high-impact data. To\naddress this, we propose GUARD-a novel framework for Guided Unlearning And\nRetention via Data attribution. At its core, GUARD introduces a lightweight\nproxy data attribution metric tailored for LLM unlearning, which quantifies the\n\"alignment\" between the forget and retain sets while remaining computationally\nefficient. Building on this, we design a novel unlearning objective that\nassigns adaptive, nonuniform unlearning weights to samples, inversely\nproportional to their proxy attribution scores. Through such a reallocation of\nunlearning power, GUARD mitigates unintended losses in retention. We provide\nrigorous theoretical guarantees that GUARD significantly enhances retention\nwhile maintaining forgetting metrics comparable to prior methods. Extensive\nexperiments on the TOFU benchmark across multiple LLM architectures demonstrate\nthat GUARD substantially improves utility preservation while ensuring effective\nunlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to\n194.92% in terms of Truth Ratio when forgetting 10% of the training data.\n", "link": "http://arxiv.org/abs/2506.10946v1", "date": "2025-06-12", "relevancy": 2.0738, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5521}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5238}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUARD%3A%20Guided%20Unlearning%20and%20Retention%20via%20Data%20Attribution%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20GUARD%3A%20Guided%20Unlearning%20and%20Retention%20via%20Data%20Attribution%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Evelyn%20Ma%20and%20Duo%20Zhou%20and%20Peizhi%20Niu%20and%20Huiting%20Zhou%20and%20Huan%20Zhang%20and%20Olgica%20Milenkovic%20and%20S.%20Rasoul%20Etesami%0AAbstract%3A%20%20%20Unlearning%20in%20large%20language%20models%20%28LLMs%29%20is%20becoming%20increasingly%20important%0Adue%20to%20regulatory%20compliance%2C%20copyright%20protection%2C%20and%20privacy%20concerns.%0AHowever%2C%20a%20key%20challenge%20in%20LLM%20unlearning%20is%20unintended%20forgetting%2C%20where%20the%0Aremoval%20of%20specific%20data%20inadvertently%20impairs%20the%20utility%20of%20the%20model%20and%20its%0Aretention%20of%20valuable%2C%20desired%20information.%20While%20prior%20work%20has%20primarily%0Afocused%20on%20architectural%20innovations%2C%20the%20influence%20of%20data-level%20factors%20on%0Aunlearning%20performance%20remains%20underexplored.%20As%20a%20result%2C%20existing%20methods%0Aoften%20suffer%20from%20degraded%20retention%20when%20forgetting%20high-impact%20data.%20To%0Aaddress%20this%2C%20we%20propose%20GUARD-a%20novel%20framework%20for%20Guided%20Unlearning%20And%0ARetention%20via%20Data%20attribution.%20At%20its%20core%2C%20GUARD%20introduces%20a%20lightweight%0Aproxy%20data%20attribution%20metric%20tailored%20for%20LLM%20unlearning%2C%20which%20quantifies%20the%0A%22alignment%22%20between%20the%20forget%20and%20retain%20sets%20while%20remaining%20computationally%0Aefficient.%20Building%20on%20this%2C%20we%20design%20a%20novel%20unlearning%20objective%20that%0Aassigns%20adaptive%2C%20nonuniform%20unlearning%20weights%20to%20samples%2C%20inversely%0Aproportional%20to%20their%20proxy%20attribution%20scores.%20Through%20such%20a%20reallocation%20of%0Aunlearning%20power%2C%20GUARD%20mitigates%20unintended%20losses%20in%20retention.%20We%20provide%0Arigorous%20theoretical%20guarantees%20that%20GUARD%20significantly%20enhances%20retention%0Awhile%20maintaining%20forgetting%20metrics%20comparable%20to%20prior%20methods.%20Extensive%0Aexperiments%20on%20the%20TOFU%20benchmark%20across%20multiple%20LLM%20architectures%20demonstrate%0Athat%20GUARD%20substantially%20improves%20utility%20preservation%20while%20ensuring%20effective%0Aunlearning.%20Notably%2C%20GUARD%20reduces%20utility%20sacrifice%20on%20the%20Retain%20Set%20by%20up%20to%0A194.92%25%20in%20terms%20of%20Truth%20Ratio%20when%20forgetting%2010%25%20of%20the%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUARD%253A%2520Guided%2520Unlearning%2520and%2520Retention%2520via%2520Data%2520Attribution%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DEvelyn%2520Ma%2520and%2520Duo%2520Zhou%2520and%2520Peizhi%2520Niu%2520and%2520Huiting%2520Zhou%2520and%2520Huan%2520Zhang%2520and%2520Olgica%2520Milenkovic%2520and%2520S.%2520Rasoul%2520Etesami%26entry.1292438233%3D%2520%2520Unlearning%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520becoming%2520increasingly%2520important%250Adue%2520to%2520regulatory%2520compliance%252C%2520copyright%2520protection%252C%2520and%2520privacy%2520concerns.%250AHowever%252C%2520a%2520key%2520challenge%2520in%2520LLM%2520unlearning%2520is%2520unintended%2520forgetting%252C%2520where%2520the%250Aremoval%2520of%2520specific%2520data%2520inadvertently%2520impairs%2520the%2520utility%2520of%2520the%2520model%2520and%2520its%250Aretention%2520of%2520valuable%252C%2520desired%2520information.%2520While%2520prior%2520work%2520has%2520primarily%250Afocused%2520on%2520architectural%2520innovations%252C%2520the%2520influence%2520of%2520data-level%2520factors%2520on%250Aunlearning%2520performance%2520remains%2520underexplored.%2520As%2520a%2520result%252C%2520existing%2520methods%250Aoften%2520suffer%2520from%2520degraded%2520retention%2520when%2520forgetting%2520high-impact%2520data.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520GUARD-a%2520novel%2520framework%2520for%2520Guided%2520Unlearning%2520And%250ARetention%2520via%2520Data%2520attribution.%2520At%2520its%2520core%252C%2520GUARD%2520introduces%2520a%2520lightweight%250Aproxy%2520data%2520attribution%2520metric%2520tailored%2520for%2520LLM%2520unlearning%252C%2520which%2520quantifies%2520the%250A%2522alignment%2522%2520between%2520the%2520forget%2520and%2520retain%2520sets%2520while%2520remaining%2520computationally%250Aefficient.%2520Building%2520on%2520this%252C%2520we%2520design%2520a%2520novel%2520unlearning%2520objective%2520that%250Aassigns%2520adaptive%252C%2520nonuniform%2520unlearning%2520weights%2520to%2520samples%252C%2520inversely%250Aproportional%2520to%2520their%2520proxy%2520attribution%2520scores.%2520Through%2520such%2520a%2520reallocation%2520of%250Aunlearning%2520power%252C%2520GUARD%2520mitigates%2520unintended%2520losses%2520in%2520retention.%2520We%2520provide%250Arigorous%2520theoretical%2520guarantees%2520that%2520GUARD%2520significantly%2520enhances%2520retention%250Awhile%2520maintaining%2520forgetting%2520metrics%2520comparable%2520to%2520prior%2520methods.%2520Extensive%250Aexperiments%2520on%2520the%2520TOFU%2520benchmark%2520across%2520multiple%2520LLM%2520architectures%2520demonstrate%250Athat%2520GUARD%2520substantially%2520improves%2520utility%2520preservation%2520while%2520ensuring%2520effective%250Aunlearning.%2520Notably%252C%2520GUARD%2520reduces%2520utility%2520sacrifice%2520on%2520the%2520Retain%2520Set%2520by%2520up%2520to%250A194.92%2525%2520in%2520terms%2520of%2520Truth%2520Ratio%2520when%2520forgetting%252010%2525%2520of%2520the%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUARD%3A%20Guided%20Unlearning%20and%20Retention%20via%20Data%20Attribution%20for%20Large%0A%20%20Language%20Models&entry.906535625=Evelyn%20Ma%20and%20Duo%20Zhou%20and%20Peizhi%20Niu%20and%20Huiting%20Zhou%20and%20Huan%20Zhang%20and%20Olgica%20Milenkovic%20and%20S.%20Rasoul%20Etesami&entry.1292438233=%20%20Unlearning%20in%20large%20language%20models%20%28LLMs%29%20is%20becoming%20increasingly%20important%0Adue%20to%20regulatory%20compliance%2C%20copyright%20protection%2C%20and%20privacy%20concerns.%0AHowever%2C%20a%20key%20challenge%20in%20LLM%20unlearning%20is%20unintended%20forgetting%2C%20where%20the%0Aremoval%20of%20specific%20data%20inadvertently%20impairs%20the%20utility%20of%20the%20model%20and%20its%0Aretention%20of%20valuable%2C%20desired%20information.%20While%20prior%20work%20has%20primarily%0Afocused%20on%20architectural%20innovations%2C%20the%20influence%20of%20data-level%20factors%20on%0Aunlearning%20performance%20remains%20underexplored.%20As%20a%20result%2C%20existing%20methods%0Aoften%20suffer%20from%20degraded%20retention%20when%20forgetting%20high-impact%20data.%20To%0Aaddress%20this%2C%20we%20propose%20GUARD-a%20novel%20framework%20for%20Guided%20Unlearning%20And%0ARetention%20via%20Data%20attribution.%20At%20its%20core%2C%20GUARD%20introduces%20a%20lightweight%0Aproxy%20data%20attribution%20metric%20tailored%20for%20LLM%20unlearning%2C%20which%20quantifies%20the%0A%22alignment%22%20between%20the%20forget%20and%20retain%20sets%20while%20remaining%20computationally%0Aefficient.%20Building%20on%20this%2C%20we%20design%20a%20novel%20unlearning%20objective%20that%0Aassigns%20adaptive%2C%20nonuniform%20unlearning%20weights%20to%20samples%2C%20inversely%0Aproportional%20to%20their%20proxy%20attribution%20scores.%20Through%20such%20a%20reallocation%20of%0Aunlearning%20power%2C%20GUARD%20mitigates%20unintended%20losses%20in%20retention.%20We%20provide%0Arigorous%20theoretical%20guarantees%20that%20GUARD%20significantly%20enhances%20retention%0Awhile%20maintaining%20forgetting%20metrics%20comparable%20to%20prior%20methods.%20Extensive%0Aexperiments%20on%20the%20TOFU%20benchmark%20across%20multiple%20LLM%20architectures%20demonstrate%0Athat%20GUARD%20substantially%20improves%20utility%20preservation%20while%20ensuring%20effective%0Aunlearning.%20Notably%2C%20GUARD%20reduces%20utility%20sacrifice%20on%20the%20Retain%20Set%20by%20up%20to%0A194.92%25%20in%20terms%20of%20Truth%20Ratio%20when%20forgetting%2010%25%20of%20the%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10946v1&entry.124074799=Read"},
{"title": "Sequential-Parallel Duality in Prefix Scannable Models", "author": "Morris Yau and Sharut Gupta and Valerie Engelmayer and Kazuki Irie and Stefanie Jegelka and Jacob Andreas", "abstract": "  Modern neural sequence models are designed to meet the dual mandate of\nparallelizable training and fast sequential inference. Recent developments have\ngiven rise to various models, such as Gated Linear Attention (GLA) and Mamba,\nthat achieve such ``sequential-parallel duality.'' This raises a natural\nquestion: can we characterize the full class of neural sequence models that\nsupport near-constant-time parallel evaluation and linear-time, constant-space\nsequential inference? We begin by describing a broad class of such models --\nstate space models -- as those whose state updates can be computed using the\nclassic parallel prefix scan algorithm with a custom associative aggregation\noperator. We then define a more general class, Prefix-Scannable Models (PSMs),\nby relaxing the state aggregation operator to allow arbitrary (potentially\nnon-associative) functions such as softmax attention. This generalization\nunifies many existing architectures, including element-wise RNNs (e.g., Mamba)\nand linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new\nmodels with softmax-like operators that achieve O(1) amortized compute per\ntoken and log(N) memory for sequence length N. We empirically evaluate such\nmodels on illustrative small-scale language modeling and canonical synthetic\ntasks, including state tracking and associative recall. Empirically, we find\nthat PSMs retain the expressivity of transformer-based architectures while\nmatching the inference efficiency of state space models -- in some cases\nexhibiting better length generalization than either.\n", "link": "http://arxiv.org/abs/2506.10918v1", "date": "2025-06-12", "relevancy": 2.0701, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequential-Parallel%20Duality%20in%20Prefix%20Scannable%20Models&body=Title%3A%20Sequential-Parallel%20Duality%20in%20Prefix%20Scannable%20Models%0AAuthor%3A%20Morris%20Yau%20and%20Sharut%20Gupta%20and%20Valerie%20Engelmayer%20and%20Kazuki%20Irie%20and%20Stefanie%20Jegelka%20and%20Jacob%20Andreas%0AAbstract%3A%20%20%20Modern%20neural%20sequence%20models%20are%20designed%20to%20meet%20the%20dual%20mandate%20of%0Aparallelizable%20training%20and%20fast%20sequential%20inference.%20Recent%20developments%20have%0Agiven%20rise%20to%20various%20models%2C%20such%20as%20Gated%20Linear%20Attention%20%28GLA%29%20and%20Mamba%2C%0Athat%20achieve%20such%20%60%60sequential-parallel%20duality.%27%27%20This%20raises%20a%20natural%0Aquestion%3A%20can%20we%20characterize%20the%20full%20class%20of%20neural%20sequence%20models%20that%0Asupport%20near-constant-time%20parallel%20evaluation%20and%20linear-time%2C%20constant-space%0Asequential%20inference%3F%20We%20begin%20by%20describing%20a%20broad%20class%20of%20such%20models%20--%0Astate%20space%20models%20--%20as%20those%20whose%20state%20updates%20can%20be%20computed%20using%20the%0Aclassic%20parallel%20prefix%20scan%20algorithm%20with%20a%20custom%20associative%20aggregation%0Aoperator.%20We%20then%20define%20a%20more%20general%20class%2C%20Prefix-Scannable%20Models%20%28PSMs%29%2C%0Aby%20relaxing%20the%20state%20aggregation%20operator%20to%20allow%20arbitrary%20%28potentially%0Anon-associative%29%20functions%20such%20as%20softmax%20attention.%20This%20generalization%0Aunifies%20many%20existing%20architectures%2C%20including%20element-wise%20RNNs%20%28e.g.%2C%20Mamba%29%0Aand%20linear%20transformers%20%28e.g.%2C%20GLA%2C%20Mamba2%2C%20mLSTM%29%2C%20while%20also%20introducing%20new%0Amodels%20with%20softmax-like%20operators%20that%20achieve%20O%281%29%20amortized%20compute%20per%0Atoken%20and%20log%28N%29%20memory%20for%20sequence%20length%20N.%20We%20empirically%20evaluate%20such%0Amodels%20on%20illustrative%20small-scale%20language%20modeling%20and%20canonical%20synthetic%0Atasks%2C%20including%20state%20tracking%20and%20associative%20recall.%20Empirically%2C%20we%20find%0Athat%20PSMs%20retain%20the%20expressivity%20of%20transformer-based%20architectures%20while%0Amatching%20the%20inference%20efficiency%20of%20state%20space%20models%20--%20in%20some%20cases%0Aexhibiting%20better%20length%20generalization%20than%20either.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequential-Parallel%2520Duality%2520in%2520Prefix%2520Scannable%2520Models%26entry.906535625%3DMorris%2520Yau%2520and%2520Sharut%2520Gupta%2520and%2520Valerie%2520Engelmayer%2520and%2520Kazuki%2520Irie%2520and%2520Stefanie%2520Jegelka%2520and%2520Jacob%2520Andreas%26entry.1292438233%3D%2520%2520Modern%2520neural%2520sequence%2520models%2520are%2520designed%2520to%2520meet%2520the%2520dual%2520mandate%2520of%250Aparallelizable%2520training%2520and%2520fast%2520sequential%2520inference.%2520Recent%2520developments%2520have%250Agiven%2520rise%2520to%2520various%2520models%252C%2520such%2520as%2520Gated%2520Linear%2520Attention%2520%2528GLA%2529%2520and%2520Mamba%252C%250Athat%2520achieve%2520such%2520%2560%2560sequential-parallel%2520duality.%2527%2527%2520This%2520raises%2520a%2520natural%250Aquestion%253A%2520can%2520we%2520characterize%2520the%2520full%2520class%2520of%2520neural%2520sequence%2520models%2520that%250Asupport%2520near-constant-time%2520parallel%2520evaluation%2520and%2520linear-time%252C%2520constant-space%250Asequential%2520inference%253F%2520We%2520begin%2520by%2520describing%2520a%2520broad%2520class%2520of%2520such%2520models%2520--%250Astate%2520space%2520models%2520--%2520as%2520those%2520whose%2520state%2520updates%2520can%2520be%2520computed%2520using%2520the%250Aclassic%2520parallel%2520prefix%2520scan%2520algorithm%2520with%2520a%2520custom%2520associative%2520aggregation%250Aoperator.%2520We%2520then%2520define%2520a%2520more%2520general%2520class%252C%2520Prefix-Scannable%2520Models%2520%2528PSMs%2529%252C%250Aby%2520relaxing%2520the%2520state%2520aggregation%2520operator%2520to%2520allow%2520arbitrary%2520%2528potentially%250Anon-associative%2529%2520functions%2520such%2520as%2520softmax%2520attention.%2520This%2520generalization%250Aunifies%2520many%2520existing%2520architectures%252C%2520including%2520element-wise%2520RNNs%2520%2528e.g.%252C%2520Mamba%2529%250Aand%2520linear%2520transformers%2520%2528e.g.%252C%2520GLA%252C%2520Mamba2%252C%2520mLSTM%2529%252C%2520while%2520also%2520introducing%2520new%250Amodels%2520with%2520softmax-like%2520operators%2520that%2520achieve%2520O%25281%2529%2520amortized%2520compute%2520per%250Atoken%2520and%2520log%2528N%2529%2520memory%2520for%2520sequence%2520length%2520N.%2520We%2520empirically%2520evaluate%2520such%250Amodels%2520on%2520illustrative%2520small-scale%2520language%2520modeling%2520and%2520canonical%2520synthetic%250Atasks%252C%2520including%2520state%2520tracking%2520and%2520associative%2520recall.%2520Empirically%252C%2520we%2520find%250Athat%2520PSMs%2520retain%2520the%2520expressivity%2520of%2520transformer-based%2520architectures%2520while%250Amatching%2520the%2520inference%2520efficiency%2520of%2520state%2520space%2520models%2520--%2520in%2520some%2520cases%250Aexhibiting%2520better%2520length%2520generalization%2520than%2520either.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequential-Parallel%20Duality%20in%20Prefix%20Scannable%20Models&entry.906535625=Morris%20Yau%20and%20Sharut%20Gupta%20and%20Valerie%20Engelmayer%20and%20Kazuki%20Irie%20and%20Stefanie%20Jegelka%20and%20Jacob%20Andreas&entry.1292438233=%20%20Modern%20neural%20sequence%20models%20are%20designed%20to%20meet%20the%20dual%20mandate%20of%0Aparallelizable%20training%20and%20fast%20sequential%20inference.%20Recent%20developments%20have%0Agiven%20rise%20to%20various%20models%2C%20such%20as%20Gated%20Linear%20Attention%20%28GLA%29%20and%20Mamba%2C%0Athat%20achieve%20such%20%60%60sequential-parallel%20duality.%27%27%20This%20raises%20a%20natural%0Aquestion%3A%20can%20we%20characterize%20the%20full%20class%20of%20neural%20sequence%20models%20that%0Asupport%20near-constant-time%20parallel%20evaluation%20and%20linear-time%2C%20constant-space%0Asequential%20inference%3F%20We%20begin%20by%20describing%20a%20broad%20class%20of%20such%20models%20--%0Astate%20space%20models%20--%20as%20those%20whose%20state%20updates%20can%20be%20computed%20using%20the%0Aclassic%20parallel%20prefix%20scan%20algorithm%20with%20a%20custom%20associative%20aggregation%0Aoperator.%20We%20then%20define%20a%20more%20general%20class%2C%20Prefix-Scannable%20Models%20%28PSMs%29%2C%0Aby%20relaxing%20the%20state%20aggregation%20operator%20to%20allow%20arbitrary%20%28potentially%0Anon-associative%29%20functions%20such%20as%20softmax%20attention.%20This%20generalization%0Aunifies%20many%20existing%20architectures%2C%20including%20element-wise%20RNNs%20%28e.g.%2C%20Mamba%29%0Aand%20linear%20transformers%20%28e.g.%2C%20GLA%2C%20Mamba2%2C%20mLSTM%29%2C%20while%20also%20introducing%20new%0Amodels%20with%20softmax-like%20operators%20that%20achieve%20O%281%29%20amortized%20compute%20per%0Atoken%20and%20log%28N%29%20memory%20for%20sequence%20length%20N.%20We%20empirically%20evaluate%20such%0Amodels%20on%20illustrative%20small-scale%20language%20modeling%20and%20canonical%20synthetic%0Atasks%2C%20including%20state%20tracking%20and%20associative%20recall.%20Empirically%2C%20we%20find%0Athat%20PSMs%20retain%20the%20expressivity%20of%20transformer-based%20architectures%20while%0Amatching%20the%20inference%20efficiency%20of%20state%20space%20models%20--%20in%20some%20cases%0Aexhibiting%20better%20length%20generalization%20than%20either.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10918v1&entry.124074799=Read"},
{"title": "Underage Detection through a Multi-Task and MultiAge Approach for\n  Screening Minors in Unconstrained Imagery", "author": "Christopher Gaul and Eduardo Fidalgo and Enrique Alegre and Roc\u00edo Alaiz Rodr\u00edguez and Eri P\u00e9rez Corral", "abstract": "  Accurate automatic screening of minors in unconstrained images demands models\nthat are robust to distribution shift and resilient to the children\nunder-representation in publicly available data. To overcome these issues, we\npropose a multi-task architecture with dedicated under/over-age discrimination\ntasks based on a frozen FaRL vision-language backbone joined with a compact\ntwo-layer MLP that shares features across one age-regression head and four\nbinary under-age heads for age thresholds of 12, 15, 18, and 21 years, focusing\non the legally critical age range. To address the severe class imbalance, we\nintroduce an $\\alpha$-reweighted focal-style loss and age-balanced mini-batch\nsampling, which equalizes twelve age bins during stochastic optimization.\nFurther improvement is achieved with an age gap that removes edge cases from\nthe loss.\n  Moreover, we set a rigorous evaluation by proposing the Overall Under-Age\nBenchmark, with 303k cleaned training images and 110k test images, defining\nboth the \"ASORES-39k\" restricted overall test, which removes the noisiest\ndomains, and the age estimation wild shifts test \"ASWIFT-20k\" of 20k-images,\nstressing extreme pose ($>$45{\\deg}), expression, and low image quality to\nemulate real-world shifts.\n  Trained on the cleaned overall set with resampling and age gap, our multiage\nmodel \"F\" lowers the root-mean-square-error on the ASORES-39k restricted test\nfrom 5.733 (age-only baseline) to 5.656 years and lifts under-18 detection from\nF2 score of 0.801 to 0.857 at 1% false-adult rate. Under the domain shift to\nthe wild data of ASWIFT-20k, the same configuration nearly sustains 0.99 recall\nwhile boosting F2 from 0.742 to 0.833 with respect to the age-only baseline,\ndemonstrating strong generalization under distribution shift. For the under-12\nand under-15 tasks, the respective boosts in F2 are from 0.666 to 0.955 and\nfrom 0.689 to 0.916, respectively.\n", "link": "http://arxiv.org/abs/2506.10689v1", "date": "2025-06-12", "relevancy": 2.0493, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5095}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Underage%20Detection%20through%20a%20Multi-Task%20and%20MultiAge%20Approach%20for%0A%20%20Screening%20Minors%20in%20Unconstrained%20Imagery&body=Title%3A%20Underage%20Detection%20through%20a%20Multi-Task%20and%20MultiAge%20Approach%20for%0A%20%20Screening%20Minors%20in%20Unconstrained%20Imagery%0AAuthor%3A%20Christopher%20Gaul%20and%20Eduardo%20Fidalgo%20and%20Enrique%20Alegre%20and%20Roc%C3%ADo%20Alaiz%20Rodr%C3%ADguez%20and%20Eri%20P%C3%A9rez%20Corral%0AAbstract%3A%20%20%20Accurate%20automatic%20screening%20of%20minors%20in%20unconstrained%20images%20demands%20models%0Athat%20are%20robust%20to%20distribution%20shift%20and%20resilient%20to%20the%20children%0Aunder-representation%20in%20publicly%20available%20data.%20To%20overcome%20these%20issues%2C%20we%0Apropose%20a%20multi-task%20architecture%20with%20dedicated%20under/over-age%20discrimination%0Atasks%20based%20on%20a%20frozen%20FaRL%20vision-language%20backbone%20joined%20with%20a%20compact%0Atwo-layer%20MLP%20that%20shares%20features%20across%20one%20age-regression%20head%20and%20four%0Abinary%20under-age%20heads%20for%20age%20thresholds%20of%2012%2C%2015%2C%2018%2C%20and%2021%20years%2C%20focusing%0Aon%20the%20legally%20critical%20age%20range.%20To%20address%20the%20severe%20class%20imbalance%2C%20we%0Aintroduce%20an%20%24%5Calpha%24-reweighted%20focal-style%20loss%20and%20age-balanced%20mini-batch%0Asampling%2C%20which%20equalizes%20twelve%20age%20bins%20during%20stochastic%20optimization.%0AFurther%20improvement%20is%20achieved%20with%20an%20age%20gap%20that%20removes%20edge%20cases%20from%0Athe%20loss.%0A%20%20Moreover%2C%20we%20set%20a%20rigorous%20evaluation%20by%20proposing%20the%20Overall%20Under-Age%0ABenchmark%2C%20with%20303k%20cleaned%20training%20images%20and%20110k%20test%20images%2C%20defining%0Aboth%20the%20%22ASORES-39k%22%20restricted%20overall%20test%2C%20which%20removes%20the%20noisiest%0Adomains%2C%20and%20the%20age%20estimation%20wild%20shifts%20test%20%22ASWIFT-20k%22%20of%2020k-images%2C%0Astressing%20extreme%20pose%20%28%24%3E%2445%7B%5Cdeg%7D%29%2C%20expression%2C%20and%20low%20image%20quality%20to%0Aemulate%20real-world%20shifts.%0A%20%20Trained%20on%20the%20cleaned%20overall%20set%20with%20resampling%20and%20age%20gap%2C%20our%20multiage%0Amodel%20%22F%22%20lowers%20the%20root-mean-square-error%20on%20the%20ASORES-39k%20restricted%20test%0Afrom%205.733%20%28age-only%20baseline%29%20to%205.656%20years%20and%20lifts%20under-18%20detection%20from%0AF2%20score%20of%200.801%20to%200.857%20at%201%25%20false-adult%20rate.%20Under%20the%20domain%20shift%20to%0Athe%20wild%20data%20of%20ASWIFT-20k%2C%20the%20same%20configuration%20nearly%20sustains%200.99%20recall%0Awhile%20boosting%20F2%20from%200.742%20to%200.833%20with%20respect%20to%20the%20age-only%20baseline%2C%0Ademonstrating%20strong%20generalization%20under%20distribution%20shift.%20For%20the%20under-12%0Aand%20under-15%20tasks%2C%20the%20respective%20boosts%20in%20F2%20are%20from%200.666%20to%200.955%20and%0Afrom%200.689%20to%200.916%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderage%2520Detection%2520through%2520a%2520Multi-Task%2520and%2520MultiAge%2520Approach%2520for%250A%2520%2520Screening%2520Minors%2520in%2520Unconstrained%2520Imagery%26entry.906535625%3DChristopher%2520Gaul%2520and%2520Eduardo%2520Fidalgo%2520and%2520Enrique%2520Alegre%2520and%2520Roc%25C3%25ADo%2520Alaiz%2520Rodr%25C3%25ADguez%2520and%2520Eri%2520P%25C3%25A9rez%2520Corral%26entry.1292438233%3D%2520%2520Accurate%2520automatic%2520screening%2520of%2520minors%2520in%2520unconstrained%2520images%2520demands%2520models%250Athat%2520are%2520robust%2520to%2520distribution%2520shift%2520and%2520resilient%2520to%2520the%2520children%250Aunder-representation%2520in%2520publicly%2520available%2520data.%2520To%2520overcome%2520these%2520issues%252C%2520we%250Apropose%2520a%2520multi-task%2520architecture%2520with%2520dedicated%2520under/over-age%2520discrimination%250Atasks%2520based%2520on%2520a%2520frozen%2520FaRL%2520vision-language%2520backbone%2520joined%2520with%2520a%2520compact%250Atwo-layer%2520MLP%2520that%2520shares%2520features%2520across%2520one%2520age-regression%2520head%2520and%2520four%250Abinary%2520under-age%2520heads%2520for%2520age%2520thresholds%2520of%252012%252C%252015%252C%252018%252C%2520and%252021%2520years%252C%2520focusing%250Aon%2520the%2520legally%2520critical%2520age%2520range.%2520To%2520address%2520the%2520severe%2520class%2520imbalance%252C%2520we%250Aintroduce%2520an%2520%2524%255Calpha%2524-reweighted%2520focal-style%2520loss%2520and%2520age-balanced%2520mini-batch%250Asampling%252C%2520which%2520equalizes%2520twelve%2520age%2520bins%2520during%2520stochastic%2520optimization.%250AFurther%2520improvement%2520is%2520achieved%2520with%2520an%2520age%2520gap%2520that%2520removes%2520edge%2520cases%2520from%250Athe%2520loss.%250A%2520%2520Moreover%252C%2520we%2520set%2520a%2520rigorous%2520evaluation%2520by%2520proposing%2520the%2520Overall%2520Under-Age%250ABenchmark%252C%2520with%2520303k%2520cleaned%2520training%2520images%2520and%2520110k%2520test%2520images%252C%2520defining%250Aboth%2520the%2520%2522ASORES-39k%2522%2520restricted%2520overall%2520test%252C%2520which%2520removes%2520the%2520noisiest%250Adomains%252C%2520and%2520the%2520age%2520estimation%2520wild%2520shifts%2520test%2520%2522ASWIFT-20k%2522%2520of%252020k-images%252C%250Astressing%2520extreme%2520pose%2520%2528%2524%253E%252445%257B%255Cdeg%257D%2529%252C%2520expression%252C%2520and%2520low%2520image%2520quality%2520to%250Aemulate%2520real-world%2520shifts.%250A%2520%2520Trained%2520on%2520the%2520cleaned%2520overall%2520set%2520with%2520resampling%2520and%2520age%2520gap%252C%2520our%2520multiage%250Amodel%2520%2522F%2522%2520lowers%2520the%2520root-mean-square-error%2520on%2520the%2520ASORES-39k%2520restricted%2520test%250Afrom%25205.733%2520%2528age-only%2520baseline%2529%2520to%25205.656%2520years%2520and%2520lifts%2520under-18%2520detection%2520from%250AF2%2520score%2520of%25200.801%2520to%25200.857%2520at%25201%2525%2520false-adult%2520rate.%2520Under%2520the%2520domain%2520shift%2520to%250Athe%2520wild%2520data%2520of%2520ASWIFT-20k%252C%2520the%2520same%2520configuration%2520nearly%2520sustains%25200.99%2520recall%250Awhile%2520boosting%2520F2%2520from%25200.742%2520to%25200.833%2520with%2520respect%2520to%2520the%2520age-only%2520baseline%252C%250Ademonstrating%2520strong%2520generalization%2520under%2520distribution%2520shift.%2520For%2520the%2520under-12%250Aand%2520under-15%2520tasks%252C%2520the%2520respective%2520boosts%2520in%2520F2%2520are%2520from%25200.666%2520to%25200.955%2520and%250Afrom%25200.689%2520to%25200.916%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Underage%20Detection%20through%20a%20Multi-Task%20and%20MultiAge%20Approach%20for%0A%20%20Screening%20Minors%20in%20Unconstrained%20Imagery&entry.906535625=Christopher%20Gaul%20and%20Eduardo%20Fidalgo%20and%20Enrique%20Alegre%20and%20Roc%C3%ADo%20Alaiz%20Rodr%C3%ADguez%20and%20Eri%20P%C3%A9rez%20Corral&entry.1292438233=%20%20Accurate%20automatic%20screening%20of%20minors%20in%20unconstrained%20images%20demands%20models%0Athat%20are%20robust%20to%20distribution%20shift%20and%20resilient%20to%20the%20children%0Aunder-representation%20in%20publicly%20available%20data.%20To%20overcome%20these%20issues%2C%20we%0Apropose%20a%20multi-task%20architecture%20with%20dedicated%20under/over-age%20discrimination%0Atasks%20based%20on%20a%20frozen%20FaRL%20vision-language%20backbone%20joined%20with%20a%20compact%0Atwo-layer%20MLP%20that%20shares%20features%20across%20one%20age-regression%20head%20and%20four%0Abinary%20under-age%20heads%20for%20age%20thresholds%20of%2012%2C%2015%2C%2018%2C%20and%2021%20years%2C%20focusing%0Aon%20the%20legally%20critical%20age%20range.%20To%20address%20the%20severe%20class%20imbalance%2C%20we%0Aintroduce%20an%20%24%5Calpha%24-reweighted%20focal-style%20loss%20and%20age-balanced%20mini-batch%0Asampling%2C%20which%20equalizes%20twelve%20age%20bins%20during%20stochastic%20optimization.%0AFurther%20improvement%20is%20achieved%20with%20an%20age%20gap%20that%20removes%20edge%20cases%20from%0Athe%20loss.%0A%20%20Moreover%2C%20we%20set%20a%20rigorous%20evaluation%20by%20proposing%20the%20Overall%20Under-Age%0ABenchmark%2C%20with%20303k%20cleaned%20training%20images%20and%20110k%20test%20images%2C%20defining%0Aboth%20the%20%22ASORES-39k%22%20restricted%20overall%20test%2C%20which%20removes%20the%20noisiest%0Adomains%2C%20and%20the%20age%20estimation%20wild%20shifts%20test%20%22ASWIFT-20k%22%20of%2020k-images%2C%0Astressing%20extreme%20pose%20%28%24%3E%2445%7B%5Cdeg%7D%29%2C%20expression%2C%20and%20low%20image%20quality%20to%0Aemulate%20real-world%20shifts.%0A%20%20Trained%20on%20the%20cleaned%20overall%20set%20with%20resampling%20and%20age%20gap%2C%20our%20multiage%0Amodel%20%22F%22%20lowers%20the%20root-mean-square-error%20on%20the%20ASORES-39k%20restricted%20test%0Afrom%205.733%20%28age-only%20baseline%29%20to%205.656%20years%20and%20lifts%20under-18%20detection%20from%0AF2%20score%20of%200.801%20to%200.857%20at%201%25%20false-adult%20rate.%20Under%20the%20domain%20shift%20to%0Athe%20wild%20data%20of%20ASWIFT-20k%2C%20the%20same%20configuration%20nearly%20sustains%200.99%20recall%0Awhile%20boosting%20F2%20from%200.742%20to%200.833%20with%20respect%20to%20the%20age-only%20baseline%2C%0Ademonstrating%20strong%20generalization%20under%20distribution%20shift.%20For%20the%20under-12%0Aand%20under-15%20tasks%2C%20the%20respective%20boosts%20in%20F2%20are%20from%200.666%20to%200.955%20and%0Afrom%200.689%20to%200.916%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10689v1&entry.124074799=Read"},
{"title": "Exploring Performance-Complexity Trade-Offs in Sound Event Detection\n  Models", "author": "Tobias Morocutti and Florian Schmid and Jonathan Greif and Francesco Foscarin and Gerhard Widmer", "abstract": "  We target the problem of developing new low-complexity networks for the sound\nevent detection task. Our goal is to meticulously analyze the\nperformance-complexity trade-off, aiming to be competitive with the large\nstate-of-the-art models, at a fraction of the computational requirements. We\nfind that low-complexity convolutional models previously proposed for audio\ntagging can be effectively adapted for event detection (which requires\nframe-wise prediction) by adjusting convolutional strides, removing the global\npooling, and, importantly, adding a sequence model before the (now frame-wise)\nclassification heads. Systematic experiments reveal that the best choice for\nthe sequence model type depends on which complexity metric is most important\nfor the given application. We also investigate the impact of enhanced training\nstrategies such as knowledge distillation. In the end, we show that combined\nwith an optimized training strategy, we can reach event detection performance\ncomparable to state-of-the-art transformers while requiring only around 5% of\nthe parameters. We release all our pre-trained models and the code for\nreproducing this work to support future research in low-complexity sound event\ndetection at https://github.com/theMoro/EfficientSED.\n", "link": "http://arxiv.org/abs/2503.11373v2", "date": "2025-06-12", "relevancy": 2.0453, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5209}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Performance-Complexity%20Trade-Offs%20in%20Sound%20Event%20Detection%0A%20%20Models&body=Title%3A%20Exploring%20Performance-Complexity%20Trade-Offs%20in%20Sound%20Event%20Detection%0A%20%20Models%0AAuthor%3A%20Tobias%20Morocutti%20and%20Florian%20Schmid%20and%20Jonathan%20Greif%20and%20Francesco%20Foscarin%20and%20Gerhard%20Widmer%0AAbstract%3A%20%20%20We%20target%20the%20problem%20of%20developing%20new%20low-complexity%20networks%20for%20the%20sound%0Aevent%20detection%20task.%20Our%20goal%20is%20to%20meticulously%20analyze%20the%0Aperformance-complexity%20trade-off%2C%20aiming%20to%20be%20competitive%20with%20the%20large%0Astate-of-the-art%20models%2C%20at%20a%20fraction%20of%20the%20computational%20requirements.%20We%0Afind%20that%20low-complexity%20convolutional%20models%20previously%20proposed%20for%20audio%0Atagging%20can%20be%20effectively%20adapted%20for%20event%20detection%20%28which%20requires%0Aframe-wise%20prediction%29%20by%20adjusting%20convolutional%20strides%2C%20removing%20the%20global%0Apooling%2C%20and%2C%20importantly%2C%20adding%20a%20sequence%20model%20before%20the%20%28now%20frame-wise%29%0Aclassification%20heads.%20Systematic%20experiments%20reveal%20that%20the%20best%20choice%20for%0Athe%20sequence%20model%20type%20depends%20on%20which%20complexity%20metric%20is%20most%20important%0Afor%20the%20given%20application.%20We%20also%20investigate%20the%20impact%20of%20enhanced%20training%0Astrategies%20such%20as%20knowledge%20distillation.%20In%20the%20end%2C%20we%20show%20that%20combined%0Awith%20an%20optimized%20training%20strategy%2C%20we%20can%20reach%20event%20detection%20performance%0Acomparable%20to%20state-of-the-art%20transformers%20while%20requiring%20only%20around%205%25%20of%0Athe%20parameters.%20We%20release%20all%20our%20pre-trained%20models%20and%20the%20code%20for%0Areproducing%20this%20work%20to%20support%20future%20research%20in%20low-complexity%20sound%20event%0Adetection%20at%20https%3A//github.com/theMoro/EfficientSED.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.11373v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Performance-Complexity%2520Trade-Offs%2520in%2520Sound%2520Event%2520Detection%250A%2520%2520Models%26entry.906535625%3DTobias%2520Morocutti%2520and%2520Florian%2520Schmid%2520and%2520Jonathan%2520Greif%2520and%2520Francesco%2520Foscarin%2520and%2520Gerhard%2520Widmer%26entry.1292438233%3D%2520%2520We%2520target%2520the%2520problem%2520of%2520developing%2520new%2520low-complexity%2520networks%2520for%2520the%2520sound%250Aevent%2520detection%2520task.%2520Our%2520goal%2520is%2520to%2520meticulously%2520analyze%2520the%250Aperformance-complexity%2520trade-off%252C%2520aiming%2520to%2520be%2520competitive%2520with%2520the%2520large%250Astate-of-the-art%2520models%252C%2520at%2520a%2520fraction%2520of%2520the%2520computational%2520requirements.%2520We%250Afind%2520that%2520low-complexity%2520convolutional%2520models%2520previously%2520proposed%2520for%2520audio%250Atagging%2520can%2520be%2520effectively%2520adapted%2520for%2520event%2520detection%2520%2528which%2520requires%250Aframe-wise%2520prediction%2529%2520by%2520adjusting%2520convolutional%2520strides%252C%2520removing%2520the%2520global%250Apooling%252C%2520and%252C%2520importantly%252C%2520adding%2520a%2520sequence%2520model%2520before%2520the%2520%2528now%2520frame-wise%2529%250Aclassification%2520heads.%2520Systematic%2520experiments%2520reveal%2520that%2520the%2520best%2520choice%2520for%250Athe%2520sequence%2520model%2520type%2520depends%2520on%2520which%2520complexity%2520metric%2520is%2520most%2520important%250Afor%2520the%2520given%2520application.%2520We%2520also%2520investigate%2520the%2520impact%2520of%2520enhanced%2520training%250Astrategies%2520such%2520as%2520knowledge%2520distillation.%2520In%2520the%2520end%252C%2520we%2520show%2520that%2520combined%250Awith%2520an%2520optimized%2520training%2520strategy%252C%2520we%2520can%2520reach%2520event%2520detection%2520performance%250Acomparable%2520to%2520state-of-the-art%2520transformers%2520while%2520requiring%2520only%2520around%25205%2525%2520of%250Athe%2520parameters.%2520We%2520release%2520all%2520our%2520pre-trained%2520models%2520and%2520the%2520code%2520for%250Areproducing%2520this%2520work%2520to%2520support%2520future%2520research%2520in%2520low-complexity%2520sound%2520event%250Adetection%2520at%2520https%253A//github.com/theMoro/EfficientSED.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.11373v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Performance-Complexity%20Trade-Offs%20in%20Sound%20Event%20Detection%0A%20%20Models&entry.906535625=Tobias%20Morocutti%20and%20Florian%20Schmid%20and%20Jonathan%20Greif%20and%20Francesco%20Foscarin%20and%20Gerhard%20Widmer&entry.1292438233=%20%20We%20target%20the%20problem%20of%20developing%20new%20low-complexity%20networks%20for%20the%20sound%0Aevent%20detection%20task.%20Our%20goal%20is%20to%20meticulously%20analyze%20the%0Aperformance-complexity%20trade-off%2C%20aiming%20to%20be%20competitive%20with%20the%20large%0Astate-of-the-art%20models%2C%20at%20a%20fraction%20of%20the%20computational%20requirements.%20We%0Afind%20that%20low-complexity%20convolutional%20models%20previously%20proposed%20for%20audio%0Atagging%20can%20be%20effectively%20adapted%20for%20event%20detection%20%28which%20requires%0Aframe-wise%20prediction%29%20by%20adjusting%20convolutional%20strides%2C%20removing%20the%20global%0Apooling%2C%20and%2C%20importantly%2C%20adding%20a%20sequence%20model%20before%20the%20%28now%20frame-wise%29%0Aclassification%20heads.%20Systematic%20experiments%20reveal%20that%20the%20best%20choice%20for%0Athe%20sequence%20model%20type%20depends%20on%20which%20complexity%20metric%20is%20most%20important%0Afor%20the%20given%20application.%20We%20also%20investigate%20the%20impact%20of%20enhanced%20training%0Astrategies%20such%20as%20knowledge%20distillation.%20In%20the%20end%2C%20we%20show%20that%20combined%0Awith%20an%20optimized%20training%20strategy%2C%20we%20can%20reach%20event%20detection%20performance%0Acomparable%20to%20state-of-the-art%20transformers%20while%20requiring%20only%20around%205%25%20of%0Athe%20parameters.%20We%20release%20all%20our%20pre-trained%20models%20and%20the%20code%20for%0Areproducing%20this%20work%20to%20support%20future%20research%20in%20low-complexity%20sound%20event%0Adetection%20at%20https%3A//github.com/theMoro/EfficientSED.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.11373v2&entry.124074799=Read"},
{"title": "PREMISE: Scalable and Strategic Prompt Optimization for Efficient\n  Mathematical Reasoning in Large Models", "author": "Ye Yu and Yaoning Yu and Haohan Wang", "abstract": "  Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve\nstrong performance on mathematical benchmarks using lengthy chain-of-thought\n(CoT) reasoning, but the resulting traces are often unnecessarily verbose. This\ninflates token usage and cost, limiting deployment in latency-sensitive or\nAPI-constrained settings. We introduce PREMISE (PRompt-based Efficient\nMathematical Inference with Strategic Evaluation), a prompt-only framework that\nreduces reasoning overhead without modifying model weights. PREMISE combines\ntrace-level diagnostics with gradient-inspired prompt optimization to minimize\nredundant computation while preserving answer accuracy. The approach jointly\noptimizes brevity and correctness through a multi-objective textual search that\nbalances token length and answer validity. Unlike prior work, PREMISE runs in a\nsingle-pass black-box interface, so it can be applied directly to commercial\nLLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy\n($96\\%\\rightarrow96\\%$ with Claude, $91\\%\\rightarrow92\\%$ with Gemini) while\nreducing reasoning tokens by up to $87.5\\%$ and cutting dollar cost by\n$69$--$82\\%$. These results show that prompt-level optimization is a practical\nand scalable path to efficient LRM inference without compromising reasoning\nquality.\n", "link": "http://arxiv.org/abs/2506.10716v1", "date": "2025-06-12", "relevancy": 2.0389, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PREMISE%3A%20Scalable%20and%20Strategic%20Prompt%20Optimization%20for%20Efficient%0A%20%20Mathematical%20Reasoning%20in%20Large%20Models&body=Title%3A%20PREMISE%3A%20Scalable%20and%20Strategic%20Prompt%20Optimization%20for%20Efficient%0A%20%20Mathematical%20Reasoning%20in%20Large%20Models%0AAuthor%3A%20Ye%20Yu%20and%20Yaoning%20Yu%20and%20Haohan%20Wang%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%20such%20as%20Claude%203.7%20Sonnet%20and%20OpenAI%20o1%20achieve%0Astrong%20performance%20on%20mathematical%20benchmarks%20using%20lengthy%20chain-of-thought%0A%28CoT%29%20reasoning%2C%20but%20the%20resulting%20traces%20are%20often%20unnecessarily%20verbose.%20This%0Ainflates%20token%20usage%20and%20cost%2C%20limiting%20deployment%20in%20latency-sensitive%20or%0AAPI-constrained%20settings.%20We%20introduce%20PREMISE%20%28PRompt-based%20Efficient%0AMathematical%20Inference%20with%20Strategic%20Evaluation%29%2C%20a%20prompt-only%20framework%20that%0Areduces%20reasoning%20overhead%20without%20modifying%20model%20weights.%20PREMISE%20combines%0Atrace-level%20diagnostics%20with%20gradient-inspired%20prompt%20optimization%20to%20minimize%0Aredundant%20computation%20while%20preserving%20answer%20accuracy.%20The%20approach%20jointly%0Aoptimizes%20brevity%20and%20correctness%20through%20a%20multi-objective%20textual%20search%20that%0Abalances%20token%20length%20and%20answer%20validity.%20Unlike%20prior%20work%2C%20PREMISE%20runs%20in%20a%0Asingle-pass%20black-box%20interface%2C%20so%20it%20can%20be%20applied%20directly%20to%20commercial%0ALLMs.%20On%20GSM8K%2C%20SVAMP%2C%20and%20Math500%20we%20match%20or%20exceed%20baseline%20accuracy%0A%28%2496%5C%25%5Crightarrow96%5C%25%24%20with%20Claude%2C%20%2491%5C%25%5Crightarrow92%5C%25%24%20with%20Gemini%29%20while%0Areducing%20reasoning%20tokens%20by%20up%20to%20%2487.5%5C%25%24%20and%20cutting%20dollar%20cost%20by%0A%2469%24--%2482%5C%25%24.%20These%20results%20show%20that%20prompt-level%20optimization%20is%20a%20practical%0Aand%20scalable%20path%20to%20efficient%20LRM%20inference%20without%20compromising%20reasoning%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPREMISE%253A%2520Scalable%2520and%2520Strategic%2520Prompt%2520Optimization%2520for%2520Efficient%250A%2520%2520Mathematical%2520Reasoning%2520in%2520Large%2520Models%26entry.906535625%3DYe%2520Yu%2520and%2520Yaoning%2520Yu%2520and%2520Haohan%2520Wang%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%2520such%2520as%2520Claude%25203.7%2520Sonnet%2520and%2520OpenAI%2520o1%2520achieve%250Astrong%2520performance%2520on%2520mathematical%2520benchmarks%2520using%2520lengthy%2520chain-of-thought%250A%2528CoT%2529%2520reasoning%252C%2520but%2520the%2520resulting%2520traces%2520are%2520often%2520unnecessarily%2520verbose.%2520This%250Ainflates%2520token%2520usage%2520and%2520cost%252C%2520limiting%2520deployment%2520in%2520latency-sensitive%2520or%250AAPI-constrained%2520settings.%2520We%2520introduce%2520PREMISE%2520%2528PRompt-based%2520Efficient%250AMathematical%2520Inference%2520with%2520Strategic%2520Evaluation%2529%252C%2520a%2520prompt-only%2520framework%2520that%250Areduces%2520reasoning%2520overhead%2520without%2520modifying%2520model%2520weights.%2520PREMISE%2520combines%250Atrace-level%2520diagnostics%2520with%2520gradient-inspired%2520prompt%2520optimization%2520to%2520minimize%250Aredundant%2520computation%2520while%2520preserving%2520answer%2520accuracy.%2520The%2520approach%2520jointly%250Aoptimizes%2520brevity%2520and%2520correctness%2520through%2520a%2520multi-objective%2520textual%2520search%2520that%250Abalances%2520token%2520length%2520and%2520answer%2520validity.%2520Unlike%2520prior%2520work%252C%2520PREMISE%2520runs%2520in%2520a%250Asingle-pass%2520black-box%2520interface%252C%2520so%2520it%2520can%2520be%2520applied%2520directly%2520to%2520commercial%250ALLMs.%2520On%2520GSM8K%252C%2520SVAMP%252C%2520and%2520Math500%2520we%2520match%2520or%2520exceed%2520baseline%2520accuracy%250A%2528%252496%255C%2525%255Crightarrow96%255C%2525%2524%2520with%2520Claude%252C%2520%252491%255C%2525%255Crightarrow92%255C%2525%2524%2520with%2520Gemini%2529%2520while%250Areducing%2520reasoning%2520tokens%2520by%2520up%2520to%2520%252487.5%255C%2525%2524%2520and%2520cutting%2520dollar%2520cost%2520by%250A%252469%2524--%252482%255C%2525%2524.%2520These%2520results%2520show%2520that%2520prompt-level%2520optimization%2520is%2520a%2520practical%250Aand%2520scalable%2520path%2520to%2520efficient%2520LRM%2520inference%2520without%2520compromising%2520reasoning%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PREMISE%3A%20Scalable%20and%20Strategic%20Prompt%20Optimization%20for%20Efficient%0A%20%20Mathematical%20Reasoning%20in%20Large%20Models&entry.906535625=Ye%20Yu%20and%20Yaoning%20Yu%20and%20Haohan%20Wang&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%20such%20as%20Claude%203.7%20Sonnet%20and%20OpenAI%20o1%20achieve%0Astrong%20performance%20on%20mathematical%20benchmarks%20using%20lengthy%20chain-of-thought%0A%28CoT%29%20reasoning%2C%20but%20the%20resulting%20traces%20are%20often%20unnecessarily%20verbose.%20This%0Ainflates%20token%20usage%20and%20cost%2C%20limiting%20deployment%20in%20latency-sensitive%20or%0AAPI-constrained%20settings.%20We%20introduce%20PREMISE%20%28PRompt-based%20Efficient%0AMathematical%20Inference%20with%20Strategic%20Evaluation%29%2C%20a%20prompt-only%20framework%20that%0Areduces%20reasoning%20overhead%20without%20modifying%20model%20weights.%20PREMISE%20combines%0Atrace-level%20diagnostics%20with%20gradient-inspired%20prompt%20optimization%20to%20minimize%0Aredundant%20computation%20while%20preserving%20answer%20accuracy.%20The%20approach%20jointly%0Aoptimizes%20brevity%20and%20correctness%20through%20a%20multi-objective%20textual%20search%20that%0Abalances%20token%20length%20and%20answer%20validity.%20Unlike%20prior%20work%2C%20PREMISE%20runs%20in%20a%0Asingle-pass%20black-box%20interface%2C%20so%20it%20can%20be%20applied%20directly%20to%20commercial%0ALLMs.%20On%20GSM8K%2C%20SVAMP%2C%20and%20Math500%20we%20match%20or%20exceed%20baseline%20accuracy%0A%28%2496%5C%25%5Crightarrow96%5C%25%24%20with%20Claude%2C%20%2491%5C%25%5Crightarrow92%5C%25%24%20with%20Gemini%29%20while%0Areducing%20reasoning%20tokens%20by%20up%20to%20%2487.5%5C%25%24%20and%20cutting%20dollar%20cost%20by%0A%2469%24--%2482%5C%25%24.%20These%20results%20show%20that%20prompt-level%20optimization%20is%20a%20practical%0Aand%20scalable%20path%20to%20efficient%20LRM%20inference%20without%20compromising%20reasoning%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10716v1&entry.124074799=Read"},
{"title": "Decomposing MLP Activations into Interpretable Features via\n  Semi-Nonnegative Matrix Factorization", "author": "Or Shafran and Atticus Geiger and Mor Geva", "abstract": "  A central goal for mechanistic interpretability has been to identify the\nright units of analysis in large language models (LLMs) that causally explain\ntheir outputs. While early work focused on individual neurons, evidence that\nneurons often encode multiple concepts has motivated a shift toward analyzing\ndirections in activation space. A key question is how to find directions that\ncapture interpretable features in an unsupervised manner. Current methods rely\non dictionary learning with sparse autoencoders (SAEs), commonly trained over\nresidual stream activations to learn directions from scratch. However, SAEs\noften struggle in causal evaluations and lack intrinsic interpretability, as\ntheir learning is not explicitly tied to the computations of the model. Here,\nwe tackle these limitations by directly decomposing MLP activations with\nsemi-nonnegative matrix factorization (SNMF), such that the learned features\nare (a) sparse linear combinations of co-activated neurons, and (b) mapped to\ntheir activating inputs, making them directly interpretable. Experiments on\nLlama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs\nand a strong supervised baseline (difference-in-means) on causal steering,\nwhile aligning with human-interpretable concepts. Further analysis reveals that\nspecific neuron combinations are reused across semantically-related features,\nexposing a hierarchical structure in the MLP's activation space. Together,\nthese results position SNMF as a simple and effective tool for identifying\ninterpretable features and dissecting concept representations in LLMs.\n", "link": "http://arxiv.org/abs/2506.10920v1", "date": "2025-06-12", "relevancy": 2.0349, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5103}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20MLP%20Activations%20into%20Interpretable%20Features%20via%0A%20%20Semi-Nonnegative%20Matrix%20Factorization&body=Title%3A%20Decomposing%20MLP%20Activations%20into%20Interpretable%20Features%20via%0A%20%20Semi-Nonnegative%20Matrix%20Factorization%0AAuthor%3A%20Or%20Shafran%20and%20Atticus%20Geiger%20and%20Mor%20Geva%0AAbstract%3A%20%20%20A%20central%20goal%20for%20mechanistic%20interpretability%20has%20been%20to%20identify%20the%0Aright%20units%20of%20analysis%20in%20large%20language%20models%20%28LLMs%29%20that%20causally%20explain%0Atheir%20outputs.%20While%20early%20work%20focused%20on%20individual%20neurons%2C%20evidence%20that%0Aneurons%20often%20encode%20multiple%20concepts%20has%20motivated%20a%20shift%20toward%20analyzing%0Adirections%20in%20activation%20space.%20A%20key%20question%20is%20how%20to%20find%20directions%20that%0Acapture%20interpretable%20features%20in%20an%20unsupervised%20manner.%20Current%20methods%20rely%0Aon%20dictionary%20learning%20with%20sparse%20autoencoders%20%28SAEs%29%2C%20commonly%20trained%20over%0Aresidual%20stream%20activations%20to%20learn%20directions%20from%20scratch.%20However%2C%20SAEs%0Aoften%20struggle%20in%20causal%20evaluations%20and%20lack%20intrinsic%20interpretability%2C%20as%0Atheir%20learning%20is%20not%20explicitly%20tied%20to%20the%20computations%20of%20the%20model.%20Here%2C%0Awe%20tackle%20these%20limitations%20by%20directly%20decomposing%20MLP%20activations%20with%0Asemi-nonnegative%20matrix%20factorization%20%28SNMF%29%2C%20such%20that%20the%20learned%20features%0Aare%20%28a%29%20sparse%20linear%20combinations%20of%20co-activated%20neurons%2C%20and%20%28b%29%20mapped%20to%0Atheir%20activating%20inputs%2C%20making%20them%20directly%20interpretable.%20Experiments%20on%0ALlama%203.1%2C%20Gemma%202%20and%20GPT-2%20show%20that%20SNMF%20derived%20features%20outperform%20SAEs%0Aand%20a%20strong%20supervised%20baseline%20%28difference-in-means%29%20on%20causal%20steering%2C%0Awhile%20aligning%20with%20human-interpretable%20concepts.%20Further%20analysis%20reveals%20that%0Aspecific%20neuron%20combinations%20are%20reused%20across%20semantically-related%20features%2C%0Aexposing%20a%20hierarchical%20structure%20in%20the%20MLP%27s%20activation%20space.%20Together%2C%0Athese%20results%20position%20SNMF%20as%20a%20simple%20and%20effective%20tool%20for%20identifying%0Ainterpretable%20features%20and%20dissecting%20concept%20representations%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520MLP%2520Activations%2520into%2520Interpretable%2520Features%2520via%250A%2520%2520Semi-Nonnegative%2520Matrix%2520Factorization%26entry.906535625%3DOr%2520Shafran%2520and%2520Atticus%2520Geiger%2520and%2520Mor%2520Geva%26entry.1292438233%3D%2520%2520A%2520central%2520goal%2520for%2520mechanistic%2520interpretability%2520has%2520been%2520to%2520identify%2520the%250Aright%2520units%2520of%2520analysis%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520that%2520causally%2520explain%250Atheir%2520outputs.%2520While%2520early%2520work%2520focused%2520on%2520individual%2520neurons%252C%2520evidence%2520that%250Aneurons%2520often%2520encode%2520multiple%2520concepts%2520has%2520motivated%2520a%2520shift%2520toward%2520analyzing%250Adirections%2520in%2520activation%2520space.%2520A%2520key%2520question%2520is%2520how%2520to%2520find%2520directions%2520that%250Acapture%2520interpretable%2520features%2520in%2520an%2520unsupervised%2520manner.%2520Current%2520methods%2520rely%250Aon%2520dictionary%2520learning%2520with%2520sparse%2520autoencoders%2520%2528SAEs%2529%252C%2520commonly%2520trained%2520over%250Aresidual%2520stream%2520activations%2520to%2520learn%2520directions%2520from%2520scratch.%2520However%252C%2520SAEs%250Aoften%2520struggle%2520in%2520causal%2520evaluations%2520and%2520lack%2520intrinsic%2520interpretability%252C%2520as%250Atheir%2520learning%2520is%2520not%2520explicitly%2520tied%2520to%2520the%2520computations%2520of%2520the%2520model.%2520Here%252C%250Awe%2520tackle%2520these%2520limitations%2520by%2520directly%2520decomposing%2520MLP%2520activations%2520with%250Asemi-nonnegative%2520matrix%2520factorization%2520%2528SNMF%2529%252C%2520such%2520that%2520the%2520learned%2520features%250Aare%2520%2528a%2529%2520sparse%2520linear%2520combinations%2520of%2520co-activated%2520neurons%252C%2520and%2520%2528b%2529%2520mapped%2520to%250Atheir%2520activating%2520inputs%252C%2520making%2520them%2520directly%2520interpretable.%2520Experiments%2520on%250ALlama%25203.1%252C%2520Gemma%25202%2520and%2520GPT-2%2520show%2520that%2520SNMF%2520derived%2520features%2520outperform%2520SAEs%250Aand%2520a%2520strong%2520supervised%2520baseline%2520%2528difference-in-means%2529%2520on%2520causal%2520steering%252C%250Awhile%2520aligning%2520with%2520human-interpretable%2520concepts.%2520Further%2520analysis%2520reveals%2520that%250Aspecific%2520neuron%2520combinations%2520are%2520reused%2520across%2520semantically-related%2520features%252C%250Aexposing%2520a%2520hierarchical%2520structure%2520in%2520the%2520MLP%2527s%2520activation%2520space.%2520Together%252C%250Athese%2520results%2520position%2520SNMF%2520as%2520a%2520simple%2520and%2520effective%2520tool%2520for%2520identifying%250Ainterpretable%2520features%2520and%2520dissecting%2520concept%2520representations%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20MLP%20Activations%20into%20Interpretable%20Features%20via%0A%20%20Semi-Nonnegative%20Matrix%20Factorization&entry.906535625=Or%20Shafran%20and%20Atticus%20Geiger%20and%20Mor%20Geva&entry.1292438233=%20%20A%20central%20goal%20for%20mechanistic%20interpretability%20has%20been%20to%20identify%20the%0Aright%20units%20of%20analysis%20in%20large%20language%20models%20%28LLMs%29%20that%20causally%20explain%0Atheir%20outputs.%20While%20early%20work%20focused%20on%20individual%20neurons%2C%20evidence%20that%0Aneurons%20often%20encode%20multiple%20concepts%20has%20motivated%20a%20shift%20toward%20analyzing%0Adirections%20in%20activation%20space.%20A%20key%20question%20is%20how%20to%20find%20directions%20that%0Acapture%20interpretable%20features%20in%20an%20unsupervised%20manner.%20Current%20methods%20rely%0Aon%20dictionary%20learning%20with%20sparse%20autoencoders%20%28SAEs%29%2C%20commonly%20trained%20over%0Aresidual%20stream%20activations%20to%20learn%20directions%20from%20scratch.%20However%2C%20SAEs%0Aoften%20struggle%20in%20causal%20evaluations%20and%20lack%20intrinsic%20interpretability%2C%20as%0Atheir%20learning%20is%20not%20explicitly%20tied%20to%20the%20computations%20of%20the%20model.%20Here%2C%0Awe%20tackle%20these%20limitations%20by%20directly%20decomposing%20MLP%20activations%20with%0Asemi-nonnegative%20matrix%20factorization%20%28SNMF%29%2C%20such%20that%20the%20learned%20features%0Aare%20%28a%29%20sparse%20linear%20combinations%20of%20co-activated%20neurons%2C%20and%20%28b%29%20mapped%20to%0Atheir%20activating%20inputs%2C%20making%20them%20directly%20interpretable.%20Experiments%20on%0ALlama%203.1%2C%20Gemma%202%20and%20GPT-2%20show%20that%20SNMF%20derived%20features%20outperform%20SAEs%0Aand%20a%20strong%20supervised%20baseline%20%28difference-in-means%29%20on%20causal%20steering%2C%0Awhile%20aligning%20with%20human-interpretable%20concepts.%20Further%20analysis%20reveals%20that%0Aspecific%20neuron%20combinations%20are%20reused%20across%20semantically-related%20features%2C%0Aexposing%20a%20hierarchical%20structure%20in%20the%20MLP%27s%20activation%20space.%20Together%2C%0Athese%20results%20position%20SNMF%20as%20a%20simple%20and%20effective%20tool%20for%20identifying%0Ainterpretable%20features%20and%20dissecting%20concept%20representations%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10920v1&entry.124074799=Read"},
{"title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models", "author": "Jari Kolehmainen and Nikolay Blagoev and John Donaghy and O\u011fuzhan Ersoy and Christopher Nies", "abstract": "  Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication training methods still employ\na synchronization step for model parameters, which, when performed over all\nmodel replicas, can become costly on a low-bandwidth network.\n  In this work, we propose a novel optimization method, NoLoCo, that does not\nexplicitly synchronize all model parameters during training and, as a result,\ndoes not require any collective communication. NoLoCo implicitly synchronizes\nmodel weights via a novel variant of the Nesterov momentum optimizer by\npartially averaging model weights with a randomly selected other one. We\nprovide both a theoretical convergence analysis for our proposed optimizer as\nwell as empirical results from language model training.\n  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,\nbetween 125M to 6.8B parameters. Our method requires significantly less\ncommunication overhead than fully sharded data parallel training or even widely\nused low communication training method, DiLoCo. The synchronization step itself\nis estimated to be one magnitude faster than the all-reduce used in DiLoCo for\nfew hundred accelerators training over the internet. We also do not have any\nglobal blocking communication that reduces accelerator idling time. Compared to\nDiLoCo, we also observe up to $4\\%$ faster convergence rate with wide range of\nmodel sizes and accelerator counts.\n", "link": "http://arxiv.org/abs/2506.10911v1", "date": "2025-06-12", "relevancy": 1.8516, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4715}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4702}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoLoCo%3A%20No-all-reduce%20Low%20Communication%20Training%20Method%20for%20Large%20Models&body=Title%3A%20NoLoCo%3A%20No-all-reduce%20Low%20Communication%20Training%20Method%20for%20Large%20Models%0AAuthor%3A%20Jari%20Kolehmainen%20and%20Nikolay%20Blagoev%20and%20John%20Donaghy%20and%20O%C4%9Fuzhan%20Ersoy%20and%20Christopher%20Nies%0AAbstract%3A%20%20%20Training%20large%20language%20models%20is%20generally%20done%20via%20optimization%20methods%20on%0Aclusters%20containing%20tens%20of%20thousands%20of%20accelerators%2C%20communicating%20over%20a%0Ahigh-bandwidth%20interconnect.%20Scaling%20up%20these%20clusters%20is%20expensive%20and%20can%0Abecome%20impractical%2C%20imposing%20limits%20on%20the%20size%20of%20models%20that%20can%20be%20trained.%0ASeveral%20recent%20studies%20have%20proposed%20training%20methods%20that%20are%20less%0Acommunication%20intensive%2C%20avoiding%20the%20need%20for%20a%20highly%20connected%20compute%0Acluster.%20These%20state-of-the-art%20low%20communication%20training%20methods%20still%20employ%0Aa%20synchronization%20step%20for%20model%20parameters%2C%20which%2C%20when%20performed%20over%20all%0Amodel%20replicas%2C%20can%20become%20costly%20on%20a%20low-bandwidth%20network.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20optimization%20method%2C%20NoLoCo%2C%20that%20does%20not%0Aexplicitly%20synchronize%20all%20model%20parameters%20during%20training%20and%2C%20as%20a%20result%2C%0Adoes%20not%20require%20any%20collective%20communication.%20NoLoCo%20implicitly%20synchronizes%0Amodel%20weights%20via%20a%20novel%20variant%20of%20the%20Nesterov%20momentum%20optimizer%20by%0Apartially%20averaging%20model%20weights%20with%20a%20randomly%20selected%20other%20one.%20We%0Aprovide%20both%20a%20theoretical%20convergence%20analysis%20for%20our%20proposed%20optimizer%20as%0Awell%20as%20empirical%20results%20from%20language%20model%20training.%0A%20%20We%20benchmark%20NoLoCo%20on%20a%20wide%20range%20of%20accelerator%20counts%20and%20model%20sizes%2C%0Abetween%20125M%20to%206.8B%20parameters.%20Our%20method%20requires%20significantly%20less%0Acommunication%20overhead%20than%20fully%20sharded%20data%20parallel%20training%20or%20even%20widely%0Aused%20low%20communication%20training%20method%2C%20DiLoCo.%20The%20synchronization%20step%20itself%0Ais%20estimated%20to%20be%20one%20magnitude%20faster%20than%20the%20all-reduce%20used%20in%20DiLoCo%20for%0Afew%20hundred%20accelerators%20training%20over%20the%20internet.%20We%20also%20do%20not%20have%20any%0Aglobal%20blocking%20communication%20that%20reduces%20accelerator%20idling%20time.%20Compared%20to%0ADiLoCo%2C%20we%20also%20observe%20up%20to%20%244%5C%25%24%20faster%20convergence%20rate%20with%20wide%20range%20of%0Amodel%20sizes%20and%20accelerator%20counts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoLoCo%253A%2520No-all-reduce%2520Low%2520Communication%2520Training%2520Method%2520for%2520Large%2520Models%26entry.906535625%3DJari%2520Kolehmainen%2520and%2520Nikolay%2520Blagoev%2520and%2520John%2520Donaghy%2520and%2520O%25C4%259Fuzhan%2520Ersoy%2520and%2520Christopher%2520Nies%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520is%2520generally%2520done%2520via%2520optimization%2520methods%2520on%250Aclusters%2520containing%2520tens%2520of%2520thousands%2520of%2520accelerators%252C%2520communicating%2520over%2520a%250Ahigh-bandwidth%2520interconnect.%2520Scaling%2520up%2520these%2520clusters%2520is%2520expensive%2520and%2520can%250Abecome%2520impractical%252C%2520imposing%2520limits%2520on%2520the%2520size%2520of%2520models%2520that%2520can%2520be%2520trained.%250ASeveral%2520recent%2520studies%2520have%2520proposed%2520training%2520methods%2520that%2520are%2520less%250Acommunication%2520intensive%252C%2520avoiding%2520the%2520need%2520for%2520a%2520highly%2520connected%2520compute%250Acluster.%2520These%2520state-of-the-art%2520low%2520communication%2520training%2520methods%2520still%2520employ%250Aa%2520synchronization%2520step%2520for%2520model%2520parameters%252C%2520which%252C%2520when%2520performed%2520over%2520all%250Amodel%2520replicas%252C%2520can%2520become%2520costly%2520on%2520a%2520low-bandwidth%2520network.%250A%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520optimization%2520method%252C%2520NoLoCo%252C%2520that%2520does%2520not%250Aexplicitly%2520synchronize%2520all%2520model%2520parameters%2520during%2520training%2520and%252C%2520as%2520a%2520result%252C%250Adoes%2520not%2520require%2520any%2520collective%2520communication.%2520NoLoCo%2520implicitly%2520synchronizes%250Amodel%2520weights%2520via%2520a%2520novel%2520variant%2520of%2520the%2520Nesterov%2520momentum%2520optimizer%2520by%250Apartially%2520averaging%2520model%2520weights%2520with%2520a%2520randomly%2520selected%2520other%2520one.%2520We%250Aprovide%2520both%2520a%2520theoretical%2520convergence%2520analysis%2520for%2520our%2520proposed%2520optimizer%2520as%250Awell%2520as%2520empirical%2520results%2520from%2520language%2520model%2520training.%250A%2520%2520We%2520benchmark%2520NoLoCo%2520on%2520a%2520wide%2520range%2520of%2520accelerator%2520counts%2520and%2520model%2520sizes%252C%250Abetween%2520125M%2520to%25206.8B%2520parameters.%2520Our%2520method%2520requires%2520significantly%2520less%250Acommunication%2520overhead%2520than%2520fully%2520sharded%2520data%2520parallel%2520training%2520or%2520even%2520widely%250Aused%2520low%2520communication%2520training%2520method%252C%2520DiLoCo.%2520The%2520synchronization%2520step%2520itself%250Ais%2520estimated%2520to%2520be%2520one%2520magnitude%2520faster%2520than%2520the%2520all-reduce%2520used%2520in%2520DiLoCo%2520for%250Afew%2520hundred%2520accelerators%2520training%2520over%2520the%2520internet.%2520We%2520also%2520do%2520not%2520have%2520any%250Aglobal%2520blocking%2520communication%2520that%2520reduces%2520accelerator%2520idling%2520time.%2520Compared%2520to%250ADiLoCo%252C%2520we%2520also%2520observe%2520up%2520to%2520%25244%255C%2525%2524%2520faster%2520convergence%2520rate%2520with%2520wide%2520range%2520of%250Amodel%2520sizes%2520and%2520accelerator%2520counts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoLoCo%3A%20No-all-reduce%20Low%20Communication%20Training%20Method%20for%20Large%20Models&entry.906535625=Jari%20Kolehmainen%20and%20Nikolay%20Blagoev%20and%20John%20Donaghy%20and%20O%C4%9Fuzhan%20Ersoy%20and%20Christopher%20Nies&entry.1292438233=%20%20Training%20large%20language%20models%20is%20generally%20done%20via%20optimization%20methods%20on%0Aclusters%20containing%20tens%20of%20thousands%20of%20accelerators%2C%20communicating%20over%20a%0Ahigh-bandwidth%20interconnect.%20Scaling%20up%20these%20clusters%20is%20expensive%20and%20can%0Abecome%20impractical%2C%20imposing%20limits%20on%20the%20size%20of%20models%20that%20can%20be%20trained.%0ASeveral%20recent%20studies%20have%20proposed%20training%20methods%20that%20are%20less%0Acommunication%20intensive%2C%20avoiding%20the%20need%20for%20a%20highly%20connected%20compute%0Acluster.%20These%20state-of-the-art%20low%20communication%20training%20methods%20still%20employ%0Aa%20synchronization%20step%20for%20model%20parameters%2C%20which%2C%20when%20performed%20over%20all%0Amodel%20replicas%2C%20can%20become%20costly%20on%20a%20low-bandwidth%20network.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20optimization%20method%2C%20NoLoCo%2C%20that%20does%20not%0Aexplicitly%20synchronize%20all%20model%20parameters%20during%20training%20and%2C%20as%20a%20result%2C%0Adoes%20not%20require%20any%20collective%20communication.%20NoLoCo%20implicitly%20synchronizes%0Amodel%20weights%20via%20a%20novel%20variant%20of%20the%20Nesterov%20momentum%20optimizer%20by%0Apartially%20averaging%20model%20weights%20with%20a%20randomly%20selected%20other%20one.%20We%0Aprovide%20both%20a%20theoretical%20convergence%20analysis%20for%20our%20proposed%20optimizer%20as%0Awell%20as%20empirical%20results%20from%20language%20model%20training.%0A%20%20We%20benchmark%20NoLoCo%20on%20a%20wide%20range%20of%20accelerator%20counts%20and%20model%20sizes%2C%0Abetween%20125M%20to%206.8B%20parameters.%20Our%20method%20requires%20significantly%20less%0Acommunication%20overhead%20than%20fully%20sharded%20data%20parallel%20training%20or%20even%20widely%0Aused%20low%20communication%20training%20method%2C%20DiLoCo.%20The%20synchronization%20step%20itself%0Ais%20estimated%20to%20be%20one%20magnitude%20faster%20than%20the%20all-reduce%20used%20in%20DiLoCo%20for%0Afew%20hundred%20accelerators%20training%20over%20the%20internet.%20We%20also%20do%20not%20have%20any%0Aglobal%20blocking%20communication%20that%20reduces%20accelerator%20idling%20time.%20Compared%20to%0ADiLoCo%2C%20we%20also%20observe%20up%20to%20%244%5C%25%24%20faster%20convergence%20rate%20with%20wide%20range%20of%0Amodel%20sizes%20and%20accelerator%20counts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10911v1&entry.124074799=Read"},
{"title": "Structure and asymptotic preserving deep neural surrogates for\n  uncertainty quantification in multiscale kinetic equations", "author": "Wei Chen and Giacomo Dimarco and Lorenzo Pareschi", "abstract": "  The high dimensionality of kinetic equations with stochastic parameters poses\nmajor computational challenges for uncertainty quantification (UQ). Traditional\nMonte Carlo (MC) sampling methods, while widely used, suffer from slow\nconvergence and high variance, which become increasingly severe as the\ndimensionality of the parameter space grows. To accelerate MC sampling, we\nadopt a multiscale control variates strategy that leverages low-fidelity\nsolutions from simplified kinetic models to reduce variance. To further improve\nsampling efficiency and preserve the underlying physics, we introduce surrogate\nmodels based on structure and asymptotic preserving neural networks (SAPNNs).\nThese deep neural networks are specifically designed to satisfy key physical\nproperties, including positivity, conservation laws, entropy dissipation, and\nasymptotic limits. By training the SAPNNs on low-fidelity models and enriching\nthem with selected high-fidelity samples from the full Boltzmann equation, our\nmethod achieves significant variance reduction while maintaining physical\nconsistency and asymptotic accuracy. The proposed methodology enables efficient\nlarge-scale prediction in kinetic UQ and is validated across both homogeneous\nand nonhomogeneous multiscale regimes. Numerical results demonstrate improved\naccuracy and computational efficiency compared to standard MC techniques.\n", "link": "http://arxiv.org/abs/2506.10636v1", "date": "2025-06-12", "relevancy": 1.5631, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5315}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5234}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20and%20asymptotic%20preserving%20deep%20neural%20surrogates%20for%0A%20%20uncertainty%20quantification%20in%20multiscale%20kinetic%20equations&body=Title%3A%20Structure%20and%20asymptotic%20preserving%20deep%20neural%20surrogates%20for%0A%20%20uncertainty%20quantification%20in%20multiscale%20kinetic%20equations%0AAuthor%3A%20Wei%20Chen%20and%20Giacomo%20Dimarco%20and%20Lorenzo%20Pareschi%0AAbstract%3A%20%20%20The%20high%20dimensionality%20of%20kinetic%20equations%20with%20stochastic%20parameters%20poses%0Amajor%20computational%20challenges%20for%20uncertainty%20quantification%20%28UQ%29.%20Traditional%0AMonte%20Carlo%20%28MC%29%20sampling%20methods%2C%20while%20widely%20used%2C%20suffer%20from%20slow%0Aconvergence%20and%20high%20variance%2C%20which%20become%20increasingly%20severe%20as%20the%0Adimensionality%20of%20the%20parameter%20space%20grows.%20To%20accelerate%20MC%20sampling%2C%20we%0Aadopt%20a%20multiscale%20control%20variates%20strategy%20that%20leverages%20low-fidelity%0Asolutions%20from%20simplified%20kinetic%20models%20to%20reduce%20variance.%20To%20further%20improve%0Asampling%20efficiency%20and%20preserve%20the%20underlying%20physics%2C%20we%20introduce%20surrogate%0Amodels%20based%20on%20structure%20and%20asymptotic%20preserving%20neural%20networks%20%28SAPNNs%29.%0AThese%20deep%20neural%20networks%20are%20specifically%20designed%20to%20satisfy%20key%20physical%0Aproperties%2C%20including%20positivity%2C%20conservation%20laws%2C%20entropy%20dissipation%2C%20and%0Aasymptotic%20limits.%20By%20training%20the%20SAPNNs%20on%20low-fidelity%20models%20and%20enriching%0Athem%20with%20selected%20high-fidelity%20samples%20from%20the%20full%20Boltzmann%20equation%2C%20our%0Amethod%20achieves%20significant%20variance%20reduction%20while%20maintaining%20physical%0Aconsistency%20and%20asymptotic%20accuracy.%20The%20proposed%20methodology%20enables%20efficient%0Alarge-scale%20prediction%20in%20kinetic%20UQ%20and%20is%20validated%20across%20both%20homogeneous%0Aand%20nonhomogeneous%20multiscale%20regimes.%20Numerical%20results%20demonstrate%20improved%0Aaccuracy%20and%20computational%20efficiency%20compared%20to%20standard%20MC%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520and%2520asymptotic%2520preserving%2520deep%2520neural%2520surrogates%2520for%250A%2520%2520uncertainty%2520quantification%2520in%2520multiscale%2520kinetic%2520equations%26entry.906535625%3DWei%2520Chen%2520and%2520Giacomo%2520Dimarco%2520and%2520Lorenzo%2520Pareschi%26entry.1292438233%3D%2520%2520The%2520high%2520dimensionality%2520of%2520kinetic%2520equations%2520with%2520stochastic%2520parameters%2520poses%250Amajor%2520computational%2520challenges%2520for%2520uncertainty%2520quantification%2520%2528UQ%2529.%2520Traditional%250AMonte%2520Carlo%2520%2528MC%2529%2520sampling%2520methods%252C%2520while%2520widely%2520used%252C%2520suffer%2520from%2520slow%250Aconvergence%2520and%2520high%2520variance%252C%2520which%2520become%2520increasingly%2520severe%2520as%2520the%250Adimensionality%2520of%2520the%2520parameter%2520space%2520grows.%2520To%2520accelerate%2520MC%2520sampling%252C%2520we%250Aadopt%2520a%2520multiscale%2520control%2520variates%2520strategy%2520that%2520leverages%2520low-fidelity%250Asolutions%2520from%2520simplified%2520kinetic%2520models%2520to%2520reduce%2520variance.%2520To%2520further%2520improve%250Asampling%2520efficiency%2520and%2520preserve%2520the%2520underlying%2520physics%252C%2520we%2520introduce%2520surrogate%250Amodels%2520based%2520on%2520structure%2520and%2520asymptotic%2520preserving%2520neural%2520networks%2520%2528SAPNNs%2529.%250AThese%2520deep%2520neural%2520networks%2520are%2520specifically%2520designed%2520to%2520satisfy%2520key%2520physical%250Aproperties%252C%2520including%2520positivity%252C%2520conservation%2520laws%252C%2520entropy%2520dissipation%252C%2520and%250Aasymptotic%2520limits.%2520By%2520training%2520the%2520SAPNNs%2520on%2520low-fidelity%2520models%2520and%2520enriching%250Athem%2520with%2520selected%2520high-fidelity%2520samples%2520from%2520the%2520full%2520Boltzmann%2520equation%252C%2520our%250Amethod%2520achieves%2520significant%2520variance%2520reduction%2520while%2520maintaining%2520physical%250Aconsistency%2520and%2520asymptotic%2520accuracy.%2520The%2520proposed%2520methodology%2520enables%2520efficient%250Alarge-scale%2520prediction%2520in%2520kinetic%2520UQ%2520and%2520is%2520validated%2520across%2520both%2520homogeneous%250Aand%2520nonhomogeneous%2520multiscale%2520regimes.%2520Numerical%2520results%2520demonstrate%2520improved%250Aaccuracy%2520and%2520computational%2520efficiency%2520compared%2520to%2520standard%2520MC%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20and%20asymptotic%20preserving%20deep%20neural%20surrogates%20for%0A%20%20uncertainty%20quantification%20in%20multiscale%20kinetic%20equations&entry.906535625=Wei%20Chen%20and%20Giacomo%20Dimarco%20and%20Lorenzo%20Pareschi&entry.1292438233=%20%20The%20high%20dimensionality%20of%20kinetic%20equations%20with%20stochastic%20parameters%20poses%0Amajor%20computational%20challenges%20for%20uncertainty%20quantification%20%28UQ%29.%20Traditional%0AMonte%20Carlo%20%28MC%29%20sampling%20methods%2C%20while%20widely%20used%2C%20suffer%20from%20slow%0Aconvergence%20and%20high%20variance%2C%20which%20become%20increasingly%20severe%20as%20the%0Adimensionality%20of%20the%20parameter%20space%20grows.%20To%20accelerate%20MC%20sampling%2C%20we%0Aadopt%20a%20multiscale%20control%20variates%20strategy%20that%20leverages%20low-fidelity%0Asolutions%20from%20simplified%20kinetic%20models%20to%20reduce%20variance.%20To%20further%20improve%0Asampling%20efficiency%20and%20preserve%20the%20underlying%20physics%2C%20we%20introduce%20surrogate%0Amodels%20based%20on%20structure%20and%20asymptotic%20preserving%20neural%20networks%20%28SAPNNs%29.%0AThese%20deep%20neural%20networks%20are%20specifically%20designed%20to%20satisfy%20key%20physical%0Aproperties%2C%20including%20positivity%2C%20conservation%20laws%2C%20entropy%20dissipation%2C%20and%0Aasymptotic%20limits.%20By%20training%20the%20SAPNNs%20on%20low-fidelity%20models%20and%20enriching%0Athem%20with%20selected%20high-fidelity%20samples%20from%20the%20full%20Boltzmann%20equation%2C%20our%0Amethod%20achieves%20significant%20variance%20reduction%20while%20maintaining%20physical%0Aconsistency%20and%20asymptotic%20accuracy.%20The%20proposed%20methodology%20enables%20efficient%0Alarge-scale%20prediction%20in%20kinetic%20UQ%20and%20is%20validated%20across%20both%20homogeneous%0Aand%20nonhomogeneous%20multiscale%20regimes.%20Numerical%20results%20demonstrate%20improved%0Aaccuracy%20and%20computational%20efficiency%20compared%20to%20standard%20MC%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10636v1&entry.124074799=Read"},
{"title": "Mimicking Human Intuition: Cognitive Belief-Driven Reinforcement\n  Learning", "author": "Xingrui Gu and Guanren Qiao and Chuyi Jiang", "abstract": "  Traditional reinforcement learning (RL) methods mainly rely on\ntrial-and-error exploration, often lacking mechanisms to guide agents toward\nmore informative decision-making and struggling to leverage past experiences,\nresulting in low sample efficiency. To overcome this issue, we propose an\ninnovative framework inspired by cognitive principles: Cognitive Belief-Driven\nReinforcement Learning (CBD-RL). By incorporating cognitive heuristics, CBD-RL\ntransforms conventional trial-and-error learning into a more structured and\nguided learning paradigm, simulating the human reasoning process. This\nframework's core is a belief system that optimizes action probabilities by\nintegrating feedback with prior experience, thus enhancing decision making\nunder uncertainty. It also organizes state-action pairs into meaningful\ncategories, promoting generalization and improving sample efficiency. The\nconcrete implementations of this framework, CBDQ, CBDPPO, and CBDSAC,\ndemonstrate superior performance in discrete and continuous action spaces in\ndiverse environments such as Atari and MuJoCo. By bridging cognitive science\nand reinforcement learning, this research opens a new avenue for developing RL\nsystems that are more interpretable, efficient, and cognitively inspired.\n", "link": "http://arxiv.org/abs/2410.01739v3", "date": "2025-06-12", "relevancy": 1.5174, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5266}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mimicking%20Human%20Intuition%3A%20Cognitive%20Belief-Driven%20Reinforcement%0A%20%20Learning&body=Title%3A%20Mimicking%20Human%20Intuition%3A%20Cognitive%20Belief-Driven%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Xingrui%20Gu%20and%20Guanren%20Qiao%20and%20Chuyi%20Jiang%0AAbstract%3A%20%20%20Traditional%20reinforcement%20learning%20%28RL%29%20methods%20mainly%20rely%20on%0Atrial-and-error%20exploration%2C%20often%20lacking%20mechanisms%20to%20guide%20agents%20toward%0Amore%20informative%20decision-making%20and%20struggling%20to%20leverage%20past%20experiences%2C%0Aresulting%20in%20low%20sample%20efficiency.%20To%20overcome%20this%20issue%2C%20we%20propose%20an%0Ainnovative%20framework%20inspired%20by%20cognitive%20principles%3A%20Cognitive%20Belief-Driven%0AReinforcement%20Learning%20%28CBD-RL%29.%20By%20incorporating%20cognitive%20heuristics%2C%20CBD-RL%0Atransforms%20conventional%20trial-and-error%20learning%20into%20a%20more%20structured%20and%0Aguided%20learning%20paradigm%2C%20simulating%20the%20human%20reasoning%20process.%20This%0Aframework%27s%20core%20is%20a%20belief%20system%20that%20optimizes%20action%20probabilities%20by%0Aintegrating%20feedback%20with%20prior%20experience%2C%20thus%20enhancing%20decision%20making%0Aunder%20uncertainty.%20It%20also%20organizes%20state-action%20pairs%20into%20meaningful%0Acategories%2C%20promoting%20generalization%20and%20improving%20sample%20efficiency.%20The%0Aconcrete%20implementations%20of%20this%20framework%2C%20CBDQ%2C%20CBDPPO%2C%20and%20CBDSAC%2C%0Ademonstrate%20superior%20performance%20in%20discrete%20and%20continuous%20action%20spaces%20in%0Adiverse%20environments%20such%20as%20Atari%20and%20MuJoCo.%20By%20bridging%20cognitive%20science%0Aand%20reinforcement%20learning%2C%20this%20research%20opens%20a%20new%20avenue%20for%20developing%20RL%0Asystems%20that%20are%20more%20interpretable%2C%20efficient%2C%20and%20cognitively%20inspired.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01739v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimicking%2520Human%2520Intuition%253A%2520Cognitive%2520Belief-Driven%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DXingrui%2520Gu%2520and%2520Guanren%2520Qiao%2520and%2520Chuyi%2520Jiang%26entry.1292438233%3D%2520%2520Traditional%2520reinforcement%2520learning%2520%2528RL%2529%2520methods%2520mainly%2520rely%2520on%250Atrial-and-error%2520exploration%252C%2520often%2520lacking%2520mechanisms%2520to%2520guide%2520agents%2520toward%250Amore%2520informative%2520decision-making%2520and%2520struggling%2520to%2520leverage%2520past%2520experiences%252C%250Aresulting%2520in%2520low%2520sample%2520efficiency.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520propose%2520an%250Ainnovative%2520framework%2520inspired%2520by%2520cognitive%2520principles%253A%2520Cognitive%2520Belief-Driven%250AReinforcement%2520Learning%2520%2528CBD-RL%2529.%2520By%2520incorporating%2520cognitive%2520heuristics%252C%2520CBD-RL%250Atransforms%2520conventional%2520trial-and-error%2520learning%2520into%2520a%2520more%2520structured%2520and%250Aguided%2520learning%2520paradigm%252C%2520simulating%2520the%2520human%2520reasoning%2520process.%2520This%250Aframework%2527s%2520core%2520is%2520a%2520belief%2520system%2520that%2520optimizes%2520action%2520probabilities%2520by%250Aintegrating%2520feedback%2520with%2520prior%2520experience%252C%2520thus%2520enhancing%2520decision%2520making%250Aunder%2520uncertainty.%2520It%2520also%2520organizes%2520state-action%2520pairs%2520into%2520meaningful%250Acategories%252C%2520promoting%2520generalization%2520and%2520improving%2520sample%2520efficiency.%2520The%250Aconcrete%2520implementations%2520of%2520this%2520framework%252C%2520CBDQ%252C%2520CBDPPO%252C%2520and%2520CBDSAC%252C%250Ademonstrate%2520superior%2520performance%2520in%2520discrete%2520and%2520continuous%2520action%2520spaces%2520in%250Adiverse%2520environments%2520such%2520as%2520Atari%2520and%2520MuJoCo.%2520By%2520bridging%2520cognitive%2520science%250Aand%2520reinforcement%2520learning%252C%2520this%2520research%2520opens%2520a%2520new%2520avenue%2520for%2520developing%2520RL%250Asystems%2520that%2520are%2520more%2520interpretable%252C%2520efficient%252C%2520and%2520cognitively%2520inspired.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01739v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mimicking%20Human%20Intuition%3A%20Cognitive%20Belief-Driven%20Reinforcement%0A%20%20Learning&entry.906535625=Xingrui%20Gu%20and%20Guanren%20Qiao%20and%20Chuyi%20Jiang&entry.1292438233=%20%20Traditional%20reinforcement%20learning%20%28RL%29%20methods%20mainly%20rely%20on%0Atrial-and-error%20exploration%2C%20often%20lacking%20mechanisms%20to%20guide%20agents%20toward%0Amore%20informative%20decision-making%20and%20struggling%20to%20leverage%20past%20experiences%2C%0Aresulting%20in%20low%20sample%20efficiency.%20To%20overcome%20this%20issue%2C%20we%20propose%20an%0Ainnovative%20framework%20inspired%20by%20cognitive%20principles%3A%20Cognitive%20Belief-Driven%0AReinforcement%20Learning%20%28CBD-RL%29.%20By%20incorporating%20cognitive%20heuristics%2C%20CBD-RL%0Atransforms%20conventional%20trial-and-error%20learning%20into%20a%20more%20structured%20and%0Aguided%20learning%20paradigm%2C%20simulating%20the%20human%20reasoning%20process.%20This%0Aframework%27s%20core%20is%20a%20belief%20system%20that%20optimizes%20action%20probabilities%20by%0Aintegrating%20feedback%20with%20prior%20experience%2C%20thus%20enhancing%20decision%20making%0Aunder%20uncertainty.%20It%20also%20organizes%20state-action%20pairs%20into%20meaningful%0Acategories%2C%20promoting%20generalization%20and%20improving%20sample%20efficiency.%20The%0Aconcrete%20implementations%20of%20this%20framework%2C%20CBDQ%2C%20CBDPPO%2C%20and%20CBDSAC%2C%0Ademonstrate%20superior%20performance%20in%20discrete%20and%20continuous%20action%20spaces%20in%0Adiverse%20environments%20such%20as%20Atari%20and%20MuJoCo.%20By%20bridging%20cognitive%20science%0Aand%20reinforcement%20learning%2C%20this%20research%20opens%20a%20new%20avenue%20for%20developing%20RL%0Asystems%20that%20are%20more%20interpretable%2C%20efficient%2C%20and%20cognitively%20inspired.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01739v3&entry.124074799=Read"},
{"title": "Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular\n  Detoxification?", "author": "Fei Lin and Ziyang Gong and Cong Wang and Yonglin Tian and Tengchao Zhang and Xue Yang and Gen Luo and Fei-Yue Wang", "abstract": "  Toxicity remains a leading cause of early-stage drug development failure.\nDespite advances in molecular design and property prediction, the task of\nmolecular toxicity repair - generating structurally valid molecular\nalternatives with reduced toxicity - has not yet been systematically defined or\nbenchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task\nfor general-purpose Multimodal Large Language Models (MLLMs) focused on\nmolecular toxicity repair. We construct a standardized dataset covering 11\nprimary tasks and 560 representative toxic molecules spanning diverse\nmechanisms and granularities. We design a prompt annotation pipeline with\nmechanism-aware and task-adaptive capabilities, informed by expert\ntoxicological knowledge. In parallel, we propose an automated evaluation\nframework, ToxiEval, which integrates toxicity endpoint prediction, synthetic\naccessibility, drug-likeness, and structural similarity into a high-throughput\nevaluation chain for repair success. We systematically assess nearly 30\nmainstream general-purpose MLLMs and design multiple ablation studies to\nanalyze key factors such as evaluation criteria, candidate diversity, and\nfailure attribution. Experimental results show that although current MLLMs\nstill face significant challenges on this task, they begin to demonstrate\npromising capabilities in toxicity understanding, semantic constraint\nadherence, and structure-aware molecule editing.\n", "link": "http://arxiv.org/abs/2506.10912v1", "date": "2025-06-12", "relevancy": 1.7285, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Bad%20Molecules%3A%20Are%20MLLMs%20Ready%20for%20Structure-Level%20Molecular%0A%20%20Detoxification%3F&body=Title%3A%20Breaking%20Bad%20Molecules%3A%20Are%20MLLMs%20Ready%20for%20Structure-Level%20Molecular%0A%20%20Detoxification%3F%0AAuthor%3A%20Fei%20Lin%20and%20Ziyang%20Gong%20and%20Cong%20Wang%20and%20Yonglin%20Tian%20and%20Tengchao%20Zhang%20and%20Xue%20Yang%20and%20Gen%20Luo%20and%20Fei-Yue%20Wang%0AAbstract%3A%20%20%20Toxicity%20remains%20a%20leading%20cause%20of%20early-stage%20drug%20development%20failure.%0ADespite%20advances%20in%20molecular%20design%20and%20property%20prediction%2C%20the%20task%20of%0Amolecular%20toxicity%20repair%20-%20generating%20structurally%20valid%20molecular%0Aalternatives%20with%20reduced%20toxicity%20-%20has%20not%20yet%20been%20systematically%20defined%20or%0Abenchmarked.%20To%20fill%20this%20gap%2C%20we%20introduce%20ToxiMol%2C%20the%20first%20benchmark%20task%0Afor%20general-purpose%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20focused%20on%0Amolecular%20toxicity%20repair.%20We%20construct%20a%20standardized%20dataset%20covering%2011%0Aprimary%20tasks%20and%20560%20representative%20toxic%20molecules%20spanning%20diverse%0Amechanisms%20and%20granularities.%20We%20design%20a%20prompt%20annotation%20pipeline%20with%0Amechanism-aware%20and%20task-adaptive%20capabilities%2C%20informed%20by%20expert%0Atoxicological%20knowledge.%20In%20parallel%2C%20we%20propose%20an%20automated%20evaluation%0Aframework%2C%20ToxiEval%2C%20which%20integrates%20toxicity%20endpoint%20prediction%2C%20synthetic%0Aaccessibility%2C%20drug-likeness%2C%20and%20structural%20similarity%20into%20a%20high-throughput%0Aevaluation%20chain%20for%20repair%20success.%20We%20systematically%20assess%20nearly%2030%0Amainstream%20general-purpose%20MLLMs%20and%20design%20multiple%20ablation%20studies%20to%0Aanalyze%20key%20factors%20such%20as%20evaluation%20criteria%2C%20candidate%20diversity%2C%20and%0Afailure%20attribution.%20Experimental%20results%20show%20that%20although%20current%20MLLMs%0Astill%20face%20significant%20challenges%20on%20this%20task%2C%20they%20begin%20to%20demonstrate%0Apromising%20capabilities%20in%20toxicity%20understanding%2C%20semantic%20constraint%0Aadherence%2C%20and%20structure-aware%20molecule%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Bad%2520Molecules%253A%2520Are%2520MLLMs%2520Ready%2520for%2520Structure-Level%2520Molecular%250A%2520%2520Detoxification%253F%26entry.906535625%3DFei%2520Lin%2520and%2520Ziyang%2520Gong%2520and%2520Cong%2520Wang%2520and%2520Yonglin%2520Tian%2520and%2520Tengchao%2520Zhang%2520and%2520Xue%2520Yang%2520and%2520Gen%2520Luo%2520and%2520Fei-Yue%2520Wang%26entry.1292438233%3D%2520%2520Toxicity%2520remains%2520a%2520leading%2520cause%2520of%2520early-stage%2520drug%2520development%2520failure.%250ADespite%2520advances%2520in%2520molecular%2520design%2520and%2520property%2520prediction%252C%2520the%2520task%2520of%250Amolecular%2520toxicity%2520repair%2520-%2520generating%2520structurally%2520valid%2520molecular%250Aalternatives%2520with%2520reduced%2520toxicity%2520-%2520has%2520not%2520yet%2520been%2520systematically%2520defined%2520or%250Abenchmarked.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520ToxiMol%252C%2520the%2520first%2520benchmark%2520task%250Afor%2520general-purpose%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520focused%2520on%250Amolecular%2520toxicity%2520repair.%2520We%2520construct%2520a%2520standardized%2520dataset%2520covering%252011%250Aprimary%2520tasks%2520and%2520560%2520representative%2520toxic%2520molecules%2520spanning%2520diverse%250Amechanisms%2520and%2520granularities.%2520We%2520design%2520a%2520prompt%2520annotation%2520pipeline%2520with%250Amechanism-aware%2520and%2520task-adaptive%2520capabilities%252C%2520informed%2520by%2520expert%250Atoxicological%2520knowledge.%2520In%2520parallel%252C%2520we%2520propose%2520an%2520automated%2520evaluation%250Aframework%252C%2520ToxiEval%252C%2520which%2520integrates%2520toxicity%2520endpoint%2520prediction%252C%2520synthetic%250Aaccessibility%252C%2520drug-likeness%252C%2520and%2520structural%2520similarity%2520into%2520a%2520high-throughput%250Aevaluation%2520chain%2520for%2520repair%2520success.%2520We%2520systematically%2520assess%2520nearly%252030%250Amainstream%2520general-purpose%2520MLLMs%2520and%2520design%2520multiple%2520ablation%2520studies%2520to%250Aanalyze%2520key%2520factors%2520such%2520as%2520evaluation%2520criteria%252C%2520candidate%2520diversity%252C%2520and%250Afailure%2520attribution.%2520Experimental%2520results%2520show%2520that%2520although%2520current%2520MLLMs%250Astill%2520face%2520significant%2520challenges%2520on%2520this%2520task%252C%2520they%2520begin%2520to%2520demonstrate%250Apromising%2520capabilities%2520in%2520toxicity%2520understanding%252C%2520semantic%2520constraint%250Aadherence%252C%2520and%2520structure-aware%2520molecule%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Bad%20Molecules%3A%20Are%20MLLMs%20Ready%20for%20Structure-Level%20Molecular%0A%20%20Detoxification%3F&entry.906535625=Fei%20Lin%20and%20Ziyang%20Gong%20and%20Cong%20Wang%20and%20Yonglin%20Tian%20and%20Tengchao%20Zhang%20and%20Xue%20Yang%20and%20Gen%20Luo%20and%20Fei-Yue%20Wang&entry.1292438233=%20%20Toxicity%20remains%20a%20leading%20cause%20of%20early-stage%20drug%20development%20failure.%0ADespite%20advances%20in%20molecular%20design%20and%20property%20prediction%2C%20the%20task%20of%0Amolecular%20toxicity%20repair%20-%20generating%20structurally%20valid%20molecular%0Aalternatives%20with%20reduced%20toxicity%20-%20has%20not%20yet%20been%20systematically%20defined%20or%0Abenchmarked.%20To%20fill%20this%20gap%2C%20we%20introduce%20ToxiMol%2C%20the%20first%20benchmark%20task%0Afor%20general-purpose%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20focused%20on%0Amolecular%20toxicity%20repair.%20We%20construct%20a%20standardized%20dataset%20covering%2011%0Aprimary%20tasks%20and%20560%20representative%20toxic%20molecules%20spanning%20diverse%0Amechanisms%20and%20granularities.%20We%20design%20a%20prompt%20annotation%20pipeline%20with%0Amechanism-aware%20and%20task-adaptive%20capabilities%2C%20informed%20by%20expert%0Atoxicological%20knowledge.%20In%20parallel%2C%20we%20propose%20an%20automated%20evaluation%0Aframework%2C%20ToxiEval%2C%20which%20integrates%20toxicity%20endpoint%20prediction%2C%20synthetic%0Aaccessibility%2C%20drug-likeness%2C%20and%20structural%20similarity%20into%20a%20high-throughput%0Aevaluation%20chain%20for%20repair%20success.%20We%20systematically%20assess%20nearly%2030%0Amainstream%20general-purpose%20MLLMs%20and%20design%20multiple%20ablation%20studies%20to%0Aanalyze%20key%20factors%20such%20as%20evaluation%20criteria%2C%20candidate%20diversity%2C%20and%0Afailure%20attribution.%20Experimental%20results%20show%20that%20although%20current%20MLLMs%0Astill%20face%20significant%20challenges%20on%20this%20task%2C%20they%20begin%20to%20demonstrate%0Apromising%20capabilities%20in%20toxicity%20understanding%2C%20semantic%20constraint%0Aadherence%2C%20and%20structure-aware%20molecule%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10912v1&entry.124074799=Read"},
{"title": "Symmetrical Flow Matching: Unified Image Generation, Segmentation, and\n  Classification with Score-Based Generative Models", "author": "Francisco Caetano and Christiaan Viviers and Peter H. N. De With and Fons van der Sommen", "abstract": "  Flow Matching has emerged as a powerful framework for learning continuous\ntransformations between distributions, enabling high-fidelity generative\nmodeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new\nformulation that unifies semantic segmentation, classification, and image\ngeneration within a single model. Using a symmetric learning objective,\nSymmFlow models forward and reverse transformations jointly, ensuring\nbi-directional consistency, while preserving sufficient entropy for generative\ndiversity. A new training objective is introduced to explicitly retain semantic\ninformation across flows, featuring efficient sampling while preserving\nsemantic structure, allowing for one-step segmentation and classification\nwithout iterative refinement. Unlike previous approaches that impose strict\none-to-one mapping between masks and images, SymmFlow generalizes to flexible\nconditioning, supporting both pixel-level and image-level class labels.\nExperimental results on various benchmarks demonstrate that SymmFlow achieves\nstate-of-the-art performance on semantic image synthesis, obtaining FID scores\nof 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps.\nAdditionally, it delivers competitive results on semantic segmentation and\nshows promising capabilities in classification tasks. The code will be publicly\navailable.\n", "link": "http://arxiv.org/abs/2506.10634v1", "date": "2025-06-12", "relevancy": 1.1206, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6228}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.553}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symmetrical%20Flow%20Matching%3A%20Unified%20Image%20Generation%2C%20Segmentation%2C%20and%0A%20%20Classification%20with%20Score-Based%20Generative%20Models&body=Title%3A%20Symmetrical%20Flow%20Matching%3A%20Unified%20Image%20Generation%2C%20Segmentation%2C%20and%0A%20%20Classification%20with%20Score-Based%20Generative%20Models%0AAuthor%3A%20Francisco%20Caetano%20and%20Christiaan%20Viviers%20and%20Peter%20H.%20N.%20De%20With%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20%20%20Flow%20Matching%20has%20emerged%20as%20a%20powerful%20framework%20for%20learning%20continuous%0Atransformations%20between%20distributions%2C%20enabling%20high-fidelity%20generative%0Amodeling.%20This%20work%20introduces%20Symmetrical%20Flow%20Matching%20%28SymmFlow%29%2C%20a%20new%0Aformulation%20that%20unifies%20semantic%20segmentation%2C%20classification%2C%20and%20image%0Ageneration%20within%20a%20single%20model.%20Using%20a%20symmetric%20learning%20objective%2C%0ASymmFlow%20models%20forward%20and%20reverse%20transformations%20jointly%2C%20ensuring%0Abi-directional%20consistency%2C%20while%20preserving%20sufficient%20entropy%20for%20generative%0Adiversity.%20A%20new%20training%20objective%20is%20introduced%20to%20explicitly%20retain%20semantic%0Ainformation%20across%20flows%2C%20featuring%20efficient%20sampling%20while%20preserving%0Asemantic%20structure%2C%20allowing%20for%20one-step%20segmentation%20and%20classification%0Awithout%20iterative%20refinement.%20Unlike%20previous%20approaches%20that%20impose%20strict%0Aone-to-one%20mapping%20between%20masks%20and%20images%2C%20SymmFlow%20generalizes%20to%20flexible%0Aconditioning%2C%20supporting%20both%20pixel-level%20and%20image-level%20class%20labels.%0AExperimental%20results%20on%20various%20benchmarks%20demonstrate%20that%20SymmFlow%20achieves%0Astate-of-the-art%20performance%20on%20semantic%20image%20synthesis%2C%20obtaining%20FID%20scores%0Aof%2011.9%20on%20CelebAMask-HQ%20and%207.0%20on%20COCO-Stuff%20with%20only%2025%20inference%20steps.%0AAdditionally%2C%20it%20delivers%20competitive%20results%20on%20semantic%20segmentation%20and%0Ashows%20promising%20capabilities%20in%20classification%20tasks.%20The%20code%20will%20be%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymmetrical%2520Flow%2520Matching%253A%2520Unified%2520Image%2520Generation%252C%2520Segmentation%252C%2520and%250A%2520%2520Classification%2520with%2520Score-Based%2520Generative%2520Models%26entry.906535625%3DFrancisco%2520Caetano%2520and%2520Christiaan%2520Viviers%2520and%2520Peter%2520H.%2520N.%2520De%2520With%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3D%2520%2520Flow%2520Matching%2520has%2520emerged%2520as%2520a%2520powerful%2520framework%2520for%2520learning%2520continuous%250Atransformations%2520between%2520distributions%252C%2520enabling%2520high-fidelity%2520generative%250Amodeling.%2520This%2520work%2520introduces%2520Symmetrical%2520Flow%2520Matching%2520%2528SymmFlow%2529%252C%2520a%2520new%250Aformulation%2520that%2520unifies%2520semantic%2520segmentation%252C%2520classification%252C%2520and%2520image%250Ageneration%2520within%2520a%2520single%2520model.%2520Using%2520a%2520symmetric%2520learning%2520objective%252C%250ASymmFlow%2520models%2520forward%2520and%2520reverse%2520transformations%2520jointly%252C%2520ensuring%250Abi-directional%2520consistency%252C%2520while%2520preserving%2520sufficient%2520entropy%2520for%2520generative%250Adiversity.%2520A%2520new%2520training%2520objective%2520is%2520introduced%2520to%2520explicitly%2520retain%2520semantic%250Ainformation%2520across%2520flows%252C%2520featuring%2520efficient%2520sampling%2520while%2520preserving%250Asemantic%2520structure%252C%2520allowing%2520for%2520one-step%2520segmentation%2520and%2520classification%250Awithout%2520iterative%2520refinement.%2520Unlike%2520previous%2520approaches%2520that%2520impose%2520strict%250Aone-to-one%2520mapping%2520between%2520masks%2520and%2520images%252C%2520SymmFlow%2520generalizes%2520to%2520flexible%250Aconditioning%252C%2520supporting%2520both%2520pixel-level%2520and%2520image-level%2520class%2520labels.%250AExperimental%2520results%2520on%2520various%2520benchmarks%2520demonstrate%2520that%2520SymmFlow%2520achieves%250Astate-of-the-art%2520performance%2520on%2520semantic%2520image%2520synthesis%252C%2520obtaining%2520FID%2520scores%250Aof%252011.9%2520on%2520CelebAMask-HQ%2520and%25207.0%2520on%2520COCO-Stuff%2520with%2520only%252025%2520inference%2520steps.%250AAdditionally%252C%2520it%2520delivers%2520competitive%2520results%2520on%2520semantic%2520segmentation%2520and%250Ashows%2520promising%2520capabilities%2520in%2520classification%2520tasks.%2520The%2520code%2520will%2520be%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetrical%20Flow%20Matching%3A%20Unified%20Image%20Generation%2C%20Segmentation%2C%20and%0A%20%20Classification%20with%20Score-Based%20Generative%20Models&entry.906535625=Francisco%20Caetano%20and%20Christiaan%20Viviers%20and%20Peter%20H.%20N.%20De%20With%20and%20Fons%20van%20der%20Sommen&entry.1292438233=%20%20Flow%20Matching%20has%20emerged%20as%20a%20powerful%20framework%20for%20learning%20continuous%0Atransformations%20between%20distributions%2C%20enabling%20high-fidelity%20generative%0Amodeling.%20This%20work%20introduces%20Symmetrical%20Flow%20Matching%20%28SymmFlow%29%2C%20a%20new%0Aformulation%20that%20unifies%20semantic%20segmentation%2C%20classification%2C%20and%20image%0Ageneration%20within%20a%20single%20model.%20Using%20a%20symmetric%20learning%20objective%2C%0ASymmFlow%20models%20forward%20and%20reverse%20transformations%20jointly%2C%20ensuring%0Abi-directional%20consistency%2C%20while%20preserving%20sufficient%20entropy%20for%20generative%0Adiversity.%20A%20new%20training%20objective%20is%20introduced%20to%20explicitly%20retain%20semantic%0Ainformation%20across%20flows%2C%20featuring%20efficient%20sampling%20while%20preserving%0Asemantic%20structure%2C%20allowing%20for%20one-step%20segmentation%20and%20classification%0Awithout%20iterative%20refinement.%20Unlike%20previous%20approaches%20that%20impose%20strict%0Aone-to-one%20mapping%20between%20masks%20and%20images%2C%20SymmFlow%20generalizes%20to%20flexible%0Aconditioning%2C%20supporting%20both%20pixel-level%20and%20image-level%20class%20labels.%0AExperimental%20results%20on%20various%20benchmarks%20demonstrate%20that%20SymmFlow%20achieves%0Astate-of-the-art%20performance%20on%20semantic%20image%20synthesis%2C%20obtaining%20FID%20scores%0Aof%2011.9%20on%20CelebAMask-HQ%20and%207.0%20on%20COCO-Stuff%20with%20only%2025%20inference%20steps.%0AAdditionally%2C%20it%20delivers%20competitive%20results%20on%20semantic%20segmentation%20and%0Ashows%20promising%20capabilities%20in%20classification%20tasks.%20The%20code%20will%20be%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10634v1&entry.124074799=Read"},
{"title": "Bandit Convex Optimisation", "author": "Tor Lattimore", "abstract": "  Bandit convex optimisation is a fundamental framework for studying\nzeroth-order convex optimisation. This book covers the many tools used for this\nproblem, including cutting plane methods, interior point methods, continuous\nexponential weights, gradient descent and online Newton step. The nuances\nbetween the many assumptions and setups are explained. Although there is not\nmuch truly new here, some existing tools are applied in novel ways to obtain\nnew algorithms. A few bounds are improved in minor ways.\n", "link": "http://arxiv.org/abs/2402.06535v4", "date": "2025-06-12", "relevancy": 1.1625, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4233}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3924}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bandit%20Convex%20Optimisation&body=Title%3A%20Bandit%20Convex%20Optimisation%0AAuthor%3A%20Tor%20Lattimore%0AAbstract%3A%20%20%20Bandit%20convex%20optimisation%20is%20a%20fundamental%20framework%20for%20studying%0Azeroth-order%20convex%20optimisation.%20This%20book%20covers%20the%20many%20tools%20used%20for%20this%0Aproblem%2C%20including%20cutting%20plane%20methods%2C%20interior%20point%20methods%2C%20continuous%0Aexponential%20weights%2C%20gradient%20descent%20and%20online%20Newton%20step.%20The%20nuances%0Abetween%20the%20many%20assumptions%20and%20setups%20are%20explained.%20Although%20there%20is%20not%0Amuch%20truly%20new%20here%2C%20some%20existing%20tools%20are%20applied%20in%20novel%20ways%20to%20obtain%0Anew%20algorithms.%20A%20few%20bounds%20are%20improved%20in%20minor%20ways.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06535v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBandit%2520Convex%2520Optimisation%26entry.906535625%3DTor%2520Lattimore%26entry.1292438233%3D%2520%2520Bandit%2520convex%2520optimisation%2520is%2520a%2520fundamental%2520framework%2520for%2520studying%250Azeroth-order%2520convex%2520optimisation.%2520This%2520book%2520covers%2520the%2520many%2520tools%2520used%2520for%2520this%250Aproblem%252C%2520including%2520cutting%2520plane%2520methods%252C%2520interior%2520point%2520methods%252C%2520continuous%250Aexponential%2520weights%252C%2520gradient%2520descent%2520and%2520online%2520Newton%2520step.%2520The%2520nuances%250Abetween%2520the%2520many%2520assumptions%2520and%2520setups%2520are%2520explained.%2520Although%2520there%2520is%2520not%250Amuch%2520truly%2520new%2520here%252C%2520some%2520existing%2520tools%2520are%2520applied%2520in%2520novel%2520ways%2520to%2520obtain%250Anew%2520algorithms.%2520A%2520few%2520bounds%2520are%2520improved%2520in%2520minor%2520ways.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06535v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bandit%20Convex%20Optimisation&entry.906535625=Tor%20Lattimore&entry.1292438233=%20%20Bandit%20convex%20optimisation%20is%20a%20fundamental%20framework%20for%20studying%0Azeroth-order%20convex%20optimisation.%20This%20book%20covers%20the%20many%20tools%20used%20for%20this%0Aproblem%2C%20including%20cutting%20plane%20methods%2C%20interior%20point%20methods%2C%20continuous%0Aexponential%20weights%2C%20gradient%20descent%20and%20online%20Newton%20step.%20The%20nuances%0Abetween%20the%20many%20assumptions%20and%20setups%20are%20explained.%20Although%20there%20is%20not%0Amuch%20truly%20new%20here%2C%20some%20existing%20tools%20are%20applied%20in%20novel%20ways%20to%20obtain%0Anew%20algorithms.%20A%20few%20bounds%20are%20improved%20in%20minor%20ways.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06535v4&entry.124074799=Read"},
{"title": "Flexible Tails for Normalizing Flows", "author": "Tennessee Hickling and Dennis Prangle", "abstract": "  Normalizing flows are a flexible class of probability distributions,\nexpressed as transformations of a simple base distribution. A limitation of\nstandard normalizing flows is representing distributions with heavy tails,\nwhich arise in applications to both density estimation and variational\ninference. A popular current solution to this problem is to use a heavy tailed\nbase distribution. We argue this can lead to poor performance due to the\ndifficulty of optimising neural networks, such as normalizing flows, under\nheavy tailed input. We propose an alternative, \"tail transform flow\" (TTF),\nwhich uses a Gaussian base distribution and a final transformation layer which\ncan produce heavy tails. Experimental results show this approach outperforms\ncurrent methods, especially when the target distribution has large dimension or\ntail weight.\n", "link": "http://arxiv.org/abs/2406.16971v2", "date": "2025-06-12", "relevancy": 1.9709, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5812}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4776}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flexible%20Tails%20for%20Normalizing%20Flows&body=Title%3A%20Flexible%20Tails%20for%20Normalizing%20Flows%0AAuthor%3A%20Tennessee%20Hickling%20and%20Dennis%20Prangle%0AAbstract%3A%20%20%20Normalizing%20flows%20are%20a%20flexible%20class%20of%20probability%20distributions%2C%0Aexpressed%20as%20transformations%20of%20a%20simple%20base%20distribution.%20A%20limitation%20of%0Astandard%20normalizing%20flows%20is%20representing%20distributions%20with%20heavy%20tails%2C%0Awhich%20arise%20in%20applications%20to%20both%20density%20estimation%20and%20variational%0Ainference.%20A%20popular%20current%20solution%20to%20this%20problem%20is%20to%20use%20a%20heavy%20tailed%0Abase%20distribution.%20We%20argue%20this%20can%20lead%20to%20poor%20performance%20due%20to%20the%0Adifficulty%20of%20optimising%20neural%20networks%2C%20such%20as%20normalizing%20flows%2C%20under%0Aheavy%20tailed%20input.%20We%20propose%20an%20alternative%2C%20%22tail%20transform%20flow%22%20%28TTF%29%2C%0Awhich%20uses%20a%20Gaussian%20base%20distribution%20and%20a%20final%20transformation%20layer%20which%0Acan%20produce%20heavy%20tails.%20Experimental%20results%20show%20this%20approach%20outperforms%0Acurrent%20methods%2C%20especially%20when%20the%20target%20distribution%20has%20large%20dimension%20or%0Atail%20weight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16971v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexible%2520Tails%2520for%2520Normalizing%2520Flows%26entry.906535625%3DTennessee%2520Hickling%2520and%2520Dennis%2520Prangle%26entry.1292438233%3D%2520%2520Normalizing%2520flows%2520are%2520a%2520flexible%2520class%2520of%2520probability%2520distributions%252C%250Aexpressed%2520as%2520transformations%2520of%2520a%2520simple%2520base%2520distribution.%2520A%2520limitation%2520of%250Astandard%2520normalizing%2520flows%2520is%2520representing%2520distributions%2520with%2520heavy%2520tails%252C%250Awhich%2520arise%2520in%2520applications%2520to%2520both%2520density%2520estimation%2520and%2520variational%250Ainference.%2520A%2520popular%2520current%2520solution%2520to%2520this%2520problem%2520is%2520to%2520use%2520a%2520heavy%2520tailed%250Abase%2520distribution.%2520We%2520argue%2520this%2520can%2520lead%2520to%2520poor%2520performance%2520due%2520to%2520the%250Adifficulty%2520of%2520optimising%2520neural%2520networks%252C%2520such%2520as%2520normalizing%2520flows%252C%2520under%250Aheavy%2520tailed%2520input.%2520We%2520propose%2520an%2520alternative%252C%2520%2522tail%2520transform%2520flow%2522%2520%2528TTF%2529%252C%250Awhich%2520uses%2520a%2520Gaussian%2520base%2520distribution%2520and%2520a%2520final%2520transformation%2520layer%2520which%250Acan%2520produce%2520heavy%2520tails.%2520Experimental%2520results%2520show%2520this%2520approach%2520outperforms%250Acurrent%2520methods%252C%2520especially%2520when%2520the%2520target%2520distribution%2520has%2520large%2520dimension%2520or%250Atail%2520weight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16971v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flexible%20Tails%20for%20Normalizing%20Flows&entry.906535625=Tennessee%20Hickling%20and%20Dennis%20Prangle&entry.1292438233=%20%20Normalizing%20flows%20are%20a%20flexible%20class%20of%20probability%20distributions%2C%0Aexpressed%20as%20transformations%20of%20a%20simple%20base%20distribution.%20A%20limitation%20of%0Astandard%20normalizing%20flows%20is%20representing%20distributions%20with%20heavy%20tails%2C%0Awhich%20arise%20in%20applications%20to%20both%20density%20estimation%20and%20variational%0Ainference.%20A%20popular%20current%20solution%20to%20this%20problem%20is%20to%20use%20a%20heavy%20tailed%0Abase%20distribution.%20We%20argue%20this%20can%20lead%20to%20poor%20performance%20due%20to%20the%0Adifficulty%20of%20optimising%20neural%20networks%2C%20such%20as%20normalizing%20flows%2C%20under%0Aheavy%20tailed%20input.%20We%20propose%20an%20alternative%2C%20%22tail%20transform%20flow%22%20%28TTF%29%2C%0Awhich%20uses%20a%20Gaussian%20base%20distribution%20and%20a%20final%20transformation%20layer%20which%0Acan%20produce%20heavy%20tails.%20Experimental%20results%20show%20this%20approach%20outperforms%0Acurrent%20methods%2C%20especially%20when%20the%20target%20distribution%20has%20large%20dimension%20or%0Atail%20weight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16971v2&entry.124074799=Read"},
{"title": "Energy-Efficient Deep Learning for Traffic Classification on\n  Microcontrollers", "author": "Adel Chehade and Edoardo Ragusa and Paolo Gastaldo and Rodolfo Zunino", "abstract": "  In this paper, we present a practical deep learning (DL) approach for\nenergy-efficient traffic classification (TC) on resource-limited\nmicrocontrollers, which are widely used in IoT-based smart systems and\ncommunication networks. Our objective is to balance accuracy, computational\nefficiency, and real-world deployability. To that end, we develop a lightweight\n1D-CNN, optimized via hardware-aware neural architecture search (HW-NAS), which\nachieves 96.59% accuracy on the ISCX VPN-NonVPN dataset with only 88.26K\nparameters, a 20.12K maximum tensor size, and 10.08M floating-point operations\n(FLOPs). Moreover, it generalizes across various TC tasks, with accuracies\nranging from 94% to 99%. To enable deployment, the model is quantized to INT8,\nsuffering only a marginal 1-2% accuracy drop relative to its Float32\ncounterpart. We evaluate real-world inference performance on two\nmicrocontrollers: the high-performance STM32F746G-DISCO and the cost-sensitive\nNucleo-F401RE. The deployed model achieves inference latencies of 31.43ms and\n115.40ms, with energy consumption of 7.86 mJ and 29.10 mJ per inference,\nrespectively. These results demonstrate the feasibility of on-device encrypted\ntraffic analysis, paving the way for scalable, low-power IoT security\nsolutions.\n", "link": "http://arxiv.org/abs/2506.10851v1", "date": "2025-06-12", "relevancy": 2.0206, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5518}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.531}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Efficient%20Deep%20Learning%20for%20Traffic%20Classification%20on%0A%20%20Microcontrollers&body=Title%3A%20Energy-Efficient%20Deep%20Learning%20for%20Traffic%20Classification%20on%0A%20%20Microcontrollers%0AAuthor%3A%20Adel%20Chehade%20and%20Edoardo%20Ragusa%20and%20Paolo%20Gastaldo%20and%20Rodolfo%20Zunino%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20practical%20deep%20learning%20%28DL%29%20approach%20for%0Aenergy-efficient%20traffic%20classification%20%28TC%29%20on%20resource-limited%0Amicrocontrollers%2C%20which%20are%20widely%20used%20in%20IoT-based%20smart%20systems%20and%0Acommunication%20networks.%20Our%20objective%20is%20to%20balance%20accuracy%2C%20computational%0Aefficiency%2C%20and%20real-world%20deployability.%20To%20that%20end%2C%20we%20develop%20a%20lightweight%0A1D-CNN%2C%20optimized%20via%20hardware-aware%20neural%20architecture%20search%20%28HW-NAS%29%2C%20which%0Aachieves%2096.59%25%20accuracy%20on%20the%20ISCX%20VPN-NonVPN%20dataset%20with%20only%2088.26K%0Aparameters%2C%20a%2020.12K%20maximum%20tensor%20size%2C%20and%2010.08M%20floating-point%20operations%0A%28FLOPs%29.%20Moreover%2C%20it%20generalizes%20across%20various%20TC%20tasks%2C%20with%20accuracies%0Aranging%20from%2094%25%20to%2099%25.%20To%20enable%20deployment%2C%20the%20model%20is%20quantized%20to%20INT8%2C%0Asuffering%20only%20a%20marginal%201-2%25%20accuracy%20drop%20relative%20to%20its%20Float32%0Acounterpart.%20We%20evaluate%20real-world%20inference%20performance%20on%20two%0Amicrocontrollers%3A%20the%20high-performance%20STM32F746G-DISCO%20and%20the%20cost-sensitive%0ANucleo-F401RE.%20The%20deployed%20model%20achieves%20inference%20latencies%20of%2031.43ms%20and%0A115.40ms%2C%20with%20energy%20consumption%20of%207.86%20mJ%20and%2029.10%20mJ%20per%20inference%2C%0Arespectively.%20These%20results%20demonstrate%20the%20feasibility%20of%20on-device%20encrypted%0Atraffic%20analysis%2C%20paving%20the%20way%20for%20scalable%2C%20low-power%20IoT%20security%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Efficient%2520Deep%2520Learning%2520for%2520Traffic%2520Classification%2520on%250A%2520%2520Microcontrollers%26entry.906535625%3DAdel%2520Chehade%2520and%2520Edoardo%2520Ragusa%2520and%2520Paolo%2520Gastaldo%2520and%2520Rodolfo%2520Zunino%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520practical%2520deep%2520learning%2520%2528DL%2529%2520approach%2520for%250Aenergy-efficient%2520traffic%2520classification%2520%2528TC%2529%2520on%2520resource-limited%250Amicrocontrollers%252C%2520which%2520are%2520widely%2520used%2520in%2520IoT-based%2520smart%2520systems%2520and%250Acommunication%2520networks.%2520Our%2520objective%2520is%2520to%2520balance%2520accuracy%252C%2520computational%250Aefficiency%252C%2520and%2520real-world%2520deployability.%2520To%2520that%2520end%252C%2520we%2520develop%2520a%2520lightweight%250A1D-CNN%252C%2520optimized%2520via%2520hardware-aware%2520neural%2520architecture%2520search%2520%2528HW-NAS%2529%252C%2520which%250Aachieves%252096.59%2525%2520accuracy%2520on%2520the%2520ISCX%2520VPN-NonVPN%2520dataset%2520with%2520only%252088.26K%250Aparameters%252C%2520a%252020.12K%2520maximum%2520tensor%2520size%252C%2520and%252010.08M%2520floating-point%2520operations%250A%2528FLOPs%2529.%2520Moreover%252C%2520it%2520generalizes%2520across%2520various%2520TC%2520tasks%252C%2520with%2520accuracies%250Aranging%2520from%252094%2525%2520to%252099%2525.%2520To%2520enable%2520deployment%252C%2520the%2520model%2520is%2520quantized%2520to%2520INT8%252C%250Asuffering%2520only%2520a%2520marginal%25201-2%2525%2520accuracy%2520drop%2520relative%2520to%2520its%2520Float32%250Acounterpart.%2520We%2520evaluate%2520real-world%2520inference%2520performance%2520on%2520two%250Amicrocontrollers%253A%2520the%2520high-performance%2520STM32F746G-DISCO%2520and%2520the%2520cost-sensitive%250ANucleo-F401RE.%2520The%2520deployed%2520model%2520achieves%2520inference%2520latencies%2520of%252031.43ms%2520and%250A115.40ms%252C%2520with%2520energy%2520consumption%2520of%25207.86%2520mJ%2520and%252029.10%2520mJ%2520per%2520inference%252C%250Arespectively.%2520These%2520results%2520demonstrate%2520the%2520feasibility%2520of%2520on-device%2520encrypted%250Atraffic%2520analysis%252C%2520paving%2520the%2520way%2520for%2520scalable%252C%2520low-power%2520IoT%2520security%250Asolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Efficient%20Deep%20Learning%20for%20Traffic%20Classification%20on%0A%20%20Microcontrollers&entry.906535625=Adel%20Chehade%20and%20Edoardo%20Ragusa%20and%20Paolo%20Gastaldo%20and%20Rodolfo%20Zunino&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20practical%20deep%20learning%20%28DL%29%20approach%20for%0Aenergy-efficient%20traffic%20classification%20%28TC%29%20on%20resource-limited%0Amicrocontrollers%2C%20which%20are%20widely%20used%20in%20IoT-based%20smart%20systems%20and%0Acommunication%20networks.%20Our%20objective%20is%20to%20balance%20accuracy%2C%20computational%0Aefficiency%2C%20and%20real-world%20deployability.%20To%20that%20end%2C%20we%20develop%20a%20lightweight%0A1D-CNN%2C%20optimized%20via%20hardware-aware%20neural%20architecture%20search%20%28HW-NAS%29%2C%20which%0Aachieves%2096.59%25%20accuracy%20on%20the%20ISCX%20VPN-NonVPN%20dataset%20with%20only%2088.26K%0Aparameters%2C%20a%2020.12K%20maximum%20tensor%20size%2C%20and%2010.08M%20floating-point%20operations%0A%28FLOPs%29.%20Moreover%2C%20it%20generalizes%20across%20various%20TC%20tasks%2C%20with%20accuracies%0Aranging%20from%2094%25%20to%2099%25.%20To%20enable%20deployment%2C%20the%20model%20is%20quantized%20to%20INT8%2C%0Asuffering%20only%20a%20marginal%201-2%25%20accuracy%20drop%20relative%20to%20its%20Float32%0Acounterpart.%20We%20evaluate%20real-world%20inference%20performance%20on%20two%0Amicrocontrollers%3A%20the%20high-performance%20STM32F746G-DISCO%20and%20the%20cost-sensitive%0ANucleo-F401RE.%20The%20deployed%20model%20achieves%20inference%20latencies%20of%2031.43ms%20and%0A115.40ms%2C%20with%20energy%20consumption%20of%207.86%20mJ%20and%2029.10%20mJ%20per%20inference%2C%0Arespectively.%20These%20results%20demonstrate%20the%20feasibility%20of%20on-device%20encrypted%0Atraffic%20analysis%2C%20paving%20the%20way%20for%20scalable%2C%20low-power%20IoT%20security%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10851v1&entry.124074799=Read"},
{"title": "BioNeMo Framework: a modular, high-performance library for AI model\n  development in drug discovery", "author": "Peter St. John and Dejun Lin and Polina Binder and Malcolm Greaves and Vega Shah and John St. John and Adrian Lange and Patrick Hsu and Rajesh Illango and Arvind Ramanathan and Anima Anandkumar and David H Brookes and Akosua Busia and Abhishaike Mahajan and Stephen Malina and Neha Prasad and Sam Sinai and Lindsay Edwards and Thomas Gaudelet and Cristian Regep and Martin Steinegger and Burkhard Rost and Alexander Brace and Kyle Hippe and Luca Naef and Keisuke Kamata and George Armstrong and Kevin Boyd and Zhonglin Cao and Han-Yi Chou and Simon Chu and Allan dos Santos Costa and Sajad Darabi and Eric Dawson and Kieran Didi and Cong Fu and Mario Geiger and Michelle Gill and Darren J Hsu and Gagan Kaushik and Maria Korshunova and Steven Kothen-Hill and Youhan Lee and Meng Liu and Micha Livne and Zachary McClure and Jonathan Mitchell and Alireza Moradzadeh and Ohad Mosafi and Youssef Nashed and Saee Paliwal and Yuxing Peng and Sara Rabhi and Farhad Ramezanghorbani and Danny Reidenbach and Camir Ricketts and Brian C Roland and Kushal Shah and Tyler Shimko and Hassan Sirelkhatim and Savitha Srinivasan and Abraham C Stern and Dorota Toczydlowska and Srimukh Prasad Veccham and Niccol\u00f2 Alberto Elia Venanzi and Anton Vorontsov and Jared Wilber and Isabel Wilkinson and Wei Jing Wong and Eva Xue and Cory Ye and Xin Yu and Yang Zhang and Guoqing Zhou and Becca Zandstein and Alejandro Chac\u00f2n and Prashant Sohani and Maximilian Stadler and Christian Hundt and Feiwen Zhu and Christian Dallago and Bruno Trentini and Emine Kucukbenli and Saee Paliwal and Timur Rvachov and Eddie Calleja and Johnny Israeli and Harry Clifford and Risto Haukioja and Nicholas Haemel and Kyle Tretina and Neha Tadimeti and Anthony B Costa", "abstract": "  Artificial Intelligence models encoding biology and chemistry are opening new\nroutes to high-throughput and high-quality in-silico drug development. However,\ntheir training increasingly relies on computational scale, with recent protein\nlanguage models (pLM) training on hundreds of graphical processing units\n(GPUs). We introduce the BioNeMo Framework to facilitate the training of\ncomputational biology and chemistry AI models across hundreds of GPUs. Its\nmodular design allows the integration of individual components, such as data\nloaders, into existing workflows and is open to community contributions. We\ndetail technical features of the BioNeMo Framework through use cases such as\npLM pre-training and fine-tuning. On 256 NVIDIA A100s, BioNeMo Framework trains\na three billion parameter BERT-based pLM on over one trillion tokens in 4.2\ndays. The BioNeMo Framework is open-source and free for everyone to use.\n", "link": "http://arxiv.org/abs/2411.10548v3", "date": "2025-06-12", "relevancy": 1.8945, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BioNeMo%20Framework%3A%20a%20modular%2C%20high-performance%20library%20for%20AI%20model%0A%20%20development%20in%20drug%20discovery&body=Title%3A%20BioNeMo%20Framework%3A%20a%20modular%2C%20high-performance%20library%20for%20AI%20model%0A%20%20development%20in%20drug%20discovery%0AAuthor%3A%20Peter%20St.%20John%20and%20Dejun%20Lin%20and%20Polina%20Binder%20and%20Malcolm%20Greaves%20and%20Vega%20Shah%20and%20John%20St.%20John%20and%20Adrian%20Lange%20and%20Patrick%20Hsu%20and%20Rajesh%20Illango%20and%20Arvind%20Ramanathan%20and%20Anima%20Anandkumar%20and%20David%20H%20Brookes%20and%20Akosua%20Busia%20and%20Abhishaike%20Mahajan%20and%20Stephen%20Malina%20and%20Neha%20Prasad%20and%20Sam%20Sinai%20and%20Lindsay%20Edwards%20and%20Thomas%20Gaudelet%20and%20Cristian%20Regep%20and%20Martin%20Steinegger%20and%20Burkhard%20Rost%20and%20Alexander%20Brace%20and%20Kyle%20Hippe%20and%20Luca%20Naef%20and%20Keisuke%20Kamata%20and%20George%20Armstrong%20and%20Kevin%20Boyd%20and%20Zhonglin%20Cao%20and%20Han-Yi%20Chou%20and%20Simon%20Chu%20and%20Allan%20dos%20Santos%20Costa%20and%20Sajad%20Darabi%20and%20Eric%20Dawson%20and%20Kieran%20Didi%20and%20Cong%20Fu%20and%20Mario%20Geiger%20and%20Michelle%20Gill%20and%20Darren%20J%20Hsu%20and%20Gagan%20Kaushik%20and%20Maria%20Korshunova%20and%20Steven%20Kothen-Hill%20and%20Youhan%20Lee%20and%20Meng%20Liu%20and%20Micha%20Livne%20and%20Zachary%20McClure%20and%20Jonathan%20Mitchell%20and%20Alireza%20Moradzadeh%20and%20Ohad%20Mosafi%20and%20Youssef%20Nashed%20and%20Saee%20Paliwal%20and%20Yuxing%20Peng%20and%20Sara%20Rabhi%20and%20Farhad%20Ramezanghorbani%20and%20Danny%20Reidenbach%20and%20Camir%20Ricketts%20and%20Brian%20C%20Roland%20and%20Kushal%20Shah%20and%20Tyler%20Shimko%20and%20Hassan%20Sirelkhatim%20and%20Savitha%20Srinivasan%20and%20Abraham%20C%20Stern%20and%20Dorota%20Toczydlowska%20and%20Srimukh%20Prasad%20Veccham%20and%20Niccol%C3%B2%20Alberto%20Elia%20Venanzi%20and%20Anton%20Vorontsov%20and%20Jared%20Wilber%20and%20Isabel%20Wilkinson%20and%20Wei%20Jing%20Wong%20and%20Eva%20Xue%20and%20Cory%20Ye%20and%20Xin%20Yu%20and%20Yang%20Zhang%20and%20Guoqing%20Zhou%20and%20Becca%20Zandstein%20and%20Alejandro%20Chac%C3%B2n%20and%20Prashant%20Sohani%20and%20Maximilian%20Stadler%20and%20Christian%20Hundt%20and%20Feiwen%20Zhu%20and%20Christian%20Dallago%20and%20Bruno%20Trentini%20and%20Emine%20Kucukbenli%20and%20Saee%20Paliwal%20and%20Timur%20Rvachov%20and%20Eddie%20Calleja%20and%20Johnny%20Israeli%20and%20Harry%20Clifford%20and%20Risto%20Haukioja%20and%20Nicholas%20Haemel%20and%20Kyle%20Tretina%20and%20Neha%20Tadimeti%20and%20Anthony%20B%20Costa%0AAbstract%3A%20%20%20Artificial%20Intelligence%20models%20encoding%20biology%20and%20chemistry%20are%20opening%20new%0Aroutes%20to%20high-throughput%20and%20high-quality%20in-silico%20drug%20development.%20However%2C%0Atheir%20training%20increasingly%20relies%20on%20computational%20scale%2C%20with%20recent%20protein%0Alanguage%20models%20%28pLM%29%20training%20on%20hundreds%20of%20graphical%20processing%20units%0A%28GPUs%29.%20We%20introduce%20the%20BioNeMo%20Framework%20to%20facilitate%20the%20training%20of%0Acomputational%20biology%20and%20chemistry%20AI%20models%20across%20hundreds%20of%20GPUs.%20Its%0Amodular%20design%20allows%20the%20integration%20of%20individual%20components%2C%20such%20as%20data%0Aloaders%2C%20into%20existing%20workflows%20and%20is%20open%20to%20community%20contributions.%20We%0Adetail%20technical%20features%20of%20the%20BioNeMo%20Framework%20through%20use%20cases%20such%20as%0ApLM%20pre-training%20and%20fine-tuning.%20On%20256%20NVIDIA%20A100s%2C%20BioNeMo%20Framework%20trains%0Aa%20three%20billion%20parameter%20BERT-based%20pLM%20on%20over%20one%20trillion%20tokens%20in%204.2%0Adays.%20The%20BioNeMo%20Framework%20is%20open-source%20and%20free%20for%20everyone%20to%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10548v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBioNeMo%2520Framework%253A%2520a%2520modular%252C%2520high-performance%2520library%2520for%2520AI%2520model%250A%2520%2520development%2520in%2520drug%2520discovery%26entry.906535625%3DPeter%2520St.%2520John%2520and%2520Dejun%2520Lin%2520and%2520Polina%2520Binder%2520and%2520Malcolm%2520Greaves%2520and%2520Vega%2520Shah%2520and%2520John%2520St.%2520John%2520and%2520Adrian%2520Lange%2520and%2520Patrick%2520Hsu%2520and%2520Rajesh%2520Illango%2520and%2520Arvind%2520Ramanathan%2520and%2520Anima%2520Anandkumar%2520and%2520David%2520H%2520Brookes%2520and%2520Akosua%2520Busia%2520and%2520Abhishaike%2520Mahajan%2520and%2520Stephen%2520Malina%2520and%2520Neha%2520Prasad%2520and%2520Sam%2520Sinai%2520and%2520Lindsay%2520Edwards%2520and%2520Thomas%2520Gaudelet%2520and%2520Cristian%2520Regep%2520and%2520Martin%2520Steinegger%2520and%2520Burkhard%2520Rost%2520and%2520Alexander%2520Brace%2520and%2520Kyle%2520Hippe%2520and%2520Luca%2520Naef%2520and%2520Keisuke%2520Kamata%2520and%2520George%2520Armstrong%2520and%2520Kevin%2520Boyd%2520and%2520Zhonglin%2520Cao%2520and%2520Han-Yi%2520Chou%2520and%2520Simon%2520Chu%2520and%2520Allan%2520dos%2520Santos%2520Costa%2520and%2520Sajad%2520Darabi%2520and%2520Eric%2520Dawson%2520and%2520Kieran%2520Didi%2520and%2520Cong%2520Fu%2520and%2520Mario%2520Geiger%2520and%2520Michelle%2520Gill%2520and%2520Darren%2520J%2520Hsu%2520and%2520Gagan%2520Kaushik%2520and%2520Maria%2520Korshunova%2520and%2520Steven%2520Kothen-Hill%2520and%2520Youhan%2520Lee%2520and%2520Meng%2520Liu%2520and%2520Micha%2520Livne%2520and%2520Zachary%2520McClure%2520and%2520Jonathan%2520Mitchell%2520and%2520Alireza%2520Moradzadeh%2520and%2520Ohad%2520Mosafi%2520and%2520Youssef%2520Nashed%2520and%2520Saee%2520Paliwal%2520and%2520Yuxing%2520Peng%2520and%2520Sara%2520Rabhi%2520and%2520Farhad%2520Ramezanghorbani%2520and%2520Danny%2520Reidenbach%2520and%2520Camir%2520Ricketts%2520and%2520Brian%2520C%2520Roland%2520and%2520Kushal%2520Shah%2520and%2520Tyler%2520Shimko%2520and%2520Hassan%2520Sirelkhatim%2520and%2520Savitha%2520Srinivasan%2520and%2520Abraham%2520C%2520Stern%2520and%2520Dorota%2520Toczydlowska%2520and%2520Srimukh%2520Prasad%2520Veccham%2520and%2520Niccol%25C3%25B2%2520Alberto%2520Elia%2520Venanzi%2520and%2520Anton%2520Vorontsov%2520and%2520Jared%2520Wilber%2520and%2520Isabel%2520Wilkinson%2520and%2520Wei%2520Jing%2520Wong%2520and%2520Eva%2520Xue%2520and%2520Cory%2520Ye%2520and%2520Xin%2520Yu%2520and%2520Yang%2520Zhang%2520and%2520Guoqing%2520Zhou%2520and%2520Becca%2520Zandstein%2520and%2520Alejandro%2520Chac%25C3%25B2n%2520and%2520Prashant%2520Sohani%2520and%2520Maximilian%2520Stadler%2520and%2520Christian%2520Hundt%2520and%2520Feiwen%2520Zhu%2520and%2520Christian%2520Dallago%2520and%2520Bruno%2520Trentini%2520and%2520Emine%2520Kucukbenli%2520and%2520Saee%2520Paliwal%2520and%2520Timur%2520Rvachov%2520and%2520Eddie%2520Calleja%2520and%2520Johnny%2520Israeli%2520and%2520Harry%2520Clifford%2520and%2520Risto%2520Haukioja%2520and%2520Nicholas%2520Haemel%2520and%2520Kyle%2520Tretina%2520and%2520Neha%2520Tadimeti%2520and%2520Anthony%2520B%2520Costa%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520models%2520encoding%2520biology%2520and%2520chemistry%2520are%2520opening%2520new%250Aroutes%2520to%2520high-throughput%2520and%2520high-quality%2520in-silico%2520drug%2520development.%2520However%252C%250Atheir%2520training%2520increasingly%2520relies%2520on%2520computational%2520scale%252C%2520with%2520recent%2520protein%250Alanguage%2520models%2520%2528pLM%2529%2520training%2520on%2520hundreds%2520of%2520graphical%2520processing%2520units%250A%2528GPUs%2529.%2520We%2520introduce%2520the%2520BioNeMo%2520Framework%2520to%2520facilitate%2520the%2520training%2520of%250Acomputational%2520biology%2520and%2520chemistry%2520AI%2520models%2520across%2520hundreds%2520of%2520GPUs.%2520Its%250Amodular%2520design%2520allows%2520the%2520integration%2520of%2520individual%2520components%252C%2520such%2520as%2520data%250Aloaders%252C%2520into%2520existing%2520workflows%2520and%2520is%2520open%2520to%2520community%2520contributions.%2520We%250Adetail%2520technical%2520features%2520of%2520the%2520BioNeMo%2520Framework%2520through%2520use%2520cases%2520such%2520as%250ApLM%2520pre-training%2520and%2520fine-tuning.%2520On%2520256%2520NVIDIA%2520A100s%252C%2520BioNeMo%2520Framework%2520trains%250Aa%2520three%2520billion%2520parameter%2520BERT-based%2520pLM%2520on%2520over%2520one%2520trillion%2520tokens%2520in%25204.2%250Adays.%2520The%2520BioNeMo%2520Framework%2520is%2520open-source%2520and%2520free%2520for%2520everyone%2520to%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10548v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioNeMo%20Framework%3A%20a%20modular%2C%20high-performance%20library%20for%20AI%20model%0A%20%20development%20in%20drug%20discovery&entry.906535625=Peter%20St.%20John%20and%20Dejun%20Lin%20and%20Polina%20Binder%20and%20Malcolm%20Greaves%20and%20Vega%20Shah%20and%20John%20St.%20John%20and%20Adrian%20Lange%20and%20Patrick%20Hsu%20and%20Rajesh%20Illango%20and%20Arvind%20Ramanathan%20and%20Anima%20Anandkumar%20and%20David%20H%20Brookes%20and%20Akosua%20Busia%20and%20Abhishaike%20Mahajan%20and%20Stephen%20Malina%20and%20Neha%20Prasad%20and%20Sam%20Sinai%20and%20Lindsay%20Edwards%20and%20Thomas%20Gaudelet%20and%20Cristian%20Regep%20and%20Martin%20Steinegger%20and%20Burkhard%20Rost%20and%20Alexander%20Brace%20and%20Kyle%20Hippe%20and%20Luca%20Naef%20and%20Keisuke%20Kamata%20and%20George%20Armstrong%20and%20Kevin%20Boyd%20and%20Zhonglin%20Cao%20and%20Han-Yi%20Chou%20and%20Simon%20Chu%20and%20Allan%20dos%20Santos%20Costa%20and%20Sajad%20Darabi%20and%20Eric%20Dawson%20and%20Kieran%20Didi%20and%20Cong%20Fu%20and%20Mario%20Geiger%20and%20Michelle%20Gill%20and%20Darren%20J%20Hsu%20and%20Gagan%20Kaushik%20and%20Maria%20Korshunova%20and%20Steven%20Kothen-Hill%20and%20Youhan%20Lee%20and%20Meng%20Liu%20and%20Micha%20Livne%20and%20Zachary%20McClure%20and%20Jonathan%20Mitchell%20and%20Alireza%20Moradzadeh%20and%20Ohad%20Mosafi%20and%20Youssef%20Nashed%20and%20Saee%20Paliwal%20and%20Yuxing%20Peng%20and%20Sara%20Rabhi%20and%20Farhad%20Ramezanghorbani%20and%20Danny%20Reidenbach%20and%20Camir%20Ricketts%20and%20Brian%20C%20Roland%20and%20Kushal%20Shah%20and%20Tyler%20Shimko%20and%20Hassan%20Sirelkhatim%20and%20Savitha%20Srinivasan%20and%20Abraham%20C%20Stern%20and%20Dorota%20Toczydlowska%20and%20Srimukh%20Prasad%20Veccham%20and%20Niccol%C3%B2%20Alberto%20Elia%20Venanzi%20and%20Anton%20Vorontsov%20and%20Jared%20Wilber%20and%20Isabel%20Wilkinson%20and%20Wei%20Jing%20Wong%20and%20Eva%20Xue%20and%20Cory%20Ye%20and%20Xin%20Yu%20and%20Yang%20Zhang%20and%20Guoqing%20Zhou%20and%20Becca%20Zandstein%20and%20Alejandro%20Chac%C3%B2n%20and%20Prashant%20Sohani%20and%20Maximilian%20Stadler%20and%20Christian%20Hundt%20and%20Feiwen%20Zhu%20and%20Christian%20Dallago%20and%20Bruno%20Trentini%20and%20Emine%20Kucukbenli%20and%20Saee%20Paliwal%20and%20Timur%20Rvachov%20and%20Eddie%20Calleja%20and%20Johnny%20Israeli%20and%20Harry%20Clifford%20and%20Risto%20Haukioja%20and%20Nicholas%20Haemel%20and%20Kyle%20Tretina%20and%20Neha%20Tadimeti%20and%20Anthony%20B%20Costa&entry.1292438233=%20%20Artificial%20Intelligence%20models%20encoding%20biology%20and%20chemistry%20are%20opening%20new%0Aroutes%20to%20high-throughput%20and%20high-quality%20in-silico%20drug%20development.%20However%2C%0Atheir%20training%20increasingly%20relies%20on%20computational%20scale%2C%20with%20recent%20protein%0Alanguage%20models%20%28pLM%29%20training%20on%20hundreds%20of%20graphical%20processing%20units%0A%28GPUs%29.%20We%20introduce%20the%20BioNeMo%20Framework%20to%20facilitate%20the%20training%20of%0Acomputational%20biology%20and%20chemistry%20AI%20models%20across%20hundreds%20of%20GPUs.%20Its%0Amodular%20design%20allows%20the%20integration%20of%20individual%20components%2C%20such%20as%20data%0Aloaders%2C%20into%20existing%20workflows%20and%20is%20open%20to%20community%20contributions.%20We%0Adetail%20technical%20features%20of%20the%20BioNeMo%20Framework%20through%20use%20cases%20such%20as%0ApLM%20pre-training%20and%20fine-tuning.%20On%20256%20NVIDIA%20A100s%2C%20BioNeMo%20Framework%20trains%0Aa%20three%20billion%20parameter%20BERT-based%20pLM%20on%20over%20one%20trillion%20tokens%20in%204.2%0Adays.%20The%20BioNeMo%20Framework%20is%20open-source%20and%20free%20for%20everyone%20to%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10548v3&entry.124074799=Read"},
{"title": "M4V: Multi-Modal Mamba for Text-to-Video Generation", "author": "Jiancheng Huang and Gengwei Zhang and Zequn Jie and Siyu Jiao and Yinlong Qian and Ling Chen and Yunchao Wei and Lin Ma", "abstract": "  Text-to-video generation has significantly enriched content creation and\nholds the potential to evolve into powerful world simulators. However, modeling\nthe vast spatiotemporal space remains computationally demanding, particularly\nwhen employing Transformers, which incur quadratic complexity in sequence\nprocessing and thus limit practical applications. Recent advancements in\nlinear-time sequence modeling, particularly the Mamba architecture, offer a\nmore efficient alternative. Nevertheless, its plain design limits its direct\napplicability to multi-modal and spatiotemporal video generation tasks. To\naddress these challenges, we introduce M4V, a Multi-Modal Mamba framework for\ntext-to-video generation. Specifically, we propose a multi-modal diffusion\nMamba (MM-DiM) block that enables seamless integration of multi-modal\ninformation and spatiotemporal modeling through a multi-modal token\nre-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45%\ncompared to the attention-based alternative when generating videos at\n768$\\times$1280 resolution. Additionally, to mitigate the visual quality\ndegradation in long-context autoregressive generation processes, we introduce a\nreward learning strategy that further enhances per-frame visual realism.\nExtensive experiments on text-to-video benchmarks demonstrate M4V's ability to\nproduce high-quality videos while significantly lowering computational costs.\nCode and models will be publicly available at\nhttps://huangjch526.github.io/M4V_project.\n", "link": "http://arxiv.org/abs/2506.10915v1", "date": "2025-06-12", "relevancy": 1.9009, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6582}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6446}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M4V%3A%20Multi-Modal%20Mamba%20for%20Text-to-Video%20Generation&body=Title%3A%20M4V%3A%20Multi-Modal%20Mamba%20for%20Text-to-Video%20Generation%0AAuthor%3A%20Jiancheng%20Huang%20and%20Gengwei%20Zhang%20and%20Zequn%20Jie%20and%20Siyu%20Jiao%20and%20Yinlong%20Qian%20and%20Ling%20Chen%20and%20Yunchao%20Wei%20and%20Lin%20Ma%0AAbstract%3A%20%20%20Text-to-video%20generation%20has%20significantly%20enriched%20content%20creation%20and%0Aholds%20the%20potential%20to%20evolve%20into%20powerful%20world%20simulators.%20However%2C%20modeling%0Athe%20vast%20spatiotemporal%20space%20remains%20computationally%20demanding%2C%20particularly%0Awhen%20employing%20Transformers%2C%20which%20incur%20quadratic%20complexity%20in%20sequence%0Aprocessing%20and%20thus%20limit%20practical%20applications.%20Recent%20advancements%20in%0Alinear-time%20sequence%20modeling%2C%20particularly%20the%20Mamba%20architecture%2C%20offer%20a%0Amore%20efficient%20alternative.%20Nevertheless%2C%20its%20plain%20design%20limits%20its%20direct%0Aapplicability%20to%20multi-modal%20and%20spatiotemporal%20video%20generation%20tasks.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20M4V%2C%20a%20Multi-Modal%20Mamba%20framework%20for%0Atext-to-video%20generation.%20Specifically%2C%20we%20propose%20a%20multi-modal%20diffusion%0AMamba%20%28MM-DiM%29%20block%20that%20enables%20seamless%20integration%20of%20multi-modal%0Ainformation%20and%20spatiotemporal%20modeling%20through%20a%20multi-modal%20token%0Are-composition%20design.%20As%20a%20result%2C%20the%20Mamba%20blocks%20in%20M4V%20reduce%20FLOPs%20by%2045%25%0Acompared%20to%20the%20attention-based%20alternative%20when%20generating%20videos%20at%0A768%24%5Ctimes%241280%20resolution.%20Additionally%2C%20to%20mitigate%20the%20visual%20quality%0Adegradation%20in%20long-context%20autoregressive%20generation%20processes%2C%20we%20introduce%20a%0Areward%20learning%20strategy%20that%20further%20enhances%20per-frame%20visual%20realism.%0AExtensive%20experiments%20on%20text-to-video%20benchmarks%20demonstrate%20M4V%27s%20ability%20to%0Aproduce%20high-quality%20videos%20while%20significantly%20lowering%20computational%20costs.%0ACode%20and%20models%20will%20be%20publicly%20available%20at%0Ahttps%3A//huangjch526.github.io/M4V_project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM4V%253A%2520Multi-Modal%2520Mamba%2520for%2520Text-to-Video%2520Generation%26entry.906535625%3DJiancheng%2520Huang%2520and%2520Gengwei%2520Zhang%2520and%2520Zequn%2520Jie%2520and%2520Siyu%2520Jiao%2520and%2520Yinlong%2520Qian%2520and%2520Ling%2520Chen%2520and%2520Yunchao%2520Wei%2520and%2520Lin%2520Ma%26entry.1292438233%3D%2520%2520Text-to-video%2520generation%2520has%2520significantly%2520enriched%2520content%2520creation%2520and%250Aholds%2520the%2520potential%2520to%2520evolve%2520into%2520powerful%2520world%2520simulators.%2520However%252C%2520modeling%250Athe%2520vast%2520spatiotemporal%2520space%2520remains%2520computationally%2520demanding%252C%2520particularly%250Awhen%2520employing%2520Transformers%252C%2520which%2520incur%2520quadratic%2520complexity%2520in%2520sequence%250Aprocessing%2520and%2520thus%2520limit%2520practical%2520applications.%2520Recent%2520advancements%2520in%250Alinear-time%2520sequence%2520modeling%252C%2520particularly%2520the%2520Mamba%2520architecture%252C%2520offer%2520a%250Amore%2520efficient%2520alternative.%2520Nevertheless%252C%2520its%2520plain%2520design%2520limits%2520its%2520direct%250Aapplicability%2520to%2520multi-modal%2520and%2520spatiotemporal%2520video%2520generation%2520tasks.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520M4V%252C%2520a%2520Multi-Modal%2520Mamba%2520framework%2520for%250Atext-to-video%2520generation.%2520Specifically%252C%2520we%2520propose%2520a%2520multi-modal%2520diffusion%250AMamba%2520%2528MM-DiM%2529%2520block%2520that%2520enables%2520seamless%2520integration%2520of%2520multi-modal%250Ainformation%2520and%2520spatiotemporal%2520modeling%2520through%2520a%2520multi-modal%2520token%250Are-composition%2520design.%2520As%2520a%2520result%252C%2520the%2520Mamba%2520blocks%2520in%2520M4V%2520reduce%2520FLOPs%2520by%252045%2525%250Acompared%2520to%2520the%2520attention-based%2520alternative%2520when%2520generating%2520videos%2520at%250A768%2524%255Ctimes%25241280%2520resolution.%2520Additionally%252C%2520to%2520mitigate%2520the%2520visual%2520quality%250Adegradation%2520in%2520long-context%2520autoregressive%2520generation%2520processes%252C%2520we%2520introduce%2520a%250Areward%2520learning%2520strategy%2520that%2520further%2520enhances%2520per-frame%2520visual%2520realism.%250AExtensive%2520experiments%2520on%2520text-to-video%2520benchmarks%2520demonstrate%2520M4V%2527s%2520ability%2520to%250Aproduce%2520high-quality%2520videos%2520while%2520significantly%2520lowering%2520computational%2520costs.%250ACode%2520and%2520models%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//huangjch526.github.io/M4V_project.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M4V%3A%20Multi-Modal%20Mamba%20for%20Text-to-Video%20Generation&entry.906535625=Jiancheng%20Huang%20and%20Gengwei%20Zhang%20and%20Zequn%20Jie%20and%20Siyu%20Jiao%20and%20Yinlong%20Qian%20and%20Ling%20Chen%20and%20Yunchao%20Wei%20and%20Lin%20Ma&entry.1292438233=%20%20Text-to-video%20generation%20has%20significantly%20enriched%20content%20creation%20and%0Aholds%20the%20potential%20to%20evolve%20into%20powerful%20world%20simulators.%20However%2C%20modeling%0Athe%20vast%20spatiotemporal%20space%20remains%20computationally%20demanding%2C%20particularly%0Awhen%20employing%20Transformers%2C%20which%20incur%20quadratic%20complexity%20in%20sequence%0Aprocessing%20and%20thus%20limit%20practical%20applications.%20Recent%20advancements%20in%0Alinear-time%20sequence%20modeling%2C%20particularly%20the%20Mamba%20architecture%2C%20offer%20a%0Amore%20efficient%20alternative.%20Nevertheless%2C%20its%20plain%20design%20limits%20its%20direct%0Aapplicability%20to%20multi-modal%20and%20spatiotemporal%20video%20generation%20tasks.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20M4V%2C%20a%20Multi-Modal%20Mamba%20framework%20for%0Atext-to-video%20generation.%20Specifically%2C%20we%20propose%20a%20multi-modal%20diffusion%0AMamba%20%28MM-DiM%29%20block%20that%20enables%20seamless%20integration%20of%20multi-modal%0Ainformation%20and%20spatiotemporal%20modeling%20through%20a%20multi-modal%20token%0Are-composition%20design.%20As%20a%20result%2C%20the%20Mamba%20blocks%20in%20M4V%20reduce%20FLOPs%20by%2045%25%0Acompared%20to%20the%20attention-based%20alternative%20when%20generating%20videos%20at%0A768%24%5Ctimes%241280%20resolution.%20Additionally%2C%20to%20mitigate%20the%20visual%20quality%0Adegradation%20in%20long-context%20autoregressive%20generation%20processes%2C%20we%20introduce%20a%0Areward%20learning%20strategy%20that%20further%20enhances%20per-frame%20visual%20realism.%0AExtensive%20experiments%20on%20text-to-video%20benchmarks%20demonstrate%20M4V%27s%20ability%20to%0Aproduce%20high-quality%20videos%20while%20significantly%20lowering%20computational%20costs.%0ACode%20and%20models%20will%20be%20publicly%20available%20at%0Ahttps%3A//huangjch526.github.io/M4V_project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10915v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


