<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240508.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "G-Loc: Tightly-coupled Graph Localization with Prior Topo-metric\n  Information", "author": "Lorenzo Montano-Oliv\u00e1n and Julio A. Placed and Luis Montano and Mar\u00eda T. L\u00e1zaro", "abstract": "  Localization in already mapped environments is a critical component in many\nrobotics and automotive applications, where previously acquired information can\nbe exploited along with sensor fusion to provide robust and accurate\nlocalization estimates. In this work, we offer a new perspective on map-based\nlocalization by reusing prior topological and metric information. Thus, we\nreformulate this long-studied problem to go beyond the mere use of metric maps.\nOur framework seamlessly integrates LiDAR, iner\\-tial and GNSS measurements,\nand scan-to-map registrations in a sliding window graph fashion, which allows\nto accommodate the uncertainty of each observation. The modularity of our\nframework allows it to work with different sensor configurations\n(\\textit{e.g.,} LiDAR resolutions, GNSS denial) and environmental conditions\n(\\textit{e.g.,} map-less regions, large environments). We have conducted\ndifferent validation experiments, including deployment in a real-world\nautomotive application, demonstrating the accuracy, efficiency, and versatility\nof our system in online localization.\n", "link": "http://arxiv.org/abs/2405.05059v1", "date": "2024-05-08", "relevancy": 3.0259, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6517}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5997}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-Loc%3A%20Tightly-coupled%20Graph%20Localization%20with%20Prior%20Topo-metric%0A%20%20Information&body=Title%3A%20G-Loc%3A%20Tightly-coupled%20Graph%20Localization%20with%20Prior%20Topo-metric%0A%20%20Information%0AAuthor%3A%20Lorenzo%20Montano-Oliv%C3%A1n%20and%20Julio%20A.%20Placed%20and%20Luis%20Montano%20and%20Mar%C3%ADa%20T.%20L%C3%A1zaro%0AAbstract%3A%20%20%20Localization%20in%20already%20mapped%20environments%20is%20a%20critical%20component%20in%20many%0Arobotics%20and%20automotive%20applications%2C%20where%20previously%20acquired%20information%20can%0Abe%20exploited%20along%20with%20sensor%20fusion%20to%20provide%20robust%20and%20accurate%0Alocalization%20estimates.%20In%20this%20work%2C%20we%20offer%20a%20new%20perspective%20on%20map-based%0Alocalization%20by%20reusing%20prior%20topological%20and%20metric%20information.%20Thus%2C%20we%0Areformulate%20this%20long-studied%20problem%20to%20go%20beyond%20the%20mere%20use%20of%20metric%20maps.%0AOur%20framework%20seamlessly%20integrates%20LiDAR%2C%20iner%5C-tial%20and%20GNSS%20measurements%2C%0Aand%20scan-to-map%20registrations%20in%20a%20sliding%20window%20graph%20fashion%2C%20which%20allows%0Ato%20accommodate%20the%20uncertainty%20of%20each%20observation.%20The%20modularity%20of%20our%0Aframework%20allows%20it%20to%20work%20with%20different%20sensor%20configurations%0A%28%5Ctextit%7Be.g.%2C%7D%20LiDAR%20resolutions%2C%20GNSS%20denial%29%20and%20environmental%20conditions%0A%28%5Ctextit%7Be.g.%2C%7D%20map-less%20regions%2C%20large%20environments%29.%20We%20have%20conducted%0Adifferent%20validation%20experiments%2C%20including%20deployment%20in%20a%20real-world%0Aautomotive%20application%2C%20demonstrating%20the%20accuracy%2C%20efficiency%2C%20and%20versatility%0Aof%20our%20system%20in%20online%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-Loc%253A%2520Tightly-coupled%2520Graph%2520Localization%2520with%2520Prior%2520Topo-metric%250A%2520%2520Information%26entry.906535625%3DLorenzo%2520Montano-Oliv%25C3%25A1n%2520and%2520Julio%2520A.%2520Placed%2520and%2520Luis%2520Montano%2520and%2520Mar%25C3%25ADa%2520T.%2520L%25C3%25A1zaro%26entry.1292438233%3D%2520%2520Localization%2520in%2520already%2520mapped%2520environments%2520is%2520a%2520critical%2520component%2520in%2520many%250Arobotics%2520and%2520automotive%2520applications%252C%2520where%2520previously%2520acquired%2520information%2520can%250Abe%2520exploited%2520along%2520with%2520sensor%2520fusion%2520to%2520provide%2520robust%2520and%2520accurate%250Alocalization%2520estimates.%2520In%2520this%2520work%252C%2520we%2520offer%2520a%2520new%2520perspective%2520on%2520map-based%250Alocalization%2520by%2520reusing%2520prior%2520topological%2520and%2520metric%2520information.%2520Thus%252C%2520we%250Areformulate%2520this%2520long-studied%2520problem%2520to%2520go%2520beyond%2520the%2520mere%2520use%2520of%2520metric%2520maps.%250AOur%2520framework%2520seamlessly%2520integrates%2520LiDAR%252C%2520iner%255C-tial%2520and%2520GNSS%2520measurements%252C%250Aand%2520scan-to-map%2520registrations%2520in%2520a%2520sliding%2520window%2520graph%2520fashion%252C%2520which%2520allows%250Ato%2520accommodate%2520the%2520uncertainty%2520of%2520each%2520observation.%2520The%2520modularity%2520of%2520our%250Aframework%2520allows%2520it%2520to%2520work%2520with%2520different%2520sensor%2520configurations%250A%2528%255Ctextit%257Be.g.%252C%257D%2520LiDAR%2520resolutions%252C%2520GNSS%2520denial%2529%2520and%2520environmental%2520conditions%250A%2528%255Ctextit%257Be.g.%252C%257D%2520map-less%2520regions%252C%2520large%2520environments%2529.%2520We%2520have%2520conducted%250Adifferent%2520validation%2520experiments%252C%2520including%2520deployment%2520in%2520a%2520real-world%250Aautomotive%2520application%252C%2520demonstrating%2520the%2520accuracy%252C%2520efficiency%252C%2520and%2520versatility%250Aof%2520our%2520system%2520in%2520online%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-Loc%3A%20Tightly-coupled%20Graph%20Localization%20with%20Prior%20Topo-metric%0A%20%20Information&entry.906535625=Lorenzo%20Montano-Oliv%C3%A1n%20and%20Julio%20A.%20Placed%20and%20Luis%20Montano%20and%20Mar%C3%ADa%20T.%20L%C3%A1zaro&entry.1292438233=%20%20Localization%20in%20already%20mapped%20environments%20is%20a%20critical%20component%20in%20many%0Arobotics%20and%20automotive%20applications%2C%20where%20previously%20acquired%20information%20can%0Abe%20exploited%20along%20with%20sensor%20fusion%20to%20provide%20robust%20and%20accurate%0Alocalization%20estimates.%20In%20this%20work%2C%20we%20offer%20a%20new%20perspective%20on%20map-based%0Alocalization%20by%20reusing%20prior%20topological%20and%20metric%20information.%20Thus%2C%20we%0Areformulate%20this%20long-studied%20problem%20to%20go%20beyond%20the%20mere%20use%20of%20metric%20maps.%0AOur%20framework%20seamlessly%20integrates%20LiDAR%2C%20iner%5C-tial%20and%20GNSS%20measurements%2C%0Aand%20scan-to-map%20registrations%20in%20a%20sliding%20window%20graph%20fashion%2C%20which%20allows%0Ato%20accommodate%20the%20uncertainty%20of%20each%20observation.%20The%20modularity%20of%20our%0Aframework%20allows%20it%20to%20work%20with%20different%20sensor%20configurations%0A%28%5Ctextit%7Be.g.%2C%7D%20LiDAR%20resolutions%2C%20GNSS%20denial%29%20and%20environmental%20conditions%0A%28%5Ctextit%7Be.g.%2C%7D%20map-less%20regions%2C%20large%20environments%29.%20We%20have%20conducted%0Adifferent%20validation%20experiments%2C%20including%20deployment%20in%20a%20real-world%0Aautomotive%20application%2C%20demonstrating%20the%20accuracy%2C%20efficiency%2C%20and%20versatility%0Aof%20our%20system%20in%20online%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05059v1&entry.124074799=Read"},
{"title": "VRMM: A Volumetric Relightable Morphable Head Model", "author": "Haotian Yang and Mingwu Zheng and Chongyang Ma and Yu-Kun Lai and Pengfei Wan and Haibin Huang", "abstract": "  In this paper, we introduce the Volumetric Relightable Morphable Model\n(VRMM), a novel volumetric and parametric facial prior for 3D face modeling.\nWhile recent volumetric prior models offer improvements over traditional\nmethods like 3D Morphable Models (3DMMs), they face challenges in model\nlearning and personalized reconstructions. Our VRMM overcomes these by\nemploying a novel training framework that efficiently disentangles and encodes\nlatent spaces of identity, expression, and lighting into low-dimensional\nrepresentations. This framework, designed with self-supervised learning,\nsignificantly reduces the constraints for training data, making it more\nfeasible in practice. The learned VRMM offers relighting capabilities and\nencompasses a comprehensive range of expressions. We demonstrate the\nversatility and effectiveness of VRMM through various applications like avatar\ngeneration, facial reconstruction, and animation. Additionally, we address the\ncommon issue of overfitting in generative volumetric models with a novel\nprior-preserving personalization framework based on VRMM. Such an approach\nenables high-quality 3D face reconstruction from even a single portrait input.\nOur experiments showcase the potential of VRMM to significantly enhance the\nfield of 3D face modeling.\n", "link": "http://arxiv.org/abs/2402.04101v2", "date": "2024-05-08", "relevancy": 2.96, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6203}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6203}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRMM%3A%20A%20Volumetric%20Relightable%20Morphable%20Head%20Model&body=Title%3A%20VRMM%3A%20A%20Volumetric%20Relightable%20Morphable%20Head%20Model%0AAuthor%3A%20Haotian%20Yang%20and%20Mingwu%20Zheng%20and%20Chongyang%20Ma%20and%20Yu-Kun%20Lai%20and%20Pengfei%20Wan%20and%20Haibin%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20the%20Volumetric%20Relightable%20Morphable%20Model%0A%28VRMM%29%2C%20a%20novel%20volumetric%20and%20parametric%20facial%20prior%20for%203D%20face%20modeling.%0AWhile%20recent%20volumetric%20prior%20models%20offer%20improvements%20over%20traditional%0Amethods%20like%203D%20Morphable%20Models%20%283DMMs%29%2C%20they%20face%20challenges%20in%20model%0Alearning%20and%20personalized%20reconstructions.%20Our%20VRMM%20overcomes%20these%20by%0Aemploying%20a%20novel%20training%20framework%20that%20efficiently%20disentangles%20and%20encodes%0Alatent%20spaces%20of%20identity%2C%20expression%2C%20and%20lighting%20into%20low-dimensional%0Arepresentations.%20This%20framework%2C%20designed%20with%20self-supervised%20learning%2C%0Asignificantly%20reduces%20the%20constraints%20for%20training%20data%2C%20making%20it%20more%0Afeasible%20in%20practice.%20The%20learned%20VRMM%20offers%20relighting%20capabilities%20and%0Aencompasses%20a%20comprehensive%20range%20of%20expressions.%20We%20demonstrate%20the%0Aversatility%20and%20effectiveness%20of%20VRMM%20through%20various%20applications%20like%20avatar%0Ageneration%2C%20facial%20reconstruction%2C%20and%20animation.%20Additionally%2C%20we%20address%20the%0Acommon%20issue%20of%20overfitting%20in%20generative%20volumetric%20models%20with%20a%20novel%0Aprior-preserving%20personalization%20framework%20based%20on%20VRMM.%20Such%20an%20approach%0Aenables%20high-quality%203D%20face%20reconstruction%20from%20even%20a%20single%20portrait%20input.%0AOur%20experiments%20showcase%20the%20potential%20of%20VRMM%20to%20significantly%20enhance%20the%0Afield%20of%203D%20face%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRMM%253A%2520A%2520Volumetric%2520Relightable%2520Morphable%2520Head%2520Model%26entry.906535625%3DHaotian%2520Yang%2520and%2520Mingwu%2520Zheng%2520and%2520Chongyang%2520Ma%2520and%2520Yu-Kun%2520Lai%2520and%2520Pengfei%2520Wan%2520and%2520Haibin%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Volumetric%2520Relightable%2520Morphable%2520Model%250A%2528VRMM%2529%252C%2520a%2520novel%2520volumetric%2520and%2520parametric%2520facial%2520prior%2520for%25203D%2520face%2520modeling.%250AWhile%2520recent%2520volumetric%2520prior%2520models%2520offer%2520improvements%2520over%2520traditional%250Amethods%2520like%25203D%2520Morphable%2520Models%2520%25283DMMs%2529%252C%2520they%2520face%2520challenges%2520in%2520model%250Alearning%2520and%2520personalized%2520reconstructions.%2520Our%2520VRMM%2520overcomes%2520these%2520by%250Aemploying%2520a%2520novel%2520training%2520framework%2520that%2520efficiently%2520disentangles%2520and%2520encodes%250Alatent%2520spaces%2520of%2520identity%252C%2520expression%252C%2520and%2520lighting%2520into%2520low-dimensional%250Arepresentations.%2520This%2520framework%252C%2520designed%2520with%2520self-supervised%2520learning%252C%250Asignificantly%2520reduces%2520the%2520constraints%2520for%2520training%2520data%252C%2520making%2520it%2520more%250Afeasible%2520in%2520practice.%2520The%2520learned%2520VRMM%2520offers%2520relighting%2520capabilities%2520and%250Aencompasses%2520a%2520comprehensive%2520range%2520of%2520expressions.%2520We%2520demonstrate%2520the%250Aversatility%2520and%2520effectiveness%2520of%2520VRMM%2520through%2520various%2520applications%2520like%2520avatar%250Ageneration%252C%2520facial%2520reconstruction%252C%2520and%2520animation.%2520Additionally%252C%2520we%2520address%2520the%250Acommon%2520issue%2520of%2520overfitting%2520in%2520generative%2520volumetric%2520models%2520with%2520a%2520novel%250Aprior-preserving%2520personalization%2520framework%2520based%2520on%2520VRMM.%2520Such%2520an%2520approach%250Aenables%2520high-quality%25203D%2520face%2520reconstruction%2520from%2520even%2520a%2520single%2520portrait%2520input.%250AOur%2520experiments%2520showcase%2520the%2520potential%2520of%2520VRMM%2520to%2520significantly%2520enhance%2520the%250Afield%2520of%25203D%2520face%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRMM%3A%20A%20Volumetric%20Relightable%20Morphable%20Head%20Model&entry.906535625=Haotian%20Yang%20and%20Mingwu%20Zheng%20and%20Chongyang%20Ma%20and%20Yu-Kun%20Lai%20and%20Pengfei%20Wan%20and%20Haibin%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20the%20Volumetric%20Relightable%20Morphable%20Model%0A%28VRMM%29%2C%20a%20novel%20volumetric%20and%20parametric%20facial%20prior%20for%203D%20face%20modeling.%0AWhile%20recent%20volumetric%20prior%20models%20offer%20improvements%20over%20traditional%0Amethods%20like%203D%20Morphable%20Models%20%283DMMs%29%2C%20they%20face%20challenges%20in%20model%0Alearning%20and%20personalized%20reconstructions.%20Our%20VRMM%20overcomes%20these%20by%0Aemploying%20a%20novel%20training%20framework%20that%20efficiently%20disentangles%20and%20encodes%0Alatent%20spaces%20of%20identity%2C%20expression%2C%20and%20lighting%20into%20low-dimensional%0Arepresentations.%20This%20framework%2C%20designed%20with%20self-supervised%20learning%2C%0Asignificantly%20reduces%20the%20constraints%20for%20training%20data%2C%20making%20it%20more%0Afeasible%20in%20practice.%20The%20learned%20VRMM%20offers%20relighting%20capabilities%20and%0Aencompasses%20a%20comprehensive%20range%20of%20expressions.%20We%20demonstrate%20the%0Aversatility%20and%20effectiveness%20of%20VRMM%20through%20various%20applications%20like%20avatar%0Ageneration%2C%20facial%20reconstruction%2C%20and%20animation.%20Additionally%2C%20we%20address%20the%0Acommon%20issue%20of%20overfitting%20in%20generative%20volumetric%20models%20with%20a%20novel%0Aprior-preserving%20personalization%20framework%20based%20on%20VRMM.%20Such%20an%20approach%0Aenables%20high-quality%203D%20face%20reconstruction%20from%20even%20a%20single%20portrait%20input.%0AOur%20experiments%20showcase%20the%20potential%20of%20VRMM%20to%20significantly%20enhance%20the%0Afield%20of%203D%20face%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04101v2&entry.124074799=Read"},
{"title": "TENet: Targetness Entanglement Incorporating with Multi-Scale Pooling\n  and Mutually-Guided Fusion for RGB-E Object Tracking", "author": "Pengcheng Shao and Tianyang Xu and Zhangyong Tang and Linze Li and Xiao-Jun Wu and Josef Kittler", "abstract": "  There is currently strong interest in improving visual object tracking by\naugmenting the RGB modality with the output of a visual event camera that is\nparticularly informative about the scene motion. However, existing approaches\nperform event feature extraction for RGB-E tracking using traditional\nappearance models, which have been optimised for RGB only tracking, without\nadapting it for the intrinsic characteristics of the event data. To address\nthis problem, we propose an Event backbone (Pooler), designed to obtain a\nhigh-quality feature representation that is cognisant of the innate\ncharacteristics of the event data, namely its sparsity. In particular,\nMulti-Scale Pooling is introduced to capture all the motion feature trends\nwithin event data through the utilisation of diverse pooling kernel sizes. The\nassociation between the derived RGB and event representations is established by\nan innovative module performing adaptive Mutually Guided Fusion (MGF).\nExtensive experimental results show that our method significantly outperforms\nstate-of-the-art trackers on two widely used RGB-E tracking datasets, including\nVisEvent and COESOT, where the precision and success rates on COESOT are\nimproved by 4.9% and 5.2%, respectively. Our code will be available at\nhttps://github.com/SSSpc333/TENet.\n", "link": "http://arxiv.org/abs/2405.05004v1", "date": "2024-05-08", "relevancy": 2.7712, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5866}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5543}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TENet%3A%20Targetness%20Entanglement%20Incorporating%20with%20Multi-Scale%20Pooling%0A%20%20and%20Mutually-Guided%20Fusion%20for%20RGB-E%20Object%20Tracking&body=Title%3A%20TENet%3A%20Targetness%20Entanglement%20Incorporating%20with%20Multi-Scale%20Pooling%0A%20%20and%20Mutually-Guided%20Fusion%20for%20RGB-E%20Object%20Tracking%0AAuthor%3A%20Pengcheng%20Shao%20and%20Tianyang%20Xu%20and%20Zhangyong%20Tang%20and%20Linze%20Li%20and%20Xiao-Jun%20Wu%20and%20Josef%20Kittler%0AAbstract%3A%20%20%20There%20is%20currently%20strong%20interest%20in%20improving%20visual%20object%20tracking%20by%0Aaugmenting%20the%20RGB%20modality%20with%20the%20output%20of%20a%20visual%20event%20camera%20that%20is%0Aparticularly%20informative%20about%20the%20scene%20motion.%20However%2C%20existing%20approaches%0Aperform%20event%20feature%20extraction%20for%20RGB-E%20tracking%20using%20traditional%0Aappearance%20models%2C%20which%20have%20been%20optimised%20for%20RGB%20only%20tracking%2C%20without%0Aadapting%20it%20for%20the%20intrinsic%20characteristics%20of%20the%20event%20data.%20To%20address%0Athis%20problem%2C%20we%20propose%20an%20Event%20backbone%20%28Pooler%29%2C%20designed%20to%20obtain%20a%0Ahigh-quality%20feature%20representation%20that%20is%20cognisant%20of%20the%20innate%0Acharacteristics%20of%20the%20event%20data%2C%20namely%20its%20sparsity.%20In%20particular%2C%0AMulti-Scale%20Pooling%20is%20introduced%20to%20capture%20all%20the%20motion%20feature%20trends%0Awithin%20event%20data%20through%20the%20utilisation%20of%20diverse%20pooling%20kernel%20sizes.%20The%0Aassociation%20between%20the%20derived%20RGB%20and%20event%20representations%20is%20established%20by%0Aan%20innovative%20module%20performing%20adaptive%20Mutually%20Guided%20Fusion%20%28MGF%29.%0AExtensive%20experimental%20results%20show%20that%20our%20method%20significantly%20outperforms%0Astate-of-the-art%20trackers%20on%20two%20widely%20used%20RGB-E%20tracking%20datasets%2C%20including%0AVisEvent%20and%20COESOT%2C%20where%20the%20precision%20and%20success%20rates%20on%20COESOT%20are%0Aimproved%20by%204.9%25%20and%205.2%25%2C%20respectively.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/SSSpc333/TENet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTENet%253A%2520Targetness%2520Entanglement%2520Incorporating%2520with%2520Multi-Scale%2520Pooling%250A%2520%2520and%2520Mutually-Guided%2520Fusion%2520for%2520RGB-E%2520Object%2520Tracking%26entry.906535625%3DPengcheng%2520Shao%2520and%2520Tianyang%2520Xu%2520and%2520Zhangyong%2520Tang%2520and%2520Linze%2520Li%2520and%2520Xiao-Jun%2520Wu%2520and%2520Josef%2520Kittler%26entry.1292438233%3D%2520%2520There%2520is%2520currently%2520strong%2520interest%2520in%2520improving%2520visual%2520object%2520tracking%2520by%250Aaugmenting%2520the%2520RGB%2520modality%2520with%2520the%2520output%2520of%2520a%2520visual%2520event%2520camera%2520that%2520is%250Aparticularly%2520informative%2520about%2520the%2520scene%2520motion.%2520However%252C%2520existing%2520approaches%250Aperform%2520event%2520feature%2520extraction%2520for%2520RGB-E%2520tracking%2520using%2520traditional%250Aappearance%2520models%252C%2520which%2520have%2520been%2520optimised%2520for%2520RGB%2520only%2520tracking%252C%2520without%250Aadapting%2520it%2520for%2520the%2520intrinsic%2520characteristics%2520of%2520the%2520event%2520data.%2520To%2520address%250Athis%2520problem%252C%2520we%2520propose%2520an%2520Event%2520backbone%2520%2528Pooler%2529%252C%2520designed%2520to%2520obtain%2520a%250Ahigh-quality%2520feature%2520representation%2520that%2520is%2520cognisant%2520of%2520the%2520innate%250Acharacteristics%2520of%2520the%2520event%2520data%252C%2520namely%2520its%2520sparsity.%2520In%2520particular%252C%250AMulti-Scale%2520Pooling%2520is%2520introduced%2520to%2520capture%2520all%2520the%2520motion%2520feature%2520trends%250Awithin%2520event%2520data%2520through%2520the%2520utilisation%2520of%2520diverse%2520pooling%2520kernel%2520sizes.%2520The%250Aassociation%2520between%2520the%2520derived%2520RGB%2520and%2520event%2520representations%2520is%2520established%2520by%250Aan%2520innovative%2520module%2520performing%2520adaptive%2520Mutually%2520Guided%2520Fusion%2520%2528MGF%2529.%250AExtensive%2520experimental%2520results%2520show%2520that%2520our%2520method%2520significantly%2520outperforms%250Astate-of-the-art%2520trackers%2520on%2520two%2520widely%2520used%2520RGB-E%2520tracking%2520datasets%252C%2520including%250AVisEvent%2520and%2520COESOT%252C%2520where%2520the%2520precision%2520and%2520success%2520rates%2520on%2520COESOT%2520are%250Aimproved%2520by%25204.9%2525%2520and%25205.2%2525%252C%2520respectively.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/SSSpc333/TENet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TENet%3A%20Targetness%20Entanglement%20Incorporating%20with%20Multi-Scale%20Pooling%0A%20%20and%20Mutually-Guided%20Fusion%20for%20RGB-E%20Object%20Tracking&entry.906535625=Pengcheng%20Shao%20and%20Tianyang%20Xu%20and%20Zhangyong%20Tang%20and%20Linze%20Li%20and%20Xiao-Jun%20Wu%20and%20Josef%20Kittler&entry.1292438233=%20%20There%20is%20currently%20strong%20interest%20in%20improving%20visual%20object%20tracking%20by%0Aaugmenting%20the%20RGB%20modality%20with%20the%20output%20of%20a%20visual%20event%20camera%20that%20is%0Aparticularly%20informative%20about%20the%20scene%20motion.%20However%2C%20existing%20approaches%0Aperform%20event%20feature%20extraction%20for%20RGB-E%20tracking%20using%20traditional%0Aappearance%20models%2C%20which%20have%20been%20optimised%20for%20RGB%20only%20tracking%2C%20without%0Aadapting%20it%20for%20the%20intrinsic%20characteristics%20of%20the%20event%20data.%20To%20address%0Athis%20problem%2C%20we%20propose%20an%20Event%20backbone%20%28Pooler%29%2C%20designed%20to%20obtain%20a%0Ahigh-quality%20feature%20representation%20that%20is%20cognisant%20of%20the%20innate%0Acharacteristics%20of%20the%20event%20data%2C%20namely%20its%20sparsity.%20In%20particular%2C%0AMulti-Scale%20Pooling%20is%20introduced%20to%20capture%20all%20the%20motion%20feature%20trends%0Awithin%20event%20data%20through%20the%20utilisation%20of%20diverse%20pooling%20kernel%20sizes.%20The%0Aassociation%20between%20the%20derived%20RGB%20and%20event%20representations%20is%20established%20by%0Aan%20innovative%20module%20performing%20adaptive%20Mutually%20Guided%20Fusion%20%28MGF%29.%0AExtensive%20experimental%20results%20show%20that%20our%20method%20significantly%20outperforms%0Astate-of-the-art%20trackers%20on%20two%20widely%20used%20RGB-E%20tracking%20datasets%2C%20including%0AVisEvent%20and%20COESOT%2C%20where%20the%20precision%20and%20success%20rates%20on%20COESOT%20are%0Aimproved%20by%204.9%25%20and%205.2%25%2C%20respectively.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/SSSpc333/TENet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05004v1&entry.124074799=Read"},
{"title": "Unsupervised Skin Feature Tracking with Deep Neural Networks", "author": "Jose Chang and Torbj\u00f6rn E. M. Nordling", "abstract": "  Facial feature tracking is essential in imaging ballistocardiography for\naccurate heart rate estimation and enables motor degradation quantification in\nParkinson's disease through skin feature tracking. While deep convolutional\nneural networks have shown remarkable accuracy in tracking tasks, they\ntypically require extensive labeled data for supervised training. Our proposed\npipeline employs a convolutional stacked autoencoder to match image crops with\na reference crop containing the target feature, learning deep feature encodings\nspecific to the object category in an unsupervised manner, thus reducing data\nrequirements. To overcome edge effects making the performance dependent on crop\nsize, we introduced a Gaussian weight on the residual errors of the pixels when\ncalculating the loss function. Training the autoencoder on facial images and\nvalidating its performance on manually labeled face and hand videos, our Deep\nFeature Encodings (DFE) method demonstrated superior tracking accuracy with a\nmean error ranging from 0.6 to 3.3 pixels, outperforming traditional methods\nlike SIFT, SURF, Lucas Kanade, and the latest transformers like PIPs++ and\nCoTracker. Overall, our unsupervised learning approach excels in tracking\nvarious skin features under significant motion conditions, providing superior\nfeature descriptors for tracking, matching, and image registration compared to\nboth traditional and state-of-the-art supervised learning methods.\n", "link": "http://arxiv.org/abs/2405.04943v1", "date": "2024-05-08", "relevancy": 2.7247, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5847}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5324}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Skin%20Feature%20Tracking%20with%20Deep%20Neural%20Networks&body=Title%3A%20Unsupervised%20Skin%20Feature%20Tracking%20with%20Deep%20Neural%20Networks%0AAuthor%3A%20Jose%20Chang%20and%20Torbj%C3%B6rn%20E.%20M.%20Nordling%0AAbstract%3A%20%20%20Facial%20feature%20tracking%20is%20essential%20in%20imaging%20ballistocardiography%20for%0Aaccurate%20heart%20rate%20estimation%20and%20enables%20motor%20degradation%20quantification%20in%0AParkinson%27s%20disease%20through%20skin%20feature%20tracking.%20While%20deep%20convolutional%0Aneural%20networks%20have%20shown%20remarkable%20accuracy%20in%20tracking%20tasks%2C%20they%0Atypically%20require%20extensive%20labeled%20data%20for%20supervised%20training.%20Our%20proposed%0Apipeline%20employs%20a%20convolutional%20stacked%20autoencoder%20to%20match%20image%20crops%20with%0Aa%20reference%20crop%20containing%20the%20target%20feature%2C%20learning%20deep%20feature%20encodings%0Aspecific%20to%20the%20object%20category%20in%20an%20unsupervised%20manner%2C%20thus%20reducing%20data%0Arequirements.%20To%20overcome%20edge%20effects%20making%20the%20performance%20dependent%20on%20crop%0Asize%2C%20we%20introduced%20a%20Gaussian%20weight%20on%20the%20residual%20errors%20of%20the%20pixels%20when%0Acalculating%20the%20loss%20function.%20Training%20the%20autoencoder%20on%20facial%20images%20and%0Avalidating%20its%20performance%20on%20manually%20labeled%20face%20and%20hand%20videos%2C%20our%20Deep%0AFeature%20Encodings%20%28DFE%29%20method%20demonstrated%20superior%20tracking%20accuracy%20with%20a%0Amean%20error%20ranging%20from%200.6%20to%203.3%20pixels%2C%20outperforming%20traditional%20methods%0Alike%20SIFT%2C%20SURF%2C%20Lucas%20Kanade%2C%20and%20the%20latest%20transformers%20like%20PIPs%2B%2B%20and%0ACoTracker.%20Overall%2C%20our%20unsupervised%20learning%20approach%20excels%20in%20tracking%0Avarious%20skin%20features%20under%20significant%20motion%20conditions%2C%20providing%20superior%0Afeature%20descriptors%20for%20tracking%2C%20matching%2C%20and%20image%20registration%20compared%20to%0Aboth%20traditional%20and%20state-of-the-art%20supervised%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Skin%2520Feature%2520Tracking%2520with%2520Deep%2520Neural%2520Networks%26entry.906535625%3DJose%2520Chang%2520and%2520Torbj%25C3%25B6rn%2520E.%2520M.%2520Nordling%26entry.1292438233%3D%2520%2520Facial%2520feature%2520tracking%2520is%2520essential%2520in%2520imaging%2520ballistocardiography%2520for%250Aaccurate%2520heart%2520rate%2520estimation%2520and%2520enables%2520motor%2520degradation%2520quantification%2520in%250AParkinson%2527s%2520disease%2520through%2520skin%2520feature%2520tracking.%2520While%2520deep%2520convolutional%250Aneural%2520networks%2520have%2520shown%2520remarkable%2520accuracy%2520in%2520tracking%2520tasks%252C%2520they%250Atypically%2520require%2520extensive%2520labeled%2520data%2520for%2520supervised%2520training.%2520Our%2520proposed%250Apipeline%2520employs%2520a%2520convolutional%2520stacked%2520autoencoder%2520to%2520match%2520image%2520crops%2520with%250Aa%2520reference%2520crop%2520containing%2520the%2520target%2520feature%252C%2520learning%2520deep%2520feature%2520encodings%250Aspecific%2520to%2520the%2520object%2520category%2520in%2520an%2520unsupervised%2520manner%252C%2520thus%2520reducing%2520data%250Arequirements.%2520To%2520overcome%2520edge%2520effects%2520making%2520the%2520performance%2520dependent%2520on%2520crop%250Asize%252C%2520we%2520introduced%2520a%2520Gaussian%2520weight%2520on%2520the%2520residual%2520errors%2520of%2520the%2520pixels%2520when%250Acalculating%2520the%2520loss%2520function.%2520Training%2520the%2520autoencoder%2520on%2520facial%2520images%2520and%250Avalidating%2520its%2520performance%2520on%2520manually%2520labeled%2520face%2520and%2520hand%2520videos%252C%2520our%2520Deep%250AFeature%2520Encodings%2520%2528DFE%2529%2520method%2520demonstrated%2520superior%2520tracking%2520accuracy%2520with%2520a%250Amean%2520error%2520ranging%2520from%25200.6%2520to%25203.3%2520pixels%252C%2520outperforming%2520traditional%2520methods%250Alike%2520SIFT%252C%2520SURF%252C%2520Lucas%2520Kanade%252C%2520and%2520the%2520latest%2520transformers%2520like%2520PIPs%252B%252B%2520and%250ACoTracker.%2520Overall%252C%2520our%2520unsupervised%2520learning%2520approach%2520excels%2520in%2520tracking%250Avarious%2520skin%2520features%2520under%2520significant%2520motion%2520conditions%252C%2520providing%2520superior%250Afeature%2520descriptors%2520for%2520tracking%252C%2520matching%252C%2520and%2520image%2520registration%2520compared%2520to%250Aboth%2520traditional%2520and%2520state-of-the-art%2520supervised%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Skin%20Feature%20Tracking%20with%20Deep%20Neural%20Networks&entry.906535625=Jose%20Chang%20and%20Torbj%C3%B6rn%20E.%20M.%20Nordling&entry.1292438233=%20%20Facial%20feature%20tracking%20is%20essential%20in%20imaging%20ballistocardiography%20for%0Aaccurate%20heart%20rate%20estimation%20and%20enables%20motor%20degradation%20quantification%20in%0AParkinson%27s%20disease%20through%20skin%20feature%20tracking.%20While%20deep%20convolutional%0Aneural%20networks%20have%20shown%20remarkable%20accuracy%20in%20tracking%20tasks%2C%20they%0Atypically%20require%20extensive%20labeled%20data%20for%20supervised%20training.%20Our%20proposed%0Apipeline%20employs%20a%20convolutional%20stacked%20autoencoder%20to%20match%20image%20crops%20with%0Aa%20reference%20crop%20containing%20the%20target%20feature%2C%20learning%20deep%20feature%20encodings%0Aspecific%20to%20the%20object%20category%20in%20an%20unsupervised%20manner%2C%20thus%20reducing%20data%0Arequirements.%20To%20overcome%20edge%20effects%20making%20the%20performance%20dependent%20on%20crop%0Asize%2C%20we%20introduced%20a%20Gaussian%20weight%20on%20the%20residual%20errors%20of%20the%20pixels%20when%0Acalculating%20the%20loss%20function.%20Training%20the%20autoencoder%20on%20facial%20images%20and%0Avalidating%20its%20performance%20on%20manually%20labeled%20face%20and%20hand%20videos%2C%20our%20Deep%0AFeature%20Encodings%20%28DFE%29%20method%20demonstrated%20superior%20tracking%20accuracy%20with%20a%0Amean%20error%20ranging%20from%200.6%20to%203.3%20pixels%2C%20outperforming%20traditional%20methods%0Alike%20SIFT%2C%20SURF%2C%20Lucas%20Kanade%2C%20and%20the%20latest%20transformers%20like%20PIPs%2B%2B%20and%0ACoTracker.%20Overall%2C%20our%20unsupervised%20learning%20approach%20excels%20in%20tracking%0Avarious%20skin%20features%20under%20significant%20motion%20conditions%2C%20providing%20superior%0Afeature%20descriptors%20for%20tracking%2C%20matching%2C%20and%20image%20registration%20compared%20to%0Aboth%20traditional%20and%20state-of-the-art%20supervised%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04943v1&entry.124074799=Read"},
{"title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion\n  Models", "author": "Hongjie Wang and Difan Liu and Yan Kang and Yijun Li and Zhe Lin and Niraj K. Jha and Yuchen Liu", "abstract": "  Diffusion Models (DMs) have exhibited superior performance in generating\nhigh-quality and diverse images. However, this exceptional performance comes at\nthe cost of expensive architectural design, particularly due to the attention\nmodule heavily used in leading models. Existing works mainly adopt a retraining\nprocess to enhance DM efficiency. This is computationally expensive and not\nvery scalable. To this end, we introduce the Attention-driven Training-free\nEfficient Diffusion Model (AT-EDM) framework that leverages attention maps to\nperform run-time pruning of redundant tokens, without the need for any\nretraining. Specifically, for single-denoising-step pruning, we develop a novel\nranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify\nredundant tokens, and a similarity-based recovery method to restore tokens for\nthe convolution operation. In addition, we propose a Denoising-Steps-Aware\nPruning (DSAP) approach to adjust the pruning budget across different denoising\ntimesteps for better generation quality. Extensive evaluations show that AT-EDM\nperforms favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs\nsaving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining\nnearly the same FID and CLIP scores as the full model. Project webpage:\nhttps://atedm.github.io.\n", "link": "http://arxiv.org/abs/2405.05252v1", "date": "2024-05-08", "relevancy": 2.7058, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.714}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7015}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-Driven%20Training-Free%20Efficiency%20Enhancement%20of%20Diffusion%0A%20%20Models&body=Title%3A%20Attention-Driven%20Training-Free%20Efficiency%20Enhancement%20of%20Diffusion%0A%20%20Models%0AAuthor%3A%20Hongjie%20Wang%20and%20Difan%20Liu%20and%20Yan%20Kang%20and%20Yijun%20Li%20and%20Zhe%20Lin%20and%20Niraj%20K.%20Jha%20and%20Yuchen%20Liu%0AAbstract%3A%20%20%20Diffusion%20Models%20%28DMs%29%20have%20exhibited%20superior%20performance%20in%20generating%0Ahigh-quality%20and%20diverse%20images.%20However%2C%20this%20exceptional%20performance%20comes%20at%0Athe%20cost%20of%20expensive%20architectural%20design%2C%20particularly%20due%20to%20the%20attention%0Amodule%20heavily%20used%20in%20leading%20models.%20Existing%20works%20mainly%20adopt%20a%20retraining%0Aprocess%20to%20enhance%20DM%20efficiency.%20This%20is%20computationally%20expensive%20and%20not%0Avery%20scalable.%20To%20this%20end%2C%20we%20introduce%20the%20Attention-driven%20Training-free%0AEfficient%20Diffusion%20Model%20%28AT-EDM%29%20framework%20that%20leverages%20attention%20maps%20to%0Aperform%20run-time%20pruning%20of%20redundant%20tokens%2C%20without%20the%20need%20for%20any%0Aretraining.%20Specifically%2C%20for%20single-denoising-step%20pruning%2C%20we%20develop%20a%20novel%0Aranking%20algorithm%2C%20Generalized%20Weighted%20Page%20Rank%20%28G-WPR%29%2C%20to%20identify%0Aredundant%20tokens%2C%20and%20a%20similarity-based%20recovery%20method%20to%20restore%20tokens%20for%0Athe%20convolution%20operation.%20In%20addition%2C%20we%20propose%20a%20Denoising-Steps-Aware%0APruning%20%28DSAP%29%20approach%20to%20adjust%20the%20pruning%20budget%20across%20different%20denoising%0Atimesteps%20for%20better%20generation%20quality.%20Extensive%20evaluations%20show%20that%20AT-EDM%0Aperforms%20favorably%20against%20prior%20art%20in%20terms%20of%20efficiency%20%28e.g.%2C%2038.8%25%20FLOPs%0Asaving%20and%20up%20to%201.53x%20speed-up%20over%20Stable%20Diffusion%20XL%29%20while%20maintaining%0Anearly%20the%20same%20FID%20and%20CLIP%20scores%20as%20the%20full%20model.%20Project%20webpage%3A%0Ahttps%3A//atedm.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-Driven%2520Training-Free%2520Efficiency%2520Enhancement%2520of%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DHongjie%2520Wang%2520and%2520Difan%2520Liu%2520and%2520Yan%2520Kang%2520and%2520Yijun%2520Li%2520and%2520Zhe%2520Lin%2520and%2520Niraj%2520K.%2520Jha%2520and%2520Yuchen%2520Liu%26entry.1292438233%3D%2520%2520Diffusion%2520Models%2520%2528DMs%2529%2520have%2520exhibited%2520superior%2520performance%2520in%2520generating%250Ahigh-quality%2520and%2520diverse%2520images.%2520However%252C%2520this%2520exceptional%2520performance%2520comes%2520at%250Athe%2520cost%2520of%2520expensive%2520architectural%2520design%252C%2520particularly%2520due%2520to%2520the%2520attention%250Amodule%2520heavily%2520used%2520in%2520leading%2520models.%2520Existing%2520works%2520mainly%2520adopt%2520a%2520retraining%250Aprocess%2520to%2520enhance%2520DM%2520efficiency.%2520This%2520is%2520computationally%2520expensive%2520and%2520not%250Avery%2520scalable.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520Attention-driven%2520Training-free%250AEfficient%2520Diffusion%2520Model%2520%2528AT-EDM%2529%2520framework%2520that%2520leverages%2520attention%2520maps%2520to%250Aperform%2520run-time%2520pruning%2520of%2520redundant%2520tokens%252C%2520without%2520the%2520need%2520for%2520any%250Aretraining.%2520Specifically%252C%2520for%2520single-denoising-step%2520pruning%252C%2520we%2520develop%2520a%2520novel%250Aranking%2520algorithm%252C%2520Generalized%2520Weighted%2520Page%2520Rank%2520%2528G-WPR%2529%252C%2520to%2520identify%250Aredundant%2520tokens%252C%2520and%2520a%2520similarity-based%2520recovery%2520method%2520to%2520restore%2520tokens%2520for%250Athe%2520convolution%2520operation.%2520In%2520addition%252C%2520we%2520propose%2520a%2520Denoising-Steps-Aware%250APruning%2520%2528DSAP%2529%2520approach%2520to%2520adjust%2520the%2520pruning%2520budget%2520across%2520different%2520denoising%250Atimesteps%2520for%2520better%2520generation%2520quality.%2520Extensive%2520evaluations%2520show%2520that%2520AT-EDM%250Aperforms%2520favorably%2520against%2520prior%2520art%2520in%2520terms%2520of%2520efficiency%2520%2528e.g.%252C%252038.8%2525%2520FLOPs%250Asaving%2520and%2520up%2520to%25201.53x%2520speed-up%2520over%2520Stable%2520Diffusion%2520XL%2529%2520while%2520maintaining%250Anearly%2520the%2520same%2520FID%2520and%2520CLIP%2520scores%2520as%2520the%2520full%2520model.%2520Project%2520webpage%253A%250Ahttps%253A//atedm.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-Driven%20Training-Free%20Efficiency%20Enhancement%20of%20Diffusion%0A%20%20Models&entry.906535625=Hongjie%20Wang%20and%20Difan%20Liu%20and%20Yan%20Kang%20and%20Yijun%20Li%20and%20Zhe%20Lin%20and%20Niraj%20K.%20Jha%20and%20Yuchen%20Liu&entry.1292438233=%20%20Diffusion%20Models%20%28DMs%29%20have%20exhibited%20superior%20performance%20in%20generating%0Ahigh-quality%20and%20diverse%20images.%20However%2C%20this%20exceptional%20performance%20comes%20at%0Athe%20cost%20of%20expensive%20architectural%20design%2C%20particularly%20due%20to%20the%20attention%0Amodule%20heavily%20used%20in%20leading%20models.%20Existing%20works%20mainly%20adopt%20a%20retraining%0Aprocess%20to%20enhance%20DM%20efficiency.%20This%20is%20computationally%20expensive%20and%20not%0Avery%20scalable.%20To%20this%20end%2C%20we%20introduce%20the%20Attention-driven%20Training-free%0AEfficient%20Diffusion%20Model%20%28AT-EDM%29%20framework%20that%20leverages%20attention%20maps%20to%0Aperform%20run-time%20pruning%20of%20redundant%20tokens%2C%20without%20the%20need%20for%20any%0Aretraining.%20Specifically%2C%20for%20single-denoising-step%20pruning%2C%20we%20develop%20a%20novel%0Aranking%20algorithm%2C%20Generalized%20Weighted%20Page%20Rank%20%28G-WPR%29%2C%20to%20identify%0Aredundant%20tokens%2C%20and%20a%20similarity-based%20recovery%20method%20to%20restore%20tokens%20for%0Athe%20convolution%20operation.%20In%20addition%2C%20we%20propose%20a%20Denoising-Steps-Aware%0APruning%20%28DSAP%29%20approach%20to%20adjust%20the%20pruning%20budget%20across%20different%20denoising%0Atimesteps%20for%20better%20generation%20quality.%20Extensive%20evaluations%20show%20that%20AT-EDM%0Aperforms%20favorably%20against%20prior%20art%20in%20terms%20of%20efficiency%20%28e.g.%2C%2038.8%25%20FLOPs%0Asaving%20and%20up%20to%201.53x%20speed-up%20over%20Stable%20Diffusion%20XL%29%20while%20maintaining%0Anearly%20the%20same%20FID%20and%20CLIP%20scores%20as%20the%20full%20model.%20Project%20webpage%3A%0Ahttps%3A//atedm.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05252v1&entry.124074799=Read"},
{"title": "DenserRadar: A 4D millimeter-wave radar point cloud detector based on\n  dense LiDAR point clouds", "author": "Zeyu Han and Junkai Jiang and Xiaokang Ding and Qingwen Meng and Shaobing Xu and Lei He and Jianqiang Wang", "abstract": "  The 4D millimeter-wave (mmWave) radar, with its robustness in extreme\nenvironments, extensive detection range, and capabilities for measuring\nvelocity and elevation, has demonstrated significant potential for enhancing\nthe perception abilities of autonomous driving systems in corner-case\nscenarios. Nevertheless, the inherent sparsity and noise of 4D mmWave radar\npoint clouds restrict its further development and practical application. In\nthis paper, we introduce a novel 4D mmWave radar point cloud detector, which\nleverages high-resolution dense LiDAR point clouds. Our approach constructs\ndense 3D occupancy ground truth from stitched LiDAR point clouds, and employs a\nspecially designed network named DenserRadar. The proposed method surpasses\nexisting probability-based and learning-based radar point cloud detectors in\nterms of both point cloud density and accuracy on the K-Radar dataset.\n", "link": "http://arxiv.org/abs/2405.05131v1", "date": "2024-05-08", "relevancy": 2.6408, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5341}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.529}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenserRadar%3A%20A%204D%20millimeter-wave%20radar%20point%20cloud%20detector%20based%20on%0A%20%20dense%20LiDAR%20point%20clouds&body=Title%3A%20DenserRadar%3A%20A%204D%20millimeter-wave%20radar%20point%20cloud%20detector%20based%20on%0A%20%20dense%20LiDAR%20point%20clouds%0AAuthor%3A%20Zeyu%20Han%20and%20Junkai%20Jiang%20and%20Xiaokang%20Ding%20and%20Qingwen%20Meng%20and%20Shaobing%20Xu%20and%20Lei%20He%20and%20Jianqiang%20Wang%0AAbstract%3A%20%20%20The%204D%20millimeter-wave%20%28mmWave%29%20radar%2C%20with%20its%20robustness%20in%20extreme%0Aenvironments%2C%20extensive%20detection%20range%2C%20and%20capabilities%20for%20measuring%0Avelocity%20and%20elevation%2C%20has%20demonstrated%20significant%20potential%20for%20enhancing%0Athe%20perception%20abilities%20of%20autonomous%20driving%20systems%20in%20corner-case%0Ascenarios.%20Nevertheless%2C%20the%20inherent%20sparsity%20and%20noise%20of%204D%20mmWave%20radar%0Apoint%20clouds%20restrict%20its%20further%20development%20and%20practical%20application.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%204D%20mmWave%20radar%20point%20cloud%20detector%2C%20which%0Aleverages%20high-resolution%20dense%20LiDAR%20point%20clouds.%20Our%20approach%20constructs%0Adense%203D%20occupancy%20ground%20truth%20from%20stitched%20LiDAR%20point%20clouds%2C%20and%20employs%20a%0Aspecially%20designed%20network%20named%20DenserRadar.%20The%20proposed%20method%20surpasses%0Aexisting%20probability-based%20and%20learning-based%20radar%20point%20cloud%20detectors%20in%0Aterms%20of%20both%20point%20cloud%20density%20and%20accuracy%20on%20the%20K-Radar%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenserRadar%253A%2520A%25204D%2520millimeter-wave%2520radar%2520point%2520cloud%2520detector%2520based%2520on%250A%2520%2520dense%2520LiDAR%2520point%2520clouds%26entry.906535625%3DZeyu%2520Han%2520and%2520Junkai%2520Jiang%2520and%2520Xiaokang%2520Ding%2520and%2520Qingwen%2520Meng%2520and%2520Shaobing%2520Xu%2520and%2520Lei%2520He%2520and%2520Jianqiang%2520Wang%26entry.1292438233%3D%2520%2520The%25204D%2520millimeter-wave%2520%2528mmWave%2529%2520radar%252C%2520with%2520its%2520robustness%2520in%2520extreme%250Aenvironments%252C%2520extensive%2520detection%2520range%252C%2520and%2520capabilities%2520for%2520measuring%250Avelocity%2520and%2520elevation%252C%2520has%2520demonstrated%2520significant%2520potential%2520for%2520enhancing%250Athe%2520perception%2520abilities%2520of%2520autonomous%2520driving%2520systems%2520in%2520corner-case%250Ascenarios.%2520Nevertheless%252C%2520the%2520inherent%2520sparsity%2520and%2520noise%2520of%25204D%2520mmWave%2520radar%250Apoint%2520clouds%2520restrict%2520its%2520further%2520development%2520and%2520practical%2520application.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%25204D%2520mmWave%2520radar%2520point%2520cloud%2520detector%252C%2520which%250Aleverages%2520high-resolution%2520dense%2520LiDAR%2520point%2520clouds.%2520Our%2520approach%2520constructs%250Adense%25203D%2520occupancy%2520ground%2520truth%2520from%2520stitched%2520LiDAR%2520point%2520clouds%252C%2520and%2520employs%2520a%250Aspecially%2520designed%2520network%2520named%2520DenserRadar.%2520The%2520proposed%2520method%2520surpasses%250Aexisting%2520probability-based%2520and%2520learning-based%2520radar%2520point%2520cloud%2520detectors%2520in%250Aterms%2520of%2520both%2520point%2520cloud%2520density%2520and%2520accuracy%2520on%2520the%2520K-Radar%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenserRadar%3A%20A%204D%20millimeter-wave%20radar%20point%20cloud%20detector%20based%20on%0A%20%20dense%20LiDAR%20point%20clouds&entry.906535625=Zeyu%20Han%20and%20Junkai%20Jiang%20and%20Xiaokang%20Ding%20and%20Qingwen%20Meng%20and%20Shaobing%20Xu%20and%20Lei%20He%20and%20Jianqiang%20Wang&entry.1292438233=%20%20The%204D%20millimeter-wave%20%28mmWave%29%20radar%2C%20with%20its%20robustness%20in%20extreme%0Aenvironments%2C%20extensive%20detection%20range%2C%20and%20capabilities%20for%20measuring%0Avelocity%20and%20elevation%2C%20has%20demonstrated%20significant%20potential%20for%20enhancing%0Athe%20perception%20abilities%20of%20autonomous%20driving%20systems%20in%20corner-case%0Ascenarios.%20Nevertheless%2C%20the%20inherent%20sparsity%20and%20noise%20of%204D%20mmWave%20radar%0Apoint%20clouds%20restrict%20its%20further%20development%20and%20practical%20application.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%204D%20mmWave%20radar%20point%20cloud%20detector%2C%20which%0Aleverages%20high-resolution%20dense%20LiDAR%20point%20clouds.%20Our%20approach%20constructs%0Adense%203D%20occupancy%20ground%20truth%20from%20stitched%20LiDAR%20point%20clouds%2C%20and%20employs%20a%0Aspecially%20designed%20network%20named%20DenserRadar.%20The%20proposed%20method%20surpasses%0Aexisting%20probability-based%20and%20learning-based%20radar%20point%20cloud%20detectors%20in%0Aterms%20of%20both%20point%20cloud%20density%20and%20accuracy%20on%20the%20K-Radar%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05131v1&entry.124074799=Read"},
{"title": "A review on discriminative self-supervised learning methods", "author": "Nikolaos Giakoumoglou and Tania Stathaki", "abstract": "  In the field of computer vision, self-supervised learning has emerged as a\nmethod to extract robust features from unlabeled data, where models derive\nlabels autonomously from the data itself, without the need for manual\nannotation. This paper provides a comprehensive review of discriminative\napproaches of self-supervised learning within the domain of computer vision,\nexamining their evolution and current status. Through an exploration of various\nmethods including contrastive, self-distillation, knowledge distillation,\nfeature decorrelation, and clustering techniques, we investigate how these\napproaches leverage the abundance of unlabeled data. Finally, we have\ncomparison of self-supervised learning methods on the standard ImageNet\nclassification benchmark.\n", "link": "http://arxiv.org/abs/2405.04969v1", "date": "2024-05-08", "relevancy": 2.6258, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6034}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.493}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20review%20on%20discriminative%20self-supervised%20learning%20methods&body=Title%3A%20A%20review%20on%20discriminative%20self-supervised%20learning%20methods%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20In%20the%20field%20of%20computer%20vision%2C%20self-supervised%20learning%20has%20emerged%20as%20a%0Amethod%20to%20extract%20robust%20features%20from%20unlabeled%20data%2C%20where%20models%20derive%0Alabels%20autonomously%20from%20the%20data%20itself%2C%20without%20the%20need%20for%20manual%0Aannotation.%20This%20paper%20provides%20a%20comprehensive%20review%20of%20discriminative%0Aapproaches%20of%20self-supervised%20learning%20within%20the%20domain%20of%20computer%20vision%2C%0Aexamining%20their%20evolution%20and%20current%20status.%20Through%20an%20exploration%20of%20various%0Amethods%20including%20contrastive%2C%20self-distillation%2C%20knowledge%20distillation%2C%0Afeature%20decorrelation%2C%20and%20clustering%20techniques%2C%20we%20investigate%20how%20these%0Aapproaches%20leverage%20the%20abundance%20of%20unlabeled%20data.%20Finally%2C%20we%20have%0Acomparison%20of%20self-supervised%20learning%20methods%20on%20the%20standard%20ImageNet%0Aclassification%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520review%2520on%2520discriminative%2520self-supervised%2520learning%2520methods%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520computer%2520vision%252C%2520self-supervised%2520learning%2520has%2520emerged%2520as%2520a%250Amethod%2520to%2520extract%2520robust%2520features%2520from%2520unlabeled%2520data%252C%2520where%2520models%2520derive%250Alabels%2520autonomously%2520from%2520the%2520data%2520itself%252C%2520without%2520the%2520need%2520for%2520manual%250Aannotation.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520review%2520of%2520discriminative%250Aapproaches%2520of%2520self-supervised%2520learning%2520within%2520the%2520domain%2520of%2520computer%2520vision%252C%250Aexamining%2520their%2520evolution%2520and%2520current%2520status.%2520Through%2520an%2520exploration%2520of%2520various%250Amethods%2520including%2520contrastive%252C%2520self-distillation%252C%2520knowledge%2520distillation%252C%250Afeature%2520decorrelation%252C%2520and%2520clustering%2520techniques%252C%2520we%2520investigate%2520how%2520these%250Aapproaches%2520leverage%2520the%2520abundance%2520of%2520unlabeled%2520data.%2520Finally%252C%2520we%2520have%250Acomparison%2520of%2520self-supervised%2520learning%2520methods%2520on%2520the%2520standard%2520ImageNet%250Aclassification%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20review%20on%20discriminative%20self-supervised%20learning%20methods&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki&entry.1292438233=%20%20In%20the%20field%20of%20computer%20vision%2C%20self-supervised%20learning%20has%20emerged%20as%20a%0Amethod%20to%20extract%20robust%20features%20from%20unlabeled%20data%2C%20where%20models%20derive%0Alabels%20autonomously%20from%20the%20data%20itself%2C%20without%20the%20need%20for%20manual%0Aannotation.%20This%20paper%20provides%20a%20comprehensive%20review%20of%20discriminative%0Aapproaches%20of%20self-supervised%20learning%20within%20the%20domain%20of%20computer%20vision%2C%0Aexamining%20their%20evolution%20and%20current%20status.%20Through%20an%20exploration%20of%20various%0Amethods%20including%20contrastive%2C%20self-distillation%2C%20knowledge%20distillation%2C%0Afeature%20decorrelation%2C%20and%20clustering%20techniques%2C%20we%20investigate%20how%20these%0Aapproaches%20leverage%20the%20abundance%20of%20unlabeled%20data.%20Finally%2C%20we%20have%0Acomparison%20of%20self-supervised%20learning%20methods%20on%20the%20standard%20ImageNet%0Aclassification%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04969v1&entry.124074799=Read"},
{"title": "The Potential and Implications of Generative AI on HCI Education", "author": "Ahmed Kharrufa and Ian G Johnson", "abstract": "  Generative AI (GAI) is impacting teaching and learning directly or indirectly\nacross a range of subjects and disciplines. As educators, we need to understand\nthe potential and limitations of AI in HCI education and ensure our graduating\nHCI students are aware of the potential and limitations of AI in HCI. In this\npaper, we report on the main pedagogical insights gained from the inclusion of\ngenerative AI into a 10 week undergraduate module. We designed the module to\nencourage student experimentation with GAI models as part of the design brief\nrequirement and planned practical sessions and discussions. Our insights are\nbased on replies to a survey sent out to the students after completing the\nmodule. Our key findings, for HCI educators, report on the use of AI as a\npersona for developing project ideas and creating resources for design, and AI\nas a mirror for reflecting students' understanding of key concepts and ideas\nand highlighting knowledge gaps. We also discuss potential pitfalls that should\nbe considered and the need to assess students' literacies and assumptions of\nGAIs as pedagogical tools. Finally, we put forward the case for educators to\ntake the opportunities GAI presents as an educational tool and be experimental,\ncreative, and courageous in their practice. We end with a discussion of our\nfindings in relation to the TPACK framework in HCI.\n", "link": "http://arxiv.org/abs/2405.05154v1", "date": "2024-05-08", "relevancy": 2.5412, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5531}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4941}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Potential%20and%20Implications%20of%20Generative%20AI%20on%20HCI%20Education&body=Title%3A%20The%20Potential%20and%20Implications%20of%20Generative%20AI%20on%20HCI%20Education%0AAuthor%3A%20Ahmed%20Kharrufa%20and%20Ian%20G%20Johnson%0AAbstract%3A%20%20%20Generative%20AI%20%28GAI%29%20is%20impacting%20teaching%20and%20learning%20directly%20or%20indirectly%0Aacross%20a%20range%20of%20subjects%20and%20disciplines.%20As%20educators%2C%20we%20need%20to%20understand%0Athe%20potential%20and%20limitations%20of%20AI%20in%20HCI%20education%20and%20ensure%20our%20graduating%0AHCI%20students%20are%20aware%20of%20the%20potential%20and%20limitations%20of%20AI%20in%20HCI.%20In%20this%0Apaper%2C%20we%20report%20on%20the%20main%20pedagogical%20insights%20gained%20from%20the%20inclusion%20of%0Agenerative%20AI%20into%20a%2010%20week%20undergraduate%20module.%20We%20designed%20the%20module%20to%0Aencourage%20student%20experimentation%20with%20GAI%20models%20as%20part%20of%20the%20design%20brief%0Arequirement%20and%20planned%20practical%20sessions%20and%20discussions.%20Our%20insights%20are%0Abased%20on%20replies%20to%20a%20survey%20sent%20out%20to%20the%20students%20after%20completing%20the%0Amodule.%20Our%20key%20findings%2C%20for%20HCI%20educators%2C%20report%20on%20the%20use%20of%20AI%20as%20a%0Apersona%20for%20developing%20project%20ideas%20and%20creating%20resources%20for%20design%2C%20and%20AI%0Aas%20a%20mirror%20for%20reflecting%20students%27%20understanding%20of%20key%20concepts%20and%20ideas%0Aand%20highlighting%20knowledge%20gaps.%20We%20also%20discuss%20potential%20pitfalls%20that%20should%0Abe%20considered%20and%20the%20need%20to%20assess%20students%27%20literacies%20and%20assumptions%20of%0AGAIs%20as%20pedagogical%20tools.%20Finally%2C%20we%20put%20forward%20the%20case%20for%20educators%20to%0Atake%20the%20opportunities%20GAI%20presents%20as%20an%20educational%20tool%20and%20be%20experimental%2C%0Acreative%2C%20and%20courageous%20in%20their%20practice.%20We%20end%20with%20a%20discussion%20of%20our%0Afindings%20in%20relation%20to%20the%20TPACK%20framework%20in%20HCI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Potential%2520and%2520Implications%2520of%2520Generative%2520AI%2520on%2520HCI%2520Education%26entry.906535625%3DAhmed%2520Kharrufa%2520and%2520Ian%2520G%2520Johnson%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GAI%2529%2520is%2520impacting%2520teaching%2520and%2520learning%2520directly%2520or%2520indirectly%250Aacross%2520a%2520range%2520of%2520subjects%2520and%2520disciplines.%2520As%2520educators%252C%2520we%2520need%2520to%2520understand%250Athe%2520potential%2520and%2520limitations%2520of%2520AI%2520in%2520HCI%2520education%2520and%2520ensure%2520our%2520graduating%250AHCI%2520students%2520are%2520aware%2520of%2520the%2520potential%2520and%2520limitations%2520of%2520AI%2520in%2520HCI.%2520In%2520this%250Apaper%252C%2520we%2520report%2520on%2520the%2520main%2520pedagogical%2520insights%2520gained%2520from%2520the%2520inclusion%2520of%250Agenerative%2520AI%2520into%2520a%252010%2520week%2520undergraduate%2520module.%2520We%2520designed%2520the%2520module%2520to%250Aencourage%2520student%2520experimentation%2520with%2520GAI%2520models%2520as%2520part%2520of%2520the%2520design%2520brief%250Arequirement%2520and%2520planned%2520practical%2520sessions%2520and%2520discussions.%2520Our%2520insights%2520are%250Abased%2520on%2520replies%2520to%2520a%2520survey%2520sent%2520out%2520to%2520the%2520students%2520after%2520completing%2520the%250Amodule.%2520Our%2520key%2520findings%252C%2520for%2520HCI%2520educators%252C%2520report%2520on%2520the%2520use%2520of%2520AI%2520as%2520a%250Apersona%2520for%2520developing%2520project%2520ideas%2520and%2520creating%2520resources%2520for%2520design%252C%2520and%2520AI%250Aas%2520a%2520mirror%2520for%2520reflecting%2520students%2527%2520understanding%2520of%2520key%2520concepts%2520and%2520ideas%250Aand%2520highlighting%2520knowledge%2520gaps.%2520We%2520also%2520discuss%2520potential%2520pitfalls%2520that%2520should%250Abe%2520considered%2520and%2520the%2520need%2520to%2520assess%2520students%2527%2520literacies%2520and%2520assumptions%2520of%250AGAIs%2520as%2520pedagogical%2520tools.%2520Finally%252C%2520we%2520put%2520forward%2520the%2520case%2520for%2520educators%2520to%250Atake%2520the%2520opportunities%2520GAI%2520presents%2520as%2520an%2520educational%2520tool%2520and%2520be%2520experimental%252C%250Acreative%252C%2520and%2520courageous%2520in%2520their%2520practice.%2520We%2520end%2520with%2520a%2520discussion%2520of%2520our%250Afindings%2520in%2520relation%2520to%2520the%2520TPACK%2520framework%2520in%2520HCI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Potential%20and%20Implications%20of%20Generative%20AI%20on%20HCI%20Education&entry.906535625=Ahmed%20Kharrufa%20and%20Ian%20G%20Johnson&entry.1292438233=%20%20Generative%20AI%20%28GAI%29%20is%20impacting%20teaching%20and%20learning%20directly%20or%20indirectly%0Aacross%20a%20range%20of%20subjects%20and%20disciplines.%20As%20educators%2C%20we%20need%20to%20understand%0Athe%20potential%20and%20limitations%20of%20AI%20in%20HCI%20education%20and%20ensure%20our%20graduating%0AHCI%20students%20are%20aware%20of%20the%20potential%20and%20limitations%20of%20AI%20in%20HCI.%20In%20this%0Apaper%2C%20we%20report%20on%20the%20main%20pedagogical%20insights%20gained%20from%20the%20inclusion%20of%0Agenerative%20AI%20into%20a%2010%20week%20undergraduate%20module.%20We%20designed%20the%20module%20to%0Aencourage%20student%20experimentation%20with%20GAI%20models%20as%20part%20of%20the%20design%20brief%0Arequirement%20and%20planned%20practical%20sessions%20and%20discussions.%20Our%20insights%20are%0Abased%20on%20replies%20to%20a%20survey%20sent%20out%20to%20the%20students%20after%20completing%20the%0Amodule.%20Our%20key%20findings%2C%20for%20HCI%20educators%2C%20report%20on%20the%20use%20of%20AI%20as%20a%0Apersona%20for%20developing%20project%20ideas%20and%20creating%20resources%20for%20design%2C%20and%20AI%0Aas%20a%20mirror%20for%20reflecting%20students%27%20understanding%20of%20key%20concepts%20and%20ideas%0Aand%20highlighting%20knowledge%20gaps.%20We%20also%20discuss%20potential%20pitfalls%20that%20should%0Abe%20considered%20and%20the%20need%20to%20assess%20students%27%20literacies%20and%20assumptions%20of%0AGAIs%20as%20pedagogical%20tools.%20Finally%2C%20we%20put%20forward%20the%20case%20for%20educators%20to%0Atake%20the%20opportunities%20GAI%20presents%20as%20an%20educational%20tool%20and%20be%20experimental%2C%0Acreative%2C%20and%20courageous%20in%20their%20practice.%20We%20end%20with%20a%20discussion%20of%20our%0Afindings%20in%20relation%20to%20the%20TPACK%20framework%20in%20HCI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05154v1&entry.124074799=Read"},
{"title": "HeadArtist: Text-conditioned 3D Head Generation with Self Score\n  Distillation", "author": "Hongyu Liu and Xuan Wang and Ziyu Wan and Yujun Shen and Yibing Song and Jing Liao and Qifeng Chen", "abstract": "  This work presents HeadArtist for 3D head generation from text descriptions.\nWith a landmark-guided ControlNet serving as the generative prior, we come up\nwith an efficient pipeline that optimizes a parameterized 3D head model under\nthe supervision of the prior distillation itself. We call such a process self\nscore distillation (SSD). In detail, given a sampled camera pose, we first\nrender an image and its corresponding landmarks from the head model, and add\nsome particular level of noise onto the image. The noisy image, landmarks, and\ntext condition are then fed into the frozen ControlNet twice for noise\nprediction. Two different classifier-free guidance (CFG) weights are applied\nduring these two predictions, and the prediction difference offers a direction\non how the rendered image can better match the text of interest. Experimental\nresults suggest that our approach delivers high-quality 3D head sculptures with\nadequate geometry and photorealistic appearance, significantly outperforming\nstate-ofthe-art methods. We also show that the same pipeline well supports\nediting the generated heads, including both geometry deformation and appearance\nchange.\n", "link": "http://arxiv.org/abs/2312.07539v2", "date": "2024-05-08", "relevancy": 2.5154, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6352}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6352}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeadArtist%3A%20Text-conditioned%203D%20Head%20Generation%20with%20Self%20Score%0A%20%20Distillation&body=Title%3A%20HeadArtist%3A%20Text-conditioned%203D%20Head%20Generation%20with%20Self%20Score%0A%20%20Distillation%0AAuthor%3A%20Hongyu%20Liu%20and%20Xuan%20Wang%20and%20Ziyu%20Wan%20and%20Yujun%20Shen%20and%20Yibing%20Song%20and%20Jing%20Liao%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20This%20work%20presents%20HeadArtist%20for%203D%20head%20generation%20from%20text%20descriptions.%0AWith%20a%20landmark-guided%20ControlNet%20serving%20as%20the%20generative%20prior%2C%20we%20come%20up%0Awith%20an%20efficient%20pipeline%20that%20optimizes%20a%20parameterized%203D%20head%20model%20under%0Athe%20supervision%20of%20the%20prior%20distillation%20itself.%20We%20call%20such%20a%20process%20self%0Ascore%20distillation%20%28SSD%29.%20In%20detail%2C%20given%20a%20sampled%20camera%20pose%2C%20we%20first%0Arender%20an%20image%20and%20its%20corresponding%20landmarks%20from%20the%20head%20model%2C%20and%20add%0Asome%20particular%20level%20of%20noise%20onto%20the%20image.%20The%20noisy%20image%2C%20landmarks%2C%20and%0Atext%20condition%20are%20then%20fed%20into%20the%20frozen%20ControlNet%20twice%20for%20noise%0Aprediction.%20Two%20different%20classifier-free%20guidance%20%28CFG%29%20weights%20are%20applied%0Aduring%20these%20two%20predictions%2C%20and%20the%20prediction%20difference%20offers%20a%20direction%0Aon%20how%20the%20rendered%20image%20can%20better%20match%20the%20text%20of%20interest.%20Experimental%0Aresults%20suggest%20that%20our%20approach%20delivers%20high-quality%203D%20head%20sculptures%20with%0Aadequate%20geometry%20and%20photorealistic%20appearance%2C%20significantly%20outperforming%0Astate-ofthe-art%20methods.%20We%20also%20show%20that%20the%20same%20pipeline%20well%20supports%0Aediting%20the%20generated%20heads%2C%20including%20both%20geometry%20deformation%20and%20appearance%0Achange.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeadArtist%253A%2520Text-conditioned%25203D%2520Head%2520Generation%2520with%2520Self%2520Score%250A%2520%2520Distillation%26entry.906535625%3DHongyu%2520Liu%2520and%2520Xuan%2520Wang%2520and%2520Ziyu%2520Wan%2520and%2520Yujun%2520Shen%2520and%2520Yibing%2520Song%2520and%2520Jing%2520Liao%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520HeadArtist%2520for%25203D%2520head%2520generation%2520from%2520text%2520descriptions.%250AWith%2520a%2520landmark-guided%2520ControlNet%2520serving%2520as%2520the%2520generative%2520prior%252C%2520we%2520come%2520up%250Awith%2520an%2520efficient%2520pipeline%2520that%2520optimizes%2520a%2520parameterized%25203D%2520head%2520model%2520under%250Athe%2520supervision%2520of%2520the%2520prior%2520distillation%2520itself.%2520We%2520call%2520such%2520a%2520process%2520self%250Ascore%2520distillation%2520%2528SSD%2529.%2520In%2520detail%252C%2520given%2520a%2520sampled%2520camera%2520pose%252C%2520we%2520first%250Arender%2520an%2520image%2520and%2520its%2520corresponding%2520landmarks%2520from%2520the%2520head%2520model%252C%2520and%2520add%250Asome%2520particular%2520level%2520of%2520noise%2520onto%2520the%2520image.%2520The%2520noisy%2520image%252C%2520landmarks%252C%2520and%250Atext%2520condition%2520are%2520then%2520fed%2520into%2520the%2520frozen%2520ControlNet%2520twice%2520for%2520noise%250Aprediction.%2520Two%2520different%2520classifier-free%2520guidance%2520%2528CFG%2529%2520weights%2520are%2520applied%250Aduring%2520these%2520two%2520predictions%252C%2520and%2520the%2520prediction%2520difference%2520offers%2520a%2520direction%250Aon%2520how%2520the%2520rendered%2520image%2520can%2520better%2520match%2520the%2520text%2520of%2520interest.%2520Experimental%250Aresults%2520suggest%2520that%2520our%2520approach%2520delivers%2520high-quality%25203D%2520head%2520sculptures%2520with%250Aadequate%2520geometry%2520and%2520photorealistic%2520appearance%252C%2520significantly%2520outperforming%250Astate-ofthe-art%2520methods.%2520We%2520also%2520show%2520that%2520the%2520same%2520pipeline%2520well%2520supports%250Aediting%2520the%2520generated%2520heads%252C%2520including%2520both%2520geometry%2520deformation%2520and%2520appearance%250Achange.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeadArtist%3A%20Text-conditioned%203D%20Head%20Generation%20with%20Self%20Score%0A%20%20Distillation&entry.906535625=Hongyu%20Liu%20and%20Xuan%20Wang%20and%20Ziyu%20Wan%20and%20Yujun%20Shen%20and%20Yibing%20Song%20and%20Jing%20Liao%20and%20Qifeng%20Chen&entry.1292438233=%20%20This%20work%20presents%20HeadArtist%20for%203D%20head%20generation%20from%20text%20descriptions.%0AWith%20a%20landmark-guided%20ControlNet%20serving%20as%20the%20generative%20prior%2C%20we%20come%20up%0Awith%20an%20efficient%20pipeline%20that%20optimizes%20a%20parameterized%203D%20head%20model%20under%0Athe%20supervision%20of%20the%20prior%20distillation%20itself.%20We%20call%20such%20a%20process%20self%0Ascore%20distillation%20%28SSD%29.%20In%20detail%2C%20given%20a%20sampled%20camera%20pose%2C%20we%20first%0Arender%20an%20image%20and%20its%20corresponding%20landmarks%20from%20the%20head%20model%2C%20and%20add%0Asome%20particular%20level%20of%20noise%20onto%20the%20image.%20The%20noisy%20image%2C%20landmarks%2C%20and%0Atext%20condition%20are%20then%20fed%20into%20the%20frozen%20ControlNet%20twice%20for%20noise%0Aprediction.%20Two%20different%20classifier-free%20guidance%20%28CFG%29%20weights%20are%20applied%0Aduring%20these%20two%20predictions%2C%20and%20the%20prediction%20difference%20offers%20a%20direction%0Aon%20how%20the%20rendered%20image%20can%20better%20match%20the%20text%20of%20interest.%20Experimental%0Aresults%20suggest%20that%20our%20approach%20delivers%20high-quality%203D%20head%20sculptures%20with%0Aadequate%20geometry%20and%20photorealistic%20appearance%2C%20significantly%20outperforming%0Astate-ofthe-art%20methods.%20We%20also%20show%20that%20the%20same%20pipeline%20well%20supports%0Aediting%20the%20generated%20heads%2C%20including%20both%20geometry%20deformation%20and%20appearance%0Achange.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07539v2&entry.124074799=Read"},
{"title": "A Sparse Tensor Generator with Efficient Feature Extraction", "author": "Tugba Torun and Eren Yenigul and Ameer Taweel and Didem Unat", "abstract": "  Sparse tensor operations are gaining attention in emerging applications such\nas social networks, deep learning, diagnosis, crime, and review analysis.\nHowever, a major obstacle for research in sparse tensor operations is the\ndeficiency of a broad-scale sparse tensor dataset. Another challenge in sparse\ntensor operations is examining the sparse tensor features, which are not only\nimportant for revealing its nonzero pattern but also have a significant impact\non determining the best-suited storage format, the decomposition algorithm, and\nthe reordering methods. However, due to the large sizes of real tensors, even\nextracting these features becomes costly without caution. To address these gaps\nin the literature, we have developed a smart sparse tensor generator that\nmimics the substantial features of real sparse tensors. Moreover, we propose\nvarious methods for efficiently extracting an extensive set of features for\nsparse tensors. The effectiveness of our generator is validated through the\nquality of features and the performance of decomposition in the generated\ntensors. Both the sparse tensor feature extractor and the tensor generator are\nopen source with all the artifacts available at\nhttps://github.com/sparcityeu/feaTen and https://github.com/sparcityeu/genTen,\nrespectively.\n", "link": "http://arxiv.org/abs/2405.04944v1", "date": "2024-05-08", "relevancy": 2.4891, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5339}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4847}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sparse%20Tensor%20Generator%20with%20Efficient%20Feature%20Extraction&body=Title%3A%20A%20Sparse%20Tensor%20Generator%20with%20Efficient%20Feature%20Extraction%0AAuthor%3A%20Tugba%20Torun%20and%20Eren%20Yenigul%20and%20Ameer%20Taweel%20and%20Didem%20Unat%0AAbstract%3A%20%20%20Sparse%20tensor%20operations%20are%20gaining%20attention%20in%20emerging%20applications%20such%0Aas%20social%20networks%2C%20deep%20learning%2C%20diagnosis%2C%20crime%2C%20and%20review%20analysis.%0AHowever%2C%20a%20major%20obstacle%20for%20research%20in%20sparse%20tensor%20operations%20is%20the%0Adeficiency%20of%20a%20broad-scale%20sparse%20tensor%20dataset.%20Another%20challenge%20in%20sparse%0Atensor%20operations%20is%20examining%20the%20sparse%20tensor%20features%2C%20which%20are%20not%20only%0Aimportant%20for%20revealing%20its%20nonzero%20pattern%20but%20also%20have%20a%20significant%20impact%0Aon%20determining%20the%20best-suited%20storage%20format%2C%20the%20decomposition%20algorithm%2C%20and%0Athe%20reordering%20methods.%20However%2C%20due%20to%20the%20large%20sizes%20of%20real%20tensors%2C%20even%0Aextracting%20these%20features%20becomes%20costly%20without%20caution.%20To%20address%20these%20gaps%0Ain%20the%20literature%2C%20we%20have%20developed%20a%20smart%20sparse%20tensor%20generator%20that%0Amimics%20the%20substantial%20features%20of%20real%20sparse%20tensors.%20Moreover%2C%20we%20propose%0Avarious%20methods%20for%20efficiently%20extracting%20an%20extensive%20set%20of%20features%20for%0Asparse%20tensors.%20The%20effectiveness%20of%20our%20generator%20is%20validated%20through%20the%0Aquality%20of%20features%20and%20the%20performance%20of%20decomposition%20in%20the%20generated%0Atensors.%20Both%20the%20sparse%20tensor%20feature%20extractor%20and%20the%20tensor%20generator%20are%0Aopen%20source%20with%20all%20the%20artifacts%20available%20at%0Ahttps%3A//github.com/sparcityeu/feaTen%20and%20https%3A//github.com/sparcityeu/genTen%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sparse%2520Tensor%2520Generator%2520with%2520Efficient%2520Feature%2520Extraction%26entry.906535625%3DTugba%2520Torun%2520and%2520Eren%2520Yenigul%2520and%2520Ameer%2520Taweel%2520and%2520Didem%2520Unat%26entry.1292438233%3D%2520%2520Sparse%2520tensor%2520operations%2520are%2520gaining%2520attention%2520in%2520emerging%2520applications%2520such%250Aas%2520social%2520networks%252C%2520deep%2520learning%252C%2520diagnosis%252C%2520crime%252C%2520and%2520review%2520analysis.%250AHowever%252C%2520a%2520major%2520obstacle%2520for%2520research%2520in%2520sparse%2520tensor%2520operations%2520is%2520the%250Adeficiency%2520of%2520a%2520broad-scale%2520sparse%2520tensor%2520dataset.%2520Another%2520challenge%2520in%2520sparse%250Atensor%2520operations%2520is%2520examining%2520the%2520sparse%2520tensor%2520features%252C%2520which%2520are%2520not%2520only%250Aimportant%2520for%2520revealing%2520its%2520nonzero%2520pattern%2520but%2520also%2520have%2520a%2520significant%2520impact%250Aon%2520determining%2520the%2520best-suited%2520storage%2520format%252C%2520the%2520decomposition%2520algorithm%252C%2520and%250Athe%2520reordering%2520methods.%2520However%252C%2520due%2520to%2520the%2520large%2520sizes%2520of%2520real%2520tensors%252C%2520even%250Aextracting%2520these%2520features%2520becomes%2520costly%2520without%2520caution.%2520To%2520address%2520these%2520gaps%250Ain%2520the%2520literature%252C%2520we%2520have%2520developed%2520a%2520smart%2520sparse%2520tensor%2520generator%2520that%250Amimics%2520the%2520substantial%2520features%2520of%2520real%2520sparse%2520tensors.%2520Moreover%252C%2520we%2520propose%250Avarious%2520methods%2520for%2520efficiently%2520extracting%2520an%2520extensive%2520set%2520of%2520features%2520for%250Asparse%2520tensors.%2520The%2520effectiveness%2520of%2520our%2520generator%2520is%2520validated%2520through%2520the%250Aquality%2520of%2520features%2520and%2520the%2520performance%2520of%2520decomposition%2520in%2520the%2520generated%250Atensors.%2520Both%2520the%2520sparse%2520tensor%2520feature%2520extractor%2520and%2520the%2520tensor%2520generator%2520are%250Aopen%2520source%2520with%2520all%2520the%2520artifacts%2520available%2520at%250Ahttps%253A//github.com/sparcityeu/feaTen%2520and%2520https%253A//github.com/sparcityeu/genTen%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sparse%20Tensor%20Generator%20with%20Efficient%20Feature%20Extraction&entry.906535625=Tugba%20Torun%20and%20Eren%20Yenigul%20and%20Ameer%20Taweel%20and%20Didem%20Unat&entry.1292438233=%20%20Sparse%20tensor%20operations%20are%20gaining%20attention%20in%20emerging%20applications%20such%0Aas%20social%20networks%2C%20deep%20learning%2C%20diagnosis%2C%20crime%2C%20and%20review%20analysis.%0AHowever%2C%20a%20major%20obstacle%20for%20research%20in%20sparse%20tensor%20operations%20is%20the%0Adeficiency%20of%20a%20broad-scale%20sparse%20tensor%20dataset.%20Another%20challenge%20in%20sparse%0Atensor%20operations%20is%20examining%20the%20sparse%20tensor%20features%2C%20which%20are%20not%20only%0Aimportant%20for%20revealing%20its%20nonzero%20pattern%20but%20also%20have%20a%20significant%20impact%0Aon%20determining%20the%20best-suited%20storage%20format%2C%20the%20decomposition%20algorithm%2C%20and%0Athe%20reordering%20methods.%20However%2C%20due%20to%20the%20large%20sizes%20of%20real%20tensors%2C%20even%0Aextracting%20these%20features%20becomes%20costly%20without%20caution.%20To%20address%20these%20gaps%0Ain%20the%20literature%2C%20we%20have%20developed%20a%20smart%20sparse%20tensor%20generator%20that%0Amimics%20the%20substantial%20features%20of%20real%20sparse%20tensors.%20Moreover%2C%20we%20propose%0Avarious%20methods%20for%20efficiently%20extracting%20an%20extensive%20set%20of%20features%20for%0Asparse%20tensors.%20The%20effectiveness%20of%20our%20generator%20is%20validated%20through%20the%0Aquality%20of%20features%20and%20the%20performance%20of%20decomposition%20in%20the%20generated%0Atensors.%20Both%20the%20sparse%20tensor%20feature%20extractor%20and%20the%20tensor%20generator%20are%0Aopen%20source%20with%20all%20the%20artifacts%20available%20at%0Ahttps%3A//github.com/sparcityeu/feaTen%20and%20https%3A//github.com/sparcityeu/genTen%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04944v1&entry.124074799=Read"},
{"title": "The Entropy Enigma: Success and Failure of Entropy Minimization", "author": "Ori Press and Ravid Shwartz-Ziv and Yann LeCun and Matthias Bethge", "abstract": "  Entropy minimization (EM) is frequently used to increase the accuracy of\nclassification models when they're faced with new data at test time. EM is a\nself-supervised learning method that optimizes classifiers to assign even\nhigher probabilities to their top predicted classes. In this paper, we analyze\nwhy EM works when adapting a model for a few steps and why it eventually fails\nafter adapting for many steps. We show that, at first, EM causes the model to\nembed test images close to training images, thereby increasing model accuracy.\nAfter many steps of optimization, EM makes the model embed test images far away\nfrom the embeddings of training images, which results in a degradation of\naccuracy. Building upon our insights, we present a method for solving a\npractical problem: estimating a model's accuracy on a given arbitrary dataset\nwithout having access to its labels. Our method estimates accuracy by looking\nat how the embeddings of input images change as the model is optimized to\nminimize entropy. Experiments on 23 challenging datasets show that our method\nsets the SoTA with a mean absolute error of $5.75\\%$, an improvement of\n$29.62\\%$ over the previous SoTA on this task. Our code is available at\nhttps://github.com/oripress/EntropyEnigma\n", "link": "http://arxiv.org/abs/2405.05012v1", "date": "2024-05-08", "relevancy": 2.479, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5084}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5032}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Entropy%20Enigma%3A%20Success%20and%20Failure%20of%20Entropy%20Minimization&body=Title%3A%20The%20Entropy%20Enigma%3A%20Success%20and%20Failure%20of%20Entropy%20Minimization%0AAuthor%3A%20Ori%20Press%20and%20Ravid%20Shwartz-Ziv%20and%20Yann%20LeCun%20and%20Matthias%20Bethge%0AAbstract%3A%20%20%20Entropy%20minimization%20%28EM%29%20is%20frequently%20used%20to%20increase%20the%20accuracy%20of%0Aclassification%20models%20when%20they%27re%20faced%20with%20new%20data%20at%20test%20time.%20EM%20is%20a%0Aself-supervised%20learning%20method%20that%20optimizes%20classifiers%20to%20assign%20even%0Ahigher%20probabilities%20to%20their%20top%20predicted%20classes.%20In%20this%20paper%2C%20we%20analyze%0Awhy%20EM%20works%20when%20adapting%20a%20model%20for%20a%20few%20steps%20and%20why%20it%20eventually%20fails%0Aafter%20adapting%20for%20many%20steps.%20We%20show%20that%2C%20at%20first%2C%20EM%20causes%20the%20model%20to%0Aembed%20test%20images%20close%20to%20training%20images%2C%20thereby%20increasing%20model%20accuracy.%0AAfter%20many%20steps%20of%20optimization%2C%20EM%20makes%20the%20model%20embed%20test%20images%20far%20away%0Afrom%20the%20embeddings%20of%20training%20images%2C%20which%20results%20in%20a%20degradation%20of%0Aaccuracy.%20Building%20upon%20our%20insights%2C%20we%20present%20a%20method%20for%20solving%20a%0Apractical%20problem%3A%20estimating%20a%20model%27s%20accuracy%20on%20a%20given%20arbitrary%20dataset%0Awithout%20having%20access%20to%20its%20labels.%20Our%20method%20estimates%20accuracy%20by%20looking%0Aat%20how%20the%20embeddings%20of%20input%20images%20change%20as%20the%20model%20is%20optimized%20to%0Aminimize%20entropy.%20Experiments%20on%2023%20challenging%20datasets%20show%20that%20our%20method%0Asets%20the%20SoTA%20with%20a%20mean%20absolute%20error%20of%20%245.75%5C%25%24%2C%20an%20improvement%20of%0A%2429.62%5C%25%24%20over%20the%20previous%20SoTA%20on%20this%20task.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/oripress/EntropyEnigma%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Entropy%2520Enigma%253A%2520Success%2520and%2520Failure%2520of%2520Entropy%2520Minimization%26entry.906535625%3DOri%2520Press%2520and%2520Ravid%2520Shwartz-Ziv%2520and%2520Yann%2520LeCun%2520and%2520Matthias%2520Bethge%26entry.1292438233%3D%2520%2520Entropy%2520minimization%2520%2528EM%2529%2520is%2520frequently%2520used%2520to%2520increase%2520the%2520accuracy%2520of%250Aclassification%2520models%2520when%2520they%2527re%2520faced%2520with%2520new%2520data%2520at%2520test%2520time.%2520EM%2520is%2520a%250Aself-supervised%2520learning%2520method%2520that%2520optimizes%2520classifiers%2520to%2520assign%2520even%250Ahigher%2520probabilities%2520to%2520their%2520top%2520predicted%2520classes.%2520In%2520this%2520paper%252C%2520we%2520analyze%250Awhy%2520EM%2520works%2520when%2520adapting%2520a%2520model%2520for%2520a%2520few%2520steps%2520and%2520why%2520it%2520eventually%2520fails%250Aafter%2520adapting%2520for%2520many%2520steps.%2520We%2520show%2520that%252C%2520at%2520first%252C%2520EM%2520causes%2520the%2520model%2520to%250Aembed%2520test%2520images%2520close%2520to%2520training%2520images%252C%2520thereby%2520increasing%2520model%2520accuracy.%250AAfter%2520many%2520steps%2520of%2520optimization%252C%2520EM%2520makes%2520the%2520model%2520embed%2520test%2520images%2520far%2520away%250Afrom%2520the%2520embeddings%2520of%2520training%2520images%252C%2520which%2520results%2520in%2520a%2520degradation%2520of%250Aaccuracy.%2520Building%2520upon%2520our%2520insights%252C%2520we%2520present%2520a%2520method%2520for%2520solving%2520a%250Apractical%2520problem%253A%2520estimating%2520a%2520model%2527s%2520accuracy%2520on%2520a%2520given%2520arbitrary%2520dataset%250Awithout%2520having%2520access%2520to%2520its%2520labels.%2520Our%2520method%2520estimates%2520accuracy%2520by%2520looking%250Aat%2520how%2520the%2520embeddings%2520of%2520input%2520images%2520change%2520as%2520the%2520model%2520is%2520optimized%2520to%250Aminimize%2520entropy.%2520Experiments%2520on%252023%2520challenging%2520datasets%2520show%2520that%2520our%2520method%250Asets%2520the%2520SoTA%2520with%2520a%2520mean%2520absolute%2520error%2520of%2520%25245.75%255C%2525%2524%252C%2520an%2520improvement%2520of%250A%252429.62%255C%2525%2524%2520over%2520the%2520previous%2520SoTA%2520on%2520this%2520task.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/oripress/EntropyEnigma%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Entropy%20Enigma%3A%20Success%20and%20Failure%20of%20Entropy%20Minimization&entry.906535625=Ori%20Press%20and%20Ravid%20Shwartz-Ziv%20and%20Yann%20LeCun%20and%20Matthias%20Bethge&entry.1292438233=%20%20Entropy%20minimization%20%28EM%29%20is%20frequently%20used%20to%20increase%20the%20accuracy%20of%0Aclassification%20models%20when%20they%27re%20faced%20with%20new%20data%20at%20test%20time.%20EM%20is%20a%0Aself-supervised%20learning%20method%20that%20optimizes%20classifiers%20to%20assign%20even%0Ahigher%20probabilities%20to%20their%20top%20predicted%20classes.%20In%20this%20paper%2C%20we%20analyze%0Awhy%20EM%20works%20when%20adapting%20a%20model%20for%20a%20few%20steps%20and%20why%20it%20eventually%20fails%0Aafter%20adapting%20for%20many%20steps.%20We%20show%20that%2C%20at%20first%2C%20EM%20causes%20the%20model%20to%0Aembed%20test%20images%20close%20to%20training%20images%2C%20thereby%20increasing%20model%20accuracy.%0AAfter%20many%20steps%20of%20optimization%2C%20EM%20makes%20the%20model%20embed%20test%20images%20far%20away%0Afrom%20the%20embeddings%20of%20training%20images%2C%20which%20results%20in%20a%20degradation%20of%0Aaccuracy.%20Building%20upon%20our%20insights%2C%20we%20present%20a%20method%20for%20solving%20a%0Apractical%20problem%3A%20estimating%20a%20model%27s%20accuracy%20on%20a%20given%20arbitrary%20dataset%0Awithout%20having%20access%20to%20its%20labels.%20Our%20method%20estimates%20accuracy%20by%20looking%0Aat%20how%20the%20embeddings%20of%20input%20images%20change%20as%20the%20model%20is%20optimized%20to%0Aminimize%20entropy.%20Experiments%20on%2023%20challenging%20datasets%20show%20that%20our%20method%0Asets%20the%20SoTA%20with%20a%20mean%20absolute%20error%20of%20%245.75%5C%25%24%2C%20an%20improvement%20of%0A%2429.62%5C%25%24%20over%20the%20previous%20SoTA%20on%20this%20task.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/oripress/EntropyEnigma%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05012v1&entry.124074799=Read"},
{"title": "Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving", "author": "Lingdong Kong and Xiang Xu and Jiawei Ren and Wenwei Zhang and Liang Pan and Kai Chen and Wei Tsang Ooi and Ziwei Liu", "abstract": "  Efficient data utilization is crucial for advancing 3D scene understanding in\nautonomous driving, where reliance on heavily human-annotated LiDAR point\nclouds challenges fully supervised methods. Addressing this, our study extends\ninto semi-supervised learning for LiDAR semantic segmentation, leveraging the\nintrinsic spatial priors of driving scenes and multi-sensor complements to\naugment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved\nframework that integrates laser beam manipulations from disparate LiDAR scans\nand incorporates LiDAR-camera correspondences to further assist data-efficient\nlearning. Our framework is tailored to enhance 3D scene consistency\nregularization by incorporating multi-modality, including 1) multi-modal\nLaserMix operation for fine-grained cross-sensor interactions; 2)\ncamera-to-LiDAR feature distillation that enhances LiDAR feature learning; and\n3) language-driven knowledge guidance generating auxiliary supervisions using\nopen-vocabulary models. The versatility of LaserMix++ enables applications\nacross LiDAR representations, establishing it as a universally applicable\nsolution. Our framework is rigorously validated through theoretical analysis\nand extensive experiments on popular driving perception datasets. Results\ndemonstrate that LaserMix++ markedly outperforms fully supervised alternatives,\nachieving comparable accuracy with five times fewer annotations and\nsignificantly improving the supervised-only baselines. This substantial\nadvancement underscores the potential of semi-supervised approaches in reducing\nthe reliance on extensive labeled data in LiDAR-based 3D scene understanding\nsystems.\n", "link": "http://arxiv.org/abs/2405.05258v1", "date": "2024-05-08", "relevancy": 2.4649, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6279}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6181}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Data-Efficient%203D%20Scene%20Understanding%20for%20Autonomous%20Driving&body=Title%3A%20Multi-Modal%20Data-Efficient%203D%20Scene%20Understanding%20for%20Autonomous%20Driving%0AAuthor%3A%20Lingdong%20Kong%20and%20Xiang%20Xu%20and%20Jiawei%20Ren%20and%20Wenwei%20Zhang%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Wei%20Tsang%20Ooi%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Efficient%20data%20utilization%20is%20crucial%20for%20advancing%203D%20scene%20understanding%20in%0Aautonomous%20driving%2C%20where%20reliance%20on%20heavily%20human-annotated%20LiDAR%20point%0Aclouds%20challenges%20fully%20supervised%20methods.%20Addressing%20this%2C%20our%20study%20extends%0Ainto%20semi-supervised%20learning%20for%20LiDAR%20semantic%20segmentation%2C%20leveraging%20the%0Aintrinsic%20spatial%20priors%20of%20driving%20scenes%20and%20multi-sensor%20complements%20to%0Aaugment%20the%20efficacy%20of%20unlabeled%20datasets.%20We%20introduce%20LaserMix%2B%2B%2C%20an%20evolved%0Aframework%20that%20integrates%20laser%20beam%20manipulations%20from%20disparate%20LiDAR%20scans%0Aand%20incorporates%20LiDAR-camera%20correspondences%20to%20further%20assist%20data-efficient%0Alearning.%20Our%20framework%20is%20tailored%20to%20enhance%203D%20scene%20consistency%0Aregularization%20by%20incorporating%20multi-modality%2C%20including%201%29%20multi-modal%0ALaserMix%20operation%20for%20fine-grained%20cross-sensor%20interactions%3B%202%29%0Acamera-to-LiDAR%20feature%20distillation%20that%20enhances%20LiDAR%20feature%20learning%3B%20and%0A3%29%20language-driven%20knowledge%20guidance%20generating%20auxiliary%20supervisions%20using%0Aopen-vocabulary%20models.%20The%20versatility%20of%20LaserMix%2B%2B%20enables%20applications%0Aacross%20LiDAR%20representations%2C%20establishing%20it%20as%20a%20universally%20applicable%0Asolution.%20Our%20framework%20is%20rigorously%20validated%20through%20theoretical%20analysis%0Aand%20extensive%20experiments%20on%20popular%20driving%20perception%20datasets.%20Results%0Ademonstrate%20that%20LaserMix%2B%2B%20markedly%20outperforms%20fully%20supervised%20alternatives%2C%0Aachieving%20comparable%20accuracy%20with%20five%20times%20fewer%20annotations%20and%0Asignificantly%20improving%20the%20supervised-only%20baselines.%20This%20substantial%0Aadvancement%20underscores%20the%20potential%20of%20semi-supervised%20approaches%20in%20reducing%0Athe%20reliance%20on%20extensive%20labeled%20data%20in%20LiDAR-based%203D%20scene%20understanding%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Data-Efficient%25203D%2520Scene%2520Understanding%2520for%2520Autonomous%2520Driving%26entry.906535625%3DLingdong%2520Kong%2520and%2520Xiang%2520Xu%2520and%2520Jiawei%2520Ren%2520and%2520Wenwei%2520Zhang%2520and%2520Liang%2520Pan%2520and%2520Kai%2520Chen%2520and%2520Wei%2520Tsang%2520Ooi%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Efficient%2520data%2520utilization%2520is%2520crucial%2520for%2520advancing%25203D%2520scene%2520understanding%2520in%250Aautonomous%2520driving%252C%2520where%2520reliance%2520on%2520heavily%2520human-annotated%2520LiDAR%2520point%250Aclouds%2520challenges%2520fully%2520supervised%2520methods.%2520Addressing%2520this%252C%2520our%2520study%2520extends%250Ainto%2520semi-supervised%2520learning%2520for%2520LiDAR%2520semantic%2520segmentation%252C%2520leveraging%2520the%250Aintrinsic%2520spatial%2520priors%2520of%2520driving%2520scenes%2520and%2520multi-sensor%2520complements%2520to%250Aaugment%2520the%2520efficacy%2520of%2520unlabeled%2520datasets.%2520We%2520introduce%2520LaserMix%252B%252B%252C%2520an%2520evolved%250Aframework%2520that%2520integrates%2520laser%2520beam%2520manipulations%2520from%2520disparate%2520LiDAR%2520scans%250Aand%2520incorporates%2520LiDAR-camera%2520correspondences%2520to%2520further%2520assist%2520data-efficient%250Alearning.%2520Our%2520framework%2520is%2520tailored%2520to%2520enhance%25203D%2520scene%2520consistency%250Aregularization%2520by%2520incorporating%2520multi-modality%252C%2520including%25201%2529%2520multi-modal%250ALaserMix%2520operation%2520for%2520fine-grained%2520cross-sensor%2520interactions%253B%25202%2529%250Acamera-to-LiDAR%2520feature%2520distillation%2520that%2520enhances%2520LiDAR%2520feature%2520learning%253B%2520and%250A3%2529%2520language-driven%2520knowledge%2520guidance%2520generating%2520auxiliary%2520supervisions%2520using%250Aopen-vocabulary%2520models.%2520The%2520versatility%2520of%2520LaserMix%252B%252B%2520enables%2520applications%250Aacross%2520LiDAR%2520representations%252C%2520establishing%2520it%2520as%2520a%2520universally%2520applicable%250Asolution.%2520Our%2520framework%2520is%2520rigorously%2520validated%2520through%2520theoretical%2520analysis%250Aand%2520extensive%2520experiments%2520on%2520popular%2520driving%2520perception%2520datasets.%2520Results%250Ademonstrate%2520that%2520LaserMix%252B%252B%2520markedly%2520outperforms%2520fully%2520supervised%2520alternatives%252C%250Aachieving%2520comparable%2520accuracy%2520with%2520five%2520times%2520fewer%2520annotations%2520and%250Asignificantly%2520improving%2520the%2520supervised-only%2520baselines.%2520This%2520substantial%250Aadvancement%2520underscores%2520the%2520potential%2520of%2520semi-supervised%2520approaches%2520in%2520reducing%250Athe%2520reliance%2520on%2520extensive%2520labeled%2520data%2520in%2520LiDAR-based%25203D%2520scene%2520understanding%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Data-Efficient%203D%20Scene%20Understanding%20for%20Autonomous%20Driving&entry.906535625=Lingdong%20Kong%20and%20Xiang%20Xu%20and%20Jiawei%20Ren%20and%20Wenwei%20Zhang%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Wei%20Tsang%20Ooi%20and%20Ziwei%20Liu&entry.1292438233=%20%20Efficient%20data%20utilization%20is%20crucial%20for%20advancing%203D%20scene%20understanding%20in%0Aautonomous%20driving%2C%20where%20reliance%20on%20heavily%20human-annotated%20LiDAR%20point%0Aclouds%20challenges%20fully%20supervised%20methods.%20Addressing%20this%2C%20our%20study%20extends%0Ainto%20semi-supervised%20learning%20for%20LiDAR%20semantic%20segmentation%2C%20leveraging%20the%0Aintrinsic%20spatial%20priors%20of%20driving%20scenes%20and%20multi-sensor%20complements%20to%0Aaugment%20the%20efficacy%20of%20unlabeled%20datasets.%20We%20introduce%20LaserMix%2B%2B%2C%20an%20evolved%0Aframework%20that%20integrates%20laser%20beam%20manipulations%20from%20disparate%20LiDAR%20scans%0Aand%20incorporates%20LiDAR-camera%20correspondences%20to%20further%20assist%20data-efficient%0Alearning.%20Our%20framework%20is%20tailored%20to%20enhance%203D%20scene%20consistency%0Aregularization%20by%20incorporating%20multi-modality%2C%20including%201%29%20multi-modal%0ALaserMix%20operation%20for%20fine-grained%20cross-sensor%20interactions%3B%202%29%0Acamera-to-LiDAR%20feature%20distillation%20that%20enhances%20LiDAR%20feature%20learning%3B%20and%0A3%29%20language-driven%20knowledge%20guidance%20generating%20auxiliary%20supervisions%20using%0Aopen-vocabulary%20models.%20The%20versatility%20of%20LaserMix%2B%2B%20enables%20applications%0Aacross%20LiDAR%20representations%2C%20establishing%20it%20as%20a%20universally%20applicable%0Asolution.%20Our%20framework%20is%20rigorously%20validated%20through%20theoretical%20analysis%0Aand%20extensive%20experiments%20on%20popular%20driving%20perception%20datasets.%20Results%0Ademonstrate%20that%20LaserMix%2B%2B%20markedly%20outperforms%20fully%20supervised%20alternatives%2C%0Aachieving%20comparable%20accuracy%20with%20five%20times%20fewer%20annotations%20and%0Asignificantly%20improving%20the%20supervised-only%20baselines.%20This%20substantial%0Aadvancement%20underscores%20the%20potential%20of%20semi-supervised%20approaches%20in%20reducing%0Athe%20reliance%20on%20extensive%20labeled%20data%20in%20LiDAR-based%203D%20scene%20understanding%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05258v1&entry.124074799=Read"},
{"title": "Reviewing Intelligent Cinematography: AI research for camera-based video\n  production", "author": "Adrian Azzarelli and Nantheera Anantrasirichai and David R Bull", "abstract": "  This paper offers a comprehensive review of artificial intelligence (AI)\nresearch in the context of real camera content acquisition for entertainment\npurposes and is aimed at both researchers and cinematographers. Considering the\nbreadth of computer vision research and the lack of review papers tied to\nintelligent cinematography (IC), this review introduces a holistic view of the\nIC landscape while providing the technical insight for experts across across\ndisciplines. We preface the main discussion with technical background on\ngenerative AI, object detection, automated camera calibration and 3-D content\nacquisition, and link explanatory articles to assist non-technical readers. The\nmain discussion categorizes work by four production types: General Production,\nVirtual Production, Live Production and Aerial Production. Note that for\nVirtual Production we do not discuss research relating to virtual content\nacquisition, including work on automated video generation, like Stable\nDiffusion. Within each section, we (1) sub-classify work by the technical field\nof research - reflected by the subsections, and (2) evaluate the trends and\nchallenge w.r.t to each type of production. In the final chapter, we present\nour concluding remarks on the greater scope of IC research and outline work\nthat we believe has significant potential to influence the whole industry. We\nfind that work relating to virtual production has the greatest potential to\nimpact other mediums of production, driven by the growing interest in LED\nvolumes/stages for in-camera virtual effects (ICVFX) and automated 3-D capture\nfor a virtual modelling of real world scenes and actors. This is the first\npiece of literature to offer a structured and comprehensive examination of IC\nresearch. Consequently, we address ethical and legal concerns regarding the use\nof creative AI involving artists, actors and the general public, in the...\n", "link": "http://arxiv.org/abs/2405.05039v1", "date": "2024-05-08", "relevancy": 2.4527, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5056}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4927}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production&body=Title%3A%20Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production%0AAuthor%3A%20Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull%0AAbstract%3A%20%20%20This%20paper%20offers%20a%20comprehensive%20review%20of%20artificial%20intelligence%20%28AI%29%0Aresearch%20in%20the%20context%20of%20real%20camera%20content%20acquisition%20for%20entertainment%0Apurposes%20and%20is%20aimed%20at%20both%20researchers%20and%20cinematographers.%20Considering%20the%0Abreadth%20of%20computer%20vision%20research%20and%20the%20lack%20of%20review%20papers%20tied%20to%0Aintelligent%20cinematography%20%28IC%29%2C%20this%20review%20introduces%20a%20holistic%20view%20of%20the%0AIC%20landscape%20while%20providing%20the%20technical%20insight%20for%20experts%20across%20across%0Adisciplines.%20We%20preface%20the%20main%20discussion%20with%20technical%20background%20on%0Agenerative%20AI%2C%20object%20detection%2C%20automated%20camera%20calibration%20and%203-D%20content%0Aacquisition%2C%20and%20link%20explanatory%20articles%20to%20assist%20non-technical%20readers.%20The%0Amain%20discussion%20categorizes%20work%20by%20four%20production%20types%3A%20General%20Production%2C%0AVirtual%20Production%2C%20Live%20Production%20and%20Aerial%20Production.%20Note%20that%20for%0AVirtual%20Production%20we%20do%20not%20discuss%20research%20relating%20to%20virtual%20content%0Aacquisition%2C%20including%20work%20on%20automated%20video%20generation%2C%20like%20Stable%0ADiffusion.%20Within%20each%20section%2C%20we%20%281%29%20sub-classify%20work%20by%20the%20technical%20field%0Aof%20research%20-%20reflected%20by%20the%20subsections%2C%20and%20%282%29%20evaluate%20the%20trends%20and%0Achallenge%20w.r.t%20to%20each%20type%20of%20production.%20In%20the%20final%20chapter%2C%20we%20present%0Aour%20concluding%20remarks%20on%20the%20greater%20scope%20of%20IC%20research%20and%20outline%20work%0Athat%20we%20believe%20has%20significant%20potential%20to%20influence%20the%20whole%20industry.%20We%0Afind%20that%20work%20relating%20to%20virtual%20production%20has%20the%20greatest%20potential%20to%0Aimpact%20other%20mediums%20of%20production%2C%20driven%20by%20the%20growing%20interest%20in%20LED%0Avolumes/stages%20for%20in-camera%20virtual%20effects%20%28ICVFX%29%20and%20automated%203-D%20capture%0Afor%20a%20virtual%20modelling%20of%20real%20world%20scenes%20and%20actors.%20This%20is%20the%20first%0Apiece%20of%20literature%20to%20offer%20a%20structured%20and%20comprehensive%20examination%20of%20IC%0Aresearch.%20Consequently%2C%20we%20address%20ethical%20and%20legal%20concerns%20regarding%20the%20use%0Aof%20creative%20AI%20involving%20artists%2C%20actors%20and%20the%20general%20public%2C%20in%20the...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReviewing%2520Intelligent%2520Cinematography%253A%2520AI%2520research%2520for%2520camera-based%2520video%250A%2520%2520production%26entry.906535625%3DAdrian%2520Azzarelli%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520David%2520R%2520Bull%26entry.1292438233%3D%2520%2520This%2520paper%2520offers%2520a%2520comprehensive%2520review%2520of%2520artificial%2520intelligence%2520%2528AI%2529%250Aresearch%2520in%2520the%2520context%2520of%2520real%2520camera%2520content%2520acquisition%2520for%2520entertainment%250Apurposes%2520and%2520is%2520aimed%2520at%2520both%2520researchers%2520and%2520cinematographers.%2520Considering%2520the%250Abreadth%2520of%2520computer%2520vision%2520research%2520and%2520the%2520lack%2520of%2520review%2520papers%2520tied%2520to%250Aintelligent%2520cinematography%2520%2528IC%2529%252C%2520this%2520review%2520introduces%2520a%2520holistic%2520view%2520of%2520the%250AIC%2520landscape%2520while%2520providing%2520the%2520technical%2520insight%2520for%2520experts%2520across%2520across%250Adisciplines.%2520We%2520preface%2520the%2520main%2520discussion%2520with%2520technical%2520background%2520on%250Agenerative%2520AI%252C%2520object%2520detection%252C%2520automated%2520camera%2520calibration%2520and%25203-D%2520content%250Aacquisition%252C%2520and%2520link%2520explanatory%2520articles%2520to%2520assist%2520non-technical%2520readers.%2520The%250Amain%2520discussion%2520categorizes%2520work%2520by%2520four%2520production%2520types%253A%2520General%2520Production%252C%250AVirtual%2520Production%252C%2520Live%2520Production%2520and%2520Aerial%2520Production.%2520Note%2520that%2520for%250AVirtual%2520Production%2520we%2520do%2520not%2520discuss%2520research%2520relating%2520to%2520virtual%2520content%250Aacquisition%252C%2520including%2520work%2520on%2520automated%2520video%2520generation%252C%2520like%2520Stable%250ADiffusion.%2520Within%2520each%2520section%252C%2520we%2520%25281%2529%2520sub-classify%2520work%2520by%2520the%2520technical%2520field%250Aof%2520research%2520-%2520reflected%2520by%2520the%2520subsections%252C%2520and%2520%25282%2529%2520evaluate%2520the%2520trends%2520and%250Achallenge%2520w.r.t%2520to%2520each%2520type%2520of%2520production.%2520In%2520the%2520final%2520chapter%252C%2520we%2520present%250Aour%2520concluding%2520remarks%2520on%2520the%2520greater%2520scope%2520of%2520IC%2520research%2520and%2520outline%2520work%250Athat%2520we%2520believe%2520has%2520significant%2520potential%2520to%2520influence%2520the%2520whole%2520industry.%2520We%250Afind%2520that%2520work%2520relating%2520to%2520virtual%2520production%2520has%2520the%2520greatest%2520potential%2520to%250Aimpact%2520other%2520mediums%2520of%2520production%252C%2520driven%2520by%2520the%2520growing%2520interest%2520in%2520LED%250Avolumes/stages%2520for%2520in-camera%2520virtual%2520effects%2520%2528ICVFX%2529%2520and%2520automated%25203-D%2520capture%250Afor%2520a%2520virtual%2520modelling%2520of%2520real%2520world%2520scenes%2520and%2520actors.%2520This%2520is%2520the%2520first%250Apiece%2520of%2520literature%2520to%2520offer%2520a%2520structured%2520and%2520comprehensive%2520examination%2520of%2520IC%250Aresearch.%2520Consequently%252C%2520we%2520address%2520ethical%2520and%2520legal%2520concerns%2520regarding%2520the%2520use%250Aof%2520creative%2520AI%2520involving%2520artists%252C%2520actors%2520and%2520the%2520general%2520public%252C%2520in%2520the...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production&entry.906535625=Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull&entry.1292438233=%20%20This%20paper%20offers%20a%20comprehensive%20review%20of%20artificial%20intelligence%20%28AI%29%0Aresearch%20in%20the%20context%20of%20real%20camera%20content%20acquisition%20for%20entertainment%0Apurposes%20and%20is%20aimed%20at%20both%20researchers%20and%20cinematographers.%20Considering%20the%0Abreadth%20of%20computer%20vision%20research%20and%20the%20lack%20of%20review%20papers%20tied%20to%0Aintelligent%20cinematography%20%28IC%29%2C%20this%20review%20introduces%20a%20holistic%20view%20of%20the%0AIC%20landscape%20while%20providing%20the%20technical%20insight%20for%20experts%20across%20across%0Adisciplines.%20We%20preface%20the%20main%20discussion%20with%20technical%20background%20on%0Agenerative%20AI%2C%20object%20detection%2C%20automated%20camera%20calibration%20and%203-D%20content%0Aacquisition%2C%20and%20link%20explanatory%20articles%20to%20assist%20non-technical%20readers.%20The%0Amain%20discussion%20categorizes%20work%20by%20four%20production%20types%3A%20General%20Production%2C%0AVirtual%20Production%2C%20Live%20Production%20and%20Aerial%20Production.%20Note%20that%20for%0AVirtual%20Production%20we%20do%20not%20discuss%20research%20relating%20to%20virtual%20content%0Aacquisition%2C%20including%20work%20on%20automated%20video%20generation%2C%20like%20Stable%0ADiffusion.%20Within%20each%20section%2C%20we%20%281%29%20sub-classify%20work%20by%20the%20technical%20field%0Aof%20research%20-%20reflected%20by%20the%20subsections%2C%20and%20%282%29%20evaluate%20the%20trends%20and%0Achallenge%20w.r.t%20to%20each%20type%20of%20production.%20In%20the%20final%20chapter%2C%20we%20present%0Aour%20concluding%20remarks%20on%20the%20greater%20scope%20of%20IC%20research%20and%20outline%20work%0Athat%20we%20believe%20has%20significant%20potential%20to%20influence%20the%20whole%20industry.%20We%0Afind%20that%20work%20relating%20to%20virtual%20production%20has%20the%20greatest%20potential%20to%0Aimpact%20other%20mediums%20of%20production%2C%20driven%20by%20the%20growing%20interest%20in%20LED%0Avolumes/stages%20for%20in-camera%20virtual%20effects%20%28ICVFX%29%20and%20automated%203-D%20capture%0Afor%20a%20virtual%20modelling%20of%20real%20world%20scenes%20and%20actors.%20This%20is%20the%20first%0Apiece%20of%20literature%20to%20offer%20a%20structured%20and%20comprehensive%20examination%20of%20IC%0Aresearch.%20Consequently%2C%20we%20address%20ethical%20and%20legal%20concerns%20regarding%20the%20use%0Aof%20creative%20AI%20involving%20artists%2C%20actors%20and%20the%20general%20public%2C%20in%20the...%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05039v1&entry.124074799=Read"},
{"title": "DistGrid: Scalable Scene Reconstruction with Distributed\n  Multi-resolution Hash Grid", "author": "Sidun Liu and Peng Qiao and Zongxin Ye and Wenyu Li and Yong Dou", "abstract": "  Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled\nand indoor scene reconstruction. However, there exist some challenges when\nreconstructing large-scale scenes. MLP-based NeRFs suffer from limited network\ncapacity, while volume-based NeRFs are heavily memory-consuming when the scene\nresolution increases. Recent approaches propose to geographically partition the\nscene and learn each sub-region using an individual NeRF. Such partitioning\nstrategies help volume-based NeRF exceed the single GPU memory limit and scale\nto larger scenes. However, this approach requires multiple background NeRF to\nhandle out-of-partition rays, which leads to redundancy of learning. Inspired\nby the fact that the background of current partition is the foreground of\nadjacent partition, we propose a scalable scene reconstruction method based on\njoint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is\ndivided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding\nBoxes, and a novel segmented volume rendering method is proposed to handle\ncross-boundary rays, thereby eliminating the need for background NeRFs. The\nexperiments demonstrate that our method outperforms existing methods on all\nevaluated large-scale scenes, and provides visually plausible scene\nreconstruction. The scalability of our method on reconstruction quality is\nfurther evaluated qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2405.04416v2", "date": "2024-05-08", "relevancy": 2.4293, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6241}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6056}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DistGrid%3A%20Scalable%20Scene%20Reconstruction%20with%20Distributed%0A%20%20Multi-resolution%20Hash%20Grid&body=Title%3A%20DistGrid%3A%20Scalable%20Scene%20Reconstruction%20with%20Distributed%0A%20%20Multi-resolution%20Hash%20Grid%0AAuthor%3A%20Sidun%20Liu%20and%20Peng%20Qiao%20and%20Zongxin%20Ye%20and%20Wenyu%20Li%20and%20Yong%20Dou%0AAbstract%3A%20%20%20Neural%20Radiance%20Field~%28NeRF%29%20achieves%20extremely%20high%20quality%20in%20object-scaled%0Aand%20indoor%20scene%20reconstruction.%20However%2C%20there%20exist%20some%20challenges%20when%0Areconstructing%20large-scale%20scenes.%20MLP-based%20NeRFs%20suffer%20from%20limited%20network%0Acapacity%2C%20while%20volume-based%20NeRFs%20are%20heavily%20memory-consuming%20when%20the%20scene%0Aresolution%20increases.%20Recent%20approaches%20propose%20to%20geographically%20partition%20the%0Ascene%20and%20learn%20each%20sub-region%20using%20an%20individual%20NeRF.%20Such%20partitioning%0Astrategies%20help%20volume-based%20NeRF%20exceed%20the%20single%20GPU%20memory%20limit%20and%20scale%0Ato%20larger%20scenes.%20However%2C%20this%20approach%20requires%20multiple%20background%20NeRF%20to%0Ahandle%20out-of-partition%20rays%2C%20which%20leads%20to%20redundancy%20of%20learning.%20Inspired%0Aby%20the%20fact%20that%20the%20background%20of%20current%20partition%20is%20the%20foreground%20of%0Aadjacent%20partition%2C%20we%20propose%20a%20scalable%20scene%20reconstruction%20method%20based%20on%0Ajoint%20Multi-resolution%20Hash%20Grids%2C%20named%20DistGrid.%20In%20this%20method%2C%20the%20scene%20is%0Adivided%20into%20multiple%20closely-paved%20yet%20non-overlapped%20Axis-Aligned%20Bounding%0ABoxes%2C%20and%20a%20novel%20segmented%20volume%20rendering%20method%20is%20proposed%20to%20handle%0Across-boundary%20rays%2C%20thereby%20eliminating%20the%20need%20for%20background%20NeRFs.%20The%0Aexperiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20methods%20on%20all%0Aevaluated%20large-scale%20scenes%2C%20and%20provides%20visually%20plausible%20scene%0Areconstruction.%20The%20scalability%20of%20our%20method%20on%20reconstruction%20quality%20is%0Afurther%20evaluated%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04416v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistGrid%253A%2520Scalable%2520Scene%2520Reconstruction%2520with%2520Distributed%250A%2520%2520Multi-resolution%2520Hash%2520Grid%26entry.906535625%3DSidun%2520Liu%2520and%2520Peng%2520Qiao%2520and%2520Zongxin%2520Ye%2520and%2520Wenyu%2520Li%2520and%2520Yong%2520Dou%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Field~%2528NeRF%2529%2520achieves%2520extremely%2520high%2520quality%2520in%2520object-scaled%250Aand%2520indoor%2520scene%2520reconstruction.%2520However%252C%2520there%2520exist%2520some%2520challenges%2520when%250Areconstructing%2520large-scale%2520scenes.%2520MLP-based%2520NeRFs%2520suffer%2520from%2520limited%2520network%250Acapacity%252C%2520while%2520volume-based%2520NeRFs%2520are%2520heavily%2520memory-consuming%2520when%2520the%2520scene%250Aresolution%2520increases.%2520Recent%2520approaches%2520propose%2520to%2520geographically%2520partition%2520the%250Ascene%2520and%2520learn%2520each%2520sub-region%2520using%2520an%2520individual%2520NeRF.%2520Such%2520partitioning%250Astrategies%2520help%2520volume-based%2520NeRF%2520exceed%2520the%2520single%2520GPU%2520memory%2520limit%2520and%2520scale%250Ato%2520larger%2520scenes.%2520However%252C%2520this%2520approach%2520requires%2520multiple%2520background%2520NeRF%2520to%250Ahandle%2520out-of-partition%2520rays%252C%2520which%2520leads%2520to%2520redundancy%2520of%2520learning.%2520Inspired%250Aby%2520the%2520fact%2520that%2520the%2520background%2520of%2520current%2520partition%2520is%2520the%2520foreground%2520of%250Aadjacent%2520partition%252C%2520we%2520propose%2520a%2520scalable%2520scene%2520reconstruction%2520method%2520based%2520on%250Ajoint%2520Multi-resolution%2520Hash%2520Grids%252C%2520named%2520DistGrid.%2520In%2520this%2520method%252C%2520the%2520scene%2520is%250Adivided%2520into%2520multiple%2520closely-paved%2520yet%2520non-overlapped%2520Axis-Aligned%2520Bounding%250ABoxes%252C%2520and%2520a%2520novel%2520segmented%2520volume%2520rendering%2520method%2520is%2520proposed%2520to%2520handle%250Across-boundary%2520rays%252C%2520thereby%2520eliminating%2520the%2520need%2520for%2520background%2520NeRFs.%2520The%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520methods%2520on%2520all%250Aevaluated%2520large-scale%2520scenes%252C%2520and%2520provides%2520visually%2520plausible%2520scene%250Areconstruction.%2520The%2520scalability%2520of%2520our%2520method%2520on%2520reconstruction%2520quality%2520is%250Afurther%2520evaluated%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04416v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DistGrid%3A%20Scalable%20Scene%20Reconstruction%20with%20Distributed%0A%20%20Multi-resolution%20Hash%20Grid&entry.906535625=Sidun%20Liu%20and%20Peng%20Qiao%20and%20Zongxin%20Ye%20and%20Wenyu%20Li%20and%20Yong%20Dou&entry.1292438233=%20%20Neural%20Radiance%20Field~%28NeRF%29%20achieves%20extremely%20high%20quality%20in%20object-scaled%0Aand%20indoor%20scene%20reconstruction.%20However%2C%20there%20exist%20some%20challenges%20when%0Areconstructing%20large-scale%20scenes.%20MLP-based%20NeRFs%20suffer%20from%20limited%20network%0Acapacity%2C%20while%20volume-based%20NeRFs%20are%20heavily%20memory-consuming%20when%20the%20scene%0Aresolution%20increases.%20Recent%20approaches%20propose%20to%20geographically%20partition%20the%0Ascene%20and%20learn%20each%20sub-region%20using%20an%20individual%20NeRF.%20Such%20partitioning%0Astrategies%20help%20volume-based%20NeRF%20exceed%20the%20single%20GPU%20memory%20limit%20and%20scale%0Ato%20larger%20scenes.%20However%2C%20this%20approach%20requires%20multiple%20background%20NeRF%20to%0Ahandle%20out-of-partition%20rays%2C%20which%20leads%20to%20redundancy%20of%20learning.%20Inspired%0Aby%20the%20fact%20that%20the%20background%20of%20current%20partition%20is%20the%20foreground%20of%0Aadjacent%20partition%2C%20we%20propose%20a%20scalable%20scene%20reconstruction%20method%20based%20on%0Ajoint%20Multi-resolution%20Hash%20Grids%2C%20named%20DistGrid.%20In%20this%20method%2C%20the%20scene%20is%0Adivided%20into%20multiple%20closely-paved%20yet%20non-overlapped%20Axis-Aligned%20Bounding%0ABoxes%2C%20and%20a%20novel%20segmented%20volume%20rendering%20method%20is%20proposed%20to%20handle%0Across-boundary%20rays%2C%20thereby%20eliminating%20the%20need%20for%20background%20NeRFs.%20The%0Aexperiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20methods%20on%20all%0Aevaluated%20large-scale%20scenes%2C%20and%20provides%20visually%20plausible%20scene%0Areconstruction.%20The%20scalability%20of%20our%20method%20on%20reconstruction%20quality%20is%0Afurther%20evaluated%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04416v2&entry.124074799=Read"},
{"title": "Improving Long Text Understanding with Knowledge Distilled from\n  Summarization Model", "author": "Yan Liu and Yazheng Yang and Xiaokang Chen", "abstract": "  Long text understanding is important yet challenging for natural language\nprocessing. A long article or document usually contains many redundant words\nthat are not pertinent to its gist and sometimes can be regarded as noise. With\nrecent advances of abstractive summarization, we propose our \\emph{Gist\nDetector} to leverage the gist detection ability of a summarization model and\nintegrate the extracted gist into downstream models to enhance their long text\nunderstanding ability. Specifically, Gist Detector first learns the gist\ndetection knowledge distilled from a summarization model, and then produces\ngist-aware representations to augment downstream models. We evaluate our method\non three different tasks: long document classification, distantly supervised\nopen-domain question answering, and non-parallel text style transfer. The\nexperimental results show that our method can significantly improve the\nperformance of baseline models on all tasks.\n", "link": "http://arxiv.org/abs/2405.04955v1", "date": "2024-05-08", "relevancy": 2.3912, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4911}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4775}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Long%20Text%20Understanding%20with%20Knowledge%20Distilled%20from%0A%20%20Summarization%20Model&body=Title%3A%20Improving%20Long%20Text%20Understanding%20with%20Knowledge%20Distilled%20from%0A%20%20Summarization%20Model%0AAuthor%3A%20Yan%20Liu%20and%20Yazheng%20Yang%20and%20Xiaokang%20Chen%0AAbstract%3A%20%20%20Long%20text%20understanding%20is%20important%20yet%20challenging%20for%20natural%20language%0Aprocessing.%20A%20long%20article%20or%20document%20usually%20contains%20many%20redundant%20words%0Athat%20are%20not%20pertinent%20to%20its%20gist%20and%20sometimes%20can%20be%20regarded%20as%20noise.%20With%0Arecent%20advances%20of%20abstractive%20summarization%2C%20we%20propose%20our%20%5Cemph%7BGist%0ADetector%7D%20to%20leverage%20the%20gist%20detection%20ability%20of%20a%20summarization%20model%20and%0Aintegrate%20the%20extracted%20gist%20into%20downstream%20models%20to%20enhance%20their%20long%20text%0Aunderstanding%20ability.%20Specifically%2C%20Gist%20Detector%20first%20learns%20the%20gist%0Adetection%20knowledge%20distilled%20from%20a%20summarization%20model%2C%20and%20then%20produces%0Agist-aware%20representations%20to%20augment%20downstream%20models.%20We%20evaluate%20our%20method%0Aon%20three%20different%20tasks%3A%20long%20document%20classification%2C%20distantly%20supervised%0Aopen-domain%20question%20answering%2C%20and%20non-parallel%20text%20style%20transfer.%20The%0Aexperimental%20results%20show%20that%20our%20method%20can%20significantly%20improve%20the%0Aperformance%20of%20baseline%20models%20on%20all%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Long%2520Text%2520Understanding%2520with%2520Knowledge%2520Distilled%2520from%250A%2520%2520Summarization%2520Model%26entry.906535625%3DYan%2520Liu%2520and%2520Yazheng%2520Yang%2520and%2520Xiaokang%2520Chen%26entry.1292438233%3D%2520%2520Long%2520text%2520understanding%2520is%2520important%2520yet%2520challenging%2520for%2520natural%2520language%250Aprocessing.%2520A%2520long%2520article%2520or%2520document%2520usually%2520contains%2520many%2520redundant%2520words%250Athat%2520are%2520not%2520pertinent%2520to%2520its%2520gist%2520and%2520sometimes%2520can%2520be%2520regarded%2520as%2520noise.%2520With%250Arecent%2520advances%2520of%2520abstractive%2520summarization%252C%2520we%2520propose%2520our%2520%255Cemph%257BGist%250ADetector%257D%2520to%2520leverage%2520the%2520gist%2520detection%2520ability%2520of%2520a%2520summarization%2520model%2520and%250Aintegrate%2520the%2520extracted%2520gist%2520into%2520downstream%2520models%2520to%2520enhance%2520their%2520long%2520text%250Aunderstanding%2520ability.%2520Specifically%252C%2520Gist%2520Detector%2520first%2520learns%2520the%2520gist%250Adetection%2520knowledge%2520distilled%2520from%2520a%2520summarization%2520model%252C%2520and%2520then%2520produces%250Agist-aware%2520representations%2520to%2520augment%2520downstream%2520models.%2520We%2520evaluate%2520our%2520method%250Aon%2520three%2520different%2520tasks%253A%2520long%2520document%2520classification%252C%2520distantly%2520supervised%250Aopen-domain%2520question%2520answering%252C%2520and%2520non-parallel%2520text%2520style%2520transfer.%2520The%250Aexperimental%2520results%2520show%2520that%2520our%2520method%2520can%2520significantly%2520improve%2520the%250Aperformance%2520of%2520baseline%2520models%2520on%2520all%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Long%20Text%20Understanding%20with%20Knowledge%20Distilled%20from%0A%20%20Summarization%20Model&entry.906535625=Yan%20Liu%20and%20Yazheng%20Yang%20and%20Xiaokang%20Chen&entry.1292438233=%20%20Long%20text%20understanding%20is%20important%20yet%20challenging%20for%20natural%20language%0Aprocessing.%20A%20long%20article%20or%20document%20usually%20contains%20many%20redundant%20words%0Athat%20are%20not%20pertinent%20to%20its%20gist%20and%20sometimes%20can%20be%20regarded%20as%20noise.%20With%0Arecent%20advances%20of%20abstractive%20summarization%2C%20we%20propose%20our%20%5Cemph%7BGist%0ADetector%7D%20to%20leverage%20the%20gist%20detection%20ability%20of%20a%20summarization%20model%20and%0Aintegrate%20the%20extracted%20gist%20into%20downstream%20models%20to%20enhance%20their%20long%20text%0Aunderstanding%20ability.%20Specifically%2C%20Gist%20Detector%20first%20learns%20the%20gist%0Adetection%20knowledge%20distilled%20from%20a%20summarization%20model%2C%20and%20then%20produces%0Agist-aware%20representations%20to%20augment%20downstream%20models.%20We%20evaluate%20our%20method%0Aon%20three%20different%20tasks%3A%20long%20document%20classification%2C%20distantly%20supervised%0Aopen-domain%20question%20answering%2C%20and%20non-parallel%20text%20style%20transfer.%20The%0Aexperimental%20results%20show%20that%20our%20method%20can%20significantly%20improve%20the%0Aperformance%20of%20baseline%20models%20on%20all%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04955v1&entry.124074799=Read"},
{"title": "Predictive Mapping of Spectral Signatures from RGB Imagery for Off-Road\n  Terrain Analysis", "author": "Sarvesh Prajapati and Ananya Trivedi and Bruce Maxwell and Taskin Padir", "abstract": "  Accurate identification of complex terrain characteristics, such as soil\ncomposition and coefficient of friction, is essential for model-based planning\nand control of mobile robots in off-road environments. Spectral signatures\nleverage distinct patterns of light absorption and reflection to identify\nvarious materials, enabling precise characterization of their inherent\nproperties. Recent research in robotics has explored the adoption of\nspectroscopy to enhance perception and interaction with environments. However,\nthe significant cost and elaborate setup required for mounting these sensors\npresent formidable barriers to widespread adoption. In this study, we introduce\nRS-Net (RGB to Spectral Network), a deep neural network architecture designed\nto map RGB images to corresponding spectral signatures. We illustrate how\nRS-Net can be synergistically combined with Co-Learning techniques for terrain\nproperty estimation. Initial results demonstrate the effectiveness of this\napproach in characterizing spectral signatures across an extensive off-road\nreal-world dataset. These findings highlight the feasibility of terrain\nproperty estimation using only RGB cameras.\n", "link": "http://arxiv.org/abs/2405.04979v1", "date": "2024-05-08", "relevancy": 2.3432, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6382}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5792}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictive%20Mapping%20of%20Spectral%20Signatures%20from%20RGB%20Imagery%20for%20Off-Road%0A%20%20Terrain%20Analysis&body=Title%3A%20Predictive%20Mapping%20of%20Spectral%20Signatures%20from%20RGB%20Imagery%20for%20Off-Road%0A%20%20Terrain%20Analysis%0AAuthor%3A%20Sarvesh%20Prajapati%20and%20Ananya%20Trivedi%20and%20Bruce%20Maxwell%20and%20Taskin%20Padir%0AAbstract%3A%20%20%20Accurate%20identification%20of%20complex%20terrain%20characteristics%2C%20such%20as%20soil%0Acomposition%20and%20coefficient%20of%20friction%2C%20is%20essential%20for%20model-based%20planning%0Aand%20control%20of%20mobile%20robots%20in%20off-road%20environments.%20Spectral%20signatures%0Aleverage%20distinct%20patterns%20of%20light%20absorption%20and%20reflection%20to%20identify%0Avarious%20materials%2C%20enabling%20precise%20characterization%20of%20their%20inherent%0Aproperties.%20Recent%20research%20in%20robotics%20has%20explored%20the%20adoption%20of%0Aspectroscopy%20to%20enhance%20perception%20and%20interaction%20with%20environments.%20However%2C%0Athe%20significant%20cost%20and%20elaborate%20setup%20required%20for%20mounting%20these%20sensors%0Apresent%20formidable%20barriers%20to%20widespread%20adoption.%20In%20this%20study%2C%20we%20introduce%0ARS-Net%20%28RGB%20to%20Spectral%20Network%29%2C%20a%20deep%20neural%20network%20architecture%20designed%0Ato%20map%20RGB%20images%20to%20corresponding%20spectral%20signatures.%20We%20illustrate%20how%0ARS-Net%20can%20be%20synergistically%20combined%20with%20Co-Learning%20techniques%20for%20terrain%0Aproperty%20estimation.%20Initial%20results%20demonstrate%20the%20effectiveness%20of%20this%0Aapproach%20in%20characterizing%20spectral%20signatures%20across%20an%20extensive%20off-road%0Areal-world%20dataset.%20These%20findings%20highlight%20the%20feasibility%20of%20terrain%0Aproperty%20estimation%20using%20only%20RGB%20cameras.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictive%2520Mapping%2520of%2520Spectral%2520Signatures%2520from%2520RGB%2520Imagery%2520for%2520Off-Road%250A%2520%2520Terrain%2520Analysis%26entry.906535625%3DSarvesh%2520Prajapati%2520and%2520Ananya%2520Trivedi%2520and%2520Bruce%2520Maxwell%2520and%2520Taskin%2520Padir%26entry.1292438233%3D%2520%2520Accurate%2520identification%2520of%2520complex%2520terrain%2520characteristics%252C%2520such%2520as%2520soil%250Acomposition%2520and%2520coefficient%2520of%2520friction%252C%2520is%2520essential%2520for%2520model-based%2520planning%250Aand%2520control%2520of%2520mobile%2520robots%2520in%2520off-road%2520environments.%2520Spectral%2520signatures%250Aleverage%2520distinct%2520patterns%2520of%2520light%2520absorption%2520and%2520reflection%2520to%2520identify%250Avarious%2520materials%252C%2520enabling%2520precise%2520characterization%2520of%2520their%2520inherent%250Aproperties.%2520Recent%2520research%2520in%2520robotics%2520has%2520explored%2520the%2520adoption%2520of%250Aspectroscopy%2520to%2520enhance%2520perception%2520and%2520interaction%2520with%2520environments.%2520However%252C%250Athe%2520significant%2520cost%2520and%2520elaborate%2520setup%2520required%2520for%2520mounting%2520these%2520sensors%250Apresent%2520formidable%2520barriers%2520to%2520widespread%2520adoption.%2520In%2520this%2520study%252C%2520we%2520introduce%250ARS-Net%2520%2528RGB%2520to%2520Spectral%2520Network%2529%252C%2520a%2520deep%2520neural%2520network%2520architecture%2520designed%250Ato%2520map%2520RGB%2520images%2520to%2520corresponding%2520spectral%2520signatures.%2520We%2520illustrate%2520how%250ARS-Net%2520can%2520be%2520synergistically%2520combined%2520with%2520Co-Learning%2520techniques%2520for%2520terrain%250Aproperty%2520estimation.%2520Initial%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520this%250Aapproach%2520in%2520characterizing%2520spectral%2520signatures%2520across%2520an%2520extensive%2520off-road%250Areal-world%2520dataset.%2520These%2520findings%2520highlight%2520the%2520feasibility%2520of%2520terrain%250Aproperty%2520estimation%2520using%2520only%2520RGB%2520cameras.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Mapping%20of%20Spectral%20Signatures%20from%20RGB%20Imagery%20for%20Off-Road%0A%20%20Terrain%20Analysis&entry.906535625=Sarvesh%20Prajapati%20and%20Ananya%20Trivedi%20and%20Bruce%20Maxwell%20and%20Taskin%20Padir&entry.1292438233=%20%20Accurate%20identification%20of%20complex%20terrain%20characteristics%2C%20such%20as%20soil%0Acomposition%20and%20coefficient%20of%20friction%2C%20is%20essential%20for%20model-based%20planning%0Aand%20control%20of%20mobile%20robots%20in%20off-road%20environments.%20Spectral%20signatures%0Aleverage%20distinct%20patterns%20of%20light%20absorption%20and%20reflection%20to%20identify%0Avarious%20materials%2C%20enabling%20precise%20characterization%20of%20their%20inherent%0Aproperties.%20Recent%20research%20in%20robotics%20has%20explored%20the%20adoption%20of%0Aspectroscopy%20to%20enhance%20perception%20and%20interaction%20with%20environments.%20However%2C%0Athe%20significant%20cost%20and%20elaborate%20setup%20required%20for%20mounting%20these%20sensors%0Apresent%20formidable%20barriers%20to%20widespread%20adoption.%20In%20this%20study%2C%20we%20introduce%0ARS-Net%20%28RGB%20to%20Spectral%20Network%29%2C%20a%20deep%20neural%20network%20architecture%20designed%0Ato%20map%20RGB%20images%20to%20corresponding%20spectral%20signatures.%20We%20illustrate%20how%0ARS-Net%20can%20be%20synergistically%20combined%20with%20Co-Learning%20techniques%20for%20terrain%0Aproperty%20estimation.%20Initial%20results%20demonstrate%20the%20effectiveness%20of%20this%0Aapproach%20in%20characterizing%20spectral%20signatures%20across%20an%20extensive%20off-road%0Areal-world%20dataset.%20These%20findings%20highlight%20the%20feasibility%20of%20terrain%0Aproperty%20estimation%20using%20only%20RGB%20cameras.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04979v1&entry.124074799=Read"},
{"title": "MOTLEE: Collaborative Multi-Object Tracking Using Temporal Consistency\n  for Neighboring Robot Frame Alignment", "author": "Mason B. Peterson and Parker C. Lusk and Antonio Avila and Jonathan P. How", "abstract": "  Knowing the locations of nearby moving objects is important for a mobile\nrobot to operate safely in a dynamic environment. Dynamic object tracking\nperformance can be improved if robots share observations of tracked objects\nwith nearby team members in real-time. To share observations, a robot must make\nup-to-date estimates of the transformation from its coordinate frame to the\nframe of each neighbor, which can be challenging because of odometry drift. We\npresent Multiple Object Tracking with Localization Error Elimination (MOTLEE),\na complete system for a multi-robot team to accurately estimate frame\ntransformations and collaboratively track dynamic objects. To accomplish this,\nrobots use open-set image-segmentation methods to build object maps of their\nenvironment and then use our Temporally Consistent Alignment of Frames Filter\n(TCAFF) to align maps and estimate coordinate frame transformations without any\ninitial knowledge of neighboring robot poses. We show that our method for\naligning frames enables a team of four robots to collaboratively track six\npedestrians with accuracy similar to that of a system with ground truth\nlocalization in a challenging hardware demonstration. The code and hardware\ndataset are available at https://github.com/mit-acl/motlee.\n", "link": "http://arxiv.org/abs/2405.05210v1", "date": "2024-05-08", "relevancy": 2.3329, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5824}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOTLEE%3A%20Collaborative%20Multi-Object%20Tracking%20Using%20Temporal%20Consistency%0A%20%20for%20Neighboring%20Robot%20Frame%20Alignment&body=Title%3A%20MOTLEE%3A%20Collaborative%20Multi-Object%20Tracking%20Using%20Temporal%20Consistency%0A%20%20for%20Neighboring%20Robot%20Frame%20Alignment%0AAuthor%3A%20Mason%20B.%20Peterson%20and%20Parker%20C.%20Lusk%20and%20Antonio%20Avila%20and%20Jonathan%20P.%20How%0AAbstract%3A%20%20%20Knowing%20the%20locations%20of%20nearby%20moving%20objects%20is%20important%20for%20a%20mobile%0Arobot%20to%20operate%20safely%20in%20a%20dynamic%20environment.%20Dynamic%20object%20tracking%0Aperformance%20can%20be%20improved%20if%20robots%20share%20observations%20of%20tracked%20objects%0Awith%20nearby%20team%20members%20in%20real-time.%20To%20share%20observations%2C%20a%20robot%20must%20make%0Aup-to-date%20estimates%20of%20the%20transformation%20from%20its%20coordinate%20frame%20to%20the%0Aframe%20of%20each%20neighbor%2C%20which%20can%20be%20challenging%20because%20of%20odometry%20drift.%20We%0Apresent%20Multiple%20Object%20Tracking%20with%20Localization%20Error%20Elimination%20%28MOTLEE%29%2C%0Aa%20complete%20system%20for%20a%20multi-robot%20team%20to%20accurately%20estimate%20frame%0Atransformations%20and%20collaboratively%20track%20dynamic%20objects.%20To%20accomplish%20this%2C%0Arobots%20use%20open-set%20image-segmentation%20methods%20to%20build%20object%20maps%20of%20their%0Aenvironment%20and%20then%20use%20our%20Temporally%20Consistent%20Alignment%20of%20Frames%20Filter%0A%28TCAFF%29%20to%20align%20maps%20and%20estimate%20coordinate%20frame%20transformations%20without%20any%0Ainitial%20knowledge%20of%20neighboring%20robot%20poses.%20We%20show%20that%20our%20method%20for%0Aaligning%20frames%20enables%20a%20team%20of%20four%20robots%20to%20collaboratively%20track%20six%0Apedestrians%20with%20accuracy%20similar%20to%20that%20of%20a%20system%20with%20ground%20truth%0Alocalization%20in%20a%20challenging%20hardware%20demonstration.%20The%20code%20and%20hardware%0Adataset%20are%20available%20at%20https%3A//github.com/mit-acl/motlee.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOTLEE%253A%2520Collaborative%2520Multi-Object%2520Tracking%2520Using%2520Temporal%2520Consistency%250A%2520%2520for%2520Neighboring%2520Robot%2520Frame%2520Alignment%26entry.906535625%3DMason%2520B.%2520Peterson%2520and%2520Parker%2520C.%2520Lusk%2520and%2520Antonio%2520Avila%2520and%2520Jonathan%2520P.%2520How%26entry.1292438233%3D%2520%2520Knowing%2520the%2520locations%2520of%2520nearby%2520moving%2520objects%2520is%2520important%2520for%2520a%2520mobile%250Arobot%2520to%2520operate%2520safely%2520in%2520a%2520dynamic%2520environment.%2520Dynamic%2520object%2520tracking%250Aperformance%2520can%2520be%2520improved%2520if%2520robots%2520share%2520observations%2520of%2520tracked%2520objects%250Awith%2520nearby%2520team%2520members%2520in%2520real-time.%2520To%2520share%2520observations%252C%2520a%2520robot%2520must%2520make%250Aup-to-date%2520estimates%2520of%2520the%2520transformation%2520from%2520its%2520coordinate%2520frame%2520to%2520the%250Aframe%2520of%2520each%2520neighbor%252C%2520which%2520can%2520be%2520challenging%2520because%2520of%2520odometry%2520drift.%2520We%250Apresent%2520Multiple%2520Object%2520Tracking%2520with%2520Localization%2520Error%2520Elimination%2520%2528MOTLEE%2529%252C%250Aa%2520complete%2520system%2520for%2520a%2520multi-robot%2520team%2520to%2520accurately%2520estimate%2520frame%250Atransformations%2520and%2520collaboratively%2520track%2520dynamic%2520objects.%2520To%2520accomplish%2520this%252C%250Arobots%2520use%2520open-set%2520image-segmentation%2520methods%2520to%2520build%2520object%2520maps%2520of%2520their%250Aenvironment%2520and%2520then%2520use%2520our%2520Temporally%2520Consistent%2520Alignment%2520of%2520Frames%2520Filter%250A%2528TCAFF%2529%2520to%2520align%2520maps%2520and%2520estimate%2520coordinate%2520frame%2520transformations%2520without%2520any%250Ainitial%2520knowledge%2520of%2520neighboring%2520robot%2520poses.%2520We%2520show%2520that%2520our%2520method%2520for%250Aaligning%2520frames%2520enables%2520a%2520team%2520of%2520four%2520robots%2520to%2520collaboratively%2520track%2520six%250Apedestrians%2520with%2520accuracy%2520similar%2520to%2520that%2520of%2520a%2520system%2520with%2520ground%2520truth%250Alocalization%2520in%2520a%2520challenging%2520hardware%2520demonstration.%2520The%2520code%2520and%2520hardware%250Adataset%2520are%2520available%2520at%2520https%253A//github.com/mit-acl/motlee.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOTLEE%3A%20Collaborative%20Multi-Object%20Tracking%20Using%20Temporal%20Consistency%0A%20%20for%20Neighboring%20Robot%20Frame%20Alignment&entry.906535625=Mason%20B.%20Peterson%20and%20Parker%20C.%20Lusk%20and%20Antonio%20Avila%20and%20Jonathan%20P.%20How&entry.1292438233=%20%20Knowing%20the%20locations%20of%20nearby%20moving%20objects%20is%20important%20for%20a%20mobile%0Arobot%20to%20operate%20safely%20in%20a%20dynamic%20environment.%20Dynamic%20object%20tracking%0Aperformance%20can%20be%20improved%20if%20robots%20share%20observations%20of%20tracked%20objects%0Awith%20nearby%20team%20members%20in%20real-time.%20To%20share%20observations%2C%20a%20robot%20must%20make%0Aup-to-date%20estimates%20of%20the%20transformation%20from%20its%20coordinate%20frame%20to%20the%0Aframe%20of%20each%20neighbor%2C%20which%20can%20be%20challenging%20because%20of%20odometry%20drift.%20We%0Apresent%20Multiple%20Object%20Tracking%20with%20Localization%20Error%20Elimination%20%28MOTLEE%29%2C%0Aa%20complete%20system%20for%20a%20multi-robot%20team%20to%20accurately%20estimate%20frame%0Atransformations%20and%20collaboratively%20track%20dynamic%20objects.%20To%20accomplish%20this%2C%0Arobots%20use%20open-set%20image-segmentation%20methods%20to%20build%20object%20maps%20of%20their%0Aenvironment%20and%20then%20use%20our%20Temporally%20Consistent%20Alignment%20of%20Frames%20Filter%0A%28TCAFF%29%20to%20align%20maps%20and%20estimate%20coordinate%20frame%20transformations%20without%20any%0Ainitial%20knowledge%20of%20neighboring%20robot%20poses.%20We%20show%20that%20our%20method%20for%0Aaligning%20frames%20enables%20a%20team%20of%20four%20robots%20to%20collaboratively%20track%20six%0Apedestrians%20with%20accuracy%20similar%20to%20that%20of%20a%20system%20with%20ground%20truth%0Alocalization%20in%20a%20challenging%20hardware%20demonstration.%20The%20code%20and%20hardware%0Adataset%20are%20available%20at%20https%3A//github.com/mit-acl/motlee.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05210v1&entry.124074799=Read"},
{"title": "Dynamic Data Layout Optimization with Worst-case Guarantees", "author": "Kexin Rong and Paul Liu and Sarah Ashok Sonje and Moses Charikar", "abstract": "  Many data analytics systems store and process large datasets in partitions\ncontaining millions of rows. By mapping rows to partitions in an optimized way,\nit is possible to improve query performance by skipping over large numbers of\nirrelevant partitions during query processing. This mapping is referred to as a\ndata layout. Recent works have shown that customizing the data layout to the\nanticipated query workload greatly improves query performance, but the\nperformance benefits may disappear if the workload changes. Reorganizing data\nlayouts to accommodate workload drift can resolve this issue, but\nreorganization costs could exceed query savings if not done carefully.\n  In this paper, we present an algorithmic framework OReO that makes online\nreorganization decisions to balance the benefits of improved query performance\nwith the costs of reorganization. Our framework extends results from Metrical\nTask Systems to provide a tight bound on the worst-case performance guarantee\nfor online reorganization, without prior knowledge of the query workload.\nThrough evaluation on real-world datasets and query workloads, our experiments\ndemonstrate that online reorganization with OReO can lead to an up to 32%\nimprovement in combined query and reorganization time compared to using a\nsingle, optimized data layout for the entire workload.\n", "link": "http://arxiv.org/abs/2405.04984v1", "date": "2024-05-08", "relevancy": 2.3148, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5054}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4509}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Data%20Layout%20Optimization%20with%20Worst-case%20Guarantees&body=Title%3A%20Dynamic%20Data%20Layout%20Optimization%20with%20Worst-case%20Guarantees%0AAuthor%3A%20Kexin%20Rong%20and%20Paul%20Liu%20and%20Sarah%20Ashok%20Sonje%20and%20Moses%20Charikar%0AAbstract%3A%20%20%20Many%20data%20analytics%20systems%20store%20and%20process%20large%20datasets%20in%20partitions%0Acontaining%20millions%20of%20rows.%20By%20mapping%20rows%20to%20partitions%20in%20an%20optimized%20way%2C%0Ait%20is%20possible%20to%20improve%20query%20performance%20by%20skipping%20over%20large%20numbers%20of%0Airrelevant%20partitions%20during%20query%20processing.%20This%20mapping%20is%20referred%20to%20as%20a%0Adata%20layout.%20Recent%20works%20have%20shown%20that%20customizing%20the%20data%20layout%20to%20the%0Aanticipated%20query%20workload%20greatly%20improves%20query%20performance%2C%20but%20the%0Aperformance%20benefits%20may%20disappear%20if%20the%20workload%20changes.%20Reorganizing%20data%0Alayouts%20to%20accommodate%20workload%20drift%20can%20resolve%20this%20issue%2C%20but%0Areorganization%20costs%20could%20exceed%20query%20savings%20if%20not%20done%20carefully.%0A%20%20In%20this%20paper%2C%20we%20present%20an%20algorithmic%20framework%20OReO%20that%20makes%20online%0Areorganization%20decisions%20to%20balance%20the%20benefits%20of%20improved%20query%20performance%0Awith%20the%20costs%20of%20reorganization.%20Our%20framework%20extends%20results%20from%20Metrical%0ATask%20Systems%20to%20provide%20a%20tight%20bound%20on%20the%20worst-case%20performance%20guarantee%0Afor%20online%20reorganization%2C%20without%20prior%20knowledge%20of%20the%20query%20workload.%0AThrough%20evaluation%20on%20real-world%20datasets%20and%20query%20workloads%2C%20our%20experiments%0Ademonstrate%20that%20online%20reorganization%20with%20OReO%20can%20lead%20to%20an%20up%20to%2032%25%0Aimprovement%20in%20combined%20query%20and%20reorganization%20time%20compared%20to%20using%20a%0Asingle%2C%20optimized%20data%20layout%20for%20the%20entire%20workload.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Data%2520Layout%2520Optimization%2520with%2520Worst-case%2520Guarantees%26entry.906535625%3DKexin%2520Rong%2520and%2520Paul%2520Liu%2520and%2520Sarah%2520Ashok%2520Sonje%2520and%2520Moses%2520Charikar%26entry.1292438233%3D%2520%2520Many%2520data%2520analytics%2520systems%2520store%2520and%2520process%2520large%2520datasets%2520in%2520partitions%250Acontaining%2520millions%2520of%2520rows.%2520By%2520mapping%2520rows%2520to%2520partitions%2520in%2520an%2520optimized%2520way%252C%250Ait%2520is%2520possible%2520to%2520improve%2520query%2520performance%2520by%2520skipping%2520over%2520large%2520numbers%2520of%250Airrelevant%2520partitions%2520during%2520query%2520processing.%2520This%2520mapping%2520is%2520referred%2520to%2520as%2520a%250Adata%2520layout.%2520Recent%2520works%2520have%2520shown%2520that%2520customizing%2520the%2520data%2520layout%2520to%2520the%250Aanticipated%2520query%2520workload%2520greatly%2520improves%2520query%2520performance%252C%2520but%2520the%250Aperformance%2520benefits%2520may%2520disappear%2520if%2520the%2520workload%2520changes.%2520Reorganizing%2520data%250Alayouts%2520to%2520accommodate%2520workload%2520drift%2520can%2520resolve%2520this%2520issue%252C%2520but%250Areorganization%2520costs%2520could%2520exceed%2520query%2520savings%2520if%2520not%2520done%2520carefully.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520algorithmic%2520framework%2520OReO%2520that%2520makes%2520online%250Areorganization%2520decisions%2520to%2520balance%2520the%2520benefits%2520of%2520improved%2520query%2520performance%250Awith%2520the%2520costs%2520of%2520reorganization.%2520Our%2520framework%2520extends%2520results%2520from%2520Metrical%250ATask%2520Systems%2520to%2520provide%2520a%2520tight%2520bound%2520on%2520the%2520worst-case%2520performance%2520guarantee%250Afor%2520online%2520reorganization%252C%2520without%2520prior%2520knowledge%2520of%2520the%2520query%2520workload.%250AThrough%2520evaluation%2520on%2520real-world%2520datasets%2520and%2520query%2520workloads%252C%2520our%2520experiments%250Ademonstrate%2520that%2520online%2520reorganization%2520with%2520OReO%2520can%2520lead%2520to%2520an%2520up%2520to%252032%2525%250Aimprovement%2520in%2520combined%2520query%2520and%2520reorganization%2520time%2520compared%2520to%2520using%2520a%250Asingle%252C%2520optimized%2520data%2520layout%2520for%2520the%2520entire%2520workload.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Data%20Layout%20Optimization%20with%20Worst-case%20Guarantees&entry.906535625=Kexin%20Rong%20and%20Paul%20Liu%20and%20Sarah%20Ashok%20Sonje%20and%20Moses%20Charikar&entry.1292438233=%20%20Many%20data%20analytics%20systems%20store%20and%20process%20large%20datasets%20in%20partitions%0Acontaining%20millions%20of%20rows.%20By%20mapping%20rows%20to%20partitions%20in%20an%20optimized%20way%2C%0Ait%20is%20possible%20to%20improve%20query%20performance%20by%20skipping%20over%20large%20numbers%20of%0Airrelevant%20partitions%20during%20query%20processing.%20This%20mapping%20is%20referred%20to%20as%20a%0Adata%20layout.%20Recent%20works%20have%20shown%20that%20customizing%20the%20data%20layout%20to%20the%0Aanticipated%20query%20workload%20greatly%20improves%20query%20performance%2C%20but%20the%0Aperformance%20benefits%20may%20disappear%20if%20the%20workload%20changes.%20Reorganizing%20data%0Alayouts%20to%20accommodate%20workload%20drift%20can%20resolve%20this%20issue%2C%20but%0Areorganization%20costs%20could%20exceed%20query%20savings%20if%20not%20done%20carefully.%0A%20%20In%20this%20paper%2C%20we%20present%20an%20algorithmic%20framework%20OReO%20that%20makes%20online%0Areorganization%20decisions%20to%20balance%20the%20benefits%20of%20improved%20query%20performance%0Awith%20the%20costs%20of%20reorganization.%20Our%20framework%20extends%20results%20from%20Metrical%0ATask%20Systems%20to%20provide%20a%20tight%20bound%20on%20the%20worst-case%20performance%20guarantee%0Afor%20online%20reorganization%2C%20without%20prior%20knowledge%20of%20the%20query%20workload.%0AThrough%20evaluation%20on%20real-world%20datasets%20and%20query%20workloads%2C%20our%20experiments%0Ademonstrate%20that%20online%20reorganization%20with%20OReO%20can%20lead%20to%20an%20up%20to%2032%25%0Aimprovement%20in%20combined%20query%20and%20reorganization%20time%20compared%20to%20using%20a%0Asingle%2C%20optimized%20data%20layout%20for%20the%20entire%20workload.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04984v1&entry.124074799=Read"},
{"title": "Multi-scale Bottleneck Transformer for Weakly Supervised Multimodal\n  Violence Detection", "author": "Shengyang Sun and Xiaojin Gong", "abstract": "  Weakly supervised multimodal violence detection aims to learn a violence\ndetection model by leveraging multiple modalities such as RGB, optical flow,\nand audio, while only video-level annotations are available. In the pursuit of\neffective multimodal violence detection (MVD), information redundancy, modality\nimbalance, and modality asynchrony are identified as three key challenges. In\nthis work, we propose a new weakly supervised MVD method that explicitly\naddresses these challenges. Specifically, we introduce a multi-scale bottleneck\ntransformer (MSBT) based fusion module that employs a reduced number of\nbottleneck tokens to gradually condense information and fuse each pair of\nmodalities and utilizes a bottleneck token-based weighting scheme to highlight\nmore important fused features. Furthermore, we propose a temporal consistency\ncontrast loss to semantically align pairwise fused features. Experiments on the\nlargest-scale XD-Violence dataset demonstrate that the proposed method achieves\nstate-of-the-art performance. Code is available at\nhttps://github.com/shengyangsun/MSBT.\n", "link": "http://arxiv.org/abs/2405.05130v1", "date": "2024-05-08", "relevancy": 2.3123, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5816}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5809}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20Bottleneck%20Transformer%20for%20Weakly%20Supervised%20Multimodal%0A%20%20Violence%20Detection&body=Title%3A%20Multi-scale%20Bottleneck%20Transformer%20for%20Weakly%20Supervised%20Multimodal%0A%20%20Violence%20Detection%0AAuthor%3A%20Shengyang%20Sun%20and%20Xiaojin%20Gong%0AAbstract%3A%20%20%20Weakly%20supervised%20multimodal%20violence%20detection%20aims%20to%20learn%20a%20violence%0Adetection%20model%20by%20leveraging%20multiple%20modalities%20such%20as%20RGB%2C%20optical%20flow%2C%0Aand%20audio%2C%20while%20only%20video-level%20annotations%20are%20available.%20In%20the%20pursuit%20of%0Aeffective%20multimodal%20violence%20detection%20%28MVD%29%2C%20information%20redundancy%2C%20modality%0Aimbalance%2C%20and%20modality%20asynchrony%20are%20identified%20as%20three%20key%20challenges.%20In%0Athis%20work%2C%20we%20propose%20a%20new%20weakly%20supervised%20MVD%20method%20that%20explicitly%0Aaddresses%20these%20challenges.%20Specifically%2C%20we%20introduce%20a%20multi-scale%20bottleneck%0Atransformer%20%28MSBT%29%20based%20fusion%20module%20that%20employs%20a%20reduced%20number%20of%0Abottleneck%20tokens%20to%20gradually%20condense%20information%20and%20fuse%20each%20pair%20of%0Amodalities%20and%20utilizes%20a%20bottleneck%20token-based%20weighting%20scheme%20to%20highlight%0Amore%20important%20fused%20features.%20Furthermore%2C%20we%20propose%20a%20temporal%20consistency%0Acontrast%20loss%20to%20semantically%20align%20pairwise%20fused%20features.%20Experiments%20on%20the%0Alargest-scale%20XD-Violence%20dataset%20demonstrate%20that%20the%20proposed%20method%20achieves%0Astate-of-the-art%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/shengyangsun/MSBT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-scale%2520Bottleneck%2520Transformer%2520for%2520Weakly%2520Supervised%2520Multimodal%250A%2520%2520Violence%2520Detection%26entry.906535625%3DShengyang%2520Sun%2520and%2520Xiaojin%2520Gong%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520multimodal%2520violence%2520detection%2520aims%2520to%2520learn%2520a%2520violence%250Adetection%2520model%2520by%2520leveraging%2520multiple%2520modalities%2520such%2520as%2520RGB%252C%2520optical%2520flow%252C%250Aand%2520audio%252C%2520while%2520only%2520video-level%2520annotations%2520are%2520available.%2520In%2520the%2520pursuit%2520of%250Aeffective%2520multimodal%2520violence%2520detection%2520%2528MVD%2529%252C%2520information%2520redundancy%252C%2520modality%250Aimbalance%252C%2520and%2520modality%2520asynchrony%2520are%2520identified%2520as%2520three%2520key%2520challenges.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520new%2520weakly%2520supervised%2520MVD%2520method%2520that%2520explicitly%250Aaddresses%2520these%2520challenges.%2520Specifically%252C%2520we%2520introduce%2520a%2520multi-scale%2520bottleneck%250Atransformer%2520%2528MSBT%2529%2520based%2520fusion%2520module%2520that%2520employs%2520a%2520reduced%2520number%2520of%250Abottleneck%2520tokens%2520to%2520gradually%2520condense%2520information%2520and%2520fuse%2520each%2520pair%2520of%250Amodalities%2520and%2520utilizes%2520a%2520bottleneck%2520token-based%2520weighting%2520scheme%2520to%2520highlight%250Amore%2520important%2520fused%2520features.%2520Furthermore%252C%2520we%2520propose%2520a%2520temporal%2520consistency%250Acontrast%2520loss%2520to%2520semantically%2520align%2520pairwise%2520fused%2520features.%2520Experiments%2520on%2520the%250Alargest-scale%2520XD-Violence%2520dataset%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%250Astate-of-the-art%2520performance.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/shengyangsun/MSBT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20Bottleneck%20Transformer%20for%20Weakly%20Supervised%20Multimodal%0A%20%20Violence%20Detection&entry.906535625=Shengyang%20Sun%20and%20Xiaojin%20Gong&entry.1292438233=%20%20Weakly%20supervised%20multimodal%20violence%20detection%20aims%20to%20learn%20a%20violence%0Adetection%20model%20by%20leveraging%20multiple%20modalities%20such%20as%20RGB%2C%20optical%20flow%2C%0Aand%20audio%2C%20while%20only%20video-level%20annotations%20are%20available.%20In%20the%20pursuit%20of%0Aeffective%20multimodal%20violence%20detection%20%28MVD%29%2C%20information%20redundancy%2C%20modality%0Aimbalance%2C%20and%20modality%20asynchrony%20are%20identified%20as%20three%20key%20challenges.%20In%0Athis%20work%2C%20we%20propose%20a%20new%20weakly%20supervised%20MVD%20method%20that%20explicitly%0Aaddresses%20these%20challenges.%20Specifically%2C%20we%20introduce%20a%20multi-scale%20bottleneck%0Atransformer%20%28MSBT%29%20based%20fusion%20module%20that%20employs%20a%20reduced%20number%20of%0Abottleneck%20tokens%20to%20gradually%20condense%20information%20and%20fuse%20each%20pair%20of%0Amodalities%20and%20utilizes%20a%20bottleneck%20token-based%20weighting%20scheme%20to%20highlight%0Amore%20important%20fused%20features.%20Furthermore%2C%20we%20propose%20a%20temporal%20consistency%0Acontrast%20loss%20to%20semantically%20align%20pairwise%20fused%20features.%20Experiments%20on%20the%0Alargest-scale%20XD-Violence%20dataset%20demonstrate%20that%20the%20proposed%20method%20achieves%0Astate-of-the-art%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/shengyangsun/MSBT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05130v1&entry.124074799=Read"},
{"title": "MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images", "author": "Junwen Huang and Hao Yu and Kuan-Ting Yu and Nassir Navab and Slobodan Ilic and Benjamin Busam", "abstract": "  Recent learning methods for object pose estimation require resource-intensive\ntraining for each individual object instance or category, hampering their\nscalability in real applications when confronted with previously unseen\nobjects. In this paper, we propose MatchU, a Fuse-Describe-Match strategy for\n6D pose estimation from RGB-D images. MatchU is a generic approach that fuses\n2D texture and 3D geometric cues for 6D pose prediction of unseen objects. We\nrely on learning geometric 3D descriptors that are rotation-invariant by\ndesign. By encoding pose-agnostic geometry, the learned descriptors naturally\ngeneralize to unseen objects and capture symmetries. To tackle ambiguous\nassociations using 3D geometry only, we fuse additional RGB information into\nour descriptor. This is achieved through a novel attention-based mechanism that\nfuses cross-modal information, together with a matching loss that leverages the\nlatent space learned from RGB data to guide the descriptor learning process.\nExtensive experiments reveal the generalizability of both the RGB-D fusion\nstrategy as well as the descriptor efficacy. Benefiting from the novel designs,\nMatchU surpasses all existing methods by a significant margin in terms of both\naccuracy and speed, even without the requirement of expensive re-training or\nrendering.\n", "link": "http://arxiv.org/abs/2403.01517v2", "date": "2024-05-08", "relevancy": 2.3039, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6046}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5712}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatchU%3A%20Matching%20Unseen%20Objects%20for%206D%20Pose%20Estimation%20from%20RGB-D%20Images&body=Title%3A%20MatchU%3A%20Matching%20Unseen%20Objects%20for%206D%20Pose%20Estimation%20from%20RGB-D%20Images%0AAuthor%3A%20Junwen%20Huang%20and%20Hao%20Yu%20and%20Kuan-Ting%20Yu%20and%20Nassir%20Navab%20and%20Slobodan%20Ilic%20and%20Benjamin%20Busam%0AAbstract%3A%20%20%20Recent%20learning%20methods%20for%20object%20pose%20estimation%20require%20resource-intensive%0Atraining%20for%20each%20individual%20object%20instance%20or%20category%2C%20hampering%20their%0Ascalability%20in%20real%20applications%20when%20confronted%20with%20previously%20unseen%0Aobjects.%20In%20this%20paper%2C%20we%20propose%20MatchU%2C%20a%20Fuse-Describe-Match%20strategy%20for%0A6D%20pose%20estimation%20from%20RGB-D%20images.%20MatchU%20is%20a%20generic%20approach%20that%20fuses%0A2D%20texture%20and%203D%20geometric%20cues%20for%206D%20pose%20prediction%20of%20unseen%20objects.%20We%0Arely%20on%20learning%20geometric%203D%20descriptors%20that%20are%20rotation-invariant%20by%0Adesign.%20By%20encoding%20pose-agnostic%20geometry%2C%20the%20learned%20descriptors%20naturally%0Ageneralize%20to%20unseen%20objects%20and%20capture%20symmetries.%20To%20tackle%20ambiguous%0Aassociations%20using%203D%20geometry%20only%2C%20we%20fuse%20additional%20RGB%20information%20into%0Aour%20descriptor.%20This%20is%20achieved%20through%20a%20novel%20attention-based%20mechanism%20that%0Afuses%20cross-modal%20information%2C%20together%20with%20a%20matching%20loss%20that%20leverages%20the%0Alatent%20space%20learned%20from%20RGB%20data%20to%20guide%20the%20descriptor%20learning%20process.%0AExtensive%20experiments%20reveal%20the%20generalizability%20of%20both%20the%20RGB-D%20fusion%0Astrategy%20as%20well%20as%20the%20descriptor%20efficacy.%20Benefiting%20from%20the%20novel%20designs%2C%0AMatchU%20surpasses%20all%20existing%20methods%20by%20a%20significant%20margin%20in%20terms%20of%20both%0Aaccuracy%20and%20speed%2C%20even%20without%20the%20requirement%20of%20expensive%20re-training%20or%0Arendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01517v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatchU%253A%2520Matching%2520Unseen%2520Objects%2520for%25206D%2520Pose%2520Estimation%2520from%2520RGB-D%2520Images%26entry.906535625%3DJunwen%2520Huang%2520and%2520Hao%2520Yu%2520and%2520Kuan-Ting%2520Yu%2520and%2520Nassir%2520Navab%2520and%2520Slobodan%2520Ilic%2520and%2520Benjamin%2520Busam%26entry.1292438233%3D%2520%2520Recent%2520learning%2520methods%2520for%2520object%2520pose%2520estimation%2520require%2520resource-intensive%250Atraining%2520for%2520each%2520individual%2520object%2520instance%2520or%2520category%252C%2520hampering%2520their%250Ascalability%2520in%2520real%2520applications%2520when%2520confronted%2520with%2520previously%2520unseen%250Aobjects.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MatchU%252C%2520a%2520Fuse-Describe-Match%2520strategy%2520for%250A6D%2520pose%2520estimation%2520from%2520RGB-D%2520images.%2520MatchU%2520is%2520a%2520generic%2520approach%2520that%2520fuses%250A2D%2520texture%2520and%25203D%2520geometric%2520cues%2520for%25206D%2520pose%2520prediction%2520of%2520unseen%2520objects.%2520We%250Arely%2520on%2520learning%2520geometric%25203D%2520descriptors%2520that%2520are%2520rotation-invariant%2520by%250Adesign.%2520By%2520encoding%2520pose-agnostic%2520geometry%252C%2520the%2520learned%2520descriptors%2520naturally%250Ageneralize%2520to%2520unseen%2520objects%2520and%2520capture%2520symmetries.%2520To%2520tackle%2520ambiguous%250Aassociations%2520using%25203D%2520geometry%2520only%252C%2520we%2520fuse%2520additional%2520RGB%2520information%2520into%250Aour%2520descriptor.%2520This%2520is%2520achieved%2520through%2520a%2520novel%2520attention-based%2520mechanism%2520that%250Afuses%2520cross-modal%2520information%252C%2520together%2520with%2520a%2520matching%2520loss%2520that%2520leverages%2520the%250Alatent%2520space%2520learned%2520from%2520RGB%2520data%2520to%2520guide%2520the%2520descriptor%2520learning%2520process.%250AExtensive%2520experiments%2520reveal%2520the%2520generalizability%2520of%2520both%2520the%2520RGB-D%2520fusion%250Astrategy%2520as%2520well%2520as%2520the%2520descriptor%2520efficacy.%2520Benefiting%2520from%2520the%2520novel%2520designs%252C%250AMatchU%2520surpasses%2520all%2520existing%2520methods%2520by%2520a%2520significant%2520margin%2520in%2520terms%2520of%2520both%250Aaccuracy%2520and%2520speed%252C%2520even%2520without%2520the%2520requirement%2520of%2520expensive%2520re-training%2520or%250Arendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01517v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatchU%3A%20Matching%20Unseen%20Objects%20for%206D%20Pose%20Estimation%20from%20RGB-D%20Images&entry.906535625=Junwen%20Huang%20and%20Hao%20Yu%20and%20Kuan-Ting%20Yu%20and%20Nassir%20Navab%20and%20Slobodan%20Ilic%20and%20Benjamin%20Busam&entry.1292438233=%20%20Recent%20learning%20methods%20for%20object%20pose%20estimation%20require%20resource-intensive%0Atraining%20for%20each%20individual%20object%20instance%20or%20category%2C%20hampering%20their%0Ascalability%20in%20real%20applications%20when%20confronted%20with%20previously%20unseen%0Aobjects.%20In%20this%20paper%2C%20we%20propose%20MatchU%2C%20a%20Fuse-Describe-Match%20strategy%20for%0A6D%20pose%20estimation%20from%20RGB-D%20images.%20MatchU%20is%20a%20generic%20approach%20that%20fuses%0A2D%20texture%20and%203D%20geometric%20cues%20for%206D%20pose%20prediction%20of%20unseen%20objects.%20We%0Arely%20on%20learning%20geometric%203D%20descriptors%20that%20are%20rotation-invariant%20by%0Adesign.%20By%20encoding%20pose-agnostic%20geometry%2C%20the%20learned%20descriptors%20naturally%0Ageneralize%20to%20unseen%20objects%20and%20capture%20symmetries.%20To%20tackle%20ambiguous%0Aassociations%20using%203D%20geometry%20only%2C%20we%20fuse%20additional%20RGB%20information%20into%0Aour%20descriptor.%20This%20is%20achieved%20through%20a%20novel%20attention-based%20mechanism%20that%0Afuses%20cross-modal%20information%2C%20together%20with%20a%20matching%20loss%20that%20leverages%20the%0Alatent%20space%20learned%20from%20RGB%20data%20to%20guide%20the%20descriptor%20learning%20process.%0AExtensive%20experiments%20reveal%20the%20generalizability%20of%20both%20the%20RGB-D%20fusion%0Astrategy%20as%20well%20as%20the%20descriptor%20efficacy.%20Benefiting%20from%20the%20novel%20designs%2C%0AMatchU%20surpasses%20all%20existing%20methods%20by%20a%20significant%20margin%20in%20terms%20of%20both%0Aaccuracy%20and%20speed%2C%20even%20without%20the%20requirement%20of%20expensive%20re-training%20or%0Arendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01517v2&entry.124074799=Read"},
{"title": "Persistent Homology for High-dimensional Data Based on Spectral Methods", "author": "Sebastian Damrich and Philipp Berens and Dmitry Kobak", "abstract": "  Persistent homology is a popular computational tool for analyzing the\ntopology of point clouds, such as the presence of loops or voids. However, many\nreal-world datasets with low intrinsic dimensionality reside in an ambient\nspace of much higher dimensionality. We show that in this case traditional\npersistent homology becomes very sensitive to noise and fails to detect the\ncorrect topology. The same holds true for existing refinements of persistent\nhomology. As a remedy, we find that spectral distances on the\n$k$-nearest-neighbor graph of the data, such as diffusion distance and\neffective resistance, allow to detect the correct topology even in the presence\nof high-dimensional noise. Moreover, we derive a novel closed-form formula for\neffective resistance, and describe its relation to diffusion distances.\nFinally, we apply these methods to high-dimensional single-cell RNA-sequencing\ndata and show that spectral distances allow robust detection of cell cycle\nloops.\n", "link": "http://arxiv.org/abs/2311.03087v2", "date": "2024-05-08", "relevancy": 2.2683, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4642}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4498}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Homology%20for%20High-dimensional%20Data%20Based%20on%20Spectral%20Methods&body=Title%3A%20Persistent%20Homology%20for%20High-dimensional%20Data%20Based%20on%20Spectral%20Methods%0AAuthor%3A%20Sebastian%20Damrich%20and%20Philipp%20Berens%20and%20Dmitry%20Kobak%0AAbstract%3A%20%20%20Persistent%20homology%20is%20a%20popular%20computational%20tool%20for%20analyzing%20the%0Atopology%20of%20point%20clouds%2C%20such%20as%20the%20presence%20of%20loops%20or%20voids.%20However%2C%20many%0Areal-world%20datasets%20with%20low%20intrinsic%20dimensionality%20reside%20in%20an%20ambient%0Aspace%20of%20much%20higher%20dimensionality.%20We%20show%20that%20in%20this%20case%20traditional%0Apersistent%20homology%20becomes%20very%20sensitive%20to%20noise%20and%20fails%20to%20detect%20the%0Acorrect%20topology.%20The%20same%20holds%20true%20for%20existing%20refinements%20of%20persistent%0Ahomology.%20As%20a%20remedy%2C%20we%20find%20that%20spectral%20distances%20on%20the%0A%24k%24-nearest-neighbor%20graph%20of%20the%20data%2C%20such%20as%20diffusion%20distance%20and%0Aeffective%20resistance%2C%20allow%20to%20detect%20the%20correct%20topology%20even%20in%20the%20presence%0Aof%20high-dimensional%20noise.%20Moreover%2C%20we%20derive%20a%20novel%20closed-form%20formula%20for%0Aeffective%20resistance%2C%20and%20describe%20its%20relation%20to%20diffusion%20distances.%0AFinally%2C%20we%20apply%20these%20methods%20to%20high-dimensional%20single-cell%20RNA-sequencing%0Adata%20and%20show%20that%20spectral%20distances%20allow%20robust%20detection%20of%20cell%20cycle%0Aloops.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.03087v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Homology%2520for%2520High-dimensional%2520Data%2520Based%2520on%2520Spectral%2520Methods%26entry.906535625%3DSebastian%2520Damrich%2520and%2520Philipp%2520Berens%2520and%2520Dmitry%2520Kobak%26entry.1292438233%3D%2520%2520Persistent%2520homology%2520is%2520a%2520popular%2520computational%2520tool%2520for%2520analyzing%2520the%250Atopology%2520of%2520point%2520clouds%252C%2520such%2520as%2520the%2520presence%2520of%2520loops%2520or%2520voids.%2520However%252C%2520many%250Areal-world%2520datasets%2520with%2520low%2520intrinsic%2520dimensionality%2520reside%2520in%2520an%2520ambient%250Aspace%2520of%2520much%2520higher%2520dimensionality.%2520We%2520show%2520that%2520in%2520this%2520case%2520traditional%250Apersistent%2520homology%2520becomes%2520very%2520sensitive%2520to%2520noise%2520and%2520fails%2520to%2520detect%2520the%250Acorrect%2520topology.%2520The%2520same%2520holds%2520true%2520for%2520existing%2520refinements%2520of%2520persistent%250Ahomology.%2520As%2520a%2520remedy%252C%2520we%2520find%2520that%2520spectral%2520distances%2520on%2520the%250A%2524k%2524-nearest-neighbor%2520graph%2520of%2520the%2520data%252C%2520such%2520as%2520diffusion%2520distance%2520and%250Aeffective%2520resistance%252C%2520allow%2520to%2520detect%2520the%2520correct%2520topology%2520even%2520in%2520the%2520presence%250Aof%2520high-dimensional%2520noise.%2520Moreover%252C%2520we%2520derive%2520a%2520novel%2520closed-form%2520formula%2520for%250Aeffective%2520resistance%252C%2520and%2520describe%2520its%2520relation%2520to%2520diffusion%2520distances.%250AFinally%252C%2520we%2520apply%2520these%2520methods%2520to%2520high-dimensional%2520single-cell%2520RNA-sequencing%250Adata%2520and%2520show%2520that%2520spectral%2520distances%2520allow%2520robust%2520detection%2520of%2520cell%2520cycle%250Aloops.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.03087v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Homology%20for%20High-dimensional%20Data%20Based%20on%20Spectral%20Methods&entry.906535625=Sebastian%20Damrich%20and%20Philipp%20Berens%20and%20Dmitry%20Kobak&entry.1292438233=%20%20Persistent%20homology%20is%20a%20popular%20computational%20tool%20for%20analyzing%20the%0Atopology%20of%20point%20clouds%2C%20such%20as%20the%20presence%20of%20loops%20or%20voids.%20However%2C%20many%0Areal-world%20datasets%20with%20low%20intrinsic%20dimensionality%20reside%20in%20an%20ambient%0Aspace%20of%20much%20higher%20dimensionality.%20We%20show%20that%20in%20this%20case%20traditional%0Apersistent%20homology%20becomes%20very%20sensitive%20to%20noise%20and%20fails%20to%20detect%20the%0Acorrect%20topology.%20The%20same%20holds%20true%20for%20existing%20refinements%20of%20persistent%0Ahomology.%20As%20a%20remedy%2C%20we%20find%20that%20spectral%20distances%20on%20the%0A%24k%24-nearest-neighbor%20graph%20of%20the%20data%2C%20such%20as%20diffusion%20distance%20and%0Aeffective%20resistance%2C%20allow%20to%20detect%20the%20correct%20topology%20even%20in%20the%20presence%0Aof%20high-dimensional%20noise.%20Moreover%2C%20we%20derive%20a%20novel%20closed-form%20formula%20for%0Aeffective%20resistance%2C%20and%20describe%20its%20relation%20to%20diffusion%20distances.%0AFinally%2C%20we%20apply%20these%20methods%20to%20high-dimensional%20single-cell%20RNA-sequencing%0Adata%20and%20show%20that%20spectral%20distances%20allow%20robust%20detection%20of%20cell%20cycle%0Aloops.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.03087v2&entry.124074799=Read"},
{"title": "Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID", "author": "Wentao Tan and Changxing Ding and Jiayu Jiang and Fei Wang and Yibing Zhan and Dapeng Tao", "abstract": "  Text-to-image person re-identification (ReID) retrieves pedestrian images\naccording to textual descriptions. Manually annotating textual descriptions is\ntime-consuming, restricting the scale of existing datasets and therefore the\ngeneralization ability of ReID models. As a result, we study the transferable\ntext-to-image ReID problem, where we train a model on our proposed large-scale\ndatabase and directly deploy it to various datasets for evaluation. We obtain\nsubstantial training data via Multi-modal Large Language Models (MLLMs).\nMoreover, we identify and address two key challenges in utilizing the obtained\ntextual descriptions. First, an MLLM tends to generate descriptions with\nsimilar structures, causing the model to overfit specific sentence patterns.\nThus, we propose a novel method that uses MLLMs to caption images according to\nvarious templates. These templates are obtained using a multi-turn dialogue\nwith a Large Language Model (LLM). Therefore, we can build a large-scale\ndataset with diverse textual descriptions. Second, an MLLM may produce\nincorrect descriptions. Hence, we introduce a novel method that automatically\nidentifies words in a description that do not correspond with the image. This\nmethod is based on the similarity between one text and all patch token\nembeddings in the image. Then, we mask these words with a larger probability in\nthe subsequent training epoch, alleviating the impact of noisy textual\ndescriptions. The experimental results demonstrate that our methods\nsignificantly boost the direct transfer text-to-image ReID performance.\nBenefiting from the pre-trained model weights, we also achieve state-of-the-art\nperformance in the traditional evaluation settings.\n", "link": "http://arxiv.org/abs/2405.04940v1", "date": "2024-05-08", "relevancy": 2.251, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.557}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20the%20Power%20of%20MLLMs%20for%20Transferable%20Text-to-Image%20Person%20ReID&body=Title%3A%20Harnessing%20the%20Power%20of%20MLLMs%20for%20Transferable%20Text-to-Image%20Person%20ReID%0AAuthor%3A%20Wentao%20Tan%20and%20Changxing%20Ding%20and%20Jiayu%20Jiang%20and%20Fei%20Wang%20and%20Yibing%20Zhan%20and%20Dapeng%20Tao%0AAbstract%3A%20%20%20Text-to-image%20person%20re-identification%20%28ReID%29%20retrieves%20pedestrian%20images%0Aaccording%20to%20textual%20descriptions.%20Manually%20annotating%20textual%20descriptions%20is%0Atime-consuming%2C%20restricting%20the%20scale%20of%20existing%20datasets%20and%20therefore%20the%0Ageneralization%20ability%20of%20ReID%20models.%20As%20a%20result%2C%20we%20study%20the%20transferable%0Atext-to-image%20ReID%20problem%2C%20where%20we%20train%20a%20model%20on%20our%20proposed%20large-scale%0Adatabase%20and%20directly%20deploy%20it%20to%20various%20datasets%20for%20evaluation.%20We%20obtain%0Asubstantial%20training%20data%20via%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29.%0AMoreover%2C%20we%20identify%20and%20address%20two%20key%20challenges%20in%20utilizing%20the%20obtained%0Atextual%20descriptions.%20First%2C%20an%20MLLM%20tends%20to%20generate%20descriptions%20with%0Asimilar%20structures%2C%20causing%20the%20model%20to%20overfit%20specific%20sentence%20patterns.%0AThus%2C%20we%20propose%20a%20novel%20method%20that%20uses%20MLLMs%20to%20caption%20images%20according%20to%0Avarious%20templates.%20These%20templates%20are%20obtained%20using%20a%20multi-turn%20dialogue%0Awith%20a%20Large%20Language%20Model%20%28LLM%29.%20Therefore%2C%20we%20can%20build%20a%20large-scale%0Adataset%20with%20diverse%20textual%20descriptions.%20Second%2C%20an%20MLLM%20may%20produce%0Aincorrect%20descriptions.%20Hence%2C%20we%20introduce%20a%20novel%20method%20that%20automatically%0Aidentifies%20words%20in%20a%20description%20that%20do%20not%20correspond%20with%20the%20image.%20This%0Amethod%20is%20based%20on%20the%20similarity%20between%20one%20text%20and%20all%20patch%20token%0Aembeddings%20in%20the%20image.%20Then%2C%20we%20mask%20these%20words%20with%20a%20larger%20probability%20in%0Athe%20subsequent%20training%20epoch%2C%20alleviating%20the%20impact%20of%20noisy%20textual%0Adescriptions.%20The%20experimental%20results%20demonstrate%20that%20our%20methods%0Asignificantly%20boost%20the%20direct%20transfer%20text-to-image%20ReID%20performance.%0ABenefiting%20from%20the%20pre-trained%20model%20weights%2C%20we%20also%20achieve%20state-of-the-art%0Aperformance%20in%20the%20traditional%20evaluation%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520the%2520Power%2520of%2520MLLMs%2520for%2520Transferable%2520Text-to-Image%2520Person%2520ReID%26entry.906535625%3DWentao%2520Tan%2520and%2520Changxing%2520Ding%2520and%2520Jiayu%2520Jiang%2520and%2520Fei%2520Wang%2520and%2520Yibing%2520Zhan%2520and%2520Dapeng%2520Tao%26entry.1292438233%3D%2520%2520Text-to-image%2520person%2520re-identification%2520%2528ReID%2529%2520retrieves%2520pedestrian%2520images%250Aaccording%2520to%2520textual%2520descriptions.%2520Manually%2520annotating%2520textual%2520descriptions%2520is%250Atime-consuming%252C%2520restricting%2520the%2520scale%2520of%2520existing%2520datasets%2520and%2520therefore%2520the%250Ageneralization%2520ability%2520of%2520ReID%2520models.%2520As%2520a%2520result%252C%2520we%2520study%2520the%2520transferable%250Atext-to-image%2520ReID%2520problem%252C%2520where%2520we%2520train%2520a%2520model%2520on%2520our%2520proposed%2520large-scale%250Adatabase%2520and%2520directly%2520deploy%2520it%2520to%2520various%2520datasets%2520for%2520evaluation.%2520We%2520obtain%250Asubstantial%2520training%2520data%2520via%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%250AMoreover%252C%2520we%2520identify%2520and%2520address%2520two%2520key%2520challenges%2520in%2520utilizing%2520the%2520obtained%250Atextual%2520descriptions.%2520First%252C%2520an%2520MLLM%2520tends%2520to%2520generate%2520descriptions%2520with%250Asimilar%2520structures%252C%2520causing%2520the%2520model%2520to%2520overfit%2520specific%2520sentence%2520patterns.%250AThus%252C%2520we%2520propose%2520a%2520novel%2520method%2520that%2520uses%2520MLLMs%2520to%2520caption%2520images%2520according%2520to%250Avarious%2520templates.%2520These%2520templates%2520are%2520obtained%2520using%2520a%2520multi-turn%2520dialogue%250Awith%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529.%2520Therefore%252C%2520we%2520can%2520build%2520a%2520large-scale%250Adataset%2520with%2520diverse%2520textual%2520descriptions.%2520Second%252C%2520an%2520MLLM%2520may%2520produce%250Aincorrect%2520descriptions.%2520Hence%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%2520automatically%250Aidentifies%2520words%2520in%2520a%2520description%2520that%2520do%2520not%2520correspond%2520with%2520the%2520image.%2520This%250Amethod%2520is%2520based%2520on%2520the%2520similarity%2520between%2520one%2520text%2520and%2520all%2520patch%2520token%250Aembeddings%2520in%2520the%2520image.%2520Then%252C%2520we%2520mask%2520these%2520words%2520with%2520a%2520larger%2520probability%2520in%250Athe%2520subsequent%2520training%2520epoch%252C%2520alleviating%2520the%2520impact%2520of%2520noisy%2520textual%250Adescriptions.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%2520methods%250Asignificantly%2520boost%2520the%2520direct%2520transfer%2520text-to-image%2520ReID%2520performance.%250ABenefiting%2520from%2520the%2520pre-trained%2520model%2520weights%252C%2520we%2520also%2520achieve%2520state-of-the-art%250Aperformance%2520in%2520the%2520traditional%2520evaluation%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20the%20Power%20of%20MLLMs%20for%20Transferable%20Text-to-Image%20Person%20ReID&entry.906535625=Wentao%20Tan%20and%20Changxing%20Ding%20and%20Jiayu%20Jiang%20and%20Fei%20Wang%20and%20Yibing%20Zhan%20and%20Dapeng%20Tao&entry.1292438233=%20%20Text-to-image%20person%20re-identification%20%28ReID%29%20retrieves%20pedestrian%20images%0Aaccording%20to%20textual%20descriptions.%20Manually%20annotating%20textual%20descriptions%20is%0Atime-consuming%2C%20restricting%20the%20scale%20of%20existing%20datasets%20and%20therefore%20the%0Ageneralization%20ability%20of%20ReID%20models.%20As%20a%20result%2C%20we%20study%20the%20transferable%0Atext-to-image%20ReID%20problem%2C%20where%20we%20train%20a%20model%20on%20our%20proposed%20large-scale%0Adatabase%20and%20directly%20deploy%20it%20to%20various%20datasets%20for%20evaluation.%20We%20obtain%0Asubstantial%20training%20data%20via%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29.%0AMoreover%2C%20we%20identify%20and%20address%20two%20key%20challenges%20in%20utilizing%20the%20obtained%0Atextual%20descriptions.%20First%2C%20an%20MLLM%20tends%20to%20generate%20descriptions%20with%0Asimilar%20structures%2C%20causing%20the%20model%20to%20overfit%20specific%20sentence%20patterns.%0AThus%2C%20we%20propose%20a%20novel%20method%20that%20uses%20MLLMs%20to%20caption%20images%20according%20to%0Avarious%20templates.%20These%20templates%20are%20obtained%20using%20a%20multi-turn%20dialogue%0Awith%20a%20Large%20Language%20Model%20%28LLM%29.%20Therefore%2C%20we%20can%20build%20a%20large-scale%0Adataset%20with%20diverse%20textual%20descriptions.%20Second%2C%20an%20MLLM%20may%20produce%0Aincorrect%20descriptions.%20Hence%2C%20we%20introduce%20a%20novel%20method%20that%20automatically%0Aidentifies%20words%20in%20a%20description%20that%20do%20not%20correspond%20with%20the%20image.%20This%0Amethod%20is%20based%20on%20the%20similarity%20between%20one%20text%20and%20all%20patch%20token%0Aembeddings%20in%20the%20image.%20Then%2C%20we%20mask%20these%20words%20with%20a%20larger%20probability%20in%0Athe%20subsequent%20training%20epoch%2C%20alleviating%20the%20impact%20of%20noisy%20textual%0Adescriptions.%20The%20experimental%20results%20demonstrate%20that%20our%20methods%0Asignificantly%20boost%20the%20direct%20transfer%20text-to-image%20ReID%20performance.%0ABenefiting%20from%20the%20pre-trained%20model%20weights%2C%20we%20also%20achieve%20state-of-the-art%0Aperformance%20in%20the%20traditional%20evaluation%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04940v1&entry.124074799=Read"},
{"title": "${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields", "author": "Ning Wang and Lefei Zhang and Angel X Chang", "abstract": "  Neural fields (NeRF) have emerged as a promising approach for representing\ncontinuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs\nposes a significant challenge for scene decomposition. To address this\nchallenge, we present a single model, Multi-Modal Decomposition NeRF\n(${M^2D}$NeRF), that is capable of both text-based and visual patch-based\nedits. Specifically, we use multi-modal feature distillation to integrate\nteacher features from pretrained visual and language models into 3D semantic\nfeature volumes, thereby facilitating consistent 3D editing. To enforce\nconsistency between the visual and language features in our 3D feature volumes,\nwe introduce a multi-modal similarity constraint. We also introduce a\npatch-based joint contrastive loss that helps to encourage object-regions to\ncoalesce in the 3D feature space, resulting in more precise boundaries.\nExperiments on various real-world scenes show superior performance in 3D scene\ndecomposition tasks compared to prior NeRF-based methods.\n", "link": "http://arxiv.org/abs/2405.05010v1", "date": "2024-05-08", "relevancy": 2.245, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%7BM%5E2D%7D%24NeRF%3A%20Multi-Modal%20Decomposition%20NeRF%20with%203D%20Feature%20Fields&body=Title%3A%20%24%7BM%5E2D%7D%24NeRF%3A%20Multi-Modal%20Decomposition%20NeRF%20with%203D%20Feature%20Fields%0AAuthor%3A%20Ning%20Wang%20and%20Lefei%20Zhang%20and%20Angel%20X%20Chang%0AAbstract%3A%20%20%20Neural%20fields%20%28NeRF%29%20have%20emerged%20as%20a%20promising%20approach%20for%20representing%0Acontinuous%203D%20scenes.%20Nevertheless%2C%20the%20lack%20of%20semantic%20encoding%20in%20NeRFs%0Aposes%20a%20significant%20challenge%20for%20scene%20decomposition.%20To%20address%20this%0Achallenge%2C%20we%20present%20a%20single%20model%2C%20Multi-Modal%20Decomposition%20NeRF%0A%28%24%7BM%5E2D%7D%24NeRF%29%2C%20that%20is%20capable%20of%20both%20text-based%20and%20visual%20patch-based%0Aedits.%20Specifically%2C%20we%20use%20multi-modal%20feature%20distillation%20to%20integrate%0Ateacher%20features%20from%20pretrained%20visual%20and%20language%20models%20into%203D%20semantic%0Afeature%20volumes%2C%20thereby%20facilitating%20consistent%203D%20editing.%20To%20enforce%0Aconsistency%20between%20the%20visual%20and%20language%20features%20in%20our%203D%20feature%20volumes%2C%0Awe%20introduce%20a%20multi-modal%20similarity%20constraint.%20We%20also%20introduce%20a%0Apatch-based%20joint%20contrastive%20loss%20that%20helps%20to%20encourage%20object-regions%20to%0Acoalesce%20in%20the%203D%20feature%20space%2C%20resulting%20in%20more%20precise%20boundaries.%0AExperiments%20on%20various%20real-world%20scenes%20show%20superior%20performance%20in%203D%20scene%0Adecomposition%20tasks%20compared%20to%20prior%20NeRF-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%257BM%255E2D%257D%2524NeRF%253A%2520Multi-Modal%2520Decomposition%2520NeRF%2520with%25203D%2520Feature%2520Fields%26entry.906535625%3DNing%2520Wang%2520and%2520Lefei%2520Zhang%2520and%2520Angel%2520X%2520Chang%26entry.1292438233%3D%2520%2520Neural%2520fields%2520%2528NeRF%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520representing%250Acontinuous%25203D%2520scenes.%2520Nevertheless%252C%2520the%2520lack%2520of%2520semantic%2520encoding%2520in%2520NeRFs%250Aposes%2520a%2520significant%2520challenge%2520for%2520scene%2520decomposition.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520present%2520a%2520single%2520model%252C%2520Multi-Modal%2520Decomposition%2520NeRF%250A%2528%2524%257BM%255E2D%257D%2524NeRF%2529%252C%2520that%2520is%2520capable%2520of%2520both%2520text-based%2520and%2520visual%2520patch-based%250Aedits.%2520Specifically%252C%2520we%2520use%2520multi-modal%2520feature%2520distillation%2520to%2520integrate%250Ateacher%2520features%2520from%2520pretrained%2520visual%2520and%2520language%2520models%2520into%25203D%2520semantic%250Afeature%2520volumes%252C%2520thereby%2520facilitating%2520consistent%25203D%2520editing.%2520To%2520enforce%250Aconsistency%2520between%2520the%2520visual%2520and%2520language%2520features%2520in%2520our%25203D%2520feature%2520volumes%252C%250Awe%2520introduce%2520a%2520multi-modal%2520similarity%2520constraint.%2520We%2520also%2520introduce%2520a%250Apatch-based%2520joint%2520contrastive%2520loss%2520that%2520helps%2520to%2520encourage%2520object-regions%2520to%250Acoalesce%2520in%2520the%25203D%2520feature%2520space%252C%2520resulting%2520in%2520more%2520precise%2520boundaries.%250AExperiments%2520on%2520various%2520real-world%2520scenes%2520show%2520superior%2520performance%2520in%25203D%2520scene%250Adecomposition%2520tasks%2520compared%2520to%2520prior%2520NeRF-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%7BM%5E2D%7D%24NeRF%3A%20Multi-Modal%20Decomposition%20NeRF%20with%203D%20Feature%20Fields&entry.906535625=Ning%20Wang%20and%20Lefei%20Zhang%20and%20Angel%20X%20Chang&entry.1292438233=%20%20Neural%20fields%20%28NeRF%29%20have%20emerged%20as%20a%20promising%20approach%20for%20representing%0Acontinuous%203D%20scenes.%20Nevertheless%2C%20the%20lack%20of%20semantic%20encoding%20in%20NeRFs%0Aposes%20a%20significant%20challenge%20for%20scene%20decomposition.%20To%20address%20this%0Achallenge%2C%20we%20present%20a%20single%20model%2C%20Multi-Modal%20Decomposition%20NeRF%0A%28%24%7BM%5E2D%7D%24NeRF%29%2C%20that%20is%20capable%20of%20both%20text-based%20and%20visual%20patch-based%0Aedits.%20Specifically%2C%20we%20use%20multi-modal%20feature%20distillation%20to%20integrate%0Ateacher%20features%20from%20pretrained%20visual%20and%20language%20models%20into%203D%20semantic%0Afeature%20volumes%2C%20thereby%20facilitating%20consistent%203D%20editing.%20To%20enforce%0Aconsistency%20between%20the%20visual%20and%20language%20features%20in%20our%203D%20feature%20volumes%2C%0Awe%20introduce%20a%20multi-modal%20similarity%20constraint.%20We%20also%20introduce%20a%0Apatch-based%20joint%20contrastive%20loss%20that%20helps%20to%20encourage%20object-regions%20to%0Acoalesce%20in%20the%203D%20feature%20space%2C%20resulting%20in%20more%20precise%20boundaries.%0AExperiments%20on%20various%20real-world%20scenes%20show%20superior%20performance%20in%203D%20scene%0Adecomposition%20tasks%20compared%20to%20prior%20NeRF-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05010v1&entry.124074799=Read"},
{"title": "A Survey on Occupancy Perception for Autonomous Driving: The Information\n  Fusion Perspective", "author": "Huaiyuan Xu and Junliang Chen and Shiyu Meng and Yi Wang and Lap-Pui Chau", "abstract": "  3D occupancy perception technology aims to observe and understand dense 3D\nenvironments for autonomous vehicles. Owing to its comprehensive perception\ncapability, this technology is emerging as a trend in autonomous driving\nperception systems, and is attracting significant attention from both industry\nand academia. Similar to traditional bird's-eye view (BEV) perception, 3D\noccupancy perception has the nature of multi-source input and the necessity for\ninformation fusion. However, the difference is that it captures vertical\nstructures that are ignored by 2D BEV. In this survey, we review the most\nrecent works on 3D occupancy perception, and provide in-depth analyses of\nmethodologies with various input modalities. Specifically, we summarize general\nnetwork pipelines, highlight information fusion techniques, and discuss\neffective network training. We evaluate and analyze the occupancy perception\nperformance of the state-of-the-art on the most popular datasets. Furthermore,\nchallenges and future research directions are discussed. We hope this report\nwill inspire the community and encourage more research work on 3D occupancy\nperception. A comprehensive list of studies in this survey is available in an\nactive repository that continuously collects the latest work:\nhttps://github.com/HuaiyuanXu/3D-Occupancy-Perception.\n", "link": "http://arxiv.org/abs/2405.05173v1", "date": "2024-05-08", "relevancy": 2.2254, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5957}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Occupancy%20Perception%20for%20Autonomous%20Driving%3A%20The%20Information%0A%20%20Fusion%20Perspective&body=Title%3A%20A%20Survey%20on%20Occupancy%20Perception%20for%20Autonomous%20Driving%3A%20The%20Information%0A%20%20Fusion%20Perspective%0AAuthor%3A%20Huaiyuan%20Xu%20and%20Junliang%20Chen%20and%20Shiyu%20Meng%20and%20Yi%20Wang%20and%20Lap-Pui%20Chau%0AAbstract%3A%20%20%203D%20occupancy%20perception%20technology%20aims%20to%20observe%20and%20understand%20dense%203D%0Aenvironments%20for%20autonomous%20vehicles.%20Owing%20to%20its%20comprehensive%20perception%0Acapability%2C%20this%20technology%20is%20emerging%20as%20a%20trend%20in%20autonomous%20driving%0Aperception%20systems%2C%20and%20is%20attracting%20significant%20attention%20from%20both%20industry%0Aand%20academia.%20Similar%20to%20traditional%20bird%27s-eye%20view%20%28BEV%29%20perception%2C%203D%0Aoccupancy%20perception%20has%20the%20nature%20of%20multi-source%20input%20and%20the%20necessity%20for%0Ainformation%20fusion.%20However%2C%20the%20difference%20is%20that%20it%20captures%20vertical%0Astructures%20that%20are%20ignored%20by%202D%20BEV.%20In%20this%20survey%2C%20we%20review%20the%20most%0Arecent%20works%20on%203D%20occupancy%20perception%2C%20and%20provide%20in-depth%20analyses%20of%0Amethodologies%20with%20various%20input%20modalities.%20Specifically%2C%20we%20summarize%20general%0Anetwork%20pipelines%2C%20highlight%20information%20fusion%20techniques%2C%20and%20discuss%0Aeffective%20network%20training.%20We%20evaluate%20and%20analyze%20the%20occupancy%20perception%0Aperformance%20of%20the%20state-of-the-art%20on%20the%20most%20popular%20datasets.%20Furthermore%2C%0Achallenges%20and%20future%20research%20directions%20are%20discussed.%20We%20hope%20this%20report%0Awill%20inspire%20the%20community%20and%20encourage%20more%20research%20work%20on%203D%20occupancy%0Aperception.%20A%20comprehensive%20list%20of%20studies%20in%20this%20survey%20is%20available%20in%20an%0Aactive%20repository%20that%20continuously%20collects%20the%20latest%20work%3A%0Ahttps%3A//github.com/HuaiyuanXu/3D-Occupancy-Perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Occupancy%2520Perception%2520for%2520Autonomous%2520Driving%253A%2520The%2520Information%250A%2520%2520Fusion%2520Perspective%26entry.906535625%3DHuaiyuan%2520Xu%2520and%2520Junliang%2520Chen%2520and%2520Shiyu%2520Meng%2520and%2520Yi%2520Wang%2520and%2520Lap-Pui%2520Chau%26entry.1292438233%3D%2520%25203D%2520occupancy%2520perception%2520technology%2520aims%2520to%2520observe%2520and%2520understand%2520dense%25203D%250Aenvironments%2520for%2520autonomous%2520vehicles.%2520Owing%2520to%2520its%2520comprehensive%2520perception%250Acapability%252C%2520this%2520technology%2520is%2520emerging%2520as%2520a%2520trend%2520in%2520autonomous%2520driving%250Aperception%2520systems%252C%2520and%2520is%2520attracting%2520significant%2520attention%2520from%2520both%2520industry%250Aand%2520academia.%2520Similar%2520to%2520traditional%2520bird%2527s-eye%2520view%2520%2528BEV%2529%2520perception%252C%25203D%250Aoccupancy%2520perception%2520has%2520the%2520nature%2520of%2520multi-source%2520input%2520and%2520the%2520necessity%2520for%250Ainformation%2520fusion.%2520However%252C%2520the%2520difference%2520is%2520that%2520it%2520captures%2520vertical%250Astructures%2520that%2520are%2520ignored%2520by%25202D%2520BEV.%2520In%2520this%2520survey%252C%2520we%2520review%2520the%2520most%250Arecent%2520works%2520on%25203D%2520occupancy%2520perception%252C%2520and%2520provide%2520in-depth%2520analyses%2520of%250Amethodologies%2520with%2520various%2520input%2520modalities.%2520Specifically%252C%2520we%2520summarize%2520general%250Anetwork%2520pipelines%252C%2520highlight%2520information%2520fusion%2520techniques%252C%2520and%2520discuss%250Aeffective%2520network%2520training.%2520We%2520evaluate%2520and%2520analyze%2520the%2520occupancy%2520perception%250Aperformance%2520of%2520the%2520state-of-the-art%2520on%2520the%2520most%2520popular%2520datasets.%2520Furthermore%252C%250Achallenges%2520and%2520future%2520research%2520directions%2520are%2520discussed.%2520We%2520hope%2520this%2520report%250Awill%2520inspire%2520the%2520community%2520and%2520encourage%2520more%2520research%2520work%2520on%25203D%2520occupancy%250Aperception.%2520A%2520comprehensive%2520list%2520of%2520studies%2520in%2520this%2520survey%2520is%2520available%2520in%2520an%250Aactive%2520repository%2520that%2520continuously%2520collects%2520the%2520latest%2520work%253A%250Ahttps%253A//github.com/HuaiyuanXu/3D-Occupancy-Perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Occupancy%20Perception%20for%20Autonomous%20Driving%3A%20The%20Information%0A%20%20Fusion%20Perspective&entry.906535625=Huaiyuan%20Xu%20and%20Junliang%20Chen%20and%20Shiyu%20Meng%20and%20Yi%20Wang%20and%20Lap-Pui%20Chau&entry.1292438233=%20%203D%20occupancy%20perception%20technology%20aims%20to%20observe%20and%20understand%20dense%203D%0Aenvironments%20for%20autonomous%20vehicles.%20Owing%20to%20its%20comprehensive%20perception%0Acapability%2C%20this%20technology%20is%20emerging%20as%20a%20trend%20in%20autonomous%20driving%0Aperception%20systems%2C%20and%20is%20attracting%20significant%20attention%20from%20both%20industry%0Aand%20academia.%20Similar%20to%20traditional%20bird%27s-eye%20view%20%28BEV%29%20perception%2C%203D%0Aoccupancy%20perception%20has%20the%20nature%20of%20multi-source%20input%20and%20the%20necessity%20for%0Ainformation%20fusion.%20However%2C%20the%20difference%20is%20that%20it%20captures%20vertical%0Astructures%20that%20are%20ignored%20by%202D%20BEV.%20In%20this%20survey%2C%20we%20review%20the%20most%0Arecent%20works%20on%203D%20occupancy%20perception%2C%20and%20provide%20in-depth%20analyses%20of%0Amethodologies%20with%20various%20input%20modalities.%20Specifically%2C%20we%20summarize%20general%0Anetwork%20pipelines%2C%20highlight%20information%20fusion%20techniques%2C%20and%20discuss%0Aeffective%20network%20training.%20We%20evaluate%20and%20analyze%20the%20occupancy%20perception%0Aperformance%20of%20the%20state-of-the-art%20on%20the%20most%20popular%20datasets.%20Furthermore%2C%0Achallenges%20and%20future%20research%20directions%20are%20discussed.%20We%20hope%20this%20report%0Awill%20inspire%20the%20community%20and%20encourage%20more%20research%20work%20on%203D%20occupancy%0Aperception.%20A%20comprehensive%20list%20of%20studies%20in%20this%20survey%20is%20available%20in%20an%0Aactive%20repository%20that%20continuously%20collects%20the%20latest%20work%3A%0Ahttps%3A//github.com/HuaiyuanXu/3D-Occupancy-Perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05173v1&entry.124074799=Read"},
{"title": "Test-Time Adaptation for Depth Completion", "author": "Hyoungseob Park and Anjali Gupta and Alex Wong", "abstract": "  It is common to observe performance degradation when transferring models\ntrained on some (source) datasets to target testing data due to a domain gap\nbetween them. Existing methods for bridging this gap, such as domain adaptation\n(DA), may require the source data on which the model was trained (often not\navailable), while others, i.e., source-free DA, require many passes through the\ntesting data. We propose an online test-time adaptation method for depth\ncompletion, the task of inferring a dense depth map from a single image and\nassociated sparse depth map, that closes the performance gap in a single pass.\nWe first present a study on how the domain shift in each data modality affects\nmodel performance. Based on our observations that the sparse depth modality\nexhibits a much smaller covariate shift than the image, we design an embedding\nmodule trained in the source domain that preserves a mapping from features\nencoding only sparse depth to those encoding image and sparse depth. During\ntest time, sparse depth features are projected using this map as a proxy for\nsource domain features and are used as guidance to train a set of auxiliary\nparameters (i.e., adaptation layer) to align image and sparse depth features\nfrom the target test domain to that of the source domain. We evaluate our\nmethod on indoor and outdoor scenarios and show that it improves over baselines\nby an average of 21.1%.\n", "link": "http://arxiv.org/abs/2402.03312v3", "date": "2024-05-08", "relevancy": 2.2211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5922}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Adaptation%20for%20Depth%20Completion&body=Title%3A%20Test-Time%20Adaptation%20for%20Depth%20Completion%0AAuthor%3A%20Hyoungseob%20Park%20and%20Anjali%20Gupta%20and%20Alex%20Wong%0AAbstract%3A%20%20%20It%20is%20common%20to%20observe%20performance%20degradation%20when%20transferring%20models%0Atrained%20on%20some%20%28source%29%20datasets%20to%20target%20testing%20data%20due%20to%20a%20domain%20gap%0Abetween%20them.%20Existing%20methods%20for%20bridging%20this%20gap%2C%20such%20as%20domain%20adaptation%0A%28DA%29%2C%20may%20require%20the%20source%20data%20on%20which%20the%20model%20was%20trained%20%28often%20not%0Aavailable%29%2C%20while%20others%2C%20i.e.%2C%20source-free%20DA%2C%20require%20many%20passes%20through%20the%0Atesting%20data.%20We%20propose%20an%20online%20test-time%20adaptation%20method%20for%20depth%0Acompletion%2C%20the%20task%20of%20inferring%20a%20dense%20depth%20map%20from%20a%20single%20image%20and%0Aassociated%20sparse%20depth%20map%2C%20that%20closes%20the%20performance%20gap%20in%20a%20single%20pass.%0AWe%20first%20present%20a%20study%20on%20how%20the%20domain%20shift%20in%20each%20data%20modality%20affects%0Amodel%20performance.%20Based%20on%20our%20observations%20that%20the%20sparse%20depth%20modality%0Aexhibits%20a%20much%20smaller%20covariate%20shift%20than%20the%20image%2C%20we%20design%20an%20embedding%0Amodule%20trained%20in%20the%20source%20domain%20that%20preserves%20a%20mapping%20from%20features%0Aencoding%20only%20sparse%20depth%20to%20those%20encoding%20image%20and%20sparse%20depth.%20During%0Atest%20time%2C%20sparse%20depth%20features%20are%20projected%20using%20this%20map%20as%20a%20proxy%20for%0Asource%20domain%20features%20and%20are%20used%20as%20guidance%20to%20train%20a%20set%20of%20auxiliary%0Aparameters%20%28i.e.%2C%20adaptation%20layer%29%20to%20align%20image%20and%20sparse%20depth%20features%0Afrom%20the%20target%20test%20domain%20to%20that%20of%20the%20source%20domain.%20We%20evaluate%20our%0Amethod%20on%20indoor%20and%20outdoor%20scenarios%20and%20show%20that%20it%20improves%20over%20baselines%0Aby%20an%20average%20of%2021.1%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03312v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Adaptation%2520for%2520Depth%2520Completion%26entry.906535625%3DHyoungseob%2520Park%2520and%2520Anjali%2520Gupta%2520and%2520Alex%2520Wong%26entry.1292438233%3D%2520%2520It%2520is%2520common%2520to%2520observe%2520performance%2520degradation%2520when%2520transferring%2520models%250Atrained%2520on%2520some%2520%2528source%2529%2520datasets%2520to%2520target%2520testing%2520data%2520due%2520to%2520a%2520domain%2520gap%250Abetween%2520them.%2520Existing%2520methods%2520for%2520bridging%2520this%2520gap%252C%2520such%2520as%2520domain%2520adaptation%250A%2528DA%2529%252C%2520may%2520require%2520the%2520source%2520data%2520on%2520which%2520the%2520model%2520was%2520trained%2520%2528often%2520not%250Aavailable%2529%252C%2520while%2520others%252C%2520i.e.%252C%2520source-free%2520DA%252C%2520require%2520many%2520passes%2520through%2520the%250Atesting%2520data.%2520We%2520propose%2520an%2520online%2520test-time%2520adaptation%2520method%2520for%2520depth%250Acompletion%252C%2520the%2520task%2520of%2520inferring%2520a%2520dense%2520depth%2520map%2520from%2520a%2520single%2520image%2520and%250Aassociated%2520sparse%2520depth%2520map%252C%2520that%2520closes%2520the%2520performance%2520gap%2520in%2520a%2520single%2520pass.%250AWe%2520first%2520present%2520a%2520study%2520on%2520how%2520the%2520domain%2520shift%2520in%2520each%2520data%2520modality%2520affects%250Amodel%2520performance.%2520Based%2520on%2520our%2520observations%2520that%2520the%2520sparse%2520depth%2520modality%250Aexhibits%2520a%2520much%2520smaller%2520covariate%2520shift%2520than%2520the%2520image%252C%2520we%2520design%2520an%2520embedding%250Amodule%2520trained%2520in%2520the%2520source%2520domain%2520that%2520preserves%2520a%2520mapping%2520from%2520features%250Aencoding%2520only%2520sparse%2520depth%2520to%2520those%2520encoding%2520image%2520and%2520sparse%2520depth.%2520During%250Atest%2520time%252C%2520sparse%2520depth%2520features%2520are%2520projected%2520using%2520this%2520map%2520as%2520a%2520proxy%2520for%250Asource%2520domain%2520features%2520and%2520are%2520used%2520as%2520guidance%2520to%2520train%2520a%2520set%2520of%2520auxiliary%250Aparameters%2520%2528i.e.%252C%2520adaptation%2520layer%2529%2520to%2520align%2520image%2520and%2520sparse%2520depth%2520features%250Afrom%2520the%2520target%2520test%2520domain%2520to%2520that%2520of%2520the%2520source%2520domain.%2520We%2520evaluate%2520our%250Amethod%2520on%2520indoor%2520and%2520outdoor%2520scenarios%2520and%2520show%2520that%2520it%2520improves%2520over%2520baselines%250Aby%2520an%2520average%2520of%252021.1%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03312v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Adaptation%20for%20Depth%20Completion&entry.906535625=Hyoungseob%20Park%20and%20Anjali%20Gupta%20and%20Alex%20Wong&entry.1292438233=%20%20It%20is%20common%20to%20observe%20performance%20degradation%20when%20transferring%20models%0Atrained%20on%20some%20%28source%29%20datasets%20to%20target%20testing%20data%20due%20to%20a%20domain%20gap%0Abetween%20them.%20Existing%20methods%20for%20bridging%20this%20gap%2C%20such%20as%20domain%20adaptation%0A%28DA%29%2C%20may%20require%20the%20source%20data%20on%20which%20the%20model%20was%20trained%20%28often%20not%0Aavailable%29%2C%20while%20others%2C%20i.e.%2C%20source-free%20DA%2C%20require%20many%20passes%20through%20the%0Atesting%20data.%20We%20propose%20an%20online%20test-time%20adaptation%20method%20for%20depth%0Acompletion%2C%20the%20task%20of%20inferring%20a%20dense%20depth%20map%20from%20a%20single%20image%20and%0Aassociated%20sparse%20depth%20map%2C%20that%20closes%20the%20performance%20gap%20in%20a%20single%20pass.%0AWe%20first%20present%20a%20study%20on%20how%20the%20domain%20shift%20in%20each%20data%20modality%20affects%0Amodel%20performance.%20Based%20on%20our%20observations%20that%20the%20sparse%20depth%20modality%0Aexhibits%20a%20much%20smaller%20covariate%20shift%20than%20the%20image%2C%20we%20design%20an%20embedding%0Amodule%20trained%20in%20the%20source%20domain%20that%20preserves%20a%20mapping%20from%20features%0Aencoding%20only%20sparse%20depth%20to%20those%20encoding%20image%20and%20sparse%20depth.%20During%0Atest%20time%2C%20sparse%20depth%20features%20are%20projected%20using%20this%20map%20as%20a%20proxy%20for%0Asource%20domain%20features%20and%20are%20used%20as%20guidance%20to%20train%20a%20set%20of%20auxiliary%0Aparameters%20%28i.e.%2C%20adaptation%20layer%29%20to%20align%20image%20and%20sparse%20depth%20features%0Afrom%20the%20target%20test%20domain%20to%20that%20of%20the%20source%20domain.%20We%20evaluate%20our%0Amethod%20on%20indoor%20and%20outdoor%20scenarios%20and%20show%20that%20it%20improves%20over%20baselines%0Aby%20an%20average%20of%2021.1%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03312v3&entry.124074799=Read"},
{"title": "Cooperative Students: Navigating Unsupervised Domain Adaptation in\n  Nighttime Object Detection", "author": "Jicheng Yuan and Anh Le-Tuan and Manfred Hauswirth and Danh Le-Phuoc", "abstract": "  Unsupervised Domain Adaptation (UDA) has shown significant advancements in\nobject detection under well-lit conditions; however, its performance degrades\nnotably in low-visibility scenarios, especially at night, posing challenges not\nonly for its adaptability in low signal-to-noise ratio (SNR) conditions but\nalso for the reliability and efficiency of automated vehicles. To address this\nproblem, we propose a \\textbf{Co}operative \\textbf{S}tudents (\\textbf{CoS})\nframework that innovatively employs global-local transformations (GLT) and a\nproxy-based target consistency (PTC) mechanism to capture the spatial\nconsistency in day- and night-time scenarios effectively, and thus bridge the\nsignificant domain shift across contexts. Building upon this, we further devise\nan adaptive IoU-informed thresholding (AIT) module to gradually avoid\noverlooking potential true positives and enrich the latent information in the\ntarget domain. Comprehensive experiments show that CoS essentially enhanced UDA\nperformance in low-visibility conditions and surpasses current state-of-the-art\ntechniques, achieving an increase in mAP of 3.0\\%, 1.9\\%, and 2.5\\% on BDD100K,\nSHIFT, and ACDC datasets, respectively. Code is available at\nhttps://github.com/jichengyuan/Cooperitive_Students.\n", "link": "http://arxiv.org/abs/2404.01988v3", "date": "2024-05-08", "relevancy": 2.2127, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5607}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5495}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Students%3A%20Navigating%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Nighttime%20Object%20Detection&body=Title%3A%20Cooperative%20Students%3A%20Navigating%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Nighttime%20Object%20Detection%0AAuthor%3A%20Jicheng%20Yuan%20and%20Anh%20Le-Tuan%20and%20Manfred%20Hauswirth%20and%20Danh%20Le-Phuoc%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20has%20shown%20significant%20advancements%20in%0Aobject%20detection%20under%20well-lit%20conditions%3B%20however%2C%20its%20performance%20degrades%0Anotably%20in%20low-visibility%20scenarios%2C%20especially%20at%20night%2C%20posing%20challenges%20not%0Aonly%20for%20its%20adaptability%20in%20low%20signal-to-noise%20ratio%20%28SNR%29%20conditions%20but%0Aalso%20for%20the%20reliability%20and%20efficiency%20of%20automated%20vehicles.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20%5Ctextbf%7BCo%7Doperative%20%5Ctextbf%7BS%7Dtudents%20%28%5Ctextbf%7BCoS%7D%29%0Aframework%20that%20innovatively%20employs%20global-local%20transformations%20%28GLT%29%20and%20a%0Aproxy-based%20target%20consistency%20%28PTC%29%20mechanism%20to%20capture%20the%20spatial%0Aconsistency%20in%20day-%20and%20night-time%20scenarios%20effectively%2C%20and%20thus%20bridge%20the%0Asignificant%20domain%20shift%20across%20contexts.%20Building%20upon%20this%2C%20we%20further%20devise%0Aan%20adaptive%20IoU-informed%20thresholding%20%28AIT%29%20module%20to%20gradually%20avoid%0Aoverlooking%20potential%20true%20positives%20and%20enrich%20the%20latent%20information%20in%20the%0Atarget%20domain.%20Comprehensive%20experiments%20show%20that%20CoS%20essentially%20enhanced%20UDA%0Aperformance%20in%20low-visibility%20conditions%20and%20surpasses%20current%20state-of-the-art%0Atechniques%2C%20achieving%20an%20increase%20in%20mAP%20of%203.0%5C%25%2C%201.9%5C%25%2C%20and%202.5%5C%25%20on%20BDD100K%2C%0ASHIFT%2C%20and%20ACDC%20datasets%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jichengyuan/Cooperitive_Students.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01988v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Students%253A%2520Navigating%2520Unsupervised%2520Domain%2520Adaptation%2520in%250A%2520%2520Nighttime%2520Object%2520Detection%26entry.906535625%3DJicheng%2520Yuan%2520and%2520Anh%2520Le-Tuan%2520and%2520Manfred%2520Hauswirth%2520and%2520Danh%2520Le-Phuoc%26entry.1292438233%3D%2520%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520has%2520shown%2520significant%2520advancements%2520in%250Aobject%2520detection%2520under%2520well-lit%2520conditions%253B%2520however%252C%2520its%2520performance%2520degrades%250Anotably%2520in%2520low-visibility%2520scenarios%252C%2520especially%2520at%2520night%252C%2520posing%2520challenges%2520not%250Aonly%2520for%2520its%2520adaptability%2520in%2520low%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2520conditions%2520but%250Aalso%2520for%2520the%2520reliability%2520and%2520efficiency%2520of%2520automated%2520vehicles.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520%255Ctextbf%257BCo%257Doperative%2520%255Ctextbf%257BS%257Dtudents%2520%2528%255Ctextbf%257BCoS%257D%2529%250Aframework%2520that%2520innovatively%2520employs%2520global-local%2520transformations%2520%2528GLT%2529%2520and%2520a%250Aproxy-based%2520target%2520consistency%2520%2528PTC%2529%2520mechanism%2520to%2520capture%2520the%2520spatial%250Aconsistency%2520in%2520day-%2520and%2520night-time%2520scenarios%2520effectively%252C%2520and%2520thus%2520bridge%2520the%250Asignificant%2520domain%2520shift%2520across%2520contexts.%2520Building%2520upon%2520this%252C%2520we%2520further%2520devise%250Aan%2520adaptive%2520IoU-informed%2520thresholding%2520%2528AIT%2529%2520module%2520to%2520gradually%2520avoid%250Aoverlooking%2520potential%2520true%2520positives%2520and%2520enrich%2520the%2520latent%2520information%2520in%2520the%250Atarget%2520domain.%2520Comprehensive%2520experiments%2520show%2520that%2520CoS%2520essentially%2520enhanced%2520UDA%250Aperformance%2520in%2520low-visibility%2520conditions%2520and%2520surpasses%2520current%2520state-of-the-art%250Atechniques%252C%2520achieving%2520an%2520increase%2520in%2520mAP%2520of%25203.0%255C%2525%252C%25201.9%255C%2525%252C%2520and%25202.5%255C%2525%2520on%2520BDD100K%252C%250ASHIFT%252C%2520and%2520ACDC%2520datasets%252C%2520respectively.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/jichengyuan/Cooperitive_Students.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01988v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Students%3A%20Navigating%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Nighttime%20Object%20Detection&entry.906535625=Jicheng%20Yuan%20and%20Anh%20Le-Tuan%20and%20Manfred%20Hauswirth%20and%20Danh%20Le-Phuoc&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20has%20shown%20significant%20advancements%20in%0Aobject%20detection%20under%20well-lit%20conditions%3B%20however%2C%20its%20performance%20degrades%0Anotably%20in%20low-visibility%20scenarios%2C%20especially%20at%20night%2C%20posing%20challenges%20not%0Aonly%20for%20its%20adaptability%20in%20low%20signal-to-noise%20ratio%20%28SNR%29%20conditions%20but%0Aalso%20for%20the%20reliability%20and%20efficiency%20of%20automated%20vehicles.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20%5Ctextbf%7BCo%7Doperative%20%5Ctextbf%7BS%7Dtudents%20%28%5Ctextbf%7BCoS%7D%29%0Aframework%20that%20innovatively%20employs%20global-local%20transformations%20%28GLT%29%20and%20a%0Aproxy-based%20target%20consistency%20%28PTC%29%20mechanism%20to%20capture%20the%20spatial%0Aconsistency%20in%20day-%20and%20night-time%20scenarios%20effectively%2C%20and%20thus%20bridge%20the%0Asignificant%20domain%20shift%20across%20contexts.%20Building%20upon%20this%2C%20we%20further%20devise%0Aan%20adaptive%20IoU-informed%20thresholding%20%28AIT%29%20module%20to%20gradually%20avoid%0Aoverlooking%20potential%20true%20positives%20and%20enrich%20the%20latent%20information%20in%20the%0Atarget%20domain.%20Comprehensive%20experiments%20show%20that%20CoS%20essentially%20enhanced%20UDA%0Aperformance%20in%20low-visibility%20conditions%20and%20surpasses%20current%20state-of-the-art%0Atechniques%2C%20achieving%20an%20increase%20in%20mAP%20of%203.0%5C%25%2C%201.9%5C%25%2C%20and%202.5%5C%25%20on%20BDD100K%2C%0ASHIFT%2C%20and%20ACDC%20datasets%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jichengyuan/Cooperitive_Students.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01988v3&entry.124074799=Read"},
{"title": "Selective Classification Under Distribution Shifts", "author": "Hengyue Liang and Le Peng and Ju Sun", "abstract": "  In selective classification (SC), a classifier abstains from making\npredictions that are likely to be wrong to avoid excessive errors. To deploy\nimperfect classifiers -- imperfect either due to intrinsic statistical noise of\ndata or for robustness issue of the classifier or beyond -- in high-stakes\nscenarios, SC appears to be an attractive and necessary path to follow. Despite\ndecades of research in SC, most previous SC methods still focus on the ideal\nstatistical setting only, i.e., the data distribution at deployment is the same\nas that of training, although practical data can come from the wild. To bridge\nthis gap, in this paper, we propose an SC framework that takes into account\ndistribution shifts, termed generalized selective classification, that covers\nlabel-shifted (or out-of-distribution) and covariate-shifted samples, in\naddition to typical in-distribution samples, the first of its kind in the SC\nliterature. We focus on non-training-based confidence-score functions for\ngeneralized SC on deep learning (DL) classifiers and propose two novel\nmargin-based score functions. Through extensive analysis and experiments, we\nshow that our proposed score functions are more effective and reliable than the\nexisting ones for generalized SC on a variety of classification tasks and DL\nclassifiers.\n", "link": "http://arxiv.org/abs/2405.05160v1", "date": "2024-05-08", "relevancy": 2.2119, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4481}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4405}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Classification%20Under%20Distribution%20Shifts&body=Title%3A%20Selective%20Classification%20Under%20Distribution%20Shifts%0AAuthor%3A%20Hengyue%20Liang%20and%20Le%20Peng%20and%20Ju%20Sun%0AAbstract%3A%20%20%20In%20selective%20classification%20%28SC%29%2C%20a%20classifier%20abstains%20from%20making%0Apredictions%20that%20are%20likely%20to%20be%20wrong%20to%20avoid%20excessive%20errors.%20To%20deploy%0Aimperfect%20classifiers%20--%20imperfect%20either%20due%20to%20intrinsic%20statistical%20noise%20of%0Adata%20or%20for%20robustness%20issue%20of%20the%20classifier%20or%20beyond%20--%20in%20high-stakes%0Ascenarios%2C%20SC%20appears%20to%20be%20an%20attractive%20and%20necessary%20path%20to%20follow.%20Despite%0Adecades%20of%20research%20in%20SC%2C%20most%20previous%20SC%20methods%20still%20focus%20on%20the%20ideal%0Astatistical%20setting%20only%2C%20i.e.%2C%20the%20data%20distribution%20at%20deployment%20is%20the%20same%0Aas%20that%20of%20training%2C%20although%20practical%20data%20can%20come%20from%20the%20wild.%20To%20bridge%0Athis%20gap%2C%20in%20this%20paper%2C%20we%20propose%20an%20SC%20framework%20that%20takes%20into%20account%0Adistribution%20shifts%2C%20termed%20generalized%20selective%20classification%2C%20that%20covers%0Alabel-shifted%20%28or%20out-of-distribution%29%20and%20covariate-shifted%20samples%2C%20in%0Aaddition%20to%20typical%20in-distribution%20samples%2C%20the%20first%20of%20its%20kind%20in%20the%20SC%0Aliterature.%20We%20focus%20on%20non-training-based%20confidence-score%20functions%20for%0Ageneralized%20SC%20on%20deep%20learning%20%28DL%29%20classifiers%20and%20propose%20two%20novel%0Amargin-based%20score%20functions.%20Through%20extensive%20analysis%20and%20experiments%2C%20we%0Ashow%20that%20our%20proposed%20score%20functions%20are%20more%20effective%20and%20reliable%20than%20the%0Aexisting%20ones%20for%20generalized%20SC%20on%20a%20variety%20of%20classification%20tasks%20and%20DL%0Aclassifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Classification%2520Under%2520Distribution%2520Shifts%26entry.906535625%3DHengyue%2520Liang%2520and%2520Le%2520Peng%2520and%2520Ju%2520Sun%26entry.1292438233%3D%2520%2520In%2520selective%2520classification%2520%2528SC%2529%252C%2520a%2520classifier%2520abstains%2520from%2520making%250Apredictions%2520that%2520are%2520likely%2520to%2520be%2520wrong%2520to%2520avoid%2520excessive%2520errors.%2520To%2520deploy%250Aimperfect%2520classifiers%2520--%2520imperfect%2520either%2520due%2520to%2520intrinsic%2520statistical%2520noise%2520of%250Adata%2520or%2520for%2520robustness%2520issue%2520of%2520the%2520classifier%2520or%2520beyond%2520--%2520in%2520high-stakes%250Ascenarios%252C%2520SC%2520appears%2520to%2520be%2520an%2520attractive%2520and%2520necessary%2520path%2520to%2520follow.%2520Despite%250Adecades%2520of%2520research%2520in%2520SC%252C%2520most%2520previous%2520SC%2520methods%2520still%2520focus%2520on%2520the%2520ideal%250Astatistical%2520setting%2520only%252C%2520i.e.%252C%2520the%2520data%2520distribution%2520at%2520deployment%2520is%2520the%2520same%250Aas%2520that%2520of%2520training%252C%2520although%2520practical%2520data%2520can%2520come%2520from%2520the%2520wild.%2520To%2520bridge%250Athis%2520gap%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520an%2520SC%2520framework%2520that%2520takes%2520into%2520account%250Adistribution%2520shifts%252C%2520termed%2520generalized%2520selective%2520classification%252C%2520that%2520covers%250Alabel-shifted%2520%2528or%2520out-of-distribution%2529%2520and%2520covariate-shifted%2520samples%252C%2520in%250Aaddition%2520to%2520typical%2520in-distribution%2520samples%252C%2520the%2520first%2520of%2520its%2520kind%2520in%2520the%2520SC%250Aliterature.%2520We%2520focus%2520on%2520non-training-based%2520confidence-score%2520functions%2520for%250Ageneralized%2520SC%2520on%2520deep%2520learning%2520%2528DL%2529%2520classifiers%2520and%2520propose%2520two%2520novel%250Amargin-based%2520score%2520functions.%2520Through%2520extensive%2520analysis%2520and%2520experiments%252C%2520we%250Ashow%2520that%2520our%2520proposed%2520score%2520functions%2520are%2520more%2520effective%2520and%2520reliable%2520than%2520the%250Aexisting%2520ones%2520for%2520generalized%2520SC%2520on%2520a%2520variety%2520of%2520classification%2520tasks%2520and%2520DL%250Aclassifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Classification%20Under%20Distribution%20Shifts&entry.906535625=Hengyue%20Liang%20and%20Le%20Peng%20and%20Ju%20Sun&entry.1292438233=%20%20In%20selective%20classification%20%28SC%29%2C%20a%20classifier%20abstains%20from%20making%0Apredictions%20that%20are%20likely%20to%20be%20wrong%20to%20avoid%20excessive%20errors.%20To%20deploy%0Aimperfect%20classifiers%20--%20imperfect%20either%20due%20to%20intrinsic%20statistical%20noise%20of%0Adata%20or%20for%20robustness%20issue%20of%20the%20classifier%20or%20beyond%20--%20in%20high-stakes%0Ascenarios%2C%20SC%20appears%20to%20be%20an%20attractive%20and%20necessary%20path%20to%20follow.%20Despite%0Adecades%20of%20research%20in%20SC%2C%20most%20previous%20SC%20methods%20still%20focus%20on%20the%20ideal%0Astatistical%20setting%20only%2C%20i.e.%2C%20the%20data%20distribution%20at%20deployment%20is%20the%20same%0Aas%20that%20of%20training%2C%20although%20practical%20data%20can%20come%20from%20the%20wild.%20To%20bridge%0Athis%20gap%2C%20in%20this%20paper%2C%20we%20propose%20an%20SC%20framework%20that%20takes%20into%20account%0Adistribution%20shifts%2C%20termed%20generalized%20selective%20classification%2C%20that%20covers%0Alabel-shifted%20%28or%20out-of-distribution%29%20and%20covariate-shifted%20samples%2C%20in%0Aaddition%20to%20typical%20in-distribution%20samples%2C%20the%20first%20of%20its%20kind%20in%20the%20SC%0Aliterature.%20We%20focus%20on%20non-training-based%20confidence-score%20functions%20for%0Ageneralized%20SC%20on%20deep%20learning%20%28DL%29%20classifiers%20and%20propose%20two%20novel%0Amargin-based%20score%20functions.%20Through%20extensive%20analysis%20and%20experiments%2C%20we%0Ashow%20that%20our%20proposed%20score%20functions%20are%20more%20effective%20and%20reliable%20than%20the%0Aexisting%20ones%20for%20generalized%20SC%20on%20a%20variety%20of%20classification%20tasks%20and%20DL%0Aclassifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05160v1&entry.124074799=Read"},
{"title": "Is Transductive Learning Equivalent to PAC Learning?", "author": "Shaddin Dughmi and Yusuf Kalayci and Grayson York", "abstract": "  Most work in the area of learning theory has focused on designing effective\nProbably Approximately Correct (PAC) learners. Recently, other models of\nlearning such as transductive error have seen more scrutiny. We move toward\nshowing that these problems are equivalent by reducing agnostic learning with a\nPAC guarantee to agnostic learning with a transductive guarantee by adding a\nsmall number of samples to the dataset. We first rederive the result of\nAden-Ali et al. arXiv:2304.09167 reducing PAC learning to transductive learning\nin the realizable setting using simpler techniques and at more generality as\nbackground for our main positive result. Our agnostic transductive to PAC\nconversion technique extends the aforementioned argument to the agnostic case,\nshowing that an agnostic transductive learner can be efficiently converted to\nan agnostic PAC learner. Finally, we characterize the performance of the\nagnostic one inclusion graph algorithm of Asilis et al. arXiv:2309.13692 for\nbinary classification, and show that plugging it into our reduction leads to an\nagnostic PAC learner that is essentially optimal. Our results imply that\ntransductive and PAC learning are essentially equivalent for supervised\nlearning with pseudometric losses in the realizable setting, and for binary\nclassification in the agnostic setting. We conjecture this is true more\ngenerally for the agnostic setting.\n", "link": "http://arxiv.org/abs/2405.05190v1", "date": "2024-05-08", "relevancy": 2.168, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4421}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4308}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Transductive%20Learning%20Equivalent%20to%20PAC%20Learning%3F&body=Title%3A%20Is%20Transductive%20Learning%20Equivalent%20to%20PAC%20Learning%3F%0AAuthor%3A%20Shaddin%20Dughmi%20and%20Yusuf%20Kalayci%20and%20Grayson%20York%0AAbstract%3A%20%20%20Most%20work%20in%20the%20area%20of%20learning%20theory%20has%20focused%20on%20designing%20effective%0AProbably%20Approximately%20Correct%20%28PAC%29%20learners.%20Recently%2C%20other%20models%20of%0Alearning%20such%20as%20transductive%20error%20have%20seen%20more%20scrutiny.%20We%20move%20toward%0Ashowing%20that%20these%20problems%20are%20equivalent%20by%20reducing%20agnostic%20learning%20with%20a%0APAC%20guarantee%20to%20agnostic%20learning%20with%20a%20transductive%20guarantee%20by%20adding%20a%0Asmall%20number%20of%20samples%20to%20the%20dataset.%20We%20first%20rederive%20the%20result%20of%0AAden-Ali%20et%20al.%20arXiv%3A2304.09167%20reducing%20PAC%20learning%20to%20transductive%20learning%0Ain%20the%20realizable%20setting%20using%20simpler%20techniques%20and%20at%20more%20generality%20as%0Abackground%20for%20our%20main%20positive%20result.%20Our%20agnostic%20transductive%20to%20PAC%0Aconversion%20technique%20extends%20the%20aforementioned%20argument%20to%20the%20agnostic%20case%2C%0Ashowing%20that%20an%20agnostic%20transductive%20learner%20can%20be%20efficiently%20converted%20to%0Aan%20agnostic%20PAC%20learner.%20Finally%2C%20we%20characterize%20the%20performance%20of%20the%0Aagnostic%20one%20inclusion%20graph%20algorithm%20of%20Asilis%20et%20al.%20arXiv%3A2309.13692%20for%0Abinary%20classification%2C%20and%20show%20that%20plugging%20it%20into%20our%20reduction%20leads%20to%20an%0Aagnostic%20PAC%20learner%20that%20is%20essentially%20optimal.%20Our%20results%20imply%20that%0Atransductive%20and%20PAC%20learning%20are%20essentially%20equivalent%20for%20supervised%0Alearning%20with%20pseudometric%20losses%20in%20the%20realizable%20setting%2C%20and%20for%20binary%0Aclassification%20in%20the%20agnostic%20setting.%20We%20conjecture%20this%20is%20true%20more%0Agenerally%20for%20the%20agnostic%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Transductive%2520Learning%2520Equivalent%2520to%2520PAC%2520Learning%253F%26entry.906535625%3DShaddin%2520Dughmi%2520and%2520Yusuf%2520Kalayci%2520and%2520Grayson%2520York%26entry.1292438233%3D%2520%2520Most%2520work%2520in%2520the%2520area%2520of%2520learning%2520theory%2520has%2520focused%2520on%2520designing%2520effective%250AProbably%2520Approximately%2520Correct%2520%2528PAC%2529%2520learners.%2520Recently%252C%2520other%2520models%2520of%250Alearning%2520such%2520as%2520transductive%2520error%2520have%2520seen%2520more%2520scrutiny.%2520We%2520move%2520toward%250Ashowing%2520that%2520these%2520problems%2520are%2520equivalent%2520by%2520reducing%2520agnostic%2520learning%2520with%2520a%250APAC%2520guarantee%2520to%2520agnostic%2520learning%2520with%2520a%2520transductive%2520guarantee%2520by%2520adding%2520a%250Asmall%2520number%2520of%2520samples%2520to%2520the%2520dataset.%2520We%2520first%2520rederive%2520the%2520result%2520of%250AAden-Ali%2520et%2520al.%2520arXiv%253A2304.09167%2520reducing%2520PAC%2520learning%2520to%2520transductive%2520learning%250Ain%2520the%2520realizable%2520setting%2520using%2520simpler%2520techniques%2520and%2520at%2520more%2520generality%2520as%250Abackground%2520for%2520our%2520main%2520positive%2520result.%2520Our%2520agnostic%2520transductive%2520to%2520PAC%250Aconversion%2520technique%2520extends%2520the%2520aforementioned%2520argument%2520to%2520the%2520agnostic%2520case%252C%250Ashowing%2520that%2520an%2520agnostic%2520transductive%2520learner%2520can%2520be%2520efficiently%2520converted%2520to%250Aan%2520agnostic%2520PAC%2520learner.%2520Finally%252C%2520we%2520characterize%2520the%2520performance%2520of%2520the%250Aagnostic%2520one%2520inclusion%2520graph%2520algorithm%2520of%2520Asilis%2520et%2520al.%2520arXiv%253A2309.13692%2520for%250Abinary%2520classification%252C%2520and%2520show%2520that%2520plugging%2520it%2520into%2520our%2520reduction%2520leads%2520to%2520an%250Aagnostic%2520PAC%2520learner%2520that%2520is%2520essentially%2520optimal.%2520Our%2520results%2520imply%2520that%250Atransductive%2520and%2520PAC%2520learning%2520are%2520essentially%2520equivalent%2520for%2520supervised%250Alearning%2520with%2520pseudometric%2520losses%2520in%2520the%2520realizable%2520setting%252C%2520and%2520for%2520binary%250Aclassification%2520in%2520the%2520agnostic%2520setting.%2520We%2520conjecture%2520this%2520is%2520true%2520more%250Agenerally%2520for%2520the%2520agnostic%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Transductive%20Learning%20Equivalent%20to%20PAC%20Learning%3F&entry.906535625=Shaddin%20Dughmi%20and%20Yusuf%20Kalayci%20and%20Grayson%20York&entry.1292438233=%20%20Most%20work%20in%20the%20area%20of%20learning%20theory%20has%20focused%20on%20designing%20effective%0AProbably%20Approximately%20Correct%20%28PAC%29%20learners.%20Recently%2C%20other%20models%20of%0Alearning%20such%20as%20transductive%20error%20have%20seen%20more%20scrutiny.%20We%20move%20toward%0Ashowing%20that%20these%20problems%20are%20equivalent%20by%20reducing%20agnostic%20learning%20with%20a%0APAC%20guarantee%20to%20agnostic%20learning%20with%20a%20transductive%20guarantee%20by%20adding%20a%0Asmall%20number%20of%20samples%20to%20the%20dataset.%20We%20first%20rederive%20the%20result%20of%0AAden-Ali%20et%20al.%20arXiv%3A2304.09167%20reducing%20PAC%20learning%20to%20transductive%20learning%0Ain%20the%20realizable%20setting%20using%20simpler%20techniques%20and%20at%20more%20generality%20as%0Abackground%20for%20our%20main%20positive%20result.%20Our%20agnostic%20transductive%20to%20PAC%0Aconversion%20technique%20extends%20the%20aforementioned%20argument%20to%20the%20agnostic%20case%2C%0Ashowing%20that%20an%20agnostic%20transductive%20learner%20can%20be%20efficiently%20converted%20to%0Aan%20agnostic%20PAC%20learner.%20Finally%2C%20we%20characterize%20the%20performance%20of%20the%0Aagnostic%20one%20inclusion%20graph%20algorithm%20of%20Asilis%20et%20al.%20arXiv%3A2309.13692%20for%0Abinary%20classification%2C%20and%20show%20that%20plugging%20it%20into%20our%20reduction%20leads%20to%20an%0Aagnostic%20PAC%20learner%20that%20is%20essentially%20optimal.%20Our%20results%20imply%20that%0Atransductive%20and%20PAC%20learning%20are%20essentially%20equivalent%20for%20supervised%0Alearning%20with%20pseudometric%20losses%20in%20the%20realizable%20setting%2C%20and%20for%20binary%0Aclassification%20in%20the%20agnostic%20setting.%20We%20conjecture%20this%20is%20true%20more%0Agenerally%20for%20the%20agnostic%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05190v1&entry.124074799=Read"},
{"title": "Ethical Implications of ChatGPT in Higher Education: A Scoping Review", "author": "Ming Li and Ariunaa Enkhtur and Fei Cheng and Beverley Anne Yamamoto", "abstract": "  This scoping review explores the ethical challenges of using ChatGPT in\nhigher education. By reviewing recent academic articles in English, Chinese,\nand Japanese, we aimed to provide a deep dive review and identify gaps in the\nliterature. Drawing on Arksey and O'Malley's (2005) scoping review framework,\nwe defined search terms and identified relevant publications from four\ndatabases in the three target languages. The research results showed that the\nmajority of the papers were discussion papers, but there was some early\nempirical work. The ethical issues highlighted in these works mainly concern\nacademic integrity, assessment issues, and data protection. Given the rapid\ndeployment of generative artificial intelligence, it is imperative for\neducators to conduct more empirical studies to develop sound ethical policies\nfor its use.\n", "link": "http://arxiv.org/abs/2311.14378v2", "date": "2024-05-08", "relevancy": 2.166, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4635}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4378}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ethical%20Implications%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%20Review&body=Title%3A%20Ethical%20Implications%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%20Review%0AAuthor%3A%20Ming%20Li%20and%20Ariunaa%20Enkhtur%20and%20Fei%20Cheng%20and%20Beverley%20Anne%20Yamamoto%0AAbstract%3A%20%20%20This%20scoping%20review%20explores%20the%20ethical%20challenges%20of%20using%20ChatGPT%20in%0Ahigher%20education.%20By%20reviewing%20recent%20academic%20articles%20in%20English%2C%20Chinese%2C%0Aand%20Japanese%2C%20we%20aimed%20to%20provide%20a%20deep%20dive%20review%20and%20identify%20gaps%20in%20the%0Aliterature.%20Drawing%20on%20Arksey%20and%20O%27Malley%27s%20%282005%29%20scoping%20review%20framework%2C%0Awe%20defined%20search%20terms%20and%20identified%20relevant%20publications%20from%20four%0Adatabases%20in%20the%20three%20target%20languages.%20The%20research%20results%20showed%20that%20the%0Amajority%20of%20the%20papers%20were%20discussion%20papers%2C%20but%20there%20was%20some%20early%0Aempirical%20work.%20The%20ethical%20issues%20highlighted%20in%20these%20works%20mainly%20concern%0Aacademic%20integrity%2C%20assessment%20issues%2C%20and%20data%20protection.%20Given%20the%20rapid%0Adeployment%20of%20generative%20artificial%20intelligence%2C%20it%20is%20imperative%20for%0Aeducators%20to%20conduct%20more%20empirical%20studies%20to%20develop%20sound%20ethical%20policies%0Afor%20its%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEthical%2520Implications%2520of%2520ChatGPT%2520in%2520Higher%2520Education%253A%2520A%2520Scoping%2520Review%26entry.906535625%3DMing%2520Li%2520and%2520Ariunaa%2520Enkhtur%2520and%2520Fei%2520Cheng%2520and%2520Beverley%2520Anne%2520Yamamoto%26entry.1292438233%3D%2520%2520This%2520scoping%2520review%2520explores%2520the%2520ethical%2520challenges%2520of%2520using%2520ChatGPT%2520in%250Ahigher%2520education.%2520By%2520reviewing%2520recent%2520academic%2520articles%2520in%2520English%252C%2520Chinese%252C%250Aand%2520Japanese%252C%2520we%2520aimed%2520to%2520provide%2520a%2520deep%2520dive%2520review%2520and%2520identify%2520gaps%2520in%2520the%250Aliterature.%2520Drawing%2520on%2520Arksey%2520and%2520O%2527Malley%2527s%2520%25282005%2529%2520scoping%2520review%2520framework%252C%250Awe%2520defined%2520search%2520terms%2520and%2520identified%2520relevant%2520publications%2520from%2520four%250Adatabases%2520in%2520the%2520three%2520target%2520languages.%2520The%2520research%2520results%2520showed%2520that%2520the%250Amajority%2520of%2520the%2520papers%2520were%2520discussion%2520papers%252C%2520but%2520there%2520was%2520some%2520early%250Aempirical%2520work.%2520The%2520ethical%2520issues%2520highlighted%2520in%2520these%2520works%2520mainly%2520concern%250Aacademic%2520integrity%252C%2520assessment%2520issues%252C%2520and%2520data%2520protection.%2520Given%2520the%2520rapid%250Adeployment%2520of%2520generative%2520artificial%2520intelligence%252C%2520it%2520is%2520imperative%2520for%250Aeducators%2520to%2520conduct%2520more%2520empirical%2520studies%2520to%2520develop%2520sound%2520ethical%2520policies%250Afor%2520its%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ethical%20Implications%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%20Review&entry.906535625=Ming%20Li%20and%20Ariunaa%20Enkhtur%20and%20Fei%20Cheng%20and%20Beverley%20Anne%20Yamamoto&entry.1292438233=%20%20This%20scoping%20review%20explores%20the%20ethical%20challenges%20of%20using%20ChatGPT%20in%0Ahigher%20education.%20By%20reviewing%20recent%20academic%20articles%20in%20English%2C%20Chinese%2C%0Aand%20Japanese%2C%20we%20aimed%20to%20provide%20a%20deep%20dive%20review%20and%20identify%20gaps%20in%20the%0Aliterature.%20Drawing%20on%20Arksey%20and%20O%27Malley%27s%20%282005%29%20scoping%20review%20framework%2C%0Awe%20defined%20search%20terms%20and%20identified%20relevant%20publications%20from%20four%0Adatabases%20in%20the%20three%20target%20languages.%20The%20research%20results%20showed%20that%20the%0Amajority%20of%20the%20papers%20were%20discussion%20papers%2C%20but%20there%20was%20some%20early%0Aempirical%20work.%20The%20ethical%20issues%20highlighted%20in%20these%20works%20mainly%20concern%0Aacademic%20integrity%2C%20assessment%20issues%2C%20and%20data%20protection.%20Given%20the%20rapid%0Adeployment%20of%20generative%20artificial%20intelligence%2C%20it%20is%20imperative%20for%0Aeducators%20to%20conduct%20more%20empirical%20studies%20to%20develop%20sound%20ethical%20policies%0Afor%20its%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14378v2&entry.124074799=Read"},
{"title": "Real-Time Motion Detection Using Dynamic Mode Decomposition", "author": "Marco Mignacca and Simone Brugiapaglia and Jason J. Bramburger", "abstract": "  Dynamic Mode Decomposition (DMD) is a numerical method that seeks to fit\ntimeseries data to a linear dynamical system. In doing so, DMD decomposes\ndynamic data into spatially coherent modes that evolve in time according to\nexponential growth/decay or with a fixed frequency of oscillation. A prolific\napplication of DMD has been to video, where one interprets the high-dimensional\npixel space evolving through time as the video plays. In this work, we propose\na simple and interpretable motion detection algorithm for streaming video data\nrooted in DMD. Our method leverages the fact that there exists a correspondence\nbetween the evolution of important video features, such as foreground motion,\nand the eigenvalues of the matrix which results from applying DMD to segments\nof video. We apply the method to a database of test videos which emulate\nsecurity footage under varying realistic conditions. Effectiveness is analyzed\nusing receiver operating characteristic curves, while we use cross-validation\nto optimize the threshold parameter that identifies movement.\n", "link": "http://arxiv.org/abs/2405.05057v1", "date": "2024-05-08", "relevancy": 2.1505, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5391}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5387}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Motion%20Detection%20Using%20Dynamic%20Mode%20Decomposition&body=Title%3A%20Real-Time%20Motion%20Detection%20Using%20Dynamic%20Mode%20Decomposition%0AAuthor%3A%20Marco%20Mignacca%20and%20Simone%20Brugiapaglia%20and%20Jason%20J.%20Bramburger%0AAbstract%3A%20%20%20Dynamic%20Mode%20Decomposition%20%28DMD%29%20is%20a%20numerical%20method%20that%20seeks%20to%20fit%0Atimeseries%20data%20to%20a%20linear%20dynamical%20system.%20In%20doing%20so%2C%20DMD%20decomposes%0Adynamic%20data%20into%20spatially%20coherent%20modes%20that%20evolve%20in%20time%20according%20to%0Aexponential%20growth/decay%20or%20with%20a%20fixed%20frequency%20of%20oscillation.%20A%20prolific%0Aapplication%20of%20DMD%20has%20been%20to%20video%2C%20where%20one%20interprets%20the%20high-dimensional%0Apixel%20space%20evolving%20through%20time%20as%20the%20video%20plays.%20In%20this%20work%2C%20we%20propose%0Aa%20simple%20and%20interpretable%20motion%20detection%20algorithm%20for%20streaming%20video%20data%0Arooted%20in%20DMD.%20Our%20method%20leverages%20the%20fact%20that%20there%20exists%20a%20correspondence%0Abetween%20the%20evolution%20of%20important%20video%20features%2C%20such%20as%20foreground%20motion%2C%0Aand%20the%20eigenvalues%20of%20the%20matrix%20which%20results%20from%20applying%20DMD%20to%20segments%0Aof%20video.%20We%20apply%20the%20method%20to%20a%20database%20of%20test%20videos%20which%20emulate%0Asecurity%20footage%20under%20varying%20realistic%20conditions.%20Effectiveness%20is%20analyzed%0Ausing%20receiver%20operating%20characteristic%20curves%2C%20while%20we%20use%20cross-validation%0Ato%20optimize%20the%20threshold%20parameter%20that%20identifies%20movement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Motion%2520Detection%2520Using%2520Dynamic%2520Mode%2520Decomposition%26entry.906535625%3DMarco%2520Mignacca%2520and%2520Simone%2520Brugiapaglia%2520and%2520Jason%2520J.%2520Bramburger%26entry.1292438233%3D%2520%2520Dynamic%2520Mode%2520Decomposition%2520%2528DMD%2529%2520is%2520a%2520numerical%2520method%2520that%2520seeks%2520to%2520fit%250Atimeseries%2520data%2520to%2520a%2520linear%2520dynamical%2520system.%2520In%2520doing%2520so%252C%2520DMD%2520decomposes%250Adynamic%2520data%2520into%2520spatially%2520coherent%2520modes%2520that%2520evolve%2520in%2520time%2520according%2520to%250Aexponential%2520growth/decay%2520or%2520with%2520a%2520fixed%2520frequency%2520of%2520oscillation.%2520A%2520prolific%250Aapplication%2520of%2520DMD%2520has%2520been%2520to%2520video%252C%2520where%2520one%2520interprets%2520the%2520high-dimensional%250Apixel%2520space%2520evolving%2520through%2520time%2520as%2520the%2520video%2520plays.%2520In%2520this%2520work%252C%2520we%2520propose%250Aa%2520simple%2520and%2520interpretable%2520motion%2520detection%2520algorithm%2520for%2520streaming%2520video%2520data%250Arooted%2520in%2520DMD.%2520Our%2520method%2520leverages%2520the%2520fact%2520that%2520there%2520exists%2520a%2520correspondence%250Abetween%2520the%2520evolution%2520of%2520important%2520video%2520features%252C%2520such%2520as%2520foreground%2520motion%252C%250Aand%2520the%2520eigenvalues%2520of%2520the%2520matrix%2520which%2520results%2520from%2520applying%2520DMD%2520to%2520segments%250Aof%2520video.%2520We%2520apply%2520the%2520method%2520to%2520a%2520database%2520of%2520test%2520videos%2520which%2520emulate%250Asecurity%2520footage%2520under%2520varying%2520realistic%2520conditions.%2520Effectiveness%2520is%2520analyzed%250Ausing%2520receiver%2520operating%2520characteristic%2520curves%252C%2520while%2520we%2520use%2520cross-validation%250Ato%2520optimize%2520the%2520threshold%2520parameter%2520that%2520identifies%2520movement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Motion%20Detection%20Using%20Dynamic%20Mode%20Decomposition&entry.906535625=Marco%20Mignacca%20and%20Simone%20Brugiapaglia%20and%20Jason%20J.%20Bramburger&entry.1292438233=%20%20Dynamic%20Mode%20Decomposition%20%28DMD%29%20is%20a%20numerical%20method%20that%20seeks%20to%20fit%0Atimeseries%20data%20to%20a%20linear%20dynamical%20system.%20In%20doing%20so%2C%20DMD%20decomposes%0Adynamic%20data%20into%20spatially%20coherent%20modes%20that%20evolve%20in%20time%20according%20to%0Aexponential%20growth/decay%20or%20with%20a%20fixed%20frequency%20of%20oscillation.%20A%20prolific%0Aapplication%20of%20DMD%20has%20been%20to%20video%2C%20where%20one%20interprets%20the%20high-dimensional%0Apixel%20space%20evolving%20through%20time%20as%20the%20video%20plays.%20In%20this%20work%2C%20we%20propose%0Aa%20simple%20and%20interpretable%20motion%20detection%20algorithm%20for%20streaming%20video%20data%0Arooted%20in%20DMD.%20Our%20method%20leverages%20the%20fact%20that%20there%20exists%20a%20correspondence%0Abetween%20the%20evolution%20of%20important%20video%20features%2C%20such%20as%20foreground%20motion%2C%0Aand%20the%20eigenvalues%20of%20the%20matrix%20which%20results%20from%20applying%20DMD%20to%20segments%0Aof%20video.%20We%20apply%20the%20method%20to%20a%20database%20of%20test%20videos%20which%20emulate%0Asecurity%20footage%20under%20varying%20realistic%20conditions.%20Effectiveness%20is%20analyzed%0Ausing%20receiver%20operating%20characteristic%20curves%2C%20while%20we%20use%20cross-validation%0Ato%20optimize%20the%20threshold%20parameter%20that%20identifies%20movement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05057v1&entry.124074799=Read"},
{"title": "HMANet: Hybrid Multi-Axis Aggregation Network for Image Super-Resolution", "author": "Shu-Chuan Chu and Zhi-Chao Dou and Jeng-Shyang Pan and Shaowei Weng and Junbao Li", "abstract": "  Transformer-based methods have demonstrated excellent performance on\nsuper-resolution visual tasks, surpassing conventional convolutional neural\nnetworks. However, existing work typically restricts self-attention computation\nto non-overlapping windows to save computational costs. This means that\nTransformer-based networks can only use input information from a limited\nspatial range. Therefore, a novel Hybrid Multi-Axis Aggregation network (HMA)\nis proposed in this paper to exploit feature potential information better. HMA\nis constructed by stacking Residual Hybrid Transformer Blocks(RHTB) and Grid\nAttention Blocks(GAB). On the one side, RHTB combines channel attention and\nself-attention to enhance non-local feature fusion and produce more attractive\nvisual results. Conversely, GAB is used in cross-domain information interaction\nto jointly model similar features and obtain a larger perceptual field. For the\nsuper-resolution task in the training phase, a novel pre-training method is\ndesigned to enhance the model representation capabilities further and validate\nthe proposed model's effectiveness through many experiments. The experimental\nresults show that HMA outperforms the state-of-the-art methods on the benchmark\ndataset. We provide code and models at https://github.com/korouuuuu/HMA.\n", "link": "http://arxiv.org/abs/2405.05001v1", "date": "2024-05-08", "relevancy": 2.1502, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5408}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HMANet%3A%20Hybrid%20Multi-Axis%20Aggregation%20Network%20for%20Image%20Super-Resolution&body=Title%3A%20HMANet%3A%20Hybrid%20Multi-Axis%20Aggregation%20Network%20for%20Image%20Super-Resolution%0AAuthor%3A%20Shu-Chuan%20Chu%20and%20Zhi-Chao%20Dou%20and%20Jeng-Shyang%20Pan%20and%20Shaowei%20Weng%20and%20Junbao%20Li%0AAbstract%3A%20%20%20Transformer-based%20methods%20have%20demonstrated%20excellent%20performance%20on%0Asuper-resolution%20visual%20tasks%2C%20surpassing%20conventional%20convolutional%20neural%0Anetworks.%20However%2C%20existing%20work%20typically%20restricts%20self-attention%20computation%0Ato%20non-overlapping%20windows%20to%20save%20computational%20costs.%20This%20means%20that%0ATransformer-based%20networks%20can%20only%20use%20input%20information%20from%20a%20limited%0Aspatial%20range.%20Therefore%2C%20a%20novel%20Hybrid%20Multi-Axis%20Aggregation%20network%20%28HMA%29%0Ais%20proposed%20in%20this%20paper%20to%20exploit%20feature%20potential%20information%20better.%20HMA%0Ais%20constructed%20by%20stacking%20Residual%20Hybrid%20Transformer%20Blocks%28RHTB%29%20and%20Grid%0AAttention%20Blocks%28GAB%29.%20On%20the%20one%20side%2C%20RHTB%20combines%20channel%20attention%20and%0Aself-attention%20to%20enhance%20non-local%20feature%20fusion%20and%20produce%20more%20attractive%0Avisual%20results.%20Conversely%2C%20GAB%20is%20used%20in%20cross-domain%20information%20interaction%0Ato%20jointly%20model%20similar%20features%20and%20obtain%20a%20larger%20perceptual%20field.%20For%20the%0Asuper-resolution%20task%20in%20the%20training%20phase%2C%20a%20novel%20pre-training%20method%20is%0Adesigned%20to%20enhance%20the%20model%20representation%20capabilities%20further%20and%20validate%0Athe%20proposed%20model%27s%20effectiveness%20through%20many%20experiments.%20The%20experimental%0Aresults%20show%20that%20HMA%20outperforms%20the%20state-of-the-art%20methods%20on%20the%20benchmark%0Adataset.%20We%20provide%20code%20and%20models%20at%20https%3A//github.com/korouuuuu/HMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHMANet%253A%2520Hybrid%2520Multi-Axis%2520Aggregation%2520Network%2520for%2520Image%2520Super-Resolution%26entry.906535625%3DShu-Chuan%2520Chu%2520and%2520Zhi-Chao%2520Dou%2520and%2520Jeng-Shyang%2520Pan%2520and%2520Shaowei%2520Weng%2520and%2520Junbao%2520Li%26entry.1292438233%3D%2520%2520Transformer-based%2520methods%2520have%2520demonstrated%2520excellent%2520performance%2520on%250Asuper-resolution%2520visual%2520tasks%252C%2520surpassing%2520conventional%2520convolutional%2520neural%250Anetworks.%2520However%252C%2520existing%2520work%2520typically%2520restricts%2520self-attention%2520computation%250Ato%2520non-overlapping%2520windows%2520to%2520save%2520computational%2520costs.%2520This%2520means%2520that%250ATransformer-based%2520networks%2520can%2520only%2520use%2520input%2520information%2520from%2520a%2520limited%250Aspatial%2520range.%2520Therefore%252C%2520a%2520novel%2520Hybrid%2520Multi-Axis%2520Aggregation%2520network%2520%2528HMA%2529%250Ais%2520proposed%2520in%2520this%2520paper%2520to%2520exploit%2520feature%2520potential%2520information%2520better.%2520HMA%250Ais%2520constructed%2520by%2520stacking%2520Residual%2520Hybrid%2520Transformer%2520Blocks%2528RHTB%2529%2520and%2520Grid%250AAttention%2520Blocks%2528GAB%2529.%2520On%2520the%2520one%2520side%252C%2520RHTB%2520combines%2520channel%2520attention%2520and%250Aself-attention%2520to%2520enhance%2520non-local%2520feature%2520fusion%2520and%2520produce%2520more%2520attractive%250Avisual%2520results.%2520Conversely%252C%2520GAB%2520is%2520used%2520in%2520cross-domain%2520information%2520interaction%250Ato%2520jointly%2520model%2520similar%2520features%2520and%2520obtain%2520a%2520larger%2520perceptual%2520field.%2520For%2520the%250Asuper-resolution%2520task%2520in%2520the%2520training%2520phase%252C%2520a%2520novel%2520pre-training%2520method%2520is%250Adesigned%2520to%2520enhance%2520the%2520model%2520representation%2520capabilities%2520further%2520and%2520validate%250Athe%2520proposed%2520model%2527s%2520effectiveness%2520through%2520many%2520experiments.%2520The%2520experimental%250Aresults%2520show%2520that%2520HMA%2520outperforms%2520the%2520state-of-the-art%2520methods%2520on%2520the%2520benchmark%250Adataset.%2520We%2520provide%2520code%2520and%2520models%2520at%2520https%253A//github.com/korouuuuu/HMA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HMANet%3A%20Hybrid%20Multi-Axis%20Aggregation%20Network%20for%20Image%20Super-Resolution&entry.906535625=Shu-Chuan%20Chu%20and%20Zhi-Chao%20Dou%20and%20Jeng-Shyang%20Pan%20and%20Shaowei%20Weng%20and%20Junbao%20Li&entry.1292438233=%20%20Transformer-based%20methods%20have%20demonstrated%20excellent%20performance%20on%0Asuper-resolution%20visual%20tasks%2C%20surpassing%20conventional%20convolutional%20neural%0Anetworks.%20However%2C%20existing%20work%20typically%20restricts%20self-attention%20computation%0Ato%20non-overlapping%20windows%20to%20save%20computational%20costs.%20This%20means%20that%0ATransformer-based%20networks%20can%20only%20use%20input%20information%20from%20a%20limited%0Aspatial%20range.%20Therefore%2C%20a%20novel%20Hybrid%20Multi-Axis%20Aggregation%20network%20%28HMA%29%0Ais%20proposed%20in%20this%20paper%20to%20exploit%20feature%20potential%20information%20better.%20HMA%0Ais%20constructed%20by%20stacking%20Residual%20Hybrid%20Transformer%20Blocks%28RHTB%29%20and%20Grid%0AAttention%20Blocks%28GAB%29.%20On%20the%20one%20side%2C%20RHTB%20combines%20channel%20attention%20and%0Aself-attention%20to%20enhance%20non-local%20feature%20fusion%20and%20produce%20more%20attractive%0Avisual%20results.%20Conversely%2C%20GAB%20is%20used%20in%20cross-domain%20information%20interaction%0Ato%20jointly%20model%20similar%20features%20and%20obtain%20a%20larger%20perceptual%20field.%20For%20the%0Asuper-resolution%20task%20in%20the%20training%20phase%2C%20a%20novel%20pre-training%20method%20is%0Adesigned%20to%20enhance%20the%20model%20representation%20capabilities%20further%20and%20validate%0Athe%20proposed%20model%27s%20effectiveness%20through%20many%20experiments.%20The%20experimental%0Aresults%20show%20that%20HMA%20outperforms%20the%20state-of-the-art%20methods%20on%20the%20benchmark%0Adataset.%20We%20provide%20code%20and%20models%20at%20https%3A//github.com/korouuuuu/HMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05001v1&entry.124074799=Read"},
{"title": "TGTM: TinyML-based Global Tone Mapping for HDR Sensors", "author": "Peter Todorov and Julian Hartig and Jan Meyer-Siemon and Martin Fiedler and Gregor Schewior", "abstract": "  Advanced driver assistance systems (ADAS) relying on multiple cameras are\nincreasingly prevalent in vehicle technology. Yet, conventional imaging sensors\nstruggle to capture clear images in conditions with intense illumination\ncontrast, such as tunnel exits, due to their limited dynamic range. Introducing\nhigh dynamic range (HDR) sensors addresses this issue. However, the process of\nconverting HDR content to a displayable range via tone mapping often leads to\ninefficient computations, when performed directly on pixel data. In this paper,\nwe focus on HDR image tone mapping using a lightweight neural network applied\non image histogram data. Our proposed TinyML-based global tone mapping method,\ntermed as TGTM, operates at 9,000 FLOPS per RGB image of any resolution.\nAdditionally, TGTM offers a generic approach that can be incorporated to any\nclassical tone mapping method. Experimental results demonstrate that TGTM\noutperforms state-of-the-art methods on real HDR camera images by up to 5.85 dB\nhigher PSNR with orders of magnitude less computations.\n", "link": "http://arxiv.org/abs/2405.05016v1", "date": "2024-05-08", "relevancy": 2.137, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5644}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5261}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TGTM%3A%20TinyML-based%20Global%20Tone%20Mapping%20for%20HDR%20Sensors&body=Title%3A%20TGTM%3A%20TinyML-based%20Global%20Tone%20Mapping%20for%20HDR%20Sensors%0AAuthor%3A%20Peter%20Todorov%20and%20Julian%20Hartig%20and%20Jan%20Meyer-Siemon%20and%20Martin%20Fiedler%20and%20Gregor%20Schewior%0AAbstract%3A%20%20%20Advanced%20driver%20assistance%20systems%20%28ADAS%29%20relying%20on%20multiple%20cameras%20are%0Aincreasingly%20prevalent%20in%20vehicle%20technology.%20Yet%2C%20conventional%20imaging%20sensors%0Astruggle%20to%20capture%20clear%20images%20in%20conditions%20with%20intense%20illumination%0Acontrast%2C%20such%20as%20tunnel%20exits%2C%20due%20to%20their%20limited%20dynamic%20range.%20Introducing%0Ahigh%20dynamic%20range%20%28HDR%29%20sensors%20addresses%20this%20issue.%20However%2C%20the%20process%20of%0Aconverting%20HDR%20content%20to%20a%20displayable%20range%20via%20tone%20mapping%20often%20leads%20to%0Ainefficient%20computations%2C%20when%20performed%20directly%20on%20pixel%20data.%20In%20this%20paper%2C%0Awe%20focus%20on%20HDR%20image%20tone%20mapping%20using%20a%20lightweight%20neural%20network%20applied%0Aon%20image%20histogram%20data.%20Our%20proposed%20TinyML-based%20global%20tone%20mapping%20method%2C%0Atermed%20as%20TGTM%2C%20operates%20at%209%2C000%20FLOPS%20per%20RGB%20image%20of%20any%20resolution.%0AAdditionally%2C%20TGTM%20offers%20a%20generic%20approach%20that%20can%20be%20incorporated%20to%20any%0Aclassical%20tone%20mapping%20method.%20Experimental%20results%20demonstrate%20that%20TGTM%0Aoutperforms%20state-of-the-art%20methods%20on%20real%20HDR%20camera%20images%20by%20up%20to%205.85%20dB%0Ahigher%20PSNR%20with%20orders%20of%20magnitude%20less%20computations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTGTM%253A%2520TinyML-based%2520Global%2520Tone%2520Mapping%2520for%2520HDR%2520Sensors%26entry.906535625%3DPeter%2520Todorov%2520and%2520Julian%2520Hartig%2520and%2520Jan%2520Meyer-Siemon%2520and%2520Martin%2520Fiedler%2520and%2520Gregor%2520Schewior%26entry.1292438233%3D%2520%2520Advanced%2520driver%2520assistance%2520systems%2520%2528ADAS%2529%2520relying%2520on%2520multiple%2520cameras%2520are%250Aincreasingly%2520prevalent%2520in%2520vehicle%2520technology.%2520Yet%252C%2520conventional%2520imaging%2520sensors%250Astruggle%2520to%2520capture%2520clear%2520images%2520in%2520conditions%2520with%2520intense%2520illumination%250Acontrast%252C%2520such%2520as%2520tunnel%2520exits%252C%2520due%2520to%2520their%2520limited%2520dynamic%2520range.%2520Introducing%250Ahigh%2520dynamic%2520range%2520%2528HDR%2529%2520sensors%2520addresses%2520this%2520issue.%2520However%252C%2520the%2520process%2520of%250Aconverting%2520HDR%2520content%2520to%2520a%2520displayable%2520range%2520via%2520tone%2520mapping%2520often%2520leads%2520to%250Ainefficient%2520computations%252C%2520when%2520performed%2520directly%2520on%2520pixel%2520data.%2520In%2520this%2520paper%252C%250Awe%2520focus%2520on%2520HDR%2520image%2520tone%2520mapping%2520using%2520a%2520lightweight%2520neural%2520network%2520applied%250Aon%2520image%2520histogram%2520data.%2520Our%2520proposed%2520TinyML-based%2520global%2520tone%2520mapping%2520method%252C%250Atermed%2520as%2520TGTM%252C%2520operates%2520at%25209%252C000%2520FLOPS%2520per%2520RGB%2520image%2520of%2520any%2520resolution.%250AAdditionally%252C%2520TGTM%2520offers%2520a%2520generic%2520approach%2520that%2520can%2520be%2520incorporated%2520to%2520any%250Aclassical%2520tone%2520mapping%2520method.%2520Experimental%2520results%2520demonstrate%2520that%2520TGTM%250Aoutperforms%2520state-of-the-art%2520methods%2520on%2520real%2520HDR%2520camera%2520images%2520by%2520up%2520to%25205.85%2520dB%250Ahigher%2520PSNR%2520with%2520orders%2520of%2520magnitude%2520less%2520computations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TGTM%3A%20TinyML-based%20Global%20Tone%20Mapping%20for%20HDR%20Sensors&entry.906535625=Peter%20Todorov%20and%20Julian%20Hartig%20and%20Jan%20Meyer-Siemon%20and%20Martin%20Fiedler%20and%20Gregor%20Schewior&entry.1292438233=%20%20Advanced%20driver%20assistance%20systems%20%28ADAS%29%20relying%20on%20multiple%20cameras%20are%0Aincreasingly%20prevalent%20in%20vehicle%20technology.%20Yet%2C%20conventional%20imaging%20sensors%0Astruggle%20to%20capture%20clear%20images%20in%20conditions%20with%20intense%20illumination%0Acontrast%2C%20such%20as%20tunnel%20exits%2C%20due%20to%20their%20limited%20dynamic%20range.%20Introducing%0Ahigh%20dynamic%20range%20%28HDR%29%20sensors%20addresses%20this%20issue.%20However%2C%20the%20process%20of%0Aconverting%20HDR%20content%20to%20a%20displayable%20range%20via%20tone%20mapping%20often%20leads%20to%0Ainefficient%20computations%2C%20when%20performed%20directly%20on%20pixel%20data.%20In%20this%20paper%2C%0Awe%20focus%20on%20HDR%20image%20tone%20mapping%20using%20a%20lightweight%20neural%20network%20applied%0Aon%20image%20histogram%20data.%20Our%20proposed%20TinyML-based%20global%20tone%20mapping%20method%2C%0Atermed%20as%20TGTM%2C%20operates%20at%209%2C000%20FLOPS%20per%20RGB%20image%20of%20any%20resolution.%0AAdditionally%2C%20TGTM%20offers%20a%20generic%20approach%20that%20can%20be%20incorporated%20to%20any%0Aclassical%20tone%20mapping%20method.%20Experimental%20results%20demonstrate%20that%20TGTM%0Aoutperforms%20state-of-the-art%20methods%20on%20real%20HDR%20camera%20images%20by%20up%20to%205.85%20dB%0Ahigher%20PSNR%20with%20orders%20of%20magnitude%20less%20computations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05016v1&entry.124074799=Read"},
{"title": "End-to-End Semi-Supervised approach with Modulated Object Queries for\n  Table Detection in Documents", "author": "Iqraa Ehsan and Tahira Shehzadi and Didier Stricker and Muhammad Zeshan Afzal", "abstract": "  Table detection, a pivotal task in document analysis, aims to precisely\nrecognize and locate tables within document images. Although deep learning has\nshown remarkable progress in this realm, it typically requires an extensive\ndataset of labeled data for proficient training. Current CNN-based\nsemi-supervised table detection approaches use the anchor generation process\nand Non-Maximum Suppression (NMS) in their detection process, limiting training\nefficiency. Meanwhile, transformer-based semi-supervised techniques adopted a\none-to-one match strategy that provides noisy pseudo-labels, limiting overall\nefficiency. This study presents an innovative transformer-based semi-supervised\ntable detector. It improves the quality of pseudo-labels through a novel\nmatching strategy combining one-to-one and one-to-many assignment techniques.\nThis approach significantly enhances training efficiency during the early\nstages, ensuring superior pseudo-labels for further training. Our\nsemi-supervised approach is comprehensively evaluated on benchmark datasets,\nincluding PubLayNet, ICADR-19, and TableBank. It achieves new state-of-the-art\nresults, with a mAP of 95.7% and 97.9% on TableBank (word) and PubLaynet with\n30% label data, marking a 7.4 and 7.6 point improvement over previous\nsemi-supervised table detection approach, respectively. The results clearly\nshow the superiority of our semi-supervised approach, surpassing all existing\nstate-of-the-art methods by substantial margins. This research represents a\nsignificant advancement in semi-supervised table detection methods, offering a\nmore efficient and accurate solution for practical document analysis tasks.\n", "link": "http://arxiv.org/abs/2405.04971v1", "date": "2024-05-08", "relevancy": 2.1332, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.544}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5263}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Semi-Supervised%20approach%20with%20Modulated%20Object%20Queries%20for%0A%20%20Table%20Detection%20in%20Documents&body=Title%3A%20End-to-End%20Semi-Supervised%20approach%20with%20Modulated%20Object%20Queries%20for%0A%20%20Table%20Detection%20in%20Documents%0AAuthor%3A%20Iqraa%20Ehsan%20and%20Tahira%20Shehzadi%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal%0AAbstract%3A%20%20%20Table%20detection%2C%20a%20pivotal%20task%20in%20document%20analysis%2C%20aims%20to%20precisely%0Arecognize%20and%20locate%20tables%20within%20document%20images.%20Although%20deep%20learning%20has%0Ashown%20remarkable%20progress%20in%20this%20realm%2C%20it%20typically%20requires%20an%20extensive%0Adataset%20of%20labeled%20data%20for%20proficient%20training.%20Current%20CNN-based%0Asemi-supervised%20table%20detection%20approaches%20use%20the%20anchor%20generation%20process%0Aand%20Non-Maximum%20Suppression%20%28NMS%29%20in%20their%20detection%20process%2C%20limiting%20training%0Aefficiency.%20Meanwhile%2C%20transformer-based%20semi-supervised%20techniques%20adopted%20a%0Aone-to-one%20match%20strategy%20that%20provides%20noisy%20pseudo-labels%2C%20limiting%20overall%0Aefficiency.%20This%20study%20presents%20an%20innovative%20transformer-based%20semi-supervised%0Atable%20detector.%20It%20improves%20the%20quality%20of%20pseudo-labels%20through%20a%20novel%0Amatching%20strategy%20combining%20one-to-one%20and%20one-to-many%20assignment%20techniques.%0AThis%20approach%20significantly%20enhances%20training%20efficiency%20during%20the%20early%0Astages%2C%20ensuring%20superior%20pseudo-labels%20for%20further%20training.%20Our%0Asemi-supervised%20approach%20is%20comprehensively%20evaluated%20on%20benchmark%20datasets%2C%0Aincluding%20PubLayNet%2C%20ICADR-19%2C%20and%20TableBank.%20It%20achieves%20new%20state-of-the-art%0Aresults%2C%20with%20a%20mAP%20of%2095.7%25%20and%2097.9%25%20on%20TableBank%20%28word%29%20and%20PubLaynet%20with%0A30%25%20label%20data%2C%20marking%20a%207.4%20and%207.6%20point%20improvement%20over%20previous%0Asemi-supervised%20table%20detection%20approach%2C%20respectively.%20The%20results%20clearly%0Ashow%20the%20superiority%20of%20our%20semi-supervised%20approach%2C%20surpassing%20all%20existing%0Astate-of-the-art%20methods%20by%20substantial%20margins.%20This%20research%20represents%20a%0Asignificant%20advancement%20in%20semi-supervised%20table%20detection%20methods%2C%20offering%20a%0Amore%20efficient%20and%20accurate%20solution%20for%20practical%20document%20analysis%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Semi-Supervised%2520approach%2520with%2520Modulated%2520Object%2520Queries%2520for%250A%2520%2520Table%2520Detection%2520in%2520Documents%26entry.906535625%3DIqraa%2520Ehsan%2520and%2520Tahira%2520Shehzadi%2520and%2520Didier%2520Stricker%2520and%2520Muhammad%2520Zeshan%2520Afzal%26entry.1292438233%3D%2520%2520Table%2520detection%252C%2520a%2520pivotal%2520task%2520in%2520document%2520analysis%252C%2520aims%2520to%2520precisely%250Arecognize%2520and%2520locate%2520tables%2520within%2520document%2520images.%2520Although%2520deep%2520learning%2520has%250Ashown%2520remarkable%2520progress%2520in%2520this%2520realm%252C%2520it%2520typically%2520requires%2520an%2520extensive%250Adataset%2520of%2520labeled%2520data%2520for%2520proficient%2520training.%2520Current%2520CNN-based%250Asemi-supervised%2520table%2520detection%2520approaches%2520use%2520the%2520anchor%2520generation%2520process%250Aand%2520Non-Maximum%2520Suppression%2520%2528NMS%2529%2520in%2520their%2520detection%2520process%252C%2520limiting%2520training%250Aefficiency.%2520Meanwhile%252C%2520transformer-based%2520semi-supervised%2520techniques%2520adopted%2520a%250Aone-to-one%2520match%2520strategy%2520that%2520provides%2520noisy%2520pseudo-labels%252C%2520limiting%2520overall%250Aefficiency.%2520This%2520study%2520presents%2520an%2520innovative%2520transformer-based%2520semi-supervised%250Atable%2520detector.%2520It%2520improves%2520the%2520quality%2520of%2520pseudo-labels%2520through%2520a%2520novel%250Amatching%2520strategy%2520combining%2520one-to-one%2520and%2520one-to-many%2520assignment%2520techniques.%250AThis%2520approach%2520significantly%2520enhances%2520training%2520efficiency%2520during%2520the%2520early%250Astages%252C%2520ensuring%2520superior%2520pseudo-labels%2520for%2520further%2520training.%2520Our%250Asemi-supervised%2520approach%2520is%2520comprehensively%2520evaluated%2520on%2520benchmark%2520datasets%252C%250Aincluding%2520PubLayNet%252C%2520ICADR-19%252C%2520and%2520TableBank.%2520It%2520achieves%2520new%2520state-of-the-art%250Aresults%252C%2520with%2520a%2520mAP%2520of%252095.7%2525%2520and%252097.9%2525%2520on%2520TableBank%2520%2528word%2529%2520and%2520PubLaynet%2520with%250A30%2525%2520label%2520data%252C%2520marking%2520a%25207.4%2520and%25207.6%2520point%2520improvement%2520over%2520previous%250Asemi-supervised%2520table%2520detection%2520approach%252C%2520respectively.%2520The%2520results%2520clearly%250Ashow%2520the%2520superiority%2520of%2520our%2520semi-supervised%2520approach%252C%2520surpassing%2520all%2520existing%250Astate-of-the-art%2520methods%2520by%2520substantial%2520margins.%2520This%2520research%2520represents%2520a%250Asignificant%2520advancement%2520in%2520semi-supervised%2520table%2520detection%2520methods%252C%2520offering%2520a%250Amore%2520efficient%2520and%2520accurate%2520solution%2520for%2520practical%2520document%2520analysis%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Semi-Supervised%20approach%20with%20Modulated%20Object%20Queries%20for%0A%20%20Table%20Detection%20in%20Documents&entry.906535625=Iqraa%20Ehsan%20and%20Tahira%20Shehzadi%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal&entry.1292438233=%20%20Table%20detection%2C%20a%20pivotal%20task%20in%20document%20analysis%2C%20aims%20to%20precisely%0Arecognize%20and%20locate%20tables%20within%20document%20images.%20Although%20deep%20learning%20has%0Ashown%20remarkable%20progress%20in%20this%20realm%2C%20it%20typically%20requires%20an%20extensive%0Adataset%20of%20labeled%20data%20for%20proficient%20training.%20Current%20CNN-based%0Asemi-supervised%20table%20detection%20approaches%20use%20the%20anchor%20generation%20process%0Aand%20Non-Maximum%20Suppression%20%28NMS%29%20in%20their%20detection%20process%2C%20limiting%20training%0Aefficiency.%20Meanwhile%2C%20transformer-based%20semi-supervised%20techniques%20adopted%20a%0Aone-to-one%20match%20strategy%20that%20provides%20noisy%20pseudo-labels%2C%20limiting%20overall%0Aefficiency.%20This%20study%20presents%20an%20innovative%20transformer-based%20semi-supervised%0Atable%20detector.%20It%20improves%20the%20quality%20of%20pseudo-labels%20through%20a%20novel%0Amatching%20strategy%20combining%20one-to-one%20and%20one-to-many%20assignment%20techniques.%0AThis%20approach%20significantly%20enhances%20training%20efficiency%20during%20the%20early%0Astages%2C%20ensuring%20superior%20pseudo-labels%20for%20further%20training.%20Our%0Asemi-supervised%20approach%20is%20comprehensively%20evaluated%20on%20benchmark%20datasets%2C%0Aincluding%20PubLayNet%2C%20ICADR-19%2C%20and%20TableBank.%20It%20achieves%20new%20state-of-the-art%0Aresults%2C%20with%20a%20mAP%20of%2095.7%25%20and%2097.9%25%20on%20TableBank%20%28word%29%20and%20PubLaynet%20with%0A30%25%20label%20data%2C%20marking%20a%207.4%20and%207.6%20point%20improvement%20over%20previous%0Asemi-supervised%20table%20detection%20approach%2C%20respectively.%20The%20results%20clearly%0Ashow%20the%20superiority%20of%20our%20semi-supervised%20approach%2C%20surpassing%20all%20existing%0Astate-of-the-art%20methods%20by%20substantial%20margins.%20This%20research%20represents%20a%0Asignificant%20advancement%20in%20semi-supervised%20table%20detection%20methods%2C%20offering%20a%0Amore%20efficient%20and%20accurate%20solution%20for%20practical%20document%20analysis%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04971v1&entry.124074799=Read"},
{"title": "ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with\n  Probability Map Guided Multi-Format Feature Fusion", "author": "Bing Zhu and Zixin He and Weiyi Xiong and Guanhua Ding and Jianan Liu and Tao Huang and Wei Chen and Wei Xiang", "abstract": "  Millimetre wave (mmWave) radar is a non-intrusive privacy and relatively\nconvenient and inexpensive device, which has been demonstrated to be applicable\nin place of RGB cameras in human indoor pose estimation tasks. However, mmWave\nradar relies on the collection of reflected signals from the target, and the\nradar signals containing information is difficult to be fully applied. This has\nbeen a long-standing hindrance to the improvement of pose estimation accuracy.\nTo address this major challenge, this paper introduces a probability map guided\nmulti-format feature fusion model, ProbRadarM3F. This is a novel radar feature\nextraction framework using a traditional FFT method in parallel with a\nprobability map based positional encoding method. ProbRadarM3F fuses the\ntraditional heatmap features and the positional features, then effectively\nachieves the estimation of 14 keypoints of the human body. Experimental\nevaluation on the HuPR dataset proves the effectiveness of the model proposed\nin this paper, outperforming other methods experimented on this dataset with an\nAP of 69.9 %. The emphasis of our study is focusing on the position information\nthat is not exploited before in radar singal. This provides direction to\ninvestigate other potential non-redundant information from mmWave rader.\n", "link": "http://arxiv.org/abs/2405.05164v1", "date": "2024-05-08", "relevancy": 2.1144, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5258}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProbRadarM3F%3A%20mmWave%20Radar%20based%20Human%20Skeletal%20Pose%20Estimation%20with%0A%20%20Probability%20Map%20Guided%20Multi-Format%20Feature%20Fusion&body=Title%3A%20ProbRadarM3F%3A%20mmWave%20Radar%20based%20Human%20Skeletal%20Pose%20Estimation%20with%0A%20%20Probability%20Map%20Guided%20Multi-Format%20Feature%20Fusion%0AAuthor%3A%20Bing%20Zhu%20and%20Zixin%20He%20and%20Weiyi%20Xiong%20and%20Guanhua%20Ding%20and%20Jianan%20Liu%20and%20Tao%20Huang%20and%20Wei%20Chen%20and%20Wei%20Xiang%0AAbstract%3A%20%20%20Millimetre%20wave%20%28mmWave%29%20radar%20is%20a%20non-intrusive%20privacy%20and%20relatively%0Aconvenient%20and%20inexpensive%20device%2C%20which%20has%20been%20demonstrated%20to%20be%20applicable%0Ain%20place%20of%20RGB%20cameras%20in%20human%20indoor%20pose%20estimation%20tasks.%20However%2C%20mmWave%0Aradar%20relies%20on%20the%20collection%20of%20reflected%20signals%20from%20the%20target%2C%20and%20the%0Aradar%20signals%20containing%20information%20is%20difficult%20to%20be%20fully%20applied.%20This%20has%0Abeen%20a%20long-standing%20hindrance%20to%20the%20improvement%20of%20pose%20estimation%20accuracy.%0ATo%20address%20this%20major%20challenge%2C%20this%20paper%20introduces%20a%20probability%20map%20guided%0Amulti-format%20feature%20fusion%20model%2C%20ProbRadarM3F.%20This%20is%20a%20novel%20radar%20feature%0Aextraction%20framework%20using%20a%20traditional%20FFT%20method%20in%20parallel%20with%20a%0Aprobability%20map%20based%20positional%20encoding%20method.%20ProbRadarM3F%20fuses%20the%0Atraditional%20heatmap%20features%20and%20the%20positional%20features%2C%20then%20effectively%0Aachieves%20the%20estimation%20of%2014%20keypoints%20of%20the%20human%20body.%20Experimental%0Aevaluation%20on%20the%20HuPR%20dataset%20proves%20the%20effectiveness%20of%20the%20model%20proposed%0Ain%20this%20paper%2C%20outperforming%20other%20methods%20experimented%20on%20this%20dataset%20with%20an%0AAP%20of%2069.9%20%25.%20The%20emphasis%20of%20our%20study%20is%20focusing%20on%20the%20position%20information%0Athat%20is%20not%20exploited%20before%20in%20radar%20singal.%20This%20provides%20direction%20to%0Ainvestigate%20other%20potential%20non-redundant%20information%20from%20mmWave%20rader.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbRadarM3F%253A%2520mmWave%2520Radar%2520based%2520Human%2520Skeletal%2520Pose%2520Estimation%2520with%250A%2520%2520Probability%2520Map%2520Guided%2520Multi-Format%2520Feature%2520Fusion%26entry.906535625%3DBing%2520Zhu%2520and%2520Zixin%2520He%2520and%2520Weiyi%2520Xiong%2520and%2520Guanhua%2520Ding%2520and%2520Jianan%2520Liu%2520and%2520Tao%2520Huang%2520and%2520Wei%2520Chen%2520and%2520Wei%2520Xiang%26entry.1292438233%3D%2520%2520Millimetre%2520wave%2520%2528mmWave%2529%2520radar%2520is%2520a%2520non-intrusive%2520privacy%2520and%2520relatively%250Aconvenient%2520and%2520inexpensive%2520device%252C%2520which%2520has%2520been%2520demonstrated%2520to%2520be%2520applicable%250Ain%2520place%2520of%2520RGB%2520cameras%2520in%2520human%2520indoor%2520pose%2520estimation%2520tasks.%2520However%252C%2520mmWave%250Aradar%2520relies%2520on%2520the%2520collection%2520of%2520reflected%2520signals%2520from%2520the%2520target%252C%2520and%2520the%250Aradar%2520signals%2520containing%2520information%2520is%2520difficult%2520to%2520be%2520fully%2520applied.%2520This%2520has%250Abeen%2520a%2520long-standing%2520hindrance%2520to%2520the%2520improvement%2520of%2520pose%2520estimation%2520accuracy.%250ATo%2520address%2520this%2520major%2520challenge%252C%2520this%2520paper%2520introduces%2520a%2520probability%2520map%2520guided%250Amulti-format%2520feature%2520fusion%2520model%252C%2520ProbRadarM3F.%2520This%2520is%2520a%2520novel%2520radar%2520feature%250Aextraction%2520framework%2520using%2520a%2520traditional%2520FFT%2520method%2520in%2520parallel%2520with%2520a%250Aprobability%2520map%2520based%2520positional%2520encoding%2520method.%2520ProbRadarM3F%2520fuses%2520the%250Atraditional%2520heatmap%2520features%2520and%2520the%2520positional%2520features%252C%2520then%2520effectively%250Aachieves%2520the%2520estimation%2520of%252014%2520keypoints%2520of%2520the%2520human%2520body.%2520Experimental%250Aevaluation%2520on%2520the%2520HuPR%2520dataset%2520proves%2520the%2520effectiveness%2520of%2520the%2520model%2520proposed%250Ain%2520this%2520paper%252C%2520outperforming%2520other%2520methods%2520experimented%2520on%2520this%2520dataset%2520with%2520an%250AAP%2520of%252069.9%2520%2525.%2520The%2520emphasis%2520of%2520our%2520study%2520is%2520focusing%2520on%2520the%2520position%2520information%250Athat%2520is%2520not%2520exploited%2520before%2520in%2520radar%2520singal.%2520This%2520provides%2520direction%2520to%250Ainvestigate%2520other%2520potential%2520non-redundant%2520information%2520from%2520mmWave%2520rader.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProbRadarM3F%3A%20mmWave%20Radar%20based%20Human%20Skeletal%20Pose%20Estimation%20with%0A%20%20Probability%20Map%20Guided%20Multi-Format%20Feature%20Fusion&entry.906535625=Bing%20Zhu%20and%20Zixin%20He%20and%20Weiyi%20Xiong%20and%20Guanhua%20Ding%20and%20Jianan%20Liu%20and%20Tao%20Huang%20and%20Wei%20Chen%20and%20Wei%20Xiang&entry.1292438233=%20%20Millimetre%20wave%20%28mmWave%29%20radar%20is%20a%20non-intrusive%20privacy%20and%20relatively%0Aconvenient%20and%20inexpensive%20device%2C%20which%20has%20been%20demonstrated%20to%20be%20applicable%0Ain%20place%20of%20RGB%20cameras%20in%20human%20indoor%20pose%20estimation%20tasks.%20However%2C%20mmWave%0Aradar%20relies%20on%20the%20collection%20of%20reflected%20signals%20from%20the%20target%2C%20and%20the%0Aradar%20signals%20containing%20information%20is%20difficult%20to%20be%20fully%20applied.%20This%20has%0Abeen%20a%20long-standing%20hindrance%20to%20the%20improvement%20of%20pose%20estimation%20accuracy.%0ATo%20address%20this%20major%20challenge%2C%20this%20paper%20introduces%20a%20probability%20map%20guided%0Amulti-format%20feature%20fusion%20model%2C%20ProbRadarM3F.%20This%20is%20a%20novel%20radar%20feature%0Aextraction%20framework%20using%20a%20traditional%20FFT%20method%20in%20parallel%20with%20a%0Aprobability%20map%20based%20positional%20encoding%20method.%20ProbRadarM3F%20fuses%20the%0Atraditional%20heatmap%20features%20and%20the%20positional%20features%2C%20then%20effectively%0Aachieves%20the%20estimation%20of%2014%20keypoints%20of%20the%20human%20body.%20Experimental%0Aevaluation%20on%20the%20HuPR%20dataset%20proves%20the%20effectiveness%20of%20the%20model%20proposed%0Ain%20this%20paper%2C%20outperforming%20other%20methods%20experimented%20on%20this%20dataset%20with%20an%0AAP%20of%2069.9%20%25.%20The%20emphasis%20of%20our%20study%20is%20focusing%20on%20the%20position%20information%0Athat%20is%20not%20exploited%20before%20in%20radar%20singal.%20This%20provides%20direction%20to%0Ainvestigate%20other%20potential%20non-redundant%20information%20from%20mmWave%20rader.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05164v1&entry.124074799=Read"},
{"title": "Hundred-Kilobyte Lookup Tables for Efficient Single-Image\n  Super-Resolution", "author": "Binxiao Huang and Jason Chun Lok Li and Jie Ran and Boyu Li and Jiajun Zhou and Dahai Yu and Ngai Wong", "abstract": "  Conventional super-resolution (SR) schemes make heavy use of convolutional\nneural networks (CNNs), which involve intensive multiply-accumulate (MAC)\noperations, and require specialized hardware such as graphics processing units.\nThis contradicts the regime of edge AI that often runs on devices strained by\npower, computing, and storage resources. Such a challenge has motivated a\nseries of lookup table (LUT)-based SR schemes that employ simple LUT readout\nand largely elude CNN computation. Nonetheless, the multi-megabyte LUTs in\nexisting methods still prohibit on-chip storage and necessitate off-chip memory\ntransport. This work tackles this storage hurdle and innovates hundred-kilobyte\nLUT (HKLUT) models amenable to on-chip cache. Utilizing an asymmetric\ntwo-branch multistage network coupled with a suite of specialized kernel\npatterns, HKLUT demonstrates an uncompromising performance and superior\nhardware efficiency over existing LUT schemes. Our implementation is publicly\navailable at: https://github.com/jasonli0707/hklut.\n", "link": "http://arxiv.org/abs/2312.06101v2", "date": "2024-05-08", "relevancy": 2.1099, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5359}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5272}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hundred-Kilobyte%20Lookup%20Tables%20for%20Efficient%20Single-Image%0A%20%20Super-Resolution&body=Title%3A%20Hundred-Kilobyte%20Lookup%20Tables%20for%20Efficient%20Single-Image%0A%20%20Super-Resolution%0AAuthor%3A%20Binxiao%20Huang%20and%20Jason%20Chun%20Lok%20Li%20and%20Jie%20Ran%20and%20Boyu%20Li%20and%20Jiajun%20Zhou%20and%20Dahai%20Yu%20and%20Ngai%20Wong%0AAbstract%3A%20%20%20Conventional%20super-resolution%20%28SR%29%20schemes%20make%20heavy%20use%20of%20convolutional%0Aneural%20networks%20%28CNNs%29%2C%20which%20involve%20intensive%20multiply-accumulate%20%28MAC%29%0Aoperations%2C%20and%20require%20specialized%20hardware%20such%20as%20graphics%20processing%20units.%0AThis%20contradicts%20the%20regime%20of%20edge%20AI%20that%20often%20runs%20on%20devices%20strained%20by%0Apower%2C%20computing%2C%20and%20storage%20resources.%20Such%20a%20challenge%20has%20motivated%20a%0Aseries%20of%20lookup%20table%20%28LUT%29-based%20SR%20schemes%20that%20employ%20simple%20LUT%20readout%0Aand%20largely%20elude%20CNN%20computation.%20Nonetheless%2C%20the%20multi-megabyte%20LUTs%20in%0Aexisting%20methods%20still%20prohibit%20on-chip%20storage%20and%20necessitate%20off-chip%20memory%0Atransport.%20This%20work%20tackles%20this%20storage%20hurdle%20and%20innovates%20hundred-kilobyte%0ALUT%20%28HKLUT%29%20models%20amenable%20to%20on-chip%20cache.%20Utilizing%20an%20asymmetric%0Atwo-branch%20multistage%20network%20coupled%20with%20a%20suite%20of%20specialized%20kernel%0Apatterns%2C%20HKLUT%20demonstrates%20an%20uncompromising%20performance%20and%20superior%0Ahardware%20efficiency%20over%20existing%20LUT%20schemes.%20Our%20implementation%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/jasonli0707/hklut.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHundred-Kilobyte%2520Lookup%2520Tables%2520for%2520Efficient%2520Single-Image%250A%2520%2520Super-Resolution%26entry.906535625%3DBinxiao%2520Huang%2520and%2520Jason%2520Chun%2520Lok%2520Li%2520and%2520Jie%2520Ran%2520and%2520Boyu%2520Li%2520and%2520Jiajun%2520Zhou%2520and%2520Dahai%2520Yu%2520and%2520Ngai%2520Wong%26entry.1292438233%3D%2520%2520Conventional%2520super-resolution%2520%2528SR%2529%2520schemes%2520make%2520heavy%2520use%2520of%2520convolutional%250Aneural%2520networks%2520%2528CNNs%2529%252C%2520which%2520involve%2520intensive%2520multiply-accumulate%2520%2528MAC%2529%250Aoperations%252C%2520and%2520require%2520specialized%2520hardware%2520such%2520as%2520graphics%2520processing%2520units.%250AThis%2520contradicts%2520the%2520regime%2520of%2520edge%2520AI%2520that%2520often%2520runs%2520on%2520devices%2520strained%2520by%250Apower%252C%2520computing%252C%2520and%2520storage%2520resources.%2520Such%2520a%2520challenge%2520has%2520motivated%2520a%250Aseries%2520of%2520lookup%2520table%2520%2528LUT%2529-based%2520SR%2520schemes%2520that%2520employ%2520simple%2520LUT%2520readout%250Aand%2520largely%2520elude%2520CNN%2520computation.%2520Nonetheless%252C%2520the%2520multi-megabyte%2520LUTs%2520in%250Aexisting%2520methods%2520still%2520prohibit%2520on-chip%2520storage%2520and%2520necessitate%2520off-chip%2520memory%250Atransport.%2520This%2520work%2520tackles%2520this%2520storage%2520hurdle%2520and%2520innovates%2520hundred-kilobyte%250ALUT%2520%2528HKLUT%2529%2520models%2520amenable%2520to%2520on-chip%2520cache.%2520Utilizing%2520an%2520asymmetric%250Atwo-branch%2520multistage%2520network%2520coupled%2520with%2520a%2520suite%2520of%2520specialized%2520kernel%250Apatterns%252C%2520HKLUT%2520demonstrates%2520an%2520uncompromising%2520performance%2520and%2520superior%250Ahardware%2520efficiency%2520over%2520existing%2520LUT%2520schemes.%2520Our%2520implementation%2520is%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/jasonli0707/hklut.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hundred-Kilobyte%20Lookup%20Tables%20for%20Efficient%20Single-Image%0A%20%20Super-Resolution&entry.906535625=Binxiao%20Huang%20and%20Jason%20Chun%20Lok%20Li%20and%20Jie%20Ran%20and%20Boyu%20Li%20and%20Jiajun%20Zhou%20and%20Dahai%20Yu%20and%20Ngai%20Wong&entry.1292438233=%20%20Conventional%20super-resolution%20%28SR%29%20schemes%20make%20heavy%20use%20of%20convolutional%0Aneural%20networks%20%28CNNs%29%2C%20which%20involve%20intensive%20multiply-accumulate%20%28MAC%29%0Aoperations%2C%20and%20require%20specialized%20hardware%20such%20as%20graphics%20processing%20units.%0AThis%20contradicts%20the%20regime%20of%20edge%20AI%20that%20often%20runs%20on%20devices%20strained%20by%0Apower%2C%20computing%2C%20and%20storage%20resources.%20Such%20a%20challenge%20has%20motivated%20a%0Aseries%20of%20lookup%20table%20%28LUT%29-based%20SR%20schemes%20that%20employ%20simple%20LUT%20readout%0Aand%20largely%20elude%20CNN%20computation.%20Nonetheless%2C%20the%20multi-megabyte%20LUTs%20in%0Aexisting%20methods%20still%20prohibit%20on-chip%20storage%20and%20necessitate%20off-chip%20memory%0Atransport.%20This%20work%20tackles%20this%20storage%20hurdle%20and%20innovates%20hundred-kilobyte%0ALUT%20%28HKLUT%29%20models%20amenable%20to%20on-chip%20cache.%20Utilizing%20an%20asymmetric%0Atwo-branch%20multistage%20network%20coupled%20with%20a%20suite%20of%20specialized%20kernel%0Apatterns%2C%20HKLUT%20demonstrates%20an%20uncompromising%20performance%20and%20superior%0Ahardware%20efficiency%20over%20existing%20LUT%20schemes.%20Our%20implementation%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/jasonli0707/hklut.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06101v2&entry.124074799=Read"},
{"title": "HEAL-SWIN: A Vision Transformer On The Sphere", "author": "Oscar Carlsson and Jan E. Gerken and Hampus Linander and Heiner Spie\u00df and Fredrik Ohlsson and Christoffer Petersson and Daniel Persson", "abstract": "  High-resolution wide-angle fisheye images are becoming more and more\nimportant for robotics applications such as autonomous driving. However, using\nordinary convolutional neural networks or vision transformers on this data is\nproblematic due to projection and distortion losses introduced when projecting\nto a rectangular grid on the plane. We introduce the HEAL-SWIN transformer,\nwhich combines the highly uniform Hierarchical Equal Area iso-Latitude\nPixelation (HEALPix) grid used in astrophysics and cosmology with the\nHierarchical Shifted-Window (SWIN) transformer to yield an efficient and\nflexible model capable of training on high-resolution, distortion-free\nspherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used\nto perform the patching and windowing operations of the SWIN transformer,\nenabling the network to process spherical representations with minimal\ncomputational overhead. We demonstrate the superior performance of our model on\nboth synthetic and real automotive datasets, as well as a selection of other\nimage datasets, for semantic segmentation, depth regression and classification\ntasks. Our code is publicly available at\nhttps://github.com/JanEGerken/HEAL-SWIN.\n", "link": "http://arxiv.org/abs/2307.07313v2", "date": "2024-05-08", "relevancy": 2.1077, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.545}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5328}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEAL-SWIN%3A%20A%20Vision%20Transformer%20On%20The%20Sphere&body=Title%3A%20HEAL-SWIN%3A%20A%20Vision%20Transformer%20On%20The%20Sphere%0AAuthor%3A%20Oscar%20Carlsson%20and%20Jan%20E.%20Gerken%20and%20Hampus%20Linander%20and%20Heiner%20Spie%C3%9F%20and%20Fredrik%20Ohlsson%20and%20Christoffer%20Petersson%20and%20Daniel%20Persson%0AAbstract%3A%20%20%20High-resolution%20wide-angle%20fisheye%20images%20are%20becoming%20more%20and%20more%0Aimportant%20for%20robotics%20applications%20such%20as%20autonomous%20driving.%20However%2C%20using%0Aordinary%20convolutional%20neural%20networks%20or%20vision%20transformers%20on%20this%20data%20is%0Aproblematic%20due%20to%20projection%20and%20distortion%20losses%20introduced%20when%20projecting%0Ato%20a%20rectangular%20grid%20on%20the%20plane.%20We%20introduce%20the%20HEAL-SWIN%20transformer%2C%0Awhich%20combines%20the%20highly%20uniform%20Hierarchical%20Equal%20Area%20iso-Latitude%0APixelation%20%28HEALPix%29%20grid%20used%20in%20astrophysics%20and%20cosmology%20with%20the%0AHierarchical%20Shifted-Window%20%28SWIN%29%20transformer%20to%20yield%20an%20efficient%20and%0Aflexible%20model%20capable%20of%20training%20on%20high-resolution%2C%20distortion-free%0Aspherical%20data.%20In%20HEAL-SWIN%2C%20the%20nested%20structure%20of%20the%20HEALPix%20grid%20is%20used%0Ato%20perform%20the%20patching%20and%20windowing%20operations%20of%20the%20SWIN%20transformer%2C%0Aenabling%20the%20network%20to%20process%20spherical%20representations%20with%20minimal%0Acomputational%20overhead.%20We%20demonstrate%20the%20superior%20performance%20of%20our%20model%20on%0Aboth%20synthetic%20and%20real%20automotive%20datasets%2C%20as%20well%20as%20a%20selection%20of%20other%0Aimage%20datasets%2C%20for%20semantic%20segmentation%2C%20depth%20regression%20and%20classification%0Atasks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/JanEGerken/HEAL-SWIN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.07313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEAL-SWIN%253A%2520A%2520Vision%2520Transformer%2520On%2520The%2520Sphere%26entry.906535625%3DOscar%2520Carlsson%2520and%2520Jan%2520E.%2520Gerken%2520and%2520Hampus%2520Linander%2520and%2520Heiner%2520Spie%25C3%259F%2520and%2520Fredrik%2520Ohlsson%2520and%2520Christoffer%2520Petersson%2520and%2520Daniel%2520Persson%26entry.1292438233%3D%2520%2520High-resolution%2520wide-angle%2520fisheye%2520images%2520are%2520becoming%2520more%2520and%2520more%250Aimportant%2520for%2520robotics%2520applications%2520such%2520as%2520autonomous%2520driving.%2520However%252C%2520using%250Aordinary%2520convolutional%2520neural%2520networks%2520or%2520vision%2520transformers%2520on%2520this%2520data%2520is%250Aproblematic%2520due%2520to%2520projection%2520and%2520distortion%2520losses%2520introduced%2520when%2520projecting%250Ato%2520a%2520rectangular%2520grid%2520on%2520the%2520plane.%2520We%2520introduce%2520the%2520HEAL-SWIN%2520transformer%252C%250Awhich%2520combines%2520the%2520highly%2520uniform%2520Hierarchical%2520Equal%2520Area%2520iso-Latitude%250APixelation%2520%2528HEALPix%2529%2520grid%2520used%2520in%2520astrophysics%2520and%2520cosmology%2520with%2520the%250AHierarchical%2520Shifted-Window%2520%2528SWIN%2529%2520transformer%2520to%2520yield%2520an%2520efficient%2520and%250Aflexible%2520model%2520capable%2520of%2520training%2520on%2520high-resolution%252C%2520distortion-free%250Aspherical%2520data.%2520In%2520HEAL-SWIN%252C%2520the%2520nested%2520structure%2520of%2520the%2520HEALPix%2520grid%2520is%2520used%250Ato%2520perform%2520the%2520patching%2520and%2520windowing%2520operations%2520of%2520the%2520SWIN%2520transformer%252C%250Aenabling%2520the%2520network%2520to%2520process%2520spherical%2520representations%2520with%2520minimal%250Acomputational%2520overhead.%2520We%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520model%2520on%250Aboth%2520synthetic%2520and%2520real%2520automotive%2520datasets%252C%2520as%2520well%2520as%2520a%2520selection%2520of%2520other%250Aimage%2520datasets%252C%2520for%2520semantic%2520segmentation%252C%2520depth%2520regression%2520and%2520classification%250Atasks.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/JanEGerken/HEAL-SWIN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.07313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEAL-SWIN%3A%20A%20Vision%20Transformer%20On%20The%20Sphere&entry.906535625=Oscar%20Carlsson%20and%20Jan%20E.%20Gerken%20and%20Hampus%20Linander%20and%20Heiner%20Spie%C3%9F%20and%20Fredrik%20Ohlsson%20and%20Christoffer%20Petersson%20and%20Daniel%20Persson&entry.1292438233=%20%20High-resolution%20wide-angle%20fisheye%20images%20are%20becoming%20more%20and%20more%0Aimportant%20for%20robotics%20applications%20such%20as%20autonomous%20driving.%20However%2C%20using%0Aordinary%20convolutional%20neural%20networks%20or%20vision%20transformers%20on%20this%20data%20is%0Aproblematic%20due%20to%20projection%20and%20distortion%20losses%20introduced%20when%20projecting%0Ato%20a%20rectangular%20grid%20on%20the%20plane.%20We%20introduce%20the%20HEAL-SWIN%20transformer%2C%0Awhich%20combines%20the%20highly%20uniform%20Hierarchical%20Equal%20Area%20iso-Latitude%0APixelation%20%28HEALPix%29%20grid%20used%20in%20astrophysics%20and%20cosmology%20with%20the%0AHierarchical%20Shifted-Window%20%28SWIN%29%20transformer%20to%20yield%20an%20efficient%20and%0Aflexible%20model%20capable%20of%20training%20on%20high-resolution%2C%20distortion-free%0Aspherical%20data.%20In%20HEAL-SWIN%2C%20the%20nested%20structure%20of%20the%20HEALPix%20grid%20is%20used%0Ato%20perform%20the%20patching%20and%20windowing%20operations%20of%20the%20SWIN%20transformer%2C%0Aenabling%20the%20network%20to%20process%20spherical%20representations%20with%20minimal%0Acomputational%20overhead.%20We%20demonstrate%20the%20superior%20performance%20of%20our%20model%20on%0Aboth%20synthetic%20and%20real%20automotive%20datasets%2C%20as%20well%20as%20a%20selection%20of%20other%0Aimage%20datasets%2C%20for%20semantic%20segmentation%2C%20depth%20regression%20and%20classification%0Atasks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/JanEGerken/HEAL-SWIN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.07313v2&entry.124074799=Read"},
{"title": "Variational Self-Supervised Contrastive Learning Using Beta Divergence", "author": "Mehmet Can Yavuz and Berrin Yanikoglu", "abstract": "  Learning a discriminative semantic space using unlabelled and noisy data\nremains unaddressed in a multi-label setting. We present a contrastive\nself-supervised learning method which is robust to data noise, grounded in the\ndomain of variational methods. The method (VCL) utilizes variational\ncontrastive learning with beta-divergence to learn robustly from unlabelled\ndatasets, including uncurated and noisy datasets. We demonstrate the\neffectiveness of the proposed method through rigorous experiments including\nlinear evaluation and fine-tuning scenarios with multi-label datasets in the\nface understanding domain. In almost all tested scenarios, VCL surpasses the\nperformance of state-of-the-art self-supervised methods, achieving a noteworthy\nincrease in accuracy.\n", "link": "http://arxiv.org/abs/2312.00824v3", "date": "2024-05-08", "relevancy": 2.0999, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5309}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.521}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Self-Supervised%20Contrastive%20Learning%20Using%20Beta%20Divergence&body=Title%3A%20Variational%20Self-Supervised%20Contrastive%20Learning%20Using%20Beta%20Divergence%0AAuthor%3A%20Mehmet%20Can%20Yavuz%20and%20Berrin%20Yanikoglu%0AAbstract%3A%20%20%20Learning%20a%20discriminative%20semantic%20space%20using%20unlabelled%20and%20noisy%20data%0Aremains%20unaddressed%20in%20a%20multi-label%20setting.%20We%20present%20a%20contrastive%0Aself-supervised%20learning%20method%20which%20is%20robust%20to%20data%20noise%2C%20grounded%20in%20the%0Adomain%20of%20variational%20methods.%20The%20method%20%28VCL%29%20utilizes%20variational%0Acontrastive%20learning%20with%20beta-divergence%20to%20learn%20robustly%20from%20unlabelled%0Adatasets%2C%20including%20uncurated%20and%20noisy%20datasets.%20We%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20through%20rigorous%20experiments%20including%0Alinear%20evaluation%20and%20fine-tuning%20scenarios%20with%20multi-label%20datasets%20in%20the%0Aface%20understanding%20domain.%20In%20almost%20all%20tested%20scenarios%2C%20VCL%20surpasses%20the%0Aperformance%20of%20state-of-the-art%20self-supervised%20methods%2C%20achieving%20a%20noteworthy%0Aincrease%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00824v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Self-Supervised%2520Contrastive%2520Learning%2520Using%2520Beta%2520Divergence%26entry.906535625%3DMehmet%2520Can%2520Yavuz%2520and%2520Berrin%2520Yanikoglu%26entry.1292438233%3D%2520%2520Learning%2520a%2520discriminative%2520semantic%2520space%2520using%2520unlabelled%2520and%2520noisy%2520data%250Aremains%2520unaddressed%2520in%2520a%2520multi-label%2520setting.%2520We%2520present%2520a%2520contrastive%250Aself-supervised%2520learning%2520method%2520which%2520is%2520robust%2520to%2520data%2520noise%252C%2520grounded%2520in%2520the%250Adomain%2520of%2520variational%2520methods.%2520The%2520method%2520%2528VCL%2529%2520utilizes%2520variational%250Acontrastive%2520learning%2520with%2520beta-divergence%2520to%2520learn%2520robustly%2520from%2520unlabelled%250Adatasets%252C%2520including%2520uncurated%2520and%2520noisy%2520datasets.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520through%2520rigorous%2520experiments%2520including%250Alinear%2520evaluation%2520and%2520fine-tuning%2520scenarios%2520with%2520multi-label%2520datasets%2520in%2520the%250Aface%2520understanding%2520domain.%2520In%2520almost%2520all%2520tested%2520scenarios%252C%2520VCL%2520surpasses%2520the%250Aperformance%2520of%2520state-of-the-art%2520self-supervised%2520methods%252C%2520achieving%2520a%2520noteworthy%250Aincrease%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00824v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Self-Supervised%20Contrastive%20Learning%20Using%20Beta%20Divergence&entry.906535625=Mehmet%20Can%20Yavuz%20and%20Berrin%20Yanikoglu&entry.1292438233=%20%20Learning%20a%20discriminative%20semantic%20space%20using%20unlabelled%20and%20noisy%20data%0Aremains%20unaddressed%20in%20a%20multi-label%20setting.%20We%20present%20a%20contrastive%0Aself-supervised%20learning%20method%20which%20is%20robust%20to%20data%20noise%2C%20grounded%20in%20the%0Adomain%20of%20variational%20methods.%20The%20method%20%28VCL%29%20utilizes%20variational%0Acontrastive%20learning%20with%20beta-divergence%20to%20learn%20robustly%20from%20unlabelled%0Adatasets%2C%20including%20uncurated%20and%20noisy%20datasets.%20We%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20through%20rigorous%20experiments%20including%0Alinear%20evaluation%20and%20fine-tuning%20scenarios%20with%20multi-label%20datasets%20in%20the%0Aface%20understanding%20domain.%20In%20almost%20all%20tested%20scenarios%2C%20VCL%20surpasses%20the%0Aperformance%20of%20state-of-the-art%20self-supervised%20methods%2C%20achieving%20a%20noteworthy%0Aincrease%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00824v3&entry.124074799=Read"},
{"title": "LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous\n  Navigation in Agriculture Fields", "author": "Ruiji Liu and Francisco Yandun and George Kantor", "abstract": "  Autonomous navigation is crucial for various robotics applications in\nagriculture. However, many existing methods depend on RTK-GPS systems, which\nare expensive and susceptible to poor signal coverage. This paper introduces a\nstate-of-the-art LiDAR-based navigation system that can achieve over-canopy\nautonomous navigation in row-crop fields, even when the canopy fully blocks the\ninterrow spacing. Our crop row detection algorithm can detect crop rows across\ndiverse scenarios, encompassing various crop types, growth stages, weeds\npresence, and discontinuities within the crop rows. Without utilizing the\nglobal localization of the robot, our navigation system can perform autonomous\nnavigation in these challenging scenarios, detect the end of the crop rows, and\nnavigate to the next crop row autonomously, providing a crop-agnostic approach\nto navigate the whole row-crop field. This navigation system has undergone\ntests in various simulated agricultural fields, achieving an average of 2.98cm\nautonomous driving accuracy without human intervention on the custom Amiga\nrobot. In addition, the qualitative results of our crop row detection algorithm\nfrom the actual soybean fields validate our LiDAR-based crop row detection\nalgorithm's potential for practical agricultural applications.\n", "link": "http://arxiv.org/abs/2403.17774v2", "date": "2024-05-08", "relevancy": 2.0998, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5421}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5198}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-Based%20Crop%20Row%20Detection%20Algorithm%20for%20Over-Canopy%20Autonomous%0A%20%20Navigation%20in%20Agriculture%20Fields&body=Title%3A%20LiDAR-Based%20Crop%20Row%20Detection%20Algorithm%20for%20Over-Canopy%20Autonomous%0A%20%20Navigation%20in%20Agriculture%20Fields%0AAuthor%3A%20Ruiji%20Liu%20and%20Francisco%20Yandun%20and%20George%20Kantor%0AAbstract%3A%20%20%20Autonomous%20navigation%20is%20crucial%20for%20various%20robotics%20applications%20in%0Aagriculture.%20However%2C%20many%20existing%20methods%20depend%20on%20RTK-GPS%20systems%2C%20which%0Aare%20expensive%20and%20susceptible%20to%20poor%20signal%20coverage.%20This%20paper%20introduces%20a%0Astate-of-the-art%20LiDAR-based%20navigation%20system%20that%20can%20achieve%20over-canopy%0Aautonomous%20navigation%20in%20row-crop%20fields%2C%20even%20when%20the%20canopy%20fully%20blocks%20the%0Ainterrow%20spacing.%20Our%20crop%20row%20detection%20algorithm%20can%20detect%20crop%20rows%20across%0Adiverse%20scenarios%2C%20encompassing%20various%20crop%20types%2C%20growth%20stages%2C%20weeds%0Apresence%2C%20and%20discontinuities%20within%20the%20crop%20rows.%20Without%20utilizing%20the%0Aglobal%20localization%20of%20the%20robot%2C%20our%20navigation%20system%20can%20perform%20autonomous%0Anavigation%20in%20these%20challenging%20scenarios%2C%20detect%20the%20end%20of%20the%20crop%20rows%2C%20and%0Anavigate%20to%20the%20next%20crop%20row%20autonomously%2C%20providing%20a%20crop-agnostic%20approach%0Ato%20navigate%20the%20whole%20row-crop%20field.%20This%20navigation%20system%20has%20undergone%0Atests%20in%20various%20simulated%20agricultural%20fields%2C%20achieving%20an%20average%20of%202.98cm%0Aautonomous%20driving%20accuracy%20without%20human%20intervention%20on%20the%20custom%20Amiga%0Arobot.%20In%20addition%2C%20the%20qualitative%20results%20of%20our%20crop%20row%20detection%20algorithm%0Afrom%20the%20actual%20soybean%20fields%20validate%20our%20LiDAR-based%20crop%20row%20detection%0Aalgorithm%27s%20potential%20for%20practical%20agricultural%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-Based%2520Crop%2520Row%2520Detection%2520Algorithm%2520for%2520Over-Canopy%2520Autonomous%250A%2520%2520Navigation%2520in%2520Agriculture%2520Fields%26entry.906535625%3DRuiji%2520Liu%2520and%2520Francisco%2520Yandun%2520and%2520George%2520Kantor%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520is%2520crucial%2520for%2520various%2520robotics%2520applications%2520in%250Aagriculture.%2520However%252C%2520many%2520existing%2520methods%2520depend%2520on%2520RTK-GPS%2520systems%252C%2520which%250Aare%2520expensive%2520and%2520susceptible%2520to%2520poor%2520signal%2520coverage.%2520This%2520paper%2520introduces%2520a%250Astate-of-the-art%2520LiDAR-based%2520navigation%2520system%2520that%2520can%2520achieve%2520over-canopy%250Aautonomous%2520navigation%2520in%2520row-crop%2520fields%252C%2520even%2520when%2520the%2520canopy%2520fully%2520blocks%2520the%250Ainterrow%2520spacing.%2520Our%2520crop%2520row%2520detection%2520algorithm%2520can%2520detect%2520crop%2520rows%2520across%250Adiverse%2520scenarios%252C%2520encompassing%2520various%2520crop%2520types%252C%2520growth%2520stages%252C%2520weeds%250Apresence%252C%2520and%2520discontinuities%2520within%2520the%2520crop%2520rows.%2520Without%2520utilizing%2520the%250Aglobal%2520localization%2520of%2520the%2520robot%252C%2520our%2520navigation%2520system%2520can%2520perform%2520autonomous%250Anavigation%2520in%2520these%2520challenging%2520scenarios%252C%2520detect%2520the%2520end%2520of%2520the%2520crop%2520rows%252C%2520and%250Anavigate%2520to%2520the%2520next%2520crop%2520row%2520autonomously%252C%2520providing%2520a%2520crop-agnostic%2520approach%250Ato%2520navigate%2520the%2520whole%2520row-crop%2520field.%2520This%2520navigation%2520system%2520has%2520undergone%250Atests%2520in%2520various%2520simulated%2520agricultural%2520fields%252C%2520achieving%2520an%2520average%2520of%25202.98cm%250Aautonomous%2520driving%2520accuracy%2520without%2520human%2520intervention%2520on%2520the%2520custom%2520Amiga%250Arobot.%2520In%2520addition%252C%2520the%2520qualitative%2520results%2520of%2520our%2520crop%2520row%2520detection%2520algorithm%250Afrom%2520the%2520actual%2520soybean%2520fields%2520validate%2520our%2520LiDAR-based%2520crop%2520row%2520detection%250Aalgorithm%2527s%2520potential%2520for%2520practical%2520agricultural%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-Based%20Crop%20Row%20Detection%20Algorithm%20for%20Over-Canopy%20Autonomous%0A%20%20Navigation%20in%20Agriculture%20Fields&entry.906535625=Ruiji%20Liu%20and%20Francisco%20Yandun%20and%20George%20Kantor&entry.1292438233=%20%20Autonomous%20navigation%20is%20crucial%20for%20various%20robotics%20applications%20in%0Aagriculture.%20However%2C%20many%20existing%20methods%20depend%20on%20RTK-GPS%20systems%2C%20which%0Aare%20expensive%20and%20susceptible%20to%20poor%20signal%20coverage.%20This%20paper%20introduces%20a%0Astate-of-the-art%20LiDAR-based%20navigation%20system%20that%20can%20achieve%20over-canopy%0Aautonomous%20navigation%20in%20row-crop%20fields%2C%20even%20when%20the%20canopy%20fully%20blocks%20the%0Ainterrow%20spacing.%20Our%20crop%20row%20detection%20algorithm%20can%20detect%20crop%20rows%20across%0Adiverse%20scenarios%2C%20encompassing%20various%20crop%20types%2C%20growth%20stages%2C%20weeds%0Apresence%2C%20and%20discontinuities%20within%20the%20crop%20rows.%20Without%20utilizing%20the%0Aglobal%20localization%20of%20the%20robot%2C%20our%20navigation%20system%20can%20perform%20autonomous%0Anavigation%20in%20these%20challenging%20scenarios%2C%20detect%20the%20end%20of%20the%20crop%20rows%2C%20and%0Anavigate%20to%20the%20next%20crop%20row%20autonomously%2C%20providing%20a%20crop-agnostic%20approach%0Ato%20navigate%20the%20whole%20row-crop%20field.%20This%20navigation%20system%20has%20undergone%0Atests%20in%20various%20simulated%20agricultural%20fields%2C%20achieving%20an%20average%20of%202.98cm%0Aautonomous%20driving%20accuracy%20without%20human%20intervention%20on%20the%20custom%20Amiga%0Arobot.%20In%20addition%2C%20the%20qualitative%20results%20of%20our%20crop%20row%20detection%20algorithm%0Afrom%20the%20actual%20soybean%20fields%20validate%20our%20LiDAR-based%20crop%20row%20detection%0Aalgorithm%27s%20potential%20for%20practical%20agricultural%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17774v2&entry.124074799=Read"},
{"title": "An LSTM-Based Chord Generation System Using Chroma Histogram\n  Representations", "author": "Jack Hardwick", "abstract": "  This paper proposes a system for chord generation to monophonic symbolic\nmelodies using an LSTM-based model trained on chroma histogram representations\nof chords. Chroma representations promise more harmonically rich generation\nthan chord label-based approaches, whilst maintaining a small number of\ndimensions in the dataset. This system is shown to be suitable for limited\nreal-time use. While it does not meet the state-of-the-art for coherent\nlong-term generation, it does show diatonic generation with cadential chord\nrelationships. The need for further study into chroma histograms as an\nextracted feature in chord generation tasks is highlighted.\n", "link": "http://arxiv.org/abs/2405.05240v1", "date": "2024-05-08", "relevancy": 2.0868, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4237}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4162}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20LSTM-Based%20Chord%20Generation%20System%20Using%20Chroma%20Histogram%0A%20%20Representations&body=Title%3A%20An%20LSTM-Based%20Chord%20Generation%20System%20Using%20Chroma%20Histogram%0A%20%20Representations%0AAuthor%3A%20Jack%20Hardwick%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20system%20for%20chord%20generation%20to%20monophonic%20symbolic%0Amelodies%20using%20an%20LSTM-based%20model%20trained%20on%20chroma%20histogram%20representations%0Aof%20chords.%20Chroma%20representations%20promise%20more%20harmonically%20rich%20generation%0Athan%20chord%20label-based%20approaches%2C%20whilst%20maintaining%20a%20small%20number%20of%0Adimensions%20in%20the%20dataset.%20This%20system%20is%20shown%20to%20be%20suitable%20for%20limited%0Areal-time%20use.%20While%20it%20does%20not%20meet%20the%20state-of-the-art%20for%20coherent%0Along-term%20generation%2C%20it%20does%20show%20diatonic%20generation%20with%20cadential%20chord%0Arelationships.%20The%20need%20for%20further%20study%20into%20chroma%20histograms%20as%20an%0Aextracted%20feature%20in%20chord%20generation%20tasks%20is%20highlighted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520LSTM-Based%2520Chord%2520Generation%2520System%2520Using%2520Chroma%2520Histogram%250A%2520%2520Representations%26entry.906535625%3DJack%2520Hardwick%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520system%2520for%2520chord%2520generation%2520to%2520monophonic%2520symbolic%250Amelodies%2520using%2520an%2520LSTM-based%2520model%2520trained%2520on%2520chroma%2520histogram%2520representations%250Aof%2520chords.%2520Chroma%2520representations%2520promise%2520more%2520harmonically%2520rich%2520generation%250Athan%2520chord%2520label-based%2520approaches%252C%2520whilst%2520maintaining%2520a%2520small%2520number%2520of%250Adimensions%2520in%2520the%2520dataset.%2520This%2520system%2520is%2520shown%2520to%2520be%2520suitable%2520for%2520limited%250Areal-time%2520use.%2520While%2520it%2520does%2520not%2520meet%2520the%2520state-of-the-art%2520for%2520coherent%250Along-term%2520generation%252C%2520it%2520does%2520show%2520diatonic%2520generation%2520with%2520cadential%2520chord%250Arelationships.%2520The%2520need%2520for%2520further%2520study%2520into%2520chroma%2520histograms%2520as%2520an%250Aextracted%2520feature%2520in%2520chord%2520generation%2520tasks%2520is%2520highlighted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20LSTM-Based%20Chord%20Generation%20System%20Using%20Chroma%20Histogram%0A%20%20Representations&entry.906535625=Jack%20Hardwick&entry.1292438233=%20%20This%20paper%20proposes%20a%20system%20for%20chord%20generation%20to%20monophonic%20symbolic%0Amelodies%20using%20an%20LSTM-based%20model%20trained%20on%20chroma%20histogram%20representations%0Aof%20chords.%20Chroma%20representations%20promise%20more%20harmonically%20rich%20generation%0Athan%20chord%20label-based%20approaches%2C%20whilst%20maintaining%20a%20small%20number%20of%0Adimensions%20in%20the%20dataset.%20This%20system%20is%20shown%20to%20be%20suitable%20for%20limited%0Areal-time%20use.%20While%20it%20does%20not%20meet%20the%20state-of-the-art%20for%20coherent%0Along-term%20generation%2C%20it%20does%20show%20diatonic%20generation%20with%20cadential%20chord%0Arelationships.%20The%20need%20for%20further%20study%20into%20chroma%20histograms%20as%20an%0Aextracted%20feature%20in%20chord%20generation%20tasks%20is%20highlighted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05240v1&entry.124074799=Read"},
{"title": "AMPLIFY:Attention-based Mixup for Performance Improvement and Label\n  Smoothing in Transformer", "author": "Leixin Yang and Yu Xiang", "abstract": "  Mixup is an effective data augmentation method that generates new augmented\nsamples by aggregating linear combinations of different original samples.\nHowever, if there are noises or aberrant features in the original samples,\nMixup may propagate them to the augmented samples, leading to over-sensitivity\nof the model to these outliers . To solve this problem, this paper proposes a\nnew Mixup method called AMPLIFY. This method uses the Attention mechanism of\nTransformer itself to reduce the influence of noises and aberrant values in the\noriginal samples on the prediction results, without increasing additional\ntrainable parameters, and the computational cost is very low, thereby avoiding\nthe problem of high resource consumption in common Mixup methods such as\nSentence Mixup . The experimental results show that, under a smaller\ncomputational resource cost, AMPLIFY outperforms other Mixup methods in text\nclassification tasks on 7 benchmark datasets, providing new ideas and new ways\nto further improve the performance of pre-trained models based on the Attention\nmechanism, such as BERT, ALBERT, RoBERTa, and GPT. Our code can be obtained at\nhttps://github.com/kiwi-lilo/AMPLIFY.\n", "link": "http://arxiv.org/abs/2309.12689v3", "date": "2024-05-08", "relevancy": 2.0751, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5348}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5193}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMPLIFY%3AAttention-based%20Mixup%20for%20Performance%20Improvement%20and%20Label%0A%20%20Smoothing%20in%20Transformer&body=Title%3A%20AMPLIFY%3AAttention-based%20Mixup%20for%20Performance%20Improvement%20and%20Label%0A%20%20Smoothing%20in%20Transformer%0AAuthor%3A%20Leixin%20Yang%20and%20Yu%20Xiang%0AAbstract%3A%20%20%20Mixup%20is%20an%20effective%20data%20augmentation%20method%20that%20generates%20new%20augmented%0Asamples%20by%20aggregating%20linear%20combinations%20of%20different%20original%20samples.%0AHowever%2C%20if%20there%20are%20noises%20or%20aberrant%20features%20in%20the%20original%20samples%2C%0AMixup%20may%20propagate%20them%20to%20the%20augmented%20samples%2C%20leading%20to%20over-sensitivity%0Aof%20the%20model%20to%20these%20outliers%20.%20To%20solve%20this%20problem%2C%20this%20paper%20proposes%20a%0Anew%20Mixup%20method%20called%20AMPLIFY.%20This%20method%20uses%20the%20Attention%20mechanism%20of%0ATransformer%20itself%20to%20reduce%20the%20influence%20of%20noises%20and%20aberrant%20values%20in%20the%0Aoriginal%20samples%20on%20the%20prediction%20results%2C%20without%20increasing%20additional%0Atrainable%20parameters%2C%20and%20the%20computational%20cost%20is%20very%20low%2C%20thereby%20avoiding%0Athe%20problem%20of%20high%20resource%20consumption%20in%20common%20Mixup%20methods%20such%20as%0ASentence%20Mixup%20.%20The%20experimental%20results%20show%20that%2C%20under%20a%20smaller%0Acomputational%20resource%20cost%2C%20AMPLIFY%20outperforms%20other%20Mixup%20methods%20in%20text%0Aclassification%20tasks%20on%207%20benchmark%20datasets%2C%20providing%20new%20ideas%20and%20new%20ways%0Ato%20further%20improve%20the%20performance%20of%20pre-trained%20models%20based%20on%20the%20Attention%0Amechanism%2C%20such%20as%20BERT%2C%20ALBERT%2C%20RoBERTa%2C%20and%20GPT.%20Our%20code%20can%20be%20obtained%20at%0Ahttps%3A//github.com/kiwi-lilo/AMPLIFY.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12689v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMPLIFY%253AAttention-based%2520Mixup%2520for%2520Performance%2520Improvement%2520and%2520Label%250A%2520%2520Smoothing%2520in%2520Transformer%26entry.906535625%3DLeixin%2520Yang%2520and%2520Yu%2520Xiang%26entry.1292438233%3D%2520%2520Mixup%2520is%2520an%2520effective%2520data%2520augmentation%2520method%2520that%2520generates%2520new%2520augmented%250Asamples%2520by%2520aggregating%2520linear%2520combinations%2520of%2520different%2520original%2520samples.%250AHowever%252C%2520if%2520there%2520are%2520noises%2520or%2520aberrant%2520features%2520in%2520the%2520original%2520samples%252C%250AMixup%2520may%2520propagate%2520them%2520to%2520the%2520augmented%2520samples%252C%2520leading%2520to%2520over-sensitivity%250Aof%2520the%2520model%2520to%2520these%2520outliers%2520.%2520To%2520solve%2520this%2520problem%252C%2520this%2520paper%2520proposes%2520a%250Anew%2520Mixup%2520method%2520called%2520AMPLIFY.%2520This%2520method%2520uses%2520the%2520Attention%2520mechanism%2520of%250ATransformer%2520itself%2520to%2520reduce%2520the%2520influence%2520of%2520noises%2520and%2520aberrant%2520values%2520in%2520the%250Aoriginal%2520samples%2520on%2520the%2520prediction%2520results%252C%2520without%2520increasing%2520additional%250Atrainable%2520parameters%252C%2520and%2520the%2520computational%2520cost%2520is%2520very%2520low%252C%2520thereby%2520avoiding%250Athe%2520problem%2520of%2520high%2520resource%2520consumption%2520in%2520common%2520Mixup%2520methods%2520such%2520as%250ASentence%2520Mixup%2520.%2520The%2520experimental%2520results%2520show%2520that%252C%2520under%2520a%2520smaller%250Acomputational%2520resource%2520cost%252C%2520AMPLIFY%2520outperforms%2520other%2520Mixup%2520methods%2520in%2520text%250Aclassification%2520tasks%2520on%25207%2520benchmark%2520datasets%252C%2520providing%2520new%2520ideas%2520and%2520new%2520ways%250Ato%2520further%2520improve%2520the%2520performance%2520of%2520pre-trained%2520models%2520based%2520on%2520the%2520Attention%250Amechanism%252C%2520such%2520as%2520BERT%252C%2520ALBERT%252C%2520RoBERTa%252C%2520and%2520GPT.%2520Our%2520code%2520can%2520be%2520obtained%2520at%250Ahttps%253A//github.com/kiwi-lilo/AMPLIFY.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12689v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMPLIFY%3AAttention-based%20Mixup%20for%20Performance%20Improvement%20and%20Label%0A%20%20Smoothing%20in%20Transformer&entry.906535625=Leixin%20Yang%20and%20Yu%20Xiang&entry.1292438233=%20%20Mixup%20is%20an%20effective%20data%20augmentation%20method%20that%20generates%20new%20augmented%0Asamples%20by%20aggregating%20linear%20combinations%20of%20different%20original%20samples.%0AHowever%2C%20if%20there%20are%20noises%20or%20aberrant%20features%20in%20the%20original%20samples%2C%0AMixup%20may%20propagate%20them%20to%20the%20augmented%20samples%2C%20leading%20to%20over-sensitivity%0Aof%20the%20model%20to%20these%20outliers%20.%20To%20solve%20this%20problem%2C%20this%20paper%20proposes%20a%0Anew%20Mixup%20method%20called%20AMPLIFY.%20This%20method%20uses%20the%20Attention%20mechanism%20of%0ATransformer%20itself%20to%20reduce%20the%20influence%20of%20noises%20and%20aberrant%20values%20in%20the%0Aoriginal%20samples%20on%20the%20prediction%20results%2C%20without%20increasing%20additional%0Atrainable%20parameters%2C%20and%20the%20computational%20cost%20is%20very%20low%2C%20thereby%20avoiding%0Athe%20problem%20of%20high%20resource%20consumption%20in%20common%20Mixup%20methods%20such%20as%0ASentence%20Mixup%20.%20The%20experimental%20results%20show%20that%2C%20under%20a%20smaller%0Acomputational%20resource%20cost%2C%20AMPLIFY%20outperforms%20other%20Mixup%20methods%20in%20text%0Aclassification%20tasks%20on%207%20benchmark%20datasets%2C%20providing%20new%20ideas%20and%20new%20ways%0Ato%20further%20improve%20the%20performance%20of%20pre-trained%20models%20based%20on%20the%20Attention%0Amechanism%2C%20such%20as%20BERT%2C%20ALBERT%2C%20RoBERTa%2C%20and%20GPT.%20Our%20code%20can%20be%20obtained%20at%0Ahttps%3A//github.com/kiwi-lilo/AMPLIFY.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12689v3&entry.124074799=Read"},
{"title": "Deep-learning-based decomposition of overlapping-sparse images:\n  application at the vertex of neutrino interactions", "author": "Sa\u00fal Alonso-Monsalve and Davide Sgalaberna and Xingyu Zhao and Adrien Molines and Clark McGrew and Andr\u00e9 Rubbia", "abstract": "  Image decomposition plays a crucial role in various computer vision tasks,\nenabling the analysis and manipulation of visual content at a fundamental\nlevel. Overlapping images, which occur when multiple objects or scenes\npartially occlude each other, pose unique challenges for decomposition\nalgorithms. The task intensifies when working with sparse images, where the\nscarcity of meaningful information complicates the precise extraction of\ncomponents. This paper presents a solution that leverages the power of deep\nlearning to accurately extract individual objects within multi-dimensional\noverlapping-sparse images, with a direct application in high-energy physics\nwith decomposition of overlaid elementary particles obtained from imaging\ndetectors. In particular, the proposed approach tackles a highly complex yet\nunsolved problem: identifying and measuring independent particles at the vertex\nof neutrino interactions, where one expects to observe detector images with\nmultiple indiscernible overlapping charged particles. By decomposing the image\nof the detector activity at the vertex through deep learning, it is possible to\ninfer the kinematic parameters of the identified low-momentum particles - which\notherwise would remain neglected - and enhance the reconstructed energy\nresolution of the neutrino event. We also present an additional step - that can\nbe tuned directly on detector data - combining the above method with a\nfully-differentiable generative model to improve the image decomposition\nfurther and, consequently, the resolution of the measured parameters, achieving\nunprecedented results. This improvement is crucial for precisely measuring the\nparameters that govern neutrino flavour oscillations and searching for\nasymmetries between matter and antimatter.\n", "link": "http://arxiv.org/abs/2310.19695v3", "date": "2024-05-08", "relevancy": 2.0739, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5395}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5307}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-learning-based%20decomposition%20of%20overlapping-sparse%20images%3A%0A%20%20application%20at%20the%20vertex%20of%20neutrino%20interactions&body=Title%3A%20Deep-learning-based%20decomposition%20of%20overlapping-sparse%20images%3A%0A%20%20application%20at%20the%20vertex%20of%20neutrino%20interactions%0AAuthor%3A%20Sa%C3%BAl%20Alonso-Monsalve%20and%20Davide%20Sgalaberna%20and%20Xingyu%20Zhao%20and%20Adrien%20Molines%20and%20Clark%20McGrew%20and%20Andr%C3%A9%20Rubbia%0AAbstract%3A%20%20%20Image%20decomposition%20plays%20a%20crucial%20role%20in%20various%20computer%20vision%20tasks%2C%0Aenabling%20the%20analysis%20and%20manipulation%20of%20visual%20content%20at%20a%20fundamental%0Alevel.%20Overlapping%20images%2C%20which%20occur%20when%20multiple%20objects%20or%20scenes%0Apartially%20occlude%20each%20other%2C%20pose%20unique%20challenges%20for%20decomposition%0Aalgorithms.%20The%20task%20intensifies%20when%20working%20with%20sparse%20images%2C%20where%20the%0Ascarcity%20of%20meaningful%20information%20complicates%20the%20precise%20extraction%20of%0Acomponents.%20This%20paper%20presents%20a%20solution%20that%20leverages%20the%20power%20of%20deep%0Alearning%20to%20accurately%20extract%20individual%20objects%20within%20multi-dimensional%0Aoverlapping-sparse%20images%2C%20with%20a%20direct%20application%20in%20high-energy%20physics%0Awith%20decomposition%20of%20overlaid%20elementary%20particles%20obtained%20from%20imaging%0Adetectors.%20In%20particular%2C%20the%20proposed%20approach%20tackles%20a%20highly%20complex%20yet%0Aunsolved%20problem%3A%20identifying%20and%20measuring%20independent%20particles%20at%20the%20vertex%0Aof%20neutrino%20interactions%2C%20where%20one%20expects%20to%20observe%20detector%20images%20with%0Amultiple%20indiscernible%20overlapping%20charged%20particles.%20By%20decomposing%20the%20image%0Aof%20the%20detector%20activity%20at%20the%20vertex%20through%20deep%20learning%2C%20it%20is%20possible%20to%0Ainfer%20the%20kinematic%20parameters%20of%20the%20identified%20low-momentum%20particles%20-%20which%0Aotherwise%20would%20remain%20neglected%20-%20and%20enhance%20the%20reconstructed%20energy%0Aresolution%20of%20the%20neutrino%20event.%20We%20also%20present%20an%20additional%20step%20-%20that%20can%0Abe%20tuned%20directly%20on%20detector%20data%20-%20combining%20the%20above%20method%20with%20a%0Afully-differentiable%20generative%20model%20to%20improve%20the%20image%20decomposition%0Afurther%20and%2C%20consequently%2C%20the%20resolution%20of%20the%20measured%20parameters%2C%20achieving%0Aunprecedented%20results.%20This%20improvement%20is%20crucial%20for%20precisely%20measuring%20the%0Aparameters%20that%20govern%20neutrino%20flavour%20oscillations%20and%20searching%20for%0Aasymmetries%20between%20matter%20and%20antimatter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19695v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-learning-based%2520decomposition%2520of%2520overlapping-sparse%2520images%253A%250A%2520%2520application%2520at%2520the%2520vertex%2520of%2520neutrino%2520interactions%26entry.906535625%3DSa%25C3%25BAl%2520Alonso-Monsalve%2520and%2520Davide%2520Sgalaberna%2520and%2520Xingyu%2520Zhao%2520and%2520Adrien%2520Molines%2520and%2520Clark%2520McGrew%2520and%2520Andr%25C3%25A9%2520Rubbia%26entry.1292438233%3D%2520%2520Image%2520decomposition%2520plays%2520a%2520crucial%2520role%2520in%2520various%2520computer%2520vision%2520tasks%252C%250Aenabling%2520the%2520analysis%2520and%2520manipulation%2520of%2520visual%2520content%2520at%2520a%2520fundamental%250Alevel.%2520Overlapping%2520images%252C%2520which%2520occur%2520when%2520multiple%2520objects%2520or%2520scenes%250Apartially%2520occlude%2520each%2520other%252C%2520pose%2520unique%2520challenges%2520for%2520decomposition%250Aalgorithms.%2520The%2520task%2520intensifies%2520when%2520working%2520with%2520sparse%2520images%252C%2520where%2520the%250Ascarcity%2520of%2520meaningful%2520information%2520complicates%2520the%2520precise%2520extraction%2520of%250Acomponents.%2520This%2520paper%2520presents%2520a%2520solution%2520that%2520leverages%2520the%2520power%2520of%2520deep%250Alearning%2520to%2520accurately%2520extract%2520individual%2520objects%2520within%2520multi-dimensional%250Aoverlapping-sparse%2520images%252C%2520with%2520a%2520direct%2520application%2520in%2520high-energy%2520physics%250Awith%2520decomposition%2520of%2520overlaid%2520elementary%2520particles%2520obtained%2520from%2520imaging%250Adetectors.%2520In%2520particular%252C%2520the%2520proposed%2520approach%2520tackles%2520a%2520highly%2520complex%2520yet%250Aunsolved%2520problem%253A%2520identifying%2520and%2520measuring%2520independent%2520particles%2520at%2520the%2520vertex%250Aof%2520neutrino%2520interactions%252C%2520where%2520one%2520expects%2520to%2520observe%2520detector%2520images%2520with%250Amultiple%2520indiscernible%2520overlapping%2520charged%2520particles.%2520By%2520decomposing%2520the%2520image%250Aof%2520the%2520detector%2520activity%2520at%2520the%2520vertex%2520through%2520deep%2520learning%252C%2520it%2520is%2520possible%2520to%250Ainfer%2520the%2520kinematic%2520parameters%2520of%2520the%2520identified%2520low-momentum%2520particles%2520-%2520which%250Aotherwise%2520would%2520remain%2520neglected%2520-%2520and%2520enhance%2520the%2520reconstructed%2520energy%250Aresolution%2520of%2520the%2520neutrino%2520event.%2520We%2520also%2520present%2520an%2520additional%2520step%2520-%2520that%2520can%250Abe%2520tuned%2520directly%2520on%2520detector%2520data%2520-%2520combining%2520the%2520above%2520method%2520with%2520a%250Afully-differentiable%2520generative%2520model%2520to%2520improve%2520the%2520image%2520decomposition%250Afurther%2520and%252C%2520consequently%252C%2520the%2520resolution%2520of%2520the%2520measured%2520parameters%252C%2520achieving%250Aunprecedented%2520results.%2520This%2520improvement%2520is%2520crucial%2520for%2520precisely%2520measuring%2520the%250Aparameters%2520that%2520govern%2520neutrino%2520flavour%2520oscillations%2520and%2520searching%2520for%250Aasymmetries%2520between%2520matter%2520and%2520antimatter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19695v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-learning-based%20decomposition%20of%20overlapping-sparse%20images%3A%0A%20%20application%20at%20the%20vertex%20of%20neutrino%20interactions&entry.906535625=Sa%C3%BAl%20Alonso-Monsalve%20and%20Davide%20Sgalaberna%20and%20Xingyu%20Zhao%20and%20Adrien%20Molines%20and%20Clark%20McGrew%20and%20Andr%C3%A9%20Rubbia&entry.1292438233=%20%20Image%20decomposition%20plays%20a%20crucial%20role%20in%20various%20computer%20vision%20tasks%2C%0Aenabling%20the%20analysis%20and%20manipulation%20of%20visual%20content%20at%20a%20fundamental%0Alevel.%20Overlapping%20images%2C%20which%20occur%20when%20multiple%20objects%20or%20scenes%0Apartially%20occlude%20each%20other%2C%20pose%20unique%20challenges%20for%20decomposition%0Aalgorithms.%20The%20task%20intensifies%20when%20working%20with%20sparse%20images%2C%20where%20the%0Ascarcity%20of%20meaningful%20information%20complicates%20the%20precise%20extraction%20of%0Acomponents.%20This%20paper%20presents%20a%20solution%20that%20leverages%20the%20power%20of%20deep%0Alearning%20to%20accurately%20extract%20individual%20objects%20within%20multi-dimensional%0Aoverlapping-sparse%20images%2C%20with%20a%20direct%20application%20in%20high-energy%20physics%0Awith%20decomposition%20of%20overlaid%20elementary%20particles%20obtained%20from%20imaging%0Adetectors.%20In%20particular%2C%20the%20proposed%20approach%20tackles%20a%20highly%20complex%20yet%0Aunsolved%20problem%3A%20identifying%20and%20measuring%20independent%20particles%20at%20the%20vertex%0Aof%20neutrino%20interactions%2C%20where%20one%20expects%20to%20observe%20detector%20images%20with%0Amultiple%20indiscernible%20overlapping%20charged%20particles.%20By%20decomposing%20the%20image%0Aof%20the%20detector%20activity%20at%20the%20vertex%20through%20deep%20learning%2C%20it%20is%20possible%20to%0Ainfer%20the%20kinematic%20parameters%20of%20the%20identified%20low-momentum%20particles%20-%20which%0Aotherwise%20would%20remain%20neglected%20-%20and%20enhance%20the%20reconstructed%20energy%0Aresolution%20of%20the%20neutrino%20event.%20We%20also%20present%20an%20additional%20step%20-%20that%20can%0Abe%20tuned%20directly%20on%20detector%20data%20-%20combining%20the%20above%20method%20with%20a%0Afully-differentiable%20generative%20model%20to%20improve%20the%20image%20decomposition%0Afurther%20and%2C%20consequently%2C%20the%20resolution%20of%20the%20measured%20parameters%2C%20achieving%0Aunprecedented%20results.%20This%20improvement%20is%20crucial%20for%20precisely%20measuring%20the%0Aparameters%20that%20govern%20neutrino%20flavour%20oscillations%20and%20searching%20for%0Aasymmetries%20between%20matter%20and%20antimatter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19695v3&entry.124074799=Read"},
{"title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training", "author": "Renjie Liu and Yichuan Wang and Xiao Yan and Zhenkun Cai and Minjie Wang and Haitian Jiang and Bo Tang and Jinyang Li", "abstract": "  Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.\n", "link": "http://arxiv.org/abs/2405.05231v1", "date": "2024-05-08", "relevancy": 2.0578, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5224}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5153}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiskGNN%3A%20Bridging%20I/O%20Efficiency%20and%20Model%20Accuracy%20for%20Out-of-Core%20GNN%0A%20%20Training&body=Title%3A%20DiskGNN%3A%20Bridging%20I/O%20Efficiency%20and%20Model%20Accuracy%20for%20Out-of-Core%20GNN%0A%20%20Training%0AAuthor%3A%20Renjie%20Liu%20and%20Yichuan%20Wang%20and%20Xiao%20Yan%20and%20Zhenkun%20Cai%20and%20Minjie%20Wang%20and%20Haitian%20Jiang%20and%20Bo%20Tang%20and%20Jinyang%20Li%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20machine%20learning%20models%20specialized%20for%0Agraph%20data%20and%20widely%20used%20in%20many%20applications.%20To%20train%20GNNs%20on%20large%20graphs%0Athat%20exceed%20CPU%20memory%2C%20several%20systems%20store%20data%20on%20disk%20and%20conduct%0Aout-of-core%20processing.%20However%2C%20these%20systems%20suffer%20from%20either%20read%0Aamplification%20when%20reading%20node%20features%20that%20are%20usually%20smaller%20than%20a%20disk%0Apage%20or%20degraded%20model%20accuracy%20by%20treating%20the%20graph%20as%20disconnected%0Apartitions.%20To%20close%20this%20gap%2C%20we%20build%20a%20system%20called%20DiskGNN%2C%20which%20achieves%0Ahigh%20I/O%20efficiency%20and%20thus%20fast%20training%20without%20hurting%20model%20accuracy.%20The%0Akey%20technique%20used%20by%20DiskGNN%20is%20offline%20sampling%2C%20which%20helps%20decouple%20graph%0Asampling%20from%20model%20computation.%20In%20particular%2C%20by%20conducting%20graph%20sampling%0Abeforehand%2C%20DiskGNN%20acquires%20the%20node%20features%20that%20will%20be%20accessed%20by%20model%0Acomputation%2C%20and%20such%20information%20is%20utilized%20to%20pack%20the%20target%20node%20features%0Acontiguously%20on%20disk%20to%20avoid%20read%20amplification.%20Besides%2C%20%5Cname%7B%7D%20also%20adopts%0Adesigns%20including%20four-level%20feature%20store%20to%20fully%20utilize%20the%20memory%0Ahierarchy%20to%20cache%20node%20features%20and%20reduce%20disk%20access%2C%20batched%20packing%20to%0Aaccelerate%20the%20feature%20packing%20process%2C%20and%20pipelined%20training%20to%20overlap%20disk%0Aaccess%20with%20other%20operations.%20We%20compare%20DiskGNN%20with%20Ginex%20and%20MariusGNN%2C%0Awhich%20are%20state-of-the-art%20systems%20for%20out-of-core%20GNN%20training.%20The%20results%0Ashow%20that%20DiskGNN%20can%20speed%20up%20the%20baselines%20by%20over%208x%20while%20matching%20their%0Abest%20model%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiskGNN%253A%2520Bridging%2520I/O%2520Efficiency%2520and%2520Model%2520Accuracy%2520for%2520Out-of-Core%2520GNN%250A%2520%2520Training%26entry.906535625%3DRenjie%2520Liu%2520and%2520Yichuan%2520Wang%2520and%2520Xiao%2520Yan%2520and%2520Zhenkun%2520Cai%2520and%2520Minjie%2520Wang%2520and%2520Haitian%2520Jiang%2520and%2520Bo%2520Tang%2520and%2520Jinyang%2520Li%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520machine%2520learning%2520models%2520specialized%2520for%250Agraph%2520data%2520and%2520widely%2520used%2520in%2520many%2520applications.%2520To%2520train%2520GNNs%2520on%2520large%2520graphs%250Athat%2520exceed%2520CPU%2520memory%252C%2520several%2520systems%2520store%2520data%2520on%2520disk%2520and%2520conduct%250Aout-of-core%2520processing.%2520However%252C%2520these%2520systems%2520suffer%2520from%2520either%2520read%250Aamplification%2520when%2520reading%2520node%2520features%2520that%2520are%2520usually%2520smaller%2520than%2520a%2520disk%250Apage%2520or%2520degraded%2520model%2520accuracy%2520by%2520treating%2520the%2520graph%2520as%2520disconnected%250Apartitions.%2520To%2520close%2520this%2520gap%252C%2520we%2520build%2520a%2520system%2520called%2520DiskGNN%252C%2520which%2520achieves%250Ahigh%2520I/O%2520efficiency%2520and%2520thus%2520fast%2520training%2520without%2520hurting%2520model%2520accuracy.%2520The%250Akey%2520technique%2520used%2520by%2520DiskGNN%2520is%2520offline%2520sampling%252C%2520which%2520helps%2520decouple%2520graph%250Asampling%2520from%2520model%2520computation.%2520In%2520particular%252C%2520by%2520conducting%2520graph%2520sampling%250Abeforehand%252C%2520DiskGNN%2520acquires%2520the%2520node%2520features%2520that%2520will%2520be%2520accessed%2520by%2520model%250Acomputation%252C%2520and%2520such%2520information%2520is%2520utilized%2520to%2520pack%2520the%2520target%2520node%2520features%250Acontiguously%2520on%2520disk%2520to%2520avoid%2520read%2520amplification.%2520Besides%252C%2520%255Cname%257B%257D%2520also%2520adopts%250Adesigns%2520including%2520four-level%2520feature%2520store%2520to%2520fully%2520utilize%2520the%2520memory%250Ahierarchy%2520to%2520cache%2520node%2520features%2520and%2520reduce%2520disk%2520access%252C%2520batched%2520packing%2520to%250Aaccelerate%2520the%2520feature%2520packing%2520process%252C%2520and%2520pipelined%2520training%2520to%2520overlap%2520disk%250Aaccess%2520with%2520other%2520operations.%2520We%2520compare%2520DiskGNN%2520with%2520Ginex%2520and%2520MariusGNN%252C%250Awhich%2520are%2520state-of-the-art%2520systems%2520for%2520out-of-core%2520GNN%2520training.%2520The%2520results%250Ashow%2520that%2520DiskGNN%2520can%2520speed%2520up%2520the%2520baselines%2520by%2520over%25208x%2520while%2520matching%2520their%250Abest%2520model%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiskGNN%3A%20Bridging%20I/O%20Efficiency%20and%20Model%20Accuracy%20for%20Out-of-Core%20GNN%0A%20%20Training&entry.906535625=Renjie%20Liu%20and%20Yichuan%20Wang%20and%20Xiao%20Yan%20and%20Zhenkun%20Cai%20and%20Minjie%20Wang%20and%20Haitian%20Jiang%20and%20Bo%20Tang%20and%20Jinyang%20Li&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20machine%20learning%20models%20specialized%20for%0Agraph%20data%20and%20widely%20used%20in%20many%20applications.%20To%20train%20GNNs%20on%20large%20graphs%0Athat%20exceed%20CPU%20memory%2C%20several%20systems%20store%20data%20on%20disk%20and%20conduct%0Aout-of-core%20processing.%20However%2C%20these%20systems%20suffer%20from%20either%20read%0Aamplification%20when%20reading%20node%20features%20that%20are%20usually%20smaller%20than%20a%20disk%0Apage%20or%20degraded%20model%20accuracy%20by%20treating%20the%20graph%20as%20disconnected%0Apartitions.%20To%20close%20this%20gap%2C%20we%20build%20a%20system%20called%20DiskGNN%2C%20which%20achieves%0Ahigh%20I/O%20efficiency%20and%20thus%20fast%20training%20without%20hurting%20model%20accuracy.%20The%0Akey%20technique%20used%20by%20DiskGNN%20is%20offline%20sampling%2C%20which%20helps%20decouple%20graph%0Asampling%20from%20model%20computation.%20In%20particular%2C%20by%20conducting%20graph%20sampling%0Abeforehand%2C%20DiskGNN%20acquires%20the%20node%20features%20that%20will%20be%20accessed%20by%20model%0Acomputation%2C%20and%20such%20information%20is%20utilized%20to%20pack%20the%20target%20node%20features%0Acontiguously%20on%20disk%20to%20avoid%20read%20amplification.%20Besides%2C%20%5Cname%7B%7D%20also%20adopts%0Adesigns%20including%20four-level%20feature%20store%20to%20fully%20utilize%20the%20memory%0Ahierarchy%20to%20cache%20node%20features%20and%20reduce%20disk%20access%2C%20batched%20packing%20to%0Aaccelerate%20the%20feature%20packing%20process%2C%20and%20pipelined%20training%20to%20overlap%20disk%0Aaccess%20with%20other%20operations.%20We%20compare%20DiskGNN%20with%20Ginex%20and%20MariusGNN%2C%0Awhich%20are%20state-of-the-art%20systems%20for%20out-of-core%20GNN%20training.%20The%20results%0Ashow%20that%20DiskGNN%20can%20speed%20up%20the%20baselines%20by%20over%208x%20while%20matching%20their%0Abest%20model%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05231v1&entry.124074799=Read"},
{"title": "SINBAD: Saliency-informed detection of breakage caused by ad blocking", "author": "Saiid El Hajj Chehade and Sandra Siby and Carmela Troncoso", "abstract": "  Privacy-enhancing blocking tools based on filter-list rules tend to break\nlegitimate functionality. Filter-list maintainers could benefit from automated\nbreakage detection tools that allow them to proactively fix problematic rules\nbefore deploying them to millions of users. We introduce SINBAD, an automated\nbreakage detector that improves the accuracy over the state of the art by 20%,\nand is the first to detect dynamic breakage and breakage caused by\nstyle-oriented filter rules. The success of SINBAD is rooted in three\ninnovations: (1) the use of user-reported breakage issues in forums that enable\nthe creation of a high-quality dataset for training in which only breakage that\nusers perceive as an issue is included; (2) the use of 'web saliency' to\nautomatically identify user-relevant regions of a website on which to\nprioritize automated interactions aimed at triggering breakage; and (3) the\nanalysis of webpages via subtrees which enables fine-grained identification of\nproblematic filter rules.\n", "link": "http://arxiv.org/abs/2405.05196v1", "date": "2024-05-08", "relevancy": 2.0444, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4263}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4007}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SINBAD%3A%20Saliency-informed%20detection%20of%20breakage%20caused%20by%20ad%20blocking&body=Title%3A%20SINBAD%3A%20Saliency-informed%20detection%20of%20breakage%20caused%20by%20ad%20blocking%0AAuthor%3A%20Saiid%20El%20Hajj%20Chehade%20and%20Sandra%20Siby%20and%20Carmela%20Troncoso%0AAbstract%3A%20%20%20Privacy-enhancing%20blocking%20tools%20based%20on%20filter-list%20rules%20tend%20to%20break%0Alegitimate%20functionality.%20Filter-list%20maintainers%20could%20benefit%20from%20automated%0Abreakage%20detection%20tools%20that%20allow%20them%20to%20proactively%20fix%20problematic%20rules%0Abefore%20deploying%20them%20to%20millions%20of%20users.%20We%20introduce%20SINBAD%2C%20an%20automated%0Abreakage%20detector%20that%20improves%20the%20accuracy%20over%20the%20state%20of%20the%20art%20by%2020%25%2C%0Aand%20is%20the%20first%20to%20detect%20dynamic%20breakage%20and%20breakage%20caused%20by%0Astyle-oriented%20filter%20rules.%20The%20success%20of%20SINBAD%20is%20rooted%20in%20three%0Ainnovations%3A%20%281%29%20the%20use%20of%20user-reported%20breakage%20issues%20in%20forums%20that%20enable%0Athe%20creation%20of%20a%20high-quality%20dataset%20for%20training%20in%20which%20only%20breakage%20that%0Ausers%20perceive%20as%20an%20issue%20is%20included%3B%20%282%29%20the%20use%20of%20%27web%20saliency%27%20to%0Aautomatically%20identify%20user-relevant%20regions%20of%20a%20website%20on%20which%20to%0Aprioritize%20automated%20interactions%20aimed%20at%20triggering%20breakage%3B%20and%20%283%29%20the%0Aanalysis%20of%20webpages%20via%20subtrees%20which%20enables%20fine-grained%20identification%20of%0Aproblematic%20filter%20rules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSINBAD%253A%2520Saliency-informed%2520detection%2520of%2520breakage%2520caused%2520by%2520ad%2520blocking%26entry.906535625%3DSaiid%2520El%2520Hajj%2520Chehade%2520and%2520Sandra%2520Siby%2520and%2520Carmela%2520Troncoso%26entry.1292438233%3D%2520%2520Privacy-enhancing%2520blocking%2520tools%2520based%2520on%2520filter-list%2520rules%2520tend%2520to%2520break%250Alegitimate%2520functionality.%2520Filter-list%2520maintainers%2520could%2520benefit%2520from%2520automated%250Abreakage%2520detection%2520tools%2520that%2520allow%2520them%2520to%2520proactively%2520fix%2520problematic%2520rules%250Abefore%2520deploying%2520them%2520to%2520millions%2520of%2520users.%2520We%2520introduce%2520SINBAD%252C%2520an%2520automated%250Abreakage%2520detector%2520that%2520improves%2520the%2520accuracy%2520over%2520the%2520state%2520of%2520the%2520art%2520by%252020%2525%252C%250Aand%2520is%2520the%2520first%2520to%2520detect%2520dynamic%2520breakage%2520and%2520breakage%2520caused%2520by%250Astyle-oriented%2520filter%2520rules.%2520The%2520success%2520of%2520SINBAD%2520is%2520rooted%2520in%2520three%250Ainnovations%253A%2520%25281%2529%2520the%2520use%2520of%2520user-reported%2520breakage%2520issues%2520in%2520forums%2520that%2520enable%250Athe%2520creation%2520of%2520a%2520high-quality%2520dataset%2520for%2520training%2520in%2520which%2520only%2520breakage%2520that%250Ausers%2520perceive%2520as%2520an%2520issue%2520is%2520included%253B%2520%25282%2529%2520the%2520use%2520of%2520%2527web%2520saliency%2527%2520to%250Aautomatically%2520identify%2520user-relevant%2520regions%2520of%2520a%2520website%2520on%2520which%2520to%250Aprioritize%2520automated%2520interactions%2520aimed%2520at%2520triggering%2520breakage%253B%2520and%2520%25283%2529%2520the%250Aanalysis%2520of%2520webpages%2520via%2520subtrees%2520which%2520enables%2520fine-grained%2520identification%2520of%250Aproblematic%2520filter%2520rules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SINBAD%3A%20Saliency-informed%20detection%20of%20breakage%20caused%20by%20ad%20blocking&entry.906535625=Saiid%20El%20Hajj%20Chehade%20and%20Sandra%20Siby%20and%20Carmela%20Troncoso&entry.1292438233=%20%20Privacy-enhancing%20blocking%20tools%20based%20on%20filter-list%20rules%20tend%20to%20break%0Alegitimate%20functionality.%20Filter-list%20maintainers%20could%20benefit%20from%20automated%0Abreakage%20detection%20tools%20that%20allow%20them%20to%20proactively%20fix%20problematic%20rules%0Abefore%20deploying%20them%20to%20millions%20of%20users.%20We%20introduce%20SINBAD%2C%20an%20automated%0Abreakage%20detector%20that%20improves%20the%20accuracy%20over%20the%20state%20of%20the%20art%20by%2020%25%2C%0Aand%20is%20the%20first%20to%20detect%20dynamic%20breakage%20and%20breakage%20caused%20by%0Astyle-oriented%20filter%20rules.%20The%20success%20of%20SINBAD%20is%20rooted%20in%20three%0Ainnovations%3A%20%281%29%20the%20use%20of%20user-reported%20breakage%20issues%20in%20forums%20that%20enable%0Athe%20creation%20of%20a%20high-quality%20dataset%20for%20training%20in%20which%20only%20breakage%20that%0Ausers%20perceive%20as%20an%20issue%20is%20included%3B%20%282%29%20the%20use%20of%20%27web%20saliency%27%20to%0Aautomatically%20identify%20user-relevant%20regions%20of%20a%20website%20on%20which%20to%0Aprioritize%20automated%20interactions%20aimed%20at%20triggering%20breakage%3B%20and%20%283%29%20the%0Aanalysis%20of%20webpages%20via%20subtrees%20which%20enables%20fine-grained%20identification%20of%0Aproblematic%20filter%20rules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05196v1&entry.124074799=Read"},
{"title": "Novel Actor-Critic Algorithm for Robust Decision Making of CAV under\n  Delays and Loss of V2X Data", "author": "Zine el abidine Kherroubi", "abstract": "  Current autonomous driving systems heavily rely on V2X communication data to\nenhance situational awareness and the cooperation between vehicles. However, a\nmajor challenge when using V2X data is that it may not be available\nperiodically because of unpredictable delays and data loss during wireless\ntransmission between road stations and the receiver vehicle. This issue should\nbe considered when designing control strategies for connected and autonomous\nvehicles. Therefore, this paper proposes a novel 'Blind Actor-Critic' algorithm\nthat guarantees robust driving performance in V2X environment with delayed\nand/or lost data. The novel algorithm incorporates three key mechanisms: a\nvirtual fixed sampling period, a combination of Temporal-Difference and Monte\nCarlo learning, and a numerical approximation of immediate reward values. To\naddress the temporal aperiodicity problem of V2X data, we first illustrate this\nchallenge. Then, we provide a detailed explanation of the Blind Actor-Critic\nalgorithm where we highlight the proposed components to compensate for the\ntemporal aperiodicity problem of V2X data. We evaluate the performance of our\nalgorithm in a simulation environment and compare it to benchmark approaches.\nThe results demonstrate that training metrics are improved compared to\nconventional actor-critic algorithms. Additionally, testing results show that\nour approach provides robust control, even under low V2X network reliability\nlevels.\n", "link": "http://arxiv.org/abs/2405.05072v1", "date": "2024-05-08", "relevancy": 2.039, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5136}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5113}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Actor-Critic%20Algorithm%20for%20Robust%20Decision%20Making%20of%20CAV%20under%0A%20%20Delays%20and%20Loss%20of%20V2X%20Data&body=Title%3A%20Novel%20Actor-Critic%20Algorithm%20for%20Robust%20Decision%20Making%20of%20CAV%20under%0A%20%20Delays%20and%20Loss%20of%20V2X%20Data%0AAuthor%3A%20Zine%20el%20abidine%20Kherroubi%0AAbstract%3A%20%20%20Current%20autonomous%20driving%20systems%20heavily%20rely%20on%20V2X%20communication%20data%20to%0Aenhance%20situational%20awareness%20and%20the%20cooperation%20between%20vehicles.%20However%2C%20a%0Amajor%20challenge%20when%20using%20V2X%20data%20is%20that%20it%20may%20not%20be%20available%0Aperiodically%20because%20of%20unpredictable%20delays%20and%20data%20loss%20during%20wireless%0Atransmission%20between%20road%20stations%20and%20the%20receiver%20vehicle.%20This%20issue%20should%0Abe%20considered%20when%20designing%20control%20strategies%20for%20connected%20and%20autonomous%0Avehicles.%20Therefore%2C%20this%20paper%20proposes%20a%20novel%20%27Blind%20Actor-Critic%27%20algorithm%0Athat%20guarantees%20robust%20driving%20performance%20in%20V2X%20environment%20with%20delayed%0Aand/or%20lost%20data.%20The%20novel%20algorithm%20incorporates%20three%20key%20mechanisms%3A%20a%0Avirtual%20fixed%20sampling%20period%2C%20a%20combination%20of%20Temporal-Difference%20and%20Monte%0ACarlo%20learning%2C%20and%20a%20numerical%20approximation%20of%20immediate%20reward%20values.%20To%0Aaddress%20the%20temporal%20aperiodicity%20problem%20of%20V2X%20data%2C%20we%20first%20illustrate%20this%0Achallenge.%20Then%2C%20we%20provide%20a%20detailed%20explanation%20of%20the%20Blind%20Actor-Critic%0Aalgorithm%20where%20we%20highlight%20the%20proposed%20components%20to%20compensate%20for%20the%0Atemporal%20aperiodicity%20problem%20of%20V2X%20data.%20We%20evaluate%20the%20performance%20of%20our%0Aalgorithm%20in%20a%20simulation%20environment%20and%20compare%20it%20to%20benchmark%20approaches.%0AThe%20results%20demonstrate%20that%20training%20metrics%20are%20improved%20compared%20to%0Aconventional%20actor-critic%20algorithms.%20Additionally%2C%20testing%20results%20show%20that%0Aour%20approach%20provides%20robust%20control%2C%20even%20under%20low%20V2X%20network%20reliability%0Alevels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Actor-Critic%2520Algorithm%2520for%2520Robust%2520Decision%2520Making%2520of%2520CAV%2520under%250A%2520%2520Delays%2520and%2520Loss%2520of%2520V2X%2520Data%26entry.906535625%3DZine%2520el%2520abidine%2520Kherroubi%26entry.1292438233%3D%2520%2520Current%2520autonomous%2520driving%2520systems%2520heavily%2520rely%2520on%2520V2X%2520communication%2520data%2520to%250Aenhance%2520situational%2520awareness%2520and%2520the%2520cooperation%2520between%2520vehicles.%2520However%252C%2520a%250Amajor%2520challenge%2520when%2520using%2520V2X%2520data%2520is%2520that%2520it%2520may%2520not%2520be%2520available%250Aperiodically%2520because%2520of%2520unpredictable%2520delays%2520and%2520data%2520loss%2520during%2520wireless%250Atransmission%2520between%2520road%2520stations%2520and%2520the%2520receiver%2520vehicle.%2520This%2520issue%2520should%250Abe%2520considered%2520when%2520designing%2520control%2520strategies%2520for%2520connected%2520and%2520autonomous%250Avehicles.%2520Therefore%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520%2527Blind%2520Actor-Critic%2527%2520algorithm%250Athat%2520guarantees%2520robust%2520driving%2520performance%2520in%2520V2X%2520environment%2520with%2520delayed%250Aand/or%2520lost%2520data.%2520The%2520novel%2520algorithm%2520incorporates%2520three%2520key%2520mechanisms%253A%2520a%250Avirtual%2520fixed%2520sampling%2520period%252C%2520a%2520combination%2520of%2520Temporal-Difference%2520and%2520Monte%250ACarlo%2520learning%252C%2520and%2520a%2520numerical%2520approximation%2520of%2520immediate%2520reward%2520values.%2520To%250Aaddress%2520the%2520temporal%2520aperiodicity%2520problem%2520of%2520V2X%2520data%252C%2520we%2520first%2520illustrate%2520this%250Achallenge.%2520Then%252C%2520we%2520provide%2520a%2520detailed%2520explanation%2520of%2520the%2520Blind%2520Actor-Critic%250Aalgorithm%2520where%2520we%2520highlight%2520the%2520proposed%2520components%2520to%2520compensate%2520for%2520the%250Atemporal%2520aperiodicity%2520problem%2520of%2520V2X%2520data.%2520We%2520evaluate%2520the%2520performance%2520of%2520our%250Aalgorithm%2520in%2520a%2520simulation%2520environment%2520and%2520compare%2520it%2520to%2520benchmark%2520approaches.%250AThe%2520results%2520demonstrate%2520that%2520training%2520metrics%2520are%2520improved%2520compared%2520to%250Aconventional%2520actor-critic%2520algorithms.%2520Additionally%252C%2520testing%2520results%2520show%2520that%250Aour%2520approach%2520provides%2520robust%2520control%252C%2520even%2520under%2520low%2520V2X%2520network%2520reliability%250Alevels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Actor-Critic%20Algorithm%20for%20Robust%20Decision%20Making%20of%20CAV%20under%0A%20%20Delays%20and%20Loss%20of%20V2X%20Data&entry.906535625=Zine%20el%20abidine%20Kherroubi&entry.1292438233=%20%20Current%20autonomous%20driving%20systems%20heavily%20rely%20on%20V2X%20communication%20data%20to%0Aenhance%20situational%20awareness%20and%20the%20cooperation%20between%20vehicles.%20However%2C%20a%0Amajor%20challenge%20when%20using%20V2X%20data%20is%20that%20it%20may%20not%20be%20available%0Aperiodically%20because%20of%20unpredictable%20delays%20and%20data%20loss%20during%20wireless%0Atransmission%20between%20road%20stations%20and%20the%20receiver%20vehicle.%20This%20issue%20should%0Abe%20considered%20when%20designing%20control%20strategies%20for%20connected%20and%20autonomous%0Avehicles.%20Therefore%2C%20this%20paper%20proposes%20a%20novel%20%27Blind%20Actor-Critic%27%20algorithm%0Athat%20guarantees%20robust%20driving%20performance%20in%20V2X%20environment%20with%20delayed%0Aand/or%20lost%20data.%20The%20novel%20algorithm%20incorporates%20three%20key%20mechanisms%3A%20a%0Avirtual%20fixed%20sampling%20period%2C%20a%20combination%20of%20Temporal-Difference%20and%20Monte%0ACarlo%20learning%2C%20and%20a%20numerical%20approximation%20of%20immediate%20reward%20values.%20To%0Aaddress%20the%20temporal%20aperiodicity%20problem%20of%20V2X%20data%2C%20we%20first%20illustrate%20this%0Achallenge.%20Then%2C%20we%20provide%20a%20detailed%20explanation%20of%20the%20Blind%20Actor-Critic%0Aalgorithm%20where%20we%20highlight%20the%20proposed%20components%20to%20compensate%20for%20the%0Atemporal%20aperiodicity%20problem%20of%20V2X%20data.%20We%20evaluate%20the%20performance%20of%20our%0Aalgorithm%20in%20a%20simulation%20environment%20and%20compare%20it%20to%20benchmark%20approaches.%0AThe%20results%20demonstrate%20that%20training%20metrics%20are%20improved%20compared%20to%0Aconventional%20actor-critic%20algorithms.%20Additionally%2C%20testing%20results%20show%20that%0Aour%20approach%20provides%20robust%20control%2C%20even%20under%20low%20V2X%20network%20reliability%0Alevels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05072v1&entry.124074799=Read"},
{"title": "BenthicNet: A global compilation of seafloor images for deep learning\n  applications", "author": "Scott C. Lowe and Benjamin Misiuk and Isaac Xu and Shakhboz Abdulazizov and Amit R. Baroi and Alex C. Bastos and Merlin Best and Vicki Ferrini and Ariell Friedman and Deborah Hart and Ove Hoegh-Guldberg and Daniel Ierodiaconou and Julia Mackin-McLaughlin and Kathryn Markey and Pedro S. Menandro and Jacquomo Monk and Shreya Nemani and John O'Brien and Elizabeth Oh and Luba Y. Reshitnyk and Katleen Robert and Chris M. Roelfsema and Jessica A. Sameoto and Alexandre C. G. Schimel and Jordan A. Thomson and Brittany R. Wilson and Melisa C. Wong and Craig J. Brown and Thomas Trappenberg", "abstract": "  Advances in underwater imaging enable the collection of extensive seafloor\nimage datasets that are necessary for monitoring important benthic ecosystems.\nThe ability to collect seafloor imagery has outpaced our capacity to analyze\nit, hindering expedient mobilization of this crucial environmental information.\nRecent machine learning approaches provide opportunities to increase the\nefficiency with which seafloor image datasets are analyzed, yet large and\nconsistent datasets necessary to support development of such approaches are\nscarce. Here we present BenthicNet: a global compilation of seafloor imagery\ndesigned to support the training and evaluation of large-scale image\nrecognition models. An initial set of over 11.4 million images was collected\nand curated to represent a diversity of seafloor environments using a\nrepresentative subset of 1.3 million images. These are accompanied by 2.6\nmillion annotations translated to the CATAMI scheme, which span 190,000 of the\nimages. A large deep learning model was trained on this compilation and\npreliminary results suggest it has utility for automating large and small-scale\nimage analysis tasks. The compilation and model are made openly available for\nuse by the scientific community at https://doi.org/10.20383/103.0614.\n", "link": "http://arxiv.org/abs/2405.05241v1", "date": "2024-05-08", "relevancy": 2.0313, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5231}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4993}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications&body=Title%3A%20BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications%0AAuthor%3A%20Scott%20C.%20Lowe%20and%20Benjamin%20Misiuk%20and%20Isaac%20Xu%20and%20Shakhboz%20Abdulazizov%20and%20Amit%20R.%20Baroi%20and%20Alex%20C.%20Bastos%20and%20Merlin%20Best%20and%20Vicki%20Ferrini%20and%20Ariell%20Friedman%20and%20Deborah%20Hart%20and%20Ove%20Hoegh-Guldberg%20and%20Daniel%20Ierodiaconou%20and%20Julia%20Mackin-McLaughlin%20and%20Kathryn%20Markey%20and%20Pedro%20S.%20Menandro%20and%20Jacquomo%20Monk%20and%20Shreya%20Nemani%20and%20John%20O%27Brien%20and%20Elizabeth%20Oh%20and%20Luba%20Y.%20Reshitnyk%20and%20Katleen%20Robert%20and%20Chris%20M.%20Roelfsema%20and%20Jessica%20A.%20Sameoto%20and%20Alexandre%20C.%20G.%20Schimel%20and%20Jordan%20A.%20Thomson%20and%20Brittany%20R.%20Wilson%20and%20Melisa%20C.%20Wong%20and%20Craig%20J.%20Brown%20and%20Thomas%20Trappenberg%0AAbstract%3A%20%20%20Advances%20in%20underwater%20imaging%20enable%20the%20collection%20of%20extensive%20seafloor%0Aimage%20datasets%20that%20are%20necessary%20for%20monitoring%20important%20benthic%20ecosystems.%0AThe%20ability%20to%20collect%20seafloor%20imagery%20has%20outpaced%20our%20capacity%20to%20analyze%0Ait%2C%20hindering%20expedient%20mobilization%20of%20this%20crucial%20environmental%20information.%0ARecent%20machine%20learning%20approaches%20provide%20opportunities%20to%20increase%20the%0Aefficiency%20with%20which%20seafloor%20image%20datasets%20are%20analyzed%2C%20yet%20large%20and%0Aconsistent%20datasets%20necessary%20to%20support%20development%20of%20such%20approaches%20are%0Ascarce.%20Here%20we%20present%20BenthicNet%3A%20a%20global%20compilation%20of%20seafloor%20imagery%0Adesigned%20to%20support%20the%20training%20and%20evaluation%20of%20large-scale%20image%0Arecognition%20models.%20An%20initial%20set%20of%20over%2011.4%20million%20images%20was%20collected%0Aand%20curated%20to%20represent%20a%20diversity%20of%20seafloor%20environments%20using%20a%0Arepresentative%20subset%20of%201.3%20million%20images.%20These%20are%20accompanied%20by%202.6%0Amillion%20annotations%20translated%20to%20the%20CATAMI%20scheme%2C%20which%20span%20190%2C000%20of%20the%0Aimages.%20A%20large%20deep%20learning%20model%20was%20trained%20on%20this%20compilation%20and%0Apreliminary%20results%20suggest%20it%20has%20utility%20for%20automating%20large%20and%20small-scale%0Aimage%20analysis%20tasks.%20The%20compilation%20and%20model%20are%20made%20openly%20available%20for%0Ause%20by%20the%20scientific%20community%20at%20https%3A//doi.org/10.20383/103.0614.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenthicNet%253A%2520A%2520global%2520compilation%2520of%2520seafloor%2520images%2520for%2520deep%2520learning%250A%2520%2520applications%26entry.906535625%3DScott%2520C.%2520Lowe%2520and%2520Benjamin%2520Misiuk%2520and%2520Isaac%2520Xu%2520and%2520Shakhboz%2520Abdulazizov%2520and%2520Amit%2520R.%2520Baroi%2520and%2520Alex%2520C.%2520Bastos%2520and%2520Merlin%2520Best%2520and%2520Vicki%2520Ferrini%2520and%2520Ariell%2520Friedman%2520and%2520Deborah%2520Hart%2520and%2520Ove%2520Hoegh-Guldberg%2520and%2520Daniel%2520Ierodiaconou%2520and%2520Julia%2520Mackin-McLaughlin%2520and%2520Kathryn%2520Markey%2520and%2520Pedro%2520S.%2520Menandro%2520and%2520Jacquomo%2520Monk%2520and%2520Shreya%2520Nemani%2520and%2520John%2520O%2527Brien%2520and%2520Elizabeth%2520Oh%2520and%2520Luba%2520Y.%2520Reshitnyk%2520and%2520Katleen%2520Robert%2520and%2520Chris%2520M.%2520Roelfsema%2520and%2520Jessica%2520A.%2520Sameoto%2520and%2520Alexandre%2520C.%2520G.%2520Schimel%2520and%2520Jordan%2520A.%2520Thomson%2520and%2520Brittany%2520R.%2520Wilson%2520and%2520Melisa%2520C.%2520Wong%2520and%2520Craig%2520J.%2520Brown%2520and%2520Thomas%2520Trappenberg%26entry.1292438233%3D%2520%2520Advances%2520in%2520underwater%2520imaging%2520enable%2520the%2520collection%2520of%2520extensive%2520seafloor%250Aimage%2520datasets%2520that%2520are%2520necessary%2520for%2520monitoring%2520important%2520benthic%2520ecosystems.%250AThe%2520ability%2520to%2520collect%2520seafloor%2520imagery%2520has%2520outpaced%2520our%2520capacity%2520to%2520analyze%250Ait%252C%2520hindering%2520expedient%2520mobilization%2520of%2520this%2520crucial%2520environmental%2520information.%250ARecent%2520machine%2520learning%2520approaches%2520provide%2520opportunities%2520to%2520increase%2520the%250Aefficiency%2520with%2520which%2520seafloor%2520image%2520datasets%2520are%2520analyzed%252C%2520yet%2520large%2520and%250Aconsistent%2520datasets%2520necessary%2520to%2520support%2520development%2520of%2520such%2520approaches%2520are%250Ascarce.%2520Here%2520we%2520present%2520BenthicNet%253A%2520a%2520global%2520compilation%2520of%2520seafloor%2520imagery%250Adesigned%2520to%2520support%2520the%2520training%2520and%2520evaluation%2520of%2520large-scale%2520image%250Arecognition%2520models.%2520An%2520initial%2520set%2520of%2520over%252011.4%2520million%2520images%2520was%2520collected%250Aand%2520curated%2520to%2520represent%2520a%2520diversity%2520of%2520seafloor%2520environments%2520using%2520a%250Arepresentative%2520subset%2520of%25201.3%2520million%2520images.%2520These%2520are%2520accompanied%2520by%25202.6%250Amillion%2520annotations%2520translated%2520to%2520the%2520CATAMI%2520scheme%252C%2520which%2520span%2520190%252C000%2520of%2520the%250Aimages.%2520A%2520large%2520deep%2520learning%2520model%2520was%2520trained%2520on%2520this%2520compilation%2520and%250Apreliminary%2520results%2520suggest%2520it%2520has%2520utility%2520for%2520automating%2520large%2520and%2520small-scale%250Aimage%2520analysis%2520tasks.%2520The%2520compilation%2520and%2520model%2520are%2520made%2520openly%2520available%2520for%250Ause%2520by%2520the%2520scientific%2520community%2520at%2520https%253A//doi.org/10.20383/103.0614.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications&entry.906535625=Scott%20C.%20Lowe%20and%20Benjamin%20Misiuk%20and%20Isaac%20Xu%20and%20Shakhboz%20Abdulazizov%20and%20Amit%20R.%20Baroi%20and%20Alex%20C.%20Bastos%20and%20Merlin%20Best%20and%20Vicki%20Ferrini%20and%20Ariell%20Friedman%20and%20Deborah%20Hart%20and%20Ove%20Hoegh-Guldberg%20and%20Daniel%20Ierodiaconou%20and%20Julia%20Mackin-McLaughlin%20and%20Kathryn%20Markey%20and%20Pedro%20S.%20Menandro%20and%20Jacquomo%20Monk%20and%20Shreya%20Nemani%20and%20John%20O%27Brien%20and%20Elizabeth%20Oh%20and%20Luba%20Y.%20Reshitnyk%20and%20Katleen%20Robert%20and%20Chris%20M.%20Roelfsema%20and%20Jessica%20A.%20Sameoto%20and%20Alexandre%20C.%20G.%20Schimel%20and%20Jordan%20A.%20Thomson%20and%20Brittany%20R.%20Wilson%20and%20Melisa%20C.%20Wong%20and%20Craig%20J.%20Brown%20and%20Thomas%20Trappenberg&entry.1292438233=%20%20Advances%20in%20underwater%20imaging%20enable%20the%20collection%20of%20extensive%20seafloor%0Aimage%20datasets%20that%20are%20necessary%20for%20monitoring%20important%20benthic%20ecosystems.%0AThe%20ability%20to%20collect%20seafloor%20imagery%20has%20outpaced%20our%20capacity%20to%20analyze%0Ait%2C%20hindering%20expedient%20mobilization%20of%20this%20crucial%20environmental%20information.%0ARecent%20machine%20learning%20approaches%20provide%20opportunities%20to%20increase%20the%0Aefficiency%20with%20which%20seafloor%20image%20datasets%20are%20analyzed%2C%20yet%20large%20and%0Aconsistent%20datasets%20necessary%20to%20support%20development%20of%20such%20approaches%20are%0Ascarce.%20Here%20we%20present%20BenthicNet%3A%20a%20global%20compilation%20of%20seafloor%20imagery%0Adesigned%20to%20support%20the%20training%20and%20evaluation%20of%20large-scale%20image%0Arecognition%20models.%20An%20initial%20set%20of%20over%2011.4%20million%20images%20was%20collected%0Aand%20curated%20to%20represent%20a%20diversity%20of%20seafloor%20environments%20using%20a%0Arepresentative%20subset%20of%201.3%20million%20images.%20These%20are%20accompanied%20by%202.6%0Amillion%20annotations%20translated%20to%20the%20CATAMI%20scheme%2C%20which%20span%20190%2C000%20of%20the%0Aimages.%20A%20large%20deep%20learning%20model%20was%20trained%20on%20this%20compilation%20and%0Apreliminary%20results%20suggest%20it%20has%20utility%20for%20automating%20large%20and%20small-scale%0Aimage%20analysis%20tasks.%20The%20compilation%20and%20model%20are%20made%20openly%20available%20for%0Ause%20by%20the%20scientific%20community%20at%20https%3A//doi.org/10.20383/103.0614.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05241v1&entry.124074799=Read"},
{"title": "Hybrid Convolutional Neural Networks with Reliability Guarantee", "author": "Hans Dermot Doran and Suzana Veljanovska", "abstract": "  Making AI safe and dependable requires the generation of dependable models\nand dependable execution of those models. We propose redundant execution as a\nwell-known technique that can be used to ensure reliable execution of the AI\nmodel. This generic technique will extend the application scope of\nAI-accelerators that do not feature well-documented safety or dependability\nproperties. Typical redundancy techniques incur at least double or triple the\ncomputational expense of the original. We adopt a co-design approach,\nintegrating reliable model execution with non-reliable execution, focusing that\nadditional computational expense only where it is strictly necessary. We\ndescribe the design, implementation and some preliminary results of a hybrid\nCNN.\n", "link": "http://arxiv.org/abs/2405.05146v1", "date": "2024-05-08", "relevancy": 2.0064, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5263}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4882}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Convolutional%20Neural%20Networks%20with%20Reliability%20Guarantee&body=Title%3A%20Hybrid%20Convolutional%20Neural%20Networks%20with%20Reliability%20Guarantee%0AAuthor%3A%20Hans%20Dermot%20Doran%20and%20Suzana%20Veljanovska%0AAbstract%3A%20%20%20Making%20AI%20safe%20and%20dependable%20requires%20the%20generation%20of%20dependable%20models%0Aand%20dependable%20execution%20of%20those%20models.%20We%20propose%20redundant%20execution%20as%20a%0Awell-known%20technique%20that%20can%20be%20used%20to%20ensure%20reliable%20execution%20of%20the%20AI%0Amodel.%20This%20generic%20technique%20will%20extend%20the%20application%20scope%20of%0AAI-accelerators%20that%20do%20not%20feature%20well-documented%20safety%20or%20dependability%0Aproperties.%20Typical%20redundancy%20techniques%20incur%20at%20least%20double%20or%20triple%20the%0Acomputational%20expense%20of%20the%20original.%20We%20adopt%20a%20co-design%20approach%2C%0Aintegrating%20reliable%20model%20execution%20with%20non-reliable%20execution%2C%20focusing%20that%0Aadditional%20computational%20expense%20only%20where%20it%20is%20strictly%20necessary.%20We%0Adescribe%20the%20design%2C%20implementation%20and%20some%20preliminary%20results%20of%20a%20hybrid%0ACNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Convolutional%2520Neural%2520Networks%2520with%2520Reliability%2520Guarantee%26entry.906535625%3DHans%2520Dermot%2520Doran%2520and%2520Suzana%2520Veljanovska%26entry.1292438233%3D%2520%2520Making%2520AI%2520safe%2520and%2520dependable%2520requires%2520the%2520generation%2520of%2520dependable%2520models%250Aand%2520dependable%2520execution%2520of%2520those%2520models.%2520We%2520propose%2520redundant%2520execution%2520as%2520a%250Awell-known%2520technique%2520that%2520can%2520be%2520used%2520to%2520ensure%2520reliable%2520execution%2520of%2520the%2520AI%250Amodel.%2520This%2520generic%2520technique%2520will%2520extend%2520the%2520application%2520scope%2520of%250AAI-accelerators%2520that%2520do%2520not%2520feature%2520well-documented%2520safety%2520or%2520dependability%250Aproperties.%2520Typical%2520redundancy%2520techniques%2520incur%2520at%2520least%2520double%2520or%2520triple%2520the%250Acomputational%2520expense%2520of%2520the%2520original.%2520We%2520adopt%2520a%2520co-design%2520approach%252C%250Aintegrating%2520reliable%2520model%2520execution%2520with%2520non-reliable%2520execution%252C%2520focusing%2520that%250Aadditional%2520computational%2520expense%2520only%2520where%2520it%2520is%2520strictly%2520necessary.%2520We%250Adescribe%2520the%2520design%252C%2520implementation%2520and%2520some%2520preliminary%2520results%2520of%2520a%2520hybrid%250ACNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Convolutional%20Neural%20Networks%20with%20Reliability%20Guarantee&entry.906535625=Hans%20Dermot%20Doran%20and%20Suzana%20Veljanovska&entry.1292438233=%20%20Making%20AI%20safe%20and%20dependable%20requires%20the%20generation%20of%20dependable%20models%0Aand%20dependable%20execution%20of%20those%20models.%20We%20propose%20redundant%20execution%20as%20a%0Awell-known%20technique%20that%20can%20be%20used%20to%20ensure%20reliable%20execution%20of%20the%20AI%0Amodel.%20This%20generic%20technique%20will%20extend%20the%20application%20scope%20of%0AAI-accelerators%20that%20do%20not%20feature%20well-documented%20safety%20or%20dependability%0Aproperties.%20Typical%20redundancy%20techniques%20incur%20at%20least%20double%20or%20triple%20the%0Acomputational%20expense%20of%20the%20original.%20We%20adopt%20a%20co-design%20approach%2C%0Aintegrating%20reliable%20model%20execution%20with%20non-reliable%20execution%2C%20focusing%20that%0Aadditional%20computational%20expense%20only%20where%20it%20is%20strictly%20necessary.%20We%0Adescribe%20the%20design%2C%20implementation%20and%20some%20preliminary%20results%20of%20a%20hybrid%0ACNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05146v1&entry.124074799=Read"},
{"title": "Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization\n  Algorithm for Deep Learning", "author": "Jiawu Tian and Liwei Xu and Xiaowei Zhang and Yongqi Li", "abstract": "  Training deep neural networks is a challenging task. In order to speed up\ntraining and enhance the performance of deep neural networks, we rectify the\nvanilla conjugate gradient as conjugate-gradient-like and incorporate it into\nthe generic Adam, and thus propose a new optimization algorithm named\nCG-like-Adam for deep learning. Specifically, both the first-order and the\nsecond-order moment estimation of generic Adam are replaced by the\nconjugate-gradient-like. Convergence analysis handles the cases where the\nexponential moving average coefficient of the first-order moment estimation is\nconstant and the first-order moment estimation is unbiased. Numerical\nexperiments show the superiority of the proposed algorithm based on the\nCIFAR10/100 dataset.\n", "link": "http://arxiv.org/abs/2404.01714v2", "date": "2024-05-08", "relevancy": 1.9949, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5522}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4659}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conjugate-Gradient-like%20Based%20Adaptive%20Moment%20Estimation%20Optimization%0A%20%20Algorithm%20for%20Deep%20Learning&body=Title%3A%20Conjugate-Gradient-like%20Based%20Adaptive%20Moment%20Estimation%20Optimization%0A%20%20Algorithm%20for%20Deep%20Learning%0AAuthor%3A%20Jiawu%20Tian%20and%20Liwei%20Xu%20and%20Xiaowei%20Zhang%20and%20Yongqi%20Li%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks%20is%20a%20challenging%20task.%20In%20order%20to%20speed%20up%0Atraining%20and%20enhance%20the%20performance%20of%20deep%20neural%20networks%2C%20we%20rectify%20the%0Avanilla%20conjugate%20gradient%20as%20conjugate-gradient-like%20and%20incorporate%20it%20into%0Athe%20generic%20Adam%2C%20and%20thus%20propose%20a%20new%20optimization%20algorithm%20named%0ACG-like-Adam%20for%20deep%20learning.%20Specifically%2C%20both%20the%20first-order%20and%20the%0Asecond-order%20moment%20estimation%20of%20generic%20Adam%20are%20replaced%20by%20the%0Aconjugate-gradient-like.%20Convergence%20analysis%20handles%20the%20cases%20where%20the%0Aexponential%20moving%20average%20coefficient%20of%20the%20first-order%20moment%20estimation%20is%0Aconstant%20and%20the%20first-order%20moment%20estimation%20is%20unbiased.%20Numerical%0Aexperiments%20show%20the%20superiority%20of%20the%20proposed%20algorithm%20based%20on%20the%0ACIFAR10/100%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConjugate-Gradient-like%2520Based%2520Adaptive%2520Moment%2520Estimation%2520Optimization%250A%2520%2520Algorithm%2520for%2520Deep%2520Learning%26entry.906535625%3DJiawu%2520Tian%2520and%2520Liwei%2520Xu%2520and%2520Xiaowei%2520Zhang%2520and%2520Yongqi%2520Li%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks%2520is%2520a%2520challenging%2520task.%2520In%2520order%2520to%2520speed%2520up%250Atraining%2520and%2520enhance%2520the%2520performance%2520of%2520deep%2520neural%2520networks%252C%2520we%2520rectify%2520the%250Avanilla%2520conjugate%2520gradient%2520as%2520conjugate-gradient-like%2520and%2520incorporate%2520it%2520into%250Athe%2520generic%2520Adam%252C%2520and%2520thus%2520propose%2520a%2520new%2520optimization%2520algorithm%2520named%250ACG-like-Adam%2520for%2520deep%2520learning.%2520Specifically%252C%2520both%2520the%2520first-order%2520and%2520the%250Asecond-order%2520moment%2520estimation%2520of%2520generic%2520Adam%2520are%2520replaced%2520by%2520the%250Aconjugate-gradient-like.%2520Convergence%2520analysis%2520handles%2520the%2520cases%2520where%2520the%250Aexponential%2520moving%2520average%2520coefficient%2520of%2520the%2520first-order%2520moment%2520estimation%2520is%250Aconstant%2520and%2520the%2520first-order%2520moment%2520estimation%2520is%2520unbiased.%2520Numerical%250Aexperiments%2520show%2520the%2520superiority%2520of%2520the%2520proposed%2520algorithm%2520based%2520on%2520the%250ACIFAR10/100%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conjugate-Gradient-like%20Based%20Adaptive%20Moment%20Estimation%20Optimization%0A%20%20Algorithm%20for%20Deep%20Learning&entry.906535625=Jiawu%20Tian%20and%20Liwei%20Xu%20and%20Xiaowei%20Zhang%20and%20Yongqi%20Li&entry.1292438233=%20%20Training%20deep%20neural%20networks%20is%20a%20challenging%20task.%20In%20order%20to%20speed%20up%0Atraining%20and%20enhance%20the%20performance%20of%20deep%20neural%20networks%2C%20we%20rectify%20the%0Avanilla%20conjugate%20gradient%20as%20conjugate-gradient-like%20and%20incorporate%20it%20into%0Athe%20generic%20Adam%2C%20and%20thus%20propose%20a%20new%20optimization%20algorithm%20named%0ACG-like-Adam%20for%20deep%20learning.%20Specifically%2C%20both%20the%20first-order%20and%20the%0Asecond-order%20moment%20estimation%20of%20generic%20Adam%20are%20replaced%20by%20the%0Aconjugate-gradient-like.%20Convergence%20analysis%20handles%20the%20cases%20where%20the%0Aexponential%20moving%20average%20coefficient%20of%20the%20first-order%20moment%20estimation%20is%0Aconstant%20and%20the%20first-order%20moment%20estimation%20is%20unbiased.%20Numerical%0Aexperiments%20show%20the%20superiority%20of%20the%20proposed%20algorithm%20based%20on%20the%0ACIFAR10/100%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01714v2&entry.124074799=Read"},
{"title": "Imprecise Probabilities Meet Partial Observability: Game Semantics for\n  Robust POMDPs", "author": "Eline M. Bovy and Marnix Suilen and Sebastian Junges and Nils Jansen", "abstract": "  Partially observable Markov decision processes (POMDPs) rely on the key\nassumption that probability distributions are precisely known. Robust POMDPs\n(RPOMDPs) alleviate this concern by defining imprecise probabilities, referred\nto as uncertainty sets. While robust MDPs have been studied extensively, work\non RPOMDPs is limited and primarily focuses on algorithmic solution methods. We\nexpand the theoretical understanding of RPOMDPs by showing that 1) different\nassumptions on the uncertainty sets affect optimal policies and values; 2)\nRPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the\nsame RPOMDP with different assumptions leads to semantically different POSGs\nand, thus, different policies and values. These novel semantics for RPOMDPS\ngive access to results for the widely studied POSG model; concretely, we show\nthe existence of a Nash equilibrium. Finally, we classify the existing RPOMDP\nliterature using our semantics, clarifying under which uncertainty assumptions\nthese existing works operate.\n", "link": "http://arxiv.org/abs/2405.04941v1", "date": "2024-05-08", "relevancy": 1.9934, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5077}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imprecise%20Probabilities%20Meet%20Partial%20Observability%3A%20Game%20Semantics%20for%0A%20%20Robust%20POMDPs&body=Title%3A%20Imprecise%20Probabilities%20Meet%20Partial%20Observability%3A%20Game%20Semantics%20for%0A%20%20Robust%20POMDPs%0AAuthor%3A%20Eline%20M.%20Bovy%20and%20Marnix%20Suilen%20and%20Sebastian%20Junges%20and%20Nils%20Jansen%0AAbstract%3A%20%20%20Partially%20observable%20Markov%20decision%20processes%20%28POMDPs%29%20rely%20on%20the%20key%0Aassumption%20that%20probability%20distributions%20are%20precisely%20known.%20Robust%20POMDPs%0A%28RPOMDPs%29%20alleviate%20this%20concern%20by%20defining%20imprecise%20probabilities%2C%20referred%0Ato%20as%20uncertainty%20sets.%20While%20robust%20MDPs%20have%20been%20studied%20extensively%2C%20work%0Aon%20RPOMDPs%20is%20limited%20and%20primarily%20focuses%20on%20algorithmic%20solution%20methods.%20We%0Aexpand%20the%20theoretical%20understanding%20of%20RPOMDPs%20by%20showing%20that%201%29%20different%0Aassumptions%20on%20the%20uncertainty%20sets%20affect%20optimal%20policies%20and%20values%3B%202%29%0ARPOMDPs%20have%20a%20partially%20observable%20stochastic%20game%20%28POSG%29%20semantic%3B%20and%203%29%20the%0Asame%20RPOMDP%20with%20different%20assumptions%20leads%20to%20semantically%20different%20POSGs%0Aand%2C%20thus%2C%20different%20policies%20and%20values.%20These%20novel%20semantics%20for%20RPOMDPS%0Agive%20access%20to%20results%20for%20the%20widely%20studied%20POSG%20model%3B%20concretely%2C%20we%20show%0Athe%20existence%20of%20a%20Nash%20equilibrium.%20Finally%2C%20we%20classify%20the%20existing%20RPOMDP%0Aliterature%20using%20our%20semantics%2C%20clarifying%20under%20which%20uncertainty%20assumptions%0Athese%20existing%20works%20operate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprecise%2520Probabilities%2520Meet%2520Partial%2520Observability%253A%2520Game%2520Semantics%2520for%250A%2520%2520Robust%2520POMDPs%26entry.906535625%3DEline%2520M.%2520Bovy%2520and%2520Marnix%2520Suilen%2520and%2520Sebastian%2520Junges%2520and%2520Nils%2520Jansen%26entry.1292438233%3D%2520%2520Partially%2520observable%2520Markov%2520decision%2520processes%2520%2528POMDPs%2529%2520rely%2520on%2520the%2520key%250Aassumption%2520that%2520probability%2520distributions%2520are%2520precisely%2520known.%2520Robust%2520POMDPs%250A%2528RPOMDPs%2529%2520alleviate%2520this%2520concern%2520by%2520defining%2520imprecise%2520probabilities%252C%2520referred%250Ato%2520as%2520uncertainty%2520sets.%2520While%2520robust%2520MDPs%2520have%2520been%2520studied%2520extensively%252C%2520work%250Aon%2520RPOMDPs%2520is%2520limited%2520and%2520primarily%2520focuses%2520on%2520algorithmic%2520solution%2520methods.%2520We%250Aexpand%2520the%2520theoretical%2520understanding%2520of%2520RPOMDPs%2520by%2520showing%2520that%25201%2529%2520different%250Aassumptions%2520on%2520the%2520uncertainty%2520sets%2520affect%2520optimal%2520policies%2520and%2520values%253B%25202%2529%250ARPOMDPs%2520have%2520a%2520partially%2520observable%2520stochastic%2520game%2520%2528POSG%2529%2520semantic%253B%2520and%25203%2529%2520the%250Asame%2520RPOMDP%2520with%2520different%2520assumptions%2520leads%2520to%2520semantically%2520different%2520POSGs%250Aand%252C%2520thus%252C%2520different%2520policies%2520and%2520values.%2520These%2520novel%2520semantics%2520for%2520RPOMDPS%250Agive%2520access%2520to%2520results%2520for%2520the%2520widely%2520studied%2520POSG%2520model%253B%2520concretely%252C%2520we%2520show%250Athe%2520existence%2520of%2520a%2520Nash%2520equilibrium.%2520Finally%252C%2520we%2520classify%2520the%2520existing%2520RPOMDP%250Aliterature%2520using%2520our%2520semantics%252C%2520clarifying%2520under%2520which%2520uncertainty%2520assumptions%250Athese%2520existing%2520works%2520operate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imprecise%20Probabilities%20Meet%20Partial%20Observability%3A%20Game%20Semantics%20for%0A%20%20Robust%20POMDPs&entry.906535625=Eline%20M.%20Bovy%20and%20Marnix%20Suilen%20and%20Sebastian%20Junges%20and%20Nils%20Jansen&entry.1292438233=%20%20Partially%20observable%20Markov%20decision%20processes%20%28POMDPs%29%20rely%20on%20the%20key%0Aassumption%20that%20probability%20distributions%20are%20precisely%20known.%20Robust%20POMDPs%0A%28RPOMDPs%29%20alleviate%20this%20concern%20by%20defining%20imprecise%20probabilities%2C%20referred%0Ato%20as%20uncertainty%20sets.%20While%20robust%20MDPs%20have%20been%20studied%20extensively%2C%20work%0Aon%20RPOMDPs%20is%20limited%20and%20primarily%20focuses%20on%20algorithmic%20solution%20methods.%20We%0Aexpand%20the%20theoretical%20understanding%20of%20RPOMDPs%20by%20showing%20that%201%29%20different%0Aassumptions%20on%20the%20uncertainty%20sets%20affect%20optimal%20policies%20and%20values%3B%202%29%0ARPOMDPs%20have%20a%20partially%20observable%20stochastic%20game%20%28POSG%29%20semantic%3B%20and%203%29%20the%0Asame%20RPOMDP%20with%20different%20assumptions%20leads%20to%20semantically%20different%20POSGs%0Aand%2C%20thus%2C%20different%20policies%20and%20values.%20These%20novel%20semantics%20for%20RPOMDPS%0Agive%20access%20to%20results%20for%20the%20widely%20studied%20POSG%20model%3B%20concretely%2C%20we%20show%0Athe%20existence%20of%20a%20Nash%20equilibrium.%20Finally%2C%20we%20classify%20the%20existing%20RPOMDP%0Aliterature%20using%20our%20semantics%2C%20clarifying%20under%20which%20uncertainty%20assumptions%0Athese%20existing%20works%20operate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04941v1&entry.124074799=Read"},
{"title": "Power Variable Projection for Initialization-Free Large-Scale Bundle\n  Adjustment", "author": "Simon Weber and Je Hyeong Hong and Daniel Cremers", "abstract": "  Initialization-free bundle adjustment (BA) remains largely uncharted. While\nLevenberg-Marquardt algorithm is the golden method to solve the BA problem, it\ngenerally relies on a good initialization. In contrast, the under-explored\nVariable Projection algorithm (VarPro) exhibits a wide convergence basin even\nwithout initialization. Coupled with object space error formulation, recent\nworks have shown its ability to solve (small-scale) initialization-free bundle\nadjustment problem. We introduce Power Variable Projection (PoVar), extending a\nrecent inverse expansion method based on power series. Importantly, we link the\npower series expansion to Riemannian manifold optimization. This projective\nframework is crucial to solve large-scale bundle adjustment problem without\ninitialization. Using the real-world BAL dataset, we experimentally demonstrate\nthat our solver achieves state-of-the-art results in terms of speed and\naccuracy. In particular, our work is the first, to our knowledge, that\naddresses the scalability of BA without initialization and opens new venues for\ninitialization-free Structure-from-Motion.\n", "link": "http://arxiv.org/abs/2405.05079v1", "date": "2024-05-08", "relevancy": 1.9908, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5045}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4966}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment&body=Title%3A%20Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment%0AAuthor%3A%20Simon%20Weber%20and%20Je%20Hyeong%20Hong%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Initialization-free%20bundle%20adjustment%20%28BA%29%20remains%20largely%20uncharted.%20While%0ALevenberg-Marquardt%20algorithm%20is%20the%20golden%20method%20to%20solve%20the%20BA%20problem%2C%20it%0Agenerally%20relies%20on%20a%20good%20initialization.%20In%20contrast%2C%20the%20under-explored%0AVariable%20Projection%20algorithm%20%28VarPro%29%20exhibits%20a%20wide%20convergence%20basin%20even%0Awithout%20initialization.%20Coupled%20with%20object%20space%20error%20formulation%2C%20recent%0Aworks%20have%20shown%20its%20ability%20to%20solve%20%28small-scale%29%20initialization-free%20bundle%0Aadjustment%20problem.%20We%20introduce%20Power%20Variable%20Projection%20%28PoVar%29%2C%20extending%20a%0Arecent%20inverse%20expansion%20method%20based%20on%20power%20series.%20Importantly%2C%20we%20link%20the%0Apower%20series%20expansion%20to%20Riemannian%20manifold%20optimization.%20This%20projective%0Aframework%20is%20crucial%20to%20solve%20large-scale%20bundle%20adjustment%20problem%20without%0Ainitialization.%20Using%20the%20real-world%20BAL%20dataset%2C%20we%20experimentally%20demonstrate%0Athat%20our%20solver%20achieves%20state-of-the-art%20results%20in%20terms%20of%20speed%20and%0Aaccuracy.%20In%20particular%2C%20our%20work%20is%20the%20first%2C%20to%20our%20knowledge%2C%20that%0Aaddresses%20the%20scalability%20of%20BA%20without%20initialization%20and%20opens%20new%20venues%20for%0Ainitialization-free%20Structure-from-Motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPower%2520Variable%2520Projection%2520for%2520Initialization-Free%2520Large-Scale%2520Bundle%250A%2520%2520Adjustment%26entry.906535625%3DSimon%2520Weber%2520and%2520Je%2520Hyeong%2520Hong%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Initialization-free%2520bundle%2520adjustment%2520%2528BA%2529%2520remains%2520largely%2520uncharted.%2520While%250ALevenberg-Marquardt%2520algorithm%2520is%2520the%2520golden%2520method%2520to%2520solve%2520the%2520BA%2520problem%252C%2520it%250Agenerally%2520relies%2520on%2520a%2520good%2520initialization.%2520In%2520contrast%252C%2520the%2520under-explored%250AVariable%2520Projection%2520algorithm%2520%2528VarPro%2529%2520exhibits%2520a%2520wide%2520convergence%2520basin%2520even%250Awithout%2520initialization.%2520Coupled%2520with%2520object%2520space%2520error%2520formulation%252C%2520recent%250Aworks%2520have%2520shown%2520its%2520ability%2520to%2520solve%2520%2528small-scale%2529%2520initialization-free%2520bundle%250Aadjustment%2520problem.%2520We%2520introduce%2520Power%2520Variable%2520Projection%2520%2528PoVar%2529%252C%2520extending%2520a%250Arecent%2520inverse%2520expansion%2520method%2520based%2520on%2520power%2520series.%2520Importantly%252C%2520we%2520link%2520the%250Apower%2520series%2520expansion%2520to%2520Riemannian%2520manifold%2520optimization.%2520This%2520projective%250Aframework%2520is%2520crucial%2520to%2520solve%2520large-scale%2520bundle%2520adjustment%2520problem%2520without%250Ainitialization.%2520Using%2520the%2520real-world%2520BAL%2520dataset%252C%2520we%2520experimentally%2520demonstrate%250Athat%2520our%2520solver%2520achieves%2520state-of-the-art%2520results%2520in%2520terms%2520of%2520speed%2520and%250Aaccuracy.%2520In%2520particular%252C%2520our%2520work%2520is%2520the%2520first%252C%2520to%2520our%2520knowledge%252C%2520that%250Aaddresses%2520the%2520scalability%2520of%2520BA%2520without%2520initialization%2520and%2520opens%2520new%2520venues%2520for%250Ainitialization-free%2520Structure-from-Motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Power%20Variable%20Projection%20for%20Initialization-Free%20Large-Scale%20Bundle%0A%20%20Adjustment&entry.906535625=Simon%20Weber%20and%20Je%20Hyeong%20Hong%20and%20Daniel%20Cremers&entry.1292438233=%20%20Initialization-free%20bundle%20adjustment%20%28BA%29%20remains%20largely%20uncharted.%20While%0ALevenberg-Marquardt%20algorithm%20is%20the%20golden%20method%20to%20solve%20the%20BA%20problem%2C%20it%0Agenerally%20relies%20on%20a%20good%20initialization.%20In%20contrast%2C%20the%20under-explored%0AVariable%20Projection%20algorithm%20%28VarPro%29%20exhibits%20a%20wide%20convergence%20basin%20even%0Awithout%20initialization.%20Coupled%20with%20object%20space%20error%20formulation%2C%20recent%0Aworks%20have%20shown%20its%20ability%20to%20solve%20%28small-scale%29%20initialization-free%20bundle%0Aadjustment%20problem.%20We%20introduce%20Power%20Variable%20Projection%20%28PoVar%29%2C%20extending%20a%0Arecent%20inverse%20expansion%20method%20based%20on%20power%20series.%20Importantly%2C%20we%20link%20the%0Apower%20series%20expansion%20to%20Riemannian%20manifold%20optimization.%20This%20projective%0Aframework%20is%20crucial%20to%20solve%20large-scale%20bundle%20adjustment%20problem%20without%0Ainitialization.%20Using%20the%20real-world%20BAL%20dataset%2C%20we%20experimentally%20demonstrate%0Athat%20our%20solver%20achieves%20state-of-the-art%20results%20in%20terms%20of%20speed%20and%0Aaccuracy.%20In%20particular%2C%20our%20work%20is%20the%20first%2C%20to%20our%20knowledge%2C%20that%0Aaddresses%20the%20scalability%20of%20BA%20without%20initialization%20and%20opens%20new%20venues%20for%0Ainitialization-free%20Structure-from-Motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05079v1&entry.124074799=Read"},
{"title": "Causal Flow-based Variational Auto-Encoder for Disentangled Causal\n  Representation Learning", "author": "Di Fan and Yannian Kou and Chuanhou Gao", "abstract": "  Disentangled representation learning aims to learn low-dimensional\nrepresentations of data, where each dimension corresponds to an underlying\ngenerative factor. Currently, Variational Auto-Encoder (VAE) are widely used\nfor disentangled representation learning, with the majority of methods assuming\nindependence among generative factors. However, in real-world scenarios,\ngenerative factors typically exhibit complex causal relationships. We thus\ndesign a new VAE-based framework named Disentangled Causal Variational\nAuto-Encoder (DCVAE), which includes a variant of autoregressive flows known as\ncausal flows, capable of learning effective causal disentangled\nrepresentations. We provide a theoretical analysis of the disentanglement\nidentifiability of DCVAE, ensuring that our model can effectively learn causal\ndisentangled representations. The performance of DCVAE is evaluated on both\nsynthetic and real-world datasets, demonstrating its outstanding capability in\nachieving causal disentanglement and performing intervention experiments.\nMoreover, DCVAE exhibits remarkable performance on downstream tasks and has the\npotential to learn the true causal structure among factors.\n", "link": "http://arxiv.org/abs/2304.09010v4", "date": "2024-05-08", "relevancy": 1.9905, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5053}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4929}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Flow-based%20Variational%20Auto-Encoder%20for%20Disentangled%20Causal%0A%20%20Representation%20Learning&body=Title%3A%20Causal%20Flow-based%20Variational%20Auto-Encoder%20for%20Disentangled%20Causal%0A%20%20Representation%20Learning%0AAuthor%3A%20Di%20Fan%20and%20Yannian%20Kou%20and%20Chuanhou%20Gao%0AAbstract%3A%20%20%20Disentangled%20representation%20learning%20aims%20to%20learn%20low-dimensional%0Arepresentations%20of%20data%2C%20where%20each%20dimension%20corresponds%20to%20an%20underlying%0Agenerative%20factor.%20Currently%2C%20Variational%20Auto-Encoder%20%28VAE%29%20are%20widely%20used%0Afor%20disentangled%20representation%20learning%2C%20with%20the%20majority%20of%20methods%20assuming%0Aindependence%20among%20generative%20factors.%20However%2C%20in%20real-world%20scenarios%2C%0Agenerative%20factors%20typically%20exhibit%20complex%20causal%20relationships.%20We%20thus%0Adesign%20a%20new%20VAE-based%20framework%20named%20Disentangled%20Causal%20Variational%0AAuto-Encoder%20%28DCVAE%29%2C%20which%20includes%20a%20variant%20of%20autoregressive%20flows%20known%20as%0Acausal%20flows%2C%20capable%20of%20learning%20effective%20causal%20disentangled%0Arepresentations.%20We%20provide%20a%20theoretical%20analysis%20of%20the%20disentanglement%0Aidentifiability%20of%20DCVAE%2C%20ensuring%20that%20our%20model%20can%20effectively%20learn%20causal%0Adisentangled%20representations.%20The%20performance%20of%20DCVAE%20is%20evaluated%20on%20both%0Asynthetic%20and%20real-world%20datasets%2C%20demonstrating%20its%20outstanding%20capability%20in%0Aachieving%20causal%20disentanglement%20and%20performing%20intervention%20experiments.%0AMoreover%2C%20DCVAE%20exhibits%20remarkable%20performance%20on%20downstream%20tasks%20and%20has%20the%0Apotential%20to%20learn%20the%20true%20causal%20structure%20among%20factors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09010v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Flow-based%2520Variational%2520Auto-Encoder%2520for%2520Disentangled%2520Causal%250A%2520%2520Representation%2520Learning%26entry.906535625%3DDi%2520Fan%2520and%2520Yannian%2520Kou%2520and%2520Chuanhou%2520Gao%26entry.1292438233%3D%2520%2520Disentangled%2520representation%2520learning%2520aims%2520to%2520learn%2520low-dimensional%250Arepresentations%2520of%2520data%252C%2520where%2520each%2520dimension%2520corresponds%2520to%2520an%2520underlying%250Agenerative%2520factor.%2520Currently%252C%2520Variational%2520Auto-Encoder%2520%2528VAE%2529%2520are%2520widely%2520used%250Afor%2520disentangled%2520representation%2520learning%252C%2520with%2520the%2520majority%2520of%2520methods%2520assuming%250Aindependence%2520among%2520generative%2520factors.%2520However%252C%2520in%2520real-world%2520scenarios%252C%250Agenerative%2520factors%2520typically%2520exhibit%2520complex%2520causal%2520relationships.%2520We%2520thus%250Adesign%2520a%2520new%2520VAE-based%2520framework%2520named%2520Disentangled%2520Causal%2520Variational%250AAuto-Encoder%2520%2528DCVAE%2529%252C%2520which%2520includes%2520a%2520variant%2520of%2520autoregressive%2520flows%2520known%2520as%250Acausal%2520flows%252C%2520capable%2520of%2520learning%2520effective%2520causal%2520disentangled%250Arepresentations.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520of%2520the%2520disentanglement%250Aidentifiability%2520of%2520DCVAE%252C%2520ensuring%2520that%2520our%2520model%2520can%2520effectively%2520learn%2520causal%250Adisentangled%2520representations.%2520The%2520performance%2520of%2520DCVAE%2520is%2520evaluated%2520on%2520both%250Asynthetic%2520and%2520real-world%2520datasets%252C%2520demonstrating%2520its%2520outstanding%2520capability%2520in%250Aachieving%2520causal%2520disentanglement%2520and%2520performing%2520intervention%2520experiments.%250AMoreover%252C%2520DCVAE%2520exhibits%2520remarkable%2520performance%2520on%2520downstream%2520tasks%2520and%2520has%2520the%250Apotential%2520to%2520learn%2520the%2520true%2520causal%2520structure%2520among%2520factors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.09010v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Flow-based%20Variational%20Auto-Encoder%20for%20Disentangled%20Causal%0A%20%20Representation%20Learning&entry.906535625=Di%20Fan%20and%20Yannian%20Kou%20and%20Chuanhou%20Gao&entry.1292438233=%20%20Disentangled%20representation%20learning%20aims%20to%20learn%20low-dimensional%0Arepresentations%20of%20data%2C%20where%20each%20dimension%20corresponds%20to%20an%20underlying%0Agenerative%20factor.%20Currently%2C%20Variational%20Auto-Encoder%20%28VAE%29%20are%20widely%20used%0Afor%20disentangled%20representation%20learning%2C%20with%20the%20majority%20of%20methods%20assuming%0Aindependence%20among%20generative%20factors.%20However%2C%20in%20real-world%20scenarios%2C%0Agenerative%20factors%20typically%20exhibit%20complex%20causal%20relationships.%20We%20thus%0Adesign%20a%20new%20VAE-based%20framework%20named%20Disentangled%20Causal%20Variational%0AAuto-Encoder%20%28DCVAE%29%2C%20which%20includes%20a%20variant%20of%20autoregressive%20flows%20known%20as%0Acausal%20flows%2C%20capable%20of%20learning%20effective%20causal%20disentangled%0Arepresentations.%20We%20provide%20a%20theoretical%20analysis%20of%20the%20disentanglement%0Aidentifiability%20of%20DCVAE%2C%20ensuring%20that%20our%20model%20can%20effectively%20learn%20causal%0Adisentangled%20representations.%20The%20performance%20of%20DCVAE%20is%20evaluated%20on%20both%0Asynthetic%20and%20real-world%20datasets%2C%20demonstrating%20its%20outstanding%20capability%20in%0Aachieving%20causal%20disentanglement%20and%20performing%20intervention%20experiments.%0AMoreover%2C%20DCVAE%20exhibits%20remarkable%20performance%20on%20downstream%20tasks%20and%20has%20the%0Apotential%20to%20learn%20the%20true%20causal%20structure%20among%20factors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09010v4&entry.124074799=Read"},
{"title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from\n  a Parametric Perspective", "author": "Ming Zhong and Chenxin An and Weizhu Chen and Jiawei Han and Pengcheng He", "abstract": "  Large Language Models (LLMs) inherently encode a wealth of knowledge within\ntheir parameters through pre-training on extensive corpora. While prior\nresearch has delved into operations on these parameters to manipulate the\nunderlying implicit knowledge (encompassing detection, editing, and merging),\nthere remains an ambiguous understanding regarding their transferability across\nmodels with varying scales. In this paper, we seek to empirically investigate\nknowledge transfer from larger to smaller models through a parametric\nperspective. To achieve this, we employ sensitivity-based techniques to extract\nand align knowledge-specific parameters between different LLMs. Moreover, the\nLoRA module is used as the intermediary mechanism for injecting the extracted\nknowledge into smaller models. Evaluations across four benchmarks validate the\nefficacy of our proposed method. Our findings highlight the critical factors\ncontributing to the process of parametric knowledge transfer, underscoring the\ntransferability of model parameters across LLMs of different scales. Project\nwebsite: https://maszhongming.github.io/ParaKnowTransfer.\n", "link": "http://arxiv.org/abs/2310.11451v2", "date": "2024-05-08", "relevancy": 1.988, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5005}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeking%20Neural%20Nuggets%3A%20Knowledge%20Transfer%20in%20Large%20Language%20Models%20from%0A%20%20a%20Parametric%20Perspective&body=Title%3A%20Seeking%20Neural%20Nuggets%3A%20Knowledge%20Transfer%20in%20Large%20Language%20Models%20from%0A%20%20a%20Parametric%20Perspective%0AAuthor%3A%20Ming%20Zhong%20and%20Chenxin%20An%20and%20Weizhu%20Chen%20and%20Jiawei%20Han%20and%20Pengcheng%20He%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20inherently%20encode%20a%20wealth%20of%20knowledge%20within%0Atheir%20parameters%20through%20pre-training%20on%20extensive%20corpora.%20While%20prior%0Aresearch%20has%20delved%20into%20operations%20on%20these%20parameters%20to%20manipulate%20the%0Aunderlying%20implicit%20knowledge%20%28encompassing%20detection%2C%20editing%2C%20and%20merging%29%2C%0Athere%20remains%20an%20ambiguous%20understanding%20regarding%20their%20transferability%20across%0Amodels%20with%20varying%20scales.%20In%20this%20paper%2C%20we%20seek%20to%20empirically%20investigate%0Aknowledge%20transfer%20from%20larger%20to%20smaller%20models%20through%20a%20parametric%0Aperspective.%20To%20achieve%20this%2C%20we%20employ%20sensitivity-based%20techniques%20to%20extract%0Aand%20align%20knowledge-specific%20parameters%20between%20different%20LLMs.%20Moreover%2C%20the%0ALoRA%20module%20is%20used%20as%20the%20intermediary%20mechanism%20for%20injecting%20the%20extracted%0Aknowledge%20into%20smaller%20models.%20Evaluations%20across%20four%20benchmarks%20validate%20the%0Aefficacy%20of%20our%20proposed%20method.%20Our%20findings%20highlight%20the%20critical%20factors%0Acontributing%20to%20the%20process%20of%20parametric%20knowledge%20transfer%2C%20underscoring%20the%0Atransferability%20of%20model%20parameters%20across%20LLMs%20of%20different%20scales.%20Project%0Awebsite%3A%20https%3A//maszhongming.github.io/ParaKnowTransfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeking%2520Neural%2520Nuggets%253A%2520Knowledge%2520Transfer%2520in%2520Large%2520Language%2520Models%2520from%250A%2520%2520a%2520Parametric%2520Perspective%26entry.906535625%3DMing%2520Zhong%2520and%2520Chenxin%2520An%2520and%2520Weizhu%2520Chen%2520and%2520Jiawei%2520Han%2520and%2520Pengcheng%2520He%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520inherently%2520encode%2520a%2520wealth%2520of%2520knowledge%2520within%250Atheir%2520parameters%2520through%2520pre-training%2520on%2520extensive%2520corpora.%2520While%2520prior%250Aresearch%2520has%2520delved%2520into%2520operations%2520on%2520these%2520parameters%2520to%2520manipulate%2520the%250Aunderlying%2520implicit%2520knowledge%2520%2528encompassing%2520detection%252C%2520editing%252C%2520and%2520merging%2529%252C%250Athere%2520remains%2520an%2520ambiguous%2520understanding%2520regarding%2520their%2520transferability%2520across%250Amodels%2520with%2520varying%2520scales.%2520In%2520this%2520paper%252C%2520we%2520seek%2520to%2520empirically%2520investigate%250Aknowledge%2520transfer%2520from%2520larger%2520to%2520smaller%2520models%2520through%2520a%2520parametric%250Aperspective.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520sensitivity-based%2520techniques%2520to%2520extract%250Aand%2520align%2520knowledge-specific%2520parameters%2520between%2520different%2520LLMs.%2520Moreover%252C%2520the%250ALoRA%2520module%2520is%2520used%2520as%2520the%2520intermediary%2520mechanism%2520for%2520injecting%2520the%2520extracted%250Aknowledge%2520into%2520smaller%2520models.%2520Evaluations%2520across%2520four%2520benchmarks%2520validate%2520the%250Aefficacy%2520of%2520our%2520proposed%2520method.%2520Our%2520findings%2520highlight%2520the%2520critical%2520factors%250Acontributing%2520to%2520the%2520process%2520of%2520parametric%2520knowledge%2520transfer%252C%2520underscoring%2520the%250Atransferability%2520of%2520model%2520parameters%2520across%2520LLMs%2520of%2520different%2520scales.%2520Project%250Awebsite%253A%2520https%253A//maszhongming.github.io/ParaKnowTransfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeking%20Neural%20Nuggets%3A%20Knowledge%20Transfer%20in%20Large%20Language%20Models%20from%0A%20%20a%20Parametric%20Perspective&entry.906535625=Ming%20Zhong%20and%20Chenxin%20An%20and%20Weizhu%20Chen%20and%20Jiawei%20Han%20and%20Pengcheng%20He&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20inherently%20encode%20a%20wealth%20of%20knowledge%20within%0Atheir%20parameters%20through%20pre-training%20on%20extensive%20corpora.%20While%20prior%0Aresearch%20has%20delved%20into%20operations%20on%20these%20parameters%20to%20manipulate%20the%0Aunderlying%20implicit%20knowledge%20%28encompassing%20detection%2C%20editing%2C%20and%20merging%29%2C%0Athere%20remains%20an%20ambiguous%20understanding%20regarding%20their%20transferability%20across%0Amodels%20with%20varying%20scales.%20In%20this%20paper%2C%20we%20seek%20to%20empirically%20investigate%0Aknowledge%20transfer%20from%20larger%20to%20smaller%20models%20through%20a%20parametric%0Aperspective.%20To%20achieve%20this%2C%20we%20employ%20sensitivity-based%20techniques%20to%20extract%0Aand%20align%20knowledge-specific%20parameters%20between%20different%20LLMs.%20Moreover%2C%20the%0ALoRA%20module%20is%20used%20as%20the%20intermediary%20mechanism%20for%20injecting%20the%20extracted%0Aknowledge%20into%20smaller%20models.%20Evaluations%20across%20four%20benchmarks%20validate%20the%0Aefficacy%20of%20our%20proposed%20method.%20Our%20findings%20highlight%20the%20critical%20factors%0Acontributing%20to%20the%20process%20of%20parametric%20knowledge%20transfer%2C%20underscoring%20the%0Atransferability%20of%20model%20parameters%20across%20LLMs%20of%20different%20scales.%20Project%0Awebsite%3A%20https%3A//maszhongming.github.io/ParaKnowTransfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11451v2&entry.124074799=Read"},
{"title": "D4C Glove-train: Solving the RPM and Bongard-logo Problem by\n  Circumscribing and Building Distribution for Concepts", "author": "Ruizhuo Song and Beiming Yuan", "abstract": "  This paper achieves noteworthy progress in the realm of abstract reasoning,\nparticularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo\nchallenges. Initially, we introduce Lico-Net, a novel baseline model that\nresolves RPM problems with remarkable accuracy. Leveraging this foundation, we\nadvance with the D3C approach, which advocates representing the underlying\nconcepts in abstract reasoning problems through distributions. This perspective\nenhances the performance of both Lico-Net and a baseline model excelling in\nBongard-Logo tasks. To bolster the computational efficiency of D3C, we present\nthe D3C-cos variant, offering a streamlined yet precise solution. Furthermore,\nwe propose the D2C method, redefining conceptual boundaries within these\ndomains and bridging the divide between high-level abstractions and their\nlower-dimensional counterparts. Finally, we extend our methodology to D4C,\nemploying adversarial techniques to refine conceptual boundaries further and\ndemonstrate substantial improvements in both RPM and Bongard-Logo challenges.\nOverall, our contributions present a fresh outlook and practical advancements\nin the field of abstract reasoning.\n", "link": "http://arxiv.org/abs/2403.03452v6", "date": "2024-05-08", "relevancy": 1.9862, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4976}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D4C%20Glove-train%3A%20Solving%20the%20RPM%20and%20Bongard-logo%20Problem%20by%0A%20%20Circumscribing%20and%20Building%20Distribution%20for%20Concepts&body=Title%3A%20D4C%20Glove-train%3A%20Solving%20the%20RPM%20and%20Bongard-logo%20Problem%20by%0A%20%20Circumscribing%20and%20Building%20Distribution%20for%20Concepts%0AAuthor%3A%20Ruizhuo%20Song%20and%20Beiming%20Yuan%0AAbstract%3A%20%20%20This%20paper%20achieves%20noteworthy%20progress%20in%20the%20realm%20of%20abstract%20reasoning%2C%0Aparticularly%20in%20addressing%20Raven%27s%20Progressive%20Matrices%20%28RPM%29%20and%20Bongard-Logo%0Achallenges.%20Initially%2C%20we%20introduce%20Lico-Net%2C%20a%20novel%20baseline%20model%20that%0Aresolves%20RPM%20problems%20with%20remarkable%20accuracy.%20Leveraging%20this%20foundation%2C%20we%0Aadvance%20with%20the%20D3C%20approach%2C%20which%20advocates%20representing%20the%20underlying%0Aconcepts%20in%20abstract%20reasoning%20problems%20through%20distributions.%20This%20perspective%0Aenhances%20the%20performance%20of%20both%20Lico-Net%20and%20a%20baseline%20model%20excelling%20in%0ABongard-Logo%20tasks.%20To%20bolster%20the%20computational%20efficiency%20of%20D3C%2C%20we%20present%0Athe%20D3C-cos%20variant%2C%20offering%20a%20streamlined%20yet%20precise%20solution.%20Furthermore%2C%0Awe%20propose%20the%20D2C%20method%2C%20redefining%20conceptual%20boundaries%20within%20these%0Adomains%20and%20bridging%20the%20divide%20between%20high-level%20abstractions%20and%20their%0Alower-dimensional%20counterparts.%20Finally%2C%20we%20extend%20our%20methodology%20to%20D4C%2C%0Aemploying%20adversarial%20techniques%20to%20refine%20conceptual%20boundaries%20further%20and%0Ademonstrate%20substantial%20improvements%20in%20both%20RPM%20and%20Bongard-Logo%20challenges.%0AOverall%2C%20our%20contributions%20present%20a%20fresh%20outlook%20and%20practical%20advancements%0Ain%20the%20field%20of%20abstract%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03452v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD4C%2520Glove-train%253A%2520Solving%2520the%2520RPM%2520and%2520Bongard-logo%2520Problem%2520by%250A%2520%2520Circumscribing%2520and%2520Building%2520Distribution%2520for%2520Concepts%26entry.906535625%3DRuizhuo%2520Song%2520and%2520Beiming%2520Yuan%26entry.1292438233%3D%2520%2520This%2520paper%2520achieves%2520noteworthy%2520progress%2520in%2520the%2520realm%2520of%2520abstract%2520reasoning%252C%250Aparticularly%2520in%2520addressing%2520Raven%2527s%2520Progressive%2520Matrices%2520%2528RPM%2529%2520and%2520Bongard-Logo%250Achallenges.%2520Initially%252C%2520we%2520introduce%2520Lico-Net%252C%2520a%2520novel%2520baseline%2520model%2520that%250Aresolves%2520RPM%2520problems%2520with%2520remarkable%2520accuracy.%2520Leveraging%2520this%2520foundation%252C%2520we%250Aadvance%2520with%2520the%2520D3C%2520approach%252C%2520which%2520advocates%2520representing%2520the%2520underlying%250Aconcepts%2520in%2520abstract%2520reasoning%2520problems%2520through%2520distributions.%2520This%2520perspective%250Aenhances%2520the%2520performance%2520of%2520both%2520Lico-Net%2520and%2520a%2520baseline%2520model%2520excelling%2520in%250ABongard-Logo%2520tasks.%2520To%2520bolster%2520the%2520computational%2520efficiency%2520of%2520D3C%252C%2520we%2520present%250Athe%2520D3C-cos%2520variant%252C%2520offering%2520a%2520streamlined%2520yet%2520precise%2520solution.%2520Furthermore%252C%250Awe%2520propose%2520the%2520D2C%2520method%252C%2520redefining%2520conceptual%2520boundaries%2520within%2520these%250Adomains%2520and%2520bridging%2520the%2520divide%2520between%2520high-level%2520abstractions%2520and%2520their%250Alower-dimensional%2520counterparts.%2520Finally%252C%2520we%2520extend%2520our%2520methodology%2520to%2520D4C%252C%250Aemploying%2520adversarial%2520techniques%2520to%2520refine%2520conceptual%2520boundaries%2520further%2520and%250Ademonstrate%2520substantial%2520improvements%2520in%2520both%2520RPM%2520and%2520Bongard-Logo%2520challenges.%250AOverall%252C%2520our%2520contributions%2520present%2520a%2520fresh%2520outlook%2520and%2520practical%2520advancements%250Ain%2520the%2520field%2520of%2520abstract%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03452v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D4C%20Glove-train%3A%20Solving%20the%20RPM%20and%20Bongard-logo%20Problem%20by%0A%20%20Circumscribing%20and%20Building%20Distribution%20for%20Concepts&entry.906535625=Ruizhuo%20Song%20and%20Beiming%20Yuan&entry.1292438233=%20%20This%20paper%20achieves%20noteworthy%20progress%20in%20the%20realm%20of%20abstract%20reasoning%2C%0Aparticularly%20in%20addressing%20Raven%27s%20Progressive%20Matrices%20%28RPM%29%20and%20Bongard-Logo%0Achallenges.%20Initially%2C%20we%20introduce%20Lico-Net%2C%20a%20novel%20baseline%20model%20that%0Aresolves%20RPM%20problems%20with%20remarkable%20accuracy.%20Leveraging%20this%20foundation%2C%20we%0Aadvance%20with%20the%20D3C%20approach%2C%20which%20advocates%20representing%20the%20underlying%0Aconcepts%20in%20abstract%20reasoning%20problems%20through%20distributions.%20This%20perspective%0Aenhances%20the%20performance%20of%20both%20Lico-Net%20and%20a%20baseline%20model%20excelling%20in%0ABongard-Logo%20tasks.%20To%20bolster%20the%20computational%20efficiency%20of%20D3C%2C%20we%20present%0Athe%20D3C-cos%20variant%2C%20offering%20a%20streamlined%20yet%20precise%20solution.%20Furthermore%2C%0Awe%20propose%20the%20D2C%20method%2C%20redefining%20conceptual%20boundaries%20within%20these%0Adomains%20and%20bridging%20the%20divide%20between%20high-level%20abstractions%20and%20their%0Alower-dimensional%20counterparts.%20Finally%2C%20we%20extend%20our%20methodology%20to%20D4C%2C%0Aemploying%20adversarial%20techniques%20to%20refine%20conceptual%20boundaries%20further%20and%0Ademonstrate%20substantial%20improvements%20in%20both%20RPM%20and%20Bongard-Logo%20challenges.%0AOverall%2C%20our%20contributions%20present%20a%20fresh%20outlook%20and%20practical%20advancements%0Ain%20the%20field%20of%20abstract%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03452v6&entry.124074799=Read"},
{"title": "Supervised Anomaly Detection for Complex Industrial Images", "author": "Aimira Baitieva and David Hurych and Victor Besnier and Olivier Bernard", "abstract": "  Automating visual inspection in industrial production lines is essential for\nincreasing product quality across various industries. Anomaly detection (AD)\nmethods serve as robust tools for this purpose. However, existing public\ndatasets primarily consist of images without anomalies, limiting the practical\napplication of AD methods in production settings. To address this challenge, we\npresent (1) the Valeo Anomaly Dataset (VAD), a novel real-world industrial\ndataset comprising 5000 images, including 2000 instances of challenging real\ndefects across more than 20 subclasses. Acknowledging that traditional AD\nmethods struggle with this dataset, we introduce (2) Segmentation-based Anomaly\nDetector (SegAD). First, SegAD leverages anomaly maps as well as segmentation\nmaps to compute local statistics. Next, SegAD uses these statistics and an\noptional supervised classifier score as input features for a Boosted Random\nForest (BRF) classifier, yielding the final anomaly score. Our SegAD achieves\nstate-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset\n(+0.4% AUROC). The code and the models are publicly available.\n", "link": "http://arxiv.org/abs/2405.04953v1", "date": "2024-05-08", "relevancy": 1.975, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.504}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4871}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Anomaly%20Detection%20for%20Complex%20Industrial%20Images&body=Title%3A%20Supervised%20Anomaly%20Detection%20for%20Complex%20Industrial%20Images%0AAuthor%3A%20Aimira%20Baitieva%20and%20David%20Hurych%20and%20Victor%20Besnier%20and%20Olivier%20Bernard%0AAbstract%3A%20%20%20Automating%20visual%20inspection%20in%20industrial%20production%20lines%20is%20essential%20for%0Aincreasing%20product%20quality%20across%20various%20industries.%20Anomaly%20detection%20%28AD%29%0Amethods%20serve%20as%20robust%20tools%20for%20this%20purpose.%20However%2C%20existing%20public%0Adatasets%20primarily%20consist%20of%20images%20without%20anomalies%2C%20limiting%20the%20practical%0Aapplication%20of%20AD%20methods%20in%20production%20settings.%20To%20address%20this%20challenge%2C%20we%0Apresent%20%281%29%20the%20Valeo%20Anomaly%20Dataset%20%28VAD%29%2C%20a%20novel%20real-world%20industrial%0Adataset%20comprising%205000%20images%2C%20including%202000%20instances%20of%20challenging%20real%0Adefects%20across%20more%20than%2020%20subclasses.%20Acknowledging%20that%20traditional%20AD%0Amethods%20struggle%20with%20this%20dataset%2C%20we%20introduce%20%282%29%20Segmentation-based%20Anomaly%0ADetector%20%28SegAD%29.%20First%2C%20SegAD%20leverages%20anomaly%20maps%20as%20well%20as%20segmentation%0Amaps%20to%20compute%20local%20statistics.%20Next%2C%20SegAD%20uses%20these%20statistics%20and%20an%0Aoptional%20supervised%20classifier%20score%20as%20input%20features%20for%20a%20Boosted%20Random%0AForest%20%28BRF%29%20classifier%2C%20yielding%20the%20final%20anomaly%20score.%20Our%20SegAD%20achieves%0Astate-of-the-art%20performance%20on%20both%20VAD%20%28%2B2.1%25%20AUROC%29%20and%20the%20VisA%20dataset%0A%28%2B0.4%25%20AUROC%29.%20The%20code%20and%20the%20models%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Anomaly%2520Detection%2520for%2520Complex%2520Industrial%2520Images%26entry.906535625%3DAimira%2520Baitieva%2520and%2520David%2520Hurych%2520and%2520Victor%2520Besnier%2520and%2520Olivier%2520Bernard%26entry.1292438233%3D%2520%2520Automating%2520visual%2520inspection%2520in%2520industrial%2520production%2520lines%2520is%2520essential%2520for%250Aincreasing%2520product%2520quality%2520across%2520various%2520industries.%2520Anomaly%2520detection%2520%2528AD%2529%250Amethods%2520serve%2520as%2520robust%2520tools%2520for%2520this%2520purpose.%2520However%252C%2520existing%2520public%250Adatasets%2520primarily%2520consist%2520of%2520images%2520without%2520anomalies%252C%2520limiting%2520the%2520practical%250Aapplication%2520of%2520AD%2520methods%2520in%2520production%2520settings.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apresent%2520%25281%2529%2520the%2520Valeo%2520Anomaly%2520Dataset%2520%2528VAD%2529%252C%2520a%2520novel%2520real-world%2520industrial%250Adataset%2520comprising%25205000%2520images%252C%2520including%25202000%2520instances%2520of%2520challenging%2520real%250Adefects%2520across%2520more%2520than%252020%2520subclasses.%2520Acknowledging%2520that%2520traditional%2520AD%250Amethods%2520struggle%2520with%2520this%2520dataset%252C%2520we%2520introduce%2520%25282%2529%2520Segmentation-based%2520Anomaly%250ADetector%2520%2528SegAD%2529.%2520First%252C%2520SegAD%2520leverages%2520anomaly%2520maps%2520as%2520well%2520as%2520segmentation%250Amaps%2520to%2520compute%2520local%2520statistics.%2520Next%252C%2520SegAD%2520uses%2520these%2520statistics%2520and%2520an%250Aoptional%2520supervised%2520classifier%2520score%2520as%2520input%2520features%2520for%2520a%2520Boosted%2520Random%250AForest%2520%2528BRF%2529%2520classifier%252C%2520yielding%2520the%2520final%2520anomaly%2520score.%2520Our%2520SegAD%2520achieves%250Astate-of-the-art%2520performance%2520on%2520both%2520VAD%2520%2528%252B2.1%2525%2520AUROC%2529%2520and%2520the%2520VisA%2520dataset%250A%2528%252B0.4%2525%2520AUROC%2529.%2520The%2520code%2520and%2520the%2520models%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Anomaly%20Detection%20for%20Complex%20Industrial%20Images&entry.906535625=Aimira%20Baitieva%20and%20David%20Hurych%20and%20Victor%20Besnier%20and%20Olivier%20Bernard&entry.1292438233=%20%20Automating%20visual%20inspection%20in%20industrial%20production%20lines%20is%20essential%20for%0Aincreasing%20product%20quality%20across%20various%20industries.%20Anomaly%20detection%20%28AD%29%0Amethods%20serve%20as%20robust%20tools%20for%20this%20purpose.%20However%2C%20existing%20public%0Adatasets%20primarily%20consist%20of%20images%20without%20anomalies%2C%20limiting%20the%20practical%0Aapplication%20of%20AD%20methods%20in%20production%20settings.%20To%20address%20this%20challenge%2C%20we%0Apresent%20%281%29%20the%20Valeo%20Anomaly%20Dataset%20%28VAD%29%2C%20a%20novel%20real-world%20industrial%0Adataset%20comprising%205000%20images%2C%20including%202000%20instances%20of%20challenging%20real%0Adefects%20across%20more%20than%2020%20subclasses.%20Acknowledging%20that%20traditional%20AD%0Amethods%20struggle%20with%20this%20dataset%2C%20we%20introduce%20%282%29%20Segmentation-based%20Anomaly%0ADetector%20%28SegAD%29.%20First%2C%20SegAD%20leverages%20anomaly%20maps%20as%20well%20as%20segmentation%0Amaps%20to%20compute%20local%20statistics.%20Next%2C%20SegAD%20uses%20these%20statistics%20and%20an%0Aoptional%20supervised%20classifier%20score%20as%20input%20features%20for%20a%20Boosted%20Random%0AForest%20%28BRF%29%20classifier%2C%20yielding%20the%20final%20anomaly%20score.%20Our%20SegAD%20achieves%0Astate-of-the-art%20performance%20on%20both%20VAD%20%28%2B2.1%25%20AUROC%29%20and%20the%20VisA%20dataset%0A%28%2B0.4%25%20AUROC%29.%20The%20code%20and%20the%20models%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04953v1&entry.124074799=Read"},
{"title": "Custom Gradient Estimators are Straight-Through Estimators in Disguise", "author": "Matt Schoenbauer and Daniele Moro and Lukasz Lew and Andrew Howard", "abstract": "  Quantization-aware training comes with a fundamental challenge: the\nderivative of quantization functions such as rounding are zero almost\neverywhere and nonexistent elsewhere. Various differentiable approximations of\nquantization functions have been proposed to address this issue. In this paper,\nwe prove that when the learning rate is sufficiently small, a large class of\nweight gradient estimators is equivalent with the straight through estimator\n(STE). Specifically, after swapping in the STE and adjusting both the weight\ninitialization and the learning rate in SGD, the model will train in almost\nexactly the same way as it did with the original gradient estimator. Moreover,\nwe show that for adaptive learning rate algorithms like Adam, the same result\ncan be seen without any modifications to the weight initialization and learning\nrate. We experimentally show that these results hold for both a small\nconvolutional model trained on the MNIST dataset and for a ResNet50 model\ntrained on ImageNet.\n", "link": "http://arxiv.org/abs/2405.05171v1", "date": "2024-05-08", "relevancy": 1.9669, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5041}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4871}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Custom%20Gradient%20Estimators%20are%20Straight-Through%20Estimators%20in%20Disguise&body=Title%3A%20Custom%20Gradient%20Estimators%20are%20Straight-Through%20Estimators%20in%20Disguise%0AAuthor%3A%20Matt%20Schoenbauer%20and%20Daniele%20Moro%20and%20Lukasz%20Lew%20and%20Andrew%20Howard%0AAbstract%3A%20%20%20Quantization-aware%20training%20comes%20with%20a%20fundamental%20challenge%3A%20the%0Aderivative%20of%20quantization%20functions%20such%20as%20rounding%20are%20zero%20almost%0Aeverywhere%20and%20nonexistent%20elsewhere.%20Various%20differentiable%20approximations%20of%0Aquantization%20functions%20have%20been%20proposed%20to%20address%20this%20issue.%20In%20this%20paper%2C%0Awe%20prove%20that%20when%20the%20learning%20rate%20is%20sufficiently%20small%2C%20a%20large%20class%20of%0Aweight%20gradient%20estimators%20is%20equivalent%20with%20the%20straight%20through%20estimator%0A%28STE%29.%20Specifically%2C%20after%20swapping%20in%20the%20STE%20and%20adjusting%20both%20the%20weight%0Ainitialization%20and%20the%20learning%20rate%20in%20SGD%2C%20the%20model%20will%20train%20in%20almost%0Aexactly%20the%20same%20way%20as%20it%20did%20with%20the%20original%20gradient%20estimator.%20Moreover%2C%0Awe%20show%20that%20for%20adaptive%20learning%20rate%20algorithms%20like%20Adam%2C%20the%20same%20result%0Acan%20be%20seen%20without%20any%20modifications%20to%20the%20weight%20initialization%20and%20learning%0Arate.%20We%20experimentally%20show%20that%20these%20results%20hold%20for%20both%20a%20small%0Aconvolutional%20model%20trained%20on%20the%20MNIST%20dataset%20and%20for%20a%20ResNet50%20model%0Atrained%20on%20ImageNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustom%2520Gradient%2520Estimators%2520are%2520Straight-Through%2520Estimators%2520in%2520Disguise%26entry.906535625%3DMatt%2520Schoenbauer%2520and%2520Daniele%2520Moro%2520and%2520Lukasz%2520Lew%2520and%2520Andrew%2520Howard%26entry.1292438233%3D%2520%2520Quantization-aware%2520training%2520comes%2520with%2520a%2520fundamental%2520challenge%253A%2520the%250Aderivative%2520of%2520quantization%2520functions%2520such%2520as%2520rounding%2520are%2520zero%2520almost%250Aeverywhere%2520and%2520nonexistent%2520elsewhere.%2520Various%2520differentiable%2520approximations%2520of%250Aquantization%2520functions%2520have%2520been%2520proposed%2520to%2520address%2520this%2520issue.%2520In%2520this%2520paper%252C%250Awe%2520prove%2520that%2520when%2520the%2520learning%2520rate%2520is%2520sufficiently%2520small%252C%2520a%2520large%2520class%2520of%250Aweight%2520gradient%2520estimators%2520is%2520equivalent%2520with%2520the%2520straight%2520through%2520estimator%250A%2528STE%2529.%2520Specifically%252C%2520after%2520swapping%2520in%2520the%2520STE%2520and%2520adjusting%2520both%2520the%2520weight%250Ainitialization%2520and%2520the%2520learning%2520rate%2520in%2520SGD%252C%2520the%2520model%2520will%2520train%2520in%2520almost%250Aexactly%2520the%2520same%2520way%2520as%2520it%2520did%2520with%2520the%2520original%2520gradient%2520estimator.%2520Moreover%252C%250Awe%2520show%2520that%2520for%2520adaptive%2520learning%2520rate%2520algorithms%2520like%2520Adam%252C%2520the%2520same%2520result%250Acan%2520be%2520seen%2520without%2520any%2520modifications%2520to%2520the%2520weight%2520initialization%2520and%2520learning%250Arate.%2520We%2520experimentally%2520show%2520that%2520these%2520results%2520hold%2520for%2520both%2520a%2520small%250Aconvolutional%2520model%2520trained%2520on%2520the%2520MNIST%2520dataset%2520and%2520for%2520a%2520ResNet50%2520model%250Atrained%2520on%2520ImageNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Custom%20Gradient%20Estimators%20are%20Straight-Through%20Estimators%20in%20Disguise&entry.906535625=Matt%20Schoenbauer%20and%20Daniele%20Moro%20and%20Lukasz%20Lew%20and%20Andrew%20Howard&entry.1292438233=%20%20Quantization-aware%20training%20comes%20with%20a%20fundamental%20challenge%3A%20the%0Aderivative%20of%20quantization%20functions%20such%20as%20rounding%20are%20zero%20almost%0Aeverywhere%20and%20nonexistent%20elsewhere.%20Various%20differentiable%20approximations%20of%0Aquantization%20functions%20have%20been%20proposed%20to%20address%20this%20issue.%20In%20this%20paper%2C%0Awe%20prove%20that%20when%20the%20learning%20rate%20is%20sufficiently%20small%2C%20a%20large%20class%20of%0Aweight%20gradient%20estimators%20is%20equivalent%20with%20the%20straight%20through%20estimator%0A%28STE%29.%20Specifically%2C%20after%20swapping%20in%20the%20STE%20and%20adjusting%20both%20the%20weight%0Ainitialization%20and%20the%20learning%20rate%20in%20SGD%2C%20the%20model%20will%20train%20in%20almost%0Aexactly%20the%20same%20way%20as%20it%20did%20with%20the%20original%20gradient%20estimator.%20Moreover%2C%0Awe%20show%20that%20for%20adaptive%20learning%20rate%20algorithms%20like%20Adam%2C%20the%20same%20result%0Acan%20be%20seen%20without%20any%20modifications%20to%20the%20weight%20initialization%20and%20learning%0Arate.%20We%20experimentally%20show%20that%20these%20results%20hold%20for%20both%20a%20small%0Aconvolutional%20model%20trained%20on%20the%20MNIST%20dataset%20and%20for%20a%20ResNet50%20model%0Atrained%20on%20ImageNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05171v1&entry.124074799=Read"},
{"title": "PACIA: Parameter-Efficient Adapter for Few-Shot Molecular Property\n  Prediction", "author": "Shiguang Wu and Yaqing Wang and Quanming Yao", "abstract": "  Molecular property prediction (MPP) plays a crucial role in biomedical\napplications, but it often encounters challenges due to a scarcity of labeled\ndata. Existing works commonly adopt gradient-based strategy to update a large\namount of parameters for task-level adaptation. However, the increase of\nadaptive parameters can lead to overfitting and poor performance. Observing\nthat graph neural network (GNN) performs well as both encoder and predictor, we\npropose PACIA, a parameter-efficient GNN adapter for few-shot MPP. We design a\nunified adapter to generate a few adaptive parameters to modulate the message\npassing process of GNN. We then adopt a hierarchical adaptation mechanism to\nadapt the encoder at task-level and the predictor at query-level by the unified\nGNN adapter. Extensive results show that PACIA obtains the state-of-the-art\nperformance in few-shot MPP problems, and our proposed hierarchical adaptation\nmechanism is rational and effective.\n", "link": "http://arxiv.org/abs/2310.00614v2", "date": "2024-05-08", "relevancy": 1.9669, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACIA%3A%20Parameter-Efficient%20Adapter%20for%20Few-Shot%20Molecular%20Property%0A%20%20Prediction&body=Title%3A%20PACIA%3A%20Parameter-Efficient%20Adapter%20for%20Few-Shot%20Molecular%20Property%0A%20%20Prediction%0AAuthor%3A%20Shiguang%20Wu%20and%20Yaqing%20Wang%20and%20Quanming%20Yao%0AAbstract%3A%20%20%20Molecular%20property%20prediction%20%28MPP%29%20plays%20a%20crucial%20role%20in%20biomedical%0Aapplications%2C%20but%20it%20often%20encounters%20challenges%20due%20to%20a%20scarcity%20of%20labeled%0Adata.%20Existing%20works%20commonly%20adopt%20gradient-based%20strategy%20to%20update%20a%20large%0Aamount%20of%20parameters%20for%20task-level%20adaptation.%20However%2C%20the%20increase%20of%0Aadaptive%20parameters%20can%20lead%20to%20overfitting%20and%20poor%20performance.%20Observing%0Athat%20graph%20neural%20network%20%28GNN%29%20performs%20well%20as%20both%20encoder%20and%20predictor%2C%20we%0Apropose%20PACIA%2C%20a%20parameter-efficient%20GNN%20adapter%20for%20few-shot%20MPP.%20We%20design%20a%0Aunified%20adapter%20to%20generate%20a%20few%20adaptive%20parameters%20to%20modulate%20the%20message%0Apassing%20process%20of%20GNN.%20We%20then%20adopt%20a%20hierarchical%20adaptation%20mechanism%20to%0Aadapt%20the%20encoder%20at%20task-level%20and%20the%20predictor%20at%20query-level%20by%20the%20unified%0AGNN%20adapter.%20Extensive%20results%20show%20that%20PACIA%20obtains%20the%20state-of-the-art%0Aperformance%20in%20few-shot%20MPP%20problems%2C%20and%20our%20proposed%20hierarchical%20adaptation%0Amechanism%20is%20rational%20and%20effective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACIA%253A%2520Parameter-Efficient%2520Adapter%2520for%2520Few-Shot%2520Molecular%2520Property%250A%2520%2520Prediction%26entry.906535625%3DShiguang%2520Wu%2520and%2520Yaqing%2520Wang%2520and%2520Quanming%2520Yao%26entry.1292438233%3D%2520%2520Molecular%2520property%2520prediction%2520%2528MPP%2529%2520plays%2520a%2520crucial%2520role%2520in%2520biomedical%250Aapplications%252C%2520but%2520it%2520often%2520encounters%2520challenges%2520due%2520to%2520a%2520scarcity%2520of%2520labeled%250Adata.%2520Existing%2520works%2520commonly%2520adopt%2520gradient-based%2520strategy%2520to%2520update%2520a%2520large%250Aamount%2520of%2520parameters%2520for%2520task-level%2520adaptation.%2520However%252C%2520the%2520increase%2520of%250Aadaptive%2520parameters%2520can%2520lead%2520to%2520overfitting%2520and%2520poor%2520performance.%2520Observing%250Athat%2520graph%2520neural%2520network%2520%2528GNN%2529%2520performs%2520well%2520as%2520both%2520encoder%2520and%2520predictor%252C%2520we%250Apropose%2520PACIA%252C%2520a%2520parameter-efficient%2520GNN%2520adapter%2520for%2520few-shot%2520MPP.%2520We%2520design%2520a%250Aunified%2520adapter%2520to%2520generate%2520a%2520few%2520adaptive%2520parameters%2520to%2520modulate%2520the%2520message%250Apassing%2520process%2520of%2520GNN.%2520We%2520then%2520adopt%2520a%2520hierarchical%2520adaptation%2520mechanism%2520to%250Aadapt%2520the%2520encoder%2520at%2520task-level%2520and%2520the%2520predictor%2520at%2520query-level%2520by%2520the%2520unified%250AGNN%2520adapter.%2520Extensive%2520results%2520show%2520that%2520PACIA%2520obtains%2520the%2520state-of-the-art%250Aperformance%2520in%2520few-shot%2520MPP%2520problems%252C%2520and%2520our%2520proposed%2520hierarchical%2520adaptation%250Amechanism%2520is%2520rational%2520and%2520effective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACIA%3A%20Parameter-Efficient%20Adapter%20for%20Few-Shot%20Molecular%20Property%0A%20%20Prediction&entry.906535625=Shiguang%20Wu%20and%20Yaqing%20Wang%20and%20Quanming%20Yao&entry.1292438233=%20%20Molecular%20property%20prediction%20%28MPP%29%20plays%20a%20crucial%20role%20in%20biomedical%0Aapplications%2C%20but%20it%20often%20encounters%20challenges%20due%20to%20a%20scarcity%20of%20labeled%0Adata.%20Existing%20works%20commonly%20adopt%20gradient-based%20strategy%20to%20update%20a%20large%0Aamount%20of%20parameters%20for%20task-level%20adaptation.%20However%2C%20the%20increase%20of%0Aadaptive%20parameters%20can%20lead%20to%20overfitting%20and%20poor%20performance.%20Observing%0Athat%20graph%20neural%20network%20%28GNN%29%20performs%20well%20as%20both%20encoder%20and%20predictor%2C%20we%0Apropose%20PACIA%2C%20a%20parameter-efficient%20GNN%20adapter%20for%20few-shot%20MPP.%20We%20design%20a%0Aunified%20adapter%20to%20generate%20a%20few%20adaptive%20parameters%20to%20modulate%20the%20message%0Apassing%20process%20of%20GNN.%20We%20then%20adopt%20a%20hierarchical%20adaptation%20mechanism%20to%0Aadapt%20the%20encoder%20at%20task-level%20and%20the%20predictor%20at%20query-level%20by%20the%20unified%0AGNN%20adapter.%20Extensive%20results%20show%20that%20PACIA%20obtains%20the%20state-of-the-art%0Aperformance%20in%20few-shot%20MPP%20problems%2C%20and%20our%20proposed%20hierarchical%20adaptation%0Amechanism%20is%20rational%20and%20effective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00614v2&entry.124074799=Read"},
{"title": "Identifying every building's function in large-scale urban areas with\n  multi-modality remote-sensing data", "author": "Zhuohong Li and Wei He and Jiepan Li and Hongyan Zhang", "abstract": "  Buildings, as fundamental man-made structures in urban environments, serve as\ncrucial indicators for understanding various city function zones. Rapid\nurbanization has raised an urgent need for efficiently surveying building\nfootprints and functions. In this study, we proposed a semi-supervised\nframework to identify every building's function in large-scale urban areas with\nmulti-modality remote-sensing data. In detail, optical images, building height,\nand nighttime-light data are collected to describe the morphological attributes\nof buildings. Then, the area of interest (AOI) and building masks from the\nvolunteered geographic information (VGI) data are collected to form sparsely\nlabeled samples. Furthermore, the multi-modality data and weak labels are\nutilized to train a segmentation model with a semi-supervised strategy.\nFinally, results are evaluated by 20,000 validation points and statistical\nsurvey reports from the government. The evaluations reveal that the produced\nfunction maps achieve an OA of 82% and Kappa of 71% among 1,616,796 buildings\nin Shanghai, China. This study has the potential to support large-scale urban\nmanagement and sustainable urban development. All collected data and produced\nmaps are open access at https://github.com/LiZhuoHong/BuildingMap.\n", "link": "http://arxiv.org/abs/2405.05133v1", "date": "2024-05-08", "relevancy": 1.966, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5071}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.501}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20every%20building%27s%20function%20in%20large-scale%20urban%20areas%20with%0A%20%20multi-modality%20remote-sensing%20data&body=Title%3A%20Identifying%20every%20building%27s%20function%20in%20large-scale%20urban%20areas%20with%0A%20%20multi-modality%20remote-sensing%20data%0AAuthor%3A%20Zhuohong%20Li%20and%20Wei%20He%20and%20Jiepan%20Li%20and%20Hongyan%20Zhang%0AAbstract%3A%20%20%20Buildings%2C%20as%20fundamental%20man-made%20structures%20in%20urban%20environments%2C%20serve%20as%0Acrucial%20indicators%20for%20understanding%20various%20city%20function%20zones.%20Rapid%0Aurbanization%20has%20raised%20an%20urgent%20need%20for%20efficiently%20surveying%20building%0Afootprints%20and%20functions.%20In%20this%20study%2C%20we%20proposed%20a%20semi-supervised%0Aframework%20to%20identify%20every%20building%27s%20function%20in%20large-scale%20urban%20areas%20with%0Amulti-modality%20remote-sensing%20data.%20In%20detail%2C%20optical%20images%2C%20building%20height%2C%0Aand%20nighttime-light%20data%20are%20collected%20to%20describe%20the%20morphological%20attributes%0Aof%20buildings.%20Then%2C%20the%20area%20of%20interest%20%28AOI%29%20and%20building%20masks%20from%20the%0Avolunteered%20geographic%20information%20%28VGI%29%20data%20are%20collected%20to%20form%20sparsely%0Alabeled%20samples.%20Furthermore%2C%20the%20multi-modality%20data%20and%20weak%20labels%20are%0Autilized%20to%20train%20a%20segmentation%20model%20with%20a%20semi-supervised%20strategy.%0AFinally%2C%20results%20are%20evaluated%20by%2020%2C000%20validation%20points%20and%20statistical%0Asurvey%20reports%20from%20the%20government.%20The%20evaluations%20reveal%20that%20the%20produced%0Afunction%20maps%20achieve%20an%20OA%20of%2082%25%20and%20Kappa%20of%2071%25%20among%201%2C616%2C796%20buildings%0Ain%20Shanghai%2C%20China.%20This%20study%20has%20the%20potential%20to%20support%20large-scale%20urban%0Amanagement%20and%20sustainable%20urban%20development.%20All%20collected%20data%20and%20produced%0Amaps%20are%20open%20access%20at%20https%3A//github.com/LiZhuoHong/BuildingMap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520every%2520building%2527s%2520function%2520in%2520large-scale%2520urban%2520areas%2520with%250A%2520%2520multi-modality%2520remote-sensing%2520data%26entry.906535625%3DZhuohong%2520Li%2520and%2520Wei%2520He%2520and%2520Jiepan%2520Li%2520and%2520Hongyan%2520Zhang%26entry.1292438233%3D%2520%2520Buildings%252C%2520as%2520fundamental%2520man-made%2520structures%2520in%2520urban%2520environments%252C%2520serve%2520as%250Acrucial%2520indicators%2520for%2520understanding%2520various%2520city%2520function%2520zones.%2520Rapid%250Aurbanization%2520has%2520raised%2520an%2520urgent%2520need%2520for%2520efficiently%2520surveying%2520building%250Afootprints%2520and%2520functions.%2520In%2520this%2520study%252C%2520we%2520proposed%2520a%2520semi-supervised%250Aframework%2520to%2520identify%2520every%2520building%2527s%2520function%2520in%2520large-scale%2520urban%2520areas%2520with%250Amulti-modality%2520remote-sensing%2520data.%2520In%2520detail%252C%2520optical%2520images%252C%2520building%2520height%252C%250Aand%2520nighttime-light%2520data%2520are%2520collected%2520to%2520describe%2520the%2520morphological%2520attributes%250Aof%2520buildings.%2520Then%252C%2520the%2520area%2520of%2520interest%2520%2528AOI%2529%2520and%2520building%2520masks%2520from%2520the%250Avolunteered%2520geographic%2520information%2520%2528VGI%2529%2520data%2520are%2520collected%2520to%2520form%2520sparsely%250Alabeled%2520samples.%2520Furthermore%252C%2520the%2520multi-modality%2520data%2520and%2520weak%2520labels%2520are%250Autilized%2520to%2520train%2520a%2520segmentation%2520model%2520with%2520a%2520semi-supervised%2520strategy.%250AFinally%252C%2520results%2520are%2520evaluated%2520by%252020%252C000%2520validation%2520points%2520and%2520statistical%250Asurvey%2520reports%2520from%2520the%2520government.%2520The%2520evaluations%2520reveal%2520that%2520the%2520produced%250Afunction%2520maps%2520achieve%2520an%2520OA%2520of%252082%2525%2520and%2520Kappa%2520of%252071%2525%2520among%25201%252C616%252C796%2520buildings%250Ain%2520Shanghai%252C%2520China.%2520This%2520study%2520has%2520the%2520potential%2520to%2520support%2520large-scale%2520urban%250Amanagement%2520and%2520sustainable%2520urban%2520development.%2520All%2520collected%2520data%2520and%2520produced%250Amaps%2520are%2520open%2520access%2520at%2520https%253A//github.com/LiZhuoHong/BuildingMap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20every%20building%27s%20function%20in%20large-scale%20urban%20areas%20with%0A%20%20multi-modality%20remote-sensing%20data&entry.906535625=Zhuohong%20Li%20and%20Wei%20He%20and%20Jiepan%20Li%20and%20Hongyan%20Zhang&entry.1292438233=%20%20Buildings%2C%20as%20fundamental%20man-made%20structures%20in%20urban%20environments%2C%20serve%20as%0Acrucial%20indicators%20for%20understanding%20various%20city%20function%20zones.%20Rapid%0Aurbanization%20has%20raised%20an%20urgent%20need%20for%20efficiently%20surveying%20building%0Afootprints%20and%20functions.%20In%20this%20study%2C%20we%20proposed%20a%20semi-supervised%0Aframework%20to%20identify%20every%20building%27s%20function%20in%20large-scale%20urban%20areas%20with%0Amulti-modality%20remote-sensing%20data.%20In%20detail%2C%20optical%20images%2C%20building%20height%2C%0Aand%20nighttime-light%20data%20are%20collected%20to%20describe%20the%20morphological%20attributes%0Aof%20buildings.%20Then%2C%20the%20area%20of%20interest%20%28AOI%29%20and%20building%20masks%20from%20the%0Avolunteered%20geographic%20information%20%28VGI%29%20data%20are%20collected%20to%20form%20sparsely%0Alabeled%20samples.%20Furthermore%2C%20the%20multi-modality%20data%20and%20weak%20labels%20are%0Autilized%20to%20train%20a%20segmentation%20model%20with%20a%20semi-supervised%20strategy.%0AFinally%2C%20results%20are%20evaluated%20by%2020%2C000%20validation%20points%20and%20statistical%0Asurvey%20reports%20from%20the%20government.%20The%20evaluations%20reveal%20that%20the%20produced%0Afunction%20maps%20achieve%20an%20OA%20of%2082%25%20and%20Kappa%20of%2071%25%20among%201%2C616%2C796%20buildings%0Ain%20Shanghai%2C%20China.%20This%20study%20has%20the%20potential%20to%20support%20large-scale%20urban%0Amanagement%20and%20sustainable%20urban%20development.%20All%20collected%20data%20and%20produced%0Amaps%20are%20open%20access%20at%20https%3A//github.com/LiZhuoHong/BuildingMap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05133v1&entry.124074799=Read"},
{"title": "Mitigating Bias Using Model-Agnostic Data Attribution", "author": "Sander De Coninck and Wei-Cheng Wang and Sam Leroux and Pieter Simoens", "abstract": "  Mitigating bias in machine learning models is a critical endeavor for\nensuring fairness and equity. In this paper, we propose a novel approach to\naddress bias by leveraging pixel image attributions to identify and regularize\nregions of images containing significant information about bias attributes. Our\nmethod utilizes a model-agnostic approach to extract pixel attributions by\nemploying a convolutional neural network (CNN) classifier trained on small\nimage patches. By training the classifier to predict a property of the entire\nimage using only a single patch, we achieve region-based attributions that\nprovide insights into the distribution of important information across the\nimage. We propose utilizing these attributions to introduce targeted noise into\ndatasets with confounding attributes that bias the data, thereby constraining\nneural networks from learning these biases and emphasizing the primary\nattributes. Our approach demonstrates its efficacy in enabling the training of\nunbiased classifiers on heavily biased datasets.\n", "link": "http://arxiv.org/abs/2405.05031v1", "date": "2024-05-08", "relevancy": 1.9626, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5023}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4959}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Bias%20Using%20Model-Agnostic%20Data%20Attribution&body=Title%3A%20Mitigating%20Bias%20Using%20Model-Agnostic%20Data%20Attribution%0AAuthor%3A%20Sander%20De%20Coninck%20and%20Wei-Cheng%20Wang%20and%20Sam%20Leroux%20and%20Pieter%20Simoens%0AAbstract%3A%20%20%20Mitigating%20bias%20in%20machine%20learning%20models%20is%20a%20critical%20endeavor%20for%0Aensuring%20fairness%20and%20equity.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%0Aaddress%20bias%20by%20leveraging%20pixel%20image%20attributions%20to%20identify%20and%20regularize%0Aregions%20of%20images%20containing%20significant%20information%20about%20bias%20attributes.%20Our%0Amethod%20utilizes%20a%20model-agnostic%20approach%20to%20extract%20pixel%20attributions%20by%0Aemploying%20a%20convolutional%20neural%20network%20%28CNN%29%20classifier%20trained%20on%20small%0Aimage%20patches.%20By%20training%20the%20classifier%20to%20predict%20a%20property%20of%20the%20entire%0Aimage%20using%20only%20a%20single%20patch%2C%20we%20achieve%20region-based%20attributions%20that%0Aprovide%20insights%20into%20the%20distribution%20of%20important%20information%20across%20the%0Aimage.%20We%20propose%20utilizing%20these%20attributions%20to%20introduce%20targeted%20noise%20into%0Adatasets%20with%20confounding%20attributes%20that%20bias%20the%20data%2C%20thereby%20constraining%0Aneural%20networks%20from%20learning%20these%20biases%20and%20emphasizing%20the%20primary%0Aattributes.%20Our%20approach%20demonstrates%20its%20efficacy%20in%20enabling%20the%20training%20of%0Aunbiased%20classifiers%20on%20heavily%20biased%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Bias%2520Using%2520Model-Agnostic%2520Data%2520Attribution%26entry.906535625%3DSander%2520De%2520Coninck%2520and%2520Wei-Cheng%2520Wang%2520and%2520Sam%2520Leroux%2520and%2520Pieter%2520Simoens%26entry.1292438233%3D%2520%2520Mitigating%2520bias%2520in%2520machine%2520learning%2520models%2520is%2520a%2520critical%2520endeavor%2520for%250Aensuring%2520fairness%2520and%2520equity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%250Aaddress%2520bias%2520by%2520leveraging%2520pixel%2520image%2520attributions%2520to%2520identify%2520and%2520regularize%250Aregions%2520of%2520images%2520containing%2520significant%2520information%2520about%2520bias%2520attributes.%2520Our%250Amethod%2520utilizes%2520a%2520model-agnostic%2520approach%2520to%2520extract%2520pixel%2520attributions%2520by%250Aemploying%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520classifier%2520trained%2520on%2520small%250Aimage%2520patches.%2520By%2520training%2520the%2520classifier%2520to%2520predict%2520a%2520property%2520of%2520the%2520entire%250Aimage%2520using%2520only%2520a%2520single%2520patch%252C%2520we%2520achieve%2520region-based%2520attributions%2520that%250Aprovide%2520insights%2520into%2520the%2520distribution%2520of%2520important%2520information%2520across%2520the%250Aimage.%2520We%2520propose%2520utilizing%2520these%2520attributions%2520to%2520introduce%2520targeted%2520noise%2520into%250Adatasets%2520with%2520confounding%2520attributes%2520that%2520bias%2520the%2520data%252C%2520thereby%2520constraining%250Aneural%2520networks%2520from%2520learning%2520these%2520biases%2520and%2520emphasizing%2520the%2520primary%250Aattributes.%2520Our%2520approach%2520demonstrates%2520its%2520efficacy%2520in%2520enabling%2520the%2520training%2520of%250Aunbiased%2520classifiers%2520on%2520heavily%2520biased%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Bias%20Using%20Model-Agnostic%20Data%20Attribution&entry.906535625=Sander%20De%20Coninck%20and%20Wei-Cheng%20Wang%20and%20Sam%20Leroux%20and%20Pieter%20Simoens&entry.1292438233=%20%20Mitigating%20bias%20in%20machine%20learning%20models%20is%20a%20critical%20endeavor%20for%0Aensuring%20fairness%20and%20equity.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%0Aaddress%20bias%20by%20leveraging%20pixel%20image%20attributions%20to%20identify%20and%20regularize%0Aregions%20of%20images%20containing%20significant%20information%20about%20bias%20attributes.%20Our%0Amethod%20utilizes%20a%20model-agnostic%20approach%20to%20extract%20pixel%20attributions%20by%0Aemploying%20a%20convolutional%20neural%20network%20%28CNN%29%20classifier%20trained%20on%20small%0Aimage%20patches.%20By%20training%20the%20classifier%20to%20predict%20a%20property%20of%20the%20entire%0Aimage%20using%20only%20a%20single%20patch%2C%20we%20achieve%20region-based%20attributions%20that%0Aprovide%20insights%20into%20the%20distribution%20of%20important%20information%20across%20the%0Aimage.%20We%20propose%20utilizing%20these%20attributions%20to%20introduce%20targeted%20noise%20into%0Adatasets%20with%20confounding%20attributes%20that%20bias%20the%20data%2C%20thereby%20constraining%0Aneural%20networks%20from%20learning%20these%20biases%20and%20emphasizing%20the%20primary%0Aattributes.%20Our%20approach%20demonstrates%20its%20efficacy%20in%20enabling%20the%20training%20of%0Aunbiased%20classifiers%20on%20heavily%20biased%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05031v1&entry.124074799=Read"},
{"title": "ADELT: Transpilation Between Deep Learning Frameworks", "author": "Linyuan Gong and Jiayi Wang and Alvin Cheung", "abstract": "  We propose the Adversarial DEep Learning Transpiler (ADELT), a novel approach\nto source-to-source transpilation between deep learning frameworks. ADELT\nuniquely decouples code skeleton transpilation and API keyword mapping. For\ncode skeleton transpilation, it uses few-shot prompting on large language\nmodels (LLMs), while for API keyword mapping, it uses contextual embeddings\nfrom a code-specific BERT. These embeddings are trained in a domain-adversarial\nsetup to generate a keyword translation dictionary. ADELT is trained on an\nunlabeled web-crawled deep learning corpus, without relying on any hand-crafted\nrules or parallel data. It outperforms state-of-the-art transpilers, improving\npass@1 rate by 17.4 pts and 15.0 pts for PyTorch-Keras and PyTorch-MXNet\ntranspilation pairs respectively. We provide open access to our code at\nhttps://github.com/gonglinyuan/adelt.\n", "link": "http://arxiv.org/abs/2303.03593v3", "date": "2024-05-08", "relevancy": 1.9547, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4759}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADELT%3A%20Transpilation%20Between%20Deep%20Learning%20Frameworks&body=Title%3A%20ADELT%3A%20Transpilation%20Between%20Deep%20Learning%20Frameworks%0AAuthor%3A%20Linyuan%20Gong%20and%20Jiayi%20Wang%20and%20Alvin%20Cheung%0AAbstract%3A%20%20%20We%20propose%20the%20Adversarial%20DEep%20Learning%20Transpiler%20%28ADELT%29%2C%20a%20novel%20approach%0Ato%20source-to-source%20transpilation%20between%20deep%20learning%20frameworks.%20ADELT%0Auniquely%20decouples%20code%20skeleton%20transpilation%20and%20API%20keyword%20mapping.%20For%0Acode%20skeleton%20transpilation%2C%20it%20uses%20few-shot%20prompting%20on%20large%20language%0Amodels%20%28LLMs%29%2C%20while%20for%20API%20keyword%20mapping%2C%20it%20uses%20contextual%20embeddings%0Afrom%20a%20code-specific%20BERT.%20These%20embeddings%20are%20trained%20in%20a%20domain-adversarial%0Asetup%20to%20generate%20a%20keyword%20translation%20dictionary.%20ADELT%20is%20trained%20on%20an%0Aunlabeled%20web-crawled%20deep%20learning%20corpus%2C%20without%20relying%20on%20any%20hand-crafted%0Arules%20or%20parallel%20data.%20It%20outperforms%20state-of-the-art%20transpilers%2C%20improving%0Apass%401%20rate%20by%2017.4%20pts%20and%2015.0%20pts%20for%20PyTorch-Keras%20and%20PyTorch-MXNet%0Atranspilation%20pairs%20respectively.%20We%20provide%20open%20access%20to%20our%20code%20at%0Ahttps%3A//github.com/gonglinyuan/adelt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.03593v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADELT%253A%2520Transpilation%2520Between%2520Deep%2520Learning%2520Frameworks%26entry.906535625%3DLinyuan%2520Gong%2520and%2520Jiayi%2520Wang%2520and%2520Alvin%2520Cheung%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520Adversarial%2520DEep%2520Learning%2520Transpiler%2520%2528ADELT%2529%252C%2520a%2520novel%2520approach%250Ato%2520source-to-source%2520transpilation%2520between%2520deep%2520learning%2520frameworks.%2520ADELT%250Auniquely%2520decouples%2520code%2520skeleton%2520transpilation%2520and%2520API%2520keyword%2520mapping.%2520For%250Acode%2520skeleton%2520transpilation%252C%2520it%2520uses%2520few-shot%2520prompting%2520on%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520while%2520for%2520API%2520keyword%2520mapping%252C%2520it%2520uses%2520contextual%2520embeddings%250Afrom%2520a%2520code-specific%2520BERT.%2520These%2520embeddings%2520are%2520trained%2520in%2520a%2520domain-adversarial%250Asetup%2520to%2520generate%2520a%2520keyword%2520translation%2520dictionary.%2520ADELT%2520is%2520trained%2520on%2520an%250Aunlabeled%2520web-crawled%2520deep%2520learning%2520corpus%252C%2520without%2520relying%2520on%2520any%2520hand-crafted%250Arules%2520or%2520parallel%2520data.%2520It%2520outperforms%2520state-of-the-art%2520transpilers%252C%2520improving%250Apass%25401%2520rate%2520by%252017.4%2520pts%2520and%252015.0%2520pts%2520for%2520PyTorch-Keras%2520and%2520PyTorch-MXNet%250Atranspilation%2520pairs%2520respectively.%2520We%2520provide%2520open%2520access%2520to%2520our%2520code%2520at%250Ahttps%253A//github.com/gonglinyuan/adelt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.03593v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADELT%3A%20Transpilation%20Between%20Deep%20Learning%20Frameworks&entry.906535625=Linyuan%20Gong%20and%20Jiayi%20Wang%20and%20Alvin%20Cheung&entry.1292438233=%20%20We%20propose%20the%20Adversarial%20DEep%20Learning%20Transpiler%20%28ADELT%29%2C%20a%20novel%20approach%0Ato%20source-to-source%20transpilation%20between%20deep%20learning%20frameworks.%20ADELT%0Auniquely%20decouples%20code%20skeleton%20transpilation%20and%20API%20keyword%20mapping.%20For%0Acode%20skeleton%20transpilation%2C%20it%20uses%20few-shot%20prompting%20on%20large%20language%0Amodels%20%28LLMs%29%2C%20while%20for%20API%20keyword%20mapping%2C%20it%20uses%20contextual%20embeddings%0Afrom%20a%20code-specific%20BERT.%20These%20embeddings%20are%20trained%20in%20a%20domain-adversarial%0Asetup%20to%20generate%20a%20keyword%20translation%20dictionary.%20ADELT%20is%20trained%20on%20an%0Aunlabeled%20web-crawled%20deep%20learning%20corpus%2C%20without%20relying%20on%20any%20hand-crafted%0Arules%20or%20parallel%20data.%20It%20outperforms%20state-of-the-art%20transpilers%2C%20improving%0Apass%401%20rate%20by%2017.4%20pts%20and%2015.0%20pts%20for%20PyTorch-Keras%20and%20PyTorch-MXNet%0Atranspilation%20pairs%20respectively.%20We%20provide%20open%20access%20to%20our%20code%20at%0Ahttps%3A//github.com/gonglinyuan/adelt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.03593v3&entry.124074799=Read"},
{"title": "Approximation properties relative to continuous scale space for hybrid\n  discretizations of Gaussian derivative operators", "author": "Tony Lindeberg", "abstract": "  This paper presents an analysis of properties of two hybrid discretization\nmethods for Gaussian derivatives, based on convolutions with either the\nnormalized sampled Gaussian kernel or the integrated Gaussian kernel followed\nby central differences. The motivation for studying these discretization\nmethods is that in situations when multiple spatial derivatives of different\norder are needed at the same scale level, they can be computed significantly\nmore efficiently compared to more direct derivative approximations based on\nexplicit convolutions with either sampled Gaussian kernels or integrated\nGaussian kernels.\n  While these computational benefits do also hold for the genuinely discrete\napproach for computing discrete analogues of Gaussian derivatives, based on\nconvolution with the discrete analogue of the Gaussian kernel followed by\ncentral differences, the underlying mathematical primitives for the discrete\nanalogue of the Gaussian kernel, in terms of modified Bessel functions of\ninteger order, may not be available in certain frameworks for image processing,\nsuch as when performing deep learning based on scale-parameterized filters in\nterms of Gaussian derivatives, with learning of the scale levels.\n  In this paper, we present a characterization of the properties of these\nhybrid discretization methods, in terms of quantitative performance measures\nconcerning the amount of spatial smoothing that they imply, as well as the\nrelative consistency of scale estimates obtained from scale-invariant feature\ndetectors with automatic scale selection, with an emphasis on the behaviour for\nvery small values of the scale parameter, which may differ significantly from\ncorresponding results obtained from the fully continuous scale-space theory, as\nwell as between different types of discretization methods.\n", "link": "http://arxiv.org/abs/2405.05095v1", "date": "2024-05-08", "relevancy": 1.9377, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4966}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4833}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximation%20properties%20relative%20to%20continuous%20scale%20space%20for%20hybrid%0A%20%20discretizations%20of%20Gaussian%20derivative%20operators&body=Title%3A%20Approximation%20properties%20relative%20to%20continuous%20scale%20space%20for%20hybrid%0A%20%20discretizations%20of%20Gaussian%20derivative%20operators%0AAuthor%3A%20Tony%20Lindeberg%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20analysis%20of%20properties%20of%20two%20hybrid%20discretization%0Amethods%20for%20Gaussian%20derivatives%2C%20based%20on%20convolutions%20with%20either%20the%0Anormalized%20sampled%20Gaussian%20kernel%20or%20the%20integrated%20Gaussian%20kernel%20followed%0Aby%20central%20differences.%20The%20motivation%20for%20studying%20these%20discretization%0Amethods%20is%20that%20in%20situations%20when%20multiple%20spatial%20derivatives%20of%20different%0Aorder%20are%20needed%20at%20the%20same%20scale%20level%2C%20they%20can%20be%20computed%20significantly%0Amore%20efficiently%20compared%20to%20more%20direct%20derivative%20approximations%20based%20on%0Aexplicit%20convolutions%20with%20either%20sampled%20Gaussian%20kernels%20or%20integrated%0AGaussian%20kernels.%0A%20%20While%20these%20computational%20benefits%20do%20also%20hold%20for%20the%20genuinely%20discrete%0Aapproach%20for%20computing%20discrete%20analogues%20of%20Gaussian%20derivatives%2C%20based%20on%0Aconvolution%20with%20the%20discrete%20analogue%20of%20the%20Gaussian%20kernel%20followed%20by%0Acentral%20differences%2C%20the%20underlying%20mathematical%20primitives%20for%20the%20discrete%0Aanalogue%20of%20the%20Gaussian%20kernel%2C%20in%20terms%20of%20modified%20Bessel%20functions%20of%0Ainteger%20order%2C%20may%20not%20be%20available%20in%20certain%20frameworks%20for%20image%20processing%2C%0Asuch%20as%20when%20performing%20deep%20learning%20based%20on%20scale-parameterized%20filters%20in%0Aterms%20of%20Gaussian%20derivatives%2C%20with%20learning%20of%20the%20scale%20levels.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20characterization%20of%20the%20properties%20of%20these%0Ahybrid%20discretization%20methods%2C%20in%20terms%20of%20quantitative%20performance%20measures%0Aconcerning%20the%20amount%20of%20spatial%20smoothing%20that%20they%20imply%2C%20as%20well%20as%20the%0Arelative%20consistency%20of%20scale%20estimates%20obtained%20from%20scale-invariant%20feature%0Adetectors%20with%20automatic%20scale%20selection%2C%20with%20an%20emphasis%20on%20the%20behaviour%20for%0Avery%20small%20values%20of%20the%20scale%20parameter%2C%20which%20may%20differ%20significantly%20from%0Acorresponding%20results%20obtained%20from%20the%20fully%20continuous%20scale-space%20theory%2C%20as%0Awell%20as%20between%20different%20types%20of%20discretization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximation%2520properties%2520relative%2520to%2520continuous%2520scale%2520space%2520for%2520hybrid%250A%2520%2520discretizations%2520of%2520Gaussian%2520derivative%2520operators%26entry.906535625%3DTony%2520Lindeberg%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520analysis%2520of%2520properties%2520of%2520two%2520hybrid%2520discretization%250Amethods%2520for%2520Gaussian%2520derivatives%252C%2520based%2520on%2520convolutions%2520with%2520either%2520the%250Anormalized%2520sampled%2520Gaussian%2520kernel%2520or%2520the%2520integrated%2520Gaussian%2520kernel%2520followed%250Aby%2520central%2520differences.%2520The%2520motivation%2520for%2520studying%2520these%2520discretization%250Amethods%2520is%2520that%2520in%2520situations%2520when%2520multiple%2520spatial%2520derivatives%2520of%2520different%250Aorder%2520are%2520needed%2520at%2520the%2520same%2520scale%2520level%252C%2520they%2520can%2520be%2520computed%2520significantly%250Amore%2520efficiently%2520compared%2520to%2520more%2520direct%2520derivative%2520approximations%2520based%2520on%250Aexplicit%2520convolutions%2520with%2520either%2520sampled%2520Gaussian%2520kernels%2520or%2520integrated%250AGaussian%2520kernels.%250A%2520%2520While%2520these%2520computational%2520benefits%2520do%2520also%2520hold%2520for%2520the%2520genuinely%2520discrete%250Aapproach%2520for%2520computing%2520discrete%2520analogues%2520of%2520Gaussian%2520derivatives%252C%2520based%2520on%250Aconvolution%2520with%2520the%2520discrete%2520analogue%2520of%2520the%2520Gaussian%2520kernel%2520followed%2520by%250Acentral%2520differences%252C%2520the%2520underlying%2520mathematical%2520primitives%2520for%2520the%2520discrete%250Aanalogue%2520of%2520the%2520Gaussian%2520kernel%252C%2520in%2520terms%2520of%2520modified%2520Bessel%2520functions%2520of%250Ainteger%2520order%252C%2520may%2520not%2520be%2520available%2520in%2520certain%2520frameworks%2520for%2520image%2520processing%252C%250Asuch%2520as%2520when%2520performing%2520deep%2520learning%2520based%2520on%2520scale-parameterized%2520filters%2520in%250Aterms%2520of%2520Gaussian%2520derivatives%252C%2520with%2520learning%2520of%2520the%2520scale%2520levels.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520characterization%2520of%2520the%2520properties%2520of%2520these%250Ahybrid%2520discretization%2520methods%252C%2520in%2520terms%2520of%2520quantitative%2520performance%2520measures%250Aconcerning%2520the%2520amount%2520of%2520spatial%2520smoothing%2520that%2520they%2520imply%252C%2520as%2520well%2520as%2520the%250Arelative%2520consistency%2520of%2520scale%2520estimates%2520obtained%2520from%2520scale-invariant%2520feature%250Adetectors%2520with%2520automatic%2520scale%2520selection%252C%2520with%2520an%2520emphasis%2520on%2520the%2520behaviour%2520for%250Avery%2520small%2520values%2520of%2520the%2520scale%2520parameter%252C%2520which%2520may%2520differ%2520significantly%2520from%250Acorresponding%2520results%2520obtained%2520from%2520the%2520fully%2520continuous%2520scale-space%2520theory%252C%2520as%250Awell%2520as%2520between%2520different%2520types%2520of%2520discretization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximation%20properties%20relative%20to%20continuous%20scale%20space%20for%20hybrid%0A%20%20discretizations%20of%20Gaussian%20derivative%20operators&entry.906535625=Tony%20Lindeberg&entry.1292438233=%20%20This%20paper%20presents%20an%20analysis%20of%20properties%20of%20two%20hybrid%20discretization%0Amethods%20for%20Gaussian%20derivatives%2C%20based%20on%20convolutions%20with%20either%20the%0Anormalized%20sampled%20Gaussian%20kernel%20or%20the%20integrated%20Gaussian%20kernel%20followed%0Aby%20central%20differences.%20The%20motivation%20for%20studying%20these%20discretization%0Amethods%20is%20that%20in%20situations%20when%20multiple%20spatial%20derivatives%20of%20different%0Aorder%20are%20needed%20at%20the%20same%20scale%20level%2C%20they%20can%20be%20computed%20significantly%0Amore%20efficiently%20compared%20to%20more%20direct%20derivative%20approximations%20based%20on%0Aexplicit%20convolutions%20with%20either%20sampled%20Gaussian%20kernels%20or%20integrated%0AGaussian%20kernels.%0A%20%20While%20these%20computational%20benefits%20do%20also%20hold%20for%20the%20genuinely%20discrete%0Aapproach%20for%20computing%20discrete%20analogues%20of%20Gaussian%20derivatives%2C%20based%20on%0Aconvolution%20with%20the%20discrete%20analogue%20of%20the%20Gaussian%20kernel%20followed%20by%0Acentral%20differences%2C%20the%20underlying%20mathematical%20primitives%20for%20the%20discrete%0Aanalogue%20of%20the%20Gaussian%20kernel%2C%20in%20terms%20of%20modified%20Bessel%20functions%20of%0Ainteger%20order%2C%20may%20not%20be%20available%20in%20certain%20frameworks%20for%20image%20processing%2C%0Asuch%20as%20when%20performing%20deep%20learning%20based%20on%20scale-parameterized%20filters%20in%0Aterms%20of%20Gaussian%20derivatives%2C%20with%20learning%20of%20the%20scale%20levels.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20characterization%20of%20the%20properties%20of%20these%0Ahybrid%20discretization%20methods%2C%20in%20terms%20of%20quantitative%20performance%20measures%0Aconcerning%20the%20amount%20of%20spatial%20smoothing%20that%20they%20imply%2C%20as%20well%20as%20the%0Arelative%20consistency%20of%20scale%20estimates%20obtained%20from%20scale-invariant%20feature%0Adetectors%20with%20automatic%20scale%20selection%2C%20with%20an%20emphasis%20on%20the%20behaviour%20for%0Avery%20small%20values%20of%20the%20scale%20parameter%2C%20which%20may%20differ%20significantly%20from%0Acorresponding%20results%20obtained%20from%20the%20fully%20continuous%20scale-space%20theory%2C%20as%0Awell%20as%20between%20different%20types%20of%20discretization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05095v1&entry.124074799=Read"},
{"title": "A Study of Bayesian Neural Network Surrogates for Bayesian Optimization", "author": "Yucen Lily Li and Tim G. J. Rudner and Andrew Gordon Wilson", "abstract": "  Bayesian optimization is a highly efficient approach to optimizing objective\nfunctions which are expensive to query. These objectives are typically\nrepresented by Gaussian process (GP) surrogate models which are easy to\noptimize and support exact inference. While standard GP surrogates have been\nwell-established in Bayesian optimization, Bayesian neural networks (BNNs) have\nrecently become practical function approximators, with many benefits over\nstandard GPs such as the ability to naturally handle non-stationarity and learn\nrepresentations for high-dimensional data. In this paper, we study BNNs as\nalternatives to standard GP surrogates for optimization. We consider a variety\nof approximate inference procedures for finite-width BNNs, including\nhigh-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics\nsuch as deep ensembles. We also consider infinite-width BNNs, linearized\nLaplace approximations, and partially stochastic models such as deep kernel\nlearning. We evaluate this collection of surrogate models on diverse problems\nwith varying dimensionality, number of objectives, non-stationarity, and\ndiscrete and continuous inputs. We find: (i) the ranking of methods is highly\nproblem dependent, suggesting the need for tailored inductive biases; (ii) HMC\nis the most successful approximate inference procedure for fully stochastic\nBNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is\nrelatively competitive; (iv) deep ensembles perform relatively poorly; (v)\ninfinite-width BNNs are particularly promising, especially in high dimensions.\n", "link": "http://arxiv.org/abs/2305.20028v2", "date": "2024-05-08", "relevancy": 1.9329, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5206}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5177}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20of%20Bayesian%20Neural%20Network%20Surrogates%20for%20Bayesian%20Optimization&body=Title%3A%20A%20Study%20of%20Bayesian%20Neural%20Network%20Surrogates%20for%20Bayesian%20Optimization%0AAuthor%3A%20Yucen%20Lily%20Li%20and%20Tim%20G.%20J.%20Rudner%20and%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20Bayesian%20optimization%20is%20a%20highly%20efficient%20approach%20to%20optimizing%20objective%0Afunctions%20which%20are%20expensive%20to%20query.%20These%20objectives%20are%20typically%0Arepresented%20by%20Gaussian%20process%20%28GP%29%20surrogate%20models%20which%20are%20easy%20to%0Aoptimize%20and%20support%20exact%20inference.%20While%20standard%20GP%20surrogates%20have%20been%0Awell-established%20in%20Bayesian%20optimization%2C%20Bayesian%20neural%20networks%20%28BNNs%29%20have%0Arecently%20become%20practical%20function%20approximators%2C%20with%20many%20benefits%20over%0Astandard%20GPs%20such%20as%20the%20ability%20to%20naturally%20handle%20non-stationarity%20and%20learn%0Arepresentations%20for%20high-dimensional%20data.%20In%20this%20paper%2C%20we%20study%20BNNs%20as%0Aalternatives%20to%20standard%20GP%20surrogates%20for%20optimization.%20We%20consider%20a%20variety%0Aof%20approximate%20inference%20procedures%20for%20finite-width%20BNNs%2C%20including%0Ahigh-quality%20Hamiltonian%20Monte%20Carlo%2C%20low-cost%20stochastic%20MCMC%2C%20and%20heuristics%0Asuch%20as%20deep%20ensembles.%20We%20also%20consider%20infinite-width%20BNNs%2C%20linearized%0ALaplace%20approximations%2C%20and%20partially%20stochastic%20models%20such%20as%20deep%20kernel%0Alearning.%20We%20evaluate%20this%20collection%20of%20surrogate%20models%20on%20diverse%20problems%0Awith%20varying%20dimensionality%2C%20number%20of%20objectives%2C%20non-stationarity%2C%20and%0Adiscrete%20and%20continuous%20inputs.%20We%20find%3A%20%28i%29%20the%20ranking%20of%20methods%20is%20highly%0Aproblem%20dependent%2C%20suggesting%20the%20need%20for%20tailored%20inductive%20biases%3B%20%28ii%29%20HMC%0Ais%20the%20most%20successful%20approximate%20inference%20procedure%20for%20fully%20stochastic%0ABNNs%3B%20%28iii%29%20full%20stochasticity%20may%20be%20unnecessary%20as%20deep%20kernel%20learning%20is%0Arelatively%20competitive%3B%20%28iv%29%20deep%20ensembles%20perform%20relatively%20poorly%3B%20%28v%29%0Ainfinite-width%20BNNs%20are%20particularly%20promising%2C%20especially%20in%20high%20dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.20028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520of%2520Bayesian%2520Neural%2520Network%2520Surrogates%2520for%2520Bayesian%2520Optimization%26entry.906535625%3DYucen%2520Lily%2520Li%2520and%2520Tim%2520G.%2520J.%2520Rudner%2520and%2520Andrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520Bayesian%2520optimization%2520is%2520a%2520highly%2520efficient%2520approach%2520to%2520optimizing%2520objective%250Afunctions%2520which%2520are%2520expensive%2520to%2520query.%2520These%2520objectives%2520are%2520typically%250Arepresented%2520by%2520Gaussian%2520process%2520%2528GP%2529%2520surrogate%2520models%2520which%2520are%2520easy%2520to%250Aoptimize%2520and%2520support%2520exact%2520inference.%2520While%2520standard%2520GP%2520surrogates%2520have%2520been%250Awell-established%2520in%2520Bayesian%2520optimization%252C%2520Bayesian%2520neural%2520networks%2520%2528BNNs%2529%2520have%250Arecently%2520become%2520practical%2520function%2520approximators%252C%2520with%2520many%2520benefits%2520over%250Astandard%2520GPs%2520such%2520as%2520the%2520ability%2520to%2520naturally%2520handle%2520non-stationarity%2520and%2520learn%250Arepresentations%2520for%2520high-dimensional%2520data.%2520In%2520this%2520paper%252C%2520we%2520study%2520BNNs%2520as%250Aalternatives%2520to%2520standard%2520GP%2520surrogates%2520for%2520optimization.%2520We%2520consider%2520a%2520variety%250Aof%2520approximate%2520inference%2520procedures%2520for%2520finite-width%2520BNNs%252C%2520including%250Ahigh-quality%2520Hamiltonian%2520Monte%2520Carlo%252C%2520low-cost%2520stochastic%2520MCMC%252C%2520and%2520heuristics%250Asuch%2520as%2520deep%2520ensembles.%2520We%2520also%2520consider%2520infinite-width%2520BNNs%252C%2520linearized%250ALaplace%2520approximations%252C%2520and%2520partially%2520stochastic%2520models%2520such%2520as%2520deep%2520kernel%250Alearning.%2520We%2520evaluate%2520this%2520collection%2520of%2520surrogate%2520models%2520on%2520diverse%2520problems%250Awith%2520varying%2520dimensionality%252C%2520number%2520of%2520objectives%252C%2520non-stationarity%252C%2520and%250Adiscrete%2520and%2520continuous%2520inputs.%2520We%2520find%253A%2520%2528i%2529%2520the%2520ranking%2520of%2520methods%2520is%2520highly%250Aproblem%2520dependent%252C%2520suggesting%2520the%2520need%2520for%2520tailored%2520inductive%2520biases%253B%2520%2528ii%2529%2520HMC%250Ais%2520the%2520most%2520successful%2520approximate%2520inference%2520procedure%2520for%2520fully%2520stochastic%250ABNNs%253B%2520%2528iii%2529%2520full%2520stochasticity%2520may%2520be%2520unnecessary%2520as%2520deep%2520kernel%2520learning%2520is%250Arelatively%2520competitive%253B%2520%2528iv%2529%2520deep%2520ensembles%2520perform%2520relatively%2520poorly%253B%2520%2528v%2529%250Ainfinite-width%2520BNNs%2520are%2520particularly%2520promising%252C%2520especially%2520in%2520high%2520dimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.20028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20of%20Bayesian%20Neural%20Network%20Surrogates%20for%20Bayesian%20Optimization&entry.906535625=Yucen%20Lily%20Li%20and%20Tim%20G.%20J.%20Rudner%20and%20Andrew%20Gordon%20Wilson&entry.1292438233=%20%20Bayesian%20optimization%20is%20a%20highly%20efficient%20approach%20to%20optimizing%20objective%0Afunctions%20which%20are%20expensive%20to%20query.%20These%20objectives%20are%20typically%0Arepresented%20by%20Gaussian%20process%20%28GP%29%20surrogate%20models%20which%20are%20easy%20to%0Aoptimize%20and%20support%20exact%20inference.%20While%20standard%20GP%20surrogates%20have%20been%0Awell-established%20in%20Bayesian%20optimization%2C%20Bayesian%20neural%20networks%20%28BNNs%29%20have%0Arecently%20become%20practical%20function%20approximators%2C%20with%20many%20benefits%20over%0Astandard%20GPs%20such%20as%20the%20ability%20to%20naturally%20handle%20non-stationarity%20and%20learn%0Arepresentations%20for%20high-dimensional%20data.%20In%20this%20paper%2C%20we%20study%20BNNs%20as%0Aalternatives%20to%20standard%20GP%20surrogates%20for%20optimization.%20We%20consider%20a%20variety%0Aof%20approximate%20inference%20procedures%20for%20finite-width%20BNNs%2C%20including%0Ahigh-quality%20Hamiltonian%20Monte%20Carlo%2C%20low-cost%20stochastic%20MCMC%2C%20and%20heuristics%0Asuch%20as%20deep%20ensembles.%20We%20also%20consider%20infinite-width%20BNNs%2C%20linearized%0ALaplace%20approximations%2C%20and%20partially%20stochastic%20models%20such%20as%20deep%20kernel%0Alearning.%20We%20evaluate%20this%20collection%20of%20surrogate%20models%20on%20diverse%20problems%0Awith%20varying%20dimensionality%2C%20number%20of%20objectives%2C%20non-stationarity%2C%20and%0Adiscrete%20and%20continuous%20inputs.%20We%20find%3A%20%28i%29%20the%20ranking%20of%20methods%20is%20highly%0Aproblem%20dependent%2C%20suggesting%20the%20need%20for%20tailored%20inductive%20biases%3B%20%28ii%29%20HMC%0Ais%20the%20most%20successful%20approximate%20inference%20procedure%20for%20fully%20stochastic%0ABNNs%3B%20%28iii%29%20full%20stochasticity%20may%20be%20unnecessary%20as%20deep%20kernel%20learning%20is%0Arelatively%20competitive%3B%20%28iv%29%20deep%20ensembles%20perform%20relatively%20poorly%3B%20%28v%29%0Ainfinite-width%20BNNs%20are%20particularly%20promising%2C%20especially%20in%20high%20dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.20028v2&entry.124074799=Read"},
{"title": "Locally Differentially Private In-Context Learning", "author": "Chunyan Zheng and Keke Sun and Wenhao Zhao and Haibo Zhou and Lixin Jiang and Shaoyang Song and Chunlai Zhou", "abstract": "  Large pretrained language models (LLMs) have shown surprising In-Context\nLearning (ICL) ability. An important application in deploying large language\nmodels is to augment LLMs with a private database for some specific task. The\nmain problem with this promising commercial use is that LLMs have been shown to\nmemorize their training data and their prompt data are vulnerable to membership\ninference attacks (MIA) and prompt leaking attacks. In order to deal with this\nproblem, we treat LLMs as untrusted in privacy and propose a locally\ndifferentially private framework of in-context learning(LDP-ICL) in the\nsettings where labels are sensitive. Considering the mechanisms of in-context\nlearning in Transformers by gradient descent, we provide an analysis of the\ntrade-off between privacy and utility in such LDP-ICL for classification.\nMoreover, we apply LDP-ICL to the discrete distribution estimation problem. In\nthe end, we perform several experiments to demonstrate our analysis results.\n", "link": "http://arxiv.org/abs/2405.04032v2", "date": "2024-05-08", "relevancy": 1.9277, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5052}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4773}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Differentially%20Private%20In-Context%20Learning&body=Title%3A%20Locally%20Differentially%20Private%20In-Context%20Learning%0AAuthor%3A%20Chunyan%20Zheng%20and%20Keke%20Sun%20and%20Wenhao%20Zhao%20and%20Haibo%20Zhou%20and%20Lixin%20Jiang%20and%20Shaoyang%20Song%20and%20Chunlai%20Zhou%0AAbstract%3A%20%20%20Large%20pretrained%20language%20models%20%28LLMs%29%20have%20shown%20surprising%20In-Context%0ALearning%20%28ICL%29%20ability.%20An%20important%20application%20in%20deploying%20large%20language%0Amodels%20is%20to%20augment%20LLMs%20with%20a%20private%20database%20for%20some%20specific%20task.%20The%0Amain%20problem%20with%20this%20promising%20commercial%20use%20is%20that%20LLMs%20have%20been%20shown%20to%0Amemorize%20their%20training%20data%20and%20their%20prompt%20data%20are%20vulnerable%20to%20membership%0Ainference%20attacks%20%28MIA%29%20and%20prompt%20leaking%20attacks.%20In%20order%20to%20deal%20with%20this%0Aproblem%2C%20we%20treat%20LLMs%20as%20untrusted%20in%20privacy%20and%20propose%20a%20locally%0Adifferentially%20private%20framework%20of%20in-context%20learning%28LDP-ICL%29%20in%20the%0Asettings%20where%20labels%20are%20sensitive.%20Considering%20the%20mechanisms%20of%20in-context%0Alearning%20in%20Transformers%20by%20gradient%20descent%2C%20we%20provide%20an%20analysis%20of%20the%0Atrade-off%20between%20privacy%20and%20utility%20in%20such%20LDP-ICL%20for%20classification.%0AMoreover%2C%20we%20apply%20LDP-ICL%20to%20the%20discrete%20distribution%20estimation%20problem.%20In%0Athe%20end%2C%20we%20perform%20several%20experiments%20to%20demonstrate%20our%20analysis%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04032v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Differentially%2520Private%2520In-Context%2520Learning%26entry.906535625%3DChunyan%2520Zheng%2520and%2520Keke%2520Sun%2520and%2520Wenhao%2520Zhao%2520and%2520Haibo%2520Zhou%2520and%2520Lixin%2520Jiang%2520and%2520Shaoyang%2520Song%2520and%2520Chunlai%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520pretrained%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520surprising%2520In-Context%250ALearning%2520%2528ICL%2529%2520ability.%2520An%2520important%2520application%2520in%2520deploying%2520large%2520language%250Amodels%2520is%2520to%2520augment%2520LLMs%2520with%2520a%2520private%2520database%2520for%2520some%2520specific%2520task.%2520The%250Amain%2520problem%2520with%2520this%2520promising%2520commercial%2520use%2520is%2520that%2520LLMs%2520have%2520been%2520shown%2520to%250Amemorize%2520their%2520training%2520data%2520and%2520their%2520prompt%2520data%2520are%2520vulnerable%2520to%2520membership%250Ainference%2520attacks%2520%2528MIA%2529%2520and%2520prompt%2520leaking%2520attacks.%2520In%2520order%2520to%2520deal%2520with%2520this%250Aproblem%252C%2520we%2520treat%2520LLMs%2520as%2520untrusted%2520in%2520privacy%2520and%2520propose%2520a%2520locally%250Adifferentially%2520private%2520framework%2520of%2520in-context%2520learning%2528LDP-ICL%2529%2520in%2520the%250Asettings%2520where%2520labels%2520are%2520sensitive.%2520Considering%2520the%2520mechanisms%2520of%2520in-context%250Alearning%2520in%2520Transformers%2520by%2520gradient%2520descent%252C%2520we%2520provide%2520an%2520analysis%2520of%2520the%250Atrade-off%2520between%2520privacy%2520and%2520utility%2520in%2520such%2520LDP-ICL%2520for%2520classification.%250AMoreover%252C%2520we%2520apply%2520LDP-ICL%2520to%2520the%2520discrete%2520distribution%2520estimation%2520problem.%2520In%250Athe%2520end%252C%2520we%2520perform%2520several%2520experiments%2520to%2520demonstrate%2520our%2520analysis%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04032v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Differentially%20Private%20In-Context%20Learning&entry.906535625=Chunyan%20Zheng%20and%20Keke%20Sun%20and%20Wenhao%20Zhao%20and%20Haibo%20Zhou%20and%20Lixin%20Jiang%20and%20Shaoyang%20Song%20and%20Chunlai%20Zhou&entry.1292438233=%20%20Large%20pretrained%20language%20models%20%28LLMs%29%20have%20shown%20surprising%20In-Context%0ALearning%20%28ICL%29%20ability.%20An%20important%20application%20in%20deploying%20large%20language%0Amodels%20is%20to%20augment%20LLMs%20with%20a%20private%20database%20for%20some%20specific%20task.%20The%0Amain%20problem%20with%20this%20promising%20commercial%20use%20is%20that%20LLMs%20have%20been%20shown%20to%0Amemorize%20their%20training%20data%20and%20their%20prompt%20data%20are%20vulnerable%20to%20membership%0Ainference%20attacks%20%28MIA%29%20and%20prompt%20leaking%20attacks.%20In%20order%20to%20deal%20with%20this%0Aproblem%2C%20we%20treat%20LLMs%20as%20untrusted%20in%20privacy%20and%20propose%20a%20locally%0Adifferentially%20private%20framework%20of%20in-context%20learning%28LDP-ICL%29%20in%20the%0Asettings%20where%20labels%20are%20sensitive.%20Considering%20the%20mechanisms%20of%20in-context%0Alearning%20in%20Transformers%20by%20gradient%20descent%2C%20we%20provide%20an%20analysis%20of%20the%0Atrade-off%20between%20privacy%20and%20utility%20in%20such%20LDP-ICL%20for%20classification.%0AMoreover%2C%20we%20apply%20LDP-ICL%20to%20the%20discrete%20distribution%20estimation%20problem.%20In%0Athe%20end%2C%20we%20perform%20several%20experiments%20to%20demonstrate%20our%20analysis%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04032v2&entry.124074799=Read"},
{"title": "Hybrid Quantum Graph Neural Network for Molecular Property Prediction", "author": "Michael Vitz and Hamed Mohammadbagherpoor and Samarth Sandeep and Andrew Vlasic and Richard Padbury and Anh Pham", "abstract": "  To accelerate the process of materials design, materials science has\nincreasingly used data driven techniques to extract information from collected\ndata. Specially, machine learning (ML) algorithms, which span the ML\ndiscipline, have demonstrated ability to predict various properties of\nmaterials with the level of accuracy similar to explicit calculation of quantum\nmechanical theories, but with significantly reduced run time and computational\nresources. Within ML, graph neural networks have emerged as an important\nalgorithm within the field of machine learning, since they are capable of\npredicting accurately a wide range of important physical, chemical and\nelectronic properties due to their higher learning ability based on the graph\nrepresentation of material and molecular descriptors through the aggregation of\ninformation embedded within the graph. In parallel with the development of\nstate of the art classical machine learning applications, the fusion of quantum\ncomputing and machine learning have created a new paradigm where classical\nmachine learning model can be augmented with quantum layers which are able to\nencode high dimensional data more efficiently. Leveraging the structure of\nexisting algorithms, we developed a unique and novel gradient free hybrid\nquantum classical convoluted graph neural network (HyQCGNN) to predict\nformation energies of perovskite materials. The performance of our hybrid\nstatistical model is competitive with the results obtained purely from a\nclassical convoluted graph neural network, and other classical machine learning\nalgorithms, such as XGBoost. Consequently, our study suggests a new pathway to\nexplore how quantum feature encoding and parametric quantum circuits can yield\ndrastic improvements of complex ML algorithm like graph neural network.\n", "link": "http://arxiv.org/abs/2405.05205v1", "date": "2024-05-08", "relevancy": 1.9229, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5416}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4767}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Quantum%20Graph%20Neural%20Network%20for%20Molecular%20Property%20Prediction&body=Title%3A%20Hybrid%20Quantum%20Graph%20Neural%20Network%20for%20Molecular%20Property%20Prediction%0AAuthor%3A%20Michael%20Vitz%20and%20Hamed%20Mohammadbagherpoor%20and%20Samarth%20Sandeep%20and%20Andrew%20Vlasic%20and%20Richard%20Padbury%20and%20Anh%20Pham%0AAbstract%3A%20%20%20To%20accelerate%20the%20process%20of%20materials%20design%2C%20materials%20science%20has%0Aincreasingly%20used%20data%20driven%20techniques%20to%20extract%20information%20from%20collected%0Adata.%20Specially%2C%20machine%20learning%20%28ML%29%20algorithms%2C%20which%20span%20the%20ML%0Adiscipline%2C%20have%20demonstrated%20ability%20to%20predict%20various%20properties%20of%0Amaterials%20with%20the%20level%20of%20accuracy%20similar%20to%20explicit%20calculation%20of%20quantum%0Amechanical%20theories%2C%20but%20with%20significantly%20reduced%20run%20time%20and%20computational%0Aresources.%20Within%20ML%2C%20graph%20neural%20networks%20have%20emerged%20as%20an%20important%0Aalgorithm%20within%20the%20field%20of%20machine%20learning%2C%20since%20they%20are%20capable%20of%0Apredicting%20accurately%20a%20wide%20range%20of%20important%20physical%2C%20chemical%20and%0Aelectronic%20properties%20due%20to%20their%20higher%20learning%20ability%20based%20on%20the%20graph%0Arepresentation%20of%20material%20and%20molecular%20descriptors%20through%20the%20aggregation%20of%0Ainformation%20embedded%20within%20the%20graph.%20In%20parallel%20with%20the%20development%20of%0Astate%20of%20the%20art%20classical%20machine%20learning%20applications%2C%20the%20fusion%20of%20quantum%0Acomputing%20and%20machine%20learning%20have%20created%20a%20new%20paradigm%20where%20classical%0Amachine%20learning%20model%20can%20be%20augmented%20with%20quantum%20layers%20which%20are%20able%20to%0Aencode%20high%20dimensional%20data%20more%20efficiently.%20Leveraging%20the%20structure%20of%0Aexisting%20algorithms%2C%20we%20developed%20a%20unique%20and%20novel%20gradient%20free%20hybrid%0Aquantum%20classical%20convoluted%20graph%20neural%20network%20%28HyQCGNN%29%20to%20predict%0Aformation%20energies%20of%20perovskite%20materials.%20The%20performance%20of%20our%20hybrid%0Astatistical%20model%20is%20competitive%20with%20the%20results%20obtained%20purely%20from%20a%0Aclassical%20convoluted%20graph%20neural%20network%2C%20and%20other%20classical%20machine%20learning%0Aalgorithms%2C%20such%20as%20XGBoost.%20Consequently%2C%20our%20study%20suggests%20a%20new%20pathway%20to%0Aexplore%20how%20quantum%20feature%20encoding%20and%20parametric%20quantum%20circuits%20can%20yield%0Adrastic%20improvements%20of%20complex%20ML%20algorithm%20like%20graph%20neural%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Quantum%2520Graph%2520Neural%2520Network%2520for%2520Molecular%2520Property%2520Prediction%26entry.906535625%3DMichael%2520Vitz%2520and%2520Hamed%2520Mohammadbagherpoor%2520and%2520Samarth%2520Sandeep%2520and%2520Andrew%2520Vlasic%2520and%2520Richard%2520Padbury%2520and%2520Anh%2520Pham%26entry.1292438233%3D%2520%2520To%2520accelerate%2520the%2520process%2520of%2520materials%2520design%252C%2520materials%2520science%2520has%250Aincreasingly%2520used%2520data%2520driven%2520techniques%2520to%2520extract%2520information%2520from%2520collected%250Adata.%2520Specially%252C%2520machine%2520learning%2520%2528ML%2529%2520algorithms%252C%2520which%2520span%2520the%2520ML%250Adiscipline%252C%2520have%2520demonstrated%2520ability%2520to%2520predict%2520various%2520properties%2520of%250Amaterials%2520with%2520the%2520level%2520of%2520accuracy%2520similar%2520to%2520explicit%2520calculation%2520of%2520quantum%250Amechanical%2520theories%252C%2520but%2520with%2520significantly%2520reduced%2520run%2520time%2520and%2520computational%250Aresources.%2520Within%2520ML%252C%2520graph%2520neural%2520networks%2520have%2520emerged%2520as%2520an%2520important%250Aalgorithm%2520within%2520the%2520field%2520of%2520machine%2520learning%252C%2520since%2520they%2520are%2520capable%2520of%250Apredicting%2520accurately%2520a%2520wide%2520range%2520of%2520important%2520physical%252C%2520chemical%2520and%250Aelectronic%2520properties%2520due%2520to%2520their%2520higher%2520learning%2520ability%2520based%2520on%2520the%2520graph%250Arepresentation%2520of%2520material%2520and%2520molecular%2520descriptors%2520through%2520the%2520aggregation%2520of%250Ainformation%2520embedded%2520within%2520the%2520graph.%2520In%2520parallel%2520with%2520the%2520development%2520of%250Astate%2520of%2520the%2520art%2520classical%2520machine%2520learning%2520applications%252C%2520the%2520fusion%2520of%2520quantum%250Acomputing%2520and%2520machine%2520learning%2520have%2520created%2520a%2520new%2520paradigm%2520where%2520classical%250Amachine%2520learning%2520model%2520can%2520be%2520augmented%2520with%2520quantum%2520layers%2520which%2520are%2520able%2520to%250Aencode%2520high%2520dimensional%2520data%2520more%2520efficiently.%2520Leveraging%2520the%2520structure%2520of%250Aexisting%2520algorithms%252C%2520we%2520developed%2520a%2520unique%2520and%2520novel%2520gradient%2520free%2520hybrid%250Aquantum%2520classical%2520convoluted%2520graph%2520neural%2520network%2520%2528HyQCGNN%2529%2520to%2520predict%250Aformation%2520energies%2520of%2520perovskite%2520materials.%2520The%2520performance%2520of%2520our%2520hybrid%250Astatistical%2520model%2520is%2520competitive%2520with%2520the%2520results%2520obtained%2520purely%2520from%2520a%250Aclassical%2520convoluted%2520graph%2520neural%2520network%252C%2520and%2520other%2520classical%2520machine%2520learning%250Aalgorithms%252C%2520such%2520as%2520XGBoost.%2520Consequently%252C%2520our%2520study%2520suggests%2520a%2520new%2520pathway%2520to%250Aexplore%2520how%2520quantum%2520feature%2520encoding%2520and%2520parametric%2520quantum%2520circuits%2520can%2520yield%250Adrastic%2520improvements%2520of%2520complex%2520ML%2520algorithm%2520like%2520graph%2520neural%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Quantum%20Graph%20Neural%20Network%20for%20Molecular%20Property%20Prediction&entry.906535625=Michael%20Vitz%20and%20Hamed%20Mohammadbagherpoor%20and%20Samarth%20Sandeep%20and%20Andrew%20Vlasic%20and%20Richard%20Padbury%20and%20Anh%20Pham&entry.1292438233=%20%20To%20accelerate%20the%20process%20of%20materials%20design%2C%20materials%20science%20has%0Aincreasingly%20used%20data%20driven%20techniques%20to%20extract%20information%20from%20collected%0Adata.%20Specially%2C%20machine%20learning%20%28ML%29%20algorithms%2C%20which%20span%20the%20ML%0Adiscipline%2C%20have%20demonstrated%20ability%20to%20predict%20various%20properties%20of%0Amaterials%20with%20the%20level%20of%20accuracy%20similar%20to%20explicit%20calculation%20of%20quantum%0Amechanical%20theories%2C%20but%20with%20significantly%20reduced%20run%20time%20and%20computational%0Aresources.%20Within%20ML%2C%20graph%20neural%20networks%20have%20emerged%20as%20an%20important%0Aalgorithm%20within%20the%20field%20of%20machine%20learning%2C%20since%20they%20are%20capable%20of%0Apredicting%20accurately%20a%20wide%20range%20of%20important%20physical%2C%20chemical%20and%0Aelectronic%20properties%20due%20to%20their%20higher%20learning%20ability%20based%20on%20the%20graph%0Arepresentation%20of%20material%20and%20molecular%20descriptors%20through%20the%20aggregation%20of%0Ainformation%20embedded%20within%20the%20graph.%20In%20parallel%20with%20the%20development%20of%0Astate%20of%20the%20art%20classical%20machine%20learning%20applications%2C%20the%20fusion%20of%20quantum%0Acomputing%20and%20machine%20learning%20have%20created%20a%20new%20paradigm%20where%20classical%0Amachine%20learning%20model%20can%20be%20augmented%20with%20quantum%20layers%20which%20are%20able%20to%0Aencode%20high%20dimensional%20data%20more%20efficiently.%20Leveraging%20the%20structure%20of%0Aexisting%20algorithms%2C%20we%20developed%20a%20unique%20and%20novel%20gradient%20free%20hybrid%0Aquantum%20classical%20convoluted%20graph%20neural%20network%20%28HyQCGNN%29%20to%20predict%0Aformation%20energies%20of%20perovskite%20materials.%20The%20performance%20of%20our%20hybrid%0Astatistical%20model%20is%20competitive%20with%20the%20results%20obtained%20purely%20from%20a%0Aclassical%20convoluted%20graph%20neural%20network%2C%20and%20other%20classical%20machine%20learning%0Aalgorithms%2C%20such%20as%20XGBoost.%20Consequently%2C%20our%20study%20suggests%20a%20new%20pathway%20to%0Aexplore%20how%20quantum%20feature%20encoding%20and%20parametric%20quantum%20circuits%20can%20yield%0Adrastic%20improvements%20of%20complex%20ML%20algorithm%20like%20graph%20neural%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05205v1&entry.124074799=Read"},
{"title": "Biology-inspired joint distribution neurons based on Hierarchical\n  Correlation Reconstruction allowing for multidirectional neural networks", "author": "Jarek Duda", "abstract": "  Popular artificial neural networks (ANN) optimize parameters for\nunidirectional value propagation, assuming some guessed parametrization type\nlike Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN). In\ncontrast, for biological neurons e.g. \"it is not uncommon for axonal\npropagation of action potentials to happen in both directions\" \\cite{axon} -\nsuggesting they are optimized to continuously operate in multidirectional way.\nAdditionally, statistical dependencies a single neuron could model is not just\n(expected) value dependence, but entire joint distributions including also\nhigher moments. Such agnostic joint distribution neuron would allow for\nmultidirectional propagation (of distributions or values) e.g. $\\rho(x|y,z)$ or\n$\\rho(y,z|x)$ by substituting to $\\rho(x,y,z)$ and normalizing. There will be\ndiscussed Hierarchical Correlation Reconstruction (HCR) for such neuron model:\nassuming $\\rho(x,y,z)=\\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type\nparametrization of joint distribution with polynomial basis $f_i$, which allows\nfor flexible, inexpensive processing including nonlinearities, direct model\nestimation and update, trained through standard backpropagation or novel ways\nfor such structure up to tensor decomposition. Using only pairwise\n(input-output) dependencies, its expected value prediction becomes KAN-like\nwith trained activation functions as polynomials, can be extended by adding\nhigher order dependencies through included products - in conscious\ninterpretable way, allowing for multidirectional propagation of both values and\nprobability densities.\n", "link": "http://arxiv.org/abs/2405.05097v1", "date": "2024-05-08", "relevancy": 1.9193, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5246}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5106}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks&body=Title%3A%20Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks%0AAuthor%3A%20Jarek%20Duda%0AAbstract%3A%20%20%20Popular%20artificial%20neural%20networks%20%28ANN%29%20optimize%20parameters%20for%0Aunidirectional%20value%20propagation%2C%20assuming%20some%20guessed%20parametrization%20type%0Alike%20Multi-Layer%20Perceptron%20%28MLP%29%20or%20Kolmogorov-Arnold%20Network%20%28KAN%29.%20In%0Acontrast%2C%20for%20biological%20neurons%20e.g.%20%22it%20is%20not%20uncommon%20for%20axonal%0Apropagation%20of%20action%20potentials%20to%20happen%20in%20both%20directions%22%20%5Ccite%7Baxon%7D%20-%0Asuggesting%20they%20are%20optimized%20to%20continuously%20operate%20in%20multidirectional%20way.%0AAdditionally%2C%20statistical%20dependencies%20a%20single%20neuron%20could%20model%20is%20not%20just%0A%28expected%29%20value%20dependence%2C%20but%20entire%20joint%20distributions%20including%20also%0Ahigher%20moments.%20Such%20agnostic%20joint%20distribution%20neuron%20would%20allow%20for%0Amultidirectional%20propagation%20%28of%20distributions%20or%20values%29%20e.g.%20%24%5Crho%28x%7Cy%2Cz%29%24%20or%0A%24%5Crho%28y%2Cz%7Cx%29%24%20by%20substituting%20to%20%24%5Crho%28x%2Cy%2Cz%29%24%20and%20normalizing.%20There%20will%20be%0Adiscussed%20Hierarchical%20Correlation%20Reconstruction%20%28HCR%29%20for%20such%20neuron%20model%3A%0Aassuming%20%24%5Crho%28x%2Cy%2Cz%29%3D%5Csum_%7Bijk%7D%20a_%7Bijk%7D%20f_i%28x%29%20f_j%28y%29%20f_k%28z%29%24%20type%0Aparametrization%20of%20joint%20distribution%20with%20polynomial%20basis%20%24f_i%24%2C%20which%20allows%0Afor%20flexible%2C%20inexpensive%20processing%20including%20nonlinearities%2C%20direct%20model%0Aestimation%20and%20update%2C%20trained%20through%20standard%20backpropagation%20or%20novel%20ways%0Afor%20such%20structure%20up%20to%20tensor%20decomposition.%20Using%20only%20pairwise%0A%28input-output%29%20dependencies%2C%20its%20expected%20value%20prediction%20becomes%20KAN-like%0Awith%20trained%20activation%20functions%20as%20polynomials%2C%20can%20be%20extended%20by%20adding%0Ahigher%20order%20dependencies%20through%20included%20products%20-%20in%20conscious%0Ainterpretable%20way%2C%20allowing%20for%20multidirectional%20propagation%20of%20both%20values%20and%0Aprobability%20densities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiology-inspired%2520joint%2520distribution%2520neurons%2520based%2520on%2520Hierarchical%250A%2520%2520Correlation%2520Reconstruction%2520allowing%2520for%2520multidirectional%2520neural%2520networks%26entry.906535625%3DJarek%2520Duda%26entry.1292438233%3D%2520%2520Popular%2520artificial%2520neural%2520networks%2520%2528ANN%2529%2520optimize%2520parameters%2520for%250Aunidirectional%2520value%2520propagation%252C%2520assuming%2520some%2520guessed%2520parametrization%2520type%250Alike%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520or%2520Kolmogorov-Arnold%2520Network%2520%2528KAN%2529.%2520In%250Acontrast%252C%2520for%2520biological%2520neurons%2520e.g.%2520%2522it%2520is%2520not%2520uncommon%2520for%2520axonal%250Apropagation%2520of%2520action%2520potentials%2520to%2520happen%2520in%2520both%2520directions%2522%2520%255Ccite%257Baxon%257D%2520-%250Asuggesting%2520they%2520are%2520optimized%2520to%2520continuously%2520operate%2520in%2520multidirectional%2520way.%250AAdditionally%252C%2520statistical%2520dependencies%2520a%2520single%2520neuron%2520could%2520model%2520is%2520not%2520just%250A%2528expected%2529%2520value%2520dependence%252C%2520but%2520entire%2520joint%2520distributions%2520including%2520also%250Ahigher%2520moments.%2520Such%2520agnostic%2520joint%2520distribution%2520neuron%2520would%2520allow%2520for%250Amultidirectional%2520propagation%2520%2528of%2520distributions%2520or%2520values%2529%2520e.g.%2520%2524%255Crho%2528x%257Cy%252Cz%2529%2524%2520or%250A%2524%255Crho%2528y%252Cz%257Cx%2529%2524%2520by%2520substituting%2520to%2520%2524%255Crho%2528x%252Cy%252Cz%2529%2524%2520and%2520normalizing.%2520There%2520will%2520be%250Adiscussed%2520Hierarchical%2520Correlation%2520Reconstruction%2520%2528HCR%2529%2520for%2520such%2520neuron%2520model%253A%250Aassuming%2520%2524%255Crho%2528x%252Cy%252Cz%2529%253D%255Csum_%257Bijk%257D%2520a_%257Bijk%257D%2520f_i%2528x%2529%2520f_j%2528y%2529%2520f_k%2528z%2529%2524%2520type%250Aparametrization%2520of%2520joint%2520distribution%2520with%2520polynomial%2520basis%2520%2524f_i%2524%252C%2520which%2520allows%250Afor%2520flexible%252C%2520inexpensive%2520processing%2520including%2520nonlinearities%252C%2520direct%2520model%250Aestimation%2520and%2520update%252C%2520trained%2520through%2520standard%2520backpropagation%2520or%2520novel%2520ways%250Afor%2520such%2520structure%2520up%2520to%2520tensor%2520decomposition.%2520Using%2520only%2520pairwise%250A%2528input-output%2529%2520dependencies%252C%2520its%2520expected%2520value%2520prediction%2520becomes%2520KAN-like%250Awith%2520trained%2520activation%2520functions%2520as%2520polynomials%252C%2520can%2520be%2520extended%2520by%2520adding%250Ahigher%2520order%2520dependencies%2520through%2520included%2520products%2520-%2520in%2520conscious%250Ainterpretable%2520way%252C%2520allowing%2520for%2520multidirectional%2520propagation%2520of%2520both%2520values%2520and%250Aprobability%2520densities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks&entry.906535625=Jarek%20Duda&entry.1292438233=%20%20Popular%20artificial%20neural%20networks%20%28ANN%29%20optimize%20parameters%20for%0Aunidirectional%20value%20propagation%2C%20assuming%20some%20guessed%20parametrization%20type%0Alike%20Multi-Layer%20Perceptron%20%28MLP%29%20or%20Kolmogorov-Arnold%20Network%20%28KAN%29.%20In%0Acontrast%2C%20for%20biological%20neurons%20e.g.%20%22it%20is%20not%20uncommon%20for%20axonal%0Apropagation%20of%20action%20potentials%20to%20happen%20in%20both%20directions%22%20%5Ccite%7Baxon%7D%20-%0Asuggesting%20they%20are%20optimized%20to%20continuously%20operate%20in%20multidirectional%20way.%0AAdditionally%2C%20statistical%20dependencies%20a%20single%20neuron%20could%20model%20is%20not%20just%0A%28expected%29%20value%20dependence%2C%20but%20entire%20joint%20distributions%20including%20also%0Ahigher%20moments.%20Such%20agnostic%20joint%20distribution%20neuron%20would%20allow%20for%0Amultidirectional%20propagation%20%28of%20distributions%20or%20values%29%20e.g.%20%24%5Crho%28x%7Cy%2Cz%29%24%20or%0A%24%5Crho%28y%2Cz%7Cx%29%24%20by%20substituting%20to%20%24%5Crho%28x%2Cy%2Cz%29%24%20and%20normalizing.%20There%20will%20be%0Adiscussed%20Hierarchical%20Correlation%20Reconstruction%20%28HCR%29%20for%20such%20neuron%20model%3A%0Aassuming%20%24%5Crho%28x%2Cy%2Cz%29%3D%5Csum_%7Bijk%7D%20a_%7Bijk%7D%20f_i%28x%29%20f_j%28y%29%20f_k%28z%29%24%20type%0Aparametrization%20of%20joint%20distribution%20with%20polynomial%20basis%20%24f_i%24%2C%20which%20allows%0Afor%20flexible%2C%20inexpensive%20processing%20including%20nonlinearities%2C%20direct%20model%0Aestimation%20and%20update%2C%20trained%20through%20standard%20backpropagation%20or%20novel%20ways%0Afor%20such%20structure%20up%20to%20tensor%20decomposition.%20Using%20only%20pairwise%0A%28input-output%29%20dependencies%2C%20its%20expected%20value%20prediction%20becomes%20KAN-like%0Awith%20trained%20activation%20functions%20as%20polynomials%2C%20can%20be%20extended%20by%20adding%0Ahigher%20order%20dependencies%20through%20included%20products%20-%20in%20conscious%0Ainterpretable%20way%2C%20allowing%20for%20multidirectional%20propagation%20of%20both%20values%20and%0Aprobability%20densities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05097v1&entry.124074799=Read"},
{"title": "Fault Identification Enhancement with Reinforcement Learning (FIERL)", "author": "Valentina Zaccaria and Davide Sartor and Simone Del Favero and Gian Antonio Susto", "abstract": "  This letter presents a novel approach in the field of Active Fault Detection\n(AFD), by explicitly separating the task into two parts: Passive Fault\nDetection (PFD) and control input design. This formulation is very general, and\nmost existing AFD literature can be viewed through this lens. By recognizing\nthis separation, PFD methods can be leveraged to provide components that make\nefficient use of the available information, while the control input is designed\nin order to optimize the gathering of information. The core contribution of\nthis work is FIERL, a general simulation-based approach for the design of such\ncontrol strategies, using Constrained Reinforcement Learning (CRL) to optimize\nthe performance of arbitrary passive detectors. The control policy is learned\nwithout the need of knowing the passive detector inner workings, making FIERL\nbroadly applicable. However, it is especially useful when paired with the\ndesign of an efficient passive component. Unlike most AFD approaches, FIERL can\nhandle fairly complex scenarios such as continuous sets of fault modes. The\neffectiveness of FIERL is tested on a benchmark problem for actuator fault\ndiagnosis, where FIERL is shown to be fairly robust, being able to generalize\nto fault dynamics not seen in training.\n", "link": "http://arxiv.org/abs/2405.04938v1", "date": "2024-05-08", "relevancy": 1.9063, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4939}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4789}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fault%20Identification%20Enhancement%20with%20Reinforcement%20Learning%20%28FIERL%29&body=Title%3A%20Fault%20Identification%20Enhancement%20with%20Reinforcement%20Learning%20%28FIERL%29%0AAuthor%3A%20Valentina%20Zaccaria%20and%20Davide%20Sartor%20and%20Simone%20Del%20Favero%20and%20Gian%20Antonio%20Susto%0AAbstract%3A%20%20%20This%20letter%20presents%20a%20novel%20approach%20in%20the%20field%20of%20Active%20Fault%20Detection%0A%28AFD%29%2C%20by%20explicitly%20separating%20the%20task%20into%20two%20parts%3A%20Passive%20Fault%0ADetection%20%28PFD%29%20and%20control%20input%20design.%20This%20formulation%20is%20very%20general%2C%20and%0Amost%20existing%20AFD%20literature%20can%20be%20viewed%20through%20this%20lens.%20By%20recognizing%0Athis%20separation%2C%20PFD%20methods%20can%20be%20leveraged%20to%20provide%20components%20that%20make%0Aefficient%20use%20of%20the%20available%20information%2C%20while%20the%20control%20input%20is%20designed%0Ain%20order%20to%20optimize%20the%20gathering%20of%20information.%20The%20core%20contribution%20of%0Athis%20work%20is%20FIERL%2C%20a%20general%20simulation-based%20approach%20for%20the%20design%20of%20such%0Acontrol%20strategies%2C%20using%20Constrained%20Reinforcement%20Learning%20%28CRL%29%20to%20optimize%0Athe%20performance%20of%20arbitrary%20passive%20detectors.%20The%20control%20policy%20is%20learned%0Awithout%20the%20need%20of%20knowing%20the%20passive%20detector%20inner%20workings%2C%20making%20FIERL%0Abroadly%20applicable.%20However%2C%20it%20is%20especially%20useful%20when%20paired%20with%20the%0Adesign%20of%20an%20efficient%20passive%20component.%20Unlike%20most%20AFD%20approaches%2C%20FIERL%20can%0Ahandle%20fairly%20complex%20scenarios%20such%20as%20continuous%20sets%20of%20fault%20modes.%20The%0Aeffectiveness%20of%20FIERL%20is%20tested%20on%20a%20benchmark%20problem%20for%20actuator%20fault%0Adiagnosis%2C%20where%20FIERL%20is%20shown%20to%20be%20fairly%20robust%2C%20being%20able%20to%20generalize%0Ato%20fault%20dynamics%20not%20seen%20in%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFault%2520Identification%2520Enhancement%2520with%2520Reinforcement%2520Learning%2520%2528FIERL%2529%26entry.906535625%3DValentina%2520Zaccaria%2520and%2520Davide%2520Sartor%2520and%2520Simone%2520Del%2520Favero%2520and%2520Gian%2520Antonio%2520Susto%26entry.1292438233%3D%2520%2520This%2520letter%2520presents%2520a%2520novel%2520approach%2520in%2520the%2520field%2520of%2520Active%2520Fault%2520Detection%250A%2528AFD%2529%252C%2520by%2520explicitly%2520separating%2520the%2520task%2520into%2520two%2520parts%253A%2520Passive%2520Fault%250ADetection%2520%2528PFD%2529%2520and%2520control%2520input%2520design.%2520This%2520formulation%2520is%2520very%2520general%252C%2520and%250Amost%2520existing%2520AFD%2520literature%2520can%2520be%2520viewed%2520through%2520this%2520lens.%2520By%2520recognizing%250Athis%2520separation%252C%2520PFD%2520methods%2520can%2520be%2520leveraged%2520to%2520provide%2520components%2520that%2520make%250Aefficient%2520use%2520of%2520the%2520available%2520information%252C%2520while%2520the%2520control%2520input%2520is%2520designed%250Ain%2520order%2520to%2520optimize%2520the%2520gathering%2520of%2520information.%2520The%2520core%2520contribution%2520of%250Athis%2520work%2520is%2520FIERL%252C%2520a%2520general%2520simulation-based%2520approach%2520for%2520the%2520design%2520of%2520such%250Acontrol%2520strategies%252C%2520using%2520Constrained%2520Reinforcement%2520Learning%2520%2528CRL%2529%2520to%2520optimize%250Athe%2520performance%2520of%2520arbitrary%2520passive%2520detectors.%2520The%2520control%2520policy%2520is%2520learned%250Awithout%2520the%2520need%2520of%2520knowing%2520the%2520passive%2520detector%2520inner%2520workings%252C%2520making%2520FIERL%250Abroadly%2520applicable.%2520However%252C%2520it%2520is%2520especially%2520useful%2520when%2520paired%2520with%2520the%250Adesign%2520of%2520an%2520efficient%2520passive%2520component.%2520Unlike%2520most%2520AFD%2520approaches%252C%2520FIERL%2520can%250Ahandle%2520fairly%2520complex%2520scenarios%2520such%2520as%2520continuous%2520sets%2520of%2520fault%2520modes.%2520The%250Aeffectiveness%2520of%2520FIERL%2520is%2520tested%2520on%2520a%2520benchmark%2520problem%2520for%2520actuator%2520fault%250Adiagnosis%252C%2520where%2520FIERL%2520is%2520shown%2520to%2520be%2520fairly%2520robust%252C%2520being%2520able%2520to%2520generalize%250Ato%2520fault%2520dynamics%2520not%2520seen%2520in%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fault%20Identification%20Enhancement%20with%20Reinforcement%20Learning%20%28FIERL%29&entry.906535625=Valentina%20Zaccaria%20and%20Davide%20Sartor%20and%20Simone%20Del%20Favero%20and%20Gian%20Antonio%20Susto&entry.1292438233=%20%20This%20letter%20presents%20a%20novel%20approach%20in%20the%20field%20of%20Active%20Fault%20Detection%0A%28AFD%29%2C%20by%20explicitly%20separating%20the%20task%20into%20two%20parts%3A%20Passive%20Fault%0ADetection%20%28PFD%29%20and%20control%20input%20design.%20This%20formulation%20is%20very%20general%2C%20and%0Amost%20existing%20AFD%20literature%20can%20be%20viewed%20through%20this%20lens.%20By%20recognizing%0Athis%20separation%2C%20PFD%20methods%20can%20be%20leveraged%20to%20provide%20components%20that%20make%0Aefficient%20use%20of%20the%20available%20information%2C%20while%20the%20control%20input%20is%20designed%0Ain%20order%20to%20optimize%20the%20gathering%20of%20information.%20The%20core%20contribution%20of%0Athis%20work%20is%20FIERL%2C%20a%20general%20simulation-based%20approach%20for%20the%20design%20of%20such%0Acontrol%20strategies%2C%20using%20Constrained%20Reinforcement%20Learning%20%28CRL%29%20to%20optimize%0Athe%20performance%20of%20arbitrary%20passive%20detectors.%20The%20control%20policy%20is%20learned%0Awithout%20the%20need%20of%20knowing%20the%20passive%20detector%20inner%20workings%2C%20making%20FIERL%0Abroadly%20applicable.%20However%2C%20it%20is%20especially%20useful%20when%20paired%20with%20the%0Adesign%20of%20an%20efficient%20passive%20component.%20Unlike%20most%20AFD%20approaches%2C%20FIERL%20can%0Ahandle%20fairly%20complex%20scenarios%20such%20as%20continuous%20sets%20of%20fault%20modes.%20The%0Aeffectiveness%20of%20FIERL%20is%20tested%20on%20a%20benchmark%20problem%20for%20actuator%20fault%0Adiagnosis%2C%20where%20FIERL%20is%20shown%20to%20be%20fairly%20robust%2C%20being%20able%20to%20generalize%0Ato%20fault%20dynamics%20not%20seen%20in%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04938v1&entry.124074799=Read"},
{"title": "Imagine Flash: Accelerating Emu Diffusion Models with Backward\n  Distillation", "author": "Jonas Kohler and Albert Pumarola and Edgar Sch\u00f6nfeld and Artsiom Sanakoyeu and Roshan Sumbaly and Peter Vajda and Ali Thabet", "abstract": "  Diffusion models are a powerful generative framework, but come with expensive\ninference. Existing acceleration methods often compromise image quality or fail\nunder complex conditioning when operating in an extremely low-step regime. In\nthis work, we propose a novel distillation framework tailored to enable\nhigh-fidelity, diverse sample generation using just one to three steps. Our\napproach comprises three key components: (i) Backward Distillation, which\nmitigates training-inference discrepancies by calibrating the student on its\nown backward trajectory; (ii) Shifted Reconstruction Loss that dynamically\nadapts knowledge transfer based on the current time step; and (iii) Noise\nCorrection, an inference-time technique that enhances sample quality by\naddressing singularities in noise prediction. Through extensive experiments, we\ndemonstrate that our method outperforms existing competitors in quantitative\nmetrics and human evaluations. Remarkably, it achieves performance comparable\nto the teacher model using only three denoising steps, enabling efficient\nhigh-quality generation.\n", "link": "http://arxiv.org/abs/2405.05224v1", "date": "2024-05-08", "relevancy": 1.9056, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6723}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6269}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imagine%20Flash%3A%20Accelerating%20Emu%20Diffusion%20Models%20with%20Backward%0A%20%20Distillation&body=Title%3A%20Imagine%20Flash%3A%20Accelerating%20Emu%20Diffusion%20Models%20with%20Backward%0A%20%20Distillation%0AAuthor%3A%20Jonas%20Kohler%20and%20Albert%20Pumarola%20and%20Edgar%20Sch%C3%B6nfeld%20and%20Artsiom%20Sanakoyeu%20and%20Roshan%20Sumbaly%20and%20Peter%20Vajda%20and%20Ali%20Thabet%0AAbstract%3A%20%20%20Diffusion%20models%20are%20a%20powerful%20generative%20framework%2C%20but%20come%20with%20expensive%0Ainference.%20Existing%20acceleration%20methods%20often%20compromise%20image%20quality%20or%20fail%0Aunder%20complex%20conditioning%20when%20operating%20in%20an%20extremely%20low-step%20regime.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20distillation%20framework%20tailored%20to%20enable%0Ahigh-fidelity%2C%20diverse%20sample%20generation%20using%20just%20one%20to%20three%20steps.%20Our%0Aapproach%20comprises%20three%20key%20components%3A%20%28i%29%20Backward%20Distillation%2C%20which%0Amitigates%20training-inference%20discrepancies%20by%20calibrating%20the%20student%20on%20its%0Aown%20backward%20trajectory%3B%20%28ii%29%20Shifted%20Reconstruction%20Loss%20that%20dynamically%0Aadapts%20knowledge%20transfer%20based%20on%20the%20current%20time%20step%3B%20and%20%28iii%29%20Noise%0ACorrection%2C%20an%20inference-time%20technique%20that%20enhances%20sample%20quality%20by%0Aaddressing%20singularities%20in%20noise%20prediction.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20competitors%20in%20quantitative%0Ametrics%20and%20human%20evaluations.%20Remarkably%2C%20it%20achieves%20performance%20comparable%0Ato%20the%20teacher%20model%20using%20only%20three%20denoising%20steps%2C%20enabling%20efficient%0Ahigh-quality%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagine%2520Flash%253A%2520Accelerating%2520Emu%2520Diffusion%2520Models%2520with%2520Backward%250A%2520%2520Distillation%26entry.906535625%3DJonas%2520Kohler%2520and%2520Albert%2520Pumarola%2520and%2520Edgar%2520Sch%25C3%25B6nfeld%2520and%2520Artsiom%2520Sanakoyeu%2520and%2520Roshan%2520Sumbaly%2520and%2520Peter%2520Vajda%2520and%2520Ali%2520Thabet%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520are%2520a%2520powerful%2520generative%2520framework%252C%2520but%2520come%2520with%2520expensive%250Ainference.%2520Existing%2520acceleration%2520methods%2520often%2520compromise%2520image%2520quality%2520or%2520fail%250Aunder%2520complex%2520conditioning%2520when%2520operating%2520in%2520an%2520extremely%2520low-step%2520regime.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520novel%2520distillation%2520framework%2520tailored%2520to%2520enable%250Ahigh-fidelity%252C%2520diverse%2520sample%2520generation%2520using%2520just%2520one%2520to%2520three%2520steps.%2520Our%250Aapproach%2520comprises%2520three%2520key%2520components%253A%2520%2528i%2529%2520Backward%2520Distillation%252C%2520which%250Amitigates%2520training-inference%2520discrepancies%2520by%2520calibrating%2520the%2520student%2520on%2520its%250Aown%2520backward%2520trajectory%253B%2520%2528ii%2529%2520Shifted%2520Reconstruction%2520Loss%2520that%2520dynamically%250Aadapts%2520knowledge%2520transfer%2520based%2520on%2520the%2520current%2520time%2520step%253B%2520and%2520%2528iii%2529%2520Noise%250ACorrection%252C%2520an%2520inference-time%2520technique%2520that%2520enhances%2520sample%2520quality%2520by%250Aaddressing%2520singularities%2520in%2520noise%2520prediction.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520competitors%2520in%2520quantitative%250Ametrics%2520and%2520human%2520evaluations.%2520Remarkably%252C%2520it%2520achieves%2520performance%2520comparable%250Ato%2520the%2520teacher%2520model%2520using%2520only%2520three%2520denoising%2520steps%252C%2520enabling%2520efficient%250Ahigh-quality%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagine%20Flash%3A%20Accelerating%20Emu%20Diffusion%20Models%20with%20Backward%0A%20%20Distillation&entry.906535625=Jonas%20Kohler%20and%20Albert%20Pumarola%20and%20Edgar%20Sch%C3%B6nfeld%20and%20Artsiom%20Sanakoyeu%20and%20Roshan%20Sumbaly%20and%20Peter%20Vajda%20and%20Ali%20Thabet&entry.1292438233=%20%20Diffusion%20models%20are%20a%20powerful%20generative%20framework%2C%20but%20come%20with%20expensive%0Ainference.%20Existing%20acceleration%20methods%20often%20compromise%20image%20quality%20or%20fail%0Aunder%20complex%20conditioning%20when%20operating%20in%20an%20extremely%20low-step%20regime.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20distillation%20framework%20tailored%20to%20enable%0Ahigh-fidelity%2C%20diverse%20sample%20generation%20using%20just%20one%20to%20three%20steps.%20Our%0Aapproach%20comprises%20three%20key%20components%3A%20%28i%29%20Backward%20Distillation%2C%20which%0Amitigates%20training-inference%20discrepancies%20by%20calibrating%20the%20student%20on%20its%0Aown%20backward%20trajectory%3B%20%28ii%29%20Shifted%20Reconstruction%20Loss%20that%20dynamically%0Aadapts%20knowledge%20transfer%20based%20on%20the%20current%20time%20step%3B%20and%20%28iii%29%20Noise%0ACorrection%2C%20an%20inference-time%20technique%20that%20enhances%20sample%20quality%20by%0Aaddressing%20singularities%20in%20noise%20prediction.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20competitors%20in%20quantitative%0Ametrics%20and%20human%20evaluations.%20Remarkably%2C%20it%20achieves%20performance%20comparable%0Ato%20the%20teacher%20model%20using%20only%20three%20denoising%20steps%2C%20enabling%20efficient%0Ahigh-quality%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05224v1&entry.124074799=Read"},
{"title": "Unveiling Molecular Moieties through Hierarchical Graph Explainability", "author": "Paolo Sortino and Salvatore Contino and Ugo Perricone and Roberto Pirrone", "abstract": "  Background: Graph Neural Networks (GNN) have emerged in very recent years as\na powerful tool for supporting in silico Virtual Screening. In this work we\npresent a GNN which uses Graph Convolutional architectures to achieve very\naccurate multi-target screening. We also devised a hierarchical Explainable\nArtificial Intelligence (XAI) technique to catch information directly at atom,\nring, and whole molecule level by leveraging the message passing mechanism. In\nthis way, we find the most relevant moieties involved in bioactivity\nprediction. Results: We report a state-of-the-art GNN classifier on twenty\nCyclin-dependent Kinase targets in support of VS. Our classifier outperforms\nprevious SOTA approaches proposed by the authors. Moreover, a CDK1-only\nhigh-sensitivity version of the GNN has been designed to use our explainer in\norder to avoid the inherent bias of multi-class models. The hierarchical\nexplainer has been validated by an expert chemist on 19 approved drugs on CDK1.\nOur explainer provided information in accordance to the docking analysis for 17\nout of the 19 test drugs. Conclusion: Our approach is a valid support for\nshortening both the screening and the hit-to-lead phase. Detailed knowledge\nabout the molecular substructures that play a role in the inhibitory action,\ncan help the computational chemist to gain insights into the pharmacophoric\nfunction of the molecule also for repurposing purposes. Scientific Contribution\nStatement: The core scientific innovation of our work is the use of a\nhierarchical XAI approach on a GNN trained for a ligand-based VS task. The\napplication of the hierarchical explainer allows for eliciting also structural\ninformation...\n", "link": "http://arxiv.org/abs/2402.01744v3", "date": "2024-05-08", "relevancy": 1.889, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5008}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4683}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Molecular%20Moieties%20through%20Hierarchical%20Graph%20Explainability&body=Title%3A%20Unveiling%20Molecular%20Moieties%20through%20Hierarchical%20Graph%20Explainability%0AAuthor%3A%20Paolo%20Sortino%20and%20Salvatore%20Contino%20and%20Ugo%20Perricone%20and%20Roberto%20Pirrone%0AAbstract%3A%20%20%20Background%3A%20Graph%20Neural%20Networks%20%28GNN%29%20have%20emerged%20in%20very%20recent%20years%20as%0Aa%20powerful%20tool%20for%20supporting%20in%20silico%20Virtual%20Screening.%20In%20this%20work%20we%0Apresent%20a%20GNN%20which%20uses%20Graph%20Convolutional%20architectures%20to%20achieve%20very%0Aaccurate%20multi-target%20screening.%20We%20also%20devised%20a%20hierarchical%20Explainable%0AArtificial%20Intelligence%20%28XAI%29%20technique%20to%20catch%20information%20directly%20at%20atom%2C%0Aring%2C%20and%20whole%20molecule%20level%20by%20leveraging%20the%20message%20passing%20mechanism.%20In%0Athis%20way%2C%20we%20find%20the%20most%20relevant%20moieties%20involved%20in%20bioactivity%0Aprediction.%20Results%3A%20We%20report%20a%20state-of-the-art%20GNN%20classifier%20on%20twenty%0ACyclin-dependent%20Kinase%20targets%20in%20support%20of%20VS.%20Our%20classifier%20outperforms%0Aprevious%20SOTA%20approaches%20proposed%20by%20the%20authors.%20Moreover%2C%20a%20CDK1-only%0Ahigh-sensitivity%20version%20of%20the%20GNN%20has%20been%20designed%20to%20use%20our%20explainer%20in%0Aorder%20to%20avoid%20the%20inherent%20bias%20of%20multi-class%20models.%20The%20hierarchical%0Aexplainer%20has%20been%20validated%20by%20an%20expert%20chemist%20on%2019%20approved%20drugs%20on%20CDK1.%0AOur%20explainer%20provided%20information%20in%20accordance%20to%20the%20docking%20analysis%20for%2017%0Aout%20of%20the%2019%20test%20drugs.%20Conclusion%3A%20Our%20approach%20is%20a%20valid%20support%20for%0Ashortening%20both%20the%20screening%20and%20the%20hit-to-lead%20phase.%20Detailed%20knowledge%0Aabout%20the%20molecular%20substructures%20that%20play%20a%20role%20in%20the%20inhibitory%20action%2C%0Acan%20help%20the%20computational%20chemist%20to%20gain%20insights%20into%20the%20pharmacophoric%0Afunction%20of%20the%20molecule%20also%20for%20repurposing%20purposes.%20Scientific%20Contribution%0AStatement%3A%20The%20core%20scientific%20innovation%20of%20our%20work%20is%20the%20use%20of%20a%0Ahierarchical%20XAI%20approach%20on%20a%20GNN%20trained%20for%20a%20ligand-based%20VS%20task.%20The%0Aapplication%20of%20the%20hierarchical%20explainer%20allows%20for%20eliciting%20also%20structural%0Ainformation...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01744v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Molecular%2520Moieties%2520through%2520Hierarchical%2520Graph%2520Explainability%26entry.906535625%3DPaolo%2520Sortino%2520and%2520Salvatore%2520Contino%2520and%2520Ugo%2520Perricone%2520and%2520Roberto%2520Pirrone%26entry.1292438233%3D%2520%2520Background%253A%2520Graph%2520Neural%2520Networks%2520%2528GNN%2529%2520have%2520emerged%2520in%2520very%2520recent%2520years%2520as%250Aa%2520powerful%2520tool%2520for%2520supporting%2520in%2520silico%2520Virtual%2520Screening.%2520In%2520this%2520work%2520we%250Apresent%2520a%2520GNN%2520which%2520uses%2520Graph%2520Convolutional%2520architectures%2520to%2520achieve%2520very%250Aaccurate%2520multi-target%2520screening.%2520We%2520also%2520devised%2520a%2520hierarchical%2520Explainable%250AArtificial%2520Intelligence%2520%2528XAI%2529%2520technique%2520to%2520catch%2520information%2520directly%2520at%2520atom%252C%250Aring%252C%2520and%2520whole%2520molecule%2520level%2520by%2520leveraging%2520the%2520message%2520passing%2520mechanism.%2520In%250Athis%2520way%252C%2520we%2520find%2520the%2520most%2520relevant%2520moieties%2520involved%2520in%2520bioactivity%250Aprediction.%2520Results%253A%2520We%2520report%2520a%2520state-of-the-art%2520GNN%2520classifier%2520on%2520twenty%250ACyclin-dependent%2520Kinase%2520targets%2520in%2520support%2520of%2520VS.%2520Our%2520classifier%2520outperforms%250Aprevious%2520SOTA%2520approaches%2520proposed%2520by%2520the%2520authors.%2520Moreover%252C%2520a%2520CDK1-only%250Ahigh-sensitivity%2520version%2520of%2520the%2520GNN%2520has%2520been%2520designed%2520to%2520use%2520our%2520explainer%2520in%250Aorder%2520to%2520avoid%2520the%2520inherent%2520bias%2520of%2520multi-class%2520models.%2520The%2520hierarchical%250Aexplainer%2520has%2520been%2520validated%2520by%2520an%2520expert%2520chemist%2520on%252019%2520approved%2520drugs%2520on%2520CDK1.%250AOur%2520explainer%2520provided%2520information%2520in%2520accordance%2520to%2520the%2520docking%2520analysis%2520for%252017%250Aout%2520of%2520the%252019%2520test%2520drugs.%2520Conclusion%253A%2520Our%2520approach%2520is%2520a%2520valid%2520support%2520for%250Ashortening%2520both%2520the%2520screening%2520and%2520the%2520hit-to-lead%2520phase.%2520Detailed%2520knowledge%250Aabout%2520the%2520molecular%2520substructures%2520that%2520play%2520a%2520role%2520in%2520the%2520inhibitory%2520action%252C%250Acan%2520help%2520the%2520computational%2520chemist%2520to%2520gain%2520insights%2520into%2520the%2520pharmacophoric%250Afunction%2520of%2520the%2520molecule%2520also%2520for%2520repurposing%2520purposes.%2520Scientific%2520Contribution%250AStatement%253A%2520The%2520core%2520scientific%2520innovation%2520of%2520our%2520work%2520is%2520the%2520use%2520of%2520a%250Ahierarchical%2520XAI%2520approach%2520on%2520a%2520GNN%2520trained%2520for%2520a%2520ligand-based%2520VS%2520task.%2520The%250Aapplication%2520of%2520the%2520hierarchical%2520explainer%2520allows%2520for%2520eliciting%2520also%2520structural%250Ainformation...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01744v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Molecular%20Moieties%20through%20Hierarchical%20Graph%20Explainability&entry.906535625=Paolo%20Sortino%20and%20Salvatore%20Contino%20and%20Ugo%20Perricone%20and%20Roberto%20Pirrone&entry.1292438233=%20%20Background%3A%20Graph%20Neural%20Networks%20%28GNN%29%20have%20emerged%20in%20very%20recent%20years%20as%0Aa%20powerful%20tool%20for%20supporting%20in%20silico%20Virtual%20Screening.%20In%20this%20work%20we%0Apresent%20a%20GNN%20which%20uses%20Graph%20Convolutional%20architectures%20to%20achieve%20very%0Aaccurate%20multi-target%20screening.%20We%20also%20devised%20a%20hierarchical%20Explainable%0AArtificial%20Intelligence%20%28XAI%29%20technique%20to%20catch%20information%20directly%20at%20atom%2C%0Aring%2C%20and%20whole%20molecule%20level%20by%20leveraging%20the%20message%20passing%20mechanism.%20In%0Athis%20way%2C%20we%20find%20the%20most%20relevant%20moieties%20involved%20in%20bioactivity%0Aprediction.%20Results%3A%20We%20report%20a%20state-of-the-art%20GNN%20classifier%20on%20twenty%0ACyclin-dependent%20Kinase%20targets%20in%20support%20of%20VS.%20Our%20classifier%20outperforms%0Aprevious%20SOTA%20approaches%20proposed%20by%20the%20authors.%20Moreover%2C%20a%20CDK1-only%0Ahigh-sensitivity%20version%20of%20the%20GNN%20has%20been%20designed%20to%20use%20our%20explainer%20in%0Aorder%20to%20avoid%20the%20inherent%20bias%20of%20multi-class%20models.%20The%20hierarchical%0Aexplainer%20has%20been%20validated%20by%20an%20expert%20chemist%20on%2019%20approved%20drugs%20on%20CDK1.%0AOur%20explainer%20provided%20information%20in%20accordance%20to%20the%20docking%20analysis%20for%2017%0Aout%20of%20the%2019%20test%20drugs.%20Conclusion%3A%20Our%20approach%20is%20a%20valid%20support%20for%0Ashortening%20both%20the%20screening%20and%20the%20hit-to-lead%20phase.%20Detailed%20knowledge%0Aabout%20the%20molecular%20substructures%20that%20play%20a%20role%20in%20the%20inhibitory%20action%2C%0Acan%20help%20the%20computational%20chemist%20to%20gain%20insights%20into%20the%20pharmacophoric%0Afunction%20of%20the%20molecule%20also%20for%20repurposing%20purposes.%20Scientific%20Contribution%0AStatement%3A%20The%20core%20scientific%20innovation%20of%20our%20work%20is%20the%20use%20of%20a%0Ahierarchical%20XAI%20approach%20on%20a%20GNN%20trained%20for%20a%20ligand-based%20VS%20task.%20The%0Aapplication%20of%20the%20hierarchical%20explainer%20allows%20for%20eliciting%20also%20structural%0Ainformation...%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01744v3&entry.124074799=Read"},
{"title": "Multi-fidelity Hamiltonian Monte Carlo", "author": "Dhruv V. Patel and Jonghyun Lee and Matthew W. Farthing and Peter K. Kitanidis and Eric F. Darve", "abstract": "  Numerous applications in biology, statistics, science, and engineering\nrequire generating samples from high-dimensional probability distributions. In\nrecent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a\nstate-of-the-art Markov chain Monte Carlo technique, exploiting the shape of\nsuch high-dimensional target distributions to efficiently generate samples.\nDespite its impressive empirical success and increasing popularity, its\nwide-scale adoption remains limited due to the high computational cost of\ngradient calculation. Moreover, applying this method is impossible when the\ngradient of the posterior cannot be computed (for example, with black-box\nsimulators). To overcome these challenges, we propose a novel two-stage\nHamiltonian Monte Carlo algorithm with a surrogate model. In this\nmulti-fidelity algorithm, the acceptance probability is computed in the first\nstage via a standard HMC proposal using an inexpensive differentiable surrogate\nmodel, and if the proposal is accepted, the posterior is evaluated in the\nsecond stage using the high-fidelity (HF) numerical solver. Splitting the\nstandard HMC algorithm into these two stages allows for approximating the\ngradient of the posterior efficiently, while producing accurate posterior\nsamples by using HF numerical solvers in the second stage. We demonstrate the\neffectiveness of this algorithm for a range of problems, including linear and\nnonlinear Bayesian inverse problems with in-silico data and experimental data.\nThe proposed algorithm is shown to seamlessly integrate with various\nlow-fidelity and HF models, priors, and datasets. Remarkably, our proposed\nmethod outperforms the traditional HMC algorithm in both computational and\nstatistical efficiency by several orders of magnitude, all while retaining or\nimproving the accuracy in computed posterior statistics.\n", "link": "http://arxiv.org/abs/2405.05033v1", "date": "2024-05-08", "relevancy": 1.8841, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4655}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-fidelity%20Hamiltonian%20Monte%20Carlo&body=Title%3A%20Multi-fidelity%20Hamiltonian%20Monte%20Carlo%0AAuthor%3A%20Dhruv%20V.%20Patel%20and%20Jonghyun%20Lee%20and%20Matthew%20W.%20Farthing%20and%20Peter%20K.%20Kitanidis%20and%20Eric%20F.%20Darve%0AAbstract%3A%20%20%20Numerous%20applications%20in%20biology%2C%20statistics%2C%20science%2C%20and%20engineering%0Arequire%20generating%20samples%20from%20high-dimensional%20probability%20distributions.%20In%0Arecent%20years%2C%20the%20Hamiltonian%20Monte%20Carlo%20%28HMC%29%20method%20has%20emerged%20as%20a%0Astate-of-the-art%20Markov%20chain%20Monte%20Carlo%20technique%2C%20exploiting%20the%20shape%20of%0Asuch%20high-dimensional%20target%20distributions%20to%20efficiently%20generate%20samples.%0ADespite%20its%20impressive%20empirical%20success%20and%20increasing%20popularity%2C%20its%0Awide-scale%20adoption%20remains%20limited%20due%20to%20the%20high%20computational%20cost%20of%0Agradient%20calculation.%20Moreover%2C%20applying%20this%20method%20is%20impossible%20when%20the%0Agradient%20of%20the%20posterior%20cannot%20be%20computed%20%28for%20example%2C%20with%20black-box%0Asimulators%29.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%20two-stage%0AHamiltonian%20Monte%20Carlo%20algorithm%20with%20a%20surrogate%20model.%20In%20this%0Amulti-fidelity%20algorithm%2C%20the%20acceptance%20probability%20is%20computed%20in%20the%20first%0Astage%20via%20a%20standard%20HMC%20proposal%20using%20an%20inexpensive%20differentiable%20surrogate%0Amodel%2C%20and%20if%20the%20proposal%20is%20accepted%2C%20the%20posterior%20is%20evaluated%20in%20the%0Asecond%20stage%20using%20the%20high-fidelity%20%28HF%29%20numerical%20solver.%20Splitting%20the%0Astandard%20HMC%20algorithm%20into%20these%20two%20stages%20allows%20for%20approximating%20the%0Agradient%20of%20the%20posterior%20efficiently%2C%20while%20producing%20accurate%20posterior%0Asamples%20by%20using%20HF%20numerical%20solvers%20in%20the%20second%20stage.%20We%20demonstrate%20the%0Aeffectiveness%20of%20this%20algorithm%20for%20a%20range%20of%20problems%2C%20including%20linear%20and%0Anonlinear%20Bayesian%20inverse%20problems%20with%20in-silico%20data%20and%20experimental%20data.%0AThe%20proposed%20algorithm%20is%20shown%20to%20seamlessly%20integrate%20with%20various%0Alow-fidelity%20and%20HF%20models%2C%20priors%2C%20and%20datasets.%20Remarkably%2C%20our%20proposed%0Amethod%20outperforms%20the%20traditional%20HMC%20algorithm%20in%20both%20computational%20and%0Astatistical%20efficiency%20by%20several%20orders%20of%20magnitude%2C%20all%20while%20retaining%20or%0Aimproving%20the%20accuracy%20in%20computed%20posterior%20statistics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-fidelity%2520Hamiltonian%2520Monte%2520Carlo%26entry.906535625%3DDhruv%2520V.%2520Patel%2520and%2520Jonghyun%2520Lee%2520and%2520Matthew%2520W.%2520Farthing%2520and%2520Peter%2520K.%2520Kitanidis%2520and%2520Eric%2520F.%2520Darve%26entry.1292438233%3D%2520%2520Numerous%2520applications%2520in%2520biology%252C%2520statistics%252C%2520science%252C%2520and%2520engineering%250Arequire%2520generating%2520samples%2520from%2520high-dimensional%2520probability%2520distributions.%2520In%250Arecent%2520years%252C%2520the%2520Hamiltonian%2520Monte%2520Carlo%2520%2528HMC%2529%2520method%2520has%2520emerged%2520as%2520a%250Astate-of-the-art%2520Markov%2520chain%2520Monte%2520Carlo%2520technique%252C%2520exploiting%2520the%2520shape%2520of%250Asuch%2520high-dimensional%2520target%2520distributions%2520to%2520efficiently%2520generate%2520samples.%250ADespite%2520its%2520impressive%2520empirical%2520success%2520and%2520increasing%2520popularity%252C%2520its%250Awide-scale%2520adoption%2520remains%2520limited%2520due%2520to%2520the%2520high%2520computational%2520cost%2520of%250Agradient%2520calculation.%2520Moreover%252C%2520applying%2520this%2520method%2520is%2520impossible%2520when%2520the%250Agradient%2520of%2520the%2520posterior%2520cannot%2520be%2520computed%2520%2528for%2520example%252C%2520with%2520black-box%250Asimulators%2529.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520two-stage%250AHamiltonian%2520Monte%2520Carlo%2520algorithm%2520with%2520a%2520surrogate%2520model.%2520In%2520this%250Amulti-fidelity%2520algorithm%252C%2520the%2520acceptance%2520probability%2520is%2520computed%2520in%2520the%2520first%250Astage%2520via%2520a%2520standard%2520HMC%2520proposal%2520using%2520an%2520inexpensive%2520differentiable%2520surrogate%250Amodel%252C%2520and%2520if%2520the%2520proposal%2520is%2520accepted%252C%2520the%2520posterior%2520is%2520evaluated%2520in%2520the%250Asecond%2520stage%2520using%2520the%2520high-fidelity%2520%2528HF%2529%2520numerical%2520solver.%2520Splitting%2520the%250Astandard%2520HMC%2520algorithm%2520into%2520these%2520two%2520stages%2520allows%2520for%2520approximating%2520the%250Agradient%2520of%2520the%2520posterior%2520efficiently%252C%2520while%2520producing%2520accurate%2520posterior%250Asamples%2520by%2520using%2520HF%2520numerical%2520solvers%2520in%2520the%2520second%2520stage.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520this%2520algorithm%2520for%2520a%2520range%2520of%2520problems%252C%2520including%2520linear%2520and%250Anonlinear%2520Bayesian%2520inverse%2520problems%2520with%2520in-silico%2520data%2520and%2520experimental%2520data.%250AThe%2520proposed%2520algorithm%2520is%2520shown%2520to%2520seamlessly%2520integrate%2520with%2520various%250Alow-fidelity%2520and%2520HF%2520models%252C%2520priors%252C%2520and%2520datasets.%2520Remarkably%252C%2520our%2520proposed%250Amethod%2520outperforms%2520the%2520traditional%2520HMC%2520algorithm%2520in%2520both%2520computational%2520and%250Astatistical%2520efficiency%2520by%2520several%2520orders%2520of%2520magnitude%252C%2520all%2520while%2520retaining%2520or%250Aimproving%2520the%2520accuracy%2520in%2520computed%2520posterior%2520statistics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-fidelity%20Hamiltonian%20Monte%20Carlo&entry.906535625=Dhruv%20V.%20Patel%20and%20Jonghyun%20Lee%20and%20Matthew%20W.%20Farthing%20and%20Peter%20K.%20Kitanidis%20and%20Eric%20F.%20Darve&entry.1292438233=%20%20Numerous%20applications%20in%20biology%2C%20statistics%2C%20science%2C%20and%20engineering%0Arequire%20generating%20samples%20from%20high-dimensional%20probability%20distributions.%20In%0Arecent%20years%2C%20the%20Hamiltonian%20Monte%20Carlo%20%28HMC%29%20method%20has%20emerged%20as%20a%0Astate-of-the-art%20Markov%20chain%20Monte%20Carlo%20technique%2C%20exploiting%20the%20shape%20of%0Asuch%20high-dimensional%20target%20distributions%20to%20efficiently%20generate%20samples.%0ADespite%20its%20impressive%20empirical%20success%20and%20increasing%20popularity%2C%20its%0Awide-scale%20adoption%20remains%20limited%20due%20to%20the%20high%20computational%20cost%20of%0Agradient%20calculation.%20Moreover%2C%20applying%20this%20method%20is%20impossible%20when%20the%0Agradient%20of%20the%20posterior%20cannot%20be%20computed%20%28for%20example%2C%20with%20black-box%0Asimulators%29.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%20two-stage%0AHamiltonian%20Monte%20Carlo%20algorithm%20with%20a%20surrogate%20model.%20In%20this%0Amulti-fidelity%20algorithm%2C%20the%20acceptance%20probability%20is%20computed%20in%20the%20first%0Astage%20via%20a%20standard%20HMC%20proposal%20using%20an%20inexpensive%20differentiable%20surrogate%0Amodel%2C%20and%20if%20the%20proposal%20is%20accepted%2C%20the%20posterior%20is%20evaluated%20in%20the%0Asecond%20stage%20using%20the%20high-fidelity%20%28HF%29%20numerical%20solver.%20Splitting%20the%0Astandard%20HMC%20algorithm%20into%20these%20two%20stages%20allows%20for%20approximating%20the%0Agradient%20of%20the%20posterior%20efficiently%2C%20while%20producing%20accurate%20posterior%0Asamples%20by%20using%20HF%20numerical%20solvers%20in%20the%20second%20stage.%20We%20demonstrate%20the%0Aeffectiveness%20of%20this%20algorithm%20for%20a%20range%20of%20problems%2C%20including%20linear%20and%0Anonlinear%20Bayesian%20inverse%20problems%20with%20in-silico%20data%20and%20experimental%20data.%0AThe%20proposed%20algorithm%20is%20shown%20to%20seamlessly%20integrate%20with%20various%0Alow-fidelity%20and%20HF%20models%2C%20priors%2C%20and%20datasets.%20Remarkably%2C%20our%20proposed%0Amethod%20outperforms%20the%20traditional%20HMC%20algorithm%20in%20both%20computational%20and%0Astatistical%20efficiency%20by%20several%20orders%20of%20magnitude%2C%20all%20while%20retaining%20or%0Aimproving%20the%20accuracy%20in%20computed%20posterior%20statistics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05033v1&entry.124074799=Read"},
{"title": "Communication-Efficient Collaborative Perception via Information Filling\n  with Codebook", "author": "Yue Hu and Juntong Peng and Sifei Liu and Junhao Ge and Si Liu and Siheng Chen", "abstract": "  Collaborative perception empowers each agent to improve its perceptual\nability through the exchange of perceptual messages with other agents. It\ninherently results in a fundamental trade-off between perception ability and\ncommunication cost. To address this bottleneck issue, our core idea is to\noptimize the collaborative messages from two key aspects: representation and\nselection. The proposed codebook-based message representation enables the\ntransmission of integer codes, rather than high-dimensional feature maps. The\nproposed information-filling-driven message selection optimizes local messages\nto collectively fill each agent's information demand, preventing information\noverflow among multiple agents. By integrating these two designs, we propose\nCodeFilling, a novel communication-efficient collaborative perception system,\nwhich significantly advances the perception-communication trade-off and is\ninclusive to both homogeneous and heterogeneous collaboration settings. We\nevaluate CodeFilling in both a real-world dataset, DAIR-V2X, and a new\nsimulation dataset, OPV2VH+. Results show that CodeFilling outperforms previous\nSOTA Where2comm on DAIR-V2X/OPV2VH+ with 1,333/1,206 times lower communication\nvolume. Our code is available at https://github.com/PhyllisH/CodeFilling.\n", "link": "http://arxiv.org/abs/2405.04966v1", "date": "2024-05-08", "relevancy": 1.8781, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4825}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Collaborative%20Perception%20via%20Information%20Filling%0A%20%20with%20Codebook&body=Title%3A%20Communication-Efficient%20Collaborative%20Perception%20via%20Information%20Filling%0A%20%20with%20Codebook%0AAuthor%3A%20Yue%20Hu%20and%20Juntong%20Peng%20and%20Sifei%20Liu%20and%20Junhao%20Ge%20and%20Si%20Liu%20and%20Siheng%20Chen%0AAbstract%3A%20%20%20Collaborative%20perception%20empowers%20each%20agent%20to%20improve%20its%20perceptual%0Aability%20through%20the%20exchange%20of%20perceptual%20messages%20with%20other%20agents.%20It%0Ainherently%20results%20in%20a%20fundamental%20trade-off%20between%20perception%20ability%20and%0Acommunication%20cost.%20To%20address%20this%20bottleneck%20issue%2C%20our%20core%20idea%20is%20to%0Aoptimize%20the%20collaborative%20messages%20from%20two%20key%20aspects%3A%20representation%20and%0Aselection.%20The%20proposed%20codebook-based%20message%20representation%20enables%20the%0Atransmission%20of%20integer%20codes%2C%20rather%20than%20high-dimensional%20feature%20maps.%20The%0Aproposed%20information-filling-driven%20message%20selection%20optimizes%20local%20messages%0Ato%20collectively%20fill%20each%20agent%27s%20information%20demand%2C%20preventing%20information%0Aoverflow%20among%20multiple%20agents.%20By%20integrating%20these%20two%20designs%2C%20we%20propose%0ACodeFilling%2C%20a%20novel%20communication-efficient%20collaborative%20perception%20system%2C%0Awhich%20significantly%20advances%20the%20perception-communication%20trade-off%20and%20is%0Ainclusive%20to%20both%20homogeneous%20and%20heterogeneous%20collaboration%20settings.%20We%0Aevaluate%20CodeFilling%20in%20both%20a%20real-world%20dataset%2C%20DAIR-V2X%2C%20and%20a%20new%0Asimulation%20dataset%2C%20OPV2VH%2B.%20Results%20show%20that%20CodeFilling%20outperforms%20previous%0ASOTA%20Where2comm%20on%20DAIR-V2X/OPV2VH%2B%20with%201%2C333/1%2C206%20times%20lower%20communication%0Avolume.%20Our%20code%20is%20available%20at%20https%3A//github.com/PhyllisH/CodeFilling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Collaborative%2520Perception%2520via%2520Information%2520Filling%250A%2520%2520with%2520Codebook%26entry.906535625%3DYue%2520Hu%2520and%2520Juntong%2520Peng%2520and%2520Sifei%2520Liu%2520and%2520Junhao%2520Ge%2520and%2520Si%2520Liu%2520and%2520Siheng%2520Chen%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520empowers%2520each%2520agent%2520to%2520improve%2520its%2520perceptual%250Aability%2520through%2520the%2520exchange%2520of%2520perceptual%2520messages%2520with%2520other%2520agents.%2520It%250Ainherently%2520results%2520in%2520a%2520fundamental%2520trade-off%2520between%2520perception%2520ability%2520and%250Acommunication%2520cost.%2520To%2520address%2520this%2520bottleneck%2520issue%252C%2520our%2520core%2520idea%2520is%2520to%250Aoptimize%2520the%2520collaborative%2520messages%2520from%2520two%2520key%2520aspects%253A%2520representation%2520and%250Aselection.%2520The%2520proposed%2520codebook-based%2520message%2520representation%2520enables%2520the%250Atransmission%2520of%2520integer%2520codes%252C%2520rather%2520than%2520high-dimensional%2520feature%2520maps.%2520The%250Aproposed%2520information-filling-driven%2520message%2520selection%2520optimizes%2520local%2520messages%250Ato%2520collectively%2520fill%2520each%2520agent%2527s%2520information%2520demand%252C%2520preventing%2520information%250Aoverflow%2520among%2520multiple%2520agents.%2520By%2520integrating%2520these%2520two%2520designs%252C%2520we%2520propose%250ACodeFilling%252C%2520a%2520novel%2520communication-efficient%2520collaborative%2520perception%2520system%252C%250Awhich%2520significantly%2520advances%2520the%2520perception-communication%2520trade-off%2520and%2520is%250Ainclusive%2520to%2520both%2520homogeneous%2520and%2520heterogeneous%2520collaboration%2520settings.%2520We%250Aevaluate%2520CodeFilling%2520in%2520both%2520a%2520real-world%2520dataset%252C%2520DAIR-V2X%252C%2520and%2520a%2520new%250Asimulation%2520dataset%252C%2520OPV2VH%252B.%2520Results%2520show%2520that%2520CodeFilling%2520outperforms%2520previous%250ASOTA%2520Where2comm%2520on%2520DAIR-V2X/OPV2VH%252B%2520with%25201%252C333/1%252C206%2520times%2520lower%2520communication%250Avolume.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/PhyllisH/CodeFilling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Collaborative%20Perception%20via%20Information%20Filling%0A%20%20with%20Codebook&entry.906535625=Yue%20Hu%20and%20Juntong%20Peng%20and%20Sifei%20Liu%20and%20Junhao%20Ge%20and%20Si%20Liu%20and%20Siheng%20Chen&entry.1292438233=%20%20Collaborative%20perception%20empowers%20each%20agent%20to%20improve%20its%20perceptual%0Aability%20through%20the%20exchange%20of%20perceptual%20messages%20with%20other%20agents.%20It%0Ainherently%20results%20in%20a%20fundamental%20trade-off%20between%20perception%20ability%20and%0Acommunication%20cost.%20To%20address%20this%20bottleneck%20issue%2C%20our%20core%20idea%20is%20to%0Aoptimize%20the%20collaborative%20messages%20from%20two%20key%20aspects%3A%20representation%20and%0Aselection.%20The%20proposed%20codebook-based%20message%20representation%20enables%20the%0Atransmission%20of%20integer%20codes%2C%20rather%20than%20high-dimensional%20feature%20maps.%20The%0Aproposed%20information-filling-driven%20message%20selection%20optimizes%20local%20messages%0Ato%20collectively%20fill%20each%20agent%27s%20information%20demand%2C%20preventing%20information%0Aoverflow%20among%20multiple%20agents.%20By%20integrating%20these%20two%20designs%2C%20we%20propose%0ACodeFilling%2C%20a%20novel%20communication-efficient%20collaborative%20perception%20system%2C%0Awhich%20significantly%20advances%20the%20perception-communication%20trade-off%20and%20is%0Ainclusive%20to%20both%20homogeneous%20and%20heterogeneous%20collaboration%20settings.%20We%0Aevaluate%20CodeFilling%20in%20both%20a%20real-world%20dataset%2C%20DAIR-V2X%2C%20and%20a%20new%0Asimulation%20dataset%2C%20OPV2VH%2B.%20Results%20show%20that%20CodeFilling%20outperforms%20previous%0ASOTA%20Where2comm%20on%20DAIR-V2X/OPV2VH%2B%20with%201%2C333/1%2C206%20times%20lower%20communication%0Avolume.%20Our%20code%20is%20available%20at%20https%3A//github.com/PhyllisH/CodeFilling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04966v1&entry.124074799=Read"},
{"title": "Anomaly Detection in Certificate Transparency Logs", "author": "Richard Ostert\u00e1g and Martin Stanek", "abstract": "  We propose an anomaly detection technique for X.509 certificates utilizing\nIsolation Forest. This method can be beneficial when compliance testing with\nX.509 linters proves unsatisfactory, and we seek to identify anomalies beyond\nstandards compliance. The technique is validated on a sample of certificates\nfrom Certificate Transparency logs.\n", "link": "http://arxiv.org/abs/2405.05206v1", "date": "2024-05-08", "relevancy": 1.8742, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4043}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3627}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomaly%20Detection%20in%20Certificate%20Transparency%20Logs&body=Title%3A%20Anomaly%20Detection%20in%20Certificate%20Transparency%20Logs%0AAuthor%3A%20Richard%20Ostert%C3%A1g%20and%20Martin%20Stanek%0AAbstract%3A%20%20%20We%20propose%20an%20anomaly%20detection%20technique%20for%20X.509%20certificates%20utilizing%0AIsolation%20Forest.%20This%20method%20can%20be%20beneficial%20when%20compliance%20testing%20with%0AX.509%20linters%20proves%20unsatisfactory%2C%20and%20we%20seek%20to%20identify%20anomalies%20beyond%0Astandards%20compliance.%20The%20technique%20is%20validated%20on%20a%20sample%20of%20certificates%0Afrom%20Certificate%20Transparency%20logs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomaly%2520Detection%2520in%2520Certificate%2520Transparency%2520Logs%26entry.906535625%3DRichard%2520Ostert%25C3%25A1g%2520and%2520Martin%2520Stanek%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520anomaly%2520detection%2520technique%2520for%2520X.509%2520certificates%2520utilizing%250AIsolation%2520Forest.%2520This%2520method%2520can%2520be%2520beneficial%2520when%2520compliance%2520testing%2520with%250AX.509%2520linters%2520proves%2520unsatisfactory%252C%2520and%2520we%2520seek%2520to%2520identify%2520anomalies%2520beyond%250Astandards%2520compliance.%2520The%2520technique%2520is%2520validated%2520on%2520a%2520sample%2520of%2520certificates%250Afrom%2520Certificate%2520Transparency%2520logs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly%20Detection%20in%20Certificate%20Transparency%20Logs&entry.906535625=Richard%20Ostert%C3%A1g%20and%20Martin%20Stanek&entry.1292438233=%20%20We%20propose%20an%20anomaly%20detection%20technique%20for%20X.509%20certificates%20utilizing%0AIsolation%20Forest.%20This%20method%20can%20be%20beneficial%20when%20compliance%20testing%20with%0AX.509%20linters%20proves%20unsatisfactory%2C%20and%20we%20seek%20to%20identify%20anomalies%20beyond%0Astandards%20compliance.%20The%20technique%20is%20validated%20on%20a%20sample%20of%20certificates%0Afrom%20Certificate%20Transparency%20logs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05206v1&entry.124074799=Read"},
{"title": "Scalable Decentralized Algorithms for Online Personalized Mean\n  Estimation", "author": "Franco Galante and Giovanni Neglia and Emilio Leonardi", "abstract": "  In numerous settings, agents lack sufficient data to directly learn a model.\nCollaborating with other agents may help, but it introduces a bias-variance\ntrade-off, when local data distributions differ. A key challenge is for each\nagent to identify clients with similar distributions while learning the model,\na problem that remains largely unresolved. This study focuses on a simplified\nversion of the overarching problem, where each agent collects samples from a\nreal-valued distribution over time to estimate its mean. Existing algorithms\nface impractical space and time complexities (quadratic in the number of agents\nA). To address scalability challenges, we propose a framework where agents\nself-organize into a graph, allowing each agent to communicate with only a\nselected number of peers r. We introduce two collaborative mean estimation\nalgorithms: one draws inspiration from belief propagation, while the other\nemploys a consensus-based approach, with complexity of O( r |A| log |A|) and\nO(r |A|), respectively. We establish conditions under which both algorithms\nyield asymptotically optimal estimates and offer a theoretical characterization\nof their performance.\n", "link": "http://arxiv.org/abs/2402.12812v3", "date": "2024-05-08", "relevancy": 1.868, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4818}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4661}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Decentralized%20Algorithms%20for%20Online%20Personalized%20Mean%0A%20%20Estimation&body=Title%3A%20Scalable%20Decentralized%20Algorithms%20for%20Online%20Personalized%20Mean%0A%20%20Estimation%0AAuthor%3A%20Franco%20Galante%20and%20Giovanni%20Neglia%20and%20Emilio%20Leonardi%0AAbstract%3A%20%20%20In%20numerous%20settings%2C%20agents%20lack%20sufficient%20data%20to%20directly%20learn%20a%20model.%0ACollaborating%20with%20other%20agents%20may%20help%2C%20but%20it%20introduces%20a%20bias-variance%0Atrade-off%2C%20when%20local%20data%20distributions%20differ.%20A%20key%20challenge%20is%20for%20each%0Aagent%20to%20identify%20clients%20with%20similar%20distributions%20while%20learning%20the%20model%2C%0Aa%20problem%20that%20remains%20largely%20unresolved.%20This%20study%20focuses%20on%20a%20simplified%0Aversion%20of%20the%20overarching%20problem%2C%20where%20each%20agent%20collects%20samples%20from%20a%0Areal-valued%20distribution%20over%20time%20to%20estimate%20its%20mean.%20Existing%20algorithms%0Aface%20impractical%20space%20and%20time%20complexities%20%28quadratic%20in%20the%20number%20of%20agents%0AA%29.%20To%20address%20scalability%20challenges%2C%20we%20propose%20a%20framework%20where%20agents%0Aself-organize%20into%20a%20graph%2C%20allowing%20each%20agent%20to%20communicate%20with%20only%20a%0Aselected%20number%20of%20peers%20r.%20We%20introduce%20two%20collaborative%20mean%20estimation%0Aalgorithms%3A%20one%20draws%20inspiration%20from%20belief%20propagation%2C%20while%20the%20other%0Aemploys%20a%20consensus-based%20approach%2C%20with%20complexity%20of%20O%28%20r%20%7CA%7C%20log%20%7CA%7C%29%20and%0AO%28r%20%7CA%7C%29%2C%20respectively.%20We%20establish%20conditions%20under%20which%20both%20algorithms%0Ayield%20asymptotically%20optimal%20estimates%20and%20offer%20a%20theoretical%20characterization%0Aof%20their%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12812v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Decentralized%2520Algorithms%2520for%2520Online%2520Personalized%2520Mean%250A%2520%2520Estimation%26entry.906535625%3DFranco%2520Galante%2520and%2520Giovanni%2520Neglia%2520and%2520Emilio%2520Leonardi%26entry.1292438233%3D%2520%2520In%2520numerous%2520settings%252C%2520agents%2520lack%2520sufficient%2520data%2520to%2520directly%2520learn%2520a%2520model.%250ACollaborating%2520with%2520other%2520agents%2520may%2520help%252C%2520but%2520it%2520introduces%2520a%2520bias-variance%250Atrade-off%252C%2520when%2520local%2520data%2520distributions%2520differ.%2520A%2520key%2520challenge%2520is%2520for%2520each%250Aagent%2520to%2520identify%2520clients%2520with%2520similar%2520distributions%2520while%2520learning%2520the%2520model%252C%250Aa%2520problem%2520that%2520remains%2520largely%2520unresolved.%2520This%2520study%2520focuses%2520on%2520a%2520simplified%250Aversion%2520of%2520the%2520overarching%2520problem%252C%2520where%2520each%2520agent%2520collects%2520samples%2520from%2520a%250Areal-valued%2520distribution%2520over%2520time%2520to%2520estimate%2520its%2520mean.%2520Existing%2520algorithms%250Aface%2520impractical%2520space%2520and%2520time%2520complexities%2520%2528quadratic%2520in%2520the%2520number%2520of%2520agents%250AA%2529.%2520To%2520address%2520scalability%2520challenges%252C%2520we%2520propose%2520a%2520framework%2520where%2520agents%250Aself-organize%2520into%2520a%2520graph%252C%2520allowing%2520each%2520agent%2520to%2520communicate%2520with%2520only%2520a%250Aselected%2520number%2520of%2520peers%2520r.%2520We%2520introduce%2520two%2520collaborative%2520mean%2520estimation%250Aalgorithms%253A%2520one%2520draws%2520inspiration%2520from%2520belief%2520propagation%252C%2520while%2520the%2520other%250Aemploys%2520a%2520consensus-based%2520approach%252C%2520with%2520complexity%2520of%2520O%2528%2520r%2520%257CA%257C%2520log%2520%257CA%257C%2529%2520and%250AO%2528r%2520%257CA%257C%2529%252C%2520respectively.%2520We%2520establish%2520conditions%2520under%2520which%2520both%2520algorithms%250Ayield%2520asymptotically%2520optimal%2520estimates%2520and%2520offer%2520a%2520theoretical%2520characterization%250Aof%2520their%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12812v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Decentralized%20Algorithms%20for%20Online%20Personalized%20Mean%0A%20%20Estimation&entry.906535625=Franco%20Galante%20and%20Giovanni%20Neglia%20and%20Emilio%20Leonardi&entry.1292438233=%20%20In%20numerous%20settings%2C%20agents%20lack%20sufficient%20data%20to%20directly%20learn%20a%20model.%0ACollaborating%20with%20other%20agents%20may%20help%2C%20but%20it%20introduces%20a%20bias-variance%0Atrade-off%2C%20when%20local%20data%20distributions%20differ.%20A%20key%20challenge%20is%20for%20each%0Aagent%20to%20identify%20clients%20with%20similar%20distributions%20while%20learning%20the%20model%2C%0Aa%20problem%20that%20remains%20largely%20unresolved.%20This%20study%20focuses%20on%20a%20simplified%0Aversion%20of%20the%20overarching%20problem%2C%20where%20each%20agent%20collects%20samples%20from%20a%0Areal-valued%20distribution%20over%20time%20to%20estimate%20its%20mean.%20Existing%20algorithms%0Aface%20impractical%20space%20and%20time%20complexities%20%28quadratic%20in%20the%20number%20of%20agents%0AA%29.%20To%20address%20scalability%20challenges%2C%20we%20propose%20a%20framework%20where%20agents%0Aself-organize%20into%20a%20graph%2C%20allowing%20each%20agent%20to%20communicate%20with%20only%20a%0Aselected%20number%20of%20peers%20r.%20We%20introduce%20two%20collaborative%20mean%20estimation%0Aalgorithms%3A%20one%20draws%20inspiration%20from%20belief%20propagation%2C%20while%20the%20other%0Aemploys%20a%20consensus-based%20approach%2C%20with%20complexity%20of%20O%28%20r%20%7CA%7C%20log%20%7CA%7C%29%20and%0AO%28r%20%7CA%7C%29%2C%20respectively.%20We%20establish%20conditions%20under%20which%20both%20algorithms%0Ayield%20asymptotically%20optimal%20estimates%20and%20offer%20a%20theoretical%20characterization%0Aof%20their%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12812v3&entry.124074799=Read"},
{"title": "Concrete Dense Network for Long-Sequence Time Series Clustering", "author": "Redemptor Jr Laceda Taloma and Patrizio Pisani and Danilo Comminiello", "abstract": "  Time series clustering is fundamental in data analysis for discovering\ntemporal patterns. Despite recent advancements, learning cluster-friendly\nrepresentations is still challenging, particularly with long and complex time\nseries. Deep temporal clustering methods have been trying to integrate the\ncanonical k-means into end-to-end training of neural networks but fall back on\nsurrogate losses due to the non-differentiability of the hard cluster\nassignment, yielding sub-optimal solutions. In addition, the autoregressive\nstrategy used in the state-of-the-art RNNs is subject to error accumulation and\nslow training, while recent research findings have revealed that Transformers\nare less effective due to time points lacking semantic meaning, to the\npermutation invariance of attention that discards the chronological order and\nhigh computation cost. In light of these observations, we present LoSTer which\nis a novel dense autoencoder architecture for the long-sequence time series\nclustering problem (LSTC) capable of optimizing the k-means objective via the\nGumbel-softmax reparameterization trick and designed specifically for accurate\nand fast clustering of long time series. Extensive experiments on numerous\nbenchmark datasets and two real-world applications prove the effectiveness of\nLoSTer over state-of-the-art RNNs and Transformer-based deep clustering\nmethods.\n", "link": "http://arxiv.org/abs/2405.05015v1", "date": "2024-05-08", "relevancy": 1.8646, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4741}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4688}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concrete%20Dense%20Network%20for%20Long-Sequence%20Time%20Series%20Clustering&body=Title%3A%20Concrete%20Dense%20Network%20for%20Long-Sequence%20Time%20Series%20Clustering%0AAuthor%3A%20Redemptor%20Jr%20Laceda%20Taloma%20and%20Patrizio%20Pisani%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20Time%20series%20clustering%20is%20fundamental%20in%20data%20analysis%20for%20discovering%0Atemporal%20patterns.%20Despite%20recent%20advancements%2C%20learning%20cluster-friendly%0Arepresentations%20is%20still%20challenging%2C%20particularly%20with%20long%20and%20complex%20time%0Aseries.%20Deep%20temporal%20clustering%20methods%20have%20been%20trying%20to%20integrate%20the%0Acanonical%20k-means%20into%20end-to-end%20training%20of%20neural%20networks%20but%20fall%20back%20on%0Asurrogate%20losses%20due%20to%20the%20non-differentiability%20of%20the%20hard%20cluster%0Aassignment%2C%20yielding%20sub-optimal%20solutions.%20In%20addition%2C%20the%20autoregressive%0Astrategy%20used%20in%20the%20state-of-the-art%20RNNs%20is%20subject%20to%20error%20accumulation%20and%0Aslow%20training%2C%20while%20recent%20research%20findings%20have%20revealed%20that%20Transformers%0Aare%20less%20effective%20due%20to%20time%20points%20lacking%20semantic%20meaning%2C%20to%20the%0Apermutation%20invariance%20of%20attention%20that%20discards%20the%20chronological%20order%20and%0Ahigh%20computation%20cost.%20In%20light%20of%20these%20observations%2C%20we%20present%20LoSTer%20which%0Ais%20a%20novel%20dense%20autoencoder%20architecture%20for%20the%20long-sequence%20time%20series%0Aclustering%20problem%20%28LSTC%29%20capable%20of%20optimizing%20the%20k-means%20objective%20via%20the%0AGumbel-softmax%20reparameterization%20trick%20and%20designed%20specifically%20for%20accurate%0Aand%20fast%20clustering%20of%20long%20time%20series.%20Extensive%20experiments%20on%20numerous%0Abenchmark%20datasets%20and%20two%20real-world%20applications%20prove%20the%20effectiveness%20of%0ALoSTer%20over%20state-of-the-art%20RNNs%20and%20Transformer-based%20deep%20clustering%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcrete%2520Dense%2520Network%2520for%2520Long-Sequence%2520Time%2520Series%2520Clustering%26entry.906535625%3DRedemptor%2520Jr%2520Laceda%2520Taloma%2520and%2520Patrizio%2520Pisani%2520and%2520Danilo%2520Comminiello%26entry.1292438233%3D%2520%2520Time%2520series%2520clustering%2520is%2520fundamental%2520in%2520data%2520analysis%2520for%2520discovering%250Atemporal%2520patterns.%2520Despite%2520recent%2520advancements%252C%2520learning%2520cluster-friendly%250Arepresentations%2520is%2520still%2520challenging%252C%2520particularly%2520with%2520long%2520and%2520complex%2520time%250Aseries.%2520Deep%2520temporal%2520clustering%2520methods%2520have%2520been%2520trying%2520to%2520integrate%2520the%250Acanonical%2520k-means%2520into%2520end-to-end%2520training%2520of%2520neural%2520networks%2520but%2520fall%2520back%2520on%250Asurrogate%2520losses%2520due%2520to%2520the%2520non-differentiability%2520of%2520the%2520hard%2520cluster%250Aassignment%252C%2520yielding%2520sub-optimal%2520solutions.%2520In%2520addition%252C%2520the%2520autoregressive%250Astrategy%2520used%2520in%2520the%2520state-of-the-art%2520RNNs%2520is%2520subject%2520to%2520error%2520accumulation%2520and%250Aslow%2520training%252C%2520while%2520recent%2520research%2520findings%2520have%2520revealed%2520that%2520Transformers%250Aare%2520less%2520effective%2520due%2520to%2520time%2520points%2520lacking%2520semantic%2520meaning%252C%2520to%2520the%250Apermutation%2520invariance%2520of%2520attention%2520that%2520discards%2520the%2520chronological%2520order%2520and%250Ahigh%2520computation%2520cost.%2520In%2520light%2520of%2520these%2520observations%252C%2520we%2520present%2520LoSTer%2520which%250Ais%2520a%2520novel%2520dense%2520autoencoder%2520architecture%2520for%2520the%2520long-sequence%2520time%2520series%250Aclustering%2520problem%2520%2528LSTC%2529%2520capable%2520of%2520optimizing%2520the%2520k-means%2520objective%2520via%2520the%250AGumbel-softmax%2520reparameterization%2520trick%2520and%2520designed%2520specifically%2520for%2520accurate%250Aand%2520fast%2520clustering%2520of%2520long%2520time%2520series.%2520Extensive%2520experiments%2520on%2520numerous%250Abenchmark%2520datasets%2520and%2520two%2520real-world%2520applications%2520prove%2520the%2520effectiveness%2520of%250ALoSTer%2520over%2520state-of-the-art%2520RNNs%2520and%2520Transformer-based%2520deep%2520clustering%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concrete%20Dense%20Network%20for%20Long-Sequence%20Time%20Series%20Clustering&entry.906535625=Redemptor%20Jr%20Laceda%20Taloma%20and%20Patrizio%20Pisani%20and%20Danilo%20Comminiello&entry.1292438233=%20%20Time%20series%20clustering%20is%20fundamental%20in%20data%20analysis%20for%20discovering%0Atemporal%20patterns.%20Despite%20recent%20advancements%2C%20learning%20cluster-friendly%0Arepresentations%20is%20still%20challenging%2C%20particularly%20with%20long%20and%20complex%20time%0Aseries.%20Deep%20temporal%20clustering%20methods%20have%20been%20trying%20to%20integrate%20the%0Acanonical%20k-means%20into%20end-to-end%20training%20of%20neural%20networks%20but%20fall%20back%20on%0Asurrogate%20losses%20due%20to%20the%20non-differentiability%20of%20the%20hard%20cluster%0Aassignment%2C%20yielding%20sub-optimal%20solutions.%20In%20addition%2C%20the%20autoregressive%0Astrategy%20used%20in%20the%20state-of-the-art%20RNNs%20is%20subject%20to%20error%20accumulation%20and%0Aslow%20training%2C%20while%20recent%20research%20findings%20have%20revealed%20that%20Transformers%0Aare%20less%20effective%20due%20to%20time%20points%20lacking%20semantic%20meaning%2C%20to%20the%0Apermutation%20invariance%20of%20attention%20that%20discards%20the%20chronological%20order%20and%0Ahigh%20computation%20cost.%20In%20light%20of%20these%20observations%2C%20we%20present%20LoSTer%20which%0Ais%20a%20novel%20dense%20autoencoder%20architecture%20for%20the%20long-sequence%20time%20series%0Aclustering%20problem%20%28LSTC%29%20capable%20of%20optimizing%20the%20k-means%20objective%20via%20the%0AGumbel-softmax%20reparameterization%20trick%20and%20designed%20specifically%20for%20accurate%0Aand%20fast%20clustering%20of%20long%20time%20series.%20Extensive%20experiments%20on%20numerous%0Abenchmark%20datasets%20and%20two%20real-world%20applications%20prove%20the%20effectiveness%20of%0ALoSTer%20over%20state-of-the-art%20RNNs%20and%20Transformer-based%20deep%20clustering%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05015v1&entry.124074799=Read"},
{"title": "Cultural Alignment in Large Language Models: An Explanatory Analysis\n  Based on Hofstede's Cultural Dimensions", "author": "Reem I. Masoud and Ziquan Liu and Martin Ferianc and Philip Treleaven and Miguel Rodrigues", "abstract": "  The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals and\nsocieties with diverse cultural backgrounds. While the discourse has focused\nmainly on political and social biases, our research proposes a Cultural\nAlignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's\ncultural dimension framework, which offers an explanatory cross-cultural\ncomparison through the latent variable analysis. We apply our approach to\nquantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the\ncultural dimensions of regions like the United States, China, and Arab\ncountries, using different prompting styles and exploring the effects of\nlanguage-specific fine-tuning on the models' behavioural tendencies and\ncultural values. Our results quantify the cultural alignment of LLMs and reveal\nthe difference between LLMs in explanatory cultural dimensions. Our study\ndemonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows\na unique capability to adapt to cultural nuances, particularly in Chinese\nsettings. However, it faces challenges with American and Arab cultures. The\nresearch also highlights that fine-tuning LLama 2 models with different\nlanguages changes their responses to cultural questions, emphasizing the need\nfor culturally diverse development in AI for worldwide acceptance and ethical\nuse. For more details or to contribute to this research, visit our GitHub page\nhttps://github.com/reemim/Hofstedes_CAT/\n", "link": "http://arxiv.org/abs/2309.12342v2", "date": "2024-05-08", "relevancy": 1.8371, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4466}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cultural%20Alignment%20in%20Large%20Language%20Models%3A%20An%20Explanatory%20Analysis%0A%20%20Based%20on%20Hofstede%27s%20Cultural%20Dimensions&body=Title%3A%20Cultural%20Alignment%20in%20Large%20Language%20Models%3A%20An%20Explanatory%20Analysis%0A%20%20Based%20on%20Hofstede%27s%20Cultural%20Dimensions%0AAuthor%3A%20Reem%20I.%20Masoud%20and%20Ziquan%20Liu%20and%20Martin%20Ferianc%20and%20Philip%20Treleaven%20and%20Miguel%20Rodrigues%0AAbstract%3A%20%20%20The%20deployment%20of%20large%20language%20models%20%28LLMs%29%20raises%20concerns%20regarding%0Atheir%20cultural%20misalignment%20and%20potential%20ramifications%20on%20individuals%20and%0Asocieties%20with%20diverse%20cultural%20backgrounds.%20While%20the%20discourse%20has%20focused%0Amainly%20on%20political%20and%20social%20biases%2C%20our%20research%20proposes%20a%20Cultural%0AAlignment%20Test%20%28Hoftede%27s%20CAT%29%20to%20quantify%20cultural%20alignment%20using%20Hofstede%27s%0Acultural%20dimension%20framework%2C%20which%20offers%20an%20explanatory%20cross-cultural%0Acomparison%20through%20the%20latent%20variable%20analysis.%20We%20apply%20our%20approach%20to%0Aquantitatively%20evaluate%20LLMs%2C%20namely%20Llama%202%2C%20GPT-3.5%2C%20and%20GPT-4%2C%20against%20the%0Acultural%20dimensions%20of%20regions%20like%20the%20United%20States%2C%20China%2C%20and%20Arab%0Acountries%2C%20using%20different%20prompting%20styles%20and%20exploring%20the%20effects%20of%0Alanguage-specific%20fine-tuning%20on%20the%20models%27%20behavioural%20tendencies%20and%0Acultural%20values.%20Our%20results%20quantify%20the%20cultural%20alignment%20of%20LLMs%20and%20reveal%0Athe%20difference%20between%20LLMs%20in%20explanatory%20cultural%20dimensions.%20Our%20study%0Ademonstrates%20that%20while%20all%20LLMs%20struggle%20to%20grasp%20cultural%20values%2C%20GPT-4%20shows%0Aa%20unique%20capability%20to%20adapt%20to%20cultural%20nuances%2C%20particularly%20in%20Chinese%0Asettings.%20However%2C%20it%20faces%20challenges%20with%20American%20and%20Arab%20cultures.%20The%0Aresearch%20also%20highlights%20that%20fine-tuning%20LLama%202%20models%20with%20different%0Alanguages%20changes%20their%20responses%20to%20cultural%20questions%2C%20emphasizing%20the%20need%0Afor%20culturally%20diverse%20development%20in%20AI%20for%20worldwide%20acceptance%20and%20ethical%0Ause.%20For%20more%20details%20or%20to%20contribute%20to%20this%20research%2C%20visit%20our%20GitHub%20page%0Ahttps%3A//github.com/reemim/Hofstedes_CAT/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCultural%2520Alignment%2520in%2520Large%2520Language%2520Models%253A%2520An%2520Explanatory%2520Analysis%250A%2520%2520Based%2520on%2520Hofstede%2527s%2520Cultural%2520Dimensions%26entry.906535625%3DReem%2520I.%2520Masoud%2520and%2520Ziquan%2520Liu%2520and%2520Martin%2520Ferianc%2520and%2520Philip%2520Treleaven%2520and%2520Miguel%2520Rodrigues%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520raises%2520concerns%2520regarding%250Atheir%2520cultural%2520misalignment%2520and%2520potential%2520ramifications%2520on%2520individuals%2520and%250Asocieties%2520with%2520diverse%2520cultural%2520backgrounds.%2520While%2520the%2520discourse%2520has%2520focused%250Amainly%2520on%2520political%2520and%2520social%2520biases%252C%2520our%2520research%2520proposes%2520a%2520Cultural%250AAlignment%2520Test%2520%2528Hoftede%2527s%2520CAT%2529%2520to%2520quantify%2520cultural%2520alignment%2520using%2520Hofstede%2527s%250Acultural%2520dimension%2520framework%252C%2520which%2520offers%2520an%2520explanatory%2520cross-cultural%250Acomparison%2520through%2520the%2520latent%2520variable%2520analysis.%2520We%2520apply%2520our%2520approach%2520to%250Aquantitatively%2520evaluate%2520LLMs%252C%2520namely%2520Llama%25202%252C%2520GPT-3.5%252C%2520and%2520GPT-4%252C%2520against%2520the%250Acultural%2520dimensions%2520of%2520regions%2520like%2520the%2520United%2520States%252C%2520China%252C%2520and%2520Arab%250Acountries%252C%2520using%2520different%2520prompting%2520styles%2520and%2520exploring%2520the%2520effects%2520of%250Alanguage-specific%2520fine-tuning%2520on%2520the%2520models%2527%2520behavioural%2520tendencies%2520and%250Acultural%2520values.%2520Our%2520results%2520quantify%2520the%2520cultural%2520alignment%2520of%2520LLMs%2520and%2520reveal%250Athe%2520difference%2520between%2520LLMs%2520in%2520explanatory%2520cultural%2520dimensions.%2520Our%2520study%250Ademonstrates%2520that%2520while%2520all%2520LLMs%2520struggle%2520to%2520grasp%2520cultural%2520values%252C%2520GPT-4%2520shows%250Aa%2520unique%2520capability%2520to%2520adapt%2520to%2520cultural%2520nuances%252C%2520particularly%2520in%2520Chinese%250Asettings.%2520However%252C%2520it%2520faces%2520challenges%2520with%2520American%2520and%2520Arab%2520cultures.%2520The%250Aresearch%2520also%2520highlights%2520that%2520fine-tuning%2520LLama%25202%2520models%2520with%2520different%250Alanguages%2520changes%2520their%2520responses%2520to%2520cultural%2520questions%252C%2520emphasizing%2520the%2520need%250Afor%2520culturally%2520diverse%2520development%2520in%2520AI%2520for%2520worldwide%2520acceptance%2520and%2520ethical%250Ause.%2520For%2520more%2520details%2520or%2520to%2520contribute%2520to%2520this%2520research%252C%2520visit%2520our%2520GitHub%2520page%250Ahttps%253A//github.com/reemim/Hofstedes_CAT/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cultural%20Alignment%20in%20Large%20Language%20Models%3A%20An%20Explanatory%20Analysis%0A%20%20Based%20on%20Hofstede%27s%20Cultural%20Dimensions&entry.906535625=Reem%20I.%20Masoud%20and%20Ziquan%20Liu%20and%20Martin%20Ferianc%20and%20Philip%20Treleaven%20and%20Miguel%20Rodrigues&entry.1292438233=%20%20The%20deployment%20of%20large%20language%20models%20%28LLMs%29%20raises%20concerns%20regarding%0Atheir%20cultural%20misalignment%20and%20potential%20ramifications%20on%20individuals%20and%0Asocieties%20with%20diverse%20cultural%20backgrounds.%20While%20the%20discourse%20has%20focused%0Amainly%20on%20political%20and%20social%20biases%2C%20our%20research%20proposes%20a%20Cultural%0AAlignment%20Test%20%28Hoftede%27s%20CAT%29%20to%20quantify%20cultural%20alignment%20using%20Hofstede%27s%0Acultural%20dimension%20framework%2C%20which%20offers%20an%20explanatory%20cross-cultural%0Acomparison%20through%20the%20latent%20variable%20analysis.%20We%20apply%20our%20approach%20to%0Aquantitatively%20evaluate%20LLMs%2C%20namely%20Llama%202%2C%20GPT-3.5%2C%20and%20GPT-4%2C%20against%20the%0Acultural%20dimensions%20of%20regions%20like%20the%20United%20States%2C%20China%2C%20and%20Arab%0Acountries%2C%20using%20different%20prompting%20styles%20and%20exploring%20the%20effects%20of%0Alanguage-specific%20fine-tuning%20on%20the%20models%27%20behavioural%20tendencies%20and%0Acultural%20values.%20Our%20results%20quantify%20the%20cultural%20alignment%20of%20LLMs%20and%20reveal%0Athe%20difference%20between%20LLMs%20in%20explanatory%20cultural%20dimensions.%20Our%20study%0Ademonstrates%20that%20while%20all%20LLMs%20struggle%20to%20grasp%20cultural%20values%2C%20GPT-4%20shows%0Aa%20unique%20capability%20to%20adapt%20to%20cultural%20nuances%2C%20particularly%20in%20Chinese%0Asettings.%20However%2C%20it%20faces%20challenges%20with%20American%20and%20Arab%20cultures.%20The%0Aresearch%20also%20highlights%20that%20fine-tuning%20LLama%202%20models%20with%20different%0Alanguages%20changes%20their%20responses%20to%20cultural%20questions%2C%20emphasizing%20the%20need%0Afor%20culturally%20diverse%20development%20in%20AI%20for%20worldwide%20acceptance%20and%20ethical%0Ause.%20For%20more%20details%20or%20to%20contribute%20to%20this%20research%2C%20visit%20our%20GitHub%20page%0Ahttps%3A//github.com/reemim/Hofstedes_CAT/%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12342v2&entry.124074799=Read"},
{"title": "Deep learning-based variational autoencoder for classification of\n  quantum and classical states of light", "author": "Mahesh Bhupati and Abhishek Mall and Anshuman Kumar and Pankaj K. Jha", "abstract": "  Advancements in optical quantum technologies have been enabled by the\ngeneration, manipulation, and characterization of light, with identification\nbased on its photon statistics. However, characterizing light and its sources\nthrough single photon measurements often requires efficient detectors and\nlonger measurement times to obtain high-quality photon statistics. Here we\nintroduce a deep learning-based variational autoencoder (VAE) method for\nclassifying single photon added coherent state (SPACS), single photon added\nthermal state (SPACS), mixed states between coherent/SPACS and thermal/SPATS of\nlight. Our semisupervised learning-based VAE efficiently maps the photon\nstatistics features of light to a lower dimension, enabling quasi-instantaneous\nclassification with low average photon counts. The proposed VAE method is\nrobust and maintains classification accuracy in the presence of losses inherent\nin an experiment, such as finite collection efficiency, non-unity quantum\nefficiency, finite number of detectors, etc. Additionally, leveraging the\ntransfer learning capabilities of VAE enables successful classification of data\nof any quality using a single trained model. We envision that such a deep\nlearning methodology will enable better classification of quantum light and\nlight sources even in the presence of poor detection quality.\n", "link": "http://arxiv.org/abs/2405.05243v1", "date": "2024-05-08", "relevancy": 1.8286, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.469}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4565}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning-based%20variational%20autoencoder%20for%20classification%20of%0A%20%20quantum%20and%20classical%20states%20of%20light&body=Title%3A%20Deep%20learning-based%20variational%20autoencoder%20for%20classification%20of%0A%20%20quantum%20and%20classical%20states%20of%20light%0AAuthor%3A%20Mahesh%20Bhupati%20and%20Abhishek%20Mall%20and%20Anshuman%20Kumar%20and%20Pankaj%20K.%20Jha%0AAbstract%3A%20%20%20Advancements%20in%20optical%20quantum%20technologies%20have%20been%20enabled%20by%20the%0Ageneration%2C%20manipulation%2C%20and%20characterization%20of%20light%2C%20with%20identification%0Abased%20on%20its%20photon%20statistics.%20However%2C%20characterizing%20light%20and%20its%20sources%0Athrough%20single%20photon%20measurements%20often%20requires%20efficient%20detectors%20and%0Alonger%20measurement%20times%20to%20obtain%20high-quality%20photon%20statistics.%20Here%20we%0Aintroduce%20a%20deep%20learning-based%20variational%20autoencoder%20%28VAE%29%20method%20for%0Aclassifying%20single%20photon%20added%20coherent%20state%20%28SPACS%29%2C%20single%20photon%20added%0Athermal%20state%20%28SPACS%29%2C%20mixed%20states%20between%20coherent/SPACS%20and%20thermal/SPATS%20of%0Alight.%20Our%20semisupervised%20learning-based%20VAE%20efficiently%20maps%20the%20photon%0Astatistics%20features%20of%20light%20to%20a%20lower%20dimension%2C%20enabling%20quasi-instantaneous%0Aclassification%20with%20low%20average%20photon%20counts.%20The%20proposed%20VAE%20method%20is%0Arobust%20and%20maintains%20classification%20accuracy%20in%20the%20presence%20of%20losses%20inherent%0Ain%20an%20experiment%2C%20such%20as%20finite%20collection%20efficiency%2C%20non-unity%20quantum%0Aefficiency%2C%20finite%20number%20of%20detectors%2C%20etc.%20Additionally%2C%20leveraging%20the%0Atransfer%20learning%20capabilities%20of%20VAE%20enables%20successful%20classification%20of%20data%0Aof%20any%20quality%20using%20a%20single%20trained%20model.%20We%20envision%20that%20such%20a%20deep%0Alearning%20methodology%20will%20enable%20better%20classification%20of%20quantum%20light%20and%0Alight%20sources%20even%20in%20the%20presence%20of%20poor%20detection%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning-based%2520variational%2520autoencoder%2520for%2520classification%2520of%250A%2520%2520quantum%2520and%2520classical%2520states%2520of%2520light%26entry.906535625%3DMahesh%2520Bhupati%2520and%2520Abhishek%2520Mall%2520and%2520Anshuman%2520Kumar%2520and%2520Pankaj%2520K.%2520Jha%26entry.1292438233%3D%2520%2520Advancements%2520in%2520optical%2520quantum%2520technologies%2520have%2520been%2520enabled%2520by%2520the%250Ageneration%252C%2520manipulation%252C%2520and%2520characterization%2520of%2520light%252C%2520with%2520identification%250Abased%2520on%2520its%2520photon%2520statistics.%2520However%252C%2520characterizing%2520light%2520and%2520its%2520sources%250Athrough%2520single%2520photon%2520measurements%2520often%2520requires%2520efficient%2520detectors%2520and%250Alonger%2520measurement%2520times%2520to%2520obtain%2520high-quality%2520photon%2520statistics.%2520Here%2520we%250Aintroduce%2520a%2520deep%2520learning-based%2520variational%2520autoencoder%2520%2528VAE%2529%2520method%2520for%250Aclassifying%2520single%2520photon%2520added%2520coherent%2520state%2520%2528SPACS%2529%252C%2520single%2520photon%2520added%250Athermal%2520state%2520%2528SPACS%2529%252C%2520mixed%2520states%2520between%2520coherent/SPACS%2520and%2520thermal/SPATS%2520of%250Alight.%2520Our%2520semisupervised%2520learning-based%2520VAE%2520efficiently%2520maps%2520the%2520photon%250Astatistics%2520features%2520of%2520light%2520to%2520a%2520lower%2520dimension%252C%2520enabling%2520quasi-instantaneous%250Aclassification%2520with%2520low%2520average%2520photon%2520counts.%2520The%2520proposed%2520VAE%2520method%2520is%250Arobust%2520and%2520maintains%2520classification%2520accuracy%2520in%2520the%2520presence%2520of%2520losses%2520inherent%250Ain%2520an%2520experiment%252C%2520such%2520as%2520finite%2520collection%2520efficiency%252C%2520non-unity%2520quantum%250Aefficiency%252C%2520finite%2520number%2520of%2520detectors%252C%2520etc.%2520Additionally%252C%2520leveraging%2520the%250Atransfer%2520learning%2520capabilities%2520of%2520VAE%2520enables%2520successful%2520classification%2520of%2520data%250Aof%2520any%2520quality%2520using%2520a%2520single%2520trained%2520model.%2520We%2520envision%2520that%2520such%2520a%2520deep%250Alearning%2520methodology%2520will%2520enable%2520better%2520classification%2520of%2520quantum%2520light%2520and%250Alight%2520sources%2520even%2520in%2520the%2520presence%2520of%2520poor%2520detection%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning-based%20variational%20autoencoder%20for%20classification%20of%0A%20%20quantum%20and%20classical%20states%20of%20light&entry.906535625=Mahesh%20Bhupati%20and%20Abhishek%20Mall%20and%20Anshuman%20Kumar%20and%20Pankaj%20K.%20Jha&entry.1292438233=%20%20Advancements%20in%20optical%20quantum%20technologies%20have%20been%20enabled%20by%20the%0Ageneration%2C%20manipulation%2C%20and%20characterization%20of%20light%2C%20with%20identification%0Abased%20on%20its%20photon%20statistics.%20However%2C%20characterizing%20light%20and%20its%20sources%0Athrough%20single%20photon%20measurements%20often%20requires%20efficient%20detectors%20and%0Alonger%20measurement%20times%20to%20obtain%20high-quality%20photon%20statistics.%20Here%20we%0Aintroduce%20a%20deep%20learning-based%20variational%20autoencoder%20%28VAE%29%20method%20for%0Aclassifying%20single%20photon%20added%20coherent%20state%20%28SPACS%29%2C%20single%20photon%20added%0Athermal%20state%20%28SPACS%29%2C%20mixed%20states%20between%20coherent/SPACS%20and%20thermal/SPATS%20of%0Alight.%20Our%20semisupervised%20learning-based%20VAE%20efficiently%20maps%20the%20photon%0Astatistics%20features%20of%20light%20to%20a%20lower%20dimension%2C%20enabling%20quasi-instantaneous%0Aclassification%20with%20low%20average%20photon%20counts.%20The%20proposed%20VAE%20method%20is%0Arobust%20and%20maintains%20classification%20accuracy%20in%20the%20presence%20of%20losses%20inherent%0Ain%20an%20experiment%2C%20such%20as%20finite%20collection%20efficiency%2C%20non-unity%20quantum%0Aefficiency%2C%20finite%20number%20of%20detectors%2C%20etc.%20Additionally%2C%20leveraging%20the%0Atransfer%20learning%20capabilities%20of%20VAE%20enables%20successful%20classification%20of%20data%0Aof%20any%20quality%20using%20a%20single%20trained%20model.%20We%20envision%20that%20such%20a%20deep%0Alearning%20methodology%20will%20enable%20better%20classification%20of%20quantum%20light%20and%0Alight%20sources%20even%20in%20the%20presence%20of%20poor%20detection%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05243v1&entry.124074799=Read"},
{"title": "UniBEV: Multi-modal 3D Object Detection with Uniform BEV Encoders for\n  Robustness against Missing Sensor Modalities", "author": "Shiming Wang and Holger Caesar and Liangliang Nan and Julian F. P. Kooij", "abstract": "  Multi-sensor object detection is an active research topic in automated\ndriving, but the robustness of such detection models against missing sensor\ninput (modality missing), e.g., due to a sudden sensor failure, is a critical\nproblem which remains under-studied. In this work, we propose UniBEV, an\nend-to-end multi-modal 3D object detection framework designed for robustness\nagainst missing modalities: UniBEV can operate on LiDAR plus camera input, but\nalso on LiDAR-only or camera-only input without retraining. To facilitate its\ndetector head to handle different input combinations, UniBEV aims to create\nwell-aligned Bird's Eye View (BEV) feature maps from each available modality.\nUnlike prior BEV-based multi-modal detection methods, all sensor modalities\nfollow a uniform approach to resample features from the native sensor\ncoordinate systems to the BEV features. We furthermore investigate the\nrobustness of various fusion strategies w.r.t. missing modalities: the commonly\nused feature concatenation, but also channel-wise averaging, and a\ngeneralization to weighted averaging termed Channel Normalized Weights. To\nvalidate its effectiveness, we compare UniBEV to state-of-the-art BEVFusion and\nMetaBEV on nuScenes over all sensor input combinations. In this setting, UniBEV\nachieves $52.5 \\%$ mAP on average over all input combinations, significantly\nimproving over the baselines ($43.5 \\%$ mAP on average for BEVFusion, $48.7 \\%$\nmAP on average for MetaBEV). An ablation study shows the robustness benefits of\nfusing by weighted averaging over regular concatenation, and of sharing queries\nbetween the BEV encoders of each modality. Our code is available at\nhttps://github.com/tudelft-iv/UniBEV.\n", "link": "http://arxiv.org/abs/2309.14516v3", "date": "2024-05-08", "relevancy": 1.8187, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6279}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6178}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniBEV%3A%20Multi-modal%203D%20Object%20Detection%20with%20Uniform%20BEV%20Encoders%20for%0A%20%20Robustness%20against%20Missing%20Sensor%20Modalities&body=Title%3A%20UniBEV%3A%20Multi-modal%203D%20Object%20Detection%20with%20Uniform%20BEV%20Encoders%20for%0A%20%20Robustness%20against%20Missing%20Sensor%20Modalities%0AAuthor%3A%20Shiming%20Wang%20and%20Holger%20Caesar%20and%20Liangliang%20Nan%20and%20Julian%20F.%20P.%20Kooij%0AAbstract%3A%20%20%20Multi-sensor%20object%20detection%20is%20an%20active%20research%20topic%20in%20automated%0Adriving%2C%20but%20the%20robustness%20of%20such%20detection%20models%20against%20missing%20sensor%0Ainput%20%28modality%20missing%29%2C%20e.g.%2C%20due%20to%20a%20sudden%20sensor%20failure%2C%20is%20a%20critical%0Aproblem%20which%20remains%20under-studied.%20In%20this%20work%2C%20we%20propose%20UniBEV%2C%20an%0Aend-to-end%20multi-modal%203D%20object%20detection%20framework%20designed%20for%20robustness%0Aagainst%20missing%20modalities%3A%20UniBEV%20can%20operate%20on%20LiDAR%20plus%20camera%20input%2C%20but%0Aalso%20on%20LiDAR-only%20or%20camera-only%20input%20without%20retraining.%20To%20facilitate%20its%0Adetector%20head%20to%20handle%20different%20input%20combinations%2C%20UniBEV%20aims%20to%20create%0Awell-aligned%20Bird%27s%20Eye%20View%20%28BEV%29%20feature%20maps%20from%20each%20available%20modality.%0AUnlike%20prior%20BEV-based%20multi-modal%20detection%20methods%2C%20all%20sensor%20modalities%0Afollow%20a%20uniform%20approach%20to%20resample%20features%20from%20the%20native%20sensor%0Acoordinate%20systems%20to%20the%20BEV%20features.%20We%20furthermore%20investigate%20the%0Arobustness%20of%20various%20fusion%20strategies%20w.r.t.%20missing%20modalities%3A%20the%20commonly%0Aused%20feature%20concatenation%2C%20but%20also%20channel-wise%20averaging%2C%20and%20a%0Ageneralization%20to%20weighted%20averaging%20termed%20Channel%20Normalized%20Weights.%20To%0Avalidate%20its%20effectiveness%2C%20we%20compare%20UniBEV%20to%20state-of-the-art%20BEVFusion%20and%0AMetaBEV%20on%20nuScenes%20over%20all%20sensor%20input%20combinations.%20In%20this%20setting%2C%20UniBEV%0Aachieves%20%2452.5%20%5C%25%24%20mAP%20on%20average%20over%20all%20input%20combinations%2C%20significantly%0Aimproving%20over%20the%20baselines%20%28%2443.5%20%5C%25%24%20mAP%20on%20average%20for%20BEVFusion%2C%20%2448.7%20%5C%25%24%0AmAP%20on%20average%20for%20MetaBEV%29.%20An%20ablation%20study%20shows%20the%20robustness%20benefits%20of%0Afusing%20by%20weighted%20averaging%20over%20regular%20concatenation%2C%20and%20of%20sharing%20queries%0Abetween%20the%20BEV%20encoders%20of%20each%20modality.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tudelft-iv/UniBEV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14516v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniBEV%253A%2520Multi-modal%25203D%2520Object%2520Detection%2520with%2520Uniform%2520BEV%2520Encoders%2520for%250A%2520%2520Robustness%2520against%2520Missing%2520Sensor%2520Modalities%26entry.906535625%3DShiming%2520Wang%2520and%2520Holger%2520Caesar%2520and%2520Liangliang%2520Nan%2520and%2520Julian%2520F.%2520P.%2520Kooij%26entry.1292438233%3D%2520%2520Multi-sensor%2520object%2520detection%2520is%2520an%2520active%2520research%2520topic%2520in%2520automated%250Adriving%252C%2520but%2520the%2520robustness%2520of%2520such%2520detection%2520models%2520against%2520missing%2520sensor%250Ainput%2520%2528modality%2520missing%2529%252C%2520e.g.%252C%2520due%2520to%2520a%2520sudden%2520sensor%2520failure%252C%2520is%2520a%2520critical%250Aproblem%2520which%2520remains%2520under-studied.%2520In%2520this%2520work%252C%2520we%2520propose%2520UniBEV%252C%2520an%250Aend-to-end%2520multi-modal%25203D%2520object%2520detection%2520framework%2520designed%2520for%2520robustness%250Aagainst%2520missing%2520modalities%253A%2520UniBEV%2520can%2520operate%2520on%2520LiDAR%2520plus%2520camera%2520input%252C%2520but%250Aalso%2520on%2520LiDAR-only%2520or%2520camera-only%2520input%2520without%2520retraining.%2520To%2520facilitate%2520its%250Adetector%2520head%2520to%2520handle%2520different%2520input%2520combinations%252C%2520UniBEV%2520aims%2520to%2520create%250Awell-aligned%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520feature%2520maps%2520from%2520each%2520available%2520modality.%250AUnlike%2520prior%2520BEV-based%2520multi-modal%2520detection%2520methods%252C%2520all%2520sensor%2520modalities%250Afollow%2520a%2520uniform%2520approach%2520to%2520resample%2520features%2520from%2520the%2520native%2520sensor%250Acoordinate%2520systems%2520to%2520the%2520BEV%2520features.%2520We%2520furthermore%2520investigate%2520the%250Arobustness%2520of%2520various%2520fusion%2520strategies%2520w.r.t.%2520missing%2520modalities%253A%2520the%2520commonly%250Aused%2520feature%2520concatenation%252C%2520but%2520also%2520channel-wise%2520averaging%252C%2520and%2520a%250Ageneralization%2520to%2520weighted%2520averaging%2520termed%2520Channel%2520Normalized%2520Weights.%2520To%250Avalidate%2520its%2520effectiveness%252C%2520we%2520compare%2520UniBEV%2520to%2520state-of-the-art%2520BEVFusion%2520and%250AMetaBEV%2520on%2520nuScenes%2520over%2520all%2520sensor%2520input%2520combinations.%2520In%2520this%2520setting%252C%2520UniBEV%250Aachieves%2520%252452.5%2520%255C%2525%2524%2520mAP%2520on%2520average%2520over%2520all%2520input%2520combinations%252C%2520significantly%250Aimproving%2520over%2520the%2520baselines%2520%2528%252443.5%2520%255C%2525%2524%2520mAP%2520on%2520average%2520for%2520BEVFusion%252C%2520%252448.7%2520%255C%2525%2524%250AmAP%2520on%2520average%2520for%2520MetaBEV%2529.%2520An%2520ablation%2520study%2520shows%2520the%2520robustness%2520benefits%2520of%250Afusing%2520by%2520weighted%2520averaging%2520over%2520regular%2520concatenation%252C%2520and%2520of%2520sharing%2520queries%250Abetween%2520the%2520BEV%2520encoders%2520of%2520each%2520modality.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/tudelft-iv/UniBEV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14516v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniBEV%3A%20Multi-modal%203D%20Object%20Detection%20with%20Uniform%20BEV%20Encoders%20for%0A%20%20Robustness%20against%20Missing%20Sensor%20Modalities&entry.906535625=Shiming%20Wang%20and%20Holger%20Caesar%20and%20Liangliang%20Nan%20and%20Julian%20F.%20P.%20Kooij&entry.1292438233=%20%20Multi-sensor%20object%20detection%20is%20an%20active%20research%20topic%20in%20automated%0Adriving%2C%20but%20the%20robustness%20of%20such%20detection%20models%20against%20missing%20sensor%0Ainput%20%28modality%20missing%29%2C%20e.g.%2C%20due%20to%20a%20sudden%20sensor%20failure%2C%20is%20a%20critical%0Aproblem%20which%20remains%20under-studied.%20In%20this%20work%2C%20we%20propose%20UniBEV%2C%20an%0Aend-to-end%20multi-modal%203D%20object%20detection%20framework%20designed%20for%20robustness%0Aagainst%20missing%20modalities%3A%20UniBEV%20can%20operate%20on%20LiDAR%20plus%20camera%20input%2C%20but%0Aalso%20on%20LiDAR-only%20or%20camera-only%20input%20without%20retraining.%20To%20facilitate%20its%0Adetector%20head%20to%20handle%20different%20input%20combinations%2C%20UniBEV%20aims%20to%20create%0Awell-aligned%20Bird%27s%20Eye%20View%20%28BEV%29%20feature%20maps%20from%20each%20available%20modality.%0AUnlike%20prior%20BEV-based%20multi-modal%20detection%20methods%2C%20all%20sensor%20modalities%0Afollow%20a%20uniform%20approach%20to%20resample%20features%20from%20the%20native%20sensor%0Acoordinate%20systems%20to%20the%20BEV%20features.%20We%20furthermore%20investigate%20the%0Arobustness%20of%20various%20fusion%20strategies%20w.r.t.%20missing%20modalities%3A%20the%20commonly%0Aused%20feature%20concatenation%2C%20but%20also%20channel-wise%20averaging%2C%20and%20a%0Ageneralization%20to%20weighted%20averaging%20termed%20Channel%20Normalized%20Weights.%20To%0Avalidate%20its%20effectiveness%2C%20we%20compare%20UniBEV%20to%20state-of-the-art%20BEVFusion%20and%0AMetaBEV%20on%20nuScenes%20over%20all%20sensor%20input%20combinations.%20In%20this%20setting%2C%20UniBEV%0Aachieves%20%2452.5%20%5C%25%24%20mAP%20on%20average%20over%20all%20input%20combinations%2C%20significantly%0Aimproving%20over%20the%20baselines%20%28%2443.5%20%5C%25%24%20mAP%20on%20average%20for%20BEVFusion%2C%20%2448.7%20%5C%25%24%0AmAP%20on%20average%20for%20MetaBEV%29.%20An%20ablation%20study%20shows%20the%20robustness%20benefits%20of%0Afusing%20by%20weighted%20averaging%20over%20regular%20concatenation%2C%20and%20of%20sharing%20queries%0Abetween%20the%20BEV%20encoders%20of%20each%20modality.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tudelft-iv/UniBEV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14516v3&entry.124074799=Read"},
{"title": "The Promise and Challenges of Using LLMs to Accelerate the Screening\n  Process of Systematic Reviews", "author": "Aleksi Huotala and Miikka Kuutila and Paul Ralph and Mika M\u00e4ntyl\u00e4", "abstract": "  Systematic review (SR) is a popular research method in software engineering\n(SE). However, conducting an SR takes an average of 67 weeks. Thus, automating\nany step of the SR process could reduce the effort associated with SRs. Our\nobjective is to investigate if Large Language Models (LLMs) can accelerate\ntitle-abstract screening by simplifying abstracts for human screeners, and\nautomating title-abstract screening. We performed an experiment where humans\nscreened titles and abstracts for 20 papers with both original and simplified\nabstracts from a prior SR. The experiment with human screeners was reproduced\nwith GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also\nstudied if different prompting techniques (Zero-shot (ZS), One-shot (OS),\nFew-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the\nscreening performance of LLMs. Lastly, we studied if redesigning the prompt\nused in the LLM reproduction of screening leads to improved performance. Text\nsimplification did not increase the screeners' screening performance, but\nreduced the time used in screening. Screeners' scientific literacy skills and\nresearcher status predict screening performance. Some LLM and prompt\ncombinations perform as well as human screeners in the screening tasks. Our\nresults indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5.\nAdditionally, Few-shot and One-shot prompting outperforms Zero-shot prompting.\nUsing LLMs for text simplification in the screening process does not\nsignificantly improve human performance. Using LLMs to automate title-abstract\nscreening seems promising, but current LLMs are not significantly more accurate\nthan human screeners. To recommend the use of LLMs in the screening process of\nSRs, more research is needed. We recommend future SR studies publish\nreplication packages with screening data to enable more conclusive\nexperimenting with LLM screening.\n", "link": "http://arxiv.org/abs/2404.15667v4", "date": "2024-05-08", "relevancy": 1.7874, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4544}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Promise%20and%20Challenges%20of%20Using%20LLMs%20to%20Accelerate%20the%20Screening%0A%20%20Process%20of%20Systematic%20Reviews&body=Title%3A%20The%20Promise%20and%20Challenges%20of%20Using%20LLMs%20to%20Accelerate%20the%20Screening%0A%20%20Process%20of%20Systematic%20Reviews%0AAuthor%3A%20Aleksi%20Huotala%20and%20Miikka%20Kuutila%20and%20Paul%20Ralph%20and%20Mika%20M%C3%A4ntyl%C3%A4%0AAbstract%3A%20%20%20Systematic%20review%20%28SR%29%20is%20a%20popular%20research%20method%20in%20software%20engineering%0A%28SE%29.%20However%2C%20conducting%20an%20SR%20takes%20an%20average%20of%2067%20weeks.%20Thus%2C%20automating%0Aany%20step%20of%20the%20SR%20process%20could%20reduce%20the%20effort%20associated%20with%20SRs.%20Our%0Aobjective%20is%20to%20investigate%20if%20Large%20Language%20Models%20%28LLMs%29%20can%20accelerate%0Atitle-abstract%20screening%20by%20simplifying%20abstracts%20for%20human%20screeners%2C%20and%0Aautomating%20title-abstract%20screening.%20We%20performed%20an%20experiment%20where%20humans%0Ascreened%20titles%20and%20abstracts%20for%2020%20papers%20with%20both%20original%20and%20simplified%0Aabstracts%20from%20a%20prior%20SR.%20The%20experiment%20with%20human%20screeners%20was%20reproduced%0Awith%20GPT-3.5%20and%20GPT-4%20LLMs%20to%20perform%20the%20same%20screening%20tasks.%20We%20also%0Astudied%20if%20different%20prompting%20techniques%20%28Zero-shot%20%28ZS%29%2C%20One-shot%20%28OS%29%2C%0AFew-shot%20%28FS%29%2C%20and%20Few-shot%20with%20Chain-of-Thought%20%28FS-CoT%29%29%20improve%20the%0Ascreening%20performance%20of%20LLMs.%20Lastly%2C%20we%20studied%20if%20redesigning%20the%20prompt%0Aused%20in%20the%20LLM%20reproduction%20of%20screening%20leads%20to%20improved%20performance.%20Text%0Asimplification%20did%20not%20increase%20the%20screeners%27%20screening%20performance%2C%20but%0Areduced%20the%20time%20used%20in%20screening.%20Screeners%27%20scientific%20literacy%20skills%20and%0Aresearcher%20status%20predict%20screening%20performance.%20Some%20LLM%20and%20prompt%0Acombinations%20perform%20as%20well%20as%20human%20screeners%20in%20the%20screening%20tasks.%20Our%0Aresults%20indicate%20that%20the%20GPT-4%20LLM%20is%20better%20than%20its%20predecessor%2C%20GPT-3.5.%0AAdditionally%2C%20Few-shot%20and%20One-shot%20prompting%20outperforms%20Zero-shot%20prompting.%0AUsing%20LLMs%20for%20text%20simplification%20in%20the%20screening%20process%20does%20not%0Asignificantly%20improve%20human%20performance.%20Using%20LLMs%20to%20automate%20title-abstract%0Ascreening%20seems%20promising%2C%20but%20current%20LLMs%20are%20not%20significantly%20more%20accurate%0Athan%20human%20screeners.%20To%20recommend%20the%20use%20of%20LLMs%20in%20the%20screening%20process%20of%0ASRs%2C%20more%20research%20is%20needed.%20We%20recommend%20future%20SR%20studies%20publish%0Areplication%20packages%20with%20screening%20data%20to%20enable%20more%20conclusive%0Aexperimenting%20with%20LLM%20screening.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15667v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Promise%2520and%2520Challenges%2520of%2520Using%2520LLMs%2520to%2520Accelerate%2520the%2520Screening%250A%2520%2520Process%2520of%2520Systematic%2520Reviews%26entry.906535625%3DAleksi%2520Huotala%2520and%2520Miikka%2520Kuutila%2520and%2520Paul%2520Ralph%2520and%2520Mika%2520M%25C3%25A4ntyl%25C3%25A4%26entry.1292438233%3D%2520%2520Systematic%2520review%2520%2528SR%2529%2520is%2520a%2520popular%2520research%2520method%2520in%2520software%2520engineering%250A%2528SE%2529.%2520However%252C%2520conducting%2520an%2520SR%2520takes%2520an%2520average%2520of%252067%2520weeks.%2520Thus%252C%2520automating%250Aany%2520step%2520of%2520the%2520SR%2520process%2520could%2520reduce%2520the%2520effort%2520associated%2520with%2520SRs.%2520Our%250Aobjective%2520is%2520to%2520investigate%2520if%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520accelerate%250Atitle-abstract%2520screening%2520by%2520simplifying%2520abstracts%2520for%2520human%2520screeners%252C%2520and%250Aautomating%2520title-abstract%2520screening.%2520We%2520performed%2520an%2520experiment%2520where%2520humans%250Ascreened%2520titles%2520and%2520abstracts%2520for%252020%2520papers%2520with%2520both%2520original%2520and%2520simplified%250Aabstracts%2520from%2520a%2520prior%2520SR.%2520The%2520experiment%2520with%2520human%2520screeners%2520was%2520reproduced%250Awith%2520GPT-3.5%2520and%2520GPT-4%2520LLMs%2520to%2520perform%2520the%2520same%2520screening%2520tasks.%2520We%2520also%250Astudied%2520if%2520different%2520prompting%2520techniques%2520%2528Zero-shot%2520%2528ZS%2529%252C%2520One-shot%2520%2528OS%2529%252C%250AFew-shot%2520%2528FS%2529%252C%2520and%2520Few-shot%2520with%2520Chain-of-Thought%2520%2528FS-CoT%2529%2529%2520improve%2520the%250Ascreening%2520performance%2520of%2520LLMs.%2520Lastly%252C%2520we%2520studied%2520if%2520redesigning%2520the%2520prompt%250Aused%2520in%2520the%2520LLM%2520reproduction%2520of%2520screening%2520leads%2520to%2520improved%2520performance.%2520Text%250Asimplification%2520did%2520not%2520increase%2520the%2520screeners%2527%2520screening%2520performance%252C%2520but%250Areduced%2520the%2520time%2520used%2520in%2520screening.%2520Screeners%2527%2520scientific%2520literacy%2520skills%2520and%250Aresearcher%2520status%2520predict%2520screening%2520performance.%2520Some%2520LLM%2520and%2520prompt%250Acombinations%2520perform%2520as%2520well%2520as%2520human%2520screeners%2520in%2520the%2520screening%2520tasks.%2520Our%250Aresults%2520indicate%2520that%2520the%2520GPT-4%2520LLM%2520is%2520better%2520than%2520its%2520predecessor%252C%2520GPT-3.5.%250AAdditionally%252C%2520Few-shot%2520and%2520One-shot%2520prompting%2520outperforms%2520Zero-shot%2520prompting.%250AUsing%2520LLMs%2520for%2520text%2520simplification%2520in%2520the%2520screening%2520process%2520does%2520not%250Asignificantly%2520improve%2520human%2520performance.%2520Using%2520LLMs%2520to%2520automate%2520title-abstract%250Ascreening%2520seems%2520promising%252C%2520but%2520current%2520LLMs%2520are%2520not%2520significantly%2520more%2520accurate%250Athan%2520human%2520screeners.%2520To%2520recommend%2520the%2520use%2520of%2520LLMs%2520in%2520the%2520screening%2520process%2520of%250ASRs%252C%2520more%2520research%2520is%2520needed.%2520We%2520recommend%2520future%2520SR%2520studies%2520publish%250Areplication%2520packages%2520with%2520screening%2520data%2520to%2520enable%2520more%2520conclusive%250Aexperimenting%2520with%2520LLM%2520screening.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15667v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Promise%20and%20Challenges%20of%20Using%20LLMs%20to%20Accelerate%20the%20Screening%0A%20%20Process%20of%20Systematic%20Reviews&entry.906535625=Aleksi%20Huotala%20and%20Miikka%20Kuutila%20and%20Paul%20Ralph%20and%20Mika%20M%C3%A4ntyl%C3%A4&entry.1292438233=%20%20Systematic%20review%20%28SR%29%20is%20a%20popular%20research%20method%20in%20software%20engineering%0A%28SE%29.%20However%2C%20conducting%20an%20SR%20takes%20an%20average%20of%2067%20weeks.%20Thus%2C%20automating%0Aany%20step%20of%20the%20SR%20process%20could%20reduce%20the%20effort%20associated%20with%20SRs.%20Our%0Aobjective%20is%20to%20investigate%20if%20Large%20Language%20Models%20%28LLMs%29%20can%20accelerate%0Atitle-abstract%20screening%20by%20simplifying%20abstracts%20for%20human%20screeners%2C%20and%0Aautomating%20title-abstract%20screening.%20We%20performed%20an%20experiment%20where%20humans%0Ascreened%20titles%20and%20abstracts%20for%2020%20papers%20with%20both%20original%20and%20simplified%0Aabstracts%20from%20a%20prior%20SR.%20The%20experiment%20with%20human%20screeners%20was%20reproduced%0Awith%20GPT-3.5%20and%20GPT-4%20LLMs%20to%20perform%20the%20same%20screening%20tasks.%20We%20also%0Astudied%20if%20different%20prompting%20techniques%20%28Zero-shot%20%28ZS%29%2C%20One-shot%20%28OS%29%2C%0AFew-shot%20%28FS%29%2C%20and%20Few-shot%20with%20Chain-of-Thought%20%28FS-CoT%29%29%20improve%20the%0Ascreening%20performance%20of%20LLMs.%20Lastly%2C%20we%20studied%20if%20redesigning%20the%20prompt%0Aused%20in%20the%20LLM%20reproduction%20of%20screening%20leads%20to%20improved%20performance.%20Text%0Asimplification%20did%20not%20increase%20the%20screeners%27%20screening%20performance%2C%20but%0Areduced%20the%20time%20used%20in%20screening.%20Screeners%27%20scientific%20literacy%20skills%20and%0Aresearcher%20status%20predict%20screening%20performance.%20Some%20LLM%20and%20prompt%0Acombinations%20perform%20as%20well%20as%20human%20screeners%20in%20the%20screening%20tasks.%20Our%0Aresults%20indicate%20that%20the%20GPT-4%20LLM%20is%20better%20than%20its%20predecessor%2C%20GPT-3.5.%0AAdditionally%2C%20Few-shot%20and%20One-shot%20prompting%20outperforms%20Zero-shot%20prompting.%0AUsing%20LLMs%20for%20text%20simplification%20in%20the%20screening%20process%20does%20not%0Asignificantly%20improve%20human%20performance.%20Using%20LLMs%20to%20automate%20title-abstract%0Ascreening%20seems%20promising%2C%20but%20current%20LLMs%20are%20not%20significantly%20more%20accurate%0Athan%20human%20screeners.%20To%20recommend%20the%20use%20of%20LLMs%20in%20the%20screening%20process%20of%0ASRs%2C%20more%20research%20is%20needed.%20We%20recommend%20future%20SR%20studies%20publish%0Areplication%20packages%20with%20screening%20data%20to%20enable%20more%20conclusive%0Aexperimenting%20with%20LLM%20screening.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15667v4&entry.124074799=Read"},
{"title": "Online Long-run Constrained Optimization", "author": "Shijie Pan and Wenjie Huang", "abstract": "  A novel Follow-the-Perturbed-Leader type algorithm is proposed and analyzed\nfor solving general long-term constrained optimization problems in online\nmanner, where the objective and constraints are arbitrarily generated and not\nnecessarily convex. In each period, random linear perturbation and strongly\nconcave perturbation are incorporated in primal and dual directions,\nrespectively, to the offline oracle, and a global minimax point is searched as\nthe solution. Based on a proposed expected static cumulative regret, we derive\nthe first sublinear $O(T^{8/9})$ regret complexity for this class of problems.\nThe proposed algorithm is applied to tackle a long-term (extreme value)\nconstrained river pollutant source identification problem, validate the\ntheoretical results and exhibit superior performance compared to existing\nmethods.\n", "link": "http://arxiv.org/abs/2311.02426v2", "date": "2024-05-08", "relevancy": 1.7855, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.46}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4396}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Long-run%20Constrained%20Optimization&body=Title%3A%20Online%20Long-run%20Constrained%20Optimization%0AAuthor%3A%20Shijie%20Pan%20and%20Wenjie%20Huang%0AAbstract%3A%20%20%20A%20novel%20Follow-the-Perturbed-Leader%20type%20algorithm%20is%20proposed%20and%20analyzed%0Afor%20solving%20general%20long-term%20constrained%20optimization%20problems%20in%20online%0Amanner%2C%20where%20the%20objective%20and%20constraints%20are%20arbitrarily%20generated%20and%20not%0Anecessarily%20convex.%20In%20each%20period%2C%20random%20linear%20perturbation%20and%20strongly%0Aconcave%20perturbation%20are%20incorporated%20in%20primal%20and%20dual%20directions%2C%0Arespectively%2C%20to%20the%20offline%20oracle%2C%20and%20a%20global%20minimax%20point%20is%20searched%20as%0Athe%20solution.%20Based%20on%20a%20proposed%20expected%20static%20cumulative%20regret%2C%20we%20derive%0Athe%20first%20sublinear%20%24O%28T%5E%7B8/9%7D%29%24%20regret%20complexity%20for%20this%20class%20of%20problems.%0AThe%20proposed%20algorithm%20is%20applied%20to%20tackle%20a%20long-term%20%28extreme%20value%29%0Aconstrained%20river%20pollutant%20source%20identification%20problem%2C%20validate%20the%0Atheoretical%20results%20and%20exhibit%20superior%20performance%20compared%20to%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Long-run%2520Constrained%2520Optimization%26entry.906535625%3DShijie%2520Pan%2520and%2520Wenjie%2520Huang%26entry.1292438233%3D%2520%2520A%2520novel%2520Follow-the-Perturbed-Leader%2520type%2520algorithm%2520is%2520proposed%2520and%2520analyzed%250Afor%2520solving%2520general%2520long-term%2520constrained%2520optimization%2520problems%2520in%2520online%250Amanner%252C%2520where%2520the%2520objective%2520and%2520constraints%2520are%2520arbitrarily%2520generated%2520and%2520not%250Anecessarily%2520convex.%2520In%2520each%2520period%252C%2520random%2520linear%2520perturbation%2520and%2520strongly%250Aconcave%2520perturbation%2520are%2520incorporated%2520in%2520primal%2520and%2520dual%2520directions%252C%250Arespectively%252C%2520to%2520the%2520offline%2520oracle%252C%2520and%2520a%2520global%2520minimax%2520point%2520is%2520searched%2520as%250Athe%2520solution.%2520Based%2520on%2520a%2520proposed%2520expected%2520static%2520cumulative%2520regret%252C%2520we%2520derive%250Athe%2520first%2520sublinear%2520%2524O%2528T%255E%257B8/9%257D%2529%2524%2520regret%2520complexity%2520for%2520this%2520class%2520of%2520problems.%250AThe%2520proposed%2520algorithm%2520is%2520applied%2520to%2520tackle%2520a%2520long-term%2520%2528extreme%2520value%2529%250Aconstrained%2520river%2520pollutant%2520source%2520identification%2520problem%252C%2520validate%2520the%250Atheoretical%2520results%2520and%2520exhibit%2520superior%2520performance%2520compared%2520to%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Long-run%20Constrained%20Optimization&entry.906535625=Shijie%20Pan%20and%20Wenjie%20Huang&entry.1292438233=%20%20A%20novel%20Follow-the-Perturbed-Leader%20type%20algorithm%20is%20proposed%20and%20analyzed%0Afor%20solving%20general%20long-term%20constrained%20optimization%20problems%20in%20online%0Amanner%2C%20where%20the%20objective%20and%20constraints%20are%20arbitrarily%20generated%20and%20not%0Anecessarily%20convex.%20In%20each%20period%2C%20random%20linear%20perturbation%20and%20strongly%0Aconcave%20perturbation%20are%20incorporated%20in%20primal%20and%20dual%20directions%2C%0Arespectively%2C%20to%20the%20offline%20oracle%2C%20and%20a%20global%20minimax%20point%20is%20searched%20as%0Athe%20solution.%20Based%20on%20a%20proposed%20expected%20static%20cumulative%20regret%2C%20we%20derive%0Athe%20first%20sublinear%20%24O%28T%5E%7B8/9%7D%29%24%20regret%20complexity%20for%20this%20class%20of%20problems.%0AThe%20proposed%20algorithm%20is%20applied%20to%20tackle%20a%20long-term%20%28extreme%20value%29%0Aconstrained%20river%20pollutant%20source%20identification%20problem%2C%20validate%20the%0Atheoretical%20results%20and%20exhibit%20superior%20performance%20compared%20to%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02426v2&entry.124074799=Read"},
{"title": "Air Gap: Protecting Privacy-Conscious Conversational Agents", "author": "Eugene Bagdasaryan and Ren Yi and Sahra Ghalebikesabi and Peter Kairouz and Marco Gruteser and Sewoong Oh and Borja Balle and Daniel Ramage", "abstract": "  The growing use of large language model (LLM)-based conversational agents to\nmanage sensitive user data raises significant privacy concerns. While these\nagents excel at understanding and acting on context, this capability can be\nexploited by malicious actors. We introduce a novel threat model where\nadversarial third-party apps manipulate the context of interaction to trick\nLLM-based agents into revealing private information not relevant to the task at\nhand.\n  Grounded in the framework of contextual integrity, we introduce AirGapAgent,\na privacy-conscious agent designed to prevent unintended data leakage by\nrestricting the agent's access to only the data necessary for a specific task.\nExtensive experiments using Gemini, GPT, and Mistral models as agents validate\nour approach's effectiveness in mitigating this form of context hijacking while\nmaintaining core agent functionality. For example, we show that a single-query\ncontext hijacking attack on a Gemini Ultra agent reduces its ability to protect\nuser data from 94% to 45%, while an AirGapAgent achieves 97% protection,\nrendering the same attack ineffective.\n", "link": "http://arxiv.org/abs/2405.05175v1", "date": "2024-05-08", "relevancy": 1.7845, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4598}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4365}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Air%20Gap%3A%20Protecting%20Privacy-Conscious%20Conversational%20Agents&body=Title%3A%20Air%20Gap%3A%20Protecting%20Privacy-Conscious%20Conversational%20Agents%0AAuthor%3A%20Eugene%20Bagdasaryan%20and%20Ren%20Yi%20and%20Sahra%20Ghalebikesabi%20and%20Peter%20Kairouz%20and%20Marco%20Gruteser%20and%20Sewoong%20Oh%20and%20Borja%20Balle%20and%20Daniel%20Ramage%0AAbstract%3A%20%20%20The%20growing%20use%20of%20large%20language%20model%20%28LLM%29-based%20conversational%20agents%20to%0Amanage%20sensitive%20user%20data%20raises%20significant%20privacy%20concerns.%20While%20these%0Aagents%20excel%20at%20understanding%20and%20acting%20on%20context%2C%20this%20capability%20can%20be%0Aexploited%20by%20malicious%20actors.%20We%20introduce%20a%20novel%20threat%20model%20where%0Aadversarial%20third-party%20apps%20manipulate%20the%20context%20of%20interaction%20to%20trick%0ALLM-based%20agents%20into%20revealing%20private%20information%20not%20relevant%20to%20the%20task%20at%0Ahand.%0A%20%20Grounded%20in%20the%20framework%20of%20contextual%20integrity%2C%20we%20introduce%20AirGapAgent%2C%0Aa%20privacy-conscious%20agent%20designed%20to%20prevent%20unintended%20data%20leakage%20by%0Arestricting%20the%20agent%27s%20access%20to%20only%20the%20data%20necessary%20for%20a%20specific%20task.%0AExtensive%20experiments%20using%20Gemini%2C%20GPT%2C%20and%20Mistral%20models%20as%20agents%20validate%0Aour%20approach%27s%20effectiveness%20in%20mitigating%20this%20form%20of%20context%20hijacking%20while%0Amaintaining%20core%20agent%20functionality.%20For%20example%2C%20we%20show%20that%20a%20single-query%0Acontext%20hijacking%20attack%20on%20a%20Gemini%20Ultra%20agent%20reduces%20its%20ability%20to%20protect%0Auser%20data%20from%2094%25%20to%2045%25%2C%20while%20an%20AirGapAgent%20achieves%2097%25%20protection%2C%0Arendering%20the%20same%20attack%20ineffective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAir%2520Gap%253A%2520Protecting%2520Privacy-Conscious%2520Conversational%2520Agents%26entry.906535625%3DEugene%2520Bagdasaryan%2520and%2520Ren%2520Yi%2520and%2520Sahra%2520Ghalebikesabi%2520and%2520Peter%2520Kairouz%2520and%2520Marco%2520Gruteser%2520and%2520Sewoong%2520Oh%2520and%2520Borja%2520Balle%2520and%2520Daniel%2520Ramage%26entry.1292438233%3D%2520%2520The%2520growing%2520use%2520of%2520large%2520language%2520model%2520%2528LLM%2529-based%2520conversational%2520agents%2520to%250Amanage%2520sensitive%2520user%2520data%2520raises%2520significant%2520privacy%2520concerns.%2520While%2520these%250Aagents%2520excel%2520at%2520understanding%2520and%2520acting%2520on%2520context%252C%2520this%2520capability%2520can%2520be%250Aexploited%2520by%2520malicious%2520actors.%2520We%2520introduce%2520a%2520novel%2520threat%2520model%2520where%250Aadversarial%2520third-party%2520apps%2520manipulate%2520the%2520context%2520of%2520interaction%2520to%2520trick%250ALLM-based%2520agents%2520into%2520revealing%2520private%2520information%2520not%2520relevant%2520to%2520the%2520task%2520at%250Ahand.%250A%2520%2520Grounded%2520in%2520the%2520framework%2520of%2520contextual%2520integrity%252C%2520we%2520introduce%2520AirGapAgent%252C%250Aa%2520privacy-conscious%2520agent%2520designed%2520to%2520prevent%2520unintended%2520data%2520leakage%2520by%250Arestricting%2520the%2520agent%2527s%2520access%2520to%2520only%2520the%2520data%2520necessary%2520for%2520a%2520specific%2520task.%250AExtensive%2520experiments%2520using%2520Gemini%252C%2520GPT%252C%2520and%2520Mistral%2520models%2520as%2520agents%2520validate%250Aour%2520approach%2527s%2520effectiveness%2520in%2520mitigating%2520this%2520form%2520of%2520context%2520hijacking%2520while%250Amaintaining%2520core%2520agent%2520functionality.%2520For%2520example%252C%2520we%2520show%2520that%2520a%2520single-query%250Acontext%2520hijacking%2520attack%2520on%2520a%2520Gemini%2520Ultra%2520agent%2520reduces%2520its%2520ability%2520to%2520protect%250Auser%2520data%2520from%252094%2525%2520to%252045%2525%252C%2520while%2520an%2520AirGapAgent%2520achieves%252097%2525%2520protection%252C%250Arendering%2520the%2520same%2520attack%2520ineffective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Air%20Gap%3A%20Protecting%20Privacy-Conscious%20Conversational%20Agents&entry.906535625=Eugene%20Bagdasaryan%20and%20Ren%20Yi%20and%20Sahra%20Ghalebikesabi%20and%20Peter%20Kairouz%20and%20Marco%20Gruteser%20and%20Sewoong%20Oh%20and%20Borja%20Balle%20and%20Daniel%20Ramage&entry.1292438233=%20%20The%20growing%20use%20of%20large%20language%20model%20%28LLM%29-based%20conversational%20agents%20to%0Amanage%20sensitive%20user%20data%20raises%20significant%20privacy%20concerns.%20While%20these%0Aagents%20excel%20at%20understanding%20and%20acting%20on%20context%2C%20this%20capability%20can%20be%0Aexploited%20by%20malicious%20actors.%20We%20introduce%20a%20novel%20threat%20model%20where%0Aadversarial%20third-party%20apps%20manipulate%20the%20context%20of%20interaction%20to%20trick%0ALLM-based%20agents%20into%20revealing%20private%20information%20not%20relevant%20to%20the%20task%20at%0Ahand.%0A%20%20Grounded%20in%20the%20framework%20of%20contextual%20integrity%2C%20we%20introduce%20AirGapAgent%2C%0Aa%20privacy-conscious%20agent%20designed%20to%20prevent%20unintended%20data%20leakage%20by%0Arestricting%20the%20agent%27s%20access%20to%20only%20the%20data%20necessary%20for%20a%20specific%20task.%0AExtensive%20experiments%20using%20Gemini%2C%20GPT%2C%20and%20Mistral%20models%20as%20agents%20validate%0Aour%20approach%27s%20effectiveness%20in%20mitigating%20this%20form%20of%20context%20hijacking%20while%0Amaintaining%20core%20agent%20functionality.%20For%20example%2C%20we%20show%20that%20a%20single-query%0Acontext%20hijacking%20attack%20on%20a%20Gemini%20Ultra%20agent%20reduces%20its%20ability%20to%20protect%0Auser%20data%20from%2094%25%20to%2045%25%2C%20while%20an%20AirGapAgent%20achieves%2097%25%20protection%2C%0Arendering%20the%20same%20attack%20ineffective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05175v1&entry.124074799=Read"},
{"title": "Stability and Performance Analysis of Discrete-Time ReLU Recurrent\n  Neural Networks", "author": "Sahel Vahedi Noori and Bin Hu and Geir Dullerud and Peter Seiler", "abstract": "  This paper presents sufficient conditions for the stability and $\\ell_2$-gain\nperformance of recurrent neural networks (RNNs) with ReLU activation functions.\nThese conditions are derived by combining Lyapunov/dissipativity theory with\nQuadratic Constraints (QCs) satisfied by repeated ReLUs. We write a general\nclass of QCs for repeated RELUs using known properties for the scalar ReLU. Our\nstability and performance condition uses these QCs along with a \"lifted\"\nrepresentation for the ReLU RNN. We show that the positive homogeneity property\nsatisfied by a scalar ReLU does not expand the class of QCs for the repeated\nReLU. We present examples to demonstrate the stability / performance condition\nand study the effect of the lifting horizon.\n", "link": "http://arxiv.org/abs/2405.05236v1", "date": "2024-05-08", "relevancy": 1.7776, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4451}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4446}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stability%20and%20Performance%20Analysis%20of%20Discrete-Time%20ReLU%20Recurrent%0A%20%20Neural%20Networks&body=Title%3A%20Stability%20and%20Performance%20Analysis%20of%20Discrete-Time%20ReLU%20Recurrent%0A%20%20Neural%20Networks%0AAuthor%3A%20Sahel%20Vahedi%20Noori%20and%20Bin%20Hu%20and%20Geir%20Dullerud%20and%20Peter%20Seiler%0AAbstract%3A%20%20%20This%20paper%20presents%20sufficient%20conditions%20for%20the%20stability%20and%20%24%5Cell_2%24-gain%0Aperformance%20of%20recurrent%20neural%20networks%20%28RNNs%29%20with%20ReLU%20activation%20functions.%0AThese%20conditions%20are%20derived%20by%20combining%20Lyapunov/dissipativity%20theory%20with%0AQuadratic%20Constraints%20%28QCs%29%20satisfied%20by%20repeated%20ReLUs.%20We%20write%20a%20general%0Aclass%20of%20QCs%20for%20repeated%20RELUs%20using%20known%20properties%20for%20the%20scalar%20ReLU.%20Our%0Astability%20and%20performance%20condition%20uses%20these%20QCs%20along%20with%20a%20%22lifted%22%0Arepresentation%20for%20the%20ReLU%20RNN.%20We%20show%20that%20the%20positive%20homogeneity%20property%0Asatisfied%20by%20a%20scalar%20ReLU%20does%20not%20expand%20the%20class%20of%20QCs%20for%20the%20repeated%0AReLU.%20We%20present%20examples%20to%20demonstrate%20the%20stability%20/%20performance%20condition%0Aand%20study%20the%20effect%20of%20the%20lifting%20horizon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStability%2520and%2520Performance%2520Analysis%2520of%2520Discrete-Time%2520ReLU%2520Recurrent%250A%2520%2520Neural%2520Networks%26entry.906535625%3DSahel%2520Vahedi%2520Noori%2520and%2520Bin%2520Hu%2520and%2520Geir%2520Dullerud%2520and%2520Peter%2520Seiler%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520sufficient%2520conditions%2520for%2520the%2520stability%2520and%2520%2524%255Cell_2%2524-gain%250Aperformance%2520of%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520with%2520ReLU%2520activation%2520functions.%250AThese%2520conditions%2520are%2520derived%2520by%2520combining%2520Lyapunov/dissipativity%2520theory%2520with%250AQuadratic%2520Constraints%2520%2528QCs%2529%2520satisfied%2520by%2520repeated%2520ReLUs.%2520We%2520write%2520a%2520general%250Aclass%2520of%2520QCs%2520for%2520repeated%2520RELUs%2520using%2520known%2520properties%2520for%2520the%2520scalar%2520ReLU.%2520Our%250Astability%2520and%2520performance%2520condition%2520uses%2520these%2520QCs%2520along%2520with%2520a%2520%2522lifted%2522%250Arepresentation%2520for%2520the%2520ReLU%2520RNN.%2520We%2520show%2520that%2520the%2520positive%2520homogeneity%2520property%250Asatisfied%2520by%2520a%2520scalar%2520ReLU%2520does%2520not%2520expand%2520the%2520class%2520of%2520QCs%2520for%2520the%2520repeated%250AReLU.%2520We%2520present%2520examples%2520to%2520demonstrate%2520the%2520stability%2520/%2520performance%2520condition%250Aand%2520study%2520the%2520effect%2520of%2520the%2520lifting%2520horizon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stability%20and%20Performance%20Analysis%20of%20Discrete-Time%20ReLU%20Recurrent%0A%20%20Neural%20Networks&entry.906535625=Sahel%20Vahedi%20Noori%20and%20Bin%20Hu%20and%20Geir%20Dullerud%20and%20Peter%20Seiler&entry.1292438233=%20%20This%20paper%20presents%20sufficient%20conditions%20for%20the%20stability%20and%20%24%5Cell_2%24-gain%0Aperformance%20of%20recurrent%20neural%20networks%20%28RNNs%29%20with%20ReLU%20activation%20functions.%0AThese%20conditions%20are%20derived%20by%20combining%20Lyapunov/dissipativity%20theory%20with%0AQuadratic%20Constraints%20%28QCs%29%20satisfied%20by%20repeated%20ReLUs.%20We%20write%20a%20general%0Aclass%20of%20QCs%20for%20repeated%20RELUs%20using%20known%20properties%20for%20the%20scalar%20ReLU.%20Our%0Astability%20and%20performance%20condition%20uses%20these%20QCs%20along%20with%20a%20%22lifted%22%0Arepresentation%20for%20the%20ReLU%20RNN.%20We%20show%20that%20the%20positive%20homogeneity%20property%0Asatisfied%20by%20a%20scalar%20ReLU%20does%20not%20expand%20the%20class%20of%20QCs%20for%20the%20repeated%0AReLU.%20We%20present%20examples%20to%20demonstrate%20the%20stability%20/%20performance%20condition%0Aand%20study%20the%20effect%20of%20the%20lifting%20horizon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05236v1&entry.124074799=Read"},
{"title": "Speech Understanding on Tiny Devices with A Learning Cache", "author": "Afsara Benazir and Zhiming Xu and Felix Xiaozhu Lin", "abstract": "  This paper addresses spoken language understanding (SLU) on\nmicrocontroller-like embedded devices, integrating on-device execution with\ncloud offloading in a novel fashion. We leverage temporal locality in the\nspeech inputs to a device and reuse recent SLU inferences accordingly. Our idea\nis simple: let the device match incoming inputs against cached results, and\nonly offload inputs not matched to any cached ones to the cloud for full\ninference. Realization of this idea, however, is non-trivial: the device needs\nto compare acoustic features in a robust yet low-cost way. To this end, we\npresent SpeechCache (or SC), a speech cache for tiny devices. It matches speech\ninputs at two levels of representations: first by sequences of clustered raw\nsound units, then as sequences of phonemes. Working in tandem, the two\nrepresentations offer complementary tradeoffs between cost and efficiency. To\nboost accuracy even further, our cache learns to personalize: with the\nmismatched and then offloaded inputs, it continuously finetunes the device's\nfeature extractors with the assistance of the cloud. We implement SC on an\noff-the-shelf STM32 microcontroller. The complete implementation has a small\nmemory footprint of 2MB. Evaluated on challenging speech benchmarks, our system\nresolves 45%-90% of inputs on device, reducing the average latency by up to 80%\ncompared to offloading to popular cloud speech recognition services. The\nbenefit brought by our proposed SC is notable even in adversarial settings -\nnoisy environments, cold cache, or one device shared by a number of users.\n", "link": "http://arxiv.org/abs/2311.18188v4", "date": "2024-05-08", "relevancy": 1.7581, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4727}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4394}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speech%20Understanding%20on%20Tiny%20Devices%20with%20A%20Learning%20Cache&body=Title%3A%20Speech%20Understanding%20on%20Tiny%20Devices%20with%20A%20Learning%20Cache%0AAuthor%3A%20Afsara%20Benazir%20and%20Zhiming%20Xu%20and%20Felix%20Xiaozhu%20Lin%0AAbstract%3A%20%20%20This%20paper%20addresses%20spoken%20language%20understanding%20%28SLU%29%20on%0Amicrocontroller-like%20embedded%20devices%2C%20integrating%20on-device%20execution%20with%0Acloud%20offloading%20in%20a%20novel%20fashion.%20We%20leverage%20temporal%20locality%20in%20the%0Aspeech%20inputs%20to%20a%20device%20and%20reuse%20recent%20SLU%20inferences%20accordingly.%20Our%20idea%0Ais%20simple%3A%20let%20the%20device%20match%20incoming%20inputs%20against%20cached%20results%2C%20and%0Aonly%20offload%20inputs%20not%20matched%20to%20any%20cached%20ones%20to%20the%20cloud%20for%20full%0Ainference.%20Realization%20of%20this%20idea%2C%20however%2C%20is%20non-trivial%3A%20the%20device%20needs%0Ato%20compare%20acoustic%20features%20in%20a%20robust%20yet%20low-cost%20way.%20To%20this%20end%2C%20we%0Apresent%20SpeechCache%20%28or%20SC%29%2C%20a%20speech%20cache%20for%20tiny%20devices.%20It%20matches%20speech%0Ainputs%20at%20two%20levels%20of%20representations%3A%20first%20by%20sequences%20of%20clustered%20raw%0Asound%20units%2C%20then%20as%20sequences%20of%20phonemes.%20Working%20in%20tandem%2C%20the%20two%0Arepresentations%20offer%20complementary%20tradeoffs%20between%20cost%20and%20efficiency.%20To%0Aboost%20accuracy%20even%20further%2C%20our%20cache%20learns%20to%20personalize%3A%20with%20the%0Amismatched%20and%20then%20offloaded%20inputs%2C%20it%20continuously%20finetunes%20the%20device%27s%0Afeature%20extractors%20with%20the%20assistance%20of%20the%20cloud.%20We%20implement%20SC%20on%20an%0Aoff-the-shelf%20STM32%20microcontroller.%20The%20complete%20implementation%20has%20a%20small%0Amemory%20footprint%20of%202MB.%20Evaluated%20on%20challenging%20speech%20benchmarks%2C%20our%20system%0Aresolves%2045%25-90%25%20of%20inputs%20on%20device%2C%20reducing%20the%20average%20latency%20by%20up%20to%2080%25%0Acompared%20to%20offloading%20to%20popular%20cloud%20speech%20recognition%20services.%20The%0Abenefit%20brought%20by%20our%20proposed%20SC%20is%20notable%20even%20in%20adversarial%20settings%20-%0Anoisy%20environments%2C%20cold%20cache%2C%20or%20one%20device%20shared%20by%20a%20number%20of%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18188v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeech%2520Understanding%2520on%2520Tiny%2520Devices%2520with%2520A%2520Learning%2520Cache%26entry.906535625%3DAfsara%2520Benazir%2520and%2520Zhiming%2520Xu%2520and%2520Felix%2520Xiaozhu%2520Lin%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520spoken%2520language%2520understanding%2520%2528SLU%2529%2520on%250Amicrocontroller-like%2520embedded%2520devices%252C%2520integrating%2520on-device%2520execution%2520with%250Acloud%2520offloading%2520in%2520a%2520novel%2520fashion.%2520We%2520leverage%2520temporal%2520locality%2520in%2520the%250Aspeech%2520inputs%2520to%2520a%2520device%2520and%2520reuse%2520recent%2520SLU%2520inferences%2520accordingly.%2520Our%2520idea%250Ais%2520simple%253A%2520let%2520the%2520device%2520match%2520incoming%2520inputs%2520against%2520cached%2520results%252C%2520and%250Aonly%2520offload%2520inputs%2520not%2520matched%2520to%2520any%2520cached%2520ones%2520to%2520the%2520cloud%2520for%2520full%250Ainference.%2520Realization%2520of%2520this%2520idea%252C%2520however%252C%2520is%2520non-trivial%253A%2520the%2520device%2520needs%250Ato%2520compare%2520acoustic%2520features%2520in%2520a%2520robust%2520yet%2520low-cost%2520way.%2520To%2520this%2520end%252C%2520we%250Apresent%2520SpeechCache%2520%2528or%2520SC%2529%252C%2520a%2520speech%2520cache%2520for%2520tiny%2520devices.%2520It%2520matches%2520speech%250Ainputs%2520at%2520two%2520levels%2520of%2520representations%253A%2520first%2520by%2520sequences%2520of%2520clustered%2520raw%250Asound%2520units%252C%2520then%2520as%2520sequences%2520of%2520phonemes.%2520Working%2520in%2520tandem%252C%2520the%2520two%250Arepresentations%2520offer%2520complementary%2520tradeoffs%2520between%2520cost%2520and%2520efficiency.%2520To%250Aboost%2520accuracy%2520even%2520further%252C%2520our%2520cache%2520learns%2520to%2520personalize%253A%2520with%2520the%250Amismatched%2520and%2520then%2520offloaded%2520inputs%252C%2520it%2520continuously%2520finetunes%2520the%2520device%2527s%250Afeature%2520extractors%2520with%2520the%2520assistance%2520of%2520the%2520cloud.%2520We%2520implement%2520SC%2520on%2520an%250Aoff-the-shelf%2520STM32%2520microcontroller.%2520The%2520complete%2520implementation%2520has%2520a%2520small%250Amemory%2520footprint%2520of%25202MB.%2520Evaluated%2520on%2520challenging%2520speech%2520benchmarks%252C%2520our%2520system%250Aresolves%252045%2525-90%2525%2520of%2520inputs%2520on%2520device%252C%2520reducing%2520the%2520average%2520latency%2520by%2520up%2520to%252080%2525%250Acompared%2520to%2520offloading%2520to%2520popular%2520cloud%2520speech%2520recognition%2520services.%2520The%250Abenefit%2520brought%2520by%2520our%2520proposed%2520SC%2520is%2520notable%2520even%2520in%2520adversarial%2520settings%2520-%250Anoisy%2520environments%252C%2520cold%2520cache%252C%2520or%2520one%2520device%2520shared%2520by%2520a%2520number%2520of%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18188v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speech%20Understanding%20on%20Tiny%20Devices%20with%20A%20Learning%20Cache&entry.906535625=Afsara%20Benazir%20and%20Zhiming%20Xu%20and%20Felix%20Xiaozhu%20Lin&entry.1292438233=%20%20This%20paper%20addresses%20spoken%20language%20understanding%20%28SLU%29%20on%0Amicrocontroller-like%20embedded%20devices%2C%20integrating%20on-device%20execution%20with%0Acloud%20offloading%20in%20a%20novel%20fashion.%20We%20leverage%20temporal%20locality%20in%20the%0Aspeech%20inputs%20to%20a%20device%20and%20reuse%20recent%20SLU%20inferences%20accordingly.%20Our%20idea%0Ais%20simple%3A%20let%20the%20device%20match%20incoming%20inputs%20against%20cached%20results%2C%20and%0Aonly%20offload%20inputs%20not%20matched%20to%20any%20cached%20ones%20to%20the%20cloud%20for%20full%0Ainference.%20Realization%20of%20this%20idea%2C%20however%2C%20is%20non-trivial%3A%20the%20device%20needs%0Ato%20compare%20acoustic%20features%20in%20a%20robust%20yet%20low-cost%20way.%20To%20this%20end%2C%20we%0Apresent%20SpeechCache%20%28or%20SC%29%2C%20a%20speech%20cache%20for%20tiny%20devices.%20It%20matches%20speech%0Ainputs%20at%20two%20levels%20of%20representations%3A%20first%20by%20sequences%20of%20clustered%20raw%0Asound%20units%2C%20then%20as%20sequences%20of%20phonemes.%20Working%20in%20tandem%2C%20the%20two%0Arepresentations%20offer%20complementary%20tradeoffs%20between%20cost%20and%20efficiency.%20To%0Aboost%20accuracy%20even%20further%2C%20our%20cache%20learns%20to%20personalize%3A%20with%20the%0Amismatched%20and%20then%20offloaded%20inputs%2C%20it%20continuously%20finetunes%20the%20device%27s%0Afeature%20extractors%20with%20the%20assistance%20of%20the%20cloud.%20We%20implement%20SC%20on%20an%0Aoff-the-shelf%20STM32%20microcontroller.%20The%20complete%20implementation%20has%20a%20small%0Amemory%20footprint%20of%202MB.%20Evaluated%20on%20challenging%20speech%20benchmarks%2C%20our%20system%0Aresolves%2045%25-90%25%20of%20inputs%20on%20device%2C%20reducing%20the%20average%20latency%20by%20up%20to%2080%25%0Acompared%20to%20offloading%20to%20popular%20cloud%20speech%20recognition%20services.%20The%0Abenefit%20brought%20by%20our%20proposed%20SC%20is%20notable%20even%20in%20adversarial%20settings%20-%0Anoisy%20environments%2C%20cold%20cache%2C%20or%20one%20device%20shared%20by%20a%20number%20of%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18188v4&entry.124074799=Read"},
{"title": "Improved Generalization Bounds for Communication Efficient Federated\n  Learning", "author": "Peyman Gholami and Hulya Seferoglu", "abstract": "  This paper focuses on reducing the communication cost of federated learning\nby exploring generalization bounds and representation learning. We first\ncharacterize a tighter generalization bound for one-round federated learning\nbased on local clients' generalizations and heterogeneity of data distribution\n(non-iid scenario). We also characterize a generalization bound in R-round\nfederated learning and its relation to the number of local updates (local\nstochastic gradient descents (SGDs)). Then, based on our generalization bound\nanalysis and our representation learning interpretation of this analysis, we\nshow for the first time that less frequent aggregations, hence more local\nupdates, for the representation extractor (usually corresponds to initial\nlayers) leads to the creation of more generalizable models, particularly for\nnon-iid scenarios. We design a novel Federated Learning with Adaptive Local\nSteps (FedALS) algorithm based on our generalization bound and representation\nlearning analysis. FedALS employs varying aggregation frequencies for different\nparts of the model, so reduces the communication cost. The paper is followed\nwith experimental results showing the effectiveness of FedALS.\n", "link": "http://arxiv.org/abs/2404.11754v2", "date": "2024-05-08", "relevancy": 1.7539, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4507}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4379}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Generalization%20Bounds%20for%20Communication%20Efficient%20Federated%0A%20%20Learning&body=Title%3A%20Improved%20Generalization%20Bounds%20for%20Communication%20Efficient%20Federated%0A%20%20Learning%0AAuthor%3A%20Peyman%20Gholami%20and%20Hulya%20Seferoglu%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20reducing%20the%20communication%20cost%20of%20federated%20learning%0Aby%20exploring%20generalization%20bounds%20and%20representation%20learning.%20We%20first%0Acharacterize%20a%20tighter%20generalization%20bound%20for%20one-round%20federated%20learning%0Abased%20on%20local%20clients%27%20generalizations%20and%20heterogeneity%20of%20data%20distribution%0A%28non-iid%20scenario%29.%20We%20also%20characterize%20a%20generalization%20bound%20in%20R-round%0Afederated%20learning%20and%20its%20relation%20to%20the%20number%20of%20local%20updates%20%28local%0Astochastic%20gradient%20descents%20%28SGDs%29%29.%20Then%2C%20based%20on%20our%20generalization%20bound%0Aanalysis%20and%20our%20representation%20learning%20interpretation%20of%20this%20analysis%2C%20we%0Ashow%20for%20the%20first%20time%20that%20less%20frequent%20aggregations%2C%20hence%20more%20local%0Aupdates%2C%20for%20the%20representation%20extractor%20%28usually%20corresponds%20to%20initial%0Alayers%29%20leads%20to%20the%20creation%20of%20more%20generalizable%20models%2C%20particularly%20for%0Anon-iid%20scenarios.%20We%20design%20a%20novel%20Federated%20Learning%20with%20Adaptive%20Local%0ASteps%20%28FedALS%29%20algorithm%20based%20on%20our%20generalization%20bound%20and%20representation%0Alearning%20analysis.%20FedALS%20employs%20varying%20aggregation%20frequencies%20for%20different%0Aparts%20of%20the%20model%2C%20so%20reduces%20the%20communication%20cost.%20The%20paper%20is%20followed%0Awith%20experimental%20results%20showing%20the%20effectiveness%20of%20FedALS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Generalization%2520Bounds%2520for%2520Communication%2520Efficient%2520Federated%250A%2520%2520Learning%26entry.906535625%3DPeyman%2520Gholami%2520and%2520Hulya%2520Seferoglu%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520reducing%2520the%2520communication%2520cost%2520of%2520federated%2520learning%250Aby%2520exploring%2520generalization%2520bounds%2520and%2520representation%2520learning.%2520We%2520first%250Acharacterize%2520a%2520tighter%2520generalization%2520bound%2520for%2520one-round%2520federated%2520learning%250Abased%2520on%2520local%2520clients%2527%2520generalizations%2520and%2520heterogeneity%2520of%2520data%2520distribution%250A%2528non-iid%2520scenario%2529.%2520We%2520also%2520characterize%2520a%2520generalization%2520bound%2520in%2520R-round%250Afederated%2520learning%2520and%2520its%2520relation%2520to%2520the%2520number%2520of%2520local%2520updates%2520%2528local%250Astochastic%2520gradient%2520descents%2520%2528SGDs%2529%2529.%2520Then%252C%2520based%2520on%2520our%2520generalization%2520bound%250Aanalysis%2520and%2520our%2520representation%2520learning%2520interpretation%2520of%2520this%2520analysis%252C%2520we%250Ashow%2520for%2520the%2520first%2520time%2520that%2520less%2520frequent%2520aggregations%252C%2520hence%2520more%2520local%250Aupdates%252C%2520for%2520the%2520representation%2520extractor%2520%2528usually%2520corresponds%2520to%2520initial%250Alayers%2529%2520leads%2520to%2520the%2520creation%2520of%2520more%2520generalizable%2520models%252C%2520particularly%2520for%250Anon-iid%2520scenarios.%2520We%2520design%2520a%2520novel%2520Federated%2520Learning%2520with%2520Adaptive%2520Local%250ASteps%2520%2528FedALS%2529%2520algorithm%2520based%2520on%2520our%2520generalization%2520bound%2520and%2520representation%250Alearning%2520analysis.%2520FedALS%2520employs%2520varying%2520aggregation%2520frequencies%2520for%2520different%250Aparts%2520of%2520the%2520model%252C%2520so%2520reduces%2520the%2520communication%2520cost.%2520The%2520paper%2520is%2520followed%250Awith%2520experimental%2520results%2520showing%2520the%2520effectiveness%2520of%2520FedALS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Generalization%20Bounds%20for%20Communication%20Efficient%20Federated%0A%20%20Learning&entry.906535625=Peyman%20Gholami%20and%20Hulya%20Seferoglu&entry.1292438233=%20%20This%20paper%20focuses%20on%20reducing%20the%20communication%20cost%20of%20federated%20learning%0Aby%20exploring%20generalization%20bounds%20and%20representation%20learning.%20We%20first%0Acharacterize%20a%20tighter%20generalization%20bound%20for%20one-round%20federated%20learning%0Abased%20on%20local%20clients%27%20generalizations%20and%20heterogeneity%20of%20data%20distribution%0A%28non-iid%20scenario%29.%20We%20also%20characterize%20a%20generalization%20bound%20in%20R-round%0Afederated%20learning%20and%20its%20relation%20to%20the%20number%20of%20local%20updates%20%28local%0Astochastic%20gradient%20descents%20%28SGDs%29%29.%20Then%2C%20based%20on%20our%20generalization%20bound%0Aanalysis%20and%20our%20representation%20learning%20interpretation%20of%20this%20analysis%2C%20we%0Ashow%20for%20the%20first%20time%20that%20less%20frequent%20aggregations%2C%20hence%20more%20local%0Aupdates%2C%20for%20the%20representation%20extractor%20%28usually%20corresponds%20to%20initial%0Alayers%29%20leads%20to%20the%20creation%20of%20more%20generalizable%20models%2C%20particularly%20for%0Anon-iid%20scenarios.%20We%20design%20a%20novel%20Federated%20Learning%20with%20Adaptive%20Local%0ASteps%20%28FedALS%29%20algorithm%20based%20on%20our%20generalization%20bound%20and%20representation%0Alearning%20analysis.%20FedALS%20employs%20varying%20aggregation%20frequencies%20for%20different%0Aparts%20of%20the%20model%2C%20so%20reduces%20the%20communication%20cost.%20The%20paper%20is%20followed%0Awith%20experimental%20results%20showing%20the%20effectiveness%20of%20FedALS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11754v2&entry.124074799=Read"},
{"title": "SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge\n  Evaluation Plan", "author": "You Zhang and Yongyi Zang and Jiatong Shi and Ryuichi Yamamoto and Jionghao Han and Yuxun Tang and Tomoki Toda and Zhiyao Duan", "abstract": "  The rapid advancement of AI-generated singing voices, which now closely mimic\nnatural human singing and align seamlessly with musical scores, has led to\nheightened concerns for artists and the music industry. Unlike spoken voice,\nsinging voice presents unique challenges due to its musical nature and the\npresence of strong background music, making singing voice deepfake detection\n(SVDD) a specialized field requiring focused attention. To promote SVDD\nresearch, we recently proposed the \"SVDD Challenge,\" the very first research\nchallenge focusing on SVDD for lab-controlled and in-the-wild bonafide and\ndeepfake singing voice recordings. The challenge will be held in conjunction\nwith the 2024 IEEE Spoken Language Technology Workshop (SLT 2024).\n", "link": "http://arxiv.org/abs/2405.05244v1", "date": "2024-05-08", "relevancy": 1.7218, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4297}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVDD%20Challenge%202024%3A%20A%20Singing%20Voice%20Deepfake%20Detection%20Challenge%0A%20%20Evaluation%20Plan&body=Title%3A%20SVDD%20Challenge%202024%3A%20A%20Singing%20Voice%20Deepfake%20Detection%20Challenge%0A%20%20Evaluation%20Plan%0AAuthor%3A%20You%20Zhang%20and%20Yongyi%20Zang%20and%20Jiatong%20Shi%20and%20Ryuichi%20Yamamoto%20and%20Jionghao%20Han%20and%20Yuxun%20Tang%20and%20Tomoki%20Toda%20and%20Zhiyao%20Duan%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20AI-generated%20singing%20voices%2C%20which%20now%20closely%20mimic%0Anatural%20human%20singing%20and%20align%20seamlessly%20with%20musical%20scores%2C%20has%20led%20to%0Aheightened%20concerns%20for%20artists%20and%20the%20music%20industry.%20Unlike%20spoken%20voice%2C%0Asinging%20voice%20presents%20unique%20challenges%20due%20to%20its%20musical%20nature%20and%20the%0Apresence%20of%20strong%20background%20music%2C%20making%20singing%20voice%20deepfake%20detection%0A%28SVDD%29%20a%20specialized%20field%20requiring%20focused%20attention.%20To%20promote%20SVDD%0Aresearch%2C%20we%20recently%20proposed%20the%20%22SVDD%20Challenge%2C%22%20the%20very%20first%20research%0Achallenge%20focusing%20on%20SVDD%20for%20lab-controlled%20and%20in-the-wild%20bonafide%20and%0Adeepfake%20singing%20voice%20recordings.%20The%20challenge%20will%20be%20held%20in%20conjunction%0Awith%20the%202024%20IEEE%20Spoken%20Language%20Technology%20Workshop%20%28SLT%202024%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVDD%2520Challenge%25202024%253A%2520A%2520Singing%2520Voice%2520Deepfake%2520Detection%2520Challenge%250A%2520%2520Evaluation%2520Plan%26entry.906535625%3DYou%2520Zhang%2520and%2520Yongyi%2520Zang%2520and%2520Jiatong%2520Shi%2520and%2520Ryuichi%2520Yamamoto%2520and%2520Jionghao%2520Han%2520and%2520Yuxun%2520Tang%2520and%2520Tomoki%2520Toda%2520and%2520Zhiyao%2520Duan%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520AI-generated%2520singing%2520voices%252C%2520which%2520now%2520closely%2520mimic%250Anatural%2520human%2520singing%2520and%2520align%2520seamlessly%2520with%2520musical%2520scores%252C%2520has%2520led%2520to%250Aheightened%2520concerns%2520for%2520artists%2520and%2520the%2520music%2520industry.%2520Unlike%2520spoken%2520voice%252C%250Asinging%2520voice%2520presents%2520unique%2520challenges%2520due%2520to%2520its%2520musical%2520nature%2520and%2520the%250Apresence%2520of%2520strong%2520background%2520music%252C%2520making%2520singing%2520voice%2520deepfake%2520detection%250A%2528SVDD%2529%2520a%2520specialized%2520field%2520requiring%2520focused%2520attention.%2520To%2520promote%2520SVDD%250Aresearch%252C%2520we%2520recently%2520proposed%2520the%2520%2522SVDD%2520Challenge%252C%2522%2520the%2520very%2520first%2520research%250Achallenge%2520focusing%2520on%2520SVDD%2520for%2520lab-controlled%2520and%2520in-the-wild%2520bonafide%2520and%250Adeepfake%2520singing%2520voice%2520recordings.%2520The%2520challenge%2520will%2520be%2520held%2520in%2520conjunction%250Awith%2520the%25202024%2520IEEE%2520Spoken%2520Language%2520Technology%2520Workshop%2520%2528SLT%25202024%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVDD%20Challenge%202024%3A%20A%20Singing%20Voice%20Deepfake%20Detection%20Challenge%0A%20%20Evaluation%20Plan&entry.906535625=You%20Zhang%20and%20Yongyi%20Zang%20and%20Jiatong%20Shi%20and%20Ryuichi%20Yamamoto%20and%20Jionghao%20Han%20and%20Yuxun%20Tang%20and%20Tomoki%20Toda%20and%20Zhiyao%20Duan&entry.1292438233=%20%20The%20rapid%20advancement%20of%20AI-generated%20singing%20voices%2C%20which%20now%20closely%20mimic%0Anatural%20human%20singing%20and%20align%20seamlessly%20with%20musical%20scores%2C%20has%20led%20to%0Aheightened%20concerns%20for%20artists%20and%20the%20music%20industry.%20Unlike%20spoken%20voice%2C%0Asinging%20voice%20presents%20unique%20challenges%20due%20to%20its%20musical%20nature%20and%20the%0Apresence%20of%20strong%20background%20music%2C%20making%20singing%20voice%20deepfake%20detection%0A%28SVDD%29%20a%20specialized%20field%20requiring%20focused%20attention.%20To%20promote%20SVDD%0Aresearch%2C%20we%20recently%20proposed%20the%20%22SVDD%20Challenge%2C%22%20the%20very%20first%20research%0Achallenge%20focusing%20on%20SVDD%20for%20lab-controlled%20and%20in-the-wild%20bonafide%20and%0Adeepfake%20singing%20voice%20recordings.%20The%20challenge%20will%20be%20held%20in%20conjunction%0Awith%20the%202024%20IEEE%20Spoken%20Language%20Technology%20Workshop%20%28SLT%202024%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05244v1&entry.124074799=Read"},
{"title": "A View on Out-of-Distribution Identification from a Statistical Testing\n  Theory Perspective", "author": "Alberto Caron and Chris Hicks and Vasilios Mavroudis", "abstract": "  We study the problem of efficiently detecting Out-of-Distribution (OOD)\nsamples at test time in supervised and unsupervised learning contexts. While ML\nmodels are typically trained under the assumption that training and test data\nstem from the same distribution, this is often not the case in realistic\nsettings, thus reliably detecting distribution shifts is crucial at deployment.\nWe re-formulate the OOD problem under the lenses of statistical testing and\nthen discuss conditions that render the OOD problem identifiable in statistical\nterms. Building on this framework, we study convergence guarantees of an OOD\ntest based on the Wasserstein distance, and provide a simple empirical\nevaluation.\n", "link": "http://arxiv.org/abs/2405.03052v2", "date": "2024-05-08", "relevancy": 1.7207, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4451}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4294}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20View%20on%20Out-of-Distribution%20Identification%20from%20a%20Statistical%20Testing%0A%20%20Theory%20Perspective&body=Title%3A%20A%20View%20on%20Out-of-Distribution%20Identification%20from%20a%20Statistical%20Testing%0A%20%20Theory%20Perspective%0AAuthor%3A%20Alberto%20Caron%20and%20Chris%20Hicks%20and%20Vasilios%20Mavroudis%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20efficiently%20detecting%20Out-of-Distribution%20%28OOD%29%0Asamples%20at%20test%20time%20in%20supervised%20and%20unsupervised%20learning%20contexts.%20While%20ML%0Amodels%20are%20typically%20trained%20under%20the%20assumption%20that%20training%20and%20test%20data%0Astem%20from%20the%20same%20distribution%2C%20this%20is%20often%20not%20the%20case%20in%20realistic%0Asettings%2C%20thus%20reliably%20detecting%20distribution%20shifts%20is%20crucial%20at%20deployment.%0AWe%20re-formulate%20the%20OOD%20problem%20under%20the%20lenses%20of%20statistical%20testing%20and%0Athen%20discuss%20conditions%20that%20render%20the%20OOD%20problem%20identifiable%20in%20statistical%0Aterms.%20Building%20on%20this%20framework%2C%20we%20study%20convergence%20guarantees%20of%20an%20OOD%0Atest%20based%20on%20the%20Wasserstein%20distance%2C%20and%20provide%20a%20simple%20empirical%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520View%2520on%2520Out-of-Distribution%2520Identification%2520from%2520a%2520Statistical%2520Testing%250A%2520%2520Theory%2520Perspective%26entry.906535625%3DAlberto%2520Caron%2520and%2520Chris%2520Hicks%2520and%2520Vasilios%2520Mavroudis%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520efficiently%2520detecting%2520Out-of-Distribution%2520%2528OOD%2529%250Asamples%2520at%2520test%2520time%2520in%2520supervised%2520and%2520unsupervised%2520learning%2520contexts.%2520While%2520ML%250Amodels%2520are%2520typically%2520trained%2520under%2520the%2520assumption%2520that%2520training%2520and%2520test%2520data%250Astem%2520from%2520the%2520same%2520distribution%252C%2520this%2520is%2520often%2520not%2520the%2520case%2520in%2520realistic%250Asettings%252C%2520thus%2520reliably%2520detecting%2520distribution%2520shifts%2520is%2520crucial%2520at%2520deployment.%250AWe%2520re-formulate%2520the%2520OOD%2520problem%2520under%2520the%2520lenses%2520of%2520statistical%2520testing%2520and%250Athen%2520discuss%2520conditions%2520that%2520render%2520the%2520OOD%2520problem%2520identifiable%2520in%2520statistical%250Aterms.%2520Building%2520on%2520this%2520framework%252C%2520we%2520study%2520convergence%2520guarantees%2520of%2520an%2520OOD%250Atest%2520based%2520on%2520the%2520Wasserstein%2520distance%252C%2520and%2520provide%2520a%2520simple%2520empirical%250Aevaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20View%20on%20Out-of-Distribution%20Identification%20from%20a%20Statistical%20Testing%0A%20%20Theory%20Perspective&entry.906535625=Alberto%20Caron%20and%20Chris%20Hicks%20and%20Vasilios%20Mavroudis&entry.1292438233=%20%20We%20study%20the%20problem%20of%20efficiently%20detecting%20Out-of-Distribution%20%28OOD%29%0Asamples%20at%20test%20time%20in%20supervised%20and%20unsupervised%20learning%20contexts.%20While%20ML%0Amodels%20are%20typically%20trained%20under%20the%20assumption%20that%20training%20and%20test%20data%0Astem%20from%20the%20same%20distribution%2C%20this%20is%20often%20not%20the%20case%20in%20realistic%0Asettings%2C%20thus%20reliably%20detecting%20distribution%20shifts%20is%20crucial%20at%20deployment.%0AWe%20re-formulate%20the%20OOD%20problem%20under%20the%20lenses%20of%20statistical%20testing%20and%0Athen%20discuss%20conditions%20that%20render%20the%20OOD%20problem%20identifiable%20in%20statistical%0Aterms.%20Building%20on%20this%20framework%2C%20we%20study%20convergence%20guarantees%20of%20an%20OOD%0Atest%20based%20on%20the%20Wasserstein%20distance%2C%20and%20provide%20a%20simple%20empirical%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03052v2&entry.124074799=Read"},
{"title": "Leafy Spurge Dataset: Real-world Weed Classification Within Aerial Drone\n  Imagery", "author": "Kyle Doherty and Max Gurinas and Erik Samsoe and Charles Casper and Beau Larkin and Philip Ramsey and Brandon Trabucco and Ruslan Salakhutdinov", "abstract": "  Invasive plant species are detrimental to the ecology of both agricultural\nand wildland areas. Euphorbia esula, or leafy spurge, is one such plant that\nhas spread through much of North America from Eastern Europe. When paired with\ncontemporary computer vision systems, unmanned aerial vehicles, or drones,\noffer the means to track expansion of problem plants, such as leafy spurge, and\nimprove chances of controlling these weeds. We gathered a dataset of leafy\nspurge presence and absence in grasslands of western Montana, USA, then\nsurveyed these areas with a commercial drone. We trained image classifiers on\nthese data, and our best performing model, a pre-trained DINOv2 vision\ntransformer, identified leafy spurge with 0.84 accuracy (test set). This result\nindicates that classification of leafy spurge is tractable, but not solved. We\nrelease this unique dataset of labelled and unlabelled, aerial drone imagery\nfor the machine learning community to explore. Improving classification\nperformance of leafy spurge would benefit the fields of ecology, conservation,\nand remote sensing alike. Code and data are available at our website:\nleafy-spurge-dataset.github.io.\n", "link": "http://arxiv.org/abs/2405.03702v2", "date": "2024-05-08", "relevancy": 1.7133, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4455}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4216}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leafy%20Spurge%20Dataset%3A%20Real-world%20Weed%20Classification%20Within%20Aerial%20Drone%0A%20%20Imagery&body=Title%3A%20Leafy%20Spurge%20Dataset%3A%20Real-world%20Weed%20Classification%20Within%20Aerial%20Drone%0A%20%20Imagery%0AAuthor%3A%20Kyle%20Doherty%20and%20Max%20Gurinas%20and%20Erik%20Samsoe%20and%20Charles%20Casper%20and%20Beau%20Larkin%20and%20Philip%20Ramsey%20and%20Brandon%20Trabucco%20and%20Ruslan%20Salakhutdinov%0AAbstract%3A%20%20%20Invasive%20plant%20species%20are%20detrimental%20to%20the%20ecology%20of%20both%20agricultural%0Aand%20wildland%20areas.%20Euphorbia%20esula%2C%20or%20leafy%20spurge%2C%20is%20one%20such%20plant%20that%0Ahas%20spread%20through%20much%20of%20North%20America%20from%20Eastern%20Europe.%20When%20paired%20with%0Acontemporary%20computer%20vision%20systems%2C%20unmanned%20aerial%20vehicles%2C%20or%20drones%2C%0Aoffer%20the%20means%20to%20track%20expansion%20of%20problem%20plants%2C%20such%20as%20leafy%20spurge%2C%20and%0Aimprove%20chances%20of%20controlling%20these%20weeds.%20We%20gathered%20a%20dataset%20of%20leafy%0Aspurge%20presence%20and%20absence%20in%20grasslands%20of%20western%20Montana%2C%20USA%2C%20then%0Asurveyed%20these%20areas%20with%20a%20commercial%20drone.%20We%20trained%20image%20classifiers%20on%0Athese%20data%2C%20and%20our%20best%20performing%20model%2C%20a%20pre-trained%20DINOv2%20vision%0Atransformer%2C%20identified%20leafy%20spurge%20with%200.84%20accuracy%20%28test%20set%29.%20This%20result%0Aindicates%20that%20classification%20of%20leafy%20spurge%20is%20tractable%2C%20but%20not%20solved.%20We%0Arelease%20this%20unique%20dataset%20of%20labelled%20and%20unlabelled%2C%20aerial%20drone%20imagery%0Afor%20the%20machine%20learning%20community%20to%20explore.%20Improving%20classification%0Aperformance%20of%20leafy%20spurge%20would%20benefit%20the%20fields%20of%20ecology%2C%20conservation%2C%0Aand%20remote%20sensing%20alike.%20Code%20and%20data%20are%20available%20at%20our%20website%3A%0Aleafy-spurge-dataset.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03702v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeafy%2520Spurge%2520Dataset%253A%2520Real-world%2520Weed%2520Classification%2520Within%2520Aerial%2520Drone%250A%2520%2520Imagery%26entry.906535625%3DKyle%2520Doherty%2520and%2520Max%2520Gurinas%2520and%2520Erik%2520Samsoe%2520and%2520Charles%2520Casper%2520and%2520Beau%2520Larkin%2520and%2520Philip%2520Ramsey%2520and%2520Brandon%2520Trabucco%2520and%2520Ruslan%2520Salakhutdinov%26entry.1292438233%3D%2520%2520Invasive%2520plant%2520species%2520are%2520detrimental%2520to%2520the%2520ecology%2520of%2520both%2520agricultural%250Aand%2520wildland%2520areas.%2520Euphorbia%2520esula%252C%2520or%2520leafy%2520spurge%252C%2520is%2520one%2520such%2520plant%2520that%250Ahas%2520spread%2520through%2520much%2520of%2520North%2520America%2520from%2520Eastern%2520Europe.%2520When%2520paired%2520with%250Acontemporary%2520computer%2520vision%2520systems%252C%2520unmanned%2520aerial%2520vehicles%252C%2520or%2520drones%252C%250Aoffer%2520the%2520means%2520to%2520track%2520expansion%2520of%2520problem%2520plants%252C%2520such%2520as%2520leafy%2520spurge%252C%2520and%250Aimprove%2520chances%2520of%2520controlling%2520these%2520weeds.%2520We%2520gathered%2520a%2520dataset%2520of%2520leafy%250Aspurge%2520presence%2520and%2520absence%2520in%2520grasslands%2520of%2520western%2520Montana%252C%2520USA%252C%2520then%250Asurveyed%2520these%2520areas%2520with%2520a%2520commercial%2520drone.%2520We%2520trained%2520image%2520classifiers%2520on%250Athese%2520data%252C%2520and%2520our%2520best%2520performing%2520model%252C%2520a%2520pre-trained%2520DINOv2%2520vision%250Atransformer%252C%2520identified%2520leafy%2520spurge%2520with%25200.84%2520accuracy%2520%2528test%2520set%2529.%2520This%2520result%250Aindicates%2520that%2520classification%2520of%2520leafy%2520spurge%2520is%2520tractable%252C%2520but%2520not%2520solved.%2520We%250Arelease%2520this%2520unique%2520dataset%2520of%2520labelled%2520and%2520unlabelled%252C%2520aerial%2520drone%2520imagery%250Afor%2520the%2520machine%2520learning%2520community%2520to%2520explore.%2520Improving%2520classification%250Aperformance%2520of%2520leafy%2520spurge%2520would%2520benefit%2520the%2520fields%2520of%2520ecology%252C%2520conservation%252C%250Aand%2520remote%2520sensing%2520alike.%2520Code%2520and%2520data%2520are%2520available%2520at%2520our%2520website%253A%250Aleafy-spurge-dataset.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03702v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leafy%20Spurge%20Dataset%3A%20Real-world%20Weed%20Classification%20Within%20Aerial%20Drone%0A%20%20Imagery&entry.906535625=Kyle%20Doherty%20and%20Max%20Gurinas%20and%20Erik%20Samsoe%20and%20Charles%20Casper%20and%20Beau%20Larkin%20and%20Philip%20Ramsey%20and%20Brandon%20Trabucco%20and%20Ruslan%20Salakhutdinov&entry.1292438233=%20%20Invasive%20plant%20species%20are%20detrimental%20to%20the%20ecology%20of%20both%20agricultural%0Aand%20wildland%20areas.%20Euphorbia%20esula%2C%20or%20leafy%20spurge%2C%20is%20one%20such%20plant%20that%0Ahas%20spread%20through%20much%20of%20North%20America%20from%20Eastern%20Europe.%20When%20paired%20with%0Acontemporary%20computer%20vision%20systems%2C%20unmanned%20aerial%20vehicles%2C%20or%20drones%2C%0Aoffer%20the%20means%20to%20track%20expansion%20of%20problem%20plants%2C%20such%20as%20leafy%20spurge%2C%20and%0Aimprove%20chances%20of%20controlling%20these%20weeds.%20We%20gathered%20a%20dataset%20of%20leafy%0Aspurge%20presence%20and%20absence%20in%20grasslands%20of%20western%20Montana%2C%20USA%2C%20then%0Asurveyed%20these%20areas%20with%20a%20commercial%20drone.%20We%20trained%20image%20classifiers%20on%0Athese%20data%2C%20and%20our%20best%20performing%20model%2C%20a%20pre-trained%20DINOv2%20vision%0Atransformer%2C%20identified%20leafy%20spurge%20with%200.84%20accuracy%20%28test%20set%29.%20This%20result%0Aindicates%20that%20classification%20of%20leafy%20spurge%20is%20tractable%2C%20but%20not%20solved.%20We%0Arelease%20this%20unique%20dataset%20of%20labelled%20and%20unlabelled%2C%20aerial%20drone%20imagery%0Afor%20the%20machine%20learning%20community%20to%20explore.%20Improving%20classification%0Aperformance%20of%20leafy%20spurge%20would%20benefit%20the%20fields%20of%20ecology%2C%20conservation%2C%0Aand%20remote%20sensing%20alike.%20Code%20and%20data%20are%20available%20at%20our%20website%3A%0Aleafy-spurge-dataset.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03702v2&entry.124074799=Read"},
{"title": "QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs", "author": "Weijia Zhang and Vaishali Pal and Jia-Hong Huang and Evangelos Kanoulas and Maarten de Rijke", "abstract": "  Table summarization is a crucial task aimed at condensing information from\ntabular data into concise and comprehensible textual summaries. However,\nexisting approaches often fall short of adequately meeting users' information\nand quality requirements and tend to overlook the complexities of real-world\nqueries. In this paper, we propose a novel method to address these limitations\nby introducing query-focused multi-table summarization. Our approach, which\ncomprises a table serialization module, a summarization controller, and a large\nlanguage model (LLM), utilizes textual queries and multiple tables to generate\nquery-dependent table summaries tailored to users' information needs. To\nfacilitate research in this area, we present a comprehensive dataset\nspecifically tailored for this task, consisting of 4909 query-summary pairs,\neach associated with multiple tables. Through extensive experiments using our\ncurated dataset, we demonstrate the effectiveness of our proposed method\ncompared to baseline approaches. Our findings offer insights into the\nchallenges of complex table reasoning for precise summarization, contributing\nto the advancement of research in query-focused multi-table summarization.\n", "link": "http://arxiv.org/abs/2405.05109v1", "date": "2024-05-08", "relevancy": 1.6908, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4363}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4231}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QFMTS%3A%20Generating%20Query-Focused%20Summaries%20over%20Multi-Table%20Inputs&body=Title%3A%20QFMTS%3A%20Generating%20Query-Focused%20Summaries%20over%20Multi-Table%20Inputs%0AAuthor%3A%20Weijia%20Zhang%20and%20Vaishali%20Pal%20and%20Jia-Hong%20Huang%20and%20Evangelos%20Kanoulas%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20%20%20Table%20summarization%20is%20a%20crucial%20task%20aimed%20at%20condensing%20information%20from%0Atabular%20data%20into%20concise%20and%20comprehensible%20textual%20summaries.%20However%2C%0Aexisting%20approaches%20often%20fall%20short%20of%20adequately%20meeting%20users%27%20information%0Aand%20quality%20requirements%20and%20tend%20to%20overlook%20the%20complexities%20of%20real-world%0Aqueries.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20to%20address%20these%20limitations%0Aby%20introducing%20query-focused%20multi-table%20summarization.%20Our%20approach%2C%20which%0Acomprises%20a%20table%20serialization%20module%2C%20a%20summarization%20controller%2C%20and%20a%20large%0Alanguage%20model%20%28LLM%29%2C%20utilizes%20textual%20queries%20and%20multiple%20tables%20to%20generate%0Aquery-dependent%20table%20summaries%20tailored%20to%20users%27%20information%20needs.%20To%0Afacilitate%20research%20in%20this%20area%2C%20we%20present%20a%20comprehensive%20dataset%0Aspecifically%20tailored%20for%20this%20task%2C%20consisting%20of%204909%20query-summary%20pairs%2C%0Aeach%20associated%20with%20multiple%20tables.%20Through%20extensive%20experiments%20using%20our%0Acurated%20dataset%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%0Acompared%20to%20baseline%20approaches.%20Our%20findings%20offer%20insights%20into%20the%0Achallenges%20of%20complex%20table%20reasoning%20for%20precise%20summarization%2C%20contributing%0Ato%20the%20advancement%20of%20research%20in%20query-focused%20multi-table%20summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQFMTS%253A%2520Generating%2520Query-Focused%2520Summaries%2520over%2520Multi-Table%2520Inputs%26entry.906535625%3DWeijia%2520Zhang%2520and%2520Vaishali%2520Pal%2520and%2520Jia-Hong%2520Huang%2520and%2520Evangelos%2520Kanoulas%2520and%2520Maarten%2520de%2520Rijke%26entry.1292438233%3D%2520%2520Table%2520summarization%2520is%2520a%2520crucial%2520task%2520aimed%2520at%2520condensing%2520information%2520from%250Atabular%2520data%2520into%2520concise%2520and%2520comprehensible%2520textual%2520summaries.%2520However%252C%250Aexisting%2520approaches%2520often%2520fall%2520short%2520of%2520adequately%2520meeting%2520users%2527%2520information%250Aand%2520quality%2520requirements%2520and%2520tend%2520to%2520overlook%2520the%2520complexities%2520of%2520real-world%250Aqueries.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520to%2520address%2520these%2520limitations%250Aby%2520introducing%2520query-focused%2520multi-table%2520summarization.%2520Our%2520approach%252C%2520which%250Acomprises%2520a%2520table%2520serialization%2520module%252C%2520a%2520summarization%2520controller%252C%2520and%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%252C%2520utilizes%2520textual%2520queries%2520and%2520multiple%2520tables%2520to%2520generate%250Aquery-dependent%2520table%2520summaries%2520tailored%2520to%2520users%2527%2520information%2520needs.%2520To%250Afacilitate%2520research%2520in%2520this%2520area%252C%2520we%2520present%2520a%2520comprehensive%2520dataset%250Aspecifically%2520tailored%2520for%2520this%2520task%252C%2520consisting%2520of%25204909%2520query-summary%2520pairs%252C%250Aeach%2520associated%2520with%2520multiple%2520tables.%2520Through%2520extensive%2520experiments%2520using%2520our%250Acurated%2520dataset%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%250Acompared%2520to%2520baseline%2520approaches.%2520Our%2520findings%2520offer%2520insights%2520into%2520the%250Achallenges%2520of%2520complex%2520table%2520reasoning%2520for%2520precise%2520summarization%252C%2520contributing%250Ato%2520the%2520advancement%2520of%2520research%2520in%2520query-focused%2520multi-table%2520summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QFMTS%3A%20Generating%20Query-Focused%20Summaries%20over%20Multi-Table%20Inputs&entry.906535625=Weijia%20Zhang%20and%20Vaishali%20Pal%20and%20Jia-Hong%20Huang%20and%20Evangelos%20Kanoulas%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20Table%20summarization%20is%20a%20crucial%20task%20aimed%20at%20condensing%20information%20from%0Atabular%20data%20into%20concise%20and%20comprehensible%20textual%20summaries.%20However%2C%0Aexisting%20approaches%20often%20fall%20short%20of%20adequately%20meeting%20users%27%20information%0Aand%20quality%20requirements%20and%20tend%20to%20overlook%20the%20complexities%20of%20real-world%0Aqueries.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20to%20address%20these%20limitations%0Aby%20introducing%20query-focused%20multi-table%20summarization.%20Our%20approach%2C%20which%0Acomprises%20a%20table%20serialization%20module%2C%20a%20summarization%20controller%2C%20and%20a%20large%0Alanguage%20model%20%28LLM%29%2C%20utilizes%20textual%20queries%20and%20multiple%20tables%20to%20generate%0Aquery-dependent%20table%20summaries%20tailored%20to%20users%27%20information%20needs.%20To%0Afacilitate%20research%20in%20this%20area%2C%20we%20present%20a%20comprehensive%20dataset%0Aspecifically%20tailored%20for%20this%20task%2C%20consisting%20of%204909%20query-summary%20pairs%2C%0Aeach%20associated%20with%20multiple%20tables.%20Through%20extensive%20experiments%20using%20our%0Acurated%20dataset%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%0Acompared%20to%20baseline%20approaches.%20Our%20findings%20offer%20insights%20into%20the%0Achallenges%20of%20complex%20table%20reasoning%20for%20precise%20summarization%2C%20contributing%0Ato%20the%20advancement%20of%20research%20in%20query-focused%20multi-table%20summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05109v1&entry.124074799=Read"},
{"title": "StepMix: A Python Package for Pseudo-Likelihood Estimation of\n  Generalized Mixture Models with External Variables", "author": "Sacha Morin and Robin Legault and F\u00e9lix Lalibert\u00e9 and Zsuzsa Bakk and Charles-\u00c9douard Gigu\u00e8re and Roxane de la Sablonni\u00e8re and \u00c9ric Lacourse", "abstract": "  StepMix is an open-source Python package for the pseudo-likelihood estimation\n(one-, two- and three-step approaches) of generalized finite mixture models\n(latent profile and latent class analysis) with external variables (covariates\nand distal outcomes). In many applications in social sciences, the main\nobjective is not only to cluster individuals into latent classes, but also to\nuse these classes to develop more complex statistical models. These models\ngenerally divide into a measurement model that relates the latent classes to\nobserved indicators, and a structural model that relates covariates and outcome\nvariables to the latent classes. The measurement and structural models can be\nestimated jointly using the so-called one-step approach or sequentially using\nstepwise methods, which present significant advantages for practitioners\nregarding the interpretability of the estimated latent classes. In addition to\nthe one-step approach, StepMix implements the most important stepwise\nestimation methods from the literature, including the bias-adjusted three-step\nmethods with Bolk-Croon-Hagenaars and maximum likelihood corrections and the\nmore recent two-step approach. These pseudo-likelihood estimators are presented\nin this paper under a unified framework as specific expectation-maximization\nsubroutines. To facilitate and promote their adoption among the data science\ncommunity, StepMix follows the object-oriented design of the scikit-learn\nlibrary and provides an additional R wrapper.\n", "link": "http://arxiv.org/abs/2304.03853v5", "date": "2024-05-08", "relevancy": 1.6878, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4459}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4215}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StepMix%3A%20A%20Python%20Package%20for%20Pseudo-Likelihood%20Estimation%20of%0A%20%20Generalized%20Mixture%20Models%20with%20External%20Variables&body=Title%3A%20StepMix%3A%20A%20Python%20Package%20for%20Pseudo-Likelihood%20Estimation%20of%0A%20%20Generalized%20Mixture%20Models%20with%20External%20Variables%0AAuthor%3A%20Sacha%20Morin%20and%20Robin%20Legault%20and%20F%C3%A9lix%20Lalibert%C3%A9%20and%20Zsuzsa%20Bakk%20and%20Charles-%C3%89douard%20Gigu%C3%A8re%20and%20Roxane%20de%20la%20Sablonni%C3%A8re%20and%20%C3%89ric%20Lacourse%0AAbstract%3A%20%20%20StepMix%20is%20an%20open-source%20Python%20package%20for%20the%20pseudo-likelihood%20estimation%0A%28one-%2C%20two-%20and%20three-step%20approaches%29%20of%20generalized%20finite%20mixture%20models%0A%28latent%20profile%20and%20latent%20class%20analysis%29%20with%20external%20variables%20%28covariates%0Aand%20distal%20outcomes%29.%20In%20many%20applications%20in%20social%20sciences%2C%20the%20main%0Aobjective%20is%20not%20only%20to%20cluster%20individuals%20into%20latent%20classes%2C%20but%20also%20to%0Ause%20these%20classes%20to%20develop%20more%20complex%20statistical%20models.%20These%20models%0Agenerally%20divide%20into%20a%20measurement%20model%20that%20relates%20the%20latent%20classes%20to%0Aobserved%20indicators%2C%20and%20a%20structural%20model%20that%20relates%20covariates%20and%20outcome%0Avariables%20to%20the%20latent%20classes.%20The%20measurement%20and%20structural%20models%20can%20be%0Aestimated%20jointly%20using%20the%20so-called%20one-step%20approach%20or%20sequentially%20using%0Astepwise%20methods%2C%20which%20present%20significant%20advantages%20for%20practitioners%0Aregarding%20the%20interpretability%20of%20the%20estimated%20latent%20classes.%20In%20addition%20to%0Athe%20one-step%20approach%2C%20StepMix%20implements%20the%20most%20important%20stepwise%0Aestimation%20methods%20from%20the%20literature%2C%20including%20the%20bias-adjusted%20three-step%0Amethods%20with%20Bolk-Croon-Hagenaars%20and%20maximum%20likelihood%20corrections%20and%20the%0Amore%20recent%20two-step%20approach.%20These%20pseudo-likelihood%20estimators%20are%20presented%0Ain%20this%20paper%20under%20a%20unified%20framework%20as%20specific%20expectation-maximization%0Asubroutines.%20To%20facilitate%20and%20promote%20their%20adoption%20among%20the%20data%20science%0Acommunity%2C%20StepMix%20follows%20the%20object-oriented%20design%20of%20the%20scikit-learn%0Alibrary%20and%20provides%20an%20additional%20R%20wrapper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03853v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepMix%253A%2520A%2520Python%2520Package%2520for%2520Pseudo-Likelihood%2520Estimation%2520of%250A%2520%2520Generalized%2520Mixture%2520Models%2520with%2520External%2520Variables%26entry.906535625%3DSacha%2520Morin%2520and%2520Robin%2520Legault%2520and%2520F%25C3%25A9lix%2520Lalibert%25C3%25A9%2520and%2520Zsuzsa%2520Bakk%2520and%2520Charles-%25C3%2589douard%2520Gigu%25C3%25A8re%2520and%2520Roxane%2520de%2520la%2520Sablonni%25C3%25A8re%2520and%2520%25C3%2589ric%2520Lacourse%26entry.1292438233%3D%2520%2520StepMix%2520is%2520an%2520open-source%2520Python%2520package%2520for%2520the%2520pseudo-likelihood%2520estimation%250A%2528one-%252C%2520two-%2520and%2520three-step%2520approaches%2529%2520of%2520generalized%2520finite%2520mixture%2520models%250A%2528latent%2520profile%2520and%2520latent%2520class%2520analysis%2529%2520with%2520external%2520variables%2520%2528covariates%250Aand%2520distal%2520outcomes%2529.%2520In%2520many%2520applications%2520in%2520social%2520sciences%252C%2520the%2520main%250Aobjective%2520is%2520not%2520only%2520to%2520cluster%2520individuals%2520into%2520latent%2520classes%252C%2520but%2520also%2520to%250Ause%2520these%2520classes%2520to%2520develop%2520more%2520complex%2520statistical%2520models.%2520These%2520models%250Agenerally%2520divide%2520into%2520a%2520measurement%2520model%2520that%2520relates%2520the%2520latent%2520classes%2520to%250Aobserved%2520indicators%252C%2520and%2520a%2520structural%2520model%2520that%2520relates%2520covariates%2520and%2520outcome%250Avariables%2520to%2520the%2520latent%2520classes.%2520The%2520measurement%2520and%2520structural%2520models%2520can%2520be%250Aestimated%2520jointly%2520using%2520the%2520so-called%2520one-step%2520approach%2520or%2520sequentially%2520using%250Astepwise%2520methods%252C%2520which%2520present%2520significant%2520advantages%2520for%2520practitioners%250Aregarding%2520the%2520interpretability%2520of%2520the%2520estimated%2520latent%2520classes.%2520In%2520addition%2520to%250Athe%2520one-step%2520approach%252C%2520StepMix%2520implements%2520the%2520most%2520important%2520stepwise%250Aestimation%2520methods%2520from%2520the%2520literature%252C%2520including%2520the%2520bias-adjusted%2520three-step%250Amethods%2520with%2520Bolk-Croon-Hagenaars%2520and%2520maximum%2520likelihood%2520corrections%2520and%2520the%250Amore%2520recent%2520two-step%2520approach.%2520These%2520pseudo-likelihood%2520estimators%2520are%2520presented%250Ain%2520this%2520paper%2520under%2520a%2520unified%2520framework%2520as%2520specific%2520expectation-maximization%250Asubroutines.%2520To%2520facilitate%2520and%2520promote%2520their%2520adoption%2520among%2520the%2520data%2520science%250Acommunity%252C%2520StepMix%2520follows%2520the%2520object-oriented%2520design%2520of%2520the%2520scikit-learn%250Alibrary%2520and%2520provides%2520an%2520additional%2520R%2520wrapper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.03853v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StepMix%3A%20A%20Python%20Package%20for%20Pseudo-Likelihood%20Estimation%20of%0A%20%20Generalized%20Mixture%20Models%20with%20External%20Variables&entry.906535625=Sacha%20Morin%20and%20Robin%20Legault%20and%20F%C3%A9lix%20Lalibert%C3%A9%20and%20Zsuzsa%20Bakk%20and%20Charles-%C3%89douard%20Gigu%C3%A8re%20and%20Roxane%20de%20la%20Sablonni%C3%A8re%20and%20%C3%89ric%20Lacourse&entry.1292438233=%20%20StepMix%20is%20an%20open-source%20Python%20package%20for%20the%20pseudo-likelihood%20estimation%0A%28one-%2C%20two-%20and%20three-step%20approaches%29%20of%20generalized%20finite%20mixture%20models%0A%28latent%20profile%20and%20latent%20class%20analysis%29%20with%20external%20variables%20%28covariates%0Aand%20distal%20outcomes%29.%20In%20many%20applications%20in%20social%20sciences%2C%20the%20main%0Aobjective%20is%20not%20only%20to%20cluster%20individuals%20into%20latent%20classes%2C%20but%20also%20to%0Ause%20these%20classes%20to%20develop%20more%20complex%20statistical%20models.%20These%20models%0Agenerally%20divide%20into%20a%20measurement%20model%20that%20relates%20the%20latent%20classes%20to%0Aobserved%20indicators%2C%20and%20a%20structural%20model%20that%20relates%20covariates%20and%20outcome%0Avariables%20to%20the%20latent%20classes.%20The%20measurement%20and%20structural%20models%20can%20be%0Aestimated%20jointly%20using%20the%20so-called%20one-step%20approach%20or%20sequentially%20using%0Astepwise%20methods%2C%20which%20present%20significant%20advantages%20for%20practitioners%0Aregarding%20the%20interpretability%20of%20the%20estimated%20latent%20classes.%20In%20addition%20to%0Athe%20one-step%20approach%2C%20StepMix%20implements%20the%20most%20important%20stepwise%0Aestimation%20methods%20from%20the%20literature%2C%20including%20the%20bias-adjusted%20three-step%0Amethods%20with%20Bolk-Croon-Hagenaars%20and%20maximum%20likelihood%20corrections%20and%20the%0Amore%20recent%20two-step%20approach.%20These%20pseudo-likelihood%20estimators%20are%20presented%0Ain%20this%20paper%20under%20a%20unified%20framework%20as%20specific%20expectation-maximization%0Asubroutines.%20To%20facilitate%20and%20promote%20their%20adoption%20among%20the%20data%20science%0Acommunity%2C%20StepMix%20follows%20the%20object-oriented%20design%20of%20the%20scikit-learn%0Alibrary%20and%20provides%20an%20additional%20R%20wrapper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03853v5&entry.124074799=Read"},
{"title": "LIO-EKF: High Frequency LiDAR-Inertial Odometry using Extended Kalman\n  Filters", "author": "Yibin Wu and Tiziano Guadagnino and Louis Wiesmann and Lasse Klingbeil and Cyrill Stachniss and Heiner Kuhlmann", "abstract": "  Odometry estimation is crucial for every autonomous system requiring\nnavigation in an unknown environment. In modern mobile robots, 3D\nLiDAR-inertial systems are often used for this task. By fusing LiDAR scans and\nIMU measurements, these systems can reduce the accumulated drift caused by\nsequentially registering individual LiDAR scans and provide a robust pose\nestimate. Although effective, LiDAR-inertial odometry systems require proper\nparameter tuning to be deployed. In this paper, we propose LIO-EKF, a\ntightly-coupled LiDAR-inertial odometry system based on point-to-point\nregistration and the classical extended Kalman filter scheme. We propose an\nadaptive data association that considers the relative pose uncertainty, the map\ndiscretization errors, and the LiDAR noise. In this way, we can substantially\nreduce the parameters to tune for a given type of environment. The experimental\nevaluation suggests that the proposed system performs on par with the\nstate-of-the-art LiDAR-inertial odometry pipelines but is significantly faster\nin computing the odometry. The source code of our implementation is publicly\navailable (https://github.com/YibinWu/LIO-EKF).\n", "link": "http://arxiv.org/abs/2311.09887v2", "date": "2024-05-08", "relevancy": 1.6826, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5725}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5493}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIO-EKF%3A%20High%20Frequency%20LiDAR-Inertial%20Odometry%20using%20Extended%20Kalman%0A%20%20Filters&body=Title%3A%20LIO-EKF%3A%20High%20Frequency%20LiDAR-Inertial%20Odometry%20using%20Extended%20Kalman%0A%20%20Filters%0AAuthor%3A%20Yibin%20Wu%20and%20Tiziano%20Guadagnino%20and%20Louis%20Wiesmann%20and%20Lasse%20Klingbeil%20and%20Cyrill%20Stachniss%20and%20Heiner%20Kuhlmann%0AAbstract%3A%20%20%20Odometry%20estimation%20is%20crucial%20for%20every%20autonomous%20system%20requiring%0Anavigation%20in%20an%20unknown%20environment.%20In%20modern%20mobile%20robots%2C%203D%0ALiDAR-inertial%20systems%20are%20often%20used%20for%20this%20task.%20By%20fusing%20LiDAR%20scans%20and%0AIMU%20measurements%2C%20these%20systems%20can%20reduce%20the%20accumulated%20drift%20caused%20by%0Asequentially%20registering%20individual%20LiDAR%20scans%20and%20provide%20a%20robust%20pose%0Aestimate.%20Although%20effective%2C%20LiDAR-inertial%20odometry%20systems%20require%20proper%0Aparameter%20tuning%20to%20be%20deployed.%20In%20this%20paper%2C%20we%20propose%20LIO-EKF%2C%20a%0Atightly-coupled%20LiDAR-inertial%20odometry%20system%20based%20on%20point-to-point%0Aregistration%20and%20the%20classical%20extended%20Kalman%20filter%20scheme.%20We%20propose%20an%0Aadaptive%20data%20association%20that%20considers%20the%20relative%20pose%20uncertainty%2C%20the%20map%0Adiscretization%20errors%2C%20and%20the%20LiDAR%20noise.%20In%20this%20way%2C%20we%20can%20substantially%0Areduce%20the%20parameters%20to%20tune%20for%20a%20given%20type%20of%20environment.%20The%20experimental%0Aevaluation%20suggests%20that%20the%20proposed%20system%20performs%20on%20par%20with%20the%0Astate-of-the-art%20LiDAR-inertial%20odometry%20pipelines%20but%20is%20significantly%20faster%0Ain%20computing%20the%20odometry.%20The%20source%20code%20of%20our%20implementation%20is%20publicly%0Aavailable%20%28https%3A//github.com/YibinWu/LIO-EKF%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIO-EKF%253A%2520High%2520Frequency%2520LiDAR-Inertial%2520Odometry%2520using%2520Extended%2520Kalman%250A%2520%2520Filters%26entry.906535625%3DYibin%2520Wu%2520and%2520Tiziano%2520Guadagnino%2520and%2520Louis%2520Wiesmann%2520and%2520Lasse%2520Klingbeil%2520and%2520Cyrill%2520Stachniss%2520and%2520Heiner%2520Kuhlmann%26entry.1292438233%3D%2520%2520Odometry%2520estimation%2520is%2520crucial%2520for%2520every%2520autonomous%2520system%2520requiring%250Anavigation%2520in%2520an%2520unknown%2520environment.%2520In%2520modern%2520mobile%2520robots%252C%25203D%250ALiDAR-inertial%2520systems%2520are%2520often%2520used%2520for%2520this%2520task.%2520By%2520fusing%2520LiDAR%2520scans%2520and%250AIMU%2520measurements%252C%2520these%2520systems%2520can%2520reduce%2520the%2520accumulated%2520drift%2520caused%2520by%250Asequentially%2520registering%2520individual%2520LiDAR%2520scans%2520and%2520provide%2520a%2520robust%2520pose%250Aestimate.%2520Although%2520effective%252C%2520LiDAR-inertial%2520odometry%2520systems%2520require%2520proper%250Aparameter%2520tuning%2520to%2520be%2520deployed.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LIO-EKF%252C%2520a%250Atightly-coupled%2520LiDAR-inertial%2520odometry%2520system%2520based%2520on%2520point-to-point%250Aregistration%2520and%2520the%2520classical%2520extended%2520Kalman%2520filter%2520scheme.%2520We%2520propose%2520an%250Aadaptive%2520data%2520association%2520that%2520considers%2520the%2520relative%2520pose%2520uncertainty%252C%2520the%2520map%250Adiscretization%2520errors%252C%2520and%2520the%2520LiDAR%2520noise.%2520In%2520this%2520way%252C%2520we%2520can%2520substantially%250Areduce%2520the%2520parameters%2520to%2520tune%2520for%2520a%2520given%2520type%2520of%2520environment.%2520The%2520experimental%250Aevaluation%2520suggests%2520that%2520the%2520proposed%2520system%2520performs%2520on%2520par%2520with%2520the%250Astate-of-the-art%2520LiDAR-inertial%2520odometry%2520pipelines%2520but%2520is%2520significantly%2520faster%250Ain%2520computing%2520the%2520odometry.%2520The%2520source%2520code%2520of%2520our%2520implementation%2520is%2520publicly%250Aavailable%2520%2528https%253A//github.com/YibinWu/LIO-EKF%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIO-EKF%3A%20High%20Frequency%20LiDAR-Inertial%20Odometry%20using%20Extended%20Kalman%0A%20%20Filters&entry.906535625=Yibin%20Wu%20and%20Tiziano%20Guadagnino%20and%20Louis%20Wiesmann%20and%20Lasse%20Klingbeil%20and%20Cyrill%20Stachniss%20and%20Heiner%20Kuhlmann&entry.1292438233=%20%20Odometry%20estimation%20is%20crucial%20for%20every%20autonomous%20system%20requiring%0Anavigation%20in%20an%20unknown%20environment.%20In%20modern%20mobile%20robots%2C%203D%0ALiDAR-inertial%20systems%20are%20often%20used%20for%20this%20task.%20By%20fusing%20LiDAR%20scans%20and%0AIMU%20measurements%2C%20these%20systems%20can%20reduce%20the%20accumulated%20drift%20caused%20by%0Asequentially%20registering%20individual%20LiDAR%20scans%20and%20provide%20a%20robust%20pose%0Aestimate.%20Although%20effective%2C%20LiDAR-inertial%20odometry%20systems%20require%20proper%0Aparameter%20tuning%20to%20be%20deployed.%20In%20this%20paper%2C%20we%20propose%20LIO-EKF%2C%20a%0Atightly-coupled%20LiDAR-inertial%20odometry%20system%20based%20on%20point-to-point%0Aregistration%20and%20the%20classical%20extended%20Kalman%20filter%20scheme.%20We%20propose%20an%0Aadaptive%20data%20association%20that%20considers%20the%20relative%20pose%20uncertainty%2C%20the%20map%0Adiscretization%20errors%2C%20and%20the%20LiDAR%20noise.%20In%20this%20way%2C%20we%20can%20substantially%0Areduce%20the%20parameters%20to%20tune%20for%20a%20given%20type%20of%20environment.%20The%20experimental%0Aevaluation%20suggests%20that%20the%20proposed%20system%20performs%20on%20par%20with%20the%0Astate-of-the-art%20LiDAR-inertial%20odometry%20pipelines%20but%20is%20significantly%20faster%0Ain%20computing%20the%20odometry.%20The%20source%20code%20of%20our%20implementation%20is%20publicly%0Aavailable%20%28https%3A//github.com/YibinWu/LIO-EKF%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09887v2&entry.124074799=Read"},
{"title": "A score-based particle method for homogeneous Landau equation", "author": "Yan Huang and Li Wang", "abstract": "  We propose a novel score-based particle method for solving the Landau\nequation in plasmas, that seamlessly integrates learning with\nstructure-preserving particle methods [arXiv:1910.03080]. Building upon the\nLagrangian viewpoint of the Landau equation, a central challenge stems from the\nnonlinear dependence of the velocity field on the density. Our primary\ninnovation lies in recognizing that this nonlinearity is in the form of the\nscore function, which can be approximated dynamically via techniques from\nscore-matching. The resulting method inherits the conservation properties of\nthe deterministic particle method while sidestepping the necessity for kernel\ndensity estimation in [arXiv:1910.03080]. This streamlines computation and\nenhances scalability with dimensionality. Furthermore, we provide a theoretical\nestimate by demonstrating that the KL divergence between our approximation and\nthe true solution can be effectively controlled by the score-matching loss.\nAdditionally, by adopting the flow map viewpoint, we derive an update formula\nfor exact density computation. Extensive examples have been provided to show\nthe efficiency of the method, including a physically relevant case of Coulomb\ninteraction.\n", "link": "http://arxiv.org/abs/2405.05187v1", "date": "2024-05-08", "relevancy": 1.6818, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4386}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.412}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20score-based%20particle%20method%20for%20homogeneous%20Landau%20equation&body=Title%3A%20A%20score-based%20particle%20method%20for%20homogeneous%20Landau%20equation%0AAuthor%3A%20Yan%20Huang%20and%20Li%20Wang%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20score-based%20particle%20method%20for%20solving%20the%20Landau%0Aequation%20in%20plasmas%2C%20that%20seamlessly%20integrates%20learning%20with%0Astructure-preserving%20particle%20methods%20%5BarXiv%3A1910.03080%5D.%20Building%20upon%20the%0ALagrangian%20viewpoint%20of%20the%20Landau%20equation%2C%20a%20central%20challenge%20stems%20from%20the%0Anonlinear%20dependence%20of%20the%20velocity%20field%20on%20the%20density.%20Our%20primary%0Ainnovation%20lies%20in%20recognizing%20that%20this%20nonlinearity%20is%20in%20the%20form%20of%20the%0Ascore%20function%2C%20which%20can%20be%20approximated%20dynamically%20via%20techniques%20from%0Ascore-matching.%20The%20resulting%20method%20inherits%20the%20conservation%20properties%20of%0Athe%20deterministic%20particle%20method%20while%20sidestepping%20the%20necessity%20for%20kernel%0Adensity%20estimation%20in%20%5BarXiv%3A1910.03080%5D.%20This%20streamlines%20computation%20and%0Aenhances%20scalability%20with%20dimensionality.%20Furthermore%2C%20we%20provide%20a%20theoretical%0Aestimate%20by%20demonstrating%20that%20the%20KL%20divergence%20between%20our%20approximation%20and%0Athe%20true%20solution%20can%20be%20effectively%20controlled%20by%20the%20score-matching%20loss.%0AAdditionally%2C%20by%20adopting%20the%20flow%20map%20viewpoint%2C%20we%20derive%20an%20update%20formula%0Afor%20exact%20density%20computation.%20Extensive%20examples%20have%20been%20provided%20to%20show%0Athe%20efficiency%20of%20the%20method%2C%20including%20a%20physically%20relevant%20case%20of%20Coulomb%0Ainteraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520score-based%2520particle%2520method%2520for%2520homogeneous%2520Landau%2520equation%26entry.906535625%3DYan%2520Huang%2520and%2520Li%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520score-based%2520particle%2520method%2520for%2520solving%2520the%2520Landau%250Aequation%2520in%2520plasmas%252C%2520that%2520seamlessly%2520integrates%2520learning%2520with%250Astructure-preserving%2520particle%2520methods%2520%255BarXiv%253A1910.03080%255D.%2520Building%2520upon%2520the%250ALagrangian%2520viewpoint%2520of%2520the%2520Landau%2520equation%252C%2520a%2520central%2520challenge%2520stems%2520from%2520the%250Anonlinear%2520dependence%2520of%2520the%2520velocity%2520field%2520on%2520the%2520density.%2520Our%2520primary%250Ainnovation%2520lies%2520in%2520recognizing%2520that%2520this%2520nonlinearity%2520is%2520in%2520the%2520form%2520of%2520the%250Ascore%2520function%252C%2520which%2520can%2520be%2520approximated%2520dynamically%2520via%2520techniques%2520from%250Ascore-matching.%2520The%2520resulting%2520method%2520inherits%2520the%2520conservation%2520properties%2520of%250Athe%2520deterministic%2520particle%2520method%2520while%2520sidestepping%2520the%2520necessity%2520for%2520kernel%250Adensity%2520estimation%2520in%2520%255BarXiv%253A1910.03080%255D.%2520This%2520streamlines%2520computation%2520and%250Aenhances%2520scalability%2520with%2520dimensionality.%2520Furthermore%252C%2520we%2520provide%2520a%2520theoretical%250Aestimate%2520by%2520demonstrating%2520that%2520the%2520KL%2520divergence%2520between%2520our%2520approximation%2520and%250Athe%2520true%2520solution%2520can%2520be%2520effectively%2520controlled%2520by%2520the%2520score-matching%2520loss.%250AAdditionally%252C%2520by%2520adopting%2520the%2520flow%2520map%2520viewpoint%252C%2520we%2520derive%2520an%2520update%2520formula%250Afor%2520exact%2520density%2520computation.%2520Extensive%2520examples%2520have%2520been%2520provided%2520to%2520show%250Athe%2520efficiency%2520of%2520the%2520method%252C%2520including%2520a%2520physically%2520relevant%2520case%2520of%2520Coulomb%250Ainteraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20score-based%20particle%20method%20for%20homogeneous%20Landau%20equation&entry.906535625=Yan%20Huang%20and%20Li%20Wang&entry.1292438233=%20%20We%20propose%20a%20novel%20score-based%20particle%20method%20for%20solving%20the%20Landau%0Aequation%20in%20plasmas%2C%20that%20seamlessly%20integrates%20learning%20with%0Astructure-preserving%20particle%20methods%20%5BarXiv%3A1910.03080%5D.%20Building%20upon%20the%0ALagrangian%20viewpoint%20of%20the%20Landau%20equation%2C%20a%20central%20challenge%20stems%20from%20the%0Anonlinear%20dependence%20of%20the%20velocity%20field%20on%20the%20density.%20Our%20primary%0Ainnovation%20lies%20in%20recognizing%20that%20this%20nonlinearity%20is%20in%20the%20form%20of%20the%0Ascore%20function%2C%20which%20can%20be%20approximated%20dynamically%20via%20techniques%20from%0Ascore-matching.%20The%20resulting%20method%20inherits%20the%20conservation%20properties%20of%0Athe%20deterministic%20particle%20method%20while%20sidestepping%20the%20necessity%20for%20kernel%0Adensity%20estimation%20in%20%5BarXiv%3A1910.03080%5D.%20This%20streamlines%20computation%20and%0Aenhances%20scalability%20with%20dimensionality.%20Furthermore%2C%20we%20provide%20a%20theoretical%0Aestimate%20by%20demonstrating%20that%20the%20KL%20divergence%20between%20our%20approximation%20and%0Athe%20true%20solution%20can%20be%20effectively%20controlled%20by%20the%20score-matching%20loss.%0AAdditionally%2C%20by%20adopting%20the%20flow%20map%20viewpoint%2C%20we%20derive%20an%20update%20formula%0Afor%20exact%20density%20computation.%20Extensive%20examples%20have%20been%20provided%20to%20show%0Athe%20efficiency%20of%20the%20method%2C%20including%20a%20physically%20relevant%20case%20of%20Coulomb%0Ainteraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05187v1&entry.124074799=Read"},
{"title": "Picking watermarks from noise (PWFN): an improved robust watermarking\n  model against intensive distortions", "author": "Sijing Xie and Chengxin Zhao and Nan Sun and Wei Li and Hefei Ling", "abstract": "  Digital watermarking is the process of embedding secret information by\naltering images in a way that is undetectable to the human eye. To increase the\nrobustness of the model, many deep learning-based watermarking methods use the\nencoder-decoder architecture by adding different noises to the noise layer. The\ndecoder then extracts the watermarked information from the distorted image.\nHowever, this method can only resist weak noise attacks. To improve the\nrobustness of the algorithm against stronger noise, this paper proposes to\nintroduce a denoise module between the noise layer and the decoder. The module\nis aimed at reducing noise and recovering some of the information lost during\nan attack. Additionally, the paper introduces the SE module to fuse the\nwatermarking information pixel-wise and channel dimensions-wise, improving the\nencoder's efficiency. Experimental results show that our proposed method is\ncomparable to existing models and outperforms state-of-the-art under different\nnoise intensities. In addition, ablation experiments show the superiority of\nour proposed module.\n", "link": "http://arxiv.org/abs/2405.05170v1", "date": "2024-05-08", "relevancy": 1.6797, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5937}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5478}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Picking%20watermarks%20from%20noise%20%28PWFN%29%3A%20an%20improved%20robust%20watermarking%0A%20%20model%20against%20intensive%20distortions&body=Title%3A%20Picking%20watermarks%20from%20noise%20%28PWFN%29%3A%20an%20improved%20robust%20watermarking%0A%20%20model%20against%20intensive%20distortions%0AAuthor%3A%20Sijing%20Xie%20and%20Chengxin%20Zhao%20and%20Nan%20Sun%20and%20Wei%20Li%20and%20Hefei%20Ling%0AAbstract%3A%20%20%20Digital%20watermarking%20is%20the%20process%20of%20embedding%20secret%20information%20by%0Aaltering%20images%20in%20a%20way%20that%20is%20undetectable%20to%20the%20human%20eye.%20To%20increase%20the%0Arobustness%20of%20the%20model%2C%20many%20deep%20learning-based%20watermarking%20methods%20use%20the%0Aencoder-decoder%20architecture%20by%20adding%20different%20noises%20to%20the%20noise%20layer.%20The%0Adecoder%20then%20extracts%20the%20watermarked%20information%20from%20the%20distorted%20image.%0AHowever%2C%20this%20method%20can%20only%20resist%20weak%20noise%20attacks.%20To%20improve%20the%0Arobustness%20of%20the%20algorithm%20against%20stronger%20noise%2C%20this%20paper%20proposes%20to%0Aintroduce%20a%20denoise%20module%20between%20the%20noise%20layer%20and%20the%20decoder.%20The%20module%0Ais%20aimed%20at%20reducing%20noise%20and%20recovering%20some%20of%20the%20information%20lost%20during%0Aan%20attack.%20Additionally%2C%20the%20paper%20introduces%20the%20SE%20module%20to%20fuse%20the%0Awatermarking%20information%20pixel-wise%20and%20channel%20dimensions-wise%2C%20improving%20the%0Aencoder%27s%20efficiency.%20Experimental%20results%20show%20that%20our%20proposed%20method%20is%0Acomparable%20to%20existing%20models%20and%20outperforms%20state-of-the-art%20under%20different%0Anoise%20intensities.%20In%20addition%2C%20ablation%20experiments%20show%20the%20superiority%20of%0Aour%20proposed%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPicking%2520watermarks%2520from%2520noise%2520%2528PWFN%2529%253A%2520an%2520improved%2520robust%2520watermarking%250A%2520%2520model%2520against%2520intensive%2520distortions%26entry.906535625%3DSijing%2520Xie%2520and%2520Chengxin%2520Zhao%2520and%2520Nan%2520Sun%2520and%2520Wei%2520Li%2520and%2520Hefei%2520Ling%26entry.1292438233%3D%2520%2520Digital%2520watermarking%2520is%2520the%2520process%2520of%2520embedding%2520secret%2520information%2520by%250Aaltering%2520images%2520in%2520a%2520way%2520that%2520is%2520undetectable%2520to%2520the%2520human%2520eye.%2520To%2520increase%2520the%250Arobustness%2520of%2520the%2520model%252C%2520many%2520deep%2520learning-based%2520watermarking%2520methods%2520use%2520the%250Aencoder-decoder%2520architecture%2520by%2520adding%2520different%2520noises%2520to%2520the%2520noise%2520layer.%2520The%250Adecoder%2520then%2520extracts%2520the%2520watermarked%2520information%2520from%2520the%2520distorted%2520image.%250AHowever%252C%2520this%2520method%2520can%2520only%2520resist%2520weak%2520noise%2520attacks.%2520To%2520improve%2520the%250Arobustness%2520of%2520the%2520algorithm%2520against%2520stronger%2520noise%252C%2520this%2520paper%2520proposes%2520to%250Aintroduce%2520a%2520denoise%2520module%2520between%2520the%2520noise%2520layer%2520and%2520the%2520decoder.%2520The%2520module%250Ais%2520aimed%2520at%2520reducing%2520noise%2520and%2520recovering%2520some%2520of%2520the%2520information%2520lost%2520during%250Aan%2520attack.%2520Additionally%252C%2520the%2520paper%2520introduces%2520the%2520SE%2520module%2520to%2520fuse%2520the%250Awatermarking%2520information%2520pixel-wise%2520and%2520channel%2520dimensions-wise%252C%2520improving%2520the%250Aencoder%2527s%2520efficiency.%2520Experimental%2520results%2520show%2520that%2520our%2520proposed%2520method%2520is%250Acomparable%2520to%2520existing%2520models%2520and%2520outperforms%2520state-of-the-art%2520under%2520different%250Anoise%2520intensities.%2520In%2520addition%252C%2520ablation%2520experiments%2520show%2520the%2520superiority%2520of%250Aour%2520proposed%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Picking%20watermarks%20from%20noise%20%28PWFN%29%3A%20an%20improved%20robust%20watermarking%0A%20%20model%20against%20intensive%20distortions&entry.906535625=Sijing%20Xie%20and%20Chengxin%20Zhao%20and%20Nan%20Sun%20and%20Wei%20Li%20and%20Hefei%20Ling&entry.1292438233=%20%20Digital%20watermarking%20is%20the%20process%20of%20embedding%20secret%20information%20by%0Aaltering%20images%20in%20a%20way%20that%20is%20undetectable%20to%20the%20human%20eye.%20To%20increase%20the%0Arobustness%20of%20the%20model%2C%20many%20deep%20learning-based%20watermarking%20methods%20use%20the%0Aencoder-decoder%20architecture%20by%20adding%20different%20noises%20to%20the%20noise%20layer.%20The%0Adecoder%20then%20extracts%20the%20watermarked%20information%20from%20the%20distorted%20image.%0AHowever%2C%20this%20method%20can%20only%20resist%20weak%20noise%20attacks.%20To%20improve%20the%0Arobustness%20of%20the%20algorithm%20against%20stronger%20noise%2C%20this%20paper%20proposes%20to%0Aintroduce%20a%20denoise%20module%20between%20the%20noise%20layer%20and%20the%20decoder.%20The%20module%0Ais%20aimed%20at%20reducing%20noise%20and%20recovering%20some%20of%20the%20information%20lost%20during%0Aan%20attack.%20Additionally%2C%20the%20paper%20introduces%20the%20SE%20module%20to%20fuse%20the%0Awatermarking%20information%20pixel-wise%20and%20channel%20dimensions-wise%2C%20improving%20the%0Aencoder%27s%20efficiency.%20Experimental%20results%20show%20that%20our%20proposed%20method%20is%0Acomparable%20to%20existing%20models%20and%20outperforms%20state-of-the-art%20under%20different%0Anoise%20intensities.%20In%20addition%2C%20ablation%20experiments%20show%20the%20superiority%20of%0Aour%20proposed%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05170v1&entry.124074799=Read"},
{"title": "Investigating Self-Supervised Image Denoising with Denaturation", "author": "Hiroki Waida and Kimihiro Yamazaki and Atsushi Tokuhisa and Mutsuyo Wada and Yuichiro Wada", "abstract": "  Self-supervised learning for image denoising problems in the presence of\ndenaturation for noisy data is a crucial approach in machine learning. However,\ntheoretical understanding of the performance of the approach that uses\ndenatured data is lacking. To provide better understanding of the approach, in\nthis paper, we analyze a self-supervised denoising algorithm that uses\ndenatured data in depth through theoretical analysis and numerical experiments.\nThrough the theoretical analysis, we discuss that the algorithm finds desired\nsolutions to the optimization problem with the population risk, while the\nguarantee for the empirical risk depends on the hardness of the denoising task\nin terms of denaturation levels. We also conduct several experiments to\ninvestigate the performance of an extended algorithm in practice. The results\nindicate that the algorithm training with denatured images works, and the\nempirical performance aligns with the theoretical results. These results\nsuggest several insights for further improvement of self-supervised image\ndenoising that uses denatured data in future directions.\n", "link": "http://arxiv.org/abs/2405.01124v2", "date": "2024-05-08", "relevancy": 1.6745, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5819}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5437}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Self-Supervised%20Image%20Denoising%20with%20Denaturation&body=Title%3A%20Investigating%20Self-Supervised%20Image%20Denoising%20with%20Denaturation%0AAuthor%3A%20Hiroki%20Waida%20and%20Kimihiro%20Yamazaki%20and%20Atsushi%20Tokuhisa%20and%20Mutsuyo%20Wada%20and%20Yuichiro%20Wada%0AAbstract%3A%20%20%20Self-supervised%20learning%20for%20image%20denoising%20problems%20in%20the%20presence%20of%0Adenaturation%20for%20noisy%20data%20is%20a%20crucial%20approach%20in%20machine%20learning.%20However%2C%0Atheoretical%20understanding%20of%20the%20performance%20of%20the%20approach%20that%20uses%0Adenatured%20data%20is%20lacking.%20To%20provide%20better%20understanding%20of%20the%20approach%2C%20in%0Athis%20paper%2C%20we%20analyze%20a%20self-supervised%20denoising%20algorithm%20that%20uses%0Adenatured%20data%20in%20depth%20through%20theoretical%20analysis%20and%20numerical%20experiments.%0AThrough%20the%20theoretical%20analysis%2C%20we%20discuss%20that%20the%20algorithm%20finds%20desired%0Asolutions%20to%20the%20optimization%20problem%20with%20the%20population%20risk%2C%20while%20the%0Aguarantee%20for%20the%20empirical%20risk%20depends%20on%20the%20hardness%20of%20the%20denoising%20task%0Ain%20terms%20of%20denaturation%20levels.%20We%20also%20conduct%20several%20experiments%20to%0Ainvestigate%20the%20performance%20of%20an%20extended%20algorithm%20in%20practice.%20The%20results%0Aindicate%20that%20the%20algorithm%20training%20with%20denatured%20images%20works%2C%20and%20the%0Aempirical%20performance%20aligns%20with%20the%20theoretical%20results.%20These%20results%0Asuggest%20several%20insights%20for%20further%20improvement%20of%20self-supervised%20image%0Adenoising%20that%20uses%20denatured%20data%20in%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01124v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Self-Supervised%2520Image%2520Denoising%2520with%2520Denaturation%26entry.906535625%3DHiroki%2520Waida%2520and%2520Kimihiro%2520Yamazaki%2520and%2520Atsushi%2520Tokuhisa%2520and%2520Mutsuyo%2520Wada%2520and%2520Yuichiro%2520Wada%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520for%2520image%2520denoising%2520problems%2520in%2520the%2520presence%2520of%250Adenaturation%2520for%2520noisy%2520data%2520is%2520a%2520crucial%2520approach%2520in%2520machine%2520learning.%2520However%252C%250Atheoretical%2520understanding%2520of%2520the%2520performance%2520of%2520the%2520approach%2520that%2520uses%250Adenatured%2520data%2520is%2520lacking.%2520To%2520provide%2520better%2520understanding%2520of%2520the%2520approach%252C%2520in%250Athis%2520paper%252C%2520we%2520analyze%2520a%2520self-supervised%2520denoising%2520algorithm%2520that%2520uses%250Adenatured%2520data%2520in%2520depth%2520through%2520theoretical%2520analysis%2520and%2520numerical%2520experiments.%250AThrough%2520the%2520theoretical%2520analysis%252C%2520we%2520discuss%2520that%2520the%2520algorithm%2520finds%2520desired%250Asolutions%2520to%2520the%2520optimization%2520problem%2520with%2520the%2520population%2520risk%252C%2520while%2520the%250Aguarantee%2520for%2520the%2520empirical%2520risk%2520depends%2520on%2520the%2520hardness%2520of%2520the%2520denoising%2520task%250Ain%2520terms%2520of%2520denaturation%2520levels.%2520We%2520also%2520conduct%2520several%2520experiments%2520to%250Ainvestigate%2520the%2520performance%2520of%2520an%2520extended%2520algorithm%2520in%2520practice.%2520The%2520results%250Aindicate%2520that%2520the%2520algorithm%2520training%2520with%2520denatured%2520images%2520works%252C%2520and%2520the%250Aempirical%2520performance%2520aligns%2520with%2520the%2520theoretical%2520results.%2520These%2520results%250Asuggest%2520several%2520insights%2520for%2520further%2520improvement%2520of%2520self-supervised%2520image%250Adenoising%2520that%2520uses%2520denatured%2520data%2520in%2520future%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01124v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Self-Supervised%20Image%20Denoising%20with%20Denaturation&entry.906535625=Hiroki%20Waida%20and%20Kimihiro%20Yamazaki%20and%20Atsushi%20Tokuhisa%20and%20Mutsuyo%20Wada%20and%20Yuichiro%20Wada&entry.1292438233=%20%20Self-supervised%20learning%20for%20image%20denoising%20problems%20in%20the%20presence%20of%0Adenaturation%20for%20noisy%20data%20is%20a%20crucial%20approach%20in%20machine%20learning.%20However%2C%0Atheoretical%20understanding%20of%20the%20performance%20of%20the%20approach%20that%20uses%0Adenatured%20data%20is%20lacking.%20To%20provide%20better%20understanding%20of%20the%20approach%2C%20in%0Athis%20paper%2C%20we%20analyze%20a%20self-supervised%20denoising%20algorithm%20that%20uses%0Adenatured%20data%20in%20depth%20through%20theoretical%20analysis%20and%20numerical%20experiments.%0AThrough%20the%20theoretical%20analysis%2C%20we%20discuss%20that%20the%20algorithm%20finds%20desired%0Asolutions%20to%20the%20optimization%20problem%20with%20the%20population%20risk%2C%20while%20the%0Aguarantee%20for%20the%20empirical%20risk%20depends%20on%20the%20hardness%20of%20the%20denoising%20task%0Ain%20terms%20of%20denaturation%20levels.%20We%20also%20conduct%20several%20experiments%20to%0Ainvestigate%20the%20performance%20of%20an%20extended%20algorithm%20in%20practice.%20The%20results%0Aindicate%20that%20the%20algorithm%20training%20with%20denatured%20images%20works%2C%20and%20the%0Aempirical%20performance%20aligns%20with%20the%20theoretical%20results.%20These%20results%0Asuggest%20several%20insights%20for%20further%20improvement%20of%20self-supervised%20image%0Adenoising%20that%20uses%20denatured%20data%20in%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01124v2&entry.124074799=Read"},
{"title": "FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via\n  Diffusion Models", "author": "Jinglin Xu and Yijie Guo and Yuxin Peng", "abstract": "  The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to\npredict human joint coordinates in 3D space. Despite recent advancements in\ndeep learning-based methods, they mostly ignore the capability of coupling\naccessible texts and naturally feasible knowledge of humans, missing out on\nvaluable implicit supervision to guide the 3D HPE task. Moreover, previous\nefforts often study this task from the perspective of the whole human body,\nneglecting fine-grained guidance hidden in different body parts. To this end,\nwe present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model\nfor 3D HPE, named \\textbf{FinePOSE}. It consists of three core blocks enhancing\nthe reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt\nlearning (FPP) block constructs fine-grained part-aware prompts via coupling\naccessible texts and naturally feasible knowledge of body parts with learnable\nprompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication\n(FPC) block establishes fine-grained communications between learned part-aware\nprompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp\nStylization (PTS) block integrates learned prompt embedding and temporal\ninformation related to the noise level to enable adaptive adjustment at each\ndenoising step. Extensive experiments on public single-human pose estimation\ndatasets show that FinePOSE outperforms state-of-the-art methods. We further\nextend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE\non the EgoHumans dataset demonstrates the potential of FinePOSE to deal with\ncomplex multi-human scenarios. Code is available at\nhttps://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024.\n", "link": "http://arxiv.org/abs/2405.05216v1", "date": "2024-05-08", "relevancy": 1.6712, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5794}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5629}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinePOSE%3A%20Fine-Grained%20Prompt-Driven%203D%20Human%20Pose%20Estimation%20via%0A%20%20Diffusion%20Models&body=Title%3A%20FinePOSE%3A%20Fine-Grained%20Prompt-Driven%203D%20Human%20Pose%20Estimation%20via%0A%20%20Diffusion%20Models%0AAuthor%3A%20Jinglin%20Xu%20and%20Yijie%20Guo%20and%20Yuxin%20Peng%0AAbstract%3A%20%20%20The%203D%20Human%20Pose%20Estimation%20%283D%20HPE%29%20task%20uses%202D%20images%20or%20videos%20to%0Apredict%20human%20joint%20coordinates%20in%203D%20space.%20Despite%20recent%20advancements%20in%0Adeep%20learning-based%20methods%2C%20they%20mostly%20ignore%20the%20capability%20of%20coupling%0Aaccessible%20texts%20and%20naturally%20feasible%20knowledge%20of%20humans%2C%20missing%20out%20on%0Avaluable%20implicit%20supervision%20to%20guide%20the%203D%20HPE%20task.%20Moreover%2C%20previous%0Aefforts%20often%20study%20this%20task%20from%20the%20perspective%20of%20the%20whole%20human%20body%2C%0Aneglecting%20fine-grained%20guidance%20hidden%20in%20different%20body%20parts.%20To%20this%20end%2C%0Awe%20present%20a%20new%20Fine-Grained%20Prompt-Driven%20Denoiser%20based%20on%20a%20diffusion%20model%0Afor%203D%20HPE%2C%20named%20%5Ctextbf%7BFinePOSE%7D.%20It%20consists%20of%20three%20core%20blocks%20enhancing%0Athe%20reverse%20process%20of%20the%20diffusion%20model%3A%20%281%29%20Fine-grained%20Part-aware%20Prompt%0Alearning%20%28FPP%29%20block%20constructs%20fine-grained%20part-aware%20prompts%20via%20coupling%0Aaccessible%20texts%20and%20naturally%20feasible%20knowledge%20of%20body%20parts%20with%20learnable%0Aprompts%20to%20model%20implicit%20guidance.%20%282%29%20Fine-grained%20Prompt-pose%20Communication%0A%28FPC%29%20block%20establishes%20fine-grained%20communications%20between%20learned%20part-aware%0Aprompts%20and%20poses%20to%20improve%20the%20denoising%20quality.%20%283%29%20Prompt-driven%20Timestamp%0AStylization%20%28PTS%29%20block%20integrates%20learned%20prompt%20embedding%20and%20temporal%0Ainformation%20related%20to%20the%20noise%20level%20to%20enable%20adaptive%20adjustment%20at%20each%0Adenoising%20step.%20Extensive%20experiments%20on%20public%20single-human%20pose%20estimation%0Adatasets%20show%20that%20FinePOSE%20outperforms%20state-of-the-art%20methods.%20We%20further%0Aextend%20FinePOSE%20to%20multi-human%20pose%20estimation.%20Achieving%2034.3mm%20average%20MPJPE%0Aon%20the%20EgoHumans%20dataset%20demonstrates%20the%20potential%20of%20FinePOSE%20to%20deal%20with%0Acomplex%20multi-human%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinePOSE%253A%2520Fine-Grained%2520Prompt-Driven%25203D%2520Human%2520Pose%2520Estimation%2520via%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DJinglin%2520Xu%2520and%2520Yijie%2520Guo%2520and%2520Yuxin%2520Peng%26entry.1292438233%3D%2520%2520The%25203D%2520Human%2520Pose%2520Estimation%2520%25283D%2520HPE%2529%2520task%2520uses%25202D%2520images%2520or%2520videos%2520to%250Apredict%2520human%2520joint%2520coordinates%2520in%25203D%2520space.%2520Despite%2520recent%2520advancements%2520in%250Adeep%2520learning-based%2520methods%252C%2520they%2520mostly%2520ignore%2520the%2520capability%2520of%2520coupling%250Aaccessible%2520texts%2520and%2520naturally%2520feasible%2520knowledge%2520of%2520humans%252C%2520missing%2520out%2520on%250Avaluable%2520implicit%2520supervision%2520to%2520guide%2520the%25203D%2520HPE%2520task.%2520Moreover%252C%2520previous%250Aefforts%2520often%2520study%2520this%2520task%2520from%2520the%2520perspective%2520of%2520the%2520whole%2520human%2520body%252C%250Aneglecting%2520fine-grained%2520guidance%2520hidden%2520in%2520different%2520body%2520parts.%2520To%2520this%2520end%252C%250Awe%2520present%2520a%2520new%2520Fine-Grained%2520Prompt-Driven%2520Denoiser%2520based%2520on%2520a%2520diffusion%2520model%250Afor%25203D%2520HPE%252C%2520named%2520%255Ctextbf%257BFinePOSE%257D.%2520It%2520consists%2520of%2520three%2520core%2520blocks%2520enhancing%250Athe%2520reverse%2520process%2520of%2520the%2520diffusion%2520model%253A%2520%25281%2529%2520Fine-grained%2520Part-aware%2520Prompt%250Alearning%2520%2528FPP%2529%2520block%2520constructs%2520fine-grained%2520part-aware%2520prompts%2520via%2520coupling%250Aaccessible%2520texts%2520and%2520naturally%2520feasible%2520knowledge%2520of%2520body%2520parts%2520with%2520learnable%250Aprompts%2520to%2520model%2520implicit%2520guidance.%2520%25282%2529%2520Fine-grained%2520Prompt-pose%2520Communication%250A%2528FPC%2529%2520block%2520establishes%2520fine-grained%2520communications%2520between%2520learned%2520part-aware%250Aprompts%2520and%2520poses%2520to%2520improve%2520the%2520denoising%2520quality.%2520%25283%2529%2520Prompt-driven%2520Timestamp%250AStylization%2520%2528PTS%2529%2520block%2520integrates%2520learned%2520prompt%2520embedding%2520and%2520temporal%250Ainformation%2520related%2520to%2520the%2520noise%2520level%2520to%2520enable%2520adaptive%2520adjustment%2520at%2520each%250Adenoising%2520step.%2520Extensive%2520experiments%2520on%2520public%2520single-human%2520pose%2520estimation%250Adatasets%2520show%2520that%2520FinePOSE%2520outperforms%2520state-of-the-art%2520methods.%2520We%2520further%250Aextend%2520FinePOSE%2520to%2520multi-human%2520pose%2520estimation.%2520Achieving%252034.3mm%2520average%2520MPJPE%250Aon%2520the%2520EgoHumans%2520dataset%2520demonstrates%2520the%2520potential%2520of%2520FinePOSE%2520to%2520deal%2520with%250Acomplex%2520multi-human%2520scenarios.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinePOSE%3A%20Fine-Grained%20Prompt-Driven%203D%20Human%20Pose%20Estimation%20via%0A%20%20Diffusion%20Models&entry.906535625=Jinglin%20Xu%20and%20Yijie%20Guo%20and%20Yuxin%20Peng&entry.1292438233=%20%20The%203D%20Human%20Pose%20Estimation%20%283D%20HPE%29%20task%20uses%202D%20images%20or%20videos%20to%0Apredict%20human%20joint%20coordinates%20in%203D%20space.%20Despite%20recent%20advancements%20in%0Adeep%20learning-based%20methods%2C%20they%20mostly%20ignore%20the%20capability%20of%20coupling%0Aaccessible%20texts%20and%20naturally%20feasible%20knowledge%20of%20humans%2C%20missing%20out%20on%0Avaluable%20implicit%20supervision%20to%20guide%20the%203D%20HPE%20task.%20Moreover%2C%20previous%0Aefforts%20often%20study%20this%20task%20from%20the%20perspective%20of%20the%20whole%20human%20body%2C%0Aneglecting%20fine-grained%20guidance%20hidden%20in%20different%20body%20parts.%20To%20this%20end%2C%0Awe%20present%20a%20new%20Fine-Grained%20Prompt-Driven%20Denoiser%20based%20on%20a%20diffusion%20model%0Afor%203D%20HPE%2C%20named%20%5Ctextbf%7BFinePOSE%7D.%20It%20consists%20of%20three%20core%20blocks%20enhancing%0Athe%20reverse%20process%20of%20the%20diffusion%20model%3A%20%281%29%20Fine-grained%20Part-aware%20Prompt%0Alearning%20%28FPP%29%20block%20constructs%20fine-grained%20part-aware%20prompts%20via%20coupling%0Aaccessible%20texts%20and%20naturally%20feasible%20knowledge%20of%20body%20parts%20with%20learnable%0Aprompts%20to%20model%20implicit%20guidance.%20%282%29%20Fine-grained%20Prompt-pose%20Communication%0A%28FPC%29%20block%20establishes%20fine-grained%20communications%20between%20learned%20part-aware%0Aprompts%20and%20poses%20to%20improve%20the%20denoising%20quality.%20%283%29%20Prompt-driven%20Timestamp%0AStylization%20%28PTS%29%20block%20integrates%20learned%20prompt%20embedding%20and%20temporal%0Ainformation%20related%20to%20the%20noise%20level%20to%20enable%20adaptive%20adjustment%20at%20each%0Adenoising%20step.%20Extensive%20experiments%20on%20public%20single-human%20pose%20estimation%0Adatasets%20show%20that%20FinePOSE%20outperforms%20state-of-the-art%20methods.%20We%20further%0Aextend%20FinePOSE%20to%20multi-human%20pose%20estimation.%20Achieving%2034.3mm%20average%20MPJPE%0Aon%20the%20EgoHumans%20dataset%20demonstrates%20the%20potential%20of%20FinePOSE%20to%20deal%20with%0Acomplex%20multi-human%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05216v1&entry.124074799=Read"},
{"title": "Diffusion-HMC: Parameter Inference with Diffusion Model driven\n  Hamiltonian Monte Carlo", "author": "Nayantara Mudur and Carolina Cuesta-Lazaro and Douglas P. Finkbeiner", "abstract": "  Diffusion generative models have excelled at diverse image generation and\nreconstruction tasks across fields. A less explored avenue is their application\nto discriminative tasks involving regression or classification problems. The\ncornerstone of modern cosmology is the ability to generate predictions for\nobserved astrophysical fields from theory and constrain physical models from\nobservations using these predictions. This work uses a single diffusion\ngenerative model to address these interlinked objectives -- as a surrogate\nmodel or emulator for cold dark matter density fields conditional on input\ncosmological parameters, and as a parameter inference model that solves the\ninverse problem of constraining the cosmological parameters of an input field.\nThe model is able to emulate fields with summary statistics consistent with\nthose of the simulated target distribution. We then leverage the approximate\nlikelihood of the diffusion generative model to derive tight constraints on\ncosmology by using the Hamiltonian Monte Carlo method to sample the posterior\non cosmological parameters for a given test image. Finally, we demonstrate that\nthis parameter inference approach is more robust to the addition of noise than\nbaseline parameter inference networks.\n", "link": "http://arxiv.org/abs/2405.05255v1", "date": "2024-05-08", "relevancy": 1.1536, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.61}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5692}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-HMC%3A%20Parameter%20Inference%20with%20Diffusion%20Model%20driven%0A%20%20Hamiltonian%20Monte%20Carlo&body=Title%3A%20Diffusion-HMC%3A%20Parameter%20Inference%20with%20Diffusion%20Model%20driven%0A%20%20Hamiltonian%20Monte%20Carlo%0AAuthor%3A%20Nayantara%20Mudur%20and%20Carolina%20Cuesta-Lazaro%20and%20Douglas%20P.%20Finkbeiner%0AAbstract%3A%20%20%20Diffusion%20generative%20models%20have%20excelled%20at%20diverse%20image%20generation%20and%0Areconstruction%20tasks%20across%20fields.%20A%20less%20explored%20avenue%20is%20their%20application%0Ato%20discriminative%20tasks%20involving%20regression%20or%20classification%20problems.%20The%0Acornerstone%20of%20modern%20cosmology%20is%20the%20ability%20to%20generate%20predictions%20for%0Aobserved%20astrophysical%20fields%20from%20theory%20and%20constrain%20physical%20models%20from%0Aobservations%20using%20these%20predictions.%20This%20work%20uses%20a%20single%20diffusion%0Agenerative%20model%20to%20address%20these%20interlinked%20objectives%20--%20as%20a%20surrogate%0Amodel%20or%20emulator%20for%20cold%20dark%20matter%20density%20fields%20conditional%20on%20input%0Acosmological%20parameters%2C%20and%20as%20a%20parameter%20inference%20model%20that%20solves%20the%0Ainverse%20problem%20of%20constraining%20the%20cosmological%20parameters%20of%20an%20input%20field.%0AThe%20model%20is%20able%20to%20emulate%20fields%20with%20summary%20statistics%20consistent%20with%0Athose%20of%20the%20simulated%20target%20distribution.%20We%20then%20leverage%20the%20approximate%0Alikelihood%20of%20the%20diffusion%20generative%20model%20to%20derive%20tight%20constraints%20on%0Acosmology%20by%20using%20the%20Hamiltonian%20Monte%20Carlo%20method%20to%20sample%20the%20posterior%0Aon%20cosmological%20parameters%20for%20a%20given%20test%20image.%20Finally%2C%20we%20demonstrate%20that%0Athis%20parameter%20inference%20approach%20is%20more%20robust%20to%20the%20addition%20of%20noise%20than%0Abaseline%20parameter%20inference%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-HMC%253A%2520Parameter%2520Inference%2520with%2520Diffusion%2520Model%2520driven%250A%2520%2520Hamiltonian%2520Monte%2520Carlo%26entry.906535625%3DNayantara%2520Mudur%2520and%2520Carolina%2520Cuesta-Lazaro%2520and%2520Douglas%2520P.%2520Finkbeiner%26entry.1292438233%3D%2520%2520Diffusion%2520generative%2520models%2520have%2520excelled%2520at%2520diverse%2520image%2520generation%2520and%250Areconstruction%2520tasks%2520across%2520fields.%2520A%2520less%2520explored%2520avenue%2520is%2520their%2520application%250Ato%2520discriminative%2520tasks%2520involving%2520regression%2520or%2520classification%2520problems.%2520The%250Acornerstone%2520of%2520modern%2520cosmology%2520is%2520the%2520ability%2520to%2520generate%2520predictions%2520for%250Aobserved%2520astrophysical%2520fields%2520from%2520theory%2520and%2520constrain%2520physical%2520models%2520from%250Aobservations%2520using%2520these%2520predictions.%2520This%2520work%2520uses%2520a%2520single%2520diffusion%250Agenerative%2520model%2520to%2520address%2520these%2520interlinked%2520objectives%2520--%2520as%2520a%2520surrogate%250Amodel%2520or%2520emulator%2520for%2520cold%2520dark%2520matter%2520density%2520fields%2520conditional%2520on%2520input%250Acosmological%2520parameters%252C%2520and%2520as%2520a%2520parameter%2520inference%2520model%2520that%2520solves%2520the%250Ainverse%2520problem%2520of%2520constraining%2520the%2520cosmological%2520parameters%2520of%2520an%2520input%2520field.%250AThe%2520model%2520is%2520able%2520to%2520emulate%2520fields%2520with%2520summary%2520statistics%2520consistent%2520with%250Athose%2520of%2520the%2520simulated%2520target%2520distribution.%2520We%2520then%2520leverage%2520the%2520approximate%250Alikelihood%2520of%2520the%2520diffusion%2520generative%2520model%2520to%2520derive%2520tight%2520constraints%2520on%250Acosmology%2520by%2520using%2520the%2520Hamiltonian%2520Monte%2520Carlo%2520method%2520to%2520sample%2520the%2520posterior%250Aon%2520cosmological%2520parameters%2520for%2520a%2520given%2520test%2520image.%2520Finally%252C%2520we%2520demonstrate%2520that%250Athis%2520parameter%2520inference%2520approach%2520is%2520more%2520robust%2520to%2520the%2520addition%2520of%2520noise%2520than%250Abaseline%2520parameter%2520inference%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-HMC%3A%20Parameter%20Inference%20with%20Diffusion%20Model%20driven%0A%20%20Hamiltonian%20Monte%20Carlo&entry.906535625=Nayantara%20Mudur%20and%20Carolina%20Cuesta-Lazaro%20and%20Douglas%20P.%20Finkbeiner&entry.1292438233=%20%20Diffusion%20generative%20models%20have%20excelled%20at%20diverse%20image%20generation%20and%0Areconstruction%20tasks%20across%20fields.%20A%20less%20explored%20avenue%20is%20their%20application%0Ato%20discriminative%20tasks%20involving%20regression%20or%20classification%20problems.%20The%0Acornerstone%20of%20modern%20cosmology%20is%20the%20ability%20to%20generate%20predictions%20for%0Aobserved%20astrophysical%20fields%20from%20theory%20and%20constrain%20physical%20models%20from%0Aobservations%20using%20these%20predictions.%20This%20work%20uses%20a%20single%20diffusion%0Agenerative%20model%20to%20address%20these%20interlinked%20objectives%20--%20as%20a%20surrogate%0Amodel%20or%20emulator%20for%20cold%20dark%20matter%20density%20fields%20conditional%20on%20input%0Acosmological%20parameters%2C%20and%20as%20a%20parameter%20inference%20model%20that%20solves%20the%0Ainverse%20problem%20of%20constraining%20the%20cosmological%20parameters%20of%20an%20input%20field.%0AThe%20model%20is%20able%20to%20emulate%20fields%20with%20summary%20statistics%20consistent%20with%0Athose%20of%20the%20simulated%20target%20distribution.%20We%20then%20leverage%20the%20approximate%0Alikelihood%20of%20the%20diffusion%20generative%20model%20to%20derive%20tight%20constraints%20on%0Acosmology%20by%20using%20the%20Hamiltonian%20Monte%20Carlo%20method%20to%20sample%20the%20posterior%0Aon%20cosmological%20parameters%20for%20a%20given%20test%20image.%20Finally%2C%20we%20demonstrate%20that%0Athis%20parameter%20inference%20approach%20is%20more%20robust%20to%20the%20addition%20of%20noise%20than%0Abaseline%20parameter%20inference%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05255v1&entry.124074799=Read"},
{"title": "Personalized Autonomous Driving with Large Language Models: Field\n  Experiments", "author": "Can Cui and Zichong Yang and Yupeng Zhou and Yunsheng Ma and Juanwu Lu and Lingxi Li and Yaobin Chen and Jitesh Panchal and Ziran Wang", "abstract": "  Integrating large language models (LLMs) in autonomous vehicles enables\nconversation with AI systems to drive the vehicle. However, it also emphasizes\nthe requirement for such systems to comprehend commands accurately and achieve\nhigher-level personalization to adapt to the preferences of drivers or\npassengers over a more extended period. In this paper, we introduce an\nLLM-based framework, Talk2Drive, capable of translating natural verbal commands\ninto executable controls and learning to satisfy personal preferences for\nsafety, efficiency, and comfort with a proposed memory module. This is the\nfirst-of-its-kind multi-scenario field experiment that deploys LLMs on a\nreal-world autonomous vehicle. Experiments showcase that the proposed system\ncan comprehend human intentions at different intuition levels, ranging from\ndirect commands like \"can you drive faster\" to indirect commands like \"I am\nreally in a hurry now\". Additionally, we use the takeover rate to quantify the\ntrust of human drivers in the LLM-based autonomous driving system, where\nTalk2Drive significantly reduces the takeover rate in highway, intersection,\nand parking scenarios. We also validate that the proposed memory module\nconsiders personalized preferences and further reduces the takeover rate by up\nto 65.2% compared with those without a memory module. The experiment video can\nbe watched at https://www.youtube.com/watch?v=4BWsfPaq1Ro\n", "link": "http://arxiv.org/abs/2312.09397v3", "date": "2024-05-08", "relevancy": 1.5769, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Autonomous%20Driving%20with%20Large%20Language%20Models%3A%20Field%0A%20%20Experiments&body=Title%3A%20Personalized%20Autonomous%20Driving%20with%20Large%20Language%20Models%3A%20Field%0A%20%20Experiments%0AAuthor%3A%20Can%20Cui%20and%20Zichong%20Yang%20and%20Yupeng%20Zhou%20and%20Yunsheng%20Ma%20and%20Juanwu%20Lu%20and%20Lingxi%20Li%20and%20Yaobin%20Chen%20and%20Jitesh%20Panchal%20and%20Ziran%20Wang%0AAbstract%3A%20%20%20Integrating%20large%20language%20models%20%28LLMs%29%20in%20autonomous%20vehicles%20enables%0Aconversation%20with%20AI%20systems%20to%20drive%20the%20vehicle.%20However%2C%20it%20also%20emphasizes%0Athe%20requirement%20for%20such%20systems%20to%20comprehend%20commands%20accurately%20and%20achieve%0Ahigher-level%20personalization%20to%20adapt%20to%20the%20preferences%20of%20drivers%20or%0Apassengers%20over%20a%20more%20extended%20period.%20In%20this%20paper%2C%20we%20introduce%20an%0ALLM-based%20framework%2C%20Talk2Drive%2C%20capable%20of%20translating%20natural%20verbal%20commands%0Ainto%20executable%20controls%20and%20learning%20to%20satisfy%20personal%20preferences%20for%0Asafety%2C%20efficiency%2C%20and%20comfort%20with%20a%20proposed%20memory%20module.%20This%20is%20the%0Afirst-of-its-kind%20multi-scenario%20field%20experiment%20that%20deploys%20LLMs%20on%20a%0Areal-world%20autonomous%20vehicle.%20Experiments%20showcase%20that%20the%20proposed%20system%0Acan%20comprehend%20human%20intentions%20at%20different%20intuition%20levels%2C%20ranging%20from%0Adirect%20commands%20like%20%22can%20you%20drive%20faster%22%20to%20indirect%20commands%20like%20%22I%20am%0Areally%20in%20a%20hurry%20now%22.%20Additionally%2C%20we%20use%20the%20takeover%20rate%20to%20quantify%20the%0Atrust%20of%20human%20drivers%20in%20the%20LLM-based%20autonomous%20driving%20system%2C%20where%0ATalk2Drive%20significantly%20reduces%20the%20takeover%20rate%20in%20highway%2C%20intersection%2C%0Aand%20parking%20scenarios.%20We%20also%20validate%20that%20the%20proposed%20memory%20module%0Aconsiders%20personalized%20preferences%20and%20further%20reduces%20the%20takeover%20rate%20by%20up%0Ato%2065.2%25%20compared%20with%20those%20without%20a%20memory%20module.%20The%20experiment%20video%20can%0Abe%20watched%20at%20https%3A//www.youtube.com/watch%3Fv%3D4BWsfPaq1Ro%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09397v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Autonomous%2520Driving%2520with%2520Large%2520Language%2520Models%253A%2520Field%250A%2520%2520Experiments%26entry.906535625%3DCan%2520Cui%2520and%2520Zichong%2520Yang%2520and%2520Yupeng%2520Zhou%2520and%2520Yunsheng%2520Ma%2520and%2520Juanwu%2520Lu%2520and%2520Lingxi%2520Li%2520and%2520Yaobin%2520Chen%2520and%2520Jitesh%2520Panchal%2520and%2520Ziran%2520Wang%26entry.1292438233%3D%2520%2520Integrating%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520autonomous%2520vehicles%2520enables%250Aconversation%2520with%2520AI%2520systems%2520to%2520drive%2520the%2520vehicle.%2520However%252C%2520it%2520also%2520emphasizes%250Athe%2520requirement%2520for%2520such%2520systems%2520to%2520comprehend%2520commands%2520accurately%2520and%2520achieve%250Ahigher-level%2520personalization%2520to%2520adapt%2520to%2520the%2520preferences%2520of%2520drivers%2520or%250Apassengers%2520over%2520a%2520more%2520extended%2520period.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%250ALLM-based%2520framework%252C%2520Talk2Drive%252C%2520capable%2520of%2520translating%2520natural%2520verbal%2520commands%250Ainto%2520executable%2520controls%2520and%2520learning%2520to%2520satisfy%2520personal%2520preferences%2520for%250Asafety%252C%2520efficiency%252C%2520and%2520comfort%2520with%2520a%2520proposed%2520memory%2520module.%2520This%2520is%2520the%250Afirst-of-its-kind%2520multi-scenario%2520field%2520experiment%2520that%2520deploys%2520LLMs%2520on%2520a%250Areal-world%2520autonomous%2520vehicle.%2520Experiments%2520showcase%2520that%2520the%2520proposed%2520system%250Acan%2520comprehend%2520human%2520intentions%2520at%2520different%2520intuition%2520levels%252C%2520ranging%2520from%250Adirect%2520commands%2520like%2520%2522can%2520you%2520drive%2520faster%2522%2520to%2520indirect%2520commands%2520like%2520%2522I%2520am%250Areally%2520in%2520a%2520hurry%2520now%2522.%2520Additionally%252C%2520we%2520use%2520the%2520takeover%2520rate%2520to%2520quantify%2520the%250Atrust%2520of%2520human%2520drivers%2520in%2520the%2520LLM-based%2520autonomous%2520driving%2520system%252C%2520where%250ATalk2Drive%2520significantly%2520reduces%2520the%2520takeover%2520rate%2520in%2520highway%252C%2520intersection%252C%250Aand%2520parking%2520scenarios.%2520We%2520also%2520validate%2520that%2520the%2520proposed%2520memory%2520module%250Aconsiders%2520personalized%2520preferences%2520and%2520further%2520reduces%2520the%2520takeover%2520rate%2520by%2520up%250Ato%252065.2%2525%2520compared%2520with%2520those%2520without%2520a%2520memory%2520module.%2520The%2520experiment%2520video%2520can%250Abe%2520watched%2520at%2520https%253A//www.youtube.com/watch%253Fv%253D4BWsfPaq1Ro%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09397v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Autonomous%20Driving%20with%20Large%20Language%20Models%3A%20Field%0A%20%20Experiments&entry.906535625=Can%20Cui%20and%20Zichong%20Yang%20and%20Yupeng%20Zhou%20and%20Yunsheng%20Ma%20and%20Juanwu%20Lu%20and%20Lingxi%20Li%20and%20Yaobin%20Chen%20and%20Jitesh%20Panchal%20and%20Ziran%20Wang&entry.1292438233=%20%20Integrating%20large%20language%20models%20%28LLMs%29%20in%20autonomous%20vehicles%20enables%0Aconversation%20with%20AI%20systems%20to%20drive%20the%20vehicle.%20However%2C%20it%20also%20emphasizes%0Athe%20requirement%20for%20such%20systems%20to%20comprehend%20commands%20accurately%20and%20achieve%0Ahigher-level%20personalization%20to%20adapt%20to%20the%20preferences%20of%20drivers%20or%0Apassengers%20over%20a%20more%20extended%20period.%20In%20this%20paper%2C%20we%20introduce%20an%0ALLM-based%20framework%2C%20Talk2Drive%2C%20capable%20of%20translating%20natural%20verbal%20commands%0Ainto%20executable%20controls%20and%20learning%20to%20satisfy%20personal%20preferences%20for%0Asafety%2C%20efficiency%2C%20and%20comfort%20with%20a%20proposed%20memory%20module.%20This%20is%20the%0Afirst-of-its-kind%20multi-scenario%20field%20experiment%20that%20deploys%20LLMs%20on%20a%0Areal-world%20autonomous%20vehicle.%20Experiments%20showcase%20that%20the%20proposed%20system%0Acan%20comprehend%20human%20intentions%20at%20different%20intuition%20levels%2C%20ranging%20from%0Adirect%20commands%20like%20%22can%20you%20drive%20faster%22%20to%20indirect%20commands%20like%20%22I%20am%0Areally%20in%20a%20hurry%20now%22.%20Additionally%2C%20we%20use%20the%20takeover%20rate%20to%20quantify%20the%0Atrust%20of%20human%20drivers%20in%20the%20LLM-based%20autonomous%20driving%20system%2C%20where%0ATalk2Drive%20significantly%20reduces%20the%20takeover%20rate%20in%20highway%2C%20intersection%2C%0Aand%20parking%20scenarios.%20We%20also%20validate%20that%20the%20proposed%20memory%20module%0Aconsiders%20personalized%20preferences%20and%20further%20reduces%20the%20takeover%20rate%20by%20up%0Ato%2065.2%25%20compared%20with%20those%20without%20a%20memory%20module.%20The%20experiment%20video%20can%0Abe%20watched%20at%20https%3A//www.youtube.com/watch%3Fv%3D4BWsfPaq1Ro%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09397v3&entry.124074799=Read"},
{"title": "Cellular Traffic Prediction Using Online Prediction Algorithms", "author": "Hossein Mehri and Hao Chen and Hani Mehrpouyan", "abstract": "  The advent of 5G technology promises a paradigm shift in the realm of\ntelecommunications, offering unprecedented speeds and connectivity. However,\nthe efficient management of traffic in 5G networks remains a critical\nchallenge. It is due to the dynamic and heterogeneous nature of network\ntraffic, varying user behaviors, extended network size, and diverse\napplications, all of which demand highly accurate and adaptable prediction\nmodels to optimize network resource allocation and management. This paper\ninvestigates the efficacy of live prediction algorithms for forecasting\ncellular network traffic in real-time scenarios. We apply two live prediction\nalgorithms on machine learning models, one of which is recently proposed Fast\nLiveStream Prediction (FLSP) algorithm. We examine the performance of these\nalgorithms under two distinct data gathering methodologies: synchronous, where\nall network cells report statistics simultaneously, and asynchronous, where\nreporting occurs across consecutive time slots. Our study delves into the\nimpact of these gathering scenarios on the predictive performance of traffic\nmodels. Our study reveals that the FLSP algorithm can halve the required\nbandwidth for asynchronous data reporting compared to conventional online\nprediction algorithms, while simultaneously enhancing prediction accuracy and\nreducing processing load. Additionally, we conduct a thorough analysis of\nalgorithmic complexity and memory requirements across various machine learning\nmodels. Through empirical evaluation, we provide insights into the trade-offs\ninherent in different prediction strategies, offering valuable guidance for\nnetwork optimization and resource allocation in dynamic environments.\n", "link": "http://arxiv.org/abs/2405.05239v1", "date": "2024-05-08", "relevancy": 1.3742, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4706}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4619}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cellular%20Traffic%20Prediction%20Using%20Online%20Prediction%20Algorithms&body=Title%3A%20Cellular%20Traffic%20Prediction%20Using%20Online%20Prediction%20Algorithms%0AAuthor%3A%20Hossein%20Mehri%20and%20Hao%20Chen%20and%20Hani%20Mehrpouyan%0AAbstract%3A%20%20%20The%20advent%20of%205G%20technology%20promises%20a%20paradigm%20shift%20in%20the%20realm%20of%0Atelecommunications%2C%20offering%20unprecedented%20speeds%20and%20connectivity.%20However%2C%0Athe%20efficient%20management%20of%20traffic%20in%205G%20networks%20remains%20a%20critical%0Achallenge.%20It%20is%20due%20to%20the%20dynamic%20and%20heterogeneous%20nature%20of%20network%0Atraffic%2C%20varying%20user%20behaviors%2C%20extended%20network%20size%2C%20and%20diverse%0Aapplications%2C%20all%20of%20which%20demand%20highly%20accurate%20and%20adaptable%20prediction%0Amodels%20to%20optimize%20network%20resource%20allocation%20and%20management.%20This%20paper%0Ainvestigates%20the%20efficacy%20of%20live%20prediction%20algorithms%20for%20forecasting%0Acellular%20network%20traffic%20in%20real-time%20scenarios.%20We%20apply%20two%20live%20prediction%0Aalgorithms%20on%20machine%20learning%20models%2C%20one%20of%20which%20is%20recently%20proposed%20Fast%0ALiveStream%20Prediction%20%28FLSP%29%20algorithm.%20We%20examine%20the%20performance%20of%20these%0Aalgorithms%20under%20two%20distinct%20data%20gathering%20methodologies%3A%20synchronous%2C%20where%0Aall%20network%20cells%20report%20statistics%20simultaneously%2C%20and%20asynchronous%2C%20where%0Areporting%20occurs%20across%20consecutive%20time%20slots.%20Our%20study%20delves%20into%20the%0Aimpact%20of%20these%20gathering%20scenarios%20on%20the%20predictive%20performance%20of%20traffic%0Amodels.%20Our%20study%20reveals%20that%20the%20FLSP%20algorithm%20can%20halve%20the%20required%0Abandwidth%20for%20asynchronous%20data%20reporting%20compared%20to%20conventional%20online%0Aprediction%20algorithms%2C%20while%20simultaneously%20enhancing%20prediction%20accuracy%20and%0Areducing%20processing%20load.%20Additionally%2C%20we%20conduct%20a%20thorough%20analysis%20of%0Aalgorithmic%20complexity%20and%20memory%20requirements%20across%20various%20machine%20learning%0Amodels.%20Through%20empirical%20evaluation%2C%20we%20provide%20insights%20into%20the%20trade-offs%0Ainherent%20in%20different%20prediction%20strategies%2C%20offering%20valuable%20guidance%20for%0Anetwork%20optimization%20and%20resource%20allocation%20in%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCellular%2520Traffic%2520Prediction%2520Using%2520Online%2520Prediction%2520Algorithms%26entry.906535625%3DHossein%2520Mehri%2520and%2520Hao%2520Chen%2520and%2520Hani%2520Mehrpouyan%26entry.1292438233%3D%2520%2520The%2520advent%2520of%25205G%2520technology%2520promises%2520a%2520paradigm%2520shift%2520in%2520the%2520realm%2520of%250Atelecommunications%252C%2520offering%2520unprecedented%2520speeds%2520and%2520connectivity.%2520However%252C%250Athe%2520efficient%2520management%2520of%2520traffic%2520in%25205G%2520networks%2520remains%2520a%2520critical%250Achallenge.%2520It%2520is%2520due%2520to%2520the%2520dynamic%2520and%2520heterogeneous%2520nature%2520of%2520network%250Atraffic%252C%2520varying%2520user%2520behaviors%252C%2520extended%2520network%2520size%252C%2520and%2520diverse%250Aapplications%252C%2520all%2520of%2520which%2520demand%2520highly%2520accurate%2520and%2520adaptable%2520prediction%250Amodels%2520to%2520optimize%2520network%2520resource%2520allocation%2520and%2520management.%2520This%2520paper%250Ainvestigates%2520the%2520efficacy%2520of%2520live%2520prediction%2520algorithms%2520for%2520forecasting%250Acellular%2520network%2520traffic%2520in%2520real-time%2520scenarios.%2520We%2520apply%2520two%2520live%2520prediction%250Aalgorithms%2520on%2520machine%2520learning%2520models%252C%2520one%2520of%2520which%2520is%2520recently%2520proposed%2520Fast%250ALiveStream%2520Prediction%2520%2528FLSP%2529%2520algorithm.%2520We%2520examine%2520the%2520performance%2520of%2520these%250Aalgorithms%2520under%2520two%2520distinct%2520data%2520gathering%2520methodologies%253A%2520synchronous%252C%2520where%250Aall%2520network%2520cells%2520report%2520statistics%2520simultaneously%252C%2520and%2520asynchronous%252C%2520where%250Areporting%2520occurs%2520across%2520consecutive%2520time%2520slots.%2520Our%2520study%2520delves%2520into%2520the%250Aimpact%2520of%2520these%2520gathering%2520scenarios%2520on%2520the%2520predictive%2520performance%2520of%2520traffic%250Amodels.%2520Our%2520study%2520reveals%2520that%2520the%2520FLSP%2520algorithm%2520can%2520halve%2520the%2520required%250Abandwidth%2520for%2520asynchronous%2520data%2520reporting%2520compared%2520to%2520conventional%2520online%250Aprediction%2520algorithms%252C%2520while%2520simultaneously%2520enhancing%2520prediction%2520accuracy%2520and%250Areducing%2520processing%2520load.%2520Additionally%252C%2520we%2520conduct%2520a%2520thorough%2520analysis%2520of%250Aalgorithmic%2520complexity%2520and%2520memory%2520requirements%2520across%2520various%2520machine%2520learning%250Amodels.%2520Through%2520empirical%2520evaluation%252C%2520we%2520provide%2520insights%2520into%2520the%2520trade-offs%250Ainherent%2520in%2520different%2520prediction%2520strategies%252C%2520offering%2520valuable%2520guidance%2520for%250Anetwork%2520optimization%2520and%2520resource%2520allocation%2520in%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cellular%20Traffic%20Prediction%20Using%20Online%20Prediction%20Algorithms&entry.906535625=Hossein%20Mehri%20and%20Hao%20Chen%20and%20Hani%20Mehrpouyan&entry.1292438233=%20%20The%20advent%20of%205G%20technology%20promises%20a%20paradigm%20shift%20in%20the%20realm%20of%0Atelecommunications%2C%20offering%20unprecedented%20speeds%20and%20connectivity.%20However%2C%0Athe%20efficient%20management%20of%20traffic%20in%205G%20networks%20remains%20a%20critical%0Achallenge.%20It%20is%20due%20to%20the%20dynamic%20and%20heterogeneous%20nature%20of%20network%0Atraffic%2C%20varying%20user%20behaviors%2C%20extended%20network%20size%2C%20and%20diverse%0Aapplications%2C%20all%20of%20which%20demand%20highly%20accurate%20and%20adaptable%20prediction%0Amodels%20to%20optimize%20network%20resource%20allocation%20and%20management.%20This%20paper%0Ainvestigates%20the%20efficacy%20of%20live%20prediction%20algorithms%20for%20forecasting%0Acellular%20network%20traffic%20in%20real-time%20scenarios.%20We%20apply%20two%20live%20prediction%0Aalgorithms%20on%20machine%20learning%20models%2C%20one%20of%20which%20is%20recently%20proposed%20Fast%0ALiveStream%20Prediction%20%28FLSP%29%20algorithm.%20We%20examine%20the%20performance%20of%20these%0Aalgorithms%20under%20two%20distinct%20data%20gathering%20methodologies%3A%20synchronous%2C%20where%0Aall%20network%20cells%20report%20statistics%20simultaneously%2C%20and%20asynchronous%2C%20where%0Areporting%20occurs%20across%20consecutive%20time%20slots.%20Our%20study%20delves%20into%20the%0Aimpact%20of%20these%20gathering%20scenarios%20on%20the%20predictive%20performance%20of%20traffic%0Amodels.%20Our%20study%20reveals%20that%20the%20FLSP%20algorithm%20can%20halve%20the%20required%0Abandwidth%20for%20asynchronous%20data%20reporting%20compared%20to%20conventional%20online%0Aprediction%20algorithms%2C%20while%20simultaneously%20enhancing%20prediction%20accuracy%20and%0Areducing%20processing%20load.%20Additionally%2C%20we%20conduct%20a%20thorough%20analysis%20of%0Aalgorithmic%20complexity%20and%20memory%20requirements%20across%20various%20machine%20learning%0Amodels.%20Through%20empirical%20evaluation%2C%20we%20provide%20insights%20into%20the%20trade-offs%0Ainherent%20in%20different%20prediction%20strategies%2C%20offering%20valuable%20guidance%20for%0Anetwork%20optimization%20and%20resource%20allocation%20in%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05239v1&entry.124074799=Read"},
{"title": "Few-Shot Detection of Machine-Generated Text using Style Representations", "author": "Rafael Rivera Soto and Kailin Koch and Aleem Khan and Barry Chen and Marcus Bishop and Nicholas Andrews", "abstract": "  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n", "link": "http://arxiv.org/abs/2401.06712v3", "date": "2024-05-08", "relevancy": 1.5495, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5217}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5135}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Detection%20of%20Machine-Generated%20Text%20using%20Style%20Representations&body=Title%3A%20Few-Shot%20Detection%20of%20Machine-Generated%20Text%20using%20Style%20Representations%0AAuthor%3A%20Rafael%20Rivera%20Soto%20and%20Kailin%20Koch%20and%20Aleem%20Khan%20and%20Barry%20Chen%20and%20Marcus%20Bishop%20and%20Nicholas%20Andrews%0AAbstract%3A%20%20%20The%20advent%20of%20instruction-tuned%20language%20models%20that%20convincingly%20mimic%20human%0Awriting%20poses%20a%20significant%20risk%20of%20abuse.%20However%2C%20such%20abuse%20may%20be%0Acounteracted%20with%20the%20ability%20to%20detect%20whether%20a%20piece%20of%20text%20was%20composed%20by%0Aa%20language%20model%20rather%20than%20a%20human%20author.%20Some%20previous%20approaches%20to%20this%0Aproblem%20have%20relied%20on%20supervised%20methods%20by%20training%20on%20corpora%20of%20confirmed%0Ahuman-%20and%20machine-%20written%20documents.%20Unfortunately%2C%20model%20under-specification%0Aposes%20an%20unavoidable%20challenge%20for%20neural%20network-based%20detectors%2C%20making%20them%0Abrittle%20in%20the%20face%20of%20data%20shifts%2C%20such%20as%20the%20release%20of%20newer%20language%0Amodels%20producing%20still%20more%20fluent%20text%20than%20the%20models%20used%20to%20train%20the%0Adetectors.%20Other%20approaches%20require%20access%20to%20the%20models%20that%20may%20have%0Agenerated%20a%20document%20in%20question%2C%20which%20is%20often%20impractical.%20In%20light%20of%20these%0Achallenges%2C%20we%20pursue%20a%20fundamentally%20different%20approach%20not%20relying%20on%20samples%0Afrom%20language%20models%20of%20concern%20at%20training%20time.%20Instead%2C%20we%20propose%20to%0Aleverage%20representations%20of%20writing%20style%20estimated%20from%20human-authored%20text.%0AIndeed%2C%20we%20find%20that%20features%20effective%20at%20distinguishing%20among%20human%20authors%0Aare%20also%20effective%20at%20distinguishing%20human%20from%20machine%20authors%2C%20including%0Astate-of-the-art%20large%20language%20models%20like%20Llama-2%2C%20ChatGPT%2C%20and%20GPT-4.%0AFurthermore%2C%20given%20a%20handful%20of%20examples%20composed%20by%20each%20of%20several%20specific%0Alanguage%20models%20of%20interest%2C%20our%20approach%20affords%20the%20ability%20to%20predict%20which%0Amodel%20generated%20a%20given%20document.%20The%20code%20and%20data%20to%20reproduce%20our%0Aexperiments%20are%20available%20at%0Ahttps%3A//github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06712v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Detection%2520of%2520Machine-Generated%2520Text%2520using%2520Style%2520Representations%26entry.906535625%3DRafael%2520Rivera%2520Soto%2520and%2520Kailin%2520Koch%2520and%2520Aleem%2520Khan%2520and%2520Barry%2520Chen%2520and%2520Marcus%2520Bishop%2520and%2520Nicholas%2520Andrews%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520instruction-tuned%2520language%2520models%2520that%2520convincingly%2520mimic%2520human%250Awriting%2520poses%2520a%2520significant%2520risk%2520of%2520abuse.%2520However%252C%2520such%2520abuse%2520may%2520be%250Acounteracted%2520with%2520the%2520ability%2520to%2520detect%2520whether%2520a%2520piece%2520of%2520text%2520was%2520composed%2520by%250Aa%2520language%2520model%2520rather%2520than%2520a%2520human%2520author.%2520Some%2520previous%2520approaches%2520to%2520this%250Aproblem%2520have%2520relied%2520on%2520supervised%2520methods%2520by%2520training%2520on%2520corpora%2520of%2520confirmed%250Ahuman-%2520and%2520machine-%2520written%2520documents.%2520Unfortunately%252C%2520model%2520under-specification%250Aposes%2520an%2520unavoidable%2520challenge%2520for%2520neural%2520network-based%2520detectors%252C%2520making%2520them%250Abrittle%2520in%2520the%2520face%2520of%2520data%2520shifts%252C%2520such%2520as%2520the%2520release%2520of%2520newer%2520language%250Amodels%2520producing%2520still%2520more%2520fluent%2520text%2520than%2520the%2520models%2520used%2520to%2520train%2520the%250Adetectors.%2520Other%2520approaches%2520require%2520access%2520to%2520the%2520models%2520that%2520may%2520have%250Agenerated%2520a%2520document%2520in%2520question%252C%2520which%2520is%2520often%2520impractical.%2520In%2520light%2520of%2520these%250Achallenges%252C%2520we%2520pursue%2520a%2520fundamentally%2520different%2520approach%2520not%2520relying%2520on%2520samples%250Afrom%2520language%2520models%2520of%2520concern%2520at%2520training%2520time.%2520Instead%252C%2520we%2520propose%2520to%250Aleverage%2520representations%2520of%2520writing%2520style%2520estimated%2520from%2520human-authored%2520text.%250AIndeed%252C%2520we%2520find%2520that%2520features%2520effective%2520at%2520distinguishing%2520among%2520human%2520authors%250Aare%2520also%2520effective%2520at%2520distinguishing%2520human%2520from%2520machine%2520authors%252C%2520including%250Astate-of-the-art%2520large%2520language%2520models%2520like%2520Llama-2%252C%2520ChatGPT%252C%2520and%2520GPT-4.%250AFurthermore%252C%2520given%2520a%2520handful%2520of%2520examples%2520composed%2520by%2520each%2520of%2520several%2520specific%250Alanguage%2520models%2520of%2520interest%252C%2520our%2520approach%2520affords%2520the%2520ability%2520to%2520predict%2520which%250Amodel%2520generated%2520a%2520given%2520document.%2520The%2520code%2520and%2520data%2520to%2520reproduce%2520our%250Aexperiments%2520are%2520available%2520at%250Ahttps%253A//github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06712v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Detection%20of%20Machine-Generated%20Text%20using%20Style%20Representations&entry.906535625=Rafael%20Rivera%20Soto%20and%20Kailin%20Koch%20and%20Aleem%20Khan%20and%20Barry%20Chen%20and%20Marcus%20Bishop%20and%20Nicholas%20Andrews&entry.1292438233=%20%20The%20advent%20of%20instruction-tuned%20language%20models%20that%20convincingly%20mimic%20human%0Awriting%20poses%20a%20significant%20risk%20of%20abuse.%20However%2C%20such%20abuse%20may%20be%0Acounteracted%20with%20the%20ability%20to%20detect%20whether%20a%20piece%20of%20text%20was%20composed%20by%0Aa%20language%20model%20rather%20than%20a%20human%20author.%20Some%20previous%20approaches%20to%20this%0Aproblem%20have%20relied%20on%20supervised%20methods%20by%20training%20on%20corpora%20of%20confirmed%0Ahuman-%20and%20machine-%20written%20documents.%20Unfortunately%2C%20model%20under-specification%0Aposes%20an%20unavoidable%20challenge%20for%20neural%20network-based%20detectors%2C%20making%20them%0Abrittle%20in%20the%20face%20of%20data%20shifts%2C%20such%20as%20the%20release%20of%20newer%20language%0Amodels%20producing%20still%20more%20fluent%20text%20than%20the%20models%20used%20to%20train%20the%0Adetectors.%20Other%20approaches%20require%20access%20to%20the%20models%20that%20may%20have%0Agenerated%20a%20document%20in%20question%2C%20which%20is%20often%20impractical.%20In%20light%20of%20these%0Achallenges%2C%20we%20pursue%20a%20fundamentally%20different%20approach%20not%20relying%20on%20samples%0Afrom%20language%20models%20of%20concern%20at%20training%20time.%20Instead%2C%20we%20propose%20to%0Aleverage%20representations%20of%20writing%20style%20estimated%20from%20human-authored%20text.%0AIndeed%2C%20we%20find%20that%20features%20effective%20at%20distinguishing%20among%20human%20authors%0Aare%20also%20effective%20at%20distinguishing%20human%20from%20machine%20authors%2C%20including%0Astate-of-the-art%20large%20language%20models%20like%20Llama-2%2C%20ChatGPT%2C%20and%20GPT-4.%0AFurthermore%2C%20given%20a%20handful%20of%20examples%20composed%20by%20each%20of%20several%20specific%0Alanguage%20models%20of%20interest%2C%20our%20approach%20affords%20the%20ability%20to%20predict%20which%0Amodel%20generated%20a%20given%20document.%20The%20code%20and%20data%20to%20reproduce%20our%0Aexperiments%20are%20available%20at%0Ahttps%3A//github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06712v3&entry.124074799=Read"},
{"title": "Concerns on Bias in Large Language Models when Creating Synthetic\n  Personae", "author": "Helena A. Haxvig", "abstract": "  This position paper explores the benefits, drawbacks, and ethical\nconsiderations of incorporating synthetic personae in HCI research,\nparticularly focusing on the customization challenges beyond the limitations of\ncurrent Large Language Models (LLMs). These perspectives are derived from the\ninitial results of a sub-study employing vignettes to showcase the existence of\nbias within black-box LLMs and explore methods for manipulating them. The study\naims to establish a foundation for understanding the challenges associated with\nthese models, emphasizing the necessity of thorough testing before utilizing\nthem to create synthetic personae for HCI research.\n", "link": "http://arxiv.org/abs/2405.05080v1", "date": "2024-05-08", "relevancy": 0.9458, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4807}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4695}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concerns%20on%20Bias%20in%20Large%20Language%20Models%20when%20Creating%20Synthetic%0A%20%20Personae&body=Title%3A%20Concerns%20on%20Bias%20in%20Large%20Language%20Models%20when%20Creating%20Synthetic%0A%20%20Personae%0AAuthor%3A%20Helena%20A.%20Haxvig%0AAbstract%3A%20%20%20This%20position%20paper%20explores%20the%20benefits%2C%20drawbacks%2C%20and%20ethical%0Aconsiderations%20of%20incorporating%20synthetic%20personae%20in%20HCI%20research%2C%0Aparticularly%20focusing%20on%20the%20customization%20challenges%20beyond%20the%20limitations%20of%0Acurrent%20Large%20Language%20Models%20%28LLMs%29.%20These%20perspectives%20are%20derived%20from%20the%0Ainitial%20results%20of%20a%20sub-study%20employing%20vignettes%20to%20showcase%20the%20existence%20of%0Abias%20within%20black-box%20LLMs%20and%20explore%20methods%20for%20manipulating%20them.%20The%20study%0Aaims%20to%20establish%20a%20foundation%20for%20understanding%20the%20challenges%20associated%20with%0Athese%20models%2C%20emphasizing%20the%20necessity%20of%20thorough%20testing%20before%20utilizing%0Athem%20to%20create%20synthetic%20personae%20for%20HCI%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcerns%2520on%2520Bias%2520in%2520Large%2520Language%2520Models%2520when%2520Creating%2520Synthetic%250A%2520%2520Personae%26entry.906535625%3DHelena%2520A.%2520Haxvig%26entry.1292438233%3D%2520%2520This%2520position%2520paper%2520explores%2520the%2520benefits%252C%2520drawbacks%252C%2520and%2520ethical%250Aconsiderations%2520of%2520incorporating%2520synthetic%2520personae%2520in%2520HCI%2520research%252C%250Aparticularly%2520focusing%2520on%2520the%2520customization%2520challenges%2520beyond%2520the%2520limitations%2520of%250Acurrent%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520These%2520perspectives%2520are%2520derived%2520from%2520the%250Ainitial%2520results%2520of%2520a%2520sub-study%2520employing%2520vignettes%2520to%2520showcase%2520the%2520existence%2520of%250Abias%2520within%2520black-box%2520LLMs%2520and%2520explore%2520methods%2520for%2520manipulating%2520them.%2520The%2520study%250Aaims%2520to%2520establish%2520a%2520foundation%2520for%2520understanding%2520the%2520challenges%2520associated%2520with%250Athese%2520models%252C%2520emphasizing%2520the%2520necessity%2520of%2520thorough%2520testing%2520before%2520utilizing%250Athem%2520to%2520create%2520synthetic%2520personae%2520for%2520HCI%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concerns%20on%20Bias%20in%20Large%20Language%20Models%20when%20Creating%20Synthetic%0A%20%20Personae&entry.906535625=Helena%20A.%20Haxvig&entry.1292438233=%20%20This%20position%20paper%20explores%20the%20benefits%2C%20drawbacks%2C%20and%20ethical%0Aconsiderations%20of%20incorporating%20synthetic%20personae%20in%20HCI%20research%2C%0Aparticularly%20focusing%20on%20the%20customization%20challenges%20beyond%20the%20limitations%20of%0Acurrent%20Large%20Language%20Models%20%28LLMs%29.%20These%20perspectives%20are%20derived%20from%20the%0Ainitial%20results%20of%20a%20sub-study%20employing%20vignettes%20to%20showcase%20the%20existence%20of%0Abias%20within%20black-box%20LLMs%20and%20explore%20methods%20for%20manipulating%20them.%20The%20study%0Aaims%20to%20establish%20a%20foundation%20for%20understanding%20the%20challenges%20associated%20with%0Athese%20models%2C%20emphasizing%20the%20necessity%20of%20thorough%20testing%20before%20utilizing%0Athem%20to%20create%20synthetic%20personae%20for%20HCI%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05080v1&entry.124074799=Read"},
{"title": "Enhancing Social Media Post Popularity Prediction with Visual Content", "author": "Dahyun Jeong and Hyelim Son and Yunjin Choi and Keunwoo Kim", "abstract": "  Our study presents a framework for predicting image-based social media\ncontent popularity that focuses on addressing complex image information and a\nhierarchical data structure. We utilize the Google Cloud Vision API to\neffectively extract key image and color information from users' postings,\nachieving 6.8% higher accuracy compared to using non-image covariates alone.\nFor prediction, we explore a wide range of prediction models, including Linear\nMixed Model, Support Vector Regression, Multi-layer Perceptron, Random Forest,\nand XGBoost, with linear regression as the benchmark. Our comparative study\ndemonstrates that models that are capable of capturing the underlying nonlinear\ninteractions between covariates outperform other methods.\n", "link": "http://arxiv.org/abs/2405.02367v2", "date": "2024-05-08", "relevancy": 1.5169, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5454}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4968}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Social%20Media%20Post%20Popularity%20Prediction%20with%20Visual%20Content&body=Title%3A%20Enhancing%20Social%20Media%20Post%20Popularity%20Prediction%20with%20Visual%20Content%0AAuthor%3A%20Dahyun%20Jeong%20and%20Hyelim%20Son%20and%20Yunjin%20Choi%20and%20Keunwoo%20Kim%0AAbstract%3A%20%20%20Our%20study%20presents%20a%20framework%20for%20predicting%20image-based%20social%20media%0Acontent%20popularity%20that%20focuses%20on%20addressing%20complex%20image%20information%20and%20a%0Ahierarchical%20data%20structure.%20We%20utilize%20the%20Google%20Cloud%20Vision%20API%20to%0Aeffectively%20extract%20key%20image%20and%20color%20information%20from%20users%27%20postings%2C%0Aachieving%206.8%25%20higher%20accuracy%20compared%20to%20using%20non-image%20covariates%20alone.%0AFor%20prediction%2C%20we%20explore%20a%20wide%20range%20of%20prediction%20models%2C%20including%20Linear%0AMixed%20Model%2C%20Support%20Vector%20Regression%2C%20Multi-layer%20Perceptron%2C%20Random%20Forest%2C%0Aand%20XGBoost%2C%20with%20linear%20regression%20as%20the%20benchmark.%20Our%20comparative%20study%0Ademonstrates%20that%20models%20that%20are%20capable%20of%20capturing%20the%20underlying%20nonlinear%0Ainteractions%20between%20covariates%20outperform%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02367v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Social%2520Media%2520Post%2520Popularity%2520Prediction%2520with%2520Visual%2520Content%26entry.906535625%3DDahyun%2520Jeong%2520and%2520Hyelim%2520Son%2520and%2520Yunjin%2520Choi%2520and%2520Keunwoo%2520Kim%26entry.1292438233%3D%2520%2520Our%2520study%2520presents%2520a%2520framework%2520for%2520predicting%2520image-based%2520social%2520media%250Acontent%2520popularity%2520that%2520focuses%2520on%2520addressing%2520complex%2520image%2520information%2520and%2520a%250Ahierarchical%2520data%2520structure.%2520We%2520utilize%2520the%2520Google%2520Cloud%2520Vision%2520API%2520to%250Aeffectively%2520extract%2520key%2520image%2520and%2520color%2520information%2520from%2520users%2527%2520postings%252C%250Aachieving%25206.8%2525%2520higher%2520accuracy%2520compared%2520to%2520using%2520non-image%2520covariates%2520alone.%250AFor%2520prediction%252C%2520we%2520explore%2520a%2520wide%2520range%2520of%2520prediction%2520models%252C%2520including%2520Linear%250AMixed%2520Model%252C%2520Support%2520Vector%2520Regression%252C%2520Multi-layer%2520Perceptron%252C%2520Random%2520Forest%252C%250Aand%2520XGBoost%252C%2520with%2520linear%2520regression%2520as%2520the%2520benchmark.%2520Our%2520comparative%2520study%250Ademonstrates%2520that%2520models%2520that%2520are%2520capable%2520of%2520capturing%2520the%2520underlying%2520nonlinear%250Ainteractions%2520between%2520covariates%2520outperform%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02367v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Social%20Media%20Post%20Popularity%20Prediction%20with%20Visual%20Content&entry.906535625=Dahyun%20Jeong%20and%20Hyelim%20Son%20and%20Yunjin%20Choi%20and%20Keunwoo%20Kim&entry.1292438233=%20%20Our%20study%20presents%20a%20framework%20for%20predicting%20image-based%20social%20media%0Acontent%20popularity%20that%20focuses%20on%20addressing%20complex%20image%20information%20and%20a%0Ahierarchical%20data%20structure.%20We%20utilize%20the%20Google%20Cloud%20Vision%20API%20to%0Aeffectively%20extract%20key%20image%20and%20color%20information%20from%20users%27%20postings%2C%0Aachieving%206.8%25%20higher%20accuracy%20compared%20to%20using%20non-image%20covariates%20alone.%0AFor%20prediction%2C%20we%20explore%20a%20wide%20range%20of%20prediction%20models%2C%20including%20Linear%0AMixed%20Model%2C%20Support%20Vector%20Regression%2C%20Multi-layer%20Perceptron%2C%20Random%20Forest%2C%0Aand%20XGBoost%2C%20with%20linear%20regression%20as%20the%20benchmark.%20Our%20comparative%20study%0Ademonstrates%20that%20models%20that%20are%20capable%20of%20capturing%20the%20underlying%20nonlinear%0Ainteractions%20between%20covariates%20outperform%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02367v2&entry.124074799=Read"},
{"title": "Unravelling Responsibility for AI", "author": "Zoe Porter and Philippa Ryan and Phillip Morgan and Joanna Al-Qaddoumi and Bernard Twomey and John McDermid and Ibrahim Habli", "abstract": "  It is widely acknowledged that we need to establish where responsibility lies\nfor the outputs and impacts of AI-enabled systems. But without a clear and\nprecise understanding of what \"responsibility\" means, deliberations about where\nresponsibility lies will be, at best, unfocused and incomplete and, at worst,\nmisguided. To address this concern, this paper draws upon central distinctions\nin philosophy and law to clarify the concept of responsibility for AI for\npolicymakers, practitioners, researchers and students from non-philosophical\nand non-legal backgrounds. Taking the three-part formulation \"Actor A is\nresponsible for Occurrence O,\" the paper unravels the concept of responsibility\nto clarify that there are different possibilities of who is responsible for AI,\nthe senses in which they are responsible, and aspects of events they are\nresponsible for. Criteria and conditions for fitting attributions of\nresponsibility in the core senses (causal responsibility, role-responsibility,\nliability responsibility and moral responsibility) are articulated to promote\nan understanding of when responsibility attributions would be inappropriate or\nunjust. The analysis is presented with a graphical notation to facilitate\ninformal diagrammatic reasoning and discussion about specific cases. It is\nillustrated by application to a scenario of a fatal collision between an\nautonomous AI-enabled ship and a traditional, crewed vessel at sea.\n", "link": "http://arxiv.org/abs/2308.02608v2", "date": "2024-05-08", "relevancy": 1.6491, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4368}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4094}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unravelling%20Responsibility%20for%20AI&body=Title%3A%20Unravelling%20Responsibility%20for%20AI%0AAuthor%3A%20Zoe%20Porter%20and%20Philippa%20Ryan%20and%20Phillip%20Morgan%20and%20Joanna%20Al-Qaddoumi%20and%20Bernard%20Twomey%20and%20John%20McDermid%20and%20Ibrahim%20Habli%0AAbstract%3A%20%20%20It%20is%20widely%20acknowledged%20that%20we%20need%20to%20establish%20where%20responsibility%20lies%0Afor%20the%20outputs%20and%20impacts%20of%20AI-enabled%20systems.%20But%20without%20a%20clear%20and%0Aprecise%20understanding%20of%20what%20%22responsibility%22%20means%2C%20deliberations%20about%20where%0Aresponsibility%20lies%20will%20be%2C%20at%20best%2C%20unfocused%20and%20incomplete%20and%2C%20at%20worst%2C%0Amisguided.%20To%20address%20this%20concern%2C%20this%20paper%20draws%20upon%20central%20distinctions%0Ain%20philosophy%20and%20law%20to%20clarify%20the%20concept%20of%20responsibility%20for%20AI%20for%0Apolicymakers%2C%20practitioners%2C%20researchers%20and%20students%20from%20non-philosophical%0Aand%20non-legal%20backgrounds.%20Taking%20the%20three-part%20formulation%20%22Actor%20A%20is%0Aresponsible%20for%20Occurrence%20O%2C%22%20the%20paper%20unravels%20the%20concept%20of%20responsibility%0Ato%20clarify%20that%20there%20are%20different%20possibilities%20of%20who%20is%20responsible%20for%20AI%2C%0Athe%20senses%20in%20which%20they%20are%20responsible%2C%20and%20aspects%20of%20events%20they%20are%0Aresponsible%20for.%20Criteria%20and%20conditions%20for%20fitting%20attributions%20of%0Aresponsibility%20in%20the%20core%20senses%20%28causal%20responsibility%2C%20role-responsibility%2C%0Aliability%20responsibility%20and%20moral%20responsibility%29%20are%20articulated%20to%20promote%0Aan%20understanding%20of%20when%20responsibility%20attributions%20would%20be%20inappropriate%20or%0Aunjust.%20The%20analysis%20is%20presented%20with%20a%20graphical%20notation%20to%20facilitate%0Ainformal%20diagrammatic%20reasoning%20and%20discussion%20about%20specific%20cases.%20It%20is%0Aillustrated%20by%20application%20to%20a%20scenario%20of%20a%20fatal%20collision%20between%20an%0Aautonomous%20AI-enabled%20ship%20and%20a%20traditional%2C%20crewed%20vessel%20at%20sea.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.02608v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnravelling%2520Responsibility%2520for%2520AI%26entry.906535625%3DZoe%2520Porter%2520and%2520Philippa%2520Ryan%2520and%2520Phillip%2520Morgan%2520and%2520Joanna%2520Al-Qaddoumi%2520and%2520Bernard%2520Twomey%2520and%2520John%2520McDermid%2520and%2520Ibrahim%2520Habli%26entry.1292438233%3D%2520%2520It%2520is%2520widely%2520acknowledged%2520that%2520we%2520need%2520to%2520establish%2520where%2520responsibility%2520lies%250Afor%2520the%2520outputs%2520and%2520impacts%2520of%2520AI-enabled%2520systems.%2520But%2520without%2520a%2520clear%2520and%250Aprecise%2520understanding%2520of%2520what%2520%2522responsibility%2522%2520means%252C%2520deliberations%2520about%2520where%250Aresponsibility%2520lies%2520will%2520be%252C%2520at%2520best%252C%2520unfocused%2520and%2520incomplete%2520and%252C%2520at%2520worst%252C%250Amisguided.%2520To%2520address%2520this%2520concern%252C%2520this%2520paper%2520draws%2520upon%2520central%2520distinctions%250Ain%2520philosophy%2520and%2520law%2520to%2520clarify%2520the%2520concept%2520of%2520responsibility%2520for%2520AI%2520for%250Apolicymakers%252C%2520practitioners%252C%2520researchers%2520and%2520students%2520from%2520non-philosophical%250Aand%2520non-legal%2520backgrounds.%2520Taking%2520the%2520three-part%2520formulation%2520%2522Actor%2520A%2520is%250Aresponsible%2520for%2520Occurrence%2520O%252C%2522%2520the%2520paper%2520unravels%2520the%2520concept%2520of%2520responsibility%250Ato%2520clarify%2520that%2520there%2520are%2520different%2520possibilities%2520of%2520who%2520is%2520responsible%2520for%2520AI%252C%250Athe%2520senses%2520in%2520which%2520they%2520are%2520responsible%252C%2520and%2520aspects%2520of%2520events%2520they%2520are%250Aresponsible%2520for.%2520Criteria%2520and%2520conditions%2520for%2520fitting%2520attributions%2520of%250Aresponsibility%2520in%2520the%2520core%2520senses%2520%2528causal%2520responsibility%252C%2520role-responsibility%252C%250Aliability%2520responsibility%2520and%2520moral%2520responsibility%2529%2520are%2520articulated%2520to%2520promote%250Aan%2520understanding%2520of%2520when%2520responsibility%2520attributions%2520would%2520be%2520inappropriate%2520or%250Aunjust.%2520The%2520analysis%2520is%2520presented%2520with%2520a%2520graphical%2520notation%2520to%2520facilitate%250Ainformal%2520diagrammatic%2520reasoning%2520and%2520discussion%2520about%2520specific%2520cases.%2520It%2520is%250Aillustrated%2520by%2520application%2520to%2520a%2520scenario%2520of%2520a%2520fatal%2520collision%2520between%2520an%250Aautonomous%2520AI-enabled%2520ship%2520and%2520a%2520traditional%252C%2520crewed%2520vessel%2520at%2520sea.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.02608v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unravelling%20Responsibility%20for%20AI&entry.906535625=Zoe%20Porter%20and%20Philippa%20Ryan%20and%20Phillip%20Morgan%20and%20Joanna%20Al-Qaddoumi%20and%20Bernard%20Twomey%20and%20John%20McDermid%20and%20Ibrahim%20Habli&entry.1292438233=%20%20It%20is%20widely%20acknowledged%20that%20we%20need%20to%20establish%20where%20responsibility%20lies%0Afor%20the%20outputs%20and%20impacts%20of%20AI-enabled%20systems.%20But%20without%20a%20clear%20and%0Aprecise%20understanding%20of%20what%20%22responsibility%22%20means%2C%20deliberations%20about%20where%0Aresponsibility%20lies%20will%20be%2C%20at%20best%2C%20unfocused%20and%20incomplete%20and%2C%20at%20worst%2C%0Amisguided.%20To%20address%20this%20concern%2C%20this%20paper%20draws%20upon%20central%20distinctions%0Ain%20philosophy%20and%20law%20to%20clarify%20the%20concept%20of%20responsibility%20for%20AI%20for%0Apolicymakers%2C%20practitioners%2C%20researchers%20and%20students%20from%20non-philosophical%0Aand%20non-legal%20backgrounds.%20Taking%20the%20three-part%20formulation%20%22Actor%20A%20is%0Aresponsible%20for%20Occurrence%20O%2C%22%20the%20paper%20unravels%20the%20concept%20of%20responsibility%0Ato%20clarify%20that%20there%20are%20different%20possibilities%20of%20who%20is%20responsible%20for%20AI%2C%0Athe%20senses%20in%20which%20they%20are%20responsible%2C%20and%20aspects%20of%20events%20they%20are%0Aresponsible%20for.%20Criteria%20and%20conditions%20for%20fitting%20attributions%20of%0Aresponsibility%20in%20the%20core%20senses%20%28causal%20responsibility%2C%20role-responsibility%2C%0Aliability%20responsibility%20and%20moral%20responsibility%29%20are%20articulated%20to%20promote%0Aan%20understanding%20of%20when%20responsibility%20attributions%20would%20be%20inappropriate%20or%0Aunjust.%20The%20analysis%20is%20presented%20with%20a%20graphical%20notation%20to%20facilitate%0Ainformal%20diagrammatic%20reasoning%20and%20discussion%20about%20specific%20cases.%20It%20is%0Aillustrated%20by%20application%20to%20a%20scenario%20of%20a%20fatal%20collision%20between%20an%0Aautonomous%20AI-enabled%20ship%20and%20a%20traditional%2C%20crewed%20vessel%20at%20sea.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.02608v2&entry.124074799=Read"},
{"title": "EVA-X: A Foundation Model for General Chest X-ray Analysis with\n  Self-supervised Learning", "author": "Jingfeng Yao and Xinggang Wang and Yuehao Song and Huangxuan Zhao and Jun Ma and Yajie Chen and Wenyu Liu and Bo Wang", "abstract": "  The diagnosis and treatment of chest diseases play a crucial role in\nmaintaining human health. X-ray examination has become the most common clinical\nexamination means due to its efficiency and cost-effectiveness. Artificial\nintelligence analysis methods for chest X-ray images are limited by\ninsufficient annotation data and varying levels of annotation, resulting in\nweak generalization ability and difficulty in clinical dissemination. Here we\npresent EVA-X, an innovative foundational model based on X-ray images with\nbroad applicability to various chest disease detection tasks. EVA-X is the\nfirst X-ray image based self-supervised learning method capable of capturing\nboth semantic and geometric information from unlabeled images for universal\nX-ray image representation. Through extensive experimentation, EVA-X has\ndemonstrated exceptional performance in chest disease analysis and\nlocalization, becoming the first model capable of spanning over 20 different\nchest diseases and achieving leading results in over 11 different detection\ntasks in the medical field. Additionally, EVA-X significantly reduces the\nburden of data annotation in the medical AI field, showcasing strong potential\nin the domain of few-shot learning. The emergence of EVA-X will greatly propel\nthe development and application of foundational medical models, bringing about\nrevolutionary changes in future medical research and clinical practice. Our\ncodes and models are available at: https://github.com/hustvl/EVA-X.\n", "link": "http://arxiv.org/abs/2405.05237v1", "date": "2024-05-08", "relevancy": 1.5019, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5266}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5016}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVA-X%3A%20A%20Foundation%20Model%20for%20General%20Chest%20X-ray%20Analysis%20with%0A%20%20Self-supervised%20Learning&body=Title%3A%20EVA-X%3A%20A%20Foundation%20Model%20for%20General%20Chest%20X-ray%20Analysis%20with%0A%20%20Self-supervised%20Learning%0AAuthor%3A%20Jingfeng%20Yao%20and%20Xinggang%20Wang%20and%20Yuehao%20Song%20and%20Huangxuan%20Zhao%20and%20Jun%20Ma%20and%20Yajie%20Chen%20and%20Wenyu%20Liu%20and%20Bo%20Wang%0AAbstract%3A%20%20%20The%20diagnosis%20and%20treatment%20of%20chest%20diseases%20play%20a%20crucial%20role%20in%0Amaintaining%20human%20health.%20X-ray%20examination%20has%20become%20the%20most%20common%20clinical%0Aexamination%20means%20due%20to%20its%20efficiency%20and%20cost-effectiveness.%20Artificial%0Aintelligence%20analysis%20methods%20for%20chest%20X-ray%20images%20are%20limited%20by%0Ainsufficient%20annotation%20data%20and%20varying%20levels%20of%20annotation%2C%20resulting%20in%0Aweak%20generalization%20ability%20and%20difficulty%20in%20clinical%20dissemination.%20Here%20we%0Apresent%20EVA-X%2C%20an%20innovative%20foundational%20model%20based%20on%20X-ray%20images%20with%0Abroad%20applicability%20to%20various%20chest%20disease%20detection%20tasks.%20EVA-X%20is%20the%0Afirst%20X-ray%20image%20based%20self-supervised%20learning%20method%20capable%20of%20capturing%0Aboth%20semantic%20and%20geometric%20information%20from%20unlabeled%20images%20for%20universal%0AX-ray%20image%20representation.%20Through%20extensive%20experimentation%2C%20EVA-X%20has%0Ademonstrated%20exceptional%20performance%20in%20chest%20disease%20analysis%20and%0Alocalization%2C%20becoming%20the%20first%20model%20capable%20of%20spanning%20over%2020%20different%0Achest%20diseases%20and%20achieving%20leading%20results%20in%20over%2011%20different%20detection%0Atasks%20in%20the%20medical%20field.%20Additionally%2C%20EVA-X%20significantly%20reduces%20the%0Aburden%20of%20data%20annotation%20in%20the%20medical%20AI%20field%2C%20showcasing%20strong%20potential%0Ain%20the%20domain%20of%20few-shot%20learning.%20The%20emergence%20of%20EVA-X%20will%20greatly%20propel%0Athe%20development%20and%20application%20of%20foundational%20medical%20models%2C%20bringing%20about%0Arevolutionary%20changes%20in%20future%20medical%20research%20and%20clinical%20practice.%20Our%0Acodes%20and%20models%20are%20available%20at%3A%20https%3A//github.com/hustvl/EVA-X.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVA-X%253A%2520A%2520Foundation%2520Model%2520for%2520General%2520Chest%2520X-ray%2520Analysis%2520with%250A%2520%2520Self-supervised%2520Learning%26entry.906535625%3DJingfeng%2520Yao%2520and%2520Xinggang%2520Wang%2520and%2520Yuehao%2520Song%2520and%2520Huangxuan%2520Zhao%2520and%2520Jun%2520Ma%2520and%2520Yajie%2520Chen%2520and%2520Wenyu%2520Liu%2520and%2520Bo%2520Wang%26entry.1292438233%3D%2520%2520The%2520diagnosis%2520and%2520treatment%2520of%2520chest%2520diseases%2520play%2520a%2520crucial%2520role%2520in%250Amaintaining%2520human%2520health.%2520X-ray%2520examination%2520has%2520become%2520the%2520most%2520common%2520clinical%250Aexamination%2520means%2520due%2520to%2520its%2520efficiency%2520and%2520cost-effectiveness.%2520Artificial%250Aintelligence%2520analysis%2520methods%2520for%2520chest%2520X-ray%2520images%2520are%2520limited%2520by%250Ainsufficient%2520annotation%2520data%2520and%2520varying%2520levels%2520of%2520annotation%252C%2520resulting%2520in%250Aweak%2520generalization%2520ability%2520and%2520difficulty%2520in%2520clinical%2520dissemination.%2520Here%2520we%250Apresent%2520EVA-X%252C%2520an%2520innovative%2520foundational%2520model%2520based%2520on%2520X-ray%2520images%2520with%250Abroad%2520applicability%2520to%2520various%2520chest%2520disease%2520detection%2520tasks.%2520EVA-X%2520is%2520the%250Afirst%2520X-ray%2520image%2520based%2520self-supervised%2520learning%2520method%2520capable%2520of%2520capturing%250Aboth%2520semantic%2520and%2520geometric%2520information%2520from%2520unlabeled%2520images%2520for%2520universal%250AX-ray%2520image%2520representation.%2520Through%2520extensive%2520experimentation%252C%2520EVA-X%2520has%250Ademonstrated%2520exceptional%2520performance%2520in%2520chest%2520disease%2520analysis%2520and%250Alocalization%252C%2520becoming%2520the%2520first%2520model%2520capable%2520of%2520spanning%2520over%252020%2520different%250Achest%2520diseases%2520and%2520achieving%2520leading%2520results%2520in%2520over%252011%2520different%2520detection%250Atasks%2520in%2520the%2520medical%2520field.%2520Additionally%252C%2520EVA-X%2520significantly%2520reduces%2520the%250Aburden%2520of%2520data%2520annotation%2520in%2520the%2520medical%2520AI%2520field%252C%2520showcasing%2520strong%2520potential%250Ain%2520the%2520domain%2520of%2520few-shot%2520learning.%2520The%2520emergence%2520of%2520EVA-X%2520will%2520greatly%2520propel%250Athe%2520development%2520and%2520application%2520of%2520foundational%2520medical%2520models%252C%2520bringing%2520about%250Arevolutionary%2520changes%2520in%2520future%2520medical%2520research%2520and%2520clinical%2520practice.%2520Our%250Acodes%2520and%2520models%2520are%2520available%2520at%253A%2520https%253A//github.com/hustvl/EVA-X.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVA-X%3A%20A%20Foundation%20Model%20for%20General%20Chest%20X-ray%20Analysis%20with%0A%20%20Self-supervised%20Learning&entry.906535625=Jingfeng%20Yao%20and%20Xinggang%20Wang%20and%20Yuehao%20Song%20and%20Huangxuan%20Zhao%20and%20Jun%20Ma%20and%20Yajie%20Chen%20and%20Wenyu%20Liu%20and%20Bo%20Wang&entry.1292438233=%20%20The%20diagnosis%20and%20treatment%20of%20chest%20diseases%20play%20a%20crucial%20role%20in%0Amaintaining%20human%20health.%20X-ray%20examination%20has%20become%20the%20most%20common%20clinical%0Aexamination%20means%20due%20to%20its%20efficiency%20and%20cost-effectiveness.%20Artificial%0Aintelligence%20analysis%20methods%20for%20chest%20X-ray%20images%20are%20limited%20by%0Ainsufficient%20annotation%20data%20and%20varying%20levels%20of%20annotation%2C%20resulting%20in%0Aweak%20generalization%20ability%20and%20difficulty%20in%20clinical%20dissemination.%20Here%20we%0Apresent%20EVA-X%2C%20an%20innovative%20foundational%20model%20based%20on%20X-ray%20images%20with%0Abroad%20applicability%20to%20various%20chest%20disease%20detection%20tasks.%20EVA-X%20is%20the%0Afirst%20X-ray%20image%20based%20self-supervised%20learning%20method%20capable%20of%20capturing%0Aboth%20semantic%20and%20geometric%20information%20from%20unlabeled%20images%20for%20universal%0AX-ray%20image%20representation.%20Through%20extensive%20experimentation%2C%20EVA-X%20has%0Ademonstrated%20exceptional%20performance%20in%20chest%20disease%20analysis%20and%0Alocalization%2C%20becoming%20the%20first%20model%20capable%20of%20spanning%20over%2020%20different%0Achest%20diseases%20and%20achieving%20leading%20results%20in%20over%2011%20different%20detection%0Atasks%20in%20the%20medical%20field.%20Additionally%2C%20EVA-X%20significantly%20reduces%20the%0Aburden%20of%20data%20annotation%20in%20the%20medical%20AI%20field%2C%20showcasing%20strong%20potential%0Ain%20the%20domain%20of%20few-shot%20learning.%20The%20emergence%20of%20EVA-X%20will%20greatly%20propel%0Athe%20development%20and%20application%20of%20foundational%20medical%20models%2C%20bringing%20about%0Arevolutionary%20changes%20in%20future%20medical%20research%20and%20clinical%20practice.%20Our%0Acodes%20and%20models%20are%20available%20at%3A%20https%3A//github.com/hustvl/EVA-X.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05237v1&entry.124074799=Read"},
{"title": "Bayesian taut splines for estimating the number of modes", "author": "Jos\u00e9 E. Chac\u00f3n and Javier Fern\u00e1ndez Serrano", "abstract": "  The number of modes in a probability density function is representative of\nthe complexity of a model and can also be viewed as the number of\nsubpopulations. Despite its relevance, there has been limited research in this\narea. A novel approach to estimating the number of modes in the univariate\nsetting is presented, focusing on prediction accuracy and inspired by some\noverlooked aspects of the problem: the need for structure in the solutions, the\nsubjective and uncertain nature of modes, and the convenience of a holistic\nview that blends local and global density properties. The technique combines\nflexible kernel estimators and parsimonious compositional splines in the\nBayesian inference paradigm, providing soft solutions and incorporating expert\njudgment. The procedure includes feature exploration, model selection, and mode\ntesting, illustrated in a sports analytics case study showcasing multiple\ncompanion visualisation tools. A thorough simulation study also demonstrates\nthat traditional modality-driven approaches paradoxically struggle to provide\naccurate results. In this context, the new method emerges as a top-tier\nalternative, offering innovative solutions for analysts.\n", "link": "http://arxiv.org/abs/2307.05825v3", "date": "2024-05-08", "relevancy": 1.3025, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5133}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4289}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20taut%20splines%20for%20estimating%20the%20number%20of%20modes&body=Title%3A%20Bayesian%20taut%20splines%20for%20estimating%20the%20number%20of%20modes%0AAuthor%3A%20Jos%C3%A9%20E.%20Chac%C3%B3n%20and%20Javier%20Fern%C3%A1ndez%20Serrano%0AAbstract%3A%20%20%20The%20number%20of%20modes%20in%20a%20probability%20density%20function%20is%20representative%20of%0Athe%20complexity%20of%20a%20model%20and%20can%20also%20be%20viewed%20as%20the%20number%20of%0Asubpopulations.%20Despite%20its%20relevance%2C%20there%20has%20been%20limited%20research%20in%20this%0Aarea.%20A%20novel%20approach%20to%20estimating%20the%20number%20of%20modes%20in%20the%20univariate%0Asetting%20is%20presented%2C%20focusing%20on%20prediction%20accuracy%20and%20inspired%20by%20some%0Aoverlooked%20aspects%20of%20the%20problem%3A%20the%20need%20for%20structure%20in%20the%20solutions%2C%20the%0Asubjective%20and%20uncertain%20nature%20of%20modes%2C%20and%20the%20convenience%20of%20a%20holistic%0Aview%20that%20blends%20local%20and%20global%20density%20properties.%20The%20technique%20combines%0Aflexible%20kernel%20estimators%20and%20parsimonious%20compositional%20splines%20in%20the%0ABayesian%20inference%20paradigm%2C%20providing%20soft%20solutions%20and%20incorporating%20expert%0Ajudgment.%20The%20procedure%20includes%20feature%20exploration%2C%20model%20selection%2C%20and%20mode%0Atesting%2C%20illustrated%20in%20a%20sports%20analytics%20case%20study%20showcasing%20multiple%0Acompanion%20visualisation%20tools.%20A%20thorough%20simulation%20study%20also%20demonstrates%0Athat%20traditional%20modality-driven%20approaches%20paradoxically%20struggle%20to%20provide%0Aaccurate%20results.%20In%20this%20context%2C%20the%20new%20method%20emerges%20as%20a%20top-tier%0Aalternative%2C%20offering%20innovative%20solutions%20for%20analysts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.05825v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520taut%2520splines%2520for%2520estimating%2520the%2520number%2520of%2520modes%26entry.906535625%3DJos%25C3%25A9%2520E.%2520Chac%25C3%25B3n%2520and%2520Javier%2520Fern%25C3%25A1ndez%2520Serrano%26entry.1292438233%3D%2520%2520The%2520number%2520of%2520modes%2520in%2520a%2520probability%2520density%2520function%2520is%2520representative%2520of%250Athe%2520complexity%2520of%2520a%2520model%2520and%2520can%2520also%2520be%2520viewed%2520as%2520the%2520number%2520of%250Asubpopulations.%2520Despite%2520its%2520relevance%252C%2520there%2520has%2520been%2520limited%2520research%2520in%2520this%250Aarea.%2520A%2520novel%2520approach%2520to%2520estimating%2520the%2520number%2520of%2520modes%2520in%2520the%2520univariate%250Asetting%2520is%2520presented%252C%2520focusing%2520on%2520prediction%2520accuracy%2520and%2520inspired%2520by%2520some%250Aoverlooked%2520aspects%2520of%2520the%2520problem%253A%2520the%2520need%2520for%2520structure%2520in%2520the%2520solutions%252C%2520the%250Asubjective%2520and%2520uncertain%2520nature%2520of%2520modes%252C%2520and%2520the%2520convenience%2520of%2520a%2520holistic%250Aview%2520that%2520blends%2520local%2520and%2520global%2520density%2520properties.%2520The%2520technique%2520combines%250Aflexible%2520kernel%2520estimators%2520and%2520parsimonious%2520compositional%2520splines%2520in%2520the%250ABayesian%2520inference%2520paradigm%252C%2520providing%2520soft%2520solutions%2520and%2520incorporating%2520expert%250Ajudgment.%2520The%2520procedure%2520includes%2520feature%2520exploration%252C%2520model%2520selection%252C%2520and%2520mode%250Atesting%252C%2520illustrated%2520in%2520a%2520sports%2520analytics%2520case%2520study%2520showcasing%2520multiple%250Acompanion%2520visualisation%2520tools.%2520A%2520thorough%2520simulation%2520study%2520also%2520demonstrates%250Athat%2520traditional%2520modality-driven%2520approaches%2520paradoxically%2520struggle%2520to%2520provide%250Aaccurate%2520results.%2520In%2520this%2520context%252C%2520the%2520new%2520method%2520emerges%2520as%2520a%2520top-tier%250Aalternative%252C%2520offering%2520innovative%2520solutions%2520for%2520analysts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.05825v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20taut%20splines%20for%20estimating%20the%20number%20of%20modes&entry.906535625=Jos%C3%A9%20E.%20Chac%C3%B3n%20and%20Javier%20Fern%C3%A1ndez%20Serrano&entry.1292438233=%20%20The%20number%20of%20modes%20in%20a%20probability%20density%20function%20is%20representative%20of%0Athe%20complexity%20of%20a%20model%20and%20can%20also%20be%20viewed%20as%20the%20number%20of%0Asubpopulations.%20Despite%20its%20relevance%2C%20there%20has%20been%20limited%20research%20in%20this%0Aarea.%20A%20novel%20approach%20to%20estimating%20the%20number%20of%20modes%20in%20the%20univariate%0Asetting%20is%20presented%2C%20focusing%20on%20prediction%20accuracy%20and%20inspired%20by%20some%0Aoverlooked%20aspects%20of%20the%20problem%3A%20the%20need%20for%20structure%20in%20the%20solutions%2C%20the%0Asubjective%20and%20uncertain%20nature%20of%20modes%2C%20and%20the%20convenience%20of%20a%20holistic%0Aview%20that%20blends%20local%20and%20global%20density%20properties.%20The%20technique%20combines%0Aflexible%20kernel%20estimators%20and%20parsimonious%20compositional%20splines%20in%20the%0ABayesian%20inference%20paradigm%2C%20providing%20soft%20solutions%20and%20incorporating%20expert%0Ajudgment.%20The%20procedure%20includes%20feature%20exploration%2C%20model%20selection%2C%20and%20mode%0Atesting%2C%20illustrated%20in%20a%20sports%20analytics%20case%20study%20showcasing%20multiple%0Acompanion%20visualisation%20tools.%20A%20thorough%20simulation%20study%20also%20demonstrates%0Athat%20traditional%20modality-driven%20approaches%20paradoxically%20struggle%20to%20provide%0Aaccurate%20results.%20In%20this%20context%2C%20the%20new%20method%20emerges%20as%20a%20top-tier%0Aalternative%2C%20offering%20innovative%20solutions%20for%20analysts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.05825v3&entry.124074799=Read"},
{"title": "Machine Learning Assisted Dynamical Classification of Trans-Neptunian\n  Objects", "author": "Kathryn Volk and Renu Malhotra", "abstract": "  Trans-Neptunian objects (TNOs) are small, icy bodies in the outer solar\nsystem. They are observed to have a complex orbital distribution that was\nshaped by the early dynamical history and migration of the giant planets.\nComparisons between the different dynamical classes of modeled and observed\nTNOs can help constrain the history of the outer solar system. Because of the\ncomplex dynamics of TNOs, particularly those in and near mean motion resonances\nwith Neptune, classification has traditionally been done by human inspection of\nplots of the time evolution of orbital parameters. This is very inefficient.\nThe Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected\nto increase the number of known TNOs by a factor of $\\sim$10, necessitating a\nmuch more automated process. In this chapter we present an improved supervised\nmachine learning classifier for TNOs. Using a large and diverse training set as\nwell as carefully chosen, dynamically motivated data features calculated from\nnumerical integrations of TNO orbits, our classifier returns results that match\nthose of a human classifier 98% of the time, and dynamically relevant\nclassifications 99.7% of the time. This classifier is dramatically more\nefficient than human classification, and it will improve classification of both\nobserved and modeled TNO data.\n", "link": "http://arxiv.org/abs/2405.05185v1", "date": "2024-05-08", "relevancy": 1.3173, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4373}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Assisted%20Dynamical%20Classification%20of%20Trans-Neptunian%0A%20%20Objects&body=Title%3A%20Machine%20Learning%20Assisted%20Dynamical%20Classification%20of%20Trans-Neptunian%0A%20%20Objects%0AAuthor%3A%20Kathryn%20Volk%20and%20Renu%20Malhotra%0AAbstract%3A%20%20%20Trans-Neptunian%20objects%20%28TNOs%29%20are%20small%2C%20icy%20bodies%20in%20the%20outer%20solar%0Asystem.%20They%20are%20observed%20to%20have%20a%20complex%20orbital%20distribution%20that%20was%0Ashaped%20by%20the%20early%20dynamical%20history%20and%20migration%20of%20the%20giant%20planets.%0AComparisons%20between%20the%20different%20dynamical%20classes%20of%20modeled%20and%20observed%0ATNOs%20can%20help%20constrain%20the%20history%20of%20the%20outer%20solar%20system.%20Because%20of%20the%0Acomplex%20dynamics%20of%20TNOs%2C%20particularly%20those%20in%20and%20near%20mean%20motion%20resonances%0Awith%20Neptune%2C%20classification%20has%20traditionally%20been%20done%20by%20human%20inspection%20of%0Aplots%20of%20the%20time%20evolution%20of%20orbital%20parameters.%20This%20is%20very%20inefficient.%0AThe%20Vera%20Rubin%20Observatory%27s%20Legacy%20Survey%20of%20Space%20and%20Time%20%28LSST%29%20is%20expected%0Ato%20increase%20the%20number%20of%20known%20TNOs%20by%20a%20factor%20of%20%24%5Csim%2410%2C%20necessitating%20a%0Amuch%20more%20automated%20process.%20In%20this%20chapter%20we%20present%20an%20improved%20supervised%0Amachine%20learning%20classifier%20for%20TNOs.%20Using%20a%20large%20and%20diverse%20training%20set%20as%0Awell%20as%20carefully%20chosen%2C%20dynamically%20motivated%20data%20features%20calculated%20from%0Anumerical%20integrations%20of%20TNO%20orbits%2C%20our%20classifier%20returns%20results%20that%20match%0Athose%20of%20a%20human%20classifier%2098%25%20of%20the%20time%2C%20and%20dynamically%20relevant%0Aclassifications%2099.7%25%20of%20the%20time.%20This%20classifier%20is%20dramatically%20more%0Aefficient%20than%20human%20classification%2C%20and%20it%20will%20improve%20classification%20of%20both%0Aobserved%20and%20modeled%20TNO%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520Assisted%2520Dynamical%2520Classification%2520of%2520Trans-Neptunian%250A%2520%2520Objects%26entry.906535625%3DKathryn%2520Volk%2520and%2520Renu%2520Malhotra%26entry.1292438233%3D%2520%2520Trans-Neptunian%2520objects%2520%2528TNOs%2529%2520are%2520small%252C%2520icy%2520bodies%2520in%2520the%2520outer%2520solar%250Asystem.%2520They%2520are%2520observed%2520to%2520have%2520a%2520complex%2520orbital%2520distribution%2520that%2520was%250Ashaped%2520by%2520the%2520early%2520dynamical%2520history%2520and%2520migration%2520of%2520the%2520giant%2520planets.%250AComparisons%2520between%2520the%2520different%2520dynamical%2520classes%2520of%2520modeled%2520and%2520observed%250ATNOs%2520can%2520help%2520constrain%2520the%2520history%2520of%2520the%2520outer%2520solar%2520system.%2520Because%2520of%2520the%250Acomplex%2520dynamics%2520of%2520TNOs%252C%2520particularly%2520those%2520in%2520and%2520near%2520mean%2520motion%2520resonances%250Awith%2520Neptune%252C%2520classification%2520has%2520traditionally%2520been%2520done%2520by%2520human%2520inspection%2520of%250Aplots%2520of%2520the%2520time%2520evolution%2520of%2520orbital%2520parameters.%2520This%2520is%2520very%2520inefficient.%250AThe%2520Vera%2520Rubin%2520Observatory%2527s%2520Legacy%2520Survey%2520of%2520Space%2520and%2520Time%2520%2528LSST%2529%2520is%2520expected%250Ato%2520increase%2520the%2520number%2520of%2520known%2520TNOs%2520by%2520a%2520factor%2520of%2520%2524%255Csim%252410%252C%2520necessitating%2520a%250Amuch%2520more%2520automated%2520process.%2520In%2520this%2520chapter%2520we%2520present%2520an%2520improved%2520supervised%250Amachine%2520learning%2520classifier%2520for%2520TNOs.%2520Using%2520a%2520large%2520and%2520diverse%2520training%2520set%2520as%250Awell%2520as%2520carefully%2520chosen%252C%2520dynamically%2520motivated%2520data%2520features%2520calculated%2520from%250Anumerical%2520integrations%2520of%2520TNO%2520orbits%252C%2520our%2520classifier%2520returns%2520results%2520that%2520match%250Athose%2520of%2520a%2520human%2520classifier%252098%2525%2520of%2520the%2520time%252C%2520and%2520dynamically%2520relevant%250Aclassifications%252099.7%2525%2520of%2520the%2520time.%2520This%2520classifier%2520is%2520dramatically%2520more%250Aefficient%2520than%2520human%2520classification%252C%2520and%2520it%2520will%2520improve%2520classification%2520of%2520both%250Aobserved%2520and%2520modeled%2520TNO%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Assisted%20Dynamical%20Classification%20of%20Trans-Neptunian%0A%20%20Objects&entry.906535625=Kathryn%20Volk%20and%20Renu%20Malhotra&entry.1292438233=%20%20Trans-Neptunian%20objects%20%28TNOs%29%20are%20small%2C%20icy%20bodies%20in%20the%20outer%20solar%0Asystem.%20They%20are%20observed%20to%20have%20a%20complex%20orbital%20distribution%20that%20was%0Ashaped%20by%20the%20early%20dynamical%20history%20and%20migration%20of%20the%20giant%20planets.%0AComparisons%20between%20the%20different%20dynamical%20classes%20of%20modeled%20and%20observed%0ATNOs%20can%20help%20constrain%20the%20history%20of%20the%20outer%20solar%20system.%20Because%20of%20the%0Acomplex%20dynamics%20of%20TNOs%2C%20particularly%20those%20in%20and%20near%20mean%20motion%20resonances%0Awith%20Neptune%2C%20classification%20has%20traditionally%20been%20done%20by%20human%20inspection%20of%0Aplots%20of%20the%20time%20evolution%20of%20orbital%20parameters.%20This%20is%20very%20inefficient.%0AThe%20Vera%20Rubin%20Observatory%27s%20Legacy%20Survey%20of%20Space%20and%20Time%20%28LSST%29%20is%20expected%0Ato%20increase%20the%20number%20of%20known%20TNOs%20by%20a%20factor%20of%20%24%5Csim%2410%2C%20necessitating%20a%0Amuch%20more%20automated%20process.%20In%20this%20chapter%20we%20present%20an%20improved%20supervised%0Amachine%20learning%20classifier%20for%20TNOs.%20Using%20a%20large%20and%20diverse%20training%20set%20as%0Awell%20as%20carefully%20chosen%2C%20dynamically%20motivated%20data%20features%20calculated%20from%0Anumerical%20integrations%20of%20TNO%20orbits%2C%20our%20classifier%20returns%20results%20that%20match%0Athose%20of%20a%20human%20classifier%2098%25%20of%20the%20time%2C%20and%20dynamically%20relevant%0Aclassifications%2099.7%25%20of%20the%20time.%20This%20classifier%20is%20dramatically%20more%0Aefficient%20than%20human%20classification%2C%20and%20it%20will%20improve%20classification%20of%20both%0Aobserved%20and%20modeled%20TNO%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05185v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


