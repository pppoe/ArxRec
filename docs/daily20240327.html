<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "To Supervise or Not to Supervise: Understanding and Addressing the Key\n  Challenges of 3D Transfer Learning", "author": "Souhail Hadgi and Lei Li and Maks Ovsjanikov", "abstract": "  Transfer learning has long been a key factor in the advancement of many\nfields including 2D image analysis. Unfortunately, its applicability in 3D data\nprocessing has been relatively limited. While several approaches for 3D\ntransfer learning have been proposed in recent literature, with contrastive\nlearning gaining particular prominence, most existing methods in this domain\nhave only been studied and evaluated in limited scenarios. Most importantly,\nthere is currently a lack of principled understanding of both when and why 3D\ntransfer learning methods are applicable. Remarkably, even the applicability of\nstandard supervised pre-training is poorly understood. In this work, we conduct\nthe first in-depth quantitative and qualitative investigation of supervised and\ncontrastive pre-training strategies and their utility in downstream 3D tasks.\nWe demonstrate that layer-wise analysis of learned features provides\nsignificant insight into the downstream utility of trained networks. Informed\nby this analysis, we propose a simple geometric regularization strategy, which\nimproves the transferability of supervised pre-training. Our work thus sheds\nlight onto both the specific challenges of 3D transfer learning, as well as\nstrategies to overcome them.\n", "link": "http://arxiv.org/abs/2403.17869v1", "date": "2024-03-26", "relevancy": 2.8473, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5858}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5636}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20To%20Supervise%20or%20Not%20to%20Supervise%3A%20Understanding%20and%20Addressing%20the%20Key%0A%20%20Challenges%20of%203D%20Transfer%20Learning&body=Title%3A%20To%20Supervise%20or%20Not%20to%20Supervise%3A%20Understanding%20and%20Addressing%20the%20Key%0A%20%20Challenges%20of%203D%20Transfer%20Learning%0AAuthor%3A%20Souhail%20Hadgi%20and%20Lei%20Li%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20Transfer%20learning%20has%20long%20been%20a%20key%20factor%20in%20the%20advancement%20of%20many%0Afields%20including%202D%20image%20analysis.%20Unfortunately%2C%20its%20applicability%20in%203D%20data%0Aprocessing%20has%20been%20relatively%20limited.%20While%20several%20approaches%20for%203D%0Atransfer%20learning%20have%20been%20proposed%20in%20recent%20literature%2C%20with%20contrastive%0Alearning%20gaining%20particular%20prominence%2C%20most%20existing%20methods%20in%20this%20domain%0Ahave%20only%20been%20studied%20and%20evaluated%20in%20limited%20scenarios.%20Most%20importantly%2C%0Athere%20is%20currently%20a%20lack%20of%20principled%20understanding%20of%20both%20when%20and%20why%203D%0Atransfer%20learning%20methods%20are%20applicable.%20Remarkably%2C%20even%20the%20applicability%20of%0Astandard%20supervised%20pre-training%20is%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%0Athe%20first%20in-depth%20quantitative%20and%20qualitative%20investigation%20of%20supervised%20and%0Acontrastive%20pre-training%20strategies%20and%20their%20utility%20in%20downstream%203D%20tasks.%0AWe%20demonstrate%20that%20layer-wise%20analysis%20of%20learned%20features%20provides%0Asignificant%20insight%20into%20the%20downstream%20utility%20of%20trained%20networks.%20Informed%0Aby%20this%20analysis%2C%20we%20propose%20a%20simple%20geometric%20regularization%20strategy%2C%20which%0Aimproves%20the%20transferability%20of%20supervised%20pre-training.%20Our%20work%20thus%20sheds%0Alight%20onto%20both%20the%20specific%20challenges%20of%203D%20transfer%20learning%2C%20as%20well%20as%0Astrategies%20to%20overcome%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17869v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Supervise%20or%20Not%20to%20Supervise%3A%20Understanding%20and%20Addressing%20the%20Key%0A%20%20Challenges%20of%203D%20Transfer%20Learning&entry.906535625=Souhail%20Hadgi%20and%20Lei%20Li%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20Transfer%20learning%20has%20long%20been%20a%20key%20factor%20in%20the%20advancement%20of%20many%0Afields%20including%202D%20image%20analysis.%20Unfortunately%2C%20its%20applicability%20in%203D%20data%0Aprocessing%20has%20been%20relatively%20limited.%20While%20several%20approaches%20for%203D%0Atransfer%20learning%20have%20been%20proposed%20in%20recent%20literature%2C%20with%20contrastive%0Alearning%20gaining%20particular%20prominence%2C%20most%20existing%20methods%20in%20this%20domain%0Ahave%20only%20been%20studied%20and%20evaluated%20in%20limited%20scenarios.%20Most%20importantly%2C%0Athere%20is%20currently%20a%20lack%20of%20principled%20understanding%20of%20both%20when%20and%20why%203D%0Atransfer%20learning%20methods%20are%20applicable.%20Remarkably%2C%20even%20the%20applicability%20of%0Astandard%20supervised%20pre-training%20is%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%0Athe%20first%20in-depth%20quantitative%20and%20qualitative%20investigation%20of%20supervised%20and%0Acontrastive%20pre-training%20strategies%20and%20their%20utility%20in%20downstream%203D%20tasks.%0AWe%20demonstrate%20that%20layer-wise%20analysis%20of%20learned%20features%20provides%0Asignificant%20insight%20into%20the%20downstream%20utility%20of%20trained%20networks.%20Informed%0Aby%20this%20analysis%2C%20we%20propose%20a%20simple%20geometric%20regularization%20strategy%2C%20which%0Aimproves%20the%20transferability%20of%20supervised%20pre-training.%20Our%20work%20thus%20sheds%0Alight%20onto%20both%20the%20specific%20challenges%20of%203D%20transfer%20learning%2C%20as%20well%20as%0Astrategies%20to%20overcome%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17869v1&entry.124074799=Read"},
{"title": "Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders", "author": "Alexandre Eyma\u00ebl and Renaud Vandeghen and Anthony Cioppa and Silvio Giancola and Bernard Ghanem and Marc Van Droogenbroeck", "abstract": "  Self-supervised pre-training of image encoders is omnipresent in the\nliterature, particularly following the introduction of Masked autoencoders\n(MAE). Current efforts attempt to learn object-centric representations from\nmotion in videos. In particular, SiamMAE recently introduced a Siamese network,\ntraining a shared-weight encoder from two frames of a video with a high\nasymmetric masking ratio (95%). In this work, we propose CropMAE, an\nalternative approach to the Siamese pre-training introduced by SiamMAE. Our\nmethod specifically differs by exclusively considering pairs of cropped images\nsourced from the same image but cropped differently, deviating from the\nconventional pairs of frames extracted from a video. CropMAE therefore\nalleviates the need for video datasets, while maintaining competitive\nperformances and drastically reducing pre-training time. Furthermore, we\ndemonstrate that CropMAE learns similar object-centric representations without\nexplicit motion, showing that current self-supervised learning methods do not\nlearn objects from motion, but rather thanks to the Siamese architecture.\nFinally, CropMAE achieves the highest masking ratio to date (98.5%), enabling\nthe reconstruction of images using only two visible patches. Our code is\navailable at https://github.com/alexandre-eymael/CropMAE.\n", "link": "http://arxiv.org/abs/2403.17823v1", "date": "2024-03-26", "relevancy": 2.7869, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.572}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5571}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.543}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Image%20Pre-Training%20with%20Siamese%20Cropped%20Masked%20Autoencoders&body=Title%3A%20Efficient%20Image%20Pre-Training%20with%20Siamese%20Cropped%20Masked%20Autoencoders%0AAuthor%3A%20Alexandre%20Eyma%C3%ABl%20and%20Renaud%20Vandeghen%20and%20Anthony%20Cioppa%20and%20Silvio%20Giancola%20and%20Bernard%20Ghanem%20and%20Marc%20Van%20Droogenbroeck%0AAbstract%3A%20%20%20Self-supervised%20pre-training%20of%20image%20encoders%20is%20omnipresent%20in%20the%0Aliterature%2C%20particularly%20following%20the%20introduction%20of%20Masked%20autoencoders%0A%28MAE%29.%20Current%20efforts%20attempt%20to%20learn%20object-centric%20representations%20from%0Amotion%20in%20videos.%20In%20particular%2C%20SiamMAE%20recently%20introduced%20a%20Siamese%20network%2C%0Atraining%20a%20shared-weight%20encoder%20from%20two%20frames%20of%20a%20video%20with%20a%20high%0Aasymmetric%20masking%20ratio%20%2895%25%29.%20In%20this%20work%2C%20we%20propose%20CropMAE%2C%20an%0Aalternative%20approach%20to%20the%20Siamese%20pre-training%20introduced%20by%20SiamMAE.%20Our%0Amethod%20specifically%20differs%20by%20exclusively%20considering%20pairs%20of%20cropped%20images%0Asourced%20from%20the%20same%20image%20but%20cropped%20differently%2C%20deviating%20from%20the%0Aconventional%20pairs%20of%20frames%20extracted%20from%20a%20video.%20CropMAE%20therefore%0Aalleviates%20the%20need%20for%20video%20datasets%2C%20while%20maintaining%20competitive%0Aperformances%20and%20drastically%20reducing%20pre-training%20time.%20Furthermore%2C%20we%0Ademonstrate%20that%20CropMAE%20learns%20similar%20object-centric%20representations%20without%0Aexplicit%20motion%2C%20showing%20that%20current%20self-supervised%20learning%20methods%20do%20not%0Alearn%20objects%20from%20motion%2C%20but%20rather%20thanks%20to%20the%20Siamese%20architecture.%0AFinally%2C%20CropMAE%20achieves%20the%20highest%20masking%20ratio%20to%20date%20%2898.5%25%29%2C%20enabling%0Athe%20reconstruction%20of%20images%20using%20only%20two%20visible%20patches.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/alexandre-eymael/CropMAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17823v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Image%20Pre-Training%20with%20Siamese%20Cropped%20Masked%20Autoencoders&entry.906535625=Alexandre%20Eyma%C3%ABl%20and%20Renaud%20Vandeghen%20and%20Anthony%20Cioppa%20and%20Silvio%20Giancola%20and%20Bernard%20Ghanem%20and%20Marc%20Van%20Droogenbroeck&entry.1292438233=%20%20Self-supervised%20pre-training%20of%20image%20encoders%20is%20omnipresent%20in%20the%0Aliterature%2C%20particularly%20following%20the%20introduction%20of%20Masked%20autoencoders%0A%28MAE%29.%20Current%20efforts%20attempt%20to%20learn%20object-centric%20representations%20from%0Amotion%20in%20videos.%20In%20particular%2C%20SiamMAE%20recently%20introduced%20a%20Siamese%20network%2C%0Atraining%20a%20shared-weight%20encoder%20from%20two%20frames%20of%20a%20video%20with%20a%20high%0Aasymmetric%20masking%20ratio%20%2895%25%29.%20In%20this%20work%2C%20we%20propose%20CropMAE%2C%20an%0Aalternative%20approach%20to%20the%20Siamese%20pre-training%20introduced%20by%20SiamMAE.%20Our%0Amethod%20specifically%20differs%20by%20exclusively%20considering%20pairs%20of%20cropped%20images%0Asourced%20from%20the%20same%20image%20but%20cropped%20differently%2C%20deviating%20from%20the%0Aconventional%20pairs%20of%20frames%20extracted%20from%20a%20video.%20CropMAE%20therefore%0Aalleviates%20the%20need%20for%20video%20datasets%2C%20while%20maintaining%20competitive%0Aperformances%20and%20drastically%20reducing%20pre-training%20time.%20Furthermore%2C%20we%0Ademonstrate%20that%20CropMAE%20learns%20similar%20object-centric%20representations%20without%0Aexplicit%20motion%2C%20showing%20that%20current%20self-supervised%20learning%20methods%20do%20not%0Alearn%20objects%20from%20motion%2C%20but%20rather%20thanks%20to%20the%20Siamese%20architecture.%0AFinally%2C%20CropMAE%20achieves%20the%20highest%20masking%20ratio%20to%20date%20%2898.5%25%29%2C%20enabling%0Athe%20reconstruction%20of%20images%20using%20only%20two%20visible%20patches.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/alexandre-eymael/CropMAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17823v1&entry.124074799=Read"},
{"title": "HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map\n  Construction", "author": "Yi Zhou and Hui Zhang and Jiaqian Yu and Yifan Yang and Sangil Jung and Seung-In Park and ByungIn Yoo", "abstract": "  Vectorized High-Definition (HD) map construction requires predictions of the\ncategory and point coordinates of map elements (e.g. road boundary, lane\ndivider, pedestrian crossing, etc.). State-of-the-art methods are mainly based\non point-level representation learning for regressing accurate point\ncoordinates. However, this pipeline has limitations in obtaining element-level\ninformation and handling element-level failures, e.g. erroneous element shape\nor entanglement between elements. To tackle the above issues, we propose a\nsimple yet effective HybrId framework named HIMap to sufficiently learn and\ninteract both point-level and element-level information. Concretely, we\nintroduce a hybrid representation called HIQuery to represent all map elements,\nand propose a point-element interactor to interactively extract and encode the\nhybrid information of elements, e.g. point position and element shape, into the\nHIQuery. Additionally, we present a point-element consistency constraint to\nenhance the consistency between the point-level and element-level information.\nFinally, the output point-element integrated HIQuery can be directly converted\ninto map elements' class, point coordinates, and mask. We conduct extensive\nexperiments and consistently outperform previous methods on both nuScenes and\nArgoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes\ndataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.\n", "link": "http://arxiv.org/abs/2403.08639v2", "date": "2024-03-26", "relevancy": 2.6496, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5415}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5133}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction&body=Title%3A%20HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction%0AAuthor%3A%20Yi%20Zhou%20and%20Hui%20Zhang%20and%20Jiaqian%20Yu%20and%20Yifan%20Yang%20and%20Sangil%20Jung%20and%20Seung-In%20Park%20and%20ByungIn%20Yoo%0AAbstract%3A%20%20%20Vectorized%20High-Definition%20%28HD%29%20map%20construction%20requires%20predictions%20of%20the%0Acategory%20and%20point%20coordinates%20of%20map%20elements%20%28e.g.%20road%20boundary%2C%20lane%0Adivider%2C%20pedestrian%20crossing%2C%20etc.%29.%20State-of-the-art%20methods%20are%20mainly%20based%0Aon%20point-level%20representation%20learning%20for%20regressing%20accurate%20point%0Acoordinates.%20However%2C%20this%20pipeline%20has%20limitations%20in%20obtaining%20element-level%0Ainformation%20and%20handling%20element-level%20failures%2C%20e.g.%20erroneous%20element%20shape%0Aor%20entanglement%20between%20elements.%20To%20tackle%20the%20above%20issues%2C%20we%20propose%20a%0Asimple%20yet%20effective%20HybrId%20framework%20named%20HIMap%20to%20sufficiently%20learn%20and%0Ainteract%20both%20point-level%20and%20element-level%20information.%20Concretely%2C%20we%0Aintroduce%20a%20hybrid%20representation%20called%20HIQuery%20to%20represent%20all%20map%20elements%2C%0Aand%20propose%20a%20point-element%20interactor%20to%20interactively%20extract%20and%20encode%20the%0Ahybrid%20information%20of%20elements%2C%20e.g.%20point%20position%20and%20element%20shape%2C%20into%20the%0AHIQuery.%20Additionally%2C%20we%20present%20a%20point-element%20consistency%20constraint%20to%0Aenhance%20the%20consistency%20between%20the%20point-level%20and%20element-level%20information.%0AFinally%2C%20the%20output%20point-element%20integrated%20HIQuery%20can%20be%20directly%20converted%0Ainto%20map%20elements%27%20class%2C%20point%20coordinates%2C%20and%20mask.%20We%20conduct%20extensive%0Aexperiments%20and%20consistently%20outperform%20previous%20methods%20on%20both%20nuScenes%20and%0AArgoverse2%20datasets.%20Notably%2C%20our%20method%20achieves%20%2477.8%24%20mAP%20on%20the%20nuScenes%0Adataset%2C%20remarkably%20superior%20to%20previous%20SOTAs%20by%20%248.3%24%20mAP%20at%20least.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08639v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction&entry.906535625=Yi%20Zhou%20and%20Hui%20Zhang%20and%20Jiaqian%20Yu%20and%20Yifan%20Yang%20and%20Sangil%20Jung%20and%20Seung-In%20Park%20and%20ByungIn%20Yoo&entry.1292438233=%20%20Vectorized%20High-Definition%20%28HD%29%20map%20construction%20requires%20predictions%20of%20the%0Acategory%20and%20point%20coordinates%20of%20map%20elements%20%28e.g.%20road%20boundary%2C%20lane%0Adivider%2C%20pedestrian%20crossing%2C%20etc.%29.%20State-of-the-art%20methods%20are%20mainly%20based%0Aon%20point-level%20representation%20learning%20for%20regressing%20accurate%20point%0Acoordinates.%20However%2C%20this%20pipeline%20has%20limitations%20in%20obtaining%20element-level%0Ainformation%20and%20handling%20element-level%20failures%2C%20e.g.%20erroneous%20element%20shape%0Aor%20entanglement%20between%20elements.%20To%20tackle%20the%20above%20issues%2C%20we%20propose%20a%0Asimple%20yet%20effective%20HybrId%20framework%20named%20HIMap%20to%20sufficiently%20learn%20and%0Ainteract%20both%20point-level%20and%20element-level%20information.%20Concretely%2C%20we%0Aintroduce%20a%20hybrid%20representation%20called%20HIQuery%20to%20represent%20all%20map%20elements%2C%0Aand%20propose%20a%20point-element%20interactor%20to%20interactively%20extract%20and%20encode%20the%0Ahybrid%20information%20of%20elements%2C%20e.g.%20point%20position%20and%20element%20shape%2C%20into%20the%0AHIQuery.%20Additionally%2C%20we%20present%20a%20point-element%20consistency%20constraint%20to%0Aenhance%20the%20consistency%20between%20the%20point-level%20and%20element-level%20information.%0AFinally%2C%20the%20output%20point-element%20integrated%20HIQuery%20can%20be%20directly%20converted%0Ainto%20map%20elements%27%20class%2C%20point%20coordinates%2C%20and%20mask.%20We%20conduct%20extensive%0Aexperiments%20and%20consistently%20outperform%20previous%20methods%20on%20both%20nuScenes%20and%0AArgoverse2%20datasets.%20Notably%2C%20our%20method%20achieves%20%2477.8%24%20mAP%20on%20the%20nuScenes%0Adataset%2C%20remarkably%20superior%20to%20previous%20SOTAs%20by%20%248.3%24%20mAP%20at%20least.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08639v2&entry.124074799=Read"},
{"title": "Deepfake Generation and Detection: A Benchmark and Survey", "author": "Gan Pei and Jiangning Zhang and Menghan Hu and Guangtao Zhai and Chengjie Wang and Zhenyu Zhang and Jian Yang and Chunhua Shen and Dacheng Tao", "abstract": "  In addition to the advancements in deepfake generation, corresponding\ndetection technologies need to continuously evolve to regulate the potential\nmisuse of deepfakes, such as for privacy invasion and phishing attacks. This\nsurvey comprehensively reviews the latest developments in deepfake generation\nand detection, summarizing and analyzing the current state of the art in this\nrapidly evolving field. We first unify task definitions, comprehensively\nintroduce datasets and metrics, and discuss the development of generation and\ndetection technology frameworks. Then, we discuss the development of several\nrelated sub-fields and focus on researching four mainstream deepfake fields:\npopular face swap, face reenactment, talking face generation, and facial\nattribute editing, as well as foreign detection. Subsequently, we\ncomprehensively benchmark representative methods on popular datasets for each\nfield, fully evaluating the latest and influential works published in top\nconferences/journals. Finally, we analyze the challenges and future research\ndirections of the discussed fields. We closely follow the latest developments\nin https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.\n", "link": "http://arxiv.org/abs/2403.17881v1", "date": "2024-03-26", "relevancy": 2.4746, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4988}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4933}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4926}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey&body=Title%3A%20Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey%0AAuthor%3A%20Gan%20Pei%20and%20Jiangning%20Zhang%20and%20Menghan%20Hu%20and%20Guangtao%20Zhai%20and%20Chengjie%20Wang%20and%20Zhenyu%20Zhang%20and%20Jian%20Yang%20and%20Chunhua%20Shen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20In%20addition%20to%20the%20advancements%20in%20deepfake%20generation%2C%20corresponding%0Adetection%20technologies%20need%20to%20continuously%20evolve%20to%20regulate%20the%20potential%0Amisuse%20of%20deepfakes%2C%20such%20as%20for%20privacy%20invasion%20and%20phishing%20attacks.%20This%0Asurvey%20comprehensively%20reviews%20the%20latest%20developments%20in%20deepfake%20generation%0Aand%20detection%2C%20summarizing%20and%20analyzing%20the%20current%20state%20of%20the%20art%20in%20this%0Arapidly%20evolving%20field.%20We%20first%20unify%20task%20definitions%2C%20comprehensively%0Aintroduce%20datasets%20and%20metrics%2C%20and%20discuss%20the%20development%20of%20generation%20and%0Adetection%20technology%20frameworks.%20Then%2C%20we%20discuss%20the%20development%20of%20several%0Arelated%20sub-fields%20and%20focus%20on%20researching%20four%20mainstream%20deepfake%20fields%3A%0Apopular%20face%20swap%2C%20face%20reenactment%2C%20talking%20face%20generation%2C%20and%20facial%0Aattribute%20editing%2C%20as%20well%20as%20foreign%20detection.%20Subsequently%2C%20we%0Acomprehensively%20benchmark%20representative%20methods%20on%20popular%20datasets%20for%20each%0Afield%2C%20fully%20evaluating%20the%20latest%20and%20influential%20works%20published%20in%20top%0Aconferences/journals.%20Finally%2C%20we%20analyze%20the%20challenges%20and%20future%20research%0Adirections%20of%20the%20discussed%20fields.%20We%20closely%20follow%20the%20latest%20developments%0Ain%20https%3A//github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17881v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey&entry.906535625=Gan%20Pei%20and%20Jiangning%20Zhang%20and%20Menghan%20Hu%20and%20Guangtao%20Zhai%20and%20Chengjie%20Wang%20and%20Zhenyu%20Zhang%20and%20Jian%20Yang%20and%20Chunhua%20Shen%20and%20Dacheng%20Tao&entry.1292438233=%20%20In%20addition%20to%20the%20advancements%20in%20deepfake%20generation%2C%20corresponding%0Adetection%20technologies%20need%20to%20continuously%20evolve%20to%20regulate%20the%20potential%0Amisuse%20of%20deepfakes%2C%20such%20as%20for%20privacy%20invasion%20and%20phishing%20attacks.%20This%0Asurvey%20comprehensively%20reviews%20the%20latest%20developments%20in%20deepfake%20generation%0Aand%20detection%2C%20summarizing%20and%20analyzing%20the%20current%20state%20of%20the%20art%20in%20this%0Arapidly%20evolving%20field.%20We%20first%20unify%20task%20definitions%2C%20comprehensively%0Aintroduce%20datasets%20and%20metrics%2C%20and%20discuss%20the%20development%20of%20generation%20and%0Adetection%20technology%20frameworks.%20Then%2C%20we%20discuss%20the%20development%20of%20several%0Arelated%20sub-fields%20and%20focus%20on%20researching%20four%20mainstream%20deepfake%20fields%3A%0Apopular%20face%20swap%2C%20face%20reenactment%2C%20talking%20face%20generation%2C%20and%20facial%0Aattribute%20editing%2C%20as%20well%20as%20foreign%20detection.%20Subsequently%2C%20we%0Acomprehensively%20benchmark%20representative%20methods%20on%20popular%20datasets%20for%20each%0Afield%2C%20fully%20evaluating%20the%20latest%20and%20influential%20works%20published%20in%20top%0Aconferences/journals.%20Finally%2C%20we%20analyze%20the%20challenges%20and%20future%20research%0Adirections%20of%20the%20discussed%20fields.%20We%20closely%20follow%20the%20latest%20developments%0Ain%20https%3A//github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17881v1&entry.124074799=Read"},
{"title": "OmniVid: A Generative Framework for Universal Video Understanding", "author": "Junke Wang and Dongdong Chen and Chong Luo and Bo He and Lu Yuan and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  The core of video understanding tasks, such as recognition, captioning, and\ntracking, is to automatically detect objects or actions in a video and analyze\ntheir temporal evolution. Despite sharing a common goal, different tasks often\nrely on distinct model architectures and annotation formats. In contrast,\nnatural language processing benefits from a unified output space, i.e., text\nsequences, which simplifies the training of powerful foundational language\nmodels, such as GPT-3, with extensive training corpora. Inspired by this, we\nseek to unify the output space of video understanding tasks by using languages\nas labels and additionally introducing time and box tokens. In this way, a\nvariety of video tasks could be formulated as video-grounded token generation.\nThis enables us to address various types of video tasks, including\nclassification (such as action recognition), captioning (covering clip\ncaptioning, video question answering, and dense video captioning), and\nlocalization tasks (such as visual object tracking) within a fully shared\nencoder-decoder architecture, following a generative framework. Through\ncomprehensive experiments, we demonstrate such a simple and straightforward\nidea is quite effective and can achieve state-of-the-art or competitive results\non seven video benchmarks, providing a novel perspective for more universal\nvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.\n", "link": "http://arxiv.org/abs/2403.17935v1", "date": "2024-03-26", "relevancy": 2.4335, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6138}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6129}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5836}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OmniVid%3A%20A%20Generative%20Framework%20for%20Universal%20Video%20Understanding&body=Title%3A%20OmniVid%3A%20A%20Generative%20Framework%20for%20Universal%20Video%20Understanding%0AAuthor%3A%20Junke%20Wang%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Bo%20He%20and%20Lu%20Yuan%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20The%20core%20of%20video%20understanding%20tasks%2C%20such%20as%20recognition%2C%20captioning%2C%20and%0Atracking%2C%20is%20to%20automatically%20detect%20objects%20or%20actions%20in%20a%20video%20and%20analyze%0Atheir%20temporal%20evolution.%20Despite%20sharing%20a%20common%20goal%2C%20different%20tasks%20often%0Arely%20on%20distinct%20model%20architectures%20and%20annotation%20formats.%20In%20contrast%2C%0Anatural%20language%20processing%20benefits%20from%20a%20unified%20output%20space%2C%20i.e.%2C%20text%0Asequences%2C%20which%20simplifies%20the%20training%20of%20powerful%20foundational%20language%0Amodels%2C%20such%20as%20GPT-3%2C%20with%20extensive%20training%20corpora.%20Inspired%20by%20this%2C%20we%0Aseek%20to%20unify%20the%20output%20space%20of%20video%20understanding%20tasks%20by%20using%20languages%0Aas%20labels%20and%20additionally%20introducing%20time%20and%20box%20tokens.%20In%20this%20way%2C%20a%0Avariety%20of%20video%20tasks%20could%20be%20formulated%20as%20video-grounded%20token%20generation.%0AThis%20enables%20us%20to%20address%20various%20types%20of%20video%20tasks%2C%20including%0Aclassification%20%28such%20as%20action%20recognition%29%2C%20captioning%20%28covering%20clip%0Acaptioning%2C%20video%20question%20answering%2C%20and%20dense%20video%20captioning%29%2C%20and%0Alocalization%20tasks%20%28such%20as%20visual%20object%20tracking%29%20within%20a%20fully%20shared%0Aencoder-decoder%20architecture%2C%20following%20a%20generative%20framework.%20Through%0Acomprehensive%20experiments%2C%20we%20demonstrate%20such%20a%20simple%20and%20straightforward%0Aidea%20is%20quite%20effective%20and%20can%20achieve%20state-of-the-art%20or%20competitive%20results%0Aon%20seven%20video%20benchmarks%2C%20providing%20a%20novel%20perspective%20for%20more%20universal%0Avideo%20understanding.%20Code%20is%20available%20at%20https%3A//github.com/wangjk666/OmniVid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17935v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniVid%3A%20A%20Generative%20Framework%20for%20Universal%20Video%20Understanding&entry.906535625=Junke%20Wang%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Bo%20He%20and%20Lu%20Yuan%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20The%20core%20of%20video%20understanding%20tasks%2C%20such%20as%20recognition%2C%20captioning%2C%20and%0Atracking%2C%20is%20to%20automatically%20detect%20objects%20or%20actions%20in%20a%20video%20and%20analyze%0Atheir%20temporal%20evolution.%20Despite%20sharing%20a%20common%20goal%2C%20different%20tasks%20often%0Arely%20on%20distinct%20model%20architectures%20and%20annotation%20formats.%20In%20contrast%2C%0Anatural%20language%20processing%20benefits%20from%20a%20unified%20output%20space%2C%20i.e.%2C%20text%0Asequences%2C%20which%20simplifies%20the%20training%20of%20powerful%20foundational%20language%0Amodels%2C%20such%20as%20GPT-3%2C%20with%20extensive%20training%20corpora.%20Inspired%20by%20this%2C%20we%0Aseek%20to%20unify%20the%20output%20space%20of%20video%20understanding%20tasks%20by%20using%20languages%0Aas%20labels%20and%20additionally%20introducing%20time%20and%20box%20tokens.%20In%20this%20way%2C%20a%0Avariety%20of%20video%20tasks%20could%20be%20formulated%20as%20video-grounded%20token%20generation.%0AThis%20enables%20us%20to%20address%20various%20types%20of%20video%20tasks%2C%20including%0Aclassification%20%28such%20as%20action%20recognition%29%2C%20captioning%20%28covering%20clip%0Acaptioning%2C%20video%20question%20answering%2C%20and%20dense%20video%20captioning%29%2C%20and%0Alocalization%20tasks%20%28such%20as%20visual%20object%20tracking%29%20within%20a%20fully%20shared%0Aencoder-decoder%20architecture%2C%20following%20a%20generative%20framework.%20Through%0Acomprehensive%20experiments%2C%20we%20demonstrate%20such%20a%20simple%20and%20straightforward%0Aidea%20is%20quite%20effective%20and%20can%20achieve%20state-of-the-art%20or%20competitive%20results%0Aon%20seven%20video%20benchmarks%2C%20providing%20a%20novel%20perspective%20for%20more%20universal%0Avideo%20understanding.%20Code%20is%20available%20at%20https%3A//github.com/wangjk666/OmniVid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17935v1&entry.124074799=Read"},
{"title": "Annotated Biomedical Video Generation using Denoising Diffusion\n  Probabilistic Models and Flow Fields", "author": "R\u00fcveyda Yilmaz and Dennis Eschweiler and Johannes Stegmaier", "abstract": "  The segmentation and tracking of living cells play a vital role within the\nbiomedical domain, particularly in cancer research, drug development, and\ndevelopmental biology. These are usually tedious and time-consuming tasks that\nare traditionally done by biomedical experts. Recently, to automatize these\nprocesses, deep learning based segmentation and tracking methods have been\nproposed. These methods require large-scale datasets and their full potential\nis constrained by the scarcity of annotated data in the biomedical imaging\ndomain. To address this limitation, we propose Biomedical Video Diffusion Model\n(BVDM), capable of generating realistic-looking synthetic microscopy videos.\nTrained only on a single real video, BVDM can generate videos of arbitrary\nlength with pixel-level annotations that can be used for training data-hungry\nmodels. It is composed of a denoising diffusion probabilistic model (DDPM)\ngenerating high-fidelity synthetic cell microscopy images and a flow prediction\nmodel (FPM) predicting the non-rigid transformation between consecutive video\nframes. During inference, initially, the DDPM imposes realistic cell textures\non synthetic cell masks which are generated based on real data statistics. The\nflow prediction model predicts the flow field between consecutive masks and\napplies that to the DDPM output from the previous time frame to create the next\none while keeping temporal consistency. BVDM outperforms state-of-the-art\nsynthetic live cell microscopy video generation models. Furthermore, we\ndemonstrate that a sufficiently large synthetic dataset enhances the\nperformance of cell segmentation and tracking models compared to using a\nlimited amount of available real data.\n", "link": "http://arxiv.org/abs/2403.17808v1", "date": "2024-03-26", "relevancy": 2.4214, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.63}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5901}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5868}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Annotated%20Biomedical%20Video%20Generation%20using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models%20and%20Flow%20Fields&body=Title%3A%20Annotated%20Biomedical%20Video%20Generation%20using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models%20and%20Flow%20Fields%0AAuthor%3A%20R%C3%BCveyda%20Yilmaz%20and%20Dennis%20Eschweiler%20and%20Johannes%20Stegmaier%0AAbstract%3A%20%20%20The%20segmentation%20and%20tracking%20of%20living%20cells%20play%20a%20vital%20role%20within%20the%0Abiomedical%20domain%2C%20particularly%20in%20cancer%20research%2C%20drug%20development%2C%20and%0Adevelopmental%20biology.%20These%20are%20usually%20tedious%20and%20time-consuming%20tasks%20that%0Aare%20traditionally%20done%20by%20biomedical%20experts.%20Recently%2C%20to%20automatize%20these%0Aprocesses%2C%20deep%20learning%20based%20segmentation%20and%20tracking%20methods%20have%20been%0Aproposed.%20These%20methods%20require%20large-scale%20datasets%20and%20their%20full%20potential%0Ais%20constrained%20by%20the%20scarcity%20of%20annotated%20data%20in%20the%20biomedical%20imaging%0Adomain.%20To%20address%20this%20limitation%2C%20we%20propose%20Biomedical%20Video%20Diffusion%20Model%0A%28BVDM%29%2C%20capable%20of%20generating%20realistic-looking%20synthetic%20microscopy%20videos.%0ATrained%20only%20on%20a%20single%20real%20video%2C%20BVDM%20can%20generate%20videos%20of%20arbitrary%0Alength%20with%20pixel-level%20annotations%20that%20can%20be%20used%20for%20training%20data-hungry%0Amodels.%20It%20is%20composed%20of%20a%20denoising%20diffusion%20probabilistic%20model%20%28DDPM%29%0Agenerating%20high-fidelity%20synthetic%20cell%20microscopy%20images%20and%20a%20flow%20prediction%0Amodel%20%28FPM%29%20predicting%20the%20non-rigid%20transformation%20between%20consecutive%20video%0Aframes.%20During%20inference%2C%20initially%2C%20the%20DDPM%20imposes%20realistic%20cell%20textures%0Aon%20synthetic%20cell%20masks%20which%20are%20generated%20based%20on%20real%20data%20statistics.%20The%0Aflow%20prediction%20model%20predicts%20the%20flow%20field%20between%20consecutive%20masks%20and%0Aapplies%20that%20to%20the%20DDPM%20output%20from%20the%20previous%20time%20frame%20to%20create%20the%20next%0Aone%20while%20keeping%20temporal%20consistency.%20BVDM%20outperforms%20state-of-the-art%0Asynthetic%20live%20cell%20microscopy%20video%20generation%20models.%20Furthermore%2C%20we%0Ademonstrate%20that%20a%20sufficiently%20large%20synthetic%20dataset%20enhances%20the%0Aperformance%20of%20cell%20segmentation%20and%20tracking%20models%20compared%20to%20using%20a%0Alimited%20amount%20of%20available%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17808v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotated%20Biomedical%20Video%20Generation%20using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models%20and%20Flow%20Fields&entry.906535625=R%C3%BCveyda%20Yilmaz%20and%20Dennis%20Eschweiler%20and%20Johannes%20Stegmaier&entry.1292438233=%20%20The%20segmentation%20and%20tracking%20of%20living%20cells%20play%20a%20vital%20role%20within%20the%0Abiomedical%20domain%2C%20particularly%20in%20cancer%20research%2C%20drug%20development%2C%20and%0Adevelopmental%20biology.%20These%20are%20usually%20tedious%20and%20time-consuming%20tasks%20that%0Aare%20traditionally%20done%20by%20biomedical%20experts.%20Recently%2C%20to%20automatize%20these%0Aprocesses%2C%20deep%20learning%20based%20segmentation%20and%20tracking%20methods%20have%20been%0Aproposed.%20These%20methods%20require%20large-scale%20datasets%20and%20their%20full%20potential%0Ais%20constrained%20by%20the%20scarcity%20of%20annotated%20data%20in%20the%20biomedical%20imaging%0Adomain.%20To%20address%20this%20limitation%2C%20we%20propose%20Biomedical%20Video%20Diffusion%20Model%0A%28BVDM%29%2C%20capable%20of%20generating%20realistic-looking%20synthetic%20microscopy%20videos.%0ATrained%20only%20on%20a%20single%20real%20video%2C%20BVDM%20can%20generate%20videos%20of%20arbitrary%0Alength%20with%20pixel-level%20annotations%20that%20can%20be%20used%20for%20training%20data-hungry%0Amodels.%20It%20is%20composed%20of%20a%20denoising%20diffusion%20probabilistic%20model%20%28DDPM%29%0Agenerating%20high-fidelity%20synthetic%20cell%20microscopy%20images%20and%20a%20flow%20prediction%0Amodel%20%28FPM%29%20predicting%20the%20non-rigid%20transformation%20between%20consecutive%20video%0Aframes.%20During%20inference%2C%20initially%2C%20the%20DDPM%20imposes%20realistic%20cell%20textures%0Aon%20synthetic%20cell%20masks%20which%20are%20generated%20based%20on%20real%20data%20statistics.%20The%0Aflow%20prediction%20model%20predicts%20the%20flow%20field%20between%20consecutive%20masks%20and%0Aapplies%20that%20to%20the%20DDPM%20output%20from%20the%20previous%20time%20frame%20to%20create%20the%20next%0Aone%20while%20keeping%20temporal%20consistency.%20BVDM%20outperforms%20state-of-the-art%0Asynthetic%20live%20cell%20microscopy%20video%20generation%20models.%20Furthermore%2C%20we%0Ademonstrate%20that%20a%20sufficiently%20large%20synthetic%20dataset%20enhances%20the%0Aperformance%20of%20cell%20segmentation%20and%20tracking%20models%20compared%20to%20using%20a%0Alimited%20amount%20of%20available%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17808v1&entry.124074799=Read"},
{"title": "Leveraging Near-Field Lighting for Monocular Depth Estimation from\n  Endoscopy Videos", "author": "Akshay Paruchuri and Samuel Ehrenstein and Shuxian Wang and Inbar Fried and Stephen M. Pizer and Marc Niethammer and Roni Sengupta", "abstract": "  Monocular depth estimation in endoscopy videos can enable assistive and\nrobotic surgery to obtain better coverage of the organ and detection of various\nhealth issues. Despite promising progress on mainstream, natural image depth\nestimation, techniques perform poorly on endoscopy images due to a lack of\nstrong geometric features and challenging illumination effects. In this paper,\nwe utilize the photometric cues, i.e., the light emitted from an endoscope and\nreflected by the surface, to improve monocular depth estimation. We first\ncreate two novel loss functions with supervised and self-supervised variants\nthat utilize a per-pixel shading representation. We then propose a novel depth\nrefinement network (PPSNet) that leverages the same per-pixel shading\nrepresentation. Finally, we introduce teacher-student transfer learning to\nproduce better depth maps from both synthetic data with supervision and\nclinical data with self-supervision. We achieve state-of-the-art results on the\nC3VD dataset while estimating high-quality depth maps from clinical data. Our\ncode, pre-trained models, and supplementary materials can be found on our\nproject page: https://ppsnet.github.io/\n", "link": "http://arxiv.org/abs/2403.17915v1", "date": "2024-03-26", "relevancy": 2.3797, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6228}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5884}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5696}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Near-Field%20Lighting%20for%20Monocular%20Depth%20Estimation%20from%0A%20%20Endoscopy%20Videos&body=Title%3A%20Leveraging%20Near-Field%20Lighting%20for%20Monocular%20Depth%20Estimation%20from%0A%20%20Endoscopy%20Videos%0AAuthor%3A%20Akshay%20Paruchuri%20and%20Samuel%20Ehrenstein%20and%20Shuxian%20Wang%20and%20Inbar%20Fried%20and%20Stephen%20M.%20Pizer%20and%20Marc%20Niethammer%20and%20Roni%20Sengupta%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20in%20endoscopy%20videos%20can%20enable%20assistive%20and%0Arobotic%20surgery%20to%20obtain%20better%20coverage%20of%20the%20organ%20and%20detection%20of%20various%0Ahealth%20issues.%20Despite%20promising%20progress%20on%20mainstream%2C%20natural%20image%20depth%0Aestimation%2C%20techniques%20perform%20poorly%20on%20endoscopy%20images%20due%20to%20a%20lack%20of%0Astrong%20geometric%20features%20and%20challenging%20illumination%20effects.%20In%20this%20paper%2C%0Awe%20utilize%20the%20photometric%20cues%2C%20i.e.%2C%20the%20light%20emitted%20from%20an%20endoscope%20and%0Areflected%20by%20the%20surface%2C%20to%20improve%20monocular%20depth%20estimation.%20We%20first%0Acreate%20two%20novel%20loss%20functions%20with%20supervised%20and%20self-supervised%20variants%0Athat%20utilize%20a%20per-pixel%20shading%20representation.%20We%20then%20propose%20a%20novel%20depth%0Arefinement%20network%20%28PPSNet%29%20that%20leverages%20the%20same%20per-pixel%20shading%0Arepresentation.%20Finally%2C%20we%20introduce%20teacher-student%20transfer%20learning%20to%0Aproduce%20better%20depth%20maps%20from%20both%20synthetic%20data%20with%20supervision%20and%0Aclinical%20data%20with%20self-supervision.%20We%20achieve%20state-of-the-art%20results%20on%20the%0AC3VD%20dataset%20while%20estimating%20high-quality%20depth%20maps%20from%20clinical%20data.%20Our%0Acode%2C%20pre-trained%20models%2C%20and%20supplementary%20materials%20can%20be%20found%20on%20our%0Aproject%20page%3A%20https%3A//ppsnet.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17915v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Near-Field%20Lighting%20for%20Monocular%20Depth%20Estimation%20from%0A%20%20Endoscopy%20Videos&entry.906535625=Akshay%20Paruchuri%20and%20Samuel%20Ehrenstein%20and%20Shuxian%20Wang%20and%20Inbar%20Fried%20and%20Stephen%20M.%20Pizer%20and%20Marc%20Niethammer%20and%20Roni%20Sengupta&entry.1292438233=%20%20Monocular%20depth%20estimation%20in%20endoscopy%20videos%20can%20enable%20assistive%20and%0Arobotic%20surgery%20to%20obtain%20better%20coverage%20of%20the%20organ%20and%20detection%20of%20various%0Ahealth%20issues.%20Despite%20promising%20progress%20on%20mainstream%2C%20natural%20image%20depth%0Aestimation%2C%20techniques%20perform%20poorly%20on%20endoscopy%20images%20due%20to%20a%20lack%20of%0Astrong%20geometric%20features%20and%20challenging%20illumination%20effects.%20In%20this%20paper%2C%0Awe%20utilize%20the%20photometric%20cues%2C%20i.e.%2C%20the%20light%20emitted%20from%20an%20endoscope%20and%0Areflected%20by%20the%20surface%2C%20to%20improve%20monocular%20depth%20estimation.%20We%20first%0Acreate%20two%20novel%20loss%20functions%20with%20supervised%20and%20self-supervised%20variants%0Athat%20utilize%20a%20per-pixel%20shading%20representation.%20We%20then%20propose%20a%20novel%20depth%0Arefinement%20network%20%28PPSNet%29%20that%20leverages%20the%20same%20per-pixel%20shading%0Arepresentation.%20Finally%2C%20we%20introduce%20teacher-student%20transfer%20learning%20to%0Aproduce%20better%20depth%20maps%20from%20both%20synthetic%20data%20with%20supervision%20and%0Aclinical%20data%20with%20self-supervision.%20We%20achieve%20state-of-the-art%20results%20on%20the%0AC3VD%20dataset%20while%20estimating%20high-quality%20depth%20maps%20from%20clinical%20data.%20Our%0Acode%2C%20pre-trained%20models%2C%20and%20supplementary%20materials%20can%20be%20found%20on%20our%0Aproject%20page%3A%20https%3A//ppsnet.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17915v1&entry.124074799=Read"},
{"title": "GPFL: A Gradient Projection-Based Client Selection Framework for\n  Efficient Federated Learning", "author": "Shijie Na and Yuzhi Liang and Siu-Ming Yiu", "abstract": "  Federated learning client selection is crucial for determining participant\nclients while balancing model accuracy and communication efficiency. Existing\nmethods have limitations in handling data heterogeneity, computational burdens,\nand independent client treatment. To address these challenges, we propose GPFL,\nwhich measures client value by comparing local and global descent directions.\nWe also employ an Exploit-Explore mechanism to enhance performance.\nExperimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL\noutperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in\nFEMINST test accuracy. Moreover, GPFL exhibits shorter computation times\nthrough pre-selection and parameter reuse in federated learning.\n", "link": "http://arxiv.org/abs/2403.17833v1", "date": "2024-03-26", "relevancy": 2.3393, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4746}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4663}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4626}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GPFL%3A%20A%20Gradient%20Projection-Based%20Client%20Selection%20Framework%20for%0A%20%20Efficient%20Federated%20Learning&body=Title%3A%20GPFL%3A%20A%20Gradient%20Projection-Based%20Client%20Selection%20Framework%20for%0A%20%20Efficient%20Federated%20Learning%0AAuthor%3A%20Shijie%20Na%20and%20Yuzhi%20Liang%20and%20Siu-Ming%20Yiu%0AAbstract%3A%20%20%20Federated%20learning%20client%20selection%20is%20crucial%20for%20determining%20participant%0Aclients%20while%20balancing%20model%20accuracy%20and%20communication%20efficiency.%20Existing%0Amethods%20have%20limitations%20in%20handling%20data%20heterogeneity%2C%20computational%20burdens%2C%0Aand%20independent%20client%20treatment.%20To%20address%20these%20challenges%2C%20we%20propose%20GPFL%2C%0Awhich%20measures%20client%20value%20by%20comparing%20local%20and%20global%20descent%20directions.%0AWe%20also%20employ%20an%20Exploit-Explore%20mechanism%20to%20enhance%20performance.%0AExperimental%20results%20on%20FEMINST%20and%20CIFAR-10%20datasets%20demonstrate%20that%20GPFL%0Aoutperforms%20baselines%20in%20Non-IID%20scenarios%2C%20achieving%20over%209%5C%25%20improvement%20in%0AFEMINST%20test%20accuracy.%20Moreover%2C%20GPFL%20exhibits%20shorter%20computation%20times%0Athrough%20pre-selection%20and%20parameter%20reuse%20in%20federated%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17833v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPFL%3A%20A%20Gradient%20Projection-Based%20Client%20Selection%20Framework%20for%0A%20%20Efficient%20Federated%20Learning&entry.906535625=Shijie%20Na%20and%20Yuzhi%20Liang%20and%20Siu-Ming%20Yiu&entry.1292438233=%20%20Federated%20learning%20client%20selection%20is%20crucial%20for%20determining%20participant%0Aclients%20while%20balancing%20model%20accuracy%20and%20communication%20efficiency.%20Existing%0Amethods%20have%20limitations%20in%20handling%20data%20heterogeneity%2C%20computational%20burdens%2C%0Aand%20independent%20client%20treatment.%20To%20address%20these%20challenges%2C%20we%20propose%20GPFL%2C%0Awhich%20measures%20client%20value%20by%20comparing%20local%20and%20global%20descent%20directions.%0AWe%20also%20employ%20an%20Exploit-Explore%20mechanism%20to%20enhance%20performance.%0AExperimental%20results%20on%20FEMINST%20and%20CIFAR-10%20datasets%20demonstrate%20that%20GPFL%0Aoutperforms%20baselines%20in%20Non-IID%20scenarios%2C%20achieving%20over%209%5C%25%20improvement%20in%0AFEMINST%20test%20accuracy.%20Moreover%2C%20GPFL%20exhibits%20shorter%20computation%20times%0Athrough%20pre-selection%20and%20parameter%20reuse%20in%20federated%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17833v1&entry.124074799=Read"},
{"title": "AgentStudio: A Toolkit for Building General Virtual Agents", "author": "Longtao Zheng and Zhiyuan Huang and Zhenghai Xue and Xinrun Wang and Bo An and Shuicheng Yan", "abstract": "  Creating autonomous virtual agents capable of using arbitrary software on any\ndigital device remains a major challenge for artificial intelligence. Two key\nobstacles hinder progress: insufficient infrastructure for building virtual\nagents in real-world environments, and the need for in-the-wild evaluation of\nfundamental agent abilities. To address this, we introduce AgentStudio, an\nonline, realistic, and multimodal toolkit that covers the entire lifecycle of\nagent development. This includes environment setups, data collection, agent\nevaluation, and visualization. The observation and action spaces are highly\ngeneric, supporting both function calling and human-computer interfaces. This\nversatility is further enhanced by AgentStudio's graphical user interfaces,\nwhich allow efficient development of datasets and benchmarks in real-world\nsettings. To illustrate, we introduce a visual grounding dataset and a\nreal-world benchmark suite, both created with our graphical interfaces.\nFurthermore, we present several actionable insights derived from AgentStudio,\ne.g., general visual grounding, open-ended tool creation, learning from videos,\netc. We have open-sourced the environments, datasets, benchmarks, and\ninterfaces to promote research towards developing general virtual agents for\nthe future.\n", "link": "http://arxiv.org/abs/2403.17918v1", "date": "2024-03-26", "relevancy": 2.3074, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6102}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5859}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5398}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AgentStudio%3A%20A%20Toolkit%20for%20Building%20General%20Virtual%20Agents&body=Title%3A%20AgentStudio%3A%20A%20Toolkit%20for%20Building%20General%20Virtual%20Agents%0AAuthor%3A%20Longtao%20Zheng%20and%20Zhiyuan%20Huang%20and%20Zhenghai%20Xue%20and%20Xinrun%20Wang%20and%20Bo%20An%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Creating%20autonomous%20virtual%20agents%20capable%20of%20using%20arbitrary%20software%20on%20any%0Adigital%20device%20remains%20a%20major%20challenge%20for%20artificial%20intelligence.%20Two%20key%0Aobstacles%20hinder%20progress%3A%20insufficient%20infrastructure%20for%20building%20virtual%0Aagents%20in%20real-world%20environments%2C%20and%20the%20need%20for%20in-the-wild%20evaluation%20of%0Afundamental%20agent%20abilities.%20To%20address%20this%2C%20we%20introduce%20AgentStudio%2C%20an%0Aonline%2C%20realistic%2C%20and%20multimodal%20toolkit%20that%20covers%20the%20entire%20lifecycle%20of%0Aagent%20development.%20This%20includes%20environment%20setups%2C%20data%20collection%2C%20agent%0Aevaluation%2C%20and%20visualization.%20The%20observation%20and%20action%20spaces%20are%20highly%0Ageneric%2C%20supporting%20both%20function%20calling%20and%20human-computer%20interfaces.%20This%0Aversatility%20is%20further%20enhanced%20by%20AgentStudio%27s%20graphical%20user%20interfaces%2C%0Awhich%20allow%20efficient%20development%20of%20datasets%20and%20benchmarks%20in%20real-world%0Asettings.%20To%20illustrate%2C%20we%20introduce%20a%20visual%20grounding%20dataset%20and%20a%0Areal-world%20benchmark%20suite%2C%20both%20created%20with%20our%20graphical%20interfaces.%0AFurthermore%2C%20we%20present%20several%20actionable%20insights%20derived%20from%20AgentStudio%2C%0Ae.g.%2C%20general%20visual%20grounding%2C%20open-ended%20tool%20creation%2C%20learning%20from%20videos%2C%0Aetc.%20We%20have%20open-sourced%20the%20environments%2C%20datasets%2C%20benchmarks%2C%20and%0Ainterfaces%20to%20promote%20research%20towards%20developing%20general%20virtual%20agents%20for%0Athe%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17918v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentStudio%3A%20A%20Toolkit%20for%20Building%20General%20Virtual%20Agents&entry.906535625=Longtao%20Zheng%20and%20Zhiyuan%20Huang%20and%20Zhenghai%20Xue%20and%20Xinrun%20Wang%20and%20Bo%20An%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Creating%20autonomous%20virtual%20agents%20capable%20of%20using%20arbitrary%20software%20on%20any%0Adigital%20device%20remains%20a%20major%20challenge%20for%20artificial%20intelligence.%20Two%20key%0Aobstacles%20hinder%20progress%3A%20insufficient%20infrastructure%20for%20building%20virtual%0Aagents%20in%20real-world%20environments%2C%20and%20the%20need%20for%20in-the-wild%20evaluation%20of%0Afundamental%20agent%20abilities.%20To%20address%20this%2C%20we%20introduce%20AgentStudio%2C%20an%0Aonline%2C%20realistic%2C%20and%20multimodal%20toolkit%20that%20covers%20the%20entire%20lifecycle%20of%0Aagent%20development.%20This%20includes%20environment%20setups%2C%20data%20collection%2C%20agent%0Aevaluation%2C%20and%20visualization.%20The%20observation%20and%20action%20spaces%20are%20highly%0Ageneric%2C%20supporting%20both%20function%20calling%20and%20human-computer%20interfaces.%20This%0Aversatility%20is%20further%20enhanced%20by%20AgentStudio%27s%20graphical%20user%20interfaces%2C%0Awhich%20allow%20efficient%20development%20of%20datasets%20and%20benchmarks%20in%20real-world%0Asettings.%20To%20illustrate%2C%20we%20introduce%20a%20visual%20grounding%20dataset%20and%20a%0Areal-world%20benchmark%20suite%2C%20both%20created%20with%20our%20graphical%20interfaces.%0AFurthermore%2C%20we%20present%20several%20actionable%20insights%20derived%20from%20AgentStudio%2C%0Ae.g.%2C%20general%20visual%20grounding%2C%20open-ended%20tool%20creation%2C%20learning%20from%20videos%2C%0Aetc.%20We%20have%20open-sourced%20the%20environments%2C%20datasets%2C%20benchmarks%2C%20and%0Ainterfaces%20to%20promote%20research%20towards%20developing%20general%20virtual%20agents%20for%0Athe%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17918v1&entry.124074799=Read"},
{"title": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory", "author": "Abdelrahman Shaker and Syed Talal Wasim and Martin Danelljan and Salman Khan and Ming-Hsuan Yang and Fahad Shahbaz Khan", "abstract": "  Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS.\n", "link": "http://arxiv.org/abs/2403.17937v1", "date": "2024-03-26", "relevancy": 2.2827, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5763}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5629}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Video%20Object%20Segmentation%20via%20Modulated%20Cross-Attention%20Memory&body=Title%3A%20Efficient%20Video%20Object%20Segmentation%20via%20Modulated%20Cross-Attention%20Memory%0AAuthor%3A%20Abdelrahman%20Shaker%20and%20Syed%20Talal%20Wasim%20and%20Martin%20Danelljan%20and%20Salman%20Khan%20and%20Ming-Hsuan%20Yang%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Recently%2C%20transformer-based%20approaches%20have%20shown%20promising%20results%20for%0Asemi-supervised%20video%20object%20segmentation.%20However%2C%20these%20approaches%20typically%0Astruggle%20on%20long%20videos%20due%20to%20increased%20GPU%20memory%20demands%2C%20as%20they%20frequently%0Aexpand%20the%20memory%20bank%20every%20few%20frames.%20We%20propose%20a%20transformer-based%0Aapproach%2C%20named%20MAVOS%2C%20that%20introduces%20an%20optimized%20and%20dynamic%20long-term%0Amodulated%20cross-attention%20%28MCA%29%20memory%20to%20model%20temporal%20smoothness%20without%0Arequiring%20frequent%20memory%20expansion.%20The%20proposed%20MCA%20effectively%20encodes%20both%0Alocal%20and%20global%20features%20at%20various%20levels%20of%20granularity%20while%20efficiently%0Amaintaining%20consistent%20speed%20regardless%20of%20the%20video%20length.%20Extensive%0Aexperiments%20on%20multiple%20benchmarks%2C%20LVOS%2C%20Long-Time%20Video%2C%20and%20DAVIS%202017%2C%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20contributions%20leading%20to%0Areal-time%20inference%20and%20markedly%20reduced%20memory%20demands%20without%20any%20degradation%0Ain%20segmentation%20accuracy%20on%20long%20videos.%20Compared%20to%20the%20best%20existing%0Atransformer-based%20approach%2C%20our%20MAVOS%20increases%20the%20speed%20by%207.6x%2C%20while%0Asignificantly%20reducing%20the%20GPU%20memory%20by%2087%25%20with%20comparable%20segmentation%0Aperformance%20on%20short%20and%20long%20video%20datasets.%20Notably%20on%20the%20LVOS%20dataset%2C%20our%0AMAVOS%20achieves%20a%20J%26F%20score%20of%2063.3%25%20while%20operating%20at%2037%20frames%20per%20second%0A%28FPS%29%20on%20a%20single%20V100%20GPU.%20Our%20code%20and%20models%20will%20be%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Amshaker/MAVOS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17937v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Video%20Object%20Segmentation%20via%20Modulated%20Cross-Attention%20Memory&entry.906535625=Abdelrahman%20Shaker%20and%20Syed%20Talal%20Wasim%20and%20Martin%20Danelljan%20and%20Salman%20Khan%20and%20Ming-Hsuan%20Yang%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Recently%2C%20transformer-based%20approaches%20have%20shown%20promising%20results%20for%0Asemi-supervised%20video%20object%20segmentation.%20However%2C%20these%20approaches%20typically%0Astruggle%20on%20long%20videos%20due%20to%20increased%20GPU%20memory%20demands%2C%20as%20they%20frequently%0Aexpand%20the%20memory%20bank%20every%20few%20frames.%20We%20propose%20a%20transformer-based%0Aapproach%2C%20named%20MAVOS%2C%20that%20introduces%20an%20optimized%20and%20dynamic%20long-term%0Amodulated%20cross-attention%20%28MCA%29%20memory%20to%20model%20temporal%20smoothness%20without%0Arequiring%20frequent%20memory%20expansion.%20The%20proposed%20MCA%20effectively%20encodes%20both%0Alocal%20and%20global%20features%20at%20various%20levels%20of%20granularity%20while%20efficiently%0Amaintaining%20consistent%20speed%20regardless%20of%20the%20video%20length.%20Extensive%0Aexperiments%20on%20multiple%20benchmarks%2C%20LVOS%2C%20Long-Time%20Video%2C%20and%20DAVIS%202017%2C%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20contributions%20leading%20to%0Areal-time%20inference%20and%20markedly%20reduced%20memory%20demands%20without%20any%20degradation%0Ain%20segmentation%20accuracy%20on%20long%20videos.%20Compared%20to%20the%20best%20existing%0Atransformer-based%20approach%2C%20our%20MAVOS%20increases%20the%20speed%20by%207.6x%2C%20while%0Asignificantly%20reducing%20the%20GPU%20memory%20by%2087%25%20with%20comparable%20segmentation%0Aperformance%20on%20short%20and%20long%20video%20datasets.%20Notably%20on%20the%20LVOS%20dataset%2C%20our%0AMAVOS%20achieves%20a%20J%26F%20score%20of%2063.3%25%20while%20operating%20at%2037%20frames%20per%20second%0A%28FPS%29%20on%20a%20single%20V100%20GPU.%20Our%20code%20and%20models%20will%20be%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Amshaker/MAVOS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17937v1&entry.124074799=Read"},
{"title": "CMP: Cooperative Motion Prediction with Multi-Agent Communication", "author": "Zhuoyuan Wu and Yuping Wang and Hengbo Ma and Zhaowei Li and Hang Qiu and Jiachen Li", "abstract": "  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n", "link": "http://arxiv.org/abs/2403.17916v1", "date": "2024-03-26", "relevancy": 2.2282, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6102}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5789}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CMP%3A%20Cooperative%20Motion%20Prediction%20with%20Multi-Agent%20Communication&body=Title%3A%20CMP%3A%20Cooperative%20Motion%20Prediction%20with%20Multi-Agent%20Communication%0AAuthor%3A%20Zhuoyuan%20Wu%20and%20Yuping%20Wang%20and%20Hengbo%20Ma%20and%20Zhaowei%20Li%20and%20Hang%20Qiu%20and%20Jiachen%20Li%0AAbstract%3A%20%20%20The%20confluence%20of%20the%20advancement%20of%20Autonomous%20Vehicles%20%28AVs%29%20and%20the%0Amaturity%20of%20Vehicle-to-Everything%20%28V2X%29%20communication%20has%20enabled%20the%0Acapability%20of%20cooperative%20connected%20and%20automated%20vehicles%20%28CAVs%29.%20Building%20on%0Atop%20of%20cooperative%20perception%2C%20this%20paper%20explores%20the%20feasibility%20and%0Aeffectiveness%20of%20cooperative%20motion%20prediction.%20Our%20method%2C%20CMP%2C%20takes%20LiDAR%0Asignals%20as%20input%20to%20enhance%20tracking%20and%20prediction%20capabilities.%20Unlike%0Aprevious%20work%20that%20focuses%20separately%20on%20either%20cooperative%20perception%20or%0Amotion%20prediction%2C%20our%20framework%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%20to%0Aaddress%20the%20unified%20problem%20where%20CAVs%20share%20information%20in%20both%20perception%20and%0Aprediction%20modules.%20Incorporated%20into%20our%20design%20is%20the%20unique%20capability%20to%0Atolerate%20realistic%20V2X%20bandwidth%20limitations%20and%20transmission%20delays%2C%20while%0Adealing%20with%20bulky%20perception%20representations.%20We%20also%20propose%20a%20prediction%0Aaggregation%20module%2C%20which%20unifies%20the%20predictions%20obtained%20by%20different%20CAVs%0Aand%20generates%20the%20final%20prediction.%20Through%20extensive%20experiments%20and%20ablation%0Astudies%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20cooperative%0Aperception%2C%20tracking%2C%20and%20motion%20prediction%20tasks.%20In%20particular%2C%20CMP%20reduces%0Athe%20average%20prediction%20error%20by%2017.2%5C%25%20with%20fewer%20missing%20detections%20compared%0Awith%20the%20no%20cooperation%20setting.%20Our%20work%20marks%20a%20significant%20step%20forward%20in%0Athe%20cooperative%20capabilities%20of%20CAVs%2C%20showcasing%20enhanced%20performance%20in%0Acomplex%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17916v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMP%3A%20Cooperative%20Motion%20Prediction%20with%20Multi-Agent%20Communication&entry.906535625=Zhuoyuan%20Wu%20and%20Yuping%20Wang%20and%20Hengbo%20Ma%20and%20Zhaowei%20Li%20and%20Hang%20Qiu%20and%20Jiachen%20Li&entry.1292438233=%20%20The%20confluence%20of%20the%20advancement%20of%20Autonomous%20Vehicles%20%28AVs%29%20and%20the%0Amaturity%20of%20Vehicle-to-Everything%20%28V2X%29%20communication%20has%20enabled%20the%0Acapability%20of%20cooperative%20connected%20and%20automated%20vehicles%20%28CAVs%29.%20Building%20on%0Atop%20of%20cooperative%20perception%2C%20this%20paper%20explores%20the%20feasibility%20and%0Aeffectiveness%20of%20cooperative%20motion%20prediction.%20Our%20method%2C%20CMP%2C%20takes%20LiDAR%0Asignals%20as%20input%20to%20enhance%20tracking%20and%20prediction%20capabilities.%20Unlike%0Aprevious%20work%20that%20focuses%20separately%20on%20either%20cooperative%20perception%20or%0Amotion%20prediction%2C%20our%20framework%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%20to%0Aaddress%20the%20unified%20problem%20where%20CAVs%20share%20information%20in%20both%20perception%20and%0Aprediction%20modules.%20Incorporated%20into%20our%20design%20is%20the%20unique%20capability%20to%0Atolerate%20realistic%20V2X%20bandwidth%20limitations%20and%20transmission%20delays%2C%20while%0Adealing%20with%20bulky%20perception%20representations.%20We%20also%20propose%20a%20prediction%0Aaggregation%20module%2C%20which%20unifies%20the%20predictions%20obtained%20by%20different%20CAVs%0Aand%20generates%20the%20final%20prediction.%20Through%20extensive%20experiments%20and%20ablation%0Astudies%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20cooperative%0Aperception%2C%20tracking%2C%20and%20motion%20prediction%20tasks.%20In%20particular%2C%20CMP%20reduces%0Athe%20average%20prediction%20error%20by%2017.2%5C%25%20with%20fewer%20missing%20detections%20compared%0Awith%20the%20no%20cooperation%20setting.%20Our%20work%20marks%20a%20significant%20step%20forward%20in%0Athe%20cooperative%20capabilities%20of%20CAVs%2C%20showcasing%20enhanced%20performance%20in%0Acomplex%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17916v1&entry.124074799=Read"},
{"title": "Superior and Pragmatic Talking Face Generation with Teacher-Student\n  Framework", "author": "Chao Liang and Jianwen Jiang and Tianyun Zhong and Gaojie Lin and Zhengkun Rong and Jiaqi Yang and Yongming Zhu", "abstract": "  Talking face generation technology creates talking videos from arbitrary\nappearance and motion signal, with the \"arbitrary\" offering ease of use but\nalso introducing challenges in practical applications. Existing methods work\nwell with standard inputs but suffer serious performance degradation with\nintricate real-world ones. Moreover, efficiency is also an important concern in\ndeployment. To comprehensively address these issues, we introduce SuperFace, a\nteacher-student framework that balances quality, robustness, cost and\neditability. We first propose a simple but effective teacher model capable of\nhandling inputs of varying qualities to generate high-quality results. Building\non this, we devise an efficient distillation strategy to acquire an\nidentity-specific student model that maintains quality with significantly\nreduced computational load. Our experiments validate that SuperFace offers a\nmore comprehensive solution than existing methods for the four mentioned\nobjectives, especially in reducing FLOPs by 99\\% with the student model.\nSuperFace can be driven by both video and audio and allows for localized facial\nattributes editing.\n", "link": "http://arxiv.org/abs/2403.17883v1", "date": "2024-03-26", "relevancy": 2.2149, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5775}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5666}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5248}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Superior%20and%20Pragmatic%20Talking%20Face%20Generation%20with%20Teacher-Student%0A%20%20Framework&body=Title%3A%20Superior%20and%20Pragmatic%20Talking%20Face%20Generation%20with%20Teacher-Student%0A%20%20Framework%0AAuthor%3A%20Chao%20Liang%20and%20Jianwen%20Jiang%20and%20Tianyun%20Zhong%20and%20Gaojie%20Lin%20and%20Zhengkun%20Rong%20and%20Jiaqi%20Yang%20and%20Yongming%20Zhu%0AAbstract%3A%20%20%20Talking%20face%20generation%20technology%20creates%20talking%20videos%20from%20arbitrary%0Aappearance%20and%20motion%20signal%2C%20with%20the%20%22arbitrary%22%20offering%20ease%20of%20use%20but%0Aalso%20introducing%20challenges%20in%20practical%20applications.%20Existing%20methods%20work%0Awell%20with%20standard%20inputs%20but%20suffer%20serious%20performance%20degradation%20with%0Aintricate%20real-world%20ones.%20Moreover%2C%20efficiency%20is%20also%20an%20important%20concern%20in%0Adeployment.%20To%20comprehensively%20address%20these%20issues%2C%20we%20introduce%20SuperFace%2C%20a%0Ateacher-student%20framework%20that%20balances%20quality%2C%20robustness%2C%20cost%20and%0Aeditability.%20We%20first%20propose%20a%20simple%20but%20effective%20teacher%20model%20capable%20of%0Ahandling%20inputs%20of%20varying%20qualities%20to%20generate%20high-quality%20results.%20Building%0Aon%20this%2C%20we%20devise%20an%20efficient%20distillation%20strategy%20to%20acquire%20an%0Aidentity-specific%20student%20model%20that%20maintains%20quality%20with%20significantly%0Areduced%20computational%20load.%20Our%20experiments%20validate%20that%20SuperFace%20offers%20a%0Amore%20comprehensive%20solution%20than%20existing%20methods%20for%20the%20four%20mentioned%0Aobjectives%2C%20especially%20in%20reducing%20FLOPs%20by%2099%5C%25%20with%20the%20student%20model.%0ASuperFace%20can%20be%20driven%20by%20both%20video%20and%20audio%20and%20allows%20for%20localized%20facial%0Aattributes%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17883v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superior%20and%20Pragmatic%20Talking%20Face%20Generation%20with%20Teacher-Student%0A%20%20Framework&entry.906535625=Chao%20Liang%20and%20Jianwen%20Jiang%20and%20Tianyun%20Zhong%20and%20Gaojie%20Lin%20and%20Zhengkun%20Rong%20and%20Jiaqi%20Yang%20and%20Yongming%20Zhu&entry.1292438233=%20%20Talking%20face%20generation%20technology%20creates%20talking%20videos%20from%20arbitrary%0Aappearance%20and%20motion%20signal%2C%20with%20the%20%22arbitrary%22%20offering%20ease%20of%20use%20but%0Aalso%20introducing%20challenges%20in%20practical%20applications.%20Existing%20methods%20work%0Awell%20with%20standard%20inputs%20but%20suffer%20serious%20performance%20degradation%20with%0Aintricate%20real-world%20ones.%20Moreover%2C%20efficiency%20is%20also%20an%20important%20concern%20in%0Adeployment.%20To%20comprehensively%20address%20these%20issues%2C%20we%20introduce%20SuperFace%2C%20a%0Ateacher-student%20framework%20that%20balances%20quality%2C%20robustness%2C%20cost%20and%0Aeditability.%20We%20first%20propose%20a%20simple%20but%20effective%20teacher%20model%20capable%20of%0Ahandling%20inputs%20of%20varying%20qualities%20to%20generate%20high-quality%20results.%20Building%0Aon%20this%2C%20we%20devise%20an%20efficient%20distillation%20strategy%20to%20acquire%20an%0Aidentity-specific%20student%20model%20that%20maintains%20quality%20with%20significantly%0Areduced%20computational%20load.%20Our%20experiments%20validate%20that%20SuperFace%20offers%20a%0Amore%20comprehensive%20solution%20than%20existing%20methods%20for%20the%20four%20mentioned%0Aobjectives%2C%20especially%20in%20reducing%20FLOPs%20by%2099%5C%25%20with%20the%20student%20model.%0ASuperFace%20can%20be%20driven%20by%20both%20video%20and%20audio%20and%20allows%20for%20localized%20facial%0Aattributes%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17883v1&entry.124074799=Read"},
{"title": "SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models", "author": "Kashyap Chitta and Daniel Dauner and Andreas Geiger", "abstract": "  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n", "link": "http://arxiv.org/abs/2403.17933v1", "date": "2024-03-26", "relevancy": 2.1887, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.575}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5305}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SLEDGE%3A%20Synthesizing%20Simulation%20Environments%20for%20Driving%20Agents%20with%0A%20%20Generative%20Models&body=Title%3A%20SLEDGE%3A%20Synthesizing%20Simulation%20Environments%20for%20Driving%20Agents%20with%0A%20%20Generative%20Models%0AAuthor%3A%20Kashyap%20Chitta%20and%20Daniel%20Dauner%20and%20Andreas%20Geiger%0AAbstract%3A%20%20%20SLEDGE%20is%20the%20first%20generative%20simulator%20for%20vehicle%20motion%20planning%20trained%0Aon%20real-world%20driving%20logs.%20Its%20core%20component%20is%20a%20learned%20model%20that%20is%20able%0Ato%20generate%20agent%20bounding%20boxes%20and%20lane%20graphs.%20The%20model%27s%20outputs%20serve%20as%0Aan%20initial%20state%20for%20traffic%20simulation.%20The%20unique%20properties%20of%20the%20entities%0Ato%20be%20generated%20for%20SLEDGE%2C%20such%20as%20their%20connectivity%20and%20variable%20count%20per%0Ascene%2C%20render%20the%20naive%20application%20of%20most%20modern%20generative%20models%20to%20this%0Atask%20non-trivial.%20Therefore%2C%20together%20with%20a%20systematic%20study%20of%20existing%20lane%0Agraph%20representations%2C%20we%20introduce%20a%20novel%20raster-to-vector%20autoencoder%0A%28RVAE%29.%20It%20encodes%20agents%20and%20the%20lane%20graph%20into%20distinct%20channels%20in%20a%0Arasterized%20latent%20map.%20This%20facilitates%20both%20lane-conditioned%20agent%20generation%0Aand%20combined%20generation%20of%20lanes%20and%20agents%20with%20a%20Diffusion%20Transformer.%20Using%0Agenerated%20entities%20in%20SLEDGE%20enables%20greater%20control%20over%20the%20simulation%2C%20e.g.%0Aupsampling%20turns%20or%20increasing%20traffic%20density.%20Further%2C%20SLEDGE%20can%20support%0A500m%20long%20routes%2C%20a%20capability%20not%20found%20in%20existing%20data-driven%20simulators%0Alike%20nuPlan.%20It%20presents%20new%20challenges%20for%20planning%20algorithms%2C%20evidenced%20by%0Afailure%20rates%20of%20over%2040%25%20for%20PDM%2C%20the%20winner%20of%20the%202023%20nuPlan%20challenge%2C%0Awhen%20tested%20on%20hard%20routes%20and%20dense%20traffic%20generated%20by%20our%20model.%20Compared%0Ato%20nuPlan%2C%20SLEDGE%20requires%20500%24%5Ctimes%24%20less%20storage%20to%20set%20up%20%28%3C4GB%29%2C%20making%20it%0Aa%20more%20accessible%20option%20and%20helping%20with%20democratizing%20future%20research%20in%20this%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17933v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLEDGE%3A%20Synthesizing%20Simulation%20Environments%20for%20Driving%20Agents%20with%0A%20%20Generative%20Models&entry.906535625=Kashyap%20Chitta%20and%20Daniel%20Dauner%20and%20Andreas%20Geiger&entry.1292438233=%20%20SLEDGE%20is%20the%20first%20generative%20simulator%20for%20vehicle%20motion%20planning%20trained%0Aon%20real-world%20driving%20logs.%20Its%20core%20component%20is%20a%20learned%20model%20that%20is%20able%0Ato%20generate%20agent%20bounding%20boxes%20and%20lane%20graphs.%20The%20model%27s%20outputs%20serve%20as%0Aan%20initial%20state%20for%20traffic%20simulation.%20The%20unique%20properties%20of%20the%20entities%0Ato%20be%20generated%20for%20SLEDGE%2C%20such%20as%20their%20connectivity%20and%20variable%20count%20per%0Ascene%2C%20render%20the%20naive%20application%20of%20most%20modern%20generative%20models%20to%20this%0Atask%20non-trivial.%20Therefore%2C%20together%20with%20a%20systematic%20study%20of%20existing%20lane%0Agraph%20representations%2C%20we%20introduce%20a%20novel%20raster-to-vector%20autoencoder%0A%28RVAE%29.%20It%20encodes%20agents%20and%20the%20lane%20graph%20into%20distinct%20channels%20in%20a%0Arasterized%20latent%20map.%20This%20facilitates%20both%20lane-conditioned%20agent%20generation%0Aand%20combined%20generation%20of%20lanes%20and%20agents%20with%20a%20Diffusion%20Transformer.%20Using%0Agenerated%20entities%20in%20SLEDGE%20enables%20greater%20control%20over%20the%20simulation%2C%20e.g.%0Aupsampling%20turns%20or%20increasing%20traffic%20density.%20Further%2C%20SLEDGE%20can%20support%0A500m%20long%20routes%2C%20a%20capability%20not%20found%20in%20existing%20data-driven%20simulators%0Alike%20nuPlan.%20It%20presents%20new%20challenges%20for%20planning%20algorithms%2C%20evidenced%20by%0Afailure%20rates%20of%20over%2040%25%20for%20PDM%2C%20the%20winner%20of%20the%202023%20nuPlan%20challenge%2C%0Awhen%20tested%20on%20hard%20routes%20and%20dense%20traffic%20generated%20by%20our%20model.%20Compared%0Ato%20nuPlan%2C%20SLEDGE%20requires%20500%24%5Ctimes%24%20less%20storage%20to%20set%20up%20%28%3C4GB%29%2C%20making%20it%0Aa%20more%20accessible%20option%20and%20helping%20with%20democratizing%20future%20research%20in%20this%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17933v1&entry.124074799=Read"},
{"title": "ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing\n  Change Detection", "author": "Mubashir Noman and Mustansar Fiaz and Hisham Cholakkal and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Deep learning has shown remarkable success in remote sensing change detection\n(CD), aiming to identify semantic change regions between co-registered\nsatellite image pairs acquired at distinct time stamps. However, existing\nconvolutional neural network and transformer-based frameworks often struggle to\naccurately segment semantic change regions. Moreover, transformers-based\nmethods with standard self-attention suffer from quadratic computational\ncomplexity with respect to the image resolution, making them less practical for\nCD tasks with limited training data. To address these issues, we propose an\nefficient change detection framework, ELGC-Net, which leverages rich contextual\ninformation to precisely estimate change regions while reducing the model size.\nOur ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The\nfocus of our design is the introduction of an Efficient Local-Global Context\nAggregator module within the encoder, capturing enhanced global context and\nlocal spatial information through a novel pooled-transpose (PT) attention and\ndepthwise convolution, respectively. The PT attention employs pooling\noperations for robust feature extraction and minimizes computational cost with\ntransposed attention. Extensive experiments on three challenging CD datasets\ndemonstrate that ELGC-Net outperforms existing methods. Compared to the recent\ntransformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in\nintersection over union metric on the LEVIR-CD dataset, while significantly\nreducing trainable parameters. Our proposed ELGC-Net sets a new\nstate-of-the-art performance in remote sensing change detection benchmarks.\nFinally, we also introduce ELGC-Net-LW, a lighter variant with significantly\nreduced computational complexity, suitable for resource-constrained settings,\nwhile achieving comparable performance. Project url\nhttps://github.com/techmn/elgcnet.\n", "link": "http://arxiv.org/abs/2403.17909v1", "date": "2024-03-26", "relevancy": 2.1584, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5576}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5271}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ELGC-Net%3A%20Efficient%20Local-Global%20Context%20Aggregation%20for%20Remote%20Sensing%0A%20%20Change%20Detection&body=Title%3A%20ELGC-Net%3A%20Efficient%20Local-Global%20Context%20Aggregation%20for%20Remote%20Sensing%0A%20%20Change%20Detection%0AAuthor%3A%20Mubashir%20Noman%20and%20Mustansar%20Fiaz%20and%20Hisham%20Cholakkal%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Deep%20learning%20has%20shown%20remarkable%20success%20in%20remote%20sensing%20change%20detection%0A%28CD%29%2C%20aiming%20to%20identify%20semantic%20change%20regions%20between%20co-registered%0Asatellite%20image%20pairs%20acquired%20at%20distinct%20time%20stamps.%20However%2C%20existing%0Aconvolutional%20neural%20network%20and%20transformer-based%20frameworks%20often%20struggle%20to%0Aaccurately%20segment%20semantic%20change%20regions.%20Moreover%2C%20transformers-based%0Amethods%20with%20standard%20self-attention%20suffer%20from%20quadratic%20computational%0Acomplexity%20with%20respect%20to%20the%20image%20resolution%2C%20making%20them%20less%20practical%20for%0ACD%20tasks%20with%20limited%20training%20data.%20To%20address%20these%20issues%2C%20we%20propose%20an%0Aefficient%20change%20detection%20framework%2C%20ELGC-Net%2C%20which%20leverages%20rich%20contextual%0Ainformation%20to%20precisely%20estimate%20change%20regions%20while%20reducing%20the%20model%20size.%0AOur%20ELGC-Net%20comprises%20a%20Siamese%20encoder%2C%20fusion%20modules%2C%20and%20a%20decoder.%20The%0Afocus%20of%20our%20design%20is%20the%20introduction%20of%20an%20Efficient%20Local-Global%20Context%0AAggregator%20module%20within%20the%20encoder%2C%20capturing%20enhanced%20global%20context%20and%0Alocal%20spatial%20information%20through%20a%20novel%20pooled-transpose%20%28PT%29%20attention%20and%0Adepthwise%20convolution%2C%20respectively.%20The%20PT%20attention%20employs%20pooling%0Aoperations%20for%20robust%20feature%20extraction%20and%20minimizes%20computational%20cost%20with%0Atransposed%20attention.%20Extensive%20experiments%20on%20three%20challenging%20CD%20datasets%0Ademonstrate%20that%20ELGC-Net%20outperforms%20existing%20methods.%20Compared%20to%20the%20recent%0Atransformer-based%20CD%20approach%20%28ChangeFormer%29%2C%20ELGC-Net%20achieves%20a%201.4%25%20gain%20in%0Aintersection%20over%20union%20metric%20on%20the%20LEVIR-CD%20dataset%2C%20while%20significantly%0Areducing%20trainable%20parameters.%20Our%20proposed%20ELGC-Net%20sets%20a%20new%0Astate-of-the-art%20performance%20in%20remote%20sensing%20change%20detection%20benchmarks.%0AFinally%2C%20we%20also%20introduce%20ELGC-Net-LW%2C%20a%20lighter%20variant%20with%20significantly%0Areduced%20computational%20complexity%2C%20suitable%20for%20resource-constrained%20settings%2C%0Awhile%20achieving%20comparable%20performance.%20Project%20url%0Ahttps%3A//github.com/techmn/elgcnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17909v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELGC-Net%3A%20Efficient%20Local-Global%20Context%20Aggregation%20for%20Remote%20Sensing%0A%20%20Change%20Detection&entry.906535625=Mubashir%20Noman%20and%20Mustansar%20Fiaz%20and%20Hisham%20Cholakkal%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Deep%20learning%20has%20shown%20remarkable%20success%20in%20remote%20sensing%20change%20detection%0A%28CD%29%2C%20aiming%20to%20identify%20semantic%20change%20regions%20between%20co-registered%0Asatellite%20image%20pairs%20acquired%20at%20distinct%20time%20stamps.%20However%2C%20existing%0Aconvolutional%20neural%20network%20and%20transformer-based%20frameworks%20often%20struggle%20to%0Aaccurately%20segment%20semantic%20change%20regions.%20Moreover%2C%20transformers-based%0Amethods%20with%20standard%20self-attention%20suffer%20from%20quadratic%20computational%0Acomplexity%20with%20respect%20to%20the%20image%20resolution%2C%20making%20them%20less%20practical%20for%0ACD%20tasks%20with%20limited%20training%20data.%20To%20address%20these%20issues%2C%20we%20propose%20an%0Aefficient%20change%20detection%20framework%2C%20ELGC-Net%2C%20which%20leverages%20rich%20contextual%0Ainformation%20to%20precisely%20estimate%20change%20regions%20while%20reducing%20the%20model%20size.%0AOur%20ELGC-Net%20comprises%20a%20Siamese%20encoder%2C%20fusion%20modules%2C%20and%20a%20decoder.%20The%0Afocus%20of%20our%20design%20is%20the%20introduction%20of%20an%20Efficient%20Local-Global%20Context%0AAggregator%20module%20within%20the%20encoder%2C%20capturing%20enhanced%20global%20context%20and%0Alocal%20spatial%20information%20through%20a%20novel%20pooled-transpose%20%28PT%29%20attention%20and%0Adepthwise%20convolution%2C%20respectively.%20The%20PT%20attention%20employs%20pooling%0Aoperations%20for%20robust%20feature%20extraction%20and%20minimizes%20computational%20cost%20with%0Atransposed%20attention.%20Extensive%20experiments%20on%20three%20challenging%20CD%20datasets%0Ademonstrate%20that%20ELGC-Net%20outperforms%20existing%20methods.%20Compared%20to%20the%20recent%0Atransformer-based%20CD%20approach%20%28ChangeFormer%29%2C%20ELGC-Net%20achieves%20a%201.4%25%20gain%20in%0Aintersection%20over%20union%20metric%20on%20the%20LEVIR-CD%20dataset%2C%20while%20significantly%0Areducing%20trainable%20parameters.%20Our%20proposed%20ELGC-Net%20sets%20a%20new%0Astate-of-the-art%20performance%20in%20remote%20sensing%20change%20detection%20benchmarks.%0AFinally%2C%20we%20also%20introduce%20ELGC-Net-LW%2C%20a%20lighter%20variant%20with%20significantly%0Areduced%20computational%20complexity%2C%20suitable%20for%20resource-constrained%20settings%2C%0Awhile%20achieving%20comparable%20performance.%20Project%20url%0Ahttps%3A//github.com/techmn/elgcnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17909v1&entry.124074799=Read"},
{"title": "Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using\n  Sentinel Data", "author": "Yonghao Xu and Amanda Berg and Leif Haglund", "abstract": "  Utilizing satellite imagery for wildfire detection presents substantial\npotential for practical applications. To advance the development of machine\nlearning algorithms in this domain, our study introduces the \\textit{Sen2Fire}\ndataset--a challenging satellite remote sensing dataset tailored for wildfire\ndetection. This dataset is curated from Sentinel-2 multi-spectral data and\nSentinel-5P aerosol product, comprising a total of 2466 image patches. Each\npatch has a size of 512$\\times$512 pixels with 13 bands. Given the distinctive\nsensitivities of various wavebands to wildfire responses, our research focuses\non optimizing wildfire detection by evaluating different wavebands and\nemploying a combination of spectral indices, such as normalized burn ratio\n(NBR) and normalized difference vegetation index (NDVI). The results suggest\nthat, in contrast to using all bands for wildfire detection, selecting specific\nband combinations yields superior performance. Additionally, our study\nunderscores the positive impact of integrating Sentinel-5 aerosol data for\nwildfire detection. The code and dataset are available online\n(https://zenodo.org/records/10881058).\n", "link": "http://arxiv.org/abs/2403.17884v1", "date": "2024-03-26", "relevancy": 2.1564, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4392}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4309}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4237}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sen2Fire%3A%20A%20Challenging%20Benchmark%20Dataset%20for%20Wildfire%20Detection%20using%0A%20%20Sentinel%20Data&body=Title%3A%20Sen2Fire%3A%20A%20Challenging%20Benchmark%20Dataset%20for%20Wildfire%20Detection%20using%0A%20%20Sentinel%20Data%0AAuthor%3A%20Yonghao%20Xu%20and%20Amanda%20Berg%20and%20Leif%20Haglund%0AAbstract%3A%20%20%20Utilizing%20satellite%20imagery%20for%20wildfire%20detection%20presents%20substantial%0Apotential%20for%20practical%20applications.%20To%20advance%20the%20development%20of%20machine%0Alearning%20algorithms%20in%20this%20domain%2C%20our%20study%20introduces%20the%20%5Ctextit%7BSen2Fire%7D%0Adataset--a%20challenging%20satellite%20remote%20sensing%20dataset%20tailored%20for%20wildfire%0Adetection.%20This%20dataset%20is%20curated%20from%20Sentinel-2%20multi-spectral%20data%20and%0ASentinel-5P%20aerosol%20product%2C%20comprising%20a%20total%20of%202466%20image%20patches.%20Each%0Apatch%20has%20a%20size%20of%20512%24%5Ctimes%24512%20pixels%20with%2013%20bands.%20Given%20the%20distinctive%0Asensitivities%20of%20various%20wavebands%20to%20wildfire%20responses%2C%20our%20research%20focuses%0Aon%20optimizing%20wildfire%20detection%20by%20evaluating%20different%20wavebands%20and%0Aemploying%20a%20combination%20of%20spectral%20indices%2C%20such%20as%20normalized%20burn%20ratio%0A%28NBR%29%20and%20normalized%20difference%20vegetation%20index%20%28NDVI%29.%20The%20results%20suggest%0Athat%2C%20in%20contrast%20to%20using%20all%20bands%20for%20wildfire%20detection%2C%20selecting%20specific%0Aband%20combinations%20yields%20superior%20performance.%20Additionally%2C%20our%20study%0Aunderscores%20the%20positive%20impact%20of%20integrating%20Sentinel-5%20aerosol%20data%20for%0Awildfire%20detection.%20The%20code%20and%20dataset%20are%20available%20online%0A%28https%3A//zenodo.org/records/10881058%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17884v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sen2Fire%3A%20A%20Challenging%20Benchmark%20Dataset%20for%20Wildfire%20Detection%20using%0A%20%20Sentinel%20Data&entry.906535625=Yonghao%20Xu%20and%20Amanda%20Berg%20and%20Leif%20Haglund&entry.1292438233=%20%20Utilizing%20satellite%20imagery%20for%20wildfire%20detection%20presents%20substantial%0Apotential%20for%20practical%20applications.%20To%20advance%20the%20development%20of%20machine%0Alearning%20algorithms%20in%20this%20domain%2C%20our%20study%20introduces%20the%20%5Ctextit%7BSen2Fire%7D%0Adataset--a%20challenging%20satellite%20remote%20sensing%20dataset%20tailored%20for%20wildfire%0Adetection.%20This%20dataset%20is%20curated%20from%20Sentinel-2%20multi-spectral%20data%20and%0ASentinel-5P%20aerosol%20product%2C%20comprising%20a%20total%20of%202466%20image%20patches.%20Each%0Apatch%20has%20a%20size%20of%20512%24%5Ctimes%24512%20pixels%20with%2013%20bands.%20Given%20the%20distinctive%0Asensitivities%20of%20various%20wavebands%20to%20wildfire%20responses%2C%20our%20research%20focuses%0Aon%20optimizing%20wildfire%20detection%20by%20evaluating%20different%20wavebands%20and%0Aemploying%20a%20combination%20of%20spectral%20indices%2C%20such%20as%20normalized%20burn%20ratio%0A%28NBR%29%20and%20normalized%20difference%20vegetation%20index%20%28NDVI%29.%20The%20results%20suggest%0Athat%2C%20in%20contrast%20to%20using%20all%20bands%20for%20wildfire%20detection%2C%20selecting%20specific%0Aband%20combinations%20yields%20superior%20performance.%20Additionally%2C%20our%20study%0Aunderscores%20the%20positive%20impact%20of%20integrating%20Sentinel-5%20aerosol%20data%20for%0Awildfire%20detection.%20The%20code%20and%20dataset%20are%20available%20online%0A%28https%3A//zenodo.org/records/10881058%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17884v1&entry.124074799=Read"},
{"title": "Improving Text-to-Image Consistency via Automatic Prompt Optimization", "author": "Oscar Ma\u00f1as and Pietro Astolfi and Melissa Hall and Candace Ross and Jack Urbanek and Adina Williams and Aishwarya Agrawal and Adriana Romero-Soriano and Michal Drozdzal", "abstract": "  Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.\n", "link": "http://arxiv.org/abs/2403.17804v1", "date": "2024-03-26", "relevancy": 2.1529, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5561}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5365}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5328}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Text-to-Image%20Consistency%20via%20Automatic%20Prompt%20Optimization&body=Title%3A%20Improving%20Text-to-Image%20Consistency%20via%20Automatic%20Prompt%20Optimization%0AAuthor%3A%20Oscar%20Ma%C3%B1as%20and%20Pietro%20Astolfi%20and%20Melissa%20Hall%20and%20Candace%20Ross%20and%20Jack%20Urbanek%20and%20Adina%20Williams%20and%20Aishwarya%20Agrawal%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal%0AAbstract%3A%20%20%20Impressive%20advances%20in%20text-to-image%20%28T2I%29%20generative%20models%20have%20yielded%20a%0Aplethora%20of%20high%20performing%20models%20which%20are%20able%20to%20generate%20aesthetically%0Aappealing%2C%20photorealistic%20images.%20Despite%20the%20progress%2C%20these%20models%20still%0Astruggle%20to%20produce%20images%20that%20are%20consistent%20with%20the%20input%20prompt%2C%0Aoftentimes%20failing%20to%20capture%20object%20quantities%2C%20relations%20and%20attributes%0Aproperly.%20Existing%20solutions%20to%20improve%20prompt-image%20consistency%20suffer%20from%0Athe%20following%20challenges%3A%20%281%29%20they%20oftentimes%20require%20model%20fine-tuning%2C%20%282%29%0Athey%20only%20focus%20on%20nearby%20prompt%20samples%2C%20and%20%283%29%20they%20are%20affected%20by%0Aunfavorable%20trade-offs%20among%20image%20quality%2C%20representation%20diversity%2C%20and%0Aprompt-image%20consistency.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20and%0Aintroduce%20a%20T2I%20optimization-by-prompting%20framework%2C%20OPT2I%2C%20which%20leverages%20a%0Alarge%20language%20model%20%28LLM%29%20to%20improve%20prompt-image%20consistency%20in%20T2I%20models.%0AOur%20framework%20starts%20from%20a%20user%20prompt%20and%20iteratively%20generates%20revised%0Aprompts%20with%20the%20goal%20of%20maximizing%20a%20consistency%20score.%20Our%20extensive%0Avalidation%20on%20two%20datasets%2C%20MSCOCO%20and%20PartiPrompts%2C%20shows%20that%20OPT2I%20can%20boost%0Athe%20initial%20consistency%20score%20by%20up%20to%2024.9%25%20in%20terms%20of%20DSG%20score%20while%0Apreserving%20the%20FID%20and%20increasing%20the%20recall%20between%20generated%20and%20real%20data.%0AOur%20work%20paves%20the%20way%20toward%20building%20more%20reliable%20and%20robust%20T2I%20systems%20by%0Aharnessing%20the%20power%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17804v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Text-to-Image%20Consistency%20via%20Automatic%20Prompt%20Optimization&entry.906535625=Oscar%20Ma%C3%B1as%20and%20Pietro%20Astolfi%20and%20Melissa%20Hall%20and%20Candace%20Ross%20and%20Jack%20Urbanek%20and%20Adina%20Williams%20and%20Aishwarya%20Agrawal%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal&entry.1292438233=%20%20Impressive%20advances%20in%20text-to-image%20%28T2I%29%20generative%20models%20have%20yielded%20a%0Aplethora%20of%20high%20performing%20models%20which%20are%20able%20to%20generate%20aesthetically%0Aappealing%2C%20photorealistic%20images.%20Despite%20the%20progress%2C%20these%20models%20still%0Astruggle%20to%20produce%20images%20that%20are%20consistent%20with%20the%20input%20prompt%2C%0Aoftentimes%20failing%20to%20capture%20object%20quantities%2C%20relations%20and%20attributes%0Aproperly.%20Existing%20solutions%20to%20improve%20prompt-image%20consistency%20suffer%20from%0Athe%20following%20challenges%3A%20%281%29%20they%20oftentimes%20require%20model%20fine-tuning%2C%20%282%29%0Athey%20only%20focus%20on%20nearby%20prompt%20samples%2C%20and%20%283%29%20they%20are%20affected%20by%0Aunfavorable%20trade-offs%20among%20image%20quality%2C%20representation%20diversity%2C%20and%0Aprompt-image%20consistency.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20and%0Aintroduce%20a%20T2I%20optimization-by-prompting%20framework%2C%20OPT2I%2C%20which%20leverages%20a%0Alarge%20language%20model%20%28LLM%29%20to%20improve%20prompt-image%20consistency%20in%20T2I%20models.%0AOur%20framework%20starts%20from%20a%20user%20prompt%20and%20iteratively%20generates%20revised%0Aprompts%20with%20the%20goal%20of%20maximizing%20a%20consistency%20score.%20Our%20extensive%0Avalidation%20on%20two%20datasets%2C%20MSCOCO%20and%20PartiPrompts%2C%20shows%20that%20OPT2I%20can%20boost%0Athe%20initial%20consistency%20score%20by%20up%20to%2024.9%25%20in%20terms%20of%20DSG%20score%20while%0Apreserving%20the%20FID%20and%20increasing%20the%20recall%20between%20generated%20and%20real%20data.%0AOur%20work%20paves%20the%20way%20toward%20building%20more%20reliable%20and%20robust%20T2I%20systems%20by%0Aharnessing%20the%20power%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17804v1&entry.124074799=Read"},
{"title": "AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation", "author": "Qingping Sun and Yanjun Wang and Ailing Zeng and Wanqi Yin and Chen Wei and Wenjia Wang and Haiyi Mei and Chi Sing Leung and Ziwei Liu and Lei Yang and Zhongang Cai", "abstract": "  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh\nrecovery) involves the human body, hand, and expression estimation. Most\nexisting methods have tackled this task in a two-stage manner, first detecting\nthe human body part with an off-the-shelf detection model and inferring the\ndifferent human body parts individually. Despite the impressive results\nachieved, these methods suffer from 1) loss of valuable contextual information\nvia cropping, 2) introducing distractions, and 3) lacking inter-association\namong different persons and body parts, inevitably causing performance\ndegradation, especially for crowded scenes. To address these issues, we\nintroduce a novel all-in-one-stage framework, AiOS, for multiple expressive\nhuman pose and shape recovery without an additional human detection step.\nSpecifically, our method is built upon DETR, which treats multi-person\nwhole-body mesh recovery task as a progressive set prediction problem with\nvarious sequential detection. We devise the decoder tokens and extend them to\nour task. Specifically, we first employ a human token to probe a human location\nin the image and encode global features for each instance, which provides a\ncoarse location for the later transformer block. Then, we introduce a\njoint-related token to probe the human joint in the image and encoder a\nfine-grained local feature, which collaborates with the global feature to\nregress the whole-body mesh. This straightforward but effective model\noutperforms previous state-of-the-art methods by a 9% reduction in NMVE on\nAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a\n3% reduction in PVE on EgoBody.\n", "link": "http://arxiv.org/abs/2403.17934v1", "date": "2024-03-26", "relevancy": 2.1484, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5444}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5394}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5319}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AiOS%3A%20All-in-One-Stage%20Expressive%20Human%20Pose%20and%20Shape%20Estimation&body=Title%3A%20AiOS%3A%20All-in-One-Stage%20Expressive%20Human%20Pose%20and%20Shape%20Estimation%0AAuthor%3A%20Qingping%20Sun%20and%20Yanjun%20Wang%20and%20Ailing%20Zeng%20and%20Wanqi%20Yin%20and%20Chen%20Wei%20and%20Wenjia%20Wang%20and%20Haiyi%20Mei%20and%20Chi%20Sing%20Leung%20and%20Ziwei%20Liu%20and%20Lei%20Yang%20and%20Zhongang%20Cai%0AAbstract%3A%20%20%20Expressive%20human%20pose%20and%20shape%20estimation%20%28a.k.a.%203D%20whole-body%20mesh%0Arecovery%29%20involves%20the%20human%20body%2C%20hand%2C%20and%20expression%20estimation.%20Most%0Aexisting%20methods%20have%20tackled%20this%20task%20in%20a%20two-stage%20manner%2C%20first%20detecting%0Athe%20human%20body%20part%20with%20an%20off-the-shelf%20detection%20model%20and%20inferring%20the%0Adifferent%20human%20body%20parts%20individually.%20Despite%20the%20impressive%20results%0Aachieved%2C%20these%20methods%20suffer%20from%201%29%20loss%20of%20valuable%20contextual%20information%0Avia%20cropping%2C%202%29%20introducing%20distractions%2C%20and%203%29%20lacking%20inter-association%0Aamong%20different%20persons%20and%20body%20parts%2C%20inevitably%20causing%20performance%0Adegradation%2C%20especially%20for%20crowded%20scenes.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20all-in-one-stage%20framework%2C%20AiOS%2C%20for%20multiple%20expressive%0Ahuman%20pose%20and%20shape%20recovery%20without%20an%20additional%20human%20detection%20step.%0ASpecifically%2C%20our%20method%20is%20built%20upon%20DETR%2C%20which%20treats%20multi-person%0Awhole-body%20mesh%20recovery%20task%20as%20a%20progressive%20set%20prediction%20problem%20with%0Avarious%20sequential%20detection.%20We%20devise%20the%20decoder%20tokens%20and%20extend%20them%20to%0Aour%20task.%20Specifically%2C%20we%20first%20employ%20a%20human%20token%20to%20probe%20a%20human%20location%0Ain%20the%20image%20and%20encode%20global%20features%20for%20each%20instance%2C%20which%20provides%20a%0Acoarse%20location%20for%20the%20later%20transformer%20block.%20Then%2C%20we%20introduce%20a%0Ajoint-related%20token%20to%20probe%20the%20human%20joint%20in%20the%20image%20and%20encoder%20a%0Afine-grained%20local%20feature%2C%20which%20collaborates%20with%20the%20global%20feature%20to%0Aregress%20the%20whole-body%20mesh.%20This%20straightforward%20but%20effective%20model%0Aoutperforms%20previous%20state-of-the-art%20methods%20by%20a%209%25%20reduction%20in%20NMVE%20on%0AAGORA%2C%20a%2030%25%20reduction%20in%20PVE%20on%20EHF%2C%20a%2010%25%20reduction%20in%20PVE%20on%20ARCTIC%2C%20and%20a%0A3%25%20reduction%20in%20PVE%20on%20EgoBody.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17934v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AiOS%3A%20All-in-One-Stage%20Expressive%20Human%20Pose%20and%20Shape%20Estimation&entry.906535625=Qingping%20Sun%20and%20Yanjun%20Wang%20and%20Ailing%20Zeng%20and%20Wanqi%20Yin%20and%20Chen%20Wei%20and%20Wenjia%20Wang%20and%20Haiyi%20Mei%20and%20Chi%20Sing%20Leung%20and%20Ziwei%20Liu%20and%20Lei%20Yang%20and%20Zhongang%20Cai&entry.1292438233=%20%20Expressive%20human%20pose%20and%20shape%20estimation%20%28a.k.a.%203D%20whole-body%20mesh%0Arecovery%29%20involves%20the%20human%20body%2C%20hand%2C%20and%20expression%20estimation.%20Most%0Aexisting%20methods%20have%20tackled%20this%20task%20in%20a%20two-stage%20manner%2C%20first%20detecting%0Athe%20human%20body%20part%20with%20an%20off-the-shelf%20detection%20model%20and%20inferring%20the%0Adifferent%20human%20body%20parts%20individually.%20Despite%20the%20impressive%20results%0Aachieved%2C%20these%20methods%20suffer%20from%201%29%20loss%20of%20valuable%20contextual%20information%0Avia%20cropping%2C%202%29%20introducing%20distractions%2C%20and%203%29%20lacking%20inter-association%0Aamong%20different%20persons%20and%20body%20parts%2C%20inevitably%20causing%20performance%0Adegradation%2C%20especially%20for%20crowded%20scenes.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20all-in-one-stage%20framework%2C%20AiOS%2C%20for%20multiple%20expressive%0Ahuman%20pose%20and%20shape%20recovery%20without%20an%20additional%20human%20detection%20step.%0ASpecifically%2C%20our%20method%20is%20built%20upon%20DETR%2C%20which%20treats%20multi-person%0Awhole-body%20mesh%20recovery%20task%20as%20a%20progressive%20set%20prediction%20problem%20with%0Avarious%20sequential%20detection.%20We%20devise%20the%20decoder%20tokens%20and%20extend%20them%20to%0Aour%20task.%20Specifically%2C%20we%20first%20employ%20a%20human%20token%20to%20probe%20a%20human%20location%0Ain%20the%20image%20and%20encode%20global%20features%20for%20each%20instance%2C%20which%20provides%20a%0Acoarse%20location%20for%20the%20later%20transformer%20block.%20Then%2C%20we%20introduce%20a%0Ajoint-related%20token%20to%20probe%20the%20human%20joint%20in%20the%20image%20and%20encoder%20a%0Afine-grained%20local%20feature%2C%20which%20collaborates%20with%20the%20global%20feature%20to%0Aregress%20the%20whole-body%20mesh.%20This%20straightforward%20but%20effective%20model%0Aoutperforms%20previous%20state-of-the-art%20methods%20by%20a%209%25%20reduction%20in%20NMVE%20on%0AAGORA%2C%20a%2030%25%20reduction%20in%20PVE%20on%20EHF%2C%20a%2010%25%20reduction%20in%20PVE%20on%20ARCTIC%2C%20and%20a%0A3%25%20reduction%20in%20PVE%20on%20EgoBody.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17934v1&entry.124074799=Read"},
{"title": "Optimal Data Splitting in Distributed Optimization for Machine Learning", "author": "Daniil Medyakov and Gleb Molodtsov and Aleksandr Beznosikov and Alexander Gasnikov", "abstract": "  The distributed optimization problem has become increasingly relevant\nrecently. It has a lot of advantages such as processing a large amount of data\nin less time compared to non-distributed methods. However, most distributed\napproaches suffer from a significant bottleneck - the cost of communications.\nTherefore, a large amount of research has recently been directed at solving\nthis problem. One such approach uses local data similarity. In particular,\nthere exists an algorithm provably optimally exploiting the similarity\nproperty. But this result, as well as results from other works solve the\ncommunication bottleneck by focusing only on the fact that communication is\nsignificantly more expensive than local computing and does not take into\naccount the various capacities of network devices and the different\nrelationship between communication time and local computing expenses. We\nconsider this setup and the objective of this study is to achieve an optimal\nratio of distributed data between the server and local machines for any costs\nof communications and local computations. The running times of the network are\ncompared between uniform and optimal distributions. The superior theoretical\nperformance of our solutions is experimentally validated.\n", "link": "http://arxiv.org/abs/2401.07809v2", "date": "2024-03-26", "relevancy": 2.1256, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4437}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4171}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4146}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Optimal%20Data%20Splitting%20in%20Distributed%20Optimization%20for%20Machine%20Learning&body=Title%3A%20Optimal%20Data%20Splitting%20in%20Distributed%20Optimization%20for%20Machine%20Learning%0AAuthor%3A%20Daniil%20Medyakov%20and%20Gleb%20Molodtsov%20and%20Aleksandr%20Beznosikov%20and%20Alexander%20Gasnikov%0AAbstract%3A%20%20%20The%20distributed%20optimization%20problem%20has%20become%20increasingly%20relevant%0Arecently.%20It%20has%20a%20lot%20of%20advantages%20such%20as%20processing%20a%20large%20amount%20of%20data%0Ain%20less%20time%20compared%20to%20non-distributed%20methods.%20However%2C%20most%20distributed%0Aapproaches%20suffer%20from%20a%20significant%20bottleneck%20-%20the%20cost%20of%20communications.%0ATherefore%2C%20a%20large%20amount%20of%20research%20has%20recently%20been%20directed%20at%20solving%0Athis%20problem.%20One%20such%20approach%20uses%20local%20data%20similarity.%20In%20particular%2C%0Athere%20exists%20an%20algorithm%20provably%20optimally%20exploiting%20the%20similarity%0Aproperty.%20But%20this%20result%2C%20as%20well%20as%20results%20from%20other%20works%20solve%20the%0Acommunication%20bottleneck%20by%20focusing%20only%20on%20the%20fact%20that%20communication%20is%0Asignificantly%20more%20expensive%20than%20local%20computing%20and%20does%20not%20take%20into%0Aaccount%20the%20various%20capacities%20of%20network%20devices%20and%20the%20different%0Arelationship%20between%20communication%20time%20and%20local%20computing%20expenses.%20We%0Aconsider%20this%20setup%20and%20the%20objective%20of%20this%20study%20is%20to%20achieve%20an%20optimal%0Aratio%20of%20distributed%20data%20between%20the%20server%20and%20local%20machines%20for%20any%20costs%0Aof%20communications%20and%20local%20computations.%20The%20running%20times%20of%20the%20network%20are%0Acompared%20between%20uniform%20and%20optimal%20distributions.%20The%20superior%20theoretical%0Aperformance%20of%20our%20solutions%20is%20experimentally%20validated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07809v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Data%20Splitting%20in%20Distributed%20Optimization%20for%20Machine%20Learning&entry.906535625=Daniil%20Medyakov%20and%20Gleb%20Molodtsov%20and%20Aleksandr%20Beznosikov%20and%20Alexander%20Gasnikov&entry.1292438233=%20%20The%20distributed%20optimization%20problem%20has%20become%20increasingly%20relevant%0Arecently.%20It%20has%20a%20lot%20of%20advantages%20such%20as%20processing%20a%20large%20amount%20of%20data%0Ain%20less%20time%20compared%20to%20non-distributed%20methods.%20However%2C%20most%20distributed%0Aapproaches%20suffer%20from%20a%20significant%20bottleneck%20-%20the%20cost%20of%20communications.%0ATherefore%2C%20a%20large%20amount%20of%20research%20has%20recently%20been%20directed%20at%20solving%0Athis%20problem.%20One%20such%20approach%20uses%20local%20data%20similarity.%20In%20particular%2C%0Athere%20exists%20an%20algorithm%20provably%20optimally%20exploiting%20the%20similarity%0Aproperty.%20But%20this%20result%2C%20as%20well%20as%20results%20from%20other%20works%20solve%20the%0Acommunication%20bottleneck%20by%20focusing%20only%20on%20the%20fact%20that%20communication%20is%0Asignificantly%20more%20expensive%20than%20local%20computing%20and%20does%20not%20take%20into%0Aaccount%20the%20various%20capacities%20of%20network%20devices%20and%20the%20different%0Arelationship%20between%20communication%20time%20and%20local%20computing%20expenses.%20We%0Aconsider%20this%20setup%20and%20the%20objective%20of%20this%20study%20is%20to%20achieve%20an%20optimal%0Aratio%20of%20distributed%20data%20between%20the%20server%20and%20local%20machines%20for%20any%20costs%0Aof%20communications%20and%20local%20computations.%20The%20running%20times%20of%20the%20network%20are%0Acompared%20between%20uniform%20and%20optimal%20distributions.%20The%20superior%20theoretical%0Aperformance%20of%20our%20solutions%20is%20experimentally%20validated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07809v2&entry.124074799=Read"},
{"title": "System Calibration of a Field Phenotyping Robot with Multiple\n  High-Precision Profile Laser Scanners", "author": "Felix Esser and Gereon Tombrink and Andre Corneli\u00dfen and Lasse Klingbeil and Heiner Kuhlmann", "abstract": "  The creation of precise and high-resolution crop point clouds in agricultural\nfields has become a key challenge for high-throughput phenotyping applications.\nThis work implements a novel calibration method to calibrate the laser scanning\nsystem of an agricultural field robot consisting of two industrial-grade laser\nscanners used for high-precise 3D crop point cloud creation. The calibration\nmethod optimizes the transformation between the scanner origins and the robot\npose by minimizing 3D point omnivariances within the point cloud. Moreover, we\npresent a novel factor graph-based pose estimation method that fuses total\nstation prism measurements with IMU and GNSS heading information for\nhigh-precise pose determination during calibration. The root-mean-square error\nof the distances to a georeferenced ground truth point cloud results in 0.8 cm\nafter parameter optimization. Furthermore, our results show the importance of a\nreference point cloud in the calibration method needed to estimate the vertical\ntranslation of the calibration. Challenges arise due to non-static parameters\nwhile the robot moves, indicated by systematic deviations to a ground truth\nterrestrial laser scan.\n", "link": "http://arxiv.org/abs/2403.17788v1", "date": "2024-03-26", "relevancy": 2.1189, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5644}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5146}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5011}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20System%20Calibration%20of%20a%20Field%20Phenotyping%20Robot%20with%20Multiple%0A%20%20High-Precision%20Profile%20Laser%20Scanners&body=Title%3A%20System%20Calibration%20of%20a%20Field%20Phenotyping%20Robot%20with%20Multiple%0A%20%20High-Precision%20Profile%20Laser%20Scanners%0AAuthor%3A%20Felix%20Esser%20and%20Gereon%20Tombrink%20and%20Andre%20Corneli%C3%9Fen%20and%20Lasse%20Klingbeil%20and%20Heiner%20Kuhlmann%0AAbstract%3A%20%20%20The%20creation%20of%20precise%20and%20high-resolution%20crop%20point%20clouds%20in%20agricultural%0Afields%20has%20become%20a%20key%20challenge%20for%20high-throughput%20phenotyping%20applications.%0AThis%20work%20implements%20a%20novel%20calibration%20method%20to%20calibrate%20the%20laser%20scanning%0Asystem%20of%20an%20agricultural%20field%20robot%20consisting%20of%20two%20industrial-grade%20laser%0Ascanners%20used%20for%20high-precise%203D%20crop%20point%20cloud%20creation.%20The%20calibration%0Amethod%20optimizes%20the%20transformation%20between%20the%20scanner%20origins%20and%20the%20robot%0Apose%20by%20minimizing%203D%20point%20omnivariances%20within%20the%20point%20cloud.%20Moreover%2C%20we%0Apresent%20a%20novel%20factor%20graph-based%20pose%20estimation%20method%20that%20fuses%20total%0Astation%20prism%20measurements%20with%20IMU%20and%20GNSS%20heading%20information%20for%0Ahigh-precise%20pose%20determination%20during%20calibration.%20The%20root-mean-square%20error%0Aof%20the%20distances%20to%20a%20georeferenced%20ground%20truth%20point%20cloud%20results%20in%200.8%20cm%0Aafter%20parameter%20optimization.%20Furthermore%2C%20our%20results%20show%20the%20importance%20of%20a%0Areference%20point%20cloud%20in%20the%20calibration%20method%20needed%20to%20estimate%20the%20vertical%0Atranslation%20of%20the%20calibration.%20Challenges%20arise%20due%20to%20non-static%20parameters%0Awhile%20the%20robot%20moves%2C%20indicated%20by%20systematic%20deviations%20to%20a%20ground%20truth%0Aterrestrial%20laser%20scan.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17788v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=System%20Calibration%20of%20a%20Field%20Phenotyping%20Robot%20with%20Multiple%0A%20%20High-Precision%20Profile%20Laser%20Scanners&entry.906535625=Felix%20Esser%20and%20Gereon%20Tombrink%20and%20Andre%20Corneli%C3%9Fen%20and%20Lasse%20Klingbeil%20and%20Heiner%20Kuhlmann&entry.1292438233=%20%20The%20creation%20of%20precise%20and%20high-resolution%20crop%20point%20clouds%20in%20agricultural%0Afields%20has%20become%20a%20key%20challenge%20for%20high-throughput%20phenotyping%20applications.%0AThis%20work%20implements%20a%20novel%20calibration%20method%20to%20calibrate%20the%20laser%20scanning%0Asystem%20of%20an%20agricultural%20field%20robot%20consisting%20of%20two%20industrial-grade%20laser%0Ascanners%20used%20for%20high-precise%203D%20crop%20point%20cloud%20creation.%20The%20calibration%0Amethod%20optimizes%20the%20transformation%20between%20the%20scanner%20origins%20and%20the%20robot%0Apose%20by%20minimizing%203D%20point%20omnivariances%20within%20the%20point%20cloud.%20Moreover%2C%20we%0Apresent%20a%20novel%20factor%20graph-based%20pose%20estimation%20method%20that%20fuses%20total%0Astation%20prism%20measurements%20with%20IMU%20and%20GNSS%20heading%20information%20for%0Ahigh-precise%20pose%20determination%20during%20calibration.%20The%20root-mean-square%20error%0Aof%20the%20distances%20to%20a%20georeferenced%20ground%20truth%20point%20cloud%20results%20in%200.8%20cm%0Aafter%20parameter%20optimization.%20Furthermore%2C%20our%20results%20show%20the%20importance%20of%20a%0Areference%20point%20cloud%20in%20the%20calibration%20method%20needed%20to%20estimate%20the%20vertical%0Atranslation%20of%20the%20calibration.%20Challenges%20arise%20due%20to%20non-static%20parameters%0Awhile%20the%20robot%20moves%2C%20indicated%20by%20systematic%20deviations%20to%20a%20ground%20truth%0Aterrestrial%20laser%20scan.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17788v1&entry.124074799=Read"},
{"title": "Towards 3D Vision with Low-Cost Single-Photon Cameras", "author": "Fangzhou Mu and Carter Sifferman and Sacha Jungerman and Yiquan Li and Mark Han and Michael Gleicher and Mohit Gupta and Yin Li", "abstract": "  We present a method for reconstructing 3D shape of arbitrary Lambertian\nobjects based on measurements by miniature, energy-efficient, low-cost\nsingle-photon cameras. These cameras, operating as time resolved image sensors,\nilluminate the scene with a very fast pulse of diffuse light and record the\nshape of that pulse as it returns back from the scene at a high temporal\nresolution. We propose to model this image formation process, account for its\nnon-idealities, and adapt neural rendering to reconstruct 3D geometry from a\nset of spatially distributed sensors with known poses. We show that our\napproach can successfully recover complex 3D shapes from simulated data. We\nfurther demonstrate 3D object reconstruction from real-world captures,\nutilizing measurements from a commodity proximity sensor. Our work draws a\nconnection between image-based modeling and active range scanning and is a step\ntowards 3D vision with single-photon cameras.\n", "link": "http://arxiv.org/abs/2403.17801v1", "date": "2024-03-26", "relevancy": 2.101, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5398}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5328}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5119}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%203D%20Vision%20with%20Low-Cost%20Single-Photon%20Cameras&body=Title%3A%20Towards%203D%20Vision%20with%20Low-Cost%20Single-Photon%20Cameras%0AAuthor%3A%20Fangzhou%20Mu%20and%20Carter%20Sifferman%20and%20Sacha%20Jungerman%20and%20Yiquan%20Li%20and%20Mark%20Han%20and%20Michael%20Gleicher%20and%20Mohit%20Gupta%20and%20Yin%20Li%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20reconstructing%203D%20shape%20of%20arbitrary%20Lambertian%0Aobjects%20based%20on%20measurements%20by%20miniature%2C%20energy-efficient%2C%20low-cost%0Asingle-photon%20cameras.%20These%20cameras%2C%20operating%20as%20time%20resolved%20image%20sensors%2C%0Ailluminate%20the%20scene%20with%20a%20very%20fast%20pulse%20of%20diffuse%20light%20and%20record%20the%0Ashape%20of%20that%20pulse%20as%20it%20returns%20back%20from%20the%20scene%20at%20a%20high%20temporal%0Aresolution.%20We%20propose%20to%20model%20this%20image%20formation%20process%2C%20account%20for%20its%0Anon-idealities%2C%20and%20adapt%20neural%20rendering%20to%20reconstruct%203D%20geometry%20from%20a%0Aset%20of%20spatially%20distributed%20sensors%20with%20known%20poses.%20We%20show%20that%20our%0Aapproach%20can%20successfully%20recover%20complex%203D%20shapes%20from%20simulated%20data.%20We%0Afurther%20demonstrate%203D%20object%20reconstruction%20from%20real-world%20captures%2C%0Autilizing%20measurements%20from%20a%20commodity%20proximity%20sensor.%20Our%20work%20draws%20a%0Aconnection%20between%20image-based%20modeling%20and%20active%20range%20scanning%20and%20is%20a%20step%0Atowards%203D%20vision%20with%20single-photon%20cameras.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17801v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%203D%20Vision%20with%20Low-Cost%20Single-Photon%20Cameras&entry.906535625=Fangzhou%20Mu%20and%20Carter%20Sifferman%20and%20Sacha%20Jungerman%20and%20Yiquan%20Li%20and%20Mark%20Han%20and%20Michael%20Gleicher%20and%20Mohit%20Gupta%20and%20Yin%20Li&entry.1292438233=%20%20We%20present%20a%20method%20for%20reconstructing%203D%20shape%20of%20arbitrary%20Lambertian%0Aobjects%20based%20on%20measurements%20by%20miniature%2C%20energy-efficient%2C%20low-cost%0Asingle-photon%20cameras.%20These%20cameras%2C%20operating%20as%20time%20resolved%20image%20sensors%2C%0Ailluminate%20the%20scene%20with%20a%20very%20fast%20pulse%20of%20diffuse%20light%20and%20record%20the%0Ashape%20of%20that%20pulse%20as%20it%20returns%20back%20from%20the%20scene%20at%20a%20high%20temporal%0Aresolution.%20We%20propose%20to%20model%20this%20image%20formation%20process%2C%20account%20for%20its%0Anon-idealities%2C%20and%20adapt%20neural%20rendering%20to%20reconstruct%203D%20geometry%20from%20a%0Aset%20of%20spatially%20distributed%20sensors%20with%20known%20poses.%20We%20show%20that%20our%0Aapproach%20can%20successfully%20recover%20complex%203D%20shapes%20from%20simulated%20data.%20We%0Afurther%20demonstrate%203D%20object%20reconstruction%20from%20real-world%20captures%2C%0Autilizing%20measurements%20from%20a%20commodity%20proximity%20sensor.%20Our%20work%20draws%20a%0Aconnection%20between%20image-based%20modeling%20and%20active%20range%20scanning%20and%20is%20a%20step%0Atowards%203D%20vision%20with%20single-photon%20cameras.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17801v1&entry.124074799=Read"},
{"title": "HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic\n  Intersection and Vehicle-Infrastructure Cooperative", "author": "Cong Ma and Lei Qiao and Chengkai Zhu and Kai Liu and Zelong Kong and Qing Li and Xueqi Zhou and Yuheng Kan and Wei Wu", "abstract": "  Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous\nDriving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one\nof the important research area. Due to the complexity of traffic conditions\nsuch as blind spots and occlusion, it greatly limits the perception\ncapabilities of single-view roadside sensing systems. To further enhance the\naccuracy of roadside perception and provide better information to the vehicle\nside, in this paper, we constructed holographic intersections with various\nlayouts to build a large-scale multi-sensor holographic vehicle-infrastructure\ncooperation dataset, called HoloVIC. Our dataset includes 3 different types of\nsensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the\ndifferent intersections. Each intersection is equipped with 6-18 sensors to\ncapture synchronous data. While autonomous vehicles pass through these\nintersections for collecting VIC data. HoloVIC contains in total on 100k+\nsynchronous frames from different sensors. Additionally, we annotated 3D\nbounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs\nof the same objects across different devices and consecutive frames in\nsequence. Based on HoloVIC, we formulated four tasks to facilitate the\ndevelopment of related research. We also provide benchmarks for these tasks.\n", "link": "http://arxiv.org/abs/2403.02640v3", "date": "2024-03-26", "relevancy": 2.0985, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5366}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5266}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4897}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HoloVIC%3A%20Large-scale%20Dataset%20and%20Benchmark%20for%20Multi-Sensor%20Holographic%0A%20%20Intersection%20and%20Vehicle-Infrastructure%20Cooperative&body=Title%3A%20HoloVIC%3A%20Large-scale%20Dataset%20and%20Benchmark%20for%20Multi-Sensor%20Holographic%0A%20%20Intersection%20and%20Vehicle-Infrastructure%20Cooperative%0AAuthor%3A%20Cong%20Ma%20and%20Lei%20Qiao%20and%20Chengkai%20Zhu%20and%20Kai%20Liu%20and%20Zelong%20Kong%20and%20Qing%20Li%20and%20Xueqi%20Zhou%20and%20Yuheng%20Kan%20and%20Wei%20Wu%0AAbstract%3A%20%20%20Vehicle-to-everything%20%28V2X%29%20is%20a%20popular%20topic%20in%20the%20field%20of%20Autonomous%0ADriving%20in%20recent%20years.%20Vehicle-infrastructure%20cooperation%20%28VIC%29%20becomes%20one%0Aof%20the%20important%20research%20area.%20Due%20to%20the%20complexity%20of%20traffic%20conditions%0Asuch%20as%20blind%20spots%20and%20occlusion%2C%20it%20greatly%20limits%20the%20perception%0Acapabilities%20of%20single-view%20roadside%20sensing%20systems.%20To%20further%20enhance%20the%0Aaccuracy%20of%20roadside%20perception%20and%20provide%20better%20information%20to%20the%20vehicle%0Aside%2C%20in%20this%20paper%2C%20we%20constructed%20holographic%20intersections%20with%20various%0Alayouts%20to%20build%20a%20large-scale%20multi-sensor%20holographic%20vehicle-infrastructure%0Acooperation%20dataset%2C%20called%20HoloVIC.%20Our%20dataset%20includes%203%20different%20types%20of%0Asensors%20%28Camera%2C%20Lidar%2C%20Fisheye%29%20and%20employs%204%20sensor-layouts%20based%20on%20the%0Adifferent%20intersections.%20Each%20intersection%20is%20equipped%20with%206-18%20sensors%20to%0Acapture%20synchronous%20data.%20While%20autonomous%20vehicles%20pass%20through%20these%0Aintersections%20for%20collecting%20VIC%20data.%20HoloVIC%20contains%20in%20total%20on%20100k%2B%0Asynchronous%20frames%20from%20different%20sensors.%20Additionally%2C%20we%20annotated%203D%0Abounding%20boxes%20based%20on%20Camera%2C%20Fisheye%2C%20and%20Lidar.%20We%20also%20associate%20the%20IDs%0Aof%20the%20same%20objects%20across%20different%20devices%20and%20consecutive%20frames%20in%0Asequence.%20Based%20on%20HoloVIC%2C%20we%20formulated%20four%20tasks%20to%20facilitate%20the%0Adevelopment%20of%20related%20research.%20We%20also%20provide%20benchmarks%20for%20these%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02640v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloVIC%3A%20Large-scale%20Dataset%20and%20Benchmark%20for%20Multi-Sensor%20Holographic%0A%20%20Intersection%20and%20Vehicle-Infrastructure%20Cooperative&entry.906535625=Cong%20Ma%20and%20Lei%20Qiao%20and%20Chengkai%20Zhu%20and%20Kai%20Liu%20and%20Zelong%20Kong%20and%20Qing%20Li%20and%20Xueqi%20Zhou%20and%20Yuheng%20Kan%20and%20Wei%20Wu&entry.1292438233=%20%20Vehicle-to-everything%20%28V2X%29%20is%20a%20popular%20topic%20in%20the%20field%20of%20Autonomous%0ADriving%20in%20recent%20years.%20Vehicle-infrastructure%20cooperation%20%28VIC%29%20becomes%20one%0Aof%20the%20important%20research%20area.%20Due%20to%20the%20complexity%20of%20traffic%20conditions%0Asuch%20as%20blind%20spots%20and%20occlusion%2C%20it%20greatly%20limits%20the%20perception%0Acapabilities%20of%20single-view%20roadside%20sensing%20systems.%20To%20further%20enhance%20the%0Aaccuracy%20of%20roadside%20perception%20and%20provide%20better%20information%20to%20the%20vehicle%0Aside%2C%20in%20this%20paper%2C%20we%20constructed%20holographic%20intersections%20with%20various%0Alayouts%20to%20build%20a%20large-scale%20multi-sensor%20holographic%20vehicle-infrastructure%0Acooperation%20dataset%2C%20called%20HoloVIC.%20Our%20dataset%20includes%203%20different%20types%20of%0Asensors%20%28Camera%2C%20Lidar%2C%20Fisheye%29%20and%20employs%204%20sensor-layouts%20based%20on%20the%0Adifferent%20intersections.%20Each%20intersection%20is%20equipped%20with%206-18%20sensors%20to%0Acapture%20synchronous%20data.%20While%20autonomous%20vehicles%20pass%20through%20these%0Aintersections%20for%20collecting%20VIC%20data.%20HoloVIC%20contains%20in%20total%20on%20100k%2B%0Asynchronous%20frames%20from%20different%20sensors.%20Additionally%2C%20we%20annotated%203D%0Abounding%20boxes%20based%20on%20Camera%2C%20Fisheye%2C%20and%20Lidar.%20We%20also%20associate%20the%20IDs%0Aof%20the%20same%20objects%20across%20different%20devices%20and%20consecutive%20frames%20in%0Asequence.%20Based%20on%20HoloVIC%2C%20we%20formulated%20four%20tasks%20to%20facilitate%20the%0Adevelopment%20of%20related%20research.%20We%20also%20provide%20benchmarks%20for%20these%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02640v3&entry.124074799=Read"},
{"title": "The Need for Speed: Pruning Transformers with One Recipe", "author": "Samir Khaki and Konstantinos N. Plataniotis", "abstract": "  We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique\nfor $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework\nas a tool to increase the efficiency of pre-trained transformer architectures\n$\\textit{without requiring re-training}$. Recent works have explored improving\ntransformer efficiency, however often incur computationally expensive\nre-training procedures or depend on architecture-specific characteristics, thus\nimpeding practical wide-scale adoption. To address these shortcomings, the\nOPTIN framework leverages intermediate feature distillation, capturing the\nlong-range dependencies of model parameters (coined $\\textit{trajectory}$), to\nproduce state-of-the-art results on natural language, image classification,\ntransfer learning, and semantic segmentation tasks $\\textit{without\nre-training}$. Given a FLOP constraint, the OPTIN framework will compress the\nnetwork while maintaining competitive accuracy performance and improved\nthroughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP\nbaselines and a $0.5$% improvement from state-of-the-art methods on image\nclassification at competitive FLOPs reductions. We further demonstrate the\ngeneralization of tasks and architecture with comparative performance using\nMask2Former for semantic segmentation and cnn-style networks. OPTIN presents\none of the first one-shot efficient frameworks for compressing transformer\narchitectures that generalizes well across different class domains, in\nparticular: natural language and image-related tasks, without\n$\\textit{re-training}$.\n", "link": "http://arxiv.org/abs/2403.17921v1", "date": "2024-03-26", "relevancy": 2.0943, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5631}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5251}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5062}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Need%20for%20Speed%3A%20Pruning%20Transformers%20with%20One%20Recipe&body=Title%3A%20The%20Need%20for%20Speed%3A%20Pruning%20Transformers%20with%20One%20Recipe%0AAuthor%3A%20Samir%20Khaki%20and%20Konstantinos%20N.%20Plataniotis%0AAbstract%3A%20%20%20We%20introduce%20the%20%24%5Ctextbf%7BO%7D%24ne-shot%20%24%5Ctextbf%7BP%7D%24runing%20%24%5Ctextbf%7BT%7D%24echnique%0Afor%20%24%5Ctextbf%7BI%7D%24nterchangeable%20%24%5Ctextbf%7BN%7D%24etworks%20%28%24%5Ctextbf%7BOPTIN%7D%24%29%20framework%0Aas%20a%20tool%20to%20increase%20the%20efficiency%20of%20pre-trained%20transformer%20architectures%0A%24%5Ctextit%7Bwithout%20requiring%20re-training%7D%24.%20Recent%20works%20have%20explored%20improving%0Atransformer%20efficiency%2C%20however%20often%20incur%20computationally%20expensive%0Are-training%20procedures%20or%20depend%20on%20architecture-specific%20characteristics%2C%20thus%0Aimpeding%20practical%20wide-scale%20adoption.%20To%20address%20these%20shortcomings%2C%20the%0AOPTIN%20framework%20leverages%20intermediate%20feature%20distillation%2C%20capturing%20the%0Along-range%20dependencies%20of%20model%20parameters%20%28coined%20%24%5Ctextit%7Btrajectory%7D%24%29%2C%20to%0Aproduce%20state-of-the-art%20results%20on%20natural%20language%2C%20image%20classification%2C%0Atransfer%20learning%2C%20and%20semantic%20segmentation%20tasks%20%24%5Ctextit%7Bwithout%0Are-training%7D%24.%20Given%20a%20FLOP%20constraint%2C%20the%20OPTIN%20framework%20will%20compress%20the%0Anetwork%20while%20maintaining%20competitive%20accuracy%20performance%20and%20improved%0Athroughput.%20Particularly%2C%20we%20show%20a%20%24%5Cleq%202%24%25%20accuracy%20degradation%20from%20NLP%0Abaselines%20and%20a%20%240.5%24%25%20improvement%20from%20state-of-the-art%20methods%20on%20image%0Aclassification%20at%20competitive%20FLOPs%20reductions.%20We%20further%20demonstrate%20the%0Ageneralization%20of%20tasks%20and%20architecture%20with%20comparative%20performance%20using%0AMask2Former%20for%20semantic%20segmentation%20and%20cnn-style%20networks.%20OPTIN%20presents%0Aone%20of%20the%20first%20one-shot%20efficient%20frameworks%20for%20compressing%20transformer%0Aarchitectures%20that%20generalizes%20well%20across%20different%20class%20domains%2C%20in%0Aparticular%3A%20natural%20language%20and%20image-related%20tasks%2C%20without%0A%24%5Ctextit%7Bre-training%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17921v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Need%20for%20Speed%3A%20Pruning%20Transformers%20with%20One%20Recipe&entry.906535625=Samir%20Khaki%20and%20Konstantinos%20N.%20Plataniotis&entry.1292438233=%20%20We%20introduce%20the%20%24%5Ctextbf%7BO%7D%24ne-shot%20%24%5Ctextbf%7BP%7D%24runing%20%24%5Ctextbf%7BT%7D%24echnique%0Afor%20%24%5Ctextbf%7BI%7D%24nterchangeable%20%24%5Ctextbf%7BN%7D%24etworks%20%28%24%5Ctextbf%7BOPTIN%7D%24%29%20framework%0Aas%20a%20tool%20to%20increase%20the%20efficiency%20of%20pre-trained%20transformer%20architectures%0A%24%5Ctextit%7Bwithout%20requiring%20re-training%7D%24.%20Recent%20works%20have%20explored%20improving%0Atransformer%20efficiency%2C%20however%20often%20incur%20computationally%20expensive%0Are-training%20procedures%20or%20depend%20on%20architecture-specific%20characteristics%2C%20thus%0Aimpeding%20practical%20wide-scale%20adoption.%20To%20address%20these%20shortcomings%2C%20the%0AOPTIN%20framework%20leverages%20intermediate%20feature%20distillation%2C%20capturing%20the%0Along-range%20dependencies%20of%20model%20parameters%20%28coined%20%24%5Ctextit%7Btrajectory%7D%24%29%2C%20to%0Aproduce%20state-of-the-art%20results%20on%20natural%20language%2C%20image%20classification%2C%0Atransfer%20learning%2C%20and%20semantic%20segmentation%20tasks%20%24%5Ctextit%7Bwithout%0Are-training%7D%24.%20Given%20a%20FLOP%20constraint%2C%20the%20OPTIN%20framework%20will%20compress%20the%0Anetwork%20while%20maintaining%20competitive%20accuracy%20performance%20and%20improved%0Athroughput.%20Particularly%2C%20we%20show%20a%20%24%5Cleq%202%24%25%20accuracy%20degradation%20from%20NLP%0Abaselines%20and%20a%20%240.5%24%25%20improvement%20from%20state-of-the-art%20methods%20on%20image%0Aclassification%20at%20competitive%20FLOPs%20reductions.%20We%20further%20demonstrate%20the%0Ageneralization%20of%20tasks%20and%20architecture%20with%20comparative%20performance%20using%0AMask2Former%20for%20semantic%20segmentation%20and%20cnn-style%20networks.%20OPTIN%20presents%0Aone%20of%20the%20first%20one-shot%20efficient%20frameworks%20for%20compressing%20transformer%0Aarchitectures%20that%20generalizes%20well%20across%20different%20class%20domains%2C%20in%0Aparticular%3A%20natural%20language%20and%20image-related%20tasks%2C%20without%0A%24%5Ctextit%7Bre-training%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17921v1&entry.124074799=Read"},
{"title": "Efficient Pre-training for Localized Instruction Generation of Videos", "author": "Anil Batra and Davide Moltisanti and Laura Sevilla-Lara and Marcus Rohrbach and Frank Keller", "abstract": "  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n", "link": "http://arxiv.org/abs/2311.15964v2", "date": "2024-03-26", "relevancy": 2.0782, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5388}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5171}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5143}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Pre-training%20for%20Localized%20Instruction%20Generation%20of%20Videos&body=Title%3A%20Efficient%20Pre-training%20for%20Localized%20Instruction%20Generation%20of%20Videos%0AAuthor%3A%20Anil%20Batra%20and%20Davide%20Moltisanti%20and%20Laura%20Sevilla-Lara%20and%20Marcus%20Rohrbach%20and%20Frank%20Keller%0AAbstract%3A%20%20%20Procedural%20videos%20show%20step-by-step%20demonstrations%20of%20tasks%20like%20recipe%0Apreparation.%20Understanding%20such%20videos%20is%20challenging%2C%20involving%20the%20precise%0Alocalization%20of%20steps%20and%20the%20generation%20of%20textual%20instructions.%20Manually%0Aannotating%20steps%20and%20writing%20instructions%20is%20costly%2C%20which%20limits%20the%20size%20of%0Acurrent%20datasets%20and%20hinders%20effective%20learning.%20Leveraging%20large%20but%20noisy%0Avideo-transcript%20datasets%20for%20pre-training%20can%20boost%20performance%2C%20but%20demands%0Asignificant%20computational%20resources.%20Furthermore%2C%20transcripts%20contain%0Airrelevant%20content%20and%20exhibit%20style%20variation%20compared%20to%20instructions%20written%0Aby%20human%20annotators.%20To%20mitigate%20both%20issues%2C%20we%20propose%20a%20technique%2C%0ASieve-%26-Swap%2C%20to%20automatically%20curate%20a%20smaller%20dataset%3A%20%28i%29%20Sieve%20filters%0Airrelevant%20transcripts%20and%20%28ii%29%20Swap%20enhances%20the%20quality%20of%20the%20text%0Ainstruction%20by%20automatically%20replacing%20the%20transcripts%20with%20human-written%0Ainstructions%20from%20a%20text-only%20recipe%20dataset.%20The%20curated%20dataset%2C%20three%20orders%0Aof%20magnitude%20smaller%20than%20current%20web-scale%20datasets%2C%20enables%20efficient%0Atraining%20of%20large-scale%20models%20with%20competitive%20performance.%20We%20complement%20our%0ASieve-%5C%26-Swap%20approach%20with%20a%20Procedure%20Transformer%20%28ProcX%29%20for%20end-to-end%20step%0Alocalization%20and%20instruction%20generation%20for%20procedural%20videos.%20When%20this%20model%0Ais%20pre-trained%20on%20our%20curated%20dataset%2C%20it%20achieves%20state-of-the-art%20performance%0Ain%20zero-shot%20and%20finetuning%20settings%20on%20YouCook2%20and%20Tasty%2C%20while%20using%20a%0Afraction%20of%20the%20computational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15964v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Pre-training%20for%20Localized%20Instruction%20Generation%20of%20Videos&entry.906535625=Anil%20Batra%20and%20Davide%20Moltisanti%20and%20Laura%20Sevilla-Lara%20and%20Marcus%20Rohrbach%20and%20Frank%20Keller&entry.1292438233=%20%20Procedural%20videos%20show%20step-by-step%20demonstrations%20of%20tasks%20like%20recipe%0Apreparation.%20Understanding%20such%20videos%20is%20challenging%2C%20involving%20the%20precise%0Alocalization%20of%20steps%20and%20the%20generation%20of%20textual%20instructions.%20Manually%0Aannotating%20steps%20and%20writing%20instructions%20is%20costly%2C%20which%20limits%20the%20size%20of%0Acurrent%20datasets%20and%20hinders%20effective%20learning.%20Leveraging%20large%20but%20noisy%0Avideo-transcript%20datasets%20for%20pre-training%20can%20boost%20performance%2C%20but%20demands%0Asignificant%20computational%20resources.%20Furthermore%2C%20transcripts%20contain%0Airrelevant%20content%20and%20exhibit%20style%20variation%20compared%20to%20instructions%20written%0Aby%20human%20annotators.%20To%20mitigate%20both%20issues%2C%20we%20propose%20a%20technique%2C%0ASieve-%26-Swap%2C%20to%20automatically%20curate%20a%20smaller%20dataset%3A%20%28i%29%20Sieve%20filters%0Airrelevant%20transcripts%20and%20%28ii%29%20Swap%20enhances%20the%20quality%20of%20the%20text%0Ainstruction%20by%20automatically%20replacing%20the%20transcripts%20with%20human-written%0Ainstructions%20from%20a%20text-only%20recipe%20dataset.%20The%20curated%20dataset%2C%20three%20orders%0Aof%20magnitude%20smaller%20than%20current%20web-scale%20datasets%2C%20enables%20efficient%0Atraining%20of%20large-scale%20models%20with%20competitive%20performance.%20We%20complement%20our%0ASieve-%5C%26-Swap%20approach%20with%20a%20Procedure%20Transformer%20%28ProcX%29%20for%20end-to-end%20step%0Alocalization%20and%20instruction%20generation%20for%20procedural%20videos.%20When%20this%20model%0Ais%20pre-trained%20on%20our%20curated%20dataset%2C%20it%20achieves%20state-of-the-art%20performance%0Ain%20zero-shot%20and%20finetuning%20settings%20on%20YouCook2%20and%20Tasty%2C%20while%20using%20a%0Afraction%20of%20the%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15964v2&entry.124074799=Read"},
{"title": "DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields", "author": "Cheng-You Lu and Peisen Zhou and Angela Xing and Chandradeep Pokhariya and Arnab Dey and Ishaan Shah and Rugved Mavidipalli and Dylan Hu and Andrew Comport and Kefan Chen and Srinath Sridhar", "abstract": "  Advances in neural fields are enabling high-fidelity capture of the shape and\nappearance of dynamic 3D scenes. However, their capabilities lag behind those\noffered by conventional representations such as 2D videos because of\nalgorithmic challenges and the lack of large-scale multi-view real-world\ndatasets. We address the dataset limitation with DiVa-360, a real-world 360\ndynamic visual dataset that contains synchronized high-resolution and\nlong-duration multi-view video sequences of table-scale scenes captured using a\ncustomized low-cost system with 53 cameras. It contains 21 object-centric\nsequences categorized by different motion types, 25 intricate hand-object\ninteraction sequences, and 8 long-duration sequences for a total of 17.4 M\nimage frames. In addition, we provide foreground-background segmentation masks,\nsynchronized audio, and text descriptions. We benchmark the state-of-the-art\ndynamic neural field methods on DiVa-360 and provide insights about existing\nmethods and future challenges on long-duration neural field capture.\n", "link": "http://arxiv.org/abs/2307.16897v2", "date": "2024-03-26", "relevancy": 2.0754, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.528}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5252}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5088}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiVa-360%3A%20The%20Dynamic%20Visual%20Dataset%20for%20Immersive%20Neural%20Fields&body=Title%3A%20DiVa-360%3A%20The%20Dynamic%20Visual%20Dataset%20for%20Immersive%20Neural%20Fields%0AAuthor%3A%20Cheng-You%20Lu%20and%20Peisen%20Zhou%20and%20Angela%20Xing%20and%20Chandradeep%20Pokhariya%20and%20Arnab%20Dey%20and%20Ishaan%20Shah%20and%20Rugved%20Mavidipalli%20and%20Dylan%20Hu%20and%20Andrew%20Comport%20and%20Kefan%20Chen%20and%20Srinath%20Sridhar%0AAbstract%3A%20%20%20Advances%20in%20neural%20fields%20are%20enabling%20high-fidelity%20capture%20of%20the%20shape%20and%0Aappearance%20of%20dynamic%203D%20scenes.%20However%2C%20their%20capabilities%20lag%20behind%20those%0Aoffered%20by%20conventional%20representations%20such%20as%202D%20videos%20because%20of%0Aalgorithmic%20challenges%20and%20the%20lack%20of%20large-scale%20multi-view%20real-world%0Adatasets.%20We%20address%20the%20dataset%20limitation%20with%20DiVa-360%2C%20a%20real-world%20360%0Adynamic%20visual%20dataset%20that%20contains%20synchronized%20high-resolution%20and%0Along-duration%20multi-view%20video%20sequences%20of%20table-scale%20scenes%20captured%20using%20a%0Acustomized%20low-cost%20system%20with%2053%20cameras.%20It%20contains%2021%20object-centric%0Asequences%20categorized%20by%20different%20motion%20types%2C%2025%20intricate%20hand-object%0Ainteraction%20sequences%2C%20and%208%20long-duration%20sequences%20for%20a%20total%20of%2017.4%20M%0Aimage%20frames.%20In%20addition%2C%20we%20provide%20foreground-background%20segmentation%20masks%2C%0Asynchronized%20audio%2C%20and%20text%20descriptions.%20We%20benchmark%20the%20state-of-the-art%0Adynamic%20neural%20field%20methods%20on%20DiVa-360%20and%20provide%20insights%20about%20existing%0Amethods%20and%20future%20challenges%20on%20long-duration%20neural%20field%20capture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16897v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiVa-360%3A%20The%20Dynamic%20Visual%20Dataset%20for%20Immersive%20Neural%20Fields&entry.906535625=Cheng-You%20Lu%20and%20Peisen%20Zhou%20and%20Angela%20Xing%20and%20Chandradeep%20Pokhariya%20and%20Arnab%20Dey%20and%20Ishaan%20Shah%20and%20Rugved%20Mavidipalli%20and%20Dylan%20Hu%20and%20Andrew%20Comport%20and%20Kefan%20Chen%20and%20Srinath%20Sridhar&entry.1292438233=%20%20Advances%20in%20neural%20fields%20are%20enabling%20high-fidelity%20capture%20of%20the%20shape%20and%0Aappearance%20of%20dynamic%203D%20scenes.%20However%2C%20their%20capabilities%20lag%20behind%20those%0Aoffered%20by%20conventional%20representations%20such%20as%202D%20videos%20because%20of%0Aalgorithmic%20challenges%20and%20the%20lack%20of%20large-scale%20multi-view%20real-world%0Adatasets.%20We%20address%20the%20dataset%20limitation%20with%20DiVa-360%2C%20a%20real-world%20360%0Adynamic%20visual%20dataset%20that%20contains%20synchronized%20high-resolution%20and%0Along-duration%20multi-view%20video%20sequences%20of%20table-scale%20scenes%20captured%20using%20a%0Acustomized%20low-cost%20system%20with%2053%20cameras.%20It%20contains%2021%20object-centric%0Asequences%20categorized%20by%20different%20motion%20types%2C%2025%20intricate%20hand-object%0Ainteraction%20sequences%2C%20and%208%20long-duration%20sequences%20for%20a%20total%20of%2017.4%20M%0Aimage%20frames.%20In%20addition%2C%20we%20provide%20foreground-background%20segmentation%20masks%2C%0Asynchronized%20audio%2C%20and%20text%20descriptions.%20We%20benchmark%20the%20state-of-the-art%0Adynamic%20neural%20field%20methods%20on%20DiVa-360%20and%20provide%20insights%20about%20existing%0Amethods%20and%20future%20challenges%20on%20long-duration%20neural%20field%20capture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16897v2&entry.124074799=Read"},
{"title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models", "author": "Adam Ibrahim and Benjamin Th\u00e9rien and Kshitij Gupta and Mats L. Richter and Quentin Anthony and Timoth\u00e9e Lesort and Eugene Belilovsky and Irina Rish", "abstract": "  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n", "link": "http://arxiv.org/abs/2403.08763v3", "date": "2024-03-26", "relevancy": 2.0727, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5255}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5138}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models&body=Title%3A%20Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Adam%20Ibrahim%20and%20Benjamin%20Th%C3%A9rien%20and%20Kshitij%20Gupta%20and%20Mats%20L.%20Richter%20and%20Quentin%20Anthony%20and%20Timoth%C3%A9e%20Lesort%20and%20Eugene%20Belilovsky%20and%20Irina%20Rish%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20routinely%20pre-trained%20on%20billions%20of%20tokens%2C%0Aonly%20to%20start%20the%20process%20over%20again%20once%20new%20data%20becomes%20available.%20A%20much%0Amore%20efficient%20solution%20is%20to%20continually%20pre-train%20these%20models%2C%20saving%0Asignificant%20compute%20compared%20to%20re-training.%20However%2C%20the%20distribution%20shift%0Ainduced%20by%20new%20data%20typically%20results%20in%20degraded%20performance%20on%20previous%20data%0Aor%20poor%20adaptation%20to%20the%20new%20data.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20and%0Ascalable%20combination%20of%20learning%20rate%20%28LR%29%20re-warming%2C%20LR%20re-decaying%2C%20and%0Areplay%20of%20previous%20data%20is%20sufficient%20to%20match%20the%20performance%20of%20fully%0Are-training%20from%20scratch%20on%20all%20available%20data%2C%20as%20measured%20by%20the%20final%20loss%0Aand%20the%20average%20score%20on%20several%20language%20model%20%28LM%29%20evaluation%20benchmarks.%0ASpecifically%2C%20we%20show%20this%20for%20a%20weak%20but%20realistic%20distribution%20shift%20between%0Atwo%20commonly%20used%20LLM%20pre-training%20datasets%20%28English%24%5Crightarrow%24English%29%20and%20a%0Astronger%20distribution%20shift%20%28English%24%5Crightarrow%24German%29%20at%20the%20%24405%24M%0Aparameter%20model%20scale%20with%20large%20dataset%20sizes%20%28hundreds%20of%20billions%20of%0Atokens%29.%20Selecting%20the%20weak%20but%20realistic%20shift%20for%20larger-scale%20experiments%2C%0Awe%20also%20find%20that%20our%20continual%20learning%20strategies%20match%20the%20re-training%0Abaseline%20for%20a%2010B%20parameter%20LLM.%20Our%20results%20demonstrate%20that%20LLMs%20can%20be%0Asuccessfully%20updated%20via%20simple%20and%20scalable%20continual%20learning%20strategies%2C%0Amatching%20the%20re-training%20baseline%20using%20only%20a%20fraction%20of%20the%20compute.%0AFinally%2C%20inspired%20by%20previous%20work%2C%20we%20propose%20alternatives%20to%20the%20cosine%0Alearning%20rate%20schedule%20that%20help%20circumvent%20forgetting%20induced%20by%20LR%20re-warming%0Aand%20that%20are%20not%20bound%20to%20a%20fixed%20token%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08763v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models&entry.906535625=Adam%20Ibrahim%20and%20Benjamin%20Th%C3%A9rien%20and%20Kshitij%20Gupta%20and%20Mats%20L.%20Richter%20and%20Quentin%20Anthony%20and%20Timoth%C3%A9e%20Lesort%20and%20Eugene%20Belilovsky%20and%20Irina%20Rish&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20routinely%20pre-trained%20on%20billions%20of%20tokens%2C%0Aonly%20to%20start%20the%20process%20over%20again%20once%20new%20data%20becomes%20available.%20A%20much%0Amore%20efficient%20solution%20is%20to%20continually%20pre-train%20these%20models%2C%20saving%0Asignificant%20compute%20compared%20to%20re-training.%20However%2C%20the%20distribution%20shift%0Ainduced%20by%20new%20data%20typically%20results%20in%20degraded%20performance%20on%20previous%20data%0Aor%20poor%20adaptation%20to%20the%20new%20data.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20and%0Ascalable%20combination%20of%20learning%20rate%20%28LR%29%20re-warming%2C%20LR%20re-decaying%2C%20and%0Areplay%20of%20previous%20data%20is%20sufficient%20to%20match%20the%20performance%20of%20fully%0Are-training%20from%20scratch%20on%20all%20available%20data%2C%20as%20measured%20by%20the%20final%20loss%0Aand%20the%20average%20score%20on%20several%20language%20model%20%28LM%29%20evaluation%20benchmarks.%0ASpecifically%2C%20we%20show%20this%20for%20a%20weak%20but%20realistic%20distribution%20shift%20between%0Atwo%20commonly%20used%20LLM%20pre-training%20datasets%20%28English%24%5Crightarrow%24English%29%20and%20a%0Astronger%20distribution%20shift%20%28English%24%5Crightarrow%24German%29%20at%20the%20%24405%24M%0Aparameter%20model%20scale%20with%20large%20dataset%20sizes%20%28hundreds%20of%20billions%20of%0Atokens%29.%20Selecting%20the%20weak%20but%20realistic%20shift%20for%20larger-scale%20experiments%2C%0Awe%20also%20find%20that%20our%20continual%20learning%20strategies%20match%20the%20re-training%0Abaseline%20for%20a%2010B%20parameter%20LLM.%20Our%20results%20demonstrate%20that%20LLMs%20can%20be%0Asuccessfully%20updated%20via%20simple%20and%20scalable%20continual%20learning%20strategies%2C%0Amatching%20the%20re-training%20baseline%20using%20only%20a%20fraction%20of%20the%20compute.%0AFinally%2C%20inspired%20by%20previous%20work%2C%20we%20propose%20alternatives%20to%20the%20cosine%0Alearning%20rate%20schedule%20that%20help%20circumvent%20forgetting%20induced%20by%20LR%20re-warming%0Aand%20that%20are%20not%20bound%20to%20a%20fixed%20token%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08763v3&entry.124074799=Read"},
{"title": "SimLVSeg: Simplifying Left Ventricular Segmentation in 2D+Time\n  Echocardiograms with Self- and Weakly-Supervised Learning", "author": "Fadillah Maani and Asim Ukaye and Nada Saadi and Numan Saeed and Mohammad Yaqub", "abstract": "  Echocardiography has become an indispensable clinical imaging modality for\ngeneral heart health assessment. From calculating biomarkers such as ejection\nfraction to the probability of a patient's heart failure, accurate segmentation\nof the heart structures allows doctors to assess the heart's condition and\ndevise treatments with greater precision and accuracy. However, achieving\naccurate and reliable left ventricle segmentation is time-consuming and\nchallenging due to different reasons. Hence, clinicians often rely on\nsegmenting the left ventricular (LV) in two specific echocardiogram frames to\nmake a diagnosis. This limited coverage in manual LV segmentation poses a\nchallenge for developing automatic LV segmentation with high temporal\nconsistency, as the resulting dataset is typically annotated sparsely. In\nresponse to this challenge, this work introduces SimLVSeg, a novel paradigm\nthat enables video-based networks for consistent LV segmentation from sparsely\nannotated echocardiogram videos. SimLVSeg consists of self-supervised\npre-training with temporal masking, followed by weakly supervised learning\ntailored for LV segmentation from sparse annotations. We demonstrate how\nSimLVSeg outperforms the state-of-the-art solutions by achieving a 93.32%\n(95%CI 93.21-93.43%) dice score on the largest 2D+time echocardiography dataset\n(EchoNet-Dynamic) while being more efficient. SimLVSeg is compatible with two\ntypes of video segmentation networks: 2D super image and 3D segmentation. To\nshow the effectiveness of our approach, we provide extensive ablation studies,\nincluding pre-training settings and various deep learning backbones. We further\nconduct an out-of-distribution test to showcase SimLVSeg's generalizability on\nunseen distribution (CAMUS dataset). The code is publicly available at\nhttps://github.com/fadamsyah/SimLVSeg.\n", "link": "http://arxiv.org/abs/2310.00454v3", "date": "2024-03-26", "relevancy": 2.0688, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5238}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5122}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SimLVSeg%3A%20Simplifying%20Left%20Ventricular%20Segmentation%20in%202D%2BTime%0A%20%20Echocardiograms%20with%20Self-%20and%20Weakly-Supervised%20Learning&body=Title%3A%20SimLVSeg%3A%20Simplifying%20Left%20Ventricular%20Segmentation%20in%202D%2BTime%0A%20%20Echocardiograms%20with%20Self-%20and%20Weakly-Supervised%20Learning%0AAuthor%3A%20Fadillah%20Maani%20and%20Asim%20Ukaye%20and%20Nada%20Saadi%20and%20Numan%20Saeed%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Echocardiography%20has%20become%20an%20indispensable%20clinical%20imaging%20modality%20for%0Ageneral%20heart%20health%20assessment.%20From%20calculating%20biomarkers%20such%20as%20ejection%0Afraction%20to%20the%20probability%20of%20a%20patient%27s%20heart%20failure%2C%20accurate%20segmentation%0Aof%20the%20heart%20structures%20allows%20doctors%20to%20assess%20the%20heart%27s%20condition%20and%0Adevise%20treatments%20with%20greater%20precision%20and%20accuracy.%20However%2C%20achieving%0Aaccurate%20and%20reliable%20left%20ventricle%20segmentation%20is%20time-consuming%20and%0Achallenging%20due%20to%20different%20reasons.%20Hence%2C%20clinicians%20often%20rely%20on%0Asegmenting%20the%20left%20ventricular%20%28LV%29%20in%20two%20specific%20echocardiogram%20frames%20to%0Amake%20a%20diagnosis.%20This%20limited%20coverage%20in%20manual%20LV%20segmentation%20poses%20a%0Achallenge%20for%20developing%20automatic%20LV%20segmentation%20with%20high%20temporal%0Aconsistency%2C%20as%20the%20resulting%20dataset%20is%20typically%20annotated%20sparsely.%20In%0Aresponse%20to%20this%20challenge%2C%20this%20work%20introduces%20SimLVSeg%2C%20a%20novel%20paradigm%0Athat%20enables%20video-based%20networks%20for%20consistent%20LV%20segmentation%20from%20sparsely%0Aannotated%20echocardiogram%20videos.%20SimLVSeg%20consists%20of%20self-supervised%0Apre-training%20with%20temporal%20masking%2C%20followed%20by%20weakly%20supervised%20learning%0Atailored%20for%20LV%20segmentation%20from%20sparse%20annotations.%20We%20demonstrate%20how%0ASimLVSeg%20outperforms%20the%20state-of-the-art%20solutions%20by%20achieving%20a%2093.32%25%0A%2895%25CI%2093.21-93.43%25%29%20dice%20score%20on%20the%20largest%202D%2Btime%20echocardiography%20dataset%0A%28EchoNet-Dynamic%29%20while%20being%20more%20efficient.%20SimLVSeg%20is%20compatible%20with%20two%0Atypes%20of%20video%20segmentation%20networks%3A%202D%20super%20image%20and%203D%20segmentation.%20To%0Ashow%20the%20effectiveness%20of%20our%20approach%2C%20we%20provide%20extensive%20ablation%20studies%2C%0Aincluding%20pre-training%20settings%20and%20various%20deep%20learning%20backbones.%20We%20further%0Aconduct%20an%20out-of-distribution%20test%20to%20showcase%20SimLVSeg%27s%20generalizability%20on%0Aunseen%20distribution%20%28CAMUS%20dataset%29.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/fadamsyah/SimLVSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00454v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimLVSeg%3A%20Simplifying%20Left%20Ventricular%20Segmentation%20in%202D%2BTime%0A%20%20Echocardiograms%20with%20Self-%20and%20Weakly-Supervised%20Learning&entry.906535625=Fadillah%20Maani%20and%20Asim%20Ukaye%20and%20Nada%20Saadi%20and%20Numan%20Saeed%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Echocardiography%20has%20become%20an%20indispensable%20clinical%20imaging%20modality%20for%0Ageneral%20heart%20health%20assessment.%20From%20calculating%20biomarkers%20such%20as%20ejection%0Afraction%20to%20the%20probability%20of%20a%20patient%27s%20heart%20failure%2C%20accurate%20segmentation%0Aof%20the%20heart%20structures%20allows%20doctors%20to%20assess%20the%20heart%27s%20condition%20and%0Adevise%20treatments%20with%20greater%20precision%20and%20accuracy.%20However%2C%20achieving%0Aaccurate%20and%20reliable%20left%20ventricle%20segmentation%20is%20time-consuming%20and%0Achallenging%20due%20to%20different%20reasons.%20Hence%2C%20clinicians%20often%20rely%20on%0Asegmenting%20the%20left%20ventricular%20%28LV%29%20in%20two%20specific%20echocardiogram%20frames%20to%0Amake%20a%20diagnosis.%20This%20limited%20coverage%20in%20manual%20LV%20segmentation%20poses%20a%0Achallenge%20for%20developing%20automatic%20LV%20segmentation%20with%20high%20temporal%0Aconsistency%2C%20as%20the%20resulting%20dataset%20is%20typically%20annotated%20sparsely.%20In%0Aresponse%20to%20this%20challenge%2C%20this%20work%20introduces%20SimLVSeg%2C%20a%20novel%20paradigm%0Athat%20enables%20video-based%20networks%20for%20consistent%20LV%20segmentation%20from%20sparsely%0Aannotated%20echocardiogram%20videos.%20SimLVSeg%20consists%20of%20self-supervised%0Apre-training%20with%20temporal%20masking%2C%20followed%20by%20weakly%20supervised%20learning%0Atailored%20for%20LV%20segmentation%20from%20sparse%20annotations.%20We%20demonstrate%20how%0ASimLVSeg%20outperforms%20the%20state-of-the-art%20solutions%20by%20achieving%20a%2093.32%25%0A%2895%25CI%2093.21-93.43%25%29%20dice%20score%20on%20the%20largest%202D%2Btime%20echocardiography%20dataset%0A%28EchoNet-Dynamic%29%20while%20being%20more%20efficient.%20SimLVSeg%20is%20compatible%20with%20two%0Atypes%20of%20video%20segmentation%20networks%3A%202D%20super%20image%20and%203D%20segmentation.%20To%0Ashow%20the%20effectiveness%20of%20our%20approach%2C%20we%20provide%20extensive%20ablation%20studies%2C%0Aincluding%20pre-training%20settings%20and%20various%20deep%20learning%20backbones.%20We%20further%0Aconduct%20an%20out-of-distribution%20test%20to%20showcase%20SimLVSeg%27s%20generalizability%20on%0Aunseen%20distribution%20%28CAMUS%20dataset%29.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/fadamsyah/SimLVSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00454v3&entry.124074799=Read"},
{"title": "DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing", "author": "Matias Turkulainen and Xuqian Ren and Iaroslav Melekhov and Otto Seiskari and Esa Rahtu and Juho Kannala", "abstract": "  3D Gaussian splatting, a novel differentiable rendering technique, has\nachieved state-of-the-art novel view synthesis results with high rendering\nspeeds and relatively low training times. However, its performance on scenes\ncommonly seen in indoor datasets is poor due to the lack of geometric\nconstraints during optimization. We extend 3D Gaussian splatting with depth and\nnormal cues to tackle challenging indoor datasets and showcase techniques for\nefficient mesh extraction, an important downstream application. Specifically,\nwe regularize the optimization procedure with depth information, enforce local\nsmoothness of nearby Gaussians, and use the geometry of the 3D Gaussians\nsupervised by normal cues to achieve better alignment with the true scene\ngeometry. We improve depth estimation and novel view synthesis results over\nbaselines and show how this simple yet effective regularization technique can\nbe used to directly extract meshes from the Gaussian representation yielding\nmore physically accurate reconstructions on indoor scenes. Our code will be\nreleased in https://github.com/maturk/dn-splatter.\n", "link": "http://arxiv.org/abs/2403.17822v1", "date": "2024-03-26", "relevancy": 2.0481, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5352}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5083}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5065}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DN-Splatter%3A%20Depth%20and%20Normal%20Priors%20for%20Gaussian%20Splatting%20and%20Meshing&body=Title%3A%20DN-Splatter%3A%20Depth%20and%20Normal%20Priors%20for%20Gaussian%20Splatting%20and%20Meshing%0AAuthor%3A%20Matias%20Turkulainen%20and%20Xuqian%20Ren%20and%20Iaroslav%20Melekhov%20and%20Otto%20Seiskari%20and%20Esa%20Rahtu%20and%20Juho%20Kannala%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%2C%20a%20novel%20differentiable%20rendering%20technique%2C%20has%0Aachieved%20state-of-the-art%20novel%20view%20synthesis%20results%20with%20high%20rendering%0Aspeeds%20and%20relatively%20low%20training%20times.%20However%2C%20its%20performance%20on%20scenes%0Acommonly%20seen%20in%20indoor%20datasets%20is%20poor%20due%20to%20the%20lack%20of%20geometric%0Aconstraints%20during%20optimization.%20We%20extend%203D%20Gaussian%20splatting%20with%20depth%20and%0Anormal%20cues%20to%20tackle%20challenging%20indoor%20datasets%20and%20showcase%20techniques%20for%0Aefficient%20mesh%20extraction%2C%20an%20important%20downstream%20application.%20Specifically%2C%0Awe%20regularize%20the%20optimization%20procedure%20with%20depth%20information%2C%20enforce%20local%0Asmoothness%20of%20nearby%20Gaussians%2C%20and%20use%20the%20geometry%20of%20the%203D%20Gaussians%0Asupervised%20by%20normal%20cues%20to%20achieve%20better%20alignment%20with%20the%20true%20scene%0Ageometry.%20We%20improve%20depth%20estimation%20and%20novel%20view%20synthesis%20results%20over%0Abaselines%20and%20show%20how%20this%20simple%20yet%20effective%20regularization%20technique%20can%0Abe%20used%20to%20directly%20extract%20meshes%20from%20the%20Gaussian%20representation%20yielding%0Amore%20physically%20accurate%20reconstructions%20on%20indoor%20scenes.%20Our%20code%20will%20be%0Areleased%20in%20https%3A//github.com/maturk/dn-splatter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17822v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DN-Splatter%3A%20Depth%20and%20Normal%20Priors%20for%20Gaussian%20Splatting%20and%20Meshing&entry.906535625=Matias%20Turkulainen%20and%20Xuqian%20Ren%20and%20Iaroslav%20Melekhov%20and%20Otto%20Seiskari%20and%20Esa%20Rahtu%20and%20Juho%20Kannala&entry.1292438233=%20%203D%20Gaussian%20splatting%2C%20a%20novel%20differentiable%20rendering%20technique%2C%20has%0Aachieved%20state-of-the-art%20novel%20view%20synthesis%20results%20with%20high%20rendering%0Aspeeds%20and%20relatively%20low%20training%20times.%20However%2C%20its%20performance%20on%20scenes%0Acommonly%20seen%20in%20indoor%20datasets%20is%20poor%20due%20to%20the%20lack%20of%20geometric%0Aconstraints%20during%20optimization.%20We%20extend%203D%20Gaussian%20splatting%20with%20depth%20and%0Anormal%20cues%20to%20tackle%20challenging%20indoor%20datasets%20and%20showcase%20techniques%20for%0Aefficient%20mesh%20extraction%2C%20an%20important%20downstream%20application.%20Specifically%2C%0Awe%20regularize%20the%20optimization%20procedure%20with%20depth%20information%2C%20enforce%20local%0Asmoothness%20of%20nearby%20Gaussians%2C%20and%20use%20the%20geometry%20of%20the%203D%20Gaussians%0Asupervised%20by%20normal%20cues%20to%20achieve%20better%20alignment%20with%20the%20true%20scene%0Ageometry.%20We%20improve%20depth%20estimation%20and%20novel%20view%20synthesis%20results%20over%0Abaselines%20and%20show%20how%20this%20simple%20yet%20effective%20regularization%20technique%20can%0Abe%20used%20to%20directly%20extract%20meshes%20from%20the%20Gaussian%20representation%20yielding%0Amore%20physically%20accurate%20reconstructions%20on%20indoor%20scenes.%20Our%20code%20will%20be%0Areleased%20in%20https%3A//github.com/maturk/dn-splatter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17822v1&entry.124074799=Read"},
{"title": "LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data", "author": "Vijeta Deshpande and Minhwa Lee and Zonghai Yao and Zihao Zhang and Jason Brian Gibbons and Hong Yu", "abstract": "  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n", "link": "http://arxiv.org/abs/2402.13452v2", "date": "2024-03-26", "relevancy": 2.0452, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4119}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4076}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4076}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LocalTweets%20to%20LocalHealth%3A%20A%20Mental%20Health%20Surveillance%20Framework%20Based%0A%20%20on%20Twitter%20Data&body=Title%3A%20LocalTweets%20to%20LocalHealth%3A%20A%20Mental%20Health%20Surveillance%20Framework%20Based%0A%20%20on%20Twitter%20Data%0AAuthor%3A%20Vijeta%20Deshpande%20and%20Minhwa%20Lee%20and%20Zonghai%20Yao%20and%20Zihao%20Zhang%20and%20Jason%20Brian%20Gibbons%20and%20Hong%20Yu%0AAbstract%3A%20%20%20Prior%20research%20on%20Twitter%20%28now%20X%29%20data%20has%20provided%20positive%20evidence%20of%20its%0Autility%20in%20developing%20supplementary%20health%20surveillance%20systems.%20In%20this%20study%2C%0Awe%20present%20a%20new%20framework%20to%20surveil%20public%20health%2C%20focusing%20on%20mental%20health%0A%28MH%29%20outcomes.%20We%20hypothesize%20that%20locally%20posted%20tweets%20are%20indicative%20of%0Alocal%20MH%20outcomes%20and%20collect%20tweets%20posted%20from%20765%20neighborhoods%20%28census%0Ablock%20groups%29%20in%20the%20USA.%20We%20pair%20these%20tweets%20from%20each%20neighborhood%20with%20the%0Acorresponding%20MH%20outcome%20reported%20by%20the%20Center%20for%20Disease%20Control%20%28CDC%29%20to%0Acreate%20a%20benchmark%20dataset%2C%20LocalTweets.%20With%20LocalTweets%2C%20we%20present%20the%20first%0Apopulation-level%20evaluation%20task%20for%20Twitter-based%20MH%20surveillance%20systems.%20We%0Athen%20develop%20an%20efficient%20and%20effective%20method%2C%20LocalHealth%2C%20for%20predicting%20MH%0Aoutcomes%20based%20on%20LocalTweets.%20When%20used%20with%20GPT3.5%2C%20LocalHealth%20achieves%20the%0Ahighest%20F1-score%20and%20accuracy%20of%200.7429%20and%2079.78%5C%25%2C%20respectively%2C%20a%2059%5C%25%0Aimprovement%20in%20F1-score%20over%20the%20GPT3.5%20in%20zero-shot%20setting.%20We%20also%20utilize%0ALocalHealth%20to%20extrapolate%20CDC%27s%20estimates%20to%20proxy%20unreported%20neighborhoods%2C%0Aachieving%20an%20F1-score%20of%200.7291.%20Our%20work%20suggests%20that%20Twitter%20data%20can%20be%0Aeffectively%20leveraged%20to%20simulate%20neighborhood-level%20MH%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13452v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocalTweets%20to%20LocalHealth%3A%20A%20Mental%20Health%20Surveillance%20Framework%20Based%0A%20%20on%20Twitter%20Data&entry.906535625=Vijeta%20Deshpande%20and%20Minhwa%20Lee%20and%20Zonghai%20Yao%20and%20Zihao%20Zhang%20and%20Jason%20Brian%20Gibbons%20and%20Hong%20Yu&entry.1292438233=%20%20Prior%20research%20on%20Twitter%20%28now%20X%29%20data%20has%20provided%20positive%20evidence%20of%20its%0Autility%20in%20developing%20supplementary%20health%20surveillance%20systems.%20In%20this%20study%2C%0Awe%20present%20a%20new%20framework%20to%20surveil%20public%20health%2C%20focusing%20on%20mental%20health%0A%28MH%29%20outcomes.%20We%20hypothesize%20that%20locally%20posted%20tweets%20are%20indicative%20of%0Alocal%20MH%20outcomes%20and%20collect%20tweets%20posted%20from%20765%20neighborhoods%20%28census%0Ablock%20groups%29%20in%20the%20USA.%20We%20pair%20these%20tweets%20from%20each%20neighborhood%20with%20the%0Acorresponding%20MH%20outcome%20reported%20by%20the%20Center%20for%20Disease%20Control%20%28CDC%29%20to%0Acreate%20a%20benchmark%20dataset%2C%20LocalTweets.%20With%20LocalTweets%2C%20we%20present%20the%20first%0Apopulation-level%20evaluation%20task%20for%20Twitter-based%20MH%20surveillance%20systems.%20We%0Athen%20develop%20an%20efficient%20and%20effective%20method%2C%20LocalHealth%2C%20for%20predicting%20MH%0Aoutcomes%20based%20on%20LocalTweets.%20When%20used%20with%20GPT3.5%2C%20LocalHealth%20achieves%20the%0Ahighest%20F1-score%20and%20accuracy%20of%200.7429%20and%2079.78%5C%25%2C%20respectively%2C%20a%2059%5C%25%0Aimprovement%20in%20F1-score%20over%20the%20GPT3.5%20in%20zero-shot%20setting.%20We%20also%20utilize%0ALocalHealth%20to%20extrapolate%20CDC%27s%20estimates%20to%20proxy%20unreported%20neighborhoods%2C%0Aachieving%20an%20F1-score%20of%200.7291.%20Our%20work%20suggests%20that%20Twitter%20data%20can%20be%0Aeffectively%20leveraged%20to%20simulate%20neighborhood-level%20MH%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13452v2&entry.124074799=Read"},
{"title": "Semi-Supervised Crowd Counting from Unlabeled Data", "author": "Haoran Duan and Fan Wan and Rui Sun and Zeyu Wang and Varun Ojha and Yu Guan and Hubert P. H. Shum and Bingzhang Hu and Yang Long", "abstract": "  Automatic Crowd behavior analysis can be applied to effectively help the\ndaily transportation statistics and planning, which helps the smart city\nconstruction. As one of the most important keys, crowd counting has drawn\nincreasing attention. Recent works achieved promising performance but relied on\nthe supervised paradigm with expensive crowd annotations. To alleviate the\nannotation cost in real-world transportation scenarios, in this work we\nproposed a semi-supervised learning framework $S^{4}\\textit{Crowd}$, which can\nleverage both unlabeled/labeled data for robust crowd counting. In the\nunsupervised pathway, two \\textit{self-supervised losses} were proposed to\nsimulate the crowd variations such as scale, illumination, based on which\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit \\textit{Gated-Crowd-Recurrent-Unit\n(GCRU)}, which can preserve discriminant crowd information by extracting\nsecond-order statistics, yielding pseudo labels with improved quality. A joint\nloss including both unsupervised/supervised information was proposed, and a\ndynamic weighting strategy was employed to balance the importance of the\nunsupervised loss and supervised loss at different training stages. We\nconducted extensive experiments on four popular crowd counting datasets in\nsemi-supervised settings. Experimental results supported the effectiveness of\neach proposed component in our $S^{4}$Crowd framework. Our method achieved\ncompetitive performance in semi-supervised learning approaches on these crowd\ncounting datasets.\n", "link": "http://arxiv.org/abs/2108.13969v3", "date": "2024-03-26", "relevancy": 2.0437, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5177}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5176}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4774}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Crowd%20Counting%20from%20Unlabeled%20Data&body=Title%3A%20Semi-Supervised%20Crowd%20Counting%20from%20Unlabeled%20Data%0AAuthor%3A%20Haoran%20Duan%20and%20Fan%20Wan%20and%20Rui%20Sun%20and%20Zeyu%20Wang%20and%20Varun%20Ojha%20and%20Yu%20Guan%20and%20Hubert%20P.%20H.%20Shum%20and%20Bingzhang%20Hu%20and%20Yang%20Long%0AAbstract%3A%20%20%20Automatic%20Crowd%20behavior%20analysis%20can%20be%20applied%20to%20effectively%20help%20the%0Adaily%20transportation%20statistics%20and%20planning%2C%20which%20helps%20the%20smart%20city%0Aconstruction.%20As%20one%20of%20the%20most%20important%20keys%2C%20crowd%20counting%20has%20drawn%0Aincreasing%20attention.%20Recent%20works%20achieved%20promising%20performance%20but%20relied%20on%0Athe%20supervised%20paradigm%20with%20expensive%20crowd%20annotations.%20To%20alleviate%20the%0Aannotation%20cost%20in%20real-world%20transportation%20scenarios%2C%20in%20this%20work%20we%0Aproposed%20a%20semi-supervised%20learning%20framework%20%24S%5E%7B4%7D%5Ctextit%7BCrowd%7D%24%2C%20which%20can%0Aleverage%20both%20unlabeled/labeled%20data%20for%20robust%20crowd%20counting.%20In%20the%0Aunsupervised%20pathway%2C%20two%20%5Ctextit%7Bself-supervised%20losses%7D%20were%20proposed%20to%0Asimulate%20the%20crowd%20variations%20such%20as%20scale%2C%20illumination%2C%20based%20on%20which%0Asupervised%20information%20pseudo%20labels%20were%20generated%20and%20gradually%20refined.%20We%0Aalso%20proposed%20a%20crowd-driven%20recurrent%20unit%20%5Ctextit%7BGated-Crowd-Recurrent-Unit%0A%28GCRU%29%7D%2C%20which%20can%20preserve%20discriminant%20crowd%20information%20by%20extracting%0Asecond-order%20statistics%2C%20yielding%20pseudo%20labels%20with%20improved%20quality.%20A%20joint%0Aloss%20including%20both%20unsupervised/supervised%20information%20was%20proposed%2C%20and%20a%0Adynamic%20weighting%20strategy%20was%20employed%20to%20balance%20the%20importance%20of%20the%0Aunsupervised%20loss%20and%20supervised%20loss%20at%20different%20training%20stages.%20We%0Aconducted%20extensive%20experiments%20on%20four%20popular%20crowd%20counting%20datasets%20in%0Asemi-supervised%20settings.%20Experimental%20results%20supported%20the%20effectiveness%20of%0Aeach%20proposed%20component%20in%20our%20%24S%5E%7B4%7D%24Crowd%20framework.%20Our%20method%20achieved%0Acompetitive%20performance%20in%20semi-supervised%20learning%20approaches%20on%20these%20crowd%0Acounting%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.13969v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Crowd%20Counting%20from%20Unlabeled%20Data&entry.906535625=Haoran%20Duan%20and%20Fan%20Wan%20and%20Rui%20Sun%20and%20Zeyu%20Wang%20and%20Varun%20Ojha%20and%20Yu%20Guan%20and%20Hubert%20P.%20H.%20Shum%20and%20Bingzhang%20Hu%20and%20Yang%20Long&entry.1292438233=%20%20Automatic%20Crowd%20behavior%20analysis%20can%20be%20applied%20to%20effectively%20help%20the%0Adaily%20transportation%20statistics%20and%20planning%2C%20which%20helps%20the%20smart%20city%0Aconstruction.%20As%20one%20of%20the%20most%20important%20keys%2C%20crowd%20counting%20has%20drawn%0Aincreasing%20attention.%20Recent%20works%20achieved%20promising%20performance%20but%20relied%20on%0Athe%20supervised%20paradigm%20with%20expensive%20crowd%20annotations.%20To%20alleviate%20the%0Aannotation%20cost%20in%20real-world%20transportation%20scenarios%2C%20in%20this%20work%20we%0Aproposed%20a%20semi-supervised%20learning%20framework%20%24S%5E%7B4%7D%5Ctextit%7BCrowd%7D%24%2C%20which%20can%0Aleverage%20both%20unlabeled/labeled%20data%20for%20robust%20crowd%20counting.%20In%20the%0Aunsupervised%20pathway%2C%20two%20%5Ctextit%7Bself-supervised%20losses%7D%20were%20proposed%20to%0Asimulate%20the%20crowd%20variations%20such%20as%20scale%2C%20illumination%2C%20based%20on%20which%0Asupervised%20information%20pseudo%20labels%20were%20generated%20and%20gradually%20refined.%20We%0Aalso%20proposed%20a%20crowd-driven%20recurrent%20unit%20%5Ctextit%7BGated-Crowd-Recurrent-Unit%0A%28GCRU%29%7D%2C%20which%20can%20preserve%20discriminant%20crowd%20information%20by%20extracting%0Asecond-order%20statistics%2C%20yielding%20pseudo%20labels%20with%20improved%20quality.%20A%20joint%0Aloss%20including%20both%20unsupervised/supervised%20information%20was%20proposed%2C%20and%20a%0Adynamic%20weighting%20strategy%20was%20employed%20to%20balance%20the%20importance%20of%20the%0Aunsupervised%20loss%20and%20supervised%20loss%20at%20different%20training%20stages.%20We%0Aconducted%20extensive%20experiments%20on%20four%20popular%20crowd%20counting%20datasets%20in%0Asemi-supervised%20settings.%20Experimental%20results%20supported%20the%20effectiveness%20of%0Aeach%20proposed%20component%20in%20our%20%24S%5E%7B4%7D%24Crowd%20framework.%20Our%20method%20achieved%0Acompetitive%20performance%20in%20semi-supervised%20learning%20approaches%20on%20these%20crowd%0Acounting%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.13969v3&entry.124074799=Read"},
{"title": "Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language\n  Models for Media Forensics", "author": "Shan Jia and Reilin Lyu and Kangran Zhao and Yize Chen and Zhiyuan Yan and Yan Ju and Chuanbo Hu and Xin Li and Baoyuan Wu and Siwei Lyu", "abstract": "  DeepFakes, which refer to AI-generated media content, have become an\nincreasing concern due to their use as a means for disinformation. Detecting\nDeepFakes is currently solved with programmed machine learning algorithms. In\nthis work, we investigate the capabilities of multimodal large language models\n(LLMs) in DeepFake detection. We conducted qualitative and quantitative\nexperiments to demonstrate multimodal LLMs and show that they can expose\nAI-generated images through careful experimental design and prompt engineering.\nThis is interesting, considering that LLMs are not inherently tailored for\nmedia forensic tasks, and the process does not require programming. We discuss\nthe limitations of multimodal LLMs for these tasks and suggest possible\nimprovements.\n", "link": "http://arxiv.org/abs/2403.14077v2", "date": "2024-03-26", "relevancy": 2.0179, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5166}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5056}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Can%20ChatGPT%20Detect%20DeepFakes%3F%20A%20Study%20of%20Using%20Multimodal%20Large%20Language%0A%20%20Models%20for%20Media%20Forensics&body=Title%3A%20Can%20ChatGPT%20Detect%20DeepFakes%3F%20A%20Study%20of%20Using%20Multimodal%20Large%20Language%0A%20%20Models%20for%20Media%20Forensics%0AAuthor%3A%20Shan%20Jia%20and%20Reilin%20Lyu%20and%20Kangran%20Zhao%20and%20Yize%20Chen%20and%20Zhiyuan%20Yan%20and%20Yan%20Ju%20and%20Chuanbo%20Hu%20and%20Xin%20Li%20and%20Baoyuan%20Wu%20and%20Siwei%20Lyu%0AAbstract%3A%20%20%20DeepFakes%2C%20which%20refer%20to%20AI-generated%20media%20content%2C%20have%20become%20an%0Aincreasing%20concern%20due%20to%20their%20use%20as%20a%20means%20for%20disinformation.%20Detecting%0ADeepFakes%20is%20currently%20solved%20with%20programmed%20machine%20learning%20algorithms.%20In%0Athis%20work%2C%20we%20investigate%20the%20capabilities%20of%20multimodal%20large%20language%20models%0A%28LLMs%29%20in%20DeepFake%20detection.%20We%20conducted%20qualitative%20and%20quantitative%0Aexperiments%20to%20demonstrate%20multimodal%20LLMs%20and%20show%20that%20they%20can%20expose%0AAI-generated%20images%20through%20careful%20experimental%20design%20and%20prompt%20engineering.%0AThis%20is%20interesting%2C%20considering%20that%20LLMs%20are%20not%20inherently%20tailored%20for%0Amedia%20forensic%20tasks%2C%20and%20the%20process%20does%20not%20require%20programming.%20We%20discuss%0Athe%20limitations%20of%20multimodal%20LLMs%20for%20these%20tasks%20and%20suggest%20possible%0Aimprovements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14077v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20ChatGPT%20Detect%20DeepFakes%3F%20A%20Study%20of%20Using%20Multimodal%20Large%20Language%0A%20%20Models%20for%20Media%20Forensics&entry.906535625=Shan%20Jia%20and%20Reilin%20Lyu%20and%20Kangran%20Zhao%20and%20Yize%20Chen%20and%20Zhiyuan%20Yan%20and%20Yan%20Ju%20and%20Chuanbo%20Hu%20and%20Xin%20Li%20and%20Baoyuan%20Wu%20and%20Siwei%20Lyu&entry.1292438233=%20%20DeepFakes%2C%20which%20refer%20to%20AI-generated%20media%20content%2C%20have%20become%20an%0Aincreasing%20concern%20due%20to%20their%20use%20as%20a%20means%20for%20disinformation.%20Detecting%0ADeepFakes%20is%20currently%20solved%20with%20programmed%20machine%20learning%20algorithms.%20In%0Athis%20work%2C%20we%20investigate%20the%20capabilities%20of%20multimodal%20large%20language%20models%0A%28LLMs%29%20in%20DeepFake%20detection.%20We%20conducted%20qualitative%20and%20quantitative%0Aexperiments%20to%20demonstrate%20multimodal%20LLMs%20and%20show%20that%20they%20can%20expose%0AAI-generated%20images%20through%20careful%20experimental%20design%20and%20prompt%20engineering.%0AThis%20is%20interesting%2C%20considering%20that%20LLMs%20are%20not%20inherently%20tailored%20for%0Amedia%20forensic%20tasks%2C%20and%20the%20process%20does%20not%20require%20programming.%20We%20discuss%0Athe%20limitations%20of%20multimodal%20LLMs%20for%20these%20tasks%20and%20suggest%20possible%0Aimprovements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14077v2&entry.124074799=Read"},
{"title": "Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial\n  Vehicles", "author": "Drew Scott and Satyanarayana G. Manyam and David W. Casbeer and Manish Kumar and Isaac E. Weintraub", "abstract": "  Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple\nagents from respective start to goal locations such that no paths conflict. We\naddress the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles\nwhich are subject to location-dependent noise restrictions. We solve this\nproblem by searching a constraint tree for which the subproblem at each node is\na set of shortest path problems subject to the noise and fuel constraints and\nconflict zone avoidance. A labeling algorithm is presented to solve this\nsubproblem, including the conflict zones which are treated as dynamic\nobstacles. We present the experimental results of the algorithms for various\ngraph sizes and number of agents.\n", "link": "http://arxiv.org/abs/2403.17849v1", "date": "2024-03-26", "relevancy": 2.0089, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5144}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4662}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi%20Agent%20Pathfinding%20for%20Noise%20Restricted%20Hybrid%20Fuel%20Unmanned%20Aerial%0A%20%20Vehicles&body=Title%3A%20Multi%20Agent%20Pathfinding%20for%20Noise%20Restricted%20Hybrid%20Fuel%20Unmanned%20Aerial%0A%20%20Vehicles%0AAuthor%3A%20Drew%20Scott%20and%20Satyanarayana%20G.%20Manyam%20and%20David%20W.%20Casbeer%20and%20Manish%20Kumar%20and%20Isaac%20E.%20Weintraub%0AAbstract%3A%20%20%20Multi%20Agent%20Path%20Finding%20%28MAPF%29%20seeks%20the%20optimal%20set%20of%20paths%20for%20multiple%0Aagents%20from%20respective%20start%20to%20goal%20locations%20such%20that%20no%20paths%20conflict.%20We%0Aaddress%20the%20MAPF%20problem%20for%20a%20fleet%20of%20hybrid-fuel%20unmanned%20aerial%20vehicles%0Awhich%20are%20subject%20to%20location-dependent%20noise%20restrictions.%20We%20solve%20this%0Aproblem%20by%20searching%20a%20constraint%20tree%20for%20which%20the%20subproblem%20at%20each%20node%20is%0Aa%20set%20of%20shortest%20path%20problems%20subject%20to%20the%20noise%20and%20fuel%20constraints%20and%0Aconflict%20zone%20avoidance.%20A%20labeling%20algorithm%20is%20presented%20to%20solve%20this%0Asubproblem%2C%20including%20the%20conflict%20zones%20which%20are%20treated%20as%20dynamic%0Aobstacles.%20We%20present%20the%20experimental%20results%20of%20the%20algorithms%20for%20various%0Agraph%20sizes%20and%20number%20of%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17849v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi%20Agent%20Pathfinding%20for%20Noise%20Restricted%20Hybrid%20Fuel%20Unmanned%20Aerial%0A%20%20Vehicles&entry.906535625=Drew%20Scott%20and%20Satyanarayana%20G.%20Manyam%20and%20David%20W.%20Casbeer%20and%20Manish%20Kumar%20and%20Isaac%20E.%20Weintraub&entry.1292438233=%20%20Multi%20Agent%20Path%20Finding%20%28MAPF%29%20seeks%20the%20optimal%20set%20of%20paths%20for%20multiple%0Aagents%20from%20respective%20start%20to%20goal%20locations%20such%20that%20no%20paths%20conflict.%20We%0Aaddress%20the%20MAPF%20problem%20for%20a%20fleet%20of%20hybrid-fuel%20unmanned%20aerial%20vehicles%0Awhich%20are%20subject%20to%20location-dependent%20noise%20restrictions.%20We%20solve%20this%0Aproblem%20by%20searching%20a%20constraint%20tree%20for%20which%20the%20subproblem%20at%20each%20node%20is%0Aa%20set%20of%20shortest%20path%20problems%20subject%20to%20the%20noise%20and%20fuel%20constraints%20and%0Aconflict%20zone%20avoidance.%20A%20labeling%20algorithm%20is%20presented%20to%20solve%20this%0Asubproblem%2C%20including%20the%20conflict%20zones%20which%20are%20treated%20as%20dynamic%0Aobstacles.%20We%20present%20the%20experimental%20results%20of%20the%20algorithms%20for%20various%0Agraph%20sizes%20and%20number%20of%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17849v1&entry.124074799=Read"},
{"title": "Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes", "author": "Devansh R. Agrawal and Dimitra Panagou", "abstract": "  This paper presents two algorithms for multi-agent dynamic coverage in\nspatiotemporal environments, where the coverage algorithms are informed by the\nmethod of data assimilation. In particular, we show that by considering the\ninformation assimilation algorithm, here a Numerical Gaussian Process Kalman\nFilter, the influence of measurements taken at one position on the uncertainty\nof the estimate at another location can be computed. We use this relationship\nto propose new coverage algorithms. Furthermore, we show that the controllers\nnaturally extend to the multi-agent context, allowing for a distributed-control\ncentral-information paradigm for multi-agent coverage. Finally, we demonstrate\nthe algorithms through a realistic simulation of a team of UAVs collecting wind\ndata over a region in Austria.\n", "link": "http://arxiv.org/abs/2403.17917v1", "date": "2024-03-26", "relevancy": 1.998, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5352}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5302}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4545}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Clarity-Aware%20Dynamic%20Coverage%20with%20Gaussian%20Processes&body=Title%3A%20Multi-Agent%20Clarity-Aware%20Dynamic%20Coverage%20with%20Gaussian%20Processes%0AAuthor%3A%20Devansh%20R.%20Agrawal%20and%20Dimitra%20Panagou%0AAbstract%3A%20%20%20This%20paper%20presents%20two%20algorithms%20for%20multi-agent%20dynamic%20coverage%20in%0Aspatiotemporal%20environments%2C%20where%20the%20coverage%20algorithms%20are%20informed%20by%20the%0Amethod%20of%20data%20assimilation.%20In%20particular%2C%20we%20show%20that%20by%20considering%20the%0Ainformation%20assimilation%20algorithm%2C%20here%20a%20Numerical%20Gaussian%20Process%20Kalman%0AFilter%2C%20the%20influence%20of%20measurements%20taken%20at%20one%20position%20on%20the%20uncertainty%0Aof%20the%20estimate%20at%20another%20location%20can%20be%20computed.%20We%20use%20this%20relationship%0Ato%20propose%20new%20coverage%20algorithms.%20Furthermore%2C%20we%20show%20that%20the%20controllers%0Anaturally%20extend%20to%20the%20multi-agent%20context%2C%20allowing%20for%20a%20distributed-control%0Acentral-information%20paradigm%20for%20multi-agent%20coverage.%20Finally%2C%20we%20demonstrate%0Athe%20algorithms%20through%20a%20realistic%20simulation%20of%20a%20team%20of%20UAVs%20collecting%20wind%0Adata%20over%20a%20region%20in%20Austria.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17917v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Clarity-Aware%20Dynamic%20Coverage%20with%20Gaussian%20Processes&entry.906535625=Devansh%20R.%20Agrawal%20and%20Dimitra%20Panagou&entry.1292438233=%20%20This%20paper%20presents%20two%20algorithms%20for%20multi-agent%20dynamic%20coverage%20in%0Aspatiotemporal%20environments%2C%20where%20the%20coverage%20algorithms%20are%20informed%20by%20the%0Amethod%20of%20data%20assimilation.%20In%20particular%2C%20we%20show%20that%20by%20considering%20the%0Ainformation%20assimilation%20algorithm%2C%20here%20a%20Numerical%20Gaussian%20Process%20Kalman%0AFilter%2C%20the%20influence%20of%20measurements%20taken%20at%20one%20position%20on%20the%20uncertainty%0Aof%20the%20estimate%20at%20another%20location%20can%20be%20computed.%20We%20use%20this%20relationship%0Ato%20propose%20new%20coverage%20algorithms.%20Furthermore%2C%20we%20show%20that%20the%20controllers%0Anaturally%20extend%20to%20the%20multi-agent%20context%2C%20allowing%20for%20a%20distributed-control%0Acentral-information%20paradigm%20for%20multi-agent%20coverage.%20Finally%2C%20we%20demonstrate%0Athe%20algorithms%20through%20a%20realistic%20simulation%20of%20a%20team%20of%20UAVs%20collecting%20wind%0Adata%20over%20a%20region%20in%20Austria.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17917v1&entry.124074799=Read"},
{"title": "Blinded by Generated Contexts: How Language Models Merge Generated and\n  Retrieved Contexts for Open-Domain QA?", "author": "Hexiang Tan and Fei Sun and Wanli Yang and Yuanzhuo Wang and Qi Cao and Xueqi Cheng", "abstract": "  While auxiliary information has become a key to enhancing Large Language\nModels (LLMs), relatively little is known about how LLMs merge these contexts,\nspecifically contexts generated by LLMs and those retrieved from external\nsources. To investigate this, we formulate a systematic framework to identify\nwhether LLMs' responses, derived from the integration of generated and\nretrieved contexts, are attributed to either generated or retrieved contexts.\nTo easily trace the origin of the response, we construct datasets with\nconflicting contexts, i.e., each question is paired with both generated and\nretrieved contexts, yet only one of them contains the correct answer. Our\nexperiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to\nfavor generated contexts, even when they provide incorrect information. We\nfurther identify two key factors contributing to this bias: i) contexts\ngenerated by LLMs typically show greater similarity to the questions,\nincreasing their likelihood of being selected; ii) the segmentation process\nused in retrieved contexts disrupts their completeness, thereby hindering their\nfull utilization in LLMs. Our analysis enhances the understanding of how LLMs\nmerge diverse contexts, offering valuable insights for advancing current\naugmentation methods for LLMs.\n", "link": "http://arxiv.org/abs/2401.11911v4", "date": "2024-03-26", "relevancy": 1.995, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5179}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4925}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Blinded%20by%20Generated%20Contexts%3A%20How%20Language%20Models%20Merge%20Generated%20and%0A%20%20Retrieved%20Contexts%20for%20Open-Domain%20QA%3F&body=Title%3A%20Blinded%20by%20Generated%20Contexts%3A%20How%20Language%20Models%20Merge%20Generated%20and%0A%20%20Retrieved%20Contexts%20for%20Open-Domain%20QA%3F%0AAuthor%3A%20Hexiang%20Tan%20and%20Fei%20Sun%20and%20Wanli%20Yang%20and%20Yuanzhuo%20Wang%20and%20Qi%20Cao%20and%20Xueqi%20Cheng%0AAbstract%3A%20%20%20While%20auxiliary%20information%20has%20become%20a%20key%20to%20enhancing%20Large%20Language%0AModels%20%28LLMs%29%2C%20relatively%20little%20is%20known%20about%20how%20LLMs%20merge%20these%20contexts%2C%0Aspecifically%20contexts%20generated%20by%20LLMs%20and%20those%20retrieved%20from%20external%0Asources.%20To%20investigate%20this%2C%20we%20formulate%20a%20systematic%20framework%20to%20identify%0Awhether%20LLMs%27%20responses%2C%20derived%20from%20the%20integration%20of%20generated%20and%0Aretrieved%20contexts%2C%20are%20attributed%20to%20either%20generated%20or%20retrieved%20contexts.%0ATo%20easily%20trace%20the%20origin%20of%20the%20response%2C%20we%20construct%20datasets%20with%0Aconflicting%20contexts%2C%20i.e.%2C%20each%20question%20is%20paired%20with%20both%20generated%20and%0Aretrieved%20contexts%2C%20yet%20only%20one%20of%20them%20contains%20the%20correct%20answer.%20Our%0Aexperiments%20reveal%20a%20significant%20bias%20in%20several%20LLMs%20%28GPT-4/3.5%20and%20Llama2%29%20to%0Afavor%20generated%20contexts%2C%20even%20when%20they%20provide%20incorrect%20information.%20We%0Afurther%20identify%20two%20key%20factors%20contributing%20to%20this%20bias%3A%20i%29%20contexts%0Agenerated%20by%20LLMs%20typically%20show%20greater%20similarity%20to%20the%20questions%2C%0Aincreasing%20their%20likelihood%20of%20being%20selected%3B%20ii%29%20the%20segmentation%20process%0Aused%20in%20retrieved%20contexts%20disrupts%20their%20completeness%2C%20thereby%20hindering%20their%0Afull%20utilization%20in%20LLMs.%20Our%20analysis%20enhances%20the%20understanding%20of%20how%20LLMs%0Amerge%20diverse%20contexts%2C%20offering%20valuable%20insights%20for%20advancing%20current%0Aaugmentation%20methods%20for%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11911v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blinded%20by%20Generated%20Contexts%3A%20How%20Language%20Models%20Merge%20Generated%20and%0A%20%20Retrieved%20Contexts%20for%20Open-Domain%20QA%3F&entry.906535625=Hexiang%20Tan%20and%20Fei%20Sun%20and%20Wanli%20Yang%20and%20Yuanzhuo%20Wang%20and%20Qi%20Cao%20and%20Xueqi%20Cheng&entry.1292438233=%20%20While%20auxiliary%20information%20has%20become%20a%20key%20to%20enhancing%20Large%20Language%0AModels%20%28LLMs%29%2C%20relatively%20little%20is%20known%20about%20how%20LLMs%20merge%20these%20contexts%2C%0Aspecifically%20contexts%20generated%20by%20LLMs%20and%20those%20retrieved%20from%20external%0Asources.%20To%20investigate%20this%2C%20we%20formulate%20a%20systematic%20framework%20to%20identify%0Awhether%20LLMs%27%20responses%2C%20derived%20from%20the%20integration%20of%20generated%20and%0Aretrieved%20contexts%2C%20are%20attributed%20to%20either%20generated%20or%20retrieved%20contexts.%0ATo%20easily%20trace%20the%20origin%20of%20the%20response%2C%20we%20construct%20datasets%20with%0Aconflicting%20contexts%2C%20i.e.%2C%20each%20question%20is%20paired%20with%20both%20generated%20and%0Aretrieved%20contexts%2C%20yet%20only%20one%20of%20them%20contains%20the%20correct%20answer.%20Our%0Aexperiments%20reveal%20a%20significant%20bias%20in%20several%20LLMs%20%28GPT-4/3.5%20and%20Llama2%29%20to%0Afavor%20generated%20contexts%2C%20even%20when%20they%20provide%20incorrect%20information.%20We%0Afurther%20identify%20two%20key%20factors%20contributing%20to%20this%20bias%3A%20i%29%20contexts%0Agenerated%20by%20LLMs%20typically%20show%20greater%20similarity%20to%20the%20questions%2C%0Aincreasing%20their%20likelihood%20of%20being%20selected%3B%20ii%29%20the%20segmentation%20process%0Aused%20in%20retrieved%20contexts%20disrupts%20their%20completeness%2C%20thereby%20hindering%20their%0Afull%20utilization%20in%20LLMs.%20Our%20analysis%20enhances%20the%20understanding%20of%20how%20LLMs%0Amerge%20diverse%20contexts%2C%20offering%20valuable%20insights%20for%20advancing%20current%0Aaugmentation%20methods%20for%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11911v4&entry.124074799=Read"},
{"title": "A Survey on 3D Egocentric Human Pose Estimation", "author": "Md Mushfiqur Azam and Kevin Desai", "abstract": "  Egocentric human pose estimation aims to estimate human body poses and\ndevelop body representations from a first-person camera perspective. It has\ngained vast popularity in recent years because of its wide range of\napplications in sectors like XR-technologies, human-computer interaction, and\nfitness tracking. However, to the best of our knowledge, there is no systematic\nliterature review based on the proposed solutions regarding egocentric 3D human\npose estimation. To that end, the aim of this survey paper is to provide an\nextensive overview of the current state of egocentric pose estimation research.\nIn this paper, we categorize and discuss the popular datasets and the different\npose estimation models, highlighting the strengths and weaknesses of different\nmethods by comparative analysis. This survey can be a valuable resource for\nboth researchers and practitioners in the field, offering insights into key\nconcepts and cutting-edge solutions in egocentric pose estimation, its\nwide-ranging applications, as well as the open problems with future scope.\n", "link": "http://arxiv.org/abs/2403.17893v1", "date": "2024-03-26", "relevancy": 1.9887, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5176}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4996}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4758}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%203D%20Egocentric%20Human%20Pose%20Estimation&body=Title%3A%20A%20Survey%20on%203D%20Egocentric%20Human%20Pose%20Estimation%0AAuthor%3A%20Md%20Mushfiqur%20Azam%20and%20Kevin%20Desai%0AAbstract%3A%20%20%20Egocentric%20human%20pose%20estimation%20aims%20to%20estimate%20human%20body%20poses%20and%0Adevelop%20body%20representations%20from%20a%20first-person%20camera%20perspective.%20It%20has%0Agained%20vast%20popularity%20in%20recent%20years%20because%20of%20its%20wide%20range%20of%0Aapplications%20in%20sectors%20like%20XR-technologies%2C%20human-computer%20interaction%2C%20and%0Afitness%20tracking.%20However%2C%20to%20the%20best%20of%20our%20knowledge%2C%20there%20is%20no%20systematic%0Aliterature%20review%20based%20on%20the%20proposed%20solutions%20regarding%20egocentric%203D%20human%0Apose%20estimation.%20To%20that%20end%2C%20the%20aim%20of%20this%20survey%20paper%20is%20to%20provide%20an%0Aextensive%20overview%20of%20the%20current%20state%20of%20egocentric%20pose%20estimation%20research.%0AIn%20this%20paper%2C%20we%20categorize%20and%20discuss%20the%20popular%20datasets%20and%20the%20different%0Apose%20estimation%20models%2C%20highlighting%20the%20strengths%20and%20weaknesses%20of%20different%0Amethods%20by%20comparative%20analysis.%20This%20survey%20can%20be%20a%20valuable%20resource%20for%0Aboth%20researchers%20and%20practitioners%20in%20the%20field%2C%20offering%20insights%20into%20key%0Aconcepts%20and%20cutting-edge%20solutions%20in%20egocentric%20pose%20estimation%2C%20its%0Awide-ranging%20applications%2C%20as%20well%20as%20the%20open%20problems%20with%20future%20scope.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17893v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%203D%20Egocentric%20Human%20Pose%20Estimation&entry.906535625=Md%20Mushfiqur%20Azam%20and%20Kevin%20Desai&entry.1292438233=%20%20Egocentric%20human%20pose%20estimation%20aims%20to%20estimate%20human%20body%20poses%20and%0Adevelop%20body%20representations%20from%20a%20first-person%20camera%20perspective.%20It%20has%0Agained%20vast%20popularity%20in%20recent%20years%20because%20of%20its%20wide%20range%20of%0Aapplications%20in%20sectors%20like%20XR-technologies%2C%20human-computer%20interaction%2C%20and%0Afitness%20tracking.%20However%2C%20to%20the%20best%20of%20our%20knowledge%2C%20there%20is%20no%20systematic%0Aliterature%20review%20based%20on%20the%20proposed%20solutions%20regarding%20egocentric%203D%20human%0Apose%20estimation.%20To%20that%20end%2C%20the%20aim%20of%20this%20survey%20paper%20is%20to%20provide%20an%0Aextensive%20overview%20of%20the%20current%20state%20of%20egocentric%20pose%20estimation%20research.%0AIn%20this%20paper%2C%20we%20categorize%20and%20discuss%20the%20popular%20datasets%20and%20the%20different%0Apose%20estimation%20models%2C%20highlighting%20the%20strengths%20and%20weaknesses%20of%20different%0Amethods%20by%20comparative%20analysis.%20This%20survey%20can%20be%20a%20valuable%20resource%20for%0Aboth%20researchers%20and%20practitioners%20in%20the%20field%2C%20offering%20insights%20into%20key%0Aconcepts%20and%20cutting-edge%20solutions%20in%20egocentric%20pose%20estimation%2C%20its%0Awide-ranging%20applications%2C%20as%20well%20as%20the%20open%20problems%20with%20future%20scope.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17893v1&entry.124074799=Read"},
{"title": "FastCAR: Fast Classification And Regression Multi-Task Learning via Task\n  Consolidation for Modelling a Continuous Property Variable of Object Classes", "author": "Anoop Kini and Andreas Jansche and Timo Bernthaler and Gerhard Schneider", "abstract": "  FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)\nfor a classification and a regression task, despite task heterogeneity with\nonly subtle correlation. It addresses object classification and continuous\nproperty variable regression, a crucial use case in science and engineering.\nFastCAR involves a labeling transformation approach that can be used with a\nsingle-task regression network architecture. FastCAR outperforms traditional\nMTL model families, parametrized in the landscape of architecture and loss\nweighting schemes, when learning of both tasks are collectively considered\n(classification accuracy of 99.54%, regression mean absolute percentage error\nof 2.3%). The experiments performed used an Advanced Steel Property dataset\ncontributed by us. The dataset comprises 4536 images of 224x224 pixels,\nannotated with object classes and hardness properties that take continuous\nvalues. With the labeling transformation and single-task regression network\narchitecture, FastCAR achieves reduced latency and time efficiency.\n", "link": "http://arxiv.org/abs/2403.17926v1", "date": "2024-03-26", "relevancy": 1.971, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.522}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4501}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FastCAR%3A%20Fast%20Classification%20And%20Regression%20Multi-Task%20Learning%20via%20Task%0A%20%20Consolidation%20for%20Modelling%20a%20Continuous%20Property%20Variable%20of%20Object%20Classes&body=Title%3A%20FastCAR%3A%20Fast%20Classification%20And%20Regression%20Multi-Task%20Learning%20via%20Task%0A%20%20Consolidation%20for%20Modelling%20a%20Continuous%20Property%20Variable%20of%20Object%20Classes%0AAuthor%3A%20Anoop%20Kini%20and%20Andreas%20Jansche%20and%20Timo%20Bernthaler%20and%20Gerhard%20Schneider%0AAbstract%3A%20%20%20FastCAR%20is%20a%20novel%20task%20consolidation%20approach%20in%20Multi-Task%20Learning%20%28MTL%29%0Afor%20a%20classification%20and%20a%20regression%20task%2C%20despite%20task%20heterogeneity%20with%0Aonly%20subtle%20correlation.%20It%20addresses%20object%20classification%20and%20continuous%0Aproperty%20variable%20regression%2C%20a%20crucial%20use%20case%20in%20science%20and%20engineering.%0AFastCAR%20involves%20a%20labeling%20transformation%20approach%20that%20can%20be%20used%20with%20a%0Asingle-task%20regression%20network%20architecture.%20FastCAR%20outperforms%20traditional%0AMTL%20model%20families%2C%20parametrized%20in%20the%20landscape%20of%20architecture%20and%20loss%0Aweighting%20schemes%2C%20when%20learning%20of%20both%20tasks%20are%20collectively%20considered%0A%28classification%20accuracy%20of%2099.54%25%2C%20regression%20mean%20absolute%20percentage%20error%0Aof%202.3%25%29.%20The%20experiments%20performed%20used%20an%20Advanced%20Steel%20Property%20dataset%0Acontributed%20by%20us.%20The%20dataset%20comprises%204536%20images%20of%20224x224%20pixels%2C%0Aannotated%20with%20object%20classes%20and%20hardness%20properties%20that%20take%20continuous%0Avalues.%20With%20the%20labeling%20transformation%20and%20single-task%20regression%20network%0Aarchitecture%2C%20FastCAR%20achieves%20reduced%20latency%20and%20time%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17926v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastCAR%3A%20Fast%20Classification%20And%20Regression%20Multi-Task%20Learning%20via%20Task%0A%20%20Consolidation%20for%20Modelling%20a%20Continuous%20Property%20Variable%20of%20Object%20Classes&entry.906535625=Anoop%20Kini%20and%20Andreas%20Jansche%20and%20Timo%20Bernthaler%20and%20Gerhard%20Schneider&entry.1292438233=%20%20FastCAR%20is%20a%20novel%20task%20consolidation%20approach%20in%20Multi-Task%20Learning%20%28MTL%29%0Afor%20a%20classification%20and%20a%20regression%20task%2C%20despite%20task%20heterogeneity%20with%0Aonly%20subtle%20correlation.%20It%20addresses%20object%20classification%20and%20continuous%0Aproperty%20variable%20regression%2C%20a%20crucial%20use%20case%20in%20science%20and%20engineering.%0AFastCAR%20involves%20a%20labeling%20transformation%20approach%20that%20can%20be%20used%20with%20a%0Asingle-task%20regression%20network%20architecture.%20FastCAR%20outperforms%20traditional%0AMTL%20model%20families%2C%20parametrized%20in%20the%20landscape%20of%20architecture%20and%20loss%0Aweighting%20schemes%2C%20when%20learning%20of%20both%20tasks%20are%20collectively%20considered%0A%28classification%20accuracy%20of%2099.54%25%2C%20regression%20mean%20absolute%20percentage%20error%0Aof%202.3%25%29.%20The%20experiments%20performed%20used%20an%20Advanced%20Steel%20Property%20dataset%0Acontributed%20by%20us.%20The%20dataset%20comprises%204536%20images%20of%20224x224%20pixels%2C%0Aannotated%20with%20object%20classes%20and%20hardness%20properties%20that%20take%20continuous%0Avalues.%20With%20the%20labeling%20transformation%20and%20single-task%20regression%20network%0Aarchitecture%2C%20FastCAR%20achieves%20reduced%20latency%20and%20time%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17926v1&entry.124074799=Read"},
{"title": "AI and Generative AI for Research Discovery and Summarization", "author": "Mark Glickman and Yi Zhang", "abstract": "  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n", "link": "http://arxiv.org/abs/2401.06795v2", "date": "2024-03-26", "relevancy": 1.9659, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5379}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4782}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4503}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AI%20and%20Generative%20AI%20for%20Research%20Discovery%20and%20Summarization&body=Title%3A%20AI%20and%20Generative%20AI%20for%20Research%20Discovery%20and%20Summarization%0AAuthor%3A%20Mark%20Glickman%20and%20Yi%20Zhang%0AAbstract%3A%20%20%20AI%20and%20generative%20AI%20tools%2C%20including%20chatbots%20like%20ChatGPT%20that%20rely%20on%0Alarge%20language%20models%20%28LLMs%29%2C%20have%20burst%20onto%20the%20scene%20this%20year%2C%20creating%0Aincredible%20opportunities%20to%20increase%20work%20productivity%20and%20improve%20our%20lives.%0AStatisticians%20and%20data%20scientists%20have%20begun%20experiencing%20the%20benefits%20from%20the%0Aavailability%20of%20these%20tools%20in%20numerous%20ways%2C%20such%20as%20the%20generation%20of%0Aprogramming%20code%20from%20text%20prompts%20to%20analyze%20data%20or%20fit%20statistical%20models.%0AOne%20area%20that%20these%20tools%20can%20make%20a%20substantial%20impact%20is%20in%20research%0Adiscovery%20and%20summarization.%20Standalone%20tools%20and%20plugins%20to%20chatbots%20are%20being%0Adeveloped%20that%20allow%20researchers%20to%20more%20quickly%20find%20relevant%20literature%20than%0Apre-2023%20search%20tools.%20Furthermore%2C%20generative%20AI%20tools%20have%20improved%20to%20the%0Apoint%20where%20they%20can%20summarize%20and%20extract%20the%20key%20points%20from%20research%0Aarticles%20in%20succinct%20language.%20Finally%2C%20chatbots%20based%20on%20highly%20parameterized%0ALLMs%20can%20be%20used%20to%20simulate%20abductive%20reasoning%2C%20which%20provides%20researchers%0Athe%20ability%20to%20make%20connections%20among%20related%20technical%20topics%2C%20which%20can%20also%0Abe%20used%20for%20research%20discovery.%20We%20review%20the%20developments%20in%20AI%20and%20generative%0AAI%20for%20research%20discovery%20and%20summarization%2C%20and%20propose%20directions%20where%20these%0Atypes%20of%20tools%20are%20likely%20to%20head%20in%20the%20future%20that%20may%20be%20of%20interest%20to%0Astatistician%20and%20data%20scientists.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06795v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20and%20Generative%20AI%20for%20Research%20Discovery%20and%20Summarization&entry.906535625=Mark%20Glickman%20and%20Yi%20Zhang&entry.1292438233=%20%20AI%20and%20generative%20AI%20tools%2C%20including%20chatbots%20like%20ChatGPT%20that%20rely%20on%0Alarge%20language%20models%20%28LLMs%29%2C%20have%20burst%20onto%20the%20scene%20this%20year%2C%20creating%0Aincredible%20opportunities%20to%20increase%20work%20productivity%20and%20improve%20our%20lives.%0AStatisticians%20and%20data%20scientists%20have%20begun%20experiencing%20the%20benefits%20from%20the%0Aavailability%20of%20these%20tools%20in%20numerous%20ways%2C%20such%20as%20the%20generation%20of%0Aprogramming%20code%20from%20text%20prompts%20to%20analyze%20data%20or%20fit%20statistical%20models.%0AOne%20area%20that%20these%20tools%20can%20make%20a%20substantial%20impact%20is%20in%20research%0Adiscovery%20and%20summarization.%20Standalone%20tools%20and%20plugins%20to%20chatbots%20are%20being%0Adeveloped%20that%20allow%20researchers%20to%20more%20quickly%20find%20relevant%20literature%20than%0Apre-2023%20search%20tools.%20Furthermore%2C%20generative%20AI%20tools%20have%20improved%20to%20the%0Apoint%20where%20they%20can%20summarize%20and%20extract%20the%20key%20points%20from%20research%0Aarticles%20in%20succinct%20language.%20Finally%2C%20chatbots%20based%20on%20highly%20parameterized%0ALLMs%20can%20be%20used%20to%20simulate%20abductive%20reasoning%2C%20which%20provides%20researchers%0Athe%20ability%20to%20make%20connections%20among%20related%20technical%20topics%2C%20which%20can%20also%0Abe%20used%20for%20research%20discovery.%20We%20review%20the%20developments%20in%20AI%20and%20generative%0AAI%20for%20research%20discovery%20and%20summarization%2C%20and%20propose%20directions%20where%20these%0Atypes%20of%20tools%20are%20likely%20to%20head%20in%20the%20future%20that%20may%20be%20of%20interest%20to%0Astatistician%20and%20data%20scientists.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06795v2&entry.124074799=Read"},
{"title": "Fully Independent Communication in Multi-Agent Reinforcement Learning", "author": "Rafael Pina and Varuna De Silva and Corentin Artaud and Xiaolan Liu", "abstract": "  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research\nwithin the field of multi-agent systems. Several recent works have focused\nspecifically on the study of communication approaches in MARL. While multiple\ncommunication methods have been proposed, these might still be too complex and\nnot easily transferable to more practical contexts. One of the reasons for that\nis due to the use of the famous parameter sharing trick. In this paper, we\ninvestigate how independent learners in MARL that do not share parameters can\ncommunicate. We demonstrate that this setting might incur into some problems,\nto which we propose a new learning scheme as a solution. Our results show that,\ndespite the challenges, independent agents can still learn communication\nstrategies following our method. Additionally, we use this method to\ninvestigate how communication in MARL is affected by different network\ncapacities, both for sharing and not sharing parameters. We observe that\ncommunication may not always be needed and that the chosen agent network sizes\nneed to be considered when used together with communication in order to achieve\nefficient learning.\n", "link": "http://arxiv.org/abs/2401.15059v2", "date": "2024-03-26", "relevancy": 1.9638, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5343}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4982}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fully%20Independent%20Communication%20in%20Multi-Agent%20Reinforcement%20Learning&body=Title%3A%20Fully%20Independent%20Communication%20in%20Multi-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Rafael%20Pina%20and%20Varuna%20De%20Silva%20and%20Corentin%20Artaud%20and%20Xiaolan%20Liu%0AAbstract%3A%20%20%20Multi-Agent%20Reinforcement%20Learning%20%28MARL%29%20comprises%20a%20broad%20area%20of%20research%0Awithin%20the%20field%20of%20multi-agent%20systems.%20Several%20recent%20works%20have%20focused%0Aspecifically%20on%20the%20study%20of%20communication%20approaches%20in%20MARL.%20While%20multiple%0Acommunication%20methods%20have%20been%20proposed%2C%20these%20might%20still%20be%20too%20complex%20and%0Anot%20easily%20transferable%20to%20more%20practical%20contexts.%20One%20of%20the%20reasons%20for%20that%0Ais%20due%20to%20the%20use%20of%20the%20famous%20parameter%20sharing%20trick.%20In%20this%20paper%2C%20we%0Ainvestigate%20how%20independent%20learners%20in%20MARL%20that%20do%20not%20share%20parameters%20can%0Acommunicate.%20We%20demonstrate%20that%20this%20setting%20might%20incur%20into%20some%20problems%2C%0Ato%20which%20we%20propose%20a%20new%20learning%20scheme%20as%20a%20solution.%20Our%20results%20show%20that%2C%0Adespite%20the%20challenges%2C%20independent%20agents%20can%20still%20learn%20communication%0Astrategies%20following%20our%20method.%20Additionally%2C%20we%20use%20this%20method%20to%0Ainvestigate%20how%20communication%20in%20MARL%20is%20affected%20by%20different%20network%0Acapacities%2C%20both%20for%20sharing%20and%20not%20sharing%20parameters.%20We%20observe%20that%0Acommunication%20may%20not%20always%20be%20needed%20and%20that%20the%20chosen%20agent%20network%20sizes%0Aneed%20to%20be%20considered%20when%20used%20together%20with%20communication%20in%20order%20to%20achieve%0Aefficient%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15059v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Independent%20Communication%20in%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Rafael%20Pina%20and%20Varuna%20De%20Silva%20and%20Corentin%20Artaud%20and%20Xiaolan%20Liu&entry.1292438233=%20%20Multi-Agent%20Reinforcement%20Learning%20%28MARL%29%20comprises%20a%20broad%20area%20of%20research%0Awithin%20the%20field%20of%20multi-agent%20systems.%20Several%20recent%20works%20have%20focused%0Aspecifically%20on%20the%20study%20of%20communication%20approaches%20in%20MARL.%20While%20multiple%0Acommunication%20methods%20have%20been%20proposed%2C%20these%20might%20still%20be%20too%20complex%20and%0Anot%20easily%20transferable%20to%20more%20practical%20contexts.%20One%20of%20the%20reasons%20for%20that%0Ais%20due%20to%20the%20use%20of%20the%20famous%20parameter%20sharing%20trick.%20In%20this%20paper%2C%20we%0Ainvestigate%20how%20independent%20learners%20in%20MARL%20that%20do%20not%20share%20parameters%20can%0Acommunicate.%20We%20demonstrate%20that%20this%20setting%20might%20incur%20into%20some%20problems%2C%0Ato%20which%20we%20propose%20a%20new%20learning%20scheme%20as%20a%20solution.%20Our%20results%20show%20that%2C%0Adespite%20the%20challenges%2C%20independent%20agents%20can%20still%20learn%20communication%0Astrategies%20following%20our%20method.%20Additionally%2C%20we%20use%20this%20method%20to%0Ainvestigate%20how%20communication%20in%20MARL%20is%20affected%20by%20different%20network%0Acapacities%2C%20both%20for%20sharing%20and%20not%20sharing%20parameters.%20We%20observe%20that%0Acommunication%20may%20not%20always%20be%20needed%20and%20that%20the%20chosen%20agent%20network%20sizes%0Aneed%20to%20be%20considered%20when%20used%20together%20with%20communication%20in%20order%20to%20achieve%0Aefficient%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15059v2&entry.124074799=Read"},
{"title": "Compressed Multi-task embeddings for Data-Efficient Downstream training\n  and inference in Earth Observation", "author": "Carlos Gomes and Thomas Brunschwiler", "abstract": "  As repositories of large scale data in earth observation (EO) have grown, so\nhave transfer and storage costs for model training and inference, expending\nsignificant resources. We introduce Neural Embedding Compression (NEC), based\non the transfer of compressed embeddings to data consumers instead of raw data.\nWe adapt foundation models (FM) through learned neural compression to generate\nmulti-task embeddings while navigating the tradeoff between compression rate\nand embedding utility. We update only a small fraction of the FM parameters\n(10%) for a short training period (1% of the iterations of pre-training). We\nevaluate NEC on two EO tasks: scene classification and semantic segmentation.\nCompared with applying traditional compression to the raw data, NEC achieves\nsimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%\ncompression, performance drops by only 5% on the scene classification task.\nOverall, NEC is a data-efficient yet performant approach for multi-task EO\nmodelling.\n", "link": "http://arxiv.org/abs/2403.17886v1", "date": "2024-03-26", "relevancy": 1.9585, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5024}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4917}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4824}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Compressed%20Multi-task%20embeddings%20for%20Data-Efficient%20Downstream%20training%0A%20%20and%20inference%20in%20Earth%20Observation&body=Title%3A%20Compressed%20Multi-task%20embeddings%20for%20Data-Efficient%20Downstream%20training%0A%20%20and%20inference%20in%20Earth%20Observation%0AAuthor%3A%20Carlos%20Gomes%20and%20Thomas%20Brunschwiler%0AAbstract%3A%20%20%20As%20repositories%20of%20large%20scale%20data%20in%20earth%20observation%20%28EO%29%20have%20grown%2C%20so%0Ahave%20transfer%20and%20storage%20costs%20for%20model%20training%20and%20inference%2C%20expending%0Asignificant%20resources.%20We%20introduce%20Neural%20Embedding%20Compression%20%28NEC%29%2C%20based%0Aon%20the%20transfer%20of%20compressed%20embeddings%20to%20data%20consumers%20instead%20of%20raw%20data.%0AWe%20adapt%20foundation%20models%20%28FM%29%20through%20learned%20neural%20compression%20to%20generate%0Amulti-task%20embeddings%20while%20navigating%20the%20tradeoff%20between%20compression%20rate%0Aand%20embedding%20utility.%20We%20update%20only%20a%20small%20fraction%20of%20the%20FM%20parameters%0A%2810%25%29%20for%20a%20short%20training%20period%20%281%25%20of%20the%20iterations%20of%20pre-training%29.%20We%0Aevaluate%20NEC%20on%20two%20EO%20tasks%3A%20scene%20classification%20and%20semantic%20segmentation.%0ACompared%20with%20applying%20traditional%20compression%20to%20the%20raw%20data%2C%20NEC%20achieves%0Asimilar%20accuracy%20with%20a%2075%25%20to%2090%25%20reduction%20in%20data.%20Even%20at%2099.7%25%0Acompression%2C%20performance%20drops%20by%20only%205%25%20on%20the%20scene%20classification%20task.%0AOverall%2C%20NEC%20is%20a%20data-efficient%20yet%20performant%20approach%20for%20multi-task%20EO%0Amodelling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17886v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressed%20Multi-task%20embeddings%20for%20Data-Efficient%20Downstream%20training%0A%20%20and%20inference%20in%20Earth%20Observation&entry.906535625=Carlos%20Gomes%20and%20Thomas%20Brunschwiler&entry.1292438233=%20%20As%20repositories%20of%20large%20scale%20data%20in%20earth%20observation%20%28EO%29%20have%20grown%2C%20so%0Ahave%20transfer%20and%20storage%20costs%20for%20model%20training%20and%20inference%2C%20expending%0Asignificant%20resources.%20We%20introduce%20Neural%20Embedding%20Compression%20%28NEC%29%2C%20based%0Aon%20the%20transfer%20of%20compressed%20embeddings%20to%20data%20consumers%20instead%20of%20raw%20data.%0AWe%20adapt%20foundation%20models%20%28FM%29%20through%20learned%20neural%20compression%20to%20generate%0Amulti-task%20embeddings%20while%20navigating%20the%20tradeoff%20between%20compression%20rate%0Aand%20embedding%20utility.%20We%20update%20only%20a%20small%20fraction%20of%20the%20FM%20parameters%0A%2810%25%29%20for%20a%20short%20training%20period%20%281%25%20of%20the%20iterations%20of%20pre-training%29.%20We%0Aevaluate%20NEC%20on%20two%20EO%20tasks%3A%20scene%20classification%20and%20semantic%20segmentation.%0ACompared%20with%20applying%20traditional%20compression%20to%20the%20raw%20data%2C%20NEC%20achieves%0Asimilar%20accuracy%20with%20a%2075%25%20to%2090%25%20reduction%20in%20data.%20Even%20at%2099.7%25%0Acompression%2C%20performance%20drops%20by%20only%205%25%20on%20the%20scene%20classification%20task.%0AOverall%2C%20NEC%20is%20a%20data-efficient%20yet%20performant%20approach%20for%20multi-task%20EO%0Amodelling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17886v1&entry.124074799=Read"},
{"title": "As Good As A Coin Toss: Human detection of AI-generated images, videos,\n  audio, and audiovisual stimuli", "author": "Di Cooke and Abigail Edwards and Sophia Barkoff and Kathryn Kelly", "abstract": "  As synthetic media becomes progressively more realistic and barriers to using\nit continue to lower, the technology has been increasingly utilized for\nmalicious purposes, from financial fraud to nonconsensual pornography. Today,\nthe principal defense against being misled by synthetic media relies on the\nability of the human observer to visually and auditorily discern between real\nand fake. However, it remains unclear just how vulnerable people actually are\nto deceptive synthetic media in the course of their day to day lives. We\nconducted a perceptual study with 1276 participants to assess how accurate\npeople were at distinguishing synthetic images, audio only, video only, and\naudiovisual stimuli from authentic. To reflect the circumstances under which\npeople would likely encounter synthetic media in the wild, testing conditions\nand stimuli emulated a typical online platform, while all synthetic media used\nin the survey was sourced from publicly accessible generative AI technology.\n  We find that overall, participants struggled to meaningfully discern between\nsynthetic and authentic content. We also find that detection performance\nworsens when the stimuli contains synthetic content as compared to authentic\ncontent, images featuring human faces as compared to non face objects, a single\nmodality as compared to multimodal stimuli, mixed authenticity as compared to\nbeing fully synthetic for audiovisual stimuli, and features foreign languages\nas compared to languages the observer is fluent in. Finally, we also find that\nprior knowledge of synthetic media does not meaningfully impact their detection\nperformance. Collectively, these results indicate that people are highly\nsusceptible to being tricked by synthetic media in their daily lives and that\nhuman perceptual detection capabilities can no longer be relied upon as an\neffective counterdefense.\n", "link": "http://arxiv.org/abs/2403.16760v2", "date": "2024-03-26", "relevancy": 1.9459, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4934}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4924}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4544}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli&body=Title%3A%20As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli%0AAuthor%3A%20Di%20Cooke%20and%20Abigail%20Edwards%20and%20Sophia%20Barkoff%20and%20Kathryn%20Kelly%0AAbstract%3A%20%20%20As%20synthetic%20media%20becomes%20progressively%20more%20realistic%20and%20barriers%20to%20using%0Ait%20continue%20to%20lower%2C%20the%20technology%20has%20been%20increasingly%20utilized%20for%0Amalicious%20purposes%2C%20from%20financial%20fraud%20to%20nonconsensual%20pornography.%20Today%2C%0Athe%20principal%20defense%20against%20being%20misled%20by%20synthetic%20media%20relies%20on%20the%0Aability%20of%20the%20human%20observer%20to%20visually%20and%20auditorily%20discern%20between%20real%0Aand%20fake.%20However%2C%20it%20remains%20unclear%20just%20how%20vulnerable%20people%20actually%20are%0Ato%20deceptive%20synthetic%20media%20in%20the%20course%20of%20their%20day%20to%20day%20lives.%20We%0Aconducted%20a%20perceptual%20study%20with%201276%20participants%20to%20assess%20how%20accurate%0Apeople%20were%20at%20distinguishing%20synthetic%20images%2C%20audio%20only%2C%20video%20only%2C%20and%0Aaudiovisual%20stimuli%20from%20authentic.%20To%20reflect%20the%20circumstances%20under%20which%0Apeople%20would%20likely%20encounter%20synthetic%20media%20in%20the%20wild%2C%20testing%20conditions%0Aand%20stimuli%20emulated%20a%20typical%20online%20platform%2C%20while%20all%20synthetic%20media%20used%0Ain%20the%20survey%20was%20sourced%20from%20publicly%20accessible%20generative%20AI%20technology.%0A%20%20We%20find%20that%20overall%2C%20participants%20struggled%20to%20meaningfully%20discern%20between%0Asynthetic%20and%20authentic%20content.%20We%20also%20find%20that%20detection%20performance%0Aworsens%20when%20the%20stimuli%20contains%20synthetic%20content%20as%20compared%20to%20authentic%0Acontent%2C%20images%20featuring%20human%20faces%20as%20compared%20to%20non%20face%20objects%2C%20a%20single%0Amodality%20as%20compared%20to%20multimodal%20stimuli%2C%20mixed%20authenticity%20as%20compared%20to%0Abeing%20fully%20synthetic%20for%20audiovisual%20stimuli%2C%20and%20features%20foreign%20languages%0Aas%20compared%20to%20languages%20the%20observer%20is%20fluent%20in.%20Finally%2C%20we%20also%20find%20that%0Aprior%20knowledge%20of%20synthetic%20media%20does%20not%20meaningfully%20impact%20their%20detection%0Aperformance.%20Collectively%2C%20these%20results%20indicate%20that%20people%20are%20highly%0Asusceptible%20to%20being%20tricked%20by%20synthetic%20media%20in%20their%20daily%20lives%20and%20that%0Ahuman%20perceptual%20detection%20capabilities%20can%20no%20longer%20be%20relied%20upon%20as%20an%0Aeffective%20counterdefense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16760v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli&entry.906535625=Di%20Cooke%20and%20Abigail%20Edwards%20and%20Sophia%20Barkoff%20and%20Kathryn%20Kelly&entry.1292438233=%20%20As%20synthetic%20media%20becomes%20progressively%20more%20realistic%20and%20barriers%20to%20using%0Ait%20continue%20to%20lower%2C%20the%20technology%20has%20been%20increasingly%20utilized%20for%0Amalicious%20purposes%2C%20from%20financial%20fraud%20to%20nonconsensual%20pornography.%20Today%2C%0Athe%20principal%20defense%20against%20being%20misled%20by%20synthetic%20media%20relies%20on%20the%0Aability%20of%20the%20human%20observer%20to%20visually%20and%20auditorily%20discern%20between%20real%0Aand%20fake.%20However%2C%20it%20remains%20unclear%20just%20how%20vulnerable%20people%20actually%20are%0Ato%20deceptive%20synthetic%20media%20in%20the%20course%20of%20their%20day%20to%20day%20lives.%20We%0Aconducted%20a%20perceptual%20study%20with%201276%20participants%20to%20assess%20how%20accurate%0Apeople%20were%20at%20distinguishing%20synthetic%20images%2C%20audio%20only%2C%20video%20only%2C%20and%0Aaudiovisual%20stimuli%20from%20authentic.%20To%20reflect%20the%20circumstances%20under%20which%0Apeople%20would%20likely%20encounter%20synthetic%20media%20in%20the%20wild%2C%20testing%20conditions%0Aand%20stimuli%20emulated%20a%20typical%20online%20platform%2C%20while%20all%20synthetic%20media%20used%0Ain%20the%20survey%20was%20sourced%20from%20publicly%20accessible%20generative%20AI%20technology.%0A%20%20We%20find%20that%20overall%2C%20participants%20struggled%20to%20meaningfully%20discern%20between%0Asynthetic%20and%20authentic%20content.%20We%20also%20find%20that%20detection%20performance%0Aworsens%20when%20the%20stimuli%20contains%20synthetic%20content%20as%20compared%20to%20authentic%0Acontent%2C%20images%20featuring%20human%20faces%20as%20compared%20to%20non%20face%20objects%2C%20a%20single%0Amodality%20as%20compared%20to%20multimodal%20stimuli%2C%20mixed%20authenticity%20as%20compared%20to%0Abeing%20fully%20synthetic%20for%20audiovisual%20stimuli%2C%20and%20features%20foreign%20languages%0Aas%20compared%20to%20languages%20the%20observer%20is%20fluent%20in.%20Finally%2C%20we%20also%20find%20that%0Aprior%20knowledge%20of%20synthetic%20media%20does%20not%20meaningfully%20impact%20their%20detection%0Aperformance.%20Collectively%2C%20these%20results%20indicate%20that%20people%20are%20highly%0Asusceptible%20to%20being%20tricked%20by%20synthetic%20media%20in%20their%20daily%20lives%20and%20that%0Ahuman%20perceptual%20detection%20capabilities%20can%20no%20longer%20be%20relied%20upon%20as%20an%0Aeffective%20counterdefense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16760v2&entry.124074799=Read"},
{"title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning", "author": "Rui Pan and Xiang Liu and Shizhe Diao and Renjie Pi and Jipeng Zhang and Chi Han and Tong Zhang", "abstract": "  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n", "link": "http://arxiv.org/abs/2403.17919v1", "date": "2024-03-26", "relevancy": 1.941, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4851}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LISA%3A%20Layerwise%20Importance%20Sampling%20for%20Memory-Efficient%20Large%20Language%0A%20%20Model%20Fine-Tuning&body=Title%3A%20LISA%3A%20Layerwise%20Importance%20Sampling%20for%20Memory-Efficient%20Large%20Language%0A%20%20Model%20Fine-Tuning%0AAuthor%3A%20Rui%20Pan%20and%20Xiang%20Liu%20and%20Shizhe%20Diao%20and%20Renjie%20Pi%20and%20Jipeng%20Zhang%20and%20Chi%20Han%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20The%20machine%20learning%20community%20has%20witnessed%20impressive%20advancements%20since%0Athe%20first%20appearance%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20their%20huge%20memory%0Aconsumption%20has%20become%20a%20major%20roadblock%20to%20large-scale%20training.%20Parameter%0AEfficient%20Fine-Tuning%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20have%20been%0Aproposed%20to%20alleviate%20this%20problem%2C%20but%20their%20performance%20still%20fails%20to%20match%0Afull%20parameter%20training%20in%20most%20large-scale%20fine-tuning%20settings.%20Attempting%20to%0Acomplement%20this%20deficiency%2C%20we%20investigate%20layerwise%20properties%20of%20LoRA%20on%0Afine-tuning%20tasks%20and%20observe%20an%20uncommon%20skewness%20of%20weight%20norms%20across%0Adifferent%20layers.%20Utilizing%20this%20key%20observation%2C%20a%20surprisingly%20simple%0Atraining%20strategy%20is%20discovered%2C%20which%20outperforms%20both%20LoRA%20and%20full%20parameter%0Atraining%20in%20a%20wide%20range%20of%20settings%20with%20memory%20costs%20as%20low%20as%20LoRA.%20We%20name%0Ait%20Layerwise%20Importance%20Sampled%20AdamW%20%28LISA%29%2C%20a%20promising%20alternative%20for%20LoRA%2C%0Awhich%20applies%20the%20idea%20of%20importance%20sampling%20to%20different%20layers%20in%20LLMs%20and%0Arandomly%20freeze%20most%20middle%20layers%20during%20optimization.%20Experimental%20results%0Ashow%20that%20with%20similar%20or%20less%20GPU%20memory%20consumption%2C%20LISA%20surpasses%20LoRA%20or%0Aeven%20full%20parameter%20tuning%20in%20downstream%20fine-tuning%20tasks%2C%20where%20LISA%0Aconsistently%20outperforms%20LoRA%20by%20over%20%2411%5C%25%24-%2437%5C%25%24%20in%20terms%20of%20MT-Bench%0Ascores.%20On%20large%20models%2C%20specifically%20LLaMA-2-70B%2C%20LISA%20achieves%20on-par%20or%0Abetter%20performance%20than%20LoRA%20on%20MT-Bench%2C%20GSM8K%2C%20and%20PubMedQA%2C%20demonstrating%0Aits%20effectiveness%20across%20different%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17919v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LISA%3A%20Layerwise%20Importance%20Sampling%20for%20Memory-Efficient%20Large%20Language%0A%20%20Model%20Fine-Tuning&entry.906535625=Rui%20Pan%20and%20Xiang%20Liu%20and%20Shizhe%20Diao%20and%20Renjie%20Pi%20and%20Jipeng%20Zhang%20and%20Chi%20Han%20and%20Tong%20Zhang&entry.1292438233=%20%20The%20machine%20learning%20community%20has%20witnessed%20impressive%20advancements%20since%0Athe%20first%20appearance%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20their%20huge%20memory%0Aconsumption%20has%20become%20a%20major%20roadblock%20to%20large-scale%20training.%20Parameter%0AEfficient%20Fine-Tuning%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20have%20been%0Aproposed%20to%20alleviate%20this%20problem%2C%20but%20their%20performance%20still%20fails%20to%20match%0Afull%20parameter%20training%20in%20most%20large-scale%20fine-tuning%20settings.%20Attempting%20to%0Acomplement%20this%20deficiency%2C%20we%20investigate%20layerwise%20properties%20of%20LoRA%20on%0Afine-tuning%20tasks%20and%20observe%20an%20uncommon%20skewness%20of%20weight%20norms%20across%0Adifferent%20layers.%20Utilizing%20this%20key%20observation%2C%20a%20surprisingly%20simple%0Atraining%20strategy%20is%20discovered%2C%20which%20outperforms%20both%20LoRA%20and%20full%20parameter%0Atraining%20in%20a%20wide%20range%20of%20settings%20with%20memory%20costs%20as%20low%20as%20LoRA.%20We%20name%0Ait%20Layerwise%20Importance%20Sampled%20AdamW%20%28LISA%29%2C%20a%20promising%20alternative%20for%20LoRA%2C%0Awhich%20applies%20the%20idea%20of%20importance%20sampling%20to%20different%20layers%20in%20LLMs%20and%0Arandomly%20freeze%20most%20middle%20layers%20during%20optimization.%20Experimental%20results%0Ashow%20that%20with%20similar%20or%20less%20GPU%20memory%20consumption%2C%20LISA%20surpasses%20LoRA%20or%0Aeven%20full%20parameter%20tuning%20in%20downstream%20fine-tuning%20tasks%2C%20where%20LISA%0Aconsistently%20outperforms%20LoRA%20by%20over%20%2411%5C%25%24-%2437%5C%25%24%20in%20terms%20of%20MT-Bench%0Ascores.%20On%20large%20models%2C%20specifically%20LLaMA-2-70B%2C%20LISA%20achieves%20on-par%20or%0Abetter%20performance%20than%20LoRA%20on%20MT-Bench%2C%20GSM8K%2C%20and%20PubMedQA%2C%20demonstrating%0Aits%20effectiveness%20across%20different%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17919v1&entry.124074799=Read"},
{"title": "Disentangling the Spectral Properties of the Hodge Laplacian: Not All\n  Small Eigenvalues Are Equal", "author": "Vincent P. Grande and Michael T. Schaub", "abstract": "  The rich spectral information of the graph Laplacian has been instrumental in\ngraph theory, machine learning, and graph signal processing for applications\nsuch as graph classification, clustering, or eigenmode analysis. Recently, the\nHodge Laplacian has come into focus as a generalisation of the ordinary\nLaplacian for higher-order graph models such as simplicial and cellular\ncomplexes. Akin to the traditional analysis of graph Laplacians, many authors\nanalyse the smallest eigenvalues of the Hodge Laplacian, which are connected to\nimportant topological properties such as homology. However, small eigenvalues\nof the Hodge Laplacian can carry different information depending on whether\nthey are related to curl or gradient eigenmodes, and thus may not be\ncomparable. We therefore introduce the notion of persistent eigenvector\nsimilarity and provide a method to track individual harmonic, curl, and\ngradient eigenvectors/-values through the so-called persistence filtration,\nleveraging the full information contained in the Hodge-Laplacian spectrum\nacross all possible scales of a point cloud. Finally, we use our insights (a)\nto introduce a novel form of Hodge spectral clustering and (b) to classify\nedges and higher-order simplices based on their relationship to the smallest\nharmonic, curl, and gradient eigenvectors.\n", "link": "http://arxiv.org/abs/2311.14427v2", "date": "2024-03-26", "relevancy": 1.9232, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3979}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3813}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3748}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Disentangling%20the%20Spectral%20Properties%20of%20the%20Hodge%20Laplacian%3A%20Not%20All%0A%20%20Small%20Eigenvalues%20Are%20Equal&body=Title%3A%20Disentangling%20the%20Spectral%20Properties%20of%20the%20Hodge%20Laplacian%3A%20Not%20All%0A%20%20Small%20Eigenvalues%20Are%20Equal%0AAuthor%3A%20Vincent%20P.%20Grande%20and%20Michael%20T.%20Schaub%0AAbstract%3A%20%20%20The%20rich%20spectral%20information%20of%20the%20graph%20Laplacian%20has%20been%20instrumental%20in%0Agraph%20theory%2C%20machine%20learning%2C%20and%20graph%20signal%20processing%20for%20applications%0Asuch%20as%20graph%20classification%2C%20clustering%2C%20or%20eigenmode%20analysis.%20Recently%2C%20the%0AHodge%20Laplacian%20has%20come%20into%20focus%20as%20a%20generalisation%20of%20the%20ordinary%0ALaplacian%20for%20higher-order%20graph%20models%20such%20as%20simplicial%20and%20cellular%0Acomplexes.%20Akin%20to%20the%20traditional%20analysis%20of%20graph%20Laplacians%2C%20many%20authors%0Aanalyse%20the%20smallest%20eigenvalues%20of%20the%20Hodge%20Laplacian%2C%20which%20are%20connected%20to%0Aimportant%20topological%20properties%20such%20as%20homology.%20However%2C%20small%20eigenvalues%0Aof%20the%20Hodge%20Laplacian%20can%20carry%20different%20information%20depending%20on%20whether%0Athey%20are%20related%20to%20curl%20or%20gradient%20eigenmodes%2C%20and%20thus%20may%20not%20be%0Acomparable.%20We%20therefore%20introduce%20the%20notion%20of%20persistent%20eigenvector%0Asimilarity%20and%20provide%20a%20method%20to%20track%20individual%20harmonic%2C%20curl%2C%20and%0Agradient%20eigenvectors/-values%20through%20the%20so-called%20persistence%20filtration%2C%0Aleveraging%20the%20full%20information%20contained%20in%20the%20Hodge-Laplacian%20spectrum%0Aacross%20all%20possible%20scales%20of%20a%20point%20cloud.%20Finally%2C%20we%20use%20our%20insights%20%28a%29%0Ato%20introduce%20a%20novel%20form%20of%20Hodge%20spectral%20clustering%20and%20%28b%29%20to%20classify%0Aedges%20and%20higher-order%20simplices%20based%20on%20their%20relationship%20to%20the%20smallest%0Aharmonic%2C%20curl%2C%20and%20gradient%20eigenvectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14427v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20the%20Spectral%20Properties%20of%20the%20Hodge%20Laplacian%3A%20Not%20All%0A%20%20Small%20Eigenvalues%20Are%20Equal&entry.906535625=Vincent%20P.%20Grande%20and%20Michael%20T.%20Schaub&entry.1292438233=%20%20The%20rich%20spectral%20information%20of%20the%20graph%20Laplacian%20has%20been%20instrumental%20in%0Agraph%20theory%2C%20machine%20learning%2C%20and%20graph%20signal%20processing%20for%20applications%0Asuch%20as%20graph%20classification%2C%20clustering%2C%20or%20eigenmode%20analysis.%20Recently%2C%20the%0AHodge%20Laplacian%20has%20come%20into%20focus%20as%20a%20generalisation%20of%20the%20ordinary%0ALaplacian%20for%20higher-order%20graph%20models%20such%20as%20simplicial%20and%20cellular%0Acomplexes.%20Akin%20to%20the%20traditional%20analysis%20of%20graph%20Laplacians%2C%20many%20authors%0Aanalyse%20the%20smallest%20eigenvalues%20of%20the%20Hodge%20Laplacian%2C%20which%20are%20connected%20to%0Aimportant%20topological%20properties%20such%20as%20homology.%20However%2C%20small%20eigenvalues%0Aof%20the%20Hodge%20Laplacian%20can%20carry%20different%20information%20depending%20on%20whether%0Athey%20are%20related%20to%20curl%20or%20gradient%20eigenmodes%2C%20and%20thus%20may%20not%20be%0Acomparable.%20We%20therefore%20introduce%20the%20notion%20of%20persistent%20eigenvector%0Asimilarity%20and%20provide%20a%20method%20to%20track%20individual%20harmonic%2C%20curl%2C%20and%0Agradient%20eigenvectors/-values%20through%20the%20so-called%20persistence%20filtration%2C%0Aleveraging%20the%20full%20information%20contained%20in%20the%20Hodge-Laplacian%20spectrum%0Aacross%20all%20possible%20scales%20of%20a%20point%20cloud.%20Finally%2C%20we%20use%20our%20insights%20%28a%29%0Ato%20introduce%20a%20novel%20form%20of%20Hodge%20spectral%20clustering%20and%20%28b%29%20to%20classify%0Aedges%20and%20higher-order%20simplices%20based%20on%20their%20relationship%20to%20the%20smallest%0Aharmonic%2C%20curl%2C%20and%20gradient%20eigenvectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14427v2&entry.124074799=Read"},
{"title": "D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time\n  Series Forecasting", "author": "Xiaobing Yuan and Ling Chen", "abstract": "  In time series forecasting, effectively disentangling intricate temporal\npatterns is crucial. While recent works endeavor to combine decomposition\ntechniques with deep learning, multiple frequencies may still be mixed in the\ndecomposed components, e.g., trend and seasonal. Furthermore, frequency domain\nanalysis methods, e.g., Fourier and wavelet transforms, have limitations in\nresolution in the time domain and adaptability. In this paper, we propose\nD-PAD, a deep-shallow multi-frequency patterns disentangling neural network for\ntime series forecasting. Specifically, a multi-component decomposing (MCD)\nblock is introduced to decompose the series into components with different\nfrequency ranges, corresponding to the \"shallow\" aspect. A\ndecomposition-reconstruction-decomposition (D-R-D) module is proposed to\nprogressively extract the information of frequencies mixed in the components,\ncorresponding to the \"deep\" aspect. After that, an interaction and fusion (IF)\nmodule is used to further analyze the components. Extensive experiments on\nseven real-world datasets demonstrate that D-PAD achieves the state-of-the-art\nperformance, outperforming the best baseline by an average of 9.48% and 7.15%\nin MSE and MAE, respectively.\n", "link": "http://arxiv.org/abs/2403.17814v1", "date": "2024-03-26", "relevancy": 1.9104, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.503}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4809}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4641}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20D-PAD%3A%20Deep-Shallow%20Multi-Frequency%20Patterns%20Disentangling%20for%20Time%0A%20%20Series%20Forecasting&body=Title%3A%20D-PAD%3A%20Deep-Shallow%20Multi-Frequency%20Patterns%20Disentangling%20for%20Time%0A%20%20Series%20Forecasting%0AAuthor%3A%20Xiaobing%20Yuan%20and%20Ling%20Chen%0AAbstract%3A%20%20%20In%20time%20series%20forecasting%2C%20effectively%20disentangling%20intricate%20temporal%0Apatterns%20is%20crucial.%20While%20recent%20works%20endeavor%20to%20combine%20decomposition%0Atechniques%20with%20deep%20learning%2C%20multiple%20frequencies%20may%20still%20be%20mixed%20in%20the%0Adecomposed%20components%2C%20e.g.%2C%20trend%20and%20seasonal.%20Furthermore%2C%20frequency%20domain%0Aanalysis%20methods%2C%20e.g.%2C%20Fourier%20and%20wavelet%20transforms%2C%20have%20limitations%20in%0Aresolution%20in%20the%20time%20domain%20and%20adaptability.%20In%20this%20paper%2C%20we%20propose%0AD-PAD%2C%20a%20deep-shallow%20multi-frequency%20patterns%20disentangling%20neural%20network%20for%0Atime%20series%20forecasting.%20Specifically%2C%20a%20multi-component%20decomposing%20%28MCD%29%0Ablock%20is%20introduced%20to%20decompose%20the%20series%20into%20components%20with%20different%0Afrequency%20ranges%2C%20corresponding%20to%20the%20%22shallow%22%20aspect.%20A%0Adecomposition-reconstruction-decomposition%20%28D-R-D%29%20module%20is%20proposed%20to%0Aprogressively%20extract%20the%20information%20of%20frequencies%20mixed%20in%20the%20components%2C%0Acorresponding%20to%20the%20%22deep%22%20aspect.%20After%20that%2C%20an%20interaction%20and%20fusion%20%28IF%29%0Amodule%20is%20used%20to%20further%20analyze%20the%20components.%20Extensive%20experiments%20on%0Aseven%20real-world%20datasets%20demonstrate%20that%20D-PAD%20achieves%20the%20state-of-the-art%0Aperformance%2C%20outperforming%20the%20best%20baseline%20by%20an%20average%20of%209.48%25%20and%207.15%25%0Ain%20MSE%20and%20MAE%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17814v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-PAD%3A%20Deep-Shallow%20Multi-Frequency%20Patterns%20Disentangling%20for%20Time%0A%20%20Series%20Forecasting&entry.906535625=Xiaobing%20Yuan%20and%20Ling%20Chen&entry.1292438233=%20%20In%20time%20series%20forecasting%2C%20effectively%20disentangling%20intricate%20temporal%0Apatterns%20is%20crucial.%20While%20recent%20works%20endeavor%20to%20combine%20decomposition%0Atechniques%20with%20deep%20learning%2C%20multiple%20frequencies%20may%20still%20be%20mixed%20in%20the%0Adecomposed%20components%2C%20e.g.%2C%20trend%20and%20seasonal.%20Furthermore%2C%20frequency%20domain%0Aanalysis%20methods%2C%20e.g.%2C%20Fourier%20and%20wavelet%20transforms%2C%20have%20limitations%20in%0Aresolution%20in%20the%20time%20domain%20and%20adaptability.%20In%20this%20paper%2C%20we%20propose%0AD-PAD%2C%20a%20deep-shallow%20multi-frequency%20patterns%20disentangling%20neural%20network%20for%0Atime%20series%20forecasting.%20Specifically%2C%20a%20multi-component%20decomposing%20%28MCD%29%0Ablock%20is%20introduced%20to%20decompose%20the%20series%20into%20components%20with%20different%0Afrequency%20ranges%2C%20corresponding%20to%20the%20%22shallow%22%20aspect.%20A%0Adecomposition-reconstruction-decomposition%20%28D-R-D%29%20module%20is%20proposed%20to%0Aprogressively%20extract%20the%20information%20of%20frequencies%20mixed%20in%20the%20components%2C%0Acorresponding%20to%20the%20%22deep%22%20aspect.%20After%20that%2C%20an%20interaction%20and%20fusion%20%28IF%29%0Amodule%20is%20used%20to%20further%20analyze%20the%20components.%20Extensive%20experiments%20on%0Aseven%20real-world%20datasets%20demonstrate%20that%20D-PAD%20achieves%20the%20state-of-the-art%0Aperformance%2C%20outperforming%20the%20best%20baseline%20by%20an%20average%20of%209.48%25%20and%207.15%25%0Ain%20MSE%20and%20MAE%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17814v1&entry.124074799=Read"},
{"title": "GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction", "author": "Hrishav Bakul Barua and Kalin Stefanov and KokSheik Wong and Abhinav Dhall and Ganesh Krishnasamy", "abstract": "  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n", "link": "http://arxiv.org/abs/2403.17837v1", "date": "2024-03-26", "relevancy": 1.9025, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4804}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4796}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4698}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GTA-HDR%3A%20A%20Large-Scale%20Synthetic%20Dataset%20for%20HDR%20Image%20Reconstruction&body=Title%3A%20GTA-HDR%3A%20A%20Large-Scale%20Synthetic%20Dataset%20for%20HDR%20Image%20Reconstruction%0AAuthor%3A%20Hrishav%20Bakul%20Barua%20and%20Kalin%20Stefanov%20and%20KokSheik%20Wong%20and%20Abhinav%20Dhall%20and%20Ganesh%20Krishnasamy%0AAbstract%3A%20%20%20High%20Dynamic%20Range%20%28HDR%29%20content%20%28i.e.%2C%20images%20and%20videos%29%20has%20a%20broad%20range%0Aof%20applications.%20However%2C%20capturing%20HDR%20content%20from%20real-world%20scenes%20is%0Aexpensive%20and%20time-%20consuming.%20Therefore%2C%20the%20challenging%20task%20of%0Areconstructing%20visually%20accurate%20HDR%20images%20from%20their%20Low%20Dynamic%20Range%20%28LDR%29%0Acounterparts%20is%20gaining%20attention%20in%20the%20vision%20research%20community.%20A%20major%0Achallenge%20in%20this%20research%20problem%20is%20the%20lack%20of%20datasets%2C%20which%20capture%0Adiverse%20scene%20conditions%20%28e.g.%2C%20lighting%2C%20shadows%2C%20weather%2C%20locations%2C%0Alandscapes%2C%20objects%2C%20humans%2C%20buildings%29%20and%20various%20image%20features%20%28e.g.%2C%0Acolor%2C%20contrast%2C%20saturation%2C%20hue%2C%20luminance%2C%20brightness%2C%20radiance%29.%20To%20address%0Athis%20gap%2C%20in%20this%20paper%2C%20we%20introduce%20GTA-HDR%2C%20a%20large-scale%20synthetic%20dataset%0Aof%20photo-realistic%20HDR%20images%20sampled%20from%20the%20GTA-V%20video%20game.%20We%20perform%0Athorough%20evaluation%20of%20the%20proposed%20dataset%2C%20which%20demonstrates%20significant%0Aqualitative%20and%20quantitative%20improvements%20of%20the%20state-of-the-art%20HDR%20image%0Areconstruction%20methods.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20dataset%20and%20its%20impact%20on%20additional%20computer%20vision%20tasks%20including%0A3D%20human%20pose%20estimation%2C%20human%20body%20part%20segmentation%2C%20and%20holistic%20scene%0Asegmentation.%20The%20dataset%2C%20data%20collection%20pipeline%2C%20and%20evaluation%20code%20are%0Aavailable%20at%3A%20https%3A//github.com/HrishavBakulBarua/GTA-HDR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17837v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTA-HDR%3A%20A%20Large-Scale%20Synthetic%20Dataset%20for%20HDR%20Image%20Reconstruction&entry.906535625=Hrishav%20Bakul%20Barua%20and%20Kalin%20Stefanov%20and%20KokSheik%20Wong%20and%20Abhinav%20Dhall%20and%20Ganesh%20Krishnasamy&entry.1292438233=%20%20High%20Dynamic%20Range%20%28HDR%29%20content%20%28i.e.%2C%20images%20and%20videos%29%20has%20a%20broad%20range%0Aof%20applications.%20However%2C%20capturing%20HDR%20content%20from%20real-world%20scenes%20is%0Aexpensive%20and%20time-%20consuming.%20Therefore%2C%20the%20challenging%20task%20of%0Areconstructing%20visually%20accurate%20HDR%20images%20from%20their%20Low%20Dynamic%20Range%20%28LDR%29%0Acounterparts%20is%20gaining%20attention%20in%20the%20vision%20research%20community.%20A%20major%0Achallenge%20in%20this%20research%20problem%20is%20the%20lack%20of%20datasets%2C%20which%20capture%0Adiverse%20scene%20conditions%20%28e.g.%2C%20lighting%2C%20shadows%2C%20weather%2C%20locations%2C%0Alandscapes%2C%20objects%2C%20humans%2C%20buildings%29%20and%20various%20image%20features%20%28e.g.%2C%0Acolor%2C%20contrast%2C%20saturation%2C%20hue%2C%20luminance%2C%20brightness%2C%20radiance%29.%20To%20address%0Athis%20gap%2C%20in%20this%20paper%2C%20we%20introduce%20GTA-HDR%2C%20a%20large-scale%20synthetic%20dataset%0Aof%20photo-realistic%20HDR%20images%20sampled%20from%20the%20GTA-V%20video%20game.%20We%20perform%0Athorough%20evaluation%20of%20the%20proposed%20dataset%2C%20which%20demonstrates%20significant%0Aqualitative%20and%20quantitative%20improvements%20of%20the%20state-of-the-art%20HDR%20image%0Areconstruction%20methods.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20dataset%20and%20its%20impact%20on%20additional%20computer%20vision%20tasks%20including%0A3D%20human%20pose%20estimation%2C%20human%20body%20part%20segmentation%2C%20and%20holistic%20scene%0Asegmentation.%20The%20dataset%2C%20data%20collection%20pipeline%2C%20and%20evaluation%20code%20are%0Aavailable%20at%3A%20https%3A//github.com/HrishavBakulBarua/GTA-HDR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17837v1&entry.124074799=Read"},
{"title": "AID: Attention Interpolation of Text-to-Image Diffusion", "author": "Qiyuan He and Jinghao Wang and Ziwei Liu and Angela Yao", "abstract": "  Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.\n", "link": "http://arxiv.org/abs/2403.17924v1", "date": "2024-03-26", "relevancy": 1.8865, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6315}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6309}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6269}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AID%3A%20Attention%20Interpolation%20of%20Text-to-Image%20Diffusion&body=Title%3A%20AID%3A%20Attention%20Interpolation%20of%20Text-to-Image%20Diffusion%0AAuthor%3A%20Qiyuan%20He%20and%20Jinghao%20Wang%20and%20Ziwei%20Liu%20and%20Angela%20Yao%0AAbstract%3A%20%20%20Conditional%20diffusion%20models%20can%20create%20unseen%20images%20in%20various%20settings%2C%0Aaiding%20image%20interpolation.%20Interpolation%20in%20latent%20spaces%20is%20well-studied%2C%20but%0Ainterpolation%20with%20specific%20conditions%20like%20text%20or%20poses%20is%20less%20understood.%0ASimple%20approaches%2C%20such%20as%20linear%20interpolation%20in%20the%20space%20of%20conditions%2C%0Aoften%20result%20in%20images%20that%20lack%20consistency%2C%20smoothness%2C%20and%20fidelity.%20To%20that%0Aend%2C%20we%20introduce%20a%20novel%20training-free%20technique%20named%20Attention%20Interpolation%0Avia%20Diffusion%20%28AID%29.%20Our%20key%20contributions%20include%201%29%20proposing%20an%20inner/outer%0Ainterpolated%20attention%20layer%3B%202%29%20fusing%20the%20interpolated%20attention%20with%0Aself-attention%20to%20boost%20fidelity%3B%20and%203%29%20applying%20beta%20distribution%20to%0Aselection%20to%20increase%20smoothness.%20We%20also%20present%20a%20variant%2C%20Prompt-guided%0AAttention%20Interpolation%20via%20Diffusion%20%28PAID%29%2C%20that%20considers%20interpolation%20as%20a%0Acondition-dependent%20generative%20process.%20This%20method%20enables%20the%20creation%20of%20new%0Aimages%20with%20greater%20consistency%2C%20smoothness%2C%20and%20efficiency%2C%20and%20offers%20control%0Aover%20the%20exact%20path%20of%20interpolation.%20Our%20approach%20demonstrates%20effectiveness%0Afor%20conceptual%20and%20spatial%20interpolation.%20Code%20and%20demo%20are%20available%20at%0Ahttps%3A//github.com/QY-H00/attention-interpolation-diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17924v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AID%3A%20Attention%20Interpolation%20of%20Text-to-Image%20Diffusion&entry.906535625=Qiyuan%20He%20and%20Jinghao%20Wang%20and%20Ziwei%20Liu%20and%20Angela%20Yao&entry.1292438233=%20%20Conditional%20diffusion%20models%20can%20create%20unseen%20images%20in%20various%20settings%2C%0Aaiding%20image%20interpolation.%20Interpolation%20in%20latent%20spaces%20is%20well-studied%2C%20but%0Ainterpolation%20with%20specific%20conditions%20like%20text%20or%20poses%20is%20less%20understood.%0ASimple%20approaches%2C%20such%20as%20linear%20interpolation%20in%20the%20space%20of%20conditions%2C%0Aoften%20result%20in%20images%20that%20lack%20consistency%2C%20smoothness%2C%20and%20fidelity.%20To%20that%0Aend%2C%20we%20introduce%20a%20novel%20training-free%20technique%20named%20Attention%20Interpolation%0Avia%20Diffusion%20%28AID%29.%20Our%20key%20contributions%20include%201%29%20proposing%20an%20inner/outer%0Ainterpolated%20attention%20layer%3B%202%29%20fusing%20the%20interpolated%20attention%20with%0Aself-attention%20to%20boost%20fidelity%3B%20and%203%29%20applying%20beta%20distribution%20to%0Aselection%20to%20increase%20smoothness.%20We%20also%20present%20a%20variant%2C%20Prompt-guided%0AAttention%20Interpolation%20via%20Diffusion%20%28PAID%29%2C%20that%20considers%20interpolation%20as%20a%0Acondition-dependent%20generative%20process.%20This%20method%20enables%20the%20creation%20of%20new%0Aimages%20with%20greater%20consistency%2C%20smoothness%2C%20and%20efficiency%2C%20and%20offers%20control%0Aover%20the%20exact%20path%20of%20interpolation.%20Our%20approach%20demonstrates%20effectiveness%0Afor%20conceptual%20and%20spatial%20interpolation.%20Code%20and%20demo%20are%20available%20at%0Ahttps%3A//github.com/QY-H00/attention-interpolation-diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17924v1&entry.124074799=Read"},
{"title": "SciCapenter: Supporting Caption Composition for Scientific Figures with\n  Machine-Generated Captions and Ratings", "author": "Ting-Yao Hsu and Chieh-Yang Huang and Shih-Hong Huang and Ryan Rossi and Sungchul Kim and Tong Yu and C. Lee Giles and Ting-Hao K. Huang", "abstract": "  Crafting effective captions for figures is important. Readers heavily depend\non these captions to grasp the figure's message. However, despite a\nwell-developed set of AI technologies for figures and captions, these have\nrarely been tested for usefulness in aiding caption writing. This paper\nintroduces SciCapenter, an interactive system that puts together cutting-edge\nAI technologies for scientific figure captions to aid caption composition.\nSciCapenter generates a variety of captions for each figure in a scholarly\narticle, providing scores and a comprehensive checklist to assess caption\nquality across multiple critical aspects, such as helpfulness, OCR mention, key\ntakeaways, and visual properties reference. Users can directly edit captions in\nSciCapenter, resubmit for revised evaluations, and iteratively refine them. A\nuser study with Ph.D. students indicates that SciCapenter significantly lowers\nthe cognitive load of caption writing. Participants' feedback further offers\nvaluable design insights for future systems aiming to enhance caption writing.\n", "link": "http://arxiv.org/abs/2403.17784v1", "date": "2024-03-26", "relevancy": 1.8756, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.481}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4601}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SciCapenter%3A%20Supporting%20Caption%20Composition%20for%20Scientific%20Figures%20with%0A%20%20Machine-Generated%20Captions%20and%20Ratings&body=Title%3A%20SciCapenter%3A%20Supporting%20Caption%20Composition%20for%20Scientific%20Figures%20with%0A%20%20Machine-Generated%20Captions%20and%20Ratings%0AAuthor%3A%20Ting-Yao%20Hsu%20and%20Chieh-Yang%20Huang%20and%20Shih-Hong%20Huang%20and%20Ryan%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20C.%20Lee%20Giles%20and%20Ting-Hao%20K.%20Huang%0AAbstract%3A%20%20%20Crafting%20effective%20captions%20for%20figures%20is%20important.%20Readers%20heavily%20depend%0Aon%20these%20captions%20to%20grasp%20the%20figure%27s%20message.%20However%2C%20despite%20a%0Awell-developed%20set%20of%20AI%20technologies%20for%20figures%20and%20captions%2C%20these%20have%0Ararely%20been%20tested%20for%20usefulness%20in%20aiding%20caption%20writing.%20This%20paper%0Aintroduces%20SciCapenter%2C%20an%20interactive%20system%20that%20puts%20together%20cutting-edge%0AAI%20technologies%20for%20scientific%20figure%20captions%20to%20aid%20caption%20composition.%0ASciCapenter%20generates%20a%20variety%20of%20captions%20for%20each%20figure%20in%20a%20scholarly%0Aarticle%2C%20providing%20scores%20and%20a%20comprehensive%20checklist%20to%20assess%20caption%0Aquality%20across%20multiple%20critical%20aspects%2C%20such%20as%20helpfulness%2C%20OCR%20mention%2C%20key%0Atakeaways%2C%20and%20visual%20properties%20reference.%20Users%20can%20directly%20edit%20captions%20in%0ASciCapenter%2C%20resubmit%20for%20revised%20evaluations%2C%20and%20iteratively%20refine%20them.%20A%0Auser%20study%20with%20Ph.D.%20students%20indicates%20that%20SciCapenter%20significantly%20lowers%0Athe%20cognitive%20load%20of%20caption%20writing.%20Participants%27%20feedback%20further%20offers%0Avaluable%20design%20insights%20for%20future%20systems%20aiming%20to%20enhance%20caption%20writing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17784v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SciCapenter%3A%20Supporting%20Caption%20Composition%20for%20Scientific%20Figures%20with%0A%20%20Machine-Generated%20Captions%20and%20Ratings&entry.906535625=Ting-Yao%20Hsu%20and%20Chieh-Yang%20Huang%20and%20Shih-Hong%20Huang%20and%20Ryan%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20C.%20Lee%20Giles%20and%20Ting-Hao%20K.%20Huang&entry.1292438233=%20%20Crafting%20effective%20captions%20for%20figures%20is%20important.%20Readers%20heavily%20depend%0Aon%20these%20captions%20to%20grasp%20the%20figure%27s%20message.%20However%2C%20despite%20a%0Awell-developed%20set%20of%20AI%20technologies%20for%20figures%20and%20captions%2C%20these%20have%0Ararely%20been%20tested%20for%20usefulness%20in%20aiding%20caption%20writing.%20This%20paper%0Aintroduces%20SciCapenter%2C%20an%20interactive%20system%20that%20puts%20together%20cutting-edge%0AAI%20technologies%20for%20scientific%20figure%20captions%20to%20aid%20caption%20composition.%0ASciCapenter%20generates%20a%20variety%20of%20captions%20for%20each%20figure%20in%20a%20scholarly%0Aarticle%2C%20providing%20scores%20and%20a%20comprehensive%20checklist%20to%20assess%20caption%0Aquality%20across%20multiple%20critical%20aspects%2C%20such%20as%20helpfulness%2C%20OCR%20mention%2C%20key%0Atakeaways%2C%20and%20visual%20properties%20reference.%20Users%20can%20directly%20edit%20captions%20in%0ASciCapenter%2C%20resubmit%20for%20revised%20evaluations%2C%20and%20iteratively%20refine%20them.%20A%0Auser%20study%20with%20Ph.D.%20students%20indicates%20that%20SciCapenter%20significantly%20lowers%0Athe%20cognitive%20load%20of%20caption%20writing.%20Participants%27%20feedback%20further%20offers%0Avaluable%20design%20insights%20for%20future%20systems%20aiming%20to%20enhance%20caption%20writing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17784v1&entry.124074799=Read"},
{"title": "Empowering Data Mesh with Federated Learning", "author": "Haoyuan Li and Salman Toor", "abstract": "  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n", "link": "http://arxiv.org/abs/2403.17878v1", "date": "2024-03-26", "relevancy": 1.8289, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4641}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4506}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Empowering%20Data%20Mesh%20with%20Federated%20Learning&body=Title%3A%20Empowering%20Data%20Mesh%20with%20Federated%20Learning%0AAuthor%3A%20Haoyuan%20Li%20and%20Salman%20Toor%0AAbstract%3A%20%20%20The%20evolution%20of%20data%20architecture%20has%20seen%20the%20rise%20of%20data%20lakes%2C%20aiming%20to%0Asolve%20the%20bottlenecks%20of%20data%20management%20and%20promote%20intelligent%0Adecision-making.%20However%2C%20this%20centralized%20architecture%20is%20limited%20by%20the%0Aproliferation%20of%20data%20sources%20and%20the%20growing%20demand%20for%20timely%20analysis%20and%0Aprocessing.%20A%20new%20data%20paradigm%2C%20Data%20Mesh%2C%20is%20proposed%20to%20overcome%20these%0Achallenges.%20Data%20Mesh%20treats%20domains%20as%20a%20first-class%20concern%20by%20distributing%0Athe%20data%20ownership%20from%20the%20central%20team%20to%20each%20data%20domain%2C%20while%20keeping%20the%0Afederated%20governance%20to%20monitor%20domains%20and%20their%20data%20products.%20Many%0Amulti-million%20dollar%20organizations%20like%20Paypal%2C%20Netflix%2C%20and%20Zalando%20have%0Aalready%20transformed%20their%20data%20analysis%20pipelines%20based%20on%20this%20new%0Aarchitecture.%20In%20this%20decentralized%20architecture%20where%20data%20is%20locally%0Apreserved%20by%20each%20domain%20team%2C%20traditional%20centralized%20machine%20learning%20is%0Aincapable%20of%20conducting%20effective%20analysis%20across%20multiple%20domains%2C%20especially%0Afor%20security-sensitive%20organizations.%20To%20this%20end%2C%20we%20introduce%20a%20pioneering%0Aapproach%20that%20incorporates%20Federated%20Learning%20into%20Data%20Mesh.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20open-source%20applied%20work%20that%20represents%20a%0Acritical%20advancement%20toward%20the%20integration%20of%20federated%20learning%20methods%20into%0Athe%20Data%20Mesh%20paradigm%2C%20underscoring%20the%20promising%20prospects%20for%0Aprivacy-preserving%20and%20decentralized%20data%20analysis%20strategies%20within%20Data%20Mesh%0Aarchitecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17878v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Data%20Mesh%20with%20Federated%20Learning&entry.906535625=Haoyuan%20Li%20and%20Salman%20Toor&entry.1292438233=%20%20The%20evolution%20of%20data%20architecture%20has%20seen%20the%20rise%20of%20data%20lakes%2C%20aiming%20to%0Asolve%20the%20bottlenecks%20of%20data%20management%20and%20promote%20intelligent%0Adecision-making.%20However%2C%20this%20centralized%20architecture%20is%20limited%20by%20the%0Aproliferation%20of%20data%20sources%20and%20the%20growing%20demand%20for%20timely%20analysis%20and%0Aprocessing.%20A%20new%20data%20paradigm%2C%20Data%20Mesh%2C%20is%20proposed%20to%20overcome%20these%0Achallenges.%20Data%20Mesh%20treats%20domains%20as%20a%20first-class%20concern%20by%20distributing%0Athe%20data%20ownership%20from%20the%20central%20team%20to%20each%20data%20domain%2C%20while%20keeping%20the%0Afederated%20governance%20to%20monitor%20domains%20and%20their%20data%20products.%20Many%0Amulti-million%20dollar%20organizations%20like%20Paypal%2C%20Netflix%2C%20and%20Zalando%20have%0Aalready%20transformed%20their%20data%20analysis%20pipelines%20based%20on%20this%20new%0Aarchitecture.%20In%20this%20decentralized%20architecture%20where%20data%20is%20locally%0Apreserved%20by%20each%20domain%20team%2C%20traditional%20centralized%20machine%20learning%20is%0Aincapable%20of%20conducting%20effective%20analysis%20across%20multiple%20domains%2C%20especially%0Afor%20security-sensitive%20organizations.%20To%20this%20end%2C%20we%20introduce%20a%20pioneering%0Aapproach%20that%20incorporates%20Federated%20Learning%20into%20Data%20Mesh.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20open-source%20applied%20work%20that%20represents%20a%0Acritical%20advancement%20toward%20the%20integration%20of%20federated%20learning%20methods%20into%0Athe%20Data%20Mesh%20paradigm%2C%20underscoring%20the%20promising%20prospects%20for%0Aprivacy-preserving%20and%20decentralized%20data%20analysis%20strategies%20within%20Data%20Mesh%0Aarchitecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17878v1&entry.124074799=Read"},
{"title": "DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions", "author": "Sammy Christen and Shreyas Hampali and Fadime Sener and Edoardo Remelli and Tomas Hodan and Eric Sauser and Shugao Ma and Bugra Tekin", "abstract": "  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n", "link": "http://arxiv.org/abs/2403.17827v1", "date": "2024-03-26", "relevancy": 1.8282, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6414}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5873}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiffH2O%3A%20Diffusion-Based%20Synthesis%20of%20Hand-Object%20Interactions%20from%0A%20%20Textual%20Descriptions&body=Title%3A%20DiffH2O%3A%20Diffusion-Based%20Synthesis%20of%20Hand-Object%20Interactions%20from%0A%20%20Textual%20Descriptions%0AAuthor%3A%20Sammy%20Christen%20and%20Shreyas%20Hampali%20and%20Fadime%20Sener%20and%20Edoardo%20Remelli%20and%20Tomas%20Hodan%20and%20Eric%20Sauser%20and%20Shugao%20Ma%20and%20Bugra%20Tekin%0AAbstract%3A%20%20%20Generating%20natural%20hand-object%20interactions%20in%203D%20is%20challenging%20as%20the%0Aresulting%20hand%20and%20object%20motions%20are%20expected%20to%20be%20physically%20plausible%20and%0Asemantically%20meaningful.%20Furthermore%2C%20generalization%20to%20unseen%20objects%20is%0Ahindered%20by%20the%20limited%20scale%20of%20available%20hand-object%20interaction%20datasets.%20We%0Apropose%20DiffH2O%2C%20a%20novel%20method%20to%20synthesize%20realistic%2C%20one%20or%20two-handed%0Aobject%20interactions%20from%20provided%20text%20prompts%20and%20geometry%20of%20the%20object.%20The%0Amethod%20introduces%20three%20techniques%20that%20enable%20effective%20learning%20from%20limited%0Adata.%20First%2C%20we%20decompose%20the%20task%20into%20a%20grasping%20stage%20and%20a%20text-based%0Ainteraction%20stage%20and%20use%20separate%20diffusion%20models%20for%20each.%20In%20the%20grasping%0Astage%2C%20the%20model%20only%20generates%20hand%20motions%2C%20whereas%20in%20the%20interaction%20phase%0Aboth%20hand%20and%20object%20poses%20are%20synthesized.%20Second%2C%20we%20propose%20a%20compact%0Arepresentation%20that%20tightly%20couples%20hand%20and%20object%20poses.%20Third%2C%20we%20propose%0Atwo%20different%20guidance%20schemes%20to%20allow%20more%20control%20of%20the%20generated%20motions%3A%0Agrasp%20guidance%20and%20detailed%20textual%20guidance.%20Grasp%20guidance%20takes%20a%20single%0Atarget%20grasping%20pose%20and%20guides%20the%20diffusion%20model%20to%20reach%20this%20grasp%20at%20the%0Aend%20of%20the%20grasping%20stage%2C%20which%20provides%20control%20over%20the%20grasping%20pose.%20Given%0Aa%20grasping%20motion%20from%20this%20stage%2C%20multiple%20different%20actions%20can%20be%20prompted%0Ain%20the%20interaction%20phase.%20For%20textual%20guidance%2C%20we%20contribute%20comprehensive%0Atext%20descriptions%20to%20the%20GRAB%20dataset%20and%20show%20that%20they%20enable%20our%20method%20to%0Ahave%20more%20fine-grained%20control%20over%20hand-object%20interactions.%20Our%20quantitative%0Aand%20qualitative%20evaluation%20demonstrates%20that%20the%20proposed%20method%20outperforms%0Abaseline%20methods%20and%20leads%20to%20natural%20hand-object%20motions.%20Moreover%2C%20we%0Ademonstrate%20the%20practicality%20of%20our%20framework%20by%20utilizing%20a%20hand%20pose%20estimate%0Afrom%20an%20off-the-shelf%20pose%20estimator%20for%20guidance%2C%20and%20then%20sampling%20multiple%0Adifferent%20actions%20in%20the%20interaction%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17827v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffH2O%3A%20Diffusion-Based%20Synthesis%20of%20Hand-Object%20Interactions%20from%0A%20%20Textual%20Descriptions&entry.906535625=Sammy%20Christen%20and%20Shreyas%20Hampali%20and%20Fadime%20Sener%20and%20Edoardo%20Remelli%20and%20Tomas%20Hodan%20and%20Eric%20Sauser%20and%20Shugao%20Ma%20and%20Bugra%20Tekin&entry.1292438233=%20%20Generating%20natural%20hand-object%20interactions%20in%203D%20is%20challenging%20as%20the%0Aresulting%20hand%20and%20object%20motions%20are%20expected%20to%20be%20physically%20plausible%20and%0Asemantically%20meaningful.%20Furthermore%2C%20generalization%20to%20unseen%20objects%20is%0Ahindered%20by%20the%20limited%20scale%20of%20available%20hand-object%20interaction%20datasets.%20We%0Apropose%20DiffH2O%2C%20a%20novel%20method%20to%20synthesize%20realistic%2C%20one%20or%20two-handed%0Aobject%20interactions%20from%20provided%20text%20prompts%20and%20geometry%20of%20the%20object.%20The%0Amethod%20introduces%20three%20techniques%20that%20enable%20effective%20learning%20from%20limited%0Adata.%20First%2C%20we%20decompose%20the%20task%20into%20a%20grasping%20stage%20and%20a%20text-based%0Ainteraction%20stage%20and%20use%20separate%20diffusion%20models%20for%20each.%20In%20the%20grasping%0Astage%2C%20the%20model%20only%20generates%20hand%20motions%2C%20whereas%20in%20the%20interaction%20phase%0Aboth%20hand%20and%20object%20poses%20are%20synthesized.%20Second%2C%20we%20propose%20a%20compact%0Arepresentation%20that%20tightly%20couples%20hand%20and%20object%20poses.%20Third%2C%20we%20propose%0Atwo%20different%20guidance%20schemes%20to%20allow%20more%20control%20of%20the%20generated%20motions%3A%0Agrasp%20guidance%20and%20detailed%20textual%20guidance.%20Grasp%20guidance%20takes%20a%20single%0Atarget%20grasping%20pose%20and%20guides%20the%20diffusion%20model%20to%20reach%20this%20grasp%20at%20the%0Aend%20of%20the%20grasping%20stage%2C%20which%20provides%20control%20over%20the%20grasping%20pose.%20Given%0Aa%20grasping%20motion%20from%20this%20stage%2C%20multiple%20different%20actions%20can%20be%20prompted%0Ain%20the%20interaction%20phase.%20For%20textual%20guidance%2C%20we%20contribute%20comprehensive%0Atext%20descriptions%20to%20the%20GRAB%20dataset%20and%20show%20that%20they%20enable%20our%20method%20to%0Ahave%20more%20fine-grained%20control%20over%20hand-object%20interactions.%20Our%20quantitative%0Aand%20qualitative%20evaluation%20demonstrates%20that%20the%20proposed%20method%20outperforms%0Abaseline%20methods%20and%20leads%20to%20natural%20hand-object%20motions.%20Moreover%2C%20we%0Ademonstrate%20the%20practicality%20of%20our%20framework%20by%20utilizing%20a%20hand%20pose%20estimate%0Afrom%20an%20off-the-shelf%20pose%20estimator%20for%20guidance%2C%20and%20then%20sampling%20multiple%0Adifferent%20actions%20in%20the%20interaction%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17827v1&entry.124074799=Read"},
{"title": "TractOracle: towards an anatomically-informed reward function for\n  RL-based tractography", "author": "Antoine Th\u00e9berge and Maxime Descoteaux and Pierre-Marc Jodoin", "abstract": "  Reinforcement learning (RL)-based tractography is a competitive alternative\nto machine learning and classical tractography algorithms due to its high\nanatomical accuracy obtained without the need for any annotated data. However,\nthe reward functions so far used to train RL agents do not encapsulate\nanatomical knowledge which causes agents to generate spurious false positives\ntracts. In this paper, we propose a new RL tractography system, TractOracle,\nwhich relies on a reward network trained for streamline classification. This\nnetwork is used both as a reward function during training as well as a mean for\nstopping the tracking process early and thus reduce the number of false\npositive streamlines. This makes our system a unique method that evaluates and\nreconstructs WM streamlines at the same time. We report an improvement of true\npositive ratios by almost 20\\% and a reduction of 3x of false positive ratios\non one dataset and an increase between 2x and 7x in the number true positive\nstreamlines on another dataset.\n", "link": "http://arxiv.org/abs/2403.17845v1", "date": "2024-03-26", "relevancy": 1.8271, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4633}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TractOracle%3A%20towards%20an%20anatomically-informed%20reward%20function%20for%0A%20%20RL-based%20tractography&body=Title%3A%20TractOracle%3A%20towards%20an%20anatomically-informed%20reward%20function%20for%0A%20%20RL-based%20tractography%0AAuthor%3A%20Antoine%20Th%C3%A9berge%20and%20Maxime%20Descoteaux%20and%20Pierre-Marc%20Jodoin%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29-based%20tractography%20is%20a%20competitive%20alternative%0Ato%20machine%20learning%20and%20classical%20tractography%20algorithms%20due%20to%20its%20high%0Aanatomical%20accuracy%20obtained%20without%20the%20need%20for%20any%20annotated%20data.%20However%2C%0Athe%20reward%20functions%20so%20far%20used%20to%20train%20RL%20agents%20do%20not%20encapsulate%0Aanatomical%20knowledge%20which%20causes%20agents%20to%20generate%20spurious%20false%20positives%0Atracts.%20In%20this%20paper%2C%20we%20propose%20a%20new%20RL%20tractography%20system%2C%20TractOracle%2C%0Awhich%20relies%20on%20a%20reward%20network%20trained%20for%20streamline%20classification.%20This%0Anetwork%20is%20used%20both%20as%20a%20reward%20function%20during%20training%20as%20well%20as%20a%20mean%20for%0Astopping%20the%20tracking%20process%20early%20and%20thus%20reduce%20the%20number%20of%20false%0Apositive%20streamlines.%20This%20makes%20our%20system%20a%20unique%20method%20that%20evaluates%20and%0Areconstructs%20WM%20streamlines%20at%20the%20same%20time.%20We%20report%20an%20improvement%20of%20true%0Apositive%20ratios%20by%20almost%2020%5C%25%20and%20a%20reduction%20of%203x%20of%20false%20positive%20ratios%0Aon%20one%20dataset%20and%20an%20increase%20between%202x%20and%207x%20in%20the%20number%20true%20positive%0Astreamlines%20on%20another%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17845v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TractOracle%3A%20towards%20an%20anatomically-informed%20reward%20function%20for%0A%20%20RL-based%20tractography&entry.906535625=Antoine%20Th%C3%A9berge%20and%20Maxime%20Descoteaux%20and%20Pierre-Marc%20Jodoin&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29-based%20tractography%20is%20a%20competitive%20alternative%0Ato%20machine%20learning%20and%20classical%20tractography%20algorithms%20due%20to%20its%20high%0Aanatomical%20accuracy%20obtained%20without%20the%20need%20for%20any%20annotated%20data.%20However%2C%0Athe%20reward%20functions%20so%20far%20used%20to%20train%20RL%20agents%20do%20not%20encapsulate%0Aanatomical%20knowledge%20which%20causes%20agents%20to%20generate%20spurious%20false%20positives%0Atracts.%20In%20this%20paper%2C%20we%20propose%20a%20new%20RL%20tractography%20system%2C%20TractOracle%2C%0Awhich%20relies%20on%20a%20reward%20network%20trained%20for%20streamline%20classification.%20This%0Anetwork%20is%20used%20both%20as%20a%20reward%20function%20during%20training%20as%20well%20as%20a%20mean%20for%0Astopping%20the%20tracking%20process%20early%20and%20thus%20reduce%20the%20number%20of%20false%0Apositive%20streamlines.%20This%20makes%20our%20system%20a%20unique%20method%20that%20evaluates%20and%0Areconstructs%20WM%20streamlines%20at%20the%20same%20time.%20We%20report%20an%20improvement%20of%20true%0Apositive%20ratios%20by%20almost%2020%5C%25%20and%20a%20reduction%20of%203x%20of%20false%20positive%20ratios%0Aon%20one%20dataset%20and%20an%20increase%20between%202x%20and%207x%20in%20the%20number%20true%20positive%0Astreamlines%20on%20another%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17845v1&entry.124074799=Read"},
{"title": "Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation", "author": "Abdelrhman Werby and Chenguang Huang and Martin B\u00fcchner and Abhinav Valada and Wolfram Burgard", "abstract": "  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n", "link": "http://arxiv.org/abs/2403.17846v1", "date": "2024-03-26", "relevancy": 1.7993, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6586}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6023}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5752}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Language-Grounded%20Robot%0A%20%20Navigation&body=Title%3A%20Hierarchical%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Language-Grounded%20Robot%0A%20%20Navigation%0AAuthor%3A%20Abdelrhman%20Werby%20and%20Chenguang%20Huang%20and%20Martin%20B%C3%BCchner%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20Recent%20open-vocabulary%20robot%20mapping%20methods%20enrich%20dense%20geometric%20maps%20with%0Apre-trained%20visual-language%20features.%20While%20these%20maps%20allow%20for%20the%20prediction%0Aof%20point-wise%20saliency%20maps%20when%20queried%20for%20a%20certain%20language%20concept%2C%0Alarge-scale%20environments%20and%20abstract%20queries%20beyond%20the%20object%20level%20still%0Apose%20a%20considerable%20hurdle%2C%20ultimately%20limiting%20language-grounded%20robotic%0Anavigation.%20In%20this%20work%2C%20we%20present%20HOV-SG%2C%20a%20hierarchical%20open-vocabulary%203D%0Ascene%20graph%20mapping%20approach%20for%20language-grounded%20robot%20navigation.%20Leveraging%0Aopen-vocabulary%20vision%20foundation%20models%2C%20we%20first%20obtain%20state-of-the-art%0Aopen-vocabulary%20segment-level%20maps%20in%203D%20and%20subsequently%20construct%20a%203D%20scene%0Agraph%20hierarchy%20consisting%20of%20floor%2C%20room%2C%20and%20object%20concepts%2C%20each%20enriched%0Awith%20open-vocabulary%20features.%20Our%20approach%20is%20able%20to%20represent%20multi-story%0Abuildings%20and%20allows%20robotic%20traversal%20of%20those%20using%20a%20cross-floor%20Voronoi%0Agraph.%20HOV-SG%20is%20evaluated%20on%20three%20distinct%20datasets%20and%20surpasses%20previous%0Abaselines%20in%20open-vocabulary%20semantic%20accuracy%20on%20the%20object%2C%20room%2C%20and%20floor%0Alevel%20while%20producing%20a%2075%25%20reduction%20in%20representation%20size%20compared%20to%20dense%0Aopen-vocabulary%20maps.%20In%20order%20to%20prove%20the%20efficacy%20and%20generalization%0Acapabilities%20of%20HOV-SG%2C%20we%20showcase%20successful%20long-horizon%0Alanguage-conditioned%20robot%20navigation%20within%20real-world%20multi-storage%0Aenvironments.%20We%20provide%20code%20and%20trial%20video%20data%20at%20http%3A//hovsg.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17846v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Language-Grounded%20Robot%0A%20%20Navigation&entry.906535625=Abdelrhman%20Werby%20and%20Chenguang%20Huang%20and%20Martin%20B%C3%BCchner%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard&entry.1292438233=%20%20Recent%20open-vocabulary%20robot%20mapping%20methods%20enrich%20dense%20geometric%20maps%20with%0Apre-trained%20visual-language%20features.%20While%20these%20maps%20allow%20for%20the%20prediction%0Aof%20point-wise%20saliency%20maps%20when%20queried%20for%20a%20certain%20language%20concept%2C%0Alarge-scale%20environments%20and%20abstract%20queries%20beyond%20the%20object%20level%20still%0Apose%20a%20considerable%20hurdle%2C%20ultimately%20limiting%20language-grounded%20robotic%0Anavigation.%20In%20this%20work%2C%20we%20present%20HOV-SG%2C%20a%20hierarchical%20open-vocabulary%203D%0Ascene%20graph%20mapping%20approach%20for%20language-grounded%20robot%20navigation.%20Leveraging%0Aopen-vocabulary%20vision%20foundation%20models%2C%20we%20first%20obtain%20state-of-the-art%0Aopen-vocabulary%20segment-level%20maps%20in%203D%20and%20subsequently%20construct%20a%203D%20scene%0Agraph%20hierarchy%20consisting%20of%20floor%2C%20room%2C%20and%20object%20concepts%2C%20each%20enriched%0Awith%20open-vocabulary%20features.%20Our%20approach%20is%20able%20to%20represent%20multi-story%0Abuildings%20and%20allows%20robotic%20traversal%20of%20those%20using%20a%20cross-floor%20Voronoi%0Agraph.%20HOV-SG%20is%20evaluated%20on%20three%20distinct%20datasets%20and%20surpasses%20previous%0Abaselines%20in%20open-vocabulary%20semantic%20accuracy%20on%20the%20object%2C%20room%2C%20and%20floor%0Alevel%20while%20producing%20a%2075%25%20reduction%20in%20representation%20size%20compared%20to%20dense%0Aopen-vocabulary%20maps.%20In%20order%20to%20prove%20the%20efficacy%20and%20generalization%0Acapabilities%20of%20HOV-SG%2C%20we%20showcase%20successful%20long-horizon%0Alanguage-conditioned%20robot%20navigation%20within%20real-world%20multi-storage%0Aenvironments.%20We%20provide%20code%20and%20trial%20video%20data%20at%20http%3A//hovsg.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17846v1&entry.124074799=Read"},
{"title": "Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative\n  Matrix Factorization", "author": "Ryan Barron and Maksim E. Eren and Manish Bhattarai and Selma Wanna and Nicholas Solovyev and Kim Rasmussen and Boian S. Alexandrov and Charles Nicholas and Cynthia Matuszek", "abstract": "  Much of human knowledge in cybersecurity is encapsulated within the\never-growing volume of scientific papers. As this textual data continues to\nexpand, the importance of document organization methods becomes increasingly\ncrucial for extracting actionable insights hidden within large text datasets.\nKnowledge Graphs (KGs) serve as a means to store factual information in a\nstructured manner, providing explicit, interpretable knowledge that includes\ndomain-specific information from the cybersecurity scientific literature. One\nof the challenges in constructing a KG from scientific literature is the\nextraction of ontology from unstructured text. In this paper, we address this\ntopic and introduce a method for building a multi-modal KG by extracting\nstructured ontology from scientific papers. We demonstrate this concept in the\ncybersecurity domain. One modality of the KG represents observable information\nfrom the papers, such as the categories in which they were published or the\nauthors. The second modality uncovers latent (hidden) patterns of text\nextracted through hierarchical and semantic non-negative matrix factorization\n(NMF), such as named entities, topics or clusters, and keywords. We illustrate\nthis concept by consolidating more than two million scientific papers uploaded\nto arXiv into the cyber-domain, using hierarchical and semantic NMF, and by\nbuilding a cyber-domain-specific KG.\n", "link": "http://arxiv.org/abs/2403.16222v2", "date": "2024-03-26", "relevancy": 1.7937, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4546}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4509}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cyber-Security%20Knowledge%20Graph%20Generation%20by%20Hierarchical%20Nonnegative%0A%20%20Matrix%20Factorization&body=Title%3A%20Cyber-Security%20Knowledge%20Graph%20Generation%20by%20Hierarchical%20Nonnegative%0A%20%20Matrix%20Factorization%0AAuthor%3A%20Ryan%20Barron%20and%20Maksim%20E.%20Eren%20and%20Manish%20Bhattarai%20and%20Selma%20Wanna%20and%20Nicholas%20Solovyev%20and%20Kim%20Rasmussen%20and%20Boian%20S.%20Alexandrov%20and%20Charles%20Nicholas%20and%20Cynthia%20Matuszek%0AAbstract%3A%20%20%20Much%20of%20human%20knowledge%20in%20cybersecurity%20is%20encapsulated%20within%20the%0Aever-growing%20volume%20of%20scientific%20papers.%20As%20this%20textual%20data%20continues%20to%0Aexpand%2C%20the%20importance%20of%20document%20organization%20methods%20becomes%20increasingly%0Acrucial%20for%20extracting%20actionable%20insights%20hidden%20within%20large%20text%20datasets.%0AKnowledge%20Graphs%20%28KGs%29%20serve%20as%20a%20means%20to%20store%20factual%20information%20in%20a%0Astructured%20manner%2C%20providing%20explicit%2C%20interpretable%20knowledge%20that%20includes%0Adomain-specific%20information%20from%20the%20cybersecurity%20scientific%20literature.%20One%0Aof%20the%20challenges%20in%20constructing%20a%20KG%20from%20scientific%20literature%20is%20the%0Aextraction%20of%20ontology%20from%20unstructured%20text.%20In%20this%20paper%2C%20we%20address%20this%0Atopic%20and%20introduce%20a%20method%20for%20building%20a%20multi-modal%20KG%20by%20extracting%0Astructured%20ontology%20from%20scientific%20papers.%20We%20demonstrate%20this%20concept%20in%20the%0Acybersecurity%20domain.%20One%20modality%20of%20the%20KG%20represents%20observable%20information%0Afrom%20the%20papers%2C%20such%20as%20the%20categories%20in%20which%20they%20were%20published%20or%20the%0Aauthors.%20The%20second%20modality%20uncovers%20latent%20%28hidden%29%20patterns%20of%20text%0Aextracted%20through%20hierarchical%20and%20semantic%20non-negative%20matrix%20factorization%0A%28NMF%29%2C%20such%20as%20named%20entities%2C%20topics%20or%20clusters%2C%20and%20keywords.%20We%20illustrate%0Athis%20concept%20by%20consolidating%20more%20than%20two%20million%20scientific%20papers%20uploaded%0Ato%20arXiv%20into%20the%20cyber-domain%2C%20using%20hierarchical%20and%20semantic%20NMF%2C%20and%20by%0Abuilding%20a%20cyber-domain-specific%20KG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16222v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cyber-Security%20Knowledge%20Graph%20Generation%20by%20Hierarchical%20Nonnegative%0A%20%20Matrix%20Factorization&entry.906535625=Ryan%20Barron%20and%20Maksim%20E.%20Eren%20and%20Manish%20Bhattarai%20and%20Selma%20Wanna%20and%20Nicholas%20Solovyev%20and%20Kim%20Rasmussen%20and%20Boian%20S.%20Alexandrov%20and%20Charles%20Nicholas%20and%20Cynthia%20Matuszek&entry.1292438233=%20%20Much%20of%20human%20knowledge%20in%20cybersecurity%20is%20encapsulated%20within%20the%0Aever-growing%20volume%20of%20scientific%20papers.%20As%20this%20textual%20data%20continues%20to%0Aexpand%2C%20the%20importance%20of%20document%20organization%20methods%20becomes%20increasingly%0Acrucial%20for%20extracting%20actionable%20insights%20hidden%20within%20large%20text%20datasets.%0AKnowledge%20Graphs%20%28KGs%29%20serve%20as%20a%20means%20to%20store%20factual%20information%20in%20a%0Astructured%20manner%2C%20providing%20explicit%2C%20interpretable%20knowledge%20that%20includes%0Adomain-specific%20information%20from%20the%20cybersecurity%20scientific%20literature.%20One%0Aof%20the%20challenges%20in%20constructing%20a%20KG%20from%20scientific%20literature%20is%20the%0Aextraction%20of%20ontology%20from%20unstructured%20text.%20In%20this%20paper%2C%20we%20address%20this%0Atopic%20and%20introduce%20a%20method%20for%20building%20a%20multi-modal%20KG%20by%20extracting%0Astructured%20ontology%20from%20scientific%20papers.%20We%20demonstrate%20this%20concept%20in%20the%0Acybersecurity%20domain.%20One%20modality%20of%20the%20KG%20represents%20observable%20information%0Afrom%20the%20papers%2C%20such%20as%20the%20categories%20in%20which%20they%20were%20published%20or%20the%0Aauthors.%20The%20second%20modality%20uncovers%20latent%20%28hidden%29%20patterns%20of%20text%0Aextracted%20through%20hierarchical%20and%20semantic%20non-negative%20matrix%20factorization%0A%28NMF%29%2C%20such%20as%20named%20entities%2C%20topics%20or%20clusters%2C%20and%20keywords.%20We%20illustrate%0Athis%20concept%20by%20consolidating%20more%20than%20two%20million%20scientific%20papers%20uploaded%0Ato%20arXiv%20into%20the%20cyber-domain%2C%20using%20hierarchical%20and%20semantic%20NMF%2C%20and%20by%0Abuilding%20a%20cyber-domain-specific%20KG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16222v2&entry.124074799=Read"},
{"title": "Towards Explaining Hypercomplex Neural Networks", "author": "Eleonora Lopez and Eleonora Grassucci and Debora Capriotti and Danilo Comminiello", "abstract": "  Hypercomplex neural networks are gaining increasing interest in the deep\nlearning community. The attention directed towards hypercomplex models\noriginates from several aspects, spanning from purely theoretical and\nmathematical characteristics to the practical advantage of lightweight models\nover conventional networks, and their unique properties to capture both global\nand local relations. In particular, a branch of these architectures,\nparameterized hypercomplex neural networks (PHNNs), has also gained popularity\ndue to their versatility across a multitude of application domains.\nNonetheless, only few attempts have been made to explain or interpret their\nintricacies. In this paper, we propose inherently interpretable PHNNs and\nquaternion-like networks, thus without the need for any post-hoc method. To\nachieve this, we define a type of cosine-similarity transform within the\nparameterized hypercomplex domain. This PHB-cos transform induces weight\nalignment with relevant input features and allows to reduce the model into a\nsingle linear transform, rendering it directly interpretable. In this work, we\nstart to draw insights into how this unique branch of neural models operates.\nWe observe that hypercomplex networks exhibit a tendency to concentrate on the\nshape around the main object of interest, in addition to the shape of the\nobject itself. We provide a thorough analysis, studying single neurons of\ndifferent layers and comparing them against how real-valued networks learn. The\ncode of the paper is available at https://github.com/ispamm/HxAI.\n", "link": "http://arxiv.org/abs/2403.17929v1", "date": "2024-03-26", "relevancy": 1.7887, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4697}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.457}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4284}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Explaining%20Hypercomplex%20Neural%20Networks&body=Title%3A%20Towards%20Explaining%20Hypercomplex%20Neural%20Networks%0AAuthor%3A%20Eleonora%20Lopez%20and%20Eleonora%20Grassucci%20and%20Debora%20Capriotti%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20Hypercomplex%20neural%20networks%20are%20gaining%20increasing%20interest%20in%20the%20deep%0Alearning%20community.%20The%20attention%20directed%20towards%20hypercomplex%20models%0Aoriginates%20from%20several%20aspects%2C%20spanning%20from%20purely%20theoretical%20and%0Amathematical%20characteristics%20to%20the%20practical%20advantage%20of%20lightweight%20models%0Aover%20conventional%20networks%2C%20and%20their%20unique%20properties%20to%20capture%20both%20global%0Aand%20local%20relations.%20In%20particular%2C%20a%20branch%20of%20these%20architectures%2C%0Aparameterized%20hypercomplex%20neural%20networks%20%28PHNNs%29%2C%20has%20also%20gained%20popularity%0Adue%20to%20their%20versatility%20across%20a%20multitude%20of%20application%20domains.%0ANonetheless%2C%20only%20few%20attempts%20have%20been%20made%20to%20explain%20or%20interpret%20their%0Aintricacies.%20In%20this%20paper%2C%20we%20propose%20inherently%20interpretable%20PHNNs%20and%0Aquaternion-like%20networks%2C%20thus%20without%20the%20need%20for%20any%20post-hoc%20method.%20To%0Aachieve%20this%2C%20we%20define%20a%20type%20of%20cosine-similarity%20transform%20within%20the%0Aparameterized%20hypercomplex%20domain.%20This%20PHB-cos%20transform%20induces%20weight%0Aalignment%20with%20relevant%20input%20features%20and%20allows%20to%20reduce%20the%20model%20into%20a%0Asingle%20linear%20transform%2C%20rendering%20it%20directly%20interpretable.%20In%20this%20work%2C%20we%0Astart%20to%20draw%20insights%20into%20how%20this%20unique%20branch%20of%20neural%20models%20operates.%0AWe%20observe%20that%20hypercomplex%20networks%20exhibit%20a%20tendency%20to%20concentrate%20on%20the%0Ashape%20around%20the%20main%20object%20of%20interest%2C%20in%20addition%20to%20the%20shape%20of%20the%0Aobject%20itself.%20We%20provide%20a%20thorough%20analysis%2C%20studying%20single%20neurons%20of%0Adifferent%20layers%20and%20comparing%20them%20against%20how%20real-valued%20networks%20learn.%20The%0Acode%20of%20the%20paper%20is%20available%20at%20https%3A//github.com/ispamm/HxAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17929v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Explaining%20Hypercomplex%20Neural%20Networks&entry.906535625=Eleonora%20Lopez%20and%20Eleonora%20Grassucci%20and%20Debora%20Capriotti%20and%20Danilo%20Comminiello&entry.1292438233=%20%20Hypercomplex%20neural%20networks%20are%20gaining%20increasing%20interest%20in%20the%20deep%0Alearning%20community.%20The%20attention%20directed%20towards%20hypercomplex%20models%0Aoriginates%20from%20several%20aspects%2C%20spanning%20from%20purely%20theoretical%20and%0Amathematical%20characteristics%20to%20the%20practical%20advantage%20of%20lightweight%20models%0Aover%20conventional%20networks%2C%20and%20their%20unique%20properties%20to%20capture%20both%20global%0Aand%20local%20relations.%20In%20particular%2C%20a%20branch%20of%20these%20architectures%2C%0Aparameterized%20hypercomplex%20neural%20networks%20%28PHNNs%29%2C%20has%20also%20gained%20popularity%0Adue%20to%20their%20versatility%20across%20a%20multitude%20of%20application%20domains.%0ANonetheless%2C%20only%20few%20attempts%20have%20been%20made%20to%20explain%20or%20interpret%20their%0Aintricacies.%20In%20this%20paper%2C%20we%20propose%20inherently%20interpretable%20PHNNs%20and%0Aquaternion-like%20networks%2C%20thus%20without%20the%20need%20for%20any%20post-hoc%20method.%20To%0Aachieve%20this%2C%20we%20define%20a%20type%20of%20cosine-similarity%20transform%20within%20the%0Aparameterized%20hypercomplex%20domain.%20This%20PHB-cos%20transform%20induces%20weight%0Aalignment%20with%20relevant%20input%20features%20and%20allows%20to%20reduce%20the%20model%20into%20a%0Asingle%20linear%20transform%2C%20rendering%20it%20directly%20interpretable.%20In%20this%20work%2C%20we%0Astart%20to%20draw%20insights%20into%20how%20this%20unique%20branch%20of%20neural%20models%20operates.%0AWe%20observe%20that%20hypercomplex%20networks%20exhibit%20a%20tendency%20to%20concentrate%20on%20the%0Ashape%20around%20the%20main%20object%20of%20interest%2C%20in%20addition%20to%20the%20shape%20of%20the%0Aobject%20itself.%20We%20provide%20a%20thorough%20analysis%2C%20studying%20single%20neurons%20of%0Adifferent%20layers%20and%20comparing%20them%20against%20how%20real-valued%20networks%20learn.%20The%0Acode%20of%20the%20paper%20is%20available%20at%20https%3A//github.com/ispamm/HxAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17929v1&entry.124074799=Read"},
{"title": "TC4D: Trajectory-Conditioned Text-to-4D Generation", "author": "Sherwin Bahmani and Xian Liu and Yifan Wang and Ivan Skorokhodov and Victor Rong and Ziwei Liu and Xihui Liu and Jeong Joon Park and Sergey Tulyakov and Gordon Wetzstein and Andrea Tagliasacchi and David B. Lindell", "abstract": "  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n", "link": "http://arxiv.org/abs/2403.17920v1", "date": "2024-03-26", "relevancy": 1.7543, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6448}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5746}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5501}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TC4D%3A%20Trajectory-Conditioned%20Text-to-4D%20Generation&body=Title%3A%20TC4D%3A%20Trajectory-Conditioned%20Text-to-4D%20Generation%0AAuthor%3A%20Sherwin%20Bahmani%20and%20Xian%20Liu%20and%20Yifan%20Wang%20and%20Ivan%20Skorokhodov%20and%20Victor%20Rong%20and%20Ziwei%20Liu%20and%20Xihui%20Liu%20and%20Jeong%20Joon%20Park%20and%20Sergey%20Tulyakov%20and%20Gordon%20Wetzstein%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%0AAbstract%3A%20%20%20Recent%20techniques%20for%20text-to-4D%20generation%20synthesize%20dynamic%203D%20scenes%0Ausing%20supervision%20from%20pre-trained%20text-to-video%20models.%20However%2C%20existing%0Arepresentations%20for%20motion%2C%20such%20as%20deformation%20models%20or%20time-dependent%20neural%0Arepresentations%2C%20are%20limited%20in%20the%20amount%20of%20motion%20they%20can%20generate-they%0Acannot%20synthesize%20motion%20extending%20far%20beyond%20the%20bounding%20box%20used%20for%20volume%0Arendering.%20The%20lack%20of%20a%20more%20flexible%20motion%20model%20contributes%20to%20the%20gap%20in%0Arealism%20between%204D%20generation%20methods%20and%20recent%2C%20near-photorealistic%20video%0Ageneration%20models.%20Here%2C%20we%20propose%20TC4D%3A%20trajectory-conditioned%20text-to-4D%0Ageneration%2C%20which%20factors%20motion%20into%20global%20and%20local%20components.%20We%20represent%0Athe%20global%20motion%20of%20a%20scene%27s%20bounding%20box%20using%20rigid%20transformation%20along%20a%0Atrajectory%20parameterized%20by%20a%20spline.%20We%20learn%20local%20deformations%20that%20conform%0Ato%20the%20global%20trajectory%20using%20supervision%20from%20a%20text-to-video%20model.%20Our%0Aapproach%20enables%20the%20synthesis%20of%20scenes%20animated%20along%20arbitrary%20trajectories%2C%0Acompositional%20scene%20generation%2C%20and%20significant%20improvements%20to%20the%20realism%20and%0Aamount%20of%20generated%20motion%2C%20which%20we%20evaluate%20qualitatively%20and%20through%20a%20user%0Astudy.%20Video%20results%20can%20be%20viewed%20on%20our%20website%3A%0Ahttps%3A//sherwinbahmani.github.io/tc4d.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17920v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TC4D%3A%20Trajectory-Conditioned%20Text-to-4D%20Generation&entry.906535625=Sherwin%20Bahmani%20and%20Xian%20Liu%20and%20Yifan%20Wang%20and%20Ivan%20Skorokhodov%20and%20Victor%20Rong%20and%20Ziwei%20Liu%20and%20Xihui%20Liu%20and%20Jeong%20Joon%20Park%20and%20Sergey%20Tulyakov%20and%20Gordon%20Wetzstein%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell&entry.1292438233=%20%20Recent%20techniques%20for%20text-to-4D%20generation%20synthesize%20dynamic%203D%20scenes%0Ausing%20supervision%20from%20pre-trained%20text-to-video%20models.%20However%2C%20existing%0Arepresentations%20for%20motion%2C%20such%20as%20deformation%20models%20or%20time-dependent%20neural%0Arepresentations%2C%20are%20limited%20in%20the%20amount%20of%20motion%20they%20can%20generate-they%0Acannot%20synthesize%20motion%20extending%20far%20beyond%20the%20bounding%20box%20used%20for%20volume%0Arendering.%20The%20lack%20of%20a%20more%20flexible%20motion%20model%20contributes%20to%20the%20gap%20in%0Arealism%20between%204D%20generation%20methods%20and%20recent%2C%20near-photorealistic%20video%0Ageneration%20models.%20Here%2C%20we%20propose%20TC4D%3A%20trajectory-conditioned%20text-to-4D%0Ageneration%2C%20which%20factors%20motion%20into%20global%20and%20local%20components.%20We%20represent%0Athe%20global%20motion%20of%20a%20scene%27s%20bounding%20box%20using%20rigid%20transformation%20along%20a%0Atrajectory%20parameterized%20by%20a%20spline.%20We%20learn%20local%20deformations%20that%20conform%0Ato%20the%20global%20trajectory%20using%20supervision%20from%20a%20text-to-video%20model.%20Our%0Aapproach%20enables%20the%20synthesis%20of%20scenes%20animated%20along%20arbitrary%20trajectories%2C%0Acompositional%20scene%20generation%2C%20and%20significant%20improvements%20to%20the%20realism%20and%0Aamount%20of%20generated%20motion%2C%20which%20we%20evaluate%20qualitatively%20and%20through%20a%20user%0Astudy.%20Video%20results%20can%20be%20viewed%20on%20our%20website%3A%0Ahttps%3A//sherwinbahmani.github.io/tc4d.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17920v1&entry.124074799=Read"},
{"title": "Boosting Diffusion Models with Moving Average Sampling in Frequency\n  Domain", "author": "Yurui Qian and Qi Cai and Yingwei Pan and Yehao Li and Ting Yao and Qibin Sun and Tao Mei", "abstract": "  Diffusion models have recently brought a powerful revolution in image\ngeneration. Despite showing impressive generative capabilities, most of these\nmodels rely on the current sample to denoise the next one, possibly resulting\nin denoising instability. In this paper, we reinterpret the iterative denoising\nprocess as model optimization and leverage a moving average mechanism to\nensemble all the prior samples. Instead of simply applying moving average to\nthe denoised samples at different timesteps, we first map the denoised samples\nto data space and then perform moving average to avoid distribution shift\nacross timesteps. In view that diffusion models evolve the recovery from\nlow-frequency components to high-frequency details, we further decompose the\nsamples into different frequency components and execute moving average\nseparately on each component. We name the complete approach \"Moving Average\nSampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into\nmainstream pre-trained diffusion models and sampling schedules. Extensive\nexperiments on both unconditional and conditional diffusion models demonstrate\nthat our MASF leads to superior performances compared to the baselines, with\nalmost negligible additional complexity cost.\n", "link": "http://arxiv.org/abs/2403.17870v1", "date": "2024-03-26", "relevancy": 1.7525, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6183}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5755}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5718}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Boosting%20Diffusion%20Models%20with%20Moving%20Average%20Sampling%20in%20Frequency%0A%20%20Domain&body=Title%3A%20Boosting%20Diffusion%20Models%20with%20Moving%20Average%20Sampling%20in%20Frequency%0A%20%20Domain%0AAuthor%3A%20Yurui%20Qian%20and%20Qi%20Cai%20and%20Yingwei%20Pan%20and%20Yehao%20Li%20and%20Ting%20Yao%20and%20Qibin%20Sun%20and%20Tao%20Mei%0AAbstract%3A%20%20%20Diffusion%20models%20have%20recently%20brought%20a%20powerful%20revolution%20in%20image%0Ageneration.%20Despite%20showing%20impressive%20generative%20capabilities%2C%20most%20of%20these%0Amodels%20rely%20on%20the%20current%20sample%20to%20denoise%20the%20next%20one%2C%20possibly%20resulting%0Ain%20denoising%20instability.%20In%20this%20paper%2C%20we%20reinterpret%20the%20iterative%20denoising%0Aprocess%20as%20model%20optimization%20and%20leverage%20a%20moving%20average%20mechanism%20to%0Aensemble%20all%20the%20prior%20samples.%20Instead%20of%20simply%20applying%20moving%20average%20to%0Athe%20denoised%20samples%20at%20different%20timesteps%2C%20we%20first%20map%20the%20denoised%20samples%0Ato%20data%20space%20and%20then%20perform%20moving%20average%20to%20avoid%20distribution%20shift%0Aacross%20timesteps.%20In%20view%20that%20diffusion%20models%20evolve%20the%20recovery%20from%0Alow-frequency%20components%20to%20high-frequency%20details%2C%20we%20further%20decompose%20the%0Asamples%20into%20different%20frequency%20components%20and%20execute%20moving%20average%0Aseparately%20on%20each%20component.%20We%20name%20the%20complete%20approach%20%22Moving%20Average%0ASampling%20in%20Frequency%20domain%20%28MASF%29%22.%20MASF%20could%20be%20seamlessly%20integrated%20into%0Amainstream%20pre-trained%20diffusion%20models%20and%20sampling%20schedules.%20Extensive%0Aexperiments%20on%20both%20unconditional%20and%20conditional%20diffusion%20models%20demonstrate%0Athat%20our%20MASF%20leads%20to%20superior%20performances%20compared%20to%20the%20baselines%2C%20with%0Aalmost%20negligible%20additional%20complexity%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17870v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Diffusion%20Models%20with%20Moving%20Average%20Sampling%20in%20Frequency%0A%20%20Domain&entry.906535625=Yurui%20Qian%20and%20Qi%20Cai%20and%20Yingwei%20Pan%20and%20Yehao%20Li%20and%20Ting%20Yao%20and%20Qibin%20Sun%20and%20Tao%20Mei&entry.1292438233=%20%20Diffusion%20models%20have%20recently%20brought%20a%20powerful%20revolution%20in%20image%0Ageneration.%20Despite%20showing%20impressive%20generative%20capabilities%2C%20most%20of%20these%0Amodels%20rely%20on%20the%20current%20sample%20to%20denoise%20the%20next%20one%2C%20possibly%20resulting%0Ain%20denoising%20instability.%20In%20this%20paper%2C%20we%20reinterpret%20the%20iterative%20denoising%0Aprocess%20as%20model%20optimization%20and%20leverage%20a%20moving%20average%20mechanism%20to%0Aensemble%20all%20the%20prior%20samples.%20Instead%20of%20simply%20applying%20moving%20average%20to%0Athe%20denoised%20samples%20at%20different%20timesteps%2C%20we%20first%20map%20the%20denoised%20samples%0Ato%20data%20space%20and%20then%20perform%20moving%20average%20to%20avoid%20distribution%20shift%0Aacross%20timesteps.%20In%20view%20that%20diffusion%20models%20evolve%20the%20recovery%20from%0Alow-frequency%20components%20to%20high-frequency%20details%2C%20we%20further%20decompose%20the%0Asamples%20into%20different%20frequency%20components%20and%20execute%20moving%20average%0Aseparately%20on%20each%20component.%20We%20name%20the%20complete%20approach%20%22Moving%20Average%0ASampling%20in%20Frequency%20domain%20%28MASF%29%22.%20MASF%20could%20be%20seamlessly%20integrated%20into%0Amainstream%20pre-trained%20diffusion%20models%20and%20sampling%20schedules.%20Extensive%0Aexperiments%20on%20both%20unconditional%20and%20conditional%20diffusion%20models%20demonstrate%0Athat%20our%20MASF%20leads%20to%20superior%20performances%20compared%20to%20the%20baselines%2C%20with%0Aalmost%20negligible%20additional%20complexity%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17870v1&entry.124074799=Read"},
{"title": "GenesisTex: Adapting Image Denoising Diffusion to Texture Space", "author": "Chenjian Gao and Boyan Jiang and Xinghui Li and Yingpeng Zhang and Qian Yu", "abstract": "  We present GenesisTex, a novel method for synthesizing textures for 3D\ngeometries from text descriptions. GenesisTex adapts the pretrained image\ndiffusion model to texture space by texture space sampling. Specifically, we\nmaintain a latent texture map for each viewpoint, which is updated with\npredicted noise on the rendering of the corresponding viewpoint. The sampled\nlatent texture maps are then decoded into a final texture map. During the\nsampling process, we focus on both global and local consistency across multiple\nviewpoints: global consistency is achieved through the integration of style\nconsistency mechanisms within the noise prediction network, and low-level\nconsistency is achieved by dynamically aligning latent textures. Finally, we\napply reference-based inpainting and img2img on denser views for texture\nrefinement. Our approach overcomes the limitations of slow optimization in\ndistillation-based methods and instability in inpainting-based methods.\nExperiments on meshes from various sources demonstrate that our method\nsurpasses the baseline methods quantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2403.17782v1", "date": "2024-03-26", "relevancy": 1.7254, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5919}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5779}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5673}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GenesisTex%3A%20Adapting%20Image%20Denoising%20Diffusion%20to%20Texture%20Space&body=Title%3A%20GenesisTex%3A%20Adapting%20Image%20Denoising%20Diffusion%20to%20Texture%20Space%0AAuthor%3A%20Chenjian%20Gao%20and%20Boyan%20Jiang%20and%20Xinghui%20Li%20and%20Yingpeng%20Zhang%20and%20Qian%20Yu%0AAbstract%3A%20%20%20We%20present%20GenesisTex%2C%20a%20novel%20method%20for%20synthesizing%20textures%20for%203D%0Ageometries%20from%20text%20descriptions.%20GenesisTex%20adapts%20the%20pretrained%20image%0Adiffusion%20model%20to%20texture%20space%20by%20texture%20space%20sampling.%20Specifically%2C%20we%0Amaintain%20a%20latent%20texture%20map%20for%20each%20viewpoint%2C%20which%20is%20updated%20with%0Apredicted%20noise%20on%20the%20rendering%20of%20the%20corresponding%20viewpoint.%20The%20sampled%0Alatent%20texture%20maps%20are%20then%20decoded%20into%20a%20final%20texture%20map.%20During%20the%0Asampling%20process%2C%20we%20focus%20on%20both%20global%20and%20local%20consistency%20across%20multiple%0Aviewpoints%3A%20global%20consistency%20is%20achieved%20through%20the%20integration%20of%20style%0Aconsistency%20mechanisms%20within%20the%20noise%20prediction%20network%2C%20and%20low-level%0Aconsistency%20is%20achieved%20by%20dynamically%20aligning%20latent%20textures.%20Finally%2C%20we%0Aapply%20reference-based%20inpainting%20and%20img2img%20on%20denser%20views%20for%20texture%0Arefinement.%20Our%20approach%20overcomes%20the%20limitations%20of%20slow%20optimization%20in%0Adistillation-based%20methods%20and%20instability%20in%20inpainting-based%20methods.%0AExperiments%20on%20meshes%20from%20various%20sources%20demonstrate%20that%20our%20method%0Asurpasses%20the%20baseline%20methods%20quantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17782v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenesisTex%3A%20Adapting%20Image%20Denoising%20Diffusion%20to%20Texture%20Space&entry.906535625=Chenjian%20Gao%20and%20Boyan%20Jiang%20and%20Xinghui%20Li%20and%20Yingpeng%20Zhang%20and%20Qian%20Yu&entry.1292438233=%20%20We%20present%20GenesisTex%2C%20a%20novel%20method%20for%20synthesizing%20textures%20for%203D%0Ageometries%20from%20text%20descriptions.%20GenesisTex%20adapts%20the%20pretrained%20image%0Adiffusion%20model%20to%20texture%20space%20by%20texture%20space%20sampling.%20Specifically%2C%20we%0Amaintain%20a%20latent%20texture%20map%20for%20each%20viewpoint%2C%20which%20is%20updated%20with%0Apredicted%20noise%20on%20the%20rendering%20of%20the%20corresponding%20viewpoint.%20The%20sampled%0Alatent%20texture%20maps%20are%20then%20decoded%20into%20a%20final%20texture%20map.%20During%20the%0Asampling%20process%2C%20we%20focus%20on%20both%20global%20and%20local%20consistency%20across%20multiple%0Aviewpoints%3A%20global%20consistency%20is%20achieved%20through%20the%20integration%20of%20style%0Aconsistency%20mechanisms%20within%20the%20noise%20prediction%20network%2C%20and%20low-level%0Aconsistency%20is%20achieved%20by%20dynamically%20aligning%20latent%20textures.%20Finally%2C%20we%0Aapply%20reference-based%20inpainting%20and%20img2img%20on%20denser%20views%20for%20texture%0Arefinement.%20Our%20approach%20overcomes%20the%20limitations%20of%20slow%20optimization%20in%0Adistillation-based%20methods%20and%20instability%20in%20inpainting-based%20methods.%0AExperiments%20on%20meshes%20from%20various%20sources%20demonstrate%20that%20our%20method%0Asurpasses%20the%20baseline%20methods%20quantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17782v1&entry.124074799=Read"},
{"title": "Borrowing Treasures from Neighbors: In-Context Learning for Multimodal\n  Learning with Missing Modalities and Data Scarcity", "author": "Zhuo Zhi and Ziquan Liu and Moe Elbadawi and Adam Daneshmend and Mine Orlu and Abdul Basit and Andreas Demosthenous and Miguel Rodrigues", "abstract": "  Multimodal machine learning with missing modalities is an increasingly\nrelevant challenge arising in various applications such as healthcare. This\npaper extends the current research into missing modalities to the low-data\nregime, i.e., a downstream task has both missing modalities and limited sample\nsize issues. This problem setting is particularly challenging and also\npractical as it is often expensive to get full-modality data and sufficient\nannotated training samples. We propose to use retrieval-augmented in-context\nlearning to address these two crucial issues by unleashing the potential of a\ntransformer's in-context learning ability. Diverging from existing methods,\nwhich primarily belong to the parametric paradigm and often require sufficient\ntraining samples, our work exploits the value of the available full-modality\ndata, offering a novel perspective on resolving the challenge. The proposed\ndata-dependent framework exhibits a higher degree of sample efficiency and is\nempirically demonstrated to enhance the classification model's performance on\nboth full- and missing-modality data in the low-data regime across various\nmultimodal learning tasks. When only 1% of the training data are available, our\nproposed method demonstrates an average improvement of 6.1% over a recent\nstrong baseline across various datasets and missing states. Notably, our method\nalso reduces the performance gap between full-modality and missing-modality\ndata compared with the baseline.\n", "link": "http://arxiv.org/abs/2403.09428v2", "date": "2024-03-26", "relevancy": 1.7224, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5929}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5656}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5358}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Borrowing%20Treasures%20from%20Neighbors%3A%20In-Context%20Learning%20for%20Multimodal%0A%20%20Learning%20with%20Missing%20Modalities%20and%20Data%20Scarcity&body=Title%3A%20Borrowing%20Treasures%20from%20Neighbors%3A%20In-Context%20Learning%20for%20Multimodal%0A%20%20Learning%20with%20Missing%20Modalities%20and%20Data%20Scarcity%0AAuthor%3A%20Zhuo%20Zhi%20and%20Ziquan%20Liu%20and%20Moe%20Elbadawi%20and%20Adam%20Daneshmend%20and%20Mine%20Orlu%20and%20Abdul%20Basit%20and%20Andreas%20Demosthenous%20and%20Miguel%20Rodrigues%0AAbstract%3A%20%20%20Multimodal%20machine%20learning%20with%20missing%20modalities%20is%20an%20increasingly%0Arelevant%20challenge%20arising%20in%20various%20applications%20such%20as%20healthcare.%20This%0Apaper%20extends%20the%20current%20research%20into%20missing%20modalities%20to%20the%20low-data%0Aregime%2C%20i.e.%2C%20a%20downstream%20task%20has%20both%20missing%20modalities%20and%20limited%20sample%0Asize%20issues.%20This%20problem%20setting%20is%20particularly%20challenging%20and%20also%0Apractical%20as%20it%20is%20often%20expensive%20to%20get%20full-modality%20data%20and%20sufficient%0Aannotated%20training%20samples.%20We%20propose%20to%20use%20retrieval-augmented%20in-context%0Alearning%20to%20address%20these%20two%20crucial%20issues%20by%20unleashing%20the%20potential%20of%20a%0Atransformer%27s%20in-context%20learning%20ability.%20Diverging%20from%20existing%20methods%2C%0Awhich%20primarily%20belong%20to%20the%20parametric%20paradigm%20and%20often%20require%20sufficient%0Atraining%20samples%2C%20our%20work%20exploits%20the%20value%20of%20the%20available%20full-modality%0Adata%2C%20offering%20a%20novel%20perspective%20on%20resolving%20the%20challenge.%20The%20proposed%0Adata-dependent%20framework%20exhibits%20a%20higher%20degree%20of%20sample%20efficiency%20and%20is%0Aempirically%20demonstrated%20to%20enhance%20the%20classification%20model%27s%20performance%20on%0Aboth%20full-%20and%20missing-modality%20data%20in%20the%20low-data%20regime%20across%20various%0Amultimodal%20learning%20tasks.%20When%20only%201%25%20of%20the%20training%20data%20are%20available%2C%20our%0Aproposed%20method%20demonstrates%20an%20average%20improvement%20of%206.1%25%20over%20a%20recent%0Astrong%20baseline%20across%20various%20datasets%20and%20missing%20states.%20Notably%2C%20our%20method%0Aalso%20reduces%20the%20performance%20gap%20between%20full-modality%20and%20missing-modality%0Adata%20compared%20with%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09428v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Borrowing%20Treasures%20from%20Neighbors%3A%20In-Context%20Learning%20for%20Multimodal%0A%20%20Learning%20with%20Missing%20Modalities%20and%20Data%20Scarcity&entry.906535625=Zhuo%20Zhi%20and%20Ziquan%20Liu%20and%20Moe%20Elbadawi%20and%20Adam%20Daneshmend%20and%20Mine%20Orlu%20and%20Abdul%20Basit%20and%20Andreas%20Demosthenous%20and%20Miguel%20Rodrigues&entry.1292438233=%20%20Multimodal%20machine%20learning%20with%20missing%20modalities%20is%20an%20increasingly%0Arelevant%20challenge%20arising%20in%20various%20applications%20such%20as%20healthcare.%20This%0Apaper%20extends%20the%20current%20research%20into%20missing%20modalities%20to%20the%20low-data%0Aregime%2C%20i.e.%2C%20a%20downstream%20task%20has%20both%20missing%20modalities%20and%20limited%20sample%0Asize%20issues.%20This%20problem%20setting%20is%20particularly%20challenging%20and%20also%0Apractical%20as%20it%20is%20often%20expensive%20to%20get%20full-modality%20data%20and%20sufficient%0Aannotated%20training%20samples.%20We%20propose%20to%20use%20retrieval-augmented%20in-context%0Alearning%20to%20address%20these%20two%20crucial%20issues%20by%20unleashing%20the%20potential%20of%20a%0Atransformer%27s%20in-context%20learning%20ability.%20Diverging%20from%20existing%20methods%2C%0Awhich%20primarily%20belong%20to%20the%20parametric%20paradigm%20and%20often%20require%20sufficient%0Atraining%20samples%2C%20our%20work%20exploits%20the%20value%20of%20the%20available%20full-modality%0Adata%2C%20offering%20a%20novel%20perspective%20on%20resolving%20the%20challenge.%20The%20proposed%0Adata-dependent%20framework%20exhibits%20a%20higher%20degree%20of%20sample%20efficiency%20and%20is%0Aempirically%20demonstrated%20to%20enhance%20the%20classification%20model%27s%20performance%20on%0Aboth%20full-%20and%20missing-modality%20data%20in%20the%20low-data%20regime%20across%20various%0Amultimodal%20learning%20tasks.%20When%20only%201%25%20of%20the%20training%20data%20are%20available%2C%20our%0Aproposed%20method%20demonstrates%20an%20average%20improvement%20of%206.1%25%20over%20a%20recent%0Astrong%20baseline%20across%20various%20datasets%20and%20missing%20states.%20Notably%2C%20our%20method%0Aalso%20reduces%20the%20performance%20gap%20between%20full-modality%20and%20missing-modality%0Adata%20compared%20with%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09428v2&entry.124074799=Read"},
{"title": "A foundation model utilizing chest CT volumes and radiology reports for\n  supervised-level zero-shot detection of abnormalities", "author": "Ibrahim Ethem Hamamci and Sezgin Er and Furkan Almas and Ayse Gulnihan Simsek and Sevval Nil Esirgun and Irem Dogan and Muhammed Furkan Dasdelen and Bastian Wittmann and Enis Simsar and Mehmet Simsar and Emine Bensu Erdemir and Abdullah Alanbay and Anjany Sekuboyina and Berkan Lafci and Mehmet K. Ozdemir and Bjoern Menze", "abstract": "  A major challenge in computational research in 3D medical imaging is the lack\nof comprehensive datasets. Addressing this issue, our study introduces CT-RATE,\nthe first 3D medical imaging dataset that pairs images with textual reports.\nCT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188\nthrough various reconstructions, from 21,304 unique patients, along with\ncorresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP,\na CT-focused contrastive language-image pre-training framework. As a versatile,\nself-supervised model, CT-CLIP is designed for broad application and does not\nrequire task-specific training. Remarkably, CT-CLIP outperforms\nstate-of-the-art, fully supervised methods in multi-abnormality detection\nacross all key metrics, thus eliminating the need for manual annotation. We\nalso demonstrate its utility in case retrieval, whether using imagery or\ntextual queries, thereby advancing knowledge dissemination. The open-source\nrelease of CT-RATE and CT-CLIP marks a significant advancement in medical AI,\nenhancing 3D imaging analysis and fostering innovation in healthcare.\n", "link": "http://arxiv.org/abs/2403.17834v1", "date": "2024-03-26", "relevancy": 1.7033, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6025}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5441}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5046}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20foundation%20model%20utilizing%20chest%20CT%20volumes%20and%20radiology%20reports%20for%0A%20%20supervised-level%20zero-shot%20detection%20of%20abnormalities&body=Title%3A%20A%20foundation%20model%20utilizing%20chest%20CT%20volumes%20and%20radiology%20reports%20for%0A%20%20supervised-level%20zero-shot%20detection%20of%20abnormalities%0AAuthor%3A%20Ibrahim%20Ethem%20Hamamci%20and%20Sezgin%20Er%20and%20Furkan%20Almas%20and%20Ayse%20Gulnihan%20Simsek%20and%20Sevval%20Nil%20Esirgun%20and%20Irem%20Dogan%20and%20Muhammed%20Furkan%20Dasdelen%20and%20Bastian%20Wittmann%20and%20Enis%20Simsar%20and%20Mehmet%20Simsar%20and%20Emine%20Bensu%20Erdemir%20and%20Abdullah%20Alanbay%20and%20Anjany%20Sekuboyina%20and%20Berkan%20Lafci%20and%20Mehmet%20K.%20Ozdemir%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20A%20major%20challenge%20in%20computational%20research%20in%203D%20medical%20imaging%20is%20the%20lack%0Aof%20comprehensive%20datasets.%20Addressing%20this%20issue%2C%20our%20study%20introduces%20CT-RATE%2C%0Athe%20first%203D%20medical%20imaging%20dataset%20that%20pairs%20images%20with%20textual%20reports.%0ACT-RATE%20consists%20of%2025%2C692%20non-contrast%20chest%20CT%20volumes%2C%20expanded%20to%2050%2C188%0Athrough%20various%20reconstructions%2C%20from%2021%2C304%20unique%20patients%2C%20along%20with%0Acorresponding%20radiology%20text%20reports.%20Leveraging%20CT-RATE%2C%20we%20developed%20CT-CLIP%2C%0Aa%20CT-focused%20contrastive%20language-image%20pre-training%20framework.%20As%20a%20versatile%2C%0Aself-supervised%20model%2C%20CT-CLIP%20is%20designed%20for%20broad%20application%20and%20does%20not%0Arequire%20task-specific%20training.%20Remarkably%2C%20CT-CLIP%20outperforms%0Astate-of-the-art%2C%20fully%20supervised%20methods%20in%20multi-abnormality%20detection%0Aacross%20all%20key%20metrics%2C%20thus%20eliminating%20the%20need%20for%20manual%20annotation.%20We%0Aalso%20demonstrate%20its%20utility%20in%20case%20retrieval%2C%20whether%20using%20imagery%20or%0Atextual%20queries%2C%20thereby%20advancing%20knowledge%20dissemination.%20The%20open-source%0Arelease%20of%20CT-RATE%20and%20CT-CLIP%20marks%20a%20significant%20advancement%20in%20medical%20AI%2C%0Aenhancing%203D%20imaging%20analysis%20and%20fostering%20innovation%20in%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17834v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20foundation%20model%20utilizing%20chest%20CT%20volumes%20and%20radiology%20reports%20for%0A%20%20supervised-level%20zero-shot%20detection%20of%20abnormalities&entry.906535625=Ibrahim%20Ethem%20Hamamci%20and%20Sezgin%20Er%20and%20Furkan%20Almas%20and%20Ayse%20Gulnihan%20Simsek%20and%20Sevval%20Nil%20Esirgun%20and%20Irem%20Dogan%20and%20Muhammed%20Furkan%20Dasdelen%20and%20Bastian%20Wittmann%20and%20Enis%20Simsar%20and%20Mehmet%20Simsar%20and%20Emine%20Bensu%20Erdemir%20and%20Abdullah%20Alanbay%20and%20Anjany%20Sekuboyina%20and%20Berkan%20Lafci%20and%20Mehmet%20K.%20Ozdemir%20and%20Bjoern%20Menze&entry.1292438233=%20%20A%20major%20challenge%20in%20computational%20research%20in%203D%20medical%20imaging%20is%20the%20lack%0Aof%20comprehensive%20datasets.%20Addressing%20this%20issue%2C%20our%20study%20introduces%20CT-RATE%2C%0Athe%20first%203D%20medical%20imaging%20dataset%20that%20pairs%20images%20with%20textual%20reports.%0ACT-RATE%20consists%20of%2025%2C692%20non-contrast%20chest%20CT%20volumes%2C%20expanded%20to%2050%2C188%0Athrough%20various%20reconstructions%2C%20from%2021%2C304%20unique%20patients%2C%20along%20with%0Acorresponding%20radiology%20text%20reports.%20Leveraging%20CT-RATE%2C%20we%20developed%20CT-CLIP%2C%0Aa%20CT-focused%20contrastive%20language-image%20pre-training%20framework.%20As%20a%20versatile%2C%0Aself-supervised%20model%2C%20CT-CLIP%20is%20designed%20for%20broad%20application%20and%20does%20not%0Arequire%20task-specific%20training.%20Remarkably%2C%20CT-CLIP%20outperforms%0Astate-of-the-art%2C%20fully%20supervised%20methods%20in%20multi-abnormality%20detection%0Aacross%20all%20key%20metrics%2C%20thus%20eliminating%20the%20need%20for%20manual%20annotation.%20We%0Aalso%20demonstrate%20its%20utility%20in%20case%20retrieval%2C%20whether%20using%20imagery%20or%0Atextual%20queries%2C%20thereby%20advancing%20knowledge%20dissemination.%20The%20open-source%0Arelease%20of%20CT-RATE%20and%20CT-CLIP%20marks%20a%20significant%20advancement%20in%20medical%20AI%2C%0Aenhancing%203D%20imaging%20analysis%20and%20fostering%20innovation%20in%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17834v1&entry.124074799=Read"},
{"title": "Track Everything Everywhere Fast and Robustly", "author": "Yunzhou Song and Jiahui Lei and Ziyun Wang and Lingjie Liu and Kostas Daniilidis", "abstract": "  We propose a novel test-time optimization approach for efficiently and\nrobustly tracking any pixel at any time in a video. The latest state-of-the-art\noptimization-based tracking technique, OmniMotion, requires a prohibitively\nlong optimization time, rendering it impractical for downstream applications.\nOmniMotion is sensitive to the choice of random seeds, leading to unstable\nconvergence. To improve efficiency and robustness, we introduce a novel\ninvertible deformation network, CaDeX++, which factorizes the function\nrepresentation into a local spatial-temporal feature grid and enhances the\nexpressivity of the coupling blocks with non-linear functions. While CaDeX++\nincorporates a stronger geometric bias within its architectural design, it also\ntakes advantage of the inductive bias provided by the vision foundation models.\nOur system utilizes monocular depth estimation to represent scene geometry and\nenhances the objective by incorporating DINOv2 long-term semantics to regulate\nthe optimization process. Our experiments demonstrate a substantial improvement\nin training speed (more than \\textbf{10 times} faster), robustness, and\naccuracy in tracking over the SoTA optimization-based method OmniMotion.\n", "link": "http://arxiv.org/abs/2403.17931v1", "date": "2024-03-26", "relevancy": 1.6953, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5758}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5629}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5599}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Track%20Everything%20Everywhere%20Fast%20and%20Robustly&body=Title%3A%20Track%20Everything%20Everywhere%20Fast%20and%20Robustly%0AAuthor%3A%20Yunzhou%20Song%20and%20Jiahui%20Lei%20and%20Ziyun%20Wang%20and%20Lingjie%20Liu%20and%20Kostas%20Daniilidis%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20test-time%20optimization%20approach%20for%20efficiently%20and%0Arobustly%20tracking%20any%20pixel%20at%20any%20time%20in%20a%20video.%20The%20latest%20state-of-the-art%0Aoptimization-based%20tracking%20technique%2C%20OmniMotion%2C%20requires%20a%20prohibitively%0Along%20optimization%20time%2C%20rendering%20it%20impractical%20for%20downstream%20applications.%0AOmniMotion%20is%20sensitive%20to%20the%20choice%20of%20random%20seeds%2C%20leading%20to%20unstable%0Aconvergence.%20To%20improve%20efficiency%20and%20robustness%2C%20we%20introduce%20a%20novel%0Ainvertible%20deformation%20network%2C%20CaDeX%2B%2B%2C%20which%20factorizes%20the%20function%0Arepresentation%20into%20a%20local%20spatial-temporal%20feature%20grid%20and%20enhances%20the%0Aexpressivity%20of%20the%20coupling%20blocks%20with%20non-linear%20functions.%20While%20CaDeX%2B%2B%0Aincorporates%20a%20stronger%20geometric%20bias%20within%20its%20architectural%20design%2C%20it%20also%0Atakes%20advantage%20of%20the%20inductive%20bias%20provided%20by%20the%20vision%20foundation%20models.%0AOur%20system%20utilizes%20monocular%20depth%20estimation%20to%20represent%20scene%20geometry%20and%0Aenhances%20the%20objective%20by%20incorporating%20DINOv2%20long-term%20semantics%20to%20regulate%0Athe%20optimization%20process.%20Our%20experiments%20demonstrate%20a%20substantial%20improvement%0Ain%20training%20speed%20%28more%20than%20%5Ctextbf%7B10%20times%7D%20faster%29%2C%20robustness%2C%20and%0Aaccuracy%20in%20tracking%20over%20the%20SoTA%20optimization-based%20method%20OmniMotion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17931v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Track%20Everything%20Everywhere%20Fast%20and%20Robustly&entry.906535625=Yunzhou%20Song%20and%20Jiahui%20Lei%20and%20Ziyun%20Wang%20and%20Lingjie%20Liu%20and%20Kostas%20Daniilidis&entry.1292438233=%20%20We%20propose%20a%20novel%20test-time%20optimization%20approach%20for%20efficiently%20and%0Arobustly%20tracking%20any%20pixel%20at%20any%20time%20in%20a%20video.%20The%20latest%20state-of-the-art%0Aoptimization-based%20tracking%20technique%2C%20OmniMotion%2C%20requires%20a%20prohibitively%0Along%20optimization%20time%2C%20rendering%20it%20impractical%20for%20downstream%20applications.%0AOmniMotion%20is%20sensitive%20to%20the%20choice%20of%20random%20seeds%2C%20leading%20to%20unstable%0Aconvergence.%20To%20improve%20efficiency%20and%20robustness%2C%20we%20introduce%20a%20novel%0Ainvertible%20deformation%20network%2C%20CaDeX%2B%2B%2C%20which%20factorizes%20the%20function%0Arepresentation%20into%20a%20local%20spatial-temporal%20feature%20grid%20and%20enhances%20the%0Aexpressivity%20of%20the%20coupling%20blocks%20with%20non-linear%20functions.%20While%20CaDeX%2B%2B%0Aincorporates%20a%20stronger%20geometric%20bias%20within%20its%20architectural%20design%2C%20it%20also%0Atakes%20advantage%20of%20the%20inductive%20bias%20provided%20by%20the%20vision%20foundation%20models.%0AOur%20system%20utilizes%20monocular%20depth%20estimation%20to%20represent%20scene%20geometry%20and%0Aenhances%20the%20objective%20by%20incorporating%20DINOv2%20long-term%20semantics%20to%20regulate%0Athe%20optimization%20process.%20Our%20experiments%20demonstrate%20a%20substantial%20improvement%0Ain%20training%20speed%20%28more%20than%20%5Ctextbf%7B10%20times%7D%20faster%29%2C%20robustness%2C%20and%0Aaccuracy%20in%20tracking%20over%20the%20SoTA%20optimization-based%20method%20OmniMotion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17931v1&entry.124074799=Read"},
{"title": "ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture\n  Synthesis", "author": "Muhammad Hamza Mughal and Rishabh Dabral and Ikhsanul Habibie and Lucia Donatelli and Marc Habermann and Christian Theobalt", "abstract": "  Gestures play a key role in human communication. Recent methods for co-speech\ngesture generation, while managing to generate beat-aligned motions, struggle\ngenerating gestures that are semantically aligned with the utterance. Compared\nto beat gestures that align naturally to the audio signal, semantically\ncoherent gestures require modeling the complex interactions between the\nlanguage and human motion, and can be controlled by focusing on certain words.\nTherefore, we present ConvoFusion, a diffusion-based approach for multi-modal\ngesture synthesis, which can not only generate gestures based on multi-modal\nspeech inputs, but can also facilitate controllability in gesture synthesis.\nOur method proposes two guidance objectives that allow the users to modulate\nthe impact of different conditioning modalities (e.g. audio vs text) as well as\nto choose certain words to be emphasized during gesturing. Our method is\nversatile in that it can be trained either for generating monologue gestures or\neven the conversational gestures. To further advance the research on\nmulti-party interactive gestures, the DnD Group Gesture dataset is released,\nwhich contains 6 hours of gesture data showing 5 people interacting with one\nanother. We compare our method with several recent works and demonstrate\neffectiveness of our method on a variety of tasks. We urge the reader to watch\nour supplementary video at our website.\n", "link": "http://arxiv.org/abs/2403.17936v1", "date": "2024-03-26", "relevancy": 1.6587, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5563}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5524}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5517}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ConvoFusion%3A%20Multi-Modal%20Conversational%20Diffusion%20for%20Co-Speech%20Gesture%0A%20%20Synthesis&body=Title%3A%20ConvoFusion%3A%20Multi-Modal%20Conversational%20Diffusion%20for%20Co-Speech%20Gesture%0A%20%20Synthesis%0AAuthor%3A%20Muhammad%20Hamza%20Mughal%20and%20Rishabh%20Dabral%20and%20Ikhsanul%20Habibie%20and%20Lucia%20Donatelli%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Gestures%20play%20a%20key%20role%20in%20human%20communication.%20Recent%20methods%20for%20co-speech%0Agesture%20generation%2C%20while%20managing%20to%20generate%20beat-aligned%20motions%2C%20struggle%0Agenerating%20gestures%20that%20are%20semantically%20aligned%20with%20the%20utterance.%20Compared%0Ato%20beat%20gestures%20that%20align%20naturally%20to%20the%20audio%20signal%2C%20semantically%0Acoherent%20gestures%20require%20modeling%20the%20complex%20interactions%20between%20the%0Alanguage%20and%20human%20motion%2C%20and%20can%20be%20controlled%20by%20focusing%20on%20certain%20words.%0ATherefore%2C%20we%20present%20ConvoFusion%2C%20a%20diffusion-based%20approach%20for%20multi-modal%0Agesture%20synthesis%2C%20which%20can%20not%20only%20generate%20gestures%20based%20on%20multi-modal%0Aspeech%20inputs%2C%20but%20can%20also%20facilitate%20controllability%20in%20gesture%20synthesis.%0AOur%20method%20proposes%20two%20guidance%20objectives%20that%20allow%20the%20users%20to%20modulate%0Athe%20impact%20of%20different%20conditioning%20modalities%20%28e.g.%20audio%20vs%20text%29%20as%20well%20as%0Ato%20choose%20certain%20words%20to%20be%20emphasized%20during%20gesturing.%20Our%20method%20is%0Aversatile%20in%20that%20it%20can%20be%20trained%20either%20for%20generating%20monologue%20gestures%20or%0Aeven%20the%20conversational%20gestures.%20To%20further%20advance%20the%20research%20on%0Amulti-party%20interactive%20gestures%2C%20the%20DnD%20Group%20Gesture%20dataset%20is%20released%2C%0Awhich%20contains%206%20hours%20of%20gesture%20data%20showing%205%20people%20interacting%20with%20one%0Aanother.%20We%20compare%20our%20method%20with%20several%20recent%20works%20and%20demonstrate%0Aeffectiveness%20of%20our%20method%20on%20a%20variety%20of%20tasks.%20We%20urge%20the%20reader%20to%20watch%0Aour%20supplementary%20video%20at%20our%20website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17936v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConvoFusion%3A%20Multi-Modal%20Conversational%20Diffusion%20for%20Co-Speech%20Gesture%0A%20%20Synthesis&entry.906535625=Muhammad%20Hamza%20Mughal%20and%20Rishabh%20Dabral%20and%20Ikhsanul%20Habibie%20and%20Lucia%20Donatelli%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=%20%20Gestures%20play%20a%20key%20role%20in%20human%20communication.%20Recent%20methods%20for%20co-speech%0Agesture%20generation%2C%20while%20managing%20to%20generate%20beat-aligned%20motions%2C%20struggle%0Agenerating%20gestures%20that%20are%20semantically%20aligned%20with%20the%20utterance.%20Compared%0Ato%20beat%20gestures%20that%20align%20naturally%20to%20the%20audio%20signal%2C%20semantically%0Acoherent%20gestures%20require%20modeling%20the%20complex%20interactions%20between%20the%0Alanguage%20and%20human%20motion%2C%20and%20can%20be%20controlled%20by%20focusing%20on%20certain%20words.%0ATherefore%2C%20we%20present%20ConvoFusion%2C%20a%20diffusion-based%20approach%20for%20multi-modal%0Agesture%20synthesis%2C%20which%20can%20not%20only%20generate%20gestures%20based%20on%20multi-modal%0Aspeech%20inputs%2C%20but%20can%20also%20facilitate%20controllability%20in%20gesture%20synthesis.%0AOur%20method%20proposes%20two%20guidance%20objectives%20that%20allow%20the%20users%20to%20modulate%0Athe%20impact%20of%20different%20conditioning%20modalities%20%28e.g.%20audio%20vs%20text%29%20as%20well%20as%0Ato%20choose%20certain%20words%20to%20be%20emphasized%20during%20gesturing.%20Our%20method%20is%0Aversatile%20in%20that%20it%20can%20be%20trained%20either%20for%20generating%20monologue%20gestures%20or%0Aeven%20the%20conversational%20gestures.%20To%20further%20advance%20the%20research%20on%0Amulti-party%20interactive%20gestures%2C%20the%20DnD%20Group%20Gesture%20dataset%20is%20released%2C%0Awhich%20contains%206%20hours%20of%20gesture%20data%20showing%205%20people%20interacting%20with%20one%0Aanother.%20We%20compare%20our%20method%20with%20several%20recent%20works%20and%20demonstrate%0Aeffectiveness%20of%20our%20method%20on%20a%20variety%20of%20tasks.%20We%20urge%20the%20reader%20to%20watch%0Aour%20supplementary%20video%20at%20our%20website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17936v1&entry.124074799=Read"},
{"title": "Climate Downscaling: A Deep-Learning Based Super-resolution Model of\n  Precipitation Data with Attention Block and Skip Connections", "author": "Chia-Hao Chiang and Zheng-Han Huang and Liwen Liu and Hsin-Chien Liang and Yi-Chi Wang and Wan-Ling Tseng and Chao Wang and Che-Ta Chen and Ko-Chih Wang", "abstract": "  Human activities accelerate consumption of fossil fuels and produce\ngreenhouse gases, resulting in urgent issues today: global warming and the\nclimate change. These indirectly cause severe natural disasters, plenty of\nlives suffering and huge losses of agricultural properties. To mitigate impacts\non our lands, scientists are developing renewable, reusable, and clean energies\nand climatologists are trying to predict the extremes. Meanwhile, governments\nare publicizing resource-saving policies for a more eco-friendly society and\narousing environment awareness. One of the most influencing factors is the\nprecipitation, bringing condensed water vapor onto lands. Water resources are\nthe most significant but basic needs in society, not only supporting our\nlivings, but also economics. In Taiwan, although the average annual\nprecipitation is up to 2,500 millimeter (mm), the water allocation for each\nperson is lower than the global average due to drastically geographical\nelevation changes and uneven distribution through the year. Thus, it is crucial\nto track and predict the rainfall to make the most use of it and to prevent the\nfloods. However, climate models have limited resolution and require intensive\ncomputational power for local-scale use. Therefore, we proposed a deep\nconvolutional neural network with skip connections, attention blocks, and\nauxiliary data concatenation, in order to downscale the low-resolution\nprecipitation data into high-resolution one. Eventually, we compare with other\nclimate downscaling methods and show better performance in metrics of Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,\nstructural similarity index (SSIM), and forecast indicators.\n", "link": "http://arxiv.org/abs/2403.17847v1", "date": "2024-03-26", "relevancy": 1.6584, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5616}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5532}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5305}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Climate%20Downscaling%3A%20A%20Deep-Learning%20Based%20Super-resolution%20Model%20of%0A%20%20Precipitation%20Data%20with%20Attention%20Block%20and%20Skip%20Connections&body=Title%3A%20Climate%20Downscaling%3A%20A%20Deep-Learning%20Based%20Super-resolution%20Model%20of%0A%20%20Precipitation%20Data%20with%20Attention%20Block%20and%20Skip%20Connections%0AAuthor%3A%20Chia-Hao%20Chiang%20and%20Zheng-Han%20Huang%20and%20Liwen%20Liu%20and%20Hsin-Chien%20Liang%20and%20Yi-Chi%20Wang%20and%20Wan-Ling%20Tseng%20and%20Chao%20Wang%20and%20Che-Ta%20Chen%20and%20Ko-Chih%20Wang%0AAbstract%3A%20%20%20Human%20activities%20accelerate%20consumption%20of%20fossil%20fuels%20and%20produce%0Agreenhouse%20gases%2C%20resulting%20in%20urgent%20issues%20today%3A%20global%20warming%20and%20the%0Aclimate%20change.%20These%20indirectly%20cause%20severe%20natural%20disasters%2C%20plenty%20of%0Alives%20suffering%20and%20huge%20losses%20of%20agricultural%20properties.%20To%20mitigate%20impacts%0Aon%20our%20lands%2C%20scientists%20are%20developing%20renewable%2C%20reusable%2C%20and%20clean%20energies%0Aand%20climatologists%20are%20trying%20to%20predict%20the%20extremes.%20Meanwhile%2C%20governments%0Aare%20publicizing%20resource-saving%20policies%20for%20a%20more%20eco-friendly%20society%20and%0Aarousing%20environment%20awareness.%20One%20of%20the%20most%20influencing%20factors%20is%20the%0Aprecipitation%2C%20bringing%20condensed%20water%20vapor%20onto%20lands.%20Water%20resources%20are%0Athe%20most%20significant%20but%20basic%20needs%20in%20society%2C%20not%20only%20supporting%20our%0Alivings%2C%20but%20also%20economics.%20In%20Taiwan%2C%20although%20the%20average%20annual%0Aprecipitation%20is%20up%20to%202%2C500%20millimeter%20%28mm%29%2C%20the%20water%20allocation%20for%20each%0Aperson%20is%20lower%20than%20the%20global%20average%20due%20to%20drastically%20geographical%0Aelevation%20changes%20and%20uneven%20distribution%20through%20the%20year.%20Thus%2C%20it%20is%20crucial%0Ato%20track%20and%20predict%20the%20rainfall%20to%20make%20the%20most%20use%20of%20it%20and%20to%20prevent%20the%0Afloods.%20However%2C%20climate%20models%20have%20limited%20resolution%20and%20require%20intensive%0Acomputational%20power%20for%20local-scale%20use.%20Therefore%2C%20we%20proposed%20a%20deep%0Aconvolutional%20neural%20network%20with%20skip%20connections%2C%20attention%20blocks%2C%20and%0Aauxiliary%20data%20concatenation%2C%20in%20order%20to%20downscale%20the%20low-resolution%0Aprecipitation%20data%20into%20high-resolution%20one.%20Eventually%2C%20we%20compare%20with%20other%0Aclimate%20downscaling%20methods%20and%20show%20better%20performance%20in%20metrics%20of%20Mean%0AAbsolute%20Error%20%28MAE%29%2C%20Root%20Mean%20Square%20Error%20%28RMSE%29%2C%20Pearson%20Correlation%2C%0Astructural%20similarity%20index%20%28SSIM%29%2C%20and%20forecast%20indicators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17847v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Climate%20Downscaling%3A%20A%20Deep-Learning%20Based%20Super-resolution%20Model%20of%0A%20%20Precipitation%20Data%20with%20Attention%20Block%20and%20Skip%20Connections&entry.906535625=Chia-Hao%20Chiang%20and%20Zheng-Han%20Huang%20and%20Liwen%20Liu%20and%20Hsin-Chien%20Liang%20and%20Yi-Chi%20Wang%20and%20Wan-Ling%20Tseng%20and%20Chao%20Wang%20and%20Che-Ta%20Chen%20and%20Ko-Chih%20Wang&entry.1292438233=%20%20Human%20activities%20accelerate%20consumption%20of%20fossil%20fuels%20and%20produce%0Agreenhouse%20gases%2C%20resulting%20in%20urgent%20issues%20today%3A%20global%20warming%20and%20the%0Aclimate%20change.%20These%20indirectly%20cause%20severe%20natural%20disasters%2C%20plenty%20of%0Alives%20suffering%20and%20huge%20losses%20of%20agricultural%20properties.%20To%20mitigate%20impacts%0Aon%20our%20lands%2C%20scientists%20are%20developing%20renewable%2C%20reusable%2C%20and%20clean%20energies%0Aand%20climatologists%20are%20trying%20to%20predict%20the%20extremes.%20Meanwhile%2C%20governments%0Aare%20publicizing%20resource-saving%20policies%20for%20a%20more%20eco-friendly%20society%20and%0Aarousing%20environment%20awareness.%20One%20of%20the%20most%20influencing%20factors%20is%20the%0Aprecipitation%2C%20bringing%20condensed%20water%20vapor%20onto%20lands.%20Water%20resources%20are%0Athe%20most%20significant%20but%20basic%20needs%20in%20society%2C%20not%20only%20supporting%20our%0Alivings%2C%20but%20also%20economics.%20In%20Taiwan%2C%20although%20the%20average%20annual%0Aprecipitation%20is%20up%20to%202%2C500%20millimeter%20%28mm%29%2C%20the%20water%20allocation%20for%20each%0Aperson%20is%20lower%20than%20the%20global%20average%20due%20to%20drastically%20geographical%0Aelevation%20changes%20and%20uneven%20distribution%20through%20the%20year.%20Thus%2C%20it%20is%20crucial%0Ato%20track%20and%20predict%20the%20rainfall%20to%20make%20the%20most%20use%20of%20it%20and%20to%20prevent%20the%0Afloods.%20However%2C%20climate%20models%20have%20limited%20resolution%20and%20require%20intensive%0Acomputational%20power%20for%20local-scale%20use.%20Therefore%2C%20we%20proposed%20a%20deep%0Aconvolutional%20neural%20network%20with%20skip%20connections%2C%20attention%20blocks%2C%20and%0Aauxiliary%20data%20concatenation%2C%20in%20order%20to%20downscale%20the%20low-resolution%0Aprecipitation%20data%20into%20high-resolution%20one.%20Eventually%2C%20we%20compare%20with%20other%0Aclimate%20downscaling%20methods%20and%20show%20better%20performance%20in%20metrics%20of%20Mean%0AAbsolute%20Error%20%28MAE%29%2C%20Root%20Mean%20Square%20Error%20%28RMSE%29%2C%20Pearson%20Correlation%2C%0Astructural%20similarity%20index%20%28SSIM%29%2C%20and%20forecast%20indicators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17847v1&entry.124074799=Read"},
{"title": "ReMamber: Referring Image Segmentation with Mamba Twister", "author": "Yuhuan Yang and Chaofan Ma and Jiangchao Yao and Zhun Zhong and Ya Zhang and Yanfeng Wang", "abstract": "  Referring Image Segmentation (RIS) leveraging transformers has achieved great\nsuccess on the interpretation of complex visual-language tasks. However, the\nquadratic computation cost makes it resource-consuming in capturing long-range\nvisual-language dependencies. Fortunately, Mamba addresses this with efficient\nlinear complexity in processing. However, directly applying Mamba to\nmulti-modal interactions presents challenges, primarily due to inadequate\nchannel interactions for the effective fusion of multi-modal data. In this\npaper, we propose ReMamber, a novel RIS architecture that integrates the power\nof Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly\nmodels image-text interaction, and fuses textual and visual features through\nits unique channel and spatial twisting mechanism. We achieve the\nstate-of-the-art on three challenging benchmarks. Moreover, we conduct thorough\nanalyses of ReMamber and discuss other fusion designs using Mamba. These\nprovide valuable perspectives for future research.\n", "link": "http://arxiv.org/abs/2403.17839v1", "date": "2024-03-26", "relevancy": 1.6488, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5813}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5109}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5092}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ReMamber%3A%20Referring%20Image%20Segmentation%20with%20Mamba%20Twister&body=Title%3A%20ReMamber%3A%20Referring%20Image%20Segmentation%20with%20Mamba%20Twister%0AAuthor%3A%20Yuhuan%20Yang%20and%20Chaofan%20Ma%20and%20Jiangchao%20Yao%20and%20Zhun%20Zhong%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Referring%20Image%20Segmentation%20%28RIS%29%20leveraging%20transformers%20has%20achieved%20great%0Asuccess%20on%20the%20interpretation%20of%20complex%20visual-language%20tasks.%20However%2C%20the%0Aquadratic%20computation%20cost%20makes%20it%20resource-consuming%20in%20capturing%20long-range%0Avisual-language%20dependencies.%20Fortunately%2C%20Mamba%20addresses%20this%20with%20efficient%0Alinear%20complexity%20in%20processing.%20However%2C%20directly%20applying%20Mamba%20to%0Amulti-modal%20interactions%20presents%20challenges%2C%20primarily%20due%20to%20inadequate%0Achannel%20interactions%20for%20the%20effective%20fusion%20of%20multi-modal%20data.%20In%20this%0Apaper%2C%20we%20propose%20ReMamber%2C%20a%20novel%20RIS%20architecture%20that%20integrates%20the%20power%0Aof%20Mamba%20with%20a%20multi-modal%20Mamba%20Twister%20block.%20The%20Mamba%20Twister%20explicitly%0Amodels%20image-text%20interaction%2C%20and%20fuses%20textual%20and%20visual%20features%20through%0Aits%20unique%20channel%20and%20spatial%20twisting%20mechanism.%20We%20achieve%20the%0Astate-of-the-art%20on%20three%20challenging%20benchmarks.%20Moreover%2C%20we%20conduct%20thorough%0Aanalyses%20of%20ReMamber%20and%20discuss%20other%20fusion%20designs%20using%20Mamba.%20These%0Aprovide%20valuable%20perspectives%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17839v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReMamber%3A%20Referring%20Image%20Segmentation%20with%20Mamba%20Twister&entry.906535625=Yuhuan%20Yang%20and%20Chaofan%20Ma%20and%20Jiangchao%20Yao%20and%20Zhun%20Zhong%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Referring%20Image%20Segmentation%20%28RIS%29%20leveraging%20transformers%20has%20achieved%20great%0Asuccess%20on%20the%20interpretation%20of%20complex%20visual-language%20tasks.%20However%2C%20the%0Aquadratic%20computation%20cost%20makes%20it%20resource-consuming%20in%20capturing%20long-range%0Avisual-language%20dependencies.%20Fortunately%2C%20Mamba%20addresses%20this%20with%20efficient%0Alinear%20complexity%20in%20processing.%20However%2C%20directly%20applying%20Mamba%20to%0Amulti-modal%20interactions%20presents%20challenges%2C%20primarily%20due%20to%20inadequate%0Achannel%20interactions%20for%20the%20effective%20fusion%20of%20multi-modal%20data.%20In%20this%0Apaper%2C%20we%20propose%20ReMamber%2C%20a%20novel%20RIS%20architecture%20that%20integrates%20the%20power%0Aof%20Mamba%20with%20a%20multi-modal%20Mamba%20Twister%20block.%20The%20Mamba%20Twister%20explicitly%0Amodels%20image-text%20interaction%2C%20and%20fuses%20textual%20and%20visual%20features%20through%0Aits%20unique%20channel%20and%20spatial%20twisting%20mechanism.%20We%20achieve%20the%0Astate-of-the-art%20on%20three%20challenging%20benchmarks.%20Moreover%2C%20we%20conduct%20thorough%0Aanalyses%20of%20ReMamber%20and%20discuss%20other%20fusion%20designs%20using%20Mamba.%20These%0Aprovide%20valuable%20perspectives%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17839v1&entry.124074799=Read"},
{"title": "Serpent: Scalable and Efficient Image Restoration via Multi-scale\n  Structured State Space Models", "author": "Mohammad Shahab Sepehri and Zalan Fabian and Mahdi Soltanolkotabi", "abstract": "  The landscape of computational building blocks of efficient image restoration\narchitectures is dominated by a combination of convolutional processing and\nvarious attention mechanisms. However, convolutional filters are inherently\nlocal and therefore struggle at modeling long-range dependencies in images. On\nthe other hand, attention excels at capturing global interactions between\narbitrary image regions, however at a quadratic cost in image dimension. In\nthis work, we propose Serpent, an architecture that leverages recent advances\nin state space models (SSMs) in its core computational block. SSMs, originally\nintroduced for sequence modeling, can maintain a global receptive field with a\nfavorable linear scaling in input size. Our preliminary results demonstrate\nthat Serpent can achieve reconstruction quality on par with state-of-the-art\ntechniques, while requiring orders of magnitude less compute (up to $150$ fold\nreduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while\nmaintaining a compact model size.\n", "link": "http://arxiv.org/abs/2403.17902v1", "date": "2024-03-26", "relevancy": 1.6462, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5859}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5565}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5308}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Serpent%3A%20Scalable%20and%20Efficient%20Image%20Restoration%20via%20Multi-scale%0A%20%20Structured%20State%20Space%20Models&body=Title%3A%20Serpent%3A%20Scalable%20and%20Efficient%20Image%20Restoration%20via%20Multi-scale%0A%20%20Structured%20State%20Space%20Models%0AAuthor%3A%20Mohammad%20Shahab%20Sepehri%20and%20Zalan%20Fabian%20and%20Mahdi%20Soltanolkotabi%0AAbstract%3A%20%20%20The%20landscape%20of%20computational%20building%20blocks%20of%20efficient%20image%20restoration%0Aarchitectures%20is%20dominated%20by%20a%20combination%20of%20convolutional%20processing%20and%0Avarious%20attention%20mechanisms.%20However%2C%20convolutional%20filters%20are%20inherently%0Alocal%20and%20therefore%20struggle%20at%20modeling%20long-range%20dependencies%20in%20images.%20On%0Athe%20other%20hand%2C%20attention%20excels%20at%20capturing%20global%20interactions%20between%0Aarbitrary%20image%20regions%2C%20however%20at%20a%20quadratic%20cost%20in%20image%20dimension.%20In%0Athis%20work%2C%20we%20propose%20Serpent%2C%20an%20architecture%20that%20leverages%20recent%20advances%0Ain%20state%20space%20models%20%28SSMs%29%20in%20its%20core%20computational%20block.%20SSMs%2C%20originally%0Aintroduced%20for%20sequence%20modeling%2C%20can%20maintain%20a%20global%20receptive%20field%20with%20a%0Afavorable%20linear%20scaling%20in%20input%20size.%20Our%20preliminary%20results%20demonstrate%0Athat%20Serpent%20can%20achieve%20reconstruction%20quality%20on%20par%20with%20state-of-the-art%0Atechniques%2C%20while%20requiring%20orders%20of%20magnitude%20less%20compute%20%28up%20to%20%24150%24%20fold%0Areduction%20in%20FLOPS%29%20and%20a%20factor%20of%20up%20to%20%245%5Ctimes%24%20less%20GPU%20memory%20while%0Amaintaining%20a%20compact%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17902v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Serpent%3A%20Scalable%20and%20Efficient%20Image%20Restoration%20via%20Multi-scale%0A%20%20Structured%20State%20Space%20Models&entry.906535625=Mohammad%20Shahab%20Sepehri%20and%20Zalan%20Fabian%20and%20Mahdi%20Soltanolkotabi&entry.1292438233=%20%20The%20landscape%20of%20computational%20building%20blocks%20of%20efficient%20image%20restoration%0Aarchitectures%20is%20dominated%20by%20a%20combination%20of%20convolutional%20processing%20and%0Avarious%20attention%20mechanisms.%20However%2C%20convolutional%20filters%20are%20inherently%0Alocal%20and%20therefore%20struggle%20at%20modeling%20long-range%20dependencies%20in%20images.%20On%0Athe%20other%20hand%2C%20attention%20excels%20at%20capturing%20global%20interactions%20between%0Aarbitrary%20image%20regions%2C%20however%20at%20a%20quadratic%20cost%20in%20image%20dimension.%20In%0Athis%20work%2C%20we%20propose%20Serpent%2C%20an%20architecture%20that%20leverages%20recent%20advances%0Ain%20state%20space%20models%20%28SSMs%29%20in%20its%20core%20computational%20block.%20SSMs%2C%20originally%0Aintroduced%20for%20sequence%20modeling%2C%20can%20maintain%20a%20global%20receptive%20field%20with%20a%0Afavorable%20linear%20scaling%20in%20input%20size.%20Our%20preliminary%20results%20demonstrate%0Athat%20Serpent%20can%20achieve%20reconstruction%20quality%20on%20par%20with%20state-of-the-art%0Atechniques%2C%20while%20requiring%20orders%20of%20magnitude%20less%20compute%20%28up%20to%20%24150%24%20fold%0Areduction%20in%20FLOPS%29%20and%20a%20factor%20of%20up%20to%20%245%5Ctimes%24%20less%20GPU%20memory%20while%0Amaintaining%20a%20compact%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17902v1&entry.124074799=Read"},
{"title": "A randomized algorithm for nonconvex minimization with inexact\n  evaluations and complexity guarantees", "author": "Shuyao Li and Stephen J. Wright", "abstract": "  We consider minimization of a smooth nonconvex function with inexact oracle\naccess to gradient and Hessian (without assuming access to the function value)\nto achieve approximate second-order optimality. A novel feature of our method\nis that if an approximate direction of negative curvature is chosen as the\nstep, we choose its sense to be positive or negative with equal probability. We\nallow gradients to be inexact in a relative sense and relax the coupling\nbetween inexactness thresholds for the first- and second-order optimality\nconditions. Our convergence analysis includes both an expectation bound based\non martingale analysis and a high-probability bound based on concentration\ninequalities. We apply our algorithm to empirical risk minimization problems\nand obtain improved gradient sample complexity over existing works.\n", "link": "http://arxiv.org/abs/2310.18841v2", "date": "2024-03-26", "relevancy": 1.6393, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4158}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4158}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4015}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20randomized%20algorithm%20for%20nonconvex%20minimization%20with%20inexact%0A%20%20evaluations%20and%20complexity%20guarantees&body=Title%3A%20A%20randomized%20algorithm%20for%20nonconvex%20minimization%20with%20inexact%0A%20%20evaluations%20and%20complexity%20guarantees%0AAuthor%3A%20Shuyao%20Li%20and%20Stephen%20J.%20Wright%0AAbstract%3A%20%20%20We%20consider%20minimization%20of%20a%20smooth%20nonconvex%20function%20with%20inexact%20oracle%0Aaccess%20to%20gradient%20and%20Hessian%20%28without%20assuming%20access%20to%20the%20function%20value%29%0Ato%20achieve%20approximate%20second-order%20optimality.%20A%20novel%20feature%20of%20our%20method%0Ais%20that%20if%20an%20approximate%20direction%20of%20negative%20curvature%20is%20chosen%20as%20the%0Astep%2C%20we%20choose%20its%20sense%20to%20be%20positive%20or%20negative%20with%20equal%20probability.%20We%0Aallow%20gradients%20to%20be%20inexact%20in%20a%20relative%20sense%20and%20relax%20the%20coupling%0Abetween%20inexactness%20thresholds%20for%20the%20first-%20and%20second-order%20optimality%0Aconditions.%20Our%20convergence%20analysis%20includes%20both%20an%20expectation%20bound%20based%0Aon%20martingale%20analysis%20and%20a%20high-probability%20bound%20based%20on%20concentration%0Ainequalities.%20We%20apply%20our%20algorithm%20to%20empirical%20risk%20minimization%20problems%0Aand%20obtain%20improved%20gradient%20sample%20complexity%20over%20existing%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18841v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20randomized%20algorithm%20for%20nonconvex%20minimization%20with%20inexact%0A%20%20evaluations%20and%20complexity%20guarantees&entry.906535625=Shuyao%20Li%20and%20Stephen%20J.%20Wright&entry.1292438233=%20%20We%20consider%20minimization%20of%20a%20smooth%20nonconvex%20function%20with%20inexact%20oracle%0Aaccess%20to%20gradient%20and%20Hessian%20%28without%20assuming%20access%20to%20the%20function%20value%29%0Ato%20achieve%20approximate%20second-order%20optimality.%20A%20novel%20feature%20of%20our%20method%0Ais%20that%20if%20an%20approximate%20direction%20of%20negative%20curvature%20is%20chosen%20as%20the%0Astep%2C%20we%20choose%20its%20sense%20to%20be%20positive%20or%20negative%20with%20equal%20probability.%20We%0Aallow%20gradients%20to%20be%20inexact%20in%20a%20relative%20sense%20and%20relax%20the%20coupling%0Abetween%20inexactness%20thresholds%20for%20the%20first-%20and%20second-order%20optimality%0Aconditions.%20Our%20convergence%20analysis%20includes%20both%20an%20expectation%20bound%20based%0Aon%20martingale%20analysis%20and%20a%20high-probability%20bound%20based%20on%20concentration%0Ainequalities.%20We%20apply%20our%20algorithm%20to%20empirical%20risk%20minimization%20problems%0Aand%20obtain%20improved%20gradient%20sample%20complexity%20over%20existing%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18841v2&entry.124074799=Read"},
{"title": "Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2", "author": "Chen Yiwei and Tang Chao and Aghabiglou Amir and Chu Chung San and Wiaux Yves", "abstract": "  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n", "link": "http://arxiv.org/abs/2403.17905v1", "date": "2024-03-26", "relevancy": 1.6309, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5456}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5452}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.537}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scalable%20Non-Cartesian%20Magnetic%20Resonance%20Imaging%20with%20R2D2&body=Title%3A%20Scalable%20Non-Cartesian%20Magnetic%20Resonance%20Imaging%20with%20R2D2%0AAuthor%3A%20Chen%20Yiwei%20and%20Tang%20Chao%20and%20Aghabiglou%20Amir%20and%20Chu%20Chung%20San%20and%20Wiaux%20Yves%0AAbstract%3A%20%20%20We%20propose%20a%20new%20approach%20for%20non-Cartesian%20magnetic%20resonance%20image%0Areconstruction.%20While%20unrolled%20architectures%20provide%20robustness%20via%0Adata-consistency%20layers%2C%20embedding%20measurement%20operators%20in%20Deep%20Neural%20Network%0A%28DNN%29%20can%20become%20impractical%20at%20large%20scale.%20Alternative%20Plug-and-Play%20%28PnP%29%0Aapproaches%2C%20where%20the%20denoising%20DNNs%20are%20blind%20to%20the%20measurement%20setting%2C%20are%0Anot%20affected%20by%20this%20limitation%20and%20have%20also%20proven%20effective%2C%20but%20their%0Ahighly%20iterative%20nature%20also%20affects%20scalability.%20To%20address%20this%20scalability%0Achallenge%2C%20we%20leverage%20the%20%22Residual-to-Residual%20DNN%20series%20for%20high-Dynamic%0Arange%20imaging%20%28R2D2%29%22%20approach%20recently%20introduced%20in%20astronomical%20imaging.%0AR2D2%27s%20reconstruction%20is%20formed%20as%20a%20series%20of%20residual%20images%2C%20iteratively%0Aestimated%20as%20outputs%20of%20DNNs%20taking%20the%20previous%20iteration%27s%20image%20estimate%20and%0Aassociated%20data%20residual%20as%20inputs.%20The%20method%20can%20be%20interpreted%20as%20a%20learned%0Aversion%20of%20the%20Matching%20Pursuit%20algorithm.%20We%20demonstrate%20R2D2%20in%20simulation%2C%0Aconsidering%20radial%20k-space%20sampling%20acquisition%20sequences.%20Our%20preliminary%0Aresults%20suggest%20that%20R2D2%20achieves%3A%20%28i%29%20suboptimal%20performance%20compared%20to%20its%0Aunrolled%20incarnation%20R2D2-Net%2C%20which%20is%20however%20non-scalable%20due%20to%20the%0Anecessary%20embedding%20of%20NUFFT-based%20data-consistency%20layers%3B%20%28ii%29%20superior%0Areconstruction%20quality%20to%20a%20scalable%20version%20of%20R2D2-Net%20embedding%20an%20FFT-based%0Aapproximation%20for%20data%20consistency%3B%20%28iii%29%20superior%20reconstruction%20quality%20to%0APnP%2C%20while%20only%20requiring%20few%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17905v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Non-Cartesian%20Magnetic%20Resonance%20Imaging%20with%20R2D2&entry.906535625=Chen%20Yiwei%20and%20Tang%20Chao%20and%20Aghabiglou%20Amir%20and%20Chu%20Chung%20San%20and%20Wiaux%20Yves&entry.1292438233=%20%20We%20propose%20a%20new%20approach%20for%20non-Cartesian%20magnetic%20resonance%20image%0Areconstruction.%20While%20unrolled%20architectures%20provide%20robustness%20via%0Adata-consistency%20layers%2C%20embedding%20measurement%20operators%20in%20Deep%20Neural%20Network%0A%28DNN%29%20can%20become%20impractical%20at%20large%20scale.%20Alternative%20Plug-and-Play%20%28PnP%29%0Aapproaches%2C%20where%20the%20denoising%20DNNs%20are%20blind%20to%20the%20measurement%20setting%2C%20are%0Anot%20affected%20by%20this%20limitation%20and%20have%20also%20proven%20effective%2C%20but%20their%0Ahighly%20iterative%20nature%20also%20affects%20scalability.%20To%20address%20this%20scalability%0Achallenge%2C%20we%20leverage%20the%20%22Residual-to-Residual%20DNN%20series%20for%20high-Dynamic%0Arange%20imaging%20%28R2D2%29%22%20approach%20recently%20introduced%20in%20astronomical%20imaging.%0AR2D2%27s%20reconstruction%20is%20formed%20as%20a%20series%20of%20residual%20images%2C%20iteratively%0Aestimated%20as%20outputs%20of%20DNNs%20taking%20the%20previous%20iteration%27s%20image%20estimate%20and%0Aassociated%20data%20residual%20as%20inputs.%20The%20method%20can%20be%20interpreted%20as%20a%20learned%0Aversion%20of%20the%20Matching%20Pursuit%20algorithm.%20We%20demonstrate%20R2D2%20in%20simulation%2C%0Aconsidering%20radial%20k-space%20sampling%20acquisition%20sequences.%20Our%20preliminary%0Aresults%20suggest%20that%20R2D2%20achieves%3A%20%28i%29%20suboptimal%20performance%20compared%20to%20its%0Aunrolled%20incarnation%20R2D2-Net%2C%20which%20is%20however%20non-scalable%20due%20to%20the%0Anecessary%20embedding%20of%20NUFFT-based%20data-consistency%20layers%3B%20%28ii%29%20superior%0Areconstruction%20quality%20to%20a%20scalable%20version%20of%20R2D2-Net%20embedding%20an%20FFT-based%0Aapproximation%20for%20data%20consistency%3B%20%28iii%29%20superior%20reconstruction%20quality%20to%0APnP%2C%20while%20only%20requiring%20few%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17905v1&entry.124074799=Read"},
{"title": "Mechanistic Design and Scaling of Hybrid Architectures", "author": "Michael Poli and Armin W Thomas and Eric Nguyen and Pragaash Ponnusamy and Bj\u00f6rn Deiseroth and Kristian Kersting and Taiji Suzuki and Brian Hie and Stefano Ermon and Christopher R\u00e9 and Ce Zhang and Stefano Massaroli", "abstract": "  The development of deep learning architectures is a resource-demanding\nprocess, due to a vast design space, long prototyping times, and high compute\ncosts associated with at-scale model training and evaluation. We set out to\nsimplify this process by grounding it in an end-to-end mechanistic architecture\ndesign (MAD) pipeline, encompassing small-scale capability unit tests\npredictive of scaling laws. Through a suite of synthetic token manipulation\ntasks such as compression and recall, designed to probe capabilities, we\nidentify and test new hybrid architectures constructed from a variety of\ncomputational primitives. We experimentally validate the resulting\narchitectures via an extensive compute-optimal and a new state-optimal scaling\nlaw analysis, training over 500 language models between 70M to 7B parameters.\nSurprisingly, we find MAD synthetics to correlate with compute-optimal\nperplexity, enabling accurate evaluation of new architectures via isolated\nproxy tasks. The new architectures found via MAD, based on simple ideas such as\nhybridization and sparsity, outperform state-of-the-art Transformer,\nconvolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in\nscaling, both at compute-optimal budgets and in overtrained regimes. Overall,\nthese results provide evidence that performance on curated synthetic tasks can\nbe predictive of scaling laws, and that an optimal architecture should leverage\nspecialized layers via a hybrid topology.\n", "link": "http://arxiv.org/abs/2403.17844v1", "date": "2024-03-26", "relevancy": 1.6107, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5872}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5238}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5193}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mechanistic%20Design%20and%20Scaling%20of%20Hybrid%20Architectures&body=Title%3A%20Mechanistic%20Design%20and%20Scaling%20of%20Hybrid%20Architectures%0AAuthor%3A%20Michael%20Poli%20and%20Armin%20W%20Thomas%20and%20Eric%20Nguyen%20and%20Pragaash%20Ponnusamy%20and%20Bj%C3%B6rn%20Deiseroth%20and%20Kristian%20Kersting%20and%20Taiji%20Suzuki%20and%20Brian%20Hie%20and%20Stefano%20Ermon%20and%20Christopher%20R%C3%A9%20and%20Ce%20Zhang%20and%20Stefano%20Massaroli%0AAbstract%3A%20%20%20The%20development%20of%20deep%20learning%20architectures%20is%20a%20resource-demanding%0Aprocess%2C%20due%20to%20a%20vast%20design%20space%2C%20long%20prototyping%20times%2C%20and%20high%20compute%0Acosts%20associated%20with%20at-scale%20model%20training%20and%20evaluation.%20We%20set%20out%20to%0Asimplify%20this%20process%20by%20grounding%20it%20in%20an%20end-to-end%20mechanistic%20architecture%0Adesign%20%28MAD%29%20pipeline%2C%20encompassing%20small-scale%20capability%20unit%20tests%0Apredictive%20of%20scaling%20laws.%20Through%20a%20suite%20of%20synthetic%20token%20manipulation%0Atasks%20such%20as%20compression%20and%20recall%2C%20designed%20to%20probe%20capabilities%2C%20we%0Aidentify%20and%20test%20new%20hybrid%20architectures%20constructed%20from%20a%20variety%20of%0Acomputational%20primitives.%20We%20experimentally%20validate%20the%20resulting%0Aarchitectures%20via%20an%20extensive%20compute-optimal%20and%20a%20new%20state-optimal%20scaling%0Alaw%20analysis%2C%20training%20over%20500%20language%20models%20between%2070M%20to%207B%20parameters.%0ASurprisingly%2C%20we%20find%20MAD%20synthetics%20to%20correlate%20with%20compute-optimal%0Aperplexity%2C%20enabling%20accurate%20evaluation%20of%20new%20architectures%20via%20isolated%0Aproxy%20tasks.%20The%20new%20architectures%20found%20via%20MAD%2C%20based%20on%20simple%20ideas%20such%20as%0Ahybridization%20and%20sparsity%2C%20outperform%20state-of-the-art%20Transformer%2C%0Aconvolutional%2C%20and%20recurrent%20architectures%20%28Transformer%2B%2B%2C%20Hyena%2C%20Mamba%29%20in%0Ascaling%2C%20both%20at%20compute-optimal%20budgets%20and%20in%20overtrained%20regimes.%20Overall%2C%0Athese%20results%20provide%20evidence%20that%20performance%20on%20curated%20synthetic%20tasks%20can%0Abe%20predictive%20of%20scaling%20laws%2C%20and%20that%20an%20optimal%20architecture%20should%20leverage%0Aspecialized%20layers%20via%20a%20hybrid%20topology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17844v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistic%20Design%20and%20Scaling%20of%20Hybrid%20Architectures&entry.906535625=Michael%20Poli%20and%20Armin%20W%20Thomas%20and%20Eric%20Nguyen%20and%20Pragaash%20Ponnusamy%20and%20Bj%C3%B6rn%20Deiseroth%20and%20Kristian%20Kersting%20and%20Taiji%20Suzuki%20and%20Brian%20Hie%20and%20Stefano%20Ermon%20and%20Christopher%20R%C3%A9%20and%20Ce%20Zhang%20and%20Stefano%20Massaroli&entry.1292438233=%20%20The%20development%20of%20deep%20learning%20architectures%20is%20a%20resource-demanding%0Aprocess%2C%20due%20to%20a%20vast%20design%20space%2C%20long%20prototyping%20times%2C%20and%20high%20compute%0Acosts%20associated%20with%20at-scale%20model%20training%20and%20evaluation.%20We%20set%20out%20to%0Asimplify%20this%20process%20by%20grounding%20it%20in%20an%20end-to-end%20mechanistic%20architecture%0Adesign%20%28MAD%29%20pipeline%2C%20encompassing%20small-scale%20capability%20unit%20tests%0Apredictive%20of%20scaling%20laws.%20Through%20a%20suite%20of%20synthetic%20token%20manipulation%0Atasks%20such%20as%20compression%20and%20recall%2C%20designed%20to%20probe%20capabilities%2C%20we%0Aidentify%20and%20test%20new%20hybrid%20architectures%20constructed%20from%20a%20variety%20of%0Acomputational%20primitives.%20We%20experimentally%20validate%20the%20resulting%0Aarchitectures%20via%20an%20extensive%20compute-optimal%20and%20a%20new%20state-optimal%20scaling%0Alaw%20analysis%2C%20training%20over%20500%20language%20models%20between%2070M%20to%207B%20parameters.%0ASurprisingly%2C%20we%20find%20MAD%20synthetics%20to%20correlate%20with%20compute-optimal%0Aperplexity%2C%20enabling%20accurate%20evaluation%20of%20new%20architectures%20via%20isolated%0Aproxy%20tasks.%20The%20new%20architectures%20found%20via%20MAD%2C%20based%20on%20simple%20ideas%20such%20as%0Ahybridization%20and%20sparsity%2C%20outperform%20state-of-the-art%20Transformer%2C%0Aconvolutional%2C%20and%20recurrent%20architectures%20%28Transformer%2B%2B%2C%20Hyena%2C%20Mamba%29%20in%0Ascaling%2C%20both%20at%20compute-optimal%20budgets%20and%20in%20overtrained%20regimes.%20Overall%2C%0Athese%20results%20provide%20evidence%20that%20performance%20on%20curated%20synthetic%20tasks%20can%0Abe%20predictive%20of%20scaling%20laws%2C%20and%20that%20an%20optimal%20architecture%20should%20leverage%0Aspecialized%20layers%20via%20a%20hybrid%20topology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17844v1&entry.124074799=Read"},
{"title": "Assessment of Multimodal Large Language Models in Alignment with Human\n  Values", "author": "Zhelun Shi and Zhipin Wang and Hongxing Fan and Zaibin Zhang and Lijun Li and Yongting Zhang and Zhenfei Yin and Lu Sheng and Yu Qiao and Jing Shao", "abstract": "  Large Language Models (LLMs) aim to serve as versatile assistants aligned\nwith human values, as defined by the principles of being helpful, honest, and\nharmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),\ndespite their commendable performance in perception and reasoning tasks, their\nalignment with human values remains largely unexplored, given the complexity of\ndefining hhh dimensions in the visual world and the difficulty in collecting\nrelevant data that accurately mirrors real-world situations. To address this\ngap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for\nassessing alignment with human expectations. Ch3Ef dataset contains 1002\nhuman-annotated data samples, covering 12 domains and 46 tasks based on the hhh\nprinciple. We also present a unified evaluation strategy supporting assessment\nacross various scenarios and different perspectives. Based on the evaluation\nresults, we summarize over 10 key findings that deepen the understanding of\nMLLM capabilities, limitations, and the dynamic relationships between\nevaluation levels, guiding future advancements in the field.\n", "link": "http://arxiv.org/abs/2403.17830v1", "date": "2024-03-26", "relevancy": 1.6078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5455}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4925}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Assessment%20of%20Multimodal%20Large%20Language%20Models%20in%20Alignment%20with%20Human%0A%20%20Values&body=Title%3A%20Assessment%20of%20Multimodal%20Large%20Language%20Models%20in%20Alignment%20with%20Human%0A%20%20Values%0AAuthor%3A%20Zhelun%20Shi%20and%20Zhipin%20Wang%20and%20Hongxing%20Fan%20and%20Zaibin%20Zhang%20and%20Lijun%20Li%20and%20Yongting%20Zhang%20and%20Zhenfei%20Yin%20and%20Lu%20Sheng%20and%20Yu%20Qiao%20and%20Jing%20Shao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20aim%20to%20serve%20as%20versatile%20assistants%20aligned%0Awith%20human%20values%2C%20as%20defined%20by%20the%20principles%20of%20being%20helpful%2C%20honest%2C%20and%0Aharmless%20%28hhh%29.%20However%2C%20in%20terms%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%0Adespite%20their%20commendable%20performance%20in%20perception%20and%20reasoning%20tasks%2C%20their%0Aalignment%20with%20human%20values%20remains%20largely%20unexplored%2C%20given%20the%20complexity%20of%0Adefining%20hhh%20dimensions%20in%20the%20visual%20world%20and%20the%20difficulty%20in%20collecting%0Arelevant%20data%20that%20accurately%20mirrors%20real-world%20situations.%20To%20address%20this%0Agap%2C%20we%20introduce%20Ch3Ef%2C%20a%20Compreh3ensive%20Evaluation%20dataset%20and%20strategy%20for%0Aassessing%20alignment%20with%20human%20expectations.%20Ch3Ef%20dataset%20contains%201002%0Ahuman-annotated%20data%20samples%2C%20covering%2012%20domains%20and%2046%20tasks%20based%20on%20the%20hhh%0Aprinciple.%20We%20also%20present%20a%20unified%20evaluation%20strategy%20supporting%20assessment%0Aacross%20various%20scenarios%20and%20different%20perspectives.%20Based%20on%20the%20evaluation%0Aresults%2C%20we%20summarize%20over%2010%20key%20findings%20that%20deepen%20the%20understanding%20of%0AMLLM%20capabilities%2C%20limitations%2C%20and%20the%20dynamic%20relationships%20between%0Aevaluation%20levels%2C%20guiding%20future%20advancements%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17830v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessment%20of%20Multimodal%20Large%20Language%20Models%20in%20Alignment%20with%20Human%0A%20%20Values&entry.906535625=Zhelun%20Shi%20and%20Zhipin%20Wang%20and%20Hongxing%20Fan%20and%20Zaibin%20Zhang%20and%20Lijun%20Li%20and%20Yongting%20Zhang%20and%20Zhenfei%20Yin%20and%20Lu%20Sheng%20and%20Yu%20Qiao%20and%20Jing%20Shao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20aim%20to%20serve%20as%20versatile%20assistants%20aligned%0Awith%20human%20values%2C%20as%20defined%20by%20the%20principles%20of%20being%20helpful%2C%20honest%2C%20and%0Aharmless%20%28hhh%29.%20However%2C%20in%20terms%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%0Adespite%20their%20commendable%20performance%20in%20perception%20and%20reasoning%20tasks%2C%20their%0Aalignment%20with%20human%20values%20remains%20largely%20unexplored%2C%20given%20the%20complexity%20of%0Adefining%20hhh%20dimensions%20in%20the%20visual%20world%20and%20the%20difficulty%20in%20collecting%0Arelevant%20data%20that%20accurately%20mirrors%20real-world%20situations.%20To%20address%20this%0Agap%2C%20we%20introduce%20Ch3Ef%2C%20a%20Compreh3ensive%20Evaluation%20dataset%20and%20strategy%20for%0Aassessing%20alignment%20with%20human%20expectations.%20Ch3Ef%20dataset%20contains%201002%0Ahuman-annotated%20data%20samples%2C%20covering%2012%20domains%20and%2046%20tasks%20based%20on%20the%20hhh%0Aprinciple.%20We%20also%20present%20a%20unified%20evaluation%20strategy%20supporting%20assessment%0Aacross%20various%20scenarios%20and%20different%20perspectives.%20Based%20on%20the%20evaluation%0Aresults%2C%20we%20summarize%20over%2010%20key%20findings%20that%20deepen%20the%20understanding%20of%0AMLLM%20capabilities%2C%20limitations%2C%20and%20the%20dynamic%20relationships%20between%0Aevaluation%20levels%2C%20guiding%20future%20advancements%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17830v1&entry.124074799=Read"},
{"title": "Safe Explicable Planning", "author": "Akkamahadevi Hanni and Andrew Boateng and Yu Zhang", "abstract": "  Human expectations arise from their understanding of others and the world. In\nthe context of human-AI interaction, this understanding may not align with\nreality, leading to the AI agent failing to meet expectations and compromising\nteam performance. Explicable planning, introduced as a method to bridge this\ngap, aims to reconcile human expectations with the agent's optimal behavior,\nfacilitating interpretable decision-making. However, an unresolved critical\nissue is ensuring safety in explicable planning, as it could result in\nexplicable behaviors that are unsafe. To address this, we propose Safe\nExplicable Planning (SEP), which extends the prior work to support the\nspecification of a safety bound. The goal of SEP is to find behaviors that\nalign with human expectations while adhering to the specified safety criterion.\nOur approach generalizes the consideration of multiple objectives stemming from\nmultiple models rather than a single model, yielding a Pareto set of safe\nexplicable policies. We present both an exact method, guaranteeing finding the\nPareto set, and a more efficient greedy method that finds one of the policies\nin the Pareto set. Additionally, we offer approximate solutions based on state\naggregation to improve scalability. We provide formal proofs that validate the\ndesired theoretical properties of these methods. Evaluation through simulations\nand physical robot experiments confirms the effectiveness of our approach for\nsafe explicable planning.\n", "link": "http://arxiv.org/abs/2304.03773v3", "date": "2024-03-26", "relevancy": 1.5934, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5827}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5247}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5131}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Safe%20Explicable%20Planning&body=Title%3A%20Safe%20Explicable%20Planning%0AAuthor%3A%20Akkamahadevi%20Hanni%20and%20Andrew%20Boateng%20and%20Yu%20Zhang%0AAbstract%3A%20%20%20Human%20expectations%20arise%20from%20their%20understanding%20of%20others%20and%20the%20world.%20In%0Athe%20context%20of%20human-AI%20interaction%2C%20this%20understanding%20may%20not%20align%20with%0Areality%2C%20leading%20to%20the%20AI%20agent%20failing%20to%20meet%20expectations%20and%20compromising%0Ateam%20performance.%20Explicable%20planning%2C%20introduced%20as%20a%20method%20to%20bridge%20this%0Agap%2C%20aims%20to%20reconcile%20human%20expectations%20with%20the%20agent%27s%20optimal%20behavior%2C%0Afacilitating%20interpretable%20decision-making.%20However%2C%20an%20unresolved%20critical%0Aissue%20is%20ensuring%20safety%20in%20explicable%20planning%2C%20as%20it%20could%20result%20in%0Aexplicable%20behaviors%20that%20are%20unsafe.%20To%20address%20this%2C%20we%20propose%20Safe%0AExplicable%20Planning%20%28SEP%29%2C%20which%20extends%20the%20prior%20work%20to%20support%20the%0Aspecification%20of%20a%20safety%20bound.%20The%20goal%20of%20SEP%20is%20to%20find%20behaviors%20that%0Aalign%20with%20human%20expectations%20while%20adhering%20to%20the%20specified%20safety%20criterion.%0AOur%20approach%20generalizes%20the%20consideration%20of%20multiple%20objectives%20stemming%20from%0Amultiple%20models%20rather%20than%20a%20single%20model%2C%20yielding%20a%20Pareto%20set%20of%20safe%0Aexplicable%20policies.%20We%20present%20both%20an%20exact%20method%2C%20guaranteeing%20finding%20the%0APareto%20set%2C%20and%20a%20more%20efficient%20greedy%20method%20that%20finds%20one%20of%20the%20policies%0Ain%20the%20Pareto%20set.%20Additionally%2C%20we%20offer%20approximate%20solutions%20based%20on%20state%0Aaggregation%20to%20improve%20scalability.%20We%20provide%20formal%20proofs%20that%20validate%20the%0Adesired%20theoretical%20properties%20of%20these%20methods.%20Evaluation%20through%20simulations%0Aand%20physical%20robot%20experiments%20confirms%20the%20effectiveness%20of%20our%20approach%20for%0Asafe%20explicable%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03773v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Explicable%20Planning&entry.906535625=Akkamahadevi%20Hanni%20and%20Andrew%20Boateng%20and%20Yu%20Zhang&entry.1292438233=%20%20Human%20expectations%20arise%20from%20their%20understanding%20of%20others%20and%20the%20world.%20In%0Athe%20context%20of%20human-AI%20interaction%2C%20this%20understanding%20may%20not%20align%20with%0Areality%2C%20leading%20to%20the%20AI%20agent%20failing%20to%20meet%20expectations%20and%20compromising%0Ateam%20performance.%20Explicable%20planning%2C%20introduced%20as%20a%20method%20to%20bridge%20this%0Agap%2C%20aims%20to%20reconcile%20human%20expectations%20with%20the%20agent%27s%20optimal%20behavior%2C%0Afacilitating%20interpretable%20decision-making.%20However%2C%20an%20unresolved%20critical%0Aissue%20is%20ensuring%20safety%20in%20explicable%20planning%2C%20as%20it%20could%20result%20in%0Aexplicable%20behaviors%20that%20are%20unsafe.%20To%20address%20this%2C%20we%20propose%20Safe%0AExplicable%20Planning%20%28SEP%29%2C%20which%20extends%20the%20prior%20work%20to%20support%20the%0Aspecification%20of%20a%20safety%20bound.%20The%20goal%20of%20SEP%20is%20to%20find%20behaviors%20that%0Aalign%20with%20human%20expectations%20while%20adhering%20to%20the%20specified%20safety%20criterion.%0AOur%20approach%20generalizes%20the%20consideration%20of%20multiple%20objectives%20stemming%20from%0Amultiple%20models%20rather%20than%20a%20single%20model%2C%20yielding%20a%20Pareto%20set%20of%20safe%0Aexplicable%20policies.%20We%20present%20both%20an%20exact%20method%2C%20guaranteeing%20finding%20the%0APareto%20set%2C%20and%20a%20more%20efficient%20greedy%20method%20that%20finds%20one%20of%20the%20policies%0Ain%20the%20Pareto%20set.%20Additionally%2C%20we%20offer%20approximate%20solutions%20based%20on%20state%0Aaggregation%20to%20improve%20scalability.%20We%20provide%20formal%20proofs%20that%20validate%20the%0Adesired%20theoretical%20properties%20of%20these%20methods.%20Evaluation%20through%20simulations%0Aand%20physical%20robot%20experiments%20confirms%20the%20effectiveness%20of%20our%20approach%20for%0Asafe%20explicable%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03773v3&entry.124074799=Read"},
{"title": "Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving", "author": "Axel Brunnbauer and Luigi Berducci and Peter Priller and Dejan Nickovic and Radu Grosu", "abstract": "  The automated generation of diverse and complex training scenarios has been\nan important ingredient in many complex learning tasks. Especially in\nreal-world application domains, such as autonomous driving, auto-curriculum\ngeneration is considered vital for obtaining robust and general policies.\nHowever, crafting traffic scenarios with multiple, heterogeneous agents is\ntypically considered as a tedious and time-consuming task, especially in more\ncomplex simulation environments. In our work, we introduce MATS-Gym, a\nMulti-Agent Traffic Scenario framework to train agents in CARLA, a\nhigh-fidelity driving simulator. MATS-Gym is a multi-agent training framework\nfor autonomous driving that uses partial scenario specifications to generate\ntraffic scenarios with variable numbers of agents. This paper unifies various\nexisting approaches to traffic scenario description into a single training\nframework and demonstrates how it can be integrated with techniques from\nunsupervised environment design to automate the generation of adaptive\nauto-curricula. The code is available at\nhttps://github.com/AutonomousDrivingExaminer/mats-gym.\n", "link": "http://arxiv.org/abs/2403.17805v1", "date": "2024-03-26", "relevancy": 1.5931, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5732}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5186}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scenario-Based%20Curriculum%20Generation%20for%20Multi-Agent%20Autonomous%20Driving&body=Title%3A%20Scenario-Based%20Curriculum%20Generation%20for%20Multi-Agent%20Autonomous%20Driving%0AAuthor%3A%20Axel%20Brunnbauer%20and%20Luigi%20Berducci%20and%20Peter%20Priller%20and%20Dejan%20Nickovic%20and%20Radu%20Grosu%0AAbstract%3A%20%20%20The%20automated%20generation%20of%20diverse%20and%20complex%20training%20scenarios%20has%20been%0Aan%20important%20ingredient%20in%20many%20complex%20learning%20tasks.%20Especially%20in%0Areal-world%20application%20domains%2C%20such%20as%20autonomous%20driving%2C%20auto-curriculum%0Ageneration%20is%20considered%20vital%20for%20obtaining%20robust%20and%20general%20policies.%0AHowever%2C%20crafting%20traffic%20scenarios%20with%20multiple%2C%20heterogeneous%20agents%20is%0Atypically%20considered%20as%20a%20tedious%20and%20time-consuming%20task%2C%20especially%20in%20more%0Acomplex%20simulation%20environments.%20In%20our%20work%2C%20we%20introduce%20MATS-Gym%2C%20a%0AMulti-Agent%20Traffic%20Scenario%20framework%20to%20train%20agents%20in%20CARLA%2C%20a%0Ahigh-fidelity%20driving%20simulator.%20MATS-Gym%20is%20a%20multi-agent%20training%20framework%0Afor%20autonomous%20driving%20that%20uses%20partial%20scenario%20specifications%20to%20generate%0Atraffic%20scenarios%20with%20variable%20numbers%20of%20agents.%20This%20paper%20unifies%20various%0Aexisting%20approaches%20to%20traffic%20scenario%20description%20into%20a%20single%20training%0Aframework%20and%20demonstrates%20how%20it%20can%20be%20integrated%20with%20techniques%20from%0Aunsupervised%20environment%20design%20to%20automate%20the%20generation%20of%20adaptive%0Aauto-curricula.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AutonomousDrivingExaminer/mats-gym.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17805v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scenario-Based%20Curriculum%20Generation%20for%20Multi-Agent%20Autonomous%20Driving&entry.906535625=Axel%20Brunnbauer%20and%20Luigi%20Berducci%20and%20Peter%20Priller%20and%20Dejan%20Nickovic%20and%20Radu%20Grosu&entry.1292438233=%20%20The%20automated%20generation%20of%20diverse%20and%20complex%20training%20scenarios%20has%20been%0Aan%20important%20ingredient%20in%20many%20complex%20learning%20tasks.%20Especially%20in%0Areal-world%20application%20domains%2C%20such%20as%20autonomous%20driving%2C%20auto-curriculum%0Ageneration%20is%20considered%20vital%20for%20obtaining%20robust%20and%20general%20policies.%0AHowever%2C%20crafting%20traffic%20scenarios%20with%20multiple%2C%20heterogeneous%20agents%20is%0Atypically%20considered%20as%20a%20tedious%20and%20time-consuming%20task%2C%20especially%20in%20more%0Acomplex%20simulation%20environments.%20In%20our%20work%2C%20we%20introduce%20MATS-Gym%2C%20a%0AMulti-Agent%20Traffic%20Scenario%20framework%20to%20train%20agents%20in%20CARLA%2C%20a%0Ahigh-fidelity%20driving%20simulator.%20MATS-Gym%20is%20a%20multi-agent%20training%20framework%0Afor%20autonomous%20driving%20that%20uses%20partial%20scenario%20specifications%20to%20generate%0Atraffic%20scenarios%20with%20variable%20numbers%20of%20agents.%20This%20paper%20unifies%20various%0Aexisting%20approaches%20to%20traffic%20scenario%20description%20into%20a%20single%20training%0Aframework%20and%20demonstrates%20how%20it%20can%20be%20integrated%20with%20techniques%20from%0Aunsupervised%20environment%20design%20to%20automate%20the%20generation%20of%20adaptive%0Aauto-curricula.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AutonomousDrivingExaminer/mats-gym.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17805v1&entry.124074799=Read"},
{"title": "Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models", "author": "Minchan Kim and Minyeong Kim and Junik Bae and Suhwan Choi and Sungkyung Kim and Buru Chang", "abstract": "  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n", "link": "http://arxiv.org/abs/2403.16167v2", "date": "2024-03-26", "relevancy": 1.5928, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5368}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5329}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5278}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Semantic%20Reconstruction%20to%20Mitigate%20Hallucinations%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20Exploiting%20Semantic%20Reconstruction%20to%20Mitigate%20Hallucinations%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Minchan%20Kim%20and%20Minyeong%20Kim%20and%20Junik%20Bae%20and%20Suhwan%20Choi%20and%20Sungkyung%20Kim%20and%20Buru%20Chang%0AAbstract%3A%20%20%20Hallucinations%20in%20vision-language%20models%20pose%20a%20significant%20challenge%20to%0Atheir%20reliability%2C%20particularly%20in%20the%20generation%20of%20long%20captions.%20Current%0Amethods%20fall%20short%20of%20accurately%20identifying%20and%20mitigating%20these%0Ahallucinations.%20To%20address%20this%20issue%2C%20we%20introduce%20ESREAL%2C%20a%20novel%0Aunsupervised%20learning%20framework%20designed%20to%20suppress%20the%20generation%20of%0Ahallucinations%20through%20accurate%20localization%20and%20penalization%20of%20hallucinated%0Atokens.%20Initially%2C%20ESREAL%20creates%20a%20reconstructed%20image%20based%20on%20the%20generated%0Acaption%20and%20aligns%20its%20corresponding%20regions%20with%20those%20of%20the%20original%20image.%0AThis%20semantic%20reconstruction%20aids%20in%20identifying%20both%20the%20presence%20and%20type%20of%0Atoken-level%20hallucinations%20within%20the%20generated%20caption.%20Subsequently%2C%20ESREAL%0Acomputes%20token-level%20hallucination%20scores%20by%20assessing%20the%20semantic%20similarity%0Aof%20aligned%20regions%20based%20on%20the%20type%20of%20hallucination.%20Finally%2C%20ESREAL%20employs%0Aa%20proximal%20policy%20optimization%20algorithm%2C%20where%20it%20selectively%20penalizes%0Ahallucinated%20tokens%20according%20to%20their%20token-level%20hallucination%20scores.%20Our%0Aframework%20notably%20reduces%20hallucinations%20in%20LLaVA%2C%20InstructBLIP%2C%20and%20mPLUG-Owl2%0Aby%2032.81%25%2C%2027.08%25%2C%20and%207.46%25%20on%20the%20CHAIR%20metric.%20This%20improvement%20is%20achieved%0Asolely%20through%20signals%20derived%20from%20the%20image%20itself%2C%20without%20the%20need%20for%20any%0Aimage-text%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16167v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Semantic%20Reconstruction%20to%20Mitigate%20Hallucinations%20in%0A%20%20Vision-Language%20Models&entry.906535625=Minchan%20Kim%20and%20Minyeong%20Kim%20and%20Junik%20Bae%20and%20Suhwan%20Choi%20and%20Sungkyung%20Kim%20and%20Buru%20Chang&entry.1292438233=%20%20Hallucinations%20in%20vision-language%20models%20pose%20a%20significant%20challenge%20to%0Atheir%20reliability%2C%20particularly%20in%20the%20generation%20of%20long%20captions.%20Current%0Amethods%20fall%20short%20of%20accurately%20identifying%20and%20mitigating%20these%0Ahallucinations.%20To%20address%20this%20issue%2C%20we%20introduce%20ESREAL%2C%20a%20novel%0Aunsupervised%20learning%20framework%20designed%20to%20suppress%20the%20generation%20of%0Ahallucinations%20through%20accurate%20localization%20and%20penalization%20of%20hallucinated%0Atokens.%20Initially%2C%20ESREAL%20creates%20a%20reconstructed%20image%20based%20on%20the%20generated%0Acaption%20and%20aligns%20its%20corresponding%20regions%20with%20those%20of%20the%20original%20image.%0AThis%20semantic%20reconstruction%20aids%20in%20identifying%20both%20the%20presence%20and%20type%20of%0Atoken-level%20hallucinations%20within%20the%20generated%20caption.%20Subsequently%2C%20ESREAL%0Acomputes%20token-level%20hallucination%20scores%20by%20assessing%20the%20semantic%20similarity%0Aof%20aligned%20regions%20based%20on%20the%20type%20of%20hallucination.%20Finally%2C%20ESREAL%20employs%0Aa%20proximal%20policy%20optimization%20algorithm%2C%20where%20it%20selectively%20penalizes%0Ahallucinated%20tokens%20according%20to%20their%20token-level%20hallucination%20scores.%20Our%0Aframework%20notably%20reduces%20hallucinations%20in%20LLaVA%2C%20InstructBLIP%2C%20and%20mPLUG-Owl2%0Aby%2032.81%25%2C%2027.08%25%2C%20and%207.46%25%20on%20the%20CHAIR%20metric.%20This%20improvement%20is%20achieved%0Asolely%20through%20signals%20derived%20from%20the%20image%20itself%2C%20without%20the%20need%20for%20any%0Aimage-text%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16167v2&entry.124074799=Read"},
{"title": "An optimal control perspective on diffusion-based generative modeling", "author": "Julius Berner and Lorenz Richter and Karen Ullrich", "abstract": "  We establish a connection between stochastic optimal control and generative\nmodels based on stochastic differential equations (SDEs), such as recently\ndeveloped diffusion probabilistic models. In particular, we derive a\nHamilton-Jacobi-Bellman equation that governs the evolution of the\nlog-densities of the underlying SDE marginals. This perspective allows to\ntransfer methods from optimal control theory to generative modeling. First, we\nshow that the evidence lower bound is a direct consequence of the well-known\nverification theorem from control theory. Further, we can formulate\ndiffusion-based generative modeling as a minimization of the Kullback-Leibler\ndivergence between suitable measures in path space. Finally, we develop a novel\ndiffusion-based method for sampling from unnormalized densities -- a problem\nfrequently occurring in statistics and computational sciences. We demonstrate\nthat our time-reversed diffusion sampler (DIS) can outperform other\ndiffusion-based sampling approaches on multiple numerical examples.\n", "link": "http://arxiv.org/abs/2211.01364v3", "date": "2024-03-26", "relevancy": 1.588, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5411}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.539}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5207}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20optimal%20control%20perspective%20on%20diffusion-based%20generative%20modeling&body=Title%3A%20An%20optimal%20control%20perspective%20on%20diffusion-based%20generative%20modeling%0AAuthor%3A%20Julius%20Berner%20and%20Lorenz%20Richter%20and%20Karen%20Ullrich%0AAbstract%3A%20%20%20We%20establish%20a%20connection%20between%20stochastic%20optimal%20control%20and%20generative%0Amodels%20based%20on%20stochastic%20differential%20equations%20%28SDEs%29%2C%20such%20as%20recently%0Adeveloped%20diffusion%20probabilistic%20models.%20In%20particular%2C%20we%20derive%20a%0AHamilton-Jacobi-Bellman%20equation%20that%20governs%20the%20evolution%20of%20the%0Alog-densities%20of%20the%20underlying%20SDE%20marginals.%20This%20perspective%20allows%20to%0Atransfer%20methods%20from%20optimal%20control%20theory%20to%20generative%20modeling.%20First%2C%20we%0Ashow%20that%20the%20evidence%20lower%20bound%20is%20a%20direct%20consequence%20of%20the%20well-known%0Averification%20theorem%20from%20control%20theory.%20Further%2C%20we%20can%20formulate%0Adiffusion-based%20generative%20modeling%20as%20a%20minimization%20of%20the%20Kullback-Leibler%0Adivergence%20between%20suitable%20measures%20in%20path%20space.%20Finally%2C%20we%20develop%20a%20novel%0Adiffusion-based%20method%20for%20sampling%20from%20unnormalized%20densities%20--%20a%20problem%0Afrequently%20occurring%20in%20statistics%20and%20computational%20sciences.%20We%20demonstrate%0Athat%20our%20time-reversed%20diffusion%20sampler%20%28DIS%29%20can%20outperform%20other%0Adiffusion-based%20sampling%20approaches%20on%20multiple%20numerical%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.01364v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20optimal%20control%20perspective%20on%20diffusion-based%20generative%20modeling&entry.906535625=Julius%20Berner%20and%20Lorenz%20Richter%20and%20Karen%20Ullrich&entry.1292438233=%20%20We%20establish%20a%20connection%20between%20stochastic%20optimal%20control%20and%20generative%0Amodels%20based%20on%20stochastic%20differential%20equations%20%28SDEs%29%2C%20such%20as%20recently%0Adeveloped%20diffusion%20probabilistic%20models.%20In%20particular%2C%20we%20derive%20a%0AHamilton-Jacobi-Bellman%20equation%20that%20governs%20the%20evolution%20of%20the%0Alog-densities%20of%20the%20underlying%20SDE%20marginals.%20This%20perspective%20allows%20to%0Atransfer%20methods%20from%20optimal%20control%20theory%20to%20generative%20modeling.%20First%2C%20we%0Ashow%20that%20the%20evidence%20lower%20bound%20is%20a%20direct%20consequence%20of%20the%20well-known%0Averification%20theorem%20from%20control%20theory.%20Further%2C%20we%20can%20formulate%0Adiffusion-based%20generative%20modeling%20as%20a%20minimization%20of%20the%20Kullback-Leibler%0Adivergence%20between%20suitable%20measures%20in%20path%20space.%20Finally%2C%20we%20develop%20a%20novel%0Adiffusion-based%20method%20for%20sampling%20from%20unnormalized%20densities%20--%20a%20problem%0Afrequently%20occurring%20in%20statistics%20and%20computational%20sciences.%20We%20demonstrate%0Athat%20our%20time-reversed%20diffusion%20sampler%20%28DIS%29%20can%20outperform%20other%0Adiffusion-based%20sampling%20approaches%20on%20multiple%20numerical%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.01364v3&entry.124074799=Read"},
{"title": "Activations and Gradients Compression for Model-Parallel Training", "author": "Mikhail Rudakov and Aleksandr Beznosikov and Yaroslav Kholodov and Alexander Gasnikov", "abstract": "  Large neural networks require enormous computational clusters of machines.\nModel-parallel training, when the model architecture is partitioned\nsequentially between workers, is a popular approach for training modern models.\nInformation compression can be applied to decrease workers communication time,\nas it is often a bottleneck in such systems. This work explores how\nsimultaneous compression of activations and gradients in model-parallel\ndistributed training setup affects convergence. We analyze compression methods\nsuch as quantization and TopK compression, and also experiment with error\ncompensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error\nfeedback approach. We conduct experiments on image classification and language\nmodel fine-tuning tasks. Our findings demonstrate that gradients require milder\ncompression rates than activations. We observe that $K=10\\%$ is the lowest TopK\ncompression level, which does not harm model convergence severely. Experiments\nalso show that models trained with TopK perform well only when compression is\nalso applied during inference. We find that error feedback techniques do not\nimprove model-parallel training compared to plain compression, but allow model\ninference without compression with almost no quality drop. Finally, when\napplied with the AQ-SGD approach, TopK stronger than with $ K=30\\%$ worsens\nmodel performance significantly.\n", "link": "http://arxiv.org/abs/2401.07788v2", "date": "2024-03-26", "relevancy": 1.5713, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5441}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5235}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5157}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Activations%20and%20Gradients%20Compression%20for%20Model-Parallel%20Training&body=Title%3A%20Activations%20and%20Gradients%20Compression%20for%20Model-Parallel%20Training%0AAuthor%3A%20Mikhail%20Rudakov%20and%20Aleksandr%20Beznosikov%20and%20Yaroslav%20Kholodov%20and%20Alexander%20Gasnikov%0AAbstract%3A%20%20%20Large%20neural%20networks%20require%20enormous%20computational%20clusters%20of%20machines.%0AModel-parallel%20training%2C%20when%20the%20model%20architecture%20is%20partitioned%0Asequentially%20between%20workers%2C%20is%20a%20popular%20approach%20for%20training%20modern%20models.%0AInformation%20compression%20can%20be%20applied%20to%20decrease%20workers%20communication%20time%2C%0Aas%20it%20is%20often%20a%20bottleneck%20in%20such%20systems.%20This%20work%20explores%20how%0Asimultaneous%20compression%20of%20activations%20and%20gradients%20in%20model-parallel%0Adistributed%20training%20setup%20affects%20convergence.%20We%20analyze%20compression%20methods%0Asuch%20as%20quantization%20and%20TopK%20compression%2C%20and%20also%20experiment%20with%20error%0Acompensation%20techniques.%20Moreover%2C%20we%20employ%20TopK%20with%20AQ-SGD%20per-batch%20error%0Afeedback%20approach.%20We%20conduct%20experiments%20on%20image%20classification%20and%20language%0Amodel%20fine-tuning%20tasks.%20Our%20findings%20demonstrate%20that%20gradients%20require%20milder%0Acompression%20rates%20than%20activations.%20We%20observe%20that%20%24K%3D10%5C%25%24%20is%20the%20lowest%20TopK%0Acompression%20level%2C%20which%20does%20not%20harm%20model%20convergence%20severely.%20Experiments%0Aalso%20show%20that%20models%20trained%20with%20TopK%20perform%20well%20only%20when%20compression%20is%0Aalso%20applied%20during%20inference.%20We%20find%20that%20error%20feedback%20techniques%20do%20not%0Aimprove%20model-parallel%20training%20compared%20to%20plain%20compression%2C%20but%20allow%20model%0Ainference%20without%20compression%20with%20almost%20no%20quality%20drop.%20Finally%2C%20when%0Aapplied%20with%20the%20AQ-SGD%20approach%2C%20TopK%20stronger%20than%20with%20%24%20K%3D30%5C%25%24%20worsens%0Amodel%20performance%20significantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07788v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activations%20and%20Gradients%20Compression%20for%20Model-Parallel%20Training&entry.906535625=Mikhail%20Rudakov%20and%20Aleksandr%20Beznosikov%20and%20Yaroslav%20Kholodov%20and%20Alexander%20Gasnikov&entry.1292438233=%20%20Large%20neural%20networks%20require%20enormous%20computational%20clusters%20of%20machines.%0AModel-parallel%20training%2C%20when%20the%20model%20architecture%20is%20partitioned%0Asequentially%20between%20workers%2C%20is%20a%20popular%20approach%20for%20training%20modern%20models.%0AInformation%20compression%20can%20be%20applied%20to%20decrease%20workers%20communication%20time%2C%0Aas%20it%20is%20often%20a%20bottleneck%20in%20such%20systems.%20This%20work%20explores%20how%0Asimultaneous%20compression%20of%20activations%20and%20gradients%20in%20model-parallel%0Adistributed%20training%20setup%20affects%20convergence.%20We%20analyze%20compression%20methods%0Asuch%20as%20quantization%20and%20TopK%20compression%2C%20and%20also%20experiment%20with%20error%0Acompensation%20techniques.%20Moreover%2C%20we%20employ%20TopK%20with%20AQ-SGD%20per-batch%20error%0Afeedback%20approach.%20We%20conduct%20experiments%20on%20image%20classification%20and%20language%0Amodel%20fine-tuning%20tasks.%20Our%20findings%20demonstrate%20that%20gradients%20require%20milder%0Acompression%20rates%20than%20activations.%20We%20observe%20that%20%24K%3D10%5C%25%24%20is%20the%20lowest%20TopK%0Acompression%20level%2C%20which%20does%20not%20harm%20model%20convergence%20severely.%20Experiments%0Aalso%20show%20that%20models%20trained%20with%20TopK%20perform%20well%20only%20when%20compression%20is%0Aalso%20applied%20during%20inference.%20We%20find%20that%20error%20feedback%20techniques%20do%20not%0Aimprove%20model-parallel%20training%20compared%20to%20plain%20compression%2C%20but%20allow%20model%0Ainference%20without%20compression%20with%20almost%20no%20quality%20drop.%20Finally%2C%20when%0Aapplied%20with%20the%20AQ-SGD%20approach%2C%20TopK%20stronger%20than%20with%20%24%20K%3D30%5C%25%24%20worsens%0Amodel%20performance%20significantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07788v2&entry.124074799=Read"},
{"title": "Low-Latency Neural Stereo Streaming", "author": "Qiqi Hou and Farzad Farhadzadeh and Amir Said and Guillaume Sautiere and Hoang Le", "abstract": "  The rise of new video modalities like virtual reality or autonomous driving\nhas increased the demand for efficient multi-view video compression methods,\nboth in terms of rate-distortion (R-D) performance and in terms of delay and\nruntime. While most recent stereo video compression approaches have shown\npromising performance, they compress left and right views sequentially, leading\nto poor parallelization and runtime performance. This work presents Low-Latency\nneural codec for Stereo video Streaming (LLSS), a novel parallel stereo video\ncoding method designed for fast and efficient low-latency stereo video\nstreaming. Instead of using a sequential cross-view motion compensation like\nexisting methods, LLSS introduces a bidirectional feature shifting module to\ndirectly exploit mutual information among views and encode them effectively\nwith a joint cross-view prior model for entropy coding. Thanks to this design,\nLLSS processes left and right views in parallel, minimizing latency; all while\nsubstantially improving R-D performance compared to both existing neural and\nconventional codecs.\n", "link": "http://arxiv.org/abs/2403.17879v1", "date": "2024-03-26", "relevancy": 1.5459, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5334}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5264}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4694}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Low-Latency%20Neural%20Stereo%20Streaming&body=Title%3A%20Low-Latency%20Neural%20Stereo%20Streaming%0AAuthor%3A%20Qiqi%20Hou%20and%20Farzad%20Farhadzadeh%20and%20Amir%20Said%20and%20Guillaume%20Sautiere%20and%20Hoang%20Le%0AAbstract%3A%20%20%20The%20rise%20of%20new%20video%20modalities%20like%20virtual%20reality%20or%20autonomous%20driving%0Ahas%20increased%20the%20demand%20for%20efficient%20multi-view%20video%20compression%20methods%2C%0Aboth%20in%20terms%20of%20rate-distortion%20%28R-D%29%20performance%20and%20in%20terms%20of%20delay%20and%0Aruntime.%20While%20most%20recent%20stereo%20video%20compression%20approaches%20have%20shown%0Apromising%20performance%2C%20they%20compress%20left%20and%20right%20views%20sequentially%2C%20leading%0Ato%20poor%20parallelization%20and%20runtime%20performance.%20This%20work%20presents%20Low-Latency%0Aneural%20codec%20for%20Stereo%20video%20Streaming%20%28LLSS%29%2C%20a%20novel%20parallel%20stereo%20video%0Acoding%20method%20designed%20for%20fast%20and%20efficient%20low-latency%20stereo%20video%0Astreaming.%20Instead%20of%20using%20a%20sequential%20cross-view%20motion%20compensation%20like%0Aexisting%20methods%2C%20LLSS%20introduces%20a%20bidirectional%20feature%20shifting%20module%20to%0Adirectly%20exploit%20mutual%20information%20among%20views%20and%20encode%20them%20effectively%0Awith%20a%20joint%20cross-view%20prior%20model%20for%20entropy%20coding.%20Thanks%20to%20this%20design%2C%0ALLSS%20processes%20left%20and%20right%20views%20in%20parallel%2C%20minimizing%20latency%3B%20all%20while%0Asubstantially%20improving%20R-D%20performance%20compared%20to%20both%20existing%20neural%20and%0Aconventional%20codecs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17879v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Latency%20Neural%20Stereo%20Streaming&entry.906535625=Qiqi%20Hou%20and%20Farzad%20Farhadzadeh%20and%20Amir%20Said%20and%20Guillaume%20Sautiere%20and%20Hoang%20Le&entry.1292438233=%20%20The%20rise%20of%20new%20video%20modalities%20like%20virtual%20reality%20or%20autonomous%20driving%0Ahas%20increased%20the%20demand%20for%20efficient%20multi-view%20video%20compression%20methods%2C%0Aboth%20in%20terms%20of%20rate-distortion%20%28R-D%29%20performance%20and%20in%20terms%20of%20delay%20and%0Aruntime.%20While%20most%20recent%20stereo%20video%20compression%20approaches%20have%20shown%0Apromising%20performance%2C%20they%20compress%20left%20and%20right%20views%20sequentially%2C%20leading%0Ato%20poor%20parallelization%20and%20runtime%20performance.%20This%20work%20presents%20Low-Latency%0Aneural%20codec%20for%20Stereo%20video%20Streaming%20%28LLSS%29%2C%20a%20novel%20parallel%20stereo%20video%0Acoding%20method%20designed%20for%20fast%20and%20efficient%20low-latency%20stereo%20video%0Astreaming.%20Instead%20of%20using%20a%20sequential%20cross-view%20motion%20compensation%20like%0Aexisting%20methods%2C%20LLSS%20introduces%20a%20bidirectional%20feature%20shifting%20module%20to%0Adirectly%20exploit%20mutual%20information%20among%20views%20and%20encode%20them%20effectively%0Awith%20a%20joint%20cross-view%20prior%20model%20for%20entropy%20coding.%20Thanks%20to%20this%20design%2C%0ALLSS%20processes%20left%20and%20right%20views%20in%20parallel%2C%20minimizing%20latency%3B%20all%20while%0Asubstantially%20improving%20R-D%20performance%20compared%20to%20both%20existing%20neural%20and%0Aconventional%20codecs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17879v1&entry.124074799=Read"},
{"title": "Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models\n  Versus Fine-Tuned Vision Transformers in Image-Based Security Applications", "author": "Fouad Trad and Ali Chehab", "abstract": "  The success of Large Language Models (LLMs) has led to a parallel rise in the\ndevelopment of Large Multimodal Models (LMMs), such as Gemini-pro, which have\nbegun to transform a variety of applications. These sophisticated multimodal\nmodels are designed to interpret and analyze complex data, integrating both\ntextual and visual information on a scale previously unattainable, opening new\navenues for a range of applications. This paper investigates the applicability\nand effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision\nTransformer (ViT) models in addressing critical security challenges. We focus\non two distinct tasks: a visually evident task of detecting simple triggers,\nsuch as small squares in images, indicative of potential backdoors, and a\nnon-visually evident task of malware classification through visual\nrepresentations. Our results highlight a significant divergence in performance,\nwith Gemini-pro falling short in accuracy and reliability when compared to\nfine-tuned ViT models. The ViT models, on the other hand, demonstrate\nexceptional accuracy, achieving near-perfect performance on both tasks. This\nstudy not only showcases the strengths and limitations of prompt-engineered\nLMMs in cybersecurity applications but also emphasizes the unmatched efficacy\nof fine-tuned ViT models for precise and dependable tasks.\n", "link": "http://arxiv.org/abs/2403.17787v1", "date": "2024-03-26", "relevancy": 1.5381, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5212}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5041}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4999}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Efficacy%20of%20Prompt-Engineered%20Large%20Multimodal%20Models%0A%20%20Versus%20Fine-Tuned%20Vision%20Transformers%20in%20Image-Based%20Security%20Applications&body=Title%3A%20Evaluating%20the%20Efficacy%20of%20Prompt-Engineered%20Large%20Multimodal%20Models%0A%20%20Versus%20Fine-Tuned%20Vision%20Transformers%20in%20Image-Based%20Security%20Applications%0AAuthor%3A%20Fouad%20Trad%20and%20Ali%20Chehab%0AAbstract%3A%20%20%20The%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20a%20parallel%20rise%20in%20the%0Adevelopment%20of%20Large%20Multimodal%20Models%20%28LMMs%29%2C%20such%20as%20Gemini-pro%2C%20which%20have%0Abegun%20to%20transform%20a%20variety%20of%20applications.%20These%20sophisticated%20multimodal%0Amodels%20are%20designed%20to%20interpret%20and%20analyze%20complex%20data%2C%20integrating%20both%0Atextual%20and%20visual%20information%20on%20a%20scale%20previously%20unattainable%2C%20opening%20new%0Aavenues%20for%20a%20range%20of%20applications.%20This%20paper%20investigates%20the%20applicability%0Aand%20effectiveness%20of%20prompt-engineered%20Gemini-pro%20LMMs%20versus%20fine-tuned%20Vision%0ATransformer%20%28ViT%29%20models%20in%20addressing%20critical%20security%20challenges.%20We%20focus%0Aon%20two%20distinct%20tasks%3A%20a%20visually%20evident%20task%20of%20detecting%20simple%20triggers%2C%0Asuch%20as%20small%20squares%20in%20images%2C%20indicative%20of%20potential%20backdoors%2C%20and%20a%0Anon-visually%20evident%20task%20of%20malware%20classification%20through%20visual%0Arepresentations.%20Our%20results%20highlight%20a%20significant%20divergence%20in%20performance%2C%0Awith%20Gemini-pro%20falling%20short%20in%20accuracy%20and%20reliability%20when%20compared%20to%0Afine-tuned%20ViT%20models.%20The%20ViT%20models%2C%20on%20the%20other%20hand%2C%20demonstrate%0Aexceptional%20accuracy%2C%20achieving%20near-perfect%20performance%20on%20both%20tasks.%20This%0Astudy%20not%20only%20showcases%20the%20strengths%20and%20limitations%20of%20prompt-engineered%0ALMMs%20in%20cybersecurity%20applications%20but%20also%20emphasizes%20the%20unmatched%20efficacy%0Aof%20fine-tuned%20ViT%20models%20for%20precise%20and%20dependable%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17787v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Efficacy%20of%20Prompt-Engineered%20Large%20Multimodal%20Models%0A%20%20Versus%20Fine-Tuned%20Vision%20Transformers%20in%20Image-Based%20Security%20Applications&entry.906535625=Fouad%20Trad%20and%20Ali%20Chehab&entry.1292438233=%20%20The%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20a%20parallel%20rise%20in%20the%0Adevelopment%20of%20Large%20Multimodal%20Models%20%28LMMs%29%2C%20such%20as%20Gemini-pro%2C%20which%20have%0Abegun%20to%20transform%20a%20variety%20of%20applications.%20These%20sophisticated%20multimodal%0Amodels%20are%20designed%20to%20interpret%20and%20analyze%20complex%20data%2C%20integrating%20both%0Atextual%20and%20visual%20information%20on%20a%20scale%20previously%20unattainable%2C%20opening%20new%0Aavenues%20for%20a%20range%20of%20applications.%20This%20paper%20investigates%20the%20applicability%0Aand%20effectiveness%20of%20prompt-engineered%20Gemini-pro%20LMMs%20versus%20fine-tuned%20Vision%0ATransformer%20%28ViT%29%20models%20in%20addressing%20critical%20security%20challenges.%20We%20focus%0Aon%20two%20distinct%20tasks%3A%20a%20visually%20evident%20task%20of%20detecting%20simple%20triggers%2C%0Asuch%20as%20small%20squares%20in%20images%2C%20indicative%20of%20potential%20backdoors%2C%20and%20a%0Anon-visually%20evident%20task%20of%20malware%20classification%20through%20visual%0Arepresentations.%20Our%20results%20highlight%20a%20significant%20divergence%20in%20performance%2C%0Awith%20Gemini-pro%20falling%20short%20in%20accuracy%20and%20reliability%20when%20compared%20to%0Afine-tuned%20ViT%20models.%20The%20ViT%20models%2C%20on%20the%20other%20hand%2C%20demonstrate%0Aexceptional%20accuracy%2C%20achieving%20near-perfect%20performance%20on%20both%20tasks.%20This%0Astudy%20not%20only%20showcases%20the%20strengths%20and%20limitations%20of%20prompt-engineered%0ALMMs%20in%20cybersecurity%20applications%20but%20also%20emphasizes%20the%20unmatched%20efficacy%0Aof%20fine-tuned%20ViT%20models%20for%20precise%20and%20dependable%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17787v1&entry.124074799=Read"},
{"title": "Resilient source seeking with robot swarms", "author": "Antonio Acuaviva and Jesus Bautista and Weijia Yao and Juan Jimenez and Hector Garcia de Marina", "abstract": "  We present a solution for locating the source, or maximum, of an unknown\nscalar field using a swarm of mobile robots. Unlike relying on the traditional\ngradient information, the swarm determines an ascending direction to approach\nthe source with arbitrary precision. The ascending direction is calculated from\nmeasurements of the field strength at the robot locations and their relative\npositions concerning the centroid. Rather than focusing on individual robots,\nwe focus the analysis on the density of robots per unit area to guarantee a\nmore resilient swarm, i.e., the functionality remains even if individuals go\nmissing or are misplaced during the mission. We reinforce the robustness of the\nalgorithm by providing sufficient conditions for the swarm shape so that the\nascending direction is almost parallel to the gradient. The swarm can respond\nto an unexpected environment by morphing its shape and exploiting the existence\nof multiple ascending directions. Finally, we validate our approach numerically\nwith hundreds of robots. The fact that a large number of robots always\ncalculate an ascending direction compensates for the loss of individuals and\nmitigates issues arising from the actuator and sensor noises.\n", "link": "http://arxiv.org/abs/2309.02937v2", "date": "2024-03-26", "relevancy": 1.5149, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5129}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4979}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4922}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Resilient%20source%20seeking%20with%20robot%20swarms&body=Title%3A%20Resilient%20source%20seeking%20with%20robot%20swarms%0AAuthor%3A%20Antonio%20Acuaviva%20and%20Jesus%20Bautista%20and%20Weijia%20Yao%20and%20Juan%20Jimenez%20and%20Hector%20Garcia%20de%20Marina%0AAbstract%3A%20%20%20We%20present%20a%20solution%20for%20locating%20the%20source%2C%20or%20maximum%2C%20of%20an%20unknown%0Ascalar%20field%20using%20a%20swarm%20of%20mobile%20robots.%20Unlike%20relying%20on%20the%20traditional%0Agradient%20information%2C%20the%20swarm%20determines%20an%20ascending%20direction%20to%20approach%0Athe%20source%20with%20arbitrary%20precision.%20The%20ascending%20direction%20is%20calculated%20from%0Ameasurements%20of%20the%20field%20strength%20at%20the%20robot%20locations%20and%20their%20relative%0Apositions%20concerning%20the%20centroid.%20Rather%20than%20focusing%20on%20individual%20robots%2C%0Awe%20focus%20the%20analysis%20on%20the%20density%20of%20robots%20per%20unit%20area%20to%20guarantee%20a%0Amore%20resilient%20swarm%2C%20i.e.%2C%20the%20functionality%20remains%20even%20if%20individuals%20go%0Amissing%20or%20are%20misplaced%20during%20the%20mission.%20We%20reinforce%20the%20robustness%20of%20the%0Aalgorithm%20by%20providing%20sufficient%20conditions%20for%20the%20swarm%20shape%20so%20that%20the%0Aascending%20direction%20is%20almost%20parallel%20to%20the%20gradient.%20The%20swarm%20can%20respond%0Ato%20an%20unexpected%20environment%20by%20morphing%20its%20shape%20and%20exploiting%20the%20existence%0Aof%20multiple%20ascending%20directions.%20Finally%2C%20we%20validate%20our%20approach%20numerically%0Awith%20hundreds%20of%20robots.%20The%20fact%20that%20a%20large%20number%20of%20robots%20always%0Acalculate%20an%20ascending%20direction%20compensates%20for%20the%20loss%20of%20individuals%20and%0Amitigates%20issues%20arising%20from%20the%20actuator%20and%20sensor%20noises.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.02937v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resilient%20source%20seeking%20with%20robot%20swarms&entry.906535625=Antonio%20Acuaviva%20and%20Jesus%20Bautista%20and%20Weijia%20Yao%20and%20Juan%20Jimenez%20and%20Hector%20Garcia%20de%20Marina&entry.1292438233=%20%20We%20present%20a%20solution%20for%20locating%20the%20source%2C%20or%20maximum%2C%20of%20an%20unknown%0Ascalar%20field%20using%20a%20swarm%20of%20mobile%20robots.%20Unlike%20relying%20on%20the%20traditional%0Agradient%20information%2C%20the%20swarm%20determines%20an%20ascending%20direction%20to%20approach%0Athe%20source%20with%20arbitrary%20precision.%20The%20ascending%20direction%20is%20calculated%20from%0Ameasurements%20of%20the%20field%20strength%20at%20the%20robot%20locations%20and%20their%20relative%0Apositions%20concerning%20the%20centroid.%20Rather%20than%20focusing%20on%20individual%20robots%2C%0Awe%20focus%20the%20analysis%20on%20the%20density%20of%20robots%20per%20unit%20area%20to%20guarantee%20a%0Amore%20resilient%20swarm%2C%20i.e.%2C%20the%20functionality%20remains%20even%20if%20individuals%20go%0Amissing%20or%20are%20misplaced%20during%20the%20mission.%20We%20reinforce%20the%20robustness%20of%20the%0Aalgorithm%20by%20providing%20sufficient%20conditions%20for%20the%20swarm%20shape%20so%20that%20the%0Aascending%20direction%20is%20almost%20parallel%20to%20the%20gradient.%20The%20swarm%20can%20respond%0Ato%20an%20unexpected%20environment%20by%20morphing%20its%20shape%20and%20exploiting%20the%20existence%0Aof%20multiple%20ascending%20directions.%20Finally%2C%20we%20validate%20our%20approach%20numerically%0Awith%20hundreds%20of%20robots.%20The%20fact%20that%20a%20large%20number%20of%20robots%20always%0Acalculate%20an%20ascending%20direction%20compensates%20for%20the%20loss%20of%20individuals%20and%0Amitigates%20issues%20arising%20from%20the%20actuator%20and%20sensor%20noises.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.02937v2&entry.124074799=Read"},
{"title": "TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering", "author": "Linus Franke and Darius R\u00fcckert and Laura Fink and Marc Stamminger", "abstract": "  Point-based radiance field rendering has demonstrated impressive results for\nnovel view synthesis, offering a compelling blend of rendering quality and\ncomputational efficiency. However, also latest approaches in this domain are\nnot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.\n2023] struggles when tasked with rendering highly detailed scenes, due to\nblurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022]\ncan accommodate crisper images, but the neural reconstruction network decreases\nperformance, it grapples with temporal instability and it is unable to\neffectively address large gaps in the point cloud.\n  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that\ncombines ideas from both Gaussian Splatting and ADOP. The fundamental concept\nbehind our novel technique involves rasterizing points into a screen-space\nimage pyramid, with the selection of the pyramid layer determined by the\nprojected point size. This approach allows rendering arbitrarily large points\nusing a single trilinear write. A lightweight neural network is then used to\nreconstruct a hole-free image including detail beyond splat resolution.\nImportantly, our render pipeline is entirely differentiable, allowing for\nautomatic optimization of both point sizes and positions.\n  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art\nmethods in terms of rendering quality while maintaining a real-time frame rate\nof 60 frames per second on readily available hardware. This performance extends\nto challenging scenarios, such as scenes featuring intricate geometry,\nexpansive landscapes, and auto-exposed footage.\n  The project page is located at: https://lfranke.github.io/trips/\n", "link": "http://arxiv.org/abs/2401.06003v2", "date": "2024-03-26", "relevancy": 1.5029, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5159}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4987}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4918}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TRIPS%3A%20Trilinear%20Point%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering&body=Title%3A%20TRIPS%3A%20Trilinear%20Point%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering%0AAuthor%3A%20Linus%20Franke%20and%20Darius%20R%C3%BCckert%20and%20Laura%20Fink%20and%20Marc%20Stamminger%0AAbstract%3A%20%20%20Point-based%20radiance%20field%20rendering%20has%20demonstrated%20impressive%20results%20for%0Anovel%20view%20synthesis%2C%20offering%20a%20compelling%20blend%20of%20rendering%20quality%20and%0Acomputational%20efficiency.%20However%2C%20also%20latest%20approaches%20in%20this%20domain%20are%0Anot%20without%20their%20shortcomings.%203D%20Gaussian%20Splatting%20%5BKerbl%20and%20Kopanas%20et%20al.%0A2023%5D%20struggles%20when%20tasked%20with%20rendering%20highly%20detailed%20scenes%2C%20due%20to%0Ablurring%20and%20cloudy%20artifacts.%20On%20the%20other%20hand%2C%20ADOP%20%5BR%5C%22uckert%20et%20al.%202022%5D%0Acan%20accommodate%20crisper%20images%2C%20but%20the%20neural%20reconstruction%20network%20decreases%0Aperformance%2C%20it%20grapples%20with%20temporal%20instability%20and%20it%20is%20unable%20to%0Aeffectively%20address%20large%20gaps%20in%20the%20point%20cloud.%0A%20%20In%20this%20paper%2C%20we%20present%20TRIPS%20%28Trilinear%20Point%20Splatting%29%2C%20an%20approach%20that%0Acombines%20ideas%20from%20both%20Gaussian%20Splatting%20and%20ADOP.%20The%20fundamental%20concept%0Abehind%20our%20novel%20technique%20involves%20rasterizing%20points%20into%20a%20screen-space%0Aimage%20pyramid%2C%20with%20the%20selection%20of%20the%20pyramid%20layer%20determined%20by%20the%0Aprojected%20point%20size.%20This%20approach%20allows%20rendering%20arbitrarily%20large%20points%0Ausing%20a%20single%20trilinear%20write.%20A%20lightweight%20neural%20network%20is%20then%20used%20to%0Areconstruct%20a%20hole-free%20image%20including%20detail%20beyond%20splat%20resolution.%0AImportantly%2C%20our%20render%20pipeline%20is%20entirely%20differentiable%2C%20allowing%20for%0Aautomatic%20optimization%20of%20both%20point%20sizes%20and%20positions.%0A%20%20Our%20evaluation%20demonstrate%20that%20TRIPS%20surpasses%20existing%20state-of-the-art%0Amethods%20in%20terms%20of%20rendering%20quality%20while%20maintaining%20a%20real-time%20frame%20rate%0Aof%2060%20frames%20per%20second%20on%20readily%20available%20hardware.%20This%20performance%20extends%0Ato%20challenging%20scenarios%2C%20such%20as%20scenes%20featuring%20intricate%20geometry%2C%0Aexpansive%20landscapes%2C%20and%20auto-exposed%20footage.%0A%20%20The%20project%20page%20is%20located%20at%3A%20https%3A//lfranke.github.io/trips/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06003v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRIPS%3A%20Trilinear%20Point%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering&entry.906535625=Linus%20Franke%20and%20Darius%20R%C3%BCckert%20and%20Laura%20Fink%20and%20Marc%20Stamminger&entry.1292438233=%20%20Point-based%20radiance%20field%20rendering%20has%20demonstrated%20impressive%20results%20for%0Anovel%20view%20synthesis%2C%20offering%20a%20compelling%20blend%20of%20rendering%20quality%20and%0Acomputational%20efficiency.%20However%2C%20also%20latest%20approaches%20in%20this%20domain%20are%0Anot%20without%20their%20shortcomings.%203D%20Gaussian%20Splatting%20%5BKerbl%20and%20Kopanas%20et%20al.%0A2023%5D%20struggles%20when%20tasked%20with%20rendering%20highly%20detailed%20scenes%2C%20due%20to%0Ablurring%20and%20cloudy%20artifacts.%20On%20the%20other%20hand%2C%20ADOP%20%5BR%5C%22uckert%20et%20al.%202022%5D%0Acan%20accommodate%20crisper%20images%2C%20but%20the%20neural%20reconstruction%20network%20decreases%0Aperformance%2C%20it%20grapples%20with%20temporal%20instability%20and%20it%20is%20unable%20to%0Aeffectively%20address%20large%20gaps%20in%20the%20point%20cloud.%0A%20%20In%20this%20paper%2C%20we%20present%20TRIPS%20%28Trilinear%20Point%20Splatting%29%2C%20an%20approach%20that%0Acombines%20ideas%20from%20both%20Gaussian%20Splatting%20and%20ADOP.%20The%20fundamental%20concept%0Abehind%20our%20novel%20technique%20involves%20rasterizing%20points%20into%20a%20screen-space%0Aimage%20pyramid%2C%20with%20the%20selection%20of%20the%20pyramid%20layer%20determined%20by%20the%0Aprojected%20point%20size.%20This%20approach%20allows%20rendering%20arbitrarily%20large%20points%0Ausing%20a%20single%20trilinear%20write.%20A%20lightweight%20neural%20network%20is%20then%20used%20to%0Areconstruct%20a%20hole-free%20image%20including%20detail%20beyond%20splat%20resolution.%0AImportantly%2C%20our%20render%20pipeline%20is%20entirely%20differentiable%2C%20allowing%20for%0Aautomatic%20optimization%20of%20both%20point%20sizes%20and%20positions.%0A%20%20Our%20evaluation%20demonstrate%20that%20TRIPS%20surpasses%20existing%20state-of-the-art%0Amethods%20in%20terms%20of%20rendering%20quality%20while%20maintaining%20a%20real-time%20frame%20rate%0Aof%2060%20frames%20per%20second%20on%20readily%20available%20hardware.%20This%20performance%20extends%0Ato%20challenging%20scenarios%2C%20such%20as%20scenes%20featuring%20intricate%20geometry%2C%0Aexpansive%20landscapes%2C%20and%20auto-exposed%20footage.%0A%20%20The%20project%20page%20is%20located%20at%3A%20https%3A//lfranke.github.io/trips/%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06003v2&entry.124074799=Read"},
{"title": "Probabilistically Rewired Message-Passing Neural Networks", "author": "Chendi Qian and Andrei Manolache and Kareem Ahmed and Zhe Zeng and Guy Van den Broeck and Mathias Niepert and Christopher Morris", "abstract": "  Message-passing graph neural networks (MPNNs) emerged as powerful tools for\nprocessing graph-structured input. However, they operate on a fixed input graph\nstructure, ignoring potential noise and missing information. Furthermore, their\nlocal aggregation mechanism can lead to problems such as over-squashing and\nlimited expressive power in capturing relevant graph structures. Existing\nsolutions to these challenges have primarily relied on heuristic methods, often\ndisregarding the underlying data distribution. Hence, devising principled\napproaches for learning to infer graph structures relevant to the given\nprediction task remains an open challenge. In this work, leveraging recent\nprogress in exact and differentiable $k$-subset sampling, we devise\nprobabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges\nwhile omitting less beneficial ones. For the first time, our theoretical\nanalysis explores how PR-MPNNs enhance expressive power, and we identify\nprecise conditions under which they outperform purely randomized approaches.\nEmpirically, we demonstrate that our approach effectively mitigates issues like\nover-squashing and under-reaching. In addition, on established real-world\ndatasets, our method exhibits competitive or superior predictive performance\ncompared to traditional MPNN models and recent graph transformer architectures.\n", "link": "http://arxiv.org/abs/2310.02156v4", "date": "2024-03-26", "relevancy": 1.5, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5037}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5032}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4876}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probabilistically%20Rewired%20Message-Passing%20Neural%20Networks&body=Title%3A%20Probabilistically%20Rewired%20Message-Passing%20Neural%20Networks%0AAuthor%3A%20Chendi%20Qian%20and%20Andrei%20Manolache%20and%20Kareem%20Ahmed%20and%20Zhe%20Zeng%20and%20Guy%20Van%20den%20Broeck%20and%20Mathias%20Niepert%20and%20Christopher%20Morris%0AAbstract%3A%20%20%20Message-passing%20graph%20neural%20networks%20%28MPNNs%29%20emerged%20as%20powerful%20tools%20for%0Aprocessing%20graph-structured%20input.%20However%2C%20they%20operate%20on%20a%20fixed%20input%20graph%0Astructure%2C%20ignoring%20potential%20noise%20and%20missing%20information.%20Furthermore%2C%20their%0Alocal%20aggregation%20mechanism%20can%20lead%20to%20problems%20such%20as%20over-squashing%20and%0Alimited%20expressive%20power%20in%20capturing%20relevant%20graph%20structures.%20Existing%0Asolutions%20to%20these%20challenges%20have%20primarily%20relied%20on%20heuristic%20methods%2C%20often%0Adisregarding%20the%20underlying%20data%20distribution.%20Hence%2C%20devising%20principled%0Aapproaches%20for%20learning%20to%20infer%20graph%20structures%20relevant%20to%20the%20given%0Aprediction%20task%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20leveraging%20recent%0Aprogress%20in%20exact%20and%20differentiable%20%24k%24-subset%20sampling%2C%20we%20devise%0Aprobabilistically%20rewired%20MPNNs%20%28PR-MPNNs%29%2C%20which%20learn%20to%20add%20relevant%20edges%0Awhile%20omitting%20less%20beneficial%20ones.%20For%20the%20first%20time%2C%20our%20theoretical%0Aanalysis%20explores%20how%20PR-MPNNs%20enhance%20expressive%20power%2C%20and%20we%20identify%0Aprecise%20conditions%20under%20which%20they%20outperform%20purely%20randomized%20approaches.%0AEmpirically%2C%20we%20demonstrate%20that%20our%20approach%20effectively%20mitigates%20issues%20like%0Aover-squashing%20and%20under-reaching.%20In%20addition%2C%20on%20established%20real-world%0Adatasets%2C%20our%20method%20exhibits%20competitive%20or%20superior%20predictive%20performance%0Acompared%20to%20traditional%20MPNN%20models%20and%20recent%20graph%20transformer%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02156v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistically%20Rewired%20Message-Passing%20Neural%20Networks&entry.906535625=Chendi%20Qian%20and%20Andrei%20Manolache%20and%20Kareem%20Ahmed%20and%20Zhe%20Zeng%20and%20Guy%20Van%20den%20Broeck%20and%20Mathias%20Niepert%20and%20Christopher%20Morris&entry.1292438233=%20%20Message-passing%20graph%20neural%20networks%20%28MPNNs%29%20emerged%20as%20powerful%20tools%20for%0Aprocessing%20graph-structured%20input.%20However%2C%20they%20operate%20on%20a%20fixed%20input%20graph%0Astructure%2C%20ignoring%20potential%20noise%20and%20missing%20information.%20Furthermore%2C%20their%0Alocal%20aggregation%20mechanism%20can%20lead%20to%20problems%20such%20as%20over-squashing%20and%0Alimited%20expressive%20power%20in%20capturing%20relevant%20graph%20structures.%20Existing%0Asolutions%20to%20these%20challenges%20have%20primarily%20relied%20on%20heuristic%20methods%2C%20often%0Adisregarding%20the%20underlying%20data%20distribution.%20Hence%2C%20devising%20principled%0Aapproaches%20for%20learning%20to%20infer%20graph%20structures%20relevant%20to%20the%20given%0Aprediction%20task%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20leveraging%20recent%0Aprogress%20in%20exact%20and%20differentiable%20%24k%24-subset%20sampling%2C%20we%20devise%0Aprobabilistically%20rewired%20MPNNs%20%28PR-MPNNs%29%2C%20which%20learn%20to%20add%20relevant%20edges%0Awhile%20omitting%20less%20beneficial%20ones.%20For%20the%20first%20time%2C%20our%20theoretical%0Aanalysis%20explores%20how%20PR-MPNNs%20enhance%20expressive%20power%2C%20and%20we%20identify%0Aprecise%20conditions%20under%20which%20they%20outperform%20purely%20randomized%20approaches.%0AEmpirically%2C%20we%20demonstrate%20that%20our%20approach%20effectively%20mitigates%20issues%20like%0Aover-squashing%20and%20under-reaching.%20In%20addition%2C%20on%20established%20real-world%0Adatasets%2C%20our%20method%20exhibits%20competitive%20or%20superior%20predictive%20performance%0Acompared%20to%20traditional%20MPNN%20models%20and%20recent%20graph%20transformer%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02156v4&entry.124074799=Read"},
{"title": "Image-based Novel Fault Detection with Deep Learning Classifiers using\n  Hierarchical Labels", "author": "Nurettin Sergin and Jiayu Huang and Tzyy-Shuh Chang and Hao Yan", "abstract": "  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n", "link": "http://arxiv.org/abs/2403.17891v1", "date": "2024-03-26", "relevancy": 1.4886, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5086}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4852}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Image-based%20Novel%20Fault%20Detection%20with%20Deep%20Learning%20Classifiers%20using%0A%20%20Hierarchical%20Labels&body=Title%3A%20Image-based%20Novel%20Fault%20Detection%20with%20Deep%20Learning%20Classifiers%20using%0A%20%20Hierarchical%20Labels%0AAuthor%3A%20Nurettin%20Sergin%20and%20Jiayu%20Huang%20and%20Tzyy-Shuh%20Chang%20and%20Hao%20Yan%0AAbstract%3A%20%20%20One%20important%20characteristic%20of%20modern%20fault%20classification%20systems%20is%20the%0Aability%20to%20flag%20the%20system%20when%20faced%20with%20previously%20unseen%20fault%20types.%20This%0Awork%20considers%20the%20unknown%20fault%20detection%20capabilities%20of%20deep%20neural%0Anetwork-based%20fault%20classifiers.%20Specifically%2C%20we%20propose%20a%20methodology%20on%20how%2C%0Awhen%20available%2C%20labels%20regarding%20the%20fault%20taxonomy%20can%20be%20used%20to%20increase%0Aunknown%20fault%20detection%20performance%20without%20sacrificing%20model%20performance.%20To%0Aachieve%20this%2C%20we%20propose%20to%20utilize%20soft%20label%20techniques%20to%20improve%20the%0Astate-of-the-art%20deep%20novel%20fault%20detection%20techniques%20during%20the%20training%0Aprocess%20and%20novel%20hierarchically%20consistent%20detection%20statistics%20for%20online%0Anovel%20fault%20detection.%20Finally%2C%20we%20demonstrated%20increased%20detection%20performance%0Aon%20novel%20fault%20detection%20in%20inspection%20images%20from%20the%20hot%20steel%20rolling%0Aprocess%2C%20with%20results%20well%20replicated%20across%20multiple%20scenarios%20and%20baseline%0Adetection%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17891v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-based%20Novel%20Fault%20Detection%20with%20Deep%20Learning%20Classifiers%20using%0A%20%20Hierarchical%20Labels&entry.906535625=Nurettin%20Sergin%20and%20Jiayu%20Huang%20and%20Tzyy-Shuh%20Chang%20and%20Hao%20Yan&entry.1292438233=%20%20One%20important%20characteristic%20of%20modern%20fault%20classification%20systems%20is%20the%0Aability%20to%20flag%20the%20system%20when%20faced%20with%20previously%20unseen%20fault%20types.%20This%0Awork%20considers%20the%20unknown%20fault%20detection%20capabilities%20of%20deep%20neural%0Anetwork-based%20fault%20classifiers.%20Specifically%2C%20we%20propose%20a%20methodology%20on%20how%2C%0Awhen%20available%2C%20labels%20regarding%20the%20fault%20taxonomy%20can%20be%20used%20to%20increase%0Aunknown%20fault%20detection%20performance%20without%20sacrificing%20model%20performance.%20To%0Aachieve%20this%2C%20we%20propose%20to%20utilize%20soft%20label%20techniques%20to%20improve%20the%0Astate-of-the-art%20deep%20novel%20fault%20detection%20techniques%20during%20the%20training%0Aprocess%20and%20novel%20hierarchically%20consistent%20detection%20statistics%20for%20online%0Anovel%20fault%20detection.%20Finally%2C%20we%20demonstrated%20increased%20detection%20performance%0Aon%20novel%20fault%20detection%20in%20inspection%20images%20from%20the%20hot%20steel%20rolling%0Aprocess%2C%20with%20results%20well%20replicated%20across%20multiple%20scenarios%20and%20baseline%0Adetection%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17891v1&entry.124074799=Read"},
{"title": "Hierarchical Multi-label Classification for Fine-level Event Extraction\n  from Aviation Accident Reports", "author": "Xinyu Zhao and Hao Yan and Yongming Liu", "abstract": "  A large volume of accident reports is recorded in the aviation domain, which\ngreatly values improving aviation safety. To better use those reports, we need\nto understand the most important events or impact factors according to the\naccident reports. However, the increasing number of accident reports requires\nlarge efforts from domain experts to label those reports. In order to make the\nlabeling process more efficient, many researchers have started developing\nalgorithms to identify the underlying events from accident reports\nautomatically. This article argues that we can identify the events more\naccurately by leveraging the event taxonomy. More specifically, we consider the\nproblem a hierarchical classification task where we first identify the\ncoarse-level information and then predict the fine-level information. We\nachieve this hierarchical classification process by incorporating a novel\nhierarchical attention module into BERT. To further utilize the information\nfrom event taxonomy, we regularize the proposed model according to the\nrelationship and distribution among labels. The effectiveness of our framework\nis evaluated with the data collected by National Transportation Safety Board\n(NTSB). It has been shown that fine-level prediction accuracy is highly\nimproved, and the regularization term can be beneficial to the rare event\nidentification problem.\n", "link": "http://arxiv.org/abs/2403.17914v1", "date": "2024-03-26", "relevancy": 1.4881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.495}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.492}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Multi-label%20Classification%20for%20Fine-level%20Event%20Extraction%0A%20%20from%20Aviation%20Accident%20Reports&body=Title%3A%20Hierarchical%20Multi-label%20Classification%20for%20Fine-level%20Event%20Extraction%0A%20%20from%20Aviation%20Accident%20Reports%0AAuthor%3A%20Xinyu%20Zhao%20and%20Hao%20Yan%20and%20Yongming%20Liu%0AAbstract%3A%20%20%20A%20large%20volume%20of%20accident%20reports%20is%20recorded%20in%20the%20aviation%20domain%2C%20which%0Agreatly%20values%20improving%20aviation%20safety.%20To%20better%20use%20those%20reports%2C%20we%20need%0Ato%20understand%20the%20most%20important%20events%20or%20impact%20factors%20according%20to%20the%0Aaccident%20reports.%20However%2C%20the%20increasing%20number%20of%20accident%20reports%20requires%0Alarge%20efforts%20from%20domain%20experts%20to%20label%20those%20reports.%20In%20order%20to%20make%20the%0Alabeling%20process%20more%20efficient%2C%20many%20researchers%20have%20started%20developing%0Aalgorithms%20to%20identify%20the%20underlying%20events%20from%20accident%20reports%0Aautomatically.%20This%20article%20argues%20that%20we%20can%20identify%20the%20events%20more%0Aaccurately%20by%20leveraging%20the%20event%20taxonomy.%20More%20specifically%2C%20we%20consider%20the%0Aproblem%20a%20hierarchical%20classification%20task%20where%20we%20first%20identify%20the%0Acoarse-level%20information%20and%20then%20predict%20the%20fine-level%20information.%20We%0Aachieve%20this%20hierarchical%20classification%20process%20by%20incorporating%20a%20novel%0Ahierarchical%20attention%20module%20into%20BERT.%20To%20further%20utilize%20the%20information%0Afrom%20event%20taxonomy%2C%20we%20regularize%20the%20proposed%20model%20according%20to%20the%0Arelationship%20and%20distribution%20among%20labels.%20The%20effectiveness%20of%20our%20framework%0Ais%20evaluated%20with%20the%20data%20collected%20by%20National%20Transportation%20Safety%20Board%0A%28NTSB%29.%20It%20has%20been%20shown%20that%20fine-level%20prediction%20accuracy%20is%20highly%0Aimproved%2C%20and%20the%20regularization%20term%20can%20be%20beneficial%20to%20the%20rare%20event%0Aidentification%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17914v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Multi-label%20Classification%20for%20Fine-level%20Event%20Extraction%0A%20%20from%20Aviation%20Accident%20Reports&entry.906535625=Xinyu%20Zhao%20and%20Hao%20Yan%20and%20Yongming%20Liu&entry.1292438233=%20%20A%20large%20volume%20of%20accident%20reports%20is%20recorded%20in%20the%20aviation%20domain%2C%20which%0Agreatly%20values%20improving%20aviation%20safety.%20To%20better%20use%20those%20reports%2C%20we%20need%0Ato%20understand%20the%20most%20important%20events%20or%20impact%20factors%20according%20to%20the%0Aaccident%20reports.%20However%2C%20the%20increasing%20number%20of%20accident%20reports%20requires%0Alarge%20efforts%20from%20domain%20experts%20to%20label%20those%20reports.%20In%20order%20to%20make%20the%0Alabeling%20process%20more%20efficient%2C%20many%20researchers%20have%20started%20developing%0Aalgorithms%20to%20identify%20the%20underlying%20events%20from%20accident%20reports%0Aautomatically.%20This%20article%20argues%20that%20we%20can%20identify%20the%20events%20more%0Aaccurately%20by%20leveraging%20the%20event%20taxonomy.%20More%20specifically%2C%20we%20consider%20the%0Aproblem%20a%20hierarchical%20classification%20task%20where%20we%20first%20identify%20the%0Acoarse-level%20information%20and%20then%20predict%20the%20fine-level%20information.%20We%0Aachieve%20this%20hierarchical%20classification%20process%20by%20incorporating%20a%20novel%0Ahierarchical%20attention%20module%20into%20BERT.%20To%20further%20utilize%20the%20information%0Afrom%20event%20taxonomy%2C%20we%20regularize%20the%20proposed%20model%20according%20to%20the%0Arelationship%20and%20distribution%20among%20labels.%20The%20effectiveness%20of%20our%20framework%0Ais%20evaluated%20with%20the%20data%20collected%20by%20National%20Transportation%20Safety%20Board%0A%28NTSB%29.%20It%20has%20been%20shown%20that%20fine-level%20prediction%20accuracy%20is%20highly%0Aimproved%2C%20and%20the%20regularization%20term%20can%20be%20beneficial%20to%20the%20rare%20event%0Aidentification%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17914v1&entry.124074799=Read"},
{"title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents", "author": "Elizaveta Tennant and Stephen Hailes and Mirco Musolesi", "abstract": "  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents. A promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents. However, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., caring about maximizing some\noutcome over time) or norm-based (i.e., focusing on conforming to a specific\nnorm here and now). The extent to which agents' co-development may be impacted\nby such moral heterogeneity in populations is not well understood. In this\npaper, we present a study of the learning dynamics of morally heterogeneous\npopulations interacting in a social dilemma setting. Using a Prisoner's Dilemma\nenvironment with a partner selection mechanism, we investigate the extent to\nwhich the prevalence of diverse moral agents in populations affects individual\nagents' learning behaviors and emergent population-level outcomes. We observe\nseveral types of non-trivial interactions between pro-social and anti-social\nagents, and find that certain classes of moral agents are able to steer selfish\nagents towards more cooperative behavior.\n", "link": "http://arxiv.org/abs/2403.04202v3", "date": "2024-03-26", "relevancy": 1.4635, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4891}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4753}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents&body=Title%3A%20Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents%0AAuthor%3A%20Elizaveta%20Tennant%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Growing%20concerns%20about%20safety%20and%20alignment%20of%20AI%20systems%20highlight%20the%0Aimportance%20of%20embedding%20moral%20capabilities%20in%20artificial%20agents.%20A%20promising%0Asolution%20is%20the%20use%20of%20learning%20from%20experience%2C%20i.e.%2C%20Reinforcement%20Learning.%0AIn%20multi-agent%20%28social%29%20environments%2C%20complex%20population-level%20phenomena%20may%0Aemerge%20from%20interactions%20between%20individual%20learning%20agents.%20Many%20of%20the%0Aexisting%20studies%20rely%20on%20simulated%20social%20dilemma%20environments%20to%20study%20the%0Ainteractions%20of%20independent%20learning%20agents.%20However%2C%20they%20tend%20to%20ignore%20the%0Amoral%20heterogeneity%20that%20is%20likely%20to%20be%20present%20in%20societies%20of%20agents%20in%0Apractice.%20For%20example%2C%20at%20different%20points%20in%20time%20a%20single%20learning%20agent%20may%0Aface%20opponents%20who%20are%20consequentialist%20%28i.e.%2C%20caring%20about%20maximizing%20some%0Aoutcome%20over%20time%29%20or%20norm-based%20%28i.e.%2C%20focusing%20on%20conforming%20to%20a%20specific%0Anorm%20here%20and%20now%29.%20The%20extent%20to%20which%20agents%27%20co-development%20may%20be%20impacted%0Aby%20such%20moral%20heterogeneity%20in%20populations%20is%20not%20well%20understood.%20In%20this%0Apaper%2C%20we%20present%20a%20study%20of%20the%20learning%20dynamics%20of%20morally%20heterogeneous%0Apopulations%20interacting%20in%20a%20social%20dilemma%20setting.%20Using%20a%20Prisoner%27s%20Dilemma%0Aenvironment%20with%20a%20partner%20selection%20mechanism%2C%20we%20investigate%20the%20extent%20to%0Awhich%20the%20prevalence%20of%20diverse%20moral%20agents%20in%20populations%20affects%20individual%0Aagents%27%20learning%20behaviors%20and%20emergent%20population-level%20outcomes.%20We%20observe%0Aseveral%20types%20of%20non-trivial%20interactions%20between%20pro-social%20and%20anti-social%0Aagents%2C%20and%20find%20that%20certain%20classes%20of%20moral%20agents%20are%20able%20to%20steer%20selfish%0Aagents%20towards%20more%20cooperative%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04202v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents&entry.906535625=Elizaveta%20Tennant%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Growing%20concerns%20about%20safety%20and%20alignment%20of%20AI%20systems%20highlight%20the%0Aimportance%20of%20embedding%20moral%20capabilities%20in%20artificial%20agents.%20A%20promising%0Asolution%20is%20the%20use%20of%20learning%20from%20experience%2C%20i.e.%2C%20Reinforcement%20Learning.%0AIn%20multi-agent%20%28social%29%20environments%2C%20complex%20population-level%20phenomena%20may%0Aemerge%20from%20interactions%20between%20individual%20learning%20agents.%20Many%20of%20the%0Aexisting%20studies%20rely%20on%20simulated%20social%20dilemma%20environments%20to%20study%20the%0Ainteractions%20of%20independent%20learning%20agents.%20However%2C%20they%20tend%20to%20ignore%20the%0Amoral%20heterogeneity%20that%20is%20likely%20to%20be%20present%20in%20societies%20of%20agents%20in%0Apractice.%20For%20example%2C%20at%20different%20points%20in%20time%20a%20single%20learning%20agent%20may%0Aface%20opponents%20who%20are%20consequentialist%20%28i.e.%2C%20caring%20about%20maximizing%20some%0Aoutcome%20over%20time%29%20or%20norm-based%20%28i.e.%2C%20focusing%20on%20conforming%20to%20a%20specific%0Anorm%20here%20and%20now%29.%20The%20extent%20to%20which%20agents%27%20co-development%20may%20be%20impacted%0Aby%20such%20moral%20heterogeneity%20in%20populations%20is%20not%20well%20understood.%20In%20this%0Apaper%2C%20we%20present%20a%20study%20of%20the%20learning%20dynamics%20of%20morally%20heterogeneous%0Apopulations%20interacting%20in%20a%20social%20dilemma%20setting.%20Using%20a%20Prisoner%27s%20Dilemma%0Aenvironment%20with%20a%20partner%20selection%20mechanism%2C%20we%20investigate%20the%20extent%20to%0Awhich%20the%20prevalence%20of%20diverse%20moral%20agents%20in%20populations%20affects%20individual%0Aagents%27%20learning%20behaviors%20and%20emergent%20population-level%20outcomes.%20We%20observe%0Aseveral%20types%20of%20non-trivial%20interactions%20between%20pro-social%20and%20anti-social%0Aagents%2C%20and%20find%20that%20certain%20classes%20of%20moral%20agents%20are%20able%20to%20steer%20selfish%0Aagents%20towards%20more%20cooperative%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04202v3&entry.124074799=Read"},
{"title": "Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n  Gaussians", "author": "Kerui Ren and Lihan Jiang and Tao Lu and Mulin Yu and Linning Xu and Zhangkai Ni and Bo Dai", "abstract": "  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\nfidelity and efficiency compared to NeRF-based neural scene representations.\nWhile demonstrating the potential for real-time rendering, 3D-GS encounters\nrendering bottlenecks in large scenes with complex details due to an excessive\nnumber of Gaussian primitives located within the viewing frustum. This\nlimitation is particularly noticeable in zoom-out views and can lead to\ninconsistent rendering speeds in scenes with varying details. Moreover, it\noften struggles to capture the corresponding level of details at different\nscales with its heuristic density control operation. Inspired by the\nLevel-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an\nLOD-structured 3D Gaussian approach supporting level-of-detail decomposition\nfor scene representation that contributes to the final rendering results. Our\nmodel dynamically selects the appropriate level from the set of\nmulti-resolution anchor points, ensuring consistent rendering performance with\nadaptive LOD adjustments while maintaining high-fidelity rendering results.\n", "link": "http://arxiv.org/abs/2403.17898v1", "date": "2024-03-26", "relevancy": 1.4431, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5057}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4758}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4733}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Octree-GS%3A%20Towards%20Consistent%20Real-time%20Rendering%20with%20LOD-Structured%203D%0A%20%20Gaussians&body=Title%3A%20Octree-GS%3A%20Towards%20Consistent%20Real-time%20Rendering%20with%20LOD-Structured%203D%0A%20%20Gaussians%0AAuthor%3A%20Kerui%20Ren%20and%20Lihan%20Jiang%20and%20Tao%20Lu%20and%20Mulin%20Yu%20and%20Linning%20Xu%20and%20Zhangkai%20Ni%20and%20Bo%20Dai%0AAbstract%3A%20%20%20The%20recent%203D%20Gaussian%20splatting%20%283D-GS%29%20has%20shown%20remarkable%20rendering%0Afidelity%20and%20efficiency%20compared%20to%20NeRF-based%20neural%20scene%20representations.%0AWhile%20demonstrating%20the%20potential%20for%20real-time%20rendering%2C%203D-GS%20encounters%0Arendering%20bottlenecks%20in%20large%20scenes%20with%20complex%20details%20due%20to%20an%20excessive%0Anumber%20of%20Gaussian%20primitives%20located%20within%20the%20viewing%20frustum.%20This%0Alimitation%20is%20particularly%20noticeable%20in%20zoom-out%20views%20and%20can%20lead%20to%0Ainconsistent%20rendering%20speeds%20in%20scenes%20with%20varying%20details.%20Moreover%2C%20it%0Aoften%20struggles%20to%20capture%20the%20corresponding%20level%20of%20details%20at%20different%0Ascales%20with%20its%20heuristic%20density%20control%20operation.%20Inspired%20by%20the%0ALevel-of-Detail%20%28LOD%29%20techniques%2C%20we%20introduce%20Octree-GS%2C%20featuring%20an%0ALOD-structured%203D%20Gaussian%20approach%20supporting%20level-of-detail%20decomposition%0Afor%20scene%20representation%20that%20contributes%20to%20the%20final%20rendering%20results.%20Our%0Amodel%20dynamically%20selects%20the%20appropriate%20level%20from%20the%20set%20of%0Amulti-resolution%20anchor%20points%2C%20ensuring%20consistent%20rendering%20performance%20with%0Aadaptive%20LOD%20adjustments%20while%20maintaining%20high-fidelity%20rendering%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17898v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Octree-GS%3A%20Towards%20Consistent%20Real-time%20Rendering%20with%20LOD-Structured%203D%0A%20%20Gaussians&entry.906535625=Kerui%20Ren%20and%20Lihan%20Jiang%20and%20Tao%20Lu%20and%20Mulin%20Yu%20and%20Linning%20Xu%20and%20Zhangkai%20Ni%20and%20Bo%20Dai&entry.1292438233=%20%20The%20recent%203D%20Gaussian%20splatting%20%283D-GS%29%20has%20shown%20remarkable%20rendering%0Afidelity%20and%20efficiency%20compared%20to%20NeRF-based%20neural%20scene%20representations.%0AWhile%20demonstrating%20the%20potential%20for%20real-time%20rendering%2C%203D-GS%20encounters%0Arendering%20bottlenecks%20in%20large%20scenes%20with%20complex%20details%20due%20to%20an%20excessive%0Anumber%20of%20Gaussian%20primitives%20located%20within%20the%20viewing%20frustum.%20This%0Alimitation%20is%20particularly%20noticeable%20in%20zoom-out%20views%20and%20can%20lead%20to%0Ainconsistent%20rendering%20speeds%20in%20scenes%20with%20varying%20details.%20Moreover%2C%20it%0Aoften%20struggles%20to%20capture%20the%20corresponding%20level%20of%20details%20at%20different%0Ascales%20with%20its%20heuristic%20density%20control%20operation.%20Inspired%20by%20the%0ALevel-of-Detail%20%28LOD%29%20techniques%2C%20we%20introduce%20Octree-GS%2C%20featuring%20an%0ALOD-structured%203D%20Gaussian%20approach%20supporting%20level-of-detail%20decomposition%0Afor%20scene%20representation%20that%20contributes%20to%20the%20final%20rendering%20results.%20Our%0Amodel%20dynamically%20selects%20the%20appropriate%20level%20from%20the%20set%20of%0Amulti-resolution%20anchor%20points%2C%20ensuring%20consistent%20rendering%20performance%20with%0Aadaptive%20LOD%20adjustments%20while%20maintaining%20high-fidelity%20rendering%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17898v1&entry.124074799=Read"},
{"title": "Learning the Optimal Power Flow: Environment Design Matters", "author": "Thomas Wolgast and Astrid Nie\u00dfe", "abstract": "  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)\nemerges as a promising new approach. However, the RL-OPF literature is strongly\ndivided regarding the exact formulation of the OPF problem as an RL\nenvironment. In this work, we collect and implement diverse environment design\ndecisions from the literature regarding training data, observation space,\nepisode definition, and reward function choice. In an experimental analysis, we\nshow the significant impact of these environment design options on RL-OPF\ntraining performance. Further, we derive some first recommendations regarding\nthe choice of these design decisions. The created environment framework is\nfully open-source and can serve as a benchmark for future research in the\nRL-OPF field.\n", "link": "http://arxiv.org/abs/2403.17831v1", "date": "2024-03-26", "relevancy": 1.4376, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4814}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4795}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4763}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20Optimal%20Power%20Flow%3A%20Environment%20Design%20Matters&body=Title%3A%20Learning%20the%20Optimal%20Power%20Flow%3A%20Environment%20Design%20Matters%0AAuthor%3A%20Thomas%20Wolgast%20and%20Astrid%20Nie%C3%9Fe%0AAbstract%3A%20%20%20To%20solve%20the%20optimal%20power%20flow%20%28OPF%29%20problem%2C%20reinforcement%20learning%20%28RL%29%0Aemerges%20as%20a%20promising%20new%20approach.%20However%2C%20the%20RL-OPF%20literature%20is%20strongly%0Adivided%20regarding%20the%20exact%20formulation%20of%20the%20OPF%20problem%20as%20an%20RL%0Aenvironment.%20In%20this%20work%2C%20we%20collect%20and%20implement%20diverse%20environment%20design%0Adecisions%20from%20the%20literature%20regarding%20training%20data%2C%20observation%20space%2C%0Aepisode%20definition%2C%20and%20reward%20function%20choice.%20In%20an%20experimental%20analysis%2C%20we%0Ashow%20the%20significant%20impact%20of%20these%20environment%20design%20options%20on%20RL-OPF%0Atraining%20performance.%20Further%2C%20we%20derive%20some%20first%20recommendations%20regarding%0Athe%20choice%20of%20these%20design%20decisions.%20The%20created%20environment%20framework%20is%0Afully%20open-source%20and%20can%20serve%20as%20a%20benchmark%20for%20future%20research%20in%20the%0ARL-OPF%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17831v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20Optimal%20Power%20Flow%3A%20Environment%20Design%20Matters&entry.906535625=Thomas%20Wolgast%20and%20Astrid%20Nie%C3%9Fe&entry.1292438233=%20%20To%20solve%20the%20optimal%20power%20flow%20%28OPF%29%20problem%2C%20reinforcement%20learning%20%28RL%29%0Aemerges%20as%20a%20promising%20new%20approach.%20However%2C%20the%20RL-OPF%20literature%20is%20strongly%0Adivided%20regarding%20the%20exact%20formulation%20of%20the%20OPF%20problem%20as%20an%20RL%0Aenvironment.%20In%20this%20work%2C%20we%20collect%20and%20implement%20diverse%20environment%20design%0Adecisions%20from%20the%20literature%20regarding%20training%20data%2C%20observation%20space%2C%0Aepisode%20definition%2C%20and%20reward%20function%20choice.%20In%20an%20experimental%20analysis%2C%20we%0Ashow%20the%20significant%20impact%20of%20these%20environment%20design%20options%20on%20RL-OPF%0Atraining%20performance.%20Further%2C%20we%20derive%20some%20first%20recommendations%20regarding%0Athe%20choice%20of%20these%20design%20decisions.%20The%20created%20environment%20framework%20is%0Afully%20open-source%20and%20can%20serve%20as%20a%20benchmark%20for%20future%20research%20in%20the%0ARL-OPF%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17831v1&entry.124074799=Read"},
{"title": "2D Gaussian Splatting for Geometrically Accurate Radiance Fields", "author": "Binbin Huang and Zehao Yu and Anpei Chen and Andreas Geiger and Shenghua Gao", "abstract": "  3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\nreconstruction, achieving high quality novel view synthesis and fast rendering\nspeed without baking. However, 3DGS fails to accurately represent surfaces due\nto the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\nSplatting (2DGS), a novel approach to model and reconstruct geometrically\naccurate radiance fields from multi-view images. Our key idea is to collapse\nthe 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D\nGaussians, 2D Gaussians provide view-consistent geometry while modeling\nsurfaces intrinsically. To accurately recover thin surfaces and achieve stable\noptimization, we introduce a perspective-accurate 2D splatting process\nutilizing ray-splat intersection and rasterization. Additionally, we\nincorporate depth distortion and normal consistency terms to further enhance\nthe quality of the reconstructions. We demonstrate that our differentiable\nrenderer allows for noise-free and detailed geometry reconstruction while\nmaintaining competitive appearance quality, fast training speed, and real-time\nrendering. Our code will be made publicly available.\n", "link": "http://arxiv.org/abs/2403.17888v1", "date": "2024-03-26", "relevancy": 1.4372, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4837}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4827}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4757}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%202D%20Gaussian%20Splatting%20for%20Geometrically%20Accurate%20Radiance%20Fields&body=Title%3A%202D%20Gaussian%20Splatting%20for%20Geometrically%20Accurate%20Radiance%20Fields%0AAuthor%3A%20Binbin%20Huang%20and%20Zehao%20Yu%20and%20Anpei%20Chen%20and%20Andreas%20Geiger%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20revolutionized%20radiance%20field%0Areconstruction%2C%20achieving%20high%20quality%20novel%20view%20synthesis%20and%20fast%20rendering%0Aspeed%20without%20baking.%20However%2C%203DGS%20fails%20to%20accurately%20represent%20surfaces%20due%0Ato%20the%20multi-view%20inconsistent%20nature%20of%203D%20Gaussians.%20We%20present%202D%20Gaussian%0ASplatting%20%282DGS%29%2C%20a%20novel%20approach%20to%20model%20and%20reconstruct%20geometrically%0Aaccurate%20radiance%20fields%20from%20multi-view%20images.%20Our%20key%20idea%20is%20to%20collapse%0Athe%203D%20volume%20into%20a%20set%20of%202D%20oriented%20planar%20Gaussian%20disks.%20Unlike%203D%0AGaussians%2C%202D%20Gaussians%20provide%20view-consistent%20geometry%20while%20modeling%0Asurfaces%20intrinsically.%20To%20accurately%20recover%20thin%20surfaces%20and%20achieve%20stable%0Aoptimization%2C%20we%20introduce%20a%20perspective-accurate%202D%20splatting%20process%0Autilizing%20ray-splat%20intersection%20and%20rasterization.%20Additionally%2C%20we%0Aincorporate%20depth%20distortion%20and%20normal%20consistency%20terms%20to%20further%20enhance%0Athe%20quality%20of%20the%20reconstructions.%20We%20demonstrate%20that%20our%20differentiable%0Arenderer%20allows%20for%20noise-free%20and%20detailed%20geometry%20reconstruction%20while%0Amaintaining%20competitive%20appearance%20quality%2C%20fast%20training%20speed%2C%20and%20real-time%0Arendering.%20Our%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17888v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2D%20Gaussian%20Splatting%20for%20Geometrically%20Accurate%20Radiance%20Fields&entry.906535625=Binbin%20Huang%20and%20Zehao%20Yu%20and%20Anpei%20Chen%20and%20Andreas%20Geiger%20and%20Shenghua%20Gao&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20revolutionized%20radiance%20field%0Areconstruction%2C%20achieving%20high%20quality%20novel%20view%20synthesis%20and%20fast%20rendering%0Aspeed%20without%20baking.%20However%2C%203DGS%20fails%20to%20accurately%20represent%20surfaces%20due%0Ato%20the%20multi-view%20inconsistent%20nature%20of%203D%20Gaussians.%20We%20present%202D%20Gaussian%0ASplatting%20%282DGS%29%2C%20a%20novel%20approach%20to%20model%20and%20reconstruct%20geometrically%0Aaccurate%20radiance%20fields%20from%20multi-view%20images.%20Our%20key%20idea%20is%20to%20collapse%0Athe%203D%20volume%20into%20a%20set%20of%202D%20oriented%20planar%20Gaussian%20disks.%20Unlike%203D%0AGaussians%2C%202D%20Gaussians%20provide%20view-consistent%20geometry%20while%20modeling%0Asurfaces%20intrinsically.%20To%20accurately%20recover%20thin%20surfaces%20and%20achieve%20stable%0Aoptimization%2C%20we%20introduce%20a%20perspective-accurate%202D%20splatting%20process%0Autilizing%20ray-splat%20intersection%20and%20rasterization.%20Additionally%2C%20we%0Aincorporate%20depth%20distortion%20and%20normal%20consistency%20terms%20to%20further%20enhance%0Athe%20quality%20of%20the%20reconstructions.%20We%20demonstrate%20that%20our%20differentiable%0Arenderer%20allows%20for%20noise-free%20and%20detailed%20geometry%20reconstruction%20while%0Amaintaining%20competitive%20appearance%20quality%2C%20fast%20training%20speed%2C%20and%20real-time%0Arendering.%20Our%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17888v1&entry.124074799=Read"},
{"title": "Addressing Social Misattributions of Large Language Models: An\n  HCXAI-based Approach", "author": "Andrea Ferrario and Alberto Termine and Alessandro Facchini", "abstract": "  Human-centered explainable AI (HCXAI) advocates for the integration of social\naspects into AI explanations. Central to the HCXAI discourse is the Social\nTransparency (ST) framework, which aims to make the socio-organizational\ncontext of AI systems accessible to their users. In this work, we suggest\nextending the ST framework to address the risks of social misattributions in\nLarge Language Models (LLMs), particularly in sensitive areas like mental\nhealth. In fact LLMs, which are remarkably capable of simulating roles and\npersonas, may lead to mismatches between designers' intentions and users'\nperceptions of social attributes, risking to promote emotional manipulation and\ndangerous behaviors, cases of epistemic injustice, and unwarranted trust. To\naddress these issues, we propose enhancing the ST framework with a fifth\n'W-question' to clarify the specific social attributions assigned to LLMs by\nits designers and users. This addition aims to bridge the gap between LLM\ncapabilities and user perceptions, promoting the ethically responsible\ndevelopment and use of LLM-based technology.\n", "link": "http://arxiv.org/abs/2403.17873v1", "date": "2024-03-26", "relevancy": 1.4206, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4807}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4744}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4642}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Addressing%20Social%20Misattributions%20of%20Large%20Language%20Models%3A%20An%0A%20%20HCXAI-based%20Approach&body=Title%3A%20Addressing%20Social%20Misattributions%20of%20Large%20Language%20Models%3A%20An%0A%20%20HCXAI-based%20Approach%0AAuthor%3A%20Andrea%20Ferrario%20and%20Alberto%20Termine%20and%20Alessandro%20Facchini%0AAbstract%3A%20%20%20Human-centered%20explainable%20AI%20%28HCXAI%29%20advocates%20for%20the%20integration%20of%20social%0Aaspects%20into%20AI%20explanations.%20Central%20to%20the%20HCXAI%20discourse%20is%20the%20Social%0ATransparency%20%28ST%29%20framework%2C%20which%20aims%20to%20make%20the%20socio-organizational%0Acontext%20of%20AI%20systems%20accessible%20to%20their%20users.%20In%20this%20work%2C%20we%20suggest%0Aextending%20the%20ST%20framework%20to%20address%20the%20risks%20of%20social%20misattributions%20in%0ALarge%20Language%20Models%20%28LLMs%29%2C%20particularly%20in%20sensitive%20areas%20like%20mental%0Ahealth.%20In%20fact%20LLMs%2C%20which%20are%20remarkably%20capable%20of%20simulating%20roles%20and%0Apersonas%2C%20may%20lead%20to%20mismatches%20between%20designers%27%20intentions%20and%20users%27%0Aperceptions%20of%20social%20attributes%2C%20risking%20to%20promote%20emotional%20manipulation%20and%0Adangerous%20behaviors%2C%20cases%20of%20epistemic%20injustice%2C%20and%20unwarranted%20trust.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20enhancing%20the%20ST%20framework%20with%20a%20fifth%0A%27W-question%27%20to%20clarify%20the%20specific%20social%20attributions%20assigned%20to%20LLMs%20by%0Aits%20designers%20and%20users.%20This%20addition%20aims%20to%20bridge%20the%20gap%20between%20LLM%0Acapabilities%20and%20user%20perceptions%2C%20promoting%20the%20ethically%20responsible%0Adevelopment%20and%20use%20of%20LLM-based%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17873v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Social%20Misattributions%20of%20Large%20Language%20Models%3A%20An%0A%20%20HCXAI-based%20Approach&entry.906535625=Andrea%20Ferrario%20and%20Alberto%20Termine%20and%20Alessandro%20Facchini&entry.1292438233=%20%20Human-centered%20explainable%20AI%20%28HCXAI%29%20advocates%20for%20the%20integration%20of%20social%0Aaspects%20into%20AI%20explanations.%20Central%20to%20the%20HCXAI%20discourse%20is%20the%20Social%0ATransparency%20%28ST%29%20framework%2C%20which%20aims%20to%20make%20the%20socio-organizational%0Acontext%20of%20AI%20systems%20accessible%20to%20their%20users.%20In%20this%20work%2C%20we%20suggest%0Aextending%20the%20ST%20framework%20to%20address%20the%20risks%20of%20social%20misattributions%20in%0ALarge%20Language%20Models%20%28LLMs%29%2C%20particularly%20in%20sensitive%20areas%20like%20mental%0Ahealth.%20In%20fact%20LLMs%2C%20which%20are%20remarkably%20capable%20of%20simulating%20roles%20and%0Apersonas%2C%20may%20lead%20to%20mismatches%20between%20designers%27%20intentions%20and%20users%27%0Aperceptions%20of%20social%20attributes%2C%20risking%20to%20promote%20emotional%20manipulation%20and%0Adangerous%20behaviors%2C%20cases%20of%20epistemic%20injustice%2C%20and%20unwarranted%20trust.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20enhancing%20the%20ST%20framework%20with%20a%20fifth%0A%27W-question%27%20to%20clarify%20the%20specific%20social%20attributions%20assigned%20to%20LLMs%20by%0Aits%20designers%20and%20users.%20This%20addition%20aims%20to%20bridge%20the%20gap%20between%20LLM%0Acapabilities%20and%20user%20perceptions%2C%20promoting%20the%20ethically%20responsible%0Adevelopment%20and%20use%20of%20LLM-based%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17873v1&entry.124074799=Read"},
{"title": "Decode Neural signal as Speech", "author": "Yiqian Yang and Yiqun Duan and Qiang Zhang and Renjing Xu and Hui Xiong", "abstract": "  Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used ``teacher-forcing\" during generative decoding, which is\nimpractical; 3) prior works are mostly ``BART-based\" not fully auto-regressive,\nwhich performs better in other sequence tasks. In this paper, we explore the\nbrain-to-text translation of MEG signals in a speech-decoding formation. Here\nwe are the first to investigate a cross-attention-based ``whisper\" model for\ngenerating text directly from MEG signals without teacher forcing. Our model\nachieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \\&\nteacher-forcing on two major datasets (\\textit{GWilliams} and\n\\textit{Schoffelen}). This paper conducts a comprehensive review to understand\nhow speech decoding formation performs on the neural decoding tasks, including\npretraining initialization, training \\& evaluation set splitting, augmentation,\nand scaling law.\n", "link": "http://arxiv.org/abs/2403.01748v2", "date": "2024-03-26", "relevancy": 1.4194, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5034}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4666}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4593}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Decode%20Neural%20signal%20as%20Speech&body=Title%3A%20Decode%20Neural%20signal%20as%20Speech%0AAuthor%3A%20Yiqian%20Yang%20and%20Yiqun%20Duan%20and%20Qiang%20Zhang%20and%20Renjing%20Xu%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Decoding%20language%20from%20brain%20dynamics%20is%20an%20important%20open%20direction%20in%20the%0Arealm%20of%20brain-computer%20interface%20%28BCI%29%2C%20especially%20considering%20the%20rapid%0Agrowth%20of%20large%20language%20models.%20Compared%20to%20invasive-based%20signals%20which%0Arequire%20electrode%20implantation%20surgery%2C%20non-invasive%20neural%20signals%20%28e.g.%20EEG%2C%0AMEG%29%20have%20attracted%20increasing%20attention%20considering%20their%20safety%20and%0Agenerality.%20However%2C%20the%20exploration%20is%20not%20adequate%20in%20three%20aspects%3A%201%29%0Aprevious%20methods%20mainly%20focus%20on%20EEG%20but%20none%20of%20the%20previous%20works%20address%0Athis%20problem%20on%20MEG%20with%20better%20signal%20quality%3B%202%29%20prior%20works%20have%0Apredominantly%20used%20%60%60teacher-forcing%22%20during%20generative%20decoding%2C%20which%20is%0Aimpractical%3B%203%29%20prior%20works%20are%20mostly%20%60%60BART-based%22%20not%20fully%20auto-regressive%2C%0Awhich%20performs%20better%20in%20other%20sequence%20tasks.%20In%20this%20paper%2C%20we%20explore%20the%0Abrain-to-text%20translation%20of%20MEG%20signals%20in%20a%20speech-decoding%20formation.%20Here%0Awe%20are%20the%20first%20to%20investigate%20a%20cross-attention-based%20%60%60whisper%22%20model%20for%0Agenerating%20text%20directly%20from%20MEG%20signals%20without%20teacher%20forcing.%20Our%20model%0Aachieves%20impressive%20BLEU-1%20scores%20of%2060.30%20and%2052.89%20without%20pretraining%20%5C%26%0Ateacher-forcing%20on%20two%20major%20datasets%20%28%5Ctextit%7BGWilliams%7D%20and%0A%5Ctextit%7BSchoffelen%7D%29.%20This%20paper%20conducts%20a%20comprehensive%20review%20to%20understand%0Ahow%20speech%20decoding%20formation%20performs%20on%20the%20neural%20decoding%20tasks%2C%20including%0Apretraining%20initialization%2C%20training%20%5C%26%20evaluation%20set%20splitting%2C%20augmentation%2C%0Aand%20scaling%20law.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01748v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decode%20Neural%20signal%20as%20Speech&entry.906535625=Yiqian%20Yang%20and%20Yiqun%20Duan%20and%20Qiang%20Zhang%20and%20Renjing%20Xu%20and%20Hui%20Xiong&entry.1292438233=%20%20Decoding%20language%20from%20brain%20dynamics%20is%20an%20important%20open%20direction%20in%20the%0Arealm%20of%20brain-computer%20interface%20%28BCI%29%2C%20especially%20considering%20the%20rapid%0Agrowth%20of%20large%20language%20models.%20Compared%20to%20invasive-based%20signals%20which%0Arequire%20electrode%20implantation%20surgery%2C%20non-invasive%20neural%20signals%20%28e.g.%20EEG%2C%0AMEG%29%20have%20attracted%20increasing%20attention%20considering%20their%20safety%20and%0Agenerality.%20However%2C%20the%20exploration%20is%20not%20adequate%20in%20three%20aspects%3A%201%29%0Aprevious%20methods%20mainly%20focus%20on%20EEG%20but%20none%20of%20the%20previous%20works%20address%0Athis%20problem%20on%20MEG%20with%20better%20signal%20quality%3B%202%29%20prior%20works%20have%0Apredominantly%20used%20%60%60teacher-forcing%22%20during%20generative%20decoding%2C%20which%20is%0Aimpractical%3B%203%29%20prior%20works%20are%20mostly%20%60%60BART-based%22%20not%20fully%20auto-regressive%2C%0Awhich%20performs%20better%20in%20other%20sequence%20tasks.%20In%20this%20paper%2C%20we%20explore%20the%0Abrain-to-text%20translation%20of%20MEG%20signals%20in%20a%20speech-decoding%20formation.%20Here%0Awe%20are%20the%20first%20to%20investigate%20a%20cross-attention-based%20%60%60whisper%22%20model%20for%0Agenerating%20text%20directly%20from%20MEG%20signals%20without%20teacher%20forcing.%20Our%20model%0Aachieves%20impressive%20BLEU-1%20scores%20of%2060.30%20and%2052.89%20without%20pretraining%20%5C%26%0Ateacher-forcing%20on%20two%20major%20datasets%20%28%5Ctextit%7BGWilliams%7D%20and%0A%5Ctextit%7BSchoffelen%7D%29.%20This%20paper%20conducts%20a%20comprehensive%20review%20to%20understand%0Ahow%20speech%20decoding%20formation%20performs%20on%20the%20neural%20decoding%20tasks%2C%20including%0Apretraining%20initialization%2C%20training%20%5C%26%20evaluation%20set%20splitting%2C%20augmentation%2C%0Aand%20scaling%20law.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01748v2&entry.124074799=Read"},
{"title": "Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones", "author": "Francesca Ronchini and Luca Comanducci and Mirco Pezzoli and Fabio Antonacci and Augusto Sarti", "abstract": "  Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several impor- tant real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex- valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.\n", "link": "http://arxiv.org/abs/2402.04866v2", "date": "2024-03-26", "relevancy": 1.412, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4888}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4696}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4552}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Room%20Transfer%20Function%20Reconstruction%20Using%20Complex-valued%20Neural%0A%20%20Networks%20and%20Irregularly%20Distributed%20Microphones&body=Title%3A%20Room%20Transfer%20Function%20Reconstruction%20Using%20Complex-valued%20Neural%0A%20%20Networks%20and%20Irregularly%20Distributed%20Microphones%0AAuthor%3A%20Francesca%20Ronchini%20and%20Luca%20Comanducci%20and%20Mirco%20Pezzoli%20and%20Fabio%20Antonacci%20and%20Augusto%20Sarti%0AAbstract%3A%20%20%20Reconstructing%20the%20room%20transfer%20functions%20needed%20to%20calculate%20the%20complex%0Asound%20field%20in%20a%20room%20has%20several%20impor-%20tant%20real-world%20applications.%20However%2C%0Aan%20unpractical%20number%20of%20microphones%20is%20often%20required.%20Recently%2C%20in%20addition%0Ato%20classical%20signal%20processing%20methods%2C%20deep%20learning%20techniques%20have%20been%0Aapplied%20to%20reconstruct%20the%20room%20transfer%20function%20starting%20from%20a%20very%20limited%0Aset%20of%20measurements%20at%20scattered%20points%20in%20the%20room.%20In%20this%20paper%2C%20we%20employ%0Acomplex-valued%20neural%20networks%20to%20estimate%20room%20transfer%20functions%20in%20the%0Afrequency%20range%20of%20the%20first%20room%20resonances%2C%20using%20a%20few%20irregularly%0Adistributed%20microphones.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%0Athat%20complex-valued%20neural%20networks%20are%20used%20to%20estimate%20room%20transfer%0Afunctions.%20To%20analyze%20the%20benefits%20of%20applying%20complex-%20valued%20optimization%20to%0Athe%20considered%20task%2C%20we%20compare%20the%20proposed%20technique%20with%20a%20state-of-the-art%0Akernel-based%20signal%20processing%20approach%20for%20sound%20field%20reconstruction%2C%20showing%0Athat%20the%20proposed%20technique%20exhibits%20relevant%20advantages%20in%20terms%20of%20phase%0Aaccuracy%20and%20overall%20quality%20of%20the%20reconstructed%20sound%20field.%20For%20informative%0Apurposes%2C%20we%20also%20compare%20the%20model%20with%20a%20similarly-structured%20data-driven%0Aapproach%20that%2C%20however%2C%20applies%20a%20real-valued%20neural%20network%20to%20reconstruct%0Aonly%20the%20magnitude%20of%20the%20sound%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04866v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Room%20Transfer%20Function%20Reconstruction%20Using%20Complex-valued%20Neural%0A%20%20Networks%20and%20Irregularly%20Distributed%20Microphones&entry.906535625=Francesca%20Ronchini%20and%20Luca%20Comanducci%20and%20Mirco%20Pezzoli%20and%20Fabio%20Antonacci%20and%20Augusto%20Sarti&entry.1292438233=%20%20Reconstructing%20the%20room%20transfer%20functions%20needed%20to%20calculate%20the%20complex%0Asound%20field%20in%20a%20room%20has%20several%20impor-%20tant%20real-world%20applications.%20However%2C%0Aan%20unpractical%20number%20of%20microphones%20is%20often%20required.%20Recently%2C%20in%20addition%0Ato%20classical%20signal%20processing%20methods%2C%20deep%20learning%20techniques%20have%20been%0Aapplied%20to%20reconstruct%20the%20room%20transfer%20function%20starting%20from%20a%20very%20limited%0Aset%20of%20measurements%20at%20scattered%20points%20in%20the%20room.%20In%20this%20paper%2C%20we%20employ%0Acomplex-valued%20neural%20networks%20to%20estimate%20room%20transfer%20functions%20in%20the%0Afrequency%20range%20of%20the%20first%20room%20resonances%2C%20using%20a%20few%20irregularly%0Adistributed%20microphones.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%0Athat%20complex-valued%20neural%20networks%20are%20used%20to%20estimate%20room%20transfer%0Afunctions.%20To%20analyze%20the%20benefits%20of%20applying%20complex-%20valued%20optimization%20to%0Athe%20considered%20task%2C%20we%20compare%20the%20proposed%20technique%20with%20a%20state-of-the-art%0Akernel-based%20signal%20processing%20approach%20for%20sound%20field%20reconstruction%2C%20showing%0Athat%20the%20proposed%20technique%20exhibits%20relevant%20advantages%20in%20terms%20of%20phase%0Aaccuracy%20and%20overall%20quality%20of%20the%20reconstructed%20sound%20field.%20For%20informative%0Apurposes%2C%20we%20also%20compare%20the%20model%20with%20a%20similarly-structured%20data-driven%0Aapproach%20that%2C%20however%2C%20applies%20a%20real-valued%20neural%20network%20to%20reconstruct%0Aonly%20the%20magnitude%20of%20the%20sound%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04866v2&entry.124074799=Read"},
{"title": "The Unreasonable Ineffectiveness of the Deeper Layers", "author": "Andrey Gromov and Kushal Tirumala and Hassan Shapourian and Paolo Glorioso and Daniel A. Roberts", "abstract": "  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n", "link": "http://arxiv.org/abs/2403.17887v1", "date": "2024-03-26", "relevancy": 1.4068, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.47}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4692}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4674}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Unreasonable%20Ineffectiveness%20of%20the%20Deeper%20Layers&body=Title%3A%20The%20Unreasonable%20Ineffectiveness%20of%20the%20Deeper%20Layers%0AAuthor%3A%20Andrey%20Gromov%20and%20Kushal%20Tirumala%20and%20Hassan%20Shapourian%20and%20Paolo%20Glorioso%20and%20Daniel%20A.%20Roberts%0AAbstract%3A%20%20%20We%20empirically%20study%20a%20simple%20layer-pruning%20strategy%20for%20popular%20families%20of%0Aopen-weight%20pretrained%20LLMs%2C%20finding%20minimal%20degradation%20of%20performance%20on%0Adifferent%20question-answering%20benchmarks%20until%20after%20a%20large%20fraction%20%28up%20to%0Ahalf%29%20of%20the%20layers%20are%20removed.%20To%20prune%20these%20models%2C%20we%20identify%20the%20optimal%0Ablock%20of%20layers%20to%20prune%20by%20considering%20similarity%20across%20layers%3B%20then%2C%20to%0A%22heal%22%20the%20damage%2C%20we%20perform%20a%20small%20amount%20of%20finetuning.%20In%20particular%2C%20we%0Ause%20parameter-efficient%20finetuning%20%28PEFT%29%20methods%2C%20specifically%20quantization%0Aand%20Low%20Rank%20Adapters%20%28QLoRA%29%2C%20such%20that%20each%20of%20our%20experiments%20can%20be%0Aperformed%20on%20a%20single%20A100%20GPU.%20From%20a%20practical%20perspective%2C%20these%20results%0Asuggest%20that%20layer%20pruning%20methods%20can%20complement%20other%20PEFT%20strategies%20to%0Afurther%20reduce%20computational%20resources%20of%20finetuning%20on%20the%20one%20hand%2C%20and%20can%0Aimprove%20the%20memory%20and%20latency%20of%20inference%20on%20the%20other%20hand.%20From%20a%0Ascientific%20perspective%2C%20the%20robustness%20of%20these%20LLMs%20to%20the%20deletion%20of%20layers%0Aimplies%20either%20that%20current%20pretraining%20methods%20are%20not%20properly%20leveraging%20the%0Aparameters%20in%20the%20deeper%20layers%20of%20the%20network%20or%20that%20the%20shallow%20layers%20play%0Aa%20critical%20role%20in%20storing%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17887v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unreasonable%20Ineffectiveness%20of%20the%20Deeper%20Layers&entry.906535625=Andrey%20Gromov%20and%20Kushal%20Tirumala%20and%20Hassan%20Shapourian%20and%20Paolo%20Glorioso%20and%20Daniel%20A.%20Roberts&entry.1292438233=%20%20We%20empirically%20study%20a%20simple%20layer-pruning%20strategy%20for%20popular%20families%20of%0Aopen-weight%20pretrained%20LLMs%2C%20finding%20minimal%20degradation%20of%20performance%20on%0Adifferent%20question-answering%20benchmarks%20until%20after%20a%20large%20fraction%20%28up%20to%0Ahalf%29%20of%20the%20layers%20are%20removed.%20To%20prune%20these%20models%2C%20we%20identify%20the%20optimal%0Ablock%20of%20layers%20to%20prune%20by%20considering%20similarity%20across%20layers%3B%20then%2C%20to%0A%22heal%22%20the%20damage%2C%20we%20perform%20a%20small%20amount%20of%20finetuning.%20In%20particular%2C%20we%0Ause%20parameter-efficient%20finetuning%20%28PEFT%29%20methods%2C%20specifically%20quantization%0Aand%20Low%20Rank%20Adapters%20%28QLoRA%29%2C%20such%20that%20each%20of%20our%20experiments%20can%20be%0Aperformed%20on%20a%20single%20A100%20GPU.%20From%20a%20practical%20perspective%2C%20these%20results%0Asuggest%20that%20layer%20pruning%20methods%20can%20complement%20other%20PEFT%20strategies%20to%0Afurther%20reduce%20computational%20resources%20of%20finetuning%20on%20the%20one%20hand%2C%20and%20can%0Aimprove%20the%20memory%20and%20latency%20of%20inference%20on%20the%20other%20hand.%20From%20a%0Ascientific%20perspective%2C%20the%20robustness%20of%20these%20LLMs%20to%20the%20deletion%20of%20layers%0Aimplies%20either%20that%20current%20pretraining%20methods%20are%20not%20properly%20leveraging%20the%0Aparameters%20in%20the%20deeper%20layers%20of%20the%20network%20or%20that%20the%20shallow%20layers%20play%0Aa%20critical%20role%20in%20storing%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17887v1&entry.124074799=Read"},
{"title": "Counterfactual Fairness through Transforming Data Orthogonal to Bias", "author": "Shuyi Chen and Shixiang Zhu", "abstract": "  Machine learning models have shown exceptional prowess in solving complex\nissues across various domains. Nonetheless, these models can sometimes exhibit\nbiased decision-making, leading to disparities in treatment across different\ngroups. Despite the extensive research on fairness, the nuanced effects of\nmultivariate and continuous sensitive variables on decision-making outcomes\nremain insufficiently studied. We introduce a novel data pre-processing\nalgorithm, Orthogonal to Bias (OB), designed to remove the influence of a group\nof continuous sensitive variables, thereby facilitating counterfactual fairness\nin machine learning applications. Our approach is grounded in the assumption of\na jointly normal distribution within a structural causal model (SCM), proving\nthat counterfactual fairness can be achieved by ensuring the data is\nuncorrelated with sensitive variables. The OB algorithm is model-agnostic,\ncatering to a wide array of machine learning models and tasks, and includes a\nsparse variant to enhance numerical stability through regularization. Through\nempirical evaluation on simulated and real-world datasets - including the adult\nincome and the COMPAS recidivism datasets - our methodology demonstrates its\ncapacity to enable fairer outcomes without compromising accuracy.\n", "link": "http://arxiv.org/abs/2403.17852v1", "date": "2024-03-26", "relevancy": 1.398, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4644}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4578}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Fairness%20through%20Transforming%20Data%20Orthogonal%20to%20Bias&body=Title%3A%20Counterfactual%20Fairness%20through%20Transforming%20Data%20Orthogonal%20to%20Bias%0AAuthor%3A%20Shuyi%20Chen%20and%20Shixiang%20Zhu%0AAbstract%3A%20%20%20Machine%20learning%20models%20have%20shown%20exceptional%20prowess%20in%20solving%20complex%0Aissues%20across%20various%20domains.%20Nonetheless%2C%20these%20models%20can%20sometimes%20exhibit%0Abiased%20decision-making%2C%20leading%20to%20disparities%20in%20treatment%20across%20different%0Agroups.%20Despite%20the%20extensive%20research%20on%20fairness%2C%20the%20nuanced%20effects%20of%0Amultivariate%20and%20continuous%20sensitive%20variables%20on%20decision-making%20outcomes%0Aremain%20insufficiently%20studied.%20We%20introduce%20a%20novel%20data%20pre-processing%0Aalgorithm%2C%20Orthogonal%20to%20Bias%20%28OB%29%2C%20designed%20to%20remove%20the%20influence%20of%20a%20group%0Aof%20continuous%20sensitive%20variables%2C%20thereby%20facilitating%20counterfactual%20fairness%0Ain%20machine%20learning%20applications.%20Our%20approach%20is%20grounded%20in%20the%20assumption%20of%0Aa%20jointly%20normal%20distribution%20within%20a%20structural%20causal%20model%20%28SCM%29%2C%20proving%0Athat%20counterfactual%20fairness%20can%20be%20achieved%20by%20ensuring%20the%20data%20is%0Auncorrelated%20with%20sensitive%20variables.%20The%20OB%20algorithm%20is%20model-agnostic%2C%0Acatering%20to%20a%20wide%20array%20of%20machine%20learning%20models%20and%20tasks%2C%20and%20includes%20a%0Asparse%20variant%20to%20enhance%20numerical%20stability%20through%20regularization.%20Through%0Aempirical%20evaluation%20on%20simulated%20and%20real-world%20datasets%20-%20including%20the%20adult%0Aincome%20and%20the%20COMPAS%20recidivism%20datasets%20-%20our%20methodology%20demonstrates%20its%0Acapacity%20to%20enable%20fairer%20outcomes%20without%20compromising%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17852v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Fairness%20through%20Transforming%20Data%20Orthogonal%20to%20Bias&entry.906535625=Shuyi%20Chen%20and%20Shixiang%20Zhu&entry.1292438233=%20%20Machine%20learning%20models%20have%20shown%20exceptional%20prowess%20in%20solving%20complex%0Aissues%20across%20various%20domains.%20Nonetheless%2C%20these%20models%20can%20sometimes%20exhibit%0Abiased%20decision-making%2C%20leading%20to%20disparities%20in%20treatment%20across%20different%0Agroups.%20Despite%20the%20extensive%20research%20on%20fairness%2C%20the%20nuanced%20effects%20of%0Amultivariate%20and%20continuous%20sensitive%20variables%20on%20decision-making%20outcomes%0Aremain%20insufficiently%20studied.%20We%20introduce%20a%20novel%20data%20pre-processing%0Aalgorithm%2C%20Orthogonal%20to%20Bias%20%28OB%29%2C%20designed%20to%20remove%20the%20influence%20of%20a%20group%0Aof%20continuous%20sensitive%20variables%2C%20thereby%20facilitating%20counterfactual%20fairness%0Ain%20machine%20learning%20applications.%20Our%20approach%20is%20grounded%20in%20the%20assumption%20of%0Aa%20jointly%20normal%20distribution%20within%20a%20structural%20causal%20model%20%28SCM%29%2C%20proving%0Athat%20counterfactual%20fairness%20can%20be%20achieved%20by%20ensuring%20the%20data%20is%0Auncorrelated%20with%20sensitive%20variables.%20The%20OB%20algorithm%20is%20model-agnostic%2C%0Acatering%20to%20a%20wide%20array%20of%20machine%20learning%20models%20and%20tasks%2C%20and%20includes%20a%0Asparse%20variant%20to%20enhance%20numerical%20stability%20through%20regularization.%20Through%0Aempirical%20evaluation%20on%20simulated%20and%20real-world%20datasets%20-%20including%20the%20adult%0Aincome%20and%20the%20COMPAS%20recidivism%20datasets%20-%20our%20methodology%20demonstrates%20its%0Acapacity%20to%20enable%20fairer%20outcomes%20without%20compromising%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17852v1&entry.124074799=Read"},
{"title": "Analyzing the Quality Attributes of AI Vision Models in Open\n  Repositories Under Adversarial Attacks", "author": "Zerui Wang and Yan Liu", "abstract": "  As AI models rapidly evolve, they are frequently released to open\nrepositories, such as HuggingFace. It is essential to perform quality assurance\nvalidation on these models before integrating them into the production\ndevelopment lifecycle. In addition to evaluating efficiency in terms of\nbalanced accuracy and computing costs, adversarial attacks are potential\nthreats to the robustness and explainability of AI models. Meanwhile, XAI\napplies algorithms that approximate inputs to outputs post-hoc to identify the\ncontributing features. Adversarial perturbations may also degrade the utility\nof XAI explanations that require further investigation. In this paper, we\npresent an integrated process designed for downstream evaluation tasks,\nincluding validating AI model accuracy, evaluating robustness with benchmark\nperturbations, comparing explanation utility, and assessing overhead. We\ndemonstrate an evaluation scenario involving six computer vision models, which\ninclude CNN-based, Transformer-based, and hybrid architectures, three types of\nperturbations, and five XAI methods, resulting in ninety unique combinations.\nThe process reveals the explanation utility among the XAI methods in terms of\nthe identified key areas responding to the adversarial perturbation. The\nprocess produces aggregated results that illustrate multiple attributes of each\nAI model.\n", "link": "http://arxiv.org/abs/2401.12261v2", "date": "2024-03-26", "relevancy": 1.3927, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4685}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4635}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4618}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Analyzing%20the%20Quality%20Attributes%20of%20AI%20Vision%20Models%20in%20Open%0A%20%20Repositories%20Under%20Adversarial%20Attacks&body=Title%3A%20Analyzing%20the%20Quality%20Attributes%20of%20AI%20Vision%20Models%20in%20Open%0A%20%20Repositories%20Under%20Adversarial%20Attacks%0AAuthor%3A%20Zerui%20Wang%20and%20Yan%20Liu%0AAbstract%3A%20%20%20As%20AI%20models%20rapidly%20evolve%2C%20they%20are%20frequently%20released%20to%20open%0Arepositories%2C%20such%20as%20HuggingFace.%20It%20is%20essential%20to%20perform%20quality%20assurance%0Avalidation%20on%20these%20models%20before%20integrating%20them%20into%20the%20production%0Adevelopment%20lifecycle.%20In%20addition%20to%20evaluating%20efficiency%20in%20terms%20of%0Abalanced%20accuracy%20and%20computing%20costs%2C%20adversarial%20attacks%20are%20potential%0Athreats%20to%20the%20robustness%20and%20explainability%20of%20AI%20models.%20Meanwhile%2C%20XAI%0Aapplies%20algorithms%20that%20approximate%20inputs%20to%20outputs%20post-hoc%20to%20identify%20the%0Acontributing%20features.%20Adversarial%20perturbations%20may%20also%20degrade%20the%20utility%0Aof%20XAI%20explanations%20that%20require%20further%20investigation.%20In%20this%20paper%2C%20we%0Apresent%20an%20integrated%20process%20designed%20for%20downstream%20evaluation%20tasks%2C%0Aincluding%20validating%20AI%20model%20accuracy%2C%20evaluating%20robustness%20with%20benchmark%0Aperturbations%2C%20comparing%20explanation%20utility%2C%20and%20assessing%20overhead.%20We%0Ademonstrate%20an%20evaluation%20scenario%20involving%20six%20computer%20vision%20models%2C%20which%0Ainclude%20CNN-based%2C%20Transformer-based%2C%20and%20hybrid%20architectures%2C%20three%20types%20of%0Aperturbations%2C%20and%20five%20XAI%20methods%2C%20resulting%20in%20ninety%20unique%20combinations.%0AThe%20process%20reveals%20the%20explanation%20utility%20among%20the%20XAI%20methods%20in%20terms%20of%0Athe%20identified%20key%20areas%20responding%20to%20the%20adversarial%20perturbation.%20The%0Aprocess%20produces%20aggregated%20results%20that%20illustrate%20multiple%20attributes%20of%20each%0AAI%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12261v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20the%20Quality%20Attributes%20of%20AI%20Vision%20Models%20in%20Open%0A%20%20Repositories%20Under%20Adversarial%20Attacks&entry.906535625=Zerui%20Wang%20and%20Yan%20Liu&entry.1292438233=%20%20As%20AI%20models%20rapidly%20evolve%2C%20they%20are%20frequently%20released%20to%20open%0Arepositories%2C%20such%20as%20HuggingFace.%20It%20is%20essential%20to%20perform%20quality%20assurance%0Avalidation%20on%20these%20models%20before%20integrating%20them%20into%20the%20production%0Adevelopment%20lifecycle.%20In%20addition%20to%20evaluating%20efficiency%20in%20terms%20of%0Abalanced%20accuracy%20and%20computing%20costs%2C%20adversarial%20attacks%20are%20potential%0Athreats%20to%20the%20robustness%20and%20explainability%20of%20AI%20models.%20Meanwhile%2C%20XAI%0Aapplies%20algorithms%20that%20approximate%20inputs%20to%20outputs%20post-hoc%20to%20identify%20the%0Acontributing%20features.%20Adversarial%20perturbations%20may%20also%20degrade%20the%20utility%0Aof%20XAI%20explanations%20that%20require%20further%20investigation.%20In%20this%20paper%2C%20we%0Apresent%20an%20integrated%20process%20designed%20for%20downstream%20evaluation%20tasks%2C%0Aincluding%20validating%20AI%20model%20accuracy%2C%20evaluating%20robustness%20with%20benchmark%0Aperturbations%2C%20comparing%20explanation%20utility%2C%20and%20assessing%20overhead.%20We%0Ademonstrate%20an%20evaluation%20scenario%20involving%20six%20computer%20vision%20models%2C%20which%0Ainclude%20CNN-based%2C%20Transformer-based%2C%20and%20hybrid%20architectures%2C%20three%20types%20of%0Aperturbations%2C%20and%20five%20XAI%20methods%2C%20resulting%20in%20ninety%20unique%20combinations.%0AThe%20process%20reveals%20the%20explanation%20utility%20among%20the%20XAI%20methods%20in%20terms%20of%0Athe%20identified%20key%20areas%20responding%20to%20the%20adversarial%20perturbation.%20The%0Aprocess%20produces%20aggregated%20results%20that%20illustrate%20multiple%20attributes%20of%20each%0AAI%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12261v2&entry.124074799=Read"},
{"title": "Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms", "author": "Michael Hanna and Sandro Pezzelle and Yonatan Belinkov", "abstract": "  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n", "link": "http://arxiv.org/abs/2403.17806v1", "date": "2024-03-26", "relevancy": 1.3881, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4724}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4635}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4585}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Have%20Faith%20in%20Faithfulness%3A%20Going%20Beyond%20Circuit%20Overlap%20When%20Finding%0A%20%20Model%20Mechanisms&body=Title%3A%20Have%20Faith%20in%20Faithfulness%3A%20Going%20Beyond%20Circuit%20Overlap%20When%20Finding%0A%20%20Model%20Mechanisms%0AAuthor%3A%20Michael%20Hanna%20and%20Sandro%20Pezzelle%20and%20Yonatan%20Belinkov%0AAbstract%3A%20%20%20Many%20recent%20language%20model%20%28LM%29%20interpretability%20studies%20have%20adopted%20the%0Acircuits%20framework%2C%20which%20aims%20to%20find%20the%20minimal%20computational%20subgraph%2C%20or%0Acircuit%2C%20that%20explains%20LM%20behavior%20on%20a%20given%20task.%20Most%20studies%20determine%0Awhich%20edges%20belong%20in%20a%20LM%27s%20circuit%20by%20performing%20causal%20interventions%20on%20each%0Aedge%20independently%2C%20but%20this%20scales%20poorly%20with%20model%20size.%20Edge%20attribution%0Apatching%20%28EAP%29%2C%20gradient-based%20approximation%20to%20interventions%2C%20has%20emerged%20as%20a%0Ascalable%20but%20imperfect%20solution%20to%20this%20problem.%20In%20this%20paper%2C%20we%20introduce%20a%0Anew%20method%20-%20EAP%20with%20integrated%20gradients%20%28EAP-IG%29%20-%20that%20aims%20to%20better%0Amaintain%20a%20core%20property%20of%20circuits%3A%20faithfulness.%20A%20circuit%20is%20faithful%20if%0Aall%20model%20edges%20outside%20the%20circuit%20can%20be%20ablated%20without%20changing%20the%20model%27s%0Aperformance%20on%20the%20task%3B%20faithfulness%20is%20what%20justifies%20studying%20circuits%2C%0Arather%20than%20the%20full%20model.%20Our%20experiments%20demonstrate%20that%20circuits%20found%0Ausing%20EAP%20are%20less%20faithful%20than%20those%20found%20using%20EAP-IG%2C%20even%20though%20both%0Ahave%20high%20node%20overlap%20with%20circuits%20found%20previously%20using%20causal%0Ainterventions.%20We%20conclude%20more%20generally%20that%20when%20using%20circuits%20to%20compare%0Athe%20mechanisms%20models%20use%20to%20solve%20tasks%2C%20faithfulness%2C%20not%20overlap%2C%20is%20what%0Ashould%20be%20measured.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17806v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Have%20Faith%20in%20Faithfulness%3A%20Going%20Beyond%20Circuit%20Overlap%20When%20Finding%0A%20%20Model%20Mechanisms&entry.906535625=Michael%20Hanna%20and%20Sandro%20Pezzelle%20and%20Yonatan%20Belinkov&entry.1292438233=%20%20Many%20recent%20language%20model%20%28LM%29%20interpretability%20studies%20have%20adopted%20the%0Acircuits%20framework%2C%20which%20aims%20to%20find%20the%20minimal%20computational%20subgraph%2C%20or%0Acircuit%2C%20that%20explains%20LM%20behavior%20on%20a%20given%20task.%20Most%20studies%20determine%0Awhich%20edges%20belong%20in%20a%20LM%27s%20circuit%20by%20performing%20causal%20interventions%20on%20each%0Aedge%20independently%2C%20but%20this%20scales%20poorly%20with%20model%20size.%20Edge%20attribution%0Apatching%20%28EAP%29%2C%20gradient-based%20approximation%20to%20interventions%2C%20has%20emerged%20as%20a%0Ascalable%20but%20imperfect%20solution%20to%20this%20problem.%20In%20this%20paper%2C%20we%20introduce%20a%0Anew%20method%20-%20EAP%20with%20integrated%20gradients%20%28EAP-IG%29%20-%20that%20aims%20to%20better%0Amaintain%20a%20core%20property%20of%20circuits%3A%20faithfulness.%20A%20circuit%20is%20faithful%20if%0Aall%20model%20edges%20outside%20the%20circuit%20can%20be%20ablated%20without%20changing%20the%20model%27s%0Aperformance%20on%20the%20task%3B%20faithfulness%20is%20what%20justifies%20studying%20circuits%2C%0Arather%20than%20the%20full%20model.%20Our%20experiments%20demonstrate%20that%20circuits%20found%0Ausing%20EAP%20are%20less%20faithful%20than%20those%20found%20using%20EAP-IG%2C%20even%20though%20both%0Ahave%20high%20node%20overlap%20with%20circuits%20found%20previously%20using%20causal%0Ainterventions.%20We%20conclude%20more%20generally%20that%20when%20using%20circuits%20to%20compare%0Athe%20mechanisms%20models%20use%20to%20solve%20tasks%2C%20faithfulness%2C%20not%20overlap%2C%20is%20what%0Ashould%20be%20measured.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17806v1&entry.124074799=Read"},
{"title": "PINN surrogate of Li-ion battery models for parameter inference. Part I:\n  Implementation and multi-fidelity hierarchies for the single-particle model", "author": "Malik Hassanaly and Peter J. Weddle and Ryan N. King and Subhayan De and Alireza Doostan and Corey R. Randall and Eric J. Dufek and Andrew M. Colclasure and Kandler Smith", "abstract": "  To plan and optimize energy storage demands that account for Li-ion battery\naging dynamics, techniques need to be developed to diagnose battery internal\nstates accurately and rapidly. This study seeks to reduce the computational\nresources needed to determine a battery's internal states by replacing\nphysics-based Li-ion battery models -- such as the single-particle model (SPM)\nand the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)\nsurrogate. The surrogate model makes high-throughput techniques, such as\nBayesian calibration, tractable to determine battery internal parameters from\nvoltage responses. This manuscript is the first of a two-part series that\nintroduces PINN surrogates of Li-ion battery models for parameter inference\n(i.e., state-of-health diagnostics). In this first part, a method is presented\nfor constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical\ntraining, where several neural nets are trained with multiple physics-loss\nfidelities is shown to significantly improve the surrogate accuracy when only\ntraining on the governing equation residuals. The implementation is made\navailable in a companion repository (https://github.com/NREL/pinnstripes). The\ntechniques used to develop a PINN surrogate of the SPM are extended in Part II\nfor the PINN surrogate for the P2D battery model, and explore the Bayesian\ncalibration capabilities of both surrogates.\n", "link": "http://arxiv.org/abs/2312.17329v2", "date": "2024-03-26", "relevancy": 1.3477, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5053}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4348}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4293}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PINN%20surrogate%20of%20Li-ion%20battery%20models%20for%20parameter%20inference.%20Part%20I%3A%0A%20%20Implementation%20and%20multi-fidelity%20hierarchies%20for%20the%20single-particle%20model&body=Title%3A%20PINN%20surrogate%20of%20Li-ion%20battery%20models%20for%20parameter%20inference.%20Part%20I%3A%0A%20%20Implementation%20and%20multi-fidelity%20hierarchies%20for%20the%20single-particle%20model%0AAuthor%3A%20Malik%20Hassanaly%20and%20Peter%20J.%20Weddle%20and%20Ryan%20N.%20King%20and%20Subhayan%20De%20and%20Alireza%20Doostan%20and%20Corey%20R.%20Randall%20and%20Eric%20J.%20Dufek%20and%20Andrew%20M.%20Colclasure%20and%20Kandler%20Smith%0AAbstract%3A%20%20%20To%20plan%20and%20optimize%20energy%20storage%20demands%20that%20account%20for%20Li-ion%20battery%0Aaging%20dynamics%2C%20techniques%20need%20to%20be%20developed%20to%20diagnose%20battery%20internal%0Astates%20accurately%20and%20rapidly.%20This%20study%20seeks%20to%20reduce%20the%20computational%0Aresources%20needed%20to%20determine%20a%20battery%27s%20internal%20states%20by%20replacing%0Aphysics-based%20Li-ion%20battery%20models%20--%20such%20as%20the%20single-particle%20model%20%28SPM%29%0Aand%20the%20pseudo-2D%20%28P2D%29%20model%20--%20with%20a%20physics-informed%20neural%20network%20%28PINN%29%0Asurrogate.%20The%20surrogate%20model%20makes%20high-throughput%20techniques%2C%20such%20as%0ABayesian%20calibration%2C%20tractable%20to%20determine%20battery%20internal%20parameters%20from%0Avoltage%20responses.%20This%20manuscript%20is%20the%20first%20of%20a%20two-part%20series%20that%0Aintroduces%20PINN%20surrogates%20of%20Li-ion%20battery%20models%20for%20parameter%20inference%0A%28i.e.%2C%20state-of-health%20diagnostics%29.%20In%20this%20first%20part%2C%20a%20method%20is%20presented%0Afor%20constructing%20a%20PINN%20surrogate%20of%20the%20SPM.%20A%20multi-fidelity%20hierarchical%0Atraining%2C%20where%20several%20neural%20nets%20are%20trained%20with%20multiple%20physics-loss%0Afidelities%20is%20shown%20to%20significantly%20improve%20the%20surrogate%20accuracy%20when%20only%0Atraining%20on%20the%20governing%20equation%20residuals.%20The%20implementation%20is%20made%0Aavailable%20in%20a%20companion%20repository%20%28https%3A//github.com/NREL/pinnstripes%29.%20The%0Atechniques%20used%20to%20develop%20a%20PINN%20surrogate%20of%20the%20SPM%20are%20extended%20in%20Part%20II%0Afor%20the%20PINN%20surrogate%20for%20the%20P2D%20battery%20model%2C%20and%20explore%20the%20Bayesian%0Acalibration%20capabilities%20of%20both%20surrogates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17329v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PINN%20surrogate%20of%20Li-ion%20battery%20models%20for%20parameter%20inference.%20Part%20I%3A%0A%20%20Implementation%20and%20multi-fidelity%20hierarchies%20for%20the%20single-particle%20model&entry.906535625=Malik%20Hassanaly%20and%20Peter%20J.%20Weddle%20and%20Ryan%20N.%20King%20and%20Subhayan%20De%20and%20Alireza%20Doostan%20and%20Corey%20R.%20Randall%20and%20Eric%20J.%20Dufek%20and%20Andrew%20M.%20Colclasure%20and%20Kandler%20Smith&entry.1292438233=%20%20To%20plan%20and%20optimize%20energy%20storage%20demands%20that%20account%20for%20Li-ion%20battery%0Aaging%20dynamics%2C%20techniques%20need%20to%20be%20developed%20to%20diagnose%20battery%20internal%0Astates%20accurately%20and%20rapidly.%20This%20study%20seeks%20to%20reduce%20the%20computational%0Aresources%20needed%20to%20determine%20a%20battery%27s%20internal%20states%20by%20replacing%0Aphysics-based%20Li-ion%20battery%20models%20--%20such%20as%20the%20single-particle%20model%20%28SPM%29%0Aand%20the%20pseudo-2D%20%28P2D%29%20model%20--%20with%20a%20physics-informed%20neural%20network%20%28PINN%29%0Asurrogate.%20The%20surrogate%20model%20makes%20high-throughput%20techniques%2C%20such%20as%0ABayesian%20calibration%2C%20tractable%20to%20determine%20battery%20internal%20parameters%20from%0Avoltage%20responses.%20This%20manuscript%20is%20the%20first%20of%20a%20two-part%20series%20that%0Aintroduces%20PINN%20surrogates%20of%20Li-ion%20battery%20models%20for%20parameter%20inference%0A%28i.e.%2C%20state-of-health%20diagnostics%29.%20In%20this%20first%20part%2C%20a%20method%20is%20presented%0Afor%20constructing%20a%20PINN%20surrogate%20of%20the%20SPM.%20A%20multi-fidelity%20hierarchical%0Atraining%2C%20where%20several%20neural%20nets%20are%20trained%20with%20multiple%20physics-loss%0Afidelities%20is%20shown%20to%20significantly%20improve%20the%20surrogate%20accuracy%20when%20only%0Atraining%20on%20the%20governing%20equation%20residuals.%20The%20implementation%20is%20made%0Aavailable%20in%20a%20companion%20repository%20%28https%3A//github.com/NREL/pinnstripes%29.%20The%0Atechniques%20used%20to%20develop%20a%20PINN%20surrogate%20of%20the%20SPM%20are%20extended%20in%20Part%20II%0Afor%20the%20PINN%20surrogate%20for%20the%20P2D%20battery%20model%2C%20and%20explore%20the%20Bayesian%0Acalibration%20capabilities%20of%20both%20surrogates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17329v2&entry.124074799=Read"},
{"title": "Large scale paired antibody language models", "author": "Henry Kenlay and Fr\u00e9d\u00e9ric A. Dreyer and Aleksandr Kovaltsuk and Dom Miketa and Douglas Pires and Charlotte M. Deane", "abstract": "  Antibodies are proteins produced by the immune system that can identify and\nneutralise a wide variety of antigens with high specificity and affinity, and\nconstitute the most successful class of biotherapeutics. With the advent of\nnext-generation sequencing, billions of antibody sequences have been collected\nin recent years, though their application in the design of better therapeutics\nhas been constrained by the sheer volume and complexity of the data. To address\nthis challenge, we present IgBert and IgT5, the best performing\nantibody-specific language models developed to date which can consistently\nhandle both paired and unpaired variable region sequences as input. These\nmodels are trained comprehensively using the more than two billion unpaired\nsequences and two million paired sequences of light and heavy chains present in\nthe Observed Antibody Space dataset. We show that our models outperform\nexisting antibody and protein language models on a diverse range of design and\nregression tasks relevant to antibody engineering. This advancement marks a\nsignificant leap forward in leveraging machine learning, large scale data sets\nand high-performance computing for enhancing antibody design for therapeutic\ndevelopment.\n", "link": "http://arxiv.org/abs/2403.17889v1", "date": "2024-03-26", "relevancy": 1.3475, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4542}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4467}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20scale%20paired%20antibody%20language%20models&body=Title%3A%20Large%20scale%20paired%20antibody%20language%20models%0AAuthor%3A%20Henry%20Kenlay%20and%20Fr%C3%A9d%C3%A9ric%20A.%20Dreyer%20and%20Aleksandr%20Kovaltsuk%20and%20Dom%20Miketa%20and%20Douglas%20Pires%20and%20Charlotte%20M.%20Deane%0AAbstract%3A%20%20%20Antibodies%20are%20proteins%20produced%20by%20the%20immune%20system%20that%20can%20identify%20and%0Aneutralise%20a%20wide%20variety%20of%20antigens%20with%20high%20specificity%20and%20affinity%2C%20and%0Aconstitute%20the%20most%20successful%20class%20of%20biotherapeutics.%20With%20the%20advent%20of%0Anext-generation%20sequencing%2C%20billions%20of%20antibody%20sequences%20have%20been%20collected%0Ain%20recent%20years%2C%20though%20their%20application%20in%20the%20design%20of%20better%20therapeutics%0Ahas%20been%20constrained%20by%20the%20sheer%20volume%20and%20complexity%20of%20the%20data.%20To%20address%0Athis%20challenge%2C%20we%20present%20IgBert%20and%20IgT5%2C%20the%20best%20performing%0Aantibody-specific%20language%20models%20developed%20to%20date%20which%20can%20consistently%0Ahandle%20both%20paired%20and%20unpaired%20variable%20region%20sequences%20as%20input.%20These%0Amodels%20are%20trained%20comprehensively%20using%20the%20more%20than%20two%20billion%20unpaired%0Asequences%20and%20two%20million%20paired%20sequences%20of%20light%20and%20heavy%20chains%20present%20in%0Athe%20Observed%20Antibody%20Space%20dataset.%20We%20show%20that%20our%20models%20outperform%0Aexisting%20antibody%20and%20protein%20language%20models%20on%20a%20diverse%20range%20of%20design%20and%0Aregression%20tasks%20relevant%20to%20antibody%20engineering.%20This%20advancement%20marks%20a%0Asignificant%20leap%20forward%20in%20leveraging%20machine%20learning%2C%20large%20scale%20data%20sets%0Aand%20high-performance%20computing%20for%20enhancing%20antibody%20design%20for%20therapeutic%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17889v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20scale%20paired%20antibody%20language%20models&entry.906535625=Henry%20Kenlay%20and%20Fr%C3%A9d%C3%A9ric%20A.%20Dreyer%20and%20Aleksandr%20Kovaltsuk%20and%20Dom%20Miketa%20and%20Douglas%20Pires%20and%20Charlotte%20M.%20Deane&entry.1292438233=%20%20Antibodies%20are%20proteins%20produced%20by%20the%20immune%20system%20that%20can%20identify%20and%0Aneutralise%20a%20wide%20variety%20of%20antigens%20with%20high%20specificity%20and%20affinity%2C%20and%0Aconstitute%20the%20most%20successful%20class%20of%20biotherapeutics.%20With%20the%20advent%20of%0Anext-generation%20sequencing%2C%20billions%20of%20antibody%20sequences%20have%20been%20collected%0Ain%20recent%20years%2C%20though%20their%20application%20in%20the%20design%20of%20better%20therapeutics%0Ahas%20been%20constrained%20by%20the%20sheer%20volume%20and%20complexity%20of%20the%20data.%20To%20address%0Athis%20challenge%2C%20we%20present%20IgBert%20and%20IgT5%2C%20the%20best%20performing%0Aantibody-specific%20language%20models%20developed%20to%20date%20which%20can%20consistently%0Ahandle%20both%20paired%20and%20unpaired%20variable%20region%20sequences%20as%20input.%20These%0Amodels%20are%20trained%20comprehensively%20using%20the%20more%20than%20two%20billion%20unpaired%0Asequences%20and%20two%20million%20paired%20sequences%20of%20light%20and%20heavy%20chains%20present%20in%0Athe%20Observed%20Antibody%20Space%20dataset.%20We%20show%20that%20our%20models%20outperform%0Aexisting%20antibody%20and%20protein%20language%20models%20on%20a%20diverse%20range%20of%20design%20and%0Aregression%20tasks%20relevant%20to%20antibody%20engineering.%20This%20advancement%20marks%20a%0Asignificant%20leap%20forward%20in%20leveraging%20machine%20learning%2C%20large%20scale%20data%20sets%0Aand%20high-performance%20computing%20for%20enhancing%20antibody%20design%20for%20therapeutic%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17889v1&entry.124074799=Read"},
{"title": "Differentially private multivariate medians", "author": "Kelly Ramsay and Aukosh Jagannath and Shoja'eddin Chenouri", "abstract": "  Statistical tools which satisfy rigorous privacy guarantees are necessary for\nmodern data analysis. It is well-known that robustness against contamination is\nlinked to differential privacy. Despite this fact, using multivariate medians\nfor differentially private and robust multivariate location estimation has not\nbeen systematically studied. We develop novel finite-sample performance\nguarantees for differentially private multivariate depth-based medians, which\nare essentially sharp. Our results cover commonly used depth functions, such as\nthe halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.\nWe show that under Cauchy marginals, the cost of heavy-tailed location\nestimation outweighs the cost of privacy. We demonstrate our results\nnumerically using a Gaussian contamination model in dimensions up to d = 100,\nand compare them to a state-of-the-art private mean estimation algorithm. As a\nby-product of our investigation, we prove concentration inequalities for the\noutput of the exponential mechanism about the maximizer of the population\nobjective function. This bound applies to objective functions that satisfy a\nmild regularity condition.\n", "link": "http://arxiv.org/abs/2210.06459v2", "date": "2024-03-26", "relevancy": 1.323, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4848}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4286}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4284}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Differentially%20private%20multivariate%20medians&body=Title%3A%20Differentially%20private%20multivariate%20medians%0AAuthor%3A%20Kelly%20Ramsay%20and%20Aukosh%20Jagannath%20and%20Shoja%27eddin%20Chenouri%0AAbstract%3A%20%20%20Statistical%20tools%20which%20satisfy%20rigorous%20privacy%20guarantees%20are%20necessary%20for%0Amodern%20data%20analysis.%20It%20is%20well-known%20that%20robustness%20against%20contamination%20is%0Alinked%20to%20differential%20privacy.%20Despite%20this%20fact%2C%20using%20multivariate%20medians%0Afor%20differentially%20private%20and%20robust%20multivariate%20location%20estimation%20has%20not%0Abeen%20systematically%20studied.%20We%20develop%20novel%20finite-sample%20performance%0Aguarantees%20for%20differentially%20private%20multivariate%20depth-based%20medians%2C%20which%0Aare%20essentially%20sharp.%20Our%20results%20cover%20commonly%20used%20depth%20functions%2C%20such%20as%0Athe%20halfspace%20%28or%20Tukey%29%20depth%2C%20spatial%20depth%2C%20and%20the%20integrated%20dual%20depth.%0AWe%20show%20that%20under%20Cauchy%20marginals%2C%20the%20cost%20of%20heavy-tailed%20location%0Aestimation%20outweighs%20the%20cost%20of%20privacy.%20We%20demonstrate%20our%20results%0Anumerically%20using%20a%20Gaussian%20contamination%20model%20in%20dimensions%20up%20to%20d%20%3D%20100%2C%0Aand%20compare%20them%20to%20a%20state-of-the-art%20private%20mean%20estimation%20algorithm.%20As%20a%0Aby-product%20of%20our%20investigation%2C%20we%20prove%20concentration%20inequalities%20for%20the%0Aoutput%20of%20the%20exponential%20mechanism%20about%20the%20maximizer%20of%20the%20population%0Aobjective%20function.%20This%20bound%20applies%20to%20objective%20functions%20that%20satisfy%20a%0Amild%20regularity%20condition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.06459v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20private%20multivariate%20medians&entry.906535625=Kelly%20Ramsay%20and%20Aukosh%20Jagannath%20and%20Shoja%27eddin%20Chenouri&entry.1292438233=%20%20Statistical%20tools%20which%20satisfy%20rigorous%20privacy%20guarantees%20are%20necessary%20for%0Amodern%20data%20analysis.%20It%20is%20well-known%20that%20robustness%20against%20contamination%20is%0Alinked%20to%20differential%20privacy.%20Despite%20this%20fact%2C%20using%20multivariate%20medians%0Afor%20differentially%20private%20and%20robust%20multivariate%20location%20estimation%20has%20not%0Abeen%20systematically%20studied.%20We%20develop%20novel%20finite-sample%20performance%0Aguarantees%20for%20differentially%20private%20multivariate%20depth-based%20medians%2C%20which%0Aare%20essentially%20sharp.%20Our%20results%20cover%20commonly%20used%20depth%20functions%2C%20such%20as%0Athe%20halfspace%20%28or%20Tukey%29%20depth%2C%20spatial%20depth%2C%20and%20the%20integrated%20dual%20depth.%0AWe%20show%20that%20under%20Cauchy%20marginals%2C%20the%20cost%20of%20heavy-tailed%20location%0Aestimation%20outweighs%20the%20cost%20of%20privacy.%20We%20demonstrate%20our%20results%0Anumerically%20using%20a%20Gaussian%20contamination%20model%20in%20dimensions%20up%20to%20d%20%3D%20100%2C%0Aand%20compare%20them%20to%20a%20state-of-the-art%20private%20mean%20estimation%20algorithm.%20As%20a%0Aby-product%20of%20our%20investigation%2C%20we%20prove%20concentration%20inequalities%20for%20the%0Aoutput%20of%20the%20exponential%20mechanism%20about%20the%20maximizer%20of%20the%20population%0Aobjective%20function.%20This%20bound%20applies%20to%20objective%20functions%20that%20satisfy%20a%0Amild%20regularity%20condition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.06459v2&entry.124074799=Read"},
{"title": "Toward a Theory of Causation for Interpreting Neural Code Models", "author": "David N. Palacio and Alejandro Velasco and Nathan Cooper and Alvaro Rodriguez and Kevin Moran and Denys Poshyvanyk", "abstract": "  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n", "link": "http://arxiv.org/abs/2302.03788v4", "date": "2024-03-26", "relevancy": 1.3366, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4902}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4335}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4309}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toward%20a%20Theory%20of%20Causation%20for%20Interpreting%20Neural%20Code%20Models&body=Title%3A%20Toward%20a%20Theory%20of%20Causation%20for%20Interpreting%20Neural%20Code%20Models%0AAuthor%3A%20David%20N.%20Palacio%20and%20Alejandro%20Velasco%20and%20Nathan%20Cooper%20and%20Alvaro%20Rodriguez%20and%20Kevin%20Moran%20and%20Denys%20Poshyvanyk%0AAbstract%3A%20%20%20Neural%20Language%20Models%20of%20Code%2C%20or%20Neural%20Code%20Models%20%28NCMs%29%2C%20are%20rapidly%0Aprogressing%20from%20research%20prototypes%20to%20commercial%20developer%20tools.%20As%20such%2C%0Aunderstanding%20the%20capabilities%20and%20limitations%20of%20such%20models%20is%20becoming%0Acritical.%20However%2C%20the%20abilities%20of%20these%20models%20are%20typically%20measured%20using%0Aautomated%20metrics%20that%20often%20only%20reveal%20a%20portion%20of%20their%20real-world%0Aperformance.%20While%2C%20in%20general%2C%20the%20performance%20of%20NCMs%20appears%20promising%2C%0Acurrently%20much%20is%20unknown%20about%20how%20such%20models%20arrive%20at%20decisions.%20To%20this%0Aend%2C%20this%20paper%20introduces%20%24do_%7Bcode%7D%24%2C%20a%20post%20hoc%20interpretability%20method%0Aspecific%20to%20NCMs%20that%20is%20capable%20of%20explaining%20model%20predictions.%20%24do_%7Bcode%7D%24%0Ais%20based%20upon%20causal%20inference%20to%20enable%20programming%20language-oriented%0Aexplanations.%20While%20the%20theoretical%20underpinnings%20of%20%24do_%7Bcode%7D%24%20are%20extensible%0Ato%20exploring%20different%20model%20properties%2C%20we%20provide%20a%20concrete%20instantiation%0Athat%20aims%20to%20mitigate%20the%20impact%20of%20spurious%20correlations%20by%20grounding%0Aexplanations%20of%20model%20behavior%20in%20properties%20of%20programming%20languages.%20To%0Ademonstrate%20the%20practical%20benefit%20of%20%24do_%7Bcode%7D%24%2C%20we%20illustrate%20the%20insights%0Athat%20our%20framework%20can%20provide%20by%20performing%20a%20case%20study%20on%20two%20popular%20deep%0Alearning%20architectures%20and%20ten%20NCMs.%20The%20results%20of%20this%20case%20study%20illustrate%0Athat%20our%20studied%20NCMs%20are%20sensitive%20to%20changes%20in%20code%20syntax.%20All%20our%20NCMs%2C%0Aexcept%20for%20the%20BERT-like%20model%2C%20statistically%20learn%20to%20predict%20tokens%20related%0Ato%20blocks%20of%20code%20%28%5Ceg%20brackets%2C%20parenthesis%2C%20semicolon%29%20with%20less%20confounding%0Abias%20as%20compared%20to%20other%20programming%20language%20constructs.%20These%20insights%0Ademonstrate%20the%20potential%20of%20%24do_%7Bcode%7D%24%20as%20a%20useful%20method%20to%20detect%20and%0Afacilitate%20the%20elimination%20of%20confounding%20bias%20in%20NCMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03788v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20a%20Theory%20of%20Causation%20for%20Interpreting%20Neural%20Code%20Models&entry.906535625=David%20N.%20Palacio%20and%20Alejandro%20Velasco%20and%20Nathan%20Cooper%20and%20Alvaro%20Rodriguez%20and%20Kevin%20Moran%20and%20Denys%20Poshyvanyk&entry.1292438233=%20%20Neural%20Language%20Models%20of%20Code%2C%20or%20Neural%20Code%20Models%20%28NCMs%29%2C%20are%20rapidly%0Aprogressing%20from%20research%20prototypes%20to%20commercial%20developer%20tools.%20As%20such%2C%0Aunderstanding%20the%20capabilities%20and%20limitations%20of%20such%20models%20is%20becoming%0Acritical.%20However%2C%20the%20abilities%20of%20these%20models%20are%20typically%20measured%20using%0Aautomated%20metrics%20that%20often%20only%20reveal%20a%20portion%20of%20their%20real-world%0Aperformance.%20While%2C%20in%20general%2C%20the%20performance%20of%20NCMs%20appears%20promising%2C%0Acurrently%20much%20is%20unknown%20about%20how%20such%20models%20arrive%20at%20decisions.%20To%20this%0Aend%2C%20this%20paper%20introduces%20%24do_%7Bcode%7D%24%2C%20a%20post%20hoc%20interpretability%20method%0Aspecific%20to%20NCMs%20that%20is%20capable%20of%20explaining%20model%20predictions.%20%24do_%7Bcode%7D%24%0Ais%20based%20upon%20causal%20inference%20to%20enable%20programming%20language-oriented%0Aexplanations.%20While%20the%20theoretical%20underpinnings%20of%20%24do_%7Bcode%7D%24%20are%20extensible%0Ato%20exploring%20different%20model%20properties%2C%20we%20provide%20a%20concrete%20instantiation%0Athat%20aims%20to%20mitigate%20the%20impact%20of%20spurious%20correlations%20by%20grounding%0Aexplanations%20of%20model%20behavior%20in%20properties%20of%20programming%20languages.%20To%0Ademonstrate%20the%20practical%20benefit%20of%20%24do_%7Bcode%7D%24%2C%20we%20illustrate%20the%20insights%0Athat%20our%20framework%20can%20provide%20by%20performing%20a%20case%20study%20on%20two%20popular%20deep%0Alearning%20architectures%20and%20ten%20NCMs.%20The%20results%20of%20this%20case%20study%20illustrate%0Athat%20our%20studied%20NCMs%20are%20sensitive%20to%20changes%20in%20code%20syntax.%20All%20our%20NCMs%2C%0Aexcept%20for%20the%20BERT-like%20model%2C%20statistically%20learn%20to%20predict%20tokens%20related%0Ato%20blocks%20of%20code%20%28%5Ceg%20brackets%2C%20parenthesis%2C%20semicolon%29%20with%20less%20confounding%0Abias%20as%20compared%20to%20other%20programming%20language%20constructs.%20These%20insights%0Ademonstrate%20the%20potential%20of%20%24do_%7Bcode%7D%24%20as%20a%20useful%20method%20to%20detect%20and%0Afacilitate%20the%20elimination%20of%20confounding%20bias%20in%20NCMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03788v4&entry.124074799=Read"},
{"title": "Are Compressed Language Models Less Subgroup Robust?", "author": "Leonidas Gee and Andrea Zugarini and Novi Quadrianto", "abstract": "  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n", "link": "http://arxiv.org/abs/2403.17811v1", "date": "2024-03-26", "relevancy": 1.2045, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.409}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4065}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3965}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Are%20Compressed%20Language%20Models%20Less%20Subgroup%20Robust%3F&body=Title%3A%20Are%20Compressed%20Language%20Models%20Less%20Subgroup%20Robust%3F%0AAuthor%3A%20Leonidas%20Gee%20and%20Andrea%20Zugarini%20and%20Novi%20Quadrianto%0AAbstract%3A%20%20%20To%20reduce%20the%20inference%20cost%20of%20large%20language%20models%2C%20model%20compression%20is%0Aincreasingly%20used%20to%20create%20smaller%20scalable%20models.%20However%2C%20little%20is%20known%0Aabout%20their%20robustness%20to%20minority%20subgroups%20defined%20by%20the%20labels%20and%0Aattributes%20of%20a%20dataset.%20In%20this%20paper%2C%20we%20investigate%20the%20effects%20of%2018%0Adifferent%20compression%20methods%20and%20settings%20on%20the%20subgroup%20robustness%20of%20BERT%0Alanguage%20models.%20We%20show%20that%20worst-group%20performance%20does%20not%20depend%20on%20model%0Asize%20alone%2C%20but%20also%20on%20the%20compression%20method%20used.%20Additionally%2C%20we%20find%20that%0Amodel%20compression%20does%20not%20always%20worsen%20the%20performance%20on%20minority%20subgroups.%0AAltogether%2C%20our%20analysis%20serves%20to%20further%20research%20into%20the%20subgroup%0Arobustness%20of%20model%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17811v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Compressed%20Language%20Models%20Less%20Subgroup%20Robust%3F&entry.906535625=Leonidas%20Gee%20and%20Andrea%20Zugarini%20and%20Novi%20Quadrianto&entry.1292438233=%20%20To%20reduce%20the%20inference%20cost%20of%20large%20language%20models%2C%20model%20compression%20is%0Aincreasingly%20used%20to%20create%20smaller%20scalable%20models.%20However%2C%20little%20is%20known%0Aabout%20their%20robustness%20to%20minority%20subgroups%20defined%20by%20the%20labels%20and%0Aattributes%20of%20a%20dataset.%20In%20this%20paper%2C%20we%20investigate%20the%20effects%20of%2018%0Adifferent%20compression%20methods%20and%20settings%20on%20the%20subgroup%20robustness%20of%20BERT%0Alanguage%20models.%20We%20show%20that%20worst-group%20performance%20does%20not%20depend%20on%20model%0Asize%20alone%2C%20but%20also%20on%20the%20compression%20method%20used.%20Additionally%2C%20we%20find%20that%0Amodel%20compression%20does%20not%20always%20worsen%20the%20performance%20on%20minority%20subgroups.%0AAltogether%2C%20our%20analysis%20serves%20to%20further%20research%20into%20the%20subgroup%0Arobustness%20of%20model%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17811v1&entry.124074799=Read"},
{"title": "On the Computational Complexity of Stackelberg Planning and\n  Meta-Operator Verification: Technical Report", "author": "Gregor Behnke and Marcel Steinmetz", "abstract": "  Stackelberg planning is a recently introduced single-turn two-player\nadversarial planning model, where two players are acting in a joint classical\nplanning task, the objective of the first player being hampering the second\nplayer from achieving its goal. This places the Stackelberg planning problem\nsomewhere between classical planning and general combinatorial two-player\ngames. But, where exactly? All investigations of Stackelberg planning so far\nfocused on practical aspects. We close this gap by conducting the first\ntheoretical complexity analysis of Stackelberg planning. We show that in\ngeneral Stackelberg planning is actually no harder than classical planning.\nUnder a polynomial plan-length restriction, however, Stackelberg planning is a\nlevel higher up in the polynomial complexity hierarchy, suggesting that\ncompilations into classical planning come with a worst-case exponential\nplan-length increase. In attempts to identify tractable fragments, we further\nstudy its complexity under various planning task restrictions, showing that\nStackelberg planning remains intractable where classical planning is not. We\nfinally inspect the complexity of meta-operator verification, a problem that\nhas been recently connected to Stackelberg planning.\n", "link": "http://arxiv.org/abs/2403.17826v1", "date": "2024-03-26", "relevancy": 1.1692, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4154}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3939}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3778}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Computational%20Complexity%20of%20Stackelberg%20Planning%20and%0A%20%20Meta-Operator%20Verification%3A%20Technical%20Report&body=Title%3A%20On%20the%20Computational%20Complexity%20of%20Stackelberg%20Planning%20and%0A%20%20Meta-Operator%20Verification%3A%20Technical%20Report%0AAuthor%3A%20Gregor%20Behnke%20and%20Marcel%20Steinmetz%0AAbstract%3A%20%20%20Stackelberg%20planning%20is%20a%20recently%20introduced%20single-turn%20two-player%0Aadversarial%20planning%20model%2C%20where%20two%20players%20are%20acting%20in%20a%20joint%20classical%0Aplanning%20task%2C%20the%20objective%20of%20the%20first%20player%20being%20hampering%20the%20second%0Aplayer%20from%20achieving%20its%20goal.%20This%20places%20the%20Stackelberg%20planning%20problem%0Asomewhere%20between%20classical%20planning%20and%20general%20combinatorial%20two-player%0Agames.%20But%2C%20where%20exactly%3F%20All%20investigations%20of%20Stackelberg%20planning%20so%20far%0Afocused%20on%20practical%20aspects.%20We%20close%20this%20gap%20by%20conducting%20the%20first%0Atheoretical%20complexity%20analysis%20of%20Stackelberg%20planning.%20We%20show%20that%20in%0Ageneral%20Stackelberg%20planning%20is%20actually%20no%20harder%20than%20classical%20planning.%0AUnder%20a%20polynomial%20plan-length%20restriction%2C%20however%2C%20Stackelberg%20planning%20is%20a%0Alevel%20higher%20up%20in%20the%20polynomial%20complexity%20hierarchy%2C%20suggesting%20that%0Acompilations%20into%20classical%20planning%20come%20with%20a%20worst-case%20exponential%0Aplan-length%20increase.%20In%20attempts%20to%20identify%20tractable%20fragments%2C%20we%20further%0Astudy%20its%20complexity%20under%20various%20planning%20task%20restrictions%2C%20showing%20that%0AStackelberg%20planning%20remains%20intractable%20where%20classical%20planning%20is%20not.%20We%0Afinally%20inspect%20the%20complexity%20of%20meta-operator%20verification%2C%20a%20problem%20that%0Ahas%20been%20recently%20connected%20to%20Stackelberg%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17826v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Computational%20Complexity%20of%20Stackelberg%20Planning%20and%0A%20%20Meta-Operator%20Verification%3A%20Technical%20Report&entry.906535625=Gregor%20Behnke%20and%20Marcel%20Steinmetz&entry.1292438233=%20%20Stackelberg%20planning%20is%20a%20recently%20introduced%20single-turn%20two-player%0Aadversarial%20planning%20model%2C%20where%20two%20players%20are%20acting%20in%20a%20joint%20classical%0Aplanning%20task%2C%20the%20objective%20of%20the%20first%20player%20being%20hampering%20the%20second%0Aplayer%20from%20achieving%20its%20goal.%20This%20places%20the%20Stackelberg%20planning%20problem%0Asomewhere%20between%20classical%20planning%20and%20general%20combinatorial%20two-player%0Agames.%20But%2C%20where%20exactly%3F%20All%20investigations%20of%20Stackelberg%20planning%20so%20far%0Afocused%20on%20practical%20aspects.%20We%20close%20this%20gap%20by%20conducting%20the%20first%0Atheoretical%20complexity%20analysis%20of%20Stackelberg%20planning.%20We%20show%20that%20in%0Ageneral%20Stackelberg%20planning%20is%20actually%20no%20harder%20than%20classical%20planning.%0AUnder%20a%20polynomial%20plan-length%20restriction%2C%20however%2C%20Stackelberg%20planning%20is%20a%0Alevel%20higher%20up%20in%20the%20polynomial%20complexity%20hierarchy%2C%20suggesting%20that%0Acompilations%20into%20classical%20planning%20come%20with%20a%20worst-case%20exponential%0Aplan-length%20increase.%20In%20attempts%20to%20identify%20tractable%20fragments%2C%20we%20further%0Astudy%20its%20complexity%20under%20various%20planning%20task%20restrictions%2C%20showing%20that%0AStackelberg%20planning%20remains%20intractable%20where%20classical%20planning%20is%20not.%20We%0Afinally%20inspect%20the%20complexity%20of%20meta-operator%20verification%2C%20a%20problem%20that%0Ahas%20been%20recently%20connected%20to%20Stackelberg%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17826v1&entry.124074799=Read"},
{"title": "Accelerating Radio Spectrum Regulation Workflows with Large Language\n  Models (LLMs)", "author": "Amir Ghasemi and Paul Guinand", "abstract": "  Wireless spectrum regulation is a complex and demanding process due to the\nrapid pace of technological progress, increasing demand for spectrum, and a\nmultitude of stakeholders with potentially conflicting interests, alongside\nsignificant economic implications. To navigate this, regulators must engage\neffectively with all parties, keep pace with global technology trends, conduct\ntechnical evaluations, issue licenses in a timely manner, and comply with\nvarious legal and policy frameworks.\n  In light of these challenges, this paper demonstrates example applications of\nLarge Language Models (LLMs) to expedite spectrum regulatory processes. We\nexplore various roles that LLMs can play in this context while identifying some\nof the challenges to address. The paper also offers practical case studies and\ninsights, with appropriate experiments, highlighting the transformative\npotential of LLMs in spectrum management.\n", "link": "http://arxiv.org/abs/2403.17819v1", "date": "2024-03-26", "relevancy": 0.7507, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.376}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3751}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3749}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Radio%20Spectrum%20Regulation%20Workflows%20with%20Large%20Language%0A%20%20Models%20%28LLMs%29&body=Title%3A%20Accelerating%20Radio%20Spectrum%20Regulation%20Workflows%20with%20Large%20Language%0A%20%20Models%20%28LLMs%29%0AAuthor%3A%20Amir%20Ghasemi%20and%20Paul%20Guinand%0AAbstract%3A%20%20%20Wireless%20spectrum%20regulation%20is%20a%20complex%20and%20demanding%20process%20due%20to%20the%0Arapid%20pace%20of%20technological%20progress%2C%20increasing%20demand%20for%20spectrum%2C%20and%20a%0Amultitude%20of%20stakeholders%20with%20potentially%20conflicting%20interests%2C%20alongside%0Asignificant%20economic%20implications.%20To%20navigate%20this%2C%20regulators%20must%20engage%0Aeffectively%20with%20all%20parties%2C%20keep%20pace%20with%20global%20technology%20trends%2C%20conduct%0Atechnical%20evaluations%2C%20issue%20licenses%20in%20a%20timely%20manner%2C%20and%20comply%20with%0Avarious%20legal%20and%20policy%20frameworks.%0A%20%20In%20light%20of%20these%20challenges%2C%20this%20paper%20demonstrates%20example%20applications%20of%0ALarge%20Language%20Models%20%28LLMs%29%20to%20expedite%20spectrum%20regulatory%20processes.%20We%0Aexplore%20various%20roles%20that%20LLMs%20can%20play%20in%20this%20context%20while%20identifying%20some%0Aof%20the%20challenges%20to%20address.%20The%20paper%20also%20offers%20practical%20case%20studies%20and%0Ainsights%2C%20with%20appropriate%20experiments%2C%20highlighting%20the%20transformative%0Apotential%20of%20LLMs%20in%20spectrum%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17819v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Radio%20Spectrum%20Regulation%20Workflows%20with%20Large%20Language%0A%20%20Models%20%28LLMs%29&entry.906535625=Amir%20Ghasemi%20and%20Paul%20Guinand&entry.1292438233=%20%20Wireless%20spectrum%20regulation%20is%20a%20complex%20and%20demanding%20process%20due%20to%20the%0Arapid%20pace%20of%20technological%20progress%2C%20increasing%20demand%20for%20spectrum%2C%20and%20a%0Amultitude%20of%20stakeholders%20with%20potentially%20conflicting%20interests%2C%20alongside%0Asignificant%20economic%20implications.%20To%20navigate%20this%2C%20regulators%20must%20engage%0Aeffectively%20with%20all%20parties%2C%20keep%20pace%20with%20global%20technology%20trends%2C%20conduct%0Atechnical%20evaluations%2C%20issue%20licenses%20in%20a%20timely%20manner%2C%20and%20comply%20with%0Avarious%20legal%20and%20policy%20frameworks.%0A%20%20In%20light%20of%20these%20challenges%2C%20this%20paper%20demonstrates%20example%20applications%20of%0ALarge%20Language%20Models%20%28LLMs%29%20to%20expedite%20spectrum%20regulatory%20processes.%20We%0Aexplore%20various%20roles%20that%20LLMs%20can%20play%20in%20this%20context%20while%20identifying%20some%0Aof%20the%20challenges%20to%20address.%20The%20paper%20also%20offers%20practical%20case%20studies%20and%0Ainsights%2C%20with%20appropriate%20experiments%2C%20highlighting%20the%20transformative%0Apotential%20of%20LLMs%20in%20spectrum%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17819v1&entry.124074799=Read"},
{"title": "Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic", "author": "Connor Pryor and Quan Yuan and Jeremiah Liu and Mehran Kazemi and Deepak Ramachandran and Tania Bedrax-Weiss and Lise Getoor", "abstract": "  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n", "link": "http://arxiv.org/abs/2403.17853v1", "date": "2024-03-26", "relevancy": 0.965, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4849}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4821}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4804}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20Domain%20Knowledge%20to%20Guide%20Dialog%20Structure%20Induction%20via%20Neural%0A%20%20Probabilistic%20Soft%20Logic&body=Title%3A%20Using%20Domain%20Knowledge%20to%20Guide%20Dialog%20Structure%20Induction%20via%20Neural%0A%20%20Probabilistic%20Soft%20Logic%0AAuthor%3A%20Connor%20Pryor%20and%20Quan%20Yuan%20and%20Jeremiah%20Liu%20and%20Mehran%20Kazemi%20and%20Deepak%20Ramachandran%20and%20Tania%20Bedrax-Weiss%20and%20Lise%20Getoor%0AAbstract%3A%20%20%20Dialog%20Structure%20Induction%20%28DSI%29%20is%20the%20task%20of%20inferring%20the%20latent%20dialog%0Astructure%20%28i.e.%2C%20a%20set%20of%20dialog%20states%20and%20their%20temporal%20transitions%29%20of%20a%0Agiven%20goal-oriented%20dialog.%20It%20is%20a%20critical%20component%20for%20modern%20dialog%20system%0Adesign%20and%20discourse%20analysis.%20Existing%20DSI%20approaches%20are%20often%20purely%0Adata-driven%2C%20deploy%20models%20that%20infer%20latent%20states%20without%20access%20to%20domain%0Aknowledge%2C%20underperform%20when%20the%20training%20corpus%20is%20limited/noisy%2C%20or%20have%0Adifficulty%20when%20test%20dialogs%20exhibit%20distributional%20shifts%20from%20the%20training%0Adomain.%20This%20work%20explores%20a%20neural-symbolic%20approach%20as%20a%20potential%20solution%0Ato%20these%20problems.%20We%20introduce%20Neural%20Probabilistic%20Soft%20Logic%20Dialogue%0AStructure%20Induction%20%28NEUPSL%20DSI%29%2C%20a%20principled%20approach%20that%20injects%20symbolic%0Aknowledge%20into%20the%20latent%20space%20of%20a%20generative%20neural%20model.%20We%20conduct%20a%0Athorough%20empirical%20investigation%20on%20the%20effect%20of%20NEUPSL%20DSI%20learning%20on%20hidden%0Arepresentation%20quality%2C%20few-shot%20learning%2C%20and%20out-of-domain%20generalization%0Aperformance.%20Over%20three%20dialog%20structure%20induction%20datasets%20and%20across%0Aunsupervised%20and%20semi-supervised%20settings%20for%20standard%20and%20cross-domain%0Ageneralization%2C%20the%20injection%20of%20symbolic%20knowledge%20using%20NEUPSL%20DSI%20provides%20a%0Aconsistent%20boost%20in%20performance%20over%20the%20canonical%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17853v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Domain%20Knowledge%20to%20Guide%20Dialog%20Structure%20Induction%20via%20Neural%0A%20%20Probabilistic%20Soft%20Logic&entry.906535625=Connor%20Pryor%20and%20Quan%20Yuan%20and%20Jeremiah%20Liu%20and%20Mehran%20Kazemi%20and%20Deepak%20Ramachandran%20and%20Tania%20Bedrax-Weiss%20and%20Lise%20Getoor&entry.1292438233=%20%20Dialog%20Structure%20Induction%20%28DSI%29%20is%20the%20task%20of%20inferring%20the%20latent%20dialog%0Astructure%20%28i.e.%2C%20a%20set%20of%20dialog%20states%20and%20their%20temporal%20transitions%29%20of%20a%0Agiven%20goal-oriented%20dialog.%20It%20is%20a%20critical%20component%20for%20modern%20dialog%20system%0Adesign%20and%20discourse%20analysis.%20Existing%20DSI%20approaches%20are%20often%20purely%0Adata-driven%2C%20deploy%20models%20that%20infer%20latent%20states%20without%20access%20to%20domain%0Aknowledge%2C%20underperform%20when%20the%20training%20corpus%20is%20limited/noisy%2C%20or%20have%0Adifficulty%20when%20test%20dialogs%20exhibit%20distributional%20shifts%20from%20the%20training%0Adomain.%20This%20work%20explores%20a%20neural-symbolic%20approach%20as%20a%20potential%20solution%0Ato%20these%20problems.%20We%20introduce%20Neural%20Probabilistic%20Soft%20Logic%20Dialogue%0AStructure%20Induction%20%28NEUPSL%20DSI%29%2C%20a%20principled%20approach%20that%20injects%20symbolic%0Aknowledge%20into%20the%20latent%20space%20of%20a%20generative%20neural%20model.%20We%20conduct%20a%0Athorough%20empirical%20investigation%20on%20the%20effect%20of%20NEUPSL%20DSI%20learning%20on%20hidden%0Arepresentation%20quality%2C%20few-shot%20learning%2C%20and%20out-of-domain%20generalization%0Aperformance.%20Over%20three%20dialog%20structure%20induction%20datasets%20and%20across%0Aunsupervised%20and%20semi-supervised%20settings%20for%20standard%20and%20cross-domain%0Ageneralization%2C%20the%20injection%20of%20symbolic%20knowledge%20using%20NEUPSL%20DSI%20provides%20a%0Aconsistent%20boost%20in%20performance%20over%20the%20canonical%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17853v1&entry.124074799=Read"},
{"title": "PINN surrogate of Li-ion battery models for parameter inference. Part\n  II: Regularization and application of the pseudo-2D model", "author": "Malik Hassanaly and Peter J. Weddle and Ryan N. King and Subhayan De and Alireza Doostan and Corey R. Randall and Eric J. Dufek and Andrew M. Colclasure and Kandler Smith", "abstract": "  Bayesian parameter inference is useful to improve Li-ion battery diagnostics\nand can help formulate battery aging models. However, it is computationally\nintensive and cannot be easily repeated for multiple cycles, multiple operating\nconditions, or multiple replicate cells. To reduce the computational cost of\nBayesian calibration, numerical solvers for physics-based models can be\nreplaced with faster surrogates. A physics-informed neural network (PINN) is\ndeveloped as a surrogate for the pseudo-2D (P2D) battery model calibration. For\nthe P2D surrogate, additional training regularization was needed as compared to\nthe PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and\nP2D surrogate models are exercised for parameter inference and compared to data\nobtained from a direct numerical solution of the governing equations. A\nparameter inference study highlights the ability to use these PINNs to\ncalibrate scaling parameters for the cathode Li diffusion and the anode\nexchange current density. By realizing computational speed-ups of 2250x for the\nP2D model, as compared to using standard integrating methods, the PINN\nsurrogates enable rapid state-of-health diagnostics. In the low-data\navailability scenario, the testing error was estimated to 2mV for the SPM\nsurrogate and 10mV for the P2D surrogate which could be mitigated with\nadditional data.\n", "link": "http://arxiv.org/abs/2312.17336v2", "date": "2024-03-26", "relevancy": 1.3298, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4833}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4498}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4247}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PINN%20surrogate%20of%20Li-ion%20battery%20models%20for%20parameter%20inference.%20Part%0A%20%20II%3A%20Regularization%20and%20application%20of%20the%20pseudo-2D%20model&body=Title%3A%20PINN%20surrogate%20of%20Li-ion%20battery%20models%20for%20parameter%20inference.%20Part%0A%20%20II%3A%20Regularization%20and%20application%20of%20the%20pseudo-2D%20model%0AAuthor%3A%20Malik%20Hassanaly%20and%20Peter%20J.%20Weddle%20and%20Ryan%20N.%20King%20and%20Subhayan%20De%20and%20Alireza%20Doostan%20and%20Corey%20R.%20Randall%20and%20Eric%20J.%20Dufek%20and%20Andrew%20M.%20Colclasure%20and%20Kandler%20Smith%0AAbstract%3A%20%20%20Bayesian%20parameter%20inference%20is%20useful%20to%20improve%20Li-ion%20battery%20diagnostics%0Aand%20can%20help%20formulate%20battery%20aging%20models.%20However%2C%20it%20is%20computationally%0Aintensive%20and%20cannot%20be%20easily%20repeated%20for%20multiple%20cycles%2C%20multiple%20operating%0Aconditions%2C%20or%20multiple%20replicate%20cells.%20To%20reduce%20the%20computational%20cost%20of%0ABayesian%20calibration%2C%20numerical%20solvers%20for%20physics-based%20models%20can%20be%0Areplaced%20with%20faster%20surrogates.%20A%20physics-informed%20neural%20network%20%28PINN%29%20is%0Adeveloped%20as%20a%20surrogate%20for%20the%20pseudo-2D%20%28P2D%29%20battery%20model%20calibration.%20For%0Athe%20P2D%20surrogate%2C%20additional%20training%20regularization%20was%20needed%20as%20compared%20to%0Athe%20PINN%20single-particle%20model%20%28SPM%29%20developed%20in%20Part%20I.%20Both%20the%20PINN%20SPM%20and%0AP2D%20surrogate%20models%20are%20exercised%20for%20parameter%20inference%20and%20compared%20to%20data%0Aobtained%20from%20a%20direct%20numerical%20solution%20of%20the%20governing%20equations.%20A%0Aparameter%20inference%20study%20highlights%20the%20ability%20to%20use%20these%20PINNs%20to%0Acalibrate%20scaling%20parameters%20for%20the%20cathode%20Li%20diffusion%20and%20the%20anode%0Aexchange%20current%20density.%20By%20realizing%20computational%20speed-ups%20of%202250x%20for%20the%0AP2D%20model%2C%20as%20compared%20to%20using%20standard%20integrating%20methods%2C%20the%20PINN%0Asurrogates%20enable%20rapid%20state-of-health%20diagnostics.%20In%20the%20low-data%0Aavailability%20scenario%2C%20the%20testing%20error%20was%20estimated%20to%202mV%20for%20the%20SPM%0Asurrogate%20and%2010mV%20for%20the%20P2D%20surrogate%20which%20could%20be%20mitigated%20with%0Aadditional%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17336v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PINN%20surrogate%20of%20Li-ion%20battery%20models%20for%20parameter%20inference.%20Part%0A%20%20II%3A%20Regularization%20and%20application%20of%20the%20pseudo-2D%20model&entry.906535625=Malik%20Hassanaly%20and%20Peter%20J.%20Weddle%20and%20Ryan%20N.%20King%20and%20Subhayan%20De%20and%20Alireza%20Doostan%20and%20Corey%20R.%20Randall%20and%20Eric%20J.%20Dufek%20and%20Andrew%20M.%20Colclasure%20and%20Kandler%20Smith&entry.1292438233=%20%20Bayesian%20parameter%20inference%20is%20useful%20to%20improve%20Li-ion%20battery%20diagnostics%0Aand%20can%20help%20formulate%20battery%20aging%20models.%20However%2C%20it%20is%20computationally%0Aintensive%20and%20cannot%20be%20easily%20repeated%20for%20multiple%20cycles%2C%20multiple%20operating%0Aconditions%2C%20or%20multiple%20replicate%20cells.%20To%20reduce%20the%20computational%20cost%20of%0ABayesian%20calibration%2C%20numerical%20solvers%20for%20physics-based%20models%20can%20be%0Areplaced%20with%20faster%20surrogates.%20A%20physics-informed%20neural%20network%20%28PINN%29%20is%0Adeveloped%20as%20a%20surrogate%20for%20the%20pseudo-2D%20%28P2D%29%20battery%20model%20calibration.%20For%0Athe%20P2D%20surrogate%2C%20additional%20training%20regularization%20was%20needed%20as%20compared%20to%0Athe%20PINN%20single-particle%20model%20%28SPM%29%20developed%20in%20Part%20I.%20Both%20the%20PINN%20SPM%20and%0AP2D%20surrogate%20models%20are%20exercised%20for%20parameter%20inference%20and%20compared%20to%20data%0Aobtained%20from%20a%20direct%20numerical%20solution%20of%20the%20governing%20equations.%20A%0Aparameter%20inference%20study%20highlights%20the%20ability%20to%20use%20these%20PINNs%20to%0Acalibrate%20scaling%20parameters%20for%20the%20cathode%20Li%20diffusion%20and%20the%20anode%0Aexchange%20current%20density.%20By%20realizing%20computational%20speed-ups%20of%202250x%20for%20the%0AP2D%20model%2C%20as%20compared%20to%20using%20standard%20integrating%20methods%2C%20the%20PINN%0Asurrogates%20enable%20rapid%20state-of-health%20diagnostics.%20In%20the%20low-data%0Aavailability%20scenario%2C%20the%20testing%20error%20was%20estimated%20to%202mV%20for%20the%20SPM%0Asurrogate%20and%2010mV%20for%20the%20P2D%20surrogate%20which%20could%20be%20mitigated%20with%0Aadditional%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17336v2&entry.124074799=Read"},
{"title": "Sample complexity of quantum hypothesis testing", "author": "Hao-Chung Cheng and Nilanjana Datta and Nana Liu and Theshani Nuradha and Robert Salzmann and Mark M. Wilde", "abstract": "  Quantum hypothesis testing has been traditionally studied from the\ninformation-theoretic perspective, wherein one is interested in the optimal\ndecay rate of error probabilities as a function of the number of samples of an\nunknown state. In this paper, we study the sample complexity of quantum\nhypothesis testing, wherein the goal is to determine the minimum number of\nsamples needed to reach a desired error probability. By making use of the\nwealth of knowledge that already exists in the literature on quantum hypothesis\ntesting, we characterize the sample complexity of binary quantum hypothesis\ntesting in the symmetric and asymmetric settings, and we provide bounds on the\nsample complexity of multiple quantum hypothesis testing. In more detail, we\nprove that the sample complexity of symmetric binary quantum hypothesis testing\ndepends logarithmically on the inverse error probability and inversely on the\nnegative logarithm of the fidelity. As a counterpart of the quantum Stein's\nlemma, we also find that the sample complexity of asymmetric binary quantum\nhypothesis testing depends logarithmically on the inverse type~II error\nprobability and inversely on the quantum relative entropy. Finally, we provide\nlower and upper bounds on the sample complexity of multiple quantum hypothesis\ntesting, with it remaining an intriguing open question to improve these bounds.\n", "link": "http://arxiv.org/abs/2403.17868v1", "date": "2024-03-26", "relevancy": 1.3101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3292}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3287}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3257}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sample%20complexity%20of%20quantum%20hypothesis%20testing&body=Title%3A%20Sample%20complexity%20of%20quantum%20hypothesis%20testing%0AAuthor%3A%20Hao-Chung%20Cheng%20and%20Nilanjana%20Datta%20and%20Nana%20Liu%20and%20Theshani%20Nuradha%20and%20Robert%20Salzmann%20and%20Mark%20M.%20Wilde%0AAbstract%3A%20%20%20Quantum%20hypothesis%20testing%20has%20been%20traditionally%20studied%20from%20the%0Ainformation-theoretic%20perspective%2C%20wherein%20one%20is%20interested%20in%20the%20optimal%0Adecay%20rate%20of%20error%20probabilities%20as%20a%20function%20of%20the%20number%20of%20samples%20of%20an%0Aunknown%20state.%20In%20this%20paper%2C%20we%20study%20the%20sample%20complexity%20of%20quantum%0Ahypothesis%20testing%2C%20wherein%20the%20goal%20is%20to%20determine%20the%20minimum%20number%20of%0Asamples%20needed%20to%20reach%20a%20desired%20error%20probability.%20By%20making%20use%20of%20the%0Awealth%20of%20knowledge%20that%20already%20exists%20in%20the%20literature%20on%20quantum%20hypothesis%0Atesting%2C%20we%20characterize%20the%20sample%20complexity%20of%20binary%20quantum%20hypothesis%0Atesting%20in%20the%20symmetric%20and%20asymmetric%20settings%2C%20and%20we%20provide%20bounds%20on%20the%0Asample%20complexity%20of%20multiple%20quantum%20hypothesis%20testing.%20In%20more%20detail%2C%20we%0Aprove%20that%20the%20sample%20complexity%20of%20symmetric%20binary%20quantum%20hypothesis%20testing%0Adepends%20logarithmically%20on%20the%20inverse%20error%20probability%20and%20inversely%20on%20the%0Anegative%20logarithm%20of%20the%20fidelity.%20As%20a%20counterpart%20of%20the%20quantum%20Stein%27s%0Alemma%2C%20we%20also%20find%20that%20the%20sample%20complexity%20of%20asymmetric%20binary%20quantum%0Ahypothesis%20testing%20depends%20logarithmically%20on%20the%20inverse%20type~II%20error%0Aprobability%20and%20inversely%20on%20the%20quantum%20relative%20entropy.%20Finally%2C%20we%20provide%0Alower%20and%20upper%20bounds%20on%20the%20sample%20complexity%20of%20multiple%20quantum%20hypothesis%0Atesting%2C%20with%20it%20remaining%20an%20intriguing%20open%20question%20to%20improve%20these%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17868v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%20complexity%20of%20quantum%20hypothesis%20testing&entry.906535625=Hao-Chung%20Cheng%20and%20Nilanjana%20Datta%20and%20Nana%20Liu%20and%20Theshani%20Nuradha%20and%20Robert%20Salzmann%20and%20Mark%20M.%20Wilde&entry.1292438233=%20%20Quantum%20hypothesis%20testing%20has%20been%20traditionally%20studied%20from%20the%0Ainformation-theoretic%20perspective%2C%20wherein%20one%20is%20interested%20in%20the%20optimal%0Adecay%20rate%20of%20error%20probabilities%20as%20a%20function%20of%20the%20number%20of%20samples%20of%20an%0Aunknown%20state.%20In%20this%20paper%2C%20we%20study%20the%20sample%20complexity%20of%20quantum%0Ahypothesis%20testing%2C%20wherein%20the%20goal%20is%20to%20determine%20the%20minimum%20number%20of%0Asamples%20needed%20to%20reach%20a%20desired%20error%20probability.%20By%20making%20use%20of%20the%0Awealth%20of%20knowledge%20that%20already%20exists%20in%20the%20literature%20on%20quantum%20hypothesis%0Atesting%2C%20we%20characterize%20the%20sample%20complexity%20of%20binary%20quantum%20hypothesis%0Atesting%20in%20the%20symmetric%20and%20asymmetric%20settings%2C%20and%20we%20provide%20bounds%20on%20the%0Asample%20complexity%20of%20multiple%20quantum%20hypothesis%20testing.%20In%20more%20detail%2C%20we%0Aprove%20that%20the%20sample%20complexity%20of%20symmetric%20binary%20quantum%20hypothesis%20testing%0Adepends%20logarithmically%20on%20the%20inverse%20error%20probability%20and%20inversely%20on%20the%0Anegative%20logarithm%20of%20the%20fidelity.%20As%20a%20counterpart%20of%20the%20quantum%20Stein%27s%0Alemma%2C%20we%20also%20find%20that%20the%20sample%20complexity%20of%20asymmetric%20binary%20quantum%0Ahypothesis%20testing%20depends%20logarithmically%20on%20the%20inverse%20type~II%20error%0Aprobability%20and%20inversely%20on%20the%20quantum%20relative%20entropy.%20Finally%2C%20we%20provide%0Alower%20and%20upper%20bounds%20on%20the%20sample%20complexity%20of%20multiple%20quantum%20hypothesis%0Atesting%2C%20with%20it%20remaining%20an%20intriguing%20open%20question%20to%20improve%20these%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17868v1&entry.124074799=Read"},
{"title": "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review", "author": "Sunder Ali Khowaja and Parus Khuwaja and Kapal Dev and Weizheng Wang and Lewis Nkenyereye", "abstract": "  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for AI policy act, if designed by the governments.\n", "link": "http://arxiv.org/abs/2305.03123v2", "date": "2024-03-26", "relevancy": 1.2931, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.443}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4328}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4147}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ChatGPT%20Needs%20SPADE%20%28Sustainability%2C%20PrivAcy%2C%20Digital%20divide%2C%20and%0A%20%20Ethics%29%20Evaluation%3A%20A%20Review&body=Title%3A%20ChatGPT%20Needs%20SPADE%20%28Sustainability%2C%20PrivAcy%2C%20Digital%20divide%2C%20and%0A%20%20Ethics%29%20Evaluation%3A%20A%20Review%0AAuthor%3A%20Sunder%20Ali%20Khowaja%20and%20Parus%20Khuwaja%20and%20Kapal%20Dev%20and%20Weizheng%20Wang%20and%20Lewis%20Nkenyereye%0AAbstract%3A%20%20%20ChatGPT%20is%20another%20large%20language%20model%20%28LLM%29%20vastly%20available%20for%20the%0Aconsumers%20on%20their%20devices%20but%20due%20to%20its%20performance%20and%20ability%20to%20converse%0Aeffectively%2C%20it%20has%20gained%20a%20huge%20popularity%20amongst%20research%20as%20well%20as%0Aindustrial%20community.%20Recently%2C%20many%20studies%20have%20been%20published%20to%20show%20the%0Aeffectiveness%2C%20efficiency%2C%20integration%2C%20and%20sentiments%20of%20chatGPT%20and%20other%0ALLMs.%20In%20contrast%2C%20this%20study%20focuses%20on%20the%20important%20aspects%20that%20are%20mostly%0Aoverlooked%2C%20i.e.%20sustainability%2C%20privacy%2C%20digital%20divide%2C%20and%20ethics%20and%0Asuggests%20that%20not%20only%20chatGPT%20but%20every%20subsequent%20entry%20in%20the%20category%20of%0Aconversational%20bots%20should%20undergo%20Sustainability%2C%20PrivAcy%2C%20Digital%20divide%2C%20and%0AEthics%20%28SPADE%29%20evaluation.%20This%20paper%20discusses%20in%20detail%20the%20issues%20and%0Aconcerns%20raised%20over%20chatGPT%20in%20line%20with%20aforementioned%20characteristics.%20We%0Aalso%20discuss%20the%20recent%20EU%20AI%20Act%20briefly%20in%20accordance%20with%20the%20SPADE%0Aevaluation.%20We%20support%20our%20hypothesis%20by%20some%20preliminary%20data%20collection%20and%0Avisualizations%20along%20with%20hypothesized%20facts.%20We%20also%20suggest%20mitigations%20and%0Arecommendations%20for%20each%20of%20the%20concerns.%20Furthermore%2C%20we%20also%20suggest%20some%0Apolicies%20and%20recommendations%20for%20AI%20policy%20act%2C%20if%20designed%20by%20the%20governments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.03123v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatGPT%20Needs%20SPADE%20%28Sustainability%2C%20PrivAcy%2C%20Digital%20divide%2C%20and%0A%20%20Ethics%29%20Evaluation%3A%20A%20Review&entry.906535625=Sunder%20Ali%20Khowaja%20and%20Parus%20Khuwaja%20and%20Kapal%20Dev%20and%20Weizheng%20Wang%20and%20Lewis%20Nkenyereye&entry.1292438233=%20%20ChatGPT%20is%20another%20large%20language%20model%20%28LLM%29%20vastly%20available%20for%20the%0Aconsumers%20on%20their%20devices%20but%20due%20to%20its%20performance%20and%20ability%20to%20converse%0Aeffectively%2C%20it%20has%20gained%20a%20huge%20popularity%20amongst%20research%20as%20well%20as%0Aindustrial%20community.%20Recently%2C%20many%20studies%20have%20been%20published%20to%20show%20the%0Aeffectiveness%2C%20efficiency%2C%20integration%2C%20and%20sentiments%20of%20chatGPT%20and%20other%0ALLMs.%20In%20contrast%2C%20this%20study%20focuses%20on%20the%20important%20aspects%20that%20are%20mostly%0Aoverlooked%2C%20i.e.%20sustainability%2C%20privacy%2C%20digital%20divide%2C%20and%20ethics%20and%0Asuggests%20that%20not%20only%20chatGPT%20but%20every%20subsequent%20entry%20in%20the%20category%20of%0Aconversational%20bots%20should%20undergo%20Sustainability%2C%20PrivAcy%2C%20Digital%20divide%2C%20and%0AEthics%20%28SPADE%29%20evaluation.%20This%20paper%20discusses%20in%20detail%20the%20issues%20and%0Aconcerns%20raised%20over%20chatGPT%20in%20line%20with%20aforementioned%20characteristics.%20We%0Aalso%20discuss%20the%20recent%20EU%20AI%20Act%20briefly%20in%20accordance%20with%20the%20SPADE%0Aevaluation.%20We%20support%20our%20hypothesis%20by%20some%20preliminary%20data%20collection%20and%0Avisualizations%20along%20with%20hypothesized%20facts.%20We%20also%20suggest%20mitigations%20and%0Arecommendations%20for%20each%20of%20the%20concerns.%20Furthermore%2C%20we%20also%20suggest%20some%0Apolicies%20and%20recommendations%20for%20AI%20policy%20act%2C%20if%20designed%20by%20the%20governments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.03123v2&entry.124074799=Read"},
{"title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution", "author": "Wei Tao and Yucheng Zhou and Wenqiang Zhang and Yu Cheng", "abstract": "  In software evolution, resolving the emergent issues within GitHub\nrepositories is a complex challenge that involves not only the incorporation of\nnew code but also the maintenance of existing functionalities. Large Language\nModels (LLMs) have shown promise in code generation and understanding but face\ndifficulties in code change, particularly at the repository level. To overcome\nthese challenges, we empirically study the reason why LLMs mostly fail to\nresolve GitHub issues and analyze some impact factors. Motivated by the\nempirical findings, we propose a novel LLM-based Multi-Agent framework for\nGitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized\nfor the software evolution: Manager, Repository Custodian, Developer, and\nQuality Assurance Engineer agents. This framework leverages the collaboration\nof various agents in the planning and coding process to unlock the potential of\nLLMs to resolve GitHub issues. In experiments, we employ the SWE-bench\nbenchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and\nClaude-2. MAGIS can resolve 13.94% GitHub issues, which significantly\noutperforms the baselines. Specifically, MAGIS achieves an eight-fold increase\nin resolved ratio over the direct application of GPT-4, the based LLM of our\nmethod. We also analyze the factors for improving GitHub issue resolution\nrates, such as line location, task allocation, etc.\n", "link": "http://arxiv.org/abs/2403.17927v1", "date": "2024-03-26", "relevancy": 0.9164, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.46}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4587}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4559}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MAGIS%3A%20LLM-Based%20Multi-Agent%20Framework%20for%20GitHub%20Issue%20Resolution&body=Title%3A%20MAGIS%3A%20LLM-Based%20Multi-Agent%20Framework%20for%20GitHub%20Issue%20Resolution%0AAuthor%3A%20Wei%20Tao%20and%20Yucheng%20Zhou%20and%20Wenqiang%20Zhang%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20In%20software%20evolution%2C%20resolving%20the%20emergent%20issues%20within%20GitHub%0Arepositories%20is%20a%20complex%20challenge%20that%20involves%20not%20only%20the%20incorporation%20of%0Anew%20code%20but%20also%20the%20maintenance%20of%20existing%20functionalities.%20Large%20Language%0AModels%20%28LLMs%29%20have%20shown%20promise%20in%20code%20generation%20and%20understanding%20but%20face%0Adifficulties%20in%20code%20change%2C%20particularly%20at%20the%20repository%20level.%20To%20overcome%0Athese%20challenges%2C%20we%20empirically%20study%20the%20reason%20why%20LLMs%20mostly%20fail%20to%0Aresolve%20GitHub%20issues%20and%20analyze%20some%20impact%20factors.%20Motivated%20by%20the%0Aempirical%20findings%2C%20we%20propose%20a%20novel%20LLM-based%20Multi-Agent%20framework%20for%0AGitHub%20Issue%20reSolution%2C%20MAGIS%2C%20consisting%20of%20four%20kinds%20of%20agents%20customized%0Afor%20the%20software%20evolution%3A%20Manager%2C%20Repository%20Custodian%2C%20Developer%2C%20and%0AQuality%20Assurance%20Engineer%20agents.%20This%20framework%20leverages%20the%20collaboration%0Aof%20various%20agents%20in%20the%20planning%20and%20coding%20process%20to%20unlock%20the%20potential%20of%0ALLMs%20to%20resolve%20GitHub%20issues.%20In%20experiments%2C%20we%20employ%20the%20SWE-bench%0Abenchmark%20to%20compare%20MAGIS%20with%20popular%20LLMs%2C%20including%20GPT-3.5%2C%20GPT-4%2C%20and%0AClaude-2.%20MAGIS%20can%20resolve%2013.94%25%20GitHub%20issues%2C%20which%20significantly%0Aoutperforms%20the%20baselines.%20Specifically%2C%20MAGIS%20achieves%20an%20eight-fold%20increase%0Ain%20resolved%20ratio%20over%20the%20direct%20application%20of%20GPT-4%2C%20the%20based%20LLM%20of%20our%0Amethod.%20We%20also%20analyze%20the%20factors%20for%20improving%20GitHub%20issue%20resolution%0Arates%2C%20such%20as%20line%20location%2C%20task%20allocation%2C%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17927v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGIS%3A%20LLM-Based%20Multi-Agent%20Framework%20for%20GitHub%20Issue%20Resolution&entry.906535625=Wei%20Tao%20and%20Yucheng%20Zhou%20and%20Wenqiang%20Zhang%20and%20Yu%20Cheng&entry.1292438233=%20%20In%20software%20evolution%2C%20resolving%20the%20emergent%20issues%20within%20GitHub%0Arepositories%20is%20a%20complex%20challenge%20that%20involves%20not%20only%20the%20incorporation%20of%0Anew%20code%20but%20also%20the%20maintenance%20of%20existing%20functionalities.%20Large%20Language%0AModels%20%28LLMs%29%20have%20shown%20promise%20in%20code%20generation%20and%20understanding%20but%20face%0Adifficulties%20in%20code%20change%2C%20particularly%20at%20the%20repository%20level.%20To%20overcome%0Athese%20challenges%2C%20we%20empirically%20study%20the%20reason%20why%20LLMs%20mostly%20fail%20to%0Aresolve%20GitHub%20issues%20and%20analyze%20some%20impact%20factors.%20Motivated%20by%20the%0Aempirical%20findings%2C%20we%20propose%20a%20novel%20LLM-based%20Multi-Agent%20framework%20for%0AGitHub%20Issue%20reSolution%2C%20MAGIS%2C%20consisting%20of%20four%20kinds%20of%20agents%20customized%0Afor%20the%20software%20evolution%3A%20Manager%2C%20Repository%20Custodian%2C%20Developer%2C%20and%0AQuality%20Assurance%20Engineer%20agents.%20This%20framework%20leverages%20the%20collaboration%0Aof%20various%20agents%20in%20the%20planning%20and%20coding%20process%20to%20unlock%20the%20potential%20of%0ALLMs%20to%20resolve%20GitHub%20issues.%20In%20experiments%2C%20we%20employ%20the%20SWE-bench%0Abenchmark%20to%20compare%20MAGIS%20with%20popular%20LLMs%2C%20including%20GPT-3.5%2C%20GPT-4%2C%20and%0AClaude-2.%20MAGIS%20can%20resolve%2013.94%25%20GitHub%20issues%2C%20which%20significantly%0Aoutperforms%20the%20baselines.%20Specifically%2C%20MAGIS%20achieves%20an%20eight-fold%20increase%0Ain%20resolved%20ratio%20over%20the%20direct%20application%20of%20GPT-4%2C%20the%20based%20LLM%20of%20our%0Amethod.%20We%20also%20analyze%20the%20factors%20for%20improving%20GitHub%20issue%20resolution%0Arates%2C%20such%20as%20line%20location%2C%20task%20allocation%2C%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17927v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


