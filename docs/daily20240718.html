<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240717.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo\n  Views", "author": "Yaniv Wolf and Amit Bracha and Ron Kimmel", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has emerged as an efficient approach\nfor accurately representing scenes. However, despite its superior novel view\nsynthesis capabilities, extracting the geometry of the scene directly from the\nGaussian properties remains a challenge, as those are optimized based on a\nphotometric loss. While some concurrent models have tried adding geometric\nconstraints during the Gaussian optimization process, they still produce noisy,\nunrealistic surfaces.\n  We propose a novel approach for bridging the gap between the noisy 3DGS\nrepresentation and the smooth 3D mesh representation, by injecting real-world\nknowledge into the depth extraction process. Instead of extracting the geometry\nof the scene directly from the Gaussian properties, we instead extract the\ngeometry through a pre-trained stereo-matching model. We render stereo-aligned\npairs of images corresponding to the original training poses, feed the pairs\ninto a stereo model to get a depth profile, and finally fuse all of the\nprofiles together to get a single mesh.\n  The resulting reconstruction is smoother, more accurate and shows more\nintricate details compared to other methods for surface reconstruction from\nGaussian Splatting, while only requiring a small overhead on top of the fairly\nshort 3DGS optimization process.\n  We performed extensive testing of the proposed method on in-the-wild scenes,\nobtained using a smartphone, showcasing its superior reconstruction abilities.\nAdditionally, we tested the method on the Tanks and Temples and DTU benchmarks,\nachieving state-of-the-art results.\n", "link": "http://arxiv.org/abs/2404.01810v2", "date": "2024-07-17", "relevancy": 3.3052, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7113}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7013}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS2Mesh%3A%20Surface%20Reconstruction%20from%20Gaussian%20Splatting%20via%20Novel%20Stereo%0A%20%20Views&body=Title%3A%20GS2Mesh%3A%20Surface%20Reconstruction%20from%20Gaussian%20Splatting%20via%20Novel%20Stereo%0A%20%20Views%0AAuthor%3A%20Yaniv%20Wolf%20and%20Amit%20Bracha%20and%20Ron%20Kimmel%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20an%20efficient%20approach%0Afor%20accurately%20representing%20scenes.%20However%2C%20despite%20its%20superior%20novel%20view%0Asynthesis%20capabilities%2C%20extracting%20the%20geometry%20of%20the%20scene%20directly%20from%20the%0AGaussian%20properties%20remains%20a%20challenge%2C%20as%20those%20are%20optimized%20based%20on%20a%0Aphotometric%20loss.%20While%20some%20concurrent%20models%20have%20tried%20adding%20geometric%0Aconstraints%20during%20the%20Gaussian%20optimization%20process%2C%20they%20still%20produce%20noisy%2C%0Aunrealistic%20surfaces.%0A%20%20We%20propose%20a%20novel%20approach%20for%20bridging%20the%20gap%20between%20the%20noisy%203DGS%0Arepresentation%20and%20the%20smooth%203D%20mesh%20representation%2C%20by%20injecting%20real-world%0Aknowledge%20into%20the%20depth%20extraction%20process.%20Instead%20of%20extracting%20the%20geometry%0Aof%20the%20scene%20directly%20from%20the%20Gaussian%20properties%2C%20we%20instead%20extract%20the%0Ageometry%20through%20a%20pre-trained%20stereo-matching%20model.%20We%20render%20stereo-aligned%0Apairs%20of%20images%20corresponding%20to%20the%20original%20training%20poses%2C%20feed%20the%20pairs%0Ainto%20a%20stereo%20model%20to%20get%20a%20depth%20profile%2C%20and%20finally%20fuse%20all%20of%20the%0Aprofiles%20together%20to%20get%20a%20single%20mesh.%0A%20%20The%20resulting%20reconstruction%20is%20smoother%2C%20more%20accurate%20and%20shows%20more%0Aintricate%20details%20compared%20to%20other%20methods%20for%20surface%20reconstruction%20from%0AGaussian%20Splatting%2C%20while%20only%20requiring%20a%20small%20overhead%20on%20top%20of%20the%20fairly%0Ashort%203DGS%20optimization%20process.%0A%20%20We%20performed%20extensive%20testing%20of%20the%20proposed%20method%20on%20in-the-wild%20scenes%2C%0Aobtained%20using%20a%20smartphone%2C%20showcasing%20its%20superior%20reconstruction%20abilities.%0AAdditionally%2C%20we%20tested%20the%20method%20on%20the%20Tanks%20and%20Temples%20and%20DTU%20benchmarks%2C%0Aachieving%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS2Mesh%253A%2520Surface%2520Reconstruction%2520from%2520Gaussian%2520Splatting%2520via%2520Novel%2520Stereo%250A%2520%2520Views%26entry.906535625%3DYaniv%2520Wolf%2520and%2520Amit%2520Bracha%2520and%2520Ron%2520Kimmel%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520an%2520efficient%2520approach%250Afor%2520accurately%2520representing%2520scenes.%2520However%252C%2520despite%2520its%2520superior%2520novel%2520view%250Asynthesis%2520capabilities%252C%2520extracting%2520the%2520geometry%2520of%2520the%2520scene%2520directly%2520from%2520the%250AGaussian%2520properties%2520remains%2520a%2520challenge%252C%2520as%2520those%2520are%2520optimized%2520based%2520on%2520a%250Aphotometric%2520loss.%2520While%2520some%2520concurrent%2520models%2520have%2520tried%2520adding%2520geometric%250Aconstraints%2520during%2520the%2520Gaussian%2520optimization%2520process%252C%2520they%2520still%2520produce%2520noisy%252C%250Aunrealistic%2520surfaces.%250A%2520%2520We%2520propose%2520a%2520novel%2520approach%2520for%2520bridging%2520the%2520gap%2520between%2520the%2520noisy%25203DGS%250Arepresentation%2520and%2520the%2520smooth%25203D%2520mesh%2520representation%252C%2520by%2520injecting%2520real-world%250Aknowledge%2520into%2520the%2520depth%2520extraction%2520process.%2520Instead%2520of%2520extracting%2520the%2520geometry%250Aof%2520the%2520scene%2520directly%2520from%2520the%2520Gaussian%2520properties%252C%2520we%2520instead%2520extract%2520the%250Ageometry%2520through%2520a%2520pre-trained%2520stereo-matching%2520model.%2520We%2520render%2520stereo-aligned%250Apairs%2520of%2520images%2520corresponding%2520to%2520the%2520original%2520training%2520poses%252C%2520feed%2520the%2520pairs%250Ainto%2520a%2520stereo%2520model%2520to%2520get%2520a%2520depth%2520profile%252C%2520and%2520finally%2520fuse%2520all%2520of%2520the%250Aprofiles%2520together%2520to%2520get%2520a%2520single%2520mesh.%250A%2520%2520The%2520resulting%2520reconstruction%2520is%2520smoother%252C%2520more%2520accurate%2520and%2520shows%2520more%250Aintricate%2520details%2520compared%2520to%2520other%2520methods%2520for%2520surface%2520reconstruction%2520from%250AGaussian%2520Splatting%252C%2520while%2520only%2520requiring%2520a%2520small%2520overhead%2520on%2520top%2520of%2520the%2520fairly%250Ashort%25203DGS%2520optimization%2520process.%250A%2520%2520We%2520performed%2520extensive%2520testing%2520of%2520the%2520proposed%2520method%2520on%2520in-the-wild%2520scenes%252C%250Aobtained%2520using%2520a%2520smartphone%252C%2520showcasing%2520its%2520superior%2520reconstruction%2520abilities.%250AAdditionally%252C%2520we%2520tested%2520the%2520method%2520on%2520the%2520Tanks%2520and%2520Temples%2520and%2520DTU%2520benchmarks%252C%250Aachieving%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS2Mesh%3A%20Surface%20Reconstruction%20from%20Gaussian%20Splatting%20via%20Novel%20Stereo%0A%20%20Views&entry.906535625=Yaniv%20Wolf%20and%20Amit%20Bracha%20and%20Ron%20Kimmel&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20an%20efficient%20approach%0Afor%20accurately%20representing%20scenes.%20However%2C%20despite%20its%20superior%20novel%20view%0Asynthesis%20capabilities%2C%20extracting%20the%20geometry%20of%20the%20scene%20directly%20from%20the%0AGaussian%20properties%20remains%20a%20challenge%2C%20as%20those%20are%20optimized%20based%20on%20a%0Aphotometric%20loss.%20While%20some%20concurrent%20models%20have%20tried%20adding%20geometric%0Aconstraints%20during%20the%20Gaussian%20optimization%20process%2C%20they%20still%20produce%20noisy%2C%0Aunrealistic%20surfaces.%0A%20%20We%20propose%20a%20novel%20approach%20for%20bridging%20the%20gap%20between%20the%20noisy%203DGS%0Arepresentation%20and%20the%20smooth%203D%20mesh%20representation%2C%20by%20injecting%20real-world%0Aknowledge%20into%20the%20depth%20extraction%20process.%20Instead%20of%20extracting%20the%20geometry%0Aof%20the%20scene%20directly%20from%20the%20Gaussian%20properties%2C%20we%20instead%20extract%20the%0Ageometry%20through%20a%20pre-trained%20stereo-matching%20model.%20We%20render%20stereo-aligned%0Apairs%20of%20images%20corresponding%20to%20the%20original%20training%20poses%2C%20feed%20the%20pairs%0Ainto%20a%20stereo%20model%20to%20get%20a%20depth%20profile%2C%20and%20finally%20fuse%20all%20of%20the%0Aprofiles%20together%20to%20get%20a%20single%20mesh.%0A%20%20The%20resulting%20reconstruction%20is%20smoother%2C%20more%20accurate%20and%20shows%20more%0Aintricate%20details%20compared%20to%20other%20methods%20for%20surface%20reconstruction%20from%0AGaussian%20Splatting%2C%20while%20only%20requiring%20a%20small%20overhead%20on%20top%20of%20the%20fairly%0Ashort%203DGS%20optimization%20process.%0A%20%20We%20performed%20extensive%20testing%20of%20the%20proposed%20method%20on%20in-the-wild%20scenes%2C%0Aobtained%20using%20a%20smartphone%2C%20showcasing%20its%20superior%20reconstruction%20abilities.%0AAdditionally%2C%20we%20tested%20the%20method%20on%20the%20Tanks%20and%20Temples%20and%20DTU%20benchmarks%2C%0Aachieving%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01810v2&entry.124074799=Read"},
{"title": "Generalizable Human Gaussians for Sparse View Synthesis", "author": "Youngjoong Kwon and Baole Fang and Yixing Lu and Haoye Dong and Cheng Zhang and Francisco Vicente Carrasco and Albert Mosella-Montoro and Jianjin Xu and Shingo Takagi and Daeil Kim and Aayush Prakash and Fernando De la Torre", "abstract": "  Recent progress in neural rendering has brought forth pioneering methods,\nsuch as NeRF and Gaussian Splatting, which revolutionize view rendering across\nvarious domains like AR/VR, gaming, and content creation. While these methods\nexcel at interpolating {\\em within the training data}, the challenge of\ngeneralizing to new scenes and objects from very sparse views persists.\nSpecifically, modeling 3D humans from sparse views presents formidable hurdles\ndue to the inherent complexity of human geometry, resulting in inaccurate\nreconstructions of geometry and textures. To tackle this challenge, this paper\nleverages recent advancements in Gaussian Splatting and introduces a new method\nto learn generalizable human Gaussians that allows photorealistic and accurate\nview-rendering of a new human subject from a limited set of sparse views in a\nfeed-forward manner. A pivotal innovation of our approach involves\nreformulating the learning of 3D Gaussian parameters into a regression process\ndefined on the 2D UV space of a human template, which allows leveraging the\nstrong geometry prior and the advantages of 2D convolutions. In addition, a\nmulti-scaffold is proposed to effectively represent the offset details. Our\nmethod outperforms recent methods on both within-dataset generalization as well\nas cross-dataset generalization settings.\n", "link": "http://arxiv.org/abs/2407.12777v1", "date": "2024-07-17", "relevancy": 3.2399, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6562}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.656}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Human%20Gaussians%20for%20Sparse%20View%20Synthesis&body=Title%3A%20Generalizable%20Human%20Gaussians%20for%20Sparse%20View%20Synthesis%0AAuthor%3A%20Youngjoong%20Kwon%20and%20Baole%20Fang%20and%20Yixing%20Lu%20and%20Haoye%20Dong%20and%20Cheng%20Zhang%20and%20Francisco%20Vicente%20Carrasco%20and%20Albert%20Mosella-Montoro%20and%20Jianjin%20Xu%20and%20Shingo%20Takagi%20and%20Daeil%20Kim%20and%20Aayush%20Prakash%20and%20Fernando%20De%20la%20Torre%0AAbstract%3A%20%20%20Recent%20progress%20in%20neural%20rendering%20has%20brought%20forth%20pioneering%20methods%2C%0Asuch%20as%20NeRF%20and%20Gaussian%20Splatting%2C%20which%20revolutionize%20view%20rendering%20across%0Avarious%20domains%20like%20AR/VR%2C%20gaming%2C%20and%20content%20creation.%20While%20these%20methods%0Aexcel%20at%20interpolating%20%7B%5Cem%20within%20the%20training%20data%7D%2C%20the%20challenge%20of%0Ageneralizing%20to%20new%20scenes%20and%20objects%20from%20very%20sparse%20views%20persists.%0ASpecifically%2C%20modeling%203D%20humans%20from%20sparse%20views%20presents%20formidable%20hurdles%0Adue%20to%20the%20inherent%20complexity%20of%20human%20geometry%2C%20resulting%20in%20inaccurate%0Areconstructions%20of%20geometry%20and%20textures.%20To%20tackle%20this%20challenge%2C%20this%20paper%0Aleverages%20recent%20advancements%20in%20Gaussian%20Splatting%20and%20introduces%20a%20new%20method%0Ato%20learn%20generalizable%20human%20Gaussians%20that%20allows%20photorealistic%20and%20accurate%0Aview-rendering%20of%20a%20new%20human%20subject%20from%20a%20limited%20set%20of%20sparse%20views%20in%20a%0Afeed-forward%20manner.%20A%20pivotal%20innovation%20of%20our%20approach%20involves%0Areformulating%20the%20learning%20of%203D%20Gaussian%20parameters%20into%20a%20regression%20process%0Adefined%20on%20the%202D%20UV%20space%20of%20a%20human%20template%2C%20which%20allows%20leveraging%20the%0Astrong%20geometry%20prior%20and%20the%20advantages%20of%202D%20convolutions.%20In%20addition%2C%20a%0Amulti-scaffold%20is%20proposed%20to%20effectively%20represent%20the%20offset%20details.%20Our%0Amethod%20outperforms%20recent%20methods%20on%20both%20within-dataset%20generalization%20as%20well%0Aas%20cross-dataset%20generalization%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Human%2520Gaussians%2520for%2520Sparse%2520View%2520Synthesis%26entry.906535625%3DYoungjoong%2520Kwon%2520and%2520Baole%2520Fang%2520and%2520Yixing%2520Lu%2520and%2520Haoye%2520Dong%2520and%2520Cheng%2520Zhang%2520and%2520Francisco%2520Vicente%2520Carrasco%2520and%2520Albert%2520Mosella-Montoro%2520and%2520Jianjin%2520Xu%2520and%2520Shingo%2520Takagi%2520and%2520Daeil%2520Kim%2520and%2520Aayush%2520Prakash%2520and%2520Fernando%2520De%2520la%2520Torre%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520neural%2520rendering%2520has%2520brought%2520forth%2520pioneering%2520methods%252C%250Asuch%2520as%2520NeRF%2520and%2520Gaussian%2520Splatting%252C%2520which%2520revolutionize%2520view%2520rendering%2520across%250Avarious%2520domains%2520like%2520AR/VR%252C%2520gaming%252C%2520and%2520content%2520creation.%2520While%2520these%2520methods%250Aexcel%2520at%2520interpolating%2520%257B%255Cem%2520within%2520the%2520training%2520data%257D%252C%2520the%2520challenge%2520of%250Ageneralizing%2520to%2520new%2520scenes%2520and%2520objects%2520from%2520very%2520sparse%2520views%2520persists.%250ASpecifically%252C%2520modeling%25203D%2520humans%2520from%2520sparse%2520views%2520presents%2520formidable%2520hurdles%250Adue%2520to%2520the%2520inherent%2520complexity%2520of%2520human%2520geometry%252C%2520resulting%2520in%2520inaccurate%250Areconstructions%2520of%2520geometry%2520and%2520textures.%2520To%2520tackle%2520this%2520challenge%252C%2520this%2520paper%250Aleverages%2520recent%2520advancements%2520in%2520Gaussian%2520Splatting%2520and%2520introduces%2520a%2520new%2520method%250Ato%2520learn%2520generalizable%2520human%2520Gaussians%2520that%2520allows%2520photorealistic%2520and%2520accurate%250Aview-rendering%2520of%2520a%2520new%2520human%2520subject%2520from%2520a%2520limited%2520set%2520of%2520sparse%2520views%2520in%2520a%250Afeed-forward%2520manner.%2520A%2520pivotal%2520innovation%2520of%2520our%2520approach%2520involves%250Areformulating%2520the%2520learning%2520of%25203D%2520Gaussian%2520parameters%2520into%2520a%2520regression%2520process%250Adefined%2520on%2520the%25202D%2520UV%2520space%2520of%2520a%2520human%2520template%252C%2520which%2520allows%2520leveraging%2520the%250Astrong%2520geometry%2520prior%2520and%2520the%2520advantages%2520of%25202D%2520convolutions.%2520In%2520addition%252C%2520a%250Amulti-scaffold%2520is%2520proposed%2520to%2520effectively%2520represent%2520the%2520offset%2520details.%2520Our%250Amethod%2520outperforms%2520recent%2520methods%2520on%2520both%2520within-dataset%2520generalization%2520as%2520well%250Aas%2520cross-dataset%2520generalization%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Human%20Gaussians%20for%20Sparse%20View%20Synthesis&entry.906535625=Youngjoong%20Kwon%20and%20Baole%20Fang%20and%20Yixing%20Lu%20and%20Haoye%20Dong%20and%20Cheng%20Zhang%20and%20Francisco%20Vicente%20Carrasco%20and%20Albert%20Mosella-Montoro%20and%20Jianjin%20Xu%20and%20Shingo%20Takagi%20and%20Daeil%20Kim%20and%20Aayush%20Prakash%20and%20Fernando%20De%20la%20Torre&entry.1292438233=%20%20Recent%20progress%20in%20neural%20rendering%20has%20brought%20forth%20pioneering%20methods%2C%0Asuch%20as%20NeRF%20and%20Gaussian%20Splatting%2C%20which%20revolutionize%20view%20rendering%20across%0Avarious%20domains%20like%20AR/VR%2C%20gaming%2C%20and%20content%20creation.%20While%20these%20methods%0Aexcel%20at%20interpolating%20%7B%5Cem%20within%20the%20training%20data%7D%2C%20the%20challenge%20of%0Ageneralizing%20to%20new%20scenes%20and%20objects%20from%20very%20sparse%20views%20persists.%0ASpecifically%2C%20modeling%203D%20humans%20from%20sparse%20views%20presents%20formidable%20hurdles%0Adue%20to%20the%20inherent%20complexity%20of%20human%20geometry%2C%20resulting%20in%20inaccurate%0Areconstructions%20of%20geometry%20and%20textures.%20To%20tackle%20this%20challenge%2C%20this%20paper%0Aleverages%20recent%20advancements%20in%20Gaussian%20Splatting%20and%20introduces%20a%20new%20method%0Ato%20learn%20generalizable%20human%20Gaussians%20that%20allows%20photorealistic%20and%20accurate%0Aview-rendering%20of%20a%20new%20human%20subject%20from%20a%20limited%20set%20of%20sparse%20views%20in%20a%0Afeed-forward%20manner.%20A%20pivotal%20innovation%20of%20our%20approach%20involves%0Areformulating%20the%20learning%20of%203D%20Gaussian%20parameters%20into%20a%20regression%20process%0Adefined%20on%20the%202D%20UV%20space%20of%20a%20human%20template%2C%20which%20allows%20leveraging%20the%0Astrong%20geometry%20prior%20and%20the%20advantages%20of%202D%20convolutions.%20In%20addition%2C%20a%0Amulti-scaffold%20is%20proposed%20to%20effectively%20represent%20the%20offset%20details.%20Our%0Amethod%20outperforms%20recent%20methods%20on%20both%20within-dataset%20generalization%20as%20well%0Aas%20cross-dataset%20generalization%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12777v1&entry.124074799=Read"},
{"title": "DiverseDream: Diverse Text-to-3D Synthesis with Augmented Text Embedding", "author": "Uy Dieu Tran and Minh Luu and Phong Ha Nguyen and Khoi Nguyen and Binh-Son Hua", "abstract": "  Text-to-3D synthesis has recently emerged as a new approach to sampling 3D\nmodels by adopting pretrained text-to-image models as guiding visual priors. An\nintriguing but underexplored problem with existing text-to-3D methods is that\n3D models obtained from the sampling-by-optimization procedure tend to have\nmode collapses, and hence poor diversity in their results. In this paper, we\nprovide an analysis and identify potential causes of such a limited diversity,\nwhich motivates us to devise a new method that considers the joint generation\nof different 3D models from the same text prompt. We propose to use augmented\ntext prompts via textual inversion of reference images to diversify the joint\ngeneration. We show that our method leads to improved diversity in text-to-3D\nsynthesis qualitatively and quantitatively. Project page:\nhttps://diversedream.github.io\n", "link": "http://arxiv.org/abs/2312.02192v2", "date": "2024-07-17", "relevancy": 3.1686, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6443}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6443}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiverseDream%3A%20Diverse%20Text-to-3D%20Synthesis%20with%20Augmented%20Text%20Embedding&body=Title%3A%20DiverseDream%3A%20Diverse%20Text-to-3D%20Synthesis%20with%20Augmented%20Text%20Embedding%0AAuthor%3A%20Uy%20Dieu%20Tran%20and%20Minh%20Luu%20and%20Phong%20Ha%20Nguyen%20and%20Khoi%20Nguyen%20and%20Binh-Son%20Hua%0AAbstract%3A%20%20%20Text-to-3D%20synthesis%20has%20recently%20emerged%20as%20a%20new%20approach%20to%20sampling%203D%0Amodels%20by%20adopting%20pretrained%20text-to-image%20models%20as%20guiding%20visual%20priors.%20An%0Aintriguing%20but%20underexplored%20problem%20with%20existing%20text-to-3D%20methods%20is%20that%0A3D%20models%20obtained%20from%20the%20sampling-by-optimization%20procedure%20tend%20to%20have%0Amode%20collapses%2C%20and%20hence%20poor%20diversity%20in%20their%20results.%20In%20this%20paper%2C%20we%0Aprovide%20an%20analysis%20and%20identify%20potential%20causes%20of%20such%20a%20limited%20diversity%2C%0Awhich%20motivates%20us%20to%20devise%20a%20new%20method%20that%20considers%20the%20joint%20generation%0Aof%20different%203D%20models%20from%20the%20same%20text%20prompt.%20We%20propose%20to%20use%20augmented%0Atext%20prompts%20via%20textual%20inversion%20of%20reference%20images%20to%20diversify%20the%20joint%0Ageneration.%20We%20show%20that%20our%20method%20leads%20to%20improved%20diversity%20in%20text-to-3D%0Asynthesis%20qualitatively%20and%20quantitatively.%20Project%20page%3A%0Ahttps%3A//diversedream.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02192v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverseDream%253A%2520Diverse%2520Text-to-3D%2520Synthesis%2520with%2520Augmented%2520Text%2520Embedding%26entry.906535625%3DUy%2520Dieu%2520Tran%2520and%2520Minh%2520Luu%2520and%2520Phong%2520Ha%2520Nguyen%2520and%2520Khoi%2520Nguyen%2520and%2520Binh-Son%2520Hua%26entry.1292438233%3D%2520%2520Text-to-3D%2520synthesis%2520has%2520recently%2520emerged%2520as%2520a%2520new%2520approach%2520to%2520sampling%25203D%250Amodels%2520by%2520adopting%2520pretrained%2520text-to-image%2520models%2520as%2520guiding%2520visual%2520priors.%2520An%250Aintriguing%2520but%2520underexplored%2520problem%2520with%2520existing%2520text-to-3D%2520methods%2520is%2520that%250A3D%2520models%2520obtained%2520from%2520the%2520sampling-by-optimization%2520procedure%2520tend%2520to%2520have%250Amode%2520collapses%252C%2520and%2520hence%2520poor%2520diversity%2520in%2520their%2520results.%2520In%2520this%2520paper%252C%2520we%250Aprovide%2520an%2520analysis%2520and%2520identify%2520potential%2520causes%2520of%2520such%2520a%2520limited%2520diversity%252C%250Awhich%2520motivates%2520us%2520to%2520devise%2520a%2520new%2520method%2520that%2520considers%2520the%2520joint%2520generation%250Aof%2520different%25203D%2520models%2520from%2520the%2520same%2520text%2520prompt.%2520We%2520propose%2520to%2520use%2520augmented%250Atext%2520prompts%2520via%2520textual%2520inversion%2520of%2520reference%2520images%2520to%2520diversify%2520the%2520joint%250Ageneration.%2520We%2520show%2520that%2520our%2520method%2520leads%2520to%2520improved%2520diversity%2520in%2520text-to-3D%250Asynthesis%2520qualitatively%2520and%2520quantitatively.%2520Project%2520page%253A%250Ahttps%253A//diversedream.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02192v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiverseDream%3A%20Diverse%20Text-to-3D%20Synthesis%20with%20Augmented%20Text%20Embedding&entry.906535625=Uy%20Dieu%20Tran%20and%20Minh%20Luu%20and%20Phong%20Ha%20Nguyen%20and%20Khoi%20Nguyen%20and%20Binh-Son%20Hua&entry.1292438233=%20%20Text-to-3D%20synthesis%20has%20recently%20emerged%20as%20a%20new%20approach%20to%20sampling%203D%0Amodels%20by%20adopting%20pretrained%20text-to-image%20models%20as%20guiding%20visual%20priors.%20An%0Aintriguing%20but%20underexplored%20problem%20with%20existing%20text-to-3D%20methods%20is%20that%0A3D%20models%20obtained%20from%20the%20sampling-by-optimization%20procedure%20tend%20to%20have%0Amode%20collapses%2C%20and%20hence%20poor%20diversity%20in%20their%20results.%20In%20this%20paper%2C%20we%0Aprovide%20an%20analysis%20and%20identify%20potential%20causes%20of%20such%20a%20limited%20diversity%2C%0Awhich%20motivates%20us%20to%20devise%20a%20new%20method%20that%20considers%20the%20joint%20generation%0Aof%20different%203D%20models%20from%20the%20same%20text%20prompt.%20We%20propose%20to%20use%20augmented%0Atext%20prompts%20via%20textual%20inversion%20of%20reference%20images%20to%20diversify%20the%20joint%0Ageneration.%20We%20show%20that%20our%20method%20leads%20to%20improved%20diversity%20in%20text-to-3D%0Asynthesis%20qualitatively%20and%20quantitatively.%20Project%20page%3A%0Ahttps%3A//diversedream.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02192v2&entry.124074799=Read"},
{"title": "SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization", "author": "Yiyang Chen and Siyan Dong and Xulong Wang and Lulu Cai and Youyi Zheng and Yanchao Yang", "abstract": "  3D surface reconstruction from images is essential for numerous applications.\nRecently, Neural Radiance Fields (NeRFs) have emerged as a promising framework\nfor 3D modeling. However, NeRFs require accurate camera poses as input, and\nexisting methods struggle to handle significantly noisy pose estimates (i.e.,\noutliers), which are commonly encountered in real-world scenarios. To tackle\nthis challenge, we present a novel approach that optimizes radiance fields with\nscene graphs to mitigate the influence of outlier poses. Our method\nincorporates an adaptive inlier-outlier confidence estimation scheme based on\nscene graphs, emphasizing images of high compatibility with the neighborhood\nand consistency in the rendering quality. We also introduce an effective\nintersection-over-union (IoU) loss to optimize the camera pose and surface\ngeometry, together with a coarse-to-fine strategy to facilitate the training.\nFurthermore, we propose a new dataset containing typical outlier poses for a\ndetailed evaluation. Experimental results on various datasets consistently\ndemonstrate the effectiveness and superiority of our method over existing\napproaches, showcasing its robustness in handling outliers and producing\nhigh-quality 3D reconstructions. Our code and data are available at:\n\\url{https://github.com/Iris-cyy/SG-NeRF}.\n", "link": "http://arxiv.org/abs/2407.12667v1", "date": "2024-07-17", "relevancy": 2.9622, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6509}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.58}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SG-NeRF%3A%20Neural%20Surface%20Reconstruction%20with%20Scene%20Graph%20Optimization&body=Title%3A%20SG-NeRF%3A%20Neural%20Surface%20Reconstruction%20with%20Scene%20Graph%20Optimization%0AAuthor%3A%20Yiyang%20Chen%20and%20Siyan%20Dong%20and%20Xulong%20Wang%20and%20Lulu%20Cai%20and%20Youyi%20Zheng%20and%20Yanchao%20Yang%0AAbstract%3A%20%20%203D%20surface%20reconstruction%20from%20images%20is%20essential%20for%20numerous%20applications.%0ARecently%2C%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20a%20promising%20framework%0Afor%203D%20modeling.%20However%2C%20NeRFs%20require%20accurate%20camera%20poses%20as%20input%2C%20and%0Aexisting%20methods%20struggle%20to%20handle%20significantly%20noisy%20pose%20estimates%20%28i.e.%2C%0Aoutliers%29%2C%20which%20are%20commonly%20encountered%20in%20real-world%20scenarios.%20To%20tackle%0Athis%20challenge%2C%20we%20present%20a%20novel%20approach%20that%20optimizes%20radiance%20fields%20with%0Ascene%20graphs%20to%20mitigate%20the%20influence%20of%20outlier%20poses.%20Our%20method%0Aincorporates%20an%20adaptive%20inlier-outlier%20confidence%20estimation%20scheme%20based%20on%0Ascene%20graphs%2C%20emphasizing%20images%20of%20high%20compatibility%20with%20the%20neighborhood%0Aand%20consistency%20in%20the%20rendering%20quality.%20We%20also%20introduce%20an%20effective%0Aintersection-over-union%20%28IoU%29%20loss%20to%20optimize%20the%20camera%20pose%20and%20surface%0Ageometry%2C%20together%20with%20a%20coarse-to-fine%20strategy%20to%20facilitate%20the%20training.%0AFurthermore%2C%20we%20propose%20a%20new%20dataset%20containing%20typical%20outlier%20poses%20for%20a%0Adetailed%20evaluation.%20Experimental%20results%20on%20various%20datasets%20consistently%0Ademonstrate%20the%20effectiveness%20and%20superiority%20of%20our%20method%20over%20existing%0Aapproaches%2C%20showcasing%20its%20robustness%20in%20handling%20outliers%20and%20producing%0Ahigh-quality%203D%20reconstructions.%20Our%20code%20and%20data%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Iris-cyy/SG-NeRF%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSG-NeRF%253A%2520Neural%2520Surface%2520Reconstruction%2520with%2520Scene%2520Graph%2520Optimization%26entry.906535625%3DYiyang%2520Chen%2520and%2520Siyan%2520Dong%2520and%2520Xulong%2520Wang%2520and%2520Lulu%2520Cai%2520and%2520Youyi%2520Zheng%2520and%2520Yanchao%2520Yang%26entry.1292438233%3D%2520%25203D%2520surface%2520reconstruction%2520from%2520images%2520is%2520essential%2520for%2520numerous%2520applications.%250ARecently%252C%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520framework%250Afor%25203D%2520modeling.%2520However%252C%2520NeRFs%2520require%2520accurate%2520camera%2520poses%2520as%2520input%252C%2520and%250Aexisting%2520methods%2520struggle%2520to%2520handle%2520significantly%2520noisy%2520pose%2520estimates%2520%2528i.e.%252C%250Aoutliers%2529%252C%2520which%2520are%2520commonly%2520encountered%2520in%2520real-world%2520scenarios.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520present%2520a%2520novel%2520approach%2520that%2520optimizes%2520radiance%2520fields%2520with%250Ascene%2520graphs%2520to%2520mitigate%2520the%2520influence%2520of%2520outlier%2520poses.%2520Our%2520method%250Aincorporates%2520an%2520adaptive%2520inlier-outlier%2520confidence%2520estimation%2520scheme%2520based%2520on%250Ascene%2520graphs%252C%2520emphasizing%2520images%2520of%2520high%2520compatibility%2520with%2520the%2520neighborhood%250Aand%2520consistency%2520in%2520the%2520rendering%2520quality.%2520We%2520also%2520introduce%2520an%2520effective%250Aintersection-over-union%2520%2528IoU%2529%2520loss%2520to%2520optimize%2520the%2520camera%2520pose%2520and%2520surface%250Ageometry%252C%2520together%2520with%2520a%2520coarse-to-fine%2520strategy%2520to%2520facilitate%2520the%2520training.%250AFurthermore%252C%2520we%2520propose%2520a%2520new%2520dataset%2520containing%2520typical%2520outlier%2520poses%2520for%2520a%250Adetailed%2520evaluation.%2520Experimental%2520results%2520on%2520various%2520datasets%2520consistently%250Ademonstrate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520our%2520method%2520over%2520existing%250Aapproaches%252C%2520showcasing%2520its%2520robustness%2520in%2520handling%2520outliers%2520and%2520producing%250Ahigh-quality%25203D%2520reconstructions.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/Iris-cyy/SG-NeRF%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG-NeRF%3A%20Neural%20Surface%20Reconstruction%20with%20Scene%20Graph%20Optimization&entry.906535625=Yiyang%20Chen%20and%20Siyan%20Dong%20and%20Xulong%20Wang%20and%20Lulu%20Cai%20and%20Youyi%20Zheng%20and%20Yanchao%20Yang&entry.1292438233=%20%203D%20surface%20reconstruction%20from%20images%20is%20essential%20for%20numerous%20applications.%0ARecently%2C%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20a%20promising%20framework%0Afor%203D%20modeling.%20However%2C%20NeRFs%20require%20accurate%20camera%20poses%20as%20input%2C%20and%0Aexisting%20methods%20struggle%20to%20handle%20significantly%20noisy%20pose%20estimates%20%28i.e.%2C%0Aoutliers%29%2C%20which%20are%20commonly%20encountered%20in%20real-world%20scenarios.%20To%20tackle%0Athis%20challenge%2C%20we%20present%20a%20novel%20approach%20that%20optimizes%20radiance%20fields%20with%0Ascene%20graphs%20to%20mitigate%20the%20influence%20of%20outlier%20poses.%20Our%20method%0Aincorporates%20an%20adaptive%20inlier-outlier%20confidence%20estimation%20scheme%20based%20on%0Ascene%20graphs%2C%20emphasizing%20images%20of%20high%20compatibility%20with%20the%20neighborhood%0Aand%20consistency%20in%20the%20rendering%20quality.%20We%20also%20introduce%20an%20effective%0Aintersection-over-union%20%28IoU%29%20loss%20to%20optimize%20the%20camera%20pose%20and%20surface%0Ageometry%2C%20together%20with%20a%20coarse-to-fine%20strategy%20to%20facilitate%20the%20training.%0AFurthermore%2C%20we%20propose%20a%20new%20dataset%20containing%20typical%20outlier%20poses%20for%20a%0Adetailed%20evaluation.%20Experimental%20results%20on%20various%20datasets%20consistently%0Ademonstrate%20the%20effectiveness%20and%20superiority%20of%20our%20method%20over%20existing%0Aapproaches%2C%20showcasing%20its%20robustness%20in%20handling%20outliers%20and%20producing%0Ahigh-quality%203D%20reconstructions.%20Our%20code%20and%20data%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Iris-cyy/SG-NeRF%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12667v1&entry.124074799=Read"},
{"title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation", "author": "Zhening Huang and Xiaoyang Wu and Xi Chen and Hengshuang Zhao and Lei Zhu and Joan Lasenby", "abstract": "  In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D\nopen-vocabulary scene understanding. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds, the \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision-language models to extract\ninteresting objects, and the \"Lookup\" module searches through the outcomes of\n\"Snap\" to assign category names to the proposed masks. This approach, yet\nsimple, achieves state-of-the-art performance across a wide range of 3D\nopen-vocabulary tasks, including recognition, object detection, and instance\nsegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D\nfacilitates effortless switching between different 2D detectors without\nrequiring retraining. When integrated with powerful 2D open-world models, it\nachieves excellent results in scene understanding tasks. Furthermore, when\ncombined with LLM-powered 2D models, OpenIns3D exhibits an impressive\ncapability to comprehend and process highly complex text queries that demand\nintricate reasoning and real-world knowledge. Project page:\nhttps://zheninghuang.github.io/OpenIns3D/\n", "link": "http://arxiv.org/abs/2309.00616v4", "date": "2024-07-17", "relevancy": 2.9582, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5886}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenIns3D%3A%20Snap%20and%20Lookup%20for%203D%20Open-vocabulary%20Instance%20Segmentation&body=Title%3A%20OpenIns3D%3A%20Snap%20and%20Lookup%20for%203D%20Open-vocabulary%20Instance%20Segmentation%0AAuthor%3A%20Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Xi%20Chen%20and%20Hengshuang%20Zhao%20and%20Lei%20Zhu%20and%20Joan%20Lasenby%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20OpenIns3D%2C%20a%20new%203D-input-only%20framework%20for%203D%0Aopen-vocabulary%20scene%20understanding.%20The%20OpenIns3D%20framework%20employs%20a%0A%22Mask-Snap-Lookup%22%20scheme.%20The%20%22Mask%22%20module%20learns%20class-agnostic%20mask%0Aproposals%20in%203D%20point%20clouds%2C%20the%20%22Snap%22%20module%20generates%20synthetic%20scene-level%0Aimages%20at%20multiple%20scales%20and%20leverages%202D%20vision-language%20models%20to%20extract%0Ainteresting%20objects%2C%20and%20the%20%22Lookup%22%20module%20searches%20through%20the%20outcomes%20of%0A%22Snap%22%20to%20assign%20category%20names%20to%20the%20proposed%20masks.%20This%20approach%2C%20yet%0Asimple%2C%20achieves%20state-of-the-art%20performance%20across%20a%20wide%20range%20of%203D%0Aopen-vocabulary%20tasks%2C%20including%20recognition%2C%20object%20detection%2C%20and%20instance%0Asegmentation%2C%20on%20both%20indoor%20and%20outdoor%20datasets.%20Moreover%2C%20OpenIns3D%0Afacilitates%20effortless%20switching%20between%20different%202D%20detectors%20without%0Arequiring%20retraining.%20When%20integrated%20with%20powerful%202D%20open-world%20models%2C%20it%0Aachieves%20excellent%20results%20in%20scene%20understanding%20tasks.%20Furthermore%2C%20when%0Acombined%20with%20LLM-powered%202D%20models%2C%20OpenIns3D%20exhibits%20an%20impressive%0Acapability%20to%20comprehend%20and%20process%20highly%20complex%20text%20queries%20that%20demand%0Aintricate%20reasoning%20and%20real-world%20knowledge.%20Project%20page%3A%0Ahttps%3A//zheninghuang.github.io/OpenIns3D/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00616v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenIns3D%253A%2520Snap%2520and%2520Lookup%2520for%25203D%2520Open-vocabulary%2520Instance%2520Segmentation%26entry.906535625%3DZhening%2520Huang%2520and%2520Xiaoyang%2520Wu%2520and%2520Xi%2520Chen%2520and%2520Hengshuang%2520Zhao%2520and%2520Lei%2520Zhu%2520and%2520Joan%2520Lasenby%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520OpenIns3D%252C%2520a%2520new%25203D-input-only%2520framework%2520for%25203D%250Aopen-vocabulary%2520scene%2520understanding.%2520The%2520OpenIns3D%2520framework%2520employs%2520a%250A%2522Mask-Snap-Lookup%2522%2520scheme.%2520The%2520%2522Mask%2522%2520module%2520learns%2520class-agnostic%2520mask%250Aproposals%2520in%25203D%2520point%2520clouds%252C%2520the%2520%2522Snap%2522%2520module%2520generates%2520synthetic%2520scene-level%250Aimages%2520at%2520multiple%2520scales%2520and%2520leverages%25202D%2520vision-language%2520models%2520to%2520extract%250Ainteresting%2520objects%252C%2520and%2520the%2520%2522Lookup%2522%2520module%2520searches%2520through%2520the%2520outcomes%2520of%250A%2522Snap%2522%2520to%2520assign%2520category%2520names%2520to%2520the%2520proposed%2520masks.%2520This%2520approach%252C%2520yet%250Asimple%252C%2520achieves%2520state-of-the-art%2520performance%2520across%2520a%2520wide%2520range%2520of%25203D%250Aopen-vocabulary%2520tasks%252C%2520including%2520recognition%252C%2520object%2520detection%252C%2520and%2520instance%250Asegmentation%252C%2520on%2520both%2520indoor%2520and%2520outdoor%2520datasets.%2520Moreover%252C%2520OpenIns3D%250Afacilitates%2520effortless%2520switching%2520between%2520different%25202D%2520detectors%2520without%250Arequiring%2520retraining.%2520When%2520integrated%2520with%2520powerful%25202D%2520open-world%2520models%252C%2520it%250Aachieves%2520excellent%2520results%2520in%2520scene%2520understanding%2520tasks.%2520Furthermore%252C%2520when%250Acombined%2520with%2520LLM-powered%25202D%2520models%252C%2520OpenIns3D%2520exhibits%2520an%2520impressive%250Acapability%2520to%2520comprehend%2520and%2520process%2520highly%2520complex%2520text%2520queries%2520that%2520demand%250Aintricate%2520reasoning%2520and%2520real-world%2520knowledge.%2520Project%2520page%253A%250Ahttps%253A//zheninghuang.github.io/OpenIns3D/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00616v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenIns3D%3A%20Snap%20and%20Lookup%20for%203D%20Open-vocabulary%20Instance%20Segmentation&entry.906535625=Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Xi%20Chen%20and%20Hengshuang%20Zhao%20and%20Lei%20Zhu%20and%20Joan%20Lasenby&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20OpenIns3D%2C%20a%20new%203D-input-only%20framework%20for%203D%0Aopen-vocabulary%20scene%20understanding.%20The%20OpenIns3D%20framework%20employs%20a%0A%22Mask-Snap-Lookup%22%20scheme.%20The%20%22Mask%22%20module%20learns%20class-agnostic%20mask%0Aproposals%20in%203D%20point%20clouds%2C%20the%20%22Snap%22%20module%20generates%20synthetic%20scene-level%0Aimages%20at%20multiple%20scales%20and%20leverages%202D%20vision-language%20models%20to%20extract%0Ainteresting%20objects%2C%20and%20the%20%22Lookup%22%20module%20searches%20through%20the%20outcomes%20of%0A%22Snap%22%20to%20assign%20category%20names%20to%20the%20proposed%20masks.%20This%20approach%2C%20yet%0Asimple%2C%20achieves%20state-of-the-art%20performance%20across%20a%20wide%20range%20of%203D%0Aopen-vocabulary%20tasks%2C%20including%20recognition%2C%20object%20detection%2C%20and%20instance%0Asegmentation%2C%20on%20both%20indoor%20and%20outdoor%20datasets.%20Moreover%2C%20OpenIns3D%0Afacilitates%20effortless%20switching%20between%20different%202D%20detectors%20without%0Arequiring%20retraining.%20When%20integrated%20with%20powerful%202D%20open-world%20models%2C%20it%0Aachieves%20excellent%20results%20in%20scene%20understanding%20tasks.%20Furthermore%2C%20when%0Acombined%20with%20LLM-powered%202D%20models%2C%20OpenIns3D%20exhibits%20an%20impressive%0Acapability%20to%20comprehend%20and%20process%20highly%20complex%20text%20queries%20that%20demand%0Aintricate%20reasoning%20and%20real-world%20knowledge.%20Project%20page%3A%0Ahttps%3A//zheninghuang.github.io/OpenIns3D/%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00616v4&entry.124074799=Read"},
{"title": "LTRL: Boosting Long-tail Recognition via Reflective Learning", "author": "Qihao Zhao and Yalun Dai and Shen Lin and Wei Hu and Fan Zhang and Jun Liu", "abstract": "  In real-world scenarios, where knowledge distributions exhibit long-tail.\nHumans manage to master knowledge uniformly across imbalanced distributions, a\nfeat attributed to their diligent practices of reviewing, summarizing, and\ncorrecting errors. Motivated by this learning process, we propose a novel\nlearning paradigm, called reflecting learning, in handling long-tail\nrecognition. Our method integrates three processes for reviewing past\npredictions during training, summarizing and leveraging the feature relation\nacross classes, and correcting gradient conflict for loss functions. These\ndesigns are lightweight enough to plug and play with existing long-tail\nlearning methods, achieving state-of-the-art performance in popular long-tail\nvisual benchmarks. The experimental results highlight the great potential of\nreflecting learning in dealing with long-tail recognition.\n", "link": "http://arxiv.org/abs/2407.12568v1", "date": "2024-07-17", "relevancy": 2.94, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6398}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5664}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LTRL%3A%20Boosting%20Long-tail%20Recognition%20via%20Reflective%20Learning&body=Title%3A%20LTRL%3A%20Boosting%20Long-tail%20Recognition%20via%20Reflective%20Learning%0AAuthor%3A%20Qihao%20Zhao%20and%20Yalun%20Dai%20and%20Shen%20Lin%20and%20Wei%20Hu%20and%20Fan%20Zhang%20and%20Jun%20Liu%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20where%20knowledge%20distributions%20exhibit%20long-tail.%0AHumans%20manage%20to%20master%20knowledge%20uniformly%20across%20imbalanced%20distributions%2C%20a%0Afeat%20attributed%20to%20their%20diligent%20practices%20of%20reviewing%2C%20summarizing%2C%20and%0Acorrecting%20errors.%20Motivated%20by%20this%20learning%20process%2C%20we%20propose%20a%20novel%0Alearning%20paradigm%2C%20called%20reflecting%20learning%2C%20in%20handling%20long-tail%0Arecognition.%20Our%20method%20integrates%20three%20processes%20for%20reviewing%20past%0Apredictions%20during%20training%2C%20summarizing%20and%20leveraging%20the%20feature%20relation%0Aacross%20classes%2C%20and%20correcting%20gradient%20conflict%20for%20loss%20functions.%20These%0Adesigns%20are%20lightweight%20enough%20to%20plug%20and%20play%20with%20existing%20long-tail%0Alearning%20methods%2C%20achieving%20state-of-the-art%20performance%20in%20popular%20long-tail%0Avisual%20benchmarks.%20The%20experimental%20results%20highlight%20the%20great%20potential%20of%0Areflecting%20learning%20in%20dealing%20with%20long-tail%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLTRL%253A%2520Boosting%2520Long-tail%2520Recognition%2520via%2520Reflective%2520Learning%26entry.906535625%3DQihao%2520Zhao%2520and%2520Yalun%2520Dai%2520and%2520Shen%2520Lin%2520and%2520Wei%2520Hu%2520and%2520Fan%2520Zhang%2520and%2520Jun%2520Liu%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520where%2520knowledge%2520distributions%2520exhibit%2520long-tail.%250AHumans%2520manage%2520to%2520master%2520knowledge%2520uniformly%2520across%2520imbalanced%2520distributions%252C%2520a%250Afeat%2520attributed%2520to%2520their%2520diligent%2520practices%2520of%2520reviewing%252C%2520summarizing%252C%2520and%250Acorrecting%2520errors.%2520Motivated%2520by%2520this%2520learning%2520process%252C%2520we%2520propose%2520a%2520novel%250Alearning%2520paradigm%252C%2520called%2520reflecting%2520learning%252C%2520in%2520handling%2520long-tail%250Arecognition.%2520Our%2520method%2520integrates%2520three%2520processes%2520for%2520reviewing%2520past%250Apredictions%2520during%2520training%252C%2520summarizing%2520and%2520leveraging%2520the%2520feature%2520relation%250Aacross%2520classes%252C%2520and%2520correcting%2520gradient%2520conflict%2520for%2520loss%2520functions.%2520These%250Adesigns%2520are%2520lightweight%2520enough%2520to%2520plug%2520and%2520play%2520with%2520existing%2520long-tail%250Alearning%2520methods%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520popular%2520long-tail%250Avisual%2520benchmarks.%2520The%2520experimental%2520results%2520highlight%2520the%2520great%2520potential%2520of%250Areflecting%2520learning%2520in%2520dealing%2520with%2520long-tail%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTRL%3A%20Boosting%20Long-tail%20Recognition%20via%20Reflective%20Learning&entry.906535625=Qihao%20Zhao%20and%20Yalun%20Dai%20and%20Shen%20Lin%20and%20Wei%20Hu%20and%20Fan%20Zhang%20and%20Jun%20Liu&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20where%20knowledge%20distributions%20exhibit%20long-tail.%0AHumans%20manage%20to%20master%20knowledge%20uniformly%20across%20imbalanced%20distributions%2C%20a%0Afeat%20attributed%20to%20their%20diligent%20practices%20of%20reviewing%2C%20summarizing%2C%20and%0Acorrecting%20errors.%20Motivated%20by%20this%20learning%20process%2C%20we%20propose%20a%20novel%0Alearning%20paradigm%2C%20called%20reflecting%20learning%2C%20in%20handling%20long-tail%0Arecognition.%20Our%20method%20integrates%20three%20processes%20for%20reviewing%20past%0Apredictions%20during%20training%2C%20summarizing%20and%20leveraging%20the%20feature%20relation%0Aacross%20classes%2C%20and%20correcting%20gradient%20conflict%20for%20loss%20functions.%20These%0Adesigns%20are%20lightweight%20enough%20to%20plug%20and%20play%20with%20existing%20long-tail%0Alearning%20methods%2C%20achieving%20state-of-the-art%20performance%20in%20popular%20long-tail%0Avisual%20benchmarks.%20The%20experimental%20results%20highlight%20the%20great%20potential%20of%0Areflecting%20learning%20in%20dealing%20with%20long-tail%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12568v1&entry.124074799=Read"},
{"title": "Beyond Viewpoint: Robust 3D Object Recognition under Arbitrary Views\n  through Joint Multi-Part Representation", "author": "Linlong Fan and Ye Huang and Yanqi Ge and Wen Li and Lixin Duan", "abstract": "  Existing view-based methods excel at recognizing 3D objects from predefined\nviewpoints, but their exploration of recognition under arbitrary views is\nlimited. This is a challenging and realistic setting because each object has\ndifferent viewpoint positions and quantities, and their poses are not aligned.\nHowever, most view-based methods, which aggregate multiple view features to\nobtain a global feature representation, hard to address 3D object recognition\nunder arbitrary views. Due to the unaligned inputs from arbitrary views, it is\nchallenging to robustly aggregate features, leading to performance degradation.\nIn this paper, we introduce a novel Part-aware Network (PANet), which is a\npart-based representation, to address these issues. This part-based\nrepresentation aims to localize and understand different parts of 3D objects,\nsuch as airplane wings and tails. It has properties such as viewpoint\ninvariance and rotation robustness, which give it an advantage in addressing\nthe 3D object recognition problem under arbitrary views. Our results on\nbenchmark datasets clearly demonstrate that our proposed method outperforms\nexisting view-based aggregation baselines for the task of 3D object recognition\nunder arbitrary views, even surpassing most fixed viewpoint methods.\n", "link": "http://arxiv.org/abs/2407.03842v2", "date": "2024-07-17", "relevancy": 2.918, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6048}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5835}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Viewpoint%3A%20Robust%203D%20Object%20Recognition%20under%20Arbitrary%20Views%0A%20%20through%20Joint%20Multi-Part%20Representation&body=Title%3A%20Beyond%20Viewpoint%3A%20Robust%203D%20Object%20Recognition%20under%20Arbitrary%20Views%0A%20%20through%20Joint%20Multi-Part%20Representation%0AAuthor%3A%20Linlong%20Fan%20and%20Ye%20Huang%20and%20Yanqi%20Ge%20and%20Wen%20Li%20and%20Lixin%20Duan%0AAbstract%3A%20%20%20Existing%20view-based%20methods%20excel%20at%20recognizing%203D%20objects%20from%20predefined%0Aviewpoints%2C%20but%20their%20exploration%20of%20recognition%20under%20arbitrary%20views%20is%0Alimited.%20This%20is%20a%20challenging%20and%20realistic%20setting%20because%20each%20object%20has%0Adifferent%20viewpoint%20positions%20and%20quantities%2C%20and%20their%20poses%20are%20not%20aligned.%0AHowever%2C%20most%20view-based%20methods%2C%20which%20aggregate%20multiple%20view%20features%20to%0Aobtain%20a%20global%20feature%20representation%2C%20hard%20to%20address%203D%20object%20recognition%0Aunder%20arbitrary%20views.%20Due%20to%20the%20unaligned%20inputs%20from%20arbitrary%20views%2C%20it%20is%0Achallenging%20to%20robustly%20aggregate%20features%2C%20leading%20to%20performance%20degradation.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20Part-aware%20Network%20%28PANet%29%2C%20which%20is%20a%0Apart-based%20representation%2C%20to%20address%20these%20issues.%20This%20part-based%0Arepresentation%20aims%20to%20localize%20and%20understand%20different%20parts%20of%203D%20objects%2C%0Asuch%20as%20airplane%20wings%20and%20tails.%20It%20has%20properties%20such%20as%20viewpoint%0Ainvariance%20and%20rotation%20robustness%2C%20which%20give%20it%20an%20advantage%20in%20addressing%0Athe%203D%20object%20recognition%20problem%20under%20arbitrary%20views.%20Our%20results%20on%0Abenchmark%20datasets%20clearly%20demonstrate%20that%20our%20proposed%20method%20outperforms%0Aexisting%20view-based%20aggregation%20baselines%20for%20the%20task%20of%203D%20object%20recognition%0Aunder%20arbitrary%20views%2C%20even%20surpassing%20most%20fixed%20viewpoint%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03842v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Viewpoint%253A%2520Robust%25203D%2520Object%2520Recognition%2520under%2520Arbitrary%2520Views%250A%2520%2520through%2520Joint%2520Multi-Part%2520Representation%26entry.906535625%3DLinlong%2520Fan%2520and%2520Ye%2520Huang%2520and%2520Yanqi%2520Ge%2520and%2520Wen%2520Li%2520and%2520Lixin%2520Duan%26entry.1292438233%3D%2520%2520Existing%2520view-based%2520methods%2520excel%2520at%2520recognizing%25203D%2520objects%2520from%2520predefined%250Aviewpoints%252C%2520but%2520their%2520exploration%2520of%2520recognition%2520under%2520arbitrary%2520views%2520is%250Alimited.%2520This%2520is%2520a%2520challenging%2520and%2520realistic%2520setting%2520because%2520each%2520object%2520has%250Adifferent%2520viewpoint%2520positions%2520and%2520quantities%252C%2520and%2520their%2520poses%2520are%2520not%2520aligned.%250AHowever%252C%2520most%2520view-based%2520methods%252C%2520which%2520aggregate%2520multiple%2520view%2520features%2520to%250Aobtain%2520a%2520global%2520feature%2520representation%252C%2520hard%2520to%2520address%25203D%2520object%2520recognition%250Aunder%2520arbitrary%2520views.%2520Due%2520to%2520the%2520unaligned%2520inputs%2520from%2520arbitrary%2520views%252C%2520it%2520is%250Achallenging%2520to%2520robustly%2520aggregate%2520features%252C%2520leading%2520to%2520performance%2520degradation.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520Part-aware%2520Network%2520%2528PANet%2529%252C%2520which%2520is%2520a%250Apart-based%2520representation%252C%2520to%2520address%2520these%2520issues.%2520This%2520part-based%250Arepresentation%2520aims%2520to%2520localize%2520and%2520understand%2520different%2520parts%2520of%25203D%2520objects%252C%250Asuch%2520as%2520airplane%2520wings%2520and%2520tails.%2520It%2520has%2520properties%2520such%2520as%2520viewpoint%250Ainvariance%2520and%2520rotation%2520robustness%252C%2520which%2520give%2520it%2520an%2520advantage%2520in%2520addressing%250Athe%25203D%2520object%2520recognition%2520problem%2520under%2520arbitrary%2520views.%2520Our%2520results%2520on%250Abenchmark%2520datasets%2520clearly%2520demonstrate%2520that%2520our%2520proposed%2520method%2520outperforms%250Aexisting%2520view-based%2520aggregation%2520baselines%2520for%2520the%2520task%2520of%25203D%2520object%2520recognition%250Aunder%2520arbitrary%2520views%252C%2520even%2520surpassing%2520most%2520fixed%2520viewpoint%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03842v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Viewpoint%3A%20Robust%203D%20Object%20Recognition%20under%20Arbitrary%20Views%0A%20%20through%20Joint%20Multi-Part%20Representation&entry.906535625=Linlong%20Fan%20and%20Ye%20Huang%20and%20Yanqi%20Ge%20and%20Wen%20Li%20and%20Lixin%20Duan&entry.1292438233=%20%20Existing%20view-based%20methods%20excel%20at%20recognizing%203D%20objects%20from%20predefined%0Aviewpoints%2C%20but%20their%20exploration%20of%20recognition%20under%20arbitrary%20views%20is%0Alimited.%20This%20is%20a%20challenging%20and%20realistic%20setting%20because%20each%20object%20has%0Adifferent%20viewpoint%20positions%20and%20quantities%2C%20and%20their%20poses%20are%20not%20aligned.%0AHowever%2C%20most%20view-based%20methods%2C%20which%20aggregate%20multiple%20view%20features%20to%0Aobtain%20a%20global%20feature%20representation%2C%20hard%20to%20address%203D%20object%20recognition%0Aunder%20arbitrary%20views.%20Due%20to%20the%20unaligned%20inputs%20from%20arbitrary%20views%2C%20it%20is%0Achallenging%20to%20robustly%20aggregate%20features%2C%20leading%20to%20performance%20degradation.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20Part-aware%20Network%20%28PANet%29%2C%20which%20is%20a%0Apart-based%20representation%2C%20to%20address%20these%20issues.%20This%20part-based%0Arepresentation%20aims%20to%20localize%20and%20understand%20different%20parts%20of%203D%20objects%2C%0Asuch%20as%20airplane%20wings%20and%20tails.%20It%20has%20properties%20such%20as%20viewpoint%0Ainvariance%20and%20rotation%20robustness%2C%20which%20give%20it%20an%20advantage%20in%20addressing%0Athe%203D%20object%20recognition%20problem%20under%20arbitrary%20views.%20Our%20results%20on%0Abenchmark%20datasets%20clearly%20demonstrate%20that%20our%20proposed%20method%20outperforms%0Aexisting%20view-based%20aggregation%20baselines%20for%20the%20task%20of%203D%20object%20recognition%0Aunder%20arbitrary%20views%2C%20even%20surpassing%20most%20fixed%20viewpoint%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03842v2&entry.124074799=Read"},
{"title": "Towards Understanding Unsafe Video Generation", "author": "Yan Pang and Aiping Xiong and Yang Zhang and Tianhao Wang", "abstract": "  Video generation models (VGMs) have demonstrated the capability to synthesize\nhigh-quality output. It is important to understand their potential to produce\nunsafe content, such as violent or terrifying videos. In this work, we provide\na comprehensive understanding of unsafe video generation.\n  First, to confirm the possibility that these models could indeed generate\nunsafe videos, we choose unsafe content generation prompts collected from 4chan\nand Lexica, and three open-source SOTA VGMs to generate unsafe videos. After\nfiltering out duplicates and poorly generated content, we created an initial\nset of 2112 unsafe videos from an original pool of 5607 videos. Through\nclustering and thematic coding analysis of these generated videos, we identify\n5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic,\nViolent/Bloody, and Political. With IRB approval, we then recruit online\nparticipants to help label the generated videos. Based on the annotations\nsubmitted by 403 participants, we identified 937 unsafe videos from the initial\nvideo set. With the labeled information and the corresponding prompts, we\ncreated the first dataset of unsafe videos generated by VGMs.\n  We then study possible defense mechanisms to prevent the generation of unsafe\nvideos. Existing defense methods in image generation focus on filtering either\ninput prompt or output results. We propose a new approach called Latent\nVariable Defense (LVD), which works within the model's internal sampling\nprocess. LVD can achieve 0.90 defense accuracy while reducing time and\ncomputing resources by 10x when sampling a large number of unsafe prompts.\n", "link": "http://arxiv.org/abs/2407.12581v1", "date": "2024-07-17", "relevancy": 2.8874, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5974}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.595}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20Unsafe%20Video%20Generation&body=Title%3A%20Towards%20Understanding%20Unsafe%20Video%20Generation%0AAuthor%3A%20Yan%20Pang%20and%20Aiping%20Xiong%20and%20Yang%20Zhang%20and%20Tianhao%20Wang%0AAbstract%3A%20%20%20Video%20generation%20models%20%28VGMs%29%20have%20demonstrated%20the%20capability%20to%20synthesize%0Ahigh-quality%20output.%20It%20is%20important%20to%20understand%20their%20potential%20to%20produce%0Aunsafe%20content%2C%20such%20as%20violent%20or%20terrifying%20videos.%20In%20this%20work%2C%20we%20provide%0Aa%20comprehensive%20understanding%20of%20unsafe%20video%20generation.%0A%20%20First%2C%20to%20confirm%20the%20possibility%20that%20these%20models%20could%20indeed%20generate%0Aunsafe%20videos%2C%20we%20choose%20unsafe%20content%20generation%20prompts%20collected%20from%204chan%0Aand%20Lexica%2C%20and%20three%20open-source%20SOTA%20VGMs%20to%20generate%20unsafe%20videos.%20After%0Afiltering%20out%20duplicates%20and%20poorly%20generated%20content%2C%20we%20created%20an%20initial%0Aset%20of%202112%20unsafe%20videos%20from%20an%20original%20pool%20of%205607%20videos.%20Through%0Aclustering%20and%20thematic%20coding%20analysis%20of%20these%20generated%20videos%2C%20we%20identify%0A5%20unsafe%20video%20categories%3A%20Distorted/Weird%2C%20Terrifying%2C%20Pornographic%2C%0AViolent/Bloody%2C%20and%20Political.%20With%20IRB%20approval%2C%20we%20then%20recruit%20online%0Aparticipants%20to%20help%20label%20the%20generated%20videos.%20Based%20on%20the%20annotations%0Asubmitted%20by%20403%20participants%2C%20we%20identified%20937%20unsafe%20videos%20from%20the%20initial%0Avideo%20set.%20With%20the%20labeled%20information%20and%20the%20corresponding%20prompts%2C%20we%0Acreated%20the%20first%20dataset%20of%20unsafe%20videos%20generated%20by%20VGMs.%0A%20%20We%20then%20study%20possible%20defense%20mechanisms%20to%20prevent%20the%20generation%20of%20unsafe%0Avideos.%20Existing%20defense%20methods%20in%20image%20generation%20focus%20on%20filtering%20either%0Ainput%20prompt%20or%20output%20results.%20We%20propose%20a%20new%20approach%20called%20Latent%0AVariable%20Defense%20%28LVD%29%2C%20which%20works%20within%20the%20model%27s%20internal%20sampling%0Aprocess.%20LVD%20can%20achieve%200.90%20defense%20accuracy%20while%20reducing%20time%20and%0Acomputing%20resources%20by%2010x%20when%20sampling%20a%20large%20number%20of%20unsafe%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520Unsafe%2520Video%2520Generation%26entry.906535625%3DYan%2520Pang%2520and%2520Aiping%2520Xiong%2520and%2520Yang%2520Zhang%2520and%2520Tianhao%2520Wang%26entry.1292438233%3D%2520%2520Video%2520generation%2520models%2520%2528VGMs%2529%2520have%2520demonstrated%2520the%2520capability%2520to%2520synthesize%250Ahigh-quality%2520output.%2520It%2520is%2520important%2520to%2520understand%2520their%2520potential%2520to%2520produce%250Aunsafe%2520content%252C%2520such%2520as%2520violent%2520or%2520terrifying%2520videos.%2520In%2520this%2520work%252C%2520we%2520provide%250Aa%2520comprehensive%2520understanding%2520of%2520unsafe%2520video%2520generation.%250A%2520%2520First%252C%2520to%2520confirm%2520the%2520possibility%2520that%2520these%2520models%2520could%2520indeed%2520generate%250Aunsafe%2520videos%252C%2520we%2520choose%2520unsafe%2520content%2520generation%2520prompts%2520collected%2520from%25204chan%250Aand%2520Lexica%252C%2520and%2520three%2520open-source%2520SOTA%2520VGMs%2520to%2520generate%2520unsafe%2520videos.%2520After%250Afiltering%2520out%2520duplicates%2520and%2520poorly%2520generated%2520content%252C%2520we%2520created%2520an%2520initial%250Aset%2520of%25202112%2520unsafe%2520videos%2520from%2520an%2520original%2520pool%2520of%25205607%2520videos.%2520Through%250Aclustering%2520and%2520thematic%2520coding%2520analysis%2520of%2520these%2520generated%2520videos%252C%2520we%2520identify%250A5%2520unsafe%2520video%2520categories%253A%2520Distorted/Weird%252C%2520Terrifying%252C%2520Pornographic%252C%250AViolent/Bloody%252C%2520and%2520Political.%2520With%2520IRB%2520approval%252C%2520we%2520then%2520recruit%2520online%250Aparticipants%2520to%2520help%2520label%2520the%2520generated%2520videos.%2520Based%2520on%2520the%2520annotations%250Asubmitted%2520by%2520403%2520participants%252C%2520we%2520identified%2520937%2520unsafe%2520videos%2520from%2520the%2520initial%250Avideo%2520set.%2520With%2520the%2520labeled%2520information%2520and%2520the%2520corresponding%2520prompts%252C%2520we%250Acreated%2520the%2520first%2520dataset%2520of%2520unsafe%2520videos%2520generated%2520by%2520VGMs.%250A%2520%2520We%2520then%2520study%2520possible%2520defense%2520mechanisms%2520to%2520prevent%2520the%2520generation%2520of%2520unsafe%250Avideos.%2520Existing%2520defense%2520methods%2520in%2520image%2520generation%2520focus%2520on%2520filtering%2520either%250Ainput%2520prompt%2520or%2520output%2520results.%2520We%2520propose%2520a%2520new%2520approach%2520called%2520Latent%250AVariable%2520Defense%2520%2528LVD%2529%252C%2520which%2520works%2520within%2520the%2520model%2527s%2520internal%2520sampling%250Aprocess.%2520LVD%2520can%2520achieve%25200.90%2520defense%2520accuracy%2520while%2520reducing%2520time%2520and%250Acomputing%2520resources%2520by%252010x%2520when%2520sampling%2520a%2520large%2520number%2520of%2520unsafe%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20Unsafe%20Video%20Generation&entry.906535625=Yan%20Pang%20and%20Aiping%20Xiong%20and%20Yang%20Zhang%20and%20Tianhao%20Wang&entry.1292438233=%20%20Video%20generation%20models%20%28VGMs%29%20have%20demonstrated%20the%20capability%20to%20synthesize%0Ahigh-quality%20output.%20It%20is%20important%20to%20understand%20their%20potential%20to%20produce%0Aunsafe%20content%2C%20such%20as%20violent%20or%20terrifying%20videos.%20In%20this%20work%2C%20we%20provide%0Aa%20comprehensive%20understanding%20of%20unsafe%20video%20generation.%0A%20%20First%2C%20to%20confirm%20the%20possibility%20that%20these%20models%20could%20indeed%20generate%0Aunsafe%20videos%2C%20we%20choose%20unsafe%20content%20generation%20prompts%20collected%20from%204chan%0Aand%20Lexica%2C%20and%20three%20open-source%20SOTA%20VGMs%20to%20generate%20unsafe%20videos.%20After%0Afiltering%20out%20duplicates%20and%20poorly%20generated%20content%2C%20we%20created%20an%20initial%0Aset%20of%202112%20unsafe%20videos%20from%20an%20original%20pool%20of%205607%20videos.%20Through%0Aclustering%20and%20thematic%20coding%20analysis%20of%20these%20generated%20videos%2C%20we%20identify%0A5%20unsafe%20video%20categories%3A%20Distorted/Weird%2C%20Terrifying%2C%20Pornographic%2C%0AViolent/Bloody%2C%20and%20Political.%20With%20IRB%20approval%2C%20we%20then%20recruit%20online%0Aparticipants%20to%20help%20label%20the%20generated%20videos.%20Based%20on%20the%20annotations%0Asubmitted%20by%20403%20participants%2C%20we%20identified%20937%20unsafe%20videos%20from%20the%20initial%0Avideo%20set.%20With%20the%20labeled%20information%20and%20the%20corresponding%20prompts%2C%20we%0Acreated%20the%20first%20dataset%20of%20unsafe%20videos%20generated%20by%20VGMs.%0A%20%20We%20then%20study%20possible%20defense%20mechanisms%20to%20prevent%20the%20generation%20of%20unsafe%0Avideos.%20Existing%20defense%20methods%20in%20image%20generation%20focus%20on%20filtering%20either%0Ainput%20prompt%20or%20output%20results.%20We%20propose%20a%20new%20approach%20called%20Latent%0AVariable%20Defense%20%28LVD%29%2C%20which%20works%20within%20the%20model%27s%20internal%20sampling%0Aprocess.%20LVD%20can%20achieve%200.90%20defense%20accuracy%20while%20reducing%20time%20and%0Acomputing%20resources%20by%2010x%20when%20sampling%20a%20large%20number%20of%20unsafe%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12581v1&entry.124074799=Read"},
{"title": "TransCAD: A Hierarchical Transformer for CAD Sequence Inference from\n  Point Clouds", "author": "Elona Dupont and Kseniya Cherenkova and Dimitrios Mallis and Gleb Gusev and Anis Kacem and Djamila Aouada", "abstract": "  3D reverse engineering, in which a CAD model is inferred given a 3D scan of a\nphysical object, is a research direction that offers many promising practical\napplications. This paper proposes TransCAD, an end-to-end transformer-based\narchitecture that predicts the CAD sequence from a point cloud. TransCAD\nleverages the structure of CAD sequences by using a hierarchical learning\nstrategy. A loop refiner is also introduced to regress sketch primitive\nparameters. Rigorous experimentation on the DeepCAD and Fusion360 datasets show\nthat TransCAD achieves state-of-the-art results. The result analysis is\nsupported with a proposed metric for CAD sequence, the mean Average Precision\nof CAD Sequence, that addresses the limitations of existing metrics.\n", "link": "http://arxiv.org/abs/2407.12702v1", "date": "2024-07-17", "relevancy": 2.877, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5889}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5889}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransCAD%3A%20A%20Hierarchical%20Transformer%20for%20CAD%20Sequence%20Inference%20from%0A%20%20Point%20Clouds&body=Title%3A%20TransCAD%3A%20A%20Hierarchical%20Transformer%20for%20CAD%20Sequence%20Inference%20from%0A%20%20Point%20Clouds%0AAuthor%3A%20Elona%20Dupont%20and%20Kseniya%20Cherenkova%20and%20Dimitrios%20Mallis%20and%20Gleb%20Gusev%20and%20Anis%20Kacem%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%203D%20reverse%20engineering%2C%20in%20which%20a%20CAD%20model%20is%20inferred%20given%20a%203D%20scan%20of%20a%0Aphysical%20object%2C%20is%20a%20research%20direction%20that%20offers%20many%20promising%20practical%0Aapplications.%20This%20paper%20proposes%20TransCAD%2C%20an%20end-to-end%20transformer-based%0Aarchitecture%20that%20predicts%20the%20CAD%20sequence%20from%20a%20point%20cloud.%20TransCAD%0Aleverages%20the%20structure%20of%20CAD%20sequences%20by%20using%20a%20hierarchical%20learning%0Astrategy.%20A%20loop%20refiner%20is%20also%20introduced%20to%20regress%20sketch%20primitive%0Aparameters.%20Rigorous%20experimentation%20on%20the%20DeepCAD%20and%20Fusion360%20datasets%20show%0Athat%20TransCAD%20achieves%20state-of-the-art%20results.%20The%20result%20analysis%20is%0Asupported%20with%20a%20proposed%20metric%20for%20CAD%20sequence%2C%20the%20mean%20Average%20Precision%0Aof%20CAD%20Sequence%2C%20that%20addresses%20the%20limitations%20of%20existing%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransCAD%253A%2520A%2520Hierarchical%2520Transformer%2520for%2520CAD%2520Sequence%2520Inference%2520from%250A%2520%2520Point%2520Clouds%26entry.906535625%3DElona%2520Dupont%2520and%2520Kseniya%2520Cherenkova%2520and%2520Dimitrios%2520Mallis%2520and%2520Gleb%2520Gusev%2520and%2520Anis%2520Kacem%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%25203D%2520reverse%2520engineering%252C%2520in%2520which%2520a%2520CAD%2520model%2520is%2520inferred%2520given%2520a%25203D%2520scan%2520of%2520a%250Aphysical%2520object%252C%2520is%2520a%2520research%2520direction%2520that%2520offers%2520many%2520promising%2520practical%250Aapplications.%2520This%2520paper%2520proposes%2520TransCAD%252C%2520an%2520end-to-end%2520transformer-based%250Aarchitecture%2520that%2520predicts%2520the%2520CAD%2520sequence%2520from%2520a%2520point%2520cloud.%2520TransCAD%250Aleverages%2520the%2520structure%2520of%2520CAD%2520sequences%2520by%2520using%2520a%2520hierarchical%2520learning%250Astrategy.%2520A%2520loop%2520refiner%2520is%2520also%2520introduced%2520to%2520regress%2520sketch%2520primitive%250Aparameters.%2520Rigorous%2520experimentation%2520on%2520the%2520DeepCAD%2520and%2520Fusion360%2520datasets%2520show%250Athat%2520TransCAD%2520achieves%2520state-of-the-art%2520results.%2520The%2520result%2520analysis%2520is%250Asupported%2520with%2520a%2520proposed%2520metric%2520for%2520CAD%2520sequence%252C%2520the%2520mean%2520Average%2520Precision%250Aof%2520CAD%2520Sequence%252C%2520that%2520addresses%2520the%2520limitations%2520of%2520existing%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransCAD%3A%20A%20Hierarchical%20Transformer%20for%20CAD%20Sequence%20Inference%20from%0A%20%20Point%20Clouds&entry.906535625=Elona%20Dupont%20and%20Kseniya%20Cherenkova%20and%20Dimitrios%20Mallis%20and%20Gleb%20Gusev%20and%20Anis%20Kacem%20and%20Djamila%20Aouada&entry.1292438233=%20%203D%20reverse%20engineering%2C%20in%20which%20a%20CAD%20model%20is%20inferred%20given%20a%203D%20scan%20of%20a%0Aphysical%20object%2C%20is%20a%20research%20direction%20that%20offers%20many%20promising%20practical%0Aapplications.%20This%20paper%20proposes%20TransCAD%2C%20an%20end-to-end%20transformer-based%0Aarchitecture%20that%20predicts%20the%20CAD%20sequence%20from%20a%20point%20cloud.%20TransCAD%0Aleverages%20the%20structure%20of%20CAD%20sequences%20by%20using%20a%20hierarchical%20learning%0Astrategy.%20A%20loop%20refiner%20is%20also%20introduced%20to%20regress%20sketch%20primitive%0Aparameters.%20Rigorous%20experimentation%20on%20the%20DeepCAD%20and%20Fusion360%20datasets%20show%0Athat%20TransCAD%20achieves%20state-of-the-art%20results.%20The%20result%20analysis%20is%0Asupported%20with%20a%20proposed%20metric%20for%20CAD%20sequence%2C%20the%20mean%20Average%20Precision%0Aof%20CAD%20Sequence%2C%20that%20addresses%20the%20limitations%20of%20existing%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12702v1&entry.124074799=Read"},
{"title": "IMAGDressing-v1: Customizable Virtual Dressing", "author": "Fei Shen and Xin Jiang and Xin He and Hu Ye and Cong Wang and Xiaoyu Du and Zechao Li and Jinghui Tang", "abstract": "  Latest advances have achieved realistic virtual try-on (VTON) through\nlocalized garment inpainting using latent diffusion models, significantly\nenhancing consumers' online shopping experience. However, existing VTON\ntechnologies neglect the need for merchants to showcase garments\ncomprehensively, including flexible control over garments, optional faces,\nposes, and scenes. To address this issue, we define a virtual dressing (VD)\ntask focused on generating freely editable human images with fixed garments and\noptional conditions. Meanwhile, we design a comprehensive affinity metric index\n(CAMI) to evaluate the consistency between generated images and reference\ngarments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet\nthat captures semantic features from CLIP and texture features from VAE. We\npresent a hybrid attention module, including a frozen self-attention and a\ntrainable cross-attention, to integrate garment features from the garment UNet\ninto a frozen denoising UNet, ensuring users can control different scenes\nthrough text. IMAGDressing-v1 can be combined with other extension plugins,\nsuch as ControlNet and IP-Adapter, to enhance the diversity and controllability\nof generated images. Furthermore, to address the lack of data, we release the\ninteractive garment pairing (IGPair) dataset, containing over 300,000 pairs of\nclothing and dressed images, and establish a standard pipeline for data\nassembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves\nstate-of-the-art human image synthesis performance under various controlled\nconditions. The code and model will be available at\nhttps://github.com/muzishen/IMAGDressing.\n", "link": "http://arxiv.org/abs/2407.12705v1", "date": "2024-07-17", "relevancy": 2.8371, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.713}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7087}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.7057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMAGDressing-v1%3A%20Customizable%20Virtual%20Dressing&body=Title%3A%20IMAGDressing-v1%3A%20Customizable%20Virtual%20Dressing%0AAuthor%3A%20Fei%20Shen%20and%20Xin%20Jiang%20and%20Xin%20He%20and%20Hu%20Ye%20and%20Cong%20Wang%20and%20Xiaoyu%20Du%20and%20Zechao%20Li%20and%20Jinghui%20Tang%0AAbstract%3A%20%20%20Latest%20advances%20have%20achieved%20realistic%20virtual%20try-on%20%28VTON%29%20through%0Alocalized%20garment%20inpainting%20using%20latent%20diffusion%20models%2C%20significantly%0Aenhancing%20consumers%27%20online%20shopping%20experience.%20However%2C%20existing%20VTON%0Atechnologies%20neglect%20the%20need%20for%20merchants%20to%20showcase%20garments%0Acomprehensively%2C%20including%20flexible%20control%20over%20garments%2C%20optional%20faces%2C%0Aposes%2C%20and%20scenes.%20To%20address%20this%20issue%2C%20we%20define%20a%20virtual%20dressing%20%28VD%29%0Atask%20focused%20on%20generating%20freely%20editable%20human%20images%20with%20fixed%20garments%20and%0Aoptional%20conditions.%20Meanwhile%2C%20we%20design%20a%20comprehensive%20affinity%20metric%20index%0A%28CAMI%29%20to%20evaluate%20the%20consistency%20between%20generated%20images%20and%20reference%0Agarments.%20Then%2C%20we%20propose%20IMAGDressing-v1%2C%20which%20incorporates%20a%20garment%20UNet%0Athat%20captures%20semantic%20features%20from%20CLIP%20and%20texture%20features%20from%20VAE.%20We%0Apresent%20a%20hybrid%20attention%20module%2C%20including%20a%20frozen%20self-attention%20and%20a%0Atrainable%20cross-attention%2C%20to%20integrate%20garment%20features%20from%20the%20garment%20UNet%0Ainto%20a%20frozen%20denoising%20UNet%2C%20ensuring%20users%20can%20control%20different%20scenes%0Athrough%20text.%20IMAGDressing-v1%20can%20be%20combined%20with%20other%20extension%20plugins%2C%0Asuch%20as%20ControlNet%20and%20IP-Adapter%2C%20to%20enhance%20the%20diversity%20and%20controllability%0Aof%20generated%20images.%20Furthermore%2C%20to%20address%20the%20lack%20of%20data%2C%20we%20release%20the%0Ainteractive%20garment%20pairing%20%28IGPair%29%20dataset%2C%20containing%20over%20300%2C000%20pairs%20of%0Aclothing%20and%20dressed%20images%2C%20and%20establish%20a%20standard%20pipeline%20for%20data%0Aassembly.%20Extensive%20experiments%20demonstrate%20that%20our%20IMAGDressing-v1%20achieves%0Astate-of-the-art%20human%20image%20synthesis%20performance%20under%20various%20controlled%0Aconditions.%20The%20code%20and%20model%20will%20be%20available%20at%0Ahttps%3A//github.com/muzishen/IMAGDressing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMAGDressing-v1%253A%2520Customizable%2520Virtual%2520Dressing%26entry.906535625%3DFei%2520Shen%2520and%2520Xin%2520Jiang%2520and%2520Xin%2520He%2520and%2520Hu%2520Ye%2520and%2520Cong%2520Wang%2520and%2520Xiaoyu%2520Du%2520and%2520Zechao%2520Li%2520and%2520Jinghui%2520Tang%26entry.1292438233%3D%2520%2520Latest%2520advances%2520have%2520achieved%2520realistic%2520virtual%2520try-on%2520%2528VTON%2529%2520through%250Alocalized%2520garment%2520inpainting%2520using%2520latent%2520diffusion%2520models%252C%2520significantly%250Aenhancing%2520consumers%2527%2520online%2520shopping%2520experience.%2520However%252C%2520existing%2520VTON%250Atechnologies%2520neglect%2520the%2520need%2520for%2520merchants%2520to%2520showcase%2520garments%250Acomprehensively%252C%2520including%2520flexible%2520control%2520over%2520garments%252C%2520optional%2520faces%252C%250Aposes%252C%2520and%2520scenes.%2520To%2520address%2520this%2520issue%252C%2520we%2520define%2520a%2520virtual%2520dressing%2520%2528VD%2529%250Atask%2520focused%2520on%2520generating%2520freely%2520editable%2520human%2520images%2520with%2520fixed%2520garments%2520and%250Aoptional%2520conditions.%2520Meanwhile%252C%2520we%2520design%2520a%2520comprehensive%2520affinity%2520metric%2520index%250A%2528CAMI%2529%2520to%2520evaluate%2520the%2520consistency%2520between%2520generated%2520images%2520and%2520reference%250Agarments.%2520Then%252C%2520we%2520propose%2520IMAGDressing-v1%252C%2520which%2520incorporates%2520a%2520garment%2520UNet%250Athat%2520captures%2520semantic%2520features%2520from%2520CLIP%2520and%2520texture%2520features%2520from%2520VAE.%2520We%250Apresent%2520a%2520hybrid%2520attention%2520module%252C%2520including%2520a%2520frozen%2520self-attention%2520and%2520a%250Atrainable%2520cross-attention%252C%2520to%2520integrate%2520garment%2520features%2520from%2520the%2520garment%2520UNet%250Ainto%2520a%2520frozen%2520denoising%2520UNet%252C%2520ensuring%2520users%2520can%2520control%2520different%2520scenes%250Athrough%2520text.%2520IMAGDressing-v1%2520can%2520be%2520combined%2520with%2520other%2520extension%2520plugins%252C%250Asuch%2520as%2520ControlNet%2520and%2520IP-Adapter%252C%2520to%2520enhance%2520the%2520diversity%2520and%2520controllability%250Aof%2520generated%2520images.%2520Furthermore%252C%2520to%2520address%2520the%2520lack%2520of%2520data%252C%2520we%2520release%2520the%250Ainteractive%2520garment%2520pairing%2520%2528IGPair%2529%2520dataset%252C%2520containing%2520over%2520300%252C000%2520pairs%2520of%250Aclothing%2520and%2520dressed%2520images%252C%2520and%2520establish%2520a%2520standard%2520pipeline%2520for%2520data%250Aassembly.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520IMAGDressing-v1%2520achieves%250Astate-of-the-art%2520human%2520image%2520synthesis%2520performance%2520under%2520various%2520controlled%250Aconditions.%2520The%2520code%2520and%2520model%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/muzishen/IMAGDressing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMAGDressing-v1%3A%20Customizable%20Virtual%20Dressing&entry.906535625=Fei%20Shen%20and%20Xin%20Jiang%20and%20Xin%20He%20and%20Hu%20Ye%20and%20Cong%20Wang%20and%20Xiaoyu%20Du%20and%20Zechao%20Li%20and%20Jinghui%20Tang&entry.1292438233=%20%20Latest%20advances%20have%20achieved%20realistic%20virtual%20try-on%20%28VTON%29%20through%0Alocalized%20garment%20inpainting%20using%20latent%20diffusion%20models%2C%20significantly%0Aenhancing%20consumers%27%20online%20shopping%20experience.%20However%2C%20existing%20VTON%0Atechnologies%20neglect%20the%20need%20for%20merchants%20to%20showcase%20garments%0Acomprehensively%2C%20including%20flexible%20control%20over%20garments%2C%20optional%20faces%2C%0Aposes%2C%20and%20scenes.%20To%20address%20this%20issue%2C%20we%20define%20a%20virtual%20dressing%20%28VD%29%0Atask%20focused%20on%20generating%20freely%20editable%20human%20images%20with%20fixed%20garments%20and%0Aoptional%20conditions.%20Meanwhile%2C%20we%20design%20a%20comprehensive%20affinity%20metric%20index%0A%28CAMI%29%20to%20evaluate%20the%20consistency%20between%20generated%20images%20and%20reference%0Agarments.%20Then%2C%20we%20propose%20IMAGDressing-v1%2C%20which%20incorporates%20a%20garment%20UNet%0Athat%20captures%20semantic%20features%20from%20CLIP%20and%20texture%20features%20from%20VAE.%20We%0Apresent%20a%20hybrid%20attention%20module%2C%20including%20a%20frozen%20self-attention%20and%20a%0Atrainable%20cross-attention%2C%20to%20integrate%20garment%20features%20from%20the%20garment%20UNet%0Ainto%20a%20frozen%20denoising%20UNet%2C%20ensuring%20users%20can%20control%20different%20scenes%0Athrough%20text.%20IMAGDressing-v1%20can%20be%20combined%20with%20other%20extension%20plugins%2C%0Asuch%20as%20ControlNet%20and%20IP-Adapter%2C%20to%20enhance%20the%20diversity%20and%20controllability%0Aof%20generated%20images.%20Furthermore%2C%20to%20address%20the%20lack%20of%20data%2C%20we%20release%20the%0Ainteractive%20garment%20pairing%20%28IGPair%29%20dataset%2C%20containing%20over%20300%2C000%20pairs%20of%0Aclothing%20and%20dressed%20images%2C%20and%20establish%20a%20standard%20pipeline%20for%20data%0Aassembly.%20Extensive%20experiments%20demonstrate%20that%20our%20IMAGDressing-v1%20achieves%0Astate-of-the-art%20human%20image%20synthesis%20performance%20under%20various%20controlled%0Aconditions.%20The%20code%20and%20model%20will%20be%20available%20at%0Ahttps%3A//github.com/muzishen/IMAGDressing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12705v1&entry.124074799=Read"},
{"title": "Benchmarking Robust Self-Supervised Learning Across Diverse Downstream\n  Tasks", "author": "Antoni Kowalczuk and Jan Dubi\u0144ski and Atiyeh Ashari Ghomi and Yi Sui and George Stein and Jiapeng Wu and Jesse C. Cresswell and Franziska Boenisch and Adam Dziedzic", "abstract": "  Large-scale vision models have become integral in many applications due to\ntheir unprecedented performance and versatility across downstream tasks.\nHowever, the robustness of these foundation models has primarily been explored\nfor a single task, namely image classification. The vulnerability of other\ncommon vision tasks, such as semantic segmentation and depth estimation,\nremains largely unknown. We present a comprehensive empirical evaluation of the\nadversarial robustness of self-supervised vision encoders across multiple\ndownstream tasks. Our attacks operate in the encoder embedding space and at the\ndownstream task output level. In both cases, current state-of-the-art\nadversarial fine-tuning techniques tested only for classification significantly\ndegrade clean and robust performance on other tasks. Since the purpose of a\nfoundation model is to cater to multiple applications at once, our findings\nreveal the need to enhance encoder robustness more broadly. %We discuss\npotential strategies for more robust foundation vision models across diverse\ndownstream tasks. Our code is available at\n$\\href{https://github.com/layer6ai-labs/ssl-robustness}{github.com/layer6ai-labs/ssl-robustness}$.\n", "link": "http://arxiv.org/abs/2407.12588v1", "date": "2024-07-17", "relevancy": 2.7339, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5777}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5322}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Robust%20Self-Supervised%20Learning%20Across%20Diverse%20Downstream%0A%20%20Tasks&body=Title%3A%20Benchmarking%20Robust%20Self-Supervised%20Learning%20Across%20Diverse%20Downstream%0A%20%20Tasks%0AAuthor%3A%20Antoni%20Kowalczuk%20and%20Jan%20Dubi%C5%84ski%20and%20Atiyeh%20Ashari%20Ghomi%20and%20Yi%20Sui%20and%20George%20Stein%20and%20Jiapeng%20Wu%20and%20Jesse%20C.%20Cresswell%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic%0AAbstract%3A%20%20%20Large-scale%20vision%20models%20have%20become%20integral%20in%20many%20applications%20due%20to%0Atheir%20unprecedented%20performance%20and%20versatility%20across%20downstream%20tasks.%0AHowever%2C%20the%20robustness%20of%20these%20foundation%20models%20has%20primarily%20been%20explored%0Afor%20a%20single%20task%2C%20namely%20image%20classification.%20The%20vulnerability%20of%20other%0Acommon%20vision%20tasks%2C%20such%20as%20semantic%20segmentation%20and%20depth%20estimation%2C%0Aremains%20largely%20unknown.%20We%20present%20a%20comprehensive%20empirical%20evaluation%20of%20the%0Aadversarial%20robustness%20of%20self-supervised%20vision%20encoders%20across%20multiple%0Adownstream%20tasks.%20Our%20attacks%20operate%20in%20the%20encoder%20embedding%20space%20and%20at%20the%0Adownstream%20task%20output%20level.%20In%20both%20cases%2C%20current%20state-of-the-art%0Aadversarial%20fine-tuning%20techniques%20tested%20only%20for%20classification%20significantly%0Adegrade%20clean%20and%20robust%20performance%20on%20other%20tasks.%20Since%20the%20purpose%20of%20a%0Afoundation%20model%20is%20to%20cater%20to%20multiple%20applications%20at%20once%2C%20our%20findings%0Areveal%20the%20need%20to%20enhance%20encoder%20robustness%20more%20broadly.%20%25We%20discuss%0Apotential%20strategies%20for%20more%20robust%20foundation%20vision%20models%20across%20diverse%0Adownstream%20tasks.%20Our%20code%20is%20available%20at%0A%24%5Chref%7Bhttps%3A//github.com/layer6ai-labs/ssl-robustness%7D%7Bgithub.com/layer6ai-labs/ssl-robustness%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Robust%2520Self-Supervised%2520Learning%2520Across%2520Diverse%2520Downstream%250A%2520%2520Tasks%26entry.906535625%3DAntoni%2520Kowalczuk%2520and%2520Jan%2520Dubi%25C5%2584ski%2520and%2520Atiyeh%2520Ashari%2520Ghomi%2520and%2520Yi%2520Sui%2520and%2520George%2520Stein%2520and%2520Jiapeng%2520Wu%2520and%2520Jesse%2520C.%2520Cresswell%2520and%2520Franziska%2520Boenisch%2520and%2520Adam%2520Dziedzic%26entry.1292438233%3D%2520%2520Large-scale%2520vision%2520models%2520have%2520become%2520integral%2520in%2520many%2520applications%2520due%2520to%250Atheir%2520unprecedented%2520performance%2520and%2520versatility%2520across%2520downstream%2520tasks.%250AHowever%252C%2520the%2520robustness%2520of%2520these%2520foundation%2520models%2520has%2520primarily%2520been%2520explored%250Afor%2520a%2520single%2520task%252C%2520namely%2520image%2520classification.%2520The%2520vulnerability%2520of%2520other%250Acommon%2520vision%2520tasks%252C%2520such%2520as%2520semantic%2520segmentation%2520and%2520depth%2520estimation%252C%250Aremains%2520largely%2520unknown.%2520We%2520present%2520a%2520comprehensive%2520empirical%2520evaluation%2520of%2520the%250Aadversarial%2520robustness%2520of%2520self-supervised%2520vision%2520encoders%2520across%2520multiple%250Adownstream%2520tasks.%2520Our%2520attacks%2520operate%2520in%2520the%2520encoder%2520embedding%2520space%2520and%2520at%2520the%250Adownstream%2520task%2520output%2520level.%2520In%2520both%2520cases%252C%2520current%2520state-of-the-art%250Aadversarial%2520fine-tuning%2520techniques%2520tested%2520only%2520for%2520classification%2520significantly%250Adegrade%2520clean%2520and%2520robust%2520performance%2520on%2520other%2520tasks.%2520Since%2520the%2520purpose%2520of%2520a%250Afoundation%2520model%2520is%2520to%2520cater%2520to%2520multiple%2520applications%2520at%2520once%252C%2520our%2520findings%250Areveal%2520the%2520need%2520to%2520enhance%2520encoder%2520robustness%2520more%2520broadly.%2520%2525We%2520discuss%250Apotential%2520strategies%2520for%2520more%2520robust%2520foundation%2520vision%2520models%2520across%2520diverse%250Adownstream%2520tasks.%2520Our%2520code%2520is%2520available%2520at%250A%2524%255Chref%257Bhttps%253A//github.com/layer6ai-labs/ssl-robustness%257D%257Bgithub.com/layer6ai-labs/ssl-robustness%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Robust%20Self-Supervised%20Learning%20Across%20Diverse%20Downstream%0A%20%20Tasks&entry.906535625=Antoni%20Kowalczuk%20and%20Jan%20Dubi%C5%84ski%20and%20Atiyeh%20Ashari%20Ghomi%20and%20Yi%20Sui%20and%20George%20Stein%20and%20Jiapeng%20Wu%20and%20Jesse%20C.%20Cresswell%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic&entry.1292438233=%20%20Large-scale%20vision%20models%20have%20become%20integral%20in%20many%20applications%20due%20to%0Atheir%20unprecedented%20performance%20and%20versatility%20across%20downstream%20tasks.%0AHowever%2C%20the%20robustness%20of%20these%20foundation%20models%20has%20primarily%20been%20explored%0Afor%20a%20single%20task%2C%20namely%20image%20classification.%20The%20vulnerability%20of%20other%0Acommon%20vision%20tasks%2C%20such%20as%20semantic%20segmentation%20and%20depth%20estimation%2C%0Aremains%20largely%20unknown.%20We%20present%20a%20comprehensive%20empirical%20evaluation%20of%20the%0Aadversarial%20robustness%20of%20self-supervised%20vision%20encoders%20across%20multiple%0Adownstream%20tasks.%20Our%20attacks%20operate%20in%20the%20encoder%20embedding%20space%20and%20at%20the%0Adownstream%20task%20output%20level.%20In%20both%20cases%2C%20current%20state-of-the-art%0Aadversarial%20fine-tuning%20techniques%20tested%20only%20for%20classification%20significantly%0Adegrade%20clean%20and%20robust%20performance%20on%20other%20tasks.%20Since%20the%20purpose%20of%20a%0Afoundation%20model%20is%20to%20cater%20to%20multiple%20applications%20at%20once%2C%20our%20findings%0Areveal%20the%20need%20to%20enhance%20encoder%20robustness%20more%20broadly.%20%25We%20discuss%0Apotential%20strategies%20for%20more%20robust%20foundation%20vision%20models%20across%20diverse%0Adownstream%20tasks.%20Our%20code%20is%20available%20at%0A%24%5Chref%7Bhttps%3A//github.com/layer6ai-labs/ssl-robustness%7D%7Bgithub.com/layer6ai-labs/ssl-robustness%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12588v1&entry.124074799=Read"},
{"title": "InfoNorm: Mutual Information Shaping of Normals for Sparse-View\n  Reconstruction", "author": "Xulong Wang and Siyan Dong and Youyi Zheng and Yanchao Yang", "abstract": "  3D surface reconstruction from multi-view images is essential for scene\nunderstanding and interaction. However, complex indoor scenes pose challenges\nsuch as ambiguity due to limited observations. Recent implicit surface\nrepresentations, such as Neural Radiance Fields (NeRFs) and signed distance\nfunctions (SDFs), employ various geometric priors to resolve the lack of\nobserved information. Nevertheless, their performance heavily depends on the\nquality of the pre-trained geometry estimation models. To ease such dependence,\nwe propose regularizing the geometric modeling by explicitly encouraging the\nmutual information among surface normals of highly correlated scene points. In\nthis way, the geometry learning process is modulated by the second-order\ncorrelations from noisy (first-order) geometric priors, thus eliminating the\nbias due to poor generalization. Additionally, we introduce a simple yet\neffective scheme that utilizes semantic and geometric features to identify\ncorrelated points, enhancing their mutual information accordingly. The proposed\ntechnique can serve as a plugin for SDF-based neural surface representations.\nOur experiments demonstrate the effectiveness of the proposed in improving the\nsurface reconstruction quality of major states of the arts. Our code is\navailable at: \\url{https://github.com/Muliphein/InfoNorm}.\n", "link": "http://arxiv.org/abs/2407.12661v1", "date": "2024-07-17", "relevancy": 2.7324, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.565}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoNorm%3A%20Mutual%20Information%20Shaping%20of%20Normals%20for%20Sparse-View%0A%20%20Reconstruction&body=Title%3A%20InfoNorm%3A%20Mutual%20Information%20Shaping%20of%20Normals%20for%20Sparse-View%0A%20%20Reconstruction%0AAuthor%3A%20Xulong%20Wang%20and%20Siyan%20Dong%20and%20Youyi%20Zheng%20and%20Yanchao%20Yang%0AAbstract%3A%20%20%203D%20surface%20reconstruction%20from%20multi-view%20images%20is%20essential%20for%20scene%0Aunderstanding%20and%20interaction.%20However%2C%20complex%20indoor%20scenes%20pose%20challenges%0Asuch%20as%20ambiguity%20due%20to%20limited%20observations.%20Recent%20implicit%20surface%0Arepresentations%2C%20such%20as%20Neural%20Radiance%20Fields%20%28NeRFs%29%20and%20signed%20distance%0Afunctions%20%28SDFs%29%2C%20employ%20various%20geometric%20priors%20to%20resolve%20the%20lack%20of%0Aobserved%20information.%20Nevertheless%2C%20their%20performance%20heavily%20depends%20on%20the%0Aquality%20of%20the%20pre-trained%20geometry%20estimation%20models.%20To%20ease%20such%20dependence%2C%0Awe%20propose%20regularizing%20the%20geometric%20modeling%20by%20explicitly%20encouraging%20the%0Amutual%20information%20among%20surface%20normals%20of%20highly%20correlated%20scene%20points.%20In%0Athis%20way%2C%20the%20geometry%20learning%20process%20is%20modulated%20by%20the%20second-order%0Acorrelations%20from%20noisy%20%28first-order%29%20geometric%20priors%2C%20thus%20eliminating%20the%0Abias%20due%20to%20poor%20generalization.%20Additionally%2C%20we%20introduce%20a%20simple%20yet%0Aeffective%20scheme%20that%20utilizes%20semantic%20and%20geometric%20features%20to%20identify%0Acorrelated%20points%2C%20enhancing%20their%20mutual%20information%20accordingly.%20The%20proposed%0Atechnique%20can%20serve%20as%20a%20plugin%20for%20SDF-based%20neural%20surface%20representations.%0AOur%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20in%20improving%20the%0Asurface%20reconstruction%20quality%20of%20major%20states%20of%20the%20arts.%20Our%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/Muliphein/InfoNorm%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoNorm%253A%2520Mutual%2520Information%2520Shaping%2520of%2520Normals%2520for%2520Sparse-View%250A%2520%2520Reconstruction%26entry.906535625%3DXulong%2520Wang%2520and%2520Siyan%2520Dong%2520and%2520Youyi%2520Zheng%2520and%2520Yanchao%2520Yang%26entry.1292438233%3D%2520%25203D%2520surface%2520reconstruction%2520from%2520multi-view%2520images%2520is%2520essential%2520for%2520scene%250Aunderstanding%2520and%2520interaction.%2520However%252C%2520complex%2520indoor%2520scenes%2520pose%2520challenges%250Asuch%2520as%2520ambiguity%2520due%2520to%2520limited%2520observations.%2520Recent%2520implicit%2520surface%250Arepresentations%252C%2520such%2520as%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520and%2520signed%2520distance%250Afunctions%2520%2528SDFs%2529%252C%2520employ%2520various%2520geometric%2520priors%2520to%2520resolve%2520the%2520lack%2520of%250Aobserved%2520information.%2520Nevertheless%252C%2520their%2520performance%2520heavily%2520depends%2520on%2520the%250Aquality%2520of%2520the%2520pre-trained%2520geometry%2520estimation%2520models.%2520To%2520ease%2520such%2520dependence%252C%250Awe%2520propose%2520regularizing%2520the%2520geometric%2520modeling%2520by%2520explicitly%2520encouraging%2520the%250Amutual%2520information%2520among%2520surface%2520normals%2520of%2520highly%2520correlated%2520scene%2520points.%2520In%250Athis%2520way%252C%2520the%2520geometry%2520learning%2520process%2520is%2520modulated%2520by%2520the%2520second-order%250Acorrelations%2520from%2520noisy%2520%2528first-order%2529%2520geometric%2520priors%252C%2520thus%2520eliminating%2520the%250Abias%2520due%2520to%2520poor%2520generalization.%2520Additionally%252C%2520we%2520introduce%2520a%2520simple%2520yet%250Aeffective%2520scheme%2520that%2520utilizes%2520semantic%2520and%2520geometric%2520features%2520to%2520identify%250Acorrelated%2520points%252C%2520enhancing%2520their%2520mutual%2520information%2520accordingly.%2520The%2520proposed%250Atechnique%2520can%2520serve%2520as%2520a%2520plugin%2520for%2520SDF-based%2520neural%2520surface%2520representations.%250AOur%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520in%2520improving%2520the%250Asurface%2520reconstruction%2520quality%2520of%2520major%2520states%2520of%2520the%2520arts.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/Muliphein/InfoNorm%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoNorm%3A%20Mutual%20Information%20Shaping%20of%20Normals%20for%20Sparse-View%0A%20%20Reconstruction&entry.906535625=Xulong%20Wang%20and%20Siyan%20Dong%20and%20Youyi%20Zheng%20and%20Yanchao%20Yang&entry.1292438233=%20%203D%20surface%20reconstruction%20from%20multi-view%20images%20is%20essential%20for%20scene%0Aunderstanding%20and%20interaction.%20However%2C%20complex%20indoor%20scenes%20pose%20challenges%0Asuch%20as%20ambiguity%20due%20to%20limited%20observations.%20Recent%20implicit%20surface%0Arepresentations%2C%20such%20as%20Neural%20Radiance%20Fields%20%28NeRFs%29%20and%20signed%20distance%0Afunctions%20%28SDFs%29%2C%20employ%20various%20geometric%20priors%20to%20resolve%20the%20lack%20of%0Aobserved%20information.%20Nevertheless%2C%20their%20performance%20heavily%20depends%20on%20the%0Aquality%20of%20the%20pre-trained%20geometry%20estimation%20models.%20To%20ease%20such%20dependence%2C%0Awe%20propose%20regularizing%20the%20geometric%20modeling%20by%20explicitly%20encouraging%20the%0Amutual%20information%20among%20surface%20normals%20of%20highly%20correlated%20scene%20points.%20In%0Athis%20way%2C%20the%20geometry%20learning%20process%20is%20modulated%20by%20the%20second-order%0Acorrelations%20from%20noisy%20%28first-order%29%20geometric%20priors%2C%20thus%20eliminating%20the%0Abias%20due%20to%20poor%20generalization.%20Additionally%2C%20we%20introduce%20a%20simple%20yet%0Aeffective%20scheme%20that%20utilizes%20semantic%20and%20geometric%20features%20to%20identify%0Acorrelated%20points%2C%20enhancing%20their%20mutual%20information%20accordingly.%20The%20proposed%0Atechnique%20can%20serve%20as%20a%20plugin%20for%20SDF-based%20neural%20surface%20representations.%0AOur%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20in%20improving%20the%0Asurface%20reconstruction%20quality%20of%20major%20states%20of%20the%20arts.%20Our%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/Muliphein/InfoNorm%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12661v1&entry.124074799=Read"},
{"title": "Mutual Information Guided Optimal Transport for Unsupervised\n  Visible-Infrared Person Re-identification", "author": "Zhizhong Zhang and Jiangming Wang and Xin Tan and Yanyun Qu and Junping Wang and Yong Xie and Yuan Xie", "abstract": "  Unsupervised visible infrared person re-identification (USVI-ReID) is a\nchallenging retrieval task that aims to retrieve cross-modality pedestrian\nimages without using any label information. In this task, the large\ncross-modality variance makes it difficult to generate reliable cross-modality\nlabels, and the lack of annotations also provides additional difficulties for\nlearning modality-invariant features. In this paper, we first deduce an\noptimization objective for unsupervised VI-ReID based on the mutual information\nbetween the model's cross-modality input and output. With equivalent\nderivation, three learning principles, i.e., \"Sharpness\" (entropy\nminimization), \"Fairness\" (uniform label distribution), and \"Fitness\" (reliable\ncross-modality matching) are obtained. Under their guidance, we design a loop\niterative training strategy alternating between model training and\ncross-modality matching. In the matching stage, a uniform prior guided optimal\ntransport assignment (\"Fitness\", \"Fairness\") is proposed to select matched\nvisible and infrared prototypes. In the training stage, we utilize this\nmatching information to introduce prototype-based contrastive learning for\nminimizing the intra- and cross-modality entropy (\"Sharpness\"). Extensive\nexperimental results on benchmarks demonstrate the effectiveness of our method,\ne.g., 60.6% and 90.3% of Rank-1 accuracy on SYSU-MM01 and RegDB without any\nannotations.\n", "link": "http://arxiv.org/abs/2407.12758v1", "date": "2024-07-17", "relevancy": 2.7244, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5477}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mutual%20Information%20Guided%20Optimal%20Transport%20for%20Unsupervised%0A%20%20Visible-Infrared%20Person%20Re-identification&body=Title%3A%20Mutual%20Information%20Guided%20Optimal%20Transport%20for%20Unsupervised%0A%20%20Visible-Infrared%20Person%20Re-identification%0AAuthor%3A%20Zhizhong%20Zhang%20and%20Jiangming%20Wang%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Junping%20Wang%20and%20Yong%20Xie%20and%20Yuan%20Xie%0AAbstract%3A%20%20%20Unsupervised%20visible%20infrared%20person%20re-identification%20%28USVI-ReID%29%20is%20a%0Achallenging%20retrieval%20task%20that%20aims%20to%20retrieve%20cross-modality%20pedestrian%0Aimages%20without%20using%20any%20label%20information.%20In%20this%20task%2C%20the%20large%0Across-modality%20variance%20makes%20it%20difficult%20to%20generate%20reliable%20cross-modality%0Alabels%2C%20and%20the%20lack%20of%20annotations%20also%20provides%20additional%20difficulties%20for%0Alearning%20modality-invariant%20features.%20In%20this%20paper%2C%20we%20first%20deduce%20an%0Aoptimization%20objective%20for%20unsupervised%20VI-ReID%20based%20on%20the%20mutual%20information%0Abetween%20the%20model%27s%20cross-modality%20input%20and%20output.%20With%20equivalent%0Aderivation%2C%20three%20learning%20principles%2C%20i.e.%2C%20%22Sharpness%22%20%28entropy%0Aminimization%29%2C%20%22Fairness%22%20%28uniform%20label%20distribution%29%2C%20and%20%22Fitness%22%20%28reliable%0Across-modality%20matching%29%20are%20obtained.%20Under%20their%20guidance%2C%20we%20design%20a%20loop%0Aiterative%20training%20strategy%20alternating%20between%20model%20training%20and%0Across-modality%20matching.%20In%20the%20matching%20stage%2C%20a%20uniform%20prior%20guided%20optimal%0Atransport%20assignment%20%28%22Fitness%22%2C%20%22Fairness%22%29%20is%20proposed%20to%20select%20matched%0Avisible%20and%20infrared%20prototypes.%20In%20the%20training%20stage%2C%20we%20utilize%20this%0Amatching%20information%20to%20introduce%20prototype-based%20contrastive%20learning%20for%0Aminimizing%20the%20intra-%20and%20cross-modality%20entropy%20%28%22Sharpness%22%29.%20Extensive%0Aexperimental%20results%20on%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%0Ae.g.%2C%2060.6%25%20and%2090.3%25%20of%20Rank-1%20accuracy%20on%20SYSU-MM01%20and%20RegDB%20without%20any%0Aannotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMutual%2520Information%2520Guided%2520Optimal%2520Transport%2520for%2520Unsupervised%250A%2520%2520Visible-Infrared%2520Person%2520Re-identification%26entry.906535625%3DZhizhong%2520Zhang%2520and%2520Jiangming%2520Wang%2520and%2520Xin%2520Tan%2520and%2520Yanyun%2520Qu%2520and%2520Junping%2520Wang%2520and%2520Yong%2520Xie%2520and%2520Yuan%2520Xie%26entry.1292438233%3D%2520%2520Unsupervised%2520visible%2520infrared%2520person%2520re-identification%2520%2528USVI-ReID%2529%2520is%2520a%250Achallenging%2520retrieval%2520task%2520that%2520aims%2520to%2520retrieve%2520cross-modality%2520pedestrian%250Aimages%2520without%2520using%2520any%2520label%2520information.%2520In%2520this%2520task%252C%2520the%2520large%250Across-modality%2520variance%2520makes%2520it%2520difficult%2520to%2520generate%2520reliable%2520cross-modality%250Alabels%252C%2520and%2520the%2520lack%2520of%2520annotations%2520also%2520provides%2520additional%2520difficulties%2520for%250Alearning%2520modality-invariant%2520features.%2520In%2520this%2520paper%252C%2520we%2520first%2520deduce%2520an%250Aoptimization%2520objective%2520for%2520unsupervised%2520VI-ReID%2520based%2520on%2520the%2520mutual%2520information%250Abetween%2520the%2520model%2527s%2520cross-modality%2520input%2520and%2520output.%2520With%2520equivalent%250Aderivation%252C%2520three%2520learning%2520principles%252C%2520i.e.%252C%2520%2522Sharpness%2522%2520%2528entropy%250Aminimization%2529%252C%2520%2522Fairness%2522%2520%2528uniform%2520label%2520distribution%2529%252C%2520and%2520%2522Fitness%2522%2520%2528reliable%250Across-modality%2520matching%2529%2520are%2520obtained.%2520Under%2520their%2520guidance%252C%2520we%2520design%2520a%2520loop%250Aiterative%2520training%2520strategy%2520alternating%2520between%2520model%2520training%2520and%250Across-modality%2520matching.%2520In%2520the%2520matching%2520stage%252C%2520a%2520uniform%2520prior%2520guided%2520optimal%250Atransport%2520assignment%2520%2528%2522Fitness%2522%252C%2520%2522Fairness%2522%2529%2520is%2520proposed%2520to%2520select%2520matched%250Avisible%2520and%2520infrared%2520prototypes.%2520In%2520the%2520training%2520stage%252C%2520we%2520utilize%2520this%250Amatching%2520information%2520to%2520introduce%2520prototype-based%2520contrastive%2520learning%2520for%250Aminimizing%2520the%2520intra-%2520and%2520cross-modality%2520entropy%2520%2528%2522Sharpness%2522%2529.%2520Extensive%250Aexperimental%2520results%2520on%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%250Ae.g.%252C%252060.6%2525%2520and%252090.3%2525%2520of%2520Rank-1%2520accuracy%2520on%2520SYSU-MM01%2520and%2520RegDB%2520without%2520any%250Aannotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mutual%20Information%20Guided%20Optimal%20Transport%20for%20Unsupervised%0A%20%20Visible-Infrared%20Person%20Re-identification&entry.906535625=Zhizhong%20Zhang%20and%20Jiangming%20Wang%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Junping%20Wang%20and%20Yong%20Xie%20and%20Yuan%20Xie&entry.1292438233=%20%20Unsupervised%20visible%20infrared%20person%20re-identification%20%28USVI-ReID%29%20is%20a%0Achallenging%20retrieval%20task%20that%20aims%20to%20retrieve%20cross-modality%20pedestrian%0Aimages%20without%20using%20any%20label%20information.%20In%20this%20task%2C%20the%20large%0Across-modality%20variance%20makes%20it%20difficult%20to%20generate%20reliable%20cross-modality%0Alabels%2C%20and%20the%20lack%20of%20annotations%20also%20provides%20additional%20difficulties%20for%0Alearning%20modality-invariant%20features.%20In%20this%20paper%2C%20we%20first%20deduce%20an%0Aoptimization%20objective%20for%20unsupervised%20VI-ReID%20based%20on%20the%20mutual%20information%0Abetween%20the%20model%27s%20cross-modality%20input%20and%20output.%20With%20equivalent%0Aderivation%2C%20three%20learning%20principles%2C%20i.e.%2C%20%22Sharpness%22%20%28entropy%0Aminimization%29%2C%20%22Fairness%22%20%28uniform%20label%20distribution%29%2C%20and%20%22Fitness%22%20%28reliable%0Across-modality%20matching%29%20are%20obtained.%20Under%20their%20guidance%2C%20we%20design%20a%20loop%0Aiterative%20training%20strategy%20alternating%20between%20model%20training%20and%0Across-modality%20matching.%20In%20the%20matching%20stage%2C%20a%20uniform%20prior%20guided%20optimal%0Atransport%20assignment%20%28%22Fitness%22%2C%20%22Fairness%22%29%20is%20proposed%20to%20select%20matched%0Avisible%20and%20infrared%20prototypes.%20In%20the%20training%20stage%2C%20we%20utilize%20this%0Amatching%20information%20to%20introduce%20prototype-based%20contrastive%20learning%20for%0Aminimizing%20the%20intra-%20and%20cross-modality%20entropy%20%28%22Sharpness%22%29.%20Extensive%0Aexperimental%20results%20on%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%0Ae.g.%2C%2060.6%25%20and%2090.3%25%20of%20Rank-1%20accuracy%20on%20SYSU-MM01%20and%20RegDB%20without%20any%0Aannotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12758v1&entry.124074799=Read"},
{"title": "Embracing Events and Frames with Hierarchical Feature Refinement Network\n  for Object Detection", "author": "Hu Cao and Zehua Zhang and Yan Xia and Xinyi Li and Jiahao Xia and Guang Chen and Alois Knoll", "abstract": "  In frame-based vision, object detection faces substantial performance\ndegradation under challenging conditions due to the limited sensing capability\nof conventional cameras. Event cameras output sparse and asynchronous events,\nproviding a potential solution to solve these problems. However, effectively\nfusing two heterogeneous modalities remains an open issue. In this work, we\npropose a novel hierarchical feature refinement network for event-frame fusion.\nThe core concept is the design of the coarse-to-fine fusion module, denoted as\nthe cross-modality adaptive feature refinement (CAFR) module. In the initial\nphase, the bidirectional cross-modality interaction (BCI) part facilitates\ninformation bridging from two distinct sources. Subsequently, the features are\nfurther refined by aligning the channel-level mean and variance in the two-fold\nadaptive feature refinement (TAFR) part. We conducted extensive experiments on\ntwo benchmarks: the low-resolution PKU-DDD17-Car dataset and the\nhigh-resolution DSEC dataset. Experimental results show that our method\nsurpasses the state-of-the-art by an impressive margin of $\\textbf{8.0}\\%$ on\nthe DSEC dataset. Besides, our method exhibits significantly better robustness\n(\\textbf{69.5}\\% versus \\textbf{38.7}\\%) when introducing 15 different\ncorruption types to the frame images. The code can be found at the link\n(https://github.com/HuCaoFighting/FRN).\n", "link": "http://arxiv.org/abs/2407.12582v1", "date": "2024-07-17", "relevancy": 2.7089, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5597}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5404}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embracing%20Events%20and%20Frames%20with%20Hierarchical%20Feature%20Refinement%20Network%0A%20%20for%20Object%20Detection&body=Title%3A%20Embracing%20Events%20and%20Frames%20with%20Hierarchical%20Feature%20Refinement%20Network%0A%20%20for%20Object%20Detection%0AAuthor%3A%20Hu%20Cao%20and%20Zehua%20Zhang%20and%20Yan%20Xia%20and%20Xinyi%20Li%20and%20Jiahao%20Xia%20and%20Guang%20Chen%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20In%20frame-based%20vision%2C%20object%20detection%20faces%20substantial%20performance%0Adegradation%20under%20challenging%20conditions%20due%20to%20the%20limited%20sensing%20capability%0Aof%20conventional%20cameras.%20Event%20cameras%20output%20sparse%20and%20asynchronous%20events%2C%0Aproviding%20a%20potential%20solution%20to%20solve%20these%20problems.%20However%2C%20effectively%0Afusing%20two%20heterogeneous%20modalities%20remains%20an%20open%20issue.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20hierarchical%20feature%20refinement%20network%20for%20event-frame%20fusion.%0AThe%20core%20concept%20is%20the%20design%20of%20the%20coarse-to-fine%20fusion%20module%2C%20denoted%20as%0Athe%20cross-modality%20adaptive%20feature%20refinement%20%28CAFR%29%20module.%20In%20the%20initial%0Aphase%2C%20the%20bidirectional%20cross-modality%20interaction%20%28BCI%29%20part%20facilitates%0Ainformation%20bridging%20from%20two%20distinct%20sources.%20Subsequently%2C%20the%20features%20are%0Afurther%20refined%20by%20aligning%20the%20channel-level%20mean%20and%20variance%20in%20the%20two-fold%0Aadaptive%20feature%20refinement%20%28TAFR%29%20part.%20We%20conducted%20extensive%20experiments%20on%0Atwo%20benchmarks%3A%20the%20low-resolution%20PKU-DDD17-Car%20dataset%20and%20the%0Ahigh-resolution%20DSEC%20dataset.%20Experimental%20results%20show%20that%20our%20method%0Asurpasses%20the%20state-of-the-art%20by%20an%20impressive%20margin%20of%20%24%5Ctextbf%7B8.0%7D%5C%25%24%20on%0Athe%20DSEC%20dataset.%20Besides%2C%20our%20method%20exhibits%20significantly%20better%20robustness%0A%28%5Ctextbf%7B69.5%7D%5C%25%20versus%20%5Ctextbf%7B38.7%7D%5C%25%29%20when%20introducing%2015%20different%0Acorruption%20types%20to%20the%20frame%20images.%20The%20code%20can%20be%20found%20at%20the%20link%0A%28https%3A//github.com/HuCaoFighting/FRN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbracing%2520Events%2520and%2520Frames%2520with%2520Hierarchical%2520Feature%2520Refinement%2520Network%250A%2520%2520for%2520Object%2520Detection%26entry.906535625%3DHu%2520Cao%2520and%2520Zehua%2520Zhang%2520and%2520Yan%2520Xia%2520and%2520Xinyi%2520Li%2520and%2520Jiahao%2520Xia%2520and%2520Guang%2520Chen%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520In%2520frame-based%2520vision%252C%2520object%2520detection%2520faces%2520substantial%2520performance%250Adegradation%2520under%2520challenging%2520conditions%2520due%2520to%2520the%2520limited%2520sensing%2520capability%250Aof%2520conventional%2520cameras.%2520Event%2520cameras%2520output%2520sparse%2520and%2520asynchronous%2520events%252C%250Aproviding%2520a%2520potential%2520solution%2520to%2520solve%2520these%2520problems.%2520However%252C%2520effectively%250Afusing%2520two%2520heterogeneous%2520modalities%2520remains%2520an%2520open%2520issue.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520hierarchical%2520feature%2520refinement%2520network%2520for%2520event-frame%2520fusion.%250AThe%2520core%2520concept%2520is%2520the%2520design%2520of%2520the%2520coarse-to-fine%2520fusion%2520module%252C%2520denoted%2520as%250Athe%2520cross-modality%2520adaptive%2520feature%2520refinement%2520%2528CAFR%2529%2520module.%2520In%2520the%2520initial%250Aphase%252C%2520the%2520bidirectional%2520cross-modality%2520interaction%2520%2528BCI%2529%2520part%2520facilitates%250Ainformation%2520bridging%2520from%2520two%2520distinct%2520sources.%2520Subsequently%252C%2520the%2520features%2520are%250Afurther%2520refined%2520by%2520aligning%2520the%2520channel-level%2520mean%2520and%2520variance%2520in%2520the%2520two-fold%250Aadaptive%2520feature%2520refinement%2520%2528TAFR%2529%2520part.%2520We%2520conducted%2520extensive%2520experiments%2520on%250Atwo%2520benchmarks%253A%2520the%2520low-resolution%2520PKU-DDD17-Car%2520dataset%2520and%2520the%250Ahigh-resolution%2520DSEC%2520dataset.%2520Experimental%2520results%2520show%2520that%2520our%2520method%250Asurpasses%2520the%2520state-of-the-art%2520by%2520an%2520impressive%2520margin%2520of%2520%2524%255Ctextbf%257B8.0%257D%255C%2525%2524%2520on%250Athe%2520DSEC%2520dataset.%2520Besides%252C%2520our%2520method%2520exhibits%2520significantly%2520better%2520robustness%250A%2528%255Ctextbf%257B69.5%257D%255C%2525%2520versus%2520%255Ctextbf%257B38.7%257D%255C%2525%2529%2520when%2520introducing%252015%2520different%250Acorruption%2520types%2520to%2520the%2520frame%2520images.%2520The%2520code%2520can%2520be%2520found%2520at%2520the%2520link%250A%2528https%253A//github.com/HuCaoFighting/FRN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embracing%20Events%20and%20Frames%20with%20Hierarchical%20Feature%20Refinement%20Network%0A%20%20for%20Object%20Detection&entry.906535625=Hu%20Cao%20and%20Zehua%20Zhang%20and%20Yan%20Xia%20and%20Xinyi%20Li%20and%20Jiahao%20Xia%20and%20Guang%20Chen%20and%20Alois%20Knoll&entry.1292438233=%20%20In%20frame-based%20vision%2C%20object%20detection%20faces%20substantial%20performance%0Adegradation%20under%20challenging%20conditions%20due%20to%20the%20limited%20sensing%20capability%0Aof%20conventional%20cameras.%20Event%20cameras%20output%20sparse%20and%20asynchronous%20events%2C%0Aproviding%20a%20potential%20solution%20to%20solve%20these%20problems.%20However%2C%20effectively%0Afusing%20two%20heterogeneous%20modalities%20remains%20an%20open%20issue.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20hierarchical%20feature%20refinement%20network%20for%20event-frame%20fusion.%0AThe%20core%20concept%20is%20the%20design%20of%20the%20coarse-to-fine%20fusion%20module%2C%20denoted%20as%0Athe%20cross-modality%20adaptive%20feature%20refinement%20%28CAFR%29%20module.%20In%20the%20initial%0Aphase%2C%20the%20bidirectional%20cross-modality%20interaction%20%28BCI%29%20part%20facilitates%0Ainformation%20bridging%20from%20two%20distinct%20sources.%20Subsequently%2C%20the%20features%20are%0Afurther%20refined%20by%20aligning%20the%20channel-level%20mean%20and%20variance%20in%20the%20two-fold%0Aadaptive%20feature%20refinement%20%28TAFR%29%20part.%20We%20conducted%20extensive%20experiments%20on%0Atwo%20benchmarks%3A%20the%20low-resolution%20PKU-DDD17-Car%20dataset%20and%20the%0Ahigh-resolution%20DSEC%20dataset.%20Experimental%20results%20show%20that%20our%20method%0Asurpasses%20the%20state-of-the-art%20by%20an%20impressive%20margin%20of%20%24%5Ctextbf%7B8.0%7D%5C%25%24%20on%0Athe%20DSEC%20dataset.%20Besides%2C%20our%20method%20exhibits%20significantly%20better%20robustness%0A%28%5Ctextbf%7B69.5%7D%5C%25%20versus%20%5Ctextbf%7B38.7%7D%5C%25%29%20when%20introducing%2015%20different%0Acorruption%20types%20to%20the%20frame%20images.%20The%20code%20can%20be%20found%20at%20the%20link%0A%28https%3A//github.com/HuCaoFighting/FRN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12582v1&entry.124074799=Read"},
{"title": "GroundUp: Rapid Sketch-Based 3D City Massing", "author": "Gizem Esra Unlu and Mohamed Sayed and Yulia Gryaditskaya and Gabriel Brostow", "abstract": "  We propose GroundUp, the first sketch-based ideation tool for 3D city massing\nof urban areas. We focus on early-stage urban design, where sketching is a\ncommon tool and the design starts from balancing building volumes (masses) and\nopen spaces. With Human-Centered AI in mind, we aim to help architects quickly\nrevise their ideas by easily switching between 2D sketches and 3D models,\nallowing for smoother iteration and sharing of ideas. Inspired by feedback from\narchitects and existing workflows, our system takes as a first input a user\nsketch of multiple buildings in a top-down view. The user then draws a\nperspective sketch of the envisioned site. Our method is designed to exploit\nthe complementarity of information in the two sketches and allows users to\nquickly preview and adjust the inferred 3D shapes. Our model has two main\ncomponents. First, we propose a novel sketch-to-depth prediction network for\nperspective sketches that exploits top-down sketch shapes. Second, we use depth\ncues derived from the perspective sketch as a condition to our diffusion model,\nwhich ultimately completes the geometry in a top-down view. Thus, our final 3D\ngeometry is represented as a heightfield, allowing users to construct the city\n`from the ground up'.\n", "link": "http://arxiv.org/abs/2407.12739v1", "date": "2024-07-17", "relevancy": 2.7075, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5717}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5717}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GroundUp%3A%20Rapid%20Sketch-Based%203D%20City%20Massing&body=Title%3A%20GroundUp%3A%20Rapid%20Sketch-Based%203D%20City%20Massing%0AAuthor%3A%20Gizem%20Esra%20Unlu%20and%20Mohamed%20Sayed%20and%20Yulia%20Gryaditskaya%20and%20Gabriel%20Brostow%0AAbstract%3A%20%20%20We%20propose%20GroundUp%2C%20the%20first%20sketch-based%20ideation%20tool%20for%203D%20city%20massing%0Aof%20urban%20areas.%20We%20focus%20on%20early-stage%20urban%20design%2C%20where%20sketching%20is%20a%0Acommon%20tool%20and%20the%20design%20starts%20from%20balancing%20building%20volumes%20%28masses%29%20and%0Aopen%20spaces.%20With%20Human-Centered%20AI%20in%20mind%2C%20we%20aim%20to%20help%20architects%20quickly%0Arevise%20their%20ideas%20by%20easily%20switching%20between%202D%20sketches%20and%203D%20models%2C%0Aallowing%20for%20smoother%20iteration%20and%20sharing%20of%20ideas.%20Inspired%20by%20feedback%20from%0Aarchitects%20and%20existing%20workflows%2C%20our%20system%20takes%20as%20a%20first%20input%20a%20user%0Asketch%20of%20multiple%20buildings%20in%20a%20top-down%20view.%20The%20user%20then%20draws%20a%0Aperspective%20sketch%20of%20the%20envisioned%20site.%20Our%20method%20is%20designed%20to%20exploit%0Athe%20complementarity%20of%20information%20in%20the%20two%20sketches%20and%20allows%20users%20to%0Aquickly%20preview%20and%20adjust%20the%20inferred%203D%20shapes.%20Our%20model%20has%20two%20main%0Acomponents.%20First%2C%20we%20propose%20a%20novel%20sketch-to-depth%20prediction%20network%20for%0Aperspective%20sketches%20that%20exploits%20top-down%20sketch%20shapes.%20Second%2C%20we%20use%20depth%0Acues%20derived%20from%20the%20perspective%20sketch%20as%20a%20condition%20to%20our%20diffusion%20model%2C%0Awhich%20ultimately%20completes%20the%20geometry%20in%20a%20top-down%20view.%20Thus%2C%20our%20final%203D%0Ageometry%20is%20represented%20as%20a%20heightfield%2C%20allowing%20users%20to%20construct%20the%20city%0A%60from%20the%20ground%20up%27.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroundUp%253A%2520Rapid%2520Sketch-Based%25203D%2520City%2520Massing%26entry.906535625%3DGizem%2520Esra%2520Unlu%2520and%2520Mohamed%2520Sayed%2520and%2520Yulia%2520Gryaditskaya%2520and%2520Gabriel%2520Brostow%26entry.1292438233%3D%2520%2520We%2520propose%2520GroundUp%252C%2520the%2520first%2520sketch-based%2520ideation%2520tool%2520for%25203D%2520city%2520massing%250Aof%2520urban%2520areas.%2520We%2520focus%2520on%2520early-stage%2520urban%2520design%252C%2520where%2520sketching%2520is%2520a%250Acommon%2520tool%2520and%2520the%2520design%2520starts%2520from%2520balancing%2520building%2520volumes%2520%2528masses%2529%2520and%250Aopen%2520spaces.%2520With%2520Human-Centered%2520AI%2520in%2520mind%252C%2520we%2520aim%2520to%2520help%2520architects%2520quickly%250Arevise%2520their%2520ideas%2520by%2520easily%2520switching%2520between%25202D%2520sketches%2520and%25203D%2520models%252C%250Aallowing%2520for%2520smoother%2520iteration%2520and%2520sharing%2520of%2520ideas.%2520Inspired%2520by%2520feedback%2520from%250Aarchitects%2520and%2520existing%2520workflows%252C%2520our%2520system%2520takes%2520as%2520a%2520first%2520input%2520a%2520user%250Asketch%2520of%2520multiple%2520buildings%2520in%2520a%2520top-down%2520view.%2520The%2520user%2520then%2520draws%2520a%250Aperspective%2520sketch%2520of%2520the%2520envisioned%2520site.%2520Our%2520method%2520is%2520designed%2520to%2520exploit%250Athe%2520complementarity%2520of%2520information%2520in%2520the%2520two%2520sketches%2520and%2520allows%2520users%2520to%250Aquickly%2520preview%2520and%2520adjust%2520the%2520inferred%25203D%2520shapes.%2520Our%2520model%2520has%2520two%2520main%250Acomponents.%2520First%252C%2520we%2520propose%2520a%2520novel%2520sketch-to-depth%2520prediction%2520network%2520for%250Aperspective%2520sketches%2520that%2520exploits%2520top-down%2520sketch%2520shapes.%2520Second%252C%2520we%2520use%2520depth%250Acues%2520derived%2520from%2520the%2520perspective%2520sketch%2520as%2520a%2520condition%2520to%2520our%2520diffusion%2520model%252C%250Awhich%2520ultimately%2520completes%2520the%2520geometry%2520in%2520a%2520top-down%2520view.%2520Thus%252C%2520our%2520final%25203D%250Ageometry%2520is%2520represented%2520as%2520a%2520heightfield%252C%2520allowing%2520users%2520to%2520construct%2520the%2520city%250A%2560from%2520the%2520ground%2520up%2527.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroundUp%3A%20Rapid%20Sketch-Based%203D%20City%20Massing&entry.906535625=Gizem%20Esra%20Unlu%20and%20Mohamed%20Sayed%20and%20Yulia%20Gryaditskaya%20and%20Gabriel%20Brostow&entry.1292438233=%20%20We%20propose%20GroundUp%2C%20the%20first%20sketch-based%20ideation%20tool%20for%203D%20city%20massing%0Aof%20urban%20areas.%20We%20focus%20on%20early-stage%20urban%20design%2C%20where%20sketching%20is%20a%0Acommon%20tool%20and%20the%20design%20starts%20from%20balancing%20building%20volumes%20%28masses%29%20and%0Aopen%20spaces.%20With%20Human-Centered%20AI%20in%20mind%2C%20we%20aim%20to%20help%20architects%20quickly%0Arevise%20their%20ideas%20by%20easily%20switching%20between%202D%20sketches%20and%203D%20models%2C%0Aallowing%20for%20smoother%20iteration%20and%20sharing%20of%20ideas.%20Inspired%20by%20feedback%20from%0Aarchitects%20and%20existing%20workflows%2C%20our%20system%20takes%20as%20a%20first%20input%20a%20user%0Asketch%20of%20multiple%20buildings%20in%20a%20top-down%20view.%20The%20user%20then%20draws%20a%0Aperspective%20sketch%20of%20the%20envisioned%20site.%20Our%20method%20is%20designed%20to%20exploit%0Athe%20complementarity%20of%20information%20in%20the%20two%20sketches%20and%20allows%20users%20to%0Aquickly%20preview%20and%20adjust%20the%20inferred%203D%20shapes.%20Our%20model%20has%20two%20main%0Acomponents.%20First%2C%20we%20propose%20a%20novel%20sketch-to-depth%20prediction%20network%20for%0Aperspective%20sketches%20that%20exploits%20top-down%20sketch%20shapes.%20Second%2C%20we%20use%20depth%0Acues%20derived%20from%20the%20perspective%20sketch%20as%20a%20condition%20to%20our%20diffusion%20model%2C%0Awhich%20ultimately%20completes%20the%20geometry%20in%20a%20top-down%20view.%20Thus%2C%20our%20final%203D%0Ageometry%20is%20represented%20as%20a%20heightfield%2C%20allowing%20users%20to%20construct%20the%20city%0A%60from%20the%20ground%20up%27.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12739v1&entry.124074799=Read"},
{"title": "Contrastive Adversarial Training for Unsupervised Domain Adaptation", "author": "Jiahong Chen and Zhilin Zhang and Lucy Li and Behzad Shahrasbi and Arjun Mishra", "abstract": "  Domain adversarial training has shown its effective capability for finding\ndomain invariant feature representations and been successfully adopted for\nvarious domain adaptation tasks. However, recent advances of large models\n(e.g., vision transformers) and emerging of complex adaptation scenarios (e.g.,\nDomainNet) make adversarial training being easily biased towards source domain\nand hardly adapted to target domain. The reason is twofold: relying on large\namount of labelled data from source domain for large model training and lacking\nof labelled data from target domain for fine-tuning. Existing approaches widely\nfocused on either enhancing discriminator or improving the training stability\nfor the backbone networks. Due to unbalanced competition between the feature\nextractor and the discriminator during the adversarial training, existing\nsolutions fail to function well on complex datasets. To address this issue, we\nproposed a novel contrastive adversarial training (CAT) approach that leverages\nthe labeled source domain samples to reinforce and regulate the feature\ngeneration for target domain. Typically, the regulation forces the target\nfeature distribution being similar to the source feature distribution. CAT\naddressed three major challenges in adversarial learning: 1) ensure the feature\ndistributions from two domains as indistinguishable as possible for the\ndiscriminator, resulting in a more robust domain-invariant feature generation;\n2) encourage target samples moving closer to the source in the feature space,\nreducing the requirement for generalizing classifier trained on the labeled\nsource domain to unlabeled target domain; 3) avoid directly aligning unpaired\nsource and target samples within mini-batch. CAT can be easily plugged into\nexisting models and exhibits significant performance improvements.\n", "link": "http://arxiv.org/abs/2407.12782v1", "date": "2024-07-17", "relevancy": 2.6482, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Adversarial%20Training%20for%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20Contrastive%20Adversarial%20Training%20for%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Jiahong%20Chen%20and%20Zhilin%20Zhang%20and%20Lucy%20Li%20and%20Behzad%20Shahrasbi%20and%20Arjun%20Mishra%0AAbstract%3A%20%20%20Domain%20adversarial%20training%20has%20shown%20its%20effective%20capability%20for%20finding%0Adomain%20invariant%20feature%20representations%20and%20been%20successfully%20adopted%20for%0Avarious%20domain%20adaptation%20tasks.%20However%2C%20recent%20advances%20of%20large%20models%0A%28e.g.%2C%20vision%20transformers%29%20and%20emerging%20of%20complex%20adaptation%20scenarios%20%28e.g.%2C%0ADomainNet%29%20make%20adversarial%20training%20being%20easily%20biased%20towards%20source%20domain%0Aand%20hardly%20adapted%20to%20target%20domain.%20The%20reason%20is%20twofold%3A%20relying%20on%20large%0Aamount%20of%20labelled%20data%20from%20source%20domain%20for%20large%20model%20training%20and%20lacking%0Aof%20labelled%20data%20from%20target%20domain%20for%20fine-tuning.%20Existing%20approaches%20widely%0Afocused%20on%20either%20enhancing%20discriminator%20or%20improving%20the%20training%20stability%0Afor%20the%20backbone%20networks.%20Due%20to%20unbalanced%20competition%20between%20the%20feature%0Aextractor%20and%20the%20discriminator%20during%20the%20adversarial%20training%2C%20existing%0Asolutions%20fail%20to%20function%20well%20on%20complex%20datasets.%20To%20address%20this%20issue%2C%20we%0Aproposed%20a%20novel%20contrastive%20adversarial%20training%20%28CAT%29%20approach%20that%20leverages%0Athe%20labeled%20source%20domain%20samples%20to%20reinforce%20and%20regulate%20the%20feature%0Ageneration%20for%20target%20domain.%20Typically%2C%20the%20regulation%20forces%20the%20target%0Afeature%20distribution%20being%20similar%20to%20the%20source%20feature%20distribution.%20CAT%0Aaddressed%20three%20major%20challenges%20in%20adversarial%20learning%3A%201%29%20ensure%20the%20feature%0Adistributions%20from%20two%20domains%20as%20indistinguishable%20as%20possible%20for%20the%0Adiscriminator%2C%20resulting%20in%20a%20more%20robust%20domain-invariant%20feature%20generation%3B%0A2%29%20encourage%20target%20samples%20moving%20closer%20to%20the%20source%20in%20the%20feature%20space%2C%0Areducing%20the%20requirement%20for%20generalizing%20classifier%20trained%20on%20the%20labeled%0Asource%20domain%20to%20unlabeled%20target%20domain%3B%203%29%20avoid%20directly%20aligning%20unpaired%0Asource%20and%20target%20samples%20within%20mini-batch.%20CAT%20can%20be%20easily%20plugged%20into%0Aexisting%20models%20and%20exhibits%20significant%20performance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Adversarial%2520Training%2520for%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DJiahong%2520Chen%2520and%2520Zhilin%2520Zhang%2520and%2520Lucy%2520Li%2520and%2520Behzad%2520Shahrasbi%2520and%2520Arjun%2520Mishra%26entry.1292438233%3D%2520%2520Domain%2520adversarial%2520training%2520has%2520shown%2520its%2520effective%2520capability%2520for%2520finding%250Adomain%2520invariant%2520feature%2520representations%2520and%2520been%2520successfully%2520adopted%2520for%250Avarious%2520domain%2520adaptation%2520tasks.%2520However%252C%2520recent%2520advances%2520of%2520large%2520models%250A%2528e.g.%252C%2520vision%2520transformers%2529%2520and%2520emerging%2520of%2520complex%2520adaptation%2520scenarios%2520%2528e.g.%252C%250ADomainNet%2529%2520make%2520adversarial%2520training%2520being%2520easily%2520biased%2520towards%2520source%2520domain%250Aand%2520hardly%2520adapted%2520to%2520target%2520domain.%2520The%2520reason%2520is%2520twofold%253A%2520relying%2520on%2520large%250Aamount%2520of%2520labelled%2520data%2520from%2520source%2520domain%2520for%2520large%2520model%2520training%2520and%2520lacking%250Aof%2520labelled%2520data%2520from%2520target%2520domain%2520for%2520fine-tuning.%2520Existing%2520approaches%2520widely%250Afocused%2520on%2520either%2520enhancing%2520discriminator%2520or%2520improving%2520the%2520training%2520stability%250Afor%2520the%2520backbone%2520networks.%2520Due%2520to%2520unbalanced%2520competition%2520between%2520the%2520feature%250Aextractor%2520and%2520the%2520discriminator%2520during%2520the%2520adversarial%2520training%252C%2520existing%250Asolutions%2520fail%2520to%2520function%2520well%2520on%2520complex%2520datasets.%2520To%2520address%2520this%2520issue%252C%2520we%250Aproposed%2520a%2520novel%2520contrastive%2520adversarial%2520training%2520%2528CAT%2529%2520approach%2520that%2520leverages%250Athe%2520labeled%2520source%2520domain%2520samples%2520to%2520reinforce%2520and%2520regulate%2520the%2520feature%250Ageneration%2520for%2520target%2520domain.%2520Typically%252C%2520the%2520regulation%2520forces%2520the%2520target%250Afeature%2520distribution%2520being%2520similar%2520to%2520the%2520source%2520feature%2520distribution.%2520CAT%250Aaddressed%2520three%2520major%2520challenges%2520in%2520adversarial%2520learning%253A%25201%2529%2520ensure%2520the%2520feature%250Adistributions%2520from%2520two%2520domains%2520as%2520indistinguishable%2520as%2520possible%2520for%2520the%250Adiscriminator%252C%2520resulting%2520in%2520a%2520more%2520robust%2520domain-invariant%2520feature%2520generation%253B%250A2%2529%2520encourage%2520target%2520samples%2520moving%2520closer%2520to%2520the%2520source%2520in%2520the%2520feature%2520space%252C%250Areducing%2520the%2520requirement%2520for%2520generalizing%2520classifier%2520trained%2520on%2520the%2520labeled%250Asource%2520domain%2520to%2520unlabeled%2520target%2520domain%253B%25203%2529%2520avoid%2520directly%2520aligning%2520unpaired%250Asource%2520and%2520target%2520samples%2520within%2520mini-batch.%2520CAT%2520can%2520be%2520easily%2520plugged%2520into%250Aexisting%2520models%2520and%2520exhibits%2520significant%2520performance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Adversarial%20Training%20for%20Unsupervised%20Domain%20Adaptation&entry.906535625=Jiahong%20Chen%20and%20Zhilin%20Zhang%20and%20Lucy%20Li%20and%20Behzad%20Shahrasbi%20and%20Arjun%20Mishra&entry.1292438233=%20%20Domain%20adversarial%20training%20has%20shown%20its%20effective%20capability%20for%20finding%0Adomain%20invariant%20feature%20representations%20and%20been%20successfully%20adopted%20for%0Avarious%20domain%20adaptation%20tasks.%20However%2C%20recent%20advances%20of%20large%20models%0A%28e.g.%2C%20vision%20transformers%29%20and%20emerging%20of%20complex%20adaptation%20scenarios%20%28e.g.%2C%0ADomainNet%29%20make%20adversarial%20training%20being%20easily%20biased%20towards%20source%20domain%0Aand%20hardly%20adapted%20to%20target%20domain.%20The%20reason%20is%20twofold%3A%20relying%20on%20large%0Aamount%20of%20labelled%20data%20from%20source%20domain%20for%20large%20model%20training%20and%20lacking%0Aof%20labelled%20data%20from%20target%20domain%20for%20fine-tuning.%20Existing%20approaches%20widely%0Afocused%20on%20either%20enhancing%20discriminator%20or%20improving%20the%20training%20stability%0Afor%20the%20backbone%20networks.%20Due%20to%20unbalanced%20competition%20between%20the%20feature%0Aextractor%20and%20the%20discriminator%20during%20the%20adversarial%20training%2C%20existing%0Asolutions%20fail%20to%20function%20well%20on%20complex%20datasets.%20To%20address%20this%20issue%2C%20we%0Aproposed%20a%20novel%20contrastive%20adversarial%20training%20%28CAT%29%20approach%20that%20leverages%0Athe%20labeled%20source%20domain%20samples%20to%20reinforce%20and%20regulate%20the%20feature%0Ageneration%20for%20target%20domain.%20Typically%2C%20the%20regulation%20forces%20the%20target%0Afeature%20distribution%20being%20similar%20to%20the%20source%20feature%20distribution.%20CAT%0Aaddressed%20three%20major%20challenges%20in%20adversarial%20learning%3A%201%29%20ensure%20the%20feature%0Adistributions%20from%20two%20domains%20as%20indistinguishable%20as%20possible%20for%20the%0Adiscriminator%2C%20resulting%20in%20a%20more%20robust%20domain-invariant%20feature%20generation%3B%0A2%29%20encourage%20target%20samples%20moving%20closer%20to%20the%20source%20in%20the%20feature%20space%2C%0Areducing%20the%20requirement%20for%20generalizing%20classifier%20trained%20on%20the%20labeled%0Asource%20domain%20to%20unlabeled%20target%20domain%3B%203%29%20avoid%20directly%20aligning%20unpaired%0Asource%20and%20target%20samples%20within%20mini-batch.%20CAT%20can%20be%20easily%20plugged%20into%0Aexisting%20models%20and%20exhibits%20significant%20performance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12782v1&entry.124074799=Read"},
{"title": "Gradient Projection For Continual Parameter-Efficient Tuning", "author": "Jingyang Qiao and Zhizhong Zhang and Xin Tan and Yanyun Qu and Wensheng Zhang and Zhi Han and Yuan Xie", "abstract": "  Parameter-efficient tunings (PETs) have demonstrated impressive performance\nand promising perspectives in training large models, while they are still\nconfronted with a common problem: the trade-off between learning new content\nand protecting old knowledge, leading to zero-shot generalization collapse, and\ncross-modal hallucination. In this paper, we reformulate Adapter, LoRA,\nPrefix-tuning, and Prompt-tuning from the perspective of gradient projection,\nand firstly propose a unified framework called Parameter Efficient Gradient\nProjection (PEGP). We introduce orthogonal gradient projection into different\nPET paradigms and theoretically demonstrate that the orthogonal condition for\nthe gradient can effectively resist forgetting even for large-scale models. It\ntherefore modifies the gradient towards the direction that has less impact on\nthe old feature space, with less extra memory space and training time. We\nextensively evaluate our method with different backbones, including ViT and\nCLIP, on diverse datasets, and experiments comprehensively demonstrate its\nefficiency in reducing forgetting in class, online class, domain, task, and\nmulti-modality continual settings. The project page is available at\nhttps://dmcv-ecnu-pegp.github.io/.\n", "link": "http://arxiv.org/abs/2405.13383v3", "date": "2024-07-17", "relevancy": 2.6043, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5302}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5188}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Projection%20For%20Continual%20Parameter-Efficient%20Tuning&body=Title%3A%20Gradient%20Projection%20For%20Continual%20Parameter-Efficient%20Tuning%0AAuthor%3A%20Jingyang%20Qiao%20and%20Zhizhong%20Zhang%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Wensheng%20Zhang%20and%20Zhi%20Han%20and%20Yuan%20Xie%0AAbstract%3A%20%20%20Parameter-efficient%20tunings%20%28PETs%29%20have%20demonstrated%20impressive%20performance%0Aand%20promising%20perspectives%20in%20training%20large%20models%2C%20while%20they%20are%20still%0Aconfronted%20with%20a%20common%20problem%3A%20the%20trade-off%20between%20learning%20new%20content%0Aand%20protecting%20old%20knowledge%2C%20leading%20to%20zero-shot%20generalization%20collapse%2C%20and%0Across-modal%20hallucination.%20In%20this%20paper%2C%20we%20reformulate%20Adapter%2C%20LoRA%2C%0APrefix-tuning%2C%20and%20Prompt-tuning%20from%20the%20perspective%20of%20gradient%20projection%2C%0Aand%20firstly%20propose%20a%20unified%20framework%20called%20Parameter%20Efficient%20Gradient%0AProjection%20%28PEGP%29.%20We%20introduce%20orthogonal%20gradient%20projection%20into%20different%0APET%20paradigms%20and%20theoretically%20demonstrate%20that%20the%20orthogonal%20condition%20for%0Athe%20gradient%20can%20effectively%20resist%20forgetting%20even%20for%20large-scale%20models.%20It%0Atherefore%20modifies%20the%20gradient%20towards%20the%20direction%20that%20has%20less%20impact%20on%0Athe%20old%20feature%20space%2C%20with%20less%20extra%20memory%20space%20and%20training%20time.%20We%0Aextensively%20evaluate%20our%20method%20with%20different%20backbones%2C%20including%20ViT%20and%0ACLIP%2C%20on%20diverse%20datasets%2C%20and%20experiments%20comprehensively%20demonstrate%20its%0Aefficiency%20in%20reducing%20forgetting%20in%20class%2C%20online%20class%2C%20domain%2C%20task%2C%20and%0Amulti-modality%20continual%20settings.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//dmcv-ecnu-pegp.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13383v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Projection%2520For%2520Continual%2520Parameter-Efficient%2520Tuning%26entry.906535625%3DJingyang%2520Qiao%2520and%2520Zhizhong%2520Zhang%2520and%2520Xin%2520Tan%2520and%2520Yanyun%2520Qu%2520and%2520Wensheng%2520Zhang%2520and%2520Zhi%2520Han%2520and%2520Yuan%2520Xie%26entry.1292438233%3D%2520%2520Parameter-efficient%2520tunings%2520%2528PETs%2529%2520have%2520demonstrated%2520impressive%2520performance%250Aand%2520promising%2520perspectives%2520in%2520training%2520large%2520models%252C%2520while%2520they%2520are%2520still%250Aconfronted%2520with%2520a%2520common%2520problem%253A%2520the%2520trade-off%2520between%2520learning%2520new%2520content%250Aand%2520protecting%2520old%2520knowledge%252C%2520leading%2520to%2520zero-shot%2520generalization%2520collapse%252C%2520and%250Across-modal%2520hallucination.%2520In%2520this%2520paper%252C%2520we%2520reformulate%2520Adapter%252C%2520LoRA%252C%250APrefix-tuning%252C%2520and%2520Prompt-tuning%2520from%2520the%2520perspective%2520of%2520gradient%2520projection%252C%250Aand%2520firstly%2520propose%2520a%2520unified%2520framework%2520called%2520Parameter%2520Efficient%2520Gradient%250AProjection%2520%2528PEGP%2529.%2520We%2520introduce%2520orthogonal%2520gradient%2520projection%2520into%2520different%250APET%2520paradigms%2520and%2520theoretically%2520demonstrate%2520that%2520the%2520orthogonal%2520condition%2520for%250Athe%2520gradient%2520can%2520effectively%2520resist%2520forgetting%2520even%2520for%2520large-scale%2520models.%2520It%250Atherefore%2520modifies%2520the%2520gradient%2520towards%2520the%2520direction%2520that%2520has%2520less%2520impact%2520on%250Athe%2520old%2520feature%2520space%252C%2520with%2520less%2520extra%2520memory%2520space%2520and%2520training%2520time.%2520We%250Aextensively%2520evaluate%2520our%2520method%2520with%2520different%2520backbones%252C%2520including%2520ViT%2520and%250ACLIP%252C%2520on%2520diverse%2520datasets%252C%2520and%2520experiments%2520comprehensively%2520demonstrate%2520its%250Aefficiency%2520in%2520reducing%2520forgetting%2520in%2520class%252C%2520online%2520class%252C%2520domain%252C%2520task%252C%2520and%250Amulti-modality%2520continual%2520settings.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//dmcv-ecnu-pegp.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13383v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Projection%20For%20Continual%20Parameter-Efficient%20Tuning&entry.906535625=Jingyang%20Qiao%20and%20Zhizhong%20Zhang%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Wensheng%20Zhang%20and%20Zhi%20Han%20and%20Yuan%20Xie&entry.1292438233=%20%20Parameter-efficient%20tunings%20%28PETs%29%20have%20demonstrated%20impressive%20performance%0Aand%20promising%20perspectives%20in%20training%20large%20models%2C%20while%20they%20are%20still%0Aconfronted%20with%20a%20common%20problem%3A%20the%20trade-off%20between%20learning%20new%20content%0Aand%20protecting%20old%20knowledge%2C%20leading%20to%20zero-shot%20generalization%20collapse%2C%20and%0Across-modal%20hallucination.%20In%20this%20paper%2C%20we%20reformulate%20Adapter%2C%20LoRA%2C%0APrefix-tuning%2C%20and%20Prompt-tuning%20from%20the%20perspective%20of%20gradient%20projection%2C%0Aand%20firstly%20propose%20a%20unified%20framework%20called%20Parameter%20Efficient%20Gradient%0AProjection%20%28PEGP%29.%20We%20introduce%20orthogonal%20gradient%20projection%20into%20different%0APET%20paradigms%20and%20theoretically%20demonstrate%20that%20the%20orthogonal%20condition%20for%0Athe%20gradient%20can%20effectively%20resist%20forgetting%20even%20for%20large-scale%20models.%20It%0Atherefore%20modifies%20the%20gradient%20towards%20the%20direction%20that%20has%20less%20impact%20on%0Athe%20old%20feature%20space%2C%20with%20less%20extra%20memory%20space%20and%20training%20time.%20We%0Aextensively%20evaluate%20our%20method%20with%20different%20backbones%2C%20including%20ViT%20and%0ACLIP%2C%20on%20diverse%20datasets%2C%20and%20experiments%20comprehensively%20demonstrate%20its%0Aefficiency%20in%20reducing%20forgetting%20in%20class%2C%20online%20class%2C%20domain%2C%20task%2C%20and%0Amulti-modality%20continual%20settings.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//dmcv-ecnu-pegp.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13383v3&entry.124074799=Read"},
{"title": "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control", "author": "Sherwin Bahmani and Ivan Skorokhodov and Aliaksandr Siarohin and Willi Menapace and Guocheng Qian and Michael Vasilkovsky and Hsin-Ying Lee and Chaoyang Wang and Jiaxu Zou and Andrea Tagliasacchi and David B. Lindell and Sergey Tulyakov", "abstract": "  Modern text-to-video synthesis models demonstrate coherent, photorealistic\ngeneration of complex videos from a text description. However, most existing\nmodels lack fine-grained control over camera movement, which is critical for\ndownstream applications related to content creation, visual effects, and 3D\nvision. Recently, new methods demonstrate the ability to generate videos with\ncontrollable camera poses these techniques leverage pre-trained U-Net-based\ndiffusion models that explicitly disentangle spatial and temporal generation.\nStill, no existing approach enables camera control for new, transformer-based\nvideo diffusion models that process spatial and temporal information jointly.\nHere, we propose to tame video transformers for 3D camera control using a\nControlNet-like conditioning mechanism that incorporates spatiotemporal camera\nembeddings based on Plucker coordinates. The approach demonstrates\nstate-of-the-art performance for controllable video generation after\nfine-tuning on the RealEstate10K dataset. To the best of our knowledge, our\nwork is the first to enable camera control for transformer-based video\ndiffusion models.\n", "link": "http://arxiv.org/abs/2407.12781v1", "date": "2024-07-17", "relevancy": 2.5918, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6624}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6516}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VD3D%3A%20Taming%20Large%20Video%20Diffusion%20Transformers%20for%203D%20Camera%20Control&body=Title%3A%20VD3D%3A%20Taming%20Large%20Video%20Diffusion%20Transformers%20for%203D%20Camera%20Control%0AAuthor%3A%20Sherwin%20Bahmani%20and%20Ivan%20Skorokhodov%20and%20Aliaksandr%20Siarohin%20and%20Willi%20Menapace%20and%20Guocheng%20Qian%20and%20Michael%20Vasilkovsky%20and%20Hsin-Ying%20Lee%20and%20Chaoyang%20Wang%20and%20Jiaxu%20Zou%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%20and%20Sergey%20Tulyakov%0AAbstract%3A%20%20%20Modern%20text-to-video%20synthesis%20models%20demonstrate%20coherent%2C%20photorealistic%0Ageneration%20of%20complex%20videos%20from%20a%20text%20description.%20However%2C%20most%20existing%0Amodels%20lack%20fine-grained%20control%20over%20camera%20movement%2C%20which%20is%20critical%20for%0Adownstream%20applications%20related%20to%20content%20creation%2C%20visual%20effects%2C%20and%203D%0Avision.%20Recently%2C%20new%20methods%20demonstrate%20the%20ability%20to%20generate%20videos%20with%0Acontrollable%20camera%20poses%20these%20techniques%20leverage%20pre-trained%20U-Net-based%0Adiffusion%20models%20that%20explicitly%20disentangle%20spatial%20and%20temporal%20generation.%0AStill%2C%20no%20existing%20approach%20enables%20camera%20control%20for%20new%2C%20transformer-based%0Avideo%20diffusion%20models%20that%20process%20spatial%20and%20temporal%20information%20jointly.%0AHere%2C%20we%20propose%20to%20tame%20video%20transformers%20for%203D%20camera%20control%20using%20a%0AControlNet-like%20conditioning%20mechanism%20that%20incorporates%20spatiotemporal%20camera%0Aembeddings%20based%20on%20Plucker%20coordinates.%20The%20approach%20demonstrates%0Astate-of-the-art%20performance%20for%20controllable%20video%20generation%20after%0Afine-tuning%20on%20the%20RealEstate10K%20dataset.%20To%20the%20best%20of%20our%20knowledge%2C%20our%0Awork%20is%20the%20first%20to%20enable%20camera%20control%20for%20transformer-based%20video%0Adiffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVD3D%253A%2520Taming%2520Large%2520Video%2520Diffusion%2520Transformers%2520for%25203D%2520Camera%2520Control%26entry.906535625%3DSherwin%2520Bahmani%2520and%2520Ivan%2520Skorokhodov%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Willi%2520Menapace%2520and%2520Guocheng%2520Qian%2520and%2520Michael%2520Vasilkovsky%2520and%2520Hsin-Ying%2520Lee%2520and%2520Chaoyang%2520Wang%2520and%2520Jiaxu%2520Zou%2520and%2520Andrea%2520Tagliasacchi%2520and%2520David%2520B.%2520Lindell%2520and%2520Sergey%2520Tulyakov%26entry.1292438233%3D%2520%2520Modern%2520text-to-video%2520synthesis%2520models%2520demonstrate%2520coherent%252C%2520photorealistic%250Ageneration%2520of%2520complex%2520videos%2520from%2520a%2520text%2520description.%2520However%252C%2520most%2520existing%250Amodels%2520lack%2520fine-grained%2520control%2520over%2520camera%2520movement%252C%2520which%2520is%2520critical%2520for%250Adownstream%2520applications%2520related%2520to%2520content%2520creation%252C%2520visual%2520effects%252C%2520and%25203D%250Avision.%2520Recently%252C%2520new%2520methods%2520demonstrate%2520the%2520ability%2520to%2520generate%2520videos%2520with%250Acontrollable%2520camera%2520poses%2520these%2520techniques%2520leverage%2520pre-trained%2520U-Net-based%250Adiffusion%2520models%2520that%2520explicitly%2520disentangle%2520spatial%2520and%2520temporal%2520generation.%250AStill%252C%2520no%2520existing%2520approach%2520enables%2520camera%2520control%2520for%2520new%252C%2520transformer-based%250Avideo%2520diffusion%2520models%2520that%2520process%2520spatial%2520and%2520temporal%2520information%2520jointly.%250AHere%252C%2520we%2520propose%2520to%2520tame%2520video%2520transformers%2520for%25203D%2520camera%2520control%2520using%2520a%250AControlNet-like%2520conditioning%2520mechanism%2520that%2520incorporates%2520spatiotemporal%2520camera%250Aembeddings%2520based%2520on%2520Plucker%2520coordinates.%2520The%2520approach%2520demonstrates%250Astate-of-the-art%2520performance%2520for%2520controllable%2520video%2520generation%2520after%250Afine-tuning%2520on%2520the%2520RealEstate10K%2520dataset.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%250Awork%2520is%2520the%2520first%2520to%2520enable%2520camera%2520control%2520for%2520transformer-based%2520video%250Adiffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VD3D%3A%20Taming%20Large%20Video%20Diffusion%20Transformers%20for%203D%20Camera%20Control&entry.906535625=Sherwin%20Bahmani%20and%20Ivan%20Skorokhodov%20and%20Aliaksandr%20Siarohin%20and%20Willi%20Menapace%20and%20Guocheng%20Qian%20and%20Michael%20Vasilkovsky%20and%20Hsin-Ying%20Lee%20and%20Chaoyang%20Wang%20and%20Jiaxu%20Zou%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%20and%20Sergey%20Tulyakov&entry.1292438233=%20%20Modern%20text-to-video%20synthesis%20models%20demonstrate%20coherent%2C%20photorealistic%0Ageneration%20of%20complex%20videos%20from%20a%20text%20description.%20However%2C%20most%20existing%0Amodels%20lack%20fine-grained%20control%20over%20camera%20movement%2C%20which%20is%20critical%20for%0Adownstream%20applications%20related%20to%20content%20creation%2C%20visual%20effects%2C%20and%203D%0Avision.%20Recently%2C%20new%20methods%20demonstrate%20the%20ability%20to%20generate%20videos%20with%0Acontrollable%20camera%20poses%20these%20techniques%20leverage%20pre-trained%20U-Net-based%0Adiffusion%20models%20that%20explicitly%20disentangle%20spatial%20and%20temporal%20generation.%0AStill%2C%20no%20existing%20approach%20enables%20camera%20control%20for%20new%2C%20transformer-based%0Avideo%20diffusion%20models%20that%20process%20spatial%20and%20temporal%20information%20jointly.%0AHere%2C%20we%20propose%20to%20tame%20video%20transformers%20for%203D%20camera%20control%20using%20a%0AControlNet-like%20conditioning%20mechanism%20that%20incorporates%20spatiotemporal%20camera%0Aembeddings%20based%20on%20Plucker%20coordinates.%20The%20approach%20demonstrates%0Astate-of-the-art%20performance%20for%20controllable%20video%20generation%20after%0Afine-tuning%20on%20the%20RealEstate10K%20dataset.%20To%20the%20best%20of%20our%20knowledge%2C%20our%0Awork%20is%20the%20first%20to%20enable%20camera%20control%20for%20transformer-based%20video%0Adiffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12781v1&entry.124074799=Read"},
{"title": "Fusion Flow-enhanced Graph Pooling Residual Networks for Unmanned Aerial\n  Vehicles Surveillance in Day and Night Dual Visions", "author": "Alam Noor and Kai Li and Eduardo Tovar and Pei Zhang and Bo Wei", "abstract": "  Recognizing unauthorized Unmanned Aerial Vehicles (UAVs) within designated\nno-fly zones throughout the day and night is of paramount importance, where the\nunauthorized UAVs pose a substantial threat to both civil and military aviation\nsafety. However, recognizing UAVs day and night with dual-vision cameras is\nnontrivial, since red-green-blue (RGB) images suffer from a low detection rate\nunder an insufficient light condition, such as on cloudy or stormy days, while\nblack-and-white infrared (IR) images struggle to capture UAVs that overlap with\nthe background at night. In this paper, we propose a new optical flow-assisted\ngraph-pooling residual network (OF-GPRN), which significantly enhances the UAV\ndetection rate in day and night dual visions. The proposed OF-GPRN develops a\nnew optical fusion to remove superfluous backgrounds, which improves RGB/IR\nimaging clarity. Furthermore, OF-GPRN extends optical fusion by incorporating a\ngraph residual split attention network and a feature pyramid, which refines the\nperception of UAVs, leading to a higher success rate in UAV detection. A\ncomprehensive performance evaluation is conducted using a benchmark UAV catch\ndataset. The results indicate that the proposed OF-GPRN elevates the UAV mean\naverage precision (mAP) detection rate to 87.8%, marking a 17.9% advancement\ncompared to the residual graph neural network (ResGCN)-based approach.\n", "link": "http://arxiv.org/abs/2407.12647v1", "date": "2024-07-17", "relevancy": 2.564, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.544}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5103}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20Flow-enhanced%20Graph%20Pooling%20Residual%20Networks%20for%20Unmanned%20Aerial%0A%20%20Vehicles%20Surveillance%20in%20Day%20and%20Night%20Dual%20Visions&body=Title%3A%20Fusion%20Flow-enhanced%20Graph%20Pooling%20Residual%20Networks%20for%20Unmanned%20Aerial%0A%20%20Vehicles%20Surveillance%20in%20Day%20and%20Night%20Dual%20Visions%0AAuthor%3A%20Alam%20Noor%20and%20Kai%20Li%20and%20Eduardo%20Tovar%20and%20Pei%20Zhang%20and%20Bo%20Wei%0AAbstract%3A%20%20%20Recognizing%20unauthorized%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20within%20designated%0Ano-fly%20zones%20throughout%20the%20day%20and%20night%20is%20of%20paramount%20importance%2C%20where%20the%0Aunauthorized%20UAVs%20pose%20a%20substantial%20threat%20to%20both%20civil%20and%20military%20aviation%0Asafety.%20However%2C%20recognizing%20UAVs%20day%20and%20night%20with%20dual-vision%20cameras%20is%0Anontrivial%2C%20since%20red-green-blue%20%28RGB%29%20images%20suffer%20from%20a%20low%20detection%20rate%0Aunder%20an%20insufficient%20light%20condition%2C%20such%20as%20on%20cloudy%20or%20stormy%20days%2C%20while%0Ablack-and-white%20infrared%20%28IR%29%20images%20struggle%20to%20capture%20UAVs%20that%20overlap%20with%0Athe%20background%20at%20night.%20In%20this%20paper%2C%20we%20propose%20a%20new%20optical%20flow-assisted%0Agraph-pooling%20residual%20network%20%28OF-GPRN%29%2C%20which%20significantly%20enhances%20the%20UAV%0Adetection%20rate%20in%20day%20and%20night%20dual%20visions.%20The%20proposed%20OF-GPRN%20develops%20a%0Anew%20optical%20fusion%20to%20remove%20superfluous%20backgrounds%2C%20which%20improves%20RGB/IR%0Aimaging%20clarity.%20Furthermore%2C%20OF-GPRN%20extends%20optical%20fusion%20by%20incorporating%20a%0Agraph%20residual%20split%20attention%20network%20and%20a%20feature%20pyramid%2C%20which%20refines%20the%0Aperception%20of%20UAVs%2C%20leading%20to%20a%20higher%20success%20rate%20in%20UAV%20detection.%20A%0Acomprehensive%20performance%20evaluation%20is%20conducted%20using%20a%20benchmark%20UAV%20catch%0Adataset.%20The%20results%20indicate%20that%20the%20proposed%20OF-GPRN%20elevates%20the%20UAV%20mean%0Aaverage%20precision%20%28mAP%29%20detection%20rate%20to%2087.8%25%2C%20marking%20a%2017.9%25%20advancement%0Acompared%20to%20the%20residual%20graph%20neural%20network%20%28ResGCN%29-based%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520Flow-enhanced%2520Graph%2520Pooling%2520Residual%2520Networks%2520for%2520Unmanned%2520Aerial%250A%2520%2520Vehicles%2520Surveillance%2520in%2520Day%2520and%2520Night%2520Dual%2520Visions%26entry.906535625%3DAlam%2520Noor%2520and%2520Kai%2520Li%2520and%2520Eduardo%2520Tovar%2520and%2520Pei%2520Zhang%2520and%2520Bo%2520Wei%26entry.1292438233%3D%2520%2520Recognizing%2520unauthorized%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520within%2520designated%250Ano-fly%2520zones%2520throughout%2520the%2520day%2520and%2520night%2520is%2520of%2520paramount%2520importance%252C%2520where%2520the%250Aunauthorized%2520UAVs%2520pose%2520a%2520substantial%2520threat%2520to%2520both%2520civil%2520and%2520military%2520aviation%250Asafety.%2520However%252C%2520recognizing%2520UAVs%2520day%2520and%2520night%2520with%2520dual-vision%2520cameras%2520is%250Anontrivial%252C%2520since%2520red-green-blue%2520%2528RGB%2529%2520images%2520suffer%2520from%2520a%2520low%2520detection%2520rate%250Aunder%2520an%2520insufficient%2520light%2520condition%252C%2520such%2520as%2520on%2520cloudy%2520or%2520stormy%2520days%252C%2520while%250Ablack-and-white%2520infrared%2520%2528IR%2529%2520images%2520struggle%2520to%2520capture%2520UAVs%2520that%2520overlap%2520with%250Athe%2520background%2520at%2520night.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520optical%2520flow-assisted%250Agraph-pooling%2520residual%2520network%2520%2528OF-GPRN%2529%252C%2520which%2520significantly%2520enhances%2520the%2520UAV%250Adetection%2520rate%2520in%2520day%2520and%2520night%2520dual%2520visions.%2520The%2520proposed%2520OF-GPRN%2520develops%2520a%250Anew%2520optical%2520fusion%2520to%2520remove%2520superfluous%2520backgrounds%252C%2520which%2520improves%2520RGB/IR%250Aimaging%2520clarity.%2520Furthermore%252C%2520OF-GPRN%2520extends%2520optical%2520fusion%2520by%2520incorporating%2520a%250Agraph%2520residual%2520split%2520attention%2520network%2520and%2520a%2520feature%2520pyramid%252C%2520which%2520refines%2520the%250Aperception%2520of%2520UAVs%252C%2520leading%2520to%2520a%2520higher%2520success%2520rate%2520in%2520UAV%2520detection.%2520A%250Acomprehensive%2520performance%2520evaluation%2520is%2520conducted%2520using%2520a%2520benchmark%2520UAV%2520catch%250Adataset.%2520The%2520results%2520indicate%2520that%2520the%2520proposed%2520OF-GPRN%2520elevates%2520the%2520UAV%2520mean%250Aaverage%2520precision%2520%2528mAP%2529%2520detection%2520rate%2520to%252087.8%2525%252C%2520marking%2520a%252017.9%2525%2520advancement%250Acompared%2520to%2520the%2520residual%2520graph%2520neural%2520network%2520%2528ResGCN%2529-based%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20Flow-enhanced%20Graph%20Pooling%20Residual%20Networks%20for%20Unmanned%20Aerial%0A%20%20Vehicles%20Surveillance%20in%20Day%20and%20Night%20Dual%20Visions&entry.906535625=Alam%20Noor%20and%20Kai%20Li%20and%20Eduardo%20Tovar%20and%20Pei%20Zhang%20and%20Bo%20Wei&entry.1292438233=%20%20Recognizing%20unauthorized%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20within%20designated%0Ano-fly%20zones%20throughout%20the%20day%20and%20night%20is%20of%20paramount%20importance%2C%20where%20the%0Aunauthorized%20UAVs%20pose%20a%20substantial%20threat%20to%20both%20civil%20and%20military%20aviation%0Asafety.%20However%2C%20recognizing%20UAVs%20day%20and%20night%20with%20dual-vision%20cameras%20is%0Anontrivial%2C%20since%20red-green-blue%20%28RGB%29%20images%20suffer%20from%20a%20low%20detection%20rate%0Aunder%20an%20insufficient%20light%20condition%2C%20such%20as%20on%20cloudy%20or%20stormy%20days%2C%20while%0Ablack-and-white%20infrared%20%28IR%29%20images%20struggle%20to%20capture%20UAVs%20that%20overlap%20with%0Athe%20background%20at%20night.%20In%20this%20paper%2C%20we%20propose%20a%20new%20optical%20flow-assisted%0Agraph-pooling%20residual%20network%20%28OF-GPRN%29%2C%20which%20significantly%20enhances%20the%20UAV%0Adetection%20rate%20in%20day%20and%20night%20dual%20visions.%20The%20proposed%20OF-GPRN%20develops%20a%0Anew%20optical%20fusion%20to%20remove%20superfluous%20backgrounds%2C%20which%20improves%20RGB/IR%0Aimaging%20clarity.%20Furthermore%2C%20OF-GPRN%20extends%20optical%20fusion%20by%20incorporating%20a%0Agraph%20residual%20split%20attention%20network%20and%20a%20feature%20pyramid%2C%20which%20refines%20the%0Aperception%20of%20UAVs%2C%20leading%20to%20a%20higher%20success%20rate%20in%20UAV%20detection.%20A%0Acomprehensive%20performance%20evaluation%20is%20conducted%20using%20a%20benchmark%20UAV%20catch%0Adataset.%20The%20results%20indicate%20that%20the%20proposed%20OF-GPRN%20elevates%20the%20UAV%20mean%0Aaverage%20precision%20%28mAP%29%20detection%20rate%20to%2087.8%25%2C%20marking%20a%2017.9%25%20advancement%0Acompared%20to%20the%20residual%20graph%20neural%20network%20%28ResGCN%29-based%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12647v1&entry.124074799=Read"},
{"title": "On Diversity in Discriminative Neural Networks", "author": "Brahim Oubaha and Claude Berrou and Xueyao Ji and Yehya Nasser and Rapha\u00ebl Le Bidan", "abstract": "  Diversity is a concept of prime importance in almost all disciplines based on\ninformation processing. In telecommunications, for example, spatial, temporal,\nand frequency diversity, as well as redundant coding, are fundamental concepts\nthat have enabled the design of extremely efficient systems. In machine\nlearning, in particular with neural networks, diversity is not always a concept\nthat is emphasized or at least clearly identified. This paper proposes a neural\nnetwork architecture that builds upon various diversity principles, some of\nthem already known, others more original. Our architecture obtains remarkable\nresults, with a record self-supervised learning accuracy of 99. 57% in MNIST,\nand a top tier promising semi-supervised learning accuracy of 94.21% in\nCIFAR-10 using only 25 labels per class.\n", "link": "http://arxiv.org/abs/2407.12599v1", "date": "2024-07-17", "relevancy": 2.5025, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5335}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4917}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Diversity%20in%20Discriminative%20Neural%20Networks&body=Title%3A%20On%20Diversity%20in%20Discriminative%20Neural%20Networks%0AAuthor%3A%20Brahim%20Oubaha%20and%20Claude%20Berrou%20and%20Xueyao%20Ji%20and%20Yehya%20Nasser%20and%20Rapha%C3%ABl%20Le%20Bidan%0AAbstract%3A%20%20%20Diversity%20is%20a%20concept%20of%20prime%20importance%20in%20almost%20all%20disciplines%20based%20on%0Ainformation%20processing.%20In%20telecommunications%2C%20for%20example%2C%20spatial%2C%20temporal%2C%0Aand%20frequency%20diversity%2C%20as%20well%20as%20redundant%20coding%2C%20are%20fundamental%20concepts%0Athat%20have%20enabled%20the%20design%20of%20extremely%20efficient%20systems.%20In%20machine%0Alearning%2C%20in%20particular%20with%20neural%20networks%2C%20diversity%20is%20not%20always%20a%20concept%0Athat%20is%20emphasized%20or%20at%20least%20clearly%20identified.%20This%20paper%20proposes%20a%20neural%0Anetwork%20architecture%20that%20builds%20upon%20various%20diversity%20principles%2C%20some%20of%0Athem%20already%20known%2C%20others%20more%20original.%20Our%20architecture%20obtains%20remarkable%0Aresults%2C%20with%20a%20record%20self-supervised%20learning%20accuracy%20of%2099.%2057%25%20in%20MNIST%2C%0Aand%20a%20top%20tier%20promising%20semi-supervised%20learning%20accuracy%20of%2094.21%25%20in%0ACIFAR-10%20using%20only%2025%20labels%20per%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Diversity%2520in%2520Discriminative%2520Neural%2520Networks%26entry.906535625%3DBrahim%2520Oubaha%2520and%2520Claude%2520Berrou%2520and%2520Xueyao%2520Ji%2520and%2520Yehya%2520Nasser%2520and%2520Rapha%25C3%25ABl%2520Le%2520Bidan%26entry.1292438233%3D%2520%2520Diversity%2520is%2520a%2520concept%2520of%2520prime%2520importance%2520in%2520almost%2520all%2520disciplines%2520based%2520on%250Ainformation%2520processing.%2520In%2520telecommunications%252C%2520for%2520example%252C%2520spatial%252C%2520temporal%252C%250Aand%2520frequency%2520diversity%252C%2520as%2520well%2520as%2520redundant%2520coding%252C%2520are%2520fundamental%2520concepts%250Athat%2520have%2520enabled%2520the%2520design%2520of%2520extremely%2520efficient%2520systems.%2520In%2520machine%250Alearning%252C%2520in%2520particular%2520with%2520neural%2520networks%252C%2520diversity%2520is%2520not%2520always%2520a%2520concept%250Athat%2520is%2520emphasized%2520or%2520at%2520least%2520clearly%2520identified.%2520This%2520paper%2520proposes%2520a%2520neural%250Anetwork%2520architecture%2520that%2520builds%2520upon%2520various%2520diversity%2520principles%252C%2520some%2520of%250Athem%2520already%2520known%252C%2520others%2520more%2520original.%2520Our%2520architecture%2520obtains%2520remarkable%250Aresults%252C%2520with%2520a%2520record%2520self-supervised%2520learning%2520accuracy%2520of%252099.%252057%2525%2520in%2520MNIST%252C%250Aand%2520a%2520top%2520tier%2520promising%2520semi-supervised%2520learning%2520accuracy%2520of%252094.21%2525%2520in%250ACIFAR-10%2520using%2520only%252025%2520labels%2520per%2520class.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Diversity%20in%20Discriminative%20Neural%20Networks&entry.906535625=Brahim%20Oubaha%20and%20Claude%20Berrou%20and%20Xueyao%20Ji%20and%20Yehya%20Nasser%20and%20Rapha%C3%ABl%20Le%20Bidan&entry.1292438233=%20%20Diversity%20is%20a%20concept%20of%20prime%20importance%20in%20almost%20all%20disciplines%20based%20on%0Ainformation%20processing.%20In%20telecommunications%2C%20for%20example%2C%20spatial%2C%20temporal%2C%0Aand%20frequency%20diversity%2C%20as%20well%20as%20redundant%20coding%2C%20are%20fundamental%20concepts%0Athat%20have%20enabled%20the%20design%20of%20extremely%20efficient%20systems.%20In%20machine%0Alearning%2C%20in%20particular%20with%20neural%20networks%2C%20diversity%20is%20not%20always%20a%20concept%0Athat%20is%20emphasized%20or%20at%20least%20clearly%20identified.%20This%20paper%20proposes%20a%20neural%0Anetwork%20architecture%20that%20builds%20upon%20various%20diversity%20principles%2C%20some%20of%0Athem%20already%20known%2C%20others%20more%20original.%20Our%20architecture%20obtains%20remarkable%0Aresults%2C%20with%20a%20record%20self-supervised%20learning%20accuracy%20of%2099.%2057%25%20in%20MNIST%2C%0Aand%20a%20top%20tier%20promising%20semi-supervised%20learning%20accuracy%20of%2094.21%25%20in%0ACIFAR-10%20using%20only%2025%20labels%20per%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12599v1&entry.124074799=Read"},
{"title": "GIVT: Generative Infinite-Vocabulary Transformers", "author": "Michael Tschannen and Cian Eastwood and Fabian Mentzer", "abstract": "  We introduce Generative Infinite-Vocabulary Transformers (GIVT) which\ngenerate vector sequences with real-valued entries, instead of discrete tokens\nfrom a finite vocabulary. To this end, we propose two surprisingly simple\nmodifications to decoder-only transformers: 1) at the input, we replace the\nfinite-vocabulary lookup table with a linear projection of the input vectors;\nand 2) at the output, we replace the logits prediction (usually mapped to a\ncategorical distribution) with the parameters of a multivariate Gaussian\nmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,\nwhere transformers are used to model the discrete latent sequences of a VQ-VAE,\nwe use GIVT to model the unquantized real-valued latent sequences of a\n$\\beta$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and\nimproved variants thereof) as well as MaskGIT, and achieves performance\ncompetitive with recent latent diffusion models. Finally, we obtain strong\nresults outside of image generation when applying GIVT to panoptic segmentation\nand depth estimation with a VAE variant of the UViM framework.\n", "link": "http://arxiv.org/abs/2312.02116v4", "date": "2024-07-17", "relevancy": 2.4348, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6181}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6037}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIVT%3A%20Generative%20Infinite-Vocabulary%20Transformers&body=Title%3A%20GIVT%3A%20Generative%20Infinite-Vocabulary%20Transformers%0AAuthor%3A%20Michael%20Tschannen%20and%20Cian%20Eastwood%20and%20Fabian%20Mentzer%0AAbstract%3A%20%20%20We%20introduce%20Generative%20Infinite-Vocabulary%20Transformers%20%28GIVT%29%20which%0Agenerate%20vector%20sequences%20with%20real-valued%20entries%2C%20instead%20of%20discrete%20tokens%0Afrom%20a%20finite%20vocabulary.%20To%20this%20end%2C%20we%20propose%20two%20surprisingly%20simple%0Amodifications%20to%20decoder-only%20transformers%3A%201%29%20at%20the%20input%2C%20we%20replace%20the%0Afinite-vocabulary%20lookup%20table%20with%20a%20linear%20projection%20of%20the%20input%20vectors%3B%0Aand%202%29%20at%20the%20output%2C%20we%20replace%20the%20logits%20prediction%20%28usually%20mapped%20to%20a%0Acategorical%20distribution%29%20with%20the%20parameters%20of%20a%20multivariate%20Gaussian%0Amixture%20model.%20Inspired%20by%20the%20image-generation%20paradigm%20of%20VQ-GAN%20and%20MaskGIT%2C%0Awhere%20transformers%20are%20used%20to%20model%20the%20discrete%20latent%20sequences%20of%20a%20VQ-VAE%2C%0Awe%20use%20GIVT%20to%20model%20the%20unquantized%20real-valued%20latent%20sequences%20of%20a%0A%24%5Cbeta%24-VAE.%20In%20class-conditional%20image%20generation%20GIVT%20outperforms%20VQ-GAN%20%28and%0Aimproved%20variants%20thereof%29%20as%20well%20as%20MaskGIT%2C%20and%20achieves%20performance%0Acompetitive%20with%20recent%20latent%20diffusion%20models.%20Finally%2C%20we%20obtain%20strong%0Aresults%20outside%20of%20image%20generation%20when%20applying%20GIVT%20to%20panoptic%20segmentation%0Aand%20depth%20estimation%20with%20a%20VAE%20variant%20of%20the%20UViM%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02116v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIVT%253A%2520Generative%2520Infinite-Vocabulary%2520Transformers%26entry.906535625%3DMichael%2520Tschannen%2520and%2520Cian%2520Eastwood%2520and%2520Fabian%2520Mentzer%26entry.1292438233%3D%2520%2520We%2520introduce%2520Generative%2520Infinite-Vocabulary%2520Transformers%2520%2528GIVT%2529%2520which%250Agenerate%2520vector%2520sequences%2520with%2520real-valued%2520entries%252C%2520instead%2520of%2520discrete%2520tokens%250Afrom%2520a%2520finite%2520vocabulary.%2520To%2520this%2520end%252C%2520we%2520propose%2520two%2520surprisingly%2520simple%250Amodifications%2520to%2520decoder-only%2520transformers%253A%25201%2529%2520at%2520the%2520input%252C%2520we%2520replace%2520the%250Afinite-vocabulary%2520lookup%2520table%2520with%2520a%2520linear%2520projection%2520of%2520the%2520input%2520vectors%253B%250Aand%25202%2529%2520at%2520the%2520output%252C%2520we%2520replace%2520the%2520logits%2520prediction%2520%2528usually%2520mapped%2520to%2520a%250Acategorical%2520distribution%2529%2520with%2520the%2520parameters%2520of%2520a%2520multivariate%2520Gaussian%250Amixture%2520model.%2520Inspired%2520by%2520the%2520image-generation%2520paradigm%2520of%2520VQ-GAN%2520and%2520MaskGIT%252C%250Awhere%2520transformers%2520are%2520used%2520to%2520model%2520the%2520discrete%2520latent%2520sequences%2520of%2520a%2520VQ-VAE%252C%250Awe%2520use%2520GIVT%2520to%2520model%2520the%2520unquantized%2520real-valued%2520latent%2520sequences%2520of%2520a%250A%2524%255Cbeta%2524-VAE.%2520In%2520class-conditional%2520image%2520generation%2520GIVT%2520outperforms%2520VQ-GAN%2520%2528and%250Aimproved%2520variants%2520thereof%2529%2520as%2520well%2520as%2520MaskGIT%252C%2520and%2520achieves%2520performance%250Acompetitive%2520with%2520recent%2520latent%2520diffusion%2520models.%2520Finally%252C%2520we%2520obtain%2520strong%250Aresults%2520outside%2520of%2520image%2520generation%2520when%2520applying%2520GIVT%2520to%2520panoptic%2520segmentation%250Aand%2520depth%2520estimation%2520with%2520a%2520VAE%2520variant%2520of%2520the%2520UViM%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02116v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIVT%3A%20Generative%20Infinite-Vocabulary%20Transformers&entry.906535625=Michael%20Tschannen%20and%20Cian%20Eastwood%20and%20Fabian%20Mentzer&entry.1292438233=%20%20We%20introduce%20Generative%20Infinite-Vocabulary%20Transformers%20%28GIVT%29%20which%0Agenerate%20vector%20sequences%20with%20real-valued%20entries%2C%20instead%20of%20discrete%20tokens%0Afrom%20a%20finite%20vocabulary.%20To%20this%20end%2C%20we%20propose%20two%20surprisingly%20simple%0Amodifications%20to%20decoder-only%20transformers%3A%201%29%20at%20the%20input%2C%20we%20replace%20the%0Afinite-vocabulary%20lookup%20table%20with%20a%20linear%20projection%20of%20the%20input%20vectors%3B%0Aand%202%29%20at%20the%20output%2C%20we%20replace%20the%20logits%20prediction%20%28usually%20mapped%20to%20a%0Acategorical%20distribution%29%20with%20the%20parameters%20of%20a%20multivariate%20Gaussian%0Amixture%20model.%20Inspired%20by%20the%20image-generation%20paradigm%20of%20VQ-GAN%20and%20MaskGIT%2C%0Awhere%20transformers%20are%20used%20to%20model%20the%20discrete%20latent%20sequences%20of%20a%20VQ-VAE%2C%0Awe%20use%20GIVT%20to%20model%20the%20unquantized%20real-valued%20latent%20sequences%20of%20a%0A%24%5Cbeta%24-VAE.%20In%20class-conditional%20image%20generation%20GIVT%20outperforms%20VQ-GAN%20%28and%0Aimproved%20variants%20thereof%29%20as%20well%20as%20MaskGIT%2C%20and%20achieves%20performance%0Acompetitive%20with%20recent%20latent%20diffusion%20models.%20Finally%2C%20we%20obtain%20strong%0Aresults%20outside%20of%20image%20generation%20when%20applying%20GIVT%20to%20panoptic%20segmentation%0Aand%20depth%20estimation%20with%20a%20VAE%20variant%20of%20the%20UViM%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02116v4&entry.124074799=Read"},
{"title": "In-Situ Infrared Camera Monitoring for Defect and Anomaly Detection in\n  Laser Powder Bed Fusion: Calibration, Data Mapping, and Feature Extraction", "author": "Shawn Hinnebusch and David Anderson and Berkay Bostan and Albert C. To", "abstract": "  Laser powder bed fusion (LPBF) process can incur defects due to melt pool\ninstabilities, spattering, temperature increase, and powder spread anomalies.\nIdentifying defects through in-situ monitoring typically requires collecting,\nstoring, and analyzing large amounts of data generated. The first goal of this\nwork is to propose a new approach to accurately map in-situ data to a\nthree-dimensional (3D) geometry, aiming to reduce the amount of storage. The\nsecond goal of this work is to introduce several new IR features for defect\ndetection or process model calibration, which include laser scan order, local\npreheat temperature, maximum pre-laser scanning temperature, and number of\nspatters generated locally and their landing locations. For completeness,\nprocessing of other common IR features, such as interpass temperature, heat\nintensity, cooling rates, and melt pool area, are also presented with the\nunderlying algorithm and Python implementation. A number of different parts are\nprinted, monitored, and characterized to provide evidence of process defects\nand anomalies that different IR features are capable of detecting.\n", "link": "http://arxiv.org/abs/2407.12682v1", "date": "2024-07-17", "relevancy": 2.4215, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4997}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.495}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Situ%20Infrared%20Camera%20Monitoring%20for%20Defect%20and%20Anomaly%20Detection%20in%0A%20%20Laser%20Powder%20Bed%20Fusion%3A%20Calibration%2C%20Data%20Mapping%2C%20and%20Feature%20Extraction&body=Title%3A%20In-Situ%20Infrared%20Camera%20Monitoring%20for%20Defect%20and%20Anomaly%20Detection%20in%0A%20%20Laser%20Powder%20Bed%20Fusion%3A%20Calibration%2C%20Data%20Mapping%2C%20and%20Feature%20Extraction%0AAuthor%3A%20Shawn%20Hinnebusch%20and%20David%20Anderson%20and%20Berkay%20Bostan%20and%20Albert%20C.%20To%0AAbstract%3A%20%20%20Laser%20powder%20bed%20fusion%20%28LPBF%29%20process%20can%20incur%20defects%20due%20to%20melt%20pool%0Ainstabilities%2C%20spattering%2C%20temperature%20increase%2C%20and%20powder%20spread%20anomalies.%0AIdentifying%20defects%20through%20in-situ%20monitoring%20typically%20requires%20collecting%2C%0Astoring%2C%20and%20analyzing%20large%20amounts%20of%20data%20generated.%20The%20first%20goal%20of%20this%0Awork%20is%20to%20propose%20a%20new%20approach%20to%20accurately%20map%20in-situ%20data%20to%20a%0Athree-dimensional%20%283D%29%20geometry%2C%20aiming%20to%20reduce%20the%20amount%20of%20storage.%20The%0Asecond%20goal%20of%20this%20work%20is%20to%20introduce%20several%20new%20IR%20features%20for%20defect%0Adetection%20or%20process%20model%20calibration%2C%20which%20include%20laser%20scan%20order%2C%20local%0Apreheat%20temperature%2C%20maximum%20pre-laser%20scanning%20temperature%2C%20and%20number%20of%0Aspatters%20generated%20locally%20and%20their%20landing%20locations.%20For%20completeness%2C%0Aprocessing%20of%20other%20common%20IR%20features%2C%20such%20as%20interpass%20temperature%2C%20heat%0Aintensity%2C%20cooling%20rates%2C%20and%20melt%20pool%20area%2C%20are%20also%20presented%20with%20the%0Aunderlying%20algorithm%20and%20Python%20implementation.%20A%20number%20of%20different%20parts%20are%0Aprinted%2C%20monitored%2C%20and%20characterized%20to%20provide%20evidence%20of%20process%20defects%0Aand%20anomalies%20that%20different%20IR%20features%20are%20capable%20of%20detecting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Situ%2520Infrared%2520Camera%2520Monitoring%2520for%2520Defect%2520and%2520Anomaly%2520Detection%2520in%250A%2520%2520Laser%2520Powder%2520Bed%2520Fusion%253A%2520Calibration%252C%2520Data%2520Mapping%252C%2520and%2520Feature%2520Extraction%26entry.906535625%3DShawn%2520Hinnebusch%2520and%2520David%2520Anderson%2520and%2520Berkay%2520Bostan%2520and%2520Albert%2520C.%2520To%26entry.1292438233%3D%2520%2520Laser%2520powder%2520bed%2520fusion%2520%2528LPBF%2529%2520process%2520can%2520incur%2520defects%2520due%2520to%2520melt%2520pool%250Ainstabilities%252C%2520spattering%252C%2520temperature%2520increase%252C%2520and%2520powder%2520spread%2520anomalies.%250AIdentifying%2520defects%2520through%2520in-situ%2520monitoring%2520typically%2520requires%2520collecting%252C%250Astoring%252C%2520and%2520analyzing%2520large%2520amounts%2520of%2520data%2520generated.%2520The%2520first%2520goal%2520of%2520this%250Awork%2520is%2520to%2520propose%2520a%2520new%2520approach%2520to%2520accurately%2520map%2520in-situ%2520data%2520to%2520a%250Athree-dimensional%2520%25283D%2529%2520geometry%252C%2520aiming%2520to%2520reduce%2520the%2520amount%2520of%2520storage.%2520The%250Asecond%2520goal%2520of%2520this%2520work%2520is%2520to%2520introduce%2520several%2520new%2520IR%2520features%2520for%2520defect%250Adetection%2520or%2520process%2520model%2520calibration%252C%2520which%2520include%2520laser%2520scan%2520order%252C%2520local%250Apreheat%2520temperature%252C%2520maximum%2520pre-laser%2520scanning%2520temperature%252C%2520and%2520number%2520of%250Aspatters%2520generated%2520locally%2520and%2520their%2520landing%2520locations.%2520For%2520completeness%252C%250Aprocessing%2520of%2520other%2520common%2520IR%2520features%252C%2520such%2520as%2520interpass%2520temperature%252C%2520heat%250Aintensity%252C%2520cooling%2520rates%252C%2520and%2520melt%2520pool%2520area%252C%2520are%2520also%2520presented%2520with%2520the%250Aunderlying%2520algorithm%2520and%2520Python%2520implementation.%2520A%2520number%2520of%2520different%2520parts%2520are%250Aprinted%252C%2520monitored%252C%2520and%2520characterized%2520to%2520provide%2520evidence%2520of%2520process%2520defects%250Aand%2520anomalies%2520that%2520different%2520IR%2520features%2520are%2520capable%2520of%2520detecting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Situ%20Infrared%20Camera%20Monitoring%20for%20Defect%20and%20Anomaly%20Detection%20in%0A%20%20Laser%20Powder%20Bed%20Fusion%3A%20Calibration%2C%20Data%20Mapping%2C%20and%20Feature%20Extraction&entry.906535625=Shawn%20Hinnebusch%20and%20David%20Anderson%20and%20Berkay%20Bostan%20and%20Albert%20C.%20To&entry.1292438233=%20%20Laser%20powder%20bed%20fusion%20%28LPBF%29%20process%20can%20incur%20defects%20due%20to%20melt%20pool%0Ainstabilities%2C%20spattering%2C%20temperature%20increase%2C%20and%20powder%20spread%20anomalies.%0AIdentifying%20defects%20through%20in-situ%20monitoring%20typically%20requires%20collecting%2C%0Astoring%2C%20and%20analyzing%20large%20amounts%20of%20data%20generated.%20The%20first%20goal%20of%20this%0Awork%20is%20to%20propose%20a%20new%20approach%20to%20accurately%20map%20in-situ%20data%20to%20a%0Athree-dimensional%20%283D%29%20geometry%2C%20aiming%20to%20reduce%20the%20amount%20of%20storage.%20The%0Asecond%20goal%20of%20this%20work%20is%20to%20introduce%20several%20new%20IR%20features%20for%20defect%0Adetection%20or%20process%20model%20calibration%2C%20which%20include%20laser%20scan%20order%2C%20local%0Apreheat%20temperature%2C%20maximum%20pre-laser%20scanning%20temperature%2C%20and%20number%20of%0Aspatters%20generated%20locally%20and%20their%20landing%20locations.%20For%20completeness%2C%0Aprocessing%20of%20other%20common%20IR%20features%2C%20such%20as%20interpass%20temperature%2C%20heat%0Aintensity%2C%20cooling%20rates%2C%20and%20melt%20pool%20area%2C%20are%20also%20presented%20with%20the%0Aunderlying%20algorithm%20and%20Python%20implementation.%20A%20number%20of%20different%20parts%20are%0Aprinted%2C%20monitored%2C%20and%20characterized%20to%20provide%20evidence%20of%20process%20defects%0Aand%20anomalies%20that%20different%20IR%20features%20are%20capable%20of%20detecting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12682v1&entry.124074799=Read"},
{"title": "Exploring the Untouched Sweeps for Conflict-Aware 3D Segmentation\n  Pretraining", "author": "Tianfang Sun and Zhizhong Zhang and Xin Tan and Yanyun Qu and Yuan Xie", "abstract": "  LiDAR-camera 3D representation pretraining has shown significant promise for\n3D perception tasks and related applications. However, two issues widely exist\nin this framework: 1) Solely keyframes are used for training. For example, in\nnuScenes, a substantial quantity of unpaired LiDAR and camera frames remain\nunutilized, limiting the representation capabilities of the pretrained network.\n2) The contrastive loss erroneously distances points and image regions with\nidentical semantics but from different frames, disturbing the semantic\nconsistency of the learned presentations. In this paper, we propose a novel\nVision-Foundation-Model-driven sample exploring module to meticulously select\nLiDAR-Image pairs from unexplored frames, enriching the original training set.\nWe utilized timestamps and the semantic priors from VFMs to identify\nwell-synchronized training pairs and to discover samples with diverse content.\nMoreover, we design a cross- and intra-modal conflict-aware contrastive loss\nusing the semantic mask labels of VFMs to avoid contrasting semantically\nsimilar points and image regions. Our method consistently outperforms existing\nstate-of-the-art pretraining frameworks across three major public autonomous\ndriving datasets: nuScenes, SemanticKITTI, and Waymo on 3D semantic\nsegmentation by +3.0\\%, +3.0\\%, and +3.3\\% in mIoU, respectively. Furthermore,\nour approach exhibits adaptable generalization to different 3D backbones and\ntypical semantic masks generated by non-VFM models.\n", "link": "http://arxiv.org/abs/2407.07465v2", "date": "2024-07-17", "relevancy": 2.3691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5976}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5894}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Untouched%20Sweeps%20for%20Conflict-Aware%203D%20Segmentation%0A%20%20Pretraining&body=Title%3A%20Exploring%20the%20Untouched%20Sweeps%20for%20Conflict-Aware%203D%20Segmentation%0A%20%20Pretraining%0AAuthor%3A%20Tianfang%20Sun%20and%20Zhizhong%20Zhang%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Yuan%20Xie%0AAbstract%3A%20%20%20LiDAR-camera%203D%20representation%20pretraining%20has%20shown%20significant%20promise%20for%0A3D%20perception%20tasks%20and%20related%20applications.%20However%2C%20two%20issues%20widely%20exist%0Ain%20this%20framework%3A%201%29%20Solely%20keyframes%20are%20used%20for%20training.%20For%20example%2C%20in%0AnuScenes%2C%20a%20substantial%20quantity%20of%20unpaired%20LiDAR%20and%20camera%20frames%20remain%0Aunutilized%2C%20limiting%20the%20representation%20capabilities%20of%20the%20pretrained%20network.%0A2%29%20The%20contrastive%20loss%20erroneously%20distances%20points%20and%20image%20regions%20with%0Aidentical%20semantics%20but%20from%20different%20frames%2C%20disturbing%20the%20semantic%0Aconsistency%20of%20the%20learned%20presentations.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AVision-Foundation-Model-driven%20sample%20exploring%20module%20to%20meticulously%20select%0ALiDAR-Image%20pairs%20from%20unexplored%20frames%2C%20enriching%20the%20original%20training%20set.%0AWe%20utilized%20timestamps%20and%20the%20semantic%20priors%20from%20VFMs%20to%20identify%0Awell-synchronized%20training%20pairs%20and%20to%20discover%20samples%20with%20diverse%20content.%0AMoreover%2C%20we%20design%20a%20cross-%20and%20intra-modal%20conflict-aware%20contrastive%20loss%0Ausing%20the%20semantic%20mask%20labels%20of%20VFMs%20to%20avoid%20contrasting%20semantically%0Asimilar%20points%20and%20image%20regions.%20Our%20method%20consistently%20outperforms%20existing%0Astate-of-the-art%20pretraining%20frameworks%20across%20three%20major%20public%20autonomous%0Adriving%20datasets%3A%20nuScenes%2C%20SemanticKITTI%2C%20and%20Waymo%20on%203D%20semantic%0Asegmentation%20by%20%2B3.0%5C%25%2C%20%2B3.0%5C%25%2C%20and%20%2B3.3%5C%25%20in%20mIoU%2C%20respectively.%20Furthermore%2C%0Aour%20approach%20exhibits%20adaptable%20generalization%20to%20different%203D%20backbones%20and%0Atypical%20semantic%20masks%20generated%20by%20non-VFM%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Untouched%2520Sweeps%2520for%2520Conflict-Aware%25203D%2520Segmentation%250A%2520%2520Pretraining%26entry.906535625%3DTianfang%2520Sun%2520and%2520Zhizhong%2520Zhang%2520and%2520Xin%2520Tan%2520and%2520Yanyun%2520Qu%2520and%2520Yuan%2520Xie%26entry.1292438233%3D%2520%2520LiDAR-camera%25203D%2520representation%2520pretraining%2520has%2520shown%2520significant%2520promise%2520for%250A3D%2520perception%2520tasks%2520and%2520related%2520applications.%2520However%252C%2520two%2520issues%2520widely%2520exist%250Ain%2520this%2520framework%253A%25201%2529%2520Solely%2520keyframes%2520are%2520used%2520for%2520training.%2520For%2520example%252C%2520in%250AnuScenes%252C%2520a%2520substantial%2520quantity%2520of%2520unpaired%2520LiDAR%2520and%2520camera%2520frames%2520remain%250Aunutilized%252C%2520limiting%2520the%2520representation%2520capabilities%2520of%2520the%2520pretrained%2520network.%250A2%2529%2520The%2520contrastive%2520loss%2520erroneously%2520distances%2520points%2520and%2520image%2520regions%2520with%250Aidentical%2520semantics%2520but%2520from%2520different%2520frames%252C%2520disturbing%2520the%2520semantic%250Aconsistency%2520of%2520the%2520learned%2520presentations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250AVision-Foundation-Model-driven%2520sample%2520exploring%2520module%2520to%2520meticulously%2520select%250ALiDAR-Image%2520pairs%2520from%2520unexplored%2520frames%252C%2520enriching%2520the%2520original%2520training%2520set.%250AWe%2520utilized%2520timestamps%2520and%2520the%2520semantic%2520priors%2520from%2520VFMs%2520to%2520identify%250Awell-synchronized%2520training%2520pairs%2520and%2520to%2520discover%2520samples%2520with%2520diverse%2520content.%250AMoreover%252C%2520we%2520design%2520a%2520cross-%2520and%2520intra-modal%2520conflict-aware%2520contrastive%2520loss%250Ausing%2520the%2520semantic%2520mask%2520labels%2520of%2520VFMs%2520to%2520avoid%2520contrasting%2520semantically%250Asimilar%2520points%2520and%2520image%2520regions.%2520Our%2520method%2520consistently%2520outperforms%2520existing%250Astate-of-the-art%2520pretraining%2520frameworks%2520across%2520three%2520major%2520public%2520autonomous%250Adriving%2520datasets%253A%2520nuScenes%252C%2520SemanticKITTI%252C%2520and%2520Waymo%2520on%25203D%2520semantic%250Asegmentation%2520by%2520%252B3.0%255C%2525%252C%2520%252B3.0%255C%2525%252C%2520and%2520%252B3.3%255C%2525%2520in%2520mIoU%252C%2520respectively.%2520Furthermore%252C%250Aour%2520approach%2520exhibits%2520adaptable%2520generalization%2520to%2520different%25203D%2520backbones%2520and%250Atypical%2520semantic%2520masks%2520generated%2520by%2520non-VFM%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Untouched%20Sweeps%20for%20Conflict-Aware%203D%20Segmentation%0A%20%20Pretraining&entry.906535625=Tianfang%20Sun%20and%20Zhizhong%20Zhang%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Yuan%20Xie&entry.1292438233=%20%20LiDAR-camera%203D%20representation%20pretraining%20has%20shown%20significant%20promise%20for%0A3D%20perception%20tasks%20and%20related%20applications.%20However%2C%20two%20issues%20widely%20exist%0Ain%20this%20framework%3A%201%29%20Solely%20keyframes%20are%20used%20for%20training.%20For%20example%2C%20in%0AnuScenes%2C%20a%20substantial%20quantity%20of%20unpaired%20LiDAR%20and%20camera%20frames%20remain%0Aunutilized%2C%20limiting%20the%20representation%20capabilities%20of%20the%20pretrained%20network.%0A2%29%20The%20contrastive%20loss%20erroneously%20distances%20points%20and%20image%20regions%20with%0Aidentical%20semantics%20but%20from%20different%20frames%2C%20disturbing%20the%20semantic%0Aconsistency%20of%20the%20learned%20presentations.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AVision-Foundation-Model-driven%20sample%20exploring%20module%20to%20meticulously%20select%0ALiDAR-Image%20pairs%20from%20unexplored%20frames%2C%20enriching%20the%20original%20training%20set.%0AWe%20utilized%20timestamps%20and%20the%20semantic%20priors%20from%20VFMs%20to%20identify%0Awell-synchronized%20training%20pairs%20and%20to%20discover%20samples%20with%20diverse%20content.%0AMoreover%2C%20we%20design%20a%20cross-%20and%20intra-modal%20conflict-aware%20contrastive%20loss%0Ausing%20the%20semantic%20mask%20labels%20of%20VFMs%20to%20avoid%20contrasting%20semantically%0Asimilar%20points%20and%20image%20regions.%20Our%20method%20consistently%20outperforms%20existing%0Astate-of-the-art%20pretraining%20frameworks%20across%20three%20major%20public%20autonomous%0Adriving%20datasets%3A%20nuScenes%2C%20SemanticKITTI%2C%20and%20Waymo%20on%203D%20semantic%0Asegmentation%20by%20%2B3.0%5C%25%2C%20%2B3.0%5C%25%2C%20and%20%2B3.3%5C%25%20in%20mIoU%2C%20respectively.%20Furthermore%2C%0Aour%20approach%20exhibits%20adaptable%20generalization%20to%20different%203D%20backbones%20and%0Atypical%20semantic%20masks%20generated%20by%20non-VFM%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07465v2&entry.124074799=Read"},
{"title": "Similarity of Neural Architectures using Adversarial Attack\n  Transferability", "author": "Jaehui Hwang and Dongyoon Han and Byeongho Heo and Song Park and Sanghyuk Chun and Jong-Seok Lee", "abstract": "  In recent years, many deep neural architectures have been developed for image\nclassification. Whether they are similar or dissimilar and what factors\ncontribute to their (dis)similarities remains curious. To address this\nquestion, we aim to design a quantitative and scalable similarity measure\nbetween neural architectures. We propose Similarity by Attack Transferability\n(SAT) from the observation that adversarial attack transferability contains\ninformation related to input gradients and decision boundaries widely used to\nunderstand model behaviors. We conduct a large-scale analysis on 69\nstate-of-the-art ImageNet classifiers using our proposed similarity function to\nanswer the question. Moreover, we observe neural architecture-related phenomena\nusing model similarity that model diversity can lead to better performance on\nmodel ensembles and knowledge distillation under specific conditions. Our\nresults provide insights into why developing diverse neural architectures with\ndistinct components is necessary.\n", "link": "http://arxiv.org/abs/2210.11407v4", "date": "2024-07-17", "relevancy": 2.3489, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4694}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Similarity%20of%20Neural%20Architectures%20using%20Adversarial%20Attack%0A%20%20Transferability&body=Title%3A%20Similarity%20of%20Neural%20Architectures%20using%20Adversarial%20Attack%0A%20%20Transferability%0AAuthor%3A%20Jaehui%20Hwang%20and%20Dongyoon%20Han%20and%20Byeongho%20Heo%20and%20Song%20Park%20and%20Sanghyuk%20Chun%20and%20Jong-Seok%20Lee%0AAbstract%3A%20%20%20In%20recent%20years%2C%20many%20deep%20neural%20architectures%20have%20been%20developed%20for%20image%0Aclassification.%20Whether%20they%20are%20similar%20or%20dissimilar%20and%20what%20factors%0Acontribute%20to%20their%20%28dis%29similarities%20remains%20curious.%20To%20address%20this%0Aquestion%2C%20we%20aim%20to%20design%20a%20quantitative%20and%20scalable%20similarity%20measure%0Abetween%20neural%20architectures.%20We%20propose%20Similarity%20by%20Attack%20Transferability%0A%28SAT%29%20from%20the%20observation%20that%20adversarial%20attack%20transferability%20contains%0Ainformation%20related%20to%20input%20gradients%20and%20decision%20boundaries%20widely%20used%20to%0Aunderstand%20model%20behaviors.%20We%20conduct%20a%20large-scale%20analysis%20on%2069%0Astate-of-the-art%20ImageNet%20classifiers%20using%20our%20proposed%20similarity%20function%20to%0Aanswer%20the%20question.%20Moreover%2C%20we%20observe%20neural%20architecture-related%20phenomena%0Ausing%20model%20similarity%20that%20model%20diversity%20can%20lead%20to%20better%20performance%20on%0Amodel%20ensembles%20and%20knowledge%20distillation%20under%20specific%20conditions.%20Our%0Aresults%20provide%20insights%20into%20why%20developing%20diverse%20neural%20architectures%20with%0Adistinct%20components%20is%20necessary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.11407v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimilarity%2520of%2520Neural%2520Architectures%2520using%2520Adversarial%2520Attack%250A%2520%2520Transferability%26entry.906535625%3DJaehui%2520Hwang%2520and%2520Dongyoon%2520Han%2520and%2520Byeongho%2520Heo%2520and%2520Song%2520Park%2520and%2520Sanghyuk%2520Chun%2520and%2520Jong-Seok%2520Lee%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520many%2520deep%2520neural%2520architectures%2520have%2520been%2520developed%2520for%2520image%250Aclassification.%2520Whether%2520they%2520are%2520similar%2520or%2520dissimilar%2520and%2520what%2520factors%250Acontribute%2520to%2520their%2520%2528dis%2529similarities%2520remains%2520curious.%2520To%2520address%2520this%250Aquestion%252C%2520we%2520aim%2520to%2520design%2520a%2520quantitative%2520and%2520scalable%2520similarity%2520measure%250Abetween%2520neural%2520architectures.%2520We%2520propose%2520Similarity%2520by%2520Attack%2520Transferability%250A%2528SAT%2529%2520from%2520the%2520observation%2520that%2520adversarial%2520attack%2520transferability%2520contains%250Ainformation%2520related%2520to%2520input%2520gradients%2520and%2520decision%2520boundaries%2520widely%2520used%2520to%250Aunderstand%2520model%2520behaviors.%2520We%2520conduct%2520a%2520large-scale%2520analysis%2520on%252069%250Astate-of-the-art%2520ImageNet%2520classifiers%2520using%2520our%2520proposed%2520similarity%2520function%2520to%250Aanswer%2520the%2520question.%2520Moreover%252C%2520we%2520observe%2520neural%2520architecture-related%2520phenomena%250Ausing%2520model%2520similarity%2520that%2520model%2520diversity%2520can%2520lead%2520to%2520better%2520performance%2520on%250Amodel%2520ensembles%2520and%2520knowledge%2520distillation%2520under%2520specific%2520conditions.%2520Our%250Aresults%2520provide%2520insights%2520into%2520why%2520developing%2520diverse%2520neural%2520architectures%2520with%250Adistinct%2520components%2520is%2520necessary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.11407v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Similarity%20of%20Neural%20Architectures%20using%20Adversarial%20Attack%0A%20%20Transferability&entry.906535625=Jaehui%20Hwang%20and%20Dongyoon%20Han%20and%20Byeongho%20Heo%20and%20Song%20Park%20and%20Sanghyuk%20Chun%20and%20Jong-Seok%20Lee&entry.1292438233=%20%20In%20recent%20years%2C%20many%20deep%20neural%20architectures%20have%20been%20developed%20for%20image%0Aclassification.%20Whether%20they%20are%20similar%20or%20dissimilar%20and%20what%20factors%0Acontribute%20to%20their%20%28dis%29similarities%20remains%20curious.%20To%20address%20this%0Aquestion%2C%20we%20aim%20to%20design%20a%20quantitative%20and%20scalable%20similarity%20measure%0Abetween%20neural%20architectures.%20We%20propose%20Similarity%20by%20Attack%20Transferability%0A%28SAT%29%20from%20the%20observation%20that%20adversarial%20attack%20transferability%20contains%0Ainformation%20related%20to%20input%20gradients%20and%20decision%20boundaries%20widely%20used%20to%0Aunderstand%20model%20behaviors.%20We%20conduct%20a%20large-scale%20analysis%20on%2069%0Astate-of-the-art%20ImageNet%20classifiers%20using%20our%20proposed%20similarity%20function%20to%0Aanswer%20the%20question.%20Moreover%2C%20we%20observe%20neural%20architecture-related%20phenomena%0Ausing%20model%20similarity%20that%20model%20diversity%20can%20lead%20to%20better%20performance%20on%0Amodel%20ensembles%20and%20knowledge%20distillation%20under%20specific%20conditions.%20Our%0Aresults%20provide%20insights%20into%20why%20developing%20diverse%20neural%20architectures%20with%0Adistinct%20components%20is%20necessary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.11407v4&entry.124074799=Read"},
{"title": "Progressive Classifier and Feature Extractor Adaptation for Unsupervised\n  Domain Adaptation on Point Clouds", "author": "Zicheng Wang and Zhen Zhao and Yiming Wu and Luping Zhou and Dong Xu", "abstract": "  Unsupervised domain adaptation (UDA) is a critical challenge in the field of\npoint cloud analysis. Previous works tackle the problem either by feature\nextractor adaptation to enable a shared classifier to distinguish\ndomain-invariant features, or by classifier adaptation to evolve the classifier\nto recognize target-styled source features to increase its adaptation ability.\nHowever, by learning domain-invariant features, feature extractor adaptation\nmethods fail to encode semantically meaningful target-specific information,\nwhile classifier adaptation methods rely heavily on the accurate estimation of\nthe target distribution. In this work, we propose a novel framework that deeply\ncouples the classifier and feature extractor adaption for 3D UDA, dubbed\nProgressive Classifier and Feature Extractor Adaptation (PCFEA). Our PCFEA\nconducts 3D UDA from two distinct perspectives: macro and micro levels. On the\nmacro level, we propose a progressive target-styled feature augmentation (PTFA)\nthat establishes a series of intermediate domains to enable the model to\nprogressively adapt to the target domain. Throughout this process, the source\nclassifier is evolved to recognize target-styled source features (\\ie,\nclassifier adaptation). On the micro level, we develop an intermediate domain\nfeature extractor adaptation (IDFA) that performs a compact feature alignment\nto encourage the target-styled feature extraction gradually. In this way, PTFA\nand IDFA can mutually benefit each other: IDFA contributes to the distribution\nestimation of PTFA while PTFA constructs smoother intermediate domains to\nencourage an accurate feature alignment of IDFA. We validate our method on\npopular benchmark datasets, where our method achieves new state-of-the-art\nperformance. Our code is available at https://github.com/xiaoyao3302/PCFEA.\n", "link": "http://arxiv.org/abs/2311.16474v2", "date": "2024-07-17", "relevancy": 2.3461, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6005}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5859}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Classifier%20and%20Feature%20Extractor%20Adaptation%20for%20Unsupervised%0A%20%20Domain%20Adaptation%20on%20Point%20Clouds&body=Title%3A%20Progressive%20Classifier%20and%20Feature%20Extractor%20Adaptation%20for%20Unsupervised%0A%20%20Domain%20Adaptation%20on%20Point%20Clouds%0AAuthor%3A%20Zicheng%20Wang%20and%20Zhen%20Zhao%20and%20Yiming%20Wu%20and%20Luping%20Zhou%20and%20Dong%20Xu%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20is%20a%20critical%20challenge%20in%20the%20field%20of%0Apoint%20cloud%20analysis.%20Previous%20works%20tackle%20the%20problem%20either%20by%20feature%0Aextractor%20adaptation%20to%20enable%20a%20shared%20classifier%20to%20distinguish%0Adomain-invariant%20features%2C%20or%20by%20classifier%20adaptation%20to%20evolve%20the%20classifier%0Ato%20recognize%20target-styled%20source%20features%20to%20increase%20its%20adaptation%20ability.%0AHowever%2C%20by%20learning%20domain-invariant%20features%2C%20feature%20extractor%20adaptation%0Amethods%20fail%20to%20encode%20semantically%20meaningful%20target-specific%20information%2C%0Awhile%20classifier%20adaptation%20methods%20rely%20heavily%20on%20the%20accurate%20estimation%20of%0Athe%20target%20distribution.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%20deeply%0Acouples%20the%20classifier%20and%20feature%20extractor%20adaption%20for%203D%20UDA%2C%20dubbed%0AProgressive%20Classifier%20and%20Feature%20Extractor%20Adaptation%20%28PCFEA%29.%20Our%20PCFEA%0Aconducts%203D%20UDA%20from%20two%20distinct%20perspectives%3A%20macro%20and%20micro%20levels.%20On%20the%0Amacro%20level%2C%20we%20propose%20a%20progressive%20target-styled%20feature%20augmentation%20%28PTFA%29%0Athat%20establishes%20a%20series%20of%20intermediate%20domains%20to%20enable%20the%20model%20to%0Aprogressively%20adapt%20to%20the%20target%20domain.%20Throughout%20this%20process%2C%20the%20source%0Aclassifier%20is%20evolved%20to%20recognize%20target-styled%20source%20features%20%28%5Cie%2C%0Aclassifier%20adaptation%29.%20On%20the%20micro%20level%2C%20we%20develop%20an%20intermediate%20domain%0Afeature%20extractor%20adaptation%20%28IDFA%29%20that%20performs%20a%20compact%20feature%20alignment%0Ato%20encourage%20the%20target-styled%20feature%20extraction%20gradually.%20In%20this%20way%2C%20PTFA%0Aand%20IDFA%20can%20mutually%20benefit%20each%20other%3A%20IDFA%20contributes%20to%20the%20distribution%0Aestimation%20of%20PTFA%20while%20PTFA%20constructs%20smoother%20intermediate%20domains%20to%0Aencourage%20an%20accurate%20feature%20alignment%20of%20IDFA.%20We%20validate%20our%20method%20on%0Apopular%20benchmark%20datasets%2C%20where%20our%20method%20achieves%20new%20state-of-the-art%0Aperformance.%20Our%20code%20is%20available%20at%20https%3A//github.com/xiaoyao3302/PCFEA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16474v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Classifier%2520and%2520Feature%2520Extractor%2520Adaptation%2520for%2520Unsupervised%250A%2520%2520Domain%2520Adaptation%2520on%2520Point%2520Clouds%26entry.906535625%3DZicheng%2520Wang%2520and%2520Zhen%2520Zhao%2520and%2520Yiming%2520Wu%2520and%2520Luping%2520Zhou%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520is%2520a%2520critical%2520challenge%2520in%2520the%2520field%2520of%250Apoint%2520cloud%2520analysis.%2520Previous%2520works%2520tackle%2520the%2520problem%2520either%2520by%2520feature%250Aextractor%2520adaptation%2520to%2520enable%2520a%2520shared%2520classifier%2520to%2520distinguish%250Adomain-invariant%2520features%252C%2520or%2520by%2520classifier%2520adaptation%2520to%2520evolve%2520the%2520classifier%250Ato%2520recognize%2520target-styled%2520source%2520features%2520to%2520increase%2520its%2520adaptation%2520ability.%250AHowever%252C%2520by%2520learning%2520domain-invariant%2520features%252C%2520feature%2520extractor%2520adaptation%250Amethods%2520fail%2520to%2520encode%2520semantically%2520meaningful%2520target-specific%2520information%252C%250Awhile%2520classifier%2520adaptation%2520methods%2520rely%2520heavily%2520on%2520the%2520accurate%2520estimation%2520of%250Athe%2520target%2520distribution.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520deeply%250Acouples%2520the%2520classifier%2520and%2520feature%2520extractor%2520adaption%2520for%25203D%2520UDA%252C%2520dubbed%250AProgressive%2520Classifier%2520and%2520Feature%2520Extractor%2520Adaptation%2520%2528PCFEA%2529.%2520Our%2520PCFEA%250Aconducts%25203D%2520UDA%2520from%2520two%2520distinct%2520perspectives%253A%2520macro%2520and%2520micro%2520levels.%2520On%2520the%250Amacro%2520level%252C%2520we%2520propose%2520a%2520progressive%2520target-styled%2520feature%2520augmentation%2520%2528PTFA%2529%250Athat%2520establishes%2520a%2520series%2520of%2520intermediate%2520domains%2520to%2520enable%2520the%2520model%2520to%250Aprogressively%2520adapt%2520to%2520the%2520target%2520domain.%2520Throughout%2520this%2520process%252C%2520the%2520source%250Aclassifier%2520is%2520evolved%2520to%2520recognize%2520target-styled%2520source%2520features%2520%2528%255Cie%252C%250Aclassifier%2520adaptation%2529.%2520On%2520the%2520micro%2520level%252C%2520we%2520develop%2520an%2520intermediate%2520domain%250Afeature%2520extractor%2520adaptation%2520%2528IDFA%2529%2520that%2520performs%2520a%2520compact%2520feature%2520alignment%250Ato%2520encourage%2520the%2520target-styled%2520feature%2520extraction%2520gradually.%2520In%2520this%2520way%252C%2520PTFA%250Aand%2520IDFA%2520can%2520mutually%2520benefit%2520each%2520other%253A%2520IDFA%2520contributes%2520to%2520the%2520distribution%250Aestimation%2520of%2520PTFA%2520while%2520PTFA%2520constructs%2520smoother%2520intermediate%2520domains%2520to%250Aencourage%2520an%2520accurate%2520feature%2520alignment%2520of%2520IDFA.%2520We%2520validate%2520our%2520method%2520on%250Apopular%2520benchmark%2520datasets%252C%2520where%2520our%2520method%2520achieves%2520new%2520state-of-the-art%250Aperformance.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/xiaoyao3302/PCFEA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16474v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Classifier%20and%20Feature%20Extractor%20Adaptation%20for%20Unsupervised%0A%20%20Domain%20Adaptation%20on%20Point%20Clouds&entry.906535625=Zicheng%20Wang%20and%20Zhen%20Zhao%20and%20Yiming%20Wu%20and%20Luping%20Zhou%20and%20Dong%20Xu&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20is%20a%20critical%20challenge%20in%20the%20field%20of%0Apoint%20cloud%20analysis.%20Previous%20works%20tackle%20the%20problem%20either%20by%20feature%0Aextractor%20adaptation%20to%20enable%20a%20shared%20classifier%20to%20distinguish%0Adomain-invariant%20features%2C%20or%20by%20classifier%20adaptation%20to%20evolve%20the%20classifier%0Ato%20recognize%20target-styled%20source%20features%20to%20increase%20its%20adaptation%20ability.%0AHowever%2C%20by%20learning%20domain-invariant%20features%2C%20feature%20extractor%20adaptation%0Amethods%20fail%20to%20encode%20semantically%20meaningful%20target-specific%20information%2C%0Awhile%20classifier%20adaptation%20methods%20rely%20heavily%20on%20the%20accurate%20estimation%20of%0Athe%20target%20distribution.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%20deeply%0Acouples%20the%20classifier%20and%20feature%20extractor%20adaption%20for%203D%20UDA%2C%20dubbed%0AProgressive%20Classifier%20and%20Feature%20Extractor%20Adaptation%20%28PCFEA%29.%20Our%20PCFEA%0Aconducts%203D%20UDA%20from%20two%20distinct%20perspectives%3A%20macro%20and%20micro%20levels.%20On%20the%0Amacro%20level%2C%20we%20propose%20a%20progressive%20target-styled%20feature%20augmentation%20%28PTFA%29%0Athat%20establishes%20a%20series%20of%20intermediate%20domains%20to%20enable%20the%20model%20to%0Aprogressively%20adapt%20to%20the%20target%20domain.%20Throughout%20this%20process%2C%20the%20source%0Aclassifier%20is%20evolved%20to%20recognize%20target-styled%20source%20features%20%28%5Cie%2C%0Aclassifier%20adaptation%29.%20On%20the%20micro%20level%2C%20we%20develop%20an%20intermediate%20domain%0Afeature%20extractor%20adaptation%20%28IDFA%29%20that%20performs%20a%20compact%20feature%20alignment%0Ato%20encourage%20the%20target-styled%20feature%20extraction%20gradually.%20In%20this%20way%2C%20PTFA%0Aand%20IDFA%20can%20mutually%20benefit%20each%20other%3A%20IDFA%20contributes%20to%20the%20distribution%0Aestimation%20of%20PTFA%20while%20PTFA%20constructs%20smoother%20intermediate%20domains%20to%0Aencourage%20an%20accurate%20feature%20alignment%20of%20IDFA.%20We%20validate%20our%20method%20on%0Apopular%20benchmark%20datasets%2C%20where%20our%20method%20achieves%20new%20state-of-the-art%0Aperformance.%20Our%20code%20is%20available%20at%20https%3A//github.com/xiaoyao3302/PCFEA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16474v2&entry.124074799=Read"},
{"title": "Jigsaw Game: Federated Clustering", "author": "Jinxuan Xu and Hong-You Chen and Wei-Lun Chao and Yuqian Zhang", "abstract": "  Federated learning has recently garnered significant attention, especially\nwithin the domain of supervised learning. However, despite the abundance of\nunlabeled data on end-users, unsupervised learning problems such as clustering\nin the federated setting remain underexplored. In this paper, we investigate\nthe federated clustering problem, with a focus on federated k-means. We outline\nthe challenge posed by its non-convex objective and data heterogeneity in the\nfederated framework. To tackle these challenges, we adopt a new perspective by\nstudying the structures of local solutions in k-means and propose a one-shot\nalgorithm called FeCA (Federated Centroid Aggregation). FeCA adaptively refines\nlocal solutions on clients, then aggregates these refined solutions to recover\nthe global solution of the entire dataset in a single round. We empirically\ndemonstrate the robustness of FeCA under various federated scenarios on both\nsynthetic and real-world data. Additionally, we extend FeCA to representation\nlearning and present DeepFeCA, which combines DeepCluster and FeCA for\nunsupervised feature learning in the federated setting.\n", "link": "http://arxiv.org/abs/2407.12764v1", "date": "2024-07-17", "relevancy": 2.3447, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4727}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4675}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jigsaw%20Game%3A%20Federated%20Clustering&body=Title%3A%20Jigsaw%20Game%3A%20Federated%20Clustering%0AAuthor%3A%20Jinxuan%20Xu%20and%20Hong-You%20Chen%20and%20Wei-Lun%20Chao%20and%20Yuqian%20Zhang%0AAbstract%3A%20%20%20Federated%20learning%20has%20recently%20garnered%20significant%20attention%2C%20especially%0Awithin%20the%20domain%20of%20supervised%20learning.%20However%2C%20despite%20the%20abundance%20of%0Aunlabeled%20data%20on%20end-users%2C%20unsupervised%20learning%20problems%20such%20as%20clustering%0Ain%20the%20federated%20setting%20remain%20underexplored.%20In%20this%20paper%2C%20we%20investigate%0Athe%20federated%20clustering%20problem%2C%20with%20a%20focus%20on%20federated%20k-means.%20We%20outline%0Athe%20challenge%20posed%20by%20its%20non-convex%20objective%20and%20data%20heterogeneity%20in%20the%0Afederated%20framework.%20To%20tackle%20these%20challenges%2C%20we%20adopt%20a%20new%20perspective%20by%0Astudying%20the%20structures%20of%20local%20solutions%20in%20k-means%20and%20propose%20a%20one-shot%0Aalgorithm%20called%20FeCA%20%28Federated%20Centroid%20Aggregation%29.%20FeCA%20adaptively%20refines%0Alocal%20solutions%20on%20clients%2C%20then%20aggregates%20these%20refined%20solutions%20to%20recover%0Athe%20global%20solution%20of%20the%20entire%20dataset%20in%20a%20single%20round.%20We%20empirically%0Ademonstrate%20the%20robustness%20of%20FeCA%20under%20various%20federated%20scenarios%20on%20both%0Asynthetic%20and%20real-world%20data.%20Additionally%2C%20we%20extend%20FeCA%20to%20representation%0Alearning%20and%20present%20DeepFeCA%2C%20which%20combines%20DeepCluster%20and%20FeCA%20for%0Aunsupervised%20feature%20learning%20in%20the%20federated%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJigsaw%2520Game%253A%2520Federated%2520Clustering%26entry.906535625%3DJinxuan%2520Xu%2520and%2520Hong-You%2520Chen%2520and%2520Wei-Lun%2520Chao%2520and%2520Yuqian%2520Zhang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520has%2520recently%2520garnered%2520significant%2520attention%252C%2520especially%250Awithin%2520the%2520domain%2520of%2520supervised%2520learning.%2520However%252C%2520despite%2520the%2520abundance%2520of%250Aunlabeled%2520data%2520on%2520end-users%252C%2520unsupervised%2520learning%2520problems%2520such%2520as%2520clustering%250Ain%2520the%2520federated%2520setting%2520remain%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520investigate%250Athe%2520federated%2520clustering%2520problem%252C%2520with%2520a%2520focus%2520on%2520federated%2520k-means.%2520We%2520outline%250Athe%2520challenge%2520posed%2520by%2520its%2520non-convex%2520objective%2520and%2520data%2520heterogeneity%2520in%2520the%250Afederated%2520framework.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520adopt%2520a%2520new%2520perspective%2520by%250Astudying%2520the%2520structures%2520of%2520local%2520solutions%2520in%2520k-means%2520and%2520propose%2520a%2520one-shot%250Aalgorithm%2520called%2520FeCA%2520%2528Federated%2520Centroid%2520Aggregation%2529.%2520FeCA%2520adaptively%2520refines%250Alocal%2520solutions%2520on%2520clients%252C%2520then%2520aggregates%2520these%2520refined%2520solutions%2520to%2520recover%250Athe%2520global%2520solution%2520of%2520the%2520entire%2520dataset%2520in%2520a%2520single%2520round.%2520We%2520empirically%250Ademonstrate%2520the%2520robustness%2520of%2520FeCA%2520under%2520various%2520federated%2520scenarios%2520on%2520both%250Asynthetic%2520and%2520real-world%2520data.%2520Additionally%252C%2520we%2520extend%2520FeCA%2520to%2520representation%250Alearning%2520and%2520present%2520DeepFeCA%252C%2520which%2520combines%2520DeepCluster%2520and%2520FeCA%2520for%250Aunsupervised%2520feature%2520learning%2520in%2520the%2520federated%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jigsaw%20Game%3A%20Federated%20Clustering&entry.906535625=Jinxuan%20Xu%20and%20Hong-You%20Chen%20and%20Wei-Lun%20Chao%20and%20Yuqian%20Zhang&entry.1292438233=%20%20Federated%20learning%20has%20recently%20garnered%20significant%20attention%2C%20especially%0Awithin%20the%20domain%20of%20supervised%20learning.%20However%2C%20despite%20the%20abundance%20of%0Aunlabeled%20data%20on%20end-users%2C%20unsupervised%20learning%20problems%20such%20as%20clustering%0Ain%20the%20federated%20setting%20remain%20underexplored.%20In%20this%20paper%2C%20we%20investigate%0Athe%20federated%20clustering%20problem%2C%20with%20a%20focus%20on%20federated%20k-means.%20We%20outline%0Athe%20challenge%20posed%20by%20its%20non-convex%20objective%20and%20data%20heterogeneity%20in%20the%0Afederated%20framework.%20To%20tackle%20these%20challenges%2C%20we%20adopt%20a%20new%20perspective%20by%0Astudying%20the%20structures%20of%20local%20solutions%20in%20k-means%20and%20propose%20a%20one-shot%0Aalgorithm%20called%20FeCA%20%28Federated%20Centroid%20Aggregation%29.%20FeCA%20adaptively%20refines%0Alocal%20solutions%20on%20clients%2C%20then%20aggregates%20these%20refined%20solutions%20to%20recover%0Athe%20global%20solution%20of%20the%20entire%20dataset%20in%20a%20single%20round.%20We%20empirically%0Ademonstrate%20the%20robustness%20of%20FeCA%20under%20various%20federated%20scenarios%20on%20both%0Asynthetic%20and%20real-world%20data.%20Additionally%2C%20we%20extend%20FeCA%20to%20representation%0Alearning%20and%20present%20DeepFeCA%2C%20which%20combines%20DeepCluster%20and%20FeCA%20for%0Aunsupervised%20feature%20learning%20in%20the%20federated%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12764v1&entry.124074799=Read"},
{"title": "Abstraction Alignment: Comparing Model and Human Conceptual\n  Relationships", "author": "Angie Boggust and Hyemin Bang and Hendrik Strobelt and Arvind Satyanarayan", "abstract": "  Abstraction -- the process of generalizing specific examples into broad\nreusable patterns -- is central to how people efficiently process and store\ninformation and apply their knowledge to new data. Promisingly, research has\nshown that ML models learn representations that span levels of abstraction,\nfrom specific concepts like \"bolo tie\" and \"car tire\" to more general concepts\nlike \"CEO\" and \"model\". However, existing techniques analyze these\nrepresentations in isolation, treating learned concepts as independent\nartifacts rather than an interconnected web of abstraction. As a result,\nalthough we can identify the concepts a model uses to produce its output, it is\ndifficult to assess if it has learned a human-aligned abstraction of the\nconcepts that will generalize to new data. To address this gap, we introduce\nabstraction alignment, a methodology to measure the agreement between a model's\nlearned abstraction and the expected human abstraction. We quantify abstraction\nalignment by comparing model outputs against a human abstraction graph, such as\nlinguistic relationships or medical disease hierarchies. In evaluation tasks\ninterpreting image models, benchmarking language models, and analyzing medical\ndatasets, abstraction alignment provides a deeper understanding of model\nbehavior and dataset content, differentiating errors based on their agreement\nwith human knowledge, expanding the verbosity of current model quality metrics,\nand revealing ways to improve existing human abstractions.\n", "link": "http://arxiv.org/abs/2407.12543v1", "date": "2024-07-17", "relevancy": 2.3259, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4857}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4566}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstraction%20Alignment%3A%20Comparing%20Model%20and%20Human%20Conceptual%0A%20%20Relationships&body=Title%3A%20Abstraction%20Alignment%3A%20Comparing%20Model%20and%20Human%20Conceptual%0A%20%20Relationships%0AAuthor%3A%20Angie%20Boggust%20and%20Hyemin%20Bang%20and%20Hendrik%20Strobelt%20and%20Arvind%20Satyanarayan%0AAbstract%3A%20%20%20Abstraction%20--%20the%20process%20of%20generalizing%20specific%20examples%20into%20broad%0Areusable%20patterns%20--%20is%20central%20to%20how%20people%20efficiently%20process%20and%20store%0Ainformation%20and%20apply%20their%20knowledge%20to%20new%20data.%20Promisingly%2C%20research%20has%0Ashown%20that%20ML%20models%20learn%20representations%20that%20span%20levels%20of%20abstraction%2C%0Afrom%20specific%20concepts%20like%20%22bolo%20tie%22%20and%20%22car%20tire%22%20to%20more%20general%20concepts%0Alike%20%22CEO%22%20and%20%22model%22.%20However%2C%20existing%20techniques%20analyze%20these%0Arepresentations%20in%20isolation%2C%20treating%20learned%20concepts%20as%20independent%0Aartifacts%20rather%20than%20an%20interconnected%20web%20of%20abstraction.%20As%20a%20result%2C%0Aalthough%20we%20can%20identify%20the%20concepts%20a%20model%20uses%20to%20produce%20its%20output%2C%20it%20is%0Adifficult%20to%20assess%20if%20it%20has%20learned%20a%20human-aligned%20abstraction%20of%20the%0Aconcepts%20that%20will%20generalize%20to%20new%20data.%20To%20address%20this%20gap%2C%20we%20introduce%0Aabstraction%20alignment%2C%20a%20methodology%20to%20measure%20the%20agreement%20between%20a%20model%27s%0Alearned%20abstraction%20and%20the%20expected%20human%20abstraction.%20We%20quantify%20abstraction%0Aalignment%20by%20comparing%20model%20outputs%20against%20a%20human%20abstraction%20graph%2C%20such%20as%0Alinguistic%20relationships%20or%20medical%20disease%20hierarchies.%20In%20evaluation%20tasks%0Ainterpreting%20image%20models%2C%20benchmarking%20language%20models%2C%20and%20analyzing%20medical%0Adatasets%2C%20abstraction%20alignment%20provides%20a%20deeper%20understanding%20of%20model%0Abehavior%20and%20dataset%20content%2C%20differentiating%20errors%20based%20on%20their%20agreement%0Awith%20human%20knowledge%2C%20expanding%20the%20verbosity%20of%20current%20model%20quality%20metrics%2C%0Aand%20revealing%20ways%20to%20improve%20existing%20human%20abstractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstraction%2520Alignment%253A%2520Comparing%2520Model%2520and%2520Human%2520Conceptual%250A%2520%2520Relationships%26entry.906535625%3DAngie%2520Boggust%2520and%2520Hyemin%2520Bang%2520and%2520Hendrik%2520Strobelt%2520and%2520Arvind%2520Satyanarayan%26entry.1292438233%3D%2520%2520Abstraction%2520--%2520the%2520process%2520of%2520generalizing%2520specific%2520examples%2520into%2520broad%250Areusable%2520patterns%2520--%2520is%2520central%2520to%2520how%2520people%2520efficiently%2520process%2520and%2520store%250Ainformation%2520and%2520apply%2520their%2520knowledge%2520to%2520new%2520data.%2520Promisingly%252C%2520research%2520has%250Ashown%2520that%2520ML%2520models%2520learn%2520representations%2520that%2520span%2520levels%2520of%2520abstraction%252C%250Afrom%2520specific%2520concepts%2520like%2520%2522bolo%2520tie%2522%2520and%2520%2522car%2520tire%2522%2520to%2520more%2520general%2520concepts%250Alike%2520%2522CEO%2522%2520and%2520%2522model%2522.%2520However%252C%2520existing%2520techniques%2520analyze%2520these%250Arepresentations%2520in%2520isolation%252C%2520treating%2520learned%2520concepts%2520as%2520independent%250Aartifacts%2520rather%2520than%2520an%2520interconnected%2520web%2520of%2520abstraction.%2520As%2520a%2520result%252C%250Aalthough%2520we%2520can%2520identify%2520the%2520concepts%2520a%2520model%2520uses%2520to%2520produce%2520its%2520output%252C%2520it%2520is%250Adifficult%2520to%2520assess%2520if%2520it%2520has%2520learned%2520a%2520human-aligned%2520abstraction%2520of%2520the%250Aconcepts%2520that%2520will%2520generalize%2520to%2520new%2520data.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250Aabstraction%2520alignment%252C%2520a%2520methodology%2520to%2520measure%2520the%2520agreement%2520between%2520a%2520model%2527s%250Alearned%2520abstraction%2520and%2520the%2520expected%2520human%2520abstraction.%2520We%2520quantify%2520abstraction%250Aalignment%2520by%2520comparing%2520model%2520outputs%2520against%2520a%2520human%2520abstraction%2520graph%252C%2520such%2520as%250Alinguistic%2520relationships%2520or%2520medical%2520disease%2520hierarchies.%2520In%2520evaluation%2520tasks%250Ainterpreting%2520image%2520models%252C%2520benchmarking%2520language%2520models%252C%2520and%2520analyzing%2520medical%250Adatasets%252C%2520abstraction%2520alignment%2520provides%2520a%2520deeper%2520understanding%2520of%2520model%250Abehavior%2520and%2520dataset%2520content%252C%2520differentiating%2520errors%2520based%2520on%2520their%2520agreement%250Awith%2520human%2520knowledge%252C%2520expanding%2520the%2520verbosity%2520of%2520current%2520model%2520quality%2520metrics%252C%250Aand%2520revealing%2520ways%2520to%2520improve%2520existing%2520human%2520abstractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstraction%20Alignment%3A%20Comparing%20Model%20and%20Human%20Conceptual%0A%20%20Relationships&entry.906535625=Angie%20Boggust%20and%20Hyemin%20Bang%20and%20Hendrik%20Strobelt%20and%20Arvind%20Satyanarayan&entry.1292438233=%20%20Abstraction%20--%20the%20process%20of%20generalizing%20specific%20examples%20into%20broad%0Areusable%20patterns%20--%20is%20central%20to%20how%20people%20efficiently%20process%20and%20store%0Ainformation%20and%20apply%20their%20knowledge%20to%20new%20data.%20Promisingly%2C%20research%20has%0Ashown%20that%20ML%20models%20learn%20representations%20that%20span%20levels%20of%20abstraction%2C%0Afrom%20specific%20concepts%20like%20%22bolo%20tie%22%20and%20%22car%20tire%22%20to%20more%20general%20concepts%0Alike%20%22CEO%22%20and%20%22model%22.%20However%2C%20existing%20techniques%20analyze%20these%0Arepresentations%20in%20isolation%2C%20treating%20learned%20concepts%20as%20independent%0Aartifacts%20rather%20than%20an%20interconnected%20web%20of%20abstraction.%20As%20a%20result%2C%0Aalthough%20we%20can%20identify%20the%20concepts%20a%20model%20uses%20to%20produce%20its%20output%2C%20it%20is%0Adifficult%20to%20assess%20if%20it%20has%20learned%20a%20human-aligned%20abstraction%20of%20the%0Aconcepts%20that%20will%20generalize%20to%20new%20data.%20To%20address%20this%20gap%2C%20we%20introduce%0Aabstraction%20alignment%2C%20a%20methodology%20to%20measure%20the%20agreement%20between%20a%20model%27s%0Alearned%20abstraction%20and%20the%20expected%20human%20abstraction.%20We%20quantify%20abstraction%0Aalignment%20by%20comparing%20model%20outputs%20against%20a%20human%20abstraction%20graph%2C%20such%20as%0Alinguistic%20relationships%20or%20medical%20disease%20hierarchies.%20In%20evaluation%20tasks%0Ainterpreting%20image%20models%2C%20benchmarking%20language%20models%2C%20and%20analyzing%20medical%0Adatasets%2C%20abstraction%20alignment%20provides%20a%20deeper%20understanding%20of%20model%0Abehavior%20and%20dataset%20content%2C%20differentiating%20errors%20based%20on%20their%20agreement%0Awith%20human%20knowledge%2C%20expanding%20the%20verbosity%20of%20current%20model%20quality%20metrics%2C%0Aand%20revealing%20ways%20to%20improve%20existing%20human%20abstractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12543v1&entry.124074799=Read"},
{"title": "Investigating Adversarial Vulnerability and Implicit Bias through\n  Frequency Analysis", "author": "Lorenzo Basile and Nikos Karantzas and Alberto D'Onofrio and Luca Bortolussi and Alex Rodriguez and Fabio Anselmi", "abstract": "  Despite their impressive performance in classification tasks, neural networks\nare known to be vulnerable to adversarial attacks, subtle perturbations of the\ninput data designed to deceive the model. In this work, we investigate the\nrelation between these perturbations and the implicit bias of neural networks\ntrained with gradient-based algorithms. To this end, we analyse the network's\nimplicit bias through the lens of the Fourier transform. Specifically, we\nidentify the minimal and most critical frequencies necessary for accurate\nclassification or misclassification respectively for each input image and its\nadversarially perturbed version, and uncover the correlation among those. To\nthis end, among other methods, we use a newly introduced technique capable of\ndetecting non-linear correlations between high-dimensional datasets. Our\nresults provide empirical evidence that the network bias in Fourier space and\nthe target frequencies of adversarial attacks are highly correlated and suggest\nnew potential strategies for adversarial defence.\n", "link": "http://arxiv.org/abs/2305.15203v2", "date": "2024-07-17", "relevancy": 2.3174, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.47}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.461}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Adversarial%20Vulnerability%20and%20Implicit%20Bias%20through%0A%20%20Frequency%20Analysis&body=Title%3A%20Investigating%20Adversarial%20Vulnerability%20and%20Implicit%20Bias%20through%0A%20%20Frequency%20Analysis%0AAuthor%3A%20Lorenzo%20Basile%20and%20Nikos%20Karantzas%20and%20Alberto%20D%27Onofrio%20and%20Luca%20Bortolussi%20and%20Alex%20Rodriguez%20and%20Fabio%20Anselmi%0AAbstract%3A%20%20%20Despite%20their%20impressive%20performance%20in%20classification%20tasks%2C%20neural%20networks%0Aare%20known%20to%20be%20vulnerable%20to%20adversarial%20attacks%2C%20subtle%20perturbations%20of%20the%0Ainput%20data%20designed%20to%20deceive%20the%20model.%20In%20this%20work%2C%20we%20investigate%20the%0Arelation%20between%20these%20perturbations%20and%20the%20implicit%20bias%20of%20neural%20networks%0Atrained%20with%20gradient-based%20algorithms.%20To%20this%20end%2C%20we%20analyse%20the%20network%27s%0Aimplicit%20bias%20through%20the%20lens%20of%20the%20Fourier%20transform.%20Specifically%2C%20we%0Aidentify%20the%20minimal%20and%20most%20critical%20frequencies%20necessary%20for%20accurate%0Aclassification%20or%20misclassification%20respectively%20for%20each%20input%20image%20and%20its%0Aadversarially%20perturbed%20version%2C%20and%20uncover%20the%20correlation%20among%20those.%20To%0Athis%20end%2C%20among%20other%20methods%2C%20we%20use%20a%20newly%20introduced%20technique%20capable%20of%0Adetecting%20non-linear%20correlations%20between%20high-dimensional%20datasets.%20Our%0Aresults%20provide%20empirical%20evidence%20that%20the%20network%20bias%20in%20Fourier%20space%20and%0Athe%20target%20frequencies%20of%20adversarial%20attacks%20are%20highly%20correlated%20and%20suggest%0Anew%20potential%20strategies%20for%20adversarial%20defence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15203v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Adversarial%2520Vulnerability%2520and%2520Implicit%2520Bias%2520through%250A%2520%2520Frequency%2520Analysis%26entry.906535625%3DLorenzo%2520Basile%2520and%2520Nikos%2520Karantzas%2520and%2520Alberto%2520D%2527Onofrio%2520and%2520Luca%2520Bortolussi%2520and%2520Alex%2520Rodriguez%2520and%2520Fabio%2520Anselmi%26entry.1292438233%3D%2520%2520Despite%2520their%2520impressive%2520performance%2520in%2520classification%2520tasks%252C%2520neural%2520networks%250Aare%2520known%2520to%2520be%2520vulnerable%2520to%2520adversarial%2520attacks%252C%2520subtle%2520perturbations%2520of%2520the%250Ainput%2520data%2520designed%2520to%2520deceive%2520the%2520model.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%250Arelation%2520between%2520these%2520perturbations%2520and%2520the%2520implicit%2520bias%2520of%2520neural%2520networks%250Atrained%2520with%2520gradient-based%2520algorithms.%2520To%2520this%2520end%252C%2520we%2520analyse%2520the%2520network%2527s%250Aimplicit%2520bias%2520through%2520the%2520lens%2520of%2520the%2520Fourier%2520transform.%2520Specifically%252C%2520we%250Aidentify%2520the%2520minimal%2520and%2520most%2520critical%2520frequencies%2520necessary%2520for%2520accurate%250Aclassification%2520or%2520misclassification%2520respectively%2520for%2520each%2520input%2520image%2520and%2520its%250Aadversarially%2520perturbed%2520version%252C%2520and%2520uncover%2520the%2520correlation%2520among%2520those.%2520To%250Athis%2520end%252C%2520among%2520other%2520methods%252C%2520we%2520use%2520a%2520newly%2520introduced%2520technique%2520capable%2520of%250Adetecting%2520non-linear%2520correlations%2520between%2520high-dimensional%2520datasets.%2520Our%250Aresults%2520provide%2520empirical%2520evidence%2520that%2520the%2520network%2520bias%2520in%2520Fourier%2520space%2520and%250Athe%2520target%2520frequencies%2520of%2520adversarial%2520attacks%2520are%2520highly%2520correlated%2520and%2520suggest%250Anew%2520potential%2520strategies%2520for%2520adversarial%2520defence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15203v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Adversarial%20Vulnerability%20and%20Implicit%20Bias%20through%0A%20%20Frequency%20Analysis&entry.906535625=Lorenzo%20Basile%20and%20Nikos%20Karantzas%20and%20Alberto%20D%27Onofrio%20and%20Luca%20Bortolussi%20and%20Alex%20Rodriguez%20and%20Fabio%20Anselmi&entry.1292438233=%20%20Despite%20their%20impressive%20performance%20in%20classification%20tasks%2C%20neural%20networks%0Aare%20known%20to%20be%20vulnerable%20to%20adversarial%20attacks%2C%20subtle%20perturbations%20of%20the%0Ainput%20data%20designed%20to%20deceive%20the%20model.%20In%20this%20work%2C%20we%20investigate%20the%0Arelation%20between%20these%20perturbations%20and%20the%20implicit%20bias%20of%20neural%20networks%0Atrained%20with%20gradient-based%20algorithms.%20To%20this%20end%2C%20we%20analyse%20the%20network%27s%0Aimplicit%20bias%20through%20the%20lens%20of%20the%20Fourier%20transform.%20Specifically%2C%20we%0Aidentify%20the%20minimal%20and%20most%20critical%20frequencies%20necessary%20for%20accurate%0Aclassification%20or%20misclassification%20respectively%20for%20each%20input%20image%20and%20its%0Aadversarially%20perturbed%20version%2C%20and%20uncover%20the%20correlation%20among%20those.%20To%0Athis%20end%2C%20among%20other%20methods%2C%20we%20use%20a%20newly%20introduced%20technique%20capable%20of%0Adetecting%20non-linear%20correlations%20between%20high-dimensional%20datasets.%20Our%0Aresults%20provide%20empirical%20evidence%20that%20the%20network%20bias%20in%20Fourier%20space%20and%0Athe%20target%20frequencies%20of%20adversarial%20attacks%20are%20highly%20correlated%20and%20suggest%0Anew%20potential%20strategies%20for%20adversarial%20defence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15203v2&entry.124074799=Read"},
{"title": "NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with\n  Diffusion Model", "author": "Zhongqun Zhang and Hengfei Wang and Ziwei Yu and Yihua Cheng and Angela Yao and Hyung Jin Chang", "abstract": "  Modeling the physical contacts between the hand and object is standard for\nrefining inaccurate hand poses and generating novel human grasp in 3D\nhand-object reconstruction. However, existing methods rely on geometric\nconstraints that cannot be specified or controlled. This paper introduces a\nnovel task of controllable 3D hand-object contact modeling with natural\nlanguage descriptions. Challenges include i) the complexity of cross-modal\nmodeling from language to contact, and ii) a lack of descriptive text for\ncontact patterns. To address these issues, we propose NL2Contact, a model that\ngenerates controllable contacts by leveraging staged diffusion models. Given a\nlanguage description of the hand and contact, NL2Contact generates realistic\nand faithful 3D hand-object contacts. To train the model, we build\n\\textit{ContactDescribe}, the first dataset with hand-centered contact\ndescriptions. It contains multi-level and diverse descriptions generated by\nlarge language models based on carefully designed prompts (e.g., grasp action,\ngrasp type, contact location, free finger status). We show applications of our\nmodel to grasp pose optimization and novel human grasp generation, both based\non a textual contact description.\n", "link": "http://arxiv.org/abs/2407.12727v1", "date": "2024-07-17", "relevancy": 2.3113, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5847}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5777}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NL2Contact%3A%20Natural%20Language%20Guided%203D%20Hand-Object%20Contact%20Modeling%20with%0A%20%20Diffusion%20Model&body=Title%3A%20NL2Contact%3A%20Natural%20Language%20Guided%203D%20Hand-Object%20Contact%20Modeling%20with%0A%20%20Diffusion%20Model%0AAuthor%3A%20Zhongqun%20Zhang%20and%20Hengfei%20Wang%20and%20Ziwei%20Yu%20and%20Yihua%20Cheng%20and%20Angela%20Yao%20and%20Hyung%20Jin%20Chang%0AAbstract%3A%20%20%20Modeling%20the%20physical%20contacts%20between%20the%20hand%20and%20object%20is%20standard%20for%0Arefining%20inaccurate%20hand%20poses%20and%20generating%20novel%20human%20grasp%20in%203D%0Ahand-object%20reconstruction.%20However%2C%20existing%20methods%20rely%20on%20geometric%0Aconstraints%20that%20cannot%20be%20specified%20or%20controlled.%20This%20paper%20introduces%20a%0Anovel%20task%20of%20controllable%203D%20hand-object%20contact%20modeling%20with%20natural%0Alanguage%20descriptions.%20Challenges%20include%20i%29%20the%20complexity%20of%20cross-modal%0Amodeling%20from%20language%20to%20contact%2C%20and%20ii%29%20a%20lack%20of%20descriptive%20text%20for%0Acontact%20patterns.%20To%20address%20these%20issues%2C%20we%20propose%20NL2Contact%2C%20a%20model%20that%0Agenerates%20controllable%20contacts%20by%20leveraging%20staged%20diffusion%20models.%20Given%20a%0Alanguage%20description%20of%20the%20hand%20and%20contact%2C%20NL2Contact%20generates%20realistic%0Aand%20faithful%203D%20hand-object%20contacts.%20To%20train%20the%20model%2C%20we%20build%0A%5Ctextit%7BContactDescribe%7D%2C%20the%20first%20dataset%20with%20hand-centered%20contact%0Adescriptions.%20It%20contains%20multi-level%20and%20diverse%20descriptions%20generated%20by%0Alarge%20language%20models%20based%20on%20carefully%20designed%20prompts%20%28e.g.%2C%20grasp%20action%2C%0Agrasp%20type%2C%20contact%20location%2C%20free%20finger%20status%29.%20We%20show%20applications%20of%20our%0Amodel%20to%20grasp%20pose%20optimization%20and%20novel%20human%20grasp%20generation%2C%20both%20based%0Aon%20a%20textual%20contact%20description.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNL2Contact%253A%2520Natural%2520Language%2520Guided%25203D%2520Hand-Object%2520Contact%2520Modeling%2520with%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DZhongqun%2520Zhang%2520and%2520Hengfei%2520Wang%2520and%2520Ziwei%2520Yu%2520and%2520Yihua%2520Cheng%2520and%2520Angela%2520Yao%2520and%2520Hyung%2520Jin%2520Chang%26entry.1292438233%3D%2520%2520Modeling%2520the%2520physical%2520contacts%2520between%2520the%2520hand%2520and%2520object%2520is%2520standard%2520for%250Arefining%2520inaccurate%2520hand%2520poses%2520and%2520generating%2520novel%2520human%2520grasp%2520in%25203D%250Ahand-object%2520reconstruction.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520geometric%250Aconstraints%2520that%2520cannot%2520be%2520specified%2520or%2520controlled.%2520This%2520paper%2520introduces%2520a%250Anovel%2520task%2520of%2520controllable%25203D%2520hand-object%2520contact%2520modeling%2520with%2520natural%250Alanguage%2520descriptions.%2520Challenges%2520include%2520i%2529%2520the%2520complexity%2520of%2520cross-modal%250Amodeling%2520from%2520language%2520to%2520contact%252C%2520and%2520ii%2529%2520a%2520lack%2520of%2520descriptive%2520text%2520for%250Acontact%2520patterns.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520NL2Contact%252C%2520a%2520model%2520that%250Agenerates%2520controllable%2520contacts%2520by%2520leveraging%2520staged%2520diffusion%2520models.%2520Given%2520a%250Alanguage%2520description%2520of%2520the%2520hand%2520and%2520contact%252C%2520NL2Contact%2520generates%2520realistic%250Aand%2520faithful%25203D%2520hand-object%2520contacts.%2520To%2520train%2520the%2520model%252C%2520we%2520build%250A%255Ctextit%257BContactDescribe%257D%252C%2520the%2520first%2520dataset%2520with%2520hand-centered%2520contact%250Adescriptions.%2520It%2520contains%2520multi-level%2520and%2520diverse%2520descriptions%2520generated%2520by%250Alarge%2520language%2520models%2520based%2520on%2520carefully%2520designed%2520prompts%2520%2528e.g.%252C%2520grasp%2520action%252C%250Agrasp%2520type%252C%2520contact%2520location%252C%2520free%2520finger%2520status%2529.%2520We%2520show%2520applications%2520of%2520our%250Amodel%2520to%2520grasp%2520pose%2520optimization%2520and%2520novel%2520human%2520grasp%2520generation%252C%2520both%2520based%250Aon%2520a%2520textual%2520contact%2520description.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NL2Contact%3A%20Natural%20Language%20Guided%203D%20Hand-Object%20Contact%20Modeling%20with%0A%20%20Diffusion%20Model&entry.906535625=Zhongqun%20Zhang%20and%20Hengfei%20Wang%20and%20Ziwei%20Yu%20and%20Yihua%20Cheng%20and%20Angela%20Yao%20and%20Hyung%20Jin%20Chang&entry.1292438233=%20%20Modeling%20the%20physical%20contacts%20between%20the%20hand%20and%20object%20is%20standard%20for%0Arefining%20inaccurate%20hand%20poses%20and%20generating%20novel%20human%20grasp%20in%203D%0Ahand-object%20reconstruction.%20However%2C%20existing%20methods%20rely%20on%20geometric%0Aconstraints%20that%20cannot%20be%20specified%20or%20controlled.%20This%20paper%20introduces%20a%0Anovel%20task%20of%20controllable%203D%20hand-object%20contact%20modeling%20with%20natural%0Alanguage%20descriptions.%20Challenges%20include%20i%29%20the%20complexity%20of%20cross-modal%0Amodeling%20from%20language%20to%20contact%2C%20and%20ii%29%20a%20lack%20of%20descriptive%20text%20for%0Acontact%20patterns.%20To%20address%20these%20issues%2C%20we%20propose%20NL2Contact%2C%20a%20model%20that%0Agenerates%20controllable%20contacts%20by%20leveraging%20staged%20diffusion%20models.%20Given%20a%0Alanguage%20description%20of%20the%20hand%20and%20contact%2C%20NL2Contact%20generates%20realistic%0Aand%20faithful%203D%20hand-object%20contacts.%20To%20train%20the%20model%2C%20we%20build%0A%5Ctextit%7BContactDescribe%7D%2C%20the%20first%20dataset%20with%20hand-centered%20contact%0Adescriptions.%20It%20contains%20multi-level%20and%20diverse%20descriptions%20generated%20by%0Alarge%20language%20models%20based%20on%20carefully%20designed%20prompts%20%28e.g.%2C%20grasp%20action%2C%0Agrasp%20type%2C%20contact%20location%2C%20free%20finger%20status%29.%20We%20show%20applications%20of%20our%0Amodel%20to%20grasp%20pose%20optimization%20and%20novel%20human%20grasp%20generation%2C%20both%20based%0Aon%20a%20textual%20contact%20description.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12727v1&entry.124074799=Read"},
{"title": "Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection\n  Enhanced by Comprehensive Guidance from Text and Image", "author": "Pengkun Jiao and Na Zhao and Jingjing Chen and Yu-Gang Jiang", "abstract": "  Open-vocabulary 3D object detection (OV-3DDet) aims to localize and recognize\nboth seen and previously unseen object categories within any new 3D scene.\nWhile language and vision foundation models have achieved success in handling\nvarious open-vocabulary tasks with abundant training data, OV-3DDet faces a\nsignificant challenge due to the limited availability of training data.\nAlthough some pioneering efforts have integrated vision-language models (VLM)\nknowledge into OV-3DDet learning, the full potential of these foundational\nmodels has yet to be fully exploited. In this paper, we unlock the textual and\nvisual wisdom to tackle the open-vocabulary 3D detection task by leveraging the\nlanguage and vision foundation models. We leverage a vision foundation model to\nprovide image-wise guidance for discovering novel classes in 3D scenes.\nSpecifically, we utilize a object detection vision foundation model to enable\nthe zero-shot discovery of objects in images, which serves as the initial seeds\nand filtering guidance to identify novel 3D objects. Additionally, to align the\n3D space with the powerful vision-language space, we introduce a hierarchical\nalignment approach, where the 3D feature space is aligned with the\nvision-language feature space using a pre-trained VLM at the instance,\ncategory, and scene levels. Through extensive experimentation, we demonstrate\nsignificant improvements in accuracy and generalization, highlighting the\npotential of foundation models in advancing open-vocabulary 3D object detection\nin real-world scenarios.\n", "link": "http://arxiv.org/abs/2407.05256v2", "date": "2024-07-17", "relevancy": 2.3078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5669}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Textual%20and%20Visual%20Wisdom%3A%20Open-Vocabulary%203D%20Object%20Detection%0A%20%20Enhanced%20by%20Comprehensive%20Guidance%20from%20Text%20and%20Image&body=Title%3A%20Unlocking%20Textual%20and%20Visual%20Wisdom%3A%20Open-Vocabulary%203D%20Object%20Detection%0A%20%20Enhanced%20by%20Comprehensive%20Guidance%20from%20Text%20and%20Image%0AAuthor%3A%20Pengkun%20Jiao%20and%20Na%20Zhao%20and%20Jingjing%20Chen%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Open-vocabulary%203D%20object%20detection%20%28OV-3DDet%29%20aims%20to%20localize%20and%20recognize%0Aboth%20seen%20and%20previously%20unseen%20object%20categories%20within%20any%20new%203D%20scene.%0AWhile%20language%20and%20vision%20foundation%20models%20have%20achieved%20success%20in%20handling%0Avarious%20open-vocabulary%20tasks%20with%20abundant%20training%20data%2C%20OV-3DDet%20faces%20a%0Asignificant%20challenge%20due%20to%20the%20limited%20availability%20of%20training%20data.%0AAlthough%20some%20pioneering%20efforts%20have%20integrated%20vision-language%20models%20%28VLM%29%0Aknowledge%20into%20OV-3DDet%20learning%2C%20the%20full%20potential%20of%20these%20foundational%0Amodels%20has%20yet%20to%20be%20fully%20exploited.%20In%20this%20paper%2C%20we%20unlock%20the%20textual%20and%0Avisual%20wisdom%20to%20tackle%20the%20open-vocabulary%203D%20detection%20task%20by%20leveraging%20the%0Alanguage%20and%20vision%20foundation%20models.%20We%20leverage%20a%20vision%20foundation%20model%20to%0Aprovide%20image-wise%20guidance%20for%20discovering%20novel%20classes%20in%203D%20scenes.%0ASpecifically%2C%20we%20utilize%20a%20object%20detection%20vision%20foundation%20model%20to%20enable%0Athe%20zero-shot%20discovery%20of%20objects%20in%20images%2C%20which%20serves%20as%20the%20initial%20seeds%0Aand%20filtering%20guidance%20to%20identify%20novel%203D%20objects.%20Additionally%2C%20to%20align%20the%0A3D%20space%20with%20the%20powerful%20vision-language%20space%2C%20we%20introduce%20a%20hierarchical%0Aalignment%20approach%2C%20where%20the%203D%20feature%20space%20is%20aligned%20with%20the%0Avision-language%20feature%20space%20using%20a%20pre-trained%20VLM%20at%20the%20instance%2C%0Acategory%2C%20and%20scene%20levels.%20Through%20extensive%20experimentation%2C%20we%20demonstrate%0Asignificant%20improvements%20in%20accuracy%20and%20generalization%2C%20highlighting%20the%0Apotential%20of%20foundation%20models%20in%20advancing%20open-vocabulary%203D%20object%20detection%0Ain%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05256v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Textual%2520and%2520Visual%2520Wisdom%253A%2520Open-Vocabulary%25203D%2520Object%2520Detection%250A%2520%2520Enhanced%2520by%2520Comprehensive%2520Guidance%2520from%2520Text%2520and%2520Image%26entry.906535625%3DPengkun%2520Jiao%2520and%2520Na%2520Zhao%2520and%2520Jingjing%2520Chen%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520object%2520detection%2520%2528OV-3DDet%2529%2520aims%2520to%2520localize%2520and%2520recognize%250Aboth%2520seen%2520and%2520previously%2520unseen%2520object%2520categories%2520within%2520any%2520new%25203D%2520scene.%250AWhile%2520language%2520and%2520vision%2520foundation%2520models%2520have%2520achieved%2520success%2520in%2520handling%250Avarious%2520open-vocabulary%2520tasks%2520with%2520abundant%2520training%2520data%252C%2520OV-3DDet%2520faces%2520a%250Asignificant%2520challenge%2520due%2520to%2520the%2520limited%2520availability%2520of%2520training%2520data.%250AAlthough%2520some%2520pioneering%2520efforts%2520have%2520integrated%2520vision-language%2520models%2520%2528VLM%2529%250Aknowledge%2520into%2520OV-3DDet%2520learning%252C%2520the%2520full%2520potential%2520of%2520these%2520foundational%250Amodels%2520has%2520yet%2520to%2520be%2520fully%2520exploited.%2520In%2520this%2520paper%252C%2520we%2520unlock%2520the%2520textual%2520and%250Avisual%2520wisdom%2520to%2520tackle%2520the%2520open-vocabulary%25203D%2520detection%2520task%2520by%2520leveraging%2520the%250Alanguage%2520and%2520vision%2520foundation%2520models.%2520We%2520leverage%2520a%2520vision%2520foundation%2520model%2520to%250Aprovide%2520image-wise%2520guidance%2520for%2520discovering%2520novel%2520classes%2520in%25203D%2520scenes.%250ASpecifically%252C%2520we%2520utilize%2520a%2520object%2520detection%2520vision%2520foundation%2520model%2520to%2520enable%250Athe%2520zero-shot%2520discovery%2520of%2520objects%2520in%2520images%252C%2520which%2520serves%2520as%2520the%2520initial%2520seeds%250Aand%2520filtering%2520guidance%2520to%2520identify%2520novel%25203D%2520objects.%2520Additionally%252C%2520to%2520align%2520the%250A3D%2520space%2520with%2520the%2520powerful%2520vision-language%2520space%252C%2520we%2520introduce%2520a%2520hierarchical%250Aalignment%2520approach%252C%2520where%2520the%25203D%2520feature%2520space%2520is%2520aligned%2520with%2520the%250Avision-language%2520feature%2520space%2520using%2520a%2520pre-trained%2520VLM%2520at%2520the%2520instance%252C%250Acategory%252C%2520and%2520scene%2520levels.%2520Through%2520extensive%2520experimentation%252C%2520we%2520demonstrate%250Asignificant%2520improvements%2520in%2520accuracy%2520and%2520generalization%252C%2520highlighting%2520the%250Apotential%2520of%2520foundation%2520models%2520in%2520advancing%2520open-vocabulary%25203D%2520object%2520detection%250Ain%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05256v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Textual%20and%20Visual%20Wisdom%3A%20Open-Vocabulary%203D%20Object%20Detection%0A%20%20Enhanced%20by%20Comprehensive%20Guidance%20from%20Text%20and%20Image&entry.906535625=Pengkun%20Jiao%20and%20Na%20Zhao%20and%20Jingjing%20Chen%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Open-vocabulary%203D%20object%20detection%20%28OV-3DDet%29%20aims%20to%20localize%20and%20recognize%0Aboth%20seen%20and%20previously%20unseen%20object%20categories%20within%20any%20new%203D%20scene.%0AWhile%20language%20and%20vision%20foundation%20models%20have%20achieved%20success%20in%20handling%0Avarious%20open-vocabulary%20tasks%20with%20abundant%20training%20data%2C%20OV-3DDet%20faces%20a%0Asignificant%20challenge%20due%20to%20the%20limited%20availability%20of%20training%20data.%0AAlthough%20some%20pioneering%20efforts%20have%20integrated%20vision-language%20models%20%28VLM%29%0Aknowledge%20into%20OV-3DDet%20learning%2C%20the%20full%20potential%20of%20these%20foundational%0Amodels%20has%20yet%20to%20be%20fully%20exploited.%20In%20this%20paper%2C%20we%20unlock%20the%20textual%20and%0Avisual%20wisdom%20to%20tackle%20the%20open-vocabulary%203D%20detection%20task%20by%20leveraging%20the%0Alanguage%20and%20vision%20foundation%20models.%20We%20leverage%20a%20vision%20foundation%20model%20to%0Aprovide%20image-wise%20guidance%20for%20discovering%20novel%20classes%20in%203D%20scenes.%0ASpecifically%2C%20we%20utilize%20a%20object%20detection%20vision%20foundation%20model%20to%20enable%0Athe%20zero-shot%20discovery%20of%20objects%20in%20images%2C%20which%20serves%20as%20the%20initial%20seeds%0Aand%20filtering%20guidance%20to%20identify%20novel%203D%20objects.%20Additionally%2C%20to%20align%20the%0A3D%20space%20with%20the%20powerful%20vision-language%20space%2C%20we%20introduce%20a%20hierarchical%0Aalignment%20approach%2C%20where%20the%203D%20feature%20space%20is%20aligned%20with%20the%0Avision-language%20feature%20space%20using%20a%20pre-trained%20VLM%20at%20the%20instance%2C%0Acategory%2C%20and%20scene%20levels.%20Through%20extensive%20experimentation%2C%20we%20demonstrate%0Asignificant%20improvements%20in%20accuracy%20and%20generalization%2C%20highlighting%20the%0Apotential%20of%20foundation%20models%20in%20advancing%20open-vocabulary%203D%20object%20detection%0Ain%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05256v2&entry.124074799=Read"},
{"title": "Pseudo-keypoint RKHS Learning for Self-supervised 6DoF Pose Estimation", "author": "Yangzheng Wu and Michael Greenspan", "abstract": "  We address the simulation-to-real domain gap in six degree-of-freedom pose\nestimation (6DoF PE), and propose a novel self-supervised keypoint voting-based\n6DoF PE framework, effectively narrowing this gap using a learnable kernel in\nRKHS. We formulate this domain gap as a distance in high-dimensional feature\nspace, distinct from previous iterative matching methods. We propose an adapter\nnetwork, which is pre-trained on purely synthetic data with synthetic ground\ntruth poses, and which evolves the network parameters from this source\nsynthetic domain to the target real domain. Importantly, the real data training\nonly uses pseudo-poses estimated by pseudo-keypoints, and thereby requires no\nreal ground truth data annotations. Our proposed method is called RKHSPose, and\nachieves state-of-the-art performance among self-supervised methods on three\ncommonly used 6DoF PE datasets including LINEMOD (+4.2%), Occlusion LINEMOD\n(+2%), and YCB-Video (+3%). It also compares favorably to fully supervised\nmethods on all six applicable BOP core datasets, achieving within -11.3% to\n+0.2% of the top fully supervised results.\n", "link": "http://arxiv.org/abs/2311.09500v3", "date": "2024-07-17", "relevancy": 2.2784, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6129}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-keypoint%20RKHS%20Learning%20for%20Self-supervised%206DoF%20Pose%20Estimation&body=Title%3A%20Pseudo-keypoint%20RKHS%20Learning%20for%20Self-supervised%206DoF%20Pose%20Estimation%0AAuthor%3A%20Yangzheng%20Wu%20and%20Michael%20Greenspan%0AAbstract%3A%20%20%20We%20address%20the%20simulation-to-real%20domain%20gap%20in%20six%20degree-of-freedom%20pose%0Aestimation%20%286DoF%20PE%29%2C%20and%20propose%20a%20novel%20self-supervised%20keypoint%20voting-based%0A6DoF%20PE%20framework%2C%20effectively%20narrowing%20this%20gap%20using%20a%20learnable%20kernel%20in%0ARKHS.%20We%20formulate%20this%20domain%20gap%20as%20a%20distance%20in%20high-dimensional%20feature%0Aspace%2C%20distinct%20from%20previous%20iterative%20matching%20methods.%20We%20propose%20an%20adapter%0Anetwork%2C%20which%20is%20pre-trained%20on%20purely%20synthetic%20data%20with%20synthetic%20ground%0Atruth%20poses%2C%20and%20which%20evolves%20the%20network%20parameters%20from%20this%20source%0Asynthetic%20domain%20to%20the%20target%20real%20domain.%20Importantly%2C%20the%20real%20data%20training%0Aonly%20uses%20pseudo-poses%20estimated%20by%20pseudo-keypoints%2C%20and%20thereby%20requires%20no%0Areal%20ground%20truth%20data%20annotations.%20Our%20proposed%20method%20is%20called%20RKHSPose%2C%20and%0Aachieves%20state-of-the-art%20performance%20among%20self-supervised%20methods%20on%20three%0Acommonly%20used%206DoF%20PE%20datasets%20including%20LINEMOD%20%28%2B4.2%25%29%2C%20Occlusion%20LINEMOD%0A%28%2B2%25%29%2C%20and%20YCB-Video%20%28%2B3%25%29.%20It%20also%20compares%20favorably%20to%20fully%20supervised%0Amethods%20on%20all%20six%20applicable%20BOP%20core%20datasets%2C%20achieving%20within%20-11.3%25%20to%0A%2B0.2%25%20of%20the%20top%20fully%20supervised%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09500v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-keypoint%2520RKHS%2520Learning%2520for%2520Self-supervised%25206DoF%2520Pose%2520Estimation%26entry.906535625%3DYangzheng%2520Wu%2520and%2520Michael%2520Greenspan%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520simulation-to-real%2520domain%2520gap%2520in%2520six%2520degree-of-freedom%2520pose%250Aestimation%2520%25286DoF%2520PE%2529%252C%2520and%2520propose%2520a%2520novel%2520self-supervised%2520keypoint%2520voting-based%250A6DoF%2520PE%2520framework%252C%2520effectively%2520narrowing%2520this%2520gap%2520using%2520a%2520learnable%2520kernel%2520in%250ARKHS.%2520We%2520formulate%2520this%2520domain%2520gap%2520as%2520a%2520distance%2520in%2520high-dimensional%2520feature%250Aspace%252C%2520distinct%2520from%2520previous%2520iterative%2520matching%2520methods.%2520We%2520propose%2520an%2520adapter%250Anetwork%252C%2520which%2520is%2520pre-trained%2520on%2520purely%2520synthetic%2520data%2520with%2520synthetic%2520ground%250Atruth%2520poses%252C%2520and%2520which%2520evolves%2520the%2520network%2520parameters%2520from%2520this%2520source%250Asynthetic%2520domain%2520to%2520the%2520target%2520real%2520domain.%2520Importantly%252C%2520the%2520real%2520data%2520training%250Aonly%2520uses%2520pseudo-poses%2520estimated%2520by%2520pseudo-keypoints%252C%2520and%2520thereby%2520requires%2520no%250Areal%2520ground%2520truth%2520data%2520annotations.%2520Our%2520proposed%2520method%2520is%2520called%2520RKHSPose%252C%2520and%250Aachieves%2520state-of-the-art%2520performance%2520among%2520self-supervised%2520methods%2520on%2520three%250Acommonly%2520used%25206DoF%2520PE%2520datasets%2520including%2520LINEMOD%2520%2528%252B4.2%2525%2529%252C%2520Occlusion%2520LINEMOD%250A%2528%252B2%2525%2529%252C%2520and%2520YCB-Video%2520%2528%252B3%2525%2529.%2520It%2520also%2520compares%2520favorably%2520to%2520fully%2520supervised%250Amethods%2520on%2520all%2520six%2520applicable%2520BOP%2520core%2520datasets%252C%2520achieving%2520within%2520-11.3%2525%2520to%250A%252B0.2%2525%2520of%2520the%2520top%2520fully%2520supervised%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09500v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-keypoint%20RKHS%20Learning%20for%20Self-supervised%206DoF%20Pose%20Estimation&entry.906535625=Yangzheng%20Wu%20and%20Michael%20Greenspan&entry.1292438233=%20%20We%20address%20the%20simulation-to-real%20domain%20gap%20in%20six%20degree-of-freedom%20pose%0Aestimation%20%286DoF%20PE%29%2C%20and%20propose%20a%20novel%20self-supervised%20keypoint%20voting-based%0A6DoF%20PE%20framework%2C%20effectively%20narrowing%20this%20gap%20using%20a%20learnable%20kernel%20in%0ARKHS.%20We%20formulate%20this%20domain%20gap%20as%20a%20distance%20in%20high-dimensional%20feature%0Aspace%2C%20distinct%20from%20previous%20iterative%20matching%20methods.%20We%20propose%20an%20adapter%0Anetwork%2C%20which%20is%20pre-trained%20on%20purely%20synthetic%20data%20with%20synthetic%20ground%0Atruth%20poses%2C%20and%20which%20evolves%20the%20network%20parameters%20from%20this%20source%0Asynthetic%20domain%20to%20the%20target%20real%20domain.%20Importantly%2C%20the%20real%20data%20training%0Aonly%20uses%20pseudo-poses%20estimated%20by%20pseudo-keypoints%2C%20and%20thereby%20requires%20no%0Areal%20ground%20truth%20data%20annotations.%20Our%20proposed%20method%20is%20called%20RKHSPose%2C%20and%0Aachieves%20state-of-the-art%20performance%20among%20self-supervised%20methods%20on%20three%0Acommonly%20used%206DoF%20PE%20datasets%20including%20LINEMOD%20%28%2B4.2%25%29%2C%20Occlusion%20LINEMOD%0A%28%2B2%25%29%2C%20and%20YCB-Video%20%28%2B3%25%29.%20It%20also%20compares%20favorably%20to%20fully%20supervised%0Amethods%20on%20all%20six%20applicable%20BOP%20core%20datasets%2C%20achieving%20within%20-11.3%25%20to%0A%2B0.2%25%20of%20the%20top%20fully%20supervised%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09500v3&entry.124074799=Read"},
{"title": "Rethinking the Architecture Design for Efficient Generic Event Boundary\n  Detection", "author": "Ziwei Zheng and Zechuan Zhang and Yulin Wang and Shiji Song and Gao Huang and Le Yang", "abstract": "  Generic event boundary detection (GEBD), inspired by human visual cognitive\nbehaviors of consistently segmenting videos into meaningful temporal chunks,\nfinds utility in various applications such as video editing and. In this paper,\nwe demonstrate that SOTA GEBD models often prioritize final performance over\nmodel complexity, resulting in low inference speed and hindering efficient\ndeployment in real-world scenarios. We contribute to addressing this challenge\nby experimentally reexamining the architecture of GEBD models and uncovering\nseveral surprising findings. Firstly, we reveal that a concise GEBD baseline\nmodel already achieves promising performance without any sophisticated design.\nSecondly, we find that the widely applied image-domain backbones in GEBD models\ncan contain plenty of architecture redundancy, motivating us to gradually\n``modernize'' each component to enhance efficiency. Thirdly, we show that the\nGEBD models using image-domain backbones conducting the spatiotemporal learning\nin a spatial-then-temporal greedy manner can suffer from a distraction issue,\nwhich might be the inefficient villain for GEBD. Using a video-domain backbone\nto jointly conduct spatiotemporal modeling is an effective solution for this\nissue. The outcome of our exploration is a family of GEBD models, named\nEfficientGEBD, significantly outperforms the previous SOTA methods by up to\n1.7\\% performance gain and 280\\% speedup under the same backbone. Our research\nprompts the community to design modern GEBD methods with the consideration of\nmodel complexity, particularly in resource-aware applications. The code is\navailable at \\url{https://github.com/Ziwei-Zheng/EfficientGEBD}.\n", "link": "http://arxiv.org/abs/2407.12622v1", "date": "2024-07-17", "relevancy": 2.2041, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5535}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5506}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Architecture%20Design%20for%20Efficient%20Generic%20Event%20Boundary%0A%20%20Detection&body=Title%3A%20Rethinking%20the%20Architecture%20Design%20for%20Efficient%20Generic%20Event%20Boundary%0A%20%20Detection%0AAuthor%3A%20Ziwei%20Zheng%20and%20Zechuan%20Zhang%20and%20Yulin%20Wang%20and%20Shiji%20Song%20and%20Gao%20Huang%20and%20Le%20Yang%0AAbstract%3A%20%20%20Generic%20event%20boundary%20detection%20%28GEBD%29%2C%20inspired%20by%20human%20visual%20cognitive%0Abehaviors%20of%20consistently%20segmenting%20videos%20into%20meaningful%20temporal%20chunks%2C%0Afinds%20utility%20in%20various%20applications%20such%20as%20video%20editing%20and.%20In%20this%20paper%2C%0Awe%20demonstrate%20that%20SOTA%20GEBD%20models%20often%20prioritize%20final%20performance%20over%0Amodel%20complexity%2C%20resulting%20in%20low%20inference%20speed%20and%20hindering%20efficient%0Adeployment%20in%20real-world%20scenarios.%20We%20contribute%20to%20addressing%20this%20challenge%0Aby%20experimentally%20reexamining%20the%20architecture%20of%20GEBD%20models%20and%20uncovering%0Aseveral%20surprising%20findings.%20Firstly%2C%20we%20reveal%20that%20a%20concise%20GEBD%20baseline%0Amodel%20already%20achieves%20promising%20performance%20without%20any%20sophisticated%20design.%0ASecondly%2C%20we%20find%20that%20the%20widely%20applied%20image-domain%20backbones%20in%20GEBD%20models%0Acan%20contain%20plenty%20of%20architecture%20redundancy%2C%20motivating%20us%20to%20gradually%0A%60%60modernize%27%27%20each%20component%20to%20enhance%20efficiency.%20Thirdly%2C%20we%20show%20that%20the%0AGEBD%20models%20using%20image-domain%20backbones%20conducting%20the%20spatiotemporal%20learning%0Ain%20a%20spatial-then-temporal%20greedy%20manner%20can%20suffer%20from%20a%20distraction%20issue%2C%0Awhich%20might%20be%20the%20inefficient%20villain%20for%20GEBD.%20Using%20a%20video-domain%20backbone%0Ato%20jointly%20conduct%20spatiotemporal%20modeling%20is%20an%20effective%20solution%20for%20this%0Aissue.%20The%20outcome%20of%20our%20exploration%20is%20a%20family%20of%20GEBD%20models%2C%20named%0AEfficientGEBD%2C%20significantly%20outperforms%20the%20previous%20SOTA%20methods%20by%20up%20to%0A1.7%5C%25%20performance%20gain%20and%20280%5C%25%20speedup%20under%20the%20same%20backbone.%20Our%20research%0Aprompts%20the%20community%20to%20design%20modern%20GEBD%20methods%20with%20the%20consideration%20of%0Amodel%20complexity%2C%20particularly%20in%20resource-aware%20applications.%20The%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/Ziwei-Zheng/EfficientGEBD%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Architecture%2520Design%2520for%2520Efficient%2520Generic%2520Event%2520Boundary%250A%2520%2520Detection%26entry.906535625%3DZiwei%2520Zheng%2520and%2520Zechuan%2520Zhang%2520and%2520Yulin%2520Wang%2520and%2520Shiji%2520Song%2520and%2520Gao%2520Huang%2520and%2520Le%2520Yang%26entry.1292438233%3D%2520%2520Generic%2520event%2520boundary%2520detection%2520%2528GEBD%2529%252C%2520inspired%2520by%2520human%2520visual%2520cognitive%250Abehaviors%2520of%2520consistently%2520segmenting%2520videos%2520into%2520meaningful%2520temporal%2520chunks%252C%250Afinds%2520utility%2520in%2520various%2520applications%2520such%2520as%2520video%2520editing%2520and.%2520In%2520this%2520paper%252C%250Awe%2520demonstrate%2520that%2520SOTA%2520GEBD%2520models%2520often%2520prioritize%2520final%2520performance%2520over%250Amodel%2520complexity%252C%2520resulting%2520in%2520low%2520inference%2520speed%2520and%2520hindering%2520efficient%250Adeployment%2520in%2520real-world%2520scenarios.%2520We%2520contribute%2520to%2520addressing%2520this%2520challenge%250Aby%2520experimentally%2520reexamining%2520the%2520architecture%2520of%2520GEBD%2520models%2520and%2520uncovering%250Aseveral%2520surprising%2520findings.%2520Firstly%252C%2520we%2520reveal%2520that%2520a%2520concise%2520GEBD%2520baseline%250Amodel%2520already%2520achieves%2520promising%2520performance%2520without%2520any%2520sophisticated%2520design.%250ASecondly%252C%2520we%2520find%2520that%2520the%2520widely%2520applied%2520image-domain%2520backbones%2520in%2520GEBD%2520models%250Acan%2520contain%2520plenty%2520of%2520architecture%2520redundancy%252C%2520motivating%2520us%2520to%2520gradually%250A%2560%2560modernize%2527%2527%2520each%2520component%2520to%2520enhance%2520efficiency.%2520Thirdly%252C%2520we%2520show%2520that%2520the%250AGEBD%2520models%2520using%2520image-domain%2520backbones%2520conducting%2520the%2520spatiotemporal%2520learning%250Ain%2520a%2520spatial-then-temporal%2520greedy%2520manner%2520can%2520suffer%2520from%2520a%2520distraction%2520issue%252C%250Awhich%2520might%2520be%2520the%2520inefficient%2520villain%2520for%2520GEBD.%2520Using%2520a%2520video-domain%2520backbone%250Ato%2520jointly%2520conduct%2520spatiotemporal%2520modeling%2520is%2520an%2520effective%2520solution%2520for%2520this%250Aissue.%2520The%2520outcome%2520of%2520our%2520exploration%2520is%2520a%2520family%2520of%2520GEBD%2520models%252C%2520named%250AEfficientGEBD%252C%2520significantly%2520outperforms%2520the%2520previous%2520SOTA%2520methods%2520by%2520up%2520to%250A1.7%255C%2525%2520performance%2520gain%2520and%2520280%255C%2525%2520speedup%2520under%2520the%2520same%2520backbone.%2520Our%2520research%250Aprompts%2520the%2520community%2520to%2520design%2520modern%2520GEBD%2520methods%2520with%2520the%2520consideration%2520of%250Amodel%2520complexity%252C%2520particularly%2520in%2520resource-aware%2520applications.%2520The%2520code%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/Ziwei-Zheng/EfficientGEBD%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Architecture%20Design%20for%20Efficient%20Generic%20Event%20Boundary%0A%20%20Detection&entry.906535625=Ziwei%20Zheng%20and%20Zechuan%20Zhang%20and%20Yulin%20Wang%20and%20Shiji%20Song%20and%20Gao%20Huang%20and%20Le%20Yang&entry.1292438233=%20%20Generic%20event%20boundary%20detection%20%28GEBD%29%2C%20inspired%20by%20human%20visual%20cognitive%0Abehaviors%20of%20consistently%20segmenting%20videos%20into%20meaningful%20temporal%20chunks%2C%0Afinds%20utility%20in%20various%20applications%20such%20as%20video%20editing%20and.%20In%20this%20paper%2C%0Awe%20demonstrate%20that%20SOTA%20GEBD%20models%20often%20prioritize%20final%20performance%20over%0Amodel%20complexity%2C%20resulting%20in%20low%20inference%20speed%20and%20hindering%20efficient%0Adeployment%20in%20real-world%20scenarios.%20We%20contribute%20to%20addressing%20this%20challenge%0Aby%20experimentally%20reexamining%20the%20architecture%20of%20GEBD%20models%20and%20uncovering%0Aseveral%20surprising%20findings.%20Firstly%2C%20we%20reveal%20that%20a%20concise%20GEBD%20baseline%0Amodel%20already%20achieves%20promising%20performance%20without%20any%20sophisticated%20design.%0ASecondly%2C%20we%20find%20that%20the%20widely%20applied%20image-domain%20backbones%20in%20GEBD%20models%0Acan%20contain%20plenty%20of%20architecture%20redundancy%2C%20motivating%20us%20to%20gradually%0A%60%60modernize%27%27%20each%20component%20to%20enhance%20efficiency.%20Thirdly%2C%20we%20show%20that%20the%0AGEBD%20models%20using%20image-domain%20backbones%20conducting%20the%20spatiotemporal%20learning%0Ain%20a%20spatial-then-temporal%20greedy%20manner%20can%20suffer%20from%20a%20distraction%20issue%2C%0Awhich%20might%20be%20the%20inefficient%20villain%20for%20GEBD.%20Using%20a%20video-domain%20backbone%0Ato%20jointly%20conduct%20spatiotemporal%20modeling%20is%20an%20effective%20solution%20for%20this%0Aissue.%20The%20outcome%20of%20our%20exploration%20is%20a%20family%20of%20GEBD%20models%2C%20named%0AEfficientGEBD%2C%20significantly%20outperforms%20the%20previous%20SOTA%20methods%20by%20up%20to%0A1.7%5C%25%20performance%20gain%20and%20280%5C%25%20speedup%20under%20the%20same%20backbone.%20Our%20research%0Aprompts%20the%20community%20to%20design%20modern%20GEBD%20methods%20with%20the%20consideration%20of%0Amodel%20complexity%2C%20particularly%20in%20resource-aware%20applications.%20The%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/Ziwei-Zheng/EfficientGEBD%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12622v1&entry.124074799=Read"},
{"title": "MMVR: Millimeter-wave Multi-View Radar Dataset and Benchmark for Indoor\n  Perception", "author": "M. Mahbubur Rahman and Ryoma Yataka and Sorachi Kato and Pu Perry Wang and Peizhao Li and Adriano Cardace and Petros Boufounos", "abstract": "  Compared with an extensive list of automotive radar datasets that support\nautonomous driving, indoor radar datasets are scarce at a smaller scale in the\nformat of low-resolution radar point clouds and usually under an open-space\nsingle-room setting. In this paper, we scale up indoor radar data collection\nusing multi-view high-resolution radar heatmap in a multi-day, multi-room, and\nmulti-subject setting, with an emphasis on the diversity of environment and\nsubjects. Referred to as the millimeter-wave multi-view radar (MMVR) dataset,\nit consists of $345$K multi-view radar frames collected from $25$ human\nsubjects over $6$ different rooms, $446$K annotated bounding boxes/segmentation\ninstances, and $7.59$ million annotated keypoints to support three major\nperception tasks of object detection, pose estimation, and instance\nsegmentation, respectively. For each task, we report performance benchmarks\nunder two protocols: a single subject in an open space and multiple subjects in\nseveral cluttered rooms with two data splits: random split and\ncross-environment split over $395$ 1-min data segments. We anticipate that MMVR\nfacilitates indoor radar perception development for indoor vehicle\n(robot/humanoid) navigation, building energy management, and elderly care for\nbetter efficiency, user experience, and safety. The MMVR dataset is available\nat https://doi.org/10.5281/zenodo.12611978.\n", "link": "http://arxiv.org/abs/2406.10708v2", "date": "2024-07-17", "relevancy": 2.1949, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5685}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5648}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMVR%3A%20Millimeter-wave%20Multi-View%20Radar%20Dataset%20and%20Benchmark%20for%20Indoor%0A%20%20Perception&body=Title%3A%20MMVR%3A%20Millimeter-wave%20Multi-View%20Radar%20Dataset%20and%20Benchmark%20for%20Indoor%0A%20%20Perception%0AAuthor%3A%20M.%20Mahbubur%20Rahman%20and%20Ryoma%20Yataka%20and%20Sorachi%20Kato%20and%20Pu%20Perry%20Wang%20and%20Peizhao%20Li%20and%20Adriano%20Cardace%20and%20Petros%20Boufounos%0AAbstract%3A%20%20%20Compared%20with%20an%20extensive%20list%20of%20automotive%20radar%20datasets%20that%20support%0Aautonomous%20driving%2C%20indoor%20radar%20datasets%20are%20scarce%20at%20a%20smaller%20scale%20in%20the%0Aformat%20of%20low-resolution%20radar%20point%20clouds%20and%20usually%20under%20an%20open-space%0Asingle-room%20setting.%20In%20this%20paper%2C%20we%20scale%20up%20indoor%20radar%20data%20collection%0Ausing%20multi-view%20high-resolution%20radar%20heatmap%20in%20a%20multi-day%2C%20multi-room%2C%20and%0Amulti-subject%20setting%2C%20with%20an%20emphasis%20on%20the%20diversity%20of%20environment%20and%0Asubjects.%20Referred%20to%20as%20the%20millimeter-wave%20multi-view%20radar%20%28MMVR%29%20dataset%2C%0Ait%20consists%20of%20%24345%24K%20multi-view%20radar%20frames%20collected%20from%20%2425%24%20human%0Asubjects%20over%20%246%24%20different%20rooms%2C%20%24446%24K%20annotated%20bounding%20boxes/segmentation%0Ainstances%2C%20and%20%247.59%24%20million%20annotated%20keypoints%20to%20support%20three%20major%0Aperception%20tasks%20of%20object%20detection%2C%20pose%20estimation%2C%20and%20instance%0Asegmentation%2C%20respectively.%20For%20each%20task%2C%20we%20report%20performance%20benchmarks%0Aunder%20two%20protocols%3A%20a%20single%20subject%20in%20an%20open%20space%20and%20multiple%20subjects%20in%0Aseveral%20cluttered%20rooms%20with%20two%20data%20splits%3A%20random%20split%20and%0Across-environment%20split%20over%20%24395%24%201-min%20data%20segments.%20We%20anticipate%20that%20MMVR%0Afacilitates%20indoor%20radar%20perception%20development%20for%20indoor%20vehicle%0A%28robot/humanoid%29%20navigation%2C%20building%20energy%20management%2C%20and%20elderly%20care%20for%0Abetter%20efficiency%2C%20user%20experience%2C%20and%20safety.%20The%20MMVR%20dataset%20is%20available%0Aat%20https%3A//doi.org/10.5281/zenodo.12611978.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMVR%253A%2520Millimeter-wave%2520Multi-View%2520Radar%2520Dataset%2520and%2520Benchmark%2520for%2520Indoor%250A%2520%2520Perception%26entry.906535625%3DM.%2520Mahbubur%2520Rahman%2520and%2520Ryoma%2520Yataka%2520and%2520Sorachi%2520Kato%2520and%2520Pu%2520Perry%2520Wang%2520and%2520Peizhao%2520Li%2520and%2520Adriano%2520Cardace%2520and%2520Petros%2520Boufounos%26entry.1292438233%3D%2520%2520Compared%2520with%2520an%2520extensive%2520list%2520of%2520automotive%2520radar%2520datasets%2520that%2520support%250Aautonomous%2520driving%252C%2520indoor%2520radar%2520datasets%2520are%2520scarce%2520at%2520a%2520smaller%2520scale%2520in%2520the%250Aformat%2520of%2520low-resolution%2520radar%2520point%2520clouds%2520and%2520usually%2520under%2520an%2520open-space%250Asingle-room%2520setting.%2520In%2520this%2520paper%252C%2520we%2520scale%2520up%2520indoor%2520radar%2520data%2520collection%250Ausing%2520multi-view%2520high-resolution%2520radar%2520heatmap%2520in%2520a%2520multi-day%252C%2520multi-room%252C%2520and%250Amulti-subject%2520setting%252C%2520with%2520an%2520emphasis%2520on%2520the%2520diversity%2520of%2520environment%2520and%250Asubjects.%2520Referred%2520to%2520as%2520the%2520millimeter-wave%2520multi-view%2520radar%2520%2528MMVR%2529%2520dataset%252C%250Ait%2520consists%2520of%2520%2524345%2524K%2520multi-view%2520radar%2520frames%2520collected%2520from%2520%252425%2524%2520human%250Asubjects%2520over%2520%25246%2524%2520different%2520rooms%252C%2520%2524446%2524K%2520annotated%2520bounding%2520boxes/segmentation%250Ainstances%252C%2520and%2520%25247.59%2524%2520million%2520annotated%2520keypoints%2520to%2520support%2520three%2520major%250Aperception%2520tasks%2520of%2520object%2520detection%252C%2520pose%2520estimation%252C%2520and%2520instance%250Asegmentation%252C%2520respectively.%2520For%2520each%2520task%252C%2520we%2520report%2520performance%2520benchmarks%250Aunder%2520two%2520protocols%253A%2520a%2520single%2520subject%2520in%2520an%2520open%2520space%2520and%2520multiple%2520subjects%2520in%250Aseveral%2520cluttered%2520rooms%2520with%2520two%2520data%2520splits%253A%2520random%2520split%2520and%250Across-environment%2520split%2520over%2520%2524395%2524%25201-min%2520data%2520segments.%2520We%2520anticipate%2520that%2520MMVR%250Afacilitates%2520indoor%2520radar%2520perception%2520development%2520for%2520indoor%2520vehicle%250A%2528robot/humanoid%2529%2520navigation%252C%2520building%2520energy%2520management%252C%2520and%2520elderly%2520care%2520for%250Abetter%2520efficiency%252C%2520user%2520experience%252C%2520and%2520safety.%2520The%2520MMVR%2520dataset%2520is%2520available%250Aat%2520https%253A//doi.org/10.5281/zenodo.12611978.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMVR%3A%20Millimeter-wave%20Multi-View%20Radar%20Dataset%20and%20Benchmark%20for%20Indoor%0A%20%20Perception&entry.906535625=M.%20Mahbubur%20Rahman%20and%20Ryoma%20Yataka%20and%20Sorachi%20Kato%20and%20Pu%20Perry%20Wang%20and%20Peizhao%20Li%20and%20Adriano%20Cardace%20and%20Petros%20Boufounos&entry.1292438233=%20%20Compared%20with%20an%20extensive%20list%20of%20automotive%20radar%20datasets%20that%20support%0Aautonomous%20driving%2C%20indoor%20radar%20datasets%20are%20scarce%20at%20a%20smaller%20scale%20in%20the%0Aformat%20of%20low-resolution%20radar%20point%20clouds%20and%20usually%20under%20an%20open-space%0Asingle-room%20setting.%20In%20this%20paper%2C%20we%20scale%20up%20indoor%20radar%20data%20collection%0Ausing%20multi-view%20high-resolution%20radar%20heatmap%20in%20a%20multi-day%2C%20multi-room%2C%20and%0Amulti-subject%20setting%2C%20with%20an%20emphasis%20on%20the%20diversity%20of%20environment%20and%0Asubjects.%20Referred%20to%20as%20the%20millimeter-wave%20multi-view%20radar%20%28MMVR%29%20dataset%2C%0Ait%20consists%20of%20%24345%24K%20multi-view%20radar%20frames%20collected%20from%20%2425%24%20human%0Asubjects%20over%20%246%24%20different%20rooms%2C%20%24446%24K%20annotated%20bounding%20boxes/segmentation%0Ainstances%2C%20and%20%247.59%24%20million%20annotated%20keypoints%20to%20support%20three%20major%0Aperception%20tasks%20of%20object%20detection%2C%20pose%20estimation%2C%20and%20instance%0Asegmentation%2C%20respectively.%20For%20each%20task%2C%20we%20report%20performance%20benchmarks%0Aunder%20two%20protocols%3A%20a%20single%20subject%20in%20an%20open%20space%20and%20multiple%20subjects%20in%0Aseveral%20cluttered%20rooms%20with%20two%20data%20splits%3A%20random%20split%20and%0Across-environment%20split%20over%20%24395%24%201-min%20data%20segments.%20We%20anticipate%20that%20MMVR%0Afacilitates%20indoor%20radar%20perception%20development%20for%20indoor%20vehicle%0A%28robot/humanoid%29%20navigation%2C%20building%20energy%20management%2C%20and%20elderly%20care%20for%0Abetter%20efficiency%2C%20user%20experience%2C%20and%20safety.%20The%20MMVR%20dataset%20is%20available%0Aat%20https%3A//doi.org/10.5281/zenodo.12611978.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10708v2&entry.124074799=Read"},
{"title": "Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive\n  Prompt", "author": "Chenxi Liu and Zhenyi Wang and Tianyi Xiong and Ruibo Chen and Yihan Wu and Junfeng Guo and Heng Huang", "abstract": "  Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn\nnew classes with scarce samples while preserving knowledge of old ones.\nExisting FSCIL methods usually fine-tune the entire backbone, leading to\noverfitting and hindering the potential to learn new classes. On the other\nhand, recent prompt-based CIL approaches alleviate forgetting by training\nprompts with sufficient data in each task. In this work, we propose a novel\nframework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages\ntask-invariant prompts to capture shared knowledge by reducing specific\ninformation from the attention aspect. Additionally, self-adaptive\ntask-specific prompts in ASP provide specific information and transfer\nknowledge from old classes to new classes with an Information Bottleneck\nlearning objective. In summary, ASP prevents overfitting on base task and does\nnot require enormous data in few-shot incremental tasks. Extensive experiments\non three benchmark datasets validate that ASP consistently outperforms\nstate-of-the-art FSCIL and prompt-based CIL methods in terms of both learning\nnew classes and mitigating forgetting.\n", "link": "http://arxiv.org/abs/2403.09857v3", "date": "2024-07-17", "relevancy": 2.1915, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4573}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4403}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Class%20Incremental%20Learning%20with%20Attention-Aware%20Self-Adaptive%0A%20%20Prompt&body=Title%3A%20Few-Shot%20Class%20Incremental%20Learning%20with%20Attention-Aware%20Self-Adaptive%0A%20%20Prompt%0AAuthor%3A%20Chenxi%20Liu%20and%20Zhenyi%20Wang%20and%20Tianyi%20Xiong%20and%20Ruibo%20Chen%20and%20Yihan%20Wu%20and%20Junfeng%20Guo%20and%20Heng%20Huang%0AAbstract%3A%20%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20models%20aim%20to%20incrementally%20learn%0Anew%20classes%20with%20scarce%20samples%20while%20preserving%20knowledge%20of%20old%20ones.%0AExisting%20FSCIL%20methods%20usually%20fine-tune%20the%20entire%20backbone%2C%20leading%20to%0Aoverfitting%20and%20hindering%20the%20potential%20to%20learn%20new%20classes.%20On%20the%20other%0Ahand%2C%20recent%20prompt-based%20CIL%20approaches%20alleviate%20forgetting%20by%20training%0Aprompts%20with%20sufficient%20data%20in%20each%20task.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aframework%20named%20Attention-aware%20Self-adaptive%20Prompt%20%28ASP%29.%20ASP%20encourages%0Atask-invariant%20prompts%20to%20capture%20shared%20knowledge%20by%20reducing%20specific%0Ainformation%20from%20the%20attention%20aspect.%20Additionally%2C%20self-adaptive%0Atask-specific%20prompts%20in%20ASP%20provide%20specific%20information%20and%20transfer%0Aknowledge%20from%20old%20classes%20to%20new%20classes%20with%20an%20Information%20Bottleneck%0Alearning%20objective.%20In%20summary%2C%20ASP%20prevents%20overfitting%20on%20base%20task%20and%20does%0Anot%20require%20enormous%20data%20in%20few-shot%20incremental%20tasks.%20Extensive%20experiments%0Aon%20three%20benchmark%20datasets%20validate%20that%20ASP%20consistently%20outperforms%0Astate-of-the-art%20FSCIL%20and%20prompt-based%20CIL%20methods%20in%20terms%20of%20both%20learning%0Anew%20classes%20and%20mitigating%20forgetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09857v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Class%2520Incremental%2520Learning%2520with%2520Attention-Aware%2520Self-Adaptive%250A%2520%2520Prompt%26entry.906535625%3DChenxi%2520Liu%2520and%2520Zhenyi%2520Wang%2520and%2520Tianyi%2520Xiong%2520and%2520Ruibo%2520Chen%2520and%2520Yihan%2520Wu%2520and%2520Junfeng%2520Guo%2520and%2520Heng%2520Huang%26entry.1292438233%3D%2520%2520Few-Shot%2520Class-Incremental%2520Learning%2520%2528FSCIL%2529%2520models%2520aim%2520to%2520incrementally%2520learn%250Anew%2520classes%2520with%2520scarce%2520samples%2520while%2520preserving%2520knowledge%2520of%2520old%2520ones.%250AExisting%2520FSCIL%2520methods%2520usually%2520fine-tune%2520the%2520entire%2520backbone%252C%2520leading%2520to%250Aoverfitting%2520and%2520hindering%2520the%2520potential%2520to%2520learn%2520new%2520classes.%2520On%2520the%2520other%250Ahand%252C%2520recent%2520prompt-based%2520CIL%2520approaches%2520alleviate%2520forgetting%2520by%2520training%250Aprompts%2520with%2520sufficient%2520data%2520in%2520each%2520task.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520named%2520Attention-aware%2520Self-adaptive%2520Prompt%2520%2528ASP%2529.%2520ASP%2520encourages%250Atask-invariant%2520prompts%2520to%2520capture%2520shared%2520knowledge%2520by%2520reducing%2520specific%250Ainformation%2520from%2520the%2520attention%2520aspect.%2520Additionally%252C%2520self-adaptive%250Atask-specific%2520prompts%2520in%2520ASP%2520provide%2520specific%2520information%2520and%2520transfer%250Aknowledge%2520from%2520old%2520classes%2520to%2520new%2520classes%2520with%2520an%2520Information%2520Bottleneck%250Alearning%2520objective.%2520In%2520summary%252C%2520ASP%2520prevents%2520overfitting%2520on%2520base%2520task%2520and%2520does%250Anot%2520require%2520enormous%2520data%2520in%2520few-shot%2520incremental%2520tasks.%2520Extensive%2520experiments%250Aon%2520three%2520benchmark%2520datasets%2520validate%2520that%2520ASP%2520consistently%2520outperforms%250Astate-of-the-art%2520FSCIL%2520and%2520prompt-based%2520CIL%2520methods%2520in%2520terms%2520of%2520both%2520learning%250Anew%2520classes%2520and%2520mitigating%2520forgetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09857v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Class%20Incremental%20Learning%20with%20Attention-Aware%20Self-Adaptive%0A%20%20Prompt&entry.906535625=Chenxi%20Liu%20and%20Zhenyi%20Wang%20and%20Tianyi%20Xiong%20and%20Ruibo%20Chen%20and%20Yihan%20Wu%20and%20Junfeng%20Guo%20and%20Heng%20Huang&entry.1292438233=%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20models%20aim%20to%20incrementally%20learn%0Anew%20classes%20with%20scarce%20samples%20while%20preserving%20knowledge%20of%20old%20ones.%0AExisting%20FSCIL%20methods%20usually%20fine-tune%20the%20entire%20backbone%2C%20leading%20to%0Aoverfitting%20and%20hindering%20the%20potential%20to%20learn%20new%20classes.%20On%20the%20other%0Ahand%2C%20recent%20prompt-based%20CIL%20approaches%20alleviate%20forgetting%20by%20training%0Aprompts%20with%20sufficient%20data%20in%20each%20task.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aframework%20named%20Attention-aware%20Self-adaptive%20Prompt%20%28ASP%29.%20ASP%20encourages%0Atask-invariant%20prompts%20to%20capture%20shared%20knowledge%20by%20reducing%20specific%0Ainformation%20from%20the%20attention%20aspect.%20Additionally%2C%20self-adaptive%0Atask-specific%20prompts%20in%20ASP%20provide%20specific%20information%20and%20transfer%0Aknowledge%20from%20old%20classes%20to%20new%20classes%20with%20an%20Information%20Bottleneck%0Alearning%20objective.%20In%20summary%2C%20ASP%20prevents%20overfitting%20on%20base%20task%20and%20does%0Anot%20require%20enormous%20data%20in%20few-shot%20incremental%20tasks.%20Extensive%20experiments%0Aon%20three%20benchmark%20datasets%20validate%20that%20ASP%20consistently%20outperforms%0Astate-of-the-art%20FSCIL%20and%20prompt-based%20CIL%20methods%20in%20terms%20of%20both%20learning%0Anew%20classes%20and%20mitigating%20forgetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09857v3&entry.124074799=Read"},
{"title": "Deep Mutual Learning among Partially Labeled Datasets for Multi-Organ\n  Segmentation", "author": "Xiaoyu Liu and Linhao Qu and Ziyue Xie and Yonghong Shi and Zhijian Song", "abstract": "  The task of labeling multiple organs for segmentation is a complex and\ntime-consuming process, resulting in a scarcity of comprehensively labeled\nmulti-organ datasets while the emergence of numerous partially labeled\ndatasets. Current methods are inadequate in effectively utilizing the\nsupervised information available from these datasets, thereby impeding the\nprogress in improving the segmentation accuracy. This paper proposes a\ntwo-stage multi-organ segmentation method based on mutual learning, aiming to\nimprove multi-organ segmentation performance by complementing information among\npartially labeled datasets. In the first stage, each partial-organ segmentation\nmodel utilizes the non-overlapping organ labels from different datasets and the\ndistinct organ features extracted by different models, introducing additional\nmutual difference learning to generate higher quality pseudo labels for\nunlabeled organs. In the second stage, each full-organ segmentation model is\nsupervised by fully labeled datasets with pseudo labels and leverages true\nlabels from other datasets, while dynamically sharing accurate features across\ndifferent models, introducing additional mutual similarity learning to enhance\nmulti-organ segmentation performance. Extensive experiments were conducted on\nnine datasets that included the head and neck, chest, abdomen, and pelvis. The\nresults indicate that our method has achieved SOTA performance in segmentation\ntasks that rely on partial labels, and the ablation studies have thoroughly\nconfirmed the efficacy of the mutual learning mechanism.\n", "link": "http://arxiv.org/abs/2407.12611v1", "date": "2024-07-17", "relevancy": 2.1907, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5599}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Mutual%20Learning%20among%20Partially%20Labeled%20Datasets%20for%20Multi-Organ%0A%20%20Segmentation&body=Title%3A%20Deep%20Mutual%20Learning%20among%20Partially%20Labeled%20Datasets%20for%20Multi-Organ%0A%20%20Segmentation%0AAuthor%3A%20Xiaoyu%20Liu%20and%20Linhao%20Qu%20and%20Ziyue%20Xie%20and%20Yonghong%20Shi%20and%20Zhijian%20Song%0AAbstract%3A%20%20%20The%20task%20of%20labeling%20multiple%20organs%20for%20segmentation%20is%20a%20complex%20and%0Atime-consuming%20process%2C%20resulting%20in%20a%20scarcity%20of%20comprehensively%20labeled%0Amulti-organ%20datasets%20while%20the%20emergence%20of%20numerous%20partially%20labeled%0Adatasets.%20Current%20methods%20are%20inadequate%20in%20effectively%20utilizing%20the%0Asupervised%20information%20available%20from%20these%20datasets%2C%20thereby%20impeding%20the%0Aprogress%20in%20improving%20the%20segmentation%20accuracy.%20This%20paper%20proposes%20a%0Atwo-stage%20multi-organ%20segmentation%20method%20based%20on%20mutual%20learning%2C%20aiming%20to%0Aimprove%20multi-organ%20segmentation%20performance%20by%20complementing%20information%20among%0Apartially%20labeled%20datasets.%20In%20the%20first%20stage%2C%20each%20partial-organ%20segmentation%0Amodel%20utilizes%20the%20non-overlapping%20organ%20labels%20from%20different%20datasets%20and%20the%0Adistinct%20organ%20features%20extracted%20by%20different%20models%2C%20introducing%20additional%0Amutual%20difference%20learning%20to%20generate%20higher%20quality%20pseudo%20labels%20for%0Aunlabeled%20organs.%20In%20the%20second%20stage%2C%20each%20full-organ%20segmentation%20model%20is%0Asupervised%20by%20fully%20labeled%20datasets%20with%20pseudo%20labels%20and%20leverages%20true%0Alabels%20from%20other%20datasets%2C%20while%20dynamically%20sharing%20accurate%20features%20across%0Adifferent%20models%2C%20introducing%20additional%20mutual%20similarity%20learning%20to%20enhance%0Amulti-organ%20segmentation%20performance.%20Extensive%20experiments%20were%20conducted%20on%0Anine%20datasets%20that%20included%20the%20head%20and%20neck%2C%20chest%2C%20abdomen%2C%20and%20pelvis.%20The%0Aresults%20indicate%20that%20our%20method%20has%20achieved%20SOTA%20performance%20in%20segmentation%0Atasks%20that%20rely%20on%20partial%20labels%2C%20and%20the%20ablation%20studies%20have%20thoroughly%0Aconfirmed%20the%20efficacy%20of%20the%20mutual%20learning%20mechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Mutual%2520Learning%2520among%2520Partially%2520Labeled%2520Datasets%2520for%2520Multi-Organ%250A%2520%2520Segmentation%26entry.906535625%3DXiaoyu%2520Liu%2520and%2520Linhao%2520Qu%2520and%2520Ziyue%2520Xie%2520and%2520Yonghong%2520Shi%2520and%2520Zhijian%2520Song%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520labeling%2520multiple%2520organs%2520for%2520segmentation%2520is%2520a%2520complex%2520and%250Atime-consuming%2520process%252C%2520resulting%2520in%2520a%2520scarcity%2520of%2520comprehensively%2520labeled%250Amulti-organ%2520datasets%2520while%2520the%2520emergence%2520of%2520numerous%2520partially%2520labeled%250Adatasets.%2520Current%2520methods%2520are%2520inadequate%2520in%2520effectively%2520utilizing%2520the%250Asupervised%2520information%2520available%2520from%2520these%2520datasets%252C%2520thereby%2520impeding%2520the%250Aprogress%2520in%2520improving%2520the%2520segmentation%2520accuracy.%2520This%2520paper%2520proposes%2520a%250Atwo-stage%2520multi-organ%2520segmentation%2520method%2520based%2520on%2520mutual%2520learning%252C%2520aiming%2520to%250Aimprove%2520multi-organ%2520segmentation%2520performance%2520by%2520complementing%2520information%2520among%250Apartially%2520labeled%2520datasets.%2520In%2520the%2520first%2520stage%252C%2520each%2520partial-organ%2520segmentation%250Amodel%2520utilizes%2520the%2520non-overlapping%2520organ%2520labels%2520from%2520different%2520datasets%2520and%2520the%250Adistinct%2520organ%2520features%2520extracted%2520by%2520different%2520models%252C%2520introducing%2520additional%250Amutual%2520difference%2520learning%2520to%2520generate%2520higher%2520quality%2520pseudo%2520labels%2520for%250Aunlabeled%2520organs.%2520In%2520the%2520second%2520stage%252C%2520each%2520full-organ%2520segmentation%2520model%2520is%250Asupervised%2520by%2520fully%2520labeled%2520datasets%2520with%2520pseudo%2520labels%2520and%2520leverages%2520true%250Alabels%2520from%2520other%2520datasets%252C%2520while%2520dynamically%2520sharing%2520accurate%2520features%2520across%250Adifferent%2520models%252C%2520introducing%2520additional%2520mutual%2520similarity%2520learning%2520to%2520enhance%250Amulti-organ%2520segmentation%2520performance.%2520Extensive%2520experiments%2520were%2520conducted%2520on%250Anine%2520datasets%2520that%2520included%2520the%2520head%2520and%2520neck%252C%2520chest%252C%2520abdomen%252C%2520and%2520pelvis.%2520The%250Aresults%2520indicate%2520that%2520our%2520method%2520has%2520achieved%2520SOTA%2520performance%2520in%2520segmentation%250Atasks%2520that%2520rely%2520on%2520partial%2520labels%252C%2520and%2520the%2520ablation%2520studies%2520have%2520thoroughly%250Aconfirmed%2520the%2520efficacy%2520of%2520the%2520mutual%2520learning%2520mechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Mutual%20Learning%20among%20Partially%20Labeled%20Datasets%20for%20Multi-Organ%0A%20%20Segmentation&entry.906535625=Xiaoyu%20Liu%20and%20Linhao%20Qu%20and%20Ziyue%20Xie%20and%20Yonghong%20Shi%20and%20Zhijian%20Song&entry.1292438233=%20%20The%20task%20of%20labeling%20multiple%20organs%20for%20segmentation%20is%20a%20complex%20and%0Atime-consuming%20process%2C%20resulting%20in%20a%20scarcity%20of%20comprehensively%20labeled%0Amulti-organ%20datasets%20while%20the%20emergence%20of%20numerous%20partially%20labeled%0Adatasets.%20Current%20methods%20are%20inadequate%20in%20effectively%20utilizing%20the%0Asupervised%20information%20available%20from%20these%20datasets%2C%20thereby%20impeding%20the%0Aprogress%20in%20improving%20the%20segmentation%20accuracy.%20This%20paper%20proposes%20a%0Atwo-stage%20multi-organ%20segmentation%20method%20based%20on%20mutual%20learning%2C%20aiming%20to%0Aimprove%20multi-organ%20segmentation%20performance%20by%20complementing%20information%20among%0Apartially%20labeled%20datasets.%20In%20the%20first%20stage%2C%20each%20partial-organ%20segmentation%0Amodel%20utilizes%20the%20non-overlapping%20organ%20labels%20from%20different%20datasets%20and%20the%0Adistinct%20organ%20features%20extracted%20by%20different%20models%2C%20introducing%20additional%0Amutual%20difference%20learning%20to%20generate%20higher%20quality%20pseudo%20labels%20for%0Aunlabeled%20organs.%20In%20the%20second%20stage%2C%20each%20full-organ%20segmentation%20model%20is%0Asupervised%20by%20fully%20labeled%20datasets%20with%20pseudo%20labels%20and%20leverages%20true%0Alabels%20from%20other%20datasets%2C%20while%20dynamically%20sharing%20accurate%20features%20across%0Adifferent%20models%2C%20introducing%20additional%20mutual%20similarity%20learning%20to%20enhance%0Amulti-organ%20segmentation%20performance.%20Extensive%20experiments%20were%20conducted%20on%0Anine%20datasets%20that%20included%20the%20head%20and%20neck%2C%20chest%2C%20abdomen%2C%20and%20pelvis.%20The%0Aresults%20indicate%20that%20our%20method%20has%20achieved%20SOTA%20performance%20in%20segmentation%0Atasks%20that%20rely%20on%20partial%20labels%2C%20and%20the%20ablation%20studies%20have%20thoroughly%0Aconfirmed%20the%20efficacy%20of%20the%20mutual%20learning%20mechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12611v1&entry.124074799=Read"},
{"title": "PV-S3: Advancing Automatic Photovoltaic Defect Detection using\n  Semi-Supervised Semantic Segmentation of Electroluminescence Images", "author": "Abhishek Jha and Yogesh Rawat and Shruti Vyas", "abstract": "  Photovoltaic (PV) systems allow us to tap into all abundant solar energy,\nhowever they require regular maintenance for high efficiency and to prevent\ndegradation. Traditional manual health check, using Electroluminescence (EL)\nimaging, is expensive and logistically challenging which makes automated defect\ndetection essential. Current automation approaches require extensive manual\nexpert labeling, which is time-consuming, expensive, and prone to errors. We\npropose PV-S3 (Photovoltaic-Semi Supervised Segmentation), a Semi-Supervised\nLearning approach for semantic segmentation of defects in EL images that\nreduces reliance on extensive labeling. PV-S3 is a Deep learning model trained\nusing a few labeled images along with numerous unlabeled images. We evaluate\nPV-S3 on multiple datasets and demonstrate its effectiveness and adaptability.\nWith merely 20% labeled samples, we achieve an absolute improvement of 9.7% in\nIoU, 13.5% in Precision, 29.15% in Recall, and 20.42% in F1-Score over prior\nstate-of-the-art supervised method (which uses 100% labeled samples) on UCF-EL\ndataset (largest dataset available for semantic segmentation of EL\nimages)showing improvement in performance while reducing the annotation costs\nby 80%.\n", "link": "http://arxiv.org/abs/2404.13693v2", "date": "2024-07-17", "relevancy": 2.1891, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5727}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5552}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PV-S3%3A%20Advancing%20Automatic%20Photovoltaic%20Defect%20Detection%20using%0A%20%20Semi-Supervised%20Semantic%20Segmentation%20of%20Electroluminescence%20Images&body=Title%3A%20PV-S3%3A%20Advancing%20Automatic%20Photovoltaic%20Defect%20Detection%20using%0A%20%20Semi-Supervised%20Semantic%20Segmentation%20of%20Electroluminescence%20Images%0AAuthor%3A%20Abhishek%20Jha%20and%20Yogesh%20Rawat%20and%20Shruti%20Vyas%0AAbstract%3A%20%20%20Photovoltaic%20%28PV%29%20systems%20allow%20us%20to%20tap%20into%20all%20abundant%20solar%20energy%2C%0Ahowever%20they%20require%20regular%20maintenance%20for%20high%20efficiency%20and%20to%20prevent%0Adegradation.%20Traditional%20manual%20health%20check%2C%20using%20Electroluminescence%20%28EL%29%0Aimaging%2C%20is%20expensive%20and%20logistically%20challenging%20which%20makes%20automated%20defect%0Adetection%20essential.%20Current%20automation%20approaches%20require%20extensive%20manual%0Aexpert%20labeling%2C%20which%20is%20time-consuming%2C%20expensive%2C%20and%20prone%20to%20errors.%20We%0Apropose%20PV-S3%20%28Photovoltaic-Semi%20Supervised%20Segmentation%29%2C%20a%20Semi-Supervised%0ALearning%20approach%20for%20semantic%20segmentation%20of%20defects%20in%20EL%20images%20that%0Areduces%20reliance%20on%20extensive%20labeling.%20PV-S3%20is%20a%20Deep%20learning%20model%20trained%0Ausing%20a%20few%20labeled%20images%20along%20with%20numerous%20unlabeled%20images.%20We%20evaluate%0APV-S3%20on%20multiple%20datasets%20and%20demonstrate%20its%20effectiveness%20and%20adaptability.%0AWith%20merely%2020%25%20labeled%20samples%2C%20we%20achieve%20an%20absolute%20improvement%20of%209.7%25%20in%0AIoU%2C%2013.5%25%20in%20Precision%2C%2029.15%25%20in%20Recall%2C%20and%2020.42%25%20in%20F1-Score%20over%20prior%0Astate-of-the-art%20supervised%20method%20%28which%20uses%20100%25%20labeled%20samples%29%20on%20UCF-EL%0Adataset%20%28largest%20dataset%20available%20for%20semantic%20segmentation%20of%20EL%0Aimages%29showing%20improvement%20in%20performance%20while%20reducing%20the%20annotation%20costs%0Aby%2080%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPV-S3%253A%2520Advancing%2520Automatic%2520Photovoltaic%2520Defect%2520Detection%2520using%250A%2520%2520Semi-Supervised%2520Semantic%2520Segmentation%2520of%2520Electroluminescence%2520Images%26entry.906535625%3DAbhishek%2520Jha%2520and%2520Yogesh%2520Rawat%2520and%2520Shruti%2520Vyas%26entry.1292438233%3D%2520%2520Photovoltaic%2520%2528PV%2529%2520systems%2520allow%2520us%2520to%2520tap%2520into%2520all%2520abundant%2520solar%2520energy%252C%250Ahowever%2520they%2520require%2520regular%2520maintenance%2520for%2520high%2520efficiency%2520and%2520to%2520prevent%250Adegradation.%2520Traditional%2520manual%2520health%2520check%252C%2520using%2520Electroluminescence%2520%2528EL%2529%250Aimaging%252C%2520is%2520expensive%2520and%2520logistically%2520challenging%2520which%2520makes%2520automated%2520defect%250Adetection%2520essential.%2520Current%2520automation%2520approaches%2520require%2520extensive%2520manual%250Aexpert%2520labeling%252C%2520which%2520is%2520time-consuming%252C%2520expensive%252C%2520and%2520prone%2520to%2520errors.%2520We%250Apropose%2520PV-S3%2520%2528Photovoltaic-Semi%2520Supervised%2520Segmentation%2529%252C%2520a%2520Semi-Supervised%250ALearning%2520approach%2520for%2520semantic%2520segmentation%2520of%2520defects%2520in%2520EL%2520images%2520that%250Areduces%2520reliance%2520on%2520extensive%2520labeling.%2520PV-S3%2520is%2520a%2520Deep%2520learning%2520model%2520trained%250Ausing%2520a%2520few%2520labeled%2520images%2520along%2520with%2520numerous%2520unlabeled%2520images.%2520We%2520evaluate%250APV-S3%2520on%2520multiple%2520datasets%2520and%2520demonstrate%2520its%2520effectiveness%2520and%2520adaptability.%250AWith%2520merely%252020%2525%2520labeled%2520samples%252C%2520we%2520achieve%2520an%2520absolute%2520improvement%2520of%25209.7%2525%2520in%250AIoU%252C%252013.5%2525%2520in%2520Precision%252C%252029.15%2525%2520in%2520Recall%252C%2520and%252020.42%2525%2520in%2520F1-Score%2520over%2520prior%250Astate-of-the-art%2520supervised%2520method%2520%2528which%2520uses%2520100%2525%2520labeled%2520samples%2529%2520on%2520UCF-EL%250Adataset%2520%2528largest%2520dataset%2520available%2520for%2520semantic%2520segmentation%2520of%2520EL%250Aimages%2529showing%2520improvement%2520in%2520performance%2520while%2520reducing%2520the%2520annotation%2520costs%250Aby%252080%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PV-S3%3A%20Advancing%20Automatic%20Photovoltaic%20Defect%20Detection%20using%0A%20%20Semi-Supervised%20Semantic%20Segmentation%20of%20Electroluminescence%20Images&entry.906535625=Abhishek%20Jha%20and%20Yogesh%20Rawat%20and%20Shruti%20Vyas&entry.1292438233=%20%20Photovoltaic%20%28PV%29%20systems%20allow%20us%20to%20tap%20into%20all%20abundant%20solar%20energy%2C%0Ahowever%20they%20require%20regular%20maintenance%20for%20high%20efficiency%20and%20to%20prevent%0Adegradation.%20Traditional%20manual%20health%20check%2C%20using%20Electroluminescence%20%28EL%29%0Aimaging%2C%20is%20expensive%20and%20logistically%20challenging%20which%20makes%20automated%20defect%0Adetection%20essential.%20Current%20automation%20approaches%20require%20extensive%20manual%0Aexpert%20labeling%2C%20which%20is%20time-consuming%2C%20expensive%2C%20and%20prone%20to%20errors.%20We%0Apropose%20PV-S3%20%28Photovoltaic-Semi%20Supervised%20Segmentation%29%2C%20a%20Semi-Supervised%0ALearning%20approach%20for%20semantic%20segmentation%20of%20defects%20in%20EL%20images%20that%0Areduces%20reliance%20on%20extensive%20labeling.%20PV-S3%20is%20a%20Deep%20learning%20model%20trained%0Ausing%20a%20few%20labeled%20images%20along%20with%20numerous%20unlabeled%20images.%20We%20evaluate%0APV-S3%20on%20multiple%20datasets%20and%20demonstrate%20its%20effectiveness%20and%20adaptability.%0AWith%20merely%2020%25%20labeled%20samples%2C%20we%20achieve%20an%20absolute%20improvement%20of%209.7%25%20in%0AIoU%2C%2013.5%25%20in%20Precision%2C%2029.15%25%20in%20Recall%2C%20and%2020.42%25%20in%20F1-Score%20over%20prior%0Astate-of-the-art%20supervised%20method%20%28which%20uses%20100%25%20labeled%20samples%29%20on%20UCF-EL%0Adataset%20%28largest%20dataset%20available%20for%20semantic%20segmentation%20of%20EL%0Aimages%29showing%20improvement%20in%20performance%20while%20reducing%20the%20annotation%20costs%0Aby%2080%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13693v2&entry.124074799=Read"},
{"title": "LaSe-E2V: Towards Language-guided Semantic-Aware Event-to-Video\n  Reconstruction", "author": "Kanghao Chen and Hangyu Li and JiaZhou Zhou and Zeyu Wang and Lin Wang", "abstract": "  Event cameras harness advantages such as low latency, high temporal\nresolution, and high dynamic range (HDR), compared to standard cameras. Due to\nthe distinct imaging paradigm shift, a dominant line of research focuses on\nevent-to-video (E2V) reconstruction to bridge event-based and standard computer\nvision. However, this task remains challenging due to its inherently ill-posed\nnature: event cameras only detect the edge and motion information locally.\nConsequently, the reconstructed videos are often plagued by artifacts and\nregional blur, primarily caused by the ambiguous semantics of event data. In\nthis paper, we find language naturally conveys abundant semantic information,\nrendering it stunningly superior in ensuring semantic consistency for E2V\nreconstruction. Accordingly, we propose a novel framework, called LaSe-E2V,\nthat can achieve semantic-aware high-quality E2V reconstruction from a\nlanguage-guided perspective, buttressed by the text-conditional diffusion\nmodels. However, due to diffusion models' inherent diversity and randomness, it\nis hardly possible to directly apply them to achieve spatial and temporal\nconsistency for E2V reconstruction. Thus, we first propose an Event-guided\nSpatiotemporal Attention (ESA) module to condition the event data to the\ndenoising pipeline effectively. We then introduce an event-aware mask loss to\nensure temporal coherence and a noise initialization strategy to enhance\nspatial consistency. Given the absence of event-text-video paired data, we\naggregate existing E2V datasets and generate textual descriptions using the\ntagging models for training and evaluation. Extensive experiments on three\ndatasets covering diverse challenging scenarios (e.g., fast motion, low light)\ndemonstrate the superiority of our method.\n", "link": "http://arxiv.org/abs/2407.05547v3", "date": "2024-07-17", "relevancy": 2.1697, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.544}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5416}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaSe-E2V%3A%20Towards%20Language-guided%20Semantic-Aware%20Event-to-Video%0A%20%20Reconstruction&body=Title%3A%20LaSe-E2V%3A%20Towards%20Language-guided%20Semantic-Aware%20Event-to-Video%0A%20%20Reconstruction%0AAuthor%3A%20Kanghao%20Chen%20and%20Hangyu%20Li%20and%20JiaZhou%20Zhou%20and%20Zeyu%20Wang%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Event%20cameras%20harness%20advantages%20such%20as%20low%20latency%2C%20high%20temporal%0Aresolution%2C%20and%20high%20dynamic%20range%20%28HDR%29%2C%20compared%20to%20standard%20cameras.%20Due%20to%0Athe%20distinct%20imaging%20paradigm%20shift%2C%20a%20dominant%20line%20of%20research%20focuses%20on%0Aevent-to-video%20%28E2V%29%20reconstruction%20to%20bridge%20event-based%20and%20standard%20computer%0Avision.%20However%2C%20this%20task%20remains%20challenging%20due%20to%20its%20inherently%20ill-posed%0Anature%3A%20event%20cameras%20only%20detect%20the%20edge%20and%20motion%20information%20locally.%0AConsequently%2C%20the%20reconstructed%20videos%20are%20often%20plagued%20by%20artifacts%20and%0Aregional%20blur%2C%20primarily%20caused%20by%20the%20ambiguous%20semantics%20of%20event%20data.%20In%0Athis%20paper%2C%20we%20find%20language%20naturally%20conveys%20abundant%20semantic%20information%2C%0Arendering%20it%20stunningly%20superior%20in%20ensuring%20semantic%20consistency%20for%20E2V%0Areconstruction.%20Accordingly%2C%20we%20propose%20a%20novel%20framework%2C%20called%20LaSe-E2V%2C%0Athat%20can%20achieve%20semantic-aware%20high-quality%20E2V%20reconstruction%20from%20a%0Alanguage-guided%20perspective%2C%20buttressed%20by%20the%20text-conditional%20diffusion%0Amodels.%20However%2C%20due%20to%20diffusion%20models%27%20inherent%20diversity%20and%20randomness%2C%20it%0Ais%20hardly%20possible%20to%20directly%20apply%20them%20to%20achieve%20spatial%20and%20temporal%0Aconsistency%20for%20E2V%20reconstruction.%20Thus%2C%20we%20first%20propose%20an%20Event-guided%0ASpatiotemporal%20Attention%20%28ESA%29%20module%20to%20condition%20the%20event%20data%20to%20the%0Adenoising%20pipeline%20effectively.%20We%20then%20introduce%20an%20event-aware%20mask%20loss%20to%0Aensure%20temporal%20coherence%20and%20a%20noise%20initialization%20strategy%20to%20enhance%0Aspatial%20consistency.%20Given%20the%20absence%20of%20event-text-video%20paired%20data%2C%20we%0Aaggregate%20existing%20E2V%20datasets%20and%20generate%20textual%20descriptions%20using%20the%0Atagging%20models%20for%20training%20and%20evaluation.%20Extensive%20experiments%20on%20three%0Adatasets%20covering%20diverse%20challenging%20scenarios%20%28e.g.%2C%20fast%20motion%2C%20low%20light%29%0Ademonstrate%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05547v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaSe-E2V%253A%2520Towards%2520Language-guided%2520Semantic-Aware%2520Event-to-Video%250A%2520%2520Reconstruction%26entry.906535625%3DKanghao%2520Chen%2520and%2520Hangyu%2520Li%2520and%2520JiaZhou%2520Zhou%2520and%2520Zeyu%2520Wang%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520Event%2520cameras%2520harness%2520advantages%2520such%2520as%2520low%2520latency%252C%2520high%2520temporal%250Aresolution%252C%2520and%2520high%2520dynamic%2520range%2520%2528HDR%2529%252C%2520compared%2520to%2520standard%2520cameras.%2520Due%2520to%250Athe%2520distinct%2520imaging%2520paradigm%2520shift%252C%2520a%2520dominant%2520line%2520of%2520research%2520focuses%2520on%250Aevent-to-video%2520%2528E2V%2529%2520reconstruction%2520to%2520bridge%2520event-based%2520and%2520standard%2520computer%250Avision.%2520However%252C%2520this%2520task%2520remains%2520challenging%2520due%2520to%2520its%2520inherently%2520ill-posed%250Anature%253A%2520event%2520cameras%2520only%2520detect%2520the%2520edge%2520and%2520motion%2520information%2520locally.%250AConsequently%252C%2520the%2520reconstructed%2520videos%2520are%2520often%2520plagued%2520by%2520artifacts%2520and%250Aregional%2520blur%252C%2520primarily%2520caused%2520by%2520the%2520ambiguous%2520semantics%2520of%2520event%2520data.%2520In%250Athis%2520paper%252C%2520we%2520find%2520language%2520naturally%2520conveys%2520abundant%2520semantic%2520information%252C%250Arendering%2520it%2520stunningly%2520superior%2520in%2520ensuring%2520semantic%2520consistency%2520for%2520E2V%250Areconstruction.%2520Accordingly%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520called%2520LaSe-E2V%252C%250Athat%2520can%2520achieve%2520semantic-aware%2520high-quality%2520E2V%2520reconstruction%2520from%2520a%250Alanguage-guided%2520perspective%252C%2520buttressed%2520by%2520the%2520text-conditional%2520diffusion%250Amodels.%2520However%252C%2520due%2520to%2520diffusion%2520models%2527%2520inherent%2520diversity%2520and%2520randomness%252C%2520it%250Ais%2520hardly%2520possible%2520to%2520directly%2520apply%2520them%2520to%2520achieve%2520spatial%2520and%2520temporal%250Aconsistency%2520for%2520E2V%2520reconstruction.%2520Thus%252C%2520we%2520first%2520propose%2520an%2520Event-guided%250ASpatiotemporal%2520Attention%2520%2528ESA%2529%2520module%2520to%2520condition%2520the%2520event%2520data%2520to%2520the%250Adenoising%2520pipeline%2520effectively.%2520We%2520then%2520introduce%2520an%2520event-aware%2520mask%2520loss%2520to%250Aensure%2520temporal%2520coherence%2520and%2520a%2520noise%2520initialization%2520strategy%2520to%2520enhance%250Aspatial%2520consistency.%2520Given%2520the%2520absence%2520of%2520event-text-video%2520paired%2520data%252C%2520we%250Aaggregate%2520existing%2520E2V%2520datasets%2520and%2520generate%2520textual%2520descriptions%2520using%2520the%250Atagging%2520models%2520for%2520training%2520and%2520evaluation.%2520Extensive%2520experiments%2520on%2520three%250Adatasets%2520covering%2520diverse%2520challenging%2520scenarios%2520%2528e.g.%252C%2520fast%2520motion%252C%2520low%2520light%2529%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05547v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaSe-E2V%3A%20Towards%20Language-guided%20Semantic-Aware%20Event-to-Video%0A%20%20Reconstruction&entry.906535625=Kanghao%20Chen%20and%20Hangyu%20Li%20and%20JiaZhou%20Zhou%20and%20Zeyu%20Wang%20and%20Lin%20Wang&entry.1292438233=%20%20Event%20cameras%20harness%20advantages%20such%20as%20low%20latency%2C%20high%20temporal%0Aresolution%2C%20and%20high%20dynamic%20range%20%28HDR%29%2C%20compared%20to%20standard%20cameras.%20Due%20to%0Athe%20distinct%20imaging%20paradigm%20shift%2C%20a%20dominant%20line%20of%20research%20focuses%20on%0Aevent-to-video%20%28E2V%29%20reconstruction%20to%20bridge%20event-based%20and%20standard%20computer%0Avision.%20However%2C%20this%20task%20remains%20challenging%20due%20to%20its%20inherently%20ill-posed%0Anature%3A%20event%20cameras%20only%20detect%20the%20edge%20and%20motion%20information%20locally.%0AConsequently%2C%20the%20reconstructed%20videos%20are%20often%20plagued%20by%20artifacts%20and%0Aregional%20blur%2C%20primarily%20caused%20by%20the%20ambiguous%20semantics%20of%20event%20data.%20In%0Athis%20paper%2C%20we%20find%20language%20naturally%20conveys%20abundant%20semantic%20information%2C%0Arendering%20it%20stunningly%20superior%20in%20ensuring%20semantic%20consistency%20for%20E2V%0Areconstruction.%20Accordingly%2C%20we%20propose%20a%20novel%20framework%2C%20called%20LaSe-E2V%2C%0Athat%20can%20achieve%20semantic-aware%20high-quality%20E2V%20reconstruction%20from%20a%0Alanguage-guided%20perspective%2C%20buttressed%20by%20the%20text-conditional%20diffusion%0Amodels.%20However%2C%20due%20to%20diffusion%20models%27%20inherent%20diversity%20and%20randomness%2C%20it%0Ais%20hardly%20possible%20to%20directly%20apply%20them%20to%20achieve%20spatial%20and%20temporal%0Aconsistency%20for%20E2V%20reconstruction.%20Thus%2C%20we%20first%20propose%20an%20Event-guided%0ASpatiotemporal%20Attention%20%28ESA%29%20module%20to%20condition%20the%20event%20data%20to%20the%0Adenoising%20pipeline%20effectively.%20We%20then%20introduce%20an%20event-aware%20mask%20loss%20to%0Aensure%20temporal%20coherence%20and%20a%20noise%20initialization%20strategy%20to%20enhance%0Aspatial%20consistency.%20Given%20the%20absence%20of%20event-text-video%20paired%20data%2C%20we%0Aaggregate%20existing%20E2V%20datasets%20and%20generate%20textual%20descriptions%20using%20the%0Atagging%20models%20for%20training%20and%20evaluation.%20Extensive%20experiments%20on%20three%0Adatasets%20covering%20diverse%20challenging%20scenarios%20%28e.g.%2C%20fast%20motion%2C%20low%20light%29%0Ademonstrate%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05547v3&entry.124074799=Read"},
{"title": "Attribute Based Interpretable Evaluation Metrics for Generative Models", "author": "Dongkyun Kim and Mingi Kwon and Youngjung Uh", "abstract": "  When the training dataset comprises a 1:1 proportion of dogs to cats, a\ngenerative model that produces 1:1 dogs and cats better resembles the training\nspecies distribution than another model with 3:1 dogs and cats. Can we capture\nthis phenomenon using existing metrics? Unfortunately, we cannot, because these\nmetrics do not provide any interpretability beyond \"diversity\". In this\ncontext, we propose a new evaluation protocol that measures the divergence of a\nset of generated images from the training set regarding the distribution of\nattribute strengths as follows. Single-attribute Divergence (SaD) measures the\ndivergence regarding PDFs of a single attribute. Paired-attribute Divergence\n(PaD) measures the divergence regarding joint PDFs of a pair of attributes.\nThey provide which attributes the models struggle. For measuring the attribute\nstrengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures\nthe cosine similarity between image and text vectors with heterogeneous initial\npoints. With SaD and PaD, we reveal the following about existing generative\nmodels. ProjectedGAN generates implausible attribute relationships such as a\nbaby with a beard even though it has competitive scores of existing metrics.\nDiffusion models struggle to capture diverse colors in the datasets. The larger\nsampling timesteps of latent diffusion model generate the more minor objects\nincluding earrings and necklaces. Stable Diffusion v1.5 better captures the\nattributes than v2.1. Our metrics lay a foundation for explainable evaluations\nof generative models.\n", "link": "http://arxiv.org/abs/2310.17261v3", "date": "2024-07-17", "relevancy": 2.1592, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5456}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5404}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attribute%20Based%20Interpretable%20Evaluation%20Metrics%20for%20Generative%20Models&body=Title%3A%20Attribute%20Based%20Interpretable%20Evaluation%20Metrics%20for%20Generative%20Models%0AAuthor%3A%20Dongkyun%20Kim%20and%20Mingi%20Kwon%20and%20Youngjung%20Uh%0AAbstract%3A%20%20%20When%20the%20training%20dataset%20comprises%20a%201%3A1%20proportion%20of%20dogs%20to%20cats%2C%20a%0Agenerative%20model%20that%20produces%201%3A1%20dogs%20and%20cats%20better%20resembles%20the%20training%0Aspecies%20distribution%20than%20another%20model%20with%203%3A1%20dogs%20and%20cats.%20Can%20we%20capture%0Athis%20phenomenon%20using%20existing%20metrics%3F%20Unfortunately%2C%20we%20cannot%2C%20because%20these%0Ametrics%20do%20not%20provide%20any%20interpretability%20beyond%20%22diversity%22.%20In%20this%0Acontext%2C%20we%20propose%20a%20new%20evaluation%20protocol%20that%20measures%20the%20divergence%20of%20a%0Aset%20of%20generated%20images%20from%20the%20training%20set%20regarding%20the%20distribution%20of%0Aattribute%20strengths%20as%20follows.%20Single-attribute%20Divergence%20%28SaD%29%20measures%20the%0Adivergence%20regarding%20PDFs%20of%20a%20single%20attribute.%20Paired-attribute%20Divergence%0A%28PaD%29%20measures%20the%20divergence%20regarding%20joint%20PDFs%20of%20a%20pair%20of%20attributes.%0AThey%20provide%20which%20attributes%20the%20models%20struggle.%20For%20measuring%20the%20attribute%0Astrengths%20of%20an%20image%2C%20we%20propose%20Heterogeneous%20CLIPScore%20%28HCS%29%20which%20measures%0Athe%20cosine%20similarity%20between%20image%20and%20text%20vectors%20with%20heterogeneous%20initial%0Apoints.%20With%20SaD%20and%20PaD%2C%20we%20reveal%20the%20following%20about%20existing%20generative%0Amodels.%20ProjectedGAN%20generates%20implausible%20attribute%20relationships%20such%20as%20a%0Ababy%20with%20a%20beard%20even%20though%20it%20has%20competitive%20scores%20of%20existing%20metrics.%0ADiffusion%20models%20struggle%20to%20capture%20diverse%20colors%20in%20the%20datasets.%20The%20larger%0Asampling%20timesteps%20of%20latent%20diffusion%20model%20generate%20the%20more%20minor%20objects%0Aincluding%20earrings%20and%20necklaces.%20Stable%20Diffusion%20v1.5%20better%20captures%20the%0Aattributes%20than%20v2.1.%20Our%20metrics%20lay%20a%20foundation%20for%20explainable%20evaluations%0Aof%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17261v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttribute%2520Based%2520Interpretable%2520Evaluation%2520Metrics%2520for%2520Generative%2520Models%26entry.906535625%3DDongkyun%2520Kim%2520and%2520Mingi%2520Kwon%2520and%2520Youngjung%2520Uh%26entry.1292438233%3D%2520%2520When%2520the%2520training%2520dataset%2520comprises%2520a%25201%253A1%2520proportion%2520of%2520dogs%2520to%2520cats%252C%2520a%250Agenerative%2520model%2520that%2520produces%25201%253A1%2520dogs%2520and%2520cats%2520better%2520resembles%2520the%2520training%250Aspecies%2520distribution%2520than%2520another%2520model%2520with%25203%253A1%2520dogs%2520and%2520cats.%2520Can%2520we%2520capture%250Athis%2520phenomenon%2520using%2520existing%2520metrics%253F%2520Unfortunately%252C%2520we%2520cannot%252C%2520because%2520these%250Ametrics%2520do%2520not%2520provide%2520any%2520interpretability%2520beyond%2520%2522diversity%2522.%2520In%2520this%250Acontext%252C%2520we%2520propose%2520a%2520new%2520evaluation%2520protocol%2520that%2520measures%2520the%2520divergence%2520of%2520a%250Aset%2520of%2520generated%2520images%2520from%2520the%2520training%2520set%2520regarding%2520the%2520distribution%2520of%250Aattribute%2520strengths%2520as%2520follows.%2520Single-attribute%2520Divergence%2520%2528SaD%2529%2520measures%2520the%250Adivergence%2520regarding%2520PDFs%2520of%2520a%2520single%2520attribute.%2520Paired-attribute%2520Divergence%250A%2528PaD%2529%2520measures%2520the%2520divergence%2520regarding%2520joint%2520PDFs%2520of%2520a%2520pair%2520of%2520attributes.%250AThey%2520provide%2520which%2520attributes%2520the%2520models%2520struggle.%2520For%2520measuring%2520the%2520attribute%250Astrengths%2520of%2520an%2520image%252C%2520we%2520propose%2520Heterogeneous%2520CLIPScore%2520%2528HCS%2529%2520which%2520measures%250Athe%2520cosine%2520similarity%2520between%2520image%2520and%2520text%2520vectors%2520with%2520heterogeneous%2520initial%250Apoints.%2520With%2520SaD%2520and%2520PaD%252C%2520we%2520reveal%2520the%2520following%2520about%2520existing%2520generative%250Amodels.%2520ProjectedGAN%2520generates%2520implausible%2520attribute%2520relationships%2520such%2520as%2520a%250Ababy%2520with%2520a%2520beard%2520even%2520though%2520it%2520has%2520competitive%2520scores%2520of%2520existing%2520metrics.%250ADiffusion%2520models%2520struggle%2520to%2520capture%2520diverse%2520colors%2520in%2520the%2520datasets.%2520The%2520larger%250Asampling%2520timesteps%2520of%2520latent%2520diffusion%2520model%2520generate%2520the%2520more%2520minor%2520objects%250Aincluding%2520earrings%2520and%2520necklaces.%2520Stable%2520Diffusion%2520v1.5%2520better%2520captures%2520the%250Aattributes%2520than%2520v2.1.%2520Our%2520metrics%2520lay%2520a%2520foundation%2520for%2520explainable%2520evaluations%250Aof%2520generative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17261v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attribute%20Based%20Interpretable%20Evaluation%20Metrics%20for%20Generative%20Models&entry.906535625=Dongkyun%20Kim%20and%20Mingi%20Kwon%20and%20Youngjung%20Uh&entry.1292438233=%20%20When%20the%20training%20dataset%20comprises%20a%201%3A1%20proportion%20of%20dogs%20to%20cats%2C%20a%0Agenerative%20model%20that%20produces%201%3A1%20dogs%20and%20cats%20better%20resembles%20the%20training%0Aspecies%20distribution%20than%20another%20model%20with%203%3A1%20dogs%20and%20cats.%20Can%20we%20capture%0Athis%20phenomenon%20using%20existing%20metrics%3F%20Unfortunately%2C%20we%20cannot%2C%20because%20these%0Ametrics%20do%20not%20provide%20any%20interpretability%20beyond%20%22diversity%22.%20In%20this%0Acontext%2C%20we%20propose%20a%20new%20evaluation%20protocol%20that%20measures%20the%20divergence%20of%20a%0Aset%20of%20generated%20images%20from%20the%20training%20set%20regarding%20the%20distribution%20of%0Aattribute%20strengths%20as%20follows.%20Single-attribute%20Divergence%20%28SaD%29%20measures%20the%0Adivergence%20regarding%20PDFs%20of%20a%20single%20attribute.%20Paired-attribute%20Divergence%0A%28PaD%29%20measures%20the%20divergence%20regarding%20joint%20PDFs%20of%20a%20pair%20of%20attributes.%0AThey%20provide%20which%20attributes%20the%20models%20struggle.%20For%20measuring%20the%20attribute%0Astrengths%20of%20an%20image%2C%20we%20propose%20Heterogeneous%20CLIPScore%20%28HCS%29%20which%20measures%0Athe%20cosine%20similarity%20between%20image%20and%20text%20vectors%20with%20heterogeneous%20initial%0Apoints.%20With%20SaD%20and%20PaD%2C%20we%20reveal%20the%20following%20about%20existing%20generative%0Amodels.%20ProjectedGAN%20generates%20implausible%20attribute%20relationships%20such%20as%20a%0Ababy%20with%20a%20beard%20even%20though%20it%20has%20competitive%20scores%20of%20existing%20metrics.%0ADiffusion%20models%20struggle%20to%20capture%20diverse%20colors%20in%20the%20datasets.%20The%20larger%0Asampling%20timesteps%20of%20latent%20diffusion%20model%20generate%20the%20more%20minor%20objects%0Aincluding%20earrings%20and%20necklaces.%20Stable%20Diffusion%20v1.5%20better%20captures%20the%0Aattributes%20than%20v2.1.%20Our%20metrics%20lay%20a%20foundation%20for%20explainable%20evaluations%0Aof%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17261v3&entry.124074799=Read"},
{"title": "From Fake to Real: Pretraining on Balanced Synthetic Images to Prevent\n  Spurious Correlations in Image Recognition", "author": "Maan Qraitem and Kate Saenko and Bryan A. Plummer", "abstract": "  Visual recognition models are prone to learning spurious correlations induced\nby a biased training set where certain conditions $B$ (\\eg, Indoors) are\nover-represented in certain classes $Y$ (\\eg, Big Dogs). Synthetic data from\noff-the-shelf large-scale generative models offers a promising direction to\nmitigate this issue by augmenting underrepresented subgroups in the real\ndataset. However, by using a mixed distribution of real and synthetic data, we\nintroduce another source of bias due to distributional differences between\nsynthetic and real data (\\eg synthetic artifacts). As we will show, prior\nwork's approach for using synthetic data to resolve the model's bias toward $B$\ndo not correct the model's bias toward the pair $(B, G)$, where $G$ denotes\nwhether the sample is real or synthetic. Thus, the model could simply learn\nsignals based on the pair $(B, G)$ (\\eg, Synthetic Indoors) to make predictions\nabout $Y$ (\\eg, Big Dogs). To address this issue, we propose a simple,\neasy-to-implement, two-step training pipeline that we call From Fake to Real\n(FFR). The first step of FFR pre-trains a model on balanced synthetic data to\nlearn robust representations across subgroups. In the second step, FFR\nfine-tunes the model on real data using ERM or common loss-based bias\nmitigation methods. By training on real and synthetic data separately, FFR does\nnot expose the model to the statistical differences between real and synthetic\ndata and thus avoids the issue of bias toward the pair $(B, G)$. Our\nexperiments show that FFR improves worst group accuracy over the\nstate-of-the-art by up to 20\\% over three datasets. Code available:\n\\url{https://github.com/mqraitem/From-Fake-to-Real}\n", "link": "http://arxiv.org/abs/2308.04553v3", "date": "2024-07-17", "relevancy": 2.1569, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5497}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5476}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Fake%20to%20Real%3A%20Pretraining%20on%20Balanced%20Synthetic%20Images%20to%20Prevent%0A%20%20Spurious%20Correlations%20in%20Image%20Recognition&body=Title%3A%20From%20Fake%20to%20Real%3A%20Pretraining%20on%20Balanced%20Synthetic%20Images%20to%20Prevent%0A%20%20Spurious%20Correlations%20in%20Image%20Recognition%0AAuthor%3A%20Maan%20Qraitem%20and%20Kate%20Saenko%20and%20Bryan%20A.%20Plummer%0AAbstract%3A%20%20%20Visual%20recognition%20models%20are%20prone%20to%20learning%20spurious%20correlations%20induced%0Aby%20a%20biased%20training%20set%20where%20certain%20conditions%20%24B%24%20%28%5Ceg%2C%20Indoors%29%20are%0Aover-represented%20in%20certain%20classes%20%24Y%24%20%28%5Ceg%2C%20Big%20Dogs%29.%20Synthetic%20data%20from%0Aoff-the-shelf%20large-scale%20generative%20models%20offers%20a%20promising%20direction%20to%0Amitigate%20this%20issue%20by%20augmenting%20underrepresented%20subgroups%20in%20the%20real%0Adataset.%20However%2C%20by%20using%20a%20mixed%20distribution%20of%20real%20and%20synthetic%20data%2C%20we%0Aintroduce%20another%20source%20of%20bias%20due%20to%20distributional%20differences%20between%0Asynthetic%20and%20real%20data%20%28%5Ceg%20synthetic%20artifacts%29.%20As%20we%20will%20show%2C%20prior%0Awork%27s%20approach%20for%20using%20synthetic%20data%20to%20resolve%20the%20model%27s%20bias%20toward%20%24B%24%0Ado%20not%20correct%20the%20model%27s%20bias%20toward%20the%20pair%20%24%28B%2C%20G%29%24%2C%20where%20%24G%24%20denotes%0Awhether%20the%20sample%20is%20real%20or%20synthetic.%20Thus%2C%20the%20model%20could%20simply%20learn%0Asignals%20based%20on%20the%20pair%20%24%28B%2C%20G%29%24%20%28%5Ceg%2C%20Synthetic%20Indoors%29%20to%20make%20predictions%0Aabout%20%24Y%24%20%28%5Ceg%2C%20Big%20Dogs%29.%20To%20address%20this%20issue%2C%20we%20propose%20a%20simple%2C%0Aeasy-to-implement%2C%20two-step%20training%20pipeline%20that%20we%20call%20From%20Fake%20to%20Real%0A%28FFR%29.%20The%20first%20step%20of%20FFR%20pre-trains%20a%20model%20on%20balanced%20synthetic%20data%20to%0Alearn%20robust%20representations%20across%20subgroups.%20In%20the%20second%20step%2C%20FFR%0Afine-tunes%20the%20model%20on%20real%20data%20using%20ERM%20or%20common%20loss-based%20bias%0Amitigation%20methods.%20By%20training%20on%20real%20and%20synthetic%20data%20separately%2C%20FFR%20does%0Anot%20expose%20the%20model%20to%20the%20statistical%20differences%20between%20real%20and%20synthetic%0Adata%20and%20thus%20avoids%20the%20issue%20of%20bias%20toward%20the%20pair%20%24%28B%2C%20G%29%24.%20Our%0Aexperiments%20show%20that%20FFR%20improves%20worst%20group%20accuracy%20over%20the%0Astate-of-the-art%20by%20up%20to%2020%5C%25%20over%20three%20datasets.%20Code%20available%3A%0A%5Curl%7Bhttps%3A//github.com/mqraitem/From-Fake-to-Real%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04553v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Fake%2520to%2520Real%253A%2520Pretraining%2520on%2520Balanced%2520Synthetic%2520Images%2520to%2520Prevent%250A%2520%2520Spurious%2520Correlations%2520in%2520Image%2520Recognition%26entry.906535625%3DMaan%2520Qraitem%2520and%2520Kate%2520Saenko%2520and%2520Bryan%2520A.%2520Plummer%26entry.1292438233%3D%2520%2520Visual%2520recognition%2520models%2520are%2520prone%2520to%2520learning%2520spurious%2520correlations%2520induced%250Aby%2520a%2520biased%2520training%2520set%2520where%2520certain%2520conditions%2520%2524B%2524%2520%2528%255Ceg%252C%2520Indoors%2529%2520are%250Aover-represented%2520in%2520certain%2520classes%2520%2524Y%2524%2520%2528%255Ceg%252C%2520Big%2520Dogs%2529.%2520Synthetic%2520data%2520from%250Aoff-the-shelf%2520large-scale%2520generative%2520models%2520offers%2520a%2520promising%2520direction%2520to%250Amitigate%2520this%2520issue%2520by%2520augmenting%2520underrepresented%2520subgroups%2520in%2520the%2520real%250Adataset.%2520However%252C%2520by%2520using%2520a%2520mixed%2520distribution%2520of%2520real%2520and%2520synthetic%2520data%252C%2520we%250Aintroduce%2520another%2520source%2520of%2520bias%2520due%2520to%2520distributional%2520differences%2520between%250Asynthetic%2520and%2520real%2520data%2520%2528%255Ceg%2520synthetic%2520artifacts%2529.%2520As%2520we%2520will%2520show%252C%2520prior%250Awork%2527s%2520approach%2520for%2520using%2520synthetic%2520data%2520to%2520resolve%2520the%2520model%2527s%2520bias%2520toward%2520%2524B%2524%250Ado%2520not%2520correct%2520the%2520model%2527s%2520bias%2520toward%2520the%2520pair%2520%2524%2528B%252C%2520G%2529%2524%252C%2520where%2520%2524G%2524%2520denotes%250Awhether%2520the%2520sample%2520is%2520real%2520or%2520synthetic.%2520Thus%252C%2520the%2520model%2520could%2520simply%2520learn%250Asignals%2520based%2520on%2520the%2520pair%2520%2524%2528B%252C%2520G%2529%2524%2520%2528%255Ceg%252C%2520Synthetic%2520Indoors%2529%2520to%2520make%2520predictions%250Aabout%2520%2524Y%2524%2520%2528%255Ceg%252C%2520Big%2520Dogs%2529.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520simple%252C%250Aeasy-to-implement%252C%2520two-step%2520training%2520pipeline%2520that%2520we%2520call%2520From%2520Fake%2520to%2520Real%250A%2528FFR%2529.%2520The%2520first%2520step%2520of%2520FFR%2520pre-trains%2520a%2520model%2520on%2520balanced%2520synthetic%2520data%2520to%250Alearn%2520robust%2520representations%2520across%2520subgroups.%2520In%2520the%2520second%2520step%252C%2520FFR%250Afine-tunes%2520the%2520model%2520on%2520real%2520data%2520using%2520ERM%2520or%2520common%2520loss-based%2520bias%250Amitigation%2520methods.%2520By%2520training%2520on%2520real%2520and%2520synthetic%2520data%2520separately%252C%2520FFR%2520does%250Anot%2520expose%2520the%2520model%2520to%2520the%2520statistical%2520differences%2520between%2520real%2520and%2520synthetic%250Adata%2520and%2520thus%2520avoids%2520the%2520issue%2520of%2520bias%2520toward%2520the%2520pair%2520%2524%2528B%252C%2520G%2529%2524.%2520Our%250Aexperiments%2520show%2520that%2520FFR%2520improves%2520worst%2520group%2520accuracy%2520over%2520the%250Astate-of-the-art%2520by%2520up%2520to%252020%255C%2525%2520over%2520three%2520datasets.%2520Code%2520available%253A%250A%255Curl%257Bhttps%253A//github.com/mqraitem/From-Fake-to-Real%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04553v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Fake%20to%20Real%3A%20Pretraining%20on%20Balanced%20Synthetic%20Images%20to%20Prevent%0A%20%20Spurious%20Correlations%20in%20Image%20Recognition&entry.906535625=Maan%20Qraitem%20and%20Kate%20Saenko%20and%20Bryan%20A.%20Plummer&entry.1292438233=%20%20Visual%20recognition%20models%20are%20prone%20to%20learning%20spurious%20correlations%20induced%0Aby%20a%20biased%20training%20set%20where%20certain%20conditions%20%24B%24%20%28%5Ceg%2C%20Indoors%29%20are%0Aover-represented%20in%20certain%20classes%20%24Y%24%20%28%5Ceg%2C%20Big%20Dogs%29.%20Synthetic%20data%20from%0Aoff-the-shelf%20large-scale%20generative%20models%20offers%20a%20promising%20direction%20to%0Amitigate%20this%20issue%20by%20augmenting%20underrepresented%20subgroups%20in%20the%20real%0Adataset.%20However%2C%20by%20using%20a%20mixed%20distribution%20of%20real%20and%20synthetic%20data%2C%20we%0Aintroduce%20another%20source%20of%20bias%20due%20to%20distributional%20differences%20between%0Asynthetic%20and%20real%20data%20%28%5Ceg%20synthetic%20artifacts%29.%20As%20we%20will%20show%2C%20prior%0Awork%27s%20approach%20for%20using%20synthetic%20data%20to%20resolve%20the%20model%27s%20bias%20toward%20%24B%24%0Ado%20not%20correct%20the%20model%27s%20bias%20toward%20the%20pair%20%24%28B%2C%20G%29%24%2C%20where%20%24G%24%20denotes%0Awhether%20the%20sample%20is%20real%20or%20synthetic.%20Thus%2C%20the%20model%20could%20simply%20learn%0Asignals%20based%20on%20the%20pair%20%24%28B%2C%20G%29%24%20%28%5Ceg%2C%20Synthetic%20Indoors%29%20to%20make%20predictions%0Aabout%20%24Y%24%20%28%5Ceg%2C%20Big%20Dogs%29.%20To%20address%20this%20issue%2C%20we%20propose%20a%20simple%2C%0Aeasy-to-implement%2C%20two-step%20training%20pipeline%20that%20we%20call%20From%20Fake%20to%20Real%0A%28FFR%29.%20The%20first%20step%20of%20FFR%20pre-trains%20a%20model%20on%20balanced%20synthetic%20data%20to%0Alearn%20robust%20representations%20across%20subgroups.%20In%20the%20second%20step%2C%20FFR%0Afine-tunes%20the%20model%20on%20real%20data%20using%20ERM%20or%20common%20loss-based%20bias%0Amitigation%20methods.%20By%20training%20on%20real%20and%20synthetic%20data%20separately%2C%20FFR%20does%0Anot%20expose%20the%20model%20to%20the%20statistical%20differences%20between%20real%20and%20synthetic%0Adata%20and%20thus%20avoids%20the%20issue%20of%20bias%20toward%20the%20pair%20%24%28B%2C%20G%29%24.%20Our%0Aexperiments%20show%20that%20FFR%20improves%20worst%20group%20accuracy%20over%20the%0Astate-of-the-art%20by%20up%20to%2020%5C%25%20over%20three%20datasets.%20Code%20available%3A%0A%5Curl%7Bhttps%3A//github.com/mqraitem/From-Fake-to-Real%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04553v3&entry.124074799=Read"},
{"title": "Occam Gradient Descent", "author": "B. N. Kausik", "abstract": "  Deep learning neural network models must be large enough to adapt to their\nproblem domain, while small enough to avoid overfitting training data during\ngradient descent. To balance these competing demands, overprovisioned deep\nlearning models such as transformers are trained for a single epoch on large\ndata sets, and hence inefficient with both computing resources and training\ndata. In response to these inefficiencies, we exploit learning theory to derive\nOccam Gradient Descent, an algorithm that interleaves adaptive reduction of\nmodel size to minimize generalization error, with gradient descent on model\nweights to minimize fitting error. In contrast, traditional gradient descent\ngreedily minimizes fitting error without regard to generalization error. Our\nalgorithm simultaneously descends the space of weights and topological size of\nany neural network without modification, and is effective in our image\nclassification experiments in outperforming traditional gradient descent with\nor without post-train pruning in loss, compute and model size. Furthermore,\napplying our algorithm to tabular data classification we find that across a\nrange of data sets, neural networks trained with Occam Gradient Descent\noutperform neural networks trained with gradient descent, as well as Random\nForests, in both loss and model size.\n", "link": "http://arxiv.org/abs/2405.20194v3", "date": "2024-07-17", "relevancy": 2.1526, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5467}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5449}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occam%20Gradient%20Descent&body=Title%3A%20Occam%20Gradient%20Descent%0AAuthor%3A%20B.%20N.%20Kausik%0AAbstract%3A%20%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification%2C%20and%20is%20effective%20in%20our%20image%0Aclassification%20experiments%20in%20outperforming%20traditional%20gradient%20descent%20with%0Aor%20without%20post-train%20pruning%20in%20loss%2C%20compute%20and%20model%20size.%20Furthermore%2C%0Aapplying%20our%20algorithm%20to%20tabular%20data%20classification%20we%20find%20that%20across%20a%0Arange%20of%20data%20sets%2C%20neural%20networks%20trained%20with%20Occam%20Gradient%20Descent%0Aoutperform%20neural%20networks%20trained%20with%20gradient%20descent%2C%20as%20well%20as%20Random%0AForests%2C%20in%20both%20loss%20and%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20194v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccam%2520Gradient%2520Descent%26entry.906535625%3DB.%2520N.%2520Kausik%26entry.1292438233%3D%2520%2520Deep%2520learning%2520neural%2520network%2520models%2520must%2520be%2520large%2520enough%2520to%2520adapt%2520to%2520their%250Aproblem%2520domain%252C%2520while%2520small%2520enough%2520to%2520avoid%2520overfitting%2520training%2520data%2520during%250Agradient%2520descent.%2520To%2520balance%2520these%2520competing%2520demands%252C%2520overprovisioned%2520deep%250Alearning%2520models%2520such%2520as%2520transformers%2520are%2520trained%2520for%2520a%2520single%2520epoch%2520on%2520large%250Adata%2520sets%252C%2520and%2520hence%2520inefficient%2520with%2520both%2520computing%2520resources%2520and%2520training%250Adata.%2520In%2520response%2520to%2520these%2520inefficiencies%252C%2520we%2520exploit%2520learning%2520theory%2520to%2520derive%250AOccam%2520Gradient%2520Descent%252C%2520an%2520algorithm%2520that%2520interleaves%2520adaptive%2520reduction%2520of%250Amodel%2520size%2520to%2520minimize%2520generalization%2520error%252C%2520with%2520gradient%2520descent%2520on%2520model%250Aweights%2520to%2520minimize%2520fitting%2520error.%2520In%2520contrast%252C%2520traditional%2520gradient%2520descent%250Agreedily%2520minimizes%2520fitting%2520error%2520without%2520regard%2520to%2520generalization%2520error.%2520Our%250Aalgorithm%2520simultaneously%2520descends%2520the%2520space%2520of%2520weights%2520and%2520topological%2520size%2520of%250Aany%2520neural%2520network%2520without%2520modification%252C%2520and%2520is%2520effective%2520in%2520our%2520image%250Aclassification%2520experiments%2520in%2520outperforming%2520traditional%2520gradient%2520descent%2520with%250Aor%2520without%2520post-train%2520pruning%2520in%2520loss%252C%2520compute%2520and%2520model%2520size.%2520Furthermore%252C%250Aapplying%2520our%2520algorithm%2520to%2520tabular%2520data%2520classification%2520we%2520find%2520that%2520across%2520a%250Arange%2520of%2520data%2520sets%252C%2520neural%2520networks%2520trained%2520with%2520Occam%2520Gradient%2520Descent%250Aoutperform%2520neural%2520networks%2520trained%2520with%2520gradient%2520descent%252C%2520as%2520well%2520as%2520Random%250AForests%252C%2520in%2520both%2520loss%2520and%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20194v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occam%20Gradient%20Descent&entry.906535625=B.%20N.%20Kausik&entry.1292438233=%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification%2C%20and%20is%20effective%20in%20our%20image%0Aclassification%20experiments%20in%20outperforming%20traditional%20gradient%20descent%20with%0Aor%20without%20post-train%20pruning%20in%20loss%2C%20compute%20and%20model%20size.%20Furthermore%2C%0Aapplying%20our%20algorithm%20to%20tabular%20data%20classification%20we%20find%20that%20across%20a%0Arange%20of%20data%20sets%2C%20neural%20networks%20trained%20with%20Occam%20Gradient%20Descent%0Aoutperform%20neural%20networks%20trained%20with%20gradient%20descent%2C%20as%20well%20as%20Random%0AForests%2C%20in%20both%20loss%20and%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20194v3&entry.124074799=Read"},
{"title": "MoME: Mixture of Multimodal Experts for Generalist Multimodal Large\n  Language Models", "author": "Leyang Shen and Gongwei Chen and Rui Shao and Weili Guan and Liqiang Nie", "abstract": "  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities across various vision-language tasks. However, a generalist MLLM\ntypically underperforms compared with a specialist MLLM on most VL tasks, which\ncan be attributed to task interference. In this paper, we propose a mixture of\nmultimodal experts (MoME) to mitigate task interference and obtain a generalist\nMLLM. Our MoME is composed of two key components, a mixture of vision experts\n(MoVE) and a mixture of language experts (MoLE). MoVE can adaptively modulate\nthe features transformed from various vision encoders, and has a strong\ncompatibility in transformation architecture. MoLE incorporates sparsely gated\nexperts into LLMs to achieve painless improvements with roughly unchanged\ninference costs. In response to task interference, our MoME specializes in both\nvision and language modality to adapt to task discrepancies. Extensive\nexperiments show that MoME significantly improves the performance of generalist\nMLLMs across various VL tasks. The source code is released at\nhttps://github.com/JiuTian-VL/MoME\n", "link": "http://arxiv.org/abs/2407.12709v1", "date": "2024-07-17", "relevancy": 2.1461, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5369}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoME%3A%20Mixture%20of%20Multimodal%20Experts%20for%20Generalist%20Multimodal%20Large%0A%20%20Language%20Models&body=Title%3A%20MoME%3A%20Mixture%20of%20Multimodal%20Experts%20for%20Generalist%20Multimodal%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Leyang%20Shen%20and%20Gongwei%20Chen%20and%20Rui%20Shao%20and%20Weili%20Guan%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20across%20various%20vision-language%20tasks.%20However%2C%20a%20generalist%20MLLM%0Atypically%20underperforms%20compared%20with%20a%20specialist%20MLLM%20on%20most%20VL%20tasks%2C%20which%0Acan%20be%20attributed%20to%20task%20interference.%20In%20this%20paper%2C%20we%20propose%20a%20mixture%20of%0Amultimodal%20experts%20%28MoME%29%20to%20mitigate%20task%20interference%20and%20obtain%20a%20generalist%0AMLLM.%20Our%20MoME%20is%20composed%20of%20two%20key%20components%2C%20a%20mixture%20of%20vision%20experts%0A%28MoVE%29%20and%20a%20mixture%20of%20language%20experts%20%28MoLE%29.%20MoVE%20can%20adaptively%20modulate%0Athe%20features%20transformed%20from%20various%20vision%20encoders%2C%20and%20has%20a%20strong%0Acompatibility%20in%20transformation%20architecture.%20MoLE%20incorporates%20sparsely%20gated%0Aexperts%20into%20LLMs%20to%20achieve%20painless%20improvements%20with%20roughly%20unchanged%0Ainference%20costs.%20In%20response%20to%20task%20interference%2C%20our%20MoME%20specializes%20in%20both%0Avision%20and%20language%20modality%20to%20adapt%20to%20task%20discrepancies.%20Extensive%0Aexperiments%20show%20that%20MoME%20significantly%20improves%20the%20performance%20of%20generalist%0AMLLMs%20across%20various%20VL%20tasks.%20The%20source%20code%20is%20released%20at%0Ahttps%3A//github.com/JiuTian-VL/MoME%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoME%253A%2520Mixture%2520of%2520Multimodal%2520Experts%2520for%2520Generalist%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DLeyang%2520Shen%2520and%2520Gongwei%2520Chen%2520and%2520Rui%2520Shao%2520and%2520Weili%2520Guan%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520impressive%250Acapabilities%2520across%2520various%2520vision-language%2520tasks.%2520However%252C%2520a%2520generalist%2520MLLM%250Atypically%2520underperforms%2520compared%2520with%2520a%2520specialist%2520MLLM%2520on%2520most%2520VL%2520tasks%252C%2520which%250Acan%2520be%2520attributed%2520to%2520task%2520interference.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520mixture%2520of%250Amultimodal%2520experts%2520%2528MoME%2529%2520to%2520mitigate%2520task%2520interference%2520and%2520obtain%2520a%2520generalist%250AMLLM.%2520Our%2520MoME%2520is%2520composed%2520of%2520two%2520key%2520components%252C%2520a%2520mixture%2520of%2520vision%2520experts%250A%2528MoVE%2529%2520and%2520a%2520mixture%2520of%2520language%2520experts%2520%2528MoLE%2529.%2520MoVE%2520can%2520adaptively%2520modulate%250Athe%2520features%2520transformed%2520from%2520various%2520vision%2520encoders%252C%2520and%2520has%2520a%2520strong%250Acompatibility%2520in%2520transformation%2520architecture.%2520MoLE%2520incorporates%2520sparsely%2520gated%250Aexperts%2520into%2520LLMs%2520to%2520achieve%2520painless%2520improvements%2520with%2520roughly%2520unchanged%250Ainference%2520costs.%2520In%2520response%2520to%2520task%2520interference%252C%2520our%2520MoME%2520specializes%2520in%2520both%250Avision%2520and%2520language%2520modality%2520to%2520adapt%2520to%2520task%2520discrepancies.%2520Extensive%250Aexperiments%2520show%2520that%2520MoME%2520significantly%2520improves%2520the%2520performance%2520of%2520generalist%250AMLLMs%2520across%2520various%2520VL%2520tasks.%2520The%2520source%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/JiuTian-VL/MoME%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoME%3A%20Mixture%20of%20Multimodal%20Experts%20for%20Generalist%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Leyang%20Shen%20and%20Gongwei%20Chen%20and%20Rui%20Shao%20and%20Weili%20Guan%20and%20Liqiang%20Nie&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20across%20various%20vision-language%20tasks.%20However%2C%20a%20generalist%20MLLM%0Atypically%20underperforms%20compared%20with%20a%20specialist%20MLLM%20on%20most%20VL%20tasks%2C%20which%0Acan%20be%20attributed%20to%20task%20interference.%20In%20this%20paper%2C%20we%20propose%20a%20mixture%20of%0Amultimodal%20experts%20%28MoME%29%20to%20mitigate%20task%20interference%20and%20obtain%20a%20generalist%0AMLLM.%20Our%20MoME%20is%20composed%20of%20two%20key%20components%2C%20a%20mixture%20of%20vision%20experts%0A%28MoVE%29%20and%20a%20mixture%20of%20language%20experts%20%28MoLE%29.%20MoVE%20can%20adaptively%20modulate%0Athe%20features%20transformed%20from%20various%20vision%20encoders%2C%20and%20has%20a%20strong%0Acompatibility%20in%20transformation%20architecture.%20MoLE%20incorporates%20sparsely%20gated%0Aexperts%20into%20LLMs%20to%20achieve%20painless%20improvements%20with%20roughly%20unchanged%0Ainference%20costs.%20In%20response%20to%20task%20interference%2C%20our%20MoME%20specializes%20in%20both%0Avision%20and%20language%20modality%20to%20adapt%20to%20task%20discrepancies.%20Extensive%0Aexperiments%20show%20that%20MoME%20significantly%20improves%20the%20performance%20of%20generalist%0AMLLMs%20across%20various%20VL%20tasks.%20The%20source%20code%20is%20released%20at%0Ahttps%3A//github.com/JiuTian-VL/MoME%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12709v1&entry.124074799=Read"},
{"title": "To Believe or Not to Believe Your LLM", "author": "Yasin Abbasi Yadkori and Ilja Kuzborskij and Andr\u00e1s Gy\u00f6rgy and Csaba Szepesv\u00e1ri", "abstract": "  We explore uncertainty quantification in large language models (LLMs), with\nthe goal to identify when uncertainty in responses given a query is large. We\nsimultaneously consider both epistemic and aleatoric uncertainties, where the\nformer comes from the lack of knowledge about the ground truth (such as about\nfacts or the language), and the latter comes from irreducible randomness (such\nas multiple possible answers). In particular, we derive an\ninformation-theoretic metric that allows to reliably detect when only epistemic\nuncertainty is large, in which case the output of the model is unreliable. This\ncondition can be computed based solely on the output of the model obtained\nsimply by some special iterative prompting based on the previous responses.\nSuch quantification, for instance, allows to detect hallucinations (cases when\nepistemic uncertainty is high) in both single- and multi-answer responses. This\nis in contrast to many standard uncertainty quantification strategies (such as\nthresholding the log-likelihood of a response) where hallucinations in the\nmulti-answer case cannot be detected. We conduct a series of experiments which\ndemonstrate the advantage of our formulation. Further, our investigations shed\nsome light on how the probabilities assigned to a given output by an LLM can be\namplified by iterative prompting, which might be of independent interest.\n", "link": "http://arxiv.org/abs/2406.02543v2", "date": "2024-07-17", "relevancy": 2.1362, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5742}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5464}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Believe%20or%20Not%20to%20Believe%20Your%20LLM&body=Title%3A%20To%20Believe%20or%20Not%20to%20Believe%20Your%20LLM%0AAuthor%3A%20Yasin%20Abbasi%20Yadkori%20and%20Ilja%20Kuzborskij%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Csaba%20Szepesv%C3%A1ri%0AAbstract%3A%20%20%20We%20explore%20uncertainty%20quantification%20in%20large%20language%20models%20%28LLMs%29%2C%20with%0Athe%20goal%20to%20identify%20when%20uncertainty%20in%20responses%20given%20a%20query%20is%20large.%20We%0Asimultaneously%20consider%20both%20epistemic%20and%20aleatoric%20uncertainties%2C%20where%20the%0Aformer%20comes%20from%20the%20lack%20of%20knowledge%20about%20the%20ground%20truth%20%28such%20as%20about%0Afacts%20or%20the%20language%29%2C%20and%20the%20latter%20comes%20from%20irreducible%20randomness%20%28such%0Aas%20multiple%20possible%20answers%29.%20In%20particular%2C%20we%20derive%20an%0Ainformation-theoretic%20metric%20that%20allows%20to%20reliably%20detect%20when%20only%20epistemic%0Auncertainty%20is%20large%2C%20in%20which%20case%20the%20output%20of%20the%20model%20is%20unreliable.%20This%0Acondition%20can%20be%20computed%20based%20solely%20on%20the%20output%20of%20the%20model%20obtained%0Asimply%20by%20some%20special%20iterative%20prompting%20based%20on%20the%20previous%20responses.%0ASuch%20quantification%2C%20for%20instance%2C%20allows%20to%20detect%20hallucinations%20%28cases%20when%0Aepistemic%20uncertainty%20is%20high%29%20in%20both%20single-%20and%20multi-answer%20responses.%20This%0Ais%20in%20contrast%20to%20many%20standard%20uncertainty%20quantification%20strategies%20%28such%20as%0Athresholding%20the%20log-likelihood%20of%20a%20response%29%20where%20hallucinations%20in%20the%0Amulti-answer%20case%20cannot%20be%20detected.%20We%20conduct%20a%20series%20of%20experiments%20which%0Ademonstrate%20the%20advantage%20of%20our%20formulation.%20Further%2C%20our%20investigations%20shed%0Asome%20light%20on%20how%20the%20probabilities%20assigned%20to%20a%20given%20output%20by%20an%20LLM%20can%20be%0Aamplified%20by%20iterative%20prompting%2C%20which%20might%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Believe%2520or%2520Not%2520to%2520Believe%2520Your%2520LLM%26entry.906535625%3DYasin%2520Abbasi%2520Yadkori%2520and%2520Ilja%2520Kuzborskij%2520and%2520Andr%25C3%25A1s%2520Gy%25C3%25B6rgy%2520and%2520Csaba%2520Szepesv%25C3%25A1ri%26entry.1292438233%3D%2520%2520We%2520explore%2520uncertainty%2520quantification%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%250Athe%2520goal%2520to%2520identify%2520when%2520uncertainty%2520in%2520responses%2520given%2520a%2520query%2520is%2520large.%2520We%250Asimultaneously%2520consider%2520both%2520epistemic%2520and%2520aleatoric%2520uncertainties%252C%2520where%2520the%250Aformer%2520comes%2520from%2520the%2520lack%2520of%2520knowledge%2520about%2520the%2520ground%2520truth%2520%2528such%2520as%2520about%250Afacts%2520or%2520the%2520language%2529%252C%2520and%2520the%2520latter%2520comes%2520from%2520irreducible%2520randomness%2520%2528such%250Aas%2520multiple%2520possible%2520answers%2529.%2520In%2520particular%252C%2520we%2520derive%2520an%250Ainformation-theoretic%2520metric%2520that%2520allows%2520to%2520reliably%2520detect%2520when%2520only%2520epistemic%250Auncertainty%2520is%2520large%252C%2520in%2520which%2520case%2520the%2520output%2520of%2520the%2520model%2520is%2520unreliable.%2520This%250Acondition%2520can%2520be%2520computed%2520based%2520solely%2520on%2520the%2520output%2520of%2520the%2520model%2520obtained%250Asimply%2520by%2520some%2520special%2520iterative%2520prompting%2520based%2520on%2520the%2520previous%2520responses.%250ASuch%2520quantification%252C%2520for%2520instance%252C%2520allows%2520to%2520detect%2520hallucinations%2520%2528cases%2520when%250Aepistemic%2520uncertainty%2520is%2520high%2529%2520in%2520both%2520single-%2520and%2520multi-answer%2520responses.%2520This%250Ais%2520in%2520contrast%2520to%2520many%2520standard%2520uncertainty%2520quantification%2520strategies%2520%2528such%2520as%250Athresholding%2520the%2520log-likelihood%2520of%2520a%2520response%2529%2520where%2520hallucinations%2520in%2520the%250Amulti-answer%2520case%2520cannot%2520be%2520detected.%2520We%2520conduct%2520a%2520series%2520of%2520experiments%2520which%250Ademonstrate%2520the%2520advantage%2520of%2520our%2520formulation.%2520Further%252C%2520our%2520investigations%2520shed%250Asome%2520light%2520on%2520how%2520the%2520probabilities%2520assigned%2520to%2520a%2520given%2520output%2520by%2520an%2520LLM%2520can%2520be%250Aamplified%2520by%2520iterative%2520prompting%252C%2520which%2520might%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Believe%20or%20Not%20to%20Believe%20Your%20LLM&entry.906535625=Yasin%20Abbasi%20Yadkori%20and%20Ilja%20Kuzborskij%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Csaba%20Szepesv%C3%A1ri&entry.1292438233=%20%20We%20explore%20uncertainty%20quantification%20in%20large%20language%20models%20%28LLMs%29%2C%20with%0Athe%20goal%20to%20identify%20when%20uncertainty%20in%20responses%20given%20a%20query%20is%20large.%20We%0Asimultaneously%20consider%20both%20epistemic%20and%20aleatoric%20uncertainties%2C%20where%20the%0Aformer%20comes%20from%20the%20lack%20of%20knowledge%20about%20the%20ground%20truth%20%28such%20as%20about%0Afacts%20or%20the%20language%29%2C%20and%20the%20latter%20comes%20from%20irreducible%20randomness%20%28such%0Aas%20multiple%20possible%20answers%29.%20In%20particular%2C%20we%20derive%20an%0Ainformation-theoretic%20metric%20that%20allows%20to%20reliably%20detect%20when%20only%20epistemic%0Auncertainty%20is%20large%2C%20in%20which%20case%20the%20output%20of%20the%20model%20is%20unreliable.%20This%0Acondition%20can%20be%20computed%20based%20solely%20on%20the%20output%20of%20the%20model%20obtained%0Asimply%20by%20some%20special%20iterative%20prompting%20based%20on%20the%20previous%20responses.%0ASuch%20quantification%2C%20for%20instance%2C%20allows%20to%20detect%20hallucinations%20%28cases%20when%0Aepistemic%20uncertainty%20is%20high%29%20in%20both%20single-%20and%20multi-answer%20responses.%20This%0Ais%20in%20contrast%20to%20many%20standard%20uncertainty%20quantification%20strategies%20%28such%20as%0Athresholding%20the%20log-likelihood%20of%20a%20response%29%20where%20hallucinations%20in%20the%0Amulti-answer%20case%20cannot%20be%20detected.%20We%20conduct%20a%20series%20of%20experiments%20which%0Ademonstrate%20the%20advantage%20of%20our%20formulation.%20Further%2C%20our%20investigations%20shed%0Asome%20light%20on%20how%20the%20probabilities%20assigned%20to%20a%20given%20output%20by%20an%20LLM%20can%20be%0Aamplified%20by%20iterative%20prompting%2C%20which%20might%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02543v2&entry.124074799=Read"},
{"title": "Generative Enzyme Design Guided by Functionally Important Sites and\n  Small-Molecule Substrates", "author": "Zhenqiao Song and Yunlong Zhao and Wenxian Shi and Wengong Jin and Yang Yang and Lei Li", "abstract": "  Enzymes are genetically encoded biocatalysts capable of accelerating chemical\nreactions. How can we automatically design functional enzymes? In this paper,\nwe propose EnzyGen, an approach to learn a unified model to design enzymes\nacross all functional families. Our key idea is to generate an enzyme's amino\nacid sequence and their three-dimensional (3D) coordinates based on\nfunctionally important sites and substrates corresponding to a desired\ncatalytic function. These sites are automatically mined from enzyme databases.\nEnzyGen consists of a novel interleaving network of attention and neighborhood\nequivariant layers, which captures both long-range correlation in an entire\nprotein sequence and local influence from nearest amino acids in 3D space. To\nlearn the generative model, we devise a joint training objective, including a\nsequence generation loss, a position prediction loss and an enzyme-substrate\ninteraction loss. We further construct EnzyBench, a dataset with 3157 enzyme\nfamilies, covering all available enzymes within the protein data bank (PDB).\nExperimental results show that our EnzyGen consistently achieves the best\nperformance across all 323 testing families, surpassing the best baseline by\n10.79% in terms of substrate binding affinity. These findings demonstrate\nEnzyGen's superior capability in designing well-folded and effective enzymes\nbinding to specific substrates with high affinities.\n", "link": "http://arxiv.org/abs/2405.08205v3", "date": "2024-07-17", "relevancy": 2.1355, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5804}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5051}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Enzyme%20Design%20Guided%20by%20Functionally%20Important%20Sites%20and%0A%20%20Small-Molecule%20Substrates&body=Title%3A%20Generative%20Enzyme%20Design%20Guided%20by%20Functionally%20Important%20Sites%20and%0A%20%20Small-Molecule%20Substrates%0AAuthor%3A%20Zhenqiao%20Song%20and%20Yunlong%20Zhao%20and%20Wenxian%20Shi%20and%20Wengong%20Jin%20and%20Yang%20Yang%20and%20Lei%20Li%0AAbstract%3A%20%20%20Enzymes%20are%20genetically%20encoded%20biocatalysts%20capable%20of%20accelerating%20chemical%0Areactions.%20How%20can%20we%20automatically%20design%20functional%20enzymes%3F%20In%20this%20paper%2C%0Awe%20propose%20EnzyGen%2C%20an%20approach%20to%20learn%20a%20unified%20model%20to%20design%20enzymes%0Aacross%20all%20functional%20families.%20Our%20key%20idea%20is%20to%20generate%20an%20enzyme%27s%20amino%0Aacid%20sequence%20and%20their%20three-dimensional%20%283D%29%20coordinates%20based%20on%0Afunctionally%20important%20sites%20and%20substrates%20corresponding%20to%20a%20desired%0Acatalytic%20function.%20These%20sites%20are%20automatically%20mined%20from%20enzyme%20databases.%0AEnzyGen%20consists%20of%20a%20novel%20interleaving%20network%20of%20attention%20and%20neighborhood%0Aequivariant%20layers%2C%20which%20captures%20both%20long-range%20correlation%20in%20an%20entire%0Aprotein%20sequence%20and%20local%20influence%20from%20nearest%20amino%20acids%20in%203D%20space.%20To%0Alearn%20the%20generative%20model%2C%20we%20devise%20a%20joint%20training%20objective%2C%20including%20a%0Asequence%20generation%20loss%2C%20a%20position%20prediction%20loss%20and%20an%20enzyme-substrate%0Ainteraction%20loss.%20We%20further%20construct%20EnzyBench%2C%20a%20dataset%20with%203157%20enzyme%0Afamilies%2C%20covering%20all%20available%20enzymes%20within%20the%20protein%20data%20bank%20%28PDB%29.%0AExperimental%20results%20show%20that%20our%20EnzyGen%20consistently%20achieves%20the%20best%0Aperformance%20across%20all%20323%20testing%20families%2C%20surpassing%20the%20best%20baseline%20by%0A10.79%25%20in%20terms%20of%20substrate%20binding%20affinity.%20These%20findings%20demonstrate%0AEnzyGen%27s%20superior%20capability%20in%20designing%20well-folded%20and%20effective%20enzymes%0Abinding%20to%20specific%20substrates%20with%20high%20affinities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08205v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Enzyme%2520Design%2520Guided%2520by%2520Functionally%2520Important%2520Sites%2520and%250A%2520%2520Small-Molecule%2520Substrates%26entry.906535625%3DZhenqiao%2520Song%2520and%2520Yunlong%2520Zhao%2520and%2520Wenxian%2520Shi%2520and%2520Wengong%2520Jin%2520and%2520Yang%2520Yang%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520Enzymes%2520are%2520genetically%2520encoded%2520biocatalysts%2520capable%2520of%2520accelerating%2520chemical%250Areactions.%2520How%2520can%2520we%2520automatically%2520design%2520functional%2520enzymes%253F%2520In%2520this%2520paper%252C%250Awe%2520propose%2520EnzyGen%252C%2520an%2520approach%2520to%2520learn%2520a%2520unified%2520model%2520to%2520design%2520enzymes%250Aacross%2520all%2520functional%2520families.%2520Our%2520key%2520idea%2520is%2520to%2520generate%2520an%2520enzyme%2527s%2520amino%250Aacid%2520sequence%2520and%2520their%2520three-dimensional%2520%25283D%2529%2520coordinates%2520based%2520on%250Afunctionally%2520important%2520sites%2520and%2520substrates%2520corresponding%2520to%2520a%2520desired%250Acatalytic%2520function.%2520These%2520sites%2520are%2520automatically%2520mined%2520from%2520enzyme%2520databases.%250AEnzyGen%2520consists%2520of%2520a%2520novel%2520interleaving%2520network%2520of%2520attention%2520and%2520neighborhood%250Aequivariant%2520layers%252C%2520which%2520captures%2520both%2520long-range%2520correlation%2520in%2520an%2520entire%250Aprotein%2520sequence%2520and%2520local%2520influence%2520from%2520nearest%2520amino%2520acids%2520in%25203D%2520space.%2520To%250Alearn%2520the%2520generative%2520model%252C%2520we%2520devise%2520a%2520joint%2520training%2520objective%252C%2520including%2520a%250Asequence%2520generation%2520loss%252C%2520a%2520position%2520prediction%2520loss%2520and%2520an%2520enzyme-substrate%250Ainteraction%2520loss.%2520We%2520further%2520construct%2520EnzyBench%252C%2520a%2520dataset%2520with%25203157%2520enzyme%250Afamilies%252C%2520covering%2520all%2520available%2520enzymes%2520within%2520the%2520protein%2520data%2520bank%2520%2528PDB%2529.%250AExperimental%2520results%2520show%2520that%2520our%2520EnzyGen%2520consistently%2520achieves%2520the%2520best%250Aperformance%2520across%2520all%2520323%2520testing%2520families%252C%2520surpassing%2520the%2520best%2520baseline%2520by%250A10.79%2525%2520in%2520terms%2520of%2520substrate%2520binding%2520affinity.%2520These%2520findings%2520demonstrate%250AEnzyGen%2527s%2520superior%2520capability%2520in%2520designing%2520well-folded%2520and%2520effective%2520enzymes%250Abinding%2520to%2520specific%2520substrates%2520with%2520high%2520affinities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08205v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Enzyme%20Design%20Guided%20by%20Functionally%20Important%20Sites%20and%0A%20%20Small-Molecule%20Substrates&entry.906535625=Zhenqiao%20Song%20and%20Yunlong%20Zhao%20and%20Wenxian%20Shi%20and%20Wengong%20Jin%20and%20Yang%20Yang%20and%20Lei%20Li&entry.1292438233=%20%20Enzymes%20are%20genetically%20encoded%20biocatalysts%20capable%20of%20accelerating%20chemical%0Areactions.%20How%20can%20we%20automatically%20design%20functional%20enzymes%3F%20In%20this%20paper%2C%0Awe%20propose%20EnzyGen%2C%20an%20approach%20to%20learn%20a%20unified%20model%20to%20design%20enzymes%0Aacross%20all%20functional%20families.%20Our%20key%20idea%20is%20to%20generate%20an%20enzyme%27s%20amino%0Aacid%20sequence%20and%20their%20three-dimensional%20%283D%29%20coordinates%20based%20on%0Afunctionally%20important%20sites%20and%20substrates%20corresponding%20to%20a%20desired%0Acatalytic%20function.%20These%20sites%20are%20automatically%20mined%20from%20enzyme%20databases.%0AEnzyGen%20consists%20of%20a%20novel%20interleaving%20network%20of%20attention%20and%20neighborhood%0Aequivariant%20layers%2C%20which%20captures%20both%20long-range%20correlation%20in%20an%20entire%0Aprotein%20sequence%20and%20local%20influence%20from%20nearest%20amino%20acids%20in%203D%20space.%20To%0Alearn%20the%20generative%20model%2C%20we%20devise%20a%20joint%20training%20objective%2C%20including%20a%0Asequence%20generation%20loss%2C%20a%20position%20prediction%20loss%20and%20an%20enzyme-substrate%0Ainteraction%20loss.%20We%20further%20construct%20EnzyBench%2C%20a%20dataset%20with%203157%20enzyme%0Afamilies%2C%20covering%20all%20available%20enzymes%20within%20the%20protein%20data%20bank%20%28PDB%29.%0AExperimental%20results%20show%20that%20our%20EnzyGen%20consistently%20achieves%20the%20best%0Aperformance%20across%20all%20323%20testing%20families%2C%20surpassing%20the%20best%20baseline%20by%0A10.79%25%20in%20terms%20of%20substrate%20binding%20affinity.%20These%20findings%20demonstrate%0AEnzyGen%27s%20superior%20capability%20in%20designing%20well-folded%20and%20effective%20enzymes%0Abinding%20to%20specific%20substrates%20with%20high%20affinities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08205v3&entry.124074799=Read"},
{"title": "TimeDRL: Disentangled Representation Learning for Multivariate\n  Time-Series", "author": "Ching Chang and Chiao-Tung Chan and Wei-Yao Wang and Wen-Chih Peng and Tien-Fu Chen", "abstract": "  Multivariate time-series data in numerous real-world applications (e.g.,\nhealthcare and industry) are informative but challenging due to the lack of\nlabels and high dimensionality. Recent studies in self-supervised learning have\nshown their potential in learning rich representations without relying on\nlabels, yet they fall short in learning disentangled embeddings and addressing\nissues of inductive bias (e.g., transformation-invariance). To tackle these\nchallenges, we propose TimeDRL, a generic multivariate time-series\nrepresentation learning framework with disentangled dual-level embeddings.\nTimeDRL is characterized by three novel features: (i) disentangled derivation\nof timestamp-level and instance-level embeddings from patched time-series data\nusing a [CLS] token strategy; (ii) utilization of timestamp-predictive and\ninstance-contrastive tasks for disentangled representation learning, with the\nformer optimizing timestamp-level embeddings with predictive loss, and the\nlatter optimizing instance-level embeddings with contrastive loss; and (iii)\navoidance of augmentation methods to eliminate inductive biases, such as\ntransformation-invariance from cropping and masking. Comprehensive experiments\non 6 time-series forecasting datasets and 5 time-series classification datasets\nhave shown that TimeDRL consistently surpasses existing representation learning\napproaches, achieving an average improvement of forecasting by 58.02% in MSE\nand classification by 1.48% in accuracy. Furthermore, extensive ablation\nstudies confirmed the relative contribution of each component in TimeDRL's\narchitecture, and semi-supervised learning evaluations demonstrated its\neffectiveness in real-world scenarios, even with limited labeled data. The code\nis available at https://github.com/blacksnail789521/TimeDRL.\n", "link": "http://arxiv.org/abs/2312.04142v3", "date": "2024-07-17", "relevancy": 2.1284, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5246}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeDRL%3A%20Disentangled%20Representation%20Learning%20for%20Multivariate%0A%20%20Time-Series&body=Title%3A%20TimeDRL%3A%20Disentangled%20Representation%20Learning%20for%20Multivariate%0A%20%20Time-Series%0AAuthor%3A%20Ching%20Chang%20and%20Chiao-Tung%20Chan%20and%20Wei-Yao%20Wang%20and%20Wen-Chih%20Peng%20and%20Tien-Fu%20Chen%0AAbstract%3A%20%20%20Multivariate%20time-series%20data%20in%20numerous%20real-world%20applications%20%28e.g.%2C%0Ahealthcare%20and%20industry%29%20are%20informative%20but%20challenging%20due%20to%20the%20lack%20of%0Alabels%20and%20high%20dimensionality.%20Recent%20studies%20in%20self-supervised%20learning%20have%0Ashown%20their%20potential%20in%20learning%20rich%20representations%20without%20relying%20on%0Alabels%2C%20yet%20they%20fall%20short%20in%20learning%20disentangled%20embeddings%20and%20addressing%0Aissues%20of%20inductive%20bias%20%28e.g.%2C%20transformation-invariance%29.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20TimeDRL%2C%20a%20generic%20multivariate%20time-series%0Arepresentation%20learning%20framework%20with%20disentangled%20dual-level%20embeddings.%0ATimeDRL%20is%20characterized%20by%20three%20novel%20features%3A%20%28i%29%20disentangled%20derivation%0Aof%20timestamp-level%20and%20instance-level%20embeddings%20from%20patched%20time-series%20data%0Ausing%20a%20%5BCLS%5D%20token%20strategy%3B%20%28ii%29%20utilization%20of%20timestamp-predictive%20and%0Ainstance-contrastive%20tasks%20for%20disentangled%20representation%20learning%2C%20with%20the%0Aformer%20optimizing%20timestamp-level%20embeddings%20with%20predictive%20loss%2C%20and%20the%0Alatter%20optimizing%20instance-level%20embeddings%20with%20contrastive%20loss%3B%20and%20%28iii%29%0Aavoidance%20of%20augmentation%20methods%20to%20eliminate%20inductive%20biases%2C%20such%20as%0Atransformation-invariance%20from%20cropping%20and%20masking.%20Comprehensive%20experiments%0Aon%206%20time-series%20forecasting%20datasets%20and%205%20time-series%20classification%20datasets%0Ahave%20shown%20that%20TimeDRL%20consistently%20surpasses%20existing%20representation%20learning%0Aapproaches%2C%20achieving%20an%20average%20improvement%20of%20forecasting%20by%2058.02%25%20in%20MSE%0Aand%20classification%20by%201.48%25%20in%20accuracy.%20Furthermore%2C%20extensive%20ablation%0Astudies%20confirmed%20the%20relative%20contribution%20of%20each%20component%20in%20TimeDRL%27s%0Aarchitecture%2C%20and%20semi-supervised%20learning%20evaluations%20demonstrated%20its%0Aeffectiveness%20in%20real-world%20scenarios%2C%20even%20with%20limited%20labeled%20data.%20The%20code%0Ais%20available%20at%20https%3A//github.com/blacksnail789521/TimeDRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04142v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeDRL%253A%2520Disentangled%2520Representation%2520Learning%2520for%2520Multivariate%250A%2520%2520Time-Series%26entry.906535625%3DChing%2520Chang%2520and%2520Chiao-Tung%2520Chan%2520and%2520Wei-Yao%2520Wang%2520and%2520Wen-Chih%2520Peng%2520and%2520Tien-Fu%2520Chen%26entry.1292438233%3D%2520%2520Multivariate%2520time-series%2520data%2520in%2520numerous%2520real-world%2520applications%2520%2528e.g.%252C%250Ahealthcare%2520and%2520industry%2529%2520are%2520informative%2520but%2520challenging%2520due%2520to%2520the%2520lack%2520of%250Alabels%2520and%2520high%2520dimensionality.%2520Recent%2520studies%2520in%2520self-supervised%2520learning%2520have%250Ashown%2520their%2520potential%2520in%2520learning%2520rich%2520representations%2520without%2520relying%2520on%250Alabels%252C%2520yet%2520they%2520fall%2520short%2520in%2520learning%2520disentangled%2520embeddings%2520and%2520addressing%250Aissues%2520of%2520inductive%2520bias%2520%2528e.g.%252C%2520transformation-invariance%2529.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520propose%2520TimeDRL%252C%2520a%2520generic%2520multivariate%2520time-series%250Arepresentation%2520learning%2520framework%2520with%2520disentangled%2520dual-level%2520embeddings.%250ATimeDRL%2520is%2520characterized%2520by%2520three%2520novel%2520features%253A%2520%2528i%2529%2520disentangled%2520derivation%250Aof%2520timestamp-level%2520and%2520instance-level%2520embeddings%2520from%2520patched%2520time-series%2520data%250Ausing%2520a%2520%255BCLS%255D%2520token%2520strategy%253B%2520%2528ii%2529%2520utilization%2520of%2520timestamp-predictive%2520and%250Ainstance-contrastive%2520tasks%2520for%2520disentangled%2520representation%2520learning%252C%2520with%2520the%250Aformer%2520optimizing%2520timestamp-level%2520embeddings%2520with%2520predictive%2520loss%252C%2520and%2520the%250Alatter%2520optimizing%2520instance-level%2520embeddings%2520with%2520contrastive%2520loss%253B%2520and%2520%2528iii%2529%250Aavoidance%2520of%2520augmentation%2520methods%2520to%2520eliminate%2520inductive%2520biases%252C%2520such%2520as%250Atransformation-invariance%2520from%2520cropping%2520and%2520masking.%2520Comprehensive%2520experiments%250Aon%25206%2520time-series%2520forecasting%2520datasets%2520and%25205%2520time-series%2520classification%2520datasets%250Ahave%2520shown%2520that%2520TimeDRL%2520consistently%2520surpasses%2520existing%2520representation%2520learning%250Aapproaches%252C%2520achieving%2520an%2520average%2520improvement%2520of%2520forecasting%2520by%252058.02%2525%2520in%2520MSE%250Aand%2520classification%2520by%25201.48%2525%2520in%2520accuracy.%2520Furthermore%252C%2520extensive%2520ablation%250Astudies%2520confirmed%2520the%2520relative%2520contribution%2520of%2520each%2520component%2520in%2520TimeDRL%2527s%250Aarchitecture%252C%2520and%2520semi-supervised%2520learning%2520evaluations%2520demonstrated%2520its%250Aeffectiveness%2520in%2520real-world%2520scenarios%252C%2520even%2520with%2520limited%2520labeled%2520data.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/blacksnail789521/TimeDRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04142v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeDRL%3A%20Disentangled%20Representation%20Learning%20for%20Multivariate%0A%20%20Time-Series&entry.906535625=Ching%20Chang%20and%20Chiao-Tung%20Chan%20and%20Wei-Yao%20Wang%20and%20Wen-Chih%20Peng%20and%20Tien-Fu%20Chen&entry.1292438233=%20%20Multivariate%20time-series%20data%20in%20numerous%20real-world%20applications%20%28e.g.%2C%0Ahealthcare%20and%20industry%29%20are%20informative%20but%20challenging%20due%20to%20the%20lack%20of%0Alabels%20and%20high%20dimensionality.%20Recent%20studies%20in%20self-supervised%20learning%20have%0Ashown%20their%20potential%20in%20learning%20rich%20representations%20without%20relying%20on%0Alabels%2C%20yet%20they%20fall%20short%20in%20learning%20disentangled%20embeddings%20and%20addressing%0Aissues%20of%20inductive%20bias%20%28e.g.%2C%20transformation-invariance%29.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20TimeDRL%2C%20a%20generic%20multivariate%20time-series%0Arepresentation%20learning%20framework%20with%20disentangled%20dual-level%20embeddings.%0ATimeDRL%20is%20characterized%20by%20three%20novel%20features%3A%20%28i%29%20disentangled%20derivation%0Aof%20timestamp-level%20and%20instance-level%20embeddings%20from%20patched%20time-series%20data%0Ausing%20a%20%5BCLS%5D%20token%20strategy%3B%20%28ii%29%20utilization%20of%20timestamp-predictive%20and%0Ainstance-contrastive%20tasks%20for%20disentangled%20representation%20learning%2C%20with%20the%0Aformer%20optimizing%20timestamp-level%20embeddings%20with%20predictive%20loss%2C%20and%20the%0Alatter%20optimizing%20instance-level%20embeddings%20with%20contrastive%20loss%3B%20and%20%28iii%29%0Aavoidance%20of%20augmentation%20methods%20to%20eliminate%20inductive%20biases%2C%20such%20as%0Atransformation-invariance%20from%20cropping%20and%20masking.%20Comprehensive%20experiments%0Aon%206%20time-series%20forecasting%20datasets%20and%205%20time-series%20classification%20datasets%0Ahave%20shown%20that%20TimeDRL%20consistently%20surpasses%20existing%20representation%20learning%0Aapproaches%2C%20achieving%20an%20average%20improvement%20of%20forecasting%20by%2058.02%25%20in%20MSE%0Aand%20classification%20by%201.48%25%20in%20accuracy.%20Furthermore%2C%20extensive%20ablation%0Astudies%20confirmed%20the%20relative%20contribution%20of%20each%20component%20in%20TimeDRL%27s%0Aarchitecture%2C%20and%20semi-supervised%20learning%20evaluations%20demonstrated%20its%0Aeffectiveness%20in%20real-world%20scenarios%2C%20even%20with%20limited%20labeled%20data.%20The%20code%0Ais%20available%20at%20https%3A//github.com/blacksnail789521/TimeDRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04142v3&entry.124074799=Read"},
{"title": "UniTE: A Survey and Unified Pipeline for Pre-training ST Trajectory\n  Embeddings", "author": "Yan Lin and Zeyu Zhou and Yicheng Liu and Haochen Lv and Haomin Wen and Tianyi Li and Yushuai Li and Christian S. Jensen and Shengnan Guo and Youfang Lin and Huaiyu Wan", "abstract": "  Spatio-temporal (ST) trajectories are sequences of timestamped locations,\nwhich enable a variety of analyses that in turn enable important real-world\napplications. It is common to map trajectories to vectors, called embeddings,\nbefore subsequent analyses. Thus, the qualities of embeddings are very\nimportant. Methods for pre-training embeddings, which leverage unlabeled\ntrajectories for training universal embeddings, have shown promising\napplicability across different tasks, thus attracting considerable interest.\nHowever, research progress on this topic faces two key challenges: a lack of a\ncomprehensive overview of existing methods, resulting in several related\nmethods not being well-recognized, and the absence of a unified pipeline,\ncomplicating the development new methods and the analysis of methods.\n  To overcome these obstacles and advance the field of pre-training of\ntrajectory embeddings, we present UniTE, a survey and a unified pipeline for\nthis domain. In doing so, we present a comprehensive list of existing methods\nfor pre-training trajectory embeddings, which includes methods that either\nexplicitly or implicitly employ pre-training techniques. Further, we present a\nunified and modular pipeline with publicly available underlying code,\nsimplifying the process of constructing and evaluating methods for pre-training\ntrajectory embeddings. Additionally, we contribute a selection of experimental\nresults using the proposed pipeline on real-world datasets.\n", "link": "http://arxiv.org/abs/2407.12550v1", "date": "2024-07-17", "relevancy": 2.0985, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTE%3A%20A%20Survey%20and%20Unified%20Pipeline%20for%20Pre-training%20ST%20Trajectory%0A%20%20Embeddings&body=Title%3A%20UniTE%3A%20A%20Survey%20and%20Unified%20Pipeline%20for%20Pre-training%20ST%20Trajectory%0A%20%20Embeddings%0AAuthor%3A%20Yan%20Lin%20and%20Zeyu%20Zhou%20and%20Yicheng%20Liu%20and%20Haochen%20Lv%20and%20Haomin%20Wen%20and%20Tianyi%20Li%20and%20Yushuai%20Li%20and%20Christian%20S.%20Jensen%20and%20Shengnan%20Guo%20and%20Youfang%20Lin%20and%20Huaiyu%20Wan%0AAbstract%3A%20%20%20Spatio-temporal%20%28ST%29%20trajectories%20are%20sequences%20of%20timestamped%20locations%2C%0Awhich%20enable%20a%20variety%20of%20analyses%20that%20in%20turn%20enable%20important%20real-world%0Aapplications.%20It%20is%20common%20to%20map%20trajectories%20to%20vectors%2C%20called%20embeddings%2C%0Abefore%20subsequent%20analyses.%20Thus%2C%20the%20qualities%20of%20embeddings%20are%20very%0Aimportant.%20Methods%20for%20pre-training%20embeddings%2C%20which%20leverage%20unlabeled%0Atrajectories%20for%20training%20universal%20embeddings%2C%20have%20shown%20promising%0Aapplicability%20across%20different%20tasks%2C%20thus%20attracting%20considerable%20interest.%0AHowever%2C%20research%20progress%20on%20this%20topic%20faces%20two%20key%20challenges%3A%20a%20lack%20of%20a%0Acomprehensive%20overview%20of%20existing%20methods%2C%20resulting%20in%20several%20related%0Amethods%20not%20being%20well-recognized%2C%20and%20the%20absence%20of%20a%20unified%20pipeline%2C%0Acomplicating%20the%20development%20new%20methods%20and%20the%20analysis%20of%20methods.%0A%20%20To%20overcome%20these%20obstacles%20and%20advance%20the%20field%20of%20pre-training%20of%0Atrajectory%20embeddings%2C%20we%20present%20UniTE%2C%20a%20survey%20and%20a%20unified%20pipeline%20for%0Athis%20domain.%20In%20doing%20so%2C%20we%20present%20a%20comprehensive%20list%20of%20existing%20methods%0Afor%20pre-training%20trajectory%20embeddings%2C%20which%20includes%20methods%20that%20either%0Aexplicitly%20or%20implicitly%20employ%20pre-training%20techniques.%20Further%2C%20we%20present%20a%0Aunified%20and%20modular%20pipeline%20with%20publicly%20available%20underlying%20code%2C%0Asimplifying%20the%20process%20of%20constructing%20and%20evaluating%20methods%20for%20pre-training%0Atrajectory%20embeddings.%20Additionally%2C%20we%20contribute%20a%20selection%20of%20experimental%0Aresults%20using%20the%20proposed%20pipeline%20on%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTE%253A%2520A%2520Survey%2520and%2520Unified%2520Pipeline%2520for%2520Pre-training%2520ST%2520Trajectory%250A%2520%2520Embeddings%26entry.906535625%3DYan%2520Lin%2520and%2520Zeyu%2520Zhou%2520and%2520Yicheng%2520Liu%2520and%2520Haochen%2520Lv%2520and%2520Haomin%2520Wen%2520and%2520Tianyi%2520Li%2520and%2520Yushuai%2520Li%2520and%2520Christian%2520S.%2520Jensen%2520and%2520Shengnan%2520Guo%2520and%2520Youfang%2520Lin%2520and%2520Huaiyu%2520Wan%26entry.1292438233%3D%2520%2520Spatio-temporal%2520%2528ST%2529%2520trajectories%2520are%2520sequences%2520of%2520timestamped%2520locations%252C%250Awhich%2520enable%2520a%2520variety%2520of%2520analyses%2520that%2520in%2520turn%2520enable%2520important%2520real-world%250Aapplications.%2520It%2520is%2520common%2520to%2520map%2520trajectories%2520to%2520vectors%252C%2520called%2520embeddings%252C%250Abefore%2520subsequent%2520analyses.%2520Thus%252C%2520the%2520qualities%2520of%2520embeddings%2520are%2520very%250Aimportant.%2520Methods%2520for%2520pre-training%2520embeddings%252C%2520which%2520leverage%2520unlabeled%250Atrajectories%2520for%2520training%2520universal%2520embeddings%252C%2520have%2520shown%2520promising%250Aapplicability%2520across%2520different%2520tasks%252C%2520thus%2520attracting%2520considerable%2520interest.%250AHowever%252C%2520research%2520progress%2520on%2520this%2520topic%2520faces%2520two%2520key%2520challenges%253A%2520a%2520lack%2520of%2520a%250Acomprehensive%2520overview%2520of%2520existing%2520methods%252C%2520resulting%2520in%2520several%2520related%250Amethods%2520not%2520being%2520well-recognized%252C%2520and%2520the%2520absence%2520of%2520a%2520unified%2520pipeline%252C%250Acomplicating%2520the%2520development%2520new%2520methods%2520and%2520the%2520analysis%2520of%2520methods.%250A%2520%2520To%2520overcome%2520these%2520obstacles%2520and%2520advance%2520the%2520field%2520of%2520pre-training%2520of%250Atrajectory%2520embeddings%252C%2520we%2520present%2520UniTE%252C%2520a%2520survey%2520and%2520a%2520unified%2520pipeline%2520for%250Athis%2520domain.%2520In%2520doing%2520so%252C%2520we%2520present%2520a%2520comprehensive%2520list%2520of%2520existing%2520methods%250Afor%2520pre-training%2520trajectory%2520embeddings%252C%2520which%2520includes%2520methods%2520that%2520either%250Aexplicitly%2520or%2520implicitly%2520employ%2520pre-training%2520techniques.%2520Further%252C%2520we%2520present%2520a%250Aunified%2520and%2520modular%2520pipeline%2520with%2520publicly%2520available%2520underlying%2520code%252C%250Asimplifying%2520the%2520process%2520of%2520constructing%2520and%2520evaluating%2520methods%2520for%2520pre-training%250Atrajectory%2520embeddings.%2520Additionally%252C%2520we%2520contribute%2520a%2520selection%2520of%2520experimental%250Aresults%2520using%2520the%2520proposed%2520pipeline%2520on%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTE%3A%20A%20Survey%20and%20Unified%20Pipeline%20for%20Pre-training%20ST%20Trajectory%0A%20%20Embeddings&entry.906535625=Yan%20Lin%20and%20Zeyu%20Zhou%20and%20Yicheng%20Liu%20and%20Haochen%20Lv%20and%20Haomin%20Wen%20and%20Tianyi%20Li%20and%20Yushuai%20Li%20and%20Christian%20S.%20Jensen%20and%20Shengnan%20Guo%20and%20Youfang%20Lin%20and%20Huaiyu%20Wan&entry.1292438233=%20%20Spatio-temporal%20%28ST%29%20trajectories%20are%20sequences%20of%20timestamped%20locations%2C%0Awhich%20enable%20a%20variety%20of%20analyses%20that%20in%20turn%20enable%20important%20real-world%0Aapplications.%20It%20is%20common%20to%20map%20trajectories%20to%20vectors%2C%20called%20embeddings%2C%0Abefore%20subsequent%20analyses.%20Thus%2C%20the%20qualities%20of%20embeddings%20are%20very%0Aimportant.%20Methods%20for%20pre-training%20embeddings%2C%20which%20leverage%20unlabeled%0Atrajectories%20for%20training%20universal%20embeddings%2C%20have%20shown%20promising%0Aapplicability%20across%20different%20tasks%2C%20thus%20attracting%20considerable%20interest.%0AHowever%2C%20research%20progress%20on%20this%20topic%20faces%20two%20key%20challenges%3A%20a%20lack%20of%20a%0Acomprehensive%20overview%20of%20existing%20methods%2C%20resulting%20in%20several%20related%0Amethods%20not%20being%20well-recognized%2C%20and%20the%20absence%20of%20a%20unified%20pipeline%2C%0Acomplicating%20the%20development%20new%20methods%20and%20the%20analysis%20of%20methods.%0A%20%20To%20overcome%20these%20obstacles%20and%20advance%20the%20field%20of%20pre-training%20of%0Atrajectory%20embeddings%2C%20we%20present%20UniTE%2C%20a%20survey%20and%20a%20unified%20pipeline%20for%0Athis%20domain.%20In%20doing%20so%2C%20we%20present%20a%20comprehensive%20list%20of%20existing%20methods%0Afor%20pre-training%20trajectory%20embeddings%2C%20which%20includes%20methods%20that%20either%0Aexplicitly%20or%20implicitly%20employ%20pre-training%20techniques.%20Further%2C%20we%20present%20a%0Aunified%20and%20modular%20pipeline%20with%20publicly%20available%20underlying%20code%2C%0Asimplifying%20the%20process%20of%20constructing%20and%20evaluating%20methods%20for%20pre-training%0Atrajectory%20embeddings.%20Additionally%2C%20we%20contribute%20a%20selection%20of%20experimental%0Aresults%20using%20the%20proposed%20pipeline%20on%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12550v1&entry.124074799=Read"},
{"title": "Goldfish: Vision-Language Understanding of Arbitrarily Long Videos", "author": "Kirolos Ataallah and Xiaoqian Shen and Eslam Abdelrahman and Essam Sleiman and Mingchen Zhuge and Jian Ding and Deyao Zhu and J\u00fcrgen Schmidhuber and Mohamed Elhoseiny", "abstract": "  Most current LLM-based models for video understanding can process videos\nwithin minutes. However, they struggle with lengthy videos due to challenges\nsuch as \"noise and redundancy\", as well as \"memory and computation\"\nconstraints. In this paper, we present Goldfish, a methodology tailored for\ncomprehending videos of arbitrary lengths. We also introduce the TVQA-long\nbenchmark, specifically designed to evaluate models' capabilities in\nunderstanding long videos with questions in both vision and text content.\nGoldfish approaches these challenges with an efficient retrieval mechanism that\ninitially gathers the top-k video clips relevant to the instruction before\nproceeding to provide the desired response. This design of the retrieval\nmechanism enables the Goldfish to efficiently process arbitrarily long video\nsequences, facilitating its application in contexts such as movies or\ntelevision series. To facilitate the retrieval process, we developed\nMiniGPT4-Video that generates detailed descriptions for the video clips. In\naddressing the scarcity of benchmarks for long video evaluation, we adapted the\nTVQA short video benchmark for extended content analysis by aggregating\nquestions from entire episodes, thereby shifting the evaluation from partial to\nfull episode comprehension. We attained a 41.78% accuracy rate on the TVQA-long\nbenchmark, surpassing previous methods by 14.94%. Our MiniGPT4-Video also shows\nexceptional performance in short video comprehension, exceeding existing\nstate-of-the-art methods by 3.23%, 2.03%, 16.5% and 23.59% on the MSVD, MSRVTT,\nTGIF, and TVQA short video benchmarks, respectively. These results indicate\nthat our models have significant improvements in both long and short-video\nunderstanding. Our models and code have been made publicly available at\nhttps://vision-cair.github.io/Goldfish_website/\n", "link": "http://arxiv.org/abs/2407.12679v1", "date": "2024-07-17", "relevancy": 2.0965, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5416}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5363}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goldfish%3A%20Vision-Language%20Understanding%20of%20Arbitrarily%20Long%20Videos&body=Title%3A%20Goldfish%3A%20Vision-Language%20Understanding%20of%20Arbitrarily%20Long%20Videos%0AAuthor%3A%20Kirolos%20Ataallah%20and%20Xiaoqian%20Shen%20and%20Eslam%20Abdelrahman%20and%20Essam%20Sleiman%20and%20Mingchen%20Zhuge%20and%20Jian%20Ding%20and%20Deyao%20Zhu%20and%20J%C3%BCrgen%20Schmidhuber%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20Most%20current%20LLM-based%20models%20for%20video%20understanding%20can%20process%20videos%0Awithin%20minutes.%20However%2C%20they%20struggle%20with%20lengthy%20videos%20due%20to%20challenges%0Asuch%20as%20%22noise%20and%20redundancy%22%2C%20as%20well%20as%20%22memory%20and%20computation%22%0Aconstraints.%20In%20this%20paper%2C%20we%20present%20Goldfish%2C%20a%20methodology%20tailored%20for%0Acomprehending%20videos%20of%20arbitrary%20lengths.%20We%20also%20introduce%20the%20TVQA-long%0Abenchmark%2C%20specifically%20designed%20to%20evaluate%20models%27%20capabilities%20in%0Aunderstanding%20long%20videos%20with%20questions%20in%20both%20vision%20and%20text%20content.%0AGoldfish%20approaches%20these%20challenges%20with%20an%20efficient%20retrieval%20mechanism%20that%0Ainitially%20gathers%20the%20top-k%20video%20clips%20relevant%20to%20the%20instruction%20before%0Aproceeding%20to%20provide%20the%20desired%20response.%20This%20design%20of%20the%20retrieval%0Amechanism%20enables%20the%20Goldfish%20to%20efficiently%20process%20arbitrarily%20long%20video%0Asequences%2C%20facilitating%20its%20application%20in%20contexts%20such%20as%20movies%20or%0Atelevision%20series.%20To%20facilitate%20the%20retrieval%20process%2C%20we%20developed%0AMiniGPT4-Video%20that%20generates%20detailed%20descriptions%20for%20the%20video%20clips.%20In%0Aaddressing%20the%20scarcity%20of%20benchmarks%20for%20long%20video%20evaluation%2C%20we%20adapted%20the%0ATVQA%20short%20video%20benchmark%20for%20extended%20content%20analysis%20by%20aggregating%0Aquestions%20from%20entire%20episodes%2C%20thereby%20shifting%20the%20evaluation%20from%20partial%20to%0Afull%20episode%20comprehension.%20We%20attained%20a%2041.78%25%20accuracy%20rate%20on%20the%20TVQA-long%0Abenchmark%2C%20surpassing%20previous%20methods%20by%2014.94%25.%20Our%20MiniGPT4-Video%20also%20shows%0Aexceptional%20performance%20in%20short%20video%20comprehension%2C%20exceeding%20existing%0Astate-of-the-art%20methods%20by%203.23%25%2C%202.03%25%2C%2016.5%25%20and%2023.59%25%20on%20the%20MSVD%2C%20MSRVTT%2C%0ATGIF%2C%20and%20TVQA%20short%20video%20benchmarks%2C%20respectively.%20These%20results%20indicate%0Athat%20our%20models%20have%20significant%20improvements%20in%20both%20long%20and%20short-video%0Aunderstanding.%20Our%20models%20and%20code%20have%20been%20made%20publicly%20available%20at%0Ahttps%3A//vision-cair.github.io/Goldfish_website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoldfish%253A%2520Vision-Language%2520Understanding%2520of%2520Arbitrarily%2520Long%2520Videos%26entry.906535625%3DKirolos%2520Ataallah%2520and%2520Xiaoqian%2520Shen%2520and%2520Eslam%2520Abdelrahman%2520and%2520Essam%2520Sleiman%2520and%2520Mingchen%2520Zhuge%2520and%2520Jian%2520Ding%2520and%2520Deyao%2520Zhu%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520Most%2520current%2520LLM-based%2520models%2520for%2520video%2520understanding%2520can%2520process%2520videos%250Awithin%2520minutes.%2520However%252C%2520they%2520struggle%2520with%2520lengthy%2520videos%2520due%2520to%2520challenges%250Asuch%2520as%2520%2522noise%2520and%2520redundancy%2522%252C%2520as%2520well%2520as%2520%2522memory%2520and%2520computation%2522%250Aconstraints.%2520In%2520this%2520paper%252C%2520we%2520present%2520Goldfish%252C%2520a%2520methodology%2520tailored%2520for%250Acomprehending%2520videos%2520of%2520arbitrary%2520lengths.%2520We%2520also%2520introduce%2520the%2520TVQA-long%250Abenchmark%252C%2520specifically%2520designed%2520to%2520evaluate%2520models%2527%2520capabilities%2520in%250Aunderstanding%2520long%2520videos%2520with%2520questions%2520in%2520both%2520vision%2520and%2520text%2520content.%250AGoldfish%2520approaches%2520these%2520challenges%2520with%2520an%2520efficient%2520retrieval%2520mechanism%2520that%250Ainitially%2520gathers%2520the%2520top-k%2520video%2520clips%2520relevant%2520to%2520the%2520instruction%2520before%250Aproceeding%2520to%2520provide%2520the%2520desired%2520response.%2520This%2520design%2520of%2520the%2520retrieval%250Amechanism%2520enables%2520the%2520Goldfish%2520to%2520efficiently%2520process%2520arbitrarily%2520long%2520video%250Asequences%252C%2520facilitating%2520its%2520application%2520in%2520contexts%2520such%2520as%2520movies%2520or%250Atelevision%2520series.%2520To%2520facilitate%2520the%2520retrieval%2520process%252C%2520we%2520developed%250AMiniGPT4-Video%2520that%2520generates%2520detailed%2520descriptions%2520for%2520the%2520video%2520clips.%2520In%250Aaddressing%2520the%2520scarcity%2520of%2520benchmarks%2520for%2520long%2520video%2520evaluation%252C%2520we%2520adapted%2520the%250ATVQA%2520short%2520video%2520benchmark%2520for%2520extended%2520content%2520analysis%2520by%2520aggregating%250Aquestions%2520from%2520entire%2520episodes%252C%2520thereby%2520shifting%2520the%2520evaluation%2520from%2520partial%2520to%250Afull%2520episode%2520comprehension.%2520We%2520attained%2520a%252041.78%2525%2520accuracy%2520rate%2520on%2520the%2520TVQA-long%250Abenchmark%252C%2520surpassing%2520previous%2520methods%2520by%252014.94%2525.%2520Our%2520MiniGPT4-Video%2520also%2520shows%250Aexceptional%2520performance%2520in%2520short%2520video%2520comprehension%252C%2520exceeding%2520existing%250Astate-of-the-art%2520methods%2520by%25203.23%2525%252C%25202.03%2525%252C%252016.5%2525%2520and%252023.59%2525%2520on%2520the%2520MSVD%252C%2520MSRVTT%252C%250ATGIF%252C%2520and%2520TVQA%2520short%2520video%2520benchmarks%252C%2520respectively.%2520These%2520results%2520indicate%250Athat%2520our%2520models%2520have%2520significant%2520improvements%2520in%2520both%2520long%2520and%2520short-video%250Aunderstanding.%2520Our%2520models%2520and%2520code%2520have%2520been%2520made%2520publicly%2520available%2520at%250Ahttps%253A//vision-cair.github.io/Goldfish_website/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goldfish%3A%20Vision-Language%20Understanding%20of%20Arbitrarily%20Long%20Videos&entry.906535625=Kirolos%20Ataallah%20and%20Xiaoqian%20Shen%20and%20Eslam%20Abdelrahman%20and%20Essam%20Sleiman%20and%20Mingchen%20Zhuge%20and%20Jian%20Ding%20and%20Deyao%20Zhu%20and%20J%C3%BCrgen%20Schmidhuber%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20Most%20current%20LLM-based%20models%20for%20video%20understanding%20can%20process%20videos%0Awithin%20minutes.%20However%2C%20they%20struggle%20with%20lengthy%20videos%20due%20to%20challenges%0Asuch%20as%20%22noise%20and%20redundancy%22%2C%20as%20well%20as%20%22memory%20and%20computation%22%0Aconstraints.%20In%20this%20paper%2C%20we%20present%20Goldfish%2C%20a%20methodology%20tailored%20for%0Acomprehending%20videos%20of%20arbitrary%20lengths.%20We%20also%20introduce%20the%20TVQA-long%0Abenchmark%2C%20specifically%20designed%20to%20evaluate%20models%27%20capabilities%20in%0Aunderstanding%20long%20videos%20with%20questions%20in%20both%20vision%20and%20text%20content.%0AGoldfish%20approaches%20these%20challenges%20with%20an%20efficient%20retrieval%20mechanism%20that%0Ainitially%20gathers%20the%20top-k%20video%20clips%20relevant%20to%20the%20instruction%20before%0Aproceeding%20to%20provide%20the%20desired%20response.%20This%20design%20of%20the%20retrieval%0Amechanism%20enables%20the%20Goldfish%20to%20efficiently%20process%20arbitrarily%20long%20video%0Asequences%2C%20facilitating%20its%20application%20in%20contexts%20such%20as%20movies%20or%0Atelevision%20series.%20To%20facilitate%20the%20retrieval%20process%2C%20we%20developed%0AMiniGPT4-Video%20that%20generates%20detailed%20descriptions%20for%20the%20video%20clips.%20In%0Aaddressing%20the%20scarcity%20of%20benchmarks%20for%20long%20video%20evaluation%2C%20we%20adapted%20the%0ATVQA%20short%20video%20benchmark%20for%20extended%20content%20analysis%20by%20aggregating%0Aquestions%20from%20entire%20episodes%2C%20thereby%20shifting%20the%20evaluation%20from%20partial%20to%0Afull%20episode%20comprehension.%20We%20attained%20a%2041.78%25%20accuracy%20rate%20on%20the%20TVQA-long%0Abenchmark%2C%20surpassing%20previous%20methods%20by%2014.94%25.%20Our%20MiniGPT4-Video%20also%20shows%0Aexceptional%20performance%20in%20short%20video%20comprehension%2C%20exceeding%20existing%0Astate-of-the-art%20methods%20by%203.23%25%2C%202.03%25%2C%2016.5%25%20and%2023.59%25%20on%20the%20MSVD%2C%20MSRVTT%2C%0ATGIF%2C%20and%20TVQA%20short%20video%20benchmarks%2C%20respectively.%20These%20results%20indicate%0Athat%20our%20models%20have%20significant%20improvements%20in%20both%20long%20and%20short-video%0Aunderstanding.%20Our%20models%20and%20code%20have%20been%20made%20publicly%20available%20at%0Ahttps%3A//vision-cair.github.io/Goldfish_website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12679v1&entry.124074799=Read"},
{"title": "Toward INT4 Fixed-Point Training via Exploring Quantization Error for\n  Gradients", "author": "Dohyung Kim and Junghyup Lee and Jeimin Jeon and Jaehyeon Moon and Bumsub Ham", "abstract": "  Network quantization generally converts full-precision weights and/or\nactivations into low-bit fixed-point values in order to accelerate an inference\nprocess. Recent approaches to network quantization further discretize the\ngradients into low-bit fixed-point values, enabling an efficient training. They\ntypically set a quantization interval using a min-max range of the gradients or\nadjust the interval such that the quantization error for entire gradients is\nminimized. In this paper, we analyze the quantization error of gradients for\nthe low-bit fixed-point training, and show that lowering the error for\nlarge-magnitude gradients boosts the quantization performance significantly.\nBased on this, we derive an upper bound of quantization error for the large\ngradients in terms of the quantization interval, and obtain an optimal\ncondition for the interval minimizing the quantization error for large\ngradients. We also introduce an interval update algorithm that adjusts the\nquantization interval adaptively to maintain a small quantization error for\nlarge gradients. Experimental results demonstrate the effectiveness of our\nquantization method for various combinations of network architectures and\nbit-widths on various tasks, including image classification, object detection,\nand super-resolution.\n", "link": "http://arxiv.org/abs/2407.12637v1", "date": "2024-07-17", "relevancy": 2.0889, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5537}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4998}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20INT4%20Fixed-Point%20Training%20via%20Exploring%20Quantization%20Error%20for%0A%20%20Gradients&body=Title%3A%20Toward%20INT4%20Fixed-Point%20Training%20via%20Exploring%20Quantization%20Error%20for%0A%20%20Gradients%0AAuthor%3A%20Dohyung%20Kim%20and%20Junghyup%20Lee%20and%20Jeimin%20Jeon%20and%20Jaehyeon%20Moon%20and%20Bumsub%20Ham%0AAbstract%3A%20%20%20Network%20quantization%20generally%20converts%20full-precision%20weights%20and/or%0Aactivations%20into%20low-bit%20fixed-point%20values%20in%20order%20to%20accelerate%20an%20inference%0Aprocess.%20Recent%20approaches%20to%20network%20quantization%20further%20discretize%20the%0Agradients%20into%20low-bit%20fixed-point%20values%2C%20enabling%20an%20efficient%20training.%20They%0Atypically%20set%20a%20quantization%20interval%20using%20a%20min-max%20range%20of%20the%20gradients%20or%0Aadjust%20the%20interval%20such%20that%20the%20quantization%20error%20for%20entire%20gradients%20is%0Aminimized.%20In%20this%20paper%2C%20we%20analyze%20the%20quantization%20error%20of%20gradients%20for%0Athe%20low-bit%20fixed-point%20training%2C%20and%20show%20that%20lowering%20the%20error%20for%0Alarge-magnitude%20gradients%20boosts%20the%20quantization%20performance%20significantly.%0ABased%20on%20this%2C%20we%20derive%20an%20upper%20bound%20of%20quantization%20error%20for%20the%20large%0Agradients%20in%20terms%20of%20the%20quantization%20interval%2C%20and%20obtain%20an%20optimal%0Acondition%20for%20the%20interval%20minimizing%20the%20quantization%20error%20for%20large%0Agradients.%20We%20also%20introduce%20an%20interval%20update%20algorithm%20that%20adjusts%20the%0Aquantization%20interval%20adaptively%20to%20maintain%20a%20small%20quantization%20error%20for%0Alarge%20gradients.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%0Aquantization%20method%20for%20various%20combinations%20of%20network%20architectures%20and%0Abit-widths%20on%20various%20tasks%2C%20including%20image%20classification%2C%20object%20detection%2C%0Aand%20super-resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520INT4%2520Fixed-Point%2520Training%2520via%2520Exploring%2520Quantization%2520Error%2520for%250A%2520%2520Gradients%26entry.906535625%3DDohyung%2520Kim%2520and%2520Junghyup%2520Lee%2520and%2520Jeimin%2520Jeon%2520and%2520Jaehyeon%2520Moon%2520and%2520Bumsub%2520Ham%26entry.1292438233%3D%2520%2520Network%2520quantization%2520generally%2520converts%2520full-precision%2520weights%2520and/or%250Aactivations%2520into%2520low-bit%2520fixed-point%2520values%2520in%2520order%2520to%2520accelerate%2520an%2520inference%250Aprocess.%2520Recent%2520approaches%2520to%2520network%2520quantization%2520further%2520discretize%2520the%250Agradients%2520into%2520low-bit%2520fixed-point%2520values%252C%2520enabling%2520an%2520efficient%2520training.%2520They%250Atypically%2520set%2520a%2520quantization%2520interval%2520using%2520a%2520min-max%2520range%2520of%2520the%2520gradients%2520or%250Aadjust%2520the%2520interval%2520such%2520that%2520the%2520quantization%2520error%2520for%2520entire%2520gradients%2520is%250Aminimized.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520quantization%2520error%2520of%2520gradients%2520for%250Athe%2520low-bit%2520fixed-point%2520training%252C%2520and%2520show%2520that%2520lowering%2520the%2520error%2520for%250Alarge-magnitude%2520gradients%2520boosts%2520the%2520quantization%2520performance%2520significantly.%250ABased%2520on%2520this%252C%2520we%2520derive%2520an%2520upper%2520bound%2520of%2520quantization%2520error%2520for%2520the%2520large%250Agradients%2520in%2520terms%2520of%2520the%2520quantization%2520interval%252C%2520and%2520obtain%2520an%2520optimal%250Acondition%2520for%2520the%2520interval%2520minimizing%2520the%2520quantization%2520error%2520for%2520large%250Agradients.%2520We%2520also%2520introduce%2520an%2520interval%2520update%2520algorithm%2520that%2520adjusts%2520the%250Aquantization%2520interval%2520adaptively%2520to%2520maintain%2520a%2520small%2520quantization%2520error%2520for%250Alarge%2520gradients.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aquantization%2520method%2520for%2520various%2520combinations%2520of%2520network%2520architectures%2520and%250Abit-widths%2520on%2520various%2520tasks%252C%2520including%2520image%2520classification%252C%2520object%2520detection%252C%250Aand%2520super-resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20INT4%20Fixed-Point%20Training%20via%20Exploring%20Quantization%20Error%20for%0A%20%20Gradients&entry.906535625=Dohyung%20Kim%20and%20Junghyup%20Lee%20and%20Jeimin%20Jeon%20and%20Jaehyeon%20Moon%20and%20Bumsub%20Ham&entry.1292438233=%20%20Network%20quantization%20generally%20converts%20full-precision%20weights%20and/or%0Aactivations%20into%20low-bit%20fixed-point%20values%20in%20order%20to%20accelerate%20an%20inference%0Aprocess.%20Recent%20approaches%20to%20network%20quantization%20further%20discretize%20the%0Agradients%20into%20low-bit%20fixed-point%20values%2C%20enabling%20an%20efficient%20training.%20They%0Atypically%20set%20a%20quantization%20interval%20using%20a%20min-max%20range%20of%20the%20gradients%20or%0Aadjust%20the%20interval%20such%20that%20the%20quantization%20error%20for%20entire%20gradients%20is%0Aminimized.%20In%20this%20paper%2C%20we%20analyze%20the%20quantization%20error%20of%20gradients%20for%0Athe%20low-bit%20fixed-point%20training%2C%20and%20show%20that%20lowering%20the%20error%20for%0Alarge-magnitude%20gradients%20boosts%20the%20quantization%20performance%20significantly.%0ABased%20on%20this%2C%20we%20derive%20an%20upper%20bound%20of%20quantization%20error%20for%20the%20large%0Agradients%20in%20terms%20of%20the%20quantization%20interval%2C%20and%20obtain%20an%20optimal%0Acondition%20for%20the%20interval%20minimizing%20the%20quantization%20error%20for%20large%0Agradients.%20We%20also%20introduce%20an%20interval%20update%20algorithm%20that%20adjusts%20the%0Aquantization%20interval%20adaptively%20to%20maintain%20a%20small%20quantization%20error%20for%0Alarge%20gradients.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%0Aquantization%20method%20for%20various%20combinations%20of%20network%20architectures%20and%0Abit-widths%20on%20various%20tasks%2C%20including%20image%20classification%2C%20object%20detection%2C%0Aand%20super-resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12637v1&entry.124074799=Read"},
{"title": "Efficient Continual Learning with Low Memory Footprint For Edge Device", "author": "Zeqing Wang and Fei Cheng and Kangye Ji and Bohu Huang", "abstract": "  Continual learning(CL) is a useful technique to acquire dynamic knowledge\ncontinually. Although powerful cloud platforms can fully exert the ability of\nCL,e.g., customized recommendation systems, similar personalized requirements\nfor edge devices are almost disregarded. This phenomenon stems from the huge\nresource overhead involved in training neural networks and overcoming the\nforgetting problem of CL. This paper focuses on these scenarios and proposes a\ncompact algorithm called LightCL. Different from other CL methods bringing huge\nresource consumption to acquire generalizability among all tasks for delaying\nforgetting, LightCL compress the resource consumption of already generalized\ncomponents in neural networks and uses a few extra resources to improve memory\nin other parts. We first propose two new metrics of learning plasticity and\nmemory stability to seek generalizability during CL. Based on the discovery\nthat lower and middle layers have more generalizability and deeper layers are\nopposite, we $\\textit{Maintain Generalizability}$ by freezing the lower and\nmiddle layers. Then, we $\\textit{Memorize Feature Patterns}$ to stabilize the\nfeature extracting patterns of previous tasks to improve generalizability in\ndeeper layers. In the experimental comparison, LightCL outperforms other SOTA\nmethods in delaying forgetting and reduces at most $\\textbf{6.16$\\times$}$\nmemory footprint, proving the excellent performance of LightCL in efficiency.\nWe also evaluate the efficiency of our method on an edge device, the Jetson\nNano, which further proves our method's practical effectiveness.\n", "link": "http://arxiv.org/abs/2407.10545v2", "date": "2024-07-17", "relevancy": 2.0748, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5397}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5247}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Continual%20Learning%20with%20Low%20Memory%20Footprint%20For%20Edge%20Device&body=Title%3A%20Efficient%20Continual%20Learning%20with%20Low%20Memory%20Footprint%20For%20Edge%20Device%0AAuthor%3A%20Zeqing%20Wang%20and%20Fei%20Cheng%20and%20Kangye%20Ji%20and%20Bohu%20Huang%0AAbstract%3A%20%20%20Continual%20learning%28CL%29%20is%20a%20useful%20technique%20to%20acquire%20dynamic%20knowledge%0Acontinually.%20Although%20powerful%20cloud%20platforms%20can%20fully%20exert%20the%20ability%20of%0ACL%2Ce.g.%2C%20customized%20recommendation%20systems%2C%20similar%20personalized%20requirements%0Afor%20edge%20devices%20are%20almost%20disregarded.%20This%20phenomenon%20stems%20from%20the%20huge%0Aresource%20overhead%20involved%20in%20training%20neural%20networks%20and%20overcoming%20the%0Aforgetting%20problem%20of%20CL.%20This%20paper%20focuses%20on%20these%20scenarios%20and%20proposes%20a%0Acompact%20algorithm%20called%20LightCL.%20Different%20from%20other%20CL%20methods%20bringing%20huge%0Aresource%20consumption%20to%20acquire%20generalizability%20among%20all%20tasks%20for%20delaying%0Aforgetting%2C%20LightCL%20compress%20the%20resource%20consumption%20of%20already%20generalized%0Acomponents%20in%20neural%20networks%20and%20uses%20a%20few%20extra%20resources%20to%20improve%20memory%0Ain%20other%20parts.%20We%20first%20propose%20two%20new%20metrics%20of%20learning%20plasticity%20and%0Amemory%20stability%20to%20seek%20generalizability%20during%20CL.%20Based%20on%20the%20discovery%0Athat%20lower%20and%20middle%20layers%20have%20more%20generalizability%20and%20deeper%20layers%20are%0Aopposite%2C%20we%20%24%5Ctextit%7BMaintain%20Generalizability%7D%24%20by%20freezing%20the%20lower%20and%0Amiddle%20layers.%20Then%2C%20we%20%24%5Ctextit%7BMemorize%20Feature%20Patterns%7D%24%20to%20stabilize%20the%0Afeature%20extracting%20patterns%20of%20previous%20tasks%20to%20improve%20generalizability%20in%0Adeeper%20layers.%20In%20the%20experimental%20comparison%2C%20LightCL%20outperforms%20other%20SOTA%0Amethods%20in%20delaying%20forgetting%20and%20reduces%20at%20most%20%24%5Ctextbf%7B6.16%24%5Ctimes%24%7D%24%0Amemory%20footprint%2C%20proving%20the%20excellent%20performance%20of%20LightCL%20in%20efficiency.%0AWe%20also%20evaluate%20the%20efficiency%20of%20our%20method%20on%20an%20edge%20device%2C%20the%20Jetson%0ANano%2C%20which%20further%20proves%20our%20method%27s%20practical%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Continual%2520Learning%2520with%2520Low%2520Memory%2520Footprint%2520For%2520Edge%2520Device%26entry.906535625%3DZeqing%2520Wang%2520and%2520Fei%2520Cheng%2520and%2520Kangye%2520Ji%2520and%2520Bohu%2520Huang%26entry.1292438233%3D%2520%2520Continual%2520learning%2528CL%2529%2520is%2520a%2520useful%2520technique%2520to%2520acquire%2520dynamic%2520knowledge%250Acontinually.%2520Although%2520powerful%2520cloud%2520platforms%2520can%2520fully%2520exert%2520the%2520ability%2520of%250ACL%252Ce.g.%252C%2520customized%2520recommendation%2520systems%252C%2520similar%2520personalized%2520requirements%250Afor%2520edge%2520devices%2520are%2520almost%2520disregarded.%2520This%2520phenomenon%2520stems%2520from%2520the%2520huge%250Aresource%2520overhead%2520involved%2520in%2520training%2520neural%2520networks%2520and%2520overcoming%2520the%250Aforgetting%2520problem%2520of%2520CL.%2520This%2520paper%2520focuses%2520on%2520these%2520scenarios%2520and%2520proposes%2520a%250Acompact%2520algorithm%2520called%2520LightCL.%2520Different%2520from%2520other%2520CL%2520methods%2520bringing%2520huge%250Aresource%2520consumption%2520to%2520acquire%2520generalizability%2520among%2520all%2520tasks%2520for%2520delaying%250Aforgetting%252C%2520LightCL%2520compress%2520the%2520resource%2520consumption%2520of%2520already%2520generalized%250Acomponents%2520in%2520neural%2520networks%2520and%2520uses%2520a%2520few%2520extra%2520resources%2520to%2520improve%2520memory%250Ain%2520other%2520parts.%2520We%2520first%2520propose%2520two%2520new%2520metrics%2520of%2520learning%2520plasticity%2520and%250Amemory%2520stability%2520to%2520seek%2520generalizability%2520during%2520CL.%2520Based%2520on%2520the%2520discovery%250Athat%2520lower%2520and%2520middle%2520layers%2520have%2520more%2520generalizability%2520and%2520deeper%2520layers%2520are%250Aopposite%252C%2520we%2520%2524%255Ctextit%257BMaintain%2520Generalizability%257D%2524%2520by%2520freezing%2520the%2520lower%2520and%250Amiddle%2520layers.%2520Then%252C%2520we%2520%2524%255Ctextit%257BMemorize%2520Feature%2520Patterns%257D%2524%2520to%2520stabilize%2520the%250Afeature%2520extracting%2520patterns%2520of%2520previous%2520tasks%2520to%2520improve%2520generalizability%2520in%250Adeeper%2520layers.%2520In%2520the%2520experimental%2520comparison%252C%2520LightCL%2520outperforms%2520other%2520SOTA%250Amethods%2520in%2520delaying%2520forgetting%2520and%2520reduces%2520at%2520most%2520%2524%255Ctextbf%257B6.16%2524%255Ctimes%2524%257D%2524%250Amemory%2520footprint%252C%2520proving%2520the%2520excellent%2520performance%2520of%2520LightCL%2520in%2520efficiency.%250AWe%2520also%2520evaluate%2520the%2520efficiency%2520of%2520our%2520method%2520on%2520an%2520edge%2520device%252C%2520the%2520Jetson%250ANano%252C%2520which%2520further%2520proves%2520our%2520method%2527s%2520practical%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Continual%20Learning%20with%20Low%20Memory%20Footprint%20For%20Edge%20Device&entry.906535625=Zeqing%20Wang%20and%20Fei%20Cheng%20and%20Kangye%20Ji%20and%20Bohu%20Huang&entry.1292438233=%20%20Continual%20learning%28CL%29%20is%20a%20useful%20technique%20to%20acquire%20dynamic%20knowledge%0Acontinually.%20Although%20powerful%20cloud%20platforms%20can%20fully%20exert%20the%20ability%20of%0ACL%2Ce.g.%2C%20customized%20recommendation%20systems%2C%20similar%20personalized%20requirements%0Afor%20edge%20devices%20are%20almost%20disregarded.%20This%20phenomenon%20stems%20from%20the%20huge%0Aresource%20overhead%20involved%20in%20training%20neural%20networks%20and%20overcoming%20the%0Aforgetting%20problem%20of%20CL.%20This%20paper%20focuses%20on%20these%20scenarios%20and%20proposes%20a%0Acompact%20algorithm%20called%20LightCL.%20Different%20from%20other%20CL%20methods%20bringing%20huge%0Aresource%20consumption%20to%20acquire%20generalizability%20among%20all%20tasks%20for%20delaying%0Aforgetting%2C%20LightCL%20compress%20the%20resource%20consumption%20of%20already%20generalized%0Acomponents%20in%20neural%20networks%20and%20uses%20a%20few%20extra%20resources%20to%20improve%20memory%0Ain%20other%20parts.%20We%20first%20propose%20two%20new%20metrics%20of%20learning%20plasticity%20and%0Amemory%20stability%20to%20seek%20generalizability%20during%20CL.%20Based%20on%20the%20discovery%0Athat%20lower%20and%20middle%20layers%20have%20more%20generalizability%20and%20deeper%20layers%20are%0Aopposite%2C%20we%20%24%5Ctextit%7BMaintain%20Generalizability%7D%24%20by%20freezing%20the%20lower%20and%0Amiddle%20layers.%20Then%2C%20we%20%24%5Ctextit%7BMemorize%20Feature%20Patterns%7D%24%20to%20stabilize%20the%0Afeature%20extracting%20patterns%20of%20previous%20tasks%20to%20improve%20generalizability%20in%0Adeeper%20layers.%20In%20the%20experimental%20comparison%2C%20LightCL%20outperforms%20other%20SOTA%0Amethods%20in%20delaying%20forgetting%20and%20reduces%20at%20most%20%24%5Ctextbf%7B6.16%24%5Ctimes%24%7D%24%0Amemory%20footprint%2C%20proving%20the%20excellent%20performance%20of%20LightCL%20in%20efficiency.%0AWe%20also%20evaluate%20the%20efficiency%20of%20our%20method%20on%20an%20edge%20device%2C%20the%20Jetson%0ANano%2C%20which%20further%20proves%20our%20method%27s%20practical%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10545v2&entry.124074799=Read"},
{"title": "CIC-BART-SSA: Controllable Image Captioning with Structured Semantic\n  Augmentation", "author": "Kalliopi Basioti and Mohamed A. Abdelsalam and Federico Fancellu and Vladimir Pavlovic and Afsaneh Fazly", "abstract": "  Controllable Image Captioning (CIC) aims at generating natural language\ndescriptions for an image, conditioned on information provided by end users,\ne.g., regions, entities or events of interest. However, available\nimage-language datasets mainly contain captions that describe the entirety of\nan image, making them ineffective for training CIC models that can potentially\nattend to any subset of regions or relationships. To tackle this challenge, we\npropose a novel, fully automatic method to sample additional focused and\nvisually grounded captions using a unified structured semantic representation\nbuilt on top of the existing set of captions associated with an image. We\nleverage Abstract Meaning Representation (AMR), a cross-lingual graph-based\nsemantic formalism, to encode all possible spatio-semantic relations between\nentities, beyond the typical spatial-relations-only focus of current methods.\nWe use this Structured Semantic Augmentation (SSA) framework to augment\nexisting image-caption datasets with the grounded controlled captions,\nincreasing their spatial and semantic diversity and focal coverage. We then\ndevelop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that\nsources its control signals from SSA-diversified datasets. We empirically show\nthat, compared to SOTA CIC models, CIC-BART-SSA generates captions that are\nsuperior in diversity and text quality, are competitive in controllability,\nand, importantly, minimize the gap between broad and highly focused controlled\ncaptioning performance by efficiently generalizing to the challenging highly\nfocused scenarios. Code is available at\nhttps://github.com/SamsungLabs/CIC-BART-SSA.\n", "link": "http://arxiv.org/abs/2407.11393v2", "date": "2024-07-17", "relevancy": 2.0733, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5209}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5194}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CIC-BART-SSA%3A%20Controllable%20Image%20Captioning%20with%20Structured%20Semantic%0A%20%20Augmentation&body=Title%3A%20CIC-BART-SSA%3A%20Controllable%20Image%20Captioning%20with%20Structured%20Semantic%0A%20%20Augmentation%0AAuthor%3A%20Kalliopi%20Basioti%20and%20Mohamed%20A.%20Abdelsalam%20and%20Federico%20Fancellu%20and%20Vladimir%20Pavlovic%20and%20Afsaneh%20Fazly%0AAbstract%3A%20%20%20Controllable%20Image%20Captioning%20%28CIC%29%20aims%20at%20generating%20natural%20language%0Adescriptions%20for%20an%20image%2C%20conditioned%20on%20information%20provided%20by%20end%20users%2C%0Ae.g.%2C%20regions%2C%20entities%20or%20events%20of%20interest.%20However%2C%20available%0Aimage-language%20datasets%20mainly%20contain%20captions%20that%20describe%20the%20entirety%20of%0Aan%20image%2C%20making%20them%20ineffective%20for%20training%20CIC%20models%20that%20can%20potentially%0Aattend%20to%20any%20subset%20of%20regions%20or%20relationships.%20To%20tackle%20this%20challenge%2C%20we%0Apropose%20a%20novel%2C%20fully%20automatic%20method%20to%20sample%20additional%20focused%20and%0Avisually%20grounded%20captions%20using%20a%20unified%20structured%20semantic%20representation%0Abuilt%20on%20top%20of%20the%20existing%20set%20of%20captions%20associated%20with%20an%20image.%20We%0Aleverage%20Abstract%20Meaning%20Representation%20%28AMR%29%2C%20a%20cross-lingual%20graph-based%0Asemantic%20formalism%2C%20to%20encode%20all%20possible%20spatio-semantic%20relations%20between%0Aentities%2C%20beyond%20the%20typical%20spatial-relations-only%20focus%20of%20current%20methods.%0AWe%20use%20this%20Structured%20Semantic%20Augmentation%20%28SSA%29%20framework%20to%20augment%0Aexisting%20image-caption%20datasets%20with%20the%20grounded%20controlled%20captions%2C%0Aincreasing%20their%20spatial%20and%20semantic%20diversity%20and%20focal%20coverage.%20We%20then%0Adevelop%20a%20new%20model%2C%20CIC-BART-SSA%2C%20specifically%20tailored%20for%20the%20CIC%20task%2C%20that%0Asources%20its%20control%20signals%20from%20SSA-diversified%20datasets.%20We%20empirically%20show%0Athat%2C%20compared%20to%20SOTA%20CIC%20models%2C%20CIC-BART-SSA%20generates%20captions%20that%20are%0Asuperior%20in%20diversity%20and%20text%20quality%2C%20are%20competitive%20in%20controllability%2C%0Aand%2C%20importantly%2C%20minimize%20the%20gap%20between%20broad%20and%20highly%20focused%20controlled%0Acaptioning%20performance%20by%20efficiently%20generalizing%20to%20the%20challenging%20highly%0Afocused%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/SamsungLabs/CIC-BART-SSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCIC-BART-SSA%253A%2520Controllable%2520Image%2520Captioning%2520with%2520Structured%2520Semantic%250A%2520%2520Augmentation%26entry.906535625%3DKalliopi%2520Basioti%2520and%2520Mohamed%2520A.%2520Abdelsalam%2520and%2520Federico%2520Fancellu%2520and%2520Vladimir%2520Pavlovic%2520and%2520Afsaneh%2520Fazly%26entry.1292438233%3D%2520%2520Controllable%2520Image%2520Captioning%2520%2528CIC%2529%2520aims%2520at%2520generating%2520natural%2520language%250Adescriptions%2520for%2520an%2520image%252C%2520conditioned%2520on%2520information%2520provided%2520by%2520end%2520users%252C%250Ae.g.%252C%2520regions%252C%2520entities%2520or%2520events%2520of%2520interest.%2520However%252C%2520available%250Aimage-language%2520datasets%2520mainly%2520contain%2520captions%2520that%2520describe%2520the%2520entirety%2520of%250Aan%2520image%252C%2520making%2520them%2520ineffective%2520for%2520training%2520CIC%2520models%2520that%2520can%2520potentially%250Aattend%2520to%2520any%2520subset%2520of%2520regions%2520or%2520relationships.%2520To%2520tackle%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520novel%252C%2520fully%2520automatic%2520method%2520to%2520sample%2520additional%2520focused%2520and%250Avisually%2520grounded%2520captions%2520using%2520a%2520unified%2520structured%2520semantic%2520representation%250Abuilt%2520on%2520top%2520of%2520the%2520existing%2520set%2520of%2520captions%2520associated%2520with%2520an%2520image.%2520We%250Aleverage%2520Abstract%2520Meaning%2520Representation%2520%2528AMR%2529%252C%2520a%2520cross-lingual%2520graph-based%250Asemantic%2520formalism%252C%2520to%2520encode%2520all%2520possible%2520spatio-semantic%2520relations%2520between%250Aentities%252C%2520beyond%2520the%2520typical%2520spatial-relations-only%2520focus%2520of%2520current%2520methods.%250AWe%2520use%2520this%2520Structured%2520Semantic%2520Augmentation%2520%2528SSA%2529%2520framework%2520to%2520augment%250Aexisting%2520image-caption%2520datasets%2520with%2520the%2520grounded%2520controlled%2520captions%252C%250Aincreasing%2520their%2520spatial%2520and%2520semantic%2520diversity%2520and%2520focal%2520coverage.%2520We%2520then%250Adevelop%2520a%2520new%2520model%252C%2520CIC-BART-SSA%252C%2520specifically%2520tailored%2520for%2520the%2520CIC%2520task%252C%2520that%250Asources%2520its%2520control%2520signals%2520from%2520SSA-diversified%2520datasets.%2520We%2520empirically%2520show%250Athat%252C%2520compared%2520to%2520SOTA%2520CIC%2520models%252C%2520CIC-BART-SSA%2520generates%2520captions%2520that%2520are%250Asuperior%2520in%2520diversity%2520and%2520text%2520quality%252C%2520are%2520competitive%2520in%2520controllability%252C%250Aand%252C%2520importantly%252C%2520minimize%2520the%2520gap%2520between%2520broad%2520and%2520highly%2520focused%2520controlled%250Acaptioning%2520performance%2520by%2520efficiently%2520generalizing%2520to%2520the%2520challenging%2520highly%250Afocused%2520scenarios.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/SamsungLabs/CIC-BART-SSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CIC-BART-SSA%3A%20Controllable%20Image%20Captioning%20with%20Structured%20Semantic%0A%20%20Augmentation&entry.906535625=Kalliopi%20Basioti%20and%20Mohamed%20A.%20Abdelsalam%20and%20Federico%20Fancellu%20and%20Vladimir%20Pavlovic%20and%20Afsaneh%20Fazly&entry.1292438233=%20%20Controllable%20Image%20Captioning%20%28CIC%29%20aims%20at%20generating%20natural%20language%0Adescriptions%20for%20an%20image%2C%20conditioned%20on%20information%20provided%20by%20end%20users%2C%0Ae.g.%2C%20regions%2C%20entities%20or%20events%20of%20interest.%20However%2C%20available%0Aimage-language%20datasets%20mainly%20contain%20captions%20that%20describe%20the%20entirety%20of%0Aan%20image%2C%20making%20them%20ineffective%20for%20training%20CIC%20models%20that%20can%20potentially%0Aattend%20to%20any%20subset%20of%20regions%20or%20relationships.%20To%20tackle%20this%20challenge%2C%20we%0Apropose%20a%20novel%2C%20fully%20automatic%20method%20to%20sample%20additional%20focused%20and%0Avisually%20grounded%20captions%20using%20a%20unified%20structured%20semantic%20representation%0Abuilt%20on%20top%20of%20the%20existing%20set%20of%20captions%20associated%20with%20an%20image.%20We%0Aleverage%20Abstract%20Meaning%20Representation%20%28AMR%29%2C%20a%20cross-lingual%20graph-based%0Asemantic%20formalism%2C%20to%20encode%20all%20possible%20spatio-semantic%20relations%20between%0Aentities%2C%20beyond%20the%20typical%20spatial-relations-only%20focus%20of%20current%20methods.%0AWe%20use%20this%20Structured%20Semantic%20Augmentation%20%28SSA%29%20framework%20to%20augment%0Aexisting%20image-caption%20datasets%20with%20the%20grounded%20controlled%20captions%2C%0Aincreasing%20their%20spatial%20and%20semantic%20diversity%20and%20focal%20coverage.%20We%20then%0Adevelop%20a%20new%20model%2C%20CIC-BART-SSA%2C%20specifically%20tailored%20for%20the%20CIC%20task%2C%20that%0Asources%20its%20control%20signals%20from%20SSA-diversified%20datasets.%20We%20empirically%20show%0Athat%2C%20compared%20to%20SOTA%20CIC%20models%2C%20CIC-BART-SSA%20generates%20captions%20that%20are%0Asuperior%20in%20diversity%20and%20text%20quality%2C%20are%20competitive%20in%20controllability%2C%0Aand%2C%20importantly%2C%20minimize%20the%20gap%20between%20broad%20and%20highly%20focused%20controlled%0Acaptioning%20performance%20by%20efficiently%20generalizing%20to%20the%20challenging%20highly%0Afocused%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/SamsungLabs/CIC-BART-SSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11393v2&entry.124074799=Read"},
{"title": "EvSign: Sign Language Recognition and Translation with Streaming Events", "author": "Pengyu Zhang and Hao Yin and Zeren Wang and Wenyue Chen and Shengming Li and Dong Wang and Huchuan Lu and and Xu Jia", "abstract": "  Sign language is one of the most effective communication tools for people\nwith hearing difficulties. Most existing works focus on improving the\nperformance of sign language tasks on RGB videos, which may suffer from\ndegraded recording conditions, such as fast movement of hands with motion blur\nand textured signer's appearance. The bio-inspired event camera, which\nasynchronously captures brightness change with high speed, could naturally\nperceive dynamic hand movements, providing rich manual clues for sign language\ntasks. In this work, we aim at exploring the potential of event camera in\ncontinuous sign language recognition (CSLR) and sign language translation\n(SLT). To promote the research, we first collect an event-based benchmark\nEvSign for those tasks with both gloss and spoken language annotations. EvSign\ndataset offers a substantial amount of high-quality event streams and an\nextensive vocabulary of glosses and words, thereby facilitating the development\nof sign language tasks. In addition, we propose an efficient transformer-based\nframework for event-based SLR and SLT tasks, which fully leverages the\nadvantages of streaming events. The sparse backbone is employed to extract\nvisual features from sparse events. Then, the temporal coherence is effectively\nutilized through the proposed local token fusion and gloss-aware temporal\naggregation modules. Extensive experimental results are reported on both\nsimulated (PHOENIX14T) and EvSign datasets. Our method performs favorably\nagainst existing state-of-the-art approaches with only 0.34% computational cost\n(0.84G FLOPS per video) and 44.2% network parameters. The project is available\nat https://zhang-pengyu.github.io/EVSign.\n", "link": "http://arxiv.org/abs/2407.12593v1", "date": "2024-07-17", "relevancy": 2.0704, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5309}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvSign%3A%20Sign%20Language%20Recognition%20and%20Translation%20with%20Streaming%20Events&body=Title%3A%20EvSign%3A%20Sign%20Language%20Recognition%20and%20Translation%20with%20Streaming%20Events%0AAuthor%3A%20Pengyu%20Zhang%20and%20Hao%20Yin%20and%20Zeren%20Wang%20and%20Wenyue%20Chen%20and%20Shengming%20Li%20and%20Dong%20Wang%20and%20Huchuan%20Lu%20and%20and%20Xu%20Jia%0AAbstract%3A%20%20%20Sign%20language%20is%20one%20of%20the%20most%20effective%20communication%20tools%20for%20people%0Awith%20hearing%20difficulties.%20Most%20existing%20works%20focus%20on%20improving%20the%0Aperformance%20of%20sign%20language%20tasks%20on%20RGB%20videos%2C%20which%20may%20suffer%20from%0Adegraded%20recording%20conditions%2C%20such%20as%20fast%20movement%20of%20hands%20with%20motion%20blur%0Aand%20textured%20signer%27s%20appearance.%20The%20bio-inspired%20event%20camera%2C%20which%0Aasynchronously%20captures%20brightness%20change%20with%20high%20speed%2C%20could%20naturally%0Aperceive%20dynamic%20hand%20movements%2C%20providing%20rich%20manual%20clues%20for%20sign%20language%0Atasks.%20In%20this%20work%2C%20we%20aim%20at%20exploring%20the%20potential%20of%20event%20camera%20in%0Acontinuous%20sign%20language%20recognition%20%28CSLR%29%20and%20sign%20language%20translation%0A%28SLT%29.%20To%20promote%20the%20research%2C%20we%20first%20collect%20an%20event-based%20benchmark%0AEvSign%20for%20those%20tasks%20with%20both%20gloss%20and%20spoken%20language%20annotations.%20EvSign%0Adataset%20offers%20a%20substantial%20amount%20of%20high-quality%20event%20streams%20and%20an%0Aextensive%20vocabulary%20of%20glosses%20and%20words%2C%20thereby%20facilitating%20the%20development%0Aof%20sign%20language%20tasks.%20In%20addition%2C%20we%20propose%20an%20efficient%20transformer-based%0Aframework%20for%20event-based%20SLR%20and%20SLT%20tasks%2C%20which%20fully%20leverages%20the%0Aadvantages%20of%20streaming%20events.%20The%20sparse%20backbone%20is%20employed%20to%20extract%0Avisual%20features%20from%20sparse%20events.%20Then%2C%20the%20temporal%20coherence%20is%20effectively%0Autilized%20through%20the%20proposed%20local%20token%20fusion%20and%20gloss-aware%20temporal%0Aaggregation%20modules.%20Extensive%20experimental%20results%20are%20reported%20on%20both%0Asimulated%20%28PHOENIX14T%29%20and%20EvSign%20datasets.%20Our%20method%20performs%20favorably%0Aagainst%20existing%20state-of-the-art%20approaches%20with%20only%200.34%25%20computational%20cost%0A%280.84G%20FLOPS%20per%20video%29%20and%2044.2%25%20network%20parameters.%20The%20project%20is%20available%0Aat%20https%3A//zhang-pengyu.github.io/EVSign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvSign%253A%2520Sign%2520Language%2520Recognition%2520and%2520Translation%2520with%2520Streaming%2520Events%26entry.906535625%3DPengyu%2520Zhang%2520and%2520Hao%2520Yin%2520and%2520Zeren%2520Wang%2520and%2520Wenyue%2520Chen%2520and%2520Shengming%2520Li%2520and%2520Dong%2520Wang%2520and%2520Huchuan%2520Lu%2520and%2520and%2520Xu%2520Jia%26entry.1292438233%3D%2520%2520Sign%2520language%2520is%2520one%2520of%2520the%2520most%2520effective%2520communication%2520tools%2520for%2520people%250Awith%2520hearing%2520difficulties.%2520Most%2520existing%2520works%2520focus%2520on%2520improving%2520the%250Aperformance%2520of%2520sign%2520language%2520tasks%2520on%2520RGB%2520videos%252C%2520which%2520may%2520suffer%2520from%250Adegraded%2520recording%2520conditions%252C%2520such%2520as%2520fast%2520movement%2520of%2520hands%2520with%2520motion%2520blur%250Aand%2520textured%2520signer%2527s%2520appearance.%2520The%2520bio-inspired%2520event%2520camera%252C%2520which%250Aasynchronously%2520captures%2520brightness%2520change%2520with%2520high%2520speed%252C%2520could%2520naturally%250Aperceive%2520dynamic%2520hand%2520movements%252C%2520providing%2520rich%2520manual%2520clues%2520for%2520sign%2520language%250Atasks.%2520In%2520this%2520work%252C%2520we%2520aim%2520at%2520exploring%2520the%2520potential%2520of%2520event%2520camera%2520in%250Acontinuous%2520sign%2520language%2520recognition%2520%2528CSLR%2529%2520and%2520sign%2520language%2520translation%250A%2528SLT%2529.%2520To%2520promote%2520the%2520research%252C%2520we%2520first%2520collect%2520an%2520event-based%2520benchmark%250AEvSign%2520for%2520those%2520tasks%2520with%2520both%2520gloss%2520and%2520spoken%2520language%2520annotations.%2520EvSign%250Adataset%2520offers%2520a%2520substantial%2520amount%2520of%2520high-quality%2520event%2520streams%2520and%2520an%250Aextensive%2520vocabulary%2520of%2520glosses%2520and%2520words%252C%2520thereby%2520facilitating%2520the%2520development%250Aof%2520sign%2520language%2520tasks.%2520In%2520addition%252C%2520we%2520propose%2520an%2520efficient%2520transformer-based%250Aframework%2520for%2520event-based%2520SLR%2520and%2520SLT%2520tasks%252C%2520which%2520fully%2520leverages%2520the%250Aadvantages%2520of%2520streaming%2520events.%2520The%2520sparse%2520backbone%2520is%2520employed%2520to%2520extract%250Avisual%2520features%2520from%2520sparse%2520events.%2520Then%252C%2520the%2520temporal%2520coherence%2520is%2520effectively%250Autilized%2520through%2520the%2520proposed%2520local%2520token%2520fusion%2520and%2520gloss-aware%2520temporal%250Aaggregation%2520modules.%2520Extensive%2520experimental%2520results%2520are%2520reported%2520on%2520both%250Asimulated%2520%2528PHOENIX14T%2529%2520and%2520EvSign%2520datasets.%2520Our%2520method%2520performs%2520favorably%250Aagainst%2520existing%2520state-of-the-art%2520approaches%2520with%2520only%25200.34%2525%2520computational%2520cost%250A%25280.84G%2520FLOPS%2520per%2520video%2529%2520and%252044.2%2525%2520network%2520parameters.%2520The%2520project%2520is%2520available%250Aat%2520https%253A//zhang-pengyu.github.io/EVSign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvSign%3A%20Sign%20Language%20Recognition%20and%20Translation%20with%20Streaming%20Events&entry.906535625=Pengyu%20Zhang%20and%20Hao%20Yin%20and%20Zeren%20Wang%20and%20Wenyue%20Chen%20and%20Shengming%20Li%20and%20Dong%20Wang%20and%20Huchuan%20Lu%20and%20and%20Xu%20Jia&entry.1292438233=%20%20Sign%20language%20is%20one%20of%20the%20most%20effective%20communication%20tools%20for%20people%0Awith%20hearing%20difficulties.%20Most%20existing%20works%20focus%20on%20improving%20the%0Aperformance%20of%20sign%20language%20tasks%20on%20RGB%20videos%2C%20which%20may%20suffer%20from%0Adegraded%20recording%20conditions%2C%20such%20as%20fast%20movement%20of%20hands%20with%20motion%20blur%0Aand%20textured%20signer%27s%20appearance.%20The%20bio-inspired%20event%20camera%2C%20which%0Aasynchronously%20captures%20brightness%20change%20with%20high%20speed%2C%20could%20naturally%0Aperceive%20dynamic%20hand%20movements%2C%20providing%20rich%20manual%20clues%20for%20sign%20language%0Atasks.%20In%20this%20work%2C%20we%20aim%20at%20exploring%20the%20potential%20of%20event%20camera%20in%0Acontinuous%20sign%20language%20recognition%20%28CSLR%29%20and%20sign%20language%20translation%0A%28SLT%29.%20To%20promote%20the%20research%2C%20we%20first%20collect%20an%20event-based%20benchmark%0AEvSign%20for%20those%20tasks%20with%20both%20gloss%20and%20spoken%20language%20annotations.%20EvSign%0Adataset%20offers%20a%20substantial%20amount%20of%20high-quality%20event%20streams%20and%20an%0Aextensive%20vocabulary%20of%20glosses%20and%20words%2C%20thereby%20facilitating%20the%20development%0Aof%20sign%20language%20tasks.%20In%20addition%2C%20we%20propose%20an%20efficient%20transformer-based%0Aframework%20for%20event-based%20SLR%20and%20SLT%20tasks%2C%20which%20fully%20leverages%20the%0Aadvantages%20of%20streaming%20events.%20The%20sparse%20backbone%20is%20employed%20to%20extract%0Avisual%20features%20from%20sparse%20events.%20Then%2C%20the%20temporal%20coherence%20is%20effectively%0Autilized%20through%20the%20proposed%20local%20token%20fusion%20and%20gloss-aware%20temporal%0Aaggregation%20modules.%20Extensive%20experimental%20results%20are%20reported%20on%20both%0Asimulated%20%28PHOENIX14T%29%20and%20EvSign%20datasets.%20Our%20method%20performs%20favorably%0Aagainst%20existing%20state-of-the-art%20approaches%20with%20only%200.34%25%20computational%20cost%0A%280.84G%20FLOPS%20per%20video%29%20and%2044.2%25%20network%20parameters.%20The%20project%20is%20available%0Aat%20https%3A//zhang-pengyu.github.io/EVSign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12593v1&entry.124074799=Read"},
{"title": "Struct-X: Enhancing Large Language Models Reasoning with Structured Data", "author": "Xiaoyu Tan and Haoyu Wang and Xihe Qiu and Yuan Cheng and Yinghui Xu and Wei Chu and Yuan Qi", "abstract": "  Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext.\n", "link": "http://arxiv.org/abs/2407.12522v1", "date": "2024-07-17", "relevancy": 2.0682, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5258}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5114}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Struct-X%3A%20Enhancing%20Large%20Language%20Models%20Reasoning%20with%20Structured%20Data&body=Title%3A%20Struct-X%3A%20Enhancing%20Large%20Language%20Models%20Reasoning%20with%20Structured%20Data%0AAuthor%3A%20Xiaoyu%20Tan%20and%20Haoyu%20Wang%20and%20Xihe%20Qiu%20and%20Yuan%20Cheng%20and%20Yinghui%20Xu%20and%20Wei%20Chu%20and%20Yuan%20Qi%0AAbstract%3A%20%20%20Structured%20data%2C%20rich%20in%20logical%20and%20relational%20information%2C%20has%20the%0Apotential%20to%20enhance%20the%20reasoning%20abilities%20of%20large%20language%20models%20%28LLMs%29.%0AStill%2C%20its%20integration%20poses%20a%20challenge%20due%20to%20the%20risk%20of%20overwhelming%20LLMs%0Awith%20excessive%20tokens%20and%20irrelevant%20context%20information.%20To%20address%20this%2C%20we%0Apropose%20Struct-X%2C%20a%20novel%20framework%20that%20operates%20through%20five%20key%20phases%3A%0A%60%60read-model-fill-reflect-reason%27%27%20efficiently%20enabling%20LLMs%20to%20utilize%0Astructured%20data.%20It%20begins%20by%20encoding%20structured%20data%20into%20a%20topological%20space%0Ausing%20graph%20embeddings%2C%20followed%20by%20filling%20in%20missing%20entity%20information%20with%0Aknowledge%20retrieval%20modules%2C%20and%20filtering%20out%20irrelevant%20tokens%20via%20a%0Aself-supervised%20module.%20The%20final%20phase%20involves%20constructing%20a%20topological%0Anetwork%20with%20selected%20tokens%20to%20further%20reduce%20the%20total%20token%20length%20for%20more%0Aeffective%20LLM%20inference.%20Additionally%2C%20Struct-X%20includes%20an%20Auxiliary%20Module%0Atrained%20to%20generate%20prompts%2C%20aiding%20LLMs%20in%20analyzing%20structured%20data.%0AExtensive%20experiments%20on%20benchmarks%2C%20including%20the%20knowledge%20graph%0Aquestion-answer%20task%20and%20the%20long%20document%20reading%20comprehension%20task%2C%20show%0Athat%20Struct-X%20notably%20improves%20LLM%20reasoning%2C%20demonstrating%20the%20effectiveness%0Aof%20structured%20data%20augmentation%20in%20improving%20LLM%20inference%20with%20complex%20input%0Acontext.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStruct-X%253A%2520Enhancing%2520Large%2520Language%2520Models%2520Reasoning%2520with%2520Structured%2520Data%26entry.906535625%3DXiaoyu%2520Tan%2520and%2520Haoyu%2520Wang%2520and%2520Xihe%2520Qiu%2520and%2520Yuan%2520Cheng%2520and%2520Yinghui%2520Xu%2520and%2520Wei%2520Chu%2520and%2520Yuan%2520Qi%26entry.1292438233%3D%2520%2520Structured%2520data%252C%2520rich%2520in%2520logical%2520and%2520relational%2520information%252C%2520has%2520the%250Apotential%2520to%2520enhance%2520the%2520reasoning%2520abilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%250AStill%252C%2520its%2520integration%2520poses%2520a%2520challenge%2520due%2520to%2520the%2520risk%2520of%2520overwhelming%2520LLMs%250Awith%2520excessive%2520tokens%2520and%2520irrelevant%2520context%2520information.%2520To%2520address%2520this%252C%2520we%250Apropose%2520Struct-X%252C%2520a%2520novel%2520framework%2520that%2520operates%2520through%2520five%2520key%2520phases%253A%250A%2560%2560read-model-fill-reflect-reason%2527%2527%2520efficiently%2520enabling%2520LLMs%2520to%2520utilize%250Astructured%2520data.%2520It%2520begins%2520by%2520encoding%2520structured%2520data%2520into%2520a%2520topological%2520space%250Ausing%2520graph%2520embeddings%252C%2520followed%2520by%2520filling%2520in%2520missing%2520entity%2520information%2520with%250Aknowledge%2520retrieval%2520modules%252C%2520and%2520filtering%2520out%2520irrelevant%2520tokens%2520via%2520a%250Aself-supervised%2520module.%2520The%2520final%2520phase%2520involves%2520constructing%2520a%2520topological%250Anetwork%2520with%2520selected%2520tokens%2520to%2520further%2520reduce%2520the%2520total%2520token%2520length%2520for%2520more%250Aeffective%2520LLM%2520inference.%2520Additionally%252C%2520Struct-X%2520includes%2520an%2520Auxiliary%2520Module%250Atrained%2520to%2520generate%2520prompts%252C%2520aiding%2520LLMs%2520in%2520analyzing%2520structured%2520data.%250AExtensive%2520experiments%2520on%2520benchmarks%252C%2520including%2520the%2520knowledge%2520graph%250Aquestion-answer%2520task%2520and%2520the%2520long%2520document%2520reading%2520comprehension%2520task%252C%2520show%250Athat%2520Struct-X%2520notably%2520improves%2520LLM%2520reasoning%252C%2520demonstrating%2520the%2520effectiveness%250Aof%2520structured%2520data%2520augmentation%2520in%2520improving%2520LLM%2520inference%2520with%2520complex%2520input%250Acontext.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Struct-X%3A%20Enhancing%20Large%20Language%20Models%20Reasoning%20with%20Structured%20Data&entry.906535625=Xiaoyu%20Tan%20and%20Haoyu%20Wang%20and%20Xihe%20Qiu%20and%20Yuan%20Cheng%20and%20Yinghui%20Xu%20and%20Wei%20Chu%20and%20Yuan%20Qi&entry.1292438233=%20%20Structured%20data%2C%20rich%20in%20logical%20and%20relational%20information%2C%20has%20the%0Apotential%20to%20enhance%20the%20reasoning%20abilities%20of%20large%20language%20models%20%28LLMs%29.%0AStill%2C%20its%20integration%20poses%20a%20challenge%20due%20to%20the%20risk%20of%20overwhelming%20LLMs%0Awith%20excessive%20tokens%20and%20irrelevant%20context%20information.%20To%20address%20this%2C%20we%0Apropose%20Struct-X%2C%20a%20novel%20framework%20that%20operates%20through%20five%20key%20phases%3A%0A%60%60read-model-fill-reflect-reason%27%27%20efficiently%20enabling%20LLMs%20to%20utilize%0Astructured%20data.%20It%20begins%20by%20encoding%20structured%20data%20into%20a%20topological%20space%0Ausing%20graph%20embeddings%2C%20followed%20by%20filling%20in%20missing%20entity%20information%20with%0Aknowledge%20retrieval%20modules%2C%20and%20filtering%20out%20irrelevant%20tokens%20via%20a%0Aself-supervised%20module.%20The%20final%20phase%20involves%20constructing%20a%20topological%0Anetwork%20with%20selected%20tokens%20to%20further%20reduce%20the%20total%20token%20length%20for%20more%0Aeffective%20LLM%20inference.%20Additionally%2C%20Struct-X%20includes%20an%20Auxiliary%20Module%0Atrained%20to%20generate%20prompts%2C%20aiding%20LLMs%20in%20analyzing%20structured%20data.%0AExtensive%20experiments%20on%20benchmarks%2C%20including%20the%20knowledge%20graph%0Aquestion-answer%20task%20and%20the%20long%20document%20reading%20comprehension%20task%2C%20show%0Athat%20Struct-X%20notably%20improves%20LLM%20reasoning%2C%20demonstrating%20the%20effectiveness%0Aof%20structured%20data%20augmentation%20in%20improving%20LLM%20inference%20with%20complex%20input%0Acontext.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12522v1&entry.124074799=Read"},
{"title": "Generative AI for Low-Carbon Artificial Intelligence of Things with\n  Large Language Models", "author": "Jinbo Wen and Ruichen Zhang and Dusit Niyato and Jiawen Kang and Hongyang Du and Yang Zhang and Zhu Han", "abstract": "  By integrating Artificial Intelligence (AI) with the Internet of Things\n(IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields.\nHowever, AIoT is facing the challenges of energy consumption and carbon\nemissions due to the continuous advancement of mobile technology. Fortunately,\nGenerative AI (GAI) holds immense potential to reduce carbon emissions of AIoT\ndue to its excellent reasoning and generation capabilities. In this article, we\nexplore the potential of GAI for carbon emissions reduction and propose a novel\nGAI-enabled solution for low-carbon AIoT. Specifically, we first study the main\nimpacts that cause carbon emissions in AIoT, and then introduce GAI techniques\nand their relations to carbon emissions. We then explore the application\nprospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon\nemissions of network components. Subsequently, we propose a Large Language\nModel (LLM)-enabled carbon emission optimization framework, in which we design\npluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more\naccurate and reliable optimization problems. Furthermore, we utilize Generative\nDiffusion Models (GDMs) to identify optimal strategies for carbon emission\nreduction. Numerical results demonstrate the effectiveness of the proposed\nframework. Finally, we insightfully provide open research directions for\nlow-carbon AIoT.\n", "link": "http://arxiv.org/abs/2404.18077v2", "date": "2024-07-17", "relevancy": 2.0512, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5461}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4958}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20Low-Carbon%20Artificial%20Intelligence%20of%20Things%20with%0A%20%20Large%20Language%20Models&body=Title%3A%20Generative%20AI%20for%20Low-Carbon%20Artificial%20Intelligence%20of%20Things%20with%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Jinbo%20Wen%20and%20Ruichen%20Zhang%20and%20Dusit%20Niyato%20and%20Jiawen%20Kang%20and%20Hongyang%20Du%20and%20Yang%20Zhang%20and%20Zhu%20Han%0AAbstract%3A%20%20%20By%20integrating%20Artificial%20Intelligence%20%28AI%29%20with%20the%20Internet%20of%20Things%0A%28IoT%29%2C%20Artificial%20Intelligence%20of%20Things%20%28AIoT%29%20has%20revolutionized%20many%20fields.%0AHowever%2C%20AIoT%20is%20facing%20the%20challenges%20of%20energy%20consumption%20and%20carbon%0Aemissions%20due%20to%20the%20continuous%20advancement%20of%20mobile%20technology.%20Fortunately%2C%0AGenerative%20AI%20%28GAI%29%20holds%20immense%20potential%20to%20reduce%20carbon%20emissions%20of%20AIoT%0Adue%20to%20its%20excellent%20reasoning%20and%20generation%20capabilities.%20In%20this%20article%2C%20we%0Aexplore%20the%20potential%20of%20GAI%20for%20carbon%20emissions%20reduction%20and%20propose%20a%20novel%0AGAI-enabled%20solution%20for%20low-carbon%20AIoT.%20Specifically%2C%20we%20first%20study%20the%20main%0Aimpacts%20that%20cause%20carbon%20emissions%20in%20AIoT%2C%20and%20then%20introduce%20GAI%20techniques%0Aand%20their%20relations%20to%20carbon%20emissions.%20We%20then%20explore%20the%20application%0Aprospects%20of%20GAI%20in%20low-carbon%20AIoT%2C%20focusing%20on%20how%20GAI%20can%20reduce%20carbon%0Aemissions%20of%20network%20components.%20Subsequently%2C%20we%20propose%20a%20Large%20Language%0AModel%20%28LLM%29-enabled%20carbon%20emission%20optimization%20framework%2C%20in%20which%20we%20design%0Apluggable%20LLM%20and%20Retrieval%20Augmented%20Generation%20%28RAG%29%20modules%20to%20generate%20more%0Aaccurate%20and%20reliable%20optimization%20problems.%20Furthermore%2C%20we%20utilize%20Generative%0ADiffusion%20Models%20%28GDMs%29%20to%20identify%20optimal%20strategies%20for%20carbon%20emission%0Areduction.%20Numerical%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aframework.%20Finally%2C%20we%20insightfully%20provide%20open%20research%20directions%20for%0Alow-carbon%20AIoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520Low-Carbon%2520Artificial%2520Intelligence%2520of%2520Things%2520with%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DJinbo%2520Wen%2520and%2520Ruichen%2520Zhang%2520and%2520Dusit%2520Niyato%2520and%2520Jiawen%2520Kang%2520and%2520Hongyang%2520Du%2520and%2520Yang%2520Zhang%2520and%2520Zhu%2520Han%26entry.1292438233%3D%2520%2520By%2520integrating%2520Artificial%2520Intelligence%2520%2528AI%2529%2520with%2520the%2520Internet%2520of%2520Things%250A%2528IoT%2529%252C%2520Artificial%2520Intelligence%2520of%2520Things%2520%2528AIoT%2529%2520has%2520revolutionized%2520many%2520fields.%250AHowever%252C%2520AIoT%2520is%2520facing%2520the%2520challenges%2520of%2520energy%2520consumption%2520and%2520carbon%250Aemissions%2520due%2520to%2520the%2520continuous%2520advancement%2520of%2520mobile%2520technology.%2520Fortunately%252C%250AGenerative%2520AI%2520%2528GAI%2529%2520holds%2520immense%2520potential%2520to%2520reduce%2520carbon%2520emissions%2520of%2520AIoT%250Adue%2520to%2520its%2520excellent%2520reasoning%2520and%2520generation%2520capabilities.%2520In%2520this%2520article%252C%2520we%250Aexplore%2520the%2520potential%2520of%2520GAI%2520for%2520carbon%2520emissions%2520reduction%2520and%2520propose%2520a%2520novel%250AGAI-enabled%2520solution%2520for%2520low-carbon%2520AIoT.%2520Specifically%252C%2520we%2520first%2520study%2520the%2520main%250Aimpacts%2520that%2520cause%2520carbon%2520emissions%2520in%2520AIoT%252C%2520and%2520then%2520introduce%2520GAI%2520techniques%250Aand%2520their%2520relations%2520to%2520carbon%2520emissions.%2520We%2520then%2520explore%2520the%2520application%250Aprospects%2520of%2520GAI%2520in%2520low-carbon%2520AIoT%252C%2520focusing%2520on%2520how%2520GAI%2520can%2520reduce%2520carbon%250Aemissions%2520of%2520network%2520components.%2520Subsequently%252C%2520we%2520propose%2520a%2520Large%2520Language%250AModel%2520%2528LLM%2529-enabled%2520carbon%2520emission%2520optimization%2520framework%252C%2520in%2520which%2520we%2520design%250Apluggable%2520LLM%2520and%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520modules%2520to%2520generate%2520more%250Aaccurate%2520and%2520reliable%2520optimization%2520problems.%2520Furthermore%252C%2520we%2520utilize%2520Generative%250ADiffusion%2520Models%2520%2528GDMs%2529%2520to%2520identify%2520optimal%2520strategies%2520for%2520carbon%2520emission%250Areduction.%2520Numerical%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aframework.%2520Finally%252C%2520we%2520insightfully%2520provide%2520open%2520research%2520directions%2520for%250Alow-carbon%2520AIoT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20Low-Carbon%20Artificial%20Intelligence%20of%20Things%20with%0A%20%20Large%20Language%20Models&entry.906535625=Jinbo%20Wen%20and%20Ruichen%20Zhang%20and%20Dusit%20Niyato%20and%20Jiawen%20Kang%20and%20Hongyang%20Du%20and%20Yang%20Zhang%20and%20Zhu%20Han&entry.1292438233=%20%20By%20integrating%20Artificial%20Intelligence%20%28AI%29%20with%20the%20Internet%20of%20Things%0A%28IoT%29%2C%20Artificial%20Intelligence%20of%20Things%20%28AIoT%29%20has%20revolutionized%20many%20fields.%0AHowever%2C%20AIoT%20is%20facing%20the%20challenges%20of%20energy%20consumption%20and%20carbon%0Aemissions%20due%20to%20the%20continuous%20advancement%20of%20mobile%20technology.%20Fortunately%2C%0AGenerative%20AI%20%28GAI%29%20holds%20immense%20potential%20to%20reduce%20carbon%20emissions%20of%20AIoT%0Adue%20to%20its%20excellent%20reasoning%20and%20generation%20capabilities.%20In%20this%20article%2C%20we%0Aexplore%20the%20potential%20of%20GAI%20for%20carbon%20emissions%20reduction%20and%20propose%20a%20novel%0AGAI-enabled%20solution%20for%20low-carbon%20AIoT.%20Specifically%2C%20we%20first%20study%20the%20main%0Aimpacts%20that%20cause%20carbon%20emissions%20in%20AIoT%2C%20and%20then%20introduce%20GAI%20techniques%0Aand%20their%20relations%20to%20carbon%20emissions.%20We%20then%20explore%20the%20application%0Aprospects%20of%20GAI%20in%20low-carbon%20AIoT%2C%20focusing%20on%20how%20GAI%20can%20reduce%20carbon%0Aemissions%20of%20network%20components.%20Subsequently%2C%20we%20propose%20a%20Large%20Language%0AModel%20%28LLM%29-enabled%20carbon%20emission%20optimization%20framework%2C%20in%20which%20we%20design%0Apluggable%20LLM%20and%20Retrieval%20Augmented%20Generation%20%28RAG%29%20modules%20to%20generate%20more%0Aaccurate%20and%20reliable%20optimization%20problems.%20Furthermore%2C%20we%20utilize%20Generative%0ADiffusion%20Models%20%28GDMs%29%20to%20identify%20optimal%20strategies%20for%20carbon%20emission%0Areduction.%20Numerical%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aframework.%20Finally%2C%20we%20insightfully%20provide%20open%20research%20directions%20for%0Alow-carbon%20AIoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18077v2&entry.124074799=Read"},
{"title": "VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document\n  Understanding", "author": "Ofir Abramovich and Niv Nayman and Sharon Fogel and Inbal Lavi and Ron Litman and Shahar Tsiper and Royee Tichauer and Srikar Appalaraju and Shai Mazor and R. Manmatha", "abstract": "  In recent years, notable advancements have been made in the domain of visual\ndocument understanding, with the prevailing architecture comprising a cascade\nof vision and language models. The text component can either be extracted\nexplicitly with the use of external OCR models in OCR-based approaches, or\nalternatively, the vision model can be endowed with reading capabilities in\nOCR-free approaches. Typically, the queries to the model are input exclusively\nto the language component, necessitating the visual features to encompass the\nentire document. In this paper, we present VisFocus, an OCR-free method\ndesigned to better exploit the vision encoder's capacity by coupling it\ndirectly with the language prompt. To do so, we replace the down-sampling\nlayers with layers that receive the input prompt and allow highlighting\nrelevant parts of the document, while disregarding others. We pair the\narchitecture enhancements with a novel pre-training task, using language\nmasking on a snippet of the document text fed to the visual encoder in place of\nthe prompt, to empower the model with focusing capabilities. Consequently,\nVisFocus learns to allocate its attention to text patches pertinent to the\nprovided prompt. Our experiments demonstrate that this prompt-guided visual\nencoding approach significantly improves performance, achieving\nstate-of-the-art results on various benchmarks.\n", "link": "http://arxiv.org/abs/2407.12594v1", "date": "2024-07-17", "relevancy": 2.0489, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5186}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5089}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisFocus%3A%20Prompt-Guided%20Vision%20Encoders%20for%20OCR-Free%20Dense%20Document%0A%20%20Understanding&body=Title%3A%20VisFocus%3A%20Prompt-Guided%20Vision%20Encoders%20for%20OCR-Free%20Dense%20Document%0A%20%20Understanding%0AAuthor%3A%20Ofir%20Abramovich%20and%20Niv%20Nayman%20and%20Sharon%20Fogel%20and%20Inbal%20Lavi%20and%20Ron%20Litman%20and%20Shahar%20Tsiper%20and%20Royee%20Tichauer%20and%20Srikar%20Appalaraju%20and%20Shai%20Mazor%20and%20R.%20Manmatha%0AAbstract%3A%20%20%20In%20recent%20years%2C%20notable%20advancements%20have%20been%20made%20in%20the%20domain%20of%20visual%0Adocument%20understanding%2C%20with%20the%20prevailing%20architecture%20comprising%20a%20cascade%0Aof%20vision%20and%20language%20models.%20The%20text%20component%20can%20either%20be%20extracted%0Aexplicitly%20with%20the%20use%20of%20external%20OCR%20models%20in%20OCR-based%20approaches%2C%20or%0Aalternatively%2C%20the%20vision%20model%20can%20be%20endowed%20with%20reading%20capabilities%20in%0AOCR-free%20approaches.%20Typically%2C%20the%20queries%20to%20the%20model%20are%20input%20exclusively%0Ato%20the%20language%20component%2C%20necessitating%20the%20visual%20features%20to%20encompass%20the%0Aentire%20document.%20In%20this%20paper%2C%20we%20present%20VisFocus%2C%20an%20OCR-free%20method%0Adesigned%20to%20better%20exploit%20the%20vision%20encoder%27s%20capacity%20by%20coupling%20it%0Adirectly%20with%20the%20language%20prompt.%20To%20do%20so%2C%20we%20replace%20the%20down-sampling%0Alayers%20with%20layers%20that%20receive%20the%20input%20prompt%20and%20allow%20highlighting%0Arelevant%20parts%20of%20the%20document%2C%20while%20disregarding%20others.%20We%20pair%20the%0Aarchitecture%20enhancements%20with%20a%20novel%20pre-training%20task%2C%20using%20language%0Amasking%20on%20a%20snippet%20of%20the%20document%20text%20fed%20to%20the%20visual%20encoder%20in%20place%20of%0Athe%20prompt%2C%20to%20empower%20the%20model%20with%20focusing%20capabilities.%20Consequently%2C%0AVisFocus%20learns%20to%20allocate%20its%20attention%20to%20text%20patches%20pertinent%20to%20the%0Aprovided%20prompt.%20Our%20experiments%20demonstrate%20that%20this%20prompt-guided%20visual%0Aencoding%20approach%20significantly%20improves%20performance%2C%20achieving%0Astate-of-the-art%20results%20on%20various%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisFocus%253A%2520Prompt-Guided%2520Vision%2520Encoders%2520for%2520OCR-Free%2520Dense%2520Document%250A%2520%2520Understanding%26entry.906535625%3DOfir%2520Abramovich%2520and%2520Niv%2520Nayman%2520and%2520Sharon%2520Fogel%2520and%2520Inbal%2520Lavi%2520and%2520Ron%2520Litman%2520and%2520Shahar%2520Tsiper%2520and%2520Royee%2520Tichauer%2520and%2520Srikar%2520Appalaraju%2520and%2520Shai%2520Mazor%2520and%2520R.%2520Manmatha%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520notable%2520advancements%2520have%2520been%2520made%2520in%2520the%2520domain%2520of%2520visual%250Adocument%2520understanding%252C%2520with%2520the%2520prevailing%2520architecture%2520comprising%2520a%2520cascade%250Aof%2520vision%2520and%2520language%2520models.%2520The%2520text%2520component%2520can%2520either%2520be%2520extracted%250Aexplicitly%2520with%2520the%2520use%2520of%2520external%2520OCR%2520models%2520in%2520OCR-based%2520approaches%252C%2520or%250Aalternatively%252C%2520the%2520vision%2520model%2520can%2520be%2520endowed%2520with%2520reading%2520capabilities%2520in%250AOCR-free%2520approaches.%2520Typically%252C%2520the%2520queries%2520to%2520the%2520model%2520are%2520input%2520exclusively%250Ato%2520the%2520language%2520component%252C%2520necessitating%2520the%2520visual%2520features%2520to%2520encompass%2520the%250Aentire%2520document.%2520In%2520this%2520paper%252C%2520we%2520present%2520VisFocus%252C%2520an%2520OCR-free%2520method%250Adesigned%2520to%2520better%2520exploit%2520the%2520vision%2520encoder%2527s%2520capacity%2520by%2520coupling%2520it%250Adirectly%2520with%2520the%2520language%2520prompt.%2520To%2520do%2520so%252C%2520we%2520replace%2520the%2520down-sampling%250Alayers%2520with%2520layers%2520that%2520receive%2520the%2520input%2520prompt%2520and%2520allow%2520highlighting%250Arelevant%2520parts%2520of%2520the%2520document%252C%2520while%2520disregarding%2520others.%2520We%2520pair%2520the%250Aarchitecture%2520enhancements%2520with%2520a%2520novel%2520pre-training%2520task%252C%2520using%2520language%250Amasking%2520on%2520a%2520snippet%2520of%2520the%2520document%2520text%2520fed%2520to%2520the%2520visual%2520encoder%2520in%2520place%2520of%250Athe%2520prompt%252C%2520to%2520empower%2520the%2520model%2520with%2520focusing%2520capabilities.%2520Consequently%252C%250AVisFocus%2520learns%2520to%2520allocate%2520its%2520attention%2520to%2520text%2520patches%2520pertinent%2520to%2520the%250Aprovided%2520prompt.%2520Our%2520experiments%2520demonstrate%2520that%2520this%2520prompt-guided%2520visual%250Aencoding%2520approach%2520significantly%2520improves%2520performance%252C%2520achieving%250Astate-of-the-art%2520results%2520on%2520various%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisFocus%3A%20Prompt-Guided%20Vision%20Encoders%20for%20OCR-Free%20Dense%20Document%0A%20%20Understanding&entry.906535625=Ofir%20Abramovich%20and%20Niv%20Nayman%20and%20Sharon%20Fogel%20and%20Inbal%20Lavi%20and%20Ron%20Litman%20and%20Shahar%20Tsiper%20and%20Royee%20Tichauer%20and%20Srikar%20Appalaraju%20and%20Shai%20Mazor%20and%20R.%20Manmatha&entry.1292438233=%20%20In%20recent%20years%2C%20notable%20advancements%20have%20been%20made%20in%20the%20domain%20of%20visual%0Adocument%20understanding%2C%20with%20the%20prevailing%20architecture%20comprising%20a%20cascade%0Aof%20vision%20and%20language%20models.%20The%20text%20component%20can%20either%20be%20extracted%0Aexplicitly%20with%20the%20use%20of%20external%20OCR%20models%20in%20OCR-based%20approaches%2C%20or%0Aalternatively%2C%20the%20vision%20model%20can%20be%20endowed%20with%20reading%20capabilities%20in%0AOCR-free%20approaches.%20Typically%2C%20the%20queries%20to%20the%20model%20are%20input%20exclusively%0Ato%20the%20language%20component%2C%20necessitating%20the%20visual%20features%20to%20encompass%20the%0Aentire%20document.%20In%20this%20paper%2C%20we%20present%20VisFocus%2C%20an%20OCR-free%20method%0Adesigned%20to%20better%20exploit%20the%20vision%20encoder%27s%20capacity%20by%20coupling%20it%0Adirectly%20with%20the%20language%20prompt.%20To%20do%20so%2C%20we%20replace%20the%20down-sampling%0Alayers%20with%20layers%20that%20receive%20the%20input%20prompt%20and%20allow%20highlighting%0Arelevant%20parts%20of%20the%20document%2C%20while%20disregarding%20others.%20We%20pair%20the%0Aarchitecture%20enhancements%20with%20a%20novel%20pre-training%20task%2C%20using%20language%0Amasking%20on%20a%20snippet%20of%20the%20document%20text%20fed%20to%20the%20visual%20encoder%20in%20place%20of%0Athe%20prompt%2C%20to%20empower%20the%20model%20with%20focusing%20capabilities.%20Consequently%2C%0AVisFocus%20learns%20to%20allocate%20its%20attention%20to%20text%20patches%20pertinent%20to%20the%0Aprovided%20prompt.%20Our%20experiments%20demonstrate%20that%20this%20prompt-guided%20visual%0Aencoding%20approach%20significantly%20improves%20performance%2C%20achieving%0Astate-of-the-art%20results%20on%20various%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12594v1&entry.124074799=Read"},
{"title": "Learning in Deep Factor Graphs with Gaussian Belief Propagation", "author": "Seth Nabarro and Mark van der Wilk and Andrew J Davison", "abstract": "  We propose an approach to do learning in Gaussian factor graphs. We treat all\nrelevant quantities (inputs, outputs, parameters, latents) as random variables\nin a graphical model, and view both training and prediction as inference\nproblems with different observed nodes. Our experiments show that these\nproblems can be efficiently solved with belief propagation (BP), whose updates\nare inherently local, presenting exciting opportunities for distributed and\nasynchronous training. Our approach can be scaled to deep networks and provides\na natural means to do continual learning: use the BP-estimated parameter\nmarginals of the current task as parameter priors for the next. On a video\ndenoising task we demonstrate the benefit of learnable parameters over a\nclassical factor graph approach and we show encouraging performance of deep\nfactor graphs for continual image classification.\n", "link": "http://arxiv.org/abs/2311.14649v3", "date": "2024-07-17", "relevancy": 2.0435, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5204}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5122}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20in%20Deep%20Factor%20Graphs%20with%20Gaussian%20Belief%20Propagation&body=Title%3A%20Learning%20in%20Deep%20Factor%20Graphs%20with%20Gaussian%20Belief%20Propagation%0AAuthor%3A%20Seth%20Nabarro%20and%20Mark%20van%20der%20Wilk%20and%20Andrew%20J%20Davison%0AAbstract%3A%20%20%20We%20propose%20an%20approach%20to%20do%20learning%20in%20Gaussian%20factor%20graphs.%20We%20treat%20all%0Arelevant%20quantities%20%28inputs%2C%20outputs%2C%20parameters%2C%20latents%29%20as%20random%20variables%0Ain%20a%20graphical%20model%2C%20and%20view%20both%20training%20and%20prediction%20as%20inference%0Aproblems%20with%20different%20observed%20nodes.%20Our%20experiments%20show%20that%20these%0Aproblems%20can%20be%20efficiently%20solved%20with%20belief%20propagation%20%28BP%29%2C%20whose%20updates%0Aare%20inherently%20local%2C%20presenting%20exciting%20opportunities%20for%20distributed%20and%0Aasynchronous%20training.%20Our%20approach%20can%20be%20scaled%20to%20deep%20networks%20and%20provides%0Aa%20natural%20means%20to%20do%20continual%20learning%3A%20use%20the%20BP-estimated%20parameter%0Amarginals%20of%20the%20current%20task%20as%20parameter%20priors%20for%20the%20next.%20On%20a%20video%0Adenoising%20task%20we%20demonstrate%20the%20benefit%20of%20learnable%20parameters%20over%20a%0Aclassical%20factor%20graph%20approach%20and%20we%20show%20encouraging%20performance%20of%20deep%0Afactor%20graphs%20for%20continual%20image%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14649v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520in%2520Deep%2520Factor%2520Graphs%2520with%2520Gaussian%2520Belief%2520Propagation%26entry.906535625%3DSeth%2520Nabarro%2520and%2520Mark%2520van%2520der%2520Wilk%2520and%2520Andrew%2520J%2520Davison%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520approach%2520to%2520do%2520learning%2520in%2520Gaussian%2520factor%2520graphs.%2520We%2520treat%2520all%250Arelevant%2520quantities%2520%2528inputs%252C%2520outputs%252C%2520parameters%252C%2520latents%2529%2520as%2520random%2520variables%250Ain%2520a%2520graphical%2520model%252C%2520and%2520view%2520both%2520training%2520and%2520prediction%2520as%2520inference%250Aproblems%2520with%2520different%2520observed%2520nodes.%2520Our%2520experiments%2520show%2520that%2520these%250Aproblems%2520can%2520be%2520efficiently%2520solved%2520with%2520belief%2520propagation%2520%2528BP%2529%252C%2520whose%2520updates%250Aare%2520inherently%2520local%252C%2520presenting%2520exciting%2520opportunities%2520for%2520distributed%2520and%250Aasynchronous%2520training.%2520Our%2520approach%2520can%2520be%2520scaled%2520to%2520deep%2520networks%2520and%2520provides%250Aa%2520natural%2520means%2520to%2520do%2520continual%2520learning%253A%2520use%2520the%2520BP-estimated%2520parameter%250Amarginals%2520of%2520the%2520current%2520task%2520as%2520parameter%2520priors%2520for%2520the%2520next.%2520On%2520a%2520video%250Adenoising%2520task%2520we%2520demonstrate%2520the%2520benefit%2520of%2520learnable%2520parameters%2520over%2520a%250Aclassical%2520factor%2520graph%2520approach%2520and%2520we%2520show%2520encouraging%2520performance%2520of%2520deep%250Afactor%2520graphs%2520for%2520continual%2520image%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14649v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20in%20Deep%20Factor%20Graphs%20with%20Gaussian%20Belief%20Propagation&entry.906535625=Seth%20Nabarro%20and%20Mark%20van%20der%20Wilk%20and%20Andrew%20J%20Davison&entry.1292438233=%20%20We%20propose%20an%20approach%20to%20do%20learning%20in%20Gaussian%20factor%20graphs.%20We%20treat%20all%0Arelevant%20quantities%20%28inputs%2C%20outputs%2C%20parameters%2C%20latents%29%20as%20random%20variables%0Ain%20a%20graphical%20model%2C%20and%20view%20both%20training%20and%20prediction%20as%20inference%0Aproblems%20with%20different%20observed%20nodes.%20Our%20experiments%20show%20that%20these%0Aproblems%20can%20be%20efficiently%20solved%20with%20belief%20propagation%20%28BP%29%2C%20whose%20updates%0Aare%20inherently%20local%2C%20presenting%20exciting%20opportunities%20for%20distributed%20and%0Aasynchronous%20training.%20Our%20approach%20can%20be%20scaled%20to%20deep%20networks%20and%20provides%0Aa%20natural%20means%20to%20do%20continual%20learning%3A%20use%20the%20BP-estimated%20parameter%0Amarginals%20of%20the%20current%20task%20as%20parameter%20priors%20for%20the%20next.%20On%20a%20video%0Adenoising%20task%20we%20demonstrate%20the%20benefit%20of%20learnable%20parameters%20over%20a%0Aclassical%20factor%20graph%20approach%20and%20we%20show%20encouraging%20performance%20of%20deep%0Afactor%20graphs%20for%20continual%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14649v3&entry.124074799=Read"},
{"title": "Instruction-Driven Game Engines on Large Language Models", "author": "Hongqiu Wu and Xingyuan Liu and Hai Zhao and Min Zhang", "abstract": "  The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played.\n", "link": "http://arxiv.org/abs/2404.00276v3", "date": "2024-07-17", "relevancy": 2.0269, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5432}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4829}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction-Driven%20Game%20Engines%20on%20Large%20Language%20Models&body=Title%3A%20Instruction-Driven%20Game%20Engines%20on%20Large%20Language%20Models%0AAuthor%3A%20Hongqiu%20Wu%20and%20Xingyuan%20Liu%20and%20Hai%20Zhao%20and%20Min%20Zhang%0AAbstract%3A%20%20%20The%20Instruction-Driven%20Game%20Engine%20%28IDGE%29%20project%20aims%20to%20democratize%20game%0Adevelopment%20by%20enabling%20a%20large%20language%20model%20%28LLM%29%20to%20follow%20free-form%20game%0Arules%20and%20autonomously%20generate%20game-play%20processes.%20The%20IDGE%20allows%20users%20to%0Acreate%20games%20by%20issuing%20simple%20natural%20language%20instructions%2C%20which%0Asignificantly%20lowers%20the%20barrier%20for%20game%20development.%20We%20approach%20the%20learning%0Aprocess%20for%20IDGEs%20as%20a%20Next%20State%20Prediction%20task%2C%20wherein%20the%20model%0Aautoregressively%20predicts%20in-game%20states%20given%20player%20actions.%20It%20is%20a%0Achallenging%20task%20because%20the%20computation%20of%20in-game%20states%20must%20be%20precise%3B%0Aotherwise%2C%20slight%20errors%20could%20disrupt%20the%20game-play.%20To%20address%20this%2C%20we%20train%0Athe%20IDGE%20in%20a%20curriculum%20manner%20that%20progressively%20increases%20the%20model%27s%0Aexposure%20to%20complex%20scenarios.%20Our%20initial%20progress%20lies%20in%20developing%20an%20IDGE%0Afor%20Poker%2C%20a%20universally%20cherished%20card%20game.%20The%20engine%20we%27ve%20designed%20not%0Aonly%20supports%20a%20wide%20range%20of%20poker%20variants%20but%20also%20allows%20for%20high%0Acustomization%20of%20rules%20through%20natural%20language%20inputs.%20Furthermore%2C%20it%20also%0Afavors%20rapid%20prototyping%20of%20new%20games%20from%20minimal%20samples%2C%20proposing%20an%0Ainnovative%20paradigm%20in%20game%20development%20that%20relies%20on%20minimal%20prompt%20and%20data%0Aengineering.%20This%20work%20lays%20the%20groundwork%20for%20future%20advancements%20in%0Ainstruction-driven%20game%20creation%2C%20potentially%20transforming%20how%20games%20are%0Adesigned%20and%20played.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00276v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction-Driven%2520Game%2520Engines%2520on%2520Large%2520Language%2520Models%26entry.906535625%3DHongqiu%2520Wu%2520and%2520Xingyuan%2520Liu%2520and%2520Hai%2520Zhao%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520The%2520Instruction-Driven%2520Game%2520Engine%2520%2528IDGE%2529%2520project%2520aims%2520to%2520democratize%2520game%250Adevelopment%2520by%2520enabling%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520follow%2520free-form%2520game%250Arules%2520and%2520autonomously%2520generate%2520game-play%2520processes.%2520The%2520IDGE%2520allows%2520users%2520to%250Acreate%2520games%2520by%2520issuing%2520simple%2520natural%2520language%2520instructions%252C%2520which%250Asignificantly%2520lowers%2520the%2520barrier%2520for%2520game%2520development.%2520We%2520approach%2520the%2520learning%250Aprocess%2520for%2520IDGEs%2520as%2520a%2520Next%2520State%2520Prediction%2520task%252C%2520wherein%2520the%2520model%250Aautoregressively%2520predicts%2520in-game%2520states%2520given%2520player%2520actions.%2520It%2520is%2520a%250Achallenging%2520task%2520because%2520the%2520computation%2520of%2520in-game%2520states%2520must%2520be%2520precise%253B%250Aotherwise%252C%2520slight%2520errors%2520could%2520disrupt%2520the%2520game-play.%2520To%2520address%2520this%252C%2520we%2520train%250Athe%2520IDGE%2520in%2520a%2520curriculum%2520manner%2520that%2520progressively%2520increases%2520the%2520model%2527s%250Aexposure%2520to%2520complex%2520scenarios.%2520Our%2520initial%2520progress%2520lies%2520in%2520developing%2520an%2520IDGE%250Afor%2520Poker%252C%2520a%2520universally%2520cherished%2520card%2520game.%2520The%2520engine%2520we%2527ve%2520designed%2520not%250Aonly%2520supports%2520a%2520wide%2520range%2520of%2520poker%2520variants%2520but%2520also%2520allows%2520for%2520high%250Acustomization%2520of%2520rules%2520through%2520natural%2520language%2520inputs.%2520Furthermore%252C%2520it%2520also%250Afavors%2520rapid%2520prototyping%2520of%2520new%2520games%2520from%2520minimal%2520samples%252C%2520proposing%2520an%250Ainnovative%2520paradigm%2520in%2520game%2520development%2520that%2520relies%2520on%2520minimal%2520prompt%2520and%2520data%250Aengineering.%2520This%2520work%2520lays%2520the%2520groundwork%2520for%2520future%2520advancements%2520in%250Ainstruction-driven%2520game%2520creation%252C%2520potentially%2520transforming%2520how%2520games%2520are%250Adesigned%2520and%2520played.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00276v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction-Driven%20Game%20Engines%20on%20Large%20Language%20Models&entry.906535625=Hongqiu%20Wu%20and%20Xingyuan%20Liu%20and%20Hai%20Zhao%20and%20Min%20Zhang&entry.1292438233=%20%20The%20Instruction-Driven%20Game%20Engine%20%28IDGE%29%20project%20aims%20to%20democratize%20game%0Adevelopment%20by%20enabling%20a%20large%20language%20model%20%28LLM%29%20to%20follow%20free-form%20game%0Arules%20and%20autonomously%20generate%20game-play%20processes.%20The%20IDGE%20allows%20users%20to%0Acreate%20games%20by%20issuing%20simple%20natural%20language%20instructions%2C%20which%0Asignificantly%20lowers%20the%20barrier%20for%20game%20development.%20We%20approach%20the%20learning%0Aprocess%20for%20IDGEs%20as%20a%20Next%20State%20Prediction%20task%2C%20wherein%20the%20model%0Aautoregressively%20predicts%20in-game%20states%20given%20player%20actions.%20It%20is%20a%0Achallenging%20task%20because%20the%20computation%20of%20in-game%20states%20must%20be%20precise%3B%0Aotherwise%2C%20slight%20errors%20could%20disrupt%20the%20game-play.%20To%20address%20this%2C%20we%20train%0Athe%20IDGE%20in%20a%20curriculum%20manner%20that%20progressively%20increases%20the%20model%27s%0Aexposure%20to%20complex%20scenarios.%20Our%20initial%20progress%20lies%20in%20developing%20an%20IDGE%0Afor%20Poker%2C%20a%20universally%20cherished%20card%20game.%20The%20engine%20we%27ve%20designed%20not%0Aonly%20supports%20a%20wide%20range%20of%20poker%20variants%20but%20also%20allows%20for%20high%0Acustomization%20of%20rules%20through%20natural%20language%20inputs.%20Furthermore%2C%20it%20also%0Afavors%20rapid%20prototyping%20of%20new%20games%20from%20minimal%20samples%2C%20proposing%20an%0Ainnovative%20paradigm%20in%20game%20development%20that%20relies%20on%20minimal%20prompt%20and%20data%0Aengineering.%20This%20work%20lays%20the%20groundwork%20for%20future%20advancements%20in%0Ainstruction-driven%20game%20creation%2C%20potentially%20transforming%20how%20games%20are%0Adesigned%20and%20played.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00276v3&entry.124074799=Read"},
{"title": "Forward Invariance in Trajectory Spaces for Safety-critical Control", "author": "Matti Vahs and Rafael I. Cabral Muchacho and Florian T. Pokorny and Jana Tumova", "abstract": "  Useful robot control algorithms should not only achieve performance\nobjectives but also adhere to hard safety constraints. Control Barrier\nFunctions (CBFs) have been developed to provably ensure system safety through\nforward invariance. However, they often unnecessarily sacrifice performance for\nsafety since they are purely reactive. Receding horizon control (RHC), on the\nother hand, consider planned trajectories to account for the future evolution\nof a system. This work provides a new perspective on safety-critical control by\nintroducing Forward Invariance in Trajectory Spaces (FITS). We lift the problem\nof safe RHC into the trajectory space and describe the evolution of planned\ntrajectories as a controlled dynamical system. Safety constraints defined over\nstates can be converted into sets in the trajectory space which we render\nforward invariant via a CBF framework. We derive an efficient quadratic program\n(QP) to synthesize trajectories that provably satisfy safety constraints. Our\nexperiments support that FITS improves the adherence to safety specifications\nwithout sacrificing performance over alternative CBF and NMPC methods.\n", "link": "http://arxiv.org/abs/2407.12624v1", "date": "2024-07-17", "relevancy": 2.0261, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5061}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forward%20Invariance%20in%20Trajectory%20Spaces%20for%20Safety-critical%20Control&body=Title%3A%20Forward%20Invariance%20in%20Trajectory%20Spaces%20for%20Safety-critical%20Control%0AAuthor%3A%20Matti%20Vahs%20and%20Rafael%20I.%20Cabral%20Muchacho%20and%20Florian%20T.%20Pokorny%20and%20Jana%20Tumova%0AAbstract%3A%20%20%20Useful%20robot%20control%20algorithms%20should%20not%20only%20achieve%20performance%0Aobjectives%20but%20also%20adhere%20to%20hard%20safety%20constraints.%20Control%20Barrier%0AFunctions%20%28CBFs%29%20have%20been%20developed%20to%20provably%20ensure%20system%20safety%20through%0Aforward%20invariance.%20However%2C%20they%20often%20unnecessarily%20sacrifice%20performance%20for%0Asafety%20since%20they%20are%20purely%20reactive.%20Receding%20horizon%20control%20%28RHC%29%2C%20on%20the%0Aother%20hand%2C%20consider%20planned%20trajectories%20to%20account%20for%20the%20future%20evolution%0Aof%20a%20system.%20This%20work%20provides%20a%20new%20perspective%20on%20safety-critical%20control%20by%0Aintroducing%20Forward%20Invariance%20in%20Trajectory%20Spaces%20%28FITS%29.%20We%20lift%20the%20problem%0Aof%20safe%20RHC%20into%20the%20trajectory%20space%20and%20describe%20the%20evolution%20of%20planned%0Atrajectories%20as%20a%20controlled%20dynamical%20system.%20Safety%20constraints%20defined%20over%0Astates%20can%20be%20converted%20into%20sets%20in%20the%20trajectory%20space%20which%20we%20render%0Aforward%20invariant%20via%20a%20CBF%20framework.%20We%20derive%20an%20efficient%20quadratic%20program%0A%28QP%29%20to%20synthesize%20trajectories%20that%20provably%20satisfy%20safety%20constraints.%20Our%0Aexperiments%20support%20that%20FITS%20improves%20the%20adherence%20to%20safety%20specifications%0Awithout%20sacrificing%20performance%20over%20alternative%20CBF%20and%20NMPC%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForward%2520Invariance%2520in%2520Trajectory%2520Spaces%2520for%2520Safety-critical%2520Control%26entry.906535625%3DMatti%2520Vahs%2520and%2520Rafael%2520I.%2520Cabral%2520Muchacho%2520and%2520Florian%2520T.%2520Pokorny%2520and%2520Jana%2520Tumova%26entry.1292438233%3D%2520%2520Useful%2520robot%2520control%2520algorithms%2520should%2520not%2520only%2520achieve%2520performance%250Aobjectives%2520but%2520also%2520adhere%2520to%2520hard%2520safety%2520constraints.%2520Control%2520Barrier%250AFunctions%2520%2528CBFs%2529%2520have%2520been%2520developed%2520to%2520provably%2520ensure%2520system%2520safety%2520through%250Aforward%2520invariance.%2520However%252C%2520they%2520often%2520unnecessarily%2520sacrifice%2520performance%2520for%250Asafety%2520since%2520they%2520are%2520purely%2520reactive.%2520Receding%2520horizon%2520control%2520%2528RHC%2529%252C%2520on%2520the%250Aother%2520hand%252C%2520consider%2520planned%2520trajectories%2520to%2520account%2520for%2520the%2520future%2520evolution%250Aof%2520a%2520system.%2520This%2520work%2520provides%2520a%2520new%2520perspective%2520on%2520safety-critical%2520control%2520by%250Aintroducing%2520Forward%2520Invariance%2520in%2520Trajectory%2520Spaces%2520%2528FITS%2529.%2520We%2520lift%2520the%2520problem%250Aof%2520safe%2520RHC%2520into%2520the%2520trajectory%2520space%2520and%2520describe%2520the%2520evolution%2520of%2520planned%250Atrajectories%2520as%2520a%2520controlled%2520dynamical%2520system.%2520Safety%2520constraints%2520defined%2520over%250Astates%2520can%2520be%2520converted%2520into%2520sets%2520in%2520the%2520trajectory%2520space%2520which%2520we%2520render%250Aforward%2520invariant%2520via%2520a%2520CBF%2520framework.%2520We%2520derive%2520an%2520efficient%2520quadratic%2520program%250A%2528QP%2529%2520to%2520synthesize%2520trajectories%2520that%2520provably%2520satisfy%2520safety%2520constraints.%2520Our%250Aexperiments%2520support%2520that%2520FITS%2520improves%2520the%2520adherence%2520to%2520safety%2520specifications%250Awithout%2520sacrificing%2520performance%2520over%2520alternative%2520CBF%2520and%2520NMPC%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forward%20Invariance%20in%20Trajectory%20Spaces%20for%20Safety-critical%20Control&entry.906535625=Matti%20Vahs%20and%20Rafael%20I.%20Cabral%20Muchacho%20and%20Florian%20T.%20Pokorny%20and%20Jana%20Tumova&entry.1292438233=%20%20Useful%20robot%20control%20algorithms%20should%20not%20only%20achieve%20performance%0Aobjectives%20but%20also%20adhere%20to%20hard%20safety%20constraints.%20Control%20Barrier%0AFunctions%20%28CBFs%29%20have%20been%20developed%20to%20provably%20ensure%20system%20safety%20through%0Aforward%20invariance.%20However%2C%20they%20often%20unnecessarily%20sacrifice%20performance%20for%0Asafety%20since%20they%20are%20purely%20reactive.%20Receding%20horizon%20control%20%28RHC%29%2C%20on%20the%0Aother%20hand%2C%20consider%20planned%20trajectories%20to%20account%20for%20the%20future%20evolution%0Aof%20a%20system.%20This%20work%20provides%20a%20new%20perspective%20on%20safety-critical%20control%20by%0Aintroducing%20Forward%20Invariance%20in%20Trajectory%20Spaces%20%28FITS%29.%20We%20lift%20the%20problem%0Aof%20safe%20RHC%20into%20the%20trajectory%20space%20and%20describe%20the%20evolution%20of%20planned%0Atrajectories%20as%20a%20controlled%20dynamical%20system.%20Safety%20constraints%20defined%20over%0Astates%20can%20be%20converted%20into%20sets%20in%20the%20trajectory%20space%20which%20we%20render%0Aforward%20invariant%20via%20a%20CBF%20framework.%20We%20derive%20an%20efficient%20quadratic%20program%0A%28QP%29%20to%20synthesize%20trajectories%20that%20provably%20satisfy%20safety%20constraints.%20Our%0Aexperiments%20support%20that%20FITS%20improves%20the%20adherence%20to%20safety%20specifications%0Awithout%20sacrificing%20performance%20over%20alternative%20CBF%20and%20NMPC%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12624v1&entry.124074799=Read"},
{"title": "OMG-Net: A Deep Learning Framework Deploying Segment Anything to Detect\n  Pan-Cancer Mitotic Figures from Haematoxylin and Eosin-Stained Slides", "author": "Zhuoyan Shen and Mikael Simard and Douglas Brand and Vanghelita Andrei and Ali Al-Khader and Fatine Oumlil and Katherine Trevers and Thomas Butters and Simon Haefliger and Eleanna Kara and Fernanda Amary and Roberto Tirabosco and Paul Cool and Gary Royle and Maria A. Hawkins and Adrienne M. Flanagan and Charles-Antoine Collins Fekete", "abstract": "  Mitotic activity is an important feature for grading several cancer types.\nCounting mitotic figures (MFs) is a time-consuming, laborious task prone to\ninter-observer variation. Inaccurate recognition of MFs can lead to incorrect\ngrading and hence potential suboptimal treatment. In this study, we propose an\nartificial intelligence (AI)-aided approach to detect MFs in digitised\nhaematoxylin and eosin-stained whole slide images (WSIs). Advances in this area\nare hampered by the limited number and types of cancer datasets of MFs. Here we\nestablish the largest pan-cancer dataset of mitotic figures by combining an\nin-house dataset of soft tissue tumours (STMF) with five open-source mitotic\ndatasets comprising multiple human cancers and canine specimens (ICPR, TUPAC,\nCCMCT, CMC and MIDOG++). This new dataset identifies 74,620 MFs and 105,538\nmitotic-like figures. We then employed a two-stage framework (the Optimised\nMitoses Generator Network (OMG-Net) to classify MFs. The framework first\ndeploys the Segment Anything Model (SAM) to automate the contouring of MFs and\nsurrounding objects. An adapted ResNet18 is subsequently trained to classify\nMFs. OMG-Net reaches an F1-score of 0.84 on pan-cancer MF detection (breast\ncarcinoma, neuroendocrine tumour and melanoma), largely outperforming the\nprevious state-of-the-art MIDOG++ benchmark model on its hold-out testing set\n(e.g. +16% F1-score on breast cancer detection, p<0.001) thereby providing\nsuperior accuracy in detecting MFs on various types of tumours obtained with\ndifferent scanners.\n", "link": "http://arxiv.org/abs/2407.12773v1", "date": "2024-07-17", "relevancy": 2.0233, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5204}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMG-Net%3A%20A%20Deep%20Learning%20Framework%20Deploying%20Segment%20Anything%20to%20Detect%0A%20%20Pan-Cancer%20Mitotic%20Figures%20from%20Haematoxylin%20and%20Eosin-Stained%20Slides&body=Title%3A%20OMG-Net%3A%20A%20Deep%20Learning%20Framework%20Deploying%20Segment%20Anything%20to%20Detect%0A%20%20Pan-Cancer%20Mitotic%20Figures%20from%20Haematoxylin%20and%20Eosin-Stained%20Slides%0AAuthor%3A%20Zhuoyan%20Shen%20and%20Mikael%20Simard%20and%20Douglas%20Brand%20and%20Vanghelita%20Andrei%20and%20Ali%20Al-Khader%20and%20Fatine%20Oumlil%20and%20Katherine%20Trevers%20and%20Thomas%20Butters%20and%20Simon%20Haefliger%20and%20Eleanna%20Kara%20and%20Fernanda%20Amary%20and%20Roberto%20Tirabosco%20and%20Paul%20Cool%20and%20Gary%20Royle%20and%20Maria%20A.%20Hawkins%20and%20Adrienne%20M.%20Flanagan%20and%20Charles-Antoine%20Collins%20Fekete%0AAbstract%3A%20%20%20Mitotic%20activity%20is%20an%20important%20feature%20for%20grading%20several%20cancer%20types.%0ACounting%20mitotic%20figures%20%28MFs%29%20is%20a%20time-consuming%2C%20laborious%20task%20prone%20to%0Ainter-observer%20variation.%20Inaccurate%20recognition%20of%20MFs%20can%20lead%20to%20incorrect%0Agrading%20and%20hence%20potential%20suboptimal%20treatment.%20In%20this%20study%2C%20we%20propose%20an%0Aartificial%20intelligence%20%28AI%29-aided%20approach%20to%20detect%20MFs%20in%20digitised%0Ahaematoxylin%20and%20eosin-stained%20whole%20slide%20images%20%28WSIs%29.%20Advances%20in%20this%20area%0Aare%20hampered%20by%20the%20limited%20number%20and%20types%20of%20cancer%20datasets%20of%20MFs.%20Here%20we%0Aestablish%20the%20largest%20pan-cancer%20dataset%20of%20mitotic%20figures%20by%20combining%20an%0Ain-house%20dataset%20of%20soft%20tissue%20tumours%20%28STMF%29%20with%20five%20open-source%20mitotic%0Adatasets%20comprising%20multiple%20human%20cancers%20and%20canine%20specimens%20%28ICPR%2C%20TUPAC%2C%0ACCMCT%2C%20CMC%20and%20MIDOG%2B%2B%29.%20This%20new%20dataset%20identifies%2074%2C620%20MFs%20and%20105%2C538%0Amitotic-like%20figures.%20We%20then%20employed%20a%20two-stage%20framework%20%28the%20Optimised%0AMitoses%20Generator%20Network%20%28OMG-Net%29%20to%20classify%20MFs.%20The%20framework%20first%0Adeploys%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20automate%20the%20contouring%20of%20MFs%20and%0Asurrounding%20objects.%20An%20adapted%20ResNet18%20is%20subsequently%20trained%20to%20classify%0AMFs.%20OMG-Net%20reaches%20an%20F1-score%20of%200.84%20on%20pan-cancer%20MF%20detection%20%28breast%0Acarcinoma%2C%20neuroendocrine%20tumour%20and%20melanoma%29%2C%20largely%20outperforming%20the%0Aprevious%20state-of-the-art%20MIDOG%2B%2B%20benchmark%20model%20on%20its%20hold-out%20testing%20set%0A%28e.g.%20%2B16%25%20F1-score%20on%20breast%20cancer%20detection%2C%20p%3C0.001%29%20thereby%20providing%0Asuperior%20accuracy%20in%20detecting%20MFs%20on%20various%20types%20of%20tumours%20obtained%20with%0Adifferent%20scanners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMG-Net%253A%2520A%2520Deep%2520Learning%2520Framework%2520Deploying%2520Segment%2520Anything%2520to%2520Detect%250A%2520%2520Pan-Cancer%2520Mitotic%2520Figures%2520from%2520Haematoxylin%2520and%2520Eosin-Stained%2520Slides%26entry.906535625%3DZhuoyan%2520Shen%2520and%2520Mikael%2520Simard%2520and%2520Douglas%2520Brand%2520and%2520Vanghelita%2520Andrei%2520and%2520Ali%2520Al-Khader%2520and%2520Fatine%2520Oumlil%2520and%2520Katherine%2520Trevers%2520and%2520Thomas%2520Butters%2520and%2520Simon%2520Haefliger%2520and%2520Eleanna%2520Kara%2520and%2520Fernanda%2520Amary%2520and%2520Roberto%2520Tirabosco%2520and%2520Paul%2520Cool%2520and%2520Gary%2520Royle%2520and%2520Maria%2520A.%2520Hawkins%2520and%2520Adrienne%2520M.%2520Flanagan%2520and%2520Charles-Antoine%2520Collins%2520Fekete%26entry.1292438233%3D%2520%2520Mitotic%2520activity%2520is%2520an%2520important%2520feature%2520for%2520grading%2520several%2520cancer%2520types.%250ACounting%2520mitotic%2520figures%2520%2528MFs%2529%2520is%2520a%2520time-consuming%252C%2520laborious%2520task%2520prone%2520to%250Ainter-observer%2520variation.%2520Inaccurate%2520recognition%2520of%2520MFs%2520can%2520lead%2520to%2520incorrect%250Agrading%2520and%2520hence%2520potential%2520suboptimal%2520treatment.%2520In%2520this%2520study%252C%2520we%2520propose%2520an%250Aartificial%2520intelligence%2520%2528AI%2529-aided%2520approach%2520to%2520detect%2520MFs%2520in%2520digitised%250Ahaematoxylin%2520and%2520eosin-stained%2520whole%2520slide%2520images%2520%2528WSIs%2529.%2520Advances%2520in%2520this%2520area%250Aare%2520hampered%2520by%2520the%2520limited%2520number%2520and%2520types%2520of%2520cancer%2520datasets%2520of%2520MFs.%2520Here%2520we%250Aestablish%2520the%2520largest%2520pan-cancer%2520dataset%2520of%2520mitotic%2520figures%2520by%2520combining%2520an%250Ain-house%2520dataset%2520of%2520soft%2520tissue%2520tumours%2520%2528STMF%2529%2520with%2520five%2520open-source%2520mitotic%250Adatasets%2520comprising%2520multiple%2520human%2520cancers%2520and%2520canine%2520specimens%2520%2528ICPR%252C%2520TUPAC%252C%250ACCMCT%252C%2520CMC%2520and%2520MIDOG%252B%252B%2529.%2520This%2520new%2520dataset%2520identifies%252074%252C620%2520MFs%2520and%2520105%252C538%250Amitotic-like%2520figures.%2520We%2520then%2520employed%2520a%2520two-stage%2520framework%2520%2528the%2520Optimised%250AMitoses%2520Generator%2520Network%2520%2528OMG-Net%2529%2520to%2520classify%2520MFs.%2520The%2520framework%2520first%250Adeploys%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520to%2520automate%2520the%2520contouring%2520of%2520MFs%2520and%250Asurrounding%2520objects.%2520An%2520adapted%2520ResNet18%2520is%2520subsequently%2520trained%2520to%2520classify%250AMFs.%2520OMG-Net%2520reaches%2520an%2520F1-score%2520of%25200.84%2520on%2520pan-cancer%2520MF%2520detection%2520%2528breast%250Acarcinoma%252C%2520neuroendocrine%2520tumour%2520and%2520melanoma%2529%252C%2520largely%2520outperforming%2520the%250Aprevious%2520state-of-the-art%2520MIDOG%252B%252B%2520benchmark%2520model%2520on%2520its%2520hold-out%2520testing%2520set%250A%2528e.g.%2520%252B16%2525%2520F1-score%2520on%2520breast%2520cancer%2520detection%252C%2520p%253C0.001%2529%2520thereby%2520providing%250Asuperior%2520accuracy%2520in%2520detecting%2520MFs%2520on%2520various%2520types%2520of%2520tumours%2520obtained%2520with%250Adifferent%2520scanners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMG-Net%3A%20A%20Deep%20Learning%20Framework%20Deploying%20Segment%20Anything%20to%20Detect%0A%20%20Pan-Cancer%20Mitotic%20Figures%20from%20Haematoxylin%20and%20Eosin-Stained%20Slides&entry.906535625=Zhuoyan%20Shen%20and%20Mikael%20Simard%20and%20Douglas%20Brand%20and%20Vanghelita%20Andrei%20and%20Ali%20Al-Khader%20and%20Fatine%20Oumlil%20and%20Katherine%20Trevers%20and%20Thomas%20Butters%20and%20Simon%20Haefliger%20and%20Eleanna%20Kara%20and%20Fernanda%20Amary%20and%20Roberto%20Tirabosco%20and%20Paul%20Cool%20and%20Gary%20Royle%20and%20Maria%20A.%20Hawkins%20and%20Adrienne%20M.%20Flanagan%20and%20Charles-Antoine%20Collins%20Fekete&entry.1292438233=%20%20Mitotic%20activity%20is%20an%20important%20feature%20for%20grading%20several%20cancer%20types.%0ACounting%20mitotic%20figures%20%28MFs%29%20is%20a%20time-consuming%2C%20laborious%20task%20prone%20to%0Ainter-observer%20variation.%20Inaccurate%20recognition%20of%20MFs%20can%20lead%20to%20incorrect%0Agrading%20and%20hence%20potential%20suboptimal%20treatment.%20In%20this%20study%2C%20we%20propose%20an%0Aartificial%20intelligence%20%28AI%29-aided%20approach%20to%20detect%20MFs%20in%20digitised%0Ahaematoxylin%20and%20eosin-stained%20whole%20slide%20images%20%28WSIs%29.%20Advances%20in%20this%20area%0Aare%20hampered%20by%20the%20limited%20number%20and%20types%20of%20cancer%20datasets%20of%20MFs.%20Here%20we%0Aestablish%20the%20largest%20pan-cancer%20dataset%20of%20mitotic%20figures%20by%20combining%20an%0Ain-house%20dataset%20of%20soft%20tissue%20tumours%20%28STMF%29%20with%20five%20open-source%20mitotic%0Adatasets%20comprising%20multiple%20human%20cancers%20and%20canine%20specimens%20%28ICPR%2C%20TUPAC%2C%0ACCMCT%2C%20CMC%20and%20MIDOG%2B%2B%29.%20This%20new%20dataset%20identifies%2074%2C620%20MFs%20and%20105%2C538%0Amitotic-like%20figures.%20We%20then%20employed%20a%20two-stage%20framework%20%28the%20Optimised%0AMitoses%20Generator%20Network%20%28OMG-Net%29%20to%20classify%20MFs.%20The%20framework%20first%0Adeploys%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20automate%20the%20contouring%20of%20MFs%20and%0Asurrounding%20objects.%20An%20adapted%20ResNet18%20is%20subsequently%20trained%20to%20classify%0AMFs.%20OMG-Net%20reaches%20an%20F1-score%20of%200.84%20on%20pan-cancer%20MF%20detection%20%28breast%0Acarcinoma%2C%20neuroendocrine%20tumour%20and%20melanoma%29%2C%20largely%20outperforming%20the%0Aprevious%20state-of-the-art%20MIDOG%2B%2B%20benchmark%20model%20on%20its%20hold-out%20testing%20set%0A%28e.g.%20%2B16%25%20F1-score%20on%20breast%20cancer%20detection%2C%20p%3C0.001%29%20thereby%20providing%0Asuperior%20accuracy%20in%20detecting%20MFs%20on%20various%20types%20of%20tumours%20obtained%20with%0Adifferent%20scanners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12773v1&entry.124074799=Read"},
{"title": "The inherent goodness of well educated intelligence", "author": "Michael E. Glinsky", "abstract": "  This paper will examine what makes a being intelligent, whether that be a\nbiological being or an artificial silicon being on a computer. Special\nattention will be paid to the being having the ability to characterize and\ncontrol a collective system of many identical conservative sub-systems\nconservatively interacting. The essence of intelligence will be found to be the\ngolden rule -- \"the collective acts as one\" or \"knowing the global consequences\nof local actions\". The flow of the collective is a small set of twinkling\ntextures, that are governed by a puppeteer who is pulling a small number of\nstrings according to a geodesic motion of least action, determined by the\nsymmetries. Controlling collective conservative systems is difficult and has\nhistorically been done by adding significant viscosity to the system to\nstabilize the desirable meta stable equilibriums of maximum performance, but it\ndegrades or destroys them in the process. There is an alternative. Once the\noptimum twinkling textures of the meta stable equilibriums are identified, the\ncollective system can be moved to the optimum twinkling textures, then quickly\nvibrated according to the textures so that the collective system remains at the\nmeta stable equilibrium. Well educated intelligence knows the global\nconsequences of its local actions so that it will not take short term actions\nthat will lead to poor long term outcomes. In contrast, trained intelligence or\ntrained stupidity will optimize its short term actions, leading to poor long\nterm outcomes. Well educated intelligence is inherently good, but trained\nstupidity is inherently evil and should be feared. Particular attention is paid\nto the control and optimization of economic and social collectives. These new\nresults are also applicable to physical collectives such as fields, fluids and\nplasmas.\n", "link": "http://arxiv.org/abs/2401.04846v7", "date": "2024-07-17", "relevancy": 2.0159, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4263}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3966}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20inherent%20goodness%20of%20well%20educated%20intelligence&body=Title%3A%20The%20inherent%20goodness%20of%20well%20educated%20intelligence%0AAuthor%3A%20Michael%20E.%20Glinsky%0AAbstract%3A%20%20%20This%20paper%20will%20examine%20what%20makes%20a%20being%20intelligent%2C%20whether%20that%20be%20a%0Abiological%20being%20or%20an%20artificial%20silicon%20being%20on%20a%20computer.%20Special%0Aattention%20will%20be%20paid%20to%20the%20being%20having%20the%20ability%20to%20characterize%20and%0Acontrol%20a%20collective%20system%20of%20many%20identical%20conservative%20sub-systems%0Aconservatively%20interacting.%20The%20essence%20of%20intelligence%20will%20be%20found%20to%20be%20the%0Agolden%20rule%20--%20%22the%20collective%20acts%20as%20one%22%20or%20%22knowing%20the%20global%20consequences%0Aof%20local%20actions%22.%20The%20flow%20of%20the%20collective%20is%20a%20small%20set%20of%20twinkling%0Atextures%2C%20that%20are%20governed%20by%20a%20puppeteer%20who%20is%20pulling%20a%20small%20number%20of%0Astrings%20according%20to%20a%20geodesic%20motion%20of%20least%20action%2C%20determined%20by%20the%0Asymmetries.%20Controlling%20collective%20conservative%20systems%20is%20difficult%20and%20has%0Ahistorically%20been%20done%20by%20adding%20significant%20viscosity%20to%20the%20system%20to%0Astabilize%20the%20desirable%20meta%20stable%20equilibriums%20of%20maximum%20performance%2C%20but%20it%0Adegrades%20or%20destroys%20them%20in%20the%20process.%20There%20is%20an%20alternative.%20Once%20the%0Aoptimum%20twinkling%20textures%20of%20the%20meta%20stable%20equilibriums%20are%20identified%2C%20the%0Acollective%20system%20can%20be%20moved%20to%20the%20optimum%20twinkling%20textures%2C%20then%20quickly%0Avibrated%20according%20to%20the%20textures%20so%20that%20the%20collective%20system%20remains%20at%20the%0Ameta%20stable%20equilibrium.%20Well%20educated%20intelligence%20knows%20the%20global%0Aconsequences%20of%20its%20local%20actions%20so%20that%20it%20will%20not%20take%20short%20term%20actions%0Athat%20will%20lead%20to%20poor%20long%20term%20outcomes.%20In%20contrast%2C%20trained%20intelligence%20or%0Atrained%20stupidity%20will%20optimize%20its%20short%20term%20actions%2C%20leading%20to%20poor%20long%0Aterm%20outcomes.%20Well%20educated%20intelligence%20is%20inherently%20good%2C%20but%20trained%0Astupidity%20is%20inherently%20evil%20and%20should%20be%20feared.%20Particular%20attention%20is%20paid%0Ato%20the%20control%20and%20optimization%20of%20economic%20and%20social%20collectives.%20These%20new%0Aresults%20are%20also%20applicable%20to%20physical%20collectives%20such%20as%20fields%2C%20fluids%20and%0Aplasmas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04846v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520inherent%2520goodness%2520of%2520well%2520educated%2520intelligence%26entry.906535625%3DMichael%2520E.%2520Glinsky%26entry.1292438233%3D%2520%2520This%2520paper%2520will%2520examine%2520what%2520makes%2520a%2520being%2520intelligent%252C%2520whether%2520that%2520be%2520a%250Abiological%2520being%2520or%2520an%2520artificial%2520silicon%2520being%2520on%2520a%2520computer.%2520Special%250Aattention%2520will%2520be%2520paid%2520to%2520the%2520being%2520having%2520the%2520ability%2520to%2520characterize%2520and%250Acontrol%2520a%2520collective%2520system%2520of%2520many%2520identical%2520conservative%2520sub-systems%250Aconservatively%2520interacting.%2520The%2520essence%2520of%2520intelligence%2520will%2520be%2520found%2520to%2520be%2520the%250Agolden%2520rule%2520--%2520%2522the%2520collective%2520acts%2520as%2520one%2522%2520or%2520%2522knowing%2520the%2520global%2520consequences%250Aof%2520local%2520actions%2522.%2520The%2520flow%2520of%2520the%2520collective%2520is%2520a%2520small%2520set%2520of%2520twinkling%250Atextures%252C%2520that%2520are%2520governed%2520by%2520a%2520puppeteer%2520who%2520is%2520pulling%2520a%2520small%2520number%2520of%250Astrings%2520according%2520to%2520a%2520geodesic%2520motion%2520of%2520least%2520action%252C%2520determined%2520by%2520the%250Asymmetries.%2520Controlling%2520collective%2520conservative%2520systems%2520is%2520difficult%2520and%2520has%250Ahistorically%2520been%2520done%2520by%2520adding%2520significant%2520viscosity%2520to%2520the%2520system%2520to%250Astabilize%2520the%2520desirable%2520meta%2520stable%2520equilibriums%2520of%2520maximum%2520performance%252C%2520but%2520it%250Adegrades%2520or%2520destroys%2520them%2520in%2520the%2520process.%2520There%2520is%2520an%2520alternative.%2520Once%2520the%250Aoptimum%2520twinkling%2520textures%2520of%2520the%2520meta%2520stable%2520equilibriums%2520are%2520identified%252C%2520the%250Acollective%2520system%2520can%2520be%2520moved%2520to%2520the%2520optimum%2520twinkling%2520textures%252C%2520then%2520quickly%250Avibrated%2520according%2520to%2520the%2520textures%2520so%2520that%2520the%2520collective%2520system%2520remains%2520at%2520the%250Ameta%2520stable%2520equilibrium.%2520Well%2520educated%2520intelligence%2520knows%2520the%2520global%250Aconsequences%2520of%2520its%2520local%2520actions%2520so%2520that%2520it%2520will%2520not%2520take%2520short%2520term%2520actions%250Athat%2520will%2520lead%2520to%2520poor%2520long%2520term%2520outcomes.%2520In%2520contrast%252C%2520trained%2520intelligence%2520or%250Atrained%2520stupidity%2520will%2520optimize%2520its%2520short%2520term%2520actions%252C%2520leading%2520to%2520poor%2520long%250Aterm%2520outcomes.%2520Well%2520educated%2520intelligence%2520is%2520inherently%2520good%252C%2520but%2520trained%250Astupidity%2520is%2520inherently%2520evil%2520and%2520should%2520be%2520feared.%2520Particular%2520attention%2520is%2520paid%250Ato%2520the%2520control%2520and%2520optimization%2520of%2520economic%2520and%2520social%2520collectives.%2520These%2520new%250Aresults%2520are%2520also%2520applicable%2520to%2520physical%2520collectives%2520such%2520as%2520fields%252C%2520fluids%2520and%250Aplasmas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04846v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20inherent%20goodness%20of%20well%20educated%20intelligence&entry.906535625=Michael%20E.%20Glinsky&entry.1292438233=%20%20This%20paper%20will%20examine%20what%20makes%20a%20being%20intelligent%2C%20whether%20that%20be%20a%0Abiological%20being%20or%20an%20artificial%20silicon%20being%20on%20a%20computer.%20Special%0Aattention%20will%20be%20paid%20to%20the%20being%20having%20the%20ability%20to%20characterize%20and%0Acontrol%20a%20collective%20system%20of%20many%20identical%20conservative%20sub-systems%0Aconservatively%20interacting.%20The%20essence%20of%20intelligence%20will%20be%20found%20to%20be%20the%0Agolden%20rule%20--%20%22the%20collective%20acts%20as%20one%22%20or%20%22knowing%20the%20global%20consequences%0Aof%20local%20actions%22.%20The%20flow%20of%20the%20collective%20is%20a%20small%20set%20of%20twinkling%0Atextures%2C%20that%20are%20governed%20by%20a%20puppeteer%20who%20is%20pulling%20a%20small%20number%20of%0Astrings%20according%20to%20a%20geodesic%20motion%20of%20least%20action%2C%20determined%20by%20the%0Asymmetries.%20Controlling%20collective%20conservative%20systems%20is%20difficult%20and%20has%0Ahistorically%20been%20done%20by%20adding%20significant%20viscosity%20to%20the%20system%20to%0Astabilize%20the%20desirable%20meta%20stable%20equilibriums%20of%20maximum%20performance%2C%20but%20it%0Adegrades%20or%20destroys%20them%20in%20the%20process.%20There%20is%20an%20alternative.%20Once%20the%0Aoptimum%20twinkling%20textures%20of%20the%20meta%20stable%20equilibriums%20are%20identified%2C%20the%0Acollective%20system%20can%20be%20moved%20to%20the%20optimum%20twinkling%20textures%2C%20then%20quickly%0Avibrated%20according%20to%20the%20textures%20so%20that%20the%20collective%20system%20remains%20at%20the%0Ameta%20stable%20equilibrium.%20Well%20educated%20intelligence%20knows%20the%20global%0Aconsequences%20of%20its%20local%20actions%20so%20that%20it%20will%20not%20take%20short%20term%20actions%0Athat%20will%20lead%20to%20poor%20long%20term%20outcomes.%20In%20contrast%2C%20trained%20intelligence%20or%0Atrained%20stupidity%20will%20optimize%20its%20short%20term%20actions%2C%20leading%20to%20poor%20long%0Aterm%20outcomes.%20Well%20educated%20intelligence%20is%20inherently%20good%2C%20but%20trained%0Astupidity%20is%20inherently%20evil%20and%20should%20be%20feared.%20Particular%20attention%20is%20paid%0Ato%20the%20control%20and%20optimization%20of%20economic%20and%20social%20collectives.%20These%20new%0Aresults%20are%20also%20applicable%20to%20physical%20collectives%20such%20as%20fields%2C%20fluids%20and%0Aplasmas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04846v7&entry.124074799=Read"},
{"title": "A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X\n  Communications in Dynamic Environments", "author": "Lorenzo Cazzella and Francesco Linsalata and Maurizio Magarini and Matteo Matteucci and Umberto Spagnolini", "abstract": "  Digital Twins (DTs) for physical wireless environments have been recently\nproposed as accurate virtual representations of the propagation environment\nthat can enable multi-layer decisions at the physical communication equipment.\nAt high-frequency bands, DTs can help to overcome the challenges emerging in\nhigh mobility conditions featuring vehicular environments. In this paper, we\npropose a novel data-driven workflow for the creation of the DT of a\nVehicle-to-Everything (V2X) communication scenario and a multi-modal simulation\nframework for the generation of realistic sensor data and accurate\nmmWave/sub-THz wireless channels. The proposed method leverages an automotive\nsimulation and testing framework and an accurate ray-tracing channel simulator.\nSimulations over an urban scenario show the achievable realistic sensor and\nchannel modelling both at the infrastructure and at ego-vehicles. We showcase\nthe proposed framework on the DT-aided blockage handover task for V2X link\nrestoration, leveraging the framework's dynamic channel generation capabilities\nfor realistic vehicular blockage simulation.\n", "link": "http://arxiv.org/abs/2303.06947v3", "date": "2024-07-17", "relevancy": 2.0154, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5052}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5052}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Modal%20Simulation%20Framework%20to%20Enable%20Digital%20Twin-based%20V2X%0A%20%20Communications%20in%20Dynamic%20Environments&body=Title%3A%20A%20Multi-Modal%20Simulation%20Framework%20to%20Enable%20Digital%20Twin-based%20V2X%0A%20%20Communications%20in%20Dynamic%20Environments%0AAuthor%3A%20Lorenzo%20Cazzella%20and%20Francesco%20Linsalata%20and%20Maurizio%20Magarini%20and%20Matteo%20Matteucci%20and%20Umberto%20Spagnolini%0AAbstract%3A%20%20%20Digital%20Twins%20%28DTs%29%20for%20physical%20wireless%20environments%20have%20been%20recently%0Aproposed%20as%20accurate%20virtual%20representations%20of%20the%20propagation%20environment%0Athat%20can%20enable%20multi-layer%20decisions%20at%20the%20physical%20communication%20equipment.%0AAt%20high-frequency%20bands%2C%20DTs%20can%20help%20to%20overcome%20the%20challenges%20emerging%20in%0Ahigh%20mobility%20conditions%20featuring%20vehicular%20environments.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20data-driven%20workflow%20for%20the%20creation%20of%20the%20DT%20of%20a%0AVehicle-to-Everything%20%28V2X%29%20communication%20scenario%20and%20a%20multi-modal%20simulation%0Aframework%20for%20the%20generation%20of%20realistic%20sensor%20data%20and%20accurate%0AmmWave/sub-THz%20wireless%20channels.%20The%20proposed%20method%20leverages%20an%20automotive%0Asimulation%20and%20testing%20framework%20and%20an%20accurate%20ray-tracing%20channel%20simulator.%0ASimulations%20over%20an%20urban%20scenario%20show%20the%20achievable%20realistic%20sensor%20and%0Achannel%20modelling%20both%20at%20the%20infrastructure%20and%20at%20ego-vehicles.%20We%20showcase%0Athe%20proposed%20framework%20on%20the%20DT-aided%20blockage%20handover%20task%20for%20V2X%20link%0Arestoration%2C%20leveraging%20the%20framework%27s%20dynamic%20channel%20generation%20capabilities%0Afor%20realistic%20vehicular%20blockage%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06947v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Modal%2520Simulation%2520Framework%2520to%2520Enable%2520Digital%2520Twin-based%2520V2X%250A%2520%2520Communications%2520in%2520Dynamic%2520Environments%26entry.906535625%3DLorenzo%2520Cazzella%2520and%2520Francesco%2520Linsalata%2520and%2520Maurizio%2520Magarini%2520and%2520Matteo%2520Matteucci%2520and%2520Umberto%2520Spagnolini%26entry.1292438233%3D%2520%2520Digital%2520Twins%2520%2528DTs%2529%2520for%2520physical%2520wireless%2520environments%2520have%2520been%2520recently%250Aproposed%2520as%2520accurate%2520virtual%2520representations%2520of%2520the%2520propagation%2520environment%250Athat%2520can%2520enable%2520multi-layer%2520decisions%2520at%2520the%2520physical%2520communication%2520equipment.%250AAt%2520high-frequency%2520bands%252C%2520DTs%2520can%2520help%2520to%2520overcome%2520the%2520challenges%2520emerging%2520in%250Ahigh%2520mobility%2520conditions%2520featuring%2520vehicular%2520environments.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520data-driven%2520workflow%2520for%2520the%2520creation%2520of%2520the%2520DT%2520of%2520a%250AVehicle-to-Everything%2520%2528V2X%2529%2520communication%2520scenario%2520and%2520a%2520multi-modal%2520simulation%250Aframework%2520for%2520the%2520generation%2520of%2520realistic%2520sensor%2520data%2520and%2520accurate%250AmmWave/sub-THz%2520wireless%2520channels.%2520The%2520proposed%2520method%2520leverages%2520an%2520automotive%250Asimulation%2520and%2520testing%2520framework%2520and%2520an%2520accurate%2520ray-tracing%2520channel%2520simulator.%250ASimulations%2520over%2520an%2520urban%2520scenario%2520show%2520the%2520achievable%2520realistic%2520sensor%2520and%250Achannel%2520modelling%2520both%2520at%2520the%2520infrastructure%2520and%2520at%2520ego-vehicles.%2520We%2520showcase%250Athe%2520proposed%2520framework%2520on%2520the%2520DT-aided%2520blockage%2520handover%2520task%2520for%2520V2X%2520link%250Arestoration%252C%2520leveraging%2520the%2520framework%2527s%2520dynamic%2520channel%2520generation%2520capabilities%250Afor%2520realistic%2520vehicular%2520blockage%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.06947v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Modal%20Simulation%20Framework%20to%20Enable%20Digital%20Twin-based%20V2X%0A%20%20Communications%20in%20Dynamic%20Environments&entry.906535625=Lorenzo%20Cazzella%20and%20Francesco%20Linsalata%20and%20Maurizio%20Magarini%20and%20Matteo%20Matteucci%20and%20Umberto%20Spagnolini&entry.1292438233=%20%20Digital%20Twins%20%28DTs%29%20for%20physical%20wireless%20environments%20have%20been%20recently%0Aproposed%20as%20accurate%20virtual%20representations%20of%20the%20propagation%20environment%0Athat%20can%20enable%20multi-layer%20decisions%20at%20the%20physical%20communication%20equipment.%0AAt%20high-frequency%20bands%2C%20DTs%20can%20help%20to%20overcome%20the%20challenges%20emerging%20in%0Ahigh%20mobility%20conditions%20featuring%20vehicular%20environments.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20data-driven%20workflow%20for%20the%20creation%20of%20the%20DT%20of%20a%0AVehicle-to-Everything%20%28V2X%29%20communication%20scenario%20and%20a%20multi-modal%20simulation%0Aframework%20for%20the%20generation%20of%20realistic%20sensor%20data%20and%20accurate%0AmmWave/sub-THz%20wireless%20channels.%20The%20proposed%20method%20leverages%20an%20automotive%0Asimulation%20and%20testing%20framework%20and%20an%20accurate%20ray-tracing%20channel%20simulator.%0ASimulations%20over%20an%20urban%20scenario%20show%20the%20achievable%20realistic%20sensor%20and%0Achannel%20modelling%20both%20at%20the%20infrastructure%20and%20at%20ego-vehicles.%20We%20showcase%0Athe%20proposed%20framework%20on%20the%20DT-aided%20blockage%20handover%20task%20for%20V2X%20link%0Arestoration%2C%20leveraging%20the%20framework%27s%20dynamic%20channel%20generation%20capabilities%0Afor%20realistic%20vehicular%20blockage%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06947v3&entry.124074799=Read"},
{"title": "Combining Federated Learning and Control: A Survey", "author": "Jakob Weber and Markus Gurtner and Amadeus Lobe and Adrian Trachte and Andreas Kugi", "abstract": "  This survey provides an overview of combining Federated Learning (FL) and\ncontrol to enhance adaptability, scalability, generalization, and privacy in\n(nonlinear) control applications. Traditional control methods rely on\ncontroller design models, but real-world scenarios often require online model\nretuning or learning. FL offers a distributed approach to model training,\nenabling collaborative learning across distributed devices while preserving\ndata privacy. By keeping data localized, FL mitigates concerns regarding\nprivacy and security while reducing network bandwidth requirements for\ncommunication. This survey summarizes the state-of-the-art concepts and ideas\nof combining FL and control. The methodical benefits are further discussed,\nculminating in a detailed overview of expected applications, from dynamical\nsystem modeling over controller design, focusing on adaptive control, to\nknowledge transfer in multi-agent decision-making systems.\n", "link": "http://arxiv.org/abs/2407.11069v2", "date": "2024-07-17", "relevancy": 2.0138, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5213}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4967}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Federated%20Learning%20and%20Control%3A%20A%20Survey&body=Title%3A%20Combining%20Federated%20Learning%20and%20Control%3A%20A%20Survey%0AAuthor%3A%20Jakob%20Weber%20and%20Markus%20Gurtner%20and%20Amadeus%20Lobe%20and%20Adrian%20Trachte%20and%20Andreas%20Kugi%0AAbstract%3A%20%20%20This%20survey%20provides%20an%20overview%20of%20combining%20Federated%20Learning%20%28FL%29%20and%0Acontrol%20to%20enhance%20adaptability%2C%20scalability%2C%20generalization%2C%20and%20privacy%20in%0A%28nonlinear%29%20control%20applications.%20Traditional%20control%20methods%20rely%20on%0Acontroller%20design%20models%2C%20but%20real-world%20scenarios%20often%20require%20online%20model%0Aretuning%20or%20learning.%20FL%20offers%20a%20distributed%20approach%20to%20model%20training%2C%0Aenabling%20collaborative%20learning%20across%20distributed%20devices%20while%20preserving%0Adata%20privacy.%20By%20keeping%20data%20localized%2C%20FL%20mitigates%20concerns%20regarding%0Aprivacy%20and%20security%20while%20reducing%20network%20bandwidth%20requirements%20for%0Acommunication.%20This%20survey%20summarizes%20the%20state-of-the-art%20concepts%20and%20ideas%0Aof%20combining%20FL%20and%20control.%20The%20methodical%20benefits%20are%20further%20discussed%2C%0Aculminating%20in%20a%20detailed%20overview%20of%20expected%20applications%2C%20from%20dynamical%0Asystem%20modeling%20over%20controller%20design%2C%20focusing%20on%20adaptive%20control%2C%20to%0Aknowledge%20transfer%20in%20multi-agent%20decision-making%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Federated%2520Learning%2520and%2520Control%253A%2520A%2520Survey%26entry.906535625%3DJakob%2520Weber%2520and%2520Markus%2520Gurtner%2520and%2520Amadeus%2520Lobe%2520and%2520Adrian%2520Trachte%2520and%2520Andreas%2520Kugi%26entry.1292438233%3D%2520%2520This%2520survey%2520provides%2520an%2520overview%2520of%2520combining%2520Federated%2520Learning%2520%2528FL%2529%2520and%250Acontrol%2520to%2520enhance%2520adaptability%252C%2520scalability%252C%2520generalization%252C%2520and%2520privacy%2520in%250A%2528nonlinear%2529%2520control%2520applications.%2520Traditional%2520control%2520methods%2520rely%2520on%250Acontroller%2520design%2520models%252C%2520but%2520real-world%2520scenarios%2520often%2520require%2520online%2520model%250Aretuning%2520or%2520learning.%2520FL%2520offers%2520a%2520distributed%2520approach%2520to%2520model%2520training%252C%250Aenabling%2520collaborative%2520learning%2520across%2520distributed%2520devices%2520while%2520preserving%250Adata%2520privacy.%2520By%2520keeping%2520data%2520localized%252C%2520FL%2520mitigates%2520concerns%2520regarding%250Aprivacy%2520and%2520security%2520while%2520reducing%2520network%2520bandwidth%2520requirements%2520for%250Acommunication.%2520This%2520survey%2520summarizes%2520the%2520state-of-the-art%2520concepts%2520and%2520ideas%250Aof%2520combining%2520FL%2520and%2520control.%2520The%2520methodical%2520benefits%2520are%2520further%2520discussed%252C%250Aculminating%2520in%2520a%2520detailed%2520overview%2520of%2520expected%2520applications%252C%2520from%2520dynamical%250Asystem%2520modeling%2520over%2520controller%2520design%252C%2520focusing%2520on%2520adaptive%2520control%252C%2520to%250Aknowledge%2520transfer%2520in%2520multi-agent%2520decision-making%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Federated%20Learning%20and%20Control%3A%20A%20Survey&entry.906535625=Jakob%20Weber%20and%20Markus%20Gurtner%20and%20Amadeus%20Lobe%20and%20Adrian%20Trachte%20and%20Andreas%20Kugi&entry.1292438233=%20%20This%20survey%20provides%20an%20overview%20of%20combining%20Federated%20Learning%20%28FL%29%20and%0Acontrol%20to%20enhance%20adaptability%2C%20scalability%2C%20generalization%2C%20and%20privacy%20in%0A%28nonlinear%29%20control%20applications.%20Traditional%20control%20methods%20rely%20on%0Acontroller%20design%20models%2C%20but%20real-world%20scenarios%20often%20require%20online%20model%0Aretuning%20or%20learning.%20FL%20offers%20a%20distributed%20approach%20to%20model%20training%2C%0Aenabling%20collaborative%20learning%20across%20distributed%20devices%20while%20preserving%0Adata%20privacy.%20By%20keeping%20data%20localized%2C%20FL%20mitigates%20concerns%20regarding%0Aprivacy%20and%20security%20while%20reducing%20network%20bandwidth%20requirements%20for%0Acommunication.%20This%20survey%20summarizes%20the%20state-of-the-art%20concepts%20and%20ideas%0Aof%20combining%20FL%20and%20control.%20The%20methodical%20benefits%20are%20further%20discussed%2C%0Aculminating%20in%20a%20detailed%20overview%20of%20expected%20applications%2C%20from%20dynamical%0Asystem%20modeling%20over%20controller%20design%2C%20focusing%20on%20adaptive%20control%2C%20to%0Aknowledge%20transfer%20in%20multi-agent%20decision-making%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11069v2&entry.124074799=Read"},
{"title": "Auto-Regressive Next-Token Predictors are Universal Learners", "author": "Eran Malach", "abstract": "  Large language models display remarkable capabilities in logical and\nmathematical reasoning, allowing them to solve complex tasks. Interestingly,\nthese abilities emerge in networks trained on the simple task of next-token\nprediction. In this work, we present a theoretical framework for studying\nauto-regressive next-token predictors. We demonstrate that even simple models\nsuch as linear next-token predictors, trained on Chain-of-Thought (CoT) data,\ncan approximate any function efficiently computed by a Turing machine. We\nintroduce a new complexity measure -- length complexity -- which measures the\nnumber of intermediate tokens in a CoT sequence required to approximate some\ntarget function, and analyze the interplay between length complexity and other\nnotions of complexity. Finally, we show experimentally that simple next-token\npredictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs),\ndisplay non-trivial performance on text generation and arithmetic tasks. Our\nresults demonstrate that the power of today's LLMs can be attributed, to a\ngreat extent, to the auto-regressive next-token training scheme, and not\nnecessarily to a particular choice of architecture.\n", "link": "http://arxiv.org/abs/2309.06979v2", "date": "2024-07-17", "relevancy": 2.0077, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5151}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4987}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Regressive%20Next-Token%20Predictors%20are%20Universal%20Learners&body=Title%3A%20Auto-Regressive%20Next-Token%20Predictors%20are%20Universal%20Learners%0AAuthor%3A%20Eran%20Malach%0AAbstract%3A%20%20%20Large%20language%20models%20display%20remarkable%20capabilities%20in%20logical%20and%0Amathematical%20reasoning%2C%20allowing%20them%20to%20solve%20complex%20tasks.%20Interestingly%2C%0Athese%20abilities%20emerge%20in%20networks%20trained%20on%20the%20simple%20task%20of%20next-token%0Aprediction.%20In%20this%20work%2C%20we%20present%20a%20theoretical%20framework%20for%20studying%0Aauto-regressive%20next-token%20predictors.%20We%20demonstrate%20that%20even%20simple%20models%0Asuch%20as%20linear%20next-token%20predictors%2C%20trained%20on%20Chain-of-Thought%20%28CoT%29%20data%2C%0Acan%20approximate%20any%20function%20efficiently%20computed%20by%20a%20Turing%20machine.%20We%0Aintroduce%20a%20new%20complexity%20measure%20--%20length%20complexity%20--%20which%20measures%20the%0Anumber%20of%20intermediate%20tokens%20in%20a%20CoT%20sequence%20required%20to%20approximate%20some%0Atarget%20function%2C%20and%20analyze%20the%20interplay%20between%20length%20complexity%20and%20other%0Anotions%20of%20complexity.%20Finally%2C%20we%20show%20experimentally%20that%20simple%20next-token%0Apredictors%2C%20such%20as%20linear%20networks%20and%20shallow%20Multi-Layer%20Perceptrons%20%28MLPs%29%2C%0Adisplay%20non-trivial%20performance%20on%20text%20generation%20and%20arithmetic%20tasks.%20Our%0Aresults%20demonstrate%20that%20the%20power%20of%20today%27s%20LLMs%20can%20be%20attributed%2C%20to%20a%0Agreat%20extent%2C%20to%20the%20auto-regressive%20next-token%20training%20scheme%2C%20and%20not%0Anecessarily%20to%20a%20particular%20choice%20of%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Regressive%2520Next-Token%2520Predictors%2520are%2520Universal%2520Learners%26entry.906535625%3DEran%2520Malach%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520display%2520remarkable%2520capabilities%2520in%2520logical%2520and%250Amathematical%2520reasoning%252C%2520allowing%2520them%2520to%2520solve%2520complex%2520tasks.%2520Interestingly%252C%250Athese%2520abilities%2520emerge%2520in%2520networks%2520trained%2520on%2520the%2520simple%2520task%2520of%2520next-token%250Aprediction.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520theoretical%2520framework%2520for%2520studying%250Aauto-regressive%2520next-token%2520predictors.%2520We%2520demonstrate%2520that%2520even%2520simple%2520models%250Asuch%2520as%2520linear%2520next-token%2520predictors%252C%2520trained%2520on%2520Chain-of-Thought%2520%2528CoT%2529%2520data%252C%250Acan%2520approximate%2520any%2520function%2520efficiently%2520computed%2520by%2520a%2520Turing%2520machine.%2520We%250Aintroduce%2520a%2520new%2520complexity%2520measure%2520--%2520length%2520complexity%2520--%2520which%2520measures%2520the%250Anumber%2520of%2520intermediate%2520tokens%2520in%2520a%2520CoT%2520sequence%2520required%2520to%2520approximate%2520some%250Atarget%2520function%252C%2520and%2520analyze%2520the%2520interplay%2520between%2520length%2520complexity%2520and%2520other%250Anotions%2520of%2520complexity.%2520Finally%252C%2520we%2520show%2520experimentally%2520that%2520simple%2520next-token%250Apredictors%252C%2520such%2520as%2520linear%2520networks%2520and%2520shallow%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%252C%250Adisplay%2520non-trivial%2520performance%2520on%2520text%2520generation%2520and%2520arithmetic%2520tasks.%2520Our%250Aresults%2520demonstrate%2520that%2520the%2520power%2520of%2520today%2527s%2520LLMs%2520can%2520be%2520attributed%252C%2520to%2520a%250Agreat%2520extent%252C%2520to%2520the%2520auto-regressive%2520next-token%2520training%2520scheme%252C%2520and%2520not%250Anecessarily%2520to%2520a%2520particular%2520choice%2520of%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Regressive%20Next-Token%20Predictors%20are%20Universal%20Learners&entry.906535625=Eran%20Malach&entry.1292438233=%20%20Large%20language%20models%20display%20remarkable%20capabilities%20in%20logical%20and%0Amathematical%20reasoning%2C%20allowing%20them%20to%20solve%20complex%20tasks.%20Interestingly%2C%0Athese%20abilities%20emerge%20in%20networks%20trained%20on%20the%20simple%20task%20of%20next-token%0Aprediction.%20In%20this%20work%2C%20we%20present%20a%20theoretical%20framework%20for%20studying%0Aauto-regressive%20next-token%20predictors.%20We%20demonstrate%20that%20even%20simple%20models%0Asuch%20as%20linear%20next-token%20predictors%2C%20trained%20on%20Chain-of-Thought%20%28CoT%29%20data%2C%0Acan%20approximate%20any%20function%20efficiently%20computed%20by%20a%20Turing%20machine.%20We%0Aintroduce%20a%20new%20complexity%20measure%20--%20length%20complexity%20--%20which%20measures%20the%0Anumber%20of%20intermediate%20tokens%20in%20a%20CoT%20sequence%20required%20to%20approximate%20some%0Atarget%20function%2C%20and%20analyze%20the%20interplay%20between%20length%20complexity%20and%20other%0Anotions%20of%20complexity.%20Finally%2C%20we%20show%20experimentally%20that%20simple%20next-token%0Apredictors%2C%20such%20as%20linear%20networks%20and%20shallow%20Multi-Layer%20Perceptrons%20%28MLPs%29%2C%0Adisplay%20non-trivial%20performance%20on%20text%20generation%20and%20arithmetic%20tasks.%20Our%0Aresults%20demonstrate%20that%20the%20power%20of%20today%27s%20LLMs%20can%20be%20attributed%2C%20to%20a%0Agreat%20extent%2C%20to%20the%20auto-regressive%20next-token%20training%20scheme%2C%20and%20not%0Anecessarily%20to%20a%20particular%20choice%20of%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06979v2&entry.124074799=Read"},
{"title": "How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced\n  Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation", "author": "Andr\u00e9 Ferreira and Naida Solak and Jianning Li and Philipp Dammann and Jens Kleesiek and Victor Alves and Jan Egger", "abstract": "  Deep Learning is the state-of-the-art technology for segmenting brain\ntumours. However, this requires a lot of high-quality data, which is difficult\nto obtain, especially in the medical field. Therefore, our solutions address\nthis problem by using unconventional mechanisms for data augmentation.\nGenerative adversarial networks and registration are used to massively increase\nthe amount of available samples for training three different deep learning\nmodels for brain tumour segmentation, the first task of the BraTS2023\nchallenge. The first model is the standard nnU-Net, the second is the Swin\nUNETR and the third is the winning solution of the BraTS 2021 Challenge. The\nentire pipeline is built on the nnU-Net implementation, except for the\ngeneration of the synthetic data. The use of convolutional algorithms and\ntransformers is able to fill each other's knowledge gaps. Using the new metric,\nour best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95\n14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the\nvalidation set.\n", "link": "http://arxiv.org/abs/2402.17317v2", "date": "2024-07-17", "relevancy": 1.993, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5092}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4965}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20we%20won%20BraTS%202023%20Adult%20Glioma%20challenge%3F%20Just%20faking%20it%21%20Enhanced%0A%20%20Synthetic%20Data%20Augmentation%20and%20Model%20Ensemble%20for%20brain%20tumour%20segmentation&body=Title%3A%20How%20we%20won%20BraTS%202023%20Adult%20Glioma%20challenge%3F%20Just%20faking%20it%21%20Enhanced%0A%20%20Synthetic%20Data%20Augmentation%20and%20Model%20Ensemble%20for%20brain%20tumour%20segmentation%0AAuthor%3A%20Andr%C3%A9%20Ferreira%20and%20Naida%20Solak%20and%20Jianning%20Li%20and%20Philipp%20Dammann%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger%0AAbstract%3A%20%20%20Deep%20Learning%20is%20the%20state-of-the-art%20technology%20for%20segmenting%20brain%0Atumours.%20However%2C%20this%20requires%20a%20lot%20of%20high-quality%20data%2C%20which%20is%20difficult%0Ato%20obtain%2C%20especially%20in%20the%20medical%20field.%20Therefore%2C%20our%20solutions%20address%0Athis%20problem%20by%20using%20unconventional%20mechanisms%20for%20data%20augmentation.%0AGenerative%20adversarial%20networks%20and%20registration%20are%20used%20to%20massively%20increase%0Athe%20amount%20of%20available%20samples%20for%20training%20three%20different%20deep%20learning%0Amodels%20for%20brain%20tumour%20segmentation%2C%20the%20first%20task%20of%20the%20BraTS2023%0Achallenge.%20The%20first%20model%20is%20the%20standard%20nnU-Net%2C%20the%20second%20is%20the%20Swin%0AUNETR%20and%20the%20third%20is%20the%20winning%20solution%20of%20the%20BraTS%202021%20Challenge.%20The%0Aentire%20pipeline%20is%20built%20on%20the%20nnU-Net%20implementation%2C%20except%20for%20the%0Ageneration%20of%20the%20synthetic%20data.%20The%20use%20of%20convolutional%20algorithms%20and%0Atransformers%20is%20able%20to%20fill%20each%20other%27s%20knowledge%20gaps.%20Using%20the%20new%20metric%2C%0Aour%20best%20solution%20achieves%20the%20dice%20results%200.9005%2C%200.8673%2C%200.8509%20and%20HD95%0A14.940%2C%2014.467%2C%2017.699%20%28whole%20tumour%2C%20tumour%20core%20and%20enhancing%20tumour%29%20in%20the%0Avalidation%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520we%2520won%2520BraTS%25202023%2520Adult%2520Glioma%2520challenge%253F%2520Just%2520faking%2520it%2521%2520Enhanced%250A%2520%2520Synthetic%2520Data%2520Augmentation%2520and%2520Model%2520Ensemble%2520for%2520brain%2520tumour%2520segmentation%26entry.906535625%3DAndr%25C3%25A9%2520Ferreira%2520and%2520Naida%2520Solak%2520and%2520Jianning%2520Li%2520and%2520Philipp%2520Dammann%2520and%2520Jens%2520Kleesiek%2520and%2520Victor%2520Alves%2520and%2520Jan%2520Egger%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520is%2520the%2520state-of-the-art%2520technology%2520for%2520segmenting%2520brain%250Atumours.%2520However%252C%2520this%2520requires%2520a%2520lot%2520of%2520high-quality%2520data%252C%2520which%2520is%2520difficult%250Ato%2520obtain%252C%2520especially%2520in%2520the%2520medical%2520field.%2520Therefore%252C%2520our%2520solutions%2520address%250Athis%2520problem%2520by%2520using%2520unconventional%2520mechanisms%2520for%2520data%2520augmentation.%250AGenerative%2520adversarial%2520networks%2520and%2520registration%2520are%2520used%2520to%2520massively%2520increase%250Athe%2520amount%2520of%2520available%2520samples%2520for%2520training%2520three%2520different%2520deep%2520learning%250Amodels%2520for%2520brain%2520tumour%2520segmentation%252C%2520the%2520first%2520task%2520of%2520the%2520BraTS2023%250Achallenge.%2520The%2520first%2520model%2520is%2520the%2520standard%2520nnU-Net%252C%2520the%2520second%2520is%2520the%2520Swin%250AUNETR%2520and%2520the%2520third%2520is%2520the%2520winning%2520solution%2520of%2520the%2520BraTS%25202021%2520Challenge.%2520The%250Aentire%2520pipeline%2520is%2520built%2520on%2520the%2520nnU-Net%2520implementation%252C%2520except%2520for%2520the%250Ageneration%2520of%2520the%2520synthetic%2520data.%2520The%2520use%2520of%2520convolutional%2520algorithms%2520and%250Atransformers%2520is%2520able%2520to%2520fill%2520each%2520other%2527s%2520knowledge%2520gaps.%2520Using%2520the%2520new%2520metric%252C%250Aour%2520best%2520solution%2520achieves%2520the%2520dice%2520results%25200.9005%252C%25200.8673%252C%25200.8509%2520and%2520HD95%250A14.940%252C%252014.467%252C%252017.699%2520%2528whole%2520tumour%252C%2520tumour%2520core%2520and%2520enhancing%2520tumour%2529%2520in%2520the%250Avalidation%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20we%20won%20BraTS%202023%20Adult%20Glioma%20challenge%3F%20Just%20faking%20it%21%20Enhanced%0A%20%20Synthetic%20Data%20Augmentation%20and%20Model%20Ensemble%20for%20brain%20tumour%20segmentation&entry.906535625=Andr%C3%A9%20Ferreira%20and%20Naida%20Solak%20and%20Jianning%20Li%20and%20Philipp%20Dammann%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger&entry.1292438233=%20%20Deep%20Learning%20is%20the%20state-of-the-art%20technology%20for%20segmenting%20brain%0Atumours.%20However%2C%20this%20requires%20a%20lot%20of%20high-quality%20data%2C%20which%20is%20difficult%0Ato%20obtain%2C%20especially%20in%20the%20medical%20field.%20Therefore%2C%20our%20solutions%20address%0Athis%20problem%20by%20using%20unconventional%20mechanisms%20for%20data%20augmentation.%0AGenerative%20adversarial%20networks%20and%20registration%20are%20used%20to%20massively%20increase%0Athe%20amount%20of%20available%20samples%20for%20training%20three%20different%20deep%20learning%0Amodels%20for%20brain%20tumour%20segmentation%2C%20the%20first%20task%20of%20the%20BraTS2023%0Achallenge.%20The%20first%20model%20is%20the%20standard%20nnU-Net%2C%20the%20second%20is%20the%20Swin%0AUNETR%20and%20the%20third%20is%20the%20winning%20solution%20of%20the%20BraTS%202021%20Challenge.%20The%0Aentire%20pipeline%20is%20built%20on%20the%20nnU-Net%20implementation%2C%20except%20for%20the%0Ageneration%20of%20the%20synthetic%20data.%20The%20use%20of%20convolutional%20algorithms%20and%0Atransformers%20is%20able%20to%20fill%20each%20other%27s%20knowledge%20gaps.%20Using%20the%20new%20metric%2C%0Aour%20best%20solution%20achieves%20the%20dice%20results%200.9005%2C%200.8673%2C%200.8509%20and%20HD95%0A14.940%2C%2014.467%2C%2017.699%20%28whole%20tumour%2C%20tumour%20core%20and%20enhancing%20tumour%29%20in%20the%0Avalidation%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17317v2&entry.124074799=Read"},
{"title": "Scissorhands: Scrub Data Influence via Connection Sensitivity in\n  Networks", "author": "Jing Wu and Mehrtash Harandi", "abstract": "  Machine unlearning has become a pivotal task to erase the influence of data\nfrom a trained model. It adheres to recent data regulation standards and\nenhances the privacy and security of machine learning applications. In this\nwork, we present a new machine unlearning approach Scissorhands. Initially,\nScissorhands identifies the most pertinent parameters in the given model\nrelative to the forgetting data via connection sensitivity. By reinitializing\nthe most influential top-k percent of these parameters, a trimmed model for\nerasing the influence of the forgetting data is obtained. Subsequently,\nScissorhands fine-tunes the trimmed model with a gradient projection-based\napproach, seeking parameters that preserve information on the remaining data\nwhile discarding information related to the forgetting data. Our experimental\nresults, conducted across image classification and image generation tasks,\ndemonstrate that Scissorhands, showcases competitive performance when compared\nto existing methods. Source code is available at\nhttps://github.com/JingWu321/Scissorhands.\n", "link": "http://arxiv.org/abs/2401.06187v3", "date": "2024-07-17", "relevancy": 1.9872, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5021}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4975}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scissorhands%3A%20Scrub%20Data%20Influence%20via%20Connection%20Sensitivity%20in%0A%20%20Networks&body=Title%3A%20Scissorhands%3A%20Scrub%20Data%20Influence%20via%20Connection%20Sensitivity%20in%0A%20%20Networks%0AAuthor%3A%20Jing%20Wu%20and%20Mehrtash%20Harandi%0AAbstract%3A%20%20%20Machine%20unlearning%20has%20become%20a%20pivotal%20task%20to%20erase%20the%20influence%20of%20data%0Afrom%20a%20trained%20model.%20It%20adheres%20to%20recent%20data%20regulation%20standards%20and%0Aenhances%20the%20privacy%20and%20security%20of%20machine%20learning%20applications.%20In%20this%0Awork%2C%20we%20present%20a%20new%20machine%20unlearning%20approach%20Scissorhands.%20Initially%2C%0AScissorhands%20identifies%20the%20most%20pertinent%20parameters%20in%20the%20given%20model%0Arelative%20to%20the%20forgetting%20data%20via%20connection%20sensitivity.%20By%20reinitializing%0Athe%20most%20influential%20top-k%20percent%20of%20these%20parameters%2C%20a%20trimmed%20model%20for%0Aerasing%20the%20influence%20of%20the%20forgetting%20data%20is%20obtained.%20Subsequently%2C%0AScissorhands%20fine-tunes%20the%20trimmed%20model%20with%20a%20gradient%20projection-based%0Aapproach%2C%20seeking%20parameters%20that%20preserve%20information%20on%20the%20remaining%20data%0Awhile%20discarding%20information%20related%20to%20the%20forgetting%20data.%20Our%20experimental%0Aresults%2C%20conducted%20across%20image%20classification%20and%20image%20generation%20tasks%2C%0Ademonstrate%20that%20Scissorhands%2C%20showcases%20competitive%20performance%20when%20compared%0Ato%20existing%20methods.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/JingWu321/Scissorhands.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06187v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScissorhands%253A%2520Scrub%2520Data%2520Influence%2520via%2520Connection%2520Sensitivity%2520in%250A%2520%2520Networks%26entry.906535625%3DJing%2520Wu%2520and%2520Mehrtash%2520Harandi%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520has%2520become%2520a%2520pivotal%2520task%2520to%2520erase%2520the%2520influence%2520of%2520data%250Afrom%2520a%2520trained%2520model.%2520It%2520adheres%2520to%2520recent%2520data%2520regulation%2520standards%2520and%250Aenhances%2520the%2520privacy%2520and%2520security%2520of%2520machine%2520learning%2520applications.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520new%2520machine%2520unlearning%2520approach%2520Scissorhands.%2520Initially%252C%250AScissorhands%2520identifies%2520the%2520most%2520pertinent%2520parameters%2520in%2520the%2520given%2520model%250Arelative%2520to%2520the%2520forgetting%2520data%2520via%2520connection%2520sensitivity.%2520By%2520reinitializing%250Athe%2520most%2520influential%2520top-k%2520percent%2520of%2520these%2520parameters%252C%2520a%2520trimmed%2520model%2520for%250Aerasing%2520the%2520influence%2520of%2520the%2520forgetting%2520data%2520is%2520obtained.%2520Subsequently%252C%250AScissorhands%2520fine-tunes%2520the%2520trimmed%2520model%2520with%2520a%2520gradient%2520projection-based%250Aapproach%252C%2520seeking%2520parameters%2520that%2520preserve%2520information%2520on%2520the%2520remaining%2520data%250Awhile%2520discarding%2520information%2520related%2520to%2520the%2520forgetting%2520data.%2520Our%2520experimental%250Aresults%252C%2520conducted%2520across%2520image%2520classification%2520and%2520image%2520generation%2520tasks%252C%250Ademonstrate%2520that%2520Scissorhands%252C%2520showcases%2520competitive%2520performance%2520when%2520compared%250Ato%2520existing%2520methods.%2520Source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JingWu321/Scissorhands.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06187v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scissorhands%3A%20Scrub%20Data%20Influence%20via%20Connection%20Sensitivity%20in%0A%20%20Networks&entry.906535625=Jing%20Wu%20and%20Mehrtash%20Harandi&entry.1292438233=%20%20Machine%20unlearning%20has%20become%20a%20pivotal%20task%20to%20erase%20the%20influence%20of%20data%0Afrom%20a%20trained%20model.%20It%20adheres%20to%20recent%20data%20regulation%20standards%20and%0Aenhances%20the%20privacy%20and%20security%20of%20machine%20learning%20applications.%20In%20this%0Awork%2C%20we%20present%20a%20new%20machine%20unlearning%20approach%20Scissorhands.%20Initially%2C%0AScissorhands%20identifies%20the%20most%20pertinent%20parameters%20in%20the%20given%20model%0Arelative%20to%20the%20forgetting%20data%20via%20connection%20sensitivity.%20By%20reinitializing%0Athe%20most%20influential%20top-k%20percent%20of%20these%20parameters%2C%20a%20trimmed%20model%20for%0Aerasing%20the%20influence%20of%20the%20forgetting%20data%20is%20obtained.%20Subsequently%2C%0AScissorhands%20fine-tunes%20the%20trimmed%20model%20with%20a%20gradient%20projection-based%0Aapproach%2C%20seeking%20parameters%20that%20preserve%20information%20on%20the%20remaining%20data%0Awhile%20discarding%20information%20related%20to%20the%20forgetting%20data.%20Our%20experimental%0Aresults%2C%20conducted%20across%20image%20classification%20and%20image%20generation%20tasks%2C%0Ademonstrate%20that%20Scissorhands%2C%20showcases%20competitive%20performance%20when%20compared%0Ato%20existing%20methods.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/JingWu321/Scissorhands.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06187v3&entry.124074799=Read"},
{"title": "Minimum discrepancy principle strategy for choosing $k$ in $k$-NN\n  regression", "author": "Yaroslav Averyanov and Alain Celisse", "abstract": "  We present a novel data-driven strategy to choose the hyperparameter $k$ in\nthe $k$-NN regression estimator without using any hold-out data. We treat the\nproblem of choosing the hyperparameter as an iterative procedure (over $k$) and\npropose using an easily implemented in practice strategy based on the idea of\nearly stopping and the minimum discrepancy principle. This model selection\nstrategy is proven to be minimax-optimal over some smoothness function classes,\nfor instance, the Lipschitz functions class on a bounded domain. The novel\nmethod often improves statistical performance on artificial and real-world data\nsets in comparison to other model selection strategies, such as the Hold-out\nmethod, 5-fold cross-validation, and AIC criterion. The novelty of the strategy\ncomes from reducing the computational time of the model selection procedure\nwhile preserving the statistical (minimax) optimality of the resulting\nestimator. More precisely, given a sample of size $n$, if one should choose $k$\namong $\\left\\{ 1, \\ldots, n \\right\\}$, and $\\left\\{ f^1, \\ldots, f^n \\right\\}$\nare the estimators of the regression function, the minimum discrepancy\nprinciple requires the calculation of a fraction of the estimators, while this\nis not the case for the generalized cross-validation, Akaike's AIC criteria, or\nLepskii principle.\n", "link": "http://arxiv.org/abs/2008.08718v8", "date": "2024-07-17", "relevancy": 1.9764, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4341}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3773}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimum%20discrepancy%20principle%20strategy%20for%20choosing%20%24k%24%20in%20%24k%24-NN%0A%20%20regression&body=Title%3A%20Minimum%20discrepancy%20principle%20strategy%20for%20choosing%20%24k%24%20in%20%24k%24-NN%0A%20%20regression%0AAuthor%3A%20Yaroslav%20Averyanov%20and%20Alain%20Celisse%0AAbstract%3A%20%20%20We%20present%20a%20novel%20data-driven%20strategy%20to%20choose%20the%20hyperparameter%20%24k%24%20in%0Athe%20%24k%24-NN%20regression%20estimator%20without%20using%20any%20hold-out%20data.%20We%20treat%20the%0Aproblem%20of%20choosing%20the%20hyperparameter%20as%20an%20iterative%20procedure%20%28over%20%24k%24%29%20and%0Apropose%20using%20an%20easily%20implemented%20in%20practice%20strategy%20based%20on%20the%20idea%20of%0Aearly%20stopping%20and%20the%20minimum%20discrepancy%20principle.%20This%20model%20selection%0Astrategy%20is%20proven%20to%20be%20minimax-optimal%20over%20some%20smoothness%20function%20classes%2C%0Afor%20instance%2C%20the%20Lipschitz%20functions%20class%20on%20a%20bounded%20domain.%20The%20novel%0Amethod%20often%20improves%20statistical%20performance%20on%20artificial%20and%20real-world%20data%0Asets%20in%20comparison%20to%20other%20model%20selection%20strategies%2C%20such%20as%20the%20Hold-out%0Amethod%2C%205-fold%20cross-validation%2C%20and%20AIC%20criterion.%20The%20novelty%20of%20the%20strategy%0Acomes%20from%20reducing%20the%20computational%20time%20of%20the%20model%20selection%20procedure%0Awhile%20preserving%20the%20statistical%20%28minimax%29%20optimality%20of%20the%20resulting%0Aestimator.%20More%20precisely%2C%20given%20a%20sample%20of%20size%20%24n%24%2C%20if%20one%20should%20choose%20%24k%24%0Aamong%20%24%5Cleft%5C%7B%201%2C%20%5Cldots%2C%20n%20%5Cright%5C%7D%24%2C%20and%20%24%5Cleft%5C%7B%20f%5E1%2C%20%5Cldots%2C%20f%5En%20%5Cright%5C%7D%24%0Aare%20the%20estimators%20of%20the%20regression%20function%2C%20the%20minimum%20discrepancy%0Aprinciple%20requires%20the%20calculation%20of%20a%20fraction%20of%20the%20estimators%2C%20while%20this%0Ais%20not%20the%20case%20for%20the%20generalized%20cross-validation%2C%20Akaike%27s%20AIC%20criteria%2C%20or%0ALepskii%20principle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2008.08718v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimum%2520discrepancy%2520principle%2520strategy%2520for%2520choosing%2520%2524k%2524%2520in%2520%2524k%2524-NN%250A%2520%2520regression%26entry.906535625%3DYaroslav%2520Averyanov%2520and%2520Alain%2520Celisse%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520data-driven%2520strategy%2520to%2520choose%2520the%2520hyperparameter%2520%2524k%2524%2520in%250Athe%2520%2524k%2524-NN%2520regression%2520estimator%2520without%2520using%2520any%2520hold-out%2520data.%2520We%2520treat%2520the%250Aproblem%2520of%2520choosing%2520the%2520hyperparameter%2520as%2520an%2520iterative%2520procedure%2520%2528over%2520%2524k%2524%2529%2520and%250Apropose%2520using%2520an%2520easily%2520implemented%2520in%2520practice%2520strategy%2520based%2520on%2520the%2520idea%2520of%250Aearly%2520stopping%2520and%2520the%2520minimum%2520discrepancy%2520principle.%2520This%2520model%2520selection%250Astrategy%2520is%2520proven%2520to%2520be%2520minimax-optimal%2520over%2520some%2520smoothness%2520function%2520classes%252C%250Afor%2520instance%252C%2520the%2520Lipschitz%2520functions%2520class%2520on%2520a%2520bounded%2520domain.%2520The%2520novel%250Amethod%2520often%2520improves%2520statistical%2520performance%2520on%2520artificial%2520and%2520real-world%2520data%250Asets%2520in%2520comparison%2520to%2520other%2520model%2520selection%2520strategies%252C%2520such%2520as%2520the%2520Hold-out%250Amethod%252C%25205-fold%2520cross-validation%252C%2520and%2520AIC%2520criterion.%2520The%2520novelty%2520of%2520the%2520strategy%250Acomes%2520from%2520reducing%2520the%2520computational%2520time%2520of%2520the%2520model%2520selection%2520procedure%250Awhile%2520preserving%2520the%2520statistical%2520%2528minimax%2529%2520optimality%2520of%2520the%2520resulting%250Aestimator.%2520More%2520precisely%252C%2520given%2520a%2520sample%2520of%2520size%2520%2524n%2524%252C%2520if%2520one%2520should%2520choose%2520%2524k%2524%250Aamong%2520%2524%255Cleft%255C%257B%25201%252C%2520%255Cldots%252C%2520n%2520%255Cright%255C%257D%2524%252C%2520and%2520%2524%255Cleft%255C%257B%2520f%255E1%252C%2520%255Cldots%252C%2520f%255En%2520%255Cright%255C%257D%2524%250Aare%2520the%2520estimators%2520of%2520the%2520regression%2520function%252C%2520the%2520minimum%2520discrepancy%250Aprinciple%2520requires%2520the%2520calculation%2520of%2520a%2520fraction%2520of%2520the%2520estimators%252C%2520while%2520this%250Ais%2520not%2520the%2520case%2520for%2520the%2520generalized%2520cross-validation%252C%2520Akaike%2527s%2520AIC%2520criteria%252C%2520or%250ALepskii%2520principle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2008.08718v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimum%20discrepancy%20principle%20strategy%20for%20choosing%20%24k%24%20in%20%24k%24-NN%0A%20%20regression&entry.906535625=Yaroslav%20Averyanov%20and%20Alain%20Celisse&entry.1292438233=%20%20We%20present%20a%20novel%20data-driven%20strategy%20to%20choose%20the%20hyperparameter%20%24k%24%20in%0Athe%20%24k%24-NN%20regression%20estimator%20without%20using%20any%20hold-out%20data.%20We%20treat%20the%0Aproblem%20of%20choosing%20the%20hyperparameter%20as%20an%20iterative%20procedure%20%28over%20%24k%24%29%20and%0Apropose%20using%20an%20easily%20implemented%20in%20practice%20strategy%20based%20on%20the%20idea%20of%0Aearly%20stopping%20and%20the%20minimum%20discrepancy%20principle.%20This%20model%20selection%0Astrategy%20is%20proven%20to%20be%20minimax-optimal%20over%20some%20smoothness%20function%20classes%2C%0Afor%20instance%2C%20the%20Lipschitz%20functions%20class%20on%20a%20bounded%20domain.%20The%20novel%0Amethod%20often%20improves%20statistical%20performance%20on%20artificial%20and%20real-world%20data%0Asets%20in%20comparison%20to%20other%20model%20selection%20strategies%2C%20such%20as%20the%20Hold-out%0Amethod%2C%205-fold%20cross-validation%2C%20and%20AIC%20criterion.%20The%20novelty%20of%20the%20strategy%0Acomes%20from%20reducing%20the%20computational%20time%20of%20the%20model%20selection%20procedure%0Awhile%20preserving%20the%20statistical%20%28minimax%29%20optimality%20of%20the%20resulting%0Aestimator.%20More%20precisely%2C%20given%20a%20sample%20of%20size%20%24n%24%2C%20if%20one%20should%20choose%20%24k%24%0Aamong%20%24%5Cleft%5C%7B%201%2C%20%5Cldots%2C%20n%20%5Cright%5C%7D%24%2C%20and%20%24%5Cleft%5C%7B%20f%5E1%2C%20%5Cldots%2C%20f%5En%20%5Cright%5C%7D%24%0Aare%20the%20estimators%20of%20the%20regression%20function%2C%20the%20minimum%20discrepancy%0Aprinciple%20requires%20the%20calculation%20of%20a%20fraction%20of%20the%20estimators%2C%20while%20this%0Ais%20not%20the%20case%20for%20the%20generalized%20cross-validation%2C%20Akaike%27s%20AIC%20criteria%2C%20or%0ALepskii%20principle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2008.08718v8&entry.124074799=Read"},
{"title": "Self-play with Execution Feedback: Improving Instruction-following\n  Capabilities of Large Language Models", "author": "Guanting Dong and Keming Lu and Chengpeng Li and Tingyu Xia and Bowen Yu and Chang Zhou and Jingren Zhou", "abstract": "  One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.\n", "link": "http://arxiv.org/abs/2406.13542v2", "date": "2024-07-17", "relevancy": 1.96, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.502}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4904}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-play%20with%20Execution%20Feedback%3A%20Improving%20Instruction-following%0A%20%20Capabilities%20of%20Large%20Language%20Models&body=Title%3A%20Self-play%20with%20Execution%20Feedback%3A%20Improving%20Instruction-following%0A%20%20Capabilities%20of%20Large%20Language%20Models%0AAuthor%3A%20Guanting%20Dong%20and%20Keming%20Lu%20and%20Chengpeng%20Li%20and%20Tingyu%20Xia%20and%20Bowen%20Yu%20and%20Chang%20Zhou%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20One%20core%20capability%20of%20large%20language%20models%20%28LLMs%29%20is%20to%20follow%20natural%0Alanguage%20instructions.%20However%2C%20the%20issue%20of%20automatically%20constructing%0Ahigh-quality%20training%20data%20to%20enhance%20the%20complex%20instruction-following%0Aabilities%20of%20LLMs%20without%20manual%20annotation%20remains%20unresolved.%20In%20this%20paper%2C%0Awe%20introduce%20AutoIF%2C%20the%20first%20scalable%20and%20reliable%20method%20for%20automatically%0Agenerating%20instruction-following%20training%20data.%20AutoIF%20transforms%20the%0Avalidation%20of%20instruction-following%20data%20quality%20into%20code%20verification%2C%0Arequiring%20LLMs%20to%20generate%20instructions%2C%20the%20corresponding%20code%20to%20check%20the%0Acorrectness%20of%20the%20instruction%20responses%2C%20and%20unit%20test%20samples%20to%20verify%20the%0Acode%27s%20correctness.%20Then%2C%20execution%20feedback-based%20rejection%20sampling%20can%0Agenerate%20data%20for%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20training.%20AutoIF%20achieves%20significant%20improvements%20across%0Athree%20training%20algorithms%2C%20SFT%2C%20Offline%20DPO%2C%20and%20Online%20DPO%2C%20when%20applied%20to%0Athe%20top%20open-source%20LLMs%2C%20Qwen2%20and%20LLaMA3%2C%20in%20self-alignment%20and%0Astrong-to-weak%20distillation%20settings.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/QwenLM/AutoIF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-play%2520with%2520Execution%2520Feedback%253A%2520Improving%2520Instruction-following%250A%2520%2520Capabilities%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DGuanting%2520Dong%2520and%2520Keming%2520Lu%2520and%2520Chengpeng%2520Li%2520and%2520Tingyu%2520Xia%2520and%2520Bowen%2520Yu%2520and%2520Chang%2520Zhou%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520One%2520core%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520to%2520follow%2520natural%250Alanguage%2520instructions.%2520However%252C%2520the%2520issue%2520of%2520automatically%2520constructing%250Ahigh-quality%2520training%2520data%2520to%2520enhance%2520the%2520complex%2520instruction-following%250Aabilities%2520of%2520LLMs%2520without%2520manual%2520annotation%2520remains%2520unresolved.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520AutoIF%252C%2520the%2520first%2520scalable%2520and%2520reliable%2520method%2520for%2520automatically%250Agenerating%2520instruction-following%2520training%2520data.%2520AutoIF%2520transforms%2520the%250Avalidation%2520of%2520instruction-following%2520data%2520quality%2520into%2520code%2520verification%252C%250Arequiring%2520LLMs%2520to%2520generate%2520instructions%252C%2520the%2520corresponding%2520code%2520to%2520check%2520the%250Acorrectness%2520of%2520the%2520instruction%2520responses%252C%2520and%2520unit%2520test%2520samples%2520to%2520verify%2520the%250Acode%2527s%2520correctness.%2520Then%252C%2520execution%2520feedback-based%2520rejection%2520sampling%2520can%250Agenerate%2520data%2520for%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520Reinforcement%2520Learning%2520from%250AHuman%2520Feedback%2520%2528RLHF%2529%2520training.%2520AutoIF%2520achieves%2520significant%2520improvements%2520across%250Athree%2520training%2520algorithms%252C%2520SFT%252C%2520Offline%2520DPO%252C%2520and%2520Online%2520DPO%252C%2520when%2520applied%2520to%250Athe%2520top%2520open-source%2520LLMs%252C%2520Qwen2%2520and%2520LLaMA3%252C%2520in%2520self-alignment%2520and%250Astrong-to-weak%2520distillation%2520settings.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/QwenLM/AutoIF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-play%20with%20Execution%20Feedback%3A%20Improving%20Instruction-following%0A%20%20Capabilities%20of%20Large%20Language%20Models&entry.906535625=Guanting%20Dong%20and%20Keming%20Lu%20and%20Chengpeng%20Li%20and%20Tingyu%20Xia%20and%20Bowen%20Yu%20and%20Chang%20Zhou%20and%20Jingren%20Zhou&entry.1292438233=%20%20One%20core%20capability%20of%20large%20language%20models%20%28LLMs%29%20is%20to%20follow%20natural%0Alanguage%20instructions.%20However%2C%20the%20issue%20of%20automatically%20constructing%0Ahigh-quality%20training%20data%20to%20enhance%20the%20complex%20instruction-following%0Aabilities%20of%20LLMs%20without%20manual%20annotation%20remains%20unresolved.%20In%20this%20paper%2C%0Awe%20introduce%20AutoIF%2C%20the%20first%20scalable%20and%20reliable%20method%20for%20automatically%0Agenerating%20instruction-following%20training%20data.%20AutoIF%20transforms%20the%0Avalidation%20of%20instruction-following%20data%20quality%20into%20code%20verification%2C%0Arequiring%20LLMs%20to%20generate%20instructions%2C%20the%20corresponding%20code%20to%20check%20the%0Acorrectness%20of%20the%20instruction%20responses%2C%20and%20unit%20test%20samples%20to%20verify%20the%0Acode%27s%20correctness.%20Then%2C%20execution%20feedback-based%20rejection%20sampling%20can%0Agenerate%20data%20for%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20from%0AHuman%20Feedback%20%28RLHF%29%20training.%20AutoIF%20achieves%20significant%20improvements%20across%0Athree%20training%20algorithms%2C%20SFT%2C%20Offline%20DPO%2C%20and%20Online%20DPO%2C%20when%20applied%20to%0Athe%20top%20open-source%20LLMs%2C%20Qwen2%20and%20LLaMA3%2C%20in%20self-alignment%20and%0Astrong-to-weak%20distillation%20settings.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/QwenLM/AutoIF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13542v2&entry.124074799=Read"},
{"title": "CatchBackdoor: Backdoor Detection via Critical Trojan Neural Path\n  Fuzzing", "author": "Haibo Jin and Ruoxi Chen and Jinyin Chen and Haibin Zheng and Yang Zhang and Haohan Wang", "abstract": "  The success of deep neural networks (DNNs) in real-world applications has\nbenefited from abundant pre-trained models. However, the backdoored pre-trained\nmodels can pose a significant trojan threat to the deployment of downstream\nDNNs. Numerous backdoor detection methods have been proposed but are limited to\ntwo aspects: (1) high sensitivity on trigger size, especially on stealthy\nattacks (i.e., blending attacks and defense adaptive attacks); (2) rely heavily\non benign examples for reverse engineering. To address these challenges, we\nempirically observed that trojaned behaviors triggered by various trojan\nattacks can be attributed to the trojan path, composed of top-$k$ critical\nneurons with more significant contributions to model prediction changes.\nMotivated by it, we propose CatchBackdoor, a detection method against trojan\nattacks. Based on the close connection between trojaned behaviors and trojan\npath to trigger errors, CatchBackdoor starts from the benign path and gradually\napproximates the trojan path through differential fuzzing. We then reverse\ntriggers from the trojan path, to trigger errors caused by diverse trojaned\nattacks. Extensive experiments on MINST, CIFAR-10, and a-ImageNet datasets and\n7 models (LeNet, ResNet, and VGG) demonstrate the superiority of CatchBackdoor\nover the state-of-the-art methods, in terms of (1) \\emph{effective} - it shows\nbetter detection performance, especially on stealthy attacks ($\\sim$ $\\times$ 2\non average); (2) \\emph{extensible} - it is robust to trigger size and can\nconduct detection without benign examples.\n", "link": "http://arxiv.org/abs/2112.13064v3", "date": "2024-07-17", "relevancy": 1.9516, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5157}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4706}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CatchBackdoor%3A%20Backdoor%20Detection%20via%20Critical%20Trojan%20Neural%20Path%0A%20%20Fuzzing&body=Title%3A%20CatchBackdoor%3A%20Backdoor%20Detection%20via%20Critical%20Trojan%20Neural%20Path%0A%20%20Fuzzing%0AAuthor%3A%20Haibo%20Jin%20and%20Ruoxi%20Chen%20and%20Jinyin%20Chen%20and%20Haibin%20Zheng%20and%20Yang%20Zhang%20and%20Haohan%20Wang%0AAbstract%3A%20%20%20The%20success%20of%20deep%20neural%20networks%20%28DNNs%29%20in%20real-world%20applications%20has%0Abenefited%20from%20abundant%20pre-trained%20models.%20However%2C%20the%20backdoored%20pre-trained%0Amodels%20can%20pose%20a%20significant%20trojan%20threat%20to%20the%20deployment%20of%20downstream%0ADNNs.%20Numerous%20backdoor%20detection%20methods%20have%20been%20proposed%20but%20are%20limited%20to%0Atwo%20aspects%3A%20%281%29%20high%20sensitivity%20on%20trigger%20size%2C%20especially%20on%20stealthy%0Aattacks%20%28i.e.%2C%20blending%20attacks%20and%20defense%20adaptive%20attacks%29%3B%20%282%29%20rely%20heavily%0Aon%20benign%20examples%20for%20reverse%20engineering.%20To%20address%20these%20challenges%2C%20we%0Aempirically%20observed%20that%20trojaned%20behaviors%20triggered%20by%20various%20trojan%0Aattacks%20can%20be%20attributed%20to%20the%20trojan%20path%2C%20composed%20of%20top-%24k%24%20critical%0Aneurons%20with%20more%20significant%20contributions%20to%20model%20prediction%20changes.%0AMotivated%20by%20it%2C%20we%20propose%20CatchBackdoor%2C%20a%20detection%20method%20against%20trojan%0Aattacks.%20Based%20on%20the%20close%20connection%20between%20trojaned%20behaviors%20and%20trojan%0Apath%20to%20trigger%20errors%2C%20CatchBackdoor%20starts%20from%20the%20benign%20path%20and%20gradually%0Aapproximates%20the%20trojan%20path%20through%20differential%20fuzzing.%20We%20then%20reverse%0Atriggers%20from%20the%20trojan%20path%2C%20to%20trigger%20errors%20caused%20by%20diverse%20trojaned%0Aattacks.%20Extensive%20experiments%20on%20MINST%2C%20CIFAR-10%2C%20and%20a-ImageNet%20datasets%20and%0A7%20models%20%28LeNet%2C%20ResNet%2C%20and%20VGG%29%20demonstrate%20the%20superiority%20of%20CatchBackdoor%0Aover%20the%20state-of-the-art%20methods%2C%20in%20terms%20of%20%281%29%20%5Cemph%7Beffective%7D%20-%20it%20shows%0Abetter%20detection%20performance%2C%20especially%20on%20stealthy%20attacks%20%28%24%5Csim%24%20%24%5Ctimes%24%202%0Aon%20average%29%3B%20%282%29%20%5Cemph%7Bextensible%7D%20-%20it%20is%20robust%20to%20trigger%20size%20and%20can%0Aconduct%20detection%20without%20benign%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2112.13064v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCatchBackdoor%253A%2520Backdoor%2520Detection%2520via%2520Critical%2520Trojan%2520Neural%2520Path%250A%2520%2520Fuzzing%26entry.906535625%3DHaibo%2520Jin%2520and%2520Ruoxi%2520Chen%2520and%2520Jinyin%2520Chen%2520and%2520Haibin%2520Zheng%2520and%2520Yang%2520Zhang%2520and%2520Haohan%2520Wang%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520in%2520real-world%2520applications%2520has%250Abenefited%2520from%2520abundant%2520pre-trained%2520models.%2520However%252C%2520the%2520backdoored%2520pre-trained%250Amodels%2520can%2520pose%2520a%2520significant%2520trojan%2520threat%2520to%2520the%2520deployment%2520of%2520downstream%250ADNNs.%2520Numerous%2520backdoor%2520detection%2520methods%2520have%2520been%2520proposed%2520but%2520are%2520limited%2520to%250Atwo%2520aspects%253A%2520%25281%2529%2520high%2520sensitivity%2520on%2520trigger%2520size%252C%2520especially%2520on%2520stealthy%250Aattacks%2520%2528i.e.%252C%2520blending%2520attacks%2520and%2520defense%2520adaptive%2520attacks%2529%253B%2520%25282%2529%2520rely%2520heavily%250Aon%2520benign%2520examples%2520for%2520reverse%2520engineering.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aempirically%2520observed%2520that%2520trojaned%2520behaviors%2520triggered%2520by%2520various%2520trojan%250Aattacks%2520can%2520be%2520attributed%2520to%2520the%2520trojan%2520path%252C%2520composed%2520of%2520top-%2524k%2524%2520critical%250Aneurons%2520with%2520more%2520significant%2520contributions%2520to%2520model%2520prediction%2520changes.%250AMotivated%2520by%2520it%252C%2520we%2520propose%2520CatchBackdoor%252C%2520a%2520detection%2520method%2520against%2520trojan%250Aattacks.%2520Based%2520on%2520the%2520close%2520connection%2520between%2520trojaned%2520behaviors%2520and%2520trojan%250Apath%2520to%2520trigger%2520errors%252C%2520CatchBackdoor%2520starts%2520from%2520the%2520benign%2520path%2520and%2520gradually%250Aapproximates%2520the%2520trojan%2520path%2520through%2520differential%2520fuzzing.%2520We%2520then%2520reverse%250Atriggers%2520from%2520the%2520trojan%2520path%252C%2520to%2520trigger%2520errors%2520caused%2520by%2520diverse%2520trojaned%250Aattacks.%2520Extensive%2520experiments%2520on%2520MINST%252C%2520CIFAR-10%252C%2520and%2520a-ImageNet%2520datasets%2520and%250A7%2520models%2520%2528LeNet%252C%2520ResNet%252C%2520and%2520VGG%2529%2520demonstrate%2520the%2520superiority%2520of%2520CatchBackdoor%250Aover%2520the%2520state-of-the-art%2520methods%252C%2520in%2520terms%2520of%2520%25281%2529%2520%255Cemph%257Beffective%257D%2520-%2520it%2520shows%250Abetter%2520detection%2520performance%252C%2520especially%2520on%2520stealthy%2520attacks%2520%2528%2524%255Csim%2524%2520%2524%255Ctimes%2524%25202%250Aon%2520average%2529%253B%2520%25282%2529%2520%255Cemph%257Bextensible%257D%2520-%2520it%2520is%2520robust%2520to%2520trigger%2520size%2520and%2520can%250Aconduct%2520detection%2520without%2520benign%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2112.13064v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CatchBackdoor%3A%20Backdoor%20Detection%20via%20Critical%20Trojan%20Neural%20Path%0A%20%20Fuzzing&entry.906535625=Haibo%20Jin%20and%20Ruoxi%20Chen%20and%20Jinyin%20Chen%20and%20Haibin%20Zheng%20and%20Yang%20Zhang%20and%20Haohan%20Wang&entry.1292438233=%20%20The%20success%20of%20deep%20neural%20networks%20%28DNNs%29%20in%20real-world%20applications%20has%0Abenefited%20from%20abundant%20pre-trained%20models.%20However%2C%20the%20backdoored%20pre-trained%0Amodels%20can%20pose%20a%20significant%20trojan%20threat%20to%20the%20deployment%20of%20downstream%0ADNNs.%20Numerous%20backdoor%20detection%20methods%20have%20been%20proposed%20but%20are%20limited%20to%0Atwo%20aspects%3A%20%281%29%20high%20sensitivity%20on%20trigger%20size%2C%20especially%20on%20stealthy%0Aattacks%20%28i.e.%2C%20blending%20attacks%20and%20defense%20adaptive%20attacks%29%3B%20%282%29%20rely%20heavily%0Aon%20benign%20examples%20for%20reverse%20engineering.%20To%20address%20these%20challenges%2C%20we%0Aempirically%20observed%20that%20trojaned%20behaviors%20triggered%20by%20various%20trojan%0Aattacks%20can%20be%20attributed%20to%20the%20trojan%20path%2C%20composed%20of%20top-%24k%24%20critical%0Aneurons%20with%20more%20significant%20contributions%20to%20model%20prediction%20changes.%0AMotivated%20by%20it%2C%20we%20propose%20CatchBackdoor%2C%20a%20detection%20method%20against%20trojan%0Aattacks.%20Based%20on%20the%20close%20connection%20between%20trojaned%20behaviors%20and%20trojan%0Apath%20to%20trigger%20errors%2C%20CatchBackdoor%20starts%20from%20the%20benign%20path%20and%20gradually%0Aapproximates%20the%20trojan%20path%20through%20differential%20fuzzing.%20We%20then%20reverse%0Atriggers%20from%20the%20trojan%20path%2C%20to%20trigger%20errors%20caused%20by%20diverse%20trojaned%0Aattacks.%20Extensive%20experiments%20on%20MINST%2C%20CIFAR-10%2C%20and%20a-ImageNet%20datasets%20and%0A7%20models%20%28LeNet%2C%20ResNet%2C%20and%20VGG%29%20demonstrate%20the%20superiority%20of%20CatchBackdoor%0Aover%20the%20state-of-the-art%20methods%2C%20in%20terms%20of%20%281%29%20%5Cemph%7Beffective%7D%20-%20it%20shows%0Abetter%20detection%20performance%2C%20especially%20on%20stealthy%20attacks%20%28%24%5Csim%24%20%24%5Ctimes%24%202%0Aon%20average%29%3B%20%282%29%20%5Cemph%7Bextensible%7D%20-%20it%20is%20robust%20to%20trigger%20size%20and%20can%0Aconduct%20detection%20without%20benign%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.13064v3&entry.124074799=Read"},
{"title": "Learning a Sparse Neural Network using IHT", "author": "Saeed Damadi and Soroush Zolfaghari and Mahdi Rezaie and Jinglai Shen", "abstract": "  The core of a good model is in its ability to focus only on important\ninformation that reflects the basic patterns and consistencies, thus pulling\nout a clear, noise-free signal from the dataset. This necessitates using a\nsimplified model defined by fewer parameters. The importance of theoretical\nfoundations becomes clear in this context, as this paper relies on established\nresults from the domain of advanced sparse optimization, particularly those\naddressing nonlinear differentiable functions. The need for such theoretical\nfoundations is further highlighted by the trend that as computational power for\ntraining NNs increases, so does the complexity of the models in terms of a\nhigher number of parameters. In practical scenarios, these large models are\noften simplified to more manageable versions with fewer parameters.\n  Understanding why these simplified models with less number of parameters\nremain effective raises a crucial question. Understanding why these simplified\nmodels with fewer parameters remain effective raises an important question.\nThis leads to the broader question of whether there is a theoretical framework\nthat can clearly explain these empirical observations. Recent developments,\nsuch as establishing necessary conditions for the convergence of iterative hard\nthresholding (IHT) to a sparse local minimum (a sparse method analogous to\ngradient descent) are promising. The remarkable capacity of the IHT algorithm\nto accurately identify and learn the locations of nonzero parameters\nunderscores its practical effectiveness and utility.\n  This paper aims to investigate whether the theoretical prerequisites for such\nconvergence are applicable in the realm of neural network (NN) training by\nproviding justification for all the necessary conditions for convergence. Then,\nthese conditions are validated by experiments on a single-layer NN, using the\nIRIS dataset as a testbed.\n", "link": "http://arxiv.org/abs/2404.18414v2", "date": "2024-07-17", "relevancy": 1.9455, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4967}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4908}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20a%20Sparse%20Neural%20Network%20using%20IHT&body=Title%3A%20Learning%20a%20Sparse%20Neural%20Network%20using%20IHT%0AAuthor%3A%20Saeed%20Damadi%20and%20Soroush%20Zolfaghari%20and%20Mahdi%20Rezaie%20and%20Jinglai%20Shen%0AAbstract%3A%20%20%20The%20core%20of%20a%20good%20model%20is%20in%20its%20ability%20to%20focus%20only%20on%20important%0Ainformation%20that%20reflects%20the%20basic%20patterns%20and%20consistencies%2C%20thus%20pulling%0Aout%20a%20clear%2C%20noise-free%20signal%20from%20the%20dataset.%20This%20necessitates%20using%20a%0Asimplified%20model%20defined%20by%20fewer%20parameters.%20The%20importance%20of%20theoretical%0Afoundations%20becomes%20clear%20in%20this%20context%2C%20as%20this%20paper%20relies%20on%20established%0Aresults%20from%20the%20domain%20of%20advanced%20sparse%20optimization%2C%20particularly%20those%0Aaddressing%20nonlinear%20differentiable%20functions.%20The%20need%20for%20such%20theoretical%0Afoundations%20is%20further%20highlighted%20by%20the%20trend%20that%20as%20computational%20power%20for%0Atraining%20NNs%20increases%2C%20so%20does%20the%20complexity%20of%20the%20models%20in%20terms%20of%20a%0Ahigher%20number%20of%20parameters.%20In%20practical%20scenarios%2C%20these%20large%20models%20are%0Aoften%20simplified%20to%20more%20manageable%20versions%20with%20fewer%20parameters.%0A%20%20Understanding%20why%20these%20simplified%20models%20with%20less%20number%20of%20parameters%0Aremain%20effective%20raises%20a%20crucial%20question.%20Understanding%20why%20these%20simplified%0Amodels%20with%20fewer%20parameters%20remain%20effective%20raises%20an%20important%20question.%0AThis%20leads%20to%20the%20broader%20question%20of%20whether%20there%20is%20a%20theoretical%20framework%0Athat%20can%20clearly%20explain%20these%20empirical%20observations.%20Recent%20developments%2C%0Asuch%20as%20establishing%20necessary%20conditions%20for%20the%20convergence%20of%20iterative%20hard%0Athresholding%20%28IHT%29%20to%20a%20sparse%20local%20minimum%20%28a%20sparse%20method%20analogous%20to%0Agradient%20descent%29%20are%20promising.%20The%20remarkable%20capacity%20of%20the%20IHT%20algorithm%0Ato%20accurately%20identify%20and%20learn%20the%20locations%20of%20nonzero%20parameters%0Aunderscores%20its%20practical%20effectiveness%20and%20utility.%0A%20%20This%20paper%20aims%20to%20investigate%20whether%20the%20theoretical%20prerequisites%20for%20such%0Aconvergence%20are%20applicable%20in%20the%20realm%20of%20neural%20network%20%28NN%29%20training%20by%0Aproviding%20justification%20for%20all%20the%20necessary%20conditions%20for%20convergence.%20Then%2C%0Athese%20conditions%20are%20validated%20by%20experiments%20on%20a%20single-layer%20NN%2C%20using%20the%0AIRIS%20dataset%20as%20a%20testbed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18414v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520a%2520Sparse%2520Neural%2520Network%2520using%2520IHT%26entry.906535625%3DSaeed%2520Damadi%2520and%2520Soroush%2520Zolfaghari%2520and%2520Mahdi%2520Rezaie%2520and%2520Jinglai%2520Shen%26entry.1292438233%3D%2520%2520The%2520core%2520of%2520a%2520good%2520model%2520is%2520in%2520its%2520ability%2520to%2520focus%2520only%2520on%2520important%250Ainformation%2520that%2520reflects%2520the%2520basic%2520patterns%2520and%2520consistencies%252C%2520thus%2520pulling%250Aout%2520a%2520clear%252C%2520noise-free%2520signal%2520from%2520the%2520dataset.%2520This%2520necessitates%2520using%2520a%250Asimplified%2520model%2520defined%2520by%2520fewer%2520parameters.%2520The%2520importance%2520of%2520theoretical%250Afoundations%2520becomes%2520clear%2520in%2520this%2520context%252C%2520as%2520this%2520paper%2520relies%2520on%2520established%250Aresults%2520from%2520the%2520domain%2520of%2520advanced%2520sparse%2520optimization%252C%2520particularly%2520those%250Aaddressing%2520nonlinear%2520differentiable%2520functions.%2520The%2520need%2520for%2520such%2520theoretical%250Afoundations%2520is%2520further%2520highlighted%2520by%2520the%2520trend%2520that%2520as%2520computational%2520power%2520for%250Atraining%2520NNs%2520increases%252C%2520so%2520does%2520the%2520complexity%2520of%2520the%2520models%2520in%2520terms%2520of%2520a%250Ahigher%2520number%2520of%2520parameters.%2520In%2520practical%2520scenarios%252C%2520these%2520large%2520models%2520are%250Aoften%2520simplified%2520to%2520more%2520manageable%2520versions%2520with%2520fewer%2520parameters.%250A%2520%2520Understanding%2520why%2520these%2520simplified%2520models%2520with%2520less%2520number%2520of%2520parameters%250Aremain%2520effective%2520raises%2520a%2520crucial%2520question.%2520Understanding%2520why%2520these%2520simplified%250Amodels%2520with%2520fewer%2520parameters%2520remain%2520effective%2520raises%2520an%2520important%2520question.%250AThis%2520leads%2520to%2520the%2520broader%2520question%2520of%2520whether%2520there%2520is%2520a%2520theoretical%2520framework%250Athat%2520can%2520clearly%2520explain%2520these%2520empirical%2520observations.%2520Recent%2520developments%252C%250Asuch%2520as%2520establishing%2520necessary%2520conditions%2520for%2520the%2520convergence%2520of%2520iterative%2520hard%250Athresholding%2520%2528IHT%2529%2520to%2520a%2520sparse%2520local%2520minimum%2520%2528a%2520sparse%2520method%2520analogous%2520to%250Agradient%2520descent%2529%2520are%2520promising.%2520The%2520remarkable%2520capacity%2520of%2520the%2520IHT%2520algorithm%250Ato%2520accurately%2520identify%2520and%2520learn%2520the%2520locations%2520of%2520nonzero%2520parameters%250Aunderscores%2520its%2520practical%2520effectiveness%2520and%2520utility.%250A%2520%2520This%2520paper%2520aims%2520to%2520investigate%2520whether%2520the%2520theoretical%2520prerequisites%2520for%2520such%250Aconvergence%2520are%2520applicable%2520in%2520the%2520realm%2520of%2520neural%2520network%2520%2528NN%2529%2520training%2520by%250Aproviding%2520justification%2520for%2520all%2520the%2520necessary%2520conditions%2520for%2520convergence.%2520Then%252C%250Athese%2520conditions%2520are%2520validated%2520by%2520experiments%2520on%2520a%2520single-layer%2520NN%252C%2520using%2520the%250AIRIS%2520dataset%2520as%2520a%2520testbed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18414v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Sparse%20Neural%20Network%20using%20IHT&entry.906535625=Saeed%20Damadi%20and%20Soroush%20Zolfaghari%20and%20Mahdi%20Rezaie%20and%20Jinglai%20Shen&entry.1292438233=%20%20The%20core%20of%20a%20good%20model%20is%20in%20its%20ability%20to%20focus%20only%20on%20important%0Ainformation%20that%20reflects%20the%20basic%20patterns%20and%20consistencies%2C%20thus%20pulling%0Aout%20a%20clear%2C%20noise-free%20signal%20from%20the%20dataset.%20This%20necessitates%20using%20a%0Asimplified%20model%20defined%20by%20fewer%20parameters.%20The%20importance%20of%20theoretical%0Afoundations%20becomes%20clear%20in%20this%20context%2C%20as%20this%20paper%20relies%20on%20established%0Aresults%20from%20the%20domain%20of%20advanced%20sparse%20optimization%2C%20particularly%20those%0Aaddressing%20nonlinear%20differentiable%20functions.%20The%20need%20for%20such%20theoretical%0Afoundations%20is%20further%20highlighted%20by%20the%20trend%20that%20as%20computational%20power%20for%0Atraining%20NNs%20increases%2C%20so%20does%20the%20complexity%20of%20the%20models%20in%20terms%20of%20a%0Ahigher%20number%20of%20parameters.%20In%20practical%20scenarios%2C%20these%20large%20models%20are%0Aoften%20simplified%20to%20more%20manageable%20versions%20with%20fewer%20parameters.%0A%20%20Understanding%20why%20these%20simplified%20models%20with%20less%20number%20of%20parameters%0Aremain%20effective%20raises%20a%20crucial%20question.%20Understanding%20why%20these%20simplified%0Amodels%20with%20fewer%20parameters%20remain%20effective%20raises%20an%20important%20question.%0AThis%20leads%20to%20the%20broader%20question%20of%20whether%20there%20is%20a%20theoretical%20framework%0Athat%20can%20clearly%20explain%20these%20empirical%20observations.%20Recent%20developments%2C%0Asuch%20as%20establishing%20necessary%20conditions%20for%20the%20convergence%20of%20iterative%20hard%0Athresholding%20%28IHT%29%20to%20a%20sparse%20local%20minimum%20%28a%20sparse%20method%20analogous%20to%0Agradient%20descent%29%20are%20promising.%20The%20remarkable%20capacity%20of%20the%20IHT%20algorithm%0Ato%20accurately%20identify%20and%20learn%20the%20locations%20of%20nonzero%20parameters%0Aunderscores%20its%20practical%20effectiveness%20and%20utility.%0A%20%20This%20paper%20aims%20to%20investigate%20whether%20the%20theoretical%20prerequisites%20for%20such%0Aconvergence%20are%20applicable%20in%20the%20realm%20of%20neural%20network%20%28NN%29%20training%20by%0Aproviding%20justification%20for%20all%20the%20necessary%20conditions%20for%20convergence.%20Then%2C%0Athese%20conditions%20are%20validated%20by%20experiments%20on%20a%20single-layer%20NN%2C%20using%20the%0AIRIS%20dataset%20as%20a%20testbed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18414v2&entry.124074799=Read"},
{"title": "Patch-Level Training for Large Language Models", "author": "Chenze Shao and Fandong Meng and Jie Zhou", "abstract": "  As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.\n", "link": "http://arxiv.org/abs/2407.12665v1", "date": "2024-07-17", "relevancy": 1.9398, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4955}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4779}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch-Level%20Training%20for%20Large%20Language%20Models&body=Title%3A%20Patch-Level%20Training%20for%20Large%20Language%20Models%0AAuthor%3A%20Chenze%20Shao%20and%20Fandong%20Meng%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20achieve%20remarkable%20progress%20in%20language%0Aunderstanding%20and%20generation%2C%20their%20training%20efficiency%20has%20become%20a%20critical%0Aconcern.%20Traditionally%2C%20LLMs%20are%20trained%20to%20predict%20the%20next%20token%20in%20a%0Asequence.%20Despite%20the%20success%20of%20token-level%20training%2C%20it%20suffers%20from%0Aconsiderable%20computational%20costs%20due%20to%20the%20need%20to%20process%20an%20extensive%20number%0Aof%20tokens.%20To%20mitigate%20this%20issue%2C%20this%20paper%20introduces%20patch-level%20training%0Afor%20LLMs%2C%20which%20reduces%20the%20sequence%20length%20by%20compressing%20multiple%20tokens%20into%0Aa%20single%20patch.%20During%20patch-level%20training%2C%20we%20feed%20the%20language%20model%20shorter%0Asequences%20of%20patches%20and%20train%20it%20to%20predict%20the%20next%20patch%2C%20thereby%20processing%0Athe%20majority%20of%20the%20training%20data%20at%20a%20significantly%20reduced%20computational%0Acost.%20Following%20this%2C%20the%20model%20continues%20token-level%20training%20on%20the%20remaining%0Atraining%20data%20to%20align%20with%20the%20inference%20mode.%20Experiments%20on%20a%20diverse%20range%0Aof%20models%20%28370M-2.7B%20parameters%29%20demonstrate%20that%20patch-level%20training%20can%0Areduce%20overall%20computational%20costs%20to%200.5%24%5Ctimes%24%2C%20without%20compromising%20the%0Amodel%20performance%20compared%20to%20token-level%20training.%20Source%20code%3A%0A%5Curl%7Bhttps%3A//github.com/shaochenze/PatchTrain%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch-Level%2520Training%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DChenze%2520Shao%2520and%2520Fandong%2520Meng%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520achieve%2520remarkable%2520progress%2520in%2520language%250Aunderstanding%2520and%2520generation%252C%2520their%2520training%2520efficiency%2520has%2520become%2520a%2520critical%250Aconcern.%2520Traditionally%252C%2520LLMs%2520are%2520trained%2520to%2520predict%2520the%2520next%2520token%2520in%2520a%250Asequence.%2520Despite%2520the%2520success%2520of%2520token-level%2520training%252C%2520it%2520suffers%2520from%250Aconsiderable%2520computational%2520costs%2520due%2520to%2520the%2520need%2520to%2520process%2520an%2520extensive%2520number%250Aof%2520tokens.%2520To%2520mitigate%2520this%2520issue%252C%2520this%2520paper%2520introduces%2520patch-level%2520training%250Afor%2520LLMs%252C%2520which%2520reduces%2520the%2520sequence%2520length%2520by%2520compressing%2520multiple%2520tokens%2520into%250Aa%2520single%2520patch.%2520During%2520patch-level%2520training%252C%2520we%2520feed%2520the%2520language%2520model%2520shorter%250Asequences%2520of%2520patches%2520and%2520train%2520it%2520to%2520predict%2520the%2520next%2520patch%252C%2520thereby%2520processing%250Athe%2520majority%2520of%2520the%2520training%2520data%2520at%2520a%2520significantly%2520reduced%2520computational%250Acost.%2520Following%2520this%252C%2520the%2520model%2520continues%2520token-level%2520training%2520on%2520the%2520remaining%250Atraining%2520data%2520to%2520align%2520with%2520the%2520inference%2520mode.%2520Experiments%2520on%2520a%2520diverse%2520range%250Aof%2520models%2520%2528370M-2.7B%2520parameters%2529%2520demonstrate%2520that%2520patch-level%2520training%2520can%250Areduce%2520overall%2520computational%2520costs%2520to%25200.5%2524%255Ctimes%2524%252C%2520without%2520compromising%2520the%250Amodel%2520performance%2520compared%2520to%2520token-level%2520training.%2520Source%2520code%253A%250A%255Curl%257Bhttps%253A//github.com/shaochenze/PatchTrain%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch-Level%20Training%20for%20Large%20Language%20Models&entry.906535625=Chenze%20Shao%20and%20Fandong%20Meng%20and%20Jie%20Zhou&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20achieve%20remarkable%20progress%20in%20language%0Aunderstanding%20and%20generation%2C%20their%20training%20efficiency%20has%20become%20a%20critical%0Aconcern.%20Traditionally%2C%20LLMs%20are%20trained%20to%20predict%20the%20next%20token%20in%20a%0Asequence.%20Despite%20the%20success%20of%20token-level%20training%2C%20it%20suffers%20from%0Aconsiderable%20computational%20costs%20due%20to%20the%20need%20to%20process%20an%20extensive%20number%0Aof%20tokens.%20To%20mitigate%20this%20issue%2C%20this%20paper%20introduces%20patch-level%20training%0Afor%20LLMs%2C%20which%20reduces%20the%20sequence%20length%20by%20compressing%20multiple%20tokens%20into%0Aa%20single%20patch.%20During%20patch-level%20training%2C%20we%20feed%20the%20language%20model%20shorter%0Asequences%20of%20patches%20and%20train%20it%20to%20predict%20the%20next%20patch%2C%20thereby%20processing%0Athe%20majority%20of%20the%20training%20data%20at%20a%20significantly%20reduced%20computational%0Acost.%20Following%20this%2C%20the%20model%20continues%20token-level%20training%20on%20the%20remaining%0Atraining%20data%20to%20align%20with%20the%20inference%20mode.%20Experiments%20on%20a%20diverse%20range%0Aof%20models%20%28370M-2.7B%20parameters%29%20demonstrate%20that%20patch-level%20training%20can%0Areduce%20overall%20computational%20costs%20to%200.5%24%5Ctimes%24%2C%20without%20compromising%20the%0Amodel%20performance%20compared%20to%20token-level%20training.%20Source%20code%3A%0A%5Curl%7Bhttps%3A//github.com/shaochenze/PatchTrain%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12665v1&entry.124074799=Read"},
{"title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance", "author": "Zeke Xia and Ming Hu and Dengke Yan and Xiaofei Xie and Tianlin Li and Anran Li and Junlong Zhou and Mingsong Chen", "abstract": "  Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.\n", "link": "http://arxiv.org/abs/2404.12850v2", "date": "2024-07-17", "relevancy": 1.9303, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4942}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaBaFL%3A%20Asynchronous%20Federated%20Learning%20via%20Hierarchical%20Cache%20and%0A%20%20Feature%20Balance&body=Title%3A%20CaBaFL%3A%20Asynchronous%20Federated%20Learning%20via%20Hierarchical%20Cache%20and%0A%20%20Feature%20Balance%0AAuthor%3A%20Zeke%20Xia%20and%20Ming%20Hu%20and%20Dengke%20Yan%20and%20Xiaofei%20Xie%20and%20Tianlin%20Li%20and%20Anran%20Li%20and%20Junlong%20Zhou%20and%20Mingsong%20Chen%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20as%20a%20promising%20distributed%20machine%20learning%20paradigm%0Ahas%20been%20widely%20adopted%20in%20Artificial%20Intelligence%20of%20Things%20%28AIoT%29%0Aapplications.%20However%2C%20the%20efficiency%20and%20inference%20capability%20of%20FL%20is%0Aseriously%20limited%20due%20to%20the%20presence%20of%20stragglers%20and%20data%20imbalance%20across%0Amassive%20AIoT%20devices%2C%20respectively.%20To%20address%20the%20above%20challenges%2C%20we%20present%0Aa%20novel%20asynchronous%20FL%20approach%20named%20CaBaFL%2C%20which%20includes%20a%20hierarchical%0ACache-based%20aggregation%20mechanism%20and%20a%20feature%20Balance-guided%20device%20selection%0Astrategy.%20CaBaFL%20maintains%20multiple%20intermediate%20models%20simultaneously%20for%0Alocal%20training.%20The%20hierarchical%20cache-based%20aggregation%20mechanism%20enables%20each%0Aintermediate%20model%20to%20be%20trained%20on%20multiple%20devices%20to%20align%20the%20training%20time%0Aand%20mitigate%20the%20straggler%20issue.%20In%20specific%2C%20each%20intermediate%20model%20is%0Astored%20in%20a%20low-level%20cache%20for%20local%20training%20and%20when%20it%20is%20trained%20by%0Asufficient%20local%20devices%2C%20it%20will%20be%20stored%20in%20a%20high-level%20cache%20for%0Aaggregation.%20To%20address%20the%20problem%20of%20imbalanced%20data%2C%20the%20feature%0Abalance-guided%20device%20selection%20strategy%20in%20CaBaFL%20adopts%20the%20activation%0Adistribution%20as%20a%20metric%2C%20which%20enables%20each%20intermediate%20model%20to%20be%20trained%0Aacross%20devices%20with%20totally%20balanced%20data%20distributions%20before%20aggregation.%0AExperimental%20results%20show%20that%20compared%20with%20the%20state-of-the-art%20FL%20methods%2C%0ACaBaFL%20achieves%20up%20to%209.26X%20training%20acceleration%20and%2019.71%5C%25%20accuracy%0Aimprovements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaBaFL%253A%2520Asynchronous%2520Federated%2520Learning%2520via%2520Hierarchical%2520Cache%2520and%250A%2520%2520Feature%2520Balance%26entry.906535625%3DZeke%2520Xia%2520and%2520Ming%2520Hu%2520and%2520Dengke%2520Yan%2520and%2520Xiaofei%2520Xie%2520and%2520Tianlin%2520Li%2520and%2520Anran%2520Li%2520and%2520Junlong%2520Zhou%2520and%2520Mingsong%2520Chen%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520as%2520a%2520promising%2520distributed%2520machine%2520learning%2520paradigm%250Ahas%2520been%2520widely%2520adopted%2520in%2520Artificial%2520Intelligence%2520of%2520Things%2520%2528AIoT%2529%250Aapplications.%2520However%252C%2520the%2520efficiency%2520and%2520inference%2520capability%2520of%2520FL%2520is%250Aseriously%2520limited%2520due%2520to%2520the%2520presence%2520of%2520stragglers%2520and%2520data%2520imbalance%2520across%250Amassive%2520AIoT%2520devices%252C%2520respectively.%2520To%2520address%2520the%2520above%2520challenges%252C%2520we%2520present%250Aa%2520novel%2520asynchronous%2520FL%2520approach%2520named%2520CaBaFL%252C%2520which%2520includes%2520a%2520hierarchical%250ACache-based%2520aggregation%2520mechanism%2520and%2520a%2520feature%2520Balance-guided%2520device%2520selection%250Astrategy.%2520CaBaFL%2520maintains%2520multiple%2520intermediate%2520models%2520simultaneously%2520for%250Alocal%2520training.%2520The%2520hierarchical%2520cache-based%2520aggregation%2520mechanism%2520enables%2520each%250Aintermediate%2520model%2520to%2520be%2520trained%2520on%2520multiple%2520devices%2520to%2520align%2520the%2520training%2520time%250Aand%2520mitigate%2520the%2520straggler%2520issue.%2520In%2520specific%252C%2520each%2520intermediate%2520model%2520is%250Astored%2520in%2520a%2520low-level%2520cache%2520for%2520local%2520training%2520and%2520when%2520it%2520is%2520trained%2520by%250Asufficient%2520local%2520devices%252C%2520it%2520will%2520be%2520stored%2520in%2520a%2520high-level%2520cache%2520for%250Aaggregation.%2520To%2520address%2520the%2520problem%2520of%2520imbalanced%2520data%252C%2520the%2520feature%250Abalance-guided%2520device%2520selection%2520strategy%2520in%2520CaBaFL%2520adopts%2520the%2520activation%250Adistribution%2520as%2520a%2520metric%252C%2520which%2520enables%2520each%2520intermediate%2520model%2520to%2520be%2520trained%250Aacross%2520devices%2520with%2520totally%2520balanced%2520data%2520distributions%2520before%2520aggregation.%250AExperimental%2520results%2520show%2520that%2520compared%2520with%2520the%2520state-of-the-art%2520FL%2520methods%252C%250ACaBaFL%2520achieves%2520up%2520to%25209.26X%2520training%2520acceleration%2520and%252019.71%255C%2525%2520accuracy%250Aimprovements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaBaFL%3A%20Asynchronous%20Federated%20Learning%20via%20Hierarchical%20Cache%20and%0A%20%20Feature%20Balance&entry.906535625=Zeke%20Xia%20and%20Ming%20Hu%20and%20Dengke%20Yan%20and%20Xiaofei%20Xie%20and%20Tianlin%20Li%20and%20Anran%20Li%20and%20Junlong%20Zhou%20and%20Mingsong%20Chen&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20as%20a%20promising%20distributed%20machine%20learning%20paradigm%0Ahas%20been%20widely%20adopted%20in%20Artificial%20Intelligence%20of%20Things%20%28AIoT%29%0Aapplications.%20However%2C%20the%20efficiency%20and%20inference%20capability%20of%20FL%20is%0Aseriously%20limited%20due%20to%20the%20presence%20of%20stragglers%20and%20data%20imbalance%20across%0Amassive%20AIoT%20devices%2C%20respectively.%20To%20address%20the%20above%20challenges%2C%20we%20present%0Aa%20novel%20asynchronous%20FL%20approach%20named%20CaBaFL%2C%20which%20includes%20a%20hierarchical%0ACache-based%20aggregation%20mechanism%20and%20a%20feature%20Balance-guided%20device%20selection%0Astrategy.%20CaBaFL%20maintains%20multiple%20intermediate%20models%20simultaneously%20for%0Alocal%20training.%20The%20hierarchical%20cache-based%20aggregation%20mechanism%20enables%20each%0Aintermediate%20model%20to%20be%20trained%20on%20multiple%20devices%20to%20align%20the%20training%20time%0Aand%20mitigate%20the%20straggler%20issue.%20In%20specific%2C%20each%20intermediate%20model%20is%0Astored%20in%20a%20low-level%20cache%20for%20local%20training%20and%20when%20it%20is%20trained%20by%0Asufficient%20local%20devices%2C%20it%20will%20be%20stored%20in%20a%20high-level%20cache%20for%0Aaggregation.%20To%20address%20the%20problem%20of%20imbalanced%20data%2C%20the%20feature%0Abalance-guided%20device%20selection%20strategy%20in%20CaBaFL%20adopts%20the%20activation%0Adistribution%20as%20a%20metric%2C%20which%20enables%20each%20intermediate%20model%20to%20be%20trained%0Aacross%20devices%20with%20totally%20balanced%20data%20distributions%20before%20aggregation.%0AExperimental%20results%20show%20that%20compared%20with%20the%20state-of-the-art%20FL%20methods%2C%0ACaBaFL%20achieves%20up%20to%209.26X%20training%20acceleration%20and%2019.71%5C%25%20accuracy%0Aimprovements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12850v2&entry.124074799=Read"},
{"title": "4Dynamic: Text-to-4D Generation with Hybrid Priors", "author": "Yu-Jie Yuan and Leif Kobbelt and Jiwen Liu and Yuan Zhang and Pengfei Wan and Yu-Kun Lai and Lin Gao", "abstract": "  Due to the fascinating generative performance of text-to-image diffusion\nmodels, growing text-to-3D generation works explore distilling the 2D\ngenerative priors into 3D, using the score distillation sampling (SDS) loss, to\nbypass the data scarcity problem. The existing text-to-3D methods have achieved\npromising results in realism and 3D consistency, but text-to-4D generation\nstill faces challenges, including lack of realism and insufficient dynamic\nmotions. In this paper, we propose a novel method for text-to-4D generation,\nwhich ensures the dynamic amplitude and authenticity through direct supervision\nprovided by a video prior. Specifically, we adopt a text-to-video diffusion\nmodel to generate a reference video and divide 4D generation into two stages:\nstatic generation and dynamic generation. The static 3D generation is achieved\nunder the guidance of the input text and the first frame of the reference\nvideo, while in the dynamic generation stage, we introduce a customized SDS\nloss to ensure multi-view consistency, a video-based SDS loss to improve\ntemporal consistency, and most importantly, direct priors from the reference\nvideo to ensure the quality of geometry and texture. Moreover, we design a\nprior-switching training strategy to avoid conflicts between different priors\nand fully leverage the benefits of each prior. In addition, to enrich the\ngenerated motion, we further introduce a dynamic modeling representation\ncomposed of a deformation network and a topology network, which ensures dynamic\ncontinuity while modeling topological changes. Our method not only supports\ntext-to-4D generation but also enables 4D generation from monocular videos. The\ncomparison experiments demonstrate the superiority of our method compared to\nexisting methods.\n", "link": "http://arxiv.org/abs/2407.12684v1", "date": "2024-07-17", "relevancy": 1.9286, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6476}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6429}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204Dynamic%3A%20Text-to-4D%20Generation%20with%20Hybrid%20Priors&body=Title%3A%204Dynamic%3A%20Text-to-4D%20Generation%20with%20Hybrid%20Priors%0AAuthor%3A%20Yu-Jie%20Yuan%20and%20Leif%20Kobbelt%20and%20Jiwen%20Liu%20and%20Yuan%20Zhang%20and%20Pengfei%20Wan%20and%20Yu-Kun%20Lai%20and%20Lin%20Gao%0AAbstract%3A%20%20%20Due%20to%20the%20fascinating%20generative%20performance%20of%20text-to-image%20diffusion%0Amodels%2C%20growing%20text-to-3D%20generation%20works%20explore%20distilling%20the%202D%0Agenerative%20priors%20into%203D%2C%20using%20the%20score%20distillation%20sampling%20%28SDS%29%20loss%2C%20to%0Abypass%20the%20data%20scarcity%20problem.%20The%20existing%20text-to-3D%20methods%20have%20achieved%0Apromising%20results%20in%20realism%20and%203D%20consistency%2C%20but%20text-to-4D%20generation%0Astill%20faces%20challenges%2C%20including%20lack%20of%20realism%20and%20insufficient%20dynamic%0Amotions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20text-to-4D%20generation%2C%0Awhich%20ensures%20the%20dynamic%20amplitude%20and%20authenticity%20through%20direct%20supervision%0Aprovided%20by%20a%20video%20prior.%20Specifically%2C%20we%20adopt%20a%20text-to-video%20diffusion%0Amodel%20to%20generate%20a%20reference%20video%20and%20divide%204D%20generation%20into%20two%20stages%3A%0Astatic%20generation%20and%20dynamic%20generation.%20The%20static%203D%20generation%20is%20achieved%0Aunder%20the%20guidance%20of%20the%20input%20text%20and%20the%20first%20frame%20of%20the%20reference%0Avideo%2C%20while%20in%20the%20dynamic%20generation%20stage%2C%20we%20introduce%20a%20customized%20SDS%0Aloss%20to%20ensure%20multi-view%20consistency%2C%20a%20video-based%20SDS%20loss%20to%20improve%0Atemporal%20consistency%2C%20and%20most%20importantly%2C%20direct%20priors%20from%20the%20reference%0Avideo%20to%20ensure%20the%20quality%20of%20geometry%20and%20texture.%20Moreover%2C%20we%20design%20a%0Aprior-switching%20training%20strategy%20to%20avoid%20conflicts%20between%20different%20priors%0Aand%20fully%20leverage%20the%20benefits%20of%20each%20prior.%20In%20addition%2C%20to%20enrich%20the%0Agenerated%20motion%2C%20we%20further%20introduce%20a%20dynamic%20modeling%20representation%0Acomposed%20of%20a%20deformation%20network%20and%20a%20topology%20network%2C%20which%20ensures%20dynamic%0Acontinuity%20while%20modeling%20topological%20changes.%20Our%20method%20not%20only%20supports%0Atext-to-4D%20generation%20but%20also%20enables%204D%20generation%20from%20monocular%20videos.%20The%0Acomparison%20experiments%20demonstrate%20the%20superiority%20of%20our%20method%20compared%20to%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4Dynamic%253A%2520Text-to-4D%2520Generation%2520with%2520Hybrid%2520Priors%26entry.906535625%3DYu-Jie%2520Yuan%2520and%2520Leif%2520Kobbelt%2520and%2520Jiwen%2520Liu%2520and%2520Yuan%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Yu-Kun%2520Lai%2520and%2520Lin%2520Gao%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520fascinating%2520generative%2520performance%2520of%2520text-to-image%2520diffusion%250Amodels%252C%2520growing%2520text-to-3D%2520generation%2520works%2520explore%2520distilling%2520the%25202D%250Agenerative%2520priors%2520into%25203D%252C%2520using%2520the%2520score%2520distillation%2520sampling%2520%2528SDS%2529%2520loss%252C%2520to%250Abypass%2520the%2520data%2520scarcity%2520problem.%2520The%2520existing%2520text-to-3D%2520methods%2520have%2520achieved%250Apromising%2520results%2520in%2520realism%2520and%25203D%2520consistency%252C%2520but%2520text-to-4D%2520generation%250Astill%2520faces%2520challenges%252C%2520including%2520lack%2520of%2520realism%2520and%2520insufficient%2520dynamic%250Amotions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520text-to-4D%2520generation%252C%250Awhich%2520ensures%2520the%2520dynamic%2520amplitude%2520and%2520authenticity%2520through%2520direct%2520supervision%250Aprovided%2520by%2520a%2520video%2520prior.%2520Specifically%252C%2520we%2520adopt%2520a%2520text-to-video%2520diffusion%250Amodel%2520to%2520generate%2520a%2520reference%2520video%2520and%2520divide%25204D%2520generation%2520into%2520two%2520stages%253A%250Astatic%2520generation%2520and%2520dynamic%2520generation.%2520The%2520static%25203D%2520generation%2520is%2520achieved%250Aunder%2520the%2520guidance%2520of%2520the%2520input%2520text%2520and%2520the%2520first%2520frame%2520of%2520the%2520reference%250Avideo%252C%2520while%2520in%2520the%2520dynamic%2520generation%2520stage%252C%2520we%2520introduce%2520a%2520customized%2520SDS%250Aloss%2520to%2520ensure%2520multi-view%2520consistency%252C%2520a%2520video-based%2520SDS%2520loss%2520to%2520improve%250Atemporal%2520consistency%252C%2520and%2520most%2520importantly%252C%2520direct%2520priors%2520from%2520the%2520reference%250Avideo%2520to%2520ensure%2520the%2520quality%2520of%2520geometry%2520and%2520texture.%2520Moreover%252C%2520we%2520design%2520a%250Aprior-switching%2520training%2520strategy%2520to%2520avoid%2520conflicts%2520between%2520different%2520priors%250Aand%2520fully%2520leverage%2520the%2520benefits%2520of%2520each%2520prior.%2520In%2520addition%252C%2520to%2520enrich%2520the%250Agenerated%2520motion%252C%2520we%2520further%2520introduce%2520a%2520dynamic%2520modeling%2520representation%250Acomposed%2520of%2520a%2520deformation%2520network%2520and%2520a%2520topology%2520network%252C%2520which%2520ensures%2520dynamic%250Acontinuity%2520while%2520modeling%2520topological%2520changes.%2520Our%2520method%2520not%2520only%2520supports%250Atext-to-4D%2520generation%2520but%2520also%2520enables%25204D%2520generation%2520from%2520monocular%2520videos.%2520The%250Acomparison%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520compared%2520to%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4Dynamic%3A%20Text-to-4D%20Generation%20with%20Hybrid%20Priors&entry.906535625=Yu-Jie%20Yuan%20and%20Leif%20Kobbelt%20and%20Jiwen%20Liu%20and%20Yuan%20Zhang%20and%20Pengfei%20Wan%20and%20Yu-Kun%20Lai%20and%20Lin%20Gao&entry.1292438233=%20%20Due%20to%20the%20fascinating%20generative%20performance%20of%20text-to-image%20diffusion%0Amodels%2C%20growing%20text-to-3D%20generation%20works%20explore%20distilling%20the%202D%0Agenerative%20priors%20into%203D%2C%20using%20the%20score%20distillation%20sampling%20%28SDS%29%20loss%2C%20to%0Abypass%20the%20data%20scarcity%20problem.%20The%20existing%20text-to-3D%20methods%20have%20achieved%0Apromising%20results%20in%20realism%20and%203D%20consistency%2C%20but%20text-to-4D%20generation%0Astill%20faces%20challenges%2C%20including%20lack%20of%20realism%20and%20insufficient%20dynamic%0Amotions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20text-to-4D%20generation%2C%0Awhich%20ensures%20the%20dynamic%20amplitude%20and%20authenticity%20through%20direct%20supervision%0Aprovided%20by%20a%20video%20prior.%20Specifically%2C%20we%20adopt%20a%20text-to-video%20diffusion%0Amodel%20to%20generate%20a%20reference%20video%20and%20divide%204D%20generation%20into%20two%20stages%3A%0Astatic%20generation%20and%20dynamic%20generation.%20The%20static%203D%20generation%20is%20achieved%0Aunder%20the%20guidance%20of%20the%20input%20text%20and%20the%20first%20frame%20of%20the%20reference%0Avideo%2C%20while%20in%20the%20dynamic%20generation%20stage%2C%20we%20introduce%20a%20customized%20SDS%0Aloss%20to%20ensure%20multi-view%20consistency%2C%20a%20video-based%20SDS%20loss%20to%20improve%0Atemporal%20consistency%2C%20and%20most%20importantly%2C%20direct%20priors%20from%20the%20reference%0Avideo%20to%20ensure%20the%20quality%20of%20geometry%20and%20texture.%20Moreover%2C%20we%20design%20a%0Aprior-switching%20training%20strategy%20to%20avoid%20conflicts%20between%20different%20priors%0Aand%20fully%20leverage%20the%20benefits%20of%20each%20prior.%20In%20addition%2C%20to%20enrich%20the%0Agenerated%20motion%2C%20we%20further%20introduce%20a%20dynamic%20modeling%20representation%0Acomposed%20of%20a%20deformation%20network%20and%20a%20topology%20network%2C%20which%20ensures%20dynamic%0Acontinuity%20while%20modeling%20topological%20changes.%20Our%20method%20not%20only%20supports%0Atext-to-4D%20generation%20but%20also%20enables%204D%20generation%20from%20monocular%20videos.%20The%0Acomparison%20experiments%20demonstrate%20the%20superiority%20of%20our%20method%20compared%20to%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12684v1&entry.124074799=Read"},
{"title": "MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under\n  Uncertainty", "author": "Tim Br\u00f6dermann and David Bruggemann and Christos Sakaridis and Kevin Ta and Odysseas Liagouris and Jason Corkill and Luc Van Gool", "abstract": "  Achieving level-5 driving automation in autonomous vehicles necessitates a\nrobust semantic visual perception system capable of parsing data from different\nsensors across diverse conditions. However, existing semantic perception\ndatasets often lack important non-camera modalities typically used in\nautonomous vehicles, or they do not exploit such modalities to aid and improve\nsemantic annotations in challenging conditions. To address this, we introduce\nMUSES, the MUlti-SEnsor Semantic perception dataset for driving in adverse\nconditions under increased uncertainty. MUSES includes synchronized multimodal\nrecordings with 2D panoptic annotations for 2500 images captured under diverse\nweather and illumination. The dataset integrates a frame camera, a lidar, a\nradar, an event camera, and an IMU/GNSS sensor. Our new two-stage panoptic\nannotation protocol captures both class-level and instance-level uncertainty in\nthe ground truth and enables the novel task of uncertainty-aware panoptic\nsegmentation we introduce, along with standard semantic and panoptic\nsegmentation. MUSES proves both effective for training and challenging for\nevaluating models under diverse visual conditions, and it opens new avenues for\nresearch in multimodal and uncertainty-aware dense semantic perception. Our\ndataset and benchmark are publicly available at\nhttps://muses.vision.ee.ethz.ch.\n", "link": "http://arxiv.org/abs/2401.12761v3", "date": "2024-07-17", "relevancy": 1.9188, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6582}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.64}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUSES%3A%20The%20Multi-Sensor%20Semantic%20Perception%20Dataset%20for%20Driving%20under%0A%20%20Uncertainty&body=Title%3A%20MUSES%3A%20The%20Multi-Sensor%20Semantic%20Perception%20Dataset%20for%20Driving%20under%0A%20%20Uncertainty%0AAuthor%3A%20Tim%20Br%C3%B6dermann%20and%20David%20Bruggemann%20and%20Christos%20Sakaridis%20and%20Kevin%20Ta%20and%20Odysseas%20Liagouris%20and%20Jason%20Corkill%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Achieving%20level-5%20driving%20automation%20in%20autonomous%20vehicles%20necessitates%20a%0Arobust%20semantic%20visual%20perception%20system%20capable%20of%20parsing%20data%20from%20different%0Asensors%20across%20diverse%20conditions.%20However%2C%20existing%20semantic%20perception%0Adatasets%20often%20lack%20important%20non-camera%20modalities%20typically%20used%20in%0Aautonomous%20vehicles%2C%20or%20they%20do%20not%20exploit%20such%20modalities%20to%20aid%20and%20improve%0Asemantic%20annotations%20in%20challenging%20conditions.%20To%20address%20this%2C%20we%20introduce%0AMUSES%2C%20the%20MUlti-SEnsor%20Semantic%20perception%20dataset%20for%20driving%20in%20adverse%0Aconditions%20under%20increased%20uncertainty.%20MUSES%20includes%20synchronized%20multimodal%0Arecordings%20with%202D%20panoptic%20annotations%20for%202500%20images%20captured%20under%20diverse%0Aweather%20and%20illumination.%20The%20dataset%20integrates%20a%20frame%20camera%2C%20a%20lidar%2C%20a%0Aradar%2C%20an%20event%20camera%2C%20and%20an%20IMU/GNSS%20sensor.%20Our%20new%20two-stage%20panoptic%0Aannotation%20protocol%20captures%20both%20class-level%20and%20instance-level%20uncertainty%20in%0Athe%20ground%20truth%20and%20enables%20the%20novel%20task%20of%20uncertainty-aware%20panoptic%0Asegmentation%20we%20introduce%2C%20along%20with%20standard%20semantic%20and%20panoptic%0Asegmentation.%20MUSES%20proves%20both%20effective%20for%20training%20and%20challenging%20for%0Aevaluating%20models%20under%20diverse%20visual%20conditions%2C%20and%20it%20opens%20new%20avenues%20for%0Aresearch%20in%20multimodal%20and%20uncertainty-aware%20dense%20semantic%20perception.%20Our%0Adataset%20and%20benchmark%20are%20publicly%20available%20at%0Ahttps%3A//muses.vision.ee.ethz.ch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12761v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUSES%253A%2520The%2520Multi-Sensor%2520Semantic%2520Perception%2520Dataset%2520for%2520Driving%2520under%250A%2520%2520Uncertainty%26entry.906535625%3DTim%2520Br%25C3%25B6dermann%2520and%2520David%2520Bruggemann%2520and%2520Christos%2520Sakaridis%2520and%2520Kevin%2520Ta%2520and%2520Odysseas%2520Liagouris%2520and%2520Jason%2520Corkill%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Achieving%2520level-5%2520driving%2520automation%2520in%2520autonomous%2520vehicles%2520necessitates%2520a%250Arobust%2520semantic%2520visual%2520perception%2520system%2520capable%2520of%2520parsing%2520data%2520from%2520different%250Asensors%2520across%2520diverse%2520conditions.%2520However%252C%2520existing%2520semantic%2520perception%250Adatasets%2520often%2520lack%2520important%2520non-camera%2520modalities%2520typically%2520used%2520in%250Aautonomous%2520vehicles%252C%2520or%2520they%2520do%2520not%2520exploit%2520such%2520modalities%2520to%2520aid%2520and%2520improve%250Asemantic%2520annotations%2520in%2520challenging%2520conditions.%2520To%2520address%2520this%252C%2520we%2520introduce%250AMUSES%252C%2520the%2520MUlti-SEnsor%2520Semantic%2520perception%2520dataset%2520for%2520driving%2520in%2520adverse%250Aconditions%2520under%2520increased%2520uncertainty.%2520MUSES%2520includes%2520synchronized%2520multimodal%250Arecordings%2520with%25202D%2520panoptic%2520annotations%2520for%25202500%2520images%2520captured%2520under%2520diverse%250Aweather%2520and%2520illumination.%2520The%2520dataset%2520integrates%2520a%2520frame%2520camera%252C%2520a%2520lidar%252C%2520a%250Aradar%252C%2520an%2520event%2520camera%252C%2520and%2520an%2520IMU/GNSS%2520sensor.%2520Our%2520new%2520two-stage%2520panoptic%250Aannotation%2520protocol%2520captures%2520both%2520class-level%2520and%2520instance-level%2520uncertainty%2520in%250Athe%2520ground%2520truth%2520and%2520enables%2520the%2520novel%2520task%2520of%2520uncertainty-aware%2520panoptic%250Asegmentation%2520we%2520introduce%252C%2520along%2520with%2520standard%2520semantic%2520and%2520panoptic%250Asegmentation.%2520MUSES%2520proves%2520both%2520effective%2520for%2520training%2520and%2520challenging%2520for%250Aevaluating%2520models%2520under%2520diverse%2520visual%2520conditions%252C%2520and%2520it%2520opens%2520new%2520avenues%2520for%250Aresearch%2520in%2520multimodal%2520and%2520uncertainty-aware%2520dense%2520semantic%2520perception.%2520Our%250Adataset%2520and%2520benchmark%2520are%2520publicly%2520available%2520at%250Ahttps%253A//muses.vision.ee.ethz.ch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12761v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUSES%3A%20The%20Multi-Sensor%20Semantic%20Perception%20Dataset%20for%20Driving%20under%0A%20%20Uncertainty&entry.906535625=Tim%20Br%C3%B6dermann%20and%20David%20Bruggemann%20and%20Christos%20Sakaridis%20and%20Kevin%20Ta%20and%20Odysseas%20Liagouris%20and%20Jason%20Corkill%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Achieving%20level-5%20driving%20automation%20in%20autonomous%20vehicles%20necessitates%20a%0Arobust%20semantic%20visual%20perception%20system%20capable%20of%20parsing%20data%20from%20different%0Asensors%20across%20diverse%20conditions.%20However%2C%20existing%20semantic%20perception%0Adatasets%20often%20lack%20important%20non-camera%20modalities%20typically%20used%20in%0Aautonomous%20vehicles%2C%20or%20they%20do%20not%20exploit%20such%20modalities%20to%20aid%20and%20improve%0Asemantic%20annotations%20in%20challenging%20conditions.%20To%20address%20this%2C%20we%20introduce%0AMUSES%2C%20the%20MUlti-SEnsor%20Semantic%20perception%20dataset%20for%20driving%20in%20adverse%0Aconditions%20under%20increased%20uncertainty.%20MUSES%20includes%20synchronized%20multimodal%0Arecordings%20with%202D%20panoptic%20annotations%20for%202500%20images%20captured%20under%20diverse%0Aweather%20and%20illumination.%20The%20dataset%20integrates%20a%20frame%20camera%2C%20a%20lidar%2C%20a%0Aradar%2C%20an%20event%20camera%2C%20and%20an%20IMU/GNSS%20sensor.%20Our%20new%20two-stage%20panoptic%0Aannotation%20protocol%20captures%20both%20class-level%20and%20instance-level%20uncertainty%20in%0Athe%20ground%20truth%20and%20enables%20the%20novel%20task%20of%20uncertainty-aware%20panoptic%0Asegmentation%20we%20introduce%2C%20along%20with%20standard%20semantic%20and%20panoptic%0Asegmentation.%20MUSES%20proves%20both%20effective%20for%20training%20and%20challenging%20for%0Aevaluating%20models%20under%20diverse%20visual%20conditions%2C%20and%20it%20opens%20new%20avenues%20for%0Aresearch%20in%20multimodal%20and%20uncertainty-aware%20dense%20semantic%20perception.%20Our%0Adataset%20and%20benchmark%20are%20publicly%20available%20at%0Ahttps%3A//muses.vision.ee.ethz.ch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12761v3&entry.124074799=Read"},
{"title": "SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow", "author": "Yuanzhi Zhu and Xingchao Liu and Qiang Liu", "abstract": "  Diffusion models excel in high-quality generation but suffer from slow\ninference due to iterative sampling. While recent methods have successfully\ntransformed diffusion models into one-step generators, they neglect model size\nreduction, limiting their applicability in compute-constrained scenarios. This\npaper aims to develop small, efficient one-step diffusion models based on the\npowerful rectified flow framework, by exploring joint compression of inference\nsteps and model size. The rectified flow framework trains one-step generative\nmodels using two operations, reflow and distillation. Compared with the\noriginal framework, squeezing the model size brings two new challenges: (1) the\ninitialization mismatch between large teachers and small students during\nreflow; (2) the underperformance of naive distillation on small student models.\nTo overcome these issues, we propose Annealing Reflow and Flow-Guided\nDistillation, which together comprise our SlimFlow framework. With our novel\nframework, we train a one-step diffusion model with an FID of 5.02 and 15.7M\nparameters, outperforming the previous state-of-the-art one-step diffusion\nmodel (FID=6.47, 19.4M parameters) on CIFAR10. On ImageNet 64$\\times$64 and\nFFHQ 64$\\times$64, our method yields small one-step diffusion models that are\ncomparable to larger models, showcasing the effectiveness of our method in\ncreating compact, efficient one-step diffusion models.\n", "link": "http://arxiv.org/abs/2407.12718v1", "date": "2024-07-17", "relevancy": 1.9162, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7166}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6673}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlimFlow%3A%20Training%20Smaller%20One-Step%20Diffusion%20Models%20with%20Rectified%20Flow&body=Title%3A%20SlimFlow%3A%20Training%20Smaller%20One-Step%20Diffusion%20Models%20with%20Rectified%20Flow%0AAuthor%3A%20Yuanzhi%20Zhu%20and%20Xingchao%20Liu%20and%20Qiang%20Liu%0AAbstract%3A%20%20%20Diffusion%20models%20excel%20in%20high-quality%20generation%20but%20suffer%20from%20slow%0Ainference%20due%20to%20iterative%20sampling.%20While%20recent%20methods%20have%20successfully%0Atransformed%20diffusion%20models%20into%20one-step%20generators%2C%20they%20neglect%20model%20size%0Areduction%2C%20limiting%20their%20applicability%20in%20compute-constrained%20scenarios.%20This%0Apaper%20aims%20to%20develop%20small%2C%20efficient%20one-step%20diffusion%20models%20based%20on%20the%0Apowerful%20rectified%20flow%20framework%2C%20by%20exploring%20joint%20compression%20of%20inference%0Asteps%20and%20model%20size.%20The%20rectified%20flow%20framework%20trains%20one-step%20generative%0Amodels%20using%20two%20operations%2C%20reflow%20and%20distillation.%20Compared%20with%20the%0Aoriginal%20framework%2C%20squeezing%20the%20model%20size%20brings%20two%20new%20challenges%3A%20%281%29%20the%0Ainitialization%20mismatch%20between%20large%20teachers%20and%20small%20students%20during%0Areflow%3B%20%282%29%20the%20underperformance%20of%20naive%20distillation%20on%20small%20student%20models.%0ATo%20overcome%20these%20issues%2C%20we%20propose%20Annealing%20Reflow%20and%20Flow-Guided%0ADistillation%2C%20which%20together%20comprise%20our%20SlimFlow%20framework.%20With%20our%20novel%0Aframework%2C%20we%20train%20a%20one-step%20diffusion%20model%20with%20an%20FID%20of%205.02%20and%2015.7M%0Aparameters%2C%20outperforming%20the%20previous%20state-of-the-art%20one-step%20diffusion%0Amodel%20%28FID%3D6.47%2C%2019.4M%20parameters%29%20on%20CIFAR10.%20On%20ImageNet%2064%24%5Ctimes%2464%20and%0AFFHQ%2064%24%5Ctimes%2464%2C%20our%20method%20yields%20small%20one-step%20diffusion%20models%20that%20are%0Acomparable%20to%20larger%20models%2C%20showcasing%20the%20effectiveness%20of%20our%20method%20in%0Acreating%20compact%2C%20efficient%20one-step%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlimFlow%253A%2520Training%2520Smaller%2520One-Step%2520Diffusion%2520Models%2520with%2520Rectified%2520Flow%26entry.906535625%3DYuanzhi%2520Zhu%2520and%2520Xingchao%2520Liu%2520and%2520Qiang%2520Liu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520excel%2520in%2520high-quality%2520generation%2520but%2520suffer%2520from%2520slow%250Ainference%2520due%2520to%2520iterative%2520sampling.%2520While%2520recent%2520methods%2520have%2520successfully%250Atransformed%2520diffusion%2520models%2520into%2520one-step%2520generators%252C%2520they%2520neglect%2520model%2520size%250Areduction%252C%2520limiting%2520their%2520applicability%2520in%2520compute-constrained%2520scenarios.%2520This%250Apaper%2520aims%2520to%2520develop%2520small%252C%2520efficient%2520one-step%2520diffusion%2520models%2520based%2520on%2520the%250Apowerful%2520rectified%2520flow%2520framework%252C%2520by%2520exploring%2520joint%2520compression%2520of%2520inference%250Asteps%2520and%2520model%2520size.%2520The%2520rectified%2520flow%2520framework%2520trains%2520one-step%2520generative%250Amodels%2520using%2520two%2520operations%252C%2520reflow%2520and%2520distillation.%2520Compared%2520with%2520the%250Aoriginal%2520framework%252C%2520squeezing%2520the%2520model%2520size%2520brings%2520two%2520new%2520challenges%253A%2520%25281%2529%2520the%250Ainitialization%2520mismatch%2520between%2520large%2520teachers%2520and%2520small%2520students%2520during%250Areflow%253B%2520%25282%2529%2520the%2520underperformance%2520of%2520naive%2520distillation%2520on%2520small%2520student%2520models.%250ATo%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520Annealing%2520Reflow%2520and%2520Flow-Guided%250ADistillation%252C%2520which%2520together%2520comprise%2520our%2520SlimFlow%2520framework.%2520With%2520our%2520novel%250Aframework%252C%2520we%2520train%2520a%2520one-step%2520diffusion%2520model%2520with%2520an%2520FID%2520of%25205.02%2520and%252015.7M%250Aparameters%252C%2520outperforming%2520the%2520previous%2520state-of-the-art%2520one-step%2520diffusion%250Amodel%2520%2528FID%253D6.47%252C%252019.4M%2520parameters%2529%2520on%2520CIFAR10.%2520On%2520ImageNet%252064%2524%255Ctimes%252464%2520and%250AFFHQ%252064%2524%255Ctimes%252464%252C%2520our%2520method%2520yields%2520small%2520one-step%2520diffusion%2520models%2520that%2520are%250Acomparable%2520to%2520larger%2520models%252C%2520showcasing%2520the%2520effectiveness%2520of%2520our%2520method%2520in%250Acreating%2520compact%252C%2520efficient%2520one-step%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlimFlow%3A%20Training%20Smaller%20One-Step%20Diffusion%20Models%20with%20Rectified%20Flow&entry.906535625=Yuanzhi%20Zhu%20and%20Xingchao%20Liu%20and%20Qiang%20Liu&entry.1292438233=%20%20Diffusion%20models%20excel%20in%20high-quality%20generation%20but%20suffer%20from%20slow%0Ainference%20due%20to%20iterative%20sampling.%20While%20recent%20methods%20have%20successfully%0Atransformed%20diffusion%20models%20into%20one-step%20generators%2C%20they%20neglect%20model%20size%0Areduction%2C%20limiting%20their%20applicability%20in%20compute-constrained%20scenarios.%20This%0Apaper%20aims%20to%20develop%20small%2C%20efficient%20one-step%20diffusion%20models%20based%20on%20the%0Apowerful%20rectified%20flow%20framework%2C%20by%20exploring%20joint%20compression%20of%20inference%0Asteps%20and%20model%20size.%20The%20rectified%20flow%20framework%20trains%20one-step%20generative%0Amodels%20using%20two%20operations%2C%20reflow%20and%20distillation.%20Compared%20with%20the%0Aoriginal%20framework%2C%20squeezing%20the%20model%20size%20brings%20two%20new%20challenges%3A%20%281%29%20the%0Ainitialization%20mismatch%20between%20large%20teachers%20and%20small%20students%20during%0Areflow%3B%20%282%29%20the%20underperformance%20of%20naive%20distillation%20on%20small%20student%20models.%0ATo%20overcome%20these%20issues%2C%20we%20propose%20Annealing%20Reflow%20and%20Flow-Guided%0ADistillation%2C%20which%20together%20comprise%20our%20SlimFlow%20framework.%20With%20our%20novel%0Aframework%2C%20we%20train%20a%20one-step%20diffusion%20model%20with%20an%20FID%20of%205.02%20and%2015.7M%0Aparameters%2C%20outperforming%20the%20previous%20state-of-the-art%20one-step%20diffusion%0Amodel%20%28FID%3D6.47%2C%2019.4M%20parameters%29%20on%20CIFAR10.%20On%20ImageNet%2064%24%5Ctimes%2464%20and%0AFFHQ%2064%24%5Ctimes%2464%2C%20our%20method%20yields%20small%20one-step%20diffusion%20models%20that%20are%0Acomparable%20to%20larger%20models%2C%20showcasing%20the%20effectiveness%20of%20our%20method%20in%0Acreating%20compact%2C%20efficient%20one-step%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12718v1&entry.124074799=Read"},
{"title": "n-Step Temporal Difference Learning with Optimal n", "author": "Lakshmi Mandal and Shalabh Bhatnagar", "abstract": "  We consider the problem of finding the optimal value of n in the n-step\ntemporal difference (TD) learning algorithm. Our objective function for the\noptimization problem is the average root mean squared error (RMSE). We find the\noptimal n by resorting to a model-free optimization technique involving a\none-simulation simultaneous perturbation stochastic approximation (SPSA) based\nprocedure. Whereas SPSA is a zeroth-order continuous optimization procedure, we\nadapt it to the discrete optimization setting by using a random projection\noperator. We prove the asymptotic convergence of the recursion by showing that\nthe sequence of n-updates obtained using zeroth-order stochastic gradient\nsearch converges almost surely to an internally chain transitive invariant set\nof an associated differential inclusion. This results in convergence of the\ndiscrete parameter sequence to the optimal n in n-step TD. Through experiments,\nwe show that the optimal value of n is achieved with our SDPSA algorithm for\narbitrary initial values. We further show using numerical evaluations that\nSDPSA outperforms the state-of-the-art discrete parameter stochastic\noptimization algorithm Optimal Computing Budget Allocation (OCBA) on benchmark\nRL tasks.\n", "link": "http://arxiv.org/abs/2303.07068v5", "date": "2024-07-17", "relevancy": 1.8741, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5054}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4695}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20n-Step%20Temporal%20Difference%20Learning%20with%20Optimal%20n&body=Title%3A%20n-Step%20Temporal%20Difference%20Learning%20with%20Optimal%20n%0AAuthor%3A%20Lakshmi%20Mandal%20and%20Shalabh%20Bhatnagar%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20finding%20the%20optimal%20value%20of%20n%20in%20the%20n-step%0Atemporal%20difference%20%28TD%29%20learning%20algorithm.%20Our%20objective%20function%20for%20the%0Aoptimization%20problem%20is%20the%20average%20root%20mean%20squared%20error%20%28RMSE%29.%20We%20find%20the%0Aoptimal%20n%20by%20resorting%20to%20a%20model-free%20optimization%20technique%20involving%20a%0Aone-simulation%20simultaneous%20perturbation%20stochastic%20approximation%20%28SPSA%29%20based%0Aprocedure.%20Whereas%20SPSA%20is%20a%20zeroth-order%20continuous%20optimization%20procedure%2C%20we%0Aadapt%20it%20to%20the%20discrete%20optimization%20setting%20by%20using%20a%20random%20projection%0Aoperator.%20We%20prove%20the%20asymptotic%20convergence%20of%20the%20recursion%20by%20showing%20that%0Athe%20sequence%20of%20n-updates%20obtained%20using%20zeroth-order%20stochastic%20gradient%0Asearch%20converges%20almost%20surely%20to%20an%20internally%20chain%20transitive%20invariant%20set%0Aof%20an%20associated%20differential%20inclusion.%20This%20results%20in%20convergence%20of%20the%0Adiscrete%20parameter%20sequence%20to%20the%20optimal%20n%20in%20n-step%20TD.%20Through%20experiments%2C%0Awe%20show%20that%20the%20optimal%20value%20of%20n%20is%20achieved%20with%20our%20SDPSA%20algorithm%20for%0Aarbitrary%20initial%20values.%20We%20further%20show%20using%20numerical%20evaluations%20that%0ASDPSA%20outperforms%20the%20state-of-the-art%20discrete%20parameter%20stochastic%0Aoptimization%20algorithm%20Optimal%20Computing%20Budget%20Allocation%20%28OCBA%29%20on%20benchmark%0ARL%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.07068v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dn-Step%2520Temporal%2520Difference%2520Learning%2520with%2520Optimal%2520n%26entry.906535625%3DLakshmi%2520Mandal%2520and%2520Shalabh%2520Bhatnagar%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520finding%2520the%2520optimal%2520value%2520of%2520n%2520in%2520the%2520n-step%250Atemporal%2520difference%2520%2528TD%2529%2520learning%2520algorithm.%2520Our%2520objective%2520function%2520for%2520the%250Aoptimization%2520problem%2520is%2520the%2520average%2520root%2520mean%2520squared%2520error%2520%2528RMSE%2529.%2520We%2520find%2520the%250Aoptimal%2520n%2520by%2520resorting%2520to%2520a%2520model-free%2520optimization%2520technique%2520involving%2520a%250Aone-simulation%2520simultaneous%2520perturbation%2520stochastic%2520approximation%2520%2528SPSA%2529%2520based%250Aprocedure.%2520Whereas%2520SPSA%2520is%2520a%2520zeroth-order%2520continuous%2520optimization%2520procedure%252C%2520we%250Aadapt%2520it%2520to%2520the%2520discrete%2520optimization%2520setting%2520by%2520using%2520a%2520random%2520projection%250Aoperator.%2520We%2520prove%2520the%2520asymptotic%2520convergence%2520of%2520the%2520recursion%2520by%2520showing%2520that%250Athe%2520sequence%2520of%2520n-updates%2520obtained%2520using%2520zeroth-order%2520stochastic%2520gradient%250Asearch%2520converges%2520almost%2520surely%2520to%2520an%2520internally%2520chain%2520transitive%2520invariant%2520set%250Aof%2520an%2520associated%2520differential%2520inclusion.%2520This%2520results%2520in%2520convergence%2520of%2520the%250Adiscrete%2520parameter%2520sequence%2520to%2520the%2520optimal%2520n%2520in%2520n-step%2520TD.%2520Through%2520experiments%252C%250Awe%2520show%2520that%2520the%2520optimal%2520value%2520of%2520n%2520is%2520achieved%2520with%2520our%2520SDPSA%2520algorithm%2520for%250Aarbitrary%2520initial%2520values.%2520We%2520further%2520show%2520using%2520numerical%2520evaluations%2520that%250ASDPSA%2520outperforms%2520the%2520state-of-the-art%2520discrete%2520parameter%2520stochastic%250Aoptimization%2520algorithm%2520Optimal%2520Computing%2520Budget%2520Allocation%2520%2528OCBA%2529%2520on%2520benchmark%250ARL%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.07068v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=n-Step%20Temporal%20Difference%20Learning%20with%20Optimal%20n&entry.906535625=Lakshmi%20Mandal%20and%20Shalabh%20Bhatnagar&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20finding%20the%20optimal%20value%20of%20n%20in%20the%20n-step%0Atemporal%20difference%20%28TD%29%20learning%20algorithm.%20Our%20objective%20function%20for%20the%0Aoptimization%20problem%20is%20the%20average%20root%20mean%20squared%20error%20%28RMSE%29.%20We%20find%20the%0Aoptimal%20n%20by%20resorting%20to%20a%20model-free%20optimization%20technique%20involving%20a%0Aone-simulation%20simultaneous%20perturbation%20stochastic%20approximation%20%28SPSA%29%20based%0Aprocedure.%20Whereas%20SPSA%20is%20a%20zeroth-order%20continuous%20optimization%20procedure%2C%20we%0Aadapt%20it%20to%20the%20discrete%20optimization%20setting%20by%20using%20a%20random%20projection%0Aoperator.%20We%20prove%20the%20asymptotic%20convergence%20of%20the%20recursion%20by%20showing%20that%0Athe%20sequence%20of%20n-updates%20obtained%20using%20zeroth-order%20stochastic%20gradient%0Asearch%20converges%20almost%20surely%20to%20an%20internally%20chain%20transitive%20invariant%20set%0Aof%20an%20associated%20differential%20inclusion.%20This%20results%20in%20convergence%20of%20the%0Adiscrete%20parameter%20sequence%20to%20the%20optimal%20n%20in%20n-step%20TD.%20Through%20experiments%2C%0Awe%20show%20that%20the%20optimal%20value%20of%20n%20is%20achieved%20with%20our%20SDPSA%20algorithm%20for%0Aarbitrary%20initial%20values.%20We%20further%20show%20using%20numerical%20evaluations%20that%0ASDPSA%20outperforms%20the%20state-of-the-art%20discrete%20parameter%20stochastic%0Aoptimization%20algorithm%20Optimal%20Computing%20Budget%20Allocation%20%28OCBA%29%20on%20benchmark%0ARL%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.07068v5&entry.124074799=Read"},
{"title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large\n  Language Models -- The Story Goes On", "author": "Liang Zeng and Liangjun Zhong and Liang Zhao and Tianwen Wei and Liu Yang and Jujie He and Cheng Cheng and Rui Hu and Yang Liu and Shuicheng Yan and Han Fang and Yahui Zhou", "abstract": "  In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.\n", "link": "http://arxiv.org/abs/2407.08348v2", "date": "2024-07-17", "relevancy": 1.8373, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4544}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skywork-Math%3A%20Data%20Scaling%20Laws%20for%20Mathematical%20Reasoning%20in%20Large%0A%20%20Language%20Models%20--%20The%20Story%20Goes%20On&body=Title%3A%20Skywork-Math%3A%20Data%20Scaling%20Laws%20for%20Mathematical%20Reasoning%20in%20Large%0A%20%20Language%20Models%20--%20The%20Story%20Goes%20On%0AAuthor%3A%20Liang%20Zeng%20and%20Liangjun%20Zhong%20and%20Liang%20Zhao%20and%20Tianwen%20Wei%20and%20Liu%20Yang%20and%20Jujie%20He%20and%20Cheng%20Cheng%20and%20Rui%20Hu%20and%20Yang%20Liu%20and%20Shuicheng%20Yan%20and%20Han%20Fang%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20underlying%20factors%20that%20potentially%20enhance%0Athe%20mathematical%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20We%0Aargue%20that%20the%20data%20scaling%20law%20for%20math%20reasoning%20capabilities%20in%20modern%20LLMs%0Ais%20far%20from%20being%20saturated%2C%20highlighting%20how%20the%20model%27s%20quality%20improves%20with%0Aincreases%20in%20data%20quantity.%20To%20support%20this%20claim%2C%20we%20introduce%20the%0ASkywork-Math%20model%20series%2C%20supervised%20fine-tuned%20%28SFT%29%20on%20common%207B%20LLMs%20using%0Aour%20proposed%202.5M-instance%20Skywork-MathQA%20dataset.%20Skywork-Math%207B%20has%20achieved%0Aimpressive%20accuracies%20of%2051.2%25%20on%20the%20competition-level%20MATH%20benchmark%20and%0A83.9%25%20on%20the%20GSM8K%20benchmark%20using%20only%20SFT%20data%2C%20outperforming%20an%20early%0Aversion%20of%20GPT-4%20on%20MATH.%20The%20superior%20performance%20of%20Skywork-Math%20models%0Acontributes%20to%20our%20novel%20two-stage%20data%20synthesis%20and%20model%20SFT%20pipelines%2C%0Awhich%20include%20three%20different%20augmentation%20methods%20and%20a%20diverse%20seed%20problem%0Aset%2C%20ensuring%20both%20the%20quantity%20and%20quality%20of%20Skywork-MathQA%20dataset%20across%0Avarying%20difficulty%20levels.%20Most%20importantly%2C%20we%20provide%20several%20practical%0Atakeaways%20to%20enhance%20math%20reasoning%20abilities%20in%20LLMs%20for%20both%20research%20and%0Aindustry%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08348v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkywork-Math%253A%2520Data%2520Scaling%2520Laws%2520for%2520Mathematical%2520Reasoning%2520in%2520Large%250A%2520%2520Language%2520Models%2520--%2520The%2520Story%2520Goes%2520On%26entry.906535625%3DLiang%2520Zeng%2520and%2520Liangjun%2520Zhong%2520and%2520Liang%2520Zhao%2520and%2520Tianwen%2520Wei%2520and%2520Liu%2520Yang%2520and%2520Jujie%2520He%2520and%2520Cheng%2520Cheng%2520and%2520Rui%2520Hu%2520and%2520Yang%2520Liu%2520and%2520Shuicheng%2520Yan%2520and%2520Han%2520Fang%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520underlying%2520factors%2520that%2520potentially%2520enhance%250Athe%2520mathematical%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%250Aargue%2520that%2520the%2520data%2520scaling%2520law%2520for%2520math%2520reasoning%2520capabilities%2520in%2520modern%2520LLMs%250Ais%2520far%2520from%2520being%2520saturated%252C%2520highlighting%2520how%2520the%2520model%2527s%2520quality%2520improves%2520with%250Aincreases%2520in%2520data%2520quantity.%2520To%2520support%2520this%2520claim%252C%2520we%2520introduce%2520the%250ASkywork-Math%2520model%2520series%252C%2520supervised%2520fine-tuned%2520%2528SFT%2529%2520on%2520common%25207B%2520LLMs%2520using%250Aour%2520proposed%25202.5M-instance%2520Skywork-MathQA%2520dataset.%2520Skywork-Math%25207B%2520has%2520achieved%250Aimpressive%2520accuracies%2520of%252051.2%2525%2520on%2520the%2520competition-level%2520MATH%2520benchmark%2520and%250A83.9%2525%2520on%2520the%2520GSM8K%2520benchmark%2520using%2520only%2520SFT%2520data%252C%2520outperforming%2520an%2520early%250Aversion%2520of%2520GPT-4%2520on%2520MATH.%2520The%2520superior%2520performance%2520of%2520Skywork-Math%2520models%250Acontributes%2520to%2520our%2520novel%2520two-stage%2520data%2520synthesis%2520and%2520model%2520SFT%2520pipelines%252C%250Awhich%2520include%2520three%2520different%2520augmentation%2520methods%2520and%2520a%2520diverse%2520seed%2520problem%250Aset%252C%2520ensuring%2520both%2520the%2520quantity%2520and%2520quality%2520of%2520Skywork-MathQA%2520dataset%2520across%250Avarying%2520difficulty%2520levels.%2520Most%2520importantly%252C%2520we%2520provide%2520several%2520practical%250Atakeaways%2520to%2520enhance%2520math%2520reasoning%2520abilities%2520in%2520LLMs%2520for%2520both%2520research%2520and%250Aindustry%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08348v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skywork-Math%3A%20Data%20Scaling%20Laws%20for%20Mathematical%20Reasoning%20in%20Large%0A%20%20Language%20Models%20--%20The%20Story%20Goes%20On&entry.906535625=Liang%20Zeng%20and%20Liangjun%20Zhong%20and%20Liang%20Zhao%20and%20Tianwen%20Wei%20and%20Liu%20Yang%20and%20Jujie%20He%20and%20Cheng%20Cheng%20and%20Rui%20Hu%20and%20Yang%20Liu%20and%20Shuicheng%20Yan%20and%20Han%20Fang%20and%20Yahui%20Zhou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20underlying%20factors%20that%20potentially%20enhance%0Athe%20mathematical%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20We%0Aargue%20that%20the%20data%20scaling%20law%20for%20math%20reasoning%20capabilities%20in%20modern%20LLMs%0Ais%20far%20from%20being%20saturated%2C%20highlighting%20how%20the%20model%27s%20quality%20improves%20with%0Aincreases%20in%20data%20quantity.%20To%20support%20this%20claim%2C%20we%20introduce%20the%0ASkywork-Math%20model%20series%2C%20supervised%20fine-tuned%20%28SFT%29%20on%20common%207B%20LLMs%20using%0Aour%20proposed%202.5M-instance%20Skywork-MathQA%20dataset.%20Skywork-Math%207B%20has%20achieved%0Aimpressive%20accuracies%20of%2051.2%25%20on%20the%20competition-level%20MATH%20benchmark%20and%0A83.9%25%20on%20the%20GSM8K%20benchmark%20using%20only%20SFT%20data%2C%20outperforming%20an%20early%0Aversion%20of%20GPT-4%20on%20MATH.%20The%20superior%20performance%20of%20Skywork-Math%20models%0Acontributes%20to%20our%20novel%20two-stage%20data%20synthesis%20and%20model%20SFT%20pipelines%2C%0Awhich%20include%20three%20different%20augmentation%20methods%20and%20a%20diverse%20seed%20problem%0Aset%2C%20ensuring%20both%20the%20quantity%20and%20quality%20of%20Skywork-MathQA%20dataset%20across%0Avarying%20difficulty%20levels.%20Most%20importantly%2C%20we%20provide%20several%20practical%0Atakeaways%20to%20enhance%20math%20reasoning%20abilities%20in%20LLMs%20for%20both%20research%20and%0Aindustry%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08348v2&entry.124074799=Read"},
{"title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge\n  Bases", "author": "Zhaorun Chen and Zhen Xiang and Chaowei Xiao and Dawn Song and Bo Li", "abstract": "  LLM agents have demonstrated remarkable performance across various\napplications, primarily due to their advanced capabilities in reasoning,\nutilizing external knowledge and tools, calling APIs, and executing actions to\ninteract with environments. Current agents typically utilize a memory module or\na retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and\ninstances with similar embeddings from knowledge bases to inform task planning\nand execution. However, the reliance on unverified knowledge bases raises\nsignificant concerns about their safety and trustworthiness. To uncover such\nvulnerabilities, we propose a novel red teaming approach AgentPoison, the first\nbackdoor attack targeting generic and RAG-based LLM agents by poisoning their\nlong-term memory or RAG knowledge base. In particular, we form the trigger\ngeneration process as a constrained optimization to optimize backdoor triggers\nby mapping the triggered instances to a unique embedding space, so as to ensure\nthat whenever a user instruction contains the optimized backdoor trigger, the\nmalicious demonstrations are retrieved from the poisoned memory or knowledge\nbase with high probability. In the meantime, benign instructions without the\ntrigger will still maintain normal performance. Unlike conventional backdoor\nattacks, AgentPoison requires no additional model training or fine-tuning, and\nthe optimized backdoor trigger exhibits superior transferability, in-context\ncoherence, and stealthiness. Extensive experiments demonstrate AgentPoison's\neffectiveness in attacking three types of real-world LLM agents: RAG-based\nautonomous driving agent, knowledge-intensive QA agent, and healthcare\nEHRAgent. On each agent, AgentPoison achieves an average attack success rate\nhigher than 80% with minimal impact on benign performance (less than 1%) with a\npoison rate less than 0.1%.\n", "link": "http://arxiv.org/abs/2407.12784v1", "date": "2024-07-17", "relevancy": 1.8346, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4977}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4492}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentPoison%3A%20Red-teaming%20LLM%20Agents%20via%20Poisoning%20Memory%20or%20Knowledge%0A%20%20Bases&body=Title%3A%20AgentPoison%3A%20Red-teaming%20LLM%20Agents%20via%20Poisoning%20Memory%20or%20Knowledge%0A%20%20Bases%0AAuthor%3A%20Zhaorun%20Chen%20and%20Zhen%20Xiang%20and%20Chaowei%20Xiao%20and%20Dawn%20Song%20and%20Bo%20Li%0AAbstract%3A%20%20%20LLM%20agents%20have%20demonstrated%20remarkable%20performance%20across%20various%0Aapplications%2C%20primarily%20due%20to%20their%20advanced%20capabilities%20in%20reasoning%2C%0Autilizing%20external%20knowledge%20and%20tools%2C%20calling%20APIs%2C%20and%20executing%20actions%20to%0Ainteract%20with%20environments.%20Current%20agents%20typically%20utilize%20a%20memory%20module%20or%0Aa%20retrieval-augmented%20generation%20%28RAG%29%20mechanism%2C%20retrieving%20past%20knowledge%20and%0Ainstances%20with%20similar%20embeddings%20from%20knowledge%20bases%20to%20inform%20task%20planning%0Aand%20execution.%20However%2C%20the%20reliance%20on%20unverified%20knowledge%20bases%20raises%0Asignificant%20concerns%20about%20their%20safety%20and%20trustworthiness.%20To%20uncover%20such%0Avulnerabilities%2C%20we%20propose%20a%20novel%20red%20teaming%20approach%20AgentPoison%2C%20the%20first%0Abackdoor%20attack%20targeting%20generic%20and%20RAG-based%20LLM%20agents%20by%20poisoning%20their%0Along-term%20memory%20or%20RAG%20knowledge%20base.%20In%20particular%2C%20we%20form%20the%20trigger%0Ageneration%20process%20as%20a%20constrained%20optimization%20to%20optimize%20backdoor%20triggers%0Aby%20mapping%20the%20triggered%20instances%20to%20a%20unique%20embedding%20space%2C%20so%20as%20to%20ensure%0Athat%20whenever%20a%20user%20instruction%20contains%20the%20optimized%20backdoor%20trigger%2C%20the%0Amalicious%20demonstrations%20are%20retrieved%20from%20the%20poisoned%20memory%20or%20knowledge%0Abase%20with%20high%20probability.%20In%20the%20meantime%2C%20benign%20instructions%20without%20the%0Atrigger%20will%20still%20maintain%20normal%20performance.%20Unlike%20conventional%20backdoor%0Aattacks%2C%20AgentPoison%20requires%20no%20additional%20model%20training%20or%20fine-tuning%2C%20and%0Athe%20optimized%20backdoor%20trigger%20exhibits%20superior%20transferability%2C%20in-context%0Acoherence%2C%20and%20stealthiness.%20Extensive%20experiments%20demonstrate%20AgentPoison%27s%0Aeffectiveness%20in%20attacking%20three%20types%20of%20real-world%20LLM%20agents%3A%20RAG-based%0Aautonomous%20driving%20agent%2C%20knowledge-intensive%20QA%20agent%2C%20and%20healthcare%0AEHRAgent.%20On%20each%20agent%2C%20AgentPoison%20achieves%20an%20average%20attack%20success%20rate%0Ahigher%20than%2080%25%20with%20minimal%20impact%20on%20benign%20performance%20%28less%20than%201%25%29%20with%20a%0Apoison%20rate%20less%20than%200.1%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentPoison%253A%2520Red-teaming%2520LLM%2520Agents%2520via%2520Poisoning%2520Memory%2520or%2520Knowledge%250A%2520%2520Bases%26entry.906535625%3DZhaorun%2520Chen%2520and%2520Zhen%2520Xiang%2520and%2520Chaowei%2520Xiao%2520and%2520Dawn%2520Song%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520LLM%2520agents%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520various%250Aapplications%252C%2520primarily%2520due%2520to%2520their%2520advanced%2520capabilities%2520in%2520reasoning%252C%250Autilizing%2520external%2520knowledge%2520and%2520tools%252C%2520calling%2520APIs%252C%2520and%2520executing%2520actions%2520to%250Ainteract%2520with%2520environments.%2520Current%2520agents%2520typically%2520utilize%2520a%2520memory%2520module%2520or%250Aa%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520mechanism%252C%2520retrieving%2520past%2520knowledge%2520and%250Ainstances%2520with%2520similar%2520embeddings%2520from%2520knowledge%2520bases%2520to%2520inform%2520task%2520planning%250Aand%2520execution.%2520However%252C%2520the%2520reliance%2520on%2520unverified%2520knowledge%2520bases%2520raises%250Asignificant%2520concerns%2520about%2520their%2520safety%2520and%2520trustworthiness.%2520To%2520uncover%2520such%250Avulnerabilities%252C%2520we%2520propose%2520a%2520novel%2520red%2520teaming%2520approach%2520AgentPoison%252C%2520the%2520first%250Abackdoor%2520attack%2520targeting%2520generic%2520and%2520RAG-based%2520LLM%2520agents%2520by%2520poisoning%2520their%250Along-term%2520memory%2520or%2520RAG%2520knowledge%2520base.%2520In%2520particular%252C%2520we%2520form%2520the%2520trigger%250Ageneration%2520process%2520as%2520a%2520constrained%2520optimization%2520to%2520optimize%2520backdoor%2520triggers%250Aby%2520mapping%2520the%2520triggered%2520instances%2520to%2520a%2520unique%2520embedding%2520space%252C%2520so%2520as%2520to%2520ensure%250Athat%2520whenever%2520a%2520user%2520instruction%2520contains%2520the%2520optimized%2520backdoor%2520trigger%252C%2520the%250Amalicious%2520demonstrations%2520are%2520retrieved%2520from%2520the%2520poisoned%2520memory%2520or%2520knowledge%250Abase%2520with%2520high%2520probability.%2520In%2520the%2520meantime%252C%2520benign%2520instructions%2520without%2520the%250Atrigger%2520will%2520still%2520maintain%2520normal%2520performance.%2520Unlike%2520conventional%2520backdoor%250Aattacks%252C%2520AgentPoison%2520requires%2520no%2520additional%2520model%2520training%2520or%2520fine-tuning%252C%2520and%250Athe%2520optimized%2520backdoor%2520trigger%2520exhibits%2520superior%2520transferability%252C%2520in-context%250Acoherence%252C%2520and%2520stealthiness.%2520Extensive%2520experiments%2520demonstrate%2520AgentPoison%2527s%250Aeffectiveness%2520in%2520attacking%2520three%2520types%2520of%2520real-world%2520LLM%2520agents%253A%2520RAG-based%250Aautonomous%2520driving%2520agent%252C%2520knowledge-intensive%2520QA%2520agent%252C%2520and%2520healthcare%250AEHRAgent.%2520On%2520each%2520agent%252C%2520AgentPoison%2520achieves%2520an%2520average%2520attack%2520success%2520rate%250Ahigher%2520than%252080%2525%2520with%2520minimal%2520impact%2520on%2520benign%2520performance%2520%2528less%2520than%25201%2525%2529%2520with%2520a%250Apoison%2520rate%2520less%2520than%25200.1%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentPoison%3A%20Red-teaming%20LLM%20Agents%20via%20Poisoning%20Memory%20or%20Knowledge%0A%20%20Bases&entry.906535625=Zhaorun%20Chen%20and%20Zhen%20Xiang%20and%20Chaowei%20Xiao%20and%20Dawn%20Song%20and%20Bo%20Li&entry.1292438233=%20%20LLM%20agents%20have%20demonstrated%20remarkable%20performance%20across%20various%0Aapplications%2C%20primarily%20due%20to%20their%20advanced%20capabilities%20in%20reasoning%2C%0Autilizing%20external%20knowledge%20and%20tools%2C%20calling%20APIs%2C%20and%20executing%20actions%20to%0Ainteract%20with%20environments.%20Current%20agents%20typically%20utilize%20a%20memory%20module%20or%0Aa%20retrieval-augmented%20generation%20%28RAG%29%20mechanism%2C%20retrieving%20past%20knowledge%20and%0Ainstances%20with%20similar%20embeddings%20from%20knowledge%20bases%20to%20inform%20task%20planning%0Aand%20execution.%20However%2C%20the%20reliance%20on%20unverified%20knowledge%20bases%20raises%0Asignificant%20concerns%20about%20their%20safety%20and%20trustworthiness.%20To%20uncover%20such%0Avulnerabilities%2C%20we%20propose%20a%20novel%20red%20teaming%20approach%20AgentPoison%2C%20the%20first%0Abackdoor%20attack%20targeting%20generic%20and%20RAG-based%20LLM%20agents%20by%20poisoning%20their%0Along-term%20memory%20or%20RAG%20knowledge%20base.%20In%20particular%2C%20we%20form%20the%20trigger%0Ageneration%20process%20as%20a%20constrained%20optimization%20to%20optimize%20backdoor%20triggers%0Aby%20mapping%20the%20triggered%20instances%20to%20a%20unique%20embedding%20space%2C%20so%20as%20to%20ensure%0Athat%20whenever%20a%20user%20instruction%20contains%20the%20optimized%20backdoor%20trigger%2C%20the%0Amalicious%20demonstrations%20are%20retrieved%20from%20the%20poisoned%20memory%20or%20knowledge%0Abase%20with%20high%20probability.%20In%20the%20meantime%2C%20benign%20instructions%20without%20the%0Atrigger%20will%20still%20maintain%20normal%20performance.%20Unlike%20conventional%20backdoor%0Aattacks%2C%20AgentPoison%20requires%20no%20additional%20model%20training%20or%20fine-tuning%2C%20and%0Athe%20optimized%20backdoor%20trigger%20exhibits%20superior%20transferability%2C%20in-context%0Acoherence%2C%20and%20stealthiness.%20Extensive%20experiments%20demonstrate%20AgentPoison%27s%0Aeffectiveness%20in%20attacking%20three%20types%20of%20real-world%20LLM%20agents%3A%20RAG-based%0Aautonomous%20driving%20agent%2C%20knowledge-intensive%20QA%20agent%2C%20and%20healthcare%0AEHRAgent.%20On%20each%20agent%2C%20AgentPoison%20achieves%20an%20average%20attack%20success%20rate%0Ahigher%20than%2080%25%20with%20minimal%20impact%20on%20benign%20performance%20%28less%20than%201%25%29%20with%20a%0Apoison%20rate%20less%20than%200.1%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12784v1&entry.124074799=Read"},
{"title": "High Frequency Matters: Uncertainty Guided Image Compression with\n  Wavelet Diffusion", "author": "Juan Song and Jiaxiang He and Mingtao Feng and Keyan Wang and Yunsong Li and Ajmal Mian", "abstract": "  Diffusion probabilistic models have recently achieved remarkable success in\ngenerating high-quality images. However, balancing high perceptual quality and\nlow distortion remains challenging in image compression applications. To\naddress this issue, we propose an efficient Uncertainty-Guided image\ncompression approach with wavelet Diffusion (UGDiff). Our approach focuses on\nhigh frequency compression via the wavelet transform, since high frequency\ncomponents are crucial for reconstructing image details. We introduce a wavelet\nconditional diffusion model for high frequency prediction, followed by a\nresidual codec that compresses and transmits prediction residuals to the\ndecoder. This diffusion prediction-then-residual compression paradigm\neffectively addresses the low fidelity issue common in direct reconstructions\nby existing diffusion models. Considering the uncertainty from the random\nsampling of the diffusion model, we further design an uncertainty-weighted\nrate-distortion (R-D) loss tailored for residual compression, providing a more\nrational trade-off between rate and distortion. Comprehensive experiments on\ntwo benchmark datasets validate the effectiveness of UGDiff, surpassing\nstate-of-the-art image compression methods in R-D performance, perceptual\nquality, subjective quality, and inference time. Our code is available at:\nhttps://github.com/hejiaxiang1/Wavelet-Diffusion/tree/main\n", "link": "http://arxiv.org/abs/2407.12538v1", "date": "2024-07-17", "relevancy": 1.8221, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6328}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6068}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High%20Frequency%20Matters%3A%20Uncertainty%20Guided%20Image%20Compression%20with%0A%20%20Wavelet%20Diffusion&body=Title%3A%20High%20Frequency%20Matters%3A%20Uncertainty%20Guided%20Image%20Compression%20with%0A%20%20Wavelet%20Diffusion%0AAuthor%3A%20Juan%20Song%20and%20Jiaxiang%20He%20and%20Mingtao%20Feng%20and%20Keyan%20Wang%20and%20Yunsong%20Li%20and%20Ajmal%20Mian%0AAbstract%3A%20%20%20Diffusion%20probabilistic%20models%20have%20recently%20achieved%20remarkable%20success%20in%0Agenerating%20high-quality%20images.%20However%2C%20balancing%20high%20perceptual%20quality%20and%0Alow%20distortion%20remains%20challenging%20in%20image%20compression%20applications.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20an%20efficient%20Uncertainty-Guided%20image%0Acompression%20approach%20with%20wavelet%20Diffusion%20%28UGDiff%29.%20Our%20approach%20focuses%20on%0Ahigh%20frequency%20compression%20via%20the%20wavelet%20transform%2C%20since%20high%20frequency%0Acomponents%20are%20crucial%20for%20reconstructing%20image%20details.%20We%20introduce%20a%20wavelet%0Aconditional%20diffusion%20model%20for%20high%20frequency%20prediction%2C%20followed%20by%20a%0Aresidual%20codec%20that%20compresses%20and%20transmits%20prediction%20residuals%20to%20the%0Adecoder.%20This%20diffusion%20prediction-then-residual%20compression%20paradigm%0Aeffectively%20addresses%20the%20low%20fidelity%20issue%20common%20in%20direct%20reconstructions%0Aby%20existing%20diffusion%20models.%20Considering%20the%20uncertainty%20from%20the%20random%0Asampling%20of%20the%20diffusion%20model%2C%20we%20further%20design%20an%20uncertainty-weighted%0Arate-distortion%20%28R-D%29%20loss%20tailored%20for%20residual%20compression%2C%20providing%20a%20more%0Arational%20trade-off%20between%20rate%20and%20distortion.%20Comprehensive%20experiments%20on%0Atwo%20benchmark%20datasets%20validate%20the%20effectiveness%20of%20UGDiff%2C%20surpassing%0Astate-of-the-art%20image%20compression%20methods%20in%20R-D%20performance%2C%20perceptual%0Aquality%2C%20subjective%20quality%2C%20and%20inference%20time.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/hejiaxiang1/Wavelet-Diffusion/tree/main%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh%2520Frequency%2520Matters%253A%2520Uncertainty%2520Guided%2520Image%2520Compression%2520with%250A%2520%2520Wavelet%2520Diffusion%26entry.906535625%3DJuan%2520Song%2520and%2520Jiaxiang%2520He%2520and%2520Mingtao%2520Feng%2520and%2520Keyan%2520Wang%2520and%2520Yunsong%2520Li%2520and%2520Ajmal%2520Mian%26entry.1292438233%3D%2520%2520Diffusion%2520probabilistic%2520models%2520have%2520recently%2520achieved%2520remarkable%2520success%2520in%250Agenerating%2520high-quality%2520images.%2520However%252C%2520balancing%2520high%2520perceptual%2520quality%2520and%250Alow%2520distortion%2520remains%2520challenging%2520in%2520image%2520compression%2520applications.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520an%2520efficient%2520Uncertainty-Guided%2520image%250Acompression%2520approach%2520with%2520wavelet%2520Diffusion%2520%2528UGDiff%2529.%2520Our%2520approach%2520focuses%2520on%250Ahigh%2520frequency%2520compression%2520via%2520the%2520wavelet%2520transform%252C%2520since%2520high%2520frequency%250Acomponents%2520are%2520crucial%2520for%2520reconstructing%2520image%2520details.%2520We%2520introduce%2520a%2520wavelet%250Aconditional%2520diffusion%2520model%2520for%2520high%2520frequency%2520prediction%252C%2520followed%2520by%2520a%250Aresidual%2520codec%2520that%2520compresses%2520and%2520transmits%2520prediction%2520residuals%2520to%2520the%250Adecoder.%2520This%2520diffusion%2520prediction-then-residual%2520compression%2520paradigm%250Aeffectively%2520addresses%2520the%2520low%2520fidelity%2520issue%2520common%2520in%2520direct%2520reconstructions%250Aby%2520existing%2520diffusion%2520models.%2520Considering%2520the%2520uncertainty%2520from%2520the%2520random%250Asampling%2520of%2520the%2520diffusion%2520model%252C%2520we%2520further%2520design%2520an%2520uncertainty-weighted%250Arate-distortion%2520%2528R-D%2529%2520loss%2520tailored%2520for%2520residual%2520compression%252C%2520providing%2520a%2520more%250Arational%2520trade-off%2520between%2520rate%2520and%2520distortion.%2520Comprehensive%2520experiments%2520on%250Atwo%2520benchmark%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520UGDiff%252C%2520surpassing%250Astate-of-the-art%2520image%2520compression%2520methods%2520in%2520R-D%2520performance%252C%2520perceptual%250Aquality%252C%2520subjective%2520quality%252C%2520and%2520inference%2520time.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/hejiaxiang1/Wavelet-Diffusion/tree/main%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High%20Frequency%20Matters%3A%20Uncertainty%20Guided%20Image%20Compression%20with%0A%20%20Wavelet%20Diffusion&entry.906535625=Juan%20Song%20and%20Jiaxiang%20He%20and%20Mingtao%20Feng%20and%20Keyan%20Wang%20and%20Yunsong%20Li%20and%20Ajmal%20Mian&entry.1292438233=%20%20Diffusion%20probabilistic%20models%20have%20recently%20achieved%20remarkable%20success%20in%0Agenerating%20high-quality%20images.%20However%2C%20balancing%20high%20perceptual%20quality%20and%0Alow%20distortion%20remains%20challenging%20in%20image%20compression%20applications.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20an%20efficient%20Uncertainty-Guided%20image%0Acompression%20approach%20with%20wavelet%20Diffusion%20%28UGDiff%29.%20Our%20approach%20focuses%20on%0Ahigh%20frequency%20compression%20via%20the%20wavelet%20transform%2C%20since%20high%20frequency%0Acomponents%20are%20crucial%20for%20reconstructing%20image%20details.%20We%20introduce%20a%20wavelet%0Aconditional%20diffusion%20model%20for%20high%20frequency%20prediction%2C%20followed%20by%20a%0Aresidual%20codec%20that%20compresses%20and%20transmits%20prediction%20residuals%20to%20the%0Adecoder.%20This%20diffusion%20prediction-then-residual%20compression%20paradigm%0Aeffectively%20addresses%20the%20low%20fidelity%20issue%20common%20in%20direct%20reconstructions%0Aby%20existing%20diffusion%20models.%20Considering%20the%20uncertainty%20from%20the%20random%0Asampling%20of%20the%20diffusion%20model%2C%20we%20further%20design%20an%20uncertainty-weighted%0Arate-distortion%20%28R-D%29%20loss%20tailored%20for%20residual%20compression%2C%20providing%20a%20more%0Arational%20trade-off%20between%20rate%20and%20distortion.%20Comprehensive%20experiments%20on%0Atwo%20benchmark%20datasets%20validate%20the%20effectiveness%20of%20UGDiff%2C%20surpassing%0Astate-of-the-art%20image%20compression%20methods%20in%20R-D%20performance%2C%20perceptual%0Aquality%2C%20subjective%20quality%2C%20and%20inference%20time.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/hejiaxiang1/Wavelet-Diffusion/tree/main%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12538v1&entry.124074799=Read"},
{"title": "Scalable Monte Carlo for Bayesian Learning", "author": "Paul Fearnhead and Christopher Nemeth and Chris J. Oates and Chris Sherlock", "abstract": "  This book aims to provide a graduate-level introduction to advanced topics in\nMarkov chain Monte Carlo (MCMC) algorithms, as applied broadly in the Bayesian\ncomputational context. Most, if not all of these topics (stochastic gradient\nMCMC, non-reversible MCMC, continuous time MCMC, and new techniques for\nconvergence assessment) have emerged as recently as the last decade, and have\ndriven substantial recent practical and theoretical advances in the field. A\nparticular focus is on methods that are scalable with respect to either the\namount of data, or the data dimension, motivated by the emerging high-priority\napplication areas in machine learning and AI.\n", "link": "http://arxiv.org/abs/2407.12751v1", "date": "2024-07-17", "relevancy": 1.8108, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4855}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4543}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Monte%20Carlo%20for%20Bayesian%20Learning&body=Title%3A%20Scalable%20Monte%20Carlo%20for%20Bayesian%20Learning%0AAuthor%3A%20Paul%20Fearnhead%20and%20Christopher%20Nemeth%20and%20Chris%20J.%20Oates%20and%20Chris%20Sherlock%0AAbstract%3A%20%20%20This%20book%20aims%20to%20provide%20a%20graduate-level%20introduction%20to%20advanced%20topics%20in%0AMarkov%20chain%20Monte%20Carlo%20%28MCMC%29%20algorithms%2C%20as%20applied%20broadly%20in%20the%20Bayesian%0Acomputational%20context.%20Most%2C%20if%20not%20all%20of%20these%20topics%20%28stochastic%20gradient%0AMCMC%2C%20non-reversible%20MCMC%2C%20continuous%20time%20MCMC%2C%20and%20new%20techniques%20for%0Aconvergence%20assessment%29%20have%20emerged%20as%20recently%20as%20the%20last%20decade%2C%20and%20have%0Adriven%20substantial%20recent%20practical%20and%20theoretical%20advances%20in%20the%20field.%20A%0Aparticular%20focus%20is%20on%20methods%20that%20are%20scalable%20with%20respect%20to%20either%20the%0Aamount%20of%20data%2C%20or%20the%20data%20dimension%2C%20motivated%20by%20the%20emerging%20high-priority%0Aapplication%20areas%20in%20machine%20learning%20and%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Monte%2520Carlo%2520for%2520Bayesian%2520Learning%26entry.906535625%3DPaul%2520Fearnhead%2520and%2520Christopher%2520Nemeth%2520and%2520Chris%2520J.%2520Oates%2520and%2520Chris%2520Sherlock%26entry.1292438233%3D%2520%2520This%2520book%2520aims%2520to%2520provide%2520a%2520graduate-level%2520introduction%2520to%2520advanced%2520topics%2520in%250AMarkov%2520chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520algorithms%252C%2520as%2520applied%2520broadly%2520in%2520the%2520Bayesian%250Acomputational%2520context.%2520Most%252C%2520if%2520not%2520all%2520of%2520these%2520topics%2520%2528stochastic%2520gradient%250AMCMC%252C%2520non-reversible%2520MCMC%252C%2520continuous%2520time%2520MCMC%252C%2520and%2520new%2520techniques%2520for%250Aconvergence%2520assessment%2529%2520have%2520emerged%2520as%2520recently%2520as%2520the%2520last%2520decade%252C%2520and%2520have%250Adriven%2520substantial%2520recent%2520practical%2520and%2520theoretical%2520advances%2520in%2520the%2520field.%2520A%250Aparticular%2520focus%2520is%2520on%2520methods%2520that%2520are%2520scalable%2520with%2520respect%2520to%2520either%2520the%250Aamount%2520of%2520data%252C%2520or%2520the%2520data%2520dimension%252C%2520motivated%2520by%2520the%2520emerging%2520high-priority%250Aapplication%2520areas%2520in%2520machine%2520learning%2520and%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Monte%20Carlo%20for%20Bayesian%20Learning&entry.906535625=Paul%20Fearnhead%20and%20Christopher%20Nemeth%20and%20Chris%20J.%20Oates%20and%20Chris%20Sherlock&entry.1292438233=%20%20This%20book%20aims%20to%20provide%20a%20graduate-level%20introduction%20to%20advanced%20topics%20in%0AMarkov%20chain%20Monte%20Carlo%20%28MCMC%29%20algorithms%2C%20as%20applied%20broadly%20in%20the%20Bayesian%0Acomputational%20context.%20Most%2C%20if%20not%20all%20of%20these%20topics%20%28stochastic%20gradient%0AMCMC%2C%20non-reversible%20MCMC%2C%20continuous%20time%20MCMC%2C%20and%20new%20techniques%20for%0Aconvergence%20assessment%29%20have%20emerged%20as%20recently%20as%20the%20last%20decade%2C%20and%20have%0Adriven%20substantial%20recent%20practical%20and%20theoretical%20advances%20in%20the%20field.%20A%0Aparticular%20focus%20is%20on%20methods%20that%20are%20scalable%20with%20respect%20to%20either%20the%0Aamount%20of%20data%2C%20or%20the%20data%20dimension%2C%20motivated%20by%20the%20emerging%20high-priority%0Aapplication%20areas%20in%20machine%20learning%20and%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12751v1&entry.124074799=Read"},
{"title": "Comparing Federated Stochastic Gradient Descent and Federated Averaging\n  for Predicting Hospital Length of Stay", "author": "Mehmet Yigit Balik", "abstract": "  Predicting hospital length of stay (LOS) reliably is an essential need for\nefficient resource allocation at hospitals. Traditional predictive modeling\ntools frequently have difficulty acquiring sufficient and diverse data because\nhealthcare institutions have privacy rules in place. In our study, we modeled\nthis problem as an empirical graph where nodes are the hospitals. This modeling\napproach facilitates collaborative model training by modeling decentralized\ndata sources from different hospitals without extracting sensitive data outside\nof hospitals. A local model is trained on a node (hospital) by aiming the\ngeneralized total variation minimization (GTVMin). Moreover, we implemented and\ncompared two different federated learning optimization algorithms named\nfederated stochastic gradient descent (FedSGD) and federated averaging\n(FedAVG). Our results show that federated learning enables accurate prediction\nof hospital LOS while addressing privacy concerns without extracting data\noutside healthcare institutions.\n", "link": "http://arxiv.org/abs/2407.12741v1", "date": "2024-07-17", "relevancy": 1.8091, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4569}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4503}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Federated%20Stochastic%20Gradient%20Descent%20and%20Federated%20Averaging%0A%20%20for%20Predicting%20Hospital%20Length%20of%20Stay&body=Title%3A%20Comparing%20Federated%20Stochastic%20Gradient%20Descent%20and%20Federated%20Averaging%0A%20%20for%20Predicting%20Hospital%20Length%20of%20Stay%0AAuthor%3A%20Mehmet%20Yigit%20Balik%0AAbstract%3A%20%20%20Predicting%20hospital%20length%20of%20stay%20%28LOS%29%20reliably%20is%20an%20essential%20need%20for%0Aefficient%20resource%20allocation%20at%20hospitals.%20Traditional%20predictive%20modeling%0Atools%20frequently%20have%20difficulty%20acquiring%20sufficient%20and%20diverse%20data%20because%0Ahealthcare%20institutions%20have%20privacy%20rules%20in%20place.%20In%20our%20study%2C%20we%20modeled%0Athis%20problem%20as%20an%20empirical%20graph%20where%20nodes%20are%20the%20hospitals.%20This%20modeling%0Aapproach%20facilitates%20collaborative%20model%20training%20by%20modeling%20decentralized%0Adata%20sources%20from%20different%20hospitals%20without%20extracting%20sensitive%20data%20outside%0Aof%20hospitals.%20A%20local%20model%20is%20trained%20on%20a%20node%20%28hospital%29%20by%20aiming%20the%0Ageneralized%20total%20variation%20minimization%20%28GTVMin%29.%20Moreover%2C%20we%20implemented%20and%0Acompared%20two%20different%20federated%20learning%20optimization%20algorithms%20named%0Afederated%20stochastic%20gradient%20descent%20%28FedSGD%29%20and%20federated%20averaging%0A%28FedAVG%29.%20Our%20results%20show%20that%20federated%20learning%20enables%20accurate%20prediction%0Aof%20hospital%20LOS%20while%20addressing%20privacy%20concerns%20without%20extracting%20data%0Aoutside%20healthcare%20institutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Federated%2520Stochastic%2520Gradient%2520Descent%2520and%2520Federated%2520Averaging%250A%2520%2520for%2520Predicting%2520Hospital%2520Length%2520of%2520Stay%26entry.906535625%3DMehmet%2520Yigit%2520Balik%26entry.1292438233%3D%2520%2520Predicting%2520hospital%2520length%2520of%2520stay%2520%2528LOS%2529%2520reliably%2520is%2520an%2520essential%2520need%2520for%250Aefficient%2520resource%2520allocation%2520at%2520hospitals.%2520Traditional%2520predictive%2520modeling%250Atools%2520frequently%2520have%2520difficulty%2520acquiring%2520sufficient%2520and%2520diverse%2520data%2520because%250Ahealthcare%2520institutions%2520have%2520privacy%2520rules%2520in%2520place.%2520In%2520our%2520study%252C%2520we%2520modeled%250Athis%2520problem%2520as%2520an%2520empirical%2520graph%2520where%2520nodes%2520are%2520the%2520hospitals.%2520This%2520modeling%250Aapproach%2520facilitates%2520collaborative%2520model%2520training%2520by%2520modeling%2520decentralized%250Adata%2520sources%2520from%2520different%2520hospitals%2520without%2520extracting%2520sensitive%2520data%2520outside%250Aof%2520hospitals.%2520A%2520local%2520model%2520is%2520trained%2520on%2520a%2520node%2520%2528hospital%2529%2520by%2520aiming%2520the%250Ageneralized%2520total%2520variation%2520minimization%2520%2528GTVMin%2529.%2520Moreover%252C%2520we%2520implemented%2520and%250Acompared%2520two%2520different%2520federated%2520learning%2520optimization%2520algorithms%2520named%250Afederated%2520stochastic%2520gradient%2520descent%2520%2528FedSGD%2529%2520and%2520federated%2520averaging%250A%2528FedAVG%2529.%2520Our%2520results%2520show%2520that%2520federated%2520learning%2520enables%2520accurate%2520prediction%250Aof%2520hospital%2520LOS%2520while%2520addressing%2520privacy%2520concerns%2520without%2520extracting%2520data%250Aoutside%2520healthcare%2520institutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Federated%20Stochastic%20Gradient%20Descent%20and%20Federated%20Averaging%0A%20%20for%20Predicting%20Hospital%20Length%20of%20Stay&entry.906535625=Mehmet%20Yigit%20Balik&entry.1292438233=%20%20Predicting%20hospital%20length%20of%20stay%20%28LOS%29%20reliably%20is%20an%20essential%20need%20for%0Aefficient%20resource%20allocation%20at%20hospitals.%20Traditional%20predictive%20modeling%0Atools%20frequently%20have%20difficulty%20acquiring%20sufficient%20and%20diverse%20data%20because%0Ahealthcare%20institutions%20have%20privacy%20rules%20in%20place.%20In%20our%20study%2C%20we%20modeled%0Athis%20problem%20as%20an%20empirical%20graph%20where%20nodes%20are%20the%20hospitals.%20This%20modeling%0Aapproach%20facilitates%20collaborative%20model%20training%20by%20modeling%20decentralized%0Adata%20sources%20from%20different%20hospitals%20without%20extracting%20sensitive%20data%20outside%0Aof%20hospitals.%20A%20local%20model%20is%20trained%20on%20a%20node%20%28hospital%29%20by%20aiming%20the%0Ageneralized%20total%20variation%20minimization%20%28GTVMin%29.%20Moreover%2C%20we%20implemented%20and%0Acompared%20two%20different%20federated%20learning%20optimization%20algorithms%20named%0Afederated%20stochastic%20gradient%20descent%20%28FedSGD%29%20and%20federated%20averaging%0A%28FedAVG%29.%20Our%20results%20show%20that%20federated%20learning%20enables%20accurate%20prediction%0Aof%20hospital%20LOS%20while%20addressing%20privacy%20concerns%20without%20extracting%20data%0Aoutside%20healthcare%20institutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12741v1&entry.124074799=Read"},
{"title": "On the Effect of (Near) Duplicate Subwords in Language Modelling", "author": "Anton Sch\u00e4fer and Thomas Hofmann and Imanol Schlag and Tiago Pimentel", "abstract": "  Tokenisation is a core part of language models (LMs). It involves splitting a\ncharacter sequence into subwords which are assigned arbitrary indices before\nbeing served to the LM. While typically lossless, however, this process may\nlead to less sample efficient LM training: as it removes character-level\ninformation, it could make it harder for LMs to generalise across similar\nsubwords, such as now and Now. We refer to such subwords as near duplicates. In\nthis paper, we study the impact of near duplicate subwords on LM training\nefficiency. First, we design an experiment that gives us an upper bound to how\nmuch we should expect a model to improve if we could perfectly generalise\nacross near duplicates. We do this by duplicating each subword in our LM's\nvocabulary, creating perfectly equivalent classes of subwords. Experimentally,\nwe find that LMs need roughly 17% more data when trained in a fully duplicated\nsetting. Second, we investigate the impact of naturally occurring near\nduplicates on LMs. Here, we see that merging them considerably hurts LM\nperformance. Therefore, although subword duplication negatively impacts LM\ntraining efficiency, naturally occurring near duplicates may not be as similar\nas anticipated, limiting the potential for performance improvements.\n", "link": "http://arxiv.org/abs/2404.06508v3", "date": "2024-07-17", "relevancy": 1.8072, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4349}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Effect%20of%20%28Near%29%20Duplicate%20Subwords%20in%20Language%20Modelling&body=Title%3A%20On%20the%20Effect%20of%20%28Near%29%20Duplicate%20Subwords%20in%20Language%20Modelling%0AAuthor%3A%20Anton%20Sch%C3%A4fer%20and%20Thomas%20Hofmann%20and%20Imanol%20Schlag%20and%20Tiago%20Pimentel%0AAbstract%3A%20%20%20Tokenisation%20is%20a%20core%20part%20of%20language%20models%20%28LMs%29.%20It%20involves%20splitting%20a%0Acharacter%20sequence%20into%20subwords%20which%20are%20assigned%20arbitrary%20indices%20before%0Abeing%20served%20to%20the%20LM.%20While%20typically%20lossless%2C%20however%2C%20this%20process%20may%0Alead%20to%20less%20sample%20efficient%20LM%20training%3A%20as%20it%20removes%20character-level%0Ainformation%2C%20it%20could%20make%20it%20harder%20for%20LMs%20to%20generalise%20across%20similar%0Asubwords%2C%20such%20as%20now%20and%20Now.%20We%20refer%20to%20such%20subwords%20as%20near%20duplicates.%20In%0Athis%20paper%2C%20we%20study%20the%20impact%20of%20near%20duplicate%20subwords%20on%20LM%20training%0Aefficiency.%20First%2C%20we%20design%20an%20experiment%20that%20gives%20us%20an%20upper%20bound%20to%20how%0Amuch%20we%20should%20expect%20a%20model%20to%20improve%20if%20we%20could%20perfectly%20generalise%0Aacross%20near%20duplicates.%20We%20do%20this%20by%20duplicating%20each%20subword%20in%20our%20LM%27s%0Avocabulary%2C%20creating%20perfectly%20equivalent%20classes%20of%20subwords.%20Experimentally%2C%0Awe%20find%20that%20LMs%20need%20roughly%2017%25%20more%20data%20when%20trained%20in%20a%20fully%20duplicated%0Asetting.%20Second%2C%20we%20investigate%20the%20impact%20of%20naturally%20occurring%20near%0Aduplicates%20on%20LMs.%20Here%2C%20we%20see%20that%20merging%20them%20considerably%20hurts%20LM%0Aperformance.%20Therefore%2C%20although%20subword%20duplication%20negatively%20impacts%20LM%0Atraining%20efficiency%2C%20naturally%20occurring%20near%20duplicates%20may%20not%20be%20as%20similar%0Aas%20anticipated%2C%20limiting%20the%20potential%20for%20performance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06508v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Effect%2520of%2520%2528Near%2529%2520Duplicate%2520Subwords%2520in%2520Language%2520Modelling%26entry.906535625%3DAnton%2520Sch%25C3%25A4fer%2520and%2520Thomas%2520Hofmann%2520and%2520Imanol%2520Schlag%2520and%2520Tiago%2520Pimentel%26entry.1292438233%3D%2520%2520Tokenisation%2520is%2520a%2520core%2520part%2520of%2520language%2520models%2520%2528LMs%2529.%2520It%2520involves%2520splitting%2520a%250Acharacter%2520sequence%2520into%2520subwords%2520which%2520are%2520assigned%2520arbitrary%2520indices%2520before%250Abeing%2520served%2520to%2520the%2520LM.%2520While%2520typically%2520lossless%252C%2520however%252C%2520this%2520process%2520may%250Alead%2520to%2520less%2520sample%2520efficient%2520LM%2520training%253A%2520as%2520it%2520removes%2520character-level%250Ainformation%252C%2520it%2520could%2520make%2520it%2520harder%2520for%2520LMs%2520to%2520generalise%2520across%2520similar%250Asubwords%252C%2520such%2520as%2520now%2520and%2520Now.%2520We%2520refer%2520to%2520such%2520subwords%2520as%2520near%2520duplicates.%2520In%250Athis%2520paper%252C%2520we%2520study%2520the%2520impact%2520of%2520near%2520duplicate%2520subwords%2520on%2520LM%2520training%250Aefficiency.%2520First%252C%2520we%2520design%2520an%2520experiment%2520that%2520gives%2520us%2520an%2520upper%2520bound%2520to%2520how%250Amuch%2520we%2520should%2520expect%2520a%2520model%2520to%2520improve%2520if%2520we%2520could%2520perfectly%2520generalise%250Aacross%2520near%2520duplicates.%2520We%2520do%2520this%2520by%2520duplicating%2520each%2520subword%2520in%2520our%2520LM%2527s%250Avocabulary%252C%2520creating%2520perfectly%2520equivalent%2520classes%2520of%2520subwords.%2520Experimentally%252C%250Awe%2520find%2520that%2520LMs%2520need%2520roughly%252017%2525%2520more%2520data%2520when%2520trained%2520in%2520a%2520fully%2520duplicated%250Asetting.%2520Second%252C%2520we%2520investigate%2520the%2520impact%2520of%2520naturally%2520occurring%2520near%250Aduplicates%2520on%2520LMs.%2520Here%252C%2520we%2520see%2520that%2520merging%2520them%2520considerably%2520hurts%2520LM%250Aperformance.%2520Therefore%252C%2520although%2520subword%2520duplication%2520negatively%2520impacts%2520LM%250Atraining%2520efficiency%252C%2520naturally%2520occurring%2520near%2520duplicates%2520may%2520not%2520be%2520as%2520similar%250Aas%2520anticipated%252C%2520limiting%2520the%2520potential%2520for%2520performance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06508v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effect%20of%20%28Near%29%20Duplicate%20Subwords%20in%20Language%20Modelling&entry.906535625=Anton%20Sch%C3%A4fer%20and%20Thomas%20Hofmann%20and%20Imanol%20Schlag%20and%20Tiago%20Pimentel&entry.1292438233=%20%20Tokenisation%20is%20a%20core%20part%20of%20language%20models%20%28LMs%29.%20It%20involves%20splitting%20a%0Acharacter%20sequence%20into%20subwords%20which%20are%20assigned%20arbitrary%20indices%20before%0Abeing%20served%20to%20the%20LM.%20While%20typically%20lossless%2C%20however%2C%20this%20process%20may%0Alead%20to%20less%20sample%20efficient%20LM%20training%3A%20as%20it%20removes%20character-level%0Ainformation%2C%20it%20could%20make%20it%20harder%20for%20LMs%20to%20generalise%20across%20similar%0Asubwords%2C%20such%20as%20now%20and%20Now.%20We%20refer%20to%20such%20subwords%20as%20near%20duplicates.%20In%0Athis%20paper%2C%20we%20study%20the%20impact%20of%20near%20duplicate%20subwords%20on%20LM%20training%0Aefficiency.%20First%2C%20we%20design%20an%20experiment%20that%20gives%20us%20an%20upper%20bound%20to%20how%0Amuch%20we%20should%20expect%20a%20model%20to%20improve%20if%20we%20could%20perfectly%20generalise%0Aacross%20near%20duplicates.%20We%20do%20this%20by%20duplicating%20each%20subword%20in%20our%20LM%27s%0Avocabulary%2C%20creating%20perfectly%20equivalent%20classes%20of%20subwords.%20Experimentally%2C%0Awe%20find%20that%20LMs%20need%20roughly%2017%25%20more%20data%20when%20trained%20in%20a%20fully%20duplicated%0Asetting.%20Second%2C%20we%20investigate%20the%20impact%20of%20naturally%20occurring%20near%0Aduplicates%20on%20LMs.%20Here%2C%20we%20see%20that%20merging%20them%20considerably%20hurts%20LM%0Aperformance.%20Therefore%2C%20although%20subword%20duplication%20negatively%20impacts%20LM%0Atraining%20efficiency%2C%20naturally%20occurring%20near%20duplicates%20may%20not%20be%20as%20similar%0Aas%20anticipated%2C%20limiting%20the%20potential%20for%20performance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06508v3&entry.124074799=Read"},
{"title": "Strawberry detection and counting based on YOLOv7 pruning and\n  information based tracking algorithm", "author": "Shiyu Liu and Congliang Zhou and Won Suk Lee", "abstract": "  The strawberry industry yields significant economic benefits for Florida, yet\nthe process of monitoring strawberry growth and yield is labor-intensive and\ncostly. The development of machine learning-based detection and tracking\nmethodologies has been used for helping automated monitoring and prediction of\nstrawberry yield, still, enhancement has been limited as previous studies only\napplied the deep learning method for flower and fruit detection, which did not\nconsider the unique characteristics of image datasets collected by the machine\nvision system. This study proposed an optimal pruning of detection heads of the\ndeep learning model (YOLOv7 and its variants) that could achieve fast and\nprecise strawberry flower, immature fruit, and mature fruit detection.\nThereafter, an enhanced object tracking algorithm, which is called the\nInformation Based Tracking Algorithm (IBTA) utilized the best detection result,\nremoved the Kalman Filter, and integrated moving direction, velocity, and\nspatial information to improve the precision in strawberry flower and fruit\ntracking. The proposed pruning of detection heads across YOLOv7 variants,\nnotably Pruning-YOLOv7-tiny with detection head 3 and Pruning-YOLOv7-tiny with\nheads 2 and 3 achieved the best inference speed (163.9 frames per second) and\ndetection accuracy (89.1%), respectively. On the other hand, the effect of IBTA\nwas proved by comparing it with the centroid tracking algorithm (CTA), the\nMultiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision\n(MOTP) of IBTA were 12.3% and 6.0% higher than that of CTA, accordingly. In\naddition, other object-tracking evaluation metrics, including IDF1, IDR, IDP,\nMT, and IDs, show that IBTA performed better than CTA in strawberry flower and\nfruit tracking.\n", "link": "http://arxiv.org/abs/2407.12614v1", "date": "2024-07-17", "relevancy": 1.7915, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4545}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strawberry%20detection%20and%20counting%20based%20on%20YOLOv7%20pruning%20and%0A%20%20information%20based%20tracking%20algorithm&body=Title%3A%20Strawberry%20detection%20and%20counting%20based%20on%20YOLOv7%20pruning%20and%0A%20%20information%20based%20tracking%20algorithm%0AAuthor%3A%20Shiyu%20Liu%20and%20Congliang%20Zhou%20and%20Won%20Suk%20Lee%0AAbstract%3A%20%20%20The%20strawberry%20industry%20yields%20significant%20economic%20benefits%20for%20Florida%2C%20yet%0Athe%20process%20of%20monitoring%20strawberry%20growth%20and%20yield%20is%20labor-intensive%20and%0Acostly.%20The%20development%20of%20machine%20learning-based%20detection%20and%20tracking%0Amethodologies%20has%20been%20used%20for%20helping%20automated%20monitoring%20and%20prediction%20of%0Astrawberry%20yield%2C%20still%2C%20enhancement%20has%20been%20limited%20as%20previous%20studies%20only%0Aapplied%20the%20deep%20learning%20method%20for%20flower%20and%20fruit%20detection%2C%20which%20did%20not%0Aconsider%20the%20unique%20characteristics%20of%20image%20datasets%20collected%20by%20the%20machine%0Avision%20system.%20This%20study%20proposed%20an%20optimal%20pruning%20of%20detection%20heads%20of%20the%0Adeep%20learning%20model%20%28YOLOv7%20and%20its%20variants%29%20that%20could%20achieve%20fast%20and%0Aprecise%20strawberry%20flower%2C%20immature%20fruit%2C%20and%20mature%20fruit%20detection.%0AThereafter%2C%20an%20enhanced%20object%20tracking%20algorithm%2C%20which%20is%20called%20the%0AInformation%20Based%20Tracking%20Algorithm%20%28IBTA%29%20utilized%20the%20best%20detection%20result%2C%0Aremoved%20the%20Kalman%20Filter%2C%20and%20integrated%20moving%20direction%2C%20velocity%2C%20and%0Aspatial%20information%20to%20improve%20the%20precision%20in%20strawberry%20flower%20and%20fruit%0Atracking.%20The%20proposed%20pruning%20of%20detection%20heads%20across%20YOLOv7%20variants%2C%0Anotably%20Pruning-YOLOv7-tiny%20with%20detection%20head%203%20and%20Pruning-YOLOv7-tiny%20with%0Aheads%202%20and%203%20achieved%20the%20best%20inference%20speed%20%28163.9%20frames%20per%20second%29%20and%0Adetection%20accuracy%20%2889.1%25%29%2C%20respectively.%20On%20the%20other%20hand%2C%20the%20effect%20of%20IBTA%0Awas%20proved%20by%20comparing%20it%20with%20the%20centroid%20tracking%20algorithm%20%28CTA%29%2C%20the%0AMultiple%20Object%20Tracking%20Accuracy%20%28MOTA%29%20and%20Multiple%20Object%20Tracking%20Precision%0A%28MOTP%29%20of%20IBTA%20were%2012.3%25%20and%206.0%25%20higher%20than%20that%20of%20CTA%2C%20accordingly.%20In%0Aaddition%2C%20other%20object-tracking%20evaluation%20metrics%2C%20including%20IDF1%2C%20IDR%2C%20IDP%2C%0AMT%2C%20and%20IDs%2C%20show%20that%20IBTA%20performed%20better%20than%20CTA%20in%20strawberry%20flower%20and%0Afruit%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrawberry%2520detection%2520and%2520counting%2520based%2520on%2520YOLOv7%2520pruning%2520and%250A%2520%2520information%2520based%2520tracking%2520algorithm%26entry.906535625%3DShiyu%2520Liu%2520and%2520Congliang%2520Zhou%2520and%2520Won%2520Suk%2520Lee%26entry.1292438233%3D%2520%2520The%2520strawberry%2520industry%2520yields%2520significant%2520economic%2520benefits%2520for%2520Florida%252C%2520yet%250Athe%2520process%2520of%2520monitoring%2520strawberry%2520growth%2520and%2520yield%2520is%2520labor-intensive%2520and%250Acostly.%2520The%2520development%2520of%2520machine%2520learning-based%2520detection%2520and%2520tracking%250Amethodologies%2520has%2520been%2520used%2520for%2520helping%2520automated%2520monitoring%2520and%2520prediction%2520of%250Astrawberry%2520yield%252C%2520still%252C%2520enhancement%2520has%2520been%2520limited%2520as%2520previous%2520studies%2520only%250Aapplied%2520the%2520deep%2520learning%2520method%2520for%2520flower%2520and%2520fruit%2520detection%252C%2520which%2520did%2520not%250Aconsider%2520the%2520unique%2520characteristics%2520of%2520image%2520datasets%2520collected%2520by%2520the%2520machine%250Avision%2520system.%2520This%2520study%2520proposed%2520an%2520optimal%2520pruning%2520of%2520detection%2520heads%2520of%2520the%250Adeep%2520learning%2520model%2520%2528YOLOv7%2520and%2520its%2520variants%2529%2520that%2520could%2520achieve%2520fast%2520and%250Aprecise%2520strawberry%2520flower%252C%2520immature%2520fruit%252C%2520and%2520mature%2520fruit%2520detection.%250AThereafter%252C%2520an%2520enhanced%2520object%2520tracking%2520algorithm%252C%2520which%2520is%2520called%2520the%250AInformation%2520Based%2520Tracking%2520Algorithm%2520%2528IBTA%2529%2520utilized%2520the%2520best%2520detection%2520result%252C%250Aremoved%2520the%2520Kalman%2520Filter%252C%2520and%2520integrated%2520moving%2520direction%252C%2520velocity%252C%2520and%250Aspatial%2520information%2520to%2520improve%2520the%2520precision%2520in%2520strawberry%2520flower%2520and%2520fruit%250Atracking.%2520The%2520proposed%2520pruning%2520of%2520detection%2520heads%2520across%2520YOLOv7%2520variants%252C%250Anotably%2520Pruning-YOLOv7-tiny%2520with%2520detection%2520head%25203%2520and%2520Pruning-YOLOv7-tiny%2520with%250Aheads%25202%2520and%25203%2520achieved%2520the%2520best%2520inference%2520speed%2520%2528163.9%2520frames%2520per%2520second%2529%2520and%250Adetection%2520accuracy%2520%252889.1%2525%2529%252C%2520respectively.%2520On%2520the%2520other%2520hand%252C%2520the%2520effect%2520of%2520IBTA%250Awas%2520proved%2520by%2520comparing%2520it%2520with%2520the%2520centroid%2520tracking%2520algorithm%2520%2528CTA%2529%252C%2520the%250AMultiple%2520Object%2520Tracking%2520Accuracy%2520%2528MOTA%2529%2520and%2520Multiple%2520Object%2520Tracking%2520Precision%250A%2528MOTP%2529%2520of%2520IBTA%2520were%252012.3%2525%2520and%25206.0%2525%2520higher%2520than%2520that%2520of%2520CTA%252C%2520accordingly.%2520In%250Aaddition%252C%2520other%2520object-tracking%2520evaluation%2520metrics%252C%2520including%2520IDF1%252C%2520IDR%252C%2520IDP%252C%250AMT%252C%2520and%2520IDs%252C%2520show%2520that%2520IBTA%2520performed%2520better%2520than%2520CTA%2520in%2520strawberry%2520flower%2520and%250Afruit%2520tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strawberry%20detection%20and%20counting%20based%20on%20YOLOv7%20pruning%20and%0A%20%20information%20based%20tracking%20algorithm&entry.906535625=Shiyu%20Liu%20and%20Congliang%20Zhou%20and%20Won%20Suk%20Lee&entry.1292438233=%20%20The%20strawberry%20industry%20yields%20significant%20economic%20benefits%20for%20Florida%2C%20yet%0Athe%20process%20of%20monitoring%20strawberry%20growth%20and%20yield%20is%20labor-intensive%20and%0Acostly.%20The%20development%20of%20machine%20learning-based%20detection%20and%20tracking%0Amethodologies%20has%20been%20used%20for%20helping%20automated%20monitoring%20and%20prediction%20of%0Astrawberry%20yield%2C%20still%2C%20enhancement%20has%20been%20limited%20as%20previous%20studies%20only%0Aapplied%20the%20deep%20learning%20method%20for%20flower%20and%20fruit%20detection%2C%20which%20did%20not%0Aconsider%20the%20unique%20characteristics%20of%20image%20datasets%20collected%20by%20the%20machine%0Avision%20system.%20This%20study%20proposed%20an%20optimal%20pruning%20of%20detection%20heads%20of%20the%0Adeep%20learning%20model%20%28YOLOv7%20and%20its%20variants%29%20that%20could%20achieve%20fast%20and%0Aprecise%20strawberry%20flower%2C%20immature%20fruit%2C%20and%20mature%20fruit%20detection.%0AThereafter%2C%20an%20enhanced%20object%20tracking%20algorithm%2C%20which%20is%20called%20the%0AInformation%20Based%20Tracking%20Algorithm%20%28IBTA%29%20utilized%20the%20best%20detection%20result%2C%0Aremoved%20the%20Kalman%20Filter%2C%20and%20integrated%20moving%20direction%2C%20velocity%2C%20and%0Aspatial%20information%20to%20improve%20the%20precision%20in%20strawberry%20flower%20and%20fruit%0Atracking.%20The%20proposed%20pruning%20of%20detection%20heads%20across%20YOLOv7%20variants%2C%0Anotably%20Pruning-YOLOv7-tiny%20with%20detection%20head%203%20and%20Pruning-YOLOv7-tiny%20with%0Aheads%202%20and%203%20achieved%20the%20best%20inference%20speed%20%28163.9%20frames%20per%20second%29%20and%0Adetection%20accuracy%20%2889.1%25%29%2C%20respectively.%20On%20the%20other%20hand%2C%20the%20effect%20of%20IBTA%0Awas%20proved%20by%20comparing%20it%20with%20the%20centroid%20tracking%20algorithm%20%28CTA%29%2C%20the%0AMultiple%20Object%20Tracking%20Accuracy%20%28MOTA%29%20and%20Multiple%20Object%20Tracking%20Precision%0A%28MOTP%29%20of%20IBTA%20were%2012.3%25%20and%206.0%25%20higher%20than%20that%20of%20CTA%2C%20accordingly.%20In%0Aaddition%2C%20other%20object-tracking%20evaluation%20metrics%2C%20including%20IDF1%2C%20IDR%2C%20IDP%2C%0AMT%2C%20and%20IDs%2C%20show%20that%20IBTA%20performed%20better%20than%20CTA%20in%20strawberry%20flower%20and%0Afruit%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12614v1&entry.124074799=Read"},
{"title": "A Methodology Establishing Linear Convergence of Adaptive Gradient\n  Methods under PL Inequality", "author": "Kushal Chakrabarti and Mayank Baranwal", "abstract": "  Adaptive gradient-descent optimizers are the standard choice for training\nneural network models. Despite their faster convergence than gradient-descent\nand remarkable performance in practice, the adaptive optimizers are not as well\nunderstood as vanilla gradient-descent. A reason is that the dynamic update of\nthe learning rate that helps in faster convergence of these methods also makes\ntheir analysis intricate. Particularly, the simple gradient-descent method\nconverges at a linear rate for a class of optimization problems, whereas the\npractically faster adaptive gradient methods lack such a theoretical guarantee.\nThe Polyak-{\\L}ojasiewicz (PL) inequality is the weakest known class, for which\nlinear convergence of gradient-descent and its momentum variants has been\nproved. Therefore, in this paper, we prove that AdaGrad and Adam, two\nwell-known adaptive gradient methods, converge linearly when the cost function\nis smooth and satisfies the PL inequality. Our theoretical framework follows a\nsimple and unified approach, applicable to both batch and stochastic gradients,\nwhich can potentially be utilized in analyzing linear convergence of other\nvariants of Adam.\n", "link": "http://arxiv.org/abs/2407.12629v1", "date": "2024-07-17", "relevancy": 1.7852, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4683}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Methodology%20Establishing%20Linear%20Convergence%20of%20Adaptive%20Gradient%0A%20%20Methods%20under%20PL%20Inequality&body=Title%3A%20A%20Methodology%20Establishing%20Linear%20Convergence%20of%20Adaptive%20Gradient%0A%20%20Methods%20under%20PL%20Inequality%0AAuthor%3A%20Kushal%20Chakrabarti%20and%20Mayank%20Baranwal%0AAbstract%3A%20%20%20Adaptive%20gradient-descent%20optimizers%20are%20the%20standard%20choice%20for%20training%0Aneural%20network%20models.%20Despite%20their%20faster%20convergence%20than%20gradient-descent%0Aand%20remarkable%20performance%20in%20practice%2C%20the%20adaptive%20optimizers%20are%20not%20as%20well%0Aunderstood%20as%20vanilla%20gradient-descent.%20A%20reason%20is%20that%20the%20dynamic%20update%20of%0Athe%20learning%20rate%20that%20helps%20in%20faster%20convergence%20of%20these%20methods%20also%20makes%0Atheir%20analysis%20intricate.%20Particularly%2C%20the%20simple%20gradient-descent%20method%0Aconverges%20at%20a%20linear%20rate%20for%20a%20class%20of%20optimization%20problems%2C%20whereas%20the%0Apractically%20faster%20adaptive%20gradient%20methods%20lack%20such%20a%20theoretical%20guarantee.%0AThe%20Polyak-%7B%5CL%7Dojasiewicz%20%28PL%29%20inequality%20is%20the%20weakest%20known%20class%2C%20for%20which%0Alinear%20convergence%20of%20gradient-descent%20and%20its%20momentum%20variants%20has%20been%0Aproved.%20Therefore%2C%20in%20this%20paper%2C%20we%20prove%20that%20AdaGrad%20and%20Adam%2C%20two%0Awell-known%20adaptive%20gradient%20methods%2C%20converge%20linearly%20when%20the%20cost%20function%0Ais%20smooth%20and%20satisfies%20the%20PL%20inequality.%20Our%20theoretical%20framework%20follows%20a%0Asimple%20and%20unified%20approach%2C%20applicable%20to%20both%20batch%20and%20stochastic%20gradients%2C%0Awhich%20can%20potentially%20be%20utilized%20in%20analyzing%20linear%20convergence%20of%20other%0Avariants%20of%20Adam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Methodology%2520Establishing%2520Linear%2520Convergence%2520of%2520Adaptive%2520Gradient%250A%2520%2520Methods%2520under%2520PL%2520Inequality%26entry.906535625%3DKushal%2520Chakrabarti%2520and%2520Mayank%2520Baranwal%26entry.1292438233%3D%2520%2520Adaptive%2520gradient-descent%2520optimizers%2520are%2520the%2520standard%2520choice%2520for%2520training%250Aneural%2520network%2520models.%2520Despite%2520their%2520faster%2520convergence%2520than%2520gradient-descent%250Aand%2520remarkable%2520performance%2520in%2520practice%252C%2520the%2520adaptive%2520optimizers%2520are%2520not%2520as%2520well%250Aunderstood%2520as%2520vanilla%2520gradient-descent.%2520A%2520reason%2520is%2520that%2520the%2520dynamic%2520update%2520of%250Athe%2520learning%2520rate%2520that%2520helps%2520in%2520faster%2520convergence%2520of%2520these%2520methods%2520also%2520makes%250Atheir%2520analysis%2520intricate.%2520Particularly%252C%2520the%2520simple%2520gradient-descent%2520method%250Aconverges%2520at%2520a%2520linear%2520rate%2520for%2520a%2520class%2520of%2520optimization%2520problems%252C%2520whereas%2520the%250Apractically%2520faster%2520adaptive%2520gradient%2520methods%2520lack%2520such%2520a%2520theoretical%2520guarantee.%250AThe%2520Polyak-%257B%255CL%257Dojasiewicz%2520%2528PL%2529%2520inequality%2520is%2520the%2520weakest%2520known%2520class%252C%2520for%2520which%250Alinear%2520convergence%2520of%2520gradient-descent%2520and%2520its%2520momentum%2520variants%2520has%2520been%250Aproved.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520prove%2520that%2520AdaGrad%2520and%2520Adam%252C%2520two%250Awell-known%2520adaptive%2520gradient%2520methods%252C%2520converge%2520linearly%2520when%2520the%2520cost%2520function%250Ais%2520smooth%2520and%2520satisfies%2520the%2520PL%2520inequality.%2520Our%2520theoretical%2520framework%2520follows%2520a%250Asimple%2520and%2520unified%2520approach%252C%2520applicable%2520to%2520both%2520batch%2520and%2520stochastic%2520gradients%252C%250Awhich%2520can%2520potentially%2520be%2520utilized%2520in%2520analyzing%2520linear%2520convergence%2520of%2520other%250Avariants%2520of%2520Adam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Methodology%20Establishing%20Linear%20Convergence%20of%20Adaptive%20Gradient%0A%20%20Methods%20under%20PL%20Inequality&entry.906535625=Kushal%20Chakrabarti%20and%20Mayank%20Baranwal&entry.1292438233=%20%20Adaptive%20gradient-descent%20optimizers%20are%20the%20standard%20choice%20for%20training%0Aneural%20network%20models.%20Despite%20their%20faster%20convergence%20than%20gradient-descent%0Aand%20remarkable%20performance%20in%20practice%2C%20the%20adaptive%20optimizers%20are%20not%20as%20well%0Aunderstood%20as%20vanilla%20gradient-descent.%20A%20reason%20is%20that%20the%20dynamic%20update%20of%0Athe%20learning%20rate%20that%20helps%20in%20faster%20convergence%20of%20these%20methods%20also%20makes%0Atheir%20analysis%20intricate.%20Particularly%2C%20the%20simple%20gradient-descent%20method%0Aconverges%20at%20a%20linear%20rate%20for%20a%20class%20of%20optimization%20problems%2C%20whereas%20the%0Apractically%20faster%20adaptive%20gradient%20methods%20lack%20such%20a%20theoretical%20guarantee.%0AThe%20Polyak-%7B%5CL%7Dojasiewicz%20%28PL%29%20inequality%20is%20the%20weakest%20known%20class%2C%20for%20which%0Alinear%20convergence%20of%20gradient-descent%20and%20its%20momentum%20variants%20has%20been%0Aproved.%20Therefore%2C%20in%20this%20paper%2C%20we%20prove%20that%20AdaGrad%20and%20Adam%2C%20two%0Awell-known%20adaptive%20gradient%20methods%2C%20converge%20linearly%20when%20the%20cost%20function%0Ais%20smooth%20and%20satisfies%20the%20PL%20inequality.%20Our%20theoretical%20framework%20follows%20a%0Asimple%20and%20unified%20approach%2C%20applicable%20to%20both%20batch%20and%20stochastic%20gradients%2C%0Awhich%20can%20potentially%20be%20utilized%20in%20analyzing%20linear%20convergence%20of%20other%0Avariants%20of%20Adam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12629v1&entry.124074799=Read"},
{"title": "Zero-shot Text-guided Infinite Image Synthesis with LLM guidance", "author": "Soyeong Kwon and Taegyeong Lee and Taehwan Kim", "abstract": "  Text-guided image editing and generation methods have diverse real-world\napplications. However, text-guided infinite image synthesis faces several\nchallenges. First, there is a lack of text-image paired datasets with\nhigh-resolution and contextual diversity. Second, expanding images based on\ntext requires global coherence and rich local context understanding. Previous\nstudies have mainly focused on limited categories, such as natural landscapes,\nand also required to train on high-resolution images with paired text. To\naddress these challenges, we propose a novel approach utilizing Large Language\nModels (LLMs) for both global coherence and local context understanding,\nwithout any high-resolution text-image paired training dataset. We train the\ndiffusion model to expand an image conditioned on global and local captions\ngenerated from the LLM and visual feature. At the inference stage, given an\nimage and a global caption, we use the LLM to generate a next local caption to\nexpand the input image. Then, we expand the image using the global caption,\ngenerated local caption and the visual feature to consider global consistency\nand spatial local context. In experiments, our model outperforms the baselines\nboth quantitatively and qualitatively. Furthermore, our model demonstrates the\ncapability of text-guided arbitrary-sized image generation in zero-shot manner\nwith LLM guidance.\n", "link": "http://arxiv.org/abs/2407.12642v1", "date": "2024-07-17", "relevancy": 1.7673, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6036}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5895}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Text-guided%20Infinite%20Image%20Synthesis%20with%20LLM%20guidance&body=Title%3A%20Zero-shot%20Text-guided%20Infinite%20Image%20Synthesis%20with%20LLM%20guidance%0AAuthor%3A%20Soyeong%20Kwon%20and%20Taegyeong%20Lee%20and%20Taehwan%20Kim%0AAbstract%3A%20%20%20Text-guided%20image%20editing%20and%20generation%20methods%20have%20diverse%20real-world%0Aapplications.%20However%2C%20text-guided%20infinite%20image%20synthesis%20faces%20several%0Achallenges.%20First%2C%20there%20is%20a%20lack%20of%20text-image%20paired%20datasets%20with%0Ahigh-resolution%20and%20contextual%20diversity.%20Second%2C%20expanding%20images%20based%20on%0Atext%20requires%20global%20coherence%20and%20rich%20local%20context%20understanding.%20Previous%0Astudies%20have%20mainly%20focused%20on%20limited%20categories%2C%20such%20as%20natural%20landscapes%2C%0Aand%20also%20required%20to%20train%20on%20high-resolution%20images%20with%20paired%20text.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20utilizing%20Large%20Language%0AModels%20%28LLMs%29%20for%20both%20global%20coherence%20and%20local%20context%20understanding%2C%0Awithout%20any%20high-resolution%20text-image%20paired%20training%20dataset.%20We%20train%20the%0Adiffusion%20model%20to%20expand%20an%20image%20conditioned%20on%20global%20and%20local%20captions%0Agenerated%20from%20the%20LLM%20and%20visual%20feature.%20At%20the%20inference%20stage%2C%20given%20an%0Aimage%20and%20a%20global%20caption%2C%20we%20use%20the%20LLM%20to%20generate%20a%20next%20local%20caption%20to%0Aexpand%20the%20input%20image.%20Then%2C%20we%20expand%20the%20image%20using%20the%20global%20caption%2C%0Agenerated%20local%20caption%20and%20the%20visual%20feature%20to%20consider%20global%20consistency%0Aand%20spatial%20local%20context.%20In%20experiments%2C%20our%20model%20outperforms%20the%20baselines%0Aboth%20quantitatively%20and%20qualitatively.%20Furthermore%2C%20our%20model%20demonstrates%20the%0Acapability%20of%20text-guided%20arbitrary-sized%20image%20generation%20in%20zero-shot%20manner%0Awith%20LLM%20guidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Text-guided%2520Infinite%2520Image%2520Synthesis%2520with%2520LLM%2520guidance%26entry.906535625%3DSoyeong%2520Kwon%2520and%2520Taegyeong%2520Lee%2520and%2520Taehwan%2520Kim%26entry.1292438233%3D%2520%2520Text-guided%2520image%2520editing%2520and%2520generation%2520methods%2520have%2520diverse%2520real-world%250Aapplications.%2520However%252C%2520text-guided%2520infinite%2520image%2520synthesis%2520faces%2520several%250Achallenges.%2520First%252C%2520there%2520is%2520a%2520lack%2520of%2520text-image%2520paired%2520datasets%2520with%250Ahigh-resolution%2520and%2520contextual%2520diversity.%2520Second%252C%2520expanding%2520images%2520based%2520on%250Atext%2520requires%2520global%2520coherence%2520and%2520rich%2520local%2520context%2520understanding.%2520Previous%250Astudies%2520have%2520mainly%2520focused%2520on%2520limited%2520categories%252C%2520such%2520as%2520natural%2520landscapes%252C%250Aand%2520also%2520required%2520to%2520train%2520on%2520high-resolution%2520images%2520with%2520paired%2520text.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520approach%2520utilizing%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520for%2520both%2520global%2520coherence%2520and%2520local%2520context%2520understanding%252C%250Awithout%2520any%2520high-resolution%2520text-image%2520paired%2520training%2520dataset.%2520We%2520train%2520the%250Adiffusion%2520model%2520to%2520expand%2520an%2520image%2520conditioned%2520on%2520global%2520and%2520local%2520captions%250Agenerated%2520from%2520the%2520LLM%2520and%2520visual%2520feature.%2520At%2520the%2520inference%2520stage%252C%2520given%2520an%250Aimage%2520and%2520a%2520global%2520caption%252C%2520we%2520use%2520the%2520LLM%2520to%2520generate%2520a%2520next%2520local%2520caption%2520to%250Aexpand%2520the%2520input%2520image.%2520Then%252C%2520we%2520expand%2520the%2520image%2520using%2520the%2520global%2520caption%252C%250Agenerated%2520local%2520caption%2520and%2520the%2520visual%2520feature%2520to%2520consider%2520global%2520consistency%250Aand%2520spatial%2520local%2520context.%2520In%2520experiments%252C%2520our%2520model%2520outperforms%2520the%2520baselines%250Aboth%2520quantitatively%2520and%2520qualitatively.%2520Furthermore%252C%2520our%2520model%2520demonstrates%2520the%250Acapability%2520of%2520text-guided%2520arbitrary-sized%2520image%2520generation%2520in%2520zero-shot%2520manner%250Awith%2520LLM%2520guidance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Text-guided%20Infinite%20Image%20Synthesis%20with%20LLM%20guidance&entry.906535625=Soyeong%20Kwon%20and%20Taegyeong%20Lee%20and%20Taehwan%20Kim&entry.1292438233=%20%20Text-guided%20image%20editing%20and%20generation%20methods%20have%20diverse%20real-world%0Aapplications.%20However%2C%20text-guided%20infinite%20image%20synthesis%20faces%20several%0Achallenges.%20First%2C%20there%20is%20a%20lack%20of%20text-image%20paired%20datasets%20with%0Ahigh-resolution%20and%20contextual%20diversity.%20Second%2C%20expanding%20images%20based%20on%0Atext%20requires%20global%20coherence%20and%20rich%20local%20context%20understanding.%20Previous%0Astudies%20have%20mainly%20focused%20on%20limited%20categories%2C%20such%20as%20natural%20landscapes%2C%0Aand%20also%20required%20to%20train%20on%20high-resolution%20images%20with%20paired%20text.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20utilizing%20Large%20Language%0AModels%20%28LLMs%29%20for%20both%20global%20coherence%20and%20local%20context%20understanding%2C%0Awithout%20any%20high-resolution%20text-image%20paired%20training%20dataset.%20We%20train%20the%0Adiffusion%20model%20to%20expand%20an%20image%20conditioned%20on%20global%20and%20local%20captions%0Agenerated%20from%20the%20LLM%20and%20visual%20feature.%20At%20the%20inference%20stage%2C%20given%20an%0Aimage%20and%20a%20global%20caption%2C%20we%20use%20the%20LLM%20to%20generate%20a%20next%20local%20caption%20to%0Aexpand%20the%20input%20image.%20Then%2C%20we%20expand%20the%20image%20using%20the%20global%20caption%2C%0Agenerated%20local%20caption%20and%20the%20visual%20feature%20to%20consider%20global%20consistency%0Aand%20spatial%20local%20context.%20In%20experiments%2C%20our%20model%20outperforms%20the%20baselines%0Aboth%20quantitatively%20and%20qualitatively.%20Furthermore%2C%20our%20model%20demonstrates%20the%0Acapability%20of%20text-guided%20arbitrary-sized%20image%20generation%20in%20zero-shot%20manner%0Awith%20LLM%20guidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12642v1&entry.124074799=Read"},
{"title": "RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban\n  Scenes", "author": "Thang-Anh-Quan Nguyen and Luis Rold\u00e3o and Nathan Piasco and Moussab Bennehar and Dzmitry Tsishkou", "abstract": "  The task of separating dynamic objects from static environments using NeRFs\nhas been widely studied in recent years. However, capturing large-scale scenes\nstill poses a challenge due to their complex geometric structures and\nunconstrained dynamics. Without the help of 3D motion cues, previous methods\noften require simplified setups with slow camera motion and only a few/single\ndynamic actors, leading to suboptimal solutions in most urban setups. To\novercome such limitations, we present RoDUS, a pipeline for decomposing static\nand dynamic elements in urban scenes, with thoughtfully separated NeRF models\nfor moving and non-moving components. Our approach utilizes a robust\nkernel-based initialization coupled with 4D semantic information to selectively\nguide the learning process. This strategy enables accurate capturing of the\ndynamics in the scene, resulting in reduced floating artifacts in the\nreconstructed background, all by using self-supervision. Notably, experimental\nevaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of\nour method in decomposing challenging urban scenes into precise static and\ndynamic components.\n", "link": "http://arxiv.org/abs/2403.09419v2", "date": "2024-07-17", "relevancy": 1.7643, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6243}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5911}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoDUS%3A%20Robust%20Decomposition%20of%20Static%20and%20Dynamic%20Elements%20in%20Urban%0A%20%20Scenes&body=Title%3A%20RoDUS%3A%20Robust%20Decomposition%20of%20Static%20and%20Dynamic%20Elements%20in%20Urban%0A%20%20Scenes%0AAuthor%3A%20Thang-Anh-Quan%20Nguyen%20and%20Luis%20Rold%C3%A3o%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Dzmitry%20Tsishkou%0AAbstract%3A%20%20%20The%20task%20of%20separating%20dynamic%20objects%20from%20static%20environments%20using%20NeRFs%0Ahas%20been%20widely%20studied%20in%20recent%20years.%20However%2C%20capturing%20large-scale%20scenes%0Astill%20poses%20a%20challenge%20due%20to%20their%20complex%20geometric%20structures%20and%0Aunconstrained%20dynamics.%20Without%20the%20help%20of%203D%20motion%20cues%2C%20previous%20methods%0Aoften%20require%20simplified%20setups%20with%20slow%20camera%20motion%20and%20only%20a%20few/single%0Adynamic%20actors%2C%20leading%20to%20suboptimal%20solutions%20in%20most%20urban%20setups.%20To%0Aovercome%20such%20limitations%2C%20we%20present%20RoDUS%2C%20a%20pipeline%20for%20decomposing%20static%0Aand%20dynamic%20elements%20in%20urban%20scenes%2C%20with%20thoughtfully%20separated%20NeRF%20models%0Afor%20moving%20and%20non-moving%20components.%20Our%20approach%20utilizes%20a%20robust%0Akernel-based%20initialization%20coupled%20with%204D%20semantic%20information%20to%20selectively%0Aguide%20the%20learning%20process.%20This%20strategy%20enables%20accurate%20capturing%20of%20the%0Adynamics%20in%20the%20scene%2C%20resulting%20in%20reduced%20floating%20artifacts%20in%20the%0Areconstructed%20background%2C%20all%20by%20using%20self-supervision.%20Notably%2C%20experimental%0Aevaluations%20on%20KITTI-360%20and%20Pandaset%20datasets%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20in%20decomposing%20challenging%20urban%20scenes%20into%20precise%20static%20and%0Adynamic%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoDUS%253A%2520Robust%2520Decomposition%2520of%2520Static%2520and%2520Dynamic%2520Elements%2520in%2520Urban%250A%2520%2520Scenes%26entry.906535625%3DThang-Anh-Quan%2520Nguyen%2520and%2520Luis%2520Rold%25C3%25A3o%2520and%2520Nathan%2520Piasco%2520and%2520Moussab%2520Bennehar%2520and%2520Dzmitry%2520Tsishkou%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520separating%2520dynamic%2520objects%2520from%2520static%2520environments%2520using%2520NeRFs%250Ahas%2520been%2520widely%2520studied%2520in%2520recent%2520years.%2520However%252C%2520capturing%2520large-scale%2520scenes%250Astill%2520poses%2520a%2520challenge%2520due%2520to%2520their%2520complex%2520geometric%2520structures%2520and%250Aunconstrained%2520dynamics.%2520Without%2520the%2520help%2520of%25203D%2520motion%2520cues%252C%2520previous%2520methods%250Aoften%2520require%2520simplified%2520setups%2520with%2520slow%2520camera%2520motion%2520and%2520only%2520a%2520few/single%250Adynamic%2520actors%252C%2520leading%2520to%2520suboptimal%2520solutions%2520in%2520most%2520urban%2520setups.%2520To%250Aovercome%2520such%2520limitations%252C%2520we%2520present%2520RoDUS%252C%2520a%2520pipeline%2520for%2520decomposing%2520static%250Aand%2520dynamic%2520elements%2520in%2520urban%2520scenes%252C%2520with%2520thoughtfully%2520separated%2520NeRF%2520models%250Afor%2520moving%2520and%2520non-moving%2520components.%2520Our%2520approach%2520utilizes%2520a%2520robust%250Akernel-based%2520initialization%2520coupled%2520with%25204D%2520semantic%2520information%2520to%2520selectively%250Aguide%2520the%2520learning%2520process.%2520This%2520strategy%2520enables%2520accurate%2520capturing%2520of%2520the%250Adynamics%2520in%2520the%2520scene%252C%2520resulting%2520in%2520reduced%2520floating%2520artifacts%2520in%2520the%250Areconstructed%2520background%252C%2520all%2520by%2520using%2520self-supervision.%2520Notably%252C%2520experimental%250Aevaluations%2520on%2520KITTI-360%2520and%2520Pandaset%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520method%2520in%2520decomposing%2520challenging%2520urban%2520scenes%2520into%2520precise%2520static%2520and%250Adynamic%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoDUS%3A%20Robust%20Decomposition%20of%20Static%20and%20Dynamic%20Elements%20in%20Urban%0A%20%20Scenes&entry.906535625=Thang-Anh-Quan%20Nguyen%20and%20Luis%20Rold%C3%A3o%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Dzmitry%20Tsishkou&entry.1292438233=%20%20The%20task%20of%20separating%20dynamic%20objects%20from%20static%20environments%20using%20NeRFs%0Ahas%20been%20widely%20studied%20in%20recent%20years.%20However%2C%20capturing%20large-scale%20scenes%0Astill%20poses%20a%20challenge%20due%20to%20their%20complex%20geometric%20structures%20and%0Aunconstrained%20dynamics.%20Without%20the%20help%20of%203D%20motion%20cues%2C%20previous%20methods%0Aoften%20require%20simplified%20setups%20with%20slow%20camera%20motion%20and%20only%20a%20few/single%0Adynamic%20actors%2C%20leading%20to%20suboptimal%20solutions%20in%20most%20urban%20setups.%20To%0Aovercome%20such%20limitations%2C%20we%20present%20RoDUS%2C%20a%20pipeline%20for%20decomposing%20static%0Aand%20dynamic%20elements%20in%20urban%20scenes%2C%20with%20thoughtfully%20separated%20NeRF%20models%0Afor%20moving%20and%20non-moving%20components.%20Our%20approach%20utilizes%20a%20robust%0Akernel-based%20initialization%20coupled%20with%204D%20semantic%20information%20to%20selectively%0Aguide%20the%20learning%20process.%20This%20strategy%20enables%20accurate%20capturing%20of%20the%0Adynamics%20in%20the%20scene%2C%20resulting%20in%20reduced%20floating%20artifacts%20in%20the%0Areconstructed%20background%2C%20all%20by%20using%20self-supervision.%20Notably%2C%20experimental%0Aevaluations%20on%20KITTI-360%20and%20Pandaset%20datasets%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20in%20decomposing%20challenging%20urban%20scenes%20into%20precise%20static%20and%0Adynamic%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09419v2&entry.124074799=Read"},
{"title": "Instance-wise Uncertainty for Class Imbalance in Semantic Segmentation", "author": "Lu\u00eds Almeida and In\u00eas Dutra and Francesco Renna", "abstract": "  Semantic segmentation is a fundamental computer vision task with a vast\nnumber of applications. State of the art methods increasingly rely on deep\nlearning models, known to incorrectly estimate uncertainty and being\noverconfident in predictions, especially in data not seen during training. This\nis particularly problematic in semantic segmentation due to inherent class\nimbalance. Popular uncertainty quantification approaches are task-agnostic and\nfail to leverage spatial pixel correlations in uncertainty estimates, crucial\nin this task. In this work, a novel training methodology specifically designed\nfor semantic segmentation is presented. Training samples are weighted by\ninstance-wise uncertainty masks computed by an ensemble. This is shown to\nincrease performance on minority classes, boost model generalization and\nrobustness to domain-shift when compared to using the inverse of class\nproportions or no class weights at all. This method addresses the challenges of\nclass imbalance and uncertainty estimation in semantic segmentation,\npotentially enhancing model performance and reliability across various\napplications.\n", "link": "http://arxiv.org/abs/2407.12609v1", "date": "2024-07-17", "relevancy": 1.7467, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5899}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-wise%20Uncertainty%20for%20Class%20Imbalance%20in%20Semantic%20Segmentation&body=Title%3A%20Instance-wise%20Uncertainty%20for%20Class%20Imbalance%20in%20Semantic%20Segmentation%0AAuthor%3A%20Lu%C3%ADs%20Almeida%20and%20In%C3%AAs%20Dutra%20and%20Francesco%20Renna%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20a%20fundamental%20computer%20vision%20task%20with%20a%20vast%0Anumber%20of%20applications.%20State%20of%20the%20art%20methods%20increasingly%20rely%20on%20deep%0Alearning%20models%2C%20known%20to%20incorrectly%20estimate%20uncertainty%20and%20being%0Aoverconfident%20in%20predictions%2C%20especially%20in%20data%20not%20seen%20during%20training.%20This%0Ais%20particularly%20problematic%20in%20semantic%20segmentation%20due%20to%20inherent%20class%0Aimbalance.%20Popular%20uncertainty%20quantification%20approaches%20are%20task-agnostic%20and%0Afail%20to%20leverage%20spatial%20pixel%20correlations%20in%20uncertainty%20estimates%2C%20crucial%0Ain%20this%20task.%20In%20this%20work%2C%20a%20novel%20training%20methodology%20specifically%20designed%0Afor%20semantic%20segmentation%20is%20presented.%20Training%20samples%20are%20weighted%20by%0Ainstance-wise%20uncertainty%20masks%20computed%20by%20an%20ensemble.%20This%20is%20shown%20to%0Aincrease%20performance%20on%20minority%20classes%2C%20boost%20model%20generalization%20and%0Arobustness%20to%20domain-shift%20when%20compared%20to%20using%20the%20inverse%20of%20class%0Aproportions%20or%20no%20class%20weights%20at%20all.%20This%20method%20addresses%20the%20challenges%20of%0Aclass%20imbalance%20and%20uncertainty%20estimation%20in%20semantic%20segmentation%2C%0Apotentially%20enhancing%20model%20performance%20and%20reliability%20across%20various%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-wise%2520Uncertainty%2520for%2520Class%2520Imbalance%2520in%2520Semantic%2520Segmentation%26entry.906535625%3DLu%25C3%25ADs%2520Almeida%2520and%2520In%25C3%25AAs%2520Dutra%2520and%2520Francesco%2520Renna%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520is%2520a%2520fundamental%2520computer%2520vision%2520task%2520with%2520a%2520vast%250Anumber%2520of%2520applications.%2520State%2520of%2520the%2520art%2520methods%2520increasingly%2520rely%2520on%2520deep%250Alearning%2520models%252C%2520known%2520to%2520incorrectly%2520estimate%2520uncertainty%2520and%2520being%250Aoverconfident%2520in%2520predictions%252C%2520especially%2520in%2520data%2520not%2520seen%2520during%2520training.%2520This%250Ais%2520particularly%2520problematic%2520in%2520semantic%2520segmentation%2520due%2520to%2520inherent%2520class%250Aimbalance.%2520Popular%2520uncertainty%2520quantification%2520approaches%2520are%2520task-agnostic%2520and%250Afail%2520to%2520leverage%2520spatial%2520pixel%2520correlations%2520in%2520uncertainty%2520estimates%252C%2520crucial%250Ain%2520this%2520task.%2520In%2520this%2520work%252C%2520a%2520novel%2520training%2520methodology%2520specifically%2520designed%250Afor%2520semantic%2520segmentation%2520is%2520presented.%2520Training%2520samples%2520are%2520weighted%2520by%250Ainstance-wise%2520uncertainty%2520masks%2520computed%2520by%2520an%2520ensemble.%2520This%2520is%2520shown%2520to%250Aincrease%2520performance%2520on%2520minority%2520classes%252C%2520boost%2520model%2520generalization%2520and%250Arobustness%2520to%2520domain-shift%2520when%2520compared%2520to%2520using%2520the%2520inverse%2520of%2520class%250Aproportions%2520or%2520no%2520class%2520weights%2520at%2520all.%2520This%2520method%2520addresses%2520the%2520challenges%2520of%250Aclass%2520imbalance%2520and%2520uncertainty%2520estimation%2520in%2520semantic%2520segmentation%252C%250Apotentially%2520enhancing%2520model%2520performance%2520and%2520reliability%2520across%2520various%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-wise%20Uncertainty%20for%20Class%20Imbalance%20in%20Semantic%20Segmentation&entry.906535625=Lu%C3%ADs%20Almeida%20and%20In%C3%AAs%20Dutra%20and%20Francesco%20Renna&entry.1292438233=%20%20Semantic%20segmentation%20is%20a%20fundamental%20computer%20vision%20task%20with%20a%20vast%0Anumber%20of%20applications.%20State%20of%20the%20art%20methods%20increasingly%20rely%20on%20deep%0Alearning%20models%2C%20known%20to%20incorrectly%20estimate%20uncertainty%20and%20being%0Aoverconfident%20in%20predictions%2C%20especially%20in%20data%20not%20seen%20during%20training.%20This%0Ais%20particularly%20problematic%20in%20semantic%20segmentation%20due%20to%20inherent%20class%0Aimbalance.%20Popular%20uncertainty%20quantification%20approaches%20are%20task-agnostic%20and%0Afail%20to%20leverage%20spatial%20pixel%20correlations%20in%20uncertainty%20estimates%2C%20crucial%0Ain%20this%20task.%20In%20this%20work%2C%20a%20novel%20training%20methodology%20specifically%20designed%0Afor%20semantic%20segmentation%20is%20presented.%20Training%20samples%20are%20weighted%20by%0Ainstance-wise%20uncertainty%20masks%20computed%20by%20an%20ensemble.%20This%20is%20shown%20to%0Aincrease%20performance%20on%20minority%20classes%2C%20boost%20model%20generalization%20and%0Arobustness%20to%20domain-shift%20when%20compared%20to%20using%20the%20inverse%20of%20class%0Aproportions%20or%20no%20class%20weights%20at%20all.%20This%20method%20addresses%20the%20challenges%20of%0Aclass%20imbalance%20and%20uncertainty%20estimation%20in%20semantic%20segmentation%2C%0Apotentially%20enhancing%20model%20performance%20and%20reliability%20across%20various%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12609v1&entry.124074799=Read"},
{"title": "GraphMuse: A Library for Symbolic Music Graph Processing", "author": "Emmanouil Karystinaios and Gerhard Widmer", "abstract": "  Graph Neural Networks (GNNs) have recently gained traction in symbolic music\ntasks, yet a lack of a unified framework impedes progress. Addressing this gap,\nwe present GraphMuse, a graph processing framework and library that facilitates\nefficient music graph processing and GNN training for symbolic music tasks.\nCentral to our contribution is a new neighbor sampling technique specifically\ntargeted toward meaningful behavior in musical scores. Additionally, GraphMuse\nintegrates hierarchical modeling elements that augment the expressivity and\ncapabilities of graph networks for musical tasks. Experiments with two specific\nmusical prediction tasks -- pitch spelling and cadence detection -- demonstrate\nsignificant performance improvement over previous methods. Our hope is that\nGraphMuse will lead to a boost in, and standardization of, symbolic music\nprocessing based on graph representations. The library is available at\nhttps://github.com/manoskary/graphmuse\n", "link": "http://arxiv.org/abs/2407.12671v1", "date": "2024-07-17", "relevancy": 1.7364, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4436}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4291}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphMuse%3A%20A%20Library%20for%20Symbolic%20Music%20Graph%20Processing&body=Title%3A%20GraphMuse%3A%20A%20Library%20for%20Symbolic%20Music%20Graph%20Processing%0AAuthor%3A%20Emmanouil%20Karystinaios%20and%20Gerhard%20Widmer%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20gained%20traction%20in%20symbolic%20music%0Atasks%2C%20yet%20a%20lack%20of%20a%20unified%20framework%20impedes%20progress.%20Addressing%20this%20gap%2C%0Awe%20present%20GraphMuse%2C%20a%20graph%20processing%20framework%20and%20library%20that%20facilitates%0Aefficient%20music%20graph%20processing%20and%20GNN%20training%20for%20symbolic%20music%20tasks.%0ACentral%20to%20our%20contribution%20is%20a%20new%20neighbor%20sampling%20technique%20specifically%0Atargeted%20toward%20meaningful%20behavior%20in%20musical%20scores.%20Additionally%2C%20GraphMuse%0Aintegrates%20hierarchical%20modeling%20elements%20that%20augment%20the%20expressivity%20and%0Acapabilities%20of%20graph%20networks%20for%20musical%20tasks.%20Experiments%20with%20two%20specific%0Amusical%20prediction%20tasks%20--%20pitch%20spelling%20and%20cadence%20detection%20--%20demonstrate%0Asignificant%20performance%20improvement%20over%20previous%20methods.%20Our%20hope%20is%20that%0AGraphMuse%20will%20lead%20to%20a%20boost%20in%2C%20and%20standardization%20of%2C%20symbolic%20music%0Aprocessing%20based%20on%20graph%20representations.%20The%20library%20is%20available%20at%0Ahttps%3A//github.com/manoskary/graphmuse%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphMuse%253A%2520A%2520Library%2520for%2520Symbolic%2520Music%2520Graph%2520Processing%26entry.906535625%3DEmmanouil%2520Karystinaios%2520and%2520Gerhard%2520Widmer%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520recently%2520gained%2520traction%2520in%2520symbolic%2520music%250Atasks%252C%2520yet%2520a%2520lack%2520of%2520a%2520unified%2520framework%2520impedes%2520progress.%2520Addressing%2520this%2520gap%252C%250Awe%2520present%2520GraphMuse%252C%2520a%2520graph%2520processing%2520framework%2520and%2520library%2520that%2520facilitates%250Aefficient%2520music%2520graph%2520processing%2520and%2520GNN%2520training%2520for%2520symbolic%2520music%2520tasks.%250ACentral%2520to%2520our%2520contribution%2520is%2520a%2520new%2520neighbor%2520sampling%2520technique%2520specifically%250Atargeted%2520toward%2520meaningful%2520behavior%2520in%2520musical%2520scores.%2520Additionally%252C%2520GraphMuse%250Aintegrates%2520hierarchical%2520modeling%2520elements%2520that%2520augment%2520the%2520expressivity%2520and%250Acapabilities%2520of%2520graph%2520networks%2520for%2520musical%2520tasks.%2520Experiments%2520with%2520two%2520specific%250Amusical%2520prediction%2520tasks%2520--%2520pitch%2520spelling%2520and%2520cadence%2520detection%2520--%2520demonstrate%250Asignificant%2520performance%2520improvement%2520over%2520previous%2520methods.%2520Our%2520hope%2520is%2520that%250AGraphMuse%2520will%2520lead%2520to%2520a%2520boost%2520in%252C%2520and%2520standardization%2520of%252C%2520symbolic%2520music%250Aprocessing%2520based%2520on%2520graph%2520representations.%2520The%2520library%2520is%2520available%2520at%250Ahttps%253A//github.com/manoskary/graphmuse%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphMuse%3A%20A%20Library%20for%20Symbolic%20Music%20Graph%20Processing&entry.906535625=Emmanouil%20Karystinaios%20and%20Gerhard%20Widmer&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20gained%20traction%20in%20symbolic%20music%0Atasks%2C%20yet%20a%20lack%20of%20a%20unified%20framework%20impedes%20progress.%20Addressing%20this%20gap%2C%0Awe%20present%20GraphMuse%2C%20a%20graph%20processing%20framework%20and%20library%20that%20facilitates%0Aefficient%20music%20graph%20processing%20and%20GNN%20training%20for%20symbolic%20music%20tasks.%0ACentral%20to%20our%20contribution%20is%20a%20new%20neighbor%20sampling%20technique%20specifically%0Atargeted%20toward%20meaningful%20behavior%20in%20musical%20scores.%20Additionally%2C%20GraphMuse%0Aintegrates%20hierarchical%20modeling%20elements%20that%20augment%20the%20expressivity%20and%0Acapabilities%20of%20graph%20networks%20for%20musical%20tasks.%20Experiments%20with%20two%20specific%0Amusical%20prediction%20tasks%20--%20pitch%20spelling%20and%20cadence%20detection%20--%20demonstrate%0Asignificant%20performance%20improvement%20over%20previous%20methods.%20Our%20hope%20is%20that%0AGraphMuse%20will%20lead%20to%20a%20boost%20in%2C%20and%20standardization%20of%2C%20symbolic%20music%0Aprocessing%20based%20on%20graph%20representations.%20The%20library%20is%20available%20at%0Ahttps%3A//github.com/manoskary/graphmuse%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12671v1&entry.124074799=Read"},
{"title": "SMooDi: Stylized Motion Diffusion Model", "author": "Lei Zhong and Yiming Xie and Varun Jampani and Deqing Sun and Huaizu Jiang", "abstract": "  We introduce a novel Stylized Motion Diffusion model, dubbed SMooDi, to\ngenerate stylized motion driven by content texts and style motion sequences.\nUnlike existing methods that either generate motion of various content or\ntransfer style from one sequence to another, SMooDi can rapidly generate motion\nacross a broad range of content and diverse styles. To this end, we tailor a\npre-trained text-to-motion model for stylization. Specifically, we propose\nstyle guidance to ensure that the generated motion closely matches the\nreference style, alongside a lightweight style adaptor that directs the motion\ntowards the desired style while ensuring realism. Experiments across various\napplications demonstrate that our proposed framework outperforms existing\nmethods in stylized motion generation.\n", "link": "http://arxiv.org/abs/2407.12783v1", "date": "2024-07-17", "relevancy": 1.7333, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6404}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5648}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMooDi%3A%20Stylized%20Motion%20Diffusion%20Model&body=Title%3A%20SMooDi%3A%20Stylized%20Motion%20Diffusion%20Model%0AAuthor%3A%20Lei%20Zhong%20and%20Yiming%20Xie%20and%20Varun%20Jampani%20and%20Deqing%20Sun%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20Stylized%20Motion%20Diffusion%20model%2C%20dubbed%20SMooDi%2C%20to%0Agenerate%20stylized%20motion%20driven%20by%20content%20texts%20and%20style%20motion%20sequences.%0AUnlike%20existing%20methods%20that%20either%20generate%20motion%20of%20various%20content%20or%0Atransfer%20style%20from%20one%20sequence%20to%20another%2C%20SMooDi%20can%20rapidly%20generate%20motion%0Aacross%20a%20broad%20range%20of%20content%20and%20diverse%20styles.%20To%20this%20end%2C%20we%20tailor%20a%0Apre-trained%20text-to-motion%20model%20for%20stylization.%20Specifically%2C%20we%20propose%0Astyle%20guidance%20to%20ensure%20that%20the%20generated%20motion%20closely%20matches%20the%0Areference%20style%2C%20alongside%20a%20lightweight%20style%20adaptor%20that%20directs%20the%20motion%0Atowards%20the%20desired%20style%20while%20ensuring%20realism.%20Experiments%20across%20various%0Aapplications%20demonstrate%20that%20our%20proposed%20framework%20outperforms%20existing%0Amethods%20in%20stylized%20motion%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMooDi%253A%2520Stylized%2520Motion%2520Diffusion%2520Model%26entry.906535625%3DLei%2520Zhong%2520and%2520Yiming%2520Xie%2520and%2520Varun%2520Jampani%2520and%2520Deqing%2520Sun%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520Stylized%2520Motion%2520Diffusion%2520model%252C%2520dubbed%2520SMooDi%252C%2520to%250Agenerate%2520stylized%2520motion%2520driven%2520by%2520content%2520texts%2520and%2520style%2520motion%2520sequences.%250AUnlike%2520existing%2520methods%2520that%2520either%2520generate%2520motion%2520of%2520various%2520content%2520or%250Atransfer%2520style%2520from%2520one%2520sequence%2520to%2520another%252C%2520SMooDi%2520can%2520rapidly%2520generate%2520motion%250Aacross%2520a%2520broad%2520range%2520of%2520content%2520and%2520diverse%2520styles.%2520To%2520this%2520end%252C%2520we%2520tailor%2520a%250Apre-trained%2520text-to-motion%2520model%2520for%2520stylization.%2520Specifically%252C%2520we%2520propose%250Astyle%2520guidance%2520to%2520ensure%2520that%2520the%2520generated%2520motion%2520closely%2520matches%2520the%250Areference%2520style%252C%2520alongside%2520a%2520lightweight%2520style%2520adaptor%2520that%2520directs%2520the%2520motion%250Atowards%2520the%2520desired%2520style%2520while%2520ensuring%2520realism.%2520Experiments%2520across%2520various%250Aapplications%2520demonstrate%2520that%2520our%2520proposed%2520framework%2520outperforms%2520existing%250Amethods%2520in%2520stylized%2520motion%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMooDi%3A%20Stylized%20Motion%20Diffusion%20Model&entry.906535625=Lei%20Zhong%20and%20Yiming%20Xie%20and%20Varun%20Jampani%20and%20Deqing%20Sun%20and%20Huaizu%20Jiang&entry.1292438233=%20%20We%20introduce%20a%20novel%20Stylized%20Motion%20Diffusion%20model%2C%20dubbed%20SMooDi%2C%20to%0Agenerate%20stylized%20motion%20driven%20by%20content%20texts%20and%20style%20motion%20sequences.%0AUnlike%20existing%20methods%20that%20either%20generate%20motion%20of%20various%20content%20or%0Atransfer%20style%20from%20one%20sequence%20to%20another%2C%20SMooDi%20can%20rapidly%20generate%20motion%0Aacross%20a%20broad%20range%20of%20content%20and%20diverse%20styles.%20To%20this%20end%2C%20we%20tailor%20a%0Apre-trained%20text-to-motion%20model%20for%20stylization.%20Specifically%2C%20we%20propose%0Astyle%20guidance%20to%20ensure%20that%20the%20generated%20motion%20closely%20matches%20the%0Areference%20style%2C%20alongside%20a%20lightweight%20style%20adaptor%20that%20directs%20the%20motion%0Atowards%20the%20desired%20style%20while%20ensuring%20realism.%20Experiments%20across%20various%0Aapplications%20demonstrate%20that%20our%20proposed%20framework%20outperforms%20existing%0Amethods%20in%20stylized%20motion%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12783v1&entry.124074799=Read"},
{"title": "Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion\n  Models", "author": "Richard Osuala and Daniel M. Lang and Preeti Verma and Smriti Joshi and Apostolia Tsirikoglou and Grzegorz Skorupko and Kaisar Kushibar and Lidia Garrucho and Walter H. L. Pinaya and Oliver Diaz and Julia A. Schnabel and Karim Lekadir", "abstract": "  Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow\nto localize tumors and observe their contrast kinetics, which is essential for\ncancer characterization and respective treatment decision-making. However,\ncontrast agent administration is not only associated with adverse health risks,\nbut also restricted for patients during pregnancy, and for those with kidney\nmalfunction, or other adverse reactions. With contrast uptake as key biomarker\nfor lesion malignancy, cancer recurrence risk, and treatment response, it\nbecomes pivotal to reduce the dependency on intravenous contrast agent\nadministration. To this end, we propose a multi-conditional latent diffusion\nmodel capable of acquisition time-conditioned image synthesis of DCE-MRI\ntemporal sequences. To evaluate medical image synthesis, we additionally\npropose and validate the Fr\\'echet radiomics distance as an image quality\nmeasure based on biomarker variability between synthetic and real imaging data.\nOur results demonstrate our method's ability to generate realistic\nmulti-sequence fat-saturated breast DCE-MRI and uncover the emerging potential\nof deep learning based contrast kinetics simulation. We publicly share our\naccessible codebase at https://github.com/RichardObi/ccnet and provide a\nuser-friendly library for Fr\\'echet radiomics distance calculation at\nhttps://pypi.org/project/frd-score.\n", "link": "http://arxiv.org/abs/2403.13890v3", "date": "2024-07-17", "relevancy": 1.7332, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6375}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.583}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Learning%20Contrast%20Kinetics%20with%20Multi-Condition%20Latent%20Diffusion%0A%20%20Models&body=Title%3A%20Towards%20Learning%20Contrast%20Kinetics%20with%20Multi-Condition%20Latent%20Diffusion%0A%20%20Models%0AAuthor%3A%20Richard%20Osuala%20and%20Daniel%20M.%20Lang%20and%20Preeti%20Verma%20and%20Smriti%20Joshi%20and%20Apostolia%20Tsirikoglou%20and%20Grzegorz%20Skorupko%20and%20Kaisar%20Kushibar%20and%20Lidia%20Garrucho%20and%20Walter%20H.%20L.%20Pinaya%20and%20Oliver%20Diaz%20and%20Julia%20A.%20Schnabel%20and%20Karim%20Lekadir%0AAbstract%3A%20%20%20Contrast%20agents%20in%20dynamic%20contrast%20enhanced%20magnetic%20resonance%20imaging%20allow%0Ato%20localize%20tumors%20and%20observe%20their%20contrast%20kinetics%2C%20which%20is%20essential%20for%0Acancer%20characterization%20and%20respective%20treatment%20decision-making.%20However%2C%0Acontrast%20agent%20administration%20is%20not%20only%20associated%20with%20adverse%20health%20risks%2C%0Abut%20also%20restricted%20for%20patients%20during%20pregnancy%2C%20and%20for%20those%20with%20kidney%0Amalfunction%2C%20or%20other%20adverse%20reactions.%20With%20contrast%20uptake%20as%20key%20biomarker%0Afor%20lesion%20malignancy%2C%20cancer%20recurrence%20risk%2C%20and%20treatment%20response%2C%20it%0Abecomes%20pivotal%20to%20reduce%20the%20dependency%20on%20intravenous%20contrast%20agent%0Aadministration.%20To%20this%20end%2C%20we%20propose%20a%20multi-conditional%20latent%20diffusion%0Amodel%20capable%20of%20acquisition%20time-conditioned%20image%20synthesis%20of%20DCE-MRI%0Atemporal%20sequences.%20To%20evaluate%20medical%20image%20synthesis%2C%20we%20additionally%0Apropose%20and%20validate%20the%20Fr%5C%27echet%20radiomics%20distance%20as%20an%20image%20quality%0Ameasure%20based%20on%20biomarker%20variability%20between%20synthetic%20and%20real%20imaging%20data.%0AOur%20results%20demonstrate%20our%20method%27s%20ability%20to%20generate%20realistic%0Amulti-sequence%20fat-saturated%20breast%20DCE-MRI%20and%20uncover%20the%20emerging%20potential%0Aof%20deep%20learning%20based%20contrast%20kinetics%20simulation.%20We%20publicly%20share%20our%0Aaccessible%20codebase%20at%20https%3A//github.com/RichardObi/ccnet%20and%20provide%20a%0Auser-friendly%20library%20for%20Fr%5C%27echet%20radiomics%20distance%20calculation%20at%0Ahttps%3A//pypi.org/project/frd-score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13890v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Learning%2520Contrast%2520Kinetics%2520with%2520Multi-Condition%2520Latent%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DRichard%2520Osuala%2520and%2520Daniel%2520M.%2520Lang%2520and%2520Preeti%2520Verma%2520and%2520Smriti%2520Joshi%2520and%2520Apostolia%2520Tsirikoglou%2520and%2520Grzegorz%2520Skorupko%2520and%2520Kaisar%2520Kushibar%2520and%2520Lidia%2520Garrucho%2520and%2520Walter%2520H.%2520L.%2520Pinaya%2520and%2520Oliver%2520Diaz%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Karim%2520Lekadir%26entry.1292438233%3D%2520%2520Contrast%2520agents%2520in%2520dynamic%2520contrast%2520enhanced%2520magnetic%2520resonance%2520imaging%2520allow%250Ato%2520localize%2520tumors%2520and%2520observe%2520their%2520contrast%2520kinetics%252C%2520which%2520is%2520essential%2520for%250Acancer%2520characterization%2520and%2520respective%2520treatment%2520decision-making.%2520However%252C%250Acontrast%2520agent%2520administration%2520is%2520not%2520only%2520associated%2520with%2520adverse%2520health%2520risks%252C%250Abut%2520also%2520restricted%2520for%2520patients%2520during%2520pregnancy%252C%2520and%2520for%2520those%2520with%2520kidney%250Amalfunction%252C%2520or%2520other%2520adverse%2520reactions.%2520With%2520contrast%2520uptake%2520as%2520key%2520biomarker%250Afor%2520lesion%2520malignancy%252C%2520cancer%2520recurrence%2520risk%252C%2520and%2520treatment%2520response%252C%2520it%250Abecomes%2520pivotal%2520to%2520reduce%2520the%2520dependency%2520on%2520intravenous%2520contrast%2520agent%250Aadministration.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520multi-conditional%2520latent%2520diffusion%250Amodel%2520capable%2520of%2520acquisition%2520time-conditioned%2520image%2520synthesis%2520of%2520DCE-MRI%250Atemporal%2520sequences.%2520To%2520evaluate%2520medical%2520image%2520synthesis%252C%2520we%2520additionally%250Apropose%2520and%2520validate%2520the%2520Fr%255C%2527echet%2520radiomics%2520distance%2520as%2520an%2520image%2520quality%250Ameasure%2520based%2520on%2520biomarker%2520variability%2520between%2520synthetic%2520and%2520real%2520imaging%2520data.%250AOur%2520results%2520demonstrate%2520our%2520method%2527s%2520ability%2520to%2520generate%2520realistic%250Amulti-sequence%2520fat-saturated%2520breast%2520DCE-MRI%2520and%2520uncover%2520the%2520emerging%2520potential%250Aof%2520deep%2520learning%2520based%2520contrast%2520kinetics%2520simulation.%2520We%2520publicly%2520share%2520our%250Aaccessible%2520codebase%2520at%2520https%253A//github.com/RichardObi/ccnet%2520and%2520provide%2520a%250Auser-friendly%2520library%2520for%2520Fr%255C%2527echet%2520radiomics%2520distance%2520calculation%2520at%250Ahttps%253A//pypi.org/project/frd-score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13890v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Learning%20Contrast%20Kinetics%20with%20Multi-Condition%20Latent%20Diffusion%0A%20%20Models&entry.906535625=Richard%20Osuala%20and%20Daniel%20M.%20Lang%20and%20Preeti%20Verma%20and%20Smriti%20Joshi%20and%20Apostolia%20Tsirikoglou%20and%20Grzegorz%20Skorupko%20and%20Kaisar%20Kushibar%20and%20Lidia%20Garrucho%20and%20Walter%20H.%20L.%20Pinaya%20and%20Oliver%20Diaz%20and%20Julia%20A.%20Schnabel%20and%20Karim%20Lekadir&entry.1292438233=%20%20Contrast%20agents%20in%20dynamic%20contrast%20enhanced%20magnetic%20resonance%20imaging%20allow%0Ato%20localize%20tumors%20and%20observe%20their%20contrast%20kinetics%2C%20which%20is%20essential%20for%0Acancer%20characterization%20and%20respective%20treatment%20decision-making.%20However%2C%0Acontrast%20agent%20administration%20is%20not%20only%20associated%20with%20adverse%20health%20risks%2C%0Abut%20also%20restricted%20for%20patients%20during%20pregnancy%2C%20and%20for%20those%20with%20kidney%0Amalfunction%2C%20or%20other%20adverse%20reactions.%20With%20contrast%20uptake%20as%20key%20biomarker%0Afor%20lesion%20malignancy%2C%20cancer%20recurrence%20risk%2C%20and%20treatment%20response%2C%20it%0Abecomes%20pivotal%20to%20reduce%20the%20dependency%20on%20intravenous%20contrast%20agent%0Aadministration.%20To%20this%20end%2C%20we%20propose%20a%20multi-conditional%20latent%20diffusion%0Amodel%20capable%20of%20acquisition%20time-conditioned%20image%20synthesis%20of%20DCE-MRI%0Atemporal%20sequences.%20To%20evaluate%20medical%20image%20synthesis%2C%20we%20additionally%0Apropose%20and%20validate%20the%20Fr%5C%27echet%20radiomics%20distance%20as%20an%20image%20quality%0Ameasure%20based%20on%20biomarker%20variability%20between%20synthetic%20and%20real%20imaging%20data.%0AOur%20results%20demonstrate%20our%20method%27s%20ability%20to%20generate%20realistic%0Amulti-sequence%20fat-saturated%20breast%20DCE-MRI%20and%20uncover%20the%20emerging%20potential%0Aof%20deep%20learning%20based%20contrast%20kinetics%20simulation.%20We%20publicly%20share%20our%0Aaccessible%20codebase%20at%20https%3A//github.com/RichardObi/ccnet%20and%20provide%20a%0Auser-friendly%20library%20for%20Fr%5C%27echet%20radiomics%20distance%20calculation%20at%0Ahttps%3A//pypi.org/project/frd-score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13890v3&entry.124074799=Read"},
{"title": "DP-KAN: Differentially Private Kolmogorov-Arnold Networks", "author": "Nikita P. Kalinin and Simone Bombari and Hossein Zakerinia and Christoph H. Lampert", "abstract": "  We study the Kolmogorov-Arnold Network (KAN), recently proposed as an\nalternative to the classical Multilayer Perceptron (MLP), in the application\nfor differentially private model training. Using the DP-SGD algorithm, we\ndemonstrate that KAN can be made private in a straightforward manner and\nevaluated its performance across several datasets. Our results indicate that\nthe accuracy of KAN is not only comparable with MLP but also experiences\nsimilar deterioration due to privacy constraints, making it suitable for\ndifferentially private model training.\n", "link": "http://arxiv.org/abs/2407.12569v1", "date": "2024-07-17", "relevancy": 1.7282, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4516}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4279}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-KAN%3A%20Differentially%20Private%20Kolmogorov-Arnold%20Networks&body=Title%3A%20DP-KAN%3A%20Differentially%20Private%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Nikita%20P.%20Kalinin%20and%20Simone%20Bombari%20and%20Hossein%20Zakerinia%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20%20%20We%20study%20the%20Kolmogorov-Arnold%20Network%20%28KAN%29%2C%20recently%20proposed%20as%20an%0Aalternative%20to%20the%20classical%20Multilayer%20Perceptron%20%28MLP%29%2C%20in%20the%20application%0Afor%20differentially%20private%20model%20training.%20Using%20the%20DP-SGD%20algorithm%2C%20we%0Ademonstrate%20that%20KAN%20can%20be%20made%20private%20in%20a%20straightforward%20manner%20and%0Aevaluated%20its%20performance%20across%20several%20datasets.%20Our%20results%20indicate%20that%0Athe%20accuracy%20of%20KAN%20is%20not%20only%20comparable%20with%20MLP%20but%20also%20experiences%0Asimilar%20deterioration%20due%20to%20privacy%20constraints%2C%20making%20it%20suitable%20for%0Adifferentially%20private%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-KAN%253A%2520Differentially%2520Private%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DNikita%2520P.%2520Kalinin%2520and%2520Simone%2520Bombari%2520and%2520Hossein%2520Zakerinia%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520Kolmogorov-Arnold%2520Network%2520%2528KAN%2529%252C%2520recently%2520proposed%2520as%2520an%250Aalternative%2520to%2520the%2520classical%2520Multilayer%2520Perceptron%2520%2528MLP%2529%252C%2520in%2520the%2520application%250Afor%2520differentially%2520private%2520model%2520training.%2520Using%2520the%2520DP-SGD%2520algorithm%252C%2520we%250Ademonstrate%2520that%2520KAN%2520can%2520be%2520made%2520private%2520in%2520a%2520straightforward%2520manner%2520and%250Aevaluated%2520its%2520performance%2520across%2520several%2520datasets.%2520Our%2520results%2520indicate%2520that%250Athe%2520accuracy%2520of%2520KAN%2520is%2520not%2520only%2520comparable%2520with%2520MLP%2520but%2520also%2520experiences%250Asimilar%2520deterioration%2520due%2520to%2520privacy%2520constraints%252C%2520making%2520it%2520suitable%2520for%250Adifferentially%2520private%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-KAN%3A%20Differentially%20Private%20Kolmogorov-Arnold%20Networks&entry.906535625=Nikita%20P.%20Kalinin%20and%20Simone%20Bombari%20and%20Hossein%20Zakerinia%20and%20Christoph%20H.%20Lampert&entry.1292438233=%20%20We%20study%20the%20Kolmogorov-Arnold%20Network%20%28KAN%29%2C%20recently%20proposed%20as%20an%0Aalternative%20to%20the%20classical%20Multilayer%20Perceptron%20%28MLP%29%2C%20in%20the%20application%0Afor%20differentially%20private%20model%20training.%20Using%20the%20DP-SGD%20algorithm%2C%20we%0Ademonstrate%20that%20KAN%20can%20be%20made%20private%20in%20a%20straightforward%20manner%20and%0Aevaluated%20its%20performance%20across%20several%20datasets.%20Our%20results%20indicate%20that%0Athe%20accuracy%20of%20KAN%20is%20not%20only%20comparable%20with%20MLP%20but%20also%20experiences%0Asimilar%20deterioration%20due%20to%20privacy%20constraints%2C%20making%20it%20suitable%20for%0Adifferentially%20private%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12569v1&entry.124074799=Read"},
{"title": "Chaotic Hedging with Iterated Integrals and Neural Networks", "author": "Ariel Neufeld and Philipp Schmocker", "abstract": "  In this paper, we extend the Wiener-Ito chaos decomposition to the class of\ncontinuous semimartingales that are exponentially integrable, which includes in\nparticular affine and some polynomial diffusion processes. By omitting the\northogonality in the expansion, we are able to show that every $p$-integrable\nfunctional of the semimartingale, for $p \\in [1,\\infty)$, can be represented as\na sum of iterated integrals thereof. Using finitely many terms of this\nexpansion and (possibly random) neural networks for the integrands, whose\nparameters are learned in a machine learning setting, we show that every\nfinancial derivative can be approximated arbitrarily well in the $L^p$-sense.\nIn particular, for $p = 2$, we recover the optimal hedging strategy in the\nsense of quadratic hedging. Moreover, since the hedging strategy of the\napproximating option can be computed in closed form, we obtain an efficient\nalgorithm to approximately replicate any sufficiently integrable financial\nderivative within short runtime.\n", "link": "http://arxiv.org/abs/2209.10166v3", "date": "2024-07-17", "relevancy": 1.7243, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4484}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chaotic%20Hedging%20with%20Iterated%20Integrals%20and%20Neural%20Networks&body=Title%3A%20Chaotic%20Hedging%20with%20Iterated%20Integrals%20and%20Neural%20Networks%0AAuthor%3A%20Ariel%20Neufeld%20and%20Philipp%20Schmocker%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20extend%20the%20Wiener-Ito%20chaos%20decomposition%20to%20the%20class%20of%0Acontinuous%20semimartingales%20that%20are%20exponentially%20integrable%2C%20which%20includes%20in%0Aparticular%20affine%20and%20some%20polynomial%20diffusion%20processes.%20By%20omitting%20the%0Aorthogonality%20in%20the%20expansion%2C%20we%20are%20able%20to%20show%20that%20every%20%24p%24-integrable%0Afunctional%20of%20the%20semimartingale%2C%20for%20%24p%20%5Cin%20%5B1%2C%5Cinfty%29%24%2C%20can%20be%20represented%20as%0Aa%20sum%20of%20iterated%20integrals%20thereof.%20Using%20finitely%20many%20terms%20of%20this%0Aexpansion%20and%20%28possibly%20random%29%20neural%20networks%20for%20the%20integrands%2C%20whose%0Aparameters%20are%20learned%20in%20a%20machine%20learning%20setting%2C%20we%20show%20that%20every%0Afinancial%20derivative%20can%20be%20approximated%20arbitrarily%20well%20in%20the%20%24L%5Ep%24-sense.%0AIn%20particular%2C%20for%20%24p%20%3D%202%24%2C%20we%20recover%20the%20optimal%20hedging%20strategy%20in%20the%0Asense%20of%20quadratic%20hedging.%20Moreover%2C%20since%20the%20hedging%20strategy%20of%20the%0Aapproximating%20option%20can%20be%20computed%20in%20closed%20form%2C%20we%20obtain%20an%20efficient%0Aalgorithm%20to%20approximately%20replicate%20any%20sufficiently%20integrable%20financial%0Aderivative%20within%20short%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.10166v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChaotic%2520Hedging%2520with%2520Iterated%2520Integrals%2520and%2520Neural%2520Networks%26entry.906535625%3DAriel%2520Neufeld%2520and%2520Philipp%2520Schmocker%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520Wiener-Ito%2520chaos%2520decomposition%2520to%2520the%2520class%2520of%250Acontinuous%2520semimartingales%2520that%2520are%2520exponentially%2520integrable%252C%2520which%2520includes%2520in%250Aparticular%2520affine%2520and%2520some%2520polynomial%2520diffusion%2520processes.%2520By%2520omitting%2520the%250Aorthogonality%2520in%2520the%2520expansion%252C%2520we%2520are%2520able%2520to%2520show%2520that%2520every%2520%2524p%2524-integrable%250Afunctional%2520of%2520the%2520semimartingale%252C%2520for%2520%2524p%2520%255Cin%2520%255B1%252C%255Cinfty%2529%2524%252C%2520can%2520be%2520represented%2520as%250Aa%2520sum%2520of%2520iterated%2520integrals%2520thereof.%2520Using%2520finitely%2520many%2520terms%2520of%2520this%250Aexpansion%2520and%2520%2528possibly%2520random%2529%2520neural%2520networks%2520for%2520the%2520integrands%252C%2520whose%250Aparameters%2520are%2520learned%2520in%2520a%2520machine%2520learning%2520setting%252C%2520we%2520show%2520that%2520every%250Afinancial%2520derivative%2520can%2520be%2520approximated%2520arbitrarily%2520well%2520in%2520the%2520%2524L%255Ep%2524-sense.%250AIn%2520particular%252C%2520for%2520%2524p%2520%253D%25202%2524%252C%2520we%2520recover%2520the%2520optimal%2520hedging%2520strategy%2520in%2520the%250Asense%2520of%2520quadratic%2520hedging.%2520Moreover%252C%2520since%2520the%2520hedging%2520strategy%2520of%2520the%250Aapproximating%2520option%2520can%2520be%2520computed%2520in%2520closed%2520form%252C%2520we%2520obtain%2520an%2520efficient%250Aalgorithm%2520to%2520approximately%2520replicate%2520any%2520sufficiently%2520integrable%2520financial%250Aderivative%2520within%2520short%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.10166v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chaotic%20Hedging%20with%20Iterated%20Integrals%20and%20Neural%20Networks&entry.906535625=Ariel%20Neufeld%20and%20Philipp%20Schmocker&entry.1292438233=%20%20In%20this%20paper%2C%20we%20extend%20the%20Wiener-Ito%20chaos%20decomposition%20to%20the%20class%20of%0Acontinuous%20semimartingales%20that%20are%20exponentially%20integrable%2C%20which%20includes%20in%0Aparticular%20affine%20and%20some%20polynomial%20diffusion%20processes.%20By%20omitting%20the%0Aorthogonality%20in%20the%20expansion%2C%20we%20are%20able%20to%20show%20that%20every%20%24p%24-integrable%0Afunctional%20of%20the%20semimartingale%2C%20for%20%24p%20%5Cin%20%5B1%2C%5Cinfty%29%24%2C%20can%20be%20represented%20as%0Aa%20sum%20of%20iterated%20integrals%20thereof.%20Using%20finitely%20many%20terms%20of%20this%0Aexpansion%20and%20%28possibly%20random%29%20neural%20networks%20for%20the%20integrands%2C%20whose%0Aparameters%20are%20learned%20in%20a%20machine%20learning%20setting%2C%20we%20show%20that%20every%0Afinancial%20derivative%20can%20be%20approximated%20arbitrarily%20well%20in%20the%20%24L%5Ep%24-sense.%0AIn%20particular%2C%20for%20%24p%20%3D%202%24%2C%20we%20recover%20the%20optimal%20hedging%20strategy%20in%20the%0Asense%20of%20quadratic%20hedging.%20Moreover%2C%20since%20the%20hedging%20strategy%20of%20the%0Aapproximating%20option%20can%20be%20computed%20in%20closed%20form%2C%20we%20obtain%20an%20efficient%0Aalgorithm%20to%20approximately%20replicate%20any%20sufficiently%20integrable%20financial%0Aderivative%20within%20short%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.10166v3&entry.124074799=Read"},
{"title": "LookupViT: Compressing visual information to a limited number of tokens", "author": "Rajat Koner and Gagan Jain and Prateek Jain and Volker Tresp and Sujoy Paul", "abstract": "  Vision Transformers (ViT) have emerged as the de-facto choice for numerous\nindustry grade vision solutions. But their inference cost can be prohibitive\nfor many settings, as they compute self-attention in each layer which suffers\nfrom quadratic computational complexity in the number of tokens. On the other\nhand, spatial information in images and spatio-temporal information in videos\nis usually sparse and redundant. In this work, we introduce LookupViT, that\naims to exploit this information sparsity to reduce ViT inference cost.\nLookupViT provides a novel general purpose vision transformer block that\noperates by compressing information from higher resolution tokens to a fixed\nnumber of tokens. These few compressed tokens undergo meticulous processing,\nwhile the higher-resolution tokens are passed through computationally cheaper\nlayers. Information sharing between these two token sets is enabled through a\nbidirectional cross-attention mechanism. The approach offers multiple\nadvantages - (a) easy to implement on standard ML accelerators (GPUs/TPUs) via\nstandard high-level operators, (b) applicable to standard ViT and its variants,\nthus generalizes to various tasks, (c) can handle different tokenization and\nattention approaches. LookupViT also offers flexibility for the compressed\ntokens, enabling performance-computation trade-offs in a single trained model.\nWe show LookupViT's effectiveness on multiple domains - (a) for\nimage-classification (ImageNet-1K and ImageNet-21K), (b) video classification\n(Kinetics400 and Something-Something V2), (c) image captioning (COCO-Captions)\nwith a frozen encoder. LookupViT provides $2\\times$ reduction in FLOPs while\nupholding or improving accuracy across these domains. In addition, LookupViT\nalso demonstrates out-of-the-box robustness and generalization on image\nclassification (ImageNet-C,R,A,O), improving by up to $4\\%$ over ViT.\n", "link": "http://arxiv.org/abs/2407.12753v1", "date": "2024-07-17", "relevancy": 1.7081, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5933}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5395}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LookupViT%3A%20Compressing%20visual%20information%20to%20a%20limited%20number%20of%20tokens&body=Title%3A%20LookupViT%3A%20Compressing%20visual%20information%20to%20a%20limited%20number%20of%20tokens%0AAuthor%3A%20Rajat%20Koner%20and%20Gagan%20Jain%20and%20Prateek%20Jain%20and%20Volker%20Tresp%20and%20Sujoy%20Paul%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViT%29%20have%20emerged%20as%20the%20de-facto%20choice%20for%20numerous%0Aindustry%20grade%20vision%20solutions.%20But%20their%20inference%20cost%20can%20be%20prohibitive%0Afor%20many%20settings%2C%20as%20they%20compute%20self-attention%20in%20each%20layer%20which%20suffers%0Afrom%20quadratic%20computational%20complexity%20in%20the%20number%20of%20tokens.%20On%20the%20other%0Ahand%2C%20spatial%20information%20in%20images%20and%20spatio-temporal%20information%20in%20videos%0Ais%20usually%20sparse%20and%20redundant.%20In%20this%20work%2C%20we%20introduce%20LookupViT%2C%20that%0Aaims%20to%20exploit%20this%20information%20sparsity%20to%20reduce%20ViT%20inference%20cost.%0ALookupViT%20provides%20a%20novel%20general%20purpose%20vision%20transformer%20block%20that%0Aoperates%20by%20compressing%20information%20from%20higher%20resolution%20tokens%20to%20a%20fixed%0Anumber%20of%20tokens.%20These%20few%20compressed%20tokens%20undergo%20meticulous%20processing%2C%0Awhile%20the%20higher-resolution%20tokens%20are%20passed%20through%20computationally%20cheaper%0Alayers.%20Information%20sharing%20between%20these%20two%20token%20sets%20is%20enabled%20through%20a%0Abidirectional%20cross-attention%20mechanism.%20The%20approach%20offers%20multiple%0Aadvantages%20-%20%28a%29%20easy%20to%20implement%20on%20standard%20ML%20accelerators%20%28GPUs/TPUs%29%20via%0Astandard%20high-level%20operators%2C%20%28b%29%20applicable%20to%20standard%20ViT%20and%20its%20variants%2C%0Athus%20generalizes%20to%20various%20tasks%2C%20%28c%29%20can%20handle%20different%20tokenization%20and%0Aattention%20approaches.%20LookupViT%20also%20offers%20flexibility%20for%20the%20compressed%0Atokens%2C%20enabling%20performance-computation%20trade-offs%20in%20a%20single%20trained%20model.%0AWe%20show%20LookupViT%27s%20effectiveness%20on%20multiple%20domains%20-%20%28a%29%20for%0Aimage-classification%20%28ImageNet-1K%20and%20ImageNet-21K%29%2C%20%28b%29%20video%20classification%0A%28Kinetics400%20and%20Something-Something%20V2%29%2C%20%28c%29%20image%20captioning%20%28COCO-Captions%29%0Awith%20a%20frozen%20encoder.%20LookupViT%20provides%20%242%5Ctimes%24%20reduction%20in%20FLOPs%20while%0Aupholding%20or%20improving%20accuracy%20across%20these%20domains.%20In%20addition%2C%20LookupViT%0Aalso%20demonstrates%20out-of-the-box%20robustness%20and%20generalization%20on%20image%0Aclassification%20%28ImageNet-C%2CR%2CA%2CO%29%2C%20improving%20by%20up%20to%20%244%5C%25%24%20over%20ViT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLookupViT%253A%2520Compressing%2520visual%2520information%2520to%2520a%2520limited%2520number%2520of%2520tokens%26entry.906535625%3DRajat%2520Koner%2520and%2520Gagan%2520Jain%2520and%2520Prateek%2520Jain%2520and%2520Volker%2520Tresp%2520and%2520Sujoy%2520Paul%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViT%2529%2520have%2520emerged%2520as%2520the%2520de-facto%2520choice%2520for%2520numerous%250Aindustry%2520grade%2520vision%2520solutions.%2520But%2520their%2520inference%2520cost%2520can%2520be%2520prohibitive%250Afor%2520many%2520settings%252C%2520as%2520they%2520compute%2520self-attention%2520in%2520each%2520layer%2520which%2520suffers%250Afrom%2520quadratic%2520computational%2520complexity%2520in%2520the%2520number%2520of%2520tokens.%2520On%2520the%2520other%250Ahand%252C%2520spatial%2520information%2520in%2520images%2520and%2520spatio-temporal%2520information%2520in%2520videos%250Ais%2520usually%2520sparse%2520and%2520redundant.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LookupViT%252C%2520that%250Aaims%2520to%2520exploit%2520this%2520information%2520sparsity%2520to%2520reduce%2520ViT%2520inference%2520cost.%250ALookupViT%2520provides%2520a%2520novel%2520general%2520purpose%2520vision%2520transformer%2520block%2520that%250Aoperates%2520by%2520compressing%2520information%2520from%2520higher%2520resolution%2520tokens%2520to%2520a%2520fixed%250Anumber%2520of%2520tokens.%2520These%2520few%2520compressed%2520tokens%2520undergo%2520meticulous%2520processing%252C%250Awhile%2520the%2520higher-resolution%2520tokens%2520are%2520passed%2520through%2520computationally%2520cheaper%250Alayers.%2520Information%2520sharing%2520between%2520these%2520two%2520token%2520sets%2520is%2520enabled%2520through%2520a%250Abidirectional%2520cross-attention%2520mechanism.%2520The%2520approach%2520offers%2520multiple%250Aadvantages%2520-%2520%2528a%2529%2520easy%2520to%2520implement%2520on%2520standard%2520ML%2520accelerators%2520%2528GPUs/TPUs%2529%2520via%250Astandard%2520high-level%2520operators%252C%2520%2528b%2529%2520applicable%2520to%2520standard%2520ViT%2520and%2520its%2520variants%252C%250Athus%2520generalizes%2520to%2520various%2520tasks%252C%2520%2528c%2529%2520can%2520handle%2520different%2520tokenization%2520and%250Aattention%2520approaches.%2520LookupViT%2520also%2520offers%2520flexibility%2520for%2520the%2520compressed%250Atokens%252C%2520enabling%2520performance-computation%2520trade-offs%2520in%2520a%2520single%2520trained%2520model.%250AWe%2520show%2520LookupViT%2527s%2520effectiveness%2520on%2520multiple%2520domains%2520-%2520%2528a%2529%2520for%250Aimage-classification%2520%2528ImageNet-1K%2520and%2520ImageNet-21K%2529%252C%2520%2528b%2529%2520video%2520classification%250A%2528Kinetics400%2520and%2520Something-Something%2520V2%2529%252C%2520%2528c%2529%2520image%2520captioning%2520%2528COCO-Captions%2529%250Awith%2520a%2520frozen%2520encoder.%2520LookupViT%2520provides%2520%25242%255Ctimes%2524%2520reduction%2520in%2520FLOPs%2520while%250Aupholding%2520or%2520improving%2520accuracy%2520across%2520these%2520domains.%2520In%2520addition%252C%2520LookupViT%250Aalso%2520demonstrates%2520out-of-the-box%2520robustness%2520and%2520generalization%2520on%2520image%250Aclassification%2520%2528ImageNet-C%252CR%252CA%252CO%2529%252C%2520improving%2520by%2520up%2520to%2520%25244%255C%2525%2524%2520over%2520ViT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LookupViT%3A%20Compressing%20visual%20information%20to%20a%20limited%20number%20of%20tokens&entry.906535625=Rajat%20Koner%20and%20Gagan%20Jain%20and%20Prateek%20Jain%20and%20Volker%20Tresp%20and%20Sujoy%20Paul&entry.1292438233=%20%20Vision%20Transformers%20%28ViT%29%20have%20emerged%20as%20the%20de-facto%20choice%20for%20numerous%0Aindustry%20grade%20vision%20solutions.%20But%20their%20inference%20cost%20can%20be%20prohibitive%0Afor%20many%20settings%2C%20as%20they%20compute%20self-attention%20in%20each%20layer%20which%20suffers%0Afrom%20quadratic%20computational%20complexity%20in%20the%20number%20of%20tokens.%20On%20the%20other%0Ahand%2C%20spatial%20information%20in%20images%20and%20spatio-temporal%20information%20in%20videos%0Ais%20usually%20sparse%20and%20redundant.%20In%20this%20work%2C%20we%20introduce%20LookupViT%2C%20that%0Aaims%20to%20exploit%20this%20information%20sparsity%20to%20reduce%20ViT%20inference%20cost.%0ALookupViT%20provides%20a%20novel%20general%20purpose%20vision%20transformer%20block%20that%0Aoperates%20by%20compressing%20information%20from%20higher%20resolution%20tokens%20to%20a%20fixed%0Anumber%20of%20tokens.%20These%20few%20compressed%20tokens%20undergo%20meticulous%20processing%2C%0Awhile%20the%20higher-resolution%20tokens%20are%20passed%20through%20computationally%20cheaper%0Alayers.%20Information%20sharing%20between%20these%20two%20token%20sets%20is%20enabled%20through%20a%0Abidirectional%20cross-attention%20mechanism.%20The%20approach%20offers%20multiple%0Aadvantages%20-%20%28a%29%20easy%20to%20implement%20on%20standard%20ML%20accelerators%20%28GPUs/TPUs%29%20via%0Astandard%20high-level%20operators%2C%20%28b%29%20applicable%20to%20standard%20ViT%20and%20its%20variants%2C%0Athus%20generalizes%20to%20various%20tasks%2C%20%28c%29%20can%20handle%20different%20tokenization%20and%0Aattention%20approaches.%20LookupViT%20also%20offers%20flexibility%20for%20the%20compressed%0Atokens%2C%20enabling%20performance-computation%20trade-offs%20in%20a%20single%20trained%20model.%0AWe%20show%20LookupViT%27s%20effectiveness%20on%20multiple%20domains%20-%20%28a%29%20for%0Aimage-classification%20%28ImageNet-1K%20and%20ImageNet-21K%29%2C%20%28b%29%20video%20classification%0A%28Kinetics400%20and%20Something-Something%20V2%29%2C%20%28c%29%20image%20captioning%20%28COCO-Captions%29%0Awith%20a%20frozen%20encoder.%20LookupViT%20provides%20%242%5Ctimes%24%20reduction%20in%20FLOPs%20while%0Aupholding%20or%20improving%20accuracy%20across%20these%20domains.%20In%20addition%2C%20LookupViT%0Aalso%20demonstrates%20out-of-the-box%20robustness%20and%20generalization%20on%20image%0Aclassification%20%28ImageNet-C%2CR%2CA%2CO%29%2C%20improving%20by%20up%20to%20%244%5C%25%24%20over%20ViT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12753v1&entry.124074799=Read"},
{"title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning\n  for Multi-Agent Coordination with Large Language Models", "author": "Xihe Qiu and Haoyu Wang and Xiaoyu Tan and Chao Qu and Yujie Xiong and Yuan Cheng and Yinghui Xu and Wei Chu and Yuan Qi", "abstract": "  Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.\n", "link": "http://arxiv.org/abs/2407.12532v1", "date": "2024-07-17", "relevancy": 1.5893, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5428}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5266}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Collaborative%20Intelligence%3A%20Propagating%20Intentions%20and%20Reasoning%0A%20%20for%20Multi-Agent%20Coordination%20with%20Large%20Language%20Models&body=Title%3A%20Towards%20Collaborative%20Intelligence%3A%20Propagating%20Intentions%20and%20Reasoning%0A%20%20for%20Multi-Agent%20Coordination%20with%20Large%20Language%20Models%0AAuthor%3A%20Xihe%20Qiu%20and%20Haoyu%20Wang%20and%20Xiaoyu%20Tan%20and%20Chao%20Qu%20and%20Yujie%20Xiong%20and%20Yuan%20Cheng%20and%20Yinghui%20Xu%20and%20Wei%20Chu%20and%20Yuan%20Qi%0AAbstract%3A%20%20%20Effective%20collaboration%20in%20multi-agent%20systems%20requires%20communicating%20goals%0Aand%20intentions%20between%20agents.%20Current%20agent%20frameworks%20often%20suffer%20from%0Adependencies%20on%20single-agent%20execution%20and%20lack%20robust%20inter-module%0Acommunication%2C%20frequently%20leading%20to%20suboptimal%20multi-agent%20reinforcement%0Alearning%20%28MARL%29%20policies%20and%20inadequate%20task%20coordination.%20To%20address%20these%0Achallenges%2C%20we%20present%20a%20framework%20for%20training%20large%20language%20models%20%28LLMs%29%20as%0Acollaborative%20agents%20to%20enable%20coordinated%20behaviors%20in%20cooperative%20MARL.%20Each%0Aagent%20maintains%20a%20private%20intention%20consisting%20of%20its%20current%20goal%20and%0Aassociated%20sub-tasks.%20Agents%20broadcast%20their%20intentions%20periodically%2C%20allowing%0Aother%20agents%20to%20infer%20coordination%20tasks.%20A%20propagation%20network%20transforms%0Abroadcast%20intentions%20into%20teammate-specific%20communication%20messages%2C%20sharing%0Arelevant%20goals%20with%20designated%20teammates.%20The%20architecture%20of%20our%20framework%20is%0Astructured%20into%20planning%2C%20grounding%2C%20and%20execution%20modules.%20During%20execution%2C%0Amultiple%20agents%20interact%20in%20a%20downstream%20environment%20and%20communicate%20intentions%0Ato%20enable%20coordinated%20behaviors.%20The%20grounding%20module%20dynamically%20adapts%0Acomprehension%20strategies%20based%20on%20emerging%20coordination%20patterns%2C%20while%0Afeedback%20from%20execution%20agents%20influnces%20the%20planning%20module%2C%20enabling%20the%0Adynamic%20re-planning%20of%20sub-tasks.%20Results%20in%20collaborative%20environment%0Asimulation%20demonstrate%20intention%20propagation%20reduces%20miscoordination%20errors%20by%0Aaligning%20sub-task%20dependencies%20between%20agents.%20Agents%20learn%20when%20to%20communicate%0Aintentions%20and%20which%20teammates%20require%20task%20details%2C%20resulting%20in%20emergent%0Acoordinated%20behaviors.%20This%20demonstrates%20the%20efficacy%20of%20intention%20sharing%20for%0Acooperative%20multi-agent%20RL%20based%20on%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Collaborative%2520Intelligence%253A%2520Propagating%2520Intentions%2520and%2520Reasoning%250A%2520%2520for%2520Multi-Agent%2520Coordination%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DXihe%2520Qiu%2520and%2520Haoyu%2520Wang%2520and%2520Xiaoyu%2520Tan%2520and%2520Chao%2520Qu%2520and%2520Yujie%2520Xiong%2520and%2520Yuan%2520Cheng%2520and%2520Yinghui%2520Xu%2520and%2520Wei%2520Chu%2520and%2520Yuan%2520Qi%26entry.1292438233%3D%2520%2520Effective%2520collaboration%2520in%2520multi-agent%2520systems%2520requires%2520communicating%2520goals%250Aand%2520intentions%2520between%2520agents.%2520Current%2520agent%2520frameworks%2520often%2520suffer%2520from%250Adependencies%2520on%2520single-agent%2520execution%2520and%2520lack%2520robust%2520inter-module%250Acommunication%252C%2520frequently%2520leading%2520to%2520suboptimal%2520multi-agent%2520reinforcement%250Alearning%2520%2528MARL%2529%2520policies%2520and%2520inadequate%2520task%2520coordination.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520present%2520a%2520framework%2520for%2520training%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%250Acollaborative%2520agents%2520to%2520enable%2520coordinated%2520behaviors%2520in%2520cooperative%2520MARL.%2520Each%250Aagent%2520maintains%2520a%2520private%2520intention%2520consisting%2520of%2520its%2520current%2520goal%2520and%250Aassociated%2520sub-tasks.%2520Agents%2520broadcast%2520their%2520intentions%2520periodically%252C%2520allowing%250Aother%2520agents%2520to%2520infer%2520coordination%2520tasks.%2520A%2520propagation%2520network%2520transforms%250Abroadcast%2520intentions%2520into%2520teammate-specific%2520communication%2520messages%252C%2520sharing%250Arelevant%2520goals%2520with%2520designated%2520teammates.%2520The%2520architecture%2520of%2520our%2520framework%2520is%250Astructured%2520into%2520planning%252C%2520grounding%252C%2520and%2520execution%2520modules.%2520During%2520execution%252C%250Amultiple%2520agents%2520interact%2520in%2520a%2520downstream%2520environment%2520and%2520communicate%2520intentions%250Ato%2520enable%2520coordinated%2520behaviors.%2520The%2520grounding%2520module%2520dynamically%2520adapts%250Acomprehension%2520strategies%2520based%2520on%2520emerging%2520coordination%2520patterns%252C%2520while%250Afeedback%2520from%2520execution%2520agents%2520influnces%2520the%2520planning%2520module%252C%2520enabling%2520the%250Adynamic%2520re-planning%2520of%2520sub-tasks.%2520Results%2520in%2520collaborative%2520environment%250Asimulation%2520demonstrate%2520intention%2520propagation%2520reduces%2520miscoordination%2520errors%2520by%250Aaligning%2520sub-task%2520dependencies%2520between%2520agents.%2520Agents%2520learn%2520when%2520to%2520communicate%250Aintentions%2520and%2520which%2520teammates%2520require%2520task%2520details%252C%2520resulting%2520in%2520emergent%250Acoordinated%2520behaviors.%2520This%2520demonstrates%2520the%2520efficacy%2520of%2520intention%2520sharing%2520for%250Acooperative%2520multi-agent%2520RL%2520based%2520on%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Collaborative%20Intelligence%3A%20Propagating%20Intentions%20and%20Reasoning%0A%20%20for%20Multi-Agent%20Coordination%20with%20Large%20Language%20Models&entry.906535625=Xihe%20Qiu%20and%20Haoyu%20Wang%20and%20Xiaoyu%20Tan%20and%20Chao%20Qu%20and%20Yujie%20Xiong%20and%20Yuan%20Cheng%20and%20Yinghui%20Xu%20and%20Wei%20Chu%20and%20Yuan%20Qi&entry.1292438233=%20%20Effective%20collaboration%20in%20multi-agent%20systems%20requires%20communicating%20goals%0Aand%20intentions%20between%20agents.%20Current%20agent%20frameworks%20often%20suffer%20from%0Adependencies%20on%20single-agent%20execution%20and%20lack%20robust%20inter-module%0Acommunication%2C%20frequently%20leading%20to%20suboptimal%20multi-agent%20reinforcement%0Alearning%20%28MARL%29%20policies%20and%20inadequate%20task%20coordination.%20To%20address%20these%0Achallenges%2C%20we%20present%20a%20framework%20for%20training%20large%20language%20models%20%28LLMs%29%20as%0Acollaborative%20agents%20to%20enable%20coordinated%20behaviors%20in%20cooperative%20MARL.%20Each%0Aagent%20maintains%20a%20private%20intention%20consisting%20of%20its%20current%20goal%20and%0Aassociated%20sub-tasks.%20Agents%20broadcast%20their%20intentions%20periodically%2C%20allowing%0Aother%20agents%20to%20infer%20coordination%20tasks.%20A%20propagation%20network%20transforms%0Abroadcast%20intentions%20into%20teammate-specific%20communication%20messages%2C%20sharing%0Arelevant%20goals%20with%20designated%20teammates.%20The%20architecture%20of%20our%20framework%20is%0Astructured%20into%20planning%2C%20grounding%2C%20and%20execution%20modules.%20During%20execution%2C%0Amultiple%20agents%20interact%20in%20a%20downstream%20environment%20and%20communicate%20intentions%0Ato%20enable%20coordinated%20behaviors.%20The%20grounding%20module%20dynamically%20adapts%0Acomprehension%20strategies%20based%20on%20emerging%20coordination%20patterns%2C%20while%0Afeedback%20from%20execution%20agents%20influnces%20the%20planning%20module%2C%20enabling%20the%0Adynamic%20re-planning%20of%20sub-tasks.%20Results%20in%20collaborative%20environment%0Asimulation%20demonstrate%20intention%20propagation%20reduces%20miscoordination%20errors%20by%0Aaligning%20sub-task%20dependencies%20between%20agents.%20Agents%20learn%20when%20to%20communicate%0Aintentions%20and%20which%20teammates%20require%20task%20details%2C%20resulting%20in%20emergent%0Acoordinated%20behaviors.%20This%20demonstrates%20the%20efficacy%20of%20intention%20sharing%20for%0Acooperative%20multi-agent%20RL%20based%20on%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12532v1&entry.124074799=Read"},
{"title": "Non-Vacuous Generalization Bounds for Large Language Models", "author": "Sanae Lotfi and Marc Finzi and Yilun Kuang and Tim G. J. Rudner and Micah Goldblum and Andrew Gordon Wilson", "abstract": "  Modern language models can contain billions of parameters, raising the\nquestion of whether they can generalize beyond the training data or simply\nparrot their training corpora. We provide the first non-vacuous generalization\nbounds for pretrained large language models (LLMs), indicating that language\nmodels are capable of discovering regularities that generalize to unseen data.\nIn particular, we derive a compression bound that is valid for the unbounded\nlog-likelihood loss using prediction smoothing, and we extend the bound to\nhandle subsampling, accelerating bound computation by orders of magnitude on\nmassive datasets. To achieve the extreme level of compression required for\nnon-vacuous bounds, we devise SubLoRA, a simple low-dimensional nonlinear\nparameterization that leads to non-vacuous generalization bounds for models\nwith nearly a billion parameters. Finally, we use our bounds to understand LLM\ngeneralization and find that larger models have better generalization bounds\nand are more compressible than smaller models.\n", "link": "http://arxiv.org/abs/2312.17173v3", "date": "2024-07-17", "relevancy": 1.3971, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4901}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4594}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Vacuous%20Generalization%20Bounds%20for%20Large%20Language%20Models&body=Title%3A%20Non-Vacuous%20Generalization%20Bounds%20for%20Large%20Language%20Models%0AAuthor%3A%20Sanae%20Lotfi%20and%20Marc%20Finzi%20and%20Yilun%20Kuang%20and%20Tim%20G.%20J.%20Rudner%20and%20Micah%20Goldblum%20and%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20Modern%20language%20models%20can%20contain%20billions%20of%20parameters%2C%20raising%20the%0Aquestion%20of%20whether%20they%20can%20generalize%20beyond%20the%20training%20data%20or%20simply%0Aparrot%20their%20training%20corpora.%20We%20provide%20the%20first%20non-vacuous%20generalization%0Abounds%20for%20pretrained%20large%20language%20models%20%28LLMs%29%2C%20indicating%20that%20language%0Amodels%20are%20capable%20of%20discovering%20regularities%20that%20generalize%20to%20unseen%20data.%0AIn%20particular%2C%20we%20derive%20a%20compression%20bound%20that%20is%20valid%20for%20the%20unbounded%0Alog-likelihood%20loss%20using%20prediction%20smoothing%2C%20and%20we%20extend%20the%20bound%20to%0Ahandle%20subsampling%2C%20accelerating%20bound%20computation%20by%20orders%20of%20magnitude%20on%0Amassive%20datasets.%20To%20achieve%20the%20extreme%20level%20of%20compression%20required%20for%0Anon-vacuous%20bounds%2C%20we%20devise%20SubLoRA%2C%20a%20simple%20low-dimensional%20nonlinear%0Aparameterization%20that%20leads%20to%20non-vacuous%20generalization%20bounds%20for%20models%0Awith%20nearly%20a%20billion%20parameters.%20Finally%2C%20we%20use%20our%20bounds%20to%20understand%20LLM%0Ageneralization%20and%20find%20that%20larger%20models%20have%20better%20generalization%20bounds%0Aand%20are%20more%20compressible%20than%20smaller%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17173v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Vacuous%2520Generalization%2520Bounds%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DSanae%2520Lotfi%2520and%2520Marc%2520Finzi%2520and%2520Yilun%2520Kuang%2520and%2520Tim%2520G.%2520J.%2520Rudner%2520and%2520Micah%2520Goldblum%2520and%2520Andrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520Modern%2520language%2520models%2520can%2520contain%2520billions%2520of%2520parameters%252C%2520raising%2520the%250Aquestion%2520of%2520whether%2520they%2520can%2520generalize%2520beyond%2520the%2520training%2520data%2520or%2520simply%250Aparrot%2520their%2520training%2520corpora.%2520We%2520provide%2520the%2520first%2520non-vacuous%2520generalization%250Abounds%2520for%2520pretrained%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520indicating%2520that%2520language%250Amodels%2520are%2520capable%2520of%2520discovering%2520regularities%2520that%2520generalize%2520to%2520unseen%2520data.%250AIn%2520particular%252C%2520we%2520derive%2520a%2520compression%2520bound%2520that%2520is%2520valid%2520for%2520the%2520unbounded%250Alog-likelihood%2520loss%2520using%2520prediction%2520smoothing%252C%2520and%2520we%2520extend%2520the%2520bound%2520to%250Ahandle%2520subsampling%252C%2520accelerating%2520bound%2520computation%2520by%2520orders%2520of%2520magnitude%2520on%250Amassive%2520datasets.%2520To%2520achieve%2520the%2520extreme%2520level%2520of%2520compression%2520required%2520for%250Anon-vacuous%2520bounds%252C%2520we%2520devise%2520SubLoRA%252C%2520a%2520simple%2520low-dimensional%2520nonlinear%250Aparameterization%2520that%2520leads%2520to%2520non-vacuous%2520generalization%2520bounds%2520for%2520models%250Awith%2520nearly%2520a%2520billion%2520parameters.%2520Finally%252C%2520we%2520use%2520our%2520bounds%2520to%2520understand%2520LLM%250Ageneralization%2520and%2520find%2520that%2520larger%2520models%2520have%2520better%2520generalization%2520bounds%250Aand%2520are%2520more%2520compressible%2520than%2520smaller%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17173v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Vacuous%20Generalization%20Bounds%20for%20Large%20Language%20Models&entry.906535625=Sanae%20Lotfi%20and%20Marc%20Finzi%20and%20Yilun%20Kuang%20and%20Tim%20G.%20J.%20Rudner%20and%20Micah%20Goldblum%20and%20Andrew%20Gordon%20Wilson&entry.1292438233=%20%20Modern%20language%20models%20can%20contain%20billions%20of%20parameters%2C%20raising%20the%0Aquestion%20of%20whether%20they%20can%20generalize%20beyond%20the%20training%20data%20or%20simply%0Aparrot%20their%20training%20corpora.%20We%20provide%20the%20first%20non-vacuous%20generalization%0Abounds%20for%20pretrained%20large%20language%20models%20%28LLMs%29%2C%20indicating%20that%20language%0Amodels%20are%20capable%20of%20discovering%20regularities%20that%20generalize%20to%20unseen%20data.%0AIn%20particular%2C%20we%20derive%20a%20compression%20bound%20that%20is%20valid%20for%20the%20unbounded%0Alog-likelihood%20loss%20using%20prediction%20smoothing%2C%20and%20we%20extend%20the%20bound%20to%0Ahandle%20subsampling%2C%20accelerating%20bound%20computation%20by%20orders%20of%20magnitude%20on%0Amassive%20datasets.%20To%20achieve%20the%20extreme%20level%20of%20compression%20required%20for%0Anon-vacuous%20bounds%2C%20we%20devise%20SubLoRA%2C%20a%20simple%20low-dimensional%20nonlinear%0Aparameterization%20that%20leads%20to%20non-vacuous%20generalization%20bounds%20for%20models%0Awith%20nearly%20a%20billion%20parameters.%20Finally%2C%20we%20use%20our%20bounds%20to%20understand%20LLM%0Ageneralization%20and%20find%20that%20larger%20models%20have%20better%20generalization%20bounds%0Aand%20are%20more%20compressible%20than%20smaller%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17173v3&entry.124074799=Read"},
{"title": "Optimal Control for Clutched-Elastic Robots: A Contact-Implicit Approach", "author": "Dennis Ossadnik and Vasilije Rak\u010devi\u0107 and Mehmet C. Yildirim and Edmundo Pozo Fortuni\u0107 and Hugo T. M. Kussaba and Abdalla Swikir and Sami Haddadin", "abstract": "  Intrinsically elastic robots surpass their rigid counterparts in a range of\ndifferent characteristics. By temporarily storing potential energy and\nsubsequently converting it to kinetic energy, elastic robots are capable of\nhighly dynamic motions even with limited motor power. However, the\ntime-dependency of this energy storage and release mechanism remains one of the\nmajor challenges in controlling elastic robots. A possible remedy is the\nintroduction of locking elements (i.e. clutches and brakes) in the drive train.\nThis gives rise to a new class of robots, so-called clutched-elastic robots\n(CER), with which it is possible to precisely control the energy-transfer\ntiming. A prevalent challenge in the realm of CERs is the automatic discovery\nof clutch sequences. Due to complexity, many methods still rely on pre-defined\nmodes. In this paper, we introduce a novel contact-implicit scheme designed to\noptimize both control input and clutch sequence simultaneously. A penalty in\nthe objective function ensures the prevention of unnecessary clutch\ntransitions. We empirically demonstrate the effectiveness of our proposed\nmethod on a double pendulum equipped with two of our newly proposed\nclutch-based Bi-Stiffness Actuators (BSA).\n", "link": "http://arxiv.org/abs/2407.12655v1", "date": "2024-07-17", "relevancy": 1.4367, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4985}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4984}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Control%20for%20Clutched-Elastic%20Robots%3A%20A%20Contact-Implicit%20Approach&body=Title%3A%20Optimal%20Control%20for%20Clutched-Elastic%20Robots%3A%20A%20Contact-Implicit%20Approach%0AAuthor%3A%20Dennis%20Ossadnik%20and%20Vasilije%20Rak%C4%8Devi%C4%87%20and%20Mehmet%20C.%20Yildirim%20and%20Edmundo%20Pozo%20Fortuni%C4%87%20and%20Hugo%20T.%20M.%20Kussaba%20and%20Abdalla%20Swikir%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20Intrinsically%20elastic%20robots%20surpass%20their%20rigid%20counterparts%20in%20a%20range%20of%0Adifferent%20characteristics.%20By%20temporarily%20storing%20potential%20energy%20and%0Asubsequently%20converting%20it%20to%20kinetic%20energy%2C%20elastic%20robots%20are%20capable%20of%0Ahighly%20dynamic%20motions%20even%20with%20limited%20motor%20power.%20However%2C%20the%0Atime-dependency%20of%20this%20energy%20storage%20and%20release%20mechanism%20remains%20one%20of%20the%0Amajor%20challenges%20in%20controlling%20elastic%20robots.%20A%20possible%20remedy%20is%20the%0Aintroduction%20of%20locking%20elements%20%28i.e.%20clutches%20and%20brakes%29%20in%20the%20drive%20train.%0AThis%20gives%20rise%20to%20a%20new%20class%20of%20robots%2C%20so-called%20clutched-elastic%20robots%0A%28CER%29%2C%20with%20which%20it%20is%20possible%20to%20precisely%20control%20the%20energy-transfer%0Atiming.%20A%20prevalent%20challenge%20in%20the%20realm%20of%20CERs%20is%20the%20automatic%20discovery%0Aof%20clutch%20sequences.%20Due%20to%20complexity%2C%20many%20methods%20still%20rely%20on%20pre-defined%0Amodes.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20contact-implicit%20scheme%20designed%20to%0Aoptimize%20both%20control%20input%20and%20clutch%20sequence%20simultaneously.%20A%20penalty%20in%0Athe%20objective%20function%20ensures%20the%20prevention%20of%20unnecessary%20clutch%0Atransitions.%20We%20empirically%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amethod%20on%20a%20double%20pendulum%20equipped%20with%20two%20of%20our%20newly%20proposed%0Aclutch-based%20Bi-Stiffness%20Actuators%20%28BSA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Control%2520for%2520Clutched-Elastic%2520Robots%253A%2520A%2520Contact-Implicit%2520Approach%26entry.906535625%3DDennis%2520Ossadnik%2520and%2520Vasilije%2520Rak%25C4%258Devi%25C4%2587%2520and%2520Mehmet%2520C.%2520Yildirim%2520and%2520Edmundo%2520Pozo%2520Fortuni%25C4%2587%2520and%2520Hugo%2520T.%2520M.%2520Kussaba%2520and%2520Abdalla%2520Swikir%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520Intrinsically%2520elastic%2520robots%2520surpass%2520their%2520rigid%2520counterparts%2520in%2520a%2520range%2520of%250Adifferent%2520characteristics.%2520By%2520temporarily%2520storing%2520potential%2520energy%2520and%250Asubsequently%2520converting%2520it%2520to%2520kinetic%2520energy%252C%2520elastic%2520robots%2520are%2520capable%2520of%250Ahighly%2520dynamic%2520motions%2520even%2520with%2520limited%2520motor%2520power.%2520However%252C%2520the%250Atime-dependency%2520of%2520this%2520energy%2520storage%2520and%2520release%2520mechanism%2520remains%2520one%2520of%2520the%250Amajor%2520challenges%2520in%2520controlling%2520elastic%2520robots.%2520A%2520possible%2520remedy%2520is%2520the%250Aintroduction%2520of%2520locking%2520elements%2520%2528i.e.%2520clutches%2520and%2520brakes%2529%2520in%2520the%2520drive%2520train.%250AThis%2520gives%2520rise%2520to%2520a%2520new%2520class%2520of%2520robots%252C%2520so-called%2520clutched-elastic%2520robots%250A%2528CER%2529%252C%2520with%2520which%2520it%2520is%2520possible%2520to%2520precisely%2520control%2520the%2520energy-transfer%250Atiming.%2520A%2520prevalent%2520challenge%2520in%2520the%2520realm%2520of%2520CERs%2520is%2520the%2520automatic%2520discovery%250Aof%2520clutch%2520sequences.%2520Due%2520to%2520complexity%252C%2520many%2520methods%2520still%2520rely%2520on%2520pre-defined%250Amodes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520contact-implicit%2520scheme%2520designed%2520to%250Aoptimize%2520both%2520control%2520input%2520and%2520clutch%2520sequence%2520simultaneously.%2520A%2520penalty%2520in%250Athe%2520objective%2520function%2520ensures%2520the%2520prevention%2520of%2520unnecessary%2520clutch%250Atransitions.%2520We%2520empirically%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Amethod%2520on%2520a%2520double%2520pendulum%2520equipped%2520with%2520two%2520of%2520our%2520newly%2520proposed%250Aclutch-based%2520Bi-Stiffness%2520Actuators%2520%2528BSA%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Control%20for%20Clutched-Elastic%20Robots%3A%20A%20Contact-Implicit%20Approach&entry.906535625=Dennis%20Ossadnik%20and%20Vasilije%20Rak%C4%8Devi%C4%87%20and%20Mehmet%20C.%20Yildirim%20and%20Edmundo%20Pozo%20Fortuni%C4%87%20and%20Hugo%20T.%20M.%20Kussaba%20and%20Abdalla%20Swikir%20and%20Sami%20Haddadin&entry.1292438233=%20%20Intrinsically%20elastic%20robots%20surpass%20their%20rigid%20counterparts%20in%20a%20range%20of%0Adifferent%20characteristics.%20By%20temporarily%20storing%20potential%20energy%20and%0Asubsequently%20converting%20it%20to%20kinetic%20energy%2C%20elastic%20robots%20are%20capable%20of%0Ahighly%20dynamic%20motions%20even%20with%20limited%20motor%20power.%20However%2C%20the%0Atime-dependency%20of%20this%20energy%20storage%20and%20release%20mechanism%20remains%20one%20of%20the%0Amajor%20challenges%20in%20controlling%20elastic%20robots.%20A%20possible%20remedy%20is%20the%0Aintroduction%20of%20locking%20elements%20%28i.e.%20clutches%20and%20brakes%29%20in%20the%20drive%20train.%0AThis%20gives%20rise%20to%20a%20new%20class%20of%20robots%2C%20so-called%20clutched-elastic%20robots%0A%28CER%29%2C%20with%20which%20it%20is%20possible%20to%20precisely%20control%20the%20energy-transfer%0Atiming.%20A%20prevalent%20challenge%20in%20the%20realm%20of%20CERs%20is%20the%20automatic%20discovery%0Aof%20clutch%20sequences.%20Due%20to%20complexity%2C%20many%20methods%20still%20rely%20on%20pre-defined%0Amodes.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20contact-implicit%20scheme%20designed%20to%0Aoptimize%20both%20control%20input%20and%20clutch%20sequence%20simultaneously.%20A%20penalty%20in%0Athe%20objective%20function%20ensures%20the%20prevention%20of%20unnecessary%20clutch%0Atransitions.%20We%20empirically%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amethod%20on%20a%20double%20pendulum%20equipped%20with%20two%20of%20our%20newly%20proposed%0Aclutch-based%20Bi-Stiffness%20Actuators%20%28BSA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12655v1&entry.124074799=Read"},
{"title": "ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer\n  Neural Networks", "author": "Salma Afifi and Ishan Thakkar and Sudeep Pasricha", "abstract": "  Transformers have emerged as a powerful tool for natural language processing\n(NLP) and computer vision. Through the attention mechanism, these models have\nexhibited remarkable performance gains when compared to conventional approaches\nlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\nNevertheless, transformers typically demand substantial execution time due to\ntheir extensive computations and large memory footprint. Processing in-memory\n(PIM) and near-memory computing (NMC) are promising solutions to accelerating\ntransformers as they offer high compute parallelism and memory bandwidth.\nHowever, designing PIM/NMC architectures to support the complex operations and\nmassive amounts of data that need to be moved between layers in transformer\nneural networks remains a challenge. We propose ARTEMIS, a mixed\nanalog-stochastic in-DRAM accelerator for transformer models. Through employing\nminimal changes to the conventional DRAM arrays, ARTEMIS efficiently alleviates\nthe costs associated with transformer model execution by supporting stochastic\ncomputing for multiplications and temporal analog accumulations using a novel\nin-DRAM metal-on-metal capacitor. Our analysis indicates that ARTEMIS exhibits\nat least 3.0x speedup, 1.8x lower energy, and 1.9x better energy efficiency\ncompared to GPU, TPU, CPU, and state-of-the-art PIM transformer hardware\naccelerators.\n", "link": "http://arxiv.org/abs/2407.12638v1", "date": "2024-07-17", "relevancy": 1.5294, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.554}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARTEMIS%3A%20A%20Mixed%20Analog-Stochastic%20In-DRAM%20Accelerator%20for%20Transformer%0A%20%20Neural%20Networks&body=Title%3A%20ARTEMIS%3A%20A%20Mixed%20Analog-Stochastic%20In-DRAM%20Accelerator%20for%20Transformer%0A%20%20Neural%20Networks%0AAuthor%3A%20Salma%20Afifi%20and%20Ishan%20Thakkar%20and%20Sudeep%20Pasricha%0AAbstract%3A%20%20%20Transformers%20have%20emerged%20as%20a%20powerful%20tool%20for%20natural%20language%20processing%0A%28NLP%29%20and%20computer%20vision.%20Through%20the%20attention%20mechanism%2C%20these%20models%20have%0Aexhibited%20remarkable%20performance%20gains%20when%20compared%20to%20conventional%20approaches%0Alike%20recurrent%20neural%20networks%20%28RNNs%29%20and%20convolutional%20neural%20networks%20%28CNNs%29.%0ANevertheless%2C%20transformers%20typically%20demand%20substantial%20execution%20time%20due%20to%0Atheir%20extensive%20computations%20and%20large%20memory%20footprint.%20Processing%20in-memory%0A%28PIM%29%20and%20near-memory%20computing%20%28NMC%29%20are%20promising%20solutions%20to%20accelerating%0Atransformers%20as%20they%20offer%20high%20compute%20parallelism%20and%20memory%20bandwidth.%0AHowever%2C%20designing%20PIM/NMC%20architectures%20to%20support%20the%20complex%20operations%20and%0Amassive%20amounts%20of%20data%20that%20need%20to%20be%20moved%20between%20layers%20in%20transformer%0Aneural%20networks%20remains%20a%20challenge.%20We%20propose%20ARTEMIS%2C%20a%20mixed%0Aanalog-stochastic%20in-DRAM%20accelerator%20for%20transformer%20models.%20Through%20employing%0Aminimal%20changes%20to%20the%20conventional%20DRAM%20arrays%2C%20ARTEMIS%20efficiently%20alleviates%0Athe%20costs%20associated%20with%20transformer%20model%20execution%20by%20supporting%20stochastic%0Acomputing%20for%20multiplications%20and%20temporal%20analog%20accumulations%20using%20a%20novel%0Ain-DRAM%20metal-on-metal%20capacitor.%20Our%20analysis%20indicates%20that%20ARTEMIS%20exhibits%0Aat%20least%203.0x%20speedup%2C%201.8x%20lower%20energy%2C%20and%201.9x%20better%20energy%20efficiency%0Acompared%20to%20GPU%2C%20TPU%2C%20CPU%2C%20and%20state-of-the-art%20PIM%20transformer%20hardware%0Aaccelerators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARTEMIS%253A%2520A%2520Mixed%2520Analog-Stochastic%2520In-DRAM%2520Accelerator%2520for%2520Transformer%250A%2520%2520Neural%2520Networks%26entry.906535625%3DSalma%2520Afifi%2520and%2520Ishan%2520Thakkar%2520and%2520Sudeep%2520Pasricha%26entry.1292438233%3D%2520%2520Transformers%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520natural%2520language%2520processing%250A%2528NLP%2529%2520and%2520computer%2520vision.%2520Through%2520the%2520attention%2520mechanism%252C%2520these%2520models%2520have%250Aexhibited%2520remarkable%2520performance%2520gains%2520when%2520compared%2520to%2520conventional%2520approaches%250Alike%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520and%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529.%250ANevertheless%252C%2520transformers%2520typically%2520demand%2520substantial%2520execution%2520time%2520due%2520to%250Atheir%2520extensive%2520computations%2520and%2520large%2520memory%2520footprint.%2520Processing%2520in-memory%250A%2528PIM%2529%2520and%2520near-memory%2520computing%2520%2528NMC%2529%2520are%2520promising%2520solutions%2520to%2520accelerating%250Atransformers%2520as%2520they%2520offer%2520high%2520compute%2520parallelism%2520and%2520memory%2520bandwidth.%250AHowever%252C%2520designing%2520PIM/NMC%2520architectures%2520to%2520support%2520the%2520complex%2520operations%2520and%250Amassive%2520amounts%2520of%2520data%2520that%2520need%2520to%2520be%2520moved%2520between%2520layers%2520in%2520transformer%250Aneural%2520networks%2520remains%2520a%2520challenge.%2520We%2520propose%2520ARTEMIS%252C%2520a%2520mixed%250Aanalog-stochastic%2520in-DRAM%2520accelerator%2520for%2520transformer%2520models.%2520Through%2520employing%250Aminimal%2520changes%2520to%2520the%2520conventional%2520DRAM%2520arrays%252C%2520ARTEMIS%2520efficiently%2520alleviates%250Athe%2520costs%2520associated%2520with%2520transformer%2520model%2520execution%2520by%2520supporting%2520stochastic%250Acomputing%2520for%2520multiplications%2520and%2520temporal%2520analog%2520accumulations%2520using%2520a%2520novel%250Ain-DRAM%2520metal-on-metal%2520capacitor.%2520Our%2520analysis%2520indicates%2520that%2520ARTEMIS%2520exhibits%250Aat%2520least%25203.0x%2520speedup%252C%25201.8x%2520lower%2520energy%252C%2520and%25201.9x%2520better%2520energy%2520efficiency%250Acompared%2520to%2520GPU%252C%2520TPU%252C%2520CPU%252C%2520and%2520state-of-the-art%2520PIM%2520transformer%2520hardware%250Aaccelerators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARTEMIS%3A%20A%20Mixed%20Analog-Stochastic%20In-DRAM%20Accelerator%20for%20Transformer%0A%20%20Neural%20Networks&entry.906535625=Salma%20Afifi%20and%20Ishan%20Thakkar%20and%20Sudeep%20Pasricha&entry.1292438233=%20%20Transformers%20have%20emerged%20as%20a%20powerful%20tool%20for%20natural%20language%20processing%0A%28NLP%29%20and%20computer%20vision.%20Through%20the%20attention%20mechanism%2C%20these%20models%20have%0Aexhibited%20remarkable%20performance%20gains%20when%20compared%20to%20conventional%20approaches%0Alike%20recurrent%20neural%20networks%20%28RNNs%29%20and%20convolutional%20neural%20networks%20%28CNNs%29.%0ANevertheless%2C%20transformers%20typically%20demand%20substantial%20execution%20time%20due%20to%0Atheir%20extensive%20computations%20and%20large%20memory%20footprint.%20Processing%20in-memory%0A%28PIM%29%20and%20near-memory%20computing%20%28NMC%29%20are%20promising%20solutions%20to%20accelerating%0Atransformers%20as%20they%20offer%20high%20compute%20parallelism%20and%20memory%20bandwidth.%0AHowever%2C%20designing%20PIM/NMC%20architectures%20to%20support%20the%20complex%20operations%20and%0Amassive%20amounts%20of%20data%20that%20need%20to%20be%20moved%20between%20layers%20in%20transformer%0Aneural%20networks%20remains%20a%20challenge.%20We%20propose%20ARTEMIS%2C%20a%20mixed%0Aanalog-stochastic%20in-DRAM%20accelerator%20for%20transformer%20models.%20Through%20employing%0Aminimal%20changes%20to%20the%20conventional%20DRAM%20arrays%2C%20ARTEMIS%20efficiently%20alleviates%0Athe%20costs%20associated%20with%20transformer%20model%20execution%20by%20supporting%20stochastic%0Acomputing%20for%20multiplications%20and%20temporal%20analog%20accumulations%20using%20a%20novel%0Ain-DRAM%20metal-on-metal%20capacitor.%20Our%20analysis%20indicates%20that%20ARTEMIS%20exhibits%0Aat%20least%203.0x%20speedup%2C%201.8x%20lower%20energy%2C%20and%201.9x%20better%20energy%20efficiency%0Acompared%20to%20GPU%2C%20TPU%2C%20CPU%2C%20and%20state-of-the-art%20PIM%20transformer%20hardware%0Aaccelerators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12638v1&entry.124074799=Read"},
{"title": "Navigating the Smog: A Cooperative Multi-Agent RL for Accurate Air\n  Pollution Mapping through Data Assimilation", "author": "Ichrak Mokhtari and Walid Bechkit and Mohamed Sami Assenine and Herv\u00e9 Rivano", "abstract": "  The rapid rise of air pollution events necessitates accurate, real-time\nmonitoring for informed mitigation strategies. Data Assimilation (DA) methods\nprovide promising solutions, but their effectiveness hinges heavily on optimal\nmeasurement locations. This paper presents a novel approach for air quality\nmapping where autonomous drones, guided by a collaborative multi-agent\nreinforcement learning (MARL) framework, act as airborne detectives. Ditching\nthe limitations of static sensor networks, the drones engage in a synergistic\ninteraction, adapting their flight paths in real time to gather optimal data\nfor Data Assimilation (DA). Our approach employs a tailored reward function\nwith dynamic credit assignment, enabling drones to prioritize informative\nmeasurements without requiring unavailable ground truth data, making it\npractical for real-world deployments. Extensive experiments using a real-world\ndataset demonstrate that our solution achieves significantly improved pollution\nestimates, even with limited drone resources or limited prior knowledge of the\npollution plume. Beyond air quality, this solution unlocks possibilities for\ntackling diverse environmental challenges like wildfire detection and\nmanagement through scalable and autonomous drone cooperation.\n", "link": "http://arxiv.org/abs/2407.12539v1", "date": "2024-07-17", "relevancy": 1.6136, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5475}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.543}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20the%20Smog%3A%20A%20Cooperative%20Multi-Agent%20RL%20for%20Accurate%20Air%0A%20%20Pollution%20Mapping%20through%20Data%20Assimilation&body=Title%3A%20Navigating%20the%20Smog%3A%20A%20Cooperative%20Multi-Agent%20RL%20for%20Accurate%20Air%0A%20%20Pollution%20Mapping%20through%20Data%20Assimilation%0AAuthor%3A%20Ichrak%20Mokhtari%20and%20Walid%20Bechkit%20and%20Mohamed%20Sami%20Assenine%20and%20Herv%C3%A9%20Rivano%0AAbstract%3A%20%20%20The%20rapid%20rise%20of%20air%20pollution%20events%20necessitates%20accurate%2C%20real-time%0Amonitoring%20for%20informed%20mitigation%20strategies.%20Data%20Assimilation%20%28DA%29%20methods%0Aprovide%20promising%20solutions%2C%20but%20their%20effectiveness%20hinges%20heavily%20on%20optimal%0Ameasurement%20locations.%20This%20paper%20presents%20a%20novel%20approach%20for%20air%20quality%0Amapping%20where%20autonomous%20drones%2C%20guided%20by%20a%20collaborative%20multi-agent%0Areinforcement%20learning%20%28MARL%29%20framework%2C%20act%20as%20airborne%20detectives.%20Ditching%0Athe%20limitations%20of%20static%20sensor%20networks%2C%20the%20drones%20engage%20in%20a%20synergistic%0Ainteraction%2C%20adapting%20their%20flight%20paths%20in%20real%20time%20to%20gather%20optimal%20data%0Afor%20Data%20Assimilation%20%28DA%29.%20Our%20approach%20employs%20a%20tailored%20reward%20function%0Awith%20dynamic%20credit%20assignment%2C%20enabling%20drones%20to%20prioritize%20informative%0Ameasurements%20without%20requiring%20unavailable%20ground%20truth%20data%2C%20making%20it%0Apractical%20for%20real-world%20deployments.%20Extensive%20experiments%20using%20a%20real-world%0Adataset%20demonstrate%20that%20our%20solution%20achieves%20significantly%20improved%20pollution%0Aestimates%2C%20even%20with%20limited%20drone%20resources%20or%20limited%20prior%20knowledge%20of%20the%0Apollution%20plume.%20Beyond%20air%20quality%2C%20this%20solution%20unlocks%20possibilities%20for%0Atackling%20diverse%20environmental%20challenges%20like%20wildfire%20detection%20and%0Amanagement%20through%20scalable%20and%20autonomous%20drone%20cooperation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520the%2520Smog%253A%2520A%2520Cooperative%2520Multi-Agent%2520RL%2520for%2520Accurate%2520Air%250A%2520%2520Pollution%2520Mapping%2520through%2520Data%2520Assimilation%26entry.906535625%3DIchrak%2520Mokhtari%2520and%2520Walid%2520Bechkit%2520and%2520Mohamed%2520Sami%2520Assenine%2520and%2520Herv%25C3%25A9%2520Rivano%26entry.1292438233%3D%2520%2520The%2520rapid%2520rise%2520of%2520air%2520pollution%2520events%2520necessitates%2520accurate%252C%2520real-time%250Amonitoring%2520for%2520informed%2520mitigation%2520strategies.%2520Data%2520Assimilation%2520%2528DA%2529%2520methods%250Aprovide%2520promising%2520solutions%252C%2520but%2520their%2520effectiveness%2520hinges%2520heavily%2520on%2520optimal%250Ameasurement%2520locations.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520air%2520quality%250Amapping%2520where%2520autonomous%2520drones%252C%2520guided%2520by%2520a%2520collaborative%2520multi-agent%250Areinforcement%2520learning%2520%2528MARL%2529%2520framework%252C%2520act%2520as%2520airborne%2520detectives.%2520Ditching%250Athe%2520limitations%2520of%2520static%2520sensor%2520networks%252C%2520the%2520drones%2520engage%2520in%2520a%2520synergistic%250Ainteraction%252C%2520adapting%2520their%2520flight%2520paths%2520in%2520real%2520time%2520to%2520gather%2520optimal%2520data%250Afor%2520Data%2520Assimilation%2520%2528DA%2529.%2520Our%2520approach%2520employs%2520a%2520tailored%2520reward%2520function%250Awith%2520dynamic%2520credit%2520assignment%252C%2520enabling%2520drones%2520to%2520prioritize%2520informative%250Ameasurements%2520without%2520requiring%2520unavailable%2520ground%2520truth%2520data%252C%2520making%2520it%250Apractical%2520for%2520real-world%2520deployments.%2520Extensive%2520experiments%2520using%2520a%2520real-world%250Adataset%2520demonstrate%2520that%2520our%2520solution%2520achieves%2520significantly%2520improved%2520pollution%250Aestimates%252C%2520even%2520with%2520limited%2520drone%2520resources%2520or%2520limited%2520prior%2520knowledge%2520of%2520the%250Apollution%2520plume.%2520Beyond%2520air%2520quality%252C%2520this%2520solution%2520unlocks%2520possibilities%2520for%250Atackling%2520diverse%2520environmental%2520challenges%2520like%2520wildfire%2520detection%2520and%250Amanagement%2520through%2520scalable%2520and%2520autonomous%2520drone%2520cooperation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20the%20Smog%3A%20A%20Cooperative%20Multi-Agent%20RL%20for%20Accurate%20Air%0A%20%20Pollution%20Mapping%20through%20Data%20Assimilation&entry.906535625=Ichrak%20Mokhtari%20and%20Walid%20Bechkit%20and%20Mohamed%20Sami%20Assenine%20and%20Herv%C3%A9%20Rivano&entry.1292438233=%20%20The%20rapid%20rise%20of%20air%20pollution%20events%20necessitates%20accurate%2C%20real-time%0Amonitoring%20for%20informed%20mitigation%20strategies.%20Data%20Assimilation%20%28DA%29%20methods%0Aprovide%20promising%20solutions%2C%20but%20their%20effectiveness%20hinges%20heavily%20on%20optimal%0Ameasurement%20locations.%20This%20paper%20presents%20a%20novel%20approach%20for%20air%20quality%0Amapping%20where%20autonomous%20drones%2C%20guided%20by%20a%20collaborative%20multi-agent%0Areinforcement%20learning%20%28MARL%29%20framework%2C%20act%20as%20airborne%20detectives.%20Ditching%0Athe%20limitations%20of%20static%20sensor%20networks%2C%20the%20drones%20engage%20in%20a%20synergistic%0Ainteraction%2C%20adapting%20their%20flight%20paths%20in%20real%20time%20to%20gather%20optimal%20data%0Afor%20Data%20Assimilation%20%28DA%29.%20Our%20approach%20employs%20a%20tailored%20reward%20function%0Awith%20dynamic%20credit%20assignment%2C%20enabling%20drones%20to%20prioritize%20informative%0Ameasurements%20without%20requiring%20unavailable%20ground%20truth%20data%2C%20making%20it%0Apractical%20for%20real-world%20deployments.%20Extensive%20experiments%20using%20a%20real-world%0Adataset%20demonstrate%20that%20our%20solution%20achieves%20significantly%20improved%20pollution%0Aestimates%2C%20even%20with%20limited%20drone%20resources%20or%20limited%20prior%20knowledge%20of%20the%0Apollution%20plume.%20Beyond%20air%20quality%2C%20this%20solution%20unlocks%20possibilities%20for%0Atackling%20diverse%20environmental%20challenges%20like%20wildfire%20detection%20and%0Amanagement%20through%20scalable%20and%20autonomous%20drone%20cooperation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12539v1&entry.124074799=Read"},
{"title": "A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer\n  Problems", "author": "Mohammad-Amin Charusaie and Samira Samadi", "abstract": "  Learn-to-Defer is a paradigm that enables learning algorithms to work not in\nisolation but as a team with human experts. In this paradigm, we permit the\nsystem to defer a subset of its tasks to the expert. Although there are\ncurrently systems that follow this paradigm and are designed to optimize the\naccuracy of the final human-AI team, the general methodology for developing\nsuch systems under a set of constraints (e.g., algorithmic fairness, expert\nintervention budget, defer of anomaly, etc.) remains largely unexplored. In\nthis paper, using a $d$-dimensional generalization to the fundamental lemma of\nNeyman and Pearson (d-GNP), we obtain the Bayes optimal solution for\nlearn-to-defer systems under various constraints. Furthermore, we design a\ngeneralizable algorithm to estimate that solution and apply this algorithm to\nthe COMPAS and ACSIncome datasets. Our algorithm shows improvements in terms of\nconstraint violation over a set of baselines.\n", "link": "http://arxiv.org/abs/2407.12710v1", "date": "2024-07-17", "relevancy": 1.0502, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5371}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5247}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unifying%20Post-Processing%20Framework%20for%20Multi-Objective%20Learn-to-Defer%0A%20%20Problems&body=Title%3A%20A%20Unifying%20Post-Processing%20Framework%20for%20Multi-Objective%20Learn-to-Defer%0A%20%20Problems%0AAuthor%3A%20Mohammad-Amin%20Charusaie%20and%20Samira%20Samadi%0AAbstract%3A%20%20%20Learn-to-Defer%20is%20a%20paradigm%20that%20enables%20learning%20algorithms%20to%20work%20not%20in%0Aisolation%20but%20as%20a%20team%20with%20human%20experts.%20In%20this%20paradigm%2C%20we%20permit%20the%0Asystem%20to%20defer%20a%20subset%20of%20its%20tasks%20to%20the%20expert.%20Although%20there%20are%0Acurrently%20systems%20that%20follow%20this%20paradigm%20and%20are%20designed%20to%20optimize%20the%0Aaccuracy%20of%20the%20final%20human-AI%20team%2C%20the%20general%20methodology%20for%20developing%0Asuch%20systems%20under%20a%20set%20of%20constraints%20%28e.g.%2C%20algorithmic%20fairness%2C%20expert%0Aintervention%20budget%2C%20defer%20of%20anomaly%2C%20etc.%29%20remains%20largely%20unexplored.%20In%0Athis%20paper%2C%20using%20a%20%24d%24-dimensional%20generalization%20to%20the%20fundamental%20lemma%20of%0ANeyman%20and%20Pearson%20%28d-GNP%29%2C%20we%20obtain%20the%20Bayes%20optimal%20solution%20for%0Alearn-to-defer%20systems%20under%20various%20constraints.%20Furthermore%2C%20we%20design%20a%0Ageneralizable%20algorithm%20to%20estimate%20that%20solution%20and%20apply%20this%20algorithm%20to%0Athe%20COMPAS%20and%20ACSIncome%20datasets.%20Our%20algorithm%20shows%20improvements%20in%20terms%20of%0Aconstraint%20violation%20over%20a%20set%20of%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unifying%2520Post-Processing%2520Framework%2520for%2520Multi-Objective%2520Learn-to-Defer%250A%2520%2520Problems%26entry.906535625%3DMohammad-Amin%2520Charusaie%2520and%2520Samira%2520Samadi%26entry.1292438233%3D%2520%2520Learn-to-Defer%2520is%2520a%2520paradigm%2520that%2520enables%2520learning%2520algorithms%2520to%2520work%2520not%2520in%250Aisolation%2520but%2520as%2520a%2520team%2520with%2520human%2520experts.%2520In%2520this%2520paradigm%252C%2520we%2520permit%2520the%250Asystem%2520to%2520defer%2520a%2520subset%2520of%2520its%2520tasks%2520to%2520the%2520expert.%2520Although%2520there%2520are%250Acurrently%2520systems%2520that%2520follow%2520this%2520paradigm%2520and%2520are%2520designed%2520to%2520optimize%2520the%250Aaccuracy%2520of%2520the%2520final%2520human-AI%2520team%252C%2520the%2520general%2520methodology%2520for%2520developing%250Asuch%2520systems%2520under%2520a%2520set%2520of%2520constraints%2520%2528e.g.%252C%2520algorithmic%2520fairness%252C%2520expert%250Aintervention%2520budget%252C%2520defer%2520of%2520anomaly%252C%2520etc.%2529%2520remains%2520largely%2520unexplored.%2520In%250Athis%2520paper%252C%2520using%2520a%2520%2524d%2524-dimensional%2520generalization%2520to%2520the%2520fundamental%2520lemma%2520of%250ANeyman%2520and%2520Pearson%2520%2528d-GNP%2529%252C%2520we%2520obtain%2520the%2520Bayes%2520optimal%2520solution%2520for%250Alearn-to-defer%2520systems%2520under%2520various%2520constraints.%2520Furthermore%252C%2520we%2520design%2520a%250Ageneralizable%2520algorithm%2520to%2520estimate%2520that%2520solution%2520and%2520apply%2520this%2520algorithm%2520to%250Athe%2520COMPAS%2520and%2520ACSIncome%2520datasets.%2520Our%2520algorithm%2520shows%2520improvements%2520in%2520terms%2520of%250Aconstraint%2520violation%2520over%2520a%2520set%2520of%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unifying%20Post-Processing%20Framework%20for%20Multi-Objective%20Learn-to-Defer%0A%20%20Problems&entry.906535625=Mohammad-Amin%20Charusaie%20and%20Samira%20Samadi&entry.1292438233=%20%20Learn-to-Defer%20is%20a%20paradigm%20that%20enables%20learning%20algorithms%20to%20work%20not%20in%0Aisolation%20but%20as%20a%20team%20with%20human%20experts.%20In%20this%20paradigm%2C%20we%20permit%20the%0Asystem%20to%20defer%20a%20subset%20of%20its%20tasks%20to%20the%20expert.%20Although%20there%20are%0Acurrently%20systems%20that%20follow%20this%20paradigm%20and%20are%20designed%20to%20optimize%20the%0Aaccuracy%20of%20the%20final%20human-AI%20team%2C%20the%20general%20methodology%20for%20developing%0Asuch%20systems%20under%20a%20set%20of%20constraints%20%28e.g.%2C%20algorithmic%20fairness%2C%20expert%0Aintervention%20budget%2C%20defer%20of%20anomaly%2C%20etc.%29%20remains%20largely%20unexplored.%20In%0Athis%20paper%2C%20using%20a%20%24d%24-dimensional%20generalization%20to%20the%20fundamental%20lemma%20of%0ANeyman%20and%20Pearson%20%28d-GNP%29%2C%20we%20obtain%20the%20Bayes%20optimal%20solution%20for%0Alearn-to-defer%20systems%20under%20various%20constraints.%20Furthermore%2C%20we%20design%20a%0Ageneralizable%20algorithm%20to%20estimate%20that%20solution%20and%20apply%20this%20algorithm%20to%0Athe%20COMPAS%20and%20ACSIncome%20datasets.%20Our%20algorithm%20shows%20improvements%20in%20terms%20of%0Aconstraint%20violation%20over%20a%20set%20of%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12710v1&entry.124074799=Read"},
{"title": "End-to-end Stroke imaging analysis, using reservoir computing-based\n  effective connectivity, and interpretable Artificial intelligence", "author": "Wojciech Ciezobka and Joan Falco-Roget and Cemal Koba and Alessandro Crimi", "abstract": "  In this paper, we propose a reservoir computing-based and directed graph\nanalysis pipeline. The goal of this pipeline is to define an efficient brain\nrepresentation for connectivity in stroke data derived from magnetic resonance\nimaging. Ultimately, this representation is used within a directed graph\nconvolutional architecture and investigated with explainable artificial\nintelligence (AI) tools.\n  Stroke is one of the leading causes of mortality and morbidity worldwide, and\nit demands precise diagnostic tools for timely intervention and improved\npatient outcomes. Neuroimaging data, with their rich structural and functional\ninformation, provide a fertile ground for biomarker discovery. However, the\ncomplexity and variability of information flow in the brain requires advanced\nanalysis, especially if we consider the case of disrupted networks as those\ngiven by the brain connectome of stroke patients. To address the needs given by\nthis complex scenario we proposed an end-to-end pipeline. This pipeline begins\nwith reservoir computing causality, to define effective connectivity of the\nbrain. This allows directed graph network representations which have not been\nfully investigated so far by graph convolutional network classifiers. Indeed,\nthe pipeline subsequently incorporates a classification module to categorize\nthe effective connectivity (directed graphs) of brain networks of patients\nversus matched healthy control. The classification led to an area under the\ncurve of 0.69 with the given heterogeneous dataset. Thanks to explainable\ntools, an interpretation of disrupted networks across the brain networks was\npossible. This elucidates the effective connectivity biomarker's contribution\nto stroke classification, fostering insights into disease mechanisms and\ntreatment responses.\n", "link": "http://arxiv.org/abs/2407.12553v1", "date": "2024-07-17", "relevancy": 1.4341, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4872}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-end%20Stroke%20imaging%20analysis%2C%20using%20reservoir%20computing-based%0A%20%20effective%20connectivity%2C%20and%20interpretable%20Artificial%20intelligence&body=Title%3A%20End-to-end%20Stroke%20imaging%20analysis%2C%20using%20reservoir%20computing-based%0A%20%20effective%20connectivity%2C%20and%20interpretable%20Artificial%20intelligence%0AAuthor%3A%20Wojciech%20Ciezobka%20and%20Joan%20Falco-Roget%20and%20Cemal%20Koba%20and%20Alessandro%20Crimi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20reservoir%20computing-based%20and%20directed%20graph%0Aanalysis%20pipeline.%20The%20goal%20of%20this%20pipeline%20is%20to%20define%20an%20efficient%20brain%0Arepresentation%20for%20connectivity%20in%20stroke%20data%20derived%20from%20magnetic%20resonance%0Aimaging.%20Ultimately%2C%20this%20representation%20is%20used%20within%20a%20directed%20graph%0Aconvolutional%20architecture%20and%20investigated%20with%20explainable%20artificial%0Aintelligence%20%28AI%29%20tools.%0A%20%20Stroke%20is%20one%20of%20the%20leading%20causes%20of%20mortality%20and%20morbidity%20worldwide%2C%20and%0Ait%20demands%20precise%20diagnostic%20tools%20for%20timely%20intervention%20and%20improved%0Apatient%20outcomes.%20Neuroimaging%20data%2C%20with%20their%20rich%20structural%20and%20functional%0Ainformation%2C%20provide%20a%20fertile%20ground%20for%20biomarker%20discovery.%20However%2C%20the%0Acomplexity%20and%20variability%20of%20information%20flow%20in%20the%20brain%20requires%20advanced%0Aanalysis%2C%20especially%20if%20we%20consider%20the%20case%20of%20disrupted%20networks%20as%20those%0Agiven%20by%20the%20brain%20connectome%20of%20stroke%20patients.%20To%20address%20the%20needs%20given%20by%0Athis%20complex%20scenario%20we%20proposed%20an%20end-to-end%20pipeline.%20This%20pipeline%20begins%0Awith%20reservoir%20computing%20causality%2C%20to%20define%20effective%20connectivity%20of%20the%0Abrain.%20This%20allows%20directed%20graph%20network%20representations%20which%20have%20not%20been%0Afully%20investigated%20so%20far%20by%20graph%20convolutional%20network%20classifiers.%20Indeed%2C%0Athe%20pipeline%20subsequently%20incorporates%20a%20classification%20module%20to%20categorize%0Athe%20effective%20connectivity%20%28directed%20graphs%29%20of%20brain%20networks%20of%20patients%0Aversus%20matched%20healthy%20control.%20The%20classification%20led%20to%20an%20area%20under%20the%0Acurve%20of%200.69%20with%20the%20given%20heterogeneous%20dataset.%20Thanks%20to%20explainable%0Atools%2C%20an%20interpretation%20of%20disrupted%20networks%20across%20the%20brain%20networks%20was%0Apossible.%20This%20elucidates%20the%20effective%20connectivity%20biomarker%27s%20contribution%0Ato%20stroke%20classification%2C%20fostering%20insights%20into%20disease%20mechanisms%20and%0Atreatment%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-end%2520Stroke%2520imaging%2520analysis%252C%2520using%2520reservoir%2520computing-based%250A%2520%2520effective%2520connectivity%252C%2520and%2520interpretable%2520Artificial%2520intelligence%26entry.906535625%3DWojciech%2520Ciezobka%2520and%2520Joan%2520Falco-Roget%2520and%2520Cemal%2520Koba%2520and%2520Alessandro%2520Crimi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520reservoir%2520computing-based%2520and%2520directed%2520graph%250Aanalysis%2520pipeline.%2520The%2520goal%2520of%2520this%2520pipeline%2520is%2520to%2520define%2520an%2520efficient%2520brain%250Arepresentation%2520for%2520connectivity%2520in%2520stroke%2520data%2520derived%2520from%2520magnetic%2520resonance%250Aimaging.%2520Ultimately%252C%2520this%2520representation%2520is%2520used%2520within%2520a%2520directed%2520graph%250Aconvolutional%2520architecture%2520and%2520investigated%2520with%2520explainable%2520artificial%250Aintelligence%2520%2528AI%2529%2520tools.%250A%2520%2520Stroke%2520is%2520one%2520of%2520the%2520leading%2520causes%2520of%2520mortality%2520and%2520morbidity%2520worldwide%252C%2520and%250Ait%2520demands%2520precise%2520diagnostic%2520tools%2520for%2520timely%2520intervention%2520and%2520improved%250Apatient%2520outcomes.%2520Neuroimaging%2520data%252C%2520with%2520their%2520rich%2520structural%2520and%2520functional%250Ainformation%252C%2520provide%2520a%2520fertile%2520ground%2520for%2520biomarker%2520discovery.%2520However%252C%2520the%250Acomplexity%2520and%2520variability%2520of%2520information%2520flow%2520in%2520the%2520brain%2520requires%2520advanced%250Aanalysis%252C%2520especially%2520if%2520we%2520consider%2520the%2520case%2520of%2520disrupted%2520networks%2520as%2520those%250Agiven%2520by%2520the%2520brain%2520connectome%2520of%2520stroke%2520patients.%2520To%2520address%2520the%2520needs%2520given%2520by%250Athis%2520complex%2520scenario%2520we%2520proposed%2520an%2520end-to-end%2520pipeline.%2520This%2520pipeline%2520begins%250Awith%2520reservoir%2520computing%2520causality%252C%2520to%2520define%2520effective%2520connectivity%2520of%2520the%250Abrain.%2520This%2520allows%2520directed%2520graph%2520network%2520representations%2520which%2520have%2520not%2520been%250Afully%2520investigated%2520so%2520far%2520by%2520graph%2520convolutional%2520network%2520classifiers.%2520Indeed%252C%250Athe%2520pipeline%2520subsequently%2520incorporates%2520a%2520classification%2520module%2520to%2520categorize%250Athe%2520effective%2520connectivity%2520%2528directed%2520graphs%2529%2520of%2520brain%2520networks%2520of%2520patients%250Aversus%2520matched%2520healthy%2520control.%2520The%2520classification%2520led%2520to%2520an%2520area%2520under%2520the%250Acurve%2520of%25200.69%2520with%2520the%2520given%2520heterogeneous%2520dataset.%2520Thanks%2520to%2520explainable%250Atools%252C%2520an%2520interpretation%2520of%2520disrupted%2520networks%2520across%2520the%2520brain%2520networks%2520was%250Apossible.%2520This%2520elucidates%2520the%2520effective%2520connectivity%2520biomarker%2527s%2520contribution%250Ato%2520stroke%2520classification%252C%2520fostering%2520insights%2520into%2520disease%2520mechanisms%2520and%250Atreatment%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-end%20Stroke%20imaging%20analysis%2C%20using%20reservoir%20computing-based%0A%20%20effective%20connectivity%2C%20and%20interpretable%20Artificial%20intelligence&entry.906535625=Wojciech%20Ciezobka%20and%20Joan%20Falco-Roget%20and%20Cemal%20Koba%20and%20Alessandro%20Crimi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20reservoir%20computing-based%20and%20directed%20graph%0Aanalysis%20pipeline.%20The%20goal%20of%20this%20pipeline%20is%20to%20define%20an%20efficient%20brain%0Arepresentation%20for%20connectivity%20in%20stroke%20data%20derived%20from%20magnetic%20resonance%0Aimaging.%20Ultimately%2C%20this%20representation%20is%20used%20within%20a%20directed%20graph%0Aconvolutional%20architecture%20and%20investigated%20with%20explainable%20artificial%0Aintelligence%20%28AI%29%20tools.%0A%20%20Stroke%20is%20one%20of%20the%20leading%20causes%20of%20mortality%20and%20morbidity%20worldwide%2C%20and%0Ait%20demands%20precise%20diagnostic%20tools%20for%20timely%20intervention%20and%20improved%0Apatient%20outcomes.%20Neuroimaging%20data%2C%20with%20their%20rich%20structural%20and%20functional%0Ainformation%2C%20provide%20a%20fertile%20ground%20for%20biomarker%20discovery.%20However%2C%20the%0Acomplexity%20and%20variability%20of%20information%20flow%20in%20the%20brain%20requires%20advanced%0Aanalysis%2C%20especially%20if%20we%20consider%20the%20case%20of%20disrupted%20networks%20as%20those%0Agiven%20by%20the%20brain%20connectome%20of%20stroke%20patients.%20To%20address%20the%20needs%20given%20by%0Athis%20complex%20scenario%20we%20proposed%20an%20end-to-end%20pipeline.%20This%20pipeline%20begins%0Awith%20reservoir%20computing%20causality%2C%20to%20define%20effective%20connectivity%20of%20the%0Abrain.%20This%20allows%20directed%20graph%20network%20representations%20which%20have%20not%20been%0Afully%20investigated%20so%20far%20by%20graph%20convolutional%20network%20classifiers.%20Indeed%2C%0Athe%20pipeline%20subsequently%20incorporates%20a%20classification%20module%20to%20categorize%0Athe%20effective%20connectivity%20%28directed%20graphs%29%20of%20brain%20networks%20of%20patients%0Aversus%20matched%20healthy%20control.%20The%20classification%20led%20to%20an%20area%20under%20the%0Acurve%20of%200.69%20with%20the%20given%20heterogeneous%20dataset.%20Thanks%20to%20explainable%0Atools%2C%20an%20interpretation%20of%20disrupted%20networks%20across%20the%20brain%20networks%20was%0Apossible.%20This%20elucidates%20the%20effective%20connectivity%20biomarker%27s%20contribution%0Ato%20stroke%20classification%2C%20fostering%20insights%20into%20disease%20mechanisms%20and%0Atreatment%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12553v1&entry.124074799=Read"},
{"title": "Estimate Epidemiological Parameters given Partial Observations based on\n  Algebraically Observable PINNs", "author": "Mizuka Komatsu", "abstract": "  In this study, we considered the problem of estimating epidemiological\nparameters based on physics-informed neural networks (PINNs). In practice, not\nall trajectory data corresponding to the population estimated by epidemic\nmodels can be obtained, and some observed trajectories are noisy. Learning\nPINNs to estimate unknown epidemiological parameters using such partial\nobservations is challenging. Accordingly, we introduce the concept of algebraic\nobservability into PINNs. The validity of the proposed PINN, named as an\nalgebraically observable PINNs, in terms of estimation parameters and\nprediction of unobserved variables, is demonstrated through numerical\nexperiments.\n", "link": "http://arxiv.org/abs/2407.12598v1", "date": "2024-07-17", "relevancy": 1.659, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4622}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4412}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimate%20Epidemiological%20Parameters%20given%20Partial%20Observations%20based%20on%0A%20%20Algebraically%20Observable%20PINNs&body=Title%3A%20Estimate%20Epidemiological%20Parameters%20given%20Partial%20Observations%20based%20on%0A%20%20Algebraically%20Observable%20PINNs%0AAuthor%3A%20Mizuka%20Komatsu%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20considered%20the%20problem%20of%20estimating%20epidemiological%0Aparameters%20based%20on%20physics-informed%20neural%20networks%20%28PINNs%29.%20In%20practice%2C%20not%0Aall%20trajectory%20data%20corresponding%20to%20the%20population%20estimated%20by%20epidemic%0Amodels%20can%20be%20obtained%2C%20and%20some%20observed%20trajectories%20are%20noisy.%20Learning%0APINNs%20to%20estimate%20unknown%20epidemiological%20parameters%20using%20such%20partial%0Aobservations%20is%20challenging.%20Accordingly%2C%20we%20introduce%20the%20concept%20of%20algebraic%0Aobservability%20into%20PINNs.%20The%20validity%20of%20the%20proposed%20PINN%2C%20named%20as%20an%0Aalgebraically%20observable%20PINNs%2C%20in%20terms%20of%20estimation%20parameters%20and%0Aprediction%20of%20unobserved%20variables%2C%20is%20demonstrated%20through%20numerical%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimate%2520Epidemiological%2520Parameters%2520given%2520Partial%2520Observations%2520based%2520on%250A%2520%2520Algebraically%2520Observable%2520PINNs%26entry.906535625%3DMizuka%2520Komatsu%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520considered%2520the%2520problem%2520of%2520estimating%2520epidemiological%250Aparameters%2520based%2520on%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529.%2520In%2520practice%252C%2520not%250Aall%2520trajectory%2520data%2520corresponding%2520to%2520the%2520population%2520estimated%2520by%2520epidemic%250Amodels%2520can%2520be%2520obtained%252C%2520and%2520some%2520observed%2520trajectories%2520are%2520noisy.%2520Learning%250APINNs%2520to%2520estimate%2520unknown%2520epidemiological%2520parameters%2520using%2520such%2520partial%250Aobservations%2520is%2520challenging.%2520Accordingly%252C%2520we%2520introduce%2520the%2520concept%2520of%2520algebraic%250Aobservability%2520into%2520PINNs.%2520The%2520validity%2520of%2520the%2520proposed%2520PINN%252C%2520named%2520as%2520an%250Aalgebraically%2520observable%2520PINNs%252C%2520in%2520terms%2520of%2520estimation%2520parameters%2520and%250Aprediction%2520of%2520unobserved%2520variables%252C%2520is%2520demonstrated%2520through%2520numerical%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimate%20Epidemiological%20Parameters%20given%20Partial%20Observations%20based%20on%0A%20%20Algebraically%20Observable%20PINNs&entry.906535625=Mizuka%20Komatsu&entry.1292438233=%20%20In%20this%20study%2C%20we%20considered%20the%20problem%20of%20estimating%20epidemiological%0Aparameters%20based%20on%20physics-informed%20neural%20networks%20%28PINNs%29.%20In%20practice%2C%20not%0Aall%20trajectory%20data%20corresponding%20to%20the%20population%20estimated%20by%20epidemic%0Amodels%20can%20be%20obtained%2C%20and%20some%20observed%20trajectories%20are%20noisy.%20Learning%0APINNs%20to%20estimate%20unknown%20epidemiological%20parameters%20using%20such%20partial%0Aobservations%20is%20challenging.%20Accordingly%2C%20we%20introduce%20the%20concept%20of%20algebraic%0Aobservability%20into%20PINNs.%20The%20validity%20of%20the%20proposed%20PINN%2C%20named%20as%20an%0Aalgebraically%20observable%20PINNs%2C%20in%20terms%20of%20estimation%20parameters%20and%0Aprediction%20of%20unobserved%20variables%2C%20is%20demonstrated%20through%20numerical%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12598v1&entry.124074799=Read"},
{"title": "gFlora: a topology-aware method to discover functional co-response\n  groups in soil microbial communities", "author": "Nan Chen and Merlijn Schram and Doina Bucur", "abstract": "  We aim to learn the functional co-response group: a group of taxa whose\nco-response effect (the representative characteristic of the group showing the\ntotal topological abundance of taxa) co-responds (associates well\nstatistically) to a functional variable. Different from the state-of-the-art\nmethod, we model the soil microbial community as an ecological co-occurrence\nnetwork with the taxa as nodes (weighted by their abundance) and their\nrelationships (a combination from both spatial and functional ecological\naspects) as edges (weighted by the strength of the relationships). Then, we\ndesign a method called gFlora which notably uses graph convolution over this\nco-occurrence network to get the co-response effect of the group, such that the\nnetwork topology is also considered in the discovery process. We evaluate\ngFlora on two real-world soil microbiome datasets (bacteria and nematodes) and\ncompare it with the state-of-the-art method. gFlora outperforms this on all\nevaluation metrics, and discovers new functional evidence for taxa which were\nso far under-studied. We show that the graph convolution step is crucial to\ntaxa with relatively low abundance (thus removing the bias towards taxa with\nhigher abundance), and the discovered bacteria of different genera are\ndistributed in the co-occurrence network but still tightly connected among\nthemselves, demonstrating that topologically they fill different but\ncollaborative functional roles in the ecological community.\n", "link": "http://arxiv.org/abs/2407.03897v2", "date": "2024-07-17", "relevancy": 1.1463, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4093}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3807}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20gFlora%3A%20a%20topology-aware%20method%20to%20discover%20functional%20co-response%0A%20%20groups%20in%20soil%20microbial%20communities&body=Title%3A%20gFlora%3A%20a%20topology-aware%20method%20to%20discover%20functional%20co-response%0A%20%20groups%20in%20soil%20microbial%20communities%0AAuthor%3A%20Nan%20Chen%20and%20Merlijn%20Schram%20and%20Doina%20Bucur%0AAbstract%3A%20%20%20We%20aim%20to%20learn%20the%20functional%20co-response%20group%3A%20a%20group%20of%20taxa%20whose%0Aco-response%20effect%20%28the%20representative%20characteristic%20of%20the%20group%20showing%20the%0Atotal%20topological%20abundance%20of%20taxa%29%20co-responds%20%28associates%20well%0Astatistically%29%20to%20a%20functional%20variable.%20Different%20from%20the%20state-of-the-art%0Amethod%2C%20we%20model%20the%20soil%20microbial%20community%20as%20an%20ecological%20co-occurrence%0Anetwork%20with%20the%20taxa%20as%20nodes%20%28weighted%20by%20their%20abundance%29%20and%20their%0Arelationships%20%28a%20combination%20from%20both%20spatial%20and%20functional%20ecological%0Aaspects%29%20as%20edges%20%28weighted%20by%20the%20strength%20of%20the%20relationships%29.%20Then%2C%20we%0Adesign%20a%20method%20called%20gFlora%20which%20notably%20uses%20graph%20convolution%20over%20this%0Aco-occurrence%20network%20to%20get%20the%20co-response%20effect%20of%20the%20group%2C%20such%20that%20the%0Anetwork%20topology%20is%20also%20considered%20in%20the%20discovery%20process.%20We%20evaluate%0AgFlora%20on%20two%20real-world%20soil%20microbiome%20datasets%20%28bacteria%20and%20nematodes%29%20and%0Acompare%20it%20with%20the%20state-of-the-art%20method.%20gFlora%20outperforms%20this%20on%20all%0Aevaluation%20metrics%2C%20and%20discovers%20new%20functional%20evidence%20for%20taxa%20which%20were%0Aso%20far%20under-studied.%20We%20show%20that%20the%20graph%20convolution%20step%20is%20crucial%20to%0Ataxa%20with%20relatively%20low%20abundance%20%28thus%20removing%20the%20bias%20towards%20taxa%20with%0Ahigher%20abundance%29%2C%20and%20the%20discovered%20bacteria%20of%20different%20genera%20are%0Adistributed%20in%20the%20co-occurrence%20network%20but%20still%20tightly%20connected%20among%0Athemselves%2C%20demonstrating%20that%20topologically%20they%20fill%20different%20but%0Acollaborative%20functional%20roles%20in%20the%20ecological%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03897v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DgFlora%253A%2520a%2520topology-aware%2520method%2520to%2520discover%2520functional%2520co-response%250A%2520%2520groups%2520in%2520soil%2520microbial%2520communities%26entry.906535625%3DNan%2520Chen%2520and%2520Merlijn%2520Schram%2520and%2520Doina%2520Bucur%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520learn%2520the%2520functional%2520co-response%2520group%253A%2520a%2520group%2520of%2520taxa%2520whose%250Aco-response%2520effect%2520%2528the%2520representative%2520characteristic%2520of%2520the%2520group%2520showing%2520the%250Atotal%2520topological%2520abundance%2520of%2520taxa%2529%2520co-responds%2520%2528associates%2520well%250Astatistically%2529%2520to%2520a%2520functional%2520variable.%2520Different%2520from%2520the%2520state-of-the-art%250Amethod%252C%2520we%2520model%2520the%2520soil%2520microbial%2520community%2520as%2520an%2520ecological%2520co-occurrence%250Anetwork%2520with%2520the%2520taxa%2520as%2520nodes%2520%2528weighted%2520by%2520their%2520abundance%2529%2520and%2520their%250Arelationships%2520%2528a%2520combination%2520from%2520both%2520spatial%2520and%2520functional%2520ecological%250Aaspects%2529%2520as%2520edges%2520%2528weighted%2520by%2520the%2520strength%2520of%2520the%2520relationships%2529.%2520Then%252C%2520we%250Adesign%2520a%2520method%2520called%2520gFlora%2520which%2520notably%2520uses%2520graph%2520convolution%2520over%2520this%250Aco-occurrence%2520network%2520to%2520get%2520the%2520co-response%2520effect%2520of%2520the%2520group%252C%2520such%2520that%2520the%250Anetwork%2520topology%2520is%2520also%2520considered%2520in%2520the%2520discovery%2520process.%2520We%2520evaluate%250AgFlora%2520on%2520two%2520real-world%2520soil%2520microbiome%2520datasets%2520%2528bacteria%2520and%2520nematodes%2529%2520and%250Acompare%2520it%2520with%2520the%2520state-of-the-art%2520method.%2520gFlora%2520outperforms%2520this%2520on%2520all%250Aevaluation%2520metrics%252C%2520and%2520discovers%2520new%2520functional%2520evidence%2520for%2520taxa%2520which%2520were%250Aso%2520far%2520under-studied.%2520We%2520show%2520that%2520the%2520graph%2520convolution%2520step%2520is%2520crucial%2520to%250Ataxa%2520with%2520relatively%2520low%2520abundance%2520%2528thus%2520removing%2520the%2520bias%2520towards%2520taxa%2520with%250Ahigher%2520abundance%2529%252C%2520and%2520the%2520discovered%2520bacteria%2520of%2520different%2520genera%2520are%250Adistributed%2520in%2520the%2520co-occurrence%2520network%2520but%2520still%2520tightly%2520connected%2520among%250Athemselves%252C%2520demonstrating%2520that%2520topologically%2520they%2520fill%2520different%2520but%250Acollaborative%2520functional%2520roles%2520in%2520the%2520ecological%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03897v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=gFlora%3A%20a%20topology-aware%20method%20to%20discover%20functional%20co-response%0A%20%20groups%20in%20soil%20microbial%20communities&entry.906535625=Nan%20Chen%20and%20Merlijn%20Schram%20and%20Doina%20Bucur&entry.1292438233=%20%20We%20aim%20to%20learn%20the%20functional%20co-response%20group%3A%20a%20group%20of%20taxa%20whose%0Aco-response%20effect%20%28the%20representative%20characteristic%20of%20the%20group%20showing%20the%0Atotal%20topological%20abundance%20of%20taxa%29%20co-responds%20%28associates%20well%0Astatistically%29%20to%20a%20functional%20variable.%20Different%20from%20the%20state-of-the-art%0Amethod%2C%20we%20model%20the%20soil%20microbial%20community%20as%20an%20ecological%20co-occurrence%0Anetwork%20with%20the%20taxa%20as%20nodes%20%28weighted%20by%20their%20abundance%29%20and%20their%0Arelationships%20%28a%20combination%20from%20both%20spatial%20and%20functional%20ecological%0Aaspects%29%20as%20edges%20%28weighted%20by%20the%20strength%20of%20the%20relationships%29.%20Then%2C%20we%0Adesign%20a%20method%20called%20gFlora%20which%20notably%20uses%20graph%20convolution%20over%20this%0Aco-occurrence%20network%20to%20get%20the%20co-response%20effect%20of%20the%20group%2C%20such%20that%20the%0Anetwork%20topology%20is%20also%20considered%20in%20the%20discovery%20process.%20We%20evaluate%0AgFlora%20on%20two%20real-world%20soil%20microbiome%20datasets%20%28bacteria%20and%20nematodes%29%20and%0Acompare%20it%20with%20the%20state-of-the-art%20method.%20gFlora%20outperforms%20this%20on%20all%0Aevaluation%20metrics%2C%20and%20discovers%20new%20functional%20evidence%20for%20taxa%20which%20were%0Aso%20far%20under-studied.%20We%20show%20that%20the%20graph%20convolution%20step%20is%20crucial%20to%0Ataxa%20with%20relatively%20low%20abundance%20%28thus%20removing%20the%20bias%20towards%20taxa%20with%0Ahigher%20abundance%29%2C%20and%20the%20discovered%20bacteria%20of%20different%20genera%20are%0Adistributed%20in%20the%20co-occurrence%20network%20but%20still%20tightly%20connected%20among%0Athemselves%2C%20demonstrating%20that%20topologically%20they%20fill%20different%20but%0Acollaborative%20functional%20roles%20in%20the%20ecological%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03897v2&entry.124074799=Read"},
{"title": "Modeling Relational Patterns for Logical Query Answering over Knowledge\n  Graphs", "author": "Yunjie He and Mojtaba Nayyeri and Bo Xiong and Yuqicheng Zhu and Evgeny Kharlamov and Steffen Staab", "abstract": "  Answering first-order logical (FOL) queries over knowledge graphs (KG)\nremains a challenging task mainly due to KG incompleteness. Query embedding\napproaches this problem by computing the low-dimensional vector representations\nof entities, relations, and logical queries. KGs exhibit relational patterns\nsuch as symmetry and composition and modeling the patterns can further enhance\nthe performance of query embedding models. However, the role of such patterns\nin answering FOL queries by query embedding models has not been yet studied in\nthe literature. In this paper, we fill in this research gap and empower FOL\nqueries reasoning with pattern inference by introducing an inductive bias that\nallows for learning relation patterns. To this end, we develop a novel query\nembedding method, RoConE, that defines query regions as geometric cones and\nalgebraic query operators by rotations in complex space. RoConE combines the\nadvantages of Cone as a well-specified geometric representation for query\nembedding, and also the rotation operator as a powerful algebraic operation for\npattern inference. Our experimental results on several benchmark datasets\nconfirm the advantage of relational patterns for enhancing logical query\nanswering task.\n", "link": "http://arxiv.org/abs/2303.11858v2", "date": "2024-07-17", "relevancy": 1.4051, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4732}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Relational%20Patterns%20for%20Logical%20Query%20Answering%20over%20Knowledge%0A%20%20Graphs&body=Title%3A%20Modeling%20Relational%20Patterns%20for%20Logical%20Query%20Answering%20over%20Knowledge%0A%20%20Graphs%0AAuthor%3A%20Yunjie%20He%20and%20Mojtaba%20Nayyeri%20and%20Bo%20Xiong%20and%20Yuqicheng%20Zhu%20and%20Evgeny%20Kharlamov%20and%20Steffen%20Staab%0AAbstract%3A%20%20%20Answering%20first-order%20logical%20%28FOL%29%20queries%20over%20knowledge%20graphs%20%28KG%29%0Aremains%20a%20challenging%20task%20mainly%20due%20to%20KG%20incompleteness.%20Query%20embedding%0Aapproaches%20this%20problem%20by%20computing%20the%20low-dimensional%20vector%20representations%0Aof%20entities%2C%20relations%2C%20and%20logical%20queries.%20KGs%20exhibit%20relational%20patterns%0Asuch%20as%20symmetry%20and%20composition%20and%20modeling%20the%20patterns%20can%20further%20enhance%0Athe%20performance%20of%20query%20embedding%20models.%20However%2C%20the%20role%20of%20such%20patterns%0Ain%20answering%20FOL%20queries%20by%20query%20embedding%20models%20has%20not%20been%20yet%20studied%20in%0Athe%20literature.%20In%20this%20paper%2C%20we%20fill%20in%20this%20research%20gap%20and%20empower%20FOL%0Aqueries%20reasoning%20with%20pattern%20inference%20by%20introducing%20an%20inductive%20bias%20that%0Aallows%20for%20learning%20relation%20patterns.%20To%20this%20end%2C%20we%20develop%20a%20novel%20query%0Aembedding%20method%2C%20RoConE%2C%20that%20defines%20query%20regions%20as%20geometric%20cones%20and%0Aalgebraic%20query%20operators%20by%20rotations%20in%20complex%20space.%20RoConE%20combines%20the%0Aadvantages%20of%20Cone%20as%20a%20well-specified%20geometric%20representation%20for%20query%0Aembedding%2C%20and%20also%20the%20rotation%20operator%20as%20a%20powerful%20algebraic%20operation%20for%0Apattern%20inference.%20Our%20experimental%20results%20on%20several%20benchmark%20datasets%0Aconfirm%20the%20advantage%20of%20relational%20patterns%20for%20enhancing%20logical%20query%0Aanswering%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.11858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Relational%2520Patterns%2520for%2520Logical%2520Query%2520Answering%2520over%2520Knowledge%250A%2520%2520Graphs%26entry.906535625%3DYunjie%2520He%2520and%2520Mojtaba%2520Nayyeri%2520and%2520Bo%2520Xiong%2520and%2520Yuqicheng%2520Zhu%2520and%2520Evgeny%2520Kharlamov%2520and%2520Steffen%2520Staab%26entry.1292438233%3D%2520%2520Answering%2520first-order%2520logical%2520%2528FOL%2529%2520queries%2520over%2520knowledge%2520graphs%2520%2528KG%2529%250Aremains%2520a%2520challenging%2520task%2520mainly%2520due%2520to%2520KG%2520incompleteness.%2520Query%2520embedding%250Aapproaches%2520this%2520problem%2520by%2520computing%2520the%2520low-dimensional%2520vector%2520representations%250Aof%2520entities%252C%2520relations%252C%2520and%2520logical%2520queries.%2520KGs%2520exhibit%2520relational%2520patterns%250Asuch%2520as%2520symmetry%2520and%2520composition%2520and%2520modeling%2520the%2520patterns%2520can%2520further%2520enhance%250Athe%2520performance%2520of%2520query%2520embedding%2520models.%2520However%252C%2520the%2520role%2520of%2520such%2520patterns%250Ain%2520answering%2520FOL%2520queries%2520by%2520query%2520embedding%2520models%2520has%2520not%2520been%2520yet%2520studied%2520in%250Athe%2520literature.%2520In%2520this%2520paper%252C%2520we%2520fill%2520in%2520this%2520research%2520gap%2520and%2520empower%2520FOL%250Aqueries%2520reasoning%2520with%2520pattern%2520inference%2520by%2520introducing%2520an%2520inductive%2520bias%2520that%250Aallows%2520for%2520learning%2520relation%2520patterns.%2520To%2520this%2520end%252C%2520we%2520develop%2520a%2520novel%2520query%250Aembedding%2520method%252C%2520RoConE%252C%2520that%2520defines%2520query%2520regions%2520as%2520geometric%2520cones%2520and%250Aalgebraic%2520query%2520operators%2520by%2520rotations%2520in%2520complex%2520space.%2520RoConE%2520combines%2520the%250Aadvantages%2520of%2520Cone%2520as%2520a%2520well-specified%2520geometric%2520representation%2520for%2520query%250Aembedding%252C%2520and%2520also%2520the%2520rotation%2520operator%2520as%2520a%2520powerful%2520algebraic%2520operation%2520for%250Apattern%2520inference.%2520Our%2520experimental%2520results%2520on%2520several%2520benchmark%2520datasets%250Aconfirm%2520the%2520advantage%2520of%2520relational%2520patterns%2520for%2520enhancing%2520logical%2520query%250Aanswering%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.11858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Relational%20Patterns%20for%20Logical%20Query%20Answering%20over%20Knowledge%0A%20%20Graphs&entry.906535625=Yunjie%20He%20and%20Mojtaba%20Nayyeri%20and%20Bo%20Xiong%20and%20Yuqicheng%20Zhu%20and%20Evgeny%20Kharlamov%20and%20Steffen%20Staab&entry.1292438233=%20%20Answering%20first-order%20logical%20%28FOL%29%20queries%20over%20knowledge%20graphs%20%28KG%29%0Aremains%20a%20challenging%20task%20mainly%20due%20to%20KG%20incompleteness.%20Query%20embedding%0Aapproaches%20this%20problem%20by%20computing%20the%20low-dimensional%20vector%20representations%0Aof%20entities%2C%20relations%2C%20and%20logical%20queries.%20KGs%20exhibit%20relational%20patterns%0Asuch%20as%20symmetry%20and%20composition%20and%20modeling%20the%20patterns%20can%20further%20enhance%0Athe%20performance%20of%20query%20embedding%20models.%20However%2C%20the%20role%20of%20such%20patterns%0Ain%20answering%20FOL%20queries%20by%20query%20embedding%20models%20has%20not%20been%20yet%20studied%20in%0Athe%20literature.%20In%20this%20paper%2C%20we%20fill%20in%20this%20research%20gap%20and%20empower%20FOL%0Aqueries%20reasoning%20with%20pattern%20inference%20by%20introducing%20an%20inductive%20bias%20that%0Aallows%20for%20learning%20relation%20patterns.%20To%20this%20end%2C%20we%20develop%20a%20novel%20query%0Aembedding%20method%2C%20RoConE%2C%20that%20defines%20query%20regions%20as%20geometric%20cones%20and%0Aalgebraic%20query%20operators%20by%20rotations%20in%20complex%20space.%20RoConE%20combines%20the%0Aadvantages%20of%20Cone%20as%20a%20well-specified%20geometric%20representation%20for%20query%0Aembedding%2C%20and%20also%20the%20rotation%20operator%20as%20a%20powerful%20algebraic%20operation%20for%0Apattern%20inference.%20Our%20experimental%20results%20on%20several%20benchmark%20datasets%0Aconfirm%20the%20advantage%20of%20relational%20patterns%20for%20enhancing%20logical%20query%0Aanswering%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.11858v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


