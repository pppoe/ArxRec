<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240729.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "From Flat to Spatial: Comparison of 4 methods constructing 3D, 2 and\n  1/2D Models from 2D Plans with neural networks", "author": "Jacob Sam and Karan Patel and Mike Saad", "abstract": "  In the field of architecture, the conversion of single images into 2 and 1/2D\nand 3D meshes is a promising technology that enhances design visualization and\nefficiency. This paper evaluates four innovative methods: \"One-2-3-45,\" \"CRM:\nSingle Image to 3D Textured Mesh with Convolutional Reconstruction Model,\"\n\"Instant Mesh,\" and \"Image-to-Mesh.\" These methods are at the forefront of this\ntechnology, focusing on their applicability in architectural design and\nvisualization. They streamline the creation of 3D architectural models,\nenabling rapid prototyping and detailed visualization from minimal initial\ninputs, such as photographs or simple sketches.One-2-3-45 leverages a\ndiffusion-based approach to generate multi-view reconstructions, ensuring high\ngeometric fidelity and texture quality. CRM utilizes a convolutional network to\nintegrate geometric priors into its architecture, producing detailed and\ntextured meshes quickly and efficiently. Instant Mesh combines the strengths of\nmulti-view diffusion and sparse-view models to offer speed and scalability,\nsuitable for diverse architectural projects. Image-to-Mesh leverages a\ngenerative adversarial network (GAN) to produce 3D meshes from single images,\nfocusing on maintaining high texture fidelity and geometric accuracy by\nincorporating image and depth map data into its training process. It uses a\nhybrid approach that combines voxel-based representations with surface\nreconstruction techniques to ensure detailed and realistic 3D models.This\ncomparative study highlights each method's contribution to reducing design\ncycle times, improving accuracy, and enabling flexible adaptations to various\narchitectural styles and requirements. By providing architects with powerful\ntools for rapid visualization and iteration, these advancements in 3D mesh\ngeneration are set to revolutionize architectural practices.\n", "link": "http://arxiv.org/abs/2407.19970v1", "date": "2024-07-29", "relevancy": 3.1386, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6532}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6532}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Flat%20to%20Spatial%3A%20Comparison%20of%204%20methods%20constructing%203D%2C%202%20and%0A%20%201/2D%20Models%20from%202D%20Plans%20with%20neural%20networks&body=Title%3A%20From%20Flat%20to%20Spatial%3A%20Comparison%20of%204%20methods%20constructing%203D%2C%202%20and%0A%20%201/2D%20Models%20from%202D%20Plans%20with%20neural%20networks%0AAuthor%3A%20Jacob%20Sam%20and%20Karan%20Patel%20and%20Mike%20Saad%0AAbstract%3A%20%20%20In%20the%20field%20of%20architecture%2C%20the%20conversion%20of%20single%20images%20into%202%20and%201/2D%0Aand%203D%20meshes%20is%20a%20promising%20technology%20that%20enhances%20design%20visualization%20and%0Aefficiency.%20This%20paper%20evaluates%20four%20innovative%20methods%3A%20%22One-2-3-45%2C%22%20%22CRM%3A%0ASingle%20Image%20to%203D%20Textured%20Mesh%20with%20Convolutional%20Reconstruction%20Model%2C%22%0A%22Instant%20Mesh%2C%22%20and%20%22Image-to-Mesh.%22%20These%20methods%20are%20at%20the%20forefront%20of%20this%0Atechnology%2C%20focusing%20on%20their%20applicability%20in%20architectural%20design%20and%0Avisualization.%20They%20streamline%20the%20creation%20of%203D%20architectural%20models%2C%0Aenabling%20rapid%20prototyping%20and%20detailed%20visualization%20from%20minimal%20initial%0Ainputs%2C%20such%20as%20photographs%20or%20simple%20sketches.One-2-3-45%20leverages%20a%0Adiffusion-based%20approach%20to%20generate%20multi-view%20reconstructions%2C%20ensuring%20high%0Ageometric%20fidelity%20and%20texture%20quality.%20CRM%20utilizes%20a%20convolutional%20network%20to%0Aintegrate%20geometric%20priors%20into%20its%20architecture%2C%20producing%20detailed%20and%0Atextured%20meshes%20quickly%20and%20efficiently.%20Instant%20Mesh%20combines%20the%20strengths%20of%0Amulti-view%20diffusion%20and%20sparse-view%20models%20to%20offer%20speed%20and%20scalability%2C%0Asuitable%20for%20diverse%20architectural%20projects.%20Image-to-Mesh%20leverages%20a%0Agenerative%20adversarial%20network%20%28GAN%29%20to%20produce%203D%20meshes%20from%20single%20images%2C%0Afocusing%20on%20maintaining%20high%20texture%20fidelity%20and%20geometric%20accuracy%20by%0Aincorporating%20image%20and%20depth%20map%20data%20into%20its%20training%20process.%20It%20uses%20a%0Ahybrid%20approach%20that%20combines%20voxel-based%20representations%20with%20surface%0Areconstruction%20techniques%20to%20ensure%20detailed%20and%20realistic%203D%20models.This%0Acomparative%20study%20highlights%20each%20method%27s%20contribution%20to%20reducing%20design%0Acycle%20times%2C%20improving%20accuracy%2C%20and%20enabling%20flexible%20adaptations%20to%20various%0Aarchitectural%20styles%20and%20requirements.%20By%20providing%20architects%20with%20powerful%0Atools%20for%20rapid%20visualization%20and%20iteration%2C%20these%20advancements%20in%203D%20mesh%0Ageneration%20are%20set%20to%20revolutionize%20architectural%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Flat%2520to%2520Spatial%253A%2520Comparison%2520of%25204%2520methods%2520constructing%25203D%252C%25202%2520and%250A%2520%25201/2D%2520Models%2520from%25202D%2520Plans%2520with%2520neural%2520networks%26entry.906535625%3DJacob%2520Sam%2520and%2520Karan%2520Patel%2520and%2520Mike%2520Saad%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520architecture%252C%2520the%2520conversion%2520of%2520single%2520images%2520into%25202%2520and%25201/2D%250Aand%25203D%2520meshes%2520is%2520a%2520promising%2520technology%2520that%2520enhances%2520design%2520visualization%2520and%250Aefficiency.%2520This%2520paper%2520evaluates%2520four%2520innovative%2520methods%253A%2520%2522One-2-3-45%252C%2522%2520%2522CRM%253A%250ASingle%2520Image%2520to%25203D%2520Textured%2520Mesh%2520with%2520Convolutional%2520Reconstruction%2520Model%252C%2522%250A%2522Instant%2520Mesh%252C%2522%2520and%2520%2522Image-to-Mesh.%2522%2520These%2520methods%2520are%2520at%2520the%2520forefront%2520of%2520this%250Atechnology%252C%2520focusing%2520on%2520their%2520applicability%2520in%2520architectural%2520design%2520and%250Avisualization.%2520They%2520streamline%2520the%2520creation%2520of%25203D%2520architectural%2520models%252C%250Aenabling%2520rapid%2520prototyping%2520and%2520detailed%2520visualization%2520from%2520minimal%2520initial%250Ainputs%252C%2520such%2520as%2520photographs%2520or%2520simple%2520sketches.One-2-3-45%2520leverages%2520a%250Adiffusion-based%2520approach%2520to%2520generate%2520multi-view%2520reconstructions%252C%2520ensuring%2520high%250Ageometric%2520fidelity%2520and%2520texture%2520quality.%2520CRM%2520utilizes%2520a%2520convolutional%2520network%2520to%250Aintegrate%2520geometric%2520priors%2520into%2520its%2520architecture%252C%2520producing%2520detailed%2520and%250Atextured%2520meshes%2520quickly%2520and%2520efficiently.%2520Instant%2520Mesh%2520combines%2520the%2520strengths%2520of%250Amulti-view%2520diffusion%2520and%2520sparse-view%2520models%2520to%2520offer%2520speed%2520and%2520scalability%252C%250Asuitable%2520for%2520diverse%2520architectural%2520projects.%2520Image-to-Mesh%2520leverages%2520a%250Agenerative%2520adversarial%2520network%2520%2528GAN%2529%2520to%2520produce%25203D%2520meshes%2520from%2520single%2520images%252C%250Afocusing%2520on%2520maintaining%2520high%2520texture%2520fidelity%2520and%2520geometric%2520accuracy%2520by%250Aincorporating%2520image%2520and%2520depth%2520map%2520data%2520into%2520its%2520training%2520process.%2520It%2520uses%2520a%250Ahybrid%2520approach%2520that%2520combines%2520voxel-based%2520representations%2520with%2520surface%250Areconstruction%2520techniques%2520to%2520ensure%2520detailed%2520and%2520realistic%25203D%2520models.This%250Acomparative%2520study%2520highlights%2520each%2520method%2527s%2520contribution%2520to%2520reducing%2520design%250Acycle%2520times%252C%2520improving%2520accuracy%252C%2520and%2520enabling%2520flexible%2520adaptations%2520to%2520various%250Aarchitectural%2520styles%2520and%2520requirements.%2520By%2520providing%2520architects%2520with%2520powerful%250Atools%2520for%2520rapid%2520visualization%2520and%2520iteration%252C%2520these%2520advancements%2520in%25203D%2520mesh%250Ageneration%2520are%2520set%2520to%2520revolutionize%2520architectural%2520practices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Flat%20to%20Spatial%3A%20Comparison%20of%204%20methods%20constructing%203D%2C%202%20and%0A%20%201/2D%20Models%20from%202D%20Plans%20with%20neural%20networks&entry.906535625=Jacob%20Sam%20and%20Karan%20Patel%20and%20Mike%20Saad&entry.1292438233=%20%20In%20the%20field%20of%20architecture%2C%20the%20conversion%20of%20single%20images%20into%202%20and%201/2D%0Aand%203D%20meshes%20is%20a%20promising%20technology%20that%20enhances%20design%20visualization%20and%0Aefficiency.%20This%20paper%20evaluates%20four%20innovative%20methods%3A%20%22One-2-3-45%2C%22%20%22CRM%3A%0ASingle%20Image%20to%203D%20Textured%20Mesh%20with%20Convolutional%20Reconstruction%20Model%2C%22%0A%22Instant%20Mesh%2C%22%20and%20%22Image-to-Mesh.%22%20These%20methods%20are%20at%20the%20forefront%20of%20this%0Atechnology%2C%20focusing%20on%20their%20applicability%20in%20architectural%20design%20and%0Avisualization.%20They%20streamline%20the%20creation%20of%203D%20architectural%20models%2C%0Aenabling%20rapid%20prototyping%20and%20detailed%20visualization%20from%20minimal%20initial%0Ainputs%2C%20such%20as%20photographs%20or%20simple%20sketches.One-2-3-45%20leverages%20a%0Adiffusion-based%20approach%20to%20generate%20multi-view%20reconstructions%2C%20ensuring%20high%0Ageometric%20fidelity%20and%20texture%20quality.%20CRM%20utilizes%20a%20convolutional%20network%20to%0Aintegrate%20geometric%20priors%20into%20its%20architecture%2C%20producing%20detailed%20and%0Atextured%20meshes%20quickly%20and%20efficiently.%20Instant%20Mesh%20combines%20the%20strengths%20of%0Amulti-view%20diffusion%20and%20sparse-view%20models%20to%20offer%20speed%20and%20scalability%2C%0Asuitable%20for%20diverse%20architectural%20projects.%20Image-to-Mesh%20leverages%20a%0Agenerative%20adversarial%20network%20%28GAN%29%20to%20produce%203D%20meshes%20from%20single%20images%2C%0Afocusing%20on%20maintaining%20high%20texture%20fidelity%20and%20geometric%20accuracy%20by%0Aincorporating%20image%20and%20depth%20map%20data%20into%20its%20training%20process.%20It%20uses%20a%0Ahybrid%20approach%20that%20combines%20voxel-based%20representations%20with%20surface%0Areconstruction%20techniques%20to%20ensure%20detailed%20and%20realistic%203D%20models.This%0Acomparative%20study%20highlights%20each%20method%27s%20contribution%20to%20reducing%20design%0Acycle%20times%2C%20improving%20accuracy%2C%20and%20enabling%20flexible%20adaptations%20to%20various%0Aarchitectural%20styles%20and%20requirements.%20By%20providing%20architects%20with%20powerful%0Atools%20for%20rapid%20visualization%20and%20iteration%2C%20these%20advancements%20in%203D%20mesh%0Ageneration%20are%20set%20to%20revolutionize%20architectural%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19970v1&entry.124074799=Read"},
{"title": "Geospecific View Generation -- Geometry-Context Aware High-resolution\n  Ground View Inference from Satellite Views", "author": "Ningli Xu and Rongjun Qin", "abstract": "  Predicting realistic ground views from satellite imagery in urban scenes is a\nchallenging task due to the significant view gaps between satellite and\nground-view images. We propose a novel pipeline to tackle this challenge, by\ngenerating geospecifc views that maximally respect the weak geometry and\ntexture from multi-view satellite images. Different from existing approaches\nthat hallucinate images from cues such as partial semantics or geometry from\noverhead satellite images, our method directly predicts ground-view images at\ngeolocation by using a comprehensive set of information from the satellite\nimage, resulting in ground-level images with a resolution boost at a factor of\nten or more. We leverage a novel building refinement method to reduce geometric\ndistortions in satellite data at ground level, which ensures the creation of\naccurate conditions for view synthesis using diffusion networks. Moreover, we\nproposed a novel geospecific prior, which prompts distribution learning of\ndiffusion models to respect image samples that are closer to the geolocation of\nthe predicted images. We demonstrate our pipeline is the first to generate\nclose-to-real and geospecific ground views merely based on satellite images.\n", "link": "http://arxiv.org/abs/2407.08061v2", "date": "2024-07-29", "relevancy": 3.0179, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6149}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6149}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geospecific%20View%20Generation%20--%20Geometry-Context%20Aware%20High-resolution%0A%20%20Ground%20View%20Inference%20from%20Satellite%20Views&body=Title%3A%20Geospecific%20View%20Generation%20--%20Geometry-Context%20Aware%20High-resolution%0A%20%20Ground%20View%20Inference%20from%20Satellite%20Views%0AAuthor%3A%20Ningli%20Xu%20and%20Rongjun%20Qin%0AAbstract%3A%20%20%20Predicting%20realistic%20ground%20views%20from%20satellite%20imagery%20in%20urban%20scenes%20is%20a%0Achallenging%20task%20due%20to%20the%20significant%20view%20gaps%20between%20satellite%20and%0Aground-view%20images.%20We%20propose%20a%20novel%20pipeline%20to%20tackle%20this%20challenge%2C%20by%0Agenerating%20geospecifc%20views%20that%20maximally%20respect%20the%20weak%20geometry%20and%0Atexture%20from%20multi-view%20satellite%20images.%20Different%20from%20existing%20approaches%0Athat%20hallucinate%20images%20from%20cues%20such%20as%20partial%20semantics%20or%20geometry%20from%0Aoverhead%20satellite%20images%2C%20our%20method%20directly%20predicts%20ground-view%20images%20at%0Ageolocation%20by%20using%20a%20comprehensive%20set%20of%20information%20from%20the%20satellite%0Aimage%2C%20resulting%20in%20ground-level%20images%20with%20a%20resolution%20boost%20at%20a%20factor%20of%0Aten%20or%20more.%20We%20leverage%20a%20novel%20building%20refinement%20method%20to%20reduce%20geometric%0Adistortions%20in%20satellite%20data%20at%20ground%20level%2C%20which%20ensures%20the%20creation%20of%0Aaccurate%20conditions%20for%20view%20synthesis%20using%20diffusion%20networks.%20Moreover%2C%20we%0Aproposed%20a%20novel%20geospecific%20prior%2C%20which%20prompts%20distribution%20learning%20of%0Adiffusion%20models%20to%20respect%20image%20samples%20that%20are%20closer%20to%20the%20geolocation%20of%0Athe%20predicted%20images.%20We%20demonstrate%20our%20pipeline%20is%20the%20first%20to%20generate%0Aclose-to-real%20and%20geospecific%20ground%20views%20merely%20based%20on%20satellite%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeospecific%2520View%2520Generation%2520--%2520Geometry-Context%2520Aware%2520High-resolution%250A%2520%2520Ground%2520View%2520Inference%2520from%2520Satellite%2520Views%26entry.906535625%3DNingli%2520Xu%2520and%2520Rongjun%2520Qin%26entry.1292438233%3D%2520%2520Predicting%2520realistic%2520ground%2520views%2520from%2520satellite%2520imagery%2520in%2520urban%2520scenes%2520is%2520a%250Achallenging%2520task%2520due%2520to%2520the%2520significant%2520view%2520gaps%2520between%2520satellite%2520and%250Aground-view%2520images.%2520We%2520propose%2520a%2520novel%2520pipeline%2520to%2520tackle%2520this%2520challenge%252C%2520by%250Agenerating%2520geospecifc%2520views%2520that%2520maximally%2520respect%2520the%2520weak%2520geometry%2520and%250Atexture%2520from%2520multi-view%2520satellite%2520images.%2520Different%2520from%2520existing%2520approaches%250Athat%2520hallucinate%2520images%2520from%2520cues%2520such%2520as%2520partial%2520semantics%2520or%2520geometry%2520from%250Aoverhead%2520satellite%2520images%252C%2520our%2520method%2520directly%2520predicts%2520ground-view%2520images%2520at%250Ageolocation%2520by%2520using%2520a%2520comprehensive%2520set%2520of%2520information%2520from%2520the%2520satellite%250Aimage%252C%2520resulting%2520in%2520ground-level%2520images%2520with%2520a%2520resolution%2520boost%2520at%2520a%2520factor%2520of%250Aten%2520or%2520more.%2520We%2520leverage%2520a%2520novel%2520building%2520refinement%2520method%2520to%2520reduce%2520geometric%250Adistortions%2520in%2520satellite%2520data%2520at%2520ground%2520level%252C%2520which%2520ensures%2520the%2520creation%2520of%250Aaccurate%2520conditions%2520for%2520view%2520synthesis%2520using%2520diffusion%2520networks.%2520Moreover%252C%2520we%250Aproposed%2520a%2520novel%2520geospecific%2520prior%252C%2520which%2520prompts%2520distribution%2520learning%2520of%250Adiffusion%2520models%2520to%2520respect%2520image%2520samples%2520that%2520are%2520closer%2520to%2520the%2520geolocation%2520of%250Athe%2520predicted%2520images.%2520We%2520demonstrate%2520our%2520pipeline%2520is%2520the%2520first%2520to%2520generate%250Aclose-to-real%2520and%2520geospecific%2520ground%2520views%2520merely%2520based%2520on%2520satellite%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geospecific%20View%20Generation%20--%20Geometry-Context%20Aware%20High-resolution%0A%20%20Ground%20View%20Inference%20from%20Satellite%20Views&entry.906535625=Ningli%20Xu%20and%20Rongjun%20Qin&entry.1292438233=%20%20Predicting%20realistic%20ground%20views%20from%20satellite%20imagery%20in%20urban%20scenes%20is%20a%0Achallenging%20task%20due%20to%20the%20significant%20view%20gaps%20between%20satellite%20and%0Aground-view%20images.%20We%20propose%20a%20novel%20pipeline%20to%20tackle%20this%20challenge%2C%20by%0Agenerating%20geospecifc%20views%20that%20maximally%20respect%20the%20weak%20geometry%20and%0Atexture%20from%20multi-view%20satellite%20images.%20Different%20from%20existing%20approaches%0Athat%20hallucinate%20images%20from%20cues%20such%20as%20partial%20semantics%20or%20geometry%20from%0Aoverhead%20satellite%20images%2C%20our%20method%20directly%20predicts%20ground-view%20images%20at%0Ageolocation%20by%20using%20a%20comprehensive%20set%20of%20information%20from%20the%20satellite%0Aimage%2C%20resulting%20in%20ground-level%20images%20with%20a%20resolution%20boost%20at%20a%20factor%20of%0Aten%20or%20more.%20We%20leverage%20a%20novel%20building%20refinement%20method%20to%20reduce%20geometric%0Adistortions%20in%20satellite%20data%20at%20ground%20level%2C%20which%20ensures%20the%20creation%20of%0Aaccurate%20conditions%20for%20view%20synthesis%20using%20diffusion%20networks.%20Moreover%2C%20we%0Aproposed%20a%20novel%20geospecific%20prior%2C%20which%20prompts%20distribution%20learning%20of%0Adiffusion%20models%20to%20respect%20image%20samples%20that%20are%20closer%20to%20the%20geolocation%20of%0Athe%20predicted%20images.%20We%20demonstrate%20our%20pipeline%20is%20the%20first%20to%20generate%0Aclose-to-real%20and%20geospecific%20ground%20views%20merely%20based%20on%20satellite%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08061v2&entry.124074799=Read"},
{"title": "Trimming the Fat: Efficient Compression of 3D Gaussian Splats through\n  Pruning", "author": "Muhammad Salman Ali and Maryam Qamar and Sung-Ho Bae and Enzo Tartaglione", "abstract": "  In recent times, the utilization of 3D models has gained traction, owing to\nthe capacity for end-to-end training initially offered by Neural Radiance\nFields and more recently by 3D Gaussian Splatting (3DGS) models. The latter\nholds a significant advantage by inherently easing rapid convergence during\ntraining and offering extensive editability. However, despite rapid\nadvancements, the literature still lives in its infancy regarding the\nscalability of these models. In this study, we take some initial steps in\naddressing this gap, showing an approach that enables both the memory and\ncomputational scalability of such models. Specifically, we propose \"Trimming\nthe fat\", a post-hoc gradient-informed iterative pruning technique to eliminate\nredundant information encoded in the model. Our experimental findings on widely\nacknowledged benchmarks attest to the effectiveness of our approach, revealing\nthat up to 75% of the Gaussians can be removed while maintaining or even\nimproving upon baseline performance. Our approach achieves around 50$\\times$\ncompression while preserving performance similar to the baseline model, and is\nable to speed-up computation up to 600 FPS.\n", "link": "http://arxiv.org/abs/2406.18214v2", "date": "2024-07-29", "relevancy": 3.0104, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6669}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6029}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning&body=Title%3A%20Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning%0AAuthor%3A%20Muhammad%20Salman%20Ali%20and%20Maryam%20Qamar%20and%20Sung-Ho%20Bae%20and%20Enzo%20Tartaglione%0AAbstract%3A%20%20%20In%20recent%20times%2C%20the%20utilization%20of%203D%20models%20has%20gained%20traction%2C%20owing%20to%0Athe%20capacity%20for%20end-to-end%20training%20initially%20offered%20by%20Neural%20Radiance%0AFields%20and%20more%20recently%20by%203D%20Gaussian%20Splatting%20%283DGS%29%20models.%20The%20latter%0Aholds%20a%20significant%20advantage%20by%20inherently%20easing%20rapid%20convergence%20during%0Atraining%20and%20offering%20extensive%20editability.%20However%2C%20despite%20rapid%0Aadvancements%2C%20the%20literature%20still%20lives%20in%20its%20infancy%20regarding%20the%0Ascalability%20of%20these%20models.%20In%20this%20study%2C%20we%20take%20some%20initial%20steps%20in%0Aaddressing%20this%20gap%2C%20showing%20an%20approach%20that%20enables%20both%20the%20memory%20and%0Acomputational%20scalability%20of%20such%20models.%20Specifically%2C%20we%20propose%20%22Trimming%0Athe%20fat%22%2C%20a%20post-hoc%20gradient-informed%20iterative%20pruning%20technique%20to%20eliminate%0Aredundant%20information%20encoded%20in%20the%20model.%20Our%20experimental%20findings%20on%20widely%0Aacknowledged%20benchmarks%20attest%20to%20the%20effectiveness%20of%20our%20approach%2C%20revealing%0Athat%20up%20to%2075%25%20of%20the%20Gaussians%20can%20be%20removed%20while%20maintaining%20or%20even%0Aimproving%20upon%20baseline%20performance.%20Our%20approach%20achieves%20around%2050%24%5Ctimes%24%0Acompression%20while%20preserving%20performance%20similar%20to%20the%20baseline%20model%2C%20and%20is%0Aable%20to%20speed-up%20computation%20up%20to%20600%20FPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrimming%2520the%2520Fat%253A%2520Efficient%2520Compression%2520of%25203D%2520Gaussian%2520Splats%2520through%250A%2520%2520Pruning%26entry.906535625%3DMuhammad%2520Salman%2520Ali%2520and%2520Maryam%2520Qamar%2520and%2520Sung-Ho%2520Bae%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520the%2520utilization%2520of%25203D%2520models%2520has%2520gained%2520traction%252C%2520owing%2520to%250Athe%2520capacity%2520for%2520end-to-end%2520training%2520initially%2520offered%2520by%2520Neural%2520Radiance%250AFields%2520and%2520more%2520recently%2520by%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520models.%2520The%2520latter%250Aholds%2520a%2520significant%2520advantage%2520by%2520inherently%2520easing%2520rapid%2520convergence%2520during%250Atraining%2520and%2520offering%2520extensive%2520editability.%2520However%252C%2520despite%2520rapid%250Aadvancements%252C%2520the%2520literature%2520still%2520lives%2520in%2520its%2520infancy%2520regarding%2520the%250Ascalability%2520of%2520these%2520models.%2520In%2520this%2520study%252C%2520we%2520take%2520some%2520initial%2520steps%2520in%250Aaddressing%2520this%2520gap%252C%2520showing%2520an%2520approach%2520that%2520enables%2520both%2520the%2520memory%2520and%250Acomputational%2520scalability%2520of%2520such%2520models.%2520Specifically%252C%2520we%2520propose%2520%2522Trimming%250Athe%2520fat%2522%252C%2520a%2520post-hoc%2520gradient-informed%2520iterative%2520pruning%2520technique%2520to%2520eliminate%250Aredundant%2520information%2520encoded%2520in%2520the%2520model.%2520Our%2520experimental%2520findings%2520on%2520widely%250Aacknowledged%2520benchmarks%2520attest%2520to%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520revealing%250Athat%2520up%2520to%252075%2525%2520of%2520the%2520Gaussians%2520can%2520be%2520removed%2520while%2520maintaining%2520or%2520even%250Aimproving%2520upon%2520baseline%2520performance.%2520Our%2520approach%2520achieves%2520around%252050%2524%255Ctimes%2524%250Acompression%2520while%2520preserving%2520performance%2520similar%2520to%2520the%2520baseline%2520model%252C%2520and%2520is%250Aable%2520to%2520speed-up%2520computation%2520up%2520to%2520600%2520FPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning&entry.906535625=Muhammad%20Salman%20Ali%20and%20Maryam%20Qamar%20and%20Sung-Ho%20Bae%20and%20Enzo%20Tartaglione&entry.1292438233=%20%20In%20recent%20times%2C%20the%20utilization%20of%203D%20models%20has%20gained%20traction%2C%20owing%20to%0Athe%20capacity%20for%20end-to-end%20training%20initially%20offered%20by%20Neural%20Radiance%0AFields%20and%20more%20recently%20by%203D%20Gaussian%20Splatting%20%283DGS%29%20models.%20The%20latter%0Aholds%20a%20significant%20advantage%20by%20inherently%20easing%20rapid%20convergence%20during%0Atraining%20and%20offering%20extensive%20editability.%20However%2C%20despite%20rapid%0Aadvancements%2C%20the%20literature%20still%20lives%20in%20its%20infancy%20regarding%20the%0Ascalability%20of%20these%20models.%20In%20this%20study%2C%20we%20take%20some%20initial%20steps%20in%0Aaddressing%20this%20gap%2C%20showing%20an%20approach%20that%20enables%20both%20the%20memory%20and%0Acomputational%20scalability%20of%20such%20models.%20Specifically%2C%20we%20propose%20%22Trimming%0Athe%20fat%22%2C%20a%20post-hoc%20gradient-informed%20iterative%20pruning%20technique%20to%20eliminate%0Aredundant%20information%20encoded%20in%20the%20model.%20Our%20experimental%20findings%20on%20widely%0Aacknowledged%20benchmarks%20attest%20to%20the%20effectiveness%20of%20our%20approach%2C%20revealing%0Athat%20up%20to%2075%25%20of%20the%20Gaussians%20can%20be%20removed%20while%20maintaining%20or%20even%0Aimproving%20upon%20baseline%20performance.%20Our%20approach%20achieves%20around%2050%24%5Ctimes%24%0Acompression%20while%20preserving%20performance%20similar%20to%20the%20baseline%20model%2C%20and%20is%0Aable%20to%20speed-up%20computation%20up%20to%20600%20FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18214v2&entry.124074799=Read"},
{"title": "Improving 2D Feature Representations by 3D-Aware Fine-Tuning", "author": "Yuanwen Yue and Anurag Das and Francis Engelmann and Siyu Tang and Jan Eric Lenssen", "abstract": "  Current visual foundation models are trained purely on unstructured 2D data,\nlimiting their understanding of 3D structure of objects and scenes. In this\nwork, we show that fine-tuning on 3D-aware data improves the quality of\nemerging semantic features. We design a method to lift semantic 2D features\ninto an efficient 3D Gaussian representation, which allows us to re-render them\nfor arbitrary views. Using the rendered 3D-aware features, we design a\nfine-tuning strategy to transfer such 3D awareness into a 2D foundation model.\nWe demonstrate that models fine-tuned in that way produce features that readily\nimprove downstream task performance in semantic segmentation and depth\nestimation through simple linear probing. Notably, though fined-tuned on a\nsingle indoor dataset, the improvement is transferable to a variety of indoor\ndatasets and out-of-domain datasets. We hope our study encourages the community\nto consider injecting 3D awareness when training 2D foundation models. Project\npage: https://ywyue.github.io/FiT3D.\n", "link": "http://arxiv.org/abs/2407.20229v1", "date": "2024-07-29", "relevancy": 2.9683, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5993}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5908}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%202D%20Feature%20Representations%20by%203D-Aware%20Fine-Tuning&body=Title%3A%20Improving%202D%20Feature%20Representations%20by%203D-Aware%20Fine-Tuning%0AAuthor%3A%20Yuanwen%20Yue%20and%20Anurag%20Das%20and%20Francis%20Engelmann%20and%20Siyu%20Tang%20and%20Jan%20Eric%20Lenssen%0AAbstract%3A%20%20%20Current%20visual%20foundation%20models%20are%20trained%20purely%20on%20unstructured%202D%20data%2C%0Alimiting%20their%20understanding%20of%203D%20structure%20of%20objects%20and%20scenes.%20In%20this%0Awork%2C%20we%20show%20that%20fine-tuning%20on%203D-aware%20data%20improves%20the%20quality%20of%0Aemerging%20semantic%20features.%20We%20design%20a%20method%20to%20lift%20semantic%202D%20features%0Ainto%20an%20efficient%203D%20Gaussian%20representation%2C%20which%20allows%20us%20to%20re-render%20them%0Afor%20arbitrary%20views.%20Using%20the%20rendered%203D-aware%20features%2C%20we%20design%20a%0Afine-tuning%20strategy%20to%20transfer%20such%203D%20awareness%20into%20a%202D%20foundation%20model.%0AWe%20demonstrate%20that%20models%20fine-tuned%20in%20that%20way%20produce%20features%20that%20readily%0Aimprove%20downstream%20task%20performance%20in%20semantic%20segmentation%20and%20depth%0Aestimation%20through%20simple%20linear%20probing.%20Notably%2C%20though%20fined-tuned%20on%20a%0Asingle%20indoor%20dataset%2C%20the%20improvement%20is%20transferable%20to%20a%20variety%20of%20indoor%0Adatasets%20and%20out-of-domain%20datasets.%20We%20hope%20our%20study%20encourages%20the%20community%0Ato%20consider%20injecting%203D%20awareness%20when%20training%202D%20foundation%20models.%20Project%0Apage%3A%20https%3A//ywyue.github.io/FiT3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%25202D%2520Feature%2520Representations%2520by%25203D-Aware%2520Fine-Tuning%26entry.906535625%3DYuanwen%2520Yue%2520and%2520Anurag%2520Das%2520and%2520Francis%2520Engelmann%2520and%2520Siyu%2520Tang%2520and%2520Jan%2520Eric%2520Lenssen%26entry.1292438233%3D%2520%2520Current%2520visual%2520foundation%2520models%2520are%2520trained%2520purely%2520on%2520unstructured%25202D%2520data%252C%250Alimiting%2520their%2520understanding%2520of%25203D%2520structure%2520of%2520objects%2520and%2520scenes.%2520In%2520this%250Awork%252C%2520we%2520show%2520that%2520fine-tuning%2520on%25203D-aware%2520data%2520improves%2520the%2520quality%2520of%250Aemerging%2520semantic%2520features.%2520We%2520design%2520a%2520method%2520to%2520lift%2520semantic%25202D%2520features%250Ainto%2520an%2520efficient%25203D%2520Gaussian%2520representation%252C%2520which%2520allows%2520us%2520to%2520re-render%2520them%250Afor%2520arbitrary%2520views.%2520Using%2520the%2520rendered%25203D-aware%2520features%252C%2520we%2520design%2520a%250Afine-tuning%2520strategy%2520to%2520transfer%2520such%25203D%2520awareness%2520into%2520a%25202D%2520foundation%2520model.%250AWe%2520demonstrate%2520that%2520models%2520fine-tuned%2520in%2520that%2520way%2520produce%2520features%2520that%2520readily%250Aimprove%2520downstream%2520task%2520performance%2520in%2520semantic%2520segmentation%2520and%2520depth%250Aestimation%2520through%2520simple%2520linear%2520probing.%2520Notably%252C%2520though%2520fined-tuned%2520on%2520a%250Asingle%2520indoor%2520dataset%252C%2520the%2520improvement%2520is%2520transferable%2520to%2520a%2520variety%2520of%2520indoor%250Adatasets%2520and%2520out-of-domain%2520datasets.%2520We%2520hope%2520our%2520study%2520encourages%2520the%2520community%250Ato%2520consider%2520injecting%25203D%2520awareness%2520when%2520training%25202D%2520foundation%2520models.%2520Project%250Apage%253A%2520https%253A//ywyue.github.io/FiT3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%202D%20Feature%20Representations%20by%203D-Aware%20Fine-Tuning&entry.906535625=Yuanwen%20Yue%20and%20Anurag%20Das%20and%20Francis%20Engelmann%20and%20Siyu%20Tang%20and%20Jan%20Eric%20Lenssen&entry.1292438233=%20%20Current%20visual%20foundation%20models%20are%20trained%20purely%20on%20unstructured%202D%20data%2C%0Alimiting%20their%20understanding%20of%203D%20structure%20of%20objects%20and%20scenes.%20In%20this%0Awork%2C%20we%20show%20that%20fine-tuning%20on%203D-aware%20data%20improves%20the%20quality%20of%0Aemerging%20semantic%20features.%20We%20design%20a%20method%20to%20lift%20semantic%202D%20features%0Ainto%20an%20efficient%203D%20Gaussian%20representation%2C%20which%20allows%20us%20to%20re-render%20them%0Afor%20arbitrary%20views.%20Using%20the%20rendered%203D-aware%20features%2C%20we%20design%20a%0Afine-tuning%20strategy%20to%20transfer%20such%203D%20awareness%20into%20a%202D%20foundation%20model.%0AWe%20demonstrate%20that%20models%20fine-tuned%20in%20that%20way%20produce%20features%20that%20readily%0Aimprove%20downstream%20task%20performance%20in%20semantic%20segmentation%20and%20depth%0Aestimation%20through%20simple%20linear%20probing.%20Notably%2C%20though%20fined-tuned%20on%20a%0Asingle%20indoor%20dataset%2C%20the%20improvement%20is%20transferable%20to%20a%20variety%20of%20indoor%0Adatasets%20and%20out-of-domain%20datasets.%20We%20hope%20our%20study%20encourages%20the%20community%0Ato%20consider%20injecting%203D%20awareness%20when%20training%202D%20foundation%20models.%20Project%0Apage%3A%20https%3A//ywyue.github.io/FiT3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20229v1&entry.124074799=Read"},
{"title": "Global Structure-from-Motion Revisited", "author": "Linfei Pan and D\u00e1niel Bar\u00e1th and Marc Pollefeys and Johannes L. Sch\u00f6nberger", "abstract": "  Recovering 3D structure and camera motion from images has been a\nlong-standing focus of computer vision research and is known as\nStructure-from-Motion (SfM). Solutions to this problem are categorized into\nincremental and global approaches. Until now, the most popular systems follow\nthe incremental paradigm due to its superior accuracy and robustness, while\nglobal approaches are drastically more scalable and efficient. With this work,\nwe revisit the problem of global SfM and propose GLOMAP as a new\ngeneral-purpose system that outperforms the state of the art in global SfM. In\nterms of accuracy and robustness, we achieve results on-par or superior to\nCOLMAP, the most widely used incremental SfM, while being orders of magnitude\nfaster. We share our system as an open-source implementation at\n{https://github.com/colmap/glomap}.\n", "link": "http://arxiv.org/abs/2407.20219v1", "date": "2024-07-29", "relevancy": 2.9528, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6356}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5823}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Structure-from-Motion%20Revisited&body=Title%3A%20Global%20Structure-from-Motion%20Revisited%0AAuthor%3A%20Linfei%20Pan%20and%20D%C3%A1niel%20Bar%C3%A1th%20and%20Marc%20Pollefeys%20and%20Johannes%20L.%20Sch%C3%B6nberger%0AAbstract%3A%20%20%20Recovering%203D%20structure%20and%20camera%20motion%20from%20images%20has%20been%20a%0Along-standing%20focus%20of%20computer%20vision%20research%20and%20is%20known%20as%0AStructure-from-Motion%20%28SfM%29.%20Solutions%20to%20this%20problem%20are%20categorized%20into%0Aincremental%20and%20global%20approaches.%20Until%20now%2C%20the%20most%20popular%20systems%20follow%0Athe%20incremental%20paradigm%20due%20to%20its%20superior%20accuracy%20and%20robustness%2C%20while%0Aglobal%20approaches%20are%20drastically%20more%20scalable%20and%20efficient.%20With%20this%20work%2C%0Awe%20revisit%20the%20problem%20of%20global%20SfM%20and%20propose%20GLOMAP%20as%20a%20new%0Ageneral-purpose%20system%20that%20outperforms%20the%20state%20of%20the%20art%20in%20global%20SfM.%20In%0Aterms%20of%20accuracy%20and%20robustness%2C%20we%20achieve%20results%20on-par%20or%20superior%20to%0ACOLMAP%2C%20the%20most%20widely%20used%20incremental%20SfM%2C%20while%20being%20orders%20of%20magnitude%0Afaster.%20We%20share%20our%20system%20as%20an%20open-source%20implementation%20at%0A%7Bhttps%3A//github.com/colmap/glomap%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Structure-from-Motion%2520Revisited%26entry.906535625%3DLinfei%2520Pan%2520and%2520D%25C3%25A1niel%2520Bar%25C3%25A1th%2520and%2520Marc%2520Pollefeys%2520and%2520Johannes%2520L.%2520Sch%25C3%25B6nberger%26entry.1292438233%3D%2520%2520Recovering%25203D%2520structure%2520and%2520camera%2520motion%2520from%2520images%2520has%2520been%2520a%250Along-standing%2520focus%2520of%2520computer%2520vision%2520research%2520and%2520is%2520known%2520as%250AStructure-from-Motion%2520%2528SfM%2529.%2520Solutions%2520to%2520this%2520problem%2520are%2520categorized%2520into%250Aincremental%2520and%2520global%2520approaches.%2520Until%2520now%252C%2520the%2520most%2520popular%2520systems%2520follow%250Athe%2520incremental%2520paradigm%2520due%2520to%2520its%2520superior%2520accuracy%2520and%2520robustness%252C%2520while%250Aglobal%2520approaches%2520are%2520drastically%2520more%2520scalable%2520and%2520efficient.%2520With%2520this%2520work%252C%250Awe%2520revisit%2520the%2520problem%2520of%2520global%2520SfM%2520and%2520propose%2520GLOMAP%2520as%2520a%2520new%250Ageneral-purpose%2520system%2520that%2520outperforms%2520the%2520state%2520of%2520the%2520art%2520in%2520global%2520SfM.%2520In%250Aterms%2520of%2520accuracy%2520and%2520robustness%252C%2520we%2520achieve%2520results%2520on-par%2520or%2520superior%2520to%250ACOLMAP%252C%2520the%2520most%2520widely%2520used%2520incremental%2520SfM%252C%2520while%2520being%2520orders%2520of%2520magnitude%250Afaster.%2520We%2520share%2520our%2520system%2520as%2520an%2520open-source%2520implementation%2520at%250A%257Bhttps%253A//github.com/colmap/glomap%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Structure-from-Motion%20Revisited&entry.906535625=Linfei%20Pan%20and%20D%C3%A1niel%20Bar%C3%A1th%20and%20Marc%20Pollefeys%20and%20Johannes%20L.%20Sch%C3%B6nberger&entry.1292438233=%20%20Recovering%203D%20structure%20and%20camera%20motion%20from%20images%20has%20been%20a%0Along-standing%20focus%20of%20computer%20vision%20research%20and%20is%20known%20as%0AStructure-from-Motion%20%28SfM%29.%20Solutions%20to%20this%20problem%20are%20categorized%20into%0Aincremental%20and%20global%20approaches.%20Until%20now%2C%20the%20most%20popular%20systems%20follow%0Athe%20incremental%20paradigm%20due%20to%20its%20superior%20accuracy%20and%20robustness%2C%20while%0Aglobal%20approaches%20are%20drastically%20more%20scalable%20and%20efficient.%20With%20this%20work%2C%0Awe%20revisit%20the%20problem%20of%20global%20SfM%20and%20propose%20GLOMAP%20as%20a%20new%0Ageneral-purpose%20system%20that%20outperforms%20the%20state%20of%20the%20art%20in%20global%20SfM.%20In%0Aterms%20of%20accuracy%20and%20robustness%2C%20we%20achieve%20results%20on-par%20or%20superior%20to%0ACOLMAP%2C%20the%20most%20widely%20used%20incremental%20SfM%2C%20while%20being%20orders%20of%20magnitude%0Afaster.%20We%20share%20our%20system%20as%20an%20open-source%20implementation%20at%0A%7Bhttps%3A//github.com/colmap/glomap%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20219v1&entry.124074799=Read"},
{"title": "Registering Neural 4D Gaussians for Endoscopic Surgery", "author": "Yiming Huang and Beilei Cui and Ikemura Kei and Jiekai Zhang and Long Bai and Hongliang Ren", "abstract": "  The recent advance in neural rendering has enabled the ability to reconstruct\nhigh-quality 4D scenes using neural networks. Although 4D neural reconstruction\nis popular, registration for such representations remains a challenging task,\nespecially for dynamic scene registration in surgical planning and simulation.\nIn this paper, we propose a novel strategy for dynamic surgical neural scene\nregistration. We first utilize 4D Gaussian Splatting to represent the surgical\nscene and capture both static and dynamic scenes effectively. Then, a spatial\naware feature aggregation method, Spatially Weight Cluttering (SWC) is proposed\nto accurately align the feature between surgical scenes, enabling precise and\nrealistic surgical simulations. Lastly, we present a novel strategy of\ndeformable scene registration to register two dynamic scenes. By incorporating\nboth spatial and temporal information for correspondence matching, our approach\nachieves superior performance compared to existing registration methods for\nimplicit neural representation. The proposed method has the potential to\nimprove surgical planning and training, ultimately leading to better patient\noutcomes.\n", "link": "http://arxiv.org/abs/2407.20213v1", "date": "2024-07-29", "relevancy": 2.9233, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6119}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5993}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Registering%20Neural%204D%20Gaussians%20for%20Endoscopic%20Surgery&body=Title%3A%20Registering%20Neural%204D%20Gaussians%20for%20Endoscopic%20Surgery%0AAuthor%3A%20Yiming%20Huang%20and%20Beilei%20Cui%20and%20Ikemura%20Kei%20and%20Jiekai%20Zhang%20and%20Long%20Bai%20and%20Hongliang%20Ren%0AAbstract%3A%20%20%20The%20recent%20advance%20in%20neural%20rendering%20has%20enabled%20the%20ability%20to%20reconstruct%0Ahigh-quality%204D%20scenes%20using%20neural%20networks.%20Although%204D%20neural%20reconstruction%0Ais%20popular%2C%20registration%20for%20such%20representations%20remains%20a%20challenging%20task%2C%0Aespecially%20for%20dynamic%20scene%20registration%20in%20surgical%20planning%20and%20simulation.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20strategy%20for%20dynamic%20surgical%20neural%20scene%0Aregistration.%20We%20first%20utilize%204D%20Gaussian%20Splatting%20to%20represent%20the%20surgical%0Ascene%20and%20capture%20both%20static%20and%20dynamic%20scenes%20effectively.%20Then%2C%20a%20spatial%0Aaware%20feature%20aggregation%20method%2C%20Spatially%20Weight%20Cluttering%20%28SWC%29%20is%20proposed%0Ato%20accurately%20align%20the%20feature%20between%20surgical%20scenes%2C%20enabling%20precise%20and%0Arealistic%20surgical%20simulations.%20Lastly%2C%20we%20present%20a%20novel%20strategy%20of%0Adeformable%20scene%20registration%20to%20register%20two%20dynamic%20scenes.%20By%20incorporating%0Aboth%20spatial%20and%20temporal%20information%20for%20correspondence%20matching%2C%20our%20approach%0Aachieves%20superior%20performance%20compared%20to%20existing%20registration%20methods%20for%0Aimplicit%20neural%20representation.%20The%20proposed%20method%20has%20the%20potential%20to%0Aimprove%20surgical%20planning%20and%20training%2C%20ultimately%20leading%20to%20better%20patient%0Aoutcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegistering%2520Neural%25204D%2520Gaussians%2520for%2520Endoscopic%2520Surgery%26entry.906535625%3DYiming%2520Huang%2520and%2520Beilei%2520Cui%2520and%2520Ikemura%2520Kei%2520and%2520Jiekai%2520Zhang%2520and%2520Long%2520Bai%2520and%2520Hongliang%2520Ren%26entry.1292438233%3D%2520%2520The%2520recent%2520advance%2520in%2520neural%2520rendering%2520has%2520enabled%2520the%2520ability%2520to%2520reconstruct%250Ahigh-quality%25204D%2520scenes%2520using%2520neural%2520networks.%2520Although%25204D%2520neural%2520reconstruction%250Ais%2520popular%252C%2520registration%2520for%2520such%2520representations%2520remains%2520a%2520challenging%2520task%252C%250Aespecially%2520for%2520dynamic%2520scene%2520registration%2520in%2520surgical%2520planning%2520and%2520simulation.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520strategy%2520for%2520dynamic%2520surgical%2520neural%2520scene%250Aregistration.%2520We%2520first%2520utilize%25204D%2520Gaussian%2520Splatting%2520to%2520represent%2520the%2520surgical%250Ascene%2520and%2520capture%2520both%2520static%2520and%2520dynamic%2520scenes%2520effectively.%2520Then%252C%2520a%2520spatial%250Aaware%2520feature%2520aggregation%2520method%252C%2520Spatially%2520Weight%2520Cluttering%2520%2528SWC%2529%2520is%2520proposed%250Ato%2520accurately%2520align%2520the%2520feature%2520between%2520surgical%2520scenes%252C%2520enabling%2520precise%2520and%250Arealistic%2520surgical%2520simulations.%2520Lastly%252C%2520we%2520present%2520a%2520novel%2520strategy%2520of%250Adeformable%2520scene%2520registration%2520to%2520register%2520two%2520dynamic%2520scenes.%2520By%2520incorporating%250Aboth%2520spatial%2520and%2520temporal%2520information%2520for%2520correspondence%2520matching%252C%2520our%2520approach%250Aachieves%2520superior%2520performance%2520compared%2520to%2520existing%2520registration%2520methods%2520for%250Aimplicit%2520neural%2520representation.%2520The%2520proposed%2520method%2520has%2520the%2520potential%2520to%250Aimprove%2520surgical%2520planning%2520and%2520training%252C%2520ultimately%2520leading%2520to%2520better%2520patient%250Aoutcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Registering%20Neural%204D%20Gaussians%20for%20Endoscopic%20Surgery&entry.906535625=Yiming%20Huang%20and%20Beilei%20Cui%20and%20Ikemura%20Kei%20and%20Jiekai%20Zhang%20and%20Long%20Bai%20and%20Hongliang%20Ren&entry.1292438233=%20%20The%20recent%20advance%20in%20neural%20rendering%20has%20enabled%20the%20ability%20to%20reconstruct%0Ahigh-quality%204D%20scenes%20using%20neural%20networks.%20Although%204D%20neural%20reconstruction%0Ais%20popular%2C%20registration%20for%20such%20representations%20remains%20a%20challenging%20task%2C%0Aespecially%20for%20dynamic%20scene%20registration%20in%20surgical%20planning%20and%20simulation.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20strategy%20for%20dynamic%20surgical%20neural%20scene%0Aregistration.%20We%20first%20utilize%204D%20Gaussian%20Splatting%20to%20represent%20the%20surgical%0Ascene%20and%20capture%20both%20static%20and%20dynamic%20scenes%20effectively.%20Then%2C%20a%20spatial%0Aaware%20feature%20aggregation%20method%2C%20Spatially%20Weight%20Cluttering%20%28SWC%29%20is%20proposed%0Ato%20accurately%20align%20the%20feature%20between%20surgical%20scenes%2C%20enabling%20precise%20and%0Arealistic%20surgical%20simulations.%20Lastly%2C%20we%20present%20a%20novel%20strategy%20of%0Adeformable%20scene%20registration%20to%20register%20two%20dynamic%20scenes.%20By%20incorporating%0Aboth%20spatial%20and%20temporal%20information%20for%20correspondence%20matching%2C%20our%20approach%0Aachieves%20superior%20performance%20compared%20to%20existing%20registration%20methods%20for%0Aimplicit%20neural%20representation.%20The%20proposed%20method%20has%20the%20potential%20to%0Aimprove%20surgical%20planning%20and%20training%2C%20ultimately%20leading%20to%20better%20patient%0Aoutcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20213v1&entry.124074799=Read"},
{"title": "Point2Building: Reconstructing Buildings from Airborne LiDAR Point\n  Clouds", "author": "Yujia Liu and Anton Obukhov and Jan Dirk Wegner and Konrad Schindler", "abstract": "  We present a learning-based approach to reconstruct buildings as 3D polygonal\nmeshes from airborne LiDAR point clouds. What makes 3D building reconstruction\nfrom airborne LiDAR hard is the large diversity of building designs and\nespecially roof shapes, the low and varying point density across the scene, and\nthe often incomplete coverage of building facades due to occlusions by\nvegetation or to the viewing angle of the sensor. To cope with the diversity of\nshapes and inhomogeneous and incomplete object coverage, we introduce a\ngenerative model that directly predicts 3D polygonal meshes from input point\nclouds. Our autoregressive model, called Point2Building, iteratively builds up\nthe mesh by generating sequences of vertices and faces. This approach enables\nour model to adapt flexibly to diverse geometries and building structures.\nUnlike many existing methods that rely heavily on pre-processing steps like\nexhaustive plane detection, our model learns directly from the point cloud\ndata, thereby reducing error propagation and increasing the fidelity of the\nreconstruction. We experimentally validate our method on a collection of\nairborne LiDAR data of Zurich, Berlin and Tallinn. Our method shows good\ngeneralization to diverse urban styles.\n", "link": "http://arxiv.org/abs/2403.02136v2", "date": "2024-07-29", "relevancy": 2.8396, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5735}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5651}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point2Building%3A%20Reconstructing%20Buildings%20from%20Airborne%20LiDAR%20Point%0A%20%20Clouds&body=Title%3A%20Point2Building%3A%20Reconstructing%20Buildings%20from%20Airborne%20LiDAR%20Point%0A%20%20Clouds%0AAuthor%3A%20Yujia%20Liu%20and%20Anton%20Obukhov%20and%20Jan%20Dirk%20Wegner%20and%20Konrad%20Schindler%0AAbstract%3A%20%20%20We%20present%20a%20learning-based%20approach%20to%20reconstruct%20buildings%20as%203D%20polygonal%0Ameshes%20from%20airborne%20LiDAR%20point%20clouds.%20What%20makes%203D%20building%20reconstruction%0Afrom%20airborne%20LiDAR%20hard%20is%20the%20large%20diversity%20of%20building%20designs%20and%0Aespecially%20roof%20shapes%2C%20the%20low%20and%20varying%20point%20density%20across%20the%20scene%2C%20and%0Athe%20often%20incomplete%20coverage%20of%20building%20facades%20due%20to%20occlusions%20by%0Avegetation%20or%20to%20the%20viewing%20angle%20of%20the%20sensor.%20To%20cope%20with%20the%20diversity%20of%0Ashapes%20and%20inhomogeneous%20and%20incomplete%20object%20coverage%2C%20we%20introduce%20a%0Agenerative%20model%20that%20directly%20predicts%203D%20polygonal%20meshes%20from%20input%20point%0Aclouds.%20Our%20autoregressive%20model%2C%20called%20Point2Building%2C%20iteratively%20builds%20up%0Athe%20mesh%20by%20generating%20sequences%20of%20vertices%20and%20faces.%20This%20approach%20enables%0Aour%20model%20to%20adapt%20flexibly%20to%20diverse%20geometries%20and%20building%20structures.%0AUnlike%20many%20existing%20methods%20that%20rely%20heavily%20on%20pre-processing%20steps%20like%0Aexhaustive%20plane%20detection%2C%20our%20model%20learns%20directly%20from%20the%20point%20cloud%0Adata%2C%20thereby%20reducing%20error%20propagation%20and%20increasing%20the%20fidelity%20of%20the%0Areconstruction.%20We%20experimentally%20validate%20our%20method%20on%20a%20collection%20of%0Aairborne%20LiDAR%20data%20of%20Zurich%2C%20Berlin%20and%20Tallinn.%20Our%20method%20shows%20good%0Ageneralization%20to%20diverse%20urban%20styles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02136v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint2Building%253A%2520Reconstructing%2520Buildings%2520from%2520Airborne%2520LiDAR%2520Point%250A%2520%2520Clouds%26entry.906535625%3DYujia%2520Liu%2520and%2520Anton%2520Obukhov%2520and%2520Jan%2520Dirk%2520Wegner%2520and%2520Konrad%2520Schindler%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520learning-based%2520approach%2520to%2520reconstruct%2520buildings%2520as%25203D%2520polygonal%250Ameshes%2520from%2520airborne%2520LiDAR%2520point%2520clouds.%2520What%2520makes%25203D%2520building%2520reconstruction%250Afrom%2520airborne%2520LiDAR%2520hard%2520is%2520the%2520large%2520diversity%2520of%2520building%2520designs%2520and%250Aespecially%2520roof%2520shapes%252C%2520the%2520low%2520and%2520varying%2520point%2520density%2520across%2520the%2520scene%252C%2520and%250Athe%2520often%2520incomplete%2520coverage%2520of%2520building%2520facades%2520due%2520to%2520occlusions%2520by%250Avegetation%2520or%2520to%2520the%2520viewing%2520angle%2520of%2520the%2520sensor.%2520To%2520cope%2520with%2520the%2520diversity%2520of%250Ashapes%2520and%2520inhomogeneous%2520and%2520incomplete%2520object%2520coverage%252C%2520we%2520introduce%2520a%250Agenerative%2520model%2520that%2520directly%2520predicts%25203D%2520polygonal%2520meshes%2520from%2520input%2520point%250Aclouds.%2520Our%2520autoregressive%2520model%252C%2520called%2520Point2Building%252C%2520iteratively%2520builds%2520up%250Athe%2520mesh%2520by%2520generating%2520sequences%2520of%2520vertices%2520and%2520faces.%2520This%2520approach%2520enables%250Aour%2520model%2520to%2520adapt%2520flexibly%2520to%2520diverse%2520geometries%2520and%2520building%2520structures.%250AUnlike%2520many%2520existing%2520methods%2520that%2520rely%2520heavily%2520on%2520pre-processing%2520steps%2520like%250Aexhaustive%2520plane%2520detection%252C%2520our%2520model%2520learns%2520directly%2520from%2520the%2520point%2520cloud%250Adata%252C%2520thereby%2520reducing%2520error%2520propagation%2520and%2520increasing%2520the%2520fidelity%2520of%2520the%250Areconstruction.%2520We%2520experimentally%2520validate%2520our%2520method%2520on%2520a%2520collection%2520of%250Aairborne%2520LiDAR%2520data%2520of%2520Zurich%252C%2520Berlin%2520and%2520Tallinn.%2520Our%2520method%2520shows%2520good%250Ageneralization%2520to%2520diverse%2520urban%2520styles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02136v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point2Building%3A%20Reconstructing%20Buildings%20from%20Airborne%20LiDAR%20Point%0A%20%20Clouds&entry.906535625=Yujia%20Liu%20and%20Anton%20Obukhov%20and%20Jan%20Dirk%20Wegner%20and%20Konrad%20Schindler&entry.1292438233=%20%20We%20present%20a%20learning-based%20approach%20to%20reconstruct%20buildings%20as%203D%20polygonal%0Ameshes%20from%20airborne%20LiDAR%20point%20clouds.%20What%20makes%203D%20building%20reconstruction%0Afrom%20airborne%20LiDAR%20hard%20is%20the%20large%20diversity%20of%20building%20designs%20and%0Aespecially%20roof%20shapes%2C%20the%20low%20and%20varying%20point%20density%20across%20the%20scene%2C%20and%0Athe%20often%20incomplete%20coverage%20of%20building%20facades%20due%20to%20occlusions%20by%0Avegetation%20or%20to%20the%20viewing%20angle%20of%20the%20sensor.%20To%20cope%20with%20the%20diversity%20of%0Ashapes%20and%20inhomogeneous%20and%20incomplete%20object%20coverage%2C%20we%20introduce%20a%0Agenerative%20model%20that%20directly%20predicts%203D%20polygonal%20meshes%20from%20input%20point%0Aclouds.%20Our%20autoregressive%20model%2C%20called%20Point2Building%2C%20iteratively%20builds%20up%0Athe%20mesh%20by%20generating%20sequences%20of%20vertices%20and%20faces.%20This%20approach%20enables%0Aour%20model%20to%20adapt%20flexibly%20to%20diverse%20geometries%20and%20building%20structures.%0AUnlike%20many%20existing%20methods%20that%20rely%20heavily%20on%20pre-processing%20steps%20like%0Aexhaustive%20plane%20detection%2C%20our%20model%20learns%20directly%20from%20the%20point%20cloud%0Adata%2C%20thereby%20reducing%20error%20propagation%20and%20increasing%20the%20fidelity%20of%20the%0Areconstruction.%20We%20experimentally%20validate%20our%20method%20on%20a%20collection%20of%0Aairborne%20LiDAR%20data%20of%20Zurich%2C%20Berlin%20and%20Tallinn.%20Our%20method%20shows%20good%0Ageneralization%20to%20diverse%20urban%20styles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02136v2&entry.124074799=Read"},
{"title": "Reproducibility Study of \"ITI-GEN: Inclusive Text-to-Image Generation\"", "author": "Daniel Gallo Fern\u00e1ndez and R\u0103zvan-Andrei Matisan and Alejandro Monroy Mu\u00f1oz and Janusz Partyka", "abstract": "  Text-to-image generative models often present issues regarding fairness with\nrespect to certain sensitive attributes, such as gender or skin tone. This\nstudy aims to reproduce the results presented in \"ITI-GEN: Inclusive\nText-to-Image Generation\" by Zhang et al. (2023a), which introduces a model to\nimprove inclusiveness in these kinds of models. We show that most of the claims\nmade by the authors about ITI-GEN hold: it improves the diversity and quality\nof generated images, it is scalable to different domains, it has plug-and-play\ncapabilities, and it is efficient from a computational point of view. However,\nITI-GEN sometimes uses undesired attributes as proxy features and it is unable\nto disentangle some pairs of (correlated) attributes such as gender and\nbaldness. In addition, when the number of considered attributes increases, the\ntraining time grows exponentially and ITI-GEN struggles to generate inclusive\nimages for all elements in the joint distribution. To solve these issues, we\npropose using Hard Prompt Search with negative prompting, a method that does\nnot require training and that handles negation better than vanilla Hard Prompt\nSearch. Nonetheless, Hard Prompt Search (with or without negative prompting)\ncannot be used for continuous attributes that are hard to express in natural\nlanguage, an area where ITI-GEN excels as it is guided by images during\ntraining. Finally, we propose combining ITI-GEN and Hard Prompt Search with\nnegative prompting.\n", "link": "http://arxiv.org/abs/2407.19996v1", "date": "2024-07-29", "relevancy": 2.8041, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5849}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5508}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reproducibility%20Study%20of%20%22ITI-GEN%3A%20Inclusive%20Text-to-Image%20Generation%22&body=Title%3A%20Reproducibility%20Study%20of%20%22ITI-GEN%3A%20Inclusive%20Text-to-Image%20Generation%22%0AAuthor%3A%20Daniel%20Gallo%20Fern%C3%A1ndez%20and%20R%C4%83zvan-Andrei%20Matisan%20and%20Alejandro%20Monroy%20Mu%C3%B1oz%20and%20Janusz%20Partyka%0AAbstract%3A%20%20%20Text-to-image%20generative%20models%20often%20present%20issues%20regarding%20fairness%20with%0Arespect%20to%20certain%20sensitive%20attributes%2C%20such%20as%20gender%20or%20skin%20tone.%20This%0Astudy%20aims%20to%20reproduce%20the%20results%20presented%20in%20%22ITI-GEN%3A%20Inclusive%0AText-to-Image%20Generation%22%20by%20Zhang%20et%20al.%20%282023a%29%2C%20which%20introduces%20a%20model%20to%0Aimprove%20inclusiveness%20in%20these%20kinds%20of%20models.%20We%20show%20that%20most%20of%20the%20claims%0Amade%20by%20the%20authors%20about%20ITI-GEN%20hold%3A%20it%20improves%20the%20diversity%20and%20quality%0Aof%20generated%20images%2C%20it%20is%20scalable%20to%20different%20domains%2C%20it%20has%20plug-and-play%0Acapabilities%2C%20and%20it%20is%20efficient%20from%20a%20computational%20point%20of%20view.%20However%2C%0AITI-GEN%20sometimes%20uses%20undesired%20attributes%20as%20proxy%20features%20and%20it%20is%20unable%0Ato%20disentangle%20some%20pairs%20of%20%28correlated%29%20attributes%20such%20as%20gender%20and%0Abaldness.%20In%20addition%2C%20when%20the%20number%20of%20considered%20attributes%20increases%2C%20the%0Atraining%20time%20grows%20exponentially%20and%20ITI-GEN%20struggles%20to%20generate%20inclusive%0Aimages%20for%20all%20elements%20in%20the%20joint%20distribution.%20To%20solve%20these%20issues%2C%20we%0Apropose%20using%20Hard%20Prompt%20Search%20with%20negative%20prompting%2C%20a%20method%20that%20does%0Anot%20require%20training%20and%20that%20handles%20negation%20better%20than%20vanilla%20Hard%20Prompt%0ASearch.%20Nonetheless%2C%20Hard%20Prompt%20Search%20%28with%20or%20without%20negative%20prompting%29%0Acannot%20be%20used%20for%20continuous%20attributes%20that%20are%20hard%20to%20express%20in%20natural%0Alanguage%2C%20an%20area%20where%20ITI-GEN%20excels%20as%20it%20is%20guided%20by%20images%20during%0Atraining.%20Finally%2C%20we%20propose%20combining%20ITI-GEN%20and%20Hard%20Prompt%20Search%20with%0Anegative%20prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReproducibility%2520Study%2520of%2520%2522ITI-GEN%253A%2520Inclusive%2520Text-to-Image%2520Generation%2522%26entry.906535625%3DDaniel%2520Gallo%2520Fern%25C3%25A1ndez%2520and%2520R%25C4%2583zvan-Andrei%2520Matisan%2520and%2520Alejandro%2520Monroy%2520Mu%25C3%25B1oz%2520and%2520Janusz%2520Partyka%26entry.1292438233%3D%2520%2520Text-to-image%2520generative%2520models%2520often%2520present%2520issues%2520regarding%2520fairness%2520with%250Arespect%2520to%2520certain%2520sensitive%2520attributes%252C%2520such%2520as%2520gender%2520or%2520skin%2520tone.%2520This%250Astudy%2520aims%2520to%2520reproduce%2520the%2520results%2520presented%2520in%2520%2522ITI-GEN%253A%2520Inclusive%250AText-to-Image%2520Generation%2522%2520by%2520Zhang%2520et%2520al.%2520%25282023a%2529%252C%2520which%2520introduces%2520a%2520model%2520to%250Aimprove%2520inclusiveness%2520in%2520these%2520kinds%2520of%2520models.%2520We%2520show%2520that%2520most%2520of%2520the%2520claims%250Amade%2520by%2520the%2520authors%2520about%2520ITI-GEN%2520hold%253A%2520it%2520improves%2520the%2520diversity%2520and%2520quality%250Aof%2520generated%2520images%252C%2520it%2520is%2520scalable%2520to%2520different%2520domains%252C%2520it%2520has%2520plug-and-play%250Acapabilities%252C%2520and%2520it%2520is%2520efficient%2520from%2520a%2520computational%2520point%2520of%2520view.%2520However%252C%250AITI-GEN%2520sometimes%2520uses%2520undesired%2520attributes%2520as%2520proxy%2520features%2520and%2520it%2520is%2520unable%250Ato%2520disentangle%2520some%2520pairs%2520of%2520%2528correlated%2529%2520attributes%2520such%2520as%2520gender%2520and%250Abaldness.%2520In%2520addition%252C%2520when%2520the%2520number%2520of%2520considered%2520attributes%2520increases%252C%2520the%250Atraining%2520time%2520grows%2520exponentially%2520and%2520ITI-GEN%2520struggles%2520to%2520generate%2520inclusive%250Aimages%2520for%2520all%2520elements%2520in%2520the%2520joint%2520distribution.%2520To%2520solve%2520these%2520issues%252C%2520we%250Apropose%2520using%2520Hard%2520Prompt%2520Search%2520with%2520negative%2520prompting%252C%2520a%2520method%2520that%2520does%250Anot%2520require%2520training%2520and%2520that%2520handles%2520negation%2520better%2520than%2520vanilla%2520Hard%2520Prompt%250ASearch.%2520Nonetheless%252C%2520Hard%2520Prompt%2520Search%2520%2528with%2520or%2520without%2520negative%2520prompting%2529%250Acannot%2520be%2520used%2520for%2520continuous%2520attributes%2520that%2520are%2520hard%2520to%2520express%2520in%2520natural%250Alanguage%252C%2520an%2520area%2520where%2520ITI-GEN%2520excels%2520as%2520it%2520is%2520guided%2520by%2520images%2520during%250Atraining.%2520Finally%252C%2520we%2520propose%2520combining%2520ITI-GEN%2520and%2520Hard%2520Prompt%2520Search%2520with%250Anegative%2520prompting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reproducibility%20Study%20of%20%22ITI-GEN%3A%20Inclusive%20Text-to-Image%20Generation%22&entry.906535625=Daniel%20Gallo%20Fern%C3%A1ndez%20and%20R%C4%83zvan-Andrei%20Matisan%20and%20Alejandro%20Monroy%20Mu%C3%B1oz%20and%20Janusz%20Partyka&entry.1292438233=%20%20Text-to-image%20generative%20models%20often%20present%20issues%20regarding%20fairness%20with%0Arespect%20to%20certain%20sensitive%20attributes%2C%20such%20as%20gender%20or%20skin%20tone.%20This%0Astudy%20aims%20to%20reproduce%20the%20results%20presented%20in%20%22ITI-GEN%3A%20Inclusive%0AText-to-Image%20Generation%22%20by%20Zhang%20et%20al.%20%282023a%29%2C%20which%20introduces%20a%20model%20to%0Aimprove%20inclusiveness%20in%20these%20kinds%20of%20models.%20We%20show%20that%20most%20of%20the%20claims%0Amade%20by%20the%20authors%20about%20ITI-GEN%20hold%3A%20it%20improves%20the%20diversity%20and%20quality%0Aof%20generated%20images%2C%20it%20is%20scalable%20to%20different%20domains%2C%20it%20has%20plug-and-play%0Acapabilities%2C%20and%20it%20is%20efficient%20from%20a%20computational%20point%20of%20view.%20However%2C%0AITI-GEN%20sometimes%20uses%20undesired%20attributes%20as%20proxy%20features%20and%20it%20is%20unable%0Ato%20disentangle%20some%20pairs%20of%20%28correlated%29%20attributes%20such%20as%20gender%20and%0Abaldness.%20In%20addition%2C%20when%20the%20number%20of%20considered%20attributes%20increases%2C%20the%0Atraining%20time%20grows%20exponentially%20and%20ITI-GEN%20struggles%20to%20generate%20inclusive%0Aimages%20for%20all%20elements%20in%20the%20joint%20distribution.%20To%20solve%20these%20issues%2C%20we%0Apropose%20using%20Hard%20Prompt%20Search%20with%20negative%20prompting%2C%20a%20method%20that%20does%0Anot%20require%20training%20and%20that%20handles%20negation%20better%20than%20vanilla%20Hard%20Prompt%0ASearch.%20Nonetheless%2C%20Hard%20Prompt%20Search%20%28with%20or%20without%20negative%20prompting%29%0Acannot%20be%20used%20for%20continuous%20attributes%20that%20are%20hard%20to%20express%20in%20natural%0Alanguage%2C%20an%20area%20where%20ITI-GEN%20excels%20as%20it%20is%20guided%20by%20images%20during%0Atraining.%20Finally%2C%20we%20propose%20combining%20ITI-GEN%20and%20Hard%20Prompt%20Search%20with%0Anegative%20prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19996v1&entry.124074799=Read"},
{"title": "Correspondence-Free SE(3) Point Cloud Registration in RKHS via\n  Unsupervised Equivariant Learning", "author": "Ray Zhang and Zheming Zhou and Min Sun and Omid Ghasemalizadeh and Cheng-Hao Kuo and Ryan Eustice and Maani Ghaffari and Arnie Sen", "abstract": "  This paper introduces a robust unsupervised SE(3) point cloud registration\nmethod that operates without requiring point correspondences. The method frames\npoint clouds as functions in a reproducing kernel Hilbert space (RKHS),\nleveraging SE(3)-equivariant features for direct feature space registration. A\nnovel RKHS distance metric is proposed, offering reliable performance amidst\nnoise, outliers, and asymmetrical data. An unsupervised training approach is\nintroduced to effectively handle limited ground truth data, facilitating\nadaptation to real datasets. The proposed method outperforms classical and\nsupervised methods in terms of registration accuracy on both synthetic\n(ModelNet40) and real-world (ETH3D) noisy, outlier-rich datasets. To our best\nknowledge, this marks the first instance of successful real RGB-D odometry data\nregistration using an equivariant method. The code is available at\n{https://sites.google.com/view/eccv24-equivalign}\n", "link": "http://arxiv.org/abs/2407.20223v1", "date": "2024-07-29", "relevancy": 2.8001, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5931}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5502}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correspondence-Free%20SE%283%29%20Point%20Cloud%20Registration%20in%20RKHS%20via%0A%20%20Unsupervised%20Equivariant%20Learning&body=Title%3A%20Correspondence-Free%20SE%283%29%20Point%20Cloud%20Registration%20in%20RKHS%20via%0A%20%20Unsupervised%20Equivariant%20Learning%0AAuthor%3A%20Ray%20Zhang%20and%20Zheming%20Zhou%20and%20Min%20Sun%20and%20Omid%20Ghasemalizadeh%20and%20Cheng-Hao%20Kuo%20and%20Ryan%20Eustice%20and%20Maani%20Ghaffari%20and%20Arnie%20Sen%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20robust%20unsupervised%20SE%283%29%20point%20cloud%20registration%0Amethod%20that%20operates%20without%20requiring%20point%20correspondences.%20The%20method%20frames%0Apoint%20clouds%20as%20functions%20in%20a%20reproducing%20kernel%20Hilbert%20space%20%28RKHS%29%2C%0Aleveraging%20SE%283%29-equivariant%20features%20for%20direct%20feature%20space%20registration.%20A%0Anovel%20RKHS%20distance%20metric%20is%20proposed%2C%20offering%20reliable%20performance%20amidst%0Anoise%2C%20outliers%2C%20and%20asymmetrical%20data.%20An%20unsupervised%20training%20approach%20is%0Aintroduced%20to%20effectively%20handle%20limited%20ground%20truth%20data%2C%20facilitating%0Aadaptation%20to%20real%20datasets.%20The%20proposed%20method%20outperforms%20classical%20and%0Asupervised%20methods%20in%20terms%20of%20registration%20accuracy%20on%20both%20synthetic%0A%28ModelNet40%29%20and%20real-world%20%28ETH3D%29%20noisy%2C%20outlier-rich%20datasets.%20To%20our%20best%0Aknowledge%2C%20this%20marks%20the%20first%20instance%20of%20successful%20real%20RGB-D%20odometry%20data%0Aregistration%20using%20an%20equivariant%20method.%20The%20code%20is%20available%20at%0A%7Bhttps%3A//sites.google.com/view/eccv24-equivalign%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrespondence-Free%2520SE%25283%2529%2520Point%2520Cloud%2520Registration%2520in%2520RKHS%2520via%250A%2520%2520Unsupervised%2520Equivariant%2520Learning%26entry.906535625%3DRay%2520Zhang%2520and%2520Zheming%2520Zhou%2520and%2520Min%2520Sun%2520and%2520Omid%2520Ghasemalizadeh%2520and%2520Cheng-Hao%2520Kuo%2520and%2520Ryan%2520Eustice%2520and%2520Maani%2520Ghaffari%2520and%2520Arnie%2520Sen%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520robust%2520unsupervised%2520SE%25283%2529%2520point%2520cloud%2520registration%250Amethod%2520that%2520operates%2520without%2520requiring%2520point%2520correspondences.%2520The%2520method%2520frames%250Apoint%2520clouds%2520as%2520functions%2520in%2520a%2520reproducing%2520kernel%2520Hilbert%2520space%2520%2528RKHS%2529%252C%250Aleveraging%2520SE%25283%2529-equivariant%2520features%2520for%2520direct%2520feature%2520space%2520registration.%2520A%250Anovel%2520RKHS%2520distance%2520metric%2520is%2520proposed%252C%2520offering%2520reliable%2520performance%2520amidst%250Anoise%252C%2520outliers%252C%2520and%2520asymmetrical%2520data.%2520An%2520unsupervised%2520training%2520approach%2520is%250Aintroduced%2520to%2520effectively%2520handle%2520limited%2520ground%2520truth%2520data%252C%2520facilitating%250Aadaptation%2520to%2520real%2520datasets.%2520The%2520proposed%2520method%2520outperforms%2520classical%2520and%250Asupervised%2520methods%2520in%2520terms%2520of%2520registration%2520accuracy%2520on%2520both%2520synthetic%250A%2528ModelNet40%2529%2520and%2520real-world%2520%2528ETH3D%2529%2520noisy%252C%2520outlier-rich%2520datasets.%2520To%2520our%2520best%250Aknowledge%252C%2520this%2520marks%2520the%2520first%2520instance%2520of%2520successful%2520real%2520RGB-D%2520odometry%2520data%250Aregistration%2520using%2520an%2520equivariant%2520method.%2520The%2520code%2520is%2520available%2520at%250A%257Bhttps%253A//sites.google.com/view/eccv24-equivalign%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correspondence-Free%20SE%283%29%20Point%20Cloud%20Registration%20in%20RKHS%20via%0A%20%20Unsupervised%20Equivariant%20Learning&entry.906535625=Ray%20Zhang%20and%20Zheming%20Zhou%20and%20Min%20Sun%20and%20Omid%20Ghasemalizadeh%20and%20Cheng-Hao%20Kuo%20and%20Ryan%20Eustice%20and%20Maani%20Ghaffari%20and%20Arnie%20Sen&entry.1292438233=%20%20This%20paper%20introduces%20a%20robust%20unsupervised%20SE%283%29%20point%20cloud%20registration%0Amethod%20that%20operates%20without%20requiring%20point%20correspondences.%20The%20method%20frames%0Apoint%20clouds%20as%20functions%20in%20a%20reproducing%20kernel%20Hilbert%20space%20%28RKHS%29%2C%0Aleveraging%20SE%283%29-equivariant%20features%20for%20direct%20feature%20space%20registration.%20A%0Anovel%20RKHS%20distance%20metric%20is%20proposed%2C%20offering%20reliable%20performance%20amidst%0Anoise%2C%20outliers%2C%20and%20asymmetrical%20data.%20An%20unsupervised%20training%20approach%20is%0Aintroduced%20to%20effectively%20handle%20limited%20ground%20truth%20data%2C%20facilitating%0Aadaptation%20to%20real%20datasets.%20The%20proposed%20method%20outperforms%20classical%20and%0Asupervised%20methods%20in%20terms%20of%20registration%20accuracy%20on%20both%20synthetic%0A%28ModelNet40%29%20and%20real-world%20%28ETH3D%29%20noisy%2C%20outlier-rich%20datasets.%20To%20our%20best%0Aknowledge%2C%20this%20marks%20the%20first%20instance%20of%20successful%20real%20RGB-D%20odometry%20data%0Aregistration%20using%20an%20equivariant%20method.%20The%20code%20is%20available%20at%0A%7Bhttps%3A//sites.google.com/view/eccv24-equivalign%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20223v1&entry.124074799=Read"},
{"title": "MaskInversion: Localized Embeddings via Optimization of Explainability\n  Maps", "author": "Walid Bousselham and Sofian Chaybouti and Christian Rupprecht and Vittorio Ferrari and Hilde Kuehne", "abstract": "  Vision-language foundation models such as CLIP have achieved tremendous\nresults in global vision-language alignment, but still show some limitations in\ncreating representations for specific image regions. % To address this problem,\nwe propose MaskInversion, a method that leverages the feature representations\nof pre-trained foundation models, such as CLIP, to generate a context-aware\nembedding for a query image region specified by a mask at test time.\nMaskInversion starts with initializing an embedding token and compares its\nexplainability map, derived from the foundation model, to the query mask. The\nembedding token is then subsequently refined to approximate the query region by\nminimizing the discrepancy between its explainability map and the query mask.\nDuring this process, only the embedding vector is updated, while the underlying\nfoundation model is kept frozen allowing to use MaskInversion with any\npre-trained model. As deriving the explainability map involves computing its\ngradient, which can be expensive, we propose a gradient decomposition strategy\nthat simplifies this computation. The learned region representation can be used\nfor a broad range of tasks, including open-vocabulary class retrieval,\nreferring expression comprehension, as well as for localized captioning and\nimage generation. We evaluate the proposed method on all those tasks on several\ndatasets such as PascalVOC, MSCOCO, RefCOCO, and OpenImagesV7 and show its\ncapabilities compared to other SOTA approaches.\n", "link": "http://arxiv.org/abs/2407.20034v1", "date": "2024-07-29", "relevancy": 2.7637, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5522}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaskInversion%3A%20Localized%20Embeddings%20via%20Optimization%20of%20Explainability%0A%20%20Maps&body=Title%3A%20MaskInversion%3A%20Localized%20Embeddings%20via%20Optimization%20of%20Explainability%0A%20%20Maps%0AAuthor%3A%20Walid%20Bousselham%20and%20Sofian%20Chaybouti%20and%20Christian%20Rupprecht%20and%20Vittorio%20Ferrari%20and%20Hilde%20Kuehne%0AAbstract%3A%20%20%20Vision-language%20foundation%20models%20such%20as%20CLIP%20have%20achieved%20tremendous%0Aresults%20in%20global%20vision-language%20alignment%2C%20but%20still%20show%20some%20limitations%20in%0Acreating%20representations%20for%20specific%20image%20regions.%20%25%20To%20address%20this%20problem%2C%0Awe%20propose%20MaskInversion%2C%20a%20method%20that%20leverages%20the%20feature%20representations%0Aof%20pre-trained%20foundation%20models%2C%20such%20as%20CLIP%2C%20to%20generate%20a%20context-aware%0Aembedding%20for%20a%20query%20image%20region%20specified%20by%20a%20mask%20at%20test%20time.%0AMaskInversion%20starts%20with%20initializing%20an%20embedding%20token%20and%20compares%20its%0Aexplainability%20map%2C%20derived%20from%20the%20foundation%20model%2C%20to%20the%20query%20mask.%20The%0Aembedding%20token%20is%20then%20subsequently%20refined%20to%20approximate%20the%20query%20region%20by%0Aminimizing%20the%20discrepancy%20between%20its%20explainability%20map%20and%20the%20query%20mask.%0ADuring%20this%20process%2C%20only%20the%20embedding%20vector%20is%20updated%2C%20while%20the%20underlying%0Afoundation%20model%20is%20kept%20frozen%20allowing%20to%20use%20MaskInversion%20with%20any%0Apre-trained%20model.%20As%20deriving%20the%20explainability%20map%20involves%20computing%20its%0Agradient%2C%20which%20can%20be%20expensive%2C%20we%20propose%20a%20gradient%20decomposition%20strategy%0Athat%20simplifies%20this%20computation.%20The%20learned%20region%20representation%20can%20be%20used%0Afor%20a%20broad%20range%20of%20tasks%2C%20including%20open-vocabulary%20class%20retrieval%2C%0Areferring%20expression%20comprehension%2C%20as%20well%20as%20for%20localized%20captioning%20and%0Aimage%20generation.%20We%20evaluate%20the%20proposed%20method%20on%20all%20those%20tasks%20on%20several%0Adatasets%20such%20as%20PascalVOC%2C%20MSCOCO%2C%20RefCOCO%2C%20and%20OpenImagesV7%20and%20show%20its%0Acapabilities%20compared%20to%20other%20SOTA%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaskInversion%253A%2520Localized%2520Embeddings%2520via%2520Optimization%2520of%2520Explainability%250A%2520%2520Maps%26entry.906535625%3DWalid%2520Bousselham%2520and%2520Sofian%2520Chaybouti%2520and%2520Christian%2520Rupprecht%2520and%2520Vittorio%2520Ferrari%2520and%2520Hilde%2520Kuehne%26entry.1292438233%3D%2520%2520Vision-language%2520foundation%2520models%2520such%2520as%2520CLIP%2520have%2520achieved%2520tremendous%250Aresults%2520in%2520global%2520vision-language%2520alignment%252C%2520but%2520still%2520show%2520some%2520limitations%2520in%250Acreating%2520representations%2520for%2520specific%2520image%2520regions.%2520%2525%2520To%2520address%2520this%2520problem%252C%250Awe%2520propose%2520MaskInversion%252C%2520a%2520method%2520that%2520leverages%2520the%2520feature%2520representations%250Aof%2520pre-trained%2520foundation%2520models%252C%2520such%2520as%2520CLIP%252C%2520to%2520generate%2520a%2520context-aware%250Aembedding%2520for%2520a%2520query%2520image%2520region%2520specified%2520by%2520a%2520mask%2520at%2520test%2520time.%250AMaskInversion%2520starts%2520with%2520initializing%2520an%2520embedding%2520token%2520and%2520compares%2520its%250Aexplainability%2520map%252C%2520derived%2520from%2520the%2520foundation%2520model%252C%2520to%2520the%2520query%2520mask.%2520The%250Aembedding%2520token%2520is%2520then%2520subsequently%2520refined%2520to%2520approximate%2520the%2520query%2520region%2520by%250Aminimizing%2520the%2520discrepancy%2520between%2520its%2520explainability%2520map%2520and%2520the%2520query%2520mask.%250ADuring%2520this%2520process%252C%2520only%2520the%2520embedding%2520vector%2520is%2520updated%252C%2520while%2520the%2520underlying%250Afoundation%2520model%2520is%2520kept%2520frozen%2520allowing%2520to%2520use%2520MaskInversion%2520with%2520any%250Apre-trained%2520model.%2520As%2520deriving%2520the%2520explainability%2520map%2520involves%2520computing%2520its%250Agradient%252C%2520which%2520can%2520be%2520expensive%252C%2520we%2520propose%2520a%2520gradient%2520decomposition%2520strategy%250Athat%2520simplifies%2520this%2520computation.%2520The%2520learned%2520region%2520representation%2520can%2520be%2520used%250Afor%2520a%2520broad%2520range%2520of%2520tasks%252C%2520including%2520open-vocabulary%2520class%2520retrieval%252C%250Areferring%2520expression%2520comprehension%252C%2520as%2520well%2520as%2520for%2520localized%2520captioning%2520and%250Aimage%2520generation.%2520We%2520evaluate%2520the%2520proposed%2520method%2520on%2520all%2520those%2520tasks%2520on%2520several%250Adatasets%2520such%2520as%2520PascalVOC%252C%2520MSCOCO%252C%2520RefCOCO%252C%2520and%2520OpenImagesV7%2520and%2520show%2520its%250Acapabilities%2520compared%2520to%2520other%2520SOTA%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskInversion%3A%20Localized%20Embeddings%20via%20Optimization%20of%20Explainability%0A%20%20Maps&entry.906535625=Walid%20Bousselham%20and%20Sofian%20Chaybouti%20and%20Christian%20Rupprecht%20and%20Vittorio%20Ferrari%20and%20Hilde%20Kuehne&entry.1292438233=%20%20Vision-language%20foundation%20models%20such%20as%20CLIP%20have%20achieved%20tremendous%0Aresults%20in%20global%20vision-language%20alignment%2C%20but%20still%20show%20some%20limitations%20in%0Acreating%20representations%20for%20specific%20image%20regions.%20%25%20To%20address%20this%20problem%2C%0Awe%20propose%20MaskInversion%2C%20a%20method%20that%20leverages%20the%20feature%20representations%0Aof%20pre-trained%20foundation%20models%2C%20such%20as%20CLIP%2C%20to%20generate%20a%20context-aware%0Aembedding%20for%20a%20query%20image%20region%20specified%20by%20a%20mask%20at%20test%20time.%0AMaskInversion%20starts%20with%20initializing%20an%20embedding%20token%20and%20compares%20its%0Aexplainability%20map%2C%20derived%20from%20the%20foundation%20model%2C%20to%20the%20query%20mask.%20The%0Aembedding%20token%20is%20then%20subsequently%20refined%20to%20approximate%20the%20query%20region%20by%0Aminimizing%20the%20discrepancy%20between%20its%20explainability%20map%20and%20the%20query%20mask.%0ADuring%20this%20process%2C%20only%20the%20embedding%20vector%20is%20updated%2C%20while%20the%20underlying%0Afoundation%20model%20is%20kept%20frozen%20allowing%20to%20use%20MaskInversion%20with%20any%0Apre-trained%20model.%20As%20deriving%20the%20explainability%20map%20involves%20computing%20its%0Agradient%2C%20which%20can%20be%20expensive%2C%20we%20propose%20a%20gradient%20decomposition%20strategy%0Athat%20simplifies%20this%20computation.%20The%20learned%20region%20representation%20can%20be%20used%0Afor%20a%20broad%20range%20of%20tasks%2C%20including%20open-vocabulary%20class%20retrieval%2C%0Areferring%20expression%20comprehension%2C%20as%20well%20as%20for%20localized%20captioning%20and%0Aimage%20generation.%20We%20evaluate%20the%20proposed%20method%20on%20all%20those%20tasks%20on%20several%0Adatasets%20such%20as%20PascalVOC%2C%20MSCOCO%2C%20RefCOCO%2C%20and%20OpenImagesV7%20and%20show%20its%0Acapabilities%20compared%20to%20other%20SOTA%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20034v1&entry.124074799=Read"},
{"title": "Dynamic Spiking Graph Neural Networks", "author": "Nan Yin and Mengzhu Wang and Zhenghan Chen and Giulia De Masi and Bin Gu and Huan Xiong", "abstract": "  The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks\n(GNNs) is gradually attracting attention due to the low power consumption and\nhigh efficiency in processing the non-Euclidean data represented by graphs.\nHowever, as a common problem, dynamic graph representation learning faces\nchallenges such as high complexity and large memory overheads. Current work\noften uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary\nfeatures instead of continuous ones for efficient training, which would\noverlooks graph structure information and leads to the loss of details during\npropagation. Additionally, optimizing dynamic spiking models typically requires\npropagation of information across time steps, which increases memory\nrequirements. To address these challenges, we present a framework named\n\\underline{Dy}namic \\underline{S}p\\underline{i}king \\underline{G}raph\n\\underline{N}eural Networks (\\method{}). To mitigate the information loss\nproblem, \\method{} propagates early-layer information directly to the last\nlayer for information compensation. To accommodate the memory requirements, we\napply the implicit differentiation on the equilibrium state, which does not\nrely on the exact reverse of the forward computation. While traditional\nimplicit differentiation methods are usually used for static situations,\n\\method{} extends it to the dynamic graph setting. Extensive experiments on\nthree large-scale real-world dynamic graph datasets validate the effectiveness\nof \\method{} on dynamic node classification tasks with lower computational\ncosts.\n", "link": "http://arxiv.org/abs/2401.05373v2", "date": "2024-07-29", "relevancy": 2.7131, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6368}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5086}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Spiking%20Graph%20Neural%20Networks&body=Title%3A%20Dynamic%20Spiking%20Graph%20Neural%20Networks%0AAuthor%3A%20Nan%20Yin%20and%20Mengzhu%20Wang%20and%20Zhenghan%20Chen%20and%20Giulia%20De%20Masi%20and%20Bin%20Gu%20and%20Huan%20Xiong%0AAbstract%3A%20%20%20The%20integration%20of%20Spiking%20Neural%20Networks%20%28SNNs%29%20and%20Graph%20Neural%20Networks%0A%28GNNs%29%20is%20gradually%20attracting%20attention%20due%20to%20the%20low%20power%20consumption%20and%0Ahigh%20efficiency%20in%20processing%20the%20non-Euclidean%20data%20represented%20by%20graphs.%0AHowever%2C%20as%20a%20common%20problem%2C%20dynamic%20graph%20representation%20learning%20faces%0Achallenges%20such%20as%20high%20complexity%20and%20large%20memory%20overheads.%20Current%20work%0Aoften%20uses%20SNNs%20instead%20of%20Recurrent%20Neural%20Networks%20%28RNNs%29%20by%20using%20binary%0Afeatures%20instead%20of%20continuous%20ones%20for%20efficient%20training%2C%20which%20would%0Aoverlooks%20graph%20structure%20information%20and%20leads%20to%20the%20loss%20of%20details%20during%0Apropagation.%20Additionally%2C%20optimizing%20dynamic%20spiking%20models%20typically%20requires%0Apropagation%20of%20information%20across%20time%20steps%2C%20which%20increases%20memory%0Arequirements.%20To%20address%20these%20challenges%2C%20we%20present%20a%20framework%20named%0A%5Cunderline%7BDy%7Dnamic%20%5Cunderline%7BS%7Dp%5Cunderline%7Bi%7Dking%20%5Cunderline%7BG%7Draph%0A%5Cunderline%7BN%7Deural%20Networks%20%28%5Cmethod%7B%7D%29.%20To%20mitigate%20the%20information%20loss%0Aproblem%2C%20%5Cmethod%7B%7D%20propagates%20early-layer%20information%20directly%20to%20the%20last%0Alayer%20for%20information%20compensation.%20To%20accommodate%20the%20memory%20requirements%2C%20we%0Aapply%20the%20implicit%20differentiation%20on%20the%20equilibrium%20state%2C%20which%20does%20not%0Arely%20on%20the%20exact%20reverse%20of%20the%20forward%20computation.%20While%20traditional%0Aimplicit%20differentiation%20methods%20are%20usually%20used%20for%20static%20situations%2C%0A%5Cmethod%7B%7D%20extends%20it%20to%20the%20dynamic%20graph%20setting.%20Extensive%20experiments%20on%0Athree%20large-scale%20real-world%20dynamic%20graph%20datasets%20validate%20the%20effectiveness%0Aof%20%5Cmethod%7B%7D%20on%20dynamic%20node%20classification%20tasks%20with%20lower%20computational%0Acosts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05373v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Spiking%2520Graph%2520Neural%2520Networks%26entry.906535625%3DNan%2520Yin%2520and%2520Mengzhu%2520Wang%2520and%2520Zhenghan%2520Chen%2520and%2520Giulia%2520De%2520Masi%2520and%2520Bin%2520Gu%2520and%2520Huan%2520Xiong%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520and%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529%2520is%2520gradually%2520attracting%2520attention%2520due%2520to%2520the%2520low%2520power%2520consumption%2520and%250Ahigh%2520efficiency%2520in%2520processing%2520the%2520non-Euclidean%2520data%2520represented%2520by%2520graphs.%250AHowever%252C%2520as%2520a%2520common%2520problem%252C%2520dynamic%2520graph%2520representation%2520learning%2520faces%250Achallenges%2520such%2520as%2520high%2520complexity%2520and%2520large%2520memory%2520overheads.%2520Current%2520work%250Aoften%2520uses%2520SNNs%2520instead%2520of%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520by%2520using%2520binary%250Afeatures%2520instead%2520of%2520continuous%2520ones%2520for%2520efficient%2520training%252C%2520which%2520would%250Aoverlooks%2520graph%2520structure%2520information%2520and%2520leads%2520to%2520the%2520loss%2520of%2520details%2520during%250Apropagation.%2520Additionally%252C%2520optimizing%2520dynamic%2520spiking%2520models%2520typically%2520requires%250Apropagation%2520of%2520information%2520across%2520time%2520steps%252C%2520which%2520increases%2520memory%250Arequirements.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520a%2520framework%2520named%250A%255Cunderline%257BDy%257Dnamic%2520%255Cunderline%257BS%257Dp%255Cunderline%257Bi%257Dking%2520%255Cunderline%257BG%257Draph%250A%255Cunderline%257BN%257Deural%2520Networks%2520%2528%255Cmethod%257B%257D%2529.%2520To%2520mitigate%2520the%2520information%2520loss%250Aproblem%252C%2520%255Cmethod%257B%257D%2520propagates%2520early-layer%2520information%2520directly%2520to%2520the%2520last%250Alayer%2520for%2520information%2520compensation.%2520To%2520accommodate%2520the%2520memory%2520requirements%252C%2520we%250Aapply%2520the%2520implicit%2520differentiation%2520on%2520the%2520equilibrium%2520state%252C%2520which%2520does%2520not%250Arely%2520on%2520the%2520exact%2520reverse%2520of%2520the%2520forward%2520computation.%2520While%2520traditional%250Aimplicit%2520differentiation%2520methods%2520are%2520usually%2520used%2520for%2520static%2520situations%252C%250A%255Cmethod%257B%257D%2520extends%2520it%2520to%2520the%2520dynamic%2520graph%2520setting.%2520Extensive%2520experiments%2520on%250Athree%2520large-scale%2520real-world%2520dynamic%2520graph%2520datasets%2520validate%2520the%2520effectiveness%250Aof%2520%255Cmethod%257B%257D%2520on%2520dynamic%2520node%2520classification%2520tasks%2520with%2520lower%2520computational%250Acosts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05373v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Spiking%20Graph%20Neural%20Networks&entry.906535625=Nan%20Yin%20and%20Mengzhu%20Wang%20and%20Zhenghan%20Chen%20and%20Giulia%20De%20Masi%20and%20Bin%20Gu%20and%20Huan%20Xiong&entry.1292438233=%20%20The%20integration%20of%20Spiking%20Neural%20Networks%20%28SNNs%29%20and%20Graph%20Neural%20Networks%0A%28GNNs%29%20is%20gradually%20attracting%20attention%20due%20to%20the%20low%20power%20consumption%20and%0Ahigh%20efficiency%20in%20processing%20the%20non-Euclidean%20data%20represented%20by%20graphs.%0AHowever%2C%20as%20a%20common%20problem%2C%20dynamic%20graph%20representation%20learning%20faces%0Achallenges%20such%20as%20high%20complexity%20and%20large%20memory%20overheads.%20Current%20work%0Aoften%20uses%20SNNs%20instead%20of%20Recurrent%20Neural%20Networks%20%28RNNs%29%20by%20using%20binary%0Afeatures%20instead%20of%20continuous%20ones%20for%20efficient%20training%2C%20which%20would%0Aoverlooks%20graph%20structure%20information%20and%20leads%20to%20the%20loss%20of%20details%20during%0Apropagation.%20Additionally%2C%20optimizing%20dynamic%20spiking%20models%20typically%20requires%0Apropagation%20of%20information%20across%20time%20steps%2C%20which%20increases%20memory%0Arequirements.%20To%20address%20these%20challenges%2C%20we%20present%20a%20framework%20named%0A%5Cunderline%7BDy%7Dnamic%20%5Cunderline%7BS%7Dp%5Cunderline%7Bi%7Dking%20%5Cunderline%7BG%7Draph%0A%5Cunderline%7BN%7Deural%20Networks%20%28%5Cmethod%7B%7D%29.%20To%20mitigate%20the%20information%20loss%0Aproblem%2C%20%5Cmethod%7B%7D%20propagates%20early-layer%20information%20directly%20to%20the%20last%0Alayer%20for%20information%20compensation.%20To%20accommodate%20the%20memory%20requirements%2C%20we%0Aapply%20the%20implicit%20differentiation%20on%20the%20equilibrium%20state%2C%20which%20does%20not%0Arely%20on%20the%20exact%20reverse%20of%20the%20forward%20computation.%20While%20traditional%0Aimplicit%20differentiation%20methods%20are%20usually%20used%20for%20static%20situations%2C%0A%5Cmethod%7B%7D%20extends%20it%20to%20the%20dynamic%20graph%20setting.%20Extensive%20experiments%20on%0Athree%20large-scale%20real-world%20dynamic%20graph%20datasets%20validate%20the%20effectiveness%0Aof%20%5Cmethod%7B%7D%20on%20dynamic%20node%20classification%20tasks%20with%20lower%20computational%0Acosts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05373v2&entry.124074799=Read"},
{"title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse\n  Autoencoders", "author": "Senthooran Rajamanoharan and Tom Lieberum and Nicolas Sonnerat and Arthur Conmy and Vikrant Varma and J\u00e1nos Kram\u00e1r and Neel Nanda", "abstract": "  Sparse autoencoders (SAEs) are a promising unsupervised approach for\nidentifying causally relevant and interpretable linear features in a language\nmodel's (LM) activations. To be useful for downstream tasks, SAEs need to\ndecompose LM activations faithfully; yet to be interpretable the decomposition\nmust be sparse -- two objectives that are in tension. In this paper, we\nintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity\nat a given sparsity level on Gemma 2 9B activations, compared to other recent\nadvances such as Gated and TopK SAEs. We also show that this improvement does\nnot come at the cost of interpretability through manual and automated\ninterpretability studies. JumpReLU SAEs are a simple modification of vanilla\n(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU\nactivation function -- and are similarly efficient to train and run. By\nutilising straight-through-estimators (STEs) in a principled manner, we show\nhow it is possible to train JumpReLU SAEs effectively despite the discontinuous\nJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs\nto directly train L0 to be sparse, instead of training on proxies such as L1,\navoiding problems like shrinkage.\n", "link": "http://arxiv.org/abs/2407.14435v2", "date": "2024-07-29", "relevancy": 2.6151, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5606}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5082}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders&body=Title%3A%20Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20Senthooran%20Rajamanoharan%20and%20Tom%20Lieberum%20and%20Nicolas%20Sonnerat%20and%20Arthur%20Conmy%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20unsupervised%20approach%20for%0Aidentifying%20causally%20relevant%20and%20interpretable%20linear%20features%20in%20a%20language%0Amodel%27s%20%28LM%29%20activations.%20To%20be%20useful%20for%20downstream%20tasks%2C%20SAEs%20need%20to%0Adecompose%20LM%20activations%20faithfully%3B%20yet%20to%20be%20interpretable%20the%20decomposition%0Amust%20be%20sparse%20--%20two%20objectives%20that%20are%20in%20tension.%20In%20this%20paper%2C%20we%0Aintroduce%20JumpReLU%20SAEs%2C%20which%20achieve%20state-of-the-art%20reconstruction%20fidelity%0Aat%20a%20given%20sparsity%20level%20on%20Gemma%202%209B%20activations%2C%20compared%20to%20other%20recent%0Aadvances%20such%20as%20Gated%20and%20TopK%20SAEs.%20We%20also%20show%20that%20this%20improvement%20does%0Anot%20come%20at%20the%20cost%20of%20interpretability%20through%20manual%20and%20automated%0Ainterpretability%20studies.%20JumpReLU%20SAEs%20are%20a%20simple%20modification%20of%20vanilla%0A%28ReLU%29%20SAEs%20--%20where%20we%20replace%20the%20ReLU%20with%20a%20discontinuous%20JumpReLU%0Aactivation%20function%20--%20and%20are%20similarly%20efficient%20to%20train%20and%20run.%20By%0Autilising%20straight-through-estimators%20%28STEs%29%20in%20a%20principled%20manner%2C%20we%20show%0Ahow%20it%20is%20possible%20to%20train%20JumpReLU%20SAEs%20effectively%20despite%20the%20discontinuous%0AJumpReLU%20function%20introduced%20in%20the%20SAE%27s%20forward%20pass.%20Similarly%2C%20we%20use%20STEs%0Ato%20directly%20train%20L0%20to%20be%20sparse%2C%20instead%20of%20training%20on%20proxies%20such%20as%20L1%2C%0Aavoiding%20problems%20like%20shrinkage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJumping%2520Ahead%253A%2520Improving%2520Reconstruction%2520Fidelity%2520with%2520JumpReLU%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DSenthooran%2520Rajamanoharan%2520and%2520Tom%2520Lieberum%2520and%2520Nicolas%2520Sonnerat%2520and%2520Arthur%2520Conmy%2520and%2520Vikrant%2520Varma%2520and%2520J%25C3%25A1nos%2520Kram%25C3%25A1r%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520promising%2520unsupervised%2520approach%2520for%250Aidentifying%2520causally%2520relevant%2520and%2520interpretable%2520linear%2520features%2520in%2520a%2520language%250Amodel%2527s%2520%2528LM%2529%2520activations.%2520To%2520be%2520useful%2520for%2520downstream%2520tasks%252C%2520SAEs%2520need%2520to%250Adecompose%2520LM%2520activations%2520faithfully%253B%2520yet%2520to%2520be%2520interpretable%2520the%2520decomposition%250Amust%2520be%2520sparse%2520--%2520two%2520objectives%2520that%2520are%2520in%2520tension.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520JumpReLU%2520SAEs%252C%2520which%2520achieve%2520state-of-the-art%2520reconstruction%2520fidelity%250Aat%2520a%2520given%2520sparsity%2520level%2520on%2520Gemma%25202%25209B%2520activations%252C%2520compared%2520to%2520other%2520recent%250Aadvances%2520such%2520as%2520Gated%2520and%2520TopK%2520SAEs.%2520We%2520also%2520show%2520that%2520this%2520improvement%2520does%250Anot%2520come%2520at%2520the%2520cost%2520of%2520interpretability%2520through%2520manual%2520and%2520automated%250Ainterpretability%2520studies.%2520JumpReLU%2520SAEs%2520are%2520a%2520simple%2520modification%2520of%2520vanilla%250A%2528ReLU%2529%2520SAEs%2520--%2520where%2520we%2520replace%2520the%2520ReLU%2520with%2520a%2520discontinuous%2520JumpReLU%250Aactivation%2520function%2520--%2520and%2520are%2520similarly%2520efficient%2520to%2520train%2520and%2520run.%2520By%250Autilising%2520straight-through-estimators%2520%2528STEs%2529%2520in%2520a%2520principled%2520manner%252C%2520we%2520show%250Ahow%2520it%2520is%2520possible%2520to%2520train%2520JumpReLU%2520SAEs%2520effectively%2520despite%2520the%2520discontinuous%250AJumpReLU%2520function%2520introduced%2520in%2520the%2520SAE%2527s%2520forward%2520pass.%2520Similarly%252C%2520we%2520use%2520STEs%250Ato%2520directly%2520train%2520L0%2520to%2520be%2520sparse%252C%2520instead%2520of%2520training%2520on%2520proxies%2520such%2520as%2520L1%252C%250Aavoiding%2520problems%2520like%2520shrinkage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders&entry.906535625=Senthooran%20Rajamanoharan%20and%20Tom%20Lieberum%20and%20Nicolas%20Sonnerat%20and%20Arthur%20Conmy%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Neel%20Nanda&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20unsupervised%20approach%20for%0Aidentifying%20causally%20relevant%20and%20interpretable%20linear%20features%20in%20a%20language%0Amodel%27s%20%28LM%29%20activations.%20To%20be%20useful%20for%20downstream%20tasks%2C%20SAEs%20need%20to%0Adecompose%20LM%20activations%20faithfully%3B%20yet%20to%20be%20interpretable%20the%20decomposition%0Amust%20be%20sparse%20--%20two%20objectives%20that%20are%20in%20tension.%20In%20this%20paper%2C%20we%0Aintroduce%20JumpReLU%20SAEs%2C%20which%20achieve%20state-of-the-art%20reconstruction%20fidelity%0Aat%20a%20given%20sparsity%20level%20on%20Gemma%202%209B%20activations%2C%20compared%20to%20other%20recent%0Aadvances%20such%20as%20Gated%20and%20TopK%20SAEs.%20We%20also%20show%20that%20this%20improvement%20does%0Anot%20come%20at%20the%20cost%20of%20interpretability%20through%20manual%20and%20automated%0Ainterpretability%20studies.%20JumpReLU%20SAEs%20are%20a%20simple%20modification%20of%20vanilla%0A%28ReLU%29%20SAEs%20--%20where%20we%20replace%20the%20ReLU%20with%20a%20discontinuous%20JumpReLU%0Aactivation%20function%20--%20and%20are%20similarly%20efficient%20to%20train%20and%20run.%20By%0Autilising%20straight-through-estimators%20%28STEs%29%20in%20a%20principled%20manner%2C%20we%20show%0Ahow%20it%20is%20possible%20to%20train%20JumpReLU%20SAEs%20effectively%20despite%20the%20discontinuous%0AJumpReLU%20function%20introduced%20in%20the%20SAE%27s%20forward%20pass.%20Similarly%2C%20we%20use%20STEs%0Ato%20directly%20train%20L0%20to%20be%20sparse%2C%20instead%20of%20training%20on%20proxies%20such%20as%20L1%2C%0Aavoiding%20problems%20like%20shrinkage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14435v2&entry.124074799=Read"},
{"title": "Exploring Robust Face-Voice Matching in Multilingual Environments", "author": "Jiehui Tang and Xiaofei Wang and Zhen Xiao and Jiayi Liu and Xueliang Liu and Richang Hong", "abstract": "  This paper presents Team Xaiofei's innovative approach to exploring\nFace-Voice Association in Multilingual Environments (FAME) at ACM Multimedia\n2024. We focus on the impact of different languages in face-voice matching by\nbuilding upon Fusion and Orthogonal Projection (FOP), introducing four key\ncomponents: a dual-branch structure, dynamic sample pair weighting, robust data\naugmentation, and score polarization strategy. Our dual-branch structure serves\nas an auxiliary mechanism to better integrate and provide more comprehensive\ninformation. We also introduce a dynamic weighting mechanism for various sample\npairs to optimize learning. Data augmentation techniques are employed to\nenhance the model's generalization across diverse conditions. Additionally,\nscore polarization strategy based on age and gender matching confidence\nclarifies and accentuates the final results. Our methods demonstrate\nsignificant effectiveness, achieving an equal error rate (EER) of 20.07 on the\nV2-EH dataset and 21.76 on the V1-EU dataset.\n", "link": "http://arxiv.org/abs/2407.19875v1", "date": "2024-07-29", "relevancy": 2.5762, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5274}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Robust%20Face-Voice%20Matching%20in%20Multilingual%20Environments&body=Title%3A%20Exploring%20Robust%20Face-Voice%20Matching%20in%20Multilingual%20Environments%0AAuthor%3A%20Jiehui%20Tang%20and%20Xiaofei%20Wang%20and%20Zhen%20Xiao%20and%20Jiayi%20Liu%20and%20Xueliang%20Liu%20and%20Richang%20Hong%0AAbstract%3A%20%20%20This%20paper%20presents%20Team%20Xaiofei%27s%20innovative%20approach%20to%20exploring%0AFace-Voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%20at%20ACM%20Multimedia%0A2024.%20We%20focus%20on%20the%20impact%20of%20different%20languages%20in%20face-voice%20matching%20by%0Abuilding%20upon%20Fusion%20and%20Orthogonal%20Projection%20%28FOP%29%2C%20introducing%20four%20key%0Acomponents%3A%20a%20dual-branch%20structure%2C%20dynamic%20sample%20pair%20weighting%2C%20robust%20data%0Aaugmentation%2C%20and%20score%20polarization%20strategy.%20Our%20dual-branch%20structure%20serves%0Aas%20an%20auxiliary%20mechanism%20to%20better%20integrate%20and%20provide%20more%20comprehensive%0Ainformation.%20We%20also%20introduce%20a%20dynamic%20weighting%20mechanism%20for%20various%20sample%0Apairs%20to%20optimize%20learning.%20Data%20augmentation%20techniques%20are%20employed%20to%0Aenhance%20the%20model%27s%20generalization%20across%20diverse%20conditions.%20Additionally%2C%0Ascore%20polarization%20strategy%20based%20on%20age%20and%20gender%20matching%20confidence%0Aclarifies%20and%20accentuates%20the%20final%20results.%20Our%20methods%20demonstrate%0Asignificant%20effectiveness%2C%20achieving%20an%20equal%20error%20rate%20%28EER%29%20of%2020.07%20on%20the%0AV2-EH%20dataset%20and%2021.76%20on%20the%20V1-EU%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Robust%2520Face-Voice%2520Matching%2520in%2520Multilingual%2520Environments%26entry.906535625%3DJiehui%2520Tang%2520and%2520Xiaofei%2520Wang%2520and%2520Zhen%2520Xiao%2520and%2520Jiayi%2520Liu%2520and%2520Xueliang%2520Liu%2520and%2520Richang%2520Hong%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Team%2520Xaiofei%2527s%2520innovative%2520approach%2520to%2520exploring%250AFace-Voice%2520Association%2520in%2520Multilingual%2520Environments%2520%2528FAME%2529%2520at%2520ACM%2520Multimedia%250A2024.%2520We%2520focus%2520on%2520the%2520impact%2520of%2520different%2520languages%2520in%2520face-voice%2520matching%2520by%250Abuilding%2520upon%2520Fusion%2520and%2520Orthogonal%2520Projection%2520%2528FOP%2529%252C%2520introducing%2520four%2520key%250Acomponents%253A%2520a%2520dual-branch%2520structure%252C%2520dynamic%2520sample%2520pair%2520weighting%252C%2520robust%2520data%250Aaugmentation%252C%2520and%2520score%2520polarization%2520strategy.%2520Our%2520dual-branch%2520structure%2520serves%250Aas%2520an%2520auxiliary%2520mechanism%2520to%2520better%2520integrate%2520and%2520provide%2520more%2520comprehensive%250Ainformation.%2520We%2520also%2520introduce%2520a%2520dynamic%2520weighting%2520mechanism%2520for%2520various%2520sample%250Apairs%2520to%2520optimize%2520learning.%2520Data%2520augmentation%2520techniques%2520are%2520employed%2520to%250Aenhance%2520the%2520model%2527s%2520generalization%2520across%2520diverse%2520conditions.%2520Additionally%252C%250Ascore%2520polarization%2520strategy%2520based%2520on%2520age%2520and%2520gender%2520matching%2520confidence%250Aclarifies%2520and%2520accentuates%2520the%2520final%2520results.%2520Our%2520methods%2520demonstrate%250Asignificant%2520effectiveness%252C%2520achieving%2520an%2520equal%2520error%2520rate%2520%2528EER%2529%2520of%252020.07%2520on%2520the%250AV2-EH%2520dataset%2520and%252021.76%2520on%2520the%2520V1-EU%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Robust%20Face-Voice%20Matching%20in%20Multilingual%20Environments&entry.906535625=Jiehui%20Tang%20and%20Xiaofei%20Wang%20and%20Zhen%20Xiao%20and%20Jiayi%20Liu%20and%20Xueliang%20Liu%20and%20Richang%20Hong&entry.1292438233=%20%20This%20paper%20presents%20Team%20Xaiofei%27s%20innovative%20approach%20to%20exploring%0AFace-Voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%20at%20ACM%20Multimedia%0A2024.%20We%20focus%20on%20the%20impact%20of%20different%20languages%20in%20face-voice%20matching%20by%0Abuilding%20upon%20Fusion%20and%20Orthogonal%20Projection%20%28FOP%29%2C%20introducing%20four%20key%0Acomponents%3A%20a%20dual-branch%20structure%2C%20dynamic%20sample%20pair%20weighting%2C%20robust%20data%0Aaugmentation%2C%20and%20score%20polarization%20strategy.%20Our%20dual-branch%20structure%20serves%0Aas%20an%20auxiliary%20mechanism%20to%20better%20integrate%20and%20provide%20more%20comprehensive%0Ainformation.%20We%20also%20introduce%20a%20dynamic%20weighting%20mechanism%20for%20various%20sample%0Apairs%20to%20optimize%20learning.%20Data%20augmentation%20techniques%20are%20employed%20to%0Aenhance%20the%20model%27s%20generalization%20across%20diverse%20conditions.%20Additionally%2C%0Ascore%20polarization%20strategy%20based%20on%20age%20and%20gender%20matching%20confidence%0Aclarifies%20and%20accentuates%20the%20final%20results.%20Our%20methods%20demonstrate%0Asignificant%20effectiveness%2C%20achieving%20an%20equal%20error%20rate%20%28EER%29%20of%2020.07%20on%20the%0AV2-EH%20dataset%20and%2021.76%20on%20the%20V1-EU%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19875v1&entry.124074799=Read"},
{"title": "Aircraft Trajectory Segmentation-based Contrastive Coding: A Framework\n  for Self-supervised Trajectory Representation", "author": "Thaweerath Phisannupawong and Joshua Julian Damanik and Han-Lim Choi", "abstract": "  Air traffic trajectory recognition has gained significant interest within the\nair traffic management community, particularly for fundamental tasks such as\nclassification and clustering. This paper introduces Aircraft Trajectory\nSegmentation-based Contrastive Coding (ATSCC), a novel self-supervised time\nseries representation learning framework designed to capture semantic\ninformation in air traffic trajectory data. The framework leverages the\nsegmentable characteristic of trajectories and ensures consistency within the\nself-assigned segments. Intensive experiments were conducted on datasets from\nthree different airports, totaling four datasets, comparing the learned\nrepresentation's performance of downstream classification and clustering with\nother state-of-the-art representation learning techniques. The results show\nthat ATSCC outperforms these methods by aligning with the labels defined by\naeronautical procedures. ATSCC is adaptable to various airport configurations\nand scalable to incomplete trajectories. This research has expanded upon\nexisting capabilities, achieving these improvements independently without\npredefined inputs such as airport configurations, maneuvering procedures, or\nlabeled data.\n", "link": "http://arxiv.org/abs/2407.20028v1", "date": "2024-07-29", "relevancy": 2.5749, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5061}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aircraft%20Trajectory%20Segmentation-based%20Contrastive%20Coding%3A%20A%20Framework%0A%20%20for%20Self-supervised%20Trajectory%20Representation&body=Title%3A%20Aircraft%20Trajectory%20Segmentation-based%20Contrastive%20Coding%3A%20A%20Framework%0A%20%20for%20Self-supervised%20Trajectory%20Representation%0AAuthor%3A%20Thaweerath%20Phisannupawong%20and%20Joshua%20Julian%20Damanik%20and%20Han-Lim%20Choi%0AAbstract%3A%20%20%20Air%20traffic%20trajectory%20recognition%20has%20gained%20significant%20interest%20within%20the%0Aair%20traffic%20management%20community%2C%20particularly%20for%20fundamental%20tasks%20such%20as%0Aclassification%20and%20clustering.%20This%20paper%20introduces%20Aircraft%20Trajectory%0ASegmentation-based%20Contrastive%20Coding%20%28ATSCC%29%2C%20a%20novel%20self-supervised%20time%0Aseries%20representation%20learning%20framework%20designed%20to%20capture%20semantic%0Ainformation%20in%20air%20traffic%20trajectory%20data.%20The%20framework%20leverages%20the%0Asegmentable%20characteristic%20of%20trajectories%20and%20ensures%20consistency%20within%20the%0Aself-assigned%20segments.%20Intensive%20experiments%20were%20conducted%20on%20datasets%20from%0Athree%20different%20airports%2C%20totaling%20four%20datasets%2C%20comparing%20the%20learned%0Arepresentation%27s%20performance%20of%20downstream%20classification%20and%20clustering%20with%0Aother%20state-of-the-art%20representation%20learning%20techniques.%20The%20results%20show%0Athat%20ATSCC%20outperforms%20these%20methods%20by%20aligning%20with%20the%20labels%20defined%20by%0Aaeronautical%20procedures.%20ATSCC%20is%20adaptable%20to%20various%20airport%20configurations%0Aand%20scalable%20to%20incomplete%20trajectories.%20This%20research%20has%20expanded%20upon%0Aexisting%20capabilities%2C%20achieving%20these%20improvements%20independently%20without%0Apredefined%20inputs%20such%20as%20airport%20configurations%2C%20maneuvering%20procedures%2C%20or%0Alabeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAircraft%2520Trajectory%2520Segmentation-based%2520Contrastive%2520Coding%253A%2520A%2520Framework%250A%2520%2520for%2520Self-supervised%2520Trajectory%2520Representation%26entry.906535625%3DThaweerath%2520Phisannupawong%2520and%2520Joshua%2520Julian%2520Damanik%2520and%2520Han-Lim%2520Choi%26entry.1292438233%3D%2520%2520Air%2520traffic%2520trajectory%2520recognition%2520has%2520gained%2520significant%2520interest%2520within%2520the%250Aair%2520traffic%2520management%2520community%252C%2520particularly%2520for%2520fundamental%2520tasks%2520such%2520as%250Aclassification%2520and%2520clustering.%2520This%2520paper%2520introduces%2520Aircraft%2520Trajectory%250ASegmentation-based%2520Contrastive%2520Coding%2520%2528ATSCC%2529%252C%2520a%2520novel%2520self-supervised%2520time%250Aseries%2520representation%2520learning%2520framework%2520designed%2520to%2520capture%2520semantic%250Ainformation%2520in%2520air%2520traffic%2520trajectory%2520data.%2520The%2520framework%2520leverages%2520the%250Asegmentable%2520characteristic%2520of%2520trajectories%2520and%2520ensures%2520consistency%2520within%2520the%250Aself-assigned%2520segments.%2520Intensive%2520experiments%2520were%2520conducted%2520on%2520datasets%2520from%250Athree%2520different%2520airports%252C%2520totaling%2520four%2520datasets%252C%2520comparing%2520the%2520learned%250Arepresentation%2527s%2520performance%2520of%2520downstream%2520classification%2520and%2520clustering%2520with%250Aother%2520state-of-the-art%2520representation%2520learning%2520techniques.%2520The%2520results%2520show%250Athat%2520ATSCC%2520outperforms%2520these%2520methods%2520by%2520aligning%2520with%2520the%2520labels%2520defined%2520by%250Aaeronautical%2520procedures.%2520ATSCC%2520is%2520adaptable%2520to%2520various%2520airport%2520configurations%250Aand%2520scalable%2520to%2520incomplete%2520trajectories.%2520This%2520research%2520has%2520expanded%2520upon%250Aexisting%2520capabilities%252C%2520achieving%2520these%2520improvements%2520independently%2520without%250Apredefined%2520inputs%2520such%2520as%2520airport%2520configurations%252C%2520maneuvering%2520procedures%252C%2520or%250Alabeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aircraft%20Trajectory%20Segmentation-based%20Contrastive%20Coding%3A%20A%20Framework%0A%20%20for%20Self-supervised%20Trajectory%20Representation&entry.906535625=Thaweerath%20Phisannupawong%20and%20Joshua%20Julian%20Damanik%20and%20Han-Lim%20Choi&entry.1292438233=%20%20Air%20traffic%20trajectory%20recognition%20has%20gained%20significant%20interest%20within%20the%0Aair%20traffic%20management%20community%2C%20particularly%20for%20fundamental%20tasks%20such%20as%0Aclassification%20and%20clustering.%20This%20paper%20introduces%20Aircraft%20Trajectory%0ASegmentation-based%20Contrastive%20Coding%20%28ATSCC%29%2C%20a%20novel%20self-supervised%20time%0Aseries%20representation%20learning%20framework%20designed%20to%20capture%20semantic%0Ainformation%20in%20air%20traffic%20trajectory%20data.%20The%20framework%20leverages%20the%0Asegmentable%20characteristic%20of%20trajectories%20and%20ensures%20consistency%20within%20the%0Aself-assigned%20segments.%20Intensive%20experiments%20were%20conducted%20on%20datasets%20from%0Athree%20different%20airports%2C%20totaling%20four%20datasets%2C%20comparing%20the%20learned%0Arepresentation%27s%20performance%20of%20downstream%20classification%20and%20clustering%20with%0Aother%20state-of-the-art%20representation%20learning%20techniques.%20The%20results%20show%0Athat%20ATSCC%20outperforms%20these%20methods%20by%20aligning%20with%20the%20labels%20defined%20by%0Aaeronautical%20procedures.%20ATSCC%20is%20adaptable%20to%20various%20airport%20configurations%0Aand%20scalable%20to%20incomplete%20trajectories.%20This%20research%20has%20expanded%20upon%0Aexisting%20capabilities%2C%20achieving%20these%20improvements%20independently%20without%0Apredefined%20inputs%20such%20as%20airport%20configurations%2C%20maneuvering%20procedures%2C%20or%0Alabeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20028v1&entry.124074799=Read"},
{"title": "CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive\n  Graph Classification", "author": "Nan Yin and Li Shen and Mengzhu Wang and Long Lan and Zeyu Ma and Chong Chen and Xian-Sheng Hua and Xiao Luo", "abstract": "  Although graph neural networks (GNNs) have achieved impressive achievements\nin graph classification, they often need abundant task-specific labels, which\ncould be extensively costly to acquire. A credible solution is to explore\nadditional labeled graphs to enhance unsupervised learning on the target\ndomain. However, how to apply GNNs to domain adaptation remains unsolved owing\nto the insufficient exploration of graph topology and the significant domain\ndiscrepancy. In this paper, we propose Coupled Contrastive Graph Representation\nLearning (CoCo), which extracts the topological information from coupled\nlearning branches and reduces the domain discrepancy with coupled contrastive\nlearning. CoCo contains a graph convolutional network branch and a hierarchical\ngraph kernel network branch, which explore graph topology in implicit and\nexplicit manners. Besides, we incorporate coupled branches into a holistic\nmulti-view contrastive learning framework, which not only incorporates graph\nrepresentations learned from complementary views for enhanced understanding,\nbut also encourages the similarity between cross-domain example pairs with the\nsame semantics for domain alignment. Extensive experiments on popular datasets\nshow that our CoCo outperforms these competing baselines in different settings\ngenerally.\n", "link": "http://arxiv.org/abs/2306.04979v3", "date": "2024-07-29", "relevancy": 2.5696, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5447}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoCo%3A%20A%20Coupled%20Contrastive%20Framework%20for%20Unsupervised%20Domain%20Adaptive%0A%20%20Graph%20Classification&body=Title%3A%20CoCo%3A%20A%20Coupled%20Contrastive%20Framework%20for%20Unsupervised%20Domain%20Adaptive%0A%20%20Graph%20Classification%0AAuthor%3A%20Nan%20Yin%20and%20Li%20Shen%20and%20Mengzhu%20Wang%20and%20Long%20Lan%20and%20Zeyu%20Ma%20and%20Chong%20Chen%20and%20Xian-Sheng%20Hua%20and%20Xiao%20Luo%0AAbstract%3A%20%20%20Although%20graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20impressive%20achievements%0Ain%20graph%20classification%2C%20they%20often%20need%20abundant%20task-specific%20labels%2C%20which%0Acould%20be%20extensively%20costly%20to%20acquire.%20A%20credible%20solution%20is%20to%20explore%0Aadditional%20labeled%20graphs%20to%20enhance%20unsupervised%20learning%20on%20the%20target%0Adomain.%20However%2C%20how%20to%20apply%20GNNs%20to%20domain%20adaptation%20remains%20unsolved%20owing%0Ato%20the%20insufficient%20exploration%20of%20graph%20topology%20and%20the%20significant%20domain%0Adiscrepancy.%20In%20this%20paper%2C%20we%20propose%20Coupled%20Contrastive%20Graph%20Representation%0ALearning%20%28CoCo%29%2C%20which%20extracts%20the%20topological%20information%20from%20coupled%0Alearning%20branches%20and%20reduces%20the%20domain%20discrepancy%20with%20coupled%20contrastive%0Alearning.%20CoCo%20contains%20a%20graph%20convolutional%20network%20branch%20and%20a%20hierarchical%0Agraph%20kernel%20network%20branch%2C%20which%20explore%20graph%20topology%20in%20implicit%20and%0Aexplicit%20manners.%20Besides%2C%20we%20incorporate%20coupled%20branches%20into%20a%20holistic%0Amulti-view%20contrastive%20learning%20framework%2C%20which%20not%20only%20incorporates%20graph%0Arepresentations%20learned%20from%20complementary%20views%20for%20enhanced%20understanding%2C%0Abut%20also%20encourages%20the%20similarity%20between%20cross-domain%20example%20pairs%20with%20the%0Asame%20semantics%20for%20domain%20alignment.%20Extensive%20experiments%20on%20popular%20datasets%0Ashow%20that%20our%20CoCo%20outperforms%20these%20competing%20baselines%20in%20different%20settings%0Agenerally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoCo%253A%2520A%2520Coupled%2520Contrastive%2520Framework%2520for%2520Unsupervised%2520Domain%2520Adaptive%250A%2520%2520Graph%2520Classification%26entry.906535625%3DNan%2520Yin%2520and%2520Li%2520Shen%2520and%2520Mengzhu%2520Wang%2520and%2520Long%2520Lan%2520and%2520Zeyu%2520Ma%2520and%2520Chong%2520Chen%2520and%2520Xian-Sheng%2520Hua%2520and%2520Xiao%2520Luo%26entry.1292438233%3D%2520%2520Although%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520achieved%2520impressive%2520achievements%250Ain%2520graph%2520classification%252C%2520they%2520often%2520need%2520abundant%2520task-specific%2520labels%252C%2520which%250Acould%2520be%2520extensively%2520costly%2520to%2520acquire.%2520A%2520credible%2520solution%2520is%2520to%2520explore%250Aadditional%2520labeled%2520graphs%2520to%2520enhance%2520unsupervised%2520learning%2520on%2520the%2520target%250Adomain.%2520However%252C%2520how%2520to%2520apply%2520GNNs%2520to%2520domain%2520adaptation%2520remains%2520unsolved%2520owing%250Ato%2520the%2520insufficient%2520exploration%2520of%2520graph%2520topology%2520and%2520the%2520significant%2520domain%250Adiscrepancy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Coupled%2520Contrastive%2520Graph%2520Representation%250ALearning%2520%2528CoCo%2529%252C%2520which%2520extracts%2520the%2520topological%2520information%2520from%2520coupled%250Alearning%2520branches%2520and%2520reduces%2520the%2520domain%2520discrepancy%2520with%2520coupled%2520contrastive%250Alearning.%2520CoCo%2520contains%2520a%2520graph%2520convolutional%2520network%2520branch%2520and%2520a%2520hierarchical%250Agraph%2520kernel%2520network%2520branch%252C%2520which%2520explore%2520graph%2520topology%2520in%2520implicit%2520and%250Aexplicit%2520manners.%2520Besides%252C%2520we%2520incorporate%2520coupled%2520branches%2520into%2520a%2520holistic%250Amulti-view%2520contrastive%2520learning%2520framework%252C%2520which%2520not%2520only%2520incorporates%2520graph%250Arepresentations%2520learned%2520from%2520complementary%2520views%2520for%2520enhanced%2520understanding%252C%250Abut%2520also%2520encourages%2520the%2520similarity%2520between%2520cross-domain%2520example%2520pairs%2520with%2520the%250Asame%2520semantics%2520for%2520domain%2520alignment.%2520Extensive%2520experiments%2520on%2520popular%2520datasets%250Ashow%2520that%2520our%2520CoCo%2520outperforms%2520these%2520competing%2520baselines%2520in%2520different%2520settings%250Agenerally.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoCo%3A%20A%20Coupled%20Contrastive%20Framework%20for%20Unsupervised%20Domain%20Adaptive%0A%20%20Graph%20Classification&entry.906535625=Nan%20Yin%20and%20Li%20Shen%20and%20Mengzhu%20Wang%20and%20Long%20Lan%20and%20Zeyu%20Ma%20and%20Chong%20Chen%20and%20Xian-Sheng%20Hua%20and%20Xiao%20Luo&entry.1292438233=%20%20Although%20graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20impressive%20achievements%0Ain%20graph%20classification%2C%20they%20often%20need%20abundant%20task-specific%20labels%2C%20which%0Acould%20be%20extensively%20costly%20to%20acquire.%20A%20credible%20solution%20is%20to%20explore%0Aadditional%20labeled%20graphs%20to%20enhance%20unsupervised%20learning%20on%20the%20target%0Adomain.%20However%2C%20how%20to%20apply%20GNNs%20to%20domain%20adaptation%20remains%20unsolved%20owing%0Ato%20the%20insufficient%20exploration%20of%20graph%20topology%20and%20the%20significant%20domain%0Adiscrepancy.%20In%20this%20paper%2C%20we%20propose%20Coupled%20Contrastive%20Graph%20Representation%0ALearning%20%28CoCo%29%2C%20which%20extracts%20the%20topological%20information%20from%20coupled%0Alearning%20branches%20and%20reduces%20the%20domain%20discrepancy%20with%20coupled%20contrastive%0Alearning.%20CoCo%20contains%20a%20graph%20convolutional%20network%20branch%20and%20a%20hierarchical%0Agraph%20kernel%20network%20branch%2C%20which%20explore%20graph%20topology%20in%20implicit%20and%0Aexplicit%20manners.%20Besides%2C%20we%20incorporate%20coupled%20branches%20into%20a%20holistic%0Amulti-view%20contrastive%20learning%20framework%2C%20which%20not%20only%20incorporates%20graph%0Arepresentations%20learned%20from%20complementary%20views%20for%20enhanced%20understanding%2C%0Abut%20also%20encourages%20the%20similarity%20between%20cross-domain%20example%20pairs%20with%20the%0Asame%20semantics%20for%20domain%20alignment.%20Extensive%20experiments%20on%20popular%20datasets%0Ashow%20that%20our%20CoCo%20outperforms%20these%20competing%20baselines%20in%20different%20settings%0Agenerally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04979v3&entry.124074799=Read"},
{"title": "Inference acceleration for large language models using \"stairs\" assisted\n  greedy generation", "author": "Domas Grigali\u016bnas and Mantas Luko\u0161evi\u010dius", "abstract": "  Large Language Models (LLMs) with billions of parameters are known for their\nimpressive predicting capabilities but require lots of resources to run. With\ntheir massive rise in popularity, even a small reduction in required resources\ncould have an impact on environment. On the other hand, smaller models require\nfewer resources but may sacrifice accuracy. In this work, we are proposing an\nimplementation of ``stairs'' assisted greedy generation. It is a modified\nassisted generation methodology that makes use of a smaller model's fast\ngeneration, large model's batch prediction, and \"stairs\" validation in order to\nachieve a speed up in prediction generation. Results show between 9.58 and\n17.24 percent inference time reduction compared to a stand-alone large LLM\nprediction in a text generation task without a loss in accuracy.\n", "link": "http://arxiv.org/abs/2407.19947v1", "date": "2024-07-29", "relevancy": 2.5676, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5346}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5064}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference%20acceleration%20for%20large%20language%20models%20using%20%22stairs%22%20assisted%0A%20%20greedy%20generation&body=Title%3A%20Inference%20acceleration%20for%20large%20language%20models%20using%20%22stairs%22%20assisted%0A%20%20greedy%20generation%0AAuthor%3A%20Domas%20Grigali%C5%ABnas%20and%20Mantas%20Luko%C5%A1evi%C4%8Dius%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20with%20billions%20of%20parameters%20are%20known%20for%20their%0Aimpressive%20predicting%20capabilities%20but%20require%20lots%20of%20resources%20to%20run.%20With%0Atheir%20massive%20rise%20in%20popularity%2C%20even%20a%20small%20reduction%20in%20required%20resources%0Acould%20have%20an%20impact%20on%20environment.%20On%20the%20other%20hand%2C%20smaller%20models%20require%0Afewer%20resources%20but%20may%20sacrifice%20accuracy.%20In%20this%20work%2C%20we%20are%20proposing%20an%0Aimplementation%20of%20%60%60stairs%27%27%20assisted%20greedy%20generation.%20It%20is%20a%20modified%0Aassisted%20generation%20methodology%20that%20makes%20use%20of%20a%20smaller%20model%27s%20fast%0Ageneration%2C%20large%20model%27s%20batch%20prediction%2C%20and%20%22stairs%22%20validation%20in%20order%20to%0Aachieve%20a%20speed%20up%20in%20prediction%20generation.%20Results%20show%20between%209.58%20and%0A17.24%20percent%20inference%20time%20reduction%20compared%20to%20a%20stand-alone%20large%20LLM%0Aprediction%20in%20a%20text%20generation%20task%20without%20a%20loss%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference%2520acceleration%2520for%2520large%2520language%2520models%2520using%2520%2522stairs%2522%2520assisted%250A%2520%2520greedy%2520generation%26entry.906535625%3DDomas%2520Grigali%25C5%25ABnas%2520and%2520Mantas%2520Luko%25C5%25A1evi%25C4%258Dius%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520billions%2520of%2520parameters%2520are%2520known%2520for%2520their%250Aimpressive%2520predicting%2520capabilities%2520but%2520require%2520lots%2520of%2520resources%2520to%2520run.%2520With%250Atheir%2520massive%2520rise%2520in%2520popularity%252C%2520even%2520a%2520small%2520reduction%2520in%2520required%2520resources%250Acould%2520have%2520an%2520impact%2520on%2520environment.%2520On%2520the%2520other%2520hand%252C%2520smaller%2520models%2520require%250Afewer%2520resources%2520but%2520may%2520sacrifice%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520are%2520proposing%2520an%250Aimplementation%2520of%2520%2560%2560stairs%2527%2527%2520assisted%2520greedy%2520generation.%2520It%2520is%2520a%2520modified%250Aassisted%2520generation%2520methodology%2520that%2520makes%2520use%2520of%2520a%2520smaller%2520model%2527s%2520fast%250Ageneration%252C%2520large%2520model%2527s%2520batch%2520prediction%252C%2520and%2520%2522stairs%2522%2520validation%2520in%2520order%2520to%250Aachieve%2520a%2520speed%2520up%2520in%2520prediction%2520generation.%2520Results%2520show%2520between%25209.58%2520and%250A17.24%2520percent%2520inference%2520time%2520reduction%2520compared%2520to%2520a%2520stand-alone%2520large%2520LLM%250Aprediction%2520in%2520a%2520text%2520generation%2520task%2520without%2520a%2520loss%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference%20acceleration%20for%20large%20language%20models%20using%20%22stairs%22%20assisted%0A%20%20greedy%20generation&entry.906535625=Domas%20Grigali%C5%ABnas%20and%20Mantas%20Luko%C5%A1evi%C4%8Dius&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20with%20billions%20of%20parameters%20are%20known%20for%20their%0Aimpressive%20predicting%20capabilities%20but%20require%20lots%20of%20resources%20to%20run.%20With%0Atheir%20massive%20rise%20in%20popularity%2C%20even%20a%20small%20reduction%20in%20required%20resources%0Acould%20have%20an%20impact%20on%20environment.%20On%20the%20other%20hand%2C%20smaller%20models%20require%0Afewer%20resources%20but%20may%20sacrifice%20accuracy.%20In%20this%20work%2C%20we%20are%20proposing%20an%0Aimplementation%20of%20%60%60stairs%27%27%20assisted%20greedy%20generation.%20It%20is%20a%20modified%0Aassisted%20generation%20methodology%20that%20makes%20use%20of%20a%20smaller%20model%27s%20fast%0Ageneration%2C%20large%20model%27s%20batch%20prediction%2C%20and%20%22stairs%22%20validation%20in%20order%20to%0Aachieve%20a%20speed%20up%20in%20prediction%20generation.%20Results%20show%20between%209.58%20and%0A17.24%20percent%20inference%20time%20reduction%20compared%20to%20a%20stand-alone%20large%20LLM%0Aprediction%20in%20a%20text%20generation%20task%20without%20a%20loss%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19947v1&entry.124074799=Read"},
{"title": "Boosting Graph Foundation Model from Structural Perspective", "author": "Yao Cheng and Yige Zhao and Jianxiang Yu and Xiang Li", "abstract": "  Graph foundation models have recently attracted significant attention due to\nits strong generalizability. Although existing methods resort to language\nmodels to learn unified semantic representations across domains, they disregard\nthe unique structural characteristics of graphs from different domains. To\naddress the problem, in this paper, we boost graph foundation model from\nstructural perspective and propose BooG. The model constructs virtual super\nnodes to unify structural characteristics of graph data from different domains.\nSpecifically, the super nodes fuse the information of anchor nodes and class\nlabels, where each anchor node captures the information of a node or a graph\ninstance to be classified. Instead of using the raw graph structure, we connect\nsuper nodes to all nodes within their neighborhood by virtual edges. This new\nstructure allows for effective information aggregation while unifying\ncross-domain structural characteristics. Additionally, we propose a novel\npre-training objective based on contrastive learning, which learns more\nexpressive representations for graph data and generalizes effectively to\ndifferent domains and downstream tasks. Experimental results on various\ndatasets and tasks demonstrate the superior performance of BooG. We provide our\ncode and data here: https://anonymous.4open.science/r/BooG-EE42/.\n", "link": "http://arxiv.org/abs/2407.19941v1", "date": "2024-07-29", "relevancy": 2.5037, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5053}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5021}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Graph%20Foundation%20Model%20from%20Structural%20Perspective&body=Title%3A%20Boosting%20Graph%20Foundation%20Model%20from%20Structural%20Perspective%0AAuthor%3A%20Yao%20Cheng%20and%20Yige%20Zhao%20and%20Jianxiang%20Yu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Graph%20foundation%20models%20have%20recently%20attracted%20significant%20attention%20due%20to%0Aits%20strong%20generalizability.%20Although%20existing%20methods%20resort%20to%20language%0Amodels%20to%20learn%20unified%20semantic%20representations%20across%20domains%2C%20they%20disregard%0Athe%20unique%20structural%20characteristics%20of%20graphs%20from%20different%20domains.%20To%0Aaddress%20the%20problem%2C%20in%20this%20paper%2C%20we%20boost%20graph%20foundation%20model%20from%0Astructural%20perspective%20and%20propose%20BooG.%20The%20model%20constructs%20virtual%20super%0Anodes%20to%20unify%20structural%20characteristics%20of%20graph%20data%20from%20different%20domains.%0ASpecifically%2C%20the%20super%20nodes%20fuse%20the%20information%20of%20anchor%20nodes%20and%20class%0Alabels%2C%20where%20each%20anchor%20node%20captures%20the%20information%20of%20a%20node%20or%20a%20graph%0Ainstance%20to%20be%20classified.%20Instead%20of%20using%20the%20raw%20graph%20structure%2C%20we%20connect%0Asuper%20nodes%20to%20all%20nodes%20within%20their%20neighborhood%20by%20virtual%20edges.%20This%20new%0Astructure%20allows%20for%20effective%20information%20aggregation%20while%20unifying%0Across-domain%20structural%20characteristics.%20Additionally%2C%20we%20propose%20a%20novel%0Apre-training%20objective%20based%20on%20contrastive%20learning%2C%20which%20learns%20more%0Aexpressive%20representations%20for%20graph%20data%20and%20generalizes%20effectively%20to%0Adifferent%20domains%20and%20downstream%20tasks.%20Experimental%20results%20on%20various%0Adatasets%20and%20tasks%20demonstrate%20the%20superior%20performance%20of%20BooG.%20We%20provide%20our%0Acode%20and%20data%20here%3A%20https%3A//anonymous.4open.science/r/BooG-EE42/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Graph%2520Foundation%2520Model%2520from%2520Structural%2520Perspective%26entry.906535625%3DYao%2520Cheng%2520and%2520Yige%2520Zhao%2520and%2520Jianxiang%2520Yu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Graph%2520foundation%2520models%2520have%2520recently%2520attracted%2520significant%2520attention%2520due%2520to%250Aits%2520strong%2520generalizability.%2520Although%2520existing%2520methods%2520resort%2520to%2520language%250Amodels%2520to%2520learn%2520unified%2520semantic%2520representations%2520across%2520domains%252C%2520they%2520disregard%250Athe%2520unique%2520structural%2520characteristics%2520of%2520graphs%2520from%2520different%2520domains.%2520To%250Aaddress%2520the%2520problem%252C%2520in%2520this%2520paper%252C%2520we%2520boost%2520graph%2520foundation%2520model%2520from%250Astructural%2520perspective%2520and%2520propose%2520BooG.%2520The%2520model%2520constructs%2520virtual%2520super%250Anodes%2520to%2520unify%2520structural%2520characteristics%2520of%2520graph%2520data%2520from%2520different%2520domains.%250ASpecifically%252C%2520the%2520super%2520nodes%2520fuse%2520the%2520information%2520of%2520anchor%2520nodes%2520and%2520class%250Alabels%252C%2520where%2520each%2520anchor%2520node%2520captures%2520the%2520information%2520of%2520a%2520node%2520or%2520a%2520graph%250Ainstance%2520to%2520be%2520classified.%2520Instead%2520of%2520using%2520the%2520raw%2520graph%2520structure%252C%2520we%2520connect%250Asuper%2520nodes%2520to%2520all%2520nodes%2520within%2520their%2520neighborhood%2520by%2520virtual%2520edges.%2520This%2520new%250Astructure%2520allows%2520for%2520effective%2520information%2520aggregation%2520while%2520unifying%250Across-domain%2520structural%2520characteristics.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%250Apre-training%2520objective%2520based%2520on%2520contrastive%2520learning%252C%2520which%2520learns%2520more%250Aexpressive%2520representations%2520for%2520graph%2520data%2520and%2520generalizes%2520effectively%2520to%250Adifferent%2520domains%2520and%2520downstream%2520tasks.%2520Experimental%2520results%2520on%2520various%250Adatasets%2520and%2520tasks%2520demonstrate%2520the%2520superior%2520performance%2520of%2520BooG.%2520We%2520provide%2520our%250Acode%2520and%2520data%2520here%253A%2520https%253A//anonymous.4open.science/r/BooG-EE42/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Graph%20Foundation%20Model%20from%20Structural%20Perspective&entry.906535625=Yao%20Cheng%20and%20Yige%20Zhao%20and%20Jianxiang%20Yu%20and%20Xiang%20Li&entry.1292438233=%20%20Graph%20foundation%20models%20have%20recently%20attracted%20significant%20attention%20due%20to%0Aits%20strong%20generalizability.%20Although%20existing%20methods%20resort%20to%20language%0Amodels%20to%20learn%20unified%20semantic%20representations%20across%20domains%2C%20they%20disregard%0Athe%20unique%20structural%20characteristics%20of%20graphs%20from%20different%20domains.%20To%0Aaddress%20the%20problem%2C%20in%20this%20paper%2C%20we%20boost%20graph%20foundation%20model%20from%0Astructural%20perspective%20and%20propose%20BooG.%20The%20model%20constructs%20virtual%20super%0Anodes%20to%20unify%20structural%20characteristics%20of%20graph%20data%20from%20different%20domains.%0ASpecifically%2C%20the%20super%20nodes%20fuse%20the%20information%20of%20anchor%20nodes%20and%20class%0Alabels%2C%20where%20each%20anchor%20node%20captures%20the%20information%20of%20a%20node%20or%20a%20graph%0Ainstance%20to%20be%20classified.%20Instead%20of%20using%20the%20raw%20graph%20structure%2C%20we%20connect%0Asuper%20nodes%20to%20all%20nodes%20within%20their%20neighborhood%20by%20virtual%20edges.%20This%20new%0Astructure%20allows%20for%20effective%20information%20aggregation%20while%20unifying%0Across-domain%20structural%20characteristics.%20Additionally%2C%20we%20propose%20a%20novel%0Apre-training%20objective%20based%20on%20contrastive%20learning%2C%20which%20learns%20more%0Aexpressive%20representations%20for%20graph%20data%20and%20generalizes%20effectively%20to%0Adifferent%20domains%20and%20downstream%20tasks.%20Experimental%20results%20on%20various%0Adatasets%20and%20tasks%20demonstrate%20the%20superior%20performance%20of%20BooG.%20We%20provide%20our%0Acode%20and%20data%20here%3A%20https%3A//anonymous.4open.science/r/BooG-EE42/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19941v1&entry.124074799=Read"},
{"title": "Frame Interpolation with Consecutive Brownian Bridge Diffusion", "author": "Zonglin Lyu and Ming Li and Jianbo Jiao and Chen Chen", "abstract": "  Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a\ndiffusion-based conditional image generation problem, synthesizing the\nintermediate frame given a random noise and neighboring frames. Due to the\nrelatively high resolution of videos, Latent Diffusion Models (LDMs) are\nemployed as the conditional generation model, where the autoencoder compresses\nimages into latent representations for diffusion and then reconstructs images\nfrom these latent representations. Such a formulation poses a crucial\nchallenge: VFI expects that the output is deterministically equal to the ground\ntruth intermediate frame, but LDMs randomly generate a diverse set of different\nimages when the model runs multiple times. The reason for the diverse\ngeneration is that the cumulative variance (variance accumulated at each step\nof generation) of generated latent representations in LDMs is large. This makes\nthe sampling trajectory random, resulting in diverse rather than deterministic\ngenerations. To address this problem, we propose our unique solution: Frame\nInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, we\npropose consecutive Brownian Bridge diffusion that takes a deterministic\ninitial value as input, resulting in a much smaller cumulative variance of\ngenerated latent representations. Our experiments suggest that our method can\nimprove together with the improvement of the autoencoder and achieve\nstate-of-the-art performance in VFI, leaving strong potential for further\nenhancement.\n", "link": "http://arxiv.org/abs/2405.05953v3", "date": "2024-07-29", "relevancy": 2.501, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6468}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6161}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion&body=Title%3A%20Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion%0AAuthor%3A%20Zonglin%20Lyu%20and%20Ming%20Li%20and%20Jianbo%20Jiao%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Recent%20work%20in%20Video%20Frame%20Interpolation%20%28VFI%29%20tries%20to%20formulate%20VFI%20as%20a%0Adiffusion-based%20conditional%20image%20generation%20problem%2C%20synthesizing%20the%0Aintermediate%20frame%20given%20a%20random%20noise%20and%20neighboring%20frames.%20Due%20to%20the%0Arelatively%20high%20resolution%20of%20videos%2C%20Latent%20Diffusion%20Models%20%28LDMs%29%20are%0Aemployed%20as%20the%20conditional%20generation%20model%2C%20where%20the%20autoencoder%20compresses%0Aimages%20into%20latent%20representations%20for%20diffusion%20and%20then%20reconstructs%20images%0Afrom%20these%20latent%20representations.%20Such%20a%20formulation%20poses%20a%20crucial%0Achallenge%3A%20VFI%20expects%20that%20the%20output%20is%20deterministically%20equal%20to%20the%20ground%0Atruth%20intermediate%20frame%2C%20but%20LDMs%20randomly%20generate%20a%20diverse%20set%20of%20different%0Aimages%20when%20the%20model%20runs%20multiple%20times.%20The%20reason%20for%20the%20diverse%0Ageneration%20is%20that%20the%20cumulative%20variance%20%28variance%20accumulated%20at%20each%20step%0Aof%20generation%29%20of%20generated%20latent%20representations%20in%20LDMs%20is%20large.%20This%20makes%0Athe%20sampling%20trajectory%20random%2C%20resulting%20in%20diverse%20rather%20than%20deterministic%0Agenerations.%20To%20address%20this%20problem%2C%20we%20propose%20our%20unique%20solution%3A%20Frame%0AInterpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion.%20Specifically%2C%20we%0Apropose%20consecutive%20Brownian%20Bridge%20diffusion%20that%20takes%20a%20deterministic%0Ainitial%20value%20as%20input%2C%20resulting%20in%20a%20much%20smaller%20cumulative%20variance%20of%0Agenerated%20latent%20representations.%20Our%20experiments%20suggest%20that%20our%20method%20can%0Aimprove%20together%20with%20the%20improvement%20of%20the%20autoencoder%20and%20achieve%0Astate-of-the-art%20performance%20in%20VFI%2C%20leaving%20strong%20potential%20for%20further%0Aenhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05953v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrame%2520Interpolation%2520with%2520Consecutive%2520Brownian%2520Bridge%2520Diffusion%26entry.906535625%3DZonglin%2520Lyu%2520and%2520Ming%2520Li%2520and%2520Jianbo%2520Jiao%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520work%2520in%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520tries%2520to%2520formulate%2520VFI%2520as%2520a%250Adiffusion-based%2520conditional%2520image%2520generation%2520problem%252C%2520synthesizing%2520the%250Aintermediate%2520frame%2520given%2520a%2520random%2520noise%2520and%2520neighboring%2520frames.%2520Due%2520to%2520the%250Arelatively%2520high%2520resolution%2520of%2520videos%252C%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529%2520are%250Aemployed%2520as%2520the%2520conditional%2520generation%2520model%252C%2520where%2520the%2520autoencoder%2520compresses%250Aimages%2520into%2520latent%2520representations%2520for%2520diffusion%2520and%2520then%2520reconstructs%2520images%250Afrom%2520these%2520latent%2520representations.%2520Such%2520a%2520formulation%2520poses%2520a%2520crucial%250Achallenge%253A%2520VFI%2520expects%2520that%2520the%2520output%2520is%2520deterministically%2520equal%2520to%2520the%2520ground%250Atruth%2520intermediate%2520frame%252C%2520but%2520LDMs%2520randomly%2520generate%2520a%2520diverse%2520set%2520of%2520different%250Aimages%2520when%2520the%2520model%2520runs%2520multiple%2520times.%2520The%2520reason%2520for%2520the%2520diverse%250Ageneration%2520is%2520that%2520the%2520cumulative%2520variance%2520%2528variance%2520accumulated%2520at%2520each%2520step%250Aof%2520generation%2529%2520of%2520generated%2520latent%2520representations%2520in%2520LDMs%2520is%2520large.%2520This%2520makes%250Athe%2520sampling%2520trajectory%2520random%252C%2520resulting%2520in%2520diverse%2520rather%2520than%2520deterministic%250Agenerations.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520our%2520unique%2520solution%253A%2520Frame%250AInterpolation%2520with%2520Consecutive%2520Brownian%2520Bridge%2520Diffusion.%2520Specifically%252C%2520we%250Apropose%2520consecutive%2520Brownian%2520Bridge%2520diffusion%2520that%2520takes%2520a%2520deterministic%250Ainitial%2520value%2520as%2520input%252C%2520resulting%2520in%2520a%2520much%2520smaller%2520cumulative%2520variance%2520of%250Agenerated%2520latent%2520representations.%2520Our%2520experiments%2520suggest%2520that%2520our%2520method%2520can%250Aimprove%2520together%2520with%2520the%2520improvement%2520of%2520the%2520autoencoder%2520and%2520achieve%250Astate-of-the-art%2520performance%2520in%2520VFI%252C%2520leaving%2520strong%2520potential%2520for%2520further%250Aenhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05953v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion&entry.906535625=Zonglin%20Lyu%20and%20Ming%20Li%20and%20Jianbo%20Jiao%20and%20Chen%20Chen&entry.1292438233=%20%20Recent%20work%20in%20Video%20Frame%20Interpolation%20%28VFI%29%20tries%20to%20formulate%20VFI%20as%20a%0Adiffusion-based%20conditional%20image%20generation%20problem%2C%20synthesizing%20the%0Aintermediate%20frame%20given%20a%20random%20noise%20and%20neighboring%20frames.%20Due%20to%20the%0Arelatively%20high%20resolution%20of%20videos%2C%20Latent%20Diffusion%20Models%20%28LDMs%29%20are%0Aemployed%20as%20the%20conditional%20generation%20model%2C%20where%20the%20autoencoder%20compresses%0Aimages%20into%20latent%20representations%20for%20diffusion%20and%20then%20reconstructs%20images%0Afrom%20these%20latent%20representations.%20Such%20a%20formulation%20poses%20a%20crucial%0Achallenge%3A%20VFI%20expects%20that%20the%20output%20is%20deterministically%20equal%20to%20the%20ground%0Atruth%20intermediate%20frame%2C%20but%20LDMs%20randomly%20generate%20a%20diverse%20set%20of%20different%0Aimages%20when%20the%20model%20runs%20multiple%20times.%20The%20reason%20for%20the%20diverse%0Ageneration%20is%20that%20the%20cumulative%20variance%20%28variance%20accumulated%20at%20each%20step%0Aof%20generation%29%20of%20generated%20latent%20representations%20in%20LDMs%20is%20large.%20This%20makes%0Athe%20sampling%20trajectory%20random%2C%20resulting%20in%20diverse%20rather%20than%20deterministic%0Agenerations.%20To%20address%20this%20problem%2C%20we%20propose%20our%20unique%20solution%3A%20Frame%0AInterpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion.%20Specifically%2C%20we%0Apropose%20consecutive%20Brownian%20Bridge%20diffusion%20that%20takes%20a%20deterministic%0Ainitial%20value%20as%20input%2C%20resulting%20in%20a%20much%20smaller%20cumulative%20variance%20of%0Agenerated%20latent%20representations.%20Our%20experiments%20suggest%20that%20our%20method%20can%0Aimprove%20together%20with%20the%20improvement%20of%20the%20autoencoder%20and%20achieve%0Astate-of-the-art%20performance%20in%20VFI%2C%20leaving%20strong%20potential%20for%20further%0Aenhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05953v3&entry.124074799=Read"},
{"title": "RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and\n  Accuracy in Spiking Neural Networks via Randomized Smoothing Coding", "author": "Keming Wu and Man Yao and Yuhong Chou and Xuerui Qiu and Rui Yang and Bo Xu and Guoqi Li", "abstract": "  Spiking Neural Networks (SNNs) have received widespread attention due to\ntheir unique neuronal dynamics and low-power nature. Previous research\nempirically shows that SNNs with Poisson coding are more robust than Artificial\nNeural Networks (ANNs) on small-scale datasets. However, it is still unclear in\ntheory how the adversarial robustness of SNNs is derived, and whether SNNs can\nstill maintain its adversarial robustness advantage on large-scale dataset\ntasks. This work theoretically demonstrates that SNN's inherent adversarial\nrobustness stems from its Poisson coding. We reveal the conceptual equivalence\nof Poisson coding and randomized smoothing in defense strategies, and analyze\nin depth the trade-off between accuracy and adversarial robustness in SNNs via\nthe proposed Randomized Smoothing Coding (RSC) method. Experiments demonstrate\nthat the proposed RSC-SNNs show remarkable adversarial robustness, surpassing\nANNs and achieving state-of-the-art robustness results on large-scale dataset\nImageNet. Our open-source implementation code is available at this https URL:\nhttps://github.com/KemingWu/RSC-SNN.\n", "link": "http://arxiv.org/abs/2407.20099v1", "date": "2024-07-29", "relevancy": 2.4891, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.507}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5046}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSC-SNN%3A%20Exploring%20the%20Trade-off%20Between%20Adversarial%20Robustness%20and%0A%20%20Accuracy%20in%20Spiking%20Neural%20Networks%20via%20Randomized%20Smoothing%20Coding&body=Title%3A%20RSC-SNN%3A%20Exploring%20the%20Trade-off%20Between%20Adversarial%20Robustness%20and%0A%20%20Accuracy%20in%20Spiking%20Neural%20Networks%20via%20Randomized%20Smoothing%20Coding%0AAuthor%3A%20Keming%20Wu%20and%20Man%20Yao%20and%20Yuhong%20Chou%20and%20Xuerui%20Qiu%20and%20Rui%20Yang%20and%20Bo%20Xu%20and%20Guoqi%20Li%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20received%20widespread%20attention%20due%20to%0Atheir%20unique%20neuronal%20dynamics%20and%20low-power%20nature.%20Previous%20research%0Aempirically%20shows%20that%20SNNs%20with%20Poisson%20coding%20are%20more%20robust%20than%20Artificial%0ANeural%20Networks%20%28ANNs%29%20on%20small-scale%20datasets.%20However%2C%20it%20is%20still%20unclear%20in%0Atheory%20how%20the%20adversarial%20robustness%20of%20SNNs%20is%20derived%2C%20and%20whether%20SNNs%20can%0Astill%20maintain%20its%20adversarial%20robustness%20advantage%20on%20large-scale%20dataset%0Atasks.%20This%20work%20theoretically%20demonstrates%20that%20SNN%27s%20inherent%20adversarial%0Arobustness%20stems%20from%20its%20Poisson%20coding.%20We%20reveal%20the%20conceptual%20equivalence%0Aof%20Poisson%20coding%20and%20randomized%20smoothing%20in%20defense%20strategies%2C%20and%20analyze%0Ain%20depth%20the%20trade-off%20between%20accuracy%20and%20adversarial%20robustness%20in%20SNNs%20via%0Athe%20proposed%20Randomized%20Smoothing%20Coding%20%28RSC%29%20method.%20Experiments%20demonstrate%0Athat%20the%20proposed%20RSC-SNNs%20show%20remarkable%20adversarial%20robustness%2C%20surpassing%0AANNs%20and%20achieving%20state-of-the-art%20robustness%20results%20on%20large-scale%20dataset%0AImageNet.%20Our%20open-source%20implementation%20code%20is%20available%20at%20this%20https%20URL%3A%0Ahttps%3A//github.com/KemingWu/RSC-SNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSC-SNN%253A%2520Exploring%2520the%2520Trade-off%2520Between%2520Adversarial%2520Robustness%2520and%250A%2520%2520Accuracy%2520in%2520Spiking%2520Neural%2520Networks%2520via%2520Randomized%2520Smoothing%2520Coding%26entry.906535625%3DKeming%2520Wu%2520and%2520Man%2520Yao%2520and%2520Yuhong%2520Chou%2520and%2520Xuerui%2520Qiu%2520and%2520Rui%2520Yang%2520and%2520Bo%2520Xu%2520and%2520Guoqi%2520Li%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520have%2520received%2520widespread%2520attention%2520due%2520to%250Atheir%2520unique%2520neuronal%2520dynamics%2520and%2520low-power%2520nature.%2520Previous%2520research%250Aempirically%2520shows%2520that%2520SNNs%2520with%2520Poisson%2520coding%2520are%2520more%2520robust%2520than%2520Artificial%250ANeural%2520Networks%2520%2528ANNs%2529%2520on%2520small-scale%2520datasets.%2520However%252C%2520it%2520is%2520still%2520unclear%2520in%250Atheory%2520how%2520the%2520adversarial%2520robustness%2520of%2520SNNs%2520is%2520derived%252C%2520and%2520whether%2520SNNs%2520can%250Astill%2520maintain%2520its%2520adversarial%2520robustness%2520advantage%2520on%2520large-scale%2520dataset%250Atasks.%2520This%2520work%2520theoretically%2520demonstrates%2520that%2520SNN%2527s%2520inherent%2520adversarial%250Arobustness%2520stems%2520from%2520its%2520Poisson%2520coding.%2520We%2520reveal%2520the%2520conceptual%2520equivalence%250Aof%2520Poisson%2520coding%2520and%2520randomized%2520smoothing%2520in%2520defense%2520strategies%252C%2520and%2520analyze%250Ain%2520depth%2520the%2520trade-off%2520between%2520accuracy%2520and%2520adversarial%2520robustness%2520in%2520SNNs%2520via%250Athe%2520proposed%2520Randomized%2520Smoothing%2520Coding%2520%2528RSC%2529%2520method.%2520Experiments%2520demonstrate%250Athat%2520the%2520proposed%2520RSC-SNNs%2520show%2520remarkable%2520adversarial%2520robustness%252C%2520surpassing%250AANNs%2520and%2520achieving%2520state-of-the-art%2520robustness%2520results%2520on%2520large-scale%2520dataset%250AImageNet.%2520Our%2520open-source%2520implementation%2520code%2520is%2520available%2520at%2520this%2520https%2520URL%253A%250Ahttps%253A//github.com/KemingWu/RSC-SNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSC-SNN%3A%20Exploring%20the%20Trade-off%20Between%20Adversarial%20Robustness%20and%0A%20%20Accuracy%20in%20Spiking%20Neural%20Networks%20via%20Randomized%20Smoothing%20Coding&entry.906535625=Keming%20Wu%20and%20Man%20Yao%20and%20Yuhong%20Chou%20and%20Xuerui%20Qiu%20and%20Rui%20Yang%20and%20Bo%20Xu%20and%20Guoqi%20Li&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20received%20widespread%20attention%20due%20to%0Atheir%20unique%20neuronal%20dynamics%20and%20low-power%20nature.%20Previous%20research%0Aempirically%20shows%20that%20SNNs%20with%20Poisson%20coding%20are%20more%20robust%20than%20Artificial%0ANeural%20Networks%20%28ANNs%29%20on%20small-scale%20datasets.%20However%2C%20it%20is%20still%20unclear%20in%0Atheory%20how%20the%20adversarial%20robustness%20of%20SNNs%20is%20derived%2C%20and%20whether%20SNNs%20can%0Astill%20maintain%20its%20adversarial%20robustness%20advantage%20on%20large-scale%20dataset%0Atasks.%20This%20work%20theoretically%20demonstrates%20that%20SNN%27s%20inherent%20adversarial%0Arobustness%20stems%20from%20its%20Poisson%20coding.%20We%20reveal%20the%20conceptual%20equivalence%0Aof%20Poisson%20coding%20and%20randomized%20smoothing%20in%20defense%20strategies%2C%20and%20analyze%0Ain%20depth%20the%20trade-off%20between%20accuracy%20and%20adversarial%20robustness%20in%20SNNs%20via%0Athe%20proposed%20Randomized%20Smoothing%20Coding%20%28RSC%29%20method.%20Experiments%20demonstrate%0Athat%20the%20proposed%20RSC-SNNs%20show%20remarkable%20adversarial%20robustness%2C%20surpassing%0AANNs%20and%20achieving%20state-of-the-art%20robustness%20results%20on%20large-scale%20dataset%0AImageNet.%20Our%20open-source%20implementation%20code%20is%20available%20at%20this%20https%20URL%3A%0Ahttps%3A//github.com/KemingWu/RSC-SNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20099v1&entry.124074799=Read"},
{"title": "Classification, Regression and Segmentation directly from k-Space in\n  Cardiac MRI", "author": "Ruochen Li and Jiazhen Pan and Youxiang Zhu and Juncheng Ni and Daniel Rueckert", "abstract": "  Cardiac Magnetic Resonance Imaging (CMR) is the gold standard for diagnosing\ncardiovascular diseases. Clinical diagnoses predominantly rely on\nmagnitude-only Digital Imaging and Communications in Medicine (DICOM) images,\nomitting crucial phase information that might provide additional diagnostic\nbenefits. In contrast, k-space is complex-valued and encompasses both magnitude\nand phase information, while humans cannot directly perceive. In this work, we\npropose KMAE, a Transformer-based model specifically designed to process\nk-space data directly, eliminating conventional intermediary conversion steps\nto the image domain. KMAE can handle critical cardiac disease classification,\nrelevant phenotype regression, and cardiac morphology segmentation tasks. We\nutilize this model to investigate the potential of k-space-based diagnosis in\ncardiac MRI. Notably, this model achieves competitive classification and\nregression performance compared to image-domain methods e.g. Masked\nAutoencoders (MAEs) and delivers satisfactory segmentation performance with a\nmyocardium dice score of 0.884. Last but not least, our model exhibits robust\nperformance with consistent results even when the k-space is 8* undersampled.\nWe encourage the MR community to explore the untapped potential of k-space and\npursue end-to-end, automated diagnosis with reduced human intervention.\n", "link": "http://arxiv.org/abs/2407.20108v1", "date": "2024-07-29", "relevancy": 2.4797, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4872}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%2C%20Regression%20and%20Segmentation%20directly%20from%20k-Space%20in%0A%20%20Cardiac%20MRI&body=Title%3A%20Classification%2C%20Regression%20and%20Segmentation%20directly%20from%20k-Space%20in%0A%20%20Cardiac%20MRI%0AAuthor%3A%20Ruochen%20Li%20and%20Jiazhen%20Pan%20and%20Youxiang%20Zhu%20and%20Juncheng%20Ni%20and%20Daniel%20Rueckert%0AAbstract%3A%20%20%20Cardiac%20Magnetic%20Resonance%20Imaging%20%28CMR%29%20is%20the%20gold%20standard%20for%20diagnosing%0Acardiovascular%20diseases.%20Clinical%20diagnoses%20predominantly%20rely%20on%0Amagnitude-only%20Digital%20Imaging%20and%20Communications%20in%20Medicine%20%28DICOM%29%20images%2C%0Aomitting%20crucial%20phase%20information%20that%20might%20provide%20additional%20diagnostic%0Abenefits.%20In%20contrast%2C%20k-space%20is%20complex-valued%20and%20encompasses%20both%20magnitude%0Aand%20phase%20information%2C%20while%20humans%20cannot%20directly%20perceive.%20In%20this%20work%2C%20we%0Apropose%20KMAE%2C%20a%20Transformer-based%20model%20specifically%20designed%20to%20process%0Ak-space%20data%20directly%2C%20eliminating%20conventional%20intermediary%20conversion%20steps%0Ato%20the%20image%20domain.%20KMAE%20can%20handle%20critical%20cardiac%20disease%20classification%2C%0Arelevant%20phenotype%20regression%2C%20and%20cardiac%20morphology%20segmentation%20tasks.%20We%0Autilize%20this%20model%20to%20investigate%20the%20potential%20of%20k-space-based%20diagnosis%20in%0Acardiac%20MRI.%20Notably%2C%20this%20model%20achieves%20competitive%20classification%20and%0Aregression%20performance%20compared%20to%20image-domain%20methods%20e.g.%20Masked%0AAutoencoders%20%28MAEs%29%20and%20delivers%20satisfactory%20segmentation%20performance%20with%20a%0Amyocardium%20dice%20score%20of%200.884.%20Last%20but%20not%20least%2C%20our%20model%20exhibits%20robust%0Aperformance%20with%20consistent%20results%20even%20when%20the%20k-space%20is%208%2A%20undersampled.%0AWe%20encourage%20the%20MR%20community%20to%20explore%20the%20untapped%20potential%20of%20k-space%20and%0Apursue%20end-to-end%2C%20automated%20diagnosis%20with%20reduced%20human%20intervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%252C%2520Regression%2520and%2520Segmentation%2520directly%2520from%2520k-Space%2520in%250A%2520%2520Cardiac%2520MRI%26entry.906535625%3DRuochen%2520Li%2520and%2520Jiazhen%2520Pan%2520and%2520Youxiang%2520Zhu%2520and%2520Juncheng%2520Ni%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3D%2520%2520Cardiac%2520Magnetic%2520Resonance%2520Imaging%2520%2528CMR%2529%2520is%2520the%2520gold%2520standard%2520for%2520diagnosing%250Acardiovascular%2520diseases.%2520Clinical%2520diagnoses%2520predominantly%2520rely%2520on%250Amagnitude-only%2520Digital%2520Imaging%2520and%2520Communications%2520in%2520Medicine%2520%2528DICOM%2529%2520images%252C%250Aomitting%2520crucial%2520phase%2520information%2520that%2520might%2520provide%2520additional%2520diagnostic%250Abenefits.%2520In%2520contrast%252C%2520k-space%2520is%2520complex-valued%2520and%2520encompasses%2520both%2520magnitude%250Aand%2520phase%2520information%252C%2520while%2520humans%2520cannot%2520directly%2520perceive.%2520In%2520this%2520work%252C%2520we%250Apropose%2520KMAE%252C%2520a%2520Transformer-based%2520model%2520specifically%2520designed%2520to%2520process%250Ak-space%2520data%2520directly%252C%2520eliminating%2520conventional%2520intermediary%2520conversion%2520steps%250Ato%2520the%2520image%2520domain.%2520KMAE%2520can%2520handle%2520critical%2520cardiac%2520disease%2520classification%252C%250Arelevant%2520phenotype%2520regression%252C%2520and%2520cardiac%2520morphology%2520segmentation%2520tasks.%2520We%250Autilize%2520this%2520model%2520to%2520investigate%2520the%2520potential%2520of%2520k-space-based%2520diagnosis%2520in%250Acardiac%2520MRI.%2520Notably%252C%2520this%2520model%2520achieves%2520competitive%2520classification%2520and%250Aregression%2520performance%2520compared%2520to%2520image-domain%2520methods%2520e.g.%2520Masked%250AAutoencoders%2520%2528MAEs%2529%2520and%2520delivers%2520satisfactory%2520segmentation%2520performance%2520with%2520a%250Amyocardium%2520dice%2520score%2520of%25200.884.%2520Last%2520but%2520not%2520least%252C%2520our%2520model%2520exhibits%2520robust%250Aperformance%2520with%2520consistent%2520results%2520even%2520when%2520the%2520k-space%2520is%25208%252A%2520undersampled.%250AWe%2520encourage%2520the%2520MR%2520community%2520to%2520explore%2520the%2520untapped%2520potential%2520of%2520k-space%2520and%250Apursue%2520end-to-end%252C%2520automated%2520diagnosis%2520with%2520reduced%2520human%2520intervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%2C%20Regression%20and%20Segmentation%20directly%20from%20k-Space%20in%0A%20%20Cardiac%20MRI&entry.906535625=Ruochen%20Li%20and%20Jiazhen%20Pan%20and%20Youxiang%20Zhu%20and%20Juncheng%20Ni%20and%20Daniel%20Rueckert&entry.1292438233=%20%20Cardiac%20Magnetic%20Resonance%20Imaging%20%28CMR%29%20is%20the%20gold%20standard%20for%20diagnosing%0Acardiovascular%20diseases.%20Clinical%20diagnoses%20predominantly%20rely%20on%0Amagnitude-only%20Digital%20Imaging%20and%20Communications%20in%20Medicine%20%28DICOM%29%20images%2C%0Aomitting%20crucial%20phase%20information%20that%20might%20provide%20additional%20diagnostic%0Abenefits.%20In%20contrast%2C%20k-space%20is%20complex-valued%20and%20encompasses%20both%20magnitude%0Aand%20phase%20information%2C%20while%20humans%20cannot%20directly%20perceive.%20In%20this%20work%2C%20we%0Apropose%20KMAE%2C%20a%20Transformer-based%20model%20specifically%20designed%20to%20process%0Ak-space%20data%20directly%2C%20eliminating%20conventional%20intermediary%20conversion%20steps%0Ato%20the%20image%20domain.%20KMAE%20can%20handle%20critical%20cardiac%20disease%20classification%2C%0Arelevant%20phenotype%20regression%2C%20and%20cardiac%20morphology%20segmentation%20tasks.%20We%0Autilize%20this%20model%20to%20investigate%20the%20potential%20of%20k-space-based%20diagnosis%20in%0Acardiac%20MRI.%20Notably%2C%20this%20model%20achieves%20competitive%20classification%20and%0Aregression%20performance%20compared%20to%20image-domain%20methods%20e.g.%20Masked%0AAutoencoders%20%28MAEs%29%20and%20delivers%20satisfactory%20segmentation%20performance%20with%20a%0Amyocardium%20dice%20score%20of%200.884.%20Last%20but%20not%20least%2C%20our%20model%20exhibits%20robust%0Aperformance%20with%20consistent%20results%20even%20when%20the%20k-space%20is%208%2A%20undersampled.%0AWe%20encourage%20the%20MR%20community%20to%20explore%20the%20untapped%20potential%20of%20k-space%20and%0Apursue%20end-to-end%2C%20automated%20diagnosis%20with%20reduced%20human%20intervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20108v1&entry.124074799=Read"},
{"title": "FreeLong: Training-Free Long Video Generation with SpectralBlend\n  Temporal Attention", "author": "Yu Lu and Yuanzhi Liang and Linchao Zhu and Yi Yang", "abstract": "  Video diffusion models have made substantial progress in various video\ngeneration applications. However, training models for long video generation\ntasks require significant computational and data resources, posing a challenge\nto developing long video diffusion models. This paper investigates a\nstraightforward and training-free approach to extend an existing short video\ndiffusion model (e.g. pre-trained on 16-frame videos) for consistent long video\ngeneration (e.g. 128 frames). Our preliminary observation has found that\ndirectly applying the short video diffusion model to generate long videos can\nlead to severe video quality degradation. Further investigation reveals that\nthis degradation is primarily due to the distortion of high-frequency\ncomponents in long videos, characterized by a decrease in spatial\nhigh-frequency components and an increase in temporal high-frequency\ncomponents. Motivated by this, we propose a novel solution named FreeLong to\nbalance the frequency distribution of long video features during the denoising\nprocess. FreeLong blends the low-frequency components of global video features,\nwhich encapsulate the entire video sequence, with the high-frequency components\nof local video features that focus on shorter subsequences of frames. This\napproach maintains global consistency while incorporating diverse and\nhigh-quality spatiotemporal details from local videos, enhancing both the\nconsistency and fidelity of long video generation. We evaluated FreeLong on\nmultiple base video diffusion models and observed significant improvements.\nAdditionally, our method supports coherent multi-prompt generation, ensuring\nboth visual coherence and seamless transitions between scenes.\n", "link": "http://arxiv.org/abs/2407.19918v1", "date": "2024-07-29", "relevancy": 2.4512, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6307}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6163}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeLong%3A%20Training-Free%20Long%20Video%20Generation%20with%20SpectralBlend%0A%20%20Temporal%20Attention&body=Title%3A%20FreeLong%3A%20Training-Free%20Long%20Video%20Generation%20with%20SpectralBlend%0A%20%20Temporal%20Attention%0AAuthor%3A%20Yu%20Lu%20and%20Yuanzhi%20Liang%20and%20Linchao%20Zhu%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Video%20diffusion%20models%20have%20made%20substantial%20progress%20in%20various%20video%0Ageneration%20applications.%20However%2C%20training%20models%20for%20long%20video%20generation%0Atasks%20require%20significant%20computational%20and%20data%20resources%2C%20posing%20a%20challenge%0Ato%20developing%20long%20video%20diffusion%20models.%20This%20paper%20investigates%20a%0Astraightforward%20and%20training-free%20approach%20to%20extend%20an%20existing%20short%20video%0Adiffusion%20model%20%28e.g.%20pre-trained%20on%2016-frame%20videos%29%20for%20consistent%20long%20video%0Ageneration%20%28e.g.%20128%20frames%29.%20Our%20preliminary%20observation%20has%20found%20that%0Adirectly%20applying%20the%20short%20video%20diffusion%20model%20to%20generate%20long%20videos%20can%0Alead%20to%20severe%20video%20quality%20degradation.%20Further%20investigation%20reveals%20that%0Athis%20degradation%20is%20primarily%20due%20to%20the%20distortion%20of%20high-frequency%0Acomponents%20in%20long%20videos%2C%20characterized%20by%20a%20decrease%20in%20spatial%0Ahigh-frequency%20components%20and%20an%20increase%20in%20temporal%20high-frequency%0Acomponents.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%20solution%20named%20FreeLong%20to%0Abalance%20the%20frequency%20distribution%20of%20long%20video%20features%20during%20the%20denoising%0Aprocess.%20FreeLong%20blends%20the%20low-frequency%20components%20of%20global%20video%20features%2C%0Awhich%20encapsulate%20the%20entire%20video%20sequence%2C%20with%20the%20high-frequency%20components%0Aof%20local%20video%20features%20that%20focus%20on%20shorter%20subsequences%20of%20frames.%20This%0Aapproach%20maintains%20global%20consistency%20while%20incorporating%20diverse%20and%0Ahigh-quality%20spatiotemporal%20details%20from%20local%20videos%2C%20enhancing%20both%20the%0Aconsistency%20and%20fidelity%20of%20long%20video%20generation.%20We%20evaluated%20FreeLong%20on%0Amultiple%20base%20video%20diffusion%20models%20and%20observed%20significant%20improvements.%0AAdditionally%2C%20our%20method%20supports%20coherent%20multi-prompt%20generation%2C%20ensuring%0Aboth%20visual%20coherence%20and%20seamless%20transitions%20between%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeLong%253A%2520Training-Free%2520Long%2520Video%2520Generation%2520with%2520SpectralBlend%250A%2520%2520Temporal%2520Attention%26entry.906535625%3DYu%2520Lu%2520and%2520Yuanzhi%2520Liang%2520and%2520Linchao%2520Zhu%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Video%2520diffusion%2520models%2520have%2520made%2520substantial%2520progress%2520in%2520various%2520video%250Ageneration%2520applications.%2520However%252C%2520training%2520models%2520for%2520long%2520video%2520generation%250Atasks%2520require%2520significant%2520computational%2520and%2520data%2520resources%252C%2520posing%2520a%2520challenge%250Ato%2520developing%2520long%2520video%2520diffusion%2520models.%2520This%2520paper%2520investigates%2520a%250Astraightforward%2520and%2520training-free%2520approach%2520to%2520extend%2520an%2520existing%2520short%2520video%250Adiffusion%2520model%2520%2528e.g.%2520pre-trained%2520on%252016-frame%2520videos%2529%2520for%2520consistent%2520long%2520video%250Ageneration%2520%2528e.g.%2520128%2520frames%2529.%2520Our%2520preliminary%2520observation%2520has%2520found%2520that%250Adirectly%2520applying%2520the%2520short%2520video%2520diffusion%2520model%2520to%2520generate%2520long%2520videos%2520can%250Alead%2520to%2520severe%2520video%2520quality%2520degradation.%2520Further%2520investigation%2520reveals%2520that%250Athis%2520degradation%2520is%2520primarily%2520due%2520to%2520the%2520distortion%2520of%2520high-frequency%250Acomponents%2520in%2520long%2520videos%252C%2520characterized%2520by%2520a%2520decrease%2520in%2520spatial%250Ahigh-frequency%2520components%2520and%2520an%2520increase%2520in%2520temporal%2520high-frequency%250Acomponents.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520novel%2520solution%2520named%2520FreeLong%2520to%250Abalance%2520the%2520frequency%2520distribution%2520of%2520long%2520video%2520features%2520during%2520the%2520denoising%250Aprocess.%2520FreeLong%2520blends%2520the%2520low-frequency%2520components%2520of%2520global%2520video%2520features%252C%250Awhich%2520encapsulate%2520the%2520entire%2520video%2520sequence%252C%2520with%2520the%2520high-frequency%2520components%250Aof%2520local%2520video%2520features%2520that%2520focus%2520on%2520shorter%2520subsequences%2520of%2520frames.%2520This%250Aapproach%2520maintains%2520global%2520consistency%2520while%2520incorporating%2520diverse%2520and%250Ahigh-quality%2520spatiotemporal%2520details%2520from%2520local%2520videos%252C%2520enhancing%2520both%2520the%250Aconsistency%2520and%2520fidelity%2520of%2520long%2520video%2520generation.%2520We%2520evaluated%2520FreeLong%2520on%250Amultiple%2520base%2520video%2520diffusion%2520models%2520and%2520observed%2520significant%2520improvements.%250AAdditionally%252C%2520our%2520method%2520supports%2520coherent%2520multi-prompt%2520generation%252C%2520ensuring%250Aboth%2520visual%2520coherence%2520and%2520seamless%2520transitions%2520between%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeLong%3A%20Training-Free%20Long%20Video%20Generation%20with%20SpectralBlend%0A%20%20Temporal%20Attention&entry.906535625=Yu%20Lu%20and%20Yuanzhi%20Liang%20and%20Linchao%20Zhu%20and%20Yi%20Yang&entry.1292438233=%20%20Video%20diffusion%20models%20have%20made%20substantial%20progress%20in%20various%20video%0Ageneration%20applications.%20However%2C%20training%20models%20for%20long%20video%20generation%0Atasks%20require%20significant%20computational%20and%20data%20resources%2C%20posing%20a%20challenge%0Ato%20developing%20long%20video%20diffusion%20models.%20This%20paper%20investigates%20a%0Astraightforward%20and%20training-free%20approach%20to%20extend%20an%20existing%20short%20video%0Adiffusion%20model%20%28e.g.%20pre-trained%20on%2016-frame%20videos%29%20for%20consistent%20long%20video%0Ageneration%20%28e.g.%20128%20frames%29.%20Our%20preliminary%20observation%20has%20found%20that%0Adirectly%20applying%20the%20short%20video%20diffusion%20model%20to%20generate%20long%20videos%20can%0Alead%20to%20severe%20video%20quality%20degradation.%20Further%20investigation%20reveals%20that%0Athis%20degradation%20is%20primarily%20due%20to%20the%20distortion%20of%20high-frequency%0Acomponents%20in%20long%20videos%2C%20characterized%20by%20a%20decrease%20in%20spatial%0Ahigh-frequency%20components%20and%20an%20increase%20in%20temporal%20high-frequency%0Acomponents.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%20solution%20named%20FreeLong%20to%0Abalance%20the%20frequency%20distribution%20of%20long%20video%20features%20during%20the%20denoising%0Aprocess.%20FreeLong%20blends%20the%20low-frequency%20components%20of%20global%20video%20features%2C%0Awhich%20encapsulate%20the%20entire%20video%20sequence%2C%20with%20the%20high-frequency%20components%0Aof%20local%20video%20features%20that%20focus%20on%20shorter%20subsequences%20of%20frames.%20This%0Aapproach%20maintains%20global%20consistency%20while%20incorporating%20diverse%20and%0Ahigh-quality%20spatiotemporal%20details%20from%20local%20videos%2C%20enhancing%20both%20the%0Aconsistency%20and%20fidelity%20of%20long%20video%20generation.%20We%20evaluated%20FreeLong%20on%0Amultiple%20base%20video%20diffusion%20models%20and%20observed%20significant%20improvements.%0AAdditionally%2C%20our%20method%20supports%20coherent%20multi-prompt%20generation%2C%20ensuring%0Aboth%20visual%20coherence%20and%20seamless%20transitions%20between%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19918v1&entry.124074799=Read"},
{"title": "Rethinking Domain Generalization: Discriminability and Generalizability", "author": "Shaocong Long and Qianyu Zhou and Chenhao Ying and Lizhuang Ma and Yuan Luo", "abstract": "  Domain generalization(DG) endeavors to develop robust models that possess\nstrong generalizability while preserving excellent discriminability.\nNonetheless, pivotal DG techniques tend to improve the feature generalizability\nby learning domain-invariant representations, inadvertently overlooking the\nfeature discriminability. On the one hand, the simultaneous attainment of\ngeneralizability and discriminability of features presents a complex challenge,\noften entailing inherent contradictions. This challenge becomes particularly\npronounced when domain-invariant features manifest reduced discriminability\nowing to the inclusion of unstable factors, i.e., spurious correlations. On the\nother hand, prevailing domain-invariant methods can be categorized as\ncategory-level alignment, susceptible to discarding indispensable features\npossessing substantial generalizability and narrowing intra-class variations.\nTo surmount these obstacles, we rethink DG from a new perspective that\nconcurrently imbues features with formidable discriminability and robust\ngeneralizability, and present a novel framework, namely, Discriminative\nMicroscopic Distribution Alignment~(DMDA). DMDA incorporates two core\ncomponents: Selective Channel Pruning~(SCP) and Micro-level Distribution\nAlignment~(MDA). Concretely, SCP attempts to curtail redundancy within neural\nnetworks, prioritizing stable attributes conducive to accurate classification.\nThis approach alleviates the adverse effect of spurious domain invariance and\namplifies the feature discriminability. Besides, MDA accentuates micro-level\nalignment within each class, going beyond mere category-level alignment.\nExtensive experiments on four benchmark datasets corroborate that DMDA achieves\ncomparable results to state-of-the-art methods in DG, underscoring the efficacy\nof our method.\n", "link": "http://arxiv.org/abs/2309.16483v3", "date": "2024-07-29", "relevancy": 2.4244, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4976}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4815}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Domain%20Generalization%3A%20Discriminability%20and%20Generalizability&body=Title%3A%20Rethinking%20Domain%20Generalization%3A%20Discriminability%20and%20Generalizability%0AAuthor%3A%20Shaocong%20Long%20and%20Qianyu%20Zhou%20and%20Chenhao%20Ying%20and%20Lizhuang%20Ma%20and%20Yuan%20Luo%0AAbstract%3A%20%20%20Domain%20generalization%28DG%29%20endeavors%20to%20develop%20robust%20models%20that%20possess%0Astrong%20generalizability%20while%20preserving%20excellent%20discriminability.%0ANonetheless%2C%20pivotal%20DG%20techniques%20tend%20to%20improve%20the%20feature%20generalizability%0Aby%20learning%20domain-invariant%20representations%2C%20inadvertently%20overlooking%20the%0Afeature%20discriminability.%20On%20the%20one%20hand%2C%20the%20simultaneous%20attainment%20of%0Ageneralizability%20and%20discriminability%20of%20features%20presents%20a%20complex%20challenge%2C%0Aoften%20entailing%20inherent%20contradictions.%20This%20challenge%20becomes%20particularly%0Apronounced%20when%20domain-invariant%20features%20manifest%20reduced%20discriminability%0Aowing%20to%20the%20inclusion%20of%20unstable%20factors%2C%20i.e.%2C%20spurious%20correlations.%20On%20the%0Aother%20hand%2C%20prevailing%20domain-invariant%20methods%20can%20be%20categorized%20as%0Acategory-level%20alignment%2C%20susceptible%20to%20discarding%20indispensable%20features%0Apossessing%20substantial%20generalizability%20and%20narrowing%20intra-class%20variations.%0ATo%20surmount%20these%20obstacles%2C%20we%20rethink%20DG%20from%20a%20new%20perspective%20that%0Aconcurrently%20imbues%20features%20with%20formidable%20discriminability%20and%20robust%0Ageneralizability%2C%20and%20present%20a%20novel%20framework%2C%20namely%2C%20Discriminative%0AMicroscopic%20Distribution%20Alignment~%28DMDA%29.%20DMDA%20incorporates%20two%20core%0Acomponents%3A%20Selective%20Channel%20Pruning~%28SCP%29%20and%20Micro-level%20Distribution%0AAlignment~%28MDA%29.%20Concretely%2C%20SCP%20attempts%20to%20curtail%20redundancy%20within%20neural%0Anetworks%2C%20prioritizing%20stable%20attributes%20conducive%20to%20accurate%20classification.%0AThis%20approach%20alleviates%20the%20adverse%20effect%20of%20spurious%20domain%20invariance%20and%0Aamplifies%20the%20feature%20discriminability.%20Besides%2C%20MDA%20accentuates%20micro-level%0Aalignment%20within%20each%20class%2C%20going%20beyond%20mere%20category-level%20alignment.%0AExtensive%20experiments%20on%20four%20benchmark%20datasets%20corroborate%20that%20DMDA%20achieves%0Acomparable%20results%20to%20state-of-the-art%20methods%20in%20DG%2C%20underscoring%20the%20efficacy%0Aof%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16483v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Domain%2520Generalization%253A%2520Discriminability%2520and%2520Generalizability%26entry.906535625%3DShaocong%2520Long%2520and%2520Qianyu%2520Zhou%2520and%2520Chenhao%2520Ying%2520and%2520Lizhuang%2520Ma%2520and%2520Yuan%2520Luo%26entry.1292438233%3D%2520%2520Domain%2520generalization%2528DG%2529%2520endeavors%2520to%2520develop%2520robust%2520models%2520that%2520possess%250Astrong%2520generalizability%2520while%2520preserving%2520excellent%2520discriminability.%250ANonetheless%252C%2520pivotal%2520DG%2520techniques%2520tend%2520to%2520improve%2520the%2520feature%2520generalizability%250Aby%2520learning%2520domain-invariant%2520representations%252C%2520inadvertently%2520overlooking%2520the%250Afeature%2520discriminability.%2520On%2520the%2520one%2520hand%252C%2520the%2520simultaneous%2520attainment%2520of%250Ageneralizability%2520and%2520discriminability%2520of%2520features%2520presents%2520a%2520complex%2520challenge%252C%250Aoften%2520entailing%2520inherent%2520contradictions.%2520This%2520challenge%2520becomes%2520particularly%250Apronounced%2520when%2520domain-invariant%2520features%2520manifest%2520reduced%2520discriminability%250Aowing%2520to%2520the%2520inclusion%2520of%2520unstable%2520factors%252C%2520i.e.%252C%2520spurious%2520correlations.%2520On%2520the%250Aother%2520hand%252C%2520prevailing%2520domain-invariant%2520methods%2520can%2520be%2520categorized%2520as%250Acategory-level%2520alignment%252C%2520susceptible%2520to%2520discarding%2520indispensable%2520features%250Apossessing%2520substantial%2520generalizability%2520and%2520narrowing%2520intra-class%2520variations.%250ATo%2520surmount%2520these%2520obstacles%252C%2520we%2520rethink%2520DG%2520from%2520a%2520new%2520perspective%2520that%250Aconcurrently%2520imbues%2520features%2520with%2520formidable%2520discriminability%2520and%2520robust%250Ageneralizability%252C%2520and%2520present%2520a%2520novel%2520framework%252C%2520namely%252C%2520Discriminative%250AMicroscopic%2520Distribution%2520Alignment~%2528DMDA%2529.%2520DMDA%2520incorporates%2520two%2520core%250Acomponents%253A%2520Selective%2520Channel%2520Pruning~%2528SCP%2529%2520and%2520Micro-level%2520Distribution%250AAlignment~%2528MDA%2529.%2520Concretely%252C%2520SCP%2520attempts%2520to%2520curtail%2520redundancy%2520within%2520neural%250Anetworks%252C%2520prioritizing%2520stable%2520attributes%2520conducive%2520to%2520accurate%2520classification.%250AThis%2520approach%2520alleviates%2520the%2520adverse%2520effect%2520of%2520spurious%2520domain%2520invariance%2520and%250Aamplifies%2520the%2520feature%2520discriminability.%2520Besides%252C%2520MDA%2520accentuates%2520micro-level%250Aalignment%2520within%2520each%2520class%252C%2520going%2520beyond%2520mere%2520category-level%2520alignment.%250AExtensive%2520experiments%2520on%2520four%2520benchmark%2520datasets%2520corroborate%2520that%2520DMDA%2520achieves%250Acomparable%2520results%2520to%2520state-of-the-art%2520methods%2520in%2520DG%252C%2520underscoring%2520the%2520efficacy%250Aof%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16483v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Domain%20Generalization%3A%20Discriminability%20and%20Generalizability&entry.906535625=Shaocong%20Long%20and%20Qianyu%20Zhou%20and%20Chenhao%20Ying%20and%20Lizhuang%20Ma%20and%20Yuan%20Luo&entry.1292438233=%20%20Domain%20generalization%28DG%29%20endeavors%20to%20develop%20robust%20models%20that%20possess%0Astrong%20generalizability%20while%20preserving%20excellent%20discriminability.%0ANonetheless%2C%20pivotal%20DG%20techniques%20tend%20to%20improve%20the%20feature%20generalizability%0Aby%20learning%20domain-invariant%20representations%2C%20inadvertently%20overlooking%20the%0Afeature%20discriminability.%20On%20the%20one%20hand%2C%20the%20simultaneous%20attainment%20of%0Ageneralizability%20and%20discriminability%20of%20features%20presents%20a%20complex%20challenge%2C%0Aoften%20entailing%20inherent%20contradictions.%20This%20challenge%20becomes%20particularly%0Apronounced%20when%20domain-invariant%20features%20manifest%20reduced%20discriminability%0Aowing%20to%20the%20inclusion%20of%20unstable%20factors%2C%20i.e.%2C%20spurious%20correlations.%20On%20the%0Aother%20hand%2C%20prevailing%20domain-invariant%20methods%20can%20be%20categorized%20as%0Acategory-level%20alignment%2C%20susceptible%20to%20discarding%20indispensable%20features%0Apossessing%20substantial%20generalizability%20and%20narrowing%20intra-class%20variations.%0ATo%20surmount%20these%20obstacles%2C%20we%20rethink%20DG%20from%20a%20new%20perspective%20that%0Aconcurrently%20imbues%20features%20with%20formidable%20discriminability%20and%20robust%0Ageneralizability%2C%20and%20present%20a%20novel%20framework%2C%20namely%2C%20Discriminative%0AMicroscopic%20Distribution%20Alignment~%28DMDA%29.%20DMDA%20incorporates%20two%20core%0Acomponents%3A%20Selective%20Channel%20Pruning~%28SCP%29%20and%20Micro-level%20Distribution%0AAlignment~%28MDA%29.%20Concretely%2C%20SCP%20attempts%20to%20curtail%20redundancy%20within%20neural%0Anetworks%2C%20prioritizing%20stable%20attributes%20conducive%20to%20accurate%20classification.%0AThis%20approach%20alleviates%20the%20adverse%20effect%20of%20spurious%20domain%20invariance%20and%0Aamplifies%20the%20feature%20discriminability.%20Besides%2C%20MDA%20accentuates%20micro-level%0Aalignment%20within%20each%20class%2C%20going%20beyond%20mere%20category-level%20alignment.%0AExtensive%20experiments%20on%20four%20benchmark%20datasets%20corroborate%20that%20DMDA%20achieves%0Acomparable%20results%20to%20state-of-the-art%20methods%20in%20DG%2C%20underscoring%20the%20efficacy%0Aof%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16483v3&entry.124074799=Read"},
{"title": "MimiQ: Low-Bit Data-Free Quantization of Vision Transformers", "author": "Kanghyun Choi and Hye Yoon Lee and Dain Kwon and SunJong Park and Kyuyeun Kim and Noseong Park and Jinho Lee", "abstract": "  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we identify that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From the observation of aligned attention, we find\nthat aligning attention maps of synthetic data helps to improve the overall\nperformance of quantized ViTs. Motivated by this finding, we devise \\aname, a\nnovel DFQ method designed for ViTs that focuses on inter-head attention\nsimilarity. First, we generate synthetic data by aligning head-wise attention\nresponses in relation to spatial query patches. Then, we apply head-wise\nstructural attention distillation to align the attention maps of the quantized\nnetwork to those of the full-precision teacher. The experimental results show\nthat the proposed method significantly outperforms baselines, setting a new\nstate-of-the-art performance for data-free ViT quantization.\n", "link": "http://arxiv.org/abs/2407.20021v1", "date": "2024-07-29", "relevancy": 2.3937, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6114}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5903}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MimiQ%3A%20Low-Bit%20Data-Free%20Quantization%20of%20Vision%20Transformers&body=Title%3A%20MimiQ%3A%20Low-Bit%20Data-Free%20Quantization%20of%20Vision%20Transformers%0AAuthor%3A%20Kanghyun%20Choi%20and%20Hye%20Yoon%20Lee%20and%20Dain%20Kwon%20and%20SunJong%20Park%20and%20Kyuyeun%20Kim%20and%20Noseong%20Park%20and%20Jinho%20Lee%0AAbstract%3A%20%20%20Data-free%20quantization%20%28DFQ%29%20is%20a%20technique%20that%20creates%20a%20lightweight%0Anetwork%20from%20its%20full-precision%20counterpart%20without%20the%20original%20training%20data%2C%0Aoften%20through%20a%20synthetic%20dataset.%20Although%20several%20DFQ%20methods%20have%20been%0Aproposed%20for%20vision%20transformer%20%28ViT%29%20architectures%2C%20they%20fail%20to%20achieve%0Aefficacy%20in%20low-bit%20settings.%20Examining%20the%20existing%20methods%2C%20we%20identify%20that%0Atheir%20synthetic%20data%20produce%20misaligned%20attention%20maps%2C%20while%20those%20of%20the%20real%0Asamples%20are%20highly%20aligned.%20From%20the%20observation%20of%20aligned%20attention%2C%20we%20find%0Athat%20aligning%20attention%20maps%20of%20synthetic%20data%20helps%20to%20improve%20the%20overall%0Aperformance%20of%20quantized%20ViTs.%20Motivated%20by%20this%20finding%2C%20we%20devise%20%5Caname%2C%20a%0Anovel%20DFQ%20method%20designed%20for%20ViTs%20that%20focuses%20on%20inter-head%20attention%0Asimilarity.%20First%2C%20we%20generate%20synthetic%20data%20by%20aligning%20head-wise%20attention%0Aresponses%20in%20relation%20to%20spatial%20query%20patches.%20Then%2C%20we%20apply%20head-wise%0Astructural%20attention%20distillation%20to%20align%20the%20attention%20maps%20of%20the%20quantized%0Anetwork%20to%20those%20of%20the%20full-precision%20teacher.%20The%20experimental%20results%20show%0Athat%20the%20proposed%20method%20significantly%20outperforms%20baselines%2C%20setting%20a%20new%0Astate-of-the-art%20performance%20for%20data-free%20ViT%20quantization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimiQ%253A%2520Low-Bit%2520Data-Free%2520Quantization%2520of%2520Vision%2520Transformers%26entry.906535625%3DKanghyun%2520Choi%2520and%2520Hye%2520Yoon%2520Lee%2520and%2520Dain%2520Kwon%2520and%2520SunJong%2520Park%2520and%2520Kyuyeun%2520Kim%2520and%2520Noseong%2520Park%2520and%2520Jinho%2520Lee%26entry.1292438233%3D%2520%2520Data-free%2520quantization%2520%2528DFQ%2529%2520is%2520a%2520technique%2520that%2520creates%2520a%2520lightweight%250Anetwork%2520from%2520its%2520full-precision%2520counterpart%2520without%2520the%2520original%2520training%2520data%252C%250Aoften%2520through%2520a%2520synthetic%2520dataset.%2520Although%2520several%2520DFQ%2520methods%2520have%2520been%250Aproposed%2520for%2520vision%2520transformer%2520%2528ViT%2529%2520architectures%252C%2520they%2520fail%2520to%2520achieve%250Aefficacy%2520in%2520low-bit%2520settings.%2520Examining%2520the%2520existing%2520methods%252C%2520we%2520identify%2520that%250Atheir%2520synthetic%2520data%2520produce%2520misaligned%2520attention%2520maps%252C%2520while%2520those%2520of%2520the%2520real%250Asamples%2520are%2520highly%2520aligned.%2520From%2520the%2520observation%2520of%2520aligned%2520attention%252C%2520we%2520find%250Athat%2520aligning%2520attention%2520maps%2520of%2520synthetic%2520data%2520helps%2520to%2520improve%2520the%2520overall%250Aperformance%2520of%2520quantized%2520ViTs.%2520Motivated%2520by%2520this%2520finding%252C%2520we%2520devise%2520%255Caname%252C%2520a%250Anovel%2520DFQ%2520method%2520designed%2520for%2520ViTs%2520that%2520focuses%2520on%2520inter-head%2520attention%250Asimilarity.%2520First%252C%2520we%2520generate%2520synthetic%2520data%2520by%2520aligning%2520head-wise%2520attention%250Aresponses%2520in%2520relation%2520to%2520spatial%2520query%2520patches.%2520Then%252C%2520we%2520apply%2520head-wise%250Astructural%2520attention%2520distillation%2520to%2520align%2520the%2520attention%2520maps%2520of%2520the%2520quantized%250Anetwork%2520to%2520those%2520of%2520the%2520full-precision%2520teacher.%2520The%2520experimental%2520results%2520show%250Athat%2520the%2520proposed%2520method%2520significantly%2520outperforms%2520baselines%252C%2520setting%2520a%2520new%250Astate-of-the-art%2520performance%2520for%2520data-free%2520ViT%2520quantization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MimiQ%3A%20Low-Bit%20Data-Free%20Quantization%20of%20Vision%20Transformers&entry.906535625=Kanghyun%20Choi%20and%20Hye%20Yoon%20Lee%20and%20Dain%20Kwon%20and%20SunJong%20Park%20and%20Kyuyeun%20Kim%20and%20Noseong%20Park%20and%20Jinho%20Lee&entry.1292438233=%20%20Data-free%20quantization%20%28DFQ%29%20is%20a%20technique%20that%20creates%20a%20lightweight%0Anetwork%20from%20its%20full-precision%20counterpart%20without%20the%20original%20training%20data%2C%0Aoften%20through%20a%20synthetic%20dataset.%20Although%20several%20DFQ%20methods%20have%20been%0Aproposed%20for%20vision%20transformer%20%28ViT%29%20architectures%2C%20they%20fail%20to%20achieve%0Aefficacy%20in%20low-bit%20settings.%20Examining%20the%20existing%20methods%2C%20we%20identify%20that%0Atheir%20synthetic%20data%20produce%20misaligned%20attention%20maps%2C%20while%20those%20of%20the%20real%0Asamples%20are%20highly%20aligned.%20From%20the%20observation%20of%20aligned%20attention%2C%20we%20find%0Athat%20aligning%20attention%20maps%20of%20synthetic%20data%20helps%20to%20improve%20the%20overall%0Aperformance%20of%20quantized%20ViTs.%20Motivated%20by%20this%20finding%2C%20we%20devise%20%5Caname%2C%20a%0Anovel%20DFQ%20method%20designed%20for%20ViTs%20that%20focuses%20on%20inter-head%20attention%0Asimilarity.%20First%2C%20we%20generate%20synthetic%20data%20by%20aligning%20head-wise%20attention%0Aresponses%20in%20relation%20to%20spatial%20query%20patches.%20Then%2C%20we%20apply%20head-wise%0Astructural%20attention%20distillation%20to%20align%20the%20attention%20maps%20of%20the%20quantized%0Anetwork%20to%20those%20of%20the%20full-precision%20teacher.%20The%20experimental%20results%20show%0Athat%20the%20proposed%20method%20significantly%20outperforms%20baselines%2C%20setting%20a%20new%0Astate-of-the-art%20performance%20for%20data-free%20ViT%20quantization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20021v1&entry.124074799=Read"},
{"title": "xAI-Drop: Don't Use What You Cannot Explain", "author": "Vincenzo Marco De Luca and Antonio Longa and Andrea Passerini and Pietro Li\u00f2", "abstract": "  Graph Neural Networks (GNNs) have emerged as the predominant paradigm for\nlearning from graph-structured data, offering a wide range of applications from\nsocial network analysis to bioinformatics. Despite their versatility, GNNs face\nchallenges such as oversmoothing, lack of generalization and poor\ninterpretability, which hinder their wider adoption and reliability in critical\napplications. Dropping has emerged as an effective paradigm for reducing noise\nduring training and improving robustness of GNNs. However, existing approaches\noften rely on random or heuristic-based selection criteria, lacking a\nprincipled method to identify and exclude nodes that contribute to noise and\nover-complexity in the model. In this work, we argue that explainability should\nbe a key indicator of a model's robustness throughout its training phase. To\nthis end, we introduce xAI-Drop, a novel topological-level dropping regularizer\nthat leverages explainability to pinpoint noisy network elements to be excluded\nfrom the GNN propagation mechanism. An empirical evaluation on diverse\nreal-world datasets demonstrates that our method outperforms current\nstate-of-the-art dropping approaches in accuracy, effectively reduces\nover-smoothing, and improves explanation quality.\n", "link": "http://arxiv.org/abs/2407.20067v1", "date": "2024-07-29", "relevancy": 2.366, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5044}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4663}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xAI-Drop%3A%20Don%27t%20Use%20What%20You%20Cannot%20Explain&body=Title%3A%20xAI-Drop%3A%20Don%27t%20Use%20What%20You%20Cannot%20Explain%0AAuthor%3A%20Vincenzo%20Marco%20De%20Luca%20and%20Antonio%20Longa%20and%20Andrea%20Passerini%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20the%20predominant%20paradigm%20for%0Alearning%20from%20graph-structured%20data%2C%20offering%20a%20wide%20range%20of%20applications%20from%0Asocial%20network%20analysis%20to%20bioinformatics.%20Despite%20their%20versatility%2C%20GNNs%20face%0Achallenges%20such%20as%20oversmoothing%2C%20lack%20of%20generalization%20and%20poor%0Ainterpretability%2C%20which%20hinder%20their%20wider%20adoption%20and%20reliability%20in%20critical%0Aapplications.%20Dropping%20has%20emerged%20as%20an%20effective%20paradigm%20for%20reducing%20noise%0Aduring%20training%20and%20improving%20robustness%20of%20GNNs.%20However%2C%20existing%20approaches%0Aoften%20rely%20on%20random%20or%20heuristic-based%20selection%20criteria%2C%20lacking%20a%0Aprincipled%20method%20to%20identify%20and%20exclude%20nodes%20that%20contribute%20to%20noise%20and%0Aover-complexity%20in%20the%20model.%20In%20this%20work%2C%20we%20argue%20that%20explainability%20should%0Abe%20a%20key%20indicator%20of%20a%20model%27s%20robustness%20throughout%20its%20training%20phase.%20To%0Athis%20end%2C%20we%20introduce%20xAI-Drop%2C%20a%20novel%20topological-level%20dropping%20regularizer%0Athat%20leverages%20explainability%20to%20pinpoint%20noisy%20network%20elements%20to%20be%20excluded%0Afrom%20the%20GNN%20propagation%20mechanism.%20An%20empirical%20evaluation%20on%20diverse%0Areal-world%20datasets%20demonstrates%20that%20our%20method%20outperforms%20current%0Astate-of-the-art%20dropping%20approaches%20in%20accuracy%2C%20effectively%20reduces%0Aover-smoothing%2C%20and%20improves%20explanation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxAI-Drop%253A%2520Don%2527t%2520Use%2520What%2520You%2520Cannot%2520Explain%26entry.906535625%3DVincenzo%2520Marco%2520De%2520Luca%2520and%2520Antonio%2520Longa%2520and%2520Andrea%2520Passerini%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520the%2520predominant%2520paradigm%2520for%250Alearning%2520from%2520graph-structured%2520data%252C%2520offering%2520a%2520wide%2520range%2520of%2520applications%2520from%250Asocial%2520network%2520analysis%2520to%2520bioinformatics.%2520Despite%2520their%2520versatility%252C%2520GNNs%2520face%250Achallenges%2520such%2520as%2520oversmoothing%252C%2520lack%2520of%2520generalization%2520and%2520poor%250Ainterpretability%252C%2520which%2520hinder%2520their%2520wider%2520adoption%2520and%2520reliability%2520in%2520critical%250Aapplications.%2520Dropping%2520has%2520emerged%2520as%2520an%2520effective%2520paradigm%2520for%2520reducing%2520noise%250Aduring%2520training%2520and%2520improving%2520robustness%2520of%2520GNNs.%2520However%252C%2520existing%2520approaches%250Aoften%2520rely%2520on%2520random%2520or%2520heuristic-based%2520selection%2520criteria%252C%2520lacking%2520a%250Aprincipled%2520method%2520to%2520identify%2520and%2520exclude%2520nodes%2520that%2520contribute%2520to%2520noise%2520and%250Aover-complexity%2520in%2520the%2520model.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520explainability%2520should%250Abe%2520a%2520key%2520indicator%2520of%2520a%2520model%2527s%2520robustness%2520throughout%2520its%2520training%2520phase.%2520To%250Athis%2520end%252C%2520we%2520introduce%2520xAI-Drop%252C%2520a%2520novel%2520topological-level%2520dropping%2520regularizer%250Athat%2520leverages%2520explainability%2520to%2520pinpoint%2520noisy%2520network%2520elements%2520to%2520be%2520excluded%250Afrom%2520the%2520GNN%2520propagation%2520mechanism.%2520An%2520empirical%2520evaluation%2520on%2520diverse%250Areal-world%2520datasets%2520demonstrates%2520that%2520our%2520method%2520outperforms%2520current%250Astate-of-the-art%2520dropping%2520approaches%2520in%2520accuracy%252C%2520effectively%2520reduces%250Aover-smoothing%252C%2520and%2520improves%2520explanation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xAI-Drop%3A%20Don%27t%20Use%20What%20You%20Cannot%20Explain&entry.906535625=Vincenzo%20Marco%20De%20Luca%20and%20Antonio%20Longa%20and%20Andrea%20Passerini%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20the%20predominant%20paradigm%20for%0Alearning%20from%20graph-structured%20data%2C%20offering%20a%20wide%20range%20of%20applications%20from%0Asocial%20network%20analysis%20to%20bioinformatics.%20Despite%20their%20versatility%2C%20GNNs%20face%0Achallenges%20such%20as%20oversmoothing%2C%20lack%20of%20generalization%20and%20poor%0Ainterpretability%2C%20which%20hinder%20their%20wider%20adoption%20and%20reliability%20in%20critical%0Aapplications.%20Dropping%20has%20emerged%20as%20an%20effective%20paradigm%20for%20reducing%20noise%0Aduring%20training%20and%20improving%20robustness%20of%20GNNs.%20However%2C%20existing%20approaches%0Aoften%20rely%20on%20random%20or%20heuristic-based%20selection%20criteria%2C%20lacking%20a%0Aprincipled%20method%20to%20identify%20and%20exclude%20nodes%20that%20contribute%20to%20noise%20and%0Aover-complexity%20in%20the%20model.%20In%20this%20work%2C%20we%20argue%20that%20explainability%20should%0Abe%20a%20key%20indicator%20of%20a%20model%27s%20robustness%20throughout%20its%20training%20phase.%20To%0Athis%20end%2C%20we%20introduce%20xAI-Drop%2C%20a%20novel%20topological-level%20dropping%20regularizer%0Athat%20leverages%20explainability%20to%20pinpoint%20noisy%20network%20elements%20to%20be%20excluded%0Afrom%20the%20GNN%20propagation%20mechanism.%20An%20empirical%20evaluation%20on%20diverse%0Areal-world%20datasets%20demonstrates%20that%20our%20method%20outperforms%20current%0Astate-of-the-art%20dropping%20approaches%20in%20accuracy%2C%20effectively%20reduces%0Aover-smoothing%2C%20and%20improves%20explanation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20067v1&entry.124074799=Read"},
{"title": "rLLM: Relational Table Learning with LLMs", "author": "Weichen Li and Xiaotong Huang and Jianwu Zheng and Zheng Wang and Chaokun Wang and Li Pan and Jianhua Li", "abstract": "  We introduce rLLM (relationLLM), a PyTorch library designed for Relational\nTable Learning (RTL) with Large Language Models (LLMs). The core idea is to\ndecompose state-of-the-art Graph Neural Networks, LLMs, and Table Neural\nNetworks into standardized modules, to enable the fast construction of novel\nRTL-type models in a simple \"combine, align, and co-train\" manner. To\nillustrate the usage of rLLM, we introduce a simple RTL method named\n\\textbf{BRIDGE}. Additionally, we present three novel relational tabular\ndatasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hope\nrLLM can serve as a useful and easy-to-use development framework for\nRTL-related tasks. Our code is available at:\nhttps://github.com/rllm-project/rllm.\n", "link": "http://arxiv.org/abs/2407.20157v1", "date": "2024-07-29", "relevancy": 2.3548, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4944}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4683}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20rLLM%3A%20Relational%20Table%20Learning%20with%20LLMs&body=Title%3A%20rLLM%3A%20Relational%20Table%20Learning%20with%20LLMs%0AAuthor%3A%20Weichen%20Li%20and%20Xiaotong%20Huang%20and%20Jianwu%20Zheng%20and%20Zheng%20Wang%20and%20Chaokun%20Wang%20and%20Li%20Pan%20and%20Jianhua%20Li%0AAbstract%3A%20%20%20We%20introduce%20rLLM%20%28relationLLM%29%2C%20a%20PyTorch%20library%20designed%20for%20Relational%0ATable%20Learning%20%28RTL%29%20with%20Large%20Language%20Models%20%28LLMs%29.%20The%20core%20idea%20is%20to%0Adecompose%20state-of-the-art%20Graph%20Neural%20Networks%2C%20LLMs%2C%20and%20Table%20Neural%0ANetworks%20into%20standardized%20modules%2C%20to%20enable%20the%20fast%20construction%20of%20novel%0ARTL-type%20models%20in%20a%20simple%20%22combine%2C%20align%2C%20and%20co-train%22%20manner.%20To%0Aillustrate%20the%20usage%20of%20rLLM%2C%20we%20introduce%20a%20simple%20RTL%20method%20named%0A%5Ctextbf%7BBRIDGE%7D.%20Additionally%2C%20we%20present%20three%20novel%20relational%20tabular%0Adatasets%20%28TML1M%2C%20TLF2K%2C%20and%20TACM12K%29%20by%20enhancing%20classic%20datasets.%20We%20hope%0ArLLM%20can%20serve%20as%20a%20useful%20and%20easy-to-use%20development%20framework%20for%0ARTL-related%20tasks.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/rllm-project/rllm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DrLLM%253A%2520Relational%2520Table%2520Learning%2520with%2520LLMs%26entry.906535625%3DWeichen%2520Li%2520and%2520Xiaotong%2520Huang%2520and%2520Jianwu%2520Zheng%2520and%2520Zheng%2520Wang%2520and%2520Chaokun%2520Wang%2520and%2520Li%2520Pan%2520and%2520Jianhua%2520Li%26entry.1292438233%3D%2520%2520We%2520introduce%2520rLLM%2520%2528relationLLM%2529%252C%2520a%2520PyTorch%2520library%2520designed%2520for%2520Relational%250ATable%2520Learning%2520%2528RTL%2529%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520The%2520core%2520idea%2520is%2520to%250Adecompose%2520state-of-the-art%2520Graph%2520Neural%2520Networks%252C%2520LLMs%252C%2520and%2520Table%2520Neural%250ANetworks%2520into%2520standardized%2520modules%252C%2520to%2520enable%2520the%2520fast%2520construction%2520of%2520novel%250ARTL-type%2520models%2520in%2520a%2520simple%2520%2522combine%252C%2520align%252C%2520and%2520co-train%2522%2520manner.%2520To%250Aillustrate%2520the%2520usage%2520of%2520rLLM%252C%2520we%2520introduce%2520a%2520simple%2520RTL%2520method%2520named%250A%255Ctextbf%257BBRIDGE%257D.%2520Additionally%252C%2520we%2520present%2520three%2520novel%2520relational%2520tabular%250Adatasets%2520%2528TML1M%252C%2520TLF2K%252C%2520and%2520TACM12K%2529%2520by%2520enhancing%2520classic%2520datasets.%2520We%2520hope%250ArLLM%2520can%2520serve%2520as%2520a%2520useful%2520and%2520easy-to-use%2520development%2520framework%2520for%250ARTL-related%2520tasks.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/rllm-project/rllm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=rLLM%3A%20Relational%20Table%20Learning%20with%20LLMs&entry.906535625=Weichen%20Li%20and%20Xiaotong%20Huang%20and%20Jianwu%20Zheng%20and%20Zheng%20Wang%20and%20Chaokun%20Wang%20and%20Li%20Pan%20and%20Jianhua%20Li&entry.1292438233=%20%20We%20introduce%20rLLM%20%28relationLLM%29%2C%20a%20PyTorch%20library%20designed%20for%20Relational%0ATable%20Learning%20%28RTL%29%20with%20Large%20Language%20Models%20%28LLMs%29.%20The%20core%20idea%20is%20to%0Adecompose%20state-of-the-art%20Graph%20Neural%20Networks%2C%20LLMs%2C%20and%20Table%20Neural%0ANetworks%20into%20standardized%20modules%2C%20to%20enable%20the%20fast%20construction%20of%20novel%0ARTL-type%20models%20in%20a%20simple%20%22combine%2C%20align%2C%20and%20co-train%22%20manner.%20To%0Aillustrate%20the%20usage%20of%20rLLM%2C%20we%20introduce%20a%20simple%20RTL%20method%20named%0A%5Ctextbf%7BBRIDGE%7D.%20Additionally%2C%20we%20present%20three%20novel%20relational%20tabular%0Adatasets%20%28TML1M%2C%20TLF2K%2C%20and%20TACM12K%29%20by%20enhancing%20classic%20datasets.%20We%20hope%0ArLLM%20can%20serve%20as%20a%20useful%20and%20easy-to-use%20development%20framework%20for%0ARTL-related%20tasks.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/rllm-project/rllm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20157v1&entry.124074799=Read"},
{"title": "VDB-GPDF: Online Gaussian Process Distance Field with VDB Structure", "author": "Lan Wu and Cedric Le Gentil and Teresa Vidal-Calleja", "abstract": "  Robots reason about the environment through dedicated representations.\nPopular choices for dense representations exploit Truncated Signed Distance\nFunctions (TSDF) and Octree data structures. However, TSDF is a projective\nsigned distance obtained directly from depth measurements that overestimates\nthe Euclidean distance. Octrees, despite being memory efficient, require tree\ntraversal and can lead to increased runtime in large scenarios. Other\nrepresentations based on Gaussian Process (GP) distance fields are appealing\ndue to their probabilistic and continuous nature, but the computational\ncomplexity is a concern. In this paper, we present an online efficient mapping\nframework that seamlessly couples GP distance fields and the fast-access VDB\ndata structure. This framework incrementally builds the Euclidean distance\nfield and fuses other surface properties, like intensity or colour, into a\nglobal scene representation that can cater for large-scale scenarios. The key\naspect is a latent Local GP Signed Distance Field (L-GPDF) contained in a local\nVDB structure that allows fast queries of the Euclidean distance, surface\nproperties and their uncertainties for arbitrary points in the field of view.\nProbabilistic fusion is then performed by merging the inferred values of these\npoints into a global VDB structure that is efficiently maintained over time.\nAfter fusion, the surface mesh is recovered, and a global GP Signed Distance\nField (G-GPDF) is generated and made available for downstream applications to\nquery accurate distance and gradients. A comparison with the state-of-the-art\nframeworks shows superior efficiency and accuracy of the inferred distance\nfield and comparable reconstruction performance. The accompanying code will be\npublicly available. https://github.com/UTS-RI/VDB_GPDF\n", "link": "http://arxiv.org/abs/2407.09649v2", "date": "2024-07-29", "relevancy": 2.3538, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6071}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5869}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VDB-GPDF%3A%20Online%20Gaussian%20Process%20Distance%20Field%20with%20VDB%20Structure&body=Title%3A%20VDB-GPDF%3A%20Online%20Gaussian%20Process%20Distance%20Field%20with%20VDB%20Structure%0AAuthor%3A%20Lan%20Wu%20and%20Cedric%20Le%20Gentil%20and%20Teresa%20Vidal-Calleja%0AAbstract%3A%20%20%20Robots%20reason%20about%20the%20environment%20through%20dedicated%20representations.%0APopular%20choices%20for%20dense%20representations%20exploit%20Truncated%20Signed%20Distance%0AFunctions%20%28TSDF%29%20and%20Octree%20data%20structures.%20However%2C%20TSDF%20is%20a%20projective%0Asigned%20distance%20obtained%20directly%20from%20depth%20measurements%20that%20overestimates%0Athe%20Euclidean%20distance.%20Octrees%2C%20despite%20being%20memory%20efficient%2C%20require%20tree%0Atraversal%20and%20can%20lead%20to%20increased%20runtime%20in%20large%20scenarios.%20Other%0Arepresentations%20based%20on%20Gaussian%20Process%20%28GP%29%20distance%20fields%20are%20appealing%0Adue%20to%20their%20probabilistic%20and%20continuous%20nature%2C%20but%20the%20computational%0Acomplexity%20is%20a%20concern.%20In%20this%20paper%2C%20we%20present%20an%20online%20efficient%20mapping%0Aframework%20that%20seamlessly%20couples%20GP%20distance%20fields%20and%20the%20fast-access%20VDB%0Adata%20structure.%20This%20framework%20incrementally%20builds%20the%20Euclidean%20distance%0Afield%20and%20fuses%20other%20surface%20properties%2C%20like%20intensity%20or%20colour%2C%20into%20a%0Aglobal%20scene%20representation%20that%20can%20cater%20for%20large-scale%20scenarios.%20The%20key%0Aaspect%20is%20a%20latent%20Local%20GP%20Signed%20Distance%20Field%20%28L-GPDF%29%20contained%20in%20a%20local%0AVDB%20structure%20that%20allows%20fast%20queries%20of%20the%20Euclidean%20distance%2C%20surface%0Aproperties%20and%20their%20uncertainties%20for%20arbitrary%20points%20in%20the%20field%20of%20view.%0AProbabilistic%20fusion%20is%20then%20performed%20by%20merging%20the%20inferred%20values%20of%20these%0Apoints%20into%20a%20global%20VDB%20structure%20that%20is%20efficiently%20maintained%20over%20time.%0AAfter%20fusion%2C%20the%20surface%20mesh%20is%20recovered%2C%20and%20a%20global%20GP%20Signed%20Distance%0AField%20%28G-GPDF%29%20is%20generated%20and%20made%20available%20for%20downstream%20applications%20to%0Aquery%20accurate%20distance%20and%20gradients.%20A%20comparison%20with%20the%20state-of-the-art%0Aframeworks%20shows%20superior%20efficiency%20and%20accuracy%20of%20the%20inferred%20distance%0Afield%20and%20comparable%20reconstruction%20performance.%20The%20accompanying%20code%20will%20be%0Apublicly%20available.%20https%3A//github.com/UTS-RI/VDB_GPDF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09649v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVDB-GPDF%253A%2520Online%2520Gaussian%2520Process%2520Distance%2520Field%2520with%2520VDB%2520Structure%26entry.906535625%3DLan%2520Wu%2520and%2520Cedric%2520Le%2520Gentil%2520and%2520Teresa%2520Vidal-Calleja%26entry.1292438233%3D%2520%2520Robots%2520reason%2520about%2520the%2520environment%2520through%2520dedicated%2520representations.%250APopular%2520choices%2520for%2520dense%2520representations%2520exploit%2520Truncated%2520Signed%2520Distance%250AFunctions%2520%2528TSDF%2529%2520and%2520Octree%2520data%2520structures.%2520However%252C%2520TSDF%2520is%2520a%2520projective%250Asigned%2520distance%2520obtained%2520directly%2520from%2520depth%2520measurements%2520that%2520overestimates%250Athe%2520Euclidean%2520distance.%2520Octrees%252C%2520despite%2520being%2520memory%2520efficient%252C%2520require%2520tree%250Atraversal%2520and%2520can%2520lead%2520to%2520increased%2520runtime%2520in%2520large%2520scenarios.%2520Other%250Arepresentations%2520based%2520on%2520Gaussian%2520Process%2520%2528GP%2529%2520distance%2520fields%2520are%2520appealing%250Adue%2520to%2520their%2520probabilistic%2520and%2520continuous%2520nature%252C%2520but%2520the%2520computational%250Acomplexity%2520is%2520a%2520concern.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520online%2520efficient%2520mapping%250Aframework%2520that%2520seamlessly%2520couples%2520GP%2520distance%2520fields%2520and%2520the%2520fast-access%2520VDB%250Adata%2520structure.%2520This%2520framework%2520incrementally%2520builds%2520the%2520Euclidean%2520distance%250Afield%2520and%2520fuses%2520other%2520surface%2520properties%252C%2520like%2520intensity%2520or%2520colour%252C%2520into%2520a%250Aglobal%2520scene%2520representation%2520that%2520can%2520cater%2520for%2520large-scale%2520scenarios.%2520The%2520key%250Aaspect%2520is%2520a%2520latent%2520Local%2520GP%2520Signed%2520Distance%2520Field%2520%2528L-GPDF%2529%2520contained%2520in%2520a%2520local%250AVDB%2520structure%2520that%2520allows%2520fast%2520queries%2520of%2520the%2520Euclidean%2520distance%252C%2520surface%250Aproperties%2520and%2520their%2520uncertainties%2520for%2520arbitrary%2520points%2520in%2520the%2520field%2520of%2520view.%250AProbabilistic%2520fusion%2520is%2520then%2520performed%2520by%2520merging%2520the%2520inferred%2520values%2520of%2520these%250Apoints%2520into%2520a%2520global%2520VDB%2520structure%2520that%2520is%2520efficiently%2520maintained%2520over%2520time.%250AAfter%2520fusion%252C%2520the%2520surface%2520mesh%2520is%2520recovered%252C%2520and%2520a%2520global%2520GP%2520Signed%2520Distance%250AField%2520%2528G-GPDF%2529%2520is%2520generated%2520and%2520made%2520available%2520for%2520downstream%2520applications%2520to%250Aquery%2520accurate%2520distance%2520and%2520gradients.%2520A%2520comparison%2520with%2520the%2520state-of-the-art%250Aframeworks%2520shows%2520superior%2520efficiency%2520and%2520accuracy%2520of%2520the%2520inferred%2520distance%250Afield%2520and%2520comparable%2520reconstruction%2520performance.%2520The%2520accompanying%2520code%2520will%2520be%250Apublicly%2520available.%2520https%253A//github.com/UTS-RI/VDB_GPDF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09649v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VDB-GPDF%3A%20Online%20Gaussian%20Process%20Distance%20Field%20with%20VDB%20Structure&entry.906535625=Lan%20Wu%20and%20Cedric%20Le%20Gentil%20and%20Teresa%20Vidal-Calleja&entry.1292438233=%20%20Robots%20reason%20about%20the%20environment%20through%20dedicated%20representations.%0APopular%20choices%20for%20dense%20representations%20exploit%20Truncated%20Signed%20Distance%0AFunctions%20%28TSDF%29%20and%20Octree%20data%20structures.%20However%2C%20TSDF%20is%20a%20projective%0Asigned%20distance%20obtained%20directly%20from%20depth%20measurements%20that%20overestimates%0Athe%20Euclidean%20distance.%20Octrees%2C%20despite%20being%20memory%20efficient%2C%20require%20tree%0Atraversal%20and%20can%20lead%20to%20increased%20runtime%20in%20large%20scenarios.%20Other%0Arepresentations%20based%20on%20Gaussian%20Process%20%28GP%29%20distance%20fields%20are%20appealing%0Adue%20to%20their%20probabilistic%20and%20continuous%20nature%2C%20but%20the%20computational%0Acomplexity%20is%20a%20concern.%20In%20this%20paper%2C%20we%20present%20an%20online%20efficient%20mapping%0Aframework%20that%20seamlessly%20couples%20GP%20distance%20fields%20and%20the%20fast-access%20VDB%0Adata%20structure.%20This%20framework%20incrementally%20builds%20the%20Euclidean%20distance%0Afield%20and%20fuses%20other%20surface%20properties%2C%20like%20intensity%20or%20colour%2C%20into%20a%0Aglobal%20scene%20representation%20that%20can%20cater%20for%20large-scale%20scenarios.%20The%20key%0Aaspect%20is%20a%20latent%20Local%20GP%20Signed%20Distance%20Field%20%28L-GPDF%29%20contained%20in%20a%20local%0AVDB%20structure%20that%20allows%20fast%20queries%20of%20the%20Euclidean%20distance%2C%20surface%0Aproperties%20and%20their%20uncertainties%20for%20arbitrary%20points%20in%20the%20field%20of%20view.%0AProbabilistic%20fusion%20is%20then%20performed%20by%20merging%20the%20inferred%20values%20of%20these%0Apoints%20into%20a%20global%20VDB%20structure%20that%20is%20efficiently%20maintained%20over%20time.%0AAfter%20fusion%2C%20the%20surface%20mesh%20is%20recovered%2C%20and%20a%20global%20GP%20Signed%20Distance%0AField%20%28G-GPDF%29%20is%20generated%20and%20made%20available%20for%20downstream%20applications%20to%0Aquery%20accurate%20distance%20and%20gradients.%20A%20comparison%20with%20the%20state-of-the-art%0Aframeworks%20shows%20superior%20efficiency%20and%20accuracy%20of%20the%20inferred%20distance%0Afield%20and%20comparable%20reconstruction%20performance.%20The%20accompanying%20code%20will%20be%0Apublicly%20available.%20https%3A//github.com/UTS-RI/VDB_GPDF%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09649v2&entry.124074799=Read"},
{"title": "BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor\n  Learning", "author": "Baoyuan Wu and Hongrui Chen and Mingda Zhang and Zihao Zhu and Shaokui Wei and Danni Yuan and Mingli Zhu and Ruotong Wang and Li Liu and Chao Shen", "abstract": "  As an emerging approach to explore the vulnerability of deep neural networks\n(DNNs), backdoor learning has attracted increasing interest in recent years,\nand many seminal backdoor attack and defense algorithms are being developed\nsuccessively or concurrently, in the status of a rapid arms race. However,\nmainly due to the diverse settings, and the difficulties of implementation and\nreproducibility of existing works, there is a lack of a unified and\nstandardized benchmark of backdoor learning, causing unfair comparisons or\nunreliable conclusions (e.g., misleading, biased or even false conclusions).\nConsequently, it is difficult to evaluate the current progress and design the\nfuture development roadmap of this literature. To alleviate this dilemma, we\nbuild a comprehensive benchmark of backdoor learning called BackdoorBench. Our\nbenchmark makes three valuable contributions to the research community. 1) We\nprovide an integrated implementation of state-of-the-art (SOTA) backdoor\nlearning algorithms (currently including 20 attack and 32 defense algorithms),\nbased on an extensible modular-based codebase. 2) We conduct comprehensive\nevaluations with 5 poisoning ratios, based on 4 models and 4 datasets, leading\nto 11,492 pairs of attack-against-defense evaluations in total. 3) Based on\nabove evaluations, we present abundant analysis from 10 perspectives via 18\nuseful analysis tools, and provide several inspiring insights about backdoor\nlearning. We hope that our efforts could build a solid foundation of backdoor\nlearning to facilitate researchers to investigate existing algorithms, develop\nmore innovative algorithms, and explore the intrinsic mechanism of backdoor\nlearning. Finally, we have created a user-friendly website at\nhttp://backdoorbench.com, which collects all important information of\nBackdoorBench, including codebase, docs, leaderboard, and model Zoo.\n", "link": "http://arxiv.org/abs/2407.19845v1", "date": "2024-07-29", "relevancy": 2.3324, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.485}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4595}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BackdoorBench%3A%20A%20Comprehensive%20Benchmark%20and%20Analysis%20of%20Backdoor%0A%20%20Learning&body=Title%3A%20BackdoorBench%3A%20A%20Comprehensive%20Benchmark%20and%20Analysis%20of%20Backdoor%0A%20%20Learning%0AAuthor%3A%20Baoyuan%20Wu%20and%20Hongrui%20Chen%20and%20Mingda%20Zhang%20and%20Zihao%20Zhu%20and%20Shaokui%20Wei%20and%20Danni%20Yuan%20and%20Mingli%20Zhu%20and%20Ruotong%20Wang%20and%20Li%20Liu%20and%20Chao%20Shen%0AAbstract%3A%20%20%20As%20an%20emerging%20approach%20to%20explore%20the%20vulnerability%20of%20deep%20neural%20networks%0A%28DNNs%29%2C%20backdoor%20learning%20has%20attracted%20increasing%20interest%20in%20recent%20years%2C%0Aand%20many%20seminal%20backdoor%20attack%20and%20defense%20algorithms%20are%20being%20developed%0Asuccessively%20or%20concurrently%2C%20in%20the%20status%20of%20a%20rapid%20arms%20race.%20However%2C%0Amainly%20due%20to%20the%20diverse%20settings%2C%20and%20the%20difficulties%20of%20implementation%20and%0Areproducibility%20of%20existing%20works%2C%20there%20is%20a%20lack%20of%20a%20unified%20and%0Astandardized%20benchmark%20of%20backdoor%20learning%2C%20causing%20unfair%20comparisons%20or%0Aunreliable%20conclusions%20%28e.g.%2C%20misleading%2C%20biased%20or%20even%20false%20conclusions%29.%0AConsequently%2C%20it%20is%20difficult%20to%20evaluate%20the%20current%20progress%20and%20design%20the%0Afuture%20development%20roadmap%20of%20this%20literature.%20To%20alleviate%20this%20dilemma%2C%20we%0Abuild%20a%20comprehensive%20benchmark%20of%20backdoor%20learning%20called%20BackdoorBench.%20Our%0Abenchmark%20makes%20three%20valuable%20contributions%20to%20the%20research%20community.%201%29%20We%0Aprovide%20an%20integrated%20implementation%20of%20state-of-the-art%20%28SOTA%29%20backdoor%0Alearning%20algorithms%20%28currently%20including%2020%20attack%20and%2032%20defense%20algorithms%29%2C%0Abased%20on%20an%20extensible%20modular-based%20codebase.%202%29%20We%20conduct%20comprehensive%0Aevaluations%20with%205%20poisoning%20ratios%2C%20based%20on%204%20models%20and%204%20datasets%2C%20leading%0Ato%2011%2C492%20pairs%20of%20attack-against-defense%20evaluations%20in%20total.%203%29%20Based%20on%0Aabove%20evaluations%2C%20we%20present%20abundant%20analysis%20from%2010%20perspectives%20via%2018%0Auseful%20analysis%20tools%2C%20and%20provide%20several%20inspiring%20insights%20about%20backdoor%0Alearning.%20We%20hope%20that%20our%20efforts%20could%20build%20a%20solid%20foundation%20of%20backdoor%0Alearning%20to%20facilitate%20researchers%20to%20investigate%20existing%20algorithms%2C%20develop%0Amore%20innovative%20algorithms%2C%20and%20explore%20the%20intrinsic%20mechanism%20of%20backdoor%0Alearning.%20Finally%2C%20we%20have%20created%20a%20user-friendly%20website%20at%0Ahttp%3A//backdoorbench.com%2C%20which%20collects%20all%20important%20information%20of%0ABackdoorBench%2C%20including%20codebase%2C%20docs%2C%20leaderboard%2C%20and%20model%20Zoo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackdoorBench%253A%2520A%2520Comprehensive%2520Benchmark%2520and%2520Analysis%2520of%2520Backdoor%250A%2520%2520Learning%26entry.906535625%3DBaoyuan%2520Wu%2520and%2520Hongrui%2520Chen%2520and%2520Mingda%2520Zhang%2520and%2520Zihao%2520Zhu%2520and%2520Shaokui%2520Wei%2520and%2520Danni%2520Yuan%2520and%2520Mingli%2520Zhu%2520and%2520Ruotong%2520Wang%2520and%2520Li%2520Liu%2520and%2520Chao%2520Shen%26entry.1292438233%3D%2520%2520As%2520an%2520emerging%2520approach%2520to%2520explore%2520the%2520vulnerability%2520of%2520deep%2520neural%2520networks%250A%2528DNNs%2529%252C%2520backdoor%2520learning%2520has%2520attracted%2520increasing%2520interest%2520in%2520recent%2520years%252C%250Aand%2520many%2520seminal%2520backdoor%2520attack%2520and%2520defense%2520algorithms%2520are%2520being%2520developed%250Asuccessively%2520or%2520concurrently%252C%2520in%2520the%2520status%2520of%2520a%2520rapid%2520arms%2520race.%2520However%252C%250Amainly%2520due%2520to%2520the%2520diverse%2520settings%252C%2520and%2520the%2520difficulties%2520of%2520implementation%2520and%250Areproducibility%2520of%2520existing%2520works%252C%2520there%2520is%2520a%2520lack%2520of%2520a%2520unified%2520and%250Astandardized%2520benchmark%2520of%2520backdoor%2520learning%252C%2520causing%2520unfair%2520comparisons%2520or%250Aunreliable%2520conclusions%2520%2528e.g.%252C%2520misleading%252C%2520biased%2520or%2520even%2520false%2520conclusions%2529.%250AConsequently%252C%2520it%2520is%2520difficult%2520to%2520evaluate%2520the%2520current%2520progress%2520and%2520design%2520the%250Afuture%2520development%2520roadmap%2520of%2520this%2520literature.%2520To%2520alleviate%2520this%2520dilemma%252C%2520we%250Abuild%2520a%2520comprehensive%2520benchmark%2520of%2520backdoor%2520learning%2520called%2520BackdoorBench.%2520Our%250Abenchmark%2520makes%2520three%2520valuable%2520contributions%2520to%2520the%2520research%2520community.%25201%2529%2520We%250Aprovide%2520an%2520integrated%2520implementation%2520of%2520state-of-the-art%2520%2528SOTA%2529%2520backdoor%250Alearning%2520algorithms%2520%2528currently%2520including%252020%2520attack%2520and%252032%2520defense%2520algorithms%2529%252C%250Abased%2520on%2520an%2520extensible%2520modular-based%2520codebase.%25202%2529%2520We%2520conduct%2520comprehensive%250Aevaluations%2520with%25205%2520poisoning%2520ratios%252C%2520based%2520on%25204%2520models%2520and%25204%2520datasets%252C%2520leading%250Ato%252011%252C492%2520pairs%2520of%2520attack-against-defense%2520evaluations%2520in%2520total.%25203%2529%2520Based%2520on%250Aabove%2520evaluations%252C%2520we%2520present%2520abundant%2520analysis%2520from%252010%2520perspectives%2520via%252018%250Auseful%2520analysis%2520tools%252C%2520and%2520provide%2520several%2520inspiring%2520insights%2520about%2520backdoor%250Alearning.%2520We%2520hope%2520that%2520our%2520efforts%2520could%2520build%2520a%2520solid%2520foundation%2520of%2520backdoor%250Alearning%2520to%2520facilitate%2520researchers%2520to%2520investigate%2520existing%2520algorithms%252C%2520develop%250Amore%2520innovative%2520algorithms%252C%2520and%2520explore%2520the%2520intrinsic%2520mechanism%2520of%2520backdoor%250Alearning.%2520Finally%252C%2520we%2520have%2520created%2520a%2520user-friendly%2520website%2520at%250Ahttp%253A//backdoorbench.com%252C%2520which%2520collects%2520all%2520important%2520information%2520of%250ABackdoorBench%252C%2520including%2520codebase%252C%2520docs%252C%2520leaderboard%252C%2520and%2520model%2520Zoo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BackdoorBench%3A%20A%20Comprehensive%20Benchmark%20and%20Analysis%20of%20Backdoor%0A%20%20Learning&entry.906535625=Baoyuan%20Wu%20and%20Hongrui%20Chen%20and%20Mingda%20Zhang%20and%20Zihao%20Zhu%20and%20Shaokui%20Wei%20and%20Danni%20Yuan%20and%20Mingli%20Zhu%20and%20Ruotong%20Wang%20and%20Li%20Liu%20and%20Chao%20Shen&entry.1292438233=%20%20As%20an%20emerging%20approach%20to%20explore%20the%20vulnerability%20of%20deep%20neural%20networks%0A%28DNNs%29%2C%20backdoor%20learning%20has%20attracted%20increasing%20interest%20in%20recent%20years%2C%0Aand%20many%20seminal%20backdoor%20attack%20and%20defense%20algorithms%20are%20being%20developed%0Asuccessively%20or%20concurrently%2C%20in%20the%20status%20of%20a%20rapid%20arms%20race.%20However%2C%0Amainly%20due%20to%20the%20diverse%20settings%2C%20and%20the%20difficulties%20of%20implementation%20and%0Areproducibility%20of%20existing%20works%2C%20there%20is%20a%20lack%20of%20a%20unified%20and%0Astandardized%20benchmark%20of%20backdoor%20learning%2C%20causing%20unfair%20comparisons%20or%0Aunreliable%20conclusions%20%28e.g.%2C%20misleading%2C%20biased%20or%20even%20false%20conclusions%29.%0AConsequently%2C%20it%20is%20difficult%20to%20evaluate%20the%20current%20progress%20and%20design%20the%0Afuture%20development%20roadmap%20of%20this%20literature.%20To%20alleviate%20this%20dilemma%2C%20we%0Abuild%20a%20comprehensive%20benchmark%20of%20backdoor%20learning%20called%20BackdoorBench.%20Our%0Abenchmark%20makes%20three%20valuable%20contributions%20to%20the%20research%20community.%201%29%20We%0Aprovide%20an%20integrated%20implementation%20of%20state-of-the-art%20%28SOTA%29%20backdoor%0Alearning%20algorithms%20%28currently%20including%2020%20attack%20and%2032%20defense%20algorithms%29%2C%0Abased%20on%20an%20extensible%20modular-based%20codebase.%202%29%20We%20conduct%20comprehensive%0Aevaluations%20with%205%20poisoning%20ratios%2C%20based%20on%204%20models%20and%204%20datasets%2C%20leading%0Ato%2011%2C492%20pairs%20of%20attack-against-defense%20evaluations%20in%20total.%203%29%20Based%20on%0Aabove%20evaluations%2C%20we%20present%20abundant%20analysis%20from%2010%20perspectives%20via%2018%0Auseful%20analysis%20tools%2C%20and%20provide%20several%20inspiring%20insights%20about%20backdoor%0Alearning.%20We%20hope%20that%20our%20efforts%20could%20build%20a%20solid%20foundation%20of%20backdoor%0Alearning%20to%20facilitate%20researchers%20to%20investigate%20existing%20algorithms%2C%20develop%0Amore%20innovative%20algorithms%2C%20and%20explore%20the%20intrinsic%20mechanism%20of%20backdoor%0Alearning.%20Finally%2C%20we%20have%20created%20a%20user-friendly%20website%20at%0Ahttp%3A//backdoorbench.com%2C%20which%20collects%20all%20important%20information%20of%0ABackdoorBench%2C%20including%20codebase%2C%20docs%2C%20leaderboard%2C%20and%20model%20Zoo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19845v1&entry.124074799=Read"},
{"title": "CityX: Controllable Procedural Content Generation for Unbounded 3D\n  Cities", "author": "Shougao Zhang and Mengqi Zhou and Yuxi Wang and Chuanchen Luo and Rongyu Wang and Yiwei Li and Xucheng Yin and Zhaoxiang Zhang and Junran Peng", "abstract": "  Generating a realistic, large-scale 3D virtual city remains a complex\nchallenge due to the involvement of numerous 3D assets, various city styles,\nand strict layout constraints. Existing approaches provide promising attempts\nat procedural content generation to create large-scale scenes using Blender\nagents. However, they face crucial issues such as difficulties in scaling up\ngeneration capability and achieving fine-grained control at the semantic layout\nlevel. To address these problems, we propose a novel multi-modal controllable\nprocedural content generation method, named CityX, which enhances realistic,\nunbounded 3D city generation guided by multiple layout conditions, including\nOSM, semantic maps, and satellite images. Specifically, the proposed method\ncontains a general protocol for integrating various PCG plugins and a\nmulti-agent framework for transforming instructions into executable Blender\nactions. Through this effective framework, CityX shows the potential to build\nan innovative ecosystem for 3D scene generation by bridging the gap between the\nquality of generated assets and industrial requirements. Extensive experiments\nhave demonstrated the effectiveness of our method in creating high-quality,\ndiverse, and unbounded cities guided by multi-modal conditions. Our project\npage: https://cityx-lab.github.io.\n", "link": "http://arxiv.org/abs/2407.17572v2", "date": "2024-07-29", "relevancy": 2.3146, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5836}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5777}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CityX%3A%20Controllable%20Procedural%20Content%20Generation%20for%20Unbounded%203D%0A%20%20Cities&body=Title%3A%20CityX%3A%20Controllable%20Procedural%20Content%20Generation%20for%20Unbounded%203D%0A%20%20Cities%0AAuthor%3A%20Shougao%20Zhang%20and%20Mengqi%20Zhou%20and%20Yuxi%20Wang%20and%20Chuanchen%20Luo%20and%20Rongyu%20Wang%20and%20Yiwei%20Li%20and%20Xucheng%20Yin%20and%20Zhaoxiang%20Zhang%20and%20Junran%20Peng%0AAbstract%3A%20%20%20Generating%20a%20realistic%2C%20large-scale%203D%20virtual%20city%20remains%20a%20complex%0Achallenge%20due%20to%20the%20involvement%20of%20numerous%203D%20assets%2C%20various%20city%20styles%2C%0Aand%20strict%20layout%20constraints.%20Existing%20approaches%20provide%20promising%20attempts%0Aat%20procedural%20content%20generation%20to%20create%20large-scale%20scenes%20using%20Blender%0Aagents.%20However%2C%20they%20face%20crucial%20issues%20such%20as%20difficulties%20in%20scaling%20up%0Ageneration%20capability%20and%20achieving%20fine-grained%20control%20at%20the%20semantic%20layout%0Alevel.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%20multi-modal%20controllable%0Aprocedural%20content%20generation%20method%2C%20named%20CityX%2C%20which%20enhances%20realistic%2C%0Aunbounded%203D%20city%20generation%20guided%20by%20multiple%20layout%20conditions%2C%20including%0AOSM%2C%20semantic%20maps%2C%20and%20satellite%20images.%20Specifically%2C%20the%20proposed%20method%0Acontains%20a%20general%20protocol%20for%20integrating%20various%20PCG%20plugins%20and%20a%0Amulti-agent%20framework%20for%20transforming%20instructions%20into%20executable%20Blender%0Aactions.%20Through%20this%20effective%20framework%2C%20CityX%20shows%20the%20potential%20to%20build%0Aan%20innovative%20ecosystem%20for%203D%20scene%20generation%20by%20bridging%20the%20gap%20between%20the%0Aquality%20of%20generated%20assets%20and%20industrial%20requirements.%20Extensive%20experiments%0Ahave%20demonstrated%20the%20effectiveness%20of%20our%20method%20in%20creating%20high-quality%2C%0Adiverse%2C%20and%20unbounded%20cities%20guided%20by%20multi-modal%20conditions.%20Our%20project%0Apage%3A%20https%3A//cityx-lab.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCityX%253A%2520Controllable%2520Procedural%2520Content%2520Generation%2520for%2520Unbounded%25203D%250A%2520%2520Cities%26entry.906535625%3DShougao%2520Zhang%2520and%2520Mengqi%2520Zhou%2520and%2520Yuxi%2520Wang%2520and%2520Chuanchen%2520Luo%2520and%2520Rongyu%2520Wang%2520and%2520Yiwei%2520Li%2520and%2520Xucheng%2520Yin%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Junran%2520Peng%26entry.1292438233%3D%2520%2520Generating%2520a%2520realistic%252C%2520large-scale%25203D%2520virtual%2520city%2520remains%2520a%2520complex%250Achallenge%2520due%2520to%2520the%2520involvement%2520of%2520numerous%25203D%2520assets%252C%2520various%2520city%2520styles%252C%250Aand%2520strict%2520layout%2520constraints.%2520Existing%2520approaches%2520provide%2520promising%2520attempts%250Aat%2520procedural%2520content%2520generation%2520to%2520create%2520large-scale%2520scenes%2520using%2520Blender%250Aagents.%2520However%252C%2520they%2520face%2520crucial%2520issues%2520such%2520as%2520difficulties%2520in%2520scaling%2520up%250Ageneration%2520capability%2520and%2520achieving%2520fine-grained%2520control%2520at%2520the%2520semantic%2520layout%250Alevel.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%2520multi-modal%2520controllable%250Aprocedural%2520content%2520generation%2520method%252C%2520named%2520CityX%252C%2520which%2520enhances%2520realistic%252C%250Aunbounded%25203D%2520city%2520generation%2520guided%2520by%2520multiple%2520layout%2520conditions%252C%2520including%250AOSM%252C%2520semantic%2520maps%252C%2520and%2520satellite%2520images.%2520Specifically%252C%2520the%2520proposed%2520method%250Acontains%2520a%2520general%2520protocol%2520for%2520integrating%2520various%2520PCG%2520plugins%2520and%2520a%250Amulti-agent%2520framework%2520for%2520transforming%2520instructions%2520into%2520executable%2520Blender%250Aactions.%2520Through%2520this%2520effective%2520framework%252C%2520CityX%2520shows%2520the%2520potential%2520to%2520build%250Aan%2520innovative%2520ecosystem%2520for%25203D%2520scene%2520generation%2520by%2520bridging%2520the%2520gap%2520between%2520the%250Aquality%2520of%2520generated%2520assets%2520and%2520industrial%2520requirements.%2520Extensive%2520experiments%250Ahave%2520demonstrated%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520creating%2520high-quality%252C%250Adiverse%252C%2520and%2520unbounded%2520cities%2520guided%2520by%2520multi-modal%2520conditions.%2520Our%2520project%250Apage%253A%2520https%253A//cityx-lab.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CityX%3A%20Controllable%20Procedural%20Content%20Generation%20for%20Unbounded%203D%0A%20%20Cities&entry.906535625=Shougao%20Zhang%20and%20Mengqi%20Zhou%20and%20Yuxi%20Wang%20and%20Chuanchen%20Luo%20and%20Rongyu%20Wang%20and%20Yiwei%20Li%20and%20Xucheng%20Yin%20and%20Zhaoxiang%20Zhang%20and%20Junran%20Peng&entry.1292438233=%20%20Generating%20a%20realistic%2C%20large-scale%203D%20virtual%20city%20remains%20a%20complex%0Achallenge%20due%20to%20the%20involvement%20of%20numerous%203D%20assets%2C%20various%20city%20styles%2C%0Aand%20strict%20layout%20constraints.%20Existing%20approaches%20provide%20promising%20attempts%0Aat%20procedural%20content%20generation%20to%20create%20large-scale%20scenes%20using%20Blender%0Aagents.%20However%2C%20they%20face%20crucial%20issues%20such%20as%20difficulties%20in%20scaling%20up%0Ageneration%20capability%20and%20achieving%20fine-grained%20control%20at%20the%20semantic%20layout%0Alevel.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%20multi-modal%20controllable%0Aprocedural%20content%20generation%20method%2C%20named%20CityX%2C%20which%20enhances%20realistic%2C%0Aunbounded%203D%20city%20generation%20guided%20by%20multiple%20layout%20conditions%2C%20including%0AOSM%2C%20semantic%20maps%2C%20and%20satellite%20images.%20Specifically%2C%20the%20proposed%20method%0Acontains%20a%20general%20protocol%20for%20integrating%20various%20PCG%20plugins%20and%20a%0Amulti-agent%20framework%20for%20transforming%20instructions%20into%20executable%20Blender%0Aactions.%20Through%20this%20effective%20framework%2C%20CityX%20shows%20the%20potential%20to%20build%0Aan%20innovative%20ecosystem%20for%203D%20scene%20generation%20by%20bridging%20the%20gap%20between%20the%0Aquality%20of%20generated%20assets%20and%20industrial%20requirements.%20Extensive%20experiments%0Ahave%20demonstrated%20the%20effectiveness%20of%20our%20method%20in%20creating%20high-quality%2C%0Adiverse%2C%20and%20unbounded%20cities%20guided%20by%20multi-modal%20conditions.%20Our%20project%0Apage%3A%20https%3A//cityx-lab.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17572v2&entry.124074799=Read"},
{"title": "Radiance Fields for Robotic Teleoperation", "author": "Maximum Wilder-Smith and Vaishakh Patil and Marco Hutter", "abstract": "  Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian\nSplatting (3DGS), have revolutionized graphics and novel view synthesis. Their\nability to synthesize new viewpoints with photo-realistic quality, as well as\ncapture complex volumetric and specular scenes, makes them an ideal\nvisualization for robotic teleoperation setups. Direct camera teleoperation\nprovides high-fidelity operation at the cost of maneuverability, while\nreconstruction-based approaches offer controllable scenes with lower fidelity.\nWith this in mind, we propose replacing the traditional\nreconstruction-visualization components of the robotic teleoperation pipeline\nwith online Radiance Fields, offering highly maneuverable scenes with\nphotorealistic quality. As such, there are three main contributions to state of\nthe art: (1) online training of Radiance Fields using live data from multiple\ncameras, (2) support for a variety of radiance methods including NeRF and 3DGS,\n(3) visualization suite for these methods including a virtual reality scene. To\nenable seamless integration with existing setups, these components were tested\nwith multiple robots in multiple configurations and were displayed using\ntraditional tools as well as the VR headset. The results across methods and\nrobots were compared quantitatively to a baseline of mesh reconstruction, and a\nuser study was conducted to compare the different visualization methods. For\nvideos and code, check out https://leggedrobotics.github.io/rffr.github.io/.\n", "link": "http://arxiv.org/abs/2407.20194v1", "date": "2024-07-29", "relevancy": 2.3042, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6195}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5454}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radiance%20Fields%20for%20Robotic%20Teleoperation&body=Title%3A%20Radiance%20Fields%20for%20Robotic%20Teleoperation%0AAuthor%3A%20Maximum%20Wilder-Smith%20and%20Vaishakh%20Patil%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Radiance%20field%20methods%20such%20as%20Neural%20Radiance%20Fields%20%28NeRFs%29%20or%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20have%20revolutionized%20graphics%20and%20novel%20view%20synthesis.%20Their%0Aability%20to%20synthesize%20new%20viewpoints%20with%20photo-realistic%20quality%2C%20as%20well%20as%0Acapture%20complex%20volumetric%20and%20specular%20scenes%2C%20makes%20them%20an%20ideal%0Avisualization%20for%20robotic%20teleoperation%20setups.%20Direct%20camera%20teleoperation%0Aprovides%20high-fidelity%20operation%20at%20the%20cost%20of%20maneuverability%2C%20while%0Areconstruction-based%20approaches%20offer%20controllable%20scenes%20with%20lower%20fidelity.%0AWith%20this%20in%20mind%2C%20we%20propose%20replacing%20the%20traditional%0Areconstruction-visualization%20components%20of%20the%20robotic%20teleoperation%20pipeline%0Awith%20online%20Radiance%20Fields%2C%20offering%20highly%20maneuverable%20scenes%20with%0Aphotorealistic%20quality.%20As%20such%2C%20there%20are%20three%20main%20contributions%20to%20state%20of%0Athe%20art%3A%20%281%29%20online%20training%20of%20Radiance%20Fields%20using%20live%20data%20from%20multiple%0Acameras%2C%20%282%29%20support%20for%20a%20variety%20of%20radiance%20methods%20including%20NeRF%20and%203DGS%2C%0A%283%29%20visualization%20suite%20for%20these%20methods%20including%20a%20virtual%20reality%20scene.%20To%0Aenable%20seamless%20integration%20with%20existing%20setups%2C%20these%20components%20were%20tested%0Awith%20multiple%20robots%20in%20multiple%20configurations%20and%20were%20displayed%20using%0Atraditional%20tools%20as%20well%20as%20the%20VR%20headset.%20The%20results%20across%20methods%20and%0Arobots%20were%20compared%20quantitatively%20to%20a%20baseline%20of%20mesh%20reconstruction%2C%20and%20a%0Auser%20study%20was%20conducted%20to%20compare%20the%20different%20visualization%20methods.%20For%0Avideos%20and%20code%2C%20check%20out%20https%3A//leggedrobotics.github.io/rffr.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadiance%2520Fields%2520for%2520Robotic%2520Teleoperation%26entry.906535625%3DMaximum%2520Wilder-Smith%2520and%2520Vaishakh%2520Patil%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Radiance%2520field%2520methods%2520such%2520as%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520or%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%252C%2520have%2520revolutionized%2520graphics%2520and%2520novel%2520view%2520synthesis.%2520Their%250Aability%2520to%2520synthesize%2520new%2520viewpoints%2520with%2520photo-realistic%2520quality%252C%2520as%2520well%2520as%250Acapture%2520complex%2520volumetric%2520and%2520specular%2520scenes%252C%2520makes%2520them%2520an%2520ideal%250Avisualization%2520for%2520robotic%2520teleoperation%2520setups.%2520Direct%2520camera%2520teleoperation%250Aprovides%2520high-fidelity%2520operation%2520at%2520the%2520cost%2520of%2520maneuverability%252C%2520while%250Areconstruction-based%2520approaches%2520offer%2520controllable%2520scenes%2520with%2520lower%2520fidelity.%250AWith%2520this%2520in%2520mind%252C%2520we%2520propose%2520replacing%2520the%2520traditional%250Areconstruction-visualization%2520components%2520of%2520the%2520robotic%2520teleoperation%2520pipeline%250Awith%2520online%2520Radiance%2520Fields%252C%2520offering%2520highly%2520maneuverable%2520scenes%2520with%250Aphotorealistic%2520quality.%2520As%2520such%252C%2520there%2520are%2520three%2520main%2520contributions%2520to%2520state%2520of%250Athe%2520art%253A%2520%25281%2529%2520online%2520training%2520of%2520Radiance%2520Fields%2520using%2520live%2520data%2520from%2520multiple%250Acameras%252C%2520%25282%2529%2520support%2520for%2520a%2520variety%2520of%2520radiance%2520methods%2520including%2520NeRF%2520and%25203DGS%252C%250A%25283%2529%2520visualization%2520suite%2520for%2520these%2520methods%2520including%2520a%2520virtual%2520reality%2520scene.%2520To%250Aenable%2520seamless%2520integration%2520with%2520existing%2520setups%252C%2520these%2520components%2520were%2520tested%250Awith%2520multiple%2520robots%2520in%2520multiple%2520configurations%2520and%2520were%2520displayed%2520using%250Atraditional%2520tools%2520as%2520well%2520as%2520the%2520VR%2520headset.%2520The%2520results%2520across%2520methods%2520and%250Arobots%2520were%2520compared%2520quantitatively%2520to%2520a%2520baseline%2520of%2520mesh%2520reconstruction%252C%2520and%2520a%250Auser%2520study%2520was%2520conducted%2520to%2520compare%2520the%2520different%2520visualization%2520methods.%2520For%250Avideos%2520and%2520code%252C%2520check%2520out%2520https%253A//leggedrobotics.github.io/rffr.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radiance%20Fields%20for%20Robotic%20Teleoperation&entry.906535625=Maximum%20Wilder-Smith%20and%20Vaishakh%20Patil%20and%20Marco%20Hutter&entry.1292438233=%20%20Radiance%20field%20methods%20such%20as%20Neural%20Radiance%20Fields%20%28NeRFs%29%20or%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20have%20revolutionized%20graphics%20and%20novel%20view%20synthesis.%20Their%0Aability%20to%20synthesize%20new%20viewpoints%20with%20photo-realistic%20quality%2C%20as%20well%20as%0Acapture%20complex%20volumetric%20and%20specular%20scenes%2C%20makes%20them%20an%20ideal%0Avisualization%20for%20robotic%20teleoperation%20setups.%20Direct%20camera%20teleoperation%0Aprovides%20high-fidelity%20operation%20at%20the%20cost%20of%20maneuverability%2C%20while%0Areconstruction-based%20approaches%20offer%20controllable%20scenes%20with%20lower%20fidelity.%0AWith%20this%20in%20mind%2C%20we%20propose%20replacing%20the%20traditional%0Areconstruction-visualization%20components%20of%20the%20robotic%20teleoperation%20pipeline%0Awith%20online%20Radiance%20Fields%2C%20offering%20highly%20maneuverable%20scenes%20with%0Aphotorealistic%20quality.%20As%20such%2C%20there%20are%20three%20main%20contributions%20to%20state%20of%0Athe%20art%3A%20%281%29%20online%20training%20of%20Radiance%20Fields%20using%20live%20data%20from%20multiple%0Acameras%2C%20%282%29%20support%20for%20a%20variety%20of%20radiance%20methods%20including%20NeRF%20and%203DGS%2C%0A%283%29%20visualization%20suite%20for%20these%20methods%20including%20a%20virtual%20reality%20scene.%20To%0Aenable%20seamless%20integration%20with%20existing%20setups%2C%20these%20components%20were%20tested%0Awith%20multiple%20robots%20in%20multiple%20configurations%20and%20were%20displayed%20using%0Atraditional%20tools%20as%20well%20as%20the%20VR%20headset.%20The%20results%20across%20methods%20and%0Arobots%20were%20compared%20quantitatively%20to%20a%20baseline%20of%20mesh%20reconstruction%2C%20and%20a%0Auser%20study%20was%20conducted%20to%20compare%20the%20different%20visualization%20methods.%20For%0Avideos%20and%20code%2C%20check%20out%20https%3A//leggedrobotics.github.io/rffr.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20194v1&entry.124074799=Read"},
{"title": "ImagiNet: A Multi-Content Dataset for Generalizable Synthetic Image\n  Detection via Contrastive Learning", "author": "Delyan Boychev and Radostin Cholakov", "abstract": "  Generative models, such as diffusion models (DMs), variational autoencoders\n(VAEs), and generative adversarial networks (GANs), produce images with a level\nof authenticity that makes them nearly indistinguishable from real photos and\nartwork. While this capability is beneficial for many industries, the\ndifficulty of identifying synthetic images leaves online media platforms\nvulnerable to impersonation and misinformation attempts. To support the\ndevelopment of defensive methods, we introduce ImagiNet, a high-resolution and\nbalanced dataset for synthetic image detection, designed to mitigate potential\nbiases in existing resources. It contains 200K examples, spanning four content\ncategories: photos, paintings, faces, and uncategorized. Synthetic images are\nproduced with open-source and proprietary generators, whereas real counterparts\nof the same content type are collected from public datasets. The structure of\nImagiNet allows for a two-track evaluation system: i) classification as real or\nsynthetic and ii) identification of the generative model. To establish a\nbaseline, we train a ResNet-50 model using a self-supervised contrastive\nobjective (SelfCon) for each track. The model demonstrates state-of-the-art\nperformance and high inference speed across established benchmarks, achieving\nan AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%, even under\nsocial network conditions that involve compression and resizing. Our data and\ncode are available at https://github.com/delyan-boychev/imaginet.\n", "link": "http://arxiv.org/abs/2407.20020v1", "date": "2024-07-29", "relevancy": 2.2863, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5803}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5686}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImagiNet%3A%20A%20Multi-Content%20Dataset%20for%20Generalizable%20Synthetic%20Image%0A%20%20Detection%20via%20Contrastive%20Learning&body=Title%3A%20ImagiNet%3A%20A%20Multi-Content%20Dataset%20for%20Generalizable%20Synthetic%20Image%0A%20%20Detection%20via%20Contrastive%20Learning%0AAuthor%3A%20Delyan%20Boychev%20and%20Radostin%20Cholakov%0AAbstract%3A%20%20%20Generative%20models%2C%20such%20as%20diffusion%20models%20%28DMs%29%2C%20variational%20autoencoders%0A%28VAEs%29%2C%20and%20generative%20adversarial%20networks%20%28GANs%29%2C%20produce%20images%20with%20a%20level%0Aof%20authenticity%20that%20makes%20them%20nearly%20indistinguishable%20from%20real%20photos%20and%0Aartwork.%20While%20this%20capability%20is%20beneficial%20for%20many%20industries%2C%20the%0Adifficulty%20of%20identifying%20synthetic%20images%20leaves%20online%20media%20platforms%0Avulnerable%20to%20impersonation%20and%20misinformation%20attempts.%20To%20support%20the%0Adevelopment%20of%20defensive%20methods%2C%20we%20introduce%20ImagiNet%2C%20a%20high-resolution%20and%0Abalanced%20dataset%20for%20synthetic%20image%20detection%2C%20designed%20to%20mitigate%20potential%0Abiases%20in%20existing%20resources.%20It%20contains%20200K%20examples%2C%20spanning%20four%20content%0Acategories%3A%20photos%2C%20paintings%2C%20faces%2C%20and%20uncategorized.%20Synthetic%20images%20are%0Aproduced%20with%20open-source%20and%20proprietary%20generators%2C%20whereas%20real%20counterparts%0Aof%20the%20same%20content%20type%20are%20collected%20from%20public%20datasets.%20The%20structure%20of%0AImagiNet%20allows%20for%20a%20two-track%20evaluation%20system%3A%20i%29%20classification%20as%20real%20or%0Asynthetic%20and%20ii%29%20identification%20of%20the%20generative%20model.%20To%20establish%20a%0Abaseline%2C%20we%20train%20a%20ResNet-50%20model%20using%20a%20self-supervised%20contrastive%0Aobjective%20%28SelfCon%29%20for%20each%20track.%20The%20model%20demonstrates%20state-of-the-art%0Aperformance%20and%20high%20inference%20speed%20across%20established%20benchmarks%2C%20achieving%0Aan%20AUC%20of%20up%20to%200.99%20and%20balanced%20accuracy%20ranging%20from%2086%25%20to%2095%25%2C%20even%20under%0Asocial%20network%20conditions%20that%20involve%20compression%20and%20resizing.%20Our%20data%20and%0Acode%20are%20available%20at%20https%3A//github.com/delyan-boychev/imaginet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagiNet%253A%2520A%2520Multi-Content%2520Dataset%2520for%2520Generalizable%2520Synthetic%2520Image%250A%2520%2520Detection%2520via%2520Contrastive%2520Learning%26entry.906535625%3DDelyan%2520Boychev%2520and%2520Radostin%2520Cholakov%26entry.1292438233%3D%2520%2520Generative%2520models%252C%2520such%2520as%2520diffusion%2520models%2520%2528DMs%2529%252C%2520variational%2520autoencoders%250A%2528VAEs%2529%252C%2520and%2520generative%2520adversarial%2520networks%2520%2528GANs%2529%252C%2520produce%2520images%2520with%2520a%2520level%250Aof%2520authenticity%2520that%2520makes%2520them%2520nearly%2520indistinguishable%2520from%2520real%2520photos%2520and%250Aartwork.%2520While%2520this%2520capability%2520is%2520beneficial%2520for%2520many%2520industries%252C%2520the%250Adifficulty%2520of%2520identifying%2520synthetic%2520images%2520leaves%2520online%2520media%2520platforms%250Avulnerable%2520to%2520impersonation%2520and%2520misinformation%2520attempts.%2520To%2520support%2520the%250Adevelopment%2520of%2520defensive%2520methods%252C%2520we%2520introduce%2520ImagiNet%252C%2520a%2520high-resolution%2520and%250Abalanced%2520dataset%2520for%2520synthetic%2520image%2520detection%252C%2520designed%2520to%2520mitigate%2520potential%250Abiases%2520in%2520existing%2520resources.%2520It%2520contains%2520200K%2520examples%252C%2520spanning%2520four%2520content%250Acategories%253A%2520photos%252C%2520paintings%252C%2520faces%252C%2520and%2520uncategorized.%2520Synthetic%2520images%2520are%250Aproduced%2520with%2520open-source%2520and%2520proprietary%2520generators%252C%2520whereas%2520real%2520counterparts%250Aof%2520the%2520same%2520content%2520type%2520are%2520collected%2520from%2520public%2520datasets.%2520The%2520structure%2520of%250AImagiNet%2520allows%2520for%2520a%2520two-track%2520evaluation%2520system%253A%2520i%2529%2520classification%2520as%2520real%2520or%250Asynthetic%2520and%2520ii%2529%2520identification%2520of%2520the%2520generative%2520model.%2520To%2520establish%2520a%250Abaseline%252C%2520we%2520train%2520a%2520ResNet-50%2520model%2520using%2520a%2520self-supervised%2520contrastive%250Aobjective%2520%2528SelfCon%2529%2520for%2520each%2520track.%2520The%2520model%2520demonstrates%2520state-of-the-art%250Aperformance%2520and%2520high%2520inference%2520speed%2520across%2520established%2520benchmarks%252C%2520achieving%250Aan%2520AUC%2520of%2520up%2520to%25200.99%2520and%2520balanced%2520accuracy%2520ranging%2520from%252086%2525%2520to%252095%2525%252C%2520even%2520under%250Asocial%2520network%2520conditions%2520that%2520involve%2520compression%2520and%2520resizing.%2520Our%2520data%2520and%250Acode%2520are%2520available%2520at%2520https%253A//github.com/delyan-boychev/imaginet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImagiNet%3A%20A%20Multi-Content%20Dataset%20for%20Generalizable%20Synthetic%20Image%0A%20%20Detection%20via%20Contrastive%20Learning&entry.906535625=Delyan%20Boychev%20and%20Radostin%20Cholakov&entry.1292438233=%20%20Generative%20models%2C%20such%20as%20diffusion%20models%20%28DMs%29%2C%20variational%20autoencoders%0A%28VAEs%29%2C%20and%20generative%20adversarial%20networks%20%28GANs%29%2C%20produce%20images%20with%20a%20level%0Aof%20authenticity%20that%20makes%20them%20nearly%20indistinguishable%20from%20real%20photos%20and%0Aartwork.%20While%20this%20capability%20is%20beneficial%20for%20many%20industries%2C%20the%0Adifficulty%20of%20identifying%20synthetic%20images%20leaves%20online%20media%20platforms%0Avulnerable%20to%20impersonation%20and%20misinformation%20attempts.%20To%20support%20the%0Adevelopment%20of%20defensive%20methods%2C%20we%20introduce%20ImagiNet%2C%20a%20high-resolution%20and%0Abalanced%20dataset%20for%20synthetic%20image%20detection%2C%20designed%20to%20mitigate%20potential%0Abiases%20in%20existing%20resources.%20It%20contains%20200K%20examples%2C%20spanning%20four%20content%0Acategories%3A%20photos%2C%20paintings%2C%20faces%2C%20and%20uncategorized.%20Synthetic%20images%20are%0Aproduced%20with%20open-source%20and%20proprietary%20generators%2C%20whereas%20real%20counterparts%0Aof%20the%20same%20content%20type%20are%20collected%20from%20public%20datasets.%20The%20structure%20of%0AImagiNet%20allows%20for%20a%20two-track%20evaluation%20system%3A%20i%29%20classification%20as%20real%20or%0Asynthetic%20and%20ii%29%20identification%20of%20the%20generative%20model.%20To%20establish%20a%0Abaseline%2C%20we%20train%20a%20ResNet-50%20model%20using%20a%20self-supervised%20contrastive%0Aobjective%20%28SelfCon%29%20for%20each%20track.%20The%20model%20demonstrates%20state-of-the-art%0Aperformance%20and%20high%20inference%20speed%20across%20established%20benchmarks%2C%20achieving%0Aan%20AUC%20of%20up%20to%200.99%20and%20balanced%20accuracy%20ranging%20from%2086%25%20to%2095%25%2C%20even%20under%0Asocial%20network%20conditions%20that%20involve%20compression%20and%20resizing.%20Our%20data%20and%0Acode%20are%20available%20at%20https%3A//github.com/delyan-boychev/imaginet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20020v1&entry.124074799=Read"},
{"title": "Unfolding Time: Generative Modeling for Turbulent Flows in 4D", "author": "Abdullah Saydemir and Marten Lienen and Stephan G\u00fcnnemann", "abstract": "  A recent study in turbulent flow simulation demonstrated the potential of\ngenerative diffusion models for fast 3D surrogate modeling. This approach\neliminates the need for specifying initial states or performing lengthy\nsimulations, significantly accelerating the process. While adept at sampling\nindividual frames from the learned manifold of turbulent flow states, the\nprevious model lacks the capability to generate sequences, hindering analysis\nof dynamic phenomena. This work addresses this limitation by introducing a 4D\ngenerative diffusion model and a physics-informed guidance technique that\nenables the generation of realistic sequences of flow states. Our findings\nindicate that the proposed method can successfully sample entire subsequences\nfrom the turbulent manifold, even though generalizing from individual frames to\nsequences remains a challenging task. This advancement opens doors for the\napplication of generative modeling in analyzing the temporal evolution of\nturbulent flows, providing valuable insights into their complex dynamics.\n", "link": "http://arxiv.org/abs/2406.11390v2", "date": "2024-07-29", "relevancy": 2.2788, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6263}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5584}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unfolding%20Time%3A%20Generative%20Modeling%20for%20Turbulent%20Flows%20in%204D&body=Title%3A%20Unfolding%20Time%3A%20Generative%20Modeling%20for%20Turbulent%20Flows%20in%204D%0AAuthor%3A%20Abdullah%20Saydemir%20and%20Marten%20Lienen%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20A%20recent%20study%20in%20turbulent%20flow%20simulation%20demonstrated%20the%20potential%20of%0Agenerative%20diffusion%20models%20for%20fast%203D%20surrogate%20modeling.%20This%20approach%0Aeliminates%20the%20need%20for%20specifying%20initial%20states%20or%20performing%20lengthy%0Asimulations%2C%20significantly%20accelerating%20the%20process.%20While%20adept%20at%20sampling%0Aindividual%20frames%20from%20the%20learned%20manifold%20of%20turbulent%20flow%20states%2C%20the%0Aprevious%20model%20lacks%20the%20capability%20to%20generate%20sequences%2C%20hindering%20analysis%0Aof%20dynamic%20phenomena.%20This%20work%20addresses%20this%20limitation%20by%20introducing%20a%204D%0Agenerative%20diffusion%20model%20and%20a%20physics-informed%20guidance%20technique%20that%0Aenables%20the%20generation%20of%20realistic%20sequences%20of%20flow%20states.%20Our%20findings%0Aindicate%20that%20the%20proposed%20method%20can%20successfully%20sample%20entire%20subsequences%0Afrom%20the%20turbulent%20manifold%2C%20even%20though%20generalizing%20from%20individual%20frames%20to%0Asequences%20remains%20a%20challenging%20task.%20This%20advancement%20opens%20doors%20for%20the%0Aapplication%20of%20generative%20modeling%20in%20analyzing%20the%20temporal%20evolution%20of%0Aturbulent%20flows%2C%20providing%20valuable%20insights%20into%20their%20complex%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11390v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnfolding%2520Time%253A%2520Generative%2520Modeling%2520for%2520Turbulent%2520Flows%2520in%25204D%26entry.906535625%3DAbdullah%2520Saydemir%2520and%2520Marten%2520Lienen%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520A%2520recent%2520study%2520in%2520turbulent%2520flow%2520simulation%2520demonstrated%2520the%2520potential%2520of%250Agenerative%2520diffusion%2520models%2520for%2520fast%25203D%2520surrogate%2520modeling.%2520This%2520approach%250Aeliminates%2520the%2520need%2520for%2520specifying%2520initial%2520states%2520or%2520performing%2520lengthy%250Asimulations%252C%2520significantly%2520accelerating%2520the%2520process.%2520While%2520adept%2520at%2520sampling%250Aindividual%2520frames%2520from%2520the%2520learned%2520manifold%2520of%2520turbulent%2520flow%2520states%252C%2520the%250Aprevious%2520model%2520lacks%2520the%2520capability%2520to%2520generate%2520sequences%252C%2520hindering%2520analysis%250Aof%2520dynamic%2520phenomena.%2520This%2520work%2520addresses%2520this%2520limitation%2520by%2520introducing%2520a%25204D%250Agenerative%2520diffusion%2520model%2520and%2520a%2520physics-informed%2520guidance%2520technique%2520that%250Aenables%2520the%2520generation%2520of%2520realistic%2520sequences%2520of%2520flow%2520states.%2520Our%2520findings%250Aindicate%2520that%2520the%2520proposed%2520method%2520can%2520successfully%2520sample%2520entire%2520subsequences%250Afrom%2520the%2520turbulent%2520manifold%252C%2520even%2520though%2520generalizing%2520from%2520individual%2520frames%2520to%250Asequences%2520remains%2520a%2520challenging%2520task.%2520This%2520advancement%2520opens%2520doors%2520for%2520the%250Aapplication%2520of%2520generative%2520modeling%2520in%2520analyzing%2520the%2520temporal%2520evolution%2520of%250Aturbulent%2520flows%252C%2520providing%2520valuable%2520insights%2520into%2520their%2520complex%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11390v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unfolding%20Time%3A%20Generative%20Modeling%20for%20Turbulent%20Flows%20in%204D&entry.906535625=Abdullah%20Saydemir%20and%20Marten%20Lienen%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20A%20recent%20study%20in%20turbulent%20flow%20simulation%20demonstrated%20the%20potential%20of%0Agenerative%20diffusion%20models%20for%20fast%203D%20surrogate%20modeling.%20This%20approach%0Aeliminates%20the%20need%20for%20specifying%20initial%20states%20or%20performing%20lengthy%0Asimulations%2C%20significantly%20accelerating%20the%20process.%20While%20adept%20at%20sampling%0Aindividual%20frames%20from%20the%20learned%20manifold%20of%20turbulent%20flow%20states%2C%20the%0Aprevious%20model%20lacks%20the%20capability%20to%20generate%20sequences%2C%20hindering%20analysis%0Aof%20dynamic%20phenomena.%20This%20work%20addresses%20this%20limitation%20by%20introducing%20a%204D%0Agenerative%20diffusion%20model%20and%20a%20physics-informed%20guidance%20technique%20that%0Aenables%20the%20generation%20of%20realistic%20sequences%20of%20flow%20states.%20Our%20findings%0Aindicate%20that%20the%20proposed%20method%20can%20successfully%20sample%20entire%20subsequences%0Afrom%20the%20turbulent%20manifold%2C%20even%20though%20generalizing%20from%20individual%20frames%20to%0Asequences%20remains%20a%20challenging%20task.%20This%20advancement%20opens%20doors%20for%20the%0Aapplication%20of%20generative%20modeling%20in%20analyzing%20the%20temporal%20evolution%20of%0Aturbulent%20flows%2C%20providing%20valuable%20insights%20into%20their%20complex%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11390v2&entry.124074799=Read"},
{"title": "Autonomous and Teleoperation Control of a Drawing Robot Avatar", "author": "Lingyun Chen and Abdeldjallil Naceri and Abdalla Swikir and Sandra Hirche and Sami Haddadin", "abstract": "  A drawing robot avatar is a robotic system that allows for telepresence-based\ndrawing, enabling users to remotely control a robotic arm and create drawings\nin real-time from a remote location. The proposed control framework aims to\nimprove bimanual robot telepresence quality by reducing the user workload and\nrequired prior knowledge through the automation of secondary or auxiliary\ntasks. The introduced novel method calculates the near-optimal Cartesian\nend-effector pose in terms of visual feedback quality for the attached\neye-to-hand camera with motion constraints in consideration. The effectiveness\nis demonstrated by conducting user studies of drawing reference shapes using\nthe implemented robot avatar compared to stationary and teleoperated camera\npose conditions. Our results demonstrate that the proposed control framework\noffers improved visual feedback quality and drawing performance.\n", "link": "http://arxiv.org/abs/2407.20156v1", "date": "2024-07-29", "relevancy": 2.2525, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6134}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5542}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20and%20Teleoperation%20Control%20of%20a%20Drawing%20Robot%20Avatar&body=Title%3A%20Autonomous%20and%20Teleoperation%20Control%20of%20a%20Drawing%20Robot%20Avatar%0AAuthor%3A%20Lingyun%20Chen%20and%20Abdeldjallil%20Naceri%20and%20Abdalla%20Swikir%20and%20Sandra%20Hirche%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20A%20drawing%20robot%20avatar%20is%20a%20robotic%20system%20that%20allows%20for%20telepresence-based%0Adrawing%2C%20enabling%20users%20to%20remotely%20control%20a%20robotic%20arm%20and%20create%20drawings%0Ain%20real-time%20from%20a%20remote%20location.%20The%20proposed%20control%20framework%20aims%20to%0Aimprove%20bimanual%20robot%20telepresence%20quality%20by%20reducing%20the%20user%20workload%20and%0Arequired%20prior%20knowledge%20through%20the%20automation%20of%20secondary%20or%20auxiliary%0Atasks.%20The%20introduced%20novel%20method%20calculates%20the%20near-optimal%20Cartesian%0Aend-effector%20pose%20in%20terms%20of%20visual%20feedback%20quality%20for%20the%20attached%0Aeye-to-hand%20camera%20with%20motion%20constraints%20in%20consideration.%20The%20effectiveness%0Ais%20demonstrated%20by%20conducting%20user%20studies%20of%20drawing%20reference%20shapes%20using%0Athe%20implemented%20robot%20avatar%20compared%20to%20stationary%20and%20teleoperated%20camera%0Apose%20conditions.%20Our%20results%20demonstrate%20that%20the%20proposed%20control%20framework%0Aoffers%20improved%20visual%20feedback%20quality%20and%20drawing%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520and%2520Teleoperation%2520Control%2520of%2520a%2520Drawing%2520Robot%2520Avatar%26entry.906535625%3DLingyun%2520Chen%2520and%2520Abdeldjallil%2520Naceri%2520and%2520Abdalla%2520Swikir%2520and%2520Sandra%2520Hirche%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520A%2520drawing%2520robot%2520avatar%2520is%2520a%2520robotic%2520system%2520that%2520allows%2520for%2520telepresence-based%250Adrawing%252C%2520enabling%2520users%2520to%2520remotely%2520control%2520a%2520robotic%2520arm%2520and%2520create%2520drawings%250Ain%2520real-time%2520from%2520a%2520remote%2520location.%2520The%2520proposed%2520control%2520framework%2520aims%2520to%250Aimprove%2520bimanual%2520robot%2520telepresence%2520quality%2520by%2520reducing%2520the%2520user%2520workload%2520and%250Arequired%2520prior%2520knowledge%2520through%2520the%2520automation%2520of%2520secondary%2520or%2520auxiliary%250Atasks.%2520The%2520introduced%2520novel%2520method%2520calculates%2520the%2520near-optimal%2520Cartesian%250Aend-effector%2520pose%2520in%2520terms%2520of%2520visual%2520feedback%2520quality%2520for%2520the%2520attached%250Aeye-to-hand%2520camera%2520with%2520motion%2520constraints%2520in%2520consideration.%2520The%2520effectiveness%250Ais%2520demonstrated%2520by%2520conducting%2520user%2520studies%2520of%2520drawing%2520reference%2520shapes%2520using%250Athe%2520implemented%2520robot%2520avatar%2520compared%2520to%2520stationary%2520and%2520teleoperated%2520camera%250Apose%2520conditions.%2520Our%2520results%2520demonstrate%2520that%2520the%2520proposed%2520control%2520framework%250Aoffers%2520improved%2520visual%2520feedback%2520quality%2520and%2520drawing%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20and%20Teleoperation%20Control%20of%20a%20Drawing%20Robot%20Avatar&entry.906535625=Lingyun%20Chen%20and%20Abdeldjallil%20Naceri%20and%20Abdalla%20Swikir%20and%20Sandra%20Hirche%20and%20Sami%20Haddadin&entry.1292438233=%20%20A%20drawing%20robot%20avatar%20is%20a%20robotic%20system%20that%20allows%20for%20telepresence-based%0Adrawing%2C%20enabling%20users%20to%20remotely%20control%20a%20robotic%20arm%20and%20create%20drawings%0Ain%20real-time%20from%20a%20remote%20location.%20The%20proposed%20control%20framework%20aims%20to%0Aimprove%20bimanual%20robot%20telepresence%20quality%20by%20reducing%20the%20user%20workload%20and%0Arequired%20prior%20knowledge%20through%20the%20automation%20of%20secondary%20or%20auxiliary%0Atasks.%20The%20introduced%20novel%20method%20calculates%20the%20near-optimal%20Cartesian%0Aend-effector%20pose%20in%20terms%20of%20visual%20feedback%20quality%20for%20the%20attached%0Aeye-to-hand%20camera%20with%20motion%20constraints%20in%20consideration.%20The%20effectiveness%0Ais%20demonstrated%20by%20conducting%20user%20studies%20of%20drawing%20reference%20shapes%20using%0Athe%20implemented%20robot%20avatar%20compared%20to%20stationary%20and%20teleoperated%20camera%0Apose%20conditions.%20Our%20results%20demonstrate%20that%20the%20proposed%20control%20framework%0Aoffers%20improved%20visual%20feedback%20quality%20and%20drawing%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20156v1&entry.124074799=Read"},
{"title": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive\n  Environmental Learning", "author": "Minghao Chen and Yihang Li and Yanting Yang and Shiyu Yu and Binbin Lin and Xiaofei He", "abstract": "  Large Language Models (LLM) based agents have shown promise in autonomously\ncompleting tasks across various domains, e.g., robotics, games, and web\nnavigation. However, these agents typically require elaborate design and expert\nprompts to solve tasks in specific domains, which limits their adaptability. We\nintroduce AutoManual, a framework enabling LLM agents to autonomously build\ntheir understanding through interaction and adapt to new environments.\nAutoManual categorizes environmental knowledge into diverse rules and optimizes\nthem in an online fashion by two agents: 1) The Planner codes actionable plans\nbased on current rules for interacting with the environment. 2) The Builder\nupdates the rules through a well-structured rule system that facilitates online\nrule management and essential detail retention. To mitigate hallucinations in\nmanaging rules, we introduce a case-conditioned prompting strategy for the\nBuilder. Finally, the Formulator agent compiles these rules into a\ncomprehensive manual. The self-generated manual can not only improve the\nadaptability but also guide the planning of smaller LLMs while being\nhuman-readable. Given only one simple demonstration, AutoManual significantly\nimproves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with\nGPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at\nhttps://github.com/minghchen/automanual.\n", "link": "http://arxiv.org/abs/2405.16247v2", "date": "2024-07-29", "relevancy": 2.2455, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5828}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.582}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoManual%3A%20Generating%20Instruction%20Manuals%20by%20LLM%20Agents%20via%20Interactive%0A%20%20Environmental%20Learning&body=Title%3A%20AutoManual%3A%20Generating%20Instruction%20Manuals%20by%20LLM%20Agents%20via%20Interactive%0A%20%20Environmental%20Learning%0AAuthor%3A%20Minghao%20Chen%20and%20Yihang%20Li%20and%20Yanting%20Yang%20and%20Shiyu%20Yu%20and%20Binbin%20Lin%20and%20Xiaofei%20He%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLM%29%20based%20agents%20have%20shown%20promise%20in%20autonomously%0Acompleting%20tasks%20across%20various%20domains%2C%20e.g.%2C%20robotics%2C%20games%2C%20and%20web%0Anavigation.%20However%2C%20these%20agents%20typically%20require%20elaborate%20design%20and%20expert%0Aprompts%20to%20solve%20tasks%20in%20specific%20domains%2C%20which%20limits%20their%20adaptability.%20We%0Aintroduce%20AutoManual%2C%20a%20framework%20enabling%20LLM%20agents%20to%20autonomously%20build%0Atheir%20understanding%20through%20interaction%20and%20adapt%20to%20new%20environments.%0AAutoManual%20categorizes%20environmental%20knowledge%20into%20diverse%20rules%20and%20optimizes%0Athem%20in%20an%20online%20fashion%20by%20two%20agents%3A%201%29%20The%20Planner%20codes%20actionable%20plans%0Abased%20on%20current%20rules%20for%20interacting%20with%20the%20environment.%202%29%20The%20Builder%0Aupdates%20the%20rules%20through%20a%20well-structured%20rule%20system%20that%20facilitates%20online%0Arule%20management%20and%20essential%20detail%20retention.%20To%20mitigate%20hallucinations%20in%0Amanaging%20rules%2C%20we%20introduce%20a%20case-conditioned%20prompting%20strategy%20for%20the%0ABuilder.%20Finally%2C%20the%20Formulator%20agent%20compiles%20these%20rules%20into%20a%0Acomprehensive%20manual.%20The%20self-generated%20manual%20can%20not%20only%20improve%20the%0Aadaptability%20but%20also%20guide%20the%20planning%20of%20smaller%20LLMs%20while%20being%0Ahuman-readable.%20Given%20only%20one%20simple%20demonstration%2C%20AutoManual%20significantly%0Aimproves%20task%20success%20rates%2C%20achieving%2097.4%5C%25%20with%20GPT-4-turbo%20and%2086.2%5C%25%20with%0AGPT-3.5-turbo%20on%20ALFWorld%20benchmark%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/minghchen/automanual.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoManual%253A%2520Generating%2520Instruction%2520Manuals%2520by%2520LLM%2520Agents%2520via%2520Interactive%250A%2520%2520Environmental%2520Learning%26entry.906535625%3DMinghao%2520Chen%2520and%2520Yihang%2520Li%2520and%2520Yanting%2520Yang%2520and%2520Shiyu%2520Yu%2520and%2520Binbin%2520Lin%2520and%2520Xiaofei%2520He%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520based%2520agents%2520have%2520shown%2520promise%2520in%2520autonomously%250Acompleting%2520tasks%2520across%2520various%2520domains%252C%2520e.g.%252C%2520robotics%252C%2520games%252C%2520and%2520web%250Anavigation.%2520However%252C%2520these%2520agents%2520typically%2520require%2520elaborate%2520design%2520and%2520expert%250Aprompts%2520to%2520solve%2520tasks%2520in%2520specific%2520domains%252C%2520which%2520limits%2520their%2520adaptability.%2520We%250Aintroduce%2520AutoManual%252C%2520a%2520framework%2520enabling%2520LLM%2520agents%2520to%2520autonomously%2520build%250Atheir%2520understanding%2520through%2520interaction%2520and%2520adapt%2520to%2520new%2520environments.%250AAutoManual%2520categorizes%2520environmental%2520knowledge%2520into%2520diverse%2520rules%2520and%2520optimizes%250Athem%2520in%2520an%2520online%2520fashion%2520by%2520two%2520agents%253A%25201%2529%2520The%2520Planner%2520codes%2520actionable%2520plans%250Abased%2520on%2520current%2520rules%2520for%2520interacting%2520with%2520the%2520environment.%25202%2529%2520The%2520Builder%250Aupdates%2520the%2520rules%2520through%2520a%2520well-structured%2520rule%2520system%2520that%2520facilitates%2520online%250Arule%2520management%2520and%2520essential%2520detail%2520retention.%2520To%2520mitigate%2520hallucinations%2520in%250Amanaging%2520rules%252C%2520we%2520introduce%2520a%2520case-conditioned%2520prompting%2520strategy%2520for%2520the%250ABuilder.%2520Finally%252C%2520the%2520Formulator%2520agent%2520compiles%2520these%2520rules%2520into%2520a%250Acomprehensive%2520manual.%2520The%2520self-generated%2520manual%2520can%2520not%2520only%2520improve%2520the%250Aadaptability%2520but%2520also%2520guide%2520the%2520planning%2520of%2520smaller%2520LLMs%2520while%2520being%250Ahuman-readable.%2520Given%2520only%2520one%2520simple%2520demonstration%252C%2520AutoManual%2520significantly%250Aimproves%2520task%2520success%2520rates%252C%2520achieving%252097.4%255C%2525%2520with%2520GPT-4-turbo%2520and%252086.2%255C%2525%2520with%250AGPT-3.5-turbo%2520on%2520ALFWorld%2520benchmark%2520tasks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/minghchen/automanual.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoManual%3A%20Generating%20Instruction%20Manuals%20by%20LLM%20Agents%20via%20Interactive%0A%20%20Environmental%20Learning&entry.906535625=Minghao%20Chen%20and%20Yihang%20Li%20and%20Yanting%20Yang%20and%20Shiyu%20Yu%20and%20Binbin%20Lin%20and%20Xiaofei%20He&entry.1292438233=%20%20Large%20Language%20Models%20%28LLM%29%20based%20agents%20have%20shown%20promise%20in%20autonomously%0Acompleting%20tasks%20across%20various%20domains%2C%20e.g.%2C%20robotics%2C%20games%2C%20and%20web%0Anavigation.%20However%2C%20these%20agents%20typically%20require%20elaborate%20design%20and%20expert%0Aprompts%20to%20solve%20tasks%20in%20specific%20domains%2C%20which%20limits%20their%20adaptability.%20We%0Aintroduce%20AutoManual%2C%20a%20framework%20enabling%20LLM%20agents%20to%20autonomously%20build%0Atheir%20understanding%20through%20interaction%20and%20adapt%20to%20new%20environments.%0AAutoManual%20categorizes%20environmental%20knowledge%20into%20diverse%20rules%20and%20optimizes%0Athem%20in%20an%20online%20fashion%20by%20two%20agents%3A%201%29%20The%20Planner%20codes%20actionable%20plans%0Abased%20on%20current%20rules%20for%20interacting%20with%20the%20environment.%202%29%20The%20Builder%0Aupdates%20the%20rules%20through%20a%20well-structured%20rule%20system%20that%20facilitates%20online%0Arule%20management%20and%20essential%20detail%20retention.%20To%20mitigate%20hallucinations%20in%0Amanaging%20rules%2C%20we%20introduce%20a%20case-conditioned%20prompting%20strategy%20for%20the%0ABuilder.%20Finally%2C%20the%20Formulator%20agent%20compiles%20these%20rules%20into%20a%0Acomprehensive%20manual.%20The%20self-generated%20manual%20can%20not%20only%20improve%20the%0Aadaptability%20but%20also%20guide%20the%20planning%20of%20smaller%20LLMs%20while%20being%0Ahuman-readable.%20Given%20only%20one%20simple%20demonstration%2C%20AutoManual%20significantly%0Aimproves%20task%20success%20rates%2C%20achieving%2097.4%5C%25%20with%20GPT-4-turbo%20and%2086.2%5C%25%20with%0AGPT-3.5-turbo%20on%20ALFWorld%20benchmark%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/minghchen/automanual.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16247v2&entry.124074799=Read"},
{"title": "Neural Control Barrier Functions for Safe Navigation", "author": "Marvin Harms and Mihir Kulkarni and Nikhil Khedekar and Martin Jacquet and Kostas Alexis", "abstract": "  Autonomous robot navigation can be particularly demanding, especially when\nthe surrounding environment is not known and safety of the robot is crucial.\nThis work relates to the synthesis of Control Barrier Functions (CBFs) through\ndata for safe navigation in unknown environments. A novel methodology to\njointly learn CBFs and corresponding safe controllers, in simulation, inspired\nby the State Dependent Riccati Equation (SDRE) is proposed. The CBF is used to\nobtain admissible commands from any nominal, possibly unsafe controller. An\napproach to apply the CBF inside a safety filter without the need for a\nconsistent map or position estimate is developed. Subsequently, the resulting\nreactive safety filter is deployed on a multirotor platform integrating a LiDAR\nsensor both in simulation and real-world experiments.\n", "link": "http://arxiv.org/abs/2407.19907v1", "date": "2024-07-29", "relevancy": 2.2439, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5828}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.57}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Control%20Barrier%20Functions%20for%20Safe%20Navigation&body=Title%3A%20Neural%20Control%20Barrier%20Functions%20for%20Safe%20Navigation%0AAuthor%3A%20Marvin%20Harms%20and%20Mihir%20Kulkarni%20and%20Nikhil%20Khedekar%20and%20Martin%20Jacquet%20and%20Kostas%20Alexis%0AAbstract%3A%20%20%20Autonomous%20robot%20navigation%20can%20be%20particularly%20demanding%2C%20especially%20when%0Athe%20surrounding%20environment%20is%20not%20known%20and%20safety%20of%20the%20robot%20is%20crucial.%0AThis%20work%20relates%20to%20the%20synthesis%20of%20Control%20Barrier%20Functions%20%28CBFs%29%20through%0Adata%20for%20safe%20navigation%20in%20unknown%20environments.%20A%20novel%20methodology%20to%0Ajointly%20learn%20CBFs%20and%20corresponding%20safe%20controllers%2C%20in%20simulation%2C%20inspired%0Aby%20the%20State%20Dependent%20Riccati%20Equation%20%28SDRE%29%20is%20proposed.%20The%20CBF%20is%20used%20to%0Aobtain%20admissible%20commands%20from%20any%20nominal%2C%20possibly%20unsafe%20controller.%20An%0Aapproach%20to%20apply%20the%20CBF%20inside%20a%20safety%20filter%20without%20the%20need%20for%20a%0Aconsistent%20map%20or%20position%20estimate%20is%20developed.%20Subsequently%2C%20the%20resulting%0Areactive%20safety%20filter%20is%20deployed%20on%20a%20multirotor%20platform%20integrating%20a%20LiDAR%0Asensor%20both%20in%20simulation%20and%20real-world%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Control%2520Barrier%2520Functions%2520for%2520Safe%2520Navigation%26entry.906535625%3DMarvin%2520Harms%2520and%2520Mihir%2520Kulkarni%2520and%2520Nikhil%2520Khedekar%2520and%2520Martin%2520Jacquet%2520and%2520Kostas%2520Alexis%26entry.1292438233%3D%2520%2520Autonomous%2520robot%2520navigation%2520can%2520be%2520particularly%2520demanding%252C%2520especially%2520when%250Athe%2520surrounding%2520environment%2520is%2520not%2520known%2520and%2520safety%2520of%2520the%2520robot%2520is%2520crucial.%250AThis%2520work%2520relates%2520to%2520the%2520synthesis%2520of%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529%2520through%250Adata%2520for%2520safe%2520navigation%2520in%2520unknown%2520environments.%2520A%2520novel%2520methodology%2520to%250Ajointly%2520learn%2520CBFs%2520and%2520corresponding%2520safe%2520controllers%252C%2520in%2520simulation%252C%2520inspired%250Aby%2520the%2520State%2520Dependent%2520Riccati%2520Equation%2520%2528SDRE%2529%2520is%2520proposed.%2520The%2520CBF%2520is%2520used%2520to%250Aobtain%2520admissible%2520commands%2520from%2520any%2520nominal%252C%2520possibly%2520unsafe%2520controller.%2520An%250Aapproach%2520to%2520apply%2520the%2520CBF%2520inside%2520a%2520safety%2520filter%2520without%2520the%2520need%2520for%2520a%250Aconsistent%2520map%2520or%2520position%2520estimate%2520is%2520developed.%2520Subsequently%252C%2520the%2520resulting%250Areactive%2520safety%2520filter%2520is%2520deployed%2520on%2520a%2520multirotor%2520platform%2520integrating%2520a%2520LiDAR%250Asensor%2520both%2520in%2520simulation%2520and%2520real-world%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Control%20Barrier%20Functions%20for%20Safe%20Navigation&entry.906535625=Marvin%20Harms%20and%20Mihir%20Kulkarni%20and%20Nikhil%20Khedekar%20and%20Martin%20Jacquet%20and%20Kostas%20Alexis&entry.1292438233=%20%20Autonomous%20robot%20navigation%20can%20be%20particularly%20demanding%2C%20especially%20when%0Athe%20surrounding%20environment%20is%20not%20known%20and%20safety%20of%20the%20robot%20is%20crucial.%0AThis%20work%20relates%20to%20the%20synthesis%20of%20Control%20Barrier%20Functions%20%28CBFs%29%20through%0Adata%20for%20safe%20navigation%20in%20unknown%20environments.%20A%20novel%20methodology%20to%0Ajointly%20learn%20CBFs%20and%20corresponding%20safe%20controllers%2C%20in%20simulation%2C%20inspired%0Aby%20the%20State%20Dependent%20Riccati%20Equation%20%28SDRE%29%20is%20proposed.%20The%20CBF%20is%20used%20to%0Aobtain%20admissible%20commands%20from%20any%20nominal%2C%20possibly%20unsafe%20controller.%20An%0Aapproach%20to%20apply%20the%20CBF%20inside%20a%20safety%20filter%20without%20the%20need%20for%20a%0Aconsistent%20map%20or%20position%20estimate%20is%20developed.%20Subsequently%2C%20the%20resulting%0Areactive%20safety%20filter%20is%20deployed%20on%20a%20multirotor%20platform%20integrating%20a%20LiDAR%0Asensor%20both%20in%20simulation%20and%20real-world%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19907v1&entry.124074799=Read"},
{"title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens", "author": "Gagan Jain and Nidhi Hegde and Aditya Kusupati and Arsha Nagrani and Shyamal Buch and Prateek Jain and Anurag Arnab and Sujoy Paul", "abstract": "  The visual medium (images and videos) naturally contains a large amount of\ninformation redundancy, thereby providing a great opportunity for leveraging\nefficiency in processing. While Vision Transformer (ViT) based models scale\neffectively to large data regimes, they fail to capitalize on this inherent\nredundancy, leading to higher computational costs. Mixture of Experts (MoE)\nnetworks demonstrate scalability while maintaining same inference-time costs,\nbut they come with a larger parameter footprint. We present Mixture of Nested\nExperts (MoNE), which utilizes a nested structure for experts, wherein\nindividual experts fall on an increasing compute-accuracy curve. Given a\ncompute budget, MoNE learns to dynamically choose tokens in a priority order,\nand thus redundant tokens are processed through cheaper nested experts. Using\nthis framework, we achieve equivalent performance as the baseline models, while\nreducing inference time compute by over two-fold. We validate our approach on\nstandard image and video datasets - ImageNet-21K, Kinetics400, and\nSomething-Something-v2. We further highlight MoNE$'$s adaptability by\nshowcasing its ability to maintain strong performance across different\ninference-time compute budgets on videos, using only a single trained model.\n", "link": "http://arxiv.org/abs/2407.19985v1", "date": "2024-07-29", "relevancy": 2.2343, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5701}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5525}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Nested%20Experts%3A%20Adaptive%20Processing%20of%20Visual%20Tokens&body=Title%3A%20Mixture%20of%20Nested%20Experts%3A%20Adaptive%20Processing%20of%20Visual%20Tokens%0AAuthor%3A%20Gagan%20Jain%20and%20Nidhi%20Hegde%20and%20Aditya%20Kusupati%20and%20Arsha%20Nagrani%20and%20Shyamal%20Buch%20and%20Prateek%20Jain%20and%20Anurag%20Arnab%20and%20Sujoy%20Paul%0AAbstract%3A%20%20%20The%20visual%20medium%20%28images%20and%20videos%29%20naturally%20contains%20a%20large%20amount%20of%0Ainformation%20redundancy%2C%20thereby%20providing%20a%20great%20opportunity%20for%20leveraging%0Aefficiency%20in%20processing.%20While%20Vision%20Transformer%20%28ViT%29%20based%20models%20scale%0Aeffectively%20to%20large%20data%20regimes%2C%20they%20fail%20to%20capitalize%20on%20this%20inherent%0Aredundancy%2C%20leading%20to%20higher%20computational%20costs.%20Mixture%20of%20Experts%20%28MoE%29%0Anetworks%20demonstrate%20scalability%20while%20maintaining%20same%20inference-time%20costs%2C%0Abut%20they%20come%20with%20a%20larger%20parameter%20footprint.%20We%20present%20Mixture%20of%20Nested%0AExperts%20%28MoNE%29%2C%20which%20utilizes%20a%20nested%20structure%20for%20experts%2C%20wherein%0Aindividual%20experts%20fall%20on%20an%20increasing%20compute-accuracy%20curve.%20Given%20a%0Acompute%20budget%2C%20MoNE%20learns%20to%20dynamically%20choose%20tokens%20in%20a%20priority%20order%2C%0Aand%20thus%20redundant%20tokens%20are%20processed%20through%20cheaper%20nested%20experts.%20Using%0Athis%20framework%2C%20we%20achieve%20equivalent%20performance%20as%20the%20baseline%20models%2C%20while%0Areducing%20inference%20time%20compute%20by%20over%20two-fold.%20We%20validate%20our%20approach%20on%0Astandard%20image%20and%20video%20datasets%20-%20ImageNet-21K%2C%20Kinetics400%2C%20and%0ASomething-Something-v2.%20We%20further%20highlight%20MoNE%24%27%24s%20adaptability%20by%0Ashowcasing%20its%20ability%20to%20maintain%20strong%20performance%20across%20different%0Ainference-time%20compute%20budgets%20on%20videos%2C%20using%20only%20a%20single%20trained%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Nested%2520Experts%253A%2520Adaptive%2520Processing%2520of%2520Visual%2520Tokens%26entry.906535625%3DGagan%2520Jain%2520and%2520Nidhi%2520Hegde%2520and%2520Aditya%2520Kusupati%2520and%2520Arsha%2520Nagrani%2520and%2520Shyamal%2520Buch%2520and%2520Prateek%2520Jain%2520and%2520Anurag%2520Arnab%2520and%2520Sujoy%2520Paul%26entry.1292438233%3D%2520%2520The%2520visual%2520medium%2520%2528images%2520and%2520videos%2529%2520naturally%2520contains%2520a%2520large%2520amount%2520of%250Ainformation%2520redundancy%252C%2520thereby%2520providing%2520a%2520great%2520opportunity%2520for%2520leveraging%250Aefficiency%2520in%2520processing.%2520While%2520Vision%2520Transformer%2520%2528ViT%2529%2520based%2520models%2520scale%250Aeffectively%2520to%2520large%2520data%2520regimes%252C%2520they%2520fail%2520to%2520capitalize%2520on%2520this%2520inherent%250Aredundancy%252C%2520leading%2520to%2520higher%2520computational%2520costs.%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%250Anetworks%2520demonstrate%2520scalability%2520while%2520maintaining%2520same%2520inference-time%2520costs%252C%250Abut%2520they%2520come%2520with%2520a%2520larger%2520parameter%2520footprint.%2520We%2520present%2520Mixture%2520of%2520Nested%250AExperts%2520%2528MoNE%2529%252C%2520which%2520utilizes%2520a%2520nested%2520structure%2520for%2520experts%252C%2520wherein%250Aindividual%2520experts%2520fall%2520on%2520an%2520increasing%2520compute-accuracy%2520curve.%2520Given%2520a%250Acompute%2520budget%252C%2520MoNE%2520learns%2520to%2520dynamically%2520choose%2520tokens%2520in%2520a%2520priority%2520order%252C%250Aand%2520thus%2520redundant%2520tokens%2520are%2520processed%2520through%2520cheaper%2520nested%2520experts.%2520Using%250Athis%2520framework%252C%2520we%2520achieve%2520equivalent%2520performance%2520as%2520the%2520baseline%2520models%252C%2520while%250Areducing%2520inference%2520time%2520compute%2520by%2520over%2520two-fold.%2520We%2520validate%2520our%2520approach%2520on%250Astandard%2520image%2520and%2520video%2520datasets%2520-%2520ImageNet-21K%252C%2520Kinetics400%252C%2520and%250ASomething-Something-v2.%2520We%2520further%2520highlight%2520MoNE%2524%2527%2524s%2520adaptability%2520by%250Ashowcasing%2520its%2520ability%2520to%2520maintain%2520strong%2520performance%2520across%2520different%250Ainference-time%2520compute%2520budgets%2520on%2520videos%252C%2520using%2520only%2520a%2520single%2520trained%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Nested%20Experts%3A%20Adaptive%20Processing%20of%20Visual%20Tokens&entry.906535625=Gagan%20Jain%20and%20Nidhi%20Hegde%20and%20Aditya%20Kusupati%20and%20Arsha%20Nagrani%20and%20Shyamal%20Buch%20and%20Prateek%20Jain%20and%20Anurag%20Arnab%20and%20Sujoy%20Paul&entry.1292438233=%20%20The%20visual%20medium%20%28images%20and%20videos%29%20naturally%20contains%20a%20large%20amount%20of%0Ainformation%20redundancy%2C%20thereby%20providing%20a%20great%20opportunity%20for%20leveraging%0Aefficiency%20in%20processing.%20While%20Vision%20Transformer%20%28ViT%29%20based%20models%20scale%0Aeffectively%20to%20large%20data%20regimes%2C%20they%20fail%20to%20capitalize%20on%20this%20inherent%0Aredundancy%2C%20leading%20to%20higher%20computational%20costs.%20Mixture%20of%20Experts%20%28MoE%29%0Anetworks%20demonstrate%20scalability%20while%20maintaining%20same%20inference-time%20costs%2C%0Abut%20they%20come%20with%20a%20larger%20parameter%20footprint.%20We%20present%20Mixture%20of%20Nested%0AExperts%20%28MoNE%29%2C%20which%20utilizes%20a%20nested%20structure%20for%20experts%2C%20wherein%0Aindividual%20experts%20fall%20on%20an%20increasing%20compute-accuracy%20curve.%20Given%20a%0Acompute%20budget%2C%20MoNE%20learns%20to%20dynamically%20choose%20tokens%20in%20a%20priority%20order%2C%0Aand%20thus%20redundant%20tokens%20are%20processed%20through%20cheaper%20nested%20experts.%20Using%0Athis%20framework%2C%20we%20achieve%20equivalent%20performance%20as%20the%20baseline%20models%2C%20while%0Areducing%20inference%20time%20compute%20by%20over%20two-fold.%20We%20validate%20our%20approach%20on%0Astandard%20image%20and%20video%20datasets%20-%20ImageNet-21K%2C%20Kinetics400%2C%20and%0ASomething-Something-v2.%20We%20further%20highlight%20MoNE%24%27%24s%20adaptability%20by%0Ashowcasing%20its%20ability%20to%20maintain%20strong%20performance%20across%20different%0Ainference-time%20compute%20budgets%20on%20videos%2C%20using%20only%20a%20single%20trained%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19985v1&entry.124074799=Read"},
{"title": "Making Multi-Axis Gaussian Graphical Models Scalable to Millions of\n  Samples and Features", "author": "Bailey Andrew and David R. Westhead and Luisa Cutillo", "abstract": "  Gaussian graphical models can be used to extract conditional dependencies\nbetween the features of the dataset. This is often done by making an\nindependence assumption about the samples, but this assumption is rarely\nsatisfied in reality. However, state-of-the-art approaches that avoid this\nassumption are not scalable, with $O(n^3)$ runtime and $O(n^2)$ space\ncomplexity. In this paper, we introduce a method that has $O(n^2)$ runtime and\n$O(n)$ space complexity, without assuming independence.\n  We validate our model on both synthetic and real-world datasets, showing that\nour method's accuracy is comparable to that of prior work We demonstrate that\nour approach can be used on unprecedentedly large datasets, such as a\nreal-world 1,000,000-cell scRNA-seq dataset; this was impossible with previous\napproaches. Our method maintains the flexibility of prior work, such as the\nability to handle multi-modal tensor-variate datasets and the ability to work\nwith data of arbitrary marginal distributions. An additional advantage of our\nmethod is that, unlike prior work, our hyperparameters are easily\ninterpretable.\n", "link": "http://arxiv.org/abs/2407.19892v1", "date": "2024-07-29", "relevancy": 2.2151, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5657}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Making%20Multi-Axis%20Gaussian%20Graphical%20Models%20Scalable%20to%20Millions%20of%0A%20%20Samples%20and%20Features&body=Title%3A%20Making%20Multi-Axis%20Gaussian%20Graphical%20Models%20Scalable%20to%20Millions%20of%0A%20%20Samples%20and%20Features%0AAuthor%3A%20Bailey%20Andrew%20and%20David%20R.%20Westhead%20and%20Luisa%20Cutillo%0AAbstract%3A%20%20%20Gaussian%20graphical%20models%20can%20be%20used%20to%20extract%20conditional%20dependencies%0Abetween%20the%20features%20of%20the%20dataset.%20This%20is%20often%20done%20by%20making%20an%0Aindependence%20assumption%20about%20the%20samples%2C%20but%20this%20assumption%20is%20rarely%0Asatisfied%20in%20reality.%20However%2C%20state-of-the-art%20approaches%20that%20avoid%20this%0Aassumption%20are%20not%20scalable%2C%20with%20%24O%28n%5E3%29%24%20runtime%20and%20%24O%28n%5E2%29%24%20space%0Acomplexity.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20that%20has%20%24O%28n%5E2%29%24%20runtime%20and%0A%24O%28n%29%24%20space%20complexity%2C%20without%20assuming%20independence.%0A%20%20We%20validate%20our%20model%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20showing%20that%0Aour%20method%27s%20accuracy%20is%20comparable%20to%20that%20of%20prior%20work%20We%20demonstrate%20that%0Aour%20approach%20can%20be%20used%20on%20unprecedentedly%20large%20datasets%2C%20such%20as%20a%0Areal-world%201%2C000%2C000-cell%20scRNA-seq%20dataset%3B%20this%20was%20impossible%20with%20previous%0Aapproaches.%20Our%20method%20maintains%20the%20flexibility%20of%20prior%20work%2C%20such%20as%20the%0Aability%20to%20handle%20multi-modal%20tensor-variate%20datasets%20and%20the%20ability%20to%20work%0Awith%20data%20of%20arbitrary%20marginal%20distributions.%20An%20additional%20advantage%20of%20our%0Amethod%20is%20that%2C%20unlike%20prior%20work%2C%20our%20hyperparameters%20are%20easily%0Ainterpretable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaking%2520Multi-Axis%2520Gaussian%2520Graphical%2520Models%2520Scalable%2520to%2520Millions%2520of%250A%2520%2520Samples%2520and%2520Features%26entry.906535625%3DBailey%2520Andrew%2520and%2520David%2520R.%2520Westhead%2520and%2520Luisa%2520Cutillo%26entry.1292438233%3D%2520%2520Gaussian%2520graphical%2520models%2520can%2520be%2520used%2520to%2520extract%2520conditional%2520dependencies%250Abetween%2520the%2520features%2520of%2520the%2520dataset.%2520This%2520is%2520often%2520done%2520by%2520making%2520an%250Aindependence%2520assumption%2520about%2520the%2520samples%252C%2520but%2520this%2520assumption%2520is%2520rarely%250Asatisfied%2520in%2520reality.%2520However%252C%2520state-of-the-art%2520approaches%2520that%2520avoid%2520this%250Aassumption%2520are%2520not%2520scalable%252C%2520with%2520%2524O%2528n%255E3%2529%2524%2520runtime%2520and%2520%2524O%2528n%255E2%2529%2524%2520space%250Acomplexity.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%2520that%2520has%2520%2524O%2528n%255E2%2529%2524%2520runtime%2520and%250A%2524O%2528n%2529%2524%2520space%2520complexity%252C%2520without%2520assuming%2520independence.%250A%2520%2520We%2520validate%2520our%2520model%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%2520showing%2520that%250Aour%2520method%2527s%2520accuracy%2520is%2520comparable%2520to%2520that%2520of%2520prior%2520work%2520We%2520demonstrate%2520that%250Aour%2520approach%2520can%2520be%2520used%2520on%2520unprecedentedly%2520large%2520datasets%252C%2520such%2520as%2520a%250Areal-world%25201%252C000%252C000-cell%2520scRNA-seq%2520dataset%253B%2520this%2520was%2520impossible%2520with%2520previous%250Aapproaches.%2520Our%2520method%2520maintains%2520the%2520flexibility%2520of%2520prior%2520work%252C%2520such%2520as%2520the%250Aability%2520to%2520handle%2520multi-modal%2520tensor-variate%2520datasets%2520and%2520the%2520ability%2520to%2520work%250Awith%2520data%2520of%2520arbitrary%2520marginal%2520distributions.%2520An%2520additional%2520advantage%2520of%2520our%250Amethod%2520is%2520that%252C%2520unlike%2520prior%2520work%252C%2520our%2520hyperparameters%2520are%2520easily%250Ainterpretable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Making%20Multi-Axis%20Gaussian%20Graphical%20Models%20Scalable%20to%20Millions%20of%0A%20%20Samples%20and%20Features&entry.906535625=Bailey%20Andrew%20and%20David%20R.%20Westhead%20and%20Luisa%20Cutillo&entry.1292438233=%20%20Gaussian%20graphical%20models%20can%20be%20used%20to%20extract%20conditional%20dependencies%0Abetween%20the%20features%20of%20the%20dataset.%20This%20is%20often%20done%20by%20making%20an%0Aindependence%20assumption%20about%20the%20samples%2C%20but%20this%20assumption%20is%20rarely%0Asatisfied%20in%20reality.%20However%2C%20state-of-the-art%20approaches%20that%20avoid%20this%0Aassumption%20are%20not%20scalable%2C%20with%20%24O%28n%5E3%29%24%20runtime%20and%20%24O%28n%5E2%29%24%20space%0Acomplexity.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20that%20has%20%24O%28n%5E2%29%24%20runtime%20and%0A%24O%28n%29%24%20space%20complexity%2C%20without%20assuming%20independence.%0A%20%20We%20validate%20our%20model%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20showing%20that%0Aour%20method%27s%20accuracy%20is%20comparable%20to%20that%20of%20prior%20work%20We%20demonstrate%20that%0Aour%20approach%20can%20be%20used%20on%20unprecedentedly%20large%20datasets%2C%20such%20as%20a%0Areal-world%201%2C000%2C000-cell%20scRNA-seq%20dataset%3B%20this%20was%20impossible%20with%20previous%0Aapproaches.%20Our%20method%20maintains%20the%20flexibility%20of%20prior%20work%2C%20such%20as%20the%0Aability%20to%20handle%20multi-modal%20tensor-variate%20datasets%20and%20the%20ability%20to%20work%0Awith%20data%20of%20arbitrary%20marginal%20distributions.%20An%20additional%20advantage%20of%20our%0Amethod%20is%20that%2C%20unlike%20prior%20work%2C%20our%20hyperparameters%20are%20easily%0Ainterpretable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19892v1&entry.124074799=Read"},
{"title": "Characterizing Dynamical Stability of Stochastic Gradient Descent in\n  Overparameterized Learning", "author": "Dennis Chemnitz and Maximilian Engel", "abstract": "  For overparameterized optimization tasks, such as the ones found in modern\nmachine learning, global minima are generally not unique. In order to\nunderstand generalization in these settings, it is vital to study to which\nminimum an optimization algorithm converges. The possibility of having minima\nthat are unstable under the dynamics imposed by the optimization algorithm\nlimits the potential minima that the algorithm can find. In this paper, we\ncharacterize the global minima that are dynamically stable/unstable for both\ndeterministic and stochastic gradient descent (SGD). In particular, we\nintroduce a characteristic Lyapunov exponent which depends on the local\ndynamics around a global minimum and rigorously prove that the sign of this\nLyapunov exponent determines whether SGD can accumulate at the respective\nglobal minimum.\n", "link": "http://arxiv.org/abs/2407.20209v1", "date": "2024-07-29", "relevancy": 2.2138, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.451}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning&body=Title%3A%20Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning%0AAuthor%3A%20Dennis%20Chemnitz%20and%20Maximilian%20Engel%0AAbstract%3A%20%20%20For%20overparameterized%20optimization%20tasks%2C%20such%20as%20the%20ones%20found%20in%20modern%0Amachine%20learning%2C%20global%20minima%20are%20generally%20not%20unique.%20In%20order%20to%0Aunderstand%20generalization%20in%20these%20settings%2C%20it%20is%20vital%20to%20study%20to%20which%0Aminimum%20an%20optimization%20algorithm%20converges.%20The%20possibility%20of%20having%20minima%0Athat%20are%20unstable%20under%20the%20dynamics%20imposed%20by%20the%20optimization%20algorithm%0Alimits%20the%20potential%20minima%20that%20the%20algorithm%20can%20find.%20In%20this%20paper%2C%20we%0Acharacterize%20the%20global%20minima%20that%20are%20dynamically%20stable/unstable%20for%20both%0Adeterministic%20and%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20particular%2C%20we%0Aintroduce%20a%20characteristic%20Lyapunov%20exponent%20which%20depends%20on%20the%20local%0Adynamics%20around%20a%20global%20minimum%20and%20rigorously%20prove%20that%20the%20sign%20of%20this%0ALyapunov%20exponent%20determines%20whether%20SGD%20can%20accumulate%20at%20the%20respective%0Aglobal%20minimum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Dynamical%2520Stability%2520of%2520Stochastic%2520Gradient%2520Descent%2520in%250A%2520%2520Overparameterized%2520Learning%26entry.906535625%3DDennis%2520Chemnitz%2520and%2520Maximilian%2520Engel%26entry.1292438233%3D%2520%2520For%2520overparameterized%2520optimization%2520tasks%252C%2520such%2520as%2520the%2520ones%2520found%2520in%2520modern%250Amachine%2520learning%252C%2520global%2520minima%2520are%2520generally%2520not%2520unique.%2520In%2520order%2520to%250Aunderstand%2520generalization%2520in%2520these%2520settings%252C%2520it%2520is%2520vital%2520to%2520study%2520to%2520which%250Aminimum%2520an%2520optimization%2520algorithm%2520converges.%2520The%2520possibility%2520of%2520having%2520minima%250Athat%2520are%2520unstable%2520under%2520the%2520dynamics%2520imposed%2520by%2520the%2520optimization%2520algorithm%250Alimits%2520the%2520potential%2520minima%2520that%2520the%2520algorithm%2520can%2520find.%2520In%2520this%2520paper%252C%2520we%250Acharacterize%2520the%2520global%2520minima%2520that%2520are%2520dynamically%2520stable/unstable%2520for%2520both%250Adeterministic%2520and%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529.%2520In%2520particular%252C%2520we%250Aintroduce%2520a%2520characteristic%2520Lyapunov%2520exponent%2520which%2520depends%2520on%2520the%2520local%250Adynamics%2520around%2520a%2520global%2520minimum%2520and%2520rigorously%2520prove%2520that%2520the%2520sign%2520of%2520this%250ALyapunov%2520exponent%2520determines%2520whether%2520SGD%2520can%2520accumulate%2520at%2520the%2520respective%250Aglobal%2520minimum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning&entry.906535625=Dennis%20Chemnitz%20and%20Maximilian%20Engel&entry.1292438233=%20%20For%20overparameterized%20optimization%20tasks%2C%20such%20as%20the%20ones%20found%20in%20modern%0Amachine%20learning%2C%20global%20minima%20are%20generally%20not%20unique.%20In%20order%20to%0Aunderstand%20generalization%20in%20these%20settings%2C%20it%20is%20vital%20to%20study%20to%20which%0Aminimum%20an%20optimization%20algorithm%20converges.%20The%20possibility%20of%20having%20minima%0Athat%20are%20unstable%20under%20the%20dynamics%20imposed%20by%20the%20optimization%20algorithm%0Alimits%20the%20potential%20minima%20that%20the%20algorithm%20can%20find.%20In%20this%20paper%2C%20we%0Acharacterize%20the%20global%20minima%20that%20are%20dynamically%20stable/unstable%20for%20both%0Adeterministic%20and%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20particular%2C%20we%0Aintroduce%20a%20characteristic%20Lyapunov%20exponent%20which%20depends%20on%20the%20local%0Adynamics%20around%20a%20global%20minimum%20and%20rigorously%20prove%20that%20the%20sign%20of%20this%0ALyapunov%20exponent%20determines%20whether%20SGD%20can%20accumulate%20at%20the%20respective%0Aglobal%20minimum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20209v1&entry.124074799=Read"},
{"title": "Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving\n  Imitation Learning with LLMs", "author": "Yiqun Duan and Qiang Zhang and Renjing Xu", "abstract": "  The utilization of Large Language Models (LLMs) within the realm of\nreinforcement learning, particularly as planners, has garnered a significant\ndegree of attention in recent scholarly literature. However, a substantial\nproportion of existing research predominantly focuses on planning models for\nrobotics that transmute the outputs derived from perception models into\nlinguistic forms, thus adopting a `pure-language' strategy. In this research,\nwe propose a hybrid End-to-End learning framework for autonomous driving by\ncombining basic driving imitation learning with LLMs based on multi-modality\nprompt tokens. Instead of simply converting perception results from the\nseparated train model into pure language input, our novelty lies in two\naspects. 1) The end-to-end integration of visual and LiDAR sensory input into\nlearnable multi-modality tokens, thereby intrinsically alleviating description\nbias by separated pre-trained perception models. 2) Instead of directly letting\nLLMs drive, this paper explores a hybrid setting of letting LLMs help the\ndriving model correct mistakes and complicated scenarios. The results of our\nexperiments suggest that the proposed methodology can attain driving scores of\n49.21%, coupled with an impressive route completion rate of 91.34% in the\noffline evaluation conducted via CARLA. These performance metrics are\ncomparable to the most advanced driving models.\n", "link": "http://arxiv.org/abs/2404.04869v2", "date": "2024-07-29", "relevancy": 2.1971, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5764}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20Multi-Modal%20Tokens%20to%20Enhance%20End-to-End%20Autonomous%20Driving%0A%20%20Imitation%20Learning%20with%20LLMs&body=Title%3A%20Prompting%20Multi-Modal%20Tokens%20to%20Enhance%20End-to-End%20Autonomous%20Driving%0A%20%20Imitation%20Learning%20with%20LLMs%0AAuthor%3A%20Yiqun%20Duan%20and%20Qiang%20Zhang%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20The%20utilization%20of%20Large%20Language%20Models%20%28LLMs%29%20within%20the%20realm%20of%0Areinforcement%20learning%2C%20particularly%20as%20planners%2C%20has%20garnered%20a%20significant%0Adegree%20of%20attention%20in%20recent%20scholarly%20literature.%20However%2C%20a%20substantial%0Aproportion%20of%20existing%20research%20predominantly%20focuses%20on%20planning%20models%20for%0Arobotics%20that%20transmute%20the%20outputs%20derived%20from%20perception%20models%20into%0Alinguistic%20forms%2C%20thus%20adopting%20a%20%60pure-language%27%20strategy.%20In%20this%20research%2C%0Awe%20propose%20a%20hybrid%20End-to-End%20learning%20framework%20for%20autonomous%20driving%20by%0Acombining%20basic%20driving%20imitation%20learning%20with%20LLMs%20based%20on%20multi-modality%0Aprompt%20tokens.%20Instead%20of%20simply%20converting%20perception%20results%20from%20the%0Aseparated%20train%20model%20into%20pure%20language%20input%2C%20our%20novelty%20lies%20in%20two%0Aaspects.%201%29%20The%20end-to-end%20integration%20of%20visual%20and%20LiDAR%20sensory%20input%20into%0Alearnable%20multi-modality%20tokens%2C%20thereby%20intrinsically%20alleviating%20description%0Abias%20by%20separated%20pre-trained%20perception%20models.%202%29%20Instead%20of%20directly%20letting%0ALLMs%20drive%2C%20this%20paper%20explores%20a%20hybrid%20setting%20of%20letting%20LLMs%20help%20the%0Adriving%20model%20correct%20mistakes%20and%20complicated%20scenarios.%20The%20results%20of%20our%0Aexperiments%20suggest%20that%20the%20proposed%20methodology%20can%20attain%20driving%20scores%20of%0A49.21%25%2C%20coupled%20with%20an%20impressive%20route%20completion%20rate%20of%2091.34%25%20in%20the%0Aoffline%20evaluation%20conducted%20via%20CARLA.%20These%20performance%20metrics%20are%0Acomparable%20to%20the%20most%20advanced%20driving%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04869v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520Multi-Modal%2520Tokens%2520to%2520Enhance%2520End-to-End%2520Autonomous%2520Driving%250A%2520%2520Imitation%2520Learning%2520with%2520LLMs%26entry.906535625%3DYiqun%2520Duan%2520and%2520Qiang%2520Zhang%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520The%2520utilization%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520within%2520the%2520realm%2520of%250Areinforcement%2520learning%252C%2520particularly%2520as%2520planners%252C%2520has%2520garnered%2520a%2520significant%250Adegree%2520of%2520attention%2520in%2520recent%2520scholarly%2520literature.%2520However%252C%2520a%2520substantial%250Aproportion%2520of%2520existing%2520research%2520predominantly%2520focuses%2520on%2520planning%2520models%2520for%250Arobotics%2520that%2520transmute%2520the%2520outputs%2520derived%2520from%2520perception%2520models%2520into%250Alinguistic%2520forms%252C%2520thus%2520adopting%2520a%2520%2560pure-language%2527%2520strategy.%2520In%2520this%2520research%252C%250Awe%2520propose%2520a%2520hybrid%2520End-to-End%2520learning%2520framework%2520for%2520autonomous%2520driving%2520by%250Acombining%2520basic%2520driving%2520imitation%2520learning%2520with%2520LLMs%2520based%2520on%2520multi-modality%250Aprompt%2520tokens.%2520Instead%2520of%2520simply%2520converting%2520perception%2520results%2520from%2520the%250Aseparated%2520train%2520model%2520into%2520pure%2520language%2520input%252C%2520our%2520novelty%2520lies%2520in%2520two%250Aaspects.%25201%2529%2520The%2520end-to-end%2520integration%2520of%2520visual%2520and%2520LiDAR%2520sensory%2520input%2520into%250Alearnable%2520multi-modality%2520tokens%252C%2520thereby%2520intrinsically%2520alleviating%2520description%250Abias%2520by%2520separated%2520pre-trained%2520perception%2520models.%25202%2529%2520Instead%2520of%2520directly%2520letting%250ALLMs%2520drive%252C%2520this%2520paper%2520explores%2520a%2520hybrid%2520setting%2520of%2520letting%2520LLMs%2520help%2520the%250Adriving%2520model%2520correct%2520mistakes%2520and%2520complicated%2520scenarios.%2520The%2520results%2520of%2520our%250Aexperiments%2520suggest%2520that%2520the%2520proposed%2520methodology%2520can%2520attain%2520driving%2520scores%2520of%250A49.21%2525%252C%2520coupled%2520with%2520an%2520impressive%2520route%2520completion%2520rate%2520of%252091.34%2525%2520in%2520the%250Aoffline%2520evaluation%2520conducted%2520via%2520CARLA.%2520These%2520performance%2520metrics%2520are%250Acomparable%2520to%2520the%2520most%2520advanced%2520driving%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04869v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20Multi-Modal%20Tokens%20to%20Enhance%20End-to-End%20Autonomous%20Driving%0A%20%20Imitation%20Learning%20with%20LLMs&entry.906535625=Yiqun%20Duan%20and%20Qiang%20Zhang%20and%20Renjing%20Xu&entry.1292438233=%20%20The%20utilization%20of%20Large%20Language%20Models%20%28LLMs%29%20within%20the%20realm%20of%0Areinforcement%20learning%2C%20particularly%20as%20planners%2C%20has%20garnered%20a%20significant%0Adegree%20of%20attention%20in%20recent%20scholarly%20literature.%20However%2C%20a%20substantial%0Aproportion%20of%20existing%20research%20predominantly%20focuses%20on%20planning%20models%20for%0Arobotics%20that%20transmute%20the%20outputs%20derived%20from%20perception%20models%20into%0Alinguistic%20forms%2C%20thus%20adopting%20a%20%60pure-language%27%20strategy.%20In%20this%20research%2C%0Awe%20propose%20a%20hybrid%20End-to-End%20learning%20framework%20for%20autonomous%20driving%20by%0Acombining%20basic%20driving%20imitation%20learning%20with%20LLMs%20based%20on%20multi-modality%0Aprompt%20tokens.%20Instead%20of%20simply%20converting%20perception%20results%20from%20the%0Aseparated%20train%20model%20into%20pure%20language%20input%2C%20our%20novelty%20lies%20in%20two%0Aaspects.%201%29%20The%20end-to-end%20integration%20of%20visual%20and%20LiDAR%20sensory%20input%20into%0Alearnable%20multi-modality%20tokens%2C%20thereby%20intrinsically%20alleviating%20description%0Abias%20by%20separated%20pre-trained%20perception%20models.%202%29%20Instead%20of%20directly%20letting%0ALLMs%20drive%2C%20this%20paper%20explores%20a%20hybrid%20setting%20of%20letting%20LLMs%20help%20the%0Adriving%20model%20correct%20mistakes%20and%20complicated%20scenarios.%20The%20results%20of%20our%0Aexperiments%20suggest%20that%20the%20proposed%20methodology%20can%20attain%20driving%20scores%20of%0A49.21%25%2C%20coupled%20with%20an%20impressive%20route%20completion%20rate%20of%2091.34%25%20in%20the%0Aoffline%20evaluation%20conducted%20via%20CARLA.%20These%20performance%20metrics%20are%0Acomparable%20to%20the%20most%20advanced%20driving%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04869v2&entry.124074799=Read"},
{"title": "ByteCheckpoint: A Unified Checkpointing System for LLM Development", "author": "Borui Wan and Mingji Han and Yiyao Sheng and Zhichao Lai and Mofan Zhang and Junda Zhang and Yanghua Peng and Haibin Lin and Xin Liu and Chuan Wu", "abstract": "  The development of real-world Large Language Models (LLMs) necessitates\ncheckpointing of training states in persistent storage to mitigate potential\nsoftware and hardware failures, as well as to facilitate checkpoint\ntransferring within the training pipeline and across various tasks. Due to the\nimmense size of LLMs, saving and loading checkpoints often incur intolerable\nminute-level stalls, significantly diminishing training efficiency. Besides,\nwhen transferring checkpoints across tasks, checkpoint resharding, defined as\nloading checkpoints into parallel configurations differing from those used for\nsaving, is often required according to the characteristics and resource quota\nof specific tasks. Previous checkpointing systems [16,3,33,6] assume consistent\nparallel configurations, failing to address the complexities of checkpoint\ntransformation during resharding. Furthermore, in the industry platform,\ndevelopers create checkpoints from different training frameworks[23,36,21,11],\neach with its own unique storage and I/O logic. This diversity complicates the\nimplementation of unified checkpoint management and optimization. To address\nthese challenges, we introduce ByteCheckpoint, a PyTorch-native multi-framework\nLLM checkpointing system that supports automatic online checkpoint resharding.\nByteCheckpoint employs a data/metadata disaggregated storage architecture,\ndecoupling checkpoint storage from the adopted parallelism strategies and\ntraining frameworks. We design an efficient asynchronous tensor merging\ntechnique to settle the irregular tensor sharding problem and propose several\nI/O performance optimizations to significantly enhance the efficiency of\ncheckpoint saving and loading. Experimental results demonstrate\nByteCheckpoint's substantial advantages in reducing checkpoint saving (by up to\n529.22X) and loading (by up to 3.51X) costs, compared to baseline methods.\n", "link": "http://arxiv.org/abs/2407.20143v1", "date": "2024-07-29", "relevancy": 2.1881, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4544}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ByteCheckpoint%3A%20A%20Unified%20Checkpointing%20System%20for%20LLM%20Development&body=Title%3A%20ByteCheckpoint%3A%20A%20Unified%20Checkpointing%20System%20for%20LLM%20Development%0AAuthor%3A%20Borui%20Wan%20and%20Mingji%20Han%20and%20Yiyao%20Sheng%20and%20Zhichao%20Lai%20and%20Mofan%20Zhang%20and%20Junda%20Zhang%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Xin%20Liu%20and%20Chuan%20Wu%0AAbstract%3A%20%20%20The%20development%20of%20real-world%20Large%20Language%20Models%20%28LLMs%29%20necessitates%0Acheckpointing%20of%20training%20states%20in%20persistent%20storage%20to%20mitigate%20potential%0Asoftware%20and%20hardware%20failures%2C%20as%20well%20as%20to%20facilitate%20checkpoint%0Atransferring%20within%20the%20training%20pipeline%20and%20across%20various%20tasks.%20Due%20to%20the%0Aimmense%20size%20of%20LLMs%2C%20saving%20and%20loading%20checkpoints%20often%20incur%20intolerable%0Aminute-level%20stalls%2C%20significantly%20diminishing%20training%20efficiency.%20Besides%2C%0Awhen%20transferring%20checkpoints%20across%20tasks%2C%20checkpoint%20resharding%2C%20defined%20as%0Aloading%20checkpoints%20into%20parallel%20configurations%20differing%20from%20those%20used%20for%0Asaving%2C%20is%20often%20required%20according%20to%20the%20characteristics%20and%20resource%20quota%0Aof%20specific%20tasks.%20Previous%20checkpointing%20systems%20%5B16%2C3%2C33%2C6%5D%20assume%20consistent%0Aparallel%20configurations%2C%20failing%20to%20address%20the%20complexities%20of%20checkpoint%0Atransformation%20during%20resharding.%20Furthermore%2C%20in%20the%20industry%20platform%2C%0Adevelopers%20create%20checkpoints%20from%20different%20training%20frameworks%5B23%2C36%2C21%2C11%5D%2C%0Aeach%20with%20its%20own%20unique%20storage%20and%20I/O%20logic.%20This%20diversity%20complicates%20the%0Aimplementation%20of%20unified%20checkpoint%20management%20and%20optimization.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20ByteCheckpoint%2C%20a%20PyTorch-native%20multi-framework%0ALLM%20checkpointing%20system%20that%20supports%20automatic%20online%20checkpoint%20resharding.%0AByteCheckpoint%20employs%20a%20data/metadata%20disaggregated%20storage%20architecture%2C%0Adecoupling%20checkpoint%20storage%20from%20the%20adopted%20parallelism%20strategies%20and%0Atraining%20frameworks.%20We%20design%20an%20efficient%20asynchronous%20tensor%20merging%0Atechnique%20to%20settle%20the%20irregular%20tensor%20sharding%20problem%20and%20propose%20several%0AI/O%20performance%20optimizations%20to%20significantly%20enhance%20the%20efficiency%20of%0Acheckpoint%20saving%20and%20loading.%20Experimental%20results%20demonstrate%0AByteCheckpoint%27s%20substantial%20advantages%20in%20reducing%20checkpoint%20saving%20%28by%20up%20to%0A529.22X%29%20and%20loading%20%28by%20up%20to%203.51X%29%20costs%2C%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DByteCheckpoint%253A%2520A%2520Unified%2520Checkpointing%2520System%2520for%2520LLM%2520Development%26entry.906535625%3DBorui%2520Wan%2520and%2520Mingji%2520Han%2520and%2520Yiyao%2520Sheng%2520and%2520Zhichao%2520Lai%2520and%2520Mofan%2520Zhang%2520and%2520Junda%2520Zhang%2520and%2520Yanghua%2520Peng%2520and%2520Haibin%2520Lin%2520and%2520Xin%2520Liu%2520and%2520Chuan%2520Wu%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520real-world%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520necessitates%250Acheckpointing%2520of%2520training%2520states%2520in%2520persistent%2520storage%2520to%2520mitigate%2520potential%250Asoftware%2520and%2520hardware%2520failures%252C%2520as%2520well%2520as%2520to%2520facilitate%2520checkpoint%250Atransferring%2520within%2520the%2520training%2520pipeline%2520and%2520across%2520various%2520tasks.%2520Due%2520to%2520the%250Aimmense%2520size%2520of%2520LLMs%252C%2520saving%2520and%2520loading%2520checkpoints%2520often%2520incur%2520intolerable%250Aminute-level%2520stalls%252C%2520significantly%2520diminishing%2520training%2520efficiency.%2520Besides%252C%250Awhen%2520transferring%2520checkpoints%2520across%2520tasks%252C%2520checkpoint%2520resharding%252C%2520defined%2520as%250Aloading%2520checkpoints%2520into%2520parallel%2520configurations%2520differing%2520from%2520those%2520used%2520for%250Asaving%252C%2520is%2520often%2520required%2520according%2520to%2520the%2520characteristics%2520and%2520resource%2520quota%250Aof%2520specific%2520tasks.%2520Previous%2520checkpointing%2520systems%2520%255B16%252C3%252C33%252C6%255D%2520assume%2520consistent%250Aparallel%2520configurations%252C%2520failing%2520to%2520address%2520the%2520complexities%2520of%2520checkpoint%250Atransformation%2520during%2520resharding.%2520Furthermore%252C%2520in%2520the%2520industry%2520platform%252C%250Adevelopers%2520create%2520checkpoints%2520from%2520different%2520training%2520frameworks%255B23%252C36%252C21%252C11%255D%252C%250Aeach%2520with%2520its%2520own%2520unique%2520storage%2520and%2520I/O%2520logic.%2520This%2520diversity%2520complicates%2520the%250Aimplementation%2520of%2520unified%2520checkpoint%2520management%2520and%2520optimization.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520ByteCheckpoint%252C%2520a%2520PyTorch-native%2520multi-framework%250ALLM%2520checkpointing%2520system%2520that%2520supports%2520automatic%2520online%2520checkpoint%2520resharding.%250AByteCheckpoint%2520employs%2520a%2520data/metadata%2520disaggregated%2520storage%2520architecture%252C%250Adecoupling%2520checkpoint%2520storage%2520from%2520the%2520adopted%2520parallelism%2520strategies%2520and%250Atraining%2520frameworks.%2520We%2520design%2520an%2520efficient%2520asynchronous%2520tensor%2520merging%250Atechnique%2520to%2520settle%2520the%2520irregular%2520tensor%2520sharding%2520problem%2520and%2520propose%2520several%250AI/O%2520performance%2520optimizations%2520to%2520significantly%2520enhance%2520the%2520efficiency%2520of%250Acheckpoint%2520saving%2520and%2520loading.%2520Experimental%2520results%2520demonstrate%250AByteCheckpoint%2527s%2520substantial%2520advantages%2520in%2520reducing%2520checkpoint%2520saving%2520%2528by%2520up%2520to%250A529.22X%2529%2520and%2520loading%2520%2528by%2520up%2520to%25203.51X%2529%2520costs%252C%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ByteCheckpoint%3A%20A%20Unified%20Checkpointing%20System%20for%20LLM%20Development&entry.906535625=Borui%20Wan%20and%20Mingji%20Han%20and%20Yiyao%20Sheng%20and%20Zhichao%20Lai%20and%20Mofan%20Zhang%20and%20Junda%20Zhang%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Xin%20Liu%20and%20Chuan%20Wu&entry.1292438233=%20%20The%20development%20of%20real-world%20Large%20Language%20Models%20%28LLMs%29%20necessitates%0Acheckpointing%20of%20training%20states%20in%20persistent%20storage%20to%20mitigate%20potential%0Asoftware%20and%20hardware%20failures%2C%20as%20well%20as%20to%20facilitate%20checkpoint%0Atransferring%20within%20the%20training%20pipeline%20and%20across%20various%20tasks.%20Due%20to%20the%0Aimmense%20size%20of%20LLMs%2C%20saving%20and%20loading%20checkpoints%20often%20incur%20intolerable%0Aminute-level%20stalls%2C%20significantly%20diminishing%20training%20efficiency.%20Besides%2C%0Awhen%20transferring%20checkpoints%20across%20tasks%2C%20checkpoint%20resharding%2C%20defined%20as%0Aloading%20checkpoints%20into%20parallel%20configurations%20differing%20from%20those%20used%20for%0Asaving%2C%20is%20often%20required%20according%20to%20the%20characteristics%20and%20resource%20quota%0Aof%20specific%20tasks.%20Previous%20checkpointing%20systems%20%5B16%2C3%2C33%2C6%5D%20assume%20consistent%0Aparallel%20configurations%2C%20failing%20to%20address%20the%20complexities%20of%20checkpoint%0Atransformation%20during%20resharding.%20Furthermore%2C%20in%20the%20industry%20platform%2C%0Adevelopers%20create%20checkpoints%20from%20different%20training%20frameworks%5B23%2C36%2C21%2C11%5D%2C%0Aeach%20with%20its%20own%20unique%20storage%20and%20I/O%20logic.%20This%20diversity%20complicates%20the%0Aimplementation%20of%20unified%20checkpoint%20management%20and%20optimization.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20ByteCheckpoint%2C%20a%20PyTorch-native%20multi-framework%0ALLM%20checkpointing%20system%20that%20supports%20automatic%20online%20checkpoint%20resharding.%0AByteCheckpoint%20employs%20a%20data/metadata%20disaggregated%20storage%20architecture%2C%0Adecoupling%20checkpoint%20storage%20from%20the%20adopted%20parallelism%20strategies%20and%0Atraining%20frameworks.%20We%20design%20an%20efficient%20asynchronous%20tensor%20merging%0Atechnique%20to%20settle%20the%20irregular%20tensor%20sharding%20problem%20and%20propose%20several%0AI/O%20performance%20optimizations%20to%20significantly%20enhance%20the%20efficiency%20of%0Acheckpoint%20saving%20and%20loading.%20Experimental%20results%20demonstrate%0AByteCheckpoint%27s%20substantial%20advantages%20in%20reducing%20checkpoint%20saving%20%28by%20up%20to%0A529.22X%29%20and%20loading%20%28by%20up%20to%203.51X%29%20costs%2C%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20143v1&entry.124074799=Read"},
{"title": "RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band\n  Radiomap Reconstruction via Graph Attention Networks", "author": "Xiaojie Li and Songyang Zhang and Hang Li and Xiaoyang Li and Lexi Xu and Haigao Xu and Hui Mei and Guangxu Zhu and Nan Qi and Ming Xiao", "abstract": "  Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless\ncommunications for tasks such as spectrum management and network planning.\nHowever, traditional machine-learning-based MB-RMR methods, which rely heavily\non simulated data or complete structured ground truth, face significant\ndeployment challenges. These challenges stem from the differences between\nsimulated and actual data, as well as the scarcity of real-world measurements.\nTo address these challenges, our study presents RadioGAT, a novel framework\nbased on Graph Attention Network (GAT) tailored for MB-RMR within a single\narea, eliminating the need for multi-region datasets. RadioGAT innovatively\nmerges model-based spatial-spectral correlation encoding with data-driven\nradiomap generalization, thus minimizing the reliance on extensive data\nsources. The framework begins by transforming sparse multi-band data into a\ngraph structure through an innovative encoding strategy that leverages radio\npropagation models to capture the spatial-spectral correlation inherent in the\ndata. This graph-based representation not only simplifies data handling but\nalso enables tailored label sampling during training, significantly enhancing\nthe framework's adaptability for deployment. Subsequently, The GAT is employed\nto generalize the radiomap information across various frequency bands.\nExtensive experiments using raytracing datasets based on real-world\nenvironments have demonstrated RadioGAT's enhanced accuracy in supervised\nlearning settings and its robustness in semi-supervised scenarios. These\nresults underscore RadioGAT's effectiveness and practicality for MB-RMR in\nenvironments with limited data availability.\n", "link": "http://arxiv.org/abs/2403.16397v2", "date": "2024-07-29", "relevancy": 2.1577, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5757}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5142}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadioGAT%3A%20A%20Joint%20Model-based%20and%20Data-driven%20Framework%20for%20Multi-band%0A%20%20Radiomap%20Reconstruction%20via%20Graph%20Attention%20Networks&body=Title%3A%20RadioGAT%3A%20A%20Joint%20Model-based%20and%20Data-driven%20Framework%20for%20Multi-band%0A%20%20Radiomap%20Reconstruction%20via%20Graph%20Attention%20Networks%0AAuthor%3A%20Xiaojie%20Li%20and%20Songyang%20Zhang%20and%20Hang%20Li%20and%20Xiaoyang%20Li%20and%20Lexi%20Xu%20and%20Haigao%20Xu%20and%20Hui%20Mei%20and%20Guangxu%20Zhu%20and%20Nan%20Qi%20and%20Ming%20Xiao%0AAbstract%3A%20%20%20Multi-band%20radiomap%20reconstruction%20%28MB-RMR%29%20is%20a%20key%20component%20in%20wireless%0Acommunications%20for%20tasks%20such%20as%20spectrum%20management%20and%20network%20planning.%0AHowever%2C%20traditional%20machine-learning-based%20MB-RMR%20methods%2C%20which%20rely%20heavily%0Aon%20simulated%20data%20or%20complete%20structured%20ground%20truth%2C%20face%20significant%0Adeployment%20challenges.%20These%20challenges%20stem%20from%20the%20differences%20between%0Asimulated%20and%20actual%20data%2C%20as%20well%20as%20the%20scarcity%20of%20real-world%20measurements.%0ATo%20address%20these%20challenges%2C%20our%20study%20presents%20RadioGAT%2C%20a%20novel%20framework%0Abased%20on%20Graph%20Attention%20Network%20%28GAT%29%20tailored%20for%20MB-RMR%20within%20a%20single%0Aarea%2C%20eliminating%20the%20need%20for%20multi-region%20datasets.%20RadioGAT%20innovatively%0Amerges%20model-based%20spatial-spectral%20correlation%20encoding%20with%20data-driven%0Aradiomap%20generalization%2C%20thus%20minimizing%20the%20reliance%20on%20extensive%20data%0Asources.%20The%20framework%20begins%20by%20transforming%20sparse%20multi-band%20data%20into%20a%0Agraph%20structure%20through%20an%20innovative%20encoding%20strategy%20that%20leverages%20radio%0Apropagation%20models%20to%20capture%20the%20spatial-spectral%20correlation%20inherent%20in%20the%0Adata.%20This%20graph-based%20representation%20not%20only%20simplifies%20data%20handling%20but%0Aalso%20enables%20tailored%20label%20sampling%20during%20training%2C%20significantly%20enhancing%0Athe%20framework%27s%20adaptability%20for%20deployment.%20Subsequently%2C%20The%20GAT%20is%20employed%0Ato%20generalize%20the%20radiomap%20information%20across%20various%20frequency%20bands.%0AExtensive%20experiments%20using%20raytracing%20datasets%20based%20on%20real-world%0Aenvironments%20have%20demonstrated%20RadioGAT%27s%20enhanced%20accuracy%20in%20supervised%0Alearning%20settings%20and%20its%20robustness%20in%20semi-supervised%20scenarios.%20These%0Aresults%20underscore%20RadioGAT%27s%20effectiveness%20and%20practicality%20for%20MB-RMR%20in%0Aenvironments%20with%20limited%20data%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadioGAT%253A%2520A%2520Joint%2520Model-based%2520and%2520Data-driven%2520Framework%2520for%2520Multi-band%250A%2520%2520Radiomap%2520Reconstruction%2520via%2520Graph%2520Attention%2520Networks%26entry.906535625%3DXiaojie%2520Li%2520and%2520Songyang%2520Zhang%2520and%2520Hang%2520Li%2520and%2520Xiaoyang%2520Li%2520and%2520Lexi%2520Xu%2520and%2520Haigao%2520Xu%2520and%2520Hui%2520Mei%2520and%2520Guangxu%2520Zhu%2520and%2520Nan%2520Qi%2520and%2520Ming%2520Xiao%26entry.1292438233%3D%2520%2520Multi-band%2520radiomap%2520reconstruction%2520%2528MB-RMR%2529%2520is%2520a%2520key%2520component%2520in%2520wireless%250Acommunications%2520for%2520tasks%2520such%2520as%2520spectrum%2520management%2520and%2520network%2520planning.%250AHowever%252C%2520traditional%2520machine-learning-based%2520MB-RMR%2520methods%252C%2520which%2520rely%2520heavily%250Aon%2520simulated%2520data%2520or%2520complete%2520structured%2520ground%2520truth%252C%2520face%2520significant%250Adeployment%2520challenges.%2520These%2520challenges%2520stem%2520from%2520the%2520differences%2520between%250Asimulated%2520and%2520actual%2520data%252C%2520as%2520well%2520as%2520the%2520scarcity%2520of%2520real-world%2520measurements.%250ATo%2520address%2520these%2520challenges%252C%2520our%2520study%2520presents%2520RadioGAT%252C%2520a%2520novel%2520framework%250Abased%2520on%2520Graph%2520Attention%2520Network%2520%2528GAT%2529%2520tailored%2520for%2520MB-RMR%2520within%2520a%2520single%250Aarea%252C%2520eliminating%2520the%2520need%2520for%2520multi-region%2520datasets.%2520RadioGAT%2520innovatively%250Amerges%2520model-based%2520spatial-spectral%2520correlation%2520encoding%2520with%2520data-driven%250Aradiomap%2520generalization%252C%2520thus%2520minimizing%2520the%2520reliance%2520on%2520extensive%2520data%250Asources.%2520The%2520framework%2520begins%2520by%2520transforming%2520sparse%2520multi-band%2520data%2520into%2520a%250Agraph%2520structure%2520through%2520an%2520innovative%2520encoding%2520strategy%2520that%2520leverages%2520radio%250Apropagation%2520models%2520to%2520capture%2520the%2520spatial-spectral%2520correlation%2520inherent%2520in%2520the%250Adata.%2520This%2520graph-based%2520representation%2520not%2520only%2520simplifies%2520data%2520handling%2520but%250Aalso%2520enables%2520tailored%2520label%2520sampling%2520during%2520training%252C%2520significantly%2520enhancing%250Athe%2520framework%2527s%2520adaptability%2520for%2520deployment.%2520Subsequently%252C%2520The%2520GAT%2520is%2520employed%250Ato%2520generalize%2520the%2520radiomap%2520information%2520across%2520various%2520frequency%2520bands.%250AExtensive%2520experiments%2520using%2520raytracing%2520datasets%2520based%2520on%2520real-world%250Aenvironments%2520have%2520demonstrated%2520RadioGAT%2527s%2520enhanced%2520accuracy%2520in%2520supervised%250Alearning%2520settings%2520and%2520its%2520robustness%2520in%2520semi-supervised%2520scenarios.%2520These%250Aresults%2520underscore%2520RadioGAT%2527s%2520effectiveness%2520and%2520practicality%2520for%2520MB-RMR%2520in%250Aenvironments%2520with%2520limited%2520data%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadioGAT%3A%20A%20Joint%20Model-based%20and%20Data-driven%20Framework%20for%20Multi-band%0A%20%20Radiomap%20Reconstruction%20via%20Graph%20Attention%20Networks&entry.906535625=Xiaojie%20Li%20and%20Songyang%20Zhang%20and%20Hang%20Li%20and%20Xiaoyang%20Li%20and%20Lexi%20Xu%20and%20Haigao%20Xu%20and%20Hui%20Mei%20and%20Guangxu%20Zhu%20and%20Nan%20Qi%20and%20Ming%20Xiao&entry.1292438233=%20%20Multi-band%20radiomap%20reconstruction%20%28MB-RMR%29%20is%20a%20key%20component%20in%20wireless%0Acommunications%20for%20tasks%20such%20as%20spectrum%20management%20and%20network%20planning.%0AHowever%2C%20traditional%20machine-learning-based%20MB-RMR%20methods%2C%20which%20rely%20heavily%0Aon%20simulated%20data%20or%20complete%20structured%20ground%20truth%2C%20face%20significant%0Adeployment%20challenges.%20These%20challenges%20stem%20from%20the%20differences%20between%0Asimulated%20and%20actual%20data%2C%20as%20well%20as%20the%20scarcity%20of%20real-world%20measurements.%0ATo%20address%20these%20challenges%2C%20our%20study%20presents%20RadioGAT%2C%20a%20novel%20framework%0Abased%20on%20Graph%20Attention%20Network%20%28GAT%29%20tailored%20for%20MB-RMR%20within%20a%20single%0Aarea%2C%20eliminating%20the%20need%20for%20multi-region%20datasets.%20RadioGAT%20innovatively%0Amerges%20model-based%20spatial-spectral%20correlation%20encoding%20with%20data-driven%0Aradiomap%20generalization%2C%20thus%20minimizing%20the%20reliance%20on%20extensive%20data%0Asources.%20The%20framework%20begins%20by%20transforming%20sparse%20multi-band%20data%20into%20a%0Agraph%20structure%20through%20an%20innovative%20encoding%20strategy%20that%20leverages%20radio%0Apropagation%20models%20to%20capture%20the%20spatial-spectral%20correlation%20inherent%20in%20the%0Adata.%20This%20graph-based%20representation%20not%20only%20simplifies%20data%20handling%20but%0Aalso%20enables%20tailored%20label%20sampling%20during%20training%2C%20significantly%20enhancing%0Athe%20framework%27s%20adaptability%20for%20deployment.%20Subsequently%2C%20The%20GAT%20is%20employed%0Ato%20generalize%20the%20radiomap%20information%20across%20various%20frequency%20bands.%0AExtensive%20experiments%20using%20raytracing%20datasets%20based%20on%20real-world%0Aenvironments%20have%20demonstrated%20RadioGAT%27s%20enhanced%20accuracy%20in%20supervised%0Alearning%20settings%20and%20its%20robustness%20in%20semi-supervised%20scenarios.%20These%0Aresults%20underscore%20RadioGAT%27s%20effectiveness%20and%20practicality%20for%20MB-RMR%20in%0Aenvironments%20with%20limited%20data%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16397v2&entry.124074799=Read"},
{"title": "FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training\n  with Limited Resources", "author": "Xiyuan Wei and Fanjiang Ye and Ori Yonay and Xingyu Chen and Baixi Sun and Dingwen Tao and Tianbao Yang", "abstract": "  Existing studies of training state-of-the-art Contrastive Language-Image\nPretraining (CLIP) models on large-scale data involve hundreds of or even\nthousands of GPUs due to the requirement of a large batch size. However, such a\nlarge amount of resources is not accessible to most people. While advanced\ncompositional optimization techniques for optimizing global contrastive losses\nhave been demonstrated effective for removing the requirement of large batch\nsize, their performance on large-scale data remains underexplored and not\noptimized. To bridge the gap, this paper explores several aspects of CLIP\ntraining with limited resources (e.g., up to tens of GPUs). First, we introduce\nFastCLIP, a general CLIP training framework built on advanced compositional\noptimization techniques while designed and optimized for the distributed\nsetting. Our framework is equipped with an efficient gradient reduction\nstrategy to reduce communication overhead. Second, to further boost training\nefficiency, we investigate three components of the framework from an\noptimization perspective: the schedule of the inner learning rate, the update\nrules of the temperature parameter and the model parameters, respectively.\nExperiments on different strategies for each component shed light on how to\nconduct CLIP training more efficiently. Finally, we benchmark the performance\nof FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different\ncompute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7\nmillion, 9.1 million to 315 million image-text pairs to demonstrate the\nsignificant improvement of FastCLIP in the resource-limited setting. We release\nthe code of FastCLIP at https://github.com/Optimization-AI/fast_clip .\n", "link": "http://arxiv.org/abs/2407.01445v2", "date": "2024-07-29", "relevancy": 2.1527, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5573}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.539}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastCLIP%3A%20A%20Suite%20of%20Optimization%20Techniques%20to%20Accelerate%20CLIP%20Training%0A%20%20with%20Limited%20Resources&body=Title%3A%20FastCLIP%3A%20A%20Suite%20of%20Optimization%20Techniques%20to%20Accelerate%20CLIP%20Training%0A%20%20with%20Limited%20Resources%0AAuthor%3A%20Xiyuan%20Wei%20and%20Fanjiang%20Ye%20and%20Ori%20Yonay%20and%20Xingyu%20Chen%20and%20Baixi%20Sun%20and%20Dingwen%20Tao%20and%20Tianbao%20Yang%0AAbstract%3A%20%20%20Existing%20studies%20of%20training%20state-of-the-art%20Contrastive%20Language-Image%0APretraining%20%28CLIP%29%20models%20on%20large-scale%20data%20involve%20hundreds%20of%20or%20even%0Athousands%20of%20GPUs%20due%20to%20the%20requirement%20of%20a%20large%20batch%20size.%20However%2C%20such%20a%0Alarge%20amount%20of%20resources%20is%20not%20accessible%20to%20most%20people.%20While%20advanced%0Acompositional%20optimization%20techniques%20for%20optimizing%20global%20contrastive%20losses%0Ahave%20been%20demonstrated%20effective%20for%20removing%20the%20requirement%20of%20large%20batch%0Asize%2C%20their%20performance%20on%20large-scale%20data%20remains%20underexplored%20and%20not%0Aoptimized.%20To%20bridge%20the%20gap%2C%20this%20paper%20explores%20several%20aspects%20of%20CLIP%0Atraining%20with%20limited%20resources%20%28e.g.%2C%20up%20to%20tens%20of%20GPUs%29.%20First%2C%20we%20introduce%0AFastCLIP%2C%20a%20general%20CLIP%20training%20framework%20built%20on%20advanced%20compositional%0Aoptimization%20techniques%20while%20designed%20and%20optimized%20for%20the%20distributed%0Asetting.%20Our%20framework%20is%20equipped%20with%20an%20efficient%20gradient%20reduction%0Astrategy%20to%20reduce%20communication%20overhead.%20Second%2C%20to%20further%20boost%20training%0Aefficiency%2C%20we%20investigate%20three%20components%20of%20the%20framework%20from%20an%0Aoptimization%20perspective%3A%20the%20schedule%20of%20the%20inner%20learning%20rate%2C%20the%20update%0Arules%20of%20the%20temperature%20parameter%20and%20the%20model%20parameters%2C%20respectively.%0AExperiments%20on%20different%20strategies%20for%20each%20component%20shed%20light%20on%20how%20to%0Aconduct%20CLIP%20training%20more%20efficiently.%20Finally%2C%20we%20benchmark%20the%20performance%0Aof%20FastCLIP%20and%20the%20state-of-the-art%20training%20baseline%20%28OpenCLIP%29%20on%20different%0Acompute%20scales%20up%20to%2032%20GPUs%20on%208%20nodes%2C%20and%20three%20data%20scales%20ranging%20from%202.7%0Amillion%2C%209.1%20million%20to%20315%20million%20image-text%20pairs%20to%20demonstrate%20the%0Asignificant%20improvement%20of%20FastCLIP%20in%20the%20resource-limited%20setting.%20We%20release%0Athe%20code%20of%20FastCLIP%20at%20https%3A//github.com/Optimization-AI/fast_clip%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastCLIP%253A%2520A%2520Suite%2520of%2520Optimization%2520Techniques%2520to%2520Accelerate%2520CLIP%2520Training%250A%2520%2520with%2520Limited%2520Resources%26entry.906535625%3DXiyuan%2520Wei%2520and%2520Fanjiang%2520Ye%2520and%2520Ori%2520Yonay%2520and%2520Xingyu%2520Chen%2520and%2520Baixi%2520Sun%2520and%2520Dingwen%2520Tao%2520and%2520Tianbao%2520Yang%26entry.1292438233%3D%2520%2520Existing%2520studies%2520of%2520training%2520state-of-the-art%2520Contrastive%2520Language-Image%250APretraining%2520%2528CLIP%2529%2520models%2520on%2520large-scale%2520data%2520involve%2520hundreds%2520of%2520or%2520even%250Athousands%2520of%2520GPUs%2520due%2520to%2520the%2520requirement%2520of%2520a%2520large%2520batch%2520size.%2520However%252C%2520such%2520a%250Alarge%2520amount%2520of%2520resources%2520is%2520not%2520accessible%2520to%2520most%2520people.%2520While%2520advanced%250Acompositional%2520optimization%2520techniques%2520for%2520optimizing%2520global%2520contrastive%2520losses%250Ahave%2520been%2520demonstrated%2520effective%2520for%2520removing%2520the%2520requirement%2520of%2520large%2520batch%250Asize%252C%2520their%2520performance%2520on%2520large-scale%2520data%2520remains%2520underexplored%2520and%2520not%250Aoptimized.%2520To%2520bridge%2520the%2520gap%252C%2520this%2520paper%2520explores%2520several%2520aspects%2520of%2520CLIP%250Atraining%2520with%2520limited%2520resources%2520%2528e.g.%252C%2520up%2520to%2520tens%2520of%2520GPUs%2529.%2520First%252C%2520we%2520introduce%250AFastCLIP%252C%2520a%2520general%2520CLIP%2520training%2520framework%2520built%2520on%2520advanced%2520compositional%250Aoptimization%2520techniques%2520while%2520designed%2520and%2520optimized%2520for%2520the%2520distributed%250Asetting.%2520Our%2520framework%2520is%2520equipped%2520with%2520an%2520efficient%2520gradient%2520reduction%250Astrategy%2520to%2520reduce%2520communication%2520overhead.%2520Second%252C%2520to%2520further%2520boost%2520training%250Aefficiency%252C%2520we%2520investigate%2520three%2520components%2520of%2520the%2520framework%2520from%2520an%250Aoptimization%2520perspective%253A%2520the%2520schedule%2520of%2520the%2520inner%2520learning%2520rate%252C%2520the%2520update%250Arules%2520of%2520the%2520temperature%2520parameter%2520and%2520the%2520model%2520parameters%252C%2520respectively.%250AExperiments%2520on%2520different%2520strategies%2520for%2520each%2520component%2520shed%2520light%2520on%2520how%2520to%250Aconduct%2520CLIP%2520training%2520more%2520efficiently.%2520Finally%252C%2520we%2520benchmark%2520the%2520performance%250Aof%2520FastCLIP%2520and%2520the%2520state-of-the-art%2520training%2520baseline%2520%2528OpenCLIP%2529%2520on%2520different%250Acompute%2520scales%2520up%2520to%252032%2520GPUs%2520on%25208%2520nodes%252C%2520and%2520three%2520data%2520scales%2520ranging%2520from%25202.7%250Amillion%252C%25209.1%2520million%2520to%2520315%2520million%2520image-text%2520pairs%2520to%2520demonstrate%2520the%250Asignificant%2520improvement%2520of%2520FastCLIP%2520in%2520the%2520resource-limited%2520setting.%2520We%2520release%250Athe%2520code%2520of%2520FastCLIP%2520at%2520https%253A//github.com/Optimization-AI/fast_clip%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastCLIP%3A%20A%20Suite%20of%20Optimization%20Techniques%20to%20Accelerate%20CLIP%20Training%0A%20%20with%20Limited%20Resources&entry.906535625=Xiyuan%20Wei%20and%20Fanjiang%20Ye%20and%20Ori%20Yonay%20and%20Xingyu%20Chen%20and%20Baixi%20Sun%20and%20Dingwen%20Tao%20and%20Tianbao%20Yang&entry.1292438233=%20%20Existing%20studies%20of%20training%20state-of-the-art%20Contrastive%20Language-Image%0APretraining%20%28CLIP%29%20models%20on%20large-scale%20data%20involve%20hundreds%20of%20or%20even%0Athousands%20of%20GPUs%20due%20to%20the%20requirement%20of%20a%20large%20batch%20size.%20However%2C%20such%20a%0Alarge%20amount%20of%20resources%20is%20not%20accessible%20to%20most%20people.%20While%20advanced%0Acompositional%20optimization%20techniques%20for%20optimizing%20global%20contrastive%20losses%0Ahave%20been%20demonstrated%20effective%20for%20removing%20the%20requirement%20of%20large%20batch%0Asize%2C%20their%20performance%20on%20large-scale%20data%20remains%20underexplored%20and%20not%0Aoptimized.%20To%20bridge%20the%20gap%2C%20this%20paper%20explores%20several%20aspects%20of%20CLIP%0Atraining%20with%20limited%20resources%20%28e.g.%2C%20up%20to%20tens%20of%20GPUs%29.%20First%2C%20we%20introduce%0AFastCLIP%2C%20a%20general%20CLIP%20training%20framework%20built%20on%20advanced%20compositional%0Aoptimization%20techniques%20while%20designed%20and%20optimized%20for%20the%20distributed%0Asetting.%20Our%20framework%20is%20equipped%20with%20an%20efficient%20gradient%20reduction%0Astrategy%20to%20reduce%20communication%20overhead.%20Second%2C%20to%20further%20boost%20training%0Aefficiency%2C%20we%20investigate%20three%20components%20of%20the%20framework%20from%20an%0Aoptimization%20perspective%3A%20the%20schedule%20of%20the%20inner%20learning%20rate%2C%20the%20update%0Arules%20of%20the%20temperature%20parameter%20and%20the%20model%20parameters%2C%20respectively.%0AExperiments%20on%20different%20strategies%20for%20each%20component%20shed%20light%20on%20how%20to%0Aconduct%20CLIP%20training%20more%20efficiently.%20Finally%2C%20we%20benchmark%20the%20performance%0Aof%20FastCLIP%20and%20the%20state-of-the-art%20training%20baseline%20%28OpenCLIP%29%20on%20different%0Acompute%20scales%20up%20to%2032%20GPUs%20on%208%20nodes%2C%20and%20three%20data%20scales%20ranging%20from%202.7%0Amillion%2C%209.1%20million%20to%20315%20million%20image-text%20pairs%20to%20demonstrate%20the%0Asignificant%20improvement%20of%20FastCLIP%20in%20the%20resource-limited%20setting.%20We%20release%0Athe%20code%20of%20FastCLIP%20at%20https%3A//github.com/Optimization-AI/fast_clip%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01445v2&entry.124074799=Read"},
{"title": "TabMDA: Tabular Manifold Data Augmentation for Any Classifier using\n  Transformers with In-context Subsetting", "author": "Andrei Margeloiu and Adri\u00e1n Bazaga and Nikola Simidjievski and Pietro Li\u00f2 and Mateja Jamnik", "abstract": "  Tabular data is prevalent in many critical domains, yet it is often\nchallenging to acquire in large quantities. This scarcity usually results in\npoor performance of machine learning models on such data. Data augmentation, a\ncommon strategy for performance improvement in vision and language tasks,\ntypically underperforms for tabular data due to the lack of explicit symmetries\nin the input space. To overcome this challenge, we introduce TabMDA, a novel\nmethod for manifold data augmentation on tabular data. This method utilises a\npre-trained in-context model, such as TabPFN, to map the data into an embedding\nspace. TabMDA performs label-invariant transformations by encoding the data\nmultiple times with varied contexts. This process explores the learned\nembedding space of the underlying in-context models, thereby enlarging the\ntraining dataset. TabMDA is a training-free method, making it applicable to any\nclassifier. We evaluate TabMDA on five standard classifiers and observe\nsignificant performance improvements across various tabular datasets. Our\nresults demonstrate that TabMDA provides an effective way to leverage\ninformation from pre-trained in-context models to enhance the performance of\ndownstream classifiers. Code is available at\nhttps://github.com/AdrianBZG/TabMDA.\n", "link": "http://arxiv.org/abs/2406.01805v2", "date": "2024-07-29", "relevancy": 2.1448, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5625}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5244}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabMDA%3A%20Tabular%20Manifold%20Data%20Augmentation%20for%20Any%20Classifier%20using%0A%20%20Transformers%20with%20In-context%20Subsetting&body=Title%3A%20TabMDA%3A%20Tabular%20Manifold%20Data%20Augmentation%20for%20Any%20Classifier%20using%0A%20%20Transformers%20with%20In-context%20Subsetting%0AAuthor%3A%20Andrei%20Margeloiu%20and%20Adri%C3%A1n%20Bazaga%20and%20Nikola%20Simidjievski%20and%20Pietro%20Li%C3%B2%20and%20Mateja%20Jamnik%0AAbstract%3A%20%20%20Tabular%20data%20is%20prevalent%20in%20many%20critical%20domains%2C%20yet%20it%20is%20often%0Achallenging%20to%20acquire%20in%20large%20quantities.%20This%20scarcity%20usually%20results%20in%0Apoor%20performance%20of%20machine%20learning%20models%20on%20such%20data.%20Data%20augmentation%2C%20a%0Acommon%20strategy%20for%20performance%20improvement%20in%20vision%20and%20language%20tasks%2C%0Atypically%20underperforms%20for%20tabular%20data%20due%20to%20the%20lack%20of%20explicit%20symmetries%0Ain%20the%20input%20space.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20TabMDA%2C%20a%20novel%0Amethod%20for%20manifold%20data%20augmentation%20on%20tabular%20data.%20This%20method%20utilises%20a%0Apre-trained%20in-context%20model%2C%20such%20as%20TabPFN%2C%20to%20map%20the%20data%20into%20an%20embedding%0Aspace.%20TabMDA%20performs%20label-invariant%20transformations%20by%20encoding%20the%20data%0Amultiple%20times%20with%20varied%20contexts.%20This%20process%20explores%20the%20learned%0Aembedding%20space%20of%20the%20underlying%20in-context%20models%2C%20thereby%20enlarging%20the%0Atraining%20dataset.%20TabMDA%20is%20a%20training-free%20method%2C%20making%20it%20applicable%20to%20any%0Aclassifier.%20We%20evaluate%20TabMDA%20on%20five%20standard%20classifiers%20and%20observe%0Asignificant%20performance%20improvements%20across%20various%20tabular%20datasets.%20Our%0Aresults%20demonstrate%20that%20TabMDA%20provides%20an%20effective%20way%20to%20leverage%0Ainformation%20from%20pre-trained%20in-context%20models%20to%20enhance%20the%20performance%20of%0Adownstream%20classifiers.%20Code%20is%20available%20at%0Ahttps%3A//github.com/AdrianBZG/TabMDA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01805v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabMDA%253A%2520Tabular%2520Manifold%2520Data%2520Augmentation%2520for%2520Any%2520Classifier%2520using%250A%2520%2520Transformers%2520with%2520In-context%2520Subsetting%26entry.906535625%3DAndrei%2520Margeloiu%2520and%2520Adri%25C3%25A1n%2520Bazaga%2520and%2520Nikola%2520Simidjievski%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Mateja%2520Jamnik%26entry.1292438233%3D%2520%2520Tabular%2520data%2520is%2520prevalent%2520in%2520many%2520critical%2520domains%252C%2520yet%2520it%2520is%2520often%250Achallenging%2520to%2520acquire%2520in%2520large%2520quantities.%2520This%2520scarcity%2520usually%2520results%2520in%250Apoor%2520performance%2520of%2520machine%2520learning%2520models%2520on%2520such%2520data.%2520Data%2520augmentation%252C%2520a%250Acommon%2520strategy%2520for%2520performance%2520improvement%2520in%2520vision%2520and%2520language%2520tasks%252C%250Atypically%2520underperforms%2520for%2520tabular%2520data%2520due%2520to%2520the%2520lack%2520of%2520explicit%2520symmetries%250Ain%2520the%2520input%2520space.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520TabMDA%252C%2520a%2520novel%250Amethod%2520for%2520manifold%2520data%2520augmentation%2520on%2520tabular%2520data.%2520This%2520method%2520utilises%2520a%250Apre-trained%2520in-context%2520model%252C%2520such%2520as%2520TabPFN%252C%2520to%2520map%2520the%2520data%2520into%2520an%2520embedding%250Aspace.%2520TabMDA%2520performs%2520label-invariant%2520transformations%2520by%2520encoding%2520the%2520data%250Amultiple%2520times%2520with%2520varied%2520contexts.%2520This%2520process%2520explores%2520the%2520learned%250Aembedding%2520space%2520of%2520the%2520underlying%2520in-context%2520models%252C%2520thereby%2520enlarging%2520the%250Atraining%2520dataset.%2520TabMDA%2520is%2520a%2520training-free%2520method%252C%2520making%2520it%2520applicable%2520to%2520any%250Aclassifier.%2520We%2520evaluate%2520TabMDA%2520on%2520five%2520standard%2520classifiers%2520and%2520observe%250Asignificant%2520performance%2520improvements%2520across%2520various%2520tabular%2520datasets.%2520Our%250Aresults%2520demonstrate%2520that%2520TabMDA%2520provides%2520an%2520effective%2520way%2520to%2520leverage%250Ainformation%2520from%2520pre-trained%2520in-context%2520models%2520to%2520enhance%2520the%2520performance%2520of%250Adownstream%2520classifiers.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/AdrianBZG/TabMDA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01805v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabMDA%3A%20Tabular%20Manifold%20Data%20Augmentation%20for%20Any%20Classifier%20using%0A%20%20Transformers%20with%20In-context%20Subsetting&entry.906535625=Andrei%20Margeloiu%20and%20Adri%C3%A1n%20Bazaga%20and%20Nikola%20Simidjievski%20and%20Pietro%20Li%C3%B2%20and%20Mateja%20Jamnik&entry.1292438233=%20%20Tabular%20data%20is%20prevalent%20in%20many%20critical%20domains%2C%20yet%20it%20is%20often%0Achallenging%20to%20acquire%20in%20large%20quantities.%20This%20scarcity%20usually%20results%20in%0Apoor%20performance%20of%20machine%20learning%20models%20on%20such%20data.%20Data%20augmentation%2C%20a%0Acommon%20strategy%20for%20performance%20improvement%20in%20vision%20and%20language%20tasks%2C%0Atypically%20underperforms%20for%20tabular%20data%20due%20to%20the%20lack%20of%20explicit%20symmetries%0Ain%20the%20input%20space.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20TabMDA%2C%20a%20novel%0Amethod%20for%20manifold%20data%20augmentation%20on%20tabular%20data.%20This%20method%20utilises%20a%0Apre-trained%20in-context%20model%2C%20such%20as%20TabPFN%2C%20to%20map%20the%20data%20into%20an%20embedding%0Aspace.%20TabMDA%20performs%20label-invariant%20transformations%20by%20encoding%20the%20data%0Amultiple%20times%20with%20varied%20contexts.%20This%20process%20explores%20the%20learned%0Aembedding%20space%20of%20the%20underlying%20in-context%20models%2C%20thereby%20enlarging%20the%0Atraining%20dataset.%20TabMDA%20is%20a%20training-free%20method%2C%20making%20it%20applicable%20to%20any%0Aclassifier.%20We%20evaluate%20TabMDA%20on%20five%20standard%20classifiers%20and%20observe%0Asignificant%20performance%20improvements%20across%20various%20tabular%20datasets.%20Our%0Aresults%20demonstrate%20that%20TabMDA%20provides%20an%20effective%20way%20to%20leverage%0Ainformation%20from%20pre-trained%20in-context%20models%20to%20enhance%20the%20performance%20of%0Adownstream%20classifiers.%20Code%20is%20available%20at%0Ahttps%3A//github.com/AdrianBZG/TabMDA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01805v2&entry.124074799=Read"},
{"title": "SalNAS: Efficient Saliency-prediction Neural Architecture Search with\n  self-knowledge distillation", "author": "Chakkrit Termritthikun and Ayaz Umer and Suwichaya Suwanwimolkul and Feng Xia and Ivan Lee", "abstract": "  Recent advancements in deep convolutional neural networks have significantly\nimproved the performance of saliency prediction. However, the manual\nconfiguration of the neural network architectures requires domain knowledge\nexpertise and can still be time-consuming and error-prone. To solve this, we\npropose a new Neural Architecture Search (NAS) framework for saliency\nprediction with two contributions. Firstly, a supernet for saliency prediction\nis built with a weight-sharing network containing all candidate architectures,\nby integrating a dynamic convolution into the encoder-decoder in the supernet,\ntermed SalNAS. Secondly, despite the fact that SalNAS is highly efficient\n(20.98 million parameters), it can suffer from the lack of generalization. To\nsolve this, we propose a self-knowledge distillation approach, termed Self-KD,\nthat trains the student SalNAS with the weighted average information between\nthe ground truth and the prediction from the teacher model. The teacher model,\nwhile sharing the same architecture, contains the best-performing weights\nchosen by cross-validation. Self-KD can generalize well without the need to\ncompute the gradient in the teacher model, enabling an efficient training\nsystem. By utilizing Self-KD, SalNAS outperforms other state-of-the-art\nsaliency prediction models in most evaluation rubrics across seven benchmark\ndatasets while being a lightweight model. The code will be available at\nhttps://github.com/chakkritte/SalNAS\n", "link": "http://arxiv.org/abs/2407.20062v1", "date": "2024-07-29", "relevancy": 2.1355, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5381}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5349}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SalNAS%3A%20Efficient%20Saliency-prediction%20Neural%20Architecture%20Search%20with%0A%20%20self-knowledge%20distillation&body=Title%3A%20SalNAS%3A%20Efficient%20Saliency-prediction%20Neural%20Architecture%20Search%20with%0A%20%20self-knowledge%20distillation%0AAuthor%3A%20Chakkrit%20Termritthikun%20and%20Ayaz%20Umer%20and%20Suwichaya%20Suwanwimolkul%20and%20Feng%20Xia%20and%20Ivan%20Lee%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20convolutional%20neural%20networks%20have%20significantly%0Aimproved%20the%20performance%20of%20saliency%20prediction.%20However%2C%20the%20manual%0Aconfiguration%20of%20the%20neural%20network%20architectures%20requires%20domain%20knowledge%0Aexpertise%20and%20can%20still%20be%20time-consuming%20and%20error-prone.%20To%20solve%20this%2C%20we%0Apropose%20a%20new%20Neural%20Architecture%20Search%20%28NAS%29%20framework%20for%20saliency%0Aprediction%20with%20two%20contributions.%20Firstly%2C%20a%20supernet%20for%20saliency%20prediction%0Ais%20built%20with%20a%20weight-sharing%20network%20containing%20all%20candidate%20architectures%2C%0Aby%20integrating%20a%20dynamic%20convolution%20into%20the%20encoder-decoder%20in%20the%20supernet%2C%0Atermed%20SalNAS.%20Secondly%2C%20despite%20the%20fact%20that%20SalNAS%20is%20highly%20efficient%0A%2820.98%20million%20parameters%29%2C%20it%20can%20suffer%20from%20the%20lack%20of%20generalization.%20To%0Asolve%20this%2C%20we%20propose%20a%20self-knowledge%20distillation%20approach%2C%20termed%20Self-KD%2C%0Athat%20trains%20the%20student%20SalNAS%20with%20the%20weighted%20average%20information%20between%0Athe%20ground%20truth%20and%20the%20prediction%20from%20the%20teacher%20model.%20The%20teacher%20model%2C%0Awhile%20sharing%20the%20same%20architecture%2C%20contains%20the%20best-performing%20weights%0Achosen%20by%20cross-validation.%20Self-KD%20can%20generalize%20well%20without%20the%20need%20to%0Acompute%20the%20gradient%20in%20the%20teacher%20model%2C%20enabling%20an%20efficient%20training%0Asystem.%20By%20utilizing%20Self-KD%2C%20SalNAS%20outperforms%20other%20state-of-the-art%0Asaliency%20prediction%20models%20in%20most%20evaluation%20rubrics%20across%20seven%20benchmark%0Adatasets%20while%20being%20a%20lightweight%20model.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/chakkritte/SalNAS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSalNAS%253A%2520Efficient%2520Saliency-prediction%2520Neural%2520Architecture%2520Search%2520with%250A%2520%2520self-knowledge%2520distillation%26entry.906535625%3DChakkrit%2520Termritthikun%2520and%2520Ayaz%2520Umer%2520and%2520Suwichaya%2520Suwanwimolkul%2520and%2520Feng%2520Xia%2520and%2520Ivan%2520Lee%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520convolutional%2520neural%2520networks%2520have%2520significantly%250Aimproved%2520the%2520performance%2520of%2520saliency%2520prediction.%2520However%252C%2520the%2520manual%250Aconfiguration%2520of%2520the%2520neural%2520network%2520architectures%2520requires%2520domain%2520knowledge%250Aexpertise%2520and%2520can%2520still%2520be%2520time-consuming%2520and%2520error-prone.%2520To%2520solve%2520this%252C%2520we%250Apropose%2520a%2520new%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%2520framework%2520for%2520saliency%250Aprediction%2520with%2520two%2520contributions.%2520Firstly%252C%2520a%2520supernet%2520for%2520saliency%2520prediction%250Ais%2520built%2520with%2520a%2520weight-sharing%2520network%2520containing%2520all%2520candidate%2520architectures%252C%250Aby%2520integrating%2520a%2520dynamic%2520convolution%2520into%2520the%2520encoder-decoder%2520in%2520the%2520supernet%252C%250Atermed%2520SalNAS.%2520Secondly%252C%2520despite%2520the%2520fact%2520that%2520SalNAS%2520is%2520highly%2520efficient%250A%252820.98%2520million%2520parameters%2529%252C%2520it%2520can%2520suffer%2520from%2520the%2520lack%2520of%2520generalization.%2520To%250Asolve%2520this%252C%2520we%2520propose%2520a%2520self-knowledge%2520distillation%2520approach%252C%2520termed%2520Self-KD%252C%250Athat%2520trains%2520the%2520student%2520SalNAS%2520with%2520the%2520weighted%2520average%2520information%2520between%250Athe%2520ground%2520truth%2520and%2520the%2520prediction%2520from%2520the%2520teacher%2520model.%2520The%2520teacher%2520model%252C%250Awhile%2520sharing%2520the%2520same%2520architecture%252C%2520contains%2520the%2520best-performing%2520weights%250Achosen%2520by%2520cross-validation.%2520Self-KD%2520can%2520generalize%2520well%2520without%2520the%2520need%2520to%250Acompute%2520the%2520gradient%2520in%2520the%2520teacher%2520model%252C%2520enabling%2520an%2520efficient%2520training%250Asystem.%2520By%2520utilizing%2520Self-KD%252C%2520SalNAS%2520outperforms%2520other%2520state-of-the-art%250Asaliency%2520prediction%2520models%2520in%2520most%2520evaluation%2520rubrics%2520across%2520seven%2520benchmark%250Adatasets%2520while%2520being%2520a%2520lightweight%2520model.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/chakkritte/SalNAS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SalNAS%3A%20Efficient%20Saliency-prediction%20Neural%20Architecture%20Search%20with%0A%20%20self-knowledge%20distillation&entry.906535625=Chakkrit%20Termritthikun%20and%20Ayaz%20Umer%20and%20Suwichaya%20Suwanwimolkul%20and%20Feng%20Xia%20and%20Ivan%20Lee&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20convolutional%20neural%20networks%20have%20significantly%0Aimproved%20the%20performance%20of%20saliency%20prediction.%20However%2C%20the%20manual%0Aconfiguration%20of%20the%20neural%20network%20architectures%20requires%20domain%20knowledge%0Aexpertise%20and%20can%20still%20be%20time-consuming%20and%20error-prone.%20To%20solve%20this%2C%20we%0Apropose%20a%20new%20Neural%20Architecture%20Search%20%28NAS%29%20framework%20for%20saliency%0Aprediction%20with%20two%20contributions.%20Firstly%2C%20a%20supernet%20for%20saliency%20prediction%0Ais%20built%20with%20a%20weight-sharing%20network%20containing%20all%20candidate%20architectures%2C%0Aby%20integrating%20a%20dynamic%20convolution%20into%20the%20encoder-decoder%20in%20the%20supernet%2C%0Atermed%20SalNAS.%20Secondly%2C%20despite%20the%20fact%20that%20SalNAS%20is%20highly%20efficient%0A%2820.98%20million%20parameters%29%2C%20it%20can%20suffer%20from%20the%20lack%20of%20generalization.%20To%0Asolve%20this%2C%20we%20propose%20a%20self-knowledge%20distillation%20approach%2C%20termed%20Self-KD%2C%0Athat%20trains%20the%20student%20SalNAS%20with%20the%20weighted%20average%20information%20between%0Athe%20ground%20truth%20and%20the%20prediction%20from%20the%20teacher%20model.%20The%20teacher%20model%2C%0Awhile%20sharing%20the%20same%20architecture%2C%20contains%20the%20best-performing%20weights%0Achosen%20by%20cross-validation.%20Self-KD%20can%20generalize%20well%20without%20the%20need%20to%0Acompute%20the%20gradient%20in%20the%20teacher%20model%2C%20enabling%20an%20efficient%20training%0Asystem.%20By%20utilizing%20Self-KD%2C%20SalNAS%20outperforms%20other%20state-of-the-art%0Asaliency%20prediction%20models%20in%20most%20evaluation%20rubrics%20across%20seven%20benchmark%0Adatasets%20while%20being%20a%20lightweight%20model.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/chakkritte/SalNAS%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20062v1&entry.124074799=Read"},
{"title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for\n  Training LLMs", "author": "Feiyang Kang and Yifan Sun and Bingbing Wen and Si Chen and Dawn Song and Rafid Mahmood and Ruoxi Jia", "abstract": "  To ensure performance on a diverse set of downstream tasks, LLMs are\npretrained via data mixtures over different domains. In this work, we\ndemonstrate that the optimal data composition for a fixed compute budget varies\ndepending on the scale of the training data, suggesting that the common\npractice of empirically determining an optimal composition using small-scale\nexperiments will not yield the optimal data mixtures when scaling up to the\nfinal model. To address this challenge, we propose *AutoScale*, an automated\ntool that finds a compute-optimal data composition for training at any desired\ntarget scale. AutoScale first determines the optimal composition at a small\nscale using a novel bilevel optimization framework, Direct Data Optimization\n(*DDO*), and then fits a predictor to estimate the optimal composition at\nlarger scales. The predictor's design is inspired by our theoretical analysis\nof scaling laws related to data composition, which could be of independent\ninterest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2\nLarge) on RedPajama dataset, AutoScale decreases validation perplexity at least\n25% faster than any baseline with up to 38% speed up compared to without\nreweighting, achieving the best overall performance across downstream tasks. On\npre-training Encoder-only LMs (BERT) with masked language modeling, DDO is\nshown to decrease loss on all domains while visibly improving average task\nperformance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by\n5.9% compared with without reweighting. AutoScale speeds up training by up to\n28%. Our codes are open-sourced.\n", "link": "http://arxiv.org/abs/2407.20177v1", "date": "2024-07-29", "relevancy": 2.1313, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5575}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5352}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoScale%3A%20Automatic%20Prediction%20of%20Compute-optimal%20Data%20Composition%20for%0A%20%20Training%20LLMs&body=Title%3A%20AutoScale%3A%20Automatic%20Prediction%20of%20Compute-optimal%20Data%20Composition%20for%0A%20%20Training%20LLMs%0AAuthor%3A%20Feiyang%20Kang%20and%20Yifan%20Sun%20and%20Bingbing%20Wen%20and%20Si%20Chen%20and%20Dawn%20Song%20and%20Rafid%20Mahmood%20and%20Ruoxi%20Jia%0AAbstract%3A%20%20%20To%20ensure%20performance%20on%20a%20diverse%20set%20of%20downstream%20tasks%2C%20LLMs%20are%0Apretrained%20via%20data%20mixtures%20over%20different%20domains.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20the%20optimal%20data%20composition%20for%20a%20fixed%20compute%20budget%20varies%0Adepending%20on%20the%20scale%20of%20the%20training%20data%2C%20suggesting%20that%20the%20common%0Apractice%20of%20empirically%20determining%20an%20optimal%20composition%20using%20small-scale%0Aexperiments%20will%20not%20yield%20the%20optimal%20data%20mixtures%20when%20scaling%20up%20to%20the%0Afinal%20model.%20To%20address%20this%20challenge%2C%20we%20propose%20%2AAutoScale%2A%2C%20an%20automated%0Atool%20that%20finds%20a%20compute-optimal%20data%20composition%20for%20training%20at%20any%20desired%0Atarget%20scale.%20AutoScale%20first%20determines%20the%20optimal%20composition%20at%20a%20small%0Ascale%20using%20a%20novel%20bilevel%20optimization%20framework%2C%20Direct%20Data%20Optimization%0A%28%2ADDO%2A%29%2C%20and%20then%20fits%20a%20predictor%20to%20estimate%20the%20optimal%20composition%20at%0Alarger%20scales.%20The%20predictor%27s%20design%20is%20inspired%20by%20our%20theoretical%20analysis%0Aof%20scaling%20laws%20related%20to%20data%20composition%2C%20which%20could%20be%20of%20independent%0Ainterest.%20In%20empirical%20studies%20with%20pre-training%20774M%20Decoder-only%20LMs%20%28GPT-2%0ALarge%29%20on%20RedPajama%20dataset%2C%20AutoScale%20decreases%20validation%20perplexity%20at%20least%0A25%25%20faster%20than%20any%20baseline%20with%20up%20to%2038%25%20speed%20up%20compared%20to%20without%0Areweighting%2C%20achieving%20the%20best%20overall%20performance%20across%20downstream%20tasks.%20On%0Apre-training%20Encoder-only%20LMs%20%28BERT%29%20with%20masked%20language%20modeling%2C%20DDO%20is%0Ashown%20to%20decrease%20loss%20on%20all%20domains%20while%20visibly%20improving%20average%20task%0Aperformance%20on%20GLUE%20benchmark%20by%208.7%25%20and%20on%20large-scale%20QA%20dataset%20%28SQuAD%29%20by%0A5.9%25%20compared%20with%20without%20reweighting.%20AutoScale%20speeds%20up%20training%20by%20up%20to%0A28%25.%20Our%20codes%20are%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoScale%253A%2520Automatic%2520Prediction%2520of%2520Compute-optimal%2520Data%2520Composition%2520for%250A%2520%2520Training%2520LLMs%26entry.906535625%3DFeiyang%2520Kang%2520and%2520Yifan%2520Sun%2520and%2520Bingbing%2520Wen%2520and%2520Si%2520Chen%2520and%2520Dawn%2520Song%2520and%2520Rafid%2520Mahmood%2520and%2520Ruoxi%2520Jia%26entry.1292438233%3D%2520%2520To%2520ensure%2520performance%2520on%2520a%2520diverse%2520set%2520of%2520downstream%2520tasks%252C%2520LLMs%2520are%250Apretrained%2520via%2520data%2520mixtures%2520over%2520different%2520domains.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520the%2520optimal%2520data%2520composition%2520for%2520a%2520fixed%2520compute%2520budget%2520varies%250Adepending%2520on%2520the%2520scale%2520of%2520the%2520training%2520data%252C%2520suggesting%2520that%2520the%2520common%250Apractice%2520of%2520empirically%2520determining%2520an%2520optimal%2520composition%2520using%2520small-scale%250Aexperiments%2520will%2520not%2520yield%2520the%2520optimal%2520data%2520mixtures%2520when%2520scaling%2520up%2520to%2520the%250Afinal%2520model.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520%252AAutoScale%252A%252C%2520an%2520automated%250Atool%2520that%2520finds%2520a%2520compute-optimal%2520data%2520composition%2520for%2520training%2520at%2520any%2520desired%250Atarget%2520scale.%2520AutoScale%2520first%2520determines%2520the%2520optimal%2520composition%2520at%2520a%2520small%250Ascale%2520using%2520a%2520novel%2520bilevel%2520optimization%2520framework%252C%2520Direct%2520Data%2520Optimization%250A%2528%252ADDO%252A%2529%252C%2520and%2520then%2520fits%2520a%2520predictor%2520to%2520estimate%2520the%2520optimal%2520composition%2520at%250Alarger%2520scales.%2520The%2520predictor%2527s%2520design%2520is%2520inspired%2520by%2520our%2520theoretical%2520analysis%250Aof%2520scaling%2520laws%2520related%2520to%2520data%2520composition%252C%2520which%2520could%2520be%2520of%2520independent%250Ainterest.%2520In%2520empirical%2520studies%2520with%2520pre-training%2520774M%2520Decoder-only%2520LMs%2520%2528GPT-2%250ALarge%2529%2520on%2520RedPajama%2520dataset%252C%2520AutoScale%2520decreases%2520validation%2520perplexity%2520at%2520least%250A25%2525%2520faster%2520than%2520any%2520baseline%2520with%2520up%2520to%252038%2525%2520speed%2520up%2520compared%2520to%2520without%250Areweighting%252C%2520achieving%2520the%2520best%2520overall%2520performance%2520across%2520downstream%2520tasks.%2520On%250Apre-training%2520Encoder-only%2520LMs%2520%2528BERT%2529%2520with%2520masked%2520language%2520modeling%252C%2520DDO%2520is%250Ashown%2520to%2520decrease%2520loss%2520on%2520all%2520domains%2520while%2520visibly%2520improving%2520average%2520task%250Aperformance%2520on%2520GLUE%2520benchmark%2520by%25208.7%2525%2520and%2520on%2520large-scale%2520QA%2520dataset%2520%2528SQuAD%2529%2520by%250A5.9%2525%2520compared%2520with%2520without%2520reweighting.%2520AutoScale%2520speeds%2520up%2520training%2520by%2520up%2520to%250A28%2525.%2520Our%2520codes%2520are%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoScale%3A%20Automatic%20Prediction%20of%20Compute-optimal%20Data%20Composition%20for%0A%20%20Training%20LLMs&entry.906535625=Feiyang%20Kang%20and%20Yifan%20Sun%20and%20Bingbing%20Wen%20and%20Si%20Chen%20and%20Dawn%20Song%20and%20Rafid%20Mahmood%20and%20Ruoxi%20Jia&entry.1292438233=%20%20To%20ensure%20performance%20on%20a%20diverse%20set%20of%20downstream%20tasks%2C%20LLMs%20are%0Apretrained%20via%20data%20mixtures%20over%20different%20domains.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20the%20optimal%20data%20composition%20for%20a%20fixed%20compute%20budget%20varies%0Adepending%20on%20the%20scale%20of%20the%20training%20data%2C%20suggesting%20that%20the%20common%0Apractice%20of%20empirically%20determining%20an%20optimal%20composition%20using%20small-scale%0Aexperiments%20will%20not%20yield%20the%20optimal%20data%20mixtures%20when%20scaling%20up%20to%20the%0Afinal%20model.%20To%20address%20this%20challenge%2C%20we%20propose%20%2AAutoScale%2A%2C%20an%20automated%0Atool%20that%20finds%20a%20compute-optimal%20data%20composition%20for%20training%20at%20any%20desired%0Atarget%20scale.%20AutoScale%20first%20determines%20the%20optimal%20composition%20at%20a%20small%0Ascale%20using%20a%20novel%20bilevel%20optimization%20framework%2C%20Direct%20Data%20Optimization%0A%28%2ADDO%2A%29%2C%20and%20then%20fits%20a%20predictor%20to%20estimate%20the%20optimal%20composition%20at%0Alarger%20scales.%20The%20predictor%27s%20design%20is%20inspired%20by%20our%20theoretical%20analysis%0Aof%20scaling%20laws%20related%20to%20data%20composition%2C%20which%20could%20be%20of%20independent%0Ainterest.%20In%20empirical%20studies%20with%20pre-training%20774M%20Decoder-only%20LMs%20%28GPT-2%0ALarge%29%20on%20RedPajama%20dataset%2C%20AutoScale%20decreases%20validation%20perplexity%20at%20least%0A25%25%20faster%20than%20any%20baseline%20with%20up%20to%2038%25%20speed%20up%20compared%20to%20without%0Areweighting%2C%20achieving%20the%20best%20overall%20performance%20across%20downstream%20tasks.%20On%0Apre-training%20Encoder-only%20LMs%20%28BERT%29%20with%20masked%20language%20modeling%2C%20DDO%20is%0Ashown%20to%20decrease%20loss%20on%20all%20domains%20while%20visibly%20improving%20average%20task%0Aperformance%20on%20GLUE%20benchmark%20by%208.7%25%20and%20on%20large-scale%20QA%20dataset%20%28SQuAD%29%20by%0A5.9%25%20compared%20with%20without%20reweighting.%20AutoScale%20speeds%20up%20training%20by%20up%20to%0A28%25.%20Our%20codes%20are%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20177v1&entry.124074799=Read"},
{"title": "Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised\n  Pretrained Transformers for Single- and Multi-Objective Continuous\n  Optimization Problems", "author": "Moritz Vinzent Seiler and Pascal Kerschke and Heike Trautmann", "abstract": "  In many recent works, the potential of Exploratory Landscape Analysis (ELA)\nfeatures to numerically characterize, in particular, single-objective\ncontinuous optimization problems has been demonstrated. These numerical\nfeatures provide the input for all kinds of machine learning tasks on\ncontinuous optimization problems, ranging, i.a., from High-level Property\nPrediction to Automated Algorithm Selection and Automated Algorithm\nConfiguration. Without ELA features, analyzing and understanding the\ncharacteristics of single-objective continuous optimization problems is -- to\nthe best of our knowledge -- very limited.\n  Yet, despite their usefulness, as demonstrated in several past works, ELA\nfeatures suffer from several drawbacks. These include, in particular, (1.) a\nstrong correlation between multiple features, as well as (2.) its very limited\napplicability to multi-objective continuous optimization problems. As a remedy,\nrecent works proposed deep learning-based approaches as alternatives to ELA. In\nthese works, e.g., point-cloud transformers were used to characterize an\noptimization problem's fitness landscape. However, these approaches require a\nlarge amount of labeled training data.\n  Within this work, we propose a hybrid approach, Deep-ELA, which combines (the\nbenefits of) deep learning and ELA features. Specifically, we pre-trained four\ntransformers on millions of randomly generated optimization problems to learn\ndeep representations of the landscapes of continuous single- and\nmulti-objective optimization problems. Our proposed framework can either be\nused out-of-the-box for analyzing single- and multi-objective continuous\noptimization problems, or subsequently fine-tuned to various tasks focussing on\nalgorithm behavior and problem understanding.\n", "link": "http://arxiv.org/abs/2401.01192v2", "date": "2024-07-29", "relevancy": 2.1277, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.545}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-ELA%3A%20Deep%20Exploratory%20Landscape%20Analysis%20with%20Self-Supervised%0A%20%20Pretrained%20Transformers%20for%20Single-%20and%20Multi-Objective%20Continuous%0A%20%20Optimization%20Problems&body=Title%3A%20Deep-ELA%3A%20Deep%20Exploratory%20Landscape%20Analysis%20with%20Self-Supervised%0A%20%20Pretrained%20Transformers%20for%20Single-%20and%20Multi-Objective%20Continuous%0A%20%20Optimization%20Problems%0AAuthor%3A%20Moritz%20Vinzent%20Seiler%20and%20Pascal%20Kerschke%20and%20Heike%20Trautmann%0AAbstract%3A%20%20%20In%20many%20recent%20works%2C%20the%20potential%20of%20Exploratory%20Landscape%20Analysis%20%28ELA%29%0Afeatures%20to%20numerically%20characterize%2C%20in%20particular%2C%20single-objective%0Acontinuous%20optimization%20problems%20has%20been%20demonstrated.%20These%20numerical%0Afeatures%20provide%20the%20input%20for%20all%20kinds%20of%20machine%20learning%20tasks%20on%0Acontinuous%20optimization%20problems%2C%20ranging%2C%20i.a.%2C%20from%20High-level%20Property%0APrediction%20to%20Automated%20Algorithm%20Selection%20and%20Automated%20Algorithm%0AConfiguration.%20Without%20ELA%20features%2C%20analyzing%20and%20understanding%20the%0Acharacteristics%20of%20single-objective%20continuous%20optimization%20problems%20is%20--%20to%0Athe%20best%20of%20our%20knowledge%20--%20very%20limited.%0A%20%20Yet%2C%20despite%20their%20usefulness%2C%20as%20demonstrated%20in%20several%20past%20works%2C%20ELA%0Afeatures%20suffer%20from%20several%20drawbacks.%20These%20include%2C%20in%20particular%2C%20%281.%29%20a%0Astrong%20correlation%20between%20multiple%20features%2C%20as%20well%20as%20%282.%29%20its%20very%20limited%0Aapplicability%20to%20multi-objective%20continuous%20optimization%20problems.%20As%20a%20remedy%2C%0Arecent%20works%20proposed%20deep%20learning-based%20approaches%20as%20alternatives%20to%20ELA.%20In%0Athese%20works%2C%20e.g.%2C%20point-cloud%20transformers%20were%20used%20to%20characterize%20an%0Aoptimization%20problem%27s%20fitness%20landscape.%20However%2C%20these%20approaches%20require%20a%0Alarge%20amount%20of%20labeled%20training%20data.%0A%20%20Within%20this%20work%2C%20we%20propose%20a%20hybrid%20approach%2C%20Deep-ELA%2C%20which%20combines%20%28the%0Abenefits%20of%29%20deep%20learning%20and%20ELA%20features.%20Specifically%2C%20we%20pre-trained%20four%0Atransformers%20on%20millions%20of%20randomly%20generated%20optimization%20problems%20to%20learn%0Adeep%20representations%20of%20the%20landscapes%20of%20continuous%20single-%20and%0Amulti-objective%20optimization%20problems.%20Our%20proposed%20framework%20can%20either%20be%0Aused%20out-of-the-box%20for%20analyzing%20single-%20and%20multi-objective%20continuous%0Aoptimization%20problems%2C%20or%20subsequently%20fine-tuned%20to%20various%20tasks%20focussing%20on%0Aalgorithm%20behavior%20and%20problem%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01192v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-ELA%253A%2520Deep%2520Exploratory%2520Landscape%2520Analysis%2520with%2520Self-Supervised%250A%2520%2520Pretrained%2520Transformers%2520for%2520Single-%2520and%2520Multi-Objective%2520Continuous%250A%2520%2520Optimization%2520Problems%26entry.906535625%3DMoritz%2520Vinzent%2520Seiler%2520and%2520Pascal%2520Kerschke%2520and%2520Heike%2520Trautmann%26entry.1292438233%3D%2520%2520In%2520many%2520recent%2520works%252C%2520the%2520potential%2520of%2520Exploratory%2520Landscape%2520Analysis%2520%2528ELA%2529%250Afeatures%2520to%2520numerically%2520characterize%252C%2520in%2520particular%252C%2520single-objective%250Acontinuous%2520optimization%2520problems%2520has%2520been%2520demonstrated.%2520These%2520numerical%250Afeatures%2520provide%2520the%2520input%2520for%2520all%2520kinds%2520of%2520machine%2520learning%2520tasks%2520on%250Acontinuous%2520optimization%2520problems%252C%2520ranging%252C%2520i.a.%252C%2520from%2520High-level%2520Property%250APrediction%2520to%2520Automated%2520Algorithm%2520Selection%2520and%2520Automated%2520Algorithm%250AConfiguration.%2520Without%2520ELA%2520features%252C%2520analyzing%2520and%2520understanding%2520the%250Acharacteristics%2520of%2520single-objective%2520continuous%2520optimization%2520problems%2520is%2520--%2520to%250Athe%2520best%2520of%2520our%2520knowledge%2520--%2520very%2520limited.%250A%2520%2520Yet%252C%2520despite%2520their%2520usefulness%252C%2520as%2520demonstrated%2520in%2520several%2520past%2520works%252C%2520ELA%250Afeatures%2520suffer%2520from%2520several%2520drawbacks.%2520These%2520include%252C%2520in%2520particular%252C%2520%25281.%2529%2520a%250Astrong%2520correlation%2520between%2520multiple%2520features%252C%2520as%2520well%2520as%2520%25282.%2529%2520its%2520very%2520limited%250Aapplicability%2520to%2520multi-objective%2520continuous%2520optimization%2520problems.%2520As%2520a%2520remedy%252C%250Arecent%2520works%2520proposed%2520deep%2520learning-based%2520approaches%2520as%2520alternatives%2520to%2520ELA.%2520In%250Athese%2520works%252C%2520e.g.%252C%2520point-cloud%2520transformers%2520were%2520used%2520to%2520characterize%2520an%250Aoptimization%2520problem%2527s%2520fitness%2520landscape.%2520However%252C%2520these%2520approaches%2520require%2520a%250Alarge%2520amount%2520of%2520labeled%2520training%2520data.%250A%2520%2520Within%2520this%2520work%252C%2520we%2520propose%2520a%2520hybrid%2520approach%252C%2520Deep-ELA%252C%2520which%2520combines%2520%2528the%250Abenefits%2520of%2529%2520deep%2520learning%2520and%2520ELA%2520features.%2520Specifically%252C%2520we%2520pre-trained%2520four%250Atransformers%2520on%2520millions%2520of%2520randomly%2520generated%2520optimization%2520problems%2520to%2520learn%250Adeep%2520representations%2520of%2520the%2520landscapes%2520of%2520continuous%2520single-%2520and%250Amulti-objective%2520optimization%2520problems.%2520Our%2520proposed%2520framework%2520can%2520either%2520be%250Aused%2520out-of-the-box%2520for%2520analyzing%2520single-%2520and%2520multi-objective%2520continuous%250Aoptimization%2520problems%252C%2520or%2520subsequently%2520fine-tuned%2520to%2520various%2520tasks%2520focussing%2520on%250Aalgorithm%2520behavior%2520and%2520problem%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01192v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-ELA%3A%20Deep%20Exploratory%20Landscape%20Analysis%20with%20Self-Supervised%0A%20%20Pretrained%20Transformers%20for%20Single-%20and%20Multi-Objective%20Continuous%0A%20%20Optimization%20Problems&entry.906535625=Moritz%20Vinzent%20Seiler%20and%20Pascal%20Kerschke%20and%20Heike%20Trautmann&entry.1292438233=%20%20In%20many%20recent%20works%2C%20the%20potential%20of%20Exploratory%20Landscape%20Analysis%20%28ELA%29%0Afeatures%20to%20numerically%20characterize%2C%20in%20particular%2C%20single-objective%0Acontinuous%20optimization%20problems%20has%20been%20demonstrated.%20These%20numerical%0Afeatures%20provide%20the%20input%20for%20all%20kinds%20of%20machine%20learning%20tasks%20on%0Acontinuous%20optimization%20problems%2C%20ranging%2C%20i.a.%2C%20from%20High-level%20Property%0APrediction%20to%20Automated%20Algorithm%20Selection%20and%20Automated%20Algorithm%0AConfiguration.%20Without%20ELA%20features%2C%20analyzing%20and%20understanding%20the%0Acharacteristics%20of%20single-objective%20continuous%20optimization%20problems%20is%20--%20to%0Athe%20best%20of%20our%20knowledge%20--%20very%20limited.%0A%20%20Yet%2C%20despite%20their%20usefulness%2C%20as%20demonstrated%20in%20several%20past%20works%2C%20ELA%0Afeatures%20suffer%20from%20several%20drawbacks.%20These%20include%2C%20in%20particular%2C%20%281.%29%20a%0Astrong%20correlation%20between%20multiple%20features%2C%20as%20well%20as%20%282.%29%20its%20very%20limited%0Aapplicability%20to%20multi-objective%20continuous%20optimization%20problems.%20As%20a%20remedy%2C%0Arecent%20works%20proposed%20deep%20learning-based%20approaches%20as%20alternatives%20to%20ELA.%20In%0Athese%20works%2C%20e.g.%2C%20point-cloud%20transformers%20were%20used%20to%20characterize%20an%0Aoptimization%20problem%27s%20fitness%20landscape.%20However%2C%20these%20approaches%20require%20a%0Alarge%20amount%20of%20labeled%20training%20data.%0A%20%20Within%20this%20work%2C%20we%20propose%20a%20hybrid%20approach%2C%20Deep-ELA%2C%20which%20combines%20%28the%0Abenefits%20of%29%20deep%20learning%20and%20ELA%20features.%20Specifically%2C%20we%20pre-trained%20four%0Atransformers%20on%20millions%20of%20randomly%20generated%20optimization%20problems%20to%20learn%0Adeep%20representations%20of%20the%20landscapes%20of%20continuous%20single-%20and%0Amulti-objective%20optimization%20problems.%20Our%20proposed%20framework%20can%20either%20be%0Aused%20out-of-the-box%20for%20analyzing%20single-%20and%20multi-objective%20continuous%0Aoptimization%20problems%2C%20or%20subsequently%20fine-tuned%20to%20various%20tasks%20focussing%20on%0Aalgorithm%20behavior%20and%20problem%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01192v2&entry.124074799=Read"},
{"title": "Unsupervised Training of Convex Regularizers using Maximum Likelihood\n  Estimation", "author": "Hong Ye Tan and Ziruo Cai and Marcelo Pereyra and Subhadip Mukherjee and Junqi Tang and Carola-Bibiane Sch\u00f6nlieb", "abstract": "  Imaging is a standard example of an inverse problem, where the task of\nreconstructing a ground truth from a noisy measurement is ill-posed. Recent\nstate-of-the-art approaches for imaging use deep learning, spearheaded by\nunrolled and end-to-end models and trained on various image datasets. However,\nmany such methods require the availability of ground truth data, which may be\nunavailable or expensive, leading to a fundamental barrier that can not be\nbypassed by choice of architecture. Unsupervised learning presents an\nalternative paradigm that bypasses this requirement, as they can be learned\ndirectly on noisy data and do not require any ground truths. A principled\nBayesian approach to unsupervised learning is to maximize the marginal\nlikelihood with respect to the given noisy measurements, which is intrinsically\nlinked to classical variational regularization. We propose an unsupervised\napproach using maximum marginal likelihood estimation to train a convex neural\nnetwork-based image regularization term directly on noisy measurements,\nimproving upon previous work in both model expressiveness and dataset size.\nExperiments demonstrate that the proposed method produces priors that are near\ncompetitive when compared to the analogous supervised training method for\nvarious image corruption operators, maintaining significantly better\ngeneralization properties when compared to end-to-end methods. Moreover, we\nprovide a detailed theoretical analysis of the convergence properties of our\nproposed algorithm.\n", "link": "http://arxiv.org/abs/2404.05445v2", "date": "2024-07-29", "relevancy": 2.122, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5049}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation&body=Title%3A%20Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation%0AAuthor%3A%20Hong%20Ye%20Tan%20and%20Ziruo%20Cai%20and%20Marcelo%20Pereyra%20and%20Subhadip%20Mukherjee%20and%20Junqi%20Tang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20Imaging%20is%20a%20standard%20example%20of%20an%20inverse%20problem%2C%20where%20the%20task%20of%0Areconstructing%20a%20ground%20truth%20from%20a%20noisy%20measurement%20is%20ill-posed.%20Recent%0Astate-of-the-art%20approaches%20for%20imaging%20use%20deep%20learning%2C%20spearheaded%20by%0Aunrolled%20and%20end-to-end%20models%20and%20trained%20on%20various%20image%20datasets.%20However%2C%0Amany%20such%20methods%20require%20the%20availability%20of%20ground%20truth%20data%2C%20which%20may%20be%0Aunavailable%20or%20expensive%2C%20leading%20to%20a%20fundamental%20barrier%20that%20can%20not%20be%0Abypassed%20by%20choice%20of%20architecture.%20Unsupervised%20learning%20presents%20an%0Aalternative%20paradigm%20that%20bypasses%20this%20requirement%2C%20as%20they%20can%20be%20learned%0Adirectly%20on%20noisy%20data%20and%20do%20not%20require%20any%20ground%20truths.%20A%20principled%0ABayesian%20approach%20to%20unsupervised%20learning%20is%20to%20maximize%20the%20marginal%0Alikelihood%20with%20respect%20to%20the%20given%20noisy%20measurements%2C%20which%20is%20intrinsically%0Alinked%20to%20classical%20variational%20regularization.%20We%20propose%20an%20unsupervised%0Aapproach%20using%20maximum%20marginal%20likelihood%20estimation%20to%20train%20a%20convex%20neural%0Anetwork-based%20image%20regularization%20term%20directly%20on%20noisy%20measurements%2C%0Aimproving%20upon%20previous%20work%20in%20both%20model%20expressiveness%20and%20dataset%20size.%0AExperiments%20demonstrate%20that%20the%20proposed%20method%20produces%20priors%20that%20are%20near%0Acompetitive%20when%20compared%20to%20the%20analogous%20supervised%20training%20method%20for%0Avarious%20image%20corruption%20operators%2C%20maintaining%20significantly%20better%0Ageneralization%20properties%20when%20compared%20to%20end-to-end%20methods.%20Moreover%2C%20we%0Aprovide%20a%20detailed%20theoretical%20analysis%20of%20the%20convergence%20properties%20of%20our%0Aproposed%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Training%2520of%2520Convex%2520Regularizers%2520using%2520Maximum%2520Likelihood%250A%2520%2520Estimation%26entry.906535625%3DHong%2520Ye%2520Tan%2520and%2520Ziruo%2520Cai%2520and%2520Marcelo%2520Pereyra%2520and%2520Subhadip%2520Mukherjee%2520and%2520Junqi%2520Tang%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%26entry.1292438233%3D%2520%2520Imaging%2520is%2520a%2520standard%2520example%2520of%2520an%2520inverse%2520problem%252C%2520where%2520the%2520task%2520of%250Areconstructing%2520a%2520ground%2520truth%2520from%2520a%2520noisy%2520measurement%2520is%2520ill-posed.%2520Recent%250Astate-of-the-art%2520approaches%2520for%2520imaging%2520use%2520deep%2520learning%252C%2520spearheaded%2520by%250Aunrolled%2520and%2520end-to-end%2520models%2520and%2520trained%2520on%2520various%2520image%2520datasets.%2520However%252C%250Amany%2520such%2520methods%2520require%2520the%2520availability%2520of%2520ground%2520truth%2520data%252C%2520which%2520may%2520be%250Aunavailable%2520or%2520expensive%252C%2520leading%2520to%2520a%2520fundamental%2520barrier%2520that%2520can%2520not%2520be%250Abypassed%2520by%2520choice%2520of%2520architecture.%2520Unsupervised%2520learning%2520presents%2520an%250Aalternative%2520paradigm%2520that%2520bypasses%2520this%2520requirement%252C%2520as%2520they%2520can%2520be%2520learned%250Adirectly%2520on%2520noisy%2520data%2520and%2520do%2520not%2520require%2520any%2520ground%2520truths.%2520A%2520principled%250ABayesian%2520approach%2520to%2520unsupervised%2520learning%2520is%2520to%2520maximize%2520the%2520marginal%250Alikelihood%2520with%2520respect%2520to%2520the%2520given%2520noisy%2520measurements%252C%2520which%2520is%2520intrinsically%250Alinked%2520to%2520classical%2520variational%2520regularization.%2520We%2520propose%2520an%2520unsupervised%250Aapproach%2520using%2520maximum%2520marginal%2520likelihood%2520estimation%2520to%2520train%2520a%2520convex%2520neural%250Anetwork-based%2520image%2520regularization%2520term%2520directly%2520on%2520noisy%2520measurements%252C%250Aimproving%2520upon%2520previous%2520work%2520in%2520both%2520model%2520expressiveness%2520and%2520dataset%2520size.%250AExperiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520produces%2520priors%2520that%2520are%2520near%250Acompetitive%2520when%2520compared%2520to%2520the%2520analogous%2520supervised%2520training%2520method%2520for%250Avarious%2520image%2520corruption%2520operators%252C%2520maintaining%2520significantly%2520better%250Ageneralization%2520properties%2520when%2520compared%2520to%2520end-to-end%2520methods.%2520Moreover%252C%2520we%250Aprovide%2520a%2520detailed%2520theoretical%2520analysis%2520of%2520the%2520convergence%2520properties%2520of%2520our%250Aproposed%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation&entry.906535625=Hong%20Ye%20Tan%20and%20Ziruo%20Cai%20and%20Marcelo%20Pereyra%20and%20Subhadip%20Mukherjee%20and%20Junqi%20Tang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=%20%20Imaging%20is%20a%20standard%20example%20of%20an%20inverse%20problem%2C%20where%20the%20task%20of%0Areconstructing%20a%20ground%20truth%20from%20a%20noisy%20measurement%20is%20ill-posed.%20Recent%0Astate-of-the-art%20approaches%20for%20imaging%20use%20deep%20learning%2C%20spearheaded%20by%0Aunrolled%20and%20end-to-end%20models%20and%20trained%20on%20various%20image%20datasets.%20However%2C%0Amany%20such%20methods%20require%20the%20availability%20of%20ground%20truth%20data%2C%20which%20may%20be%0Aunavailable%20or%20expensive%2C%20leading%20to%20a%20fundamental%20barrier%20that%20can%20not%20be%0Abypassed%20by%20choice%20of%20architecture.%20Unsupervised%20learning%20presents%20an%0Aalternative%20paradigm%20that%20bypasses%20this%20requirement%2C%20as%20they%20can%20be%20learned%0Adirectly%20on%20noisy%20data%20and%20do%20not%20require%20any%20ground%20truths.%20A%20principled%0ABayesian%20approach%20to%20unsupervised%20learning%20is%20to%20maximize%20the%20marginal%0Alikelihood%20with%20respect%20to%20the%20given%20noisy%20measurements%2C%20which%20is%20intrinsically%0Alinked%20to%20classical%20variational%20regularization.%20We%20propose%20an%20unsupervised%0Aapproach%20using%20maximum%20marginal%20likelihood%20estimation%20to%20train%20a%20convex%20neural%0Anetwork-based%20image%20regularization%20term%20directly%20on%20noisy%20measurements%2C%0Aimproving%20upon%20previous%20work%20in%20both%20model%20expressiveness%20and%20dataset%20size.%0AExperiments%20demonstrate%20that%20the%20proposed%20method%20produces%20priors%20that%20are%20near%0Acompetitive%20when%20compared%20to%20the%20analogous%20supervised%20training%20method%20for%0Avarious%20image%20corruption%20operators%2C%20maintaining%20significantly%20better%0Ageneralization%20properties%20when%20compared%20to%20end-to-end%20methods.%20Moreover%2C%20we%0Aprovide%20a%20detailed%20theoretical%20analysis%20of%20the%20convergence%20properties%20of%20our%0Aproposed%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05445v2&entry.124074799=Read"},
{"title": "Adversarial Robustness in RGB-Skeleton Action Recognition: Leveraging\n  Attention Modality Reweighter", "author": "Chao Liu and Xin Liu and Zitong Yu and Yonghong Hou and Huanjing Yue and Jingyu Yang", "abstract": "  Deep neural networks (DNNs) have been applied in many computer vision tasks\nand achieved state-of-the-art (SOTA) performance. However, misclassification\nwill occur when DNNs predict adversarial examples which are created by adding\nhuman-imperceptible adversarial noise to natural examples. This limits the\napplication of DNN in security-critical fields. In order to enhance the\nrobustness of models, previous research has primarily focused on the unimodal\ndomain, such as image recognition and video understanding. Although multi-modal\nlearning has achieved advanced performance in various tasks, such as action\nrecognition, research on the robustness of RGB-skeleton action recognition\nmodels is scarce. In this paper, we systematically investigate how to improve\nthe robustness of RGB-skeleton action recognition models. We initially\nconducted empirical analysis on the robustness of different modalities and\nobserved that the skeleton modality is more robust than the RGB modality.\nMotivated by this observation, we propose the \\formatword{A}ttention-based\n\\formatword{M}odality \\formatword{R}eweighter (\\formatword{AMR}), which\nutilizes an attention layer to re-weight the two modalities, enabling the model\nto learn more robust features. Our AMR is plug-and-play, allowing easy\nintegration with multimodal models. To demonstrate the effectiveness of AMR, we\nconducted extensive experiments on various datasets. For example, compared to\nthe SOTA methods, AMR exhibits a 43.77\\% improvement against PGD20 attacks on\nthe NTU-RGB+D 60 dataset. Furthermore, it effectively balances the differences\nin robustness between different modalities.\n", "link": "http://arxiv.org/abs/2407.19981v1", "date": "2024-07-29", "relevancy": 2.1143, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5348}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5245}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Robustness%20in%20RGB-Skeleton%20Action%20Recognition%3A%20Leveraging%0A%20%20Attention%20Modality%20Reweighter&body=Title%3A%20Adversarial%20Robustness%20in%20RGB-Skeleton%20Action%20Recognition%3A%20Leveraging%0A%20%20Attention%20Modality%20Reweighter%0AAuthor%3A%20Chao%20Liu%20and%20Xin%20Liu%20and%20Zitong%20Yu%20and%20Yonghong%20Hou%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20been%20applied%20in%20many%20computer%20vision%20tasks%0Aand%20achieved%20state-of-the-art%20%28SOTA%29%20performance.%20However%2C%20misclassification%0Awill%20occur%20when%20DNNs%20predict%20adversarial%20examples%20which%20are%20created%20by%20adding%0Ahuman-imperceptible%20adversarial%20noise%20to%20natural%20examples.%20This%20limits%20the%0Aapplication%20of%20DNN%20in%20security-critical%20fields.%20In%20order%20to%20enhance%20the%0Arobustness%20of%20models%2C%20previous%20research%20has%20primarily%20focused%20on%20the%20unimodal%0Adomain%2C%20such%20as%20image%20recognition%20and%20video%20understanding.%20Although%20multi-modal%0Alearning%20has%20achieved%20advanced%20performance%20in%20various%20tasks%2C%20such%20as%20action%0Arecognition%2C%20research%20on%20the%20robustness%20of%20RGB-skeleton%20action%20recognition%0Amodels%20is%20scarce.%20In%20this%20paper%2C%20we%20systematically%20investigate%20how%20to%20improve%0Athe%20robustness%20of%20RGB-skeleton%20action%20recognition%20models.%20We%20initially%0Aconducted%20empirical%20analysis%20on%20the%20robustness%20of%20different%20modalities%20and%0Aobserved%20that%20the%20skeleton%20modality%20is%20more%20robust%20than%20the%20RGB%20modality.%0AMotivated%20by%20this%20observation%2C%20we%20propose%20the%20%5Cformatword%7BA%7Dttention-based%0A%5Cformatword%7BM%7Dodality%20%5Cformatword%7BR%7Deweighter%20%28%5Cformatword%7BAMR%7D%29%2C%20which%0Autilizes%20an%20attention%20layer%20to%20re-weight%20the%20two%20modalities%2C%20enabling%20the%20model%0Ato%20learn%20more%20robust%20features.%20Our%20AMR%20is%20plug-and-play%2C%20allowing%20easy%0Aintegration%20with%20multimodal%20models.%20To%20demonstrate%20the%20effectiveness%20of%20AMR%2C%20we%0Aconducted%20extensive%20experiments%20on%20various%20datasets.%20For%20example%2C%20compared%20to%0Athe%20SOTA%20methods%2C%20AMR%20exhibits%20a%2043.77%5C%25%20improvement%20against%20PGD20%20attacks%20on%0Athe%20NTU-RGB%2BD%2060%20dataset.%20Furthermore%2C%20it%20effectively%20balances%20the%20differences%0Ain%20robustness%20between%20different%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Robustness%2520in%2520RGB-Skeleton%2520Action%2520Recognition%253A%2520Leveraging%250A%2520%2520Attention%2520Modality%2520Reweighter%26entry.906535625%3DChao%2520Liu%2520and%2520Xin%2520Liu%2520and%2520Zitong%2520Yu%2520and%2520Yonghong%2520Hou%2520and%2520Huanjing%2520Yue%2520and%2520Jingyu%2520Yang%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520been%2520applied%2520in%2520many%2520computer%2520vision%2520tasks%250Aand%2520achieved%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%2520However%252C%2520misclassification%250Awill%2520occur%2520when%2520DNNs%2520predict%2520adversarial%2520examples%2520which%2520are%2520created%2520by%2520adding%250Ahuman-imperceptible%2520adversarial%2520noise%2520to%2520natural%2520examples.%2520This%2520limits%2520the%250Aapplication%2520of%2520DNN%2520in%2520security-critical%2520fields.%2520In%2520order%2520to%2520enhance%2520the%250Arobustness%2520of%2520models%252C%2520previous%2520research%2520has%2520primarily%2520focused%2520on%2520the%2520unimodal%250Adomain%252C%2520such%2520as%2520image%2520recognition%2520and%2520video%2520understanding.%2520Although%2520multi-modal%250Alearning%2520has%2520achieved%2520advanced%2520performance%2520in%2520various%2520tasks%252C%2520such%2520as%2520action%250Arecognition%252C%2520research%2520on%2520the%2520robustness%2520of%2520RGB-skeleton%2520action%2520recognition%250Amodels%2520is%2520scarce.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520investigate%2520how%2520to%2520improve%250Athe%2520robustness%2520of%2520RGB-skeleton%2520action%2520recognition%2520models.%2520We%2520initially%250Aconducted%2520empirical%2520analysis%2520on%2520the%2520robustness%2520of%2520different%2520modalities%2520and%250Aobserved%2520that%2520the%2520skeleton%2520modality%2520is%2520more%2520robust%2520than%2520the%2520RGB%2520modality.%250AMotivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520the%2520%255Cformatword%257BA%257Dttention-based%250A%255Cformatword%257BM%257Dodality%2520%255Cformatword%257BR%257Deweighter%2520%2528%255Cformatword%257BAMR%257D%2529%252C%2520which%250Autilizes%2520an%2520attention%2520layer%2520to%2520re-weight%2520the%2520two%2520modalities%252C%2520enabling%2520the%2520model%250Ato%2520learn%2520more%2520robust%2520features.%2520Our%2520AMR%2520is%2520plug-and-play%252C%2520allowing%2520easy%250Aintegration%2520with%2520multimodal%2520models.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520AMR%252C%2520we%250Aconducted%2520extensive%2520experiments%2520on%2520various%2520datasets.%2520For%2520example%252C%2520compared%2520to%250Athe%2520SOTA%2520methods%252C%2520AMR%2520exhibits%2520a%252043.77%255C%2525%2520improvement%2520against%2520PGD20%2520attacks%2520on%250Athe%2520NTU-RGB%252BD%252060%2520dataset.%2520Furthermore%252C%2520it%2520effectively%2520balances%2520the%2520differences%250Ain%2520robustness%2520between%2520different%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustness%20in%20RGB-Skeleton%20Action%20Recognition%3A%20Leveraging%0A%20%20Attention%20Modality%20Reweighter&entry.906535625=Chao%20Liu%20and%20Xin%20Liu%20and%20Zitong%20Yu%20and%20Yonghong%20Hou%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20been%20applied%20in%20many%20computer%20vision%20tasks%0Aand%20achieved%20state-of-the-art%20%28SOTA%29%20performance.%20However%2C%20misclassification%0Awill%20occur%20when%20DNNs%20predict%20adversarial%20examples%20which%20are%20created%20by%20adding%0Ahuman-imperceptible%20adversarial%20noise%20to%20natural%20examples.%20This%20limits%20the%0Aapplication%20of%20DNN%20in%20security-critical%20fields.%20In%20order%20to%20enhance%20the%0Arobustness%20of%20models%2C%20previous%20research%20has%20primarily%20focused%20on%20the%20unimodal%0Adomain%2C%20such%20as%20image%20recognition%20and%20video%20understanding.%20Although%20multi-modal%0Alearning%20has%20achieved%20advanced%20performance%20in%20various%20tasks%2C%20such%20as%20action%0Arecognition%2C%20research%20on%20the%20robustness%20of%20RGB-skeleton%20action%20recognition%0Amodels%20is%20scarce.%20In%20this%20paper%2C%20we%20systematically%20investigate%20how%20to%20improve%0Athe%20robustness%20of%20RGB-skeleton%20action%20recognition%20models.%20We%20initially%0Aconducted%20empirical%20analysis%20on%20the%20robustness%20of%20different%20modalities%20and%0Aobserved%20that%20the%20skeleton%20modality%20is%20more%20robust%20than%20the%20RGB%20modality.%0AMotivated%20by%20this%20observation%2C%20we%20propose%20the%20%5Cformatword%7BA%7Dttention-based%0A%5Cformatword%7BM%7Dodality%20%5Cformatword%7BR%7Deweighter%20%28%5Cformatword%7BAMR%7D%29%2C%20which%0Autilizes%20an%20attention%20layer%20to%20re-weight%20the%20two%20modalities%2C%20enabling%20the%20model%0Ato%20learn%20more%20robust%20features.%20Our%20AMR%20is%20plug-and-play%2C%20allowing%20easy%0Aintegration%20with%20multimodal%20models.%20To%20demonstrate%20the%20effectiveness%20of%20AMR%2C%20we%0Aconducted%20extensive%20experiments%20on%20various%20datasets.%20For%20example%2C%20compared%20to%0Athe%20SOTA%20methods%2C%20AMR%20exhibits%20a%2043.77%5C%25%20improvement%20against%20PGD20%20attacks%20on%0Athe%20NTU-RGB%2BD%2060%20dataset.%20Furthermore%2C%20it%20effectively%20balances%20the%20differences%0Ain%20robustness%20between%20different%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19981v1&entry.124074799=Read"},
{"title": "Background Semantics Matter: Cross-Task Feature Exchange Network for\n  Clustered Infrared Small Target Detection With Sky-Annotated Dataset", "author": "Yimian Dai and Mengxuan Xiao and Yiming Zhu and Huan Wang and Kehua Guo and Jian Yang", "abstract": "  Infrared small target detection poses unique challenges due to the scarcity\nof intrinsic target features and the abundance of similar background\ndistractors. We argue that background semantics play a pivotal role in\ndistinguishing visually similar objects for this task. To address this, we\nintroduce a new task -- clustered infrared small target detection, and present\nDenseSIRST, a novel benchmark dataset that provides per-pixel semantic\nannotations for background regions, enabling the transition from sparse to\ndense target detection. Leveraging this dataset, we propose the\nBackground-Aware Feature Exchange Network (BAFE-Net), which transforms the\ndetection paradigm from a single task focused on the foreground to a multi-task\narchitecture that jointly performs target detection and background semantic\nsegmentation. BAFE-Net introduces a cross-task feature hard-exchange mechanism\nto embed target and background semantics between the two tasks. Furthermore, we\npropose the Background-Aware Gaussian Copy-Paste (BAG-CP) method, which\nselectively pastes small targets into sky regions during training, avoiding the\ncreation of false alarm targets in complex non-sky backgrounds. Extensive\nexperiments validate the effectiveness of BAG-CP and BAFE-Net in improving\ntarget detection accuracy while reducing false alarms. The DenseSIRST dataset,\ncode, and trained models are available at https://github.com/GrokCV/BAFE-Net.\n", "link": "http://arxiv.org/abs/2407.20078v1", "date": "2024-07-29", "relevancy": 2.1013, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5369}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5187}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Background%20Semantics%20Matter%3A%20Cross-Task%20Feature%20Exchange%20Network%20for%0A%20%20Clustered%20Infrared%20Small%20Target%20Detection%20With%20Sky-Annotated%20Dataset&body=Title%3A%20Background%20Semantics%20Matter%3A%20Cross-Task%20Feature%20Exchange%20Network%20for%0A%20%20Clustered%20Infrared%20Small%20Target%20Detection%20With%20Sky-Annotated%20Dataset%0AAuthor%3A%20Yimian%20Dai%20and%20Mengxuan%20Xiao%20and%20Yiming%20Zhu%20and%20Huan%20Wang%20and%20Kehua%20Guo%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Infrared%20small%20target%20detection%20poses%20unique%20challenges%20due%20to%20the%20scarcity%0Aof%20intrinsic%20target%20features%20and%20the%20abundance%20of%20similar%20background%0Adistractors.%20We%20argue%20that%20background%20semantics%20play%20a%20pivotal%20role%20in%0Adistinguishing%20visually%20similar%20objects%20for%20this%20task.%20To%20address%20this%2C%20we%0Aintroduce%20a%20new%20task%20--%20clustered%20infrared%20small%20target%20detection%2C%20and%20present%0ADenseSIRST%2C%20a%20novel%20benchmark%20dataset%20that%20provides%20per-pixel%20semantic%0Aannotations%20for%20background%20regions%2C%20enabling%20the%20transition%20from%20sparse%20to%0Adense%20target%20detection.%20Leveraging%20this%20dataset%2C%20we%20propose%20the%0ABackground-Aware%20Feature%20Exchange%20Network%20%28BAFE-Net%29%2C%20which%20transforms%20the%0Adetection%20paradigm%20from%20a%20single%20task%20focused%20on%20the%20foreground%20to%20a%20multi-task%0Aarchitecture%20that%20jointly%20performs%20target%20detection%20and%20background%20semantic%0Asegmentation.%20BAFE-Net%20introduces%20a%20cross-task%20feature%20hard-exchange%20mechanism%0Ato%20embed%20target%20and%20background%20semantics%20between%20the%20two%20tasks.%20Furthermore%2C%20we%0Apropose%20the%20Background-Aware%20Gaussian%20Copy-Paste%20%28BAG-CP%29%20method%2C%20which%0Aselectively%20pastes%20small%20targets%20into%20sky%20regions%20during%20training%2C%20avoiding%20the%0Acreation%20of%20false%20alarm%20targets%20in%20complex%20non-sky%20backgrounds.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20BAG-CP%20and%20BAFE-Net%20in%20improving%0Atarget%20detection%20accuracy%20while%20reducing%20false%20alarms.%20The%20DenseSIRST%20dataset%2C%0Acode%2C%20and%20trained%20models%20are%20available%20at%20https%3A//github.com/GrokCV/BAFE-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackground%2520Semantics%2520Matter%253A%2520Cross-Task%2520Feature%2520Exchange%2520Network%2520for%250A%2520%2520Clustered%2520Infrared%2520Small%2520Target%2520Detection%2520With%2520Sky-Annotated%2520Dataset%26entry.906535625%3DYimian%2520Dai%2520and%2520Mengxuan%2520Xiao%2520and%2520Yiming%2520Zhu%2520and%2520Huan%2520Wang%2520and%2520Kehua%2520Guo%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Infrared%2520small%2520target%2520detection%2520poses%2520unique%2520challenges%2520due%2520to%2520the%2520scarcity%250Aof%2520intrinsic%2520target%2520features%2520and%2520the%2520abundance%2520of%2520similar%2520background%250Adistractors.%2520We%2520argue%2520that%2520background%2520semantics%2520play%2520a%2520pivotal%2520role%2520in%250Adistinguishing%2520visually%2520similar%2520objects%2520for%2520this%2520task.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520new%2520task%2520--%2520clustered%2520infrared%2520small%2520target%2520detection%252C%2520and%2520present%250ADenseSIRST%252C%2520a%2520novel%2520benchmark%2520dataset%2520that%2520provides%2520per-pixel%2520semantic%250Aannotations%2520for%2520background%2520regions%252C%2520enabling%2520the%2520transition%2520from%2520sparse%2520to%250Adense%2520target%2520detection.%2520Leveraging%2520this%2520dataset%252C%2520we%2520propose%2520the%250ABackground-Aware%2520Feature%2520Exchange%2520Network%2520%2528BAFE-Net%2529%252C%2520which%2520transforms%2520the%250Adetection%2520paradigm%2520from%2520a%2520single%2520task%2520focused%2520on%2520the%2520foreground%2520to%2520a%2520multi-task%250Aarchitecture%2520that%2520jointly%2520performs%2520target%2520detection%2520and%2520background%2520semantic%250Asegmentation.%2520BAFE-Net%2520introduces%2520a%2520cross-task%2520feature%2520hard-exchange%2520mechanism%250Ato%2520embed%2520target%2520and%2520background%2520semantics%2520between%2520the%2520two%2520tasks.%2520Furthermore%252C%2520we%250Apropose%2520the%2520Background-Aware%2520Gaussian%2520Copy-Paste%2520%2528BAG-CP%2529%2520method%252C%2520which%250Aselectively%2520pastes%2520small%2520targets%2520into%2520sky%2520regions%2520during%2520training%252C%2520avoiding%2520the%250Acreation%2520of%2520false%2520alarm%2520targets%2520in%2520complex%2520non-sky%2520backgrounds.%2520Extensive%250Aexperiments%2520validate%2520the%2520effectiveness%2520of%2520BAG-CP%2520and%2520BAFE-Net%2520in%2520improving%250Atarget%2520detection%2520accuracy%2520while%2520reducing%2520false%2520alarms.%2520The%2520DenseSIRST%2520dataset%252C%250Acode%252C%2520and%2520trained%2520models%2520are%2520available%2520at%2520https%253A//github.com/GrokCV/BAFE-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Background%20Semantics%20Matter%3A%20Cross-Task%20Feature%20Exchange%20Network%20for%0A%20%20Clustered%20Infrared%20Small%20Target%20Detection%20With%20Sky-Annotated%20Dataset&entry.906535625=Yimian%20Dai%20and%20Mengxuan%20Xiao%20and%20Yiming%20Zhu%20and%20Huan%20Wang%20and%20Kehua%20Guo%20and%20Jian%20Yang&entry.1292438233=%20%20Infrared%20small%20target%20detection%20poses%20unique%20challenges%20due%20to%20the%20scarcity%0Aof%20intrinsic%20target%20features%20and%20the%20abundance%20of%20similar%20background%0Adistractors.%20We%20argue%20that%20background%20semantics%20play%20a%20pivotal%20role%20in%0Adistinguishing%20visually%20similar%20objects%20for%20this%20task.%20To%20address%20this%2C%20we%0Aintroduce%20a%20new%20task%20--%20clustered%20infrared%20small%20target%20detection%2C%20and%20present%0ADenseSIRST%2C%20a%20novel%20benchmark%20dataset%20that%20provides%20per-pixel%20semantic%0Aannotations%20for%20background%20regions%2C%20enabling%20the%20transition%20from%20sparse%20to%0Adense%20target%20detection.%20Leveraging%20this%20dataset%2C%20we%20propose%20the%0ABackground-Aware%20Feature%20Exchange%20Network%20%28BAFE-Net%29%2C%20which%20transforms%20the%0Adetection%20paradigm%20from%20a%20single%20task%20focused%20on%20the%20foreground%20to%20a%20multi-task%0Aarchitecture%20that%20jointly%20performs%20target%20detection%20and%20background%20semantic%0Asegmentation.%20BAFE-Net%20introduces%20a%20cross-task%20feature%20hard-exchange%20mechanism%0Ato%20embed%20target%20and%20background%20semantics%20between%20the%20two%20tasks.%20Furthermore%2C%20we%0Apropose%20the%20Background-Aware%20Gaussian%20Copy-Paste%20%28BAG-CP%29%20method%2C%20which%0Aselectively%20pastes%20small%20targets%20into%20sky%20regions%20during%20training%2C%20avoiding%20the%0Acreation%20of%20false%20alarm%20targets%20in%20complex%20non-sky%20backgrounds.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20BAG-CP%20and%20BAFE-Net%20in%20improving%0Atarget%20detection%20accuracy%20while%20reducing%20false%20alarms.%20The%20DenseSIRST%20dataset%2C%0Acode%2C%20and%20trained%20models%20are%20available%20at%20https%3A//github.com/GrokCV/BAFE-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20078v1&entry.124074799=Read"},
{"title": "Classification of Alzheimer's Dementia vs. Healthy subjects by studying\n  structural disparities in fMRI Time-Series of DMN", "author": "Sneha Noble and Chakka Sai Pradeep and Neelam Sinha and Thomas Gregor Issac", "abstract": "  Time series from different regions of interest (ROI) of default mode network\n(DMN) from Functional Magnetic Resonance Imaging (fMRI) can reveal significant\ndifferences between healthy and unhealthy people. Here, we propose the utility\nof an existing metric quantifying the lack/presence of structure in a signal\ncalled, \"deviation from stochasticity\" (DS) measure to characterize\nresting-state fMRI time series. The hypothesis is that differences in the level\nof structure in the time series can lead to discrimination between the subject\ngroups. In this work, an autoencoder-based model is utilized to learn efficient\nrepresentations of data by training the network to reconstruct its input data.\nThe proposed methodology is applied on fMRI time series of 50 healthy\nindividuals and 50 subjects with Alzheimer's Disease (AD), obtained from\npublicly available ADNI database. DS measure for healthy fMRI as expected turns\nout to be different compared to that of AD. Peak classification accuracy of 95%\nwas obtained using Gradient Boosting classifier, using the DS measure applied\non 100 subjects.\n", "link": "http://arxiv.org/abs/2407.19990v1", "date": "2024-07-29", "relevancy": 2.0995, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4368}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4123}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Alzheimer%27s%20Dementia%20vs.%20Healthy%20subjects%20by%20studying%0A%20%20structural%20disparities%20in%20fMRI%20Time-Series%20of%20DMN&body=Title%3A%20Classification%20of%20Alzheimer%27s%20Dementia%20vs.%20Healthy%20subjects%20by%20studying%0A%20%20structural%20disparities%20in%20fMRI%20Time-Series%20of%20DMN%0AAuthor%3A%20Sneha%20Noble%20and%20Chakka%20Sai%20Pradeep%20and%20Neelam%20Sinha%20and%20Thomas%20Gregor%20Issac%0AAbstract%3A%20%20%20Time%20series%20from%20different%20regions%20of%20interest%20%28ROI%29%20of%20default%20mode%20network%0A%28DMN%29%20from%20Functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20can%20reveal%20significant%0Adifferences%20between%20healthy%20and%20unhealthy%20people.%20Here%2C%20we%20propose%20the%20utility%0Aof%20an%20existing%20metric%20quantifying%20the%20lack/presence%20of%20structure%20in%20a%20signal%0Acalled%2C%20%22deviation%20from%20stochasticity%22%20%28DS%29%20measure%20to%20characterize%0Aresting-state%20fMRI%20time%20series.%20The%20hypothesis%20is%20that%20differences%20in%20the%20level%0Aof%20structure%20in%20the%20time%20series%20can%20lead%20to%20discrimination%20between%20the%20subject%0Agroups.%20In%20this%20work%2C%20an%20autoencoder-based%20model%20is%20utilized%20to%20learn%20efficient%0Arepresentations%20of%20data%20by%20training%20the%20network%20to%20reconstruct%20its%20input%20data.%0AThe%20proposed%20methodology%20is%20applied%20on%20fMRI%20time%20series%20of%2050%20healthy%0Aindividuals%20and%2050%20subjects%20with%20Alzheimer%27s%20Disease%20%28AD%29%2C%20obtained%20from%0Apublicly%20available%20ADNI%20database.%20DS%20measure%20for%20healthy%20fMRI%20as%20expected%20turns%0Aout%20to%20be%20different%20compared%20to%20that%20of%20AD.%20Peak%20classification%20accuracy%20of%2095%25%0Awas%20obtained%20using%20Gradient%20Boosting%20classifier%2C%20using%20the%20DS%20measure%20applied%0Aon%20100%20subjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Alzheimer%2527s%2520Dementia%2520vs.%2520Healthy%2520subjects%2520by%2520studying%250A%2520%2520structural%2520disparities%2520in%2520fMRI%2520Time-Series%2520of%2520DMN%26entry.906535625%3DSneha%2520Noble%2520and%2520Chakka%2520Sai%2520Pradeep%2520and%2520Neelam%2520Sinha%2520and%2520Thomas%2520Gregor%2520Issac%26entry.1292438233%3D%2520%2520Time%2520series%2520from%2520different%2520regions%2520of%2520interest%2520%2528ROI%2529%2520of%2520default%2520mode%2520network%250A%2528DMN%2529%2520from%2520Functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%2520can%2520reveal%2520significant%250Adifferences%2520between%2520healthy%2520and%2520unhealthy%2520people.%2520Here%252C%2520we%2520propose%2520the%2520utility%250Aof%2520an%2520existing%2520metric%2520quantifying%2520the%2520lack/presence%2520of%2520structure%2520in%2520a%2520signal%250Acalled%252C%2520%2522deviation%2520from%2520stochasticity%2522%2520%2528DS%2529%2520measure%2520to%2520characterize%250Aresting-state%2520fMRI%2520time%2520series.%2520The%2520hypothesis%2520is%2520that%2520differences%2520in%2520the%2520level%250Aof%2520structure%2520in%2520the%2520time%2520series%2520can%2520lead%2520to%2520discrimination%2520between%2520the%2520subject%250Agroups.%2520In%2520this%2520work%252C%2520an%2520autoencoder-based%2520model%2520is%2520utilized%2520to%2520learn%2520efficient%250Arepresentations%2520of%2520data%2520by%2520training%2520the%2520network%2520to%2520reconstruct%2520its%2520input%2520data.%250AThe%2520proposed%2520methodology%2520is%2520applied%2520on%2520fMRI%2520time%2520series%2520of%252050%2520healthy%250Aindividuals%2520and%252050%2520subjects%2520with%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%252C%2520obtained%2520from%250Apublicly%2520available%2520ADNI%2520database.%2520DS%2520measure%2520for%2520healthy%2520fMRI%2520as%2520expected%2520turns%250Aout%2520to%2520be%2520different%2520compared%2520to%2520that%2520of%2520AD.%2520Peak%2520classification%2520accuracy%2520of%252095%2525%250Awas%2520obtained%2520using%2520Gradient%2520Boosting%2520classifier%252C%2520using%2520the%2520DS%2520measure%2520applied%250Aon%2520100%2520subjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Alzheimer%27s%20Dementia%20vs.%20Healthy%20subjects%20by%20studying%0A%20%20structural%20disparities%20in%20fMRI%20Time-Series%20of%20DMN&entry.906535625=Sneha%20Noble%20and%20Chakka%20Sai%20Pradeep%20and%20Neelam%20Sinha%20and%20Thomas%20Gregor%20Issac&entry.1292438233=%20%20Time%20series%20from%20different%20regions%20of%20interest%20%28ROI%29%20of%20default%20mode%20network%0A%28DMN%29%20from%20Functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20can%20reveal%20significant%0Adifferences%20between%20healthy%20and%20unhealthy%20people.%20Here%2C%20we%20propose%20the%20utility%0Aof%20an%20existing%20metric%20quantifying%20the%20lack/presence%20of%20structure%20in%20a%20signal%0Acalled%2C%20%22deviation%20from%20stochasticity%22%20%28DS%29%20measure%20to%20characterize%0Aresting-state%20fMRI%20time%20series.%20The%20hypothesis%20is%20that%20differences%20in%20the%20level%0Aof%20structure%20in%20the%20time%20series%20can%20lead%20to%20discrimination%20between%20the%20subject%0Agroups.%20In%20this%20work%2C%20an%20autoencoder-based%20model%20is%20utilized%20to%20learn%20efficient%0Arepresentations%20of%20data%20by%20training%20the%20network%20to%20reconstruct%20its%20input%20data.%0AThe%20proposed%20methodology%20is%20applied%20on%20fMRI%20time%20series%20of%2050%20healthy%0Aindividuals%20and%2050%20subjects%20with%20Alzheimer%27s%20Disease%20%28AD%29%2C%20obtained%20from%0Apublicly%20available%20ADNI%20database.%20DS%20measure%20for%20healthy%20fMRI%20as%20expected%20turns%0Aout%20to%20be%20different%20compared%20to%20that%20of%20AD.%20Peak%20classification%20accuracy%20of%2095%25%0Awas%20obtained%20using%20Gradient%20Boosting%20classifier%2C%20using%20the%20DS%20measure%20applied%0Aon%20100%20subjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19990v1&entry.124074799=Read"},
{"title": "Online Multi-Source Domain Adaptation through Gaussian Mixtures and\n  Dataset Dictionary Learning", "author": "Eduardo Fernandes Montesuma and Stevan Le Stanc and Fred Ngol\u00e8 Mboula", "abstract": "  This paper addresses the challenge of online multi-source domain adaptation\n(MSDA) in transfer learning, a scenario where one needs to adapt multiple,\nheterogeneous source domains towards a target domain that comes in a stream. We\nintroduce a novel approach for the online fit of a Gaussian Mixture Model\n(GMM), based on the Wasserstein geometry of Gaussian measures. We build upon\nthis method and recent developments in dataset dictionary learning for\nproposing a novel strategy in online MSDA. Experiments on the challenging\nTennessee Eastman Process benchmark demonstrate that our approach is able to\nadapt \\emph{on the fly} to the stream of target domain data. Furthermore, our\nonline GMM serves as a memory, representing the whole stream of data.\n", "link": "http://arxiv.org/abs/2407.19853v1", "date": "2024-07-29", "relevancy": 2.0911, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5362}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.521}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Multi-Source%20Domain%20Adaptation%20through%20Gaussian%20Mixtures%20and%0A%20%20Dataset%20Dictionary%20Learning&body=Title%3A%20Online%20Multi-Source%20Domain%20Adaptation%20through%20Gaussian%20Mixtures%20and%0A%20%20Dataset%20Dictionary%20Learning%0AAuthor%3A%20Eduardo%20Fernandes%20Montesuma%20and%20Stevan%20Le%20Stanc%20and%20Fred%20Ngol%C3%A8%20Mboula%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20online%20multi-source%20domain%20adaptation%0A%28MSDA%29%20in%20transfer%20learning%2C%20a%20scenario%20where%20one%20needs%20to%20adapt%20multiple%2C%0Aheterogeneous%20source%20domains%20towards%20a%20target%20domain%20that%20comes%20in%20a%20stream.%20We%0Aintroduce%20a%20novel%20approach%20for%20the%20online%20fit%20of%20a%20Gaussian%20Mixture%20Model%0A%28GMM%29%2C%20based%20on%20the%20Wasserstein%20geometry%20of%20Gaussian%20measures.%20We%20build%20upon%0Athis%20method%20and%20recent%20developments%20in%20dataset%20dictionary%20learning%20for%0Aproposing%20a%20novel%20strategy%20in%20online%20MSDA.%20Experiments%20on%20the%20challenging%0ATennessee%20Eastman%20Process%20benchmark%20demonstrate%20that%20our%20approach%20is%20able%20to%0Aadapt%20%5Cemph%7Bon%20the%20fly%7D%20to%20the%20stream%20of%20target%20domain%20data.%20Furthermore%2C%20our%0Aonline%20GMM%20serves%20as%20a%20memory%2C%20representing%20the%20whole%20stream%20of%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Multi-Source%2520Domain%2520Adaptation%2520through%2520Gaussian%2520Mixtures%2520and%250A%2520%2520Dataset%2520Dictionary%2520Learning%26entry.906535625%3DEduardo%2520Fernandes%2520Montesuma%2520and%2520Stevan%2520Le%2520Stanc%2520and%2520Fred%2520Ngol%25C3%25A8%2520Mboula%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520online%2520multi-source%2520domain%2520adaptation%250A%2528MSDA%2529%2520in%2520transfer%2520learning%252C%2520a%2520scenario%2520where%2520one%2520needs%2520to%2520adapt%2520multiple%252C%250Aheterogeneous%2520source%2520domains%2520towards%2520a%2520target%2520domain%2520that%2520comes%2520in%2520a%2520stream.%2520We%250Aintroduce%2520a%2520novel%2520approach%2520for%2520the%2520online%2520fit%2520of%2520a%2520Gaussian%2520Mixture%2520Model%250A%2528GMM%2529%252C%2520based%2520on%2520the%2520Wasserstein%2520geometry%2520of%2520Gaussian%2520measures.%2520We%2520build%2520upon%250Athis%2520method%2520and%2520recent%2520developments%2520in%2520dataset%2520dictionary%2520learning%2520for%250Aproposing%2520a%2520novel%2520strategy%2520in%2520online%2520MSDA.%2520Experiments%2520on%2520the%2520challenging%250ATennessee%2520Eastman%2520Process%2520benchmark%2520demonstrate%2520that%2520our%2520approach%2520is%2520able%2520to%250Aadapt%2520%255Cemph%257Bon%2520the%2520fly%257D%2520to%2520the%2520stream%2520of%2520target%2520domain%2520data.%2520Furthermore%252C%2520our%250Aonline%2520GMM%2520serves%2520as%2520a%2520memory%252C%2520representing%2520the%2520whole%2520stream%2520of%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Multi-Source%20Domain%20Adaptation%20through%20Gaussian%20Mixtures%20and%0A%20%20Dataset%20Dictionary%20Learning&entry.906535625=Eduardo%20Fernandes%20Montesuma%20and%20Stevan%20Le%20Stanc%20and%20Fred%20Ngol%C3%A8%20Mboula&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20online%20multi-source%20domain%20adaptation%0A%28MSDA%29%20in%20transfer%20learning%2C%20a%20scenario%20where%20one%20needs%20to%20adapt%20multiple%2C%0Aheterogeneous%20source%20domains%20towards%20a%20target%20domain%20that%20comes%20in%20a%20stream.%20We%0Aintroduce%20a%20novel%20approach%20for%20the%20online%20fit%20of%20a%20Gaussian%20Mixture%20Model%0A%28GMM%29%2C%20based%20on%20the%20Wasserstein%20geometry%20of%20Gaussian%20measures.%20We%20build%20upon%0Athis%20method%20and%20recent%20developments%20in%20dataset%20dictionary%20learning%20for%0Aproposing%20a%20novel%20strategy%20in%20online%20MSDA.%20Experiments%20on%20the%20challenging%0ATennessee%20Eastman%20Process%20benchmark%20demonstrate%20that%20our%20approach%20is%20able%20to%0Aadapt%20%5Cemph%7Bon%20the%20fly%7D%20to%20the%20stream%20of%20target%20domain%20data.%20Furthermore%2C%20our%0Aonline%20GMM%20serves%20as%20a%20memory%2C%20representing%20the%20whole%20stream%20of%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19853v1&entry.124074799=Read"},
{"title": "Practical and Reproducible Symbolic Music Generation by Large Language\n  Models with Structural Embeddings", "author": "Seungyeon Rhyu and Kichang Yang and Sungjun Cho and Jaehyeon Kim and Kyogu Lee and Moontae Lee", "abstract": "  Music generation introduces challenging complexities to large language\nmodels. Symbolic structures of music often include vertical harmonization as\nwell as horizontal counterpoint, urging various adaptations and enhancements\nfor large-scale Transformers. However, existing works share three major\ndrawbacks: 1) their tokenization requires domain-specific annotations, such as\nbars and beats, that are typically missing in raw MIDI data; 2) the pure impact\nof enhancing token embedding methods is hardly examined without domain-specific\nannotations; and 3) existing works to overcome the aforementioned drawbacks,\nsuch as MuseNet, lack reproducibility. To tackle such limitations, we develop a\nMIDI-based music generation framework inspired by MuseNet, empirically studying\ntwo structural embeddings that do not rely on domain-specific annotations. We\nprovide various metrics and insights that can guide suitable encoding to\ndeploy. We also verify that multiple embedding configurations can selectively\nboost certain musical aspects. By providing open-source implementations via\nHuggingFace, our findings shed light on leveraging large language models toward\npractical and reproducible music generation.\n", "link": "http://arxiv.org/abs/2407.19900v1", "date": "2024-07-29", "relevancy": 2.0678, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5312}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5238}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Practical%20and%20Reproducible%20Symbolic%20Music%20Generation%20by%20Large%20Language%0A%20%20Models%20with%20Structural%20Embeddings&body=Title%3A%20Practical%20and%20Reproducible%20Symbolic%20Music%20Generation%20by%20Large%20Language%0A%20%20Models%20with%20Structural%20Embeddings%0AAuthor%3A%20Seungyeon%20Rhyu%20and%20Kichang%20Yang%20and%20Sungjun%20Cho%20and%20Jaehyeon%20Kim%20and%20Kyogu%20Lee%20and%20Moontae%20Lee%0AAbstract%3A%20%20%20Music%20generation%20introduces%20challenging%20complexities%20to%20large%20language%0Amodels.%20Symbolic%20structures%20of%20music%20often%20include%20vertical%20harmonization%20as%0Awell%20as%20horizontal%20counterpoint%2C%20urging%20various%20adaptations%20and%20enhancements%0Afor%20large-scale%20Transformers.%20However%2C%20existing%20works%20share%20three%20major%0Adrawbacks%3A%201%29%20their%20tokenization%20requires%20domain-specific%20annotations%2C%20such%20as%0Abars%20and%20beats%2C%20that%20are%20typically%20missing%20in%20raw%20MIDI%20data%3B%202%29%20the%20pure%20impact%0Aof%20enhancing%20token%20embedding%20methods%20is%20hardly%20examined%20without%20domain-specific%0Aannotations%3B%20and%203%29%20existing%20works%20to%20overcome%20the%20aforementioned%20drawbacks%2C%0Asuch%20as%20MuseNet%2C%20lack%20reproducibility.%20To%20tackle%20such%20limitations%2C%20we%20develop%20a%0AMIDI-based%20music%20generation%20framework%20inspired%20by%20MuseNet%2C%20empirically%20studying%0Atwo%20structural%20embeddings%20that%20do%20not%20rely%20on%20domain-specific%20annotations.%20We%0Aprovide%20various%20metrics%20and%20insights%20that%20can%20guide%20suitable%20encoding%20to%0Adeploy.%20We%20also%20verify%20that%20multiple%20embedding%20configurations%20can%20selectively%0Aboost%20certain%20musical%20aspects.%20By%20providing%20open-source%20implementations%20via%0AHuggingFace%2C%20our%20findings%20shed%20light%20on%20leveraging%20large%20language%20models%20toward%0Apractical%20and%20reproducible%20music%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPractical%2520and%2520Reproducible%2520Symbolic%2520Music%2520Generation%2520by%2520Large%2520Language%250A%2520%2520Models%2520with%2520Structural%2520Embeddings%26entry.906535625%3DSeungyeon%2520Rhyu%2520and%2520Kichang%2520Yang%2520and%2520Sungjun%2520Cho%2520and%2520Jaehyeon%2520Kim%2520and%2520Kyogu%2520Lee%2520and%2520Moontae%2520Lee%26entry.1292438233%3D%2520%2520Music%2520generation%2520introduces%2520challenging%2520complexities%2520to%2520large%2520language%250Amodels.%2520Symbolic%2520structures%2520of%2520music%2520often%2520include%2520vertical%2520harmonization%2520as%250Awell%2520as%2520horizontal%2520counterpoint%252C%2520urging%2520various%2520adaptations%2520and%2520enhancements%250Afor%2520large-scale%2520Transformers.%2520However%252C%2520existing%2520works%2520share%2520three%2520major%250Adrawbacks%253A%25201%2529%2520their%2520tokenization%2520requires%2520domain-specific%2520annotations%252C%2520such%2520as%250Abars%2520and%2520beats%252C%2520that%2520are%2520typically%2520missing%2520in%2520raw%2520MIDI%2520data%253B%25202%2529%2520the%2520pure%2520impact%250Aof%2520enhancing%2520token%2520embedding%2520methods%2520is%2520hardly%2520examined%2520without%2520domain-specific%250Aannotations%253B%2520and%25203%2529%2520existing%2520works%2520to%2520overcome%2520the%2520aforementioned%2520drawbacks%252C%250Asuch%2520as%2520MuseNet%252C%2520lack%2520reproducibility.%2520To%2520tackle%2520such%2520limitations%252C%2520we%2520develop%2520a%250AMIDI-based%2520music%2520generation%2520framework%2520inspired%2520by%2520MuseNet%252C%2520empirically%2520studying%250Atwo%2520structural%2520embeddings%2520that%2520do%2520not%2520rely%2520on%2520domain-specific%2520annotations.%2520We%250Aprovide%2520various%2520metrics%2520and%2520insights%2520that%2520can%2520guide%2520suitable%2520encoding%2520to%250Adeploy.%2520We%2520also%2520verify%2520that%2520multiple%2520embedding%2520configurations%2520can%2520selectively%250Aboost%2520certain%2520musical%2520aspects.%2520By%2520providing%2520open-source%2520implementations%2520via%250AHuggingFace%252C%2520our%2520findings%2520shed%2520light%2520on%2520leveraging%2520large%2520language%2520models%2520toward%250Apractical%2520and%2520reproducible%2520music%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Practical%20and%20Reproducible%20Symbolic%20Music%20Generation%20by%20Large%20Language%0A%20%20Models%20with%20Structural%20Embeddings&entry.906535625=Seungyeon%20Rhyu%20and%20Kichang%20Yang%20and%20Sungjun%20Cho%20and%20Jaehyeon%20Kim%20and%20Kyogu%20Lee%20and%20Moontae%20Lee&entry.1292438233=%20%20Music%20generation%20introduces%20challenging%20complexities%20to%20large%20language%0Amodels.%20Symbolic%20structures%20of%20music%20often%20include%20vertical%20harmonization%20as%0Awell%20as%20horizontal%20counterpoint%2C%20urging%20various%20adaptations%20and%20enhancements%0Afor%20large-scale%20Transformers.%20However%2C%20existing%20works%20share%20three%20major%0Adrawbacks%3A%201%29%20their%20tokenization%20requires%20domain-specific%20annotations%2C%20such%20as%0Abars%20and%20beats%2C%20that%20are%20typically%20missing%20in%20raw%20MIDI%20data%3B%202%29%20the%20pure%20impact%0Aof%20enhancing%20token%20embedding%20methods%20is%20hardly%20examined%20without%20domain-specific%0Aannotations%3B%20and%203%29%20existing%20works%20to%20overcome%20the%20aforementioned%20drawbacks%2C%0Asuch%20as%20MuseNet%2C%20lack%20reproducibility.%20To%20tackle%20such%20limitations%2C%20we%20develop%20a%0AMIDI-based%20music%20generation%20framework%20inspired%20by%20MuseNet%2C%20empirically%20studying%0Atwo%20structural%20embeddings%20that%20do%20not%20rely%20on%20domain-specific%20annotations.%20We%0Aprovide%20various%20metrics%20and%20insights%20that%20can%20guide%20suitable%20encoding%20to%0Adeploy.%20We%20also%20verify%20that%20multiple%20embedding%20configurations%20can%20selectively%0Aboost%20certain%20musical%20aspects.%20By%20providing%20open-source%20implementations%20via%0AHuggingFace%2C%20our%20findings%20shed%20light%20on%20leveraging%20large%20language%20models%20toward%0Apractical%20and%20reproducible%20music%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19900v1&entry.124074799=Read"},
{"title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning", "author": "Jinghuan Shang and Karl Schmeckpeper and Brandon B. May and Maria Vittoria Minniti and Tarik Kelestemur and David Watkins and Laura Herlant", "abstract": "  Vision-based robot policy learning, which maps visual inputs to actions,\nnecessitates a holistic understanding of diverse visual tasks beyond\nsingle-task needs like classification or segmentation. Inspired by this, we\nintroduce Theia, a vision foundation model for robot learning that distills\nmultiple off-the-shelf vision foundation models trained on varied vision tasks.\nTheia's rich visual representations encode diverse visual knowledge, enhancing\ndownstream robot learning. Extensive experiments demonstrate that Theia\noutperforms its teacher models and prior robot learning models using less\ntraining data and smaller model sizes. Additionally, we quantify the quality of\npre-trained visual representations and hypothesize that higher entropy in\nfeature norm distributions leads to improved robot learning performance. Code\nand models are available at https://github.com/bdaiinstitute/theia.\n", "link": "http://arxiv.org/abs/2407.20179v1", "date": "2024-07-29", "relevancy": 2.0672, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5562}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5149}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theia%3A%20Distilling%20Diverse%20Vision%20Foundation%20Models%20for%20Robot%20Learning&body=Title%3A%20Theia%3A%20Distilling%20Diverse%20Vision%20Foundation%20Models%20for%20Robot%20Learning%0AAuthor%3A%20Jinghuan%20Shang%20and%20Karl%20Schmeckpeper%20and%20Brandon%20B.%20May%20and%20Maria%20Vittoria%20Minniti%20and%20Tarik%20Kelestemur%20and%20David%20Watkins%20and%20Laura%20Herlant%0AAbstract%3A%20%20%20Vision-based%20robot%20policy%20learning%2C%20which%20maps%20visual%20inputs%20to%20actions%2C%0Anecessitates%20a%20holistic%20understanding%20of%20diverse%20visual%20tasks%20beyond%0Asingle-task%20needs%20like%20classification%20or%20segmentation.%20Inspired%20by%20this%2C%20we%0Aintroduce%20Theia%2C%20a%20vision%20foundation%20model%20for%20robot%20learning%20that%20distills%0Amultiple%20off-the-shelf%20vision%20foundation%20models%20trained%20on%20varied%20vision%20tasks.%0ATheia%27s%20rich%20visual%20representations%20encode%20diverse%20visual%20knowledge%2C%20enhancing%0Adownstream%20robot%20learning.%20Extensive%20experiments%20demonstrate%20that%20Theia%0Aoutperforms%20its%20teacher%20models%20and%20prior%20robot%20learning%20models%20using%20less%0Atraining%20data%20and%20smaller%20model%20sizes.%20Additionally%2C%20we%20quantify%20the%20quality%20of%0Apre-trained%20visual%20representations%20and%20hypothesize%20that%20higher%20entropy%20in%0Afeature%20norm%20distributions%20leads%20to%20improved%20robot%20learning%20performance.%20Code%0Aand%20models%20are%20available%20at%20https%3A//github.com/bdaiinstitute/theia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheia%253A%2520Distilling%2520Diverse%2520Vision%2520Foundation%2520Models%2520for%2520Robot%2520Learning%26entry.906535625%3DJinghuan%2520Shang%2520and%2520Karl%2520Schmeckpeper%2520and%2520Brandon%2520B.%2520May%2520and%2520Maria%2520Vittoria%2520Minniti%2520and%2520Tarik%2520Kelestemur%2520and%2520David%2520Watkins%2520and%2520Laura%2520Herlant%26entry.1292438233%3D%2520%2520Vision-based%2520robot%2520policy%2520learning%252C%2520which%2520maps%2520visual%2520inputs%2520to%2520actions%252C%250Anecessitates%2520a%2520holistic%2520understanding%2520of%2520diverse%2520visual%2520tasks%2520beyond%250Asingle-task%2520needs%2520like%2520classification%2520or%2520segmentation.%2520Inspired%2520by%2520this%252C%2520we%250Aintroduce%2520Theia%252C%2520a%2520vision%2520foundation%2520model%2520for%2520robot%2520learning%2520that%2520distills%250Amultiple%2520off-the-shelf%2520vision%2520foundation%2520models%2520trained%2520on%2520varied%2520vision%2520tasks.%250ATheia%2527s%2520rich%2520visual%2520representations%2520encode%2520diverse%2520visual%2520knowledge%252C%2520enhancing%250Adownstream%2520robot%2520learning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Theia%250Aoutperforms%2520its%2520teacher%2520models%2520and%2520prior%2520robot%2520learning%2520models%2520using%2520less%250Atraining%2520data%2520and%2520smaller%2520model%2520sizes.%2520Additionally%252C%2520we%2520quantify%2520the%2520quality%2520of%250Apre-trained%2520visual%2520representations%2520and%2520hypothesize%2520that%2520higher%2520entropy%2520in%250Afeature%2520norm%2520distributions%2520leads%2520to%2520improved%2520robot%2520learning%2520performance.%2520Code%250Aand%2520models%2520are%2520available%2520at%2520https%253A//github.com/bdaiinstitute/theia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theia%3A%20Distilling%20Diverse%20Vision%20Foundation%20Models%20for%20Robot%20Learning&entry.906535625=Jinghuan%20Shang%20and%20Karl%20Schmeckpeper%20and%20Brandon%20B.%20May%20and%20Maria%20Vittoria%20Minniti%20and%20Tarik%20Kelestemur%20and%20David%20Watkins%20and%20Laura%20Herlant&entry.1292438233=%20%20Vision-based%20robot%20policy%20learning%2C%20which%20maps%20visual%20inputs%20to%20actions%2C%0Anecessitates%20a%20holistic%20understanding%20of%20diverse%20visual%20tasks%20beyond%0Asingle-task%20needs%20like%20classification%20or%20segmentation.%20Inspired%20by%20this%2C%20we%0Aintroduce%20Theia%2C%20a%20vision%20foundation%20model%20for%20robot%20learning%20that%20distills%0Amultiple%20off-the-shelf%20vision%20foundation%20models%20trained%20on%20varied%20vision%20tasks.%0ATheia%27s%20rich%20visual%20representations%20encode%20diverse%20visual%20knowledge%2C%20enhancing%0Adownstream%20robot%20learning.%20Extensive%20experiments%20demonstrate%20that%20Theia%0Aoutperforms%20its%20teacher%20models%20and%20prior%20robot%20learning%20models%20using%20less%0Atraining%20data%20and%20smaller%20model%20sizes.%20Additionally%2C%20we%20quantify%20the%20quality%20of%0Apre-trained%20visual%20representations%20and%20hypothesize%20that%20higher%20entropy%20in%0Afeature%20norm%20distributions%20leads%20to%20improved%20robot%20learning%20performance.%20Code%0Aand%20models%20are%20available%20at%20https%3A//github.com/bdaiinstitute/theia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20179v1&entry.124074799=Read"},
{"title": "UniTTA: Unified Benchmark and Versatile Framework Towards Realistic\n  Test-Time Adaptation", "author": "Chaoqun Du and Yulin Wang and Jiayi Guo and Yizeng Han and Jie Zhou and Gao Huang", "abstract": "  Test-Time Adaptation (TTA) aims to adapt pre-trained models to the target\ndomain during testing. In reality, this adaptability can be influenced by\nmultiple factors. Researchers have identified various challenging scenarios and\ndeveloped diverse methods to address these challenges, such as dealing with\ncontinual domain shifts, mixed domains, and temporally correlated or imbalanced\nclass distributions. Despite these efforts, a unified and comprehensive\nbenchmark has yet to be established. To this end, we propose a Unified\nTest-Time Adaptation (UniTTA) benchmark, which is comprehensive and widely\napplicable. Each scenario within the benchmark is fully described by a Markov\nstate transition matrix for sampling from the original dataset. The UniTTA\nbenchmark considers both domain and class as two independent dimensions of data\nand addresses various combinations of imbalance/balance and\ni.i.d./non-i.i.d./continual conditions, covering a total of \\( (2 \\times 3)^2 =\n36 \\) scenarios. It establishes a comprehensive evaluation benchmark for\nrealistic TTA and provides a guideline for practitioners to select the most\nsuitable TTA method. Alongside this benchmark, we propose a versatile UniTTA\nframework, which includes a Balanced Domain Normalization (BDN) layer and a\nCOrrelated Feature Adaptation (COFA) method--designed to mitigate distribution\ngaps in domain and class, respectively. Extensive experiments demonstrate that\nour UniTTA framework excels within the UniTTA benchmark and achieves\nstate-of-the-art performance on average. Our code is available at\n\\url{https://github.com/LeapLabTHU/UniTTA}.\n", "link": "http://arxiv.org/abs/2407.20080v1", "date": "2024-07-29", "relevancy": 2.0566, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5322}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5081}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTTA%3A%20Unified%20Benchmark%20and%20Versatile%20Framework%20Towards%20Realistic%0A%20%20Test-Time%20Adaptation&body=Title%3A%20UniTTA%3A%20Unified%20Benchmark%20and%20Versatile%20Framework%20Towards%20Realistic%0A%20%20Test-Time%20Adaptation%0AAuthor%3A%20Chaoqun%20Du%20and%20Yulin%20Wang%20and%20Jiayi%20Guo%20and%20Yizeng%20Han%20and%20Jie%20Zhou%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Test-Time%20Adaptation%20%28TTA%29%20aims%20to%20adapt%20pre-trained%20models%20to%20the%20target%0Adomain%20during%20testing.%20In%20reality%2C%20this%20adaptability%20can%20be%20influenced%20by%0Amultiple%20factors.%20Researchers%20have%20identified%20various%20challenging%20scenarios%20and%0Adeveloped%20diverse%20methods%20to%20address%20these%20challenges%2C%20such%20as%20dealing%20with%0Acontinual%20domain%20shifts%2C%20mixed%20domains%2C%20and%20temporally%20correlated%20or%20imbalanced%0Aclass%20distributions.%20Despite%20these%20efforts%2C%20a%20unified%20and%20comprehensive%0Abenchmark%20has%20yet%20to%20be%20established.%20To%20this%20end%2C%20we%20propose%20a%20Unified%0ATest-Time%20Adaptation%20%28UniTTA%29%20benchmark%2C%20which%20is%20comprehensive%20and%20widely%0Aapplicable.%20Each%20scenario%20within%20the%20benchmark%20is%20fully%20described%20by%20a%20Markov%0Astate%20transition%20matrix%20for%20sampling%20from%20the%20original%20dataset.%20The%20UniTTA%0Abenchmark%20considers%20both%20domain%20and%20class%20as%20two%20independent%20dimensions%20of%20data%0Aand%20addresses%20various%20combinations%20of%20imbalance/balance%20and%0Ai.i.d./non-i.i.d./continual%20conditions%2C%20covering%20a%20total%20of%20%5C%28%20%282%20%5Ctimes%203%29%5E2%20%3D%0A36%20%5C%29%20scenarios.%20It%20establishes%20a%20comprehensive%20evaluation%20benchmark%20for%0Arealistic%20TTA%20and%20provides%20a%20guideline%20for%20practitioners%20to%20select%20the%20most%0Asuitable%20TTA%20method.%20Alongside%20this%20benchmark%2C%20we%20propose%20a%20versatile%20UniTTA%0Aframework%2C%20which%20includes%20a%20Balanced%20Domain%20Normalization%20%28BDN%29%20layer%20and%20a%0ACOrrelated%20Feature%20Adaptation%20%28COFA%29%20method--designed%20to%20mitigate%20distribution%0Agaps%20in%20domain%20and%20class%2C%20respectively.%20Extensive%20experiments%20demonstrate%20that%0Aour%20UniTTA%20framework%20excels%20within%20the%20UniTTA%20benchmark%20and%20achieves%0Astate-of-the-art%20performance%20on%20average.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/LeapLabTHU/UniTTA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTTA%253A%2520Unified%2520Benchmark%2520and%2520Versatile%2520Framework%2520Towards%2520Realistic%250A%2520%2520Test-Time%2520Adaptation%26entry.906535625%3DChaoqun%2520Du%2520and%2520Yulin%2520Wang%2520and%2520Jiayi%2520Guo%2520and%2520Yizeng%2520Han%2520and%2520Jie%2520Zhou%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520aims%2520to%2520adapt%2520pre-trained%2520models%2520to%2520the%2520target%250Adomain%2520during%2520testing.%2520In%2520reality%252C%2520this%2520adaptability%2520can%2520be%2520influenced%2520by%250Amultiple%2520factors.%2520Researchers%2520have%2520identified%2520various%2520challenging%2520scenarios%2520and%250Adeveloped%2520diverse%2520methods%2520to%2520address%2520these%2520challenges%252C%2520such%2520as%2520dealing%2520with%250Acontinual%2520domain%2520shifts%252C%2520mixed%2520domains%252C%2520and%2520temporally%2520correlated%2520or%2520imbalanced%250Aclass%2520distributions.%2520Despite%2520these%2520efforts%252C%2520a%2520unified%2520and%2520comprehensive%250Abenchmark%2520has%2520yet%2520to%2520be%2520established.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520Unified%250ATest-Time%2520Adaptation%2520%2528UniTTA%2529%2520benchmark%252C%2520which%2520is%2520comprehensive%2520and%2520widely%250Aapplicable.%2520Each%2520scenario%2520within%2520the%2520benchmark%2520is%2520fully%2520described%2520by%2520a%2520Markov%250Astate%2520transition%2520matrix%2520for%2520sampling%2520from%2520the%2520original%2520dataset.%2520The%2520UniTTA%250Abenchmark%2520considers%2520both%2520domain%2520and%2520class%2520as%2520two%2520independent%2520dimensions%2520of%2520data%250Aand%2520addresses%2520various%2520combinations%2520of%2520imbalance/balance%2520and%250Ai.i.d./non-i.i.d./continual%2520conditions%252C%2520covering%2520a%2520total%2520of%2520%255C%2528%2520%25282%2520%255Ctimes%25203%2529%255E2%2520%253D%250A36%2520%255C%2529%2520scenarios.%2520It%2520establishes%2520a%2520comprehensive%2520evaluation%2520benchmark%2520for%250Arealistic%2520TTA%2520and%2520provides%2520a%2520guideline%2520for%2520practitioners%2520to%2520select%2520the%2520most%250Asuitable%2520TTA%2520method.%2520Alongside%2520this%2520benchmark%252C%2520we%2520propose%2520a%2520versatile%2520UniTTA%250Aframework%252C%2520which%2520includes%2520a%2520Balanced%2520Domain%2520Normalization%2520%2528BDN%2529%2520layer%2520and%2520a%250ACOrrelated%2520Feature%2520Adaptation%2520%2528COFA%2529%2520method--designed%2520to%2520mitigate%2520distribution%250Agaps%2520in%2520domain%2520and%2520class%252C%2520respectively.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520UniTTA%2520framework%2520excels%2520within%2520the%2520UniTTA%2520benchmark%2520and%2520achieves%250Astate-of-the-art%2520performance%2520on%2520average.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/LeapLabTHU/UniTTA%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTTA%3A%20Unified%20Benchmark%20and%20Versatile%20Framework%20Towards%20Realistic%0A%20%20Test-Time%20Adaptation&entry.906535625=Chaoqun%20Du%20and%20Yulin%20Wang%20and%20Jiayi%20Guo%20and%20Yizeng%20Han%20and%20Jie%20Zhou%20and%20Gao%20Huang&entry.1292438233=%20%20Test-Time%20Adaptation%20%28TTA%29%20aims%20to%20adapt%20pre-trained%20models%20to%20the%20target%0Adomain%20during%20testing.%20In%20reality%2C%20this%20adaptability%20can%20be%20influenced%20by%0Amultiple%20factors.%20Researchers%20have%20identified%20various%20challenging%20scenarios%20and%0Adeveloped%20diverse%20methods%20to%20address%20these%20challenges%2C%20such%20as%20dealing%20with%0Acontinual%20domain%20shifts%2C%20mixed%20domains%2C%20and%20temporally%20correlated%20or%20imbalanced%0Aclass%20distributions.%20Despite%20these%20efforts%2C%20a%20unified%20and%20comprehensive%0Abenchmark%20has%20yet%20to%20be%20established.%20To%20this%20end%2C%20we%20propose%20a%20Unified%0ATest-Time%20Adaptation%20%28UniTTA%29%20benchmark%2C%20which%20is%20comprehensive%20and%20widely%0Aapplicable.%20Each%20scenario%20within%20the%20benchmark%20is%20fully%20described%20by%20a%20Markov%0Astate%20transition%20matrix%20for%20sampling%20from%20the%20original%20dataset.%20The%20UniTTA%0Abenchmark%20considers%20both%20domain%20and%20class%20as%20two%20independent%20dimensions%20of%20data%0Aand%20addresses%20various%20combinations%20of%20imbalance/balance%20and%0Ai.i.d./non-i.i.d./continual%20conditions%2C%20covering%20a%20total%20of%20%5C%28%20%282%20%5Ctimes%203%29%5E2%20%3D%0A36%20%5C%29%20scenarios.%20It%20establishes%20a%20comprehensive%20evaluation%20benchmark%20for%0Arealistic%20TTA%20and%20provides%20a%20guideline%20for%20practitioners%20to%20select%20the%20most%0Asuitable%20TTA%20method.%20Alongside%20this%20benchmark%2C%20we%20propose%20a%20versatile%20UniTTA%0Aframework%2C%20which%20includes%20a%20Balanced%20Domain%20Normalization%20%28BDN%29%20layer%20and%20a%0ACOrrelated%20Feature%20Adaptation%20%28COFA%29%20method--designed%20to%20mitigate%20distribution%0Agaps%20in%20domain%20and%20class%2C%20respectively.%20Extensive%20experiments%20demonstrate%20that%0Aour%20UniTTA%20framework%20excels%20within%20the%20UniTTA%20benchmark%20and%20achieves%0Astate-of-the-art%20performance%20on%20average.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/LeapLabTHU/UniTTA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20080v1&entry.124074799=Read"},
{"title": "Aero-Nef: Neural Fields for Rapid Aircraft Aerodynamics Simulations", "author": "Giovanni Catalani and Siddhant Agarwal and Xavier Bertrand and Frederic Tost and Michael Bauerheim and Joseph Morlier", "abstract": "  This paper presents a methodology to learn surrogate models of steady state\nfluid dynamics simulations on meshed domains, based on Implicit Neural\nRepresentations (INRs). The proposed models can be applied directly to\nunstructured domains for different flow conditions, handle non-parametric 3D\ngeometric variations, and generalize to unseen shapes at test time. The\ncoordinate-based formulation naturally leads to robustness with respect to\ndiscretization, allowing an excellent trade-off between computational cost\n(memory footprint and training time) and accuracy. The method is demonstrated\non two industrially relevant applications: a RANS dataset of the\ntwo-dimensional compressible flow over a transonic airfoil and a dataset of the\nsurface pressure distribution over 3D wings, including shape, inflow condition,\nand control surface deflection variations. On the considered test cases, our\napproach achieves a more than three times lower test error and significantly\nimproves generalization error on unseen geometries compared to state-of-the-art\nGraph Neural Network architectures. Remarkably, the method can perform\ninference five order of magnitude faster than the high fidelity solver on the\nRANS transonic airfoil dataset. Code is available at\nhttps://gitlab.isae-supaero.fr/gi.catalani/aero-nepf\n", "link": "http://arxiv.org/abs/2407.19916v1", "date": "2024-07-29", "relevancy": 2.0539, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.518}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5135}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aero-Nef%3A%20Neural%20Fields%20for%20Rapid%20Aircraft%20Aerodynamics%20Simulations&body=Title%3A%20Aero-Nef%3A%20Neural%20Fields%20for%20Rapid%20Aircraft%20Aerodynamics%20Simulations%0AAuthor%3A%20Giovanni%20Catalani%20and%20Siddhant%20Agarwal%20and%20Xavier%20Bertrand%20and%20Frederic%20Tost%20and%20Michael%20Bauerheim%20and%20Joseph%20Morlier%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20methodology%20to%20learn%20surrogate%20models%20of%20steady%20state%0Afluid%20dynamics%20simulations%20on%20meshed%20domains%2C%20based%20on%20Implicit%20Neural%0ARepresentations%20%28INRs%29.%20The%20proposed%20models%20can%20be%20applied%20directly%20to%0Aunstructured%20domains%20for%20different%20flow%20conditions%2C%20handle%20non-parametric%203D%0Ageometric%20variations%2C%20and%20generalize%20to%20unseen%20shapes%20at%20test%20time.%20The%0Acoordinate-based%20formulation%20naturally%20leads%20to%20robustness%20with%20respect%20to%0Adiscretization%2C%20allowing%20an%20excellent%20trade-off%20between%20computational%20cost%0A%28memory%20footprint%20and%20training%20time%29%20and%20accuracy.%20The%20method%20is%20demonstrated%0Aon%20two%20industrially%20relevant%20applications%3A%20a%20RANS%20dataset%20of%20the%0Atwo-dimensional%20compressible%20flow%20over%20a%20transonic%20airfoil%20and%20a%20dataset%20of%20the%0Asurface%20pressure%20distribution%20over%203D%20wings%2C%20including%20shape%2C%20inflow%20condition%2C%0Aand%20control%20surface%20deflection%20variations.%20On%20the%20considered%20test%20cases%2C%20our%0Aapproach%20achieves%20a%20more%20than%20three%20times%20lower%20test%20error%20and%20significantly%0Aimproves%20generalization%20error%20on%20unseen%20geometries%20compared%20to%20state-of-the-art%0AGraph%20Neural%20Network%20architectures.%20Remarkably%2C%20the%20method%20can%20perform%0Ainference%20five%20order%20of%20magnitude%20faster%20than%20the%20high%20fidelity%20solver%20on%20the%0ARANS%20transonic%20airfoil%20dataset.%20Code%20is%20available%20at%0Ahttps%3A//gitlab.isae-supaero.fr/gi.catalani/aero-nepf%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAero-Nef%253A%2520Neural%2520Fields%2520for%2520Rapid%2520Aircraft%2520Aerodynamics%2520Simulations%26entry.906535625%3DGiovanni%2520Catalani%2520and%2520Siddhant%2520Agarwal%2520and%2520Xavier%2520Bertrand%2520and%2520Frederic%2520Tost%2520and%2520Michael%2520Bauerheim%2520and%2520Joseph%2520Morlier%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520methodology%2520to%2520learn%2520surrogate%2520models%2520of%2520steady%2520state%250Afluid%2520dynamics%2520simulations%2520on%2520meshed%2520domains%252C%2520based%2520on%2520Implicit%2520Neural%250ARepresentations%2520%2528INRs%2529.%2520The%2520proposed%2520models%2520can%2520be%2520applied%2520directly%2520to%250Aunstructured%2520domains%2520for%2520different%2520flow%2520conditions%252C%2520handle%2520non-parametric%25203D%250Ageometric%2520variations%252C%2520and%2520generalize%2520to%2520unseen%2520shapes%2520at%2520test%2520time.%2520The%250Acoordinate-based%2520formulation%2520naturally%2520leads%2520to%2520robustness%2520with%2520respect%2520to%250Adiscretization%252C%2520allowing%2520an%2520excellent%2520trade-off%2520between%2520computational%2520cost%250A%2528memory%2520footprint%2520and%2520training%2520time%2529%2520and%2520accuracy.%2520The%2520method%2520is%2520demonstrated%250Aon%2520two%2520industrially%2520relevant%2520applications%253A%2520a%2520RANS%2520dataset%2520of%2520the%250Atwo-dimensional%2520compressible%2520flow%2520over%2520a%2520transonic%2520airfoil%2520and%2520a%2520dataset%2520of%2520the%250Asurface%2520pressure%2520distribution%2520over%25203D%2520wings%252C%2520including%2520shape%252C%2520inflow%2520condition%252C%250Aand%2520control%2520surface%2520deflection%2520variations.%2520On%2520the%2520considered%2520test%2520cases%252C%2520our%250Aapproach%2520achieves%2520a%2520more%2520than%2520three%2520times%2520lower%2520test%2520error%2520and%2520significantly%250Aimproves%2520generalization%2520error%2520on%2520unseen%2520geometries%2520compared%2520to%2520state-of-the-art%250AGraph%2520Neural%2520Network%2520architectures.%2520Remarkably%252C%2520the%2520method%2520can%2520perform%250Ainference%2520five%2520order%2520of%2520magnitude%2520faster%2520than%2520the%2520high%2520fidelity%2520solver%2520on%2520the%250ARANS%2520transonic%2520airfoil%2520dataset.%2520Code%2520is%2520available%2520at%250Ahttps%253A//gitlab.isae-supaero.fr/gi.catalani/aero-nepf%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aero-Nef%3A%20Neural%20Fields%20for%20Rapid%20Aircraft%20Aerodynamics%20Simulations&entry.906535625=Giovanni%20Catalani%20and%20Siddhant%20Agarwal%20and%20Xavier%20Bertrand%20and%20Frederic%20Tost%20and%20Michael%20Bauerheim%20and%20Joseph%20Morlier&entry.1292438233=%20%20This%20paper%20presents%20a%20methodology%20to%20learn%20surrogate%20models%20of%20steady%20state%0Afluid%20dynamics%20simulations%20on%20meshed%20domains%2C%20based%20on%20Implicit%20Neural%0ARepresentations%20%28INRs%29.%20The%20proposed%20models%20can%20be%20applied%20directly%20to%0Aunstructured%20domains%20for%20different%20flow%20conditions%2C%20handle%20non-parametric%203D%0Ageometric%20variations%2C%20and%20generalize%20to%20unseen%20shapes%20at%20test%20time.%20The%0Acoordinate-based%20formulation%20naturally%20leads%20to%20robustness%20with%20respect%20to%0Adiscretization%2C%20allowing%20an%20excellent%20trade-off%20between%20computational%20cost%0A%28memory%20footprint%20and%20training%20time%29%20and%20accuracy.%20The%20method%20is%20demonstrated%0Aon%20two%20industrially%20relevant%20applications%3A%20a%20RANS%20dataset%20of%20the%0Atwo-dimensional%20compressible%20flow%20over%20a%20transonic%20airfoil%20and%20a%20dataset%20of%20the%0Asurface%20pressure%20distribution%20over%203D%20wings%2C%20including%20shape%2C%20inflow%20condition%2C%0Aand%20control%20surface%20deflection%20variations.%20On%20the%20considered%20test%20cases%2C%20our%0Aapproach%20achieves%20a%20more%20than%20three%20times%20lower%20test%20error%20and%20significantly%0Aimproves%20generalization%20error%20on%20unseen%20geometries%20compared%20to%20state-of-the-art%0AGraph%20Neural%20Network%20architectures.%20Remarkably%2C%20the%20method%20can%20perform%0Ainference%20five%20order%20of%20magnitude%20faster%20than%20the%20high%20fidelity%20solver%20on%20the%0ARANS%20transonic%20airfoil%20dataset.%20Code%20is%20available%20at%0Ahttps%3A//gitlab.isae-supaero.fr/gi.catalani/aero-nepf%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19916v1&entry.124074799=Read"},
{"title": "SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow\n  Prediction", "author": "\u00c7a\u011fhan K\u00f6ksal and Ghazal Ghazaei and Felix Holm and Azade Farshad and Nassir Navab", "abstract": "  Graph-based holistic scene representations facilitate surgical workflow\nunderstanding and have recently demonstrated significant success. However, this\ntask is often hindered by the limited availability of densely annotated\nsurgical scene data. In this work, we introduce an end-to-end framework for the\ngeneration and optimization of surgical scene graphs on a downstream task. Our\napproach leverages the flexibility of graph-based spectral clustering and the\ngeneralization capability of foundation models to generate unsupervised scene\ngraphs with learnable properties. We reinforce the initial spatial graph with\nsparse temporal connections using local matches between consecutive frames to\npredict temporally consistent clusters across a temporal neighborhood. By\njointly optimizing the spatiotemporal relations and node features of the\ndynamic scene graph with the downstream task of phase segmentation, we address\nthe costly and annotation-burdensome task of semantic scene comprehension and\nscene graph generation in surgical videos using only weak surgical phase\nlabels. Further, by incorporating effective intermediate scene representation\ndisentanglement steps within the pipeline, our solution outperforms the SOTA on\nthe CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflow\nrecognition\n", "link": "http://arxiv.org/abs/2407.20214v1", "date": "2024-07-29", "relevancy": 2.0533, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5589}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5118}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SANGRIA%3A%20Surgical%20Video%20Scene%20Graph%20Optimization%20for%20Surgical%20Workflow%0A%20%20Prediction&body=Title%3A%20SANGRIA%3A%20Surgical%20Video%20Scene%20Graph%20Optimization%20for%20Surgical%20Workflow%0A%20%20Prediction%0AAuthor%3A%20%C3%87a%C4%9Fhan%20K%C3%B6ksal%20and%20Ghazal%20Ghazaei%20and%20Felix%20Holm%20and%20Azade%20Farshad%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20Graph-based%20holistic%20scene%20representations%20facilitate%20surgical%20workflow%0Aunderstanding%20and%20have%20recently%20demonstrated%20significant%20success.%20However%2C%20this%0Atask%20is%20often%20hindered%20by%20the%20limited%20availability%20of%20densely%20annotated%0Asurgical%20scene%20data.%20In%20this%20work%2C%20we%20introduce%20an%20end-to-end%20framework%20for%20the%0Ageneration%20and%20optimization%20of%20surgical%20scene%20graphs%20on%20a%20downstream%20task.%20Our%0Aapproach%20leverages%20the%20flexibility%20of%20graph-based%20spectral%20clustering%20and%20the%0Ageneralization%20capability%20of%20foundation%20models%20to%20generate%20unsupervised%20scene%0Agraphs%20with%20learnable%20properties.%20We%20reinforce%20the%20initial%20spatial%20graph%20with%0Asparse%20temporal%20connections%20using%20local%20matches%20between%20consecutive%20frames%20to%0Apredict%20temporally%20consistent%20clusters%20across%20a%20temporal%20neighborhood.%20By%0Ajointly%20optimizing%20the%20spatiotemporal%20relations%20and%20node%20features%20of%20the%0Adynamic%20scene%20graph%20with%20the%20downstream%20task%20of%20phase%20segmentation%2C%20we%20address%0Athe%20costly%20and%20annotation-burdensome%20task%20of%20semantic%20scene%20comprehension%20and%0Ascene%20graph%20generation%20in%20surgical%20videos%20using%20only%20weak%20surgical%20phase%0Alabels.%20Further%2C%20by%20incorporating%20effective%20intermediate%20scene%20representation%0Adisentanglement%20steps%20within%20the%20pipeline%2C%20our%20solution%20outperforms%20the%20SOTA%20on%0Athe%20CATARACTS%20dataset%20by%208%25%20accuracy%20and%2010%25%20F1%20score%20in%20surgical%20workflow%0Arecognition%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSANGRIA%253A%2520Surgical%2520Video%2520Scene%2520Graph%2520Optimization%2520for%2520Surgical%2520Workflow%250A%2520%2520Prediction%26entry.906535625%3D%25C3%2587a%25C4%259Fhan%2520K%25C3%25B6ksal%2520and%2520Ghazal%2520Ghazaei%2520and%2520Felix%2520Holm%2520and%2520Azade%2520Farshad%2520and%2520Nassir%2520Navab%26entry.1292438233%3D%2520%2520Graph-based%2520holistic%2520scene%2520representations%2520facilitate%2520surgical%2520workflow%250Aunderstanding%2520and%2520have%2520recently%2520demonstrated%2520significant%2520success.%2520However%252C%2520this%250Atask%2520is%2520often%2520hindered%2520by%2520the%2520limited%2520availability%2520of%2520densely%2520annotated%250Asurgical%2520scene%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520end-to-end%2520framework%2520for%2520the%250Ageneration%2520and%2520optimization%2520of%2520surgical%2520scene%2520graphs%2520on%2520a%2520downstream%2520task.%2520Our%250Aapproach%2520leverages%2520the%2520flexibility%2520of%2520graph-based%2520spectral%2520clustering%2520and%2520the%250Ageneralization%2520capability%2520of%2520foundation%2520models%2520to%2520generate%2520unsupervised%2520scene%250Agraphs%2520with%2520learnable%2520properties.%2520We%2520reinforce%2520the%2520initial%2520spatial%2520graph%2520with%250Asparse%2520temporal%2520connections%2520using%2520local%2520matches%2520between%2520consecutive%2520frames%2520to%250Apredict%2520temporally%2520consistent%2520clusters%2520across%2520a%2520temporal%2520neighborhood.%2520By%250Ajointly%2520optimizing%2520the%2520spatiotemporal%2520relations%2520and%2520node%2520features%2520of%2520the%250Adynamic%2520scene%2520graph%2520with%2520the%2520downstream%2520task%2520of%2520phase%2520segmentation%252C%2520we%2520address%250Athe%2520costly%2520and%2520annotation-burdensome%2520task%2520of%2520semantic%2520scene%2520comprehension%2520and%250Ascene%2520graph%2520generation%2520in%2520surgical%2520videos%2520using%2520only%2520weak%2520surgical%2520phase%250Alabels.%2520Further%252C%2520by%2520incorporating%2520effective%2520intermediate%2520scene%2520representation%250Adisentanglement%2520steps%2520within%2520the%2520pipeline%252C%2520our%2520solution%2520outperforms%2520the%2520SOTA%2520on%250Athe%2520CATARACTS%2520dataset%2520by%25208%2525%2520accuracy%2520and%252010%2525%2520F1%2520score%2520in%2520surgical%2520workflow%250Arecognition%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SANGRIA%3A%20Surgical%20Video%20Scene%20Graph%20Optimization%20for%20Surgical%20Workflow%0A%20%20Prediction&entry.906535625=%C3%87a%C4%9Fhan%20K%C3%B6ksal%20and%20Ghazal%20Ghazaei%20and%20Felix%20Holm%20and%20Azade%20Farshad%20and%20Nassir%20Navab&entry.1292438233=%20%20Graph-based%20holistic%20scene%20representations%20facilitate%20surgical%20workflow%0Aunderstanding%20and%20have%20recently%20demonstrated%20significant%20success.%20However%2C%20this%0Atask%20is%20often%20hindered%20by%20the%20limited%20availability%20of%20densely%20annotated%0Asurgical%20scene%20data.%20In%20this%20work%2C%20we%20introduce%20an%20end-to-end%20framework%20for%20the%0Ageneration%20and%20optimization%20of%20surgical%20scene%20graphs%20on%20a%20downstream%20task.%20Our%0Aapproach%20leverages%20the%20flexibility%20of%20graph-based%20spectral%20clustering%20and%20the%0Ageneralization%20capability%20of%20foundation%20models%20to%20generate%20unsupervised%20scene%0Agraphs%20with%20learnable%20properties.%20We%20reinforce%20the%20initial%20spatial%20graph%20with%0Asparse%20temporal%20connections%20using%20local%20matches%20between%20consecutive%20frames%20to%0Apredict%20temporally%20consistent%20clusters%20across%20a%20temporal%20neighborhood.%20By%0Ajointly%20optimizing%20the%20spatiotemporal%20relations%20and%20node%20features%20of%20the%0Adynamic%20scene%20graph%20with%20the%20downstream%20task%20of%20phase%20segmentation%2C%20we%20address%0Athe%20costly%20and%20annotation-burdensome%20task%20of%20semantic%20scene%20comprehension%20and%0Ascene%20graph%20generation%20in%20surgical%20videos%20using%20only%20weak%20surgical%20phase%0Alabels.%20Further%2C%20by%20incorporating%20effective%20intermediate%20scene%20representation%0Adisentanglement%20steps%20within%20the%20pipeline%2C%20our%20solution%20outperforms%20the%20SOTA%20on%0Athe%20CATARACTS%20dataset%20by%208%25%20accuracy%20and%2010%25%20F1%20score%20in%20surgical%20workflow%0Arecognition%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20214v1&entry.124074799=Read"},
{"title": "SpaER: Learning Spatio-temporal Equivariant Representations for Fetal\n  Brain Motion Tracking", "author": "Jian Wang and Razieh Faghihpirayesh and Polina Golland and Ali Ghoulipour", "abstract": "  In this paper, we introduce SpaER, a pioneering method for fetal motion\ntracking that leverages equivariant filters and self-attention mechanisms to\neffectively learn spatio-temporal representations. Different from conventional\napproaches that statically estimate fetal brain motions from pairs of images,\nour method dynamically tracks the rigid movement patterns of the fetal head\nacross temporal and spatial dimensions. Specifically, we first develop an\nequivariant neural network that efficiently learns rigid motion sequences\nthrough low-dimensional spatial representations of images. Subsequently, we\nlearn spatio-temporal representations by incorporating time encoding and\nself-attention neural network layers. This approach allows for the capture of\nlong-term dependencies of fetal brain motion and addresses alignment errors due\nto contrast changes and severe motion artifacts. Our model also provides a\ngeometric deformation estimation that properly addresses image distortions\namong all time frames. To the best of our knowledge, our approach is the first\nto learn spatial-temporal representations via deep neural networks for fetal\nmotion tracking without data augmentation. We validated our model using real\nfetal echo-planar images with simulated and real motions. Our method carries\nsignificant potential value in accurately measuring, tracking, and correcting\nfetal motion in fetal MRI sequences.\n", "link": "http://arxiv.org/abs/2407.20198v1", "date": "2024-07-29", "relevancy": 2.0429, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5155}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5122}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking&body=Title%3A%20SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking%0AAuthor%3A%20Jian%20Wang%20and%20Razieh%20Faghihpirayesh%20and%20Polina%20Golland%20and%20Ali%20Ghoulipour%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20SpaER%2C%20a%20pioneering%20method%20for%20fetal%20motion%0Atracking%20that%20leverages%20equivariant%20filters%20and%20self-attention%20mechanisms%20to%0Aeffectively%20learn%20spatio-temporal%20representations.%20Different%20from%20conventional%0Aapproaches%20that%20statically%20estimate%20fetal%20brain%20motions%20from%20pairs%20of%20images%2C%0Aour%20method%20dynamically%20tracks%20the%20rigid%20movement%20patterns%20of%20the%20fetal%20head%0Aacross%20temporal%20and%20spatial%20dimensions.%20Specifically%2C%20we%20first%20develop%20an%0Aequivariant%20neural%20network%20that%20efficiently%20learns%20rigid%20motion%20sequences%0Athrough%20low-dimensional%20spatial%20representations%20of%20images.%20Subsequently%2C%20we%0Alearn%20spatio-temporal%20representations%20by%20incorporating%20time%20encoding%20and%0Aself-attention%20neural%20network%20layers.%20This%20approach%20allows%20for%20the%20capture%20of%0Along-term%20dependencies%20of%20fetal%20brain%20motion%20and%20addresses%20alignment%20errors%20due%0Ato%20contrast%20changes%20and%20severe%20motion%20artifacts.%20Our%20model%20also%20provides%20a%0Ageometric%20deformation%20estimation%20that%20properly%20addresses%20image%20distortions%0Aamong%20all%20time%20frames.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20approach%20is%20the%20first%0Ato%20learn%20spatial-temporal%20representations%20via%20deep%20neural%20networks%20for%20fetal%0Amotion%20tracking%20without%20data%20augmentation.%20We%20validated%20our%20model%20using%20real%0Afetal%20echo-planar%20images%20with%20simulated%20and%20real%20motions.%20Our%20method%20carries%0Asignificant%20potential%20value%20in%20accurately%20measuring%2C%20tracking%2C%20and%20correcting%0Afetal%20motion%20in%20fetal%20MRI%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaER%253A%2520Learning%2520Spatio-temporal%2520Equivariant%2520Representations%2520for%2520Fetal%250A%2520%2520Brain%2520Motion%2520Tracking%26entry.906535625%3DJian%2520Wang%2520and%2520Razieh%2520Faghihpirayesh%2520and%2520Polina%2520Golland%2520and%2520Ali%2520Ghoulipour%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SpaER%252C%2520a%2520pioneering%2520method%2520for%2520fetal%2520motion%250Atracking%2520that%2520leverages%2520equivariant%2520filters%2520and%2520self-attention%2520mechanisms%2520to%250Aeffectively%2520learn%2520spatio-temporal%2520representations.%2520Different%2520from%2520conventional%250Aapproaches%2520that%2520statically%2520estimate%2520fetal%2520brain%2520motions%2520from%2520pairs%2520of%2520images%252C%250Aour%2520method%2520dynamically%2520tracks%2520the%2520rigid%2520movement%2520patterns%2520of%2520the%2520fetal%2520head%250Aacross%2520temporal%2520and%2520spatial%2520dimensions.%2520Specifically%252C%2520we%2520first%2520develop%2520an%250Aequivariant%2520neural%2520network%2520that%2520efficiently%2520learns%2520rigid%2520motion%2520sequences%250Athrough%2520low-dimensional%2520spatial%2520representations%2520of%2520images.%2520Subsequently%252C%2520we%250Alearn%2520spatio-temporal%2520representations%2520by%2520incorporating%2520time%2520encoding%2520and%250Aself-attention%2520neural%2520network%2520layers.%2520This%2520approach%2520allows%2520for%2520the%2520capture%2520of%250Along-term%2520dependencies%2520of%2520fetal%2520brain%2520motion%2520and%2520addresses%2520alignment%2520errors%2520due%250Ato%2520contrast%2520changes%2520and%2520severe%2520motion%2520artifacts.%2520Our%2520model%2520also%2520provides%2520a%250Ageometric%2520deformation%2520estimation%2520that%2520properly%2520addresses%2520image%2520distortions%250Aamong%2520all%2520time%2520frames.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520approach%2520is%2520the%2520first%250Ato%2520learn%2520spatial-temporal%2520representations%2520via%2520deep%2520neural%2520networks%2520for%2520fetal%250Amotion%2520tracking%2520without%2520data%2520augmentation.%2520We%2520validated%2520our%2520model%2520using%2520real%250Afetal%2520echo-planar%2520images%2520with%2520simulated%2520and%2520real%2520motions.%2520Our%2520method%2520carries%250Asignificant%2520potential%2520value%2520in%2520accurately%2520measuring%252C%2520tracking%252C%2520and%2520correcting%250Afetal%2520motion%2520in%2520fetal%2520MRI%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaER%3A%20Learning%20Spatio-temporal%20Equivariant%20Representations%20for%20Fetal%0A%20%20Brain%20Motion%20Tracking&entry.906535625=Jian%20Wang%20and%20Razieh%20Faghihpirayesh%20and%20Polina%20Golland%20and%20Ali%20Ghoulipour&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20SpaER%2C%20a%20pioneering%20method%20for%20fetal%20motion%0Atracking%20that%20leverages%20equivariant%20filters%20and%20self-attention%20mechanisms%20to%0Aeffectively%20learn%20spatio-temporal%20representations.%20Different%20from%20conventional%0Aapproaches%20that%20statically%20estimate%20fetal%20brain%20motions%20from%20pairs%20of%20images%2C%0Aour%20method%20dynamically%20tracks%20the%20rigid%20movement%20patterns%20of%20the%20fetal%20head%0Aacross%20temporal%20and%20spatial%20dimensions.%20Specifically%2C%20we%20first%20develop%20an%0Aequivariant%20neural%20network%20that%20efficiently%20learns%20rigid%20motion%20sequences%0Athrough%20low-dimensional%20spatial%20representations%20of%20images.%20Subsequently%2C%20we%0Alearn%20spatio-temporal%20representations%20by%20incorporating%20time%20encoding%20and%0Aself-attention%20neural%20network%20layers.%20This%20approach%20allows%20for%20the%20capture%20of%0Along-term%20dependencies%20of%20fetal%20brain%20motion%20and%20addresses%20alignment%20errors%20due%0Ato%20contrast%20changes%20and%20severe%20motion%20artifacts.%20Our%20model%20also%20provides%20a%0Ageometric%20deformation%20estimation%20that%20properly%20addresses%20image%20distortions%0Aamong%20all%20time%20frames.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20approach%20is%20the%20first%0Ato%20learn%20spatial-temporal%20representations%20via%20deep%20neural%20networks%20for%20fetal%0Amotion%20tracking%20without%20data%20augmentation.%20We%20validated%20our%20model%20using%20real%0Afetal%20echo-planar%20images%20with%20simulated%20and%20real%20motions.%20Our%20method%20carries%0Asignificant%20potential%20value%20in%20accurately%20measuring%2C%20tracking%2C%20and%20correcting%0Afetal%20motion%20in%20fetal%20MRI%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20198v1&entry.124074799=Read"},
{"title": "Infrared Small Target Detection based on Adjustable Sensitivity Strategy\n  and Multi-Scale Fusion", "author": "Jinmiao Zhao and Zelin Shi and Chuang Yu and Yunpeng Liu", "abstract": "  Recently, deep learning-based single-frame infrared small target (SIRST)\ndetection technology has made significant progress. However, existing infrared\nsmall target detection methods are often optimized for a fixed image\nresolution, a single wavelength, or a specific imaging system, limiting their\nbreadth and flexibility in practical applications. Therefore, we propose a\nrefined infrared small target detection scheme based on an adjustable\nsensitivity (AS) strategy and multi-scale fusion. Specifically, a multi-scale\nmodel fusion framework based on multi-scale direction-aware network (MSDA-Net)\nis constructed, which uses input images of multiple scales to train multiple\nmodels and fuses them. Multi-scale fusion helps characterize the shape, edge,\nand texture features of the target from different scales, making the model more\naccurate and reliable in locating the target. At the same time, we fully\nconsider the characteristics of the infrared small target detection task and\nconstruct an edge enhancement difficulty mining (EEDM) loss. The EEDM loss\nhelps alleviate the problem of category imbalance and guides the network to pay\nmore attention to difficult target areas and edge features during training. In\naddition, we propose an adjustable sensitivity strategy for post-processing.\nThis strategy significantly improves the detection rate of infrared small\ntargets while ensuring segmentation accuracy. Extensive experimental results\nshow that the proposed scheme achieves the best performance. Notably, this\nscheme won the first prize in the PRCV 2024 wide-area infrared small target\ndetection competition.\n", "link": "http://arxiv.org/abs/2407.20090v1", "date": "2024-07-29", "relevancy": 2.0397, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5206}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5101}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infrared%20Small%20Target%20Detection%20based%20on%20Adjustable%20Sensitivity%20Strategy%0A%20%20and%20Multi-Scale%20Fusion&body=Title%3A%20Infrared%20Small%20Target%20Detection%20based%20on%20Adjustable%20Sensitivity%20Strategy%0A%20%20and%20Multi-Scale%20Fusion%0AAuthor%3A%20Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu%0AAbstract%3A%20%20%20Recently%2C%20deep%20learning-based%20single-frame%20infrared%20small%20target%20%28SIRST%29%0Adetection%20technology%20has%20made%20significant%20progress.%20However%2C%20existing%20infrared%0Asmall%20target%20detection%20methods%20are%20often%20optimized%20for%20a%20fixed%20image%0Aresolution%2C%20a%20single%20wavelength%2C%20or%20a%20specific%20imaging%20system%2C%20limiting%20their%0Abreadth%20and%20flexibility%20in%20practical%20applications.%20Therefore%2C%20we%20propose%20a%0Arefined%20infrared%20small%20target%20detection%20scheme%20based%20on%20an%20adjustable%0Asensitivity%20%28AS%29%20strategy%20and%20multi-scale%20fusion.%20Specifically%2C%20a%20multi-scale%0Amodel%20fusion%20framework%20based%20on%20multi-scale%20direction-aware%20network%20%28MSDA-Net%29%0Ais%20constructed%2C%20which%20uses%20input%20images%20of%20multiple%20scales%20to%20train%20multiple%0Amodels%20and%20fuses%20them.%20Multi-scale%20fusion%20helps%20characterize%20the%20shape%2C%20edge%2C%0Aand%20texture%20features%20of%20the%20target%20from%20different%20scales%2C%20making%20the%20model%20more%0Aaccurate%20and%20reliable%20in%20locating%20the%20target.%20At%20the%20same%20time%2C%20we%20fully%0Aconsider%20the%20characteristics%20of%20the%20infrared%20small%20target%20detection%20task%20and%0Aconstruct%20an%20edge%20enhancement%20difficulty%20mining%20%28EEDM%29%20loss.%20The%20EEDM%20loss%0Ahelps%20alleviate%20the%20problem%20of%20category%20imbalance%20and%20guides%20the%20network%20to%20pay%0Amore%20attention%20to%20difficult%20target%20areas%20and%20edge%20features%20during%20training.%20In%0Aaddition%2C%20we%20propose%20an%20adjustable%20sensitivity%20strategy%20for%20post-processing.%0AThis%20strategy%20significantly%20improves%20the%20detection%20rate%20of%20infrared%20small%0Atargets%20while%20ensuring%20segmentation%20accuracy.%20Extensive%20experimental%20results%0Ashow%20that%20the%20proposed%20scheme%20achieves%20the%20best%20performance.%20Notably%2C%20this%0Ascheme%20won%20the%20first%20prize%20in%20the%20PRCV%202024%20wide-area%20infrared%20small%20target%0Adetection%20competition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfrared%2520Small%2520Target%2520Detection%2520based%2520on%2520Adjustable%2520Sensitivity%2520Strategy%250A%2520%2520and%2520Multi-Scale%2520Fusion%26entry.906535625%3DJinmiao%2520Zhao%2520and%2520Zelin%2520Shi%2520and%2520Chuang%2520Yu%2520and%2520Yunpeng%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520deep%2520learning-based%2520single-frame%2520infrared%2520small%2520target%2520%2528SIRST%2529%250Adetection%2520technology%2520has%2520made%2520significant%2520progress.%2520However%252C%2520existing%2520infrared%250Asmall%2520target%2520detection%2520methods%2520are%2520often%2520optimized%2520for%2520a%2520fixed%2520image%250Aresolution%252C%2520a%2520single%2520wavelength%252C%2520or%2520a%2520specific%2520imaging%2520system%252C%2520limiting%2520their%250Abreadth%2520and%2520flexibility%2520in%2520practical%2520applications.%2520Therefore%252C%2520we%2520propose%2520a%250Arefined%2520infrared%2520small%2520target%2520detection%2520scheme%2520based%2520on%2520an%2520adjustable%250Asensitivity%2520%2528AS%2529%2520strategy%2520and%2520multi-scale%2520fusion.%2520Specifically%252C%2520a%2520multi-scale%250Amodel%2520fusion%2520framework%2520based%2520on%2520multi-scale%2520direction-aware%2520network%2520%2528MSDA-Net%2529%250Ais%2520constructed%252C%2520which%2520uses%2520input%2520images%2520of%2520multiple%2520scales%2520to%2520train%2520multiple%250Amodels%2520and%2520fuses%2520them.%2520Multi-scale%2520fusion%2520helps%2520characterize%2520the%2520shape%252C%2520edge%252C%250Aand%2520texture%2520features%2520of%2520the%2520target%2520from%2520different%2520scales%252C%2520making%2520the%2520model%2520more%250Aaccurate%2520and%2520reliable%2520in%2520locating%2520the%2520target.%2520At%2520the%2520same%2520time%252C%2520we%2520fully%250Aconsider%2520the%2520characteristics%2520of%2520the%2520infrared%2520small%2520target%2520detection%2520task%2520and%250Aconstruct%2520an%2520edge%2520enhancement%2520difficulty%2520mining%2520%2528EEDM%2529%2520loss.%2520The%2520EEDM%2520loss%250Ahelps%2520alleviate%2520the%2520problem%2520of%2520category%2520imbalance%2520and%2520guides%2520the%2520network%2520to%2520pay%250Amore%2520attention%2520to%2520difficult%2520target%2520areas%2520and%2520edge%2520features%2520during%2520training.%2520In%250Aaddition%252C%2520we%2520propose%2520an%2520adjustable%2520sensitivity%2520strategy%2520for%2520post-processing.%250AThis%2520strategy%2520significantly%2520improves%2520the%2520detection%2520rate%2520of%2520infrared%2520small%250Atargets%2520while%2520ensuring%2520segmentation%2520accuracy.%2520Extensive%2520experimental%2520results%250Ashow%2520that%2520the%2520proposed%2520scheme%2520achieves%2520the%2520best%2520performance.%2520Notably%252C%2520this%250Ascheme%2520won%2520the%2520first%2520prize%2520in%2520the%2520PRCV%25202024%2520wide-area%2520infrared%2520small%2520target%250Adetection%2520competition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infrared%20Small%20Target%20Detection%20based%20on%20Adjustable%20Sensitivity%20Strategy%0A%20%20and%20Multi-Scale%20Fusion&entry.906535625=Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu&entry.1292438233=%20%20Recently%2C%20deep%20learning-based%20single-frame%20infrared%20small%20target%20%28SIRST%29%0Adetection%20technology%20has%20made%20significant%20progress.%20However%2C%20existing%20infrared%0Asmall%20target%20detection%20methods%20are%20often%20optimized%20for%20a%20fixed%20image%0Aresolution%2C%20a%20single%20wavelength%2C%20or%20a%20specific%20imaging%20system%2C%20limiting%20their%0Abreadth%20and%20flexibility%20in%20practical%20applications.%20Therefore%2C%20we%20propose%20a%0Arefined%20infrared%20small%20target%20detection%20scheme%20based%20on%20an%20adjustable%0Asensitivity%20%28AS%29%20strategy%20and%20multi-scale%20fusion.%20Specifically%2C%20a%20multi-scale%0Amodel%20fusion%20framework%20based%20on%20multi-scale%20direction-aware%20network%20%28MSDA-Net%29%0Ais%20constructed%2C%20which%20uses%20input%20images%20of%20multiple%20scales%20to%20train%20multiple%0Amodels%20and%20fuses%20them.%20Multi-scale%20fusion%20helps%20characterize%20the%20shape%2C%20edge%2C%0Aand%20texture%20features%20of%20the%20target%20from%20different%20scales%2C%20making%20the%20model%20more%0Aaccurate%20and%20reliable%20in%20locating%20the%20target.%20At%20the%20same%20time%2C%20we%20fully%0Aconsider%20the%20characteristics%20of%20the%20infrared%20small%20target%20detection%20task%20and%0Aconstruct%20an%20edge%20enhancement%20difficulty%20mining%20%28EEDM%29%20loss.%20The%20EEDM%20loss%0Ahelps%20alleviate%20the%20problem%20of%20category%20imbalance%20and%20guides%20the%20network%20to%20pay%0Amore%20attention%20to%20difficult%20target%20areas%20and%20edge%20features%20during%20training.%20In%0Aaddition%2C%20we%20propose%20an%20adjustable%20sensitivity%20strategy%20for%20post-processing.%0AThis%20strategy%20significantly%20improves%20the%20detection%20rate%20of%20infrared%20small%0Atargets%20while%20ensuring%20segmentation%20accuracy.%20Extensive%20experimental%20results%0Ashow%20that%20the%20proposed%20scheme%20achieves%20the%20best%20performance.%20Notably%2C%20this%0Ascheme%20won%20the%20first%20prize%20in%20the%20PRCV%202024%20wide-area%20infrared%20small%20target%0Adetection%20competition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20090v1&entry.124074799=Read"},
{"title": "Segmenting Fetal Head with Efficient Fine-tuning Strategies in\n  Low-resource Settings: an empirical study with U-Net", "author": "Fangyijie Wang and Gu\u00e9nol\u00e9 Silvestre and Kathleen M. Curran", "abstract": "  Accurate measurement of fetal head circumference is crucial for estimating\nfetal growth during routine prenatal screening. Prior to measurement, it is\nnecessary to accurately identify and segment the region of interest,\nspecifically the fetal head, in ultrasound images. Recent advancements in deep\nlearning techniques have shown significant progress in segmenting the fetal\nhead using encoder-decoder models. Among these models, U-Net has become a\nstandard approach for accurate segmentation. However, training an\nencoder-decoder model can be a time-consuming process that demands substantial\ncomputational resources. Moreover, fine-tuning these models is particularly\nchallenging when there is a limited amount of data available. There are still\nno \"best-practice\" guidelines for optimal fine-tuning of U-net for fetal\nultrasound image segmentation. This work summarizes existing fine-tuning\nstrategies with various backbone architectures, model components, and\nfine-tuning strategies across ultrasound data from Netherlands, Spain, Malawi,\nEgypt and Algeria. Our study shows that (1) fine-tuning U-Net leads to better\nperformance than training from scratch, (2) fine-tuning strategies in decoder\nare superior to other strategies, (3) network architecture with less number of\nparameters can achieve similar or better performance. We also demonstrate the\neffectiveness of fine-tuning strategies in low-resource settings and further\nexpand our experiments into few-shot learning. Lastly, we publicly released our\ncode and specific fine-tuned weights.\n", "link": "http://arxiv.org/abs/2407.20086v1", "date": "2024-07-29", "relevancy": 2.0319, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.518}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5068}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmenting%20Fetal%20Head%20with%20Efficient%20Fine-tuning%20Strategies%20in%0A%20%20Low-resource%20Settings%3A%20an%20empirical%20study%20with%20U-Net&body=Title%3A%20Segmenting%20Fetal%20Head%20with%20Efficient%20Fine-tuning%20Strategies%20in%0A%20%20Low-resource%20Settings%3A%20an%20empirical%20study%20with%20U-Net%0AAuthor%3A%20Fangyijie%20Wang%20and%20Gu%C3%A9nol%C3%A9%20Silvestre%20and%20Kathleen%20M.%20Curran%0AAbstract%3A%20%20%20Accurate%20measurement%20of%20fetal%20head%20circumference%20is%20crucial%20for%20estimating%0Afetal%20growth%20during%20routine%20prenatal%20screening.%20Prior%20to%20measurement%2C%20it%20is%0Anecessary%20to%20accurately%20identify%20and%20segment%20the%20region%20of%20interest%2C%0Aspecifically%20the%20fetal%20head%2C%20in%20ultrasound%20images.%20Recent%20advancements%20in%20deep%0Alearning%20techniques%20have%20shown%20significant%20progress%20in%20segmenting%20the%20fetal%0Ahead%20using%20encoder-decoder%20models.%20Among%20these%20models%2C%20U-Net%20has%20become%20a%0Astandard%20approach%20for%20accurate%20segmentation.%20However%2C%20training%20an%0Aencoder-decoder%20model%20can%20be%20a%20time-consuming%20process%20that%20demands%20substantial%0Acomputational%20resources.%20Moreover%2C%20fine-tuning%20these%20models%20is%20particularly%0Achallenging%20when%20there%20is%20a%20limited%20amount%20of%20data%20available.%20There%20are%20still%0Ano%20%22best-practice%22%20guidelines%20for%20optimal%20fine-tuning%20of%20U-net%20for%20fetal%0Aultrasound%20image%20segmentation.%20This%20work%20summarizes%20existing%20fine-tuning%0Astrategies%20with%20various%20backbone%20architectures%2C%20model%20components%2C%20and%0Afine-tuning%20strategies%20across%20ultrasound%20data%20from%20Netherlands%2C%20Spain%2C%20Malawi%2C%0AEgypt%20and%20Algeria.%20Our%20study%20shows%20that%20%281%29%20fine-tuning%20U-Net%20leads%20to%20better%0Aperformance%20than%20training%20from%20scratch%2C%20%282%29%20fine-tuning%20strategies%20in%20decoder%0Aare%20superior%20to%20other%20strategies%2C%20%283%29%20network%20architecture%20with%20less%20number%20of%0Aparameters%20can%20achieve%20similar%20or%20better%20performance.%20We%20also%20demonstrate%20the%0Aeffectiveness%20of%20fine-tuning%20strategies%20in%20low-resource%20settings%20and%20further%0Aexpand%20our%20experiments%20into%20few-shot%20learning.%20Lastly%2C%20we%20publicly%20released%20our%0Acode%20and%20specific%20fine-tuned%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmenting%2520Fetal%2520Head%2520with%2520Efficient%2520Fine-tuning%2520Strategies%2520in%250A%2520%2520Low-resource%2520Settings%253A%2520an%2520empirical%2520study%2520with%2520U-Net%26entry.906535625%3DFangyijie%2520Wang%2520and%2520Gu%25C3%25A9nol%25C3%25A9%2520Silvestre%2520and%2520Kathleen%2520M.%2520Curran%26entry.1292438233%3D%2520%2520Accurate%2520measurement%2520of%2520fetal%2520head%2520circumference%2520is%2520crucial%2520for%2520estimating%250Afetal%2520growth%2520during%2520routine%2520prenatal%2520screening.%2520Prior%2520to%2520measurement%252C%2520it%2520is%250Anecessary%2520to%2520accurately%2520identify%2520and%2520segment%2520the%2520region%2520of%2520interest%252C%250Aspecifically%2520the%2520fetal%2520head%252C%2520in%2520ultrasound%2520images.%2520Recent%2520advancements%2520in%2520deep%250Alearning%2520techniques%2520have%2520shown%2520significant%2520progress%2520in%2520segmenting%2520the%2520fetal%250Ahead%2520using%2520encoder-decoder%2520models.%2520Among%2520these%2520models%252C%2520U-Net%2520has%2520become%2520a%250Astandard%2520approach%2520for%2520accurate%2520segmentation.%2520However%252C%2520training%2520an%250Aencoder-decoder%2520model%2520can%2520be%2520a%2520time-consuming%2520process%2520that%2520demands%2520substantial%250Acomputational%2520resources.%2520Moreover%252C%2520fine-tuning%2520these%2520models%2520is%2520particularly%250Achallenging%2520when%2520there%2520is%2520a%2520limited%2520amount%2520of%2520data%2520available.%2520There%2520are%2520still%250Ano%2520%2522best-practice%2522%2520guidelines%2520for%2520optimal%2520fine-tuning%2520of%2520U-net%2520for%2520fetal%250Aultrasound%2520image%2520segmentation.%2520This%2520work%2520summarizes%2520existing%2520fine-tuning%250Astrategies%2520with%2520various%2520backbone%2520architectures%252C%2520model%2520components%252C%2520and%250Afine-tuning%2520strategies%2520across%2520ultrasound%2520data%2520from%2520Netherlands%252C%2520Spain%252C%2520Malawi%252C%250AEgypt%2520and%2520Algeria.%2520Our%2520study%2520shows%2520that%2520%25281%2529%2520fine-tuning%2520U-Net%2520leads%2520to%2520better%250Aperformance%2520than%2520training%2520from%2520scratch%252C%2520%25282%2529%2520fine-tuning%2520strategies%2520in%2520decoder%250Aare%2520superior%2520to%2520other%2520strategies%252C%2520%25283%2529%2520network%2520architecture%2520with%2520less%2520number%2520of%250Aparameters%2520can%2520achieve%2520similar%2520or%2520better%2520performance.%2520We%2520also%2520demonstrate%2520the%250Aeffectiveness%2520of%2520fine-tuning%2520strategies%2520in%2520low-resource%2520settings%2520and%2520further%250Aexpand%2520our%2520experiments%2520into%2520few-shot%2520learning.%2520Lastly%252C%2520we%2520publicly%2520released%2520our%250Acode%2520and%2520specific%2520fine-tuned%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmenting%20Fetal%20Head%20with%20Efficient%20Fine-tuning%20Strategies%20in%0A%20%20Low-resource%20Settings%3A%20an%20empirical%20study%20with%20U-Net&entry.906535625=Fangyijie%20Wang%20and%20Gu%C3%A9nol%C3%A9%20Silvestre%20and%20Kathleen%20M.%20Curran&entry.1292438233=%20%20Accurate%20measurement%20of%20fetal%20head%20circumference%20is%20crucial%20for%20estimating%0Afetal%20growth%20during%20routine%20prenatal%20screening.%20Prior%20to%20measurement%2C%20it%20is%0Anecessary%20to%20accurately%20identify%20and%20segment%20the%20region%20of%20interest%2C%0Aspecifically%20the%20fetal%20head%2C%20in%20ultrasound%20images.%20Recent%20advancements%20in%20deep%0Alearning%20techniques%20have%20shown%20significant%20progress%20in%20segmenting%20the%20fetal%0Ahead%20using%20encoder-decoder%20models.%20Among%20these%20models%2C%20U-Net%20has%20become%20a%0Astandard%20approach%20for%20accurate%20segmentation.%20However%2C%20training%20an%0Aencoder-decoder%20model%20can%20be%20a%20time-consuming%20process%20that%20demands%20substantial%0Acomputational%20resources.%20Moreover%2C%20fine-tuning%20these%20models%20is%20particularly%0Achallenging%20when%20there%20is%20a%20limited%20amount%20of%20data%20available.%20There%20are%20still%0Ano%20%22best-practice%22%20guidelines%20for%20optimal%20fine-tuning%20of%20U-net%20for%20fetal%0Aultrasound%20image%20segmentation.%20This%20work%20summarizes%20existing%20fine-tuning%0Astrategies%20with%20various%20backbone%20architectures%2C%20model%20components%2C%20and%0Afine-tuning%20strategies%20across%20ultrasound%20data%20from%20Netherlands%2C%20Spain%2C%20Malawi%2C%0AEgypt%20and%20Algeria.%20Our%20study%20shows%20that%20%281%29%20fine-tuning%20U-Net%20leads%20to%20better%0Aperformance%20than%20training%20from%20scratch%2C%20%282%29%20fine-tuning%20strategies%20in%20decoder%0Aare%20superior%20to%20other%20strategies%2C%20%283%29%20network%20architecture%20with%20less%20number%20of%0Aparameters%20can%20achieve%20similar%20or%20better%20performance.%20We%20also%20demonstrate%20the%0Aeffectiveness%20of%20fine-tuning%20strategies%20in%20low-resource%20settings%20and%20further%0Aexpand%20our%20experiments%20into%20few-shot%20learning.%20Lastly%2C%20we%20publicly%20released%20our%0Acode%20and%20specific%20fine-tuned%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20086v1&entry.124074799=Read"},
{"title": "Robust Fully-Asynchronous Methods for Distributed Training over General\n  Architecture", "author": "Zehan Zhu and Ye Tian and Yan Huang and Jinming Xu and Shibo He", "abstract": "  Perfect synchronization in distributed machine learning problems is\ninefficient and even impossible due to the existence of latency, package losses\nand stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient\nTracking method (R-FAST), where each device performs local computation and\ncommunication at its own pace without any form of synchronization. Different\nfrom existing asynchronous distributed algorithms, R-FAST can eliminate the\nimpact of data heterogeneity across devices and allow for packet losses by\nemploying a robust gradient tracking strategy that relies on properly designed\nauxiliary variables for tracking and buffering the overall gradient vector.\nMore importantly, the proposed method utilizes two spanning-tree graphs for\ncommunication so long as both share at least one common root, enabling flexible\ndesigns in communication architectures. We show that R-FAST converges in\nexpectation to a neighborhood of the optimum with a geometric rate for smooth\nand strongly convex objectives; and to a stationary point with a sublinear rate\nfor general non-convex settings. Extensive experiments demonstrate that R-FAST\nruns 1.5-2 times faster than synchronous benchmark algorithms, such as\nRing-AllReduce and D-PSGD, while still achieving comparable accuracy, and\noutperforms existing asynchronous SOTA algorithms, such as AD-PSGD and OSGP,\nespecially in the presence of stragglers.\n", "link": "http://arxiv.org/abs/2307.11617v2", "date": "2024-07-29", "relevancy": 2.0281, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5246}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5066}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Fully-Asynchronous%20Methods%20for%20Distributed%20Training%20over%20General%0A%20%20Architecture&body=Title%3A%20Robust%20Fully-Asynchronous%20Methods%20for%20Distributed%20Training%20over%20General%0A%20%20Architecture%0AAuthor%3A%20Zehan%20Zhu%20and%20Ye%20Tian%20and%20Yan%20Huang%20and%20Jinming%20Xu%20and%20Shibo%20He%0AAbstract%3A%20%20%20Perfect%20synchronization%20in%20distributed%20machine%20learning%20problems%20is%0Ainefficient%20and%20even%20impossible%20due%20to%20the%20existence%20of%20latency%2C%20package%20losses%0Aand%20stragglers.%20We%20propose%20a%20Robust%20Fully-Asynchronous%20Stochastic%20Gradient%0ATracking%20method%20%28R-FAST%29%2C%20where%20each%20device%20performs%20local%20computation%20and%0Acommunication%20at%20its%20own%20pace%20without%20any%20form%20of%20synchronization.%20Different%0Afrom%20existing%20asynchronous%20distributed%20algorithms%2C%20R-FAST%20can%20eliminate%20the%0Aimpact%20of%20data%20heterogeneity%20across%20devices%20and%20allow%20for%20packet%20losses%20by%0Aemploying%20a%20robust%20gradient%20tracking%20strategy%20that%20relies%20on%20properly%20designed%0Aauxiliary%20variables%20for%20tracking%20and%20buffering%20the%20overall%20gradient%20vector.%0AMore%20importantly%2C%20the%20proposed%20method%20utilizes%20two%20spanning-tree%20graphs%20for%0Acommunication%20so%20long%20as%20both%20share%20at%20least%20one%20common%20root%2C%20enabling%20flexible%0Adesigns%20in%20communication%20architectures.%20We%20show%20that%20R-FAST%20converges%20in%0Aexpectation%20to%20a%20neighborhood%20of%20the%20optimum%20with%20a%20geometric%20rate%20for%20smooth%0Aand%20strongly%20convex%20objectives%3B%20and%20to%20a%20stationary%20point%20with%20a%20sublinear%20rate%0Afor%20general%20non-convex%20settings.%20Extensive%20experiments%20demonstrate%20that%20R-FAST%0Aruns%201.5-2%20times%20faster%20than%20synchronous%20benchmark%20algorithms%2C%20such%20as%0ARing-AllReduce%20and%20D-PSGD%2C%20while%20still%20achieving%20comparable%20accuracy%2C%20and%0Aoutperforms%20existing%20asynchronous%20SOTA%20algorithms%2C%20such%20as%20AD-PSGD%20and%20OSGP%2C%0Aespecially%20in%20the%20presence%20of%20stragglers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.11617v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Fully-Asynchronous%2520Methods%2520for%2520Distributed%2520Training%2520over%2520General%250A%2520%2520Architecture%26entry.906535625%3DZehan%2520Zhu%2520and%2520Ye%2520Tian%2520and%2520Yan%2520Huang%2520and%2520Jinming%2520Xu%2520and%2520Shibo%2520He%26entry.1292438233%3D%2520%2520Perfect%2520synchronization%2520in%2520distributed%2520machine%2520learning%2520problems%2520is%250Ainefficient%2520and%2520even%2520impossible%2520due%2520to%2520the%2520existence%2520of%2520latency%252C%2520package%2520losses%250Aand%2520stragglers.%2520We%2520propose%2520a%2520Robust%2520Fully-Asynchronous%2520Stochastic%2520Gradient%250ATracking%2520method%2520%2528R-FAST%2529%252C%2520where%2520each%2520device%2520performs%2520local%2520computation%2520and%250Acommunication%2520at%2520its%2520own%2520pace%2520without%2520any%2520form%2520of%2520synchronization.%2520Different%250Afrom%2520existing%2520asynchronous%2520distributed%2520algorithms%252C%2520R-FAST%2520can%2520eliminate%2520the%250Aimpact%2520of%2520data%2520heterogeneity%2520across%2520devices%2520and%2520allow%2520for%2520packet%2520losses%2520by%250Aemploying%2520a%2520robust%2520gradient%2520tracking%2520strategy%2520that%2520relies%2520on%2520properly%2520designed%250Aauxiliary%2520variables%2520for%2520tracking%2520and%2520buffering%2520the%2520overall%2520gradient%2520vector.%250AMore%2520importantly%252C%2520the%2520proposed%2520method%2520utilizes%2520two%2520spanning-tree%2520graphs%2520for%250Acommunication%2520so%2520long%2520as%2520both%2520share%2520at%2520least%2520one%2520common%2520root%252C%2520enabling%2520flexible%250Adesigns%2520in%2520communication%2520architectures.%2520We%2520show%2520that%2520R-FAST%2520converges%2520in%250Aexpectation%2520to%2520a%2520neighborhood%2520of%2520the%2520optimum%2520with%2520a%2520geometric%2520rate%2520for%2520smooth%250Aand%2520strongly%2520convex%2520objectives%253B%2520and%2520to%2520a%2520stationary%2520point%2520with%2520a%2520sublinear%2520rate%250Afor%2520general%2520non-convex%2520settings.%2520Extensive%2520experiments%2520demonstrate%2520that%2520R-FAST%250Aruns%25201.5-2%2520times%2520faster%2520than%2520synchronous%2520benchmark%2520algorithms%252C%2520such%2520as%250ARing-AllReduce%2520and%2520D-PSGD%252C%2520while%2520still%2520achieving%2520comparable%2520accuracy%252C%2520and%250Aoutperforms%2520existing%2520asynchronous%2520SOTA%2520algorithms%252C%2520such%2520as%2520AD-PSGD%2520and%2520OSGP%252C%250Aespecially%2520in%2520the%2520presence%2520of%2520stragglers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.11617v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Fully-Asynchronous%20Methods%20for%20Distributed%20Training%20over%20General%0A%20%20Architecture&entry.906535625=Zehan%20Zhu%20and%20Ye%20Tian%20and%20Yan%20Huang%20and%20Jinming%20Xu%20and%20Shibo%20He&entry.1292438233=%20%20Perfect%20synchronization%20in%20distributed%20machine%20learning%20problems%20is%0Ainefficient%20and%20even%20impossible%20due%20to%20the%20existence%20of%20latency%2C%20package%20losses%0Aand%20stragglers.%20We%20propose%20a%20Robust%20Fully-Asynchronous%20Stochastic%20Gradient%0ATracking%20method%20%28R-FAST%29%2C%20where%20each%20device%20performs%20local%20computation%20and%0Acommunication%20at%20its%20own%20pace%20without%20any%20form%20of%20synchronization.%20Different%0Afrom%20existing%20asynchronous%20distributed%20algorithms%2C%20R-FAST%20can%20eliminate%20the%0Aimpact%20of%20data%20heterogeneity%20across%20devices%20and%20allow%20for%20packet%20losses%20by%0Aemploying%20a%20robust%20gradient%20tracking%20strategy%20that%20relies%20on%20properly%20designed%0Aauxiliary%20variables%20for%20tracking%20and%20buffering%20the%20overall%20gradient%20vector.%0AMore%20importantly%2C%20the%20proposed%20method%20utilizes%20two%20spanning-tree%20graphs%20for%0Acommunication%20so%20long%20as%20both%20share%20at%20least%20one%20common%20root%2C%20enabling%20flexible%0Adesigns%20in%20communication%20architectures.%20We%20show%20that%20R-FAST%20converges%20in%0Aexpectation%20to%20a%20neighborhood%20of%20the%20optimum%20with%20a%20geometric%20rate%20for%20smooth%0Aand%20strongly%20convex%20objectives%3B%20and%20to%20a%20stationary%20point%20with%20a%20sublinear%20rate%0Afor%20general%20non-convex%20settings.%20Extensive%20experiments%20demonstrate%20that%20R-FAST%0Aruns%201.5-2%20times%20faster%20than%20synchronous%20benchmark%20algorithms%2C%20such%20as%0ARing-AllReduce%20and%20D-PSGD%2C%20while%20still%20achieving%20comparable%20accuracy%2C%20and%0Aoutperforms%20existing%20asynchronous%20SOTA%20algorithms%2C%20such%20as%20AD-PSGD%20and%20OSGP%2C%0Aespecially%20in%20the%20presence%20of%20stragglers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.11617v2&entry.124074799=Read"},
{"title": "Quasi-Framelets: Robust Graph Neural Networks via Adaptive Framelet\n  Convolution", "author": "Mengxi Yang and Dai Shi and Xuebin Zheng and Jie Yin and Junbin Gao", "abstract": "  This paper aims to provide a novel design of a multiscale framelet\nconvolution for spectral graph neural networks (GNNs). While current spectral\nmethods excel in various graph learning tasks, they often lack the flexibility\nto adapt to noisy, incomplete, or perturbed graph signals, making them fragile\nin such conditions. Our newly proposed framelet convolution addresses these\nlimitations by decomposing graph data into low-pass and high-pass spectra\nthrough a finely-tuned multiscale approach. Our approach directly designs\nfiltering functions within the spectral domain, allowing for precise control\nover the spectral components. The proposed design excels in filtering out\nunwanted spectral information and significantly reduces the adverse effects of\nnoisy graph signals. Our approach not only enhances the robustness of GNNs but\nalso preserves crucial graph features and structures. Through extensive\nexperiments on diverse, real-world graph datasets, we demonstrate that our\nframelet convolution achieves superior performance in node classification\ntasks. It exhibits remarkable resilience to noisy data and adversarial attacks,\nhighlighting its potential as a robust solution for real-world graph\napplications. This advancement opens new avenues for more adaptive and reliable\nspectral GNN architectures.\n", "link": "http://arxiv.org/abs/2201.04728v2", "date": "2024-07-29", "relevancy": 2.0199, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5087}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5058}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quasi-Framelets%3A%20Robust%20Graph%20Neural%20Networks%20via%20Adaptive%20Framelet%0A%20%20Convolution&body=Title%3A%20Quasi-Framelets%3A%20Robust%20Graph%20Neural%20Networks%20via%20Adaptive%20Framelet%0A%20%20Convolution%0AAuthor%3A%20Mengxi%20Yang%20and%20Dai%20Shi%20and%20Xuebin%20Zheng%20and%20Jie%20Yin%20and%20Junbin%20Gao%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20provide%20a%20novel%20design%20of%20a%20multiscale%20framelet%0Aconvolution%20for%20spectral%20graph%20neural%20networks%20%28GNNs%29.%20While%20current%20spectral%0Amethods%20excel%20in%20various%20graph%20learning%20tasks%2C%20they%20often%20lack%20the%20flexibility%0Ato%20adapt%20to%20noisy%2C%20incomplete%2C%20or%20perturbed%20graph%20signals%2C%20making%20them%20fragile%0Ain%20such%20conditions.%20Our%20newly%20proposed%20framelet%20convolution%20addresses%20these%0Alimitations%20by%20decomposing%20graph%20data%20into%20low-pass%20and%20high-pass%20spectra%0Athrough%20a%20finely-tuned%20multiscale%20approach.%20Our%20approach%20directly%20designs%0Afiltering%20functions%20within%20the%20spectral%20domain%2C%20allowing%20for%20precise%20control%0Aover%20the%20spectral%20components.%20The%20proposed%20design%20excels%20in%20filtering%20out%0Aunwanted%20spectral%20information%20and%20significantly%20reduces%20the%20adverse%20effects%20of%0Anoisy%20graph%20signals.%20Our%20approach%20not%20only%20enhances%20the%20robustness%20of%20GNNs%20but%0Aalso%20preserves%20crucial%20graph%20features%20and%20structures.%20Through%20extensive%0Aexperiments%20on%20diverse%2C%20real-world%20graph%20datasets%2C%20we%20demonstrate%20that%20our%0Aframelet%20convolution%20achieves%20superior%20performance%20in%20node%20classification%0Atasks.%20It%20exhibits%20remarkable%20resilience%20to%20noisy%20data%20and%20adversarial%20attacks%2C%0Ahighlighting%20its%20potential%20as%20a%20robust%20solution%20for%20real-world%20graph%0Aapplications.%20This%20advancement%20opens%20new%20avenues%20for%20more%20adaptive%20and%20reliable%0Aspectral%20GNN%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.04728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuasi-Framelets%253A%2520Robust%2520Graph%2520Neural%2520Networks%2520via%2520Adaptive%2520Framelet%250A%2520%2520Convolution%26entry.906535625%3DMengxi%2520Yang%2520and%2520Dai%2520Shi%2520and%2520Xuebin%2520Zheng%2520and%2520Jie%2520Yin%2520and%2520Junbin%2520Gao%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520provide%2520a%2520novel%2520design%2520of%2520a%2520multiscale%2520framelet%250Aconvolution%2520for%2520spectral%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520While%2520current%2520spectral%250Amethods%2520excel%2520in%2520various%2520graph%2520learning%2520tasks%252C%2520they%2520often%2520lack%2520the%2520flexibility%250Ato%2520adapt%2520to%2520noisy%252C%2520incomplete%252C%2520or%2520perturbed%2520graph%2520signals%252C%2520making%2520them%2520fragile%250Ain%2520such%2520conditions.%2520Our%2520newly%2520proposed%2520framelet%2520convolution%2520addresses%2520these%250Alimitations%2520by%2520decomposing%2520graph%2520data%2520into%2520low-pass%2520and%2520high-pass%2520spectra%250Athrough%2520a%2520finely-tuned%2520multiscale%2520approach.%2520Our%2520approach%2520directly%2520designs%250Afiltering%2520functions%2520within%2520the%2520spectral%2520domain%252C%2520allowing%2520for%2520precise%2520control%250Aover%2520the%2520spectral%2520components.%2520The%2520proposed%2520design%2520excels%2520in%2520filtering%2520out%250Aunwanted%2520spectral%2520information%2520and%2520significantly%2520reduces%2520the%2520adverse%2520effects%2520of%250Anoisy%2520graph%2520signals.%2520Our%2520approach%2520not%2520only%2520enhances%2520the%2520robustness%2520of%2520GNNs%2520but%250Aalso%2520preserves%2520crucial%2520graph%2520features%2520and%2520structures.%2520Through%2520extensive%250Aexperiments%2520on%2520diverse%252C%2520real-world%2520graph%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%250Aframelet%2520convolution%2520achieves%2520superior%2520performance%2520in%2520node%2520classification%250Atasks.%2520It%2520exhibits%2520remarkable%2520resilience%2520to%2520noisy%2520data%2520and%2520adversarial%2520attacks%252C%250Ahighlighting%2520its%2520potential%2520as%2520a%2520robust%2520solution%2520for%2520real-world%2520graph%250Aapplications.%2520This%2520advancement%2520opens%2520new%2520avenues%2520for%2520more%2520adaptive%2520and%2520reliable%250Aspectral%2520GNN%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.04728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quasi-Framelets%3A%20Robust%20Graph%20Neural%20Networks%20via%20Adaptive%20Framelet%0A%20%20Convolution&entry.906535625=Mengxi%20Yang%20and%20Dai%20Shi%20and%20Xuebin%20Zheng%20and%20Jie%20Yin%20and%20Junbin%20Gao&entry.1292438233=%20%20This%20paper%20aims%20to%20provide%20a%20novel%20design%20of%20a%20multiscale%20framelet%0Aconvolution%20for%20spectral%20graph%20neural%20networks%20%28GNNs%29.%20While%20current%20spectral%0Amethods%20excel%20in%20various%20graph%20learning%20tasks%2C%20they%20often%20lack%20the%20flexibility%0Ato%20adapt%20to%20noisy%2C%20incomplete%2C%20or%20perturbed%20graph%20signals%2C%20making%20them%20fragile%0Ain%20such%20conditions.%20Our%20newly%20proposed%20framelet%20convolution%20addresses%20these%0Alimitations%20by%20decomposing%20graph%20data%20into%20low-pass%20and%20high-pass%20spectra%0Athrough%20a%20finely-tuned%20multiscale%20approach.%20Our%20approach%20directly%20designs%0Afiltering%20functions%20within%20the%20spectral%20domain%2C%20allowing%20for%20precise%20control%0Aover%20the%20spectral%20components.%20The%20proposed%20design%20excels%20in%20filtering%20out%0Aunwanted%20spectral%20information%20and%20significantly%20reduces%20the%20adverse%20effects%20of%0Anoisy%20graph%20signals.%20Our%20approach%20not%20only%20enhances%20the%20robustness%20of%20GNNs%20but%0Aalso%20preserves%20crucial%20graph%20features%20and%20structures.%20Through%20extensive%0Aexperiments%20on%20diverse%2C%20real-world%20graph%20datasets%2C%20we%20demonstrate%20that%20our%0Aframelet%20convolution%20achieves%20superior%20performance%20in%20node%20classification%0Atasks.%20It%20exhibits%20remarkable%20resilience%20to%20noisy%20data%20and%20adversarial%20attacks%2C%0Ahighlighting%20its%20potential%20as%20a%20robust%20solution%20for%20real-world%20graph%0Aapplications.%20This%20advancement%20opens%20new%20avenues%20for%20more%20adaptive%20and%20reliable%0Aspectral%20GNN%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.04728v2&entry.124074799=Read"},
{"title": "Multi-fidelity Gaussian process surrogate modeling for regression\n  problems in physics", "author": "Kislaya Ravi and Vladyslav Fediukov and Felix Dietrich and Tobias Neckel and Fabian Buse and Michael Bergmann and Hans-Joachim Bungartz", "abstract": "  One of the main challenges in surrogate modeling is the limited availability\nof data due to resource constraints associated with computationally expensive\nsimulations. Multi-fidelity methods provide a solution by chaining models in a\nhierarchy with increasing fidelity, associated with lower error, but increasing\ncost. In this paper, we compare different multi-fidelity methods employed in\nconstructing Gaussian process surrogates for regression. Non-linear\nautoregressive methods in the existing literature are primarily confined to\ntwo-fidelity models, and we extend these methods to handle more than two levels\nof fidelity. Additionally, we propose enhancements for an existing method\nincorporating delay terms by introducing a structured kernel. We demonstrate\nthe performance of these methods across various academic and real-world\nscenarios. Our findings reveal that multi-fidelity methods generally have a\nsmaller prediction error for the same computational cost as compared to the\nsingle-fidelity method, although their effectiveness varies across different\nscenarios.\n", "link": "http://arxiv.org/abs/2404.11965v2", "date": "2024-07-29", "relevancy": 1.9983, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5367}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4938}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-fidelity%20Gaussian%20process%20surrogate%20modeling%20for%20regression%0A%20%20problems%20in%20physics&body=Title%3A%20Multi-fidelity%20Gaussian%20process%20surrogate%20modeling%20for%20regression%0A%20%20problems%20in%20physics%0AAuthor%3A%20Kislaya%20Ravi%20and%20Vladyslav%20Fediukov%20and%20Felix%20Dietrich%20and%20Tobias%20Neckel%20and%20Fabian%20Buse%20and%20Michael%20Bergmann%20and%20Hans-Joachim%20Bungartz%0AAbstract%3A%20%20%20One%20of%20the%20main%20challenges%20in%20surrogate%20modeling%20is%20the%20limited%20availability%0Aof%20data%20due%20to%20resource%20constraints%20associated%20with%20computationally%20expensive%0Asimulations.%20Multi-fidelity%20methods%20provide%20a%20solution%20by%20chaining%20models%20in%20a%0Ahierarchy%20with%20increasing%20fidelity%2C%20associated%20with%20lower%20error%2C%20but%20increasing%0Acost.%20In%20this%20paper%2C%20we%20compare%20different%20multi-fidelity%20methods%20employed%20in%0Aconstructing%20Gaussian%20process%20surrogates%20for%20regression.%20Non-linear%0Aautoregressive%20methods%20in%20the%20existing%20literature%20are%20primarily%20confined%20to%0Atwo-fidelity%20models%2C%20and%20we%20extend%20these%20methods%20to%20handle%20more%20than%20two%20levels%0Aof%20fidelity.%20Additionally%2C%20we%20propose%20enhancements%20for%20an%20existing%20method%0Aincorporating%20delay%20terms%20by%20introducing%20a%20structured%20kernel.%20We%20demonstrate%0Athe%20performance%20of%20these%20methods%20across%20various%20academic%20and%20real-world%0Ascenarios.%20Our%20findings%20reveal%20that%20multi-fidelity%20methods%20generally%20have%20a%0Asmaller%20prediction%20error%20for%20the%20same%20computational%20cost%20as%20compared%20to%20the%0Asingle-fidelity%20method%2C%20although%20their%20effectiveness%20varies%20across%20different%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-fidelity%2520Gaussian%2520process%2520surrogate%2520modeling%2520for%2520regression%250A%2520%2520problems%2520in%2520physics%26entry.906535625%3DKislaya%2520Ravi%2520and%2520Vladyslav%2520Fediukov%2520and%2520Felix%2520Dietrich%2520and%2520Tobias%2520Neckel%2520and%2520Fabian%2520Buse%2520and%2520Michael%2520Bergmann%2520and%2520Hans-Joachim%2520Bungartz%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520main%2520challenges%2520in%2520surrogate%2520modeling%2520is%2520the%2520limited%2520availability%250Aof%2520data%2520due%2520to%2520resource%2520constraints%2520associated%2520with%2520computationally%2520expensive%250Asimulations.%2520Multi-fidelity%2520methods%2520provide%2520a%2520solution%2520by%2520chaining%2520models%2520in%2520a%250Ahierarchy%2520with%2520increasing%2520fidelity%252C%2520associated%2520with%2520lower%2520error%252C%2520but%2520increasing%250Acost.%2520In%2520this%2520paper%252C%2520we%2520compare%2520different%2520multi-fidelity%2520methods%2520employed%2520in%250Aconstructing%2520Gaussian%2520process%2520surrogates%2520for%2520regression.%2520Non-linear%250Aautoregressive%2520methods%2520in%2520the%2520existing%2520literature%2520are%2520primarily%2520confined%2520to%250Atwo-fidelity%2520models%252C%2520and%2520we%2520extend%2520these%2520methods%2520to%2520handle%2520more%2520than%2520two%2520levels%250Aof%2520fidelity.%2520Additionally%252C%2520we%2520propose%2520enhancements%2520for%2520an%2520existing%2520method%250Aincorporating%2520delay%2520terms%2520by%2520introducing%2520a%2520structured%2520kernel.%2520We%2520demonstrate%250Athe%2520performance%2520of%2520these%2520methods%2520across%2520various%2520academic%2520and%2520real-world%250Ascenarios.%2520Our%2520findings%2520reveal%2520that%2520multi-fidelity%2520methods%2520generally%2520have%2520a%250Asmaller%2520prediction%2520error%2520for%2520the%2520same%2520computational%2520cost%2520as%2520compared%2520to%2520the%250Asingle-fidelity%2520method%252C%2520although%2520their%2520effectiveness%2520varies%2520across%2520different%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-fidelity%20Gaussian%20process%20surrogate%20modeling%20for%20regression%0A%20%20problems%20in%20physics&entry.906535625=Kislaya%20Ravi%20and%20Vladyslav%20Fediukov%20and%20Felix%20Dietrich%20and%20Tobias%20Neckel%20and%20Fabian%20Buse%20and%20Michael%20Bergmann%20and%20Hans-Joachim%20Bungartz&entry.1292438233=%20%20One%20of%20the%20main%20challenges%20in%20surrogate%20modeling%20is%20the%20limited%20availability%0Aof%20data%20due%20to%20resource%20constraints%20associated%20with%20computationally%20expensive%0Asimulations.%20Multi-fidelity%20methods%20provide%20a%20solution%20by%20chaining%20models%20in%20a%0Ahierarchy%20with%20increasing%20fidelity%2C%20associated%20with%20lower%20error%2C%20but%20increasing%0Acost.%20In%20this%20paper%2C%20we%20compare%20different%20multi-fidelity%20methods%20employed%20in%0Aconstructing%20Gaussian%20process%20surrogates%20for%20regression.%20Non-linear%0Aautoregressive%20methods%20in%20the%20existing%20literature%20are%20primarily%20confined%20to%0Atwo-fidelity%20models%2C%20and%20we%20extend%20these%20methods%20to%20handle%20more%20than%20two%20levels%0Aof%20fidelity.%20Additionally%2C%20we%20propose%20enhancements%20for%20an%20existing%20method%0Aincorporating%20delay%20terms%20by%20introducing%20a%20structured%20kernel.%20We%20demonstrate%0Athe%20performance%20of%20these%20methods%20across%20various%20academic%20and%20real-world%0Ascenarios.%20Our%20findings%20reveal%20that%20multi-fidelity%20methods%20generally%20have%20a%0Asmaller%20prediction%20error%20for%20the%20same%20computational%20cost%20as%20compared%20to%20the%0Asingle-fidelity%20method%2C%20although%20their%20effectiveness%20varies%20across%20different%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11965v2&entry.124074799=Read"},
{"title": "Robust and Resource-Efficient Data-Free Knowledge Distillation by\n  Generative Pseudo Replay", "author": "Kuluhan Binici and Shivam Aggarwal and Nam Trung Pham and Karianto Leman and Tulika Mitra", "abstract": "  Data-Free Knowledge Distillation (KD) allows knowledge transfer from a\ntrained neural network (teacher) to a more compact one (student) in the absence\nof original training data. Existing works use a validation set to monitor the\naccuracy of the student over real data and report the highest performance\nthroughout the entire process. However, validation data may not be available at\ndistillation time either, making it infeasible to record the student snapshot\nthat achieved the peak accuracy. Therefore, a practical data-free KD method\nshould be robust and ideally provide monotonically increasing student accuracy\nduring distillation. This is challenging because the student experiences\nknowledge degradation due to the distribution shift of the synthetic data. A\nstraightforward approach to overcome this issue is to store and rehearse the\ngenerated samples periodically, which increases the memory footprint and\ncreates privacy concerns. We propose to model the distribution of the\npreviously observed synthetic samples with a generative network. In particular,\nwe design a Variational Autoencoder (VAE) with a training objective that is\ncustomized to learn the synthetic data representations optimally. The student\nis rehearsed by the generative pseudo replay technique, with samples produced\nby the VAE. Hence knowledge degradation can be prevented without storing any\nsamples. Experiments on image classification benchmarks show that our method\noptimizes the expected value of the distilled model accuracy while eliminating\nthe large memory overhead incurred by the sample-storing methods.\n", "link": "http://arxiv.org/abs/2201.03019v3", "date": "2024-07-29", "relevancy": 1.9972, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5064}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4943}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Resource-Efficient%20Data-Free%20Knowledge%20Distillation%20by%0A%20%20Generative%20Pseudo%20Replay&body=Title%3A%20Robust%20and%20Resource-Efficient%20Data-Free%20Knowledge%20Distillation%20by%0A%20%20Generative%20Pseudo%20Replay%0AAuthor%3A%20Kuluhan%20Binici%20and%20Shivam%20Aggarwal%20and%20Nam%20Trung%20Pham%20and%20Karianto%20Leman%20and%20Tulika%20Mitra%0AAbstract%3A%20%20%20Data-Free%20Knowledge%20Distillation%20%28KD%29%20allows%20knowledge%20transfer%20from%20a%0Atrained%20neural%20network%20%28teacher%29%20to%20a%20more%20compact%20one%20%28student%29%20in%20the%20absence%0Aof%20original%20training%20data.%20Existing%20works%20use%20a%20validation%20set%20to%20monitor%20the%0Aaccuracy%20of%20the%20student%20over%20real%20data%20and%20report%20the%20highest%20performance%0Athroughout%20the%20entire%20process.%20However%2C%20validation%20data%20may%20not%20be%20available%20at%0Adistillation%20time%20either%2C%20making%20it%20infeasible%20to%20record%20the%20student%20snapshot%0Athat%20achieved%20the%20peak%20accuracy.%20Therefore%2C%20a%20practical%20data-free%20KD%20method%0Ashould%20be%20robust%20and%20ideally%20provide%20monotonically%20increasing%20student%20accuracy%0Aduring%20distillation.%20This%20is%20challenging%20because%20the%20student%20experiences%0Aknowledge%20degradation%20due%20to%20the%20distribution%20shift%20of%20the%20synthetic%20data.%20A%0Astraightforward%20approach%20to%20overcome%20this%20issue%20is%20to%20store%20and%20rehearse%20the%0Agenerated%20samples%20periodically%2C%20which%20increases%20the%20memory%20footprint%20and%0Acreates%20privacy%20concerns.%20We%20propose%20to%20model%20the%20distribution%20of%20the%0Apreviously%20observed%20synthetic%20samples%20with%20a%20generative%20network.%20In%20particular%2C%0Awe%20design%20a%20Variational%20Autoencoder%20%28VAE%29%20with%20a%20training%20objective%20that%20is%0Acustomized%20to%20learn%20the%20synthetic%20data%20representations%20optimally.%20The%20student%0Ais%20rehearsed%20by%20the%20generative%20pseudo%20replay%20technique%2C%20with%20samples%20produced%0Aby%20the%20VAE.%20Hence%20knowledge%20degradation%20can%20be%20prevented%20without%20storing%20any%0Asamples.%20Experiments%20on%20image%20classification%20benchmarks%20show%20that%20our%20method%0Aoptimizes%20the%20expected%20value%20of%20the%20distilled%20model%20accuracy%20while%20eliminating%0Athe%20large%20memory%20overhead%20incurred%20by%20the%20sample-storing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.03019v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Resource-Efficient%2520Data-Free%2520Knowledge%2520Distillation%2520by%250A%2520%2520Generative%2520Pseudo%2520Replay%26entry.906535625%3DKuluhan%2520Binici%2520and%2520Shivam%2520Aggarwal%2520and%2520Nam%2520Trung%2520Pham%2520and%2520Karianto%2520Leman%2520and%2520Tulika%2520Mitra%26entry.1292438233%3D%2520%2520Data-Free%2520Knowledge%2520Distillation%2520%2528KD%2529%2520allows%2520knowledge%2520transfer%2520from%2520a%250Atrained%2520neural%2520network%2520%2528teacher%2529%2520to%2520a%2520more%2520compact%2520one%2520%2528student%2529%2520in%2520the%2520absence%250Aof%2520original%2520training%2520data.%2520Existing%2520works%2520use%2520a%2520validation%2520set%2520to%2520monitor%2520the%250Aaccuracy%2520of%2520the%2520student%2520over%2520real%2520data%2520and%2520report%2520the%2520highest%2520performance%250Athroughout%2520the%2520entire%2520process.%2520However%252C%2520validation%2520data%2520may%2520not%2520be%2520available%2520at%250Adistillation%2520time%2520either%252C%2520making%2520it%2520infeasible%2520to%2520record%2520the%2520student%2520snapshot%250Athat%2520achieved%2520the%2520peak%2520accuracy.%2520Therefore%252C%2520a%2520practical%2520data-free%2520KD%2520method%250Ashould%2520be%2520robust%2520and%2520ideally%2520provide%2520monotonically%2520increasing%2520student%2520accuracy%250Aduring%2520distillation.%2520This%2520is%2520challenging%2520because%2520the%2520student%2520experiences%250Aknowledge%2520degradation%2520due%2520to%2520the%2520distribution%2520shift%2520of%2520the%2520synthetic%2520data.%2520A%250Astraightforward%2520approach%2520to%2520overcome%2520this%2520issue%2520is%2520to%2520store%2520and%2520rehearse%2520the%250Agenerated%2520samples%2520periodically%252C%2520which%2520increases%2520the%2520memory%2520footprint%2520and%250Acreates%2520privacy%2520concerns.%2520We%2520propose%2520to%2520model%2520the%2520distribution%2520of%2520the%250Apreviously%2520observed%2520synthetic%2520samples%2520with%2520a%2520generative%2520network.%2520In%2520particular%252C%250Awe%2520design%2520a%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520with%2520a%2520training%2520objective%2520that%2520is%250Acustomized%2520to%2520learn%2520the%2520synthetic%2520data%2520representations%2520optimally.%2520The%2520student%250Ais%2520rehearsed%2520by%2520the%2520generative%2520pseudo%2520replay%2520technique%252C%2520with%2520samples%2520produced%250Aby%2520the%2520VAE.%2520Hence%2520knowledge%2520degradation%2520can%2520be%2520prevented%2520without%2520storing%2520any%250Asamples.%2520Experiments%2520on%2520image%2520classification%2520benchmarks%2520show%2520that%2520our%2520method%250Aoptimizes%2520the%2520expected%2520value%2520of%2520the%2520distilled%2520model%2520accuracy%2520while%2520eliminating%250Athe%2520large%2520memory%2520overhead%2520incurred%2520by%2520the%2520sample-storing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.03019v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Resource-Efficient%20Data-Free%20Knowledge%20Distillation%20by%0A%20%20Generative%20Pseudo%20Replay&entry.906535625=Kuluhan%20Binici%20and%20Shivam%20Aggarwal%20and%20Nam%20Trung%20Pham%20and%20Karianto%20Leman%20and%20Tulika%20Mitra&entry.1292438233=%20%20Data-Free%20Knowledge%20Distillation%20%28KD%29%20allows%20knowledge%20transfer%20from%20a%0Atrained%20neural%20network%20%28teacher%29%20to%20a%20more%20compact%20one%20%28student%29%20in%20the%20absence%0Aof%20original%20training%20data.%20Existing%20works%20use%20a%20validation%20set%20to%20monitor%20the%0Aaccuracy%20of%20the%20student%20over%20real%20data%20and%20report%20the%20highest%20performance%0Athroughout%20the%20entire%20process.%20However%2C%20validation%20data%20may%20not%20be%20available%20at%0Adistillation%20time%20either%2C%20making%20it%20infeasible%20to%20record%20the%20student%20snapshot%0Athat%20achieved%20the%20peak%20accuracy.%20Therefore%2C%20a%20practical%20data-free%20KD%20method%0Ashould%20be%20robust%20and%20ideally%20provide%20monotonically%20increasing%20student%20accuracy%0Aduring%20distillation.%20This%20is%20challenging%20because%20the%20student%20experiences%0Aknowledge%20degradation%20due%20to%20the%20distribution%20shift%20of%20the%20synthetic%20data.%20A%0Astraightforward%20approach%20to%20overcome%20this%20issue%20is%20to%20store%20and%20rehearse%20the%0Agenerated%20samples%20periodically%2C%20which%20increases%20the%20memory%20footprint%20and%0Acreates%20privacy%20concerns.%20We%20propose%20to%20model%20the%20distribution%20of%20the%0Apreviously%20observed%20synthetic%20samples%20with%20a%20generative%20network.%20In%20particular%2C%0Awe%20design%20a%20Variational%20Autoencoder%20%28VAE%29%20with%20a%20training%20objective%20that%20is%0Acustomized%20to%20learn%20the%20synthetic%20data%20representations%20optimally.%20The%20student%0Ais%20rehearsed%20by%20the%20generative%20pseudo%20replay%20technique%2C%20with%20samples%20produced%0Aby%20the%20VAE.%20Hence%20knowledge%20degradation%20can%20be%20prevented%20without%20storing%20any%0Asamples.%20Experiments%20on%20image%20classification%20benchmarks%20show%20that%20our%20method%0Aoptimizes%20the%20expected%20value%20of%20the%20distilled%20model%20accuracy%20while%20eliminating%0Athe%20large%20memory%20overhead%20incurred%20by%20the%20sample-storing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.03019v3&entry.124074799=Read"},
{"title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval", "author": "Hongming Tan and Shaoxiong Zhan and Hai Lin and Hai-Tao Zheng and Wai Kin and  Chan", "abstract": "  In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.\n", "link": "http://arxiv.org/abs/2407.20207v1", "date": "2024-07-29", "relevancy": 1.9936, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5275}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4931}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QAEA-DR%3A%20A%20Unified%20Text%20Augmentation%20Framework%20for%20Dense%20Retrieval&body=Title%3A%20QAEA-DR%3A%20A%20Unified%20Text%20Augmentation%20Framework%20for%20Dense%20Retrieval%0AAuthor%3A%20Hongming%20Tan%20and%20Shaoxiong%20Zhan%20and%20Hai%20Lin%20and%20Hai-Tao%20Zheng%20and%20Wai%20Kin%20and%20%20Chan%0AAbstract%3A%20%20%20In%20dense%20retrieval%2C%20embedding%20long%20texts%20into%20dense%20vectors%20can%20result%20in%0Ainformation%20loss%2C%20leading%20to%20inaccurate%20query-text%20matching.%20Additionally%2C%0Alow-quality%20texts%20with%20excessive%20noise%20or%20sparse%20key%20information%20are%20unlikely%0Ato%20align%20well%20with%20relevant%20queries.%20Recent%20studies%20mainly%20focus%20on%20improving%0Athe%20sentence%20embedding%20model%20or%20retrieval%20process.%20In%20this%20work%2C%20we%20introduce%20a%0Anovel%20text%20augmentation%20framework%20for%20dense%20retrieval.%20This%20framework%0Atransforms%20raw%20documents%20into%20information-dense%20text%20formats%2C%20which%20supplement%0Athe%20original%20texts%20to%20effectively%20address%20the%20aforementioned%20issues%20without%0Amodifying%20embedding%20or%20retrieval%20methodologies.%20Two%20text%20representations%20are%0Agenerated%20via%20large%20language%20models%20%28LLMs%29%20zero-shot%20prompting%3A%20question-answer%0Apairs%20and%20element-driven%20events.%20We%20term%20this%20approach%20QAEA-DR%3A%20unifying%0Aquestion-answer%20generation%20and%20event%20extraction%20in%20a%20text%20augmentation%0Aframework%20for%20dense%20retrieval.%20To%20further%20enhance%20the%20quality%20of%20generated%0Atexts%2C%20a%20scoring-based%20evaluation%20and%20regeneration%20mechanism%20is%20introduced%20in%0ALLM%20prompting.%20Our%20QAEA-DR%20model%20has%20a%20positive%20impact%20on%20dense%20retrieval%2C%0Asupported%20by%20both%20theoretical%20analysis%20and%20empirical%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQAEA-DR%253A%2520A%2520Unified%2520Text%2520Augmentation%2520Framework%2520for%2520Dense%2520Retrieval%26entry.906535625%3DHongming%2520Tan%2520and%2520Shaoxiong%2520Zhan%2520and%2520Hai%2520Lin%2520and%2520Hai-Tao%2520Zheng%2520and%2520Wai%2520Kin%2520and%2520%2520Chan%26entry.1292438233%3D%2520%2520In%2520dense%2520retrieval%252C%2520embedding%2520long%2520texts%2520into%2520dense%2520vectors%2520can%2520result%2520in%250Ainformation%2520loss%252C%2520leading%2520to%2520inaccurate%2520query-text%2520matching.%2520Additionally%252C%250Alow-quality%2520texts%2520with%2520excessive%2520noise%2520or%2520sparse%2520key%2520information%2520are%2520unlikely%250Ato%2520align%2520well%2520with%2520relevant%2520queries.%2520Recent%2520studies%2520mainly%2520focus%2520on%2520improving%250Athe%2520sentence%2520embedding%2520model%2520or%2520retrieval%2520process.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Anovel%2520text%2520augmentation%2520framework%2520for%2520dense%2520retrieval.%2520This%2520framework%250Atransforms%2520raw%2520documents%2520into%2520information-dense%2520text%2520formats%252C%2520which%2520supplement%250Athe%2520original%2520texts%2520to%2520effectively%2520address%2520the%2520aforementioned%2520issues%2520without%250Amodifying%2520embedding%2520or%2520retrieval%2520methodologies.%2520Two%2520text%2520representations%2520are%250Agenerated%2520via%2520large%2520language%2520models%2520%2528LLMs%2529%2520zero-shot%2520prompting%253A%2520question-answer%250Apairs%2520and%2520element-driven%2520events.%2520We%2520term%2520this%2520approach%2520QAEA-DR%253A%2520unifying%250Aquestion-answer%2520generation%2520and%2520event%2520extraction%2520in%2520a%2520text%2520augmentation%250Aframework%2520for%2520dense%2520retrieval.%2520To%2520further%2520enhance%2520the%2520quality%2520of%2520generated%250Atexts%252C%2520a%2520scoring-based%2520evaluation%2520and%2520regeneration%2520mechanism%2520is%2520introduced%2520in%250ALLM%2520prompting.%2520Our%2520QAEA-DR%2520model%2520has%2520a%2520positive%2520impact%2520on%2520dense%2520retrieval%252C%250Asupported%2520by%2520both%2520theoretical%2520analysis%2520and%2520empirical%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QAEA-DR%3A%20A%20Unified%20Text%20Augmentation%20Framework%20for%20Dense%20Retrieval&entry.906535625=Hongming%20Tan%20and%20Shaoxiong%20Zhan%20and%20Hai%20Lin%20and%20Hai-Tao%20Zheng%20and%20Wai%20Kin%20and%20%20Chan&entry.1292438233=%20%20In%20dense%20retrieval%2C%20embedding%20long%20texts%20into%20dense%20vectors%20can%20result%20in%0Ainformation%20loss%2C%20leading%20to%20inaccurate%20query-text%20matching.%20Additionally%2C%0Alow-quality%20texts%20with%20excessive%20noise%20or%20sparse%20key%20information%20are%20unlikely%0Ato%20align%20well%20with%20relevant%20queries.%20Recent%20studies%20mainly%20focus%20on%20improving%0Athe%20sentence%20embedding%20model%20or%20retrieval%20process.%20In%20this%20work%2C%20we%20introduce%20a%0Anovel%20text%20augmentation%20framework%20for%20dense%20retrieval.%20This%20framework%0Atransforms%20raw%20documents%20into%20information-dense%20text%20formats%2C%20which%20supplement%0Athe%20original%20texts%20to%20effectively%20address%20the%20aforementioned%20issues%20without%0Amodifying%20embedding%20or%20retrieval%20methodologies.%20Two%20text%20representations%20are%0Agenerated%20via%20large%20language%20models%20%28LLMs%29%20zero-shot%20prompting%3A%20question-answer%0Apairs%20and%20element-driven%20events.%20We%20term%20this%20approach%20QAEA-DR%3A%20unifying%0Aquestion-answer%20generation%20and%20event%20extraction%20in%20a%20text%20augmentation%0Aframework%20for%20dense%20retrieval.%20To%20further%20enhance%20the%20quality%20of%20generated%0Atexts%2C%20a%20scoring-based%20evaluation%20and%20regeneration%20mechanism%20is%20introduced%20in%0ALLM%20prompting.%20Our%20QAEA-DR%20model%20has%20a%20positive%20impact%20on%20dense%20retrieval%2C%0Asupported%20by%20both%20theoretical%20analysis%20and%20empirical%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20207v1&entry.124074799=Read"},
{"title": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement\n  Learning", "author": "Liyuan Mao and Haoran Xu and Weinan Zhang and Xianyuan Zhan and Amy Zhang", "abstract": "  One important property of DIstribution Correction Estimation (DICE) methods\nis that the solution is the optimal stationary distribution ratio between the\noptimized and data collection policy. In this work, we show that DICE-based\nmethods can be viewed as a transformation from the behavior distribution to the\noptimal policy distribution. Based on this, we propose a novel approach,\nDiffusion-DICE, that directly performs this transformation using diffusion\nmodels. We find that the optimal policy's score function can be decomposed into\ntwo terms: the behavior policy's score function and the gradient of a guidance\nterm which depends on the optimal distribution ratio. The first term can be\nobtained from a diffusion model trained on the dataset and we propose an\nin-sample learning objective to learn the second term. Due to the\nmulti-modality contained in the optimal policy distribution, the transformation\nin Diffusion-DICE may guide towards those local-optimal modes. We thus generate\na few candidate actions and carefully select from them to approach\nglobal-optimum. Different from all other diffusion-based offline RL methods,\nthe guide-then-select paradigm in Diffusion-DICE only uses in-sample actions\nfor training and brings minimal error exploitation in the value function. We\nuse a didatic toycase example to show how previous diffusion-based methods fail\nto generate optimal actions due to leveraging these errors and how\nDiffusion-DICE successfully avoids that. We then conduct extensive experiments\non benchmark datasets to show the strong performance of Diffusion-DICE.\n", "link": "http://arxiv.org/abs/2407.20109v1", "date": "2024-07-29", "relevancy": 1.9935, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4937}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-DICE%3A%20In-Sample%20Diffusion%20Guidance%20for%20Offline%20Reinforcement%0A%20%20Learning&body=Title%3A%20Diffusion-DICE%3A%20In-Sample%20Diffusion%20Guidance%20for%20Offline%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Liyuan%20Mao%20and%20Haoran%20Xu%20and%20Weinan%20Zhang%20and%20Xianyuan%20Zhan%20and%20Amy%20Zhang%0AAbstract%3A%20%20%20One%20important%20property%20of%20DIstribution%20Correction%20Estimation%20%28DICE%29%20methods%0Ais%20that%20the%20solution%20is%20the%20optimal%20stationary%20distribution%20ratio%20between%20the%0Aoptimized%20and%20data%20collection%20policy.%20In%20this%20work%2C%20we%20show%20that%20DICE-based%0Amethods%20can%20be%20viewed%20as%20a%20transformation%20from%20the%20behavior%20distribution%20to%20the%0Aoptimal%20policy%20distribution.%20Based%20on%20this%2C%20we%20propose%20a%20novel%20approach%2C%0ADiffusion-DICE%2C%20that%20directly%20performs%20this%20transformation%20using%20diffusion%0Amodels.%20We%20find%20that%20the%20optimal%20policy%27s%20score%20function%20can%20be%20decomposed%20into%0Atwo%20terms%3A%20the%20behavior%20policy%27s%20score%20function%20and%20the%20gradient%20of%20a%20guidance%0Aterm%20which%20depends%20on%20the%20optimal%20distribution%20ratio.%20The%20first%20term%20can%20be%0Aobtained%20from%20a%20diffusion%20model%20trained%20on%20the%20dataset%20and%20we%20propose%20an%0Ain-sample%20learning%20objective%20to%20learn%20the%20second%20term.%20Due%20to%20the%0Amulti-modality%20contained%20in%20the%20optimal%20policy%20distribution%2C%20the%20transformation%0Ain%20Diffusion-DICE%20may%20guide%20towards%20those%20local-optimal%20modes.%20We%20thus%20generate%0Aa%20few%20candidate%20actions%20and%20carefully%20select%20from%20them%20to%20approach%0Aglobal-optimum.%20Different%20from%20all%20other%20diffusion-based%20offline%20RL%20methods%2C%0Athe%20guide-then-select%20paradigm%20in%20Diffusion-DICE%20only%20uses%20in-sample%20actions%0Afor%20training%20and%20brings%20minimal%20error%20exploitation%20in%20the%20value%20function.%20We%0Ause%20a%20didatic%20toycase%20example%20to%20show%20how%20previous%20diffusion-based%20methods%20fail%0Ato%20generate%20optimal%20actions%20due%20to%20leveraging%20these%20errors%20and%20how%0ADiffusion-DICE%20successfully%20avoids%20that.%20We%20then%20conduct%20extensive%20experiments%0Aon%20benchmark%20datasets%20to%20show%20the%20strong%20performance%20of%20Diffusion-DICE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-DICE%253A%2520In-Sample%2520Diffusion%2520Guidance%2520for%2520Offline%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DLiyuan%2520Mao%2520and%2520Haoran%2520Xu%2520and%2520Weinan%2520Zhang%2520and%2520Xianyuan%2520Zhan%2520and%2520Amy%2520Zhang%26entry.1292438233%3D%2520%2520One%2520important%2520property%2520of%2520DIstribution%2520Correction%2520Estimation%2520%2528DICE%2529%2520methods%250Ais%2520that%2520the%2520solution%2520is%2520the%2520optimal%2520stationary%2520distribution%2520ratio%2520between%2520the%250Aoptimized%2520and%2520data%2520collection%2520policy.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520DICE-based%250Amethods%2520can%2520be%2520viewed%2520as%2520a%2520transformation%2520from%2520the%2520behavior%2520distribution%2520to%2520the%250Aoptimal%2520policy%2520distribution.%2520Based%2520on%2520this%252C%2520we%2520propose%2520a%2520novel%2520approach%252C%250ADiffusion-DICE%252C%2520that%2520directly%2520performs%2520this%2520transformation%2520using%2520diffusion%250Amodels.%2520We%2520find%2520that%2520the%2520optimal%2520policy%2527s%2520score%2520function%2520can%2520be%2520decomposed%2520into%250Atwo%2520terms%253A%2520the%2520behavior%2520policy%2527s%2520score%2520function%2520and%2520the%2520gradient%2520of%2520a%2520guidance%250Aterm%2520which%2520depends%2520on%2520the%2520optimal%2520distribution%2520ratio.%2520The%2520first%2520term%2520can%2520be%250Aobtained%2520from%2520a%2520diffusion%2520model%2520trained%2520on%2520the%2520dataset%2520and%2520we%2520propose%2520an%250Ain-sample%2520learning%2520objective%2520to%2520learn%2520the%2520second%2520term.%2520Due%2520to%2520the%250Amulti-modality%2520contained%2520in%2520the%2520optimal%2520policy%2520distribution%252C%2520the%2520transformation%250Ain%2520Diffusion-DICE%2520may%2520guide%2520towards%2520those%2520local-optimal%2520modes.%2520We%2520thus%2520generate%250Aa%2520few%2520candidate%2520actions%2520and%2520carefully%2520select%2520from%2520them%2520to%2520approach%250Aglobal-optimum.%2520Different%2520from%2520all%2520other%2520diffusion-based%2520offline%2520RL%2520methods%252C%250Athe%2520guide-then-select%2520paradigm%2520in%2520Diffusion-DICE%2520only%2520uses%2520in-sample%2520actions%250Afor%2520training%2520and%2520brings%2520minimal%2520error%2520exploitation%2520in%2520the%2520value%2520function.%2520We%250Ause%2520a%2520didatic%2520toycase%2520example%2520to%2520show%2520how%2520previous%2520diffusion-based%2520methods%2520fail%250Ato%2520generate%2520optimal%2520actions%2520due%2520to%2520leveraging%2520these%2520errors%2520and%2520how%250ADiffusion-DICE%2520successfully%2520avoids%2520that.%2520We%2520then%2520conduct%2520extensive%2520experiments%250Aon%2520benchmark%2520datasets%2520to%2520show%2520the%2520strong%2520performance%2520of%2520Diffusion-DICE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-DICE%3A%20In-Sample%20Diffusion%20Guidance%20for%20Offline%20Reinforcement%0A%20%20Learning&entry.906535625=Liyuan%20Mao%20and%20Haoran%20Xu%20and%20Weinan%20Zhang%20and%20Xianyuan%20Zhan%20and%20Amy%20Zhang&entry.1292438233=%20%20One%20important%20property%20of%20DIstribution%20Correction%20Estimation%20%28DICE%29%20methods%0Ais%20that%20the%20solution%20is%20the%20optimal%20stationary%20distribution%20ratio%20between%20the%0Aoptimized%20and%20data%20collection%20policy.%20In%20this%20work%2C%20we%20show%20that%20DICE-based%0Amethods%20can%20be%20viewed%20as%20a%20transformation%20from%20the%20behavior%20distribution%20to%20the%0Aoptimal%20policy%20distribution.%20Based%20on%20this%2C%20we%20propose%20a%20novel%20approach%2C%0ADiffusion-DICE%2C%20that%20directly%20performs%20this%20transformation%20using%20diffusion%0Amodels.%20We%20find%20that%20the%20optimal%20policy%27s%20score%20function%20can%20be%20decomposed%20into%0Atwo%20terms%3A%20the%20behavior%20policy%27s%20score%20function%20and%20the%20gradient%20of%20a%20guidance%0Aterm%20which%20depends%20on%20the%20optimal%20distribution%20ratio.%20The%20first%20term%20can%20be%0Aobtained%20from%20a%20diffusion%20model%20trained%20on%20the%20dataset%20and%20we%20propose%20an%0Ain-sample%20learning%20objective%20to%20learn%20the%20second%20term.%20Due%20to%20the%0Amulti-modality%20contained%20in%20the%20optimal%20policy%20distribution%2C%20the%20transformation%0Ain%20Diffusion-DICE%20may%20guide%20towards%20those%20local-optimal%20modes.%20We%20thus%20generate%0Aa%20few%20candidate%20actions%20and%20carefully%20select%20from%20them%20to%20approach%0Aglobal-optimum.%20Different%20from%20all%20other%20diffusion-based%20offline%20RL%20methods%2C%0Athe%20guide-then-select%20paradigm%20in%20Diffusion-DICE%20only%20uses%20in-sample%20actions%0Afor%20training%20and%20brings%20minimal%20error%20exploitation%20in%20the%20value%20function.%20We%0Ause%20a%20didatic%20toycase%20example%20to%20show%20how%20previous%20diffusion-based%20methods%20fail%0Ato%20generate%20optimal%20actions%20due%20to%20leveraging%20these%20errors%20and%20how%0ADiffusion-DICE%20successfully%20avoids%20that.%20We%20then%20conduct%20extensive%20experiments%0Aon%20benchmark%20datasets%20to%20show%20the%20strong%20performance%20of%20Diffusion-DICE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20109v1&entry.124074799=Read"},
{"title": "Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop\n  Feature Quality Estimation", "author": "Shiyuan Li and Yixin Liu and Qingfeng Chen and Geoffrey I. Webb and Shirui Pan", "abstract": "  Unsupervised graph representation learning (UGRL) based on graph neural\nnetworks (GNNs), has received increasing attention owing to its efficacy in\nhandling graph-structured data. However, existing UGRL methods ideally assume\nthat the node features are noise-free, which makes them fail to distinguish\nbetween useful information and noise when applied to real data with noisy\nfeatures, thus affecting the quality of learned representations. This urges us\nto take node noisy features into account in real-world UGRL. With empirical\nanalysis, we reveal that feature propagation, the essential operation in GNNs,\nacts as a \"double-edged sword\" in handling noisy features - it can both denoise\nand diffuse noise, leading to varying feature quality across nodes, even within\nthe same node at different hops. Building on this insight, we propose a novel\nUGRL method based on Multi-hop feature Quality Estimation (MQE for short).\nUnlike most UGRL models that directly utilize propagation-based GNNs to\ngenerate representations, our approach aims to learn representations through\nestimating the quality of propagated features at different hops. Specifically,\nwe introduce a Gaussian model that utilizes a learnable \"meta-representation\"\nas a condition to estimate the expectation and variance of multi-hop propagated\nfeatures via neural networks. In this way, the \"meta representation\" captures\nthe semantic and structural information underlying multiple propagated features\nbut is naturally less susceptible to interference by noise, thereby serving as\nhigh-quality node representations beneficial for downstream tasks. Extensive\nexperiments on multiple real-world datasets demonstrate that MQE in learning\nreliable node representations in scenarios with diverse types of feature noise.\n", "link": "http://arxiv.org/abs/2407.19944v1", "date": "2024-07-29", "relevancy": 1.9911, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5103}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4917}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise-Resilient%20Unsupervised%20Graph%20Representation%20Learning%20via%20Multi-Hop%0A%20%20Feature%20Quality%20Estimation&body=Title%3A%20Noise-Resilient%20Unsupervised%20Graph%20Representation%20Learning%20via%20Multi-Hop%0A%20%20Feature%20Quality%20Estimation%0AAuthor%3A%20Shiyuan%20Li%20and%20Yixin%20Liu%20and%20Qingfeng%20Chen%20and%20Geoffrey%20I.%20Webb%20and%20Shirui%20Pan%0AAbstract%3A%20%20%20Unsupervised%20graph%20representation%20learning%20%28UGRL%29%20based%20on%20graph%20neural%0Anetworks%20%28GNNs%29%2C%20has%20received%20increasing%20attention%20owing%20to%20its%20efficacy%20in%0Ahandling%20graph-structured%20data.%20However%2C%20existing%20UGRL%20methods%20ideally%20assume%0Athat%20the%20node%20features%20are%20noise-free%2C%20which%20makes%20them%20fail%20to%20distinguish%0Abetween%20useful%20information%20and%20noise%20when%20applied%20to%20real%20data%20with%20noisy%0Afeatures%2C%20thus%20affecting%20the%20quality%20of%20learned%20representations.%20This%20urges%20us%0Ato%20take%20node%20noisy%20features%20into%20account%20in%20real-world%20UGRL.%20With%20empirical%0Aanalysis%2C%20we%20reveal%20that%20feature%20propagation%2C%20the%20essential%20operation%20in%20GNNs%2C%0Aacts%20as%20a%20%22double-edged%20sword%22%20in%20handling%20noisy%20features%20-%20it%20can%20both%20denoise%0Aand%20diffuse%20noise%2C%20leading%20to%20varying%20feature%20quality%20across%20nodes%2C%20even%20within%0Athe%20same%20node%20at%20different%20hops.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20novel%0AUGRL%20method%20based%20on%20Multi-hop%20feature%20Quality%20Estimation%20%28MQE%20for%20short%29.%0AUnlike%20most%20UGRL%20models%20that%20directly%20utilize%20propagation-based%20GNNs%20to%0Agenerate%20representations%2C%20our%20approach%20aims%20to%20learn%20representations%20through%0Aestimating%20the%20quality%20of%20propagated%20features%20at%20different%20hops.%20Specifically%2C%0Awe%20introduce%20a%20Gaussian%20model%20that%20utilizes%20a%20learnable%20%22meta-representation%22%0Aas%20a%20condition%20to%20estimate%20the%20expectation%20and%20variance%20of%20multi-hop%20propagated%0Afeatures%20via%20neural%20networks.%20In%20this%20way%2C%20the%20%22meta%20representation%22%20captures%0Athe%20semantic%20and%20structural%20information%20underlying%20multiple%20propagated%20features%0Abut%20is%20naturally%20less%20susceptible%20to%20interference%20by%20noise%2C%20thereby%20serving%20as%0Ahigh-quality%20node%20representations%20beneficial%20for%20downstream%20tasks.%20Extensive%0Aexperiments%20on%20multiple%20real-world%20datasets%20demonstrate%20that%20MQE%20in%20learning%0Areliable%20node%20representations%20in%20scenarios%20with%20diverse%20types%20of%20feature%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise-Resilient%2520Unsupervised%2520Graph%2520Representation%2520Learning%2520via%2520Multi-Hop%250A%2520%2520Feature%2520Quality%2520Estimation%26entry.906535625%3DShiyuan%2520Li%2520and%2520Yixin%2520Liu%2520and%2520Qingfeng%2520Chen%2520and%2520Geoffrey%2520I.%2520Webb%2520and%2520Shirui%2520Pan%26entry.1292438233%3D%2520%2520Unsupervised%2520graph%2520representation%2520learning%2520%2528UGRL%2529%2520based%2520on%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%252C%2520has%2520received%2520increasing%2520attention%2520owing%2520to%2520its%2520efficacy%2520in%250Ahandling%2520graph-structured%2520data.%2520However%252C%2520existing%2520UGRL%2520methods%2520ideally%2520assume%250Athat%2520the%2520node%2520features%2520are%2520noise-free%252C%2520which%2520makes%2520them%2520fail%2520to%2520distinguish%250Abetween%2520useful%2520information%2520and%2520noise%2520when%2520applied%2520to%2520real%2520data%2520with%2520noisy%250Afeatures%252C%2520thus%2520affecting%2520the%2520quality%2520of%2520learned%2520representations.%2520This%2520urges%2520us%250Ato%2520take%2520node%2520noisy%2520features%2520into%2520account%2520in%2520real-world%2520UGRL.%2520With%2520empirical%250Aanalysis%252C%2520we%2520reveal%2520that%2520feature%2520propagation%252C%2520the%2520essential%2520operation%2520in%2520GNNs%252C%250Aacts%2520as%2520a%2520%2522double-edged%2520sword%2522%2520in%2520handling%2520noisy%2520features%2520-%2520it%2520can%2520both%2520denoise%250Aand%2520diffuse%2520noise%252C%2520leading%2520to%2520varying%2520feature%2520quality%2520across%2520nodes%252C%2520even%2520within%250Athe%2520same%2520node%2520at%2520different%2520hops.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%2520novel%250AUGRL%2520method%2520based%2520on%2520Multi-hop%2520feature%2520Quality%2520Estimation%2520%2528MQE%2520for%2520short%2529.%250AUnlike%2520most%2520UGRL%2520models%2520that%2520directly%2520utilize%2520propagation-based%2520GNNs%2520to%250Agenerate%2520representations%252C%2520our%2520approach%2520aims%2520to%2520learn%2520representations%2520through%250Aestimating%2520the%2520quality%2520of%2520propagated%2520features%2520at%2520different%2520hops.%2520Specifically%252C%250Awe%2520introduce%2520a%2520Gaussian%2520model%2520that%2520utilizes%2520a%2520learnable%2520%2522meta-representation%2522%250Aas%2520a%2520condition%2520to%2520estimate%2520the%2520expectation%2520and%2520variance%2520of%2520multi-hop%2520propagated%250Afeatures%2520via%2520neural%2520networks.%2520In%2520this%2520way%252C%2520the%2520%2522meta%2520representation%2522%2520captures%250Athe%2520semantic%2520and%2520structural%2520information%2520underlying%2520multiple%2520propagated%2520features%250Abut%2520is%2520naturally%2520less%2520susceptible%2520to%2520interference%2520by%2520noise%252C%2520thereby%2520serving%2520as%250Ahigh-quality%2520node%2520representations%2520beneficial%2520for%2520downstream%2520tasks.%2520Extensive%250Aexperiments%2520on%2520multiple%2520real-world%2520datasets%2520demonstrate%2520that%2520MQE%2520in%2520learning%250Areliable%2520node%2520representations%2520in%2520scenarios%2520with%2520diverse%2520types%2520of%2520feature%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise-Resilient%20Unsupervised%20Graph%20Representation%20Learning%20via%20Multi-Hop%0A%20%20Feature%20Quality%20Estimation&entry.906535625=Shiyuan%20Li%20and%20Yixin%20Liu%20and%20Qingfeng%20Chen%20and%20Geoffrey%20I.%20Webb%20and%20Shirui%20Pan&entry.1292438233=%20%20Unsupervised%20graph%20representation%20learning%20%28UGRL%29%20based%20on%20graph%20neural%0Anetworks%20%28GNNs%29%2C%20has%20received%20increasing%20attention%20owing%20to%20its%20efficacy%20in%0Ahandling%20graph-structured%20data.%20However%2C%20existing%20UGRL%20methods%20ideally%20assume%0Athat%20the%20node%20features%20are%20noise-free%2C%20which%20makes%20them%20fail%20to%20distinguish%0Abetween%20useful%20information%20and%20noise%20when%20applied%20to%20real%20data%20with%20noisy%0Afeatures%2C%20thus%20affecting%20the%20quality%20of%20learned%20representations.%20This%20urges%20us%0Ato%20take%20node%20noisy%20features%20into%20account%20in%20real-world%20UGRL.%20With%20empirical%0Aanalysis%2C%20we%20reveal%20that%20feature%20propagation%2C%20the%20essential%20operation%20in%20GNNs%2C%0Aacts%20as%20a%20%22double-edged%20sword%22%20in%20handling%20noisy%20features%20-%20it%20can%20both%20denoise%0Aand%20diffuse%20noise%2C%20leading%20to%20varying%20feature%20quality%20across%20nodes%2C%20even%20within%0Athe%20same%20node%20at%20different%20hops.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20novel%0AUGRL%20method%20based%20on%20Multi-hop%20feature%20Quality%20Estimation%20%28MQE%20for%20short%29.%0AUnlike%20most%20UGRL%20models%20that%20directly%20utilize%20propagation-based%20GNNs%20to%0Agenerate%20representations%2C%20our%20approach%20aims%20to%20learn%20representations%20through%0Aestimating%20the%20quality%20of%20propagated%20features%20at%20different%20hops.%20Specifically%2C%0Awe%20introduce%20a%20Gaussian%20model%20that%20utilizes%20a%20learnable%20%22meta-representation%22%0Aas%20a%20condition%20to%20estimate%20the%20expectation%20and%20variance%20of%20multi-hop%20propagated%0Afeatures%20via%20neural%20networks.%20In%20this%20way%2C%20the%20%22meta%20representation%22%20captures%0Athe%20semantic%20and%20structural%20information%20underlying%20multiple%20propagated%20features%0Abut%20is%20naturally%20less%20susceptible%20to%20interference%20by%20noise%2C%20thereby%20serving%20as%0Ahigh-quality%20node%20representations%20beneficial%20for%20downstream%20tasks.%20Extensive%0Aexperiments%20on%20multiple%20real-world%20datasets%20demonstrate%20that%20MQE%20in%20learning%0Areliable%20node%20representations%20in%20scenarios%20with%20diverse%20types%20of%20feature%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19944v1&entry.124074799=Read"},
{"title": "Generalized Groves of Neural Additive Models: Pursuing transparent and\n  accurate machine learning models in finance", "author": "Dangxing Chen and Weicheng Ye", "abstract": "  While machine learning methods have significantly improved model performance\nover traditional methods, their black-box structure makes it difficult for\nresearchers to interpret results. For highly regulated financial industries,\nmodel transparency is equally important to accuracy. Without understanding how\nmodels work, even highly accurate machine learning methods are unlikely to be\naccepted. We address this issue by introducing a novel class of transparent\nmachine learning models known as generalized groves of neural additive models.\nThe generalized groves of neural additive models separate features into three\ncategories: linear features, individual nonlinear features, and interacted\nnonlinear features. Additionally, interactions in the last category are only\nlocal. A stepwise selection algorithm distinguishes the linear and nonlinear\ncomponents, and interacted groups are carefully verified by applying additive\nseparation criteria. Through some empirical examples in finance, we demonstrate\nthat generalized grove of neural additive models exhibit high accuracy and\ntransparency with predominantly linear terms and only sparse nonlinear ones.\n", "link": "http://arxiv.org/abs/2209.10082v2", "date": "2024-07-29", "relevancy": 1.9874, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.505}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4911}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Groves%20of%20Neural%20Additive%20Models%3A%20Pursuing%20transparent%20and%0A%20%20accurate%20machine%20learning%20models%20in%20finance&body=Title%3A%20Generalized%20Groves%20of%20Neural%20Additive%20Models%3A%20Pursuing%20transparent%20and%0A%20%20accurate%20machine%20learning%20models%20in%20finance%0AAuthor%3A%20Dangxing%20Chen%20and%20Weicheng%20Ye%0AAbstract%3A%20%20%20While%20machine%20learning%20methods%20have%20significantly%20improved%20model%20performance%0Aover%20traditional%20methods%2C%20their%20black-box%20structure%20makes%20it%20difficult%20for%0Aresearchers%20to%20interpret%20results.%20For%20highly%20regulated%20financial%20industries%2C%0Amodel%20transparency%20is%20equally%20important%20to%20accuracy.%20Without%20understanding%20how%0Amodels%20work%2C%20even%20highly%20accurate%20machine%20learning%20methods%20are%20unlikely%20to%20be%0Aaccepted.%20We%20address%20this%20issue%20by%20introducing%20a%20novel%20class%20of%20transparent%0Amachine%20learning%20models%20known%20as%20generalized%20groves%20of%20neural%20additive%20models.%0AThe%20generalized%20groves%20of%20neural%20additive%20models%20separate%20features%20into%20three%0Acategories%3A%20linear%20features%2C%20individual%20nonlinear%20features%2C%20and%20interacted%0Anonlinear%20features.%20Additionally%2C%20interactions%20in%20the%20last%20category%20are%20only%0Alocal.%20A%20stepwise%20selection%20algorithm%20distinguishes%20the%20linear%20and%20nonlinear%0Acomponents%2C%20and%20interacted%20groups%20are%20carefully%20verified%20by%20applying%20additive%0Aseparation%20criteria.%20Through%20some%20empirical%20examples%20in%20finance%2C%20we%20demonstrate%0Athat%20generalized%20grove%20of%20neural%20additive%20models%20exhibit%20high%20accuracy%20and%0Atransparency%20with%20predominantly%20linear%20terms%20and%20only%20sparse%20nonlinear%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.10082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Groves%2520of%2520Neural%2520Additive%2520Models%253A%2520Pursuing%2520transparent%2520and%250A%2520%2520accurate%2520machine%2520learning%2520models%2520in%2520finance%26entry.906535625%3DDangxing%2520Chen%2520and%2520Weicheng%2520Ye%26entry.1292438233%3D%2520%2520While%2520machine%2520learning%2520methods%2520have%2520significantly%2520improved%2520model%2520performance%250Aover%2520traditional%2520methods%252C%2520their%2520black-box%2520structure%2520makes%2520it%2520difficult%2520for%250Aresearchers%2520to%2520interpret%2520results.%2520For%2520highly%2520regulated%2520financial%2520industries%252C%250Amodel%2520transparency%2520is%2520equally%2520important%2520to%2520accuracy.%2520Without%2520understanding%2520how%250Amodels%2520work%252C%2520even%2520highly%2520accurate%2520machine%2520learning%2520methods%2520are%2520unlikely%2520to%2520be%250Aaccepted.%2520We%2520address%2520this%2520issue%2520by%2520introducing%2520a%2520novel%2520class%2520of%2520transparent%250Amachine%2520learning%2520models%2520known%2520as%2520generalized%2520groves%2520of%2520neural%2520additive%2520models.%250AThe%2520generalized%2520groves%2520of%2520neural%2520additive%2520models%2520separate%2520features%2520into%2520three%250Acategories%253A%2520linear%2520features%252C%2520individual%2520nonlinear%2520features%252C%2520and%2520interacted%250Anonlinear%2520features.%2520Additionally%252C%2520interactions%2520in%2520the%2520last%2520category%2520are%2520only%250Alocal.%2520A%2520stepwise%2520selection%2520algorithm%2520distinguishes%2520the%2520linear%2520and%2520nonlinear%250Acomponents%252C%2520and%2520interacted%2520groups%2520are%2520carefully%2520verified%2520by%2520applying%2520additive%250Aseparation%2520criteria.%2520Through%2520some%2520empirical%2520examples%2520in%2520finance%252C%2520we%2520demonstrate%250Athat%2520generalized%2520grove%2520of%2520neural%2520additive%2520models%2520exhibit%2520high%2520accuracy%2520and%250Atransparency%2520with%2520predominantly%2520linear%2520terms%2520and%2520only%2520sparse%2520nonlinear%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.10082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Groves%20of%20Neural%20Additive%20Models%3A%20Pursuing%20transparent%20and%0A%20%20accurate%20machine%20learning%20models%20in%20finance&entry.906535625=Dangxing%20Chen%20and%20Weicheng%20Ye&entry.1292438233=%20%20While%20machine%20learning%20methods%20have%20significantly%20improved%20model%20performance%0Aover%20traditional%20methods%2C%20their%20black-box%20structure%20makes%20it%20difficult%20for%0Aresearchers%20to%20interpret%20results.%20For%20highly%20regulated%20financial%20industries%2C%0Amodel%20transparency%20is%20equally%20important%20to%20accuracy.%20Without%20understanding%20how%0Amodels%20work%2C%20even%20highly%20accurate%20machine%20learning%20methods%20are%20unlikely%20to%20be%0Aaccepted.%20We%20address%20this%20issue%20by%20introducing%20a%20novel%20class%20of%20transparent%0Amachine%20learning%20models%20known%20as%20generalized%20groves%20of%20neural%20additive%20models.%0AThe%20generalized%20groves%20of%20neural%20additive%20models%20separate%20features%20into%20three%0Acategories%3A%20linear%20features%2C%20individual%20nonlinear%20features%2C%20and%20interacted%0Anonlinear%20features.%20Additionally%2C%20interactions%20in%20the%20last%20category%20are%20only%0Alocal.%20A%20stepwise%20selection%20algorithm%20distinguishes%20the%20linear%20and%20nonlinear%0Acomponents%2C%20and%20interacted%20groups%20are%20carefully%20verified%20by%20applying%20additive%0Aseparation%20criteria.%20Through%20some%20empirical%20examples%20in%20finance%2C%20we%20demonstrate%0Athat%20generalized%20grove%20of%20neural%20additive%20models%20exhibit%20high%20accuracy%20and%0Atransparency%20with%20predominantly%20linear%20terms%20and%20only%20sparse%20nonlinear%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.10082v2&entry.124074799=Read"},
{"title": "Collision Probability Distribution Estimation via Temporal Difference\n  Learning", "author": "Thomas Steinecker and Thorsten Luettel and Mirko Maehlisch", "abstract": "  We introduce CollisionPro, a pioneering framework designed to estimate\ncumulative collision probability distributions using temporal difference\nlearning, specifically tailored to applications in robotics, with a particular\nemphasis on autonomous driving. This approach addresses the demand for\nexplainable artificial intelligence (XAI) and seeks to overcome limitations\nimposed by model-based approaches and conservative constraints. We formulate\nour framework within the context of reinforcement learning to pave the way for\nsafety-aware agents. Nevertheless, we assert that our approach could prove\nbeneficial in various contexts, including a safety alert system or analytical\npurposes. A comprehensive examination of our framework is conducted using a\nrealistic autonomous driving simulator, illustrating its high sample efficiency\nand reliable prediction capabilities for previously unseen collision events.\nThe source code is publicly available.\n", "link": "http://arxiv.org/abs/2407.20000v1", "date": "2024-07-29", "relevancy": 1.9787, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5719}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4904}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collision%20Probability%20Distribution%20Estimation%20via%20Temporal%20Difference%0A%20%20Learning&body=Title%3A%20Collision%20Probability%20Distribution%20Estimation%20via%20Temporal%20Difference%0A%20%20Learning%0AAuthor%3A%20Thomas%20Steinecker%20and%20Thorsten%20Luettel%20and%20Mirko%20Maehlisch%0AAbstract%3A%20%20%20We%20introduce%20CollisionPro%2C%20a%20pioneering%20framework%20designed%20to%20estimate%0Acumulative%20collision%20probability%20distributions%20using%20temporal%20difference%0Alearning%2C%20specifically%20tailored%20to%20applications%20in%20robotics%2C%20with%20a%20particular%0Aemphasis%20on%20autonomous%20driving.%20This%20approach%20addresses%20the%20demand%20for%0Aexplainable%20artificial%20intelligence%20%28XAI%29%20and%20seeks%20to%20overcome%20limitations%0Aimposed%20by%20model-based%20approaches%20and%20conservative%20constraints.%20We%20formulate%0Aour%20framework%20within%20the%20context%20of%20reinforcement%20learning%20to%20pave%20the%20way%20for%0Asafety-aware%20agents.%20Nevertheless%2C%20we%20assert%20that%20our%20approach%20could%20prove%0Abeneficial%20in%20various%20contexts%2C%20including%20a%20safety%20alert%20system%20or%20analytical%0Apurposes.%20A%20comprehensive%20examination%20of%20our%20framework%20is%20conducted%20using%20a%0Arealistic%20autonomous%20driving%20simulator%2C%20illustrating%20its%20high%20sample%20efficiency%0Aand%20reliable%20prediction%20capabilities%20for%20previously%20unseen%20collision%20events.%0AThe%20source%20code%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollision%2520Probability%2520Distribution%2520Estimation%2520via%2520Temporal%2520Difference%250A%2520%2520Learning%26entry.906535625%3DThomas%2520Steinecker%2520and%2520Thorsten%2520Luettel%2520and%2520Mirko%2520Maehlisch%26entry.1292438233%3D%2520%2520We%2520introduce%2520CollisionPro%252C%2520a%2520pioneering%2520framework%2520designed%2520to%2520estimate%250Acumulative%2520collision%2520probability%2520distributions%2520using%2520temporal%2520difference%250Alearning%252C%2520specifically%2520tailored%2520to%2520applications%2520in%2520robotics%252C%2520with%2520a%2520particular%250Aemphasis%2520on%2520autonomous%2520driving.%2520This%2520approach%2520addresses%2520the%2520demand%2520for%250Aexplainable%2520artificial%2520intelligence%2520%2528XAI%2529%2520and%2520seeks%2520to%2520overcome%2520limitations%250Aimposed%2520by%2520model-based%2520approaches%2520and%2520conservative%2520constraints.%2520We%2520formulate%250Aour%2520framework%2520within%2520the%2520context%2520of%2520reinforcement%2520learning%2520to%2520pave%2520the%2520way%2520for%250Asafety-aware%2520agents.%2520Nevertheless%252C%2520we%2520assert%2520that%2520our%2520approach%2520could%2520prove%250Abeneficial%2520in%2520various%2520contexts%252C%2520including%2520a%2520safety%2520alert%2520system%2520or%2520analytical%250Apurposes.%2520A%2520comprehensive%2520examination%2520of%2520our%2520framework%2520is%2520conducted%2520using%2520a%250Arealistic%2520autonomous%2520driving%2520simulator%252C%2520illustrating%2520its%2520high%2520sample%2520efficiency%250Aand%2520reliable%2520prediction%2520capabilities%2520for%2520previously%2520unseen%2520collision%2520events.%250AThe%2520source%2520code%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collision%20Probability%20Distribution%20Estimation%20via%20Temporal%20Difference%0A%20%20Learning&entry.906535625=Thomas%20Steinecker%20and%20Thorsten%20Luettel%20and%20Mirko%20Maehlisch&entry.1292438233=%20%20We%20introduce%20CollisionPro%2C%20a%20pioneering%20framework%20designed%20to%20estimate%0Acumulative%20collision%20probability%20distributions%20using%20temporal%20difference%0Alearning%2C%20specifically%20tailored%20to%20applications%20in%20robotics%2C%20with%20a%20particular%0Aemphasis%20on%20autonomous%20driving.%20This%20approach%20addresses%20the%20demand%20for%0Aexplainable%20artificial%20intelligence%20%28XAI%29%20and%20seeks%20to%20overcome%20limitations%0Aimposed%20by%20model-based%20approaches%20and%20conservative%20constraints.%20We%20formulate%0Aour%20framework%20within%20the%20context%20of%20reinforcement%20learning%20to%20pave%20the%20way%20for%0Asafety-aware%20agents.%20Nevertheless%2C%20we%20assert%20that%20our%20approach%20could%20prove%0Abeneficial%20in%20various%20contexts%2C%20including%20a%20safety%20alert%20system%20or%20analytical%0Apurposes.%20A%20comprehensive%20examination%20of%20our%20framework%20is%20conducted%20using%20a%0Arealistic%20autonomous%20driving%20simulator%2C%20illustrating%20its%20high%20sample%20efficiency%0Aand%20reliable%20prediction%20capabilities%20for%20previously%20unseen%20collision%20events.%0AThe%20source%20code%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20000v1&entry.124074799=Read"},
{"title": "Classification of freshwater snails of the genus \\emph{Radomaniola} with\n  multimodal triplet networks", "author": "Dennis Vetter and Muhammad Ahsan and Diana Delicado and Thomas A. Neubauer and Thomas Wilke and Gemma Roig", "abstract": "  In this paper, we present our first proposal of a machine learning system for\nthe classification of freshwater snails of the genus \\emph{Radomaniola}. We\nelaborate on the specific challenges encountered during system design, and how\nwe tackled them; namely a small, very imbalanced dataset with a high number of\nclasses and high visual similarity between classes. We then show how we\nemployed triplet networks and the multiple input modalities of images,\nmeasurements, and genetic information to overcome these challenges and reach a\nperformance comparable to that of a trained domain expert.\n", "link": "http://arxiv.org/abs/2407.20013v1", "date": "2024-07-29", "relevancy": 1.9723, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5189}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4762}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20freshwater%20snails%20of%20the%20genus%20%5Cemph%7BRadomaniola%7D%20with%0A%20%20multimodal%20triplet%20networks&body=Title%3A%20Classification%20of%20freshwater%20snails%20of%20the%20genus%20%5Cemph%7BRadomaniola%7D%20with%0A%20%20multimodal%20triplet%20networks%0AAuthor%3A%20Dennis%20Vetter%20and%20Muhammad%20Ahsan%20and%20Diana%20Delicado%20and%20Thomas%20A.%20Neubauer%20and%20Thomas%20Wilke%20and%20Gemma%20Roig%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20our%20first%20proposal%20of%20a%20machine%20learning%20system%20for%0Athe%20classification%20of%20freshwater%20snails%20of%20the%20genus%20%5Cemph%7BRadomaniola%7D.%20We%0Aelaborate%20on%20the%20specific%20challenges%20encountered%20during%20system%20design%2C%20and%20how%0Awe%20tackled%20them%3B%20namely%20a%20small%2C%20very%20imbalanced%20dataset%20with%20a%20high%20number%20of%0Aclasses%20and%20high%20visual%20similarity%20between%20classes.%20We%20then%20show%20how%20we%0Aemployed%20triplet%20networks%20and%20the%20multiple%20input%20modalities%20of%20images%2C%0Ameasurements%2C%20and%20genetic%20information%20to%20overcome%20these%20challenges%20and%20reach%20a%0Aperformance%20comparable%20to%20that%20of%20a%20trained%20domain%20expert.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520freshwater%2520snails%2520of%2520the%2520genus%2520%255Cemph%257BRadomaniola%257D%2520with%250A%2520%2520multimodal%2520triplet%2520networks%26entry.906535625%3DDennis%2520Vetter%2520and%2520Muhammad%2520Ahsan%2520and%2520Diana%2520Delicado%2520and%2520Thomas%2520A.%2520Neubauer%2520and%2520Thomas%2520Wilke%2520and%2520Gemma%2520Roig%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520our%2520first%2520proposal%2520of%2520a%2520machine%2520learning%2520system%2520for%250Athe%2520classification%2520of%2520freshwater%2520snails%2520of%2520the%2520genus%2520%255Cemph%257BRadomaniola%257D.%2520We%250Aelaborate%2520on%2520the%2520specific%2520challenges%2520encountered%2520during%2520system%2520design%252C%2520and%2520how%250Awe%2520tackled%2520them%253B%2520namely%2520a%2520small%252C%2520very%2520imbalanced%2520dataset%2520with%2520a%2520high%2520number%2520of%250Aclasses%2520and%2520high%2520visual%2520similarity%2520between%2520classes.%2520We%2520then%2520show%2520how%2520we%250Aemployed%2520triplet%2520networks%2520and%2520the%2520multiple%2520input%2520modalities%2520of%2520images%252C%250Ameasurements%252C%2520and%2520genetic%2520information%2520to%2520overcome%2520these%2520challenges%2520and%2520reach%2520a%250Aperformance%2520comparable%2520to%2520that%2520of%2520a%2520trained%2520domain%2520expert.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20freshwater%20snails%20of%20the%20genus%20%5Cemph%7BRadomaniola%7D%20with%0A%20%20multimodal%20triplet%20networks&entry.906535625=Dennis%20Vetter%20and%20Muhammad%20Ahsan%20and%20Diana%20Delicado%20and%20Thomas%20A.%20Neubauer%20and%20Thomas%20Wilke%20and%20Gemma%20Roig&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20our%20first%20proposal%20of%20a%20machine%20learning%20system%20for%0Athe%20classification%20of%20freshwater%20snails%20of%20the%20genus%20%5Cemph%7BRadomaniola%7D.%20We%0Aelaborate%20on%20the%20specific%20challenges%20encountered%20during%20system%20design%2C%20and%20how%0Awe%20tackled%20them%3B%20namely%20a%20small%2C%20very%20imbalanced%20dataset%20with%20a%20high%20number%20of%0Aclasses%20and%20high%20visual%20similarity%20between%20classes.%20We%20then%20show%20how%20we%0Aemployed%20triplet%20networks%20and%20the%20multiple%20input%20modalities%20of%20images%2C%0Ameasurements%2C%20and%20genetic%20information%20to%20overcome%20these%20challenges%20and%20reach%20a%0Aperformance%20comparable%20to%20that%20of%20a%20trained%20domain%20expert.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20013v1&entry.124074799=Read"},
{"title": "Imitation Learning for Intra-Day Power Grid Operation through Topology\n  Actions", "author": "Matthijs de Jong and Jan Viebahn and Yuliya Shapovalova", "abstract": "  Power grid operation is becoming increasingly complex due to the increase in\ngeneration of renewable energy. The recent series of Learning To Run a Power\nNetwork (L2RPN) competitions have encouraged the use of artificial agents to\nassist human dispatchers in operating power grids. In this paper we study the\nperformance of imitation learning for day-ahead power grid operation through\ntopology actions. In particular, we consider two rule-based expert agents: a\ngreedy agent and a N-1 agent. While the latter is more computationally\nexpensive since it takes N-1 safety considerations into account, it exhibits a\nmuch higher operational performance. We train a fully-connected neural network\n(FCNN) on expert state-action pairs and evaluate it in two ways. First, we find\nthat classification accuracy is limited despite extensive hyperparameter\ntuning, due to class imbalance and class overlap. Second, as a power system\nagent, the FCNN performs only slightly worse than expert agents. Furthermore,\nhybrid agents, which incorporate minimal additional simulations, match expert\nagents' performance with significantly lower computational cost. Consequently,\nimitation learning shows promise for developing fast, high-performing power\ngrid agents, motivating its further exploration in future L2RPN studies.\n", "link": "http://arxiv.org/abs/2407.19865v1", "date": "2024-07-29", "relevancy": 1.9723, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5392}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4762}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20Learning%20for%20Intra-Day%20Power%20Grid%20Operation%20through%20Topology%0A%20%20Actions&body=Title%3A%20Imitation%20Learning%20for%20Intra-Day%20Power%20Grid%20Operation%20through%20Topology%0A%20%20Actions%0AAuthor%3A%20Matthijs%20de%20Jong%20and%20Jan%20Viebahn%20and%20Yuliya%20Shapovalova%0AAbstract%3A%20%20%20Power%20grid%20operation%20is%20becoming%20increasingly%20complex%20due%20to%20the%20increase%20in%0Ageneration%20of%20renewable%20energy.%20The%20recent%20series%20of%20Learning%20To%20Run%20a%20Power%0ANetwork%20%28L2RPN%29%20competitions%20have%20encouraged%20the%20use%20of%20artificial%20agents%20to%0Aassist%20human%20dispatchers%20in%20operating%20power%20grids.%20In%20this%20paper%20we%20study%20the%0Aperformance%20of%20imitation%20learning%20for%20day-ahead%20power%20grid%20operation%20through%0Atopology%20actions.%20In%20particular%2C%20we%20consider%20two%20rule-based%20expert%20agents%3A%20a%0Agreedy%20agent%20and%20a%20N-1%20agent.%20While%20the%20latter%20is%20more%20computationally%0Aexpensive%20since%20it%20takes%20N-1%20safety%20considerations%20into%20account%2C%20it%20exhibits%20a%0Amuch%20higher%20operational%20performance.%20We%20train%20a%20fully-connected%20neural%20network%0A%28FCNN%29%20on%20expert%20state-action%20pairs%20and%20evaluate%20it%20in%20two%20ways.%20First%2C%20we%20find%0Athat%20classification%20accuracy%20is%20limited%20despite%20extensive%20hyperparameter%0Atuning%2C%20due%20to%20class%20imbalance%20and%20class%20overlap.%20Second%2C%20as%20a%20power%20system%0Aagent%2C%20the%20FCNN%20performs%20only%20slightly%20worse%20than%20expert%20agents.%20Furthermore%2C%0Ahybrid%20agents%2C%20which%20incorporate%20minimal%20additional%20simulations%2C%20match%20expert%0Aagents%27%20performance%20with%20significantly%20lower%20computational%20cost.%20Consequently%2C%0Aimitation%20learning%20shows%20promise%20for%20developing%20fast%2C%20high-performing%20power%0Agrid%20agents%2C%20motivating%20its%20further%20exploration%20in%20future%20L2RPN%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520Learning%2520for%2520Intra-Day%2520Power%2520Grid%2520Operation%2520through%2520Topology%250A%2520%2520Actions%26entry.906535625%3DMatthijs%2520de%2520Jong%2520and%2520Jan%2520Viebahn%2520and%2520Yuliya%2520Shapovalova%26entry.1292438233%3D%2520%2520Power%2520grid%2520operation%2520is%2520becoming%2520increasingly%2520complex%2520due%2520to%2520the%2520increase%2520in%250Ageneration%2520of%2520renewable%2520energy.%2520The%2520recent%2520series%2520of%2520Learning%2520To%2520Run%2520a%2520Power%250ANetwork%2520%2528L2RPN%2529%2520competitions%2520have%2520encouraged%2520the%2520use%2520of%2520artificial%2520agents%2520to%250Aassist%2520human%2520dispatchers%2520in%2520operating%2520power%2520grids.%2520In%2520this%2520paper%2520we%2520study%2520the%250Aperformance%2520of%2520imitation%2520learning%2520for%2520day-ahead%2520power%2520grid%2520operation%2520through%250Atopology%2520actions.%2520In%2520particular%252C%2520we%2520consider%2520two%2520rule-based%2520expert%2520agents%253A%2520a%250Agreedy%2520agent%2520and%2520a%2520N-1%2520agent.%2520While%2520the%2520latter%2520is%2520more%2520computationally%250Aexpensive%2520since%2520it%2520takes%2520N-1%2520safety%2520considerations%2520into%2520account%252C%2520it%2520exhibits%2520a%250Amuch%2520higher%2520operational%2520performance.%2520We%2520train%2520a%2520fully-connected%2520neural%2520network%250A%2528FCNN%2529%2520on%2520expert%2520state-action%2520pairs%2520and%2520evaluate%2520it%2520in%2520two%2520ways.%2520First%252C%2520we%2520find%250Athat%2520classification%2520accuracy%2520is%2520limited%2520despite%2520extensive%2520hyperparameter%250Atuning%252C%2520due%2520to%2520class%2520imbalance%2520and%2520class%2520overlap.%2520Second%252C%2520as%2520a%2520power%2520system%250Aagent%252C%2520the%2520FCNN%2520performs%2520only%2520slightly%2520worse%2520than%2520expert%2520agents.%2520Furthermore%252C%250Ahybrid%2520agents%252C%2520which%2520incorporate%2520minimal%2520additional%2520simulations%252C%2520match%2520expert%250Aagents%2527%2520performance%2520with%2520significantly%2520lower%2520computational%2520cost.%2520Consequently%252C%250Aimitation%2520learning%2520shows%2520promise%2520for%2520developing%2520fast%252C%2520high-performing%2520power%250Agrid%2520agents%252C%2520motivating%2520its%2520further%2520exploration%2520in%2520future%2520L2RPN%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Learning%20for%20Intra-Day%20Power%20Grid%20Operation%20through%20Topology%0A%20%20Actions&entry.906535625=Matthijs%20de%20Jong%20and%20Jan%20Viebahn%20and%20Yuliya%20Shapovalova&entry.1292438233=%20%20Power%20grid%20operation%20is%20becoming%20increasingly%20complex%20due%20to%20the%20increase%20in%0Ageneration%20of%20renewable%20energy.%20The%20recent%20series%20of%20Learning%20To%20Run%20a%20Power%0ANetwork%20%28L2RPN%29%20competitions%20have%20encouraged%20the%20use%20of%20artificial%20agents%20to%0Aassist%20human%20dispatchers%20in%20operating%20power%20grids.%20In%20this%20paper%20we%20study%20the%0Aperformance%20of%20imitation%20learning%20for%20day-ahead%20power%20grid%20operation%20through%0Atopology%20actions.%20In%20particular%2C%20we%20consider%20two%20rule-based%20expert%20agents%3A%20a%0Agreedy%20agent%20and%20a%20N-1%20agent.%20While%20the%20latter%20is%20more%20computationally%0Aexpensive%20since%20it%20takes%20N-1%20safety%20considerations%20into%20account%2C%20it%20exhibits%20a%0Amuch%20higher%20operational%20performance.%20We%20train%20a%20fully-connected%20neural%20network%0A%28FCNN%29%20on%20expert%20state-action%20pairs%20and%20evaluate%20it%20in%20two%20ways.%20First%2C%20we%20find%0Athat%20classification%20accuracy%20is%20limited%20despite%20extensive%20hyperparameter%0Atuning%2C%20due%20to%20class%20imbalance%20and%20class%20overlap.%20Second%2C%20as%20a%20power%20system%0Aagent%2C%20the%20FCNN%20performs%20only%20slightly%20worse%20than%20expert%20agents.%20Furthermore%2C%0Ahybrid%20agents%2C%20which%20incorporate%20minimal%20additional%20simulations%2C%20match%20expert%0Aagents%27%20performance%20with%20significantly%20lower%20computational%20cost.%20Consequently%2C%0Aimitation%20learning%20shows%20promise%20for%20developing%20fast%2C%20high-performing%20power%0Agrid%20agents%2C%20motivating%20its%20further%20exploration%20in%20future%20L2RPN%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19865v1&entry.124074799=Read"},
{"title": "SAPG: Split and Aggregate Policy Gradients", "author": "Jayesh Singla and Ananye Agarwal and Deepak Pathak", "abstract": "  Despite extreme sample inefficiency, on-policy reinforcement learning, aka\npolicy gradients, has become a fundamental tool in decision-making problems.\nWith the recent advances in GPU-driven simulation, the ability to collect large\namounts of data for RL training has scaled exponentially. However, we show that\ncurrent RL methods, e.g. PPO, fail to ingest the benefit of parallelized\nenvironments beyond a certain point and their performance saturates. To address\nthis, we propose a new on-policy RL algorithm that can effectively leverage\nlarge-scale environments by splitting them into chunks and fusing them back\ntogether via importance sampling. Our algorithm, termed SAPG, shows\nsignificantly higher performance across a variety of challenging environments\nwhere vanilla PPO and other strong baselines fail to achieve high performance.\nWebsite at https://sapg-rl.github.io/\n", "link": "http://arxiv.org/abs/2407.20230v1", "date": "2024-07-29", "relevancy": 1.9569, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5202}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.469}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAPG%3A%20Split%20and%20Aggregate%20Policy%20Gradients&body=Title%3A%20SAPG%3A%20Split%20and%20Aggregate%20Policy%20Gradients%0AAuthor%3A%20Jayesh%20Singla%20and%20Ananye%20Agarwal%20and%20Deepak%20Pathak%0AAbstract%3A%20%20%20Despite%20extreme%20sample%20inefficiency%2C%20on-policy%20reinforcement%20learning%2C%20aka%0Apolicy%20gradients%2C%20has%20become%20a%20fundamental%20tool%20in%20decision-making%20problems.%0AWith%20the%20recent%20advances%20in%20GPU-driven%20simulation%2C%20the%20ability%20to%20collect%20large%0Aamounts%20of%20data%20for%20RL%20training%20has%20scaled%20exponentially.%20However%2C%20we%20show%20that%0Acurrent%20RL%20methods%2C%20e.g.%20PPO%2C%20fail%20to%20ingest%20the%20benefit%20of%20parallelized%0Aenvironments%20beyond%20a%20certain%20point%20and%20their%20performance%20saturates.%20To%20address%0Athis%2C%20we%20propose%20a%20new%20on-policy%20RL%20algorithm%20that%20can%20effectively%20leverage%0Alarge-scale%20environments%20by%20splitting%20them%20into%20chunks%20and%20fusing%20them%20back%0Atogether%20via%20importance%20sampling.%20Our%20algorithm%2C%20termed%20SAPG%2C%20shows%0Asignificantly%20higher%20performance%20across%20a%20variety%20of%20challenging%20environments%0Awhere%20vanilla%20PPO%20and%20other%20strong%20baselines%20fail%20to%20achieve%20high%20performance.%0AWebsite%20at%20https%3A//sapg-rl.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAPG%253A%2520Split%2520and%2520Aggregate%2520Policy%2520Gradients%26entry.906535625%3DJayesh%2520Singla%2520and%2520Ananye%2520Agarwal%2520and%2520Deepak%2520Pathak%26entry.1292438233%3D%2520%2520Despite%2520extreme%2520sample%2520inefficiency%252C%2520on-policy%2520reinforcement%2520learning%252C%2520aka%250Apolicy%2520gradients%252C%2520has%2520become%2520a%2520fundamental%2520tool%2520in%2520decision-making%2520problems.%250AWith%2520the%2520recent%2520advances%2520in%2520GPU-driven%2520simulation%252C%2520the%2520ability%2520to%2520collect%2520large%250Aamounts%2520of%2520data%2520for%2520RL%2520training%2520has%2520scaled%2520exponentially.%2520However%252C%2520we%2520show%2520that%250Acurrent%2520RL%2520methods%252C%2520e.g.%2520PPO%252C%2520fail%2520to%2520ingest%2520the%2520benefit%2520of%2520parallelized%250Aenvironments%2520beyond%2520a%2520certain%2520point%2520and%2520their%2520performance%2520saturates.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520new%2520on-policy%2520RL%2520algorithm%2520that%2520can%2520effectively%2520leverage%250Alarge-scale%2520environments%2520by%2520splitting%2520them%2520into%2520chunks%2520and%2520fusing%2520them%2520back%250Atogether%2520via%2520importance%2520sampling.%2520Our%2520algorithm%252C%2520termed%2520SAPG%252C%2520shows%250Asignificantly%2520higher%2520performance%2520across%2520a%2520variety%2520of%2520challenging%2520environments%250Awhere%2520vanilla%2520PPO%2520and%2520other%2520strong%2520baselines%2520fail%2520to%2520achieve%2520high%2520performance.%250AWebsite%2520at%2520https%253A//sapg-rl.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAPG%3A%20Split%20and%20Aggregate%20Policy%20Gradients&entry.906535625=Jayesh%20Singla%20and%20Ananye%20Agarwal%20and%20Deepak%20Pathak&entry.1292438233=%20%20Despite%20extreme%20sample%20inefficiency%2C%20on-policy%20reinforcement%20learning%2C%20aka%0Apolicy%20gradients%2C%20has%20become%20a%20fundamental%20tool%20in%20decision-making%20problems.%0AWith%20the%20recent%20advances%20in%20GPU-driven%20simulation%2C%20the%20ability%20to%20collect%20large%0Aamounts%20of%20data%20for%20RL%20training%20has%20scaled%20exponentially.%20However%2C%20we%20show%20that%0Acurrent%20RL%20methods%2C%20e.g.%20PPO%2C%20fail%20to%20ingest%20the%20benefit%20of%20parallelized%0Aenvironments%20beyond%20a%20certain%20point%20and%20their%20performance%20saturates.%20To%20address%0Athis%2C%20we%20propose%20a%20new%20on-policy%20RL%20algorithm%20that%20can%20effectively%20leverage%0Alarge-scale%20environments%20by%20splitting%20them%20into%20chunks%20and%20fusing%20them%20back%0Atogether%20via%20importance%20sampling.%20Our%20algorithm%2C%20termed%20SAPG%2C%20shows%0Asignificantly%20higher%20performance%20across%20a%20variety%20of%20challenging%20environments%0Awhere%20vanilla%20PPO%20and%20other%20strong%20baselines%20fail%20to%20achieve%20high%20performance.%0AWebsite%20at%20https%3A//sapg-rl.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20230v1&entry.124074799=Read"},
{"title": "FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval\n  for comparative performance analysis", "author": "Mikel Williams-Lekuona and Georgina Cosma", "abstract": "  In the field of Image-Text Retrieval (ITR), recent advancements have\nleveraged large-scale Vision-Language Pretraining (VLP) for Fine-Grained (FG)\ninstance-level retrieval, achieving high accuracy at the cost of increased\ncomputational complexity. For Coarse-Grained (CG) category-level retrieval,\nprominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency,\nalbeit at the cost of retrieval performance. Due to differences in\nmethodologies, FG and CG models are rarely compared directly within evaluations\nin the literature, resulting in a lack of empirical data quantifying the\nretrieval performance-efficiency tradeoffs between the two. This paper\naddresses this gap by introducing the \\texttt{FiCo-ITR} library, which\nstandardises evaluation methodologies for both FG and CG models, facilitating\ndirect comparisons. We conduct empirical evaluations of representative models\nfrom both subfields, analysing precision, recall, and computational complexity\nacross varying data scales. Our findings offer new insights into the\nperformance-efficiency trade-offs between recent representative FG and CG\nmodels, highlighting their respective strengths and limitations. These findings\nprovide the foundation necessary to make more informed decisions regarding\nmodel selection for specific retrieval tasks and highlight avenues for future\nresearch into hybrid systems that leverage the strengths of both FG and CG\napproaches.\n", "link": "http://arxiv.org/abs/2407.20114v1", "date": "2024-07-29", "relevancy": 1.9458, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4893}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FiCo-ITR%3A%20bridging%20fine-grained%20and%20coarse-grained%20image-text%20retrieval%0A%20%20for%20comparative%20performance%20analysis&body=Title%3A%20FiCo-ITR%3A%20bridging%20fine-grained%20and%20coarse-grained%20image-text%20retrieval%0A%20%20for%20comparative%20performance%20analysis%0AAuthor%3A%20Mikel%20Williams-Lekuona%20and%20Georgina%20Cosma%0AAbstract%3A%20%20%20In%20the%20field%20of%20Image-Text%20Retrieval%20%28ITR%29%2C%20recent%20advancements%20have%0Aleveraged%20large-scale%20Vision-Language%20Pretraining%20%28VLP%29%20for%20Fine-Grained%20%28FG%29%0Ainstance-level%20retrieval%2C%20achieving%20high%20accuracy%20at%20the%20cost%20of%20increased%0Acomputational%20complexity.%20For%20Coarse-Grained%20%28CG%29%20category-level%20retrieval%2C%0Aprominent%20approaches%20employ%20Cross-Modal%20Hashing%20%28CMH%29%20to%20prioritise%20efficiency%2C%0Aalbeit%20at%20the%20cost%20of%20retrieval%20performance.%20Due%20to%20differences%20in%0Amethodologies%2C%20FG%20and%20CG%20models%20are%20rarely%20compared%20directly%20within%20evaluations%0Ain%20the%20literature%2C%20resulting%20in%20a%20lack%20of%20empirical%20data%20quantifying%20the%0Aretrieval%20performance-efficiency%20tradeoffs%20between%20the%20two.%20This%20paper%0Aaddresses%20this%20gap%20by%20introducing%20the%20%5Ctexttt%7BFiCo-ITR%7D%20library%2C%20which%0Astandardises%20evaluation%20methodologies%20for%20both%20FG%20and%20CG%20models%2C%20facilitating%0Adirect%20comparisons.%20We%20conduct%20empirical%20evaluations%20of%20representative%20models%0Afrom%20both%20subfields%2C%20analysing%20precision%2C%20recall%2C%20and%20computational%20complexity%0Aacross%20varying%20data%20scales.%20Our%20findings%20offer%20new%20insights%20into%20the%0Aperformance-efficiency%20trade-offs%20between%20recent%20representative%20FG%20and%20CG%0Amodels%2C%20highlighting%20their%20respective%20strengths%20and%20limitations.%20These%20findings%0Aprovide%20the%20foundation%20necessary%20to%20make%20more%20informed%20decisions%20regarding%0Amodel%20selection%20for%20specific%20retrieval%20tasks%20and%20highlight%20avenues%20for%20future%0Aresearch%20into%20hybrid%20systems%20that%20leverage%20the%20strengths%20of%20both%20FG%20and%20CG%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFiCo-ITR%253A%2520bridging%2520fine-grained%2520and%2520coarse-grained%2520image-text%2520retrieval%250A%2520%2520for%2520comparative%2520performance%2520analysis%26entry.906535625%3DMikel%2520Williams-Lekuona%2520and%2520Georgina%2520Cosma%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520Image-Text%2520Retrieval%2520%2528ITR%2529%252C%2520recent%2520advancements%2520have%250Aleveraged%2520large-scale%2520Vision-Language%2520Pretraining%2520%2528VLP%2529%2520for%2520Fine-Grained%2520%2528FG%2529%250Ainstance-level%2520retrieval%252C%2520achieving%2520high%2520accuracy%2520at%2520the%2520cost%2520of%2520increased%250Acomputational%2520complexity.%2520For%2520Coarse-Grained%2520%2528CG%2529%2520category-level%2520retrieval%252C%250Aprominent%2520approaches%2520employ%2520Cross-Modal%2520Hashing%2520%2528CMH%2529%2520to%2520prioritise%2520efficiency%252C%250Aalbeit%2520at%2520the%2520cost%2520of%2520retrieval%2520performance.%2520Due%2520to%2520differences%2520in%250Amethodologies%252C%2520FG%2520and%2520CG%2520models%2520are%2520rarely%2520compared%2520directly%2520within%2520evaluations%250Ain%2520the%2520literature%252C%2520resulting%2520in%2520a%2520lack%2520of%2520empirical%2520data%2520quantifying%2520the%250Aretrieval%2520performance-efficiency%2520tradeoffs%2520between%2520the%2520two.%2520This%2520paper%250Aaddresses%2520this%2520gap%2520by%2520introducing%2520the%2520%255Ctexttt%257BFiCo-ITR%257D%2520library%252C%2520which%250Astandardises%2520evaluation%2520methodologies%2520for%2520both%2520FG%2520and%2520CG%2520models%252C%2520facilitating%250Adirect%2520comparisons.%2520We%2520conduct%2520empirical%2520evaluations%2520of%2520representative%2520models%250Afrom%2520both%2520subfields%252C%2520analysing%2520precision%252C%2520recall%252C%2520and%2520computational%2520complexity%250Aacross%2520varying%2520data%2520scales.%2520Our%2520findings%2520offer%2520new%2520insights%2520into%2520the%250Aperformance-efficiency%2520trade-offs%2520between%2520recent%2520representative%2520FG%2520and%2520CG%250Amodels%252C%2520highlighting%2520their%2520respective%2520strengths%2520and%2520limitations.%2520These%2520findings%250Aprovide%2520the%2520foundation%2520necessary%2520to%2520make%2520more%2520informed%2520decisions%2520regarding%250Amodel%2520selection%2520for%2520specific%2520retrieval%2520tasks%2520and%2520highlight%2520avenues%2520for%2520future%250Aresearch%2520into%2520hybrid%2520systems%2520that%2520leverage%2520the%2520strengths%2520of%2520both%2520FG%2520and%2520CG%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FiCo-ITR%3A%20bridging%20fine-grained%20and%20coarse-grained%20image-text%20retrieval%0A%20%20for%20comparative%20performance%20analysis&entry.906535625=Mikel%20Williams-Lekuona%20and%20Georgina%20Cosma&entry.1292438233=%20%20In%20the%20field%20of%20Image-Text%20Retrieval%20%28ITR%29%2C%20recent%20advancements%20have%0Aleveraged%20large-scale%20Vision-Language%20Pretraining%20%28VLP%29%20for%20Fine-Grained%20%28FG%29%0Ainstance-level%20retrieval%2C%20achieving%20high%20accuracy%20at%20the%20cost%20of%20increased%0Acomputational%20complexity.%20For%20Coarse-Grained%20%28CG%29%20category-level%20retrieval%2C%0Aprominent%20approaches%20employ%20Cross-Modal%20Hashing%20%28CMH%29%20to%20prioritise%20efficiency%2C%0Aalbeit%20at%20the%20cost%20of%20retrieval%20performance.%20Due%20to%20differences%20in%0Amethodologies%2C%20FG%20and%20CG%20models%20are%20rarely%20compared%20directly%20within%20evaluations%0Ain%20the%20literature%2C%20resulting%20in%20a%20lack%20of%20empirical%20data%20quantifying%20the%0Aretrieval%20performance-efficiency%20tradeoffs%20between%20the%20two.%20This%20paper%0Aaddresses%20this%20gap%20by%20introducing%20the%20%5Ctexttt%7BFiCo-ITR%7D%20library%2C%20which%0Astandardises%20evaluation%20methodologies%20for%20both%20FG%20and%20CG%20models%2C%20facilitating%0Adirect%20comparisons.%20We%20conduct%20empirical%20evaluations%20of%20representative%20models%0Afrom%20both%20subfields%2C%20analysing%20precision%2C%20recall%2C%20and%20computational%20complexity%0Aacross%20varying%20data%20scales.%20Our%20findings%20offer%20new%20insights%20into%20the%0Aperformance-efficiency%20trade-offs%20between%20recent%20representative%20FG%20and%20CG%0Amodels%2C%20highlighting%20their%20respective%20strengths%20and%20limitations.%20These%20findings%0Aprovide%20the%20foundation%20necessary%20to%20make%20more%20informed%20decisions%20regarding%0Amodel%20selection%20for%20specific%20retrieval%20tasks%20and%20highlight%20avenues%20for%20future%0Aresearch%20into%20hybrid%20systems%20that%20leverage%20the%20strengths%20of%20both%20FG%20and%20CG%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20114v1&entry.124074799=Read"},
{"title": "Can I trust my anomaly detection system? A case study based on\n  explainable AI", "author": "Muhammad Rashid and Elvio Amparore and Enrico Ferrari and Damiano Verda", "abstract": "  Generative models based on variational autoencoders are a popular technique\nfor detecting anomalies in images in a semi-supervised context. A common\napproach employs the anomaly score to detect the presence of anomalies, and it\nis known to reach high level of accuracy on benchmark datasets. However, since\nanomaly scores are computed from reconstruction disparities, they often obscure\nthe detection of various spurious features, raising concerns regarding their\nactual efficacy. This case study explores the robustness of an anomaly\ndetection system based on variational autoencoder generative models through the\nuse of eXplainable AI methods. The goal is to get a different perspective on\nthe real performances of anomaly detectors that use reconstruction differences.\nIn our case study we discovered that, in many cases, samples are detected as\nanomalous for the wrong or misleading factors.\n", "link": "http://arxiv.org/abs/2407.19951v1", "date": "2024-07-29", "relevancy": 1.9405, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5076}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4828}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20I%20trust%20my%20anomaly%20detection%20system%3F%20A%20case%20study%20based%20on%0A%20%20explainable%20AI&body=Title%3A%20Can%20I%20trust%20my%20anomaly%20detection%20system%3F%20A%20case%20study%20based%20on%0A%20%20explainable%20AI%0AAuthor%3A%20Muhammad%20Rashid%20and%20Elvio%20Amparore%20and%20Enrico%20Ferrari%20and%20Damiano%20Verda%0AAbstract%3A%20%20%20Generative%20models%20based%20on%20variational%20autoencoders%20are%20a%20popular%20technique%0Afor%20detecting%20anomalies%20in%20images%20in%20a%20semi-supervised%20context.%20A%20common%0Aapproach%20employs%20the%20anomaly%20score%20to%20detect%20the%20presence%20of%20anomalies%2C%20and%20it%0Ais%20known%20to%20reach%20high%20level%20of%20accuracy%20on%20benchmark%20datasets.%20However%2C%20since%0Aanomaly%20scores%20are%20computed%20from%20reconstruction%20disparities%2C%20they%20often%20obscure%0Athe%20detection%20of%20various%20spurious%20features%2C%20raising%20concerns%20regarding%20their%0Aactual%20efficacy.%20This%20case%20study%20explores%20the%20robustness%20of%20an%20anomaly%0Adetection%20system%20based%20on%20variational%20autoencoder%20generative%20models%20through%20the%0Ause%20of%20eXplainable%20AI%20methods.%20The%20goal%20is%20to%20get%20a%20different%20perspective%20on%0Athe%20real%20performances%20of%20anomaly%20detectors%20that%20use%20reconstruction%20differences.%0AIn%20our%20case%20study%20we%20discovered%20that%2C%20in%20many%20cases%2C%20samples%20are%20detected%20as%0Aanomalous%20for%20the%20wrong%20or%20misleading%20factors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520I%2520trust%2520my%2520anomaly%2520detection%2520system%253F%2520A%2520case%2520study%2520based%2520on%250A%2520%2520explainable%2520AI%26entry.906535625%3DMuhammad%2520Rashid%2520and%2520Elvio%2520Amparore%2520and%2520Enrico%2520Ferrari%2520and%2520Damiano%2520Verda%26entry.1292438233%3D%2520%2520Generative%2520models%2520based%2520on%2520variational%2520autoencoders%2520are%2520a%2520popular%2520technique%250Afor%2520detecting%2520anomalies%2520in%2520images%2520in%2520a%2520semi-supervised%2520context.%2520A%2520common%250Aapproach%2520employs%2520the%2520anomaly%2520score%2520to%2520detect%2520the%2520presence%2520of%2520anomalies%252C%2520and%2520it%250Ais%2520known%2520to%2520reach%2520high%2520level%2520of%2520accuracy%2520on%2520benchmark%2520datasets.%2520However%252C%2520since%250Aanomaly%2520scores%2520are%2520computed%2520from%2520reconstruction%2520disparities%252C%2520they%2520often%2520obscure%250Athe%2520detection%2520of%2520various%2520spurious%2520features%252C%2520raising%2520concerns%2520regarding%2520their%250Aactual%2520efficacy.%2520This%2520case%2520study%2520explores%2520the%2520robustness%2520of%2520an%2520anomaly%250Adetection%2520system%2520based%2520on%2520variational%2520autoencoder%2520generative%2520models%2520through%2520the%250Ause%2520of%2520eXplainable%2520AI%2520methods.%2520The%2520goal%2520is%2520to%2520get%2520a%2520different%2520perspective%2520on%250Athe%2520real%2520performances%2520of%2520anomaly%2520detectors%2520that%2520use%2520reconstruction%2520differences.%250AIn%2520our%2520case%2520study%2520we%2520discovered%2520that%252C%2520in%2520many%2520cases%252C%2520samples%2520are%2520detected%2520as%250Aanomalous%2520for%2520the%2520wrong%2520or%2520misleading%2520factors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20I%20trust%20my%20anomaly%20detection%20system%3F%20A%20case%20study%20based%20on%0A%20%20explainable%20AI&entry.906535625=Muhammad%20Rashid%20and%20Elvio%20Amparore%20and%20Enrico%20Ferrari%20and%20Damiano%20Verda&entry.1292438233=%20%20Generative%20models%20based%20on%20variational%20autoencoders%20are%20a%20popular%20technique%0Afor%20detecting%20anomalies%20in%20images%20in%20a%20semi-supervised%20context.%20A%20common%0Aapproach%20employs%20the%20anomaly%20score%20to%20detect%20the%20presence%20of%20anomalies%2C%20and%20it%0Ais%20known%20to%20reach%20high%20level%20of%20accuracy%20on%20benchmark%20datasets.%20However%2C%20since%0Aanomaly%20scores%20are%20computed%20from%20reconstruction%20disparities%2C%20they%20often%20obscure%0Athe%20detection%20of%20various%20spurious%20features%2C%20raising%20concerns%20regarding%20their%0Aactual%20efficacy.%20This%20case%20study%20explores%20the%20robustness%20of%20an%20anomaly%0Adetection%20system%20based%20on%20variational%20autoencoder%20generative%20models%20through%20the%0Ause%20of%20eXplainable%20AI%20methods.%20The%20goal%20is%20to%20get%20a%20different%20perspective%20on%0Athe%20real%20performances%20of%20anomaly%20detectors%20that%20use%20reconstruction%20differences.%0AIn%20our%20case%20study%20we%20discovered%20that%2C%20in%20many%20cases%2C%20samples%20are%20detected%20as%0Aanomalous%20for%20the%20wrong%20or%20misleading%20factors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19951v1&entry.124074799=Read"},
{"title": "Detecting and Understanding Vulnerabilities in Language Models via\n  Mechanistic Interpretability", "author": "Jorge Garc\u00eda-Carrasco and Alejandro Mat\u00e9 and Juan Trujillo", "abstract": "  Large Language Models (LLMs), characterized by being trained on broad amounts\nof data in a self-supervised manner, have shown impressive performance across a\nwide range of tasks. Indeed, their generative abilities have aroused interest\non the application of LLMs across a wide range of contexts. However, neural\nnetworks in general, and LLMs in particular, are known to be vulnerable to\nadversarial attacks, where an imperceptible change to the input can mislead the\noutput of the model. This is a serious concern that impedes the use of LLMs on\nhigh-stakes applications, such as healthcare, where a wrong prediction can\nimply serious consequences. Even though there are many efforts on making LLMs\nmore robust to adversarial attacks, there are almost no works that study\n\\emph{how} and \\emph{where} these vulnerabilities that make LLMs prone to\nadversarial attacks happen. Motivated by these facts, we explore how to\nlocalize and understand vulnerabilities, and propose a method, based on\nMechanistic Interpretability (MI) techniques, to guide this process.\nSpecifically, this method enables us to detect vulnerabilities related to a\nconcrete task by (i) obtaining the subset of the model that is responsible for\nthat task, (ii) generating adversarial samples for that task, and (iii) using\nMI techniques together with the previous samples to discover and understand the\npossible vulnerabilities. We showcase our method on a pretrained GPT-2 Small\nmodel carrying out the task of predicting 3-letter acronyms to demonstrate its\neffectiveness on locating and understanding concrete vulnerabilities of the\nmodel.\n", "link": "http://arxiv.org/abs/2407.19842v1", "date": "2024-07-29", "relevancy": 1.9393, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Understanding%20Vulnerabilities%20in%20Language%20Models%20via%0A%20%20Mechanistic%20Interpretability&body=Title%3A%20Detecting%20and%20Understanding%20Vulnerabilities%20in%20Language%20Models%20via%0A%20%20Mechanistic%20Interpretability%0AAuthor%3A%20Jorge%20Garc%C3%ADa-Carrasco%20and%20Alejandro%20Mat%C3%A9%20and%20Juan%20Trujillo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%2C%20characterized%20by%20being%20trained%20on%20broad%20amounts%0Aof%20data%20in%20a%20self-supervised%20manner%2C%20have%20shown%20impressive%20performance%20across%20a%0Awide%20range%20of%20tasks.%20Indeed%2C%20their%20generative%20abilities%20have%20aroused%20interest%0Aon%20the%20application%20of%20LLMs%20across%20a%20wide%20range%20of%20contexts.%20However%2C%20neural%0Anetworks%20in%20general%2C%20and%20LLMs%20in%20particular%2C%20are%20known%20to%20be%20vulnerable%20to%0Aadversarial%20attacks%2C%20where%20an%20imperceptible%20change%20to%20the%20input%20can%20mislead%20the%0Aoutput%20of%20the%20model.%20This%20is%20a%20serious%20concern%20that%20impedes%20the%20use%20of%20LLMs%20on%0Ahigh-stakes%20applications%2C%20such%20as%20healthcare%2C%20where%20a%20wrong%20prediction%20can%0Aimply%20serious%20consequences.%20Even%20though%20there%20are%20many%20efforts%20on%20making%20LLMs%0Amore%20robust%20to%20adversarial%20attacks%2C%20there%20are%20almost%20no%20works%20that%20study%0A%5Cemph%7Bhow%7D%20and%20%5Cemph%7Bwhere%7D%20these%20vulnerabilities%20that%20make%20LLMs%20prone%20to%0Aadversarial%20attacks%20happen.%20Motivated%20by%20these%20facts%2C%20we%20explore%20how%20to%0Alocalize%20and%20understand%20vulnerabilities%2C%20and%20propose%20a%20method%2C%20based%20on%0AMechanistic%20Interpretability%20%28MI%29%20techniques%2C%20to%20guide%20this%20process.%0ASpecifically%2C%20this%20method%20enables%20us%20to%20detect%20vulnerabilities%20related%20to%20a%0Aconcrete%20task%20by%20%28i%29%20obtaining%20the%20subset%20of%20the%20model%20that%20is%20responsible%20for%0Athat%20task%2C%20%28ii%29%20generating%20adversarial%20samples%20for%20that%20task%2C%20and%20%28iii%29%20using%0AMI%20techniques%20together%20with%20the%20previous%20samples%20to%20discover%20and%20understand%20the%0Apossible%20vulnerabilities.%20We%20showcase%20our%20method%20on%20a%20pretrained%20GPT-2%20Small%0Amodel%20carrying%20out%20the%20task%20of%20predicting%203-letter%20acronyms%20to%20demonstrate%20its%0Aeffectiveness%20on%20locating%20and%20understanding%20concrete%20vulnerabilities%20of%20the%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Understanding%2520Vulnerabilities%2520in%2520Language%2520Models%2520via%250A%2520%2520Mechanistic%2520Interpretability%26entry.906535625%3DJorge%2520Garc%25C3%25ADa-Carrasco%2520and%2520Alejandro%2520Mat%25C3%25A9%2520and%2520Juan%2520Trujillo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520characterized%2520by%2520being%2520trained%2520on%2520broad%2520amounts%250Aof%2520data%2520in%2520a%2520self-supervised%2520manner%252C%2520have%2520shown%2520impressive%2520performance%2520across%2520a%250Awide%2520range%2520of%2520tasks.%2520Indeed%252C%2520their%2520generative%2520abilities%2520have%2520aroused%2520interest%250Aon%2520the%2520application%2520of%2520LLMs%2520across%2520a%2520wide%2520range%2520of%2520contexts.%2520However%252C%2520neural%250Anetworks%2520in%2520general%252C%2520and%2520LLMs%2520in%2520particular%252C%2520are%2520known%2520to%2520be%2520vulnerable%2520to%250Aadversarial%2520attacks%252C%2520where%2520an%2520imperceptible%2520change%2520to%2520the%2520input%2520can%2520mislead%2520the%250Aoutput%2520of%2520the%2520model.%2520This%2520is%2520a%2520serious%2520concern%2520that%2520impedes%2520the%2520use%2520of%2520LLMs%2520on%250Ahigh-stakes%2520applications%252C%2520such%2520as%2520healthcare%252C%2520where%2520a%2520wrong%2520prediction%2520can%250Aimply%2520serious%2520consequences.%2520Even%2520though%2520there%2520are%2520many%2520efforts%2520on%2520making%2520LLMs%250Amore%2520robust%2520to%2520adversarial%2520attacks%252C%2520there%2520are%2520almost%2520no%2520works%2520that%2520study%250A%255Cemph%257Bhow%257D%2520and%2520%255Cemph%257Bwhere%257D%2520these%2520vulnerabilities%2520that%2520make%2520LLMs%2520prone%2520to%250Aadversarial%2520attacks%2520happen.%2520Motivated%2520by%2520these%2520facts%252C%2520we%2520explore%2520how%2520to%250Alocalize%2520and%2520understand%2520vulnerabilities%252C%2520and%2520propose%2520a%2520method%252C%2520based%2520on%250AMechanistic%2520Interpretability%2520%2528MI%2529%2520techniques%252C%2520to%2520guide%2520this%2520process.%250ASpecifically%252C%2520this%2520method%2520enables%2520us%2520to%2520detect%2520vulnerabilities%2520related%2520to%2520a%250Aconcrete%2520task%2520by%2520%2528i%2529%2520obtaining%2520the%2520subset%2520of%2520the%2520model%2520that%2520is%2520responsible%2520for%250Athat%2520task%252C%2520%2528ii%2529%2520generating%2520adversarial%2520samples%2520for%2520that%2520task%252C%2520and%2520%2528iii%2529%2520using%250AMI%2520techniques%2520together%2520with%2520the%2520previous%2520samples%2520to%2520discover%2520and%2520understand%2520the%250Apossible%2520vulnerabilities.%2520We%2520showcase%2520our%2520method%2520on%2520a%2520pretrained%2520GPT-2%2520Small%250Amodel%2520carrying%2520out%2520the%2520task%2520of%2520predicting%25203-letter%2520acronyms%2520to%2520demonstrate%2520its%250Aeffectiveness%2520on%2520locating%2520and%2520understanding%2520concrete%2520vulnerabilities%2520of%2520the%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Understanding%20Vulnerabilities%20in%20Language%20Models%20via%0A%20%20Mechanistic%20Interpretability&entry.906535625=Jorge%20Garc%C3%ADa-Carrasco%20and%20Alejandro%20Mat%C3%A9%20and%20Juan%20Trujillo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%2C%20characterized%20by%20being%20trained%20on%20broad%20amounts%0Aof%20data%20in%20a%20self-supervised%20manner%2C%20have%20shown%20impressive%20performance%20across%20a%0Awide%20range%20of%20tasks.%20Indeed%2C%20their%20generative%20abilities%20have%20aroused%20interest%0Aon%20the%20application%20of%20LLMs%20across%20a%20wide%20range%20of%20contexts.%20However%2C%20neural%0Anetworks%20in%20general%2C%20and%20LLMs%20in%20particular%2C%20are%20known%20to%20be%20vulnerable%20to%0Aadversarial%20attacks%2C%20where%20an%20imperceptible%20change%20to%20the%20input%20can%20mislead%20the%0Aoutput%20of%20the%20model.%20This%20is%20a%20serious%20concern%20that%20impedes%20the%20use%20of%20LLMs%20on%0Ahigh-stakes%20applications%2C%20such%20as%20healthcare%2C%20where%20a%20wrong%20prediction%20can%0Aimply%20serious%20consequences.%20Even%20though%20there%20are%20many%20efforts%20on%20making%20LLMs%0Amore%20robust%20to%20adversarial%20attacks%2C%20there%20are%20almost%20no%20works%20that%20study%0A%5Cemph%7Bhow%7D%20and%20%5Cemph%7Bwhere%7D%20these%20vulnerabilities%20that%20make%20LLMs%20prone%20to%0Aadversarial%20attacks%20happen.%20Motivated%20by%20these%20facts%2C%20we%20explore%20how%20to%0Alocalize%20and%20understand%20vulnerabilities%2C%20and%20propose%20a%20method%2C%20based%20on%0AMechanistic%20Interpretability%20%28MI%29%20techniques%2C%20to%20guide%20this%20process.%0ASpecifically%2C%20this%20method%20enables%20us%20to%20detect%20vulnerabilities%20related%20to%20a%0Aconcrete%20task%20by%20%28i%29%20obtaining%20the%20subset%20of%20the%20model%20that%20is%20responsible%20for%0Athat%20task%2C%20%28ii%29%20generating%20adversarial%20samples%20for%20that%20task%2C%20and%20%28iii%29%20using%0AMI%20techniques%20together%20with%20the%20previous%20samples%20to%20discover%20and%20understand%20the%0Apossible%20vulnerabilities.%20We%20showcase%20our%20method%20on%20a%20pretrained%20GPT-2%20Small%0Amodel%20carrying%20out%20the%20task%20of%20predicting%203-letter%20acronyms%20to%20demonstrate%20its%0Aeffectiveness%20on%20locating%20and%20understanding%20concrete%20vulnerabilities%20of%20the%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19842v1&entry.124074799=Read"},
{"title": "Extreme time extrapolation capabilities and thermodynamic consistency of\n  physics-inspired Neural Networks for the 3D microstructure evolution of\n  materials", "author": "Daniele Lanzoni and Andrea Fantasia and Roberto Bergamaschini and Olivier Pierre-Louis and Francesco Montalenti", "abstract": "  A Convolutional Recurrent Neural Network (CRNN) is trained to reproduce the\nevolution of the spinodal decomposition process in three dimensions as\ndescribed by the Cahn-Hilliard equation. A specialized, physics-inspired\narchitecture is proven to provide close accordance between the predicted\nevolutions and the ground truth ones obtained via conventional integration\nschemes. The method can closely reproduce the evolution of microstructures not\nrepresented in the training set at a fraction of the computational costs.\nExtremely long-time extrapolation capabilities are achieved, up to reaching the\ntheoretically expected equilibrium state of the system, despite the training\nset containing only relatively-short, initial phases of the evolution.\nQuantitative accordance with the decay rate of the Free energy is also\ndemonstrated up to late coarsening stages, providing an example of a\ndata-driven, physically consistent and high-accuracy Machine Learning method\nfor the long timescale simulation of materials.\n", "link": "http://arxiv.org/abs/2407.20126v1", "date": "2024-07-29", "relevancy": 1.9341, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4963}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4758}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extreme%20time%20extrapolation%20capabilities%20and%20thermodynamic%20consistency%20of%0A%20%20physics-inspired%20Neural%20Networks%20for%20the%203D%20microstructure%20evolution%20of%0A%20%20materials&body=Title%3A%20Extreme%20time%20extrapolation%20capabilities%20and%20thermodynamic%20consistency%20of%0A%20%20physics-inspired%20Neural%20Networks%20for%20the%203D%20microstructure%20evolution%20of%0A%20%20materials%0AAuthor%3A%20Daniele%20Lanzoni%20and%20Andrea%20Fantasia%20and%20Roberto%20Bergamaschini%20and%20Olivier%20Pierre-Louis%20and%20Francesco%20Montalenti%0AAbstract%3A%20%20%20A%20Convolutional%20Recurrent%20Neural%20Network%20%28CRNN%29%20is%20trained%20to%20reproduce%20the%0Aevolution%20of%20the%20spinodal%20decomposition%20process%20in%20three%20dimensions%20as%0Adescribed%20by%20the%20Cahn-Hilliard%20equation.%20A%20specialized%2C%20physics-inspired%0Aarchitecture%20is%20proven%20to%20provide%20close%20accordance%20between%20the%20predicted%0Aevolutions%20and%20the%20ground%20truth%20ones%20obtained%20via%20conventional%20integration%0Aschemes.%20The%20method%20can%20closely%20reproduce%20the%20evolution%20of%20microstructures%20not%0Arepresented%20in%20the%20training%20set%20at%20a%20fraction%20of%20the%20computational%20costs.%0AExtremely%20long-time%20extrapolation%20capabilities%20are%20achieved%2C%20up%20to%20reaching%20the%0Atheoretically%20expected%20equilibrium%20state%20of%20the%20system%2C%20despite%20the%20training%0Aset%20containing%20only%20relatively-short%2C%20initial%20phases%20of%20the%20evolution.%0AQuantitative%20accordance%20with%20the%20decay%20rate%20of%20the%20Free%20energy%20is%20also%0Ademonstrated%20up%20to%20late%20coarsening%20stages%2C%20providing%20an%20example%20of%20a%0Adata-driven%2C%20physically%20consistent%20and%20high-accuracy%20Machine%20Learning%20method%0Afor%20the%20long%20timescale%20simulation%20of%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtreme%2520time%2520extrapolation%2520capabilities%2520and%2520thermodynamic%2520consistency%2520of%250A%2520%2520physics-inspired%2520Neural%2520Networks%2520for%2520the%25203D%2520microstructure%2520evolution%2520of%250A%2520%2520materials%26entry.906535625%3DDaniele%2520Lanzoni%2520and%2520Andrea%2520Fantasia%2520and%2520Roberto%2520Bergamaschini%2520and%2520Olivier%2520Pierre-Louis%2520and%2520Francesco%2520Montalenti%26entry.1292438233%3D%2520%2520A%2520Convolutional%2520Recurrent%2520Neural%2520Network%2520%2528CRNN%2529%2520is%2520trained%2520to%2520reproduce%2520the%250Aevolution%2520of%2520the%2520spinodal%2520decomposition%2520process%2520in%2520three%2520dimensions%2520as%250Adescribed%2520by%2520the%2520Cahn-Hilliard%2520equation.%2520A%2520specialized%252C%2520physics-inspired%250Aarchitecture%2520is%2520proven%2520to%2520provide%2520close%2520accordance%2520between%2520the%2520predicted%250Aevolutions%2520and%2520the%2520ground%2520truth%2520ones%2520obtained%2520via%2520conventional%2520integration%250Aschemes.%2520The%2520method%2520can%2520closely%2520reproduce%2520the%2520evolution%2520of%2520microstructures%2520not%250Arepresented%2520in%2520the%2520training%2520set%2520at%2520a%2520fraction%2520of%2520the%2520computational%2520costs.%250AExtremely%2520long-time%2520extrapolation%2520capabilities%2520are%2520achieved%252C%2520up%2520to%2520reaching%2520the%250Atheoretically%2520expected%2520equilibrium%2520state%2520of%2520the%2520system%252C%2520despite%2520the%2520training%250Aset%2520containing%2520only%2520relatively-short%252C%2520initial%2520phases%2520of%2520the%2520evolution.%250AQuantitative%2520accordance%2520with%2520the%2520decay%2520rate%2520of%2520the%2520Free%2520energy%2520is%2520also%250Ademonstrated%2520up%2520to%2520late%2520coarsening%2520stages%252C%2520providing%2520an%2520example%2520of%2520a%250Adata-driven%252C%2520physically%2520consistent%2520and%2520high-accuracy%2520Machine%2520Learning%2520method%250Afor%2520the%2520long%2520timescale%2520simulation%2520of%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extreme%20time%20extrapolation%20capabilities%20and%20thermodynamic%20consistency%20of%0A%20%20physics-inspired%20Neural%20Networks%20for%20the%203D%20microstructure%20evolution%20of%0A%20%20materials&entry.906535625=Daniele%20Lanzoni%20and%20Andrea%20Fantasia%20and%20Roberto%20Bergamaschini%20and%20Olivier%20Pierre-Louis%20and%20Francesco%20Montalenti&entry.1292438233=%20%20A%20Convolutional%20Recurrent%20Neural%20Network%20%28CRNN%29%20is%20trained%20to%20reproduce%20the%0Aevolution%20of%20the%20spinodal%20decomposition%20process%20in%20three%20dimensions%20as%0Adescribed%20by%20the%20Cahn-Hilliard%20equation.%20A%20specialized%2C%20physics-inspired%0Aarchitecture%20is%20proven%20to%20provide%20close%20accordance%20between%20the%20predicted%0Aevolutions%20and%20the%20ground%20truth%20ones%20obtained%20via%20conventional%20integration%0Aschemes.%20The%20method%20can%20closely%20reproduce%20the%20evolution%20of%20microstructures%20not%0Arepresented%20in%20the%20training%20set%20at%20a%20fraction%20of%20the%20computational%20costs.%0AExtremely%20long-time%20extrapolation%20capabilities%20are%20achieved%2C%20up%20to%20reaching%20the%0Atheoretically%20expected%20equilibrium%20state%20of%20the%20system%2C%20despite%20the%20training%0Aset%20containing%20only%20relatively-short%2C%20initial%20phases%20of%20the%20evolution.%0AQuantitative%20accordance%20with%20the%20decay%20rate%20of%20the%20Free%20energy%20is%20also%0Ademonstrated%20up%20to%20late%20coarsening%20stages%2C%20providing%20an%20example%20of%20a%0Adata-driven%2C%20physically%20consistent%20and%20high-accuracy%20Machine%20Learning%20method%0Afor%20the%20long%20timescale%20simulation%20of%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20126v1&entry.124074799=Read"},
{"title": "To accept or not to accept? An IRT-TOE Framework to Understand\n  Educators' Resistance to Generative AI in Higher Education", "author": "Jan-Erik Kalmus and Anastasija Nikiforova", "abstract": "  Since the public release of Chat Generative Pre-Trained Transformer\n(ChatGPT), extensive discourse has emerged concerning the potential advantages\nand challenges of integrating Generative Artificial Intelligence (GenAI) into\neducation. In the realm of information systems, research on technology adoption\nis crucial for understanding the diverse factors influencing the uptake of\nspecific technologies. Theoretical frameworks, refined and validated over\ndecades, serve as guiding tools to elucidate the individual and organizational\ndynamics, obstacles, and perceptions surrounding technology adoption. However,\nwhile several models have been proposed, they often prioritize elucidating the\nfactors that facilitate acceptance over those that impede it, typically\nfocusing on the student perspective and leaving a gap in empirical evidence\nregarding educators viewpoints. Given the pivotal role educators play in higher\neducation, this study aims to develop a theoretical model to empirically\npredict the barriers preventing educators from adopting GenAI in their\nclassrooms. Acknowledging the lack of theoretical models tailored to\nidentifying such barriers, our approach is grounded in the Innovation\nResistance Theory (IRT) framework and augmented with constructs from the\nTechnology-Organization-Environment (TOE) framework. This model is transformed\ninto a measurement instrument employing a quantitative approach, complemented\nby a qualitative approach to enrich the analysis and uncover concerns related\nto GenAI adoption in the higher education domain.\n", "link": "http://arxiv.org/abs/2407.20130v1", "date": "2024-07-29", "relevancy": 1.9209, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.499}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4812}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20accept%20or%20not%20to%20accept%3F%20An%20IRT-TOE%20Framework%20to%20Understand%0A%20%20Educators%27%20Resistance%20to%20Generative%20AI%20in%20Higher%20Education&body=Title%3A%20To%20accept%20or%20not%20to%20accept%3F%20An%20IRT-TOE%20Framework%20to%20Understand%0A%20%20Educators%27%20Resistance%20to%20Generative%20AI%20in%20Higher%20Education%0AAuthor%3A%20Jan-Erik%20Kalmus%20and%20Anastasija%20Nikiforova%0AAbstract%3A%20%20%20Since%20the%20public%20release%20of%20Chat%20Generative%20Pre-Trained%20Transformer%0A%28ChatGPT%29%2C%20extensive%20discourse%20has%20emerged%20concerning%20the%20potential%20advantages%0Aand%20challenges%20of%20integrating%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20into%0Aeducation.%20In%20the%20realm%20of%20information%20systems%2C%20research%20on%20technology%20adoption%0Ais%20crucial%20for%20understanding%20the%20diverse%20factors%20influencing%20the%20uptake%20of%0Aspecific%20technologies.%20Theoretical%20frameworks%2C%20refined%20and%20validated%20over%0Adecades%2C%20serve%20as%20guiding%20tools%20to%20elucidate%20the%20individual%20and%20organizational%0Adynamics%2C%20obstacles%2C%20and%20perceptions%20surrounding%20technology%20adoption.%20However%2C%0Awhile%20several%20models%20have%20been%20proposed%2C%20they%20often%20prioritize%20elucidating%20the%0Afactors%20that%20facilitate%20acceptance%20over%20those%20that%20impede%20it%2C%20typically%0Afocusing%20on%20the%20student%20perspective%20and%20leaving%20a%20gap%20in%20empirical%20evidence%0Aregarding%20educators%20viewpoints.%20Given%20the%20pivotal%20role%20educators%20play%20in%20higher%0Aeducation%2C%20this%20study%20aims%20to%20develop%20a%20theoretical%20model%20to%20empirically%0Apredict%20the%20barriers%20preventing%20educators%20from%20adopting%20GenAI%20in%20their%0Aclassrooms.%20Acknowledging%20the%20lack%20of%20theoretical%20models%20tailored%20to%0Aidentifying%20such%20barriers%2C%20our%20approach%20is%20grounded%20in%20the%20Innovation%0AResistance%20Theory%20%28IRT%29%20framework%20and%20augmented%20with%20constructs%20from%20the%0ATechnology-Organization-Environment%20%28TOE%29%20framework.%20This%20model%20is%20transformed%0Ainto%20a%20measurement%20instrument%20employing%20a%20quantitative%20approach%2C%20complemented%0Aby%20a%20qualitative%20approach%20to%20enrich%20the%20analysis%20and%20uncover%20concerns%20related%0Ato%20GenAI%20adoption%20in%20the%20higher%20education%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520accept%2520or%2520not%2520to%2520accept%253F%2520An%2520IRT-TOE%2520Framework%2520to%2520Understand%250A%2520%2520Educators%2527%2520Resistance%2520to%2520Generative%2520AI%2520in%2520Higher%2520Education%26entry.906535625%3DJan-Erik%2520Kalmus%2520and%2520Anastasija%2520Nikiforova%26entry.1292438233%3D%2520%2520Since%2520the%2520public%2520release%2520of%2520Chat%2520Generative%2520Pre-Trained%2520Transformer%250A%2528ChatGPT%2529%252C%2520extensive%2520discourse%2520has%2520emerged%2520concerning%2520the%2520potential%2520advantages%250Aand%2520challenges%2520of%2520integrating%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520into%250Aeducation.%2520In%2520the%2520realm%2520of%2520information%2520systems%252C%2520research%2520on%2520technology%2520adoption%250Ais%2520crucial%2520for%2520understanding%2520the%2520diverse%2520factors%2520influencing%2520the%2520uptake%2520of%250Aspecific%2520technologies.%2520Theoretical%2520frameworks%252C%2520refined%2520and%2520validated%2520over%250Adecades%252C%2520serve%2520as%2520guiding%2520tools%2520to%2520elucidate%2520the%2520individual%2520and%2520organizational%250Adynamics%252C%2520obstacles%252C%2520and%2520perceptions%2520surrounding%2520technology%2520adoption.%2520However%252C%250Awhile%2520several%2520models%2520have%2520been%2520proposed%252C%2520they%2520often%2520prioritize%2520elucidating%2520the%250Afactors%2520that%2520facilitate%2520acceptance%2520over%2520those%2520that%2520impede%2520it%252C%2520typically%250Afocusing%2520on%2520the%2520student%2520perspective%2520and%2520leaving%2520a%2520gap%2520in%2520empirical%2520evidence%250Aregarding%2520educators%2520viewpoints.%2520Given%2520the%2520pivotal%2520role%2520educators%2520play%2520in%2520higher%250Aeducation%252C%2520this%2520study%2520aims%2520to%2520develop%2520a%2520theoretical%2520model%2520to%2520empirically%250Apredict%2520the%2520barriers%2520preventing%2520educators%2520from%2520adopting%2520GenAI%2520in%2520their%250Aclassrooms.%2520Acknowledging%2520the%2520lack%2520of%2520theoretical%2520models%2520tailored%2520to%250Aidentifying%2520such%2520barriers%252C%2520our%2520approach%2520is%2520grounded%2520in%2520the%2520Innovation%250AResistance%2520Theory%2520%2528IRT%2529%2520framework%2520and%2520augmented%2520with%2520constructs%2520from%2520the%250ATechnology-Organization-Environment%2520%2528TOE%2529%2520framework.%2520This%2520model%2520is%2520transformed%250Ainto%2520a%2520measurement%2520instrument%2520employing%2520a%2520quantitative%2520approach%252C%2520complemented%250Aby%2520a%2520qualitative%2520approach%2520to%2520enrich%2520the%2520analysis%2520and%2520uncover%2520concerns%2520related%250Ato%2520GenAI%2520adoption%2520in%2520the%2520higher%2520education%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20accept%20or%20not%20to%20accept%3F%20An%20IRT-TOE%20Framework%20to%20Understand%0A%20%20Educators%27%20Resistance%20to%20Generative%20AI%20in%20Higher%20Education&entry.906535625=Jan-Erik%20Kalmus%20and%20Anastasija%20Nikiforova&entry.1292438233=%20%20Since%20the%20public%20release%20of%20Chat%20Generative%20Pre-Trained%20Transformer%0A%28ChatGPT%29%2C%20extensive%20discourse%20has%20emerged%20concerning%20the%20potential%20advantages%0Aand%20challenges%20of%20integrating%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20into%0Aeducation.%20In%20the%20realm%20of%20information%20systems%2C%20research%20on%20technology%20adoption%0Ais%20crucial%20for%20understanding%20the%20diverse%20factors%20influencing%20the%20uptake%20of%0Aspecific%20technologies.%20Theoretical%20frameworks%2C%20refined%20and%20validated%20over%0Adecades%2C%20serve%20as%20guiding%20tools%20to%20elucidate%20the%20individual%20and%20organizational%0Adynamics%2C%20obstacles%2C%20and%20perceptions%20surrounding%20technology%20adoption.%20However%2C%0Awhile%20several%20models%20have%20been%20proposed%2C%20they%20often%20prioritize%20elucidating%20the%0Afactors%20that%20facilitate%20acceptance%20over%20those%20that%20impede%20it%2C%20typically%0Afocusing%20on%20the%20student%20perspective%20and%20leaving%20a%20gap%20in%20empirical%20evidence%0Aregarding%20educators%20viewpoints.%20Given%20the%20pivotal%20role%20educators%20play%20in%20higher%0Aeducation%2C%20this%20study%20aims%20to%20develop%20a%20theoretical%20model%20to%20empirically%0Apredict%20the%20barriers%20preventing%20educators%20from%20adopting%20GenAI%20in%20their%0Aclassrooms.%20Acknowledging%20the%20lack%20of%20theoretical%20models%20tailored%20to%0Aidentifying%20such%20barriers%2C%20our%20approach%20is%20grounded%20in%20the%20Innovation%0AResistance%20Theory%20%28IRT%29%20framework%20and%20augmented%20with%20constructs%20from%20the%0ATechnology-Organization-Environment%20%28TOE%29%20framework.%20This%20model%20is%20transformed%0Ainto%20a%20measurement%20instrument%20employing%20a%20quantitative%20approach%2C%20complemented%0Aby%20a%20qualitative%20approach%20to%20enrich%20the%20analysis%20and%20uncover%20concerns%20related%0Ato%20GenAI%20adoption%20in%20the%20higher%20education%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20130v1&entry.124074799=Read"},
{"title": "reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis", "author": "Kai Norman Clasen and Leonard Hackel and Tom Burgert and Gencer Sumbul and Beg\u00fcm Demir and Volker Markl", "abstract": "  This paper presents refined BigEarthNet (reBEN) that is a large-scale,\nmulti-modal remote sensing dataset constructed to support deep learning (DL)\nstudies for remote sensing image analysis. The reBEN dataset consists of\n549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN,\nwe initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the\nBigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m.\nWe apply atmospheric correction to the Sentinel-2 patches using the latest\nversion of the sen2cor tool, resulting in higher-quality patches compared to\nthose present in BigEarthNet. Each patch is then associated with a pixel-level\nreference map and scene-level multi-labels. This makes reBEN suitable for\npixel- and scene-based learning tasks. The labels are derived from the most\nrecent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class\nnomenclature as in BigEarthNet. The use of the most recent CLC map results in\novercoming the label noise present in BigEarthNet. Furthermore, we introduce a\nnew geographical-based split assignment algorithm that significantly reduces\nthe spatial correlation among the train, validation, and test sets with respect\nto those present in BigEarthNet. This increases the reliability of the\nevaluation of DL models. To minimize the DL model training time, we introduce\nsoftware tools that convert the reBEN dataset into a DL-optimized data format.\nIn our experiments, we show the potential of reBEN for multi-modal multi-label\nimage classification problems by considering several state-of-the-art DL\nmodels. The pre-trained model weights, associated code, and complete dataset\nare available at https://bigearth.net.\n", "link": "http://arxiv.org/abs/2407.03653v2", "date": "2024-07-29", "relevancy": 1.9192, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4779}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20reBEN%3A%20Refined%20BigEarthNet%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis&body=Title%3A%20reBEN%3A%20Refined%20BigEarthNet%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis%0AAuthor%3A%20Kai%20Norman%20Clasen%20and%20Leonard%20Hackel%20and%20Tom%20Burgert%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir%20and%20Volker%20Markl%0AAbstract%3A%20%20%20This%20paper%20presents%20refined%20BigEarthNet%20%28reBEN%29%20that%20is%20a%20large-scale%2C%0Amulti-modal%20remote%20sensing%20dataset%20constructed%20to%20support%20deep%20learning%20%28DL%29%0Astudies%20for%20remote%20sensing%20image%20analysis.%20The%20reBEN%20dataset%20consists%20of%0A549%2C488%20pairs%20of%20Sentinel-1%20and%20Sentinel-2%20image%20patches.%20To%20construct%20reBEN%2C%0Awe%20initially%20consider%20the%20Sentinel-1%20and%20Sentinel-2%20tiles%20used%20to%20construct%20the%0ABigEarthNet%20dataset%20and%20then%20divide%20them%20into%20patches%20of%20size%201200%20m%20x%201200%20m.%0AWe%20apply%20atmospheric%20correction%20to%20the%20Sentinel-2%20patches%20using%20the%20latest%0Aversion%20of%20the%20sen2cor%20tool%2C%20resulting%20in%20higher-quality%20patches%20compared%20to%0Athose%20present%20in%20BigEarthNet.%20Each%20patch%20is%20then%20associated%20with%20a%20pixel-level%0Areference%20map%20and%20scene-level%20multi-labels.%20This%20makes%20reBEN%20suitable%20for%0Apixel-%20and%20scene-based%20learning%20tasks.%20The%20labels%20are%20derived%20from%20the%20most%0Arecent%20CORINE%20Land%20Cover%20%28CLC%29%20map%20of%202018%20by%20utilizing%20the%2019-class%0Anomenclature%20as%20in%20BigEarthNet.%20The%20use%20of%20the%20most%20recent%20CLC%20map%20results%20in%0Aovercoming%20the%20label%20noise%20present%20in%20BigEarthNet.%20Furthermore%2C%20we%20introduce%20a%0Anew%20geographical-based%20split%20assignment%20algorithm%20that%20significantly%20reduces%0Athe%20spatial%20correlation%20among%20the%20train%2C%20validation%2C%20and%20test%20sets%20with%20respect%0Ato%20those%20present%20in%20BigEarthNet.%20This%20increases%20the%20reliability%20of%20the%0Aevaluation%20of%20DL%20models.%20To%20minimize%20the%20DL%20model%20training%20time%2C%20we%20introduce%0Asoftware%20tools%20that%20convert%20the%20reBEN%20dataset%20into%20a%20DL-optimized%20data%20format.%0AIn%20our%20experiments%2C%20we%20show%20the%20potential%20of%20reBEN%20for%20multi-modal%20multi-label%0Aimage%20classification%20problems%20by%20considering%20several%20state-of-the-art%20DL%0Amodels.%20The%20pre-trained%20model%20weights%2C%20associated%20code%2C%20and%20complete%20dataset%0Aare%20available%20at%20https%3A//bigearth.net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DreBEN%253A%2520Refined%2520BigEarthNet%2520Dataset%2520for%2520Remote%2520Sensing%2520Image%2520Analysis%26entry.906535625%3DKai%2520Norman%2520Clasen%2520and%2520Leonard%2520Hackel%2520and%2520Tom%2520Burgert%2520and%2520Gencer%2520Sumbul%2520and%2520Beg%25C3%25BCm%2520Demir%2520and%2520Volker%2520Markl%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520refined%2520BigEarthNet%2520%2528reBEN%2529%2520that%2520is%2520a%2520large-scale%252C%250Amulti-modal%2520remote%2520sensing%2520dataset%2520constructed%2520to%2520support%2520deep%2520learning%2520%2528DL%2529%250Astudies%2520for%2520remote%2520sensing%2520image%2520analysis.%2520The%2520reBEN%2520dataset%2520consists%2520of%250A549%252C488%2520pairs%2520of%2520Sentinel-1%2520and%2520Sentinel-2%2520image%2520patches.%2520To%2520construct%2520reBEN%252C%250Awe%2520initially%2520consider%2520the%2520Sentinel-1%2520and%2520Sentinel-2%2520tiles%2520used%2520to%2520construct%2520the%250ABigEarthNet%2520dataset%2520and%2520then%2520divide%2520them%2520into%2520patches%2520of%2520size%25201200%2520m%2520x%25201200%2520m.%250AWe%2520apply%2520atmospheric%2520correction%2520to%2520the%2520Sentinel-2%2520patches%2520using%2520the%2520latest%250Aversion%2520of%2520the%2520sen2cor%2520tool%252C%2520resulting%2520in%2520higher-quality%2520patches%2520compared%2520to%250Athose%2520present%2520in%2520BigEarthNet.%2520Each%2520patch%2520is%2520then%2520associated%2520with%2520a%2520pixel-level%250Areference%2520map%2520and%2520scene-level%2520multi-labels.%2520This%2520makes%2520reBEN%2520suitable%2520for%250Apixel-%2520and%2520scene-based%2520learning%2520tasks.%2520The%2520labels%2520are%2520derived%2520from%2520the%2520most%250Arecent%2520CORINE%2520Land%2520Cover%2520%2528CLC%2529%2520map%2520of%25202018%2520by%2520utilizing%2520the%252019-class%250Anomenclature%2520as%2520in%2520BigEarthNet.%2520The%2520use%2520of%2520the%2520most%2520recent%2520CLC%2520map%2520results%2520in%250Aovercoming%2520the%2520label%2520noise%2520present%2520in%2520BigEarthNet.%2520Furthermore%252C%2520we%2520introduce%2520a%250Anew%2520geographical-based%2520split%2520assignment%2520algorithm%2520that%2520significantly%2520reduces%250Athe%2520spatial%2520correlation%2520among%2520the%2520train%252C%2520validation%252C%2520and%2520test%2520sets%2520with%2520respect%250Ato%2520those%2520present%2520in%2520BigEarthNet.%2520This%2520increases%2520the%2520reliability%2520of%2520the%250Aevaluation%2520of%2520DL%2520models.%2520To%2520minimize%2520the%2520DL%2520model%2520training%2520time%252C%2520we%2520introduce%250Asoftware%2520tools%2520that%2520convert%2520the%2520reBEN%2520dataset%2520into%2520a%2520DL-optimized%2520data%2520format.%250AIn%2520our%2520experiments%252C%2520we%2520show%2520the%2520potential%2520of%2520reBEN%2520for%2520multi-modal%2520multi-label%250Aimage%2520classification%2520problems%2520by%2520considering%2520several%2520state-of-the-art%2520DL%250Amodels.%2520The%2520pre-trained%2520model%2520weights%252C%2520associated%2520code%252C%2520and%2520complete%2520dataset%250Aare%2520available%2520at%2520https%253A//bigearth.net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=reBEN%3A%20Refined%20BigEarthNet%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis&entry.906535625=Kai%20Norman%20Clasen%20and%20Leonard%20Hackel%20and%20Tom%20Burgert%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir%20and%20Volker%20Markl&entry.1292438233=%20%20This%20paper%20presents%20refined%20BigEarthNet%20%28reBEN%29%20that%20is%20a%20large-scale%2C%0Amulti-modal%20remote%20sensing%20dataset%20constructed%20to%20support%20deep%20learning%20%28DL%29%0Astudies%20for%20remote%20sensing%20image%20analysis.%20The%20reBEN%20dataset%20consists%20of%0A549%2C488%20pairs%20of%20Sentinel-1%20and%20Sentinel-2%20image%20patches.%20To%20construct%20reBEN%2C%0Awe%20initially%20consider%20the%20Sentinel-1%20and%20Sentinel-2%20tiles%20used%20to%20construct%20the%0ABigEarthNet%20dataset%20and%20then%20divide%20them%20into%20patches%20of%20size%201200%20m%20x%201200%20m.%0AWe%20apply%20atmospheric%20correction%20to%20the%20Sentinel-2%20patches%20using%20the%20latest%0Aversion%20of%20the%20sen2cor%20tool%2C%20resulting%20in%20higher-quality%20patches%20compared%20to%0Athose%20present%20in%20BigEarthNet.%20Each%20patch%20is%20then%20associated%20with%20a%20pixel-level%0Areference%20map%20and%20scene-level%20multi-labels.%20This%20makes%20reBEN%20suitable%20for%0Apixel-%20and%20scene-based%20learning%20tasks.%20The%20labels%20are%20derived%20from%20the%20most%0Arecent%20CORINE%20Land%20Cover%20%28CLC%29%20map%20of%202018%20by%20utilizing%20the%2019-class%0Anomenclature%20as%20in%20BigEarthNet.%20The%20use%20of%20the%20most%20recent%20CLC%20map%20results%20in%0Aovercoming%20the%20label%20noise%20present%20in%20BigEarthNet.%20Furthermore%2C%20we%20introduce%20a%0Anew%20geographical-based%20split%20assignment%20algorithm%20that%20significantly%20reduces%0Athe%20spatial%20correlation%20among%20the%20train%2C%20validation%2C%20and%20test%20sets%20with%20respect%0Ato%20those%20present%20in%20BigEarthNet.%20This%20increases%20the%20reliability%20of%20the%0Aevaluation%20of%20DL%20models.%20To%20minimize%20the%20DL%20model%20training%20time%2C%20we%20introduce%0Asoftware%20tools%20that%20convert%20the%20reBEN%20dataset%20into%20a%20DL-optimized%20data%20format.%0AIn%20our%20experiments%2C%20we%20show%20the%20potential%20of%20reBEN%20for%20multi-modal%20multi-label%0Aimage%20classification%20problems%20by%20considering%20several%20state-of-the-art%20DL%0Amodels.%20The%20pre-trained%20model%20weights%2C%20associated%20code%2C%20and%20complete%20dataset%0Aare%20available%20at%20https%3A//bigearth.net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03653v2&entry.124074799=Read"},
{"title": "Adaptive Self-supervised Robust Clustering for Unstructured Data with\n  Unknown Cluster Number", "author": "Chen-Lu Ding and Jiancan Wu and Wei Lin and Shiyang Shen and Xiang Wang and Yancheng Yuan", "abstract": "  We introduce a novel self-supervised deep clustering approach tailored for\nunstructured data without requiring prior knowledge of the number of clusters,\ntermed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRC\nadaptively learns the graph structure and edge weights to capture both local\nand global structural information. The obtained graph enables us to learn\nclustering-friendly feature representations by an enhanced graph auto-encoder\nwith contrastive learning technique. It further leverages the clustering\nresults adaptively obtained by robust continuous clustering (RCC) to generate\nprototypes for negative sampling, which can further contribute to promoting\nconsistency among positive pairs and enlarging the gap between positive and\nnegative samples. ASRC obtains the final clustering results by applying RCC to\nthe learned feature representations with their consistent graph structure and\nedge weights. Extensive experiments conducted on seven benchmark datasets\ndemonstrate the efficacy of ASRC, demonstrating its superior performance over\nother popular clustering models. Notably, ASRC even outperforms methods that\nrely on prior knowledge of the number of clusters, highlighting its\neffectiveness in addressing the challenges of clustering unstructured data.\n", "link": "http://arxiv.org/abs/2407.20119v1", "date": "2024-07-29", "relevancy": 1.919, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4997}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.476}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Self-supervised%20Robust%20Clustering%20for%20Unstructured%20Data%20with%0A%20%20Unknown%20Cluster%20Number&body=Title%3A%20Adaptive%20Self-supervised%20Robust%20Clustering%20for%20Unstructured%20Data%20with%0A%20%20Unknown%20Cluster%20Number%0AAuthor%3A%20Chen-Lu%20Ding%20and%20Jiancan%20Wu%20and%20Wei%20Lin%20and%20Shiyang%20Shen%20and%20Xiang%20Wang%20and%20Yancheng%20Yuan%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20self-supervised%20deep%20clustering%20approach%20tailored%20for%0Aunstructured%20data%20without%20requiring%20prior%20knowledge%20of%20the%20number%20of%20clusters%2C%0Atermed%20Adaptive%20Self-supervised%20Robust%20Clustering%20%28ASRC%29.%20In%20particular%2C%20ASRC%0Aadaptively%20learns%20the%20graph%20structure%20and%20edge%20weights%20to%20capture%20both%20local%0Aand%20global%20structural%20information.%20The%20obtained%20graph%20enables%20us%20to%20learn%0Aclustering-friendly%20feature%20representations%20by%20an%20enhanced%20graph%20auto-encoder%0Awith%20contrastive%20learning%20technique.%20It%20further%20leverages%20the%20clustering%0Aresults%20adaptively%20obtained%20by%20robust%20continuous%20clustering%20%28RCC%29%20to%20generate%0Aprototypes%20for%20negative%20sampling%2C%20which%20can%20further%20contribute%20to%20promoting%0Aconsistency%20among%20positive%20pairs%20and%20enlarging%20the%20gap%20between%20positive%20and%0Anegative%20samples.%20ASRC%20obtains%20the%20final%20clustering%20results%20by%20applying%20RCC%20to%0Athe%20learned%20feature%20representations%20with%20their%20consistent%20graph%20structure%20and%0Aedge%20weights.%20Extensive%20experiments%20conducted%20on%20seven%20benchmark%20datasets%0Ademonstrate%20the%20efficacy%20of%20ASRC%2C%20demonstrating%20its%20superior%20performance%20over%0Aother%20popular%20clustering%20models.%20Notably%2C%20ASRC%20even%20outperforms%20methods%20that%0Arely%20on%20prior%20knowledge%20of%20the%20number%20of%20clusters%2C%20highlighting%20its%0Aeffectiveness%20in%20addressing%20the%20challenges%20of%20clustering%20unstructured%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Self-supervised%2520Robust%2520Clustering%2520for%2520Unstructured%2520Data%2520with%250A%2520%2520Unknown%2520Cluster%2520Number%26entry.906535625%3DChen-Lu%2520Ding%2520and%2520Jiancan%2520Wu%2520and%2520Wei%2520Lin%2520and%2520Shiyang%2520Shen%2520and%2520Xiang%2520Wang%2520and%2520Yancheng%2520Yuan%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520self-supervised%2520deep%2520clustering%2520approach%2520tailored%2520for%250Aunstructured%2520data%2520without%2520requiring%2520prior%2520knowledge%2520of%2520the%2520number%2520of%2520clusters%252C%250Atermed%2520Adaptive%2520Self-supervised%2520Robust%2520Clustering%2520%2528ASRC%2529.%2520In%2520particular%252C%2520ASRC%250Aadaptively%2520learns%2520the%2520graph%2520structure%2520and%2520edge%2520weights%2520to%2520capture%2520both%2520local%250Aand%2520global%2520structural%2520information.%2520The%2520obtained%2520graph%2520enables%2520us%2520to%2520learn%250Aclustering-friendly%2520feature%2520representations%2520by%2520an%2520enhanced%2520graph%2520auto-encoder%250Awith%2520contrastive%2520learning%2520technique.%2520It%2520further%2520leverages%2520the%2520clustering%250Aresults%2520adaptively%2520obtained%2520by%2520robust%2520continuous%2520clustering%2520%2528RCC%2529%2520to%2520generate%250Aprototypes%2520for%2520negative%2520sampling%252C%2520which%2520can%2520further%2520contribute%2520to%2520promoting%250Aconsistency%2520among%2520positive%2520pairs%2520and%2520enlarging%2520the%2520gap%2520between%2520positive%2520and%250Anegative%2520samples.%2520ASRC%2520obtains%2520the%2520final%2520clustering%2520results%2520by%2520applying%2520RCC%2520to%250Athe%2520learned%2520feature%2520representations%2520with%2520their%2520consistent%2520graph%2520structure%2520and%250Aedge%2520weights.%2520Extensive%2520experiments%2520conducted%2520on%2520seven%2520benchmark%2520datasets%250Ademonstrate%2520the%2520efficacy%2520of%2520ASRC%252C%2520demonstrating%2520its%2520superior%2520performance%2520over%250Aother%2520popular%2520clustering%2520models.%2520Notably%252C%2520ASRC%2520even%2520outperforms%2520methods%2520that%250Arely%2520on%2520prior%2520knowledge%2520of%2520the%2520number%2520of%2520clusters%252C%2520highlighting%2520its%250Aeffectiveness%2520in%2520addressing%2520the%2520challenges%2520of%2520clustering%2520unstructured%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Self-supervised%20Robust%20Clustering%20for%20Unstructured%20Data%20with%0A%20%20Unknown%20Cluster%20Number&entry.906535625=Chen-Lu%20Ding%20and%20Jiancan%20Wu%20and%20Wei%20Lin%20and%20Shiyang%20Shen%20and%20Xiang%20Wang%20and%20Yancheng%20Yuan&entry.1292438233=%20%20We%20introduce%20a%20novel%20self-supervised%20deep%20clustering%20approach%20tailored%20for%0Aunstructured%20data%20without%20requiring%20prior%20knowledge%20of%20the%20number%20of%20clusters%2C%0Atermed%20Adaptive%20Self-supervised%20Robust%20Clustering%20%28ASRC%29.%20In%20particular%2C%20ASRC%0Aadaptively%20learns%20the%20graph%20structure%20and%20edge%20weights%20to%20capture%20both%20local%0Aand%20global%20structural%20information.%20The%20obtained%20graph%20enables%20us%20to%20learn%0Aclustering-friendly%20feature%20representations%20by%20an%20enhanced%20graph%20auto-encoder%0Awith%20contrastive%20learning%20technique.%20It%20further%20leverages%20the%20clustering%0Aresults%20adaptively%20obtained%20by%20robust%20continuous%20clustering%20%28RCC%29%20to%20generate%0Aprototypes%20for%20negative%20sampling%2C%20which%20can%20further%20contribute%20to%20promoting%0Aconsistency%20among%20positive%20pairs%20and%20enlarging%20the%20gap%20between%20positive%20and%0Anegative%20samples.%20ASRC%20obtains%20the%20final%20clustering%20results%20by%20applying%20RCC%20to%0Athe%20learned%20feature%20representations%20with%20their%20consistent%20graph%20structure%20and%0Aedge%20weights.%20Extensive%20experiments%20conducted%20on%20seven%20benchmark%20datasets%0Ademonstrate%20the%20efficacy%20of%20ASRC%2C%20demonstrating%20its%20superior%20performance%20over%0Aother%20popular%20clustering%20models.%20Notably%2C%20ASRC%20even%20outperforms%20methods%20that%0Arely%20on%20prior%20knowledge%20of%20the%20number%20of%20clusters%2C%20highlighting%20its%0Aeffectiveness%20in%20addressing%20the%20challenges%20of%20clustering%20unstructured%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20119v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Text Recognition: A Critical Survey", "author": "Carlos Penarrubia and Jose J. Valero-Mas and Jorge Calvo-Zaragoza", "abstract": "  Text Recognition (TR) refers to the research area that focuses on retrieving\ntextual information from images, a topic that has seen significant advancements\nin the last decade due to the use of Deep Neural Networks (DNN). However, these\nsolutions often necessitate vast amounts of manually labeled or synthetic data.\nAddressing this challenge, Self-Supervised Learning (SSL) has gained attention\nby utilizing large datasets of unlabeled data to train DNN, thereby generating\nmeaningful and robust representations. Although SSL was initially overlooked in\nTR because of its unique characteristics, recent years have witnessed a surge\nin the development of SSL methods specifically for this field. This rapid\ndevelopment, however, has led to many methods being explored independently,\nwithout taking previous efforts in methodology or comparison into account,\nthereby hindering progress in the field of research. This paper, therefore,\nseeks to consolidate the use of SSL in the field of TR, offering a critical and\ncomprehensive overview of the current state of the art. We will review and\nanalyze the existing methods, compare their results, and highlight\ninconsistencies in the current literature. This thorough analysis aims to\nprovide general insights into the field, propose standardizations, identify new\nresearch directions, and foster its proper development.\n", "link": "http://arxiv.org/abs/2407.19889v1", "date": "2024-07-29", "relevancy": 1.9134, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5136}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4574}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Text%20Recognition%3A%20A%20Critical%20Survey&body=Title%3A%20Self-Supervised%20Learning%20for%20Text%20Recognition%3A%20A%20Critical%20Survey%0AAuthor%3A%20Carlos%20Penarrubia%20and%20Jose%20J.%20Valero-Mas%20and%20Jorge%20Calvo-Zaragoza%0AAbstract%3A%20%20%20Text%20Recognition%20%28TR%29%20refers%20to%20the%20research%20area%20that%20focuses%20on%20retrieving%0Atextual%20information%20from%20images%2C%20a%20topic%20that%20has%20seen%20significant%20advancements%0Ain%20the%20last%20decade%20due%20to%20the%20use%20of%20Deep%20Neural%20Networks%20%28DNN%29.%20However%2C%20these%0Asolutions%20often%20necessitate%20vast%20amounts%20of%20manually%20labeled%20or%20synthetic%20data.%0AAddressing%20this%20challenge%2C%20Self-Supervised%20Learning%20%28SSL%29%20has%20gained%20attention%0Aby%20utilizing%20large%20datasets%20of%20unlabeled%20data%20to%20train%20DNN%2C%20thereby%20generating%0Ameaningful%20and%20robust%20representations.%20Although%20SSL%20was%20initially%20overlooked%20in%0ATR%20because%20of%20its%20unique%20characteristics%2C%20recent%20years%20have%20witnessed%20a%20surge%0Ain%20the%20development%20of%20SSL%20methods%20specifically%20for%20this%20field.%20This%20rapid%0Adevelopment%2C%20however%2C%20has%20led%20to%20many%20methods%20being%20explored%20independently%2C%0Awithout%20taking%20previous%20efforts%20in%20methodology%20or%20comparison%20into%20account%2C%0Athereby%20hindering%20progress%20in%20the%20field%20of%20research.%20This%20paper%2C%20therefore%2C%0Aseeks%20to%20consolidate%20the%20use%20of%20SSL%20in%20the%20field%20of%20TR%2C%20offering%20a%20critical%20and%0Acomprehensive%20overview%20of%20the%20current%20state%20of%20the%20art.%20We%20will%20review%20and%0Aanalyze%20the%20existing%20methods%2C%20compare%20their%20results%2C%20and%20highlight%0Ainconsistencies%20in%20the%20current%20literature.%20This%20thorough%20analysis%20aims%20to%0Aprovide%20general%20insights%20into%20the%20field%2C%20propose%20standardizations%2C%20identify%20new%0Aresearch%20directions%2C%20and%20foster%20its%20proper%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520for%2520Text%2520Recognition%253A%2520A%2520Critical%2520Survey%26entry.906535625%3DCarlos%2520Penarrubia%2520and%2520Jose%2520J.%2520Valero-Mas%2520and%2520Jorge%2520Calvo-Zaragoza%26entry.1292438233%3D%2520%2520Text%2520Recognition%2520%2528TR%2529%2520refers%2520to%2520the%2520research%2520area%2520that%2520focuses%2520on%2520retrieving%250Atextual%2520information%2520from%2520images%252C%2520a%2520topic%2520that%2520has%2520seen%2520significant%2520advancements%250Ain%2520the%2520last%2520decade%2520due%2520to%2520the%2520use%2520of%2520Deep%2520Neural%2520Networks%2520%2528DNN%2529.%2520However%252C%2520these%250Asolutions%2520often%2520necessitate%2520vast%2520amounts%2520of%2520manually%2520labeled%2520or%2520synthetic%2520data.%250AAddressing%2520this%2520challenge%252C%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520has%2520gained%2520attention%250Aby%2520utilizing%2520large%2520datasets%2520of%2520unlabeled%2520data%2520to%2520train%2520DNN%252C%2520thereby%2520generating%250Ameaningful%2520and%2520robust%2520representations.%2520Although%2520SSL%2520was%2520initially%2520overlooked%2520in%250ATR%2520because%2520of%2520its%2520unique%2520characteristics%252C%2520recent%2520years%2520have%2520witnessed%2520a%2520surge%250Ain%2520the%2520development%2520of%2520SSL%2520methods%2520specifically%2520for%2520this%2520field.%2520This%2520rapid%250Adevelopment%252C%2520however%252C%2520has%2520led%2520to%2520many%2520methods%2520being%2520explored%2520independently%252C%250Awithout%2520taking%2520previous%2520efforts%2520in%2520methodology%2520or%2520comparison%2520into%2520account%252C%250Athereby%2520hindering%2520progress%2520in%2520the%2520field%2520of%2520research.%2520This%2520paper%252C%2520therefore%252C%250Aseeks%2520to%2520consolidate%2520the%2520use%2520of%2520SSL%2520in%2520the%2520field%2520of%2520TR%252C%2520offering%2520a%2520critical%2520and%250Acomprehensive%2520overview%2520of%2520the%2520current%2520state%2520of%2520the%2520art.%2520We%2520will%2520review%2520and%250Aanalyze%2520the%2520existing%2520methods%252C%2520compare%2520their%2520results%252C%2520and%2520highlight%250Ainconsistencies%2520in%2520the%2520current%2520literature.%2520This%2520thorough%2520analysis%2520aims%2520to%250Aprovide%2520general%2520insights%2520into%2520the%2520field%252C%2520propose%2520standardizations%252C%2520identify%2520new%250Aresearch%2520directions%252C%2520and%2520foster%2520its%2520proper%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Text%20Recognition%3A%20A%20Critical%20Survey&entry.906535625=Carlos%20Penarrubia%20and%20Jose%20J.%20Valero-Mas%20and%20Jorge%20Calvo-Zaragoza&entry.1292438233=%20%20Text%20Recognition%20%28TR%29%20refers%20to%20the%20research%20area%20that%20focuses%20on%20retrieving%0Atextual%20information%20from%20images%2C%20a%20topic%20that%20has%20seen%20significant%20advancements%0Ain%20the%20last%20decade%20due%20to%20the%20use%20of%20Deep%20Neural%20Networks%20%28DNN%29.%20However%2C%20these%0Asolutions%20often%20necessitate%20vast%20amounts%20of%20manually%20labeled%20or%20synthetic%20data.%0AAddressing%20this%20challenge%2C%20Self-Supervised%20Learning%20%28SSL%29%20has%20gained%20attention%0Aby%20utilizing%20large%20datasets%20of%20unlabeled%20data%20to%20train%20DNN%2C%20thereby%20generating%0Ameaningful%20and%20robust%20representations.%20Although%20SSL%20was%20initially%20overlooked%20in%0ATR%20because%20of%20its%20unique%20characteristics%2C%20recent%20years%20have%20witnessed%20a%20surge%0Ain%20the%20development%20of%20SSL%20methods%20specifically%20for%20this%20field.%20This%20rapid%0Adevelopment%2C%20however%2C%20has%20led%20to%20many%20methods%20being%20explored%20independently%2C%0Awithout%20taking%20previous%20efforts%20in%20methodology%20or%20comparison%20into%20account%2C%0Athereby%20hindering%20progress%20in%20the%20field%20of%20research.%20This%20paper%2C%20therefore%2C%0Aseeks%20to%20consolidate%20the%20use%20of%20SSL%20in%20the%20field%20of%20TR%2C%20offering%20a%20critical%20and%0Acomprehensive%20overview%20of%20the%20current%20state%20of%20the%20art.%20We%20will%20review%20and%0Aanalyze%20the%20existing%20methods%2C%20compare%20their%20results%2C%20and%20highlight%0Ainconsistencies%20in%20the%20current%20literature.%20This%20thorough%20analysis%20aims%20to%0Aprovide%20general%20insights%20into%20the%20field%2C%20propose%20standardizations%2C%20identify%20new%0Aresearch%20directions%2C%20and%20foster%20its%20proper%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19889v1&entry.124074799=Read"},
{"title": "Deep NURBS -- Admissible Physics-informed Neural Networks", "author": "Hamed Saidaoui and Luis Espath and R\u00e1ul Tempone", "abstract": "  In this study, we propose a new numerical scheme for physics-informed neural\nnetworks (PINNs) that enables precise and inexpensive solution for partial\ndifferential equations (PDEs) in case of arbitrary geometries while strictly\nenforcing Dirichlet boundary conditions. The proposed approach combines\nadmissible NURBS parametrizations required to define the physical domain and\nthe Dirichlet boundary conditions with a PINN solver. The fundamental boundary\nconditions are automatically satisfied in this novel Deep NURBS framework. We\nverified our new approach using two-dimensional elliptic PDEs when considering\narbitrary geometries, including non-Lipschitz domains. Compared to the\nclassical PINN solver, the Deep NURBS estimator has a remarkably high\nconvergence rate for all the studied problems. Moreover, a desirable accuracy\nwas realized for most of the studied PDEs using only one hidden layer of neural\nnetworks. This novel approach is considered to pave the way for more effective\nsolutions for high-dimensional problems by allowing for more realistic\nphysics-informed statistical learning to solve PDE-based variational problems.\n", "link": "http://arxiv.org/abs/2210.13900v2", "date": "2024-07-29", "relevancy": 1.9119, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.493}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4759}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20NURBS%20--%20Admissible%20Physics-informed%20Neural%20Networks&body=Title%3A%20Deep%20NURBS%20--%20Admissible%20Physics-informed%20Neural%20Networks%0AAuthor%3A%20Hamed%20Saidaoui%20and%20Luis%20Espath%20and%20R%C3%A1ul%20Tempone%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20propose%20a%20new%20numerical%20scheme%20for%20physics-informed%20neural%0Anetworks%20%28PINNs%29%20that%20enables%20precise%20and%20inexpensive%20solution%20for%20partial%0Adifferential%20equations%20%28PDEs%29%20in%20case%20of%20arbitrary%20geometries%20while%20strictly%0Aenforcing%20Dirichlet%20boundary%20conditions.%20The%20proposed%20approach%20combines%0Aadmissible%20NURBS%20parametrizations%20required%20to%20define%20the%20physical%20domain%20and%0Athe%20Dirichlet%20boundary%20conditions%20with%20a%20PINN%20solver.%20The%20fundamental%20boundary%0Aconditions%20are%20automatically%20satisfied%20in%20this%20novel%20Deep%20NURBS%20framework.%20We%0Averified%20our%20new%20approach%20using%20two-dimensional%20elliptic%20PDEs%20when%20considering%0Aarbitrary%20geometries%2C%20including%20non-Lipschitz%20domains.%20Compared%20to%20the%0Aclassical%20PINN%20solver%2C%20the%20Deep%20NURBS%20estimator%20has%20a%20remarkably%20high%0Aconvergence%20rate%20for%20all%20the%20studied%20problems.%20Moreover%2C%20a%20desirable%20accuracy%0Awas%20realized%20for%20most%20of%20the%20studied%20PDEs%20using%20only%20one%20hidden%20layer%20of%20neural%0Anetworks.%20This%20novel%20approach%20is%20considered%20to%20pave%20the%20way%20for%20more%20effective%0Asolutions%20for%20high-dimensional%20problems%20by%20allowing%20for%20more%20realistic%0Aphysics-informed%20statistical%20learning%20to%20solve%20PDE-based%20variational%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.13900v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520NURBS%2520--%2520Admissible%2520Physics-informed%2520Neural%2520Networks%26entry.906535625%3DHamed%2520Saidaoui%2520and%2520Luis%2520Espath%2520and%2520R%25C3%25A1ul%2520Tempone%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520new%2520numerical%2520scheme%2520for%2520physics-informed%2520neural%250Anetworks%2520%2528PINNs%2529%2520that%2520enables%2520precise%2520and%2520inexpensive%2520solution%2520for%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529%2520in%2520case%2520of%2520arbitrary%2520geometries%2520while%2520strictly%250Aenforcing%2520Dirichlet%2520boundary%2520conditions.%2520The%2520proposed%2520approach%2520combines%250Aadmissible%2520NURBS%2520parametrizations%2520required%2520to%2520define%2520the%2520physical%2520domain%2520and%250Athe%2520Dirichlet%2520boundary%2520conditions%2520with%2520a%2520PINN%2520solver.%2520The%2520fundamental%2520boundary%250Aconditions%2520are%2520automatically%2520satisfied%2520in%2520this%2520novel%2520Deep%2520NURBS%2520framework.%2520We%250Averified%2520our%2520new%2520approach%2520using%2520two-dimensional%2520elliptic%2520PDEs%2520when%2520considering%250Aarbitrary%2520geometries%252C%2520including%2520non-Lipschitz%2520domains.%2520Compared%2520to%2520the%250Aclassical%2520PINN%2520solver%252C%2520the%2520Deep%2520NURBS%2520estimator%2520has%2520a%2520remarkably%2520high%250Aconvergence%2520rate%2520for%2520all%2520the%2520studied%2520problems.%2520Moreover%252C%2520a%2520desirable%2520accuracy%250Awas%2520realized%2520for%2520most%2520of%2520the%2520studied%2520PDEs%2520using%2520only%2520one%2520hidden%2520layer%2520of%2520neural%250Anetworks.%2520This%2520novel%2520approach%2520is%2520considered%2520to%2520pave%2520the%2520way%2520for%2520more%2520effective%250Asolutions%2520for%2520high-dimensional%2520problems%2520by%2520allowing%2520for%2520more%2520realistic%250Aphysics-informed%2520statistical%2520learning%2520to%2520solve%2520PDE-based%2520variational%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.13900v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20NURBS%20--%20Admissible%20Physics-informed%20Neural%20Networks&entry.906535625=Hamed%20Saidaoui%20and%20Luis%20Espath%20and%20R%C3%A1ul%20Tempone&entry.1292438233=%20%20In%20this%20study%2C%20we%20propose%20a%20new%20numerical%20scheme%20for%20physics-informed%20neural%0Anetworks%20%28PINNs%29%20that%20enables%20precise%20and%20inexpensive%20solution%20for%20partial%0Adifferential%20equations%20%28PDEs%29%20in%20case%20of%20arbitrary%20geometries%20while%20strictly%0Aenforcing%20Dirichlet%20boundary%20conditions.%20The%20proposed%20approach%20combines%0Aadmissible%20NURBS%20parametrizations%20required%20to%20define%20the%20physical%20domain%20and%0Athe%20Dirichlet%20boundary%20conditions%20with%20a%20PINN%20solver.%20The%20fundamental%20boundary%0Aconditions%20are%20automatically%20satisfied%20in%20this%20novel%20Deep%20NURBS%20framework.%20We%0Averified%20our%20new%20approach%20using%20two-dimensional%20elliptic%20PDEs%20when%20considering%0Aarbitrary%20geometries%2C%20including%20non-Lipschitz%20domains.%20Compared%20to%20the%0Aclassical%20PINN%20solver%2C%20the%20Deep%20NURBS%20estimator%20has%20a%20remarkably%20high%0Aconvergence%20rate%20for%20all%20the%20studied%20problems.%20Moreover%2C%20a%20desirable%20accuracy%0Awas%20realized%20for%20most%20of%20the%20studied%20PDEs%20using%20only%20one%20hidden%20layer%20of%20neural%0Anetworks.%20This%20novel%20approach%20is%20considered%20to%20pave%20the%20way%20for%20more%20effective%0Asolutions%20for%20high-dimensional%20problems%20by%20allowing%20for%20more%20realistic%0Aphysics-informed%20statistical%20learning%20to%20solve%20PDE-based%20variational%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.13900v2&entry.124074799=Read"},
{"title": "Do LLMs Really Adapt to Domains? An Ontology Learning Perspective", "author": "Huu Tan Mai and Cuong Xuan Chu and Heiko Paulheim", "abstract": "  Large Language Models (LLMs) have demonstrated unprecedented prowess across\nvarious natural language processing tasks in various application domains.\nRecent studies show that LLMs can be leveraged to perform lexical semantic\ntasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).\nHowever, it has not effectively been verified whether their success is due to\ntheir ability to reason over unstructured or semi-structured data, or their\neffective learning of linguistic patterns and senses alone. This unresolved\nquestion is particularly crucial when dealing with domain-specific data, where\nthe lexical senses and their meaning can completely differ from what a LLM has\nlearned during its training stage. This paper investigates the following\nquestion: Do LLMs really adapt to domains and remain consistent in the\nextraction of structured knowledge, or do they only learn lexical senses\ninstead of reasoning? To answer this question and, we devise a controlled\nexperiment setup that uses WordNet to synthesize parallel corpora, with English\nand gibberish terms. We examine the differences in the outputs of LLMs for each\ncorpus in two OL tasks: relation extraction and taxonomy discovery. Empirical\nresults show that, while adapting to the gibberish corpora, off-the-shelf LLMs\ndo not consistently reason over semantic relationships between concepts, and\ninstead leverage senses and their frame. However, fine-tuning improves the\nperformance of LLMs on lexical semantic tasks even when the domain-specific\nterms are arbitrary and unseen during pre-training, hinting at the\napplicability of pre-trained LLMs for OL.\n", "link": "http://arxiv.org/abs/2407.19998v1", "date": "2024-07-29", "relevancy": 1.9058, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4967}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4636}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Really%20Adapt%20to%20Domains%3F%20An%20Ontology%20Learning%20Perspective&body=Title%3A%20Do%20LLMs%20Really%20Adapt%20to%20Domains%3F%20An%20Ontology%20Learning%20Perspective%0AAuthor%3A%20Huu%20Tan%20Mai%20and%20Cuong%20Xuan%20Chu%20and%20Heiko%20Paulheim%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20unprecedented%20prowess%20across%0Avarious%20natural%20language%20processing%20tasks%20in%20various%20application%20domains.%0ARecent%20studies%20show%20that%20LLMs%20can%20be%20leveraged%20to%20perform%20lexical%20semantic%0Atasks%2C%20such%20as%20Knowledge%20Base%20Completion%20%28KBC%29%20or%20Ontology%20Learning%20%28OL%29.%0AHowever%2C%20it%20has%20not%20effectively%20been%20verified%20whether%20their%20success%20is%20due%20to%0Atheir%20ability%20to%20reason%20over%20unstructured%20or%20semi-structured%20data%2C%20or%20their%0Aeffective%20learning%20of%20linguistic%20patterns%20and%20senses%20alone.%20This%20unresolved%0Aquestion%20is%20particularly%20crucial%20when%20dealing%20with%20domain-specific%20data%2C%20where%0Athe%20lexical%20senses%20and%20their%20meaning%20can%20completely%20differ%20from%20what%20a%20LLM%20has%0Alearned%20during%20its%20training%20stage.%20This%20paper%20investigates%20the%20following%0Aquestion%3A%20Do%20LLMs%20really%20adapt%20to%20domains%20and%20remain%20consistent%20in%20the%0Aextraction%20of%20structured%20knowledge%2C%20or%20do%20they%20only%20learn%20lexical%20senses%0Ainstead%20of%20reasoning%3F%20To%20answer%20this%20question%20and%2C%20we%20devise%20a%20controlled%0Aexperiment%20setup%20that%20uses%20WordNet%20to%20synthesize%20parallel%20corpora%2C%20with%20English%0Aand%20gibberish%20terms.%20We%20examine%20the%20differences%20in%20the%20outputs%20of%20LLMs%20for%20each%0Acorpus%20in%20two%20OL%20tasks%3A%20relation%20extraction%20and%20taxonomy%20discovery.%20Empirical%0Aresults%20show%20that%2C%20while%20adapting%20to%20the%20gibberish%20corpora%2C%20off-the-shelf%20LLMs%0Ado%20not%20consistently%20reason%20over%20semantic%20relationships%20between%20concepts%2C%20and%0Ainstead%20leverage%20senses%20and%20their%20frame.%20However%2C%20fine-tuning%20improves%20the%0Aperformance%20of%20LLMs%20on%20lexical%20semantic%20tasks%20even%20when%20the%20domain-specific%0Aterms%20are%20arbitrary%20and%20unseen%20during%20pre-training%2C%20hinting%20at%20the%0Aapplicability%20of%20pre-trained%20LLMs%20for%20OL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Really%2520Adapt%2520to%2520Domains%253F%2520An%2520Ontology%2520Learning%2520Perspective%26entry.906535625%3DHuu%2520Tan%2520Mai%2520and%2520Cuong%2520Xuan%2520Chu%2520and%2520Heiko%2520Paulheim%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520unprecedented%2520prowess%2520across%250Avarious%2520natural%2520language%2520processing%2520tasks%2520in%2520various%2520application%2520domains.%250ARecent%2520studies%2520show%2520that%2520LLMs%2520can%2520be%2520leveraged%2520to%2520perform%2520lexical%2520semantic%250Atasks%252C%2520such%2520as%2520Knowledge%2520Base%2520Completion%2520%2528KBC%2529%2520or%2520Ontology%2520Learning%2520%2528OL%2529.%250AHowever%252C%2520it%2520has%2520not%2520effectively%2520been%2520verified%2520whether%2520their%2520success%2520is%2520due%2520to%250Atheir%2520ability%2520to%2520reason%2520over%2520unstructured%2520or%2520semi-structured%2520data%252C%2520or%2520their%250Aeffective%2520learning%2520of%2520linguistic%2520patterns%2520and%2520senses%2520alone.%2520This%2520unresolved%250Aquestion%2520is%2520particularly%2520crucial%2520when%2520dealing%2520with%2520domain-specific%2520data%252C%2520where%250Athe%2520lexical%2520senses%2520and%2520their%2520meaning%2520can%2520completely%2520differ%2520from%2520what%2520a%2520LLM%2520has%250Alearned%2520during%2520its%2520training%2520stage.%2520This%2520paper%2520investigates%2520the%2520following%250Aquestion%253A%2520Do%2520LLMs%2520really%2520adapt%2520to%2520domains%2520and%2520remain%2520consistent%2520in%2520the%250Aextraction%2520of%2520structured%2520knowledge%252C%2520or%2520do%2520they%2520only%2520learn%2520lexical%2520senses%250Ainstead%2520of%2520reasoning%253F%2520To%2520answer%2520this%2520question%2520and%252C%2520we%2520devise%2520a%2520controlled%250Aexperiment%2520setup%2520that%2520uses%2520WordNet%2520to%2520synthesize%2520parallel%2520corpora%252C%2520with%2520English%250Aand%2520gibberish%2520terms.%2520We%2520examine%2520the%2520differences%2520in%2520the%2520outputs%2520of%2520LLMs%2520for%2520each%250Acorpus%2520in%2520two%2520OL%2520tasks%253A%2520relation%2520extraction%2520and%2520taxonomy%2520discovery.%2520Empirical%250Aresults%2520show%2520that%252C%2520while%2520adapting%2520to%2520the%2520gibberish%2520corpora%252C%2520off-the-shelf%2520LLMs%250Ado%2520not%2520consistently%2520reason%2520over%2520semantic%2520relationships%2520between%2520concepts%252C%2520and%250Ainstead%2520leverage%2520senses%2520and%2520their%2520frame.%2520However%252C%2520fine-tuning%2520improves%2520the%250Aperformance%2520of%2520LLMs%2520on%2520lexical%2520semantic%2520tasks%2520even%2520when%2520the%2520domain-specific%250Aterms%2520are%2520arbitrary%2520and%2520unseen%2520during%2520pre-training%252C%2520hinting%2520at%2520the%250Aapplicability%2520of%2520pre-trained%2520LLMs%2520for%2520OL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Really%20Adapt%20to%20Domains%3F%20An%20Ontology%20Learning%20Perspective&entry.906535625=Huu%20Tan%20Mai%20and%20Cuong%20Xuan%20Chu%20and%20Heiko%20Paulheim&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20unprecedented%20prowess%20across%0Avarious%20natural%20language%20processing%20tasks%20in%20various%20application%20domains.%0ARecent%20studies%20show%20that%20LLMs%20can%20be%20leveraged%20to%20perform%20lexical%20semantic%0Atasks%2C%20such%20as%20Knowledge%20Base%20Completion%20%28KBC%29%20or%20Ontology%20Learning%20%28OL%29.%0AHowever%2C%20it%20has%20not%20effectively%20been%20verified%20whether%20their%20success%20is%20due%20to%0Atheir%20ability%20to%20reason%20over%20unstructured%20or%20semi-structured%20data%2C%20or%20their%0Aeffective%20learning%20of%20linguistic%20patterns%20and%20senses%20alone.%20This%20unresolved%0Aquestion%20is%20particularly%20crucial%20when%20dealing%20with%20domain-specific%20data%2C%20where%0Athe%20lexical%20senses%20and%20their%20meaning%20can%20completely%20differ%20from%20what%20a%20LLM%20has%0Alearned%20during%20its%20training%20stage.%20This%20paper%20investigates%20the%20following%0Aquestion%3A%20Do%20LLMs%20really%20adapt%20to%20domains%20and%20remain%20consistent%20in%20the%0Aextraction%20of%20structured%20knowledge%2C%20or%20do%20they%20only%20learn%20lexical%20senses%0Ainstead%20of%20reasoning%3F%20To%20answer%20this%20question%20and%2C%20we%20devise%20a%20controlled%0Aexperiment%20setup%20that%20uses%20WordNet%20to%20synthesize%20parallel%20corpora%2C%20with%20English%0Aand%20gibberish%20terms.%20We%20examine%20the%20differences%20in%20the%20outputs%20of%20LLMs%20for%20each%0Acorpus%20in%20two%20OL%20tasks%3A%20relation%20extraction%20and%20taxonomy%20discovery.%20Empirical%0Aresults%20show%20that%2C%20while%20adapting%20to%20the%20gibberish%20corpora%2C%20off-the-shelf%20LLMs%0Ado%20not%20consistently%20reason%20over%20semantic%20relationships%20between%20concepts%2C%20and%0Ainstead%20leverage%20senses%20and%20their%20frame.%20However%2C%20fine-tuning%20improves%20the%0Aperformance%20of%20LLMs%20on%20lexical%20semantic%20tasks%20even%20when%20the%20domain-specific%0Aterms%20are%20arbitrary%20and%20unseen%20during%20pre-training%2C%20hinting%20at%20the%0Aapplicability%20of%20pre-trained%20LLMs%20for%20OL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19998v1&entry.124074799=Read"},
{"title": "AI-Powered Energy algorithmic Trading: Integrating Hidden Markov Models\n  with Neural Networks", "author": "Tiago Monteiro", "abstract": "  In the field of quantitative finance, machine learning methods have become\nessential for alpha generation. This paper presents a pioneering method that\nuniquely combines Hidden Markov Models (HMM) and neural networks, creating a\ndual-model alpha generation system integrated with Black-Litterman portfolio\noptimization. The methodology, implemented on the QuantConnect platform, aims\nto predict future price movements and optimize trading strategies.\nSpecifically, it filters for highly liquid, top-cap energy stocks to ensure\nstable and predictable performance while also accounting for broker payments.\nQuantConnect was selected because of its robust framework and to guarantee\nexperimental reproducibility. The algorithm achieved a 31% return between June\n1, 2023, and January 1, 2024, with a Sharpe ratio of 1.669, demonstrating its\npotential. The findings suggest significant improvements in trading strategy\nperformance through the combined use of the HMM and neural networks. This study\nexplores the architecture of the algorithm, data pre-processing techniques,\nmodel training procedures, and performance evaluation, highlighting its\npractical applicability and effectiveness in real-world trading environments.\nThe full code and backtesting data are available under the MIT license.\n", "link": "http://arxiv.org/abs/2407.19858v1", "date": "2024-07-29", "relevancy": 1.9051, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5241}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4588}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Powered%20Energy%20algorithmic%20Trading%3A%20Integrating%20Hidden%20Markov%20Models%0A%20%20with%20Neural%20Networks&body=Title%3A%20AI-Powered%20Energy%20algorithmic%20Trading%3A%20Integrating%20Hidden%20Markov%20Models%0A%20%20with%20Neural%20Networks%0AAuthor%3A%20Tiago%20Monteiro%0AAbstract%3A%20%20%20In%20the%20field%20of%20quantitative%20finance%2C%20machine%20learning%20methods%20have%20become%0Aessential%20for%20alpha%20generation.%20This%20paper%20presents%20a%20pioneering%20method%20that%0Auniquely%20combines%20Hidden%20Markov%20Models%20%28HMM%29%20and%20neural%20networks%2C%20creating%20a%0Adual-model%20alpha%20generation%20system%20integrated%20with%20Black-Litterman%20portfolio%0Aoptimization.%20The%20methodology%2C%20implemented%20on%20the%20QuantConnect%20platform%2C%20aims%0Ato%20predict%20future%20price%20movements%20and%20optimize%20trading%20strategies.%0ASpecifically%2C%20it%20filters%20for%20highly%20liquid%2C%20top-cap%20energy%20stocks%20to%20ensure%0Astable%20and%20predictable%20performance%20while%20also%20accounting%20for%20broker%20payments.%0AQuantConnect%20was%20selected%20because%20of%20its%20robust%20framework%20and%20to%20guarantee%0Aexperimental%20reproducibility.%20The%20algorithm%20achieved%20a%2031%25%20return%20between%20June%0A1%2C%202023%2C%20and%20January%201%2C%202024%2C%20with%20a%20Sharpe%20ratio%20of%201.669%2C%20demonstrating%20its%0Apotential.%20The%20findings%20suggest%20significant%20improvements%20in%20trading%20strategy%0Aperformance%20through%20the%20combined%20use%20of%20the%20HMM%20and%20neural%20networks.%20This%20study%0Aexplores%20the%20architecture%20of%20the%20algorithm%2C%20data%20pre-processing%20techniques%2C%0Amodel%20training%20procedures%2C%20and%20performance%20evaluation%2C%20highlighting%20its%0Apractical%20applicability%20and%20effectiveness%20in%20real-world%20trading%20environments.%0AThe%20full%20code%20and%20backtesting%20data%20are%20available%20under%20the%20MIT%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Powered%2520Energy%2520algorithmic%2520Trading%253A%2520Integrating%2520Hidden%2520Markov%2520Models%250A%2520%2520with%2520Neural%2520Networks%26entry.906535625%3DTiago%2520Monteiro%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520quantitative%2520finance%252C%2520machine%2520learning%2520methods%2520have%2520become%250Aessential%2520for%2520alpha%2520generation.%2520This%2520paper%2520presents%2520a%2520pioneering%2520method%2520that%250Auniquely%2520combines%2520Hidden%2520Markov%2520Models%2520%2528HMM%2529%2520and%2520neural%2520networks%252C%2520creating%2520a%250Adual-model%2520alpha%2520generation%2520system%2520integrated%2520with%2520Black-Litterman%2520portfolio%250Aoptimization.%2520The%2520methodology%252C%2520implemented%2520on%2520the%2520QuantConnect%2520platform%252C%2520aims%250Ato%2520predict%2520future%2520price%2520movements%2520and%2520optimize%2520trading%2520strategies.%250ASpecifically%252C%2520it%2520filters%2520for%2520highly%2520liquid%252C%2520top-cap%2520energy%2520stocks%2520to%2520ensure%250Astable%2520and%2520predictable%2520performance%2520while%2520also%2520accounting%2520for%2520broker%2520payments.%250AQuantConnect%2520was%2520selected%2520because%2520of%2520its%2520robust%2520framework%2520and%2520to%2520guarantee%250Aexperimental%2520reproducibility.%2520The%2520algorithm%2520achieved%2520a%252031%2525%2520return%2520between%2520June%250A1%252C%25202023%252C%2520and%2520January%25201%252C%25202024%252C%2520with%2520a%2520Sharpe%2520ratio%2520of%25201.669%252C%2520demonstrating%2520its%250Apotential.%2520The%2520findings%2520suggest%2520significant%2520improvements%2520in%2520trading%2520strategy%250Aperformance%2520through%2520the%2520combined%2520use%2520of%2520the%2520HMM%2520and%2520neural%2520networks.%2520This%2520study%250Aexplores%2520the%2520architecture%2520of%2520the%2520algorithm%252C%2520data%2520pre-processing%2520techniques%252C%250Amodel%2520training%2520procedures%252C%2520and%2520performance%2520evaluation%252C%2520highlighting%2520its%250Apractical%2520applicability%2520and%2520effectiveness%2520in%2520real-world%2520trading%2520environments.%250AThe%2520full%2520code%2520and%2520backtesting%2520data%2520are%2520available%2520under%2520the%2520MIT%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Powered%20Energy%20algorithmic%20Trading%3A%20Integrating%20Hidden%20Markov%20Models%0A%20%20with%20Neural%20Networks&entry.906535625=Tiago%20Monteiro&entry.1292438233=%20%20In%20the%20field%20of%20quantitative%20finance%2C%20machine%20learning%20methods%20have%20become%0Aessential%20for%20alpha%20generation.%20This%20paper%20presents%20a%20pioneering%20method%20that%0Auniquely%20combines%20Hidden%20Markov%20Models%20%28HMM%29%20and%20neural%20networks%2C%20creating%20a%0Adual-model%20alpha%20generation%20system%20integrated%20with%20Black-Litterman%20portfolio%0Aoptimization.%20The%20methodology%2C%20implemented%20on%20the%20QuantConnect%20platform%2C%20aims%0Ato%20predict%20future%20price%20movements%20and%20optimize%20trading%20strategies.%0ASpecifically%2C%20it%20filters%20for%20highly%20liquid%2C%20top-cap%20energy%20stocks%20to%20ensure%0Astable%20and%20predictable%20performance%20while%20also%20accounting%20for%20broker%20payments.%0AQuantConnect%20was%20selected%20because%20of%20its%20robust%20framework%20and%20to%20guarantee%0Aexperimental%20reproducibility.%20The%20algorithm%20achieved%20a%2031%25%20return%20between%20June%0A1%2C%202023%2C%20and%20January%201%2C%202024%2C%20with%20a%20Sharpe%20ratio%20of%201.669%2C%20demonstrating%20its%0Apotential.%20The%20findings%20suggest%20significant%20improvements%20in%20trading%20strategy%0Aperformance%20through%20the%20combined%20use%20of%20the%20HMM%20and%20neural%20networks.%20This%20study%0Aexplores%20the%20architecture%20of%20the%20algorithm%2C%20data%20pre-processing%20techniques%2C%0Amodel%20training%20procedures%2C%20and%20performance%20evaluation%2C%20highlighting%20its%0Apractical%20applicability%20and%20effectiveness%20in%20real-world%20trading%20environments.%0AThe%20full%20code%20and%20backtesting%20data%20are%20available%20under%20the%20MIT%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19858v1&entry.124074799=Read"},
{"title": "Response Theory via Generative Score Modeling", "author": "Ludovico Theo Giorgini and Katherine Deck and Tobias Bischoff and Andre Souza", "abstract": "  We introduce an approach for analyzing the responses of dynamical systems to\nexternal perturbations that combines score-based generative modeling with the\nGeneralized Fluctuation-Dissipation Theorem (GFDT). The methodology enables\naccurate estimation of system responses, including those with non-Gaussian\nstatistics. We numerically validate our approach using time-series data from\nthree different stochastic partial differential equations of increasing\ncomplexity: an Ornstein-Uhlenbeck process with spatially correlated noise, a\nmodified stochastic Allen-Cahn equation, and the 2D Navier-Stokes equations. We\ndemonstrate the improved accuracy of the methodology over conventional methods\nand discuss its potential as a versatile tool for predicting the statistical\nbehavior of complex dynamical systems.\n", "link": "http://arxiv.org/abs/2402.01029v2", "date": "2024-07-29", "relevancy": 1.9045, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5226}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.469}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Response%20Theory%20via%20Generative%20Score%20Modeling&body=Title%3A%20Response%20Theory%20via%20Generative%20Score%20Modeling%0AAuthor%3A%20Ludovico%20Theo%20Giorgini%20and%20Katherine%20Deck%20and%20Tobias%20Bischoff%20and%20Andre%20Souza%0AAbstract%3A%20%20%20We%20introduce%20an%20approach%20for%20analyzing%20the%20responses%20of%20dynamical%20systems%20to%0Aexternal%20perturbations%20that%20combines%20score-based%20generative%20modeling%20with%20the%0AGeneralized%20Fluctuation-Dissipation%20Theorem%20%28GFDT%29.%20The%20methodology%20enables%0Aaccurate%20estimation%20of%20system%20responses%2C%20including%20those%20with%20non-Gaussian%0Astatistics.%20We%20numerically%20validate%20our%20approach%20using%20time-series%20data%20from%0Athree%20different%20stochastic%20partial%20differential%20equations%20of%20increasing%0Acomplexity%3A%20an%20Ornstein-Uhlenbeck%20process%20with%20spatially%20correlated%20noise%2C%20a%0Amodified%20stochastic%20Allen-Cahn%20equation%2C%20and%20the%202D%20Navier-Stokes%20equations.%20We%0Ademonstrate%20the%20improved%20accuracy%20of%20the%20methodology%20over%20conventional%20methods%0Aand%20discuss%20its%20potential%20as%20a%20versatile%20tool%20for%20predicting%20the%20statistical%0Abehavior%20of%20complex%20dynamical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01029v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResponse%2520Theory%2520via%2520Generative%2520Score%2520Modeling%26entry.906535625%3DLudovico%2520Theo%2520Giorgini%2520and%2520Katherine%2520Deck%2520and%2520Tobias%2520Bischoff%2520and%2520Andre%2520Souza%26entry.1292438233%3D%2520%2520We%2520introduce%2520an%2520approach%2520for%2520analyzing%2520the%2520responses%2520of%2520dynamical%2520systems%2520to%250Aexternal%2520perturbations%2520that%2520combines%2520score-based%2520generative%2520modeling%2520with%2520the%250AGeneralized%2520Fluctuation-Dissipation%2520Theorem%2520%2528GFDT%2529.%2520The%2520methodology%2520enables%250Aaccurate%2520estimation%2520of%2520system%2520responses%252C%2520including%2520those%2520with%2520non-Gaussian%250Astatistics.%2520We%2520numerically%2520validate%2520our%2520approach%2520using%2520time-series%2520data%2520from%250Athree%2520different%2520stochastic%2520partial%2520differential%2520equations%2520of%2520increasing%250Acomplexity%253A%2520an%2520Ornstein-Uhlenbeck%2520process%2520with%2520spatially%2520correlated%2520noise%252C%2520a%250Amodified%2520stochastic%2520Allen-Cahn%2520equation%252C%2520and%2520the%25202D%2520Navier-Stokes%2520equations.%2520We%250Ademonstrate%2520the%2520improved%2520accuracy%2520of%2520the%2520methodology%2520over%2520conventional%2520methods%250Aand%2520discuss%2520its%2520potential%2520as%2520a%2520versatile%2520tool%2520for%2520predicting%2520the%2520statistical%250Abehavior%2520of%2520complex%2520dynamical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01029v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Response%20Theory%20via%20Generative%20Score%20Modeling&entry.906535625=Ludovico%20Theo%20Giorgini%20and%20Katherine%20Deck%20and%20Tobias%20Bischoff%20and%20Andre%20Souza&entry.1292438233=%20%20We%20introduce%20an%20approach%20for%20analyzing%20the%20responses%20of%20dynamical%20systems%20to%0Aexternal%20perturbations%20that%20combines%20score-based%20generative%20modeling%20with%20the%0AGeneralized%20Fluctuation-Dissipation%20Theorem%20%28GFDT%29.%20The%20methodology%20enables%0Aaccurate%20estimation%20of%20system%20responses%2C%20including%20those%20with%20non-Gaussian%0Astatistics.%20We%20numerically%20validate%20our%20approach%20using%20time-series%20data%20from%0Athree%20different%20stochastic%20partial%20differential%20equations%20of%20increasing%0Acomplexity%3A%20an%20Ornstein-Uhlenbeck%20process%20with%20spatially%20correlated%20noise%2C%20a%0Amodified%20stochastic%20Allen-Cahn%20equation%2C%20and%20the%202D%20Navier-Stokes%20equations.%20We%0Ademonstrate%20the%20improved%20accuracy%20of%20the%20methodology%20over%20conventional%20methods%0Aand%20discuss%20its%20potential%20as%20a%20versatile%20tool%20for%20predicting%20the%20statistical%0Abehavior%20of%20complex%20dynamical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01029v2&entry.124074799=Read"},
{"title": "Identifiable latent bandits: Combining observational data and\n  exploration for personalized healthcare", "author": "Ahmet Zahid Balc\u0131o\u011flu and Emil Carlsson and Fredrik D. Johansson", "abstract": "  Bandit algorithms hold great promise for improving personalized\ndecision-making but are notoriously sample-hungry. In most health applications,\nit is infeasible to fit a new bandit for each patient, and observable variables\nare often insufficient to determine optimal treatments, ruling out applying\ncontextual bandits learned from multiple patients. Latent bandits offer both\nrapid exploration and personalization beyond what context variables can reveal\nbut require that a latent variable model can be learned consistently. In this\nwork, we propose bandit algorithms based on nonlinear independent component\nanalysis that can be provably identified from observational data to a degree\nsufficient to infer the optimal action in a new bandit instance consistently.\nWe verify this strategy in simulated data, showing substantial improvement over\nlearning independent multi-armed bandits for every instance.\n", "link": "http://arxiv.org/abs/2407.16239v2", "date": "2024-07-29", "relevancy": 1.8965, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4719}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifiable%20latent%20bandits%3A%20Combining%20observational%20data%20and%0A%20%20exploration%20for%20personalized%20healthcare&body=Title%3A%20Identifiable%20latent%20bandits%3A%20Combining%20observational%20data%20and%0A%20%20exploration%20for%20personalized%20healthcare%0AAuthor%3A%20Ahmet%20Zahid%20Balc%C4%B1o%C4%9Flu%20and%20Emil%20Carlsson%20and%20Fredrik%20D.%20Johansson%0AAbstract%3A%20%20%20Bandit%20algorithms%20hold%20great%20promise%20for%20improving%20personalized%0Adecision-making%20but%20are%20notoriously%20sample-hungry.%20In%20most%20health%20applications%2C%0Ait%20is%20infeasible%20to%20fit%20a%20new%20bandit%20for%20each%20patient%2C%20and%20observable%20variables%0Aare%20often%20insufficient%20to%20determine%20optimal%20treatments%2C%20ruling%20out%20applying%0Acontextual%20bandits%20learned%20from%20multiple%20patients.%20Latent%20bandits%20offer%20both%0Arapid%20exploration%20and%20personalization%20beyond%20what%20context%20variables%20can%20reveal%0Abut%20require%20that%20a%20latent%20variable%20model%20can%20be%20learned%20consistently.%20In%20this%0Awork%2C%20we%20propose%20bandit%20algorithms%20based%20on%20nonlinear%20independent%20component%0Aanalysis%20that%20can%20be%20provably%20identified%20from%20observational%20data%20to%20a%20degree%0Asufficient%20to%20infer%20the%20optimal%20action%20in%20a%20new%20bandit%20instance%20consistently.%0AWe%20verify%20this%20strategy%20in%20simulated%20data%2C%20showing%20substantial%20improvement%20over%0Alearning%20independent%20multi-armed%20bandits%20for%20every%20instance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifiable%2520latent%2520bandits%253A%2520Combining%2520observational%2520data%2520and%250A%2520%2520exploration%2520for%2520personalized%2520healthcare%26entry.906535625%3DAhmet%2520Zahid%2520Balc%25C4%25B1o%25C4%259Flu%2520and%2520Emil%2520Carlsson%2520and%2520Fredrik%2520D.%2520Johansson%26entry.1292438233%3D%2520%2520Bandit%2520algorithms%2520hold%2520great%2520promise%2520for%2520improving%2520personalized%250Adecision-making%2520but%2520are%2520notoriously%2520sample-hungry.%2520In%2520most%2520health%2520applications%252C%250Ait%2520is%2520infeasible%2520to%2520fit%2520a%2520new%2520bandit%2520for%2520each%2520patient%252C%2520and%2520observable%2520variables%250Aare%2520often%2520insufficient%2520to%2520determine%2520optimal%2520treatments%252C%2520ruling%2520out%2520applying%250Acontextual%2520bandits%2520learned%2520from%2520multiple%2520patients.%2520Latent%2520bandits%2520offer%2520both%250Arapid%2520exploration%2520and%2520personalization%2520beyond%2520what%2520context%2520variables%2520can%2520reveal%250Abut%2520require%2520that%2520a%2520latent%2520variable%2520model%2520can%2520be%2520learned%2520consistently.%2520In%2520this%250Awork%252C%2520we%2520propose%2520bandit%2520algorithms%2520based%2520on%2520nonlinear%2520independent%2520component%250Aanalysis%2520that%2520can%2520be%2520provably%2520identified%2520from%2520observational%2520data%2520to%2520a%2520degree%250Asufficient%2520to%2520infer%2520the%2520optimal%2520action%2520in%2520a%2520new%2520bandit%2520instance%2520consistently.%250AWe%2520verify%2520this%2520strategy%2520in%2520simulated%2520data%252C%2520showing%2520substantial%2520improvement%2520over%250Alearning%2520independent%2520multi-armed%2520bandits%2520for%2520every%2520instance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifiable%20latent%20bandits%3A%20Combining%20observational%20data%20and%0A%20%20exploration%20for%20personalized%20healthcare&entry.906535625=Ahmet%20Zahid%20Balc%C4%B1o%C4%9Flu%20and%20Emil%20Carlsson%20and%20Fredrik%20D.%20Johansson&entry.1292438233=%20%20Bandit%20algorithms%20hold%20great%20promise%20for%20improving%20personalized%0Adecision-making%20but%20are%20notoriously%20sample-hungry.%20In%20most%20health%20applications%2C%0Ait%20is%20infeasible%20to%20fit%20a%20new%20bandit%20for%20each%20patient%2C%20and%20observable%20variables%0Aare%20often%20insufficient%20to%20determine%20optimal%20treatments%2C%20ruling%20out%20applying%0Acontextual%20bandits%20learned%20from%20multiple%20patients.%20Latent%20bandits%20offer%20both%0Arapid%20exploration%20and%20personalization%20beyond%20what%20context%20variables%20can%20reveal%0Abut%20require%20that%20a%20latent%20variable%20model%20can%20be%20learned%20consistently.%20In%20this%0Awork%2C%20we%20propose%20bandit%20algorithms%20based%20on%20nonlinear%20independent%20component%0Aanalysis%20that%20can%20be%20provably%20identified%20from%20observational%20data%20to%20a%20degree%0Asufficient%20to%20infer%20the%20optimal%20action%20in%20a%20new%20bandit%20instance%20consistently.%0AWe%20verify%20this%20strategy%20in%20simulated%20data%2C%20showing%20substantial%20improvement%20over%0Alearning%20independent%20multi-armed%20bandits%20for%20every%20instance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16239v2&entry.124074799=Read"},
{"title": "Orca: Ocean Significant Wave Height Estimation with Spatio-temporally\n  Aware Large Language Models", "author": "Zhe Li and Ronghui Xu and Jilin Hu and Zhong Peng and Xi Lu and Chenjuan Guo and Bin Yang", "abstract": "  Significant wave height (SWH) is a vital metric in marine science, and\naccurate SWH estimation is crucial for various applications, e.g., marine\nenergy development, fishery, early warning systems for potential risks, etc.\nTraditional SWH estimation methods that are based on numerical models and\nphysical theories are hindered by computational inefficiencies. Recently,\nmachine learning has emerged as an appealing alternative to improve accuracy\nand reduce computational time. However, due to limited observational technology\nand high costs, the scarcity of real-world data restricts the potential of\nmachine learning models. To overcome these limitations, we propose an ocean SWH\nestimation framework, namely Orca. Specifically, Orca enhances the limited\nspatio-temporal reasoning abilities of classic LLMs with a novel spatiotemporal\naware encoding module. By segmenting the limited buoy observational data\ntemporally, encoding the buoys' locations spatially, and designing prompt\ntemplates, Orca capitalizes on the robust generalization ability of LLMs to\nestimate significant wave height effectively with limited data. Experimental\nresults on the Gulf of Mexico demonstrate that Orca achieves state-of-the-art\nperformance in SWH estimation.\n", "link": "http://arxiv.org/abs/2407.20053v1", "date": "2024-07-29", "relevancy": 1.8894, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4693}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orca%3A%20Ocean%20Significant%20Wave%20Height%20Estimation%20with%20Spatio-temporally%0A%20%20Aware%20Large%20Language%20Models&body=Title%3A%20Orca%3A%20Ocean%20Significant%20Wave%20Height%20Estimation%20with%20Spatio-temporally%0A%20%20Aware%20Large%20Language%20Models%0AAuthor%3A%20Zhe%20Li%20and%20Ronghui%20Xu%20and%20Jilin%20Hu%20and%20Zhong%20Peng%20and%20Xi%20Lu%20and%20Chenjuan%20Guo%20and%20Bin%20Yang%0AAbstract%3A%20%20%20Significant%20wave%20height%20%28SWH%29%20is%20a%20vital%20metric%20in%20marine%20science%2C%20and%0Aaccurate%20SWH%20estimation%20is%20crucial%20for%20various%20applications%2C%20e.g.%2C%20marine%0Aenergy%20development%2C%20fishery%2C%20early%20warning%20systems%20for%20potential%20risks%2C%20etc.%0ATraditional%20SWH%20estimation%20methods%20that%20are%20based%20on%20numerical%20models%20and%0Aphysical%20theories%20are%20hindered%20by%20computational%20inefficiencies.%20Recently%2C%0Amachine%20learning%20has%20emerged%20as%20an%20appealing%20alternative%20to%20improve%20accuracy%0Aand%20reduce%20computational%20time.%20However%2C%20due%20to%20limited%20observational%20technology%0Aand%20high%20costs%2C%20the%20scarcity%20of%20real-world%20data%20restricts%20the%20potential%20of%0Amachine%20learning%20models.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%20ocean%20SWH%0Aestimation%20framework%2C%20namely%20Orca.%20Specifically%2C%20Orca%20enhances%20the%20limited%0Aspatio-temporal%20reasoning%20abilities%20of%20classic%20LLMs%20with%20a%20novel%20spatiotemporal%0Aaware%20encoding%20module.%20By%20segmenting%20the%20limited%20buoy%20observational%20data%0Atemporally%2C%20encoding%20the%20buoys%27%20locations%20spatially%2C%20and%20designing%20prompt%0Atemplates%2C%20Orca%20capitalizes%20on%20the%20robust%20generalization%20ability%20of%20LLMs%20to%0Aestimate%20significant%20wave%20height%20effectively%20with%20limited%20data.%20Experimental%0Aresults%20on%20the%20Gulf%20of%20Mexico%20demonstrate%20that%20Orca%20achieves%20state-of-the-art%0Aperformance%20in%20SWH%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrca%253A%2520Ocean%2520Significant%2520Wave%2520Height%2520Estimation%2520with%2520Spatio-temporally%250A%2520%2520Aware%2520Large%2520Language%2520Models%26entry.906535625%3DZhe%2520Li%2520and%2520Ronghui%2520Xu%2520and%2520Jilin%2520Hu%2520and%2520Zhong%2520Peng%2520and%2520Xi%2520Lu%2520and%2520Chenjuan%2520Guo%2520and%2520Bin%2520Yang%26entry.1292438233%3D%2520%2520Significant%2520wave%2520height%2520%2528SWH%2529%2520is%2520a%2520vital%2520metric%2520in%2520marine%2520science%252C%2520and%250Aaccurate%2520SWH%2520estimation%2520is%2520crucial%2520for%2520various%2520applications%252C%2520e.g.%252C%2520marine%250Aenergy%2520development%252C%2520fishery%252C%2520early%2520warning%2520systems%2520for%2520potential%2520risks%252C%2520etc.%250ATraditional%2520SWH%2520estimation%2520methods%2520that%2520are%2520based%2520on%2520numerical%2520models%2520and%250Aphysical%2520theories%2520are%2520hindered%2520by%2520computational%2520inefficiencies.%2520Recently%252C%250Amachine%2520learning%2520has%2520emerged%2520as%2520an%2520appealing%2520alternative%2520to%2520improve%2520accuracy%250Aand%2520reduce%2520computational%2520time.%2520However%252C%2520due%2520to%2520limited%2520observational%2520technology%250Aand%2520high%2520costs%252C%2520the%2520scarcity%2520of%2520real-world%2520data%2520restricts%2520the%2520potential%2520of%250Amachine%2520learning%2520models.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520an%2520ocean%2520SWH%250Aestimation%2520framework%252C%2520namely%2520Orca.%2520Specifically%252C%2520Orca%2520enhances%2520the%2520limited%250Aspatio-temporal%2520reasoning%2520abilities%2520of%2520classic%2520LLMs%2520with%2520a%2520novel%2520spatiotemporal%250Aaware%2520encoding%2520module.%2520By%2520segmenting%2520the%2520limited%2520buoy%2520observational%2520data%250Atemporally%252C%2520encoding%2520the%2520buoys%2527%2520locations%2520spatially%252C%2520and%2520designing%2520prompt%250Atemplates%252C%2520Orca%2520capitalizes%2520on%2520the%2520robust%2520generalization%2520ability%2520of%2520LLMs%2520to%250Aestimate%2520significant%2520wave%2520height%2520effectively%2520with%2520limited%2520data.%2520Experimental%250Aresults%2520on%2520the%2520Gulf%2520of%2520Mexico%2520demonstrate%2520that%2520Orca%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520SWH%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orca%3A%20Ocean%20Significant%20Wave%20Height%20Estimation%20with%20Spatio-temporally%0A%20%20Aware%20Large%20Language%20Models&entry.906535625=Zhe%20Li%20and%20Ronghui%20Xu%20and%20Jilin%20Hu%20and%20Zhong%20Peng%20and%20Xi%20Lu%20and%20Chenjuan%20Guo%20and%20Bin%20Yang&entry.1292438233=%20%20Significant%20wave%20height%20%28SWH%29%20is%20a%20vital%20metric%20in%20marine%20science%2C%20and%0Aaccurate%20SWH%20estimation%20is%20crucial%20for%20various%20applications%2C%20e.g.%2C%20marine%0Aenergy%20development%2C%20fishery%2C%20early%20warning%20systems%20for%20potential%20risks%2C%20etc.%0ATraditional%20SWH%20estimation%20methods%20that%20are%20based%20on%20numerical%20models%20and%0Aphysical%20theories%20are%20hindered%20by%20computational%20inefficiencies.%20Recently%2C%0Amachine%20learning%20has%20emerged%20as%20an%20appealing%20alternative%20to%20improve%20accuracy%0Aand%20reduce%20computational%20time.%20However%2C%20due%20to%20limited%20observational%20technology%0Aand%20high%20costs%2C%20the%20scarcity%20of%20real-world%20data%20restricts%20the%20potential%20of%0Amachine%20learning%20models.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%20ocean%20SWH%0Aestimation%20framework%2C%20namely%20Orca.%20Specifically%2C%20Orca%20enhances%20the%20limited%0Aspatio-temporal%20reasoning%20abilities%20of%20classic%20LLMs%20with%20a%20novel%20spatiotemporal%0Aaware%20encoding%20module.%20By%20segmenting%20the%20limited%20buoy%20observational%20data%0Atemporally%2C%20encoding%20the%20buoys%27%20locations%20spatially%2C%20and%20designing%20prompt%0Atemplates%2C%20Orca%20capitalizes%20on%20the%20robust%20generalization%20ability%20of%20LLMs%20to%0Aestimate%20significant%20wave%20height%20effectively%20with%20limited%20data.%20Experimental%0Aresults%20on%20the%20Gulf%20of%20Mexico%20demonstrate%20that%20Orca%20achieves%20state-of-the-art%0Aperformance%20in%20SWH%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20053v1&entry.124074799=Read"},
{"title": "A Study on the Implementation Method of an Agent-Based Advanced RAG\n  System Using Graph", "author": "Cheonsu Jeong", "abstract": "  This study aims to improve knowledge-based question-answering (QA) systems by\novercoming the limitations of existing Retrieval-Augmented Generation (RAG)\nmodels and implementing an advanced RAG system based on Graph technology to\ndevelop high-quality generative AI services. While existing RAG models\ndemonstrate high accuracy and fluency by utilizing retrieved information, they\nmay suffer from accuracy degradation as they generate responses using\npre-loaded knowledge without reprocessing. Additionally, they cannot\nincorporate real-time data after the RAG configuration stage, leading to issues\nwith contextual understanding and biased information. To address these\nlimitations, this study implemented an enhanced RAG system utilizing Graph\ntechnology. This system is designed to efficiently search and utilize\ninformation. Specifically, it employs LangGraph to evaluate the reliability of\nretrieved information and synthesizes diverse data to generate more accurate\nand enhanced responses. Furthermore, the study provides a detailed explanation\nof the system's operation, key implementation steps, and examples through\nimplementation code and validation results, thereby enhancing the understanding\nof advanced RAG technology. This approach offers practical guidelines for\nimplementing advanced RAG systems in corporate services, making it a valuable\nresource for practical application.\n", "link": "http://arxiv.org/abs/2407.19994v1", "date": "2024-07-29", "relevancy": 1.8874, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.544}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4625}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20on%20the%20Implementation%20Method%20of%20an%20Agent-Based%20Advanced%20RAG%0A%20%20System%20Using%20Graph&body=Title%3A%20A%20Study%20on%20the%20Implementation%20Method%20of%20an%20Agent-Based%20Advanced%20RAG%0A%20%20System%20Using%20Graph%0AAuthor%3A%20Cheonsu%20Jeong%0AAbstract%3A%20%20%20This%20study%20aims%20to%20improve%20knowledge-based%20question-answering%20%28QA%29%20systems%20by%0Aovercoming%20the%20limitations%20of%20existing%20Retrieval-Augmented%20Generation%20%28RAG%29%0Amodels%20and%20implementing%20an%20advanced%20RAG%20system%20based%20on%20Graph%20technology%20to%0Adevelop%20high-quality%20generative%20AI%20services.%20While%20existing%20RAG%20models%0Ademonstrate%20high%20accuracy%20and%20fluency%20by%20utilizing%20retrieved%20information%2C%20they%0Amay%20suffer%20from%20accuracy%20degradation%20as%20they%20generate%20responses%20using%0Apre-loaded%20knowledge%20without%20reprocessing.%20Additionally%2C%20they%20cannot%0Aincorporate%20real-time%20data%20after%20the%20RAG%20configuration%20stage%2C%20leading%20to%20issues%0Awith%20contextual%20understanding%20and%20biased%20information.%20To%20address%20these%0Alimitations%2C%20this%20study%20implemented%20an%20enhanced%20RAG%20system%20utilizing%20Graph%0Atechnology.%20This%20system%20is%20designed%20to%20efficiently%20search%20and%20utilize%0Ainformation.%20Specifically%2C%20it%20employs%20LangGraph%20to%20evaluate%20the%20reliability%20of%0Aretrieved%20information%20and%20synthesizes%20diverse%20data%20to%20generate%20more%20accurate%0Aand%20enhanced%20responses.%20Furthermore%2C%20the%20study%20provides%20a%20detailed%20explanation%0Aof%20the%20system%27s%20operation%2C%20key%20implementation%20steps%2C%20and%20examples%20through%0Aimplementation%20code%20and%20validation%20results%2C%20thereby%20enhancing%20the%20understanding%0Aof%20advanced%20RAG%20technology.%20This%20approach%20offers%20practical%20guidelines%20for%0Aimplementing%20advanced%20RAG%20systems%20in%20corporate%20services%2C%20making%20it%20a%20valuable%0Aresource%20for%20practical%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520on%2520the%2520Implementation%2520Method%2520of%2520an%2520Agent-Based%2520Advanced%2520RAG%250A%2520%2520System%2520Using%2520Graph%26entry.906535625%3DCheonsu%2520Jeong%26entry.1292438233%3D%2520%2520This%2520study%2520aims%2520to%2520improve%2520knowledge-based%2520question-answering%2520%2528QA%2529%2520systems%2520by%250Aovercoming%2520the%2520limitations%2520of%2520existing%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%250Amodels%2520and%2520implementing%2520an%2520advanced%2520RAG%2520system%2520based%2520on%2520Graph%2520technology%2520to%250Adevelop%2520high-quality%2520generative%2520AI%2520services.%2520While%2520existing%2520RAG%2520models%250Ademonstrate%2520high%2520accuracy%2520and%2520fluency%2520by%2520utilizing%2520retrieved%2520information%252C%2520they%250Amay%2520suffer%2520from%2520accuracy%2520degradation%2520as%2520they%2520generate%2520responses%2520using%250Apre-loaded%2520knowledge%2520without%2520reprocessing.%2520Additionally%252C%2520they%2520cannot%250Aincorporate%2520real-time%2520data%2520after%2520the%2520RAG%2520configuration%2520stage%252C%2520leading%2520to%2520issues%250Awith%2520contextual%2520understanding%2520and%2520biased%2520information.%2520To%2520address%2520these%250Alimitations%252C%2520this%2520study%2520implemented%2520an%2520enhanced%2520RAG%2520system%2520utilizing%2520Graph%250Atechnology.%2520This%2520system%2520is%2520designed%2520to%2520efficiently%2520search%2520and%2520utilize%250Ainformation.%2520Specifically%252C%2520it%2520employs%2520LangGraph%2520to%2520evaluate%2520the%2520reliability%2520of%250Aretrieved%2520information%2520and%2520synthesizes%2520diverse%2520data%2520to%2520generate%2520more%2520accurate%250Aand%2520enhanced%2520responses.%2520Furthermore%252C%2520the%2520study%2520provides%2520a%2520detailed%2520explanation%250Aof%2520the%2520system%2527s%2520operation%252C%2520key%2520implementation%2520steps%252C%2520and%2520examples%2520through%250Aimplementation%2520code%2520and%2520validation%2520results%252C%2520thereby%2520enhancing%2520the%2520understanding%250Aof%2520advanced%2520RAG%2520technology.%2520This%2520approach%2520offers%2520practical%2520guidelines%2520for%250Aimplementing%2520advanced%2520RAG%2520systems%2520in%2520corporate%2520services%252C%2520making%2520it%2520a%2520valuable%250Aresource%2520for%2520practical%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20on%20the%20Implementation%20Method%20of%20an%20Agent-Based%20Advanced%20RAG%0A%20%20System%20Using%20Graph&entry.906535625=Cheonsu%20Jeong&entry.1292438233=%20%20This%20study%20aims%20to%20improve%20knowledge-based%20question-answering%20%28QA%29%20systems%20by%0Aovercoming%20the%20limitations%20of%20existing%20Retrieval-Augmented%20Generation%20%28RAG%29%0Amodels%20and%20implementing%20an%20advanced%20RAG%20system%20based%20on%20Graph%20technology%20to%0Adevelop%20high-quality%20generative%20AI%20services.%20While%20existing%20RAG%20models%0Ademonstrate%20high%20accuracy%20and%20fluency%20by%20utilizing%20retrieved%20information%2C%20they%0Amay%20suffer%20from%20accuracy%20degradation%20as%20they%20generate%20responses%20using%0Apre-loaded%20knowledge%20without%20reprocessing.%20Additionally%2C%20they%20cannot%0Aincorporate%20real-time%20data%20after%20the%20RAG%20configuration%20stage%2C%20leading%20to%20issues%0Awith%20contextual%20understanding%20and%20biased%20information.%20To%20address%20these%0Alimitations%2C%20this%20study%20implemented%20an%20enhanced%20RAG%20system%20utilizing%20Graph%0Atechnology.%20This%20system%20is%20designed%20to%20efficiently%20search%20and%20utilize%0Ainformation.%20Specifically%2C%20it%20employs%20LangGraph%20to%20evaluate%20the%20reliability%20of%0Aretrieved%20information%20and%20synthesizes%20diverse%20data%20to%20generate%20more%20accurate%0Aand%20enhanced%20responses.%20Furthermore%2C%20the%20study%20provides%20a%20detailed%20explanation%0Aof%20the%20system%27s%20operation%2C%20key%20implementation%20steps%2C%20and%20examples%20through%0Aimplementation%20code%20and%20validation%20results%2C%20thereby%20enhancing%20the%20understanding%0Aof%20advanced%20RAG%20technology.%20This%20approach%20offers%20practical%20guidelines%20for%0Aimplementing%20advanced%20RAG%20systems%20in%20corporate%20services%2C%20making%20it%20a%20valuable%0Aresource%20for%20practical%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19994v1&entry.124074799=Read"},
{"title": "Selection for short-term empowerment accelerates the evolution of\n  homeostatic neural cellular automata", "author": "Caitlin Grasso and Josh Bongard", "abstract": "  Empowerment -- a domain independent, information-theoretic metric -- has\npreviously been shown to assist in the evolutionary search for neural cellular\nautomata (NCA) capable of homeostasis when employed as a fitness function. In\nour previous study, we successfully extended empowerment, defined as maximum\ntime-lagged mutual information between agents' actions and future sensations,\nto a distributed sensorimotor system embodied as an NCA. However, the\ntime-delay between actions and their corresponding sensations was arbitrarily\nchosen. Here, we expand upon previous work by exploring how the time scale at\nwhich empowerment operates impacts its efficacy as an auxiliary objective to\naccelerate the discovery of homeostatic NCAs. We show that shorter time delays\nresult in marked improvements over empowerment with longer delays, when\ncompared to evolutionary selection only for homeostasis. Moreover, we evaluate\nstability and adaptability of evolved NCAs, both hallmarks of living systems\nthat are of interest to replicate in artificial ones. We find that short-term\nempowered NCA are more stable and are capable of generalizing better to unseen\nhomeostatic challenges. Taken together, these findings motivate the use of\nempowerment during the evolution of other artifacts, and suggest how it should\nbe incorporated to accelerate evolution of desired behaviors for them. Source\ncode for the experiments in this paper can be found at:\nhttps://github.com/caitlingrasso/empowered-nca-II.\n", "link": "http://arxiv.org/abs/2305.15220v2", "date": "2024-07-29", "relevancy": 1.8852, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4987}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4801}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selection%20for%20short-term%20empowerment%20accelerates%20the%20evolution%20of%0A%20%20homeostatic%20neural%20cellular%20automata&body=Title%3A%20Selection%20for%20short-term%20empowerment%20accelerates%20the%20evolution%20of%0A%20%20homeostatic%20neural%20cellular%20automata%0AAuthor%3A%20Caitlin%20Grasso%20and%20Josh%20Bongard%0AAbstract%3A%20%20%20Empowerment%20--%20a%20domain%20independent%2C%20information-theoretic%20metric%20--%20has%0Apreviously%20been%20shown%20to%20assist%20in%20the%20evolutionary%20search%20for%20neural%20cellular%0Aautomata%20%28NCA%29%20capable%20of%20homeostasis%20when%20employed%20as%20a%20fitness%20function.%20In%0Aour%20previous%20study%2C%20we%20successfully%20extended%20empowerment%2C%20defined%20as%20maximum%0Atime-lagged%20mutual%20information%20between%20agents%27%20actions%20and%20future%20sensations%2C%0Ato%20a%20distributed%20sensorimotor%20system%20embodied%20as%20an%20NCA.%20However%2C%20the%0Atime-delay%20between%20actions%20and%20their%20corresponding%20sensations%20was%20arbitrarily%0Achosen.%20Here%2C%20we%20expand%20upon%20previous%20work%20by%20exploring%20how%20the%20time%20scale%20at%0Awhich%20empowerment%20operates%20impacts%20its%20efficacy%20as%20an%20auxiliary%20objective%20to%0Aaccelerate%20the%20discovery%20of%20homeostatic%20NCAs.%20We%20show%20that%20shorter%20time%20delays%0Aresult%20in%20marked%20improvements%20over%20empowerment%20with%20longer%20delays%2C%20when%0Acompared%20to%20evolutionary%20selection%20only%20for%20homeostasis.%20Moreover%2C%20we%20evaluate%0Astability%20and%20adaptability%20of%20evolved%20NCAs%2C%20both%20hallmarks%20of%20living%20systems%0Athat%20are%20of%20interest%20to%20replicate%20in%20artificial%20ones.%20We%20find%20that%20short-term%0Aempowered%20NCA%20are%20more%20stable%20and%20are%20capable%20of%20generalizing%20better%20to%20unseen%0Ahomeostatic%20challenges.%20Taken%20together%2C%20these%20findings%20motivate%20the%20use%20of%0Aempowerment%20during%20the%20evolution%20of%20other%20artifacts%2C%20and%20suggest%20how%20it%20should%0Abe%20incorporated%20to%20accelerate%20evolution%20of%20desired%20behaviors%20for%20them.%20Source%0Acode%20for%20the%20experiments%20in%20this%20paper%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/caitlingrasso/empowered-nca-II.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelection%2520for%2520short-term%2520empowerment%2520accelerates%2520the%2520evolution%2520of%250A%2520%2520homeostatic%2520neural%2520cellular%2520automata%26entry.906535625%3DCaitlin%2520Grasso%2520and%2520Josh%2520Bongard%26entry.1292438233%3D%2520%2520Empowerment%2520--%2520a%2520domain%2520independent%252C%2520information-theoretic%2520metric%2520--%2520has%250Apreviously%2520been%2520shown%2520to%2520assist%2520in%2520the%2520evolutionary%2520search%2520for%2520neural%2520cellular%250Aautomata%2520%2528NCA%2529%2520capable%2520of%2520homeostasis%2520when%2520employed%2520as%2520a%2520fitness%2520function.%2520In%250Aour%2520previous%2520study%252C%2520we%2520successfully%2520extended%2520empowerment%252C%2520defined%2520as%2520maximum%250Atime-lagged%2520mutual%2520information%2520between%2520agents%2527%2520actions%2520and%2520future%2520sensations%252C%250Ato%2520a%2520distributed%2520sensorimotor%2520system%2520embodied%2520as%2520an%2520NCA.%2520However%252C%2520the%250Atime-delay%2520between%2520actions%2520and%2520their%2520corresponding%2520sensations%2520was%2520arbitrarily%250Achosen.%2520Here%252C%2520we%2520expand%2520upon%2520previous%2520work%2520by%2520exploring%2520how%2520the%2520time%2520scale%2520at%250Awhich%2520empowerment%2520operates%2520impacts%2520its%2520efficacy%2520as%2520an%2520auxiliary%2520objective%2520to%250Aaccelerate%2520the%2520discovery%2520of%2520homeostatic%2520NCAs.%2520We%2520show%2520that%2520shorter%2520time%2520delays%250Aresult%2520in%2520marked%2520improvements%2520over%2520empowerment%2520with%2520longer%2520delays%252C%2520when%250Acompared%2520to%2520evolutionary%2520selection%2520only%2520for%2520homeostasis.%2520Moreover%252C%2520we%2520evaluate%250Astability%2520and%2520adaptability%2520of%2520evolved%2520NCAs%252C%2520both%2520hallmarks%2520of%2520living%2520systems%250Athat%2520are%2520of%2520interest%2520to%2520replicate%2520in%2520artificial%2520ones.%2520We%2520find%2520that%2520short-term%250Aempowered%2520NCA%2520are%2520more%2520stable%2520and%2520are%2520capable%2520of%2520generalizing%2520better%2520to%2520unseen%250Ahomeostatic%2520challenges.%2520Taken%2520together%252C%2520these%2520findings%2520motivate%2520the%2520use%2520of%250Aempowerment%2520during%2520the%2520evolution%2520of%2520other%2520artifacts%252C%2520and%2520suggest%2520how%2520it%2520should%250Abe%2520incorporated%2520to%2520accelerate%2520evolution%2520of%2520desired%2520behaviors%2520for%2520them.%2520Source%250Acode%2520for%2520the%2520experiments%2520in%2520this%2520paper%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/caitlingrasso/empowered-nca-II.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selection%20for%20short-term%20empowerment%20accelerates%20the%20evolution%20of%0A%20%20homeostatic%20neural%20cellular%20automata&entry.906535625=Caitlin%20Grasso%20and%20Josh%20Bongard&entry.1292438233=%20%20Empowerment%20--%20a%20domain%20independent%2C%20information-theoretic%20metric%20--%20has%0Apreviously%20been%20shown%20to%20assist%20in%20the%20evolutionary%20search%20for%20neural%20cellular%0Aautomata%20%28NCA%29%20capable%20of%20homeostasis%20when%20employed%20as%20a%20fitness%20function.%20In%0Aour%20previous%20study%2C%20we%20successfully%20extended%20empowerment%2C%20defined%20as%20maximum%0Atime-lagged%20mutual%20information%20between%20agents%27%20actions%20and%20future%20sensations%2C%0Ato%20a%20distributed%20sensorimotor%20system%20embodied%20as%20an%20NCA.%20However%2C%20the%0Atime-delay%20between%20actions%20and%20their%20corresponding%20sensations%20was%20arbitrarily%0Achosen.%20Here%2C%20we%20expand%20upon%20previous%20work%20by%20exploring%20how%20the%20time%20scale%20at%0Awhich%20empowerment%20operates%20impacts%20its%20efficacy%20as%20an%20auxiliary%20objective%20to%0Aaccelerate%20the%20discovery%20of%20homeostatic%20NCAs.%20We%20show%20that%20shorter%20time%20delays%0Aresult%20in%20marked%20improvements%20over%20empowerment%20with%20longer%20delays%2C%20when%0Acompared%20to%20evolutionary%20selection%20only%20for%20homeostasis.%20Moreover%2C%20we%20evaluate%0Astability%20and%20adaptability%20of%20evolved%20NCAs%2C%20both%20hallmarks%20of%20living%20systems%0Athat%20are%20of%20interest%20to%20replicate%20in%20artificial%20ones.%20We%20find%20that%20short-term%0Aempowered%20NCA%20are%20more%20stable%20and%20are%20capable%20of%20generalizing%20better%20to%20unseen%0Ahomeostatic%20challenges.%20Taken%20together%2C%20these%20findings%20motivate%20the%20use%20of%0Aempowerment%20during%20the%20evolution%20of%20other%20artifacts%2C%20and%20suggest%20how%20it%20should%0Abe%20incorporated%20to%20accelerate%20evolution%20of%20desired%20behaviors%20for%20them.%20Source%0Acode%20for%20the%20experiments%20in%20this%20paper%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/caitlingrasso/empowered-nca-II.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15220v2&entry.124074799=Read"},
{"title": "The Shape of Money Laundering: Subgraph Representation Learning on the\n  Blockchain with the Elliptic2 Dataset", "author": "Claudio Bellei and Muhua Xu and Ross Phillips and Tom Robinson and Mark Weber and Tim Kaler and Charles E. Leiserson and  Arvind and Jie Chen", "abstract": "  Subgraph representation learning is a technique for analyzing local\nstructures (or shapes) within complex networks. Enabled by recent developments\nin scalable Graph Neural Networks (GNNs), this approach encodes relational\ninformation at a subgroup level (multiple connected nodes) rather than at a\nnode level of abstraction. We posit that certain domain applications, such as\nanti-money laundering (AML), are inherently subgraph problems and mainstream\ngraph techniques have been operating at a suboptimal level of abstraction. This\nis due in part to the scarcity of annotated datasets of real-world size and\ncomplexity, as well as the lack of software tools for managing subgraph GNN\nworkflows at scale. To enable work in fundamental algorithms as well as domain\napplications in AML and beyond, we introduce Elliptic2, a large graph dataset\ncontaining 122K labeled subgraphs of Bitcoin clusters within a background graph\nconsisting of 49M node clusters and 196M edge transactions. The dataset\nprovides subgraphs known to be linked to illicit activity for learning the set\nof \"shapes\" that money laundering exhibits in cryptocurrency and accurately\nclassifying new criminal activity. Along with the dataset we share our graph\ntechniques, software tooling, promising early experimental results, and new\ndomain insights already gleaned from this approach. Taken together, we find\nimmediate practical value in this approach and the potential for a new standard\nin anti-money laundering and forensic analytics in cryptocurrencies and other\nfinancial networks.\n", "link": "http://arxiv.org/abs/2404.19109v3", "date": "2024-07-29", "relevancy": 1.8843, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5097}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4447}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Shape%20of%20Money%20Laundering%3A%20Subgraph%20Representation%20Learning%20on%20the%0A%20%20Blockchain%20with%20the%20Elliptic2%20Dataset&body=Title%3A%20The%20Shape%20of%20Money%20Laundering%3A%20Subgraph%20Representation%20Learning%20on%20the%0A%20%20Blockchain%20with%20the%20Elliptic2%20Dataset%0AAuthor%3A%20Claudio%20Bellei%20and%20Muhua%20Xu%20and%20Ross%20Phillips%20and%20Tom%20Robinson%20and%20Mark%20Weber%20and%20Tim%20Kaler%20and%20Charles%20E.%20Leiserson%20and%20%20Arvind%20and%20Jie%20Chen%0AAbstract%3A%20%20%20Subgraph%20representation%20learning%20is%20a%20technique%20for%20analyzing%20local%0Astructures%20%28or%20shapes%29%20within%20complex%20networks.%20Enabled%20by%20recent%20developments%0Ain%20scalable%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20this%20approach%20encodes%20relational%0Ainformation%20at%20a%20subgroup%20level%20%28multiple%20connected%20nodes%29%20rather%20than%20at%20a%0Anode%20level%20of%20abstraction.%20We%20posit%20that%20certain%20domain%20applications%2C%20such%20as%0Aanti-money%20laundering%20%28AML%29%2C%20are%20inherently%20subgraph%20problems%20and%20mainstream%0Agraph%20techniques%20have%20been%20operating%20at%20a%20suboptimal%20level%20of%20abstraction.%20This%0Ais%20due%20in%20part%20to%20the%20scarcity%20of%20annotated%20datasets%20of%20real-world%20size%20and%0Acomplexity%2C%20as%20well%20as%20the%20lack%20of%20software%20tools%20for%20managing%20subgraph%20GNN%0Aworkflows%20at%20scale.%20To%20enable%20work%20in%20fundamental%20algorithms%20as%20well%20as%20domain%0Aapplications%20in%20AML%20and%20beyond%2C%20we%20introduce%20Elliptic2%2C%20a%20large%20graph%20dataset%0Acontaining%20122K%20labeled%20subgraphs%20of%20Bitcoin%20clusters%20within%20a%20background%20graph%0Aconsisting%20of%2049M%20node%20clusters%20and%20196M%20edge%20transactions.%20The%20dataset%0Aprovides%20subgraphs%20known%20to%20be%20linked%20to%20illicit%20activity%20for%20learning%20the%20set%0Aof%20%22shapes%22%20that%20money%20laundering%20exhibits%20in%20cryptocurrency%20and%20accurately%0Aclassifying%20new%20criminal%20activity.%20Along%20with%20the%20dataset%20we%20share%20our%20graph%0Atechniques%2C%20software%20tooling%2C%20promising%20early%20experimental%20results%2C%20and%20new%0Adomain%20insights%20already%20gleaned%20from%20this%20approach.%20Taken%20together%2C%20we%20find%0Aimmediate%20practical%20value%20in%20this%20approach%20and%20the%20potential%20for%20a%20new%20standard%0Ain%20anti-money%20laundering%20and%20forensic%20analytics%20in%20cryptocurrencies%20and%20other%0Afinancial%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19109v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Shape%2520of%2520Money%2520Laundering%253A%2520Subgraph%2520Representation%2520Learning%2520on%2520the%250A%2520%2520Blockchain%2520with%2520the%2520Elliptic2%2520Dataset%26entry.906535625%3DClaudio%2520Bellei%2520and%2520Muhua%2520Xu%2520and%2520Ross%2520Phillips%2520and%2520Tom%2520Robinson%2520and%2520Mark%2520Weber%2520and%2520Tim%2520Kaler%2520and%2520Charles%2520E.%2520Leiserson%2520and%2520%2520Arvind%2520and%2520Jie%2520Chen%26entry.1292438233%3D%2520%2520Subgraph%2520representation%2520learning%2520is%2520a%2520technique%2520for%2520analyzing%2520local%250Astructures%2520%2528or%2520shapes%2529%2520within%2520complex%2520networks.%2520Enabled%2520by%2520recent%2520developments%250Ain%2520scalable%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520this%2520approach%2520encodes%2520relational%250Ainformation%2520at%2520a%2520subgroup%2520level%2520%2528multiple%2520connected%2520nodes%2529%2520rather%2520than%2520at%2520a%250Anode%2520level%2520of%2520abstraction.%2520We%2520posit%2520that%2520certain%2520domain%2520applications%252C%2520such%2520as%250Aanti-money%2520laundering%2520%2528AML%2529%252C%2520are%2520inherently%2520subgraph%2520problems%2520and%2520mainstream%250Agraph%2520techniques%2520have%2520been%2520operating%2520at%2520a%2520suboptimal%2520level%2520of%2520abstraction.%2520This%250Ais%2520due%2520in%2520part%2520to%2520the%2520scarcity%2520of%2520annotated%2520datasets%2520of%2520real-world%2520size%2520and%250Acomplexity%252C%2520as%2520well%2520as%2520the%2520lack%2520of%2520software%2520tools%2520for%2520managing%2520subgraph%2520GNN%250Aworkflows%2520at%2520scale.%2520To%2520enable%2520work%2520in%2520fundamental%2520algorithms%2520as%2520well%2520as%2520domain%250Aapplications%2520in%2520AML%2520and%2520beyond%252C%2520we%2520introduce%2520Elliptic2%252C%2520a%2520large%2520graph%2520dataset%250Acontaining%2520122K%2520labeled%2520subgraphs%2520of%2520Bitcoin%2520clusters%2520within%2520a%2520background%2520graph%250Aconsisting%2520of%252049M%2520node%2520clusters%2520and%2520196M%2520edge%2520transactions.%2520The%2520dataset%250Aprovides%2520subgraphs%2520known%2520to%2520be%2520linked%2520to%2520illicit%2520activity%2520for%2520learning%2520the%2520set%250Aof%2520%2522shapes%2522%2520that%2520money%2520laundering%2520exhibits%2520in%2520cryptocurrency%2520and%2520accurately%250Aclassifying%2520new%2520criminal%2520activity.%2520Along%2520with%2520the%2520dataset%2520we%2520share%2520our%2520graph%250Atechniques%252C%2520software%2520tooling%252C%2520promising%2520early%2520experimental%2520results%252C%2520and%2520new%250Adomain%2520insights%2520already%2520gleaned%2520from%2520this%2520approach.%2520Taken%2520together%252C%2520we%2520find%250Aimmediate%2520practical%2520value%2520in%2520this%2520approach%2520and%2520the%2520potential%2520for%2520a%2520new%2520standard%250Ain%2520anti-money%2520laundering%2520and%2520forensic%2520analytics%2520in%2520cryptocurrencies%2520and%2520other%250Afinancial%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19109v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Shape%20of%20Money%20Laundering%3A%20Subgraph%20Representation%20Learning%20on%20the%0A%20%20Blockchain%20with%20the%20Elliptic2%20Dataset&entry.906535625=Claudio%20Bellei%20and%20Muhua%20Xu%20and%20Ross%20Phillips%20and%20Tom%20Robinson%20and%20Mark%20Weber%20and%20Tim%20Kaler%20and%20Charles%20E.%20Leiserson%20and%20%20Arvind%20and%20Jie%20Chen&entry.1292438233=%20%20Subgraph%20representation%20learning%20is%20a%20technique%20for%20analyzing%20local%0Astructures%20%28or%20shapes%29%20within%20complex%20networks.%20Enabled%20by%20recent%20developments%0Ain%20scalable%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20this%20approach%20encodes%20relational%0Ainformation%20at%20a%20subgroup%20level%20%28multiple%20connected%20nodes%29%20rather%20than%20at%20a%0Anode%20level%20of%20abstraction.%20We%20posit%20that%20certain%20domain%20applications%2C%20such%20as%0Aanti-money%20laundering%20%28AML%29%2C%20are%20inherently%20subgraph%20problems%20and%20mainstream%0Agraph%20techniques%20have%20been%20operating%20at%20a%20suboptimal%20level%20of%20abstraction.%20This%0Ais%20due%20in%20part%20to%20the%20scarcity%20of%20annotated%20datasets%20of%20real-world%20size%20and%0Acomplexity%2C%20as%20well%20as%20the%20lack%20of%20software%20tools%20for%20managing%20subgraph%20GNN%0Aworkflows%20at%20scale.%20To%20enable%20work%20in%20fundamental%20algorithms%20as%20well%20as%20domain%0Aapplications%20in%20AML%20and%20beyond%2C%20we%20introduce%20Elliptic2%2C%20a%20large%20graph%20dataset%0Acontaining%20122K%20labeled%20subgraphs%20of%20Bitcoin%20clusters%20within%20a%20background%20graph%0Aconsisting%20of%2049M%20node%20clusters%20and%20196M%20edge%20transactions.%20The%20dataset%0Aprovides%20subgraphs%20known%20to%20be%20linked%20to%20illicit%20activity%20for%20learning%20the%20set%0Aof%20%22shapes%22%20that%20money%20laundering%20exhibits%20in%20cryptocurrency%20and%20accurately%0Aclassifying%20new%20criminal%20activity.%20Along%20with%20the%20dataset%20we%20share%20our%20graph%0Atechniques%2C%20software%20tooling%2C%20promising%20early%20experimental%20results%2C%20and%20new%0Adomain%20insights%20already%20gleaned%20from%20this%20approach.%20Taken%20together%2C%20we%20find%0Aimmediate%20practical%20value%20in%20this%20approach%20and%20the%20potential%20for%20a%20new%20standard%0Ain%20anti-money%20laundering%20and%20forensic%20analytics%20in%20cryptocurrencies%20and%20other%0Afinancial%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19109v3&entry.124074799=Read"},
{"title": "Quantum Machine Learning Architecture Search via Deep Reinforcement\n  Learning", "author": "Xin Dai and Tzu-Chieh Wei and Shinjae Yoo and Samuel Yen-Chi Chen", "abstract": "  The rapid advancement of quantum computing (QC) and machine learning (ML) has\ngiven rise to the burgeoning field of quantum machine learning (QML), aiming to\ncapitalize on the strengths of quantum computing to propel ML forward. Despite\nits promise, crafting effective QML models necessitates profound expertise to\nstrike a delicate balance between model intricacy and feasibility on Noisy\nIntermediate-Scale Quantum (NISQ) devices. While complex models offer robust\nrepresentation capabilities, their extensive circuit depth may impede seamless\nexecution on extant noisy quantum platforms. In this paper, we address this\nquandary of QML model design by employing deep reinforcement learning to\nexplore proficient QML model architectures tailored for designated supervised\nlearning tasks. Specifically, our methodology involves training an RL agent to\ndevise policies that facilitate the discovery of QML models without\npredetermined ansatz. Furthermore, we integrate an adaptive mechanism to\ndynamically adjust the learning objectives, fostering continuous improvement in\nthe agent's learning process. Through extensive numerical simulations, we\nillustrate the efficacy of our approach within the realm of classification\ntasks. Our proposed method successfully identifies VQC architectures capable of\nachieving high classification accuracy while minimizing gate depth. This\npioneering approach not only advances the study of AI-driven quantum circuit\ndesign but also holds significant promise for enhancing performance in the NISQ\nera.\n", "link": "http://arxiv.org/abs/2407.20147v1", "date": "2024-07-29", "relevancy": 1.8791, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4914}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4676}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Machine%20Learning%20Architecture%20Search%20via%20Deep%20Reinforcement%0A%20%20Learning&body=Title%3A%20Quantum%20Machine%20Learning%20Architecture%20Search%20via%20Deep%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Xin%20Dai%20and%20Tzu-Chieh%20Wei%20and%20Shinjae%20Yoo%20and%20Samuel%20Yen-Chi%20Chen%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20quantum%20computing%20%28QC%29%20and%20machine%20learning%20%28ML%29%20has%0Agiven%20rise%20to%20the%20burgeoning%20field%20of%20quantum%20machine%20learning%20%28QML%29%2C%20aiming%20to%0Acapitalize%20on%20the%20strengths%20of%20quantum%20computing%20to%20propel%20ML%20forward.%20Despite%0Aits%20promise%2C%20crafting%20effective%20QML%20models%20necessitates%20profound%20expertise%20to%0Astrike%20a%20delicate%20balance%20between%20model%20intricacy%20and%20feasibility%20on%20Noisy%0AIntermediate-Scale%20Quantum%20%28NISQ%29%20devices.%20While%20complex%20models%20offer%20robust%0Arepresentation%20capabilities%2C%20their%20extensive%20circuit%20depth%20may%20impede%20seamless%0Aexecution%20on%20extant%20noisy%20quantum%20platforms.%20In%20this%20paper%2C%20we%20address%20this%0Aquandary%20of%20QML%20model%20design%20by%20employing%20deep%20reinforcement%20learning%20to%0Aexplore%20proficient%20QML%20model%20architectures%20tailored%20for%20designated%20supervised%0Alearning%20tasks.%20Specifically%2C%20our%20methodology%20involves%20training%20an%20RL%20agent%20to%0Adevise%20policies%20that%20facilitate%20the%20discovery%20of%20QML%20models%20without%0Apredetermined%20ansatz.%20Furthermore%2C%20we%20integrate%20an%20adaptive%20mechanism%20to%0Adynamically%20adjust%20the%20learning%20objectives%2C%20fostering%20continuous%20improvement%20in%0Athe%20agent%27s%20learning%20process.%20Through%20extensive%20numerical%20simulations%2C%20we%0Aillustrate%20the%20efficacy%20of%20our%20approach%20within%20the%20realm%20of%20classification%0Atasks.%20Our%20proposed%20method%20successfully%20identifies%20VQC%20architectures%20capable%20of%0Aachieving%20high%20classification%20accuracy%20while%20minimizing%20gate%20depth.%20This%0Apioneering%20approach%20not%20only%20advances%20the%20study%20of%20AI-driven%20quantum%20circuit%0Adesign%20but%20also%20holds%20significant%20promise%20for%20enhancing%20performance%20in%20the%20NISQ%0Aera.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Machine%2520Learning%2520Architecture%2520Search%2520via%2520Deep%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DXin%2520Dai%2520and%2520Tzu-Chieh%2520Wei%2520and%2520Shinjae%2520Yoo%2520and%2520Samuel%2520Yen-Chi%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520quantum%2520computing%2520%2528QC%2529%2520and%2520machine%2520learning%2520%2528ML%2529%2520has%250Agiven%2520rise%2520to%2520the%2520burgeoning%2520field%2520of%2520quantum%2520machine%2520learning%2520%2528QML%2529%252C%2520aiming%2520to%250Acapitalize%2520on%2520the%2520strengths%2520of%2520quantum%2520computing%2520to%2520propel%2520ML%2520forward.%2520Despite%250Aits%2520promise%252C%2520crafting%2520effective%2520QML%2520models%2520necessitates%2520profound%2520expertise%2520to%250Astrike%2520a%2520delicate%2520balance%2520between%2520model%2520intricacy%2520and%2520feasibility%2520on%2520Noisy%250AIntermediate-Scale%2520Quantum%2520%2528NISQ%2529%2520devices.%2520While%2520complex%2520models%2520offer%2520robust%250Arepresentation%2520capabilities%252C%2520their%2520extensive%2520circuit%2520depth%2520may%2520impede%2520seamless%250Aexecution%2520on%2520extant%2520noisy%2520quantum%2520platforms.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%250Aquandary%2520of%2520QML%2520model%2520design%2520by%2520employing%2520deep%2520reinforcement%2520learning%2520to%250Aexplore%2520proficient%2520QML%2520model%2520architectures%2520tailored%2520for%2520designated%2520supervised%250Alearning%2520tasks.%2520Specifically%252C%2520our%2520methodology%2520involves%2520training%2520an%2520RL%2520agent%2520to%250Adevise%2520policies%2520that%2520facilitate%2520the%2520discovery%2520of%2520QML%2520models%2520without%250Apredetermined%2520ansatz.%2520Furthermore%252C%2520we%2520integrate%2520an%2520adaptive%2520mechanism%2520to%250Adynamically%2520adjust%2520the%2520learning%2520objectives%252C%2520fostering%2520continuous%2520improvement%2520in%250Athe%2520agent%2527s%2520learning%2520process.%2520Through%2520extensive%2520numerical%2520simulations%252C%2520we%250Aillustrate%2520the%2520efficacy%2520of%2520our%2520approach%2520within%2520the%2520realm%2520of%2520classification%250Atasks.%2520Our%2520proposed%2520method%2520successfully%2520identifies%2520VQC%2520architectures%2520capable%2520of%250Aachieving%2520high%2520classification%2520accuracy%2520while%2520minimizing%2520gate%2520depth.%2520This%250Apioneering%2520approach%2520not%2520only%2520advances%2520the%2520study%2520of%2520AI-driven%2520quantum%2520circuit%250Adesign%2520but%2520also%2520holds%2520significant%2520promise%2520for%2520enhancing%2520performance%2520in%2520the%2520NISQ%250Aera.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Machine%20Learning%20Architecture%20Search%20via%20Deep%20Reinforcement%0A%20%20Learning&entry.906535625=Xin%20Dai%20and%20Tzu-Chieh%20Wei%20and%20Shinjae%20Yoo%20and%20Samuel%20Yen-Chi%20Chen&entry.1292438233=%20%20The%20rapid%20advancement%20of%20quantum%20computing%20%28QC%29%20and%20machine%20learning%20%28ML%29%20has%0Agiven%20rise%20to%20the%20burgeoning%20field%20of%20quantum%20machine%20learning%20%28QML%29%2C%20aiming%20to%0Acapitalize%20on%20the%20strengths%20of%20quantum%20computing%20to%20propel%20ML%20forward.%20Despite%0Aits%20promise%2C%20crafting%20effective%20QML%20models%20necessitates%20profound%20expertise%20to%0Astrike%20a%20delicate%20balance%20between%20model%20intricacy%20and%20feasibility%20on%20Noisy%0AIntermediate-Scale%20Quantum%20%28NISQ%29%20devices.%20While%20complex%20models%20offer%20robust%0Arepresentation%20capabilities%2C%20their%20extensive%20circuit%20depth%20may%20impede%20seamless%0Aexecution%20on%20extant%20noisy%20quantum%20platforms.%20In%20this%20paper%2C%20we%20address%20this%0Aquandary%20of%20QML%20model%20design%20by%20employing%20deep%20reinforcement%20learning%20to%0Aexplore%20proficient%20QML%20model%20architectures%20tailored%20for%20designated%20supervised%0Alearning%20tasks.%20Specifically%2C%20our%20methodology%20involves%20training%20an%20RL%20agent%20to%0Adevise%20policies%20that%20facilitate%20the%20discovery%20of%20QML%20models%20without%0Apredetermined%20ansatz.%20Furthermore%2C%20we%20integrate%20an%20adaptive%20mechanism%20to%0Adynamically%20adjust%20the%20learning%20objectives%2C%20fostering%20continuous%20improvement%20in%0Athe%20agent%27s%20learning%20process.%20Through%20extensive%20numerical%20simulations%2C%20we%0Aillustrate%20the%20efficacy%20of%20our%20approach%20within%20the%20realm%20of%20classification%0Atasks.%20Our%20proposed%20method%20successfully%20identifies%20VQC%20architectures%20capable%20of%0Aachieving%20high%20classification%20accuracy%20while%20minimizing%20gate%20depth.%20This%0Apioneering%20approach%20not%20only%20advances%20the%20study%20of%20AI-driven%20quantum%20circuit%0Adesign%20but%20also%20holds%20significant%20promise%20for%20enhancing%20performance%20in%20the%20NISQ%0Aera.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20147v1&entry.124074799=Read"},
{"title": "A Differential Dynamic Programming Framework for Inverse Reinforcement\n  Learning", "author": "Kun Cao and Xinhang Xu and Wanxin Jin and Karl H. Johansson and Lihua Xie", "abstract": "  A differential dynamic programming (DDP)-based framework for inverse\nreinforcement learning (IRL) is introduced to recover the parameters in the\ncost function, system dynamics, and constraints from demonstrations. Different\nfrom existing work, where DDP was used for the inner forward problem with\ninequality constraints, our proposed framework uses it for efficient\ncomputation of the gradient required in the outer inverse problem with equality\nand inequality constraints. The equivalence between the proposed method and\nexisting methods based on Pontryagin's Maximum Principle (PMP) is established.\nMore importantly, using this DDP-based IRL with an open-loop loss function, a\nclosed-loop IRL framework is presented. In this framework, a loss function is\nproposed to capture the closed-loop nature of demonstrations. It is shown to be\nbetter than the commonly used open-loop loss function. We show that the\nclosed-loop IRL framework reduces to a constrained inverse optimal control\nproblem under certain assumptions. Under these assumptions and a rank\ncondition, it is proven that the learning parameters can be recovered from the\ndemonstration data. The proposed framework is extensively evaluated through\nfour numerical robot examples and one real-world quadrotor system. The\nexperiments validate the theoretical results and illustrate the practical\nrelevance of the approach.\n", "link": "http://arxiv.org/abs/2407.19902v1", "date": "2024-07-29", "relevancy": 1.8754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5059}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4619}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Differential%20Dynamic%20Programming%20Framework%20for%20Inverse%20Reinforcement%0A%20%20Learning&body=Title%3A%20A%20Differential%20Dynamic%20Programming%20Framework%20for%20Inverse%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Kun%20Cao%20and%20Xinhang%20Xu%20and%20Wanxin%20Jin%20and%20Karl%20H.%20Johansson%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20A%20differential%20dynamic%20programming%20%28DDP%29-based%20framework%20for%20inverse%0Areinforcement%20learning%20%28IRL%29%20is%20introduced%20to%20recover%20the%20parameters%20in%20the%0Acost%20function%2C%20system%20dynamics%2C%20and%20constraints%20from%20demonstrations.%20Different%0Afrom%20existing%20work%2C%20where%20DDP%20was%20used%20for%20the%20inner%20forward%20problem%20with%0Ainequality%20constraints%2C%20our%20proposed%20framework%20uses%20it%20for%20efficient%0Acomputation%20of%20the%20gradient%20required%20in%20the%20outer%20inverse%20problem%20with%20equality%0Aand%20inequality%20constraints.%20The%20equivalence%20between%20the%20proposed%20method%20and%0Aexisting%20methods%20based%20on%20Pontryagin%27s%20Maximum%20Principle%20%28PMP%29%20is%20established.%0AMore%20importantly%2C%20using%20this%20DDP-based%20IRL%20with%20an%20open-loop%20loss%20function%2C%20a%0Aclosed-loop%20IRL%20framework%20is%20presented.%20In%20this%20framework%2C%20a%20loss%20function%20is%0Aproposed%20to%20capture%20the%20closed-loop%20nature%20of%20demonstrations.%20It%20is%20shown%20to%20be%0Abetter%20than%20the%20commonly%20used%20open-loop%20loss%20function.%20We%20show%20that%20the%0Aclosed-loop%20IRL%20framework%20reduces%20to%20a%20constrained%20inverse%20optimal%20control%0Aproblem%20under%20certain%20assumptions.%20Under%20these%20assumptions%20and%20a%20rank%0Acondition%2C%20it%20is%20proven%20that%20the%20learning%20parameters%20can%20be%20recovered%20from%20the%0Ademonstration%20data.%20The%20proposed%20framework%20is%20extensively%20evaluated%20through%0Afour%20numerical%20robot%20examples%20and%20one%20real-world%20quadrotor%20system.%20The%0Aexperiments%20validate%20the%20theoretical%20results%20and%20illustrate%20the%20practical%0Arelevance%20of%20the%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Differential%2520Dynamic%2520Programming%2520Framework%2520for%2520Inverse%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DKun%2520Cao%2520and%2520Xinhang%2520Xu%2520and%2520Wanxin%2520Jin%2520and%2520Karl%2520H.%2520Johansson%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520A%2520differential%2520dynamic%2520programming%2520%2528DDP%2529-based%2520framework%2520for%2520inverse%250Areinforcement%2520learning%2520%2528IRL%2529%2520is%2520introduced%2520to%2520recover%2520the%2520parameters%2520in%2520the%250Acost%2520function%252C%2520system%2520dynamics%252C%2520and%2520constraints%2520from%2520demonstrations.%2520Different%250Afrom%2520existing%2520work%252C%2520where%2520DDP%2520was%2520used%2520for%2520the%2520inner%2520forward%2520problem%2520with%250Ainequality%2520constraints%252C%2520our%2520proposed%2520framework%2520uses%2520it%2520for%2520efficient%250Acomputation%2520of%2520the%2520gradient%2520required%2520in%2520the%2520outer%2520inverse%2520problem%2520with%2520equality%250Aand%2520inequality%2520constraints.%2520The%2520equivalence%2520between%2520the%2520proposed%2520method%2520and%250Aexisting%2520methods%2520based%2520on%2520Pontryagin%2527s%2520Maximum%2520Principle%2520%2528PMP%2529%2520is%2520established.%250AMore%2520importantly%252C%2520using%2520this%2520DDP-based%2520IRL%2520with%2520an%2520open-loop%2520loss%2520function%252C%2520a%250Aclosed-loop%2520IRL%2520framework%2520is%2520presented.%2520In%2520this%2520framework%252C%2520a%2520loss%2520function%2520is%250Aproposed%2520to%2520capture%2520the%2520closed-loop%2520nature%2520of%2520demonstrations.%2520It%2520is%2520shown%2520to%2520be%250Abetter%2520than%2520the%2520commonly%2520used%2520open-loop%2520loss%2520function.%2520We%2520show%2520that%2520the%250Aclosed-loop%2520IRL%2520framework%2520reduces%2520to%2520a%2520constrained%2520inverse%2520optimal%2520control%250Aproblem%2520under%2520certain%2520assumptions.%2520Under%2520these%2520assumptions%2520and%2520a%2520rank%250Acondition%252C%2520it%2520is%2520proven%2520that%2520the%2520learning%2520parameters%2520can%2520be%2520recovered%2520from%2520the%250Ademonstration%2520data.%2520The%2520proposed%2520framework%2520is%2520extensively%2520evaluated%2520through%250Afour%2520numerical%2520robot%2520examples%2520and%2520one%2520real-world%2520quadrotor%2520system.%2520The%250Aexperiments%2520validate%2520the%2520theoretical%2520results%2520and%2520illustrate%2520the%2520practical%250Arelevance%2520of%2520the%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Differential%20Dynamic%20Programming%20Framework%20for%20Inverse%20Reinforcement%0A%20%20Learning&entry.906535625=Kun%20Cao%20and%20Xinhang%20Xu%20and%20Wanxin%20Jin%20and%20Karl%20H.%20Johansson%20and%20Lihua%20Xie&entry.1292438233=%20%20A%20differential%20dynamic%20programming%20%28DDP%29-based%20framework%20for%20inverse%0Areinforcement%20learning%20%28IRL%29%20is%20introduced%20to%20recover%20the%20parameters%20in%20the%0Acost%20function%2C%20system%20dynamics%2C%20and%20constraints%20from%20demonstrations.%20Different%0Afrom%20existing%20work%2C%20where%20DDP%20was%20used%20for%20the%20inner%20forward%20problem%20with%0Ainequality%20constraints%2C%20our%20proposed%20framework%20uses%20it%20for%20efficient%0Acomputation%20of%20the%20gradient%20required%20in%20the%20outer%20inverse%20problem%20with%20equality%0Aand%20inequality%20constraints.%20The%20equivalence%20between%20the%20proposed%20method%20and%0Aexisting%20methods%20based%20on%20Pontryagin%27s%20Maximum%20Principle%20%28PMP%29%20is%20established.%0AMore%20importantly%2C%20using%20this%20DDP-based%20IRL%20with%20an%20open-loop%20loss%20function%2C%20a%0Aclosed-loop%20IRL%20framework%20is%20presented.%20In%20this%20framework%2C%20a%20loss%20function%20is%0Aproposed%20to%20capture%20the%20closed-loop%20nature%20of%20demonstrations.%20It%20is%20shown%20to%20be%0Abetter%20than%20the%20commonly%20used%20open-loop%20loss%20function.%20We%20show%20that%20the%0Aclosed-loop%20IRL%20framework%20reduces%20to%20a%20constrained%20inverse%20optimal%20control%0Aproblem%20under%20certain%20assumptions.%20Under%20these%20assumptions%20and%20a%20rank%0Acondition%2C%20it%20is%20proven%20that%20the%20learning%20parameters%20can%20be%20recovered%20from%20the%0Ademonstration%20data.%20The%20proposed%20framework%20is%20extensively%20evaluated%20through%0Afour%20numerical%20robot%20examples%20and%20one%20real-world%20quadrotor%20system.%20The%0Aexperiments%20validate%20the%20theoretical%20results%20and%20illustrate%20the%20practical%0Arelevance%20of%20the%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19902v1&entry.124074799=Read"},
{"title": "Unleash the Power of Ellipsis: Accuracy-enhanced Sparse Vector Technique\n  with Exponential Noise", "author": "Yuhan Liu and Sheng Wang and Yixuan Liu and Feifei Li and Hong Chen", "abstract": "  The Sparse Vector Technique (SVT) is one of the most fundamental tools in\ndifferential privacy (DP). It works as a backbone for adaptive data analysis by\nanswering a sequence of queries on a given dataset, and gleaning useful\ninformation in a privacy-preserving manner. Unlike the typical private query\nreleases that directly publicize the noisy query results, SVT is less\ninformative -- it keeps the noisy query results to itself and only reveals a\nbinary bit for each query, indicating whether the query result surpasses a\npredefined threshold. To provide a rigorous DP guarantee for SVT, prior works\nin the literature adopt a conservative privacy analysis by assuming the direct\ndisclosure of noisy query results as in typical private query releases. This\napproach, however, hinders SVT from achieving higher query accuracy due to an\noverestimation of the privacy risks, which further leads to an excessive noise\ninjection using the Laplacian or Gaussian noise for perturbation. Motivated by\nthis, we provide a new privacy analysis for SVT by considering its less\ninformative nature. Our analysis results not only broaden the range of\napplicable noise types for perturbation in SVT, but also identify the\nexponential noise as optimal among all evaluated noises (which, however, is\nusually deemed non-applicable in prior works). The main challenge in applying\nexponential noise to SVT is mitigating the sub-optimal performance due to the\nbias introduced by noise distributions. To address this, we develop a\nutility-oriented optimal threshold correction method and an appending strategy,\nwhich enhances the performance of SVT by increasing the precision and recall,\nrespectively. The effectiveness of our proposed methods is substantiated both\ntheoretically and empirically, demonstrating significant improvements up to\n$50\\%$ across evaluated metrics.\n", "link": "http://arxiv.org/abs/2407.20068v1", "date": "2024-07-29", "relevancy": 1.8668, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4759}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4627}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleash%20the%20Power%20of%20Ellipsis%3A%20Accuracy-enhanced%20Sparse%20Vector%20Technique%0A%20%20with%20Exponential%20Noise&body=Title%3A%20Unleash%20the%20Power%20of%20Ellipsis%3A%20Accuracy-enhanced%20Sparse%20Vector%20Technique%0A%20%20with%20Exponential%20Noise%0AAuthor%3A%20Yuhan%20Liu%20and%20Sheng%20Wang%20and%20Yixuan%20Liu%20and%20Feifei%20Li%20and%20Hong%20Chen%0AAbstract%3A%20%20%20The%20Sparse%20Vector%20Technique%20%28SVT%29%20is%20one%20of%20the%20most%20fundamental%20tools%20in%0Adifferential%20privacy%20%28DP%29.%20It%20works%20as%20a%20backbone%20for%20adaptive%20data%20analysis%20by%0Aanswering%20a%20sequence%20of%20queries%20on%20a%20given%20dataset%2C%20and%20gleaning%20useful%0Ainformation%20in%20a%20privacy-preserving%20manner.%20Unlike%20the%20typical%20private%20query%0Areleases%20that%20directly%20publicize%20the%20noisy%20query%20results%2C%20SVT%20is%20less%0Ainformative%20--%20it%20keeps%20the%20noisy%20query%20results%20to%20itself%20and%20only%20reveals%20a%0Abinary%20bit%20for%20each%20query%2C%20indicating%20whether%20the%20query%20result%20surpasses%20a%0Apredefined%20threshold.%20To%20provide%20a%20rigorous%20DP%20guarantee%20for%20SVT%2C%20prior%20works%0Ain%20the%20literature%20adopt%20a%20conservative%20privacy%20analysis%20by%20assuming%20the%20direct%0Adisclosure%20of%20noisy%20query%20results%20as%20in%20typical%20private%20query%20releases.%20This%0Aapproach%2C%20however%2C%20hinders%20SVT%20from%20achieving%20higher%20query%20accuracy%20due%20to%20an%0Aoverestimation%20of%20the%20privacy%20risks%2C%20which%20further%20leads%20to%20an%20excessive%20noise%0Ainjection%20using%20the%20Laplacian%20or%20Gaussian%20noise%20for%20perturbation.%20Motivated%20by%0Athis%2C%20we%20provide%20a%20new%20privacy%20analysis%20for%20SVT%20by%20considering%20its%20less%0Ainformative%20nature.%20Our%20analysis%20results%20not%20only%20broaden%20the%20range%20of%0Aapplicable%20noise%20types%20for%20perturbation%20in%20SVT%2C%20but%20also%20identify%20the%0Aexponential%20noise%20as%20optimal%20among%20all%20evaluated%20noises%20%28which%2C%20however%2C%20is%0Ausually%20deemed%20non-applicable%20in%20prior%20works%29.%20The%20main%20challenge%20in%20applying%0Aexponential%20noise%20to%20SVT%20is%20mitigating%20the%20sub-optimal%20performance%20due%20to%20the%0Abias%20introduced%20by%20noise%20distributions.%20To%20address%20this%2C%20we%20develop%20a%0Autility-oriented%20optimal%20threshold%20correction%20method%20and%20an%20appending%20strategy%2C%0Awhich%20enhances%20the%20performance%20of%20SVT%20by%20increasing%20the%20precision%20and%20recall%2C%0Arespectively.%20The%20effectiveness%20of%20our%20proposed%20methods%20is%20substantiated%20both%0Atheoretically%20and%20empirically%2C%20demonstrating%20significant%20improvements%20up%20to%0A%2450%5C%25%24%20across%20evaluated%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleash%2520the%2520Power%2520of%2520Ellipsis%253A%2520Accuracy-enhanced%2520Sparse%2520Vector%2520Technique%250A%2520%2520with%2520Exponential%2520Noise%26entry.906535625%3DYuhan%2520Liu%2520and%2520Sheng%2520Wang%2520and%2520Yixuan%2520Liu%2520and%2520Feifei%2520Li%2520and%2520Hong%2520Chen%26entry.1292438233%3D%2520%2520The%2520Sparse%2520Vector%2520Technique%2520%2528SVT%2529%2520is%2520one%2520of%2520the%2520most%2520fundamental%2520tools%2520in%250Adifferential%2520privacy%2520%2528DP%2529.%2520It%2520works%2520as%2520a%2520backbone%2520for%2520adaptive%2520data%2520analysis%2520by%250Aanswering%2520a%2520sequence%2520of%2520queries%2520on%2520a%2520given%2520dataset%252C%2520and%2520gleaning%2520useful%250Ainformation%2520in%2520a%2520privacy-preserving%2520manner.%2520Unlike%2520the%2520typical%2520private%2520query%250Areleases%2520that%2520directly%2520publicize%2520the%2520noisy%2520query%2520results%252C%2520SVT%2520is%2520less%250Ainformative%2520--%2520it%2520keeps%2520the%2520noisy%2520query%2520results%2520to%2520itself%2520and%2520only%2520reveals%2520a%250Abinary%2520bit%2520for%2520each%2520query%252C%2520indicating%2520whether%2520the%2520query%2520result%2520surpasses%2520a%250Apredefined%2520threshold.%2520To%2520provide%2520a%2520rigorous%2520DP%2520guarantee%2520for%2520SVT%252C%2520prior%2520works%250Ain%2520the%2520literature%2520adopt%2520a%2520conservative%2520privacy%2520analysis%2520by%2520assuming%2520the%2520direct%250Adisclosure%2520of%2520noisy%2520query%2520results%2520as%2520in%2520typical%2520private%2520query%2520releases.%2520This%250Aapproach%252C%2520however%252C%2520hinders%2520SVT%2520from%2520achieving%2520higher%2520query%2520accuracy%2520due%2520to%2520an%250Aoverestimation%2520of%2520the%2520privacy%2520risks%252C%2520which%2520further%2520leads%2520to%2520an%2520excessive%2520noise%250Ainjection%2520using%2520the%2520Laplacian%2520or%2520Gaussian%2520noise%2520for%2520perturbation.%2520Motivated%2520by%250Athis%252C%2520we%2520provide%2520a%2520new%2520privacy%2520analysis%2520for%2520SVT%2520by%2520considering%2520its%2520less%250Ainformative%2520nature.%2520Our%2520analysis%2520results%2520not%2520only%2520broaden%2520the%2520range%2520of%250Aapplicable%2520noise%2520types%2520for%2520perturbation%2520in%2520SVT%252C%2520but%2520also%2520identify%2520the%250Aexponential%2520noise%2520as%2520optimal%2520among%2520all%2520evaluated%2520noises%2520%2528which%252C%2520however%252C%2520is%250Ausually%2520deemed%2520non-applicable%2520in%2520prior%2520works%2529.%2520The%2520main%2520challenge%2520in%2520applying%250Aexponential%2520noise%2520to%2520SVT%2520is%2520mitigating%2520the%2520sub-optimal%2520performance%2520due%2520to%2520the%250Abias%2520introduced%2520by%2520noise%2520distributions.%2520To%2520address%2520this%252C%2520we%2520develop%2520a%250Autility-oriented%2520optimal%2520threshold%2520correction%2520method%2520and%2520an%2520appending%2520strategy%252C%250Awhich%2520enhances%2520the%2520performance%2520of%2520SVT%2520by%2520increasing%2520the%2520precision%2520and%2520recall%252C%250Arespectively.%2520The%2520effectiveness%2520of%2520our%2520proposed%2520methods%2520is%2520substantiated%2520both%250Atheoretically%2520and%2520empirically%252C%2520demonstrating%2520significant%2520improvements%2520up%2520to%250A%252450%255C%2525%2524%2520across%2520evaluated%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleash%20the%20Power%20of%20Ellipsis%3A%20Accuracy-enhanced%20Sparse%20Vector%20Technique%0A%20%20with%20Exponential%20Noise&entry.906535625=Yuhan%20Liu%20and%20Sheng%20Wang%20and%20Yixuan%20Liu%20and%20Feifei%20Li%20and%20Hong%20Chen&entry.1292438233=%20%20The%20Sparse%20Vector%20Technique%20%28SVT%29%20is%20one%20of%20the%20most%20fundamental%20tools%20in%0Adifferential%20privacy%20%28DP%29.%20It%20works%20as%20a%20backbone%20for%20adaptive%20data%20analysis%20by%0Aanswering%20a%20sequence%20of%20queries%20on%20a%20given%20dataset%2C%20and%20gleaning%20useful%0Ainformation%20in%20a%20privacy-preserving%20manner.%20Unlike%20the%20typical%20private%20query%0Areleases%20that%20directly%20publicize%20the%20noisy%20query%20results%2C%20SVT%20is%20less%0Ainformative%20--%20it%20keeps%20the%20noisy%20query%20results%20to%20itself%20and%20only%20reveals%20a%0Abinary%20bit%20for%20each%20query%2C%20indicating%20whether%20the%20query%20result%20surpasses%20a%0Apredefined%20threshold.%20To%20provide%20a%20rigorous%20DP%20guarantee%20for%20SVT%2C%20prior%20works%0Ain%20the%20literature%20adopt%20a%20conservative%20privacy%20analysis%20by%20assuming%20the%20direct%0Adisclosure%20of%20noisy%20query%20results%20as%20in%20typical%20private%20query%20releases.%20This%0Aapproach%2C%20however%2C%20hinders%20SVT%20from%20achieving%20higher%20query%20accuracy%20due%20to%20an%0Aoverestimation%20of%20the%20privacy%20risks%2C%20which%20further%20leads%20to%20an%20excessive%20noise%0Ainjection%20using%20the%20Laplacian%20or%20Gaussian%20noise%20for%20perturbation.%20Motivated%20by%0Athis%2C%20we%20provide%20a%20new%20privacy%20analysis%20for%20SVT%20by%20considering%20its%20less%0Ainformative%20nature.%20Our%20analysis%20results%20not%20only%20broaden%20the%20range%20of%0Aapplicable%20noise%20types%20for%20perturbation%20in%20SVT%2C%20but%20also%20identify%20the%0Aexponential%20noise%20as%20optimal%20among%20all%20evaluated%20noises%20%28which%2C%20however%2C%20is%0Ausually%20deemed%20non-applicable%20in%20prior%20works%29.%20The%20main%20challenge%20in%20applying%0Aexponential%20noise%20to%20SVT%20is%20mitigating%20the%20sub-optimal%20performance%20due%20to%20the%0Abias%20introduced%20by%20noise%20distributions.%20To%20address%20this%2C%20we%20develop%20a%0Autility-oriented%20optimal%20threshold%20correction%20method%20and%20an%20appending%20strategy%2C%0Awhich%20enhances%20the%20performance%20of%20SVT%20by%20increasing%20the%20precision%20and%20recall%2C%0Arespectively.%20The%20effectiveness%20of%20our%20proposed%20methods%20is%20substantiated%20both%0Atheoretically%20and%20empirically%2C%20demonstrating%20significant%20improvements%20up%20to%0A%2450%5C%25%24%20across%20evaluated%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20068v1&entry.124074799=Read"},
{"title": "Emergence in non-neural models: grokking modular arithmetic via average\n  gradient outer product", "author": "Neil Mallinar and Daniel Beaglehole and Libin Zhu and Adityanarayanan Radhakrishnan and Parthe Pandit and Mikhail Belkin", "abstract": "  Neural networks trained to solve modular arithmetic tasks exhibit grokking, a\nphenomenon where the test accuracy starts improving long after the model\nachieves 100% training accuracy in the training process. It is often taken as\nan example of \"emergence\", where model ability manifests sharply through a\nphase transition. In this work, we show that the phenomenon of grokking is not\nspecific to neural networks nor to gradient descent-based optimization.\nSpecifically, we show that this phenomenon occurs when learning modular\narithmetic with Recursive Feature Machines (RFM), an iterative algorithm that\nuses the Average Gradient Outer Product (AGOP) to enable task-specific feature\nlearning with general machine learning models. When used in conjunction with\nkernel machines, iterating RFM results in a fast transition from random, near\nzero, test accuracy to perfect test accuracy. This transition cannot be\npredicted from the training loss, which is identically zero, nor from the test\nloss, which remains constant in initial iterations. Instead, as we show, the\ntransition is completely determined by feature learning: RFM gradually learns\nblock-circulant features to solve modular arithmetic. Paralleling the results\nfor RFM, we show that neural networks that solve modular arithmetic also learn\nblock-circulant features. Furthermore, we present theoretical evidence that RFM\nuses such block-circulant features to implement the Fourier Multiplication\nAlgorithm, which prior work posited as the generalizing solution neural\nnetworks learn on these tasks. Our results demonstrate that emergence can\nresult purely from learning task-relevant features and is not specific to\nneural architectures nor gradient descent-based optimization methods.\nFurthermore, our work provides more evidence for AGOP as a key mechanism for\nfeature learning in neural networks.\n", "link": "http://arxiv.org/abs/2407.20199v1", "date": "2024-07-29", "relevancy": 1.7511, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4394}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4393}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20in%20non-neural%20models%3A%20grokking%20modular%20arithmetic%20via%20average%0A%20%20gradient%20outer%20product&body=Title%3A%20Emergence%20in%20non-neural%20models%3A%20grokking%20modular%20arithmetic%20via%20average%0A%20%20gradient%20outer%20product%0AAuthor%3A%20Neil%20Mallinar%20and%20Daniel%20Beaglehole%20and%20Libin%20Zhu%20and%20Adityanarayanan%20Radhakrishnan%20and%20Parthe%20Pandit%20and%20Mikhail%20Belkin%0AAbstract%3A%20%20%20Neural%20networks%20trained%20to%20solve%20modular%20arithmetic%20tasks%20exhibit%20grokking%2C%20a%0Aphenomenon%20where%20the%20test%20accuracy%20starts%20improving%20long%20after%20the%20model%0Aachieves%20100%25%20training%20accuracy%20in%20the%20training%20process.%20It%20is%20often%20taken%20as%0Aan%20example%20of%20%22emergence%22%2C%20where%20model%20ability%20manifests%20sharply%20through%20a%0Aphase%20transition.%20In%20this%20work%2C%20we%20show%20that%20the%20phenomenon%20of%20grokking%20is%20not%0Aspecific%20to%20neural%20networks%20nor%20to%20gradient%20descent-based%20optimization.%0ASpecifically%2C%20we%20show%20that%20this%20phenomenon%20occurs%20when%20learning%20modular%0Aarithmetic%20with%20Recursive%20Feature%20Machines%20%28RFM%29%2C%20an%20iterative%20algorithm%20that%0Auses%20the%20Average%20Gradient%20Outer%20Product%20%28AGOP%29%20to%20enable%20task-specific%20feature%0Alearning%20with%20general%20machine%20learning%20models.%20When%20used%20in%20conjunction%20with%0Akernel%20machines%2C%20iterating%20RFM%20results%20in%20a%20fast%20transition%20from%20random%2C%20near%0Azero%2C%20test%20accuracy%20to%20perfect%20test%20accuracy.%20This%20transition%20cannot%20be%0Apredicted%20from%20the%20training%20loss%2C%20which%20is%20identically%20zero%2C%20nor%20from%20the%20test%0Aloss%2C%20which%20remains%20constant%20in%20initial%20iterations.%20Instead%2C%20as%20we%20show%2C%20the%0Atransition%20is%20completely%20determined%20by%20feature%20learning%3A%20RFM%20gradually%20learns%0Ablock-circulant%20features%20to%20solve%20modular%20arithmetic.%20Paralleling%20the%20results%0Afor%20RFM%2C%20we%20show%20that%20neural%20networks%20that%20solve%20modular%20arithmetic%20also%20learn%0Ablock-circulant%20features.%20Furthermore%2C%20we%20present%20theoretical%20evidence%20that%20RFM%0Auses%20such%20block-circulant%20features%20to%20implement%20the%20Fourier%20Multiplication%0AAlgorithm%2C%20which%20prior%20work%20posited%20as%20the%20generalizing%20solution%20neural%0Anetworks%20learn%20on%20these%20tasks.%20Our%20results%20demonstrate%20that%20emergence%20can%0Aresult%20purely%20from%20learning%20task-relevant%20features%20and%20is%20not%20specific%20to%0Aneural%20architectures%20nor%20gradient%20descent-based%20optimization%20methods.%0AFurthermore%2C%20our%20work%20provides%20more%20evidence%20for%20AGOP%20as%20a%20key%20mechanism%20for%0Afeature%20learning%20in%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520in%2520non-neural%2520models%253A%2520grokking%2520modular%2520arithmetic%2520via%2520average%250A%2520%2520gradient%2520outer%2520product%26entry.906535625%3DNeil%2520Mallinar%2520and%2520Daniel%2520Beaglehole%2520and%2520Libin%2520Zhu%2520and%2520Adityanarayanan%2520Radhakrishnan%2520and%2520Parthe%2520Pandit%2520and%2520Mikhail%2520Belkin%26entry.1292438233%3D%2520%2520Neural%2520networks%2520trained%2520to%2520solve%2520modular%2520arithmetic%2520tasks%2520exhibit%2520grokking%252C%2520a%250Aphenomenon%2520where%2520the%2520test%2520accuracy%2520starts%2520improving%2520long%2520after%2520the%2520model%250Aachieves%2520100%2525%2520training%2520accuracy%2520in%2520the%2520training%2520process.%2520It%2520is%2520often%2520taken%2520as%250Aan%2520example%2520of%2520%2522emergence%2522%252C%2520where%2520model%2520ability%2520manifests%2520sharply%2520through%2520a%250Aphase%2520transition.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520the%2520phenomenon%2520of%2520grokking%2520is%2520not%250Aspecific%2520to%2520neural%2520networks%2520nor%2520to%2520gradient%2520descent-based%2520optimization.%250ASpecifically%252C%2520we%2520show%2520that%2520this%2520phenomenon%2520occurs%2520when%2520learning%2520modular%250Aarithmetic%2520with%2520Recursive%2520Feature%2520Machines%2520%2528RFM%2529%252C%2520an%2520iterative%2520algorithm%2520that%250Auses%2520the%2520Average%2520Gradient%2520Outer%2520Product%2520%2528AGOP%2529%2520to%2520enable%2520task-specific%2520feature%250Alearning%2520with%2520general%2520machine%2520learning%2520models.%2520When%2520used%2520in%2520conjunction%2520with%250Akernel%2520machines%252C%2520iterating%2520RFM%2520results%2520in%2520a%2520fast%2520transition%2520from%2520random%252C%2520near%250Azero%252C%2520test%2520accuracy%2520to%2520perfect%2520test%2520accuracy.%2520This%2520transition%2520cannot%2520be%250Apredicted%2520from%2520the%2520training%2520loss%252C%2520which%2520is%2520identically%2520zero%252C%2520nor%2520from%2520the%2520test%250Aloss%252C%2520which%2520remains%2520constant%2520in%2520initial%2520iterations.%2520Instead%252C%2520as%2520we%2520show%252C%2520the%250Atransition%2520is%2520completely%2520determined%2520by%2520feature%2520learning%253A%2520RFM%2520gradually%2520learns%250Ablock-circulant%2520features%2520to%2520solve%2520modular%2520arithmetic.%2520Paralleling%2520the%2520results%250Afor%2520RFM%252C%2520we%2520show%2520that%2520neural%2520networks%2520that%2520solve%2520modular%2520arithmetic%2520also%2520learn%250Ablock-circulant%2520features.%2520Furthermore%252C%2520we%2520present%2520theoretical%2520evidence%2520that%2520RFM%250Auses%2520such%2520block-circulant%2520features%2520to%2520implement%2520the%2520Fourier%2520Multiplication%250AAlgorithm%252C%2520which%2520prior%2520work%2520posited%2520as%2520the%2520generalizing%2520solution%2520neural%250Anetworks%2520learn%2520on%2520these%2520tasks.%2520Our%2520results%2520demonstrate%2520that%2520emergence%2520can%250Aresult%2520purely%2520from%2520learning%2520task-relevant%2520features%2520and%2520is%2520not%2520specific%2520to%250Aneural%2520architectures%2520nor%2520gradient%2520descent-based%2520optimization%2520methods.%250AFurthermore%252C%2520our%2520work%2520provides%2520more%2520evidence%2520for%2520AGOP%2520as%2520a%2520key%2520mechanism%2520for%250Afeature%2520learning%2520in%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20in%20non-neural%20models%3A%20grokking%20modular%20arithmetic%20via%20average%0A%20%20gradient%20outer%20product&entry.906535625=Neil%20Mallinar%20and%20Daniel%20Beaglehole%20and%20Libin%20Zhu%20and%20Adityanarayanan%20Radhakrishnan%20and%20Parthe%20Pandit%20and%20Mikhail%20Belkin&entry.1292438233=%20%20Neural%20networks%20trained%20to%20solve%20modular%20arithmetic%20tasks%20exhibit%20grokking%2C%20a%0Aphenomenon%20where%20the%20test%20accuracy%20starts%20improving%20long%20after%20the%20model%0Aachieves%20100%25%20training%20accuracy%20in%20the%20training%20process.%20It%20is%20often%20taken%20as%0Aan%20example%20of%20%22emergence%22%2C%20where%20model%20ability%20manifests%20sharply%20through%20a%0Aphase%20transition.%20In%20this%20work%2C%20we%20show%20that%20the%20phenomenon%20of%20grokking%20is%20not%0Aspecific%20to%20neural%20networks%20nor%20to%20gradient%20descent-based%20optimization.%0ASpecifically%2C%20we%20show%20that%20this%20phenomenon%20occurs%20when%20learning%20modular%0Aarithmetic%20with%20Recursive%20Feature%20Machines%20%28RFM%29%2C%20an%20iterative%20algorithm%20that%0Auses%20the%20Average%20Gradient%20Outer%20Product%20%28AGOP%29%20to%20enable%20task-specific%20feature%0Alearning%20with%20general%20machine%20learning%20models.%20When%20used%20in%20conjunction%20with%0Akernel%20machines%2C%20iterating%20RFM%20results%20in%20a%20fast%20transition%20from%20random%2C%20near%0Azero%2C%20test%20accuracy%20to%20perfect%20test%20accuracy.%20This%20transition%20cannot%20be%0Apredicted%20from%20the%20training%20loss%2C%20which%20is%20identically%20zero%2C%20nor%20from%20the%20test%0Aloss%2C%20which%20remains%20constant%20in%20initial%20iterations.%20Instead%2C%20as%20we%20show%2C%20the%0Atransition%20is%20completely%20determined%20by%20feature%20learning%3A%20RFM%20gradually%20learns%0Ablock-circulant%20features%20to%20solve%20modular%20arithmetic.%20Paralleling%20the%20results%0Afor%20RFM%2C%20we%20show%20that%20neural%20networks%20that%20solve%20modular%20arithmetic%20also%20learn%0Ablock-circulant%20features.%20Furthermore%2C%20we%20present%20theoretical%20evidence%20that%20RFM%0Auses%20such%20block-circulant%20features%20to%20implement%20the%20Fourier%20Multiplication%0AAlgorithm%2C%20which%20prior%20work%20posited%20as%20the%20generalizing%20solution%20neural%0Anetworks%20learn%20on%20these%20tasks.%20Our%20results%20demonstrate%20that%20emergence%20can%0Aresult%20purely%20from%20learning%20task-relevant%20features%20and%20is%20not%20specific%20to%0Aneural%20architectures%20nor%20gradient%20descent-based%20optimization%20methods.%0AFurthermore%2C%20our%20work%20provides%20more%20evidence%20for%20AGOP%20as%20a%20key%20mechanism%20for%0Afeature%20learning%20in%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20199v1&entry.124074799=Read"},
{"title": "On the Effects of Irrelevant Variables in Treatment Effect Estimation\n  with Deep Disentanglement", "author": "Ahmad Saeed Khan and Erik Schaffernicht and Johannes Andreas Stork", "abstract": "  Estimating treatment effects from observational data is paramount in\nhealthcare, education, and economics, but current deep disentanglement-based\nmethods to address selection bias are insufficiently handling irrelevant\nvariables. We demonstrate in experiments that this leads to prediction errors.\nWe disentangle pre-treatment variables with a deep embedding method and\nexplicitly identify and represent irrelevant variables, additionally to\ninstrumental, confounding and adjustment latent factors. To this end, we\nintroduce a reconstruction objective and create an embedding space for\nirrelevant variables using an attached autoencoder. Instead of relying on\nserendipitous suppression of irrelevant variables as in previous deep\ndisentanglement approaches, we explicitly force irrelevant variables into this\nembedding space and employ orthogonalization to prevent irrelevant information\nfrom leaking into the latent space representations of the other factors. Our\nexperiments with synthetic and real-world benchmark datasets show that we can\nbetter identify irrelevant variables and more precisely predict treatment\neffects than previous methods, while prediction quality degrades less when\nadditional irrelevant variables are introduced.\n", "link": "http://arxiv.org/abs/2407.20003v1", "date": "2024-07-29", "relevancy": 1.8063, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4995}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4438}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Effects%20of%20Irrelevant%20Variables%20in%20Treatment%20Effect%20Estimation%0A%20%20with%20Deep%20Disentanglement&body=Title%3A%20On%20the%20Effects%20of%20Irrelevant%20Variables%20in%20Treatment%20Effect%20Estimation%0A%20%20with%20Deep%20Disentanglement%0AAuthor%3A%20Ahmad%20Saeed%20Khan%20and%20Erik%20Schaffernicht%20and%20Johannes%20Andreas%20Stork%0AAbstract%3A%20%20%20Estimating%20treatment%20effects%20from%20observational%20data%20is%20paramount%20in%0Ahealthcare%2C%20education%2C%20and%20economics%2C%20but%20current%20deep%20disentanglement-based%0Amethods%20to%20address%20selection%20bias%20are%20insufficiently%20handling%20irrelevant%0Avariables.%20We%20demonstrate%20in%20experiments%20that%20this%20leads%20to%20prediction%20errors.%0AWe%20disentangle%20pre-treatment%20variables%20with%20a%20deep%20embedding%20method%20and%0Aexplicitly%20identify%20and%20represent%20irrelevant%20variables%2C%20additionally%20to%0Ainstrumental%2C%20confounding%20and%20adjustment%20latent%20factors.%20To%20this%20end%2C%20we%0Aintroduce%20a%20reconstruction%20objective%20and%20create%20an%20embedding%20space%20for%0Airrelevant%20variables%20using%20an%20attached%20autoencoder.%20Instead%20of%20relying%20on%0Aserendipitous%20suppression%20of%20irrelevant%20variables%20as%20in%20previous%20deep%0Adisentanglement%20approaches%2C%20we%20explicitly%20force%20irrelevant%20variables%20into%20this%0Aembedding%20space%20and%20employ%20orthogonalization%20to%20prevent%20irrelevant%20information%0Afrom%20leaking%20into%20the%20latent%20space%20representations%20of%20the%20other%20factors.%20Our%0Aexperiments%20with%20synthetic%20and%20real-world%20benchmark%20datasets%20show%20that%20we%20can%0Abetter%20identify%20irrelevant%20variables%20and%20more%20precisely%20predict%20treatment%0Aeffects%20than%20previous%20methods%2C%20while%20prediction%20quality%20degrades%20less%20when%0Aadditional%20irrelevant%20variables%20are%20introduced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Effects%2520of%2520Irrelevant%2520Variables%2520in%2520Treatment%2520Effect%2520Estimation%250A%2520%2520with%2520Deep%2520Disentanglement%26entry.906535625%3DAhmad%2520Saeed%2520Khan%2520and%2520Erik%2520Schaffernicht%2520and%2520Johannes%2520Andreas%2520Stork%26entry.1292438233%3D%2520%2520Estimating%2520treatment%2520effects%2520from%2520observational%2520data%2520is%2520paramount%2520in%250Ahealthcare%252C%2520education%252C%2520and%2520economics%252C%2520but%2520current%2520deep%2520disentanglement-based%250Amethods%2520to%2520address%2520selection%2520bias%2520are%2520insufficiently%2520handling%2520irrelevant%250Avariables.%2520We%2520demonstrate%2520in%2520experiments%2520that%2520this%2520leads%2520to%2520prediction%2520errors.%250AWe%2520disentangle%2520pre-treatment%2520variables%2520with%2520a%2520deep%2520embedding%2520method%2520and%250Aexplicitly%2520identify%2520and%2520represent%2520irrelevant%2520variables%252C%2520additionally%2520to%250Ainstrumental%252C%2520confounding%2520and%2520adjustment%2520latent%2520factors.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520reconstruction%2520objective%2520and%2520create%2520an%2520embedding%2520space%2520for%250Airrelevant%2520variables%2520using%2520an%2520attached%2520autoencoder.%2520Instead%2520of%2520relying%2520on%250Aserendipitous%2520suppression%2520of%2520irrelevant%2520variables%2520as%2520in%2520previous%2520deep%250Adisentanglement%2520approaches%252C%2520we%2520explicitly%2520force%2520irrelevant%2520variables%2520into%2520this%250Aembedding%2520space%2520and%2520employ%2520orthogonalization%2520to%2520prevent%2520irrelevant%2520information%250Afrom%2520leaking%2520into%2520the%2520latent%2520space%2520representations%2520of%2520the%2520other%2520factors.%2520Our%250Aexperiments%2520with%2520synthetic%2520and%2520real-world%2520benchmark%2520datasets%2520show%2520that%2520we%2520can%250Abetter%2520identify%2520irrelevant%2520variables%2520and%2520more%2520precisely%2520predict%2520treatment%250Aeffects%2520than%2520previous%2520methods%252C%2520while%2520prediction%2520quality%2520degrades%2520less%2520when%250Aadditional%2520irrelevant%2520variables%2520are%2520introduced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effects%20of%20Irrelevant%20Variables%20in%20Treatment%20Effect%20Estimation%0A%20%20with%20Deep%20Disentanglement&entry.906535625=Ahmad%20Saeed%20Khan%20and%20Erik%20Schaffernicht%20and%20Johannes%20Andreas%20Stork&entry.1292438233=%20%20Estimating%20treatment%20effects%20from%20observational%20data%20is%20paramount%20in%0Ahealthcare%2C%20education%2C%20and%20economics%2C%20but%20current%20deep%20disentanglement-based%0Amethods%20to%20address%20selection%20bias%20are%20insufficiently%20handling%20irrelevant%0Avariables.%20We%20demonstrate%20in%20experiments%20that%20this%20leads%20to%20prediction%20errors.%0AWe%20disentangle%20pre-treatment%20variables%20with%20a%20deep%20embedding%20method%20and%0Aexplicitly%20identify%20and%20represent%20irrelevant%20variables%2C%20additionally%20to%0Ainstrumental%2C%20confounding%20and%20adjustment%20latent%20factors.%20To%20this%20end%2C%20we%0Aintroduce%20a%20reconstruction%20objective%20and%20create%20an%20embedding%20space%20for%0Airrelevant%20variables%20using%20an%20attached%20autoencoder.%20Instead%20of%20relying%20on%0Aserendipitous%20suppression%20of%20irrelevant%20variables%20as%20in%20previous%20deep%0Adisentanglement%20approaches%2C%20we%20explicitly%20force%20irrelevant%20variables%20into%20this%0Aembedding%20space%20and%20employ%20orthogonalization%20to%20prevent%20irrelevant%20information%0Afrom%20leaking%20into%20the%20latent%20space%20representations%20of%20the%20other%20factors.%20Our%0Aexperiments%20with%20synthetic%20and%20real-world%20benchmark%20datasets%20show%20that%20we%20can%0Abetter%20identify%20irrelevant%20variables%20and%20more%20precisely%20predict%20treatment%0Aeffects%20than%20previous%20methods%2C%20while%20prediction%20quality%20degrades%20less%20when%0Aadditional%20irrelevant%20variables%20are%20introduced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20003v1&entry.124074799=Read"},
{"title": "Anomalous State Sequence Modeling to Enhance Safety in Reinforcement\n  Learning", "author": "Leen Kweider and Maissa Abou Kassem and Ubai Sandouk", "abstract": "  The deployment of artificial intelligence (AI) in decision-making\napplications requires ensuring an appropriate level of safety and reliability,\nparticularly in changing environments that contain a large number of unknown\nobservations. To address this challenge, we propose a novel safe reinforcement\nlearning (RL) approach that utilizes an anomalous state sequence to enhance RL\nsafety. Our proposed solution Safe Reinforcement Learning with Anomalous State\nSequences (AnoSeqs) consists of two stages. First, we train an agent in a\nnon-safety-critical offline 'source' environment to collect safe state\nsequences. Next, we use these safe sequences to build an anomaly detection\nmodel that can detect potentially unsafe state sequences in a 'target'\nsafety-critical environment where failures can have high costs. The estimated\nrisk from the anomaly detection model is utilized to train a risk-averse RL\npolicy in the target environment; this involves adjusting the reward function\nto penalize the agent for visiting anomalous states deemed unsafe by our\nanomaly model. In experiments on multiple safety-critical benchmarking\nenvironments including self-driving cars, our solution approach successfully\nlearns safer policies and proves that sequential anomaly detection can provide\nan effective supervisory signal for training safety-aware RL agents\n", "link": "http://arxiv.org/abs/2407.19860v1", "date": "2024-07-29", "relevancy": 1.6484, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5542}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5463}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomalous%20State%20Sequence%20Modeling%20to%20Enhance%20Safety%20in%20Reinforcement%0A%20%20Learning&body=Title%3A%20Anomalous%20State%20Sequence%20Modeling%20to%20Enhance%20Safety%20in%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Leen%20Kweider%20and%20Maissa%20Abou%20Kassem%20and%20Ubai%20Sandouk%0AAbstract%3A%20%20%20The%20deployment%20of%20artificial%20intelligence%20%28AI%29%20in%20decision-making%0Aapplications%20requires%20ensuring%20an%20appropriate%20level%20of%20safety%20and%20reliability%2C%0Aparticularly%20in%20changing%20environments%20that%20contain%20a%20large%20number%20of%20unknown%0Aobservations.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20safe%20reinforcement%0Alearning%20%28RL%29%20approach%20that%20utilizes%20an%20anomalous%20state%20sequence%20to%20enhance%20RL%0Asafety.%20Our%20proposed%20solution%20Safe%20Reinforcement%20Learning%20with%20Anomalous%20State%0ASequences%20%28AnoSeqs%29%20consists%20of%20two%20stages.%20First%2C%20we%20train%20an%20agent%20in%20a%0Anon-safety-critical%20offline%20%27source%27%20environment%20to%20collect%20safe%20state%0Asequences.%20Next%2C%20we%20use%20these%20safe%20sequences%20to%20build%20an%20anomaly%20detection%0Amodel%20that%20can%20detect%20potentially%20unsafe%20state%20sequences%20in%20a%20%27target%27%0Asafety-critical%20environment%20where%20failures%20can%20have%20high%20costs.%20The%20estimated%0Arisk%20from%20the%20anomaly%20detection%20model%20is%20utilized%20to%20train%20a%20risk-averse%20RL%0Apolicy%20in%20the%20target%20environment%3B%20this%20involves%20adjusting%20the%20reward%20function%0Ato%20penalize%20the%20agent%20for%20visiting%20anomalous%20states%20deemed%20unsafe%20by%20our%0Aanomaly%20model.%20In%20experiments%20on%20multiple%20safety-critical%20benchmarking%0Aenvironments%20including%20self-driving%20cars%2C%20our%20solution%20approach%20successfully%0Alearns%20safer%20policies%20and%20proves%20that%20sequential%20anomaly%20detection%20can%20provide%0Aan%20effective%20supervisory%20signal%20for%20training%20safety-aware%20RL%20agents%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalous%2520State%2520Sequence%2520Modeling%2520to%2520Enhance%2520Safety%2520in%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DLeen%2520Kweider%2520and%2520Maissa%2520Abou%2520Kassem%2520and%2520Ubai%2520Sandouk%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520in%2520decision-making%250Aapplications%2520requires%2520ensuring%2520an%2520appropriate%2520level%2520of%2520safety%2520and%2520reliability%252C%250Aparticularly%2520in%2520changing%2520environments%2520that%2520contain%2520a%2520large%2520number%2520of%2520unknown%250Aobservations.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520safe%2520reinforcement%250Alearning%2520%2528RL%2529%2520approach%2520that%2520utilizes%2520an%2520anomalous%2520state%2520sequence%2520to%2520enhance%2520RL%250Asafety.%2520Our%2520proposed%2520solution%2520Safe%2520Reinforcement%2520Learning%2520with%2520Anomalous%2520State%250ASequences%2520%2528AnoSeqs%2529%2520consists%2520of%2520two%2520stages.%2520First%252C%2520we%2520train%2520an%2520agent%2520in%2520a%250Anon-safety-critical%2520offline%2520%2527source%2527%2520environment%2520to%2520collect%2520safe%2520state%250Asequences.%2520Next%252C%2520we%2520use%2520these%2520safe%2520sequences%2520to%2520build%2520an%2520anomaly%2520detection%250Amodel%2520that%2520can%2520detect%2520potentially%2520unsafe%2520state%2520sequences%2520in%2520a%2520%2527target%2527%250Asafety-critical%2520environment%2520where%2520failures%2520can%2520have%2520high%2520costs.%2520The%2520estimated%250Arisk%2520from%2520the%2520anomaly%2520detection%2520model%2520is%2520utilized%2520to%2520train%2520a%2520risk-averse%2520RL%250Apolicy%2520in%2520the%2520target%2520environment%253B%2520this%2520involves%2520adjusting%2520the%2520reward%2520function%250Ato%2520penalize%2520the%2520agent%2520for%2520visiting%2520anomalous%2520states%2520deemed%2520unsafe%2520by%2520our%250Aanomaly%2520model.%2520In%2520experiments%2520on%2520multiple%2520safety-critical%2520benchmarking%250Aenvironments%2520including%2520self-driving%2520cars%252C%2520our%2520solution%2520approach%2520successfully%250Alearns%2520safer%2520policies%2520and%2520proves%2520that%2520sequential%2520anomaly%2520detection%2520can%2520provide%250Aan%2520effective%2520supervisory%2520signal%2520for%2520training%2520safety-aware%2520RL%2520agents%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomalous%20State%20Sequence%20Modeling%20to%20Enhance%20Safety%20in%20Reinforcement%0A%20%20Learning&entry.906535625=Leen%20Kweider%20and%20Maissa%20Abou%20Kassem%20and%20Ubai%20Sandouk&entry.1292438233=%20%20The%20deployment%20of%20artificial%20intelligence%20%28AI%29%20in%20decision-making%0Aapplications%20requires%20ensuring%20an%20appropriate%20level%20of%20safety%20and%20reliability%2C%0Aparticularly%20in%20changing%20environments%20that%20contain%20a%20large%20number%20of%20unknown%0Aobservations.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20safe%20reinforcement%0Alearning%20%28RL%29%20approach%20that%20utilizes%20an%20anomalous%20state%20sequence%20to%20enhance%20RL%0Asafety.%20Our%20proposed%20solution%20Safe%20Reinforcement%20Learning%20with%20Anomalous%20State%0ASequences%20%28AnoSeqs%29%20consists%20of%20two%20stages.%20First%2C%20we%20train%20an%20agent%20in%20a%0Anon-safety-critical%20offline%20%27source%27%20environment%20to%20collect%20safe%20state%0Asequences.%20Next%2C%20we%20use%20these%20safe%20sequences%20to%20build%20an%20anomaly%20detection%0Amodel%20that%20can%20detect%20potentially%20unsafe%20state%20sequences%20in%20a%20%27target%27%0Asafety-critical%20environment%20where%20failures%20can%20have%20high%20costs.%20The%20estimated%0Arisk%20from%20the%20anomaly%20detection%20model%20is%20utilized%20to%20train%20a%20risk-averse%20RL%0Apolicy%20in%20the%20target%20environment%3B%20this%20involves%20adjusting%20the%20reward%20function%0Ato%20penalize%20the%20agent%20for%20visiting%20anomalous%20states%20deemed%20unsafe%20by%20our%0Aanomaly%20model.%20In%20experiments%20on%20multiple%20safety-critical%20benchmarking%0Aenvironments%20including%20self-driving%20cars%2C%20our%20solution%20approach%20successfully%0Alearns%20safer%20policies%20and%20proves%20that%20sequential%20anomaly%20detection%20can%20provide%0Aan%20effective%20supervisory%20signal%20for%20training%20safety-aware%20RL%20agents%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19860v1&entry.124074799=Read"},
{"title": "Prompt Leakage effect and defense strategies for multi-turn LLM\n  interactions", "author": "Divyansh Agarwal and Alexander R. Fabbri and Ben Risher and Philippe Laban and Shafiq Joty and Chien-Sheng Wu", "abstract": "  Prompt leakage poses a compelling security and privacy threat in LLM\napplications. Leakage of system prompts may compromise intellectual property,\nand act as adversarial reconnaissance for an attacker. A systematic evaluation\nof prompt leakage threats and mitigation strategies is lacking, especially for\nmulti-turn LLM interactions. In this paper, we systematically investigate LLM\nvulnerabilities against prompt leakage for 10 closed- and open-source LLMs,\nacross four domains. We design a unique threat model which leverages the LLM\nsycophancy effect and elevates the average attack success rate (ASR) from 17.7%\nto 86.2% in a multi-turn setting. Our standardized setup further allows\ndissecting leakage of specific prompt contents such as task instructions and\nknowledge documents. We measure the mitigation effect of 7 black-box defense\nstrategies, along with finetuning an open-source model to defend against\nleakage attempts. We present different combination of defenses against our\nthreat model, including a cost analysis. Our study highlights key takeaways for\nbuilding secure LLM applications and provides directions for research in\nmulti-turn LLM interactions\n", "link": "http://arxiv.org/abs/2404.16251v3", "date": "2024-07-29", "relevancy": 1.591, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4077}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3993}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Leakage%20effect%20and%20defense%20strategies%20for%20multi-turn%20LLM%0A%20%20interactions&body=Title%3A%20Prompt%20Leakage%20effect%20and%20defense%20strategies%20for%20multi-turn%20LLM%0A%20%20interactions%0AAuthor%3A%20Divyansh%20Agarwal%20and%20Alexander%20R.%20Fabbri%20and%20Ben%20Risher%20and%20Philippe%20Laban%20and%20Shafiq%20Joty%20and%20Chien-Sheng%20Wu%0AAbstract%3A%20%20%20Prompt%20leakage%20poses%20a%20compelling%20security%20and%20privacy%20threat%20in%20LLM%0Aapplications.%20Leakage%20of%20system%20prompts%20may%20compromise%20intellectual%20property%2C%0Aand%20act%20as%20adversarial%20reconnaissance%20for%20an%20attacker.%20A%20systematic%20evaluation%0Aof%20prompt%20leakage%20threats%20and%20mitigation%20strategies%20is%20lacking%2C%20especially%20for%0Amulti-turn%20LLM%20interactions.%20In%20this%20paper%2C%20we%20systematically%20investigate%20LLM%0Avulnerabilities%20against%20prompt%20leakage%20for%2010%20closed-%20and%20open-source%20LLMs%2C%0Aacross%20four%20domains.%20We%20design%20a%20unique%20threat%20model%20which%20leverages%20the%20LLM%0Asycophancy%20effect%20and%20elevates%20the%20average%20attack%20success%20rate%20%28ASR%29%20from%2017.7%25%0Ato%2086.2%25%20in%20a%20multi-turn%20setting.%20Our%20standardized%20setup%20further%20allows%0Adissecting%20leakage%20of%20specific%20prompt%20contents%20such%20as%20task%20instructions%20and%0Aknowledge%20documents.%20We%20measure%20the%20mitigation%20effect%20of%207%20black-box%20defense%0Astrategies%2C%20along%20with%20finetuning%20an%20open-source%20model%20to%20defend%20against%0Aleakage%20attempts.%20We%20present%20different%20combination%20of%20defenses%20against%20our%0Athreat%20model%2C%20including%20a%20cost%20analysis.%20Our%20study%20highlights%20key%20takeaways%20for%0Abuilding%20secure%20LLM%20applications%20and%20provides%20directions%20for%20research%20in%0Amulti-turn%20LLM%20interactions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16251v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Leakage%2520effect%2520and%2520defense%2520strategies%2520for%2520multi-turn%2520LLM%250A%2520%2520interactions%26entry.906535625%3DDivyansh%2520Agarwal%2520and%2520Alexander%2520R.%2520Fabbri%2520and%2520Ben%2520Risher%2520and%2520Philippe%2520Laban%2520and%2520Shafiq%2520Joty%2520and%2520Chien-Sheng%2520Wu%26entry.1292438233%3D%2520%2520Prompt%2520leakage%2520poses%2520a%2520compelling%2520security%2520and%2520privacy%2520threat%2520in%2520LLM%250Aapplications.%2520Leakage%2520of%2520system%2520prompts%2520may%2520compromise%2520intellectual%2520property%252C%250Aand%2520act%2520as%2520adversarial%2520reconnaissance%2520for%2520an%2520attacker.%2520A%2520systematic%2520evaluation%250Aof%2520prompt%2520leakage%2520threats%2520and%2520mitigation%2520strategies%2520is%2520lacking%252C%2520especially%2520for%250Amulti-turn%2520LLM%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520investigate%2520LLM%250Avulnerabilities%2520against%2520prompt%2520leakage%2520for%252010%2520closed-%2520and%2520open-source%2520LLMs%252C%250Aacross%2520four%2520domains.%2520We%2520design%2520a%2520unique%2520threat%2520model%2520which%2520leverages%2520the%2520LLM%250Asycophancy%2520effect%2520and%2520elevates%2520the%2520average%2520attack%2520success%2520rate%2520%2528ASR%2529%2520from%252017.7%2525%250Ato%252086.2%2525%2520in%2520a%2520multi-turn%2520setting.%2520Our%2520standardized%2520setup%2520further%2520allows%250Adissecting%2520leakage%2520of%2520specific%2520prompt%2520contents%2520such%2520as%2520task%2520instructions%2520and%250Aknowledge%2520documents.%2520We%2520measure%2520the%2520mitigation%2520effect%2520of%25207%2520black-box%2520defense%250Astrategies%252C%2520along%2520with%2520finetuning%2520an%2520open-source%2520model%2520to%2520defend%2520against%250Aleakage%2520attempts.%2520We%2520present%2520different%2520combination%2520of%2520defenses%2520against%2520our%250Athreat%2520model%252C%2520including%2520a%2520cost%2520analysis.%2520Our%2520study%2520highlights%2520key%2520takeaways%2520for%250Abuilding%2520secure%2520LLM%2520applications%2520and%2520provides%2520directions%2520for%2520research%2520in%250Amulti-turn%2520LLM%2520interactions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16251v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Leakage%20effect%20and%20defense%20strategies%20for%20multi-turn%20LLM%0A%20%20interactions&entry.906535625=Divyansh%20Agarwal%20and%20Alexander%20R.%20Fabbri%20and%20Ben%20Risher%20and%20Philippe%20Laban%20and%20Shafiq%20Joty%20and%20Chien-Sheng%20Wu&entry.1292438233=%20%20Prompt%20leakage%20poses%20a%20compelling%20security%20and%20privacy%20threat%20in%20LLM%0Aapplications.%20Leakage%20of%20system%20prompts%20may%20compromise%20intellectual%20property%2C%0Aand%20act%20as%20adversarial%20reconnaissance%20for%20an%20attacker.%20A%20systematic%20evaluation%0Aof%20prompt%20leakage%20threats%20and%20mitigation%20strategies%20is%20lacking%2C%20especially%20for%0Amulti-turn%20LLM%20interactions.%20In%20this%20paper%2C%20we%20systematically%20investigate%20LLM%0Avulnerabilities%20against%20prompt%20leakage%20for%2010%20closed-%20and%20open-source%20LLMs%2C%0Aacross%20four%20domains.%20We%20design%20a%20unique%20threat%20model%20which%20leverages%20the%20LLM%0Asycophancy%20effect%20and%20elevates%20the%20average%20attack%20success%20rate%20%28ASR%29%20from%2017.7%25%0Ato%2086.2%25%20in%20a%20multi-turn%20setting.%20Our%20standardized%20setup%20further%20allows%0Adissecting%20leakage%20of%20specific%20prompt%20contents%20such%20as%20task%20instructions%20and%0Aknowledge%20documents.%20We%20measure%20the%20mitigation%20effect%20of%207%20black-box%20defense%0Astrategies%2C%20along%20with%20finetuning%20an%20open-source%20model%20to%20defend%20against%0Aleakage%20attempts.%20We%20present%20different%20combination%20of%20defenses%20against%20our%0Athreat%20model%2C%20including%20a%20cost%20analysis.%20Our%20study%20highlights%20key%20takeaways%20for%0Abuilding%20secure%20LLM%20applications%20and%20provides%20directions%20for%20research%20in%0Amulti-turn%20LLM%20interactions%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16251v3&entry.124074799=Read"},
{"title": "AOTree: Aspect Order Tree-based Model for Explainable Recommendation", "author": "Wenxin Zhao and Peng Zhang and Hansu Gu and Dongsheng Li and Tun Lu and Ning Gu", "abstract": "  Recent recommender systems aim to provide not only accurate recommendations\nbut also explanations that help users understand them better. However, most\nexisting explainable recommendations only consider the importance of content in\nreviews, such as words or aspects, and ignore the ordering relationship among\nthem. This oversight neglects crucial ordering dimensions in the human\ndecision-making process, leading to suboptimal performance. Therefore, in this\npaper, we propose Aspect Order Tree-based (AOTree) explainable recommendation\nmethod, inspired by the Order Effects Theory from cognitive and decision\npsychology, in order to capture the dependency relationships among decisive\nfactors. We first validate the theory in the recommendation scenario by\nanalyzing the reviews of the users. Then, according to the theory, the proposed\nAOTree expands the construction of the decision tree to capture aspect orders\nin users' decision-making processes, and use attention mechanisms to make\npredictions based on the aspect orders. Extensive experiments demonstrate our\nmethod's effectiveness on rating predictions, and our approach aligns more\nconsistently with the user' s decision-making process by displaying\nexplanations in a particular order, thereby enhancing interpretability.\n", "link": "http://arxiv.org/abs/2407.19937v1", "date": "2024-07-29", "relevancy": 0.85, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4266}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4248}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AOTree%3A%20Aspect%20Order%20Tree-based%20Model%20for%20Explainable%20Recommendation&body=Title%3A%20AOTree%3A%20Aspect%20Order%20Tree-based%20Model%20for%20Explainable%20Recommendation%0AAuthor%3A%20Wenxin%20Zhao%20and%20Peng%20Zhang%20and%20Hansu%20Gu%20and%20Dongsheng%20Li%20and%20Tun%20Lu%20and%20Ning%20Gu%0AAbstract%3A%20%20%20Recent%20recommender%20systems%20aim%20to%20provide%20not%20only%20accurate%20recommendations%0Abut%20also%20explanations%20that%20help%20users%20understand%20them%20better.%20However%2C%20most%0Aexisting%20explainable%20recommendations%20only%20consider%20the%20importance%20of%20content%20in%0Areviews%2C%20such%20as%20words%20or%20aspects%2C%20and%20ignore%20the%20ordering%20relationship%20among%0Athem.%20This%20oversight%20neglects%20crucial%20ordering%20dimensions%20in%20the%20human%0Adecision-making%20process%2C%20leading%20to%20suboptimal%20performance.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20propose%20Aspect%20Order%20Tree-based%20%28AOTree%29%20explainable%20recommendation%0Amethod%2C%20inspired%20by%20the%20Order%20Effects%20Theory%20from%20cognitive%20and%20decision%0Apsychology%2C%20in%20order%20to%20capture%20the%20dependency%20relationships%20among%20decisive%0Afactors.%20We%20first%20validate%20the%20theory%20in%20the%20recommendation%20scenario%20by%0Aanalyzing%20the%20reviews%20of%20the%20users.%20Then%2C%20according%20to%20the%20theory%2C%20the%20proposed%0AAOTree%20expands%20the%20construction%20of%20the%20decision%20tree%20to%20capture%20aspect%20orders%0Ain%20users%27%20decision-making%20processes%2C%20and%20use%20attention%20mechanisms%20to%20make%0Apredictions%20based%20on%20the%20aspect%20orders.%20Extensive%20experiments%20demonstrate%20our%0Amethod%27s%20effectiveness%20on%20rating%20predictions%2C%20and%20our%20approach%20aligns%20more%0Aconsistently%20with%20the%20user%27%20s%20decision-making%20process%20by%20displaying%0Aexplanations%20in%20a%20particular%20order%2C%20thereby%20enhancing%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAOTree%253A%2520Aspect%2520Order%2520Tree-based%2520Model%2520for%2520Explainable%2520Recommendation%26entry.906535625%3DWenxin%2520Zhao%2520and%2520Peng%2520Zhang%2520and%2520Hansu%2520Gu%2520and%2520Dongsheng%2520Li%2520and%2520Tun%2520Lu%2520and%2520Ning%2520Gu%26entry.1292438233%3D%2520%2520Recent%2520recommender%2520systems%2520aim%2520to%2520provide%2520not%2520only%2520accurate%2520recommendations%250Abut%2520also%2520explanations%2520that%2520help%2520users%2520understand%2520them%2520better.%2520However%252C%2520most%250Aexisting%2520explainable%2520recommendations%2520only%2520consider%2520the%2520importance%2520of%2520content%2520in%250Areviews%252C%2520such%2520as%2520words%2520or%2520aspects%252C%2520and%2520ignore%2520the%2520ordering%2520relationship%2520among%250Athem.%2520This%2520oversight%2520neglects%2520crucial%2520ordering%2520dimensions%2520in%2520the%2520human%250Adecision-making%2520process%252C%2520leading%2520to%2520suboptimal%2520performance.%2520Therefore%252C%2520in%2520this%250Apaper%252C%2520we%2520propose%2520Aspect%2520Order%2520Tree-based%2520%2528AOTree%2529%2520explainable%2520recommendation%250Amethod%252C%2520inspired%2520by%2520the%2520Order%2520Effects%2520Theory%2520from%2520cognitive%2520and%2520decision%250Apsychology%252C%2520in%2520order%2520to%2520capture%2520the%2520dependency%2520relationships%2520among%2520decisive%250Afactors.%2520We%2520first%2520validate%2520the%2520theory%2520in%2520the%2520recommendation%2520scenario%2520by%250Aanalyzing%2520the%2520reviews%2520of%2520the%2520users.%2520Then%252C%2520according%2520to%2520the%2520theory%252C%2520the%2520proposed%250AAOTree%2520expands%2520the%2520construction%2520of%2520the%2520decision%2520tree%2520to%2520capture%2520aspect%2520orders%250Ain%2520users%2527%2520decision-making%2520processes%252C%2520and%2520use%2520attention%2520mechanisms%2520to%2520make%250Apredictions%2520based%2520on%2520the%2520aspect%2520orders.%2520Extensive%2520experiments%2520demonstrate%2520our%250Amethod%2527s%2520effectiveness%2520on%2520rating%2520predictions%252C%2520and%2520our%2520approach%2520aligns%2520more%250Aconsistently%2520with%2520the%2520user%2527%2520s%2520decision-making%2520process%2520by%2520displaying%250Aexplanations%2520in%2520a%2520particular%2520order%252C%2520thereby%2520enhancing%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AOTree%3A%20Aspect%20Order%20Tree-based%20Model%20for%20Explainable%20Recommendation&entry.906535625=Wenxin%20Zhao%20and%20Peng%20Zhang%20and%20Hansu%20Gu%20and%20Dongsheng%20Li%20and%20Tun%20Lu%20and%20Ning%20Gu&entry.1292438233=%20%20Recent%20recommender%20systems%20aim%20to%20provide%20not%20only%20accurate%20recommendations%0Abut%20also%20explanations%20that%20help%20users%20understand%20them%20better.%20However%2C%20most%0Aexisting%20explainable%20recommendations%20only%20consider%20the%20importance%20of%20content%20in%0Areviews%2C%20such%20as%20words%20or%20aspects%2C%20and%20ignore%20the%20ordering%20relationship%20among%0Athem.%20This%20oversight%20neglects%20crucial%20ordering%20dimensions%20in%20the%20human%0Adecision-making%20process%2C%20leading%20to%20suboptimal%20performance.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20propose%20Aspect%20Order%20Tree-based%20%28AOTree%29%20explainable%20recommendation%0Amethod%2C%20inspired%20by%20the%20Order%20Effects%20Theory%20from%20cognitive%20and%20decision%0Apsychology%2C%20in%20order%20to%20capture%20the%20dependency%20relationships%20among%20decisive%0Afactors.%20We%20first%20validate%20the%20theory%20in%20the%20recommendation%20scenario%20by%0Aanalyzing%20the%20reviews%20of%20the%20users.%20Then%2C%20according%20to%20the%20theory%2C%20the%20proposed%0AAOTree%20expands%20the%20construction%20of%20the%20decision%20tree%20to%20capture%20aspect%20orders%0Ain%20users%27%20decision-making%20processes%2C%20and%20use%20attention%20mechanisms%20to%20make%0Apredictions%20based%20on%20the%20aspect%20orders.%20Extensive%20experiments%20demonstrate%20our%0Amethod%27s%20effectiveness%20on%20rating%20predictions%2C%20and%20our%20approach%20aligns%20more%0Aconsistently%20with%20the%20user%27%20s%20decision-making%20process%20by%20displaying%0Aexplanations%20in%20a%20particular%20order%2C%20thereby%20enhancing%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19937v1&entry.124074799=Read"},
{"title": "Yucca: A Deep Learning Framework For Medical Image Analysis", "author": "Sebastian N\u00f8rgaard Llambias and Julia Machnio and Asbj\u00f8rn Munk and Jakob Ambsdorf and Mads Nielsen and Mostafa Mehdipour Ghazi", "abstract": "  Medical image analysis using deep learning frameworks has advanced healthcare\nby automating complex tasks, but many existing frameworks lack flexibility,\nmodularity, and user-friendliness. To address these challenges, we introduce\nYucca, an open-source AI framework available at\nhttps://github.com/Sllambias/yucca, designed specifically for medical imaging\napplications and built on PyTorch and PyTorch Lightning. Yucca features a\nthree-tiered architecture: Functional, Modules, and Pipeline, providing a\ncomprehensive and customizable solution. Evaluated across diverse tasks such as\ncerebral microbleeds detection, white matter hyperintensity segmentation, and\nhippocampus segmentation, Yucca achieves state-of-the-art results,\ndemonstrating its robustness and versatility. Yucca offers a powerful,\nflexible, and user-friendly platform for medical image analysis, inviting\ncommunity contributions to advance its capabilities and impact.\n", "link": "http://arxiv.org/abs/2407.19888v1", "date": "2024-07-29", "relevancy": 1.3555, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4698}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yucca%3A%20A%20Deep%20Learning%20Framework%20For%20Medical%20Image%20Analysis&body=Title%3A%20Yucca%3A%20A%20Deep%20Learning%20Framework%20For%20Medical%20Image%20Analysis%0AAuthor%3A%20Sebastian%20N%C3%B8rgaard%20Llambias%20and%20Julia%20Machnio%20and%20Asbj%C3%B8rn%20Munk%20and%20Jakob%20Ambsdorf%20and%20Mads%20Nielsen%20and%20Mostafa%20Mehdipour%20Ghazi%0AAbstract%3A%20%20%20Medical%20image%20analysis%20using%20deep%20learning%20frameworks%20has%20advanced%20healthcare%0Aby%20automating%20complex%20tasks%2C%20but%20many%20existing%20frameworks%20lack%20flexibility%2C%0Amodularity%2C%20and%20user-friendliness.%20To%20address%20these%20challenges%2C%20we%20introduce%0AYucca%2C%20an%20open-source%20AI%20framework%20available%20at%0Ahttps%3A//github.com/Sllambias/yucca%2C%20designed%20specifically%20for%20medical%20imaging%0Aapplications%20and%20built%20on%20PyTorch%20and%20PyTorch%20Lightning.%20Yucca%20features%20a%0Athree-tiered%20architecture%3A%20Functional%2C%20Modules%2C%20and%20Pipeline%2C%20providing%20a%0Acomprehensive%20and%20customizable%20solution.%20Evaluated%20across%20diverse%20tasks%20such%20as%0Acerebral%20microbleeds%20detection%2C%20white%20matter%20hyperintensity%20segmentation%2C%20and%0Ahippocampus%20segmentation%2C%20Yucca%20achieves%20state-of-the-art%20results%2C%0Ademonstrating%20its%20robustness%20and%20versatility.%20Yucca%20offers%20a%20powerful%2C%0Aflexible%2C%20and%20user-friendly%20platform%20for%20medical%20image%20analysis%2C%20inviting%0Acommunity%20contributions%20to%20advance%20its%20capabilities%20and%20impact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYucca%253A%2520A%2520Deep%2520Learning%2520Framework%2520For%2520Medical%2520Image%2520Analysis%26entry.906535625%3DSebastian%2520N%25C3%25B8rgaard%2520Llambias%2520and%2520Julia%2520Machnio%2520and%2520Asbj%25C3%25B8rn%2520Munk%2520and%2520Jakob%2520Ambsdorf%2520and%2520Mads%2520Nielsen%2520and%2520Mostafa%2520Mehdipour%2520Ghazi%26entry.1292438233%3D%2520%2520Medical%2520image%2520analysis%2520using%2520deep%2520learning%2520frameworks%2520has%2520advanced%2520healthcare%250Aby%2520automating%2520complex%2520tasks%252C%2520but%2520many%2520existing%2520frameworks%2520lack%2520flexibility%252C%250Amodularity%252C%2520and%2520user-friendliness.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AYucca%252C%2520an%2520open-source%2520AI%2520framework%2520available%2520at%250Ahttps%253A//github.com/Sllambias/yucca%252C%2520designed%2520specifically%2520for%2520medical%2520imaging%250Aapplications%2520and%2520built%2520on%2520PyTorch%2520and%2520PyTorch%2520Lightning.%2520Yucca%2520features%2520a%250Athree-tiered%2520architecture%253A%2520Functional%252C%2520Modules%252C%2520and%2520Pipeline%252C%2520providing%2520a%250Acomprehensive%2520and%2520customizable%2520solution.%2520Evaluated%2520across%2520diverse%2520tasks%2520such%2520as%250Acerebral%2520microbleeds%2520detection%252C%2520white%2520matter%2520hyperintensity%2520segmentation%252C%2520and%250Ahippocampus%2520segmentation%252C%2520Yucca%2520achieves%2520state-of-the-art%2520results%252C%250Ademonstrating%2520its%2520robustness%2520and%2520versatility.%2520Yucca%2520offers%2520a%2520powerful%252C%250Aflexible%252C%2520and%2520user-friendly%2520platform%2520for%2520medical%2520image%2520analysis%252C%2520inviting%250Acommunity%2520contributions%2520to%2520advance%2520its%2520capabilities%2520and%2520impact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yucca%3A%20A%20Deep%20Learning%20Framework%20For%20Medical%20Image%20Analysis&entry.906535625=Sebastian%20N%C3%B8rgaard%20Llambias%20and%20Julia%20Machnio%20and%20Asbj%C3%B8rn%20Munk%20and%20Jakob%20Ambsdorf%20and%20Mads%20Nielsen%20and%20Mostafa%20Mehdipour%20Ghazi&entry.1292438233=%20%20Medical%20image%20analysis%20using%20deep%20learning%20frameworks%20has%20advanced%20healthcare%0Aby%20automating%20complex%20tasks%2C%20but%20many%20existing%20frameworks%20lack%20flexibility%2C%0Amodularity%2C%20and%20user-friendliness.%20To%20address%20these%20challenges%2C%20we%20introduce%0AYucca%2C%20an%20open-source%20AI%20framework%20available%20at%0Ahttps%3A//github.com/Sllambias/yucca%2C%20designed%20specifically%20for%20medical%20imaging%0Aapplications%20and%20built%20on%20PyTorch%20and%20PyTorch%20Lightning.%20Yucca%20features%20a%0Athree-tiered%20architecture%3A%20Functional%2C%20Modules%2C%20and%20Pipeline%2C%20providing%20a%0Acomprehensive%20and%20customizable%20solution.%20Evaluated%20across%20diverse%20tasks%20such%20as%0Acerebral%20microbleeds%20detection%2C%20white%20matter%20hyperintensity%20segmentation%2C%20and%0Ahippocampus%20segmentation%2C%20Yucca%20achieves%20state-of-the-art%20results%2C%0Ademonstrating%20its%20robustness%20and%20versatility.%20Yucca%20offers%20a%20powerful%2C%0Aflexible%2C%20and%20user-friendly%20platform%20for%20medical%20image%20analysis%2C%20inviting%0Acommunity%20contributions%20to%20advance%20its%20capabilities%20and%20impact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19888v1&entry.124074799=Read"},
{"title": "Practical and Robust Safety Guarantees for Advanced Counterfactual\n  Learning to Rank", "author": "Shashank Gupta and Harrie Oosterhuis and Maarten de Rijke", "abstract": "  Counterfactual learning to rank (CLTR ) can be risky; various circumstances\ncan cause it to produce sub-optimal models that hurt performance when deployed.\nSafe CLTR was introduced to mitigate these risks when using inverse propensity\nscoring to correct for position bias. However, the existing safety measure for\nCLTR is not applicable to state-of-the-art CLTR, it cannot handle trust bias,\nand its guarantees rely on specific assumptions about user behavior. Our\ncontributions are two-fold. First, we generalize the existing safe CLTR\napproach to make it applicable to state-of-the-art doubly robust (DR) CLTR and\ntrust bias. Second, we propose a novel approach, proximal ranking policy\noptimization (PRPO ), that provides safety in deployment without assumptions\nabout user behavior. PRPO removes incentives for learning ranking behavior that\nis too dissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how\nmuch learned models can degrade performance metrics, without relying on any\nspecific user assumptions. Our experiments show that both our novel safe doubly\nrobust method and PRPO provide higher performance than the existing safe\ninverse propensity scoring approach. However, when circumstances are\nunexpected, the safe doubly robust approach can become unsafe and bring\ndetrimental performance. In contrast, PRPO always maintains safety, even in\nmaximally adversarial situations. By avoiding assumptions, PRPO is the first\nmethod with unconditional safety in deployment that translates to robust safety\nfor real-world applications.\n", "link": "http://arxiv.org/abs/2407.19943v1", "date": "2024-07-29", "relevancy": 1.4562, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Practical%20and%20Robust%20Safety%20Guarantees%20for%20Advanced%20Counterfactual%0A%20%20Learning%20to%20Rank&body=Title%3A%20Practical%20and%20Robust%20Safety%20Guarantees%20for%20Advanced%20Counterfactual%0A%20%20Learning%20to%20Rank%0AAuthor%3A%20Shashank%20Gupta%20and%20Harrie%20Oosterhuis%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20%20%20Counterfactual%20learning%20to%20rank%20%28CLTR%20%29%20can%20be%20risky%3B%20various%20circumstances%0Acan%20cause%20it%20to%20produce%20sub-optimal%20models%20that%20hurt%20performance%20when%20deployed.%0ASafe%20CLTR%20was%20introduced%20to%20mitigate%20these%20risks%20when%20using%20inverse%20propensity%0Ascoring%20to%20correct%20for%20position%20bias.%20However%2C%20the%20existing%20safety%20measure%20for%0ACLTR%20is%20not%20applicable%20to%20state-of-the-art%20CLTR%2C%20it%20cannot%20handle%20trust%20bias%2C%0Aand%20its%20guarantees%20rely%20on%20specific%20assumptions%20about%20user%20behavior.%20Our%0Acontributions%20are%20two-fold.%20First%2C%20we%20generalize%20the%20existing%20safe%20CLTR%0Aapproach%20to%20make%20it%20applicable%20to%20state-of-the-art%20doubly%20robust%20%28DR%29%20CLTR%20and%0Atrust%20bias.%20Second%2C%20we%20propose%20a%20novel%20approach%2C%20proximal%20ranking%20policy%0Aoptimization%20%28PRPO%20%29%2C%20that%20provides%20safety%20in%20deployment%20without%20assumptions%0Aabout%20user%20behavior.%20PRPO%20removes%20incentives%20for%20learning%20ranking%20behavior%20that%0Ais%20too%20dissimilar%20to%20a%20safe%20ranking%20model.%20Thereby%2C%20PRPO%20imposes%20a%20limit%20on%20how%0Amuch%20learned%20models%20can%20degrade%20performance%20metrics%2C%20without%20relying%20on%20any%0Aspecific%20user%20assumptions.%20Our%20experiments%20show%20that%20both%20our%20novel%20safe%20doubly%0Arobust%20method%20and%20PRPO%20provide%20higher%20performance%20than%20the%20existing%20safe%0Ainverse%20propensity%20scoring%20approach.%20However%2C%20when%20circumstances%20are%0Aunexpected%2C%20the%20safe%20doubly%20robust%20approach%20can%20become%20unsafe%20and%20bring%0Adetrimental%20performance.%20In%20contrast%2C%20PRPO%20always%20maintains%20safety%2C%20even%20in%0Amaximally%20adversarial%20situations.%20By%20avoiding%20assumptions%2C%20PRPO%20is%20the%20first%0Amethod%20with%20unconditional%20safety%20in%20deployment%20that%20translates%20to%20robust%20safety%0Afor%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPractical%2520and%2520Robust%2520Safety%2520Guarantees%2520for%2520Advanced%2520Counterfactual%250A%2520%2520Learning%2520to%2520Rank%26entry.906535625%3DShashank%2520Gupta%2520and%2520Harrie%2520Oosterhuis%2520and%2520Maarten%2520de%2520Rijke%26entry.1292438233%3D%2520%2520Counterfactual%2520learning%2520to%2520rank%2520%2528CLTR%2520%2529%2520can%2520be%2520risky%253B%2520various%2520circumstances%250Acan%2520cause%2520it%2520to%2520produce%2520sub-optimal%2520models%2520that%2520hurt%2520performance%2520when%2520deployed.%250ASafe%2520CLTR%2520was%2520introduced%2520to%2520mitigate%2520these%2520risks%2520when%2520using%2520inverse%2520propensity%250Ascoring%2520to%2520correct%2520for%2520position%2520bias.%2520However%252C%2520the%2520existing%2520safety%2520measure%2520for%250ACLTR%2520is%2520not%2520applicable%2520to%2520state-of-the-art%2520CLTR%252C%2520it%2520cannot%2520handle%2520trust%2520bias%252C%250Aand%2520its%2520guarantees%2520rely%2520on%2520specific%2520assumptions%2520about%2520user%2520behavior.%2520Our%250Acontributions%2520are%2520two-fold.%2520First%252C%2520we%2520generalize%2520the%2520existing%2520safe%2520CLTR%250Aapproach%2520to%2520make%2520it%2520applicable%2520to%2520state-of-the-art%2520doubly%2520robust%2520%2528DR%2529%2520CLTR%2520and%250Atrust%2520bias.%2520Second%252C%2520we%2520propose%2520a%2520novel%2520approach%252C%2520proximal%2520ranking%2520policy%250Aoptimization%2520%2528PRPO%2520%2529%252C%2520that%2520provides%2520safety%2520in%2520deployment%2520without%2520assumptions%250Aabout%2520user%2520behavior.%2520PRPO%2520removes%2520incentives%2520for%2520learning%2520ranking%2520behavior%2520that%250Ais%2520too%2520dissimilar%2520to%2520a%2520safe%2520ranking%2520model.%2520Thereby%252C%2520PRPO%2520imposes%2520a%2520limit%2520on%2520how%250Amuch%2520learned%2520models%2520can%2520degrade%2520performance%2520metrics%252C%2520without%2520relying%2520on%2520any%250Aspecific%2520user%2520assumptions.%2520Our%2520experiments%2520show%2520that%2520both%2520our%2520novel%2520safe%2520doubly%250Arobust%2520method%2520and%2520PRPO%2520provide%2520higher%2520performance%2520than%2520the%2520existing%2520safe%250Ainverse%2520propensity%2520scoring%2520approach.%2520However%252C%2520when%2520circumstances%2520are%250Aunexpected%252C%2520the%2520safe%2520doubly%2520robust%2520approach%2520can%2520become%2520unsafe%2520and%2520bring%250Adetrimental%2520performance.%2520In%2520contrast%252C%2520PRPO%2520always%2520maintains%2520safety%252C%2520even%2520in%250Amaximally%2520adversarial%2520situations.%2520By%2520avoiding%2520assumptions%252C%2520PRPO%2520is%2520the%2520first%250Amethod%2520with%2520unconditional%2520safety%2520in%2520deployment%2520that%2520translates%2520to%2520robust%2520safety%250Afor%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Practical%20and%20Robust%20Safety%20Guarantees%20for%20Advanced%20Counterfactual%0A%20%20Learning%20to%20Rank&entry.906535625=Shashank%20Gupta%20and%20Harrie%20Oosterhuis%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20Counterfactual%20learning%20to%20rank%20%28CLTR%20%29%20can%20be%20risky%3B%20various%20circumstances%0Acan%20cause%20it%20to%20produce%20sub-optimal%20models%20that%20hurt%20performance%20when%20deployed.%0ASafe%20CLTR%20was%20introduced%20to%20mitigate%20these%20risks%20when%20using%20inverse%20propensity%0Ascoring%20to%20correct%20for%20position%20bias.%20However%2C%20the%20existing%20safety%20measure%20for%0ACLTR%20is%20not%20applicable%20to%20state-of-the-art%20CLTR%2C%20it%20cannot%20handle%20trust%20bias%2C%0Aand%20its%20guarantees%20rely%20on%20specific%20assumptions%20about%20user%20behavior.%20Our%0Acontributions%20are%20two-fold.%20First%2C%20we%20generalize%20the%20existing%20safe%20CLTR%0Aapproach%20to%20make%20it%20applicable%20to%20state-of-the-art%20doubly%20robust%20%28DR%29%20CLTR%20and%0Atrust%20bias.%20Second%2C%20we%20propose%20a%20novel%20approach%2C%20proximal%20ranking%20policy%0Aoptimization%20%28PRPO%20%29%2C%20that%20provides%20safety%20in%20deployment%20without%20assumptions%0Aabout%20user%20behavior.%20PRPO%20removes%20incentives%20for%20learning%20ranking%20behavior%20that%0Ais%20too%20dissimilar%20to%20a%20safe%20ranking%20model.%20Thereby%2C%20PRPO%20imposes%20a%20limit%20on%20how%0Amuch%20learned%20models%20can%20degrade%20performance%20metrics%2C%20without%20relying%20on%20any%0Aspecific%20user%20assumptions.%20Our%20experiments%20show%20that%20both%20our%20novel%20safe%20doubly%0Arobust%20method%20and%20PRPO%20provide%20higher%20performance%20than%20the%20existing%20safe%0Ainverse%20propensity%20scoring%20approach.%20However%2C%20when%20circumstances%20are%0Aunexpected%2C%20the%20safe%20doubly%20robust%20approach%20can%20become%20unsafe%20and%20bring%0Adetrimental%20performance.%20In%20contrast%2C%20PRPO%20always%20maintains%20safety%2C%20even%20in%0Amaximally%20adversarial%20situations.%20By%20avoiding%20assumptions%2C%20PRPO%20is%20the%20first%0Amethod%20with%20unconditional%20safety%20in%20deployment%20that%20translates%20to%20robust%20safety%0Afor%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19943v1&entry.124074799=Read"},
{"title": "Invariance of deep image quality metrics to affine transformations", "author": "Nuria Alabau-Bosque and Paula Daud\u00e9n-Oliver and Jorge Vila-Tom\u00e1s and Valero Laparra and Jes\u00fas Malo", "abstract": "  Deep architectures are the current state-of-the-art in predicting subjective\nimage quality. Usually, these models are evaluated according to their ability\nto correlate with human opinion in databases with a range of distortions that\nmay appear in digital media. However, these oversee affine transformations\nwhich may represent better the changes in the images actually happening in\nnatural conditions. Humans can be particularly invariant to these natural\ntransformations, as opposed to the digital ones. In this work, we evaluate\nstate-of-the-art deep image quality metrics by assessing their invariance to\naffine transformations, specifically: rotation, translation, scaling, and\nchanges in spectral illumination. Here invariance of a metric refers to the\nfact that certain distances should be neglected (considered to be zero) if\ntheir values are below a threshold. This is what we call invisibility threshold\nof a metric. We propose a methodology to assign such invisibility thresholds\nfor any perceptual metric. This methodology involves transformations to a\ndistance space common to any metric, and psychophysical measurements of\nthresholds in this common space. By doing so, we allow the analyzed metrics to\nbe directly comparable with actual human thresholds. We find that none of the\nstate-of-the-art metrics shows human-like results under this strong test based\non invisibility thresholds. This means that tuning the models exclusively to\npredict the visibility of generic distortions may disregard other properties of\nhuman vision as for instance invariances or invisibility thresholds.\n", "link": "http://arxiv.org/abs/2407.17927v2", "date": "2024-07-29", "relevancy": 1.6329, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5562}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5465}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariance%20of%20deep%20image%20quality%20metrics%20to%20affine%20transformations&body=Title%3A%20Invariance%20of%20deep%20image%20quality%20metrics%20to%20affine%20transformations%0AAuthor%3A%20Nuria%20Alabau-Bosque%20and%20Paula%20Daud%C3%A9n-Oliver%20and%20Jorge%20Vila-Tom%C3%A1s%20and%20Valero%20Laparra%20and%20Jes%C3%BAs%20Malo%0AAbstract%3A%20%20%20Deep%20architectures%20are%20the%20current%20state-of-the-art%20in%20predicting%20subjective%0Aimage%20quality.%20Usually%2C%20these%20models%20are%20evaluated%20according%20to%20their%20ability%0Ato%20correlate%20with%20human%20opinion%20in%20databases%20with%20a%20range%20of%20distortions%20that%0Amay%20appear%20in%20digital%20media.%20However%2C%20these%20oversee%20affine%20transformations%0Awhich%20may%20represent%20better%20the%20changes%20in%20the%20images%20actually%20happening%20in%0Anatural%20conditions.%20Humans%20can%20be%20particularly%20invariant%20to%20these%20natural%0Atransformations%2C%20as%20opposed%20to%20the%20digital%20ones.%20In%20this%20work%2C%20we%20evaluate%0Astate-of-the-art%20deep%20image%20quality%20metrics%20by%20assessing%20their%20invariance%20to%0Aaffine%20transformations%2C%20specifically%3A%20rotation%2C%20translation%2C%20scaling%2C%20and%0Achanges%20in%20spectral%20illumination.%20Here%20invariance%20of%20a%20metric%20refers%20to%20the%0Afact%20that%20certain%20distances%20should%20be%20neglected%20%28considered%20to%20be%20zero%29%20if%0Atheir%20values%20are%20below%20a%20threshold.%20This%20is%20what%20we%20call%20invisibility%20threshold%0Aof%20a%20metric.%20We%20propose%20a%20methodology%20to%20assign%20such%20invisibility%20thresholds%0Afor%20any%20perceptual%20metric.%20This%20methodology%20involves%20transformations%20to%20a%0Adistance%20space%20common%20to%20any%20metric%2C%20and%20psychophysical%20measurements%20of%0Athresholds%20in%20this%20common%20space.%20By%20doing%20so%2C%20we%20allow%20the%20analyzed%20metrics%20to%0Abe%20directly%20comparable%20with%20actual%20human%20thresholds.%20We%20find%20that%20none%20of%20the%0Astate-of-the-art%20metrics%20shows%20human-like%20results%20under%20this%20strong%20test%20based%0Aon%20invisibility%20thresholds.%20This%20means%20that%20tuning%20the%20models%20exclusively%20to%0Apredict%20the%20visibility%20of%20generic%20distortions%20may%20disregard%20other%20properties%20of%0Ahuman%20vision%20as%20for%20instance%20invariances%20or%20invisibility%20thresholds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariance%2520of%2520deep%2520image%2520quality%2520metrics%2520to%2520affine%2520transformations%26entry.906535625%3DNuria%2520Alabau-Bosque%2520and%2520Paula%2520Daud%25C3%25A9n-Oliver%2520and%2520Jorge%2520Vila-Tom%25C3%25A1s%2520and%2520Valero%2520Laparra%2520and%2520Jes%25C3%25BAs%2520Malo%26entry.1292438233%3D%2520%2520Deep%2520architectures%2520are%2520the%2520current%2520state-of-the-art%2520in%2520predicting%2520subjective%250Aimage%2520quality.%2520Usually%252C%2520these%2520models%2520are%2520evaluated%2520according%2520to%2520their%2520ability%250Ato%2520correlate%2520with%2520human%2520opinion%2520in%2520databases%2520with%2520a%2520range%2520of%2520distortions%2520that%250Amay%2520appear%2520in%2520digital%2520media.%2520However%252C%2520these%2520oversee%2520affine%2520transformations%250Awhich%2520may%2520represent%2520better%2520the%2520changes%2520in%2520the%2520images%2520actually%2520happening%2520in%250Anatural%2520conditions.%2520Humans%2520can%2520be%2520particularly%2520invariant%2520to%2520these%2520natural%250Atransformations%252C%2520as%2520opposed%2520to%2520the%2520digital%2520ones.%2520In%2520this%2520work%252C%2520we%2520evaluate%250Astate-of-the-art%2520deep%2520image%2520quality%2520metrics%2520by%2520assessing%2520their%2520invariance%2520to%250Aaffine%2520transformations%252C%2520specifically%253A%2520rotation%252C%2520translation%252C%2520scaling%252C%2520and%250Achanges%2520in%2520spectral%2520illumination.%2520Here%2520invariance%2520of%2520a%2520metric%2520refers%2520to%2520the%250Afact%2520that%2520certain%2520distances%2520should%2520be%2520neglected%2520%2528considered%2520to%2520be%2520zero%2529%2520if%250Atheir%2520values%2520are%2520below%2520a%2520threshold.%2520This%2520is%2520what%2520we%2520call%2520invisibility%2520threshold%250Aof%2520a%2520metric.%2520We%2520propose%2520a%2520methodology%2520to%2520assign%2520such%2520invisibility%2520thresholds%250Afor%2520any%2520perceptual%2520metric.%2520This%2520methodology%2520involves%2520transformations%2520to%2520a%250Adistance%2520space%2520common%2520to%2520any%2520metric%252C%2520and%2520psychophysical%2520measurements%2520of%250Athresholds%2520in%2520this%2520common%2520space.%2520By%2520doing%2520so%252C%2520we%2520allow%2520the%2520analyzed%2520metrics%2520to%250Abe%2520directly%2520comparable%2520with%2520actual%2520human%2520thresholds.%2520We%2520find%2520that%2520none%2520of%2520the%250Astate-of-the-art%2520metrics%2520shows%2520human-like%2520results%2520under%2520this%2520strong%2520test%2520based%250Aon%2520invisibility%2520thresholds.%2520This%2520means%2520that%2520tuning%2520the%2520models%2520exclusively%2520to%250Apredict%2520the%2520visibility%2520of%2520generic%2520distortions%2520may%2520disregard%2520other%2520properties%2520of%250Ahuman%2520vision%2520as%2520for%2520instance%2520invariances%2520or%2520invisibility%2520thresholds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariance%20of%20deep%20image%20quality%20metrics%20to%20affine%20transformations&entry.906535625=Nuria%20Alabau-Bosque%20and%20Paula%20Daud%C3%A9n-Oliver%20and%20Jorge%20Vila-Tom%C3%A1s%20and%20Valero%20Laparra%20and%20Jes%C3%BAs%20Malo&entry.1292438233=%20%20Deep%20architectures%20are%20the%20current%20state-of-the-art%20in%20predicting%20subjective%0Aimage%20quality.%20Usually%2C%20these%20models%20are%20evaluated%20according%20to%20their%20ability%0Ato%20correlate%20with%20human%20opinion%20in%20databases%20with%20a%20range%20of%20distortions%20that%0Amay%20appear%20in%20digital%20media.%20However%2C%20these%20oversee%20affine%20transformations%0Awhich%20may%20represent%20better%20the%20changes%20in%20the%20images%20actually%20happening%20in%0Anatural%20conditions.%20Humans%20can%20be%20particularly%20invariant%20to%20these%20natural%0Atransformations%2C%20as%20opposed%20to%20the%20digital%20ones.%20In%20this%20work%2C%20we%20evaluate%0Astate-of-the-art%20deep%20image%20quality%20metrics%20by%20assessing%20their%20invariance%20to%0Aaffine%20transformations%2C%20specifically%3A%20rotation%2C%20translation%2C%20scaling%2C%20and%0Achanges%20in%20spectral%20illumination.%20Here%20invariance%20of%20a%20metric%20refers%20to%20the%0Afact%20that%20certain%20distances%20should%20be%20neglected%20%28considered%20to%20be%20zero%29%20if%0Atheir%20values%20are%20below%20a%20threshold.%20This%20is%20what%20we%20call%20invisibility%20threshold%0Aof%20a%20metric.%20We%20propose%20a%20methodology%20to%20assign%20such%20invisibility%20thresholds%0Afor%20any%20perceptual%20metric.%20This%20methodology%20involves%20transformations%20to%20a%0Adistance%20space%20common%20to%20any%20metric%2C%20and%20psychophysical%20measurements%20of%0Athresholds%20in%20this%20common%20space.%20By%20doing%20so%2C%20we%20allow%20the%20analyzed%20metrics%20to%0Abe%20directly%20comparable%20with%20actual%20human%20thresholds.%20We%20find%20that%20none%20of%20the%0Astate-of-the-art%20metrics%20shows%20human-like%20results%20under%20this%20strong%20test%20based%0Aon%20invisibility%20thresholds.%20This%20means%20that%20tuning%20the%20models%20exclusively%20to%0Apredict%20the%20visibility%20of%20generic%20distortions%20may%20disregard%20other%20properties%20of%0Ahuman%20vision%20as%20for%20instance%20invariances%20or%20invisibility%20thresholds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17927v2&entry.124074799=Read"},
{"title": "MSegRNN:Enhanced SegRNN Model with Mamba for Long-Term Time Series\n  Forecasting", "author": "GaoXiang Zhao and Li Zhou and XiaoQiang Wang", "abstract": "  Long time series forecasting aims to utilize historical information to\nforecast future states over extended horizons. Traditional RNN-based series\nforecasting methods struggle to effectively address long-term dependencies and\ngradient issues in long time series problems. Recently, SegRNN has emerged as a\nleading RNN-based model tailored for long-term series forecasting,\ndemonstrating state-of-the-art performance while maintaining a streamlined\narchitecture through innovative segmentation and parallel decoding techniques.\nNevertheless, SegRNN has several limitations: its fixed segmentation disrupts\ndata continuity and fails to effectively leverage information across different\nsegments, the segmentation strategy employed by SegRNN does not fundamentally\naddress the issue of information loss within the recurrent structure. To\naddress these issues, we propose the MSegRNN method with three key\nenhancements: we introduce an implicit segmentation structure to decompose the\ntime series and map it to segmented hidden states, resulting in denser\ninformation exchange during the segmentation phase. Additionally, we\nincorporate residual structures in the encoding layer to mitigate information\nloss within the recurrent structure. To extract information more effectively,\nwe further integrate the Mamba architecture to enhance time series information\nextraction. Experiments on several real-world long time series forecasting\ndatasets demonstrate that our model surpasses the performance of current\nstate-of-the-art models.\n", "link": "http://arxiv.org/abs/2407.10768v3", "date": "2024-07-29", "relevancy": 1.7994, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4629}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4418}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSegRNN%3AEnhanced%20SegRNN%20Model%20with%20Mamba%20for%20Long-Term%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20MSegRNN%3AEnhanced%20SegRNN%20Model%20with%20Mamba%20for%20Long-Term%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20GaoXiang%20Zhao%20and%20Li%20Zhou%20and%20XiaoQiang%20Wang%0AAbstract%3A%20%20%20Long%20time%20series%20forecasting%20aims%20to%20utilize%20historical%20information%20to%0Aforecast%20future%20states%20over%20extended%20horizons.%20Traditional%20RNN-based%20series%0Aforecasting%20methods%20struggle%20to%20effectively%20address%20long-term%20dependencies%20and%0Agradient%20issues%20in%20long%20time%20series%20problems.%20Recently%2C%20SegRNN%20has%20emerged%20as%20a%0Aleading%20RNN-based%20model%20tailored%20for%20long-term%20series%20forecasting%2C%0Ademonstrating%20state-of-the-art%20performance%20while%20maintaining%20a%20streamlined%0Aarchitecture%20through%20innovative%20segmentation%20and%20parallel%20decoding%20techniques.%0ANevertheless%2C%20SegRNN%20has%20several%20limitations%3A%20its%20fixed%20segmentation%20disrupts%0Adata%20continuity%20and%20fails%20to%20effectively%20leverage%20information%20across%20different%0Asegments%2C%20the%20segmentation%20strategy%20employed%20by%20SegRNN%20does%20not%20fundamentally%0Aaddress%20the%20issue%20of%20information%20loss%20within%20the%20recurrent%20structure.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20MSegRNN%20method%20with%20three%20key%0Aenhancements%3A%20we%20introduce%20an%20implicit%20segmentation%20structure%20to%20decompose%20the%0Atime%20series%20and%20map%20it%20to%20segmented%20hidden%20states%2C%20resulting%20in%20denser%0Ainformation%20exchange%20during%20the%20segmentation%20phase.%20Additionally%2C%20we%0Aincorporate%20residual%20structures%20in%20the%20encoding%20layer%20to%20mitigate%20information%0Aloss%20within%20the%20recurrent%20structure.%20To%20extract%20information%20more%20effectively%2C%0Awe%20further%20integrate%20the%20Mamba%20architecture%20to%20enhance%20time%20series%20information%0Aextraction.%20Experiments%20on%20several%20real-world%20long%20time%20series%20forecasting%0Adatasets%20demonstrate%20that%20our%20model%20surpasses%20the%20performance%20of%20current%0Astate-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10768v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSegRNN%253AEnhanced%2520SegRNN%2520Model%2520with%2520Mamba%2520for%2520Long-Term%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DGaoXiang%2520Zhao%2520and%2520Li%2520Zhou%2520and%2520XiaoQiang%2520Wang%26entry.1292438233%3D%2520%2520Long%2520time%2520series%2520forecasting%2520aims%2520to%2520utilize%2520historical%2520information%2520to%250Aforecast%2520future%2520states%2520over%2520extended%2520horizons.%2520Traditional%2520RNN-based%2520series%250Aforecasting%2520methods%2520struggle%2520to%2520effectively%2520address%2520long-term%2520dependencies%2520and%250Agradient%2520issues%2520in%2520long%2520time%2520series%2520problems.%2520Recently%252C%2520SegRNN%2520has%2520emerged%2520as%2520a%250Aleading%2520RNN-based%2520model%2520tailored%2520for%2520long-term%2520series%2520forecasting%252C%250Ademonstrating%2520state-of-the-art%2520performance%2520while%2520maintaining%2520a%2520streamlined%250Aarchitecture%2520through%2520innovative%2520segmentation%2520and%2520parallel%2520decoding%2520techniques.%250ANevertheless%252C%2520SegRNN%2520has%2520several%2520limitations%253A%2520its%2520fixed%2520segmentation%2520disrupts%250Adata%2520continuity%2520and%2520fails%2520to%2520effectively%2520leverage%2520information%2520across%2520different%250Asegments%252C%2520the%2520segmentation%2520strategy%2520employed%2520by%2520SegRNN%2520does%2520not%2520fundamentally%250Aaddress%2520the%2520issue%2520of%2520information%2520loss%2520within%2520the%2520recurrent%2520structure.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520the%2520MSegRNN%2520method%2520with%2520three%2520key%250Aenhancements%253A%2520we%2520introduce%2520an%2520implicit%2520segmentation%2520structure%2520to%2520decompose%2520the%250Atime%2520series%2520and%2520map%2520it%2520to%2520segmented%2520hidden%2520states%252C%2520resulting%2520in%2520denser%250Ainformation%2520exchange%2520during%2520the%2520segmentation%2520phase.%2520Additionally%252C%2520we%250Aincorporate%2520residual%2520structures%2520in%2520the%2520encoding%2520layer%2520to%2520mitigate%2520information%250Aloss%2520within%2520the%2520recurrent%2520structure.%2520To%2520extract%2520information%2520more%2520effectively%252C%250Awe%2520further%2520integrate%2520the%2520Mamba%2520architecture%2520to%2520enhance%2520time%2520series%2520information%250Aextraction.%2520Experiments%2520on%2520several%2520real-world%2520long%2520time%2520series%2520forecasting%250Adatasets%2520demonstrate%2520that%2520our%2520model%2520surpasses%2520the%2520performance%2520of%2520current%250Astate-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10768v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSegRNN%3AEnhanced%20SegRNN%20Model%20with%20Mamba%20for%20Long-Term%20Time%20Series%0A%20%20Forecasting&entry.906535625=GaoXiang%20Zhao%20and%20Li%20Zhou%20and%20XiaoQiang%20Wang&entry.1292438233=%20%20Long%20time%20series%20forecasting%20aims%20to%20utilize%20historical%20information%20to%0Aforecast%20future%20states%20over%20extended%20horizons.%20Traditional%20RNN-based%20series%0Aforecasting%20methods%20struggle%20to%20effectively%20address%20long-term%20dependencies%20and%0Agradient%20issues%20in%20long%20time%20series%20problems.%20Recently%2C%20SegRNN%20has%20emerged%20as%20a%0Aleading%20RNN-based%20model%20tailored%20for%20long-term%20series%20forecasting%2C%0Ademonstrating%20state-of-the-art%20performance%20while%20maintaining%20a%20streamlined%0Aarchitecture%20through%20innovative%20segmentation%20and%20parallel%20decoding%20techniques.%0ANevertheless%2C%20SegRNN%20has%20several%20limitations%3A%20its%20fixed%20segmentation%20disrupts%0Adata%20continuity%20and%20fails%20to%20effectively%20leverage%20information%20across%20different%0Asegments%2C%20the%20segmentation%20strategy%20employed%20by%20SegRNN%20does%20not%20fundamentally%0Aaddress%20the%20issue%20of%20information%20loss%20within%20the%20recurrent%20structure.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20MSegRNN%20method%20with%20three%20key%0Aenhancements%3A%20we%20introduce%20an%20implicit%20segmentation%20structure%20to%20decompose%20the%0Atime%20series%20and%20map%20it%20to%20segmented%20hidden%20states%2C%20resulting%20in%20denser%0Ainformation%20exchange%20during%20the%20segmentation%20phase.%20Additionally%2C%20we%0Aincorporate%20residual%20structures%20in%20the%20encoding%20layer%20to%20mitigate%20information%0Aloss%20within%20the%20recurrent%20structure.%20To%20extract%20information%20more%20effectively%2C%0Awe%20further%20integrate%20the%20Mamba%20architecture%20to%20enhance%20time%20series%20information%0Aextraction.%20Experiments%20on%20several%20real-world%20long%20time%20series%20forecasting%0Adatasets%20demonstrate%20that%20our%20model%20surpasses%20the%20performance%20of%20current%0Astate-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10768v3&entry.124074799=Read"},
{"title": "Normality Addition via Normality Detection in Industrial Image Anomaly\n  Detection Models", "author": "Jihun Yi and Dahuin Jung and Sungroh Yoon", "abstract": "  The task of image anomaly detection (IAD) aims to identify deviations from\nnormality in image data. These anomalies are patterns that deviate\nsignificantly from what the IAD model has learned from the data during\ntraining. However, in real-world scenarios, the criteria for what constitutes\nnormality often change, necessitating the reclassification of previously\nanomalous instances as normal. To address this challenge, we propose a new\nscenario termed \"normality addition,\" involving the post-training adjustment of\ndecision boundaries to incorporate new normalities. To address this challenge,\nwe propose a method called Normality Addition via Normality Detection (NAND),\nleveraging a vision-language model. NAND performs normality detection which\ndetect patterns related to the intended normality within images based on\ntextual descriptions. We then modify the results of a pre-trained IAD model to\nimplement this normality addition. Using the benchmark dataset in IAD, MVTec\nAD, we establish an evaluation protocol for the normality addition task and\nempirically demonstrate the effectiveness of the NAND method.\n", "link": "http://arxiv.org/abs/2407.19849v1", "date": "2024-07-29", "relevancy": 1.4961, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5283}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4911}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normality%20Addition%20via%20Normality%20Detection%20in%20Industrial%20Image%20Anomaly%0A%20%20Detection%20Models&body=Title%3A%20Normality%20Addition%20via%20Normality%20Detection%20in%20Industrial%20Image%20Anomaly%0A%20%20Detection%20Models%0AAuthor%3A%20Jihun%20Yi%20and%20Dahuin%20Jung%20and%20Sungroh%20Yoon%0AAbstract%3A%20%20%20The%20task%20of%20image%20anomaly%20detection%20%28IAD%29%20aims%20to%20identify%20deviations%20from%0Anormality%20in%20image%20data.%20These%20anomalies%20are%20patterns%20that%20deviate%0Asignificantly%20from%20what%20the%20IAD%20model%20has%20learned%20from%20the%20data%20during%0Atraining.%20However%2C%20in%20real-world%20scenarios%2C%20the%20criteria%20for%20what%20constitutes%0Anormality%20often%20change%2C%20necessitating%20the%20reclassification%20of%20previously%0Aanomalous%20instances%20as%20normal.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20new%0Ascenario%20termed%20%22normality%20addition%2C%22%20involving%20the%20post-training%20adjustment%20of%0Adecision%20boundaries%20to%20incorporate%20new%20normalities.%20To%20address%20this%20challenge%2C%0Awe%20propose%20a%20method%20called%20Normality%20Addition%20via%20Normality%20Detection%20%28NAND%29%2C%0Aleveraging%20a%20vision-language%20model.%20NAND%20performs%20normality%20detection%20which%0Adetect%20patterns%20related%20to%20the%20intended%20normality%20within%20images%20based%20on%0Atextual%20descriptions.%20We%20then%20modify%20the%20results%20of%20a%20pre-trained%20IAD%20model%20to%0Aimplement%20this%20normality%20addition.%20Using%20the%20benchmark%20dataset%20in%20IAD%2C%20MVTec%0AAD%2C%20we%20establish%20an%20evaluation%20protocol%20for%20the%20normality%20addition%20task%20and%0Aempirically%20demonstrate%20the%20effectiveness%20of%20the%20NAND%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormality%2520Addition%2520via%2520Normality%2520Detection%2520in%2520Industrial%2520Image%2520Anomaly%250A%2520%2520Detection%2520Models%26entry.906535625%3DJihun%2520Yi%2520and%2520Dahuin%2520Jung%2520and%2520Sungroh%2520Yoon%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520image%2520anomaly%2520detection%2520%2528IAD%2529%2520aims%2520to%2520identify%2520deviations%2520from%250Anormality%2520in%2520image%2520data.%2520These%2520anomalies%2520are%2520patterns%2520that%2520deviate%250Asignificantly%2520from%2520what%2520the%2520IAD%2520model%2520has%2520learned%2520from%2520the%2520data%2520during%250Atraining.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520the%2520criteria%2520for%2520what%2520constitutes%250Anormality%2520often%2520change%252C%2520necessitating%2520the%2520reclassification%2520of%2520previously%250Aanomalous%2520instances%2520as%2520normal.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520new%250Ascenario%2520termed%2520%2522normality%2520addition%252C%2522%2520involving%2520the%2520post-training%2520adjustment%2520of%250Adecision%2520boundaries%2520to%2520incorporate%2520new%2520normalities.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520a%2520method%2520called%2520Normality%2520Addition%2520via%2520Normality%2520Detection%2520%2528NAND%2529%252C%250Aleveraging%2520a%2520vision-language%2520model.%2520NAND%2520performs%2520normality%2520detection%2520which%250Adetect%2520patterns%2520related%2520to%2520the%2520intended%2520normality%2520within%2520images%2520based%2520on%250Atextual%2520descriptions.%2520We%2520then%2520modify%2520the%2520results%2520of%2520a%2520pre-trained%2520IAD%2520model%2520to%250Aimplement%2520this%2520normality%2520addition.%2520Using%2520the%2520benchmark%2520dataset%2520in%2520IAD%252C%2520MVTec%250AAD%252C%2520we%2520establish%2520an%2520evaluation%2520protocol%2520for%2520the%2520normality%2520addition%2520task%2520and%250Aempirically%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520NAND%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normality%20Addition%20via%20Normality%20Detection%20in%20Industrial%20Image%20Anomaly%0A%20%20Detection%20Models&entry.906535625=Jihun%20Yi%20and%20Dahuin%20Jung%20and%20Sungroh%20Yoon&entry.1292438233=%20%20The%20task%20of%20image%20anomaly%20detection%20%28IAD%29%20aims%20to%20identify%20deviations%20from%0Anormality%20in%20image%20data.%20These%20anomalies%20are%20patterns%20that%20deviate%0Asignificantly%20from%20what%20the%20IAD%20model%20has%20learned%20from%20the%20data%20during%0Atraining.%20However%2C%20in%20real-world%20scenarios%2C%20the%20criteria%20for%20what%20constitutes%0Anormality%20often%20change%2C%20necessitating%20the%20reclassification%20of%20previously%0Aanomalous%20instances%20as%20normal.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20new%0Ascenario%20termed%20%22normality%20addition%2C%22%20involving%20the%20post-training%20adjustment%20of%0Adecision%20boundaries%20to%20incorporate%20new%20normalities.%20To%20address%20this%20challenge%2C%0Awe%20propose%20a%20method%20called%20Normality%20Addition%20via%20Normality%20Detection%20%28NAND%29%2C%0Aleveraging%20a%20vision-language%20model.%20NAND%20performs%20normality%20detection%20which%0Adetect%20patterns%20related%20to%20the%20intended%20normality%20within%20images%20based%20on%0Atextual%20descriptions.%20We%20then%20modify%20the%20results%20of%20a%20pre-trained%20IAD%20model%20to%0Aimplement%20this%20normality%20addition.%20Using%20the%20benchmark%20dataset%20in%20IAD%2C%20MVTec%0AAD%2C%20we%20establish%20an%20evaluation%20protocol%20for%20the%20normality%20addition%20task%20and%0Aempirically%20demonstrate%20the%20effectiveness%20of%20the%20NAND%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19849v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


