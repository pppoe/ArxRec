<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241216.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View\n  Diffusion Models", "author": "Felix Taubner and Ruihang Zhang and Mathieu Tuli and David B. Lindell", "abstract": "  Reconstructing photorealistic and dynamic portrait avatars from images is\nessential to many applications including advertising, visual effects, and\nvirtual reality. Depending on the application, avatar reconstruction involves\ndifferent capture setups and constraints $-$ for example, visual effects\nstudios use camera arrays to capture hundreds of reference images, while\ncontent creators may seek to animate a single portrait image downloaded from\nthe internet. As such, there is a large and heterogeneous ecosystem of methods\nfor avatar reconstruction. Techniques based on multi-view stereo or neural\nrendering achieve the highest quality results, but require hundreds of\nreference images. Recent generative models produce convincing avatars from a\nsingle reference image, but visual fidelity yet lags behind multi-view\ntechniques. Here, we present CAP4D: an approach that uses a morphable\nmulti-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait\navatars from any number of reference images (i.e., one to 100) and animate and\nrender them in real time. Our approach demonstrates state-of-the-art\nperformance for single-, few-, and multi-image 4D portrait avatar\nreconstruction, and takes steps to bridge the gap in visual fidelity between\nsingle-image and multi-view reconstruction techniques.\n", "link": "http://arxiv.org/abs/2412.12093v1", "date": "2024-12-16", "relevancy": 3.3945, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6953}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6953}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAP4D%3A%20Creating%20Animatable%204D%20Portrait%20Avatars%20with%20Morphable%20Multi-View%0A%20%20Diffusion%20Models&body=Title%3A%20CAP4D%3A%20Creating%20Animatable%204D%20Portrait%20Avatars%20with%20Morphable%20Multi-View%0A%20%20Diffusion%20Models%0AAuthor%3A%20Felix%20Taubner%20and%20Ruihang%20Zhang%20and%20Mathieu%20Tuli%20and%20David%20B.%20Lindell%0AAbstract%3A%20%20%20Reconstructing%20photorealistic%20and%20dynamic%20portrait%20avatars%20from%20images%20is%0Aessential%20to%20many%20applications%20including%20advertising%2C%20visual%20effects%2C%20and%0Avirtual%20reality.%20Depending%20on%20the%20application%2C%20avatar%20reconstruction%20involves%0Adifferent%20capture%20setups%20and%20constraints%20%24-%24%20for%20example%2C%20visual%20effects%0Astudios%20use%20camera%20arrays%20to%20capture%20hundreds%20of%20reference%20images%2C%20while%0Acontent%20creators%20may%20seek%20to%20animate%20a%20single%20portrait%20image%20downloaded%20from%0Athe%20internet.%20As%20such%2C%20there%20is%20a%20large%20and%20heterogeneous%20ecosystem%20of%20methods%0Afor%20avatar%20reconstruction.%20Techniques%20based%20on%20multi-view%20stereo%20or%20neural%0Arendering%20achieve%20the%20highest%20quality%20results%2C%20but%20require%20hundreds%20of%0Areference%20images.%20Recent%20generative%20models%20produce%20convincing%20avatars%20from%20a%0Asingle%20reference%20image%2C%20but%20visual%20fidelity%20yet%20lags%20behind%20multi-view%0Atechniques.%20Here%2C%20we%20present%20CAP4D%3A%20an%20approach%20that%20uses%20a%20morphable%0Amulti-view%20diffusion%20model%20to%20reconstruct%20photoreal%204D%20%28dynamic%203D%29%20portrait%0Aavatars%20from%20any%20number%20of%20reference%20images%20%28i.e.%2C%20one%20to%20100%29%20and%20animate%20and%0Arender%20them%20in%20real%20time.%20Our%20approach%20demonstrates%20state-of-the-art%0Aperformance%20for%20single-%2C%20few-%2C%20and%20multi-image%204D%20portrait%20avatar%0Areconstruction%2C%20and%20takes%20steps%20to%20bridge%20the%20gap%20in%20visual%20fidelity%20between%0Asingle-image%20and%20multi-view%20reconstruction%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAP4D%253A%2520Creating%2520Animatable%25204D%2520Portrait%2520Avatars%2520with%2520Morphable%2520Multi-View%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DFelix%2520Taubner%2520and%2520Ruihang%2520Zhang%2520and%2520Mathieu%2520Tuli%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3D%2520%2520Reconstructing%2520photorealistic%2520and%2520dynamic%2520portrait%2520avatars%2520from%2520images%2520is%250Aessential%2520to%2520many%2520applications%2520including%2520advertising%252C%2520visual%2520effects%252C%2520and%250Avirtual%2520reality.%2520Depending%2520on%2520the%2520application%252C%2520avatar%2520reconstruction%2520involves%250Adifferent%2520capture%2520setups%2520and%2520constraints%2520%2524-%2524%2520for%2520example%252C%2520visual%2520effects%250Astudios%2520use%2520camera%2520arrays%2520to%2520capture%2520hundreds%2520of%2520reference%2520images%252C%2520while%250Acontent%2520creators%2520may%2520seek%2520to%2520animate%2520a%2520single%2520portrait%2520image%2520downloaded%2520from%250Athe%2520internet.%2520As%2520such%252C%2520there%2520is%2520a%2520large%2520and%2520heterogeneous%2520ecosystem%2520of%2520methods%250Afor%2520avatar%2520reconstruction.%2520Techniques%2520based%2520on%2520multi-view%2520stereo%2520or%2520neural%250Arendering%2520achieve%2520the%2520highest%2520quality%2520results%252C%2520but%2520require%2520hundreds%2520of%250Areference%2520images.%2520Recent%2520generative%2520models%2520produce%2520convincing%2520avatars%2520from%2520a%250Asingle%2520reference%2520image%252C%2520but%2520visual%2520fidelity%2520yet%2520lags%2520behind%2520multi-view%250Atechniques.%2520Here%252C%2520we%2520present%2520CAP4D%253A%2520an%2520approach%2520that%2520uses%2520a%2520morphable%250Amulti-view%2520diffusion%2520model%2520to%2520reconstruct%2520photoreal%25204D%2520%2528dynamic%25203D%2529%2520portrait%250Aavatars%2520from%2520any%2520number%2520of%2520reference%2520images%2520%2528i.e.%252C%2520one%2520to%2520100%2529%2520and%2520animate%2520and%250Arender%2520them%2520in%2520real%2520time.%2520Our%2520approach%2520demonstrates%2520state-of-the-art%250Aperformance%2520for%2520single-%252C%2520few-%252C%2520and%2520multi-image%25204D%2520portrait%2520avatar%250Areconstruction%252C%2520and%2520takes%2520steps%2520to%2520bridge%2520the%2520gap%2520in%2520visual%2520fidelity%2520between%250Asingle-image%2520and%2520multi-view%2520reconstruction%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAP4D%3A%20Creating%20Animatable%204D%20Portrait%20Avatars%20with%20Morphable%20Multi-View%0A%20%20Diffusion%20Models&entry.906535625=Felix%20Taubner%20and%20Ruihang%20Zhang%20and%20Mathieu%20Tuli%20and%20David%20B.%20Lindell&entry.1292438233=%20%20Reconstructing%20photorealistic%20and%20dynamic%20portrait%20avatars%20from%20images%20is%0Aessential%20to%20many%20applications%20including%20advertising%2C%20visual%20effects%2C%20and%0Avirtual%20reality.%20Depending%20on%20the%20application%2C%20avatar%20reconstruction%20involves%0Adifferent%20capture%20setups%20and%20constraints%20%24-%24%20for%20example%2C%20visual%20effects%0Astudios%20use%20camera%20arrays%20to%20capture%20hundreds%20of%20reference%20images%2C%20while%0Acontent%20creators%20may%20seek%20to%20animate%20a%20single%20portrait%20image%20downloaded%20from%0Athe%20internet.%20As%20such%2C%20there%20is%20a%20large%20and%20heterogeneous%20ecosystem%20of%20methods%0Afor%20avatar%20reconstruction.%20Techniques%20based%20on%20multi-view%20stereo%20or%20neural%0Arendering%20achieve%20the%20highest%20quality%20results%2C%20but%20require%20hundreds%20of%0Areference%20images.%20Recent%20generative%20models%20produce%20convincing%20avatars%20from%20a%0Asingle%20reference%20image%2C%20but%20visual%20fidelity%20yet%20lags%20behind%20multi-view%0Atechniques.%20Here%2C%20we%20present%20CAP4D%3A%20an%20approach%20that%20uses%20a%20morphable%0Amulti-view%20diffusion%20model%20to%20reconstruct%20photoreal%204D%20%28dynamic%203D%29%20portrait%0Aavatars%20from%20any%20number%20of%20reference%20images%20%28i.e.%2C%20one%20to%20100%29%20and%20animate%20and%0Arender%20them%20in%20real%20time.%20Our%20approach%20demonstrates%20state-of-the-art%0Aperformance%20for%20single-%2C%20few-%2C%20and%20multi-image%204D%20portrait%20avatar%0Areconstruction%2C%20and%20takes%20steps%20to%20bridge%20the%20gap%20in%20visual%20fidelity%20between%0Asingle-image%20and%20multi-view%20reconstruction%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12093v1&entry.124074799=Read"},
{"title": "PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting", "author": "Cheng Zhang and Haofei Xu and Qianyi Wu and Camilo Cruz Gambardella and Dinh Phung and Jianfei Cai", "abstract": "  With the advent of portable 360{\\deg} cameras, panorama has gained\nsignificant attention in applications like virtual reality (VR), virtual tours,\nrobotics, and autonomous driving. As a result, wide-baseline panorama view\nsynthesis has emerged as a vital task, where high resolution, fast inference,\nand memory efficiency are essential. Nevertheless, existing methods are\ntypically constrained to lower resolutions (512 $\\times$ 1024) due to demanding\nmemory and computational requirements. In this paper, we present PanSplat, a\ngeneralizable, feed-forward approach that efficiently supports resolution up to\n4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian\npyramid with a Fibonacci lattice arrangement, enhancing image quality while\nreducing information redundancy. To accommodate the demands of high resolution,\nwe propose a pipeline that integrates a hierarchical spherical cost volume and\nGaussian heads with local operations, enabling two-step deferred\nbackpropagation for memory-efficient training on a single A100 GPU. Experiments\ndemonstrate that PanSplat achieves state-of-the-art results with superior\nefficiency and image quality across both synthetic and real-world datasets.\nCode will be available at \\url{https://github.com/chengzhag/PanSplat}.\n", "link": "http://arxiv.org/abs/2412.12096v1", "date": "2024-12-16", "relevancy": 3.0913, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6691}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6213}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanSplat%3A%204K%20Panorama%20Synthesis%20with%20Feed-Forward%20Gaussian%20Splatting&body=Title%3A%20PanSplat%3A%204K%20Panorama%20Synthesis%20with%20Feed-Forward%20Gaussian%20Splatting%0AAuthor%3A%20Cheng%20Zhang%20and%20Haofei%20Xu%20and%20Qianyi%20Wu%20and%20Camilo%20Cruz%20Gambardella%20and%20Dinh%20Phung%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%20With%20the%20advent%20of%20portable%20360%7B%5Cdeg%7D%20cameras%2C%20panorama%20has%20gained%0Asignificant%20attention%20in%20applications%20like%20virtual%20reality%20%28VR%29%2C%20virtual%20tours%2C%0Arobotics%2C%20and%20autonomous%20driving.%20As%20a%20result%2C%20wide-baseline%20panorama%20view%0Asynthesis%20has%20emerged%20as%20a%20vital%20task%2C%20where%20high%20resolution%2C%20fast%20inference%2C%0Aand%20memory%20efficiency%20are%20essential.%20Nevertheless%2C%20existing%20methods%20are%0Atypically%20constrained%20to%20lower%20resolutions%20%28512%20%24%5Ctimes%24%201024%29%20due%20to%20demanding%0Amemory%20and%20computational%20requirements.%20In%20this%20paper%2C%20we%20present%20PanSplat%2C%20a%0Ageneralizable%2C%20feed-forward%20approach%20that%20efficiently%20supports%20resolution%20up%20to%0A4K%20%282048%20%24%5Ctimes%24%204096%29.%20Our%20approach%20features%20a%20tailored%20spherical%203D%20Gaussian%0Apyramid%20with%20a%20Fibonacci%20lattice%20arrangement%2C%20enhancing%20image%20quality%20while%0Areducing%20information%20redundancy.%20To%20accommodate%20the%20demands%20of%20high%20resolution%2C%0Awe%20propose%20a%20pipeline%20that%20integrates%20a%20hierarchical%20spherical%20cost%20volume%20and%0AGaussian%20heads%20with%20local%20operations%2C%20enabling%20two-step%20deferred%0Abackpropagation%20for%20memory-efficient%20training%20on%20a%20single%20A100%20GPU.%20Experiments%0Ademonstrate%20that%20PanSplat%20achieves%20state-of-the-art%20results%20with%20superior%0Aefficiency%20and%20image%20quality%20across%20both%20synthetic%20and%20real-world%20datasets.%0ACode%20will%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/chengzhag/PanSplat%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanSplat%253A%25204K%2520Panorama%2520Synthesis%2520with%2520Feed-Forward%2520Gaussian%2520Splatting%26entry.906535625%3DCheng%2520Zhang%2520and%2520Haofei%2520Xu%2520and%2520Qianyi%2520Wu%2520and%2520Camilo%2520Cruz%2520Gambardella%2520and%2520Dinh%2520Phung%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520portable%2520360%257B%255Cdeg%257D%2520cameras%252C%2520panorama%2520has%2520gained%250Asignificant%2520attention%2520in%2520applications%2520like%2520virtual%2520reality%2520%2528VR%2529%252C%2520virtual%2520tours%252C%250Arobotics%252C%2520and%2520autonomous%2520driving.%2520As%2520a%2520result%252C%2520wide-baseline%2520panorama%2520view%250Asynthesis%2520has%2520emerged%2520as%2520a%2520vital%2520task%252C%2520where%2520high%2520resolution%252C%2520fast%2520inference%252C%250Aand%2520memory%2520efficiency%2520are%2520essential.%2520Nevertheless%252C%2520existing%2520methods%2520are%250Atypically%2520constrained%2520to%2520lower%2520resolutions%2520%2528512%2520%2524%255Ctimes%2524%25201024%2529%2520due%2520to%2520demanding%250Amemory%2520and%2520computational%2520requirements.%2520In%2520this%2520paper%252C%2520we%2520present%2520PanSplat%252C%2520a%250Ageneralizable%252C%2520feed-forward%2520approach%2520that%2520efficiently%2520supports%2520resolution%2520up%2520to%250A4K%2520%25282048%2520%2524%255Ctimes%2524%25204096%2529.%2520Our%2520approach%2520features%2520a%2520tailored%2520spherical%25203D%2520Gaussian%250Apyramid%2520with%2520a%2520Fibonacci%2520lattice%2520arrangement%252C%2520enhancing%2520image%2520quality%2520while%250Areducing%2520information%2520redundancy.%2520To%2520accommodate%2520the%2520demands%2520of%2520high%2520resolution%252C%250Awe%2520propose%2520a%2520pipeline%2520that%2520integrates%2520a%2520hierarchical%2520spherical%2520cost%2520volume%2520and%250AGaussian%2520heads%2520with%2520local%2520operations%252C%2520enabling%2520two-step%2520deferred%250Abackpropagation%2520for%2520memory-efficient%2520training%2520on%2520a%2520single%2520A100%2520GPU.%2520Experiments%250Ademonstrate%2520that%2520PanSplat%2520achieves%2520state-of-the-art%2520results%2520with%2520superior%250Aefficiency%2520and%2520image%2520quality%2520across%2520both%2520synthetic%2520and%2520real-world%2520datasets.%250ACode%2520will%2520be%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/chengzhag/PanSplat%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanSplat%3A%204K%20Panorama%20Synthesis%20with%20Feed-Forward%20Gaussian%20Splatting&entry.906535625=Cheng%20Zhang%20and%20Haofei%20Xu%20and%20Qianyi%20Wu%20and%20Camilo%20Cruz%20Gambardella%20and%20Dinh%20Phung%20and%20Jianfei%20Cai&entry.1292438233=%20%20With%20the%20advent%20of%20portable%20360%7B%5Cdeg%7D%20cameras%2C%20panorama%20has%20gained%0Asignificant%20attention%20in%20applications%20like%20virtual%20reality%20%28VR%29%2C%20virtual%20tours%2C%0Arobotics%2C%20and%20autonomous%20driving.%20As%20a%20result%2C%20wide-baseline%20panorama%20view%0Asynthesis%20has%20emerged%20as%20a%20vital%20task%2C%20where%20high%20resolution%2C%20fast%20inference%2C%0Aand%20memory%20efficiency%20are%20essential.%20Nevertheless%2C%20existing%20methods%20are%0Atypically%20constrained%20to%20lower%20resolutions%20%28512%20%24%5Ctimes%24%201024%29%20due%20to%20demanding%0Amemory%20and%20computational%20requirements.%20In%20this%20paper%2C%20we%20present%20PanSplat%2C%20a%0Ageneralizable%2C%20feed-forward%20approach%20that%20efficiently%20supports%20resolution%20up%20to%0A4K%20%282048%20%24%5Ctimes%24%204096%29.%20Our%20approach%20features%20a%20tailored%20spherical%203D%20Gaussian%0Apyramid%20with%20a%20Fibonacci%20lattice%20arrangement%2C%20enhancing%20image%20quality%20while%0Areducing%20information%20redundancy.%20To%20accommodate%20the%20demands%20of%20high%20resolution%2C%0Awe%20propose%20a%20pipeline%20that%20integrates%20a%20hierarchical%20spherical%20cost%20volume%20and%0AGaussian%20heads%20with%20local%20operations%2C%20enabling%20two-step%20deferred%0Abackpropagation%20for%20memory-efficient%20training%20on%20a%20single%20A100%20GPU.%20Experiments%0Ademonstrate%20that%20PanSplat%20achieves%20state-of-the-art%20results%20with%20superior%0Aefficiency%20and%20image%20quality%20across%20both%20synthetic%20and%20real-world%20datasets.%0ACode%20will%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/chengzhag/PanSplat%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12096v1&entry.124074799=Read"},
{"title": "From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach", "author": "Xilin Wang and Jia Zheng and Yuanchao Hu and Hao Zhu and Qian Yu and Zihan Zhou", "abstract": "  In this paper, we present CAD2Program, a new method for reconstructing 3D\nparametric models from 2D CAD drawings. Our proposed method is inspired by\nrecent successes in vision-language models (VLMs), and departs from traditional\nmethods which rely on task-specific data representations and/or algorithms.\nSpecifically, on the input side, we simply treat the 2D CAD drawing as a raster\nimage, regardless of its original format, and encode the image with a standard\nViT model. We show that such an encoding scheme achieves competitive\nperformance against existing methods that operate on vector-graphics inputs,\nwhile imposing substantially fewer restrictions on the 2D drawings. On the\noutput side, our method auto-regressively predicts a general-purpose language\ndescribing 3D parametric models in text form. Compared to other sequence\nmodeling methods for CAD which use domain-specific sequence representations\nwith fixed-size slots, our text-based representation is more flexible, and can\nbe easily extended to arbitrary geometric entities and semantic or functional\nproperties. Experimental results on a large-scale dataset of cabinet models\ndemonstrate the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2412.11892v1", "date": "2024-12-16", "relevancy": 3.0581, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%202D%20CAD%20Drawings%20to%203D%20Parametric%20Models%3A%20A%20Vision-Language%20Approach&body=Title%3A%20From%202D%20CAD%20Drawings%20to%203D%20Parametric%20Models%3A%20A%20Vision-Language%20Approach%0AAuthor%3A%20Xilin%20Wang%20and%20Jia%20Zheng%20and%20Yuanchao%20Hu%20and%20Hao%20Zhu%20and%20Qian%20Yu%20and%20Zihan%20Zhou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20CAD2Program%2C%20a%20new%20method%20for%20reconstructing%203D%0Aparametric%20models%20from%202D%20CAD%20drawings.%20Our%20proposed%20method%20is%20inspired%20by%0Arecent%20successes%20in%20vision-language%20models%20%28VLMs%29%2C%20and%20departs%20from%20traditional%0Amethods%20which%20rely%20on%20task-specific%20data%20representations%20and/or%20algorithms.%0ASpecifically%2C%20on%20the%20input%20side%2C%20we%20simply%20treat%20the%202D%20CAD%20drawing%20as%20a%20raster%0Aimage%2C%20regardless%20of%20its%20original%20format%2C%20and%20encode%20the%20image%20with%20a%20standard%0AViT%20model.%20We%20show%20that%20such%20an%20encoding%20scheme%20achieves%20competitive%0Aperformance%20against%20existing%20methods%20that%20operate%20on%20vector-graphics%20inputs%2C%0Awhile%20imposing%20substantially%20fewer%20restrictions%20on%20the%202D%20drawings.%20On%20the%0Aoutput%20side%2C%20our%20method%20auto-regressively%20predicts%20a%20general-purpose%20language%0Adescribing%203D%20parametric%20models%20in%20text%20form.%20Compared%20to%20other%20sequence%0Amodeling%20methods%20for%20CAD%20which%20use%20domain-specific%20sequence%20representations%0Awith%20fixed-size%20slots%2C%20our%20text-based%20representation%20is%20more%20flexible%2C%20and%20can%0Abe%20easily%20extended%20to%20arbitrary%20geometric%20entities%20and%20semantic%20or%20functional%0Aproperties.%20Experimental%20results%20on%20a%20large-scale%20dataset%20of%20cabinet%20models%0Ademonstrate%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%25202D%2520CAD%2520Drawings%2520to%25203D%2520Parametric%2520Models%253A%2520A%2520Vision-Language%2520Approach%26entry.906535625%3DXilin%2520Wang%2520and%2520Jia%2520Zheng%2520and%2520Yuanchao%2520Hu%2520and%2520Hao%2520Zhu%2520and%2520Qian%2520Yu%2520and%2520Zihan%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520CAD2Program%252C%2520a%2520new%2520method%2520for%2520reconstructing%25203D%250Aparametric%2520models%2520from%25202D%2520CAD%2520drawings.%2520Our%2520proposed%2520method%2520is%2520inspired%2520by%250Arecent%2520successes%2520in%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520and%2520departs%2520from%2520traditional%250Amethods%2520which%2520rely%2520on%2520task-specific%2520data%2520representations%2520and/or%2520algorithms.%250ASpecifically%252C%2520on%2520the%2520input%2520side%252C%2520we%2520simply%2520treat%2520the%25202D%2520CAD%2520drawing%2520as%2520a%2520raster%250Aimage%252C%2520regardless%2520of%2520its%2520original%2520format%252C%2520and%2520encode%2520the%2520image%2520with%2520a%2520standard%250AViT%2520model.%2520We%2520show%2520that%2520such%2520an%2520encoding%2520scheme%2520achieves%2520competitive%250Aperformance%2520against%2520existing%2520methods%2520that%2520operate%2520on%2520vector-graphics%2520inputs%252C%250Awhile%2520imposing%2520substantially%2520fewer%2520restrictions%2520on%2520the%25202D%2520drawings.%2520On%2520the%250Aoutput%2520side%252C%2520our%2520method%2520auto-regressively%2520predicts%2520a%2520general-purpose%2520language%250Adescribing%25203D%2520parametric%2520models%2520in%2520text%2520form.%2520Compared%2520to%2520other%2520sequence%250Amodeling%2520methods%2520for%2520CAD%2520which%2520use%2520domain-specific%2520sequence%2520representations%250Awith%2520fixed-size%2520slots%252C%2520our%2520text-based%2520representation%2520is%2520more%2520flexible%252C%2520and%2520can%250Abe%2520easily%2520extended%2520to%2520arbitrary%2520geometric%2520entities%2520and%2520semantic%2520or%2520functional%250Aproperties.%2520Experimental%2520results%2520on%2520a%2520large-scale%2520dataset%2520of%2520cabinet%2520models%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%202D%20CAD%20Drawings%20to%203D%20Parametric%20Models%3A%20A%20Vision-Language%20Approach&entry.906535625=Xilin%20Wang%20and%20Jia%20Zheng%20and%20Yuanchao%20Hu%20and%20Hao%20Zhu%20and%20Qian%20Yu%20and%20Zihan%20Zhou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20CAD2Program%2C%20a%20new%20method%20for%20reconstructing%203D%0Aparametric%20models%20from%202D%20CAD%20drawings.%20Our%20proposed%20method%20is%20inspired%20by%0Arecent%20successes%20in%20vision-language%20models%20%28VLMs%29%2C%20and%20departs%20from%20traditional%0Amethods%20which%20rely%20on%20task-specific%20data%20representations%20and/or%20algorithms.%0ASpecifically%2C%20on%20the%20input%20side%2C%20we%20simply%20treat%20the%202D%20CAD%20drawing%20as%20a%20raster%0Aimage%2C%20regardless%20of%20its%20original%20format%2C%20and%20encode%20the%20image%20with%20a%20standard%0AViT%20model.%20We%20show%20that%20such%20an%20encoding%20scheme%20achieves%20competitive%0Aperformance%20against%20existing%20methods%20that%20operate%20on%20vector-graphics%20inputs%2C%0Awhile%20imposing%20substantially%20fewer%20restrictions%20on%20the%202D%20drawings.%20On%20the%0Aoutput%20side%2C%20our%20method%20auto-regressively%20predicts%20a%20general-purpose%20language%0Adescribing%203D%20parametric%20models%20in%20text%20form.%20Compared%20to%20other%20sequence%0Amodeling%20methods%20for%20CAD%20which%20use%20domain-specific%20sequence%20representations%0Awith%20fixed-size%20slots%2C%20our%20text-based%20representation%20is%20more%20flexible%2C%20and%20can%0Abe%20easily%20extended%20to%20arbitrary%20geometric%20entities%20and%20semantic%20or%20functional%0Aproperties.%20Experimental%20results%20on%20a%20large-scale%20dataset%20of%20cabinet%20models%0Ademonstrate%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11892v1&entry.124074799=Read"},
{"title": "Probing the Mid-level Vision Capabilities of Self-Supervised Learning", "author": "Xuweiyi Chen and Markus Marks and Zezhou Cheng", "abstract": "  Mid-level vision capabilities - such as generic object localization and 3D\ngeometric understanding - are not only fundamental to human vision but are also\ncrucial for many real-world applications of computer vision. These abilities\nemerge with minimal supervision during the early stages of human visual\ndevelopment. Despite their significance, current self-supervised learning (SSL)\napproaches are primarily designed and evaluated for high-level recognition\ntasks, leaving their mid-level vision capabilities largely unexamined.\n  In this study, we introduce a suite of benchmark protocols to systematically\nassess mid-level vision capabilities and present a comprehensive, controlled\nevaluation of 22 prominent SSL models across 8 mid-level vision tasks. Our\nexperiments reveal a weak correlation between mid-level and high-level task\nperformance. We also identify several SSL methods with highly imbalanced\nperformance across mid-level and high-level capabilities, as well as some that\nexcel in both. Additionally, we investigate key factors contributing to\nmid-level vision performance, such as pretraining objectives and network\narchitectures. Our study provides a holistic and timely view of what SSL models\nhave learned, complementing existing research that primarily focuses on\nhigh-level vision tasks. We hope our findings guide future SSL research to\nbenchmark models not only on high-level vision tasks but on mid-level as well.\n", "link": "http://arxiv.org/abs/2411.17474v2", "date": "2024-12-16", "relevancy": 2.9609, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6203}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20the%20Mid-level%20Vision%20Capabilities%20of%20Self-Supervised%20Learning&body=Title%3A%20Probing%20the%20Mid-level%20Vision%20Capabilities%20of%20Self-Supervised%20Learning%0AAuthor%3A%20Xuweiyi%20Chen%20and%20Markus%20Marks%20and%20Zezhou%20Cheng%0AAbstract%3A%20%20%20Mid-level%20vision%20capabilities%20-%20such%20as%20generic%20object%20localization%20and%203D%0Ageometric%20understanding%20-%20are%20not%20only%20fundamental%20to%20human%20vision%20but%20are%20also%0Acrucial%20for%20many%20real-world%20applications%20of%20computer%20vision.%20These%20abilities%0Aemerge%20with%20minimal%20supervision%20during%20the%20early%20stages%20of%20human%20visual%0Adevelopment.%20Despite%20their%20significance%2C%20current%20self-supervised%20learning%20%28SSL%29%0Aapproaches%20are%20primarily%20designed%20and%20evaluated%20for%20high-level%20recognition%0Atasks%2C%20leaving%20their%20mid-level%20vision%20capabilities%20largely%20unexamined.%0A%20%20In%20this%20study%2C%20we%20introduce%20a%20suite%20of%20benchmark%20protocols%20to%20systematically%0Aassess%20mid-level%20vision%20capabilities%20and%20present%20a%20comprehensive%2C%20controlled%0Aevaluation%20of%2022%20prominent%20SSL%20models%20across%208%20mid-level%20vision%20tasks.%20Our%0Aexperiments%20reveal%20a%20weak%20correlation%20between%20mid-level%20and%20high-level%20task%0Aperformance.%20We%20also%20identify%20several%20SSL%20methods%20with%20highly%20imbalanced%0Aperformance%20across%20mid-level%20and%20high-level%20capabilities%2C%20as%20well%20as%20some%20that%0Aexcel%20in%20both.%20Additionally%2C%20we%20investigate%20key%20factors%20contributing%20to%0Amid-level%20vision%20performance%2C%20such%20as%20pretraining%20objectives%20and%20network%0Aarchitectures.%20Our%20study%20provides%20a%20holistic%20and%20timely%20view%20of%20what%20SSL%20models%0Ahave%20learned%2C%20complementing%20existing%20research%20that%20primarily%20focuses%20on%0Ahigh-level%20vision%20tasks.%20We%20hope%20our%20findings%20guide%20future%20SSL%20research%20to%0Abenchmark%20models%20not%20only%20on%20high-level%20vision%20tasks%20but%20on%20mid-level%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17474v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520the%2520Mid-level%2520Vision%2520Capabilities%2520of%2520Self-Supervised%2520Learning%26entry.906535625%3DXuweiyi%2520Chen%2520and%2520Markus%2520Marks%2520and%2520Zezhou%2520Cheng%26entry.1292438233%3D%2520%2520Mid-level%2520vision%2520capabilities%2520-%2520such%2520as%2520generic%2520object%2520localization%2520and%25203D%250Ageometric%2520understanding%2520-%2520are%2520not%2520only%2520fundamental%2520to%2520human%2520vision%2520but%2520are%2520also%250Acrucial%2520for%2520many%2520real-world%2520applications%2520of%2520computer%2520vision.%2520These%2520abilities%250Aemerge%2520with%2520minimal%2520supervision%2520during%2520the%2520early%2520stages%2520of%2520human%2520visual%250Adevelopment.%2520Despite%2520their%2520significance%252C%2520current%2520self-supervised%2520learning%2520%2528SSL%2529%250Aapproaches%2520are%2520primarily%2520designed%2520and%2520evaluated%2520for%2520high-level%2520recognition%250Atasks%252C%2520leaving%2520their%2520mid-level%2520vision%2520capabilities%2520largely%2520unexamined.%250A%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520suite%2520of%2520benchmark%2520protocols%2520to%2520systematically%250Aassess%2520mid-level%2520vision%2520capabilities%2520and%2520present%2520a%2520comprehensive%252C%2520controlled%250Aevaluation%2520of%252022%2520prominent%2520SSL%2520models%2520across%25208%2520mid-level%2520vision%2520tasks.%2520Our%250Aexperiments%2520reveal%2520a%2520weak%2520correlation%2520between%2520mid-level%2520and%2520high-level%2520task%250Aperformance.%2520We%2520also%2520identify%2520several%2520SSL%2520methods%2520with%2520highly%2520imbalanced%250Aperformance%2520across%2520mid-level%2520and%2520high-level%2520capabilities%252C%2520as%2520well%2520as%2520some%2520that%250Aexcel%2520in%2520both.%2520Additionally%252C%2520we%2520investigate%2520key%2520factors%2520contributing%2520to%250Amid-level%2520vision%2520performance%252C%2520such%2520as%2520pretraining%2520objectives%2520and%2520network%250Aarchitectures.%2520Our%2520study%2520provides%2520a%2520holistic%2520and%2520timely%2520view%2520of%2520what%2520SSL%2520models%250Ahave%2520learned%252C%2520complementing%2520existing%2520research%2520that%2520primarily%2520focuses%2520on%250Ahigh-level%2520vision%2520tasks.%2520We%2520hope%2520our%2520findings%2520guide%2520future%2520SSL%2520research%2520to%250Abenchmark%2520models%2520not%2520only%2520on%2520high-level%2520vision%2520tasks%2520but%2520on%2520mid-level%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17474v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20the%20Mid-level%20Vision%20Capabilities%20of%20Self-Supervised%20Learning&entry.906535625=Xuweiyi%20Chen%20and%20Markus%20Marks%20and%20Zezhou%20Cheng&entry.1292438233=%20%20Mid-level%20vision%20capabilities%20-%20such%20as%20generic%20object%20localization%20and%203D%0Ageometric%20understanding%20-%20are%20not%20only%20fundamental%20to%20human%20vision%20but%20are%20also%0Acrucial%20for%20many%20real-world%20applications%20of%20computer%20vision.%20These%20abilities%0Aemerge%20with%20minimal%20supervision%20during%20the%20early%20stages%20of%20human%20visual%0Adevelopment.%20Despite%20their%20significance%2C%20current%20self-supervised%20learning%20%28SSL%29%0Aapproaches%20are%20primarily%20designed%20and%20evaluated%20for%20high-level%20recognition%0Atasks%2C%20leaving%20their%20mid-level%20vision%20capabilities%20largely%20unexamined.%0A%20%20In%20this%20study%2C%20we%20introduce%20a%20suite%20of%20benchmark%20protocols%20to%20systematically%0Aassess%20mid-level%20vision%20capabilities%20and%20present%20a%20comprehensive%2C%20controlled%0Aevaluation%20of%2022%20prominent%20SSL%20models%20across%208%20mid-level%20vision%20tasks.%20Our%0Aexperiments%20reveal%20a%20weak%20correlation%20between%20mid-level%20and%20high-level%20task%0Aperformance.%20We%20also%20identify%20several%20SSL%20methods%20with%20highly%20imbalanced%0Aperformance%20across%20mid-level%20and%20high-level%20capabilities%2C%20as%20well%20as%20some%20that%0Aexcel%20in%20both.%20Additionally%2C%20we%20investigate%20key%20factors%20contributing%20to%0Amid-level%20vision%20performance%2C%20such%20as%20pretraining%20objectives%20and%20network%0Aarchitectures.%20Our%20study%20provides%20a%20holistic%20and%20timely%20view%20of%20what%20SSL%20models%0Ahave%20learned%2C%20complementing%20existing%20research%20that%20primarily%20focuses%20on%0Ahigh-level%20vision%20tasks.%20We%20hope%20our%20findings%20guide%20future%20SSL%20research%20to%0Abenchmark%20models%20not%20only%20on%20high-level%20vision%20tasks%20but%20on%20mid-level%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17474v2&entry.124074799=Read"},
{"title": "UniLoc: Towards Universal Place Recognition Using Any Single Modality", "author": "Yan Xia and Zhendong Li and Yun-Jin Li and Letian Shi and Hu Cao and Jo\u00e3o F. Henriques and Daniel Cremers", "abstract": "  To date, most place recognition methods focus on single-modality retrieval.\nWhile they perform well in specific environments, cross-modal methods offer\ngreater flexibility by allowing seamless switching between map and query\nsources. It also promises to reduce computation requirements by having a\nunified model, and achieving greater sample efficiency by sharing parameters.\nIn this work, we develop a universal solution to place recognition, UniLoc,\nthat works with any single query modality (natural language, image, or point\ncloud). UniLoc leverages recent advances in large-scale contrastive learning,\nand learns by matching hierarchically at two levels: instance-level matching\nand scene-level matching. Specifically, we propose a novel Self-Attention based\nPooling (SAP) module to evaluate the importance of instance descriptors when\naggregated into a place-level descriptor. Experiments on the KITTI-360 dataset\ndemonstrate the benefits of cross-modality for place recognition, achieving\nsuperior performance in cross-modal settings and competitive results also for\nuni-modal scenarios. Our project page is publicly available at\nhttps://yan-xia.github.io/projects/UniLoc/.\n", "link": "http://arxiv.org/abs/2412.12079v1", "date": "2024-12-16", "relevancy": 2.951, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6078}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6073}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniLoc%3A%20Towards%20Universal%20Place%20Recognition%20Using%20Any%20Single%20Modality&body=Title%3A%20UniLoc%3A%20Towards%20Universal%20Place%20Recognition%20Using%20Any%20Single%20Modality%0AAuthor%3A%20Yan%20Xia%20and%20Zhendong%20Li%20and%20Yun-Jin%20Li%20and%20Letian%20Shi%20and%20Hu%20Cao%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20To%20date%2C%20most%20place%20recognition%20methods%20focus%20on%20single-modality%20retrieval.%0AWhile%20they%20perform%20well%20in%20specific%20environments%2C%20cross-modal%20methods%20offer%0Agreater%20flexibility%20by%20allowing%20seamless%20switching%20between%20map%20and%20query%0Asources.%20It%20also%20promises%20to%20reduce%20computation%20requirements%20by%20having%20a%0Aunified%20model%2C%20and%20achieving%20greater%20sample%20efficiency%20by%20sharing%20parameters.%0AIn%20this%20work%2C%20we%20develop%20a%20universal%20solution%20to%20place%20recognition%2C%20UniLoc%2C%0Athat%20works%20with%20any%20single%20query%20modality%20%28natural%20language%2C%20image%2C%20or%20point%0Acloud%29.%20UniLoc%20leverages%20recent%20advances%20in%20large-scale%20contrastive%20learning%2C%0Aand%20learns%20by%20matching%20hierarchically%20at%20two%20levels%3A%20instance-level%20matching%0Aand%20scene-level%20matching.%20Specifically%2C%20we%20propose%20a%20novel%20Self-Attention%20based%0APooling%20%28SAP%29%20module%20to%20evaluate%20the%20importance%20of%20instance%20descriptors%20when%0Aaggregated%20into%20a%20place-level%20descriptor.%20Experiments%20on%20the%20KITTI-360%20dataset%0Ademonstrate%20the%20benefits%20of%20cross-modality%20for%20place%20recognition%2C%20achieving%0Asuperior%20performance%20in%20cross-modal%20settings%20and%20competitive%20results%20also%20for%0Auni-modal%20scenarios.%20Our%20project%20page%20is%20publicly%20available%20at%0Ahttps%3A//yan-xia.github.io/projects/UniLoc/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniLoc%253A%2520Towards%2520Universal%2520Place%2520Recognition%2520Using%2520Any%2520Single%2520Modality%26entry.906535625%3DYan%2520Xia%2520and%2520Zhendong%2520Li%2520and%2520Yun-Jin%2520Li%2520and%2520Letian%2520Shi%2520and%2520Hu%2520Cao%2520and%2520Jo%25C3%25A3o%2520F.%2520Henriques%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520To%2520date%252C%2520most%2520place%2520recognition%2520methods%2520focus%2520on%2520single-modality%2520retrieval.%250AWhile%2520they%2520perform%2520well%2520in%2520specific%2520environments%252C%2520cross-modal%2520methods%2520offer%250Agreater%2520flexibility%2520by%2520allowing%2520seamless%2520switching%2520between%2520map%2520and%2520query%250Asources.%2520It%2520also%2520promises%2520to%2520reduce%2520computation%2520requirements%2520by%2520having%2520a%250Aunified%2520model%252C%2520and%2520achieving%2520greater%2520sample%2520efficiency%2520by%2520sharing%2520parameters.%250AIn%2520this%2520work%252C%2520we%2520develop%2520a%2520universal%2520solution%2520to%2520place%2520recognition%252C%2520UniLoc%252C%250Athat%2520works%2520with%2520any%2520single%2520query%2520modality%2520%2528natural%2520language%252C%2520image%252C%2520or%2520point%250Acloud%2529.%2520UniLoc%2520leverages%2520recent%2520advances%2520in%2520large-scale%2520contrastive%2520learning%252C%250Aand%2520learns%2520by%2520matching%2520hierarchically%2520at%2520two%2520levels%253A%2520instance-level%2520matching%250Aand%2520scene-level%2520matching.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520Self-Attention%2520based%250APooling%2520%2528SAP%2529%2520module%2520to%2520evaluate%2520the%2520importance%2520of%2520instance%2520descriptors%2520when%250Aaggregated%2520into%2520a%2520place-level%2520descriptor.%2520Experiments%2520on%2520the%2520KITTI-360%2520dataset%250Ademonstrate%2520the%2520benefits%2520of%2520cross-modality%2520for%2520place%2520recognition%252C%2520achieving%250Asuperior%2520performance%2520in%2520cross-modal%2520settings%2520and%2520competitive%2520results%2520also%2520for%250Auni-modal%2520scenarios.%2520Our%2520project%2520page%2520is%2520publicly%2520available%2520at%250Ahttps%253A//yan-xia.github.io/projects/UniLoc/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniLoc%3A%20Towards%20Universal%20Place%20Recognition%20Using%20Any%20Single%20Modality&entry.906535625=Yan%20Xia%20and%20Zhendong%20Li%20and%20Yun-Jin%20Li%20and%20Letian%20Shi%20and%20Hu%20Cao%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Daniel%20Cremers&entry.1292438233=%20%20To%20date%2C%20most%20place%20recognition%20methods%20focus%20on%20single-modality%20retrieval.%0AWhile%20they%20perform%20well%20in%20specific%20environments%2C%20cross-modal%20methods%20offer%0Agreater%20flexibility%20by%20allowing%20seamless%20switching%20between%20map%20and%20query%0Asources.%20It%20also%20promises%20to%20reduce%20computation%20requirements%20by%20having%20a%0Aunified%20model%2C%20and%20achieving%20greater%20sample%20efficiency%20by%20sharing%20parameters.%0AIn%20this%20work%2C%20we%20develop%20a%20universal%20solution%20to%20place%20recognition%2C%20UniLoc%2C%0Athat%20works%20with%20any%20single%20query%20modality%20%28natural%20language%2C%20image%2C%20or%20point%0Acloud%29.%20UniLoc%20leverages%20recent%20advances%20in%20large-scale%20contrastive%20learning%2C%0Aand%20learns%20by%20matching%20hierarchically%20at%20two%20levels%3A%20instance-level%20matching%0Aand%20scene-level%20matching.%20Specifically%2C%20we%20propose%20a%20novel%20Self-Attention%20based%0APooling%20%28SAP%29%20module%20to%20evaluate%20the%20importance%20of%20instance%20descriptors%20when%0Aaggregated%20into%20a%20place-level%20descriptor.%20Experiments%20on%20the%20KITTI-360%20dataset%0Ademonstrate%20the%20benefits%20of%20cross-modality%20for%20place%20recognition%2C%20achieving%0Asuperior%20performance%20in%20cross-modal%20settings%20and%20competitive%20results%20also%20for%0Auni-modal%20scenarios.%20Our%20project%20page%20is%20publicly%20available%20at%0Ahttps%3A//yan-xia.github.io/projects/UniLoc/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12079v1&entry.124074799=Read"},
{"title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole\n  Slide Image Analysis in Computational Pathology", "author": "Yuxuan Sun and Yixuan Si and Chenglu Zhu and Xuan Gong and Kai Zhang and Pingyi Chen and Ye Zhang and Zhongyi Shui and Tao Lin and Lin Yang", "abstract": "  The emergence of large multimodal models (LMMs) has brought significant\nadvancements to pathology. Previous research has primarily focused on\nseparately training patch-level and whole-slide image (WSI)-level models,\nlimiting the integration of learned knowledge across patches and WSIs, and\nresulting in redundant models. In this work, we introduce CPath-Omni, the first\n15-billion-parameter LMM designed to unify both patch and WSI level image\nanalysis, consolidating a variety of tasks at both levels, including\nclassification, visual question answering, captioning, and visual referring\nprompting. Extensive experiments demonstrate that CPath-Omni achieves\nstate-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42\ndatasets, outperforming or matching task-specific models trained for individual\ntasks. Additionally, we develop a specialized pathology CLIP-based visual\nprocessor for CPath-Omni, CPath-CLIP, which, for the first time, integrates\ndifferent vision models and incorporates a large language model as a text\nencoder to build a more powerful CLIP model, which achieves SOTA performance on\nnine zero-shot and four few-shot datasets. Our findings highlight CPath-Omni's\nability to unify diverse pathology tasks, demonstrating its potential to\nstreamline and advance the field of foundation model in pathology.\n", "link": "http://arxiv.org/abs/2412.12077v1", "date": "2024-12-16", "relevancy": 2.863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CPath-Omni%3A%20A%20Unified%20Multimodal%20Foundation%20Model%20for%20Patch%20and%20Whole%0A%20%20Slide%20Image%20Analysis%20in%20Computational%20Pathology&body=Title%3A%20CPath-Omni%3A%20A%20Unified%20Multimodal%20Foundation%20Model%20for%20Patch%20and%20Whole%0A%20%20Slide%20Image%20Analysis%20in%20Computational%20Pathology%0AAuthor%3A%20Yuxuan%20Sun%20and%20Yixuan%20Si%20and%20Chenglu%20Zhu%20and%20Xuan%20Gong%20and%20Kai%20Zhang%20and%20Pingyi%20Chen%20and%20Ye%20Zhang%20and%20Zhongyi%20Shui%20and%20Tao%20Lin%20and%20Lin%20Yang%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20multimodal%20models%20%28LMMs%29%20has%20brought%20significant%0Aadvancements%20to%20pathology.%20Previous%20research%20has%20primarily%20focused%20on%0Aseparately%20training%20patch-level%20and%20whole-slide%20image%20%28WSI%29-level%20models%2C%0Alimiting%20the%20integration%20of%20learned%20knowledge%20across%20patches%20and%20WSIs%2C%20and%0Aresulting%20in%20redundant%20models.%20In%20this%20work%2C%20we%20introduce%20CPath-Omni%2C%20the%20first%0A15-billion-parameter%20LMM%20designed%20to%20unify%20both%20patch%20and%20WSI%20level%20image%0Aanalysis%2C%20consolidating%20a%20variety%20of%20tasks%20at%20both%20levels%2C%20including%0Aclassification%2C%20visual%20question%20answering%2C%20captioning%2C%20and%20visual%20referring%0Aprompting.%20Extensive%20experiments%20demonstrate%20that%20CPath-Omni%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20across%20seven%20diverse%20tasks%20on%2039%20out%20of%2042%0Adatasets%2C%20outperforming%20or%20matching%20task-specific%20models%20trained%20for%20individual%0Atasks.%20Additionally%2C%20we%20develop%20a%20specialized%20pathology%20CLIP-based%20visual%0Aprocessor%20for%20CPath-Omni%2C%20CPath-CLIP%2C%20which%2C%20for%20the%20first%20time%2C%20integrates%0Adifferent%20vision%20models%20and%20incorporates%20a%20large%20language%20model%20as%20a%20text%0Aencoder%20to%20build%20a%20more%20powerful%20CLIP%20model%2C%20which%20achieves%20SOTA%20performance%20on%0Anine%20zero-shot%20and%20four%20few-shot%20datasets.%20Our%20findings%20highlight%20CPath-Omni%27s%0Aability%20to%20unify%20diverse%20pathology%20tasks%2C%20demonstrating%20its%20potential%20to%0Astreamline%20and%20advance%20the%20field%20of%20foundation%20model%20in%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCPath-Omni%253A%2520A%2520Unified%2520Multimodal%2520Foundation%2520Model%2520for%2520Patch%2520and%2520Whole%250A%2520%2520Slide%2520Image%2520Analysis%2520in%2520Computational%2520Pathology%26entry.906535625%3DYuxuan%2520Sun%2520and%2520Yixuan%2520Si%2520and%2520Chenglu%2520Zhu%2520and%2520Xuan%2520Gong%2520and%2520Kai%2520Zhang%2520and%2520Pingyi%2520Chen%2520and%2520Ye%2520Zhang%2520and%2520Zhongyi%2520Shui%2520and%2520Tao%2520Lin%2520and%2520Lin%2520Yang%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520has%2520brought%2520significant%250Aadvancements%2520to%2520pathology.%2520Previous%2520research%2520has%2520primarily%2520focused%2520on%250Aseparately%2520training%2520patch-level%2520and%2520whole-slide%2520image%2520%2528WSI%2529-level%2520models%252C%250Alimiting%2520the%2520integration%2520of%2520learned%2520knowledge%2520across%2520patches%2520and%2520WSIs%252C%2520and%250Aresulting%2520in%2520redundant%2520models.%2520In%2520this%2520work%252C%2520we%2520introduce%2520CPath-Omni%252C%2520the%2520first%250A15-billion-parameter%2520LMM%2520designed%2520to%2520unify%2520both%2520patch%2520and%2520WSI%2520level%2520image%250Aanalysis%252C%2520consolidating%2520a%2520variety%2520of%2520tasks%2520at%2520both%2520levels%252C%2520including%250Aclassification%252C%2520visual%2520question%2520answering%252C%2520captioning%252C%2520and%2520visual%2520referring%250Aprompting.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CPath-Omni%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520across%2520seven%2520diverse%2520tasks%2520on%252039%2520out%2520of%252042%250Adatasets%252C%2520outperforming%2520or%2520matching%2520task-specific%2520models%2520trained%2520for%2520individual%250Atasks.%2520Additionally%252C%2520we%2520develop%2520a%2520specialized%2520pathology%2520CLIP-based%2520visual%250Aprocessor%2520for%2520CPath-Omni%252C%2520CPath-CLIP%252C%2520which%252C%2520for%2520the%2520first%2520time%252C%2520integrates%250Adifferent%2520vision%2520models%2520and%2520incorporates%2520a%2520large%2520language%2520model%2520as%2520a%2520text%250Aencoder%2520to%2520build%2520a%2520more%2520powerful%2520CLIP%2520model%252C%2520which%2520achieves%2520SOTA%2520performance%2520on%250Anine%2520zero-shot%2520and%2520four%2520few-shot%2520datasets.%2520Our%2520findings%2520highlight%2520CPath-Omni%2527s%250Aability%2520to%2520unify%2520diverse%2520pathology%2520tasks%252C%2520demonstrating%2520its%2520potential%2520to%250Astreamline%2520and%2520advance%2520the%2520field%2520of%2520foundation%2520model%2520in%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CPath-Omni%3A%20A%20Unified%20Multimodal%20Foundation%20Model%20for%20Patch%20and%20Whole%0A%20%20Slide%20Image%20Analysis%20in%20Computational%20Pathology&entry.906535625=Yuxuan%20Sun%20and%20Yixuan%20Si%20and%20Chenglu%20Zhu%20and%20Xuan%20Gong%20and%20Kai%20Zhang%20and%20Pingyi%20Chen%20and%20Ye%20Zhang%20and%20Zhongyi%20Shui%20and%20Tao%20Lin%20and%20Lin%20Yang&entry.1292438233=%20%20The%20emergence%20of%20large%20multimodal%20models%20%28LMMs%29%20has%20brought%20significant%0Aadvancements%20to%20pathology.%20Previous%20research%20has%20primarily%20focused%20on%0Aseparately%20training%20patch-level%20and%20whole-slide%20image%20%28WSI%29-level%20models%2C%0Alimiting%20the%20integration%20of%20learned%20knowledge%20across%20patches%20and%20WSIs%2C%20and%0Aresulting%20in%20redundant%20models.%20In%20this%20work%2C%20we%20introduce%20CPath-Omni%2C%20the%20first%0A15-billion-parameter%20LMM%20designed%20to%20unify%20both%20patch%20and%20WSI%20level%20image%0Aanalysis%2C%20consolidating%20a%20variety%20of%20tasks%20at%20both%20levels%2C%20including%0Aclassification%2C%20visual%20question%20answering%2C%20captioning%2C%20and%20visual%20referring%0Aprompting.%20Extensive%20experiments%20demonstrate%20that%20CPath-Omni%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20across%20seven%20diverse%20tasks%20on%2039%20out%20of%2042%0Adatasets%2C%20outperforming%20or%20matching%20task-specific%20models%20trained%20for%20individual%0Atasks.%20Additionally%2C%20we%20develop%20a%20specialized%20pathology%20CLIP-based%20visual%0Aprocessor%20for%20CPath-Omni%2C%20CPath-CLIP%2C%20which%2C%20for%20the%20first%20time%2C%20integrates%0Adifferent%20vision%20models%20and%20incorporates%20a%20large%20language%20model%20as%20a%20text%0Aencoder%20to%20build%20a%20more%20powerful%20CLIP%20model%2C%20which%20achieves%20SOTA%20performance%20on%0Anine%20zero-shot%20and%20four%20few-shot%20datasets.%20Our%20findings%20highlight%20CPath-Omni%27s%0Aability%20to%20unify%20diverse%20pathology%20tasks%2C%20demonstrating%20its%20potential%20to%0Astreamline%20and%20advance%20the%20field%20of%20foundation%20model%20in%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12077v1&entry.124074799=Read"},
{"title": "Wonderland: Navigating 3D Scenes from a Single Image", "author": "Hanwen Liang and Junli Cao and Vidit Goel and Guocheng Qian and Sergei Korolev and Demetri Terzopoulos and Konstantinos N. Plataniotis and Sergey Tulyakov and Jian Ren", "abstract": "  This paper addresses a challenging question: How can we efficiently create\nhigh-quality, wide-scope 3D scenes from a single arbitrary image? Existing\nmethods face several constraints, such as requiring multi-view data,\ntime-consuming per-scene optimization, low visual quality in backgrounds, and\ndistorted reconstructions in unseen areas. We propose a novel pipeline to\novercome these limitations. Specifically, we introduce a large-scale\nreconstruction model that uses latents from a video diffusion model to predict\n3D Gaussian Splattings for the scenes in a feed-forward manner. The video\ndiffusion model is designed to create videos precisely following specified\ncamera trajectories, allowing it to generate compressed video latents that\ncontain multi-view information while maintaining 3D consistency. We train the\n3D reconstruction model to operate on the video latent space with a progressive\ntraining strategy, enabling the efficient generation of high-quality,\nwide-scope, and generic 3D scenes. Extensive evaluations across various\ndatasets demonstrate that our model significantly outperforms existing methods\nfor single-view 3D scene generation, particularly with out-of-domain images.\nFor the first time, we demonstrate that a 3D reconstruction model can be\neffectively built upon the latent space of a diffusion model to realize\nefficient 3D scene generation.\n", "link": "http://arxiv.org/abs/2412.12091v1", "date": "2024-12-16", "relevancy": 2.7736, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.705}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.705}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wonderland%3A%20Navigating%203D%20Scenes%20from%20a%20Single%20Image&body=Title%3A%20Wonderland%3A%20Navigating%203D%20Scenes%20from%20a%20Single%20Image%0AAuthor%3A%20Hanwen%20Liang%20and%20Junli%20Cao%20and%20Vidit%20Goel%20and%20Guocheng%20Qian%20and%20Sergei%20Korolev%20and%20Demetri%20Terzopoulos%20and%20Konstantinos%20N.%20Plataniotis%20and%20Sergey%20Tulyakov%20and%20Jian%20Ren%0AAbstract%3A%20%20%20This%20paper%20addresses%20a%20challenging%20question%3A%20How%20can%20we%20efficiently%20create%0Ahigh-quality%2C%20wide-scope%203D%20scenes%20from%20a%20single%20arbitrary%20image%3F%20Existing%0Amethods%20face%20several%20constraints%2C%20such%20as%20requiring%20multi-view%20data%2C%0Atime-consuming%20per-scene%20optimization%2C%20low%20visual%20quality%20in%20backgrounds%2C%20and%0Adistorted%20reconstructions%20in%20unseen%20areas.%20We%20propose%20a%20novel%20pipeline%20to%0Aovercome%20these%20limitations.%20Specifically%2C%20we%20introduce%20a%20large-scale%0Areconstruction%20model%20that%20uses%20latents%20from%20a%20video%20diffusion%20model%20to%20predict%0A3D%20Gaussian%20Splattings%20for%20the%20scenes%20in%20a%20feed-forward%20manner.%20The%20video%0Adiffusion%20model%20is%20designed%20to%20create%20videos%20precisely%20following%20specified%0Acamera%20trajectories%2C%20allowing%20it%20to%20generate%20compressed%20video%20latents%20that%0Acontain%20multi-view%20information%20while%20maintaining%203D%20consistency.%20We%20train%20the%0A3D%20reconstruction%20model%20to%20operate%20on%20the%20video%20latent%20space%20with%20a%20progressive%0Atraining%20strategy%2C%20enabling%20the%20efficient%20generation%20of%20high-quality%2C%0Awide-scope%2C%20and%20generic%203D%20scenes.%20Extensive%20evaluations%20across%20various%0Adatasets%20demonstrate%20that%20our%20model%20significantly%20outperforms%20existing%20methods%0Afor%20single-view%203D%20scene%20generation%2C%20particularly%20with%20out-of-domain%20images.%0AFor%20the%20first%20time%2C%20we%20demonstrate%20that%20a%203D%20reconstruction%20model%20can%20be%0Aeffectively%20built%20upon%20the%20latent%20space%20of%20a%20diffusion%20model%20to%20realize%0Aefficient%203D%20scene%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWonderland%253A%2520Navigating%25203D%2520Scenes%2520from%2520a%2520Single%2520Image%26entry.906535625%3DHanwen%2520Liang%2520and%2520Junli%2520Cao%2520and%2520Vidit%2520Goel%2520and%2520Guocheng%2520Qian%2520and%2520Sergei%2520Korolev%2520and%2520Demetri%2520Terzopoulos%2520and%2520Konstantinos%2520N.%2520Plataniotis%2520and%2520Sergey%2520Tulyakov%2520and%2520Jian%2520Ren%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520a%2520challenging%2520question%253A%2520How%2520can%2520we%2520efficiently%2520create%250Ahigh-quality%252C%2520wide-scope%25203D%2520scenes%2520from%2520a%2520single%2520arbitrary%2520image%253F%2520Existing%250Amethods%2520face%2520several%2520constraints%252C%2520such%2520as%2520requiring%2520multi-view%2520data%252C%250Atime-consuming%2520per-scene%2520optimization%252C%2520low%2520visual%2520quality%2520in%2520backgrounds%252C%2520and%250Adistorted%2520reconstructions%2520in%2520unseen%2520areas.%2520We%2520propose%2520a%2520novel%2520pipeline%2520to%250Aovercome%2520these%2520limitations.%2520Specifically%252C%2520we%2520introduce%2520a%2520large-scale%250Areconstruction%2520model%2520that%2520uses%2520latents%2520from%2520a%2520video%2520diffusion%2520model%2520to%2520predict%250A3D%2520Gaussian%2520Splattings%2520for%2520the%2520scenes%2520in%2520a%2520feed-forward%2520manner.%2520The%2520video%250Adiffusion%2520model%2520is%2520designed%2520to%2520create%2520videos%2520precisely%2520following%2520specified%250Acamera%2520trajectories%252C%2520allowing%2520it%2520to%2520generate%2520compressed%2520video%2520latents%2520that%250Acontain%2520multi-view%2520information%2520while%2520maintaining%25203D%2520consistency.%2520We%2520train%2520the%250A3D%2520reconstruction%2520model%2520to%2520operate%2520on%2520the%2520video%2520latent%2520space%2520with%2520a%2520progressive%250Atraining%2520strategy%252C%2520enabling%2520the%2520efficient%2520generation%2520of%2520high-quality%252C%250Awide-scope%252C%2520and%2520generic%25203D%2520scenes.%2520Extensive%2520evaluations%2520across%2520various%250Adatasets%2520demonstrate%2520that%2520our%2520model%2520significantly%2520outperforms%2520existing%2520methods%250Afor%2520single-view%25203D%2520scene%2520generation%252C%2520particularly%2520with%2520out-of-domain%2520images.%250AFor%2520the%2520first%2520time%252C%2520we%2520demonstrate%2520that%2520a%25203D%2520reconstruction%2520model%2520can%2520be%250Aeffectively%2520built%2520upon%2520the%2520latent%2520space%2520of%2520a%2520diffusion%2520model%2520to%2520realize%250Aefficient%25203D%2520scene%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wonderland%3A%20Navigating%203D%20Scenes%20from%20a%20Single%20Image&entry.906535625=Hanwen%20Liang%20and%20Junli%20Cao%20and%20Vidit%20Goel%20and%20Guocheng%20Qian%20and%20Sergei%20Korolev%20and%20Demetri%20Terzopoulos%20and%20Konstantinos%20N.%20Plataniotis%20and%20Sergey%20Tulyakov%20and%20Jian%20Ren&entry.1292438233=%20%20This%20paper%20addresses%20a%20challenging%20question%3A%20How%20can%20we%20efficiently%20create%0Ahigh-quality%2C%20wide-scope%203D%20scenes%20from%20a%20single%20arbitrary%20image%3F%20Existing%0Amethods%20face%20several%20constraints%2C%20such%20as%20requiring%20multi-view%20data%2C%0Atime-consuming%20per-scene%20optimization%2C%20low%20visual%20quality%20in%20backgrounds%2C%20and%0Adistorted%20reconstructions%20in%20unseen%20areas.%20We%20propose%20a%20novel%20pipeline%20to%0Aovercome%20these%20limitations.%20Specifically%2C%20we%20introduce%20a%20large-scale%0Areconstruction%20model%20that%20uses%20latents%20from%20a%20video%20diffusion%20model%20to%20predict%0A3D%20Gaussian%20Splattings%20for%20the%20scenes%20in%20a%20feed-forward%20manner.%20The%20video%0Adiffusion%20model%20is%20designed%20to%20create%20videos%20precisely%20following%20specified%0Acamera%20trajectories%2C%20allowing%20it%20to%20generate%20compressed%20video%20latents%20that%0Acontain%20multi-view%20information%20while%20maintaining%203D%20consistency.%20We%20train%20the%0A3D%20reconstruction%20model%20to%20operate%20on%20the%20video%20latent%20space%20with%20a%20progressive%0Atraining%20strategy%2C%20enabling%20the%20efficient%20generation%20of%20high-quality%2C%0Awide-scope%2C%20and%20generic%203D%20scenes.%20Extensive%20evaluations%20across%20various%0Adatasets%20demonstrate%20that%20our%20model%20significantly%20outperforms%20existing%20methods%0Afor%20single-view%203D%20scene%20generation%2C%20particularly%20with%20out-of-domain%20images.%0AFor%20the%20first%20time%2C%20we%20demonstrate%20that%20a%203D%20reconstruction%20model%20can%20be%0Aeffectively%20built%20upon%20the%20latent%20space%20of%20a%20diffusion%20model%20to%20realize%0Aefficient%203D%20scene%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12091v1&entry.124074799=Read"},
{"title": "FSFM: A Generalizable Face Security Foundation Model via Self-Supervised\n  Facial Representation Learning", "author": "Gaojian Wang and Feng Lin and Tong Wu and Zhenguang Liu and Zhongjie Ba and Kui Ren", "abstract": "  This work asks: with abundant, unlabeled real faces, how to learn a robust\nand transferable facial representation that boosts various face security tasks\nwith respect to generalization performance? We make the first attempt and\npropose a self-supervised pretraining framework to learn fundamental\nrepresentations of real face images, FSFM, that leverages the synergy between\nmasked image modeling (MIM) and instance discrimination (ID). We explore\nvarious facial masking strategies for MIM and present a simple yet powerful\nCRFR-P masking, which explicitly forces the model to capture meaningful\nintra-region consistency and challenging inter-region coherency. Furthermore,\nwe devise the ID network that naturally couples with MIM to establish\nunderlying local-to-global correspondence via tailored self-distillation. These\nthree learning objectives, namely 3C, empower encoding both local features and\nglobal semantics of real faces. After pretraining, a vanilla ViT serves as a\nuniversal vision foundation model for downstream face security tasks:\ncross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen\ndiffusion facial forgery detection. Extensive experiments on 10 public datasets\ndemonstrate that our model transfers better than supervised pretraining, visual\nand facial self-supervised learning arts, and even outperforms task-specialized\nSOTA methods.\n", "link": "http://arxiv.org/abs/2412.12032v1", "date": "2024-12-16", "relevancy": 2.7139, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5503}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5408}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FSFM%3A%20A%20Generalizable%20Face%20Security%20Foundation%20Model%20via%20Self-Supervised%0A%20%20Facial%20Representation%20Learning&body=Title%3A%20FSFM%3A%20A%20Generalizable%20Face%20Security%20Foundation%20Model%20via%20Self-Supervised%0A%20%20Facial%20Representation%20Learning%0AAuthor%3A%20Gaojian%20Wang%20and%20Feng%20Lin%20and%20Tong%20Wu%20and%20Zhenguang%20Liu%20and%20Zhongjie%20Ba%20and%20Kui%20Ren%0AAbstract%3A%20%20%20This%20work%20asks%3A%20with%20abundant%2C%20unlabeled%20real%20faces%2C%20how%20to%20learn%20a%20robust%0Aand%20transferable%20facial%20representation%20that%20boosts%20various%20face%20security%20tasks%0Awith%20respect%20to%20generalization%20performance%3F%20We%20make%20the%20first%20attempt%20and%0Apropose%20a%20self-supervised%20pretraining%20framework%20to%20learn%20fundamental%0Arepresentations%20of%20real%20face%20images%2C%20FSFM%2C%20that%20leverages%20the%20synergy%20between%0Amasked%20image%20modeling%20%28MIM%29%20and%20instance%20discrimination%20%28ID%29.%20We%20explore%0Avarious%20facial%20masking%20strategies%20for%20MIM%20and%20present%20a%20simple%20yet%20powerful%0ACRFR-P%20masking%2C%20which%20explicitly%20forces%20the%20model%20to%20capture%20meaningful%0Aintra-region%20consistency%20and%20challenging%20inter-region%20coherency.%20Furthermore%2C%0Awe%20devise%20the%20ID%20network%20that%20naturally%20couples%20with%20MIM%20to%20establish%0Aunderlying%20local-to-global%20correspondence%20via%20tailored%20self-distillation.%20These%0Athree%20learning%20objectives%2C%20namely%203C%2C%20empower%20encoding%20both%20local%20features%20and%0Aglobal%20semantics%20of%20real%20faces.%20After%20pretraining%2C%20a%20vanilla%20ViT%20serves%20as%20a%0Auniversal%20vision%20foundation%20model%20for%20downstream%20face%20security%20tasks%3A%0Across-dataset%20deepfake%20detection%2C%20cross-domain%20face%20anti-spoofing%2C%20and%20unseen%0Adiffusion%20facial%20forgery%20detection.%20Extensive%20experiments%20on%2010%20public%20datasets%0Ademonstrate%20that%20our%20model%20transfers%20better%20than%20supervised%20pretraining%2C%20visual%0Aand%20facial%20self-supervised%20learning%20arts%2C%20and%20even%20outperforms%20task-specialized%0ASOTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFSFM%253A%2520A%2520Generalizable%2520Face%2520Security%2520Foundation%2520Model%2520via%2520Self-Supervised%250A%2520%2520Facial%2520Representation%2520Learning%26entry.906535625%3DGaojian%2520Wang%2520and%2520Feng%2520Lin%2520and%2520Tong%2520Wu%2520and%2520Zhenguang%2520Liu%2520and%2520Zhongjie%2520Ba%2520and%2520Kui%2520Ren%26entry.1292438233%3D%2520%2520This%2520work%2520asks%253A%2520with%2520abundant%252C%2520unlabeled%2520real%2520faces%252C%2520how%2520to%2520learn%2520a%2520robust%250Aand%2520transferable%2520facial%2520representation%2520that%2520boosts%2520various%2520face%2520security%2520tasks%250Awith%2520respect%2520to%2520generalization%2520performance%253F%2520We%2520make%2520the%2520first%2520attempt%2520and%250Apropose%2520a%2520self-supervised%2520pretraining%2520framework%2520to%2520learn%2520fundamental%250Arepresentations%2520of%2520real%2520face%2520images%252C%2520FSFM%252C%2520that%2520leverages%2520the%2520synergy%2520between%250Amasked%2520image%2520modeling%2520%2528MIM%2529%2520and%2520instance%2520discrimination%2520%2528ID%2529.%2520We%2520explore%250Avarious%2520facial%2520masking%2520strategies%2520for%2520MIM%2520and%2520present%2520a%2520simple%2520yet%2520powerful%250ACRFR-P%2520masking%252C%2520which%2520explicitly%2520forces%2520the%2520model%2520to%2520capture%2520meaningful%250Aintra-region%2520consistency%2520and%2520challenging%2520inter-region%2520coherency.%2520Furthermore%252C%250Awe%2520devise%2520the%2520ID%2520network%2520that%2520naturally%2520couples%2520with%2520MIM%2520to%2520establish%250Aunderlying%2520local-to-global%2520correspondence%2520via%2520tailored%2520self-distillation.%2520These%250Athree%2520learning%2520objectives%252C%2520namely%25203C%252C%2520empower%2520encoding%2520both%2520local%2520features%2520and%250Aglobal%2520semantics%2520of%2520real%2520faces.%2520After%2520pretraining%252C%2520a%2520vanilla%2520ViT%2520serves%2520as%2520a%250Auniversal%2520vision%2520foundation%2520model%2520for%2520downstream%2520face%2520security%2520tasks%253A%250Across-dataset%2520deepfake%2520detection%252C%2520cross-domain%2520face%2520anti-spoofing%252C%2520and%2520unseen%250Adiffusion%2520facial%2520forgery%2520detection.%2520Extensive%2520experiments%2520on%252010%2520public%2520datasets%250Ademonstrate%2520that%2520our%2520model%2520transfers%2520better%2520than%2520supervised%2520pretraining%252C%2520visual%250Aand%2520facial%2520self-supervised%2520learning%2520arts%252C%2520and%2520even%2520outperforms%2520task-specialized%250ASOTA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FSFM%3A%20A%20Generalizable%20Face%20Security%20Foundation%20Model%20via%20Self-Supervised%0A%20%20Facial%20Representation%20Learning&entry.906535625=Gaojian%20Wang%20and%20Feng%20Lin%20and%20Tong%20Wu%20and%20Zhenguang%20Liu%20and%20Zhongjie%20Ba%20and%20Kui%20Ren&entry.1292438233=%20%20This%20work%20asks%3A%20with%20abundant%2C%20unlabeled%20real%20faces%2C%20how%20to%20learn%20a%20robust%0Aand%20transferable%20facial%20representation%20that%20boosts%20various%20face%20security%20tasks%0Awith%20respect%20to%20generalization%20performance%3F%20We%20make%20the%20first%20attempt%20and%0Apropose%20a%20self-supervised%20pretraining%20framework%20to%20learn%20fundamental%0Arepresentations%20of%20real%20face%20images%2C%20FSFM%2C%20that%20leverages%20the%20synergy%20between%0Amasked%20image%20modeling%20%28MIM%29%20and%20instance%20discrimination%20%28ID%29.%20We%20explore%0Avarious%20facial%20masking%20strategies%20for%20MIM%20and%20present%20a%20simple%20yet%20powerful%0ACRFR-P%20masking%2C%20which%20explicitly%20forces%20the%20model%20to%20capture%20meaningful%0Aintra-region%20consistency%20and%20challenging%20inter-region%20coherency.%20Furthermore%2C%0Awe%20devise%20the%20ID%20network%20that%20naturally%20couples%20with%20MIM%20to%20establish%0Aunderlying%20local-to-global%20correspondence%20via%20tailored%20self-distillation.%20These%0Athree%20learning%20objectives%2C%20namely%203C%2C%20empower%20encoding%20both%20local%20features%20and%0Aglobal%20semantics%20of%20real%20faces.%20After%20pretraining%2C%20a%20vanilla%20ViT%20serves%20as%20a%0Auniversal%20vision%20foundation%20model%20for%20downstream%20face%20security%20tasks%3A%0Across-dataset%20deepfake%20detection%2C%20cross-domain%20face%20anti-spoofing%2C%20and%20unseen%0Adiffusion%20facial%20forgery%20detection.%20Extensive%20experiments%20on%2010%20public%20datasets%0Ademonstrate%20that%20our%20model%20transfers%20better%20than%20supervised%20pretraining%2C%20visual%0Aand%20facial%20self-supervised%20learning%20arts%2C%20and%20even%20outperforms%20task-specialized%0ASOTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12032v1&entry.124074799=Read"},
{"title": "Does VLM Classification Benefit from LLM Description Semantics?", "author": "Pingchuan Ma and Lennart Rietdorf and Dmytro Kotovenko and Vincent Tao Hu and Bj\u00f6rn Ommer", "abstract": "  Accurately describing images via text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect. Considering this, we ask how to distinguish the actual discriminative\npower of descriptions from performance boosts that potentially rely on an\nensembling effect. To study this, we propose an alternative evaluation scenario\nthat shows a characteristic behavior if the used descriptions have\ndiscriminative power. Furthermore, we propose a training-free method to select\ndiscriminative descriptions that work independently of classname ensembling\neffects. The training-free method works in the following way: A test image has\na local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then,\nw.r.t. to a small selection set, we extract descriptions that distinguish each\nclass well in the local neighborhood. Using the selected descriptions, we\ndemonstrate improved classification accuracy across seven datasets and provide\nin-depth analysis and insights into the explainability of description-based\nimage classification by VLMs.\n", "link": "http://arxiv.org/abs/2412.11917v1", "date": "2024-12-16", "relevancy": 2.6811, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.54}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.54}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20VLM%20Classification%20Benefit%20from%20LLM%20Description%20Semantics%3F&body=Title%3A%20Does%20VLM%20Classification%20Benefit%20from%20LLM%20Description%20Semantics%3F%0AAuthor%3A%20Pingchuan%20Ma%20and%20Lennart%20Rietdorf%20and%20Dmytro%20Kotovenko%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20Accurately%20describing%20images%20via%20text%20is%20a%20foundation%20of%20explainable%20AI.%0AVision-Language%20Models%20%28VLMs%29%20like%20CLIP%20have%20recently%20addressed%20this%20by%0Aaligning%20images%20and%20texts%20in%20a%20shared%20embedding%20space%2C%20expressing%20semantic%0Asimilarities%20between%20vision%20and%20language%20embeddings.%20VLM%20classification%20can%20be%0Aimproved%20with%20descriptions%20generated%20by%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%0Ait%20is%20difficult%20to%20determine%20the%20contribution%20of%20actual%20description%20semantics%2C%0Aas%20the%20performance%20gain%20may%20also%20stem%20from%20a%20semantic-agnostic%20ensembling%0Aeffect.%20Considering%20this%2C%20we%20ask%20how%20to%20distinguish%20the%20actual%20discriminative%0Apower%20of%20descriptions%20from%20performance%20boosts%20that%20potentially%20rely%20on%20an%0Aensembling%20effect.%20To%20study%20this%2C%20we%20propose%20an%20alternative%20evaluation%20scenario%0Athat%20shows%20a%20characteristic%20behavior%20if%20the%20used%20descriptions%20have%0Adiscriminative%20power.%20Furthermore%2C%20we%20propose%20a%20training-free%20method%20to%20select%0Adiscriminative%20descriptions%20that%20work%20independently%20of%20classname%20ensembling%0Aeffects.%20The%20training-free%20method%20works%20in%20the%20following%20way%3A%20A%20test%20image%20has%0Aa%20local%20CLIP%20label%20neighborhood%2C%20i.e.%2C%20its%20top-%24k%24%20label%20predictions.%20Then%2C%0Aw.r.t.%20to%20a%20small%20selection%20set%2C%20we%20extract%20descriptions%20that%20distinguish%20each%0Aclass%20well%20in%20the%20local%20neighborhood.%20Using%20the%20selected%20descriptions%2C%20we%0Ademonstrate%20improved%20classification%20accuracy%20across%20seven%20datasets%20and%20provide%0Ain-depth%20analysis%20and%20insights%20into%20the%20explainability%20of%20description-based%0Aimage%20classification%20by%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520VLM%2520Classification%2520Benefit%2520from%2520LLM%2520Description%2520Semantics%253F%26entry.906535625%3DPingchuan%2520Ma%2520and%2520Lennart%2520Rietdorf%2520and%2520Dmytro%2520Kotovenko%2520and%2520Vincent%2520Tao%2520Hu%2520and%2520Bj%25C3%25B6rn%2520Ommer%26entry.1292438233%3D%2520%2520Accurately%2520describing%2520images%2520via%2520text%2520is%2520a%2520foundation%2520of%2520explainable%2520AI.%250AVision-Language%2520Models%2520%2528VLMs%2529%2520like%2520CLIP%2520have%2520recently%2520addressed%2520this%2520by%250Aaligning%2520images%2520and%2520texts%2520in%2520a%2520shared%2520embedding%2520space%252C%2520expressing%2520semantic%250Asimilarities%2520between%2520vision%2520and%2520language%2520embeddings.%2520VLM%2520classification%2520can%2520be%250Aimproved%2520with%2520descriptions%2520generated%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%250Ait%2520is%2520difficult%2520to%2520determine%2520the%2520contribution%2520of%2520actual%2520description%2520semantics%252C%250Aas%2520the%2520performance%2520gain%2520may%2520also%2520stem%2520from%2520a%2520semantic-agnostic%2520ensembling%250Aeffect.%2520Considering%2520this%252C%2520we%2520ask%2520how%2520to%2520distinguish%2520the%2520actual%2520discriminative%250Apower%2520of%2520descriptions%2520from%2520performance%2520boosts%2520that%2520potentially%2520rely%2520on%2520an%250Aensembling%2520effect.%2520To%2520study%2520this%252C%2520we%2520propose%2520an%2520alternative%2520evaluation%2520scenario%250Athat%2520shows%2520a%2520characteristic%2520behavior%2520if%2520the%2520used%2520descriptions%2520have%250Adiscriminative%2520power.%2520Furthermore%252C%2520we%2520propose%2520a%2520training-free%2520method%2520to%2520select%250Adiscriminative%2520descriptions%2520that%2520work%2520independently%2520of%2520classname%2520ensembling%250Aeffects.%2520The%2520training-free%2520method%2520works%2520in%2520the%2520following%2520way%253A%2520A%2520test%2520image%2520has%250Aa%2520local%2520CLIP%2520label%2520neighborhood%252C%2520i.e.%252C%2520its%2520top-%2524k%2524%2520label%2520predictions.%2520Then%252C%250Aw.r.t.%2520to%2520a%2520small%2520selection%2520set%252C%2520we%2520extract%2520descriptions%2520that%2520distinguish%2520each%250Aclass%2520well%2520in%2520the%2520local%2520neighborhood.%2520Using%2520the%2520selected%2520descriptions%252C%2520we%250Ademonstrate%2520improved%2520classification%2520accuracy%2520across%2520seven%2520datasets%2520and%2520provide%250Ain-depth%2520analysis%2520and%2520insights%2520into%2520the%2520explainability%2520of%2520description-based%250Aimage%2520classification%2520by%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20VLM%20Classification%20Benefit%20from%20LLM%20Description%20Semantics%3F&entry.906535625=Pingchuan%20Ma%20and%20Lennart%20Rietdorf%20and%20Dmytro%20Kotovenko%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20Accurately%20describing%20images%20via%20text%20is%20a%20foundation%20of%20explainable%20AI.%0AVision-Language%20Models%20%28VLMs%29%20like%20CLIP%20have%20recently%20addressed%20this%20by%0Aaligning%20images%20and%20texts%20in%20a%20shared%20embedding%20space%2C%20expressing%20semantic%0Asimilarities%20between%20vision%20and%20language%20embeddings.%20VLM%20classification%20can%20be%0Aimproved%20with%20descriptions%20generated%20by%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%0Ait%20is%20difficult%20to%20determine%20the%20contribution%20of%20actual%20description%20semantics%2C%0Aas%20the%20performance%20gain%20may%20also%20stem%20from%20a%20semantic-agnostic%20ensembling%0Aeffect.%20Considering%20this%2C%20we%20ask%20how%20to%20distinguish%20the%20actual%20discriminative%0Apower%20of%20descriptions%20from%20performance%20boosts%20that%20potentially%20rely%20on%20an%0Aensembling%20effect.%20To%20study%20this%2C%20we%20propose%20an%20alternative%20evaluation%20scenario%0Athat%20shows%20a%20characteristic%20behavior%20if%20the%20used%20descriptions%20have%0Adiscriminative%20power.%20Furthermore%2C%20we%20propose%20a%20training-free%20method%20to%20select%0Adiscriminative%20descriptions%20that%20work%20independently%20of%20classname%20ensembling%0Aeffects.%20The%20training-free%20method%20works%20in%20the%20following%20way%3A%20A%20test%20image%20has%0Aa%20local%20CLIP%20label%20neighborhood%2C%20i.e.%2C%20its%20top-%24k%24%20label%20predictions.%20Then%2C%0Aw.r.t.%20to%20a%20small%20selection%20set%2C%20we%20extract%20descriptions%20that%20distinguish%20each%0Aclass%20well%20in%20the%20local%20neighborhood.%20Using%20the%20selected%20descriptions%2C%20we%0Ademonstrate%20improved%20classification%20accuracy%20across%20seven%20datasets%20and%20provide%0Ain-depth%20analysis%20and%20insights%20into%20the%20explainability%20of%20description-based%0Aimage%20classification%20by%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11917v1&entry.124074799=Read"},
{"title": "Are the Latent Representations of Foundation Models for Pathology\n  Invariant to Rotation?", "author": "Matou\u0161 Elphick and Samra Turajlic and Guang Yang", "abstract": "  Self-supervised foundation models for digital pathology encode small patches\nfrom H\\&E whole slide images into latent representations used for downstream\ntasks. However, the invariance of these representations to patch rotation\nremains unexplored. This study investigates the rotational invariance of latent\nrepresentations across twelve foundation models by quantifying the alignment\nbetween non-rotated and rotated patches using mutual $k$-nearest neighbours and\ncosine distance. Models that incorporated rotation augmentation during\nself-supervised training exhibited significantly greater invariance to\nrotations. We hypothesise that the absence of rotational inductive bias in the\ntransformer architecture necessitates rotation augmentation during training to\nachieve learned invariance. Code:\nhttps://github.com/MatousE/rot-invariance-analysis.\n", "link": "http://arxiv.org/abs/2412.11938v1", "date": "2024-12-16", "relevancy": 2.5533, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20the%20Latent%20Representations%20of%20Foundation%20Models%20for%20Pathology%0A%20%20Invariant%20to%20Rotation%3F&body=Title%3A%20Are%20the%20Latent%20Representations%20of%20Foundation%20Models%20for%20Pathology%0A%20%20Invariant%20to%20Rotation%3F%0AAuthor%3A%20Matou%C5%A1%20Elphick%20and%20Samra%20Turajlic%20and%20Guang%20Yang%0AAbstract%3A%20%20%20Self-supervised%20foundation%20models%20for%20digital%20pathology%20encode%20small%20patches%0Afrom%20H%5C%26E%20whole%20slide%20images%20into%20latent%20representations%20used%20for%20downstream%0Atasks.%20However%2C%20the%20invariance%20of%20these%20representations%20to%20patch%20rotation%0Aremains%20unexplored.%20This%20study%20investigates%20the%20rotational%20invariance%20of%20latent%0Arepresentations%20across%20twelve%20foundation%20models%20by%20quantifying%20the%20alignment%0Abetween%20non-rotated%20and%20rotated%20patches%20using%20mutual%20%24k%24-nearest%20neighbours%20and%0Acosine%20distance.%20Models%20that%20incorporated%20rotation%20augmentation%20during%0Aself-supervised%20training%20exhibited%20significantly%20greater%20invariance%20to%0Arotations.%20We%20hypothesise%20that%20the%20absence%20of%20rotational%20inductive%20bias%20in%20the%0Atransformer%20architecture%20necessitates%20rotation%20augmentation%20during%20training%20to%0Aachieve%20learned%20invariance.%20Code%3A%0Ahttps%3A//github.com/MatousE/rot-invariance-analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520the%2520Latent%2520Representations%2520of%2520Foundation%2520Models%2520for%2520Pathology%250A%2520%2520Invariant%2520to%2520Rotation%253F%26entry.906535625%3DMatou%25C5%25A1%2520Elphick%2520and%2520Samra%2520Turajlic%2520and%2520Guang%2520Yang%26entry.1292438233%3D%2520%2520Self-supervised%2520foundation%2520models%2520for%2520digital%2520pathology%2520encode%2520small%2520patches%250Afrom%2520H%255C%2526E%2520whole%2520slide%2520images%2520into%2520latent%2520representations%2520used%2520for%2520downstream%250Atasks.%2520However%252C%2520the%2520invariance%2520of%2520these%2520representations%2520to%2520patch%2520rotation%250Aremains%2520unexplored.%2520This%2520study%2520investigates%2520the%2520rotational%2520invariance%2520of%2520latent%250Arepresentations%2520across%2520twelve%2520foundation%2520models%2520by%2520quantifying%2520the%2520alignment%250Abetween%2520non-rotated%2520and%2520rotated%2520patches%2520using%2520mutual%2520%2524k%2524-nearest%2520neighbours%2520and%250Acosine%2520distance.%2520Models%2520that%2520incorporated%2520rotation%2520augmentation%2520during%250Aself-supervised%2520training%2520exhibited%2520significantly%2520greater%2520invariance%2520to%250Arotations.%2520We%2520hypothesise%2520that%2520the%2520absence%2520of%2520rotational%2520inductive%2520bias%2520in%2520the%250Atransformer%2520architecture%2520necessitates%2520rotation%2520augmentation%2520during%2520training%2520to%250Aachieve%2520learned%2520invariance.%2520Code%253A%250Ahttps%253A//github.com/MatousE/rot-invariance-analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20the%20Latent%20Representations%20of%20Foundation%20Models%20for%20Pathology%0A%20%20Invariant%20to%20Rotation%3F&entry.906535625=Matou%C5%A1%20Elphick%20and%20Samra%20Turajlic%20and%20Guang%20Yang&entry.1292438233=%20%20Self-supervised%20foundation%20models%20for%20digital%20pathology%20encode%20small%20patches%0Afrom%20H%5C%26E%20whole%20slide%20images%20into%20latent%20representations%20used%20for%20downstream%0Atasks.%20However%2C%20the%20invariance%20of%20these%20representations%20to%20patch%20rotation%0Aremains%20unexplored.%20This%20study%20investigates%20the%20rotational%20invariance%20of%20latent%0Arepresentations%20across%20twelve%20foundation%20models%20by%20quantifying%20the%20alignment%0Abetween%20non-rotated%20and%20rotated%20patches%20using%20mutual%20%24k%24-nearest%20neighbours%20and%0Acosine%20distance.%20Models%20that%20incorporated%20rotation%20augmentation%20during%0Aself-supervised%20training%20exhibited%20significantly%20greater%20invariance%20to%0Arotations.%20We%20hypothesise%20that%20the%20absence%20of%20rotational%20inductive%20bias%20in%20the%0Atransformer%20architecture%20necessitates%20rotation%20augmentation%20during%20training%20to%0Aachieve%20learned%20invariance.%20Code%3A%0Ahttps%3A//github.com/MatousE/rot-invariance-analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11938v1&entry.124074799=Read"},
{"title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context", "author": " Gemini Team and Petko Georgiev and Ving Ian Lei and Ryan Burnell and Libin Bai and Anmol Gulati and Garrett Tanzer and Damien Vincent and Zhufeng Pan and Shibo Wang and Soroosh Mariooryad and Yifan Ding and Xinyang Geng and Fred Alcober and Roy Frostig and Mark Omernick and Lexi Walker and Cosmin Paduraru and Christina Sorokin and Andrea Tacchetti and Colin Gaffney and Samira Daruki and Olcan Sercinoglu and Zach Gleicher and Juliette Love and Paul Voigtlaender and Rohan Jain and Gabriela Surita and Kareem Mohamed and Rory Blevins and Junwhan Ahn and Tao Zhu and Kornraphop Kawintiranon and Orhan Firat and Yiming Gu and Yujing Zhang and Matthew Rahtz and Manaal Faruqui and Natalie Clay and Justin Gilmer and JD Co-Reyes and Ivo Penchev and Rui Zhu and Nobuyuki Morioka and Kevin Hui and Krishna Haridasan and Victor Campos and Mahdis Mahdieh and Mandy Guo and Samer Hassan and Kevin Kilgour and Arpi Vezer and Heng-Tze Cheng and Raoul de Liedekerke and Siddharth Goyal and Paul Barham and DJ Strouse and Seb Noury and Jonas Adler and Mukund Sundararajan and Sharad Vikram and Dmitry Lepikhin and Michela Paganini and Xavier Garcia and Fan Yang and Dasha Valter and Maja Trebacz and Kiran Vodrahalli and Chulayuth Asawaroengchai and Roman Ring and Norbert Kalb and Livio Baldini Soares and Siddhartha Brahma and David Steiner and Tianhe Yu and Fabian Mentzer and Antoine He and Lucas Gonzalez and Bibo Xu and Raphael Lopez Kaufman and Laurent El Shafey and Junhyuk Oh and Tom Hennigan and George van den Driessche and Seth Odoom and Mario Lucic and Becca Roelofs and Sid Lall and Amit Marathe and Betty Chan and Santiago Ontanon and Luheng He and Denis Teplyashin and Jonathan Lai and Phil Crone and Bogdan Damoc and Lewis Ho and Sebastian Riedel and Karel Lenc and Chih-Kuan Yeh and Aakanksha Chowdhery and Yang Xu and Mehran Kazemi and Ehsan Amid and Anastasia Petrushkina and Kevin Swersky and Ali Khodaei and Gowoon Chen and Chris Larkin and Mario Pinto and Geng Yan and Adria Puigdomenech Badia and Piyush Patil and Steven Hansen and Dave Orr and Sebastien M. R. Arnold and Jordan Grimstad and Andrew Dai and Sholto Douglas and Rishika Sinha and Vikas Yadav and Xi Chen and Elena Gribovskaya and Jacob Austin and Jeffrey Zhao and Kaushal Patel and Paul Komarek and Sophia Austin and Sebastian Borgeaud and Linda Friso and Abhimanyu Goyal and Ben Caine and Kris Cao and Da-Woon Chung and Matthew Lamm and Gabe Barth-Maron and Thais Kagohara and Kate Olszewska and Mia Chen and Kaushik Shivakumar and Rishabh Agarwal and Harshal Godhia and Ravi Rajwar and Javier Snaider and Xerxes Dotiwalla and Yuan Liu and Aditya Barua and Victor Ungureanu and Yuan Zhang and Bat-Orgil Batsaikhan and Mateo Wirth and James Qin and Ivo Danihelka and Tulsee Doshi and Martin Chadwick and Jilin Chen and Sanil Jain and Quoc Le and Arjun Kar and Madhu Gurumurthy and Cheng Li and Ruoxin Sang and Fangyu Liu and Lampros Lamprou and Rich Munoz and Nathan Lintz and Harsh Mehta and Heidi Howard and Malcolm Reynolds and Lora Aroyo and Quan Wang and Lorenzo Blanco and Albin Cassirer and Jordan Griffith and Dipanjan Das and Stephan Lee and Jakub Sygnowski and Zach Fisher and James Besley and Richard Powell and Zafarali Ahmed and Dominik Paulus and David Reitter and Zalan Borsos and Rishabh Joshi and Aedan Pope and Steven Hand and Vittorio Selo and Vihan Jain and Nikhil Sethi and Megha Goel and Takaki Makino and Rhys May and Zhen Yang and Johan Schalkwyk and Christina Butterfield and Anja Hauth and Alex Goldin and Will Hawkins and Evan Senter and Sergey Brin and Oliver Woodman and Marvin Ritter and Eric Noland and Minh Giang and Vijay Bolina and Lisa Lee and Tim Blyth and Ian Mackinnon and Machel Reid and Obaid Sarvana and David Silver and Alexander Chen and Lily Wang and Loren Maggiore and Oscar Chang and Nithya Attaluri and Gregory Thornton and Chung-Cheng Chiu and Oskar Bunyan and Nir Levine and Timothy Chung and Evgenii Eltyshev and Xiance Si and Timothy Lillicrap and Demetra Brady and Vaibhav Aggarwal and Boxi Wu and Yuanzhong Xu and Ross McIlroy and Kartikeya Badola and Paramjit Sandhu and Erica Moreira and Wojciech Stokowiec and Ross Hemsley and Dong Li and Alex Tudor and Pranav Shyam and Elahe Rahimtoroghi and Salem Haykal and Pablo Sprechmann and Xiang Zhou and Diana Mincu and Yujia Li and Ravi Addanki and Kalpesh Krishna and Xiao Wu and Alexandre Frechette and Matan Eyal and Allan Dafoe and Dave Lacey and Jay Whang and Thi Avrahami and Ye Zhang and Emanuel Taropa and Hanzhao Lin and Daniel Toyama and Eliza Rutherford and Motoki Sano and HyunJeong Choe and Alex Tomala and Chalence Safranek-Shrader and Nora Kassner and Mantas Pajarskas and Matt Harvey and Sean Sechrist and Meire Fortunato and Christina Lyu and Gamaleldin Elsayed and Chenkai Kuang and James Lottes and Eric Chu and Chao Jia and Chih-Wei Chen and Peter Humphreys and Kate Baumli and Connie Tao and Rajkumar Samuel and Cicero Nogueira dos Santos and Anders Andreassen and Nemanja Raki\u0107evi\u0107 and Dominik Grewe and Aviral Kumar and Stephanie Winkler and Jonathan Caton and Andrew Brock and Sid Dalmia and Hannah Sheahan and Iain Barr and Yingjie Miao and Paul Natsev and Jacob Devlin and Feryal Behbahani and Flavien Prost and Yanhua Sun and Artiom Myaskovsky and Thanumalayan Sankaranarayana Pillai and Dan Hurt and Angeliki Lazaridou and Xi Xiong and Ce Zheng and Fabio Pardo and Xiaowei Li and Dan Horgan and Joe Stanton and Moran Ambar and Fei Xia and Alejandro Lince and Mingqiu Wang and Basil Mustafa and Albert Webson and Hyo Lee and Rohan Anil and Martin Wicke and Timothy Dozat and Abhishek Sinha and Enrique Piqueras and Elahe Dabir and Shyam Upadhyay and Anudhyan Boral and Lisa Anne Hendricks and Corey Fry and Josip Djolonga and Yi Su and Jake Walker and Jane Labanowski and Ronny Huang and Vedant Misra and Jeremy Chen and RJ Skerry-Ryan and Avi Singh and Shruti Rijhwani and Dian Yu and Alex Castro-Ros and Beer Changpinyo and Romina Datta and Sumit Bagri and Arnar Mar Hrafnkelsson and Marcello Maggioni and Daniel Zheng and Yury Sulsky and Shaobo Hou and Tom Le Paine and Antoine Yang and Jason Riesa and Dominika Rogozinska and Dror Marcus and Dalia El Badawy and Qiao Zhang and Luyu Wang and Helen Miller and Jeremy Greer and Lars Lowe Sjos and Azade Nova and Heiga Zen and Rahma Chaabouni and Mihaela Rosca and Jiepu Jiang and Charlie Chen and Ruibo Liu and Tara Sainath and Maxim Krikun and Alex Polozov and Jean-Baptiste Lespiau and Josh Newlan and Zeyncep Cankara and Soo Kwak and Yunhan Xu and Phil Chen and Andy Coenen and Clemens Meyer and Katerina Tsihlas and Ada Ma and Juraj Gottweis and Jinwei Xing and Chenjie Gu and Jin Miao and Christian Frank and Zeynep Cankara and Sanjay Ganapathy and Ishita Dasgupta and Steph Hughes-Fitt and Heng Chen and David Reid and Keran Rong and Hongmin Fan and Joost van Amersfoort and Vincent Zhuang and Aaron Cohen and Shixiang Shane Gu and Anhad Mohananey and Anastasija Ilic and Taylor Tobin and John Wieting and Anna Bortsova and Phoebe Thacker and Emma Wang and Emily Caveness and Justin Chiu and Eren Sezener and Alex Kaskasoli and Steven Baker and Katie Millican and Mohamed Elhawaty and Kostas Aisopos and Carl Lebsack and Nathan Byrd and Hanjun Dai and Wenhao Jia and Matthew Wiethoff and Elnaz Davoodi and Albert Weston and Lakshman Yagati and Arun Ahuja and Isabel Gao and Golan Pundak and Susan Zhang and Michael Azzam and Khe Chai Sim and Sergi Caelles and James Keeling and Abhanshu Sharma and Andy Swing and YaGuang Li and Chenxi Liu and Carrie Grimes Bostock and Yamini Bansal and Zachary Nado and Ankesh Anand and Josh Lipschultz and Abhijit Karmarkar and Lev Proleev and Abe Ittycheriah and Soheil Hassas Yeganeh and George Polovets and Aleksandra Faust and Jiao Sun and Alban Rrustemi and Pen Li and Rakesh Shivanna and Jeremiah Liu and Chris Welty and Federico Lebron and Anirudh Baddepudi and Sebastian Krause and Emilio Parisotto and Radu Soricut and Zheng Xu and Dawn Bloxwich and Melvin Johnson and Behnam Neyshabur and Justin Mao-Jones and Renshen Wang and Vinay Ramasesh and Zaheer Abbas and Arthur Guez and Constant Segal and Duc Dung Nguyen and James Svensson and Le Hou and Sarah York and Kieran Milan and Sophie Bridgers and Wiktor Gworek and Marco Tagliasacchi and James Lee-Thorp and Michael Chang and Alexey Guseynov and Ale Jakse Hartman and Michael Kwong and Ruizhe Zhao and Sheleem Kashem and Elizabeth Cole and Antoine Miech and Richard Tanburn and Mary Phuong and Filip Pavetic and Sebastien Cevey and Ramona Comanescu and Richard Ives and Sherry Yang and Cosmo Du and Bo Li and Zizhao Zhang and Mariko Iinuma and Clara Huiyi Hu and Aurko Roy and Shaan Bijwadia and Zhenkai Zhu and Danilo Martins and Rachel Saputro and Anita Gergely and Steven Zheng and Dawei Jia and Ioannis Antonoglou and Adam Sadovsky and Shane Gu and Yingying Bi and Alek Andreev and Sina Samangooei and Mina Khan and Tomas Kocisky and Angelos Filos and Chintu Kumar and Colton Bishop and Adams Yu and Sarah Hodkinson and Sid Mittal and Premal Shah and Alexandre Moufarek and Yong Cheng and Adam Bloniarz and Jaehoon Lee and Pedram Pejman and Paul Michel and Stephen Spencer and Vladimir Feinberg and Xuehan Xiong and Nikolay Savinov and Charlotte Smith and Siamak Shakeri and Dustin Tran and Mary Chesus and Bernd Bohnet and George Tucker and Tamara von Glehn and Carrie Muir and Yiran Mao and Hideto Kazawa and Ambrose Slone and Kedar Soparkar and Disha Shrivastava and James Cobon-Kerr and Michael Sharman and Jay Pavagadhi and Carlos Araya and Karolis Misiunas and Nimesh Ghelani and Michael Laskin and David Barker and Qiujia Li and Anton Briukhov and Neil Houlsby and Mia Glaese and Balaji Lakshminarayanan and Nathan Schucher and Yunhao Tang and Eli Collins and Hyeontaek Lim and Fangxiaoyu Feng and Adria Recasens and Guangda Lai and Alberto Magni and Nicola De Cao and Aditya Siddhant and Zoe Ashwood and Jordi Orbay and Mostafa Dehghani and Jenny Brennan and Yifan He and Kelvin Xu and Yang Gao and Carl Saroufim and James Molloy and Xinyi Wu and Seb Arnold and Solomon Chang and Julian Schrittwieser and Elena Buchatskaya and Soroush Radpour and Martin Polacek and Skye Giordano and Ankur Bapna and Simon Tokumine and Vincent Hellendoorn and Thibault Sottiaux and Sarah Cogan and Aliaksei Severyn and Mohammad Saleh and Shantanu Thakoor and Laurent Shefey and Siyuan Qiao and Meenu Gaba and Shuo-yiin Chang and Craig Swanson and Biao Zhang and Benjamin Lee and Paul Kishan Rubenstein and Gan Song and Tom Kwiatkowski and Anna Koop and Ajay Kannan and David Kao and Parker Schuh and Axel Stjerngren and Golnaz Ghiasi and Gena Gibson and Luke Vilnis and Ye Yuan and Felipe Tiengo Ferreira and Aishwarya Kamath and Ted Klimenko and Ken Franko and Kefan Xiao and Indro Bhattacharya and Miteyan Patel and Rui Wang and Alex Morris and Robin Strudel and Vivek Sharma and Peter Choy and Sayed Hadi Hashemi and Jessica Landon and Mara Finkelstein and Priya Jhakra and Justin Frye and Megan Barnes and Matthew Mauger and Dennis Daun and Khuslen Baatarsukh and Matthew Tung and Wael Farhan and Henryk Michalewski and Fabio Viola and Felix de Chaumont Quitry and Charline Le Lan and Tom Hudson and Qingze Wang and Felix Fischer and Ivy Zheng and Elspeth White and Anca Dragan and Jean-baptiste Alayrac and Eric Ni and Alexander Pritzel and Adam Iwanicki and Michael Isard and Anna Bulanova and Lukas Zilka and Ethan Dyer and Devendra Sachan and Srivatsan Srinivasan and Hannah Muckenhirn and Honglong Cai and Amol Mandhane and Mukarram Tariq and Jack W. Rae and Gary Wang and Kareem Ayoub and Nicholas FitzGerald and Yao Zhao and Woohyun Han and Chris Alberti and Dan Garrette and Kashyap Krishnakumar and Mai Gimenez and Anselm Levskaya and Daniel Sohn and Josip Matak and Inaki Iturrate and Michael B. Chang and Jackie Xiang and Yuan Cao and Nishant Ranka and Geoff Brown and Adrian Hutter and Vahab Mirrokni and Nanxin Chen and Kaisheng Yao and Zoltan Egyed and Francois Galilee and Tyler Liechty and Praveen Kallakuri and Evan Palmer and Sanjay Ghemawat and Jasmine Liu and David Tao and Chloe Thornton and Tim Green and Mimi Jasarevic and Sharon Lin and Victor Cotruta and Yi-Xuan Tan and Noah Fiedel and Hongkun Yu and Ed Chi and Alexander Neitz and Jens Heitkaemper and Anu Sinha and Denny Zhou and Yi Sun and Charbel Kaed and Brice Hulse and Swaroop Mishra and Maria Georgaki and Sneha Kudugunta and Clement Farabet and Izhak Shafran and Daniel Vlasic and Anton Tsitsulin and Rajagopal Ananthanarayanan and Alen Carin and Guolong Su and Pei Sun and Shashank V and Gabriel Carvajal and Josef Broder and Iulia Comsa and Alena Repina and William Wong and Warren Weilun Chen and Peter Hawkins and Egor Filonov and Lucia Loher and Christoph Hirnschall and Weiyi Wang and Jingchen Ye and Andrea Burns and Hardie Cate and Diana Gage Wright and Federico Piccinini and Lei Zhang and Chu-Cheng Lin and Ionel Gog and Yana Kulizhskaya and Ashwin Sreevatsa and Shuang Song and Luis C. Cobo and Anand Iyer and Chetan Tekur and Guillermo Garrido and Zhuyun Xiao and Rupert Kemp and Huaixiu Steven Zheng and Hui Li and Ananth Agarwal and Christel Ngani and Kati Goshvadi and Rebeca Santamaria-Fernandez and Wojciech Fica and Xinyun Chen and Chris Gorgolewski and Sean Sun and Roopal Garg and Xinyu Ye and S. M. Ali Eslami and Nan Hua and Jon Simon and Pratik Joshi and Yelin Kim and Ian Tenney and Sahitya Potluri and Lam Nguyen Thiet and Quan Yuan and Florian Luisier and Alexandra Chronopoulou and Salvatore Scellato and Praveen Srinivasan and Minmin Chen and Vinod Koverkathu and Valentin Dalibard and Yaming Xu and Brennan Saeta and Keith Anderson and Thibault Sellam and Nick Fernando and Fantine Huot and Junehyuk Jung and Mani Varadarajan and Michael Quinn and Amit Raul and Maigo Le and Ruslan Habalov and Jon Clark and Komal Jalan and Kalesha Bullard and Achintya Singhal and Thang Luong and Boyu Wang and Sujeevan Rajayogam and Julian Eisenschlos and Johnson Jia and Daniel Finchelstein and Alex Yakubovich and Daniel Balle and Michael Fink and Sameer Agarwal and Jing Li and Dj Dvijotham and Shalini Pal and Kai Kang and Jaclyn Konzelmann and Jennifer Beattie and Olivier Dousse and Diane Wu and Remi Crocker and Chen Elkind and Siddhartha Reddy Jonnalagadda and Jong Lee and Dan Holtmann-Rice and Krystal Kallarackal and Rosanne Liu and Denis Vnukov and Neera Vats and Luca Invernizzi and Mohsen Jafari and Huanjie Zhou and Lilly Taylor and Jennifer Prendki and Marcus Wu and Tom Eccles and Tianqi Liu and Kavya Kopparapu and Francoise Beaufays and Christof Angermueller and Andreea Marzoca and Shourya Sarcar and Hilal Dib and Jeff Stanway and Frank Perbet and Nejc Trdin and Rachel Sterneck and Andrey Khorlin and Dinghua Li and Xihui Wu and Sonam Goenka and David Madras and Sasha Goldshtein and Willi Gierke and Tong Zhou and Yaxin Liu and Yannie Liang and Anais White and Yunjie Li and Shreya Singh and Sanaz Bahargam and Mark Epstein and Sujoy Basu and Li Lao and Adnan Ozturel and Carl Crous and Alex Zhai and Han Lu and Zora Tung and Neeraj Gaur and Alanna Walton and Lucas Dixon and Ming Zhang and Amir Globerson and Grant Uy and Andrew Bolt and Olivia Wiles and Milad Nasr and Ilia Shumailov and Marco Selvi and Francesco Piccinno and Ricardo Aguilar and Sara McCarthy and Misha Khalman and Mrinal Shukla and Vlado Galic and John Carpenter and Kevin Villela and Haibin Zhang and Harry Richardson and James Martens and Matko Bosnjak and Shreyas Rammohan Belle and Jeff Seibert and Mahmoud Alnahlawi and Brian McWilliams and Sankalp Singh and Annie Louis and Wen Ding and Dan Popovici and Lenin Simicich and Laura Knight and Pulkit Mehta and Nishesh Gupta and Chongyang Shi and Saaber Fatehi and Jovana Mitrovic and Alex Grills and Joseph Pagadora and Tsendsuren Munkhdalai and Dessie Petrova and Danielle Eisenbud and Zhishuai Zhang and Damion Yates and Bhavishya Mittal and Nilesh Tripuraneni and Yannis Assael and Thomas Brovelli and Prateek Jain and Mihajlo Velimirovic and Canfer Akbulut and Jiaqi Mu and Wolfgang Macherey and Ravin Kumar and Jun Xu and Haroon Qureshi and Gheorghe Comanici and Jeremy Wiesner and Zhitao Gong and Anton Ruddock and Matthias Bauer and Nick Felt and Anirudh GP and Anurag Arnab and Dustin Zelle and Jonas Rothfuss and Bill Rosgen and Ashish Shenoy and Bryan Seybold and Xinjian Li and Jayaram Mudigonda and Goker Erdogan and Jiawei Xia and Jiri Simsa and Andrea Michi and Yi Yao and Christopher Yew and Steven Kan and Isaac Caswell and Carey Radebaugh and Andre Elisseeff and Pedro Valenzuela and Kay McKinney and Kim Paterson and Albert Cui and Eri Latorre-Chimoto and Solomon Kim and William Zeng and Ken Durden and Priya Ponnapalli and Tiberiu Sosea and Christopher A. Choquette-Choo and James Manyika and Brona Robenek and Harsha Vashisht and Sebastien Pereira and Hoi Lam and Marko Velic and Denese Owusu-Afriyie and Katherine Lee and Tolga Bolukbasi and Alicia Parrish and Shawn Lu and Jane Park and Balaji Venkatraman and Alice Talbert and Lambert Rosique and Yuchung Cheng and Andrei Sozanschi and Adam Paszke and Praveen Kumar and Jessica Austin and Lu Li and Khalid Salama and Bartek Perz and Wooyeol Kim and Nandita Dukkipati and Anthony Baryshnikov and Christos Kaplanis and XiangHai Sheng and Yuri Chervonyi and Caglar Unlu and Diego de Las Casas and Harry Askham and Kathryn Tunyasuvunakool and Felix Gimeno and Siim Poder and Chester Kwak and Matt Miecnikowski and Vahab Mirrokni and Alek Dimitriev and Aaron Parisi and Dangyi Liu and Tomy Tsai and Toby Shevlane and Christina Kouridi and Drew Garmon and Adrian Goedeckemeyer and Adam R. Brown and Anitha Vijayakumar and Ali Elqursh and Sadegh Jazayeri and Jin Huang and Sara Mc Carthy and Jay Hoover and Lucy Kim and Sandeep Kumar and Wei Chen and Courtney Biles and Garrett Bingham and Evan Rosen and Lisa Wang and Qijun Tan and David Engel and Francesco Pongetti and Dario de Cesare and Dongseong Hwang and Lily Yu and Jennifer Pullman and Srini Narayanan and Kyle Levin and Siddharth Gopal and Megan Li and Asaf Aharoni and Trieu Trinh and Jessica Lo and Norman Casagrande and Roopali Vij and Loic Matthey and Bramandia Ramadhana and Austin Matthews and CJ Carey and Matthew Johnson and Kremena Goranova and Rohin Shah and Shereen Ashraf and Kingshuk Dasgupta and Rasmus Larsen and Yicheng Wang and Manish Reddy Vuyyuru and Chong Jiang and Joana Ijazi and Kazuki Osawa and Celine Smith and Ramya Sree Boppana and Taylan Bilal and Yuma Koizumi and Ying Xu and Yasemin Altun and Nir Shabat and Ben Bariach and Alex Korchemniy and Kiam Choo and Olaf Ronneberger and Chimezie Iwuanyanwu and Shubin Zhao and David Soergel and Cho-Jui Hsieh and Irene Cai and Shariq Iqbal and Martin Sundermeyer and Zhe Chen and Elie Bursztein and Chaitanya Malaviya and Fadi Biadsy and Prakash Shroff and Inderjit Dhillon and Tejasi Latkar and Chris Dyer and Hannah Forbes and Massimo Nicosia and Vitaly Nikolaev and Somer Greene and Marin Georgiev and Pidong Wang and Nina Martin and Hanie Sedghi and John Zhang and Praseem Banzal and Doug Fritz and Vikram Rao and Xuezhi Wang and Jiageng Zhang and Viorica Patraucean and Dayou Du and Igor Mordatch and Ivan Jurin and Lewis Liu and Ayush Dubey and Abhi Mohan and Janek Nowakowski and Vlad-Doru Ion and Nan Wei and Reiko Tojo and Maria Abi Raad and Drew A. Hudson and Vaishakh Keshava and Shubham Agrawal and Kevin Ramirez and Zhichun Wu and Hoang Nguyen and Ji Liu and Madhavi Sewak and Bryce Petrini and DongHyun Choi and Ivan Philips and Ziyue Wang and Ioana Bica and Ankush Garg and Jarek Wilkiewicz and Priyanka Agrawal and Xiaowei Li and Danhao Guo and Emily Xue and Naseer Shaik and Andrew Leach and Sadh MNM Khan and Julia Wiesinger and Sammy Jerome and Abhishek Chakladar and Alek Wenjiao Wang and Tina Ornduff and Folake Abu and Alireza Ghaffarkhah and Marcus Wainwright and Mario Cortes and Frederick Liu and Joshua Maynez and Andreas Terzis and Pouya Samangouei and Riham Mansour and Tomasz K\u0119pa and Fran\u00e7ois-Xavier Aubet and Anton Algymr and Dan Banica and Agoston Weisz and Andras Orban and Alexandre Senges and Ewa Andrejczuk and Mark Geller and Niccolo Dal Santo and Valentin Anklin and Majd Al Merey and Martin Baeuml and Trevor Strohman and Junwen Bai and Slav Petrov and Yonghui Wu and Demis Hassabis and Koray Kavukcuoglu and Jeff Dean and Oriol Vinyals", "abstract": "  In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.\n", "link": "http://arxiv.org/abs/2403.05530v5", "date": "2024-12-16", "relevancy": 2.4892, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context&body=Title%3A%20Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context%0AAuthor%3A%20%20Gemini%20Team%20and%20Petko%20Georgiev%20and%20Ving%20Ian%20Lei%20and%20Ryan%20Burnell%20and%20Libin%20Bai%20and%20Anmol%20Gulati%20and%20Garrett%20Tanzer%20and%20Damien%20Vincent%20and%20Zhufeng%20Pan%20and%20Shibo%20Wang%20and%20Soroosh%20Mariooryad%20and%20Yifan%20Ding%20and%20Xinyang%20Geng%20and%20Fred%20Alcober%20and%20Roy%20Frostig%20and%20Mark%20Omernick%20and%20Lexi%20Walker%20and%20Cosmin%20Paduraru%20and%20Christina%20Sorokin%20and%20Andrea%20Tacchetti%20and%20Colin%20Gaffney%20and%20Samira%20Daruki%20and%20Olcan%20Sercinoglu%20and%20Zach%20Gleicher%20and%20Juliette%20Love%20and%20Paul%20Voigtlaender%20and%20Rohan%20Jain%20and%20Gabriela%20Surita%20and%20Kareem%20Mohamed%20and%20Rory%20Blevins%20and%20Junwhan%20Ahn%20and%20Tao%20Zhu%20and%20Kornraphop%20Kawintiranon%20and%20Orhan%20Firat%20and%20Yiming%20Gu%20and%20Yujing%20Zhang%20and%20Matthew%20Rahtz%20and%20Manaal%20Faruqui%20and%20Natalie%20Clay%20and%20Justin%20Gilmer%20and%20JD%20Co-Reyes%20and%20Ivo%20Penchev%20and%20Rui%20Zhu%20and%20Nobuyuki%20Morioka%20and%20Kevin%20Hui%20and%20Krishna%20Haridasan%20and%20Victor%20Campos%20and%20Mahdis%20Mahdieh%20and%20Mandy%20Guo%20and%20Samer%20Hassan%20and%20Kevin%20Kilgour%20and%20Arpi%20Vezer%20and%20Heng-Tze%20Cheng%20and%20Raoul%20de%20Liedekerke%20and%20Siddharth%20Goyal%20and%20Paul%20Barham%20and%20DJ%20Strouse%20and%20Seb%20Noury%20and%20Jonas%20Adler%20and%20Mukund%20Sundararajan%20and%20Sharad%20Vikram%20and%20Dmitry%20Lepikhin%20and%20Michela%20Paganini%20and%20Xavier%20Garcia%20and%20Fan%20Yang%20and%20Dasha%20Valter%20and%20Maja%20Trebacz%20and%20Kiran%20Vodrahalli%20and%20Chulayuth%20Asawaroengchai%20and%20Roman%20Ring%20and%20Norbert%20Kalb%20and%20Livio%20Baldini%20Soares%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Tianhe%20Yu%20and%20Fabian%20Mentzer%20and%20Antoine%20He%20and%20Lucas%20Gonzalez%20and%20Bibo%20Xu%20and%20Raphael%20Lopez%20Kaufman%20and%20Laurent%20El%20Shafey%20and%20Junhyuk%20Oh%20and%20Tom%20Hennigan%20and%20George%20van%20den%20Driessche%20and%20Seth%20Odoom%20and%20Mario%20Lucic%20and%20Becca%20Roelofs%20and%20Sid%20Lall%20and%20Amit%20Marathe%20and%20Betty%20Chan%20and%20Santiago%20Ontanon%20and%20Luheng%20He%20and%20Denis%20Teplyashin%20and%20Jonathan%20Lai%20and%20Phil%20Crone%20and%20Bogdan%20Damoc%20and%20Lewis%20Ho%20and%20Sebastian%20Riedel%20and%20Karel%20Lenc%20and%20Chih-Kuan%20Yeh%20and%20Aakanksha%20Chowdhery%20and%20Yang%20Xu%20and%20Mehran%20Kazemi%20and%20Ehsan%20Amid%20and%20Anastasia%20Petrushkina%20and%20Kevin%20Swersky%20and%20Ali%20Khodaei%20and%20Gowoon%20Chen%20and%20Chris%20Larkin%20and%20Mario%20Pinto%20and%20Geng%20Yan%20and%20Adria%20Puigdomenech%20Badia%20and%20Piyush%20Patil%20and%20Steven%20Hansen%20and%20Dave%20Orr%20and%20Sebastien%20M.%20R.%20Arnold%20and%20Jordan%20Grimstad%20and%20Andrew%20Dai%20and%20Sholto%20Douglas%20and%20Rishika%20Sinha%20and%20Vikas%20Yadav%20and%20Xi%20Chen%20and%20Elena%20Gribovskaya%20and%20Jacob%20Austin%20and%20Jeffrey%20Zhao%20and%20Kaushal%20Patel%20and%20Paul%20Komarek%20and%20Sophia%20Austin%20and%20Sebastian%20Borgeaud%20and%20Linda%20Friso%20and%20Abhimanyu%20Goyal%20and%20Ben%20Caine%20and%20Kris%20Cao%20and%20Da-Woon%20Chung%20and%20Matthew%20Lamm%20and%20Gabe%20Barth-Maron%20and%20Thais%20Kagohara%20and%20Kate%20Olszewska%20and%20Mia%20Chen%20and%20Kaushik%20Shivakumar%20and%20Rishabh%20Agarwal%20and%20Harshal%20Godhia%20and%20Ravi%20Rajwar%20and%20Javier%20Snaider%20and%20Xerxes%20Dotiwalla%20and%20Yuan%20Liu%20and%20Aditya%20Barua%20and%20Victor%20Ungureanu%20and%20Yuan%20Zhang%20and%20Bat-Orgil%20Batsaikhan%20and%20Mateo%20Wirth%20and%20James%20Qin%20and%20Ivo%20Danihelka%20and%20Tulsee%20Doshi%20and%20Martin%20Chadwick%20and%20Jilin%20Chen%20and%20Sanil%20Jain%20and%20Quoc%20Le%20and%20Arjun%20Kar%20and%20Madhu%20Gurumurthy%20and%20Cheng%20Li%20and%20Ruoxin%20Sang%20and%20Fangyu%20Liu%20and%20Lampros%20Lamprou%20and%20Rich%20Munoz%20and%20Nathan%20Lintz%20and%20Harsh%20Mehta%20and%20Heidi%20Howard%20and%20Malcolm%20Reynolds%20and%20Lora%20Aroyo%20and%20Quan%20Wang%20and%20Lorenzo%20Blanco%20and%20Albin%20Cassirer%20and%20Jordan%20Griffith%20and%20Dipanjan%20Das%20and%20Stephan%20Lee%20and%20Jakub%20Sygnowski%20and%20Zach%20Fisher%20and%20James%20Besley%20and%20Richard%20Powell%20and%20Zafarali%20Ahmed%20and%20Dominik%20Paulus%20and%20David%20Reitter%20and%20Zalan%20Borsos%20and%20Rishabh%20Joshi%20and%20Aedan%20Pope%20and%20Steven%20Hand%20and%20Vittorio%20Selo%20and%20Vihan%20Jain%20and%20Nikhil%20Sethi%20and%20Megha%20Goel%20and%20Takaki%20Makino%20and%20Rhys%20May%20and%20Zhen%20Yang%20and%20Johan%20Schalkwyk%20and%20Christina%20Butterfield%20and%20Anja%20Hauth%20and%20Alex%20Goldin%20and%20Will%20Hawkins%20and%20Evan%20Senter%20and%20Sergey%20Brin%20and%20Oliver%20Woodman%20and%20Marvin%20Ritter%20and%20Eric%20Noland%20and%20Minh%20Giang%20and%20Vijay%20Bolina%20and%20Lisa%20Lee%20and%20Tim%20Blyth%20and%20Ian%20Mackinnon%20and%20Machel%20Reid%20and%20Obaid%20Sarvana%20and%20David%20Silver%20and%20Alexander%20Chen%20and%20Lily%20Wang%20and%20Loren%20Maggiore%20and%20Oscar%20Chang%20and%20Nithya%20Attaluri%20and%20Gregory%20Thornton%20and%20Chung-Cheng%20Chiu%20and%20Oskar%20Bunyan%20and%20Nir%20Levine%20and%20Timothy%20Chung%20and%20Evgenii%20Eltyshev%20and%20Xiance%20Si%20and%20Timothy%20Lillicrap%20and%20Demetra%20Brady%20and%20Vaibhav%20Aggarwal%20and%20Boxi%20Wu%20and%20Yuanzhong%20Xu%20and%20Ross%20McIlroy%20and%20Kartikeya%20Badola%20and%20Paramjit%20Sandhu%20and%20Erica%20Moreira%20and%20Wojciech%20Stokowiec%20and%20Ross%20Hemsley%20and%20Dong%20Li%20and%20Alex%20Tudor%20and%20Pranav%20Shyam%20and%20Elahe%20Rahimtoroghi%20and%20Salem%20Haykal%20and%20Pablo%20Sprechmann%20and%20Xiang%20Zhou%20and%20Diana%20Mincu%20and%20Yujia%20Li%20and%20Ravi%20Addanki%20and%20Kalpesh%20Krishna%20and%20Xiao%20Wu%20and%20Alexandre%20Frechette%20and%20Matan%20Eyal%20and%20Allan%20Dafoe%20and%20Dave%20Lacey%20and%20Jay%20Whang%20and%20Thi%20Avrahami%20and%20Ye%20Zhang%20and%20Emanuel%20Taropa%20and%20Hanzhao%20Lin%20and%20Daniel%20Toyama%20and%20Eliza%20Rutherford%20and%20Motoki%20Sano%20and%20HyunJeong%20Choe%20and%20Alex%20Tomala%20and%20Chalence%20Safranek-Shrader%20and%20Nora%20Kassner%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Sean%20Sechrist%20and%20Meire%20Fortunato%20and%20Christina%20Lyu%20and%20Gamaleldin%20Elsayed%20and%20Chenkai%20Kuang%20and%20James%20Lottes%20and%20Eric%20Chu%20and%20Chao%20Jia%20and%20Chih-Wei%20Chen%20and%20Peter%20Humphreys%20and%20Kate%20Baumli%20and%20Connie%20Tao%20and%20Rajkumar%20Samuel%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Anders%20Andreassen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Dominik%20Grewe%20and%20Aviral%20Kumar%20and%20Stephanie%20Winkler%20and%20Jonathan%20Caton%20and%20Andrew%20Brock%20and%20Sid%20Dalmia%20and%20Hannah%20Sheahan%20and%20Iain%20Barr%20and%20Yingjie%20Miao%20and%20Paul%20Natsev%20and%20Jacob%20Devlin%20and%20Feryal%20Behbahani%20and%20Flavien%20Prost%20and%20Yanhua%20Sun%20and%20Artiom%20Myaskovsky%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Dan%20Hurt%20and%20Angeliki%20Lazaridou%20and%20Xi%20Xiong%20and%20Ce%20Zheng%20and%20Fabio%20Pardo%20and%20Xiaowei%20Li%20and%20Dan%20Horgan%20and%20Joe%20Stanton%20and%20Moran%20Ambar%20and%20Fei%20Xia%20and%20Alejandro%20Lince%20and%20Mingqiu%20Wang%20and%20Basil%20Mustafa%20and%20Albert%20Webson%20and%20Hyo%20Lee%20and%20Rohan%20Anil%20and%20Martin%20Wicke%20and%20Timothy%20Dozat%20and%20Abhishek%20Sinha%20and%20Enrique%20Piqueras%20and%20Elahe%20Dabir%20and%20Shyam%20Upadhyay%20and%20Anudhyan%20Boral%20and%20Lisa%20Anne%20Hendricks%20and%20Corey%20Fry%20and%20Josip%20Djolonga%20and%20Yi%20Su%20and%20Jake%20Walker%20and%20Jane%20Labanowski%20and%20Ronny%20Huang%20and%20Vedant%20Misra%20and%20Jeremy%20Chen%20and%20RJ%20Skerry-Ryan%20and%20Avi%20Singh%20and%20Shruti%20Rijhwani%20and%20Dian%20Yu%20and%20Alex%20Castro-Ros%20and%20Beer%20Changpinyo%20and%20Romina%20Datta%20and%20Sumit%20Bagri%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Marcello%20Maggioni%20and%20Daniel%20Zheng%20and%20Yury%20Sulsky%20and%20Shaobo%20Hou%20and%20Tom%20Le%20Paine%20and%20Antoine%20Yang%20and%20Jason%20Riesa%20and%20Dominika%20Rogozinska%20and%20Dror%20Marcus%20and%20Dalia%20El%20Badawy%20and%20Qiao%20Zhang%20and%20Luyu%20Wang%20and%20Helen%20Miller%20and%20Jeremy%20Greer%20and%20Lars%20Lowe%20Sjos%20and%20Azade%20Nova%20and%20Heiga%20Zen%20and%20Rahma%20Chaabouni%20and%20Mihaela%20Rosca%20and%20Jiepu%20Jiang%20and%20Charlie%20Chen%20and%20Ruibo%20Liu%20and%20Tara%20Sainath%20and%20Maxim%20Krikun%20and%20Alex%20Polozov%20and%20Jean-Baptiste%20Lespiau%20and%20Josh%20Newlan%20and%20Zeyncep%20Cankara%20and%20Soo%20Kwak%20and%20Yunhan%20Xu%20and%20Phil%20Chen%20and%20Andy%20Coenen%20and%20Clemens%20Meyer%20and%20Katerina%20Tsihlas%20and%20Ada%20Ma%20and%20Juraj%20Gottweis%20and%20Jinwei%20Xing%20and%20Chenjie%20Gu%20and%20Jin%20Miao%20and%20Christian%20Frank%20and%20Zeynep%20Cankara%20and%20Sanjay%20Ganapathy%20and%20Ishita%20Dasgupta%20and%20Steph%20Hughes-Fitt%20and%20Heng%20Chen%20and%20David%20Reid%20and%20Keran%20Rong%20and%20Hongmin%20Fan%20and%20Joost%20van%20Amersfoort%20and%20Vincent%20Zhuang%20and%20Aaron%20Cohen%20and%20Shixiang%20Shane%20Gu%20and%20Anhad%20Mohananey%20and%20Anastasija%20Ilic%20and%20Taylor%20Tobin%20and%20John%20Wieting%20and%20Anna%20Bortsova%20and%20Phoebe%20Thacker%20and%20Emma%20Wang%20and%20Emily%20Caveness%20and%20Justin%20Chiu%20and%20Eren%20Sezener%20and%20Alex%20Kaskasoli%20and%20Steven%20Baker%20and%20Katie%20Millican%20and%20Mohamed%20Elhawaty%20and%20Kostas%20Aisopos%20and%20Carl%20Lebsack%20and%20Nathan%20Byrd%20and%20Hanjun%20Dai%20and%20Wenhao%20Jia%20and%20Matthew%20Wiethoff%20and%20Elnaz%20Davoodi%20and%20Albert%20Weston%20and%20Lakshman%20Yagati%20and%20Arun%20Ahuja%20and%20Isabel%20Gao%20and%20Golan%20Pundak%20and%20Susan%20Zhang%20and%20Michael%20Azzam%20and%20Khe%20Chai%20Sim%20and%20Sergi%20Caelles%20and%20James%20Keeling%20and%20Abhanshu%20Sharma%20and%20Andy%20Swing%20and%20YaGuang%20Li%20and%20Chenxi%20Liu%20and%20Carrie%20Grimes%20Bostock%20and%20Yamini%20Bansal%20and%20Zachary%20Nado%20and%20Ankesh%20Anand%20and%20Josh%20Lipschultz%20and%20Abhijit%20Karmarkar%20and%20Lev%20Proleev%20and%20Abe%20Ittycheriah%20and%20Soheil%20Hassas%20Yeganeh%20and%20George%20Polovets%20and%20Aleksandra%20Faust%20and%20Jiao%20Sun%20and%20Alban%20Rrustemi%20and%20Pen%20Li%20and%20Rakesh%20Shivanna%20and%20Jeremiah%20Liu%20and%20Chris%20Welty%20and%20Federico%20Lebron%20and%20Anirudh%20Baddepudi%20and%20Sebastian%20Krause%20and%20Emilio%20Parisotto%20and%20Radu%20Soricut%20and%20Zheng%20Xu%20and%20Dawn%20Bloxwich%20and%20Melvin%20Johnson%20and%20Behnam%20Neyshabur%20and%20Justin%20Mao-Jones%20and%20Renshen%20Wang%20and%20Vinay%20Ramasesh%20and%20Zaheer%20Abbas%20and%20Arthur%20Guez%20and%20Constant%20Segal%20and%20Duc%20Dung%20Nguyen%20and%20James%20Svensson%20and%20Le%20Hou%20and%20Sarah%20York%20and%20Kieran%20Milan%20and%20Sophie%20Bridgers%20and%20Wiktor%20Gworek%20and%20Marco%20Tagliasacchi%20and%20James%20Lee-Thorp%20and%20Michael%20Chang%20and%20Alexey%20Guseynov%20and%20Ale%20Jakse%20Hartman%20and%20Michael%20Kwong%20and%20Ruizhe%20Zhao%20and%20Sheleem%20Kashem%20and%20Elizabeth%20Cole%20and%20Antoine%20Miech%20and%20Richard%20Tanburn%20and%20Mary%20Phuong%20and%20Filip%20Pavetic%20and%20Sebastien%20Cevey%20and%20Ramona%20Comanescu%20and%20Richard%20Ives%20and%20Sherry%20Yang%20and%20Cosmo%20Du%20and%20Bo%20Li%20and%20Zizhao%20Zhang%20and%20Mariko%20Iinuma%20and%20Clara%20Huiyi%20Hu%20and%20Aurko%20Roy%20and%20Shaan%20Bijwadia%20and%20Zhenkai%20Zhu%20and%20Danilo%20Martins%20and%20Rachel%20Saputro%20and%20Anita%20Gergely%20and%20Steven%20Zheng%20and%20Dawei%20Jia%20and%20Ioannis%20Antonoglou%20and%20Adam%20Sadovsky%20and%20Shane%20Gu%20and%20Yingying%20Bi%20and%20Alek%20Andreev%20and%20Sina%20Samangooei%20and%20Mina%20Khan%20and%20Tomas%20Kocisky%20and%20Angelos%20Filos%20and%20Chintu%20Kumar%20and%20Colton%20Bishop%20and%20Adams%20Yu%20and%20Sarah%20Hodkinson%20and%20Sid%20Mittal%20and%20Premal%20Shah%20and%20Alexandre%20Moufarek%20and%20Yong%20Cheng%20and%20Adam%20Bloniarz%20and%20Jaehoon%20Lee%20and%20Pedram%20Pejman%20and%20Paul%20Michel%20and%20Stephen%20Spencer%20and%20Vladimir%20Feinberg%20and%20Xuehan%20Xiong%20and%20Nikolay%20Savinov%20and%20Charlotte%20Smith%20and%20Siamak%20Shakeri%20and%20Dustin%20Tran%20and%20Mary%20Chesus%20and%20Bernd%20Bohnet%20and%20George%20Tucker%20and%20Tamara%20von%20Glehn%20and%20Carrie%20Muir%20and%20Yiran%20Mao%20and%20Hideto%20Kazawa%20and%20Ambrose%20Slone%20and%20Kedar%20Soparkar%20and%20Disha%20Shrivastava%20and%20James%20Cobon-Kerr%20and%20Michael%20Sharman%20and%20Jay%20Pavagadhi%20and%20Carlos%20Araya%20and%20Karolis%20Misiunas%20and%20Nimesh%20Ghelani%20and%20Michael%20Laskin%20and%20David%20Barker%20and%20Qiujia%20Li%20and%20Anton%20Briukhov%20and%20Neil%20Houlsby%20and%20Mia%20Glaese%20and%20Balaji%20Lakshminarayanan%20and%20Nathan%20Schucher%20and%20Yunhao%20Tang%20and%20Eli%20Collins%20and%20Hyeontaek%20Lim%20and%20Fangxiaoyu%20Feng%20and%20Adria%20Recasens%20and%20Guangda%20Lai%20and%20Alberto%20Magni%20and%20Nicola%20De%20Cao%20and%20Aditya%20Siddhant%20and%20Zoe%20Ashwood%20and%20Jordi%20Orbay%20and%20Mostafa%20Dehghani%20and%20Jenny%20Brennan%20and%20Yifan%20He%20and%20Kelvin%20Xu%20and%20Yang%20Gao%20and%20Carl%20Saroufim%20and%20James%20Molloy%20and%20Xinyi%20Wu%20and%20Seb%20Arnold%20and%20Solomon%20Chang%20and%20Julian%20Schrittwieser%20and%20Elena%20Buchatskaya%20and%20Soroush%20Radpour%20and%20Martin%20Polacek%20and%20Skye%20Giordano%20and%20Ankur%20Bapna%20and%20Simon%20Tokumine%20and%20Vincent%20Hellendoorn%20and%20Thibault%20Sottiaux%20and%20Sarah%20Cogan%20and%20Aliaksei%20Severyn%20and%20Mohammad%20Saleh%20and%20Shantanu%20Thakoor%20and%20Laurent%20Shefey%20and%20Siyuan%20Qiao%20and%20Meenu%20Gaba%20and%20Shuo-yiin%20Chang%20and%20Craig%20Swanson%20and%20Biao%20Zhang%20and%20Benjamin%20Lee%20and%20Paul%20Kishan%20Rubenstein%20and%20Gan%20Song%20and%20Tom%20Kwiatkowski%20and%20Anna%20Koop%20and%20Ajay%20Kannan%20and%20David%20Kao%20and%20Parker%20Schuh%20and%20Axel%20Stjerngren%20and%20Golnaz%20Ghiasi%20and%20Gena%20Gibson%20and%20Luke%20Vilnis%20and%20Ye%20Yuan%20and%20Felipe%20Tiengo%20Ferreira%20and%20Aishwarya%20Kamath%20and%20Ted%20Klimenko%20and%20Ken%20Franko%20and%20Kefan%20Xiao%20and%20Indro%20Bhattacharya%20and%20Miteyan%20Patel%20and%20Rui%20Wang%20and%20Alex%20Morris%20and%20Robin%20Strudel%20and%20Vivek%20Sharma%20and%20Peter%20Choy%20and%20Sayed%20Hadi%20Hashemi%20and%20Jessica%20Landon%20and%20Mara%20Finkelstein%20and%20Priya%20Jhakra%20and%20Justin%20Frye%20and%20Megan%20Barnes%20and%20Matthew%20Mauger%20and%20Dennis%20Daun%20and%20Khuslen%20Baatarsukh%20and%20Matthew%20Tung%20and%20Wael%20Farhan%20and%20Henryk%20Michalewski%20and%20Fabio%20Viola%20and%20Felix%20de%20Chaumont%20Quitry%20and%20Charline%20Le%20Lan%20and%20Tom%20Hudson%20and%20Qingze%20Wang%20and%20Felix%20Fischer%20and%20Ivy%20Zheng%20and%20Elspeth%20White%20and%20Anca%20Dragan%20and%20Jean-baptiste%20Alayrac%20and%20Eric%20Ni%20and%20Alexander%20Pritzel%20and%20Adam%20Iwanicki%20and%20Michael%20Isard%20and%20Anna%20Bulanova%20and%20Lukas%20Zilka%20and%20Ethan%20Dyer%20and%20Devendra%20Sachan%20and%20Srivatsan%20Srinivasan%20and%20Hannah%20Muckenhirn%20and%20Honglong%20Cai%20and%20Amol%20Mandhane%20and%20Mukarram%20Tariq%20and%20Jack%20W.%20Rae%20and%20Gary%20Wang%20and%20Kareem%20Ayoub%20and%20Nicholas%20FitzGerald%20and%20Yao%20Zhao%20and%20Woohyun%20Han%20and%20Chris%20Alberti%20and%20Dan%20Garrette%20and%20Kashyap%20Krishnakumar%20and%20Mai%20Gimenez%20and%20Anselm%20Levskaya%20and%20Daniel%20Sohn%20and%20Josip%20Matak%20and%20Inaki%20Iturrate%20and%20Michael%20B.%20Chang%20and%20Jackie%20Xiang%20and%20Yuan%20Cao%20and%20Nishant%20Ranka%20and%20Geoff%20Brown%20and%20Adrian%20Hutter%20and%20Vahab%20Mirrokni%20and%20Nanxin%20Chen%20and%20Kaisheng%20Yao%20and%20Zoltan%20Egyed%20and%20Francois%20Galilee%20and%20Tyler%20Liechty%20and%20Praveen%20Kallakuri%20and%20Evan%20Palmer%20and%20Sanjay%20Ghemawat%20and%20Jasmine%20Liu%20and%20David%20Tao%20and%20Chloe%20Thornton%20and%20Tim%20Green%20and%20Mimi%20Jasarevic%20and%20Sharon%20Lin%20and%20Victor%20Cotruta%20and%20Yi-Xuan%20Tan%20and%20Noah%20Fiedel%20and%20Hongkun%20Yu%20and%20Ed%20Chi%20and%20Alexander%20Neitz%20and%20Jens%20Heitkaemper%20and%20Anu%20Sinha%20and%20Denny%20Zhou%20and%20Yi%20Sun%20and%20Charbel%20Kaed%20and%20Brice%20Hulse%20and%20Swaroop%20Mishra%20and%20Maria%20Georgaki%20and%20Sneha%20Kudugunta%20and%20Clement%20Farabet%20and%20Izhak%20Shafran%20and%20Daniel%20Vlasic%20and%20Anton%20Tsitsulin%20and%20Rajagopal%20Ananthanarayanan%20and%20Alen%20Carin%20and%20Guolong%20Su%20and%20Pei%20Sun%20and%20Shashank%20V%20and%20Gabriel%20Carvajal%20and%20Josef%20Broder%20and%20Iulia%20Comsa%20and%20Alena%20Repina%20and%20William%20Wong%20and%20Warren%20Weilun%20Chen%20and%20Peter%20Hawkins%20and%20Egor%20Filonov%20and%20Lucia%20Loher%20and%20Christoph%20Hirnschall%20and%20Weiyi%20Wang%20and%20Jingchen%20Ye%20and%20Andrea%20Burns%20and%20Hardie%20Cate%20and%20Diana%20Gage%20Wright%20and%20Federico%20Piccinini%20and%20Lei%20Zhang%20and%20Chu-Cheng%20Lin%20and%20Ionel%20Gog%20and%20Yana%20Kulizhskaya%20and%20Ashwin%20Sreevatsa%20and%20Shuang%20Song%20and%20Luis%20C.%20Cobo%20and%20Anand%20Iyer%20and%20Chetan%20Tekur%20and%20Guillermo%20Garrido%20and%20Zhuyun%20Xiao%20and%20Rupert%20Kemp%20and%20Huaixiu%20Steven%20Zheng%20and%20Hui%20Li%20and%20Ananth%20Agarwal%20and%20Christel%20Ngani%20and%20Kati%20Goshvadi%20and%20Rebeca%20Santamaria-Fernandez%20and%20Wojciech%20Fica%20and%20Xinyun%20Chen%20and%20Chris%20Gorgolewski%20and%20Sean%20Sun%20and%20Roopal%20Garg%20and%20Xinyu%20Ye%20and%20S.%20M.%20Ali%20Eslami%20and%20Nan%20Hua%20and%20Jon%20Simon%20and%20Pratik%20Joshi%20and%20Yelin%20Kim%20and%20Ian%20Tenney%20and%20Sahitya%20Potluri%20and%20Lam%20Nguyen%20Thiet%20and%20Quan%20Yuan%20and%20Florian%20Luisier%20and%20Alexandra%20Chronopoulou%20and%20Salvatore%20Scellato%20and%20Praveen%20Srinivasan%20and%20Minmin%20Chen%20and%20Vinod%20Koverkathu%20and%20Valentin%20Dalibard%20and%20Yaming%20Xu%20and%20Brennan%20Saeta%20and%20Keith%20Anderson%20and%20Thibault%20Sellam%20and%20Nick%20Fernando%20and%20Fantine%20Huot%20and%20Junehyuk%20Jung%20and%20Mani%20Varadarajan%20and%20Michael%20Quinn%20and%20Amit%20Raul%20and%20Maigo%20Le%20and%20Ruslan%20Habalov%20and%20Jon%20Clark%20and%20Komal%20Jalan%20and%20Kalesha%20Bullard%20and%20Achintya%20Singhal%20and%20Thang%20Luong%20and%20Boyu%20Wang%20and%20Sujeevan%20Rajayogam%20and%20Julian%20Eisenschlos%20and%20Johnson%20Jia%20and%20Daniel%20Finchelstein%20and%20Alex%20Yakubovich%20and%20Daniel%20Balle%20and%20Michael%20Fink%20and%20Sameer%20Agarwal%20and%20Jing%20Li%20and%20Dj%20Dvijotham%20and%20Shalini%20Pal%20and%20Kai%20Kang%20and%20Jaclyn%20Konzelmann%20and%20Jennifer%20Beattie%20and%20Olivier%20Dousse%20and%20Diane%20Wu%20and%20Remi%20Crocker%20and%20Chen%20Elkind%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Jong%20Lee%20and%20Dan%20Holtmann-Rice%20and%20Krystal%20Kallarackal%20and%20Rosanne%20Liu%20and%20Denis%20Vnukov%20and%20Neera%20Vats%20and%20Luca%20Invernizzi%20and%20Mohsen%20Jafari%20and%20Huanjie%20Zhou%20and%20Lilly%20Taylor%20and%20Jennifer%20Prendki%20and%20Marcus%20Wu%20and%20Tom%20Eccles%20and%20Tianqi%20Liu%20and%20Kavya%20Kopparapu%20and%20Francoise%20Beaufays%20and%20Christof%20Angermueller%20and%20Andreea%20Marzoca%20and%20Shourya%20Sarcar%20and%20Hilal%20Dib%20and%20Jeff%20Stanway%20and%20Frank%20Perbet%20and%20Nejc%20Trdin%20and%20Rachel%20Sterneck%20and%20Andrey%20Khorlin%20and%20Dinghua%20Li%20and%20Xihui%20Wu%20and%20Sonam%20Goenka%20and%20David%20Madras%20and%20Sasha%20Goldshtein%20and%20Willi%20Gierke%20and%20Tong%20Zhou%20and%20Yaxin%20Liu%20and%20Yannie%20Liang%20and%20Anais%20White%20and%20Yunjie%20Li%20and%20Shreya%20Singh%20and%20Sanaz%20Bahargam%20and%20Mark%20Epstein%20and%20Sujoy%20Basu%20and%20Li%20Lao%20and%20Adnan%20Ozturel%20and%20Carl%20Crous%20and%20Alex%20Zhai%20and%20Han%20Lu%20and%20Zora%20Tung%20and%20Neeraj%20Gaur%20and%20Alanna%20Walton%20and%20Lucas%20Dixon%20and%20Ming%20Zhang%20and%20Amir%20Globerson%20and%20Grant%20Uy%20and%20Andrew%20Bolt%20and%20Olivia%20Wiles%20and%20Milad%20Nasr%20and%20Ilia%20Shumailov%20and%20Marco%20Selvi%20and%20Francesco%20Piccinno%20and%20Ricardo%20Aguilar%20and%20Sara%20McCarthy%20and%20Misha%20Khalman%20and%20Mrinal%20Shukla%20and%20Vlado%20Galic%20and%20John%20Carpenter%20and%20Kevin%20Villela%20and%20Haibin%20Zhang%20and%20Harry%20Richardson%20and%20James%20Martens%20and%20Matko%20Bosnjak%20and%20Shreyas%20Rammohan%20Belle%20and%20Jeff%20Seibert%20and%20Mahmoud%20Alnahlawi%20and%20Brian%20McWilliams%20and%20Sankalp%20Singh%20and%20Annie%20Louis%20and%20Wen%20Ding%20and%20Dan%20Popovici%20and%20Lenin%20Simicich%20and%20Laura%20Knight%20and%20Pulkit%20Mehta%20and%20Nishesh%20Gupta%20and%20Chongyang%20Shi%20and%20Saaber%20Fatehi%20and%20Jovana%20Mitrovic%20and%20Alex%20Grills%20and%20Joseph%20Pagadora%20and%20Tsendsuren%20Munkhdalai%20and%20Dessie%20Petrova%20and%20Danielle%20Eisenbud%20and%20Zhishuai%20Zhang%20and%20Damion%20Yates%20and%20Bhavishya%20Mittal%20and%20Nilesh%20Tripuraneni%20and%20Yannis%20Assael%20and%20Thomas%20Brovelli%20and%20Prateek%20Jain%20and%20Mihajlo%20Velimirovic%20and%20Canfer%20Akbulut%20and%20Jiaqi%20Mu%20and%20Wolfgang%20Macherey%20and%20Ravin%20Kumar%20and%20Jun%20Xu%20and%20Haroon%20Qureshi%20and%20Gheorghe%20Comanici%20and%20Jeremy%20Wiesner%20and%20Zhitao%20Gong%20and%20Anton%20Ruddock%20and%20Matthias%20Bauer%20and%20Nick%20Felt%20and%20Anirudh%20GP%20and%20Anurag%20Arnab%20and%20Dustin%20Zelle%20and%20Jonas%20Rothfuss%20and%20Bill%20Rosgen%20and%20Ashish%20Shenoy%20and%20Bryan%20Seybold%20and%20Xinjian%20Li%20and%20Jayaram%20Mudigonda%20and%20Goker%20Erdogan%20and%20Jiawei%20Xia%20and%20Jiri%20Simsa%20and%20Andrea%20Michi%20and%20Yi%20Yao%20and%20Christopher%20Yew%20and%20Steven%20Kan%20and%20Isaac%20Caswell%20and%20Carey%20Radebaugh%20and%20Andre%20Elisseeff%20and%20Pedro%20Valenzuela%20and%20Kay%20McKinney%20and%20Kim%20Paterson%20and%20Albert%20Cui%20and%20Eri%20Latorre-Chimoto%20and%20Solomon%20Kim%20and%20William%20Zeng%20and%20Ken%20Durden%20and%20Priya%20Ponnapalli%20and%20Tiberiu%20Sosea%20and%20Christopher%20A.%20Choquette-Choo%20and%20James%20Manyika%20and%20Brona%20Robenek%20and%20Harsha%20Vashisht%20and%20Sebastien%20Pereira%20and%20Hoi%20Lam%20and%20Marko%20Velic%20and%20Denese%20Owusu-Afriyie%20and%20Katherine%20Lee%20and%20Tolga%20Bolukbasi%20and%20Alicia%20Parrish%20and%20Shawn%20Lu%20and%20Jane%20Park%20and%20Balaji%20Venkatraman%20and%20Alice%20Talbert%20and%20Lambert%20Rosique%20and%20Yuchung%20Cheng%20and%20Andrei%20Sozanschi%20and%20Adam%20Paszke%20and%20Praveen%20Kumar%20and%20Jessica%20Austin%20and%20Lu%20Li%20and%20Khalid%20Salama%20and%20Bartek%20Perz%20and%20Wooyeol%20Kim%20and%20Nandita%20Dukkipati%20and%20Anthony%20Baryshnikov%20and%20Christos%20Kaplanis%20and%20XiangHai%20Sheng%20and%20Yuri%20Chervonyi%20and%20Caglar%20Unlu%20and%20Diego%20de%20Las%20Casas%20and%20Harry%20Askham%20and%20Kathryn%20Tunyasuvunakool%20and%20Felix%20Gimeno%20and%20Siim%20Poder%20and%20Chester%20Kwak%20and%20Matt%20Miecnikowski%20and%20Vahab%20Mirrokni%20and%20Alek%20Dimitriev%20and%20Aaron%20Parisi%20and%20Dangyi%20Liu%20and%20Tomy%20Tsai%20and%20Toby%20Shevlane%20and%20Christina%20Kouridi%20and%20Drew%20Garmon%20and%20Adrian%20Goedeckemeyer%20and%20Adam%20R.%20Brown%20and%20Anitha%20Vijayakumar%20and%20Ali%20Elqursh%20and%20Sadegh%20Jazayeri%20and%20Jin%20Huang%20and%20Sara%20Mc%20Carthy%20and%20Jay%20Hoover%20and%20Lucy%20Kim%20and%20Sandeep%20Kumar%20and%20Wei%20Chen%20and%20Courtney%20Biles%20and%20Garrett%20Bingham%20and%20Evan%20Rosen%20and%20Lisa%20Wang%20and%20Qijun%20Tan%20and%20David%20Engel%20and%20Francesco%20Pongetti%20and%20Dario%20de%20Cesare%20and%20Dongseong%20Hwang%20and%20Lily%20Yu%20and%20Jennifer%20Pullman%20and%20Srini%20Narayanan%20and%20Kyle%20Levin%20and%20Siddharth%20Gopal%20and%20Megan%20Li%20and%20Asaf%20Aharoni%20and%20Trieu%20Trinh%20and%20Jessica%20Lo%20and%20Norman%20Casagrande%20and%20Roopali%20Vij%20and%20Loic%20Matthey%20and%20Bramandia%20Ramadhana%20and%20Austin%20Matthews%20and%20CJ%20Carey%20and%20Matthew%20Johnson%20and%20Kremena%20Goranova%20and%20Rohin%20Shah%20and%20Shereen%20Ashraf%20and%20Kingshuk%20Dasgupta%20and%20Rasmus%20Larsen%20and%20Yicheng%20Wang%20and%20Manish%20Reddy%20Vuyyuru%20and%20Chong%20Jiang%20and%20Joana%20Ijazi%20and%20Kazuki%20Osawa%20and%20Celine%20Smith%20and%20Ramya%20Sree%20Boppana%20and%20Taylan%20Bilal%20and%20Yuma%20Koizumi%20and%20Ying%20Xu%20and%20Yasemin%20Altun%20and%20Nir%20Shabat%20and%20Ben%20Bariach%20and%20Alex%20Korchemniy%20and%20Kiam%20Choo%20and%20Olaf%20Ronneberger%20and%20Chimezie%20Iwuanyanwu%20and%20Shubin%20Zhao%20and%20David%20Soergel%20and%20Cho-Jui%20Hsieh%20and%20Irene%20Cai%20and%20Shariq%20Iqbal%20and%20Martin%20Sundermeyer%20and%20Zhe%20Chen%20and%20Elie%20Bursztein%20and%20Chaitanya%20Malaviya%20and%20Fadi%20Biadsy%20and%20Prakash%20Shroff%20and%20Inderjit%20Dhillon%20and%20Tejasi%20Latkar%20and%20Chris%20Dyer%20and%20Hannah%20Forbes%20and%20Massimo%20Nicosia%20and%20Vitaly%20Nikolaev%20and%20Somer%20Greene%20and%20Marin%20Georgiev%20and%20Pidong%20Wang%20and%20Nina%20Martin%20and%20Hanie%20Sedghi%20and%20John%20Zhang%20and%20Praseem%20Banzal%20and%20Doug%20Fritz%20and%20Vikram%20Rao%20and%20Xuezhi%20Wang%20and%20Jiageng%20Zhang%20and%20Viorica%20Patraucean%20and%20Dayou%20Du%20and%20Igor%20Mordatch%20and%20Ivan%20Jurin%20and%20Lewis%20Liu%20and%20Ayush%20Dubey%20and%20Abhi%20Mohan%20and%20Janek%20Nowakowski%20and%20Vlad-Doru%20Ion%20and%20Nan%20Wei%20and%20Reiko%20Tojo%20and%20Maria%20Abi%20Raad%20and%20Drew%20A.%20Hudson%20and%20Vaishakh%20Keshava%20and%20Shubham%20Agrawal%20and%20Kevin%20Ramirez%20and%20Zhichun%20Wu%20and%20Hoang%20Nguyen%20and%20Ji%20Liu%20and%20Madhavi%20Sewak%20and%20Bryce%20Petrini%20and%20DongHyun%20Choi%20and%20Ivan%20Philips%20and%20Ziyue%20Wang%20and%20Ioana%20Bica%20and%20Ankush%20Garg%20and%20Jarek%20Wilkiewicz%20and%20Priyanka%20Agrawal%20and%20Xiaowei%20Li%20and%20Danhao%20Guo%20and%20Emily%20Xue%20and%20Naseer%20Shaik%20and%20Andrew%20Leach%20and%20Sadh%20MNM%20Khan%20and%20Julia%20Wiesinger%20and%20Sammy%20Jerome%20and%20Abhishek%20Chakladar%20and%20Alek%20Wenjiao%20Wang%20and%20Tina%20Ornduff%20and%20Folake%20Abu%20and%20Alireza%20Ghaffarkhah%20and%20Marcus%20Wainwright%20and%20Mario%20Cortes%20and%20Frederick%20Liu%20and%20Joshua%20Maynez%20and%20Andreas%20Terzis%20and%20Pouya%20Samangouei%20and%20Riham%20Mansour%20and%20Tomasz%20K%C4%99pa%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Anton%20Algymr%20and%20Dan%20Banica%20and%20Agoston%20Weisz%20and%20Andras%20Orban%20and%20Alexandre%20Senges%20and%20Ewa%20Andrejczuk%20and%20Mark%20Geller%20and%20Niccolo%20Dal%20Santo%20and%20Valentin%20Anklin%20and%20Majd%20Al%20Merey%20and%20Martin%20Baeuml%20and%20Trevor%20Strohman%20and%20Junwen%20Bai%20and%20Slav%20Petrov%20and%20Yonghui%20Wu%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Jeff%20Dean%20and%20Oriol%20Vinyals%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%201.5%20family%20of%20models%2C%20representing%0Athe%20next%20generation%20of%20highly%20compute-efficient%20multimodal%20models%20capable%20of%0Arecalling%20and%20reasoning%20over%20fine-grained%20information%20from%20millions%20of%20tokens%0Aof%20context%2C%20including%20multiple%20long%20documents%20and%20hours%20of%20video%20and%20audio.%20The%0Afamily%20includes%20two%20new%20models%3A%20%281%29%20an%20updated%20Gemini%201.5%20Pro%2C%20which%20exceeds%0Athe%20February%20version%20on%20the%20great%20majority%20of%20capabilities%20and%20benchmarks%3B%20%282%29%0AGemini%201.5%20Flash%2C%20a%20more%20lightweight%20variant%20designed%20for%20efficiency%20with%0Aminimal%20regression%20in%20quality.%20Gemini%201.5%20models%20achieve%20near-perfect%20recall%20on%0Along-context%20retrieval%20tasks%20across%20modalities%2C%20improve%20the%20state-of-the-art%20in%0Along-document%20QA%2C%20long-video%20QA%20and%20long-context%20ASR%2C%20and%20match%20or%20surpass%0AGemini%201.0%20Ultra%27s%20state-of-the-art%20performance%20across%20a%20broad%20set%20of%0Abenchmarks.%20Studying%20the%20limits%20of%20Gemini%201.5%27s%20long-context%20ability%2C%20we%20find%0Acontinued%20improvement%20in%20next-token%20prediction%20and%20near-perfect%20retrieval%0A%28%3E99%25%29%20up%20to%20at%20least%2010M%20tokens%2C%20a%20generational%20leap%20over%20existing%20models%20such%0Aas%20Claude%203.0%20%28200k%29%20and%20GPT-4%20Turbo%20%28128k%29.%20Finally%2C%20we%20highlight%20real-world%0Ause%20cases%2C%20such%20as%20Gemini%201.5%20collaborating%20with%20professionals%20on%20completing%0Atheir%20tasks%20achieving%2026%20to%2075%25%20time%20savings%20across%2010%20different%20job%0Acategories%2C%20as%20well%20as%20surprising%20new%20capabilities%20of%20large%20language%20models%20at%0Athe%20frontier%3B%20when%20given%20a%20grammar%20manual%20for%20Kalamang%2C%20a%20language%20with%20fewer%0Athan%20200%20speakers%20worldwide%2C%20the%20model%20learns%20to%20translate%20English%20to%20Kalamang%0Aat%20a%20similar%20level%20to%20a%20person%20who%20learned%20from%20the%20same%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05530v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemini%25201.5%253A%2520Unlocking%2520multimodal%2520understanding%2520across%2520millions%2520of%2520tokens%250A%2520%2520of%2520context%26entry.906535625%3D%2520Gemini%2520Team%2520and%2520Petko%2520Georgiev%2520and%2520Ving%2520Ian%2520Lei%2520and%2520Ryan%2520Burnell%2520and%2520Libin%2520Bai%2520and%2520Anmol%2520Gulati%2520and%2520Garrett%2520Tanzer%2520and%2520Damien%2520Vincent%2520and%2520Zhufeng%2520Pan%2520and%2520Shibo%2520Wang%2520and%2520Soroosh%2520Mariooryad%2520and%2520Yifan%2520Ding%2520and%2520Xinyang%2520Geng%2520and%2520Fred%2520Alcober%2520and%2520Roy%2520Frostig%2520and%2520Mark%2520Omernick%2520and%2520Lexi%2520Walker%2520and%2520Cosmin%2520Paduraru%2520and%2520Christina%2520Sorokin%2520and%2520Andrea%2520Tacchetti%2520and%2520Colin%2520Gaffney%2520and%2520Samira%2520Daruki%2520and%2520Olcan%2520Sercinoglu%2520and%2520Zach%2520Gleicher%2520and%2520Juliette%2520Love%2520and%2520Paul%2520Voigtlaender%2520and%2520Rohan%2520Jain%2520and%2520Gabriela%2520Surita%2520and%2520Kareem%2520Mohamed%2520and%2520Rory%2520Blevins%2520and%2520Junwhan%2520Ahn%2520and%2520Tao%2520Zhu%2520and%2520Kornraphop%2520Kawintiranon%2520and%2520Orhan%2520Firat%2520and%2520Yiming%2520Gu%2520and%2520Yujing%2520Zhang%2520and%2520Matthew%2520Rahtz%2520and%2520Manaal%2520Faruqui%2520and%2520Natalie%2520Clay%2520and%2520Justin%2520Gilmer%2520and%2520JD%2520Co-Reyes%2520and%2520Ivo%2520Penchev%2520and%2520Rui%2520Zhu%2520and%2520Nobuyuki%2520Morioka%2520and%2520Kevin%2520Hui%2520and%2520Krishna%2520Haridasan%2520and%2520Victor%2520Campos%2520and%2520Mahdis%2520Mahdieh%2520and%2520Mandy%2520Guo%2520and%2520Samer%2520Hassan%2520and%2520Kevin%2520Kilgour%2520and%2520Arpi%2520Vezer%2520and%2520Heng-Tze%2520Cheng%2520and%2520Raoul%2520de%2520Liedekerke%2520and%2520Siddharth%2520Goyal%2520and%2520Paul%2520Barham%2520and%2520DJ%2520Strouse%2520and%2520Seb%2520Noury%2520and%2520Jonas%2520Adler%2520and%2520Mukund%2520Sundararajan%2520and%2520Sharad%2520Vikram%2520and%2520Dmitry%2520Lepikhin%2520and%2520Michela%2520Paganini%2520and%2520Xavier%2520Garcia%2520and%2520Fan%2520Yang%2520and%2520Dasha%2520Valter%2520and%2520Maja%2520Trebacz%2520and%2520Kiran%2520Vodrahalli%2520and%2520Chulayuth%2520Asawaroengchai%2520and%2520Roman%2520Ring%2520and%2520Norbert%2520Kalb%2520and%2520Livio%2520Baldini%2520Soares%2520and%2520Siddhartha%2520Brahma%2520and%2520David%2520Steiner%2520and%2520Tianhe%2520Yu%2520and%2520Fabian%2520Mentzer%2520and%2520Antoine%2520He%2520and%2520Lucas%2520Gonzalez%2520and%2520Bibo%2520Xu%2520and%2520Raphael%2520Lopez%2520Kaufman%2520and%2520Laurent%2520El%2520Shafey%2520and%2520Junhyuk%2520Oh%2520and%2520Tom%2520Hennigan%2520and%2520George%2520van%2520den%2520Driessche%2520and%2520Seth%2520Odoom%2520and%2520Mario%2520Lucic%2520and%2520Becca%2520Roelofs%2520and%2520Sid%2520Lall%2520and%2520Amit%2520Marathe%2520and%2520Betty%2520Chan%2520and%2520Santiago%2520Ontanon%2520and%2520Luheng%2520He%2520and%2520Denis%2520Teplyashin%2520and%2520Jonathan%2520Lai%2520and%2520Phil%2520Crone%2520and%2520Bogdan%2520Damoc%2520and%2520Lewis%2520Ho%2520and%2520Sebastian%2520Riedel%2520and%2520Karel%2520Lenc%2520and%2520Chih-Kuan%2520Yeh%2520and%2520Aakanksha%2520Chowdhery%2520and%2520Yang%2520Xu%2520and%2520Mehran%2520Kazemi%2520and%2520Ehsan%2520Amid%2520and%2520Anastasia%2520Petrushkina%2520and%2520Kevin%2520Swersky%2520and%2520Ali%2520Khodaei%2520and%2520Gowoon%2520Chen%2520and%2520Chris%2520Larkin%2520and%2520Mario%2520Pinto%2520and%2520Geng%2520Yan%2520and%2520Adria%2520Puigdomenech%2520Badia%2520and%2520Piyush%2520Patil%2520and%2520Steven%2520Hansen%2520and%2520Dave%2520Orr%2520and%2520Sebastien%2520M.%2520R.%2520Arnold%2520and%2520Jordan%2520Grimstad%2520and%2520Andrew%2520Dai%2520and%2520Sholto%2520Douglas%2520and%2520Rishika%2520Sinha%2520and%2520Vikas%2520Yadav%2520and%2520Xi%2520Chen%2520and%2520Elena%2520Gribovskaya%2520and%2520Jacob%2520Austin%2520and%2520Jeffrey%2520Zhao%2520and%2520Kaushal%2520Patel%2520and%2520Paul%2520Komarek%2520and%2520Sophia%2520Austin%2520and%2520Sebastian%2520Borgeaud%2520and%2520Linda%2520Friso%2520and%2520Abhimanyu%2520Goyal%2520and%2520Ben%2520Caine%2520and%2520Kris%2520Cao%2520and%2520Da-Woon%2520Chung%2520and%2520Matthew%2520Lamm%2520and%2520Gabe%2520Barth-Maron%2520and%2520Thais%2520Kagohara%2520and%2520Kate%2520Olszewska%2520and%2520Mia%2520Chen%2520and%2520Kaushik%2520Shivakumar%2520and%2520Rishabh%2520Agarwal%2520and%2520Harshal%2520Godhia%2520and%2520Ravi%2520Rajwar%2520and%2520Javier%2520Snaider%2520and%2520Xerxes%2520Dotiwalla%2520and%2520Yuan%2520Liu%2520and%2520Aditya%2520Barua%2520and%2520Victor%2520Ungureanu%2520and%2520Yuan%2520Zhang%2520and%2520Bat-Orgil%2520Batsaikhan%2520and%2520Mateo%2520Wirth%2520and%2520James%2520Qin%2520and%2520Ivo%2520Danihelka%2520and%2520Tulsee%2520Doshi%2520and%2520Martin%2520Chadwick%2520and%2520Jilin%2520Chen%2520and%2520Sanil%2520Jain%2520and%2520Quoc%2520Le%2520and%2520Arjun%2520Kar%2520and%2520Madhu%2520Gurumurthy%2520and%2520Cheng%2520Li%2520and%2520Ruoxin%2520Sang%2520and%2520Fangyu%2520Liu%2520and%2520Lampros%2520Lamprou%2520and%2520Rich%2520Munoz%2520and%2520Nathan%2520Lintz%2520and%2520Harsh%2520Mehta%2520and%2520Heidi%2520Howard%2520and%2520Malcolm%2520Reynolds%2520and%2520Lora%2520Aroyo%2520and%2520Quan%2520Wang%2520and%2520Lorenzo%2520Blanco%2520and%2520Albin%2520Cassirer%2520and%2520Jordan%2520Griffith%2520and%2520Dipanjan%2520Das%2520and%2520Stephan%2520Lee%2520and%2520Jakub%2520Sygnowski%2520and%2520Zach%2520Fisher%2520and%2520James%2520Besley%2520and%2520Richard%2520Powell%2520and%2520Zafarali%2520Ahmed%2520and%2520Dominik%2520Paulus%2520and%2520David%2520Reitter%2520and%2520Zalan%2520Borsos%2520and%2520Rishabh%2520Joshi%2520and%2520Aedan%2520Pope%2520and%2520Steven%2520Hand%2520and%2520Vittorio%2520Selo%2520and%2520Vihan%2520Jain%2520and%2520Nikhil%2520Sethi%2520and%2520Megha%2520Goel%2520and%2520Takaki%2520Makino%2520and%2520Rhys%2520May%2520and%2520Zhen%2520Yang%2520and%2520Johan%2520Schalkwyk%2520and%2520Christina%2520Butterfield%2520and%2520Anja%2520Hauth%2520and%2520Alex%2520Goldin%2520and%2520Will%2520Hawkins%2520and%2520Evan%2520Senter%2520and%2520Sergey%2520Brin%2520and%2520Oliver%2520Woodman%2520and%2520Marvin%2520Ritter%2520and%2520Eric%2520Noland%2520and%2520Minh%2520Giang%2520and%2520Vijay%2520Bolina%2520and%2520Lisa%2520Lee%2520and%2520Tim%2520Blyth%2520and%2520Ian%2520Mackinnon%2520and%2520Machel%2520Reid%2520and%2520Obaid%2520Sarvana%2520and%2520David%2520Silver%2520and%2520Alexander%2520Chen%2520and%2520Lily%2520Wang%2520and%2520Loren%2520Maggiore%2520and%2520Oscar%2520Chang%2520and%2520Nithya%2520Attaluri%2520and%2520Gregory%2520Thornton%2520and%2520Chung-Cheng%2520Chiu%2520and%2520Oskar%2520Bunyan%2520and%2520Nir%2520Levine%2520and%2520Timothy%2520Chung%2520and%2520Evgenii%2520Eltyshev%2520and%2520Xiance%2520Si%2520and%2520Timothy%2520Lillicrap%2520and%2520Demetra%2520Brady%2520and%2520Vaibhav%2520Aggarwal%2520and%2520Boxi%2520Wu%2520and%2520Yuanzhong%2520Xu%2520and%2520Ross%2520McIlroy%2520and%2520Kartikeya%2520Badola%2520and%2520Paramjit%2520Sandhu%2520and%2520Erica%2520Moreira%2520and%2520Wojciech%2520Stokowiec%2520and%2520Ross%2520Hemsley%2520and%2520Dong%2520Li%2520and%2520Alex%2520Tudor%2520and%2520Pranav%2520Shyam%2520and%2520Elahe%2520Rahimtoroghi%2520and%2520Salem%2520Haykal%2520and%2520Pablo%2520Sprechmann%2520and%2520Xiang%2520Zhou%2520and%2520Diana%2520Mincu%2520and%2520Yujia%2520Li%2520and%2520Ravi%2520Addanki%2520and%2520Kalpesh%2520Krishna%2520and%2520Xiao%2520Wu%2520and%2520Alexandre%2520Frechette%2520and%2520Matan%2520Eyal%2520and%2520Allan%2520Dafoe%2520and%2520Dave%2520Lacey%2520and%2520Jay%2520Whang%2520and%2520Thi%2520Avrahami%2520and%2520Ye%2520Zhang%2520and%2520Emanuel%2520Taropa%2520and%2520Hanzhao%2520Lin%2520and%2520Daniel%2520Toyama%2520and%2520Eliza%2520Rutherford%2520and%2520Motoki%2520Sano%2520and%2520HyunJeong%2520Choe%2520and%2520Alex%2520Tomala%2520and%2520Chalence%2520Safranek-Shrader%2520and%2520Nora%2520Kassner%2520and%2520Mantas%2520Pajarskas%2520and%2520Matt%2520Harvey%2520and%2520Sean%2520Sechrist%2520and%2520Meire%2520Fortunato%2520and%2520Christina%2520Lyu%2520and%2520Gamaleldin%2520Elsayed%2520and%2520Chenkai%2520Kuang%2520and%2520James%2520Lottes%2520and%2520Eric%2520Chu%2520and%2520Chao%2520Jia%2520and%2520Chih-Wei%2520Chen%2520and%2520Peter%2520Humphreys%2520and%2520Kate%2520Baumli%2520and%2520Connie%2520Tao%2520and%2520Rajkumar%2520Samuel%2520and%2520Cicero%2520Nogueira%2520dos%2520Santos%2520and%2520Anders%2520Andreassen%2520and%2520Nemanja%2520Raki%25C4%2587evi%25C4%2587%2520and%2520Dominik%2520Grewe%2520and%2520Aviral%2520Kumar%2520and%2520Stephanie%2520Winkler%2520and%2520Jonathan%2520Caton%2520and%2520Andrew%2520Brock%2520and%2520Sid%2520Dalmia%2520and%2520Hannah%2520Sheahan%2520and%2520Iain%2520Barr%2520and%2520Yingjie%2520Miao%2520and%2520Paul%2520Natsev%2520and%2520Jacob%2520Devlin%2520and%2520Feryal%2520Behbahani%2520and%2520Flavien%2520Prost%2520and%2520Yanhua%2520Sun%2520and%2520Artiom%2520Myaskovsky%2520and%2520Thanumalayan%2520Sankaranarayana%2520Pillai%2520and%2520Dan%2520Hurt%2520and%2520Angeliki%2520Lazaridou%2520and%2520Xi%2520Xiong%2520and%2520Ce%2520Zheng%2520and%2520Fabio%2520Pardo%2520and%2520Xiaowei%2520Li%2520and%2520Dan%2520Horgan%2520and%2520Joe%2520Stanton%2520and%2520Moran%2520Ambar%2520and%2520Fei%2520Xia%2520and%2520Alejandro%2520Lince%2520and%2520Mingqiu%2520Wang%2520and%2520Basil%2520Mustafa%2520and%2520Albert%2520Webson%2520and%2520Hyo%2520Lee%2520and%2520Rohan%2520Anil%2520and%2520Martin%2520Wicke%2520and%2520Timothy%2520Dozat%2520and%2520Abhishek%2520Sinha%2520and%2520Enrique%2520Piqueras%2520and%2520Elahe%2520Dabir%2520and%2520Shyam%2520Upadhyay%2520and%2520Anudhyan%2520Boral%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Corey%2520Fry%2520and%2520Josip%2520Djolonga%2520and%2520Yi%2520Su%2520and%2520Jake%2520Walker%2520and%2520Jane%2520Labanowski%2520and%2520Ronny%2520Huang%2520and%2520Vedant%2520Misra%2520and%2520Jeremy%2520Chen%2520and%2520RJ%2520Skerry-Ryan%2520and%2520Avi%2520Singh%2520and%2520Shruti%2520Rijhwani%2520and%2520Dian%2520Yu%2520and%2520Alex%2520Castro-Ros%2520and%2520Beer%2520Changpinyo%2520and%2520Romina%2520Datta%2520and%2520Sumit%2520Bagri%2520and%2520Arnar%2520Mar%2520Hrafnkelsson%2520and%2520Marcello%2520Maggioni%2520and%2520Daniel%2520Zheng%2520and%2520Yury%2520Sulsky%2520and%2520Shaobo%2520Hou%2520and%2520Tom%2520Le%2520Paine%2520and%2520Antoine%2520Yang%2520and%2520Jason%2520Riesa%2520and%2520Dominika%2520Rogozinska%2520and%2520Dror%2520Marcus%2520and%2520Dalia%2520El%2520Badawy%2520and%2520Qiao%2520Zhang%2520and%2520Luyu%2520Wang%2520and%2520Helen%2520Miller%2520and%2520Jeremy%2520Greer%2520and%2520Lars%2520Lowe%2520Sjos%2520and%2520Azade%2520Nova%2520and%2520Heiga%2520Zen%2520and%2520Rahma%2520Chaabouni%2520and%2520Mihaela%2520Rosca%2520and%2520Jiepu%2520Jiang%2520and%2520Charlie%2520Chen%2520and%2520Ruibo%2520Liu%2520and%2520Tara%2520Sainath%2520and%2520Maxim%2520Krikun%2520and%2520Alex%2520Polozov%2520and%2520Jean-Baptiste%2520Lespiau%2520and%2520Josh%2520Newlan%2520and%2520Zeyncep%2520Cankara%2520and%2520Soo%2520Kwak%2520and%2520Yunhan%2520Xu%2520and%2520Phil%2520Chen%2520and%2520Andy%2520Coenen%2520and%2520Clemens%2520Meyer%2520and%2520Katerina%2520Tsihlas%2520and%2520Ada%2520Ma%2520and%2520Juraj%2520Gottweis%2520and%2520Jinwei%2520Xing%2520and%2520Chenjie%2520Gu%2520and%2520Jin%2520Miao%2520and%2520Christian%2520Frank%2520and%2520Zeynep%2520Cankara%2520and%2520Sanjay%2520Ganapathy%2520and%2520Ishita%2520Dasgupta%2520and%2520Steph%2520Hughes-Fitt%2520and%2520Heng%2520Chen%2520and%2520David%2520Reid%2520and%2520Keran%2520Rong%2520and%2520Hongmin%2520Fan%2520and%2520Joost%2520van%2520Amersfoort%2520and%2520Vincent%2520Zhuang%2520and%2520Aaron%2520Cohen%2520and%2520Shixiang%2520Shane%2520Gu%2520and%2520Anhad%2520Mohananey%2520and%2520Anastasija%2520Ilic%2520and%2520Taylor%2520Tobin%2520and%2520John%2520Wieting%2520and%2520Anna%2520Bortsova%2520and%2520Phoebe%2520Thacker%2520and%2520Emma%2520Wang%2520and%2520Emily%2520Caveness%2520and%2520Justin%2520Chiu%2520and%2520Eren%2520Sezener%2520and%2520Alex%2520Kaskasoli%2520and%2520Steven%2520Baker%2520and%2520Katie%2520Millican%2520and%2520Mohamed%2520Elhawaty%2520and%2520Kostas%2520Aisopos%2520and%2520Carl%2520Lebsack%2520and%2520Nathan%2520Byrd%2520and%2520Hanjun%2520Dai%2520and%2520Wenhao%2520Jia%2520and%2520Matthew%2520Wiethoff%2520and%2520Elnaz%2520Davoodi%2520and%2520Albert%2520Weston%2520and%2520Lakshman%2520Yagati%2520and%2520Arun%2520Ahuja%2520and%2520Isabel%2520Gao%2520and%2520Golan%2520Pundak%2520and%2520Susan%2520Zhang%2520and%2520Michael%2520Azzam%2520and%2520Khe%2520Chai%2520Sim%2520and%2520Sergi%2520Caelles%2520and%2520James%2520Keeling%2520and%2520Abhanshu%2520Sharma%2520and%2520Andy%2520Swing%2520and%2520YaGuang%2520Li%2520and%2520Chenxi%2520Liu%2520and%2520Carrie%2520Grimes%2520Bostock%2520and%2520Yamini%2520Bansal%2520and%2520Zachary%2520Nado%2520and%2520Ankesh%2520Anand%2520and%2520Josh%2520Lipschultz%2520and%2520Abhijit%2520Karmarkar%2520and%2520Lev%2520Proleev%2520and%2520Abe%2520Ittycheriah%2520and%2520Soheil%2520Hassas%2520Yeganeh%2520and%2520George%2520Polovets%2520and%2520Aleksandra%2520Faust%2520and%2520Jiao%2520Sun%2520and%2520Alban%2520Rrustemi%2520and%2520Pen%2520Li%2520and%2520Rakesh%2520Shivanna%2520and%2520Jeremiah%2520Liu%2520and%2520Chris%2520Welty%2520and%2520Federico%2520Lebron%2520and%2520Anirudh%2520Baddepudi%2520and%2520Sebastian%2520Krause%2520and%2520Emilio%2520Parisotto%2520and%2520Radu%2520Soricut%2520and%2520Zheng%2520Xu%2520and%2520Dawn%2520Bloxwich%2520and%2520Melvin%2520Johnson%2520and%2520Behnam%2520Neyshabur%2520and%2520Justin%2520Mao-Jones%2520and%2520Renshen%2520Wang%2520and%2520Vinay%2520Ramasesh%2520and%2520Zaheer%2520Abbas%2520and%2520Arthur%2520Guez%2520and%2520Constant%2520Segal%2520and%2520Duc%2520Dung%2520Nguyen%2520and%2520James%2520Svensson%2520and%2520Le%2520Hou%2520and%2520Sarah%2520York%2520and%2520Kieran%2520Milan%2520and%2520Sophie%2520Bridgers%2520and%2520Wiktor%2520Gworek%2520and%2520Marco%2520Tagliasacchi%2520and%2520James%2520Lee-Thorp%2520and%2520Michael%2520Chang%2520and%2520Alexey%2520Guseynov%2520and%2520Ale%2520Jakse%2520Hartman%2520and%2520Michael%2520Kwong%2520and%2520Ruizhe%2520Zhao%2520and%2520Sheleem%2520Kashem%2520and%2520Elizabeth%2520Cole%2520and%2520Antoine%2520Miech%2520and%2520Richard%2520Tanburn%2520and%2520Mary%2520Phuong%2520and%2520Filip%2520Pavetic%2520and%2520Sebastien%2520Cevey%2520and%2520Ramona%2520Comanescu%2520and%2520Richard%2520Ives%2520and%2520Sherry%2520Yang%2520and%2520Cosmo%2520Du%2520and%2520Bo%2520Li%2520and%2520Zizhao%2520Zhang%2520and%2520Mariko%2520Iinuma%2520and%2520Clara%2520Huiyi%2520Hu%2520and%2520Aurko%2520Roy%2520and%2520Shaan%2520Bijwadia%2520and%2520Zhenkai%2520Zhu%2520and%2520Danilo%2520Martins%2520and%2520Rachel%2520Saputro%2520and%2520Anita%2520Gergely%2520and%2520Steven%2520Zheng%2520and%2520Dawei%2520Jia%2520and%2520Ioannis%2520Antonoglou%2520and%2520Adam%2520Sadovsky%2520and%2520Shane%2520Gu%2520and%2520Yingying%2520Bi%2520and%2520Alek%2520Andreev%2520and%2520Sina%2520Samangooei%2520and%2520Mina%2520Khan%2520and%2520Tomas%2520Kocisky%2520and%2520Angelos%2520Filos%2520and%2520Chintu%2520Kumar%2520and%2520Colton%2520Bishop%2520and%2520Adams%2520Yu%2520and%2520Sarah%2520Hodkinson%2520and%2520Sid%2520Mittal%2520and%2520Premal%2520Shah%2520and%2520Alexandre%2520Moufarek%2520and%2520Yong%2520Cheng%2520and%2520Adam%2520Bloniarz%2520and%2520Jaehoon%2520Lee%2520and%2520Pedram%2520Pejman%2520and%2520Paul%2520Michel%2520and%2520Stephen%2520Spencer%2520and%2520Vladimir%2520Feinberg%2520and%2520Xuehan%2520Xiong%2520and%2520Nikolay%2520Savinov%2520and%2520Charlotte%2520Smith%2520and%2520Siamak%2520Shakeri%2520and%2520Dustin%2520Tran%2520and%2520Mary%2520Chesus%2520and%2520Bernd%2520Bohnet%2520and%2520George%2520Tucker%2520and%2520Tamara%2520von%2520Glehn%2520and%2520Carrie%2520Muir%2520and%2520Yiran%2520Mao%2520and%2520Hideto%2520Kazawa%2520and%2520Ambrose%2520Slone%2520and%2520Kedar%2520Soparkar%2520and%2520Disha%2520Shrivastava%2520and%2520James%2520Cobon-Kerr%2520and%2520Michael%2520Sharman%2520and%2520Jay%2520Pavagadhi%2520and%2520Carlos%2520Araya%2520and%2520Karolis%2520Misiunas%2520and%2520Nimesh%2520Ghelani%2520and%2520Michael%2520Laskin%2520and%2520David%2520Barker%2520and%2520Qiujia%2520Li%2520and%2520Anton%2520Briukhov%2520and%2520Neil%2520Houlsby%2520and%2520Mia%2520Glaese%2520and%2520Balaji%2520Lakshminarayanan%2520and%2520Nathan%2520Schucher%2520and%2520Yunhao%2520Tang%2520and%2520Eli%2520Collins%2520and%2520Hyeontaek%2520Lim%2520and%2520Fangxiaoyu%2520Feng%2520and%2520Adria%2520Recasens%2520and%2520Guangda%2520Lai%2520and%2520Alberto%2520Magni%2520and%2520Nicola%2520De%2520Cao%2520and%2520Aditya%2520Siddhant%2520and%2520Zoe%2520Ashwood%2520and%2520Jordi%2520Orbay%2520and%2520Mostafa%2520Dehghani%2520and%2520Jenny%2520Brennan%2520and%2520Yifan%2520He%2520and%2520Kelvin%2520Xu%2520and%2520Yang%2520Gao%2520and%2520Carl%2520Saroufim%2520and%2520James%2520Molloy%2520and%2520Xinyi%2520Wu%2520and%2520Seb%2520Arnold%2520and%2520Solomon%2520Chang%2520and%2520Julian%2520Schrittwieser%2520and%2520Elena%2520Buchatskaya%2520and%2520Soroush%2520Radpour%2520and%2520Martin%2520Polacek%2520and%2520Skye%2520Giordano%2520and%2520Ankur%2520Bapna%2520and%2520Simon%2520Tokumine%2520and%2520Vincent%2520Hellendoorn%2520and%2520Thibault%2520Sottiaux%2520and%2520Sarah%2520Cogan%2520and%2520Aliaksei%2520Severyn%2520and%2520Mohammad%2520Saleh%2520and%2520Shantanu%2520Thakoor%2520and%2520Laurent%2520Shefey%2520and%2520Siyuan%2520Qiao%2520and%2520Meenu%2520Gaba%2520and%2520Shuo-yiin%2520Chang%2520and%2520Craig%2520Swanson%2520and%2520Biao%2520Zhang%2520and%2520Benjamin%2520Lee%2520and%2520Paul%2520Kishan%2520Rubenstein%2520and%2520Gan%2520Song%2520and%2520Tom%2520Kwiatkowski%2520and%2520Anna%2520Koop%2520and%2520Ajay%2520Kannan%2520and%2520David%2520Kao%2520and%2520Parker%2520Schuh%2520and%2520Axel%2520Stjerngren%2520and%2520Golnaz%2520Ghiasi%2520and%2520Gena%2520Gibson%2520and%2520Luke%2520Vilnis%2520and%2520Ye%2520Yuan%2520and%2520Felipe%2520Tiengo%2520Ferreira%2520and%2520Aishwarya%2520Kamath%2520and%2520Ted%2520Klimenko%2520and%2520Ken%2520Franko%2520and%2520Kefan%2520Xiao%2520and%2520Indro%2520Bhattacharya%2520and%2520Miteyan%2520Patel%2520and%2520Rui%2520Wang%2520and%2520Alex%2520Morris%2520and%2520Robin%2520Strudel%2520and%2520Vivek%2520Sharma%2520and%2520Peter%2520Choy%2520and%2520Sayed%2520Hadi%2520Hashemi%2520and%2520Jessica%2520Landon%2520and%2520Mara%2520Finkelstein%2520and%2520Priya%2520Jhakra%2520and%2520Justin%2520Frye%2520and%2520Megan%2520Barnes%2520and%2520Matthew%2520Mauger%2520and%2520Dennis%2520Daun%2520and%2520Khuslen%2520Baatarsukh%2520and%2520Matthew%2520Tung%2520and%2520Wael%2520Farhan%2520and%2520Henryk%2520Michalewski%2520and%2520Fabio%2520Viola%2520and%2520Felix%2520de%2520Chaumont%2520Quitry%2520and%2520Charline%2520Le%2520Lan%2520and%2520Tom%2520Hudson%2520and%2520Qingze%2520Wang%2520and%2520Felix%2520Fischer%2520and%2520Ivy%2520Zheng%2520and%2520Elspeth%2520White%2520and%2520Anca%2520Dragan%2520and%2520Jean-baptiste%2520Alayrac%2520and%2520Eric%2520Ni%2520and%2520Alexander%2520Pritzel%2520and%2520Adam%2520Iwanicki%2520and%2520Michael%2520Isard%2520and%2520Anna%2520Bulanova%2520and%2520Lukas%2520Zilka%2520and%2520Ethan%2520Dyer%2520and%2520Devendra%2520Sachan%2520and%2520Srivatsan%2520Srinivasan%2520and%2520Hannah%2520Muckenhirn%2520and%2520Honglong%2520Cai%2520and%2520Amol%2520Mandhane%2520and%2520Mukarram%2520Tariq%2520and%2520Jack%2520W.%2520Rae%2520and%2520Gary%2520Wang%2520and%2520Kareem%2520Ayoub%2520and%2520Nicholas%2520FitzGerald%2520and%2520Yao%2520Zhao%2520and%2520Woohyun%2520Han%2520and%2520Chris%2520Alberti%2520and%2520Dan%2520Garrette%2520and%2520Kashyap%2520Krishnakumar%2520and%2520Mai%2520Gimenez%2520and%2520Anselm%2520Levskaya%2520and%2520Daniel%2520Sohn%2520and%2520Josip%2520Matak%2520and%2520Inaki%2520Iturrate%2520and%2520Michael%2520B.%2520Chang%2520and%2520Jackie%2520Xiang%2520and%2520Yuan%2520Cao%2520and%2520Nishant%2520Ranka%2520and%2520Geoff%2520Brown%2520and%2520Adrian%2520Hutter%2520and%2520Vahab%2520Mirrokni%2520and%2520Nanxin%2520Chen%2520and%2520Kaisheng%2520Yao%2520and%2520Zoltan%2520Egyed%2520and%2520Francois%2520Galilee%2520and%2520Tyler%2520Liechty%2520and%2520Praveen%2520Kallakuri%2520and%2520Evan%2520Palmer%2520and%2520Sanjay%2520Ghemawat%2520and%2520Jasmine%2520Liu%2520and%2520David%2520Tao%2520and%2520Chloe%2520Thornton%2520and%2520Tim%2520Green%2520and%2520Mimi%2520Jasarevic%2520and%2520Sharon%2520Lin%2520and%2520Victor%2520Cotruta%2520and%2520Yi-Xuan%2520Tan%2520and%2520Noah%2520Fiedel%2520and%2520Hongkun%2520Yu%2520and%2520Ed%2520Chi%2520and%2520Alexander%2520Neitz%2520and%2520Jens%2520Heitkaemper%2520and%2520Anu%2520Sinha%2520and%2520Denny%2520Zhou%2520and%2520Yi%2520Sun%2520and%2520Charbel%2520Kaed%2520and%2520Brice%2520Hulse%2520and%2520Swaroop%2520Mishra%2520and%2520Maria%2520Georgaki%2520and%2520Sneha%2520Kudugunta%2520and%2520Clement%2520Farabet%2520and%2520Izhak%2520Shafran%2520and%2520Daniel%2520Vlasic%2520and%2520Anton%2520Tsitsulin%2520and%2520Rajagopal%2520Ananthanarayanan%2520and%2520Alen%2520Carin%2520and%2520Guolong%2520Su%2520and%2520Pei%2520Sun%2520and%2520Shashank%2520V%2520and%2520Gabriel%2520Carvajal%2520and%2520Josef%2520Broder%2520and%2520Iulia%2520Comsa%2520and%2520Alena%2520Repina%2520and%2520William%2520Wong%2520and%2520Warren%2520Weilun%2520Chen%2520and%2520Peter%2520Hawkins%2520and%2520Egor%2520Filonov%2520and%2520Lucia%2520Loher%2520and%2520Christoph%2520Hirnschall%2520and%2520Weiyi%2520Wang%2520and%2520Jingchen%2520Ye%2520and%2520Andrea%2520Burns%2520and%2520Hardie%2520Cate%2520and%2520Diana%2520Gage%2520Wright%2520and%2520Federico%2520Piccinini%2520and%2520Lei%2520Zhang%2520and%2520Chu-Cheng%2520Lin%2520and%2520Ionel%2520Gog%2520and%2520Yana%2520Kulizhskaya%2520and%2520Ashwin%2520Sreevatsa%2520and%2520Shuang%2520Song%2520and%2520Luis%2520C.%2520Cobo%2520and%2520Anand%2520Iyer%2520and%2520Chetan%2520Tekur%2520and%2520Guillermo%2520Garrido%2520and%2520Zhuyun%2520Xiao%2520and%2520Rupert%2520Kemp%2520and%2520Huaixiu%2520Steven%2520Zheng%2520and%2520Hui%2520Li%2520and%2520Ananth%2520Agarwal%2520and%2520Christel%2520Ngani%2520and%2520Kati%2520Goshvadi%2520and%2520Rebeca%2520Santamaria-Fernandez%2520and%2520Wojciech%2520Fica%2520and%2520Xinyun%2520Chen%2520and%2520Chris%2520Gorgolewski%2520and%2520Sean%2520Sun%2520and%2520Roopal%2520Garg%2520and%2520Xinyu%2520Ye%2520and%2520S.%2520M.%2520Ali%2520Eslami%2520and%2520Nan%2520Hua%2520and%2520Jon%2520Simon%2520and%2520Pratik%2520Joshi%2520and%2520Yelin%2520Kim%2520and%2520Ian%2520Tenney%2520and%2520Sahitya%2520Potluri%2520and%2520Lam%2520Nguyen%2520Thiet%2520and%2520Quan%2520Yuan%2520and%2520Florian%2520Luisier%2520and%2520Alexandra%2520Chronopoulou%2520and%2520Salvatore%2520Scellato%2520and%2520Praveen%2520Srinivasan%2520and%2520Minmin%2520Chen%2520and%2520Vinod%2520Koverkathu%2520and%2520Valentin%2520Dalibard%2520and%2520Yaming%2520Xu%2520and%2520Brennan%2520Saeta%2520and%2520Keith%2520Anderson%2520and%2520Thibault%2520Sellam%2520and%2520Nick%2520Fernando%2520and%2520Fantine%2520Huot%2520and%2520Junehyuk%2520Jung%2520and%2520Mani%2520Varadarajan%2520and%2520Michael%2520Quinn%2520and%2520Amit%2520Raul%2520and%2520Maigo%2520Le%2520and%2520Ruslan%2520Habalov%2520and%2520Jon%2520Clark%2520and%2520Komal%2520Jalan%2520and%2520Kalesha%2520Bullard%2520and%2520Achintya%2520Singhal%2520and%2520Thang%2520Luong%2520and%2520Boyu%2520Wang%2520and%2520Sujeevan%2520Rajayogam%2520and%2520Julian%2520Eisenschlos%2520and%2520Johnson%2520Jia%2520and%2520Daniel%2520Finchelstein%2520and%2520Alex%2520Yakubovich%2520and%2520Daniel%2520Balle%2520and%2520Michael%2520Fink%2520and%2520Sameer%2520Agarwal%2520and%2520Jing%2520Li%2520and%2520Dj%2520Dvijotham%2520and%2520Shalini%2520Pal%2520and%2520Kai%2520Kang%2520and%2520Jaclyn%2520Konzelmann%2520and%2520Jennifer%2520Beattie%2520and%2520Olivier%2520Dousse%2520and%2520Diane%2520Wu%2520and%2520Remi%2520Crocker%2520and%2520Chen%2520Elkind%2520and%2520Siddhartha%2520Reddy%2520Jonnalagadda%2520and%2520Jong%2520Lee%2520and%2520Dan%2520Holtmann-Rice%2520and%2520Krystal%2520Kallarackal%2520and%2520Rosanne%2520Liu%2520and%2520Denis%2520Vnukov%2520and%2520Neera%2520Vats%2520and%2520Luca%2520Invernizzi%2520and%2520Mohsen%2520Jafari%2520and%2520Huanjie%2520Zhou%2520and%2520Lilly%2520Taylor%2520and%2520Jennifer%2520Prendki%2520and%2520Marcus%2520Wu%2520and%2520Tom%2520Eccles%2520and%2520Tianqi%2520Liu%2520and%2520Kavya%2520Kopparapu%2520and%2520Francoise%2520Beaufays%2520and%2520Christof%2520Angermueller%2520and%2520Andreea%2520Marzoca%2520and%2520Shourya%2520Sarcar%2520and%2520Hilal%2520Dib%2520and%2520Jeff%2520Stanway%2520and%2520Frank%2520Perbet%2520and%2520Nejc%2520Trdin%2520and%2520Rachel%2520Sterneck%2520and%2520Andrey%2520Khorlin%2520and%2520Dinghua%2520Li%2520and%2520Xihui%2520Wu%2520and%2520Sonam%2520Goenka%2520and%2520David%2520Madras%2520and%2520Sasha%2520Goldshtein%2520and%2520Willi%2520Gierke%2520and%2520Tong%2520Zhou%2520and%2520Yaxin%2520Liu%2520and%2520Yannie%2520Liang%2520and%2520Anais%2520White%2520and%2520Yunjie%2520Li%2520and%2520Shreya%2520Singh%2520and%2520Sanaz%2520Bahargam%2520and%2520Mark%2520Epstein%2520and%2520Sujoy%2520Basu%2520and%2520Li%2520Lao%2520and%2520Adnan%2520Ozturel%2520and%2520Carl%2520Crous%2520and%2520Alex%2520Zhai%2520and%2520Han%2520Lu%2520and%2520Zora%2520Tung%2520and%2520Neeraj%2520Gaur%2520and%2520Alanna%2520Walton%2520and%2520Lucas%2520Dixon%2520and%2520Ming%2520Zhang%2520and%2520Amir%2520Globerson%2520and%2520Grant%2520Uy%2520and%2520Andrew%2520Bolt%2520and%2520Olivia%2520Wiles%2520and%2520Milad%2520Nasr%2520and%2520Ilia%2520Shumailov%2520and%2520Marco%2520Selvi%2520and%2520Francesco%2520Piccinno%2520and%2520Ricardo%2520Aguilar%2520and%2520Sara%2520McCarthy%2520and%2520Misha%2520Khalman%2520and%2520Mrinal%2520Shukla%2520and%2520Vlado%2520Galic%2520and%2520John%2520Carpenter%2520and%2520Kevin%2520Villela%2520and%2520Haibin%2520Zhang%2520and%2520Harry%2520Richardson%2520and%2520James%2520Martens%2520and%2520Matko%2520Bosnjak%2520and%2520Shreyas%2520Rammohan%2520Belle%2520and%2520Jeff%2520Seibert%2520and%2520Mahmoud%2520Alnahlawi%2520and%2520Brian%2520McWilliams%2520and%2520Sankalp%2520Singh%2520and%2520Annie%2520Louis%2520and%2520Wen%2520Ding%2520and%2520Dan%2520Popovici%2520and%2520Lenin%2520Simicich%2520and%2520Laura%2520Knight%2520and%2520Pulkit%2520Mehta%2520and%2520Nishesh%2520Gupta%2520and%2520Chongyang%2520Shi%2520and%2520Saaber%2520Fatehi%2520and%2520Jovana%2520Mitrovic%2520and%2520Alex%2520Grills%2520and%2520Joseph%2520Pagadora%2520and%2520Tsendsuren%2520Munkhdalai%2520and%2520Dessie%2520Petrova%2520and%2520Danielle%2520Eisenbud%2520and%2520Zhishuai%2520Zhang%2520and%2520Damion%2520Yates%2520and%2520Bhavishya%2520Mittal%2520and%2520Nilesh%2520Tripuraneni%2520and%2520Yannis%2520Assael%2520and%2520Thomas%2520Brovelli%2520and%2520Prateek%2520Jain%2520and%2520Mihajlo%2520Velimirovic%2520and%2520Canfer%2520Akbulut%2520and%2520Jiaqi%2520Mu%2520and%2520Wolfgang%2520Macherey%2520and%2520Ravin%2520Kumar%2520and%2520Jun%2520Xu%2520and%2520Haroon%2520Qureshi%2520and%2520Gheorghe%2520Comanici%2520and%2520Jeremy%2520Wiesner%2520and%2520Zhitao%2520Gong%2520and%2520Anton%2520Ruddock%2520and%2520Matthias%2520Bauer%2520and%2520Nick%2520Felt%2520and%2520Anirudh%2520GP%2520and%2520Anurag%2520Arnab%2520and%2520Dustin%2520Zelle%2520and%2520Jonas%2520Rothfuss%2520and%2520Bill%2520Rosgen%2520and%2520Ashish%2520Shenoy%2520and%2520Bryan%2520Seybold%2520and%2520Xinjian%2520Li%2520and%2520Jayaram%2520Mudigonda%2520and%2520Goker%2520Erdogan%2520and%2520Jiawei%2520Xia%2520and%2520Jiri%2520Simsa%2520and%2520Andrea%2520Michi%2520and%2520Yi%2520Yao%2520and%2520Christopher%2520Yew%2520and%2520Steven%2520Kan%2520and%2520Isaac%2520Caswell%2520and%2520Carey%2520Radebaugh%2520and%2520Andre%2520Elisseeff%2520and%2520Pedro%2520Valenzuela%2520and%2520Kay%2520McKinney%2520and%2520Kim%2520Paterson%2520and%2520Albert%2520Cui%2520and%2520Eri%2520Latorre-Chimoto%2520and%2520Solomon%2520Kim%2520and%2520William%2520Zeng%2520and%2520Ken%2520Durden%2520and%2520Priya%2520Ponnapalli%2520and%2520Tiberiu%2520Sosea%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520James%2520Manyika%2520and%2520Brona%2520Robenek%2520and%2520Harsha%2520Vashisht%2520and%2520Sebastien%2520Pereira%2520and%2520Hoi%2520Lam%2520and%2520Marko%2520Velic%2520and%2520Denese%2520Owusu-Afriyie%2520and%2520Katherine%2520Lee%2520and%2520Tolga%2520Bolukbasi%2520and%2520Alicia%2520Parrish%2520and%2520Shawn%2520Lu%2520and%2520Jane%2520Park%2520and%2520Balaji%2520Venkatraman%2520and%2520Alice%2520Talbert%2520and%2520Lambert%2520Rosique%2520and%2520Yuchung%2520Cheng%2520and%2520Andrei%2520Sozanschi%2520and%2520Adam%2520Paszke%2520and%2520Praveen%2520Kumar%2520and%2520Jessica%2520Austin%2520and%2520Lu%2520Li%2520and%2520Khalid%2520Salama%2520and%2520Bartek%2520Perz%2520and%2520Wooyeol%2520Kim%2520and%2520Nandita%2520Dukkipati%2520and%2520Anthony%2520Baryshnikov%2520and%2520Christos%2520Kaplanis%2520and%2520XiangHai%2520Sheng%2520and%2520Yuri%2520Chervonyi%2520and%2520Caglar%2520Unlu%2520and%2520Diego%2520de%2520Las%2520Casas%2520and%2520Harry%2520Askham%2520and%2520Kathryn%2520Tunyasuvunakool%2520and%2520Felix%2520Gimeno%2520and%2520Siim%2520Poder%2520and%2520Chester%2520Kwak%2520and%2520Matt%2520Miecnikowski%2520and%2520Vahab%2520Mirrokni%2520and%2520Alek%2520Dimitriev%2520and%2520Aaron%2520Parisi%2520and%2520Dangyi%2520Liu%2520and%2520Tomy%2520Tsai%2520and%2520Toby%2520Shevlane%2520and%2520Christina%2520Kouridi%2520and%2520Drew%2520Garmon%2520and%2520Adrian%2520Goedeckemeyer%2520and%2520Adam%2520R.%2520Brown%2520and%2520Anitha%2520Vijayakumar%2520and%2520Ali%2520Elqursh%2520and%2520Sadegh%2520Jazayeri%2520and%2520Jin%2520Huang%2520and%2520Sara%2520Mc%2520Carthy%2520and%2520Jay%2520Hoover%2520and%2520Lucy%2520Kim%2520and%2520Sandeep%2520Kumar%2520and%2520Wei%2520Chen%2520and%2520Courtney%2520Biles%2520and%2520Garrett%2520Bingham%2520and%2520Evan%2520Rosen%2520and%2520Lisa%2520Wang%2520and%2520Qijun%2520Tan%2520and%2520David%2520Engel%2520and%2520Francesco%2520Pongetti%2520and%2520Dario%2520de%2520Cesare%2520and%2520Dongseong%2520Hwang%2520and%2520Lily%2520Yu%2520and%2520Jennifer%2520Pullman%2520and%2520Srini%2520Narayanan%2520and%2520Kyle%2520Levin%2520and%2520Siddharth%2520Gopal%2520and%2520Megan%2520Li%2520and%2520Asaf%2520Aharoni%2520and%2520Trieu%2520Trinh%2520and%2520Jessica%2520Lo%2520and%2520Norman%2520Casagrande%2520and%2520Roopali%2520Vij%2520and%2520Loic%2520Matthey%2520and%2520Bramandia%2520Ramadhana%2520and%2520Austin%2520Matthews%2520and%2520CJ%2520Carey%2520and%2520Matthew%2520Johnson%2520and%2520Kremena%2520Goranova%2520and%2520Rohin%2520Shah%2520and%2520Shereen%2520Ashraf%2520and%2520Kingshuk%2520Dasgupta%2520and%2520Rasmus%2520Larsen%2520and%2520Yicheng%2520Wang%2520and%2520Manish%2520Reddy%2520Vuyyuru%2520and%2520Chong%2520Jiang%2520and%2520Joana%2520Ijazi%2520and%2520Kazuki%2520Osawa%2520and%2520Celine%2520Smith%2520and%2520Ramya%2520Sree%2520Boppana%2520and%2520Taylan%2520Bilal%2520and%2520Yuma%2520Koizumi%2520and%2520Ying%2520Xu%2520and%2520Yasemin%2520Altun%2520and%2520Nir%2520Shabat%2520and%2520Ben%2520Bariach%2520and%2520Alex%2520Korchemniy%2520and%2520Kiam%2520Choo%2520and%2520Olaf%2520Ronneberger%2520and%2520Chimezie%2520Iwuanyanwu%2520and%2520Shubin%2520Zhao%2520and%2520David%2520Soergel%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Irene%2520Cai%2520and%2520Shariq%2520Iqbal%2520and%2520Martin%2520Sundermeyer%2520and%2520Zhe%2520Chen%2520and%2520Elie%2520Bursztein%2520and%2520Chaitanya%2520Malaviya%2520and%2520Fadi%2520Biadsy%2520and%2520Prakash%2520Shroff%2520and%2520Inderjit%2520Dhillon%2520and%2520Tejasi%2520Latkar%2520and%2520Chris%2520Dyer%2520and%2520Hannah%2520Forbes%2520and%2520Massimo%2520Nicosia%2520and%2520Vitaly%2520Nikolaev%2520and%2520Somer%2520Greene%2520and%2520Marin%2520Georgiev%2520and%2520Pidong%2520Wang%2520and%2520Nina%2520Martin%2520and%2520Hanie%2520Sedghi%2520and%2520John%2520Zhang%2520and%2520Praseem%2520Banzal%2520and%2520Doug%2520Fritz%2520and%2520Vikram%2520Rao%2520and%2520Xuezhi%2520Wang%2520and%2520Jiageng%2520Zhang%2520and%2520Viorica%2520Patraucean%2520and%2520Dayou%2520Du%2520and%2520Igor%2520Mordatch%2520and%2520Ivan%2520Jurin%2520and%2520Lewis%2520Liu%2520and%2520Ayush%2520Dubey%2520and%2520Abhi%2520Mohan%2520and%2520Janek%2520Nowakowski%2520and%2520Vlad-Doru%2520Ion%2520and%2520Nan%2520Wei%2520and%2520Reiko%2520Tojo%2520and%2520Maria%2520Abi%2520Raad%2520and%2520Drew%2520A.%2520Hudson%2520and%2520Vaishakh%2520Keshava%2520and%2520Shubham%2520Agrawal%2520and%2520Kevin%2520Ramirez%2520and%2520Zhichun%2520Wu%2520and%2520Hoang%2520Nguyen%2520and%2520Ji%2520Liu%2520and%2520Madhavi%2520Sewak%2520and%2520Bryce%2520Petrini%2520and%2520DongHyun%2520Choi%2520and%2520Ivan%2520Philips%2520and%2520Ziyue%2520Wang%2520and%2520Ioana%2520Bica%2520and%2520Ankush%2520Garg%2520and%2520Jarek%2520Wilkiewicz%2520and%2520Priyanka%2520Agrawal%2520and%2520Xiaowei%2520Li%2520and%2520Danhao%2520Guo%2520and%2520Emily%2520Xue%2520and%2520Naseer%2520Shaik%2520and%2520Andrew%2520Leach%2520and%2520Sadh%2520MNM%2520Khan%2520and%2520Julia%2520Wiesinger%2520and%2520Sammy%2520Jerome%2520and%2520Abhishek%2520Chakladar%2520and%2520Alek%2520Wenjiao%2520Wang%2520and%2520Tina%2520Ornduff%2520and%2520Folake%2520Abu%2520and%2520Alireza%2520Ghaffarkhah%2520and%2520Marcus%2520Wainwright%2520and%2520Mario%2520Cortes%2520and%2520Frederick%2520Liu%2520and%2520Joshua%2520Maynez%2520and%2520Andreas%2520Terzis%2520and%2520Pouya%2520Samangouei%2520and%2520Riham%2520Mansour%2520and%2520Tomasz%2520K%25C4%2599pa%2520and%2520Fran%25C3%25A7ois-Xavier%2520Aubet%2520and%2520Anton%2520Algymr%2520and%2520Dan%2520Banica%2520and%2520Agoston%2520Weisz%2520and%2520Andras%2520Orban%2520and%2520Alexandre%2520Senges%2520and%2520Ewa%2520Andrejczuk%2520and%2520Mark%2520Geller%2520and%2520Niccolo%2520Dal%2520Santo%2520and%2520Valentin%2520Anklin%2520and%2520Majd%2520Al%2520Merey%2520and%2520Martin%2520Baeuml%2520and%2520Trevor%2520Strohman%2520and%2520Junwen%2520Bai%2520and%2520Slav%2520Petrov%2520and%2520Yonghui%2520Wu%2520and%2520Demis%2520Hassabis%2520and%2520Koray%2520Kavukcuoglu%2520and%2520Jeff%2520Dean%2520and%2520Oriol%2520Vinyals%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520introduce%2520the%2520Gemini%25201.5%2520family%2520of%2520models%252C%2520representing%250Athe%2520next%2520generation%2520of%2520highly%2520compute-efficient%2520multimodal%2520models%2520capable%2520of%250Arecalling%2520and%2520reasoning%2520over%2520fine-grained%2520information%2520from%2520millions%2520of%2520tokens%250Aof%2520context%252C%2520including%2520multiple%2520long%2520documents%2520and%2520hours%2520of%2520video%2520and%2520audio.%2520The%250Afamily%2520includes%2520two%2520new%2520models%253A%2520%25281%2529%2520an%2520updated%2520Gemini%25201.5%2520Pro%252C%2520which%2520exceeds%250Athe%2520February%2520version%2520on%2520the%2520great%2520majority%2520of%2520capabilities%2520and%2520benchmarks%253B%2520%25282%2529%250AGemini%25201.5%2520Flash%252C%2520a%2520more%2520lightweight%2520variant%2520designed%2520for%2520efficiency%2520with%250Aminimal%2520regression%2520in%2520quality.%2520Gemini%25201.5%2520models%2520achieve%2520near-perfect%2520recall%2520on%250Along-context%2520retrieval%2520tasks%2520across%2520modalities%252C%2520improve%2520the%2520state-of-the-art%2520in%250Along-document%2520QA%252C%2520long-video%2520QA%2520and%2520long-context%2520ASR%252C%2520and%2520match%2520or%2520surpass%250AGemini%25201.0%2520Ultra%2527s%2520state-of-the-art%2520performance%2520across%2520a%2520broad%2520set%2520of%250Abenchmarks.%2520Studying%2520the%2520limits%2520of%2520Gemini%25201.5%2527s%2520long-context%2520ability%252C%2520we%2520find%250Acontinued%2520improvement%2520in%2520next-token%2520prediction%2520and%2520near-perfect%2520retrieval%250A%2528%253E99%2525%2529%2520up%2520to%2520at%2520least%252010M%2520tokens%252C%2520a%2520generational%2520leap%2520over%2520existing%2520models%2520such%250Aas%2520Claude%25203.0%2520%2528200k%2529%2520and%2520GPT-4%2520Turbo%2520%2528128k%2529.%2520Finally%252C%2520we%2520highlight%2520real-world%250Ause%2520cases%252C%2520such%2520as%2520Gemini%25201.5%2520collaborating%2520with%2520professionals%2520on%2520completing%250Atheir%2520tasks%2520achieving%252026%2520to%252075%2525%2520time%2520savings%2520across%252010%2520different%2520job%250Acategories%252C%2520as%2520well%2520as%2520surprising%2520new%2520capabilities%2520of%2520large%2520language%2520models%2520at%250Athe%2520frontier%253B%2520when%2520given%2520a%2520grammar%2520manual%2520for%2520Kalamang%252C%2520a%2520language%2520with%2520fewer%250Athan%2520200%2520speakers%2520worldwide%252C%2520the%2520model%2520learns%2520to%2520translate%2520English%2520to%2520Kalamang%250Aat%2520a%2520similar%2520level%2520to%2520a%2520person%2520who%2520learned%2520from%2520the%2520same%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05530v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context&entry.906535625=%20Gemini%20Team%20and%20Petko%20Georgiev%20and%20Ving%20Ian%20Lei%20and%20Ryan%20Burnell%20and%20Libin%20Bai%20and%20Anmol%20Gulati%20and%20Garrett%20Tanzer%20and%20Damien%20Vincent%20and%20Zhufeng%20Pan%20and%20Shibo%20Wang%20and%20Soroosh%20Mariooryad%20and%20Yifan%20Ding%20and%20Xinyang%20Geng%20and%20Fred%20Alcober%20and%20Roy%20Frostig%20and%20Mark%20Omernick%20and%20Lexi%20Walker%20and%20Cosmin%20Paduraru%20and%20Christina%20Sorokin%20and%20Andrea%20Tacchetti%20and%20Colin%20Gaffney%20and%20Samira%20Daruki%20and%20Olcan%20Sercinoglu%20and%20Zach%20Gleicher%20and%20Juliette%20Love%20and%20Paul%20Voigtlaender%20and%20Rohan%20Jain%20and%20Gabriela%20Surita%20and%20Kareem%20Mohamed%20and%20Rory%20Blevins%20and%20Junwhan%20Ahn%20and%20Tao%20Zhu%20and%20Kornraphop%20Kawintiranon%20and%20Orhan%20Firat%20and%20Yiming%20Gu%20and%20Yujing%20Zhang%20and%20Matthew%20Rahtz%20and%20Manaal%20Faruqui%20and%20Natalie%20Clay%20and%20Justin%20Gilmer%20and%20JD%20Co-Reyes%20and%20Ivo%20Penchev%20and%20Rui%20Zhu%20and%20Nobuyuki%20Morioka%20and%20Kevin%20Hui%20and%20Krishna%20Haridasan%20and%20Victor%20Campos%20and%20Mahdis%20Mahdieh%20and%20Mandy%20Guo%20and%20Samer%20Hassan%20and%20Kevin%20Kilgour%20and%20Arpi%20Vezer%20and%20Heng-Tze%20Cheng%20and%20Raoul%20de%20Liedekerke%20and%20Siddharth%20Goyal%20and%20Paul%20Barham%20and%20DJ%20Strouse%20and%20Seb%20Noury%20and%20Jonas%20Adler%20and%20Mukund%20Sundararajan%20and%20Sharad%20Vikram%20and%20Dmitry%20Lepikhin%20and%20Michela%20Paganini%20and%20Xavier%20Garcia%20and%20Fan%20Yang%20and%20Dasha%20Valter%20and%20Maja%20Trebacz%20and%20Kiran%20Vodrahalli%20and%20Chulayuth%20Asawaroengchai%20and%20Roman%20Ring%20and%20Norbert%20Kalb%20and%20Livio%20Baldini%20Soares%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Tianhe%20Yu%20and%20Fabian%20Mentzer%20and%20Antoine%20He%20and%20Lucas%20Gonzalez%20and%20Bibo%20Xu%20and%20Raphael%20Lopez%20Kaufman%20and%20Laurent%20El%20Shafey%20and%20Junhyuk%20Oh%20and%20Tom%20Hennigan%20and%20George%20van%20den%20Driessche%20and%20Seth%20Odoom%20and%20Mario%20Lucic%20and%20Becca%20Roelofs%20and%20Sid%20Lall%20and%20Amit%20Marathe%20and%20Betty%20Chan%20and%20Santiago%20Ontanon%20and%20Luheng%20He%20and%20Denis%20Teplyashin%20and%20Jonathan%20Lai%20and%20Phil%20Crone%20and%20Bogdan%20Damoc%20and%20Lewis%20Ho%20and%20Sebastian%20Riedel%20and%20Karel%20Lenc%20and%20Chih-Kuan%20Yeh%20and%20Aakanksha%20Chowdhery%20and%20Yang%20Xu%20and%20Mehran%20Kazemi%20and%20Ehsan%20Amid%20and%20Anastasia%20Petrushkina%20and%20Kevin%20Swersky%20and%20Ali%20Khodaei%20and%20Gowoon%20Chen%20and%20Chris%20Larkin%20and%20Mario%20Pinto%20and%20Geng%20Yan%20and%20Adria%20Puigdomenech%20Badia%20and%20Piyush%20Patil%20and%20Steven%20Hansen%20and%20Dave%20Orr%20and%20Sebastien%20M.%20R.%20Arnold%20and%20Jordan%20Grimstad%20and%20Andrew%20Dai%20and%20Sholto%20Douglas%20and%20Rishika%20Sinha%20and%20Vikas%20Yadav%20and%20Xi%20Chen%20and%20Elena%20Gribovskaya%20and%20Jacob%20Austin%20and%20Jeffrey%20Zhao%20and%20Kaushal%20Patel%20and%20Paul%20Komarek%20and%20Sophia%20Austin%20and%20Sebastian%20Borgeaud%20and%20Linda%20Friso%20and%20Abhimanyu%20Goyal%20and%20Ben%20Caine%20and%20Kris%20Cao%20and%20Da-Woon%20Chung%20and%20Matthew%20Lamm%20and%20Gabe%20Barth-Maron%20and%20Thais%20Kagohara%20and%20Kate%20Olszewska%20and%20Mia%20Chen%20and%20Kaushik%20Shivakumar%20and%20Rishabh%20Agarwal%20and%20Harshal%20Godhia%20and%20Ravi%20Rajwar%20and%20Javier%20Snaider%20and%20Xerxes%20Dotiwalla%20and%20Yuan%20Liu%20and%20Aditya%20Barua%20and%20Victor%20Ungureanu%20and%20Yuan%20Zhang%20and%20Bat-Orgil%20Batsaikhan%20and%20Mateo%20Wirth%20and%20James%20Qin%20and%20Ivo%20Danihelka%20and%20Tulsee%20Doshi%20and%20Martin%20Chadwick%20and%20Jilin%20Chen%20and%20Sanil%20Jain%20and%20Quoc%20Le%20and%20Arjun%20Kar%20and%20Madhu%20Gurumurthy%20and%20Cheng%20Li%20and%20Ruoxin%20Sang%20and%20Fangyu%20Liu%20and%20Lampros%20Lamprou%20and%20Rich%20Munoz%20and%20Nathan%20Lintz%20and%20Harsh%20Mehta%20and%20Heidi%20Howard%20and%20Malcolm%20Reynolds%20and%20Lora%20Aroyo%20and%20Quan%20Wang%20and%20Lorenzo%20Blanco%20and%20Albin%20Cassirer%20and%20Jordan%20Griffith%20and%20Dipanjan%20Das%20and%20Stephan%20Lee%20and%20Jakub%20Sygnowski%20and%20Zach%20Fisher%20and%20James%20Besley%20and%20Richard%20Powell%20and%20Zafarali%20Ahmed%20and%20Dominik%20Paulus%20and%20David%20Reitter%20and%20Zalan%20Borsos%20and%20Rishabh%20Joshi%20and%20Aedan%20Pope%20and%20Steven%20Hand%20and%20Vittorio%20Selo%20and%20Vihan%20Jain%20and%20Nikhil%20Sethi%20and%20Megha%20Goel%20and%20Takaki%20Makino%20and%20Rhys%20May%20and%20Zhen%20Yang%20and%20Johan%20Schalkwyk%20and%20Christina%20Butterfield%20and%20Anja%20Hauth%20and%20Alex%20Goldin%20and%20Will%20Hawkins%20and%20Evan%20Senter%20and%20Sergey%20Brin%20and%20Oliver%20Woodman%20and%20Marvin%20Ritter%20and%20Eric%20Noland%20and%20Minh%20Giang%20and%20Vijay%20Bolina%20and%20Lisa%20Lee%20and%20Tim%20Blyth%20and%20Ian%20Mackinnon%20and%20Machel%20Reid%20and%20Obaid%20Sarvana%20and%20David%20Silver%20and%20Alexander%20Chen%20and%20Lily%20Wang%20and%20Loren%20Maggiore%20and%20Oscar%20Chang%20and%20Nithya%20Attaluri%20and%20Gregory%20Thornton%20and%20Chung-Cheng%20Chiu%20and%20Oskar%20Bunyan%20and%20Nir%20Levine%20and%20Timothy%20Chung%20and%20Evgenii%20Eltyshev%20and%20Xiance%20Si%20and%20Timothy%20Lillicrap%20and%20Demetra%20Brady%20and%20Vaibhav%20Aggarwal%20and%20Boxi%20Wu%20and%20Yuanzhong%20Xu%20and%20Ross%20McIlroy%20and%20Kartikeya%20Badola%20and%20Paramjit%20Sandhu%20and%20Erica%20Moreira%20and%20Wojciech%20Stokowiec%20and%20Ross%20Hemsley%20and%20Dong%20Li%20and%20Alex%20Tudor%20and%20Pranav%20Shyam%20and%20Elahe%20Rahimtoroghi%20and%20Salem%20Haykal%20and%20Pablo%20Sprechmann%20and%20Xiang%20Zhou%20and%20Diana%20Mincu%20and%20Yujia%20Li%20and%20Ravi%20Addanki%20and%20Kalpesh%20Krishna%20and%20Xiao%20Wu%20and%20Alexandre%20Frechette%20and%20Matan%20Eyal%20and%20Allan%20Dafoe%20and%20Dave%20Lacey%20and%20Jay%20Whang%20and%20Thi%20Avrahami%20and%20Ye%20Zhang%20and%20Emanuel%20Taropa%20and%20Hanzhao%20Lin%20and%20Daniel%20Toyama%20and%20Eliza%20Rutherford%20and%20Motoki%20Sano%20and%20HyunJeong%20Choe%20and%20Alex%20Tomala%20and%20Chalence%20Safranek-Shrader%20and%20Nora%20Kassner%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Sean%20Sechrist%20and%20Meire%20Fortunato%20and%20Christina%20Lyu%20and%20Gamaleldin%20Elsayed%20and%20Chenkai%20Kuang%20and%20James%20Lottes%20and%20Eric%20Chu%20and%20Chao%20Jia%20and%20Chih-Wei%20Chen%20and%20Peter%20Humphreys%20and%20Kate%20Baumli%20and%20Connie%20Tao%20and%20Rajkumar%20Samuel%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Anders%20Andreassen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Dominik%20Grewe%20and%20Aviral%20Kumar%20and%20Stephanie%20Winkler%20and%20Jonathan%20Caton%20and%20Andrew%20Brock%20and%20Sid%20Dalmia%20and%20Hannah%20Sheahan%20and%20Iain%20Barr%20and%20Yingjie%20Miao%20and%20Paul%20Natsev%20and%20Jacob%20Devlin%20and%20Feryal%20Behbahani%20and%20Flavien%20Prost%20and%20Yanhua%20Sun%20and%20Artiom%20Myaskovsky%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Dan%20Hurt%20and%20Angeliki%20Lazaridou%20and%20Xi%20Xiong%20and%20Ce%20Zheng%20and%20Fabio%20Pardo%20and%20Xiaowei%20Li%20and%20Dan%20Horgan%20and%20Joe%20Stanton%20and%20Moran%20Ambar%20and%20Fei%20Xia%20and%20Alejandro%20Lince%20and%20Mingqiu%20Wang%20and%20Basil%20Mustafa%20and%20Albert%20Webson%20and%20Hyo%20Lee%20and%20Rohan%20Anil%20and%20Martin%20Wicke%20and%20Timothy%20Dozat%20and%20Abhishek%20Sinha%20and%20Enrique%20Piqueras%20and%20Elahe%20Dabir%20and%20Shyam%20Upadhyay%20and%20Anudhyan%20Boral%20and%20Lisa%20Anne%20Hendricks%20and%20Corey%20Fry%20and%20Josip%20Djolonga%20and%20Yi%20Su%20and%20Jake%20Walker%20and%20Jane%20Labanowski%20and%20Ronny%20Huang%20and%20Vedant%20Misra%20and%20Jeremy%20Chen%20and%20RJ%20Skerry-Ryan%20and%20Avi%20Singh%20and%20Shruti%20Rijhwani%20and%20Dian%20Yu%20and%20Alex%20Castro-Ros%20and%20Beer%20Changpinyo%20and%20Romina%20Datta%20and%20Sumit%20Bagri%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Marcello%20Maggioni%20and%20Daniel%20Zheng%20and%20Yury%20Sulsky%20and%20Shaobo%20Hou%20and%20Tom%20Le%20Paine%20and%20Antoine%20Yang%20and%20Jason%20Riesa%20and%20Dominika%20Rogozinska%20and%20Dror%20Marcus%20and%20Dalia%20El%20Badawy%20and%20Qiao%20Zhang%20and%20Luyu%20Wang%20and%20Helen%20Miller%20and%20Jeremy%20Greer%20and%20Lars%20Lowe%20Sjos%20and%20Azade%20Nova%20and%20Heiga%20Zen%20and%20Rahma%20Chaabouni%20and%20Mihaela%20Rosca%20and%20Jiepu%20Jiang%20and%20Charlie%20Chen%20and%20Ruibo%20Liu%20and%20Tara%20Sainath%20and%20Maxim%20Krikun%20and%20Alex%20Polozov%20and%20Jean-Baptiste%20Lespiau%20and%20Josh%20Newlan%20and%20Zeyncep%20Cankara%20and%20Soo%20Kwak%20and%20Yunhan%20Xu%20and%20Phil%20Chen%20and%20Andy%20Coenen%20and%20Clemens%20Meyer%20and%20Katerina%20Tsihlas%20and%20Ada%20Ma%20and%20Juraj%20Gottweis%20and%20Jinwei%20Xing%20and%20Chenjie%20Gu%20and%20Jin%20Miao%20and%20Christian%20Frank%20and%20Zeynep%20Cankara%20and%20Sanjay%20Ganapathy%20and%20Ishita%20Dasgupta%20and%20Steph%20Hughes-Fitt%20and%20Heng%20Chen%20and%20David%20Reid%20and%20Keran%20Rong%20and%20Hongmin%20Fan%20and%20Joost%20van%20Amersfoort%20and%20Vincent%20Zhuang%20and%20Aaron%20Cohen%20and%20Shixiang%20Shane%20Gu%20and%20Anhad%20Mohananey%20and%20Anastasija%20Ilic%20and%20Taylor%20Tobin%20and%20John%20Wieting%20and%20Anna%20Bortsova%20and%20Phoebe%20Thacker%20and%20Emma%20Wang%20and%20Emily%20Caveness%20and%20Justin%20Chiu%20and%20Eren%20Sezener%20and%20Alex%20Kaskasoli%20and%20Steven%20Baker%20and%20Katie%20Millican%20and%20Mohamed%20Elhawaty%20and%20Kostas%20Aisopos%20and%20Carl%20Lebsack%20and%20Nathan%20Byrd%20and%20Hanjun%20Dai%20and%20Wenhao%20Jia%20and%20Matthew%20Wiethoff%20and%20Elnaz%20Davoodi%20and%20Albert%20Weston%20and%20Lakshman%20Yagati%20and%20Arun%20Ahuja%20and%20Isabel%20Gao%20and%20Golan%20Pundak%20and%20Susan%20Zhang%20and%20Michael%20Azzam%20and%20Khe%20Chai%20Sim%20and%20Sergi%20Caelles%20and%20James%20Keeling%20and%20Abhanshu%20Sharma%20and%20Andy%20Swing%20and%20YaGuang%20Li%20and%20Chenxi%20Liu%20and%20Carrie%20Grimes%20Bostock%20and%20Yamini%20Bansal%20and%20Zachary%20Nado%20and%20Ankesh%20Anand%20and%20Josh%20Lipschultz%20and%20Abhijit%20Karmarkar%20and%20Lev%20Proleev%20and%20Abe%20Ittycheriah%20and%20Soheil%20Hassas%20Yeganeh%20and%20George%20Polovets%20and%20Aleksandra%20Faust%20and%20Jiao%20Sun%20and%20Alban%20Rrustemi%20and%20Pen%20Li%20and%20Rakesh%20Shivanna%20and%20Jeremiah%20Liu%20and%20Chris%20Welty%20and%20Federico%20Lebron%20and%20Anirudh%20Baddepudi%20and%20Sebastian%20Krause%20and%20Emilio%20Parisotto%20and%20Radu%20Soricut%20and%20Zheng%20Xu%20and%20Dawn%20Bloxwich%20and%20Melvin%20Johnson%20and%20Behnam%20Neyshabur%20and%20Justin%20Mao-Jones%20and%20Renshen%20Wang%20and%20Vinay%20Ramasesh%20and%20Zaheer%20Abbas%20and%20Arthur%20Guez%20and%20Constant%20Segal%20and%20Duc%20Dung%20Nguyen%20and%20James%20Svensson%20and%20Le%20Hou%20and%20Sarah%20York%20and%20Kieran%20Milan%20and%20Sophie%20Bridgers%20and%20Wiktor%20Gworek%20and%20Marco%20Tagliasacchi%20and%20James%20Lee-Thorp%20and%20Michael%20Chang%20and%20Alexey%20Guseynov%20and%20Ale%20Jakse%20Hartman%20and%20Michael%20Kwong%20and%20Ruizhe%20Zhao%20and%20Sheleem%20Kashem%20and%20Elizabeth%20Cole%20and%20Antoine%20Miech%20and%20Richard%20Tanburn%20and%20Mary%20Phuong%20and%20Filip%20Pavetic%20and%20Sebastien%20Cevey%20and%20Ramona%20Comanescu%20and%20Richard%20Ives%20and%20Sherry%20Yang%20and%20Cosmo%20Du%20and%20Bo%20Li%20and%20Zizhao%20Zhang%20and%20Mariko%20Iinuma%20and%20Clara%20Huiyi%20Hu%20and%20Aurko%20Roy%20and%20Shaan%20Bijwadia%20and%20Zhenkai%20Zhu%20and%20Danilo%20Martins%20and%20Rachel%20Saputro%20and%20Anita%20Gergely%20and%20Steven%20Zheng%20and%20Dawei%20Jia%20and%20Ioannis%20Antonoglou%20and%20Adam%20Sadovsky%20and%20Shane%20Gu%20and%20Yingying%20Bi%20and%20Alek%20Andreev%20and%20Sina%20Samangooei%20and%20Mina%20Khan%20and%20Tomas%20Kocisky%20and%20Angelos%20Filos%20and%20Chintu%20Kumar%20and%20Colton%20Bishop%20and%20Adams%20Yu%20and%20Sarah%20Hodkinson%20and%20Sid%20Mittal%20and%20Premal%20Shah%20and%20Alexandre%20Moufarek%20and%20Yong%20Cheng%20and%20Adam%20Bloniarz%20and%20Jaehoon%20Lee%20and%20Pedram%20Pejman%20and%20Paul%20Michel%20and%20Stephen%20Spencer%20and%20Vladimir%20Feinberg%20and%20Xuehan%20Xiong%20and%20Nikolay%20Savinov%20and%20Charlotte%20Smith%20and%20Siamak%20Shakeri%20and%20Dustin%20Tran%20and%20Mary%20Chesus%20and%20Bernd%20Bohnet%20and%20George%20Tucker%20and%20Tamara%20von%20Glehn%20and%20Carrie%20Muir%20and%20Yiran%20Mao%20and%20Hideto%20Kazawa%20and%20Ambrose%20Slone%20and%20Kedar%20Soparkar%20and%20Disha%20Shrivastava%20and%20James%20Cobon-Kerr%20and%20Michael%20Sharman%20and%20Jay%20Pavagadhi%20and%20Carlos%20Araya%20and%20Karolis%20Misiunas%20and%20Nimesh%20Ghelani%20and%20Michael%20Laskin%20and%20David%20Barker%20and%20Qiujia%20Li%20and%20Anton%20Briukhov%20and%20Neil%20Houlsby%20and%20Mia%20Glaese%20and%20Balaji%20Lakshminarayanan%20and%20Nathan%20Schucher%20and%20Yunhao%20Tang%20and%20Eli%20Collins%20and%20Hyeontaek%20Lim%20and%20Fangxiaoyu%20Feng%20and%20Adria%20Recasens%20and%20Guangda%20Lai%20and%20Alberto%20Magni%20and%20Nicola%20De%20Cao%20and%20Aditya%20Siddhant%20and%20Zoe%20Ashwood%20and%20Jordi%20Orbay%20and%20Mostafa%20Dehghani%20and%20Jenny%20Brennan%20and%20Yifan%20He%20and%20Kelvin%20Xu%20and%20Yang%20Gao%20and%20Carl%20Saroufim%20and%20James%20Molloy%20and%20Xinyi%20Wu%20and%20Seb%20Arnold%20and%20Solomon%20Chang%20and%20Julian%20Schrittwieser%20and%20Elena%20Buchatskaya%20and%20Soroush%20Radpour%20and%20Martin%20Polacek%20and%20Skye%20Giordano%20and%20Ankur%20Bapna%20and%20Simon%20Tokumine%20and%20Vincent%20Hellendoorn%20and%20Thibault%20Sottiaux%20and%20Sarah%20Cogan%20and%20Aliaksei%20Severyn%20and%20Mohammad%20Saleh%20and%20Shantanu%20Thakoor%20and%20Laurent%20Shefey%20and%20Siyuan%20Qiao%20and%20Meenu%20Gaba%20and%20Shuo-yiin%20Chang%20and%20Craig%20Swanson%20and%20Biao%20Zhang%20and%20Benjamin%20Lee%20and%20Paul%20Kishan%20Rubenstein%20and%20Gan%20Song%20and%20Tom%20Kwiatkowski%20and%20Anna%20Koop%20and%20Ajay%20Kannan%20and%20David%20Kao%20and%20Parker%20Schuh%20and%20Axel%20Stjerngren%20and%20Golnaz%20Ghiasi%20and%20Gena%20Gibson%20and%20Luke%20Vilnis%20and%20Ye%20Yuan%20and%20Felipe%20Tiengo%20Ferreira%20and%20Aishwarya%20Kamath%20and%20Ted%20Klimenko%20and%20Ken%20Franko%20and%20Kefan%20Xiao%20and%20Indro%20Bhattacharya%20and%20Miteyan%20Patel%20and%20Rui%20Wang%20and%20Alex%20Morris%20and%20Robin%20Strudel%20and%20Vivek%20Sharma%20and%20Peter%20Choy%20and%20Sayed%20Hadi%20Hashemi%20and%20Jessica%20Landon%20and%20Mara%20Finkelstein%20and%20Priya%20Jhakra%20and%20Justin%20Frye%20and%20Megan%20Barnes%20and%20Matthew%20Mauger%20and%20Dennis%20Daun%20and%20Khuslen%20Baatarsukh%20and%20Matthew%20Tung%20and%20Wael%20Farhan%20and%20Henryk%20Michalewski%20and%20Fabio%20Viola%20and%20Felix%20de%20Chaumont%20Quitry%20and%20Charline%20Le%20Lan%20and%20Tom%20Hudson%20and%20Qingze%20Wang%20and%20Felix%20Fischer%20and%20Ivy%20Zheng%20and%20Elspeth%20White%20and%20Anca%20Dragan%20and%20Jean-baptiste%20Alayrac%20and%20Eric%20Ni%20and%20Alexander%20Pritzel%20and%20Adam%20Iwanicki%20and%20Michael%20Isard%20and%20Anna%20Bulanova%20and%20Lukas%20Zilka%20and%20Ethan%20Dyer%20and%20Devendra%20Sachan%20and%20Srivatsan%20Srinivasan%20and%20Hannah%20Muckenhirn%20and%20Honglong%20Cai%20and%20Amol%20Mandhane%20and%20Mukarram%20Tariq%20and%20Jack%20W.%20Rae%20and%20Gary%20Wang%20and%20Kareem%20Ayoub%20and%20Nicholas%20FitzGerald%20and%20Yao%20Zhao%20and%20Woohyun%20Han%20and%20Chris%20Alberti%20and%20Dan%20Garrette%20and%20Kashyap%20Krishnakumar%20and%20Mai%20Gimenez%20and%20Anselm%20Levskaya%20and%20Daniel%20Sohn%20and%20Josip%20Matak%20and%20Inaki%20Iturrate%20and%20Michael%20B.%20Chang%20and%20Jackie%20Xiang%20and%20Yuan%20Cao%20and%20Nishant%20Ranka%20and%20Geoff%20Brown%20and%20Adrian%20Hutter%20and%20Vahab%20Mirrokni%20and%20Nanxin%20Chen%20and%20Kaisheng%20Yao%20and%20Zoltan%20Egyed%20and%20Francois%20Galilee%20and%20Tyler%20Liechty%20and%20Praveen%20Kallakuri%20and%20Evan%20Palmer%20and%20Sanjay%20Ghemawat%20and%20Jasmine%20Liu%20and%20David%20Tao%20and%20Chloe%20Thornton%20and%20Tim%20Green%20and%20Mimi%20Jasarevic%20and%20Sharon%20Lin%20and%20Victor%20Cotruta%20and%20Yi-Xuan%20Tan%20and%20Noah%20Fiedel%20and%20Hongkun%20Yu%20and%20Ed%20Chi%20and%20Alexander%20Neitz%20and%20Jens%20Heitkaemper%20and%20Anu%20Sinha%20and%20Denny%20Zhou%20and%20Yi%20Sun%20and%20Charbel%20Kaed%20and%20Brice%20Hulse%20and%20Swaroop%20Mishra%20and%20Maria%20Georgaki%20and%20Sneha%20Kudugunta%20and%20Clement%20Farabet%20and%20Izhak%20Shafran%20and%20Daniel%20Vlasic%20and%20Anton%20Tsitsulin%20and%20Rajagopal%20Ananthanarayanan%20and%20Alen%20Carin%20and%20Guolong%20Su%20and%20Pei%20Sun%20and%20Shashank%20V%20and%20Gabriel%20Carvajal%20and%20Josef%20Broder%20and%20Iulia%20Comsa%20and%20Alena%20Repina%20and%20William%20Wong%20and%20Warren%20Weilun%20Chen%20and%20Peter%20Hawkins%20and%20Egor%20Filonov%20and%20Lucia%20Loher%20and%20Christoph%20Hirnschall%20and%20Weiyi%20Wang%20and%20Jingchen%20Ye%20and%20Andrea%20Burns%20and%20Hardie%20Cate%20and%20Diana%20Gage%20Wright%20and%20Federico%20Piccinini%20and%20Lei%20Zhang%20and%20Chu-Cheng%20Lin%20and%20Ionel%20Gog%20and%20Yana%20Kulizhskaya%20and%20Ashwin%20Sreevatsa%20and%20Shuang%20Song%20and%20Luis%20C.%20Cobo%20and%20Anand%20Iyer%20and%20Chetan%20Tekur%20and%20Guillermo%20Garrido%20and%20Zhuyun%20Xiao%20and%20Rupert%20Kemp%20and%20Huaixiu%20Steven%20Zheng%20and%20Hui%20Li%20and%20Ananth%20Agarwal%20and%20Christel%20Ngani%20and%20Kati%20Goshvadi%20and%20Rebeca%20Santamaria-Fernandez%20and%20Wojciech%20Fica%20and%20Xinyun%20Chen%20and%20Chris%20Gorgolewski%20and%20Sean%20Sun%20and%20Roopal%20Garg%20and%20Xinyu%20Ye%20and%20S.%20M.%20Ali%20Eslami%20and%20Nan%20Hua%20and%20Jon%20Simon%20and%20Pratik%20Joshi%20and%20Yelin%20Kim%20and%20Ian%20Tenney%20and%20Sahitya%20Potluri%20and%20Lam%20Nguyen%20Thiet%20and%20Quan%20Yuan%20and%20Florian%20Luisier%20and%20Alexandra%20Chronopoulou%20and%20Salvatore%20Scellato%20and%20Praveen%20Srinivasan%20and%20Minmin%20Chen%20and%20Vinod%20Koverkathu%20and%20Valentin%20Dalibard%20and%20Yaming%20Xu%20and%20Brennan%20Saeta%20and%20Keith%20Anderson%20and%20Thibault%20Sellam%20and%20Nick%20Fernando%20and%20Fantine%20Huot%20and%20Junehyuk%20Jung%20and%20Mani%20Varadarajan%20and%20Michael%20Quinn%20and%20Amit%20Raul%20and%20Maigo%20Le%20and%20Ruslan%20Habalov%20and%20Jon%20Clark%20and%20Komal%20Jalan%20and%20Kalesha%20Bullard%20and%20Achintya%20Singhal%20and%20Thang%20Luong%20and%20Boyu%20Wang%20and%20Sujeevan%20Rajayogam%20and%20Julian%20Eisenschlos%20and%20Johnson%20Jia%20and%20Daniel%20Finchelstein%20and%20Alex%20Yakubovich%20and%20Daniel%20Balle%20and%20Michael%20Fink%20and%20Sameer%20Agarwal%20and%20Jing%20Li%20and%20Dj%20Dvijotham%20and%20Shalini%20Pal%20and%20Kai%20Kang%20and%20Jaclyn%20Konzelmann%20and%20Jennifer%20Beattie%20and%20Olivier%20Dousse%20and%20Diane%20Wu%20and%20Remi%20Crocker%20and%20Chen%20Elkind%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Jong%20Lee%20and%20Dan%20Holtmann-Rice%20and%20Krystal%20Kallarackal%20and%20Rosanne%20Liu%20and%20Denis%20Vnukov%20and%20Neera%20Vats%20and%20Luca%20Invernizzi%20and%20Mohsen%20Jafari%20and%20Huanjie%20Zhou%20and%20Lilly%20Taylor%20and%20Jennifer%20Prendki%20and%20Marcus%20Wu%20and%20Tom%20Eccles%20and%20Tianqi%20Liu%20and%20Kavya%20Kopparapu%20and%20Francoise%20Beaufays%20and%20Christof%20Angermueller%20and%20Andreea%20Marzoca%20and%20Shourya%20Sarcar%20and%20Hilal%20Dib%20and%20Jeff%20Stanway%20and%20Frank%20Perbet%20and%20Nejc%20Trdin%20and%20Rachel%20Sterneck%20and%20Andrey%20Khorlin%20and%20Dinghua%20Li%20and%20Xihui%20Wu%20and%20Sonam%20Goenka%20and%20David%20Madras%20and%20Sasha%20Goldshtein%20and%20Willi%20Gierke%20and%20Tong%20Zhou%20and%20Yaxin%20Liu%20and%20Yannie%20Liang%20and%20Anais%20White%20and%20Yunjie%20Li%20and%20Shreya%20Singh%20and%20Sanaz%20Bahargam%20and%20Mark%20Epstein%20and%20Sujoy%20Basu%20and%20Li%20Lao%20and%20Adnan%20Ozturel%20and%20Carl%20Crous%20and%20Alex%20Zhai%20and%20Han%20Lu%20and%20Zora%20Tung%20and%20Neeraj%20Gaur%20and%20Alanna%20Walton%20and%20Lucas%20Dixon%20and%20Ming%20Zhang%20and%20Amir%20Globerson%20and%20Grant%20Uy%20and%20Andrew%20Bolt%20and%20Olivia%20Wiles%20and%20Milad%20Nasr%20and%20Ilia%20Shumailov%20and%20Marco%20Selvi%20and%20Francesco%20Piccinno%20and%20Ricardo%20Aguilar%20and%20Sara%20McCarthy%20and%20Misha%20Khalman%20and%20Mrinal%20Shukla%20and%20Vlado%20Galic%20and%20John%20Carpenter%20and%20Kevin%20Villela%20and%20Haibin%20Zhang%20and%20Harry%20Richardson%20and%20James%20Martens%20and%20Matko%20Bosnjak%20and%20Shreyas%20Rammohan%20Belle%20and%20Jeff%20Seibert%20and%20Mahmoud%20Alnahlawi%20and%20Brian%20McWilliams%20and%20Sankalp%20Singh%20and%20Annie%20Louis%20and%20Wen%20Ding%20and%20Dan%20Popovici%20and%20Lenin%20Simicich%20and%20Laura%20Knight%20and%20Pulkit%20Mehta%20and%20Nishesh%20Gupta%20and%20Chongyang%20Shi%20and%20Saaber%20Fatehi%20and%20Jovana%20Mitrovic%20and%20Alex%20Grills%20and%20Joseph%20Pagadora%20and%20Tsendsuren%20Munkhdalai%20and%20Dessie%20Petrova%20and%20Danielle%20Eisenbud%20and%20Zhishuai%20Zhang%20and%20Damion%20Yates%20and%20Bhavishya%20Mittal%20and%20Nilesh%20Tripuraneni%20and%20Yannis%20Assael%20and%20Thomas%20Brovelli%20and%20Prateek%20Jain%20and%20Mihajlo%20Velimirovic%20and%20Canfer%20Akbulut%20and%20Jiaqi%20Mu%20and%20Wolfgang%20Macherey%20and%20Ravin%20Kumar%20and%20Jun%20Xu%20and%20Haroon%20Qureshi%20and%20Gheorghe%20Comanici%20and%20Jeremy%20Wiesner%20and%20Zhitao%20Gong%20and%20Anton%20Ruddock%20and%20Matthias%20Bauer%20and%20Nick%20Felt%20and%20Anirudh%20GP%20and%20Anurag%20Arnab%20and%20Dustin%20Zelle%20and%20Jonas%20Rothfuss%20and%20Bill%20Rosgen%20and%20Ashish%20Shenoy%20and%20Bryan%20Seybold%20and%20Xinjian%20Li%20and%20Jayaram%20Mudigonda%20and%20Goker%20Erdogan%20and%20Jiawei%20Xia%20and%20Jiri%20Simsa%20and%20Andrea%20Michi%20and%20Yi%20Yao%20and%20Christopher%20Yew%20and%20Steven%20Kan%20and%20Isaac%20Caswell%20and%20Carey%20Radebaugh%20and%20Andre%20Elisseeff%20and%20Pedro%20Valenzuela%20and%20Kay%20McKinney%20and%20Kim%20Paterson%20and%20Albert%20Cui%20and%20Eri%20Latorre-Chimoto%20and%20Solomon%20Kim%20and%20William%20Zeng%20and%20Ken%20Durden%20and%20Priya%20Ponnapalli%20and%20Tiberiu%20Sosea%20and%20Christopher%20A.%20Choquette-Choo%20and%20James%20Manyika%20and%20Brona%20Robenek%20and%20Harsha%20Vashisht%20and%20Sebastien%20Pereira%20and%20Hoi%20Lam%20and%20Marko%20Velic%20and%20Denese%20Owusu-Afriyie%20and%20Katherine%20Lee%20and%20Tolga%20Bolukbasi%20and%20Alicia%20Parrish%20and%20Shawn%20Lu%20and%20Jane%20Park%20and%20Balaji%20Venkatraman%20and%20Alice%20Talbert%20and%20Lambert%20Rosique%20and%20Yuchung%20Cheng%20and%20Andrei%20Sozanschi%20and%20Adam%20Paszke%20and%20Praveen%20Kumar%20and%20Jessica%20Austin%20and%20Lu%20Li%20and%20Khalid%20Salama%20and%20Bartek%20Perz%20and%20Wooyeol%20Kim%20and%20Nandita%20Dukkipati%20and%20Anthony%20Baryshnikov%20and%20Christos%20Kaplanis%20and%20XiangHai%20Sheng%20and%20Yuri%20Chervonyi%20and%20Caglar%20Unlu%20and%20Diego%20de%20Las%20Casas%20and%20Harry%20Askham%20and%20Kathryn%20Tunyasuvunakool%20and%20Felix%20Gimeno%20and%20Siim%20Poder%20and%20Chester%20Kwak%20and%20Matt%20Miecnikowski%20and%20Vahab%20Mirrokni%20and%20Alek%20Dimitriev%20and%20Aaron%20Parisi%20and%20Dangyi%20Liu%20and%20Tomy%20Tsai%20and%20Toby%20Shevlane%20and%20Christina%20Kouridi%20and%20Drew%20Garmon%20and%20Adrian%20Goedeckemeyer%20and%20Adam%20R.%20Brown%20and%20Anitha%20Vijayakumar%20and%20Ali%20Elqursh%20and%20Sadegh%20Jazayeri%20and%20Jin%20Huang%20and%20Sara%20Mc%20Carthy%20and%20Jay%20Hoover%20and%20Lucy%20Kim%20and%20Sandeep%20Kumar%20and%20Wei%20Chen%20and%20Courtney%20Biles%20and%20Garrett%20Bingham%20and%20Evan%20Rosen%20and%20Lisa%20Wang%20and%20Qijun%20Tan%20and%20David%20Engel%20and%20Francesco%20Pongetti%20and%20Dario%20de%20Cesare%20and%20Dongseong%20Hwang%20and%20Lily%20Yu%20and%20Jennifer%20Pullman%20and%20Srini%20Narayanan%20and%20Kyle%20Levin%20and%20Siddharth%20Gopal%20and%20Megan%20Li%20and%20Asaf%20Aharoni%20and%20Trieu%20Trinh%20and%20Jessica%20Lo%20and%20Norman%20Casagrande%20and%20Roopali%20Vij%20and%20Loic%20Matthey%20and%20Bramandia%20Ramadhana%20and%20Austin%20Matthews%20and%20CJ%20Carey%20and%20Matthew%20Johnson%20and%20Kremena%20Goranova%20and%20Rohin%20Shah%20and%20Shereen%20Ashraf%20and%20Kingshuk%20Dasgupta%20and%20Rasmus%20Larsen%20and%20Yicheng%20Wang%20and%20Manish%20Reddy%20Vuyyuru%20and%20Chong%20Jiang%20and%20Joana%20Ijazi%20and%20Kazuki%20Osawa%20and%20Celine%20Smith%20and%20Ramya%20Sree%20Boppana%20and%20Taylan%20Bilal%20and%20Yuma%20Koizumi%20and%20Ying%20Xu%20and%20Yasemin%20Altun%20and%20Nir%20Shabat%20and%20Ben%20Bariach%20and%20Alex%20Korchemniy%20and%20Kiam%20Choo%20and%20Olaf%20Ronneberger%20and%20Chimezie%20Iwuanyanwu%20and%20Shubin%20Zhao%20and%20David%20Soergel%20and%20Cho-Jui%20Hsieh%20and%20Irene%20Cai%20and%20Shariq%20Iqbal%20and%20Martin%20Sundermeyer%20and%20Zhe%20Chen%20and%20Elie%20Bursztein%20and%20Chaitanya%20Malaviya%20and%20Fadi%20Biadsy%20and%20Prakash%20Shroff%20and%20Inderjit%20Dhillon%20and%20Tejasi%20Latkar%20and%20Chris%20Dyer%20and%20Hannah%20Forbes%20and%20Massimo%20Nicosia%20and%20Vitaly%20Nikolaev%20and%20Somer%20Greene%20and%20Marin%20Georgiev%20and%20Pidong%20Wang%20and%20Nina%20Martin%20and%20Hanie%20Sedghi%20and%20John%20Zhang%20and%20Praseem%20Banzal%20and%20Doug%20Fritz%20and%20Vikram%20Rao%20and%20Xuezhi%20Wang%20and%20Jiageng%20Zhang%20and%20Viorica%20Patraucean%20and%20Dayou%20Du%20and%20Igor%20Mordatch%20and%20Ivan%20Jurin%20and%20Lewis%20Liu%20and%20Ayush%20Dubey%20and%20Abhi%20Mohan%20and%20Janek%20Nowakowski%20and%20Vlad-Doru%20Ion%20and%20Nan%20Wei%20and%20Reiko%20Tojo%20and%20Maria%20Abi%20Raad%20and%20Drew%20A.%20Hudson%20and%20Vaishakh%20Keshava%20and%20Shubham%20Agrawal%20and%20Kevin%20Ramirez%20and%20Zhichun%20Wu%20and%20Hoang%20Nguyen%20and%20Ji%20Liu%20and%20Madhavi%20Sewak%20and%20Bryce%20Petrini%20and%20DongHyun%20Choi%20and%20Ivan%20Philips%20and%20Ziyue%20Wang%20and%20Ioana%20Bica%20and%20Ankush%20Garg%20and%20Jarek%20Wilkiewicz%20and%20Priyanka%20Agrawal%20and%20Xiaowei%20Li%20and%20Danhao%20Guo%20and%20Emily%20Xue%20and%20Naseer%20Shaik%20and%20Andrew%20Leach%20and%20Sadh%20MNM%20Khan%20and%20Julia%20Wiesinger%20and%20Sammy%20Jerome%20and%20Abhishek%20Chakladar%20and%20Alek%20Wenjiao%20Wang%20and%20Tina%20Ornduff%20and%20Folake%20Abu%20and%20Alireza%20Ghaffarkhah%20and%20Marcus%20Wainwright%20and%20Mario%20Cortes%20and%20Frederick%20Liu%20and%20Joshua%20Maynez%20and%20Andreas%20Terzis%20and%20Pouya%20Samangouei%20and%20Riham%20Mansour%20and%20Tomasz%20K%C4%99pa%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Anton%20Algymr%20and%20Dan%20Banica%20and%20Agoston%20Weisz%20and%20Andras%20Orban%20and%20Alexandre%20Senges%20and%20Ewa%20Andrejczuk%20and%20Mark%20Geller%20and%20Niccolo%20Dal%20Santo%20and%20Valentin%20Anklin%20and%20Majd%20Al%20Merey%20and%20Martin%20Baeuml%20and%20Trevor%20Strohman%20and%20Junwen%20Bai%20and%20Slav%20Petrov%20and%20Yonghui%20Wu%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Jeff%20Dean%20and%20Oriol%20Vinyals&entry.1292438233=%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%201.5%20family%20of%20models%2C%20representing%0Athe%20next%20generation%20of%20highly%20compute-efficient%20multimodal%20models%20capable%20of%0Arecalling%20and%20reasoning%20over%20fine-grained%20information%20from%20millions%20of%20tokens%0Aof%20context%2C%20including%20multiple%20long%20documents%20and%20hours%20of%20video%20and%20audio.%20The%0Afamily%20includes%20two%20new%20models%3A%20%281%29%20an%20updated%20Gemini%201.5%20Pro%2C%20which%20exceeds%0Athe%20February%20version%20on%20the%20great%20majority%20of%20capabilities%20and%20benchmarks%3B%20%282%29%0AGemini%201.5%20Flash%2C%20a%20more%20lightweight%20variant%20designed%20for%20efficiency%20with%0Aminimal%20regression%20in%20quality.%20Gemini%201.5%20models%20achieve%20near-perfect%20recall%20on%0Along-context%20retrieval%20tasks%20across%20modalities%2C%20improve%20the%20state-of-the-art%20in%0Along-document%20QA%2C%20long-video%20QA%20and%20long-context%20ASR%2C%20and%20match%20or%20surpass%0AGemini%201.0%20Ultra%27s%20state-of-the-art%20performance%20across%20a%20broad%20set%20of%0Abenchmarks.%20Studying%20the%20limits%20of%20Gemini%201.5%27s%20long-context%20ability%2C%20we%20find%0Acontinued%20improvement%20in%20next-token%20prediction%20and%20near-perfect%20retrieval%0A%28%3E99%25%29%20up%20to%20at%20least%2010M%20tokens%2C%20a%20generational%20leap%20over%20existing%20models%20such%0Aas%20Claude%203.0%20%28200k%29%20and%20GPT-4%20Turbo%20%28128k%29.%20Finally%2C%20we%20highlight%20real-world%0Ause%20cases%2C%20such%20as%20Gemini%201.5%20collaborating%20with%20professionals%20on%20completing%0Atheir%20tasks%20achieving%2026%20to%2075%25%20time%20savings%20across%2010%20different%20job%0Acategories%2C%20as%20well%20as%20surprising%20new%20capabilities%20of%20large%20language%20models%20at%0Athe%20frontier%3B%20when%20given%20a%20grammar%20manual%20for%20Kalamang%2C%20a%20language%20with%20fewer%0Athan%20200%20speakers%20worldwide%2C%20the%20model%20learns%20to%20translate%20English%20to%20Kalamang%0Aat%20a%20similar%20level%20to%20a%20person%20who%20learned%20from%20the%20same%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05530v5&entry.124074799=Read"},
{"title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator", "author": "Guoxuan Chen and Han Shi and Jiawei Li and Yihang Gao and Xiaozhe Ren and Yimeng Chen and Xin Jiang and Zhenguo Li and Weiyang Liu and Chao Huang", "abstract": "  Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.\n", "link": "http://arxiv.org/abs/2412.12094v1", "date": "2024-12-16", "relevancy": 2.4882, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator&body=Title%3A%20SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator%0AAuthor%3A%20Guoxuan%20Chen%20and%20Han%20Shi%20and%20Jiawei%20Li%20and%20Yihang%20Gao%20and%20Xiaozhe%20Ren%20and%20Yimeng%20Chen%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Weiyang%20Liu%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Aspectrum%20of%20natural%20language%20processing%20tasks.%20However%2C%20their%20substantial%20sizes%0Apose%20considerable%20challenges%2C%20particularly%20in%20computational%20demands%20and%0Ainference%20speed%2C%20due%20to%20their%20quadratic%20complexity.%20In%20this%20work%2C%20we%20have%0Aidentified%20a%20key%20pattern%3A%20certain%20seemingly%20meaningless%20special%20tokens%20%28i.e.%2C%0Aseparators%29%20contribute%20disproportionately%20to%20attention%20scores%20compared%20to%0Asemantically%20meaningful%20tokens.%20This%20observation%20suggests%20that%20information%20of%0Athe%20segments%20between%20these%20separator%20tokens%20can%20be%20effectively%20condensed%20into%0Athe%20separator%20tokens%20themselves%20without%20significant%20information%20loss.%20Guided%20by%0Athis%20insight%2C%20we%20introduce%20SepLLM%2C%20a%20plug-and-play%20framework%20that%20accelerates%0Ainference%20by%20compressing%20these%20segments%20and%20eliminating%20redundant%20tokens.%0AAdditionally%2C%20we%20implement%20efficient%20kernels%20for%20training%20acceleration.%0AExperimental%20results%20across%20training-free%2C%20training-from-scratch%2C%20and%0Apost-training%20settings%20demonstrate%20SepLLM%27s%20effectiveness.%20Notably%2C%20using%20the%0ALlama-3-8B%20backbone%2C%20SepLLM%20achieves%20over%2050%25%20reduction%20in%20KV%20cache%20on%20the%0AGSM8K-CoT%20benchmark%20while%20maintaining%20comparable%20performance.%20Furthermore%2C%20in%0Astreaming%20settings%2C%20SepLLM%20effectively%20processes%20sequences%20of%20up%20to%204%20million%0Atokens%20or%20more%20while%20maintaining%20consistent%20language%20modeling%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSepLLM%253A%2520Accelerate%2520Large%2520Language%2520Models%2520by%2520Compressing%2520One%2520Segment%2520into%250A%2520%2520One%2520Separator%26entry.906535625%3DGuoxuan%2520Chen%2520and%2520Han%2520Shi%2520and%2520Jiawei%2520Li%2520and%2520Yihang%2520Gao%2520and%2520Xiaozhe%2520Ren%2520and%2520Yimeng%2520Chen%2520and%2520Xin%2520Jiang%2520and%2520Zhenguo%2520Li%2520and%2520Weiyang%2520Liu%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520exhibited%2520exceptional%2520performance%2520across%2520a%250Aspectrum%2520of%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520their%2520substantial%2520sizes%250Apose%2520considerable%2520challenges%252C%2520particularly%2520in%2520computational%2520demands%2520and%250Ainference%2520speed%252C%2520due%2520to%2520their%2520quadratic%2520complexity.%2520In%2520this%2520work%252C%2520we%2520have%250Aidentified%2520a%2520key%2520pattern%253A%2520certain%2520seemingly%2520meaningless%2520special%2520tokens%2520%2528i.e.%252C%250Aseparators%2529%2520contribute%2520disproportionately%2520to%2520attention%2520scores%2520compared%2520to%250Asemantically%2520meaningful%2520tokens.%2520This%2520observation%2520suggests%2520that%2520information%2520of%250Athe%2520segments%2520between%2520these%2520separator%2520tokens%2520can%2520be%2520effectively%2520condensed%2520into%250Athe%2520separator%2520tokens%2520themselves%2520without%2520significant%2520information%2520loss.%2520Guided%2520by%250Athis%2520insight%252C%2520we%2520introduce%2520SepLLM%252C%2520a%2520plug-and-play%2520framework%2520that%2520accelerates%250Ainference%2520by%2520compressing%2520these%2520segments%2520and%2520eliminating%2520redundant%2520tokens.%250AAdditionally%252C%2520we%2520implement%2520efficient%2520kernels%2520for%2520training%2520acceleration.%250AExperimental%2520results%2520across%2520training-free%252C%2520training-from-scratch%252C%2520and%250Apost-training%2520settings%2520demonstrate%2520SepLLM%2527s%2520effectiveness.%2520Notably%252C%2520using%2520the%250ALlama-3-8B%2520backbone%252C%2520SepLLM%2520achieves%2520over%252050%2525%2520reduction%2520in%2520KV%2520cache%2520on%2520the%250AGSM8K-CoT%2520benchmark%2520while%2520maintaining%2520comparable%2520performance.%2520Furthermore%252C%2520in%250Astreaming%2520settings%252C%2520SepLLM%2520effectively%2520processes%2520sequences%2520of%2520up%2520to%25204%2520million%250Atokens%2520or%2520more%2520while%2520maintaining%2520consistent%2520language%2520modeling%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator&entry.906535625=Guoxuan%20Chen%20and%20Han%20Shi%20and%20Jiawei%20Li%20and%20Yihang%20Gao%20and%20Xiaozhe%20Ren%20and%20Yimeng%20Chen%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Weiyang%20Liu%20and%20Chao%20Huang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Aspectrum%20of%20natural%20language%20processing%20tasks.%20However%2C%20their%20substantial%20sizes%0Apose%20considerable%20challenges%2C%20particularly%20in%20computational%20demands%20and%0Ainference%20speed%2C%20due%20to%20their%20quadratic%20complexity.%20In%20this%20work%2C%20we%20have%0Aidentified%20a%20key%20pattern%3A%20certain%20seemingly%20meaningless%20special%20tokens%20%28i.e.%2C%0Aseparators%29%20contribute%20disproportionately%20to%20attention%20scores%20compared%20to%0Asemantically%20meaningful%20tokens.%20This%20observation%20suggests%20that%20information%20of%0Athe%20segments%20between%20these%20separator%20tokens%20can%20be%20effectively%20condensed%20into%0Athe%20separator%20tokens%20themselves%20without%20significant%20information%20loss.%20Guided%20by%0Athis%20insight%2C%20we%20introduce%20SepLLM%2C%20a%20plug-and-play%20framework%20that%20accelerates%0Ainference%20by%20compressing%20these%20segments%20and%20eliminating%20redundant%20tokens.%0AAdditionally%2C%20we%20implement%20efficient%20kernels%20for%20training%20acceleration.%0AExperimental%20results%20across%20training-free%2C%20training-from-scratch%2C%20and%0Apost-training%20settings%20demonstrate%20SepLLM%27s%20effectiveness.%20Notably%2C%20using%20the%0ALlama-3-8B%20backbone%2C%20SepLLM%20achieves%20over%2050%25%20reduction%20in%20KV%20cache%20on%20the%0AGSM8K-CoT%20benchmark%20while%20maintaining%20comparable%20performance.%20Furthermore%2C%20in%0Astreaming%20settings%2C%20SepLLM%20effectively%20processes%20sequences%20of%20up%20to%204%20million%0Atokens%20or%20more%20while%20maintaining%20consistent%20language%20modeling%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12094v1&entry.124074799=Read"},
{"title": "Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided\n  Self-Supervised Learning", "author": "Yuti Liu and Shice Liu and Junyuan Gao and Pengtao Jiang and Hao Zhang and Jinwei Chen and Bo Li", "abstract": "  Image Aesthetic Assessment (IAA) is a vital and intricate task that entails\nanalyzing and assessing an image's aesthetic values, and identifying its\nhighlights and areas for improvement. Traditional methods of IAA often\nconcentrate on a single aesthetic task and suffer from inadequate labeled\ndatasets, thus impairing in-depth aesthetic comprehension. Despite efforts to\novercome this challenge through the application of Multi-modal Large Language\nModels (MLLMs), such models remain underdeveloped for IAA purposes. To address\nthis, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic\ninsight. Central to our approach is an innovative multi-scale text-guided\nself-supervised learning technique. This technique features a multi-scale\nfeature alignment module and capitalizes on a wealth of unlabeled data in a\nself-supervised manner to structurally and functionally enhance aesthetic\nability. The empirical evidence indicates that accompanied with extensive\ninstruct-tuning, our model sets new state-of-the-art benchmarks across multiple\ntasks, including aesthetic scoring, aesthetic commenting, and personalized\nimage aesthetic assessment. Remarkably, it also demonstrates zero-shot learning\ncapabilities in the emerging task of aesthetic suggesting. Furthermore, for\npersonalized image aesthetic assessment, we harness the potential of in-context\nlearning and showcase its inherent advantages.\n", "link": "http://arxiv.org/abs/2412.11952v1", "date": "2024-12-16", "relevancy": 2.4446, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.627}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6014}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Comprehensive%20Aesthetic%20Insight%20with%20Multi-Scale%20Text-Guided%0A%20%20Self-Supervised%20Learning&body=Title%3A%20Advancing%20Comprehensive%20Aesthetic%20Insight%20with%20Multi-Scale%20Text-Guided%0A%20%20Self-Supervised%20Learning%0AAuthor%3A%20Yuti%20Liu%20and%20Shice%20Liu%20and%20Junyuan%20Gao%20and%20Pengtao%20Jiang%20and%20Hao%20Zhang%20and%20Jinwei%20Chen%20and%20Bo%20Li%0AAbstract%3A%20%20%20Image%20Aesthetic%20Assessment%20%28IAA%29%20is%20a%20vital%20and%20intricate%20task%20that%20entails%0Aanalyzing%20and%20assessing%20an%20image%27s%20aesthetic%20values%2C%20and%20identifying%20its%0Ahighlights%20and%20areas%20for%20improvement.%20Traditional%20methods%20of%20IAA%20often%0Aconcentrate%20on%20a%20single%20aesthetic%20task%20and%20suffer%20from%20inadequate%20labeled%0Adatasets%2C%20thus%20impairing%20in-depth%20aesthetic%20comprehension.%20Despite%20efforts%20to%0Aovercome%20this%20challenge%20through%20the%20application%20of%20Multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%2C%20such%20models%20remain%20underdeveloped%20for%20IAA%20purposes.%20To%20address%0Athis%2C%20we%20propose%20a%20comprehensive%20aesthetic%20MLLM%20capable%20of%20nuanced%20aesthetic%0Ainsight.%20Central%20to%20our%20approach%20is%20an%20innovative%20multi-scale%20text-guided%0Aself-supervised%20learning%20technique.%20This%20technique%20features%20a%20multi-scale%0Afeature%20alignment%20module%20and%20capitalizes%20on%20a%20wealth%20of%20unlabeled%20data%20in%20a%0Aself-supervised%20manner%20to%20structurally%20and%20functionally%20enhance%20aesthetic%0Aability.%20The%20empirical%20evidence%20indicates%20that%20accompanied%20with%20extensive%0Ainstruct-tuning%2C%20our%20model%20sets%20new%20state-of-the-art%20benchmarks%20across%20multiple%0Atasks%2C%20including%20aesthetic%20scoring%2C%20aesthetic%20commenting%2C%20and%20personalized%0Aimage%20aesthetic%20assessment.%20Remarkably%2C%20it%20also%20demonstrates%20zero-shot%20learning%0Acapabilities%20in%20the%20emerging%20task%20of%20aesthetic%20suggesting.%20Furthermore%2C%20for%0Apersonalized%20image%20aesthetic%20assessment%2C%20we%20harness%20the%20potential%20of%20in-context%0Alearning%20and%20showcase%20its%20inherent%20advantages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Comprehensive%2520Aesthetic%2520Insight%2520with%2520Multi-Scale%2520Text-Guided%250A%2520%2520Self-Supervised%2520Learning%26entry.906535625%3DYuti%2520Liu%2520and%2520Shice%2520Liu%2520and%2520Junyuan%2520Gao%2520and%2520Pengtao%2520Jiang%2520and%2520Hao%2520Zhang%2520and%2520Jinwei%2520Chen%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520Image%2520Aesthetic%2520Assessment%2520%2528IAA%2529%2520is%2520a%2520vital%2520and%2520intricate%2520task%2520that%2520entails%250Aanalyzing%2520and%2520assessing%2520an%2520image%2527s%2520aesthetic%2520values%252C%2520and%2520identifying%2520its%250Ahighlights%2520and%2520areas%2520for%2520improvement.%2520Traditional%2520methods%2520of%2520IAA%2520often%250Aconcentrate%2520on%2520a%2520single%2520aesthetic%2520task%2520and%2520suffer%2520from%2520inadequate%2520labeled%250Adatasets%252C%2520thus%2520impairing%2520in-depth%2520aesthetic%2520comprehension.%2520Despite%2520efforts%2520to%250Aovercome%2520this%2520challenge%2520through%2520the%2520application%2520of%2520Multi-modal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%252C%2520such%2520models%2520remain%2520underdeveloped%2520for%2520IAA%2520purposes.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520comprehensive%2520aesthetic%2520MLLM%2520capable%2520of%2520nuanced%2520aesthetic%250Ainsight.%2520Central%2520to%2520our%2520approach%2520is%2520an%2520innovative%2520multi-scale%2520text-guided%250Aself-supervised%2520learning%2520technique.%2520This%2520technique%2520features%2520a%2520multi-scale%250Afeature%2520alignment%2520module%2520and%2520capitalizes%2520on%2520a%2520wealth%2520of%2520unlabeled%2520data%2520in%2520a%250Aself-supervised%2520manner%2520to%2520structurally%2520and%2520functionally%2520enhance%2520aesthetic%250Aability.%2520The%2520empirical%2520evidence%2520indicates%2520that%2520accompanied%2520with%2520extensive%250Ainstruct-tuning%252C%2520our%2520model%2520sets%2520new%2520state-of-the-art%2520benchmarks%2520across%2520multiple%250Atasks%252C%2520including%2520aesthetic%2520scoring%252C%2520aesthetic%2520commenting%252C%2520and%2520personalized%250Aimage%2520aesthetic%2520assessment.%2520Remarkably%252C%2520it%2520also%2520demonstrates%2520zero-shot%2520learning%250Acapabilities%2520in%2520the%2520emerging%2520task%2520of%2520aesthetic%2520suggesting.%2520Furthermore%252C%2520for%250Apersonalized%2520image%2520aesthetic%2520assessment%252C%2520we%2520harness%2520the%2520potential%2520of%2520in-context%250Alearning%2520and%2520showcase%2520its%2520inherent%2520advantages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Comprehensive%20Aesthetic%20Insight%20with%20Multi-Scale%20Text-Guided%0A%20%20Self-Supervised%20Learning&entry.906535625=Yuti%20Liu%20and%20Shice%20Liu%20and%20Junyuan%20Gao%20and%20Pengtao%20Jiang%20and%20Hao%20Zhang%20and%20Jinwei%20Chen%20and%20Bo%20Li&entry.1292438233=%20%20Image%20Aesthetic%20Assessment%20%28IAA%29%20is%20a%20vital%20and%20intricate%20task%20that%20entails%0Aanalyzing%20and%20assessing%20an%20image%27s%20aesthetic%20values%2C%20and%20identifying%20its%0Ahighlights%20and%20areas%20for%20improvement.%20Traditional%20methods%20of%20IAA%20often%0Aconcentrate%20on%20a%20single%20aesthetic%20task%20and%20suffer%20from%20inadequate%20labeled%0Adatasets%2C%20thus%20impairing%20in-depth%20aesthetic%20comprehension.%20Despite%20efforts%20to%0Aovercome%20this%20challenge%20through%20the%20application%20of%20Multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%2C%20such%20models%20remain%20underdeveloped%20for%20IAA%20purposes.%20To%20address%0Athis%2C%20we%20propose%20a%20comprehensive%20aesthetic%20MLLM%20capable%20of%20nuanced%20aesthetic%0Ainsight.%20Central%20to%20our%20approach%20is%20an%20innovative%20multi-scale%20text-guided%0Aself-supervised%20learning%20technique.%20This%20technique%20features%20a%20multi-scale%0Afeature%20alignment%20module%20and%20capitalizes%20on%20a%20wealth%20of%20unlabeled%20data%20in%20a%0Aself-supervised%20manner%20to%20structurally%20and%20functionally%20enhance%20aesthetic%0Aability.%20The%20empirical%20evidence%20indicates%20that%20accompanied%20with%20extensive%0Ainstruct-tuning%2C%20our%20model%20sets%20new%20state-of-the-art%20benchmarks%20across%20multiple%0Atasks%2C%20including%20aesthetic%20scoring%2C%20aesthetic%20commenting%2C%20and%20personalized%0Aimage%20aesthetic%20assessment.%20Remarkably%2C%20it%20also%20demonstrates%20zero-shot%20learning%0Acapabilities%20in%20the%20emerging%20task%20of%20aesthetic%20suggesting.%20Furthermore%2C%20for%0Apersonalized%20image%20aesthetic%20assessment%2C%20we%20harness%20the%20potential%20of%20in-context%0Alearning%20and%20showcase%20its%20inherent%20advantages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11952v1&entry.124074799=Read"},
{"title": "Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via\n  Code Rewriting", "author": "Tong Ye and Yangkai Du and Tengfei Ma and Lingfei Wu and Xuhong Zhang and Shouling Ji and Wenhai Wang", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating code. However, the misuse of LLM-generated (synthetic) code has\nraised concerns in both educational and industrial contexts, underscoring the\nurgent need for synthetic code detectors. Existing methods for detecting\nsynthetic content are primarily designed for general text and struggle with\ncode due to the unique grammatical structure of programming languages and the\npresence of numerous ''low-entropy'' tokens. Building on this, our work\nproposes a novel zero-shot synthetic code detector based on the similarity\nbetween the original code and its LLM-rewritten variants. Our method is based\non the observation that differences between LLM-rewritten and original code\ntend to be smaller when the original code is synthetic. We utilize\nself-supervised contrastive learning to train a code similarity model and\nevaluate our approach on two synthetic code detection benchmarks. Our results\ndemonstrate a significant improvement over existing SOTA synthetic content\ndetectors, with AUROC scores increasing by 20.5% on the APPS benchmark and\n29.1% on the MBPP benchmark.\n", "link": "http://arxiv.org/abs/2405.16133v3", "date": "2024-12-16", "relevancy": 2.4242, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4978}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4812}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20LLM-Generated%20Code%3A%20A%20Zero-Shot%20Synthetic%20Code%20Detector%20via%0A%20%20Code%20Rewriting&body=Title%3A%20Uncovering%20LLM-Generated%20Code%3A%20A%20Zero-Shot%20Synthetic%20Code%20Detector%20via%0A%20%20Code%20Rewriting%0AAuthor%3A%20Tong%20Ye%20and%20Yangkai%20Du%20and%20Tengfei%20Ma%20and%20Lingfei%20Wu%20and%20Xuhong%20Zhang%20and%20Shouling%20Ji%20and%20Wenhai%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20proficiency%20in%0Agenerating%20code.%20However%2C%20the%20misuse%20of%20LLM-generated%20%28synthetic%29%20code%20has%0Araised%20concerns%20in%20both%20educational%20and%20industrial%20contexts%2C%20underscoring%20the%0Aurgent%20need%20for%20synthetic%20code%20detectors.%20Existing%20methods%20for%20detecting%0Asynthetic%20content%20are%20primarily%20designed%20for%20general%20text%20and%20struggle%20with%0Acode%20due%20to%20the%20unique%20grammatical%20structure%20of%20programming%20languages%20and%20the%0Apresence%20of%20numerous%20%27%27low-entropy%27%27%20tokens.%20Building%20on%20this%2C%20our%20work%0Aproposes%20a%20novel%20zero-shot%20synthetic%20code%20detector%20based%20on%20the%20similarity%0Abetween%20the%20original%20code%20and%20its%20LLM-rewritten%20variants.%20Our%20method%20is%20based%0Aon%20the%20observation%20that%20differences%20between%20LLM-rewritten%20and%20original%20code%0Atend%20to%20be%20smaller%20when%20the%20original%20code%20is%20synthetic.%20We%20utilize%0Aself-supervised%20contrastive%20learning%20to%20train%20a%20code%20similarity%20model%20and%0Aevaluate%20our%20approach%20on%20two%20synthetic%20code%20detection%20benchmarks.%20Our%20results%0Ademonstrate%20a%20significant%20improvement%20over%20existing%20SOTA%20synthetic%20content%0Adetectors%2C%20with%20AUROC%20scores%20increasing%20by%2020.5%25%20on%20the%20APPS%20benchmark%20and%0A29.1%25%20on%20the%20MBPP%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16133v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520LLM-Generated%2520Code%253A%2520A%2520Zero-Shot%2520Synthetic%2520Code%2520Detector%2520via%250A%2520%2520Code%2520Rewriting%26entry.906535625%3DTong%2520Ye%2520and%2520Yangkai%2520Du%2520and%2520Tengfei%2520Ma%2520and%2520Lingfei%2520Wu%2520and%2520Xuhong%2520Zhang%2520and%2520Shouling%2520Ji%2520and%2520Wenhai%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520proficiency%2520in%250Agenerating%2520code.%2520However%252C%2520the%2520misuse%2520of%2520LLM-generated%2520%2528synthetic%2529%2520code%2520has%250Araised%2520concerns%2520in%2520both%2520educational%2520and%2520industrial%2520contexts%252C%2520underscoring%2520the%250Aurgent%2520need%2520for%2520synthetic%2520code%2520detectors.%2520Existing%2520methods%2520for%2520detecting%250Asynthetic%2520content%2520are%2520primarily%2520designed%2520for%2520general%2520text%2520and%2520struggle%2520with%250Acode%2520due%2520to%2520the%2520unique%2520grammatical%2520structure%2520of%2520programming%2520languages%2520and%2520the%250Apresence%2520of%2520numerous%2520%2527%2527low-entropy%2527%2527%2520tokens.%2520Building%2520on%2520this%252C%2520our%2520work%250Aproposes%2520a%2520novel%2520zero-shot%2520synthetic%2520code%2520detector%2520based%2520on%2520the%2520similarity%250Abetween%2520the%2520original%2520code%2520and%2520its%2520LLM-rewritten%2520variants.%2520Our%2520method%2520is%2520based%250Aon%2520the%2520observation%2520that%2520differences%2520between%2520LLM-rewritten%2520and%2520original%2520code%250Atend%2520to%2520be%2520smaller%2520when%2520the%2520original%2520code%2520is%2520synthetic.%2520We%2520utilize%250Aself-supervised%2520contrastive%2520learning%2520to%2520train%2520a%2520code%2520similarity%2520model%2520and%250Aevaluate%2520our%2520approach%2520on%2520two%2520synthetic%2520code%2520detection%2520benchmarks.%2520Our%2520results%250Ademonstrate%2520a%2520significant%2520improvement%2520over%2520existing%2520SOTA%2520synthetic%2520content%250Adetectors%252C%2520with%2520AUROC%2520scores%2520increasing%2520by%252020.5%2525%2520on%2520the%2520APPS%2520benchmark%2520and%250A29.1%2525%2520on%2520the%2520MBPP%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16133v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20LLM-Generated%20Code%3A%20A%20Zero-Shot%20Synthetic%20Code%20Detector%20via%0A%20%20Code%20Rewriting&entry.906535625=Tong%20Ye%20and%20Yangkai%20Du%20and%20Tengfei%20Ma%20and%20Lingfei%20Wu%20and%20Xuhong%20Zhang%20and%20Shouling%20Ji%20and%20Wenhai%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20proficiency%20in%0Agenerating%20code.%20However%2C%20the%20misuse%20of%20LLM-generated%20%28synthetic%29%20code%20has%0Araised%20concerns%20in%20both%20educational%20and%20industrial%20contexts%2C%20underscoring%20the%0Aurgent%20need%20for%20synthetic%20code%20detectors.%20Existing%20methods%20for%20detecting%0Asynthetic%20content%20are%20primarily%20designed%20for%20general%20text%20and%20struggle%20with%0Acode%20due%20to%20the%20unique%20grammatical%20structure%20of%20programming%20languages%20and%20the%0Apresence%20of%20numerous%20%27%27low-entropy%27%27%20tokens.%20Building%20on%20this%2C%20our%20work%0Aproposes%20a%20novel%20zero-shot%20synthetic%20code%20detector%20based%20on%20the%20similarity%0Abetween%20the%20original%20code%20and%20its%20LLM-rewritten%20variants.%20Our%20method%20is%20based%0Aon%20the%20observation%20that%20differences%20between%20LLM-rewritten%20and%20original%20code%0Atend%20to%20be%20smaller%20when%20the%20original%20code%20is%20synthetic.%20We%20utilize%0Aself-supervised%20contrastive%20learning%20to%20train%20a%20code%20similarity%20model%20and%0Aevaluate%20our%20approach%20on%20two%20synthetic%20code%20detection%20benchmarks.%20Our%20results%0Ademonstrate%20a%20significant%20improvement%20over%20existing%20SOTA%20synthetic%20content%0Adetectors%2C%20with%20AUROC%20scores%20increasing%20by%2020.5%25%20on%20the%20APPS%20benchmark%20and%0A29.1%25%20on%20the%20MBPP%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16133v3&entry.124074799=Read"},
{"title": "EmotiveTalk: Expressive Talking Head Generation through Audio\n  Information Decoupling and Emotional Video Diffusion", "author": "Haotian Wang and Yuzhe Weng and Yueyan Li and Zilu Guo and Jun Du and Shutong Niu and Jiefeng Ma and Shan He and Xiaoyan Wu and Qiming Hu and Bing Yin and Cong Liu and Qingfeng Liu", "abstract": "  Diffusion models have revolutionized the field of talking head generation,\nyet still face challenges in expressiveness, controllability, and stability in\nlong-time generation. In this research, we propose an EmotiveTalk framework to\naddress these issues. Firstly, to realize better control over the generation of\nlip movement and facial expression, a Vision-guided Audio Information\nDecoupling (V-AID) approach is designed to generate audio-based decoupled\nrepresentations aligned with lip movements and expression. Specifically, to\nachieve alignment between audio and facial expression representation spaces, we\npresent a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within\nV-AID to generate expression-related representations under multi-source emotion\ncondition constraints. Then we propose a well-designed Emotional Talking Head\nDiffusion (ETHD) backbone to efficiently generate highly expressive talking\nhead videos, which contains an Expression Decoupling Injection (EDI) module to\nautomatically decouple the expressions from reference portraits while\nintegrating the target expression information, achieving more expressive\ngeneration performance. Experimental results show that EmotiveTalk can generate\nexpressive talking head videos, ensuring the promised controllability of\nemotions and stability during long-time generation, yielding state-of-the-art\nperformance compared to existing methods.\n", "link": "http://arxiv.org/abs/2411.16726v2", "date": "2024-12-16", "relevancy": 2.4227, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.648}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6021}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmotiveTalk%3A%20Expressive%20Talking%20Head%20Generation%20through%20Audio%0A%20%20Information%20Decoupling%20and%20Emotional%20Video%20Diffusion&body=Title%3A%20EmotiveTalk%3A%20Expressive%20Talking%20Head%20Generation%20through%20Audio%0A%20%20Information%20Decoupling%20and%20Emotional%20Video%20Diffusion%0AAuthor%3A%20Haotian%20Wang%20and%20Yuzhe%20Weng%20and%20Yueyan%20Li%20and%20Zilu%20Guo%20and%20Jun%20Du%20and%20Shutong%20Niu%20and%20Jiefeng%20Ma%20and%20Shan%20He%20and%20Xiaoyan%20Wu%20and%20Qiming%20Hu%20and%20Bing%20Yin%20and%20Cong%20Liu%20and%20Qingfeng%20Liu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20revolutionized%20the%20field%20of%20talking%20head%20generation%2C%0Ayet%20still%20face%20challenges%20in%20expressiveness%2C%20controllability%2C%20and%20stability%20in%0Along-time%20generation.%20In%20this%20research%2C%20we%20propose%20an%20EmotiveTalk%20framework%20to%0Aaddress%20these%20issues.%20Firstly%2C%20to%20realize%20better%20control%20over%20the%20generation%20of%0Alip%20movement%20and%20facial%20expression%2C%20a%20Vision-guided%20Audio%20Information%0ADecoupling%20%28V-AID%29%20approach%20is%20designed%20to%20generate%20audio-based%20decoupled%0Arepresentations%20aligned%20with%20lip%20movements%20and%20expression.%20Specifically%2C%20to%0Aachieve%20alignment%20between%20audio%20and%20facial%20expression%20representation%20spaces%2C%20we%0Apresent%20a%20Diffusion-based%20Co-speech%20Temporal%20Expansion%20%28Di-CTE%29%20module%20within%0AV-AID%20to%20generate%20expression-related%20representations%20under%20multi-source%20emotion%0Acondition%20constraints.%20Then%20we%20propose%20a%20well-designed%20Emotional%20Talking%20Head%0ADiffusion%20%28ETHD%29%20backbone%20to%20efficiently%20generate%20highly%20expressive%20talking%0Ahead%20videos%2C%20which%20contains%20an%20Expression%20Decoupling%20Injection%20%28EDI%29%20module%20to%0Aautomatically%20decouple%20the%20expressions%20from%20reference%20portraits%20while%0Aintegrating%20the%20target%20expression%20information%2C%20achieving%20more%20expressive%0Ageneration%20performance.%20Experimental%20results%20show%20that%20EmotiveTalk%20can%20generate%0Aexpressive%20talking%20head%20videos%2C%20ensuring%20the%20promised%20controllability%20of%0Aemotions%20and%20stability%20during%20long-time%20generation%2C%20yielding%20state-of-the-art%0Aperformance%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16726v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotiveTalk%253A%2520Expressive%2520Talking%2520Head%2520Generation%2520through%2520Audio%250A%2520%2520Information%2520Decoupling%2520and%2520Emotional%2520Video%2520Diffusion%26entry.906535625%3DHaotian%2520Wang%2520and%2520Yuzhe%2520Weng%2520and%2520Yueyan%2520Li%2520and%2520Zilu%2520Guo%2520and%2520Jun%2520Du%2520and%2520Shutong%2520Niu%2520and%2520Jiefeng%2520Ma%2520and%2520Shan%2520He%2520and%2520Xiaoyan%2520Wu%2520and%2520Qiming%2520Hu%2520and%2520Bing%2520Yin%2520and%2520Cong%2520Liu%2520and%2520Qingfeng%2520Liu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520revolutionized%2520the%2520field%2520of%2520talking%2520head%2520generation%252C%250Ayet%2520still%2520face%2520challenges%2520in%2520expressiveness%252C%2520controllability%252C%2520and%2520stability%2520in%250Along-time%2520generation.%2520In%2520this%2520research%252C%2520we%2520propose%2520an%2520EmotiveTalk%2520framework%2520to%250Aaddress%2520these%2520issues.%2520Firstly%252C%2520to%2520realize%2520better%2520control%2520over%2520the%2520generation%2520of%250Alip%2520movement%2520and%2520facial%2520expression%252C%2520a%2520Vision-guided%2520Audio%2520Information%250ADecoupling%2520%2528V-AID%2529%2520approach%2520is%2520designed%2520to%2520generate%2520audio-based%2520decoupled%250Arepresentations%2520aligned%2520with%2520lip%2520movements%2520and%2520expression.%2520Specifically%252C%2520to%250Aachieve%2520alignment%2520between%2520audio%2520and%2520facial%2520expression%2520representation%2520spaces%252C%2520we%250Apresent%2520a%2520Diffusion-based%2520Co-speech%2520Temporal%2520Expansion%2520%2528Di-CTE%2529%2520module%2520within%250AV-AID%2520to%2520generate%2520expression-related%2520representations%2520under%2520multi-source%2520emotion%250Acondition%2520constraints.%2520Then%2520we%2520propose%2520a%2520well-designed%2520Emotional%2520Talking%2520Head%250ADiffusion%2520%2528ETHD%2529%2520backbone%2520to%2520efficiently%2520generate%2520highly%2520expressive%2520talking%250Ahead%2520videos%252C%2520which%2520contains%2520an%2520Expression%2520Decoupling%2520Injection%2520%2528EDI%2529%2520module%2520to%250Aautomatically%2520decouple%2520the%2520expressions%2520from%2520reference%2520portraits%2520while%250Aintegrating%2520the%2520target%2520expression%2520information%252C%2520achieving%2520more%2520expressive%250Ageneration%2520performance.%2520Experimental%2520results%2520show%2520that%2520EmotiveTalk%2520can%2520generate%250Aexpressive%2520talking%2520head%2520videos%252C%2520ensuring%2520the%2520promised%2520controllability%2520of%250Aemotions%2520and%2520stability%2520during%2520long-time%2520generation%252C%2520yielding%2520state-of-the-art%250Aperformance%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16726v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmotiveTalk%3A%20Expressive%20Talking%20Head%20Generation%20through%20Audio%0A%20%20Information%20Decoupling%20and%20Emotional%20Video%20Diffusion&entry.906535625=Haotian%20Wang%20and%20Yuzhe%20Weng%20and%20Yueyan%20Li%20and%20Zilu%20Guo%20and%20Jun%20Du%20and%20Shutong%20Niu%20and%20Jiefeng%20Ma%20and%20Shan%20He%20and%20Xiaoyan%20Wu%20and%20Qiming%20Hu%20and%20Bing%20Yin%20and%20Cong%20Liu%20and%20Qingfeng%20Liu&entry.1292438233=%20%20Diffusion%20models%20have%20revolutionized%20the%20field%20of%20talking%20head%20generation%2C%0Ayet%20still%20face%20challenges%20in%20expressiveness%2C%20controllability%2C%20and%20stability%20in%0Along-time%20generation.%20In%20this%20research%2C%20we%20propose%20an%20EmotiveTalk%20framework%20to%0Aaddress%20these%20issues.%20Firstly%2C%20to%20realize%20better%20control%20over%20the%20generation%20of%0Alip%20movement%20and%20facial%20expression%2C%20a%20Vision-guided%20Audio%20Information%0ADecoupling%20%28V-AID%29%20approach%20is%20designed%20to%20generate%20audio-based%20decoupled%0Arepresentations%20aligned%20with%20lip%20movements%20and%20expression.%20Specifically%2C%20to%0Aachieve%20alignment%20between%20audio%20and%20facial%20expression%20representation%20spaces%2C%20we%0Apresent%20a%20Diffusion-based%20Co-speech%20Temporal%20Expansion%20%28Di-CTE%29%20module%20within%0AV-AID%20to%20generate%20expression-related%20representations%20under%20multi-source%20emotion%0Acondition%20constraints.%20Then%20we%20propose%20a%20well-designed%20Emotional%20Talking%20Head%0ADiffusion%20%28ETHD%29%20backbone%20to%20efficiently%20generate%20highly%20expressive%20talking%0Ahead%20videos%2C%20which%20contains%20an%20Expression%20Decoupling%20Injection%20%28EDI%29%20module%20to%0Aautomatically%20decouple%20the%20expressions%20from%20reference%20portraits%20while%0Aintegrating%20the%20target%20expression%20information%2C%20achieving%20more%20expressive%0Ageneration%20performance.%20Experimental%20results%20show%20that%20EmotiveTalk%20can%20generate%0Aexpressive%20talking%20head%20videos%2C%20ensuring%20the%20promised%20controllability%20of%0Aemotions%20and%20stability%20during%20long-time%20generation%2C%20yielding%20state-of-the-art%0Aperformance%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16726v2&entry.124074799=Read"},
{"title": "PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension", "author": "Kun Ouyang and Yuanxin Liu and Shicheng Li and Yi Liu and Hao Zhou and Fandong Meng and Jie Zhou and Xu Sun", "abstract": "  Multimodal punchlines, which involve humor or sarcasm conveyed in\nimage-caption pairs, are a popular way of communication on online multimedia\nplatforms. With the rapid development of multimodal large language models\n(MLLMs), it is essential to assess their ability to effectively comprehend\nthese punchlines. However, existing benchmarks on punchline comprehension\nsuffer from three major limitations: 1) language shortcuts that allow models to\nsolely rely on text, 2) lack of question diversity, and 3) narrow focus on a\nspecific domain of multimodal content (e.g., cartoon). To address these\nlimitations, we introduce a multimodal \\textbf{Punch}line comprehension\n\\textbf{Bench}mark, named \\textbf{PunchBench}, which is tailored for accurate\nand comprehensive evaluation of punchline comprehension. To enhance the\nevaluation accuracy, we generate synonymous and antonymous captions by\nmodifying original captions, which mitigates the impact of shortcuts in the\ncaptions. To provide a comprehensive evaluation, PunchBench incorporates\ndiverse question formats and image-captions from various domains. On this\nbasis, we conduct extensive evaluations and reveal a significant gap between\nstate-of-the-art MLLMs and humans in punchline comprehension. To improve\npunchline comprehension, we propose Simple-to-Complex Chain-of-Question\n(SC-CoQ) strategy, enabling the models to incrementally address complicated\nquestions by first mastering simple ones. SC-CoQ effectively enhances the\nperformance of various MLLMs on PunchBench, surpassing in-context learning and\nchain-of-thought.\n", "link": "http://arxiv.org/abs/2412.11906v1", "date": "2024-12-16", "relevancy": 2.4188, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PunchBench%3A%20Benchmarking%20MLLMs%20in%20Multimodal%20Punchline%20Comprehension&body=Title%3A%20PunchBench%3A%20Benchmarking%20MLLMs%20in%20Multimodal%20Punchline%20Comprehension%0AAuthor%3A%20Kun%20Ouyang%20and%20Yuanxin%20Liu%20and%20Shicheng%20Li%20and%20Yi%20Liu%20and%20Hao%20Zhou%20and%20Fandong%20Meng%20and%20Jie%20Zhou%20and%20Xu%20Sun%0AAbstract%3A%20%20%20Multimodal%20punchlines%2C%20which%20involve%20humor%20or%20sarcasm%20conveyed%20in%0Aimage-caption%20pairs%2C%20are%20a%20popular%20way%20of%20communication%20on%20online%20multimedia%0Aplatforms.%20With%20the%20rapid%20development%20of%20multimodal%20large%20language%20models%0A%28MLLMs%29%2C%20it%20is%20essential%20to%20assess%20their%20ability%20to%20effectively%20comprehend%0Athese%20punchlines.%20However%2C%20existing%20benchmarks%20on%20punchline%20comprehension%0Asuffer%20from%20three%20major%20limitations%3A%201%29%20language%20shortcuts%20that%20allow%20models%20to%0Asolely%20rely%20on%20text%2C%202%29%20lack%20of%20question%20diversity%2C%20and%203%29%20narrow%20focus%20on%20a%0Aspecific%20domain%20of%20multimodal%20content%20%28e.g.%2C%20cartoon%29.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20multimodal%20%5Ctextbf%7BPunch%7Dline%20comprehension%0A%5Ctextbf%7BBench%7Dmark%2C%20named%20%5Ctextbf%7BPunchBench%7D%2C%20which%20is%20tailored%20for%20accurate%0Aand%20comprehensive%20evaluation%20of%20punchline%20comprehension.%20To%20enhance%20the%0Aevaluation%20accuracy%2C%20we%20generate%20synonymous%20and%20antonymous%20captions%20by%0Amodifying%20original%20captions%2C%20which%20mitigates%20the%20impact%20of%20shortcuts%20in%20the%0Acaptions.%20To%20provide%20a%20comprehensive%20evaluation%2C%20PunchBench%20incorporates%0Adiverse%20question%20formats%20and%20image-captions%20from%20various%20domains.%20On%20this%0Abasis%2C%20we%20conduct%20extensive%20evaluations%20and%20reveal%20a%20significant%20gap%20between%0Astate-of-the-art%20MLLMs%20and%20humans%20in%20punchline%20comprehension.%20To%20improve%0Apunchline%20comprehension%2C%20we%20propose%20Simple-to-Complex%20Chain-of-Question%0A%28SC-CoQ%29%20strategy%2C%20enabling%20the%20models%20to%20incrementally%20address%20complicated%0Aquestions%20by%20first%20mastering%20simple%20ones.%20SC-CoQ%20effectively%20enhances%20the%0Aperformance%20of%20various%20MLLMs%20on%20PunchBench%2C%20surpassing%20in-context%20learning%20and%0Achain-of-thought.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPunchBench%253A%2520Benchmarking%2520MLLMs%2520in%2520Multimodal%2520Punchline%2520Comprehension%26entry.906535625%3DKun%2520Ouyang%2520and%2520Yuanxin%2520Liu%2520and%2520Shicheng%2520Li%2520and%2520Yi%2520Liu%2520and%2520Hao%2520Zhou%2520and%2520Fandong%2520Meng%2520and%2520Jie%2520Zhou%2520and%2520Xu%2520Sun%26entry.1292438233%3D%2520%2520Multimodal%2520punchlines%252C%2520which%2520involve%2520humor%2520or%2520sarcasm%2520conveyed%2520in%250Aimage-caption%2520pairs%252C%2520are%2520a%2520popular%2520way%2520of%2520communication%2520on%2520online%2520multimedia%250Aplatforms.%2520With%2520the%2520rapid%2520development%2520of%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%252C%2520it%2520is%2520essential%2520to%2520assess%2520their%2520ability%2520to%2520effectively%2520comprehend%250Athese%2520punchlines.%2520However%252C%2520existing%2520benchmarks%2520on%2520punchline%2520comprehension%250Asuffer%2520from%2520three%2520major%2520limitations%253A%25201%2529%2520language%2520shortcuts%2520that%2520allow%2520models%2520to%250Asolely%2520rely%2520on%2520text%252C%25202%2529%2520lack%2520of%2520question%2520diversity%252C%2520and%25203%2529%2520narrow%2520focus%2520on%2520a%250Aspecific%2520domain%2520of%2520multimodal%2520content%2520%2528e.g.%252C%2520cartoon%2529.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520multimodal%2520%255Ctextbf%257BPunch%257Dline%2520comprehension%250A%255Ctextbf%257BBench%257Dmark%252C%2520named%2520%255Ctextbf%257BPunchBench%257D%252C%2520which%2520is%2520tailored%2520for%2520accurate%250Aand%2520comprehensive%2520evaluation%2520of%2520punchline%2520comprehension.%2520To%2520enhance%2520the%250Aevaluation%2520accuracy%252C%2520we%2520generate%2520synonymous%2520and%2520antonymous%2520captions%2520by%250Amodifying%2520original%2520captions%252C%2520which%2520mitigates%2520the%2520impact%2520of%2520shortcuts%2520in%2520the%250Acaptions.%2520To%2520provide%2520a%2520comprehensive%2520evaluation%252C%2520PunchBench%2520incorporates%250Adiverse%2520question%2520formats%2520and%2520image-captions%2520from%2520various%2520domains.%2520On%2520this%250Abasis%252C%2520we%2520conduct%2520extensive%2520evaluations%2520and%2520reveal%2520a%2520significant%2520gap%2520between%250Astate-of-the-art%2520MLLMs%2520and%2520humans%2520in%2520punchline%2520comprehension.%2520To%2520improve%250Apunchline%2520comprehension%252C%2520we%2520propose%2520Simple-to-Complex%2520Chain-of-Question%250A%2528SC-CoQ%2529%2520strategy%252C%2520enabling%2520the%2520models%2520to%2520incrementally%2520address%2520complicated%250Aquestions%2520by%2520first%2520mastering%2520simple%2520ones.%2520SC-CoQ%2520effectively%2520enhances%2520the%250Aperformance%2520of%2520various%2520MLLMs%2520on%2520PunchBench%252C%2520surpassing%2520in-context%2520learning%2520and%250Achain-of-thought.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PunchBench%3A%20Benchmarking%20MLLMs%20in%20Multimodal%20Punchline%20Comprehension&entry.906535625=Kun%20Ouyang%20and%20Yuanxin%20Liu%20and%20Shicheng%20Li%20and%20Yi%20Liu%20and%20Hao%20Zhou%20and%20Fandong%20Meng%20and%20Jie%20Zhou%20and%20Xu%20Sun&entry.1292438233=%20%20Multimodal%20punchlines%2C%20which%20involve%20humor%20or%20sarcasm%20conveyed%20in%0Aimage-caption%20pairs%2C%20are%20a%20popular%20way%20of%20communication%20on%20online%20multimedia%0Aplatforms.%20With%20the%20rapid%20development%20of%20multimodal%20large%20language%20models%0A%28MLLMs%29%2C%20it%20is%20essential%20to%20assess%20their%20ability%20to%20effectively%20comprehend%0Athese%20punchlines.%20However%2C%20existing%20benchmarks%20on%20punchline%20comprehension%0Asuffer%20from%20three%20major%20limitations%3A%201%29%20language%20shortcuts%20that%20allow%20models%20to%0Asolely%20rely%20on%20text%2C%202%29%20lack%20of%20question%20diversity%2C%20and%203%29%20narrow%20focus%20on%20a%0Aspecific%20domain%20of%20multimodal%20content%20%28e.g.%2C%20cartoon%29.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20multimodal%20%5Ctextbf%7BPunch%7Dline%20comprehension%0A%5Ctextbf%7BBench%7Dmark%2C%20named%20%5Ctextbf%7BPunchBench%7D%2C%20which%20is%20tailored%20for%20accurate%0Aand%20comprehensive%20evaluation%20of%20punchline%20comprehension.%20To%20enhance%20the%0Aevaluation%20accuracy%2C%20we%20generate%20synonymous%20and%20antonymous%20captions%20by%0Amodifying%20original%20captions%2C%20which%20mitigates%20the%20impact%20of%20shortcuts%20in%20the%0Acaptions.%20To%20provide%20a%20comprehensive%20evaluation%2C%20PunchBench%20incorporates%0Adiverse%20question%20formats%20and%20image-captions%20from%20various%20domains.%20On%20this%0Abasis%2C%20we%20conduct%20extensive%20evaluations%20and%20reveal%20a%20significant%20gap%20between%0Astate-of-the-art%20MLLMs%20and%20humans%20in%20punchline%20comprehension.%20To%20improve%0Apunchline%20comprehension%2C%20we%20propose%20Simple-to-Complex%20Chain-of-Question%0A%28SC-CoQ%29%20strategy%2C%20enabling%20the%20models%20to%20incrementally%20address%20complicated%0Aquestions%20by%20first%20mastering%20simple%20ones.%20SC-CoQ%20effectively%20enhances%20the%0Aperformance%20of%20various%20MLLMs%20on%20PunchBench%2C%20surpassing%20in-context%20learning%20and%0Achain-of-thought.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11906v1&entry.124074799=Read"},
{"title": "Exploring Vacant Classes in Label-Skewed Federated Learning", "author": "Kuangpu Guo and Yuhe Ding and Jian Liang and Ran He and Zilei Wang and Tieniu Tan", "abstract": "  Label skews, characterized by disparities in local label distribution across\nclients, pose a significant challenge in federated learning. As minority\nclasses suffer from worse accuracy due to overfitting on local imbalanced data,\nprior methods often incorporate class-balanced learning techniques during local\ntraining. Although these methods improve the mean accuracy across all classes,\nwe observe that vacant classes-referring to categories absent from a client's\ndata distribution-remain poorly recognized. Besides, there is still a gap in\nthe accuracy of local models on minority classes compared to the global model.\nThis paper introduces FedVLS, a novel approach to label-skewed federated\nlearning that integrates both vacant-class distillation and logit suppression\nsimultaneously. Specifically, vacant-class distillation leverages knowledge\ndistillation during local training on each client to retain essential\ninformation related to vacant classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedVLS, demonstrating superior performance compared to previous\nstate-of-the-art (SOTA) methods across diverse datasets with varying degrees of\nlabel skews. Our code is available at https://github.com/krumpguo/FedVLS.\n", "link": "http://arxiv.org/abs/2401.02329v3", "date": "2024-12-16", "relevancy": 2.4161, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4962}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4904}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Vacant%20Classes%20in%20Label-Skewed%20Federated%20Learning&body=Title%3A%20Exploring%20Vacant%20Classes%20in%20Label-Skewed%20Federated%20Learning%0AAuthor%3A%20Kuangpu%20Guo%20and%20Yuhe%20Ding%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20Label%20skews%2C%20characterized%20by%20disparities%20in%20local%20label%20distribution%20across%0Aclients%2C%20pose%20a%20significant%20challenge%20in%20federated%20learning.%20As%20minority%0Aclasses%20suffer%20from%20worse%20accuracy%20due%20to%20overfitting%20on%20local%20imbalanced%20data%2C%0Aprior%20methods%20often%20incorporate%20class-balanced%20learning%20techniques%20during%20local%0Atraining.%20Although%20these%20methods%20improve%20the%20mean%20accuracy%20across%20all%20classes%2C%0Awe%20observe%20that%20vacant%20classes-referring%20to%20categories%20absent%20from%20a%20client%27s%0Adata%20distribution-remain%20poorly%20recognized.%20Besides%2C%20there%20is%20still%20a%20gap%20in%0Athe%20accuracy%20of%20local%20models%20on%20minority%20classes%20compared%20to%20the%20global%20model.%0AThis%20paper%20introduces%20FedVLS%2C%20a%20novel%20approach%20to%20label-skewed%20federated%0Alearning%20that%20integrates%20both%20vacant-class%20distillation%20and%20logit%20suppression%0Asimultaneously.%20Specifically%2C%20vacant-class%20distillation%20leverages%20knowledge%0Adistillation%20during%20local%20training%20on%20each%20client%20to%20retain%20essential%0Ainformation%20related%20to%20vacant%20classes%20from%20the%20global%20model.%20Moreover%2C%20logit%0Asuppression%20directly%20penalizes%20network%20logits%20for%20non-label%20classes%2C%0Aeffectively%20addressing%20misclassifications%20in%20minority%20classes%20that%20may%20be%0Abiased%20toward%20majority%20classes.%20Extensive%20experiments%20validate%20the%20efficacy%20of%0AFedVLS%2C%20demonstrating%20superior%20performance%20compared%20to%20previous%0Astate-of-the-art%20%28SOTA%29%20methods%20across%20diverse%20datasets%20with%20varying%20degrees%20of%0Alabel%20skews.%20Our%20code%20is%20available%20at%20https%3A//github.com/krumpguo/FedVLS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02329v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Vacant%2520Classes%2520in%2520Label-Skewed%2520Federated%2520Learning%26entry.906535625%3DKuangpu%2520Guo%2520and%2520Yuhe%2520Ding%2520and%2520Jian%2520Liang%2520and%2520Ran%2520He%2520and%2520Zilei%2520Wang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520Label%2520skews%252C%2520characterized%2520by%2520disparities%2520in%2520local%2520label%2520distribution%2520across%250Aclients%252C%2520pose%2520a%2520significant%2520challenge%2520in%2520federated%2520learning.%2520As%2520minority%250Aclasses%2520suffer%2520from%2520worse%2520accuracy%2520due%2520to%2520overfitting%2520on%2520local%2520imbalanced%2520data%252C%250Aprior%2520methods%2520often%2520incorporate%2520class-balanced%2520learning%2520techniques%2520during%2520local%250Atraining.%2520Although%2520these%2520methods%2520improve%2520the%2520mean%2520accuracy%2520across%2520all%2520classes%252C%250Awe%2520observe%2520that%2520vacant%2520classes-referring%2520to%2520categories%2520absent%2520from%2520a%2520client%2527s%250Adata%2520distribution-remain%2520poorly%2520recognized.%2520Besides%252C%2520there%2520is%2520still%2520a%2520gap%2520in%250Athe%2520accuracy%2520of%2520local%2520models%2520on%2520minority%2520classes%2520compared%2520to%2520the%2520global%2520model.%250AThis%2520paper%2520introduces%2520FedVLS%252C%2520a%2520novel%2520approach%2520to%2520label-skewed%2520federated%250Alearning%2520that%2520integrates%2520both%2520vacant-class%2520distillation%2520and%2520logit%2520suppression%250Asimultaneously.%2520Specifically%252C%2520vacant-class%2520distillation%2520leverages%2520knowledge%250Adistillation%2520during%2520local%2520training%2520on%2520each%2520client%2520to%2520retain%2520essential%250Ainformation%2520related%2520to%2520vacant%2520classes%2520from%2520the%2520global%2520model.%2520Moreover%252C%2520logit%250Asuppression%2520directly%2520penalizes%2520network%2520logits%2520for%2520non-label%2520classes%252C%250Aeffectively%2520addressing%2520misclassifications%2520in%2520minority%2520classes%2520that%2520may%2520be%250Abiased%2520toward%2520majority%2520classes.%2520Extensive%2520experiments%2520validate%2520the%2520efficacy%2520of%250AFedVLS%252C%2520demonstrating%2520superior%2520performance%2520compared%2520to%2520previous%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%2520across%2520diverse%2520datasets%2520with%2520varying%2520degrees%2520of%250Alabel%2520skews.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/krumpguo/FedVLS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02329v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Vacant%20Classes%20in%20Label-Skewed%20Federated%20Learning&entry.906535625=Kuangpu%20Guo%20and%20Yuhe%20Ding%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan&entry.1292438233=%20%20Label%20skews%2C%20characterized%20by%20disparities%20in%20local%20label%20distribution%20across%0Aclients%2C%20pose%20a%20significant%20challenge%20in%20federated%20learning.%20As%20minority%0Aclasses%20suffer%20from%20worse%20accuracy%20due%20to%20overfitting%20on%20local%20imbalanced%20data%2C%0Aprior%20methods%20often%20incorporate%20class-balanced%20learning%20techniques%20during%20local%0Atraining.%20Although%20these%20methods%20improve%20the%20mean%20accuracy%20across%20all%20classes%2C%0Awe%20observe%20that%20vacant%20classes-referring%20to%20categories%20absent%20from%20a%20client%27s%0Adata%20distribution-remain%20poorly%20recognized.%20Besides%2C%20there%20is%20still%20a%20gap%20in%0Athe%20accuracy%20of%20local%20models%20on%20minority%20classes%20compared%20to%20the%20global%20model.%0AThis%20paper%20introduces%20FedVLS%2C%20a%20novel%20approach%20to%20label-skewed%20federated%0Alearning%20that%20integrates%20both%20vacant-class%20distillation%20and%20logit%20suppression%0Asimultaneously.%20Specifically%2C%20vacant-class%20distillation%20leverages%20knowledge%0Adistillation%20during%20local%20training%20on%20each%20client%20to%20retain%20essential%0Ainformation%20related%20to%20vacant%20classes%20from%20the%20global%20model.%20Moreover%2C%20logit%0Asuppression%20directly%20penalizes%20network%20logits%20for%20non-label%20classes%2C%0Aeffectively%20addressing%20misclassifications%20in%20minority%20classes%20that%20may%20be%0Abiased%20toward%20majority%20classes.%20Extensive%20experiments%20validate%20the%20efficacy%20of%0AFedVLS%2C%20demonstrating%20superior%20performance%20compared%20to%20previous%0Astate-of-the-art%20%28SOTA%29%20methods%20across%20diverse%20datasets%20with%20varying%20degrees%20of%0Alabel%20skews.%20Our%20code%20is%20available%20at%20https%3A//github.com/krumpguo/FedVLS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02329v3&entry.124074799=Read"},
{"title": "Robust Synthetic Data-Driven Detection of Living-Off-the-Land Reverse\n  Shells", "author": "Dmitrijs Trizna and Luca Demetrio and Battista Biggio and Fabio Roli", "abstract": "  Living-off-the-land (LOTL) techniques pose a significant challenge to\nsecurity operations, exploiting legitimate tools to execute malicious commands\nthat evade traditional detection methods. To address this, we present a robust\naugmentation framework for cyber defense systems as Security Information and\nEvent Management (SIEM) solutions, enabling the detection of LOTL attacks such\nas reverse shells through machine learning. Leveraging real-world threat\nintelligence and adversarial training, our framework synthesizes diverse\nmalicious datasets while preserving the variability of legitimate activity,\nensuring high accuracy and low false-positive rates. We validate our approach\nthrough extensive experiments on enterprise-scale datasets, achieving a 90\\%\nimprovement in detection rates over non-augmented baselines at an\nindustry-grade False Positive Rate (FPR) of $10^{-5}$. We define black-box\ndata-driven attacks that successfully evade unprotected models, and develop\ndefenses to mitigate them, producing adversarially robust variants of ML\nmodels. Ethical considerations are central to this work; we discuss safeguards\nfor synthetic data generation and the responsible release of pre-trained models\nacross four best performing architectures, including both adversarially and\nregularly trained variants: https://huggingface.co/dtrizna/quasarnix.\nFurthermore, we provide a malicious LOTL dataset containing over 1 million\naugmented attack variants to enable reproducible research and community\ncollaboration: https://huggingface.co/datasets/dtrizna/QuasarNix. This work\noffers a reproducible, scalable, and production-ready defense against evolving\nLOTL threats.\n", "link": "http://arxiv.org/abs/2402.18329v2", "date": "2024-12-16", "relevancy": 2.3937, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.48}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4798}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Synthetic%20Data-Driven%20Detection%20of%20Living-Off-the-Land%20Reverse%0A%20%20Shells&body=Title%3A%20Robust%20Synthetic%20Data-Driven%20Detection%20of%20Living-Off-the-Land%20Reverse%0A%20%20Shells%0AAuthor%3A%20Dmitrijs%20Trizna%20and%20Luca%20Demetrio%20and%20Battista%20Biggio%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Living-off-the-land%20%28LOTL%29%20techniques%20pose%20a%20significant%20challenge%20to%0Asecurity%20operations%2C%20exploiting%20legitimate%20tools%20to%20execute%20malicious%20commands%0Athat%20evade%20traditional%20detection%20methods.%20To%20address%20this%2C%20we%20present%20a%20robust%0Aaugmentation%20framework%20for%20cyber%20defense%20systems%20as%20Security%20Information%20and%0AEvent%20Management%20%28SIEM%29%20solutions%2C%20enabling%20the%20detection%20of%20LOTL%20attacks%20such%0Aas%20reverse%20shells%20through%20machine%20learning.%20Leveraging%20real-world%20threat%0Aintelligence%20and%20adversarial%20training%2C%20our%20framework%20synthesizes%20diverse%0Amalicious%20datasets%20while%20preserving%20the%20variability%20of%20legitimate%20activity%2C%0Aensuring%20high%20accuracy%20and%20low%20false-positive%20rates.%20We%20validate%20our%20approach%0Athrough%20extensive%20experiments%20on%20enterprise-scale%20datasets%2C%20achieving%20a%2090%5C%25%0Aimprovement%20in%20detection%20rates%20over%20non-augmented%20baselines%20at%20an%0Aindustry-grade%20False%20Positive%20Rate%20%28FPR%29%20of%20%2410%5E%7B-5%7D%24.%20We%20define%20black-box%0Adata-driven%20attacks%20that%20successfully%20evade%20unprotected%20models%2C%20and%20develop%0Adefenses%20to%20mitigate%20them%2C%20producing%20adversarially%20robust%20variants%20of%20ML%0Amodels.%20Ethical%20considerations%20are%20central%20to%20this%20work%3B%20we%20discuss%20safeguards%0Afor%20synthetic%20data%20generation%20and%20the%20responsible%20release%20of%20pre-trained%20models%0Aacross%20four%20best%20performing%20architectures%2C%20including%20both%20adversarially%20and%0Aregularly%20trained%20variants%3A%20https%3A//huggingface.co/dtrizna/quasarnix.%0AFurthermore%2C%20we%20provide%20a%20malicious%20LOTL%20dataset%20containing%20over%201%20million%0Aaugmented%20attack%20variants%20to%20enable%20reproducible%20research%20and%20community%0Acollaboration%3A%20https%3A//huggingface.co/datasets/dtrizna/QuasarNix.%20This%20work%0Aoffers%20a%20reproducible%2C%20scalable%2C%20and%20production-ready%20defense%20against%20evolving%0ALOTL%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18329v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Synthetic%2520Data-Driven%2520Detection%2520of%2520Living-Off-the-Land%2520Reverse%250A%2520%2520Shells%26entry.906535625%3DDmitrijs%2520Trizna%2520and%2520Luca%2520Demetrio%2520and%2520Battista%2520Biggio%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Living-off-the-land%2520%2528LOTL%2529%2520techniques%2520pose%2520a%2520significant%2520challenge%2520to%250Asecurity%2520operations%252C%2520exploiting%2520legitimate%2520tools%2520to%2520execute%2520malicious%2520commands%250Athat%2520evade%2520traditional%2520detection%2520methods.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520robust%250Aaugmentation%2520framework%2520for%2520cyber%2520defense%2520systems%2520as%2520Security%2520Information%2520and%250AEvent%2520Management%2520%2528SIEM%2529%2520solutions%252C%2520enabling%2520the%2520detection%2520of%2520LOTL%2520attacks%2520such%250Aas%2520reverse%2520shells%2520through%2520machine%2520learning.%2520Leveraging%2520real-world%2520threat%250Aintelligence%2520and%2520adversarial%2520training%252C%2520our%2520framework%2520synthesizes%2520diverse%250Amalicious%2520datasets%2520while%2520preserving%2520the%2520variability%2520of%2520legitimate%2520activity%252C%250Aensuring%2520high%2520accuracy%2520and%2520low%2520false-positive%2520rates.%2520We%2520validate%2520our%2520approach%250Athrough%2520extensive%2520experiments%2520on%2520enterprise-scale%2520datasets%252C%2520achieving%2520a%252090%255C%2525%250Aimprovement%2520in%2520detection%2520rates%2520over%2520non-augmented%2520baselines%2520at%2520an%250Aindustry-grade%2520False%2520Positive%2520Rate%2520%2528FPR%2529%2520of%2520%252410%255E%257B-5%257D%2524.%2520We%2520define%2520black-box%250Adata-driven%2520attacks%2520that%2520successfully%2520evade%2520unprotected%2520models%252C%2520and%2520develop%250Adefenses%2520to%2520mitigate%2520them%252C%2520producing%2520adversarially%2520robust%2520variants%2520of%2520ML%250Amodels.%2520Ethical%2520considerations%2520are%2520central%2520to%2520this%2520work%253B%2520we%2520discuss%2520safeguards%250Afor%2520synthetic%2520data%2520generation%2520and%2520the%2520responsible%2520release%2520of%2520pre-trained%2520models%250Aacross%2520four%2520best%2520performing%2520architectures%252C%2520including%2520both%2520adversarially%2520and%250Aregularly%2520trained%2520variants%253A%2520https%253A//huggingface.co/dtrizna/quasarnix.%250AFurthermore%252C%2520we%2520provide%2520a%2520malicious%2520LOTL%2520dataset%2520containing%2520over%25201%2520million%250Aaugmented%2520attack%2520variants%2520to%2520enable%2520reproducible%2520research%2520and%2520community%250Acollaboration%253A%2520https%253A//huggingface.co/datasets/dtrizna/QuasarNix.%2520This%2520work%250Aoffers%2520a%2520reproducible%252C%2520scalable%252C%2520and%2520production-ready%2520defense%2520against%2520evolving%250ALOTL%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18329v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Synthetic%20Data-Driven%20Detection%20of%20Living-Off-the-Land%20Reverse%0A%20%20Shells&entry.906535625=Dmitrijs%20Trizna%20and%20Luca%20Demetrio%20and%20Battista%20Biggio%20and%20Fabio%20Roli&entry.1292438233=%20%20Living-off-the-land%20%28LOTL%29%20techniques%20pose%20a%20significant%20challenge%20to%0Asecurity%20operations%2C%20exploiting%20legitimate%20tools%20to%20execute%20malicious%20commands%0Athat%20evade%20traditional%20detection%20methods.%20To%20address%20this%2C%20we%20present%20a%20robust%0Aaugmentation%20framework%20for%20cyber%20defense%20systems%20as%20Security%20Information%20and%0AEvent%20Management%20%28SIEM%29%20solutions%2C%20enabling%20the%20detection%20of%20LOTL%20attacks%20such%0Aas%20reverse%20shells%20through%20machine%20learning.%20Leveraging%20real-world%20threat%0Aintelligence%20and%20adversarial%20training%2C%20our%20framework%20synthesizes%20diverse%0Amalicious%20datasets%20while%20preserving%20the%20variability%20of%20legitimate%20activity%2C%0Aensuring%20high%20accuracy%20and%20low%20false-positive%20rates.%20We%20validate%20our%20approach%0Athrough%20extensive%20experiments%20on%20enterprise-scale%20datasets%2C%20achieving%20a%2090%5C%25%0Aimprovement%20in%20detection%20rates%20over%20non-augmented%20baselines%20at%20an%0Aindustry-grade%20False%20Positive%20Rate%20%28FPR%29%20of%20%2410%5E%7B-5%7D%24.%20We%20define%20black-box%0Adata-driven%20attacks%20that%20successfully%20evade%20unprotected%20models%2C%20and%20develop%0Adefenses%20to%20mitigate%20them%2C%20producing%20adversarially%20robust%20variants%20of%20ML%0Amodels.%20Ethical%20considerations%20are%20central%20to%20this%20work%3B%20we%20discuss%20safeguards%0Afor%20synthetic%20data%20generation%20and%20the%20responsible%20release%20of%20pre-trained%20models%0Aacross%20four%20best%20performing%20architectures%2C%20including%20both%20adversarially%20and%0Aregularly%20trained%20variants%3A%20https%3A//huggingface.co/dtrizna/quasarnix.%0AFurthermore%2C%20we%20provide%20a%20malicious%20LOTL%20dataset%20containing%20over%201%20million%0Aaugmented%20attack%20variants%20to%20enable%20reproducible%20research%20and%20community%0Acollaboration%3A%20https%3A//huggingface.co/datasets/dtrizna/QuasarNix.%20This%20work%0Aoffers%20a%20reproducible%2C%20scalable%2C%20and%20production-ready%20defense%20against%20evolving%0ALOTL%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18329v2&entry.124074799=Read"},
{"title": "DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos\n  Enhanced Kaleidoscopic Images", "author": "Taslim Murad and Prakash Chourasia and Sarwan Ali and Imdad Ullah Khan and Murray Patterson", "abstract": "  Cancer is a complex disease characterized by uncontrolled cell growth. T cell\nreceptors (TCRs), crucial proteins in the immune system, play a key role in\nrecognizing antigens, including those associated with cancer. Recent\nadvancements in sequencing technologies have facilitated comprehensive\nprofiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity\nand enabling TCR-based immunotherapies. However, analyzing these intricate\nbiomolecules necessitates efficient representations that capture their\nstructural and functional information. T-cell protein sequences pose unique\nchallenges due to their relatively smaller lengths compared to other\nbiomolecules. An image-based representation approach becomes a preferred choice\nfor efficient embeddings, allowing for the preservation of essential details\nand enabling comprehensive analysis of T-cell protein sequences. In this paper,\nwe propose to generate images from the protein sequences using the idea of\nChaos Game Representation (CGR) using the Kaleidoscopic images approach. This\nDeep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced\nKaleidoscopic Images (called DANCE) provides a unique way to visualize protein\nsequences by recursively applying chaos game rules around a central seed point.\nwe perform the classification of the T cell receptors (TCRs) protein sequences\nin terms of their respective target cancer cells, as TCRs are known for their\nimmune response against cancer disease. The TCR sequences are converted into\nimages using the DANCE method. We employ deep-learning vision models to perform\nthe classification to obtain insights into the relationship between the visual\npatterns observed in the generated kaleidoscopic images and the underlying\nprotein properties. By combining CGR-based image generation with deep learning\nclassification, this study opens novel possibilities in the protein analysis\ndomain.\n", "link": "http://arxiv.org/abs/2409.06694v2", "date": "2024-12-16", "relevancy": 2.3595, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4724}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images&body=Title%3A%20DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images%0AAuthor%3A%20Taslim%20Murad%20and%20Prakash%20Chourasia%20and%20Sarwan%20Ali%20and%20Imdad%20Ullah%20Khan%20and%20Murray%20Patterson%0AAbstract%3A%20%20%20Cancer%20is%20a%20complex%20disease%20characterized%20by%20uncontrolled%20cell%20growth.%20T%20cell%0Areceptors%20%28TCRs%29%2C%20crucial%20proteins%20in%20the%20immune%20system%2C%20play%20a%20key%20role%20in%0Arecognizing%20antigens%2C%20including%20those%20associated%20with%20cancer.%20Recent%0Aadvancements%20in%20sequencing%20technologies%20have%20facilitated%20comprehensive%0Aprofiling%20of%20TCR%20repertoires%2C%20uncovering%20TCRs%20with%20potent%20anti-cancer%20activity%0Aand%20enabling%20TCR-based%20immunotherapies.%20However%2C%20analyzing%20these%20intricate%0Abiomolecules%20necessitates%20efficient%20representations%20that%20capture%20their%0Astructural%20and%20functional%20information.%20T-cell%20protein%20sequences%20pose%20unique%0Achallenges%20due%20to%20their%20relatively%20smaller%20lengths%20compared%20to%20other%0Abiomolecules.%20An%20image-based%20representation%20approach%20becomes%20a%20preferred%20choice%0Afor%20efficient%20embeddings%2C%20allowing%20for%20the%20preservation%20of%20essential%20details%0Aand%20enabling%20comprehensive%20analysis%20of%20T-cell%20protein%20sequences.%20In%20this%20paper%2C%0Awe%20propose%20to%20generate%20images%20from%20the%20protein%20sequences%20using%20the%20idea%20of%0AChaos%20Game%20Representation%20%28CGR%29%20using%20the%20Kaleidoscopic%20images%20approach.%20This%0ADeep%20Learning%20Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%20Enhanced%0AKaleidoscopic%20Images%20%28called%20DANCE%29%20provides%20a%20unique%20way%20to%20visualize%20protein%0Asequences%20by%20recursively%20applying%20chaos%20game%20rules%20around%20a%20central%20seed%20point.%0Awe%20perform%20the%20classification%20of%20the%20T%20cell%20receptors%20%28TCRs%29%20protein%20sequences%0Ain%20terms%20of%20their%20respective%20target%20cancer%20cells%2C%20as%20TCRs%20are%20known%20for%20their%0Aimmune%20response%20against%20cancer%20disease.%20The%20TCR%20sequences%20are%20converted%20into%0Aimages%20using%20the%20DANCE%20method.%20We%20employ%20deep-learning%20vision%20models%20to%20perform%0Athe%20classification%20to%20obtain%20insights%20into%20the%20relationship%20between%20the%20visual%0Apatterns%20observed%20in%20the%20generated%20kaleidoscopic%20images%20and%20the%20underlying%0Aprotein%20properties.%20By%20combining%20CGR-based%20image%20generation%20with%20deep%20learning%0Aclassification%2C%20this%20study%20opens%20novel%20possibilities%20in%20the%20protein%20analysis%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06694v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDANCE%253A%2520Deep%2520Learning-Assisted%2520Analysis%2520of%2520Protein%2520Sequences%2520Using%2520Chaos%250A%2520%2520Enhanced%2520Kaleidoscopic%2520Images%26entry.906535625%3DTaslim%2520Murad%2520and%2520Prakash%2520Chourasia%2520and%2520Sarwan%2520Ali%2520and%2520Imdad%2520Ullah%2520Khan%2520and%2520Murray%2520Patterson%26entry.1292438233%3D%2520%2520Cancer%2520is%2520a%2520complex%2520disease%2520characterized%2520by%2520uncontrolled%2520cell%2520growth.%2520T%2520cell%250Areceptors%2520%2528TCRs%2529%252C%2520crucial%2520proteins%2520in%2520the%2520immune%2520system%252C%2520play%2520a%2520key%2520role%2520in%250Arecognizing%2520antigens%252C%2520including%2520those%2520associated%2520with%2520cancer.%2520Recent%250Aadvancements%2520in%2520sequencing%2520technologies%2520have%2520facilitated%2520comprehensive%250Aprofiling%2520of%2520TCR%2520repertoires%252C%2520uncovering%2520TCRs%2520with%2520potent%2520anti-cancer%2520activity%250Aand%2520enabling%2520TCR-based%2520immunotherapies.%2520However%252C%2520analyzing%2520these%2520intricate%250Abiomolecules%2520necessitates%2520efficient%2520representations%2520that%2520capture%2520their%250Astructural%2520and%2520functional%2520information.%2520T-cell%2520protein%2520sequences%2520pose%2520unique%250Achallenges%2520due%2520to%2520their%2520relatively%2520smaller%2520lengths%2520compared%2520to%2520other%250Abiomolecules.%2520An%2520image-based%2520representation%2520approach%2520becomes%2520a%2520preferred%2520choice%250Afor%2520efficient%2520embeddings%252C%2520allowing%2520for%2520the%2520preservation%2520of%2520essential%2520details%250Aand%2520enabling%2520comprehensive%2520analysis%2520of%2520T-cell%2520protein%2520sequences.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520to%2520generate%2520images%2520from%2520the%2520protein%2520sequences%2520using%2520the%2520idea%2520of%250AChaos%2520Game%2520Representation%2520%2528CGR%2529%2520using%2520the%2520Kaleidoscopic%2520images%2520approach.%2520This%250ADeep%2520Learning%2520Assisted%2520Analysis%2520of%2520Protein%2520Sequences%2520Using%2520Chaos%2520Enhanced%250AKaleidoscopic%2520Images%2520%2528called%2520DANCE%2529%2520provides%2520a%2520unique%2520way%2520to%2520visualize%2520protein%250Asequences%2520by%2520recursively%2520applying%2520chaos%2520game%2520rules%2520around%2520a%2520central%2520seed%2520point.%250Awe%2520perform%2520the%2520classification%2520of%2520the%2520T%2520cell%2520receptors%2520%2528TCRs%2529%2520protein%2520sequences%250Ain%2520terms%2520of%2520their%2520respective%2520target%2520cancer%2520cells%252C%2520as%2520TCRs%2520are%2520known%2520for%2520their%250Aimmune%2520response%2520against%2520cancer%2520disease.%2520The%2520TCR%2520sequences%2520are%2520converted%2520into%250Aimages%2520using%2520the%2520DANCE%2520method.%2520We%2520employ%2520deep-learning%2520vision%2520models%2520to%2520perform%250Athe%2520classification%2520to%2520obtain%2520insights%2520into%2520the%2520relationship%2520between%2520the%2520visual%250Apatterns%2520observed%2520in%2520the%2520generated%2520kaleidoscopic%2520images%2520and%2520the%2520underlying%250Aprotein%2520properties.%2520By%2520combining%2520CGR-based%2520image%2520generation%2520with%2520deep%2520learning%250Aclassification%252C%2520this%2520study%2520opens%2520novel%2520possibilities%2520in%2520the%2520protein%2520analysis%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06694v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images&entry.906535625=Taslim%20Murad%20and%20Prakash%20Chourasia%20and%20Sarwan%20Ali%20and%20Imdad%20Ullah%20Khan%20and%20Murray%20Patterson&entry.1292438233=%20%20Cancer%20is%20a%20complex%20disease%20characterized%20by%20uncontrolled%20cell%20growth.%20T%20cell%0Areceptors%20%28TCRs%29%2C%20crucial%20proteins%20in%20the%20immune%20system%2C%20play%20a%20key%20role%20in%0Arecognizing%20antigens%2C%20including%20those%20associated%20with%20cancer.%20Recent%0Aadvancements%20in%20sequencing%20technologies%20have%20facilitated%20comprehensive%0Aprofiling%20of%20TCR%20repertoires%2C%20uncovering%20TCRs%20with%20potent%20anti-cancer%20activity%0Aand%20enabling%20TCR-based%20immunotherapies.%20However%2C%20analyzing%20these%20intricate%0Abiomolecules%20necessitates%20efficient%20representations%20that%20capture%20their%0Astructural%20and%20functional%20information.%20T-cell%20protein%20sequences%20pose%20unique%0Achallenges%20due%20to%20their%20relatively%20smaller%20lengths%20compared%20to%20other%0Abiomolecules.%20An%20image-based%20representation%20approach%20becomes%20a%20preferred%20choice%0Afor%20efficient%20embeddings%2C%20allowing%20for%20the%20preservation%20of%20essential%20details%0Aand%20enabling%20comprehensive%20analysis%20of%20T-cell%20protein%20sequences.%20In%20this%20paper%2C%0Awe%20propose%20to%20generate%20images%20from%20the%20protein%20sequences%20using%20the%20idea%20of%0AChaos%20Game%20Representation%20%28CGR%29%20using%20the%20Kaleidoscopic%20images%20approach.%20This%0ADeep%20Learning%20Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%20Enhanced%0AKaleidoscopic%20Images%20%28called%20DANCE%29%20provides%20a%20unique%20way%20to%20visualize%20protein%0Asequences%20by%20recursively%20applying%20chaos%20game%20rules%20around%20a%20central%20seed%20point.%0Awe%20perform%20the%20classification%20of%20the%20T%20cell%20receptors%20%28TCRs%29%20protein%20sequences%0Ain%20terms%20of%20their%20respective%20target%20cancer%20cells%2C%20as%20TCRs%20are%20known%20for%20their%0Aimmune%20response%20against%20cancer%20disease.%20The%20TCR%20sequences%20are%20converted%20into%0Aimages%20using%20the%20DANCE%20method.%20We%20employ%20deep-learning%20vision%20models%20to%20perform%0Athe%20classification%20to%20obtain%20insights%20into%20the%20relationship%20between%20the%20visual%0Apatterns%20observed%20in%20the%20generated%20kaleidoscopic%20images%20and%20the%20underlying%0Aprotein%20properties.%20By%20combining%20CGR-based%20image%20generation%20with%20deep%20learning%0Aclassification%2C%20this%20study%20opens%20novel%20possibilities%20in%20the%20protein%20analysis%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06694v2&entry.124074799=Read"},
{"title": "IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and\n  Illuminations", "author": "Zhibing Li and Tong Wu and Jing Tan and Mengchen Zhang and Jiaqi Wang and Dahua Lin", "abstract": "  Capturing geometric and material information from images remains a\nfundamental challenge in computer vision and graphics. Traditional\noptimization-based methods often require hours of computational time to\nreconstruct geometry, material properties, and environmental lighting from\ndense multi-view inputs, while still struggling with inherent ambiguities\nbetween lighting and material. On the other hand, learning-based approaches\nleverage rich material priors from existing 3D object datasets but face\nchallenges with maintaining multi-view consistency. In this paper, we introduce\nIDArb, a diffusion-based model designed to perform intrinsic decomposition on\nan arbitrary number of images under varying illuminations. Our method achieves\naccurate and multi-view consistent estimation on surface normals and material\nproperties. This is made possible through a novel cross-view, cross-domain\nattention module and an illumination-augmented, view-adaptive training\nstrategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides\nlarge-scale multi-view intrinsic data and renderings under diverse lighting\nconditions, supporting robust training. Extensive experiments demonstrate that\nIDArb outperforms state-of-the-art methods both qualitatively and\nquantitatively. Moreover, our approach facilitates a range of downstream tasks,\nincluding single-image relighting, photometric stereo, and 3D reconstruction,\nhighlighting its broad applications in realistic 3D content creation.\n", "link": "http://arxiv.org/abs/2412.12083v1", "date": "2024-12-16", "relevancy": 2.3188, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5821}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5821}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDArb%3A%20Intrinsic%20Decomposition%20for%20Arbitrary%20Number%20of%20Input%20Views%20and%0A%20%20Illuminations&body=Title%3A%20IDArb%3A%20Intrinsic%20Decomposition%20for%20Arbitrary%20Number%20of%20Input%20Views%20and%0A%20%20Illuminations%0AAuthor%3A%20Zhibing%20Li%20and%20Tong%20Wu%20and%20Jing%20Tan%20and%20Mengchen%20Zhang%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20Capturing%20geometric%20and%20material%20information%20from%20images%20remains%20a%0Afundamental%20challenge%20in%20computer%20vision%20and%20graphics.%20Traditional%0Aoptimization-based%20methods%20often%20require%20hours%20of%20computational%20time%20to%0Areconstruct%20geometry%2C%20material%20properties%2C%20and%20environmental%20lighting%20from%0Adense%20multi-view%20inputs%2C%20while%20still%20struggling%20with%20inherent%20ambiguities%0Abetween%20lighting%20and%20material.%20On%20the%20other%20hand%2C%20learning-based%20approaches%0Aleverage%20rich%20material%20priors%20from%20existing%203D%20object%20datasets%20but%20face%0Achallenges%20with%20maintaining%20multi-view%20consistency.%20In%20this%20paper%2C%20we%20introduce%0AIDArb%2C%20a%20diffusion-based%20model%20designed%20to%20perform%20intrinsic%20decomposition%20on%0Aan%20arbitrary%20number%20of%20images%20under%20varying%20illuminations.%20Our%20method%20achieves%0Aaccurate%20and%20multi-view%20consistent%20estimation%20on%20surface%20normals%20and%20material%0Aproperties.%20This%20is%20made%20possible%20through%20a%20novel%20cross-view%2C%20cross-domain%0Aattention%20module%20and%20an%20illumination-augmented%2C%20view-adaptive%20training%0Astrategy.%20Additionally%2C%20we%20introduce%20ARB-Objaverse%2C%20a%20new%20dataset%20that%20provides%0Alarge-scale%20multi-view%20intrinsic%20data%20and%20renderings%20under%20diverse%20lighting%0Aconditions%2C%20supporting%20robust%20training.%20Extensive%20experiments%20demonstrate%20that%0AIDArb%20outperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%0Aquantitatively.%20Moreover%2C%20our%20approach%20facilitates%20a%20range%20of%20downstream%20tasks%2C%0Aincluding%20single-image%20relighting%2C%20photometric%20stereo%2C%20and%203D%20reconstruction%2C%0Ahighlighting%20its%20broad%20applications%20in%20realistic%203D%20content%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDArb%253A%2520Intrinsic%2520Decomposition%2520for%2520Arbitrary%2520Number%2520of%2520Input%2520Views%2520and%250A%2520%2520Illuminations%26entry.906535625%3DZhibing%2520Li%2520and%2520Tong%2520Wu%2520and%2520Jing%2520Tan%2520and%2520Mengchen%2520Zhang%2520and%2520Jiaqi%2520Wang%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520Capturing%2520geometric%2520and%2520material%2520information%2520from%2520images%2520remains%2520a%250Afundamental%2520challenge%2520in%2520computer%2520vision%2520and%2520graphics.%2520Traditional%250Aoptimization-based%2520methods%2520often%2520require%2520hours%2520of%2520computational%2520time%2520to%250Areconstruct%2520geometry%252C%2520material%2520properties%252C%2520and%2520environmental%2520lighting%2520from%250Adense%2520multi-view%2520inputs%252C%2520while%2520still%2520struggling%2520with%2520inherent%2520ambiguities%250Abetween%2520lighting%2520and%2520material.%2520On%2520the%2520other%2520hand%252C%2520learning-based%2520approaches%250Aleverage%2520rich%2520material%2520priors%2520from%2520existing%25203D%2520object%2520datasets%2520but%2520face%250Achallenges%2520with%2520maintaining%2520multi-view%2520consistency.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AIDArb%252C%2520a%2520diffusion-based%2520model%2520designed%2520to%2520perform%2520intrinsic%2520decomposition%2520on%250Aan%2520arbitrary%2520number%2520of%2520images%2520under%2520varying%2520illuminations.%2520Our%2520method%2520achieves%250Aaccurate%2520and%2520multi-view%2520consistent%2520estimation%2520on%2520surface%2520normals%2520and%2520material%250Aproperties.%2520This%2520is%2520made%2520possible%2520through%2520a%2520novel%2520cross-view%252C%2520cross-domain%250Aattention%2520module%2520and%2520an%2520illumination-augmented%252C%2520view-adaptive%2520training%250Astrategy.%2520Additionally%252C%2520we%2520introduce%2520ARB-Objaverse%252C%2520a%2520new%2520dataset%2520that%2520provides%250Alarge-scale%2520multi-view%2520intrinsic%2520data%2520and%2520renderings%2520under%2520diverse%2520lighting%250Aconditions%252C%2520supporting%2520robust%2520training.%2520Extensive%2520experiments%2520demonstrate%2520that%250AIDArb%2520outperforms%2520state-of-the-art%2520methods%2520both%2520qualitatively%2520and%250Aquantitatively.%2520Moreover%252C%2520our%2520approach%2520facilitates%2520a%2520range%2520of%2520downstream%2520tasks%252C%250Aincluding%2520single-image%2520relighting%252C%2520photometric%2520stereo%252C%2520and%25203D%2520reconstruction%252C%250Ahighlighting%2520its%2520broad%2520applications%2520in%2520realistic%25203D%2520content%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDArb%3A%20Intrinsic%20Decomposition%20for%20Arbitrary%20Number%20of%20Input%20Views%20and%0A%20%20Illuminations&entry.906535625=Zhibing%20Li%20and%20Tong%20Wu%20and%20Jing%20Tan%20and%20Mengchen%20Zhang%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin&entry.1292438233=%20%20Capturing%20geometric%20and%20material%20information%20from%20images%20remains%20a%0Afundamental%20challenge%20in%20computer%20vision%20and%20graphics.%20Traditional%0Aoptimization-based%20methods%20often%20require%20hours%20of%20computational%20time%20to%0Areconstruct%20geometry%2C%20material%20properties%2C%20and%20environmental%20lighting%20from%0Adense%20multi-view%20inputs%2C%20while%20still%20struggling%20with%20inherent%20ambiguities%0Abetween%20lighting%20and%20material.%20On%20the%20other%20hand%2C%20learning-based%20approaches%0Aleverage%20rich%20material%20priors%20from%20existing%203D%20object%20datasets%20but%20face%0Achallenges%20with%20maintaining%20multi-view%20consistency.%20In%20this%20paper%2C%20we%20introduce%0AIDArb%2C%20a%20diffusion-based%20model%20designed%20to%20perform%20intrinsic%20decomposition%20on%0Aan%20arbitrary%20number%20of%20images%20under%20varying%20illuminations.%20Our%20method%20achieves%0Aaccurate%20and%20multi-view%20consistent%20estimation%20on%20surface%20normals%20and%20material%0Aproperties.%20This%20is%20made%20possible%20through%20a%20novel%20cross-view%2C%20cross-domain%0Aattention%20module%20and%20an%20illumination-augmented%2C%20view-adaptive%20training%0Astrategy.%20Additionally%2C%20we%20introduce%20ARB-Objaverse%2C%20a%20new%20dataset%20that%20provides%0Alarge-scale%20multi-view%20intrinsic%20data%20and%20renderings%20under%20diverse%20lighting%0Aconditions%2C%20supporting%20robust%20training.%20Extensive%20experiments%20demonstrate%20that%0AIDArb%20outperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%0Aquantitatively.%20Moreover%2C%20our%20approach%20facilitates%20a%20range%20of%20downstream%20tasks%2C%0Aincluding%20single-image%20relighting%2C%20photometric%20stereo%2C%20and%203D%20reconstruction%2C%0Ahighlighting%20its%20broad%20applications%20in%20realistic%203D%20content%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12083v1&entry.124074799=Read"},
{"title": "autrainer: A Modular and Extensible Deep Learning Toolkit for Computer\n  Audition Tasks", "author": "Simon Rampp and Andreas Triantafyllopoulos and Manuel Milling and Bj\u00f6rn W. Schuller", "abstract": "  This work introduces the key operating principles for autrainer, our new deep\nlearning training framework for computer audition tasks. autrainer is a\nPyTorch-based toolkit that allows for rapid, reproducible, and easily\nextensible training on a variety of different computer audition tasks.\nConcretely, autrainer offers low-code training and supports a wide range of\nneural networks as well as preprocessing routines. In this work, we present an\noverview of its inner workings and key capabilities.\n", "link": "http://arxiv.org/abs/2412.11943v1", "date": "2024-12-16", "relevancy": 2.307, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4647}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.462}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20autrainer%3A%20A%20Modular%20and%20Extensible%20Deep%20Learning%20Toolkit%20for%20Computer%0A%20%20Audition%20Tasks&body=Title%3A%20autrainer%3A%20A%20Modular%20and%20Extensible%20Deep%20Learning%20Toolkit%20for%20Computer%0A%20%20Audition%20Tasks%0AAuthor%3A%20Simon%20Rampp%20and%20Andreas%20Triantafyllopoulos%20and%20Manuel%20Milling%20and%20Bj%C3%B6rn%20W.%20Schuller%0AAbstract%3A%20%20%20This%20work%20introduces%20the%20key%20operating%20principles%20for%20autrainer%2C%20our%20new%20deep%0Alearning%20training%20framework%20for%20computer%20audition%20tasks.%20autrainer%20is%20a%0APyTorch-based%20toolkit%20that%20allows%20for%20rapid%2C%20reproducible%2C%20and%20easily%0Aextensible%20training%20on%20a%20variety%20of%20different%20computer%20audition%20tasks.%0AConcretely%2C%20autrainer%20offers%20low-code%20training%20and%20supports%20a%20wide%20range%20of%0Aneural%20networks%20as%20well%20as%20preprocessing%20routines.%20In%20this%20work%2C%20we%20present%20an%0Aoverview%20of%20its%20inner%20workings%20and%20key%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dautrainer%253A%2520A%2520Modular%2520and%2520Extensible%2520Deep%2520Learning%2520Toolkit%2520for%2520Computer%250A%2520%2520Audition%2520Tasks%26entry.906535625%3DSimon%2520Rampp%2520and%2520Andreas%2520Triantafyllopoulos%2520and%2520Manuel%2520Milling%2520and%2520Bj%25C3%25B6rn%2520W.%2520Schuller%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520the%2520key%2520operating%2520principles%2520for%2520autrainer%252C%2520our%2520new%2520deep%250Alearning%2520training%2520framework%2520for%2520computer%2520audition%2520tasks.%2520autrainer%2520is%2520a%250APyTorch-based%2520toolkit%2520that%2520allows%2520for%2520rapid%252C%2520reproducible%252C%2520and%2520easily%250Aextensible%2520training%2520on%2520a%2520variety%2520of%2520different%2520computer%2520audition%2520tasks.%250AConcretely%252C%2520autrainer%2520offers%2520low-code%2520training%2520and%2520supports%2520a%2520wide%2520range%2520of%250Aneural%2520networks%2520as%2520well%2520as%2520preprocessing%2520routines.%2520In%2520this%2520work%252C%2520we%2520present%2520an%250Aoverview%2520of%2520its%2520inner%2520workings%2520and%2520key%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=autrainer%3A%20A%20Modular%20and%20Extensible%20Deep%20Learning%20Toolkit%20for%20Computer%0A%20%20Audition%20Tasks&entry.906535625=Simon%20Rampp%20and%20Andreas%20Triantafyllopoulos%20and%20Manuel%20Milling%20and%20Bj%C3%B6rn%20W.%20Schuller&entry.1292438233=%20%20This%20work%20introduces%20the%20key%20operating%20principles%20for%20autrainer%2C%20our%20new%20deep%0Alearning%20training%20framework%20for%20computer%20audition%20tasks.%20autrainer%20is%20a%0APyTorch-based%20toolkit%20that%20allows%20for%20rapid%2C%20reproducible%2C%20and%20easily%0Aextensible%20training%20on%20a%20variety%20of%20different%20computer%20audition%20tasks.%0AConcretely%2C%20autrainer%20offers%20low-code%20training%20and%20supports%20a%20wide%20range%20of%0Aneural%20networks%20as%20well%20as%20preprocessing%20routines.%20In%20this%20work%2C%20we%20present%20an%0Aoverview%20of%20its%20inner%20workings%20and%20key%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11943v1&entry.124074799=Read"},
{"title": "Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment", "author": "Jun Liu and Zhenglun Kong and Pu Zhao and Changdi Yang and Hao Tang and Xuan Shen and Geng Yuan and Wei Niu and Wenbin Zhang and Xue Lin and Dong Huang and Yanzhi Wang", "abstract": "  Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82\\% in\naccuracy across seven downstream tasks when pruning LLaMA-7B by 50\\%.\n", "link": "http://arxiv.org/abs/2403.10799v4", "date": "2024-12-16", "relevancy": 2.3067, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4593}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Adaptive%20Large%20Language%20Models%20Structured%20Pruning%20via%0A%20%20Hybrid-grained%20Weight%20Importance%20Assessment&body=Title%3A%20Toward%20Adaptive%20Large%20Language%20Models%20Structured%20Pruning%20via%0A%20%20Hybrid-grained%20Weight%20Importance%20Assessment%0AAuthor%3A%20Jun%20Liu%20and%20Zhenglun%20Kong%20and%20Pu%20Zhao%20and%20Changdi%20Yang%20and%20Hao%20Tang%20and%20Xuan%20Shen%20and%20Geng%20Yuan%20and%20Wei%20Niu%20and%20Wenbin%20Zhang%20and%20Xue%20Lin%20and%20Dong%20Huang%20and%20Yanzhi%20Wang%0AAbstract%3A%20%20%20Structured%20pruning%20for%20large%20language%20models%20%28LLMs%29%20has%20garnered%20significant%0Aacademic%20interest%20due%20to%20its%20ability%20to%20efficiently%20compress%20and%20accelerate%0ALLMs%20by%20eliminating%20redundant%20weight%20groups%20at%20a%20coarse-grained%20granularity.%0ACurrent%20structured%20pruning%20methods%20for%20LLMs%20typically%20depend%20on%20a%20singular%0Agranularity%20for%20assessing%20weight%20importance%2C%20resulting%20in%20notable%20performance%0Adegradation%20in%20downstream%20tasks.%20Intriguingly%2C%20our%20empirical%20investigations%0Areveal%20that%20utilizing%20unstructured%20pruning%2C%20which%20achieves%20better%20performance%0Aretention%20by%20pruning%20weights%20at%20a%20finer%20granularity%2C%20%5Cemph%7Bi.e.%7D%2C%20individual%0Aweights%2C%20yields%20significantly%20varied%20sparse%20LLM%20structures%20when%20juxtaposed%20to%0Astructured%20pruning.%20This%20suggests%20that%20evaluating%20both%20holistic%20and%20individual%0Aassessment%20for%20weight%20importance%20is%20essential%20for%20LLM%20pruning.%20Building%20on%20this%0Ainsight%2C%20we%20introduce%20the%20Hybrid-grained%20Weight%20Importance%20Assessment%20%28HyWIA%29%2C%0Aa%20novel%20method%20that%20merges%20fine-grained%20and%20coarse-grained%20evaluations%20of%0Aweight%20importance%20for%20the%20pruning%20of%20LLMs.%20Leveraging%20an%20attention%20mechanism%2C%0AHyWIA%20adaptively%20determines%20the%20optimal%20blend%20of%20granularity%20in%20weight%0Aimportance%20assessments%20in%20an%20end-to-end%20pruning%20manner.%20Extensive%20experiments%0Aon%20LLaMA-V1/V2%2C%20Vicuna%2C%20Baichuan%2C%20and%20Bloom%20across%20various%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20HyWIA%20in%20pruning%20LLMs.%20For%20example%2C%20HyWIA%0Asurpasses%20the%20cutting-edge%20LLM-Pruner%20by%20an%20average%20margin%20of%202.82%5C%25%20in%0Aaccuracy%20across%20seven%20downstream%20tasks%20when%20pruning%20LLaMA-7B%20by%2050%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10799v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Adaptive%2520Large%2520Language%2520Models%2520Structured%2520Pruning%2520via%250A%2520%2520Hybrid-grained%2520Weight%2520Importance%2520Assessment%26entry.906535625%3DJun%2520Liu%2520and%2520Zhenglun%2520Kong%2520and%2520Pu%2520Zhao%2520and%2520Changdi%2520Yang%2520and%2520Hao%2520Tang%2520and%2520Xuan%2520Shen%2520and%2520Geng%2520Yuan%2520and%2520Wei%2520Niu%2520and%2520Wenbin%2520Zhang%2520and%2520Xue%2520Lin%2520and%2520Dong%2520Huang%2520and%2520Yanzhi%2520Wang%26entry.1292438233%3D%2520%2520Structured%2520pruning%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520garnered%2520significant%250Aacademic%2520interest%2520due%2520to%2520its%2520ability%2520to%2520efficiently%2520compress%2520and%2520accelerate%250ALLMs%2520by%2520eliminating%2520redundant%2520weight%2520groups%2520at%2520a%2520coarse-grained%2520granularity.%250ACurrent%2520structured%2520pruning%2520methods%2520for%2520LLMs%2520typically%2520depend%2520on%2520a%2520singular%250Agranularity%2520for%2520assessing%2520weight%2520importance%252C%2520resulting%2520in%2520notable%2520performance%250Adegradation%2520in%2520downstream%2520tasks.%2520Intriguingly%252C%2520our%2520empirical%2520investigations%250Areveal%2520that%2520utilizing%2520unstructured%2520pruning%252C%2520which%2520achieves%2520better%2520performance%250Aretention%2520by%2520pruning%2520weights%2520at%2520a%2520finer%2520granularity%252C%2520%255Cemph%257Bi.e.%257D%252C%2520individual%250Aweights%252C%2520yields%2520significantly%2520varied%2520sparse%2520LLM%2520structures%2520when%2520juxtaposed%2520to%250Astructured%2520pruning.%2520This%2520suggests%2520that%2520evaluating%2520both%2520holistic%2520and%2520individual%250Aassessment%2520for%2520weight%2520importance%2520is%2520essential%2520for%2520LLM%2520pruning.%2520Building%2520on%2520this%250Ainsight%252C%2520we%2520introduce%2520the%2520Hybrid-grained%2520Weight%2520Importance%2520Assessment%2520%2528HyWIA%2529%252C%250Aa%2520novel%2520method%2520that%2520merges%2520fine-grained%2520and%2520coarse-grained%2520evaluations%2520of%250Aweight%2520importance%2520for%2520the%2520pruning%2520of%2520LLMs.%2520Leveraging%2520an%2520attention%2520mechanism%252C%250AHyWIA%2520adaptively%2520determines%2520the%2520optimal%2520blend%2520of%2520granularity%2520in%2520weight%250Aimportance%2520assessments%2520in%2520an%2520end-to-end%2520pruning%2520manner.%2520Extensive%2520experiments%250Aon%2520LLaMA-V1/V2%252C%2520Vicuna%252C%2520Baichuan%252C%2520and%2520Bloom%2520across%2520various%2520benchmarks%250Ademonstrate%2520the%2520effectiveness%2520of%2520HyWIA%2520in%2520pruning%2520LLMs.%2520For%2520example%252C%2520HyWIA%250Asurpasses%2520the%2520cutting-edge%2520LLM-Pruner%2520by%2520an%2520average%2520margin%2520of%25202.82%255C%2525%2520in%250Aaccuracy%2520across%2520seven%2520downstream%2520tasks%2520when%2520pruning%2520LLaMA-7B%2520by%252050%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10799v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Adaptive%20Large%20Language%20Models%20Structured%20Pruning%20via%0A%20%20Hybrid-grained%20Weight%20Importance%20Assessment&entry.906535625=Jun%20Liu%20and%20Zhenglun%20Kong%20and%20Pu%20Zhao%20and%20Changdi%20Yang%20and%20Hao%20Tang%20and%20Xuan%20Shen%20and%20Geng%20Yuan%20and%20Wei%20Niu%20and%20Wenbin%20Zhang%20and%20Xue%20Lin%20and%20Dong%20Huang%20and%20Yanzhi%20Wang&entry.1292438233=%20%20Structured%20pruning%20for%20large%20language%20models%20%28LLMs%29%20has%20garnered%20significant%0Aacademic%20interest%20due%20to%20its%20ability%20to%20efficiently%20compress%20and%20accelerate%0ALLMs%20by%20eliminating%20redundant%20weight%20groups%20at%20a%20coarse-grained%20granularity.%0ACurrent%20structured%20pruning%20methods%20for%20LLMs%20typically%20depend%20on%20a%20singular%0Agranularity%20for%20assessing%20weight%20importance%2C%20resulting%20in%20notable%20performance%0Adegradation%20in%20downstream%20tasks.%20Intriguingly%2C%20our%20empirical%20investigations%0Areveal%20that%20utilizing%20unstructured%20pruning%2C%20which%20achieves%20better%20performance%0Aretention%20by%20pruning%20weights%20at%20a%20finer%20granularity%2C%20%5Cemph%7Bi.e.%7D%2C%20individual%0Aweights%2C%20yields%20significantly%20varied%20sparse%20LLM%20structures%20when%20juxtaposed%20to%0Astructured%20pruning.%20This%20suggests%20that%20evaluating%20both%20holistic%20and%20individual%0Aassessment%20for%20weight%20importance%20is%20essential%20for%20LLM%20pruning.%20Building%20on%20this%0Ainsight%2C%20we%20introduce%20the%20Hybrid-grained%20Weight%20Importance%20Assessment%20%28HyWIA%29%2C%0Aa%20novel%20method%20that%20merges%20fine-grained%20and%20coarse-grained%20evaluations%20of%0Aweight%20importance%20for%20the%20pruning%20of%20LLMs.%20Leveraging%20an%20attention%20mechanism%2C%0AHyWIA%20adaptively%20determines%20the%20optimal%20blend%20of%20granularity%20in%20weight%0Aimportance%20assessments%20in%20an%20end-to-end%20pruning%20manner.%20Extensive%20experiments%0Aon%20LLaMA-V1/V2%2C%20Vicuna%2C%20Baichuan%2C%20and%20Bloom%20across%20various%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20HyWIA%20in%20pruning%20LLMs.%20For%20example%2C%20HyWIA%0Asurpasses%20the%20cutting-edge%20LLM-Pruner%20by%20an%20average%20margin%20of%202.82%5C%25%20in%0Aaccuracy%20across%20seven%20downstream%20tasks%20when%20pruning%20LLaMA-7B%20by%2050%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10799v4&entry.124074799=Read"},
{"title": "EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation", "author": "Hongwei Niu and Jie Hu and Jianghang Lin and Guannan Jiang and Shengchuan Zhang", "abstract": "  Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatialaware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.5 PQ, 32.1 mIoU, and\n11.6 FPS on the ADE20K dataset and the inference time of EOV-Seg is 4-19 times\nfaster than state-of-theart methods. Especially, equipped with ResNet50\nbackbone, EOV-Seg runs 23.8 FPS with only 71M parameters on a single RTX 3090\nGPU. Code is available at https://github.com/nhw649/EOV-Seg.\n", "link": "http://arxiv.org/abs/2412.08628v2", "date": "2024-12-16", "relevancy": 2.2912, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EOV-Seg%3A%20Efficient%20Open-Vocabulary%20Panoptic%20Segmentation&body=Title%3A%20EOV-Seg%3A%20Efficient%20Open-Vocabulary%20Panoptic%20Segmentation%0AAuthor%3A%20Hongwei%20Niu%20and%20Jie%20Hu%20and%20Jianghang%20Lin%20and%20Guannan%20Jiang%20and%20Shengchuan%20Zhang%0AAbstract%3A%20%20%20Open-vocabulary%20panoptic%20segmentation%20aims%20to%20segment%20and%20classify%20everything%0Ain%20diverse%20scenes%20across%20an%20unbounded%20vocabulary.%20Existing%20methods%20typically%0Aemploy%20two-stage%20or%20single-stage%20framework.%20The%20two-stage%20framework%20involves%0Acropping%20the%20image%20multiple%20times%20using%20masks%20generated%20by%20a%20mask%20generator%2C%0Afollowed%20by%20feature%20extraction%2C%20while%20the%20single-stage%20framework%20relies%20on%20a%0Aheavyweight%20mask%20decoder%20to%20make%20up%20for%20the%20lack%20of%20spatial%20position%0Ainformation%20through%20self-attention%20and%20cross-attention%20in%20multiple%20stacked%0ATransformer%20blocks.%20Both%20methods%20incur%20substantial%20computational%20overhead%2C%0Athereby%20hindering%20the%20efficiency%20of%20model%20inference.%20To%20fill%20the%20gap%20in%0Aefficiency%2C%20we%20propose%20EOV-Seg%2C%20a%20novel%20single-stage%2C%20shared%2C%20efficient%2C%20and%0Aspatialaware%20framework%20designed%20for%20open-vocabulary%20panoptic%20segmentation.%0ASpecifically%2C%20EOV-Seg%20innovates%20in%20two%20aspects.%20First%2C%20a%20Vocabulary-Aware%0ASelection%20%28VAS%29%20module%20is%20proposed%20to%20improve%20the%20semantic%20comprehension%20of%0Avisual%20aggregated%20features%20and%20alleviate%20the%20feature%20interaction%20burden%20on%20the%0Amask%20decoder.%20Second%2C%20we%20introduce%20a%20Two-way%20Dynamic%20Embedding%20Experts%20%28TDEE%29%2C%0Awhich%20efficiently%20utilizes%20the%20spatial%20awareness%20capabilities%20of%20ViT-based%20CLIP%0Abackbone.%20To%20the%20best%20of%20our%20knowledge%2C%20EOV-Seg%20is%20the%20first%20open-vocabulary%0Apanoptic%20segmentation%20framework%20towards%20efficiency%2C%20which%20runs%20faster%20and%0Aachieves%20competitive%20performance%20compared%20with%20state-of-the-art%20methods.%0ASpecifically%2C%20with%20COCO%20training%20only%2C%20EOV-Seg%20achieves%2024.5%20PQ%2C%2032.1%20mIoU%2C%20and%0A11.6%20FPS%20on%20the%20ADE20K%20dataset%20and%20the%20inference%20time%20of%20EOV-Seg%20is%204-19%20times%0Afaster%20than%20state-of-theart%20methods.%20Especially%2C%20equipped%20with%20ResNet50%0Abackbone%2C%20EOV-Seg%20runs%2023.8%20FPS%20with%20only%2071M%20parameters%20on%20a%20single%20RTX%203090%0AGPU.%20Code%20is%20available%20at%20https%3A//github.com/nhw649/EOV-Seg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEOV-Seg%253A%2520Efficient%2520Open-Vocabulary%2520Panoptic%2520Segmentation%26entry.906535625%3DHongwei%2520Niu%2520and%2520Jie%2520Hu%2520and%2520Jianghang%2520Lin%2520and%2520Guannan%2520Jiang%2520and%2520Shengchuan%2520Zhang%26entry.1292438233%3D%2520%2520Open-vocabulary%2520panoptic%2520segmentation%2520aims%2520to%2520segment%2520and%2520classify%2520everything%250Ain%2520diverse%2520scenes%2520across%2520an%2520unbounded%2520vocabulary.%2520Existing%2520methods%2520typically%250Aemploy%2520two-stage%2520or%2520single-stage%2520framework.%2520The%2520two-stage%2520framework%2520involves%250Acropping%2520the%2520image%2520multiple%2520times%2520using%2520masks%2520generated%2520by%2520a%2520mask%2520generator%252C%250Afollowed%2520by%2520feature%2520extraction%252C%2520while%2520the%2520single-stage%2520framework%2520relies%2520on%2520a%250Aheavyweight%2520mask%2520decoder%2520to%2520make%2520up%2520for%2520the%2520lack%2520of%2520spatial%2520position%250Ainformation%2520through%2520self-attention%2520and%2520cross-attention%2520in%2520multiple%2520stacked%250ATransformer%2520blocks.%2520Both%2520methods%2520incur%2520substantial%2520computational%2520overhead%252C%250Athereby%2520hindering%2520the%2520efficiency%2520of%2520model%2520inference.%2520To%2520fill%2520the%2520gap%2520in%250Aefficiency%252C%2520we%2520propose%2520EOV-Seg%252C%2520a%2520novel%2520single-stage%252C%2520shared%252C%2520efficient%252C%2520and%250Aspatialaware%2520framework%2520designed%2520for%2520open-vocabulary%2520panoptic%2520segmentation.%250ASpecifically%252C%2520EOV-Seg%2520innovates%2520in%2520two%2520aspects.%2520First%252C%2520a%2520Vocabulary-Aware%250ASelection%2520%2528VAS%2529%2520module%2520is%2520proposed%2520to%2520improve%2520the%2520semantic%2520comprehension%2520of%250Avisual%2520aggregated%2520features%2520and%2520alleviate%2520the%2520feature%2520interaction%2520burden%2520on%2520the%250Amask%2520decoder.%2520Second%252C%2520we%2520introduce%2520a%2520Two-way%2520Dynamic%2520Embedding%2520Experts%2520%2528TDEE%2529%252C%250Awhich%2520efficiently%2520utilizes%2520the%2520spatial%2520awareness%2520capabilities%2520of%2520ViT-based%2520CLIP%250Abackbone.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520EOV-Seg%2520is%2520the%2520first%2520open-vocabulary%250Apanoptic%2520segmentation%2520framework%2520towards%2520efficiency%252C%2520which%2520runs%2520faster%2520and%250Aachieves%2520competitive%2520performance%2520compared%2520with%2520state-of-the-art%2520methods.%250ASpecifically%252C%2520with%2520COCO%2520training%2520only%252C%2520EOV-Seg%2520achieves%252024.5%2520PQ%252C%252032.1%2520mIoU%252C%2520and%250A11.6%2520FPS%2520on%2520the%2520ADE20K%2520dataset%2520and%2520the%2520inference%2520time%2520of%2520EOV-Seg%2520is%25204-19%2520times%250Afaster%2520than%2520state-of-theart%2520methods.%2520Especially%252C%2520equipped%2520with%2520ResNet50%250Abackbone%252C%2520EOV-Seg%2520runs%252023.8%2520FPS%2520with%2520only%252071M%2520parameters%2520on%2520a%2520single%2520RTX%25203090%250AGPU.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/nhw649/EOV-Seg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EOV-Seg%3A%20Efficient%20Open-Vocabulary%20Panoptic%20Segmentation&entry.906535625=Hongwei%20Niu%20and%20Jie%20Hu%20and%20Jianghang%20Lin%20and%20Guannan%20Jiang%20and%20Shengchuan%20Zhang&entry.1292438233=%20%20Open-vocabulary%20panoptic%20segmentation%20aims%20to%20segment%20and%20classify%20everything%0Ain%20diverse%20scenes%20across%20an%20unbounded%20vocabulary.%20Existing%20methods%20typically%0Aemploy%20two-stage%20or%20single-stage%20framework.%20The%20two-stage%20framework%20involves%0Acropping%20the%20image%20multiple%20times%20using%20masks%20generated%20by%20a%20mask%20generator%2C%0Afollowed%20by%20feature%20extraction%2C%20while%20the%20single-stage%20framework%20relies%20on%20a%0Aheavyweight%20mask%20decoder%20to%20make%20up%20for%20the%20lack%20of%20spatial%20position%0Ainformation%20through%20self-attention%20and%20cross-attention%20in%20multiple%20stacked%0ATransformer%20blocks.%20Both%20methods%20incur%20substantial%20computational%20overhead%2C%0Athereby%20hindering%20the%20efficiency%20of%20model%20inference.%20To%20fill%20the%20gap%20in%0Aefficiency%2C%20we%20propose%20EOV-Seg%2C%20a%20novel%20single-stage%2C%20shared%2C%20efficient%2C%20and%0Aspatialaware%20framework%20designed%20for%20open-vocabulary%20panoptic%20segmentation.%0ASpecifically%2C%20EOV-Seg%20innovates%20in%20two%20aspects.%20First%2C%20a%20Vocabulary-Aware%0ASelection%20%28VAS%29%20module%20is%20proposed%20to%20improve%20the%20semantic%20comprehension%20of%0Avisual%20aggregated%20features%20and%20alleviate%20the%20feature%20interaction%20burden%20on%20the%0Amask%20decoder.%20Second%2C%20we%20introduce%20a%20Two-way%20Dynamic%20Embedding%20Experts%20%28TDEE%29%2C%0Awhich%20efficiently%20utilizes%20the%20spatial%20awareness%20capabilities%20of%20ViT-based%20CLIP%0Abackbone.%20To%20the%20best%20of%20our%20knowledge%2C%20EOV-Seg%20is%20the%20first%20open-vocabulary%0Apanoptic%20segmentation%20framework%20towards%20efficiency%2C%20which%20runs%20faster%20and%0Aachieves%20competitive%20performance%20compared%20with%20state-of-the-art%20methods.%0ASpecifically%2C%20with%20COCO%20training%20only%2C%20EOV-Seg%20achieves%2024.5%20PQ%2C%2032.1%20mIoU%2C%20and%0A11.6%20FPS%20on%20the%20ADE20K%20dataset%20and%20the%20inference%20time%20of%20EOV-Seg%20is%204-19%20times%0Afaster%20than%20state-of-theart%20methods.%20Especially%2C%20equipped%20with%20ResNet50%0Abackbone%2C%20EOV-Seg%20runs%2023.8%20FPS%20with%20only%2071M%20parameters%20on%20a%20single%20RTX%203090%0AGPU.%20Code%20is%20available%20at%20https%3A//github.com/nhw649/EOV-Seg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08628v2&entry.124074799=Read"},
{"title": "CG-Bench: Clue-grounded Question Answering Benchmark for Long Video\n  Understanding", "author": "Guo Chen and Yicheng Liu and Yifei Huang and Yuping He and Baoqi Pei and Jilan Xu and Yali Wang and Tong Lu and Limin Wang", "abstract": "  Most existing video understanding benchmarks for multimodal large language\nmodels (MLLMs) focus only on short videos. The limited number of benchmarks for\nlong video understanding often rely solely on multiple-choice questions (MCQs).\nHowever, because of the inherent limitation of MCQ-based evaluation and the\nincreasing reasoning ability of MLLMs, models can give the current answer\npurely by combining short video understanding with elimination, without\ngenuinely understanding the video content. To address this gap, we introduce\nCG-Bench, a novel benchmark designed for clue-grounded question answering in\nlong videos. CG-Bench emphasizes the model's ability to retrieve relevant clues\nfor questions, enhancing evaluation credibility. It features 1,219 manually\ncurated videos categorized by a granular system with 14 primary categories, 171\nsecondary categories, and 638 tertiary categories, making it the largest\nbenchmark for long video analysis. The benchmark includes 12,129 QA pairs in\nthree major question types: perception, reasoning, and hallucination.\nCompensating the drawbacks of pure MCQ-based evaluation, we design two novel\nclue-based evaluation methods: clue-grounded white box and black box\nevaluations, to assess whether the model generates answers based on the correct\nunderstanding of the video. We evaluate multiple closed-source and open-source\nMLLMs on CG-Bench. Results indicate that current models significantly\nunderperform in understanding long videos compared to short ones, and a\nsignificant gap exists between open-source and commercial models. We hope\nCG-Bench can advance the development of more trustworthy and capable MLLMs for\nlong video understanding. All annotations and video data are released at\nhttps://cg-bench.github.io/leaderboard/.\n", "link": "http://arxiv.org/abs/2412.12075v1", "date": "2024-12-16", "relevancy": 2.26, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CG-Bench%3A%20Clue-grounded%20Question%20Answering%20Benchmark%20for%20Long%20Video%0A%20%20Understanding&body=Title%3A%20CG-Bench%3A%20Clue-grounded%20Question%20Answering%20Benchmark%20for%20Long%20Video%0A%20%20Understanding%0AAuthor%3A%20Guo%20Chen%20and%20Yicheng%20Liu%20and%20Yifei%20Huang%20and%20Yuping%20He%20and%20Baoqi%20Pei%20and%20Jilan%20Xu%20and%20Yali%20Wang%20and%20Tong%20Lu%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Most%20existing%20video%20understanding%20benchmarks%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20focus%20only%20on%20short%20videos.%20The%20limited%20number%20of%20benchmarks%20for%0Along%20video%20understanding%20often%20rely%20solely%20on%20multiple-choice%20questions%20%28MCQs%29.%0AHowever%2C%20because%20of%20the%20inherent%20limitation%20of%20MCQ-based%20evaluation%20and%20the%0Aincreasing%20reasoning%20ability%20of%20MLLMs%2C%20models%20can%20give%20the%20current%20answer%0Apurely%20by%20combining%20short%20video%20understanding%20with%20elimination%2C%20without%0Agenuinely%20understanding%20the%20video%20content.%20To%20address%20this%20gap%2C%20we%20introduce%0ACG-Bench%2C%20a%20novel%20benchmark%20designed%20for%20clue-grounded%20question%20answering%20in%0Along%20videos.%20CG-Bench%20emphasizes%20the%20model%27s%20ability%20to%20retrieve%20relevant%20clues%0Afor%20questions%2C%20enhancing%20evaluation%20credibility.%20It%20features%201%2C219%20manually%0Acurated%20videos%20categorized%20by%20a%20granular%20system%20with%2014%20primary%20categories%2C%20171%0Asecondary%20categories%2C%20and%20638%20tertiary%20categories%2C%20making%20it%20the%20largest%0Abenchmark%20for%20long%20video%20analysis.%20The%20benchmark%20includes%2012%2C129%20QA%20pairs%20in%0Athree%20major%20question%20types%3A%20perception%2C%20reasoning%2C%20and%20hallucination.%0ACompensating%20the%20drawbacks%20of%20pure%20MCQ-based%20evaluation%2C%20we%20design%20two%20novel%0Aclue-based%20evaluation%20methods%3A%20clue-grounded%20white%20box%20and%20black%20box%0Aevaluations%2C%20to%20assess%20whether%20the%20model%20generates%20answers%20based%20on%20the%20correct%0Aunderstanding%20of%20the%20video.%20We%20evaluate%20multiple%20closed-source%20and%20open-source%0AMLLMs%20on%20CG-Bench.%20Results%20indicate%20that%20current%20models%20significantly%0Aunderperform%20in%20understanding%20long%20videos%20compared%20to%20short%20ones%2C%20and%20a%0Asignificant%20gap%20exists%20between%20open-source%20and%20commercial%20models.%20We%20hope%0ACG-Bench%20can%20advance%20the%20development%20of%20more%20trustworthy%20and%20capable%20MLLMs%20for%0Along%20video%20understanding.%20All%20annotations%20and%20video%20data%20are%20released%20at%0Ahttps%3A//cg-bench.github.io/leaderboard/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCG-Bench%253A%2520Clue-grounded%2520Question%2520Answering%2520Benchmark%2520for%2520Long%2520Video%250A%2520%2520Understanding%26entry.906535625%3DGuo%2520Chen%2520and%2520Yicheng%2520Liu%2520and%2520Yifei%2520Huang%2520and%2520Yuping%2520He%2520and%2520Baoqi%2520Pei%2520and%2520Jilan%2520Xu%2520and%2520Yali%2520Wang%2520and%2520Tong%2520Lu%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Most%2520existing%2520video%2520understanding%2520benchmarks%2520for%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%2520focus%2520only%2520on%2520short%2520videos.%2520The%2520limited%2520number%2520of%2520benchmarks%2520for%250Along%2520video%2520understanding%2520often%2520rely%2520solely%2520on%2520multiple-choice%2520questions%2520%2528MCQs%2529.%250AHowever%252C%2520because%2520of%2520the%2520inherent%2520limitation%2520of%2520MCQ-based%2520evaluation%2520and%2520the%250Aincreasing%2520reasoning%2520ability%2520of%2520MLLMs%252C%2520models%2520can%2520give%2520the%2520current%2520answer%250Apurely%2520by%2520combining%2520short%2520video%2520understanding%2520with%2520elimination%252C%2520without%250Agenuinely%2520understanding%2520the%2520video%2520content.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250ACG-Bench%252C%2520a%2520novel%2520benchmark%2520designed%2520for%2520clue-grounded%2520question%2520answering%2520in%250Along%2520videos.%2520CG-Bench%2520emphasizes%2520the%2520model%2527s%2520ability%2520to%2520retrieve%2520relevant%2520clues%250Afor%2520questions%252C%2520enhancing%2520evaluation%2520credibility.%2520It%2520features%25201%252C219%2520manually%250Acurated%2520videos%2520categorized%2520by%2520a%2520granular%2520system%2520with%252014%2520primary%2520categories%252C%2520171%250Asecondary%2520categories%252C%2520and%2520638%2520tertiary%2520categories%252C%2520making%2520it%2520the%2520largest%250Abenchmark%2520for%2520long%2520video%2520analysis.%2520The%2520benchmark%2520includes%252012%252C129%2520QA%2520pairs%2520in%250Athree%2520major%2520question%2520types%253A%2520perception%252C%2520reasoning%252C%2520and%2520hallucination.%250ACompensating%2520the%2520drawbacks%2520of%2520pure%2520MCQ-based%2520evaluation%252C%2520we%2520design%2520two%2520novel%250Aclue-based%2520evaluation%2520methods%253A%2520clue-grounded%2520white%2520box%2520and%2520black%2520box%250Aevaluations%252C%2520to%2520assess%2520whether%2520the%2520model%2520generates%2520answers%2520based%2520on%2520the%2520correct%250Aunderstanding%2520of%2520the%2520video.%2520We%2520evaluate%2520multiple%2520closed-source%2520and%2520open-source%250AMLLMs%2520on%2520CG-Bench.%2520Results%2520indicate%2520that%2520current%2520models%2520significantly%250Aunderperform%2520in%2520understanding%2520long%2520videos%2520compared%2520to%2520short%2520ones%252C%2520and%2520a%250Asignificant%2520gap%2520exists%2520between%2520open-source%2520and%2520commercial%2520models.%2520We%2520hope%250ACG-Bench%2520can%2520advance%2520the%2520development%2520of%2520more%2520trustworthy%2520and%2520capable%2520MLLMs%2520for%250Along%2520video%2520understanding.%2520All%2520annotations%2520and%2520video%2520data%2520are%2520released%2520at%250Ahttps%253A//cg-bench.github.io/leaderboard/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CG-Bench%3A%20Clue-grounded%20Question%20Answering%20Benchmark%20for%20Long%20Video%0A%20%20Understanding&entry.906535625=Guo%20Chen%20and%20Yicheng%20Liu%20and%20Yifei%20Huang%20and%20Yuping%20He%20and%20Baoqi%20Pei%20and%20Jilan%20Xu%20and%20Yali%20Wang%20and%20Tong%20Lu%20and%20Limin%20Wang&entry.1292438233=%20%20Most%20existing%20video%20understanding%20benchmarks%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20focus%20only%20on%20short%20videos.%20The%20limited%20number%20of%20benchmarks%20for%0Along%20video%20understanding%20often%20rely%20solely%20on%20multiple-choice%20questions%20%28MCQs%29.%0AHowever%2C%20because%20of%20the%20inherent%20limitation%20of%20MCQ-based%20evaluation%20and%20the%0Aincreasing%20reasoning%20ability%20of%20MLLMs%2C%20models%20can%20give%20the%20current%20answer%0Apurely%20by%20combining%20short%20video%20understanding%20with%20elimination%2C%20without%0Agenuinely%20understanding%20the%20video%20content.%20To%20address%20this%20gap%2C%20we%20introduce%0ACG-Bench%2C%20a%20novel%20benchmark%20designed%20for%20clue-grounded%20question%20answering%20in%0Along%20videos.%20CG-Bench%20emphasizes%20the%20model%27s%20ability%20to%20retrieve%20relevant%20clues%0Afor%20questions%2C%20enhancing%20evaluation%20credibility.%20It%20features%201%2C219%20manually%0Acurated%20videos%20categorized%20by%20a%20granular%20system%20with%2014%20primary%20categories%2C%20171%0Asecondary%20categories%2C%20and%20638%20tertiary%20categories%2C%20making%20it%20the%20largest%0Abenchmark%20for%20long%20video%20analysis.%20The%20benchmark%20includes%2012%2C129%20QA%20pairs%20in%0Athree%20major%20question%20types%3A%20perception%2C%20reasoning%2C%20and%20hallucination.%0ACompensating%20the%20drawbacks%20of%20pure%20MCQ-based%20evaluation%2C%20we%20design%20two%20novel%0Aclue-based%20evaluation%20methods%3A%20clue-grounded%20white%20box%20and%20black%20box%0Aevaluations%2C%20to%20assess%20whether%20the%20model%20generates%20answers%20based%20on%20the%20correct%0Aunderstanding%20of%20the%20video.%20We%20evaluate%20multiple%20closed-source%20and%20open-source%0AMLLMs%20on%20CG-Bench.%20Results%20indicate%20that%20current%20models%20significantly%0Aunderperform%20in%20understanding%20long%20videos%20compared%20to%20short%20ones%2C%20and%20a%0Asignificant%20gap%20exists%20between%20open-source%20and%20commercial%20models.%20We%20hope%0ACG-Bench%20can%20advance%20the%20development%20of%20more%20trustworthy%20and%20capable%20MLLMs%20for%0Along%20video%20understanding.%20All%20annotations%20and%20video%20data%20are%20released%20at%0Ahttps%3A//cg-bench.github.io/leaderboard/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12075v1&entry.124074799=Read"},
{"title": "SpeechPrune: Context-aware Token Pruning for Speech Information\n  Retrieval", "author": "Yueqian Lin and Yuzhe Fu and Jingyang Zhang and Yudong Liu and Jianyi Zhang and Jingwei Sun and Hai \"Helen\" Li and Yiran Chen", "abstract": "  We introduce Speech Information Retrieval (SIR), a new long-context task for\nSpeech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-sample\nbenchmark testing models' ability to extract critical details from\napproximately 90-second spoken inputs. While current Speech LLMs excel at\nshort-form tasks, they struggle with the computational and representational\ndemands of longer audio sequences. To address this limitation, we propose\nSpeechPrune, a training-free token pruning strategy that uses speech-text\nsimilarity and approximated attention scores to efficiently discard irrelevant\ntokens. In SPIRAL, SpeechPrune achieves accuracy improvements of 29% and up to\n47% over the original model and the random pruning model at a pruning rate of\n20%, respectively. SpeechPrune can maintain network performance even at a\npruning level of 80%. This approach highlights the potential of token-level\npruning for efficient and scalable long-form speech understanding.\n", "link": "http://arxiv.org/abs/2412.12009v1", "date": "2024-12-16", "relevancy": 2.2514, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4583}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpeechPrune%3A%20Context-aware%20Token%20Pruning%20for%20Speech%20Information%0A%20%20Retrieval&body=Title%3A%20SpeechPrune%3A%20Context-aware%20Token%20Pruning%20for%20Speech%20Information%0A%20%20Retrieval%0AAuthor%3A%20Yueqian%20Lin%20and%20Yuzhe%20Fu%20and%20Jingyang%20Zhang%20and%20Yudong%20Liu%20and%20Jianyi%20Zhang%20and%20Jingwei%20Sun%20and%20Hai%20%22Helen%22%20Li%20and%20Yiran%20Chen%0AAbstract%3A%20%20%20We%20introduce%20Speech%20Information%20Retrieval%20%28SIR%29%2C%20a%20new%20long-context%20task%20for%0ASpeech%20Large%20Language%20Models%20%28Speech%20LLMs%29%2C%20and%20present%20SPIRAL%2C%20a%201%2C012-sample%0Abenchmark%20testing%20models%27%20ability%20to%20extract%20critical%20details%20from%0Aapproximately%2090-second%20spoken%20inputs.%20While%20current%20Speech%20LLMs%20excel%20at%0Ashort-form%20tasks%2C%20they%20struggle%20with%20the%20computational%20and%20representational%0Ademands%20of%20longer%20audio%20sequences.%20To%20address%20this%20limitation%2C%20we%20propose%0ASpeechPrune%2C%20a%20training-free%20token%20pruning%20strategy%20that%20uses%20speech-text%0Asimilarity%20and%20approximated%20attention%20scores%20to%20efficiently%20discard%20irrelevant%0Atokens.%20In%20SPIRAL%2C%20SpeechPrune%20achieves%20accuracy%20improvements%20of%2029%25%20and%20up%20to%0A47%25%20over%20the%20original%20model%20and%20the%20random%20pruning%20model%20at%20a%20pruning%20rate%20of%0A20%25%2C%20respectively.%20SpeechPrune%20can%20maintain%20network%20performance%20even%20at%20a%0Apruning%20level%20of%2080%25.%20This%20approach%20highlights%20the%20potential%20of%20token-level%0Apruning%20for%20efficient%20and%20scalable%20long-form%20speech%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeechPrune%253A%2520Context-aware%2520Token%2520Pruning%2520for%2520Speech%2520Information%250A%2520%2520Retrieval%26entry.906535625%3DYueqian%2520Lin%2520and%2520Yuzhe%2520Fu%2520and%2520Jingyang%2520Zhang%2520and%2520Yudong%2520Liu%2520and%2520Jianyi%2520Zhang%2520and%2520Jingwei%2520Sun%2520and%2520Hai%2520%2522Helen%2522%2520Li%2520and%2520Yiran%2520Chen%26entry.1292438233%3D%2520%2520We%2520introduce%2520Speech%2520Information%2520Retrieval%2520%2528SIR%2529%252C%2520a%2520new%2520long-context%2520task%2520for%250ASpeech%2520Large%2520Language%2520Models%2520%2528Speech%2520LLMs%2529%252C%2520and%2520present%2520SPIRAL%252C%2520a%25201%252C012-sample%250Abenchmark%2520testing%2520models%2527%2520ability%2520to%2520extract%2520critical%2520details%2520from%250Aapproximately%252090-second%2520spoken%2520inputs.%2520While%2520current%2520Speech%2520LLMs%2520excel%2520at%250Ashort-form%2520tasks%252C%2520they%2520struggle%2520with%2520the%2520computational%2520and%2520representational%250Ademands%2520of%2520longer%2520audio%2520sequences.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250ASpeechPrune%252C%2520a%2520training-free%2520token%2520pruning%2520strategy%2520that%2520uses%2520speech-text%250Asimilarity%2520and%2520approximated%2520attention%2520scores%2520to%2520efficiently%2520discard%2520irrelevant%250Atokens.%2520In%2520SPIRAL%252C%2520SpeechPrune%2520achieves%2520accuracy%2520improvements%2520of%252029%2525%2520and%2520up%2520to%250A47%2525%2520over%2520the%2520original%2520model%2520and%2520the%2520random%2520pruning%2520model%2520at%2520a%2520pruning%2520rate%2520of%250A20%2525%252C%2520respectively.%2520SpeechPrune%2520can%2520maintain%2520network%2520performance%2520even%2520at%2520a%250Apruning%2520level%2520of%252080%2525.%2520This%2520approach%2520highlights%2520the%2520potential%2520of%2520token-level%250Apruning%2520for%2520efficient%2520and%2520scalable%2520long-form%2520speech%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpeechPrune%3A%20Context-aware%20Token%20Pruning%20for%20Speech%20Information%0A%20%20Retrieval&entry.906535625=Yueqian%20Lin%20and%20Yuzhe%20Fu%20and%20Jingyang%20Zhang%20and%20Yudong%20Liu%20and%20Jianyi%20Zhang%20and%20Jingwei%20Sun%20and%20Hai%20%22Helen%22%20Li%20and%20Yiran%20Chen&entry.1292438233=%20%20We%20introduce%20Speech%20Information%20Retrieval%20%28SIR%29%2C%20a%20new%20long-context%20task%20for%0ASpeech%20Large%20Language%20Models%20%28Speech%20LLMs%29%2C%20and%20present%20SPIRAL%2C%20a%201%2C012-sample%0Abenchmark%20testing%20models%27%20ability%20to%20extract%20critical%20details%20from%0Aapproximately%2090-second%20spoken%20inputs.%20While%20current%20Speech%20LLMs%20excel%20at%0Ashort-form%20tasks%2C%20they%20struggle%20with%20the%20computational%20and%20representational%0Ademands%20of%20longer%20audio%20sequences.%20To%20address%20this%20limitation%2C%20we%20propose%0ASpeechPrune%2C%20a%20training-free%20token%20pruning%20strategy%20that%20uses%20speech-text%0Asimilarity%20and%20approximated%20attention%20scores%20to%20efficiently%20discard%20irrelevant%0Atokens.%20In%20SPIRAL%2C%20SpeechPrune%20achieves%20accuracy%20improvements%20of%2029%25%20and%20up%20to%0A47%25%20over%20the%20original%20model%20and%20the%20random%20pruning%20model%20at%20a%20pruning%20rate%20of%0A20%25%2C%20respectively.%20SpeechPrune%20can%20maintain%20network%20performance%20even%20at%20a%0Apruning%20level%20of%2080%25.%20This%20approach%20highlights%20the%20potential%20of%20token-level%0Apruning%20for%20efficient%20and%20scalable%20long-form%20speech%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12009v1&entry.124074799=Read"},
{"title": "Gramian Multimodal Representation Learning and Alignment", "author": "Giordano Cicchetti and Eleonora Grassucci and Luigi Sigillo and Danilo Comminiello", "abstract": "  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modality and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n", "link": "http://arxiv.org/abs/2412.11959v1", "date": "2024-12-16", "relevancy": 2.2415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6227}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gramian%20Multimodal%20Representation%20Learning%20and%20Alignment&body=Title%3A%20Gramian%20Multimodal%20Representation%20Learning%20and%20Alignment%0AAuthor%3A%20Giordano%20Cicchetti%20and%20Eleonora%20Grassucci%20and%20Luigi%20Sigillo%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20Human%20perception%20integrates%20multiple%20modalities%2C%20such%20as%20vision%2C%20hearing%2C%20and%0Alanguage%2C%20into%20a%20unified%20understanding%20of%20the%20surrounding%20reality.%20While%20recent%0Amultimodal%20models%20have%20achieved%20significant%20progress%20by%20aligning%20pairs%20of%0Amodalities%20via%20contrastive%20learning%2C%20their%20solutions%20are%20unsuitable%20when%0Ascaling%20to%20multiple%20modalities.%20These%20models%20typically%20align%20each%20modality%20to%20a%0Adesignated%20anchor%20without%20ensuring%20the%20alignment%20of%20all%20modalities%20with%20each%0Aother%2C%20leading%20to%20suboptimal%20performance%20in%20tasks%20requiring%20a%20joint%0Aunderstanding%20of%20multiple%20modalities.%20In%20this%20paper%2C%20we%20structurally%20rethink%0Athe%20pairwise%20conventional%20approach%20to%20multimodal%20learning%20and%20we%20present%20the%0Anovel%20Gramian%20Representation%20Alignment%20Measure%20%28GRAM%29%2C%20which%20overcomes%20the%0Aabove-mentioned%20limitations.%20GRAM%20learns%20and%20then%20aligns%20%24n%24%20modalities%0Adirectly%20in%20the%20higher-dimensional%20space%20in%20which%20modality%20embeddings%20lie%20by%0Aminimizing%20the%20Gramian%20volume%20of%20the%20%24k%24-dimensional%20parallelotope%20spanned%20by%0Athe%20modality%20vectors%2C%20ensuring%20the%20geometric%20alignment%20of%20all%20modalities%0Asimultaneously.%20GRAM%20can%20replace%20cosine%20similarity%20in%20any%20downstream%20method%2C%0Aholding%20for%202%20to%20%24n%24%20modality%20and%20providing%20more%20meaningful%20alignment%20with%0Arespect%20to%20previous%20similarity%20measures.%20The%20novel%20GRAM-based%20contrastive%20loss%0Afunction%20enhances%20the%20alignment%20of%20multimodal%20models%20in%20the%20higher-dimensional%0Aembedding%20space%2C%20leading%20to%20new%20state-of-the-art%20performance%20in%20downstream%0Atasks%20such%20as%20video-audio-text%20retrieval%20and%20audio-video%20classification.%20The%0Aproject%20page%2C%20the%20code%2C%20and%20the%20pretrained%20models%20are%20available%20at%0Ahttps%3A//ispamm.github.io/GRAM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGramian%2520Multimodal%2520Representation%2520Learning%2520and%2520Alignment%26entry.906535625%3DGiordano%2520Cicchetti%2520and%2520Eleonora%2520Grassucci%2520and%2520Luigi%2520Sigillo%2520and%2520Danilo%2520Comminiello%26entry.1292438233%3D%2520%2520Human%2520perception%2520integrates%2520multiple%2520modalities%252C%2520such%2520as%2520vision%252C%2520hearing%252C%2520and%250Alanguage%252C%2520into%2520a%2520unified%2520understanding%2520of%2520the%2520surrounding%2520reality.%2520While%2520recent%250Amultimodal%2520models%2520have%2520achieved%2520significant%2520progress%2520by%2520aligning%2520pairs%2520of%250Amodalities%2520via%2520contrastive%2520learning%252C%2520their%2520solutions%2520are%2520unsuitable%2520when%250Ascaling%2520to%2520multiple%2520modalities.%2520These%2520models%2520typically%2520align%2520each%2520modality%2520to%2520a%250Adesignated%2520anchor%2520without%2520ensuring%2520the%2520alignment%2520of%2520all%2520modalities%2520with%2520each%250Aother%252C%2520leading%2520to%2520suboptimal%2520performance%2520in%2520tasks%2520requiring%2520a%2520joint%250Aunderstanding%2520of%2520multiple%2520modalities.%2520In%2520this%2520paper%252C%2520we%2520structurally%2520rethink%250Athe%2520pairwise%2520conventional%2520approach%2520to%2520multimodal%2520learning%2520and%2520we%2520present%2520the%250Anovel%2520Gramian%2520Representation%2520Alignment%2520Measure%2520%2528GRAM%2529%252C%2520which%2520overcomes%2520the%250Aabove-mentioned%2520limitations.%2520GRAM%2520learns%2520and%2520then%2520aligns%2520%2524n%2524%2520modalities%250Adirectly%2520in%2520the%2520higher-dimensional%2520space%2520in%2520which%2520modality%2520embeddings%2520lie%2520by%250Aminimizing%2520the%2520Gramian%2520volume%2520of%2520the%2520%2524k%2524-dimensional%2520parallelotope%2520spanned%2520by%250Athe%2520modality%2520vectors%252C%2520ensuring%2520the%2520geometric%2520alignment%2520of%2520all%2520modalities%250Asimultaneously.%2520GRAM%2520can%2520replace%2520cosine%2520similarity%2520in%2520any%2520downstream%2520method%252C%250Aholding%2520for%25202%2520to%2520%2524n%2524%2520modality%2520and%2520providing%2520more%2520meaningful%2520alignment%2520with%250Arespect%2520to%2520previous%2520similarity%2520measures.%2520The%2520novel%2520GRAM-based%2520contrastive%2520loss%250Afunction%2520enhances%2520the%2520alignment%2520of%2520multimodal%2520models%2520in%2520the%2520higher-dimensional%250Aembedding%2520space%252C%2520leading%2520to%2520new%2520state-of-the-art%2520performance%2520in%2520downstream%250Atasks%2520such%2520as%2520video-audio-text%2520retrieval%2520and%2520audio-video%2520classification.%2520The%250Aproject%2520page%252C%2520the%2520code%252C%2520and%2520the%2520pretrained%2520models%2520are%2520available%2520at%250Ahttps%253A//ispamm.github.io/GRAM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gramian%20Multimodal%20Representation%20Learning%20and%20Alignment&entry.906535625=Giordano%20Cicchetti%20and%20Eleonora%20Grassucci%20and%20Luigi%20Sigillo%20and%20Danilo%20Comminiello&entry.1292438233=%20%20Human%20perception%20integrates%20multiple%20modalities%2C%20such%20as%20vision%2C%20hearing%2C%20and%0Alanguage%2C%20into%20a%20unified%20understanding%20of%20the%20surrounding%20reality.%20While%20recent%0Amultimodal%20models%20have%20achieved%20significant%20progress%20by%20aligning%20pairs%20of%0Amodalities%20via%20contrastive%20learning%2C%20their%20solutions%20are%20unsuitable%20when%0Ascaling%20to%20multiple%20modalities.%20These%20models%20typically%20align%20each%20modality%20to%20a%0Adesignated%20anchor%20without%20ensuring%20the%20alignment%20of%20all%20modalities%20with%20each%0Aother%2C%20leading%20to%20suboptimal%20performance%20in%20tasks%20requiring%20a%20joint%0Aunderstanding%20of%20multiple%20modalities.%20In%20this%20paper%2C%20we%20structurally%20rethink%0Athe%20pairwise%20conventional%20approach%20to%20multimodal%20learning%20and%20we%20present%20the%0Anovel%20Gramian%20Representation%20Alignment%20Measure%20%28GRAM%29%2C%20which%20overcomes%20the%0Aabove-mentioned%20limitations.%20GRAM%20learns%20and%20then%20aligns%20%24n%24%20modalities%0Adirectly%20in%20the%20higher-dimensional%20space%20in%20which%20modality%20embeddings%20lie%20by%0Aminimizing%20the%20Gramian%20volume%20of%20the%20%24k%24-dimensional%20parallelotope%20spanned%20by%0Athe%20modality%20vectors%2C%20ensuring%20the%20geometric%20alignment%20of%20all%20modalities%0Asimultaneously.%20GRAM%20can%20replace%20cosine%20similarity%20in%20any%20downstream%20method%2C%0Aholding%20for%202%20to%20%24n%24%20modality%20and%20providing%20more%20meaningful%20alignment%20with%0Arespect%20to%20previous%20similarity%20measures.%20The%20novel%20GRAM-based%20contrastive%20loss%0Afunction%20enhances%20the%20alignment%20of%20multimodal%20models%20in%20the%20higher-dimensional%0Aembedding%20space%2C%20leading%20to%20new%20state-of-the-art%20performance%20in%20downstream%0Atasks%20such%20as%20video-audio-text%20retrieval%20and%20audio-video%20classification.%20The%0Aproject%20page%2C%20the%20code%2C%20and%20the%20pretrained%20models%20are%20available%20at%0Ahttps%3A//ispamm.github.io/GRAM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11959v1&entry.124074799=Read"},
{"title": "RepFace: Refining Closed-Set Noise with Progressive Label Correction for\n  Face Recognition", "author": "Jie Zhang and Xun Gong and Zhonglin Sun", "abstract": "  Face recognition has made remarkable strides, driven by the expanding scale\nof datasets, advancements in various backbone and discriminative losses.\nHowever, face recognition performance is heavily affected by the label noise,\nespecially closed-set noise. While numerous studies have focused on handling\nlabel noise, addressing closed-set noise still poses challenges. This paper\nidentifies this challenge as training isn't robust to noise at the early-stage\ntraining, and necessitating an appropriate learning strategy for samples with\nlow confidence, which are often misclassified as closed-set noise in later\ntraining phases. To address these issues, we propose a new framework to\nstabilize the training at early stages and split the samples into clean,\nambiguous and noisy groups which are devised with separate training strategies.\nInitially, we employ generated auxiliary closed-set noisy samples to enable the\nmodel to identify noisy data at the early stages of training. Subsequently, we\nintroduce how samples are split into clean, ambiguous and noisy groups by their\nsimilarity to the positive and nearest negative centers. Then we perform label\nfusion for ambiguous samples by incorporating accumulated model predictions.\nFinally, we apply label smoothing within the closed set, adjusting the label to\na point between the nearest negative class and the initially assigned label.\nExtensive experiments validate the effectiveness of our method on mainstream\nface datasets, achieving state-of-the-art results. The code will be released\nupon acceptance.\n", "link": "http://arxiv.org/abs/2412.12031v1", "date": "2024-12-16", "relevancy": 2.2414, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5893}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5439}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepFace%3A%20Refining%20Closed-Set%20Noise%20with%20Progressive%20Label%20Correction%20for%0A%20%20Face%20Recognition&body=Title%3A%20RepFace%3A%20Refining%20Closed-Set%20Noise%20with%20Progressive%20Label%20Correction%20for%0A%20%20Face%20Recognition%0AAuthor%3A%20Jie%20Zhang%20and%20Xun%20Gong%20and%20Zhonglin%20Sun%0AAbstract%3A%20%20%20Face%20recognition%20has%20made%20remarkable%20strides%2C%20driven%20by%20the%20expanding%20scale%0Aof%20datasets%2C%20advancements%20in%20various%20backbone%20and%20discriminative%20losses.%0AHowever%2C%20face%20recognition%20performance%20is%20heavily%20affected%20by%20the%20label%20noise%2C%0Aespecially%20closed-set%20noise.%20While%20numerous%20studies%20have%20focused%20on%20handling%0Alabel%20noise%2C%20addressing%20closed-set%20noise%20still%20poses%20challenges.%20This%20paper%0Aidentifies%20this%20challenge%20as%20training%20isn%27t%20robust%20to%20noise%20at%20the%20early-stage%0Atraining%2C%20and%20necessitating%20an%20appropriate%20learning%20strategy%20for%20samples%20with%0Alow%20confidence%2C%20which%20are%20often%20misclassified%20as%20closed-set%20noise%20in%20later%0Atraining%20phases.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20framework%20to%0Astabilize%20the%20training%20at%20early%20stages%20and%20split%20the%20samples%20into%20clean%2C%0Aambiguous%20and%20noisy%20groups%20which%20are%20devised%20with%20separate%20training%20strategies.%0AInitially%2C%20we%20employ%20generated%20auxiliary%20closed-set%20noisy%20samples%20to%20enable%20the%0Amodel%20to%20identify%20noisy%20data%20at%20the%20early%20stages%20of%20training.%20Subsequently%2C%20we%0Aintroduce%20how%20samples%20are%20split%20into%20clean%2C%20ambiguous%20and%20noisy%20groups%20by%20their%0Asimilarity%20to%20the%20positive%20and%20nearest%20negative%20centers.%20Then%20we%20perform%20label%0Afusion%20for%20ambiguous%20samples%20by%20incorporating%20accumulated%20model%20predictions.%0AFinally%2C%20we%20apply%20label%20smoothing%20within%20the%20closed%20set%2C%20adjusting%20the%20label%20to%0Aa%20point%20between%20the%20nearest%20negative%20class%20and%20the%20initially%20assigned%20label.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20our%20method%20on%20mainstream%0Aface%20datasets%2C%20achieving%20state-of-the-art%20results.%20The%20code%20will%20be%20released%0Aupon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepFace%253A%2520Refining%2520Closed-Set%2520Noise%2520with%2520Progressive%2520Label%2520Correction%2520for%250A%2520%2520Face%2520Recognition%26entry.906535625%3DJie%2520Zhang%2520and%2520Xun%2520Gong%2520and%2520Zhonglin%2520Sun%26entry.1292438233%3D%2520%2520Face%2520recognition%2520has%2520made%2520remarkable%2520strides%252C%2520driven%2520by%2520the%2520expanding%2520scale%250Aof%2520datasets%252C%2520advancements%2520in%2520various%2520backbone%2520and%2520discriminative%2520losses.%250AHowever%252C%2520face%2520recognition%2520performance%2520is%2520heavily%2520affected%2520by%2520the%2520label%2520noise%252C%250Aespecially%2520closed-set%2520noise.%2520While%2520numerous%2520studies%2520have%2520focused%2520on%2520handling%250Alabel%2520noise%252C%2520addressing%2520closed-set%2520noise%2520still%2520poses%2520challenges.%2520This%2520paper%250Aidentifies%2520this%2520challenge%2520as%2520training%2520isn%2527t%2520robust%2520to%2520noise%2520at%2520the%2520early-stage%250Atraining%252C%2520and%2520necessitating%2520an%2520appropriate%2520learning%2520strategy%2520for%2520samples%2520with%250Alow%2520confidence%252C%2520which%2520are%2520often%2520misclassified%2520as%2520closed-set%2520noise%2520in%2520later%250Atraining%2520phases.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520new%2520framework%2520to%250Astabilize%2520the%2520training%2520at%2520early%2520stages%2520and%2520split%2520the%2520samples%2520into%2520clean%252C%250Aambiguous%2520and%2520noisy%2520groups%2520which%2520are%2520devised%2520with%2520separate%2520training%2520strategies.%250AInitially%252C%2520we%2520employ%2520generated%2520auxiliary%2520closed-set%2520noisy%2520samples%2520to%2520enable%2520the%250Amodel%2520to%2520identify%2520noisy%2520data%2520at%2520the%2520early%2520stages%2520of%2520training.%2520Subsequently%252C%2520we%250Aintroduce%2520how%2520samples%2520are%2520split%2520into%2520clean%252C%2520ambiguous%2520and%2520noisy%2520groups%2520by%2520their%250Asimilarity%2520to%2520the%2520positive%2520and%2520nearest%2520negative%2520centers.%2520Then%2520we%2520perform%2520label%250Afusion%2520for%2520ambiguous%2520samples%2520by%2520incorporating%2520accumulated%2520model%2520predictions.%250AFinally%252C%2520we%2520apply%2520label%2520smoothing%2520within%2520the%2520closed%2520set%252C%2520adjusting%2520the%2520label%2520to%250Aa%2520point%2520between%2520the%2520nearest%2520negative%2520class%2520and%2520the%2520initially%2520assigned%2520label.%250AExtensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520mainstream%250Aface%2520datasets%252C%2520achieving%2520state-of-the-art%2520results.%2520The%2520code%2520will%2520be%2520released%250Aupon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepFace%3A%20Refining%20Closed-Set%20Noise%20with%20Progressive%20Label%20Correction%20for%0A%20%20Face%20Recognition&entry.906535625=Jie%20Zhang%20and%20Xun%20Gong%20and%20Zhonglin%20Sun&entry.1292438233=%20%20Face%20recognition%20has%20made%20remarkable%20strides%2C%20driven%20by%20the%20expanding%20scale%0Aof%20datasets%2C%20advancements%20in%20various%20backbone%20and%20discriminative%20losses.%0AHowever%2C%20face%20recognition%20performance%20is%20heavily%20affected%20by%20the%20label%20noise%2C%0Aespecially%20closed-set%20noise.%20While%20numerous%20studies%20have%20focused%20on%20handling%0Alabel%20noise%2C%20addressing%20closed-set%20noise%20still%20poses%20challenges.%20This%20paper%0Aidentifies%20this%20challenge%20as%20training%20isn%27t%20robust%20to%20noise%20at%20the%20early-stage%0Atraining%2C%20and%20necessitating%20an%20appropriate%20learning%20strategy%20for%20samples%20with%0Alow%20confidence%2C%20which%20are%20often%20misclassified%20as%20closed-set%20noise%20in%20later%0Atraining%20phases.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20framework%20to%0Astabilize%20the%20training%20at%20early%20stages%20and%20split%20the%20samples%20into%20clean%2C%0Aambiguous%20and%20noisy%20groups%20which%20are%20devised%20with%20separate%20training%20strategies.%0AInitially%2C%20we%20employ%20generated%20auxiliary%20closed-set%20noisy%20samples%20to%20enable%20the%0Amodel%20to%20identify%20noisy%20data%20at%20the%20early%20stages%20of%20training.%20Subsequently%2C%20we%0Aintroduce%20how%20samples%20are%20split%20into%20clean%2C%20ambiguous%20and%20noisy%20groups%20by%20their%0Asimilarity%20to%20the%20positive%20and%20nearest%20negative%20centers.%20Then%20we%20perform%20label%0Afusion%20for%20ambiguous%20samples%20by%20incorporating%20accumulated%20model%20predictions.%0AFinally%2C%20we%20apply%20label%20smoothing%20within%20the%20closed%20set%2C%20adjusting%20the%20label%20to%0Aa%20point%20between%20the%20nearest%20negative%20class%20and%20the%20initially%20assigned%20label.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20our%20method%20on%20mainstream%0Aface%20datasets%2C%20achieving%20state-of-the-art%20results.%20The%20code%20will%20be%20released%0Aupon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12031v1&entry.124074799=Read"},
{"title": "Cost-Effective Label-free Node Classification with LLMs", "author": "Taiyan Zhang and Renchi Yang and Mingyu Yan and Xiaochun Ye and Dongrui Fan and Yurui Lai", "abstract": "  Graph neural networks (GNNs) have emerged as go-to models for node\nclassification in graph data due to their powerful abilities in fusing graph\nstructures and attributes. However, such models strongly rely on adequate\nhigh-quality labeled data for training, which are expensive to acquire in\npractice. With the advent of large language models (LLMs), a promising way is\nto leverage their superb zero-shot capabilities and massive knowledge for node\nlabeling. Despite promising results reported, this methodology either demands\nconsiderable queries to LLMs, or suffers from compromised performance caused by\nnoisy labels produced by LLMs.\n  To remedy these issues, this work presents Cella, an active self-training\nframework that integrates LLMs into GNNs in a cost-effective manner. The design\nrecipe of Cella is to iteratively identify small sets of \"critical\" samples\nusing GNNs and extract informative pseudo-labels for them with both LLMs and\nGNNs as additional supervision signals to enhance model training. Particularly,\nCella includes three major components: (i) an effective active node selection\nstrategy for initial annotations; (ii) a judicious sample selection scheme to\nsift out the \"critical\" nodes based on label disharmonicity and entropy; and\n(iii) a label refinement module combining LLMs and GNNs with rewired topology.\nOur extensive experiments over five benchmark text-attributed graph datasets\ndemonstrate that Cella significantly outperforms the state of the arts under\nthe same query budget to LLMs in terms of label-free node classification. In\nparticular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an\n8.08% conspicuous improvement in accuracy over the state-of-the-art at a cost\nof less than one cent.\n", "link": "http://arxiv.org/abs/2412.11983v1", "date": "2024-12-16", "relevancy": 2.2323, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4672}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4362}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-Effective%20Label-free%20Node%20Classification%20with%20LLMs&body=Title%3A%20Cost-Effective%20Label-free%20Node%20Classification%20with%20LLMs%0AAuthor%3A%20Taiyan%20Zhang%20and%20Renchi%20Yang%20and%20Mingyu%20Yan%20and%20Xiaochun%20Ye%20and%20Dongrui%20Fan%20and%20Yurui%20Lai%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20go-to%20models%20for%20node%0Aclassification%20in%20graph%20data%20due%20to%20their%20powerful%20abilities%20in%20fusing%20graph%0Astructures%20and%20attributes.%20However%2C%20such%20models%20strongly%20rely%20on%20adequate%0Ahigh-quality%20labeled%20data%20for%20training%2C%20which%20are%20expensive%20to%20acquire%20in%0Apractice.%20With%20the%20advent%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20promising%20way%20is%0Ato%20leverage%20their%20superb%20zero-shot%20capabilities%20and%20massive%20knowledge%20for%20node%0Alabeling.%20Despite%20promising%20results%20reported%2C%20this%20methodology%20either%20demands%0Aconsiderable%20queries%20to%20LLMs%2C%20or%20suffers%20from%20compromised%20performance%20caused%20by%0Anoisy%20labels%20produced%20by%20LLMs.%0A%20%20To%20remedy%20these%20issues%2C%20this%20work%20presents%20Cella%2C%20an%20active%20self-training%0Aframework%20that%20integrates%20LLMs%20into%20GNNs%20in%20a%20cost-effective%20manner.%20The%20design%0Arecipe%20of%20Cella%20is%20to%20iteratively%20identify%20small%20sets%20of%20%22critical%22%20samples%0Ausing%20GNNs%20and%20extract%20informative%20pseudo-labels%20for%20them%20with%20both%20LLMs%20and%0AGNNs%20as%20additional%20supervision%20signals%20to%20enhance%20model%20training.%20Particularly%2C%0ACella%20includes%20three%20major%20components%3A%20%28i%29%20an%20effective%20active%20node%20selection%0Astrategy%20for%20initial%20annotations%3B%20%28ii%29%20a%20judicious%20sample%20selection%20scheme%20to%0Asift%20out%20the%20%22critical%22%20nodes%20based%20on%20label%20disharmonicity%20and%20entropy%3B%20and%0A%28iii%29%20a%20label%20refinement%20module%20combining%20LLMs%20and%20GNNs%20with%20rewired%20topology.%0AOur%20extensive%20experiments%20over%20five%20benchmark%20text-attributed%20graph%20datasets%0Ademonstrate%20that%20Cella%20significantly%20outperforms%20the%20state%20of%20the%20arts%20under%0Athe%20same%20query%20budget%20to%20LLMs%20in%20terms%20of%20label-free%20node%20classification.%20In%0Aparticular%2C%20on%20the%20DBLP%20dataset%20with%2014.3k%20nodes%2C%20Cella%20is%20able%20to%20achieve%20an%0A8.08%25%20conspicuous%20improvement%20in%20accuracy%20over%20the%20state-of-the-art%20at%20a%20cost%0Aof%20less%20than%20one%20cent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-Effective%2520Label-free%2520Node%2520Classification%2520with%2520LLMs%26entry.906535625%3DTaiyan%2520Zhang%2520and%2520Renchi%2520Yang%2520and%2520Mingyu%2520Yan%2520and%2520Xiaochun%2520Ye%2520and%2520Dongrui%2520Fan%2520and%2520Yurui%2520Lai%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520go-to%2520models%2520for%2520node%250Aclassification%2520in%2520graph%2520data%2520due%2520to%2520their%2520powerful%2520abilities%2520in%2520fusing%2520graph%250Astructures%2520and%2520attributes.%2520However%252C%2520such%2520models%2520strongly%2520rely%2520on%2520adequate%250Ahigh-quality%2520labeled%2520data%2520for%2520training%252C%2520which%2520are%2520expensive%2520to%2520acquire%2520in%250Apractice.%2520With%2520the%2520advent%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520a%2520promising%2520way%2520is%250Ato%2520leverage%2520their%2520superb%2520zero-shot%2520capabilities%2520and%2520massive%2520knowledge%2520for%2520node%250Alabeling.%2520Despite%2520promising%2520results%2520reported%252C%2520this%2520methodology%2520either%2520demands%250Aconsiderable%2520queries%2520to%2520LLMs%252C%2520or%2520suffers%2520from%2520compromised%2520performance%2520caused%2520by%250Anoisy%2520labels%2520produced%2520by%2520LLMs.%250A%2520%2520To%2520remedy%2520these%2520issues%252C%2520this%2520work%2520presents%2520Cella%252C%2520an%2520active%2520self-training%250Aframework%2520that%2520integrates%2520LLMs%2520into%2520GNNs%2520in%2520a%2520cost-effective%2520manner.%2520The%2520design%250Arecipe%2520of%2520Cella%2520is%2520to%2520iteratively%2520identify%2520small%2520sets%2520of%2520%2522critical%2522%2520samples%250Ausing%2520GNNs%2520and%2520extract%2520informative%2520pseudo-labels%2520for%2520them%2520with%2520both%2520LLMs%2520and%250AGNNs%2520as%2520additional%2520supervision%2520signals%2520to%2520enhance%2520model%2520training.%2520Particularly%252C%250ACella%2520includes%2520three%2520major%2520components%253A%2520%2528i%2529%2520an%2520effective%2520active%2520node%2520selection%250Astrategy%2520for%2520initial%2520annotations%253B%2520%2528ii%2529%2520a%2520judicious%2520sample%2520selection%2520scheme%2520to%250Asift%2520out%2520the%2520%2522critical%2522%2520nodes%2520based%2520on%2520label%2520disharmonicity%2520and%2520entropy%253B%2520and%250A%2528iii%2529%2520a%2520label%2520refinement%2520module%2520combining%2520LLMs%2520and%2520GNNs%2520with%2520rewired%2520topology.%250AOur%2520extensive%2520experiments%2520over%2520five%2520benchmark%2520text-attributed%2520graph%2520datasets%250Ademonstrate%2520that%2520Cella%2520significantly%2520outperforms%2520the%2520state%2520of%2520the%2520arts%2520under%250Athe%2520same%2520query%2520budget%2520to%2520LLMs%2520in%2520terms%2520of%2520label-free%2520node%2520classification.%2520In%250Aparticular%252C%2520on%2520the%2520DBLP%2520dataset%2520with%252014.3k%2520nodes%252C%2520Cella%2520is%2520able%2520to%2520achieve%2520an%250A8.08%2525%2520conspicuous%2520improvement%2520in%2520accuracy%2520over%2520the%2520state-of-the-art%2520at%2520a%2520cost%250Aof%2520less%2520than%2520one%2520cent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-Effective%20Label-free%20Node%20Classification%20with%20LLMs&entry.906535625=Taiyan%20Zhang%20and%20Renchi%20Yang%20and%20Mingyu%20Yan%20and%20Xiaochun%20Ye%20and%20Dongrui%20Fan%20and%20Yurui%20Lai&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20go-to%20models%20for%20node%0Aclassification%20in%20graph%20data%20due%20to%20their%20powerful%20abilities%20in%20fusing%20graph%0Astructures%20and%20attributes.%20However%2C%20such%20models%20strongly%20rely%20on%20adequate%0Ahigh-quality%20labeled%20data%20for%20training%2C%20which%20are%20expensive%20to%20acquire%20in%0Apractice.%20With%20the%20advent%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20promising%20way%20is%0Ato%20leverage%20their%20superb%20zero-shot%20capabilities%20and%20massive%20knowledge%20for%20node%0Alabeling.%20Despite%20promising%20results%20reported%2C%20this%20methodology%20either%20demands%0Aconsiderable%20queries%20to%20LLMs%2C%20or%20suffers%20from%20compromised%20performance%20caused%20by%0Anoisy%20labels%20produced%20by%20LLMs.%0A%20%20To%20remedy%20these%20issues%2C%20this%20work%20presents%20Cella%2C%20an%20active%20self-training%0Aframework%20that%20integrates%20LLMs%20into%20GNNs%20in%20a%20cost-effective%20manner.%20The%20design%0Arecipe%20of%20Cella%20is%20to%20iteratively%20identify%20small%20sets%20of%20%22critical%22%20samples%0Ausing%20GNNs%20and%20extract%20informative%20pseudo-labels%20for%20them%20with%20both%20LLMs%20and%0AGNNs%20as%20additional%20supervision%20signals%20to%20enhance%20model%20training.%20Particularly%2C%0ACella%20includes%20three%20major%20components%3A%20%28i%29%20an%20effective%20active%20node%20selection%0Astrategy%20for%20initial%20annotations%3B%20%28ii%29%20a%20judicious%20sample%20selection%20scheme%20to%0Asift%20out%20the%20%22critical%22%20nodes%20based%20on%20label%20disharmonicity%20and%20entropy%3B%20and%0A%28iii%29%20a%20label%20refinement%20module%20combining%20LLMs%20and%20GNNs%20with%20rewired%20topology.%0AOur%20extensive%20experiments%20over%20five%20benchmark%20text-attributed%20graph%20datasets%0Ademonstrate%20that%20Cella%20significantly%20outperforms%20the%20state%20of%20the%20arts%20under%0Athe%20same%20query%20budget%20to%20LLMs%20in%20terms%20of%20label-free%20node%20classification.%20In%0Aparticular%2C%20on%20the%20DBLP%20dataset%20with%2014.3k%20nodes%2C%20Cella%20is%20able%20to%20achieve%20an%0A8.08%25%20conspicuous%20improvement%20in%20accuracy%20over%20the%20state-of-the-art%20at%20a%20cost%0Aof%20less%20than%20one%20cent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11983v1&entry.124074799=Read"},
{"title": "IRR: Image Review Ranking Framework for Evaluating Vision-Language\n  Models", "author": "Kazuki Hayashi and Kazuma Onishi and Toma Suzuki and Yusuke Ide and Seiji Gobara and Shigeki Saito and Yusuke Sakai and Hidetaka Kamigaito and Katsuhiko Hayashi and Taro Watanabe", "abstract": "  Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.\n", "link": "http://arxiv.org/abs/2402.12121v2", "date": "2024-12-16", "relevancy": 2.2211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRR%3A%20Image%20Review%20Ranking%20Framework%20for%20Evaluating%20Vision-Language%0A%20%20Models&body=Title%3A%20IRR%3A%20Image%20Review%20Ranking%20Framework%20for%20Evaluating%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Kazuki%20Hayashi%20and%20Kazuma%20Onishi%20and%20Toma%20Suzuki%20and%20Yusuke%20Ide%20and%20Seiji%20Gobara%20and%20Shigeki%20Saito%20and%20Yusuke%20Sakai%20and%20Hidetaka%20Kamigaito%20and%20Katsuhiko%20Hayashi%20and%20Taro%20Watanabe%0AAbstract%3A%20%20%20Large-scale%20Vision-Language%20Models%20%28LVLMs%29%20process%20both%20images%20and%20text%2C%0Aexcelling%20in%20multimodal%20tasks%20such%20as%20image%20captioning%20and%20description%0Ageneration.%20However%2C%20while%20these%20models%20excel%20at%20generating%20factual%20content%2C%0Atheir%20ability%20to%20generate%20and%20evaluate%20texts%20reflecting%20perspectives%20on%20the%0Asame%20image%2C%20depending%20on%20the%20context%2C%20has%20not%20been%20sufficiently%20explored.%20To%0Aaddress%20this%2C%20we%20propose%20IRR%3A%20Image%20Review%20Rank%2C%20a%20novel%20evaluation%20framework%0Adesigned%20to%20assess%20critic%20review%20texts%20from%20multiple%20perspectives.%20IRR%0Aevaluates%20LVLMs%20by%20measuring%20how%20closely%20their%20judgments%20align%20with%20human%0Ainterpretations.%20We%20validate%20it%20using%20a%20dataset%20of%20images%20from%2015%20categories%2C%0Aeach%20with%20five%20critic%20review%20texts%20and%20annotated%20rankings%20in%20both%20English%20and%0AJapanese%2C%20totaling%20over%202%2C000%20data%20instances.%20The%20datasets%20are%20available%20at%0Ahttps%3A//hf.co/datasets/naist-nlp/Wiki-ImageReview1.0.%20Our%20results%20indicate%0Athat%2C%20although%20LVLMs%20exhibited%20consistent%20performance%20across%20languages%2C%20their%0Acorrelation%20with%20human%20annotations%20was%20insufficient%2C%20highlighting%20the%20need%20for%0Afurther%20advancements.%20These%20findings%20highlight%20the%20limitations%20of%20current%0Aevaluation%20methods%20and%20the%20need%20for%20approaches%20that%20better%20capture%20human%0Areasoning%20in%20Vision%20%26%20Language%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12121v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRR%253A%2520Image%2520Review%2520Ranking%2520Framework%2520for%2520Evaluating%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DKazuki%2520Hayashi%2520and%2520Kazuma%2520Onishi%2520and%2520Toma%2520Suzuki%2520and%2520Yusuke%2520Ide%2520and%2520Seiji%2520Gobara%2520and%2520Shigeki%2520Saito%2520and%2520Yusuke%2520Sakai%2520and%2520Hidetaka%2520Kamigaito%2520and%2520Katsuhiko%2520Hayashi%2520and%2520Taro%2520Watanabe%26entry.1292438233%3D%2520%2520Large-scale%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520process%2520both%2520images%2520and%2520text%252C%250Aexcelling%2520in%2520multimodal%2520tasks%2520such%2520as%2520image%2520captioning%2520and%2520description%250Ageneration.%2520However%252C%2520while%2520these%2520models%2520excel%2520at%2520generating%2520factual%2520content%252C%250Atheir%2520ability%2520to%2520generate%2520and%2520evaluate%2520texts%2520reflecting%2520perspectives%2520on%2520the%250Asame%2520image%252C%2520depending%2520on%2520the%2520context%252C%2520has%2520not%2520been%2520sufficiently%2520explored.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520IRR%253A%2520Image%2520Review%2520Rank%252C%2520a%2520novel%2520evaluation%2520framework%250Adesigned%2520to%2520assess%2520critic%2520review%2520texts%2520from%2520multiple%2520perspectives.%2520IRR%250Aevaluates%2520LVLMs%2520by%2520measuring%2520how%2520closely%2520their%2520judgments%2520align%2520with%2520human%250Ainterpretations.%2520We%2520validate%2520it%2520using%2520a%2520dataset%2520of%2520images%2520from%252015%2520categories%252C%250Aeach%2520with%2520five%2520critic%2520review%2520texts%2520and%2520annotated%2520rankings%2520in%2520both%2520English%2520and%250AJapanese%252C%2520totaling%2520over%25202%252C000%2520data%2520instances.%2520The%2520datasets%2520are%2520available%2520at%250Ahttps%253A//hf.co/datasets/naist-nlp/Wiki-ImageReview1.0.%2520Our%2520results%2520indicate%250Athat%252C%2520although%2520LVLMs%2520exhibited%2520consistent%2520performance%2520across%2520languages%252C%2520their%250Acorrelation%2520with%2520human%2520annotations%2520was%2520insufficient%252C%2520highlighting%2520the%2520need%2520for%250Afurther%2520advancements.%2520These%2520findings%2520highlight%2520the%2520limitations%2520of%2520current%250Aevaluation%2520methods%2520and%2520the%2520need%2520for%2520approaches%2520that%2520better%2520capture%2520human%250Areasoning%2520in%2520Vision%2520%2526%2520Language%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12121v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRR%3A%20Image%20Review%20Ranking%20Framework%20for%20Evaluating%20Vision-Language%0A%20%20Models&entry.906535625=Kazuki%20Hayashi%20and%20Kazuma%20Onishi%20and%20Toma%20Suzuki%20and%20Yusuke%20Ide%20and%20Seiji%20Gobara%20and%20Shigeki%20Saito%20and%20Yusuke%20Sakai%20and%20Hidetaka%20Kamigaito%20and%20Katsuhiko%20Hayashi%20and%20Taro%20Watanabe&entry.1292438233=%20%20Large-scale%20Vision-Language%20Models%20%28LVLMs%29%20process%20both%20images%20and%20text%2C%0Aexcelling%20in%20multimodal%20tasks%20such%20as%20image%20captioning%20and%20description%0Ageneration.%20However%2C%20while%20these%20models%20excel%20at%20generating%20factual%20content%2C%0Atheir%20ability%20to%20generate%20and%20evaluate%20texts%20reflecting%20perspectives%20on%20the%0Asame%20image%2C%20depending%20on%20the%20context%2C%20has%20not%20been%20sufficiently%20explored.%20To%0Aaddress%20this%2C%20we%20propose%20IRR%3A%20Image%20Review%20Rank%2C%20a%20novel%20evaluation%20framework%0Adesigned%20to%20assess%20critic%20review%20texts%20from%20multiple%20perspectives.%20IRR%0Aevaluates%20LVLMs%20by%20measuring%20how%20closely%20their%20judgments%20align%20with%20human%0Ainterpretations.%20We%20validate%20it%20using%20a%20dataset%20of%20images%20from%2015%20categories%2C%0Aeach%20with%20five%20critic%20review%20texts%20and%20annotated%20rankings%20in%20both%20English%20and%0AJapanese%2C%20totaling%20over%202%2C000%20data%20instances.%20The%20datasets%20are%20available%20at%0Ahttps%3A//hf.co/datasets/naist-nlp/Wiki-ImageReview1.0.%20Our%20results%20indicate%0Athat%2C%20although%20LVLMs%20exhibited%20consistent%20performance%20across%20languages%2C%20their%0Acorrelation%20with%20human%20annotations%20was%20insufficient%2C%20highlighting%20the%20need%20for%0Afurther%20advancements.%20These%20findings%20highlight%20the%20limitations%20of%20current%0Aevaluation%20methods%20and%20the%20need%20for%20approaches%20that%20better%20capture%20human%0Areasoning%20in%20Vision%20%26%20Language%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12121v2&entry.124074799=Read"},
{"title": "Learning to Navigate in Mazes with Novel Layouts using Abstract Top-down\n  Maps", "author": "Linfeng Zhao and Lawson L. S. Wong", "abstract": "  Learning navigation capabilities in different environments has long been one\nof the major challenges in decision-making. In this work, we focus on zero-shot\nnavigation ability using given abstract $2$-D top-down maps. Like human\nnavigation by reading a paper map, the agent reads the map as an image when\nnavigating in a novel layout, after learning to navigate on a set of training\nmaps. We propose a model-based reinforcement learning approach for this\nmulti-task learning problem, where it jointly learns a hypermodel that takes\ntop-down maps as input and predicts the weights of the transition network. We\nuse the DeepMind Lab environment and customize layouts using generated maps.\nOur method can adapt better to novel environments in zero-shot and is more\nrobust to noise.\n", "link": "http://arxiv.org/abs/2412.12024v1", "date": "2024-12-16", "relevancy": 2.186, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5733}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5564}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Navigate%20in%20Mazes%20with%20Novel%20Layouts%20using%20Abstract%20Top-down%0A%20%20Maps&body=Title%3A%20Learning%20to%20Navigate%20in%20Mazes%20with%20Novel%20Layouts%20using%20Abstract%20Top-down%0A%20%20Maps%0AAuthor%3A%20Linfeng%20Zhao%20and%20Lawson%20L.%20S.%20Wong%0AAbstract%3A%20%20%20Learning%20navigation%20capabilities%20in%20different%20environments%20has%20long%20been%20one%0Aof%20the%20major%20challenges%20in%20decision-making.%20In%20this%20work%2C%20we%20focus%20on%20zero-shot%0Anavigation%20ability%20using%20given%20abstract%20%242%24-D%20top-down%20maps.%20Like%20human%0Anavigation%20by%20reading%20a%20paper%20map%2C%20the%20agent%20reads%20the%20map%20as%20an%20image%20when%0Anavigating%20in%20a%20novel%20layout%2C%20after%20learning%20to%20navigate%20on%20a%20set%20of%20training%0Amaps.%20We%20propose%20a%20model-based%20reinforcement%20learning%20approach%20for%20this%0Amulti-task%20learning%20problem%2C%20where%20it%20jointly%20learns%20a%20hypermodel%20that%20takes%0Atop-down%20maps%20as%20input%20and%20predicts%20the%20weights%20of%20the%20transition%20network.%20We%0Ause%20the%20DeepMind%20Lab%20environment%20and%20customize%20layouts%20using%20generated%20maps.%0AOur%20method%20can%20adapt%20better%20to%20novel%20environments%20in%20zero-shot%20and%20is%20more%0Arobust%20to%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Navigate%2520in%2520Mazes%2520with%2520Novel%2520Layouts%2520using%2520Abstract%2520Top-down%250A%2520%2520Maps%26entry.906535625%3DLinfeng%2520Zhao%2520and%2520Lawson%2520L.%2520S.%2520Wong%26entry.1292438233%3D%2520%2520Learning%2520navigation%2520capabilities%2520in%2520different%2520environments%2520has%2520long%2520been%2520one%250Aof%2520the%2520major%2520challenges%2520in%2520decision-making.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520zero-shot%250Anavigation%2520ability%2520using%2520given%2520abstract%2520%25242%2524-D%2520top-down%2520maps.%2520Like%2520human%250Anavigation%2520by%2520reading%2520a%2520paper%2520map%252C%2520the%2520agent%2520reads%2520the%2520map%2520as%2520an%2520image%2520when%250Anavigating%2520in%2520a%2520novel%2520layout%252C%2520after%2520learning%2520to%2520navigate%2520on%2520a%2520set%2520of%2520training%250Amaps.%2520We%2520propose%2520a%2520model-based%2520reinforcement%2520learning%2520approach%2520for%2520this%250Amulti-task%2520learning%2520problem%252C%2520where%2520it%2520jointly%2520learns%2520a%2520hypermodel%2520that%2520takes%250Atop-down%2520maps%2520as%2520input%2520and%2520predicts%2520the%2520weights%2520of%2520the%2520transition%2520network.%2520We%250Ause%2520the%2520DeepMind%2520Lab%2520environment%2520and%2520customize%2520layouts%2520using%2520generated%2520maps.%250AOur%2520method%2520can%2520adapt%2520better%2520to%2520novel%2520environments%2520in%2520zero-shot%2520and%2520is%2520more%250Arobust%2520to%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Navigate%20in%20Mazes%20with%20Novel%20Layouts%20using%20Abstract%20Top-down%0A%20%20Maps&entry.906535625=Linfeng%20Zhao%20and%20Lawson%20L.%20S.%20Wong&entry.1292438233=%20%20Learning%20navigation%20capabilities%20in%20different%20environments%20has%20long%20been%20one%0Aof%20the%20major%20challenges%20in%20decision-making.%20In%20this%20work%2C%20we%20focus%20on%20zero-shot%0Anavigation%20ability%20using%20given%20abstract%20%242%24-D%20top-down%20maps.%20Like%20human%0Anavigation%20by%20reading%20a%20paper%20map%2C%20the%20agent%20reads%20the%20map%20as%20an%20image%20when%0Anavigating%20in%20a%20novel%20layout%2C%20after%20learning%20to%20navigate%20on%20a%20set%20of%20training%0Amaps.%20We%20propose%20a%20model-based%20reinforcement%20learning%20approach%20for%20this%0Amulti-task%20learning%20problem%2C%20where%20it%20jointly%20learns%20a%20hypermodel%20that%20takes%0Atop-down%20maps%20as%20input%20and%20predicts%20the%20weights%20of%20the%20transition%20network.%20We%0Ause%20the%20DeepMind%20Lab%20environment%20and%20customize%20layouts%20using%20generated%20maps.%0AOur%20method%20can%20adapt%20better%20to%20novel%20environments%20in%20zero-shot%20and%20is%20more%0Arobust%20to%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12024v1&entry.124074799=Read"},
{"title": "SAMIC: Segment Anything with In-Context Spatial Prompt Engineering", "author": "Savinay Nagendra and Kashif Rashid and Chaopeng Shen and Daniel Kifer", "abstract": "  Few-shot segmentation is the problem of learning to identify specific types\nof objects (e.g., airplanes) in images from a small set of labeled reference\nimages. The current state of the art is driven by resource-intensive\nconstruction of models for every new domain-specific application. Such models\nmust be trained on enormous labeled datasets of unrelated objects (e.g., cars,\ntrains, animals) so that their ``knowledge'' can be transferred to new types of\nobjects. In this paper, we show how to leverage existing vision foundation\nmodels (VFMs) to reduce the incremental cost of creating few-shot segmentation\nmodels for new domains. Specifically, we introduce SAMIC, a small network that\nlearns how to prompt VFMs in order to segment new types of objects in\ndomain-specific applications. SAMIC enables any task to be approached as a\nfew-shot learning problem. At 2.6 million parameters, it is 94% smaller than\nthe leading models (e.g., having ResNet 101 backbone with 45+ million\nparameters). Even using 1/5th of the training data provided by one-shot\nbenchmarks, SAMIC is competitive with, or sets the state of the art, on a\nvariety of few-shot and semantic segmentation datasets including COCO-$20^i$,\nPascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10.\n", "link": "http://arxiv.org/abs/2412.11998v1", "date": "2024-12-16", "relevancy": 2.185, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMIC%3A%20Segment%20Anything%20with%20In-Context%20Spatial%20Prompt%20Engineering&body=Title%3A%20SAMIC%3A%20Segment%20Anything%20with%20In-Context%20Spatial%20Prompt%20Engineering%0AAuthor%3A%20Savinay%20Nagendra%20and%20Kashif%20Rashid%20and%20Chaopeng%20Shen%20and%20Daniel%20Kifer%0AAbstract%3A%20%20%20Few-shot%20segmentation%20is%20the%20problem%20of%20learning%20to%20identify%20specific%20types%0Aof%20objects%20%28e.g.%2C%20airplanes%29%20in%20images%20from%20a%20small%20set%20of%20labeled%20reference%0Aimages.%20The%20current%20state%20of%20the%20art%20is%20driven%20by%20resource-intensive%0Aconstruction%20of%20models%20for%20every%20new%20domain-specific%20application.%20Such%20models%0Amust%20be%20trained%20on%20enormous%20labeled%20datasets%20of%20unrelated%20objects%20%28e.g.%2C%20cars%2C%0Atrains%2C%20animals%29%20so%20that%20their%20%60%60knowledge%27%27%20can%20be%20transferred%20to%20new%20types%20of%0Aobjects.%20In%20this%20paper%2C%20we%20show%20how%20to%20leverage%20existing%20vision%20foundation%0Amodels%20%28VFMs%29%20to%20reduce%20the%20incremental%20cost%20of%20creating%20few-shot%20segmentation%0Amodels%20for%20new%20domains.%20Specifically%2C%20we%20introduce%20SAMIC%2C%20a%20small%20network%20that%0Alearns%20how%20to%20prompt%20VFMs%20in%20order%20to%20segment%20new%20types%20of%20objects%20in%0Adomain-specific%20applications.%20SAMIC%20enables%20any%20task%20to%20be%20approached%20as%20a%0Afew-shot%20learning%20problem.%20At%202.6%20million%20parameters%2C%20it%20is%2094%25%20smaller%20than%0Athe%20leading%20models%20%28e.g.%2C%20having%20ResNet%20101%20backbone%20with%2045%2B%20million%0Aparameters%29.%20Even%20using%201/5th%20of%20the%20training%20data%20provided%20by%20one-shot%0Abenchmarks%2C%20SAMIC%20is%20competitive%20with%2C%20or%20sets%20the%20state%20of%20the%20art%2C%20on%20a%0Avariety%20of%20few-shot%20and%20semantic%20segmentation%20datasets%20including%20COCO-%2420%5Ei%24%2C%0APascal-%245%5Ei%24%2C%20PerSeg%2C%20FSS-1000%2C%20and%20NWPU%20VHR-10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMIC%253A%2520Segment%2520Anything%2520with%2520In-Context%2520Spatial%2520Prompt%2520Engineering%26entry.906535625%3DSavinay%2520Nagendra%2520and%2520Kashif%2520Rashid%2520and%2520Chaopeng%2520Shen%2520and%2520Daniel%2520Kifer%26entry.1292438233%3D%2520%2520Few-shot%2520segmentation%2520is%2520the%2520problem%2520of%2520learning%2520to%2520identify%2520specific%2520types%250Aof%2520objects%2520%2528e.g.%252C%2520airplanes%2529%2520in%2520images%2520from%2520a%2520small%2520set%2520of%2520labeled%2520reference%250Aimages.%2520The%2520current%2520state%2520of%2520the%2520art%2520is%2520driven%2520by%2520resource-intensive%250Aconstruction%2520of%2520models%2520for%2520every%2520new%2520domain-specific%2520application.%2520Such%2520models%250Amust%2520be%2520trained%2520on%2520enormous%2520labeled%2520datasets%2520of%2520unrelated%2520objects%2520%2528e.g.%252C%2520cars%252C%250Atrains%252C%2520animals%2529%2520so%2520that%2520their%2520%2560%2560knowledge%2527%2527%2520can%2520be%2520transferred%2520to%2520new%2520types%2520of%250Aobjects.%2520In%2520this%2520paper%252C%2520we%2520show%2520how%2520to%2520leverage%2520existing%2520vision%2520foundation%250Amodels%2520%2528VFMs%2529%2520to%2520reduce%2520the%2520incremental%2520cost%2520of%2520creating%2520few-shot%2520segmentation%250Amodels%2520for%2520new%2520domains.%2520Specifically%252C%2520we%2520introduce%2520SAMIC%252C%2520a%2520small%2520network%2520that%250Alearns%2520how%2520to%2520prompt%2520VFMs%2520in%2520order%2520to%2520segment%2520new%2520types%2520of%2520objects%2520in%250Adomain-specific%2520applications.%2520SAMIC%2520enables%2520any%2520task%2520to%2520be%2520approached%2520as%2520a%250Afew-shot%2520learning%2520problem.%2520At%25202.6%2520million%2520parameters%252C%2520it%2520is%252094%2525%2520smaller%2520than%250Athe%2520leading%2520models%2520%2528e.g.%252C%2520having%2520ResNet%2520101%2520backbone%2520with%252045%252B%2520million%250Aparameters%2529.%2520Even%2520using%25201/5th%2520of%2520the%2520training%2520data%2520provided%2520by%2520one-shot%250Abenchmarks%252C%2520SAMIC%2520is%2520competitive%2520with%252C%2520or%2520sets%2520the%2520state%2520of%2520the%2520art%252C%2520on%2520a%250Avariety%2520of%2520few-shot%2520and%2520semantic%2520segmentation%2520datasets%2520including%2520COCO-%252420%255Ei%2524%252C%250APascal-%25245%255Ei%2524%252C%2520PerSeg%252C%2520FSS-1000%252C%2520and%2520NWPU%2520VHR-10.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMIC%3A%20Segment%20Anything%20with%20In-Context%20Spatial%20Prompt%20Engineering&entry.906535625=Savinay%20Nagendra%20and%20Kashif%20Rashid%20and%20Chaopeng%20Shen%20and%20Daniel%20Kifer&entry.1292438233=%20%20Few-shot%20segmentation%20is%20the%20problem%20of%20learning%20to%20identify%20specific%20types%0Aof%20objects%20%28e.g.%2C%20airplanes%29%20in%20images%20from%20a%20small%20set%20of%20labeled%20reference%0Aimages.%20The%20current%20state%20of%20the%20art%20is%20driven%20by%20resource-intensive%0Aconstruction%20of%20models%20for%20every%20new%20domain-specific%20application.%20Such%20models%0Amust%20be%20trained%20on%20enormous%20labeled%20datasets%20of%20unrelated%20objects%20%28e.g.%2C%20cars%2C%0Atrains%2C%20animals%29%20so%20that%20their%20%60%60knowledge%27%27%20can%20be%20transferred%20to%20new%20types%20of%0Aobjects.%20In%20this%20paper%2C%20we%20show%20how%20to%20leverage%20existing%20vision%20foundation%0Amodels%20%28VFMs%29%20to%20reduce%20the%20incremental%20cost%20of%20creating%20few-shot%20segmentation%0Amodels%20for%20new%20domains.%20Specifically%2C%20we%20introduce%20SAMIC%2C%20a%20small%20network%20that%0Alearns%20how%20to%20prompt%20VFMs%20in%20order%20to%20segment%20new%20types%20of%20objects%20in%0Adomain-specific%20applications.%20SAMIC%20enables%20any%20task%20to%20be%20approached%20as%20a%0Afew-shot%20learning%20problem.%20At%202.6%20million%20parameters%2C%20it%20is%2094%25%20smaller%20than%0Athe%20leading%20models%20%28e.g.%2C%20having%20ResNet%20101%20backbone%20with%2045%2B%20million%0Aparameters%29.%20Even%20using%201/5th%20of%20the%20training%20data%20provided%20by%20one-shot%0Abenchmarks%2C%20SAMIC%20is%20competitive%20with%2C%20or%20sets%20the%20state%20of%20the%20art%2C%20on%20a%0Avariety%20of%20few-shot%20and%20semantic%20segmentation%20datasets%20including%20COCO-%2420%5Ei%24%2C%0APascal-%245%5Ei%24%2C%20PerSeg%2C%20FSS-1000%2C%20and%20NWPU%20VHR-10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11998v1&entry.124074799=Read"},
{"title": "COSMo: CLIP Talks on Open-Set Multi-Target Domain Adaptation", "author": "Munish Monga and Sachin Kumar Giroh and Ankit Jha and Mainak Singha and Biplab Banerjee and Jocelyn Chanussot", "abstract": "  Multi-Target Domain Adaptation (MTDA) entails learning domain-invariant\ninformation from a single source domain and applying it to multiple unlabeled\ntarget domains. Yet, existing MTDA methods predominantly focus on addressing\ndomain shifts within visual features, often overlooking semantic features and\nstruggling to handle unknown classes, resulting in what is known as Open-Set\n(OS) MTDA. While large-scale vision-language foundation models like CLIP show\npromise, their potential for MTDA remains largely unexplored. This paper\nintroduces COSMo, a novel method that learns domain-agnostic prompts through\nsource domain-guided prompt learning to tackle the MTDA problem in the prompt\nspace. By leveraging a domain-specific bias network and separate prompts for\nknown and unknown classes, COSMo effectively adapts across domain and class\nshifts. To the best of our knowledge, COSMo is the first method to address\nOpen-Set Multi-Target DA (OSMTDA), offering a more realistic representation of\nreal-world scenarios and addressing the challenges of both open-set and\nmulti-target DA. COSMo demonstrates an average improvement of $5.1\\%$ across\nthree challenging datasets: Mini-DomainNet, Office-31, and Office-Home,\ncompared to other related DA methods adapted to operate within the OSMTDA\nsetting. Code is available at: https://github.com/munish30monga/COSMo\n", "link": "http://arxiv.org/abs/2409.00397v2", "date": "2024-12-16", "relevancy": 2.1652, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COSMo%3A%20CLIP%20Talks%20on%20Open-Set%20Multi-Target%20Domain%20Adaptation&body=Title%3A%20COSMo%3A%20CLIP%20Talks%20on%20Open-Set%20Multi-Target%20Domain%20Adaptation%0AAuthor%3A%20Munish%20Monga%20and%20Sachin%20Kumar%20Giroh%20and%20Ankit%20Jha%20and%20Mainak%20Singha%20and%20Biplab%20Banerjee%20and%20Jocelyn%20Chanussot%0AAbstract%3A%20%20%20Multi-Target%20Domain%20Adaptation%20%28MTDA%29%20entails%20learning%20domain-invariant%0Ainformation%20from%20a%20single%20source%20domain%20and%20applying%20it%20to%20multiple%20unlabeled%0Atarget%20domains.%20Yet%2C%20existing%20MTDA%20methods%20predominantly%20focus%20on%20addressing%0Adomain%20shifts%20within%20visual%20features%2C%20often%20overlooking%20semantic%20features%20and%0Astruggling%20to%20handle%20unknown%20classes%2C%20resulting%20in%20what%20is%20known%20as%20Open-Set%0A%28OS%29%20MTDA.%20While%20large-scale%20vision-language%20foundation%20models%20like%20CLIP%20show%0Apromise%2C%20their%20potential%20for%20MTDA%20remains%20largely%20unexplored.%20This%20paper%0Aintroduces%20COSMo%2C%20a%20novel%20method%20that%20learns%20domain-agnostic%20prompts%20through%0Asource%20domain-guided%20prompt%20learning%20to%20tackle%20the%20MTDA%20problem%20in%20the%20prompt%0Aspace.%20By%20leveraging%20a%20domain-specific%20bias%20network%20and%20separate%20prompts%20for%0Aknown%20and%20unknown%20classes%2C%20COSMo%20effectively%20adapts%20across%20domain%20and%20class%0Ashifts.%20To%20the%20best%20of%20our%20knowledge%2C%20COSMo%20is%20the%20first%20method%20to%20address%0AOpen-Set%20Multi-Target%20DA%20%28OSMTDA%29%2C%20offering%20a%20more%20realistic%20representation%20of%0Areal-world%20scenarios%20and%20addressing%20the%20challenges%20of%20both%20open-set%20and%0Amulti-target%20DA.%20COSMo%20demonstrates%20an%20average%20improvement%20of%20%245.1%5C%25%24%20across%0Athree%20challenging%20datasets%3A%20Mini-DomainNet%2C%20Office-31%2C%20and%20Office-Home%2C%0Acompared%20to%20other%20related%20DA%20methods%20adapted%20to%20operate%20within%20the%20OSMTDA%0Asetting.%20Code%20is%20available%20at%3A%20https%3A//github.com/munish30monga/COSMo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOSMo%253A%2520CLIP%2520Talks%2520on%2520Open-Set%2520Multi-Target%2520Domain%2520Adaptation%26entry.906535625%3DMunish%2520Monga%2520and%2520Sachin%2520Kumar%2520Giroh%2520and%2520Ankit%2520Jha%2520and%2520Mainak%2520Singha%2520and%2520Biplab%2520Banerjee%2520and%2520Jocelyn%2520Chanussot%26entry.1292438233%3D%2520%2520Multi-Target%2520Domain%2520Adaptation%2520%2528MTDA%2529%2520entails%2520learning%2520domain-invariant%250Ainformation%2520from%2520a%2520single%2520source%2520domain%2520and%2520applying%2520it%2520to%2520multiple%2520unlabeled%250Atarget%2520domains.%2520Yet%252C%2520existing%2520MTDA%2520methods%2520predominantly%2520focus%2520on%2520addressing%250Adomain%2520shifts%2520within%2520visual%2520features%252C%2520often%2520overlooking%2520semantic%2520features%2520and%250Astruggling%2520to%2520handle%2520unknown%2520classes%252C%2520resulting%2520in%2520what%2520is%2520known%2520as%2520Open-Set%250A%2528OS%2529%2520MTDA.%2520While%2520large-scale%2520vision-language%2520foundation%2520models%2520like%2520CLIP%2520show%250Apromise%252C%2520their%2520potential%2520for%2520MTDA%2520remains%2520largely%2520unexplored.%2520This%2520paper%250Aintroduces%2520COSMo%252C%2520a%2520novel%2520method%2520that%2520learns%2520domain-agnostic%2520prompts%2520through%250Asource%2520domain-guided%2520prompt%2520learning%2520to%2520tackle%2520the%2520MTDA%2520problem%2520in%2520the%2520prompt%250Aspace.%2520By%2520leveraging%2520a%2520domain-specific%2520bias%2520network%2520and%2520separate%2520prompts%2520for%250Aknown%2520and%2520unknown%2520classes%252C%2520COSMo%2520effectively%2520adapts%2520across%2520domain%2520and%2520class%250Ashifts.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520COSMo%2520is%2520the%2520first%2520method%2520to%2520address%250AOpen-Set%2520Multi-Target%2520DA%2520%2528OSMTDA%2529%252C%2520offering%2520a%2520more%2520realistic%2520representation%2520of%250Areal-world%2520scenarios%2520and%2520addressing%2520the%2520challenges%2520of%2520both%2520open-set%2520and%250Amulti-target%2520DA.%2520COSMo%2520demonstrates%2520an%2520average%2520improvement%2520of%2520%25245.1%255C%2525%2524%2520across%250Athree%2520challenging%2520datasets%253A%2520Mini-DomainNet%252C%2520Office-31%252C%2520and%2520Office-Home%252C%250Acompared%2520to%2520other%2520related%2520DA%2520methods%2520adapted%2520to%2520operate%2520within%2520the%2520OSMTDA%250Asetting.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/munish30monga/COSMo%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COSMo%3A%20CLIP%20Talks%20on%20Open-Set%20Multi-Target%20Domain%20Adaptation&entry.906535625=Munish%20Monga%20and%20Sachin%20Kumar%20Giroh%20and%20Ankit%20Jha%20and%20Mainak%20Singha%20and%20Biplab%20Banerjee%20and%20Jocelyn%20Chanussot&entry.1292438233=%20%20Multi-Target%20Domain%20Adaptation%20%28MTDA%29%20entails%20learning%20domain-invariant%0Ainformation%20from%20a%20single%20source%20domain%20and%20applying%20it%20to%20multiple%20unlabeled%0Atarget%20domains.%20Yet%2C%20existing%20MTDA%20methods%20predominantly%20focus%20on%20addressing%0Adomain%20shifts%20within%20visual%20features%2C%20often%20overlooking%20semantic%20features%20and%0Astruggling%20to%20handle%20unknown%20classes%2C%20resulting%20in%20what%20is%20known%20as%20Open-Set%0A%28OS%29%20MTDA.%20While%20large-scale%20vision-language%20foundation%20models%20like%20CLIP%20show%0Apromise%2C%20their%20potential%20for%20MTDA%20remains%20largely%20unexplored.%20This%20paper%0Aintroduces%20COSMo%2C%20a%20novel%20method%20that%20learns%20domain-agnostic%20prompts%20through%0Asource%20domain-guided%20prompt%20learning%20to%20tackle%20the%20MTDA%20problem%20in%20the%20prompt%0Aspace.%20By%20leveraging%20a%20domain-specific%20bias%20network%20and%20separate%20prompts%20for%0Aknown%20and%20unknown%20classes%2C%20COSMo%20effectively%20adapts%20across%20domain%20and%20class%0Ashifts.%20To%20the%20best%20of%20our%20knowledge%2C%20COSMo%20is%20the%20first%20method%20to%20address%0AOpen-Set%20Multi-Target%20DA%20%28OSMTDA%29%2C%20offering%20a%20more%20realistic%20representation%20of%0Areal-world%20scenarios%20and%20addressing%20the%20challenges%20of%20both%20open-set%20and%0Amulti-target%20DA.%20COSMo%20demonstrates%20an%20average%20improvement%20of%20%245.1%5C%25%24%20across%0Athree%20challenging%20datasets%3A%20Mini-DomainNet%2C%20Office-31%2C%20and%20Office-Home%2C%0Acompared%20to%20other%20related%20DA%20methods%20adapted%20to%20operate%20within%20the%20OSMTDA%0Asetting.%20Code%20is%20available%20at%3A%20https%3A//github.com/munish30monga/COSMo%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00397v2&entry.124074799=Read"},
{"title": "GSDiff: Synthesizing Vector Floorplans via Geometry-enhanced Structural\n  Graph Generation", "author": "Sizhe Hu and Wenming Wu and Yuntao Wang and Benzhu Xu and Liping Zheng", "abstract": "  Automating architectural floorplan design is vital for housing and interior\ndesign, offering a faster, cost-effective alternative to manual sketches by\narchitects. However, existing methods, including rule-based and learning-based\napproaches, face challenges in design complexity and constrained generation\nwith extensive post-processing, and tend to obvious geometric inconsistencies\nsuch as misalignment, overlap, and gaps. In this work, we propose a novel\ngenerative framework for vector floorplan design via structural graph\ngeneration, called GSDiff, focusing on wall junction generation and wall\nsegment prediction to capture both geometric and semantic aspects of structural\ngraphs. To improve the geometric rationality of generated structural graphs, we\npropose two innovative geometry enhancement methods. In wall junction\ngeneration, we propose a novel alignment loss function to improve geometric\nconsistency. In wall segment prediction, we propose a random self-supervision\nmethod to enhance the model's perception of the overall geometric structure,\nthereby promoting the generation of reasonable geometric structures. Employing\nthe diffusion model and the Transformer model, as well as the geometry\nenhancement strategies, our framework can generate wall junctions, wall\nsegments and room polygons with structural and semantic information, resulting\nin structural graphs that accurately represent floorplans. Extensive\nexperiments show that the proposed method surpasses existing techniques,\nenabling free generation and constrained generation, marking a shift towards\nstructure generation in architectural design. Code and data are available at\nhttps://github.com/SizheHu/GSDiff.\n", "link": "http://arxiv.org/abs/2408.16258v2", "date": "2024-12-16", "relevancy": 2.1464, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5449}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.542}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSDiff%3A%20Synthesizing%20Vector%20Floorplans%20via%20Geometry-enhanced%20Structural%0A%20%20Graph%20Generation&body=Title%3A%20GSDiff%3A%20Synthesizing%20Vector%20Floorplans%20via%20Geometry-enhanced%20Structural%0A%20%20Graph%20Generation%0AAuthor%3A%20Sizhe%20Hu%20and%20Wenming%20Wu%20and%20Yuntao%20Wang%20and%20Benzhu%20Xu%20and%20Liping%20Zheng%0AAbstract%3A%20%20%20Automating%20architectural%20floorplan%20design%20is%20vital%20for%20housing%20and%20interior%0Adesign%2C%20offering%20a%20faster%2C%20cost-effective%20alternative%20to%20manual%20sketches%20by%0Aarchitects.%20However%2C%20existing%20methods%2C%20including%20rule-based%20and%20learning-based%0Aapproaches%2C%20face%20challenges%20in%20design%20complexity%20and%20constrained%20generation%0Awith%20extensive%20post-processing%2C%20and%20tend%20to%20obvious%20geometric%20inconsistencies%0Asuch%20as%20misalignment%2C%20overlap%2C%20and%20gaps.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Agenerative%20framework%20for%20vector%20floorplan%20design%20via%20structural%20graph%0Ageneration%2C%20called%20GSDiff%2C%20focusing%20on%20wall%20junction%20generation%20and%20wall%0Asegment%20prediction%20to%20capture%20both%20geometric%20and%20semantic%20aspects%20of%20structural%0Agraphs.%20To%20improve%20the%20geometric%20rationality%20of%20generated%20structural%20graphs%2C%20we%0Apropose%20two%20innovative%20geometry%20enhancement%20methods.%20In%20wall%20junction%0Ageneration%2C%20we%20propose%20a%20novel%20alignment%20loss%20function%20to%20improve%20geometric%0Aconsistency.%20In%20wall%20segment%20prediction%2C%20we%20propose%20a%20random%20self-supervision%0Amethod%20to%20enhance%20the%20model%27s%20perception%20of%20the%20overall%20geometric%20structure%2C%0Athereby%20promoting%20the%20generation%20of%20reasonable%20geometric%20structures.%20Employing%0Athe%20diffusion%20model%20and%20the%20Transformer%20model%2C%20as%20well%20as%20the%20geometry%0Aenhancement%20strategies%2C%20our%20framework%20can%20generate%20wall%20junctions%2C%20wall%0Asegments%20and%20room%20polygons%20with%20structural%20and%20semantic%20information%2C%20resulting%0Ain%20structural%20graphs%20that%20accurately%20represent%20floorplans.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20method%20surpasses%20existing%20techniques%2C%0Aenabling%20free%20generation%20and%20constrained%20generation%2C%20marking%20a%20shift%20towards%0Astructure%20generation%20in%20architectural%20design.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/SizheHu/GSDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSDiff%253A%2520Synthesizing%2520Vector%2520Floorplans%2520via%2520Geometry-enhanced%2520Structural%250A%2520%2520Graph%2520Generation%26entry.906535625%3DSizhe%2520Hu%2520and%2520Wenming%2520Wu%2520and%2520Yuntao%2520Wang%2520and%2520Benzhu%2520Xu%2520and%2520Liping%2520Zheng%26entry.1292438233%3D%2520%2520Automating%2520architectural%2520floorplan%2520design%2520is%2520vital%2520for%2520housing%2520and%2520interior%250Adesign%252C%2520offering%2520a%2520faster%252C%2520cost-effective%2520alternative%2520to%2520manual%2520sketches%2520by%250Aarchitects.%2520However%252C%2520existing%2520methods%252C%2520including%2520rule-based%2520and%2520learning-based%250Aapproaches%252C%2520face%2520challenges%2520in%2520design%2520complexity%2520and%2520constrained%2520generation%250Awith%2520extensive%2520post-processing%252C%2520and%2520tend%2520to%2520obvious%2520geometric%2520inconsistencies%250Asuch%2520as%2520misalignment%252C%2520overlap%252C%2520and%2520gaps.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Agenerative%2520framework%2520for%2520vector%2520floorplan%2520design%2520via%2520structural%2520graph%250Ageneration%252C%2520called%2520GSDiff%252C%2520focusing%2520on%2520wall%2520junction%2520generation%2520and%2520wall%250Asegment%2520prediction%2520to%2520capture%2520both%2520geometric%2520and%2520semantic%2520aspects%2520of%2520structural%250Agraphs.%2520To%2520improve%2520the%2520geometric%2520rationality%2520of%2520generated%2520structural%2520graphs%252C%2520we%250Apropose%2520two%2520innovative%2520geometry%2520enhancement%2520methods.%2520In%2520wall%2520junction%250Ageneration%252C%2520we%2520propose%2520a%2520novel%2520alignment%2520loss%2520function%2520to%2520improve%2520geometric%250Aconsistency.%2520In%2520wall%2520segment%2520prediction%252C%2520we%2520propose%2520a%2520random%2520self-supervision%250Amethod%2520to%2520enhance%2520the%2520model%2527s%2520perception%2520of%2520the%2520overall%2520geometric%2520structure%252C%250Athereby%2520promoting%2520the%2520generation%2520of%2520reasonable%2520geometric%2520structures.%2520Employing%250Athe%2520diffusion%2520model%2520and%2520the%2520Transformer%2520model%252C%2520as%2520well%2520as%2520the%2520geometry%250Aenhancement%2520strategies%252C%2520our%2520framework%2520can%2520generate%2520wall%2520junctions%252C%2520wall%250Asegments%2520and%2520room%2520polygons%2520with%2520structural%2520and%2520semantic%2520information%252C%2520resulting%250Ain%2520structural%2520graphs%2520that%2520accurately%2520represent%2520floorplans.%2520Extensive%250Aexperiments%2520show%2520that%2520the%2520proposed%2520method%2520surpasses%2520existing%2520techniques%252C%250Aenabling%2520free%2520generation%2520and%2520constrained%2520generation%252C%2520marking%2520a%2520shift%2520towards%250Astructure%2520generation%2520in%2520architectural%2520design.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/SizheHu/GSDiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSDiff%3A%20Synthesizing%20Vector%20Floorplans%20via%20Geometry-enhanced%20Structural%0A%20%20Graph%20Generation&entry.906535625=Sizhe%20Hu%20and%20Wenming%20Wu%20and%20Yuntao%20Wang%20and%20Benzhu%20Xu%20and%20Liping%20Zheng&entry.1292438233=%20%20Automating%20architectural%20floorplan%20design%20is%20vital%20for%20housing%20and%20interior%0Adesign%2C%20offering%20a%20faster%2C%20cost-effective%20alternative%20to%20manual%20sketches%20by%0Aarchitects.%20However%2C%20existing%20methods%2C%20including%20rule-based%20and%20learning-based%0Aapproaches%2C%20face%20challenges%20in%20design%20complexity%20and%20constrained%20generation%0Awith%20extensive%20post-processing%2C%20and%20tend%20to%20obvious%20geometric%20inconsistencies%0Asuch%20as%20misalignment%2C%20overlap%2C%20and%20gaps.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Agenerative%20framework%20for%20vector%20floorplan%20design%20via%20structural%20graph%0Ageneration%2C%20called%20GSDiff%2C%20focusing%20on%20wall%20junction%20generation%20and%20wall%0Asegment%20prediction%20to%20capture%20both%20geometric%20and%20semantic%20aspects%20of%20structural%0Agraphs.%20To%20improve%20the%20geometric%20rationality%20of%20generated%20structural%20graphs%2C%20we%0Apropose%20two%20innovative%20geometry%20enhancement%20methods.%20In%20wall%20junction%0Ageneration%2C%20we%20propose%20a%20novel%20alignment%20loss%20function%20to%20improve%20geometric%0Aconsistency.%20In%20wall%20segment%20prediction%2C%20we%20propose%20a%20random%20self-supervision%0Amethod%20to%20enhance%20the%20model%27s%20perception%20of%20the%20overall%20geometric%20structure%2C%0Athereby%20promoting%20the%20generation%20of%20reasonable%20geometric%20structures.%20Employing%0Athe%20diffusion%20model%20and%20the%20Transformer%20model%2C%20as%20well%20as%20the%20geometry%0Aenhancement%20strategies%2C%20our%20framework%20can%20generate%20wall%20junctions%2C%20wall%0Asegments%20and%20room%20polygons%20with%20structural%20and%20semantic%20information%2C%20resulting%0Ain%20structural%20graphs%20that%20accurately%20represent%20floorplans.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20method%20surpasses%20existing%20techniques%2C%0Aenabling%20free%20generation%20and%20constrained%20generation%2C%20marking%20a%20shift%20towards%0Astructure%20generation%20in%20architectural%20design.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/SizheHu/GSDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16258v2&entry.124074799=Read"},
{"title": "CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's\n  Eye View Perception", "author": "Senkang Hu and Yihang Tao and Guowen Xu and Yiqin Deng and Xianhao Chen and Yuguang Fang and Sam Kwong", "abstract": "  Collaborative Perception (CP) has shown a promising technique for autonomous\ndriving, where multiple connected and autonomous vehicles (CAVs) share their\nperception information to enhance the overall perception performance and expand\nthe perception range. However, in CP, ego CAV needs to receive messages from\nits collaborators, which makes it easy to be attacked by malicious agents. For\nexample, a malicious agent can send harmful information to the ego CAV to\nmislead it. To address this critical issue, we propose a novel method,\n\\textbf{CP-Guard}, a tailored defense mechanism for CP that can be deployed by\neach agent to accurately detect and eliminate malicious agents in its\ncollaboration network. Our key idea is to enable CP to reach a consensus rather\nthan a conflict against the ego CAV's perception results. Based on this idea,\nwe first develop a probability-agnostic sample consensus (PASAC) method to\neffectively sample a subset of the collaborators and verify the consensus\nwithout prior probabilities of malicious agents. Furthermore, we define a\ncollaborative consistency loss (CCLoss) to capture the discrepancy between the\nego CAV and its collaborators, which is used as a verification criterion for\nconsensus. Finally, we conduct extensive experiments in collaborative bird's\neye view (BEV) tasks and our results demonstrate the effectiveness of our\nCP-Guard.\n", "link": "http://arxiv.org/abs/2412.12000v1", "date": "2024-12-16", "relevancy": 2.1255, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5676}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5105}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CP-Guard%3A%20Malicious%20Agent%20Detection%20and%20Defense%20in%20Collaborative%20Bird%27s%0A%20%20Eye%20View%20Perception&body=Title%3A%20CP-Guard%3A%20Malicious%20Agent%20Detection%20and%20Defense%20in%20Collaborative%20Bird%27s%0A%20%20Eye%20View%20Perception%0AAuthor%3A%20Senkang%20Hu%20and%20Yihang%20Tao%20and%20Guowen%20Xu%20and%20Yiqin%20Deng%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20Collaborative%20Perception%20%28CP%29%20has%20shown%20a%20promising%20technique%20for%20autonomous%0Adriving%2C%20where%20multiple%20connected%20and%20autonomous%20vehicles%20%28CAVs%29%20share%20their%0Aperception%20information%20to%20enhance%20the%20overall%20perception%20performance%20and%20expand%0Athe%20perception%20range.%20However%2C%20in%20CP%2C%20ego%20CAV%20needs%20to%20receive%20messages%20from%0Aits%20collaborators%2C%20which%20makes%20it%20easy%20to%20be%20attacked%20by%20malicious%20agents.%20For%0Aexample%2C%20a%20malicious%20agent%20can%20send%20harmful%20information%20to%20the%20ego%20CAV%20to%0Amislead%20it.%20To%20address%20this%20critical%20issue%2C%20we%20propose%20a%20novel%20method%2C%0A%5Ctextbf%7BCP-Guard%7D%2C%20a%20tailored%20defense%20mechanism%20for%20CP%20that%20can%20be%20deployed%20by%0Aeach%20agent%20to%20accurately%20detect%20and%20eliminate%20malicious%20agents%20in%20its%0Acollaboration%20network.%20Our%20key%20idea%20is%20to%20enable%20CP%20to%20reach%20a%20consensus%20rather%0Athan%20a%20conflict%20against%20the%20ego%20CAV%27s%20perception%20results.%20Based%20on%20this%20idea%2C%0Awe%20first%20develop%20a%20probability-agnostic%20sample%20consensus%20%28PASAC%29%20method%20to%0Aeffectively%20sample%20a%20subset%20of%20the%20collaborators%20and%20verify%20the%20consensus%0Awithout%20prior%20probabilities%20of%20malicious%20agents.%20Furthermore%2C%20we%20define%20a%0Acollaborative%20consistency%20loss%20%28CCLoss%29%20to%20capture%20the%20discrepancy%20between%20the%0Aego%20CAV%20and%20its%20collaborators%2C%20which%20is%20used%20as%20a%20verification%20criterion%20for%0Aconsensus.%20Finally%2C%20we%20conduct%20extensive%20experiments%20in%20collaborative%20bird%27s%0Aeye%20view%20%28BEV%29%20tasks%20and%20our%20results%20demonstrate%20the%20effectiveness%20of%20our%0ACP-Guard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCP-Guard%253A%2520Malicious%2520Agent%2520Detection%2520and%2520Defense%2520in%2520Collaborative%2520Bird%2527s%250A%2520%2520Eye%2520View%2520Perception%26entry.906535625%3DSenkang%2520Hu%2520and%2520Yihang%2520Tao%2520and%2520Guowen%2520Xu%2520and%2520Yiqin%2520Deng%2520and%2520Xianhao%2520Chen%2520and%2520Yuguang%2520Fang%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520Collaborative%2520Perception%2520%2528CP%2529%2520has%2520shown%2520a%2520promising%2520technique%2520for%2520autonomous%250Adriving%252C%2520where%2520multiple%2520connected%2520and%2520autonomous%2520vehicles%2520%2528CAVs%2529%2520share%2520their%250Aperception%2520information%2520to%2520enhance%2520the%2520overall%2520perception%2520performance%2520and%2520expand%250Athe%2520perception%2520range.%2520However%252C%2520in%2520CP%252C%2520ego%2520CAV%2520needs%2520to%2520receive%2520messages%2520from%250Aits%2520collaborators%252C%2520which%2520makes%2520it%2520easy%2520to%2520be%2520attacked%2520by%2520malicious%2520agents.%2520For%250Aexample%252C%2520a%2520malicious%2520agent%2520can%2520send%2520harmful%2520information%2520to%2520the%2520ego%2520CAV%2520to%250Amislead%2520it.%2520To%2520address%2520this%2520critical%2520issue%252C%2520we%2520propose%2520a%2520novel%2520method%252C%250A%255Ctextbf%257BCP-Guard%257D%252C%2520a%2520tailored%2520defense%2520mechanism%2520for%2520CP%2520that%2520can%2520be%2520deployed%2520by%250Aeach%2520agent%2520to%2520accurately%2520detect%2520and%2520eliminate%2520malicious%2520agents%2520in%2520its%250Acollaboration%2520network.%2520Our%2520key%2520idea%2520is%2520to%2520enable%2520CP%2520to%2520reach%2520a%2520consensus%2520rather%250Athan%2520a%2520conflict%2520against%2520the%2520ego%2520CAV%2527s%2520perception%2520results.%2520Based%2520on%2520this%2520idea%252C%250Awe%2520first%2520develop%2520a%2520probability-agnostic%2520sample%2520consensus%2520%2528PASAC%2529%2520method%2520to%250Aeffectively%2520sample%2520a%2520subset%2520of%2520the%2520collaborators%2520and%2520verify%2520the%2520consensus%250Awithout%2520prior%2520probabilities%2520of%2520malicious%2520agents.%2520Furthermore%252C%2520we%2520define%2520a%250Acollaborative%2520consistency%2520loss%2520%2528CCLoss%2529%2520to%2520capture%2520the%2520discrepancy%2520between%2520the%250Aego%2520CAV%2520and%2520its%2520collaborators%252C%2520which%2520is%2520used%2520as%2520a%2520verification%2520criterion%2520for%250Aconsensus.%2520Finally%252C%2520we%2520conduct%2520extensive%2520experiments%2520in%2520collaborative%2520bird%2527s%250Aeye%2520view%2520%2528BEV%2529%2520tasks%2520and%2520our%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250ACP-Guard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CP-Guard%3A%20Malicious%20Agent%20Detection%20and%20Defense%20in%20Collaborative%20Bird%27s%0A%20%20Eye%20View%20Perception&entry.906535625=Senkang%20Hu%20and%20Yihang%20Tao%20and%20Guowen%20Xu%20and%20Yiqin%20Deng%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang%20and%20Sam%20Kwong&entry.1292438233=%20%20Collaborative%20Perception%20%28CP%29%20has%20shown%20a%20promising%20technique%20for%20autonomous%0Adriving%2C%20where%20multiple%20connected%20and%20autonomous%20vehicles%20%28CAVs%29%20share%20their%0Aperception%20information%20to%20enhance%20the%20overall%20perception%20performance%20and%20expand%0Athe%20perception%20range.%20However%2C%20in%20CP%2C%20ego%20CAV%20needs%20to%20receive%20messages%20from%0Aits%20collaborators%2C%20which%20makes%20it%20easy%20to%20be%20attacked%20by%20malicious%20agents.%20For%0Aexample%2C%20a%20malicious%20agent%20can%20send%20harmful%20information%20to%20the%20ego%20CAV%20to%0Amislead%20it.%20To%20address%20this%20critical%20issue%2C%20we%20propose%20a%20novel%20method%2C%0A%5Ctextbf%7BCP-Guard%7D%2C%20a%20tailored%20defense%20mechanism%20for%20CP%20that%20can%20be%20deployed%20by%0Aeach%20agent%20to%20accurately%20detect%20and%20eliminate%20malicious%20agents%20in%20its%0Acollaboration%20network.%20Our%20key%20idea%20is%20to%20enable%20CP%20to%20reach%20a%20consensus%20rather%0Athan%20a%20conflict%20against%20the%20ego%20CAV%27s%20perception%20results.%20Based%20on%20this%20idea%2C%0Awe%20first%20develop%20a%20probability-agnostic%20sample%20consensus%20%28PASAC%29%20method%20to%0Aeffectively%20sample%20a%20subset%20of%20the%20collaborators%20and%20verify%20the%20consensus%0Awithout%20prior%20probabilities%20of%20malicious%20agents.%20Furthermore%2C%20we%20define%20a%0Acollaborative%20consistency%20loss%20%28CCLoss%29%20to%20capture%20the%20discrepancy%20between%20the%0Aego%20CAV%20and%20its%20collaborators%2C%20which%20is%20used%20as%20a%20verification%20criterion%20for%0Aconsensus.%20Finally%2C%20we%20conduct%20extensive%20experiments%20in%20collaborative%20bird%27s%0Aeye%20view%20%28BEV%29%20tasks%20and%20our%20results%20demonstrate%20the%20effectiveness%20of%20our%0ACP-Guard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12000v1&entry.124074799=Read"},
{"title": "Thermodynamics-informed graph neural networks for real-time simulation\n  of digital human twins", "author": "Lucas Tes\u00e1n and David Gonz\u00e1lez and Pedro Martins and El\u00edas Cueto", "abstract": "  The growing importance of real-time simulation in the medical field has\nexposed the limitations and bottlenecks inherent in the digital representation\nof complex biological systems. This paper presents a novel methodology aimed at\nadvancing current lines of research in soft tissue simulation. The proposed\napproach introduces a hybrid model that integrates the geometric bias of graph\nneural networks with the physical bias derived from the imposition of a\nmetriplectic structure as soft and hard constrains in the architecture, being\nable to simulate hepatic tissue with dissipative properties. This approach\nprovides an efficient solution capable of generating predictions at high\nfeedback rate while maintaining a remarkable generalization ability for\npreviously unseen anatomies. This makes these features particularly relevant in\nthe context of precision medicine and haptic rendering.\n  Based on the adopted methodologies, we propose a model that predicts human\nliver responses to traction and compression loads in as little as 7.3\nmilliseconds for optimized configurations and as fast as 1.65 milliseconds in\nthe most efficient cases, all in the forward pass. The model achieves relative\nposition errors below 0.15\\%, with stress tensor and velocity estimations\nmaintaining relative errors under 7\\%. This demonstrates the robustness of the\napproach developed, which is capable of handling diverse load states and\nanatomies effectively. This work highlights the feasibility of integrating\nreal-time simulation with patient-specific geometries through deep learning,\npaving the way for more robust digital human twins in medical applications.\n", "link": "http://arxiv.org/abs/2412.12034v1", "date": "2024-12-16", "relevancy": 2.1043, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5296}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5285}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thermodynamics-informed%20graph%20neural%20networks%20for%20real-time%20simulation%0A%20%20of%20digital%20human%20twins&body=Title%3A%20Thermodynamics-informed%20graph%20neural%20networks%20for%20real-time%20simulation%0A%20%20of%20digital%20human%20twins%0AAuthor%3A%20Lucas%20Tes%C3%A1n%20and%20David%20Gonz%C3%A1lez%20and%20Pedro%20Martins%20and%20El%C3%ADas%20Cueto%0AAbstract%3A%20%20%20The%20growing%20importance%20of%20real-time%20simulation%20in%20the%20medical%20field%20has%0Aexposed%20the%20limitations%20and%20bottlenecks%20inherent%20in%20the%20digital%20representation%0Aof%20complex%20biological%20systems.%20This%20paper%20presents%20a%20novel%20methodology%20aimed%20at%0Aadvancing%20current%20lines%20of%20research%20in%20soft%20tissue%20simulation.%20The%20proposed%0Aapproach%20introduces%20a%20hybrid%20model%20that%20integrates%20the%20geometric%20bias%20of%20graph%0Aneural%20networks%20with%20the%20physical%20bias%20derived%20from%20the%20imposition%20of%20a%0Ametriplectic%20structure%20as%20soft%20and%20hard%20constrains%20in%20the%20architecture%2C%20being%0Aable%20to%20simulate%20hepatic%20tissue%20with%20dissipative%20properties.%20This%20approach%0Aprovides%20an%20efficient%20solution%20capable%20of%20generating%20predictions%20at%20high%0Afeedback%20rate%20while%20maintaining%20a%20remarkable%20generalization%20ability%20for%0Apreviously%20unseen%20anatomies.%20This%20makes%20these%20features%20particularly%20relevant%20in%0Athe%20context%20of%20precision%20medicine%20and%20haptic%20rendering.%0A%20%20Based%20on%20the%20adopted%20methodologies%2C%20we%20propose%20a%20model%20that%20predicts%20human%0Aliver%20responses%20to%20traction%20and%20compression%20loads%20in%20as%20little%20as%207.3%0Amilliseconds%20for%20optimized%20configurations%20and%20as%20fast%20as%201.65%20milliseconds%20in%0Athe%20most%20efficient%20cases%2C%20all%20in%20the%20forward%20pass.%20The%20model%20achieves%20relative%0Aposition%20errors%20below%200.15%5C%25%2C%20with%20stress%20tensor%20and%20velocity%20estimations%0Amaintaining%20relative%20errors%20under%207%5C%25.%20This%20demonstrates%20the%20robustness%20of%20the%0Aapproach%20developed%2C%20which%20is%20capable%20of%20handling%20diverse%20load%20states%20and%0Aanatomies%20effectively.%20This%20work%20highlights%20the%20feasibility%20of%20integrating%0Areal-time%20simulation%20with%20patient-specific%20geometries%20through%20deep%20learning%2C%0Apaving%20the%20way%20for%20more%20robust%20digital%20human%20twins%20in%20medical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermodynamics-informed%2520graph%2520neural%2520networks%2520for%2520real-time%2520simulation%250A%2520%2520of%2520digital%2520human%2520twins%26entry.906535625%3DLucas%2520Tes%25C3%25A1n%2520and%2520David%2520Gonz%25C3%25A1lez%2520and%2520Pedro%2520Martins%2520and%2520El%25C3%25ADas%2520Cueto%26entry.1292438233%3D%2520%2520The%2520growing%2520importance%2520of%2520real-time%2520simulation%2520in%2520the%2520medical%2520field%2520has%250Aexposed%2520the%2520limitations%2520and%2520bottlenecks%2520inherent%2520in%2520the%2520digital%2520representation%250Aof%2520complex%2520biological%2520systems.%2520This%2520paper%2520presents%2520a%2520novel%2520methodology%2520aimed%2520at%250Aadvancing%2520current%2520lines%2520of%2520research%2520in%2520soft%2520tissue%2520simulation.%2520The%2520proposed%250Aapproach%2520introduces%2520a%2520hybrid%2520model%2520that%2520integrates%2520the%2520geometric%2520bias%2520of%2520graph%250Aneural%2520networks%2520with%2520the%2520physical%2520bias%2520derived%2520from%2520the%2520imposition%2520of%2520a%250Ametriplectic%2520structure%2520as%2520soft%2520and%2520hard%2520constrains%2520in%2520the%2520architecture%252C%2520being%250Aable%2520to%2520simulate%2520hepatic%2520tissue%2520with%2520dissipative%2520properties.%2520This%2520approach%250Aprovides%2520an%2520efficient%2520solution%2520capable%2520of%2520generating%2520predictions%2520at%2520high%250Afeedback%2520rate%2520while%2520maintaining%2520a%2520remarkable%2520generalization%2520ability%2520for%250Apreviously%2520unseen%2520anatomies.%2520This%2520makes%2520these%2520features%2520particularly%2520relevant%2520in%250Athe%2520context%2520of%2520precision%2520medicine%2520and%2520haptic%2520rendering.%250A%2520%2520Based%2520on%2520the%2520adopted%2520methodologies%252C%2520we%2520propose%2520a%2520model%2520that%2520predicts%2520human%250Aliver%2520responses%2520to%2520traction%2520and%2520compression%2520loads%2520in%2520as%2520little%2520as%25207.3%250Amilliseconds%2520for%2520optimized%2520configurations%2520and%2520as%2520fast%2520as%25201.65%2520milliseconds%2520in%250Athe%2520most%2520efficient%2520cases%252C%2520all%2520in%2520the%2520forward%2520pass.%2520The%2520model%2520achieves%2520relative%250Aposition%2520errors%2520below%25200.15%255C%2525%252C%2520with%2520stress%2520tensor%2520and%2520velocity%2520estimations%250Amaintaining%2520relative%2520errors%2520under%25207%255C%2525.%2520This%2520demonstrates%2520the%2520robustness%2520of%2520the%250Aapproach%2520developed%252C%2520which%2520is%2520capable%2520of%2520handling%2520diverse%2520load%2520states%2520and%250Aanatomies%2520effectively.%2520This%2520work%2520highlights%2520the%2520feasibility%2520of%2520integrating%250Areal-time%2520simulation%2520with%2520patient-specific%2520geometries%2520through%2520deep%2520learning%252C%250Apaving%2520the%2520way%2520for%2520more%2520robust%2520digital%2520human%2520twins%2520in%2520medical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thermodynamics-informed%20graph%20neural%20networks%20for%20real-time%20simulation%0A%20%20of%20digital%20human%20twins&entry.906535625=Lucas%20Tes%C3%A1n%20and%20David%20Gonz%C3%A1lez%20and%20Pedro%20Martins%20and%20El%C3%ADas%20Cueto&entry.1292438233=%20%20The%20growing%20importance%20of%20real-time%20simulation%20in%20the%20medical%20field%20has%0Aexposed%20the%20limitations%20and%20bottlenecks%20inherent%20in%20the%20digital%20representation%0Aof%20complex%20biological%20systems.%20This%20paper%20presents%20a%20novel%20methodology%20aimed%20at%0Aadvancing%20current%20lines%20of%20research%20in%20soft%20tissue%20simulation.%20The%20proposed%0Aapproach%20introduces%20a%20hybrid%20model%20that%20integrates%20the%20geometric%20bias%20of%20graph%0Aneural%20networks%20with%20the%20physical%20bias%20derived%20from%20the%20imposition%20of%20a%0Ametriplectic%20structure%20as%20soft%20and%20hard%20constrains%20in%20the%20architecture%2C%20being%0Aable%20to%20simulate%20hepatic%20tissue%20with%20dissipative%20properties.%20This%20approach%0Aprovides%20an%20efficient%20solution%20capable%20of%20generating%20predictions%20at%20high%0Afeedback%20rate%20while%20maintaining%20a%20remarkable%20generalization%20ability%20for%0Apreviously%20unseen%20anatomies.%20This%20makes%20these%20features%20particularly%20relevant%20in%0Athe%20context%20of%20precision%20medicine%20and%20haptic%20rendering.%0A%20%20Based%20on%20the%20adopted%20methodologies%2C%20we%20propose%20a%20model%20that%20predicts%20human%0Aliver%20responses%20to%20traction%20and%20compression%20loads%20in%20as%20little%20as%207.3%0Amilliseconds%20for%20optimized%20configurations%20and%20as%20fast%20as%201.65%20milliseconds%20in%0Athe%20most%20efficient%20cases%2C%20all%20in%20the%20forward%20pass.%20The%20model%20achieves%20relative%0Aposition%20errors%20below%200.15%5C%25%2C%20with%20stress%20tensor%20and%20velocity%20estimations%0Amaintaining%20relative%20errors%20under%207%5C%25.%20This%20demonstrates%20the%20robustness%20of%20the%0Aapproach%20developed%2C%20which%20is%20capable%20of%20handling%20diverse%20load%20states%20and%0Aanatomies%20effectively.%20This%20work%20highlights%20the%20feasibility%20of%20integrating%0Areal-time%20simulation%20with%20patient-specific%20geometries%20through%20deep%20learning%2C%0Apaving%20the%20way%20for%20more%20robust%20digital%20human%20twins%20in%20medical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12034v1&entry.124074799=Read"},
{"title": "Exploring Semantic Consistency and Style Diversity for Domain\n  Generalized Semantic Segmentation", "author": "Hongwei Niu and Linhuang Xie and Jianghang Lin and Shengchuan Zhang", "abstract": "  Domain Generalized Semantic Segmentation (DGSS) seeks to utilize source\ndomain data exclusively to enhance the generalization of semantic segmentation\nacross unknown target domains. Prevailing studies predominantly concentrate on\nfeature normalization and domain randomization, these approaches exhibit\nsignificant limitations. Feature normalization-based methods tend to confuse\nsemantic features in the process of constraining the feature space\ndistribution, resulting in classification misjudgment. Domain\nrandomization-based methods frequently incorporate domain-irrelevant noise due\nto the uncontrollability of style transformations, resulting in segmentation\nambiguity. To address these challenges, we introduce a novel framework, named\nSCSD for Semantic Consistency prediction and Style Diversity generalization. It\ncomprises three pivotal components: Firstly, a Semantic Query Booster is\ndesigned to enhance the semantic awareness and discrimination capabilities of\nobject queries in the mask decoder, enabling cross-domain semantic consistency\nprediction. Secondly, we develop a Text-Driven Style Transform module that\nutilizes domain difference text embeddings to controllably guide the style\ntransformation of image features, thereby increasing inter-domain style\ndiversity. Lastly, to prevent the collapse of similar domain feature spaces, we\nintroduce a Style Synergy Optimization mechanism that fortifies the separation\nof inter-domain features and the aggregation of intra-domain features by\nsynergistically weighting style contrastive loss and style aggregation loss.\nExtensive experiments demonstrate that the proposed SCSD significantly\noutperforms existing state-of-theart methods. Notably, SCSD trained on GTAV\nachieved an average of 49.11 mIoU on the four unseen domain datasets,\nsurpassing the previous state-of-the-art method by +4.08 mIoU. Code is\navailable at https://github.com/nhw649/SCSD.\n", "link": "http://arxiv.org/abs/2412.12050v1", "date": "2024-12-16", "relevancy": 2.0887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Semantic%20Consistency%20and%20Style%20Diversity%20for%20Domain%0A%20%20Generalized%20Semantic%20Segmentation&body=Title%3A%20Exploring%20Semantic%20Consistency%20and%20Style%20Diversity%20for%20Domain%0A%20%20Generalized%20Semantic%20Segmentation%0AAuthor%3A%20Hongwei%20Niu%20and%20Linhuang%20Xie%20and%20Jianghang%20Lin%20and%20Shengchuan%20Zhang%0AAbstract%3A%20%20%20Domain%20Generalized%20Semantic%20Segmentation%20%28DGSS%29%20seeks%20to%20utilize%20source%0Adomain%20data%20exclusively%20to%20enhance%20the%20generalization%20of%20semantic%20segmentation%0Aacross%20unknown%20target%20domains.%20Prevailing%20studies%20predominantly%20concentrate%20on%0Afeature%20normalization%20and%20domain%20randomization%2C%20these%20approaches%20exhibit%0Asignificant%20limitations.%20Feature%20normalization-based%20methods%20tend%20to%20confuse%0Asemantic%20features%20in%20the%20process%20of%20constraining%20the%20feature%20space%0Adistribution%2C%20resulting%20in%20classification%20misjudgment.%20Domain%0Arandomization-based%20methods%20frequently%20incorporate%20domain-irrelevant%20noise%20due%0Ato%20the%20uncontrollability%20of%20style%20transformations%2C%20resulting%20in%20segmentation%0Aambiguity.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20framework%2C%20named%0ASCSD%20for%20Semantic%20Consistency%20prediction%20and%20Style%20Diversity%20generalization.%20It%0Acomprises%20three%20pivotal%20components%3A%20Firstly%2C%20a%20Semantic%20Query%20Booster%20is%0Adesigned%20to%20enhance%20the%20semantic%20awareness%20and%20discrimination%20capabilities%20of%0Aobject%20queries%20in%20the%20mask%20decoder%2C%20enabling%20cross-domain%20semantic%20consistency%0Aprediction.%20Secondly%2C%20we%20develop%20a%20Text-Driven%20Style%20Transform%20module%20that%0Autilizes%20domain%20difference%20text%20embeddings%20to%20controllably%20guide%20the%20style%0Atransformation%20of%20image%20features%2C%20thereby%20increasing%20inter-domain%20style%0Adiversity.%20Lastly%2C%20to%20prevent%20the%20collapse%20of%20similar%20domain%20feature%20spaces%2C%20we%0Aintroduce%20a%20Style%20Synergy%20Optimization%20mechanism%20that%20fortifies%20the%20separation%0Aof%20inter-domain%20features%20and%20the%20aggregation%20of%20intra-domain%20features%20by%0Asynergistically%20weighting%20style%20contrastive%20loss%20and%20style%20aggregation%20loss.%0AExtensive%20experiments%20demonstrate%20that%20the%20proposed%20SCSD%20significantly%0Aoutperforms%20existing%20state-of-theart%20methods.%20Notably%2C%20SCSD%20trained%20on%20GTAV%0Aachieved%20an%20average%20of%2049.11%20mIoU%20on%20the%20four%20unseen%20domain%20datasets%2C%0Asurpassing%20the%20previous%20state-of-the-art%20method%20by%20%2B4.08%20mIoU.%20Code%20is%0Aavailable%20at%20https%3A//github.com/nhw649/SCSD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Semantic%2520Consistency%2520and%2520Style%2520Diversity%2520for%2520Domain%250A%2520%2520Generalized%2520Semantic%2520Segmentation%26entry.906535625%3DHongwei%2520Niu%2520and%2520Linhuang%2520Xie%2520and%2520Jianghang%2520Lin%2520and%2520Shengchuan%2520Zhang%26entry.1292438233%3D%2520%2520Domain%2520Generalized%2520Semantic%2520Segmentation%2520%2528DGSS%2529%2520seeks%2520to%2520utilize%2520source%250Adomain%2520data%2520exclusively%2520to%2520enhance%2520the%2520generalization%2520of%2520semantic%2520segmentation%250Aacross%2520unknown%2520target%2520domains.%2520Prevailing%2520studies%2520predominantly%2520concentrate%2520on%250Afeature%2520normalization%2520and%2520domain%2520randomization%252C%2520these%2520approaches%2520exhibit%250Asignificant%2520limitations.%2520Feature%2520normalization-based%2520methods%2520tend%2520to%2520confuse%250Asemantic%2520features%2520in%2520the%2520process%2520of%2520constraining%2520the%2520feature%2520space%250Adistribution%252C%2520resulting%2520in%2520classification%2520misjudgment.%2520Domain%250Arandomization-based%2520methods%2520frequently%2520incorporate%2520domain-irrelevant%2520noise%2520due%250Ato%2520the%2520uncontrollability%2520of%2520style%2520transformations%252C%2520resulting%2520in%2520segmentation%250Aambiguity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520framework%252C%2520named%250ASCSD%2520for%2520Semantic%2520Consistency%2520prediction%2520and%2520Style%2520Diversity%2520generalization.%2520It%250Acomprises%2520three%2520pivotal%2520components%253A%2520Firstly%252C%2520a%2520Semantic%2520Query%2520Booster%2520is%250Adesigned%2520to%2520enhance%2520the%2520semantic%2520awareness%2520and%2520discrimination%2520capabilities%2520of%250Aobject%2520queries%2520in%2520the%2520mask%2520decoder%252C%2520enabling%2520cross-domain%2520semantic%2520consistency%250Aprediction.%2520Secondly%252C%2520we%2520develop%2520a%2520Text-Driven%2520Style%2520Transform%2520module%2520that%250Autilizes%2520domain%2520difference%2520text%2520embeddings%2520to%2520controllably%2520guide%2520the%2520style%250Atransformation%2520of%2520image%2520features%252C%2520thereby%2520increasing%2520inter-domain%2520style%250Adiversity.%2520Lastly%252C%2520to%2520prevent%2520the%2520collapse%2520of%2520similar%2520domain%2520feature%2520spaces%252C%2520we%250Aintroduce%2520a%2520Style%2520Synergy%2520Optimization%2520mechanism%2520that%2520fortifies%2520the%2520separation%250Aof%2520inter-domain%2520features%2520and%2520the%2520aggregation%2520of%2520intra-domain%2520features%2520by%250Asynergistically%2520weighting%2520style%2520contrastive%2520loss%2520and%2520style%2520aggregation%2520loss.%250AExtensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520SCSD%2520significantly%250Aoutperforms%2520existing%2520state-of-theart%2520methods.%2520Notably%252C%2520SCSD%2520trained%2520on%2520GTAV%250Aachieved%2520an%2520average%2520of%252049.11%2520mIoU%2520on%2520the%2520four%2520unseen%2520domain%2520datasets%252C%250Asurpassing%2520the%2520previous%2520state-of-the-art%2520method%2520by%2520%252B4.08%2520mIoU.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/nhw649/SCSD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Semantic%20Consistency%20and%20Style%20Diversity%20for%20Domain%0A%20%20Generalized%20Semantic%20Segmentation&entry.906535625=Hongwei%20Niu%20and%20Linhuang%20Xie%20and%20Jianghang%20Lin%20and%20Shengchuan%20Zhang&entry.1292438233=%20%20Domain%20Generalized%20Semantic%20Segmentation%20%28DGSS%29%20seeks%20to%20utilize%20source%0Adomain%20data%20exclusively%20to%20enhance%20the%20generalization%20of%20semantic%20segmentation%0Aacross%20unknown%20target%20domains.%20Prevailing%20studies%20predominantly%20concentrate%20on%0Afeature%20normalization%20and%20domain%20randomization%2C%20these%20approaches%20exhibit%0Asignificant%20limitations.%20Feature%20normalization-based%20methods%20tend%20to%20confuse%0Asemantic%20features%20in%20the%20process%20of%20constraining%20the%20feature%20space%0Adistribution%2C%20resulting%20in%20classification%20misjudgment.%20Domain%0Arandomization-based%20methods%20frequently%20incorporate%20domain-irrelevant%20noise%20due%0Ato%20the%20uncontrollability%20of%20style%20transformations%2C%20resulting%20in%20segmentation%0Aambiguity.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20framework%2C%20named%0ASCSD%20for%20Semantic%20Consistency%20prediction%20and%20Style%20Diversity%20generalization.%20It%0Acomprises%20three%20pivotal%20components%3A%20Firstly%2C%20a%20Semantic%20Query%20Booster%20is%0Adesigned%20to%20enhance%20the%20semantic%20awareness%20and%20discrimination%20capabilities%20of%0Aobject%20queries%20in%20the%20mask%20decoder%2C%20enabling%20cross-domain%20semantic%20consistency%0Aprediction.%20Secondly%2C%20we%20develop%20a%20Text-Driven%20Style%20Transform%20module%20that%0Autilizes%20domain%20difference%20text%20embeddings%20to%20controllably%20guide%20the%20style%0Atransformation%20of%20image%20features%2C%20thereby%20increasing%20inter-domain%20style%0Adiversity.%20Lastly%2C%20to%20prevent%20the%20collapse%20of%20similar%20domain%20feature%20spaces%2C%20we%0Aintroduce%20a%20Style%20Synergy%20Optimization%20mechanism%20that%20fortifies%20the%20separation%0Aof%20inter-domain%20features%20and%20the%20aggregation%20of%20intra-domain%20features%20by%0Asynergistically%20weighting%20style%20contrastive%20loss%20and%20style%20aggregation%20loss.%0AExtensive%20experiments%20demonstrate%20that%20the%20proposed%20SCSD%20significantly%0Aoutperforms%20existing%20state-of-theart%20methods.%20Notably%2C%20SCSD%20trained%20on%20GTAV%0Aachieved%20an%20average%20of%2049.11%20mIoU%20on%20the%20four%20unseen%20domain%20datasets%2C%0Asurpassing%20the%20previous%20state-of-the-art%20method%20by%20%2B4.08%20mIoU.%20Code%20is%0Aavailable%20at%20https%3A//github.com/nhw649/SCSD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12050v1&entry.124074799=Read"},
{"title": "LLMs for Cold-Start Cutting Plane Separator Configuration", "author": "Connor Lawless and Yingxi Li and Anders Wikum and Madeleine Udell and Ellen Vitercik", "abstract": "  Mixed integer linear programming (MILP) solvers ship with a staggering number\nof parameters that are challenging to select a priori for all but expert\noptimization users, but can have an outsized impact on the performance of the\nMILP solver. Existing machine learning (ML) approaches to configure solvers\nrequire training ML models by solving thousands of related MILP instances,\ngeneralize poorly to new problem sizes, and often require implementing complex\nML pipelines and custom solver interfaces that can be difficult to integrate\ninto existing optimization workflows. In this paper, we introduce a new\nLLM-based framework to configure which cutting plane separators to use for a\ngiven MILP problem with little to no training data based on characteristics of\nthe instance, such as a natural language description of the problem and the\nassociated LaTeX formulation. We augment these LLMs with descriptions of\ncutting plane separators available in a given solver, grounded by summarizing\nthe existing research literature on separators. While individual solver\nconfigurations have a large variance in performance, we present a novel\nensembling strategy that clusters and aggregates configurations to create a\nsmall portfolio of high-performing configurations. Our LLM-based methodology\nrequires no custom solver interface, can find a high-performing configuration\nby solving only a small number of MILPs, and can generate the configuration\nwith simple API calls that run in under a second. Numerical results show our\napproach is competitive with existing configuration approaches on a suite of\nclassic combinatorial optimization problems and real-world datasets with only a\nfraction of the training data and computation time.\n", "link": "http://arxiv.org/abs/2412.12038v1", "date": "2024-12-16", "relevancy": 2.0882, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4181}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20for%20Cold-Start%20Cutting%20Plane%20Separator%20Configuration&body=Title%3A%20LLMs%20for%20Cold-Start%20Cutting%20Plane%20Separator%20Configuration%0AAuthor%3A%20Connor%20Lawless%20and%20Yingxi%20Li%20and%20Anders%20Wikum%20and%20Madeleine%20Udell%20and%20Ellen%20Vitercik%0AAbstract%3A%20%20%20Mixed%20integer%20linear%20programming%20%28MILP%29%20solvers%20ship%20with%20a%20staggering%20number%0Aof%20parameters%20that%20are%20challenging%20to%20select%20a%20priori%20for%20all%20but%20expert%0Aoptimization%20users%2C%20but%20can%20have%20an%20outsized%20impact%20on%20the%20performance%20of%20the%0AMILP%20solver.%20Existing%20machine%20learning%20%28ML%29%20approaches%20to%20configure%20solvers%0Arequire%20training%20ML%20models%20by%20solving%20thousands%20of%20related%20MILP%20instances%2C%0Ageneralize%20poorly%20to%20new%20problem%20sizes%2C%20and%20often%20require%20implementing%20complex%0AML%20pipelines%20and%20custom%20solver%20interfaces%20that%20can%20be%20difficult%20to%20integrate%0Ainto%20existing%20optimization%20workflows.%20In%20this%20paper%2C%20we%20introduce%20a%20new%0ALLM-based%20framework%20to%20configure%20which%20cutting%20plane%20separators%20to%20use%20for%20a%0Agiven%20MILP%20problem%20with%20little%20to%20no%20training%20data%20based%20on%20characteristics%20of%0Athe%20instance%2C%20such%20as%20a%20natural%20language%20description%20of%20the%20problem%20and%20the%0Aassociated%20LaTeX%20formulation.%20We%20augment%20these%20LLMs%20with%20descriptions%20of%0Acutting%20plane%20separators%20available%20in%20a%20given%20solver%2C%20grounded%20by%20summarizing%0Athe%20existing%20research%20literature%20on%20separators.%20While%20individual%20solver%0Aconfigurations%20have%20a%20large%20variance%20in%20performance%2C%20we%20present%20a%20novel%0Aensembling%20strategy%20that%20clusters%20and%20aggregates%20configurations%20to%20create%20a%0Asmall%20portfolio%20of%20high-performing%20configurations.%20Our%20LLM-based%20methodology%0Arequires%20no%20custom%20solver%20interface%2C%20can%20find%20a%20high-performing%20configuration%0Aby%20solving%20only%20a%20small%20number%20of%20MILPs%2C%20and%20can%20generate%20the%20configuration%0Awith%20simple%20API%20calls%20that%20run%20in%20under%20a%20second.%20Numerical%20results%20show%20our%0Aapproach%20is%20competitive%20with%20existing%20configuration%20approaches%20on%20a%20suite%20of%0Aclassic%20combinatorial%20optimization%20problems%20and%20real-world%20datasets%20with%20only%20a%0Afraction%20of%20the%20training%20data%20and%20computation%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520for%2520Cold-Start%2520Cutting%2520Plane%2520Separator%2520Configuration%26entry.906535625%3DConnor%2520Lawless%2520and%2520Yingxi%2520Li%2520and%2520Anders%2520Wikum%2520and%2520Madeleine%2520Udell%2520and%2520Ellen%2520Vitercik%26entry.1292438233%3D%2520%2520Mixed%2520integer%2520linear%2520programming%2520%2528MILP%2529%2520solvers%2520ship%2520with%2520a%2520staggering%2520number%250Aof%2520parameters%2520that%2520are%2520challenging%2520to%2520select%2520a%2520priori%2520for%2520all%2520but%2520expert%250Aoptimization%2520users%252C%2520but%2520can%2520have%2520an%2520outsized%2520impact%2520on%2520the%2520performance%2520of%2520the%250AMILP%2520solver.%2520Existing%2520machine%2520learning%2520%2528ML%2529%2520approaches%2520to%2520configure%2520solvers%250Arequire%2520training%2520ML%2520models%2520by%2520solving%2520thousands%2520of%2520related%2520MILP%2520instances%252C%250Ageneralize%2520poorly%2520to%2520new%2520problem%2520sizes%252C%2520and%2520often%2520require%2520implementing%2520complex%250AML%2520pipelines%2520and%2520custom%2520solver%2520interfaces%2520that%2520can%2520be%2520difficult%2520to%2520integrate%250Ainto%2520existing%2520optimization%2520workflows.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%250ALLM-based%2520framework%2520to%2520configure%2520which%2520cutting%2520plane%2520separators%2520to%2520use%2520for%2520a%250Agiven%2520MILP%2520problem%2520with%2520little%2520to%2520no%2520training%2520data%2520based%2520on%2520characteristics%2520of%250Athe%2520instance%252C%2520such%2520as%2520a%2520natural%2520language%2520description%2520of%2520the%2520problem%2520and%2520the%250Aassociated%2520LaTeX%2520formulation.%2520We%2520augment%2520these%2520LLMs%2520with%2520descriptions%2520of%250Acutting%2520plane%2520separators%2520available%2520in%2520a%2520given%2520solver%252C%2520grounded%2520by%2520summarizing%250Athe%2520existing%2520research%2520literature%2520on%2520separators.%2520While%2520individual%2520solver%250Aconfigurations%2520have%2520a%2520large%2520variance%2520in%2520performance%252C%2520we%2520present%2520a%2520novel%250Aensembling%2520strategy%2520that%2520clusters%2520and%2520aggregates%2520configurations%2520to%2520create%2520a%250Asmall%2520portfolio%2520of%2520high-performing%2520configurations.%2520Our%2520LLM-based%2520methodology%250Arequires%2520no%2520custom%2520solver%2520interface%252C%2520can%2520find%2520a%2520high-performing%2520configuration%250Aby%2520solving%2520only%2520a%2520small%2520number%2520of%2520MILPs%252C%2520and%2520can%2520generate%2520the%2520configuration%250Awith%2520simple%2520API%2520calls%2520that%2520run%2520in%2520under%2520a%2520second.%2520Numerical%2520results%2520show%2520our%250Aapproach%2520is%2520competitive%2520with%2520existing%2520configuration%2520approaches%2520on%2520a%2520suite%2520of%250Aclassic%2520combinatorial%2520optimization%2520problems%2520and%2520real-world%2520datasets%2520with%2520only%2520a%250Afraction%2520of%2520the%2520training%2520data%2520and%2520computation%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20for%20Cold-Start%20Cutting%20Plane%20Separator%20Configuration&entry.906535625=Connor%20Lawless%20and%20Yingxi%20Li%20and%20Anders%20Wikum%20and%20Madeleine%20Udell%20and%20Ellen%20Vitercik&entry.1292438233=%20%20Mixed%20integer%20linear%20programming%20%28MILP%29%20solvers%20ship%20with%20a%20staggering%20number%0Aof%20parameters%20that%20are%20challenging%20to%20select%20a%20priori%20for%20all%20but%20expert%0Aoptimization%20users%2C%20but%20can%20have%20an%20outsized%20impact%20on%20the%20performance%20of%20the%0AMILP%20solver.%20Existing%20machine%20learning%20%28ML%29%20approaches%20to%20configure%20solvers%0Arequire%20training%20ML%20models%20by%20solving%20thousands%20of%20related%20MILP%20instances%2C%0Ageneralize%20poorly%20to%20new%20problem%20sizes%2C%20and%20often%20require%20implementing%20complex%0AML%20pipelines%20and%20custom%20solver%20interfaces%20that%20can%20be%20difficult%20to%20integrate%0Ainto%20existing%20optimization%20workflows.%20In%20this%20paper%2C%20we%20introduce%20a%20new%0ALLM-based%20framework%20to%20configure%20which%20cutting%20plane%20separators%20to%20use%20for%20a%0Agiven%20MILP%20problem%20with%20little%20to%20no%20training%20data%20based%20on%20characteristics%20of%0Athe%20instance%2C%20such%20as%20a%20natural%20language%20description%20of%20the%20problem%20and%20the%0Aassociated%20LaTeX%20formulation.%20We%20augment%20these%20LLMs%20with%20descriptions%20of%0Acutting%20plane%20separators%20available%20in%20a%20given%20solver%2C%20grounded%20by%20summarizing%0Athe%20existing%20research%20literature%20on%20separators.%20While%20individual%20solver%0Aconfigurations%20have%20a%20large%20variance%20in%20performance%2C%20we%20present%20a%20novel%0Aensembling%20strategy%20that%20clusters%20and%20aggregates%20configurations%20to%20create%20a%0Asmall%20portfolio%20of%20high-performing%20configurations.%20Our%20LLM-based%20methodology%0Arequires%20no%20custom%20solver%20interface%2C%20can%20find%20a%20high-performing%20configuration%0Aby%20solving%20only%20a%20small%20number%20of%20MILPs%2C%20and%20can%20generate%20the%20configuration%0Awith%20simple%20API%20calls%20that%20run%20in%20under%20a%20second.%20Numerical%20results%20show%20our%0Aapproach%20is%20competitive%20with%20existing%20configuration%20approaches%20on%20a%20suite%20of%0Aclassic%20combinatorial%20optimization%20problems%20and%20real-world%20datasets%20with%20only%20a%0Afraction%20of%20the%20training%20data%20and%20computation%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12038v1&entry.124074799=Read"},
{"title": "The Open Source Advantage in Large Language Models (LLMs)", "author": "Jiya Manchanda and Laura Boettcher and Matheus Westphalen and Jasser Jasser", "abstract": "  Large language models (LLMs) mark a key shift in natural language processing\n(NLP), having advanced text generation, translation, and domain-specific\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\nextensive computational resources, lead with state-of-the-art performance\ntoday. However, they face criticism for their \"black box\" nature and for\nlimiting accessibility in a manner that hinders reproducibility and equitable\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\nprioritize democratization through community-driven development and\ncomputational efficiency. These models have significantly reduced performance\ngaps, particularly in linguistic diversity and domain-specific applications,\nwhile providing accessible tools for global researchers and developers.\nNotably, both paradigms rely on foundational architectural innovations, such as\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\nby scaling effectively, while open-source models adapt to real-world\napplications in underrepresented languages and domains. Techniques like\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\nmodels to achieve competitive results despite limited resources. To be sure,\nthe tension between closed-source and open-source approaches underscores a\nbroader debate on transparency versus proprietary control in AI. Ethical\nconsiderations further highlight this divide. Closed-source systems restrict\nexternal scrutiny, while open-source models promote reproducibility and\ncollaboration but lack standardized auditing documentation frameworks to\nmitigate biases. Hybrid approaches that leverage the strengths of both\nparadigms are likely to shape the future of LLM innovation, ensuring\naccessibility, competitive technical performance, and ethical deployment.\n", "link": "http://arxiv.org/abs/2412.12004v1", "date": "2024-12-16", "relevancy": 2.0862, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Open%20Source%20Advantage%20in%20Large%20Language%20Models%20%28LLMs%29&body=Title%3A%20The%20Open%20Source%20Advantage%20in%20Large%20Language%20Models%20%28LLMs%29%0AAuthor%3A%20Jiya%20Manchanda%20and%20Laura%20Boettcher%20and%20Matheus%20Westphalen%20and%20Jasser%20Jasser%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20mark%20a%20key%20shift%20in%20natural%20language%20processing%0A%28NLP%29%2C%20having%20advanced%20text%20generation%2C%20translation%2C%20and%20domain-specific%0Areasoning.%20Closed-source%20models%20like%20GPT-4%2C%20powered%20by%20proprietary%20datasets%20and%0Aextensive%20computational%20resources%2C%20lead%20with%20state-of-the-art%20performance%0Atoday.%20However%2C%20they%20face%20criticism%20for%20their%20%22black%20box%22%20nature%20and%20for%0Alimiting%20accessibility%20in%20a%20manner%20that%20hinders%20reproducibility%20and%20equitable%0AAI%20development.%20By%20contrast%2C%20open-source%20initiatives%20like%20LLaMA%20and%20BLOOM%0Aprioritize%20democratization%20through%20community-driven%20development%20and%0Acomputational%20efficiency.%20These%20models%20have%20significantly%20reduced%20performance%0Agaps%2C%20particularly%20in%20linguistic%20diversity%20and%20domain-specific%20applications%2C%0Awhile%20providing%20accessible%20tools%20for%20global%20researchers%20and%20developers.%0ANotably%2C%20both%20paradigms%20rely%20on%20foundational%20architectural%20innovations%2C%20such%20as%0Athe%20Transformer%20framework%20by%20Vaswani%20et%20al.%20%282017%29.%20Closed-source%20models%20excel%0Aby%20scaling%20effectively%2C%20while%20open-source%20models%20adapt%20to%20real-world%0Aapplications%20in%20underrepresented%20languages%20and%20domains.%20Techniques%20like%0ALow-Rank%20Adaptation%20%28LoRA%29%20and%20instruction-tuning%20datasets%20enable%20open-source%0Amodels%20to%20achieve%20competitive%20results%20despite%20limited%20resources.%20To%20be%20sure%2C%0Athe%20tension%20between%20closed-source%20and%20open-source%20approaches%20underscores%20a%0Abroader%20debate%20on%20transparency%20versus%20proprietary%20control%20in%20AI.%20Ethical%0Aconsiderations%20further%20highlight%20this%20divide.%20Closed-source%20systems%20restrict%0Aexternal%20scrutiny%2C%20while%20open-source%20models%20promote%20reproducibility%20and%0Acollaboration%20but%20lack%20standardized%20auditing%20documentation%20frameworks%20to%0Amitigate%20biases.%20Hybrid%20approaches%20that%20leverage%20the%20strengths%20of%20both%0Aparadigms%20are%20likely%20to%20shape%20the%20future%20of%20LLM%20innovation%2C%20ensuring%0Aaccessibility%2C%20competitive%20technical%20performance%2C%20and%20ethical%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Open%2520Source%2520Advantage%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%26entry.906535625%3DJiya%2520Manchanda%2520and%2520Laura%2520Boettcher%2520and%2520Matheus%2520Westphalen%2520and%2520Jasser%2520Jasser%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520mark%2520a%2520key%2520shift%2520in%2520natural%2520language%2520processing%250A%2528NLP%2529%252C%2520having%2520advanced%2520text%2520generation%252C%2520translation%252C%2520and%2520domain-specific%250Areasoning.%2520Closed-source%2520models%2520like%2520GPT-4%252C%2520powered%2520by%2520proprietary%2520datasets%2520and%250Aextensive%2520computational%2520resources%252C%2520lead%2520with%2520state-of-the-art%2520performance%250Atoday.%2520However%252C%2520they%2520face%2520criticism%2520for%2520their%2520%2522black%2520box%2522%2520nature%2520and%2520for%250Alimiting%2520accessibility%2520in%2520a%2520manner%2520that%2520hinders%2520reproducibility%2520and%2520equitable%250AAI%2520development.%2520By%2520contrast%252C%2520open-source%2520initiatives%2520like%2520LLaMA%2520and%2520BLOOM%250Aprioritize%2520democratization%2520through%2520community-driven%2520development%2520and%250Acomputational%2520efficiency.%2520These%2520models%2520have%2520significantly%2520reduced%2520performance%250Agaps%252C%2520particularly%2520in%2520linguistic%2520diversity%2520and%2520domain-specific%2520applications%252C%250Awhile%2520providing%2520accessible%2520tools%2520for%2520global%2520researchers%2520and%2520developers.%250ANotably%252C%2520both%2520paradigms%2520rely%2520on%2520foundational%2520architectural%2520innovations%252C%2520such%2520as%250Athe%2520Transformer%2520framework%2520by%2520Vaswani%2520et%2520al.%2520%25282017%2529.%2520Closed-source%2520models%2520excel%250Aby%2520scaling%2520effectively%252C%2520while%2520open-source%2520models%2520adapt%2520to%2520real-world%250Aapplications%2520in%2520underrepresented%2520languages%2520and%2520domains.%2520Techniques%2520like%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%2520and%2520instruction-tuning%2520datasets%2520enable%2520open-source%250Amodels%2520to%2520achieve%2520competitive%2520results%2520despite%2520limited%2520resources.%2520To%2520be%2520sure%252C%250Athe%2520tension%2520between%2520closed-source%2520and%2520open-source%2520approaches%2520underscores%2520a%250Abroader%2520debate%2520on%2520transparency%2520versus%2520proprietary%2520control%2520in%2520AI.%2520Ethical%250Aconsiderations%2520further%2520highlight%2520this%2520divide.%2520Closed-source%2520systems%2520restrict%250Aexternal%2520scrutiny%252C%2520while%2520open-source%2520models%2520promote%2520reproducibility%2520and%250Acollaboration%2520but%2520lack%2520standardized%2520auditing%2520documentation%2520frameworks%2520to%250Amitigate%2520biases.%2520Hybrid%2520approaches%2520that%2520leverage%2520the%2520strengths%2520of%2520both%250Aparadigms%2520are%2520likely%2520to%2520shape%2520the%2520future%2520of%2520LLM%2520innovation%252C%2520ensuring%250Aaccessibility%252C%2520competitive%2520technical%2520performance%252C%2520and%2520ethical%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Open%20Source%20Advantage%20in%20Large%20Language%20Models%20%28LLMs%29&entry.906535625=Jiya%20Manchanda%20and%20Laura%20Boettcher%20and%20Matheus%20Westphalen%20and%20Jasser%20Jasser&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20mark%20a%20key%20shift%20in%20natural%20language%20processing%0A%28NLP%29%2C%20having%20advanced%20text%20generation%2C%20translation%2C%20and%20domain-specific%0Areasoning.%20Closed-source%20models%20like%20GPT-4%2C%20powered%20by%20proprietary%20datasets%20and%0Aextensive%20computational%20resources%2C%20lead%20with%20state-of-the-art%20performance%0Atoday.%20However%2C%20they%20face%20criticism%20for%20their%20%22black%20box%22%20nature%20and%20for%0Alimiting%20accessibility%20in%20a%20manner%20that%20hinders%20reproducibility%20and%20equitable%0AAI%20development.%20By%20contrast%2C%20open-source%20initiatives%20like%20LLaMA%20and%20BLOOM%0Aprioritize%20democratization%20through%20community-driven%20development%20and%0Acomputational%20efficiency.%20These%20models%20have%20significantly%20reduced%20performance%0Agaps%2C%20particularly%20in%20linguistic%20diversity%20and%20domain-specific%20applications%2C%0Awhile%20providing%20accessible%20tools%20for%20global%20researchers%20and%20developers.%0ANotably%2C%20both%20paradigms%20rely%20on%20foundational%20architectural%20innovations%2C%20such%20as%0Athe%20Transformer%20framework%20by%20Vaswani%20et%20al.%20%282017%29.%20Closed-source%20models%20excel%0Aby%20scaling%20effectively%2C%20while%20open-source%20models%20adapt%20to%20real-world%0Aapplications%20in%20underrepresented%20languages%20and%20domains.%20Techniques%20like%0ALow-Rank%20Adaptation%20%28LoRA%29%20and%20instruction-tuning%20datasets%20enable%20open-source%0Amodels%20to%20achieve%20competitive%20results%20despite%20limited%20resources.%20To%20be%20sure%2C%0Athe%20tension%20between%20closed-source%20and%20open-source%20approaches%20underscores%20a%0Abroader%20debate%20on%20transparency%20versus%20proprietary%20control%20in%20AI.%20Ethical%0Aconsiderations%20further%20highlight%20this%20divide.%20Closed-source%20systems%20restrict%0Aexternal%20scrutiny%2C%20while%20open-source%20models%20promote%20reproducibility%20and%0Acollaboration%20but%20lack%20standardized%20auditing%20documentation%20frameworks%20to%0Amitigate%20biases.%20Hybrid%20approaches%20that%20leverage%20the%20strengths%20of%20both%0Aparadigms%20are%20likely%20to%20shape%20the%20future%20of%20LLM%20innovation%2C%20ensuring%0Aaccessibility%2C%20competitive%20technical%20performance%2C%20and%20ethical%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12004v1&entry.124074799=Read"},
{"title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained\n  Evidence within Generation", "author": "Xiaoxi Li and Jiajie Jin and Yujia Zhou and Yongkang Wu and Zhonghua Li and Qi Ye and Zhicheng Dou", "abstract": "  Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}.\n", "link": "http://arxiv.org/abs/2412.11919v1", "date": "2024-12-16", "relevancy": 2.0696, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RetroLLM%3A%20Empowering%20Large%20Language%20Models%20to%20Retrieve%20Fine-grained%0A%20%20Evidence%20within%20Generation&body=Title%3A%20RetroLLM%3A%20Empowering%20Large%20Language%20Models%20to%20Retrieve%20Fine-grained%0A%20%20Evidence%20within%20Generation%0AAuthor%3A%20Xiaoxi%20Li%20and%20Jiajie%20Jin%20and%20Yujia%20Zhou%20and%20Yongkang%20Wu%20and%20Zhonghua%20Li%20and%20Qi%20Ye%20and%20Zhicheng%20Dou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20generative%20capabilities%20but%0Aoften%20suffer%20from%20hallucinations.%20Retrieval-augmented%20generation%20%28RAG%29%20offers%0Aan%20effective%20solution%20by%20incorporating%20external%20knowledge%2C%20but%20existing%20methods%0Astill%20face%20several%20limitations%3A%20additional%20deployment%20costs%20of%20separate%0Aretrievers%2C%20redundant%20input%20tokens%20from%20retrieved%20text%20chunks%2C%20and%20the%20lack%20of%0Ajoint%20optimization%20of%20retrieval%20and%20generation.%20To%20address%20these%20issues%2C%20we%0Apropose%20%5Ctextbf%7BRetroLLM%7D%2C%20a%20unified%20framework%20that%20integrates%20retrieval%20and%0Ageneration%20into%20a%20single%2C%20cohesive%20process%2C%20enabling%20LLMs%20to%20directly%20generate%0Afine-grained%20evidence%20from%20the%20corpus%20with%20constrained%20decoding.%20Moreover%2C%20to%0Amitigate%20false%20pruning%20in%20the%20process%20of%20constrained%20evidence%20generation%2C%20we%0Aintroduce%20%281%29%20hierarchical%20FM-Index%20constraints%2C%20which%20generate%0Acorpus-constrained%20clues%20to%20identify%20a%20subset%20of%20relevant%20documents%20before%0Aevidence%20generation%2C%20reducing%20irrelevant%20decoding%20space%3B%20and%20%282%29%20a%0Aforward-looking%20constrained%20decoding%20strategy%2C%20which%20considers%20the%20relevance%20of%0Afuture%20sequences%20to%20improve%20evidence%20accuracy.%20Extensive%20experiments%20on%20five%0Aopen-domain%20QA%20datasets%20demonstrate%20RetroLLM%27s%20superior%20performance%20across%20both%0Ain-domain%20and%20out-of-domain%20tasks.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/sunnynexus/RetroLLM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetroLLM%253A%2520Empowering%2520Large%2520Language%2520Models%2520to%2520Retrieve%2520Fine-grained%250A%2520%2520Evidence%2520within%2520Generation%26entry.906535625%3DXiaoxi%2520Li%2520and%2520Jiajie%2520Jin%2520and%2520Yujia%2520Zhou%2520and%2520Yongkang%2520Wu%2520and%2520Zhonghua%2520Li%2520and%2520Qi%2520Ye%2520and%2520Zhicheng%2520Dou%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520generative%2520capabilities%2520but%250Aoften%2520suffer%2520from%2520hallucinations.%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520offers%250Aan%2520effective%2520solution%2520by%2520incorporating%2520external%2520knowledge%252C%2520but%2520existing%2520methods%250Astill%2520face%2520several%2520limitations%253A%2520additional%2520deployment%2520costs%2520of%2520separate%250Aretrievers%252C%2520redundant%2520input%2520tokens%2520from%2520retrieved%2520text%2520chunks%252C%2520and%2520the%2520lack%2520of%250Ajoint%2520optimization%2520of%2520retrieval%2520and%2520generation.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520%255Ctextbf%257BRetroLLM%257D%252C%2520a%2520unified%2520framework%2520that%2520integrates%2520retrieval%2520and%250Ageneration%2520into%2520a%2520single%252C%2520cohesive%2520process%252C%2520enabling%2520LLMs%2520to%2520directly%2520generate%250Afine-grained%2520evidence%2520from%2520the%2520corpus%2520with%2520constrained%2520decoding.%2520Moreover%252C%2520to%250Amitigate%2520false%2520pruning%2520in%2520the%2520process%2520of%2520constrained%2520evidence%2520generation%252C%2520we%250Aintroduce%2520%25281%2529%2520hierarchical%2520FM-Index%2520constraints%252C%2520which%2520generate%250Acorpus-constrained%2520clues%2520to%2520identify%2520a%2520subset%2520of%2520relevant%2520documents%2520before%250Aevidence%2520generation%252C%2520reducing%2520irrelevant%2520decoding%2520space%253B%2520and%2520%25282%2529%2520a%250Aforward-looking%2520constrained%2520decoding%2520strategy%252C%2520which%2520considers%2520the%2520relevance%2520of%250Afuture%2520sequences%2520to%2520improve%2520evidence%2520accuracy.%2520Extensive%2520experiments%2520on%2520five%250Aopen-domain%2520QA%2520datasets%2520demonstrate%2520RetroLLM%2527s%2520superior%2520performance%2520across%2520both%250Ain-domain%2520and%2520out-of-domain%2520tasks.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/sunnynexus/RetroLLM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RetroLLM%3A%20Empowering%20Large%20Language%20Models%20to%20Retrieve%20Fine-grained%0A%20%20Evidence%20within%20Generation&entry.906535625=Xiaoxi%20Li%20and%20Jiajie%20Jin%20and%20Yujia%20Zhou%20and%20Yongkang%20Wu%20and%20Zhonghua%20Li%20and%20Qi%20Ye%20and%20Zhicheng%20Dou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20generative%20capabilities%20but%0Aoften%20suffer%20from%20hallucinations.%20Retrieval-augmented%20generation%20%28RAG%29%20offers%0Aan%20effective%20solution%20by%20incorporating%20external%20knowledge%2C%20but%20existing%20methods%0Astill%20face%20several%20limitations%3A%20additional%20deployment%20costs%20of%20separate%0Aretrievers%2C%20redundant%20input%20tokens%20from%20retrieved%20text%20chunks%2C%20and%20the%20lack%20of%0Ajoint%20optimization%20of%20retrieval%20and%20generation.%20To%20address%20these%20issues%2C%20we%0Apropose%20%5Ctextbf%7BRetroLLM%7D%2C%20a%20unified%20framework%20that%20integrates%20retrieval%20and%0Ageneration%20into%20a%20single%2C%20cohesive%20process%2C%20enabling%20LLMs%20to%20directly%20generate%0Afine-grained%20evidence%20from%20the%20corpus%20with%20constrained%20decoding.%20Moreover%2C%20to%0Amitigate%20false%20pruning%20in%20the%20process%20of%20constrained%20evidence%20generation%2C%20we%0Aintroduce%20%281%29%20hierarchical%20FM-Index%20constraints%2C%20which%20generate%0Acorpus-constrained%20clues%20to%20identify%20a%20subset%20of%20relevant%20documents%20before%0Aevidence%20generation%2C%20reducing%20irrelevant%20decoding%20space%3B%20and%20%282%29%20a%0Aforward-looking%20constrained%20decoding%20strategy%2C%20which%20considers%20the%20relevance%20of%0Afuture%20sequences%20to%20improve%20evidence%20accuracy.%20Extensive%20experiments%20on%20five%0Aopen-domain%20QA%20datasets%20demonstrate%20RetroLLM%27s%20superior%20performance%20across%20both%0Ain-domain%20and%20out-of-domain%20tasks.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/sunnynexus/RetroLLM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11919v1&entry.124074799=Read"},
{"title": "Deep-learning-based identification of individual motion characteristics\n  from upper-limb trajectories towards disorder stage evaluation", "author": "Tim Sziburis and Susanne Blex and Tobias Glasmachers and Ioannis Iossifidis", "abstract": "  The identification of individual movement characteristics sets the foundation\nfor the assessment of personal rehabilitation progress and can provide\ndiagnostic information on levels and stages of movement disorders. This work\npresents a preliminary study for differentiating individual motion patterns\nusing a dataset of 3D upper-limb transport trajectories measured in task-space.\nIdentifying individuals by deep time series learning can be a key step to\nabstracting individual motion properties. In this study, a classification\naccuracy of about 95% is reached for a subset of nine, and about 78% for the\nfull set of 31 individuals. This provides insights into the separability of\npatient attributes by exerting a simple standardized task to be transferred to\nportable systems.\n", "link": "http://arxiv.org/abs/2412.12016v1", "date": "2024-12-16", "relevancy": 2.0459, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5272}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5222}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-learning-based%20identification%20of%20individual%20motion%20characteristics%0A%20%20from%20upper-limb%20trajectories%20towards%20disorder%20stage%20evaluation&body=Title%3A%20Deep-learning-based%20identification%20of%20individual%20motion%20characteristics%0A%20%20from%20upper-limb%20trajectories%20towards%20disorder%20stage%20evaluation%0AAuthor%3A%20Tim%20Sziburis%20and%20Susanne%20Blex%20and%20Tobias%20Glasmachers%20and%20Ioannis%20Iossifidis%0AAbstract%3A%20%20%20The%20identification%20of%20individual%20movement%20characteristics%20sets%20the%20foundation%0Afor%20the%20assessment%20of%20personal%20rehabilitation%20progress%20and%20can%20provide%0Adiagnostic%20information%20on%20levels%20and%20stages%20of%20movement%20disorders.%20This%20work%0Apresents%20a%20preliminary%20study%20for%20differentiating%20individual%20motion%20patterns%0Ausing%20a%20dataset%20of%203D%20upper-limb%20transport%20trajectories%20measured%20in%20task-space.%0AIdentifying%20individuals%20by%20deep%20time%20series%20learning%20can%20be%20a%20key%20step%20to%0Aabstracting%20individual%20motion%20properties.%20In%20this%20study%2C%20a%20classification%0Aaccuracy%20of%20about%2095%25%20is%20reached%20for%20a%20subset%20of%20nine%2C%20and%20about%2078%25%20for%20the%0Afull%20set%20of%2031%20individuals.%20This%20provides%20insights%20into%20the%20separability%20of%0Apatient%20attributes%20by%20exerting%20a%20simple%20standardized%20task%20to%20be%20transferred%20to%0Aportable%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-learning-based%2520identification%2520of%2520individual%2520motion%2520characteristics%250A%2520%2520from%2520upper-limb%2520trajectories%2520towards%2520disorder%2520stage%2520evaluation%26entry.906535625%3DTim%2520Sziburis%2520and%2520Susanne%2520Blex%2520and%2520Tobias%2520Glasmachers%2520and%2520Ioannis%2520Iossifidis%26entry.1292438233%3D%2520%2520The%2520identification%2520of%2520individual%2520movement%2520characteristics%2520sets%2520the%2520foundation%250Afor%2520the%2520assessment%2520of%2520personal%2520rehabilitation%2520progress%2520and%2520can%2520provide%250Adiagnostic%2520information%2520on%2520levels%2520and%2520stages%2520of%2520movement%2520disorders.%2520This%2520work%250Apresents%2520a%2520preliminary%2520study%2520for%2520differentiating%2520individual%2520motion%2520patterns%250Ausing%2520a%2520dataset%2520of%25203D%2520upper-limb%2520transport%2520trajectories%2520measured%2520in%2520task-space.%250AIdentifying%2520individuals%2520by%2520deep%2520time%2520series%2520learning%2520can%2520be%2520a%2520key%2520step%2520to%250Aabstracting%2520individual%2520motion%2520properties.%2520In%2520this%2520study%252C%2520a%2520classification%250Aaccuracy%2520of%2520about%252095%2525%2520is%2520reached%2520for%2520a%2520subset%2520of%2520nine%252C%2520and%2520about%252078%2525%2520for%2520the%250Afull%2520set%2520of%252031%2520individuals.%2520This%2520provides%2520insights%2520into%2520the%2520separability%2520of%250Apatient%2520attributes%2520by%2520exerting%2520a%2520simple%2520standardized%2520task%2520to%2520be%2520transferred%2520to%250Aportable%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-learning-based%20identification%20of%20individual%20motion%20characteristics%0A%20%20from%20upper-limb%20trajectories%20towards%20disorder%20stage%20evaluation&entry.906535625=Tim%20Sziburis%20and%20Susanne%20Blex%20and%20Tobias%20Glasmachers%20and%20Ioannis%20Iossifidis&entry.1292438233=%20%20The%20identification%20of%20individual%20movement%20characteristics%20sets%20the%20foundation%0Afor%20the%20assessment%20of%20personal%20rehabilitation%20progress%20and%20can%20provide%0Adiagnostic%20information%20on%20levels%20and%20stages%20of%20movement%20disorders.%20This%20work%0Apresents%20a%20preliminary%20study%20for%20differentiating%20individual%20motion%20patterns%0Ausing%20a%20dataset%20of%203D%20upper-limb%20transport%20trajectories%20measured%20in%20task-space.%0AIdentifying%20individuals%20by%20deep%20time%20series%20learning%20can%20be%20a%20key%20step%20to%0Aabstracting%20individual%20motion%20properties.%20In%20this%20study%2C%20a%20classification%0Aaccuracy%20of%20about%2095%25%20is%20reached%20for%20a%20subset%20of%20nine%2C%20and%20about%2078%25%20for%20the%0Afull%20set%20of%2031%20individuals.%20This%20provides%20insights%20into%20the%20separability%20of%0Apatient%20attributes%20by%20exerting%20a%20simple%20standardized%20task%20to%20be%20transferred%20to%0Aportable%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12016v1&entry.124074799=Read"},
{"title": "SPADE: Spectroscopic Photoacoustic Denoising using an Analytical and\n  Data-free Enhancement Framework", "author": "Fangzhou Lin and Shang Gao and Yichuan Tang and Xihan Ma and Ryo Murakami and Ziming Zhang and John D. Obayemic and Winston W. Soboyejo and Haichong K. Zhang", "abstract": "  Spectroscopic photoacoustic (sPA) imaging uses multiple wavelengths to\ndifferentiate chromophores based on their unique optical absorption spectra.\nThis technique has been widely applied in areas such as vascular mapping, tumor\ndetection, and therapeutic monitoring. However, sPA imaging is highly\nsusceptible to noise, leading to poor signal-to-noise ratio (SNR) and\ncompromised image quality. Traditional denoising techniques like frame\naveraging, though effective in improving SNR, can be impractical for dynamic\nimaging scenarios due to reduced frame rates. Advanced methods, including\nlearning-based approaches and analytical algorithms, have demonstrated promise\nbut often require extensive training data and parameter tuning, limiting their\nadaptability for real-time clinical use. In this work, we propose a sPA\ndenoising using a tuning-free analytical and data-free enhancement (SPADE)\nframework for denoising sPA images. This framework integrates a data-free\nlearning-based method with an efficient BM3D-based analytical approach while\npreserves spectral linearity, providing noise reduction and ensuring that\nfunctional information is maintained. The SPADE framework was validated through\nsimulation, phantom, ex vivo, and in vivo experiments. Results demonstrated\nthat SPADE improved SNR and preserved spectral information, outperforming\nconventional methods, especially in challenging imaging conditions. SPADE\npresents a promising solution for enhancing sPA imaging quality in clinical\napplications where noise reduction and spectral preservation are critical.\n", "link": "http://arxiv.org/abs/2412.12068v1", "date": "2024-12-16", "relevancy": 2.0391, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5121}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5108}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPADE%3A%20Spectroscopic%20Photoacoustic%20Denoising%20using%20an%20Analytical%20and%0A%20%20Data-free%20Enhancement%20Framework&body=Title%3A%20SPADE%3A%20Spectroscopic%20Photoacoustic%20Denoising%20using%20an%20Analytical%20and%0A%20%20Data-free%20Enhancement%20Framework%0AAuthor%3A%20Fangzhou%20Lin%20and%20Shang%20Gao%20and%20Yichuan%20Tang%20and%20Xihan%20Ma%20and%20Ryo%20Murakami%20and%20Ziming%20Zhang%20and%20John%20D.%20Obayemic%20and%20Winston%20W.%20Soboyejo%20and%20Haichong%20K.%20Zhang%0AAbstract%3A%20%20%20Spectroscopic%20photoacoustic%20%28sPA%29%20imaging%20uses%20multiple%20wavelengths%20to%0Adifferentiate%20chromophores%20based%20on%20their%20unique%20optical%20absorption%20spectra.%0AThis%20technique%20has%20been%20widely%20applied%20in%20areas%20such%20as%20vascular%20mapping%2C%20tumor%0Adetection%2C%20and%20therapeutic%20monitoring.%20However%2C%20sPA%20imaging%20is%20highly%0Asusceptible%20to%20noise%2C%20leading%20to%20poor%20signal-to-noise%20ratio%20%28SNR%29%20and%0Acompromised%20image%20quality.%20Traditional%20denoising%20techniques%20like%20frame%0Aaveraging%2C%20though%20effective%20in%20improving%20SNR%2C%20can%20be%20impractical%20for%20dynamic%0Aimaging%20scenarios%20due%20to%20reduced%20frame%20rates.%20Advanced%20methods%2C%20including%0Alearning-based%20approaches%20and%20analytical%20algorithms%2C%20have%20demonstrated%20promise%0Abut%20often%20require%20extensive%20training%20data%20and%20parameter%20tuning%2C%20limiting%20their%0Aadaptability%20for%20real-time%20clinical%20use.%20In%20this%20work%2C%20we%20propose%20a%20sPA%0Adenoising%20using%20a%20tuning-free%20analytical%20and%20data-free%20enhancement%20%28SPADE%29%0Aframework%20for%20denoising%20sPA%20images.%20This%20framework%20integrates%20a%20data-free%0Alearning-based%20method%20with%20an%20efficient%20BM3D-based%20analytical%20approach%20while%0Apreserves%20spectral%20linearity%2C%20providing%20noise%20reduction%20and%20ensuring%20that%0Afunctional%20information%20is%20maintained.%20The%20SPADE%20framework%20was%20validated%20through%0Asimulation%2C%20phantom%2C%20ex%20vivo%2C%20and%20in%20vivo%20experiments.%20Results%20demonstrated%0Athat%20SPADE%20improved%20SNR%20and%20preserved%20spectral%20information%2C%20outperforming%0Aconventional%20methods%2C%20especially%20in%20challenging%20imaging%20conditions.%20SPADE%0Apresents%20a%20promising%20solution%20for%20enhancing%20sPA%20imaging%20quality%20in%20clinical%0Aapplications%20where%20noise%20reduction%20and%20spectral%20preservation%20are%20critical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPADE%253A%2520Spectroscopic%2520Photoacoustic%2520Denoising%2520using%2520an%2520Analytical%2520and%250A%2520%2520Data-free%2520Enhancement%2520Framework%26entry.906535625%3DFangzhou%2520Lin%2520and%2520Shang%2520Gao%2520and%2520Yichuan%2520Tang%2520and%2520Xihan%2520Ma%2520and%2520Ryo%2520Murakami%2520and%2520Ziming%2520Zhang%2520and%2520John%2520D.%2520Obayemic%2520and%2520Winston%2520W.%2520Soboyejo%2520and%2520Haichong%2520K.%2520Zhang%26entry.1292438233%3D%2520%2520Spectroscopic%2520photoacoustic%2520%2528sPA%2529%2520imaging%2520uses%2520multiple%2520wavelengths%2520to%250Adifferentiate%2520chromophores%2520based%2520on%2520their%2520unique%2520optical%2520absorption%2520spectra.%250AThis%2520technique%2520has%2520been%2520widely%2520applied%2520in%2520areas%2520such%2520as%2520vascular%2520mapping%252C%2520tumor%250Adetection%252C%2520and%2520therapeutic%2520monitoring.%2520However%252C%2520sPA%2520imaging%2520is%2520highly%250Asusceptible%2520to%2520noise%252C%2520leading%2520to%2520poor%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2520and%250Acompromised%2520image%2520quality.%2520Traditional%2520denoising%2520techniques%2520like%2520frame%250Aaveraging%252C%2520though%2520effective%2520in%2520improving%2520SNR%252C%2520can%2520be%2520impractical%2520for%2520dynamic%250Aimaging%2520scenarios%2520due%2520to%2520reduced%2520frame%2520rates.%2520Advanced%2520methods%252C%2520including%250Alearning-based%2520approaches%2520and%2520analytical%2520algorithms%252C%2520have%2520demonstrated%2520promise%250Abut%2520often%2520require%2520extensive%2520training%2520data%2520and%2520parameter%2520tuning%252C%2520limiting%2520their%250Aadaptability%2520for%2520real-time%2520clinical%2520use.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520sPA%250Adenoising%2520using%2520a%2520tuning-free%2520analytical%2520and%2520data-free%2520enhancement%2520%2528SPADE%2529%250Aframework%2520for%2520denoising%2520sPA%2520images.%2520This%2520framework%2520integrates%2520a%2520data-free%250Alearning-based%2520method%2520with%2520an%2520efficient%2520BM3D-based%2520analytical%2520approach%2520while%250Apreserves%2520spectral%2520linearity%252C%2520providing%2520noise%2520reduction%2520and%2520ensuring%2520that%250Afunctional%2520information%2520is%2520maintained.%2520The%2520SPADE%2520framework%2520was%2520validated%2520through%250Asimulation%252C%2520phantom%252C%2520ex%2520vivo%252C%2520and%2520in%2520vivo%2520experiments.%2520Results%2520demonstrated%250Athat%2520SPADE%2520improved%2520SNR%2520and%2520preserved%2520spectral%2520information%252C%2520outperforming%250Aconventional%2520methods%252C%2520especially%2520in%2520challenging%2520imaging%2520conditions.%2520SPADE%250Apresents%2520a%2520promising%2520solution%2520for%2520enhancing%2520sPA%2520imaging%2520quality%2520in%2520clinical%250Aapplications%2520where%2520noise%2520reduction%2520and%2520spectral%2520preservation%2520are%2520critical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPADE%3A%20Spectroscopic%20Photoacoustic%20Denoising%20using%20an%20Analytical%20and%0A%20%20Data-free%20Enhancement%20Framework&entry.906535625=Fangzhou%20Lin%20and%20Shang%20Gao%20and%20Yichuan%20Tang%20and%20Xihan%20Ma%20and%20Ryo%20Murakami%20and%20Ziming%20Zhang%20and%20John%20D.%20Obayemic%20and%20Winston%20W.%20Soboyejo%20and%20Haichong%20K.%20Zhang&entry.1292438233=%20%20Spectroscopic%20photoacoustic%20%28sPA%29%20imaging%20uses%20multiple%20wavelengths%20to%0Adifferentiate%20chromophores%20based%20on%20their%20unique%20optical%20absorption%20spectra.%0AThis%20technique%20has%20been%20widely%20applied%20in%20areas%20such%20as%20vascular%20mapping%2C%20tumor%0Adetection%2C%20and%20therapeutic%20monitoring.%20However%2C%20sPA%20imaging%20is%20highly%0Asusceptible%20to%20noise%2C%20leading%20to%20poor%20signal-to-noise%20ratio%20%28SNR%29%20and%0Acompromised%20image%20quality.%20Traditional%20denoising%20techniques%20like%20frame%0Aaveraging%2C%20though%20effective%20in%20improving%20SNR%2C%20can%20be%20impractical%20for%20dynamic%0Aimaging%20scenarios%20due%20to%20reduced%20frame%20rates.%20Advanced%20methods%2C%20including%0Alearning-based%20approaches%20and%20analytical%20algorithms%2C%20have%20demonstrated%20promise%0Abut%20often%20require%20extensive%20training%20data%20and%20parameter%20tuning%2C%20limiting%20their%0Aadaptability%20for%20real-time%20clinical%20use.%20In%20this%20work%2C%20we%20propose%20a%20sPA%0Adenoising%20using%20a%20tuning-free%20analytical%20and%20data-free%20enhancement%20%28SPADE%29%0Aframework%20for%20denoising%20sPA%20images.%20This%20framework%20integrates%20a%20data-free%0Alearning-based%20method%20with%20an%20efficient%20BM3D-based%20analytical%20approach%20while%0Apreserves%20spectral%20linearity%2C%20providing%20noise%20reduction%20and%20ensuring%20that%0Afunctional%20information%20is%20maintained.%20The%20SPADE%20framework%20was%20validated%20through%0Asimulation%2C%20phantom%2C%20ex%20vivo%2C%20and%20in%20vivo%20experiments.%20Results%20demonstrated%0Athat%20SPADE%20improved%20SNR%20and%20preserved%20spectral%20information%2C%20outperforming%0Aconventional%20methods%2C%20especially%20in%20challenging%20imaging%20conditions.%20SPADE%0Apresents%20a%20promising%20solution%20for%20enhancing%20sPA%20imaging%20quality%20in%20clinical%0Aapplications%20where%20noise%20reduction%20and%20spectral%20preservation%20are%20critical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12068v1&entry.124074799=Read"},
{"title": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability\n  Detection", "author": "Ira Ceka and Feitong Qiao and Anik Dey and Aastha Valechia and Gail Kaiser and Baishakhi Ray", "abstract": "  Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on applied tasks such as vulnerability detection. We\ninvestigate various prompting strategies for vulnerability detection and, as\npart of this exploration, propose a prompting strategy that integrates natural\nlanguage descriptions of vulnerabilities with a contrastive chain-of-thought\nreasoning approach, augmented using contrastive samples from a synthetic\ndataset. Our study highlights the potential of LLMs to detect vulnerabilities\nby integrating natural language descriptions, contrastive reasoning, and\nsynthetic examples into a comprehensive prompting framework. Our results show\nthat this approach can enhance LLM understanding of vulnerabilities. On a\nhigh-quality vulnerability detection dataset such as SVEN, our prompting\nstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,\n11%, and 14%, respectively.\n", "link": "http://arxiv.org/abs/2412.12039v1", "date": "2024-12-16", "relevancy": 2.0105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLM%20Prompting%20Serve%20as%20a%20Proxy%20for%20Static%20Analysis%20in%20Vulnerability%0A%20%20Detection&body=Title%3A%20Can%20LLM%20Prompting%20Serve%20as%20a%20Proxy%20for%20Static%20Analysis%20in%20Vulnerability%0A%20%20Detection%0AAuthor%3A%20Ira%20Ceka%20and%20Feitong%20Qiao%20and%20Anik%20Dey%20and%20Aastha%20Valechia%20and%20Gail%20Kaiser%20and%20Baishakhi%20Ray%0AAbstract%3A%20%20%20Despite%20their%20remarkable%20success%2C%20large%20language%20models%20%28LLMs%29%20have%20shown%0Alimited%20ability%20on%20applied%20tasks%20such%20as%20vulnerability%20detection.%20We%0Ainvestigate%20various%20prompting%20strategies%20for%20vulnerability%20detection%20and%2C%20as%0Apart%20of%20this%20exploration%2C%20propose%20a%20prompting%20strategy%20that%20integrates%20natural%0Alanguage%20descriptions%20of%20vulnerabilities%20with%20a%20contrastive%20chain-of-thought%0Areasoning%20approach%2C%20augmented%20using%20contrastive%20samples%20from%20a%20synthetic%0Adataset.%20Our%20study%20highlights%20the%20potential%20of%20LLMs%20to%20detect%20vulnerabilities%0Aby%20integrating%20natural%20language%20descriptions%2C%20contrastive%20reasoning%2C%20and%0Asynthetic%20examples%20into%20a%20comprehensive%20prompting%20framework.%20Our%20results%20show%0Athat%20this%20approach%20can%20enhance%20LLM%20understanding%20of%20vulnerabilities.%20On%20a%0Ahigh-quality%20vulnerability%20detection%20dataset%20such%20as%20SVEN%2C%20our%20prompting%0Astrategies%20can%20improve%20accuracies%2C%20F1-scores%2C%20and%20pairwise%20accuracies%20by%2023%25%2C%0A11%25%2C%20and%2014%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLM%2520Prompting%2520Serve%2520as%2520a%2520Proxy%2520for%2520Static%2520Analysis%2520in%2520Vulnerability%250A%2520%2520Detection%26entry.906535625%3DIra%2520Ceka%2520and%2520Feitong%2520Qiao%2520and%2520Anik%2520Dey%2520and%2520Aastha%2520Valechia%2520and%2520Gail%2520Kaiser%2520and%2520Baishakhi%2520Ray%26entry.1292438233%3D%2520%2520Despite%2520their%2520remarkable%2520success%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%250Alimited%2520ability%2520on%2520applied%2520tasks%2520such%2520as%2520vulnerability%2520detection.%2520We%250Ainvestigate%2520various%2520prompting%2520strategies%2520for%2520vulnerability%2520detection%2520and%252C%2520as%250Apart%2520of%2520this%2520exploration%252C%2520propose%2520a%2520prompting%2520strategy%2520that%2520integrates%2520natural%250Alanguage%2520descriptions%2520of%2520vulnerabilities%2520with%2520a%2520contrastive%2520chain-of-thought%250Areasoning%2520approach%252C%2520augmented%2520using%2520contrastive%2520samples%2520from%2520a%2520synthetic%250Adataset.%2520Our%2520study%2520highlights%2520the%2520potential%2520of%2520LLMs%2520to%2520detect%2520vulnerabilities%250Aby%2520integrating%2520natural%2520language%2520descriptions%252C%2520contrastive%2520reasoning%252C%2520and%250Asynthetic%2520examples%2520into%2520a%2520comprehensive%2520prompting%2520framework.%2520Our%2520results%2520show%250Athat%2520this%2520approach%2520can%2520enhance%2520LLM%2520understanding%2520of%2520vulnerabilities.%2520On%2520a%250Ahigh-quality%2520vulnerability%2520detection%2520dataset%2520such%2520as%2520SVEN%252C%2520our%2520prompting%250Astrategies%2520can%2520improve%2520accuracies%252C%2520F1-scores%252C%2520and%2520pairwise%2520accuracies%2520by%252023%2525%252C%250A11%2525%252C%2520and%252014%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLM%20Prompting%20Serve%20as%20a%20Proxy%20for%20Static%20Analysis%20in%20Vulnerability%0A%20%20Detection&entry.906535625=Ira%20Ceka%20and%20Feitong%20Qiao%20and%20Anik%20Dey%20and%20Aastha%20Valechia%20and%20Gail%20Kaiser%20and%20Baishakhi%20Ray&entry.1292438233=%20%20Despite%20their%20remarkable%20success%2C%20large%20language%20models%20%28LLMs%29%20have%20shown%0Alimited%20ability%20on%20applied%20tasks%20such%20as%20vulnerability%20detection.%20We%0Ainvestigate%20various%20prompting%20strategies%20for%20vulnerability%20detection%20and%2C%20as%0Apart%20of%20this%20exploration%2C%20propose%20a%20prompting%20strategy%20that%20integrates%20natural%0Alanguage%20descriptions%20of%20vulnerabilities%20with%20a%20contrastive%20chain-of-thought%0Areasoning%20approach%2C%20augmented%20using%20contrastive%20samples%20from%20a%20synthetic%0Adataset.%20Our%20study%20highlights%20the%20potential%20of%20LLMs%20to%20detect%20vulnerabilities%0Aby%20integrating%20natural%20language%20descriptions%2C%20contrastive%20reasoning%2C%20and%0Asynthetic%20examples%20into%20a%20comprehensive%20prompting%20framework.%20Our%20results%20show%0Athat%20this%20approach%20can%20enhance%20LLM%20understanding%20of%20vulnerabilities.%20On%20a%0Ahigh-quality%20vulnerability%20detection%20dataset%20such%20as%20SVEN%2C%20our%20prompting%0Astrategies%20can%20improve%20accuracies%2C%20F1-scores%2C%20and%20pairwise%20accuracies%20by%2023%25%2C%0A11%25%2C%20and%2014%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12039v1&entry.124074799=Read"},
{"title": "Merging Text Transformer Models from Different Initializations", "author": "Neha Verma and Maha Elbayad", "abstract": "  Recent work on permutation-based model merging has shown impressive low- or\nzero-barrier mode connectivity between models from completely different\ninitializations. However, this line of work has not yet extended to the\nTransformer architecture, despite its dominant popularity in the language\ndomain. Therefore, in this work, we investigate the extent to which separate\nTransformer minima learn similar features, and propose a model merging\ntechnique to investigate the relationship between these minima in the loss\nlandscape. The specifics of the architecture, like its residual connections,\nmulti-headed attention, and discrete, sequential input, require specific\ninterventions in order to compute model permutations that remain within the\nsame functional equivalence class. In merging these models with our method, we\nconsistently find lower loss barriers between minima compared to model\naveraging, across models trained on a masked-language modeling task or\nfine-tuned on a language understanding benchmark. Our results show that the\nminima of these models are less sharp and isolated than previously understood,\nand provide a basis for future work on merging separately trained Transformer\nmodels.\n", "link": "http://arxiv.org/abs/2403.00986v3", "date": "2024-12-16", "relevancy": 2.0014, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Merging%20Text%20Transformer%20Models%20from%20Different%20Initializations&body=Title%3A%20Merging%20Text%20Transformer%20Models%20from%20Different%20Initializations%0AAuthor%3A%20Neha%20Verma%20and%20Maha%20Elbayad%0AAbstract%3A%20%20%20Recent%20work%20on%20permutation-based%20model%20merging%20has%20shown%20impressive%20low-%20or%0Azero-barrier%20mode%20connectivity%20between%20models%20from%20completely%20different%0Ainitializations.%20However%2C%20this%20line%20of%20work%20has%20not%20yet%20extended%20to%20the%0ATransformer%20architecture%2C%20despite%20its%20dominant%20popularity%20in%20the%20language%0Adomain.%20Therefore%2C%20in%20this%20work%2C%20we%20investigate%20the%20extent%20to%20which%20separate%0ATransformer%20minima%20learn%20similar%20features%2C%20and%20propose%20a%20model%20merging%0Atechnique%20to%20investigate%20the%20relationship%20between%20these%20minima%20in%20the%20loss%0Alandscape.%20The%20specifics%20of%20the%20architecture%2C%20like%20its%20residual%20connections%2C%0Amulti-headed%20attention%2C%20and%20discrete%2C%20sequential%20input%2C%20require%20specific%0Ainterventions%20in%20order%20to%20compute%20model%20permutations%20that%20remain%20within%20the%0Asame%20functional%20equivalence%20class.%20In%20merging%20these%20models%20with%20our%20method%2C%20we%0Aconsistently%20find%20lower%20loss%20barriers%20between%20minima%20compared%20to%20model%0Aaveraging%2C%20across%20models%20trained%20on%20a%20masked-language%20modeling%20task%20or%0Afine-tuned%20on%20a%20language%20understanding%20benchmark.%20Our%20results%20show%20that%20the%0Aminima%20of%20these%20models%20are%20less%20sharp%20and%20isolated%20than%20previously%20understood%2C%0Aand%20provide%20a%20basis%20for%20future%20work%20on%20merging%20separately%20trained%20Transformer%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00986v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMerging%2520Text%2520Transformer%2520Models%2520from%2520Different%2520Initializations%26entry.906535625%3DNeha%2520Verma%2520and%2520Maha%2520Elbayad%26entry.1292438233%3D%2520%2520Recent%2520work%2520on%2520permutation-based%2520model%2520merging%2520has%2520shown%2520impressive%2520low-%2520or%250Azero-barrier%2520mode%2520connectivity%2520between%2520models%2520from%2520completely%2520different%250Ainitializations.%2520However%252C%2520this%2520line%2520of%2520work%2520has%2520not%2520yet%2520extended%2520to%2520the%250ATransformer%2520architecture%252C%2520despite%2520its%2520dominant%2520popularity%2520in%2520the%2520language%250Adomain.%2520Therefore%252C%2520in%2520this%2520work%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520separate%250ATransformer%2520minima%2520learn%2520similar%2520features%252C%2520and%2520propose%2520a%2520model%2520merging%250Atechnique%2520to%2520investigate%2520the%2520relationship%2520between%2520these%2520minima%2520in%2520the%2520loss%250Alandscape.%2520The%2520specifics%2520of%2520the%2520architecture%252C%2520like%2520its%2520residual%2520connections%252C%250Amulti-headed%2520attention%252C%2520and%2520discrete%252C%2520sequential%2520input%252C%2520require%2520specific%250Ainterventions%2520in%2520order%2520to%2520compute%2520model%2520permutations%2520that%2520remain%2520within%2520the%250Asame%2520functional%2520equivalence%2520class.%2520In%2520merging%2520these%2520models%2520with%2520our%2520method%252C%2520we%250Aconsistently%2520find%2520lower%2520loss%2520barriers%2520between%2520minima%2520compared%2520to%2520model%250Aaveraging%252C%2520across%2520models%2520trained%2520on%2520a%2520masked-language%2520modeling%2520task%2520or%250Afine-tuned%2520on%2520a%2520language%2520understanding%2520benchmark.%2520Our%2520results%2520show%2520that%2520the%250Aminima%2520of%2520these%2520models%2520are%2520less%2520sharp%2520and%2520isolated%2520than%2520previously%2520understood%252C%250Aand%2520provide%2520a%2520basis%2520for%2520future%2520work%2520on%2520merging%2520separately%2520trained%2520Transformer%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00986v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Merging%20Text%20Transformer%20Models%20from%20Different%20Initializations&entry.906535625=Neha%20Verma%20and%20Maha%20Elbayad&entry.1292438233=%20%20Recent%20work%20on%20permutation-based%20model%20merging%20has%20shown%20impressive%20low-%20or%0Azero-barrier%20mode%20connectivity%20between%20models%20from%20completely%20different%0Ainitializations.%20However%2C%20this%20line%20of%20work%20has%20not%20yet%20extended%20to%20the%0ATransformer%20architecture%2C%20despite%20its%20dominant%20popularity%20in%20the%20language%0Adomain.%20Therefore%2C%20in%20this%20work%2C%20we%20investigate%20the%20extent%20to%20which%20separate%0ATransformer%20minima%20learn%20similar%20features%2C%20and%20propose%20a%20model%20merging%0Atechnique%20to%20investigate%20the%20relationship%20between%20these%20minima%20in%20the%20loss%0Alandscape.%20The%20specifics%20of%20the%20architecture%2C%20like%20its%20residual%20connections%2C%0Amulti-headed%20attention%2C%20and%20discrete%2C%20sequential%20input%2C%20require%20specific%0Ainterventions%20in%20order%20to%20compute%20model%20permutations%20that%20remain%20within%20the%0Asame%20functional%20equivalence%20class.%20In%20merging%20these%20models%20with%20our%20method%2C%20we%0Aconsistently%20find%20lower%20loss%20barriers%20between%20minima%20compared%20to%20model%0Aaveraging%2C%20across%20models%20trained%20on%20a%20masked-language%20modeling%20task%20or%0Afine-tuned%20on%20a%20language%20understanding%20benchmark.%20Our%20results%20show%20that%20the%0Aminima%20of%20these%20models%20are%20less%20sharp%20and%20isolated%20than%20previously%20understood%2C%0Aand%20provide%20a%20basis%20for%20future%20work%20on%20merging%20separately%20trained%20Transformer%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00986v3&entry.124074799=Read"},
{"title": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection", "author": "Sepideh Mamooler and Syrielle Montariol and Alexander Mathis and Antoine Bosselut", "abstract": "  In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations.\n", "link": "http://arxiv.org/abs/2412.11923v1", "date": "2024-12-16", "relevancy": 1.9743, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PICLe%3A%20Pseudo-Annotations%20for%20In-Context%20Learning%20in%20Low-Resource%20Named%0A%20%20Entity%20Detection&body=Title%3A%20PICLe%3A%20Pseudo-Annotations%20for%20In-Context%20Learning%20in%20Low-Resource%20Named%0A%20%20Entity%20Detection%0AAuthor%3A%20Sepideh%20Mamooler%20and%20Syrielle%20Montariol%20and%20Alexander%20Mathis%20and%20Antoine%20Bosselut%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20enables%20Large%20Language%20Models%20%28LLMs%29%20to%20perform%0Atasks%20using%20few%20demonstrations%2C%20facilitating%20task%20adaptation%20when%20labeled%0Aexamples%20are%20hard%20to%20obtain.%20However%2C%20ICL%20is%20sensitive%20to%20the%20choice%20of%0Ademonstrations%2C%20and%20it%20remains%20unclear%20which%20demonstration%20attributes%20enable%0Ain-context%20generalization.%20In%20this%20work%2C%20we%20conduct%20a%20perturbation%20study%20of%0Ain-context%20demonstrations%20for%20low-resource%20Named%20Entity%20Detection%20%28NED%29.%20Our%0Asurprising%20finding%20is%20that%20in-context%20demonstrations%20with%20partially%20correct%0Aannotated%20entity%20mentions%20can%20be%20as%20effective%20for%20task%20transfer%20as%20fully%0Acorrect%20demonstrations.%20Based%20off%20our%20findings%2C%20we%20propose%20Pseudo-annotated%0AIn-Context%20Learning%20%28PICLe%29%2C%20a%20framework%20for%20in-context%20learning%20with%20noisy%2C%0Apseudo-annotated%20demonstrations.%20PICLe%20leverages%20LLMs%20to%20annotate%20many%0Ademonstrations%20in%20a%20zero-shot%20first%20pass.%20We%20then%20cluster%20these%20synthetic%0Ademonstrations%2C%20sample%20specific%20sets%20of%20in-context%20demonstrations%20from%20each%0Acluster%2C%20and%20predict%20entity%20mentions%20using%20each%20set%20independently.%20Finally%2C%20we%0Ause%20self-verification%20to%20select%20the%20final%20set%20of%20entity%20mentions.%20We%20evaluate%0APICLe%20on%20five%20biomedical%20NED%20datasets%20and%20show%20that%2C%20with%20zero%20human%0Aannotation%2C%20PICLe%20outperforms%20ICL%20in%20low-resource%20settings%20where%20limited%20gold%0Aexamples%20can%20be%20used%20as%20in-context%20demonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPICLe%253A%2520Pseudo-Annotations%2520for%2520In-Context%2520Learning%2520in%2520Low-Resource%2520Named%250A%2520%2520Entity%2520Detection%26entry.906535625%3DSepideh%2520Mamooler%2520and%2520Syrielle%2520Montariol%2520and%2520Alexander%2520Mathis%2520and%2520Antoine%2520Bosselut%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520enables%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520perform%250Atasks%2520using%2520few%2520demonstrations%252C%2520facilitating%2520task%2520adaptation%2520when%2520labeled%250Aexamples%2520are%2520hard%2520to%2520obtain.%2520However%252C%2520ICL%2520is%2520sensitive%2520to%2520the%2520choice%2520of%250Ademonstrations%252C%2520and%2520it%2520remains%2520unclear%2520which%2520demonstration%2520attributes%2520enable%250Ain-context%2520generalization.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520perturbation%2520study%2520of%250Ain-context%2520demonstrations%2520for%2520low-resource%2520Named%2520Entity%2520Detection%2520%2528NED%2529.%2520Our%250Asurprising%2520finding%2520is%2520that%2520in-context%2520demonstrations%2520with%2520partially%2520correct%250Aannotated%2520entity%2520mentions%2520can%2520be%2520as%2520effective%2520for%2520task%2520transfer%2520as%2520fully%250Acorrect%2520demonstrations.%2520Based%2520off%2520our%2520findings%252C%2520we%2520propose%2520Pseudo-annotated%250AIn-Context%2520Learning%2520%2528PICLe%2529%252C%2520a%2520framework%2520for%2520in-context%2520learning%2520with%2520noisy%252C%250Apseudo-annotated%2520demonstrations.%2520PICLe%2520leverages%2520LLMs%2520to%2520annotate%2520many%250Ademonstrations%2520in%2520a%2520zero-shot%2520first%2520pass.%2520We%2520then%2520cluster%2520these%2520synthetic%250Ademonstrations%252C%2520sample%2520specific%2520sets%2520of%2520in-context%2520demonstrations%2520from%2520each%250Acluster%252C%2520and%2520predict%2520entity%2520mentions%2520using%2520each%2520set%2520independently.%2520Finally%252C%2520we%250Ause%2520self-verification%2520to%2520select%2520the%2520final%2520set%2520of%2520entity%2520mentions.%2520We%2520evaluate%250APICLe%2520on%2520five%2520biomedical%2520NED%2520datasets%2520and%2520show%2520that%252C%2520with%2520zero%2520human%250Aannotation%252C%2520PICLe%2520outperforms%2520ICL%2520in%2520low-resource%2520settings%2520where%2520limited%2520gold%250Aexamples%2520can%2520be%2520used%2520as%2520in-context%2520demonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PICLe%3A%20Pseudo-Annotations%20for%20In-Context%20Learning%20in%20Low-Resource%20Named%0A%20%20Entity%20Detection&entry.906535625=Sepideh%20Mamooler%20and%20Syrielle%20Montariol%20and%20Alexander%20Mathis%20and%20Antoine%20Bosselut&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20enables%20Large%20Language%20Models%20%28LLMs%29%20to%20perform%0Atasks%20using%20few%20demonstrations%2C%20facilitating%20task%20adaptation%20when%20labeled%0Aexamples%20are%20hard%20to%20obtain.%20However%2C%20ICL%20is%20sensitive%20to%20the%20choice%20of%0Ademonstrations%2C%20and%20it%20remains%20unclear%20which%20demonstration%20attributes%20enable%0Ain-context%20generalization.%20In%20this%20work%2C%20we%20conduct%20a%20perturbation%20study%20of%0Ain-context%20demonstrations%20for%20low-resource%20Named%20Entity%20Detection%20%28NED%29.%20Our%0Asurprising%20finding%20is%20that%20in-context%20demonstrations%20with%20partially%20correct%0Aannotated%20entity%20mentions%20can%20be%20as%20effective%20for%20task%20transfer%20as%20fully%0Acorrect%20demonstrations.%20Based%20off%20our%20findings%2C%20we%20propose%20Pseudo-annotated%0AIn-Context%20Learning%20%28PICLe%29%2C%20a%20framework%20for%20in-context%20learning%20with%20noisy%2C%0Apseudo-annotated%20demonstrations.%20PICLe%20leverages%20LLMs%20to%20annotate%20many%0Ademonstrations%20in%20a%20zero-shot%20first%20pass.%20We%20then%20cluster%20these%20synthetic%0Ademonstrations%2C%20sample%20specific%20sets%20of%20in-context%20demonstrations%20from%20each%0Acluster%2C%20and%20predict%20entity%20mentions%20using%20each%20set%20independently.%20Finally%2C%20we%0Ause%20self-verification%20to%20select%20the%20final%20set%20of%20entity%20mentions.%20We%20evaluate%0APICLe%20on%20five%20biomedical%20NED%20datasets%20and%20show%20that%2C%20with%20zero%20human%0Aannotation%2C%20PICLe%20outperforms%20ICL%20in%20low-resource%20settings%20where%20limited%20gold%0Aexamples%20can%20be%20used%20as%20in-context%20demonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11923v1&entry.124074799=Read"},
{"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models", "author": "Tao Fan and Guoqiang Ma and Yan Kang and Hanlin Gu and Yuanfeng Song and Lixin Fan and Kai Chen and Qiang Yang", "abstract": "  Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs.\n", "link": "http://arxiv.org/abs/2406.02224v4", "date": "2024-12-16", "relevancy": 1.94, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMKT%3A%20Federated%20Mutual%20Knowledge%20Transfer%20for%20Large%20and%20Small%20Language%0A%20%20Models&body=Title%3A%20FedMKT%3A%20Federated%20Mutual%20Knowledge%20Transfer%20for%20Large%20and%20Small%20Language%0A%20%20Models%0AAuthor%3A%20Tao%20Fan%20and%20Guoqiang%20Ma%20and%20Yan%20Kang%20and%20Hanlin%20Gu%20and%20Yuanfeng%20Song%20and%20Lixin%20Fan%20and%20Kai%20Chen%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20Recent%20research%20in%20federated%20large%20language%20models%20%28LLMs%29%20has%20primarily%0Afocused%20on%20enabling%20clients%20to%20fine-tune%20their%20locally%20deployed%20homogeneous%0ALLMs%20collaboratively%20or%20on%20transferring%20knowledge%20from%20server-based%20LLMs%20to%0Asmall%20language%20models%20%28SLMs%29%20at%20downstream%20clients.%20However%2C%20a%20significant%20gap%0Aremains%20in%20the%20simultaneous%20mutual%20enhancement%20of%20both%20the%20server%27s%20LLM%20and%0Aclients%27%20SLMs.%20To%20bridge%20this%20gap%2C%20we%20propose%20FedMKT%2C%20a%20parameter-efficient%0Afederated%20mutual%20knowledge%20transfer%20framework%20for%20large%20and%20small%20language%0Amodels.%20This%20framework%20is%20designed%20to%20adaptively%20transfer%20knowledge%20from%20the%0Aserver%27s%20LLM%20to%20clients%27%20SLMs%20while%20concurrently%20enriching%20the%20LLM%20with%0Aclients%27%20unique%20domain%20insights.%20We%20facilitate%20token%20alignment%20using%20minimum%0Aedit%20distance%20%28MinED%29%20and%20then%20selective%20mutual%20knowledge%20transfer%20between%0Aclient-side%20SLMs%20and%20a%20server-side%20LLM%2C%20aiming%20to%20collectively%20enhance%20their%0Aperformance.%20Through%20extensive%20experiments%20across%20three%20distinct%20scenarios%2C%20we%0Aevaluate%20the%20effectiveness%20of%20FedMKT%20using%20various%20public%20LLMs%20and%20SLMs%20on%20a%0Arange%20of%20NLP%20text%20generation%20tasks.%20Empirical%20results%20demonstrate%20that%20FedMKT%0Asimultaneously%20boosts%20the%20performance%20of%20both%20LLMs%20and%20SLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02224v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMKT%253A%2520Federated%2520Mutual%2520Knowledge%2520Transfer%2520for%2520Large%2520and%2520Small%2520Language%250A%2520%2520Models%26entry.906535625%3DTao%2520Fan%2520and%2520Guoqiang%2520Ma%2520and%2520Yan%2520Kang%2520and%2520Hanlin%2520Gu%2520and%2520Yuanfeng%2520Song%2520and%2520Lixin%2520Fan%2520and%2520Kai%2520Chen%2520and%2520Qiang%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520research%2520in%2520federated%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520primarily%250Afocused%2520on%2520enabling%2520clients%2520to%2520fine-tune%2520their%2520locally%2520deployed%2520homogeneous%250ALLMs%2520collaboratively%2520or%2520on%2520transferring%2520knowledge%2520from%2520server-based%2520LLMs%2520to%250Asmall%2520language%2520models%2520%2528SLMs%2529%2520at%2520downstream%2520clients.%2520However%252C%2520a%2520significant%2520gap%250Aremains%2520in%2520the%2520simultaneous%2520mutual%2520enhancement%2520of%2520both%2520the%2520server%2527s%2520LLM%2520and%250Aclients%2527%2520SLMs.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520FedMKT%252C%2520a%2520parameter-efficient%250Afederated%2520mutual%2520knowledge%2520transfer%2520framework%2520for%2520large%2520and%2520small%2520language%250Amodels.%2520This%2520framework%2520is%2520designed%2520to%2520adaptively%2520transfer%2520knowledge%2520from%2520the%250Aserver%2527s%2520LLM%2520to%2520clients%2527%2520SLMs%2520while%2520concurrently%2520enriching%2520the%2520LLM%2520with%250Aclients%2527%2520unique%2520domain%2520insights.%2520We%2520facilitate%2520token%2520alignment%2520using%2520minimum%250Aedit%2520distance%2520%2528MinED%2529%2520and%2520then%2520selective%2520mutual%2520knowledge%2520transfer%2520between%250Aclient-side%2520SLMs%2520and%2520a%2520server-side%2520LLM%252C%2520aiming%2520to%2520collectively%2520enhance%2520their%250Aperformance.%2520Through%2520extensive%2520experiments%2520across%2520three%2520distinct%2520scenarios%252C%2520we%250Aevaluate%2520the%2520effectiveness%2520of%2520FedMKT%2520using%2520various%2520public%2520LLMs%2520and%2520SLMs%2520on%2520a%250Arange%2520of%2520NLP%2520text%2520generation%2520tasks.%2520Empirical%2520results%2520demonstrate%2520that%2520FedMKT%250Asimultaneously%2520boosts%2520the%2520performance%2520of%2520both%2520LLMs%2520and%2520SLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02224v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMKT%3A%20Federated%20Mutual%20Knowledge%20Transfer%20for%20Large%20and%20Small%20Language%0A%20%20Models&entry.906535625=Tao%20Fan%20and%20Guoqiang%20Ma%20and%20Yan%20Kang%20and%20Hanlin%20Gu%20and%20Yuanfeng%20Song%20and%20Lixin%20Fan%20and%20Kai%20Chen%20and%20Qiang%20Yang&entry.1292438233=%20%20Recent%20research%20in%20federated%20large%20language%20models%20%28LLMs%29%20has%20primarily%0Afocused%20on%20enabling%20clients%20to%20fine-tune%20their%20locally%20deployed%20homogeneous%0ALLMs%20collaboratively%20or%20on%20transferring%20knowledge%20from%20server-based%20LLMs%20to%0Asmall%20language%20models%20%28SLMs%29%20at%20downstream%20clients.%20However%2C%20a%20significant%20gap%0Aremains%20in%20the%20simultaneous%20mutual%20enhancement%20of%20both%20the%20server%27s%20LLM%20and%0Aclients%27%20SLMs.%20To%20bridge%20this%20gap%2C%20we%20propose%20FedMKT%2C%20a%20parameter-efficient%0Afederated%20mutual%20knowledge%20transfer%20framework%20for%20large%20and%20small%20language%0Amodels.%20This%20framework%20is%20designed%20to%20adaptively%20transfer%20knowledge%20from%20the%0Aserver%27s%20LLM%20to%20clients%27%20SLMs%20while%20concurrently%20enriching%20the%20LLM%20with%0Aclients%27%20unique%20domain%20insights.%20We%20facilitate%20token%20alignment%20using%20minimum%0Aedit%20distance%20%28MinED%29%20and%20then%20selective%20mutual%20knowledge%20transfer%20between%0Aclient-side%20SLMs%20and%20a%20server-side%20LLM%2C%20aiming%20to%20collectively%20enhance%20their%0Aperformance.%20Through%20extensive%20experiments%20across%20three%20distinct%20scenarios%2C%20we%0Aevaluate%20the%20effectiveness%20of%20FedMKT%20using%20various%20public%20LLMs%20and%20SLMs%20on%20a%0Arange%20of%20NLP%20text%20generation%20tasks.%20Empirical%20results%20demonstrate%20that%20FedMKT%0Asimultaneously%20boosts%20the%20performance%20of%20both%20LLMs%20and%20SLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02224v4&entry.124074799=Read"},
{"title": "MGH Radiology Llama: A Llama 3 70B Model for Radiology", "author": "Yucheng Shi and Peng Shu and Zhengliang Liu and Zihao Wu and Quanzheng Li and Tianming Liu and Ninghao Liu and Xiang Li", "abstract": "  In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs.\n", "link": "http://arxiv.org/abs/2408.11848v2", "date": "2024-12-16", "relevancy": 1.9314, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGH%20Radiology%20Llama%3A%20A%20Llama%203%2070B%20Model%20for%20Radiology&body=Title%3A%20MGH%20Radiology%20Llama%3A%20A%20Llama%203%2070B%20Model%20for%20Radiology%0AAuthor%3A%20Yucheng%20Shi%20and%20Peng%20Shu%20and%20Zhengliang%20Liu%20and%20Zihao%20Wu%20and%20Quanzheng%20Li%20and%20Tianming%20Liu%20and%20Ninghao%20Liu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20field%20of%20radiology%20has%20increasingly%20harnessed%20the%20power%0Aof%20artificial%20intelligence%20%28AI%29%20to%20enhance%20diagnostic%20accuracy%2C%20streamline%0Aworkflows%2C%20and%20improve%20patient%20care.%20Large%20language%20models%20%28LLMs%29%20have%20emerged%0Aas%20particularly%20promising%20tools%2C%20offering%20significant%20potential%20in%20assisting%0Aradiologists%20with%20report%20generation%2C%20clinical%20decision%20support%2C%20and%20patient%0Acommunication.%20This%20paper%20presents%20an%20advanced%20radiology-focused%20large%20language%0Amodel%3A%20MGH%20Radiology%20Llama.%20It%20is%20developed%20using%20the%20Llama%203%2070B%20model%2C%0Abuilding%20upon%20previous%20domain-specific%20models%20like%20Radiology-GPT%20and%0ARadiology-Llama2.%20Leveraging%20a%20unique%20and%20comprehensive%20dataset%20from%0AMassachusetts%20General%20Hospital%2C%20comprising%20over%206.5%20million%20de-identified%0Amedical%20reports%20across%20various%20imaging%20modalities%2C%20the%20model%20demonstrates%0Asignificant%20improvements%20in%20generating%20accurate%20and%20clinically%20relevant%0Aradiology%20impressions%20given%20the%20corresponding%20findings.%20Our%20evaluation%2C%0Aincorporating%20both%20traditional%20metrics%20and%20a%20GPT-4-based%20assessment%2C%20highlights%0Athe%20enhanced%20performance%20of%20this%20work%20over%20general-purpose%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGH%2520Radiology%2520Llama%253A%2520A%2520Llama%25203%252070B%2520Model%2520for%2520Radiology%26entry.906535625%3DYucheng%2520Shi%2520and%2520Peng%2520Shu%2520and%2520Zhengliang%2520Liu%2520and%2520Zihao%2520Wu%2520and%2520Quanzheng%2520Li%2520and%2520Tianming%2520Liu%2520and%2520Ninghao%2520Liu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520field%2520of%2520radiology%2520has%2520increasingly%2520harnessed%2520the%2520power%250Aof%2520artificial%2520intelligence%2520%2528AI%2529%2520to%2520enhance%2520diagnostic%2520accuracy%252C%2520streamline%250Aworkflows%252C%2520and%2520improve%2520patient%2520care.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520emerged%250Aas%2520particularly%2520promising%2520tools%252C%2520offering%2520significant%2520potential%2520in%2520assisting%250Aradiologists%2520with%2520report%2520generation%252C%2520clinical%2520decision%2520support%252C%2520and%2520patient%250Acommunication.%2520This%2520paper%2520presents%2520an%2520advanced%2520radiology-focused%2520large%2520language%250Amodel%253A%2520MGH%2520Radiology%2520Llama.%2520It%2520is%2520developed%2520using%2520the%2520Llama%25203%252070B%2520model%252C%250Abuilding%2520upon%2520previous%2520domain-specific%2520models%2520like%2520Radiology-GPT%2520and%250ARadiology-Llama2.%2520Leveraging%2520a%2520unique%2520and%2520comprehensive%2520dataset%2520from%250AMassachusetts%2520General%2520Hospital%252C%2520comprising%2520over%25206.5%2520million%2520de-identified%250Amedical%2520reports%2520across%2520various%2520imaging%2520modalities%252C%2520the%2520model%2520demonstrates%250Asignificant%2520improvements%2520in%2520generating%2520accurate%2520and%2520clinically%2520relevant%250Aradiology%2520impressions%2520given%2520the%2520corresponding%2520findings.%2520Our%2520evaluation%252C%250Aincorporating%2520both%2520traditional%2520metrics%2520and%2520a%2520GPT-4-based%2520assessment%252C%2520highlights%250Athe%2520enhanced%2520performance%2520of%2520this%2520work%2520over%2520general-purpose%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGH%20Radiology%20Llama%3A%20A%20Llama%203%2070B%20Model%20for%20Radiology&entry.906535625=Yucheng%20Shi%20and%20Peng%20Shu%20and%20Zhengliang%20Liu%20and%20Zihao%20Wu%20and%20Quanzheng%20Li%20and%20Tianming%20Liu%20and%20Ninghao%20Liu%20and%20Xiang%20Li&entry.1292438233=%20%20In%20recent%20years%2C%20the%20field%20of%20radiology%20has%20increasingly%20harnessed%20the%20power%0Aof%20artificial%20intelligence%20%28AI%29%20to%20enhance%20diagnostic%20accuracy%2C%20streamline%0Aworkflows%2C%20and%20improve%20patient%20care.%20Large%20language%20models%20%28LLMs%29%20have%20emerged%0Aas%20particularly%20promising%20tools%2C%20offering%20significant%20potential%20in%20assisting%0Aradiologists%20with%20report%20generation%2C%20clinical%20decision%20support%2C%20and%20patient%0Acommunication.%20This%20paper%20presents%20an%20advanced%20radiology-focused%20large%20language%0Amodel%3A%20MGH%20Radiology%20Llama.%20It%20is%20developed%20using%20the%20Llama%203%2070B%20model%2C%0Abuilding%20upon%20previous%20domain-specific%20models%20like%20Radiology-GPT%20and%0ARadiology-Llama2.%20Leveraging%20a%20unique%20and%20comprehensive%20dataset%20from%0AMassachusetts%20General%20Hospital%2C%20comprising%20over%206.5%20million%20de-identified%0Amedical%20reports%20across%20various%20imaging%20modalities%2C%20the%20model%20demonstrates%0Asignificant%20improvements%20in%20generating%20accurate%20and%20clinically%20relevant%0Aradiology%20impressions%20given%20the%20corresponding%20findings.%20Our%20evaluation%2C%0Aincorporating%20both%20traditional%20metrics%20and%20a%20GPT-4-based%20assessment%2C%20highlights%0Athe%20enhanced%20performance%20of%20this%20work%20over%20general-purpose%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11848v2&entry.124074799=Read"},
{"title": "SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with\n  a GAN-Inspired Approach to Synthetic Dataset Generation", "author": "Debarshi Kundu", "abstract": "  Consider the problem: ``If one man and one woman can produce one child in one\nyear, how many children will be produced by one woman and three men in 0.5\nyears?\" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview,\nand Gemini Flash frequently answer \"0.5,\" which does not make sense. While\nthese models sometimes acknowledge the unrealistic nature of the question, in\nmany cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5\nchild.\" Additionally, temporal variation has been observed: if an LLM answers\ncorrectly once (by recognizing the faulty nature of the question), subsequent\nresponses are more likely to also reflect this understanding. However, this is\ninconsistent.\n  These types of questions have motivated us to develop a dataset of science\nquestions, SciFaultyQA, where the questions themselves are intentionally\nfaulty. We observed that LLMs often proceed to answer these flawed questions\nwithout recognizing their inherent issues, producing results that are logically\nor scientifically invalid. By analyzing such patterns, we developed a novel\nmethod for generating synthetic datasets to evaluate and benchmark the\nperformance of various LLMs in identifying these flawed questions. We have also\ndeveloped novel approaches to reduce the errors.\n", "link": "http://arxiv.org/abs/2412.11988v1", "date": "2024-12-16", "relevancy": 1.9287, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4945}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SciFaultyQA%3A%20Benchmarking%20LLMs%20on%20Faulty%20Science%20Question%20Detection%20with%0A%20%20a%20GAN-Inspired%20Approach%20to%20Synthetic%20Dataset%20Generation&body=Title%3A%20SciFaultyQA%3A%20Benchmarking%20LLMs%20on%20Faulty%20Science%20Question%20Detection%20with%0A%20%20a%20GAN-Inspired%20Approach%20to%20Synthetic%20Dataset%20Generation%0AAuthor%3A%20Debarshi%20Kundu%0AAbstract%3A%20%20%20Consider%20the%20problem%3A%20%60%60If%20one%20man%20and%20one%20woman%20can%20produce%20one%20child%20in%20one%0Ayear%2C%20how%20many%20children%20will%20be%20produced%20by%20one%20woman%20and%20three%20men%20in%200.5%0Ayears%3F%22%20Current%20large%20language%20models%20%28LLMs%29%20such%20as%20GPT-4o%2C%20GPT-o1-preview%2C%0Aand%20Gemini%20Flash%20frequently%20answer%20%220.5%2C%22%20which%20does%20not%20make%20sense.%20While%0Athese%20models%20sometimes%20acknowledge%20the%20unrealistic%20nature%20of%20the%20question%2C%20in%0Amany%20cases%20%288%20out%20of%2010%20trials%29%2C%20they%20provide%20the%20nonsensical%20answer%20of%20%220.5%0Achild.%22%20Additionally%2C%20temporal%20variation%20has%20been%20observed%3A%20if%20an%20LLM%20answers%0Acorrectly%20once%20%28by%20recognizing%20the%20faulty%20nature%20of%20the%20question%29%2C%20subsequent%0Aresponses%20are%20more%20likely%20to%20also%20reflect%20this%20understanding.%20However%2C%20this%20is%0Ainconsistent.%0A%20%20These%20types%20of%20questions%20have%20motivated%20us%20to%20develop%20a%20dataset%20of%20science%0Aquestions%2C%20SciFaultyQA%2C%20where%20the%20questions%20themselves%20are%20intentionally%0Afaulty.%20We%20observed%20that%20LLMs%20often%20proceed%20to%20answer%20these%20flawed%20questions%0Awithout%20recognizing%20their%20inherent%20issues%2C%20producing%20results%20that%20are%20logically%0Aor%20scientifically%20invalid.%20By%20analyzing%20such%20patterns%2C%20we%20developed%20a%20novel%0Amethod%20for%20generating%20synthetic%20datasets%20to%20evaluate%20and%20benchmark%20the%0Aperformance%20of%20various%20LLMs%20in%20identifying%20these%20flawed%20questions.%20We%20have%20also%0Adeveloped%20novel%20approaches%20to%20reduce%20the%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSciFaultyQA%253A%2520Benchmarking%2520LLMs%2520on%2520Faulty%2520Science%2520Question%2520Detection%2520with%250A%2520%2520a%2520GAN-Inspired%2520Approach%2520to%2520Synthetic%2520Dataset%2520Generation%26entry.906535625%3DDebarshi%2520Kundu%26entry.1292438233%3D%2520%2520Consider%2520the%2520problem%253A%2520%2560%2560If%2520one%2520man%2520and%2520one%2520woman%2520can%2520produce%2520one%2520child%2520in%2520one%250Ayear%252C%2520how%2520many%2520children%2520will%2520be%2520produced%2520by%2520one%2520woman%2520and%2520three%2520men%2520in%25200.5%250Ayears%253F%2522%2520Current%2520large%2520language%2520models%2520%2528LLMs%2529%2520such%2520as%2520GPT-4o%252C%2520GPT-o1-preview%252C%250Aand%2520Gemini%2520Flash%2520frequently%2520answer%2520%25220.5%252C%2522%2520which%2520does%2520not%2520make%2520sense.%2520While%250Athese%2520models%2520sometimes%2520acknowledge%2520the%2520unrealistic%2520nature%2520of%2520the%2520question%252C%2520in%250Amany%2520cases%2520%25288%2520out%2520of%252010%2520trials%2529%252C%2520they%2520provide%2520the%2520nonsensical%2520answer%2520of%2520%25220.5%250Achild.%2522%2520Additionally%252C%2520temporal%2520variation%2520has%2520been%2520observed%253A%2520if%2520an%2520LLM%2520answers%250Acorrectly%2520once%2520%2528by%2520recognizing%2520the%2520faulty%2520nature%2520of%2520the%2520question%2529%252C%2520subsequent%250Aresponses%2520are%2520more%2520likely%2520to%2520also%2520reflect%2520this%2520understanding.%2520However%252C%2520this%2520is%250Ainconsistent.%250A%2520%2520These%2520types%2520of%2520questions%2520have%2520motivated%2520us%2520to%2520develop%2520a%2520dataset%2520of%2520science%250Aquestions%252C%2520SciFaultyQA%252C%2520where%2520the%2520questions%2520themselves%2520are%2520intentionally%250Afaulty.%2520We%2520observed%2520that%2520LLMs%2520often%2520proceed%2520to%2520answer%2520these%2520flawed%2520questions%250Awithout%2520recognizing%2520their%2520inherent%2520issues%252C%2520producing%2520results%2520that%2520are%2520logically%250Aor%2520scientifically%2520invalid.%2520By%2520analyzing%2520such%2520patterns%252C%2520we%2520developed%2520a%2520novel%250Amethod%2520for%2520generating%2520synthetic%2520datasets%2520to%2520evaluate%2520and%2520benchmark%2520the%250Aperformance%2520of%2520various%2520LLMs%2520in%2520identifying%2520these%2520flawed%2520questions.%2520We%2520have%2520also%250Adeveloped%2520novel%2520approaches%2520to%2520reduce%2520the%2520errors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SciFaultyQA%3A%20Benchmarking%20LLMs%20on%20Faulty%20Science%20Question%20Detection%20with%0A%20%20a%20GAN-Inspired%20Approach%20to%20Synthetic%20Dataset%20Generation&entry.906535625=Debarshi%20Kundu&entry.1292438233=%20%20Consider%20the%20problem%3A%20%60%60If%20one%20man%20and%20one%20woman%20can%20produce%20one%20child%20in%20one%0Ayear%2C%20how%20many%20children%20will%20be%20produced%20by%20one%20woman%20and%20three%20men%20in%200.5%0Ayears%3F%22%20Current%20large%20language%20models%20%28LLMs%29%20such%20as%20GPT-4o%2C%20GPT-o1-preview%2C%0Aand%20Gemini%20Flash%20frequently%20answer%20%220.5%2C%22%20which%20does%20not%20make%20sense.%20While%0Athese%20models%20sometimes%20acknowledge%20the%20unrealistic%20nature%20of%20the%20question%2C%20in%0Amany%20cases%20%288%20out%20of%2010%20trials%29%2C%20they%20provide%20the%20nonsensical%20answer%20of%20%220.5%0Achild.%22%20Additionally%2C%20temporal%20variation%20has%20been%20observed%3A%20if%20an%20LLM%20answers%0Acorrectly%20once%20%28by%20recognizing%20the%20faulty%20nature%20of%20the%20question%29%2C%20subsequent%0Aresponses%20are%20more%20likely%20to%20also%20reflect%20this%20understanding.%20However%2C%20this%20is%0Ainconsistent.%0A%20%20These%20types%20of%20questions%20have%20motivated%20us%20to%20develop%20a%20dataset%20of%20science%0Aquestions%2C%20SciFaultyQA%2C%20where%20the%20questions%20themselves%20are%20intentionally%0Afaulty.%20We%20observed%20that%20LLMs%20often%20proceed%20to%20answer%20these%20flawed%20questions%0Awithout%20recognizing%20their%20inherent%20issues%2C%20producing%20results%20that%20are%20logically%0Aor%20scientifically%20invalid.%20By%20analyzing%20such%20patterns%2C%20we%20developed%20a%20novel%0Amethod%20for%20generating%20synthetic%20datasets%20to%20evaluate%20and%20benchmark%20the%0Aperformance%20of%20various%20LLMs%20in%20identifying%20these%20flawed%20questions.%20We%20have%20also%0Adeveloped%20novel%20approaches%20to%20reduce%20the%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11988v1&entry.124074799=Read"},
{"title": "Generalization Analysis for Deep Contrastive Representation Learning", "author": "Nong Minh Hieu and Antoine Ledent and Yunwen Lei and Cheng Yeaw Ku", "abstract": "  In this paper, we present generalization bounds for the unsupervised risk in\nthe Deep Contrastive Representation Learning framework, which employs deep\nneural networks as representation functions. We approach this problem from two\nangles. On the one hand, we derive a parameter-counting bound that scales with\nthe overall size of the neural networks. On the other hand, we provide a\nnorm-based bound that scales with the norms of neural networks' weight\nmatrices. Ignoring logarithmic factors, the bounds are independent of $k$, the\nsize of the tuples provided for contrastive learning. To the best of our\nknowledge, this property is only shared by one other work, which employed a\ndifferent proof strategy and suffers from very strong exponential dependence on\nthe depth of the network which is due to a use of the peeling technique. Our\nresults circumvent this by leveraging powerful results on covering numbers with\nrespect to uniform norms over samples. In addition, we utilize loss\naugmentation techniques to further reduce the dependency on matrix norms and\nthe implicit dependence on network depth. In fact, our techniques allow us to\nproduce many bounds for the contrastive learning setting with similar\narchitectural dependencies as in the study of the sample complexity of ordinary\nloss functions, thereby bridging the gap between the learning theories of\ncontrastive learning and DNNs.\n", "link": "http://arxiv.org/abs/2412.12014v1", "date": "2024-12-16", "relevancy": 1.9236, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.487}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4851}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Analysis%20for%20Deep%20Contrastive%20Representation%20Learning&body=Title%3A%20Generalization%20Analysis%20for%20Deep%20Contrastive%20Representation%20Learning%0AAuthor%3A%20Nong%20Minh%20Hieu%20and%20Antoine%20Ledent%20and%20Yunwen%20Lei%20and%20Cheng%20Yeaw%20Ku%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20generalization%20bounds%20for%20the%20unsupervised%20risk%20in%0Athe%20Deep%20Contrastive%20Representation%20Learning%20framework%2C%20which%20employs%20deep%0Aneural%20networks%20as%20representation%20functions.%20We%20approach%20this%20problem%20from%20two%0Aangles.%20On%20the%20one%20hand%2C%20we%20derive%20a%20parameter-counting%20bound%20that%20scales%20with%0Athe%20overall%20size%20of%20the%20neural%20networks.%20On%20the%20other%20hand%2C%20we%20provide%20a%0Anorm-based%20bound%20that%20scales%20with%20the%20norms%20of%20neural%20networks%27%20weight%0Amatrices.%20Ignoring%20logarithmic%20factors%2C%20the%20bounds%20are%20independent%20of%20%24k%24%2C%20the%0Asize%20of%20the%20tuples%20provided%20for%20contrastive%20learning.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20property%20is%20only%20shared%20by%20one%20other%20work%2C%20which%20employed%20a%0Adifferent%20proof%20strategy%20and%20suffers%20from%20very%20strong%20exponential%20dependence%20on%0Athe%20depth%20of%20the%20network%20which%20is%20due%20to%20a%20use%20of%20the%20peeling%20technique.%20Our%0Aresults%20circumvent%20this%20by%20leveraging%20powerful%20results%20on%20covering%20numbers%20with%0Arespect%20to%20uniform%20norms%20over%20samples.%20In%20addition%2C%20we%20utilize%20loss%0Aaugmentation%20techniques%20to%20further%20reduce%20the%20dependency%20on%20matrix%20norms%20and%0Athe%20implicit%20dependence%20on%20network%20depth.%20In%20fact%2C%20our%20techniques%20allow%20us%20to%0Aproduce%20many%20bounds%20for%20the%20contrastive%20learning%20setting%20with%20similar%0Aarchitectural%20dependencies%20as%20in%20the%20study%20of%20the%20sample%20complexity%20of%20ordinary%0Aloss%20functions%2C%20thereby%20bridging%20the%20gap%20between%20the%20learning%20theories%20of%0Acontrastive%20learning%20and%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Analysis%2520for%2520Deep%2520Contrastive%2520Representation%2520Learning%26entry.906535625%3DNong%2520Minh%2520Hieu%2520and%2520Antoine%2520Ledent%2520and%2520Yunwen%2520Lei%2520and%2520Cheng%2520Yeaw%2520Ku%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520generalization%2520bounds%2520for%2520the%2520unsupervised%2520risk%2520in%250Athe%2520Deep%2520Contrastive%2520Representation%2520Learning%2520framework%252C%2520which%2520employs%2520deep%250Aneural%2520networks%2520as%2520representation%2520functions.%2520We%2520approach%2520this%2520problem%2520from%2520two%250Aangles.%2520On%2520the%2520one%2520hand%252C%2520we%2520derive%2520a%2520parameter-counting%2520bound%2520that%2520scales%2520with%250Athe%2520overall%2520size%2520of%2520the%2520neural%2520networks.%2520On%2520the%2520other%2520hand%252C%2520we%2520provide%2520a%250Anorm-based%2520bound%2520that%2520scales%2520with%2520the%2520norms%2520of%2520neural%2520networks%2527%2520weight%250Amatrices.%2520Ignoring%2520logarithmic%2520factors%252C%2520the%2520bounds%2520are%2520independent%2520of%2520%2524k%2524%252C%2520the%250Asize%2520of%2520the%2520tuples%2520provided%2520for%2520contrastive%2520learning.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520property%2520is%2520only%2520shared%2520by%2520one%2520other%2520work%252C%2520which%2520employed%2520a%250Adifferent%2520proof%2520strategy%2520and%2520suffers%2520from%2520very%2520strong%2520exponential%2520dependence%2520on%250Athe%2520depth%2520of%2520the%2520network%2520which%2520is%2520due%2520to%2520a%2520use%2520of%2520the%2520peeling%2520technique.%2520Our%250Aresults%2520circumvent%2520this%2520by%2520leveraging%2520powerful%2520results%2520on%2520covering%2520numbers%2520with%250Arespect%2520to%2520uniform%2520norms%2520over%2520samples.%2520In%2520addition%252C%2520we%2520utilize%2520loss%250Aaugmentation%2520techniques%2520to%2520further%2520reduce%2520the%2520dependency%2520on%2520matrix%2520norms%2520and%250Athe%2520implicit%2520dependence%2520on%2520network%2520depth.%2520In%2520fact%252C%2520our%2520techniques%2520allow%2520us%2520to%250Aproduce%2520many%2520bounds%2520for%2520the%2520contrastive%2520learning%2520setting%2520with%2520similar%250Aarchitectural%2520dependencies%2520as%2520in%2520the%2520study%2520of%2520the%2520sample%2520complexity%2520of%2520ordinary%250Aloss%2520functions%252C%2520thereby%2520bridging%2520the%2520gap%2520between%2520the%2520learning%2520theories%2520of%250Acontrastive%2520learning%2520and%2520DNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Analysis%20for%20Deep%20Contrastive%20Representation%20Learning&entry.906535625=Nong%20Minh%20Hieu%20and%20Antoine%20Ledent%20and%20Yunwen%20Lei%20and%20Cheng%20Yeaw%20Ku&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20generalization%20bounds%20for%20the%20unsupervised%20risk%20in%0Athe%20Deep%20Contrastive%20Representation%20Learning%20framework%2C%20which%20employs%20deep%0Aneural%20networks%20as%20representation%20functions.%20We%20approach%20this%20problem%20from%20two%0Aangles.%20On%20the%20one%20hand%2C%20we%20derive%20a%20parameter-counting%20bound%20that%20scales%20with%0Athe%20overall%20size%20of%20the%20neural%20networks.%20On%20the%20other%20hand%2C%20we%20provide%20a%0Anorm-based%20bound%20that%20scales%20with%20the%20norms%20of%20neural%20networks%27%20weight%0Amatrices.%20Ignoring%20logarithmic%20factors%2C%20the%20bounds%20are%20independent%20of%20%24k%24%2C%20the%0Asize%20of%20the%20tuples%20provided%20for%20contrastive%20learning.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20property%20is%20only%20shared%20by%20one%20other%20work%2C%20which%20employed%20a%0Adifferent%20proof%20strategy%20and%20suffers%20from%20very%20strong%20exponential%20dependence%20on%0Athe%20depth%20of%20the%20network%20which%20is%20due%20to%20a%20use%20of%20the%20peeling%20technique.%20Our%0Aresults%20circumvent%20this%20by%20leveraging%20powerful%20results%20on%20covering%20numbers%20with%0Arespect%20to%20uniform%20norms%20over%20samples.%20In%20addition%2C%20we%20utilize%20loss%0Aaugmentation%20techniques%20to%20further%20reduce%20the%20dependency%20on%20matrix%20norms%20and%0Athe%20implicit%20dependence%20on%20network%20depth.%20In%20fact%2C%20our%20techniques%20allow%20us%20to%0Aproduce%20many%20bounds%20for%20the%20contrastive%20learning%20setting%20with%20similar%0Aarchitectural%20dependencies%20as%20in%20the%20study%20of%20the%20sample%20complexity%20of%20ordinary%0Aloss%20functions%2C%20thereby%20bridging%20the%20gap%20between%20the%20learning%20theories%20of%0Acontrastive%20learning%20and%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12014v1&entry.124074799=Read"},
{"title": "Bilevel Learning with Inexact Stochastic Gradients", "author": "Mohammad Sadegh Salehi and Subhadip Mukherjee and Lindon Roberts and Matthias J. Ehrhardt", "abstract": "  Bilevel learning has gained prominence in machine learning, inverse problems,\nand imaging applications, including hyperparameter optimization, learning\ndata-adaptive regularizers, and optimizing forward operators. The large-scale\nnature of these problems has led to the development of inexact and\ncomputationally efficient methods. Existing adaptive methods predominantly rely\non deterministic formulations, while stochastic approaches often adopt a\ndoubly-stochastic framework with impractical variance assumptions, enforces a\nfixed number of lower-level iterations, and requires extensive tuning. In this\nwork, we focus on bilevel learning with strongly convex lower-level problems\nand a nonconvex sum-of-functions in the upper-level. Stochasticity arises from\ndata sampling in the upper-level which leads to inexact stochastic\nhypergradients. We establish their connection to state-of-the-art stochastic\noptimization theory for nonconvex objectives. Furthermore, we prove the\nconvergence of inexact stochastic bilevel optimization under mild assumptions.\nOur empirical results highlight significant speed-ups and improved\ngeneralization in imaging tasks such as image denoising and deblurring in\ncomparison with adaptive deterministic bilevel methods.\n", "link": "http://arxiv.org/abs/2412.12049v1", "date": "2024-12-16", "relevancy": 1.9222, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4886}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4863}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bilevel%20Learning%20with%20Inexact%20Stochastic%20Gradients&body=Title%3A%20Bilevel%20Learning%20with%20Inexact%20Stochastic%20Gradients%0AAuthor%3A%20Mohammad%20Sadegh%20Salehi%20and%20Subhadip%20Mukherjee%20and%20Lindon%20Roberts%20and%20Matthias%20J.%20Ehrhardt%0AAbstract%3A%20%20%20Bilevel%20learning%20has%20gained%20prominence%20in%20machine%20learning%2C%20inverse%20problems%2C%0Aand%20imaging%20applications%2C%20including%20hyperparameter%20optimization%2C%20learning%0Adata-adaptive%20regularizers%2C%20and%20optimizing%20forward%20operators.%20The%20large-scale%0Anature%20of%20these%20problems%20has%20led%20to%20the%20development%20of%20inexact%20and%0Acomputationally%20efficient%20methods.%20Existing%20adaptive%20methods%20predominantly%20rely%0Aon%20deterministic%20formulations%2C%20while%20stochastic%20approaches%20often%20adopt%20a%0Adoubly-stochastic%20framework%20with%20impractical%20variance%20assumptions%2C%20enforces%20a%0Afixed%20number%20of%20lower-level%20iterations%2C%20and%20requires%20extensive%20tuning.%20In%20this%0Awork%2C%20we%20focus%20on%20bilevel%20learning%20with%20strongly%20convex%20lower-level%20problems%0Aand%20a%20nonconvex%20sum-of-functions%20in%20the%20upper-level.%20Stochasticity%20arises%20from%0Adata%20sampling%20in%20the%20upper-level%20which%20leads%20to%20inexact%20stochastic%0Ahypergradients.%20We%20establish%20their%20connection%20to%20state-of-the-art%20stochastic%0Aoptimization%20theory%20for%20nonconvex%20objectives.%20Furthermore%2C%20we%20prove%20the%0Aconvergence%20of%20inexact%20stochastic%20bilevel%20optimization%20under%20mild%20assumptions.%0AOur%20empirical%20results%20highlight%20significant%20speed-ups%20and%20improved%0Ageneralization%20in%20imaging%20tasks%20such%20as%20image%20denoising%20and%20deblurring%20in%0Acomparison%20with%20adaptive%20deterministic%20bilevel%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBilevel%2520Learning%2520with%2520Inexact%2520Stochastic%2520Gradients%26entry.906535625%3DMohammad%2520Sadegh%2520Salehi%2520and%2520Subhadip%2520Mukherjee%2520and%2520Lindon%2520Roberts%2520and%2520Matthias%2520J.%2520Ehrhardt%26entry.1292438233%3D%2520%2520Bilevel%2520learning%2520has%2520gained%2520prominence%2520in%2520machine%2520learning%252C%2520inverse%2520problems%252C%250Aand%2520imaging%2520applications%252C%2520including%2520hyperparameter%2520optimization%252C%2520learning%250Adata-adaptive%2520regularizers%252C%2520and%2520optimizing%2520forward%2520operators.%2520The%2520large-scale%250Anature%2520of%2520these%2520problems%2520has%2520led%2520to%2520the%2520development%2520of%2520inexact%2520and%250Acomputationally%2520efficient%2520methods.%2520Existing%2520adaptive%2520methods%2520predominantly%2520rely%250Aon%2520deterministic%2520formulations%252C%2520while%2520stochastic%2520approaches%2520often%2520adopt%2520a%250Adoubly-stochastic%2520framework%2520with%2520impractical%2520variance%2520assumptions%252C%2520enforces%2520a%250Afixed%2520number%2520of%2520lower-level%2520iterations%252C%2520and%2520requires%2520extensive%2520tuning.%2520In%2520this%250Awork%252C%2520we%2520focus%2520on%2520bilevel%2520learning%2520with%2520strongly%2520convex%2520lower-level%2520problems%250Aand%2520a%2520nonconvex%2520sum-of-functions%2520in%2520the%2520upper-level.%2520Stochasticity%2520arises%2520from%250Adata%2520sampling%2520in%2520the%2520upper-level%2520which%2520leads%2520to%2520inexact%2520stochastic%250Ahypergradients.%2520We%2520establish%2520their%2520connection%2520to%2520state-of-the-art%2520stochastic%250Aoptimization%2520theory%2520for%2520nonconvex%2520objectives.%2520Furthermore%252C%2520we%2520prove%2520the%250Aconvergence%2520of%2520inexact%2520stochastic%2520bilevel%2520optimization%2520under%2520mild%2520assumptions.%250AOur%2520empirical%2520results%2520highlight%2520significant%2520speed-ups%2520and%2520improved%250Ageneralization%2520in%2520imaging%2520tasks%2520such%2520as%2520image%2520denoising%2520and%2520deblurring%2520in%250Acomparison%2520with%2520adaptive%2520deterministic%2520bilevel%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bilevel%20Learning%20with%20Inexact%20Stochastic%20Gradients&entry.906535625=Mohammad%20Sadegh%20Salehi%20and%20Subhadip%20Mukherjee%20and%20Lindon%20Roberts%20and%20Matthias%20J.%20Ehrhardt&entry.1292438233=%20%20Bilevel%20learning%20has%20gained%20prominence%20in%20machine%20learning%2C%20inverse%20problems%2C%0Aand%20imaging%20applications%2C%20including%20hyperparameter%20optimization%2C%20learning%0Adata-adaptive%20regularizers%2C%20and%20optimizing%20forward%20operators.%20The%20large-scale%0Anature%20of%20these%20problems%20has%20led%20to%20the%20development%20of%20inexact%20and%0Acomputationally%20efficient%20methods.%20Existing%20adaptive%20methods%20predominantly%20rely%0Aon%20deterministic%20formulations%2C%20while%20stochastic%20approaches%20often%20adopt%20a%0Adoubly-stochastic%20framework%20with%20impractical%20variance%20assumptions%2C%20enforces%20a%0Afixed%20number%20of%20lower-level%20iterations%2C%20and%20requires%20extensive%20tuning.%20In%20this%0Awork%2C%20we%20focus%20on%20bilevel%20learning%20with%20strongly%20convex%20lower-level%20problems%0Aand%20a%20nonconvex%20sum-of-functions%20in%20the%20upper-level.%20Stochasticity%20arises%20from%0Adata%20sampling%20in%20the%20upper-level%20which%20leads%20to%20inexact%20stochastic%0Ahypergradients.%20We%20establish%20their%20connection%20to%20state-of-the-art%20stochastic%0Aoptimization%20theory%20for%20nonconvex%20objectives.%20Furthermore%2C%20we%20prove%20the%0Aconvergence%20of%20inexact%20stochastic%20bilevel%20optimization%20under%20mild%20assumptions.%0AOur%20empirical%20results%20highlight%20significant%20speed-ups%20and%20improved%0Ageneralization%20in%20imaging%20tasks%20such%20as%20image%20denoising%20and%20deblurring%20in%0Acomparison%20with%20adaptive%20deterministic%20bilevel%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12049v1&entry.124074799=Read"},
{"title": "Stepwise Reasoning Error Disruption Attack of LLMs", "author": "Jingyu Peng and Maolin Wang and Xiangyu Zhao and Kai Zhang and Wanyu Wang and Pengyue Jia and Qidong Liu and Ruocheng Guo and Qi Liu", "abstract": "  Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications.\n", "link": "http://arxiv.org/abs/2412.11934v1", "date": "2024-12-16", "relevancy": 1.921, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stepwise%20Reasoning%20Error%20Disruption%20Attack%20of%20LLMs&body=Title%3A%20Stepwise%20Reasoning%20Error%20Disruption%20Attack%20of%20LLMs%0AAuthor%3A%20Jingyu%20Peng%20and%20Maolin%20Wang%20and%20Xiangyu%20Zhao%20and%20Kai%20Zhang%20and%20Wanyu%20Wang%20and%20Pengyue%20Jia%20and%20Qidong%20Liu%20and%20Ruocheng%20Guo%20and%20Qi%20Liu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20made%20remarkable%20strides%20in%20complex%0Areasoning%20tasks%2C%20but%20their%20safety%20and%20robustness%20in%20reasoning%20processes%20remain%0Aunderexplored.%20Existing%20attacks%20on%20LLM%20reasoning%20are%20constrained%20by%20specific%0Asettings%20or%20lack%20of%20imperceptibility%2C%20limiting%20their%20feasibility%20and%0Ageneralizability.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20Stepwise%0ArEasoning%20Error%20Disruption%20%28SEED%29%20attack%2C%20which%20subtly%20injects%20errors%20into%0Aprior%20reasoning%20steps%20to%20mislead%20the%20model%20into%20producing%20incorrect%20subsequent%0Areasoning%20and%20final%20answers.%20Unlike%20previous%20methods%2C%20SEED%20is%20compatible%20with%0Azero-shot%20and%20few-shot%20settings%2C%20maintains%20the%20natural%20reasoning%20flow%2C%20and%0Aensures%20covert%20execution%20without%20modifying%20the%20instruction.%20Extensive%0Aexperiments%20on%20four%20datasets%20across%20four%20different%20models%20demonstrate%20SEED%27s%0Aeffectiveness%2C%20revealing%20the%20vulnerabilities%20of%20LLMs%20to%20disruptions%20in%0Areasoning%20processes.%20These%20findings%20underscore%20the%20need%20for%20greater%20attention%0Ato%20the%20robustness%20of%20LLM%20reasoning%20to%20ensure%20safety%20in%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepwise%2520Reasoning%2520Error%2520Disruption%2520Attack%2520of%2520LLMs%26entry.906535625%3DJingyu%2520Peng%2520and%2520Maolin%2520Wang%2520and%2520Xiangyu%2520Zhao%2520and%2520Kai%2520Zhang%2520and%2520Wanyu%2520Wang%2520and%2520Pengyue%2520Jia%2520and%2520Qidong%2520Liu%2520and%2520Ruocheng%2520Guo%2520and%2520Qi%2520Liu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520remarkable%2520strides%2520in%2520complex%250Areasoning%2520tasks%252C%2520but%2520their%2520safety%2520and%2520robustness%2520in%2520reasoning%2520processes%2520remain%250Aunderexplored.%2520Existing%2520attacks%2520on%2520LLM%2520reasoning%2520are%2520constrained%2520by%2520specific%250Asettings%2520or%2520lack%2520of%2520imperceptibility%252C%2520limiting%2520their%2520feasibility%2520and%250Ageneralizability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520Stepwise%250ArEasoning%2520Error%2520Disruption%2520%2528SEED%2529%2520attack%252C%2520which%2520subtly%2520injects%2520errors%2520into%250Aprior%2520reasoning%2520steps%2520to%2520mislead%2520the%2520model%2520into%2520producing%2520incorrect%2520subsequent%250Areasoning%2520and%2520final%2520answers.%2520Unlike%2520previous%2520methods%252C%2520SEED%2520is%2520compatible%2520with%250Azero-shot%2520and%2520few-shot%2520settings%252C%2520maintains%2520the%2520natural%2520reasoning%2520flow%252C%2520and%250Aensures%2520covert%2520execution%2520without%2520modifying%2520the%2520instruction.%2520Extensive%250Aexperiments%2520on%2520four%2520datasets%2520across%2520four%2520different%2520models%2520demonstrate%2520SEED%2527s%250Aeffectiveness%252C%2520revealing%2520the%2520vulnerabilities%2520of%2520LLMs%2520to%2520disruptions%2520in%250Areasoning%2520processes.%2520These%2520findings%2520underscore%2520the%2520need%2520for%2520greater%2520attention%250Ato%2520the%2520robustness%2520of%2520LLM%2520reasoning%2520to%2520ensure%2520safety%2520in%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stepwise%20Reasoning%20Error%20Disruption%20Attack%20of%20LLMs&entry.906535625=Jingyu%20Peng%20and%20Maolin%20Wang%20and%20Xiangyu%20Zhao%20and%20Kai%20Zhang%20and%20Wanyu%20Wang%20and%20Pengyue%20Jia%20and%20Qidong%20Liu%20and%20Ruocheng%20Guo%20and%20Qi%20Liu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20made%20remarkable%20strides%20in%20complex%0Areasoning%20tasks%2C%20but%20their%20safety%20and%20robustness%20in%20reasoning%20processes%20remain%0Aunderexplored.%20Existing%20attacks%20on%20LLM%20reasoning%20are%20constrained%20by%20specific%0Asettings%20or%20lack%20of%20imperceptibility%2C%20limiting%20their%20feasibility%20and%0Ageneralizability.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20Stepwise%0ArEasoning%20Error%20Disruption%20%28SEED%29%20attack%2C%20which%20subtly%20injects%20errors%20into%0Aprior%20reasoning%20steps%20to%20mislead%20the%20model%20into%20producing%20incorrect%20subsequent%0Areasoning%20and%20final%20answers.%20Unlike%20previous%20methods%2C%20SEED%20is%20compatible%20with%0Azero-shot%20and%20few-shot%20settings%2C%20maintains%20the%20natural%20reasoning%20flow%2C%20and%0Aensures%20covert%20execution%20without%20modifying%20the%20instruction.%20Extensive%0Aexperiments%20on%20four%20datasets%20across%20four%20different%20models%20demonstrate%20SEED%27s%0Aeffectiveness%2C%20revealing%20the%20vulnerabilities%20of%20LLMs%20to%20disruptions%20in%0Areasoning%20processes.%20These%20findings%20underscore%20the%20need%20for%20greater%20attention%0Ato%20the%20robustness%20of%20LLM%20reasoning%20to%20ensure%20safety%20in%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11934v1&entry.124074799=Read"},
{"title": "No More Tuning: Prioritized Multi-Task Learning with Lagrangian\n  Differential Multiplier Methods", "author": "Zhengxing Cheng and Yuheng Huang and Zhixuan Zhang and Dan Ou and Qingwen Liu", "abstract": "  Given the ubiquity of multi-task in practical systems, Multi-Task Learning\n(MTL) has found widespread application across diverse domains. In real-world\nscenarios, these tasks often have different priorities. For instance, In web\nsearch, relevance is often prioritized over other metrics, such as\nclick-through rates or user engagement. Existing frameworks pay insufficient\nattention to the prioritization among different tasks, which typically adjust\ntask-specific loss function weights to differentiate task priorities. However,\nthis approach encounters challenges as the number of tasks grows, leading to\nexponential increases in hyper-parameter tuning complexity. Furthermore, the\nsimultaneous optimization of multiple objectives can negatively impact the\nperformance of high-priority tasks due to interference from lower-priority\ntasks.\n  In this paper, we introduce a novel multi-task learning framework employing\nLagrangian Differential Multiplier Methods for step-wise multi-task\noptimization. It is designed to boost the performance of high-priority tasks\nwithout interference from other tasks. Its primary advantage lies in its\nability to automatically optimize multiple objectives without requiring\nbalancing hyper-parameters for different tasks, thereby eliminating the need\nfor manual tuning. Additionally, we provide theoretical analysis demonstrating\nthat our method ensures optimization guarantees, enhancing the reliability of\nthe process. We demonstrate its effectiveness through experiments on multiple\npublic datasets and its application in Taobao search, a large-scale industrial\nsearch ranking system, resulting in significant improvements across various\nbusiness metrics.\n", "link": "http://arxiv.org/abs/2412.12092v1", "date": "2024-12-16", "relevancy": 1.914, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4877}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20More%20Tuning%3A%20Prioritized%20Multi-Task%20Learning%20with%20Lagrangian%0A%20%20Differential%20Multiplier%20Methods&body=Title%3A%20No%20More%20Tuning%3A%20Prioritized%20Multi-Task%20Learning%20with%20Lagrangian%0A%20%20Differential%20Multiplier%20Methods%0AAuthor%3A%20Zhengxing%20Cheng%20and%20Yuheng%20Huang%20and%20Zhixuan%20Zhang%20and%20Dan%20Ou%20and%20Qingwen%20Liu%0AAbstract%3A%20%20%20Given%20the%20ubiquity%20of%20multi-task%20in%20practical%20systems%2C%20Multi-Task%20Learning%0A%28MTL%29%20has%20found%20widespread%20application%20across%20diverse%20domains.%20In%20real-world%0Ascenarios%2C%20these%20tasks%20often%20have%20different%20priorities.%20For%20instance%2C%20In%20web%0Asearch%2C%20relevance%20is%20often%20prioritized%20over%20other%20metrics%2C%20such%20as%0Aclick-through%20rates%20or%20user%20engagement.%20Existing%20frameworks%20pay%20insufficient%0Aattention%20to%20the%20prioritization%20among%20different%20tasks%2C%20which%20typically%20adjust%0Atask-specific%20loss%20function%20weights%20to%20differentiate%20task%20priorities.%20However%2C%0Athis%20approach%20encounters%20challenges%20as%20the%20number%20of%20tasks%20grows%2C%20leading%20to%0Aexponential%20increases%20in%20hyper-parameter%20tuning%20complexity.%20Furthermore%2C%20the%0Asimultaneous%20optimization%20of%20multiple%20objectives%20can%20negatively%20impact%20the%0Aperformance%20of%20high-priority%20tasks%20due%20to%20interference%20from%20lower-priority%0Atasks.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20multi-task%20learning%20framework%20employing%0ALagrangian%20Differential%20Multiplier%20Methods%20for%20step-wise%20multi-task%0Aoptimization.%20It%20is%20designed%20to%20boost%20the%20performance%20of%20high-priority%20tasks%0Awithout%20interference%20from%20other%20tasks.%20Its%20primary%20advantage%20lies%20in%20its%0Aability%20to%20automatically%20optimize%20multiple%20objectives%20without%20requiring%0Abalancing%20hyper-parameters%20for%20different%20tasks%2C%20thereby%20eliminating%20the%20need%0Afor%20manual%20tuning.%20Additionally%2C%20we%20provide%20theoretical%20analysis%20demonstrating%0Athat%20our%20method%20ensures%20optimization%20guarantees%2C%20enhancing%20the%20reliability%20of%0Athe%20process.%20We%20demonstrate%20its%20effectiveness%20through%20experiments%20on%20multiple%0Apublic%20datasets%20and%20its%20application%20in%20Taobao%20search%2C%20a%20large-scale%20industrial%0Asearch%20ranking%20system%2C%20resulting%20in%20significant%20improvements%20across%20various%0Abusiness%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520More%2520Tuning%253A%2520Prioritized%2520Multi-Task%2520Learning%2520with%2520Lagrangian%250A%2520%2520Differential%2520Multiplier%2520Methods%26entry.906535625%3DZhengxing%2520Cheng%2520and%2520Yuheng%2520Huang%2520and%2520Zhixuan%2520Zhang%2520and%2520Dan%2520Ou%2520and%2520Qingwen%2520Liu%26entry.1292438233%3D%2520%2520Given%2520the%2520ubiquity%2520of%2520multi-task%2520in%2520practical%2520systems%252C%2520Multi-Task%2520Learning%250A%2528MTL%2529%2520has%2520found%2520widespread%2520application%2520across%2520diverse%2520domains.%2520In%2520real-world%250Ascenarios%252C%2520these%2520tasks%2520often%2520have%2520different%2520priorities.%2520For%2520instance%252C%2520In%2520web%250Asearch%252C%2520relevance%2520is%2520often%2520prioritized%2520over%2520other%2520metrics%252C%2520such%2520as%250Aclick-through%2520rates%2520or%2520user%2520engagement.%2520Existing%2520frameworks%2520pay%2520insufficient%250Aattention%2520to%2520the%2520prioritization%2520among%2520different%2520tasks%252C%2520which%2520typically%2520adjust%250Atask-specific%2520loss%2520function%2520weights%2520to%2520differentiate%2520task%2520priorities.%2520However%252C%250Athis%2520approach%2520encounters%2520challenges%2520as%2520the%2520number%2520of%2520tasks%2520grows%252C%2520leading%2520to%250Aexponential%2520increases%2520in%2520hyper-parameter%2520tuning%2520complexity.%2520Furthermore%252C%2520the%250Asimultaneous%2520optimization%2520of%2520multiple%2520objectives%2520can%2520negatively%2520impact%2520the%250Aperformance%2520of%2520high-priority%2520tasks%2520due%2520to%2520interference%2520from%2520lower-priority%250Atasks.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520multi-task%2520learning%2520framework%2520employing%250ALagrangian%2520Differential%2520Multiplier%2520Methods%2520for%2520step-wise%2520multi-task%250Aoptimization.%2520It%2520is%2520designed%2520to%2520boost%2520the%2520performance%2520of%2520high-priority%2520tasks%250Awithout%2520interference%2520from%2520other%2520tasks.%2520Its%2520primary%2520advantage%2520lies%2520in%2520its%250Aability%2520to%2520automatically%2520optimize%2520multiple%2520objectives%2520without%2520requiring%250Abalancing%2520hyper-parameters%2520for%2520different%2520tasks%252C%2520thereby%2520eliminating%2520the%2520need%250Afor%2520manual%2520tuning.%2520Additionally%252C%2520we%2520provide%2520theoretical%2520analysis%2520demonstrating%250Athat%2520our%2520method%2520ensures%2520optimization%2520guarantees%252C%2520enhancing%2520the%2520reliability%2520of%250Athe%2520process.%2520We%2520demonstrate%2520its%2520effectiveness%2520through%2520experiments%2520on%2520multiple%250Apublic%2520datasets%2520and%2520its%2520application%2520in%2520Taobao%2520search%252C%2520a%2520large-scale%2520industrial%250Asearch%2520ranking%2520system%252C%2520resulting%2520in%2520significant%2520improvements%2520across%2520various%250Abusiness%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20More%20Tuning%3A%20Prioritized%20Multi-Task%20Learning%20with%20Lagrangian%0A%20%20Differential%20Multiplier%20Methods&entry.906535625=Zhengxing%20Cheng%20and%20Yuheng%20Huang%20and%20Zhixuan%20Zhang%20and%20Dan%20Ou%20and%20Qingwen%20Liu&entry.1292438233=%20%20Given%20the%20ubiquity%20of%20multi-task%20in%20practical%20systems%2C%20Multi-Task%20Learning%0A%28MTL%29%20has%20found%20widespread%20application%20across%20diverse%20domains.%20In%20real-world%0Ascenarios%2C%20these%20tasks%20often%20have%20different%20priorities.%20For%20instance%2C%20In%20web%0Asearch%2C%20relevance%20is%20often%20prioritized%20over%20other%20metrics%2C%20such%20as%0Aclick-through%20rates%20or%20user%20engagement.%20Existing%20frameworks%20pay%20insufficient%0Aattention%20to%20the%20prioritization%20among%20different%20tasks%2C%20which%20typically%20adjust%0Atask-specific%20loss%20function%20weights%20to%20differentiate%20task%20priorities.%20However%2C%0Athis%20approach%20encounters%20challenges%20as%20the%20number%20of%20tasks%20grows%2C%20leading%20to%0Aexponential%20increases%20in%20hyper-parameter%20tuning%20complexity.%20Furthermore%2C%20the%0Asimultaneous%20optimization%20of%20multiple%20objectives%20can%20negatively%20impact%20the%0Aperformance%20of%20high-priority%20tasks%20due%20to%20interference%20from%20lower-priority%0Atasks.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20multi-task%20learning%20framework%20employing%0ALagrangian%20Differential%20Multiplier%20Methods%20for%20step-wise%20multi-task%0Aoptimization.%20It%20is%20designed%20to%20boost%20the%20performance%20of%20high-priority%20tasks%0Awithout%20interference%20from%20other%20tasks.%20Its%20primary%20advantage%20lies%20in%20its%0Aability%20to%20automatically%20optimize%20multiple%20objectives%20without%20requiring%0Abalancing%20hyper-parameters%20for%20different%20tasks%2C%20thereby%20eliminating%20the%20need%0Afor%20manual%20tuning.%20Additionally%2C%20we%20provide%20theoretical%20analysis%20demonstrating%0Athat%20our%20method%20ensures%20optimization%20guarantees%2C%20enhancing%20the%20reliability%20of%0Athe%20process.%20We%20demonstrate%20its%20effectiveness%20through%20experiments%20on%20multiple%0Apublic%20datasets%20and%20its%20application%20in%20Taobao%20search%2C%20a%20large-scale%20industrial%0Asearch%20ranking%20system%2C%20resulting%20in%20significant%20improvements%20across%20various%0Abusiness%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12092v1&entry.124074799=Read"},
{"title": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations", "author": "Huaizhi Ge and Yiming Li and Qifan Wang and Yongfeng Zhang and Ruixiang Tang", "abstract": "  Large Language Models (LLMs) are known to be vulnerable to backdoor attacks,\nwhere triggers embedded in poisoned samples can maliciously alter LLMs'\nbehaviors. In this paper, we move beyond attacking LLMs and instead examine\nbackdoor attacks through the novel lens of natural language explanations.\nSpecifically, we leverage LLMs' generative capabilities to produce\nhuman-readable explanations for their decisions, enabling direct comparisons\nbetween explanations for clean and poisoned samples. Our results show that\nbackdoored models produce coherent explanations for clean inputs but diverse\nand logically flawed explanations for poisoned data, a pattern consistent\nacross classification and generation tasks for different backdoor attacks.\nFurther analysis reveals key insights into the explanation generation process.\nAt the token level, explanation tokens associated with poisoned samples only\nappear in the final few transformer layers. At the sentence level, attention\ndynamics indicate that poisoned inputs shift attention away from the original\ninput context during explanation generation. These findings enhance our\nunderstanding of backdoor mechanisms in LLMs and present a promising framework\nfor detecting vulnerabilities through explainability.\n", "link": "http://arxiv.org/abs/2411.12701v2", "date": "2024-12-16", "relevancy": 1.9093, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Backdoors%20Speak%3A%20Understanding%20LLM%20Backdoor%20Attacks%20Through%0A%20%20Model-Generated%20Explanations&body=Title%3A%20When%20Backdoors%20Speak%3A%20Understanding%20LLM%20Backdoor%20Attacks%20Through%0A%20%20Model-Generated%20Explanations%0AAuthor%3A%20Huaizhi%20Ge%20and%20Yiming%20Li%20and%20Qifan%20Wang%20and%20Yongfeng%20Zhang%20and%20Ruixiang%20Tang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20known%20to%20be%20vulnerable%20to%20backdoor%20attacks%2C%0Awhere%20triggers%20embedded%20in%20poisoned%20samples%20can%20maliciously%20alter%20LLMs%27%0Abehaviors.%20In%20this%20paper%2C%20we%20move%20beyond%20attacking%20LLMs%20and%20instead%20examine%0Abackdoor%20attacks%20through%20the%20novel%20lens%20of%20natural%20language%20explanations.%0ASpecifically%2C%20we%20leverage%20LLMs%27%20generative%20capabilities%20to%20produce%0Ahuman-readable%20explanations%20for%20their%20decisions%2C%20enabling%20direct%20comparisons%0Abetween%20explanations%20for%20clean%20and%20poisoned%20samples.%20Our%20results%20show%20that%0Abackdoored%20models%20produce%20coherent%20explanations%20for%20clean%20inputs%20but%20diverse%0Aand%20logically%20flawed%20explanations%20for%20poisoned%20data%2C%20a%20pattern%20consistent%0Aacross%20classification%20and%20generation%20tasks%20for%20different%20backdoor%20attacks.%0AFurther%20analysis%20reveals%20key%20insights%20into%20the%20explanation%20generation%20process.%0AAt%20the%20token%20level%2C%20explanation%20tokens%20associated%20with%20poisoned%20samples%20only%0Aappear%20in%20the%20final%20few%20transformer%20layers.%20At%20the%20sentence%20level%2C%20attention%0Adynamics%20indicate%20that%20poisoned%20inputs%20shift%20attention%20away%20from%20the%20original%0Ainput%20context%20during%20explanation%20generation.%20These%20findings%20enhance%20our%0Aunderstanding%20of%20backdoor%20mechanisms%20in%20LLMs%20and%20present%20a%20promising%20framework%0Afor%20detecting%20vulnerabilities%20through%20explainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Backdoors%2520Speak%253A%2520Understanding%2520LLM%2520Backdoor%2520Attacks%2520Through%250A%2520%2520Model-Generated%2520Explanations%26entry.906535625%3DHuaizhi%2520Ge%2520and%2520Yiming%2520Li%2520and%2520Qifan%2520Wang%2520and%2520Yongfeng%2520Zhang%2520and%2520Ruixiang%2520Tang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520known%2520to%2520be%2520vulnerable%2520to%2520backdoor%2520attacks%252C%250Awhere%2520triggers%2520embedded%2520in%2520poisoned%2520samples%2520can%2520maliciously%2520alter%2520LLMs%2527%250Abehaviors.%2520In%2520this%2520paper%252C%2520we%2520move%2520beyond%2520attacking%2520LLMs%2520and%2520instead%2520examine%250Abackdoor%2520attacks%2520through%2520the%2520novel%2520lens%2520of%2520natural%2520language%2520explanations.%250ASpecifically%252C%2520we%2520leverage%2520LLMs%2527%2520generative%2520capabilities%2520to%2520produce%250Ahuman-readable%2520explanations%2520for%2520their%2520decisions%252C%2520enabling%2520direct%2520comparisons%250Abetween%2520explanations%2520for%2520clean%2520and%2520poisoned%2520samples.%2520Our%2520results%2520show%2520that%250Abackdoored%2520models%2520produce%2520coherent%2520explanations%2520for%2520clean%2520inputs%2520but%2520diverse%250Aand%2520logically%2520flawed%2520explanations%2520for%2520poisoned%2520data%252C%2520a%2520pattern%2520consistent%250Aacross%2520classification%2520and%2520generation%2520tasks%2520for%2520different%2520backdoor%2520attacks.%250AFurther%2520analysis%2520reveals%2520key%2520insights%2520into%2520the%2520explanation%2520generation%2520process.%250AAt%2520the%2520token%2520level%252C%2520explanation%2520tokens%2520associated%2520with%2520poisoned%2520samples%2520only%250Aappear%2520in%2520the%2520final%2520few%2520transformer%2520layers.%2520At%2520the%2520sentence%2520level%252C%2520attention%250Adynamics%2520indicate%2520that%2520poisoned%2520inputs%2520shift%2520attention%2520away%2520from%2520the%2520original%250Ainput%2520context%2520during%2520explanation%2520generation.%2520These%2520findings%2520enhance%2520our%250Aunderstanding%2520of%2520backdoor%2520mechanisms%2520in%2520LLMs%2520and%2520present%2520a%2520promising%2520framework%250Afor%2520detecting%2520vulnerabilities%2520through%2520explainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Backdoors%20Speak%3A%20Understanding%20LLM%20Backdoor%20Attacks%20Through%0A%20%20Model-Generated%20Explanations&entry.906535625=Huaizhi%20Ge%20and%20Yiming%20Li%20and%20Qifan%20Wang%20and%20Yongfeng%20Zhang%20and%20Ruixiang%20Tang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20known%20to%20be%20vulnerable%20to%20backdoor%20attacks%2C%0Awhere%20triggers%20embedded%20in%20poisoned%20samples%20can%20maliciously%20alter%20LLMs%27%0Abehaviors.%20In%20this%20paper%2C%20we%20move%20beyond%20attacking%20LLMs%20and%20instead%20examine%0Abackdoor%20attacks%20through%20the%20novel%20lens%20of%20natural%20language%20explanations.%0ASpecifically%2C%20we%20leverage%20LLMs%27%20generative%20capabilities%20to%20produce%0Ahuman-readable%20explanations%20for%20their%20decisions%2C%20enabling%20direct%20comparisons%0Abetween%20explanations%20for%20clean%20and%20poisoned%20samples.%20Our%20results%20show%20that%0Abackdoored%20models%20produce%20coherent%20explanations%20for%20clean%20inputs%20but%20diverse%0Aand%20logically%20flawed%20explanations%20for%20poisoned%20data%2C%20a%20pattern%20consistent%0Aacross%20classification%20and%20generation%20tasks%20for%20different%20backdoor%20attacks.%0AFurther%20analysis%20reveals%20key%20insights%20into%20the%20explanation%20generation%20process.%0AAt%20the%20token%20level%2C%20explanation%20tokens%20associated%20with%20poisoned%20samples%20only%0Aappear%20in%20the%20final%20few%20transformer%20layers.%20At%20the%20sentence%20level%2C%20attention%0Adynamics%20indicate%20that%20poisoned%20inputs%20shift%20attention%20away%20from%20the%20original%0Ainput%20context%20during%20explanation%20generation.%20These%20findings%20enhance%20our%0Aunderstanding%20of%20backdoor%20mechanisms%20in%20LLMs%20and%20present%20a%20promising%20framework%0Afor%20detecting%20vulnerabilities%20through%20explainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12701v2&entry.124074799=Read"},
{"title": "Revelations: A Decidable Class of POMDPs with Omega-Regular Objectives", "author": "Marius Belly and Nathana\u00ebl Fijalkow and Hugo Gimbert and Florian Horn and Guillermo A. P\u00e9rez and Pierre Vandenhove", "abstract": "  Partially observable Markov decision processes (POMDPs) form a prominent\nmodel for uncertainty in sequential decision making. We are interested in\nconstructing algorithms with theoretical guarantees to determine whether the\nagent has a strategy ensuring a given specification with probability 1. This\nwell-studied problem is known to be undecidable already for very simple\nomega-regular objectives, because of the difficulty of reasoning on uncertain\nevents. We introduce a revelation mechanism which restricts information loss by\nrequiring that almost surely the agent has eventually full information of the\ncurrent state. Our main technical results are to construct exact algorithms for\ntwo classes of POMDPs called weakly and strongly revealing. Importantly, the\ndecidable cases reduce to the analysis of a finite belief-support Markov\ndecision process. This yields a conceptually simple and exact algorithm for a\nlarge class of POMDPs.\n", "link": "http://arxiv.org/abs/2412.12063v1", "date": "2024-12-16", "relevancy": 1.905, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4991}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4723}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revelations%3A%20A%20Decidable%20Class%20of%20POMDPs%20with%20Omega-Regular%20Objectives&body=Title%3A%20Revelations%3A%20A%20Decidable%20Class%20of%20POMDPs%20with%20Omega-Regular%20Objectives%0AAuthor%3A%20Marius%20Belly%20and%20Nathana%C3%ABl%20Fijalkow%20and%20Hugo%20Gimbert%20and%20Florian%20Horn%20and%20Guillermo%20A.%20P%C3%A9rez%20and%20Pierre%20Vandenhove%0AAbstract%3A%20%20%20Partially%20observable%20Markov%20decision%20processes%20%28POMDPs%29%20form%20a%20prominent%0Amodel%20for%20uncertainty%20in%20sequential%20decision%20making.%20We%20are%20interested%20in%0Aconstructing%20algorithms%20with%20theoretical%20guarantees%20to%20determine%20whether%20the%0Aagent%20has%20a%20strategy%20ensuring%20a%20given%20specification%20with%20probability%201.%20This%0Awell-studied%20problem%20is%20known%20to%20be%20undecidable%20already%20for%20very%20simple%0Aomega-regular%20objectives%2C%20because%20of%20the%20difficulty%20of%20reasoning%20on%20uncertain%0Aevents.%20We%20introduce%20a%20revelation%20mechanism%20which%20restricts%20information%20loss%20by%0Arequiring%20that%20almost%20surely%20the%20agent%20has%20eventually%20full%20information%20of%20the%0Acurrent%20state.%20Our%20main%20technical%20results%20are%20to%20construct%20exact%20algorithms%20for%0Atwo%20classes%20of%20POMDPs%20called%20weakly%20and%20strongly%20revealing.%20Importantly%2C%20the%0Adecidable%20cases%20reduce%20to%20the%20analysis%20of%20a%20finite%20belief-support%20Markov%0Adecision%20process.%20This%20yields%20a%20conceptually%20simple%20and%20exact%20algorithm%20for%20a%0Alarge%20class%20of%20POMDPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevelations%253A%2520A%2520Decidable%2520Class%2520of%2520POMDPs%2520with%2520Omega-Regular%2520Objectives%26entry.906535625%3DMarius%2520Belly%2520and%2520Nathana%25C3%25ABl%2520Fijalkow%2520and%2520Hugo%2520Gimbert%2520and%2520Florian%2520Horn%2520and%2520Guillermo%2520A.%2520P%25C3%25A9rez%2520and%2520Pierre%2520Vandenhove%26entry.1292438233%3D%2520%2520Partially%2520observable%2520Markov%2520decision%2520processes%2520%2528POMDPs%2529%2520form%2520a%2520prominent%250Amodel%2520for%2520uncertainty%2520in%2520sequential%2520decision%2520making.%2520We%2520are%2520interested%2520in%250Aconstructing%2520algorithms%2520with%2520theoretical%2520guarantees%2520to%2520determine%2520whether%2520the%250Aagent%2520has%2520a%2520strategy%2520ensuring%2520a%2520given%2520specification%2520with%2520probability%25201.%2520This%250Awell-studied%2520problem%2520is%2520known%2520to%2520be%2520undecidable%2520already%2520for%2520very%2520simple%250Aomega-regular%2520objectives%252C%2520because%2520of%2520the%2520difficulty%2520of%2520reasoning%2520on%2520uncertain%250Aevents.%2520We%2520introduce%2520a%2520revelation%2520mechanism%2520which%2520restricts%2520information%2520loss%2520by%250Arequiring%2520that%2520almost%2520surely%2520the%2520agent%2520has%2520eventually%2520full%2520information%2520of%2520the%250Acurrent%2520state.%2520Our%2520main%2520technical%2520results%2520are%2520to%2520construct%2520exact%2520algorithms%2520for%250Atwo%2520classes%2520of%2520POMDPs%2520called%2520weakly%2520and%2520strongly%2520revealing.%2520Importantly%252C%2520the%250Adecidable%2520cases%2520reduce%2520to%2520the%2520analysis%2520of%2520a%2520finite%2520belief-support%2520Markov%250Adecision%2520process.%2520This%2520yields%2520a%2520conceptually%2520simple%2520and%2520exact%2520algorithm%2520for%2520a%250Alarge%2520class%2520of%2520POMDPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revelations%3A%20A%20Decidable%20Class%20of%20POMDPs%20with%20Omega-Regular%20Objectives&entry.906535625=Marius%20Belly%20and%20Nathana%C3%ABl%20Fijalkow%20and%20Hugo%20Gimbert%20and%20Florian%20Horn%20and%20Guillermo%20A.%20P%C3%A9rez%20and%20Pierre%20Vandenhove&entry.1292438233=%20%20Partially%20observable%20Markov%20decision%20processes%20%28POMDPs%29%20form%20a%20prominent%0Amodel%20for%20uncertainty%20in%20sequential%20decision%20making.%20We%20are%20interested%20in%0Aconstructing%20algorithms%20with%20theoretical%20guarantees%20to%20determine%20whether%20the%0Aagent%20has%20a%20strategy%20ensuring%20a%20given%20specification%20with%20probability%201.%20This%0Awell-studied%20problem%20is%20known%20to%20be%20undecidable%20already%20for%20very%20simple%0Aomega-regular%20objectives%2C%20because%20of%20the%20difficulty%20of%20reasoning%20on%20uncertain%0Aevents.%20We%20introduce%20a%20revelation%20mechanism%20which%20restricts%20information%20loss%20by%0Arequiring%20that%20almost%20surely%20the%20agent%20has%20eventually%20full%20information%20of%20the%0Acurrent%20state.%20Our%20main%20technical%20results%20are%20to%20construct%20exact%20algorithms%20for%0Atwo%20classes%20of%20POMDPs%20called%20weakly%20and%20strongly%20revealing.%20Importantly%2C%20the%0Adecidable%20cases%20reduce%20to%20the%20analysis%20of%20a%20finite%20belief-support%20Markov%0Adecision%20process.%20This%20yields%20a%20conceptually%20simple%20and%20exact%20algorithm%20for%20a%0Alarge%20class%20of%20POMDPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12063v1&entry.124074799=Read"},
{"title": "Controllable Shadow Generation with Single-Step Diffusion Models from\n  Synthetic Data", "author": "Onur Tasar and Cl\u00e9ment Chadebec and Benjamin Aubin", "abstract": "  Realistic shadow generation is a critical component for high-quality image\ncompositing and visual effects, yet existing methods suffer from certain\nlimitations: Physics-based approaches require a 3D scene geometry, which is\noften unavailable, while learning-based techniques struggle with control and\nvisual artifacts. We introduce a novel method for fast, controllable, and\nbackground-free shadow generation for 2D object images. We create a large\nsynthetic dataset using a 3D rendering engine to train a diffusion model for\ncontrollable shadow generation, generating shadow maps for diverse light source\nparameters. Through extensive ablation studies, we find that rectified flow\nobjective achieves high-quality results with just a single sampling step\nenabling real-time applications. Furthermore, our experiments demonstrate that\nthe model generalizes well to real-world images. To facilitate further research\nin evaluating quality and controllability in shadow generation, we release a\nnew public benchmark containing a diverse set of object images and shadow maps\nin various settings. The project page is available at\nhttps://gojasper.github.io/controllable-shadow-generation-project/\n", "link": "http://arxiv.org/abs/2412.11972v1", "date": "2024-12-16", "relevancy": 1.8976, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6998}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.622}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Shadow%20Generation%20with%20Single-Step%20Diffusion%20Models%20from%0A%20%20Synthetic%20Data&body=Title%3A%20Controllable%20Shadow%20Generation%20with%20Single-Step%20Diffusion%20Models%20from%0A%20%20Synthetic%20Data%0AAuthor%3A%20Onur%20Tasar%20and%20Cl%C3%A9ment%20Chadebec%20and%20Benjamin%20Aubin%0AAbstract%3A%20%20%20Realistic%20shadow%20generation%20is%20a%20critical%20component%20for%20high-quality%20image%0Acompositing%20and%20visual%20effects%2C%20yet%20existing%20methods%20suffer%20from%20certain%0Alimitations%3A%20Physics-based%20approaches%20require%20a%203D%20scene%20geometry%2C%20which%20is%0Aoften%20unavailable%2C%20while%20learning-based%20techniques%20struggle%20with%20control%20and%0Avisual%20artifacts.%20We%20introduce%20a%20novel%20method%20for%20fast%2C%20controllable%2C%20and%0Abackground-free%20shadow%20generation%20for%202D%20object%20images.%20We%20create%20a%20large%0Asynthetic%20dataset%20using%20a%203D%20rendering%20engine%20to%20train%20a%20diffusion%20model%20for%0Acontrollable%20shadow%20generation%2C%20generating%20shadow%20maps%20for%20diverse%20light%20source%0Aparameters.%20Through%20extensive%20ablation%20studies%2C%20we%20find%20that%20rectified%20flow%0Aobjective%20achieves%20high-quality%20results%20with%20just%20a%20single%20sampling%20step%0Aenabling%20real-time%20applications.%20Furthermore%2C%20our%20experiments%20demonstrate%20that%0Athe%20model%20generalizes%20well%20to%20real-world%20images.%20To%20facilitate%20further%20research%0Ain%20evaluating%20quality%20and%20controllability%20in%20shadow%20generation%2C%20we%20release%20a%0Anew%20public%20benchmark%20containing%20a%20diverse%20set%20of%20object%20images%20and%20shadow%20maps%0Ain%20various%20settings.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//gojasper.github.io/controllable-shadow-generation-project/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Shadow%2520Generation%2520with%2520Single-Step%2520Diffusion%2520Models%2520from%250A%2520%2520Synthetic%2520Data%26entry.906535625%3DOnur%2520Tasar%2520and%2520Cl%25C3%25A9ment%2520Chadebec%2520and%2520Benjamin%2520Aubin%26entry.1292438233%3D%2520%2520Realistic%2520shadow%2520generation%2520is%2520a%2520critical%2520component%2520for%2520high-quality%2520image%250Acompositing%2520and%2520visual%2520effects%252C%2520yet%2520existing%2520methods%2520suffer%2520from%2520certain%250Alimitations%253A%2520Physics-based%2520approaches%2520require%2520a%25203D%2520scene%2520geometry%252C%2520which%2520is%250Aoften%2520unavailable%252C%2520while%2520learning-based%2520techniques%2520struggle%2520with%2520control%2520and%250Avisual%2520artifacts.%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520fast%252C%2520controllable%252C%2520and%250Abackground-free%2520shadow%2520generation%2520for%25202D%2520object%2520images.%2520We%2520create%2520a%2520large%250Asynthetic%2520dataset%2520using%2520a%25203D%2520rendering%2520engine%2520to%2520train%2520a%2520diffusion%2520model%2520for%250Acontrollable%2520shadow%2520generation%252C%2520generating%2520shadow%2520maps%2520for%2520diverse%2520light%2520source%250Aparameters.%2520Through%2520extensive%2520ablation%2520studies%252C%2520we%2520find%2520that%2520rectified%2520flow%250Aobjective%2520achieves%2520high-quality%2520results%2520with%2520just%2520a%2520single%2520sampling%2520step%250Aenabling%2520real-time%2520applications.%2520Furthermore%252C%2520our%2520experiments%2520demonstrate%2520that%250Athe%2520model%2520generalizes%2520well%2520to%2520real-world%2520images.%2520To%2520facilitate%2520further%2520research%250Ain%2520evaluating%2520quality%2520and%2520controllability%2520in%2520shadow%2520generation%252C%2520we%2520release%2520a%250Anew%2520public%2520benchmark%2520containing%2520a%2520diverse%2520set%2520of%2520object%2520images%2520and%2520shadow%2520maps%250Ain%2520various%2520settings.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//gojasper.github.io/controllable-shadow-generation-project/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Shadow%20Generation%20with%20Single-Step%20Diffusion%20Models%20from%0A%20%20Synthetic%20Data&entry.906535625=Onur%20Tasar%20and%20Cl%C3%A9ment%20Chadebec%20and%20Benjamin%20Aubin&entry.1292438233=%20%20Realistic%20shadow%20generation%20is%20a%20critical%20component%20for%20high-quality%20image%0Acompositing%20and%20visual%20effects%2C%20yet%20existing%20methods%20suffer%20from%20certain%0Alimitations%3A%20Physics-based%20approaches%20require%20a%203D%20scene%20geometry%2C%20which%20is%0Aoften%20unavailable%2C%20while%20learning-based%20techniques%20struggle%20with%20control%20and%0Avisual%20artifacts.%20We%20introduce%20a%20novel%20method%20for%20fast%2C%20controllable%2C%20and%0Abackground-free%20shadow%20generation%20for%202D%20object%20images.%20We%20create%20a%20large%0Asynthetic%20dataset%20using%20a%203D%20rendering%20engine%20to%20train%20a%20diffusion%20model%20for%0Acontrollable%20shadow%20generation%2C%20generating%20shadow%20maps%20for%20diverse%20light%20source%0Aparameters.%20Through%20extensive%20ablation%20studies%2C%20we%20find%20that%20rectified%20flow%0Aobjective%20achieves%20high-quality%20results%20with%20just%20a%20single%20sampling%20step%0Aenabling%20real-time%20applications.%20Furthermore%2C%20our%20experiments%20demonstrate%20that%0Athe%20model%20generalizes%20well%20to%20real-world%20images.%20To%20facilitate%20further%20research%0Ain%20evaluating%20quality%20and%20controllability%20in%20shadow%20generation%2C%20we%20release%20a%0Anew%20public%20benchmark%20containing%20a%20diverse%20set%20of%20object%20images%20and%20shadow%20maps%0Ain%20various%20settings.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//gojasper.github.io/controllable-shadow-generation-project/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11972v1&entry.124074799=Read"},
{"title": "Differentially Private Prototypes for Imbalanced Transfer Learning", "author": "Dariush Wahdany and Matthew Jagielski and Adam Dziedzic and Franziska Boenisch", "abstract": "  Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\n\\textit{pure DP}. We additionally show that privacy-utility trade-offs can be\nfurther improved when leveraging the public data beyond pre-training of the\nencoder: in particular, we can privately sample our DP prototypes from the\npublicly available data points used to train the encoder. Our experimental\nevaluation with four state-of-the-art encoders, four vision datasets, and under\ndifferent data and imbalancedness regimes demonstrate DPPL's high performance\nunder strong privacy guarantees in challenging private learning setups\n", "link": "http://arxiv.org/abs/2406.08039v2", "date": "2024-12-16", "relevancy": 1.8955, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4905}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4709}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentially%20Private%20Prototypes%20for%20Imbalanced%20Transfer%20Learning&body=Title%3A%20Differentially%20Private%20Prototypes%20for%20Imbalanced%20Transfer%20Learning%0AAuthor%3A%20Dariush%20Wahdany%20and%20Matthew%20Jagielski%20and%20Adam%20Dziedzic%20and%20Franziska%20Boenisch%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20models%20have%20been%20shown%20to%20leak%20private%20information%20from%0Atheir%20training%20datasets.%20Differential%20Privacy%20%28DP%29%2C%20typically%20implemented%0Athrough%20the%20differential%20private%20stochastic%20gradient%20descent%20algorithm%0A%28DP-SGD%29%2C%20has%20become%20the%20standard%20solution%20to%20bound%20leakage%20from%20the%20models.%0ADespite%20recent%20improvements%2C%20DP-SGD-based%20approaches%20for%20private%20learning%20still%0Ausually%20struggle%20in%20the%20high%20privacy%20%28%24%5Cvarepsilon%5Cle1%29%24%20and%20low%20data%20regimes%2C%0Aand%20when%20the%20private%20training%20datasets%20are%20imbalanced.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20Differentially%20Private%20Prototype%20Learning%20%28DPPL%29%20as%20a%0Anew%20paradigm%20for%20private%20transfer%20learning.%20DPPL%20leverages%20publicly%20pre-trained%0Aencoders%20to%20extract%20features%20from%20private%20data%20and%20generates%20DP%20prototypes%20that%0Arepresent%20each%20private%20class%20in%20the%20embedding%20space%20and%20can%20be%20publicly%0Areleased%20for%20inference.%20Since%20our%20DP%20prototypes%20can%20be%20obtained%20from%20only%20a%20few%0Aprivate%20training%20data%20points%20and%20without%20iterative%20noise%20addition%2C%20they%20offer%0Ahigh-utility%20predictions%20and%20strong%20privacy%20guarantees%20even%20under%20the%20notion%20of%0A%5Ctextit%7Bpure%20DP%7D.%20We%20additionally%20show%20that%20privacy-utility%20trade-offs%20can%20be%0Afurther%20improved%20when%20leveraging%20the%20public%20data%20beyond%20pre-training%20of%20the%0Aencoder%3A%20in%20particular%2C%20we%20can%20privately%20sample%20our%20DP%20prototypes%20from%20the%0Apublicly%20available%20data%20points%20used%20to%20train%20the%20encoder.%20Our%20experimental%0Aevaluation%20with%20four%20state-of-the-art%20encoders%2C%20four%20vision%20datasets%2C%20and%20under%0Adifferent%20data%20and%20imbalancedness%20regimes%20demonstrate%20DPPL%27s%20high%20performance%0Aunder%20strong%20privacy%20guarantees%20in%20challenging%20private%20learning%20setups%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentially%2520Private%2520Prototypes%2520for%2520Imbalanced%2520Transfer%2520Learning%26entry.906535625%3DDariush%2520Wahdany%2520and%2520Matthew%2520Jagielski%2520and%2520Adam%2520Dziedzic%2520and%2520Franziska%2520Boenisch%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520models%2520have%2520been%2520shown%2520to%2520leak%2520private%2520information%2520from%250Atheir%2520training%2520datasets.%2520Differential%2520Privacy%2520%2528DP%2529%252C%2520typically%2520implemented%250Athrough%2520the%2520differential%2520private%2520stochastic%2520gradient%2520descent%2520algorithm%250A%2528DP-SGD%2529%252C%2520has%2520become%2520the%2520standard%2520solution%2520to%2520bound%2520leakage%2520from%2520the%2520models.%250ADespite%2520recent%2520improvements%252C%2520DP-SGD-based%2520approaches%2520for%2520private%2520learning%2520still%250Ausually%2520struggle%2520in%2520the%2520high%2520privacy%2520%2528%2524%255Cvarepsilon%255Cle1%2529%2524%2520and%2520low%2520data%2520regimes%252C%250Aand%2520when%2520the%2520private%2520training%2520datasets%2520are%2520imbalanced.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520Differentially%2520Private%2520Prototype%2520Learning%2520%2528DPPL%2529%2520as%2520a%250Anew%2520paradigm%2520for%2520private%2520transfer%2520learning.%2520DPPL%2520leverages%2520publicly%2520pre-trained%250Aencoders%2520to%2520extract%2520features%2520from%2520private%2520data%2520and%2520generates%2520DP%2520prototypes%2520that%250Arepresent%2520each%2520private%2520class%2520in%2520the%2520embedding%2520space%2520and%2520can%2520be%2520publicly%250Areleased%2520for%2520inference.%2520Since%2520our%2520DP%2520prototypes%2520can%2520be%2520obtained%2520from%2520only%2520a%2520few%250Aprivate%2520training%2520data%2520points%2520and%2520without%2520iterative%2520noise%2520addition%252C%2520they%2520offer%250Ahigh-utility%2520predictions%2520and%2520strong%2520privacy%2520guarantees%2520even%2520under%2520the%2520notion%2520of%250A%255Ctextit%257Bpure%2520DP%257D.%2520We%2520additionally%2520show%2520that%2520privacy-utility%2520trade-offs%2520can%2520be%250Afurther%2520improved%2520when%2520leveraging%2520the%2520public%2520data%2520beyond%2520pre-training%2520of%2520the%250Aencoder%253A%2520in%2520particular%252C%2520we%2520can%2520privately%2520sample%2520our%2520DP%2520prototypes%2520from%2520the%250Apublicly%2520available%2520data%2520points%2520used%2520to%2520train%2520the%2520encoder.%2520Our%2520experimental%250Aevaluation%2520with%2520four%2520state-of-the-art%2520encoders%252C%2520four%2520vision%2520datasets%252C%2520and%2520under%250Adifferent%2520data%2520and%2520imbalancedness%2520regimes%2520demonstrate%2520DPPL%2527s%2520high%2520performance%250Aunder%2520strong%2520privacy%2520guarantees%2520in%2520challenging%2520private%2520learning%2520setups%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20Private%20Prototypes%20for%20Imbalanced%20Transfer%20Learning&entry.906535625=Dariush%20Wahdany%20and%20Matthew%20Jagielski%20and%20Adam%20Dziedzic%20and%20Franziska%20Boenisch&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20models%20have%20been%20shown%20to%20leak%20private%20information%20from%0Atheir%20training%20datasets.%20Differential%20Privacy%20%28DP%29%2C%20typically%20implemented%0Athrough%20the%20differential%20private%20stochastic%20gradient%20descent%20algorithm%0A%28DP-SGD%29%2C%20has%20become%20the%20standard%20solution%20to%20bound%20leakage%20from%20the%20models.%0ADespite%20recent%20improvements%2C%20DP-SGD-based%20approaches%20for%20private%20learning%20still%0Ausually%20struggle%20in%20the%20high%20privacy%20%28%24%5Cvarepsilon%5Cle1%29%24%20and%20low%20data%20regimes%2C%0Aand%20when%20the%20private%20training%20datasets%20are%20imbalanced.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20Differentially%20Private%20Prototype%20Learning%20%28DPPL%29%20as%20a%0Anew%20paradigm%20for%20private%20transfer%20learning.%20DPPL%20leverages%20publicly%20pre-trained%0Aencoders%20to%20extract%20features%20from%20private%20data%20and%20generates%20DP%20prototypes%20that%0Arepresent%20each%20private%20class%20in%20the%20embedding%20space%20and%20can%20be%20publicly%0Areleased%20for%20inference.%20Since%20our%20DP%20prototypes%20can%20be%20obtained%20from%20only%20a%20few%0Aprivate%20training%20data%20points%20and%20without%20iterative%20noise%20addition%2C%20they%20offer%0Ahigh-utility%20predictions%20and%20strong%20privacy%20guarantees%20even%20under%20the%20notion%20of%0A%5Ctextit%7Bpure%20DP%7D.%20We%20additionally%20show%20that%20privacy-utility%20trade-offs%20can%20be%0Afurther%20improved%20when%20leveraging%20the%20public%20data%20beyond%20pre-training%20of%20the%0Aencoder%3A%20in%20particular%2C%20we%20can%20privately%20sample%20our%20DP%20prototypes%20from%20the%0Apublicly%20available%20data%20points%20used%20to%20train%20the%20encoder.%20Our%20experimental%0Aevaluation%20with%20four%20state-of-the-art%20encoders%2C%20four%20vision%20datasets%2C%20and%20under%0Adifferent%20data%20and%20imbalancedness%20regimes%20demonstrate%20DPPL%27s%20high%20performance%0Aunder%20strong%20privacy%20guarantees%20in%20challenging%20private%20learning%20setups%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08039v2&entry.124074799=Read"},
{"title": "Habit Coach: Customising RAG-based chatbots to support behavior change", "author": "Arian Fooroogh Mand Arabi and Cansu Koyuturk and Michael O'Mahony and Raffaella Calati and Dimitri Ognibene", "abstract": "  This paper presents the iterative development of Habit Coach, a GPT-based\nchatbot designed to support users in habit change through personalized\ninteraction. Employing a user-centered design approach, we developed the\nchatbot using a Retrieval-Augmented Generation (RAG) system, which enables\nbehavior personalization without retraining the underlying language model\n(GPT-4). The system leverages document retrieval and specialized prompts to\ntailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and\nnarrative therapy techniques. A key challenge in the development process was\nthe difficulty of translating declarative knowledge into effective interaction\nbehaviors. In the initial phase, the chatbot was provided with declarative\nknowledge about CBT via reference textbooks and high-level conversational\ngoals. However, this approach resulted in imprecise and inefficient behavior,\nas the GPT model struggled to convert static information into dynamic and\ncontextually appropriate interactions. This highlighted the limitations of\nrelying solely on declarative knowledge to guide chatbot behavior, particularly\nin nuanced, therapeutic conversations. Over four iterations, we addressed this\nissue by gradually transitioning towards procedural knowledge, refining the\nchatbot's interaction strategies, and improving its overall effectiveness. In\nthe final evaluation, 5 participants engaged with the chatbot over five\nconsecutive days, receiving individualized CBT interventions. The Self-Report\nHabit Index (SRHI) was used to measure habit strength before and after the\nintervention, revealing a reduction in habit strength post-intervention. These\nresults underscore the importance of procedural knowledge in driving effective,\npersonalized behavior change support in RAG-based systems.\n", "link": "http://arxiv.org/abs/2411.19229v2", "date": "2024-12-16", "relevancy": 1.8892, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.485}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4801}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Habit%20Coach%3A%20Customising%20RAG-based%20chatbots%20to%20support%20behavior%20change&body=Title%3A%20Habit%20Coach%3A%20Customising%20RAG-based%20chatbots%20to%20support%20behavior%20change%0AAuthor%3A%20Arian%20Fooroogh%20Mand%20Arabi%20and%20Cansu%20Koyuturk%20and%20Michael%20O%27Mahony%20and%20Raffaella%20Calati%20and%20Dimitri%20Ognibene%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20iterative%20development%20of%20Habit%20Coach%2C%20a%20GPT-based%0Achatbot%20designed%20to%20support%20users%20in%20habit%20change%20through%20personalized%0Ainteraction.%20Employing%20a%20user-centered%20design%20approach%2C%20we%20developed%20the%0Achatbot%20using%20a%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%2C%20which%20enables%0Abehavior%20personalization%20without%20retraining%20the%20underlying%20language%20model%0A%28GPT-4%29.%20The%20system%20leverages%20document%20retrieval%20and%20specialized%20prompts%20to%0Atailor%20interactions%2C%20drawing%20from%20Cognitive%20Behavioral%20Therapy%20%28CBT%29%20and%0Anarrative%20therapy%20techniques.%20A%20key%20challenge%20in%20the%20development%20process%20was%0Athe%20difficulty%20of%20translating%20declarative%20knowledge%20into%20effective%20interaction%0Abehaviors.%20In%20the%20initial%20phase%2C%20the%20chatbot%20was%20provided%20with%20declarative%0Aknowledge%20about%20CBT%20via%20reference%20textbooks%20and%20high-level%20conversational%0Agoals.%20However%2C%20this%20approach%20resulted%20in%20imprecise%20and%20inefficient%20behavior%2C%0Aas%20the%20GPT%20model%20struggled%20to%20convert%20static%20information%20into%20dynamic%20and%0Acontextually%20appropriate%20interactions.%20This%20highlighted%20the%20limitations%20of%0Arelying%20solely%20on%20declarative%20knowledge%20to%20guide%20chatbot%20behavior%2C%20particularly%0Ain%20nuanced%2C%20therapeutic%20conversations.%20Over%20four%20iterations%2C%20we%20addressed%20this%0Aissue%20by%20gradually%20transitioning%20towards%20procedural%20knowledge%2C%20refining%20the%0Achatbot%27s%20interaction%20strategies%2C%20and%20improving%20its%20overall%20effectiveness.%20In%0Athe%20final%20evaluation%2C%205%20participants%20engaged%20with%20the%20chatbot%20over%20five%0Aconsecutive%20days%2C%20receiving%20individualized%20CBT%20interventions.%20The%20Self-Report%0AHabit%20Index%20%28SRHI%29%20was%20used%20to%20measure%20habit%20strength%20before%20and%20after%20the%0Aintervention%2C%20revealing%20a%20reduction%20in%20habit%20strength%20post-intervention.%20These%0Aresults%20underscore%20the%20importance%20of%20procedural%20knowledge%20in%20driving%20effective%2C%0Apersonalized%20behavior%20change%20support%20in%20RAG-based%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHabit%2520Coach%253A%2520Customising%2520RAG-based%2520chatbots%2520to%2520support%2520behavior%2520change%26entry.906535625%3DArian%2520Fooroogh%2520Mand%2520Arabi%2520and%2520Cansu%2520Koyuturk%2520and%2520Michael%2520O%2527Mahony%2520and%2520Raffaella%2520Calati%2520and%2520Dimitri%2520Ognibene%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520iterative%2520development%2520of%2520Habit%2520Coach%252C%2520a%2520GPT-based%250Achatbot%2520designed%2520to%2520support%2520users%2520in%2520habit%2520change%2520through%2520personalized%250Ainteraction.%2520Employing%2520a%2520user-centered%2520design%2520approach%252C%2520we%2520developed%2520the%250Achatbot%2520using%2520a%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520system%252C%2520which%2520enables%250Abehavior%2520personalization%2520without%2520retraining%2520the%2520underlying%2520language%2520model%250A%2528GPT-4%2529.%2520The%2520system%2520leverages%2520document%2520retrieval%2520and%2520specialized%2520prompts%2520to%250Atailor%2520interactions%252C%2520drawing%2520from%2520Cognitive%2520Behavioral%2520Therapy%2520%2528CBT%2529%2520and%250Anarrative%2520therapy%2520techniques.%2520A%2520key%2520challenge%2520in%2520the%2520development%2520process%2520was%250Athe%2520difficulty%2520of%2520translating%2520declarative%2520knowledge%2520into%2520effective%2520interaction%250Abehaviors.%2520In%2520the%2520initial%2520phase%252C%2520the%2520chatbot%2520was%2520provided%2520with%2520declarative%250Aknowledge%2520about%2520CBT%2520via%2520reference%2520textbooks%2520and%2520high-level%2520conversational%250Agoals.%2520However%252C%2520this%2520approach%2520resulted%2520in%2520imprecise%2520and%2520inefficient%2520behavior%252C%250Aas%2520the%2520GPT%2520model%2520struggled%2520to%2520convert%2520static%2520information%2520into%2520dynamic%2520and%250Acontextually%2520appropriate%2520interactions.%2520This%2520highlighted%2520the%2520limitations%2520of%250Arelying%2520solely%2520on%2520declarative%2520knowledge%2520to%2520guide%2520chatbot%2520behavior%252C%2520particularly%250Ain%2520nuanced%252C%2520therapeutic%2520conversations.%2520Over%2520four%2520iterations%252C%2520we%2520addressed%2520this%250Aissue%2520by%2520gradually%2520transitioning%2520towards%2520procedural%2520knowledge%252C%2520refining%2520the%250Achatbot%2527s%2520interaction%2520strategies%252C%2520and%2520improving%2520its%2520overall%2520effectiveness.%2520In%250Athe%2520final%2520evaluation%252C%25205%2520participants%2520engaged%2520with%2520the%2520chatbot%2520over%2520five%250Aconsecutive%2520days%252C%2520receiving%2520individualized%2520CBT%2520interventions.%2520The%2520Self-Report%250AHabit%2520Index%2520%2528SRHI%2529%2520was%2520used%2520to%2520measure%2520habit%2520strength%2520before%2520and%2520after%2520the%250Aintervention%252C%2520revealing%2520a%2520reduction%2520in%2520habit%2520strength%2520post-intervention.%2520These%250Aresults%2520underscore%2520the%2520importance%2520of%2520procedural%2520knowledge%2520in%2520driving%2520effective%252C%250Apersonalized%2520behavior%2520change%2520support%2520in%2520RAG-based%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Habit%20Coach%3A%20Customising%20RAG-based%20chatbots%20to%20support%20behavior%20change&entry.906535625=Arian%20Fooroogh%20Mand%20Arabi%20and%20Cansu%20Koyuturk%20and%20Michael%20O%27Mahony%20and%20Raffaella%20Calati%20and%20Dimitri%20Ognibene&entry.1292438233=%20%20This%20paper%20presents%20the%20iterative%20development%20of%20Habit%20Coach%2C%20a%20GPT-based%0Achatbot%20designed%20to%20support%20users%20in%20habit%20change%20through%20personalized%0Ainteraction.%20Employing%20a%20user-centered%20design%20approach%2C%20we%20developed%20the%0Achatbot%20using%20a%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%2C%20which%20enables%0Abehavior%20personalization%20without%20retraining%20the%20underlying%20language%20model%0A%28GPT-4%29.%20The%20system%20leverages%20document%20retrieval%20and%20specialized%20prompts%20to%0Atailor%20interactions%2C%20drawing%20from%20Cognitive%20Behavioral%20Therapy%20%28CBT%29%20and%0Anarrative%20therapy%20techniques.%20A%20key%20challenge%20in%20the%20development%20process%20was%0Athe%20difficulty%20of%20translating%20declarative%20knowledge%20into%20effective%20interaction%0Abehaviors.%20In%20the%20initial%20phase%2C%20the%20chatbot%20was%20provided%20with%20declarative%0Aknowledge%20about%20CBT%20via%20reference%20textbooks%20and%20high-level%20conversational%0Agoals.%20However%2C%20this%20approach%20resulted%20in%20imprecise%20and%20inefficient%20behavior%2C%0Aas%20the%20GPT%20model%20struggled%20to%20convert%20static%20information%20into%20dynamic%20and%0Acontextually%20appropriate%20interactions.%20This%20highlighted%20the%20limitations%20of%0Arelying%20solely%20on%20declarative%20knowledge%20to%20guide%20chatbot%20behavior%2C%20particularly%0Ain%20nuanced%2C%20therapeutic%20conversations.%20Over%20four%20iterations%2C%20we%20addressed%20this%0Aissue%20by%20gradually%20transitioning%20towards%20procedural%20knowledge%2C%20refining%20the%0Achatbot%27s%20interaction%20strategies%2C%20and%20improving%20its%20overall%20effectiveness.%20In%0Athe%20final%20evaluation%2C%205%20participants%20engaged%20with%20the%20chatbot%20over%20five%0Aconsecutive%20days%2C%20receiving%20individualized%20CBT%20interventions.%20The%20Self-Report%0AHabit%20Index%20%28SRHI%29%20was%20used%20to%20measure%20habit%20strength%20before%20and%20after%20the%0Aintervention%2C%20revealing%20a%20reduction%20in%20habit%20strength%20post-intervention.%20These%0Aresults%20underscore%20the%20importance%20of%20procedural%20knowledge%20in%20driving%20effective%2C%0Apersonalized%20behavior%20change%20support%20in%20RAG-based%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19229v2&entry.124074799=Read"},
{"title": "Qsco: A Quantum Scoring Module for Open-set Supervised Anomaly Detection", "author": "Yifeng Peng and Xinyi Li and Zhiding Liang and Ying Wang", "abstract": "  Open set anomaly detection (OSAD) is a crucial task that aims to identify\nabnormal patterns or behaviors in data sets, especially when the anomalies\nobserved during training do not represent all possible classes of anomalies.\nThe recent advances in quantum computing in handling complex data structures\nand improving machine learning models herald a paradigm shift in anomaly\ndetection methodologies. This study proposes a Quantum Scoring Module (Qsco),\nembedding quantum variational circuits into neural networks to enhance the\nmodel's processing capabilities in handling uncertainty and unlabeled data.\nExtensive experiments conducted across eight real-world anomaly detection\ndatasets demonstrate our model's superior performance in detecting anomalies\nacross varied settings and reveal that integrating quantum simulators does not\nresult in prohibitive time complexities. Our study validates the feasibility of\nquantum-enhanced anomaly detection methods in practical applications.\n", "link": "http://arxiv.org/abs/2405.16368v2", "date": "2024-12-16", "relevancy": 1.8793, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4817}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qsco%3A%20A%20Quantum%20Scoring%20Module%20for%20Open-set%20Supervised%20Anomaly%20Detection&body=Title%3A%20Qsco%3A%20A%20Quantum%20Scoring%20Module%20for%20Open-set%20Supervised%20Anomaly%20Detection%0AAuthor%3A%20Yifeng%20Peng%20and%20Xinyi%20Li%20and%20Zhiding%20Liang%20and%20Ying%20Wang%0AAbstract%3A%20%20%20Open%20set%20anomaly%20detection%20%28OSAD%29%20is%20a%20crucial%20task%20that%20aims%20to%20identify%0Aabnormal%20patterns%20or%20behaviors%20in%20data%20sets%2C%20especially%20when%20the%20anomalies%0Aobserved%20during%20training%20do%20not%20represent%20all%20possible%20classes%20of%20anomalies.%0AThe%20recent%20advances%20in%20quantum%20computing%20in%20handling%20complex%20data%20structures%0Aand%20improving%20machine%20learning%20models%20herald%20a%20paradigm%20shift%20in%20anomaly%0Adetection%20methodologies.%20This%20study%20proposes%20a%20Quantum%20Scoring%20Module%20%28Qsco%29%2C%0Aembedding%20quantum%20variational%20circuits%20into%20neural%20networks%20to%20enhance%20the%0Amodel%27s%20processing%20capabilities%20in%20handling%20uncertainty%20and%20unlabeled%20data.%0AExtensive%20experiments%20conducted%20across%20eight%20real-world%20anomaly%20detection%0Adatasets%20demonstrate%20our%20model%27s%20superior%20performance%20in%20detecting%20anomalies%0Aacross%20varied%20settings%20and%20reveal%20that%20integrating%20quantum%20simulators%20does%20not%0Aresult%20in%20prohibitive%20time%20complexities.%20Our%20study%20validates%20the%20feasibility%20of%0Aquantum-enhanced%20anomaly%20detection%20methods%20in%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQsco%253A%2520A%2520Quantum%2520Scoring%2520Module%2520for%2520Open-set%2520Supervised%2520Anomaly%2520Detection%26entry.906535625%3DYifeng%2520Peng%2520and%2520Xinyi%2520Li%2520and%2520Zhiding%2520Liang%2520and%2520Ying%2520Wang%26entry.1292438233%3D%2520%2520Open%2520set%2520anomaly%2520detection%2520%2528OSAD%2529%2520is%2520a%2520crucial%2520task%2520that%2520aims%2520to%2520identify%250Aabnormal%2520patterns%2520or%2520behaviors%2520in%2520data%2520sets%252C%2520especially%2520when%2520the%2520anomalies%250Aobserved%2520during%2520training%2520do%2520not%2520represent%2520all%2520possible%2520classes%2520of%2520anomalies.%250AThe%2520recent%2520advances%2520in%2520quantum%2520computing%2520in%2520handling%2520complex%2520data%2520structures%250Aand%2520improving%2520machine%2520learning%2520models%2520herald%2520a%2520paradigm%2520shift%2520in%2520anomaly%250Adetection%2520methodologies.%2520This%2520study%2520proposes%2520a%2520Quantum%2520Scoring%2520Module%2520%2528Qsco%2529%252C%250Aembedding%2520quantum%2520variational%2520circuits%2520into%2520neural%2520networks%2520to%2520enhance%2520the%250Amodel%2527s%2520processing%2520capabilities%2520in%2520handling%2520uncertainty%2520and%2520unlabeled%2520data.%250AExtensive%2520experiments%2520conducted%2520across%2520eight%2520real-world%2520anomaly%2520detection%250Adatasets%2520demonstrate%2520our%2520model%2527s%2520superior%2520performance%2520in%2520detecting%2520anomalies%250Aacross%2520varied%2520settings%2520and%2520reveal%2520that%2520integrating%2520quantum%2520simulators%2520does%2520not%250Aresult%2520in%2520prohibitive%2520time%2520complexities.%2520Our%2520study%2520validates%2520the%2520feasibility%2520of%250Aquantum-enhanced%2520anomaly%2520detection%2520methods%2520in%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qsco%3A%20A%20Quantum%20Scoring%20Module%20for%20Open-set%20Supervised%20Anomaly%20Detection&entry.906535625=Yifeng%20Peng%20and%20Xinyi%20Li%20and%20Zhiding%20Liang%20and%20Ying%20Wang&entry.1292438233=%20%20Open%20set%20anomaly%20detection%20%28OSAD%29%20is%20a%20crucial%20task%20that%20aims%20to%20identify%0Aabnormal%20patterns%20or%20behaviors%20in%20data%20sets%2C%20especially%20when%20the%20anomalies%0Aobserved%20during%20training%20do%20not%20represent%20all%20possible%20classes%20of%20anomalies.%0AThe%20recent%20advances%20in%20quantum%20computing%20in%20handling%20complex%20data%20structures%0Aand%20improving%20machine%20learning%20models%20herald%20a%20paradigm%20shift%20in%20anomaly%0Adetection%20methodologies.%20This%20study%20proposes%20a%20Quantum%20Scoring%20Module%20%28Qsco%29%2C%0Aembedding%20quantum%20variational%20circuits%20into%20neural%20networks%20to%20enhance%20the%0Amodel%27s%20processing%20capabilities%20in%20handling%20uncertainty%20and%20unlabeled%20data.%0AExtensive%20experiments%20conducted%20across%20eight%20real-world%20anomaly%20detection%0Adatasets%20demonstrate%20our%20model%27s%20superior%20performance%20in%20detecting%20anomalies%0Aacross%20varied%20settings%20and%20reveal%20that%20integrating%20quantum%20simulators%20does%20not%0Aresult%20in%20prohibitive%20time%20complexities.%20Our%20study%20validates%20the%20feasibility%20of%0Aquantum-enhanced%20anomaly%20detection%20methods%20in%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16368v2&entry.124074799=Read"},
{"title": "Memory-Reduced Meta-Learning with Guaranteed Convergence", "author": "Honglin Yang and Ji Ma and Xiao Yu", "abstract": "  The optimization-based meta-learning approach is gaining increased traction\nbecause of its unique ability to quickly adapt to a new task using only small\namounts of data. However, existing optimization-based meta-learning approaches,\nsuch as MAML, ANIL and their variants, generally employ backpropagation for\nupper-level gradient estimation, which requires using historical lower-level\nparameters/gradients and thus increases computational and memory overhead in\neach iteration. In this paper, we propose a meta-learning algorithm that can\navoid using historical parameters/gradients and significantly reduce memory\ncosts in each iteration compared to existing optimization-based meta-learning\napproaches. In addition to memory reduction, we prove that our proposed\nalgorithm converges sublinearly with the iteration number of upper-level\noptimization, and the convergence error decays sublinearly with the batch size\nof sampled tasks. In the specific case in terms of deterministic meta-learning,\nwe also prove that our proposed algorithm converges to an exact solution.\nMoreover, we quantify that the computational complexity of the algorithm is on\nthe order of $\\mathcal{O}(\\epsilon^{-1})$, which matches existing convergence\nresults on meta-learning even without using any historical\nparameters/gradients. Experimental results on meta-learning benchmarks confirm\nthe efficacy of our proposed algorithm.\n", "link": "http://arxiv.org/abs/2412.12030v1", "date": "2024-12-16", "relevancy": 1.8547, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4752}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-Reduced%20Meta-Learning%20with%20Guaranteed%20Convergence&body=Title%3A%20Memory-Reduced%20Meta-Learning%20with%20Guaranteed%20Convergence%0AAuthor%3A%20Honglin%20Yang%20and%20Ji%20Ma%20and%20Xiao%20Yu%0AAbstract%3A%20%20%20The%20optimization-based%20meta-learning%20approach%20is%20gaining%20increased%20traction%0Abecause%20of%20its%20unique%20ability%20to%20quickly%20adapt%20to%20a%20new%20task%20using%20only%20small%0Aamounts%20of%20data.%20However%2C%20existing%20optimization-based%20meta-learning%20approaches%2C%0Asuch%20as%20MAML%2C%20ANIL%20and%20their%20variants%2C%20generally%20employ%20backpropagation%20for%0Aupper-level%20gradient%20estimation%2C%20which%20requires%20using%20historical%20lower-level%0Aparameters/gradients%20and%20thus%20increases%20computational%20and%20memory%20overhead%20in%0Aeach%20iteration.%20In%20this%20paper%2C%20we%20propose%20a%20meta-learning%20algorithm%20that%20can%0Aavoid%20using%20historical%20parameters/gradients%20and%20significantly%20reduce%20memory%0Acosts%20in%20each%20iteration%20compared%20to%20existing%20optimization-based%20meta-learning%0Aapproaches.%20In%20addition%20to%20memory%20reduction%2C%20we%20prove%20that%20our%20proposed%0Aalgorithm%20converges%20sublinearly%20with%20the%20iteration%20number%20of%20upper-level%0Aoptimization%2C%20and%20the%20convergence%20error%20decays%20sublinearly%20with%20the%20batch%20size%0Aof%20sampled%20tasks.%20In%20the%20specific%20case%20in%20terms%20of%20deterministic%20meta-learning%2C%0Awe%20also%20prove%20that%20our%20proposed%20algorithm%20converges%20to%20an%20exact%20solution.%0AMoreover%2C%20we%20quantify%20that%20the%20computational%20complexity%20of%20the%20algorithm%20is%20on%0Athe%20order%20of%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-1%7D%29%24%2C%20which%20matches%20existing%20convergence%0Aresults%20on%20meta-learning%20even%20without%20using%20any%20historical%0Aparameters/gradients.%20Experimental%20results%20on%20meta-learning%20benchmarks%20confirm%0Athe%20efficacy%20of%20our%20proposed%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-Reduced%2520Meta-Learning%2520with%2520Guaranteed%2520Convergence%26entry.906535625%3DHonglin%2520Yang%2520and%2520Ji%2520Ma%2520and%2520Xiao%2520Yu%26entry.1292438233%3D%2520%2520The%2520optimization-based%2520meta-learning%2520approach%2520is%2520gaining%2520increased%2520traction%250Abecause%2520of%2520its%2520unique%2520ability%2520to%2520quickly%2520adapt%2520to%2520a%2520new%2520task%2520using%2520only%2520small%250Aamounts%2520of%2520data.%2520However%252C%2520existing%2520optimization-based%2520meta-learning%2520approaches%252C%250Asuch%2520as%2520MAML%252C%2520ANIL%2520and%2520their%2520variants%252C%2520generally%2520employ%2520backpropagation%2520for%250Aupper-level%2520gradient%2520estimation%252C%2520which%2520requires%2520using%2520historical%2520lower-level%250Aparameters/gradients%2520and%2520thus%2520increases%2520computational%2520and%2520memory%2520overhead%2520in%250Aeach%2520iteration.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520meta-learning%2520algorithm%2520that%2520can%250Aavoid%2520using%2520historical%2520parameters/gradients%2520and%2520significantly%2520reduce%2520memory%250Acosts%2520in%2520each%2520iteration%2520compared%2520to%2520existing%2520optimization-based%2520meta-learning%250Aapproaches.%2520In%2520addition%2520to%2520memory%2520reduction%252C%2520we%2520prove%2520that%2520our%2520proposed%250Aalgorithm%2520converges%2520sublinearly%2520with%2520the%2520iteration%2520number%2520of%2520upper-level%250Aoptimization%252C%2520and%2520the%2520convergence%2520error%2520decays%2520sublinearly%2520with%2520the%2520batch%2520size%250Aof%2520sampled%2520tasks.%2520In%2520the%2520specific%2520case%2520in%2520terms%2520of%2520deterministic%2520meta-learning%252C%250Awe%2520also%2520prove%2520that%2520our%2520proposed%2520algorithm%2520converges%2520to%2520an%2520exact%2520solution.%250AMoreover%252C%2520we%2520quantify%2520that%2520the%2520computational%2520complexity%2520of%2520the%2520algorithm%2520is%2520on%250Athe%2520order%2520of%2520%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-1%257D%2529%2524%252C%2520which%2520matches%2520existing%2520convergence%250Aresults%2520on%2520meta-learning%2520even%2520without%2520using%2520any%2520historical%250Aparameters/gradients.%2520Experimental%2520results%2520on%2520meta-learning%2520benchmarks%2520confirm%250Athe%2520efficacy%2520of%2520our%2520proposed%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-Reduced%20Meta-Learning%20with%20Guaranteed%20Convergence&entry.906535625=Honglin%20Yang%20and%20Ji%20Ma%20and%20Xiao%20Yu&entry.1292438233=%20%20The%20optimization-based%20meta-learning%20approach%20is%20gaining%20increased%20traction%0Abecause%20of%20its%20unique%20ability%20to%20quickly%20adapt%20to%20a%20new%20task%20using%20only%20small%0Aamounts%20of%20data.%20However%2C%20existing%20optimization-based%20meta-learning%20approaches%2C%0Asuch%20as%20MAML%2C%20ANIL%20and%20their%20variants%2C%20generally%20employ%20backpropagation%20for%0Aupper-level%20gradient%20estimation%2C%20which%20requires%20using%20historical%20lower-level%0Aparameters/gradients%20and%20thus%20increases%20computational%20and%20memory%20overhead%20in%0Aeach%20iteration.%20In%20this%20paper%2C%20we%20propose%20a%20meta-learning%20algorithm%20that%20can%0Aavoid%20using%20historical%20parameters/gradients%20and%20significantly%20reduce%20memory%0Acosts%20in%20each%20iteration%20compared%20to%20existing%20optimization-based%20meta-learning%0Aapproaches.%20In%20addition%20to%20memory%20reduction%2C%20we%20prove%20that%20our%20proposed%0Aalgorithm%20converges%20sublinearly%20with%20the%20iteration%20number%20of%20upper-level%0Aoptimization%2C%20and%20the%20convergence%20error%20decays%20sublinearly%20with%20the%20batch%20size%0Aof%20sampled%20tasks.%20In%20the%20specific%20case%20in%20terms%20of%20deterministic%20meta-learning%2C%0Awe%20also%20prove%20that%20our%20proposed%20algorithm%20converges%20to%20an%20exact%20solution.%0AMoreover%2C%20we%20quantify%20that%20the%20computational%20complexity%20of%20the%20algorithm%20is%20on%0Athe%20order%20of%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-1%7D%29%24%2C%20which%20matches%20existing%20convergence%0Aresults%20on%20meta-learning%20even%20without%20using%20any%20historical%0Aparameters/gradients.%20Experimental%20results%20on%20meta-learning%20benchmarks%20confirm%0Athe%20efficacy%20of%20our%20proposed%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12030v1&entry.124074799=Read"},
{"title": "Learning Human-Aware Robot Policies for Adaptive Assistance", "author": "Jason Qin and Shikun Ban and Wentao Zhu and Yizhou Wang and Dimitris Samaras", "abstract": "  Developing robots that can assist humans efficiently, safely, and adaptively\nis crucial for real-world applications such as healthcare. While previous work\noften assumes a centralized system for co-optimizing human-robot interactions,\nwe argue that real-world scenarios are much more complicated, as humans have\nindividual preferences regarding how tasks are performed. Robots typically lack\ndirect access to these implicit preferences. However, to provide effective\nassistance, robots must still be able to recognize and adapt to the individual\nneeds and preferences of different users. To address these challenges, we\npropose a novel framework in which robots infer human intentions and reason\nabout human utilities through interaction. Our approach features two critical\nmodules: the anticipation module is a motion predictor that captures the\nspatial-temporal relationship between the robot agent and user agent, which\ncontributes to predicting human behavior; the utility module infers the\nunderlying human utility functions through progressive task demonstration\nsampling. Extensive experiments across various robot types and assistive tasks\ndemonstrate that the proposed framework not only enhances task success and\nefficiency but also significantly improves user satisfaction, paving the way\nfor more personalized and adaptive assistive robotic systems. Code and demos\nare available at https://asonin.github.io/Human-Aware-Assistance/.\n", "link": "http://arxiv.org/abs/2412.11913v1", "date": "2024-12-16", "relevancy": 1.8412, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.636}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6134}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Human-Aware%20Robot%20Policies%20for%20Adaptive%20Assistance&body=Title%3A%20Learning%20Human-Aware%20Robot%20Policies%20for%20Adaptive%20Assistance%0AAuthor%3A%20Jason%20Qin%20and%20Shikun%20Ban%20and%20Wentao%20Zhu%20and%20Yizhou%20Wang%20and%20Dimitris%20Samaras%0AAbstract%3A%20%20%20Developing%20robots%20that%20can%20assist%20humans%20efficiently%2C%20safely%2C%20and%20adaptively%0Ais%20crucial%20for%20real-world%20applications%20such%20as%20healthcare.%20While%20previous%20work%0Aoften%20assumes%20a%20centralized%20system%20for%20co-optimizing%20human-robot%20interactions%2C%0Awe%20argue%20that%20real-world%20scenarios%20are%20much%20more%20complicated%2C%20as%20humans%20have%0Aindividual%20preferences%20regarding%20how%20tasks%20are%20performed.%20Robots%20typically%20lack%0Adirect%20access%20to%20these%20implicit%20preferences.%20However%2C%20to%20provide%20effective%0Aassistance%2C%20robots%20must%20still%20be%20able%20to%20recognize%20and%20adapt%20to%20the%20individual%0Aneeds%20and%20preferences%20of%20different%20users.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20framework%20in%20which%20robots%20infer%20human%20intentions%20and%20reason%0Aabout%20human%20utilities%20through%20interaction.%20Our%20approach%20features%20two%20critical%0Amodules%3A%20the%20anticipation%20module%20is%20a%20motion%20predictor%20that%20captures%20the%0Aspatial-temporal%20relationship%20between%20the%20robot%20agent%20and%20user%20agent%2C%20which%0Acontributes%20to%20predicting%20human%20behavior%3B%20the%20utility%20module%20infers%20the%0Aunderlying%20human%20utility%20functions%20through%20progressive%20task%20demonstration%0Asampling.%20Extensive%20experiments%20across%20various%20robot%20types%20and%20assistive%20tasks%0Ademonstrate%20that%20the%20proposed%20framework%20not%20only%20enhances%20task%20success%20and%0Aefficiency%20but%20also%20significantly%20improves%20user%20satisfaction%2C%20paving%20the%20way%0Afor%20more%20personalized%20and%20adaptive%20assistive%20robotic%20systems.%20Code%20and%20demos%0Aare%20available%20at%20https%3A//asonin.github.io/Human-Aware-Assistance/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Human-Aware%2520Robot%2520Policies%2520for%2520Adaptive%2520Assistance%26entry.906535625%3DJason%2520Qin%2520and%2520Shikun%2520Ban%2520and%2520Wentao%2520Zhu%2520and%2520Yizhou%2520Wang%2520and%2520Dimitris%2520Samaras%26entry.1292438233%3D%2520%2520Developing%2520robots%2520that%2520can%2520assist%2520humans%2520efficiently%252C%2520safely%252C%2520and%2520adaptively%250Ais%2520crucial%2520for%2520real-world%2520applications%2520such%2520as%2520healthcare.%2520While%2520previous%2520work%250Aoften%2520assumes%2520a%2520centralized%2520system%2520for%2520co-optimizing%2520human-robot%2520interactions%252C%250Awe%2520argue%2520that%2520real-world%2520scenarios%2520are%2520much%2520more%2520complicated%252C%2520as%2520humans%2520have%250Aindividual%2520preferences%2520regarding%2520how%2520tasks%2520are%2520performed.%2520Robots%2520typically%2520lack%250Adirect%2520access%2520to%2520these%2520implicit%2520preferences.%2520However%252C%2520to%2520provide%2520effective%250Aassistance%252C%2520robots%2520must%2520still%2520be%2520able%2520to%2520recognize%2520and%2520adapt%2520to%2520the%2520individual%250Aneeds%2520and%2520preferences%2520of%2520different%2520users.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520in%2520which%2520robots%2520infer%2520human%2520intentions%2520and%2520reason%250Aabout%2520human%2520utilities%2520through%2520interaction.%2520Our%2520approach%2520features%2520two%2520critical%250Amodules%253A%2520the%2520anticipation%2520module%2520is%2520a%2520motion%2520predictor%2520that%2520captures%2520the%250Aspatial-temporal%2520relationship%2520between%2520the%2520robot%2520agent%2520and%2520user%2520agent%252C%2520which%250Acontributes%2520to%2520predicting%2520human%2520behavior%253B%2520the%2520utility%2520module%2520infers%2520the%250Aunderlying%2520human%2520utility%2520functions%2520through%2520progressive%2520task%2520demonstration%250Asampling.%2520Extensive%2520experiments%2520across%2520various%2520robot%2520types%2520and%2520assistive%2520tasks%250Ademonstrate%2520that%2520the%2520proposed%2520framework%2520not%2520only%2520enhances%2520task%2520success%2520and%250Aefficiency%2520but%2520also%2520significantly%2520improves%2520user%2520satisfaction%252C%2520paving%2520the%2520way%250Afor%2520more%2520personalized%2520and%2520adaptive%2520assistive%2520robotic%2520systems.%2520Code%2520and%2520demos%250Aare%2520available%2520at%2520https%253A//asonin.github.io/Human-Aware-Assistance/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Human-Aware%20Robot%20Policies%20for%20Adaptive%20Assistance&entry.906535625=Jason%20Qin%20and%20Shikun%20Ban%20and%20Wentao%20Zhu%20and%20Yizhou%20Wang%20and%20Dimitris%20Samaras&entry.1292438233=%20%20Developing%20robots%20that%20can%20assist%20humans%20efficiently%2C%20safely%2C%20and%20adaptively%0Ais%20crucial%20for%20real-world%20applications%20such%20as%20healthcare.%20While%20previous%20work%0Aoften%20assumes%20a%20centralized%20system%20for%20co-optimizing%20human-robot%20interactions%2C%0Awe%20argue%20that%20real-world%20scenarios%20are%20much%20more%20complicated%2C%20as%20humans%20have%0Aindividual%20preferences%20regarding%20how%20tasks%20are%20performed.%20Robots%20typically%20lack%0Adirect%20access%20to%20these%20implicit%20preferences.%20However%2C%20to%20provide%20effective%0Aassistance%2C%20robots%20must%20still%20be%20able%20to%20recognize%20and%20adapt%20to%20the%20individual%0Aneeds%20and%20preferences%20of%20different%20users.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20framework%20in%20which%20robots%20infer%20human%20intentions%20and%20reason%0Aabout%20human%20utilities%20through%20interaction.%20Our%20approach%20features%20two%20critical%0Amodules%3A%20the%20anticipation%20module%20is%20a%20motion%20predictor%20that%20captures%20the%0Aspatial-temporal%20relationship%20between%20the%20robot%20agent%20and%20user%20agent%2C%20which%0Acontributes%20to%20predicting%20human%20behavior%3B%20the%20utility%20module%20infers%20the%0Aunderlying%20human%20utility%20functions%20through%20progressive%20task%20demonstration%0Asampling.%20Extensive%20experiments%20across%20various%20robot%20types%20and%20assistive%20tasks%0Ademonstrate%20that%20the%20proposed%20framework%20not%20only%20enhances%20task%20success%20and%0Aefficiency%20but%20also%20significantly%20improves%20user%20satisfaction%2C%20paving%20the%20way%0Afor%20more%20personalized%20and%20adaptive%20assistive%20robotic%20systems.%20Code%20and%20demos%0Aare%20available%20at%20https%3A//asonin.github.io/Human-Aware-Assistance/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11913v1&entry.124074799=Read"},
{"title": "LeARN: Learnable and Adaptive Representations for Nonlinear Dynamics in\n  System Identification", "author": "Arunabh Singh and Joyjit Mukherjee", "abstract": "  System identification, the process of deriving mathematical models of\ndynamical systems from observed input-output data, has undergone a paradigm\nshift with the advent of learning-based methods. Addressing the intricate\nchallenges of data-driven discovery in nonlinear dynamical systems, these\nmethods have garnered significant attention. Among them, Sparse Identification\nof Nonlinear Dynamics (SINDy) has emerged as a transformative approach,\ndistilling complex dynamical behaviors into interpretable linear combinations\nof basis functions. However, SINDy relies on domain-specific expertise to\nconstruct its foundational \"library\" of basis functions, which limits its\nadaptability and universality. In this work, we introduce a nonlinear system\nidentification framework called LeARN that transcends the need for prior domain\nknowledge by learning the library of basis functions directly from data. To\nenhance adaptability to evolving system dynamics under varying noise\nconditions, we employ a novel meta-learning-based system identification\napproach that uses a lightweight deep neural network (DNN) to dynamically\nrefine these basis functions. This not only captures intricate system behaviors\nbut also adapts seamlessly to new dynamical regimes. We validate our framework\non the Neural Fly dataset, showcasing its robust adaptation and generalization\ncapabilities. Despite its simplicity, our LeARN achieves competitive dynamical\nerror performance compared to SINDy. This work presents a step toward the\nautonomous discovery of dynamical systems, paving the way for a future where\nmachine learning uncovers the governing principles of complex systems without\nrequiring extensive domain-specific interventions.\n", "link": "http://arxiv.org/abs/2412.12036v1", "date": "2024-12-16", "relevancy": 1.8318, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4679}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeARN%3A%20Learnable%20and%20Adaptive%20Representations%20for%20Nonlinear%20Dynamics%20in%0A%20%20System%20Identification&body=Title%3A%20LeARN%3A%20Learnable%20and%20Adaptive%20Representations%20for%20Nonlinear%20Dynamics%20in%0A%20%20System%20Identification%0AAuthor%3A%20Arunabh%20Singh%20and%20Joyjit%20Mukherjee%0AAbstract%3A%20%20%20System%20identification%2C%20the%20process%20of%20deriving%20mathematical%20models%20of%0Adynamical%20systems%20from%20observed%20input-output%20data%2C%20has%20undergone%20a%20paradigm%0Ashift%20with%20the%20advent%20of%20learning-based%20methods.%20Addressing%20the%20intricate%0Achallenges%20of%20data-driven%20discovery%20in%20nonlinear%20dynamical%20systems%2C%20these%0Amethods%20have%20garnered%20significant%20attention.%20Among%20them%2C%20Sparse%20Identification%0Aof%20Nonlinear%20Dynamics%20%28SINDy%29%20has%20emerged%20as%20a%20transformative%20approach%2C%0Adistilling%20complex%20dynamical%20behaviors%20into%20interpretable%20linear%20combinations%0Aof%20basis%20functions.%20However%2C%20SINDy%20relies%20on%20domain-specific%20expertise%20to%0Aconstruct%20its%20foundational%20%22library%22%20of%20basis%20functions%2C%20which%20limits%20its%0Aadaptability%20and%20universality.%20In%20this%20work%2C%20we%20introduce%20a%20nonlinear%20system%0Aidentification%20framework%20called%20LeARN%20that%20transcends%20the%20need%20for%20prior%20domain%0Aknowledge%20by%20learning%20the%20library%20of%20basis%20functions%20directly%20from%20data.%20To%0Aenhance%20adaptability%20to%20evolving%20system%20dynamics%20under%20varying%20noise%0Aconditions%2C%20we%20employ%20a%20novel%20meta-learning-based%20system%20identification%0Aapproach%20that%20uses%20a%20lightweight%20deep%20neural%20network%20%28DNN%29%20to%20dynamically%0Arefine%20these%20basis%20functions.%20This%20not%20only%20captures%20intricate%20system%20behaviors%0Abut%20also%20adapts%20seamlessly%20to%20new%20dynamical%20regimes.%20We%20validate%20our%20framework%0Aon%20the%20Neural%20Fly%20dataset%2C%20showcasing%20its%20robust%20adaptation%20and%20generalization%0Acapabilities.%20Despite%20its%20simplicity%2C%20our%20LeARN%20achieves%20competitive%20dynamical%0Aerror%20performance%20compared%20to%20SINDy.%20This%20work%20presents%20a%20step%20toward%20the%0Aautonomous%20discovery%20of%20dynamical%20systems%2C%20paving%20the%20way%20for%20a%20future%20where%0Amachine%20learning%20uncovers%20the%20governing%20principles%20of%20complex%20systems%20without%0Arequiring%20extensive%20domain-specific%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeARN%253A%2520Learnable%2520and%2520Adaptive%2520Representations%2520for%2520Nonlinear%2520Dynamics%2520in%250A%2520%2520System%2520Identification%26entry.906535625%3DArunabh%2520Singh%2520and%2520Joyjit%2520Mukherjee%26entry.1292438233%3D%2520%2520System%2520identification%252C%2520the%2520process%2520of%2520deriving%2520mathematical%2520models%2520of%250Adynamical%2520systems%2520from%2520observed%2520input-output%2520data%252C%2520has%2520undergone%2520a%2520paradigm%250Ashift%2520with%2520the%2520advent%2520of%2520learning-based%2520methods.%2520Addressing%2520the%2520intricate%250Achallenges%2520of%2520data-driven%2520discovery%2520in%2520nonlinear%2520dynamical%2520systems%252C%2520these%250Amethods%2520have%2520garnered%2520significant%2520attention.%2520Among%2520them%252C%2520Sparse%2520Identification%250Aof%2520Nonlinear%2520Dynamics%2520%2528SINDy%2529%2520has%2520emerged%2520as%2520a%2520transformative%2520approach%252C%250Adistilling%2520complex%2520dynamical%2520behaviors%2520into%2520interpretable%2520linear%2520combinations%250Aof%2520basis%2520functions.%2520However%252C%2520SINDy%2520relies%2520on%2520domain-specific%2520expertise%2520to%250Aconstruct%2520its%2520foundational%2520%2522library%2522%2520of%2520basis%2520functions%252C%2520which%2520limits%2520its%250Aadaptability%2520and%2520universality.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520nonlinear%2520system%250Aidentification%2520framework%2520called%2520LeARN%2520that%2520transcends%2520the%2520need%2520for%2520prior%2520domain%250Aknowledge%2520by%2520learning%2520the%2520library%2520of%2520basis%2520functions%2520directly%2520from%2520data.%2520To%250Aenhance%2520adaptability%2520to%2520evolving%2520system%2520dynamics%2520under%2520varying%2520noise%250Aconditions%252C%2520we%2520employ%2520a%2520novel%2520meta-learning-based%2520system%2520identification%250Aapproach%2520that%2520uses%2520a%2520lightweight%2520deep%2520neural%2520network%2520%2528DNN%2529%2520to%2520dynamically%250Arefine%2520these%2520basis%2520functions.%2520This%2520not%2520only%2520captures%2520intricate%2520system%2520behaviors%250Abut%2520also%2520adapts%2520seamlessly%2520to%2520new%2520dynamical%2520regimes.%2520We%2520validate%2520our%2520framework%250Aon%2520the%2520Neural%2520Fly%2520dataset%252C%2520showcasing%2520its%2520robust%2520adaptation%2520and%2520generalization%250Acapabilities.%2520Despite%2520its%2520simplicity%252C%2520our%2520LeARN%2520achieves%2520competitive%2520dynamical%250Aerror%2520performance%2520compared%2520to%2520SINDy.%2520This%2520work%2520presents%2520a%2520step%2520toward%2520the%250Aautonomous%2520discovery%2520of%2520dynamical%2520systems%252C%2520paving%2520the%2520way%2520for%2520a%2520future%2520where%250Amachine%2520learning%2520uncovers%2520the%2520governing%2520principles%2520of%2520complex%2520systems%2520without%250Arequiring%2520extensive%2520domain-specific%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeARN%3A%20Learnable%20and%20Adaptive%20Representations%20for%20Nonlinear%20Dynamics%20in%0A%20%20System%20Identification&entry.906535625=Arunabh%20Singh%20and%20Joyjit%20Mukherjee&entry.1292438233=%20%20System%20identification%2C%20the%20process%20of%20deriving%20mathematical%20models%20of%0Adynamical%20systems%20from%20observed%20input-output%20data%2C%20has%20undergone%20a%20paradigm%0Ashift%20with%20the%20advent%20of%20learning-based%20methods.%20Addressing%20the%20intricate%0Achallenges%20of%20data-driven%20discovery%20in%20nonlinear%20dynamical%20systems%2C%20these%0Amethods%20have%20garnered%20significant%20attention.%20Among%20them%2C%20Sparse%20Identification%0Aof%20Nonlinear%20Dynamics%20%28SINDy%29%20has%20emerged%20as%20a%20transformative%20approach%2C%0Adistilling%20complex%20dynamical%20behaviors%20into%20interpretable%20linear%20combinations%0Aof%20basis%20functions.%20However%2C%20SINDy%20relies%20on%20domain-specific%20expertise%20to%0Aconstruct%20its%20foundational%20%22library%22%20of%20basis%20functions%2C%20which%20limits%20its%0Aadaptability%20and%20universality.%20In%20this%20work%2C%20we%20introduce%20a%20nonlinear%20system%0Aidentification%20framework%20called%20LeARN%20that%20transcends%20the%20need%20for%20prior%20domain%0Aknowledge%20by%20learning%20the%20library%20of%20basis%20functions%20directly%20from%20data.%20To%0Aenhance%20adaptability%20to%20evolving%20system%20dynamics%20under%20varying%20noise%0Aconditions%2C%20we%20employ%20a%20novel%20meta-learning-based%20system%20identification%0Aapproach%20that%20uses%20a%20lightweight%20deep%20neural%20network%20%28DNN%29%20to%20dynamically%0Arefine%20these%20basis%20functions.%20This%20not%20only%20captures%20intricate%20system%20behaviors%0Abut%20also%20adapts%20seamlessly%20to%20new%20dynamical%20regimes.%20We%20validate%20our%20framework%0Aon%20the%20Neural%20Fly%20dataset%2C%20showcasing%20its%20robust%20adaptation%20and%20generalization%0Acapabilities.%20Despite%20its%20simplicity%2C%20our%20LeARN%20achieves%20competitive%20dynamical%0Aerror%20performance%20compared%20to%20SINDy.%20This%20work%20presents%20a%20step%20toward%20the%0Aautonomous%20discovery%20of%20dynamical%20systems%2C%20paving%20the%20way%20for%20a%20future%20where%0Amachine%20learning%20uncovers%20the%20governing%20principles%20of%20complex%20systems%20without%0Arequiring%20extensive%20domain-specific%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12036v1&entry.124074799=Read"},
{"title": "AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power\n  Laws", "author": "Oren Neumann and Claudius Gros", "abstract": "  Neural scaling laws are observed in a range of domains, to date with no clear\nunderstanding of why they occur. Recent theories suggest that loss power laws\narise from Zipf's law, a power law observed in domains like natural language.\nOne theory suggests that language scaling laws emerge when Zipf-distributed\ntask quanta are learned in descending order of frequency. In this paper we\nexamine power-law scaling in AlphaZero, a reinforcement learning algorithm,\nusing a theory of language-model scaling. We find that game states in training\nand inference data scale with Zipf's law, which is known to arise from the tree\nstructure of the environment, and examine the correlation between scaling-law\nand Zipf's-law exponents. In agreement with quanta scaling theory, we find that\nagents optimize state loss in descending order of frequency, even though this\norder scales inversely with modelling complexity. We also find that inverse\nscaling, the failure of models to improve with size, is correlated with unusual\nZipf curves where end-game states are among the most frequent states. We show\nevidence that larger models shift their focus to these less-important states,\nsacrificing their understanding of important early-game states.\n", "link": "http://arxiv.org/abs/2412.11979v1", "date": "2024-12-16", "relevancy": 1.803, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaZero%20Neural%20Scaling%20and%20Zipf%27s%20Law%3A%20a%20Tale%20of%20Board%20Games%20and%20Power%0A%20%20Laws&body=Title%3A%20AlphaZero%20Neural%20Scaling%20and%20Zipf%27s%20Law%3A%20a%20Tale%20of%20Board%20Games%20and%20Power%0A%20%20Laws%0AAuthor%3A%20Oren%20Neumann%20and%20Claudius%20Gros%0AAbstract%3A%20%20%20Neural%20scaling%20laws%20are%20observed%20in%20a%20range%20of%20domains%2C%20to%20date%20with%20no%20clear%0Aunderstanding%20of%20why%20they%20occur.%20Recent%20theories%20suggest%20that%20loss%20power%20laws%0Aarise%20from%20Zipf%27s%20law%2C%20a%20power%20law%20observed%20in%20domains%20like%20natural%20language.%0AOne%20theory%20suggests%20that%20language%20scaling%20laws%20emerge%20when%20Zipf-distributed%0Atask%20quanta%20are%20learned%20in%20descending%20order%20of%20frequency.%20In%20this%20paper%20we%0Aexamine%20power-law%20scaling%20in%20AlphaZero%2C%20a%20reinforcement%20learning%20algorithm%2C%0Ausing%20a%20theory%20of%20language-model%20scaling.%20We%20find%20that%20game%20states%20in%20training%0Aand%20inference%20data%20scale%20with%20Zipf%27s%20law%2C%20which%20is%20known%20to%20arise%20from%20the%20tree%0Astructure%20of%20the%20environment%2C%20and%20examine%20the%20correlation%20between%20scaling-law%0Aand%20Zipf%27s-law%20exponents.%20In%20agreement%20with%20quanta%20scaling%20theory%2C%20we%20find%20that%0Aagents%20optimize%20state%20loss%20in%20descending%20order%20of%20frequency%2C%20even%20though%20this%0Aorder%20scales%20inversely%20with%20modelling%20complexity.%20We%20also%20find%20that%20inverse%0Ascaling%2C%20the%20failure%20of%20models%20to%20improve%20with%20size%2C%20is%20correlated%20with%20unusual%0AZipf%20curves%20where%20end-game%20states%20are%20among%20the%20most%20frequent%20states.%20We%20show%0Aevidence%20that%20larger%20models%20shift%20their%20focus%20to%20these%20less-important%20states%2C%0Asacrificing%20their%20understanding%20of%20important%20early-game%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaZero%2520Neural%2520Scaling%2520and%2520Zipf%2527s%2520Law%253A%2520a%2520Tale%2520of%2520Board%2520Games%2520and%2520Power%250A%2520%2520Laws%26entry.906535625%3DOren%2520Neumann%2520and%2520Claudius%2520Gros%26entry.1292438233%3D%2520%2520Neural%2520scaling%2520laws%2520are%2520observed%2520in%2520a%2520range%2520of%2520domains%252C%2520to%2520date%2520with%2520no%2520clear%250Aunderstanding%2520of%2520why%2520they%2520occur.%2520Recent%2520theories%2520suggest%2520that%2520loss%2520power%2520laws%250Aarise%2520from%2520Zipf%2527s%2520law%252C%2520a%2520power%2520law%2520observed%2520in%2520domains%2520like%2520natural%2520language.%250AOne%2520theory%2520suggests%2520that%2520language%2520scaling%2520laws%2520emerge%2520when%2520Zipf-distributed%250Atask%2520quanta%2520are%2520learned%2520in%2520descending%2520order%2520of%2520frequency.%2520In%2520this%2520paper%2520we%250Aexamine%2520power-law%2520scaling%2520in%2520AlphaZero%252C%2520a%2520reinforcement%2520learning%2520algorithm%252C%250Ausing%2520a%2520theory%2520of%2520language-model%2520scaling.%2520We%2520find%2520that%2520game%2520states%2520in%2520training%250Aand%2520inference%2520data%2520scale%2520with%2520Zipf%2527s%2520law%252C%2520which%2520is%2520known%2520to%2520arise%2520from%2520the%2520tree%250Astructure%2520of%2520the%2520environment%252C%2520and%2520examine%2520the%2520correlation%2520between%2520scaling-law%250Aand%2520Zipf%2527s-law%2520exponents.%2520In%2520agreement%2520with%2520quanta%2520scaling%2520theory%252C%2520we%2520find%2520that%250Aagents%2520optimize%2520state%2520loss%2520in%2520descending%2520order%2520of%2520frequency%252C%2520even%2520though%2520this%250Aorder%2520scales%2520inversely%2520with%2520modelling%2520complexity.%2520We%2520also%2520find%2520that%2520inverse%250Ascaling%252C%2520the%2520failure%2520of%2520models%2520to%2520improve%2520with%2520size%252C%2520is%2520correlated%2520with%2520unusual%250AZipf%2520curves%2520where%2520end-game%2520states%2520are%2520among%2520the%2520most%2520frequent%2520states.%2520We%2520show%250Aevidence%2520that%2520larger%2520models%2520shift%2520their%2520focus%2520to%2520these%2520less-important%2520states%252C%250Asacrificing%2520their%2520understanding%2520of%2520important%2520early-game%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaZero%20Neural%20Scaling%20and%20Zipf%27s%20Law%3A%20a%20Tale%20of%20Board%20Games%20and%20Power%0A%20%20Laws&entry.906535625=Oren%20Neumann%20and%20Claudius%20Gros&entry.1292438233=%20%20Neural%20scaling%20laws%20are%20observed%20in%20a%20range%20of%20domains%2C%20to%20date%20with%20no%20clear%0Aunderstanding%20of%20why%20they%20occur.%20Recent%20theories%20suggest%20that%20loss%20power%20laws%0Aarise%20from%20Zipf%27s%20law%2C%20a%20power%20law%20observed%20in%20domains%20like%20natural%20language.%0AOne%20theory%20suggests%20that%20language%20scaling%20laws%20emerge%20when%20Zipf-distributed%0Atask%20quanta%20are%20learned%20in%20descending%20order%20of%20frequency.%20In%20this%20paper%20we%0Aexamine%20power-law%20scaling%20in%20AlphaZero%2C%20a%20reinforcement%20learning%20algorithm%2C%0Ausing%20a%20theory%20of%20language-model%20scaling.%20We%20find%20that%20game%20states%20in%20training%0Aand%20inference%20data%20scale%20with%20Zipf%27s%20law%2C%20which%20is%20known%20to%20arise%20from%20the%20tree%0Astructure%20of%20the%20environment%2C%20and%20examine%20the%20correlation%20between%20scaling-law%0Aand%20Zipf%27s-law%20exponents.%20In%20agreement%20with%20quanta%20scaling%20theory%2C%20we%20find%20that%0Aagents%20optimize%20state%20loss%20in%20descending%20order%20of%20frequency%2C%20even%20though%20this%0Aorder%20scales%20inversely%20with%20modelling%20complexity.%20We%20also%20find%20that%20inverse%0Ascaling%2C%20the%20failure%20of%20models%20to%20improve%20with%20size%2C%20is%20correlated%20with%20unusual%0AZipf%20curves%20where%20end-game%20states%20are%20among%20the%20most%20frequent%20states.%20We%20show%0Aevidence%20that%20larger%20models%20shift%20their%20focus%20to%20these%20less-important%20states%2C%0Asacrificing%20their%20understanding%20of%20important%20early-game%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11979v1&entry.124074799=Read"},
{"title": "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?", "author": "Adrian de Wynter and Ishaan Watts and Tua Wongsangaroonsri and Minghui Zhang and Noura Farra and Nektar Ege Alt\u0131ntoprak and Lena Baur and Samantha Claudet and Pavel Gajdusek and Can G\u00f6ren and Qilong Gu and Anna Kaminska and Tomasz Kaminski and Ruby Kuo and Akiko Kyuba and Jongho Lee and Kartik Mathur and Petter Merok and Ivana Milovanovi\u0107 and Nani Paananen and Vesa-Matti Paananen and Anna Pavlenko and Bruno Pereira Vidal and Luciano Strika and Yueh Tsao and Davide Turcato and Oleksandr Vakhno and Judit Velcsov and Anna Vickers and St\u00e9phanie Visser and Herdyan Widarmanto and Andrey Zaikin and Si-Qing Chen", "abstract": "  Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment.\n", "link": "http://arxiv.org/abs/2404.14397v2", "date": "2024-12-16", "relevancy": 1.7949, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RTP-LX%3A%20Can%20LLMs%20Evaluate%20Toxicity%20in%20Multilingual%20Scenarios%3F&body=Title%3A%20RTP-LX%3A%20Can%20LLMs%20Evaluate%20Toxicity%20in%20Multilingual%20Scenarios%3F%0AAuthor%3A%20Adrian%20de%20Wynter%20and%20Ishaan%20Watts%20and%20Tua%20Wongsangaroonsri%20and%20Minghui%20Zhang%20and%20Noura%20Farra%20and%20Nektar%20Ege%20Alt%C4%B1ntoprak%20and%20Lena%20Baur%20and%20Samantha%20Claudet%20and%20Pavel%20Gajdusek%20and%20Can%20G%C3%B6ren%20and%20Qilong%20Gu%20and%20Anna%20Kaminska%20and%20Tomasz%20Kaminski%20and%20Ruby%20Kuo%20and%20Akiko%20Kyuba%20and%20Jongho%20Lee%20and%20Kartik%20Mathur%20and%20Petter%20Merok%20and%20Ivana%20Milovanovi%C4%87%20and%20Nani%20Paananen%20and%20Vesa-Matti%20Paananen%20and%20Anna%20Pavlenko%20and%20Bruno%20Pereira%20Vidal%20and%20Luciano%20Strika%20and%20Yueh%20Tsao%20and%20Davide%20Turcato%20and%20Oleksandr%20Vakhno%20and%20Judit%20Velcsov%20and%20Anna%20Vickers%20and%20St%C3%A9phanie%20Visser%20and%20Herdyan%20Widarmanto%20and%20Andrey%20Zaikin%20and%20Si-Qing%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20and%20small%20language%20models%20%28SLMs%29%20are%20being%0Aadopted%20at%20remarkable%20speed%2C%20although%20their%20safety%20still%20remains%20a%20serious%0Aconcern.%20With%20the%20advent%20of%20multilingual%20S/LLMs%2C%20the%20question%20now%20becomes%20a%0Amatter%20of%20scale%3A%20can%20we%20expand%20multilingual%20safety%20evaluations%20of%20these%20models%0Awith%20the%20same%20velocity%20at%20which%20they%20are%20deployed%3F%20To%20this%20end%2C%20we%20introduce%0ARTP-LX%2C%20a%20human-transcreated%20and%20human-annotated%20corpus%20of%20toxic%20prompts%20and%0Aoutputs%20in%2028%20languages.%20RTP-LX%20follows%20participatory%20design%20practices%2C%20and%20a%0Aportion%20of%20the%20corpus%20is%20especially%20designed%20to%20detect%20culturally-specific%0Atoxic%20language.%20We%20evaluate%2010%20S/LLMs%20on%20their%20ability%20to%20detect%20toxic%20content%0Ain%20a%20culturally-sensitive%2C%20multilingual%20scenario.%20We%20find%20that%2C%20although%20they%0Atypically%20score%20acceptably%20in%20terms%20of%20accuracy%2C%20they%20have%20low%20agreement%20with%0Ahuman%20judges%20when%20scoring%20holistically%20the%20toxicity%20of%20a%20prompt%3B%20and%20have%0Adifficulty%20discerning%20harm%20in%20context-dependent%20scenarios%2C%20particularly%20with%0Asubtle-yet-harmful%20content%20%28e.g.%20microaggressions%2C%20bias%29.%20We%20release%20this%0Adataset%20to%20contribute%20to%20further%20reduce%20harmful%20uses%20of%20these%20models%20and%0Aimprove%20their%20safe%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRTP-LX%253A%2520Can%2520LLMs%2520Evaluate%2520Toxicity%2520in%2520Multilingual%2520Scenarios%253F%26entry.906535625%3DAdrian%2520de%2520Wynter%2520and%2520Ishaan%2520Watts%2520and%2520Tua%2520Wongsangaroonsri%2520and%2520Minghui%2520Zhang%2520and%2520Noura%2520Farra%2520and%2520Nektar%2520Ege%2520Alt%25C4%25B1ntoprak%2520and%2520Lena%2520Baur%2520and%2520Samantha%2520Claudet%2520and%2520Pavel%2520Gajdusek%2520and%2520Can%2520G%25C3%25B6ren%2520and%2520Qilong%2520Gu%2520and%2520Anna%2520Kaminska%2520and%2520Tomasz%2520Kaminski%2520and%2520Ruby%2520Kuo%2520and%2520Akiko%2520Kyuba%2520and%2520Jongho%2520Lee%2520and%2520Kartik%2520Mathur%2520and%2520Petter%2520Merok%2520and%2520Ivana%2520Milovanovi%25C4%2587%2520and%2520Nani%2520Paananen%2520and%2520Vesa-Matti%2520Paananen%2520and%2520Anna%2520Pavlenko%2520and%2520Bruno%2520Pereira%2520Vidal%2520and%2520Luciano%2520Strika%2520and%2520Yueh%2520Tsao%2520and%2520Davide%2520Turcato%2520and%2520Oleksandr%2520Vakhno%2520and%2520Judit%2520Velcsov%2520and%2520Anna%2520Vickers%2520and%2520St%25C3%25A9phanie%2520Visser%2520and%2520Herdyan%2520Widarmanto%2520and%2520Andrey%2520Zaikin%2520and%2520Si-Qing%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520and%2520small%2520language%2520models%2520%2528SLMs%2529%2520are%2520being%250Aadopted%2520at%2520remarkable%2520speed%252C%2520although%2520their%2520safety%2520still%2520remains%2520a%2520serious%250Aconcern.%2520With%2520the%2520advent%2520of%2520multilingual%2520S/LLMs%252C%2520the%2520question%2520now%2520becomes%2520a%250Amatter%2520of%2520scale%253A%2520can%2520we%2520expand%2520multilingual%2520safety%2520evaluations%2520of%2520these%2520models%250Awith%2520the%2520same%2520velocity%2520at%2520which%2520they%2520are%2520deployed%253F%2520To%2520this%2520end%252C%2520we%2520introduce%250ARTP-LX%252C%2520a%2520human-transcreated%2520and%2520human-annotated%2520corpus%2520of%2520toxic%2520prompts%2520and%250Aoutputs%2520in%252028%2520languages.%2520RTP-LX%2520follows%2520participatory%2520design%2520practices%252C%2520and%2520a%250Aportion%2520of%2520the%2520corpus%2520is%2520especially%2520designed%2520to%2520detect%2520culturally-specific%250Atoxic%2520language.%2520We%2520evaluate%252010%2520S/LLMs%2520on%2520their%2520ability%2520to%2520detect%2520toxic%2520content%250Ain%2520a%2520culturally-sensitive%252C%2520multilingual%2520scenario.%2520We%2520find%2520that%252C%2520although%2520they%250Atypically%2520score%2520acceptably%2520in%2520terms%2520of%2520accuracy%252C%2520they%2520have%2520low%2520agreement%2520with%250Ahuman%2520judges%2520when%2520scoring%2520holistically%2520the%2520toxicity%2520of%2520a%2520prompt%253B%2520and%2520have%250Adifficulty%2520discerning%2520harm%2520in%2520context-dependent%2520scenarios%252C%2520particularly%2520with%250Asubtle-yet-harmful%2520content%2520%2528e.g.%2520microaggressions%252C%2520bias%2529.%2520We%2520release%2520this%250Adataset%2520to%2520contribute%2520to%2520further%2520reduce%2520harmful%2520uses%2520of%2520these%2520models%2520and%250Aimprove%2520their%2520safe%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTP-LX%3A%20Can%20LLMs%20Evaluate%20Toxicity%20in%20Multilingual%20Scenarios%3F&entry.906535625=Adrian%20de%20Wynter%20and%20Ishaan%20Watts%20and%20Tua%20Wongsangaroonsri%20and%20Minghui%20Zhang%20and%20Noura%20Farra%20and%20Nektar%20Ege%20Alt%C4%B1ntoprak%20and%20Lena%20Baur%20and%20Samantha%20Claudet%20and%20Pavel%20Gajdusek%20and%20Can%20G%C3%B6ren%20and%20Qilong%20Gu%20and%20Anna%20Kaminska%20and%20Tomasz%20Kaminski%20and%20Ruby%20Kuo%20and%20Akiko%20Kyuba%20and%20Jongho%20Lee%20and%20Kartik%20Mathur%20and%20Petter%20Merok%20and%20Ivana%20Milovanovi%C4%87%20and%20Nani%20Paananen%20and%20Vesa-Matti%20Paananen%20and%20Anna%20Pavlenko%20and%20Bruno%20Pereira%20Vidal%20and%20Luciano%20Strika%20and%20Yueh%20Tsao%20and%20Davide%20Turcato%20and%20Oleksandr%20Vakhno%20and%20Judit%20Velcsov%20and%20Anna%20Vickers%20and%20St%C3%A9phanie%20Visser%20and%20Herdyan%20Widarmanto%20and%20Andrey%20Zaikin%20and%20Si-Qing%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20and%20small%20language%20models%20%28SLMs%29%20are%20being%0Aadopted%20at%20remarkable%20speed%2C%20although%20their%20safety%20still%20remains%20a%20serious%0Aconcern.%20With%20the%20advent%20of%20multilingual%20S/LLMs%2C%20the%20question%20now%20becomes%20a%0Amatter%20of%20scale%3A%20can%20we%20expand%20multilingual%20safety%20evaluations%20of%20these%20models%0Awith%20the%20same%20velocity%20at%20which%20they%20are%20deployed%3F%20To%20this%20end%2C%20we%20introduce%0ARTP-LX%2C%20a%20human-transcreated%20and%20human-annotated%20corpus%20of%20toxic%20prompts%20and%0Aoutputs%20in%2028%20languages.%20RTP-LX%20follows%20participatory%20design%20practices%2C%20and%20a%0Aportion%20of%20the%20corpus%20is%20especially%20designed%20to%20detect%20culturally-specific%0Atoxic%20language.%20We%20evaluate%2010%20S/LLMs%20on%20their%20ability%20to%20detect%20toxic%20content%0Ain%20a%20culturally-sensitive%2C%20multilingual%20scenario.%20We%20find%20that%2C%20although%20they%0Atypically%20score%20acceptably%20in%20terms%20of%20accuracy%2C%20they%20have%20low%20agreement%20with%0Ahuman%20judges%20when%20scoring%20holistically%20the%20toxicity%20of%20a%20prompt%3B%20and%20have%0Adifficulty%20discerning%20harm%20in%20context-dependent%20scenarios%2C%20particularly%20with%0Asubtle-yet-harmful%20content%20%28e.g.%20microaggressions%2C%20bias%29.%20We%20release%20this%0Adataset%20to%20contribute%20to%20further%20reduce%20harmful%20uses%20of%20these%20models%20and%0Aimprove%20their%20safe%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14397v2&entry.124074799=Read"},
{"title": "OpenReviewer: A Specialized Large Language Model for Generating Critical\n  Scientific Paper Reviews", "author": "Maximilian Idahl and Zahra Ahmadi", "abstract": "  We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top ML conferences. Given a PDF paper submission\nand review template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces significantly more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool.\n", "link": "http://arxiv.org/abs/2412.11948v1", "date": "2024-12-16", "relevancy": 1.7935, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenReviewer%3A%20A%20Specialized%20Large%20Language%20Model%20for%20Generating%20Critical%0A%20%20Scientific%20Paper%20Reviews&body=Title%3A%20OpenReviewer%3A%20A%20Specialized%20Large%20Language%20Model%20for%20Generating%20Critical%0A%20%20Scientific%20Paper%20Reviews%0AAuthor%3A%20Maximilian%20Idahl%20and%20Zahra%20Ahmadi%0AAbstract%3A%20%20%20We%20present%20OpenReviewer%2C%20an%20open-source%20system%20for%20generating%20high-quality%0Apeer%20reviews%20of%20machine%20learning%20and%20AI%20conference%20papers.%20At%20its%20core%20is%0ALlama-OpenReviewer-8B%2C%20an%208B%20parameter%20language%20model%20specifically%20fine-tuned%0Aon%2079%2C000%20expert%20reviews%20from%20top%20ML%20conferences.%20Given%20a%20PDF%20paper%20submission%0Aand%20review%20template%20as%20input%2C%20OpenReviewer%20extracts%20the%20full%20text%2C%20including%0Atechnical%20content%20like%20equations%20and%20tables%2C%20and%20generates%20a%20structured%20review%0Afollowing%20conference-specific%20guidelines.%20Our%20evaluation%20on%20400%20test%20papers%0Ashows%20that%20OpenReviewer%20produces%20significantly%20more%20critical%20and%20realistic%0Areviews%20compared%20to%20general-purpose%20LLMs%20like%20GPT-4%20and%20Claude-3.5.%20While%20other%0ALLMs%20tend%20toward%20overly%20positive%20assessments%2C%20OpenReviewer%27s%20recommendations%0Aclosely%20match%20the%20distribution%20of%20human%20reviewer%20ratings.%20The%20system%20provides%0Aauthors%20with%20rapid%2C%20constructive%20feedback%20to%20improve%20their%20manuscripts%20before%0Asubmission%2C%20though%20it%20is%20not%20intended%20to%20replace%20human%20peer%20review.%0AOpenReviewer%20is%20available%20as%20an%20online%20demo%20and%20open-source%20tool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenReviewer%253A%2520A%2520Specialized%2520Large%2520Language%2520Model%2520for%2520Generating%2520Critical%250A%2520%2520Scientific%2520Paper%2520Reviews%26entry.906535625%3DMaximilian%2520Idahl%2520and%2520Zahra%2520Ahmadi%26entry.1292438233%3D%2520%2520We%2520present%2520OpenReviewer%252C%2520an%2520open-source%2520system%2520for%2520generating%2520high-quality%250Apeer%2520reviews%2520of%2520machine%2520learning%2520and%2520AI%2520conference%2520papers.%2520At%2520its%2520core%2520is%250ALlama-OpenReviewer-8B%252C%2520an%25208B%2520parameter%2520language%2520model%2520specifically%2520fine-tuned%250Aon%252079%252C000%2520expert%2520reviews%2520from%2520top%2520ML%2520conferences.%2520Given%2520a%2520PDF%2520paper%2520submission%250Aand%2520review%2520template%2520as%2520input%252C%2520OpenReviewer%2520extracts%2520the%2520full%2520text%252C%2520including%250Atechnical%2520content%2520like%2520equations%2520and%2520tables%252C%2520and%2520generates%2520a%2520structured%2520review%250Afollowing%2520conference-specific%2520guidelines.%2520Our%2520evaluation%2520on%2520400%2520test%2520papers%250Ashows%2520that%2520OpenReviewer%2520produces%2520significantly%2520more%2520critical%2520and%2520realistic%250Areviews%2520compared%2520to%2520general-purpose%2520LLMs%2520like%2520GPT-4%2520and%2520Claude-3.5.%2520While%2520other%250ALLMs%2520tend%2520toward%2520overly%2520positive%2520assessments%252C%2520OpenReviewer%2527s%2520recommendations%250Aclosely%2520match%2520the%2520distribution%2520of%2520human%2520reviewer%2520ratings.%2520The%2520system%2520provides%250Aauthors%2520with%2520rapid%252C%2520constructive%2520feedback%2520to%2520improve%2520their%2520manuscripts%2520before%250Asubmission%252C%2520though%2520it%2520is%2520not%2520intended%2520to%2520replace%2520human%2520peer%2520review.%250AOpenReviewer%2520is%2520available%2520as%2520an%2520online%2520demo%2520and%2520open-source%2520tool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenReviewer%3A%20A%20Specialized%20Large%20Language%20Model%20for%20Generating%20Critical%0A%20%20Scientific%20Paper%20Reviews&entry.906535625=Maximilian%20Idahl%20and%20Zahra%20Ahmadi&entry.1292438233=%20%20We%20present%20OpenReviewer%2C%20an%20open-source%20system%20for%20generating%20high-quality%0Apeer%20reviews%20of%20machine%20learning%20and%20AI%20conference%20papers.%20At%20its%20core%20is%0ALlama-OpenReviewer-8B%2C%20an%208B%20parameter%20language%20model%20specifically%20fine-tuned%0Aon%2079%2C000%20expert%20reviews%20from%20top%20ML%20conferences.%20Given%20a%20PDF%20paper%20submission%0Aand%20review%20template%20as%20input%2C%20OpenReviewer%20extracts%20the%20full%20text%2C%20including%0Atechnical%20content%20like%20equations%20and%20tables%2C%20and%20generates%20a%20structured%20review%0Afollowing%20conference-specific%20guidelines.%20Our%20evaluation%20on%20400%20test%20papers%0Ashows%20that%20OpenReviewer%20produces%20significantly%20more%20critical%20and%20realistic%0Areviews%20compared%20to%20general-purpose%20LLMs%20like%20GPT-4%20and%20Claude-3.5.%20While%20other%0ALLMs%20tend%20toward%20overly%20positive%20assessments%2C%20OpenReviewer%27s%20recommendations%0Aclosely%20match%20the%20distribution%20of%20human%20reviewer%20ratings.%20The%20system%20provides%0Aauthors%20with%20rapid%2C%20constructive%20feedback%20to%20improve%20their%20manuscripts%20before%0Asubmission%2C%20though%20it%20is%20not%20intended%20to%20replace%20human%20peer%20review.%0AOpenReviewer%20is%20available%20as%20an%20online%20demo%20and%20open-source%20tool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11948v1&entry.124074799=Read"},
{"title": "Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A\n  Novel Weighted Retrieval-Augmented Generation Paradigm", "author": "Rajat Khanda", "abstract": "  Technical troubleshooting in enterprise environments often involves\nnavigating diverse, heterogeneous data sources to resolve complex issues\neffectively. This paper presents a novel agentic AI solution built on a\nWeighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise\ntechnical troubleshooting. By dynamically weighting retrieval sources such as\nproduct manuals, internal knowledge bases, FAQs, and troubleshooting guides\nbased on query context, the framework prioritizes the most relevant data. For\ninstance, it gives precedence to product manuals for SKU-specific queries while\nincorporating general FAQs for broader issues. The system employs FAISS for\nefficient dense vector search, coupled with a dynamic aggregation mechanism to\nseamlessly integrate results from multiple sources. A Llama-based\nself-evaluator ensures the contextual accuracy and confidence of the generated\nresponses before delivering them. This iterative cycle of retrieval and\nvalidation enhances precision, diversity, and reliability in response\ngeneration. Preliminary evaluations on large enterprise datasets demonstrate\nthe framework's efficacy in improving troubleshooting accuracy, reducing\nresolution times, and adapting to varied technical challenges. Future research\naims to enhance the framework by integrating advanced conversational AI\ncapabilities, enabling more interactive and intuitive troubleshooting\nexperiences. Efforts will also focus on refining the dynamic weighting\nmechanism through reinforcement learning to further optimize the relevance and\nprecision of retrieved information. By incorporating these advancements, the\nproposed framework is poised to evolve into a comprehensive, autonomous AI\nsolution, redefining technical service workflows across enterprise settings.\n", "link": "http://arxiv.org/abs/2412.12006v1", "date": "2024-12-16", "relevancy": 1.7676, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4717}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4362}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20AI-Driven%20Technical%20Troubleshooting%20for%20Enterprise%20Systems%3A%20A%0A%20%20Novel%20Weighted%20Retrieval-Augmented%20Generation%20Paradigm&body=Title%3A%20Agentic%20AI-Driven%20Technical%20Troubleshooting%20for%20Enterprise%20Systems%3A%20A%0A%20%20Novel%20Weighted%20Retrieval-Augmented%20Generation%20Paradigm%0AAuthor%3A%20Rajat%20Khanda%0AAbstract%3A%20%20%20Technical%20troubleshooting%20in%20enterprise%20environments%20often%20involves%0Anavigating%20diverse%2C%20heterogeneous%20data%20sources%20to%20resolve%20complex%20issues%0Aeffectively.%20This%20paper%20presents%20a%20novel%20agentic%20AI%20solution%20built%20on%20a%0AWeighted%20Retrieval-Augmented%20Generation%20%28RAG%29%20Framework%20tailored%20for%20enterprise%0Atechnical%20troubleshooting.%20By%20dynamically%20weighting%20retrieval%20sources%20such%20as%0Aproduct%20manuals%2C%20internal%20knowledge%20bases%2C%20FAQs%2C%20and%20troubleshooting%20guides%0Abased%20on%20query%20context%2C%20the%20framework%20prioritizes%20the%20most%20relevant%20data.%20For%0Ainstance%2C%20it%20gives%20precedence%20to%20product%20manuals%20for%20SKU-specific%20queries%20while%0Aincorporating%20general%20FAQs%20for%20broader%20issues.%20The%20system%20employs%20FAISS%20for%0Aefficient%20dense%20vector%20search%2C%20coupled%20with%20a%20dynamic%20aggregation%20mechanism%20to%0Aseamlessly%20integrate%20results%20from%20multiple%20sources.%20A%20Llama-based%0Aself-evaluator%20ensures%20the%20contextual%20accuracy%20and%20confidence%20of%20the%20generated%0Aresponses%20before%20delivering%20them.%20This%20iterative%20cycle%20of%20retrieval%20and%0Avalidation%20enhances%20precision%2C%20diversity%2C%20and%20reliability%20in%20response%0Ageneration.%20Preliminary%20evaluations%20on%20large%20enterprise%20datasets%20demonstrate%0Athe%20framework%27s%20efficacy%20in%20improving%20troubleshooting%20accuracy%2C%20reducing%0Aresolution%20times%2C%20and%20adapting%20to%20varied%20technical%20challenges.%20Future%20research%0Aaims%20to%20enhance%20the%20framework%20by%20integrating%20advanced%20conversational%20AI%0Acapabilities%2C%20enabling%20more%20interactive%20and%20intuitive%20troubleshooting%0Aexperiences.%20Efforts%20will%20also%20focus%20on%20refining%20the%20dynamic%20weighting%0Amechanism%20through%20reinforcement%20learning%20to%20further%20optimize%20the%20relevance%20and%0Aprecision%20of%20retrieved%20information.%20By%20incorporating%20these%20advancements%2C%20the%0Aproposed%20framework%20is%20poised%20to%20evolve%20into%20a%20comprehensive%2C%20autonomous%20AI%0Asolution%2C%20redefining%20technical%20service%20workflows%20across%20enterprise%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520AI-Driven%2520Technical%2520Troubleshooting%2520for%2520Enterprise%2520Systems%253A%2520A%250A%2520%2520Novel%2520Weighted%2520Retrieval-Augmented%2520Generation%2520Paradigm%26entry.906535625%3DRajat%2520Khanda%26entry.1292438233%3D%2520%2520Technical%2520troubleshooting%2520in%2520enterprise%2520environments%2520often%2520involves%250Anavigating%2520diverse%252C%2520heterogeneous%2520data%2520sources%2520to%2520resolve%2520complex%2520issues%250Aeffectively.%2520This%2520paper%2520presents%2520a%2520novel%2520agentic%2520AI%2520solution%2520built%2520on%2520a%250AWeighted%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520Framework%2520tailored%2520for%2520enterprise%250Atechnical%2520troubleshooting.%2520By%2520dynamically%2520weighting%2520retrieval%2520sources%2520such%2520as%250Aproduct%2520manuals%252C%2520internal%2520knowledge%2520bases%252C%2520FAQs%252C%2520and%2520troubleshooting%2520guides%250Abased%2520on%2520query%2520context%252C%2520the%2520framework%2520prioritizes%2520the%2520most%2520relevant%2520data.%2520For%250Ainstance%252C%2520it%2520gives%2520precedence%2520to%2520product%2520manuals%2520for%2520SKU-specific%2520queries%2520while%250Aincorporating%2520general%2520FAQs%2520for%2520broader%2520issues.%2520The%2520system%2520employs%2520FAISS%2520for%250Aefficient%2520dense%2520vector%2520search%252C%2520coupled%2520with%2520a%2520dynamic%2520aggregation%2520mechanism%2520to%250Aseamlessly%2520integrate%2520results%2520from%2520multiple%2520sources.%2520A%2520Llama-based%250Aself-evaluator%2520ensures%2520the%2520contextual%2520accuracy%2520and%2520confidence%2520of%2520the%2520generated%250Aresponses%2520before%2520delivering%2520them.%2520This%2520iterative%2520cycle%2520of%2520retrieval%2520and%250Avalidation%2520enhances%2520precision%252C%2520diversity%252C%2520and%2520reliability%2520in%2520response%250Ageneration.%2520Preliminary%2520evaluations%2520on%2520large%2520enterprise%2520datasets%2520demonstrate%250Athe%2520framework%2527s%2520efficacy%2520in%2520improving%2520troubleshooting%2520accuracy%252C%2520reducing%250Aresolution%2520times%252C%2520and%2520adapting%2520to%2520varied%2520technical%2520challenges.%2520Future%2520research%250Aaims%2520to%2520enhance%2520the%2520framework%2520by%2520integrating%2520advanced%2520conversational%2520AI%250Acapabilities%252C%2520enabling%2520more%2520interactive%2520and%2520intuitive%2520troubleshooting%250Aexperiences.%2520Efforts%2520will%2520also%2520focus%2520on%2520refining%2520the%2520dynamic%2520weighting%250Amechanism%2520through%2520reinforcement%2520learning%2520to%2520further%2520optimize%2520the%2520relevance%2520and%250Aprecision%2520of%2520retrieved%2520information.%2520By%2520incorporating%2520these%2520advancements%252C%2520the%250Aproposed%2520framework%2520is%2520poised%2520to%2520evolve%2520into%2520a%2520comprehensive%252C%2520autonomous%2520AI%250Asolution%252C%2520redefining%2520technical%2520service%2520workflows%2520across%2520enterprise%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20AI-Driven%20Technical%20Troubleshooting%20for%20Enterprise%20Systems%3A%20A%0A%20%20Novel%20Weighted%20Retrieval-Augmented%20Generation%20Paradigm&entry.906535625=Rajat%20Khanda&entry.1292438233=%20%20Technical%20troubleshooting%20in%20enterprise%20environments%20often%20involves%0Anavigating%20diverse%2C%20heterogeneous%20data%20sources%20to%20resolve%20complex%20issues%0Aeffectively.%20This%20paper%20presents%20a%20novel%20agentic%20AI%20solution%20built%20on%20a%0AWeighted%20Retrieval-Augmented%20Generation%20%28RAG%29%20Framework%20tailored%20for%20enterprise%0Atechnical%20troubleshooting.%20By%20dynamically%20weighting%20retrieval%20sources%20such%20as%0Aproduct%20manuals%2C%20internal%20knowledge%20bases%2C%20FAQs%2C%20and%20troubleshooting%20guides%0Abased%20on%20query%20context%2C%20the%20framework%20prioritizes%20the%20most%20relevant%20data.%20For%0Ainstance%2C%20it%20gives%20precedence%20to%20product%20manuals%20for%20SKU-specific%20queries%20while%0Aincorporating%20general%20FAQs%20for%20broader%20issues.%20The%20system%20employs%20FAISS%20for%0Aefficient%20dense%20vector%20search%2C%20coupled%20with%20a%20dynamic%20aggregation%20mechanism%20to%0Aseamlessly%20integrate%20results%20from%20multiple%20sources.%20A%20Llama-based%0Aself-evaluator%20ensures%20the%20contextual%20accuracy%20and%20confidence%20of%20the%20generated%0Aresponses%20before%20delivering%20them.%20This%20iterative%20cycle%20of%20retrieval%20and%0Avalidation%20enhances%20precision%2C%20diversity%2C%20and%20reliability%20in%20response%0Ageneration.%20Preliminary%20evaluations%20on%20large%20enterprise%20datasets%20demonstrate%0Athe%20framework%27s%20efficacy%20in%20improving%20troubleshooting%20accuracy%2C%20reducing%0Aresolution%20times%2C%20and%20adapting%20to%20varied%20technical%20challenges.%20Future%20research%0Aaims%20to%20enhance%20the%20framework%20by%20integrating%20advanced%20conversational%20AI%0Acapabilities%2C%20enabling%20more%20interactive%20and%20intuitive%20troubleshooting%0Aexperiences.%20Efforts%20will%20also%20focus%20on%20refining%20the%20dynamic%20weighting%0Amechanism%20through%20reinforcement%20learning%20to%20further%20optimize%20the%20relevance%20and%0Aprecision%20of%20retrieved%20information.%20By%20incorporating%20these%20advancements%2C%20the%0Aproposed%20framework%20is%20poised%20to%20evolve%20into%20a%20comprehensive%2C%20autonomous%20AI%0Asolution%2C%20redefining%20technical%20service%20workflows%20across%20enterprise%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12006v1&entry.124074799=Read"},
{"title": "Coconut Palm Tree Counting on Drone Images with Deep Object Detection\n  and Synthetic Training Data", "author": "Tobias Rohe and Barbara B\u00f6hm and Michael K\u00f6lle and Jonas Stein and Robert M\u00fcller and Claudia Linnhoff-Popien", "abstract": "  Drones have revolutionized various domains, including agriculture. Recent\nadvances in deep learning have propelled among other things object detection in\ncomputer vision. This study utilized YOLO, a real-time object detector, to\nidentify and count coconut palm trees in Ghanaian farm drone footage. The farm\npresented has lost track of its trees due to different planting phases. While\nmanual counting would be very tedious and error-prone, accurately determining\nthe number of trees is crucial for efficient planning and management of\nagricultural processes, especially for optimizing yields and predicting\nproduction. We assessed YOLO for palm detection within a semi-automated\nframework, evaluated accuracy augmentations, and pondered its potential for\nfarmers. Data was captured in September 2022 via drones. To optimize YOLO with\nscarce data, synthetic images were created for model training and validation.\nThe YOLOv7 model, pretrained on the COCO dataset (excluding coconut palms), was\nadapted using tailored data. Trees from footage were repositioned on synthetic\nimages, with testing on distinct authentic images. In our experiments, we\nadjusted hyperparameters, improving YOLO's mean average precision (mAP). We\nalso tested various altitudes to determine the best drone height. From an\ninitial mAP@.5 of $0.65$, we achieved 0.88, highlighting the value of synthetic\nimages in agricultural scenarios.\n", "link": "http://arxiv.org/abs/2412.11949v1", "date": "2024-12-16", "relevancy": 1.7598, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4504}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coconut%20Palm%20Tree%20Counting%20on%20Drone%20Images%20with%20Deep%20Object%20Detection%0A%20%20and%20Synthetic%20Training%20Data&body=Title%3A%20Coconut%20Palm%20Tree%20Counting%20on%20Drone%20Images%20with%20Deep%20Object%20Detection%0A%20%20and%20Synthetic%20Training%20Data%0AAuthor%3A%20Tobias%20Rohe%20and%20Barbara%20B%C3%B6hm%20and%20Michael%20K%C3%B6lle%20and%20Jonas%20Stein%20and%20Robert%20M%C3%BCller%20and%20Claudia%20Linnhoff-Popien%0AAbstract%3A%20%20%20Drones%20have%20revolutionized%20various%20domains%2C%20including%20agriculture.%20Recent%0Aadvances%20in%20deep%20learning%20have%20propelled%20among%20other%20things%20object%20detection%20in%0Acomputer%20vision.%20This%20study%20utilized%20YOLO%2C%20a%20real-time%20object%20detector%2C%20to%0Aidentify%20and%20count%20coconut%20palm%20trees%20in%20Ghanaian%20farm%20drone%20footage.%20The%20farm%0Apresented%20has%20lost%20track%20of%20its%20trees%20due%20to%20different%20planting%20phases.%20While%0Amanual%20counting%20would%20be%20very%20tedious%20and%20error-prone%2C%20accurately%20determining%0Athe%20number%20of%20trees%20is%20crucial%20for%20efficient%20planning%20and%20management%20of%0Aagricultural%20processes%2C%20especially%20for%20optimizing%20yields%20and%20predicting%0Aproduction.%20We%20assessed%20YOLO%20for%20palm%20detection%20within%20a%20semi-automated%0Aframework%2C%20evaluated%20accuracy%20augmentations%2C%20and%20pondered%20its%20potential%20for%0Afarmers.%20Data%20was%20captured%20in%20September%202022%20via%20drones.%20To%20optimize%20YOLO%20with%0Ascarce%20data%2C%20synthetic%20images%20were%20created%20for%20model%20training%20and%20validation.%0AThe%20YOLOv7%20model%2C%20pretrained%20on%20the%20COCO%20dataset%20%28excluding%20coconut%20palms%29%2C%20was%0Aadapted%20using%20tailored%20data.%20Trees%20from%20footage%20were%20repositioned%20on%20synthetic%0Aimages%2C%20with%20testing%20on%20distinct%20authentic%20images.%20In%20our%20experiments%2C%20we%0Aadjusted%20hyperparameters%2C%20improving%20YOLO%27s%20mean%20average%20precision%20%28mAP%29.%20We%0Aalso%20tested%20various%20altitudes%20to%20determine%20the%20best%20drone%20height.%20From%20an%0Ainitial%20mAP%40.5%20of%20%240.65%24%2C%20we%20achieved%200.88%2C%20highlighting%20the%20value%20of%20synthetic%0Aimages%20in%20agricultural%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoconut%2520Palm%2520Tree%2520Counting%2520on%2520Drone%2520Images%2520with%2520Deep%2520Object%2520Detection%250A%2520%2520and%2520Synthetic%2520Training%2520Data%26entry.906535625%3DTobias%2520Rohe%2520and%2520Barbara%2520B%25C3%25B6hm%2520and%2520Michael%2520K%25C3%25B6lle%2520and%2520Jonas%2520Stein%2520and%2520Robert%2520M%25C3%25BCller%2520and%2520Claudia%2520Linnhoff-Popien%26entry.1292438233%3D%2520%2520Drones%2520have%2520revolutionized%2520various%2520domains%252C%2520including%2520agriculture.%2520Recent%250Aadvances%2520in%2520deep%2520learning%2520have%2520propelled%2520among%2520other%2520things%2520object%2520detection%2520in%250Acomputer%2520vision.%2520This%2520study%2520utilized%2520YOLO%252C%2520a%2520real-time%2520object%2520detector%252C%2520to%250Aidentify%2520and%2520count%2520coconut%2520palm%2520trees%2520in%2520Ghanaian%2520farm%2520drone%2520footage.%2520The%2520farm%250Apresented%2520has%2520lost%2520track%2520of%2520its%2520trees%2520due%2520to%2520different%2520planting%2520phases.%2520While%250Amanual%2520counting%2520would%2520be%2520very%2520tedious%2520and%2520error-prone%252C%2520accurately%2520determining%250Athe%2520number%2520of%2520trees%2520is%2520crucial%2520for%2520efficient%2520planning%2520and%2520management%2520of%250Aagricultural%2520processes%252C%2520especially%2520for%2520optimizing%2520yields%2520and%2520predicting%250Aproduction.%2520We%2520assessed%2520YOLO%2520for%2520palm%2520detection%2520within%2520a%2520semi-automated%250Aframework%252C%2520evaluated%2520accuracy%2520augmentations%252C%2520and%2520pondered%2520its%2520potential%2520for%250Afarmers.%2520Data%2520was%2520captured%2520in%2520September%25202022%2520via%2520drones.%2520To%2520optimize%2520YOLO%2520with%250Ascarce%2520data%252C%2520synthetic%2520images%2520were%2520created%2520for%2520model%2520training%2520and%2520validation.%250AThe%2520YOLOv7%2520model%252C%2520pretrained%2520on%2520the%2520COCO%2520dataset%2520%2528excluding%2520coconut%2520palms%2529%252C%2520was%250Aadapted%2520using%2520tailored%2520data.%2520Trees%2520from%2520footage%2520were%2520repositioned%2520on%2520synthetic%250Aimages%252C%2520with%2520testing%2520on%2520distinct%2520authentic%2520images.%2520In%2520our%2520experiments%252C%2520we%250Aadjusted%2520hyperparameters%252C%2520improving%2520YOLO%2527s%2520mean%2520average%2520precision%2520%2528mAP%2529.%2520We%250Aalso%2520tested%2520various%2520altitudes%2520to%2520determine%2520the%2520best%2520drone%2520height.%2520From%2520an%250Ainitial%2520mAP%2540.5%2520of%2520%25240.65%2524%252C%2520we%2520achieved%25200.88%252C%2520highlighting%2520the%2520value%2520of%2520synthetic%250Aimages%2520in%2520agricultural%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coconut%20Palm%20Tree%20Counting%20on%20Drone%20Images%20with%20Deep%20Object%20Detection%0A%20%20and%20Synthetic%20Training%20Data&entry.906535625=Tobias%20Rohe%20and%20Barbara%20B%C3%B6hm%20and%20Michael%20K%C3%B6lle%20and%20Jonas%20Stein%20and%20Robert%20M%C3%BCller%20and%20Claudia%20Linnhoff-Popien&entry.1292438233=%20%20Drones%20have%20revolutionized%20various%20domains%2C%20including%20agriculture.%20Recent%0Aadvances%20in%20deep%20learning%20have%20propelled%20among%20other%20things%20object%20detection%20in%0Acomputer%20vision.%20This%20study%20utilized%20YOLO%2C%20a%20real-time%20object%20detector%2C%20to%0Aidentify%20and%20count%20coconut%20palm%20trees%20in%20Ghanaian%20farm%20drone%20footage.%20The%20farm%0Apresented%20has%20lost%20track%20of%20its%20trees%20due%20to%20different%20planting%20phases.%20While%0Amanual%20counting%20would%20be%20very%20tedious%20and%20error-prone%2C%20accurately%20determining%0Athe%20number%20of%20trees%20is%20crucial%20for%20efficient%20planning%20and%20management%20of%0Aagricultural%20processes%2C%20especially%20for%20optimizing%20yields%20and%20predicting%0Aproduction.%20We%20assessed%20YOLO%20for%20palm%20detection%20within%20a%20semi-automated%0Aframework%2C%20evaluated%20accuracy%20augmentations%2C%20and%20pondered%20its%20potential%20for%0Afarmers.%20Data%20was%20captured%20in%20September%202022%20via%20drones.%20To%20optimize%20YOLO%20with%0Ascarce%20data%2C%20synthetic%20images%20were%20created%20for%20model%20training%20and%20validation.%0AThe%20YOLOv7%20model%2C%20pretrained%20on%20the%20COCO%20dataset%20%28excluding%20coconut%20palms%29%2C%20was%0Aadapted%20using%20tailored%20data.%20Trees%20from%20footage%20were%20repositioned%20on%20synthetic%0Aimages%2C%20with%20testing%20on%20distinct%20authentic%20images.%20In%20our%20experiments%2C%20we%0Aadjusted%20hyperparameters%2C%20improving%20YOLO%27s%20mean%20average%20precision%20%28mAP%29.%20We%0Aalso%20tested%20various%20altitudes%20to%20determine%20the%20best%20drone%20height.%20From%20an%0Ainitial%20mAP%40.5%20of%20%240.65%24%2C%20we%20achieved%200.88%2C%20highlighting%20the%20value%20of%20synthetic%0Aimages%20in%20agricultural%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11949v1&entry.124074799=Read"},
{"title": "Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning", "author": "Qi Sun and Pengfei Hong and Tej Deep Pala and Vernon Toh and U-Xuan Tan and Deepanway Ghosal and Soujanya Poria", "abstract": "  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n", "link": "http://arxiv.org/abs/2412.11974v1", "date": "2024-12-16", "relevancy": 1.7554, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emma-X%3A%20An%20Embodied%20Multimodal%20Action%20Model%20with%20Grounded%20Chain%20of%0A%20%20Thought%20and%20Look-ahead%20Spatial%20Reasoning&body=Title%3A%20Emma-X%3A%20An%20Embodied%20Multimodal%20Action%20Model%20with%20Grounded%20Chain%20of%0A%20%20Thought%20and%20Look-ahead%20Spatial%20Reasoning%0AAuthor%3A%20Qi%20Sun%20and%20Pengfei%20Hong%20and%20Tej%20Deep%20Pala%20and%20Vernon%20Toh%20and%20U-Xuan%20Tan%20and%20Deepanway%20Ghosal%20and%20Soujanya%20Poria%0AAbstract%3A%20%20%20Traditional%20reinforcement%20learning-based%20robotic%20control%20methods%20are%20often%0Atask-specific%20and%20fail%20to%20generalize%20across%20diverse%20environments%20or%20unseen%0Aobjects%20and%20instructions.%20Visual%20Language%20Models%20%28VLMs%29%20demonstrate%20strong%0Ascene%20understanding%20and%20planning%20capabilities%20but%20lack%20the%20ability%20to%20generate%0Aactionable%20policies%20tailored%20to%20specific%20robotic%20embodiments.%20To%20address%20this%2C%0AVisual-Language-Action%20%28VLA%29%20models%20have%20emerged%2C%20yet%20they%20face%20challenges%20in%0Along-horizon%20spatial%20reasoning%20and%20grounded%20task%20planning.%20In%20this%20work%2C%20we%0Apropose%20the%20Embodied%20Multimodal%20Action%20Model%20with%20Grounded%20Chain%20of%20Thought%20and%0ALook-ahead%20Spatial%20Reasoning%2C%20Emma-X.%20Emma-X%20leverages%20our%20constructed%0Ahierarchical%20embodiment%20dataset%20based%20on%20BridgeV2%2C%20containing%2060%2C000%20robot%0Amanipulation%20trajectories%20auto-annotated%20with%20grounded%20task%20reasoning%20and%0Aspatial%20guidance.%20Additionally%2C%20we%20introduce%20a%20trajectory%20segmentation%20strategy%0Abased%20on%20gripper%20states%20and%20motion%20trajectories%2C%20which%20can%20help%20mitigate%0Ahallucination%20in%20grounding%20subtask%20reasoning%20generation.%20Experimental%20results%0Ademonstrate%20that%20Emma-X%20achieves%20superior%20performance%20over%20competitive%0Abaselines%2C%20particularly%20in%20real-world%20robotic%20tasks%20requiring%20spatial%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmma-X%253A%2520An%2520Embodied%2520Multimodal%2520Action%2520Model%2520with%2520Grounded%2520Chain%2520of%250A%2520%2520Thought%2520and%2520Look-ahead%2520Spatial%2520Reasoning%26entry.906535625%3DQi%2520Sun%2520and%2520Pengfei%2520Hong%2520and%2520Tej%2520Deep%2520Pala%2520and%2520Vernon%2520Toh%2520and%2520U-Xuan%2520Tan%2520and%2520Deepanway%2520Ghosal%2520and%2520Soujanya%2520Poria%26entry.1292438233%3D%2520%2520Traditional%2520reinforcement%2520learning-based%2520robotic%2520control%2520methods%2520are%2520often%250Atask-specific%2520and%2520fail%2520to%2520generalize%2520across%2520diverse%2520environments%2520or%2520unseen%250Aobjects%2520and%2520instructions.%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520strong%250Ascene%2520understanding%2520and%2520planning%2520capabilities%2520but%2520lack%2520the%2520ability%2520to%2520generate%250Aactionable%2520policies%2520tailored%2520to%2520specific%2520robotic%2520embodiments.%2520To%2520address%2520this%252C%250AVisual-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%252C%2520yet%2520they%2520face%2520challenges%2520in%250Along-horizon%2520spatial%2520reasoning%2520and%2520grounded%2520task%2520planning.%2520In%2520this%2520work%252C%2520we%250Apropose%2520the%2520Embodied%2520Multimodal%2520Action%2520Model%2520with%2520Grounded%2520Chain%2520of%2520Thought%2520and%250ALook-ahead%2520Spatial%2520Reasoning%252C%2520Emma-X.%2520Emma-X%2520leverages%2520our%2520constructed%250Ahierarchical%2520embodiment%2520dataset%2520based%2520on%2520BridgeV2%252C%2520containing%252060%252C000%2520robot%250Amanipulation%2520trajectories%2520auto-annotated%2520with%2520grounded%2520task%2520reasoning%2520and%250Aspatial%2520guidance.%2520Additionally%252C%2520we%2520introduce%2520a%2520trajectory%2520segmentation%2520strategy%250Abased%2520on%2520gripper%2520states%2520and%2520motion%2520trajectories%252C%2520which%2520can%2520help%2520mitigate%250Ahallucination%2520in%2520grounding%2520subtask%2520reasoning%2520generation.%2520Experimental%2520results%250Ademonstrate%2520that%2520Emma-X%2520achieves%2520superior%2520performance%2520over%2520competitive%250Abaselines%252C%2520particularly%2520in%2520real-world%2520robotic%2520tasks%2520requiring%2520spatial%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emma-X%3A%20An%20Embodied%20Multimodal%20Action%20Model%20with%20Grounded%20Chain%20of%0A%20%20Thought%20and%20Look-ahead%20Spatial%20Reasoning&entry.906535625=Qi%20Sun%20and%20Pengfei%20Hong%20and%20Tej%20Deep%20Pala%20and%20Vernon%20Toh%20and%20U-Xuan%20Tan%20and%20Deepanway%20Ghosal%20and%20Soujanya%20Poria&entry.1292438233=%20%20Traditional%20reinforcement%20learning-based%20robotic%20control%20methods%20are%20often%0Atask-specific%20and%20fail%20to%20generalize%20across%20diverse%20environments%20or%20unseen%0Aobjects%20and%20instructions.%20Visual%20Language%20Models%20%28VLMs%29%20demonstrate%20strong%0Ascene%20understanding%20and%20planning%20capabilities%20but%20lack%20the%20ability%20to%20generate%0Aactionable%20policies%20tailored%20to%20specific%20robotic%20embodiments.%20To%20address%20this%2C%0AVisual-Language-Action%20%28VLA%29%20models%20have%20emerged%2C%20yet%20they%20face%20challenges%20in%0Along-horizon%20spatial%20reasoning%20and%20grounded%20task%20planning.%20In%20this%20work%2C%20we%0Apropose%20the%20Embodied%20Multimodal%20Action%20Model%20with%20Grounded%20Chain%20of%20Thought%20and%0ALook-ahead%20Spatial%20Reasoning%2C%20Emma-X.%20Emma-X%20leverages%20our%20constructed%0Ahierarchical%20embodiment%20dataset%20based%20on%20BridgeV2%2C%20containing%2060%2C000%20robot%0Amanipulation%20trajectories%20auto-annotated%20with%20grounded%20task%20reasoning%20and%0Aspatial%20guidance.%20Additionally%2C%20we%20introduce%20a%20trajectory%20segmentation%20strategy%0Abased%20on%20gripper%20states%20and%20motion%20trajectories%2C%20which%20can%20help%20mitigate%0Ahallucination%20in%20grounding%20subtask%20reasoning%20generation.%20Experimental%20results%0Ademonstrate%20that%20Emma-X%20achieves%20superior%20performance%20over%20competitive%0Abaselines%2C%20particularly%20in%20real-world%20robotic%20tasks%20requiring%20spatial%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11974v1&entry.124074799=Read"},
{"title": "SEAGraph: Unveiling the Whole Story of Paper Review Comments", "author": "Jianxiang Yu and Jiaqi Tan and Zichen Ding and Jiapeng Zhu and Jiahao Li and Yao Cheng and Qier Cui and Yunshi Lan and Xiang Li", "abstract": "  Peer review, as a cornerstone of scientific research, ensures the integrity\nand quality of scholarly work by providing authors with objective feedback for\nrefinement. However, in the traditional peer review process, authors often\nreceive vague or insufficiently detailed feedback, which provides limited\nassistance and leads to a more time-consuming review cycle. If authors can\nidentify some specific weaknesses in their paper, they can not only address the\nreviewer's concerns but also improve their work. This raises the critical\nquestion of how to enhance authors' comprehension of review comments. In this\npaper, we present SEAGraph, a novel framework developed to clarify review\ncomments by uncovering the underlying intentions behind them. We construct two\ntypes of graphs for each paper: the semantic mind graph, which captures the\nauthor's thought process, and the hierarchical background graph, which\ndelineates the research domains related to the paper. A retrieval method is\nthen designed to extract relevant content from both graphs, facilitating\ncoherent explanations for the review comments. Extensive experiments show that\nSEAGraph excels in review comment understanding tasks, offering significant\nbenefits to authors.\n", "link": "http://arxiv.org/abs/2412.11939v1", "date": "2024-12-16", "relevancy": 1.7149, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAGraph%3A%20Unveiling%20the%20Whole%20Story%20of%20Paper%20Review%20Comments&body=Title%3A%20SEAGraph%3A%20Unveiling%20the%20Whole%20Story%20of%20Paper%20Review%20Comments%0AAuthor%3A%20Jianxiang%20Yu%20and%20Jiaqi%20Tan%20and%20Zichen%20Ding%20and%20Jiapeng%20Zhu%20and%20Jiahao%20Li%20and%20Yao%20Cheng%20and%20Qier%20Cui%20and%20Yunshi%20Lan%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Peer%20review%2C%20as%20a%20cornerstone%20of%20scientific%20research%2C%20ensures%20the%20integrity%0Aand%20quality%20of%20scholarly%20work%20by%20providing%20authors%20with%20objective%20feedback%20for%0Arefinement.%20However%2C%20in%20the%20traditional%20peer%20review%20process%2C%20authors%20often%0Areceive%20vague%20or%20insufficiently%20detailed%20feedback%2C%20which%20provides%20limited%0Aassistance%20and%20leads%20to%20a%20more%20time-consuming%20review%20cycle.%20If%20authors%20can%0Aidentify%20some%20specific%20weaknesses%20in%20their%20paper%2C%20they%20can%20not%20only%20address%20the%0Areviewer%27s%20concerns%20but%20also%20improve%20their%20work.%20This%20raises%20the%20critical%0Aquestion%20of%20how%20to%20enhance%20authors%27%20comprehension%20of%20review%20comments.%20In%20this%0Apaper%2C%20we%20present%20SEAGraph%2C%20a%20novel%20framework%20developed%20to%20clarify%20review%0Acomments%20by%20uncovering%20the%20underlying%20intentions%20behind%20them.%20We%20construct%20two%0Atypes%20of%20graphs%20for%20each%20paper%3A%20the%20semantic%20mind%20graph%2C%20which%20captures%20the%0Aauthor%27s%20thought%20process%2C%20and%20the%20hierarchical%20background%20graph%2C%20which%0Adelineates%20the%20research%20domains%20related%20to%20the%20paper.%20A%20retrieval%20method%20is%0Athen%20designed%20to%20extract%20relevant%20content%20from%20both%20graphs%2C%20facilitating%0Acoherent%20explanations%20for%20the%20review%20comments.%20Extensive%20experiments%20show%20that%0ASEAGraph%20excels%20in%20review%20comment%20understanding%20tasks%2C%20offering%20significant%0Abenefits%20to%20authors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAGraph%253A%2520Unveiling%2520the%2520Whole%2520Story%2520of%2520Paper%2520Review%2520Comments%26entry.906535625%3DJianxiang%2520Yu%2520and%2520Jiaqi%2520Tan%2520and%2520Zichen%2520Ding%2520and%2520Jiapeng%2520Zhu%2520and%2520Jiahao%2520Li%2520and%2520Yao%2520Cheng%2520and%2520Qier%2520Cui%2520and%2520Yunshi%2520Lan%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Peer%2520review%252C%2520as%2520a%2520cornerstone%2520of%2520scientific%2520research%252C%2520ensures%2520the%2520integrity%250Aand%2520quality%2520of%2520scholarly%2520work%2520by%2520providing%2520authors%2520with%2520objective%2520feedback%2520for%250Arefinement.%2520However%252C%2520in%2520the%2520traditional%2520peer%2520review%2520process%252C%2520authors%2520often%250Areceive%2520vague%2520or%2520insufficiently%2520detailed%2520feedback%252C%2520which%2520provides%2520limited%250Aassistance%2520and%2520leads%2520to%2520a%2520more%2520time-consuming%2520review%2520cycle.%2520If%2520authors%2520can%250Aidentify%2520some%2520specific%2520weaknesses%2520in%2520their%2520paper%252C%2520they%2520can%2520not%2520only%2520address%2520the%250Areviewer%2527s%2520concerns%2520but%2520also%2520improve%2520their%2520work.%2520This%2520raises%2520the%2520critical%250Aquestion%2520of%2520how%2520to%2520enhance%2520authors%2527%2520comprehension%2520of%2520review%2520comments.%2520In%2520this%250Apaper%252C%2520we%2520present%2520SEAGraph%252C%2520a%2520novel%2520framework%2520developed%2520to%2520clarify%2520review%250Acomments%2520by%2520uncovering%2520the%2520underlying%2520intentions%2520behind%2520them.%2520We%2520construct%2520two%250Atypes%2520of%2520graphs%2520for%2520each%2520paper%253A%2520the%2520semantic%2520mind%2520graph%252C%2520which%2520captures%2520the%250Aauthor%2527s%2520thought%2520process%252C%2520and%2520the%2520hierarchical%2520background%2520graph%252C%2520which%250Adelineates%2520the%2520research%2520domains%2520related%2520to%2520the%2520paper.%2520A%2520retrieval%2520method%2520is%250Athen%2520designed%2520to%2520extract%2520relevant%2520content%2520from%2520both%2520graphs%252C%2520facilitating%250Acoherent%2520explanations%2520for%2520the%2520review%2520comments.%2520Extensive%2520experiments%2520show%2520that%250ASEAGraph%2520excels%2520in%2520review%2520comment%2520understanding%2520tasks%252C%2520offering%2520significant%250Abenefits%2520to%2520authors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAGraph%3A%20Unveiling%20the%20Whole%20Story%20of%20Paper%20Review%20Comments&entry.906535625=Jianxiang%20Yu%20and%20Jiaqi%20Tan%20and%20Zichen%20Ding%20and%20Jiapeng%20Zhu%20and%20Jiahao%20Li%20and%20Yao%20Cheng%20and%20Qier%20Cui%20and%20Yunshi%20Lan%20and%20Xiang%20Li&entry.1292438233=%20%20Peer%20review%2C%20as%20a%20cornerstone%20of%20scientific%20research%2C%20ensures%20the%20integrity%0Aand%20quality%20of%20scholarly%20work%20by%20providing%20authors%20with%20objective%20feedback%20for%0Arefinement.%20However%2C%20in%20the%20traditional%20peer%20review%20process%2C%20authors%20often%0Areceive%20vague%20or%20insufficiently%20detailed%20feedback%2C%20which%20provides%20limited%0Aassistance%20and%20leads%20to%20a%20more%20time-consuming%20review%20cycle.%20If%20authors%20can%0Aidentify%20some%20specific%20weaknesses%20in%20their%20paper%2C%20they%20can%20not%20only%20address%20the%0Areviewer%27s%20concerns%20but%20also%20improve%20their%20work.%20This%20raises%20the%20critical%0Aquestion%20of%20how%20to%20enhance%20authors%27%20comprehension%20of%20review%20comments.%20In%20this%0Apaper%2C%20we%20present%20SEAGraph%2C%20a%20novel%20framework%20developed%20to%20clarify%20review%0Acomments%20by%20uncovering%20the%20underlying%20intentions%20behind%20them.%20We%20construct%20two%0Atypes%20of%20graphs%20for%20each%20paper%3A%20the%20semantic%20mind%20graph%2C%20which%20captures%20the%0Aauthor%27s%20thought%20process%2C%20and%20the%20hierarchical%20background%20graph%2C%20which%0Adelineates%20the%20research%20domains%20related%20to%20the%20paper.%20A%20retrieval%20method%20is%0Athen%20designed%20to%20extract%20relevant%20content%20from%20both%20graphs%2C%20facilitating%0Acoherent%20explanations%20for%20the%20review%20comments.%20Extensive%20experiments%20show%20that%0ASEAGraph%20excels%20in%20review%20comment%20understanding%20tasks%2C%20offering%20significant%0Abenefits%20to%20authors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11939v1&entry.124074799=Read"},
{"title": "BrushEdit: All-In-One Image Inpainting and Editing", "author": "Yaowei Li and Yuxuan Bian and Xuan Ju and Zhaoyang Zhang and Ying Shan and Yuexian Zou and Qiang Xu", "abstract": "  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n", "link": "http://arxiv.org/abs/2412.10316v2", "date": "2024-12-16", "relevancy": 1.7117, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5934}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5866}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrushEdit%3A%20All-In-One%20Image%20Inpainting%20and%20Editing&body=Title%3A%20BrushEdit%3A%20All-In-One%20Image%20Inpainting%20and%20Editing%0AAuthor%3A%20Yaowei%20Li%20and%20Yuxuan%20Bian%20and%20Xuan%20Ju%20and%20Zhaoyang%20Zhang%20and%20Ying%20Shan%20and%20Yuexian%20Zou%20and%20Qiang%20Xu%0AAbstract%3A%20%20%20Image%20editing%20has%20advanced%20significantly%20with%20the%20development%20of%20diffusion%0Amodels%20using%20both%20inversion-based%20and%20instruction-based%20methods.%20However%2C%0Acurrent%20inversion-based%20approaches%20struggle%20with%20big%20modifications%20%28e.g.%2C%0Aadding%20or%20removing%20objects%29%20due%20to%20the%20structured%20nature%20of%20inversion%20noise%2C%0Awhich%20hinders%20substantial%20changes.%20Meanwhile%2C%20instruction-based%20methods%20often%0Aconstrain%20users%20to%20black-box%20operations%2C%20limiting%20direct%20interaction%20for%0Aspecifying%20editing%20regions%20and%20intensity.%20To%20address%20these%20limitations%2C%20we%0Apropose%20BrushEdit%2C%20a%20novel%20inpainting-based%20instruction-guided%20image%20editing%0Aparadigm%2C%20which%20leverages%20multimodal%20large%20language%20models%20%28MLLMs%29%20and%20image%0Ainpainting%20models%20to%20enable%20autonomous%2C%20user-friendly%2C%20and%20interactive%0Afree-form%20instruction%20editing.%20Specifically%2C%20we%20devise%20a%20system%20enabling%0Afree-form%20instruction%20editing%20by%20integrating%20MLLMs%20and%20a%20dual-branch%20image%0Ainpainting%20model%20in%20an%20agent-cooperative%20framework%20to%20perform%20editing%20category%0Aclassification%2C%20main%20object%20identification%2C%20mask%20acquisition%2C%20and%20editing%20area%0Ainpainting.%20Extensive%20experiments%20show%20that%20our%20framework%20effectively%20combines%0AMLLMs%20and%20inpainting%20models%2C%20achieving%20superior%20performance%20across%20seven%0Ametrics%20including%20mask%20region%20preservation%20and%20editing%20effect%20coherence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrushEdit%253A%2520All-In-One%2520Image%2520Inpainting%2520and%2520Editing%26entry.906535625%3DYaowei%2520Li%2520and%2520Yuxuan%2520Bian%2520and%2520Xuan%2520Ju%2520and%2520Zhaoyang%2520Zhang%2520and%2520Ying%2520Shan%2520and%2520Yuexian%2520Zou%2520and%2520Qiang%2520Xu%26entry.1292438233%3D%2520%2520Image%2520editing%2520has%2520advanced%2520significantly%2520with%2520the%2520development%2520of%2520diffusion%250Amodels%2520using%2520both%2520inversion-based%2520and%2520instruction-based%2520methods.%2520However%252C%250Acurrent%2520inversion-based%2520approaches%2520struggle%2520with%2520big%2520modifications%2520%2528e.g.%252C%250Aadding%2520or%2520removing%2520objects%2529%2520due%2520to%2520the%2520structured%2520nature%2520of%2520inversion%2520noise%252C%250Awhich%2520hinders%2520substantial%2520changes.%2520Meanwhile%252C%2520instruction-based%2520methods%2520often%250Aconstrain%2520users%2520to%2520black-box%2520operations%252C%2520limiting%2520direct%2520interaction%2520for%250Aspecifying%2520editing%2520regions%2520and%2520intensity.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520BrushEdit%252C%2520a%2520novel%2520inpainting-based%2520instruction-guided%2520image%2520editing%250Aparadigm%252C%2520which%2520leverages%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520and%2520image%250Ainpainting%2520models%2520to%2520enable%2520autonomous%252C%2520user-friendly%252C%2520and%2520interactive%250Afree-form%2520instruction%2520editing.%2520Specifically%252C%2520we%2520devise%2520a%2520system%2520enabling%250Afree-form%2520instruction%2520editing%2520by%2520integrating%2520MLLMs%2520and%2520a%2520dual-branch%2520image%250Ainpainting%2520model%2520in%2520an%2520agent-cooperative%2520framework%2520to%2520perform%2520editing%2520category%250Aclassification%252C%2520main%2520object%2520identification%252C%2520mask%2520acquisition%252C%2520and%2520editing%2520area%250Ainpainting.%2520Extensive%2520experiments%2520show%2520that%2520our%2520framework%2520effectively%2520combines%250AMLLMs%2520and%2520inpainting%2520models%252C%2520achieving%2520superior%2520performance%2520across%2520seven%250Ametrics%2520including%2520mask%2520region%2520preservation%2520and%2520editing%2520effect%2520coherence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrushEdit%3A%20All-In-One%20Image%20Inpainting%20and%20Editing&entry.906535625=Yaowei%20Li%20and%20Yuxuan%20Bian%20and%20Xuan%20Ju%20and%20Zhaoyang%20Zhang%20and%20Ying%20Shan%20and%20Yuexian%20Zou%20and%20Qiang%20Xu&entry.1292438233=%20%20Image%20editing%20has%20advanced%20significantly%20with%20the%20development%20of%20diffusion%0Amodels%20using%20both%20inversion-based%20and%20instruction-based%20methods.%20However%2C%0Acurrent%20inversion-based%20approaches%20struggle%20with%20big%20modifications%20%28e.g.%2C%0Aadding%20or%20removing%20objects%29%20due%20to%20the%20structured%20nature%20of%20inversion%20noise%2C%0Awhich%20hinders%20substantial%20changes.%20Meanwhile%2C%20instruction-based%20methods%20often%0Aconstrain%20users%20to%20black-box%20operations%2C%20limiting%20direct%20interaction%20for%0Aspecifying%20editing%20regions%20and%20intensity.%20To%20address%20these%20limitations%2C%20we%0Apropose%20BrushEdit%2C%20a%20novel%20inpainting-based%20instruction-guided%20image%20editing%0Aparadigm%2C%20which%20leverages%20multimodal%20large%20language%20models%20%28MLLMs%29%20and%20image%0Ainpainting%20models%20to%20enable%20autonomous%2C%20user-friendly%2C%20and%20interactive%0Afree-form%20instruction%20editing.%20Specifically%2C%20we%20devise%20a%20system%20enabling%0Afree-form%20instruction%20editing%20by%20integrating%20MLLMs%20and%20a%20dual-branch%20image%0Ainpainting%20model%20in%20an%20agent-cooperative%20framework%20to%20perform%20editing%20category%0Aclassification%2C%20main%20object%20identification%2C%20mask%20acquisition%2C%20and%20editing%20area%0Ainpainting.%20Extensive%20experiments%20show%20that%20our%20framework%20effectively%20combines%0AMLLMs%20and%20inpainting%20models%2C%20achieving%20superior%20performance%20across%20seven%0Ametrics%20including%20mask%20region%20preservation%20and%20editing%20effect%20coherence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10316v2&entry.124074799=Read"},
{"title": "Instruction-based Image Manipulation by Watching How Things Move", "author": "Mingdeng Cao and Xuaner Zhang and Yinqiang Zheng and Zhihao Xia", "abstract": "  This paper introduces a novel dataset construction pipeline that samples\npairs of frames from videos and uses multimodal large language models (MLLMs)\nto generate editing instructions for training instruction-based image\nmanipulation models. Video frames inherently preserve the identity of subjects\nand scenes, ensuring consistent content preservation during editing.\nAdditionally, video data captures diverse, natural dynamics-such as non-rigid\nsubject motion and complex camera movements-that are difficult to model\notherwise, making it an ideal source for scalable dataset construction. Using\nthis approach, we create a new dataset to train InstructMove, a model capable\nof instruction-based complex manipulations that are difficult to achieve with\nsynthetically generated datasets. Our model demonstrates state-of-the-art\nperformance in tasks such as adjusting subject poses, rearranging elements, and\naltering camera perspectives.\n", "link": "http://arxiv.org/abs/2412.12087v1", "date": "2024-12-16", "relevancy": 1.6919, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5872}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5597}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction-based%20Image%20Manipulation%20by%20Watching%20How%20Things%20Move&body=Title%3A%20Instruction-based%20Image%20Manipulation%20by%20Watching%20How%20Things%20Move%0AAuthor%3A%20Mingdeng%20Cao%20and%20Xuaner%20Zhang%20and%20Yinqiang%20Zheng%20and%20Zhihao%20Xia%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20dataset%20construction%20pipeline%20that%20samples%0Apairs%20of%20frames%20from%20videos%20and%20uses%20multimodal%20large%20language%20models%20%28MLLMs%29%0Ato%20generate%20editing%20instructions%20for%20training%20instruction-based%20image%0Amanipulation%20models.%20Video%20frames%20inherently%20preserve%20the%20identity%20of%20subjects%0Aand%20scenes%2C%20ensuring%20consistent%20content%20preservation%20during%20editing.%0AAdditionally%2C%20video%20data%20captures%20diverse%2C%20natural%20dynamics-such%20as%20non-rigid%0Asubject%20motion%20and%20complex%20camera%20movements-that%20are%20difficult%20to%20model%0Aotherwise%2C%20making%20it%20an%20ideal%20source%20for%20scalable%20dataset%20construction.%20Using%0Athis%20approach%2C%20we%20create%20a%20new%20dataset%20to%20train%20InstructMove%2C%20a%20model%20capable%0Aof%20instruction-based%20complex%20manipulations%20that%20are%20difficult%20to%20achieve%20with%0Asynthetically%20generated%20datasets.%20Our%20model%20demonstrates%20state-of-the-art%0Aperformance%20in%20tasks%20such%20as%20adjusting%20subject%20poses%2C%20rearranging%20elements%2C%20and%0Aaltering%20camera%20perspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction-based%2520Image%2520Manipulation%2520by%2520Watching%2520How%2520Things%2520Move%26entry.906535625%3DMingdeng%2520Cao%2520and%2520Xuaner%2520Zhang%2520and%2520Yinqiang%2520Zheng%2520and%2520Zhihao%2520Xia%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520dataset%2520construction%2520pipeline%2520that%2520samples%250Apairs%2520of%2520frames%2520from%2520videos%2520and%2520uses%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Ato%2520generate%2520editing%2520instructions%2520for%2520training%2520instruction-based%2520image%250Amanipulation%2520models.%2520Video%2520frames%2520inherently%2520preserve%2520the%2520identity%2520of%2520subjects%250Aand%2520scenes%252C%2520ensuring%2520consistent%2520content%2520preservation%2520during%2520editing.%250AAdditionally%252C%2520video%2520data%2520captures%2520diverse%252C%2520natural%2520dynamics-such%2520as%2520non-rigid%250Asubject%2520motion%2520and%2520complex%2520camera%2520movements-that%2520are%2520difficult%2520to%2520model%250Aotherwise%252C%2520making%2520it%2520an%2520ideal%2520source%2520for%2520scalable%2520dataset%2520construction.%2520Using%250Athis%2520approach%252C%2520we%2520create%2520a%2520new%2520dataset%2520to%2520train%2520InstructMove%252C%2520a%2520model%2520capable%250Aof%2520instruction-based%2520complex%2520manipulations%2520that%2520are%2520difficult%2520to%2520achieve%2520with%250Asynthetically%2520generated%2520datasets.%2520Our%2520model%2520demonstrates%2520state-of-the-art%250Aperformance%2520in%2520tasks%2520such%2520as%2520adjusting%2520subject%2520poses%252C%2520rearranging%2520elements%252C%2520and%250Aaltering%2520camera%2520perspectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction-based%20Image%20Manipulation%20by%20Watching%20How%20Things%20Move&entry.906535625=Mingdeng%20Cao%20and%20Xuaner%20Zhang%20and%20Yinqiang%20Zheng%20and%20Zhihao%20Xia&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20dataset%20construction%20pipeline%20that%20samples%0Apairs%20of%20frames%20from%20videos%20and%20uses%20multimodal%20large%20language%20models%20%28MLLMs%29%0Ato%20generate%20editing%20instructions%20for%20training%20instruction-based%20image%0Amanipulation%20models.%20Video%20frames%20inherently%20preserve%20the%20identity%20of%20subjects%0Aand%20scenes%2C%20ensuring%20consistent%20content%20preservation%20during%20editing.%0AAdditionally%2C%20video%20data%20captures%20diverse%2C%20natural%20dynamics-such%20as%20non-rigid%0Asubject%20motion%20and%20complex%20camera%20movements-that%20are%20difficult%20to%20model%0Aotherwise%2C%20making%20it%20an%20ideal%20source%20for%20scalable%20dataset%20construction.%20Using%0Athis%20approach%2C%20we%20create%20a%20new%20dataset%20to%20train%20InstructMove%2C%20a%20model%20capable%0Aof%20instruction-based%20complex%20manipulations%20that%20are%20difficult%20to%20achieve%20with%0Asynthetically%20generated%20datasets.%20Our%20model%20demonstrates%20state-of-the-art%0Aperformance%20in%20tasks%20such%20as%20adjusting%20subject%20poses%2C%20rearranging%20elements%2C%20and%0Aaltering%20camera%20perspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12087v1&entry.124074799=Read"},
{"title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs", "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan", "abstract": "  Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.\n", "link": "http://arxiv.org/abs/2412.08585v2", "date": "2024-12-16", "relevancy": 1.6737, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6015}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.522}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TurboAttention%3A%20Efficient%20Attention%20Approximation%20For%20High%20Throughputs%0A%20%20LLMs&body=Title%3A%20TurboAttention%3A%20Efficient%20Attention%20Approximation%20For%20High%20Throughputs%0A%20%20LLMs%0AAuthor%3A%20Hao%20Kang%20and%20Srikant%20Bharadwaj%20and%20James%20Hensman%20and%20Tushar%20Krishna%20and%20Victor%20Ruhle%20and%20Saravan%20Rajmohan%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20inference%20demands%20significant%20amount%20of%0Acomputation%20and%20memory%2C%20especially%20in%20the%20key%20attention%20mechanism.%20While%0Atechniques%2C%20such%20as%20quantization%20and%20acceleration%20algorithms%2C%20like%0AFlashAttention%2C%20have%20improved%20efficiency%20of%20the%20overall%20inference%2C%20they%20address%0Adifferent%20aspects%20of%20the%20problem%3A%20quantization%20focuses%20on%20weight-activation%0Aoperations%2C%20while%20FlashAttention%20improves%20execution%20but%20requires%20high-precision%0Aformats.%20Recent%20Key-value%20%28KV%29%20cache%20quantization%20reduces%20memory%20bandwidth%20but%0Astill%20needs%20floating-point%20dequantization%20for%20attention%20operation.%0A%20%20We%20present%20TurboAttention%2C%20a%20comprehensive%20approach%20to%20enable%20quantized%0Aexecution%20of%20attention%20that%20simultaneously%20addresses%20both%20memory%20and%0Acomputational%20efficiency.%20Our%20solution%20introduces%20two%20key%20innovations%3A%20FlashQ%2C%0Aa%20headwise%20attention%20quantization%20technique%20that%20enables%20both%20compression%20of%20KV%0Acache%20and%20quantized%20execution%20of%20activation-activation%20multiplication%2C%20and%0ASparsity-based%20Softmax%20Approximation%20%28SAS%29%2C%20which%20eliminates%20the%20need%20for%0Adequantization%20to%20FP32%20during%20exponentiation%20operation%20in%20attention.%0AExperimental%20results%20demonstrate%20that%20TurboAttention%20achieves%201.2-1.8x%20speedup%0Ain%20attention%2C%20reduces%20the%20KV%20cache%20size%20by%20over%204.4x%2C%20and%20enables%20up%20to%202.37x%0Amaximum%20throughput%20over%20the%20FP16%20baseline%20while%20outperforming%20state-of-the-art%0Aquantization%20and%20compression%20techniques%20across%20various%20datasets%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurboAttention%253A%2520Efficient%2520Attention%2520Approximation%2520For%2520High%2520Throughputs%250A%2520%2520LLMs%26entry.906535625%3DHao%2520Kang%2520and%2520Srikant%2520Bharadwaj%2520and%2520James%2520Hensman%2520and%2520Tushar%2520Krishna%2520and%2520Victor%2520Ruhle%2520and%2520Saravan%2520Rajmohan%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520inference%2520demands%2520significant%2520amount%2520of%250Acomputation%2520and%2520memory%252C%2520especially%2520in%2520the%2520key%2520attention%2520mechanism.%2520While%250Atechniques%252C%2520such%2520as%2520quantization%2520and%2520acceleration%2520algorithms%252C%2520like%250AFlashAttention%252C%2520have%2520improved%2520efficiency%2520of%2520the%2520overall%2520inference%252C%2520they%2520address%250Adifferent%2520aspects%2520of%2520the%2520problem%253A%2520quantization%2520focuses%2520on%2520weight-activation%250Aoperations%252C%2520while%2520FlashAttention%2520improves%2520execution%2520but%2520requires%2520high-precision%250Aformats.%2520Recent%2520Key-value%2520%2528KV%2529%2520cache%2520quantization%2520reduces%2520memory%2520bandwidth%2520but%250Astill%2520needs%2520floating-point%2520dequantization%2520for%2520attention%2520operation.%250A%2520%2520We%2520present%2520TurboAttention%252C%2520a%2520comprehensive%2520approach%2520to%2520enable%2520quantized%250Aexecution%2520of%2520attention%2520that%2520simultaneously%2520addresses%2520both%2520memory%2520and%250Acomputational%2520efficiency.%2520Our%2520solution%2520introduces%2520two%2520key%2520innovations%253A%2520FlashQ%252C%250Aa%2520headwise%2520attention%2520quantization%2520technique%2520that%2520enables%2520both%2520compression%2520of%2520KV%250Acache%2520and%2520quantized%2520execution%2520of%2520activation-activation%2520multiplication%252C%2520and%250ASparsity-based%2520Softmax%2520Approximation%2520%2528SAS%2529%252C%2520which%2520eliminates%2520the%2520need%2520for%250Adequantization%2520to%2520FP32%2520during%2520exponentiation%2520operation%2520in%2520attention.%250AExperimental%2520results%2520demonstrate%2520that%2520TurboAttention%2520achieves%25201.2-1.8x%2520speedup%250Ain%2520attention%252C%2520reduces%2520the%2520KV%2520cache%2520size%2520by%2520over%25204.4x%252C%2520and%2520enables%2520up%2520to%25202.37x%250Amaximum%2520throughput%2520over%2520the%2520FP16%2520baseline%2520while%2520outperforming%2520state-of-the-art%250Aquantization%2520and%2520compression%2520techniques%2520across%2520various%2520datasets%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TurboAttention%3A%20Efficient%20Attention%20Approximation%20For%20High%20Throughputs%0A%20%20LLMs&entry.906535625=Hao%20Kang%20and%20Srikant%20Bharadwaj%20and%20James%20Hensman%20and%20Tushar%20Krishna%20and%20Victor%20Ruhle%20and%20Saravan%20Rajmohan&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20inference%20demands%20significant%20amount%20of%0Acomputation%20and%20memory%2C%20especially%20in%20the%20key%20attention%20mechanism.%20While%0Atechniques%2C%20such%20as%20quantization%20and%20acceleration%20algorithms%2C%20like%0AFlashAttention%2C%20have%20improved%20efficiency%20of%20the%20overall%20inference%2C%20they%20address%0Adifferent%20aspects%20of%20the%20problem%3A%20quantization%20focuses%20on%20weight-activation%0Aoperations%2C%20while%20FlashAttention%20improves%20execution%20but%20requires%20high-precision%0Aformats.%20Recent%20Key-value%20%28KV%29%20cache%20quantization%20reduces%20memory%20bandwidth%20but%0Astill%20needs%20floating-point%20dequantization%20for%20attention%20operation.%0A%20%20We%20present%20TurboAttention%2C%20a%20comprehensive%20approach%20to%20enable%20quantized%0Aexecution%20of%20attention%20that%20simultaneously%20addresses%20both%20memory%20and%0Acomputational%20efficiency.%20Our%20solution%20introduces%20two%20key%20innovations%3A%20FlashQ%2C%0Aa%20headwise%20attention%20quantization%20technique%20that%20enables%20both%20compression%20of%20KV%0Acache%20and%20quantized%20execution%20of%20activation-activation%20multiplication%2C%20and%0ASparsity-based%20Softmax%20Approximation%20%28SAS%29%2C%20which%20eliminates%20the%20need%20for%0Adequantization%20to%20FP32%20during%20exponentiation%20operation%20in%20attention.%0AExperimental%20results%20demonstrate%20that%20TurboAttention%20achieves%201.2-1.8x%20speedup%0Ain%20attention%2C%20reduces%20the%20KV%20cache%20size%20by%20over%204.4x%2C%20and%20enables%20up%20to%202.37x%0Amaximum%20throughput%20over%20the%20FP16%20baseline%20while%20outperforming%20state-of-the-art%0Aquantization%20and%20compression%20techniques%20across%20various%20datasets%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08585v2&entry.124074799=Read"},
{"title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early\n  version)", "author": "Yao Mu and Tianxing Chen and Shijia Peng and Zanxin Chen and Zeyu Gao and Yude Zou and Lunkai Lin and Zhiqiang Xie and Ping Luo", "abstract": "  In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples improve the success rate of over 70% for single-arm tasks and over 40%\nfor dual-arm tasks compared to models trained solely on real-world data. This\nsignificant improvement demonstrates RoboTwin's potential to enhance the\ndevelopment and evaluation of dual-arm robotic manipulation systems. Project\nPage: https://robotwin-benchmark.github.io/early-version/.\n", "link": "http://arxiv.org/abs/2409.02920v2", "date": "2024-12-16", "relevancy": 1.6536, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5627}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboTwin%3A%20Dual-Arm%20Robot%20Benchmark%20with%20Generative%20Digital%20Twins%20%28early%0A%20%20version%29&body=Title%3A%20RoboTwin%3A%20Dual-Arm%20Robot%20Benchmark%20with%20Generative%20Digital%20Twins%20%28early%0A%20%20version%29%0AAuthor%3A%20Yao%20Mu%20and%20Tianxing%20Chen%20and%20Shijia%20Peng%20and%20Zanxin%20Chen%20and%20Zeyu%20Gao%20and%20Yude%20Zou%20and%20Lunkai%20Lin%20and%20Zhiqiang%20Xie%20and%20Ping%20Luo%0AAbstract%3A%20%20%20In%20the%20rapidly%20advancing%20field%20of%20robotics%2C%20dual-arm%20coordination%20and%20complex%0Aobject%20manipulation%20are%20essential%20capabilities%20for%20developing%20advanced%0Aautonomous%20systems.%20However%2C%20the%20scarcity%20of%20diverse%2C%20high-quality%0Ademonstration%20data%20and%20real-world-aligned%20evaluation%20benchmarks%20severely%20limits%0Asuch%20development.%20To%20address%20this%2C%20we%20introduce%20RoboTwin%2C%20a%20generative%20digital%0Atwin%20framework%20that%20uses%203D%20generative%20foundation%20models%20and%20large%20language%0Amodels%20to%20produce%20diverse%20expert%20datasets%20and%20provide%20a%20real-world-aligned%0Aevaluation%20platform%20for%20dual-arm%20robotic%20tasks.%20Specifically%2C%20RoboTwin%20creates%0Avaried%20digital%20twins%20of%20objects%20from%20single%202D%20images%2C%20generating%20realistic%20and%0Ainteractive%20scenarios.%20It%20also%20introduces%20a%20spatial%20relation-aware%20code%0Ageneration%20framework%20that%20combines%20object%20annotations%20with%20large%20language%0Amodels%20to%20break%20down%20tasks%2C%20determine%20spatial%20constraints%2C%20and%20generate%20precise%0Arobotic%20movement%20code.%20Our%20framework%20offers%20a%20comprehensive%20benchmark%20with%20both%0Asimulated%20and%20real-world%20data%2C%20enabling%20standardized%20evaluation%20and%20better%0Aalignment%20between%20simulated%20training%20and%20real-world%20performance.%20We%20validated%0Aour%20approach%20using%20the%20open-source%20COBOT%20Magic%20Robot%20platform.%20Policies%0Apre-trained%20on%20RoboTwin-generated%20data%20and%20fine-tuned%20with%20limited%20real-world%0Asamples%20improve%20the%20success%20rate%20of%20over%2070%25%20for%20single-arm%20tasks%20and%20over%2040%25%0Afor%20dual-arm%20tasks%20compared%20to%20models%20trained%20solely%20on%20real-world%20data.%20This%0Asignificant%20improvement%20demonstrates%20RoboTwin%27s%20potential%20to%20enhance%20the%0Adevelopment%20and%20evaluation%20of%20dual-arm%20robotic%20manipulation%20systems.%20Project%0APage%3A%20https%3A//robotwin-benchmark.github.io/early-version/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02920v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboTwin%253A%2520Dual-Arm%2520Robot%2520Benchmark%2520with%2520Generative%2520Digital%2520Twins%2520%2528early%250A%2520%2520version%2529%26entry.906535625%3DYao%2520Mu%2520and%2520Tianxing%2520Chen%2520and%2520Shijia%2520Peng%2520and%2520Zanxin%2520Chen%2520and%2520Zeyu%2520Gao%2520and%2520Yude%2520Zou%2520and%2520Lunkai%2520Lin%2520and%2520Zhiqiang%2520Xie%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520advancing%2520field%2520of%2520robotics%252C%2520dual-arm%2520coordination%2520and%2520complex%250Aobject%2520manipulation%2520are%2520essential%2520capabilities%2520for%2520developing%2520advanced%250Aautonomous%2520systems.%2520However%252C%2520the%2520scarcity%2520of%2520diverse%252C%2520high-quality%250Ademonstration%2520data%2520and%2520real-world-aligned%2520evaluation%2520benchmarks%2520severely%2520limits%250Asuch%2520development.%2520To%2520address%2520this%252C%2520we%2520introduce%2520RoboTwin%252C%2520a%2520generative%2520digital%250Atwin%2520framework%2520that%2520uses%25203D%2520generative%2520foundation%2520models%2520and%2520large%2520language%250Amodels%2520to%2520produce%2520diverse%2520expert%2520datasets%2520and%2520provide%2520a%2520real-world-aligned%250Aevaluation%2520platform%2520for%2520dual-arm%2520robotic%2520tasks.%2520Specifically%252C%2520RoboTwin%2520creates%250Avaried%2520digital%2520twins%2520of%2520objects%2520from%2520single%25202D%2520images%252C%2520generating%2520realistic%2520and%250Ainteractive%2520scenarios.%2520It%2520also%2520introduces%2520a%2520spatial%2520relation-aware%2520code%250Ageneration%2520framework%2520that%2520combines%2520object%2520annotations%2520with%2520large%2520language%250Amodels%2520to%2520break%2520down%2520tasks%252C%2520determine%2520spatial%2520constraints%252C%2520and%2520generate%2520precise%250Arobotic%2520movement%2520code.%2520Our%2520framework%2520offers%2520a%2520comprehensive%2520benchmark%2520with%2520both%250Asimulated%2520and%2520real-world%2520data%252C%2520enabling%2520standardized%2520evaluation%2520and%2520better%250Aalignment%2520between%2520simulated%2520training%2520and%2520real-world%2520performance.%2520We%2520validated%250Aour%2520approach%2520using%2520the%2520open-source%2520COBOT%2520Magic%2520Robot%2520platform.%2520Policies%250Apre-trained%2520on%2520RoboTwin-generated%2520data%2520and%2520fine-tuned%2520with%2520limited%2520real-world%250Asamples%2520improve%2520the%2520success%2520rate%2520of%2520over%252070%2525%2520for%2520single-arm%2520tasks%2520and%2520over%252040%2525%250Afor%2520dual-arm%2520tasks%2520compared%2520to%2520models%2520trained%2520solely%2520on%2520real-world%2520data.%2520This%250Asignificant%2520improvement%2520demonstrates%2520RoboTwin%2527s%2520potential%2520to%2520enhance%2520the%250Adevelopment%2520and%2520evaluation%2520of%2520dual-arm%2520robotic%2520manipulation%2520systems.%2520Project%250APage%253A%2520https%253A//robotwin-benchmark.github.io/early-version/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02920v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboTwin%3A%20Dual-Arm%20Robot%20Benchmark%20with%20Generative%20Digital%20Twins%20%28early%0A%20%20version%29&entry.906535625=Yao%20Mu%20and%20Tianxing%20Chen%20and%20Shijia%20Peng%20and%20Zanxin%20Chen%20and%20Zeyu%20Gao%20and%20Yude%20Zou%20and%20Lunkai%20Lin%20and%20Zhiqiang%20Xie%20and%20Ping%20Luo&entry.1292438233=%20%20In%20the%20rapidly%20advancing%20field%20of%20robotics%2C%20dual-arm%20coordination%20and%20complex%0Aobject%20manipulation%20are%20essential%20capabilities%20for%20developing%20advanced%0Aautonomous%20systems.%20However%2C%20the%20scarcity%20of%20diverse%2C%20high-quality%0Ademonstration%20data%20and%20real-world-aligned%20evaluation%20benchmarks%20severely%20limits%0Asuch%20development.%20To%20address%20this%2C%20we%20introduce%20RoboTwin%2C%20a%20generative%20digital%0Atwin%20framework%20that%20uses%203D%20generative%20foundation%20models%20and%20large%20language%0Amodels%20to%20produce%20diverse%20expert%20datasets%20and%20provide%20a%20real-world-aligned%0Aevaluation%20platform%20for%20dual-arm%20robotic%20tasks.%20Specifically%2C%20RoboTwin%20creates%0Avaried%20digital%20twins%20of%20objects%20from%20single%202D%20images%2C%20generating%20realistic%20and%0Ainteractive%20scenarios.%20It%20also%20introduces%20a%20spatial%20relation-aware%20code%0Ageneration%20framework%20that%20combines%20object%20annotations%20with%20large%20language%0Amodels%20to%20break%20down%20tasks%2C%20determine%20spatial%20constraints%2C%20and%20generate%20precise%0Arobotic%20movement%20code.%20Our%20framework%20offers%20a%20comprehensive%20benchmark%20with%20both%0Asimulated%20and%20real-world%20data%2C%20enabling%20standardized%20evaluation%20and%20better%0Aalignment%20between%20simulated%20training%20and%20real-world%20performance.%20We%20validated%0Aour%20approach%20using%20the%20open-source%20COBOT%20Magic%20Robot%20platform.%20Policies%0Apre-trained%20on%20RoboTwin-generated%20data%20and%20fine-tuned%20with%20limited%20real-world%0Asamples%20improve%20the%20success%20rate%20of%20over%2070%25%20for%20single-arm%20tasks%20and%20over%2040%25%0Afor%20dual-arm%20tasks%20compared%20to%20models%20trained%20solely%20on%20real-world%20data.%20This%0Asignificant%20improvement%20demonstrates%20RoboTwin%27s%20potential%20to%20enhance%20the%0Adevelopment%20and%20evaluation%20of%20dual-arm%20robotic%20manipulation%20systems.%20Project%0APage%3A%20https%3A//robotwin-benchmark.github.io/early-version/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02920v2&entry.124074799=Read"},
{"title": "Fairness Shields: Safeguarding against Biased Decision Makers", "author": "Filip Cano and Thomas A. Henzinger and Bettina K\u00f6nighofer and Konstantin Kueffner and Kaushik Mallik", "abstract": "  As AI-based decision-makers increasingly influence human lives, it is a\ngrowing concern that their decisions are often unfair or biased with respect to\npeople's sensitive attributes, such as gender and race. Most existing bias\nprevention measures provide probabilistic fairness guarantees in the long run,\nand it is possible that the decisions are biased on specific instances of short\ndecision sequences. We introduce fairness shielding, where a symbolic\ndecision-maker -- the fairness shield -- continuously monitors the sequence of\ndecisions of another deployed black-box decision-maker, and makes interventions\nso that a given fairness criterion is met while the total intervention costs\nare minimized. We present four different algorithms for computing fairness\nshields, among which one guarantees fairness over fixed horizons, and three\nguarantee fairness periodically after fixed intervals. Given a distribution\nover future decisions and their intervention costs, our algorithms solve\ndifferent instances of bounded-horizon optimal control problems with different\nlevels of computational costs and optimality guarantees. Our empirical\nevaluation demonstrates the effectiveness of these shields in ensuring fairness\nwhile maintaining cost efficiency across various scenarios.\n", "link": "http://arxiv.org/abs/2412.11994v1", "date": "2024-12-16", "relevancy": 1.6451, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4294}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4046}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20Shields%3A%20Safeguarding%20against%20Biased%20Decision%20Makers&body=Title%3A%20Fairness%20Shields%3A%20Safeguarding%20against%20Biased%20Decision%20Makers%0AAuthor%3A%20Filip%20Cano%20and%20Thomas%20A.%20Henzinger%20and%20Bettina%20K%C3%B6nighofer%20and%20Konstantin%20Kueffner%20and%20Kaushik%20Mallik%0AAbstract%3A%20%20%20As%20AI-based%20decision-makers%20increasingly%20influence%20human%20lives%2C%20it%20is%20a%0Agrowing%20concern%20that%20their%20decisions%20are%20often%20unfair%20or%20biased%20with%20respect%20to%0Apeople%27s%20sensitive%20attributes%2C%20such%20as%20gender%20and%20race.%20Most%20existing%20bias%0Aprevention%20measures%20provide%20probabilistic%20fairness%20guarantees%20in%20the%20long%20run%2C%0Aand%20it%20is%20possible%20that%20the%20decisions%20are%20biased%20on%20specific%20instances%20of%20short%0Adecision%20sequences.%20We%20introduce%20fairness%20shielding%2C%20where%20a%20symbolic%0Adecision-maker%20--%20the%20fairness%20shield%20--%20continuously%20monitors%20the%20sequence%20of%0Adecisions%20of%20another%20deployed%20black-box%20decision-maker%2C%20and%20makes%20interventions%0Aso%20that%20a%20given%20fairness%20criterion%20is%20met%20while%20the%20total%20intervention%20costs%0Aare%20minimized.%20We%20present%20four%20different%20algorithms%20for%20computing%20fairness%0Ashields%2C%20among%20which%20one%20guarantees%20fairness%20over%20fixed%20horizons%2C%20and%20three%0Aguarantee%20fairness%20periodically%20after%20fixed%20intervals.%20Given%20a%20distribution%0Aover%20future%20decisions%20and%20their%20intervention%20costs%2C%20our%20algorithms%20solve%0Adifferent%20instances%20of%20bounded-horizon%20optimal%20control%20problems%20with%20different%0Alevels%20of%20computational%20costs%20and%20optimality%20guarantees.%20Our%20empirical%0Aevaluation%20demonstrates%20the%20effectiveness%20of%20these%20shields%20in%20ensuring%20fairness%0Awhile%20maintaining%20cost%20efficiency%20across%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520Shields%253A%2520Safeguarding%2520against%2520Biased%2520Decision%2520Makers%26entry.906535625%3DFilip%2520Cano%2520and%2520Thomas%2520A.%2520Henzinger%2520and%2520Bettina%2520K%25C3%25B6nighofer%2520and%2520Konstantin%2520Kueffner%2520and%2520Kaushik%2520Mallik%26entry.1292438233%3D%2520%2520As%2520AI-based%2520decision-makers%2520increasingly%2520influence%2520human%2520lives%252C%2520it%2520is%2520a%250Agrowing%2520concern%2520that%2520their%2520decisions%2520are%2520often%2520unfair%2520or%2520biased%2520with%2520respect%2520to%250Apeople%2527s%2520sensitive%2520attributes%252C%2520such%2520as%2520gender%2520and%2520race.%2520Most%2520existing%2520bias%250Aprevention%2520measures%2520provide%2520probabilistic%2520fairness%2520guarantees%2520in%2520the%2520long%2520run%252C%250Aand%2520it%2520is%2520possible%2520that%2520the%2520decisions%2520are%2520biased%2520on%2520specific%2520instances%2520of%2520short%250Adecision%2520sequences.%2520We%2520introduce%2520fairness%2520shielding%252C%2520where%2520a%2520symbolic%250Adecision-maker%2520--%2520the%2520fairness%2520shield%2520--%2520continuously%2520monitors%2520the%2520sequence%2520of%250Adecisions%2520of%2520another%2520deployed%2520black-box%2520decision-maker%252C%2520and%2520makes%2520interventions%250Aso%2520that%2520a%2520given%2520fairness%2520criterion%2520is%2520met%2520while%2520the%2520total%2520intervention%2520costs%250Aare%2520minimized.%2520We%2520present%2520four%2520different%2520algorithms%2520for%2520computing%2520fairness%250Ashields%252C%2520among%2520which%2520one%2520guarantees%2520fairness%2520over%2520fixed%2520horizons%252C%2520and%2520three%250Aguarantee%2520fairness%2520periodically%2520after%2520fixed%2520intervals.%2520Given%2520a%2520distribution%250Aover%2520future%2520decisions%2520and%2520their%2520intervention%2520costs%252C%2520our%2520algorithms%2520solve%250Adifferent%2520instances%2520of%2520bounded-horizon%2520optimal%2520control%2520problems%2520with%2520different%250Alevels%2520of%2520computational%2520costs%2520and%2520optimality%2520guarantees.%2520Our%2520empirical%250Aevaluation%2520demonstrates%2520the%2520effectiveness%2520of%2520these%2520shields%2520in%2520ensuring%2520fairness%250Awhile%2520maintaining%2520cost%2520efficiency%2520across%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Shields%3A%20Safeguarding%20against%20Biased%20Decision%20Makers&entry.906535625=Filip%20Cano%20and%20Thomas%20A.%20Henzinger%20and%20Bettina%20K%C3%B6nighofer%20and%20Konstantin%20Kueffner%20and%20Kaushik%20Mallik&entry.1292438233=%20%20As%20AI-based%20decision-makers%20increasingly%20influence%20human%20lives%2C%20it%20is%20a%0Agrowing%20concern%20that%20their%20decisions%20are%20often%20unfair%20or%20biased%20with%20respect%20to%0Apeople%27s%20sensitive%20attributes%2C%20such%20as%20gender%20and%20race.%20Most%20existing%20bias%0Aprevention%20measures%20provide%20probabilistic%20fairness%20guarantees%20in%20the%20long%20run%2C%0Aand%20it%20is%20possible%20that%20the%20decisions%20are%20biased%20on%20specific%20instances%20of%20short%0Adecision%20sequences.%20We%20introduce%20fairness%20shielding%2C%20where%20a%20symbolic%0Adecision-maker%20--%20the%20fairness%20shield%20--%20continuously%20monitors%20the%20sequence%20of%0Adecisions%20of%20another%20deployed%20black-box%20decision-maker%2C%20and%20makes%20interventions%0Aso%20that%20a%20given%20fairness%20criterion%20is%20met%20while%20the%20total%20intervention%20costs%0Aare%20minimized.%20We%20present%20four%20different%20algorithms%20for%20computing%20fairness%0Ashields%2C%20among%20which%20one%20guarantees%20fairness%20over%20fixed%20horizons%2C%20and%20three%0Aguarantee%20fairness%20periodically%20after%20fixed%20intervals.%20Given%20a%20distribution%0Aover%20future%20decisions%20and%20their%20intervention%20costs%2C%20our%20algorithms%20solve%0Adifferent%20instances%20of%20bounded-horizon%20optimal%20control%20problems%20with%20different%0Alevels%20of%20computational%20costs%20and%20optimality%20guarantees.%20Our%20empirical%0Aevaluation%20demonstrates%20the%20effectiveness%20of%20these%20shields%20in%20ensuring%20fairness%0Awhile%20maintaining%20cost%20efficiency%20across%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11994v1&entry.124074799=Read"},
{"title": "Invertible ResNets for Inverse Imaging Problems: Competitive Performance\n  with Provable Regularization Properties", "author": "Clemens Arndt and Judith Nickel", "abstract": "  Learning-based methods have demonstrated remarkable performance in solving\ninverse problems, particularly in image reconstruction tasks. Despite their\nsuccess, these approaches often lack theoretical guarantees, which are crucial\nin sensitive applications such as medical imaging. Recent works by Arndt et al\n(2023 Inverse Problems 39 125018, 2024 Inverse Problems 40 045021) addressed\nthis gap by analyzing a data-driven reconstruction method based on invertible\nresidual networks (iResNets). They revealed that, under reasonable assumptions,\nthis approach constitutes a convergent regularization scheme. However, the\nperformance of the reconstruction method was only validated on academic toy\nproblems and small-scale iResNet architectures. In this work, we address this\ngap by evaluating the performance of iResNets on two real-world imaging tasks:\na linear blurring operator and a nonlinear diffusion operator. To do so, we\nextend some of the theoretical results from Arndt et al to encompass nonlinear\ninverse problems and offer insights for the design of large-scale performant\niResNet architectures. Through numerical experiments, we compare the\nperformance of our iResNet models against state-of-the-art neural networks,\nconfirming their efficacy. Additionally, we numerically investigate the\ntheoretical guarantees of this approach and demonstrate how the invertibility\nof the network enables a deeper analysis of the learned forward operator and\nits learned regularization.\n", "link": "http://arxiv.org/abs/2409.13482v2", "date": "2024-12-16", "relevancy": 1.6366, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.575}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5374}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invertible%20ResNets%20for%20Inverse%20Imaging%20Problems%3A%20Competitive%20Performance%0A%20%20with%20Provable%20Regularization%20Properties&body=Title%3A%20Invertible%20ResNets%20for%20Inverse%20Imaging%20Problems%3A%20Competitive%20Performance%0A%20%20with%20Provable%20Regularization%20Properties%0AAuthor%3A%20Clemens%20Arndt%20and%20Judith%20Nickel%0AAbstract%3A%20%20%20Learning-based%20methods%20have%20demonstrated%20remarkable%20performance%20in%20solving%0Ainverse%20problems%2C%20particularly%20in%20image%20reconstruction%20tasks.%20Despite%20their%0Asuccess%2C%20these%20approaches%20often%20lack%20theoretical%20guarantees%2C%20which%20are%20crucial%0Ain%20sensitive%20applications%20such%20as%20medical%20imaging.%20Recent%20works%20by%20Arndt%20et%20al%0A%282023%20Inverse%20Problems%2039%20125018%2C%202024%20Inverse%20Problems%2040%20045021%29%20addressed%0Athis%20gap%20by%20analyzing%20a%20data-driven%20reconstruction%20method%20based%20on%20invertible%0Aresidual%20networks%20%28iResNets%29.%20They%20revealed%20that%2C%20under%20reasonable%20assumptions%2C%0Athis%20approach%20constitutes%20a%20convergent%20regularization%20scheme.%20However%2C%20the%0Aperformance%20of%20the%20reconstruction%20method%20was%20only%20validated%20on%20academic%20toy%0Aproblems%20and%20small-scale%20iResNet%20architectures.%20In%20this%20work%2C%20we%20address%20this%0Agap%20by%20evaluating%20the%20performance%20of%20iResNets%20on%20two%20real-world%20imaging%20tasks%3A%0Aa%20linear%20blurring%20operator%20and%20a%20nonlinear%20diffusion%20operator.%20To%20do%20so%2C%20we%0Aextend%20some%20of%20the%20theoretical%20results%20from%20Arndt%20et%20al%20to%20encompass%20nonlinear%0Ainverse%20problems%20and%20offer%20insights%20for%20the%20design%20of%20large-scale%20performant%0AiResNet%20architectures.%20Through%20numerical%20experiments%2C%20we%20compare%20the%0Aperformance%20of%20our%20iResNet%20models%20against%20state-of-the-art%20neural%20networks%2C%0Aconfirming%20their%20efficacy.%20Additionally%2C%20we%20numerically%20investigate%20the%0Atheoretical%20guarantees%20of%20this%20approach%20and%20demonstrate%20how%20the%20invertibility%0Aof%20the%20network%20enables%20a%20deeper%20analysis%20of%20the%20learned%20forward%20operator%20and%0Aits%20learned%20regularization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvertible%2520ResNets%2520for%2520Inverse%2520Imaging%2520Problems%253A%2520Competitive%2520Performance%250A%2520%2520with%2520Provable%2520Regularization%2520Properties%26entry.906535625%3DClemens%2520Arndt%2520and%2520Judith%2520Nickel%26entry.1292438233%3D%2520%2520Learning-based%2520methods%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520solving%250Ainverse%2520problems%252C%2520particularly%2520in%2520image%2520reconstruction%2520tasks.%2520Despite%2520their%250Asuccess%252C%2520these%2520approaches%2520often%2520lack%2520theoretical%2520guarantees%252C%2520which%2520are%2520crucial%250Ain%2520sensitive%2520applications%2520such%2520as%2520medical%2520imaging.%2520Recent%2520works%2520by%2520Arndt%2520et%2520al%250A%25282023%2520Inverse%2520Problems%252039%2520125018%252C%25202024%2520Inverse%2520Problems%252040%2520045021%2529%2520addressed%250Athis%2520gap%2520by%2520analyzing%2520a%2520data-driven%2520reconstruction%2520method%2520based%2520on%2520invertible%250Aresidual%2520networks%2520%2528iResNets%2529.%2520They%2520revealed%2520that%252C%2520under%2520reasonable%2520assumptions%252C%250Athis%2520approach%2520constitutes%2520a%2520convergent%2520regularization%2520scheme.%2520However%252C%2520the%250Aperformance%2520of%2520the%2520reconstruction%2520method%2520was%2520only%2520validated%2520on%2520academic%2520toy%250Aproblems%2520and%2520small-scale%2520iResNet%2520architectures.%2520In%2520this%2520work%252C%2520we%2520address%2520this%250Agap%2520by%2520evaluating%2520the%2520performance%2520of%2520iResNets%2520on%2520two%2520real-world%2520imaging%2520tasks%253A%250Aa%2520linear%2520blurring%2520operator%2520and%2520a%2520nonlinear%2520diffusion%2520operator.%2520To%2520do%2520so%252C%2520we%250Aextend%2520some%2520of%2520the%2520theoretical%2520results%2520from%2520Arndt%2520et%2520al%2520to%2520encompass%2520nonlinear%250Ainverse%2520problems%2520and%2520offer%2520insights%2520for%2520the%2520design%2520of%2520large-scale%2520performant%250AiResNet%2520architectures.%2520Through%2520numerical%2520experiments%252C%2520we%2520compare%2520the%250Aperformance%2520of%2520our%2520iResNet%2520models%2520against%2520state-of-the-art%2520neural%2520networks%252C%250Aconfirming%2520their%2520efficacy.%2520Additionally%252C%2520we%2520numerically%2520investigate%2520the%250Atheoretical%2520guarantees%2520of%2520this%2520approach%2520and%2520demonstrate%2520how%2520the%2520invertibility%250Aof%2520the%2520network%2520enables%2520a%2520deeper%2520analysis%2520of%2520the%2520learned%2520forward%2520operator%2520and%250Aits%2520learned%2520regularization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invertible%20ResNets%20for%20Inverse%20Imaging%20Problems%3A%20Competitive%20Performance%0A%20%20with%20Provable%20Regularization%20Properties&entry.906535625=Clemens%20Arndt%20and%20Judith%20Nickel&entry.1292438233=%20%20Learning-based%20methods%20have%20demonstrated%20remarkable%20performance%20in%20solving%0Ainverse%20problems%2C%20particularly%20in%20image%20reconstruction%20tasks.%20Despite%20their%0Asuccess%2C%20these%20approaches%20often%20lack%20theoretical%20guarantees%2C%20which%20are%20crucial%0Ain%20sensitive%20applications%20such%20as%20medical%20imaging.%20Recent%20works%20by%20Arndt%20et%20al%0A%282023%20Inverse%20Problems%2039%20125018%2C%202024%20Inverse%20Problems%2040%20045021%29%20addressed%0Athis%20gap%20by%20analyzing%20a%20data-driven%20reconstruction%20method%20based%20on%20invertible%0Aresidual%20networks%20%28iResNets%29.%20They%20revealed%20that%2C%20under%20reasonable%20assumptions%2C%0Athis%20approach%20constitutes%20a%20convergent%20regularization%20scheme.%20However%2C%20the%0Aperformance%20of%20the%20reconstruction%20method%20was%20only%20validated%20on%20academic%20toy%0Aproblems%20and%20small-scale%20iResNet%20architectures.%20In%20this%20work%2C%20we%20address%20this%0Agap%20by%20evaluating%20the%20performance%20of%20iResNets%20on%20two%20real-world%20imaging%20tasks%3A%0Aa%20linear%20blurring%20operator%20and%20a%20nonlinear%20diffusion%20operator.%20To%20do%20so%2C%20we%0Aextend%20some%20of%20the%20theoretical%20results%20from%20Arndt%20et%20al%20to%20encompass%20nonlinear%0Ainverse%20problems%20and%20offer%20insights%20for%20the%20design%20of%20large-scale%20performant%0AiResNet%20architectures.%20Through%20numerical%20experiments%2C%20we%20compare%20the%0Aperformance%20of%20our%20iResNet%20models%20against%20state-of-the-art%20neural%20networks%2C%0Aconfirming%20their%20efficacy.%20Additionally%2C%20we%20numerically%20investigate%20the%0Atheoretical%20guarantees%20of%20this%20approach%20and%20demonstrate%20how%20the%20invertibility%0Aof%20the%20network%20enables%20a%20deeper%20analysis%20of%20the%20learned%20forward%20operator%20and%0Aits%20learned%20regularization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13482v2&entry.124074799=Read"},
{"title": "Stabilizing Reinforcement Learning in Differentiable Multiphysics\n  Simulation", "author": "Eliot Xing and Vernon Luk and Jean Oh", "abstract": "  Recent advances in GPU-based parallel simulation have enabled practitioners\nto collect large amounts of data and train complex control policies using deep\nreinforcement learning (RL), on commodity GPUs. However, such successes for RL\nin robotics have been limited to tasks sufficiently simulated by fast\nrigid-body dynamics. Simulation techniques for soft bodies are comparatively\nseveral orders of magnitude slower, thereby limiting the use of RL due to\nsample complexity requirements. To address this challenge, this paper presents\nboth a novel RL algorithm and a simulation platform to enable scaling RL on\ntasks involving rigid bodies and deformables. We introduce Soft Analytic Policy\nOptimization (SAPO), a maximum entropy first-order model-based actor-critic RL\nalgorithm, which uses first-order analytic gradients from differentiable\nsimulation to train a stochastic actor to maximize expected return and entropy.\nAlongside our approach, we develop Rewarped, a parallel differentiable\nmultiphysics simulation platform that supports simulating various materials\nbeyond rigid bodies. We re-implement challenging manipulation and locomotion\ntasks in Rewarped, and show that SAPO outperforms baselines over a range of\ntasks that involve interaction between rigid bodies, articulations, and\ndeformables.\n", "link": "http://arxiv.org/abs/2412.12089v1", "date": "2024-12-16", "relevancy": 1.5896, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5803}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5201}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizing%20Reinforcement%20Learning%20in%20Differentiable%20Multiphysics%0A%20%20Simulation&body=Title%3A%20Stabilizing%20Reinforcement%20Learning%20in%20Differentiable%20Multiphysics%0A%20%20Simulation%0AAuthor%3A%20Eliot%20Xing%20and%20Vernon%20Luk%20and%20Jean%20Oh%0AAbstract%3A%20%20%20Recent%20advances%20in%20GPU-based%20parallel%20simulation%20have%20enabled%20practitioners%0Ato%20collect%20large%20amounts%20of%20data%20and%20train%20complex%20control%20policies%20using%20deep%0Areinforcement%20learning%20%28RL%29%2C%20on%20commodity%20GPUs.%20However%2C%20such%20successes%20for%20RL%0Ain%20robotics%20have%20been%20limited%20to%20tasks%20sufficiently%20simulated%20by%20fast%0Arigid-body%20dynamics.%20Simulation%20techniques%20for%20soft%20bodies%20are%20comparatively%0Aseveral%20orders%20of%20magnitude%20slower%2C%20thereby%20limiting%20the%20use%20of%20RL%20due%20to%0Asample%20complexity%20requirements.%20To%20address%20this%20challenge%2C%20this%20paper%20presents%0Aboth%20a%20novel%20RL%20algorithm%20and%20a%20simulation%20platform%20to%20enable%20scaling%20RL%20on%0Atasks%20involving%20rigid%20bodies%20and%20deformables.%20We%20introduce%20Soft%20Analytic%20Policy%0AOptimization%20%28SAPO%29%2C%20a%20maximum%20entropy%20first-order%20model-based%20actor-critic%20RL%0Aalgorithm%2C%20which%20uses%20first-order%20analytic%20gradients%20from%20differentiable%0Asimulation%20to%20train%20a%20stochastic%20actor%20to%20maximize%20expected%20return%20and%20entropy.%0AAlongside%20our%20approach%2C%20we%20develop%20Rewarped%2C%20a%20parallel%20differentiable%0Amultiphysics%20simulation%20platform%20that%20supports%20simulating%20various%20materials%0Abeyond%20rigid%20bodies.%20We%20re-implement%20challenging%20manipulation%20and%20locomotion%0Atasks%20in%20Rewarped%2C%20and%20show%20that%20SAPO%20outperforms%20baselines%20over%20a%20range%20of%0Atasks%20that%20involve%20interaction%20between%20rigid%20bodies%2C%20articulations%2C%20and%0Adeformables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizing%2520Reinforcement%2520Learning%2520in%2520Differentiable%2520Multiphysics%250A%2520%2520Simulation%26entry.906535625%3DEliot%2520Xing%2520and%2520Vernon%2520Luk%2520and%2520Jean%2520Oh%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520GPU-based%2520parallel%2520simulation%2520have%2520enabled%2520practitioners%250Ato%2520collect%2520large%2520amounts%2520of%2520data%2520and%2520train%2520complex%2520control%2520policies%2520using%2520deep%250Areinforcement%2520learning%2520%2528RL%2529%252C%2520on%2520commodity%2520GPUs.%2520However%252C%2520such%2520successes%2520for%2520RL%250Ain%2520robotics%2520have%2520been%2520limited%2520to%2520tasks%2520sufficiently%2520simulated%2520by%2520fast%250Arigid-body%2520dynamics.%2520Simulation%2520techniques%2520for%2520soft%2520bodies%2520are%2520comparatively%250Aseveral%2520orders%2520of%2520magnitude%2520slower%252C%2520thereby%2520limiting%2520the%2520use%2520of%2520RL%2520due%2520to%250Asample%2520complexity%2520requirements.%2520To%2520address%2520this%2520challenge%252C%2520this%2520paper%2520presents%250Aboth%2520a%2520novel%2520RL%2520algorithm%2520and%2520a%2520simulation%2520platform%2520to%2520enable%2520scaling%2520RL%2520on%250Atasks%2520involving%2520rigid%2520bodies%2520and%2520deformables.%2520We%2520introduce%2520Soft%2520Analytic%2520Policy%250AOptimization%2520%2528SAPO%2529%252C%2520a%2520maximum%2520entropy%2520first-order%2520model-based%2520actor-critic%2520RL%250Aalgorithm%252C%2520which%2520uses%2520first-order%2520analytic%2520gradients%2520from%2520differentiable%250Asimulation%2520to%2520train%2520a%2520stochastic%2520actor%2520to%2520maximize%2520expected%2520return%2520and%2520entropy.%250AAlongside%2520our%2520approach%252C%2520we%2520develop%2520Rewarped%252C%2520a%2520parallel%2520differentiable%250Amultiphysics%2520simulation%2520platform%2520that%2520supports%2520simulating%2520various%2520materials%250Abeyond%2520rigid%2520bodies.%2520We%2520re-implement%2520challenging%2520manipulation%2520and%2520locomotion%250Atasks%2520in%2520Rewarped%252C%2520and%2520show%2520that%2520SAPO%2520outperforms%2520baselines%2520over%2520a%2520range%2520of%250Atasks%2520that%2520involve%2520interaction%2520between%2520rigid%2520bodies%252C%2520articulations%252C%2520and%250Adeformables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizing%20Reinforcement%20Learning%20in%20Differentiable%20Multiphysics%0A%20%20Simulation&entry.906535625=Eliot%20Xing%20and%20Vernon%20Luk%20and%20Jean%20Oh&entry.1292438233=%20%20Recent%20advances%20in%20GPU-based%20parallel%20simulation%20have%20enabled%20practitioners%0Ato%20collect%20large%20amounts%20of%20data%20and%20train%20complex%20control%20policies%20using%20deep%0Areinforcement%20learning%20%28RL%29%2C%20on%20commodity%20GPUs.%20However%2C%20such%20successes%20for%20RL%0Ain%20robotics%20have%20been%20limited%20to%20tasks%20sufficiently%20simulated%20by%20fast%0Arigid-body%20dynamics.%20Simulation%20techniques%20for%20soft%20bodies%20are%20comparatively%0Aseveral%20orders%20of%20magnitude%20slower%2C%20thereby%20limiting%20the%20use%20of%20RL%20due%20to%0Asample%20complexity%20requirements.%20To%20address%20this%20challenge%2C%20this%20paper%20presents%0Aboth%20a%20novel%20RL%20algorithm%20and%20a%20simulation%20platform%20to%20enable%20scaling%20RL%20on%0Atasks%20involving%20rigid%20bodies%20and%20deformables.%20We%20introduce%20Soft%20Analytic%20Policy%0AOptimization%20%28SAPO%29%2C%20a%20maximum%20entropy%20first-order%20model-based%20actor-critic%20RL%0Aalgorithm%2C%20which%20uses%20first-order%20analytic%20gradients%20from%20differentiable%0Asimulation%20to%20train%20a%20stochastic%20actor%20to%20maximize%20expected%20return%20and%20entropy.%0AAlongside%20our%20approach%2C%20we%20develop%20Rewarped%2C%20a%20parallel%20differentiable%0Amultiphysics%20simulation%20platform%20that%20supports%20simulating%20various%20materials%0Abeyond%20rigid%20bodies.%20We%20re-implement%20challenging%20manipulation%20and%20locomotion%0Atasks%20in%20Rewarped%2C%20and%20show%20that%20SAPO%20outperforms%20baselines%20over%20a%20range%20of%0Atasks%20that%20involve%20interaction%20between%20rigid%20bodies%2C%20articulations%2C%20and%0Adeformables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12089v1&entry.124074799=Read"},
{"title": "The Impact of AI Assistance on Radiology Reporting: A Pilot Study Using\n  Simulated AI Draft Reports", "author": "Juli\u00e1n N. Acosta and Siddhant Dogra and Subathra Adithan and Kay Wu and Michael Moritz and Stephen Kwak and Pranav Rajpurkar", "abstract": "  Radiologists face increasing workload pressures amid growing imaging volumes,\ncreating risks of burnout and delayed reporting times. While artificial\nintelligence (AI) based automated radiology report generation shows promise for\nreporting workflow optimization, evidence of its real-world impact on clinical\naccuracy and efficiency remains limited. This study evaluated the effect of\ndraft reports on radiology reporting workflows by conducting a three reader\nmulti-case study comparing standard versus AI-assisted reporting workflows. In\nboth workflows, radiologists reviewed the cases and modified either a standard\ntemplate (standard workflow) or an AI-generated draft report (AI-assisted\nworkflow) to create the final report. For controlled evaluation, we used GPT-4\nto generate simulated AI drafts and deliberately introduced 1-3 errors in half\nthe cases to mimic real AI system performance. The AI-assisted workflow\nsignificantly reduced average reporting time from 573 to 435 seconds (p=0.003),\nwithout a statistically significant difference in clinically significant errors\nbetween workflows. These findings suggest that AI-generated drafts can\nmeaningfully accelerate radiology reporting while maintaining diagnostic\naccuracy, offering a practical solution to address mounting workload challenges\nin clinical practice.\n", "link": "http://arxiv.org/abs/2412.12042v1", "date": "2024-12-16", "relevancy": 1.5886, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3975}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20AI%20Assistance%20on%20Radiology%20Reporting%3A%20A%20Pilot%20Study%20Using%0A%20%20Simulated%20AI%20Draft%20Reports&body=Title%3A%20The%20Impact%20of%20AI%20Assistance%20on%20Radiology%20Reporting%3A%20A%20Pilot%20Study%20Using%0A%20%20Simulated%20AI%20Draft%20Reports%0AAuthor%3A%20Juli%C3%A1n%20N.%20Acosta%20and%20Siddhant%20Dogra%20and%20Subathra%20Adithan%20and%20Kay%20Wu%20and%20Michael%20Moritz%20and%20Stephen%20Kwak%20and%20Pranav%20Rajpurkar%0AAbstract%3A%20%20%20Radiologists%20face%20increasing%20workload%20pressures%20amid%20growing%20imaging%20volumes%2C%0Acreating%20risks%20of%20burnout%20and%20delayed%20reporting%20times.%20While%20artificial%0Aintelligence%20%28AI%29%20based%20automated%20radiology%20report%20generation%20shows%20promise%20for%0Areporting%20workflow%20optimization%2C%20evidence%20of%20its%20real-world%20impact%20on%20clinical%0Aaccuracy%20and%20efficiency%20remains%20limited.%20This%20study%20evaluated%20the%20effect%20of%0Adraft%20reports%20on%20radiology%20reporting%20workflows%20by%20conducting%20a%20three%20reader%0Amulti-case%20study%20comparing%20standard%20versus%20AI-assisted%20reporting%20workflows.%20In%0Aboth%20workflows%2C%20radiologists%20reviewed%20the%20cases%20and%20modified%20either%20a%20standard%0Atemplate%20%28standard%20workflow%29%20or%20an%20AI-generated%20draft%20report%20%28AI-assisted%0Aworkflow%29%20to%20create%20the%20final%20report.%20For%20controlled%20evaluation%2C%20we%20used%20GPT-4%0Ato%20generate%20simulated%20AI%20drafts%20and%20deliberately%20introduced%201-3%20errors%20in%20half%0Athe%20cases%20to%20mimic%20real%20AI%20system%20performance.%20The%20AI-assisted%20workflow%0Asignificantly%20reduced%20average%20reporting%20time%20from%20573%20to%20435%20seconds%20%28p%3D0.003%29%2C%0Awithout%20a%20statistically%20significant%20difference%20in%20clinically%20significant%20errors%0Abetween%20workflows.%20These%20findings%20suggest%20that%20AI-generated%20drafts%20can%0Ameaningfully%20accelerate%20radiology%20reporting%20while%20maintaining%20diagnostic%0Aaccuracy%2C%20offering%20a%20practical%20solution%20to%20address%20mounting%20workload%20challenges%0Ain%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520AI%2520Assistance%2520on%2520Radiology%2520Reporting%253A%2520A%2520Pilot%2520Study%2520Using%250A%2520%2520Simulated%2520AI%2520Draft%2520Reports%26entry.906535625%3DJuli%25C3%25A1n%2520N.%2520Acosta%2520and%2520Siddhant%2520Dogra%2520and%2520Subathra%2520Adithan%2520and%2520Kay%2520Wu%2520and%2520Michael%2520Moritz%2520and%2520Stephen%2520Kwak%2520and%2520Pranav%2520Rajpurkar%26entry.1292438233%3D%2520%2520Radiologists%2520face%2520increasing%2520workload%2520pressures%2520amid%2520growing%2520imaging%2520volumes%252C%250Acreating%2520risks%2520of%2520burnout%2520and%2520delayed%2520reporting%2520times.%2520While%2520artificial%250Aintelligence%2520%2528AI%2529%2520based%2520automated%2520radiology%2520report%2520generation%2520shows%2520promise%2520for%250Areporting%2520workflow%2520optimization%252C%2520evidence%2520of%2520its%2520real-world%2520impact%2520on%2520clinical%250Aaccuracy%2520and%2520efficiency%2520remains%2520limited.%2520This%2520study%2520evaluated%2520the%2520effect%2520of%250Adraft%2520reports%2520on%2520radiology%2520reporting%2520workflows%2520by%2520conducting%2520a%2520three%2520reader%250Amulti-case%2520study%2520comparing%2520standard%2520versus%2520AI-assisted%2520reporting%2520workflows.%2520In%250Aboth%2520workflows%252C%2520radiologists%2520reviewed%2520the%2520cases%2520and%2520modified%2520either%2520a%2520standard%250Atemplate%2520%2528standard%2520workflow%2529%2520or%2520an%2520AI-generated%2520draft%2520report%2520%2528AI-assisted%250Aworkflow%2529%2520to%2520create%2520the%2520final%2520report.%2520For%2520controlled%2520evaluation%252C%2520we%2520used%2520GPT-4%250Ato%2520generate%2520simulated%2520AI%2520drafts%2520and%2520deliberately%2520introduced%25201-3%2520errors%2520in%2520half%250Athe%2520cases%2520to%2520mimic%2520real%2520AI%2520system%2520performance.%2520The%2520AI-assisted%2520workflow%250Asignificantly%2520reduced%2520average%2520reporting%2520time%2520from%2520573%2520to%2520435%2520seconds%2520%2528p%253D0.003%2529%252C%250Awithout%2520a%2520statistically%2520significant%2520difference%2520in%2520clinically%2520significant%2520errors%250Abetween%2520workflows.%2520These%2520findings%2520suggest%2520that%2520AI-generated%2520drafts%2520can%250Ameaningfully%2520accelerate%2520radiology%2520reporting%2520while%2520maintaining%2520diagnostic%250Aaccuracy%252C%2520offering%2520a%2520practical%2520solution%2520to%2520address%2520mounting%2520workload%2520challenges%250Ain%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20AI%20Assistance%20on%20Radiology%20Reporting%3A%20A%20Pilot%20Study%20Using%0A%20%20Simulated%20AI%20Draft%20Reports&entry.906535625=Juli%C3%A1n%20N.%20Acosta%20and%20Siddhant%20Dogra%20and%20Subathra%20Adithan%20and%20Kay%20Wu%20and%20Michael%20Moritz%20and%20Stephen%20Kwak%20and%20Pranav%20Rajpurkar&entry.1292438233=%20%20Radiologists%20face%20increasing%20workload%20pressures%20amid%20growing%20imaging%20volumes%2C%0Acreating%20risks%20of%20burnout%20and%20delayed%20reporting%20times.%20While%20artificial%0Aintelligence%20%28AI%29%20based%20automated%20radiology%20report%20generation%20shows%20promise%20for%0Areporting%20workflow%20optimization%2C%20evidence%20of%20its%20real-world%20impact%20on%20clinical%0Aaccuracy%20and%20efficiency%20remains%20limited.%20This%20study%20evaluated%20the%20effect%20of%0Adraft%20reports%20on%20radiology%20reporting%20workflows%20by%20conducting%20a%20three%20reader%0Amulti-case%20study%20comparing%20standard%20versus%20AI-assisted%20reporting%20workflows.%20In%0Aboth%20workflows%2C%20radiologists%20reviewed%20the%20cases%20and%20modified%20either%20a%20standard%0Atemplate%20%28standard%20workflow%29%20or%20an%20AI-generated%20draft%20report%20%28AI-assisted%0Aworkflow%29%20to%20create%20the%20final%20report.%20For%20controlled%20evaluation%2C%20we%20used%20GPT-4%0Ato%20generate%20simulated%20AI%20drafts%20and%20deliberately%20introduced%201-3%20errors%20in%20half%0Athe%20cases%20to%20mimic%20real%20AI%20system%20performance.%20The%20AI-assisted%20workflow%0Asignificantly%20reduced%20average%20reporting%20time%20from%20573%20to%20435%20seconds%20%28p%3D0.003%29%2C%0Awithout%20a%20statistically%20significant%20difference%20in%20clinically%20significant%20errors%0Abetween%20workflows.%20These%20findings%20suggest%20that%20AI-generated%20drafts%20can%0Ameaningfully%20accelerate%20radiology%20reporting%20while%20maintaining%20diagnostic%0Aaccuracy%2C%20offering%20a%20practical%20solution%20to%20address%20mounting%20workload%20challenges%0Ain%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12042v1&entry.124074799=Read"},
{"title": "A LoRA is Worth a Thousand Pictures", "author": "Chenxi Liu and Towaki Takikawa and Alec Jacobson", "abstract": "  Recent advances in diffusion models and parameter-efficient fine-tuning\n(PEFT) have made text-to-image generation and customization widely accessible,\nwith Low Rank Adaptation (LoRA) able to replicate an artist's style or subject\nusing minimal data and computation. In this paper, we examine the relationship\nbetween LoRA weights and artistic styles, demonstrating that LoRA weights alone\ncan serve as an effective descriptor of style, without the need for additional\nimage generation or knowledge of the original training set. Our findings show\nthat LoRA weights yield better performance in clustering of artistic styles\ncompared to traditional pre-trained features, such as CLIP and DINO, with\nstrong structural similarities between LoRA-based and conventional image-based\nembeddings observed both qualitatively and quantitatively. We identify various\nretrieval scenarios for the growing collection of customized models and show\nthat our approach enables more accurate retrieval in real-world settings where\nknowledge of the training images is unavailable and additional generation is\nrequired. We conclude with a discussion on potential future applications, such\nas zero-shot LoRA fine-tuning and model attribution.\n", "link": "http://arxiv.org/abs/2412.12048v1", "date": "2024-12-16", "relevancy": 1.5871, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5359}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5261}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20LoRA%20is%20Worth%20a%20Thousand%20Pictures&body=Title%3A%20A%20LoRA%20is%20Worth%20a%20Thousand%20Pictures%0AAuthor%3A%20Chenxi%20Liu%20and%20Towaki%20Takikawa%20and%20Alec%20Jacobson%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20and%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20have%20made%20text-to-image%20generation%20and%20customization%20widely%20accessible%2C%0Awith%20Low%20Rank%20Adaptation%20%28LoRA%29%20able%20to%20replicate%20an%20artist%27s%20style%20or%20subject%0Ausing%20minimal%20data%20and%20computation.%20In%20this%20paper%2C%20we%20examine%20the%20relationship%0Abetween%20LoRA%20weights%20and%20artistic%20styles%2C%20demonstrating%20that%20LoRA%20weights%20alone%0Acan%20serve%20as%20an%20effective%20descriptor%20of%20style%2C%20without%20the%20need%20for%20additional%0Aimage%20generation%20or%20knowledge%20of%20the%20original%20training%20set.%20Our%20findings%20show%0Athat%20LoRA%20weights%20yield%20better%20performance%20in%20clustering%20of%20artistic%20styles%0Acompared%20to%20traditional%20pre-trained%20features%2C%20such%20as%20CLIP%20and%20DINO%2C%20with%0Astrong%20structural%20similarities%20between%20LoRA-based%20and%20conventional%20image-based%0Aembeddings%20observed%20both%20qualitatively%20and%20quantitatively.%20We%20identify%20various%0Aretrieval%20scenarios%20for%20the%20growing%20collection%20of%20customized%20models%20and%20show%0Athat%20our%20approach%20enables%20more%20accurate%20retrieval%20in%20real-world%20settings%20where%0Aknowledge%20of%20the%20training%20images%20is%20unavailable%20and%20additional%20generation%20is%0Arequired.%20We%20conclude%20with%20a%20discussion%20on%20potential%20future%20applications%2C%20such%0Aas%20zero-shot%20LoRA%20fine-tuning%20and%20model%20attribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520LoRA%2520is%2520Worth%2520a%2520Thousand%2520Pictures%26entry.906535625%3DChenxi%2520Liu%2520and%2520Towaki%2520Takikawa%2520and%2520Alec%2520Jacobson%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520and%2520parameter-efficient%2520fine-tuning%250A%2528PEFT%2529%2520have%2520made%2520text-to-image%2520generation%2520and%2520customization%2520widely%2520accessible%252C%250Awith%2520Low%2520Rank%2520Adaptation%2520%2528LoRA%2529%2520able%2520to%2520replicate%2520an%2520artist%2527s%2520style%2520or%2520subject%250Ausing%2520minimal%2520data%2520and%2520computation.%2520In%2520this%2520paper%252C%2520we%2520examine%2520the%2520relationship%250Abetween%2520LoRA%2520weights%2520and%2520artistic%2520styles%252C%2520demonstrating%2520that%2520LoRA%2520weights%2520alone%250Acan%2520serve%2520as%2520an%2520effective%2520descriptor%2520of%2520style%252C%2520without%2520the%2520need%2520for%2520additional%250Aimage%2520generation%2520or%2520knowledge%2520of%2520the%2520original%2520training%2520set.%2520Our%2520findings%2520show%250Athat%2520LoRA%2520weights%2520yield%2520better%2520performance%2520in%2520clustering%2520of%2520artistic%2520styles%250Acompared%2520to%2520traditional%2520pre-trained%2520features%252C%2520such%2520as%2520CLIP%2520and%2520DINO%252C%2520with%250Astrong%2520structural%2520similarities%2520between%2520LoRA-based%2520and%2520conventional%2520image-based%250Aembeddings%2520observed%2520both%2520qualitatively%2520and%2520quantitatively.%2520We%2520identify%2520various%250Aretrieval%2520scenarios%2520for%2520the%2520growing%2520collection%2520of%2520customized%2520models%2520and%2520show%250Athat%2520our%2520approach%2520enables%2520more%2520accurate%2520retrieval%2520in%2520real-world%2520settings%2520where%250Aknowledge%2520of%2520the%2520training%2520images%2520is%2520unavailable%2520and%2520additional%2520generation%2520is%250Arequired.%2520We%2520conclude%2520with%2520a%2520discussion%2520on%2520potential%2520future%2520applications%252C%2520such%250Aas%2520zero-shot%2520LoRA%2520fine-tuning%2520and%2520model%2520attribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20LoRA%20is%20Worth%20a%20Thousand%20Pictures&entry.906535625=Chenxi%20Liu%20and%20Towaki%20Takikawa%20and%20Alec%20Jacobson&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20and%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20have%20made%20text-to-image%20generation%20and%20customization%20widely%20accessible%2C%0Awith%20Low%20Rank%20Adaptation%20%28LoRA%29%20able%20to%20replicate%20an%20artist%27s%20style%20or%20subject%0Ausing%20minimal%20data%20and%20computation.%20In%20this%20paper%2C%20we%20examine%20the%20relationship%0Abetween%20LoRA%20weights%20and%20artistic%20styles%2C%20demonstrating%20that%20LoRA%20weights%20alone%0Acan%20serve%20as%20an%20effective%20descriptor%20of%20style%2C%20without%20the%20need%20for%20additional%0Aimage%20generation%20or%20knowledge%20of%20the%20original%20training%20set.%20Our%20findings%20show%0Athat%20LoRA%20weights%20yield%20better%20performance%20in%20clustering%20of%20artistic%20styles%0Acompared%20to%20traditional%20pre-trained%20features%2C%20such%20as%20CLIP%20and%20DINO%2C%20with%0Astrong%20structural%20similarities%20between%20LoRA-based%20and%20conventional%20image-based%0Aembeddings%20observed%20both%20qualitatively%20and%20quantitatively.%20We%20identify%20various%0Aretrieval%20scenarios%20for%20the%20growing%20collection%20of%20customized%20models%20and%20show%0Athat%20our%20approach%20enables%20more%20accurate%20retrieval%20in%20real-world%20settings%20where%0Aknowledge%20of%20the%20training%20images%20is%20unavailable%20and%20additional%20generation%20is%0Arequired.%20We%20conclude%20with%20a%20discussion%20on%20potential%20future%20applications%2C%20such%0Aas%20zero-shot%20LoRA%20fine-tuning%20and%20model%20attribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12048v1&entry.124074799=Read"},
{"title": "Hierarchical Meta-Reinforcement Learning via Automated Macro-Action\n  Discovery", "author": "Minjae Cho and Chuangchuang Sun", "abstract": "  Meta-Reinforcement Learning (Meta-RL) enables fast adaptation to new testing\ntasks. Despite recent advancements, it is still challenging to learn performant\npolicies across multiple complex and high-dimensional tasks. To address this,\nwe propose a novel architecture with three hierarchical levels for 1) learning\ntask representations, 2) discovering task-agnostic macro-actions in an\nautomated manner, and 3) learning primitive actions. The macro-action can guide\nthe low-level primitive policy learning to more efficiently transition to goal\nstates. This can address the issue that the policy may forget previously\nlearned behavior while learning new, conflicting tasks. Moreover, the\ntask-agnostic nature of the macro-actions is enabled by removing task-specific\ncomponents from the state space. Hence, this makes them amenable to\nre-composition across different tasks and leads to promising fast adaptation to\nnew tasks. Also, the prospective instability from the tri-level hierarchies is\neffectively mitigated by our innovative, independently tailored training\nschemes. Experiments in the MetaWorld framework demonstrate the improved sample\nefficiency and success rate of our approach compared to previous\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2412.11930v1", "date": "2024-12-16", "relevancy": 1.5744, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5817}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5139}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Meta-Reinforcement%20Learning%20via%20Automated%20Macro-Action%0A%20%20Discovery&body=Title%3A%20Hierarchical%20Meta-Reinforcement%20Learning%20via%20Automated%20Macro-Action%0A%20%20Discovery%0AAuthor%3A%20Minjae%20Cho%20and%20Chuangchuang%20Sun%0AAbstract%3A%20%20%20Meta-Reinforcement%20Learning%20%28Meta-RL%29%20enables%20fast%20adaptation%20to%20new%20testing%0Atasks.%20Despite%20recent%20advancements%2C%20it%20is%20still%20challenging%20to%20learn%20performant%0Apolicies%20across%20multiple%20complex%20and%20high-dimensional%20tasks.%20To%20address%20this%2C%0Awe%20propose%20a%20novel%20architecture%20with%20three%20hierarchical%20levels%20for%201%29%20learning%0Atask%20representations%2C%202%29%20discovering%20task-agnostic%20macro-actions%20in%20an%0Aautomated%20manner%2C%20and%203%29%20learning%20primitive%20actions.%20The%20macro-action%20can%20guide%0Athe%20low-level%20primitive%20policy%20learning%20to%20more%20efficiently%20transition%20to%20goal%0Astates.%20This%20can%20address%20the%20issue%20that%20the%20policy%20may%20forget%20previously%0Alearned%20behavior%20while%20learning%20new%2C%20conflicting%20tasks.%20Moreover%2C%20the%0Atask-agnostic%20nature%20of%20the%20macro-actions%20is%20enabled%20by%20removing%20task-specific%0Acomponents%20from%20the%20state%20space.%20Hence%2C%20this%20makes%20them%20amenable%20to%0Are-composition%20across%20different%20tasks%20and%20leads%20to%20promising%20fast%20adaptation%20to%0Anew%20tasks.%20Also%2C%20the%20prospective%20instability%20from%20the%20tri-level%20hierarchies%20is%0Aeffectively%20mitigated%20by%20our%20innovative%2C%20independently%20tailored%20training%0Aschemes.%20Experiments%20in%20the%20MetaWorld%20framework%20demonstrate%20the%20improved%20sample%0Aefficiency%20and%20success%20rate%20of%20our%20approach%20compared%20to%20previous%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Meta-Reinforcement%2520Learning%2520via%2520Automated%2520Macro-Action%250A%2520%2520Discovery%26entry.906535625%3DMinjae%2520Cho%2520and%2520Chuangchuang%2520Sun%26entry.1292438233%3D%2520%2520Meta-Reinforcement%2520Learning%2520%2528Meta-RL%2529%2520enables%2520fast%2520adaptation%2520to%2520new%2520testing%250Atasks.%2520Despite%2520recent%2520advancements%252C%2520it%2520is%2520still%2520challenging%2520to%2520learn%2520performant%250Apolicies%2520across%2520multiple%2520complex%2520and%2520high-dimensional%2520tasks.%2520To%2520address%2520this%252C%250Awe%2520propose%2520a%2520novel%2520architecture%2520with%2520three%2520hierarchical%2520levels%2520for%25201%2529%2520learning%250Atask%2520representations%252C%25202%2529%2520discovering%2520task-agnostic%2520macro-actions%2520in%2520an%250Aautomated%2520manner%252C%2520and%25203%2529%2520learning%2520primitive%2520actions.%2520The%2520macro-action%2520can%2520guide%250Athe%2520low-level%2520primitive%2520policy%2520learning%2520to%2520more%2520efficiently%2520transition%2520to%2520goal%250Astates.%2520This%2520can%2520address%2520the%2520issue%2520that%2520the%2520policy%2520may%2520forget%2520previously%250Alearned%2520behavior%2520while%2520learning%2520new%252C%2520conflicting%2520tasks.%2520Moreover%252C%2520the%250Atask-agnostic%2520nature%2520of%2520the%2520macro-actions%2520is%2520enabled%2520by%2520removing%2520task-specific%250Acomponents%2520from%2520the%2520state%2520space.%2520Hence%252C%2520this%2520makes%2520them%2520amenable%2520to%250Are-composition%2520across%2520different%2520tasks%2520and%2520leads%2520to%2520promising%2520fast%2520adaptation%2520to%250Anew%2520tasks.%2520Also%252C%2520the%2520prospective%2520instability%2520from%2520the%2520tri-level%2520hierarchies%2520is%250Aeffectively%2520mitigated%2520by%2520our%2520innovative%252C%2520independently%2520tailored%2520training%250Aschemes.%2520Experiments%2520in%2520the%2520MetaWorld%2520framework%2520demonstrate%2520the%2520improved%2520sample%250Aefficiency%2520and%2520success%2520rate%2520of%2520our%2520approach%2520compared%2520to%2520previous%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Meta-Reinforcement%20Learning%20via%20Automated%20Macro-Action%0A%20%20Discovery&entry.906535625=Minjae%20Cho%20and%20Chuangchuang%20Sun&entry.1292438233=%20%20Meta-Reinforcement%20Learning%20%28Meta-RL%29%20enables%20fast%20adaptation%20to%20new%20testing%0Atasks.%20Despite%20recent%20advancements%2C%20it%20is%20still%20challenging%20to%20learn%20performant%0Apolicies%20across%20multiple%20complex%20and%20high-dimensional%20tasks.%20To%20address%20this%2C%0Awe%20propose%20a%20novel%20architecture%20with%20three%20hierarchical%20levels%20for%201%29%20learning%0Atask%20representations%2C%202%29%20discovering%20task-agnostic%20macro-actions%20in%20an%0Aautomated%20manner%2C%20and%203%29%20learning%20primitive%20actions.%20The%20macro-action%20can%20guide%0Athe%20low-level%20primitive%20policy%20learning%20to%20more%20efficiently%20transition%20to%20goal%0Astates.%20This%20can%20address%20the%20issue%20that%20the%20policy%20may%20forget%20previously%0Alearned%20behavior%20while%20learning%20new%2C%20conflicting%20tasks.%20Moreover%2C%20the%0Atask-agnostic%20nature%20of%20the%20macro-actions%20is%20enabled%20by%20removing%20task-specific%0Acomponents%20from%20the%20state%20space.%20Hence%2C%20this%20makes%20them%20amenable%20to%0Are-composition%20across%20different%20tasks%20and%20leads%20to%20promising%20fast%20adaptation%20to%0Anew%20tasks.%20Also%2C%20the%20prospective%20instability%20from%20the%20tri-level%20hierarchies%20is%0Aeffectively%20mitigated%20by%20our%20innovative%2C%20independently%20tailored%20training%0Aschemes.%20Experiments%20in%20the%20MetaWorld%20framework%20demonstrate%20the%20improved%20sample%0Aefficiency%20and%20success%20rate%20of%20our%20approach%20compared%20to%20previous%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11930v1&entry.124074799=Read"},
{"title": "Reliable Breast Cancer Molecular Subtype Prediction based on\n  uncertainty-aware Bayesian Deep Learning by Mammography", "author": "Mohaddeseh Chegini and Ali Mahloojifar", "abstract": "  Breast cancer is a heterogeneous disease with different molecular subtypes,\nclinical behavior, treatment responses as well as survival outcomes. The\ndevelopment of a reliable, accurate, available and inexpensive method to\npredict the molecular subtypes using medical images plays an important role in\nthe diagnosis and prognosis of breast cancer. Recently, deep learning methods\nhave shown good performance in the breast cancer classification tasks using\nvarious medical images. Despite all that success, classical deep learning\ncannot deliver the predictive uncertainty. The uncertainty represents the\nvalidity of the predictions.Therefore, the high predicted uncertainty might\ncause a negative effect in the accurate diagnosis of breast cancer molecular\nsubtypes. To overcome this, uncertainty quantification methods are used to\ndetermine the predictive uncertainty. Accordingly, in this study, we proposed\nan uncertainty-aware Bayesian deep learning model using the full mammogram\nimages. In addition, to increase the performance of the multi-class molecular\nsubtype classification task, we proposed a novel hierarchical classification\nstrategy, named the two-stage classification strategy. The separate AUC of the\nproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,\nluminal and triple-negative classes, respectively. The proposed model not only\nhas a comparable performance to other studies in the field of breast cancer\nmolecular subtypes prediction, even using full mammography images, but it is\nalso more reliable, due to quantify the predictive uncertainty.\n", "link": "http://arxiv.org/abs/2412.11953v1", "date": "2024-12-16", "relevancy": 1.5702, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5647}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5183}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Breast%20Cancer%20Molecular%20Subtype%20Prediction%20based%20on%0A%20%20uncertainty-aware%20Bayesian%20Deep%20Learning%20by%20Mammography&body=Title%3A%20Reliable%20Breast%20Cancer%20Molecular%20Subtype%20Prediction%20based%20on%0A%20%20uncertainty-aware%20Bayesian%20Deep%20Learning%20by%20Mammography%0AAuthor%3A%20Mohaddeseh%20Chegini%20and%20Ali%20Mahloojifar%0AAbstract%3A%20%20%20Breast%20cancer%20is%20a%20heterogeneous%20disease%20with%20different%20molecular%20subtypes%2C%0Aclinical%20behavior%2C%20treatment%20responses%20as%20well%20as%20survival%20outcomes.%20The%0Adevelopment%20of%20a%20reliable%2C%20accurate%2C%20available%20and%20inexpensive%20method%20to%0Apredict%20the%20molecular%20subtypes%20using%20medical%20images%20plays%20an%20important%20role%20in%0Athe%20diagnosis%20and%20prognosis%20of%20breast%20cancer.%20Recently%2C%20deep%20learning%20methods%0Ahave%20shown%20good%20performance%20in%20the%20breast%20cancer%20classification%20tasks%20using%0Avarious%20medical%20images.%20Despite%20all%20that%20success%2C%20classical%20deep%20learning%0Acannot%20deliver%20the%20predictive%20uncertainty.%20The%20uncertainty%20represents%20the%0Avalidity%20of%20the%20predictions.Therefore%2C%20the%20high%20predicted%20uncertainty%20might%0Acause%20a%20negative%20effect%20in%20the%20accurate%20diagnosis%20of%20breast%20cancer%20molecular%0Asubtypes.%20To%20overcome%20this%2C%20uncertainty%20quantification%20methods%20are%20used%20to%0Adetermine%20the%20predictive%20uncertainty.%20Accordingly%2C%20in%20this%20study%2C%20we%20proposed%0Aan%20uncertainty-aware%20Bayesian%20deep%20learning%20model%20using%20the%20full%20mammogram%0Aimages.%20In%20addition%2C%20to%20increase%20the%20performance%20of%20the%20multi-class%20molecular%0Asubtype%20classification%20task%2C%20we%20proposed%20a%20novel%20hierarchical%20classification%0Astrategy%2C%20named%20the%20two-stage%20classification%20strategy.%20The%20separate%20AUC%20of%20the%0Aproposed%20model%20for%20each%20subtype%20was%200.71%2C%200.75%20and%200.86%20for%20HER2-enriched%2C%0Aluminal%20and%20triple-negative%20classes%2C%20respectively.%20The%20proposed%20model%20not%20only%0Ahas%20a%20comparable%20performance%20to%20other%20studies%20in%20the%20field%20of%20breast%20cancer%0Amolecular%20subtypes%20prediction%2C%20even%20using%20full%20mammography%20images%2C%20but%20it%20is%0Aalso%20more%20reliable%2C%20due%20to%20quantify%20the%20predictive%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Breast%2520Cancer%2520Molecular%2520Subtype%2520Prediction%2520based%2520on%250A%2520%2520uncertainty-aware%2520Bayesian%2520Deep%2520Learning%2520by%2520Mammography%26entry.906535625%3DMohaddeseh%2520Chegini%2520and%2520Ali%2520Mahloojifar%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520is%2520a%2520heterogeneous%2520disease%2520with%2520different%2520molecular%2520subtypes%252C%250Aclinical%2520behavior%252C%2520treatment%2520responses%2520as%2520well%2520as%2520survival%2520outcomes.%2520The%250Adevelopment%2520of%2520a%2520reliable%252C%2520accurate%252C%2520available%2520and%2520inexpensive%2520method%2520to%250Apredict%2520the%2520molecular%2520subtypes%2520using%2520medical%2520images%2520plays%2520an%2520important%2520role%2520in%250Athe%2520diagnosis%2520and%2520prognosis%2520of%2520breast%2520cancer.%2520Recently%252C%2520deep%2520learning%2520methods%250Ahave%2520shown%2520good%2520performance%2520in%2520the%2520breast%2520cancer%2520classification%2520tasks%2520using%250Avarious%2520medical%2520images.%2520Despite%2520all%2520that%2520success%252C%2520classical%2520deep%2520learning%250Acannot%2520deliver%2520the%2520predictive%2520uncertainty.%2520The%2520uncertainty%2520represents%2520the%250Avalidity%2520of%2520the%2520predictions.Therefore%252C%2520the%2520high%2520predicted%2520uncertainty%2520might%250Acause%2520a%2520negative%2520effect%2520in%2520the%2520accurate%2520diagnosis%2520of%2520breast%2520cancer%2520molecular%250Asubtypes.%2520To%2520overcome%2520this%252C%2520uncertainty%2520quantification%2520methods%2520are%2520used%2520to%250Adetermine%2520the%2520predictive%2520uncertainty.%2520Accordingly%252C%2520in%2520this%2520study%252C%2520we%2520proposed%250Aan%2520uncertainty-aware%2520Bayesian%2520deep%2520learning%2520model%2520using%2520the%2520full%2520mammogram%250Aimages.%2520In%2520addition%252C%2520to%2520increase%2520the%2520performance%2520of%2520the%2520multi-class%2520molecular%250Asubtype%2520classification%2520task%252C%2520we%2520proposed%2520a%2520novel%2520hierarchical%2520classification%250Astrategy%252C%2520named%2520the%2520two-stage%2520classification%2520strategy.%2520The%2520separate%2520AUC%2520of%2520the%250Aproposed%2520model%2520for%2520each%2520subtype%2520was%25200.71%252C%25200.75%2520and%25200.86%2520for%2520HER2-enriched%252C%250Aluminal%2520and%2520triple-negative%2520classes%252C%2520respectively.%2520The%2520proposed%2520model%2520not%2520only%250Ahas%2520a%2520comparable%2520performance%2520to%2520other%2520studies%2520in%2520the%2520field%2520of%2520breast%2520cancer%250Amolecular%2520subtypes%2520prediction%252C%2520even%2520using%2520full%2520mammography%2520images%252C%2520but%2520it%2520is%250Aalso%2520more%2520reliable%252C%2520due%2520to%2520quantify%2520the%2520predictive%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Breast%20Cancer%20Molecular%20Subtype%20Prediction%20based%20on%0A%20%20uncertainty-aware%20Bayesian%20Deep%20Learning%20by%20Mammography&entry.906535625=Mohaddeseh%20Chegini%20and%20Ali%20Mahloojifar&entry.1292438233=%20%20Breast%20cancer%20is%20a%20heterogeneous%20disease%20with%20different%20molecular%20subtypes%2C%0Aclinical%20behavior%2C%20treatment%20responses%20as%20well%20as%20survival%20outcomes.%20The%0Adevelopment%20of%20a%20reliable%2C%20accurate%2C%20available%20and%20inexpensive%20method%20to%0Apredict%20the%20molecular%20subtypes%20using%20medical%20images%20plays%20an%20important%20role%20in%0Athe%20diagnosis%20and%20prognosis%20of%20breast%20cancer.%20Recently%2C%20deep%20learning%20methods%0Ahave%20shown%20good%20performance%20in%20the%20breast%20cancer%20classification%20tasks%20using%0Avarious%20medical%20images.%20Despite%20all%20that%20success%2C%20classical%20deep%20learning%0Acannot%20deliver%20the%20predictive%20uncertainty.%20The%20uncertainty%20represents%20the%0Avalidity%20of%20the%20predictions.Therefore%2C%20the%20high%20predicted%20uncertainty%20might%0Acause%20a%20negative%20effect%20in%20the%20accurate%20diagnosis%20of%20breast%20cancer%20molecular%0Asubtypes.%20To%20overcome%20this%2C%20uncertainty%20quantification%20methods%20are%20used%20to%0Adetermine%20the%20predictive%20uncertainty.%20Accordingly%2C%20in%20this%20study%2C%20we%20proposed%0Aan%20uncertainty-aware%20Bayesian%20deep%20learning%20model%20using%20the%20full%20mammogram%0Aimages.%20In%20addition%2C%20to%20increase%20the%20performance%20of%20the%20multi-class%20molecular%0Asubtype%20classification%20task%2C%20we%20proposed%20a%20novel%20hierarchical%20classification%0Astrategy%2C%20named%20the%20two-stage%20classification%20strategy.%20The%20separate%20AUC%20of%20the%0Aproposed%20model%20for%20each%20subtype%20was%200.71%2C%200.75%20and%200.86%20for%20HER2-enriched%2C%0Aluminal%20and%20triple-negative%20classes%2C%20respectively.%20The%20proposed%20model%20not%20only%0Ahas%20a%20comparable%20performance%20to%20other%20studies%20in%20the%20field%20of%20breast%20cancer%0Amolecular%20subtypes%20prediction%2C%20even%20using%20full%20mammography%20images%2C%20but%20it%20is%0Aalso%20more%20reliable%2C%20due%20to%20quantify%20the%20predictive%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11953v1&entry.124074799=Read"},
{"title": "The State of Robot Motion Generation", "author": "Kostas E. Bekris and Joe Doerr and Patrick Meng and Sumanth Tangirala", "abstract": "  This paper reviews the large spectrum of methods for generating robot motion\nproposed over the 50 years of robotics research culminating in recent\ndevelopments. It crosses the boundaries of methodologies, typically not\nsurveyed together, from those that operate over explicit models to those that\nlearn implicit ones. The paper discusses the current state-of-the-art as well\nas properties of varying methodologies, highlighting opportunities for\nintegration.\n", "link": "http://arxiv.org/abs/2410.12172v2", "date": "2024-12-16", "relevancy": 1.5605, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5234}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20State%20of%20Robot%20Motion%20Generation&body=Title%3A%20The%20State%20of%20Robot%20Motion%20Generation%0AAuthor%3A%20Kostas%20E.%20Bekris%20and%20Joe%20Doerr%20and%20Patrick%20Meng%20and%20Sumanth%20Tangirala%0AAbstract%3A%20%20%20This%20paper%20reviews%20the%20large%20spectrum%20of%20methods%20for%20generating%20robot%20motion%0Aproposed%20over%20the%2050%20years%20of%20robotics%20research%20culminating%20in%20recent%0Adevelopments.%20It%20crosses%20the%20boundaries%20of%20methodologies%2C%20typically%20not%0Asurveyed%20together%2C%20from%20those%20that%20operate%20over%20explicit%20models%20to%20those%20that%0Alearn%20implicit%20ones.%20The%20paper%20discusses%20the%20current%20state-of-the-art%20as%20well%0Aas%20properties%20of%20varying%20methodologies%2C%20highlighting%20opportunities%20for%0Aintegration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520State%2520of%2520Robot%2520Motion%2520Generation%26entry.906535625%3DKostas%2520E.%2520Bekris%2520and%2520Joe%2520Doerr%2520and%2520Patrick%2520Meng%2520and%2520Sumanth%2520Tangirala%26entry.1292438233%3D%2520%2520This%2520paper%2520reviews%2520the%2520large%2520spectrum%2520of%2520methods%2520for%2520generating%2520robot%2520motion%250Aproposed%2520over%2520the%252050%2520years%2520of%2520robotics%2520research%2520culminating%2520in%2520recent%250Adevelopments.%2520It%2520crosses%2520the%2520boundaries%2520of%2520methodologies%252C%2520typically%2520not%250Asurveyed%2520together%252C%2520from%2520those%2520that%2520operate%2520over%2520explicit%2520models%2520to%2520those%2520that%250Alearn%2520implicit%2520ones.%2520The%2520paper%2520discusses%2520the%2520current%2520state-of-the-art%2520as%2520well%250Aas%2520properties%2520of%2520varying%2520methodologies%252C%2520highlighting%2520opportunities%2520for%250Aintegration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20State%20of%20Robot%20Motion%20Generation&entry.906535625=Kostas%20E.%20Bekris%20and%20Joe%20Doerr%20and%20Patrick%20Meng%20and%20Sumanth%20Tangirala&entry.1292438233=%20%20This%20paper%20reviews%20the%20large%20spectrum%20of%20methods%20for%20generating%20robot%20motion%0Aproposed%20over%20the%2050%20years%20of%20robotics%20research%20culminating%20in%20recent%0Adevelopments.%20It%20crosses%20the%20boundaries%20of%20methodologies%2C%20typically%20not%0Asurveyed%20together%2C%20from%20those%20that%20operate%20over%20explicit%20models%20to%20those%20that%0Alearn%20implicit%20ones.%20The%20paper%20discusses%20the%20current%20state-of-the-art%20as%20well%0Aas%20properties%20of%20varying%20methodologies%2C%20highlighting%20opportunities%20for%0Aintegration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12172v2&entry.124074799=Read"},
{"title": "A Digital twin for Diesel Engines: Operator-infused PINNs with Transfer\n  Learning for Engine Health Monitoring", "author": "Kamaljyoti Nath and Varun Kumar and Daniel J. Smith and George Em Karniadakis", "abstract": "  Improving diesel engine efficiency and emission reduction have been critical\nresearch topics. Recent government regulations have shifted this focus to\nanother important area related to engine health and performance monitoring.\nAlthough the advancements in the use of deep learning methods for system\nmonitoring have shown promising results in this direction, designing efficient\nmethods suitable for field systems remains an open research challenge. The\nobjective of this study is to develop a computationally efficient neural\nnetwork-based approach for identifying unknown parameters of a mean value\ndiesel engine model to facilitate physics-based health monitoring and\nmaintenance forecasting. We propose a hybrid method combining physics informed\nneural networks, PINNs, and a deep neural operator, DeepONet to predict unknown\nparameters and gas flow dynamics in a diesel engine. The operator network\npredicts independent actuator dynamics learnt through offline training, thereby\nreducing the PINNs online computational cost. To address PINNs need for\nretraining with changing input scenarios, we propose two transfer learning (TL)\nstrategies. The first strategy involves multi-stage transfer learning for\nparameter identification. While this method is computationally efficient as\ncompared to online PINN training, improvements are required to meet field\nrequirements. The second TL strategy focuses solely on training the output\nweights and biases of a subset of multi-head networks pretrained on a larger\ndataset, substantially reducing computation time during online prediction. We\nalso evaluate our model for epistemic and aleatoric uncertainty by\nincorporating dropout in pretrained networks and Gaussian noise in the training\ndataset. This strategy offers a tailored, computationally inexpensive, and\nphysics-based approach for parameter identification in diesel engine sub\nsystems.\n", "link": "http://arxiv.org/abs/2412.11967v1", "date": "2024-12-16", "relevancy": 1.5555, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5426}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Digital%20twin%20for%20Diesel%20Engines%3A%20Operator-infused%20PINNs%20with%20Transfer%0A%20%20Learning%20for%20Engine%20Health%20Monitoring&body=Title%3A%20A%20Digital%20twin%20for%20Diesel%20Engines%3A%20Operator-infused%20PINNs%20with%20Transfer%0A%20%20Learning%20for%20Engine%20Health%20Monitoring%0AAuthor%3A%20Kamaljyoti%20Nath%20and%20Varun%20Kumar%20and%20Daniel%20J.%20Smith%20and%20George%20Em%20Karniadakis%0AAbstract%3A%20%20%20Improving%20diesel%20engine%20efficiency%20and%20emission%20reduction%20have%20been%20critical%0Aresearch%20topics.%20Recent%20government%20regulations%20have%20shifted%20this%20focus%20to%0Aanother%20important%20area%20related%20to%20engine%20health%20and%20performance%20monitoring.%0AAlthough%20the%20advancements%20in%20the%20use%20of%20deep%20learning%20methods%20for%20system%0Amonitoring%20have%20shown%20promising%20results%20in%20this%20direction%2C%20designing%20efficient%0Amethods%20suitable%20for%20field%20systems%20remains%20an%20open%20research%20challenge.%20The%0Aobjective%20of%20this%20study%20is%20to%20develop%20a%20computationally%20efficient%20neural%0Anetwork-based%20approach%20for%20identifying%20unknown%20parameters%20of%20a%20mean%20value%0Adiesel%20engine%20model%20to%20facilitate%20physics-based%20health%20monitoring%20and%0Amaintenance%20forecasting.%20We%20propose%20a%20hybrid%20method%20combining%20physics%20informed%0Aneural%20networks%2C%20PINNs%2C%20and%20a%20deep%20neural%20operator%2C%20DeepONet%20to%20predict%20unknown%0Aparameters%20and%20gas%20flow%20dynamics%20in%20a%20diesel%20engine.%20The%20operator%20network%0Apredicts%20independent%20actuator%20dynamics%20learnt%20through%20offline%20training%2C%20thereby%0Areducing%20the%20PINNs%20online%20computational%20cost.%20To%20address%20PINNs%20need%20for%0Aretraining%20with%20changing%20input%20scenarios%2C%20we%20propose%20two%20transfer%20learning%20%28TL%29%0Astrategies.%20The%20first%20strategy%20involves%20multi-stage%20transfer%20learning%20for%0Aparameter%20identification.%20While%20this%20method%20is%20computationally%20efficient%20as%0Acompared%20to%20online%20PINN%20training%2C%20improvements%20are%20required%20to%20meet%20field%0Arequirements.%20The%20second%20TL%20strategy%20focuses%20solely%20on%20training%20the%20output%0Aweights%20and%20biases%20of%20a%20subset%20of%20multi-head%20networks%20pretrained%20on%20a%20larger%0Adataset%2C%20substantially%20reducing%20computation%20time%20during%20online%20prediction.%20We%0Aalso%20evaluate%20our%20model%20for%20epistemic%20and%20aleatoric%20uncertainty%20by%0Aincorporating%20dropout%20in%20pretrained%20networks%20and%20Gaussian%20noise%20in%20the%20training%0Adataset.%20This%20strategy%20offers%20a%20tailored%2C%20computationally%20inexpensive%2C%20and%0Aphysics-based%20approach%20for%20parameter%20identification%20in%20diesel%20engine%20sub%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Digital%2520twin%2520for%2520Diesel%2520Engines%253A%2520Operator-infused%2520PINNs%2520with%2520Transfer%250A%2520%2520Learning%2520for%2520Engine%2520Health%2520Monitoring%26entry.906535625%3DKamaljyoti%2520Nath%2520and%2520Varun%2520Kumar%2520and%2520Daniel%2520J.%2520Smith%2520and%2520George%2520Em%2520Karniadakis%26entry.1292438233%3D%2520%2520Improving%2520diesel%2520engine%2520efficiency%2520and%2520emission%2520reduction%2520have%2520been%2520critical%250Aresearch%2520topics.%2520Recent%2520government%2520regulations%2520have%2520shifted%2520this%2520focus%2520to%250Aanother%2520important%2520area%2520related%2520to%2520engine%2520health%2520and%2520performance%2520monitoring.%250AAlthough%2520the%2520advancements%2520in%2520the%2520use%2520of%2520deep%2520learning%2520methods%2520for%2520system%250Amonitoring%2520have%2520shown%2520promising%2520results%2520in%2520this%2520direction%252C%2520designing%2520efficient%250Amethods%2520suitable%2520for%2520field%2520systems%2520remains%2520an%2520open%2520research%2520challenge.%2520The%250Aobjective%2520of%2520this%2520study%2520is%2520to%2520develop%2520a%2520computationally%2520efficient%2520neural%250Anetwork-based%2520approach%2520for%2520identifying%2520unknown%2520parameters%2520of%2520a%2520mean%2520value%250Adiesel%2520engine%2520model%2520to%2520facilitate%2520physics-based%2520health%2520monitoring%2520and%250Amaintenance%2520forecasting.%2520We%2520propose%2520a%2520hybrid%2520method%2520combining%2520physics%2520informed%250Aneural%2520networks%252C%2520PINNs%252C%2520and%2520a%2520deep%2520neural%2520operator%252C%2520DeepONet%2520to%2520predict%2520unknown%250Aparameters%2520and%2520gas%2520flow%2520dynamics%2520in%2520a%2520diesel%2520engine.%2520The%2520operator%2520network%250Apredicts%2520independent%2520actuator%2520dynamics%2520learnt%2520through%2520offline%2520training%252C%2520thereby%250Areducing%2520the%2520PINNs%2520online%2520computational%2520cost.%2520To%2520address%2520PINNs%2520need%2520for%250Aretraining%2520with%2520changing%2520input%2520scenarios%252C%2520we%2520propose%2520two%2520transfer%2520learning%2520%2528TL%2529%250Astrategies.%2520The%2520first%2520strategy%2520involves%2520multi-stage%2520transfer%2520learning%2520for%250Aparameter%2520identification.%2520While%2520this%2520method%2520is%2520computationally%2520efficient%2520as%250Acompared%2520to%2520online%2520PINN%2520training%252C%2520improvements%2520are%2520required%2520to%2520meet%2520field%250Arequirements.%2520The%2520second%2520TL%2520strategy%2520focuses%2520solely%2520on%2520training%2520the%2520output%250Aweights%2520and%2520biases%2520of%2520a%2520subset%2520of%2520multi-head%2520networks%2520pretrained%2520on%2520a%2520larger%250Adataset%252C%2520substantially%2520reducing%2520computation%2520time%2520during%2520online%2520prediction.%2520We%250Aalso%2520evaluate%2520our%2520model%2520for%2520epistemic%2520and%2520aleatoric%2520uncertainty%2520by%250Aincorporating%2520dropout%2520in%2520pretrained%2520networks%2520and%2520Gaussian%2520noise%2520in%2520the%2520training%250Adataset.%2520This%2520strategy%2520offers%2520a%2520tailored%252C%2520computationally%2520inexpensive%252C%2520and%250Aphysics-based%2520approach%2520for%2520parameter%2520identification%2520in%2520diesel%2520engine%2520sub%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Digital%20twin%20for%20Diesel%20Engines%3A%20Operator-infused%20PINNs%20with%20Transfer%0A%20%20Learning%20for%20Engine%20Health%20Monitoring&entry.906535625=Kamaljyoti%20Nath%20and%20Varun%20Kumar%20and%20Daniel%20J.%20Smith%20and%20George%20Em%20Karniadakis&entry.1292438233=%20%20Improving%20diesel%20engine%20efficiency%20and%20emission%20reduction%20have%20been%20critical%0Aresearch%20topics.%20Recent%20government%20regulations%20have%20shifted%20this%20focus%20to%0Aanother%20important%20area%20related%20to%20engine%20health%20and%20performance%20monitoring.%0AAlthough%20the%20advancements%20in%20the%20use%20of%20deep%20learning%20methods%20for%20system%0Amonitoring%20have%20shown%20promising%20results%20in%20this%20direction%2C%20designing%20efficient%0Amethods%20suitable%20for%20field%20systems%20remains%20an%20open%20research%20challenge.%20The%0Aobjective%20of%20this%20study%20is%20to%20develop%20a%20computationally%20efficient%20neural%0Anetwork-based%20approach%20for%20identifying%20unknown%20parameters%20of%20a%20mean%20value%0Adiesel%20engine%20model%20to%20facilitate%20physics-based%20health%20monitoring%20and%0Amaintenance%20forecasting.%20We%20propose%20a%20hybrid%20method%20combining%20physics%20informed%0Aneural%20networks%2C%20PINNs%2C%20and%20a%20deep%20neural%20operator%2C%20DeepONet%20to%20predict%20unknown%0Aparameters%20and%20gas%20flow%20dynamics%20in%20a%20diesel%20engine.%20The%20operator%20network%0Apredicts%20independent%20actuator%20dynamics%20learnt%20through%20offline%20training%2C%20thereby%0Areducing%20the%20PINNs%20online%20computational%20cost.%20To%20address%20PINNs%20need%20for%0Aretraining%20with%20changing%20input%20scenarios%2C%20we%20propose%20two%20transfer%20learning%20%28TL%29%0Astrategies.%20The%20first%20strategy%20involves%20multi-stage%20transfer%20learning%20for%0Aparameter%20identification.%20While%20this%20method%20is%20computationally%20efficient%20as%0Acompared%20to%20online%20PINN%20training%2C%20improvements%20are%20required%20to%20meet%20field%0Arequirements.%20The%20second%20TL%20strategy%20focuses%20solely%20on%20training%20the%20output%0Aweights%20and%20biases%20of%20a%20subset%20of%20multi-head%20networks%20pretrained%20on%20a%20larger%0Adataset%2C%20substantially%20reducing%20computation%20time%20during%20online%20prediction.%20We%0Aalso%20evaluate%20our%20model%20for%20epistemic%20and%20aleatoric%20uncertainty%20by%0Aincorporating%20dropout%20in%20pretrained%20networks%20and%20Gaussian%20noise%20in%20the%20training%0Adataset.%20This%20strategy%20offers%20a%20tailored%2C%20computationally%20inexpensive%2C%20and%0Aphysics-based%20approach%20for%20parameter%20identification%20in%20diesel%20engine%20sub%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11967v1&entry.124074799=Read"},
{"title": "Artificial Intelligence in Traffic Systems", "author": "Ritwik Raj Saxena", "abstract": "  Existing research on AI-based traffic management systems, utilizing\ntechniques such as fuzzy logic, reinforcement learning, deep neural networks,\nand evolutionary algorithms, demonstrates the potential of AI to transform the\ntraffic landscape. This article endeavors to review the topics where AI and\ntraffic management intersect. It comprises areas like AI-powered traffic signal\ncontrol systems, automatic distance and velocity recognition (for instance, in\nautonomous vehicles, hereafter AVs), smart parking systems, and Intelligent\nTraffic Management Systems (ITMS), which use data captured in real-time to keep\ntrack of traffic conditions, and traffic-related law enforcement and\nsurveillance using AI. AI applications in traffic management cover a wide range\nof spheres. The spheres comprise, inter alia, streamlining traffic signal\ntimings, predicting traffic bottlenecks in specific areas, detecting potential\naccidents and road hazards, managing incidents accurately, advancing public\ntransportation systems, development of innovative driver assistance systems,\nand minimizing environmental impact through simplified routes and reduced\nemissions. The benefits of AI in traffic management are also diverse. They\ncomprise improved management of traffic data, sounder route decision\nautomation, easier and speedier identification and resolution of vehicular\nissues through monitoring the condition of individual vehicles, decreased\ntraffic snarls and mishaps, superior resource utilization, alleviated stress of\ntraffic management manpower, greater on-road safety, and better emergency\nresponse time.\n", "link": "http://arxiv.org/abs/2412.12046v1", "date": "2024-12-16", "relevancy": 1.5486, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3843}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence%20in%20Traffic%20Systems&body=Title%3A%20Artificial%20Intelligence%20in%20Traffic%20Systems%0AAuthor%3A%20Ritwik%20Raj%20Saxena%0AAbstract%3A%20%20%20Existing%20research%20on%20AI-based%20traffic%20management%20systems%2C%20utilizing%0Atechniques%20such%20as%20fuzzy%20logic%2C%20reinforcement%20learning%2C%20deep%20neural%20networks%2C%0Aand%20evolutionary%20algorithms%2C%20demonstrates%20the%20potential%20of%20AI%20to%20transform%20the%0Atraffic%20landscape.%20This%20article%20endeavors%20to%20review%20the%20topics%20where%20AI%20and%0Atraffic%20management%20intersect.%20It%20comprises%20areas%20like%20AI-powered%20traffic%20signal%0Acontrol%20systems%2C%20automatic%20distance%20and%20velocity%20recognition%20%28for%20instance%2C%20in%0Aautonomous%20vehicles%2C%20hereafter%20AVs%29%2C%20smart%20parking%20systems%2C%20and%20Intelligent%0ATraffic%20Management%20Systems%20%28ITMS%29%2C%20which%20use%20data%20captured%20in%20real-time%20to%20keep%0Atrack%20of%20traffic%20conditions%2C%20and%20traffic-related%20law%20enforcement%20and%0Asurveillance%20using%20AI.%20AI%20applications%20in%20traffic%20management%20cover%20a%20wide%20range%0Aof%20spheres.%20The%20spheres%20comprise%2C%20inter%20alia%2C%20streamlining%20traffic%20signal%0Atimings%2C%20predicting%20traffic%20bottlenecks%20in%20specific%20areas%2C%20detecting%20potential%0Aaccidents%20and%20road%20hazards%2C%20managing%20incidents%20accurately%2C%20advancing%20public%0Atransportation%20systems%2C%20development%20of%20innovative%20driver%20assistance%20systems%2C%0Aand%20minimizing%20environmental%20impact%20through%20simplified%20routes%20and%20reduced%0Aemissions.%20The%20benefits%20of%20AI%20in%20traffic%20management%20are%20also%20diverse.%20They%0Acomprise%20improved%20management%20of%20traffic%20data%2C%20sounder%20route%20decision%0Aautomation%2C%20easier%20and%20speedier%20identification%20and%20resolution%20of%20vehicular%0Aissues%20through%20monitoring%20the%20condition%20of%20individual%20vehicles%2C%20decreased%0Atraffic%20snarls%20and%20mishaps%2C%20superior%20resource%20utilization%2C%20alleviated%20stress%20of%0Atraffic%20management%20manpower%2C%20greater%20on-road%20safety%2C%20and%20better%20emergency%0Aresponse%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence%2520in%2520Traffic%2520Systems%26entry.906535625%3DRitwik%2520Raj%2520Saxena%26entry.1292438233%3D%2520%2520Existing%2520research%2520on%2520AI-based%2520traffic%2520management%2520systems%252C%2520utilizing%250Atechniques%2520such%2520as%2520fuzzy%2520logic%252C%2520reinforcement%2520learning%252C%2520deep%2520neural%2520networks%252C%250Aand%2520evolutionary%2520algorithms%252C%2520demonstrates%2520the%2520potential%2520of%2520AI%2520to%2520transform%2520the%250Atraffic%2520landscape.%2520This%2520article%2520endeavors%2520to%2520review%2520the%2520topics%2520where%2520AI%2520and%250Atraffic%2520management%2520intersect.%2520It%2520comprises%2520areas%2520like%2520AI-powered%2520traffic%2520signal%250Acontrol%2520systems%252C%2520automatic%2520distance%2520and%2520velocity%2520recognition%2520%2528for%2520instance%252C%2520in%250Aautonomous%2520vehicles%252C%2520hereafter%2520AVs%2529%252C%2520smart%2520parking%2520systems%252C%2520and%2520Intelligent%250ATraffic%2520Management%2520Systems%2520%2528ITMS%2529%252C%2520which%2520use%2520data%2520captured%2520in%2520real-time%2520to%2520keep%250Atrack%2520of%2520traffic%2520conditions%252C%2520and%2520traffic-related%2520law%2520enforcement%2520and%250Asurveillance%2520using%2520AI.%2520AI%2520applications%2520in%2520traffic%2520management%2520cover%2520a%2520wide%2520range%250Aof%2520spheres.%2520The%2520spheres%2520comprise%252C%2520inter%2520alia%252C%2520streamlining%2520traffic%2520signal%250Atimings%252C%2520predicting%2520traffic%2520bottlenecks%2520in%2520specific%2520areas%252C%2520detecting%2520potential%250Aaccidents%2520and%2520road%2520hazards%252C%2520managing%2520incidents%2520accurately%252C%2520advancing%2520public%250Atransportation%2520systems%252C%2520development%2520of%2520innovative%2520driver%2520assistance%2520systems%252C%250Aand%2520minimizing%2520environmental%2520impact%2520through%2520simplified%2520routes%2520and%2520reduced%250Aemissions.%2520The%2520benefits%2520of%2520AI%2520in%2520traffic%2520management%2520are%2520also%2520diverse.%2520They%250Acomprise%2520improved%2520management%2520of%2520traffic%2520data%252C%2520sounder%2520route%2520decision%250Aautomation%252C%2520easier%2520and%2520speedier%2520identification%2520and%2520resolution%2520of%2520vehicular%250Aissues%2520through%2520monitoring%2520the%2520condition%2520of%2520individual%2520vehicles%252C%2520decreased%250Atraffic%2520snarls%2520and%2520mishaps%252C%2520superior%2520resource%2520utilization%252C%2520alleviated%2520stress%2520of%250Atraffic%2520management%2520manpower%252C%2520greater%2520on-road%2520safety%252C%2520and%2520better%2520emergency%250Aresponse%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence%20in%20Traffic%20Systems&entry.906535625=Ritwik%20Raj%20Saxena&entry.1292438233=%20%20Existing%20research%20on%20AI-based%20traffic%20management%20systems%2C%20utilizing%0Atechniques%20such%20as%20fuzzy%20logic%2C%20reinforcement%20learning%2C%20deep%20neural%20networks%2C%0Aand%20evolutionary%20algorithms%2C%20demonstrates%20the%20potential%20of%20AI%20to%20transform%20the%0Atraffic%20landscape.%20This%20article%20endeavors%20to%20review%20the%20topics%20where%20AI%20and%0Atraffic%20management%20intersect.%20It%20comprises%20areas%20like%20AI-powered%20traffic%20signal%0Acontrol%20systems%2C%20automatic%20distance%20and%20velocity%20recognition%20%28for%20instance%2C%20in%0Aautonomous%20vehicles%2C%20hereafter%20AVs%29%2C%20smart%20parking%20systems%2C%20and%20Intelligent%0ATraffic%20Management%20Systems%20%28ITMS%29%2C%20which%20use%20data%20captured%20in%20real-time%20to%20keep%0Atrack%20of%20traffic%20conditions%2C%20and%20traffic-related%20law%20enforcement%20and%0Asurveillance%20using%20AI.%20AI%20applications%20in%20traffic%20management%20cover%20a%20wide%20range%0Aof%20spheres.%20The%20spheres%20comprise%2C%20inter%20alia%2C%20streamlining%20traffic%20signal%0Atimings%2C%20predicting%20traffic%20bottlenecks%20in%20specific%20areas%2C%20detecting%20potential%0Aaccidents%20and%20road%20hazards%2C%20managing%20incidents%20accurately%2C%20advancing%20public%0Atransportation%20systems%2C%20development%20of%20innovative%20driver%20assistance%20systems%2C%0Aand%20minimizing%20environmental%20impact%20through%20simplified%20routes%20and%20reduced%0Aemissions.%20The%20benefits%20of%20AI%20in%20traffic%20management%20are%20also%20diverse.%20They%0Acomprise%20improved%20management%20of%20traffic%20data%2C%20sounder%20route%20decision%0Aautomation%2C%20easier%20and%20speedier%20identification%20and%20resolution%20of%20vehicular%0Aissues%20through%20monitoring%20the%20condition%20of%20individual%20vehicles%2C%20decreased%0Atraffic%20snarls%20and%20mishaps%2C%20superior%20resource%20utilization%2C%20alleviated%20stress%20of%0Atraffic%20management%20manpower%2C%20greater%20on-road%20safety%2C%20and%20better%20emergency%0Aresponse%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12046v1&entry.124074799=Read"},
{"title": "Explainable Procedural Mistake Detection", "author": "Shane Storks and Itamar Bar-Yossef and Yayuan Li and Zheyuan Zhang and Jason J. Corso and Joyce Chai", "abstract": "  Automated task guidance has recently attracted attention from the AI research\ncommunity. Procedural mistake detection (PMD) is a challenging sub-problem of\nclassifying whether a human user (observed through egocentric video) has\nsuccessfully executed the task at hand (specified by a procedural text).\nDespite significant efforts in building resources and models for PMD, machine\nperformance remains nonviable, and the reasoning processes underlying this\nperformance are opaque. As such, we recast PMD to an explanatory self-dialog of\nquestions and answers, which serve as evidence for a decision. As this\nreformulation enables an unprecedented transparency, we leverage a fine-tuned\nnatural language inference (NLI) model to formulate two automated coherence\nmetrics for generated explanations. Our results show that while open-source\nVLMs struggle with this task off-the-shelf, their accuracy, coherence, and\ndialog efficiency can be vastly improved by incorporating these coherence\nmetrics into common inference and fine-tuning methods. Furthermore, our\nmulti-faceted metrics can visualize common outcomes at a glance, highlighting\nareas for improvement.\n", "link": "http://arxiv.org/abs/2412.11927v1", "date": "2024-12-16", "relevancy": 1.5467, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5392}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Procedural%20Mistake%20Detection&body=Title%3A%20Explainable%20Procedural%20Mistake%20Detection%0AAuthor%3A%20Shane%20Storks%20and%20Itamar%20Bar-Yossef%20and%20Yayuan%20Li%20and%20Zheyuan%20Zhang%20and%20Jason%20J.%20Corso%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Automated%20task%20guidance%20has%20recently%20attracted%20attention%20from%20the%20AI%20research%0Acommunity.%20Procedural%20mistake%20detection%20%28PMD%29%20is%20a%20challenging%20sub-problem%20of%0Aclassifying%20whether%20a%20human%20user%20%28observed%20through%20egocentric%20video%29%20has%0Asuccessfully%20executed%20the%20task%20at%20hand%20%28specified%20by%20a%20procedural%20text%29.%0ADespite%20significant%20efforts%20in%20building%20resources%20and%20models%20for%20PMD%2C%20machine%0Aperformance%20remains%20nonviable%2C%20and%20the%20reasoning%20processes%20underlying%20this%0Aperformance%20are%20opaque.%20As%20such%2C%20we%20recast%20PMD%20to%20an%20explanatory%20self-dialog%20of%0Aquestions%20and%20answers%2C%20which%20serve%20as%20evidence%20for%20a%20decision.%20As%20this%0Areformulation%20enables%20an%20unprecedented%20transparency%2C%20we%20leverage%20a%20fine-tuned%0Anatural%20language%20inference%20%28NLI%29%20model%20to%20formulate%20two%20automated%20coherence%0Ametrics%20for%20generated%20explanations.%20Our%20results%20show%20that%20while%20open-source%0AVLMs%20struggle%20with%20this%20task%20off-the-shelf%2C%20their%20accuracy%2C%20coherence%2C%20and%0Adialog%20efficiency%20can%20be%20vastly%20improved%20by%20incorporating%20these%20coherence%0Ametrics%20into%20common%20inference%20and%20fine-tuning%20methods.%20Furthermore%2C%20our%0Amulti-faceted%20metrics%20can%20visualize%20common%20outcomes%20at%20a%20glance%2C%20highlighting%0Aareas%20for%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Procedural%2520Mistake%2520Detection%26entry.906535625%3DShane%2520Storks%2520and%2520Itamar%2520Bar-Yossef%2520and%2520Yayuan%2520Li%2520and%2520Zheyuan%2520Zhang%2520and%2520Jason%2520J.%2520Corso%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520Automated%2520task%2520guidance%2520has%2520recently%2520attracted%2520attention%2520from%2520the%2520AI%2520research%250Acommunity.%2520Procedural%2520mistake%2520detection%2520%2528PMD%2529%2520is%2520a%2520challenging%2520sub-problem%2520of%250Aclassifying%2520whether%2520a%2520human%2520user%2520%2528observed%2520through%2520egocentric%2520video%2529%2520has%250Asuccessfully%2520executed%2520the%2520task%2520at%2520hand%2520%2528specified%2520by%2520a%2520procedural%2520text%2529.%250ADespite%2520significant%2520efforts%2520in%2520building%2520resources%2520and%2520models%2520for%2520PMD%252C%2520machine%250Aperformance%2520remains%2520nonviable%252C%2520and%2520the%2520reasoning%2520processes%2520underlying%2520this%250Aperformance%2520are%2520opaque.%2520As%2520such%252C%2520we%2520recast%2520PMD%2520to%2520an%2520explanatory%2520self-dialog%2520of%250Aquestions%2520and%2520answers%252C%2520which%2520serve%2520as%2520evidence%2520for%2520a%2520decision.%2520As%2520this%250Areformulation%2520enables%2520an%2520unprecedented%2520transparency%252C%2520we%2520leverage%2520a%2520fine-tuned%250Anatural%2520language%2520inference%2520%2528NLI%2529%2520model%2520to%2520formulate%2520two%2520automated%2520coherence%250Ametrics%2520for%2520generated%2520explanations.%2520Our%2520results%2520show%2520that%2520while%2520open-source%250AVLMs%2520struggle%2520with%2520this%2520task%2520off-the-shelf%252C%2520their%2520accuracy%252C%2520coherence%252C%2520and%250Adialog%2520efficiency%2520can%2520be%2520vastly%2520improved%2520by%2520incorporating%2520these%2520coherence%250Ametrics%2520into%2520common%2520inference%2520and%2520fine-tuning%2520methods.%2520Furthermore%252C%2520our%250Amulti-faceted%2520metrics%2520can%2520visualize%2520common%2520outcomes%2520at%2520a%2520glance%252C%2520highlighting%250Aareas%2520for%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Procedural%20Mistake%20Detection&entry.906535625=Shane%20Storks%20and%20Itamar%20Bar-Yossef%20and%20Yayuan%20Li%20and%20Zheyuan%20Zhang%20and%20Jason%20J.%20Corso%20and%20Joyce%20Chai&entry.1292438233=%20%20Automated%20task%20guidance%20has%20recently%20attracted%20attention%20from%20the%20AI%20research%0Acommunity.%20Procedural%20mistake%20detection%20%28PMD%29%20is%20a%20challenging%20sub-problem%20of%0Aclassifying%20whether%20a%20human%20user%20%28observed%20through%20egocentric%20video%29%20has%0Asuccessfully%20executed%20the%20task%20at%20hand%20%28specified%20by%20a%20procedural%20text%29.%0ADespite%20significant%20efforts%20in%20building%20resources%20and%20models%20for%20PMD%2C%20machine%0Aperformance%20remains%20nonviable%2C%20and%20the%20reasoning%20processes%20underlying%20this%0Aperformance%20are%20opaque.%20As%20such%2C%20we%20recast%20PMD%20to%20an%20explanatory%20self-dialog%20of%0Aquestions%20and%20answers%2C%20which%20serve%20as%20evidence%20for%20a%20decision.%20As%20this%0Areformulation%20enables%20an%20unprecedented%20transparency%2C%20we%20leverage%20a%20fine-tuned%0Anatural%20language%20inference%20%28NLI%29%20model%20to%20formulate%20two%20automated%20coherence%0Ametrics%20for%20generated%20explanations.%20Our%20results%20show%20that%20while%20open-source%0AVLMs%20struggle%20with%20this%20task%20off-the-shelf%2C%20their%20accuracy%2C%20coherence%2C%20and%0Adialog%20efficiency%20can%20be%20vastly%20improved%20by%20incorporating%20these%20coherence%0Ametrics%20into%20common%20inference%20and%20fine-tuning%20methods.%20Furthermore%2C%20our%0Amulti-faceted%20metrics%20can%20visualize%20common%20outcomes%20at%20a%20glance%2C%20highlighting%0Aareas%20for%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11927v1&entry.124074799=Read"},
{"title": "Risk and cross validation in ridge regression with correlated samples", "author": "Alexander Atanasov and Jacob A. Zavatone-Veth and Cengiz Pehlevan", "abstract": "  Recent years have seen substantial advances in our understanding of\nhigh-dimensional ridge regression, but existing theories assume that training\nexamples are independent. By leveraging techniques from random matrix theory\nand free probability, we provide sharp asymptotics for the in- and\nout-of-sample risks of ridge regression when the data points have arbitrary\ncorrelations. We demonstrate that in this setting, the generalized cross\nvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.\nHowever, in the case where the noise residuals have the same correlations as\nthe data points, one can modify the GCV to yield an efficiently-computable\nunbiased estimator that concentrates in the high-dimensional limit, which we\ndub CorrGCV. We further extend our asymptotic analysis to the case where the\ntest point has nontrivial correlations with the training set, a setting often\nencountered in time series forecasting. Assuming knowledge of the correlation\nstructure of the time series, this again yields an extension of the GCV\nestimator, and sharply characterizes the degree to which such test points yield\nan overly optimistic prediction of long-time risk. We validate the predictions\nof our theory across a variety of high dimensional data.\n", "link": "http://arxiv.org/abs/2408.04607v3", "date": "2024-12-16", "relevancy": 1.5254, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3963}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3821}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk%20and%20cross%20validation%20in%20ridge%20regression%20with%20correlated%20samples&body=Title%3A%20Risk%20and%20cross%20validation%20in%20ridge%20regression%20with%20correlated%20samples%0AAuthor%3A%20Alexander%20Atanasov%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20substantial%20advances%20in%20our%20understanding%20of%0Ahigh-dimensional%20ridge%20regression%2C%20but%20existing%20theories%20assume%20that%20training%0Aexamples%20are%20independent.%20By%20leveraging%20techniques%20from%20random%20matrix%20theory%0Aand%20free%20probability%2C%20we%20provide%20sharp%20asymptotics%20for%20the%20in-%20and%0Aout-of-sample%20risks%20of%20ridge%20regression%20when%20the%20data%20points%20have%20arbitrary%0Acorrelations.%20We%20demonstrate%20that%20in%20this%20setting%2C%20the%20generalized%20cross%0Avalidation%20estimator%20%28GCV%29%20fails%20to%20correctly%20predict%20the%20out-of-sample%20risk.%0AHowever%2C%20in%20the%20case%20where%20the%20noise%20residuals%20have%20the%20same%20correlations%20as%0Athe%20data%20points%2C%20one%20can%20modify%20the%20GCV%20to%20yield%20an%20efficiently-computable%0Aunbiased%20estimator%20that%20concentrates%20in%20the%20high-dimensional%20limit%2C%20which%20we%0Adub%20CorrGCV.%20We%20further%20extend%20our%20asymptotic%20analysis%20to%20the%20case%20where%20the%0Atest%20point%20has%20nontrivial%20correlations%20with%20the%20training%20set%2C%20a%20setting%20often%0Aencountered%20in%20time%20series%20forecasting.%20Assuming%20knowledge%20of%20the%20correlation%0Astructure%20of%20the%20time%20series%2C%20this%20again%20yields%20an%20extension%20of%20the%20GCV%0Aestimator%2C%20and%20sharply%20characterizes%20the%20degree%20to%20which%20such%20test%20points%20yield%0Aan%20overly%20optimistic%20prediction%20of%20long-time%20risk.%20We%20validate%20the%20predictions%0Aof%20our%20theory%20across%20a%20variety%20of%20high%20dimensional%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04607v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk%2520and%2520cross%2520validation%2520in%2520ridge%2520regression%2520with%2520correlated%2520samples%26entry.906535625%3DAlexander%2520Atanasov%2520and%2520Jacob%2520A.%2520Zavatone-Veth%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520substantial%2520advances%2520in%2520our%2520understanding%2520of%250Ahigh-dimensional%2520ridge%2520regression%252C%2520but%2520existing%2520theories%2520assume%2520that%2520training%250Aexamples%2520are%2520independent.%2520By%2520leveraging%2520techniques%2520from%2520random%2520matrix%2520theory%250Aand%2520free%2520probability%252C%2520we%2520provide%2520sharp%2520asymptotics%2520for%2520the%2520in-%2520and%250Aout-of-sample%2520risks%2520of%2520ridge%2520regression%2520when%2520the%2520data%2520points%2520have%2520arbitrary%250Acorrelations.%2520We%2520demonstrate%2520that%2520in%2520this%2520setting%252C%2520the%2520generalized%2520cross%250Avalidation%2520estimator%2520%2528GCV%2529%2520fails%2520to%2520correctly%2520predict%2520the%2520out-of-sample%2520risk.%250AHowever%252C%2520in%2520the%2520case%2520where%2520the%2520noise%2520residuals%2520have%2520the%2520same%2520correlations%2520as%250Athe%2520data%2520points%252C%2520one%2520can%2520modify%2520the%2520GCV%2520to%2520yield%2520an%2520efficiently-computable%250Aunbiased%2520estimator%2520that%2520concentrates%2520in%2520the%2520high-dimensional%2520limit%252C%2520which%2520we%250Adub%2520CorrGCV.%2520We%2520further%2520extend%2520our%2520asymptotic%2520analysis%2520to%2520the%2520case%2520where%2520the%250Atest%2520point%2520has%2520nontrivial%2520correlations%2520with%2520the%2520training%2520set%252C%2520a%2520setting%2520often%250Aencountered%2520in%2520time%2520series%2520forecasting.%2520Assuming%2520knowledge%2520of%2520the%2520correlation%250Astructure%2520of%2520the%2520time%2520series%252C%2520this%2520again%2520yields%2520an%2520extension%2520of%2520the%2520GCV%250Aestimator%252C%2520and%2520sharply%2520characterizes%2520the%2520degree%2520to%2520which%2520such%2520test%2520points%2520yield%250Aan%2520overly%2520optimistic%2520prediction%2520of%2520long-time%2520risk.%2520We%2520validate%2520the%2520predictions%250Aof%2520our%2520theory%2520across%2520a%2520variety%2520of%2520high%2520dimensional%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04607v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk%20and%20cross%20validation%20in%20ridge%20regression%20with%20correlated%20samples&entry.906535625=Alexander%20Atanasov%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20Recent%20years%20have%20seen%20substantial%20advances%20in%20our%20understanding%20of%0Ahigh-dimensional%20ridge%20regression%2C%20but%20existing%20theories%20assume%20that%20training%0Aexamples%20are%20independent.%20By%20leveraging%20techniques%20from%20random%20matrix%20theory%0Aand%20free%20probability%2C%20we%20provide%20sharp%20asymptotics%20for%20the%20in-%20and%0Aout-of-sample%20risks%20of%20ridge%20regression%20when%20the%20data%20points%20have%20arbitrary%0Acorrelations.%20We%20demonstrate%20that%20in%20this%20setting%2C%20the%20generalized%20cross%0Avalidation%20estimator%20%28GCV%29%20fails%20to%20correctly%20predict%20the%20out-of-sample%20risk.%0AHowever%2C%20in%20the%20case%20where%20the%20noise%20residuals%20have%20the%20same%20correlations%20as%0Athe%20data%20points%2C%20one%20can%20modify%20the%20GCV%20to%20yield%20an%20efficiently-computable%0Aunbiased%20estimator%20that%20concentrates%20in%20the%20high-dimensional%20limit%2C%20which%20we%0Adub%20CorrGCV.%20We%20further%20extend%20our%20asymptotic%20analysis%20to%20the%20case%20where%20the%0Atest%20point%20has%20nontrivial%20correlations%20with%20the%20training%20set%2C%20a%20setting%20often%0Aencountered%20in%20time%20series%20forecasting.%20Assuming%20knowledge%20of%20the%20correlation%0Astructure%20of%20the%20time%20series%2C%20this%20again%20yields%20an%20extension%20of%20the%20GCV%0Aestimator%2C%20and%20sharply%20characterizes%20the%20degree%20to%20which%20such%20test%20points%20yield%0Aan%20overly%20optimistic%20prediction%20of%20long-time%20risk.%20We%20validate%20the%20predictions%0Aof%20our%20theory%20across%20a%20variety%20of%20high%20dimensional%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04607v3&entry.124074799=Read"},
{"title": "Lightweight Decentralized Neural Network-Based Strategies for\n  Multi-Robot Patrolling", "author": "James C. Ward and Ryan McConville and Edmund R. Hunt", "abstract": "  The problem of decentralized multi-robot patrol has previously been\napproached primarily with hand-designed strategies for minimization of\n'idlenes' over the vertices of a graph-structured environment. Here we present\ntwo lightweight neural network-based strategies to tackle this problem, and\nshow that they significantly outperform existing strategies in both idleness\nminimization and against an intelligent intruder model, as well as presenting\nan examination of robustness to communication failure. Our results also\nindicate important considerations for future strategy design.\n", "link": "http://arxiv.org/abs/2412.11916v1", "date": "2024-12-16", "relevancy": 1.5078, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5324}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4978}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Decentralized%20Neural%20Network-Based%20Strategies%20for%0A%20%20Multi-Robot%20Patrolling&body=Title%3A%20Lightweight%20Decentralized%20Neural%20Network-Based%20Strategies%20for%0A%20%20Multi-Robot%20Patrolling%0AAuthor%3A%20James%20C.%20Ward%20and%20Ryan%20McConville%20and%20Edmund%20R.%20Hunt%0AAbstract%3A%20%20%20The%20problem%20of%20decentralized%20multi-robot%20patrol%20has%20previously%20been%0Aapproached%20primarily%20with%20hand-designed%20strategies%20for%20minimization%20of%0A%27idlenes%27%20over%20the%20vertices%20of%20a%20graph-structured%20environment.%20Here%20we%20present%0Atwo%20lightweight%20neural%20network-based%20strategies%20to%20tackle%20this%20problem%2C%20and%0Ashow%20that%20they%20significantly%20outperform%20existing%20strategies%20in%20both%20idleness%0Aminimization%20and%20against%20an%20intelligent%20intruder%20model%2C%20as%20well%20as%20presenting%0Aan%20examination%20of%20robustness%20to%20communication%20failure.%20Our%20results%20also%0Aindicate%20important%20considerations%20for%20future%20strategy%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Decentralized%2520Neural%2520Network-Based%2520Strategies%2520for%250A%2520%2520Multi-Robot%2520Patrolling%26entry.906535625%3DJames%2520C.%2520Ward%2520and%2520Ryan%2520McConville%2520and%2520Edmund%2520R.%2520Hunt%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520decentralized%2520multi-robot%2520patrol%2520has%2520previously%2520been%250Aapproached%2520primarily%2520with%2520hand-designed%2520strategies%2520for%2520minimization%2520of%250A%2527idlenes%2527%2520over%2520the%2520vertices%2520of%2520a%2520graph-structured%2520environment.%2520Here%2520we%2520present%250Atwo%2520lightweight%2520neural%2520network-based%2520strategies%2520to%2520tackle%2520this%2520problem%252C%2520and%250Ashow%2520that%2520they%2520significantly%2520outperform%2520existing%2520strategies%2520in%2520both%2520idleness%250Aminimization%2520and%2520against%2520an%2520intelligent%2520intruder%2520model%252C%2520as%2520well%2520as%2520presenting%250Aan%2520examination%2520of%2520robustness%2520to%2520communication%2520failure.%2520Our%2520results%2520also%250Aindicate%2520important%2520considerations%2520for%2520future%2520strategy%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Decentralized%20Neural%20Network-Based%20Strategies%20for%0A%20%20Multi-Robot%20Patrolling&entry.906535625=James%20C.%20Ward%20and%20Ryan%20McConville%20and%20Edmund%20R.%20Hunt&entry.1292438233=%20%20The%20problem%20of%20decentralized%20multi-robot%20patrol%20has%20previously%20been%0Aapproached%20primarily%20with%20hand-designed%20strategies%20for%20minimization%20of%0A%27idlenes%27%20over%20the%20vertices%20of%20a%20graph-structured%20environment.%20Here%20we%20present%0Atwo%20lightweight%20neural%20network-based%20strategies%20to%20tackle%20this%20problem%2C%20and%0Ashow%20that%20they%20significantly%20outperform%20existing%20strategies%20in%20both%20idleness%0Aminimization%20and%20against%20an%20intelligent%20intruder%20model%2C%20as%20well%20as%20presenting%0Aan%20examination%20of%20robustness%20to%20communication%20failure.%20Our%20results%20also%0Aindicate%20important%20considerations%20for%20future%20strategy%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11916v1&entry.124074799=Read"},
{"title": "The Impact of Generalization Techniques on the Interplay Among Privacy,\n  Utility, and Fairness in Image Classification", "author": "Ahmad Hassanpour and Amir Zarei and Khawla Mallat and Anderson Santana de Oliveira and Bian Yang", "abstract": "  This study investigates the trade-offs between fairness, privacy, and utility\nin image classification using machine learning (ML). Recent research suggests\nthat generalization techniques can improve the balance between privacy and\nutility. One focus of this work is sharpness-aware training (SAT) and its\nintegration with differential privacy (DP-SAT) to further improve this balance.\nAdditionally, we examine fairness in both private and non-private learning\nmodels trained on datasets with synthetic and real-world biases. We also\nmeasure the privacy risks involved in these scenarios by performing membership\ninference attacks (MIAs) and explore the consequences of eliminating\nhigh-privacy risk samples, termed outliers. Moreover, we introduce a new\nmetric, named \\emph{harmonic score}, which combines accuracy, privacy, and\nfairness into a single measure.\n  Through empirical analysis using generalization techniques, we achieve an\naccuracy of 81.11\\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\\%\nreported by De et al. (2022). Moreover, our experiments show that memorization\nof training samples can begin before the overfitting point, and generalization\ntechniques do not guarantee the prevention of this memorization. Our analysis\nof synthetic biases shows that generalization techniques can amplify model bias\nin both private and non-private models. Additionally, our results indicate that\nincreased bias in training data leads to reduced accuracy, greater\nvulnerability to privacy attacks, and higher model bias. We validate these\nfindings with the CelebA dataset, demonstrating that similar trends persist\nwith real-world attribute imbalances. Finally, our experiments show that\nremoving outlier data decreases accuracy and further amplifies model bias.\n", "link": "http://arxiv.org/abs/2412.11951v1", "date": "2024-12-16", "relevancy": 1.4905, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5002}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.497}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Generalization%20Techniques%20on%20the%20Interplay%20Among%20Privacy%2C%0A%20%20Utility%2C%20and%20Fairness%20in%20Image%20Classification&body=Title%3A%20The%20Impact%20of%20Generalization%20Techniques%20on%20the%20Interplay%20Among%20Privacy%2C%0A%20%20Utility%2C%20and%20Fairness%20in%20Image%20Classification%0AAuthor%3A%20Ahmad%20Hassanpour%20and%20Amir%20Zarei%20and%20Khawla%20Mallat%20and%20Anderson%20Santana%20de%20Oliveira%20and%20Bian%20Yang%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20trade-offs%20between%20fairness%2C%20privacy%2C%20and%20utility%0Ain%20image%20classification%20using%20machine%20learning%20%28ML%29.%20Recent%20research%20suggests%0Athat%20generalization%20techniques%20can%20improve%20the%20balance%20between%20privacy%20and%0Autility.%20One%20focus%20of%20this%20work%20is%20sharpness-aware%20training%20%28SAT%29%20and%20its%0Aintegration%20with%20differential%20privacy%20%28DP-SAT%29%20to%20further%20improve%20this%20balance.%0AAdditionally%2C%20we%20examine%20fairness%20in%20both%20private%20and%20non-private%20learning%0Amodels%20trained%20on%20datasets%20with%20synthetic%20and%20real-world%20biases.%20We%20also%0Ameasure%20the%20privacy%20risks%20involved%20in%20these%20scenarios%20by%20performing%20membership%0Ainference%20attacks%20%28MIAs%29%20and%20explore%20the%20consequences%20of%20eliminating%0Ahigh-privacy%20risk%20samples%2C%20termed%20outliers.%20Moreover%2C%20we%20introduce%20a%20new%0Ametric%2C%20named%20%5Cemph%7Bharmonic%20score%7D%2C%20which%20combines%20accuracy%2C%20privacy%2C%20and%0Afairness%20into%20a%20single%20measure.%0A%20%20Through%20empirical%20analysis%20using%20generalization%20techniques%2C%20we%20achieve%20an%0Aaccuracy%20of%2081.11%5C%25%20under%20%24%288%2C%2010%5E%7B-5%7D%29%24-DP%20on%20CIFAR-10%2C%20surpassing%20the%2079.5%5C%25%0Areported%20by%20De%20et%20al.%20%282022%29.%20Moreover%2C%20our%20experiments%20show%20that%20memorization%0Aof%20training%20samples%20can%20begin%20before%20the%20overfitting%20point%2C%20and%20generalization%0Atechniques%20do%20not%20guarantee%20the%20prevention%20of%20this%20memorization.%20Our%20analysis%0Aof%20synthetic%20biases%20shows%20that%20generalization%20techniques%20can%20amplify%20model%20bias%0Ain%20both%20private%20and%20non-private%20models.%20Additionally%2C%20our%20results%20indicate%20that%0Aincreased%20bias%20in%20training%20data%20leads%20to%20reduced%20accuracy%2C%20greater%0Avulnerability%20to%20privacy%20attacks%2C%20and%20higher%20model%20bias.%20We%20validate%20these%0Afindings%20with%20the%20CelebA%20dataset%2C%20demonstrating%20that%20similar%20trends%20persist%0Awith%20real-world%20attribute%20imbalances.%20Finally%2C%20our%20experiments%20show%20that%0Aremoving%20outlier%20data%20decreases%20accuracy%20and%20further%20amplifies%20model%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Generalization%2520Techniques%2520on%2520the%2520Interplay%2520Among%2520Privacy%252C%250A%2520%2520Utility%252C%2520and%2520Fairness%2520in%2520Image%2520Classification%26entry.906535625%3DAhmad%2520Hassanpour%2520and%2520Amir%2520Zarei%2520and%2520Khawla%2520Mallat%2520and%2520Anderson%2520Santana%2520de%2520Oliveira%2520and%2520Bian%2520Yang%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520trade-offs%2520between%2520fairness%252C%2520privacy%252C%2520and%2520utility%250Ain%2520image%2520classification%2520using%2520machine%2520learning%2520%2528ML%2529.%2520Recent%2520research%2520suggests%250Athat%2520generalization%2520techniques%2520can%2520improve%2520the%2520balance%2520between%2520privacy%2520and%250Autility.%2520One%2520focus%2520of%2520this%2520work%2520is%2520sharpness-aware%2520training%2520%2528SAT%2529%2520and%2520its%250Aintegration%2520with%2520differential%2520privacy%2520%2528DP-SAT%2529%2520to%2520further%2520improve%2520this%2520balance.%250AAdditionally%252C%2520we%2520examine%2520fairness%2520in%2520both%2520private%2520and%2520non-private%2520learning%250Amodels%2520trained%2520on%2520datasets%2520with%2520synthetic%2520and%2520real-world%2520biases.%2520We%2520also%250Ameasure%2520the%2520privacy%2520risks%2520involved%2520in%2520these%2520scenarios%2520by%2520performing%2520membership%250Ainference%2520attacks%2520%2528MIAs%2529%2520and%2520explore%2520the%2520consequences%2520of%2520eliminating%250Ahigh-privacy%2520risk%2520samples%252C%2520termed%2520outliers.%2520Moreover%252C%2520we%2520introduce%2520a%2520new%250Ametric%252C%2520named%2520%255Cemph%257Bharmonic%2520score%257D%252C%2520which%2520combines%2520accuracy%252C%2520privacy%252C%2520and%250Afairness%2520into%2520a%2520single%2520measure.%250A%2520%2520Through%2520empirical%2520analysis%2520using%2520generalization%2520techniques%252C%2520we%2520achieve%2520an%250Aaccuracy%2520of%252081.11%255C%2525%2520under%2520%2524%25288%252C%252010%255E%257B-5%257D%2529%2524-DP%2520on%2520CIFAR-10%252C%2520surpassing%2520the%252079.5%255C%2525%250Areported%2520by%2520De%2520et%2520al.%2520%25282022%2529.%2520Moreover%252C%2520our%2520experiments%2520show%2520that%2520memorization%250Aof%2520training%2520samples%2520can%2520begin%2520before%2520the%2520overfitting%2520point%252C%2520and%2520generalization%250Atechniques%2520do%2520not%2520guarantee%2520the%2520prevention%2520of%2520this%2520memorization.%2520Our%2520analysis%250Aof%2520synthetic%2520biases%2520shows%2520that%2520generalization%2520techniques%2520can%2520amplify%2520model%2520bias%250Ain%2520both%2520private%2520and%2520non-private%2520models.%2520Additionally%252C%2520our%2520results%2520indicate%2520that%250Aincreased%2520bias%2520in%2520training%2520data%2520leads%2520to%2520reduced%2520accuracy%252C%2520greater%250Avulnerability%2520to%2520privacy%2520attacks%252C%2520and%2520higher%2520model%2520bias.%2520We%2520validate%2520these%250Afindings%2520with%2520the%2520CelebA%2520dataset%252C%2520demonstrating%2520that%2520similar%2520trends%2520persist%250Awith%2520real-world%2520attribute%2520imbalances.%2520Finally%252C%2520our%2520experiments%2520show%2520that%250Aremoving%2520outlier%2520data%2520decreases%2520accuracy%2520and%2520further%2520amplifies%2520model%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Generalization%20Techniques%20on%20the%20Interplay%20Among%20Privacy%2C%0A%20%20Utility%2C%20and%20Fairness%20in%20Image%20Classification&entry.906535625=Ahmad%20Hassanpour%20and%20Amir%20Zarei%20and%20Khawla%20Mallat%20and%20Anderson%20Santana%20de%20Oliveira%20and%20Bian%20Yang&entry.1292438233=%20%20This%20study%20investigates%20the%20trade-offs%20between%20fairness%2C%20privacy%2C%20and%20utility%0Ain%20image%20classification%20using%20machine%20learning%20%28ML%29.%20Recent%20research%20suggests%0Athat%20generalization%20techniques%20can%20improve%20the%20balance%20between%20privacy%20and%0Autility.%20One%20focus%20of%20this%20work%20is%20sharpness-aware%20training%20%28SAT%29%20and%20its%0Aintegration%20with%20differential%20privacy%20%28DP-SAT%29%20to%20further%20improve%20this%20balance.%0AAdditionally%2C%20we%20examine%20fairness%20in%20both%20private%20and%20non-private%20learning%0Amodels%20trained%20on%20datasets%20with%20synthetic%20and%20real-world%20biases.%20We%20also%0Ameasure%20the%20privacy%20risks%20involved%20in%20these%20scenarios%20by%20performing%20membership%0Ainference%20attacks%20%28MIAs%29%20and%20explore%20the%20consequences%20of%20eliminating%0Ahigh-privacy%20risk%20samples%2C%20termed%20outliers.%20Moreover%2C%20we%20introduce%20a%20new%0Ametric%2C%20named%20%5Cemph%7Bharmonic%20score%7D%2C%20which%20combines%20accuracy%2C%20privacy%2C%20and%0Afairness%20into%20a%20single%20measure.%0A%20%20Through%20empirical%20analysis%20using%20generalization%20techniques%2C%20we%20achieve%20an%0Aaccuracy%20of%2081.11%5C%25%20under%20%24%288%2C%2010%5E%7B-5%7D%29%24-DP%20on%20CIFAR-10%2C%20surpassing%20the%2079.5%5C%25%0Areported%20by%20De%20et%20al.%20%282022%29.%20Moreover%2C%20our%20experiments%20show%20that%20memorization%0Aof%20training%20samples%20can%20begin%20before%20the%20overfitting%20point%2C%20and%20generalization%0Atechniques%20do%20not%20guarantee%20the%20prevention%20of%20this%20memorization.%20Our%20analysis%0Aof%20synthetic%20biases%20shows%20that%20generalization%20techniques%20can%20amplify%20model%20bias%0Ain%20both%20private%20and%20non-private%20models.%20Additionally%2C%20our%20results%20indicate%20that%0Aincreased%20bias%20in%20training%20data%20leads%20to%20reduced%20accuracy%2C%20greater%0Avulnerability%20to%20privacy%20attacks%2C%20and%20higher%20model%20bias.%20We%20validate%20these%0Afindings%20with%20the%20CelebA%20dataset%2C%20demonstrating%20that%20similar%20trends%20persist%0Awith%20real-world%20attribute%20imbalances.%20Finally%2C%20our%20experiments%20show%20that%0Aremoving%20outlier%20data%20decreases%20accuracy%20and%20further%20amplifies%20model%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11951v1&entry.124074799=Read"},
{"title": "Comp-LTL: Temporal Logic Planning via Zero-Shot Policy Composition", "author": "Taylor Bergeron and Zachary Serlin and Kevin Leahy", "abstract": "  This work develops a zero-shot mechanism, Comp-LTL, for an agent to satisfy a\nLinear Temporal Logic (LTL) specification given existing task primitives\ntrained via reinforcement learning (RL). Autonomous robots often need to\nsatisfy spatial and temporal goals that are unknown until run time. Prior work\nfocuses on learning policies for executing a task specified using LTL, but they\nincorporate the specification into the learning process. Any change to the\nspecification requires retraining the policy, either via fine-tuning or from\nscratch. We present a more flexible approach -- to learn a set of composable\ntask primitive policies that can be used to satisfy arbitrary LTL\nspecifications without retraining or fine-tuning. Task primitives can be\nlearned offline using RL and combined using Boolean composition at deployment.\nThis work focuses on creating and pruning a transition system (TS)\nrepresentation of the environment in order to solve for deterministic,\nnon-ambiguous, and feasible solutions to LTL specifications given an\nenvironment and a set of task primitive policies. We show that our pruned TS is\ndeterministic, contains no unrealizable transitions, and is sound. We verify\nour approach via simulation and compare it to other state of the art\napproaches, showing that Comp-LTL is safer and more adaptable.\n", "link": "http://arxiv.org/abs/2408.04215v2", "date": "2024-12-16", "relevancy": 1.4872, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5207}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5049}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comp-LTL%3A%20Temporal%20Logic%20Planning%20via%20Zero-Shot%20Policy%20Composition&body=Title%3A%20Comp-LTL%3A%20Temporal%20Logic%20Planning%20via%20Zero-Shot%20Policy%20Composition%0AAuthor%3A%20Taylor%20Bergeron%20and%20Zachary%20Serlin%20and%20Kevin%20Leahy%0AAbstract%3A%20%20%20This%20work%20develops%20a%20zero-shot%20mechanism%2C%20Comp-LTL%2C%20for%20an%20agent%20to%20satisfy%20a%0ALinear%20Temporal%20Logic%20%28LTL%29%20specification%20given%20existing%20task%20primitives%0Atrained%20via%20reinforcement%20learning%20%28RL%29.%20Autonomous%20robots%20often%20need%20to%0Asatisfy%20spatial%20and%20temporal%20goals%20that%20are%20unknown%20until%20run%20time.%20Prior%20work%0Afocuses%20on%20learning%20policies%20for%20executing%20a%20task%20specified%20using%20LTL%2C%20but%20they%0Aincorporate%20the%20specification%20into%20the%20learning%20process.%20Any%20change%20to%20the%0Aspecification%20requires%20retraining%20the%20policy%2C%20either%20via%20fine-tuning%20or%20from%0Ascratch.%20We%20present%20a%20more%20flexible%20approach%20--%20to%20learn%20a%20set%20of%20composable%0Atask%20primitive%20policies%20that%20can%20be%20used%20to%20satisfy%20arbitrary%20LTL%0Aspecifications%20without%20retraining%20or%20fine-tuning.%20Task%20primitives%20can%20be%0Alearned%20offline%20using%20RL%20and%20combined%20using%20Boolean%20composition%20at%20deployment.%0AThis%20work%20focuses%20on%20creating%20and%20pruning%20a%20transition%20system%20%28TS%29%0Arepresentation%20of%20the%20environment%20in%20order%20to%20solve%20for%20deterministic%2C%0Anon-ambiguous%2C%20and%20feasible%20solutions%20to%20LTL%20specifications%20given%20an%0Aenvironment%20and%20a%20set%20of%20task%20primitive%20policies.%20We%20show%20that%20our%20pruned%20TS%20is%0Adeterministic%2C%20contains%20no%20unrealizable%20transitions%2C%20and%20is%20sound.%20We%20verify%0Aour%20approach%20via%20simulation%20and%20compare%20it%20to%20other%20state%20of%20the%20art%0Aapproaches%2C%20showing%20that%20Comp-LTL%20is%20safer%20and%20more%20adaptable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComp-LTL%253A%2520Temporal%2520Logic%2520Planning%2520via%2520Zero-Shot%2520Policy%2520Composition%26entry.906535625%3DTaylor%2520Bergeron%2520and%2520Zachary%2520Serlin%2520and%2520Kevin%2520Leahy%26entry.1292438233%3D%2520%2520This%2520work%2520develops%2520a%2520zero-shot%2520mechanism%252C%2520Comp-LTL%252C%2520for%2520an%2520agent%2520to%2520satisfy%2520a%250ALinear%2520Temporal%2520Logic%2520%2528LTL%2529%2520specification%2520given%2520existing%2520task%2520primitives%250Atrained%2520via%2520reinforcement%2520learning%2520%2528RL%2529.%2520Autonomous%2520robots%2520often%2520need%2520to%250Asatisfy%2520spatial%2520and%2520temporal%2520goals%2520that%2520are%2520unknown%2520until%2520run%2520time.%2520Prior%2520work%250Afocuses%2520on%2520learning%2520policies%2520for%2520executing%2520a%2520task%2520specified%2520using%2520LTL%252C%2520but%2520they%250Aincorporate%2520the%2520specification%2520into%2520the%2520learning%2520process.%2520Any%2520change%2520to%2520the%250Aspecification%2520requires%2520retraining%2520the%2520policy%252C%2520either%2520via%2520fine-tuning%2520or%2520from%250Ascratch.%2520We%2520present%2520a%2520more%2520flexible%2520approach%2520--%2520to%2520learn%2520a%2520set%2520of%2520composable%250Atask%2520primitive%2520policies%2520that%2520can%2520be%2520used%2520to%2520satisfy%2520arbitrary%2520LTL%250Aspecifications%2520without%2520retraining%2520or%2520fine-tuning.%2520Task%2520primitives%2520can%2520be%250Alearned%2520offline%2520using%2520RL%2520and%2520combined%2520using%2520Boolean%2520composition%2520at%2520deployment.%250AThis%2520work%2520focuses%2520on%2520creating%2520and%2520pruning%2520a%2520transition%2520system%2520%2528TS%2529%250Arepresentation%2520of%2520the%2520environment%2520in%2520order%2520to%2520solve%2520for%2520deterministic%252C%250Anon-ambiguous%252C%2520and%2520feasible%2520solutions%2520to%2520LTL%2520specifications%2520given%2520an%250Aenvironment%2520and%2520a%2520set%2520of%2520task%2520primitive%2520policies.%2520We%2520show%2520that%2520our%2520pruned%2520TS%2520is%250Adeterministic%252C%2520contains%2520no%2520unrealizable%2520transitions%252C%2520and%2520is%2520sound.%2520We%2520verify%250Aour%2520approach%2520via%2520simulation%2520and%2520compare%2520it%2520to%2520other%2520state%2520of%2520the%2520art%250Aapproaches%252C%2520showing%2520that%2520Comp-LTL%2520is%2520safer%2520and%2520more%2520adaptable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comp-LTL%3A%20Temporal%20Logic%20Planning%20via%20Zero-Shot%20Policy%20Composition&entry.906535625=Taylor%20Bergeron%20and%20Zachary%20Serlin%20and%20Kevin%20Leahy&entry.1292438233=%20%20This%20work%20develops%20a%20zero-shot%20mechanism%2C%20Comp-LTL%2C%20for%20an%20agent%20to%20satisfy%20a%0ALinear%20Temporal%20Logic%20%28LTL%29%20specification%20given%20existing%20task%20primitives%0Atrained%20via%20reinforcement%20learning%20%28RL%29.%20Autonomous%20robots%20often%20need%20to%0Asatisfy%20spatial%20and%20temporal%20goals%20that%20are%20unknown%20until%20run%20time.%20Prior%20work%0Afocuses%20on%20learning%20policies%20for%20executing%20a%20task%20specified%20using%20LTL%2C%20but%20they%0Aincorporate%20the%20specification%20into%20the%20learning%20process.%20Any%20change%20to%20the%0Aspecification%20requires%20retraining%20the%20policy%2C%20either%20via%20fine-tuning%20or%20from%0Ascratch.%20We%20present%20a%20more%20flexible%20approach%20--%20to%20learn%20a%20set%20of%20composable%0Atask%20primitive%20policies%20that%20can%20be%20used%20to%20satisfy%20arbitrary%20LTL%0Aspecifications%20without%20retraining%20or%20fine-tuning.%20Task%20primitives%20can%20be%0Alearned%20offline%20using%20RL%20and%20combined%20using%20Boolean%20composition%20at%20deployment.%0AThis%20work%20focuses%20on%20creating%20and%20pruning%20a%20transition%20system%20%28TS%29%0Arepresentation%20of%20the%20environment%20in%20order%20to%20solve%20for%20deterministic%2C%0Anon-ambiguous%2C%20and%20feasible%20solutions%20to%20LTL%20specifications%20given%20an%0Aenvironment%20and%20a%20set%20of%20task%20primitive%20policies.%20We%20show%20that%20our%20pruned%20TS%20is%0Adeterministic%2C%20contains%20no%20unrealizable%20transitions%2C%20and%20is%20sound.%20We%20verify%0Aour%20approach%20via%20simulation%20and%20compare%20it%20to%20other%20state%20of%20the%20art%0Aapproaches%2C%20showing%20that%20Comp-LTL%20is%20safer%20and%20more%20adaptable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04215v2&entry.124074799=Read"},
{"title": "BetaExplainer: A Probabilistic Method to Explain Graph Neural Networks", "author": "Whitney Sloneker and Shalin Patel and Michael Wang and Lorin Crawford and Ritambhara Singh", "abstract": "  Graph neural networks (GNNs) are powerful tools for conducting inference on\ngraph data but are often seen as \"black boxes\" due to difficulty in extracting\nmeaningful subnetworks driving predictive performance. Many interpretable GNN\nmethods exist, but they cannot quantify uncertainty in edge weights and suffer\nin predictive accuracy when applied to challenging graph structures. In this\nwork, we proposed BetaExplainer which addresses these issues by using a\nsparsity-inducing prior to mask unimportant edges during model training. To\nevaluate our approach, we examine various simulated data sets with diverse\nreal-world characteristics. Not only does this implementation provide a notion\nof edge importance uncertainty, it also improves upon evaluation metrics for\nchallenging datasets compared to state-of-the art explainer methods.\n", "link": "http://arxiv.org/abs/2412.11964v1", "date": "2024-12-16", "relevancy": 1.4842, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5176}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4742}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BetaExplainer%3A%20A%20Probabilistic%20Method%20to%20Explain%20Graph%20Neural%20Networks&body=Title%3A%20BetaExplainer%3A%20A%20Probabilistic%20Method%20to%20Explain%20Graph%20Neural%20Networks%0AAuthor%3A%20Whitney%20Sloneker%20and%20Shalin%20Patel%20and%20Michael%20Wang%20and%20Lorin%20Crawford%20and%20Ritambhara%20Singh%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20powerful%20tools%20for%20conducting%20inference%20on%0Agraph%20data%20but%20are%20often%20seen%20as%20%22black%20boxes%22%20due%20to%20difficulty%20in%20extracting%0Ameaningful%20subnetworks%20driving%20predictive%20performance.%20Many%20interpretable%20GNN%0Amethods%20exist%2C%20but%20they%20cannot%20quantify%20uncertainty%20in%20edge%20weights%20and%20suffer%0Ain%20predictive%20accuracy%20when%20applied%20to%20challenging%20graph%20structures.%20In%20this%0Awork%2C%20we%20proposed%20BetaExplainer%20which%20addresses%20these%20issues%20by%20using%20a%0Asparsity-inducing%20prior%20to%20mask%20unimportant%20edges%20during%20model%20training.%20To%0Aevaluate%20our%20approach%2C%20we%20examine%20various%20simulated%20data%20sets%20with%20diverse%0Areal-world%20characteristics.%20Not%20only%20does%20this%20implementation%20provide%20a%20notion%0Aof%20edge%20importance%20uncertainty%2C%20it%20also%20improves%20upon%20evaluation%20metrics%20for%0Achallenging%20datasets%20compared%20to%20state-of-the%20art%20explainer%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetaExplainer%253A%2520A%2520Probabilistic%2520Method%2520to%2520Explain%2520Graph%2520Neural%2520Networks%26entry.906535625%3DWhitney%2520Sloneker%2520and%2520Shalin%2520Patel%2520and%2520Michael%2520Wang%2520and%2520Lorin%2520Crawford%2520and%2520Ritambhara%2520Singh%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520powerful%2520tools%2520for%2520conducting%2520inference%2520on%250Agraph%2520data%2520but%2520are%2520often%2520seen%2520as%2520%2522black%2520boxes%2522%2520due%2520to%2520difficulty%2520in%2520extracting%250Ameaningful%2520subnetworks%2520driving%2520predictive%2520performance.%2520Many%2520interpretable%2520GNN%250Amethods%2520exist%252C%2520but%2520they%2520cannot%2520quantify%2520uncertainty%2520in%2520edge%2520weights%2520and%2520suffer%250Ain%2520predictive%2520accuracy%2520when%2520applied%2520to%2520challenging%2520graph%2520structures.%2520In%2520this%250Awork%252C%2520we%2520proposed%2520BetaExplainer%2520which%2520addresses%2520these%2520issues%2520by%2520using%2520a%250Asparsity-inducing%2520prior%2520to%2520mask%2520unimportant%2520edges%2520during%2520model%2520training.%2520To%250Aevaluate%2520our%2520approach%252C%2520we%2520examine%2520various%2520simulated%2520data%2520sets%2520with%2520diverse%250Areal-world%2520characteristics.%2520Not%2520only%2520does%2520this%2520implementation%2520provide%2520a%2520notion%250Aof%2520edge%2520importance%2520uncertainty%252C%2520it%2520also%2520improves%2520upon%2520evaluation%2520metrics%2520for%250Achallenging%2520datasets%2520compared%2520to%2520state-of-the%2520art%2520explainer%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BetaExplainer%3A%20A%20Probabilistic%20Method%20to%20Explain%20Graph%20Neural%20Networks&entry.906535625=Whitney%20Sloneker%20and%20Shalin%20Patel%20and%20Michael%20Wang%20and%20Lorin%20Crawford%20and%20Ritambhara%20Singh&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20powerful%20tools%20for%20conducting%20inference%20on%0Agraph%20data%20but%20are%20often%20seen%20as%20%22black%20boxes%22%20due%20to%20difficulty%20in%20extracting%0Ameaningful%20subnetworks%20driving%20predictive%20performance.%20Many%20interpretable%20GNN%0Amethods%20exist%2C%20but%20they%20cannot%20quantify%20uncertainty%20in%20edge%20weights%20and%20suffer%0Ain%20predictive%20accuracy%20when%20applied%20to%20challenging%20graph%20structures.%20In%20this%0Awork%2C%20we%20proposed%20BetaExplainer%20which%20addresses%20these%20issues%20by%20using%20a%0Asparsity-inducing%20prior%20to%20mask%20unimportant%20edges%20during%20model%20training.%20To%0Aevaluate%20our%20approach%2C%20we%20examine%20various%20simulated%20data%20sets%20with%20diverse%0Areal-world%20characteristics.%20Not%20only%20does%20this%20implementation%20provide%20a%20notion%0Aof%20edge%20importance%20uncertainty%2C%20it%20also%20improves%20upon%20evaluation%20metrics%20for%0Achallenging%20datasets%20compared%20to%20state-of-the%20art%20explainer%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11964v1&entry.124074799=Read"},
{"title": "Neural general circulation models optimized to predict satellite-based\n  precipitation observations", "author": "Janni Yuval and Ian Langmore and Dmitrii Kochkov and Stephan Hoyer", "abstract": "  Climate models struggle to accurately simulate precipitation, particularly\nextremes and the diurnal cycle. Here, we present a hybrid model that is trained\ndirectly on satellite-based precipitation observations. Our model runs at\n2.8$^\\circ$ resolution and is built on the differentiable NeuralGCM framework.\nThe model demonstrates significant improvements over existing general\ncirculation models, the ERA5 reanalysis, and a global cloud-resolving model in\nsimulating precipitation. Our approach yields reduced biases, a more realistic\nprecipitation distribution, improved representation of extremes, and a more\naccurate diurnal cycle. Furthermore, it outperforms the mid-range precipitation\nforecast of the ECMWF ensemble. This advance paves the way for more reliable\nsimulations of current climate and demonstrates how training on observations\ncan be used to directly improve GCMs.\n", "link": "http://arxiv.org/abs/2412.11973v1", "date": "2024-12-16", "relevancy": 1.4559, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5141}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.48}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20general%20circulation%20models%20optimized%20to%20predict%20satellite-based%0A%20%20precipitation%20observations&body=Title%3A%20Neural%20general%20circulation%20models%20optimized%20to%20predict%20satellite-based%0A%20%20precipitation%20observations%0AAuthor%3A%20Janni%20Yuval%20and%20Ian%20Langmore%20and%20Dmitrii%20Kochkov%20and%20Stephan%20Hoyer%0AAbstract%3A%20%20%20Climate%20models%20struggle%20to%20accurately%20simulate%20precipitation%2C%20particularly%0Aextremes%20and%20the%20diurnal%20cycle.%20Here%2C%20we%20present%20a%20hybrid%20model%20that%20is%20trained%0Adirectly%20on%20satellite-based%20precipitation%20observations.%20Our%20model%20runs%20at%0A2.8%24%5E%5Ccirc%24%20resolution%20and%20is%20built%20on%20the%20differentiable%20NeuralGCM%20framework.%0AThe%20model%20demonstrates%20significant%20improvements%20over%20existing%20general%0Acirculation%20models%2C%20the%20ERA5%20reanalysis%2C%20and%20a%20global%20cloud-resolving%20model%20in%0Asimulating%20precipitation.%20Our%20approach%20yields%20reduced%20biases%2C%20a%20more%20realistic%0Aprecipitation%20distribution%2C%20improved%20representation%20of%20extremes%2C%20and%20a%20more%0Aaccurate%20diurnal%20cycle.%20Furthermore%2C%20it%20outperforms%20the%20mid-range%20precipitation%0Aforecast%20of%20the%20ECMWF%20ensemble.%20This%20advance%20paves%20the%20way%20for%20more%20reliable%0Asimulations%20of%20current%20climate%20and%20demonstrates%20how%20training%20on%20observations%0Acan%20be%20used%20to%20directly%20improve%20GCMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520general%2520circulation%2520models%2520optimized%2520to%2520predict%2520satellite-based%250A%2520%2520precipitation%2520observations%26entry.906535625%3DJanni%2520Yuval%2520and%2520Ian%2520Langmore%2520and%2520Dmitrii%2520Kochkov%2520and%2520Stephan%2520Hoyer%26entry.1292438233%3D%2520%2520Climate%2520models%2520struggle%2520to%2520accurately%2520simulate%2520precipitation%252C%2520particularly%250Aextremes%2520and%2520the%2520diurnal%2520cycle.%2520Here%252C%2520we%2520present%2520a%2520hybrid%2520model%2520that%2520is%2520trained%250Adirectly%2520on%2520satellite-based%2520precipitation%2520observations.%2520Our%2520model%2520runs%2520at%250A2.8%2524%255E%255Ccirc%2524%2520resolution%2520and%2520is%2520built%2520on%2520the%2520differentiable%2520NeuralGCM%2520framework.%250AThe%2520model%2520demonstrates%2520significant%2520improvements%2520over%2520existing%2520general%250Acirculation%2520models%252C%2520the%2520ERA5%2520reanalysis%252C%2520and%2520a%2520global%2520cloud-resolving%2520model%2520in%250Asimulating%2520precipitation.%2520Our%2520approach%2520yields%2520reduced%2520biases%252C%2520a%2520more%2520realistic%250Aprecipitation%2520distribution%252C%2520improved%2520representation%2520of%2520extremes%252C%2520and%2520a%2520more%250Aaccurate%2520diurnal%2520cycle.%2520Furthermore%252C%2520it%2520outperforms%2520the%2520mid-range%2520precipitation%250Aforecast%2520of%2520the%2520ECMWF%2520ensemble.%2520This%2520advance%2520paves%2520the%2520way%2520for%2520more%2520reliable%250Asimulations%2520of%2520current%2520climate%2520and%2520demonstrates%2520how%2520training%2520on%2520observations%250Acan%2520be%2520used%2520to%2520directly%2520improve%2520GCMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20general%20circulation%20models%20optimized%20to%20predict%20satellite-based%0A%20%20precipitation%20observations&entry.906535625=Janni%20Yuval%20and%20Ian%20Langmore%20and%20Dmitrii%20Kochkov%20and%20Stephan%20Hoyer&entry.1292438233=%20%20Climate%20models%20struggle%20to%20accurately%20simulate%20precipitation%2C%20particularly%0Aextremes%20and%20the%20diurnal%20cycle.%20Here%2C%20we%20present%20a%20hybrid%20model%20that%20is%20trained%0Adirectly%20on%20satellite-based%20precipitation%20observations.%20Our%20model%20runs%20at%0A2.8%24%5E%5Ccirc%24%20resolution%20and%20is%20built%20on%20the%20differentiable%20NeuralGCM%20framework.%0AThe%20model%20demonstrates%20significant%20improvements%20over%20existing%20general%0Acirculation%20models%2C%20the%20ERA5%20reanalysis%2C%20and%20a%20global%20cloud-resolving%20model%20in%0Asimulating%20precipitation.%20Our%20approach%20yields%20reduced%20biases%2C%20a%20more%20realistic%0Aprecipitation%20distribution%2C%20improved%20representation%20of%20extremes%2C%20and%20a%20more%0Aaccurate%20diurnal%20cycle.%20Furthermore%2C%20it%20outperforms%20the%20mid-range%20precipitation%0Aforecast%20of%20the%20ECMWF%20ensemble.%20This%20advance%20paves%20the%20way%20for%20more%20reliable%0Asimulations%20of%20current%20climate%20and%20demonstrates%20how%20training%20on%20observations%0Acan%20be%20used%20to%20directly%20improve%20GCMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11973v1&entry.124074799=Read"},
{"title": "MaxInfoRL: Boosting exploration in reinforcement learning through\n  information gain maximization", "author": "Bhavya Sukhija and Stelian Coros and Andreas Krause and Pieter Abbeel and Carmelo Sferrazza", "abstract": "  Reinforcement learning (RL) algorithms aim to balance exploiting the current\nbest strategy with exploring new options that could lead to higher rewards.\nMost common RL algorithms use undirected exploration, i.e., select random\nsequences of actions. Exploration can also be directed using intrinsic rewards,\nsuch as curiosity or model epistemic uncertainty. However, effectively\nbalancing task and intrinsic rewards is challenging and often task-dependent.\nIn this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and\nextrinsic exploration. MaxInfoRL steers exploration towards informative\ntransitions, by maximizing intrinsic rewards such as the information gain about\nthe underlying task. When combined with Boltzmann exploration, this approach\nnaturally trades off maximization of the value function with that of the\nentropy over states, rewards, and actions. We show that our approach achieves\nsublinear regret in the simplified setting of multi-armed bandits. We then\napply this general formulation to a variety of off-policy model-free RL methods\nfor continuous state-action spaces, yielding novel algorithms that achieve\nsuperior performance across hard exploration problems and complex scenarios\nsuch as visual control tasks.\n", "link": "http://arxiv.org/abs/2412.12098v1", "date": "2024-12-16", "relevancy": 1.4548, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5394}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaxInfoRL%3A%20Boosting%20exploration%20in%20reinforcement%20learning%20through%0A%20%20information%20gain%20maximization&body=Title%3A%20MaxInfoRL%3A%20Boosting%20exploration%20in%20reinforcement%20learning%20through%0A%20%20information%20gain%20maximization%0AAuthor%3A%20Bhavya%20Sukhija%20and%20Stelian%20Coros%20and%20Andreas%20Krause%20and%20Pieter%20Abbeel%20and%20Carmelo%20Sferrazza%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20algorithms%20aim%20to%20balance%20exploiting%20the%20current%0Abest%20strategy%20with%20exploring%20new%20options%20that%20could%20lead%20to%20higher%20rewards.%0AMost%20common%20RL%20algorithms%20use%20undirected%20exploration%2C%20i.e.%2C%20select%20random%0Asequences%20of%20actions.%20Exploration%20can%20also%20be%20directed%20using%20intrinsic%20rewards%2C%0Asuch%20as%20curiosity%20or%20model%20epistemic%20uncertainty.%20However%2C%20effectively%0Abalancing%20task%20and%20intrinsic%20rewards%20is%20challenging%20and%20often%20task-dependent.%0AIn%20this%20work%2C%20we%20introduce%20a%20framework%2C%20MaxInfoRL%2C%20for%20balancing%20intrinsic%20and%0Aextrinsic%20exploration.%20MaxInfoRL%20steers%20exploration%20towards%20informative%0Atransitions%2C%20by%20maximizing%20intrinsic%20rewards%20such%20as%20the%20information%20gain%20about%0Athe%20underlying%20task.%20When%20combined%20with%20Boltzmann%20exploration%2C%20this%20approach%0Anaturally%20trades%20off%20maximization%20of%20the%20value%20function%20with%20that%20of%20the%0Aentropy%20over%20states%2C%20rewards%2C%20and%20actions.%20We%20show%20that%20our%20approach%20achieves%0Asublinear%20regret%20in%20the%20simplified%20setting%20of%20multi-armed%20bandits.%20We%20then%0Aapply%20this%20general%20formulation%20to%20a%20variety%20of%20off-policy%20model-free%20RL%20methods%0Afor%20continuous%20state-action%20spaces%2C%20yielding%20novel%20algorithms%20that%20achieve%0Asuperior%20performance%20across%20hard%20exploration%20problems%20and%20complex%20scenarios%0Asuch%20as%20visual%20control%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaxInfoRL%253A%2520Boosting%2520exploration%2520in%2520reinforcement%2520learning%2520through%250A%2520%2520information%2520gain%2520maximization%26entry.906535625%3DBhavya%2520Sukhija%2520and%2520Stelian%2520Coros%2520and%2520Andreas%2520Krause%2520and%2520Pieter%2520Abbeel%2520and%2520Carmelo%2520Sferrazza%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520algorithms%2520aim%2520to%2520balance%2520exploiting%2520the%2520current%250Abest%2520strategy%2520with%2520exploring%2520new%2520options%2520that%2520could%2520lead%2520to%2520higher%2520rewards.%250AMost%2520common%2520RL%2520algorithms%2520use%2520undirected%2520exploration%252C%2520i.e.%252C%2520select%2520random%250Asequences%2520of%2520actions.%2520Exploration%2520can%2520also%2520be%2520directed%2520using%2520intrinsic%2520rewards%252C%250Asuch%2520as%2520curiosity%2520or%2520model%2520epistemic%2520uncertainty.%2520However%252C%2520effectively%250Abalancing%2520task%2520and%2520intrinsic%2520rewards%2520is%2520challenging%2520and%2520often%2520task-dependent.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520framework%252C%2520MaxInfoRL%252C%2520for%2520balancing%2520intrinsic%2520and%250Aextrinsic%2520exploration.%2520MaxInfoRL%2520steers%2520exploration%2520towards%2520informative%250Atransitions%252C%2520by%2520maximizing%2520intrinsic%2520rewards%2520such%2520as%2520the%2520information%2520gain%2520about%250Athe%2520underlying%2520task.%2520When%2520combined%2520with%2520Boltzmann%2520exploration%252C%2520this%2520approach%250Anaturally%2520trades%2520off%2520maximization%2520of%2520the%2520value%2520function%2520with%2520that%2520of%2520the%250Aentropy%2520over%2520states%252C%2520rewards%252C%2520and%2520actions.%2520We%2520show%2520that%2520our%2520approach%2520achieves%250Asublinear%2520regret%2520in%2520the%2520simplified%2520setting%2520of%2520multi-armed%2520bandits.%2520We%2520then%250Aapply%2520this%2520general%2520formulation%2520to%2520a%2520variety%2520of%2520off-policy%2520model-free%2520RL%2520methods%250Afor%2520continuous%2520state-action%2520spaces%252C%2520yielding%2520novel%2520algorithms%2520that%2520achieve%250Asuperior%2520performance%2520across%2520hard%2520exploration%2520problems%2520and%2520complex%2520scenarios%250Asuch%2520as%2520visual%2520control%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaxInfoRL%3A%20Boosting%20exploration%20in%20reinforcement%20learning%20through%0A%20%20information%20gain%20maximization&entry.906535625=Bhavya%20Sukhija%20and%20Stelian%20Coros%20and%20Andreas%20Krause%20and%20Pieter%20Abbeel%20and%20Carmelo%20Sferrazza&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20algorithms%20aim%20to%20balance%20exploiting%20the%20current%0Abest%20strategy%20with%20exploring%20new%20options%20that%20could%20lead%20to%20higher%20rewards.%0AMost%20common%20RL%20algorithms%20use%20undirected%20exploration%2C%20i.e.%2C%20select%20random%0Asequences%20of%20actions.%20Exploration%20can%20also%20be%20directed%20using%20intrinsic%20rewards%2C%0Asuch%20as%20curiosity%20or%20model%20epistemic%20uncertainty.%20However%2C%20effectively%0Abalancing%20task%20and%20intrinsic%20rewards%20is%20challenging%20and%20often%20task-dependent.%0AIn%20this%20work%2C%20we%20introduce%20a%20framework%2C%20MaxInfoRL%2C%20for%20balancing%20intrinsic%20and%0Aextrinsic%20exploration.%20MaxInfoRL%20steers%20exploration%20towards%20informative%0Atransitions%2C%20by%20maximizing%20intrinsic%20rewards%20such%20as%20the%20information%20gain%20about%0Athe%20underlying%20task.%20When%20combined%20with%20Boltzmann%20exploration%2C%20this%20approach%0Anaturally%20trades%20off%20maximization%20of%20the%20value%20function%20with%20that%20of%20the%0Aentropy%20over%20states%2C%20rewards%2C%20and%20actions.%20We%20show%20that%20our%20approach%20achieves%0Asublinear%20regret%20in%20the%20simplified%20setting%20of%20multi-armed%20bandits.%20We%20then%0Aapply%20this%20general%20formulation%20to%20a%20variety%20of%20off-policy%20model-free%20RL%20methods%0Afor%20continuous%20state-action%20spaces%2C%20yielding%20novel%20algorithms%20that%20achieve%0Asuperior%20performance%20across%20hard%20exploration%20problems%20and%20complex%20scenarios%0Asuch%20as%20visual%20control%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12098v1&entry.124074799=Read"},
{"title": "Combining Large Language Models with Tutoring System Intelligence: A\n  Case Study in Caregiver Homework Support", "author": "Devika Venugopalan and Ziwen Yan and Conrad Borchers and Jionghao Lin and Vincent Aleven", "abstract": "  Caregivers (i.e., parents and members of a child's caring community) are\nunderappreciated stakeholders in learning analytics. Although caregiver\ninvolvement can enhance student academic outcomes, many obstacles hinder\ninvolvement, most notably knowledge gaps with respect to modern school\ncurricula. An emerging topic of interest in learning analytics is hybrid\ntutoring, which includes instructional and motivational support. Caregivers\nassert similar roles in homework, yet it is unknown how learning analytics can\nsupport them. Our past work with caregivers suggested that conversational\nsupport is a promising method of providing caregivers with the guidance needed\nto effectively support student learning. We developed a system that provides\ninstructional support to caregivers through conversational recommendations\ngenerated by a Large Language Model (LLM). Addressing known instructional\nlimitations of LLMs, we use instructional intelligence from tutoring systems\nwhile conducting prompt engineering experiments with the open-source Llama 3\nLLM. This LLM generated message recommendations for caregivers supporting their\nchild's math practice via chat. Few-shot prompting and combining real-time\nproblem-solving context from tutoring systems with examples of tutoring\npractices yielded desirable message recommendations. These recommendations were\nevaluated with ten middle school caregivers, who valued recommendations\nfacilitating content-level support and student metacognition through\nself-explanation. We contribute insights into how tutoring systems can best be\nmerged with LLMs to support hybrid tutoring settings through conversational\nassistance, facilitating effective caregiver involvement in tutoring systems.\n", "link": "http://arxiv.org/abs/2412.11995v1", "date": "2024-12-16", "relevancy": 1.4414, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Large%20Language%20Models%20with%20Tutoring%20System%20Intelligence%3A%20A%0A%20%20Case%20Study%20in%20Caregiver%20Homework%20Support&body=Title%3A%20Combining%20Large%20Language%20Models%20with%20Tutoring%20System%20Intelligence%3A%20A%0A%20%20Case%20Study%20in%20Caregiver%20Homework%20Support%0AAuthor%3A%20Devika%20Venugopalan%20and%20Ziwen%20Yan%20and%20Conrad%20Borchers%20and%20Jionghao%20Lin%20and%20Vincent%20Aleven%0AAbstract%3A%20%20%20Caregivers%20%28i.e.%2C%20parents%20and%20members%20of%20a%20child%27s%20caring%20community%29%20are%0Aunderappreciated%20stakeholders%20in%20learning%20analytics.%20Although%20caregiver%0Ainvolvement%20can%20enhance%20student%20academic%20outcomes%2C%20many%20obstacles%20hinder%0Ainvolvement%2C%20most%20notably%20knowledge%20gaps%20with%20respect%20to%20modern%20school%0Acurricula.%20An%20emerging%20topic%20of%20interest%20in%20learning%20analytics%20is%20hybrid%0Atutoring%2C%20which%20includes%20instructional%20and%20motivational%20support.%20Caregivers%0Aassert%20similar%20roles%20in%20homework%2C%20yet%20it%20is%20unknown%20how%20learning%20analytics%20can%0Asupport%20them.%20Our%20past%20work%20with%20caregivers%20suggested%20that%20conversational%0Asupport%20is%20a%20promising%20method%20of%20providing%20caregivers%20with%20the%20guidance%20needed%0Ato%20effectively%20support%20student%20learning.%20We%20developed%20a%20system%20that%20provides%0Ainstructional%20support%20to%20caregivers%20through%20conversational%20recommendations%0Agenerated%20by%20a%20Large%20Language%20Model%20%28LLM%29.%20Addressing%20known%20instructional%0Alimitations%20of%20LLMs%2C%20we%20use%20instructional%20intelligence%20from%20tutoring%20systems%0Awhile%20conducting%20prompt%20engineering%20experiments%20with%20the%20open-source%20Llama%203%0ALLM.%20This%20LLM%20generated%20message%20recommendations%20for%20caregivers%20supporting%20their%0Achild%27s%20math%20practice%20via%20chat.%20Few-shot%20prompting%20and%20combining%20real-time%0Aproblem-solving%20context%20from%20tutoring%20systems%20with%20examples%20of%20tutoring%0Apractices%20yielded%20desirable%20message%20recommendations.%20These%20recommendations%20were%0Aevaluated%20with%20ten%20middle%20school%20caregivers%2C%20who%20valued%20recommendations%0Afacilitating%20content-level%20support%20and%20student%20metacognition%20through%0Aself-explanation.%20We%20contribute%20insights%20into%20how%20tutoring%20systems%20can%20best%20be%0Amerged%20with%20LLMs%20to%20support%20hybrid%20tutoring%20settings%20through%20conversational%0Aassistance%2C%20facilitating%20effective%20caregiver%20involvement%20in%20tutoring%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Large%2520Language%2520Models%2520with%2520Tutoring%2520System%2520Intelligence%253A%2520A%250A%2520%2520Case%2520Study%2520in%2520Caregiver%2520Homework%2520Support%26entry.906535625%3DDevika%2520Venugopalan%2520and%2520Ziwen%2520Yan%2520and%2520Conrad%2520Borchers%2520and%2520Jionghao%2520Lin%2520and%2520Vincent%2520Aleven%26entry.1292438233%3D%2520%2520Caregivers%2520%2528i.e.%252C%2520parents%2520and%2520members%2520of%2520a%2520child%2527s%2520caring%2520community%2529%2520are%250Aunderappreciated%2520stakeholders%2520in%2520learning%2520analytics.%2520Although%2520caregiver%250Ainvolvement%2520can%2520enhance%2520student%2520academic%2520outcomes%252C%2520many%2520obstacles%2520hinder%250Ainvolvement%252C%2520most%2520notably%2520knowledge%2520gaps%2520with%2520respect%2520to%2520modern%2520school%250Acurricula.%2520An%2520emerging%2520topic%2520of%2520interest%2520in%2520learning%2520analytics%2520is%2520hybrid%250Atutoring%252C%2520which%2520includes%2520instructional%2520and%2520motivational%2520support.%2520Caregivers%250Aassert%2520similar%2520roles%2520in%2520homework%252C%2520yet%2520it%2520is%2520unknown%2520how%2520learning%2520analytics%2520can%250Asupport%2520them.%2520Our%2520past%2520work%2520with%2520caregivers%2520suggested%2520that%2520conversational%250Asupport%2520is%2520a%2520promising%2520method%2520of%2520providing%2520caregivers%2520with%2520the%2520guidance%2520needed%250Ato%2520effectively%2520support%2520student%2520learning.%2520We%2520developed%2520a%2520system%2520that%2520provides%250Ainstructional%2520support%2520to%2520caregivers%2520through%2520conversational%2520recommendations%250Agenerated%2520by%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529.%2520Addressing%2520known%2520instructional%250Alimitations%2520of%2520LLMs%252C%2520we%2520use%2520instructional%2520intelligence%2520from%2520tutoring%2520systems%250Awhile%2520conducting%2520prompt%2520engineering%2520experiments%2520with%2520the%2520open-source%2520Llama%25203%250ALLM.%2520This%2520LLM%2520generated%2520message%2520recommendations%2520for%2520caregivers%2520supporting%2520their%250Achild%2527s%2520math%2520practice%2520via%2520chat.%2520Few-shot%2520prompting%2520and%2520combining%2520real-time%250Aproblem-solving%2520context%2520from%2520tutoring%2520systems%2520with%2520examples%2520of%2520tutoring%250Apractices%2520yielded%2520desirable%2520message%2520recommendations.%2520These%2520recommendations%2520were%250Aevaluated%2520with%2520ten%2520middle%2520school%2520caregivers%252C%2520who%2520valued%2520recommendations%250Afacilitating%2520content-level%2520support%2520and%2520student%2520metacognition%2520through%250Aself-explanation.%2520We%2520contribute%2520insights%2520into%2520how%2520tutoring%2520systems%2520can%2520best%2520be%250Amerged%2520with%2520LLMs%2520to%2520support%2520hybrid%2520tutoring%2520settings%2520through%2520conversational%250Aassistance%252C%2520facilitating%2520effective%2520caregiver%2520involvement%2520in%2520tutoring%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Large%20Language%20Models%20with%20Tutoring%20System%20Intelligence%3A%20A%0A%20%20Case%20Study%20in%20Caregiver%20Homework%20Support&entry.906535625=Devika%20Venugopalan%20and%20Ziwen%20Yan%20and%20Conrad%20Borchers%20and%20Jionghao%20Lin%20and%20Vincent%20Aleven&entry.1292438233=%20%20Caregivers%20%28i.e.%2C%20parents%20and%20members%20of%20a%20child%27s%20caring%20community%29%20are%0Aunderappreciated%20stakeholders%20in%20learning%20analytics.%20Although%20caregiver%0Ainvolvement%20can%20enhance%20student%20academic%20outcomes%2C%20many%20obstacles%20hinder%0Ainvolvement%2C%20most%20notably%20knowledge%20gaps%20with%20respect%20to%20modern%20school%0Acurricula.%20An%20emerging%20topic%20of%20interest%20in%20learning%20analytics%20is%20hybrid%0Atutoring%2C%20which%20includes%20instructional%20and%20motivational%20support.%20Caregivers%0Aassert%20similar%20roles%20in%20homework%2C%20yet%20it%20is%20unknown%20how%20learning%20analytics%20can%0Asupport%20them.%20Our%20past%20work%20with%20caregivers%20suggested%20that%20conversational%0Asupport%20is%20a%20promising%20method%20of%20providing%20caregivers%20with%20the%20guidance%20needed%0Ato%20effectively%20support%20student%20learning.%20We%20developed%20a%20system%20that%20provides%0Ainstructional%20support%20to%20caregivers%20through%20conversational%20recommendations%0Agenerated%20by%20a%20Large%20Language%20Model%20%28LLM%29.%20Addressing%20known%20instructional%0Alimitations%20of%20LLMs%2C%20we%20use%20instructional%20intelligence%20from%20tutoring%20systems%0Awhile%20conducting%20prompt%20engineering%20experiments%20with%20the%20open-source%20Llama%203%0ALLM.%20This%20LLM%20generated%20message%20recommendations%20for%20caregivers%20supporting%20their%0Achild%27s%20math%20practice%20via%20chat.%20Few-shot%20prompting%20and%20combining%20real-time%0Aproblem-solving%20context%20from%20tutoring%20systems%20with%20examples%20of%20tutoring%0Apractices%20yielded%20desirable%20message%20recommendations.%20These%20recommendations%20were%0Aevaluated%20with%20ten%20middle%20school%20caregivers%2C%20who%20valued%20recommendations%0Afacilitating%20content-level%20support%20and%20student%20metacognition%20through%0Aself-explanation.%20We%20contribute%20insights%20into%20how%20tutoring%20systems%20can%20best%20be%0Amerged%20with%20LLMs%20to%20support%20hybrid%20tutoring%20settings%20through%20conversational%0Aassistance%2C%20facilitating%20effective%20caregiver%20involvement%20in%20tutoring%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11995v1&entry.124074799=Read"},
{"title": "Backstepping Control of Tendon-Driven Continuum Robots in Large\n  Deflections Using the Cosserat Rod Model", "author": "Rana Danesh and Farrokh Janabi-Sharifi", "abstract": "  This paper presents a study on the backstepping control of tendon-driven\ncontinuum robots for large deflections using the Cosserat rod model. Continuum\nrobots are known for their flexibility and adaptability, making them suitable\nfor various applications. However, modeling and controlling them pose\nchallenges due to their nonlinear dynamics. To model their dynamics, the\nCosserat rod method is employed to account for significant deflections, and a\nnumerical solution method is developed to solve the resulting partial\ndifferential equations. Previous studies on controlling tendon-driven continuum\nrobots using Cosserat rod theory focused on sliding mode control and were not\ntested for large deflections, lacking experimental validation. In this paper,\nbackstepping control is proposed as an alternative to sliding mode control for\nachieving a significant bending. The numerical results are validated through\nexperiments in this study, demonstrating that the proposed backstepping control\napproach is a promising solution for achieving large deflections with smoother\ntrajectories, reduced settling time, and lower overshoot. Furthermore, two\nscenarios involving external forces and disturbances were introduced to further\nhighlight the robustness of the backstepping control approach.\n", "link": "http://arxiv.org/abs/2412.12035v1", "date": "2024-12-16", "relevancy": 1.3666, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4854}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4774}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backstepping%20Control%20of%20Tendon-Driven%20Continuum%20Robots%20in%20Large%0A%20%20Deflections%20Using%20the%20Cosserat%20Rod%20Model&body=Title%3A%20Backstepping%20Control%20of%20Tendon-Driven%20Continuum%20Robots%20in%20Large%0A%20%20Deflections%20Using%20the%20Cosserat%20Rod%20Model%0AAuthor%3A%20Rana%20Danesh%20and%20Farrokh%20Janabi-Sharifi%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20study%20on%20the%20backstepping%20control%20of%20tendon-driven%0Acontinuum%20robots%20for%20large%20deflections%20using%20the%20Cosserat%20rod%20model.%20Continuum%0Arobots%20are%20known%20for%20their%20flexibility%20and%20adaptability%2C%20making%20them%20suitable%0Afor%20various%20applications.%20However%2C%20modeling%20and%20controlling%20them%20pose%0Achallenges%20due%20to%20their%20nonlinear%20dynamics.%20To%20model%20their%20dynamics%2C%20the%0ACosserat%20rod%20method%20is%20employed%20to%20account%20for%20significant%20deflections%2C%20and%20a%0Anumerical%20solution%20method%20is%20developed%20to%20solve%20the%20resulting%20partial%0Adifferential%20equations.%20Previous%20studies%20on%20controlling%20tendon-driven%20continuum%0Arobots%20using%20Cosserat%20rod%20theory%20focused%20on%20sliding%20mode%20control%20and%20were%20not%0Atested%20for%20large%20deflections%2C%20lacking%20experimental%20validation.%20In%20this%20paper%2C%0Abackstepping%20control%20is%20proposed%20as%20an%20alternative%20to%20sliding%20mode%20control%20for%0Aachieving%20a%20significant%20bending.%20The%20numerical%20results%20are%20validated%20through%0Aexperiments%20in%20this%20study%2C%20demonstrating%20that%20the%20proposed%20backstepping%20control%0Aapproach%20is%20a%20promising%20solution%20for%20achieving%20large%20deflections%20with%20smoother%0Atrajectories%2C%20reduced%20settling%20time%2C%20and%20lower%20overshoot.%20Furthermore%2C%20two%0Ascenarios%20involving%20external%20forces%20and%20disturbances%20were%20introduced%20to%20further%0Ahighlight%20the%20robustness%20of%20the%20backstepping%20control%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackstepping%2520Control%2520of%2520Tendon-Driven%2520Continuum%2520Robots%2520in%2520Large%250A%2520%2520Deflections%2520Using%2520the%2520Cosserat%2520Rod%2520Model%26entry.906535625%3DRana%2520Danesh%2520and%2520Farrokh%2520Janabi-Sharifi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520study%2520on%2520the%2520backstepping%2520control%2520of%2520tendon-driven%250Acontinuum%2520robots%2520for%2520large%2520deflections%2520using%2520the%2520Cosserat%2520rod%2520model.%2520Continuum%250Arobots%2520are%2520known%2520for%2520their%2520flexibility%2520and%2520adaptability%252C%2520making%2520them%2520suitable%250Afor%2520various%2520applications.%2520However%252C%2520modeling%2520and%2520controlling%2520them%2520pose%250Achallenges%2520due%2520to%2520their%2520nonlinear%2520dynamics.%2520To%2520model%2520their%2520dynamics%252C%2520the%250ACosserat%2520rod%2520method%2520is%2520employed%2520to%2520account%2520for%2520significant%2520deflections%252C%2520and%2520a%250Anumerical%2520solution%2520method%2520is%2520developed%2520to%2520solve%2520the%2520resulting%2520partial%250Adifferential%2520equations.%2520Previous%2520studies%2520on%2520controlling%2520tendon-driven%2520continuum%250Arobots%2520using%2520Cosserat%2520rod%2520theory%2520focused%2520on%2520sliding%2520mode%2520control%2520and%2520were%2520not%250Atested%2520for%2520large%2520deflections%252C%2520lacking%2520experimental%2520validation.%2520In%2520this%2520paper%252C%250Abackstepping%2520control%2520is%2520proposed%2520as%2520an%2520alternative%2520to%2520sliding%2520mode%2520control%2520for%250Aachieving%2520a%2520significant%2520bending.%2520The%2520numerical%2520results%2520are%2520validated%2520through%250Aexperiments%2520in%2520this%2520study%252C%2520demonstrating%2520that%2520the%2520proposed%2520backstepping%2520control%250Aapproach%2520is%2520a%2520promising%2520solution%2520for%2520achieving%2520large%2520deflections%2520with%2520smoother%250Atrajectories%252C%2520reduced%2520settling%2520time%252C%2520and%2520lower%2520overshoot.%2520Furthermore%252C%2520two%250Ascenarios%2520involving%2520external%2520forces%2520and%2520disturbances%2520were%2520introduced%2520to%2520further%250Ahighlight%2520the%2520robustness%2520of%2520the%2520backstepping%2520control%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backstepping%20Control%20of%20Tendon-Driven%20Continuum%20Robots%20in%20Large%0A%20%20Deflections%20Using%20the%20Cosserat%20Rod%20Model&entry.906535625=Rana%20Danesh%20and%20Farrokh%20Janabi-Sharifi&entry.1292438233=%20%20This%20paper%20presents%20a%20study%20on%20the%20backstepping%20control%20of%20tendon-driven%0Acontinuum%20robots%20for%20large%20deflections%20using%20the%20Cosserat%20rod%20model.%20Continuum%0Arobots%20are%20known%20for%20their%20flexibility%20and%20adaptability%2C%20making%20them%20suitable%0Afor%20various%20applications.%20However%2C%20modeling%20and%20controlling%20them%20pose%0Achallenges%20due%20to%20their%20nonlinear%20dynamics.%20To%20model%20their%20dynamics%2C%20the%0ACosserat%20rod%20method%20is%20employed%20to%20account%20for%20significant%20deflections%2C%20and%20a%0Anumerical%20solution%20method%20is%20developed%20to%20solve%20the%20resulting%20partial%0Adifferential%20equations.%20Previous%20studies%20on%20controlling%20tendon-driven%20continuum%0Arobots%20using%20Cosserat%20rod%20theory%20focused%20on%20sliding%20mode%20control%20and%20were%20not%0Atested%20for%20large%20deflections%2C%20lacking%20experimental%20validation.%20In%20this%20paper%2C%0Abackstepping%20control%20is%20proposed%20as%20an%20alternative%20to%20sliding%20mode%20control%20for%0Aachieving%20a%20significant%20bending.%20The%20numerical%20results%20are%20validated%20through%0Aexperiments%20in%20this%20study%2C%20demonstrating%20that%20the%20proposed%20backstepping%20control%0Aapproach%20is%20a%20promising%20solution%20for%20achieving%20large%20deflections%20with%20smoother%0Atrajectories%2C%20reduced%20settling%20time%2C%20and%20lower%20overshoot.%20Furthermore%2C%20two%0Ascenarios%20involving%20external%20forces%20and%20disturbances%20were%20introduced%20to%20further%0Ahighlight%20the%20robustness%20of%20the%20backstepping%20control%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12035v1&entry.124074799=Read"},
{"title": "Fast and Interpretable Mortality Risk Scores for Critical Care Patients", "author": "Chloe Qinyu Zhu and Muhang Tian and Lesia Semenova and Jiachang Liu and Jack Xu and Joseph Scarpa and Cynthia Rudin", "abstract": "  Prediction of mortality in intensive care unit (ICU) patients typically\nrelies on black box models (that are unacceptable for use in hospitals) or\nhand-tuned interpretable models (that might lead to the loss in performance).\nWe aim to bridge the gap between these two categories by building on modern\ninterpretable ML techniques to design interpretable mortality risk scores that\nare as accurate as black boxes. We developed a new algorithm, GroupFasterRisk,\nwhich has several important benefits: it uses both hard and soft direct\nsparsity regularization, it incorporates group sparsity to allow more cohesive\nmodels, it allows for monotonicity constraint to include domain knowledge, and\nit produces many equally-good models, which allows domain experts to choose\namong them. For evaluation, we leveraged the largest existing public ICU\nmonitoring datasets (MIMIC III and eICU). Models produced by GroupFasterRisk\noutperformed OASIS and SAPS II scores and performed similarly to APACHE IV/IVa\nwhile using at most a third of the parameters. For patients with\nsepsis/septicemia, acute myocardial infarction, heart failure, and acute kidney\nfailure, GroupFasterRisk models outperformed OASIS and SOFA. Finally, different\nmortality prediction ML approaches performed better based on variables selected\nby GroupFasterRisk as compared to OASIS variables. GroupFasterRisk's models\nperformed better than risk scores currently used in hospitals, and on par with\nblack box ML models, while being orders of magnitude sparser. Because\nGroupFasterRisk produces a variety of risk scores, it allows design flexibility\n- the key enabler of practical model creation. GroupFasterRisk is a fast,\naccessible, and flexible procedure that allows learning a diverse set of sparse\nrisk scores for mortality prediction.\n", "link": "http://arxiv.org/abs/2311.13015v2", "date": "2024-12-16", "relevancy": 1.2493, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4318}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4207}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Interpretable%20Mortality%20Risk%20Scores%20for%20Critical%20Care%20Patients&body=Title%3A%20Fast%20and%20Interpretable%20Mortality%20Risk%20Scores%20for%20Critical%20Care%20Patients%0AAuthor%3A%20Chloe%20Qinyu%20Zhu%20and%20Muhang%20Tian%20and%20Lesia%20Semenova%20and%20Jiachang%20Liu%20and%20Jack%20Xu%20and%20Joseph%20Scarpa%20and%20Cynthia%20Rudin%0AAbstract%3A%20%20%20Prediction%20of%20mortality%20in%20intensive%20care%20unit%20%28ICU%29%20patients%20typically%0Arelies%20on%20black%20box%20models%20%28that%20are%20unacceptable%20for%20use%20in%20hospitals%29%20or%0Ahand-tuned%20interpretable%20models%20%28that%20might%20lead%20to%20the%20loss%20in%20performance%29.%0AWe%20aim%20to%20bridge%20the%20gap%20between%20these%20two%20categories%20by%20building%20on%20modern%0Ainterpretable%20ML%20techniques%20to%20design%20interpretable%20mortality%20risk%20scores%20that%0Aare%20as%20accurate%20as%20black%20boxes.%20We%20developed%20a%20new%20algorithm%2C%20GroupFasterRisk%2C%0Awhich%20has%20several%20important%20benefits%3A%20it%20uses%20both%20hard%20and%20soft%20direct%0Asparsity%20regularization%2C%20it%20incorporates%20group%20sparsity%20to%20allow%20more%20cohesive%0Amodels%2C%20it%20allows%20for%20monotonicity%20constraint%20to%20include%20domain%20knowledge%2C%20and%0Ait%20produces%20many%20equally-good%20models%2C%20which%20allows%20domain%20experts%20to%20choose%0Aamong%20them.%20For%20evaluation%2C%20we%20leveraged%20the%20largest%20existing%20public%20ICU%0Amonitoring%20datasets%20%28MIMIC%20III%20and%20eICU%29.%20Models%20produced%20by%20GroupFasterRisk%0Aoutperformed%20OASIS%20and%20SAPS%20II%20scores%20and%20performed%20similarly%20to%20APACHE%20IV/IVa%0Awhile%20using%20at%20most%20a%20third%20of%20the%20parameters.%20For%20patients%20with%0Asepsis/septicemia%2C%20acute%20myocardial%20infarction%2C%20heart%20failure%2C%20and%20acute%20kidney%0Afailure%2C%20GroupFasterRisk%20models%20outperformed%20OASIS%20and%20SOFA.%20Finally%2C%20different%0Amortality%20prediction%20ML%20approaches%20performed%20better%20based%20on%20variables%20selected%0Aby%20GroupFasterRisk%20as%20compared%20to%20OASIS%20variables.%20GroupFasterRisk%27s%20models%0Aperformed%20better%20than%20risk%20scores%20currently%20used%20in%20hospitals%2C%20and%20on%20par%20with%0Ablack%20box%20ML%20models%2C%20while%20being%20orders%20of%20magnitude%20sparser.%20Because%0AGroupFasterRisk%20produces%20a%20variety%20of%20risk%20scores%2C%20it%20allows%20design%20flexibility%0A-%20the%20key%20enabler%20of%20practical%20model%20creation.%20GroupFasterRisk%20is%20a%20fast%2C%0Aaccessible%2C%20and%20flexible%20procedure%20that%20allows%20learning%20a%20diverse%20set%20of%20sparse%0Arisk%20scores%20for%20mortality%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Interpretable%2520Mortality%2520Risk%2520Scores%2520for%2520Critical%2520Care%2520Patients%26entry.906535625%3DChloe%2520Qinyu%2520Zhu%2520and%2520Muhang%2520Tian%2520and%2520Lesia%2520Semenova%2520and%2520Jiachang%2520Liu%2520and%2520Jack%2520Xu%2520and%2520Joseph%2520Scarpa%2520and%2520Cynthia%2520Rudin%26entry.1292438233%3D%2520%2520Prediction%2520of%2520mortality%2520in%2520intensive%2520care%2520unit%2520%2528ICU%2529%2520patients%2520typically%250Arelies%2520on%2520black%2520box%2520models%2520%2528that%2520are%2520unacceptable%2520for%2520use%2520in%2520hospitals%2529%2520or%250Ahand-tuned%2520interpretable%2520models%2520%2528that%2520might%2520lead%2520to%2520the%2520loss%2520in%2520performance%2529.%250AWe%2520aim%2520to%2520bridge%2520the%2520gap%2520between%2520these%2520two%2520categories%2520by%2520building%2520on%2520modern%250Ainterpretable%2520ML%2520techniques%2520to%2520design%2520interpretable%2520mortality%2520risk%2520scores%2520that%250Aare%2520as%2520accurate%2520as%2520black%2520boxes.%2520We%2520developed%2520a%2520new%2520algorithm%252C%2520GroupFasterRisk%252C%250Awhich%2520has%2520several%2520important%2520benefits%253A%2520it%2520uses%2520both%2520hard%2520and%2520soft%2520direct%250Asparsity%2520regularization%252C%2520it%2520incorporates%2520group%2520sparsity%2520to%2520allow%2520more%2520cohesive%250Amodels%252C%2520it%2520allows%2520for%2520monotonicity%2520constraint%2520to%2520include%2520domain%2520knowledge%252C%2520and%250Ait%2520produces%2520many%2520equally-good%2520models%252C%2520which%2520allows%2520domain%2520experts%2520to%2520choose%250Aamong%2520them.%2520For%2520evaluation%252C%2520we%2520leveraged%2520the%2520largest%2520existing%2520public%2520ICU%250Amonitoring%2520datasets%2520%2528MIMIC%2520III%2520and%2520eICU%2529.%2520Models%2520produced%2520by%2520GroupFasterRisk%250Aoutperformed%2520OASIS%2520and%2520SAPS%2520II%2520scores%2520and%2520performed%2520similarly%2520to%2520APACHE%2520IV/IVa%250Awhile%2520using%2520at%2520most%2520a%2520third%2520of%2520the%2520parameters.%2520For%2520patients%2520with%250Asepsis/septicemia%252C%2520acute%2520myocardial%2520infarction%252C%2520heart%2520failure%252C%2520and%2520acute%2520kidney%250Afailure%252C%2520GroupFasterRisk%2520models%2520outperformed%2520OASIS%2520and%2520SOFA.%2520Finally%252C%2520different%250Amortality%2520prediction%2520ML%2520approaches%2520performed%2520better%2520based%2520on%2520variables%2520selected%250Aby%2520GroupFasterRisk%2520as%2520compared%2520to%2520OASIS%2520variables.%2520GroupFasterRisk%2527s%2520models%250Aperformed%2520better%2520than%2520risk%2520scores%2520currently%2520used%2520in%2520hospitals%252C%2520and%2520on%2520par%2520with%250Ablack%2520box%2520ML%2520models%252C%2520while%2520being%2520orders%2520of%2520magnitude%2520sparser.%2520Because%250AGroupFasterRisk%2520produces%2520a%2520variety%2520of%2520risk%2520scores%252C%2520it%2520allows%2520design%2520flexibility%250A-%2520the%2520key%2520enabler%2520of%2520practical%2520model%2520creation.%2520GroupFasterRisk%2520is%2520a%2520fast%252C%250Aaccessible%252C%2520and%2520flexible%2520procedure%2520that%2520allows%2520learning%2520a%2520diverse%2520set%2520of%2520sparse%250Arisk%2520scores%2520for%2520mortality%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Interpretable%20Mortality%20Risk%20Scores%20for%20Critical%20Care%20Patients&entry.906535625=Chloe%20Qinyu%20Zhu%20and%20Muhang%20Tian%20and%20Lesia%20Semenova%20and%20Jiachang%20Liu%20and%20Jack%20Xu%20and%20Joseph%20Scarpa%20and%20Cynthia%20Rudin&entry.1292438233=%20%20Prediction%20of%20mortality%20in%20intensive%20care%20unit%20%28ICU%29%20patients%20typically%0Arelies%20on%20black%20box%20models%20%28that%20are%20unacceptable%20for%20use%20in%20hospitals%29%20or%0Ahand-tuned%20interpretable%20models%20%28that%20might%20lead%20to%20the%20loss%20in%20performance%29.%0AWe%20aim%20to%20bridge%20the%20gap%20between%20these%20two%20categories%20by%20building%20on%20modern%0Ainterpretable%20ML%20techniques%20to%20design%20interpretable%20mortality%20risk%20scores%20that%0Aare%20as%20accurate%20as%20black%20boxes.%20We%20developed%20a%20new%20algorithm%2C%20GroupFasterRisk%2C%0Awhich%20has%20several%20important%20benefits%3A%20it%20uses%20both%20hard%20and%20soft%20direct%0Asparsity%20regularization%2C%20it%20incorporates%20group%20sparsity%20to%20allow%20more%20cohesive%0Amodels%2C%20it%20allows%20for%20monotonicity%20constraint%20to%20include%20domain%20knowledge%2C%20and%0Ait%20produces%20many%20equally-good%20models%2C%20which%20allows%20domain%20experts%20to%20choose%0Aamong%20them.%20For%20evaluation%2C%20we%20leveraged%20the%20largest%20existing%20public%20ICU%0Amonitoring%20datasets%20%28MIMIC%20III%20and%20eICU%29.%20Models%20produced%20by%20GroupFasterRisk%0Aoutperformed%20OASIS%20and%20SAPS%20II%20scores%20and%20performed%20similarly%20to%20APACHE%20IV/IVa%0Awhile%20using%20at%20most%20a%20third%20of%20the%20parameters.%20For%20patients%20with%0Asepsis/septicemia%2C%20acute%20myocardial%20infarction%2C%20heart%20failure%2C%20and%20acute%20kidney%0Afailure%2C%20GroupFasterRisk%20models%20outperformed%20OASIS%20and%20SOFA.%20Finally%2C%20different%0Amortality%20prediction%20ML%20approaches%20performed%20better%20based%20on%20variables%20selected%0Aby%20GroupFasterRisk%20as%20compared%20to%20OASIS%20variables.%20GroupFasterRisk%27s%20models%0Aperformed%20better%20than%20risk%20scores%20currently%20used%20in%20hospitals%2C%20and%20on%20par%20with%0Ablack%20box%20ML%20models%2C%20while%20being%20orders%20of%20magnitude%20sparser.%20Because%0AGroupFasterRisk%20produces%20a%20variety%20of%20risk%20scores%2C%20it%20allows%20design%20flexibility%0A-%20the%20key%20enabler%20of%20practical%20model%20creation.%20GroupFasterRisk%20is%20a%20fast%2C%0Aaccessible%2C%20and%20flexible%20procedure%20that%20allows%20learning%20a%20diverse%20set%20of%20sparse%0Arisk%20scores%20for%20mortality%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13015v2&entry.124074799=Read"},
{"title": "Industrial-scale Prediction of Cement Clinker Phases using Machine\n  Learning", "author": "Sheikh Junaid Fayaz and Nestor Montiel-Bohorquez and Shashank Bishnoi and Matteo Romano and Manuele Gatti and N. M. Anoop Krishnan", "abstract": "  Cement production, exceeding 4.1 billion tonnes and contributing 2.4 tonnes\nof CO2 annually, faces critical challenges in quality control and process\noptimization. While traditional process models for cement manufacturing are\nconfined to steady-state conditions with limited predictive capability for\nmineralogical phases, modern plants operate under dynamic conditions that\ndemand real-time quality assessment. Here, exploiting a comprehensive two-year\noperational dataset from an industrial cement plant, we present a machine\nlearning framework that accurately predicts clinker mineralogy from process\ndata. Our model achieves unprecedented prediction accuracy for major clinker\nphases while requiring minimal input parameters, demonstrating robust\nperformance under varying operating conditions. Through post-hoc explainable\nalgorithms, we interpret the hierarchical relationships between clinker oxides\nand phase formation, providing insights into the functioning of an otherwise\nblack-box model. This digital twin framework can potentially enable real-time\noptimization of cement production, thereby providing a route toward reducing\nmaterial waste and ensuring quality while reducing the associated emissions\nunder real plant conditions. Our approach represents a significant advancement\nin industrial process control, offering a scalable solution for sustainable\ncement manufacturing.\n", "link": "http://arxiv.org/abs/2412.11981v1", "date": "2024-12-16", "relevancy": 0.94, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5163}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4588}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Industrial-scale%20Prediction%20of%20Cement%20Clinker%20Phases%20using%20Machine%0A%20%20Learning&body=Title%3A%20Industrial-scale%20Prediction%20of%20Cement%20Clinker%20Phases%20using%20Machine%0A%20%20Learning%0AAuthor%3A%20Sheikh%20Junaid%20Fayaz%20and%20Nestor%20Montiel-Bohorquez%20and%20Shashank%20Bishnoi%20and%20Matteo%20Romano%20and%20Manuele%20Gatti%20and%20N.%20M.%20Anoop%20Krishnan%0AAbstract%3A%20%20%20Cement%20production%2C%20exceeding%204.1%20billion%20tonnes%20and%20contributing%202.4%20tonnes%0Aof%20CO2%20annually%2C%20faces%20critical%20challenges%20in%20quality%20control%20and%20process%0Aoptimization.%20While%20traditional%20process%20models%20for%20cement%20manufacturing%20are%0Aconfined%20to%20steady-state%20conditions%20with%20limited%20predictive%20capability%20for%0Amineralogical%20phases%2C%20modern%20plants%20operate%20under%20dynamic%20conditions%20that%0Ademand%20real-time%20quality%20assessment.%20Here%2C%20exploiting%20a%20comprehensive%20two-year%0Aoperational%20dataset%20from%20an%20industrial%20cement%20plant%2C%20we%20present%20a%20machine%0Alearning%20framework%20that%20accurately%20predicts%20clinker%20mineralogy%20from%20process%0Adata.%20Our%20model%20achieves%20unprecedented%20prediction%20accuracy%20for%20major%20clinker%0Aphases%20while%20requiring%20minimal%20input%20parameters%2C%20demonstrating%20robust%0Aperformance%20under%20varying%20operating%20conditions.%20Through%20post-hoc%20explainable%0Aalgorithms%2C%20we%20interpret%20the%20hierarchical%20relationships%20between%20clinker%20oxides%0Aand%20phase%20formation%2C%20providing%20insights%20into%20the%20functioning%20of%20an%20otherwise%0Ablack-box%20model.%20This%20digital%20twin%20framework%20can%20potentially%20enable%20real-time%0Aoptimization%20of%20cement%20production%2C%20thereby%20providing%20a%20route%20toward%20reducing%0Amaterial%20waste%20and%20ensuring%20quality%20while%20reducing%20the%20associated%20emissions%0Aunder%20real%20plant%20conditions.%20Our%20approach%20represents%20a%20significant%20advancement%0Ain%20industrial%20process%20control%2C%20offering%20a%20scalable%20solution%20for%20sustainable%0Acement%20manufacturing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndustrial-scale%2520Prediction%2520of%2520Cement%2520Clinker%2520Phases%2520using%2520Machine%250A%2520%2520Learning%26entry.906535625%3DSheikh%2520Junaid%2520Fayaz%2520and%2520Nestor%2520Montiel-Bohorquez%2520and%2520Shashank%2520Bishnoi%2520and%2520Matteo%2520Romano%2520and%2520Manuele%2520Gatti%2520and%2520N.%2520M.%2520Anoop%2520Krishnan%26entry.1292438233%3D%2520%2520Cement%2520production%252C%2520exceeding%25204.1%2520billion%2520tonnes%2520and%2520contributing%25202.4%2520tonnes%250Aof%2520CO2%2520annually%252C%2520faces%2520critical%2520challenges%2520in%2520quality%2520control%2520and%2520process%250Aoptimization.%2520While%2520traditional%2520process%2520models%2520for%2520cement%2520manufacturing%2520are%250Aconfined%2520to%2520steady-state%2520conditions%2520with%2520limited%2520predictive%2520capability%2520for%250Amineralogical%2520phases%252C%2520modern%2520plants%2520operate%2520under%2520dynamic%2520conditions%2520that%250Ademand%2520real-time%2520quality%2520assessment.%2520Here%252C%2520exploiting%2520a%2520comprehensive%2520two-year%250Aoperational%2520dataset%2520from%2520an%2520industrial%2520cement%2520plant%252C%2520we%2520present%2520a%2520machine%250Alearning%2520framework%2520that%2520accurately%2520predicts%2520clinker%2520mineralogy%2520from%2520process%250Adata.%2520Our%2520model%2520achieves%2520unprecedented%2520prediction%2520accuracy%2520for%2520major%2520clinker%250Aphases%2520while%2520requiring%2520minimal%2520input%2520parameters%252C%2520demonstrating%2520robust%250Aperformance%2520under%2520varying%2520operating%2520conditions.%2520Through%2520post-hoc%2520explainable%250Aalgorithms%252C%2520we%2520interpret%2520the%2520hierarchical%2520relationships%2520between%2520clinker%2520oxides%250Aand%2520phase%2520formation%252C%2520providing%2520insights%2520into%2520the%2520functioning%2520of%2520an%2520otherwise%250Ablack-box%2520model.%2520This%2520digital%2520twin%2520framework%2520can%2520potentially%2520enable%2520real-time%250Aoptimization%2520of%2520cement%2520production%252C%2520thereby%2520providing%2520a%2520route%2520toward%2520reducing%250Amaterial%2520waste%2520and%2520ensuring%2520quality%2520while%2520reducing%2520the%2520associated%2520emissions%250Aunder%2520real%2520plant%2520conditions.%2520Our%2520approach%2520represents%2520a%2520significant%2520advancement%250Ain%2520industrial%2520process%2520control%252C%2520offering%2520a%2520scalable%2520solution%2520for%2520sustainable%250Acement%2520manufacturing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Industrial-scale%20Prediction%20of%20Cement%20Clinker%20Phases%20using%20Machine%0A%20%20Learning&entry.906535625=Sheikh%20Junaid%20Fayaz%20and%20Nestor%20Montiel-Bohorquez%20and%20Shashank%20Bishnoi%20and%20Matteo%20Romano%20and%20Manuele%20Gatti%20and%20N.%20M.%20Anoop%20Krishnan&entry.1292438233=%20%20Cement%20production%2C%20exceeding%204.1%20billion%20tonnes%20and%20contributing%202.4%20tonnes%0Aof%20CO2%20annually%2C%20faces%20critical%20challenges%20in%20quality%20control%20and%20process%0Aoptimization.%20While%20traditional%20process%20models%20for%20cement%20manufacturing%20are%0Aconfined%20to%20steady-state%20conditions%20with%20limited%20predictive%20capability%20for%0Amineralogical%20phases%2C%20modern%20plants%20operate%20under%20dynamic%20conditions%20that%0Ademand%20real-time%20quality%20assessment.%20Here%2C%20exploiting%20a%20comprehensive%20two-year%0Aoperational%20dataset%20from%20an%20industrial%20cement%20plant%2C%20we%20present%20a%20machine%0Alearning%20framework%20that%20accurately%20predicts%20clinker%20mineralogy%20from%20process%0Adata.%20Our%20model%20achieves%20unprecedented%20prediction%20accuracy%20for%20major%20clinker%0Aphases%20while%20requiring%20minimal%20input%20parameters%2C%20demonstrating%20robust%0Aperformance%20under%20varying%20operating%20conditions.%20Through%20post-hoc%20explainable%0Aalgorithms%2C%20we%20interpret%20the%20hierarchical%20relationships%20between%20clinker%20oxides%0Aand%20phase%20formation%2C%20providing%20insights%20into%20the%20functioning%20of%20an%20otherwise%0Ablack-box%20model.%20This%20digital%20twin%20framework%20can%20potentially%20enable%20real-time%0Aoptimization%20of%20cement%20production%2C%20thereby%20providing%20a%20route%20toward%20reducing%0Amaterial%20waste%20and%20ensuring%20quality%20while%20reducing%20the%20associated%20emissions%0Aunder%20real%20plant%20conditions.%20Our%20approach%20represents%20a%20significant%20advancement%0Ain%20industrial%20process%20control%2C%20offering%20a%20scalable%20solution%20for%20sustainable%0Acement%20manufacturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11981v1&entry.124074799=Read"},
{"title": "Echo State network for coarsening dynamics of charge density waves", "author": "Clement Dinh and Yunhao Fan and Gia-Wei Chern", "abstract": "  An echo state network (ESN) is a type of reservoir computer that uses a\nrecurrent neural network with a sparsely connected hidden layer. Compared with\nother recurrent neural networks, one great advantage of ESN is the simplicity\nof its training process. Yet, despite the seemingly restricted learnable\nparameters, ESN has been shown to successfully capture the spatial-temporal\ndynamics of complex patterns. Here we build an ESN to model the coarsening\ndynamics of charge-density waves (CDW) in a semi-classical Holstein model,\nwhich exhibits a checkerboard electron density modulation at half-filling\nstabilized by a commensurate lattice distortion. The inputs to the ESN are\nlocal CDW order-parameters in a finite neighborhood centered around a given\nsite, while the output is the predicted CDW order of the center site at the\nnext time step. Special care is taken in the design of couplings between hidden\nlayer and input nodes to ensure lattice symmetries are properly incorporated\ninto the ESN model. Since the model predictions depend only on CDW\nconfigurations of a finite domain, the ESN is scalable and transferrable in the\nsense that a model trained on dataset from a small system can be directly\napplied to dynamical simulations on larger lattices. Our work opens a new\navenue for efficient dynamical modeling of pattern formations in functional\nelectron materials.\n", "link": "http://arxiv.org/abs/2412.11982v1", "date": "2024-12-16", "relevancy": 1.4027, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4903}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.449}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Echo%20State%20network%20for%20coarsening%20dynamics%20of%20charge%20density%20waves&body=Title%3A%20Echo%20State%20network%20for%20coarsening%20dynamics%20of%20charge%20density%20waves%0AAuthor%3A%20Clement%20Dinh%20and%20Yunhao%20Fan%20and%20Gia-Wei%20Chern%0AAbstract%3A%20%20%20An%20echo%20state%20network%20%28ESN%29%20is%20a%20type%20of%20reservoir%20computer%20that%20uses%20a%0Arecurrent%20neural%20network%20with%20a%20sparsely%20connected%20hidden%20layer.%20Compared%20with%0Aother%20recurrent%20neural%20networks%2C%20one%20great%20advantage%20of%20ESN%20is%20the%20simplicity%0Aof%20its%20training%20process.%20Yet%2C%20despite%20the%20seemingly%20restricted%20learnable%0Aparameters%2C%20ESN%20has%20been%20shown%20to%20successfully%20capture%20the%20spatial-temporal%0Adynamics%20of%20complex%20patterns.%20Here%20we%20build%20an%20ESN%20to%20model%20the%20coarsening%0Adynamics%20of%20charge-density%20waves%20%28CDW%29%20in%20a%20semi-classical%20Holstein%20model%2C%0Awhich%20exhibits%20a%20checkerboard%20electron%20density%20modulation%20at%20half-filling%0Astabilized%20by%20a%20commensurate%20lattice%20distortion.%20The%20inputs%20to%20the%20ESN%20are%0Alocal%20CDW%20order-parameters%20in%20a%20finite%20neighborhood%20centered%20around%20a%20given%0Asite%2C%20while%20the%20output%20is%20the%20predicted%20CDW%20order%20of%20the%20center%20site%20at%20the%0Anext%20time%20step.%20Special%20care%20is%20taken%20in%20the%20design%20of%20couplings%20between%20hidden%0Alayer%20and%20input%20nodes%20to%20ensure%20lattice%20symmetries%20are%20properly%20incorporated%0Ainto%20the%20ESN%20model.%20Since%20the%20model%20predictions%20depend%20only%20on%20CDW%0Aconfigurations%20of%20a%20finite%20domain%2C%20the%20ESN%20is%20scalable%20and%20transferrable%20in%20the%0Asense%20that%20a%20model%20trained%20on%20dataset%20from%20a%20small%20system%20can%20be%20directly%0Aapplied%20to%20dynamical%20simulations%20on%20larger%20lattices.%20Our%20work%20opens%20a%20new%0Aavenue%20for%20efficient%20dynamical%20modeling%20of%20pattern%20formations%20in%20functional%0Aelectron%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEcho%2520State%2520network%2520for%2520coarsening%2520dynamics%2520of%2520charge%2520density%2520waves%26entry.906535625%3DClement%2520Dinh%2520and%2520Yunhao%2520Fan%2520and%2520Gia-Wei%2520Chern%26entry.1292438233%3D%2520%2520An%2520echo%2520state%2520network%2520%2528ESN%2529%2520is%2520a%2520type%2520of%2520reservoir%2520computer%2520that%2520uses%2520a%250Arecurrent%2520neural%2520network%2520with%2520a%2520sparsely%2520connected%2520hidden%2520layer.%2520Compared%2520with%250Aother%2520recurrent%2520neural%2520networks%252C%2520one%2520great%2520advantage%2520of%2520ESN%2520is%2520the%2520simplicity%250Aof%2520its%2520training%2520process.%2520Yet%252C%2520despite%2520the%2520seemingly%2520restricted%2520learnable%250Aparameters%252C%2520ESN%2520has%2520been%2520shown%2520to%2520successfully%2520capture%2520the%2520spatial-temporal%250Adynamics%2520of%2520complex%2520patterns.%2520Here%2520we%2520build%2520an%2520ESN%2520to%2520model%2520the%2520coarsening%250Adynamics%2520of%2520charge-density%2520waves%2520%2528CDW%2529%2520in%2520a%2520semi-classical%2520Holstein%2520model%252C%250Awhich%2520exhibits%2520a%2520checkerboard%2520electron%2520density%2520modulation%2520at%2520half-filling%250Astabilized%2520by%2520a%2520commensurate%2520lattice%2520distortion.%2520The%2520inputs%2520to%2520the%2520ESN%2520are%250Alocal%2520CDW%2520order-parameters%2520in%2520a%2520finite%2520neighborhood%2520centered%2520around%2520a%2520given%250Asite%252C%2520while%2520the%2520output%2520is%2520the%2520predicted%2520CDW%2520order%2520of%2520the%2520center%2520site%2520at%2520the%250Anext%2520time%2520step.%2520Special%2520care%2520is%2520taken%2520in%2520the%2520design%2520of%2520couplings%2520between%2520hidden%250Alayer%2520and%2520input%2520nodes%2520to%2520ensure%2520lattice%2520symmetries%2520are%2520properly%2520incorporated%250Ainto%2520the%2520ESN%2520model.%2520Since%2520the%2520model%2520predictions%2520depend%2520only%2520on%2520CDW%250Aconfigurations%2520of%2520a%2520finite%2520domain%252C%2520the%2520ESN%2520is%2520scalable%2520and%2520transferrable%2520in%2520the%250Asense%2520that%2520a%2520model%2520trained%2520on%2520dataset%2520from%2520a%2520small%2520system%2520can%2520be%2520directly%250Aapplied%2520to%2520dynamical%2520simulations%2520on%2520larger%2520lattices.%2520Our%2520work%2520opens%2520a%2520new%250Aavenue%2520for%2520efficient%2520dynamical%2520modeling%2520of%2520pattern%2520formations%2520in%2520functional%250Aelectron%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Echo%20State%20network%20for%20coarsening%20dynamics%20of%20charge%20density%20waves&entry.906535625=Clement%20Dinh%20and%20Yunhao%20Fan%20and%20Gia-Wei%20Chern&entry.1292438233=%20%20An%20echo%20state%20network%20%28ESN%29%20is%20a%20type%20of%20reservoir%20computer%20that%20uses%20a%0Arecurrent%20neural%20network%20with%20a%20sparsely%20connected%20hidden%20layer.%20Compared%20with%0Aother%20recurrent%20neural%20networks%2C%20one%20great%20advantage%20of%20ESN%20is%20the%20simplicity%0Aof%20its%20training%20process.%20Yet%2C%20despite%20the%20seemingly%20restricted%20learnable%0Aparameters%2C%20ESN%20has%20been%20shown%20to%20successfully%20capture%20the%20spatial-temporal%0Adynamics%20of%20complex%20patterns.%20Here%20we%20build%20an%20ESN%20to%20model%20the%20coarsening%0Adynamics%20of%20charge-density%20waves%20%28CDW%29%20in%20a%20semi-classical%20Holstein%20model%2C%0Awhich%20exhibits%20a%20checkerboard%20electron%20density%20modulation%20at%20half-filling%0Astabilized%20by%20a%20commensurate%20lattice%20distortion.%20The%20inputs%20to%20the%20ESN%20are%0Alocal%20CDW%20order-parameters%20in%20a%20finite%20neighborhood%20centered%20around%20a%20given%0Asite%2C%20while%20the%20output%20is%20the%20predicted%20CDW%20order%20of%20the%20center%20site%20at%20the%0Anext%20time%20step.%20Special%20care%20is%20taken%20in%20the%20design%20of%20couplings%20between%20hidden%0Alayer%20and%20input%20nodes%20to%20ensure%20lattice%20symmetries%20are%20properly%20incorporated%0Ainto%20the%20ESN%20model.%20Since%20the%20model%20predictions%20depend%20only%20on%20CDW%0Aconfigurations%20of%20a%20finite%20domain%2C%20the%20ESN%20is%20scalable%20and%20transferrable%20in%20the%0Asense%20that%20a%20model%20trained%20on%20dataset%20from%20a%20small%20system%20can%20be%20directly%0Aapplied%20to%20dynamical%20simulations%20on%20larger%20lattices.%20Our%20work%20opens%20a%20new%0Aavenue%20for%20efficient%20dynamical%20modeling%20of%20pattern%20formations%20in%20functional%0Aelectron%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11982v1&entry.124074799=Read"},
{"title": "Extrapolating Jet Radiation with Autoregressive Transformers", "author": "Anja Butter and Fran\u00e7ois Charton and Javier Mari\u00f1o Villadamigo and Ayodele Ore and Tilman Plehn and Jonas Spinner", "abstract": "  Generative networks are an exciting tool for fast LHC event generation.\nUsually, they are used to generate configurations with a fixed number of\nparticles. Autoregressive transformers allow us to generate events with\nvariable numbers of particles, very much in line with the physics of QCD jet\nradiation. We show how they can learn a factorized likelihood for jet radiation\nand extrapolate in terms of the number of generated jets. For this\nextrapolation, bootstrapping training data and training with modifications of\nthe likelihood loss can be used.\n", "link": "http://arxiv.org/abs/2412.12074v1", "date": "2024-12-16", "relevancy": 1.4097, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5153}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.458}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extrapolating%20Jet%20Radiation%20with%20Autoregressive%20Transformers&body=Title%3A%20Extrapolating%20Jet%20Radiation%20with%20Autoregressive%20Transformers%0AAuthor%3A%20Anja%20Butter%20and%20Fran%C3%A7ois%20Charton%20and%20Javier%20Mari%C3%B1o%20Villadamigo%20and%20Ayodele%20Ore%20and%20Tilman%20Plehn%20and%20Jonas%20Spinner%0AAbstract%3A%20%20%20Generative%20networks%20are%20an%20exciting%20tool%20for%20fast%20LHC%20event%20generation.%0AUsually%2C%20they%20are%20used%20to%20generate%20configurations%20with%20a%20fixed%20number%20of%0Aparticles.%20Autoregressive%20transformers%20allow%20us%20to%20generate%20events%20with%0Avariable%20numbers%20of%20particles%2C%20very%20much%20in%20line%20with%20the%20physics%20of%20QCD%20jet%0Aradiation.%20We%20show%20how%20they%20can%20learn%20a%20factorized%20likelihood%20for%20jet%20radiation%0Aand%20extrapolate%20in%20terms%20of%20the%20number%20of%20generated%20jets.%20For%20this%0Aextrapolation%2C%20bootstrapping%20training%20data%20and%20training%20with%20modifications%20of%0Athe%20likelihood%20loss%20can%20be%20used.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtrapolating%2520Jet%2520Radiation%2520with%2520Autoregressive%2520Transformers%26entry.906535625%3DAnja%2520Butter%2520and%2520Fran%25C3%25A7ois%2520Charton%2520and%2520Javier%2520Mari%25C3%25B1o%2520Villadamigo%2520and%2520Ayodele%2520Ore%2520and%2520Tilman%2520Plehn%2520and%2520Jonas%2520Spinner%26entry.1292438233%3D%2520%2520Generative%2520networks%2520are%2520an%2520exciting%2520tool%2520for%2520fast%2520LHC%2520event%2520generation.%250AUsually%252C%2520they%2520are%2520used%2520to%2520generate%2520configurations%2520with%2520a%2520fixed%2520number%2520of%250Aparticles.%2520Autoregressive%2520transformers%2520allow%2520us%2520to%2520generate%2520events%2520with%250Avariable%2520numbers%2520of%2520particles%252C%2520very%2520much%2520in%2520line%2520with%2520the%2520physics%2520of%2520QCD%2520jet%250Aradiation.%2520We%2520show%2520how%2520they%2520can%2520learn%2520a%2520factorized%2520likelihood%2520for%2520jet%2520radiation%250Aand%2520extrapolate%2520in%2520terms%2520of%2520the%2520number%2520of%2520generated%2520jets.%2520For%2520this%250Aextrapolation%252C%2520bootstrapping%2520training%2520data%2520and%2520training%2520with%2520modifications%2520of%250Athe%2520likelihood%2520loss%2520can%2520be%2520used.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extrapolating%20Jet%20Radiation%20with%20Autoregressive%20Transformers&entry.906535625=Anja%20Butter%20and%20Fran%C3%A7ois%20Charton%20and%20Javier%20Mari%C3%B1o%20Villadamigo%20and%20Ayodele%20Ore%20and%20Tilman%20Plehn%20and%20Jonas%20Spinner&entry.1292438233=%20%20Generative%20networks%20are%20an%20exciting%20tool%20for%20fast%20LHC%20event%20generation.%0AUsually%2C%20they%20are%20used%20to%20generate%20configurations%20with%20a%20fixed%20number%20of%0Aparticles.%20Autoregressive%20transformers%20allow%20us%20to%20generate%20events%20with%0Avariable%20numbers%20of%20particles%2C%20very%20much%20in%20line%20with%20the%20physics%20of%20QCD%20jet%0Aradiation.%20We%20show%20how%20they%20can%20learn%20a%20factorized%20likelihood%20for%20jet%20radiation%0Aand%20extrapolate%20in%20terms%20of%20the%20number%20of%20generated%20jets.%20For%20this%0Aextrapolation%2C%20bootstrapping%20training%20data%20and%20training%20with%20modifications%20of%0Athe%20likelihood%20loss%20can%20be%20used.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12074v1&entry.124074799=Read"},
{"title": "LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts", "author": "Zhuhao Wang and Yihua Sun and Zihan Li and Xuan Yang and Fang Chen and Hongen Liao", "abstract": "  Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem.\n", "link": "http://arxiv.org/abs/2412.12001v1", "date": "2024-12-16", "relevancy": 1.0049, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5166}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5028}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-RG4%3A%20Flexible%20and%20Factual%20Radiology%20Report%20Generation%20across%20Diverse%0A%20%20Input%20Contexts&body=Title%3A%20LLM-RG4%3A%20Flexible%20and%20Factual%20Radiology%20Report%20Generation%20across%20Diverse%0A%20%20Input%20Contexts%0AAuthor%3A%20Zhuhao%20Wang%20and%20Yihua%20Sun%20and%20Zihan%20Li%20and%20Xuan%20Yang%20and%20Fang%20Chen%20and%20Hongen%20Liao%0AAbstract%3A%20%20%20Drafting%20radiology%20reports%20is%20a%20complex%20task%20requiring%20flexibility%2C%20where%0Aradiologists%20tail%20content%20to%20available%20information%20and%20particular%20clinical%0Ademands.%20However%2C%20most%20current%20radiology%20report%20generation%20%28RRG%29%20models%20are%0Aconstrained%20to%20a%20fixed%20task%20paradigm%2C%20such%20as%20predicting%20the%20full%20%60%60finding%27%27%0Asection%20from%20a%20single%20image%2C%20inherently%20involving%20a%20mismatch%20between%20inputs%20and%0Aoutputs.%20The%20trained%20models%20lack%20the%20flexibility%20for%20diverse%20inputs%20and%20could%0Agenerate%20harmful%2C%20input-agnostic%20hallucinations.%20To%20bridge%20the%20gap%20between%0Acurrent%20RRG%20models%20and%20the%20clinical%20demands%20in%20practice%2C%20we%20first%20develop%20a%0Adata%20generation%20pipeline%20to%20create%20a%20new%20MIMIC-RG4%20dataset%2C%20which%20considers%0Afour%20common%20radiology%20report%20drafting%20scenarios%20and%20has%20perfectly%20corresponded%0Ainput%20and%20output.%20Secondly%2C%20we%20propose%20a%20novel%20large%20language%20model%20%28LLM%29%20based%0ARRG%20framework%2C%20namely%20LLM-RG4%2C%20which%20utilizes%20LLM%27s%20flexible%0Ainstruction-following%20capabilities%20and%20extensive%20general%20knowledge.%20We%20further%0Adevelop%20an%20adaptive%20token%20fusion%20module%20that%20offers%20flexibility%20to%20handle%0Adiverse%20scenarios%20with%20different%20input%20combinations%2C%20while%20minimizing%20the%0Aadditional%20computational%20burden%20associated%20with%20increased%20input%20volumes.%0ABesides%2C%20we%20propose%20a%20token-level%20loss%20weighting%20strategy%20to%20direct%20the%20model%27s%0Aattention%20towards%20positive%20and%20uncertain%20descriptions.%20Experimental%20results%0Ademonstrate%20that%20LLM-RG4%20achieves%20state-of-the-art%20performance%20in%20both%20clinical%0Aefficiency%20and%20natural%20language%20generation%20on%20the%20MIMIC-RG4%20and%20MIMIC-CXR%0Adatasets.%20We%20quantitatively%20demonstrate%20that%20our%20model%20has%20minimal%0Ainput-agnostic%20hallucinations%2C%20whereas%20current%20open-source%20models%20commonly%0Asuffer%20from%20this%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-RG4%253A%2520Flexible%2520and%2520Factual%2520Radiology%2520Report%2520Generation%2520across%2520Diverse%250A%2520%2520Input%2520Contexts%26entry.906535625%3DZhuhao%2520Wang%2520and%2520Yihua%2520Sun%2520and%2520Zihan%2520Li%2520and%2520Xuan%2520Yang%2520and%2520Fang%2520Chen%2520and%2520Hongen%2520Liao%26entry.1292438233%3D%2520%2520Drafting%2520radiology%2520reports%2520is%2520a%2520complex%2520task%2520requiring%2520flexibility%252C%2520where%250Aradiologists%2520tail%2520content%2520to%2520available%2520information%2520and%2520particular%2520clinical%250Ademands.%2520However%252C%2520most%2520current%2520radiology%2520report%2520generation%2520%2528RRG%2529%2520models%2520are%250Aconstrained%2520to%2520a%2520fixed%2520task%2520paradigm%252C%2520such%2520as%2520predicting%2520the%2520full%2520%2560%2560finding%2527%2527%250Asection%2520from%2520a%2520single%2520image%252C%2520inherently%2520involving%2520a%2520mismatch%2520between%2520inputs%2520and%250Aoutputs.%2520The%2520trained%2520models%2520lack%2520the%2520flexibility%2520for%2520diverse%2520inputs%2520and%2520could%250Agenerate%2520harmful%252C%2520input-agnostic%2520hallucinations.%2520To%2520bridge%2520the%2520gap%2520between%250Acurrent%2520RRG%2520models%2520and%2520the%2520clinical%2520demands%2520in%2520practice%252C%2520we%2520first%2520develop%2520a%250Adata%2520generation%2520pipeline%2520to%2520create%2520a%2520new%2520MIMIC-RG4%2520dataset%252C%2520which%2520considers%250Afour%2520common%2520radiology%2520report%2520drafting%2520scenarios%2520and%2520has%2520perfectly%2520corresponded%250Ainput%2520and%2520output.%2520Secondly%252C%2520we%2520propose%2520a%2520novel%2520large%2520language%2520model%2520%2528LLM%2529%2520based%250ARRG%2520framework%252C%2520namely%2520LLM-RG4%252C%2520which%2520utilizes%2520LLM%2527s%2520flexible%250Ainstruction-following%2520capabilities%2520and%2520extensive%2520general%2520knowledge.%2520We%2520further%250Adevelop%2520an%2520adaptive%2520token%2520fusion%2520module%2520that%2520offers%2520flexibility%2520to%2520handle%250Adiverse%2520scenarios%2520with%2520different%2520input%2520combinations%252C%2520while%2520minimizing%2520the%250Aadditional%2520computational%2520burden%2520associated%2520with%2520increased%2520input%2520volumes.%250ABesides%252C%2520we%2520propose%2520a%2520token-level%2520loss%2520weighting%2520strategy%2520to%2520direct%2520the%2520model%2527s%250Aattention%2520towards%2520positive%2520and%2520uncertain%2520descriptions.%2520Experimental%2520results%250Ademonstrate%2520that%2520LLM-RG4%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520clinical%250Aefficiency%2520and%2520natural%2520language%2520generation%2520on%2520the%2520MIMIC-RG4%2520and%2520MIMIC-CXR%250Adatasets.%2520We%2520quantitatively%2520demonstrate%2520that%2520our%2520model%2520has%2520minimal%250Ainput-agnostic%2520hallucinations%252C%2520whereas%2520current%2520open-source%2520models%2520commonly%250Asuffer%2520from%2520this%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-RG4%3A%20Flexible%20and%20Factual%20Radiology%20Report%20Generation%20across%20Diverse%0A%20%20Input%20Contexts&entry.906535625=Zhuhao%20Wang%20and%20Yihua%20Sun%20and%20Zihan%20Li%20and%20Xuan%20Yang%20and%20Fang%20Chen%20and%20Hongen%20Liao&entry.1292438233=%20%20Drafting%20radiology%20reports%20is%20a%20complex%20task%20requiring%20flexibility%2C%20where%0Aradiologists%20tail%20content%20to%20available%20information%20and%20particular%20clinical%0Ademands.%20However%2C%20most%20current%20radiology%20report%20generation%20%28RRG%29%20models%20are%0Aconstrained%20to%20a%20fixed%20task%20paradigm%2C%20such%20as%20predicting%20the%20full%20%60%60finding%27%27%0Asection%20from%20a%20single%20image%2C%20inherently%20involving%20a%20mismatch%20between%20inputs%20and%0Aoutputs.%20The%20trained%20models%20lack%20the%20flexibility%20for%20diverse%20inputs%20and%20could%0Agenerate%20harmful%2C%20input-agnostic%20hallucinations.%20To%20bridge%20the%20gap%20between%0Acurrent%20RRG%20models%20and%20the%20clinical%20demands%20in%20practice%2C%20we%20first%20develop%20a%0Adata%20generation%20pipeline%20to%20create%20a%20new%20MIMIC-RG4%20dataset%2C%20which%20considers%0Afour%20common%20radiology%20report%20drafting%20scenarios%20and%20has%20perfectly%20corresponded%0Ainput%20and%20output.%20Secondly%2C%20we%20propose%20a%20novel%20large%20language%20model%20%28LLM%29%20based%0ARRG%20framework%2C%20namely%20LLM-RG4%2C%20which%20utilizes%20LLM%27s%20flexible%0Ainstruction-following%20capabilities%20and%20extensive%20general%20knowledge.%20We%20further%0Adevelop%20an%20adaptive%20token%20fusion%20module%20that%20offers%20flexibility%20to%20handle%0Adiverse%20scenarios%20with%20different%20input%20combinations%2C%20while%20minimizing%20the%0Aadditional%20computational%20burden%20associated%20with%20increased%20input%20volumes.%0ABesides%2C%20we%20propose%20a%20token-level%20loss%20weighting%20strategy%20to%20direct%20the%20model%27s%0Aattention%20towards%20positive%20and%20uncertain%20descriptions.%20Experimental%20results%0Ademonstrate%20that%20LLM-RG4%20achieves%20state-of-the-art%20performance%20in%20both%20clinical%0Aefficiency%20and%20natural%20language%20generation%20on%20the%20MIMIC-RG4%20and%20MIMIC-CXR%0Adatasets.%20We%20quantitatively%20demonstrate%20that%20our%20model%20has%20minimal%0Ainput-agnostic%20hallucinations%2C%20whereas%20current%20open-source%20models%20commonly%0Asuffer%20from%20this%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12001v1&entry.124074799=Read"},
{"title": "Asynchronous Distributed Gaussian Process Regression for Online Learning\n  and Dynamical Systems: Complementary Document", "author": "Zewen Yang and Xiaobing Dai and Sandra Hirche", "abstract": "  This is a complementary document for the paper titled \"Asynchronous\nDistributed Gaussian Process Regression for Online Learning and Dynamical\nSystems\".\n", "link": "http://arxiv.org/abs/2412.11950v1", "date": "2024-12-16", "relevancy": 1.4226, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4987}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4482}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Distributed%20Gaussian%20Process%20Regression%20for%20Online%20Learning%0A%20%20and%20Dynamical%20Systems%3A%20Complementary%20Document&body=Title%3A%20Asynchronous%20Distributed%20Gaussian%20Process%20Regression%20for%20Online%20Learning%0A%20%20and%20Dynamical%20Systems%3A%20Complementary%20Document%0AAuthor%3A%20Zewen%20Yang%20and%20Xiaobing%20Dai%20and%20Sandra%20Hirche%0AAbstract%3A%20%20%20This%20is%20a%20complementary%20document%20for%20the%20paper%20titled%20%22Asynchronous%0ADistributed%20Gaussian%20Process%20Regression%20for%20Online%20Learning%20and%20Dynamical%0ASystems%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Distributed%2520Gaussian%2520Process%2520Regression%2520for%2520Online%2520Learning%250A%2520%2520and%2520Dynamical%2520Systems%253A%2520Complementary%2520Document%26entry.906535625%3DZewen%2520Yang%2520and%2520Xiaobing%2520Dai%2520and%2520Sandra%2520Hirche%26entry.1292438233%3D%2520%2520This%2520is%2520a%2520complementary%2520document%2520for%2520the%2520paper%2520titled%2520%2522Asynchronous%250ADistributed%2520Gaussian%2520Process%2520Regression%2520for%2520Online%2520Learning%2520and%2520Dynamical%250ASystems%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Distributed%20Gaussian%20Process%20Regression%20for%20Online%20Learning%0A%20%20and%20Dynamical%20Systems%3A%20Complementary%20Document&entry.906535625=Zewen%20Yang%20and%20Xiaobing%20Dai%20and%20Sandra%20Hirche&entry.1292438233=%20%20This%20is%20a%20complementary%20document%20for%20the%20paper%20titled%20%22Asynchronous%0ADistributed%20Gaussian%20Process%20Regression%20for%20Online%20Learning%20and%20Dynamical%0ASystems%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11950v1&entry.124074799=Read"},
{"title": "Diffusion-based Reinforcement Learning via Q-weighted Variational Policy\n  Optimization", "author": "Shutong Ding and Ke Hu and Zhenhao Zhang and Kan Ren and Weinan Zhang and Jingyi Yu and Jingya Wang and Ye Shi", "abstract": "  Diffusion models have garnered widespread attention in Reinforcement Learning\n(RL) for their powerful expressiveness and multimodality. It has been verified\nthat utilizing diffusion policies can significantly improve the performance of\nRL algorithms in continuous control tasks by overcoming the limitations of\nunimodal policies, such as Gaussian policies, and providing the agent with\nenhanced exploration capabilities. However, existing works mainly focus on the\napplication of diffusion policies in offline RL, while their incorporation into\nonline RL is less investigated. The training objective of the diffusion model,\nknown as the variational lower bound, cannot be optimized directly in online RL\ndue to the unavailability of 'good' actions. This leads to difficulties in\nconducting diffusion policy improvement. To overcome this, we propose a novel\nmodel-free diffusion-based online RL algorithm, Q-weighted Variational Policy\nOptimization (QVPO). Specifically, we introduce the Q-weighted variational\nloss, which can be proved to be a tight lower bound of the policy objective in\nonline RL under certain conditions. To fulfill these conditions, the Q-weight\ntransformation functions are introduced for general scenarios. Additionally, to\nfurther enhance the exploration capability of the diffusion policy, we design a\nspecial entropy regularization term. We also develop an efficient behavior\npolicy to enhance sample efficiency by reducing the variance of the diffusion\npolicy during online interactions. Consequently, the QVPO algorithm leverages\nthe exploration capabilities and multimodality of diffusion policies,\npreventing the RL agent from converging to a sub-optimal policy. To verify the\neffectiveness of QVPO, we conduct comprehensive experiments on MuJoCo\nbenchmarks. The final results demonstrate that QVPO achieves state-of-the-art\nperformance on both cumulative reward and sample efficiency.\n", "link": "http://arxiv.org/abs/2405.16173v3", "date": "2024-12-16", "relevancy": 0.9915, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4985}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Reinforcement%20Learning%20via%20Q-weighted%20Variational%20Policy%0A%20%20Optimization&body=Title%3A%20Diffusion-based%20Reinforcement%20Learning%20via%20Q-weighted%20Variational%20Policy%0A%20%20Optimization%0AAuthor%3A%20Shutong%20Ding%20and%20Ke%20Hu%20and%20Zhenhao%20Zhang%20and%20Kan%20Ren%20and%20Weinan%20Zhang%20and%20Jingyi%20Yu%20and%20Jingya%20Wang%20and%20Ye%20Shi%0AAbstract%3A%20%20%20Diffusion%20models%20have%20garnered%20widespread%20attention%20in%20Reinforcement%20Learning%0A%28RL%29%20for%20their%20powerful%20expressiveness%20and%20multimodality.%20It%20has%20been%20verified%0Athat%20utilizing%20diffusion%20policies%20can%20significantly%20improve%20the%20performance%20of%0ARL%20algorithms%20in%20continuous%20control%20tasks%20by%20overcoming%20the%20limitations%20of%0Aunimodal%20policies%2C%20such%20as%20Gaussian%20policies%2C%20and%20providing%20the%20agent%20with%0Aenhanced%20exploration%20capabilities.%20However%2C%20existing%20works%20mainly%20focus%20on%20the%0Aapplication%20of%20diffusion%20policies%20in%20offline%20RL%2C%20while%20their%20incorporation%20into%0Aonline%20RL%20is%20less%20investigated.%20The%20training%20objective%20of%20the%20diffusion%20model%2C%0Aknown%20as%20the%20variational%20lower%20bound%2C%20cannot%20be%20optimized%20directly%20in%20online%20RL%0Adue%20to%20the%20unavailability%20of%20%27good%27%20actions.%20This%20leads%20to%20difficulties%20in%0Aconducting%20diffusion%20policy%20improvement.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%0Amodel-free%20diffusion-based%20online%20RL%20algorithm%2C%20Q-weighted%20Variational%20Policy%0AOptimization%20%28QVPO%29.%20Specifically%2C%20we%20introduce%20the%20Q-weighted%20variational%0Aloss%2C%20which%20can%20be%20proved%20to%20be%20a%20tight%20lower%20bound%20of%20the%20policy%20objective%20in%0Aonline%20RL%20under%20certain%20conditions.%20To%20fulfill%20these%20conditions%2C%20the%20Q-weight%0Atransformation%20functions%20are%20introduced%20for%20general%20scenarios.%20Additionally%2C%20to%0Afurther%20enhance%20the%20exploration%20capability%20of%20the%20diffusion%20policy%2C%20we%20design%20a%0Aspecial%20entropy%20regularization%20term.%20We%20also%20develop%20an%20efficient%20behavior%0Apolicy%20to%20enhance%20sample%20efficiency%20by%20reducing%20the%20variance%20of%20the%20diffusion%0Apolicy%20during%20online%20interactions.%20Consequently%2C%20the%20QVPO%20algorithm%20leverages%0Athe%20exploration%20capabilities%20and%20multimodality%20of%20diffusion%20policies%2C%0Apreventing%20the%20RL%20agent%20from%20converging%20to%20a%20sub-optimal%20policy.%20To%20verify%20the%0Aeffectiveness%20of%20QVPO%2C%20we%20conduct%20comprehensive%20experiments%20on%20MuJoCo%0Abenchmarks.%20The%20final%20results%20demonstrate%20that%20QVPO%20achieves%20state-of-the-art%0Aperformance%20on%20both%20cumulative%20reward%20and%20sample%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16173v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Reinforcement%2520Learning%2520via%2520Q-weighted%2520Variational%2520Policy%250A%2520%2520Optimization%26entry.906535625%3DShutong%2520Ding%2520and%2520Ke%2520Hu%2520and%2520Zhenhao%2520Zhang%2520and%2520Kan%2520Ren%2520and%2520Weinan%2520Zhang%2520and%2520Jingyi%2520Yu%2520and%2520Jingya%2520Wang%2520and%2520Ye%2520Shi%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520garnered%2520widespread%2520attention%2520in%2520Reinforcement%2520Learning%250A%2528RL%2529%2520for%2520their%2520powerful%2520expressiveness%2520and%2520multimodality.%2520It%2520has%2520been%2520verified%250Athat%2520utilizing%2520diffusion%2520policies%2520can%2520significantly%2520improve%2520the%2520performance%2520of%250ARL%2520algorithms%2520in%2520continuous%2520control%2520tasks%2520by%2520overcoming%2520the%2520limitations%2520of%250Aunimodal%2520policies%252C%2520such%2520as%2520Gaussian%2520policies%252C%2520and%2520providing%2520the%2520agent%2520with%250Aenhanced%2520exploration%2520capabilities.%2520However%252C%2520existing%2520works%2520mainly%2520focus%2520on%2520the%250Aapplication%2520of%2520diffusion%2520policies%2520in%2520offline%2520RL%252C%2520while%2520their%2520incorporation%2520into%250Aonline%2520RL%2520is%2520less%2520investigated.%2520The%2520training%2520objective%2520of%2520the%2520diffusion%2520model%252C%250Aknown%2520as%2520the%2520variational%2520lower%2520bound%252C%2520cannot%2520be%2520optimized%2520directly%2520in%2520online%2520RL%250Adue%2520to%2520the%2520unavailability%2520of%2520%2527good%2527%2520actions.%2520This%2520leads%2520to%2520difficulties%2520in%250Aconducting%2520diffusion%2520policy%2520improvement.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520a%2520novel%250Amodel-free%2520diffusion-based%2520online%2520RL%2520algorithm%252C%2520Q-weighted%2520Variational%2520Policy%250AOptimization%2520%2528QVPO%2529.%2520Specifically%252C%2520we%2520introduce%2520the%2520Q-weighted%2520variational%250Aloss%252C%2520which%2520can%2520be%2520proved%2520to%2520be%2520a%2520tight%2520lower%2520bound%2520of%2520the%2520policy%2520objective%2520in%250Aonline%2520RL%2520under%2520certain%2520conditions.%2520To%2520fulfill%2520these%2520conditions%252C%2520the%2520Q-weight%250Atransformation%2520functions%2520are%2520introduced%2520for%2520general%2520scenarios.%2520Additionally%252C%2520to%250Afurther%2520enhance%2520the%2520exploration%2520capability%2520of%2520the%2520diffusion%2520policy%252C%2520we%2520design%2520a%250Aspecial%2520entropy%2520regularization%2520term.%2520We%2520also%2520develop%2520an%2520efficient%2520behavior%250Apolicy%2520to%2520enhance%2520sample%2520efficiency%2520by%2520reducing%2520the%2520variance%2520of%2520the%2520diffusion%250Apolicy%2520during%2520online%2520interactions.%2520Consequently%252C%2520the%2520QVPO%2520algorithm%2520leverages%250Athe%2520exploration%2520capabilities%2520and%2520multimodality%2520of%2520diffusion%2520policies%252C%250Apreventing%2520the%2520RL%2520agent%2520from%2520converging%2520to%2520a%2520sub-optimal%2520policy.%2520To%2520verify%2520the%250Aeffectiveness%2520of%2520QVPO%252C%2520we%2520conduct%2520comprehensive%2520experiments%2520on%2520MuJoCo%250Abenchmarks.%2520The%2520final%2520results%2520demonstrate%2520that%2520QVPO%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520both%2520cumulative%2520reward%2520and%2520sample%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16173v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Reinforcement%20Learning%20via%20Q-weighted%20Variational%20Policy%0A%20%20Optimization&entry.906535625=Shutong%20Ding%20and%20Ke%20Hu%20and%20Zhenhao%20Zhang%20and%20Kan%20Ren%20and%20Weinan%20Zhang%20and%20Jingyi%20Yu%20and%20Jingya%20Wang%20and%20Ye%20Shi&entry.1292438233=%20%20Diffusion%20models%20have%20garnered%20widespread%20attention%20in%20Reinforcement%20Learning%0A%28RL%29%20for%20their%20powerful%20expressiveness%20and%20multimodality.%20It%20has%20been%20verified%0Athat%20utilizing%20diffusion%20policies%20can%20significantly%20improve%20the%20performance%20of%0ARL%20algorithms%20in%20continuous%20control%20tasks%20by%20overcoming%20the%20limitations%20of%0Aunimodal%20policies%2C%20such%20as%20Gaussian%20policies%2C%20and%20providing%20the%20agent%20with%0Aenhanced%20exploration%20capabilities.%20However%2C%20existing%20works%20mainly%20focus%20on%20the%0Aapplication%20of%20diffusion%20policies%20in%20offline%20RL%2C%20while%20their%20incorporation%20into%0Aonline%20RL%20is%20less%20investigated.%20The%20training%20objective%20of%20the%20diffusion%20model%2C%0Aknown%20as%20the%20variational%20lower%20bound%2C%20cannot%20be%20optimized%20directly%20in%20online%20RL%0Adue%20to%20the%20unavailability%20of%20%27good%27%20actions.%20This%20leads%20to%20difficulties%20in%0Aconducting%20diffusion%20policy%20improvement.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%0Amodel-free%20diffusion-based%20online%20RL%20algorithm%2C%20Q-weighted%20Variational%20Policy%0AOptimization%20%28QVPO%29.%20Specifically%2C%20we%20introduce%20the%20Q-weighted%20variational%0Aloss%2C%20which%20can%20be%20proved%20to%20be%20a%20tight%20lower%20bound%20of%20the%20policy%20objective%20in%0Aonline%20RL%20under%20certain%20conditions.%20To%20fulfill%20these%20conditions%2C%20the%20Q-weight%0Atransformation%20functions%20are%20introduced%20for%20general%20scenarios.%20Additionally%2C%20to%0Afurther%20enhance%20the%20exploration%20capability%20of%20the%20diffusion%20policy%2C%20we%20design%20a%0Aspecial%20entropy%20regularization%20term.%20We%20also%20develop%20an%20efficient%20behavior%0Apolicy%20to%20enhance%20sample%20efficiency%20by%20reducing%20the%20variance%20of%20the%20diffusion%0Apolicy%20during%20online%20interactions.%20Consequently%2C%20the%20QVPO%20algorithm%20leverages%0Athe%20exploration%20capabilities%20and%20multimodality%20of%20diffusion%20policies%2C%0Apreventing%20the%20RL%20agent%20from%20converging%20to%20a%20sub-optimal%20policy.%20To%20verify%20the%0Aeffectiveness%20of%20QVPO%2C%20we%20conduct%20comprehensive%20experiments%20on%20MuJoCo%0Abenchmarks.%20The%20final%20results%20demonstrate%20that%20QVPO%20achieves%20state-of-the-art%0Aperformance%20on%20both%20cumulative%20reward%20and%20sample%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16173v3&entry.124074799=Read"},
{"title": "Causal Diffusion Transformers for Generative Modeling", "author": "Chaorui Deng and Deyao Zh and Kunchang Li and Shi Guan and Haoqi Fan", "abstract": "  We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.\n", "link": "http://arxiv.org/abs/2412.12095v1", "date": "2024-12-16", "relevancy": 1.2643, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.677}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6233}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling&body=Title%3A%20Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling%0AAuthor%3A%20Chaorui%20Deng%20and%20Deyao%20Zh%20and%20Kunchang%20Li%20and%20Shi%20Guan%20and%20Haoqi%20Fan%0AAbstract%3A%20%20%20We%20introduce%20Causal%20Diffusion%20as%20the%20autoregressive%20%28AR%29%20counterpart%20of%0ADiffusion%20models.%20It%20is%20a%20next-token%28s%29%20forecasting%20framework%20that%20is%20friendly%0Ato%20both%20discrete%20and%20continuous%20modalities%20and%20compatible%20with%20existing%0Anext-token%20prediction%20models%20like%20LLaMA%20and%20GPT.%20While%20recent%20works%20attempt%20to%0Acombine%20diffusion%20with%20AR%20models%2C%20we%20show%20that%20introducing%20sequential%0Afactorization%20to%20a%20diffusion%20model%20can%20substantially%20improve%20its%20performance%0Aand%20enables%20a%20smooth%20transition%20between%20AR%20and%20diffusion%20generation%20modes.%0AHence%2C%20we%20propose%20CausalFusion%20-%20a%20decoder-only%20transformer%20that%0Adual-factorizes%20data%20across%20sequential%20tokens%20and%20diffusion%20noise%20levels%2C%0Aleading%20to%20state-of-the-art%20results%20on%20the%20ImageNet%20generation%20benchmark%20while%0Aalso%20enjoying%20the%20AR%20advantage%20of%20generating%20an%20arbitrary%20number%20of%20tokens%20for%0Ain-context%20reasoning.%20We%20further%20demonstrate%20CausalFusion%27s%20multimodal%0Acapabilities%20through%20a%20joint%20image%20generation%20and%20captioning%20model%2C%20and%0Ashowcase%20CausalFusion%27s%20ability%20for%20zero-shot%20in-context%20image%20manipulations.%0AWe%20hope%20that%20this%20work%20could%20provide%20the%20community%20with%20a%20fresh%20perspective%20on%0Atraining%20multimodal%20models%20over%20discrete%20and%20continuous%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Diffusion%2520Transformers%2520for%2520Generative%2520Modeling%26entry.906535625%3DChaorui%2520Deng%2520and%2520Deyao%2520Zh%2520and%2520Kunchang%2520Li%2520and%2520Shi%2520Guan%2520and%2520Haoqi%2520Fan%26entry.1292438233%3D%2520%2520We%2520introduce%2520Causal%2520Diffusion%2520as%2520the%2520autoregressive%2520%2528AR%2529%2520counterpart%2520of%250ADiffusion%2520models.%2520It%2520is%2520a%2520next-token%2528s%2529%2520forecasting%2520framework%2520that%2520is%2520friendly%250Ato%2520both%2520discrete%2520and%2520continuous%2520modalities%2520and%2520compatible%2520with%2520existing%250Anext-token%2520prediction%2520models%2520like%2520LLaMA%2520and%2520GPT.%2520While%2520recent%2520works%2520attempt%2520to%250Acombine%2520diffusion%2520with%2520AR%2520models%252C%2520we%2520show%2520that%2520introducing%2520sequential%250Afactorization%2520to%2520a%2520diffusion%2520model%2520can%2520substantially%2520improve%2520its%2520performance%250Aand%2520enables%2520a%2520smooth%2520transition%2520between%2520AR%2520and%2520diffusion%2520generation%2520modes.%250AHence%252C%2520we%2520propose%2520CausalFusion%2520-%2520a%2520decoder-only%2520transformer%2520that%250Adual-factorizes%2520data%2520across%2520sequential%2520tokens%2520and%2520diffusion%2520noise%2520levels%252C%250Aleading%2520to%2520state-of-the-art%2520results%2520on%2520the%2520ImageNet%2520generation%2520benchmark%2520while%250Aalso%2520enjoying%2520the%2520AR%2520advantage%2520of%2520generating%2520an%2520arbitrary%2520number%2520of%2520tokens%2520for%250Ain-context%2520reasoning.%2520We%2520further%2520demonstrate%2520CausalFusion%2527s%2520multimodal%250Acapabilities%2520through%2520a%2520joint%2520image%2520generation%2520and%2520captioning%2520model%252C%2520and%250Ashowcase%2520CausalFusion%2527s%2520ability%2520for%2520zero-shot%2520in-context%2520image%2520manipulations.%250AWe%2520hope%2520that%2520this%2520work%2520could%2520provide%2520the%2520community%2520with%2520a%2520fresh%2520perspective%2520on%250Atraining%2520multimodal%2520models%2520over%2520discrete%2520and%2520continuous%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling&entry.906535625=Chaorui%20Deng%20and%20Deyao%20Zh%20and%20Kunchang%20Li%20and%20Shi%20Guan%20and%20Haoqi%20Fan&entry.1292438233=%20%20We%20introduce%20Causal%20Diffusion%20as%20the%20autoregressive%20%28AR%29%20counterpart%20of%0ADiffusion%20models.%20It%20is%20a%20next-token%28s%29%20forecasting%20framework%20that%20is%20friendly%0Ato%20both%20discrete%20and%20continuous%20modalities%20and%20compatible%20with%20existing%0Anext-token%20prediction%20models%20like%20LLaMA%20and%20GPT.%20While%20recent%20works%20attempt%20to%0Acombine%20diffusion%20with%20AR%20models%2C%20we%20show%20that%20introducing%20sequential%0Afactorization%20to%20a%20diffusion%20model%20can%20substantially%20improve%20its%20performance%0Aand%20enables%20a%20smooth%20transition%20between%20AR%20and%20diffusion%20generation%20modes.%0AHence%2C%20we%20propose%20CausalFusion%20-%20a%20decoder-only%20transformer%20that%0Adual-factorizes%20data%20across%20sequential%20tokens%20and%20diffusion%20noise%20levels%2C%0Aleading%20to%20state-of-the-art%20results%20on%20the%20ImageNet%20generation%20benchmark%20while%0Aalso%20enjoying%20the%20AR%20advantage%20of%20generating%20an%20arbitrary%20number%20of%20tokens%20for%0Ain-context%20reasoning.%20We%20further%20demonstrate%20CausalFusion%27s%20multimodal%0Acapabilities%20through%20a%20joint%20image%20generation%20and%20captioning%20model%2C%20and%0Ashowcase%20CausalFusion%27s%20ability%20for%20zero-shot%20in-context%20image%20manipulations.%0AWe%20hope%20that%20this%20work%20could%20provide%20the%20community%20with%20a%20fresh%20perspective%20on%0Atraining%20multimodal%20models%20over%20discrete%20and%20continuous%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12095v1&entry.124074799=Read"},
{"title": "Two-Timescale Critic-Actor for Average Reward MDPs with Function\n  Approximation", "author": "Prashansa Panda and Shalabh Bhatnagar", "abstract": "  Several recent works have focused on carrying out non-asymptotic convergence\nanalyses for AC algorithms. Recently, a two-timescale critic-actor algorithm\nhas been presented for the discounted cost setting in the look-up table case\nwhere the timescales of the actor and the critic are reversed and only\nasymptotic convergence shown. In our work, we present the first two-timescale\ncritic-actor algorithm with function approximation in the long-run average\nreward setting and present the first finite-time non-asymptotic as well as\nasymptotic convergence analysis for such a scheme. We obtain optimal learning\nrates and prove that our algorithm achieves a sample complexity of\n{$\\mathcal{\\tilde{O}}(\\epsilon^{-(2+\\delta)})$ with $\\delta >0$ arbitrarily\nclose to zero,} for the mean squared error of the critic to be upper bounded by\n$\\epsilon$ which is better than the one obtained for two-timescale AC in a\nsimilar setting. A notable feature of our analysis is that we present the\nasymptotic convergence analysis of our scheme in addition to the finite-time\nbounds that we obtain and show the almost sure asymptotic convergence of the\n(slower) critic recursion to the attractor of an associated differential\ninclusion with actor parameters corresponding to local maxima of a perturbed\naverage reward objective. We also show the results of numerical experiments on\nthree benchmark settings and observe that our critic-actor algorithm performs\nthe best amongst all algorithms.\n", "link": "http://arxiv.org/abs/2402.01371v3", "date": "2024-12-16", "relevancy": 1.2773, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4342}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4167}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-Timescale%20Critic-Actor%20for%20Average%20Reward%20MDPs%20with%20Function%0A%20%20Approximation&body=Title%3A%20Two-Timescale%20Critic-Actor%20for%20Average%20Reward%20MDPs%20with%20Function%0A%20%20Approximation%0AAuthor%3A%20Prashansa%20Panda%20and%20Shalabh%20Bhatnagar%0AAbstract%3A%20%20%20Several%20recent%20works%20have%20focused%20on%20carrying%20out%20non-asymptotic%20convergence%0Aanalyses%20for%20AC%20algorithms.%20Recently%2C%20a%20two-timescale%20critic-actor%20algorithm%0Ahas%20been%20presented%20for%20the%20discounted%20cost%20setting%20in%20the%20look-up%20table%20case%0Awhere%20the%20timescales%20of%20the%20actor%20and%20the%20critic%20are%20reversed%20and%20only%0Aasymptotic%20convergence%20shown.%20In%20our%20work%2C%20we%20present%20the%20first%20two-timescale%0Acritic-actor%20algorithm%20with%20function%20approximation%20in%20the%20long-run%20average%0Areward%20setting%20and%20present%20the%20first%20finite-time%20non-asymptotic%20as%20well%20as%0Aasymptotic%20convergence%20analysis%20for%20such%20a%20scheme.%20We%20obtain%20optimal%20learning%0Arates%20and%20prove%20that%20our%20algorithm%20achieves%20a%20sample%20complexity%20of%0A%7B%24%5Cmathcal%7B%5Ctilde%7BO%7D%7D%28%5Cepsilon%5E%7B-%282%2B%5Cdelta%29%7D%29%24%20with%20%24%5Cdelta%20%3E0%24%20arbitrarily%0Aclose%20to%20zero%2C%7D%20for%20the%20mean%20squared%20error%20of%20the%20critic%20to%20be%20upper%20bounded%20by%0A%24%5Cepsilon%24%20which%20is%20better%20than%20the%20one%20obtained%20for%20two-timescale%20AC%20in%20a%0Asimilar%20setting.%20A%20notable%20feature%20of%20our%20analysis%20is%20that%20we%20present%20the%0Aasymptotic%20convergence%20analysis%20of%20our%20scheme%20in%20addition%20to%20the%20finite-time%0Abounds%20that%20we%20obtain%20and%20show%20the%20almost%20sure%20asymptotic%20convergence%20of%20the%0A%28slower%29%20critic%20recursion%20to%20the%20attractor%20of%20an%20associated%20differential%0Ainclusion%20with%20actor%20parameters%20corresponding%20to%20local%20maxima%20of%20a%20perturbed%0Aaverage%20reward%20objective.%20We%20also%20show%20the%20results%20of%20numerical%20experiments%20on%0Athree%20benchmark%20settings%20and%20observe%20that%20our%20critic-actor%20algorithm%20performs%0Athe%20best%20amongst%20all%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01371v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-Timescale%2520Critic-Actor%2520for%2520Average%2520Reward%2520MDPs%2520with%2520Function%250A%2520%2520Approximation%26entry.906535625%3DPrashansa%2520Panda%2520and%2520Shalabh%2520Bhatnagar%26entry.1292438233%3D%2520%2520Several%2520recent%2520works%2520have%2520focused%2520on%2520carrying%2520out%2520non-asymptotic%2520convergence%250Aanalyses%2520for%2520AC%2520algorithms.%2520Recently%252C%2520a%2520two-timescale%2520critic-actor%2520algorithm%250Ahas%2520been%2520presented%2520for%2520the%2520discounted%2520cost%2520setting%2520in%2520the%2520look-up%2520table%2520case%250Awhere%2520the%2520timescales%2520of%2520the%2520actor%2520and%2520the%2520critic%2520are%2520reversed%2520and%2520only%250Aasymptotic%2520convergence%2520shown.%2520In%2520our%2520work%252C%2520we%2520present%2520the%2520first%2520two-timescale%250Acritic-actor%2520algorithm%2520with%2520function%2520approximation%2520in%2520the%2520long-run%2520average%250Areward%2520setting%2520and%2520present%2520the%2520first%2520finite-time%2520non-asymptotic%2520as%2520well%2520as%250Aasymptotic%2520convergence%2520analysis%2520for%2520such%2520a%2520scheme.%2520We%2520obtain%2520optimal%2520learning%250Arates%2520and%2520prove%2520that%2520our%2520algorithm%2520achieves%2520a%2520sample%2520complexity%2520of%250A%257B%2524%255Cmathcal%257B%255Ctilde%257BO%257D%257D%2528%255Cepsilon%255E%257B-%25282%252B%255Cdelta%2529%257D%2529%2524%2520with%2520%2524%255Cdelta%2520%253E0%2524%2520arbitrarily%250Aclose%2520to%2520zero%252C%257D%2520for%2520the%2520mean%2520squared%2520error%2520of%2520the%2520critic%2520to%2520be%2520upper%2520bounded%2520by%250A%2524%255Cepsilon%2524%2520which%2520is%2520better%2520than%2520the%2520one%2520obtained%2520for%2520two-timescale%2520AC%2520in%2520a%250Asimilar%2520setting.%2520A%2520notable%2520feature%2520of%2520our%2520analysis%2520is%2520that%2520we%2520present%2520the%250Aasymptotic%2520convergence%2520analysis%2520of%2520our%2520scheme%2520in%2520addition%2520to%2520the%2520finite-time%250Abounds%2520that%2520we%2520obtain%2520and%2520show%2520the%2520almost%2520sure%2520asymptotic%2520convergence%2520of%2520the%250A%2528slower%2529%2520critic%2520recursion%2520to%2520the%2520attractor%2520of%2520an%2520associated%2520differential%250Ainclusion%2520with%2520actor%2520parameters%2520corresponding%2520to%2520local%2520maxima%2520of%2520a%2520perturbed%250Aaverage%2520reward%2520objective.%2520We%2520also%2520show%2520the%2520results%2520of%2520numerical%2520experiments%2520on%250Athree%2520benchmark%2520settings%2520and%2520observe%2520that%2520our%2520critic-actor%2520algorithm%2520performs%250Athe%2520best%2520amongst%2520all%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01371v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Timescale%20Critic-Actor%20for%20Average%20Reward%20MDPs%20with%20Function%0A%20%20Approximation&entry.906535625=Prashansa%20Panda%20and%20Shalabh%20Bhatnagar&entry.1292438233=%20%20Several%20recent%20works%20have%20focused%20on%20carrying%20out%20non-asymptotic%20convergence%0Aanalyses%20for%20AC%20algorithms.%20Recently%2C%20a%20two-timescale%20critic-actor%20algorithm%0Ahas%20been%20presented%20for%20the%20discounted%20cost%20setting%20in%20the%20look-up%20table%20case%0Awhere%20the%20timescales%20of%20the%20actor%20and%20the%20critic%20are%20reversed%20and%20only%0Aasymptotic%20convergence%20shown.%20In%20our%20work%2C%20we%20present%20the%20first%20two-timescale%0Acritic-actor%20algorithm%20with%20function%20approximation%20in%20the%20long-run%20average%0Areward%20setting%20and%20present%20the%20first%20finite-time%20non-asymptotic%20as%20well%20as%0Aasymptotic%20convergence%20analysis%20for%20such%20a%20scheme.%20We%20obtain%20optimal%20learning%0Arates%20and%20prove%20that%20our%20algorithm%20achieves%20a%20sample%20complexity%20of%0A%7B%24%5Cmathcal%7B%5Ctilde%7BO%7D%7D%28%5Cepsilon%5E%7B-%282%2B%5Cdelta%29%7D%29%24%20with%20%24%5Cdelta%20%3E0%24%20arbitrarily%0Aclose%20to%20zero%2C%7D%20for%20the%20mean%20squared%20error%20of%20the%20critic%20to%20be%20upper%20bounded%20by%0A%24%5Cepsilon%24%20which%20is%20better%20than%20the%20one%20obtained%20for%20two-timescale%20AC%20in%20a%0Asimilar%20setting.%20A%20notable%20feature%20of%20our%20analysis%20is%20that%20we%20present%20the%0Aasymptotic%20convergence%20analysis%20of%20our%20scheme%20in%20addition%20to%20the%20finite-time%0Abounds%20that%20we%20obtain%20and%20show%20the%20almost%20sure%20asymptotic%20convergence%20of%20the%0A%28slower%29%20critic%20recursion%20to%20the%20attractor%20of%20an%20associated%20differential%0Ainclusion%20with%20actor%20parameters%20corresponding%20to%20local%20maxima%20of%20a%20perturbed%0Aaverage%20reward%20objective.%20We%20also%20show%20the%20results%20of%20numerical%20experiments%20on%0Athree%20benchmark%20settings%20and%20observe%20that%20our%20critic-actor%20algorithm%20performs%0Athe%20best%20amongst%20all%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01371v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


