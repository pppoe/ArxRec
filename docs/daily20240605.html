<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240604.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "WE-GS: An In-the-wild Efficient 3D Gaussian Representation for\n  Unconstrained Photo Collections", "author": "Yuze Wang and Junyi Wang and Yue Qi", "abstract": "  Novel View Synthesis (NVS) from unconstrained photo collections is\nchallenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has\nshown promise for photorealistic and real-time NVS of static scenes. Building\non 3DGS, we propose an efficient point-based differentiable rendering framework\nfor scene reconstruction from photo collections. Our key innovation is a\nresidual-based spherical harmonic coefficients transfer module that adapts 3DGS\nto varying lighting conditions and photometric post-processing. This\nlightweight module can be pre-computed and ensures efficient gradient\npropagation from rendered images to 3D Gaussian attributes. Additionally, we\nobserve that the appearance encoder and the transient mask predictor, the two\nmost critical parts of NVS from unconstrained photo collections, can be\nmutually beneficial. We introduce a plug-and-play lightweight spatial attention\nmodule to simultaneously predict transient occluders and latent appearance\nrepresentation for each image. After training and preprocessing, our method\naligns with the standard 3DGS format and rendering pipeline, facilitating\nseamlessly integration into various 3DGS applications. Extensive experiments on\ndiverse datasets show our approach outperforms existing approaches on the\nrendering quality of novel view and appearance synthesis with high converge and\nrendering speed.\n", "link": "http://arxiv.org/abs/2406.02407v1", "date": "2024-06-04", "relevancy": 3.367, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7268}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6642}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WE-GS%3A%20An%20In-the-wild%20Efficient%203D%20Gaussian%20Representation%20for%0A%20%20Unconstrained%20Photo%20Collections&body=Title%3A%20WE-GS%3A%20An%20In-the-wild%20Efficient%203D%20Gaussian%20Representation%20for%0A%20%20Unconstrained%20Photo%20Collections%0AAuthor%3A%20Yuze%20Wang%20and%20Junyi%20Wang%20and%20Yue%20Qi%0AAbstract%3A%20%20%20Novel%20View%20Synthesis%20%28NVS%29%20from%20unconstrained%20photo%20collections%20is%0Achallenging%20in%20computer%20graphics.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%0Ashown%20promise%20for%20photorealistic%20and%20real-time%20NVS%20of%20static%20scenes.%20Building%0Aon%203DGS%2C%20we%20propose%20an%20efficient%20point-based%20differentiable%20rendering%20framework%0Afor%20scene%20reconstruction%20from%20photo%20collections.%20Our%20key%20innovation%20is%20a%0Aresidual-based%20spherical%20harmonic%20coefficients%20transfer%20module%20that%20adapts%203DGS%0Ato%20varying%20lighting%20conditions%20and%20photometric%20post-processing.%20This%0Alightweight%20module%20can%20be%20pre-computed%20and%20ensures%20efficient%20gradient%0Apropagation%20from%20rendered%20images%20to%203D%20Gaussian%20attributes.%20Additionally%2C%20we%0Aobserve%20that%20the%20appearance%20encoder%20and%20the%20transient%20mask%20predictor%2C%20the%20two%0Amost%20critical%20parts%20of%20NVS%20from%20unconstrained%20photo%20collections%2C%20can%20be%0Amutually%20beneficial.%20We%20introduce%20a%20plug-and-play%20lightweight%20spatial%20attention%0Amodule%20to%20simultaneously%20predict%20transient%20occluders%20and%20latent%20appearance%0Arepresentation%20for%20each%20image.%20After%20training%20and%20preprocessing%2C%20our%20method%0Aaligns%20with%20the%20standard%203DGS%20format%20and%20rendering%20pipeline%2C%20facilitating%0Aseamlessly%20integration%20into%20various%203DGS%20applications.%20Extensive%20experiments%20on%0Adiverse%20datasets%20show%20our%20approach%20outperforms%20existing%20approaches%20on%20the%0Arendering%20quality%20of%20novel%20view%20and%20appearance%20synthesis%20with%20high%20converge%20and%0Arendering%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWE-GS%253A%2520An%2520In-the-wild%2520Efficient%25203D%2520Gaussian%2520Representation%2520for%250A%2520%2520Unconstrained%2520Photo%2520Collections%26entry.906535625%3DYuze%2520Wang%2520and%2520Junyi%2520Wang%2520and%2520Yue%2520Qi%26entry.1292438233%3D%2520%2520Novel%2520View%2520Synthesis%2520%2528NVS%2529%2520from%2520unconstrained%2520photo%2520collections%2520is%250Achallenging%2520in%2520computer%2520graphics.%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%250Ashown%2520promise%2520for%2520photorealistic%2520and%2520real-time%2520NVS%2520of%2520static%2520scenes.%2520Building%250Aon%25203DGS%252C%2520we%2520propose%2520an%2520efficient%2520point-based%2520differentiable%2520rendering%2520framework%250Afor%2520scene%2520reconstruction%2520from%2520photo%2520collections.%2520Our%2520key%2520innovation%2520is%2520a%250Aresidual-based%2520spherical%2520harmonic%2520coefficients%2520transfer%2520module%2520that%2520adapts%25203DGS%250Ato%2520varying%2520lighting%2520conditions%2520and%2520photometric%2520post-processing.%2520This%250Alightweight%2520module%2520can%2520be%2520pre-computed%2520and%2520ensures%2520efficient%2520gradient%250Apropagation%2520from%2520rendered%2520images%2520to%25203D%2520Gaussian%2520attributes.%2520Additionally%252C%2520we%250Aobserve%2520that%2520the%2520appearance%2520encoder%2520and%2520the%2520transient%2520mask%2520predictor%252C%2520the%2520two%250Amost%2520critical%2520parts%2520of%2520NVS%2520from%2520unconstrained%2520photo%2520collections%252C%2520can%2520be%250Amutually%2520beneficial.%2520We%2520introduce%2520a%2520plug-and-play%2520lightweight%2520spatial%2520attention%250Amodule%2520to%2520simultaneously%2520predict%2520transient%2520occluders%2520and%2520latent%2520appearance%250Arepresentation%2520for%2520each%2520image.%2520After%2520training%2520and%2520preprocessing%252C%2520our%2520method%250Aaligns%2520with%2520the%2520standard%25203DGS%2520format%2520and%2520rendering%2520pipeline%252C%2520facilitating%250Aseamlessly%2520integration%2520into%2520various%25203DGS%2520applications.%2520Extensive%2520experiments%2520on%250Adiverse%2520datasets%2520show%2520our%2520approach%2520outperforms%2520existing%2520approaches%2520on%2520the%250Arendering%2520quality%2520of%2520novel%2520view%2520and%2520appearance%2520synthesis%2520with%2520high%2520converge%2520and%250Arendering%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WE-GS%3A%20An%20In-the-wild%20Efficient%203D%20Gaussian%20Representation%20for%0A%20%20Unconstrained%20Photo%20Collections&entry.906535625=Yuze%20Wang%20and%20Junyi%20Wang%20and%20Yue%20Qi&entry.1292438233=%20%20Novel%20View%20Synthesis%20%28NVS%29%20from%20unconstrained%20photo%20collections%20is%0Achallenging%20in%20computer%20graphics.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%0Ashown%20promise%20for%20photorealistic%20and%20real-time%20NVS%20of%20static%20scenes.%20Building%0Aon%203DGS%2C%20we%20propose%20an%20efficient%20point-based%20differentiable%20rendering%20framework%0Afor%20scene%20reconstruction%20from%20photo%20collections.%20Our%20key%20innovation%20is%20a%0Aresidual-based%20spherical%20harmonic%20coefficients%20transfer%20module%20that%20adapts%203DGS%0Ato%20varying%20lighting%20conditions%20and%20photometric%20post-processing.%20This%0Alightweight%20module%20can%20be%20pre-computed%20and%20ensures%20efficient%20gradient%0Apropagation%20from%20rendered%20images%20to%203D%20Gaussian%20attributes.%20Additionally%2C%20we%0Aobserve%20that%20the%20appearance%20encoder%20and%20the%20transient%20mask%20predictor%2C%20the%20two%0Amost%20critical%20parts%20of%20NVS%20from%20unconstrained%20photo%20collections%2C%20can%20be%0Amutually%20beneficial.%20We%20introduce%20a%20plug-and-play%20lightweight%20spatial%20attention%0Amodule%20to%20simultaneously%20predict%20transient%20occluders%20and%20latent%20appearance%0Arepresentation%20for%20each%20image.%20After%20training%20and%20preprocessing%2C%20our%20method%0Aaligns%20with%20the%20standard%203DGS%20format%20and%20rendering%20pipeline%2C%20facilitating%0Aseamlessly%20integration%20into%20various%203DGS%20applications.%20Extensive%20experiments%20on%0Adiverse%20datasets%20show%20our%20approach%20outperforms%20existing%20approaches%20on%20the%0Arendering%20quality%20of%20novel%20view%20and%20appearance%20synthesis%20with%20high%20converge%20and%0Arendering%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02407v1&entry.124074799=Read"},
{"title": "SuperGaussian: Repurposing Video Models for 3D Super Resolution", "author": "Yuan Shen and Duygu Ceylan and Paul Guerrero and Zexiang Xu and Niloy J. Mitra and Shenlong Wang and Anna Fr\u00fchst\u00fcck", "abstract": "  We present a simple, modular, and generic method that upsamples coarse 3D\nmodels by adding geometric and appearance details. While generative 3D models\nnow exist, they do not yet match the quality of their counterparts in image and\nvideo domains. We demonstrate that it is possible to directly repurpose\nexisting (pretrained) video models for 3D super-resolution and thus sidestep\nthe problem of the shortage of large repositories of high-quality 3D training\nmodels. We describe how to repurpose video upsampling models, which are not 3D\nconsistent, and combine them with 3D consolidation to produce 3D-consistent\nresults. As output, we produce high quality Gaussian Splat models, which are\nobject centric and effective. Our method is category agnostic and can be easily\nincorporated into existing 3D workflows. We evaluate our proposed SuperGaussian\non a variety of 3D inputs, which are diverse both in terms of complexity and\nrepresentation (e.g., Gaussian Splats or NeRFs), and demonstrate that our\nsimple method significantly improves the fidelity of the final 3D models. Check\nour project website for details: supergaussian.github.io\n", "link": "http://arxiv.org/abs/2406.00609v2", "date": "2024-06-04", "relevancy": 3.2359, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6789}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6727}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.59}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperGaussian%3A%20Repurposing%20Video%20Models%20for%203D%20Super%20Resolution&body=Title%3A%20SuperGaussian%3A%20Repurposing%20Video%20Models%20for%203D%20Super%20Resolution%0AAuthor%3A%20Yuan%20Shen%20and%20Duygu%20Ceylan%20and%20Paul%20Guerrero%20and%20Zexiang%20Xu%20and%20Niloy%20J.%20Mitra%20and%20Shenlong%20Wang%20and%20Anna%20Fr%C3%BChst%C3%BCck%0AAbstract%3A%20%20%20We%20present%20a%20simple%2C%20modular%2C%20and%20generic%20method%20that%20upsamples%20coarse%203D%0Amodels%20by%20adding%20geometric%20and%20appearance%20details.%20While%20generative%203D%20models%0Anow%20exist%2C%20they%20do%20not%20yet%20match%20the%20quality%20of%20their%20counterparts%20in%20image%20and%0Avideo%20domains.%20We%20demonstrate%20that%20it%20is%20possible%20to%20directly%20repurpose%0Aexisting%20%28pretrained%29%20video%20models%20for%203D%20super-resolution%20and%20thus%20sidestep%0Athe%20problem%20of%20the%20shortage%20of%20large%20repositories%20of%20high-quality%203D%20training%0Amodels.%20We%20describe%20how%20to%20repurpose%20video%20upsampling%20models%2C%20which%20are%20not%203D%0Aconsistent%2C%20and%20combine%20them%20with%203D%20consolidation%20to%20produce%203D-consistent%0Aresults.%20As%20output%2C%20we%20produce%20high%20quality%20Gaussian%20Splat%20models%2C%20which%20are%0Aobject%20centric%20and%20effective.%20Our%20method%20is%20category%20agnostic%20and%20can%20be%20easily%0Aincorporated%20into%20existing%203D%20workflows.%20We%20evaluate%20our%20proposed%20SuperGaussian%0Aon%20a%20variety%20of%203D%20inputs%2C%20which%20are%20diverse%20both%20in%20terms%20of%20complexity%20and%0Arepresentation%20%28e.g.%2C%20Gaussian%20Splats%20or%20NeRFs%29%2C%20and%20demonstrate%20that%20our%0Asimple%20method%20significantly%20improves%20the%20fidelity%20of%20the%20final%203D%20models.%20Check%0Aour%20project%20website%20for%20details%3A%20supergaussian.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00609v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperGaussian%253A%2520Repurposing%2520Video%2520Models%2520for%25203D%2520Super%2520Resolution%26entry.906535625%3DYuan%2520Shen%2520and%2520Duygu%2520Ceylan%2520and%2520Paul%2520Guerrero%2520and%2520Zexiang%2520Xu%2520and%2520Niloy%2520J.%2520Mitra%2520and%2520Shenlong%2520Wang%2520and%2520Anna%2520Fr%25C3%25BChst%25C3%25BCck%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520simple%252C%2520modular%252C%2520and%2520generic%2520method%2520that%2520upsamples%2520coarse%25203D%250Amodels%2520by%2520adding%2520geometric%2520and%2520appearance%2520details.%2520While%2520generative%25203D%2520models%250Anow%2520exist%252C%2520they%2520do%2520not%2520yet%2520match%2520the%2520quality%2520of%2520their%2520counterparts%2520in%2520image%2520and%250Avideo%2520domains.%2520We%2520demonstrate%2520that%2520it%2520is%2520possible%2520to%2520directly%2520repurpose%250Aexisting%2520%2528pretrained%2529%2520video%2520models%2520for%25203D%2520super-resolution%2520and%2520thus%2520sidestep%250Athe%2520problem%2520of%2520the%2520shortage%2520of%2520large%2520repositories%2520of%2520high-quality%25203D%2520training%250Amodels.%2520We%2520describe%2520how%2520to%2520repurpose%2520video%2520upsampling%2520models%252C%2520which%2520are%2520not%25203D%250Aconsistent%252C%2520and%2520combine%2520them%2520with%25203D%2520consolidation%2520to%2520produce%25203D-consistent%250Aresults.%2520As%2520output%252C%2520we%2520produce%2520high%2520quality%2520Gaussian%2520Splat%2520models%252C%2520which%2520are%250Aobject%2520centric%2520and%2520effective.%2520Our%2520method%2520is%2520category%2520agnostic%2520and%2520can%2520be%2520easily%250Aincorporated%2520into%2520existing%25203D%2520workflows.%2520We%2520evaluate%2520our%2520proposed%2520SuperGaussian%250Aon%2520a%2520variety%2520of%25203D%2520inputs%252C%2520which%2520are%2520diverse%2520both%2520in%2520terms%2520of%2520complexity%2520and%250Arepresentation%2520%2528e.g.%252C%2520Gaussian%2520Splats%2520or%2520NeRFs%2529%252C%2520and%2520demonstrate%2520that%2520our%250Asimple%2520method%2520significantly%2520improves%2520the%2520fidelity%2520of%2520the%2520final%25203D%2520models.%2520Check%250Aour%2520project%2520website%2520for%2520details%253A%2520supergaussian.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00609v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperGaussian%3A%20Repurposing%20Video%20Models%20for%203D%20Super%20Resolution&entry.906535625=Yuan%20Shen%20and%20Duygu%20Ceylan%20and%20Paul%20Guerrero%20and%20Zexiang%20Xu%20and%20Niloy%20J.%20Mitra%20and%20Shenlong%20Wang%20and%20Anna%20Fr%C3%BChst%C3%BCck&entry.1292438233=%20%20We%20present%20a%20simple%2C%20modular%2C%20and%20generic%20method%20that%20upsamples%20coarse%203D%0Amodels%20by%20adding%20geometric%20and%20appearance%20details.%20While%20generative%203D%20models%0Anow%20exist%2C%20they%20do%20not%20yet%20match%20the%20quality%20of%20their%20counterparts%20in%20image%20and%0Avideo%20domains.%20We%20demonstrate%20that%20it%20is%20possible%20to%20directly%20repurpose%0Aexisting%20%28pretrained%29%20video%20models%20for%203D%20super-resolution%20and%20thus%20sidestep%0Athe%20problem%20of%20the%20shortage%20of%20large%20repositories%20of%20high-quality%203D%20training%0Amodels.%20We%20describe%20how%20to%20repurpose%20video%20upsampling%20models%2C%20which%20are%20not%203D%0Aconsistent%2C%20and%20combine%20them%20with%203D%20consolidation%20to%20produce%203D-consistent%0Aresults.%20As%20output%2C%20we%20produce%20high%20quality%20Gaussian%20Splat%20models%2C%20which%20are%0Aobject%20centric%20and%20effective.%20Our%20method%20is%20category%20agnostic%20and%20can%20be%20easily%0Aincorporated%20into%20existing%203D%20workflows.%20We%20evaluate%20our%20proposed%20SuperGaussian%0Aon%20a%20variety%20of%203D%20inputs%2C%20which%20are%20diverse%20both%20in%20terms%20of%20complexity%20and%0Arepresentation%20%28e.g.%2C%20Gaussian%20Splats%20or%20NeRFs%29%2C%20and%20demonstrate%20that%20our%0Asimple%20method%20significantly%20improves%20the%20fidelity%20of%20the%20final%203D%20models.%20Check%0Aour%20project%20website%20for%20details%3A%20supergaussian.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00609v2&entry.124074799=Read"},
{"title": "DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume\n  Rendering", "author": "Zhongpai Gao and Benjamin Planche and Meng Zheng and Xiao Chen and Terrence Chen and Ziyan Wu", "abstract": "  Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images\ngenerated from 3D CT volumes, widely used in preoperative settings but limited\nin intraoperative applications due to computational bottlenecks, especially for\naccurate but heavy physics-based Monte Carlo methods. While analytical DRR\nrenderers offer greater efficiency, they overlook anisotropic X-ray image\nformation phenomena, such as Compton scattering. We present a novel approach\nthat marries realistic physics-inspired X-ray simulation with efficient,\ndifferentiable DRR generation using 3D Gaussian splatting (3DGS). Our\ndirection-disentangled 3DGS (DDGS) method separates the radiosity contribution\ninto isotropic and direction-dependent components, approximating complex\nanisotropic interactions without intricate runtime simulations. Additionally,\nwe adapt the 3DGS initialization to account for tomography data properties,\nenhancing accuracy and efficiency. Our method outperforms state-of-the-art\ntechniques in image accuracy. Furthermore, our DDGS shows promise for\nintraoperative applications and inverse problems such as pose registration,\ndelivering superior registration accuracy and runtime performance compared to\nanalytical DRR methods.\n", "link": "http://arxiv.org/abs/2406.02518v1", "date": "2024-06-04", "relevancy": 3.1763, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.689}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6085}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DDGS-CT%3A%20Direction-Disentangled%20Gaussian%20Splatting%20for%20Realistic%20Volume%0A%20%20Rendering&body=Title%3A%20DDGS-CT%3A%20Direction-Disentangled%20Gaussian%20Splatting%20for%20Realistic%20Volume%0A%20%20Rendering%0AAuthor%3A%20Zhongpai%20Gao%20and%20Benjamin%20Planche%20and%20Meng%20Zheng%20and%20Xiao%20Chen%20and%20Terrence%20Chen%20and%20Ziyan%20Wu%0AAbstract%3A%20%20%20Digitally%20reconstructed%20radiographs%20%28DRRs%29%20are%20simulated%202D%20X-ray%20images%0Agenerated%20from%203D%20CT%20volumes%2C%20widely%20used%20in%20preoperative%20settings%20but%20limited%0Ain%20intraoperative%20applications%20due%20to%20computational%20bottlenecks%2C%20especially%20for%0Aaccurate%20but%20heavy%20physics-based%20Monte%20Carlo%20methods.%20While%20analytical%20DRR%0Arenderers%20offer%20greater%20efficiency%2C%20they%20overlook%20anisotropic%20X-ray%20image%0Aformation%20phenomena%2C%20such%20as%20Compton%20scattering.%20We%20present%20a%20novel%20approach%0Athat%20marries%20realistic%20physics-inspired%20X-ray%20simulation%20with%20efficient%2C%0Adifferentiable%20DRR%20generation%20using%203D%20Gaussian%20splatting%20%283DGS%29.%20Our%0Adirection-disentangled%203DGS%20%28DDGS%29%20method%20separates%20the%20radiosity%20contribution%0Ainto%20isotropic%20and%20direction-dependent%20components%2C%20approximating%20complex%0Aanisotropic%20interactions%20without%20intricate%20runtime%20simulations.%20Additionally%2C%0Awe%20adapt%20the%203DGS%20initialization%20to%20account%20for%20tomography%20data%20properties%2C%0Aenhancing%20accuracy%20and%20efficiency.%20Our%20method%20outperforms%20state-of-the-art%0Atechniques%20in%20image%20accuracy.%20Furthermore%2C%20our%20DDGS%20shows%20promise%20for%0Aintraoperative%20applications%20and%20inverse%20problems%20such%20as%20pose%20registration%2C%0Adelivering%20superior%20registration%20accuracy%20and%20runtime%20performance%20compared%20to%0Aanalytical%20DRR%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDDGS-CT%253A%2520Direction-Disentangled%2520Gaussian%2520Splatting%2520for%2520Realistic%2520Volume%250A%2520%2520Rendering%26entry.906535625%3DZhongpai%2520Gao%2520and%2520Benjamin%2520Planche%2520and%2520Meng%2520Zheng%2520and%2520Xiao%2520Chen%2520and%2520Terrence%2520Chen%2520and%2520Ziyan%2520Wu%26entry.1292438233%3D%2520%2520Digitally%2520reconstructed%2520radiographs%2520%2528DRRs%2529%2520are%2520simulated%25202D%2520X-ray%2520images%250Agenerated%2520from%25203D%2520CT%2520volumes%252C%2520widely%2520used%2520in%2520preoperative%2520settings%2520but%2520limited%250Ain%2520intraoperative%2520applications%2520due%2520to%2520computational%2520bottlenecks%252C%2520especially%2520for%250Aaccurate%2520but%2520heavy%2520physics-based%2520Monte%2520Carlo%2520methods.%2520While%2520analytical%2520DRR%250Arenderers%2520offer%2520greater%2520efficiency%252C%2520they%2520overlook%2520anisotropic%2520X-ray%2520image%250Aformation%2520phenomena%252C%2520such%2520as%2520Compton%2520scattering.%2520We%2520present%2520a%2520novel%2520approach%250Athat%2520marries%2520realistic%2520physics-inspired%2520X-ray%2520simulation%2520with%2520efficient%252C%250Adifferentiable%2520DRR%2520generation%2520using%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529.%2520Our%250Adirection-disentangled%25203DGS%2520%2528DDGS%2529%2520method%2520separates%2520the%2520radiosity%2520contribution%250Ainto%2520isotropic%2520and%2520direction-dependent%2520components%252C%2520approximating%2520complex%250Aanisotropic%2520interactions%2520without%2520intricate%2520runtime%2520simulations.%2520Additionally%252C%250Awe%2520adapt%2520the%25203DGS%2520initialization%2520to%2520account%2520for%2520tomography%2520data%2520properties%252C%250Aenhancing%2520accuracy%2520and%2520efficiency.%2520Our%2520method%2520outperforms%2520state-of-the-art%250Atechniques%2520in%2520image%2520accuracy.%2520Furthermore%252C%2520our%2520DDGS%2520shows%2520promise%2520for%250Aintraoperative%2520applications%2520and%2520inverse%2520problems%2520such%2520as%2520pose%2520registration%252C%250Adelivering%2520superior%2520registration%2520accuracy%2520and%2520runtime%2520performance%2520compared%2520to%250Aanalytical%2520DRR%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDGS-CT%3A%20Direction-Disentangled%20Gaussian%20Splatting%20for%20Realistic%20Volume%0A%20%20Rendering&entry.906535625=Zhongpai%20Gao%20and%20Benjamin%20Planche%20and%20Meng%20Zheng%20and%20Xiao%20Chen%20and%20Terrence%20Chen%20and%20Ziyan%20Wu&entry.1292438233=%20%20Digitally%20reconstructed%20radiographs%20%28DRRs%29%20are%20simulated%202D%20X-ray%20images%0Agenerated%20from%203D%20CT%20volumes%2C%20widely%20used%20in%20preoperative%20settings%20but%20limited%0Ain%20intraoperative%20applications%20due%20to%20computational%20bottlenecks%2C%20especially%20for%0Aaccurate%20but%20heavy%20physics-based%20Monte%20Carlo%20methods.%20While%20analytical%20DRR%0Arenderers%20offer%20greater%20efficiency%2C%20they%20overlook%20anisotropic%20X-ray%20image%0Aformation%20phenomena%2C%20such%20as%20Compton%20scattering.%20We%20present%20a%20novel%20approach%0Athat%20marries%20realistic%20physics-inspired%20X-ray%20simulation%20with%20efficient%2C%0Adifferentiable%20DRR%20generation%20using%203D%20Gaussian%20splatting%20%283DGS%29.%20Our%0Adirection-disentangled%203DGS%20%28DDGS%29%20method%20separates%20the%20radiosity%20contribution%0Ainto%20isotropic%20and%20direction-dependent%20components%2C%20approximating%20complex%0Aanisotropic%20interactions%20without%20intricate%20runtime%20simulations.%20Additionally%2C%0Awe%20adapt%20the%203DGS%20initialization%20to%20account%20for%20tomography%20data%20properties%2C%0Aenhancing%20accuracy%20and%20efficiency.%20Our%20method%20outperforms%20state-of-the-art%0Atechniques%20in%20image%20accuracy.%20Furthermore%2C%20our%20DDGS%20shows%20promise%20for%0Aintraoperative%20applications%20and%20inverse%20problems%20such%20as%20pose%20registration%2C%0Adelivering%20superior%20registration%20accuracy%20and%20runtime%20performance%20compared%20to%0Aanalytical%20DRR%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02518v1&entry.124074799=Read"},
{"title": "Enhancing 2D Representation Learning with a 3D Prior", "author": "Mehmet Ayg\u00fcn and Prithviraj Dhar and Zhicheng Yan and Oisin Mac Aodha and Rakesh Ranjan", "abstract": "  Learning robust and effective representations of visual data is a fundamental\ntask in computer vision. Traditionally, this is achieved by training models\nwith labeled data which can be expensive to obtain. Self-supervised learning\nattempts to circumvent the requirement for labeled data by learning\nrepresentations from raw unlabeled visual data alone. However, unlike humans\nwho obtain rich 3D information from their binocular vision and through motion,\nthe majority of current self-supervised methods are tasked with learning from\nmonocular 2D image collections. This is noteworthy as it has been demonstrated\nthat shape-centric visual processing is more robust compared to texture-biased\nautomated methods. Inspired by this, we propose a new approach for\nstrengthening existing self-supervised methods by explicitly enforcing a strong\n3D structural prior directly into the model during training. Through\nexperiments, across a range of datasets, we demonstrate that our 3D aware\nrepresentations are more robust compared to conventional self-supervised\nbaselines.\n", "link": "http://arxiv.org/abs/2406.02535v1", "date": "2024-06-04", "relevancy": 3.1279, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.626}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%202D%20Representation%20Learning%20with%20a%203D%20Prior&body=Title%3A%20Enhancing%202D%20Representation%20Learning%20with%20a%203D%20Prior%0AAuthor%3A%20Mehmet%20Ayg%C3%BCn%20and%20Prithviraj%20Dhar%20and%20Zhicheng%20Yan%20and%20Oisin%20Mac%20Aodha%20and%20Rakesh%20Ranjan%0AAbstract%3A%20%20%20Learning%20robust%20and%20effective%20representations%20of%20visual%20data%20is%20a%20fundamental%0Atask%20in%20computer%20vision.%20Traditionally%2C%20this%20is%20achieved%20by%20training%20models%0Awith%20labeled%20data%20which%20can%20be%20expensive%20to%20obtain.%20Self-supervised%20learning%0Aattempts%20to%20circumvent%20the%20requirement%20for%20labeled%20data%20by%20learning%0Arepresentations%20from%20raw%20unlabeled%20visual%20data%20alone.%20However%2C%20unlike%20humans%0Awho%20obtain%20rich%203D%20information%20from%20their%20binocular%20vision%20and%20through%20motion%2C%0Athe%20majority%20of%20current%20self-supervised%20methods%20are%20tasked%20with%20learning%20from%0Amonocular%202D%20image%20collections.%20This%20is%20noteworthy%20as%20it%20has%20been%20demonstrated%0Athat%20shape-centric%20visual%20processing%20is%20more%20robust%20compared%20to%20texture-biased%0Aautomated%20methods.%20Inspired%20by%20this%2C%20we%20propose%20a%20new%20approach%20for%0Astrengthening%20existing%20self-supervised%20methods%20by%20explicitly%20enforcing%20a%20strong%0A3D%20structural%20prior%20directly%20into%20the%20model%20during%20training.%20Through%0Aexperiments%2C%20across%20a%20range%20of%20datasets%2C%20we%20demonstrate%20that%20our%203D%20aware%0Arepresentations%20are%20more%20robust%20compared%20to%20conventional%20self-supervised%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%25202D%2520Representation%2520Learning%2520with%2520a%25203D%2520Prior%26entry.906535625%3DMehmet%2520Ayg%25C3%25BCn%2520and%2520Prithviraj%2520Dhar%2520and%2520Zhicheng%2520Yan%2520and%2520Oisin%2520Mac%2520Aodha%2520and%2520Rakesh%2520Ranjan%26entry.1292438233%3D%2520%2520Learning%2520robust%2520and%2520effective%2520representations%2520of%2520visual%2520data%2520is%2520a%2520fundamental%250Atask%2520in%2520computer%2520vision.%2520Traditionally%252C%2520this%2520is%2520achieved%2520by%2520training%2520models%250Awith%2520labeled%2520data%2520which%2520can%2520be%2520expensive%2520to%2520obtain.%2520Self-supervised%2520learning%250Aattempts%2520to%2520circumvent%2520the%2520requirement%2520for%2520labeled%2520data%2520by%2520learning%250Arepresentations%2520from%2520raw%2520unlabeled%2520visual%2520data%2520alone.%2520However%252C%2520unlike%2520humans%250Awho%2520obtain%2520rich%25203D%2520information%2520from%2520their%2520binocular%2520vision%2520and%2520through%2520motion%252C%250Athe%2520majority%2520of%2520current%2520self-supervised%2520methods%2520are%2520tasked%2520with%2520learning%2520from%250Amonocular%25202D%2520image%2520collections.%2520This%2520is%2520noteworthy%2520as%2520it%2520has%2520been%2520demonstrated%250Athat%2520shape-centric%2520visual%2520processing%2520is%2520more%2520robust%2520compared%2520to%2520texture-biased%250Aautomated%2520methods.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520new%2520approach%2520for%250Astrengthening%2520existing%2520self-supervised%2520methods%2520by%2520explicitly%2520enforcing%2520a%2520strong%250A3D%2520structural%2520prior%2520directly%2520into%2520the%2520model%2520during%2520training.%2520Through%250Aexperiments%252C%2520across%2520a%2520range%2520of%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%25203D%2520aware%250Arepresentations%2520are%2520more%2520robust%2520compared%2520to%2520conventional%2520self-supervised%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%202D%20Representation%20Learning%20with%20a%203D%20Prior&entry.906535625=Mehmet%20Ayg%C3%BCn%20and%20Prithviraj%20Dhar%20and%20Zhicheng%20Yan%20and%20Oisin%20Mac%20Aodha%20and%20Rakesh%20Ranjan&entry.1292438233=%20%20Learning%20robust%20and%20effective%20representations%20of%20visual%20data%20is%20a%20fundamental%0Atask%20in%20computer%20vision.%20Traditionally%2C%20this%20is%20achieved%20by%20training%20models%0Awith%20labeled%20data%20which%20can%20be%20expensive%20to%20obtain.%20Self-supervised%20learning%0Aattempts%20to%20circumvent%20the%20requirement%20for%20labeled%20data%20by%20learning%0Arepresentations%20from%20raw%20unlabeled%20visual%20data%20alone.%20However%2C%20unlike%20humans%0Awho%20obtain%20rich%203D%20information%20from%20their%20binocular%20vision%20and%20through%20motion%2C%0Athe%20majority%20of%20current%20self-supervised%20methods%20are%20tasked%20with%20learning%20from%0Amonocular%202D%20image%20collections.%20This%20is%20noteworthy%20as%20it%20has%20been%20demonstrated%0Athat%20shape-centric%20visual%20processing%20is%20more%20robust%20compared%20to%20texture-biased%0Aautomated%20methods.%20Inspired%20by%20this%2C%20we%20propose%20a%20new%20approach%20for%0Astrengthening%20existing%20self-supervised%20methods%20by%20explicitly%20enforcing%20a%20strong%0A3D%20structural%20prior%20directly%20into%20the%20model%20during%20training.%20Through%0Aexperiments%2C%20across%20a%20range%20of%20datasets%2C%20we%20demonstrate%20that%20our%203D%20aware%0Arepresentations%20are%20more%20robust%20compared%20to%20conventional%20self-supervised%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02535v1&entry.124074799=Read"},
{"title": "CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation", "author": "Dejia Xu and Weili Nie and Chao Liu and Sifei Liu and Jan Kautz and Zhangyang Wang and Arash Vahdat", "abstract": "  Recently video diffusion models have emerged as expressive generative tools\nfor high-quality video content creation readily available to general users.\nHowever, these models often do not offer precise control over camera poses for\nvideo generation, limiting the expression of cinematic language and user\ncontrol. To address this issue, we introduce CamCo, which allows fine-grained\nCamera pose Control for image-to-video generation. We equip a pre-trained\nimage-to-video generator with accurately parameterized camera pose input using\nPl\\\"ucker coordinates. To enhance 3D consistency in the videos produced, we\nintegrate an epipolar attention module in each attention block that enforces\nepipolar constraints to the feature maps. Additionally, we fine-tune CamCo on\nreal-world videos with camera poses estimated through structure-from-motion\nalgorithms to better synthesize object motion. Our experiments show that CamCo\nsignificantly improves 3D consistency and camera control capabilities compared\nto previous models while effectively generating plausible object motion.\nProject page: https://ir1d.github.io/CamCo/\n", "link": "http://arxiv.org/abs/2406.02509v1", "date": "2024-06-04", "relevancy": 3.111, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6287}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.619}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CamCo%3A%20Camera-Controllable%203D-Consistent%20Image-to-Video%20Generation&body=Title%3A%20CamCo%3A%20Camera-Controllable%203D-Consistent%20Image-to-Video%20Generation%0AAuthor%3A%20Dejia%20Xu%20and%20Weili%20Nie%20and%20Chao%20Liu%20and%20Sifei%20Liu%20and%20Jan%20Kautz%20and%20Zhangyang%20Wang%20and%20Arash%20Vahdat%0AAbstract%3A%20%20%20Recently%20video%20diffusion%20models%20have%20emerged%20as%20expressive%20generative%20tools%0Afor%20high-quality%20video%20content%20creation%20readily%20available%20to%20general%20users.%0AHowever%2C%20these%20models%20often%20do%20not%20offer%20precise%20control%20over%20camera%20poses%20for%0Avideo%20generation%2C%20limiting%20the%20expression%20of%20cinematic%20language%20and%20user%0Acontrol.%20To%20address%20this%20issue%2C%20we%20introduce%20CamCo%2C%20which%20allows%20fine-grained%0ACamera%20pose%20Control%20for%20image-to-video%20generation.%20We%20equip%20a%20pre-trained%0Aimage-to-video%20generator%20with%20accurately%20parameterized%20camera%20pose%20input%20using%0APl%5C%22ucker%20coordinates.%20To%20enhance%203D%20consistency%20in%20the%20videos%20produced%2C%20we%0Aintegrate%20an%20epipolar%20attention%20module%20in%20each%20attention%20block%20that%20enforces%0Aepipolar%20constraints%20to%20the%20feature%20maps.%20Additionally%2C%20we%20fine-tune%20CamCo%20on%0Areal-world%20videos%20with%20camera%20poses%20estimated%20through%20structure-from-motion%0Aalgorithms%20to%20better%20synthesize%20object%20motion.%20Our%20experiments%20show%20that%20CamCo%0Asignificantly%20improves%203D%20consistency%20and%20camera%20control%20capabilities%20compared%0Ato%20previous%20models%20while%20effectively%20generating%20plausible%20object%20motion.%0AProject%20page%3A%20https%3A//ir1d.github.io/CamCo/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamCo%253A%2520Camera-Controllable%25203D-Consistent%2520Image-to-Video%2520Generation%26entry.906535625%3DDejia%2520Xu%2520and%2520Weili%2520Nie%2520and%2520Chao%2520Liu%2520and%2520Sifei%2520Liu%2520and%2520Jan%2520Kautz%2520and%2520Zhangyang%2520Wang%2520and%2520Arash%2520Vahdat%26entry.1292438233%3D%2520%2520Recently%2520video%2520diffusion%2520models%2520have%2520emerged%2520as%2520expressive%2520generative%2520tools%250Afor%2520high-quality%2520video%2520content%2520creation%2520readily%2520available%2520to%2520general%2520users.%250AHowever%252C%2520these%2520models%2520often%2520do%2520not%2520offer%2520precise%2520control%2520over%2520camera%2520poses%2520for%250Avideo%2520generation%252C%2520limiting%2520the%2520expression%2520of%2520cinematic%2520language%2520and%2520user%250Acontrol.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520CamCo%252C%2520which%2520allows%2520fine-grained%250ACamera%2520pose%2520Control%2520for%2520image-to-video%2520generation.%2520We%2520equip%2520a%2520pre-trained%250Aimage-to-video%2520generator%2520with%2520accurately%2520parameterized%2520camera%2520pose%2520input%2520using%250APl%255C%2522ucker%2520coordinates.%2520To%2520enhance%25203D%2520consistency%2520in%2520the%2520videos%2520produced%252C%2520we%250Aintegrate%2520an%2520epipolar%2520attention%2520module%2520in%2520each%2520attention%2520block%2520that%2520enforces%250Aepipolar%2520constraints%2520to%2520the%2520feature%2520maps.%2520Additionally%252C%2520we%2520fine-tune%2520CamCo%2520on%250Areal-world%2520videos%2520with%2520camera%2520poses%2520estimated%2520through%2520structure-from-motion%250Aalgorithms%2520to%2520better%2520synthesize%2520object%2520motion.%2520Our%2520experiments%2520show%2520that%2520CamCo%250Asignificantly%2520improves%25203D%2520consistency%2520and%2520camera%2520control%2520capabilities%2520compared%250Ato%2520previous%2520models%2520while%2520effectively%2520generating%2520plausible%2520object%2520motion.%250AProject%2520page%253A%2520https%253A//ir1d.github.io/CamCo/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CamCo%3A%20Camera-Controllable%203D-Consistent%20Image-to-Video%20Generation&entry.906535625=Dejia%20Xu%20and%20Weili%20Nie%20and%20Chao%20Liu%20and%20Sifei%20Liu%20and%20Jan%20Kautz%20and%20Zhangyang%20Wang%20and%20Arash%20Vahdat&entry.1292438233=%20%20Recently%20video%20diffusion%20models%20have%20emerged%20as%20expressive%20generative%20tools%0Afor%20high-quality%20video%20content%20creation%20readily%20available%20to%20general%20users.%0AHowever%2C%20these%20models%20often%20do%20not%20offer%20precise%20control%20over%20camera%20poses%20for%0Avideo%20generation%2C%20limiting%20the%20expression%20of%20cinematic%20language%20and%20user%0Acontrol.%20To%20address%20this%20issue%2C%20we%20introduce%20CamCo%2C%20which%20allows%20fine-grained%0ACamera%20pose%20Control%20for%20image-to-video%20generation.%20We%20equip%20a%20pre-trained%0Aimage-to-video%20generator%20with%20accurately%20parameterized%20camera%20pose%20input%20using%0APl%5C%22ucker%20coordinates.%20To%20enhance%203D%20consistency%20in%20the%20videos%20produced%2C%20we%0Aintegrate%20an%20epipolar%20attention%20module%20in%20each%20attention%20block%20that%20enforces%0Aepipolar%20constraints%20to%20the%20feature%20maps.%20Additionally%2C%20we%20fine-tune%20CamCo%20on%0Areal-world%20videos%20with%20camera%20poses%20estimated%20through%20structure-from-motion%0Aalgorithms%20to%20better%20synthesize%20object%20motion.%20Our%20experiments%20show%20that%20CamCo%0Asignificantly%20improves%203D%20consistency%20and%20camera%20control%20capabilities%20compared%0Ato%20previous%20models%20while%20effectively%20generating%20plausible%20object%20motion.%0AProject%20page%3A%20https%3A//ir1d.github.io/CamCo/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02509v1&entry.124074799=Read"},
{"title": "Lay-A-Scene: Personalized 3D Object Arrangement Using Text-to-Image\n  Priors", "author": "Ohad Rahamim and Hilit Segev and Idan Achituve and Yuval Atzmon and Yoni Kasten and Gal Chechik", "abstract": "  Generating 3D visual scenes is at the forefront of visual generative AI, but\ncurrent 3D generation techniques struggle with generating scenes with multiple\nhigh-resolution objects. Here we introduce Lay-A-Scene, which solves the task\nof Open-set 3D Object Arrangement, effectively arranging unseen objects. Given\na set of 3D objects, the task is to find a plausible arrangement of these\nobjects in a scene. We address this task by leveraging pre-trained\ntext-to-image models. We personalize the model and explain how to generate\nimages of a scene that contains multiple predefined objects without neglecting\nany of them. Then, we describe how to infer the 3D poses and arrangement of\nobjects from a 2D generated image by finding a consistent projection of objects\nonto the 2D scene. We evaluate the quality of Lay-A-Scene using 3D objects from\nObjaverse and human raters and find that it often generates coherent and\nfeasible 3D object arrangements.\n", "link": "http://arxiv.org/abs/2406.00687v2", "date": "2024-06-04", "relevancy": 3.094, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6356}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6356}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lay-A-Scene%3A%20Personalized%203D%20Object%20Arrangement%20Using%20Text-to-Image%0A%20%20Priors&body=Title%3A%20Lay-A-Scene%3A%20Personalized%203D%20Object%20Arrangement%20Using%20Text-to-Image%0A%20%20Priors%0AAuthor%3A%20Ohad%20Rahamim%20and%20Hilit%20Segev%20and%20Idan%20Achituve%20and%20Yuval%20Atzmon%20and%20Yoni%20Kasten%20and%20Gal%20Chechik%0AAbstract%3A%20%20%20Generating%203D%20visual%20scenes%20is%20at%20the%20forefront%20of%20visual%20generative%20AI%2C%20but%0Acurrent%203D%20generation%20techniques%20struggle%20with%20generating%20scenes%20with%20multiple%0Ahigh-resolution%20objects.%20Here%20we%20introduce%20Lay-A-Scene%2C%20which%20solves%20the%20task%0Aof%20Open-set%203D%20Object%20Arrangement%2C%20effectively%20arranging%20unseen%20objects.%20Given%0Aa%20set%20of%203D%20objects%2C%20the%20task%20is%20to%20find%20a%20plausible%20arrangement%20of%20these%0Aobjects%20in%20a%20scene.%20We%20address%20this%20task%20by%20leveraging%20pre-trained%0Atext-to-image%20models.%20We%20personalize%20the%20model%20and%20explain%20how%20to%20generate%0Aimages%20of%20a%20scene%20that%20contains%20multiple%20predefined%20objects%20without%20neglecting%0Aany%20of%20them.%20Then%2C%20we%20describe%20how%20to%20infer%20the%203D%20poses%20and%20arrangement%20of%0Aobjects%20from%20a%202D%20generated%20image%20by%20finding%20a%20consistent%20projection%20of%20objects%0Aonto%20the%202D%20scene.%20We%20evaluate%20the%20quality%20of%20Lay-A-Scene%20using%203D%20objects%20from%0AObjaverse%20and%20human%20raters%20and%20find%20that%20it%20often%20generates%20coherent%20and%0Afeasible%203D%20object%20arrangements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLay-A-Scene%253A%2520Personalized%25203D%2520Object%2520Arrangement%2520Using%2520Text-to-Image%250A%2520%2520Priors%26entry.906535625%3DOhad%2520Rahamim%2520and%2520Hilit%2520Segev%2520and%2520Idan%2520Achituve%2520and%2520Yuval%2520Atzmon%2520and%2520Yoni%2520Kasten%2520and%2520Gal%2520Chechik%26entry.1292438233%3D%2520%2520Generating%25203D%2520visual%2520scenes%2520is%2520at%2520the%2520forefront%2520of%2520visual%2520generative%2520AI%252C%2520but%250Acurrent%25203D%2520generation%2520techniques%2520struggle%2520with%2520generating%2520scenes%2520with%2520multiple%250Ahigh-resolution%2520objects.%2520Here%2520we%2520introduce%2520Lay-A-Scene%252C%2520which%2520solves%2520the%2520task%250Aof%2520Open-set%25203D%2520Object%2520Arrangement%252C%2520effectively%2520arranging%2520unseen%2520objects.%2520Given%250Aa%2520set%2520of%25203D%2520objects%252C%2520the%2520task%2520is%2520to%2520find%2520a%2520plausible%2520arrangement%2520of%2520these%250Aobjects%2520in%2520a%2520scene.%2520We%2520address%2520this%2520task%2520by%2520leveraging%2520pre-trained%250Atext-to-image%2520models.%2520We%2520personalize%2520the%2520model%2520and%2520explain%2520how%2520to%2520generate%250Aimages%2520of%2520a%2520scene%2520that%2520contains%2520multiple%2520predefined%2520objects%2520without%2520neglecting%250Aany%2520of%2520them.%2520Then%252C%2520we%2520describe%2520how%2520to%2520infer%2520the%25203D%2520poses%2520and%2520arrangement%2520of%250Aobjects%2520from%2520a%25202D%2520generated%2520image%2520by%2520finding%2520a%2520consistent%2520projection%2520of%2520objects%250Aonto%2520the%25202D%2520scene.%2520We%2520evaluate%2520the%2520quality%2520of%2520Lay-A-Scene%2520using%25203D%2520objects%2520from%250AObjaverse%2520and%2520human%2520raters%2520and%2520find%2520that%2520it%2520often%2520generates%2520coherent%2520and%250Afeasible%25203D%2520object%2520arrangements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lay-A-Scene%3A%20Personalized%203D%20Object%20Arrangement%20Using%20Text-to-Image%0A%20%20Priors&entry.906535625=Ohad%20Rahamim%20and%20Hilit%20Segev%20and%20Idan%20Achituve%20and%20Yuval%20Atzmon%20and%20Yoni%20Kasten%20and%20Gal%20Chechik&entry.1292438233=%20%20Generating%203D%20visual%20scenes%20is%20at%20the%20forefront%20of%20visual%20generative%20AI%2C%20but%0Acurrent%203D%20generation%20techniques%20struggle%20with%20generating%20scenes%20with%20multiple%0Ahigh-resolution%20objects.%20Here%20we%20introduce%20Lay-A-Scene%2C%20which%20solves%20the%20task%0Aof%20Open-set%203D%20Object%20Arrangement%2C%20effectively%20arranging%20unseen%20objects.%20Given%0Aa%20set%20of%203D%20objects%2C%20the%20task%20is%20to%20find%20a%20plausible%20arrangement%20of%20these%0Aobjects%20in%20a%20scene.%20We%20address%20this%20task%20by%20leveraging%20pre-trained%0Atext-to-image%20models.%20We%20personalize%20the%20model%20and%20explain%20how%20to%20generate%0Aimages%20of%20a%20scene%20that%20contains%20multiple%20predefined%20objects%20without%20neglecting%0Aany%20of%20them.%20Then%2C%20we%20describe%20how%20to%20infer%20the%203D%20poses%20and%20arrangement%20of%0Aobjects%20from%20a%202D%20generated%20image%20by%20finding%20a%20consistent%20projection%20of%20objects%0Aonto%20the%202D%20scene.%20We%20evaluate%20the%20quality%20of%20Lay-A-Scene%20using%203D%20objects%20from%0AObjaverse%20and%20human%20raters%20and%20find%20that%20it%20often%20generates%20coherent%20and%0Afeasible%203D%20object%20arrangements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00687v2&entry.124074799=Read"},
{"title": "GenS: Generalizable Neural Surface Reconstruction from Multi-View Images", "author": "Rui Peng and Xiaodong Gu and Luyang Tang and Shihe Shen and Fanqi Yu and Ronggang Wang", "abstract": "  Combining the signed distance function (SDF) and differentiable volume\nrendering has emerged as a powerful paradigm for surface reconstruction from\nmulti-view images without 3D supervision. However, current methods are impeded\nby requiring long-time per-scene optimizations and cannot generalize to new\nscenes. In this paper, we present GenS, an end-to-end generalizable neural\nsurface reconstruction model. Unlike coordinate-based methods that train a\nseparate network for each scene, we construct a generalized multi-scale volume\nto directly encode all scenes. Compared with existing solutions, our\nrepresentation is more powerful, which can recover high-frequency details while\nmaintaining global smoothness. Meanwhile, we introduce a multi-scale\nfeature-metric consistency to impose the multi-view consistency in a more\ndiscriminative multi-scale feature space, which is robust to the failures of\nthe photometric consistency. And the learnable feature can be self-enhanced to\ncontinuously improve the matching accuracy and mitigate aggregation ambiguity.\nFurthermore, we design a view contrast loss to force the model to be robust to\nthose regions covered by few viewpoints through distilling the geometric prior\nfrom dense input to sparse input. Extensive experiments on popular benchmarks\nshow that our model can generalize well to new scenes and outperform existing\nstate-of-the-art methods even those employing ground-truth depth supervision.\nCode is available at https://github.com/prstrive/GenS.\n", "link": "http://arxiv.org/abs/2406.02495v1", "date": "2024-06-04", "relevancy": 3.0046, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6286}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenS%3A%20Generalizable%20Neural%20Surface%20Reconstruction%20from%20Multi-View%20Images&body=Title%3A%20GenS%3A%20Generalizable%20Neural%20Surface%20Reconstruction%20from%20Multi-View%20Images%0AAuthor%3A%20Rui%20Peng%20and%20Xiaodong%20Gu%20and%20Luyang%20Tang%20and%20Shihe%20Shen%20and%20Fanqi%20Yu%20and%20Ronggang%20Wang%0AAbstract%3A%20%20%20Combining%20the%20signed%20distance%20function%20%28SDF%29%20and%20differentiable%20volume%0Arendering%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20surface%20reconstruction%20from%0Amulti-view%20images%20without%203D%20supervision.%20However%2C%20current%20methods%20are%20impeded%0Aby%20requiring%20long-time%20per-scene%20optimizations%20and%20cannot%20generalize%20to%20new%0Ascenes.%20In%20this%20paper%2C%20we%20present%20GenS%2C%20an%20end-to-end%20generalizable%20neural%0Asurface%20reconstruction%20model.%20Unlike%20coordinate-based%20methods%20that%20train%20a%0Aseparate%20network%20for%20each%20scene%2C%20we%20construct%20a%20generalized%20multi-scale%20volume%0Ato%20directly%20encode%20all%20scenes.%20Compared%20with%20existing%20solutions%2C%20our%0Arepresentation%20is%20more%20powerful%2C%20which%20can%20recover%20high-frequency%20details%20while%0Amaintaining%20global%20smoothness.%20Meanwhile%2C%20we%20introduce%20a%20multi-scale%0Afeature-metric%20consistency%20to%20impose%20the%20multi-view%20consistency%20in%20a%20more%0Adiscriminative%20multi-scale%20feature%20space%2C%20which%20is%20robust%20to%20the%20failures%20of%0Athe%20photometric%20consistency.%20And%20the%20learnable%20feature%20can%20be%20self-enhanced%20to%0Acontinuously%20improve%20the%20matching%20accuracy%20and%20mitigate%20aggregation%20ambiguity.%0AFurthermore%2C%20we%20design%20a%20view%20contrast%20loss%20to%20force%20the%20model%20to%20be%20robust%20to%0Athose%20regions%20covered%20by%20few%20viewpoints%20through%20distilling%20the%20geometric%20prior%0Afrom%20dense%20input%20to%20sparse%20input.%20Extensive%20experiments%20on%20popular%20benchmarks%0Ashow%20that%20our%20model%20can%20generalize%20well%20to%20new%20scenes%20and%20outperform%20existing%0Astate-of-the-art%20methods%20even%20those%20employing%20ground-truth%20depth%20supervision.%0ACode%20is%20available%20at%20https%3A//github.com/prstrive/GenS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenS%253A%2520Generalizable%2520Neural%2520Surface%2520Reconstruction%2520from%2520Multi-View%2520Images%26entry.906535625%3DRui%2520Peng%2520and%2520Xiaodong%2520Gu%2520and%2520Luyang%2520Tang%2520and%2520Shihe%2520Shen%2520and%2520Fanqi%2520Yu%2520and%2520Ronggang%2520Wang%26entry.1292438233%3D%2520%2520Combining%2520the%2520signed%2520distance%2520function%2520%2528SDF%2529%2520and%2520differentiable%2520volume%250Arendering%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520surface%2520reconstruction%2520from%250Amulti-view%2520images%2520without%25203D%2520supervision.%2520However%252C%2520current%2520methods%2520are%2520impeded%250Aby%2520requiring%2520long-time%2520per-scene%2520optimizations%2520and%2520cannot%2520generalize%2520to%2520new%250Ascenes.%2520In%2520this%2520paper%252C%2520we%2520present%2520GenS%252C%2520an%2520end-to-end%2520generalizable%2520neural%250Asurface%2520reconstruction%2520model.%2520Unlike%2520coordinate-based%2520methods%2520that%2520train%2520a%250Aseparate%2520network%2520for%2520each%2520scene%252C%2520we%2520construct%2520a%2520generalized%2520multi-scale%2520volume%250Ato%2520directly%2520encode%2520all%2520scenes.%2520Compared%2520with%2520existing%2520solutions%252C%2520our%250Arepresentation%2520is%2520more%2520powerful%252C%2520which%2520can%2520recover%2520high-frequency%2520details%2520while%250Amaintaining%2520global%2520smoothness.%2520Meanwhile%252C%2520we%2520introduce%2520a%2520multi-scale%250Afeature-metric%2520consistency%2520to%2520impose%2520the%2520multi-view%2520consistency%2520in%2520a%2520more%250Adiscriminative%2520multi-scale%2520feature%2520space%252C%2520which%2520is%2520robust%2520to%2520the%2520failures%2520of%250Athe%2520photometric%2520consistency.%2520And%2520the%2520learnable%2520feature%2520can%2520be%2520self-enhanced%2520to%250Acontinuously%2520improve%2520the%2520matching%2520accuracy%2520and%2520mitigate%2520aggregation%2520ambiguity.%250AFurthermore%252C%2520we%2520design%2520a%2520view%2520contrast%2520loss%2520to%2520force%2520the%2520model%2520to%2520be%2520robust%2520to%250Athose%2520regions%2520covered%2520by%2520few%2520viewpoints%2520through%2520distilling%2520the%2520geometric%2520prior%250Afrom%2520dense%2520input%2520to%2520sparse%2520input.%2520Extensive%2520experiments%2520on%2520popular%2520benchmarks%250Ashow%2520that%2520our%2520model%2520can%2520generalize%2520well%2520to%2520new%2520scenes%2520and%2520outperform%2520existing%250Astate-of-the-art%2520methods%2520even%2520those%2520employing%2520ground-truth%2520depth%2520supervision.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/prstrive/GenS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenS%3A%20Generalizable%20Neural%20Surface%20Reconstruction%20from%20Multi-View%20Images&entry.906535625=Rui%20Peng%20and%20Xiaodong%20Gu%20and%20Luyang%20Tang%20and%20Shihe%20Shen%20and%20Fanqi%20Yu%20and%20Ronggang%20Wang&entry.1292438233=%20%20Combining%20the%20signed%20distance%20function%20%28SDF%29%20and%20differentiable%20volume%0Arendering%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20surface%20reconstruction%20from%0Amulti-view%20images%20without%203D%20supervision.%20However%2C%20current%20methods%20are%20impeded%0Aby%20requiring%20long-time%20per-scene%20optimizations%20and%20cannot%20generalize%20to%20new%0Ascenes.%20In%20this%20paper%2C%20we%20present%20GenS%2C%20an%20end-to-end%20generalizable%20neural%0Asurface%20reconstruction%20model.%20Unlike%20coordinate-based%20methods%20that%20train%20a%0Aseparate%20network%20for%20each%20scene%2C%20we%20construct%20a%20generalized%20multi-scale%20volume%0Ato%20directly%20encode%20all%20scenes.%20Compared%20with%20existing%20solutions%2C%20our%0Arepresentation%20is%20more%20powerful%2C%20which%20can%20recover%20high-frequency%20details%20while%0Amaintaining%20global%20smoothness.%20Meanwhile%2C%20we%20introduce%20a%20multi-scale%0Afeature-metric%20consistency%20to%20impose%20the%20multi-view%20consistency%20in%20a%20more%0Adiscriminative%20multi-scale%20feature%20space%2C%20which%20is%20robust%20to%20the%20failures%20of%0Athe%20photometric%20consistency.%20And%20the%20learnable%20feature%20can%20be%20self-enhanced%20to%0Acontinuously%20improve%20the%20matching%20accuracy%20and%20mitigate%20aggregation%20ambiguity.%0AFurthermore%2C%20we%20design%20a%20view%20contrast%20loss%20to%20force%20the%20model%20to%20be%20robust%20to%0Athose%20regions%20covered%20by%20few%20viewpoints%20through%20distilling%20the%20geometric%20prior%0Afrom%20dense%20input%20to%20sparse%20input.%20Extensive%20experiments%20on%20popular%20benchmarks%0Ashow%20that%20our%20model%20can%20generalize%20well%20to%20new%20scenes%20and%20outperform%20existing%0Astate-of-the-art%20methods%20even%20those%20employing%20ground-truth%20depth%20supervision.%0ACode%20is%20available%20at%20https%3A//github.com/prstrive/GenS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02495v1&entry.124074799=Read"},
{"title": "3D Gaussian Splatting with Deferred Reflection", "author": "Keyang Ye and Qiming Hou and Kun Zhou", "abstract": "  The advent of neural and Gaussian-based radiance field methods have achieved\ngreat success in the field of novel view synthesis. However, specular\nreflection remains non-trivial, as the high frequency radiance field is\nnotoriously difficult to fit stably and accurately. We present a deferred\nshading method to effectively render specular reflection with Gaussian\nsplatting. The key challenge comes from the environment map reflection model,\nwhich requires accurate surface normal while simultaneously bottlenecks normal\nestimation with discontinuous gradients. We leverage the per-pixel reflection\ngradients generated by deferred shading to bridge the optimization process of\nneighboring Gaussians, allowing nearly correct normal estimations to gradually\npropagate and eventually spread over all reflective objects. Our method\nsignificantly outperforms state-of-the-art techniques and concurrent work in\nsynthesizing high-quality specular reflection effects, demonstrating a\nconsistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic\nand real-world scenes, while running at a frame rate almost identical to\nvanilla Gaussian splatting.\n", "link": "http://arxiv.org/abs/2404.18454v2", "date": "2024-06-04", "relevancy": 3.0008, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6847}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5762}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Splatting%20with%20Deferred%20Reflection&body=Title%3A%203D%20Gaussian%20Splatting%20with%20Deferred%20Reflection%0AAuthor%3A%20Keyang%20Ye%20and%20Qiming%20Hou%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20The%20advent%20of%20neural%20and%20Gaussian-based%20radiance%20field%20methods%20have%20achieved%0Agreat%20success%20in%20the%20field%20of%20novel%20view%20synthesis.%20However%2C%20specular%0Areflection%20remains%20non-trivial%2C%20as%20the%20high%20frequency%20radiance%20field%20is%0Anotoriously%20difficult%20to%20fit%20stably%20and%20accurately.%20We%20present%20a%20deferred%0Ashading%20method%20to%20effectively%20render%20specular%20reflection%20with%20Gaussian%0Asplatting.%20The%20key%20challenge%20comes%20from%20the%20environment%20map%20reflection%20model%2C%0Awhich%20requires%20accurate%20surface%20normal%20while%20simultaneously%20bottlenecks%20normal%0Aestimation%20with%20discontinuous%20gradients.%20We%20leverage%20the%20per-pixel%20reflection%0Agradients%20generated%20by%20deferred%20shading%20to%20bridge%20the%20optimization%20process%20of%0Aneighboring%20Gaussians%2C%20allowing%20nearly%20correct%20normal%20estimations%20to%20gradually%0Apropagate%20and%20eventually%20spread%20over%20all%20reflective%20objects.%20Our%20method%0Asignificantly%20outperforms%20state-of-the-art%20techniques%20and%20concurrent%20work%20in%0Asynthesizing%20high-quality%20specular%20reflection%20effects%2C%20demonstrating%20a%0Aconsistent%20improvement%20of%20peak%20signal-to-noise%20ratio%20%28PSNR%29%20for%20both%20synthetic%0Aand%20real-world%20scenes%2C%20while%20running%20at%20a%20frame%20rate%20almost%20identical%20to%0Avanilla%20Gaussian%20splatting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18454v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Splatting%2520with%2520Deferred%2520Reflection%26entry.906535625%3DKeyang%2520Ye%2520and%2520Qiming%2520Hou%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520neural%2520and%2520Gaussian-based%2520radiance%2520field%2520methods%2520have%2520achieved%250Agreat%2520success%2520in%2520the%2520field%2520of%2520novel%2520view%2520synthesis.%2520However%252C%2520specular%250Areflection%2520remains%2520non-trivial%252C%2520as%2520the%2520high%2520frequency%2520radiance%2520field%2520is%250Anotoriously%2520difficult%2520to%2520fit%2520stably%2520and%2520accurately.%2520We%2520present%2520a%2520deferred%250Ashading%2520method%2520to%2520effectively%2520render%2520specular%2520reflection%2520with%2520Gaussian%250Asplatting.%2520The%2520key%2520challenge%2520comes%2520from%2520the%2520environment%2520map%2520reflection%2520model%252C%250Awhich%2520requires%2520accurate%2520surface%2520normal%2520while%2520simultaneously%2520bottlenecks%2520normal%250Aestimation%2520with%2520discontinuous%2520gradients.%2520We%2520leverage%2520the%2520per-pixel%2520reflection%250Agradients%2520generated%2520by%2520deferred%2520shading%2520to%2520bridge%2520the%2520optimization%2520process%2520of%250Aneighboring%2520Gaussians%252C%2520allowing%2520nearly%2520correct%2520normal%2520estimations%2520to%2520gradually%250Apropagate%2520and%2520eventually%2520spread%2520over%2520all%2520reflective%2520objects.%2520Our%2520method%250Asignificantly%2520outperforms%2520state-of-the-art%2520techniques%2520and%2520concurrent%2520work%2520in%250Asynthesizing%2520high-quality%2520specular%2520reflection%2520effects%252C%2520demonstrating%2520a%250Aconsistent%2520improvement%2520of%2520peak%2520signal-to-noise%2520ratio%2520%2528PSNR%2529%2520for%2520both%2520synthetic%250Aand%2520real-world%2520scenes%252C%2520while%2520running%2520at%2520a%2520frame%2520rate%2520almost%2520identical%2520to%250Avanilla%2520Gaussian%2520splatting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18454v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Splatting%20with%20Deferred%20Reflection&entry.906535625=Keyang%20Ye%20and%20Qiming%20Hou%20and%20Kun%20Zhou&entry.1292438233=%20%20The%20advent%20of%20neural%20and%20Gaussian-based%20radiance%20field%20methods%20have%20achieved%0Agreat%20success%20in%20the%20field%20of%20novel%20view%20synthesis.%20However%2C%20specular%0Areflection%20remains%20non-trivial%2C%20as%20the%20high%20frequency%20radiance%20field%20is%0Anotoriously%20difficult%20to%20fit%20stably%20and%20accurately.%20We%20present%20a%20deferred%0Ashading%20method%20to%20effectively%20render%20specular%20reflection%20with%20Gaussian%0Asplatting.%20The%20key%20challenge%20comes%20from%20the%20environment%20map%20reflection%20model%2C%0Awhich%20requires%20accurate%20surface%20normal%20while%20simultaneously%20bottlenecks%20normal%0Aestimation%20with%20discontinuous%20gradients.%20We%20leverage%20the%20per-pixel%20reflection%0Agradients%20generated%20by%20deferred%20shading%20to%20bridge%20the%20optimization%20process%20of%0Aneighboring%20Gaussians%2C%20allowing%20nearly%20correct%20normal%20estimations%20to%20gradually%0Apropagate%20and%20eventually%20spread%20over%20all%20reflective%20objects.%20Our%20method%0Asignificantly%20outperforms%20state-of-the-art%20techniques%20and%20concurrent%20work%20in%0Asynthesizing%20high-quality%20specular%20reflection%20effects%2C%20demonstrating%20a%0Aconsistent%20improvement%20of%20peak%20signal-to-noise%20ratio%20%28PSNR%29%20for%20both%20synthetic%0Aand%20real-world%20scenes%2C%20while%20running%20at%20a%20frame%20rate%20almost%20identical%20to%0Avanilla%20Gaussian%20splatting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18454v2&entry.124074799=Read"},
{"title": "Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance\n  Segmentation", "author": "Mohamed El Amine Boudjoghra and Angela Dai and Jean Lahoud and Hisham Cholakkal and Rao Muhammad Anwer and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Recent works on open-vocabulary 3D instance segmentation show strong promise,\nbut at the cost of slow inference speed and high computation requirements. This\nhigh computation cost is typically due to their heavy reliance on 3D clip\nfeatures, which require computationally expensive 2D foundation models like\nSegment Anything (SAM) and CLIP for multi-view aggregation into 3D. As a\nconsequence, this hampers their applicability in many real-world applications\nthat require both fast and accurate predictions. To this end, we propose a fast\nyet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO\n3D, that effectively leverages only 2D object detection from multi-view RGB\nimages for open-vocabulary 3D instance segmentation. We address this task by\ngenerating class-agnostic 3D masks for objects in the scene and associating\nthem with text prompts. We observe that the projection of class-agnostic 3D\npoint cloud instances already holds instance information; thus, using SAM might\nonly result in redundancy that unnecessarily increases the inference time. We\nempirically find that a better performance of matching text prompts to 3D masks\ncan be achieved in a faster fashion with a 2D object detector. We validate our\nOpen-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios:\n(i) with ground truth masks, where labels are required for given object\nproposals, and (ii) with class-agnostic 3D proposals generated from a 3D\nproposal network. Our Open-YOLO 3D achieves state-of-the-art performance on\nboth datasets while obtaining up to $\\sim$16$\\times$ speedup compared to the\nbest existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D\nachieves mean average precision (mAP) of 24.7\\% while operating at 22 seconds\nper scene. Code and model are available at github.com/aminebdj/OpenYOLO3D.\n", "link": "http://arxiv.org/abs/2406.02548v1", "date": "2024-06-04", "relevancy": 2.9989, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6131}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5931}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-YOLO%203D%3A%20Towards%20Fast%20and%20Accurate%20Open-Vocabulary%203D%20Instance%0A%20%20Segmentation&body=Title%3A%20Open-YOLO%203D%3A%20Towards%20Fast%20and%20Accurate%20Open-Vocabulary%203D%20Instance%0A%20%20Segmentation%0AAuthor%3A%20Mohamed%20El%20Amine%20Boudjoghra%20and%20Angela%20Dai%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Recent%20works%20on%20open-vocabulary%203D%20instance%20segmentation%20show%20strong%20promise%2C%0Abut%20at%20the%20cost%20of%20slow%20inference%20speed%20and%20high%20computation%20requirements.%20This%0Ahigh%20computation%20cost%20is%20typically%20due%20to%20their%20heavy%20reliance%20on%203D%20clip%0Afeatures%2C%20which%20require%20computationally%20expensive%202D%20foundation%20models%20like%0ASegment%20Anything%20%28SAM%29%20and%20CLIP%20for%20multi-view%20aggregation%20into%203D.%20As%20a%0Aconsequence%2C%20this%20hampers%20their%20applicability%20in%20many%20real-world%20applications%0Athat%20require%20both%20fast%20and%20accurate%20predictions.%20To%20this%20end%2C%20we%20propose%20a%20fast%0Ayet%20accurate%20open-vocabulary%203D%20instance%20segmentation%20approach%2C%20named%20Open-YOLO%0A3D%2C%20that%20effectively%20leverages%20only%202D%20object%20detection%20from%20multi-view%20RGB%0Aimages%20for%20open-vocabulary%203D%20instance%20segmentation.%20We%20address%20this%20task%20by%0Agenerating%20class-agnostic%203D%20masks%20for%20objects%20in%20the%20scene%20and%20associating%0Athem%20with%20text%20prompts.%20We%20observe%20that%20the%20projection%20of%20class-agnostic%203D%0Apoint%20cloud%20instances%20already%20holds%20instance%20information%3B%20thus%2C%20using%20SAM%20might%0Aonly%20result%20in%20redundancy%20that%20unnecessarily%20increases%20the%20inference%20time.%20We%0Aempirically%20find%20that%20a%20better%20performance%20of%20matching%20text%20prompts%20to%203D%20masks%0Acan%20be%20achieved%20in%20a%20faster%20fashion%20with%20a%202D%20object%20detector.%20We%20validate%20our%0AOpen-YOLO%203D%20on%20two%20benchmarks%2C%20ScanNet200%20and%20Replica%2C%20under%20two%20scenarios%3A%0A%28i%29%20with%20ground%20truth%20masks%2C%20where%20labels%20are%20required%20for%20given%20object%0Aproposals%2C%20and%20%28ii%29%20with%20class-agnostic%203D%20proposals%20generated%20from%20a%203D%0Aproposal%20network.%20Our%20Open-YOLO%203D%20achieves%20state-of-the-art%20performance%20on%0Aboth%20datasets%20while%20obtaining%20up%20to%20%24%5Csim%2416%24%5Ctimes%24%20speedup%20compared%20to%20the%0Abest%20existing%20method%20in%20literature.%20On%20ScanNet200%20val.%20set%2C%20our%20Open-YOLO%203D%0Aachieves%20mean%20average%20precision%20%28mAP%29%20of%2024.7%5C%25%20while%20operating%20at%2022%20seconds%0Aper%20scene.%20Code%20and%20model%20are%20available%20at%20github.com/aminebdj/OpenYOLO3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-YOLO%25203D%253A%2520Towards%2520Fast%2520and%2520Accurate%2520Open-Vocabulary%25203D%2520Instance%250A%2520%2520Segmentation%26entry.906535625%3DMohamed%2520El%2520Amine%2520Boudjoghra%2520and%2520Angela%2520Dai%2520and%2520Jean%2520Lahoud%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Salman%2520Khan%2520and%2520Fahad%2520Shahbaz%2520Khan%26entry.1292438233%3D%2520%2520Recent%2520works%2520on%2520open-vocabulary%25203D%2520instance%2520segmentation%2520show%2520strong%2520promise%252C%250Abut%2520at%2520the%2520cost%2520of%2520slow%2520inference%2520speed%2520and%2520high%2520computation%2520requirements.%2520This%250Ahigh%2520computation%2520cost%2520is%2520typically%2520due%2520to%2520their%2520heavy%2520reliance%2520on%25203D%2520clip%250Afeatures%252C%2520which%2520require%2520computationally%2520expensive%25202D%2520foundation%2520models%2520like%250ASegment%2520Anything%2520%2528SAM%2529%2520and%2520CLIP%2520for%2520multi-view%2520aggregation%2520into%25203D.%2520As%2520a%250Aconsequence%252C%2520this%2520hampers%2520their%2520applicability%2520in%2520many%2520real-world%2520applications%250Athat%2520require%2520both%2520fast%2520and%2520accurate%2520predictions.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520fast%250Ayet%2520accurate%2520open-vocabulary%25203D%2520instance%2520segmentation%2520approach%252C%2520named%2520Open-YOLO%250A3D%252C%2520that%2520effectively%2520leverages%2520only%25202D%2520object%2520detection%2520from%2520multi-view%2520RGB%250Aimages%2520for%2520open-vocabulary%25203D%2520instance%2520segmentation.%2520We%2520address%2520this%2520task%2520by%250Agenerating%2520class-agnostic%25203D%2520masks%2520for%2520objects%2520in%2520the%2520scene%2520and%2520associating%250Athem%2520with%2520text%2520prompts.%2520We%2520observe%2520that%2520the%2520projection%2520of%2520class-agnostic%25203D%250Apoint%2520cloud%2520instances%2520already%2520holds%2520instance%2520information%253B%2520thus%252C%2520using%2520SAM%2520might%250Aonly%2520result%2520in%2520redundancy%2520that%2520unnecessarily%2520increases%2520the%2520inference%2520time.%2520We%250Aempirically%2520find%2520that%2520a%2520better%2520performance%2520of%2520matching%2520text%2520prompts%2520to%25203D%2520masks%250Acan%2520be%2520achieved%2520in%2520a%2520faster%2520fashion%2520with%2520a%25202D%2520object%2520detector.%2520We%2520validate%2520our%250AOpen-YOLO%25203D%2520on%2520two%2520benchmarks%252C%2520ScanNet200%2520and%2520Replica%252C%2520under%2520two%2520scenarios%253A%250A%2528i%2529%2520with%2520ground%2520truth%2520masks%252C%2520where%2520labels%2520are%2520required%2520for%2520given%2520object%250Aproposals%252C%2520and%2520%2528ii%2529%2520with%2520class-agnostic%25203D%2520proposals%2520generated%2520from%2520a%25203D%250Aproposal%2520network.%2520Our%2520Open-YOLO%25203D%2520achieves%2520state-of-the-art%2520performance%2520on%250Aboth%2520datasets%2520while%2520obtaining%2520up%2520to%2520%2524%255Csim%252416%2524%255Ctimes%2524%2520speedup%2520compared%2520to%2520the%250Abest%2520existing%2520method%2520in%2520literature.%2520On%2520ScanNet200%2520val.%2520set%252C%2520our%2520Open-YOLO%25203D%250Aachieves%2520mean%2520average%2520precision%2520%2528mAP%2529%2520of%252024.7%255C%2525%2520while%2520operating%2520at%252022%2520seconds%250Aper%2520scene.%2520Code%2520and%2520model%2520are%2520available%2520at%2520github.com/aminebdj/OpenYOLO3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-YOLO%203D%3A%20Towards%20Fast%20and%20Accurate%20Open-Vocabulary%203D%20Instance%0A%20%20Segmentation&entry.906535625=Mohamed%20El%20Amine%20Boudjoghra%20and%20Angela%20Dai%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Recent%20works%20on%20open-vocabulary%203D%20instance%20segmentation%20show%20strong%20promise%2C%0Abut%20at%20the%20cost%20of%20slow%20inference%20speed%20and%20high%20computation%20requirements.%20This%0Ahigh%20computation%20cost%20is%20typically%20due%20to%20their%20heavy%20reliance%20on%203D%20clip%0Afeatures%2C%20which%20require%20computationally%20expensive%202D%20foundation%20models%20like%0ASegment%20Anything%20%28SAM%29%20and%20CLIP%20for%20multi-view%20aggregation%20into%203D.%20As%20a%0Aconsequence%2C%20this%20hampers%20their%20applicability%20in%20many%20real-world%20applications%0Athat%20require%20both%20fast%20and%20accurate%20predictions.%20To%20this%20end%2C%20we%20propose%20a%20fast%0Ayet%20accurate%20open-vocabulary%203D%20instance%20segmentation%20approach%2C%20named%20Open-YOLO%0A3D%2C%20that%20effectively%20leverages%20only%202D%20object%20detection%20from%20multi-view%20RGB%0Aimages%20for%20open-vocabulary%203D%20instance%20segmentation.%20We%20address%20this%20task%20by%0Agenerating%20class-agnostic%203D%20masks%20for%20objects%20in%20the%20scene%20and%20associating%0Athem%20with%20text%20prompts.%20We%20observe%20that%20the%20projection%20of%20class-agnostic%203D%0Apoint%20cloud%20instances%20already%20holds%20instance%20information%3B%20thus%2C%20using%20SAM%20might%0Aonly%20result%20in%20redundancy%20that%20unnecessarily%20increases%20the%20inference%20time.%20We%0Aempirically%20find%20that%20a%20better%20performance%20of%20matching%20text%20prompts%20to%203D%20masks%0Acan%20be%20achieved%20in%20a%20faster%20fashion%20with%20a%202D%20object%20detector.%20We%20validate%20our%0AOpen-YOLO%203D%20on%20two%20benchmarks%2C%20ScanNet200%20and%20Replica%2C%20under%20two%20scenarios%3A%0A%28i%29%20with%20ground%20truth%20masks%2C%20where%20labels%20are%20required%20for%20given%20object%0Aproposals%2C%20and%20%28ii%29%20with%20class-agnostic%203D%20proposals%20generated%20from%20a%203D%0Aproposal%20network.%20Our%20Open-YOLO%203D%20achieves%20state-of-the-art%20performance%20on%0Aboth%20datasets%20while%20obtaining%20up%20to%20%24%5Csim%2416%24%5Ctimes%24%20speedup%20compared%20to%20the%0Abest%20existing%20method%20in%20literature.%20On%20ScanNet200%20val.%20set%2C%20our%20Open-YOLO%203D%0Aachieves%20mean%20average%20precision%20%28mAP%29%20of%2024.7%5C%25%20while%20operating%20at%2022%20seconds%0Aper%20scene.%20Code%20and%20model%20are%20available%20at%20github.com/aminebdj/OpenYOLO3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02548v1&entry.124074799=Read"},
{"title": "Generative Active Learning for Long-tailed Instance Segmentation", "author": "Muzhi Zhu and Chengxiang Fan and Hao Chen and Yang Liu and Weian Mao and Xiaogang Xu and Chunhua Shen", "abstract": "  Recently, large-scale language-image generative models have gained widespread\nattention and many works have utilized generated data from these models to\nfurther enhance the performance of perception tasks. However, not all generated\ndata can positively impact downstream models, and these methods do not\nthoroughly explore how to better select and utilize generated data. On the\nother hand, there is still a lack of research oriented towards active learning\non generated data. In this paper, we explore how to perform active learning\nspecifically for generated data in the long-tailed instance segmentation task.\nSubsequently, we propose BSGAL, a new algorithm that online estimates the\ncontribution of the generated data based on gradient cache. BSGAL can handle\nunlimited generated data and complex downstream segmentation tasks effectively.\nExperiments show that BSGAL outperforms the baseline approach and effectually\nimproves the performance of long-tailed segmentation. Our code can be found at\nhttps://github.com/aim-uofa/DiverGen.\n", "link": "http://arxiv.org/abs/2406.02435v1", "date": "2024-06-04", "relevancy": 2.9671, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6122}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5901}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Active%20Learning%20for%20Long-tailed%20Instance%20Segmentation&body=Title%3A%20Generative%20Active%20Learning%20for%20Long-tailed%20Instance%20Segmentation%0AAuthor%3A%20Muzhi%20Zhu%20and%20Chengxiang%20Fan%20and%20Hao%20Chen%20and%20Yang%20Liu%20and%20Weian%20Mao%20and%20Xiaogang%20Xu%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Recently%2C%20large-scale%20language-image%20generative%20models%20have%20gained%20widespread%0Aattention%20and%20many%20works%20have%20utilized%20generated%20data%20from%20these%20models%20to%0Afurther%20enhance%20the%20performance%20of%20perception%20tasks.%20However%2C%20not%20all%20generated%0Adata%20can%20positively%20impact%20downstream%20models%2C%20and%20these%20methods%20do%20not%0Athoroughly%20explore%20how%20to%20better%20select%20and%20utilize%20generated%20data.%20On%20the%0Aother%20hand%2C%20there%20is%20still%20a%20lack%20of%20research%20oriented%20towards%20active%20learning%0Aon%20generated%20data.%20In%20this%20paper%2C%20we%20explore%20how%20to%20perform%20active%20learning%0Aspecifically%20for%20generated%20data%20in%20the%20long-tailed%20instance%20segmentation%20task.%0ASubsequently%2C%20we%20propose%20BSGAL%2C%20a%20new%20algorithm%20that%20online%20estimates%20the%0Acontribution%20of%20the%20generated%20data%20based%20on%20gradient%20cache.%20BSGAL%20can%20handle%0Aunlimited%20generated%20data%20and%20complex%20downstream%20segmentation%20tasks%20effectively.%0AExperiments%20show%20that%20BSGAL%20outperforms%20the%20baseline%20approach%20and%20effectually%0Aimproves%20the%20performance%20of%20long-tailed%20segmentation.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/aim-uofa/DiverGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Active%2520Learning%2520for%2520Long-tailed%2520Instance%2520Segmentation%26entry.906535625%3DMuzhi%2520Zhu%2520and%2520Chengxiang%2520Fan%2520and%2520Hao%2520Chen%2520and%2520Yang%2520Liu%2520and%2520Weian%2520Mao%2520and%2520Xiaogang%2520Xu%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Recently%252C%2520large-scale%2520language-image%2520generative%2520models%2520have%2520gained%2520widespread%250Aattention%2520and%2520many%2520works%2520have%2520utilized%2520generated%2520data%2520from%2520these%2520models%2520to%250Afurther%2520enhance%2520the%2520performance%2520of%2520perception%2520tasks.%2520However%252C%2520not%2520all%2520generated%250Adata%2520can%2520positively%2520impact%2520downstream%2520models%252C%2520and%2520these%2520methods%2520do%2520not%250Athoroughly%2520explore%2520how%2520to%2520better%2520select%2520and%2520utilize%2520generated%2520data.%2520On%2520the%250Aother%2520hand%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520research%2520oriented%2520towards%2520active%2520learning%250Aon%2520generated%2520data.%2520In%2520this%2520paper%252C%2520we%2520explore%2520how%2520to%2520perform%2520active%2520learning%250Aspecifically%2520for%2520generated%2520data%2520in%2520the%2520long-tailed%2520instance%2520segmentation%2520task.%250ASubsequently%252C%2520we%2520propose%2520BSGAL%252C%2520a%2520new%2520algorithm%2520that%2520online%2520estimates%2520the%250Acontribution%2520of%2520the%2520generated%2520data%2520based%2520on%2520gradient%2520cache.%2520BSGAL%2520can%2520handle%250Aunlimited%2520generated%2520data%2520and%2520complex%2520downstream%2520segmentation%2520tasks%2520effectively.%250AExperiments%2520show%2520that%2520BSGAL%2520outperforms%2520the%2520baseline%2520approach%2520and%2520effectually%250Aimproves%2520the%2520performance%2520of%2520long-tailed%2520segmentation.%2520Our%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/aim-uofa/DiverGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Active%20Learning%20for%20Long-tailed%20Instance%20Segmentation&entry.906535625=Muzhi%20Zhu%20and%20Chengxiang%20Fan%20and%20Hao%20Chen%20and%20Yang%20Liu%20and%20Weian%20Mao%20and%20Xiaogang%20Xu%20and%20Chunhua%20Shen&entry.1292438233=%20%20Recently%2C%20large-scale%20language-image%20generative%20models%20have%20gained%20widespread%0Aattention%20and%20many%20works%20have%20utilized%20generated%20data%20from%20these%20models%20to%0Afurther%20enhance%20the%20performance%20of%20perception%20tasks.%20However%2C%20not%20all%20generated%0Adata%20can%20positively%20impact%20downstream%20models%2C%20and%20these%20methods%20do%20not%0Athoroughly%20explore%20how%20to%20better%20select%20and%20utilize%20generated%20data.%20On%20the%0Aother%20hand%2C%20there%20is%20still%20a%20lack%20of%20research%20oriented%20towards%20active%20learning%0Aon%20generated%20data.%20In%20this%20paper%2C%20we%20explore%20how%20to%20perform%20active%20learning%0Aspecifically%20for%20generated%20data%20in%20the%20long-tailed%20instance%20segmentation%20task.%0ASubsequently%2C%20we%20propose%20BSGAL%2C%20a%20new%20algorithm%20that%20online%20estimates%20the%0Acontribution%20of%20the%20generated%20data%20based%20on%20gradient%20cache.%20BSGAL%20can%20handle%0Aunlimited%20generated%20data%20and%20complex%20downstream%20segmentation%20tasks%20effectively.%0AExperiments%20show%20that%20BSGAL%20outperforms%20the%20baseline%20approach%20and%20effectually%0Aimproves%20the%20performance%20of%20long-tailed%20segmentation.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/aim-uofa/DiverGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02435v1&entry.124074799=Read"},
{"title": "V-Express: Conditional Dropout for Progressive Training of Portrait\n  Video Generation", "author": "Cong Wang and Kuan Tian and Jun Zhang and Yonghang Guan and Feng Luo and Fei Shen and Zhiwei Jiang and Qing Gu and Xiao Han and Wei Yang", "abstract": "  In the field of portrait video generation, the use of single images to\ngenerate portrait videos has become increasingly prevalent. A common approach\ninvolves leveraging generative models to enhance adapters for controlled\ngeneration. However, control signals (e.g., text, audio, reference image, pose,\ndepth map, etc.) can vary in strength. Among these, weaker conditions often\nstruggle to be effective due to interference from stronger conditions, posing a\nchallenge in balancing these conditions. In our work on portrait video\ngeneration, we identified audio signals as particularly weak, often\novershadowed by stronger signals such as facial pose and reference image.\nHowever, direct training with weak signals often leads to difficulties in\nconvergence. To address this, we propose V-Express, a simple method that\nbalances different control signals through the progressive training and the\nconditional dropout operation. Our method gradually enables effective control\nby weak conditions, thereby achieving generation capabilities that\nsimultaneously take into account the facial pose, reference image, and audio.\nThe experimental results demonstrate that our method can effectively generate\nportrait videos controlled by audio. Furthermore, a potential solution is\nprovided for the simultaneous and effective use of conditions of varying\nstrengths.\n", "link": "http://arxiv.org/abs/2406.02511v1", "date": "2024-06-04", "relevancy": 2.9075, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6194}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5667}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-Express%3A%20Conditional%20Dropout%20for%20Progressive%20Training%20of%20Portrait%0A%20%20Video%20Generation&body=Title%3A%20V-Express%3A%20Conditional%20Dropout%20for%20Progressive%20Training%20of%20Portrait%0A%20%20Video%20Generation%0AAuthor%3A%20Cong%20Wang%20and%20Kuan%20Tian%20and%20Jun%20Zhang%20and%20Yonghang%20Guan%20and%20Feng%20Luo%20and%20Fei%20Shen%20and%20Zhiwei%20Jiang%20and%20Qing%20Gu%20and%20Xiao%20Han%20and%20Wei%20Yang%0AAbstract%3A%20%20%20In%20the%20field%20of%20portrait%20video%20generation%2C%20the%20use%20of%20single%20images%20to%0Agenerate%20portrait%20videos%20has%20become%20increasingly%20prevalent.%20A%20common%20approach%0Ainvolves%20leveraging%20generative%20models%20to%20enhance%20adapters%20for%20controlled%0Ageneration.%20However%2C%20control%20signals%20%28e.g.%2C%20text%2C%20audio%2C%20reference%20image%2C%20pose%2C%0Adepth%20map%2C%20etc.%29%20can%20vary%20in%20strength.%20Among%20these%2C%20weaker%20conditions%20often%0Astruggle%20to%20be%20effective%20due%20to%20interference%20from%20stronger%20conditions%2C%20posing%20a%0Achallenge%20in%20balancing%20these%20conditions.%20In%20our%20work%20on%20portrait%20video%0Ageneration%2C%20we%20identified%20audio%20signals%20as%20particularly%20weak%2C%20often%0Aovershadowed%20by%20stronger%20signals%20such%20as%20facial%20pose%20and%20reference%20image.%0AHowever%2C%20direct%20training%20with%20weak%20signals%20often%20leads%20to%20difficulties%20in%0Aconvergence.%20To%20address%20this%2C%20we%20propose%20V-Express%2C%20a%20simple%20method%20that%0Abalances%20different%20control%20signals%20through%20the%20progressive%20training%20and%20the%0Aconditional%20dropout%20operation.%20Our%20method%20gradually%20enables%20effective%20control%0Aby%20weak%20conditions%2C%20thereby%20achieving%20generation%20capabilities%20that%0Asimultaneously%20take%20into%20account%20the%20facial%20pose%2C%20reference%20image%2C%20and%20audio.%0AThe%20experimental%20results%20demonstrate%20that%20our%20method%20can%20effectively%20generate%0Aportrait%20videos%20controlled%20by%20audio.%20Furthermore%2C%20a%20potential%20solution%20is%0Aprovided%20for%20the%20simultaneous%20and%20effective%20use%20of%20conditions%20of%20varying%0Astrengths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-Express%253A%2520Conditional%2520Dropout%2520for%2520Progressive%2520Training%2520of%2520Portrait%250A%2520%2520Video%2520Generation%26entry.906535625%3DCong%2520Wang%2520and%2520Kuan%2520Tian%2520and%2520Jun%2520Zhang%2520and%2520Yonghang%2520Guan%2520and%2520Feng%2520Luo%2520and%2520Fei%2520Shen%2520and%2520Zhiwei%2520Jiang%2520and%2520Qing%2520Gu%2520and%2520Xiao%2520Han%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520portrait%2520video%2520generation%252C%2520the%2520use%2520of%2520single%2520images%2520to%250Agenerate%2520portrait%2520videos%2520has%2520become%2520increasingly%2520prevalent.%2520A%2520common%2520approach%250Ainvolves%2520leveraging%2520generative%2520models%2520to%2520enhance%2520adapters%2520for%2520controlled%250Ageneration.%2520However%252C%2520control%2520signals%2520%2528e.g.%252C%2520text%252C%2520audio%252C%2520reference%2520image%252C%2520pose%252C%250Adepth%2520map%252C%2520etc.%2529%2520can%2520vary%2520in%2520strength.%2520Among%2520these%252C%2520weaker%2520conditions%2520often%250Astruggle%2520to%2520be%2520effective%2520due%2520to%2520interference%2520from%2520stronger%2520conditions%252C%2520posing%2520a%250Achallenge%2520in%2520balancing%2520these%2520conditions.%2520In%2520our%2520work%2520on%2520portrait%2520video%250Ageneration%252C%2520we%2520identified%2520audio%2520signals%2520as%2520particularly%2520weak%252C%2520often%250Aovershadowed%2520by%2520stronger%2520signals%2520such%2520as%2520facial%2520pose%2520and%2520reference%2520image.%250AHowever%252C%2520direct%2520training%2520with%2520weak%2520signals%2520often%2520leads%2520to%2520difficulties%2520in%250Aconvergence.%2520To%2520address%2520this%252C%2520we%2520propose%2520V-Express%252C%2520a%2520simple%2520method%2520that%250Abalances%2520different%2520control%2520signals%2520through%2520the%2520progressive%2520training%2520and%2520the%250Aconditional%2520dropout%2520operation.%2520Our%2520method%2520gradually%2520enables%2520effective%2520control%250Aby%2520weak%2520conditions%252C%2520thereby%2520achieving%2520generation%2520capabilities%2520that%250Asimultaneously%2520take%2520into%2520account%2520the%2520facial%2520pose%252C%2520reference%2520image%252C%2520and%2520audio.%250AThe%2520experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520can%2520effectively%2520generate%250Aportrait%2520videos%2520controlled%2520by%2520audio.%2520Furthermore%252C%2520a%2520potential%2520solution%2520is%250Aprovided%2520for%2520the%2520simultaneous%2520and%2520effective%2520use%2520of%2520conditions%2520of%2520varying%250Astrengths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-Express%3A%20Conditional%20Dropout%20for%20Progressive%20Training%20of%20Portrait%0A%20%20Video%20Generation&entry.906535625=Cong%20Wang%20and%20Kuan%20Tian%20and%20Jun%20Zhang%20and%20Yonghang%20Guan%20and%20Feng%20Luo%20and%20Fei%20Shen%20and%20Zhiwei%20Jiang%20and%20Qing%20Gu%20and%20Xiao%20Han%20and%20Wei%20Yang&entry.1292438233=%20%20In%20the%20field%20of%20portrait%20video%20generation%2C%20the%20use%20of%20single%20images%20to%0Agenerate%20portrait%20videos%20has%20become%20increasingly%20prevalent.%20A%20common%20approach%0Ainvolves%20leveraging%20generative%20models%20to%20enhance%20adapters%20for%20controlled%0Ageneration.%20However%2C%20control%20signals%20%28e.g.%2C%20text%2C%20audio%2C%20reference%20image%2C%20pose%2C%0Adepth%20map%2C%20etc.%29%20can%20vary%20in%20strength.%20Among%20these%2C%20weaker%20conditions%20often%0Astruggle%20to%20be%20effective%20due%20to%20interference%20from%20stronger%20conditions%2C%20posing%20a%0Achallenge%20in%20balancing%20these%20conditions.%20In%20our%20work%20on%20portrait%20video%0Ageneration%2C%20we%20identified%20audio%20signals%20as%20particularly%20weak%2C%20often%0Aovershadowed%20by%20stronger%20signals%20such%20as%20facial%20pose%20and%20reference%20image.%0AHowever%2C%20direct%20training%20with%20weak%20signals%20often%20leads%20to%20difficulties%20in%0Aconvergence.%20To%20address%20this%2C%20we%20propose%20V-Express%2C%20a%20simple%20method%20that%0Abalances%20different%20control%20signals%20through%20the%20progressive%20training%20and%20the%0Aconditional%20dropout%20operation.%20Our%20method%20gradually%20enables%20effective%20control%0Aby%20weak%20conditions%2C%20thereby%20achieving%20generation%20capabilities%20that%0Asimultaneously%20take%20into%20account%20the%20facial%20pose%2C%20reference%20image%2C%20and%20audio.%0AThe%20experimental%20results%20demonstrate%20that%20our%20method%20can%20effectively%20generate%0Aportrait%20videos%20controlled%20by%20audio.%20Furthermore%2C%20a%20potential%20solution%20is%0Aprovided%20for%20the%20simultaneous%20and%20effective%20use%20of%20conditions%20of%20varying%0Astrengths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02511v1&entry.124074799=Read"},
{"title": "RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting", "author": "Qi Wang and Ruijie Lu and Xudong Xu and Jingbo Wang and Michael Yu Wang and Bo Dai and Gang Zeng and Dan Xu", "abstract": "  The advancement of diffusion models has pushed the boundary of text-to-3D\nobject generation. While it is straightforward to composite objects into a\nscene with reasonable geometry, it is nontrivial to texture such a scene\nperfectly due to style inconsistency and occlusions between objects. To tackle\nthese problems, we propose a coarse-to-fine 3D scene texturing framework,\nreferred to as RoomTex, to generate high-fidelity and style-consistent textures\nfor untextured compositional scene meshes. In the coarse stage, RoomTex first\nunwraps the scene mesh to a panoramic depth map and leverages ControlNet to\ngenerate a room panorama, which is regarded as the coarse reference to ensure\nthe global texture consistency. In the fine stage, based on the panoramic image\nand perspective depth maps, RoomTex will refine and texture every single object\nin the room iteratively along a series of selected camera views, until this\nobject is completely painted. Moreover, we propose to maintain superior\nalignment between RGB and depth spaces via subtle edge detection methods.\nExtensive experiments show our method is capable of generating high-quality and\ndiverse room textures, and more importantly, supporting interactive\nfine-grained texture control and flexible scene editing thanks to our\ninpainting-based framework and compositional mesh input. Our project page is\navailable at https://qwang666.github.io/RoomTex/.\n", "link": "http://arxiv.org/abs/2406.02461v1", "date": "2024-06-04", "relevancy": 2.882, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5939}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5939}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoomTex%3A%20Texturing%20Compositional%20Indoor%20Scenes%20via%20Iterative%20Inpainting&body=Title%3A%20RoomTex%3A%20Texturing%20Compositional%20Indoor%20Scenes%20via%20Iterative%20Inpainting%0AAuthor%3A%20Qi%20Wang%20and%20Ruijie%20Lu%20and%20Xudong%20Xu%20and%20Jingbo%20Wang%20and%20Michael%20Yu%20Wang%20and%20Bo%20Dai%20and%20Gang%20Zeng%20and%20Dan%20Xu%0AAbstract%3A%20%20%20The%20advancement%20of%20diffusion%20models%20has%20pushed%20the%20boundary%20of%20text-to-3D%0Aobject%20generation.%20While%20it%20is%20straightforward%20to%20composite%20objects%20into%20a%0Ascene%20with%20reasonable%20geometry%2C%20it%20is%20nontrivial%20to%20texture%20such%20a%20scene%0Aperfectly%20due%20to%20style%20inconsistency%20and%20occlusions%20between%20objects.%20To%20tackle%0Athese%20problems%2C%20we%20propose%20a%20coarse-to-fine%203D%20scene%20texturing%20framework%2C%0Areferred%20to%20as%20RoomTex%2C%20to%20generate%20high-fidelity%20and%20style-consistent%20textures%0Afor%20untextured%20compositional%20scene%20meshes.%20In%20the%20coarse%20stage%2C%20RoomTex%20first%0Aunwraps%20the%20scene%20mesh%20to%20a%20panoramic%20depth%20map%20and%20leverages%20ControlNet%20to%0Agenerate%20a%20room%20panorama%2C%20which%20is%20regarded%20as%20the%20coarse%20reference%20to%20ensure%0Athe%20global%20texture%20consistency.%20In%20the%20fine%20stage%2C%20based%20on%20the%20panoramic%20image%0Aand%20perspective%20depth%20maps%2C%20RoomTex%20will%20refine%20and%20texture%20every%20single%20object%0Ain%20the%20room%20iteratively%20along%20a%20series%20of%20selected%20camera%20views%2C%20until%20this%0Aobject%20is%20completely%20painted.%20Moreover%2C%20we%20propose%20to%20maintain%20superior%0Aalignment%20between%20RGB%20and%20depth%20spaces%20via%20subtle%20edge%20detection%20methods.%0AExtensive%20experiments%20show%20our%20method%20is%20capable%20of%20generating%20high-quality%20and%0Adiverse%20room%20textures%2C%20and%20more%20importantly%2C%20supporting%20interactive%0Afine-grained%20texture%20control%20and%20flexible%20scene%20editing%20thanks%20to%20our%0Ainpainting-based%20framework%20and%20compositional%20mesh%20input.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//qwang666.github.io/RoomTex/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoomTex%253A%2520Texturing%2520Compositional%2520Indoor%2520Scenes%2520via%2520Iterative%2520Inpainting%26entry.906535625%3DQi%2520Wang%2520and%2520Ruijie%2520Lu%2520and%2520Xudong%2520Xu%2520and%2520Jingbo%2520Wang%2520and%2520Michael%2520Yu%2520Wang%2520and%2520Bo%2520Dai%2520and%2520Gang%2520Zeng%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520diffusion%2520models%2520has%2520pushed%2520the%2520boundary%2520of%2520text-to-3D%250Aobject%2520generation.%2520While%2520it%2520is%2520straightforward%2520to%2520composite%2520objects%2520into%2520a%250Ascene%2520with%2520reasonable%2520geometry%252C%2520it%2520is%2520nontrivial%2520to%2520texture%2520such%2520a%2520scene%250Aperfectly%2520due%2520to%2520style%2520inconsistency%2520and%2520occlusions%2520between%2520objects.%2520To%2520tackle%250Athese%2520problems%252C%2520we%2520propose%2520a%2520coarse-to-fine%25203D%2520scene%2520texturing%2520framework%252C%250Areferred%2520to%2520as%2520RoomTex%252C%2520to%2520generate%2520high-fidelity%2520and%2520style-consistent%2520textures%250Afor%2520untextured%2520compositional%2520scene%2520meshes.%2520In%2520the%2520coarse%2520stage%252C%2520RoomTex%2520first%250Aunwraps%2520the%2520scene%2520mesh%2520to%2520a%2520panoramic%2520depth%2520map%2520and%2520leverages%2520ControlNet%2520to%250Agenerate%2520a%2520room%2520panorama%252C%2520which%2520is%2520regarded%2520as%2520the%2520coarse%2520reference%2520to%2520ensure%250Athe%2520global%2520texture%2520consistency.%2520In%2520the%2520fine%2520stage%252C%2520based%2520on%2520the%2520panoramic%2520image%250Aand%2520perspective%2520depth%2520maps%252C%2520RoomTex%2520will%2520refine%2520and%2520texture%2520every%2520single%2520object%250Ain%2520the%2520room%2520iteratively%2520along%2520a%2520series%2520of%2520selected%2520camera%2520views%252C%2520until%2520this%250Aobject%2520is%2520completely%2520painted.%2520Moreover%252C%2520we%2520propose%2520to%2520maintain%2520superior%250Aalignment%2520between%2520RGB%2520and%2520depth%2520spaces%2520via%2520subtle%2520edge%2520detection%2520methods.%250AExtensive%2520experiments%2520show%2520our%2520method%2520is%2520capable%2520of%2520generating%2520high-quality%2520and%250Adiverse%2520room%2520textures%252C%2520and%2520more%2520importantly%252C%2520supporting%2520interactive%250Afine-grained%2520texture%2520control%2520and%2520flexible%2520scene%2520editing%2520thanks%2520to%2520our%250Ainpainting-based%2520framework%2520and%2520compositional%2520mesh%2520input.%2520Our%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//qwang666.github.io/RoomTex/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoomTex%3A%20Texturing%20Compositional%20Indoor%20Scenes%20via%20Iterative%20Inpainting&entry.906535625=Qi%20Wang%20and%20Ruijie%20Lu%20and%20Xudong%20Xu%20and%20Jingbo%20Wang%20and%20Michael%20Yu%20Wang%20and%20Bo%20Dai%20and%20Gang%20Zeng%20and%20Dan%20Xu&entry.1292438233=%20%20The%20advancement%20of%20diffusion%20models%20has%20pushed%20the%20boundary%20of%20text-to-3D%0Aobject%20generation.%20While%20it%20is%20straightforward%20to%20composite%20objects%20into%20a%0Ascene%20with%20reasonable%20geometry%2C%20it%20is%20nontrivial%20to%20texture%20such%20a%20scene%0Aperfectly%20due%20to%20style%20inconsistency%20and%20occlusions%20between%20objects.%20To%20tackle%0Athese%20problems%2C%20we%20propose%20a%20coarse-to-fine%203D%20scene%20texturing%20framework%2C%0Areferred%20to%20as%20RoomTex%2C%20to%20generate%20high-fidelity%20and%20style-consistent%20textures%0Afor%20untextured%20compositional%20scene%20meshes.%20In%20the%20coarse%20stage%2C%20RoomTex%20first%0Aunwraps%20the%20scene%20mesh%20to%20a%20panoramic%20depth%20map%20and%20leverages%20ControlNet%20to%0Agenerate%20a%20room%20panorama%2C%20which%20is%20regarded%20as%20the%20coarse%20reference%20to%20ensure%0Athe%20global%20texture%20consistency.%20In%20the%20fine%20stage%2C%20based%20on%20the%20panoramic%20image%0Aand%20perspective%20depth%20maps%2C%20RoomTex%20will%20refine%20and%20texture%20every%20single%20object%0Ain%20the%20room%20iteratively%20along%20a%20series%20of%20selected%20camera%20views%2C%20until%20this%0Aobject%20is%20completely%20painted.%20Moreover%2C%20we%20propose%20to%20maintain%20superior%0Aalignment%20between%20RGB%20and%20depth%20spaces%20via%20subtle%20edge%20detection%20methods.%0AExtensive%20experiments%20show%20our%20method%20is%20capable%20of%20generating%20high-quality%20and%0Adiverse%20room%20textures%2C%20and%20more%20importantly%2C%20supporting%20interactive%0Afine-grained%20texture%20control%20and%20flexible%20scene%20editing%20thanks%20to%20our%0Ainpainting-based%20framework%20and%20compositional%20mesh%20input.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//qwang666.github.io/RoomTex/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02461v1&entry.124074799=Read"},
{"title": "I4VGen: Image as Stepping Stone for Text-to-Video Generation", "author": "Xiefan Guo and Jinlin Liu and Miaomiao Cui and Di Huang", "abstract": "  Text-to-video generation has lagged behind text-to-image synthesis in quality\nand diversity due to the complexity of spatio-temporal modeling and limited\nvideo-text datasets. This paper presents I4VGen, a training-free and\nplug-and-play video diffusion inference framework, which enhances text-to-video\ngeneration by leveraging robust image techniques. Specifically, following\ntext-to-image-to-video, I4VGen decomposes the text-to-video generation into two\nstages: anchor image synthesis and anchor image-guided video synthesis.\nCorrespondingly, a well-designed generation-selection pipeline is employed to\nachieve visually-realistic and semantically-faithful anchor image, and an\ninnovative Noise-Invariant Video Score Distillation Sampling is incorporated to\nanimate the image to a dynamic video, followed by a video regeneration process\nto refine the video. This inference strategy effectively mitigates the\nprevalent issue of non-zero terminal signal-to-noise ratio. Extensive\nevaluations show that I4VGen not only produces videos with higher visual\nrealism and textual fidelity but also integrates seamlessly into existing\nimage-to-video diffusion models, thereby improving overall video quality.\n", "link": "http://arxiv.org/abs/2406.02230v1", "date": "2024-06-04", "relevancy": 2.8046, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7156}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.7009}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I4VGen%3A%20Image%20as%20Stepping%20Stone%20for%20Text-to-Video%20Generation&body=Title%3A%20I4VGen%3A%20Image%20as%20Stepping%20Stone%20for%20Text-to-Video%20Generation%0AAuthor%3A%20Xiefan%20Guo%20and%20Jinlin%20Liu%20and%20Miaomiao%20Cui%20and%20Di%20Huang%0AAbstract%3A%20%20%20Text-to-video%20generation%20has%20lagged%20behind%20text-to-image%20synthesis%20in%20quality%0Aand%20diversity%20due%20to%20the%20complexity%20of%20spatio-temporal%20modeling%20and%20limited%0Avideo-text%20datasets.%20This%20paper%20presents%20I4VGen%2C%20a%20training-free%20and%0Aplug-and-play%20video%20diffusion%20inference%20framework%2C%20which%20enhances%20text-to-video%0Ageneration%20by%20leveraging%20robust%20image%20techniques.%20Specifically%2C%20following%0Atext-to-image-to-video%2C%20I4VGen%20decomposes%20the%20text-to-video%20generation%20into%20two%0Astages%3A%20anchor%20image%20synthesis%20and%20anchor%20image-guided%20video%20synthesis.%0ACorrespondingly%2C%20a%20well-designed%20generation-selection%20pipeline%20is%20employed%20to%0Aachieve%20visually-realistic%20and%20semantically-faithful%20anchor%20image%2C%20and%20an%0Ainnovative%20Noise-Invariant%20Video%20Score%20Distillation%20Sampling%20is%20incorporated%20to%0Aanimate%20the%20image%20to%20a%20dynamic%20video%2C%20followed%20by%20a%20video%20regeneration%20process%0Ato%20refine%20the%20video.%20This%20inference%20strategy%20effectively%20mitigates%20the%0Aprevalent%20issue%20of%20non-zero%20terminal%20signal-to-noise%20ratio.%20Extensive%0Aevaluations%20show%20that%20I4VGen%20not%20only%20produces%20videos%20with%20higher%20visual%0Arealism%20and%20textual%20fidelity%20but%20also%20integrates%20seamlessly%20into%20existing%0Aimage-to-video%20diffusion%20models%2C%20thereby%20improving%20overall%20video%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI4VGen%253A%2520Image%2520as%2520Stepping%2520Stone%2520for%2520Text-to-Video%2520Generation%26entry.906535625%3DXiefan%2520Guo%2520and%2520Jinlin%2520Liu%2520and%2520Miaomiao%2520Cui%2520and%2520Di%2520Huang%26entry.1292438233%3D%2520%2520Text-to-video%2520generation%2520has%2520lagged%2520behind%2520text-to-image%2520synthesis%2520in%2520quality%250Aand%2520diversity%2520due%2520to%2520the%2520complexity%2520of%2520spatio-temporal%2520modeling%2520and%2520limited%250Avideo-text%2520datasets.%2520This%2520paper%2520presents%2520I4VGen%252C%2520a%2520training-free%2520and%250Aplug-and-play%2520video%2520diffusion%2520inference%2520framework%252C%2520which%2520enhances%2520text-to-video%250Ageneration%2520by%2520leveraging%2520robust%2520image%2520techniques.%2520Specifically%252C%2520following%250Atext-to-image-to-video%252C%2520I4VGen%2520decomposes%2520the%2520text-to-video%2520generation%2520into%2520two%250Astages%253A%2520anchor%2520image%2520synthesis%2520and%2520anchor%2520image-guided%2520video%2520synthesis.%250ACorrespondingly%252C%2520a%2520well-designed%2520generation-selection%2520pipeline%2520is%2520employed%2520to%250Aachieve%2520visually-realistic%2520and%2520semantically-faithful%2520anchor%2520image%252C%2520and%2520an%250Ainnovative%2520Noise-Invariant%2520Video%2520Score%2520Distillation%2520Sampling%2520is%2520incorporated%2520to%250Aanimate%2520the%2520image%2520to%2520a%2520dynamic%2520video%252C%2520followed%2520by%2520a%2520video%2520regeneration%2520process%250Ato%2520refine%2520the%2520video.%2520This%2520inference%2520strategy%2520effectively%2520mitigates%2520the%250Aprevalent%2520issue%2520of%2520non-zero%2520terminal%2520signal-to-noise%2520ratio.%2520Extensive%250Aevaluations%2520show%2520that%2520I4VGen%2520not%2520only%2520produces%2520videos%2520with%2520higher%2520visual%250Arealism%2520and%2520textual%2520fidelity%2520but%2520also%2520integrates%2520seamlessly%2520into%2520existing%250Aimage-to-video%2520diffusion%2520models%252C%2520thereby%2520improving%2520overall%2520video%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I4VGen%3A%20Image%20as%20Stepping%20Stone%20for%20Text-to-Video%20Generation&entry.906535625=Xiefan%20Guo%20and%20Jinlin%20Liu%20and%20Miaomiao%20Cui%20and%20Di%20Huang&entry.1292438233=%20%20Text-to-video%20generation%20has%20lagged%20behind%20text-to-image%20synthesis%20in%20quality%0Aand%20diversity%20due%20to%20the%20complexity%20of%20spatio-temporal%20modeling%20and%20limited%0Avideo-text%20datasets.%20This%20paper%20presents%20I4VGen%2C%20a%20training-free%20and%0Aplug-and-play%20video%20diffusion%20inference%20framework%2C%20which%20enhances%20text-to-video%0Ageneration%20by%20leveraging%20robust%20image%20techniques.%20Specifically%2C%20following%0Atext-to-image-to-video%2C%20I4VGen%20decomposes%20the%20text-to-video%20generation%20into%20two%0Astages%3A%20anchor%20image%20synthesis%20and%20anchor%20image-guided%20video%20synthesis.%0ACorrespondingly%2C%20a%20well-designed%20generation-selection%20pipeline%20is%20employed%20to%0Aachieve%20visually-realistic%20and%20semantically-faithful%20anchor%20image%2C%20and%20an%0Ainnovative%20Noise-Invariant%20Video%20Score%20Distillation%20Sampling%20is%20incorporated%20to%0Aanimate%20the%20image%20to%20a%20dynamic%20video%2C%20followed%20by%20a%20video%20regeneration%20process%0Ato%20refine%20the%20video.%20This%20inference%20strategy%20effectively%20mitigates%20the%0Aprevalent%20issue%20of%20non-zero%20terminal%20signal-to-noise%20ratio.%20Extensive%0Aevaluations%20show%20that%20I4VGen%20not%20only%20produces%20videos%20with%20higher%20visual%0Arealism%20and%20textual%20fidelity%20but%20also%20integrates%20seamlessly%20into%20existing%0Aimage-to-video%20diffusion%20models%2C%20thereby%20improving%20overall%20video%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02230v1&entry.124074799=Read"},
{"title": "A Probabilistic Model behind Self-Supervised Learning", "author": "Alice Bizeul and Bernhard Sch\u00f6lkopf and Carl Allen", "abstract": "  In self-supervised learning (SSL), representations are learned via an\nauxiliary task without annotated labels. A common task is to classify\naugmentations or different modalities of the data, which share semantic content\n(e.g. an object in an image) but differ in style (e.g. the object's location).\nMany approaches to self-supervised learning have been proposed, e.g. SimCLR,\nCLIP, and VicREG, which have recently gained much attention for their\nrepresentations achieving downstream performance comparable to supervised\nlearning. However, a theoretical understanding of self-supervised methods\neludes. Addressing this, we present a generative latent variable model for\nself-supervised learning and show that several families of discriminative SSL,\nincluding contrastive methods, induce a comparable distribution over\nrepresentations, providing a unifying theoretical framework for these methods.\nThe proposed model also justifies connections drawn to mutual information and\nthe use of a \"projection head\". Learning representations by fitting the model\ngeneratively (termed SimVAE) improves performance over discriminative and other\nVAE-based methods on simple image benchmarks and significantly narrows the gap\nbetween generative and discriminative representation learning in more complex\nsettings. Importantly, as our analysis predicts, SimVAE outperforms\nself-supervised learning where style information is required, taking an\nimportant step toward understanding self-supervised methods and achieving\ntask-agnostic representations.\n", "link": "http://arxiv.org/abs/2402.01399v2", "date": "2024-06-04", "relevancy": 2.7737, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5907}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.552}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Probabilistic%20Model%20behind%20Self-Supervised%20Learning&body=Title%3A%20A%20Probabilistic%20Model%20behind%20Self-Supervised%20Learning%0AAuthor%3A%20Alice%20Bizeul%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Carl%20Allen%0AAbstract%3A%20%20%20In%20self-supervised%20learning%20%28SSL%29%2C%20representations%20are%20learned%20via%20an%0Aauxiliary%20task%20without%20annotated%20labels.%20A%20common%20task%20is%20to%20classify%0Aaugmentations%20or%20different%20modalities%20of%20the%20data%2C%20which%20share%20semantic%20content%0A%28e.g.%20an%20object%20in%20an%20image%29%20but%20differ%20in%20style%20%28e.g.%20the%20object%27s%20location%29.%0AMany%20approaches%20to%20self-supervised%20learning%20have%20been%20proposed%2C%20e.g.%20SimCLR%2C%0ACLIP%2C%20and%20VicREG%2C%20which%20have%20recently%20gained%20much%20attention%20for%20their%0Arepresentations%20achieving%20downstream%20performance%20comparable%20to%20supervised%0Alearning.%20However%2C%20a%20theoretical%20understanding%20of%20self-supervised%20methods%0Aeludes.%20Addressing%20this%2C%20we%20present%20a%20generative%20latent%20variable%20model%20for%0Aself-supervised%20learning%20and%20show%20that%20several%20families%20of%20discriminative%20SSL%2C%0Aincluding%20contrastive%20methods%2C%20induce%20a%20comparable%20distribution%20over%0Arepresentations%2C%20providing%20a%20unifying%20theoretical%20framework%20for%20these%20methods.%0AThe%20proposed%20model%20also%20justifies%20connections%20drawn%20to%20mutual%20information%20and%0Athe%20use%20of%20a%20%22projection%20head%22.%20Learning%20representations%20by%20fitting%20the%20model%0Ageneratively%20%28termed%20SimVAE%29%20improves%20performance%20over%20discriminative%20and%20other%0AVAE-based%20methods%20on%20simple%20image%20benchmarks%20and%20significantly%20narrows%20the%20gap%0Abetween%20generative%20and%20discriminative%20representation%20learning%20in%20more%20complex%0Asettings.%20Importantly%2C%20as%20our%20analysis%20predicts%2C%20SimVAE%20outperforms%0Aself-supervised%20learning%20where%20style%20information%20is%20required%2C%20taking%20an%0Aimportant%20step%20toward%20understanding%20self-supervised%20methods%20and%20achieving%0Atask-agnostic%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01399v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Probabilistic%2520Model%2520behind%2520Self-Supervised%2520Learning%26entry.906535625%3DAlice%2520Bizeul%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Carl%2520Allen%26entry.1292438233%3D%2520%2520In%2520self-supervised%2520learning%2520%2528SSL%2529%252C%2520representations%2520are%2520learned%2520via%2520an%250Aauxiliary%2520task%2520without%2520annotated%2520labels.%2520A%2520common%2520task%2520is%2520to%2520classify%250Aaugmentations%2520or%2520different%2520modalities%2520of%2520the%2520data%252C%2520which%2520share%2520semantic%2520content%250A%2528e.g.%2520an%2520object%2520in%2520an%2520image%2529%2520but%2520differ%2520in%2520style%2520%2528e.g.%2520the%2520object%2527s%2520location%2529.%250AMany%2520approaches%2520to%2520self-supervised%2520learning%2520have%2520been%2520proposed%252C%2520e.g.%2520SimCLR%252C%250ACLIP%252C%2520and%2520VicREG%252C%2520which%2520have%2520recently%2520gained%2520much%2520attention%2520for%2520their%250Arepresentations%2520achieving%2520downstream%2520performance%2520comparable%2520to%2520supervised%250Alearning.%2520However%252C%2520a%2520theoretical%2520understanding%2520of%2520self-supervised%2520methods%250Aeludes.%2520Addressing%2520this%252C%2520we%2520present%2520a%2520generative%2520latent%2520variable%2520model%2520for%250Aself-supervised%2520learning%2520and%2520show%2520that%2520several%2520families%2520of%2520discriminative%2520SSL%252C%250Aincluding%2520contrastive%2520methods%252C%2520induce%2520a%2520comparable%2520distribution%2520over%250Arepresentations%252C%2520providing%2520a%2520unifying%2520theoretical%2520framework%2520for%2520these%2520methods.%250AThe%2520proposed%2520model%2520also%2520justifies%2520connections%2520drawn%2520to%2520mutual%2520information%2520and%250Athe%2520use%2520of%2520a%2520%2522projection%2520head%2522.%2520Learning%2520representations%2520by%2520fitting%2520the%2520model%250Ageneratively%2520%2528termed%2520SimVAE%2529%2520improves%2520performance%2520over%2520discriminative%2520and%2520other%250AVAE-based%2520methods%2520on%2520simple%2520image%2520benchmarks%2520and%2520significantly%2520narrows%2520the%2520gap%250Abetween%2520generative%2520and%2520discriminative%2520representation%2520learning%2520in%2520more%2520complex%250Asettings.%2520Importantly%252C%2520as%2520our%2520analysis%2520predicts%252C%2520SimVAE%2520outperforms%250Aself-supervised%2520learning%2520where%2520style%2520information%2520is%2520required%252C%2520taking%2520an%250Aimportant%2520step%2520toward%2520understanding%2520self-supervised%2520methods%2520and%2520achieving%250Atask-agnostic%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01399v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Probabilistic%20Model%20behind%20Self-Supervised%20Learning&entry.906535625=Alice%20Bizeul%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Carl%20Allen&entry.1292438233=%20%20In%20self-supervised%20learning%20%28SSL%29%2C%20representations%20are%20learned%20via%20an%0Aauxiliary%20task%20without%20annotated%20labels.%20A%20common%20task%20is%20to%20classify%0Aaugmentations%20or%20different%20modalities%20of%20the%20data%2C%20which%20share%20semantic%20content%0A%28e.g.%20an%20object%20in%20an%20image%29%20but%20differ%20in%20style%20%28e.g.%20the%20object%27s%20location%29.%0AMany%20approaches%20to%20self-supervised%20learning%20have%20been%20proposed%2C%20e.g.%20SimCLR%2C%0ACLIP%2C%20and%20VicREG%2C%20which%20have%20recently%20gained%20much%20attention%20for%20their%0Arepresentations%20achieving%20downstream%20performance%20comparable%20to%20supervised%0Alearning.%20However%2C%20a%20theoretical%20understanding%20of%20self-supervised%20methods%0Aeludes.%20Addressing%20this%2C%20we%20present%20a%20generative%20latent%20variable%20model%20for%0Aself-supervised%20learning%20and%20show%20that%20several%20families%20of%20discriminative%20SSL%2C%0Aincluding%20contrastive%20methods%2C%20induce%20a%20comparable%20distribution%20over%0Arepresentations%2C%20providing%20a%20unifying%20theoretical%20framework%20for%20these%20methods.%0AThe%20proposed%20model%20also%20justifies%20connections%20drawn%20to%20mutual%20information%20and%0Athe%20use%20of%20a%20%22projection%20head%22.%20Learning%20representations%20by%20fitting%20the%20model%0Ageneratively%20%28termed%20SimVAE%29%20improves%20performance%20over%20discriminative%20and%20other%0AVAE-based%20methods%20on%20simple%20image%20benchmarks%20and%20significantly%20narrows%20the%20gap%0Abetween%20generative%20and%20discriminative%20representation%20learning%20in%20more%20complex%0Asettings.%20Importantly%2C%20as%20our%20analysis%20predicts%2C%20SimVAE%20outperforms%0Aself-supervised%20learning%20where%20style%20information%20is%20required%2C%20taking%20an%0Aimportant%20step%20toward%20understanding%20self-supervised%20methods%20and%20achieving%0Atask-agnostic%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01399v2&entry.124074799=Read"},
{"title": "SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection\n  Ensembles for Satellite Feature Recognition", "author": "Van Minh Nguyen and Emma Sandidge and Trupti Mahendrakar and Ryan T. White", "abstract": "  On-orbit servicing (OOS), inspection of spacecraft, and active debris removal\n(ADR). Such missions require precise rendezvous and proximity operations in the\nvicinity of non-cooperative, possibly unknown, resident space objects. Safety\nconcerns with manned missions and lag times with ground-based control\nnecessitate complete autonomy. In this article, we present an approach for\nmapping geometries and high-confidence detection of components of unknown,\nnon-cooperative satellites on orbit. We implement accelerated 3D Gaussian\nsplatting to learn a 3D representation of the satellite, render virtual views\nof the target, and ensemble the YOLOv5 object detector over the virtual views,\nresulting in reliable, accurate, and precise satellite component detections.\nThe full pipeline capable of running on-board and stand to enable downstream\nmachine intelligence tasks necessary for autonomous guidance, navigation, and\ncontrol tasks.\n", "link": "http://arxiv.org/abs/2406.02533v1", "date": "2024-06-04", "relevancy": 2.7711, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6166}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5374}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SatSplatYOLO%3A%203D%20Gaussian%20Splatting-based%20Virtual%20Object%20Detection%0A%20%20Ensembles%20for%20Satellite%20Feature%20Recognition&body=Title%3A%20SatSplatYOLO%3A%203D%20Gaussian%20Splatting-based%20Virtual%20Object%20Detection%0A%20%20Ensembles%20for%20Satellite%20Feature%20Recognition%0AAuthor%3A%20Van%20Minh%20Nguyen%20and%20Emma%20Sandidge%20and%20Trupti%20Mahendrakar%20and%20Ryan%20T.%20White%0AAbstract%3A%20%20%20On-orbit%20servicing%20%28OOS%29%2C%20inspection%20of%20spacecraft%2C%20and%20active%20debris%20removal%0A%28ADR%29.%20Such%20missions%20require%20precise%20rendezvous%20and%20proximity%20operations%20in%20the%0Avicinity%20of%20non-cooperative%2C%20possibly%20unknown%2C%20resident%20space%20objects.%20Safety%0Aconcerns%20with%20manned%20missions%20and%20lag%20times%20with%20ground-based%20control%0Anecessitate%20complete%20autonomy.%20In%20this%20article%2C%20we%20present%20an%20approach%20for%0Amapping%20geometries%20and%20high-confidence%20detection%20of%20components%20of%20unknown%2C%0Anon-cooperative%20satellites%20on%20orbit.%20We%20implement%20accelerated%203D%20Gaussian%0Asplatting%20to%20learn%20a%203D%20representation%20of%20the%20satellite%2C%20render%20virtual%20views%0Aof%20the%20target%2C%20and%20ensemble%20the%20YOLOv5%20object%20detector%20over%20the%20virtual%20views%2C%0Aresulting%20in%20reliable%2C%20accurate%2C%20and%20precise%20satellite%20component%20detections.%0AThe%20full%20pipeline%20capable%20of%20running%20on-board%20and%20stand%20to%20enable%20downstream%0Amachine%20intelligence%20tasks%20necessary%20for%20autonomous%20guidance%2C%20navigation%2C%20and%0Acontrol%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSatSplatYOLO%253A%25203D%2520Gaussian%2520Splatting-based%2520Virtual%2520Object%2520Detection%250A%2520%2520Ensembles%2520for%2520Satellite%2520Feature%2520Recognition%26entry.906535625%3DVan%2520Minh%2520Nguyen%2520and%2520Emma%2520Sandidge%2520and%2520Trupti%2520Mahendrakar%2520and%2520Ryan%2520T.%2520White%26entry.1292438233%3D%2520%2520On-orbit%2520servicing%2520%2528OOS%2529%252C%2520inspection%2520of%2520spacecraft%252C%2520and%2520active%2520debris%2520removal%250A%2528ADR%2529.%2520Such%2520missions%2520require%2520precise%2520rendezvous%2520and%2520proximity%2520operations%2520in%2520the%250Avicinity%2520of%2520non-cooperative%252C%2520possibly%2520unknown%252C%2520resident%2520space%2520objects.%2520Safety%250Aconcerns%2520with%2520manned%2520missions%2520and%2520lag%2520times%2520with%2520ground-based%2520control%250Anecessitate%2520complete%2520autonomy.%2520In%2520this%2520article%252C%2520we%2520present%2520an%2520approach%2520for%250Amapping%2520geometries%2520and%2520high-confidence%2520detection%2520of%2520components%2520of%2520unknown%252C%250Anon-cooperative%2520satellites%2520on%2520orbit.%2520We%2520implement%2520accelerated%25203D%2520Gaussian%250Asplatting%2520to%2520learn%2520a%25203D%2520representation%2520of%2520the%2520satellite%252C%2520render%2520virtual%2520views%250Aof%2520the%2520target%252C%2520and%2520ensemble%2520the%2520YOLOv5%2520object%2520detector%2520over%2520the%2520virtual%2520views%252C%250Aresulting%2520in%2520reliable%252C%2520accurate%252C%2520and%2520precise%2520satellite%2520component%2520detections.%250AThe%2520full%2520pipeline%2520capable%2520of%2520running%2520on-board%2520and%2520stand%2520to%2520enable%2520downstream%250Amachine%2520intelligence%2520tasks%2520necessary%2520for%2520autonomous%2520guidance%252C%2520navigation%252C%2520and%250Acontrol%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SatSplatYOLO%3A%203D%20Gaussian%20Splatting-based%20Virtual%20Object%20Detection%0A%20%20Ensembles%20for%20Satellite%20Feature%20Recognition&entry.906535625=Van%20Minh%20Nguyen%20and%20Emma%20Sandidge%20and%20Trupti%20Mahendrakar%20and%20Ryan%20T.%20White&entry.1292438233=%20%20On-orbit%20servicing%20%28OOS%29%2C%20inspection%20of%20spacecraft%2C%20and%20active%20debris%20removal%0A%28ADR%29.%20Such%20missions%20require%20precise%20rendezvous%20and%20proximity%20operations%20in%20the%0Avicinity%20of%20non-cooperative%2C%20possibly%20unknown%2C%20resident%20space%20objects.%20Safety%0Aconcerns%20with%20manned%20missions%20and%20lag%20times%20with%20ground-based%20control%0Anecessitate%20complete%20autonomy.%20In%20this%20article%2C%20we%20present%20an%20approach%20for%0Amapping%20geometries%20and%20high-confidence%20detection%20of%20components%20of%20unknown%2C%0Anon-cooperative%20satellites%20on%20orbit.%20We%20implement%20accelerated%203D%20Gaussian%0Asplatting%20to%20learn%20a%203D%20representation%20of%20the%20satellite%2C%20render%20virtual%20views%0Aof%20the%20target%2C%20and%20ensemble%20the%20YOLOv5%20object%20detector%20over%20the%20virtual%20views%2C%0Aresulting%20in%20reliable%2C%20accurate%2C%20and%20precise%20satellite%20component%20detections.%0AThe%20full%20pipeline%20capable%20of%20running%20on-board%20and%20stand%20to%20enable%20downstream%0Amachine%20intelligence%20tasks%20necessary%20for%20autonomous%20guidance%2C%20navigation%2C%20and%0Acontrol%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02533v1&entry.124074799=Read"},
{"title": "Enhancing Temporal Consistency in Video Editing by Reconstructing Videos\n  with 3D Gaussian Splatting", "author": "Inkyu Shin and Qihang Yu and Xiaohui Shen and In So Kweon and Kuk-Jin Yoon and Liang-Chieh Chen", "abstract": "  Recent advancements in zero-shot video diffusion models have shown promise\nfor text-driven video editing, but challenges remain in achieving high temporal\nconsistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting\n(3DGS)-based video refiner designed to enhance temporal consistency in\nzero-shot video editors. Our approach utilizes a two-stage 3D Gaussian\noptimizing process tailored for editing dynamic monocular videos. In the first\nstage, Video-3DGS employs an improved version of COLMAP, referred to as\nMC-COLMAP, which processes original videos using a Masked and Clipped approach.\nFor each video clip, MC-COLMAP generates the point clouds for dynamic\nforeground objects and complex backgrounds. These point clouds are utilized to\ninitialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent\nforeground and background views. Both foreground and background views are then\nmerged with a 2D learnable parameter map to reconstruct full views. In the\nsecond stage, we leverage the reconstruction ability developed in the first\nstage to impose the temporal constraints on the video diffusion model. To\ndemonstrate the efficacy of Video-3DGS on both stages, we conduct extensive\nexperiments across two related tasks: Video Reconstruction and Video Editing.\nVideo-3DGS trained with 3k iterations significantly improves video\nreconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency\n(x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods\non DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring\ntemporal consistency across 58 dynamic monocular videos.\n", "link": "http://arxiv.org/abs/2406.02541v1", "date": "2024-06-04", "relevancy": 2.7339, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7128}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6783}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Temporal%20Consistency%20in%20Video%20Editing%20by%20Reconstructing%20Videos%0A%20%20with%203D%20Gaussian%20Splatting&body=Title%3A%20Enhancing%20Temporal%20Consistency%20in%20Video%20Editing%20by%20Reconstructing%20Videos%0A%20%20with%203D%20Gaussian%20Splatting%0AAuthor%3A%20Inkyu%20Shin%20and%20Qihang%20Yu%20and%20Xiaohui%20Shen%20and%20In%20So%20Kweon%20and%20Kuk-Jin%20Yoon%20and%20Liang-Chieh%20Chen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20zero-shot%20video%20diffusion%20models%20have%20shown%20promise%0Afor%20text-driven%20video%20editing%2C%20but%20challenges%20remain%20in%20achieving%20high%20temporal%0Aconsistency.%20To%20address%20this%2C%20we%20introduce%20Video-3DGS%2C%20a%203D%20Gaussian%20Splatting%0A%283DGS%29-based%20video%20refiner%20designed%20to%20enhance%20temporal%20consistency%20in%0Azero-shot%20video%20editors.%20Our%20approach%20utilizes%20a%20two-stage%203D%20Gaussian%0Aoptimizing%20process%20tailored%20for%20editing%20dynamic%20monocular%20videos.%20In%20the%20first%0Astage%2C%20Video-3DGS%20employs%20an%20improved%20version%20of%20COLMAP%2C%20referred%20to%20as%0AMC-COLMAP%2C%20which%20processes%20original%20videos%20using%20a%20Masked%20and%20Clipped%20approach.%0AFor%20each%20video%20clip%2C%20MC-COLMAP%20generates%20the%20point%20clouds%20for%20dynamic%0Aforeground%20objects%20and%20complex%20backgrounds.%20These%20point%20clouds%20are%20utilized%20to%0Ainitialize%20two%20sets%20of%203D%20Gaussians%20%28Frg-3DGS%20and%20Bkg-3DGS%29%20aiming%20to%20represent%0Aforeground%20and%20background%20views.%20Both%20foreground%20and%20background%20views%20are%20then%0Amerged%20with%20a%202D%20learnable%20parameter%20map%20to%20reconstruct%20full%20views.%20In%20the%0Asecond%20stage%2C%20we%20leverage%20the%20reconstruction%20ability%20developed%20in%20the%20first%0Astage%20to%20impose%20the%20temporal%20constraints%20on%20the%20video%20diffusion%20model.%20To%0Ademonstrate%20the%20efficacy%20of%20Video-3DGS%20on%20both%20stages%2C%20we%20conduct%20extensive%0Aexperiments%20across%20two%20related%20tasks%3A%20Video%20Reconstruction%20and%20Video%20Editing.%0AVideo-3DGS%20trained%20with%203k%20iterations%20significantly%20improves%20video%0Areconstruction%20quality%20%28%2B3%20PSNR%2C%20%2B7%20PSNR%20increase%29%20and%20training%20efficiency%0A%28x1.9%2C%20x4.5%20times%20faster%29%20over%20NeRF-based%20and%203DGS-based%20state-of-art%20methods%0Aon%20DAVIS%20dataset%2C%20respectively.%20Moreover%2C%20it%20enhances%20video%20editing%20by%20ensuring%0Atemporal%20consistency%20across%2058%20dynamic%20monocular%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Temporal%2520Consistency%2520in%2520Video%2520Editing%2520by%2520Reconstructing%2520Videos%250A%2520%2520with%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DInkyu%2520Shin%2520and%2520Qihang%2520Yu%2520and%2520Xiaohui%2520Shen%2520and%2520In%2520So%2520Kweon%2520and%2520Kuk-Jin%2520Yoon%2520and%2520Liang-Chieh%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520zero-shot%2520video%2520diffusion%2520models%2520have%2520shown%2520promise%250Afor%2520text-driven%2520video%2520editing%252C%2520but%2520challenges%2520remain%2520in%2520achieving%2520high%2520temporal%250Aconsistency.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Video-3DGS%252C%2520a%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529-based%2520video%2520refiner%2520designed%2520to%2520enhance%2520temporal%2520consistency%2520in%250Azero-shot%2520video%2520editors.%2520Our%2520approach%2520utilizes%2520a%2520two-stage%25203D%2520Gaussian%250Aoptimizing%2520process%2520tailored%2520for%2520editing%2520dynamic%2520monocular%2520videos.%2520In%2520the%2520first%250Astage%252C%2520Video-3DGS%2520employs%2520an%2520improved%2520version%2520of%2520COLMAP%252C%2520referred%2520to%2520as%250AMC-COLMAP%252C%2520which%2520processes%2520original%2520videos%2520using%2520a%2520Masked%2520and%2520Clipped%2520approach.%250AFor%2520each%2520video%2520clip%252C%2520MC-COLMAP%2520generates%2520the%2520point%2520clouds%2520for%2520dynamic%250Aforeground%2520objects%2520and%2520complex%2520backgrounds.%2520These%2520point%2520clouds%2520are%2520utilized%2520to%250Ainitialize%2520two%2520sets%2520of%25203D%2520Gaussians%2520%2528Frg-3DGS%2520and%2520Bkg-3DGS%2529%2520aiming%2520to%2520represent%250Aforeground%2520and%2520background%2520views.%2520Both%2520foreground%2520and%2520background%2520views%2520are%2520then%250Amerged%2520with%2520a%25202D%2520learnable%2520parameter%2520map%2520to%2520reconstruct%2520full%2520views.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520leverage%2520the%2520reconstruction%2520ability%2520developed%2520in%2520the%2520first%250Astage%2520to%2520impose%2520the%2520temporal%2520constraints%2520on%2520the%2520video%2520diffusion%2520model.%2520To%250Ademonstrate%2520the%2520efficacy%2520of%2520Video-3DGS%2520on%2520both%2520stages%252C%2520we%2520conduct%2520extensive%250Aexperiments%2520across%2520two%2520related%2520tasks%253A%2520Video%2520Reconstruction%2520and%2520Video%2520Editing.%250AVideo-3DGS%2520trained%2520with%25203k%2520iterations%2520significantly%2520improves%2520video%250Areconstruction%2520quality%2520%2528%252B3%2520PSNR%252C%2520%252B7%2520PSNR%2520increase%2529%2520and%2520training%2520efficiency%250A%2528x1.9%252C%2520x4.5%2520times%2520faster%2529%2520over%2520NeRF-based%2520and%25203DGS-based%2520state-of-art%2520methods%250Aon%2520DAVIS%2520dataset%252C%2520respectively.%2520Moreover%252C%2520it%2520enhances%2520video%2520editing%2520by%2520ensuring%250Atemporal%2520consistency%2520across%252058%2520dynamic%2520monocular%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Temporal%20Consistency%20in%20Video%20Editing%20by%20Reconstructing%20Videos%0A%20%20with%203D%20Gaussian%20Splatting&entry.906535625=Inkyu%20Shin%20and%20Qihang%20Yu%20and%20Xiaohui%20Shen%20and%20In%20So%20Kweon%20and%20Kuk-Jin%20Yoon%20and%20Liang-Chieh%20Chen&entry.1292438233=%20%20Recent%20advancements%20in%20zero-shot%20video%20diffusion%20models%20have%20shown%20promise%0Afor%20text-driven%20video%20editing%2C%20but%20challenges%20remain%20in%20achieving%20high%20temporal%0Aconsistency.%20To%20address%20this%2C%20we%20introduce%20Video-3DGS%2C%20a%203D%20Gaussian%20Splatting%0A%283DGS%29-based%20video%20refiner%20designed%20to%20enhance%20temporal%20consistency%20in%0Azero-shot%20video%20editors.%20Our%20approach%20utilizes%20a%20two-stage%203D%20Gaussian%0Aoptimizing%20process%20tailored%20for%20editing%20dynamic%20monocular%20videos.%20In%20the%20first%0Astage%2C%20Video-3DGS%20employs%20an%20improved%20version%20of%20COLMAP%2C%20referred%20to%20as%0AMC-COLMAP%2C%20which%20processes%20original%20videos%20using%20a%20Masked%20and%20Clipped%20approach.%0AFor%20each%20video%20clip%2C%20MC-COLMAP%20generates%20the%20point%20clouds%20for%20dynamic%0Aforeground%20objects%20and%20complex%20backgrounds.%20These%20point%20clouds%20are%20utilized%20to%0Ainitialize%20two%20sets%20of%203D%20Gaussians%20%28Frg-3DGS%20and%20Bkg-3DGS%29%20aiming%20to%20represent%0Aforeground%20and%20background%20views.%20Both%20foreground%20and%20background%20views%20are%20then%0Amerged%20with%20a%202D%20learnable%20parameter%20map%20to%20reconstruct%20full%20views.%20In%20the%0Asecond%20stage%2C%20we%20leverage%20the%20reconstruction%20ability%20developed%20in%20the%20first%0Astage%20to%20impose%20the%20temporal%20constraints%20on%20the%20video%20diffusion%20model.%20To%0Ademonstrate%20the%20efficacy%20of%20Video-3DGS%20on%20both%20stages%2C%20we%20conduct%20extensive%0Aexperiments%20across%20two%20related%20tasks%3A%20Video%20Reconstruction%20and%20Video%20Editing.%0AVideo-3DGS%20trained%20with%203k%20iterations%20significantly%20improves%20video%0Areconstruction%20quality%20%28%2B3%20PSNR%2C%20%2B7%20PSNR%20increase%29%20and%20training%20efficiency%0A%28x1.9%2C%20x4.5%20times%20faster%29%20over%20NeRF-based%20and%203DGS-based%20state-of-art%20methods%0Aon%20DAVIS%20dataset%2C%20respectively.%20Moreover%2C%20it%20enhances%20video%20editing%20by%20ensuring%0Atemporal%20consistency%20across%2058%20dynamic%20monocular%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02541v1&entry.124074799=Read"},
{"title": "Harnessing Neural Unit Dynamics for Effective and Scalable\n  Class-Incremental Learning", "author": "Depeng Li and Tianqi Wang and Junwei Chen and Wei Dai and Zhigang Zeng", "abstract": "  Class-incremental learning (CIL) aims to train a model to learn new classes\nfrom non-stationary data streams without forgetting old ones. In this paper, we\npropose a new kind of connectionist model by tailoring neural unit dynamics\nthat adapt the behavior of neural networks for CIL. In each training session,\nit introduces a supervisory mechanism to guide network expansion whose growth\nsize is compactly commensurate with the intrinsic complexity of a newly\narriving task. This constructs a near-minimal network while allowing the model\nto expand its capacity when cannot sufficiently hold new classes. At inference\ntime, it automatically reactivates the required neural units to retrieve\nknowledge and leaves the remaining inactivated to prevent interference. We name\nour model AutoActivator, which is effective and scalable. To gain insights into\nthe neural unit dynamics, we theoretically analyze the model's convergence\nproperty via a universal approximation theorem on learning sequential mappings,\nwhich is under-explored in the CIL community. Experiments show that our method\nachieves strong CIL performance in rehearsal-free and minimal-expansion\nsettings with different backbones.\n", "link": "http://arxiv.org/abs/2406.02428v1", "date": "2024-06-04", "relevancy": 2.6751, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6003}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5029}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Neural%20Unit%20Dynamics%20for%20Effective%20and%20Scalable%0A%20%20Class-Incremental%20Learning&body=Title%3A%20Harnessing%20Neural%20Unit%20Dynamics%20for%20Effective%20and%20Scalable%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Depeng%20Li%20and%20Tianqi%20Wang%20and%20Junwei%20Chen%20and%20Wei%20Dai%20and%20Zhigang%20Zeng%0AAbstract%3A%20%20%20Class-incremental%20learning%20%28CIL%29%20aims%20to%20train%20a%20model%20to%20learn%20new%20classes%0Afrom%20non-stationary%20data%20streams%20without%20forgetting%20old%20ones.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20kind%20of%20connectionist%20model%20by%20tailoring%20neural%20unit%20dynamics%0Athat%20adapt%20the%20behavior%20of%20neural%20networks%20for%20CIL.%20In%20each%20training%20session%2C%0Ait%20introduces%20a%20supervisory%20mechanism%20to%20guide%20network%20expansion%20whose%20growth%0Asize%20is%20compactly%20commensurate%20with%20the%20intrinsic%20complexity%20of%20a%20newly%0Aarriving%20task.%20This%20constructs%20a%20near-minimal%20network%20while%20allowing%20the%20model%0Ato%20expand%20its%20capacity%20when%20cannot%20sufficiently%20hold%20new%20classes.%20At%20inference%0Atime%2C%20it%20automatically%20reactivates%20the%20required%20neural%20units%20to%20retrieve%0Aknowledge%20and%20leaves%20the%20remaining%20inactivated%20to%20prevent%20interference.%20We%20name%0Aour%20model%20AutoActivator%2C%20which%20is%20effective%20and%20scalable.%20To%20gain%20insights%20into%0Athe%20neural%20unit%20dynamics%2C%20we%20theoretically%20analyze%20the%20model%27s%20convergence%0Aproperty%20via%20a%20universal%20approximation%20theorem%20on%20learning%20sequential%20mappings%2C%0Awhich%20is%20under-explored%20in%20the%20CIL%20community.%20Experiments%20show%20that%20our%20method%0Aachieves%20strong%20CIL%20performance%20in%20rehearsal-free%20and%20minimal-expansion%0Asettings%20with%20different%20backbones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Neural%2520Unit%2520Dynamics%2520for%2520Effective%2520and%2520Scalable%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DDepeng%2520Li%2520and%2520Tianqi%2520Wang%2520and%2520Junwei%2520Chen%2520and%2520Wei%2520Dai%2520and%2520Zhigang%2520Zeng%26entry.1292438233%3D%2520%2520Class-incremental%2520learning%2520%2528CIL%2529%2520aims%2520to%2520train%2520a%2520model%2520to%2520learn%2520new%2520classes%250Afrom%2520non-stationary%2520data%2520streams%2520without%2520forgetting%2520old%2520ones.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520new%2520kind%2520of%2520connectionist%2520model%2520by%2520tailoring%2520neural%2520unit%2520dynamics%250Athat%2520adapt%2520the%2520behavior%2520of%2520neural%2520networks%2520for%2520CIL.%2520In%2520each%2520training%2520session%252C%250Ait%2520introduces%2520a%2520supervisory%2520mechanism%2520to%2520guide%2520network%2520expansion%2520whose%2520growth%250Asize%2520is%2520compactly%2520commensurate%2520with%2520the%2520intrinsic%2520complexity%2520of%2520a%2520newly%250Aarriving%2520task.%2520This%2520constructs%2520a%2520near-minimal%2520network%2520while%2520allowing%2520the%2520model%250Ato%2520expand%2520its%2520capacity%2520when%2520cannot%2520sufficiently%2520hold%2520new%2520classes.%2520At%2520inference%250Atime%252C%2520it%2520automatically%2520reactivates%2520the%2520required%2520neural%2520units%2520to%2520retrieve%250Aknowledge%2520and%2520leaves%2520the%2520remaining%2520inactivated%2520to%2520prevent%2520interference.%2520We%2520name%250Aour%2520model%2520AutoActivator%252C%2520which%2520is%2520effective%2520and%2520scalable.%2520To%2520gain%2520insights%2520into%250Athe%2520neural%2520unit%2520dynamics%252C%2520we%2520theoretically%2520analyze%2520the%2520model%2527s%2520convergence%250Aproperty%2520via%2520a%2520universal%2520approximation%2520theorem%2520on%2520learning%2520sequential%2520mappings%252C%250Awhich%2520is%2520under-explored%2520in%2520the%2520CIL%2520community.%2520Experiments%2520show%2520that%2520our%2520method%250Aachieves%2520strong%2520CIL%2520performance%2520in%2520rehearsal-free%2520and%2520minimal-expansion%250Asettings%2520with%2520different%2520backbones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Neural%20Unit%20Dynamics%20for%20Effective%20and%20Scalable%0A%20%20Class-Incremental%20Learning&entry.906535625=Depeng%20Li%20and%20Tianqi%20Wang%20and%20Junwei%20Chen%20and%20Wei%20Dai%20and%20Zhigang%20Zeng&entry.1292438233=%20%20Class-incremental%20learning%20%28CIL%29%20aims%20to%20train%20a%20model%20to%20learn%20new%20classes%0Afrom%20non-stationary%20data%20streams%20without%20forgetting%20old%20ones.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20kind%20of%20connectionist%20model%20by%20tailoring%20neural%20unit%20dynamics%0Athat%20adapt%20the%20behavior%20of%20neural%20networks%20for%20CIL.%20In%20each%20training%20session%2C%0Ait%20introduces%20a%20supervisory%20mechanism%20to%20guide%20network%20expansion%20whose%20growth%0Asize%20is%20compactly%20commensurate%20with%20the%20intrinsic%20complexity%20of%20a%20newly%0Aarriving%20task.%20This%20constructs%20a%20near-minimal%20network%20while%20allowing%20the%20model%0Ato%20expand%20its%20capacity%20when%20cannot%20sufficiently%20hold%20new%20classes.%20At%20inference%0Atime%2C%20it%20automatically%20reactivates%20the%20required%20neural%20units%20to%20retrieve%0Aknowledge%20and%20leaves%20the%20remaining%20inactivated%20to%20prevent%20interference.%20We%20name%0Aour%20model%20AutoActivator%2C%20which%20is%20effective%20and%20scalable.%20To%20gain%20insights%20into%0Athe%20neural%20unit%20dynamics%2C%20we%20theoretically%20analyze%20the%20model%27s%20convergence%0Aproperty%20via%20a%20universal%20approximation%20theorem%20on%20learning%20sequential%20mappings%2C%0Awhich%20is%20under-explored%20in%20the%20CIL%20community.%20Experiments%20show%20that%20our%20method%0Aachieves%20strong%20CIL%20performance%20in%20rehearsal-free%20and%20minimal-expansion%0Asettings%20with%20different%20backbones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02428v1&entry.124074799=Read"},
{"title": "Audio Mamba: Selective State Spaces for Self-Supervised Audio\n  Representations", "author": "Sarthak Yadav and Zheng-Hua Tan", "abstract": "  Despite its widespread adoption as the prominent neural architecture, the\nTransformer has spurred several independent lines of work to address its\nlimitations. One such approach is selective state space models, which have\ndemonstrated promising results for language modelling. However, their\nfeasibility for learning self-supervised, general-purpose audio representations\nis yet to be investigated. This work proposes Audio Mamba, a selective state\nspace model for learning general-purpose audio representations from randomly\nmasked spectrogram patches through self-supervision. Empirical results on ten\ndiverse audio recognition downstream tasks show that the proposed models,\npretrained on the AudioSet dataset, consistently outperform comparable\nself-supervised audio spectrogram transformer (SSAST) baselines by a\nconsiderable margin and demonstrate better performance in dataset size,\nsequence length and model size comparisons.\n", "link": "http://arxiv.org/abs/2406.02178v1", "date": "2024-06-04", "relevancy": 2.6243, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5323}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5216}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Mamba%3A%20Selective%20State%20Spaces%20for%20Self-Supervised%20Audio%0A%20%20Representations&body=Title%3A%20Audio%20Mamba%3A%20Selective%20State%20Spaces%20for%20Self-Supervised%20Audio%0A%20%20Representations%0AAuthor%3A%20Sarthak%20Yadav%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20Despite%20its%20widespread%20adoption%20as%20the%20prominent%20neural%20architecture%2C%20the%0ATransformer%20has%20spurred%20several%20independent%20lines%20of%20work%20to%20address%20its%0Alimitations.%20One%20such%20approach%20is%20selective%20state%20space%20models%2C%20which%20have%0Ademonstrated%20promising%20results%20for%20language%20modelling.%20However%2C%20their%0Afeasibility%20for%20learning%20self-supervised%2C%20general-purpose%20audio%20representations%0Ais%20yet%20to%20be%20investigated.%20This%20work%20proposes%20Audio%20Mamba%2C%20a%20selective%20state%0Aspace%20model%20for%20learning%20general-purpose%20audio%20representations%20from%20randomly%0Amasked%20spectrogram%20patches%20through%20self-supervision.%20Empirical%20results%20on%20ten%0Adiverse%20audio%20recognition%20downstream%20tasks%20show%20that%20the%20proposed%20models%2C%0Apretrained%20on%20the%20AudioSet%20dataset%2C%20consistently%20outperform%20comparable%0Aself-supervised%20audio%20spectrogram%20transformer%20%28SSAST%29%20baselines%20by%20a%0Aconsiderable%20margin%20and%20demonstrate%20better%20performance%20in%20dataset%20size%2C%0Asequence%20length%20and%20model%20size%20comparisons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Mamba%253A%2520Selective%2520State%2520Spaces%2520for%2520Self-Supervised%2520Audio%250A%2520%2520Representations%26entry.906535625%3DSarthak%2520Yadav%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520Despite%2520its%2520widespread%2520adoption%2520as%2520the%2520prominent%2520neural%2520architecture%252C%2520the%250ATransformer%2520has%2520spurred%2520several%2520independent%2520lines%2520of%2520work%2520to%2520address%2520its%250Alimitations.%2520One%2520such%2520approach%2520is%2520selective%2520state%2520space%2520models%252C%2520which%2520have%250Ademonstrated%2520promising%2520results%2520for%2520language%2520modelling.%2520However%252C%2520their%250Afeasibility%2520for%2520learning%2520self-supervised%252C%2520general-purpose%2520audio%2520representations%250Ais%2520yet%2520to%2520be%2520investigated.%2520This%2520work%2520proposes%2520Audio%2520Mamba%252C%2520a%2520selective%2520state%250Aspace%2520model%2520for%2520learning%2520general-purpose%2520audio%2520representations%2520from%2520randomly%250Amasked%2520spectrogram%2520patches%2520through%2520self-supervision.%2520Empirical%2520results%2520on%2520ten%250Adiverse%2520audio%2520recognition%2520downstream%2520tasks%2520show%2520that%2520the%2520proposed%2520models%252C%250Apretrained%2520on%2520the%2520AudioSet%2520dataset%252C%2520consistently%2520outperform%2520comparable%250Aself-supervised%2520audio%2520spectrogram%2520transformer%2520%2528SSAST%2529%2520baselines%2520by%2520a%250Aconsiderable%2520margin%2520and%2520demonstrate%2520better%2520performance%2520in%2520dataset%2520size%252C%250Asequence%2520length%2520and%2520model%2520size%2520comparisons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Mamba%3A%20Selective%20State%20Spaces%20for%20Self-Supervised%20Audio%0A%20%20Representations&entry.906535625=Sarthak%20Yadav%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20Despite%20its%20widespread%20adoption%20as%20the%20prominent%20neural%20architecture%2C%20the%0ATransformer%20has%20spurred%20several%20independent%20lines%20of%20work%20to%20address%20its%0Alimitations.%20One%20such%20approach%20is%20selective%20state%20space%20models%2C%20which%20have%0Ademonstrated%20promising%20results%20for%20language%20modelling.%20However%2C%20their%0Afeasibility%20for%20learning%20self-supervised%2C%20general-purpose%20audio%20representations%0Ais%20yet%20to%20be%20investigated.%20This%20work%20proposes%20Audio%20Mamba%2C%20a%20selective%20state%0Aspace%20model%20for%20learning%20general-purpose%20audio%20representations%20from%20randomly%0Amasked%20spectrogram%20patches%20through%20self-supervision.%20Empirical%20results%20on%20ten%0Adiverse%20audio%20recognition%20downstream%20tasks%20show%20that%20the%20proposed%20models%2C%0Apretrained%20on%20the%20AudioSet%20dataset%2C%20consistently%20outperform%20comparable%0Aself-supervised%20audio%20spectrogram%20transformer%20%28SSAST%29%20baselines%20by%20a%0Aconsiderable%20margin%20and%20demonstrate%20better%20performance%20in%20dataset%20size%2C%0Asequence%20length%20and%20model%20size%20comparisons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02178v1&entry.124074799=Read"},
{"title": "VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors", "author": "Markus Plack and Hannah Dr\u00f6ge and Leif Van Holland and Matthias B. Hullin", "abstract": "  We present a stereo-matching method for depth estimation from high-resolution\nimages using visual hulls as priors, and a memory-efficient technique for the\ncorrelation computation. Our method uses object masks extracted from\nsupplementary views of the scene to guide the disparity estimation, effectively\nreducing the search space for matches. This approach is specifically tailored\nto stereo rigs in volumetric capture systems, where an accurate depth plays a\nkey role in the downstream reconstruction task. To enable training and\nregression at high resolutions targeted by recent systems, our approach extends\na sparse correlation computation into a hybrid sparse-dense scheme suitable for\napplication in leading recurrent network architectures. We evaluate the\nperformance-efficiency trade-off of our method compared to state-of-the-art\nmethods, and demonstrate the efficacy of the visual hull guidance. In addition,\nwe propose a training scheme for a further reduction of memory requirements\nduring optimization, facilitating training on high-resolution data.\n", "link": "http://arxiv.org/abs/2406.02552v1", "date": "2024-06-04", "relevancy": 2.6079, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5367}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5178}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VHS%3A%20High-Resolution%20Iterative%20Stereo%20Matching%20with%20Visual%20Hull%20Priors&body=Title%3A%20VHS%3A%20High-Resolution%20Iterative%20Stereo%20Matching%20with%20Visual%20Hull%20Priors%0AAuthor%3A%20Markus%20Plack%20and%20Hannah%20Dr%C3%B6ge%20and%20Leif%20Van%20Holland%20and%20Matthias%20B.%20Hullin%0AAbstract%3A%20%20%20We%20present%20a%20stereo-matching%20method%20for%20depth%20estimation%20from%20high-resolution%0Aimages%20using%20visual%20hulls%20as%20priors%2C%20and%20a%20memory-efficient%20technique%20for%20the%0Acorrelation%20computation.%20Our%20method%20uses%20object%20masks%20extracted%20from%0Asupplementary%20views%20of%20the%20scene%20to%20guide%20the%20disparity%20estimation%2C%20effectively%0Areducing%20the%20search%20space%20for%20matches.%20This%20approach%20is%20specifically%20tailored%0Ato%20stereo%20rigs%20in%20volumetric%20capture%20systems%2C%20where%20an%20accurate%20depth%20plays%20a%0Akey%20role%20in%20the%20downstream%20reconstruction%20task.%20To%20enable%20training%20and%0Aregression%20at%20high%20resolutions%20targeted%20by%20recent%20systems%2C%20our%20approach%20extends%0Aa%20sparse%20correlation%20computation%20into%20a%20hybrid%20sparse-dense%20scheme%20suitable%20for%0Aapplication%20in%20leading%20recurrent%20network%20architectures.%20We%20evaluate%20the%0Aperformance-efficiency%20trade-off%20of%20our%20method%20compared%20to%20state-of-the-art%0Amethods%2C%20and%20demonstrate%20the%20efficacy%20of%20the%20visual%20hull%20guidance.%20In%20addition%2C%0Awe%20propose%20a%20training%20scheme%20for%20a%20further%20reduction%20of%20memory%20requirements%0Aduring%20optimization%2C%20facilitating%20training%20on%20high-resolution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVHS%253A%2520High-Resolution%2520Iterative%2520Stereo%2520Matching%2520with%2520Visual%2520Hull%2520Priors%26entry.906535625%3DMarkus%2520Plack%2520and%2520Hannah%2520Dr%25C3%25B6ge%2520and%2520Leif%2520Van%2520Holland%2520and%2520Matthias%2520B.%2520Hullin%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520stereo-matching%2520method%2520for%2520depth%2520estimation%2520from%2520high-resolution%250Aimages%2520using%2520visual%2520hulls%2520as%2520priors%252C%2520and%2520a%2520memory-efficient%2520technique%2520for%2520the%250Acorrelation%2520computation.%2520Our%2520method%2520uses%2520object%2520masks%2520extracted%2520from%250Asupplementary%2520views%2520of%2520the%2520scene%2520to%2520guide%2520the%2520disparity%2520estimation%252C%2520effectively%250Areducing%2520the%2520search%2520space%2520for%2520matches.%2520This%2520approach%2520is%2520specifically%2520tailored%250Ato%2520stereo%2520rigs%2520in%2520volumetric%2520capture%2520systems%252C%2520where%2520an%2520accurate%2520depth%2520plays%2520a%250Akey%2520role%2520in%2520the%2520downstream%2520reconstruction%2520task.%2520To%2520enable%2520training%2520and%250Aregression%2520at%2520high%2520resolutions%2520targeted%2520by%2520recent%2520systems%252C%2520our%2520approach%2520extends%250Aa%2520sparse%2520correlation%2520computation%2520into%2520a%2520hybrid%2520sparse-dense%2520scheme%2520suitable%2520for%250Aapplication%2520in%2520leading%2520recurrent%2520network%2520architectures.%2520We%2520evaluate%2520the%250Aperformance-efficiency%2520trade-off%2520of%2520our%2520method%2520compared%2520to%2520state-of-the-art%250Amethods%252C%2520and%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520visual%2520hull%2520guidance.%2520In%2520addition%252C%250Awe%2520propose%2520a%2520training%2520scheme%2520for%2520a%2520further%2520reduction%2520of%2520memory%2520requirements%250Aduring%2520optimization%252C%2520facilitating%2520training%2520on%2520high-resolution%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VHS%3A%20High-Resolution%20Iterative%20Stereo%20Matching%20with%20Visual%20Hull%20Priors&entry.906535625=Markus%20Plack%20and%20Hannah%20Dr%C3%B6ge%20and%20Leif%20Van%20Holland%20and%20Matthias%20B.%20Hullin&entry.1292438233=%20%20We%20present%20a%20stereo-matching%20method%20for%20depth%20estimation%20from%20high-resolution%0Aimages%20using%20visual%20hulls%20as%20priors%2C%20and%20a%20memory-efficient%20technique%20for%20the%0Acorrelation%20computation.%20Our%20method%20uses%20object%20masks%20extracted%20from%0Asupplementary%20views%20of%20the%20scene%20to%20guide%20the%20disparity%20estimation%2C%20effectively%0Areducing%20the%20search%20space%20for%20matches.%20This%20approach%20is%20specifically%20tailored%0Ato%20stereo%20rigs%20in%20volumetric%20capture%20systems%2C%20where%20an%20accurate%20depth%20plays%20a%0Akey%20role%20in%20the%20downstream%20reconstruction%20task.%20To%20enable%20training%20and%0Aregression%20at%20high%20resolutions%20targeted%20by%20recent%20systems%2C%20our%20approach%20extends%0Aa%20sparse%20correlation%20computation%20into%20a%20hybrid%20sparse-dense%20scheme%20suitable%20for%0Aapplication%20in%20leading%20recurrent%20network%20architectures.%20We%20evaluate%20the%0Aperformance-efficiency%20trade-off%20of%20our%20method%20compared%20to%20state-of-the-art%0Amethods%2C%20and%20demonstrate%20the%20efficacy%20of%20the%20visual%20hull%20guidance.%20In%20addition%2C%0Awe%20propose%20a%20training%20scheme%20for%20a%20further%20reduction%20of%20memory%20requirements%0Aduring%20optimization%2C%20facilitating%20training%20on%20high-resolution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02552v1&entry.124074799=Read"},
{"title": "Node-Level Topological Representation Learning on Point Clouds", "author": "Vincent P. Grande and Michael T. Schaub", "abstract": "  Topological Data Analysis (TDA) allows us to extract powerful topological and\nhigher-order information on the global shape of a data set or point cloud.\nTools like Persistent Homology or the Euler Transform give a single complex\ndescription of the global structure of the point cloud. However, common machine\nlearning applications like classification require point-level information and\nfeatures to be available. In this paper, we bridge this gap and propose a novel\nmethod to extract node-level topological features from complex point clouds\nusing discrete variants of concepts from algebraic topology and differential\ngeometry. We verify the effectiveness of these topological point features\n(TOPF) on both synthetic and real-world data and study their robustness under\nnoise.\n", "link": "http://arxiv.org/abs/2406.02300v1", "date": "2024-06-04", "relevancy": 2.6009, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5253}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5184}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Node-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds&body=Title%3A%20Node-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds%0AAuthor%3A%20Vincent%20P.%20Grande%20and%20Michael%20T.%20Schaub%0AAbstract%3A%20%20%20Topological%20Data%20Analysis%20%28TDA%29%20allows%20us%20to%20extract%20powerful%20topological%20and%0Ahigher-order%20information%20on%20the%20global%20shape%20of%20a%20data%20set%20or%20point%20cloud.%0ATools%20like%20Persistent%20Homology%20or%20the%20Euler%20Transform%20give%20a%20single%20complex%0Adescription%20of%20the%20global%20structure%20of%20the%20point%20cloud.%20However%2C%20common%20machine%0Alearning%20applications%20like%20classification%20require%20point-level%20information%20and%0Afeatures%20to%20be%20available.%20In%20this%20paper%2C%20we%20bridge%20this%20gap%20and%20propose%20a%20novel%0Amethod%20to%20extract%20node-level%20topological%20features%20from%20complex%20point%20clouds%0Ausing%20discrete%20variants%20of%20concepts%20from%20algebraic%20topology%20and%20differential%0Ageometry.%20We%20verify%20the%20effectiveness%20of%20these%20topological%20point%20features%0A%28TOPF%29%20on%20both%20synthetic%20and%20real-world%20data%20and%20study%20their%20robustness%20under%0Anoise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNode-Level%2520Topological%2520Representation%2520Learning%2520on%2520Point%2520Clouds%26entry.906535625%3DVincent%2520P.%2520Grande%2520and%2520Michael%2520T.%2520Schaub%26entry.1292438233%3D%2520%2520Topological%2520Data%2520Analysis%2520%2528TDA%2529%2520allows%2520us%2520to%2520extract%2520powerful%2520topological%2520and%250Ahigher-order%2520information%2520on%2520the%2520global%2520shape%2520of%2520a%2520data%2520set%2520or%2520point%2520cloud.%250ATools%2520like%2520Persistent%2520Homology%2520or%2520the%2520Euler%2520Transform%2520give%2520a%2520single%2520complex%250Adescription%2520of%2520the%2520global%2520structure%2520of%2520the%2520point%2520cloud.%2520However%252C%2520common%2520machine%250Alearning%2520applications%2520like%2520classification%2520require%2520point-level%2520information%2520and%250Afeatures%2520to%2520be%2520available.%2520In%2520this%2520paper%252C%2520we%2520bridge%2520this%2520gap%2520and%2520propose%2520a%2520novel%250Amethod%2520to%2520extract%2520node-level%2520topological%2520features%2520from%2520complex%2520point%2520clouds%250Ausing%2520discrete%2520variants%2520of%2520concepts%2520from%2520algebraic%2520topology%2520and%2520differential%250Ageometry.%2520We%2520verify%2520the%2520effectiveness%2520of%2520these%2520topological%2520point%2520features%250A%2528TOPF%2529%2520on%2520both%2520synthetic%2520and%2520real-world%2520data%2520and%2520study%2520their%2520robustness%2520under%250Anoise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Node-Level%20Topological%20Representation%20Learning%20on%20Point%20Clouds&entry.906535625=Vincent%20P.%20Grande%20and%20Michael%20T.%20Schaub&entry.1292438233=%20%20Topological%20Data%20Analysis%20%28TDA%29%20allows%20us%20to%20extract%20powerful%20topological%20and%0Ahigher-order%20information%20on%20the%20global%20shape%20of%20a%20data%20set%20or%20point%20cloud.%0ATools%20like%20Persistent%20Homology%20or%20the%20Euler%20Transform%20give%20a%20single%20complex%0Adescription%20of%20the%20global%20structure%20of%20the%20point%20cloud.%20However%2C%20common%20machine%0Alearning%20applications%20like%20classification%20require%20point-level%20information%20and%0Afeatures%20to%20be%20available.%20In%20this%20paper%2C%20we%20bridge%20this%20gap%20and%20propose%20a%20novel%0Amethod%20to%20extract%20node-level%20topological%20features%20from%20complex%20point%20clouds%0Ausing%20discrete%20variants%20of%20concepts%20from%20algebraic%20topology%20and%20differential%0Ageometry.%20We%20verify%20the%20effectiveness%20of%20these%20topological%20point%20features%0A%28TOPF%29%20on%20both%20synthetic%20and%20real-world%20data%20and%20study%20their%20robustness%20under%0Anoise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02300v1&entry.124074799=Read"},
{"title": "ExGRG: Explicitly-Generated Relation Graph for Self-Supervised\n  Representation Learning", "author": "Mahdi Naseri and Mahdi Biparva", "abstract": "  Self-supervised Learning (SSL) has emerged as a powerful technique in\npre-training deep learning models without relying on expensive annotated\nlabels, instead leveraging embedded signals in unlabeled data. While SSL has\nshown remarkable success in computer vision tasks through intuitive data\naugmentation, its application to graph-structured data poses challenges due to\nthe semantic-altering and counter-intuitive nature of graph augmentations.\nAddressing this limitation, this paper introduces a novel non-contrastive SSL\napproach to Explicitly Generate a compositional Relation Graph (ExGRG) instead\nof relying solely on the conventional augmentation-based implicit relation\ngraph. ExGRG offers a framework for incorporating prior domain knowledge and\nonline extracted information into the SSL invariance objective, drawing\ninspiration from the Laplacian Eigenmap and Expectation-Maximization (EM).\nEmploying an EM perspective on SSL, our E-step involves relation graph\ngeneration to identify candidates to guide the SSL invariance objective, and\nM-step updates the model parameters by integrating the derived relational\ninformation. Extensive experimentation on diverse node classification datasets\ndemonstrates the superiority of our method over state-of-the-art techniques,\naffirming ExGRG as an effective adoption of SSL for graph representation\nlearning.\n", "link": "http://arxiv.org/abs/2402.06737v2", "date": "2024-06-04", "relevancy": 2.5848, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5653}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5084}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExGRG%3A%20Explicitly-Generated%20Relation%20Graph%20for%20Self-Supervised%0A%20%20Representation%20Learning&body=Title%3A%20ExGRG%3A%20Explicitly-Generated%20Relation%20Graph%20for%20Self-Supervised%0A%20%20Representation%20Learning%0AAuthor%3A%20Mahdi%20Naseri%20and%20Mahdi%20Biparva%0AAbstract%3A%20%20%20Self-supervised%20Learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20technique%20in%0Apre-training%20deep%20learning%20models%20without%20relying%20on%20expensive%20annotated%0Alabels%2C%20instead%20leveraging%20embedded%20signals%20in%20unlabeled%20data.%20While%20SSL%20has%0Ashown%20remarkable%20success%20in%20computer%20vision%20tasks%20through%20intuitive%20data%0Aaugmentation%2C%20its%20application%20to%20graph-structured%20data%20poses%20challenges%20due%20to%0Athe%20semantic-altering%20and%20counter-intuitive%20nature%20of%20graph%20augmentations.%0AAddressing%20this%20limitation%2C%20this%20paper%20introduces%20a%20novel%20non-contrastive%20SSL%0Aapproach%20to%20Explicitly%20Generate%20a%20compositional%20Relation%20Graph%20%28ExGRG%29%20instead%0Aof%20relying%20solely%20on%20the%20conventional%20augmentation-based%20implicit%20relation%0Agraph.%20ExGRG%20offers%20a%20framework%20for%20incorporating%20prior%20domain%20knowledge%20and%0Aonline%20extracted%20information%20into%20the%20SSL%20invariance%20objective%2C%20drawing%0Ainspiration%20from%20the%20Laplacian%20Eigenmap%20and%20Expectation-Maximization%20%28EM%29.%0AEmploying%20an%20EM%20perspective%20on%20SSL%2C%20our%20E-step%20involves%20relation%20graph%0Ageneration%20to%20identify%20candidates%20to%20guide%20the%20SSL%20invariance%20objective%2C%20and%0AM-step%20updates%20the%20model%20parameters%20by%20integrating%20the%20derived%20relational%0Ainformation.%20Extensive%20experimentation%20on%20diverse%20node%20classification%20datasets%0Ademonstrates%20the%20superiority%20of%20our%20method%20over%20state-of-the-art%20techniques%2C%0Aaffirming%20ExGRG%20as%20an%20effective%20adoption%20of%20SSL%20for%20graph%20representation%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExGRG%253A%2520Explicitly-Generated%2520Relation%2520Graph%2520for%2520Self-Supervised%250A%2520%2520Representation%2520Learning%26entry.906535625%3DMahdi%2520Naseri%2520and%2520Mahdi%2520Biparva%26entry.1292438233%3D%2520%2520Self-supervised%2520Learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520in%250Apre-training%2520deep%2520learning%2520models%2520without%2520relying%2520on%2520expensive%2520annotated%250Alabels%252C%2520instead%2520leveraging%2520embedded%2520signals%2520in%2520unlabeled%2520data.%2520While%2520SSL%2520has%250Ashown%2520remarkable%2520success%2520in%2520computer%2520vision%2520tasks%2520through%2520intuitive%2520data%250Aaugmentation%252C%2520its%2520application%2520to%2520graph-structured%2520data%2520poses%2520challenges%2520due%2520to%250Athe%2520semantic-altering%2520and%2520counter-intuitive%2520nature%2520of%2520graph%2520augmentations.%250AAddressing%2520this%2520limitation%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520non-contrastive%2520SSL%250Aapproach%2520to%2520Explicitly%2520Generate%2520a%2520compositional%2520Relation%2520Graph%2520%2528ExGRG%2529%2520instead%250Aof%2520relying%2520solely%2520on%2520the%2520conventional%2520augmentation-based%2520implicit%2520relation%250Agraph.%2520ExGRG%2520offers%2520a%2520framework%2520for%2520incorporating%2520prior%2520domain%2520knowledge%2520and%250Aonline%2520extracted%2520information%2520into%2520the%2520SSL%2520invariance%2520objective%252C%2520drawing%250Ainspiration%2520from%2520the%2520Laplacian%2520Eigenmap%2520and%2520Expectation-Maximization%2520%2528EM%2529.%250AEmploying%2520an%2520EM%2520perspective%2520on%2520SSL%252C%2520our%2520E-step%2520involves%2520relation%2520graph%250Ageneration%2520to%2520identify%2520candidates%2520to%2520guide%2520the%2520SSL%2520invariance%2520objective%252C%2520and%250AM-step%2520updates%2520the%2520model%2520parameters%2520by%2520integrating%2520the%2520derived%2520relational%250Ainformation.%2520Extensive%2520experimentation%2520on%2520diverse%2520node%2520classification%2520datasets%250Ademonstrates%2520the%2520superiority%2520of%2520our%2520method%2520over%2520state-of-the-art%2520techniques%252C%250Aaffirming%2520ExGRG%2520as%2520an%2520effective%2520adoption%2520of%2520SSL%2520for%2520graph%2520representation%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExGRG%3A%20Explicitly-Generated%20Relation%20Graph%20for%20Self-Supervised%0A%20%20Representation%20Learning&entry.906535625=Mahdi%20Naseri%20and%20Mahdi%20Biparva&entry.1292438233=%20%20Self-supervised%20Learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20technique%20in%0Apre-training%20deep%20learning%20models%20without%20relying%20on%20expensive%20annotated%0Alabels%2C%20instead%20leveraging%20embedded%20signals%20in%20unlabeled%20data.%20While%20SSL%20has%0Ashown%20remarkable%20success%20in%20computer%20vision%20tasks%20through%20intuitive%20data%0Aaugmentation%2C%20its%20application%20to%20graph-structured%20data%20poses%20challenges%20due%20to%0Athe%20semantic-altering%20and%20counter-intuitive%20nature%20of%20graph%20augmentations.%0AAddressing%20this%20limitation%2C%20this%20paper%20introduces%20a%20novel%20non-contrastive%20SSL%0Aapproach%20to%20Explicitly%20Generate%20a%20compositional%20Relation%20Graph%20%28ExGRG%29%20instead%0Aof%20relying%20solely%20on%20the%20conventional%20augmentation-based%20implicit%20relation%0Agraph.%20ExGRG%20offers%20a%20framework%20for%20incorporating%20prior%20domain%20knowledge%20and%0Aonline%20extracted%20information%20into%20the%20SSL%20invariance%20objective%2C%20drawing%0Ainspiration%20from%20the%20Laplacian%20Eigenmap%20and%20Expectation-Maximization%20%28EM%29.%0AEmploying%20an%20EM%20perspective%20on%20SSL%2C%20our%20E-step%20involves%20relation%20graph%0Ageneration%20to%20identify%20candidates%20to%20guide%20the%20SSL%20invariance%20objective%2C%20and%0AM-step%20updates%20the%20model%20parameters%20by%20integrating%20the%20derived%20relational%0Ainformation.%20Extensive%20experimentation%20on%20diverse%20node%20classification%20datasets%0Ademonstrates%20the%20superiority%20of%20our%20method%20over%20state-of-the-art%20techniques%2C%0Aaffirming%20ExGRG%20as%20an%20effective%20adoption%20of%20SSL%20for%20graph%20representation%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06737v2&entry.124074799=Read"},
{"title": "Low-Rank Adaption on Transformer-based Oriented Object Detector for\n  Satellite Onboard Processing of Remote Sensing Images", "author": "Xinyang Pu and Feng Xu", "abstract": "  Deep learning models in satellite onboard enable real-time interpretation of\nremote sensing images, reducing the need for data transmission to the ground\nand conserving communication resources. As satellite numbers and observation\nfrequencies increase, the demand for satellite onboard real-time image\ninterpretation grows, highlighting the expanding importance and development of\nthis technology. However, updating the extensive parameters of models deployed\non the satellites for spaceborne object detection model is challenging due to\nthe limitations of uplink bandwidth in wireless satellite communications. To\naddress this issue, this paper proposes a method based on parameter-efficient\nfine-tuning technology with low-rank adaptation (LoRA) module. It involves\ntraining low-rank matrix parameters and integrating them with the original\nmodel's weight matrix through multiplication and summation, thereby fine-tuning\nthe model parameters to adapt to new data distributions with minimal weight\nupdates. The proposed method combines parameter-efficient fine-tuning with full\nfine-tuning in the parameter update strategy of the oriented object detection\nalgorithm architecture. This strategy enables model performance improvements\nclose to full fine-tuning effects with minimal parameter updates. In addition,\nlow rank approximation is conducted to pick an optimal rank value for LoRA\nmatrices. Extensive experiments verify the effectiveness of the proposed\nmethod. By fine-tuning and updating only 12.4$\\%$ of the model's total\nparameters, it is able to achieve 97$\\%$ to 100$\\%$ of the performance of full\nfine-tuning models. Additionally, the reduced number of trainable parameters\naccelerates model training iterations and enhances the generalization and\nrobustness of the oriented object detection model. The source code is available\nat: \\url{https://github.com/fudanxu/LoRA-Det}.\n", "link": "http://arxiv.org/abs/2406.02385v1", "date": "2024-06-04", "relevancy": 2.5727, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5183}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5176}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Adaption%20on%20Transformer-based%20Oriented%20Object%20Detector%20for%0A%20%20Satellite%20Onboard%20Processing%20of%20Remote%20Sensing%20Images&body=Title%3A%20Low-Rank%20Adaption%20on%20Transformer-based%20Oriented%20Object%20Detector%20for%0A%20%20Satellite%20Onboard%20Processing%20of%20Remote%20Sensing%20Images%0AAuthor%3A%20Xinyang%20Pu%20and%20Feng%20Xu%0AAbstract%3A%20%20%20Deep%20learning%20models%20in%20satellite%20onboard%20enable%20real-time%20interpretation%20of%0Aremote%20sensing%20images%2C%20reducing%20the%20need%20for%20data%20transmission%20to%20the%20ground%0Aand%20conserving%20communication%20resources.%20As%20satellite%20numbers%20and%20observation%0Afrequencies%20increase%2C%20the%20demand%20for%20satellite%20onboard%20real-time%20image%0Ainterpretation%20grows%2C%20highlighting%20the%20expanding%20importance%20and%20development%20of%0Athis%20technology.%20However%2C%20updating%20the%20extensive%20parameters%20of%20models%20deployed%0Aon%20the%20satellites%20for%20spaceborne%20object%20detection%20model%20is%20challenging%20due%20to%0Athe%20limitations%20of%20uplink%20bandwidth%20in%20wireless%20satellite%20communications.%20To%0Aaddress%20this%20issue%2C%20this%20paper%20proposes%20a%20method%20based%20on%20parameter-efficient%0Afine-tuning%20technology%20with%20low-rank%20adaptation%20%28LoRA%29%20module.%20It%20involves%0Atraining%20low-rank%20matrix%20parameters%20and%20integrating%20them%20with%20the%20original%0Amodel%27s%20weight%20matrix%20through%20multiplication%20and%20summation%2C%20thereby%20fine-tuning%0Athe%20model%20parameters%20to%20adapt%20to%20new%20data%20distributions%20with%20minimal%20weight%0Aupdates.%20The%20proposed%20method%20combines%20parameter-efficient%20fine-tuning%20with%20full%0Afine-tuning%20in%20the%20parameter%20update%20strategy%20of%20the%20oriented%20object%20detection%0Aalgorithm%20architecture.%20This%20strategy%20enables%20model%20performance%20improvements%0Aclose%20to%20full%20fine-tuning%20effects%20with%20minimal%20parameter%20updates.%20In%20addition%2C%0Alow%20rank%20approximation%20is%20conducted%20to%20pick%20an%20optimal%20rank%20value%20for%20LoRA%0Amatrices.%20Extensive%20experiments%20verify%20the%20effectiveness%20of%20the%20proposed%0Amethod.%20By%20fine-tuning%20and%20updating%20only%2012.4%24%5C%25%24%20of%20the%20model%27s%20total%0Aparameters%2C%20it%20is%20able%20to%20achieve%2097%24%5C%25%24%20to%20100%24%5C%25%24%20of%20the%20performance%20of%20full%0Afine-tuning%20models.%20Additionally%2C%20the%20reduced%20number%20of%20trainable%20parameters%0Aaccelerates%20model%20training%20iterations%20and%20enhances%20the%20generalization%20and%0Arobustness%20of%20the%20oriented%20object%20detection%20model.%20The%20source%20code%20is%20available%0Aat%3A%20%5Curl%7Bhttps%3A//github.com/fudanxu/LoRA-Det%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Adaption%2520on%2520Transformer-based%2520Oriented%2520Object%2520Detector%2520for%250A%2520%2520Satellite%2520Onboard%2520Processing%2520of%2520Remote%2520Sensing%2520Images%26entry.906535625%3DXinyang%2520Pu%2520and%2520Feng%2520Xu%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520in%2520satellite%2520onboard%2520enable%2520real-time%2520interpretation%2520of%250Aremote%2520sensing%2520images%252C%2520reducing%2520the%2520need%2520for%2520data%2520transmission%2520to%2520the%2520ground%250Aand%2520conserving%2520communication%2520resources.%2520As%2520satellite%2520numbers%2520and%2520observation%250Afrequencies%2520increase%252C%2520the%2520demand%2520for%2520satellite%2520onboard%2520real-time%2520image%250Ainterpretation%2520grows%252C%2520highlighting%2520the%2520expanding%2520importance%2520and%2520development%2520of%250Athis%2520technology.%2520However%252C%2520updating%2520the%2520extensive%2520parameters%2520of%2520models%2520deployed%250Aon%2520the%2520satellites%2520for%2520spaceborne%2520object%2520detection%2520model%2520is%2520challenging%2520due%2520to%250Athe%2520limitations%2520of%2520uplink%2520bandwidth%2520in%2520wireless%2520satellite%2520communications.%2520To%250Aaddress%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520method%2520based%2520on%2520parameter-efficient%250Afine-tuning%2520technology%2520with%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520module.%2520It%2520involves%250Atraining%2520low-rank%2520matrix%2520parameters%2520and%2520integrating%2520them%2520with%2520the%2520original%250Amodel%2527s%2520weight%2520matrix%2520through%2520multiplication%2520and%2520summation%252C%2520thereby%2520fine-tuning%250Athe%2520model%2520parameters%2520to%2520adapt%2520to%2520new%2520data%2520distributions%2520with%2520minimal%2520weight%250Aupdates.%2520The%2520proposed%2520method%2520combines%2520parameter-efficient%2520fine-tuning%2520with%2520full%250Afine-tuning%2520in%2520the%2520parameter%2520update%2520strategy%2520of%2520the%2520oriented%2520object%2520detection%250Aalgorithm%2520architecture.%2520This%2520strategy%2520enables%2520model%2520performance%2520improvements%250Aclose%2520to%2520full%2520fine-tuning%2520effects%2520with%2520minimal%2520parameter%2520updates.%2520In%2520addition%252C%250Alow%2520rank%2520approximation%2520is%2520conducted%2520to%2520pick%2520an%2520optimal%2520rank%2520value%2520for%2520LoRA%250Amatrices.%2520Extensive%2520experiments%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethod.%2520By%2520fine-tuning%2520and%2520updating%2520only%252012.4%2524%255C%2525%2524%2520of%2520the%2520model%2527s%2520total%250Aparameters%252C%2520it%2520is%2520able%2520to%2520achieve%252097%2524%255C%2525%2524%2520to%2520100%2524%255C%2525%2524%2520of%2520the%2520performance%2520of%2520full%250Afine-tuning%2520models.%2520Additionally%252C%2520the%2520reduced%2520number%2520of%2520trainable%2520parameters%250Aaccelerates%2520model%2520training%2520iterations%2520and%2520enhances%2520the%2520generalization%2520and%250Arobustness%2520of%2520the%2520oriented%2520object%2520detection%2520model.%2520The%2520source%2520code%2520is%2520available%250Aat%253A%2520%255Curl%257Bhttps%253A//github.com/fudanxu/LoRA-Det%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Adaption%20on%20Transformer-based%20Oriented%20Object%20Detector%20for%0A%20%20Satellite%20Onboard%20Processing%20of%20Remote%20Sensing%20Images&entry.906535625=Xinyang%20Pu%20and%20Feng%20Xu&entry.1292438233=%20%20Deep%20learning%20models%20in%20satellite%20onboard%20enable%20real-time%20interpretation%20of%0Aremote%20sensing%20images%2C%20reducing%20the%20need%20for%20data%20transmission%20to%20the%20ground%0Aand%20conserving%20communication%20resources.%20As%20satellite%20numbers%20and%20observation%0Afrequencies%20increase%2C%20the%20demand%20for%20satellite%20onboard%20real-time%20image%0Ainterpretation%20grows%2C%20highlighting%20the%20expanding%20importance%20and%20development%20of%0Athis%20technology.%20However%2C%20updating%20the%20extensive%20parameters%20of%20models%20deployed%0Aon%20the%20satellites%20for%20spaceborne%20object%20detection%20model%20is%20challenging%20due%20to%0Athe%20limitations%20of%20uplink%20bandwidth%20in%20wireless%20satellite%20communications.%20To%0Aaddress%20this%20issue%2C%20this%20paper%20proposes%20a%20method%20based%20on%20parameter-efficient%0Afine-tuning%20technology%20with%20low-rank%20adaptation%20%28LoRA%29%20module.%20It%20involves%0Atraining%20low-rank%20matrix%20parameters%20and%20integrating%20them%20with%20the%20original%0Amodel%27s%20weight%20matrix%20through%20multiplication%20and%20summation%2C%20thereby%20fine-tuning%0Athe%20model%20parameters%20to%20adapt%20to%20new%20data%20distributions%20with%20minimal%20weight%0Aupdates.%20The%20proposed%20method%20combines%20parameter-efficient%20fine-tuning%20with%20full%0Afine-tuning%20in%20the%20parameter%20update%20strategy%20of%20the%20oriented%20object%20detection%0Aalgorithm%20architecture.%20This%20strategy%20enables%20model%20performance%20improvements%0Aclose%20to%20full%20fine-tuning%20effects%20with%20minimal%20parameter%20updates.%20In%20addition%2C%0Alow%20rank%20approximation%20is%20conducted%20to%20pick%20an%20optimal%20rank%20value%20for%20LoRA%0Amatrices.%20Extensive%20experiments%20verify%20the%20effectiveness%20of%20the%20proposed%0Amethod.%20By%20fine-tuning%20and%20updating%20only%2012.4%24%5C%25%24%20of%20the%20model%27s%20total%0Aparameters%2C%20it%20is%20able%20to%20achieve%2097%24%5C%25%24%20to%20100%24%5C%25%24%20of%20the%20performance%20of%20full%0Afine-tuning%20models.%20Additionally%2C%20the%20reduced%20number%20of%20trainable%20parameters%0Aaccelerates%20model%20training%20iterations%20and%20enhances%20the%20generalization%20and%0Arobustness%20of%20the%20oriented%20object%20detection%20model.%20The%20source%20code%20is%20available%0Aat%3A%20%5Curl%7Bhttps%3A//github.com/fudanxu/LoRA-Det%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02385v1&entry.124074799=Read"},
{"title": "Progressive Confident Masking Attention Network for Audio-Visual\n  Segmentation", "author": "Yuxuan Wang and Feng Dong and Jinchao Zhu", "abstract": "  Audio and visual signals typically occur simultaneously, and humans possess\nan innate ability to correlate and synchronize information from these two\nmodalities. Recently, a challenging problem known as Audio-Visual Segmentation\n(AVS) has emerged, intending to produce segmentation maps for sounding objects\nwithin a scene. However, the methods proposed so far have not sufficiently\nintegrated audio and visual information, and the computational costs have been\nextremely high. Additionally, the outputs of different stages have not been\nfully utilized. To facilitate this research, we introduce a novel Progressive\nConfident Masking Attention Network (PMCANet). It leverages attention\nmechanisms to uncover the intrinsic correlations between audio signals and\nvisual frames. Furthermore, we design an efficient and effective\ncross-attention module to enhance semantic perception by selecting query\ntokens. This selection is determined through confidence-driven units based on\nthe network's multi-stage predictive outputs. Experiments demonstrate that our\nnetwork outperforms other AVS methods while requiring less computational\nresources.\n", "link": "http://arxiv.org/abs/2406.02345v1", "date": "2024-06-04", "relevancy": 2.5698, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Confident%20Masking%20Attention%20Network%20for%20Audio-Visual%0A%20%20Segmentation&body=Title%3A%20Progressive%20Confident%20Masking%20Attention%20Network%20for%20Audio-Visual%0A%20%20Segmentation%0AAuthor%3A%20Yuxuan%20Wang%20and%20Feng%20Dong%20and%20Jinchao%20Zhu%0AAbstract%3A%20%20%20Audio%20and%20visual%20signals%20typically%20occur%20simultaneously%2C%20and%20humans%20possess%0Aan%20innate%20ability%20to%20correlate%20and%20synchronize%20information%20from%20these%20two%0Amodalities.%20Recently%2C%20a%20challenging%20problem%20known%20as%20Audio-Visual%20Segmentation%0A%28AVS%29%20has%20emerged%2C%20intending%20to%20produce%20segmentation%20maps%20for%20sounding%20objects%0Awithin%20a%20scene.%20However%2C%20the%20methods%20proposed%20so%20far%20have%20not%20sufficiently%0Aintegrated%20audio%20and%20visual%20information%2C%20and%20the%20computational%20costs%20have%20been%0Aextremely%20high.%20Additionally%2C%20the%20outputs%20of%20different%20stages%20have%20not%20been%0Afully%20utilized.%20To%20facilitate%20this%20research%2C%20we%20introduce%20a%20novel%20Progressive%0AConfident%20Masking%20Attention%20Network%20%28PMCANet%29.%20It%20leverages%20attention%0Amechanisms%20to%20uncover%20the%20intrinsic%20correlations%20between%20audio%20signals%20and%0Avisual%20frames.%20Furthermore%2C%20we%20design%20an%20efficient%20and%20effective%0Across-attention%20module%20to%20enhance%20semantic%20perception%20by%20selecting%20query%0Atokens.%20This%20selection%20is%20determined%20through%20confidence-driven%20units%20based%20on%0Athe%20network%27s%20multi-stage%20predictive%20outputs.%20Experiments%20demonstrate%20that%20our%0Anetwork%20outperforms%20other%20AVS%20methods%20while%20requiring%20less%20computational%0Aresources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Confident%2520Masking%2520Attention%2520Network%2520for%2520Audio-Visual%250A%2520%2520Segmentation%26entry.906535625%3DYuxuan%2520Wang%2520and%2520Feng%2520Dong%2520and%2520Jinchao%2520Zhu%26entry.1292438233%3D%2520%2520Audio%2520and%2520visual%2520signals%2520typically%2520occur%2520simultaneously%252C%2520and%2520humans%2520possess%250Aan%2520innate%2520ability%2520to%2520correlate%2520and%2520synchronize%2520information%2520from%2520these%2520two%250Amodalities.%2520Recently%252C%2520a%2520challenging%2520problem%2520known%2520as%2520Audio-Visual%2520Segmentation%250A%2528AVS%2529%2520has%2520emerged%252C%2520intending%2520to%2520produce%2520segmentation%2520maps%2520for%2520sounding%2520objects%250Awithin%2520a%2520scene.%2520However%252C%2520the%2520methods%2520proposed%2520so%2520far%2520have%2520not%2520sufficiently%250Aintegrated%2520audio%2520and%2520visual%2520information%252C%2520and%2520the%2520computational%2520costs%2520have%2520been%250Aextremely%2520high.%2520Additionally%252C%2520the%2520outputs%2520of%2520different%2520stages%2520have%2520not%2520been%250Afully%2520utilized.%2520To%2520facilitate%2520this%2520research%252C%2520we%2520introduce%2520a%2520novel%2520Progressive%250AConfident%2520Masking%2520Attention%2520Network%2520%2528PMCANet%2529.%2520It%2520leverages%2520attention%250Amechanisms%2520to%2520uncover%2520the%2520intrinsic%2520correlations%2520between%2520audio%2520signals%2520and%250Avisual%2520frames.%2520Furthermore%252C%2520we%2520design%2520an%2520efficient%2520and%2520effective%250Across-attention%2520module%2520to%2520enhance%2520semantic%2520perception%2520by%2520selecting%2520query%250Atokens.%2520This%2520selection%2520is%2520determined%2520through%2520confidence-driven%2520units%2520based%2520on%250Athe%2520network%2527s%2520multi-stage%2520predictive%2520outputs.%2520Experiments%2520demonstrate%2520that%2520our%250Anetwork%2520outperforms%2520other%2520AVS%2520methods%2520while%2520requiring%2520less%2520computational%250Aresources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Confident%20Masking%20Attention%20Network%20for%20Audio-Visual%0A%20%20Segmentation&entry.906535625=Yuxuan%20Wang%20and%20Feng%20Dong%20and%20Jinchao%20Zhu&entry.1292438233=%20%20Audio%20and%20visual%20signals%20typically%20occur%20simultaneously%2C%20and%20humans%20possess%0Aan%20innate%20ability%20to%20correlate%20and%20synchronize%20information%20from%20these%20two%0Amodalities.%20Recently%2C%20a%20challenging%20problem%20known%20as%20Audio-Visual%20Segmentation%0A%28AVS%29%20has%20emerged%2C%20intending%20to%20produce%20segmentation%20maps%20for%20sounding%20objects%0Awithin%20a%20scene.%20However%2C%20the%20methods%20proposed%20so%20far%20have%20not%20sufficiently%0Aintegrated%20audio%20and%20visual%20information%2C%20and%20the%20computational%20costs%20have%20been%0Aextremely%20high.%20Additionally%2C%20the%20outputs%20of%20different%20stages%20have%20not%20been%0Afully%20utilized.%20To%20facilitate%20this%20research%2C%20we%20introduce%20a%20novel%20Progressive%0AConfident%20Masking%20Attention%20Network%20%28PMCANet%29.%20It%20leverages%20attention%0Amechanisms%20to%20uncover%20the%20intrinsic%20correlations%20between%20audio%20signals%20and%0Avisual%20frames.%20Furthermore%2C%20we%20design%20an%20efficient%20and%20effective%0Across-attention%20module%20to%20enhance%20semantic%20perception%20by%20selecting%20query%0Atokens.%20This%20selection%20is%20determined%20through%20confidence-driven%20units%20based%20on%0Athe%20network%27s%20multi-stage%20predictive%20outputs.%20Experiments%20demonstrate%20that%20our%0Anetwork%20outperforms%20other%20AVS%20methods%20while%20requiring%20less%20computational%0Aresources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02345v1&entry.124074799=Read"},
{"title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable\n  Cyclic Image-Report Generation", "author": "Wenting Chen and Linlin Shen and Jingyang Lin and Jiebo Luo and Xiang Li and Yixuan Yuan", "abstract": "  To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n", "link": "http://arxiv.org/abs/2312.08078v5", "date": "2024-06-04", "relevancy": 2.5635, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5017}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Image-Text%20Alignment%20in%20Medical%20Imaging%20Enables%20Explainable%0A%20%20Cyclic%20Image-Report%20Generation&body=Title%3A%20Fine-Grained%20Image-Text%20Alignment%20in%20Medical%20Imaging%20Enables%20Explainable%0A%20%20Cyclic%20Image-Report%20Generation%0AAuthor%3A%20Wenting%20Chen%20and%20Linlin%20Shen%20and%20Jingyang%20Lin%20and%20Jiebo%20Luo%20and%20Xiang%20Li%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20Adaptive%20patch-word%20Matching%0A%28AdaMatch%29%20model%20to%20correlate%20chest%20X-ray%20%28CXR%29%20image%20regions%20with%20words%20in%0Amedical%20reports%20and%20apply%20it%20to%20CXR-report%20generation%20to%20provide%20explainability%0Afor%20the%20generation%20process.%20AdaMatch%20exploits%20the%20fine-grained%20relation%20between%0Aadaptive%20patches%20and%20words%20to%20provide%20explanations%20of%20specific%20image%20regions%0Awith%20corresponding%20words.%20To%20capture%20the%20abnormal%20regions%20of%20varying%20sizes%20and%0Apositions%2C%20we%20introduce%20the%20Adaptive%20Patch%20extraction%20%28AdaPatch%29%20module%20to%0Aacquire%20the%20adaptive%20patches%20for%20these%20regions%20adaptively.%20In%20order%20to%20provide%0Aexplicit%20explainability%20for%20CXR-report%20generation%20task%2C%20we%20propose%20an%0AAdaMatch-based%20bidirectional%20large%20language%20model%20for%20Cyclic%20CXR-report%0Ageneration%20%28AdaMatch-Cyclic%29.%20It%20employs%20the%20AdaMatch%20to%20obtain%20the%20keywords%0Afor%20CXR%20images%20and%20%60keypatches%27%20for%20medical%20reports%20as%20hints%20to%20guide%0ACXR-report%20generation.%20Extensive%20experiments%20on%20two%20publicly%20available%20CXR%0Adatasets%20prove%20the%20effectiveness%20of%20our%20method%20and%20its%20superior%20performance%20to%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08078v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Image-Text%2520Alignment%2520in%2520Medical%2520Imaging%2520Enables%2520Explainable%250A%2520%2520Cyclic%2520Image-Report%2520Generation%26entry.906535625%3DWenting%2520Chen%2520and%2520Linlin%2520Shen%2520and%2520Jingyang%2520Lin%2520and%2520Jiebo%2520Luo%2520and%2520Xiang%2520Li%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520Adaptive%2520patch-word%2520Matching%250A%2528AdaMatch%2529%2520model%2520to%2520correlate%2520chest%2520X-ray%2520%2528CXR%2529%2520image%2520regions%2520with%2520words%2520in%250Amedical%2520reports%2520and%2520apply%2520it%2520to%2520CXR-report%2520generation%2520to%2520provide%2520explainability%250Afor%2520the%2520generation%2520process.%2520AdaMatch%2520exploits%2520the%2520fine-grained%2520relation%2520between%250Aadaptive%2520patches%2520and%2520words%2520to%2520provide%2520explanations%2520of%2520specific%2520image%2520regions%250Awith%2520corresponding%2520words.%2520To%2520capture%2520the%2520abnormal%2520regions%2520of%2520varying%2520sizes%2520and%250Apositions%252C%2520we%2520introduce%2520the%2520Adaptive%2520Patch%2520extraction%2520%2528AdaPatch%2529%2520module%2520to%250Aacquire%2520the%2520adaptive%2520patches%2520for%2520these%2520regions%2520adaptively.%2520In%2520order%2520to%2520provide%250Aexplicit%2520explainability%2520for%2520CXR-report%2520generation%2520task%252C%2520we%2520propose%2520an%250AAdaMatch-based%2520bidirectional%2520large%2520language%2520model%2520for%2520Cyclic%2520CXR-report%250Ageneration%2520%2528AdaMatch-Cyclic%2529.%2520It%2520employs%2520the%2520AdaMatch%2520to%2520obtain%2520the%2520keywords%250Afor%2520CXR%2520images%2520and%2520%2560keypatches%2527%2520for%2520medical%2520reports%2520as%2520hints%2520to%2520guide%250ACXR-report%2520generation.%2520Extensive%2520experiments%2520on%2520two%2520publicly%2520available%2520CXR%250Adatasets%2520prove%2520the%2520effectiveness%2520of%2520our%2520method%2520and%2520its%2520superior%2520performance%2520to%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08078v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Image-Text%20Alignment%20in%20Medical%20Imaging%20Enables%20Explainable%0A%20%20Cyclic%20Image-Report%20Generation&entry.906535625=Wenting%20Chen%20and%20Linlin%20Shen%20and%20Jingyang%20Lin%20and%20Jiebo%20Luo%20and%20Xiang%20Li%20and%20Yixuan%20Yuan&entry.1292438233=%20%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20Adaptive%20patch-word%20Matching%0A%28AdaMatch%29%20model%20to%20correlate%20chest%20X-ray%20%28CXR%29%20image%20regions%20with%20words%20in%0Amedical%20reports%20and%20apply%20it%20to%20CXR-report%20generation%20to%20provide%20explainability%0Afor%20the%20generation%20process.%20AdaMatch%20exploits%20the%20fine-grained%20relation%20between%0Aadaptive%20patches%20and%20words%20to%20provide%20explanations%20of%20specific%20image%20regions%0Awith%20corresponding%20words.%20To%20capture%20the%20abnormal%20regions%20of%20varying%20sizes%20and%0Apositions%2C%20we%20introduce%20the%20Adaptive%20Patch%20extraction%20%28AdaPatch%29%20module%20to%0Aacquire%20the%20adaptive%20patches%20for%20these%20regions%20adaptively.%20In%20order%20to%20provide%0Aexplicit%20explainability%20for%20CXR-report%20generation%20task%2C%20we%20propose%20an%0AAdaMatch-based%20bidirectional%20large%20language%20model%20for%20Cyclic%20CXR-report%0Ageneration%20%28AdaMatch-Cyclic%29.%20It%20employs%20the%20AdaMatch%20to%20obtain%20the%20keywords%0Afor%20CXR%20images%20and%20%60keypatches%27%20for%20medical%20reports%20as%20hints%20to%20guide%0ACXR-report%20generation.%20Extensive%20experiments%20on%20two%20publicly%20available%20CXR%0Adatasets%20prove%20the%20effectiveness%20of%20our%20method%20and%20its%20superior%20performance%20to%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08078v5&entry.124074799=Read"},
{"title": "An Empirical Study into Clustering of Unseen Datasets with\n  Self-Supervised Encoders", "author": "Scott C. Lowe and Joakim Bruslund Haurum and Sageev Oore and Thomas B. Moeslund and Graham W. Taylor", "abstract": "  Can pretrained models generalize to new datasets without any retraining? We\ndeploy pretrained image models on datasets they were not trained for, and\ninvestigate whether their embeddings form meaningful clusters. Our suite of\nbenchmarking experiments use encoders pretrained solely on ImageNet-1k with\neither supervised or self-supervised training techniques, deployed on image\ndatasets that were not seen during training, and clustered with conventional\nclustering algorithms. This evaluation provides new insights into the\nembeddings of self-supervised models, which prioritize different features to\nsupervised models. Supervised encoders typically offer more utility than SSL\nencoders within the training domain, and vice-versa far outside of it, however,\nfine-tuned encoders demonstrate the opposite trend. Clustering provides a way\nto evaluate the utility of self-supervised learned representations orthogonal\nto existing methods such as kNN. Additionally, we find the silhouette score\nwhen measured in a UMAP-reduced space is highly correlated with clustering\nperformance, and can therefore be used as a proxy for clustering performance on\ndata with no ground truth labels. Our code implementation is available at\n\\url{https://github.com/scottclowe/zs-ssl-clustering/}.\n", "link": "http://arxiv.org/abs/2406.02465v1", "date": "2024-06-04", "relevancy": 2.5203, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5147}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5128}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20into%20Clustering%20of%20Unseen%20Datasets%20with%0A%20%20Self-Supervised%20Encoders&body=Title%3A%20An%20Empirical%20Study%20into%20Clustering%20of%20Unseen%20Datasets%20with%0A%20%20Self-Supervised%20Encoders%0AAuthor%3A%20Scott%20C.%20Lowe%20and%20Joakim%20Bruslund%20Haurum%20and%20Sageev%20Oore%20and%20Thomas%20B.%20Moeslund%20and%20Graham%20W.%20Taylor%0AAbstract%3A%20%20%20Can%20pretrained%20models%20generalize%20to%20new%20datasets%20without%20any%20retraining%3F%20We%0Adeploy%20pretrained%20image%20models%20on%20datasets%20they%20were%20not%20trained%20for%2C%20and%0Ainvestigate%20whether%20their%20embeddings%20form%20meaningful%20clusters.%20Our%20suite%20of%0Abenchmarking%20experiments%20use%20encoders%20pretrained%20solely%20on%20ImageNet-1k%20with%0Aeither%20supervised%20or%20self-supervised%20training%20techniques%2C%20deployed%20on%20image%0Adatasets%20that%20were%20not%20seen%20during%20training%2C%20and%20clustered%20with%20conventional%0Aclustering%20algorithms.%20This%20evaluation%20provides%20new%20insights%20into%20the%0Aembeddings%20of%20self-supervised%20models%2C%20which%20prioritize%20different%20features%20to%0Asupervised%20models.%20Supervised%20encoders%20typically%20offer%20more%20utility%20than%20SSL%0Aencoders%20within%20the%20training%20domain%2C%20and%20vice-versa%20far%20outside%20of%20it%2C%20however%2C%0Afine-tuned%20encoders%20demonstrate%20the%20opposite%20trend.%20Clustering%20provides%20a%20way%0Ato%20evaluate%20the%20utility%20of%20self-supervised%20learned%20representations%20orthogonal%0Ato%20existing%20methods%20such%20as%20kNN.%20Additionally%2C%20we%20find%20the%20silhouette%20score%0Awhen%20measured%20in%20a%20UMAP-reduced%20space%20is%20highly%20correlated%20with%20clustering%0Aperformance%2C%20and%20can%20therefore%20be%20used%20as%20a%20proxy%20for%20clustering%20performance%20on%0Adata%20with%20no%20ground%20truth%20labels.%20Our%20code%20implementation%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/scottclowe/zs-ssl-clustering/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520into%2520Clustering%2520of%2520Unseen%2520Datasets%2520with%250A%2520%2520Self-Supervised%2520Encoders%26entry.906535625%3DScott%2520C.%2520Lowe%2520and%2520Joakim%2520Bruslund%2520Haurum%2520and%2520Sageev%2520Oore%2520and%2520Thomas%2520B.%2520Moeslund%2520and%2520Graham%2520W.%2520Taylor%26entry.1292438233%3D%2520%2520Can%2520pretrained%2520models%2520generalize%2520to%2520new%2520datasets%2520without%2520any%2520retraining%253F%2520We%250Adeploy%2520pretrained%2520image%2520models%2520on%2520datasets%2520they%2520were%2520not%2520trained%2520for%252C%2520and%250Ainvestigate%2520whether%2520their%2520embeddings%2520form%2520meaningful%2520clusters.%2520Our%2520suite%2520of%250Abenchmarking%2520experiments%2520use%2520encoders%2520pretrained%2520solely%2520on%2520ImageNet-1k%2520with%250Aeither%2520supervised%2520or%2520self-supervised%2520training%2520techniques%252C%2520deployed%2520on%2520image%250Adatasets%2520that%2520were%2520not%2520seen%2520during%2520training%252C%2520and%2520clustered%2520with%2520conventional%250Aclustering%2520algorithms.%2520This%2520evaluation%2520provides%2520new%2520insights%2520into%2520the%250Aembeddings%2520of%2520self-supervised%2520models%252C%2520which%2520prioritize%2520different%2520features%2520to%250Asupervised%2520models.%2520Supervised%2520encoders%2520typically%2520offer%2520more%2520utility%2520than%2520SSL%250Aencoders%2520within%2520the%2520training%2520domain%252C%2520and%2520vice-versa%2520far%2520outside%2520of%2520it%252C%2520however%252C%250Afine-tuned%2520encoders%2520demonstrate%2520the%2520opposite%2520trend.%2520Clustering%2520provides%2520a%2520way%250Ato%2520evaluate%2520the%2520utility%2520of%2520self-supervised%2520learned%2520representations%2520orthogonal%250Ato%2520existing%2520methods%2520such%2520as%2520kNN.%2520Additionally%252C%2520we%2520find%2520the%2520silhouette%2520score%250Awhen%2520measured%2520in%2520a%2520UMAP-reduced%2520space%2520is%2520highly%2520correlated%2520with%2520clustering%250Aperformance%252C%2520and%2520can%2520therefore%2520be%2520used%2520as%2520a%2520proxy%2520for%2520clustering%2520performance%2520on%250Adata%2520with%2520no%2520ground%2520truth%2520labels.%2520Our%2520code%2520implementation%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/scottclowe/zs-ssl-clustering/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20into%20Clustering%20of%20Unseen%20Datasets%20with%0A%20%20Self-Supervised%20Encoders&entry.906535625=Scott%20C.%20Lowe%20and%20Joakim%20Bruslund%20Haurum%20and%20Sageev%20Oore%20and%20Thomas%20B.%20Moeslund%20and%20Graham%20W.%20Taylor&entry.1292438233=%20%20Can%20pretrained%20models%20generalize%20to%20new%20datasets%20without%20any%20retraining%3F%20We%0Adeploy%20pretrained%20image%20models%20on%20datasets%20they%20were%20not%20trained%20for%2C%20and%0Ainvestigate%20whether%20their%20embeddings%20form%20meaningful%20clusters.%20Our%20suite%20of%0Abenchmarking%20experiments%20use%20encoders%20pretrained%20solely%20on%20ImageNet-1k%20with%0Aeither%20supervised%20or%20self-supervised%20training%20techniques%2C%20deployed%20on%20image%0Adatasets%20that%20were%20not%20seen%20during%20training%2C%20and%20clustered%20with%20conventional%0Aclustering%20algorithms.%20This%20evaluation%20provides%20new%20insights%20into%20the%0Aembeddings%20of%20self-supervised%20models%2C%20which%20prioritize%20different%20features%20to%0Asupervised%20models.%20Supervised%20encoders%20typically%20offer%20more%20utility%20than%20SSL%0Aencoders%20within%20the%20training%20domain%2C%20and%20vice-versa%20far%20outside%20of%20it%2C%20however%2C%0Afine-tuned%20encoders%20demonstrate%20the%20opposite%20trend.%20Clustering%20provides%20a%20way%0Ato%20evaluate%20the%20utility%20of%20self-supervised%20learned%20representations%20orthogonal%0Ato%20existing%20methods%20such%20as%20kNN.%20Additionally%2C%20we%20find%20the%20silhouette%20score%0Awhen%20measured%20in%20a%20UMAP-reduced%20space%20is%20highly%20correlated%20with%20clustering%0Aperformance%2C%20and%20can%20therefore%20be%20used%20as%20a%20proxy%20for%20clustering%20performance%20on%0Adata%20with%20no%20ground%20truth%20labels.%20Our%20code%20implementation%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/scottclowe/zs-ssl-clustering/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02465v1&entry.124074799=Read"},
{"title": "Reducing Bias in Federated Class-Incremental Learning with Hierarchical\n  Generative Prototypes", "author": "Riccardo Salami and Pietro Buzzega and Matteo Mosconi and Mattia Verasani and Simone Calderara", "abstract": "  Federated Learning (FL) aims at unburdening the training of deep models by\ndistributing computation across multiple devices (clients) while safeguarding\ndata privacy. On top of that, Federated Continual Learning (FCL) also accounts\nfor data distribution evolving over time, mirroring the dynamic nature of\nreal-world environments. In this work, we shed light on the Incremental and\nFederated biases that naturally emerge in FCL. While the former is a known\nproblem in Continual Learning, stemming from the prioritization of recently\nintroduced classes, the latter (i.e., the bias towards local distributions)\nremains relatively unexplored. Our proposal constrains both biases in the last\nlayer by efficiently fine-tuning a pre-trained backbone using learnable\nprompts, resulting in clients that produce less biased representations and more\nbiased classifiers. Therefore, instead of solely relying on parameter\naggregation, we also leverage generative prototypes to effectively balance the\npredictions of the global model. Our method improves on the current State Of\nThe Art, providing an average increase of +7.9% in accuracy.\n", "link": "http://arxiv.org/abs/2406.02447v1", "date": "2024-06-04", "relevancy": 2.5031, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5268}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4906}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20Bias%20in%20Federated%20Class-Incremental%20Learning%20with%20Hierarchical%0A%20%20Generative%20Prototypes&body=Title%3A%20Reducing%20Bias%20in%20Federated%20Class-Incremental%20Learning%20with%20Hierarchical%0A%20%20Generative%20Prototypes%0AAuthor%3A%20Riccardo%20Salami%20and%20Pietro%20Buzzega%20and%20Matteo%20Mosconi%20and%20Mattia%20Verasani%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20aims%20at%20unburdening%20the%20training%20of%20deep%20models%20by%0Adistributing%20computation%20across%20multiple%20devices%20%28clients%29%20while%20safeguarding%0Adata%20privacy.%20On%20top%20of%20that%2C%20Federated%20Continual%20Learning%20%28FCL%29%20also%20accounts%0Afor%20data%20distribution%20evolving%20over%20time%2C%20mirroring%20the%20dynamic%20nature%20of%0Areal-world%20environments.%20In%20this%20work%2C%20we%20shed%20light%20on%20the%20Incremental%20and%0AFederated%20biases%20that%20naturally%20emerge%20in%20FCL.%20While%20the%20former%20is%20a%20known%0Aproblem%20in%20Continual%20Learning%2C%20stemming%20from%20the%20prioritization%20of%20recently%0Aintroduced%20classes%2C%20the%20latter%20%28i.e.%2C%20the%20bias%20towards%20local%20distributions%29%0Aremains%20relatively%20unexplored.%20Our%20proposal%20constrains%20both%20biases%20in%20the%20last%0Alayer%20by%20efficiently%20fine-tuning%20a%20pre-trained%20backbone%20using%20learnable%0Aprompts%2C%20resulting%20in%20clients%20that%20produce%20less%20biased%20representations%20and%20more%0Abiased%20classifiers.%20Therefore%2C%20instead%20of%20solely%20relying%20on%20parameter%0Aaggregation%2C%20we%20also%20leverage%20generative%20prototypes%20to%20effectively%20balance%20the%0Apredictions%20of%20the%20global%20model.%20Our%20method%20improves%20on%20the%20current%20State%20Of%0AThe%20Art%2C%20providing%20an%20average%20increase%20of%20%2B7.9%25%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520Bias%2520in%2520Federated%2520Class-Incremental%2520Learning%2520with%2520Hierarchical%250A%2520%2520Generative%2520Prototypes%26entry.906535625%3DRiccardo%2520Salami%2520and%2520Pietro%2520Buzzega%2520and%2520Matteo%2520Mosconi%2520and%2520Mattia%2520Verasani%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520aims%2520at%2520unburdening%2520the%2520training%2520of%2520deep%2520models%2520by%250Adistributing%2520computation%2520across%2520multiple%2520devices%2520%2528clients%2529%2520while%2520safeguarding%250Adata%2520privacy.%2520On%2520top%2520of%2520that%252C%2520Federated%2520Continual%2520Learning%2520%2528FCL%2529%2520also%2520accounts%250Afor%2520data%2520distribution%2520evolving%2520over%2520time%252C%2520mirroring%2520the%2520dynamic%2520nature%2520of%250Areal-world%2520environments.%2520In%2520this%2520work%252C%2520we%2520shed%2520light%2520on%2520the%2520Incremental%2520and%250AFederated%2520biases%2520that%2520naturally%2520emerge%2520in%2520FCL.%2520While%2520the%2520former%2520is%2520a%2520known%250Aproblem%2520in%2520Continual%2520Learning%252C%2520stemming%2520from%2520the%2520prioritization%2520of%2520recently%250Aintroduced%2520classes%252C%2520the%2520latter%2520%2528i.e.%252C%2520the%2520bias%2520towards%2520local%2520distributions%2529%250Aremains%2520relatively%2520unexplored.%2520Our%2520proposal%2520constrains%2520both%2520biases%2520in%2520the%2520last%250Alayer%2520by%2520efficiently%2520fine-tuning%2520a%2520pre-trained%2520backbone%2520using%2520learnable%250Aprompts%252C%2520resulting%2520in%2520clients%2520that%2520produce%2520less%2520biased%2520representations%2520and%2520more%250Abiased%2520classifiers.%2520Therefore%252C%2520instead%2520of%2520solely%2520relying%2520on%2520parameter%250Aaggregation%252C%2520we%2520also%2520leverage%2520generative%2520prototypes%2520to%2520effectively%2520balance%2520the%250Apredictions%2520of%2520the%2520global%2520model.%2520Our%2520method%2520improves%2520on%2520the%2520current%2520State%2520Of%250AThe%2520Art%252C%2520providing%2520an%2520average%2520increase%2520of%2520%252B7.9%2525%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20Bias%20in%20Federated%20Class-Incremental%20Learning%20with%20Hierarchical%0A%20%20Generative%20Prototypes&entry.906535625=Riccardo%20Salami%20and%20Pietro%20Buzzega%20and%20Matteo%20Mosconi%20and%20Mattia%20Verasani%20and%20Simone%20Calderara&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20aims%20at%20unburdening%20the%20training%20of%20deep%20models%20by%0Adistributing%20computation%20across%20multiple%20devices%20%28clients%29%20while%20safeguarding%0Adata%20privacy.%20On%20top%20of%20that%2C%20Federated%20Continual%20Learning%20%28FCL%29%20also%20accounts%0Afor%20data%20distribution%20evolving%20over%20time%2C%20mirroring%20the%20dynamic%20nature%20of%0Areal-world%20environments.%20In%20this%20work%2C%20we%20shed%20light%20on%20the%20Incremental%20and%0AFederated%20biases%20that%20naturally%20emerge%20in%20FCL.%20While%20the%20former%20is%20a%20known%0Aproblem%20in%20Continual%20Learning%2C%20stemming%20from%20the%20prioritization%20of%20recently%0Aintroduced%20classes%2C%20the%20latter%20%28i.e.%2C%20the%20bias%20towards%20local%20distributions%29%0Aremains%20relatively%20unexplored.%20Our%20proposal%20constrains%20both%20biases%20in%20the%20last%0Alayer%20by%20efficiently%20fine-tuning%20a%20pre-trained%20backbone%20using%20learnable%0Aprompts%2C%20resulting%20in%20clients%20that%20produce%20less%20biased%20representations%20and%20more%0Abiased%20classifiers.%20Therefore%2C%20instead%20of%20solely%20relying%20on%20parameter%0Aaggregation%2C%20we%20also%20leverage%20generative%20prototypes%20to%20effectively%20balance%20the%0Apredictions%20of%20the%20global%20model.%20Our%20method%20improves%20on%20the%20current%20State%20Of%0AThe%20Art%2C%20providing%20an%20average%20increase%20of%20%2B7.9%25%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02447v1&entry.124074799=Read"},
{"title": "FitDiff: Robust monocular 3D facial shape and reflectance estimation\n  using Diffusion Models", "author": "Stathis Galanakis and Alexandros Lattas and Stylianos Moschoglou and Stefanos Zafeiriou", "abstract": "  The remarkable progress in 3D face reconstruction has resulted in high-detail\nand photorealistic facial representations. Recently, Diffusion Models have\nrevolutionized the capabilities of generative methods by surpassing the\nperformance of GANs. In this work, we present FitDiff, a diffusion-based 3D\nfacial avatar generative model. Leveraging diffusion principles, our model\naccurately generates relightable facial avatars, utilizing an identity\nembedding extracted from an \"in-the-wild\" 2D facial image. The introduced\nmulti-modal diffusion model is the first to concurrently output facial\nreflectance maps (diffuse and specular albedo and normals) and shapes,\nshowcasing great generalization capabilities. It is solely trained on an\nannotated subset of a public facial dataset, paired with 3D reconstructions. We\nrevisit the typical 3D facial fitting approach by guiding a reverse diffusion\nprocess using perceptual and face recognition losses. Being the first 3D LDM\nconditioned on face recognition embeddings, FitDiff reconstructs relightable\nhuman avatars, that can be used as-is in common rendering engines, starting\nonly from an unconstrained facial image, and achieving state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2312.04465v2", "date": "2024-06-04", "relevancy": 2.4736, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6201}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6201}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FitDiff%3A%20Robust%20monocular%203D%20facial%20shape%20and%20reflectance%20estimation%0A%20%20using%20Diffusion%20Models&body=Title%3A%20FitDiff%3A%20Robust%20monocular%203D%20facial%20shape%20and%20reflectance%20estimation%0A%20%20using%20Diffusion%20Models%0AAuthor%3A%20Stathis%20Galanakis%20and%20Alexandros%20Lattas%20and%20Stylianos%20Moschoglou%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20%20%20The%20remarkable%20progress%20in%203D%20face%20reconstruction%20has%20resulted%20in%20high-detail%0Aand%20photorealistic%20facial%20representations.%20Recently%2C%20Diffusion%20Models%20have%0Arevolutionized%20the%20capabilities%20of%20generative%20methods%20by%20surpassing%20the%0Aperformance%20of%20GANs.%20In%20this%20work%2C%20we%20present%20FitDiff%2C%20a%20diffusion-based%203D%0Afacial%20avatar%20generative%20model.%20Leveraging%20diffusion%20principles%2C%20our%20model%0Aaccurately%20generates%20relightable%20facial%20avatars%2C%20utilizing%20an%20identity%0Aembedding%20extracted%20from%20an%20%22in-the-wild%22%202D%20facial%20image.%20The%20introduced%0Amulti-modal%20diffusion%20model%20is%20the%20first%20to%20concurrently%20output%20facial%0Areflectance%20maps%20%28diffuse%20and%20specular%20albedo%20and%20normals%29%20and%20shapes%2C%0Ashowcasing%20great%20generalization%20capabilities.%20It%20is%20solely%20trained%20on%20an%0Aannotated%20subset%20of%20a%20public%20facial%20dataset%2C%20paired%20with%203D%20reconstructions.%20We%0Arevisit%20the%20typical%203D%20facial%20fitting%20approach%20by%20guiding%20a%20reverse%20diffusion%0Aprocess%20using%20perceptual%20and%20face%20recognition%20losses.%20Being%20the%20first%203D%20LDM%0Aconditioned%20on%20face%20recognition%20embeddings%2C%20FitDiff%20reconstructs%20relightable%0Ahuman%20avatars%2C%20that%20can%20be%20used%20as-is%20in%20common%20rendering%20engines%2C%20starting%0Aonly%20from%20an%20unconstrained%20facial%20image%2C%20and%20achieving%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFitDiff%253A%2520Robust%2520monocular%25203D%2520facial%2520shape%2520and%2520reflectance%2520estimation%250A%2520%2520using%2520Diffusion%2520Models%26entry.906535625%3DStathis%2520Galanakis%2520and%2520Alexandros%2520Lattas%2520and%2520Stylianos%2520Moschoglou%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3D%2520%2520The%2520remarkable%2520progress%2520in%25203D%2520face%2520reconstruction%2520has%2520resulted%2520in%2520high-detail%250Aand%2520photorealistic%2520facial%2520representations.%2520Recently%252C%2520Diffusion%2520Models%2520have%250Arevolutionized%2520the%2520capabilities%2520of%2520generative%2520methods%2520by%2520surpassing%2520the%250Aperformance%2520of%2520GANs.%2520In%2520this%2520work%252C%2520we%2520present%2520FitDiff%252C%2520a%2520diffusion-based%25203D%250Afacial%2520avatar%2520generative%2520model.%2520Leveraging%2520diffusion%2520principles%252C%2520our%2520model%250Aaccurately%2520generates%2520relightable%2520facial%2520avatars%252C%2520utilizing%2520an%2520identity%250Aembedding%2520extracted%2520from%2520an%2520%2522in-the-wild%2522%25202D%2520facial%2520image.%2520The%2520introduced%250Amulti-modal%2520diffusion%2520model%2520is%2520the%2520first%2520to%2520concurrently%2520output%2520facial%250Areflectance%2520maps%2520%2528diffuse%2520and%2520specular%2520albedo%2520and%2520normals%2529%2520and%2520shapes%252C%250Ashowcasing%2520great%2520generalization%2520capabilities.%2520It%2520is%2520solely%2520trained%2520on%2520an%250Aannotated%2520subset%2520of%2520a%2520public%2520facial%2520dataset%252C%2520paired%2520with%25203D%2520reconstructions.%2520We%250Arevisit%2520the%2520typical%25203D%2520facial%2520fitting%2520approach%2520by%2520guiding%2520a%2520reverse%2520diffusion%250Aprocess%2520using%2520perceptual%2520and%2520face%2520recognition%2520losses.%2520Being%2520the%2520first%25203D%2520LDM%250Aconditioned%2520on%2520face%2520recognition%2520embeddings%252C%2520FitDiff%2520reconstructs%2520relightable%250Ahuman%2520avatars%252C%2520that%2520can%2520be%2520used%2520as-is%2520in%2520common%2520rendering%2520engines%252C%2520starting%250Aonly%2520from%2520an%2520unconstrained%2520facial%2520image%252C%2520and%2520achieving%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FitDiff%3A%20Robust%20monocular%203D%20facial%20shape%20and%20reflectance%20estimation%0A%20%20using%20Diffusion%20Models&entry.906535625=Stathis%20Galanakis%20and%20Alexandros%20Lattas%20and%20Stylianos%20Moschoglou%20and%20Stefanos%20Zafeiriou&entry.1292438233=%20%20The%20remarkable%20progress%20in%203D%20face%20reconstruction%20has%20resulted%20in%20high-detail%0Aand%20photorealistic%20facial%20representations.%20Recently%2C%20Diffusion%20Models%20have%0Arevolutionized%20the%20capabilities%20of%20generative%20methods%20by%20surpassing%20the%0Aperformance%20of%20GANs.%20In%20this%20work%2C%20we%20present%20FitDiff%2C%20a%20diffusion-based%203D%0Afacial%20avatar%20generative%20model.%20Leveraging%20diffusion%20principles%2C%20our%20model%0Aaccurately%20generates%20relightable%20facial%20avatars%2C%20utilizing%20an%20identity%0Aembedding%20extracted%20from%20an%20%22in-the-wild%22%202D%20facial%20image.%20The%20introduced%0Amulti-modal%20diffusion%20model%20is%20the%20first%20to%20concurrently%20output%20facial%0Areflectance%20maps%20%28diffuse%20and%20specular%20albedo%20and%20normals%29%20and%20shapes%2C%0Ashowcasing%20great%20generalization%20capabilities.%20It%20is%20solely%20trained%20on%20an%0Aannotated%20subset%20of%20a%20public%20facial%20dataset%2C%20paired%20with%203D%20reconstructions.%20We%0Arevisit%20the%20typical%203D%20facial%20fitting%20approach%20by%20guiding%20a%20reverse%20diffusion%0Aprocess%20using%20perceptual%20and%20face%20recognition%20losses.%20Being%20the%20first%203D%20LDM%0Aconditioned%20on%20face%20recognition%20embeddings%2C%20FitDiff%20reconstructs%20relightable%0Ahuman%20avatars%2C%20that%20can%20be%20used%20as-is%20in%20common%20rendering%20engines%2C%20starting%0Aonly%20from%20an%20unconstrained%20facial%20image%2C%20and%20achieving%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04465v2&entry.124074799=Read"},
{"title": "Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning", "author": "Wenyan Li and Jiaang Li and Rita Ramos and Raphael Tang and Desmond Elliott", "abstract": "  Recent advancements in retrieval-augmented models for image captioning\nhighlight the significance of retrieving related captions for efficient,\nlightweight models with strong domain-transfer capabilities. While these models\ndemonstrate the success of retrieval augmentation, retrieval models are still\nfar from perfect in practice. Retrieved information can sometimes mislead the\nmodel generation, negatively impacting performance. In this paper, we analyze\nthe robustness of the SmallCap retrieval-augmented captioning model. Our\nanalysis shows that SmallCap is sensitive to tokens that appear in the majority\nof the retrieved captions, and integrated gradients attribution shows that\nthose tokens are likely copied into the final caption. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This reduces the probability that the model learns to copy majority\ntokens and improves both in-domain and cross-domain performance effectively.\n", "link": "http://arxiv.org/abs/2406.02265v1", "date": "2024-06-04", "relevancy": 2.471, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5029}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4916}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning&body=Title%3A%20Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning%0AAuthor%3A%20Wenyan%20Li%20and%20Jiaang%20Li%20and%20Rita%20Ramos%20and%20Raphael%20Tang%20and%20Desmond%20Elliott%0AAbstract%3A%20%20%20Recent%20advancements%20in%20retrieval-augmented%20models%20for%20image%20captioning%0Ahighlight%20the%20significance%20of%20retrieving%20related%20captions%20for%20efficient%2C%0Alightweight%20models%20with%20strong%20domain-transfer%20capabilities.%20While%20these%20models%0Ademonstrate%20the%20success%20of%20retrieval%20augmentation%2C%20retrieval%20models%20are%20still%0Afar%20from%20perfect%20in%20practice.%20Retrieved%20information%20can%20sometimes%20mislead%20the%0Amodel%20generation%2C%20negatively%20impacting%20performance.%20In%20this%20paper%2C%20we%20analyze%0Athe%20robustness%20of%20the%20SmallCap%20retrieval-augmented%20captioning%20model.%20Our%0Aanalysis%20shows%20that%20SmallCap%20is%20sensitive%20to%20tokens%20that%20appear%20in%20the%20majority%0Aof%20the%20retrieved%20captions%2C%20and%20integrated%20gradients%20attribution%20shows%20that%0Athose%20tokens%20are%20likely%20copied%20into%20the%20final%20caption.%20Given%20these%20findings%2C%20we%0Apropose%20to%20train%20the%20model%20by%20sampling%20retrieved%20captions%20from%20more%20diverse%0Asets.%20This%20reduces%20the%20probability%20that%20the%20model%20learns%20to%20copy%20majority%0Atokens%20and%20improves%20both%20in-domain%20and%20cross-domain%20performance%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Retrieval%2520Robustness%2520for%2520Retrieval-Augmented%2520Image%250A%2520%2520Captioning%26entry.906535625%3DWenyan%2520Li%2520and%2520Jiaang%2520Li%2520and%2520Rita%2520Ramos%2520and%2520Raphael%2520Tang%2520and%2520Desmond%2520Elliott%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520retrieval-augmented%2520models%2520for%2520image%2520captioning%250Ahighlight%2520the%2520significance%2520of%2520retrieving%2520related%2520captions%2520for%2520efficient%252C%250Alightweight%2520models%2520with%2520strong%2520domain-transfer%2520capabilities.%2520While%2520these%2520models%250Ademonstrate%2520the%2520success%2520of%2520retrieval%2520augmentation%252C%2520retrieval%2520models%2520are%2520still%250Afar%2520from%2520perfect%2520in%2520practice.%2520Retrieved%2520information%2520can%2520sometimes%2520mislead%2520the%250Amodel%2520generation%252C%2520negatively%2520impacting%2520performance.%2520In%2520this%2520paper%252C%2520we%2520analyze%250Athe%2520robustness%2520of%2520the%2520SmallCap%2520retrieval-augmented%2520captioning%2520model.%2520Our%250Aanalysis%2520shows%2520that%2520SmallCap%2520is%2520sensitive%2520to%2520tokens%2520that%2520appear%2520in%2520the%2520majority%250Aof%2520the%2520retrieved%2520captions%252C%2520and%2520integrated%2520gradients%2520attribution%2520shows%2520that%250Athose%2520tokens%2520are%2520likely%2520copied%2520into%2520the%2520final%2520caption.%2520Given%2520these%2520findings%252C%2520we%250Apropose%2520to%2520train%2520the%2520model%2520by%2520sampling%2520retrieved%2520captions%2520from%2520more%2520diverse%250Asets.%2520This%2520reduces%2520the%2520probability%2520that%2520the%2520model%2520learns%2520to%2520copy%2520majority%250Atokens%2520and%2520improves%2520both%2520in-domain%2520and%2520cross-domain%2520performance%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning&entry.906535625=Wenyan%20Li%20and%20Jiaang%20Li%20and%20Rita%20Ramos%20and%20Raphael%20Tang%20and%20Desmond%20Elliott&entry.1292438233=%20%20Recent%20advancements%20in%20retrieval-augmented%20models%20for%20image%20captioning%0Ahighlight%20the%20significance%20of%20retrieving%20related%20captions%20for%20efficient%2C%0Alightweight%20models%20with%20strong%20domain-transfer%20capabilities.%20While%20these%20models%0Ademonstrate%20the%20success%20of%20retrieval%20augmentation%2C%20retrieval%20models%20are%20still%0Afar%20from%20perfect%20in%20practice.%20Retrieved%20information%20can%20sometimes%20mislead%20the%0Amodel%20generation%2C%20negatively%20impacting%20performance.%20In%20this%20paper%2C%20we%20analyze%0Athe%20robustness%20of%20the%20SmallCap%20retrieval-augmented%20captioning%20model.%20Our%0Aanalysis%20shows%20that%20SmallCap%20is%20sensitive%20to%20tokens%20that%20appear%20in%20the%20majority%0Aof%20the%20retrieved%20captions%2C%20and%20integrated%20gradients%20attribution%20shows%20that%0Athose%20tokens%20are%20likely%20copied%20into%20the%20final%20caption.%20Given%20these%20findings%2C%20we%0Apropose%20to%20train%20the%20model%20by%20sampling%20retrieved%20captions%20from%20more%20diverse%0Asets.%20This%20reduces%20the%20probability%20that%20the%20model%20learns%20to%20copy%20majority%0Atokens%20and%20improves%20both%20in-domain%20and%20cross-domain%20performance%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02265v1&entry.124074799=Read"},
{"title": "Query-based Semantic Gaussian Field for Scene Representation in\n  Reinforcement Learning", "author": "Jiaxu Wang and Ziyi Zhang and Qiang Zhang and Jia Li and Jingkai Sun and Mingyuan Sun and Junhao He and Renjing Xu", "abstract": "  Latent scene representation plays a significant role in training\nreinforcement learning (RL) agents. To obtain good latent vectors describing\nthe scenes, recent works incorporate the 3D-aware latent-conditioned NeRF\npipeline into scene representation learning. However, these NeRF-related\nmethods struggle to perceive 3D structural information due to the inefficient\ndense sampling in volumetric rendering. Moreover, they lack fine-grained\nsemantic information included in their scene representation vectors because\nthey evenly consider free and occupied spaces. Both of them can destroy the\nperformance of downstream RL tasks. To address the above challenges, we propose\na novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to\nlearn 3D scene representation for the first time. In brief, we present the\nQuery-based Generalizable 3DGS to bridge the 3DGS technique and scene\nrepresentations with more geometrical awareness than those in NeRFs. Moreover,\nwe present the Hierarchical Semantics Encoding to ground the fine-grained\nsemantic features to 3D Gaussians and further distilled to the scene\nrepresentation vectors. We conduct extensive experiments on two RL platforms\nincluding Maniskill2 and Robomimic across 10 different tasks. The results show\nthat our method outperforms the other 5 baselines by a large margin. We achieve\nthe best success rates on 8 tasks and the second-best on the other two tasks.\n", "link": "http://arxiv.org/abs/2406.02370v1", "date": "2024-06-04", "relevancy": 2.4568, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6599}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5881}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Query-based%20Semantic%20Gaussian%20Field%20for%20Scene%20Representation%20in%0A%20%20Reinforcement%20Learning&body=Title%3A%20Query-based%20Semantic%20Gaussian%20Field%20for%20Scene%20Representation%20in%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Jiaxu%20Wang%20and%20Ziyi%20Zhang%20and%20Qiang%20Zhang%20and%20Jia%20Li%20and%20Jingkai%20Sun%20and%20Mingyuan%20Sun%20and%20Junhao%20He%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Latent%20scene%20representation%20plays%20a%20significant%20role%20in%20training%0Areinforcement%20learning%20%28RL%29%20agents.%20To%20obtain%20good%20latent%20vectors%20describing%0Athe%20scenes%2C%20recent%20works%20incorporate%20the%203D-aware%20latent-conditioned%20NeRF%0Apipeline%20into%20scene%20representation%20learning.%20However%2C%20these%20NeRF-related%0Amethods%20struggle%20to%20perceive%203D%20structural%20information%20due%20to%20the%20inefficient%0Adense%20sampling%20in%20volumetric%20rendering.%20Moreover%2C%20they%20lack%20fine-grained%0Asemantic%20information%20included%20in%20their%20scene%20representation%20vectors%20because%0Athey%20evenly%20consider%20free%20and%20occupied%20spaces.%20Both%20of%20them%20can%20destroy%20the%0Aperformance%20of%20downstream%20RL%20tasks.%20To%20address%20the%20above%20challenges%2C%20we%20propose%0Aa%20novel%20framework%20that%20adopts%20the%20efficient%203D%20Gaussian%20Splatting%20%283DGS%29%20to%0Alearn%203D%20scene%20representation%20for%20the%20first%20time.%20In%20brief%2C%20we%20present%20the%0AQuery-based%20Generalizable%203DGS%20to%20bridge%20the%203DGS%20technique%20and%20scene%0Arepresentations%20with%20more%20geometrical%20awareness%20than%20those%20in%20NeRFs.%20Moreover%2C%0Awe%20present%20the%20Hierarchical%20Semantics%20Encoding%20to%20ground%20the%20fine-grained%0Asemantic%20features%20to%203D%20Gaussians%20and%20further%20distilled%20to%20the%20scene%0Arepresentation%20vectors.%20We%20conduct%20extensive%20experiments%20on%20two%20RL%20platforms%0Aincluding%20Maniskill2%20and%20Robomimic%20across%2010%20different%20tasks.%20The%20results%20show%0Athat%20our%20method%20outperforms%20the%20other%205%20baselines%20by%20a%20large%20margin.%20We%20achieve%0Athe%20best%20success%20rates%20on%208%20tasks%20and%20the%20second-best%20on%20the%20other%20two%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuery-based%2520Semantic%2520Gaussian%2520Field%2520for%2520Scene%2520Representation%2520in%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DJiaxu%2520Wang%2520and%2520Ziyi%2520Zhang%2520and%2520Qiang%2520Zhang%2520and%2520Jia%2520Li%2520and%2520Jingkai%2520Sun%2520and%2520Mingyuan%2520Sun%2520and%2520Junhao%2520He%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Latent%2520scene%2520representation%2520plays%2520a%2520significant%2520role%2520in%2520training%250Areinforcement%2520learning%2520%2528RL%2529%2520agents.%2520To%2520obtain%2520good%2520latent%2520vectors%2520describing%250Athe%2520scenes%252C%2520recent%2520works%2520incorporate%2520the%25203D-aware%2520latent-conditioned%2520NeRF%250Apipeline%2520into%2520scene%2520representation%2520learning.%2520However%252C%2520these%2520NeRF-related%250Amethods%2520struggle%2520to%2520perceive%25203D%2520structural%2520information%2520due%2520to%2520the%2520inefficient%250Adense%2520sampling%2520in%2520volumetric%2520rendering.%2520Moreover%252C%2520they%2520lack%2520fine-grained%250Asemantic%2520information%2520included%2520in%2520their%2520scene%2520representation%2520vectors%2520because%250Athey%2520evenly%2520consider%2520free%2520and%2520occupied%2520spaces.%2520Both%2520of%2520them%2520can%2520destroy%2520the%250Aperformance%2520of%2520downstream%2520RL%2520tasks.%2520To%2520address%2520the%2520above%2520challenges%252C%2520we%2520propose%250Aa%2520novel%2520framework%2520that%2520adopts%2520the%2520efficient%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520to%250Alearn%25203D%2520scene%2520representation%2520for%2520the%2520first%2520time.%2520In%2520brief%252C%2520we%2520present%2520the%250AQuery-based%2520Generalizable%25203DGS%2520to%2520bridge%2520the%25203DGS%2520technique%2520and%2520scene%250Arepresentations%2520with%2520more%2520geometrical%2520awareness%2520than%2520those%2520in%2520NeRFs.%2520Moreover%252C%250Awe%2520present%2520the%2520Hierarchical%2520Semantics%2520Encoding%2520to%2520ground%2520the%2520fine-grained%250Asemantic%2520features%2520to%25203D%2520Gaussians%2520and%2520further%2520distilled%2520to%2520the%2520scene%250Arepresentation%2520vectors.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520two%2520RL%2520platforms%250Aincluding%2520Maniskill2%2520and%2520Robomimic%2520across%252010%2520different%2520tasks.%2520The%2520results%2520show%250Athat%2520our%2520method%2520outperforms%2520the%2520other%25205%2520baselines%2520by%2520a%2520large%2520margin.%2520We%2520achieve%250Athe%2520best%2520success%2520rates%2520on%25208%2520tasks%2520and%2520the%2520second-best%2520on%2520the%2520other%2520two%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Query-based%20Semantic%20Gaussian%20Field%20for%20Scene%20Representation%20in%0A%20%20Reinforcement%20Learning&entry.906535625=Jiaxu%20Wang%20and%20Ziyi%20Zhang%20and%20Qiang%20Zhang%20and%20Jia%20Li%20and%20Jingkai%20Sun%20and%20Mingyuan%20Sun%20and%20Junhao%20He%20and%20Renjing%20Xu&entry.1292438233=%20%20Latent%20scene%20representation%20plays%20a%20significant%20role%20in%20training%0Areinforcement%20learning%20%28RL%29%20agents.%20To%20obtain%20good%20latent%20vectors%20describing%0Athe%20scenes%2C%20recent%20works%20incorporate%20the%203D-aware%20latent-conditioned%20NeRF%0Apipeline%20into%20scene%20representation%20learning.%20However%2C%20these%20NeRF-related%0Amethods%20struggle%20to%20perceive%203D%20structural%20information%20due%20to%20the%20inefficient%0Adense%20sampling%20in%20volumetric%20rendering.%20Moreover%2C%20they%20lack%20fine-grained%0Asemantic%20information%20included%20in%20their%20scene%20representation%20vectors%20because%0Athey%20evenly%20consider%20free%20and%20occupied%20spaces.%20Both%20of%20them%20can%20destroy%20the%0Aperformance%20of%20downstream%20RL%20tasks.%20To%20address%20the%20above%20challenges%2C%20we%20propose%0Aa%20novel%20framework%20that%20adopts%20the%20efficient%203D%20Gaussian%20Splatting%20%283DGS%29%20to%0Alearn%203D%20scene%20representation%20for%20the%20first%20time.%20In%20brief%2C%20we%20present%20the%0AQuery-based%20Generalizable%203DGS%20to%20bridge%20the%203DGS%20technique%20and%20scene%0Arepresentations%20with%20more%20geometrical%20awareness%20than%20those%20in%20NeRFs.%20Moreover%2C%0Awe%20present%20the%20Hierarchical%20Semantics%20Encoding%20to%20ground%20the%20fine-grained%0Asemantic%20features%20to%203D%20Gaussians%20and%20further%20distilled%20to%20the%20scene%0Arepresentation%20vectors.%20We%20conduct%20extensive%20experiments%20on%20two%20RL%20platforms%0Aincluding%20Maniskill2%20and%20Robomimic%20across%2010%20different%20tasks.%20The%20results%20show%0Athat%20our%20method%20outperforms%20the%20other%205%20baselines%20by%20a%20large%20margin.%20We%20achieve%0Athe%20best%20success%20rates%20on%208%20tasks%20and%20the%20second-best%20on%20the%20other%20two%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02370v1&entry.124074799=Read"},
{"title": "GraVITON: Graph based garment warping with attention guided inversion\n  for Virtual-tryon", "author": "Sanhita Pathak and Vinay Kaushik and Brejesh Lall", "abstract": "  Virtual try-on, a rapidly evolving field in computer vision, is transforming\ne-commerce by improving customer experiences through precise garment warping\nand seamless integration onto the human body. While existing methods such as\nTPS and flow address the garment warping but overlook the finer contextual\ndetails. In this paper, we introduce a novel graph based warping technique\nwhich emphasizes the value of context in garment flow. Our graph based warping\nmodule generates warped garment as well as a coarse person image, which is\nutilised by a simple refinement network to give a coarse virtual tryon image.\nThe proposed work exploits latent diffusion model to generate the final tryon,\ntreating garment transfer as an inpainting task. The diffusion model is\nconditioned with decoupled cross attention based inversion of visual and\ntextual information. We introduce an occlusion aware warping constraint that\ngenerates dense warped garment, without any holes and occlusion. Our method,\nvalidated on VITON-HD and Dresscode datasets, showcases substantial\nstate-of-the-art qualitative and quantitative results showing considerable\nimprovement in garment warping, texture preservation, and overall realism.\n", "link": "http://arxiv.org/abs/2406.02184v1", "date": "2024-06-04", "relevancy": 2.4269, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6259}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6137}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraVITON%3A%20Graph%20based%20garment%20warping%20with%20attention%20guided%20inversion%0A%20%20for%20Virtual-tryon&body=Title%3A%20GraVITON%3A%20Graph%20based%20garment%20warping%20with%20attention%20guided%20inversion%0A%20%20for%20Virtual-tryon%0AAuthor%3A%20Sanhita%20Pathak%20and%20Vinay%20Kaushik%20and%20Brejesh%20Lall%0AAbstract%3A%20%20%20Virtual%20try-on%2C%20a%20rapidly%20evolving%20field%20in%20computer%20vision%2C%20is%20transforming%0Ae-commerce%20by%20improving%20customer%20experiences%20through%20precise%20garment%20warping%0Aand%20seamless%20integration%20onto%20the%20human%20body.%20While%20existing%20methods%20such%20as%0ATPS%20and%20flow%20address%20the%20garment%20warping%20but%20overlook%20the%20finer%20contextual%0Adetails.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20graph%20based%20warping%20technique%0Awhich%20emphasizes%20the%20value%20of%20context%20in%20garment%20flow.%20Our%20graph%20based%20warping%0Amodule%20generates%20warped%20garment%20as%20well%20as%20a%20coarse%20person%20image%2C%20which%20is%0Autilised%20by%20a%20simple%20refinement%20network%20to%20give%20a%20coarse%20virtual%20tryon%20image.%0AThe%20proposed%20work%20exploits%20latent%20diffusion%20model%20to%20generate%20the%20final%20tryon%2C%0Atreating%20garment%20transfer%20as%20an%20inpainting%20task.%20The%20diffusion%20model%20is%0Aconditioned%20with%20decoupled%20cross%20attention%20based%20inversion%20of%20visual%20and%0Atextual%20information.%20We%20introduce%20an%20occlusion%20aware%20warping%20constraint%20that%0Agenerates%20dense%20warped%20garment%2C%20without%20any%20holes%20and%20occlusion.%20Our%20method%2C%0Avalidated%20on%20VITON-HD%20and%20Dresscode%20datasets%2C%20showcases%20substantial%0Astate-of-the-art%20qualitative%20and%20quantitative%20results%20showing%20considerable%0Aimprovement%20in%20garment%20warping%2C%20texture%20preservation%2C%20and%20overall%20realism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraVITON%253A%2520Graph%2520based%2520garment%2520warping%2520with%2520attention%2520guided%2520inversion%250A%2520%2520for%2520Virtual-tryon%26entry.906535625%3DSanhita%2520Pathak%2520and%2520Vinay%2520Kaushik%2520and%2520Brejesh%2520Lall%26entry.1292438233%3D%2520%2520Virtual%2520try-on%252C%2520a%2520rapidly%2520evolving%2520field%2520in%2520computer%2520vision%252C%2520is%2520transforming%250Ae-commerce%2520by%2520improving%2520customer%2520experiences%2520through%2520precise%2520garment%2520warping%250Aand%2520seamless%2520integration%2520onto%2520the%2520human%2520body.%2520While%2520existing%2520methods%2520such%2520as%250ATPS%2520and%2520flow%2520address%2520the%2520garment%2520warping%2520but%2520overlook%2520the%2520finer%2520contextual%250Adetails.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520graph%2520based%2520warping%2520technique%250Awhich%2520emphasizes%2520the%2520value%2520of%2520context%2520in%2520garment%2520flow.%2520Our%2520graph%2520based%2520warping%250Amodule%2520generates%2520warped%2520garment%2520as%2520well%2520as%2520a%2520coarse%2520person%2520image%252C%2520which%2520is%250Autilised%2520by%2520a%2520simple%2520refinement%2520network%2520to%2520give%2520a%2520coarse%2520virtual%2520tryon%2520image.%250AThe%2520proposed%2520work%2520exploits%2520latent%2520diffusion%2520model%2520to%2520generate%2520the%2520final%2520tryon%252C%250Atreating%2520garment%2520transfer%2520as%2520an%2520inpainting%2520task.%2520The%2520diffusion%2520model%2520is%250Aconditioned%2520with%2520decoupled%2520cross%2520attention%2520based%2520inversion%2520of%2520visual%2520and%250Atextual%2520information.%2520We%2520introduce%2520an%2520occlusion%2520aware%2520warping%2520constraint%2520that%250Agenerates%2520dense%2520warped%2520garment%252C%2520without%2520any%2520holes%2520and%2520occlusion.%2520Our%2520method%252C%250Avalidated%2520on%2520VITON-HD%2520and%2520Dresscode%2520datasets%252C%2520showcases%2520substantial%250Astate-of-the-art%2520qualitative%2520and%2520quantitative%2520results%2520showing%2520considerable%250Aimprovement%2520in%2520garment%2520warping%252C%2520texture%2520preservation%252C%2520and%2520overall%2520realism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraVITON%3A%20Graph%20based%20garment%20warping%20with%20attention%20guided%20inversion%0A%20%20for%20Virtual-tryon&entry.906535625=Sanhita%20Pathak%20and%20Vinay%20Kaushik%20and%20Brejesh%20Lall&entry.1292438233=%20%20Virtual%20try-on%2C%20a%20rapidly%20evolving%20field%20in%20computer%20vision%2C%20is%20transforming%0Ae-commerce%20by%20improving%20customer%20experiences%20through%20precise%20garment%20warping%0Aand%20seamless%20integration%20onto%20the%20human%20body.%20While%20existing%20methods%20such%20as%0ATPS%20and%20flow%20address%20the%20garment%20warping%20but%20overlook%20the%20finer%20contextual%0Adetails.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20graph%20based%20warping%20technique%0Awhich%20emphasizes%20the%20value%20of%20context%20in%20garment%20flow.%20Our%20graph%20based%20warping%0Amodule%20generates%20warped%20garment%20as%20well%20as%20a%20coarse%20person%20image%2C%20which%20is%0Autilised%20by%20a%20simple%20refinement%20network%20to%20give%20a%20coarse%20virtual%20tryon%20image.%0AThe%20proposed%20work%20exploits%20latent%20diffusion%20model%20to%20generate%20the%20final%20tryon%2C%0Atreating%20garment%20transfer%20as%20an%20inpainting%20task.%20The%20diffusion%20model%20is%0Aconditioned%20with%20decoupled%20cross%20attention%20based%20inversion%20of%20visual%20and%0Atextual%20information.%20We%20introduce%20an%20occlusion%20aware%20warping%20constraint%20that%0Agenerates%20dense%20warped%20garment%2C%20without%20any%20holes%20and%20occlusion.%20Our%20method%2C%0Avalidated%20on%20VITON-HD%20and%20Dresscode%20datasets%2C%20showcases%20substantial%0Astate-of-the-art%20qualitative%20and%20quantitative%20results%20showing%20considerable%0Aimprovement%20in%20garment%20warping%2C%20texture%20preservation%2C%20and%20overall%20realism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02184v1&entry.124074799=Read"},
{"title": "Can CLIP help CLIP in learning 3D?", "author": "Cristian Sbrolli and Matteo Matteucci", "abstract": "  In this study, we explore an alternative approach to enhance contrastive\ntext-image-3D alignment in the absence of textual descriptions for 3D objects.\nWe introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP\nknowledge about textual and 2D data to compute the neural perceived similarity\nbetween two 3D samples. We employ the proposed methods to mine 3D hard\nnegatives, establishing a multimodal contrastive pipeline with hard negative\nweighting via a custom loss function. We train on different configurations of\nthe proposed hard negative mining approach, and we evaluate the accuracy of our\nmodels in 3D classification and on the cross-modal retrieval benchmark, testing\nimage-to-shape and shape-to-image retrieval. Results demonstrate that our\napproach, even without explicit text alignment, achieves comparable or superior\nperformance on zero-shot and standard 3D classification, while significantly\nimproving both image-to-shape and shape-to-image retrieval compared to previous\nmethods.\n", "link": "http://arxiv.org/abs/2406.02202v1", "date": "2024-06-04", "relevancy": 2.421, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6627}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5786}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20CLIP%20help%20CLIP%20in%20learning%203D%3F&body=Title%3A%20Can%20CLIP%20help%20CLIP%20in%20learning%203D%3F%0AAuthor%3A%20Cristian%20Sbrolli%20and%20Matteo%20Matteucci%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20explore%20an%20alternative%20approach%20to%20enhance%20contrastive%0Atext-image-3D%20alignment%20in%20the%20absence%20of%20textual%20descriptions%20for%203D%20objects.%0AWe%20introduce%20two%20unsupervised%20methods%2C%20%24I2I%24%20and%20%24%28I2L%29%5E2%24%2C%20which%20leverage%20CLIP%0Aknowledge%20about%20textual%20and%202D%20data%20to%20compute%20the%20neural%20perceived%20similarity%0Abetween%20two%203D%20samples.%20We%20employ%20the%20proposed%20methods%20to%20mine%203D%20hard%0Anegatives%2C%20establishing%20a%20multimodal%20contrastive%20pipeline%20with%20hard%20negative%0Aweighting%20via%20a%20custom%20loss%20function.%20We%20train%20on%20different%20configurations%20of%0Athe%20proposed%20hard%20negative%20mining%20approach%2C%20and%20we%20evaluate%20the%20accuracy%20of%20our%0Amodels%20in%203D%20classification%20and%20on%20the%20cross-modal%20retrieval%20benchmark%2C%20testing%0Aimage-to-shape%20and%20shape-to-image%20retrieval.%20Results%20demonstrate%20that%20our%0Aapproach%2C%20even%20without%20explicit%20text%20alignment%2C%20achieves%20comparable%20or%20superior%0Aperformance%20on%20zero-shot%20and%20standard%203D%20classification%2C%20while%20significantly%0Aimproving%20both%20image-to-shape%20and%20shape-to-image%20retrieval%20compared%20to%20previous%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520CLIP%2520help%2520CLIP%2520in%2520learning%25203D%253F%26entry.906535625%3DCristian%2520Sbrolli%2520and%2520Matteo%2520Matteucci%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520explore%2520an%2520alternative%2520approach%2520to%2520enhance%2520contrastive%250Atext-image-3D%2520alignment%2520in%2520the%2520absence%2520of%2520textual%2520descriptions%2520for%25203D%2520objects.%250AWe%2520introduce%2520two%2520unsupervised%2520methods%252C%2520%2524I2I%2524%2520and%2520%2524%2528I2L%2529%255E2%2524%252C%2520which%2520leverage%2520CLIP%250Aknowledge%2520about%2520textual%2520and%25202D%2520data%2520to%2520compute%2520the%2520neural%2520perceived%2520similarity%250Abetween%2520two%25203D%2520samples.%2520We%2520employ%2520the%2520proposed%2520methods%2520to%2520mine%25203D%2520hard%250Anegatives%252C%2520establishing%2520a%2520multimodal%2520contrastive%2520pipeline%2520with%2520hard%2520negative%250Aweighting%2520via%2520a%2520custom%2520loss%2520function.%2520We%2520train%2520on%2520different%2520configurations%2520of%250Athe%2520proposed%2520hard%2520negative%2520mining%2520approach%252C%2520and%2520we%2520evaluate%2520the%2520accuracy%2520of%2520our%250Amodels%2520in%25203D%2520classification%2520and%2520on%2520the%2520cross-modal%2520retrieval%2520benchmark%252C%2520testing%250Aimage-to-shape%2520and%2520shape-to-image%2520retrieval.%2520Results%2520demonstrate%2520that%2520our%250Aapproach%252C%2520even%2520without%2520explicit%2520text%2520alignment%252C%2520achieves%2520comparable%2520or%2520superior%250Aperformance%2520on%2520zero-shot%2520and%2520standard%25203D%2520classification%252C%2520while%2520significantly%250Aimproving%2520both%2520image-to-shape%2520and%2520shape-to-image%2520retrieval%2520compared%2520to%2520previous%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20CLIP%20help%20CLIP%20in%20learning%203D%3F&entry.906535625=Cristian%20Sbrolli%20and%20Matteo%20Matteucci&entry.1292438233=%20%20In%20this%20study%2C%20we%20explore%20an%20alternative%20approach%20to%20enhance%20contrastive%0Atext-image-3D%20alignment%20in%20the%20absence%20of%20textual%20descriptions%20for%203D%20objects.%0AWe%20introduce%20two%20unsupervised%20methods%2C%20%24I2I%24%20and%20%24%28I2L%29%5E2%24%2C%20which%20leverage%20CLIP%0Aknowledge%20about%20textual%20and%202D%20data%20to%20compute%20the%20neural%20perceived%20similarity%0Abetween%20two%203D%20samples.%20We%20employ%20the%20proposed%20methods%20to%20mine%203D%20hard%0Anegatives%2C%20establishing%20a%20multimodal%20contrastive%20pipeline%20with%20hard%20negative%0Aweighting%20via%20a%20custom%20loss%20function.%20We%20train%20on%20different%20configurations%20of%0Athe%20proposed%20hard%20negative%20mining%20approach%2C%20and%20we%20evaluate%20the%20accuracy%20of%20our%0Amodels%20in%203D%20classification%20and%20on%20the%20cross-modal%20retrieval%20benchmark%2C%20testing%0Aimage-to-shape%20and%20shape-to-image%20retrieval.%20Results%20demonstrate%20that%20our%0Aapproach%2C%20even%20without%20explicit%20text%20alignment%2C%20achieves%20comparable%20or%20superior%0Aperformance%20on%20zero-shot%20and%20standard%203D%20classification%2C%20while%20significantly%0Aimproving%20both%20image-to-shape%20and%20shape-to-image%20retrieval%20compared%20to%20previous%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02202v1&entry.124074799=Read"},
{"title": "Effects of Exponential Gaussian Distribution on (Double Sampling)\n  Randomized Smoothing", "author": "Youwei Shu and Xi Xiao and Derui Wang and Yuxin Cao and Siji Chen and Jason Xue and Linyi Li and Bo Li", "abstract": "  Randomized Smoothing (RS) is currently a scalable certified defense method\nproviding robustness certification against adversarial examples. Although\nsignificant progress has been achieved in providing defenses against $\\ell_p$\nadversaries, the interaction between the smoothing distribution and the\nrobustness certification still remains vague. In this work, we comprehensively\nstudy the effect of two families of distributions, named Exponential Standard\nGaussian (ESG) and Exponential General Gaussian (EGG) distributions, on\nRandomized Smoothing and Double Sampling Randomized Smoothing (DSRS). We derive\nan analytic formula for ESG's certified radius, which converges to the origin\nformula of RS as the dimension $d$ increases. Additionally, we prove that EGG\ncan provide tighter constant factors than DSRS in providing $\\Omega(\\sqrt{d})$\nlower bounds of $\\ell_2$ certified radius, and thus further addresses the curse\nof dimensionality in RS. Our experiments on real-world datasets confirm our\ntheoretical analysis of the ESG distributions, that they provide almost the\nsame certification under different exponents $\\eta$ for both RS and DSRS. In\naddition, EGG\n", "link": "http://arxiv.org/abs/2406.02309v1", "date": "2024-06-04", "relevancy": 2.4204, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5009}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4986}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effects%20of%20Exponential%20Gaussian%20Distribution%20on%20%28Double%20Sampling%29%0A%20%20Randomized%20Smoothing&body=Title%3A%20Effects%20of%20Exponential%20Gaussian%20Distribution%20on%20%28Double%20Sampling%29%0A%20%20Randomized%20Smoothing%0AAuthor%3A%20Youwei%20Shu%20and%20Xi%20Xiao%20and%20Derui%20Wang%20and%20Yuxin%20Cao%20and%20Siji%20Chen%20and%20Jason%20Xue%20and%20Linyi%20Li%20and%20Bo%20Li%0AAbstract%3A%20%20%20Randomized%20Smoothing%20%28RS%29%20is%20currently%20a%20scalable%20certified%20defense%20method%0Aproviding%20robustness%20certification%20against%20adversarial%20examples.%20Although%0Asignificant%20progress%20has%20been%20achieved%20in%20providing%20defenses%20against%20%24%5Cell_p%24%0Aadversaries%2C%20the%20interaction%20between%20the%20smoothing%20distribution%20and%20the%0Arobustness%20certification%20still%20remains%20vague.%20In%20this%20work%2C%20we%20comprehensively%0Astudy%20the%20effect%20of%20two%20families%20of%20distributions%2C%20named%20Exponential%20Standard%0AGaussian%20%28ESG%29%20and%20Exponential%20General%20Gaussian%20%28EGG%29%20distributions%2C%20on%0ARandomized%20Smoothing%20and%20Double%20Sampling%20Randomized%20Smoothing%20%28DSRS%29.%20We%20derive%0Aan%20analytic%20formula%20for%20ESG%27s%20certified%20radius%2C%20which%20converges%20to%20the%20origin%0Aformula%20of%20RS%20as%20the%20dimension%20%24d%24%20increases.%20Additionally%2C%20we%20prove%20that%20EGG%0Acan%20provide%20tighter%20constant%20factors%20than%20DSRS%20in%20providing%20%24%5COmega%28%5Csqrt%7Bd%7D%29%24%0Alower%20bounds%20of%20%24%5Cell_2%24%20certified%20radius%2C%20and%20thus%20further%20addresses%20the%20curse%0Aof%20dimensionality%20in%20RS.%20Our%20experiments%20on%20real-world%20datasets%20confirm%20our%0Atheoretical%20analysis%20of%20the%20ESG%20distributions%2C%20that%20they%20provide%20almost%20the%0Asame%20certification%20under%20different%20exponents%20%24%5Ceta%24%20for%20both%20RS%20and%20DSRS.%20In%0Aaddition%2C%20EGG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffects%2520of%2520Exponential%2520Gaussian%2520Distribution%2520on%2520%2528Double%2520Sampling%2529%250A%2520%2520Randomized%2520Smoothing%26entry.906535625%3DYouwei%2520Shu%2520and%2520Xi%2520Xiao%2520and%2520Derui%2520Wang%2520and%2520Yuxin%2520Cao%2520and%2520Siji%2520Chen%2520and%2520Jason%2520Xue%2520and%2520Linyi%2520Li%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520Randomized%2520Smoothing%2520%2528RS%2529%2520is%2520currently%2520a%2520scalable%2520certified%2520defense%2520method%250Aproviding%2520robustness%2520certification%2520against%2520adversarial%2520examples.%2520Although%250Asignificant%2520progress%2520has%2520been%2520achieved%2520in%2520providing%2520defenses%2520against%2520%2524%255Cell_p%2524%250Aadversaries%252C%2520the%2520interaction%2520between%2520the%2520smoothing%2520distribution%2520and%2520the%250Arobustness%2520certification%2520still%2520remains%2520vague.%2520In%2520this%2520work%252C%2520we%2520comprehensively%250Astudy%2520the%2520effect%2520of%2520two%2520families%2520of%2520distributions%252C%2520named%2520Exponential%2520Standard%250AGaussian%2520%2528ESG%2529%2520and%2520Exponential%2520General%2520Gaussian%2520%2528EGG%2529%2520distributions%252C%2520on%250ARandomized%2520Smoothing%2520and%2520Double%2520Sampling%2520Randomized%2520Smoothing%2520%2528DSRS%2529.%2520We%2520derive%250Aan%2520analytic%2520formula%2520for%2520ESG%2527s%2520certified%2520radius%252C%2520which%2520converges%2520to%2520the%2520origin%250Aformula%2520of%2520RS%2520as%2520the%2520dimension%2520%2524d%2524%2520increases.%2520Additionally%252C%2520we%2520prove%2520that%2520EGG%250Acan%2520provide%2520tighter%2520constant%2520factors%2520than%2520DSRS%2520in%2520providing%2520%2524%255COmega%2528%255Csqrt%257Bd%257D%2529%2524%250Alower%2520bounds%2520of%2520%2524%255Cell_2%2524%2520certified%2520radius%252C%2520and%2520thus%2520further%2520addresses%2520the%2520curse%250Aof%2520dimensionality%2520in%2520RS.%2520Our%2520experiments%2520on%2520real-world%2520datasets%2520confirm%2520our%250Atheoretical%2520analysis%2520of%2520the%2520ESG%2520distributions%252C%2520that%2520they%2520provide%2520almost%2520the%250Asame%2520certification%2520under%2520different%2520exponents%2520%2524%255Ceta%2524%2520for%2520both%2520RS%2520and%2520DSRS.%2520In%250Aaddition%252C%2520EGG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effects%20of%20Exponential%20Gaussian%20Distribution%20on%20%28Double%20Sampling%29%0A%20%20Randomized%20Smoothing&entry.906535625=Youwei%20Shu%20and%20Xi%20Xiao%20and%20Derui%20Wang%20and%20Yuxin%20Cao%20and%20Siji%20Chen%20and%20Jason%20Xue%20and%20Linyi%20Li%20and%20Bo%20Li&entry.1292438233=%20%20Randomized%20Smoothing%20%28RS%29%20is%20currently%20a%20scalable%20certified%20defense%20method%0Aproviding%20robustness%20certification%20against%20adversarial%20examples.%20Although%0Asignificant%20progress%20has%20been%20achieved%20in%20providing%20defenses%20against%20%24%5Cell_p%24%0Aadversaries%2C%20the%20interaction%20between%20the%20smoothing%20distribution%20and%20the%0Arobustness%20certification%20still%20remains%20vague.%20In%20this%20work%2C%20we%20comprehensively%0Astudy%20the%20effect%20of%20two%20families%20of%20distributions%2C%20named%20Exponential%20Standard%0AGaussian%20%28ESG%29%20and%20Exponential%20General%20Gaussian%20%28EGG%29%20distributions%2C%20on%0ARandomized%20Smoothing%20and%20Double%20Sampling%20Randomized%20Smoothing%20%28DSRS%29.%20We%20derive%0Aan%20analytic%20formula%20for%20ESG%27s%20certified%20radius%2C%20which%20converges%20to%20the%20origin%0Aformula%20of%20RS%20as%20the%20dimension%20%24d%24%20increases.%20Additionally%2C%20we%20prove%20that%20EGG%0Acan%20provide%20tighter%20constant%20factors%20than%20DSRS%20in%20providing%20%24%5COmega%28%5Csqrt%7Bd%7D%29%24%0Alower%20bounds%20of%20%24%5Cell_2%24%20certified%20radius%2C%20and%20thus%20further%20addresses%20the%20curse%0Aof%20dimensionality%20in%20RS.%20Our%20experiments%20on%20real-world%20datasets%20confirm%20our%0Atheoretical%20analysis%20of%20the%20ESG%20distributions%2C%20that%20they%20provide%20almost%20the%0Asame%20certification%20under%20different%20exponents%20%24%5Ceta%24%20for%20both%20RS%20and%20DSRS.%20In%0Aaddition%2C%20EGG%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02309v1&entry.124074799=Read"},
{"title": "Loki: Low-Rank Keys for Efficient Sparse Attention", "author": "Prajwal Singhania and Siddharth Singh and Shwai He and Soheil Feizi and Abhinav Bhatele", "abstract": "  Inference on large language models can be expensive in terms of the compute\nand memory costs involved, especially when long sequence lengths are used. In\nparticular, the self-attention mechanism used in such models contributes\nsignificantly to these costs, which has resulted in several recent works that\npropose sparse attention approximations for inference. In this work, we propose\nto approximate the self-attention computation by focusing on the dimensionality\nof key vectors computed in the attention block. Our analysis reveals that the\nkey vectors lie in a significantly lower-dimensional space, consistently across\nseveral datasets and models. Exploiting this observation, we propose Loki, a\nnovel sparse attention method that ranks and selects tokens in the KV-cache\nbased on attention scores computed in low-dimensional space. Our evaluations\nshow that Loki is able to maintain the efficacy of the models better than other\npopular approximation methods, while speeding up the attention computation due\nto reduced data movement (load/store) and compute costs.\n", "link": "http://arxiv.org/abs/2406.02542v1", "date": "2024-06-04", "relevancy": 2.4171, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5714}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4488}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loki%3A%20Low-Rank%20Keys%20for%20Efficient%20Sparse%20Attention&body=Title%3A%20Loki%3A%20Low-Rank%20Keys%20for%20Efficient%20Sparse%20Attention%0AAuthor%3A%20Prajwal%20Singhania%20and%20Siddharth%20Singh%20and%20Shwai%20He%20and%20Soheil%20Feizi%20and%20Abhinav%20Bhatele%0AAbstract%3A%20%20%20Inference%20on%20large%20language%20models%20can%20be%20expensive%20in%20terms%20of%20the%20compute%0Aand%20memory%20costs%20involved%2C%20especially%20when%20long%20sequence%20lengths%20are%20used.%20In%0Aparticular%2C%20the%20self-attention%20mechanism%20used%20in%20such%20models%20contributes%0Asignificantly%20to%20these%20costs%2C%20which%20has%20resulted%20in%20several%20recent%20works%20that%0Apropose%20sparse%20attention%20approximations%20for%20inference.%20In%20this%20work%2C%20we%20propose%0Ato%20approximate%20the%20self-attention%20computation%20by%20focusing%20on%20the%20dimensionality%0Aof%20key%20vectors%20computed%20in%20the%20attention%20block.%20Our%20analysis%20reveals%20that%20the%0Akey%20vectors%20lie%20in%20a%20significantly%20lower-dimensional%20space%2C%20consistently%20across%0Aseveral%20datasets%20and%20models.%20Exploiting%20this%20observation%2C%20we%20propose%20Loki%2C%20a%0Anovel%20sparse%20attention%20method%20that%20ranks%20and%20selects%20tokens%20in%20the%20KV-cache%0Abased%20on%20attention%20scores%20computed%20in%20low-dimensional%20space.%20Our%20evaluations%0Ashow%20that%20Loki%20is%20able%20to%20maintain%20the%20efficacy%20of%20the%20models%20better%20than%20other%0Apopular%20approximation%20methods%2C%20while%20speeding%20up%20the%20attention%20computation%20due%0Ato%20reduced%20data%20movement%20%28load/store%29%20and%20compute%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoki%253A%2520Low-Rank%2520Keys%2520for%2520Efficient%2520Sparse%2520Attention%26entry.906535625%3DPrajwal%2520Singhania%2520and%2520Siddharth%2520Singh%2520and%2520Shwai%2520He%2520and%2520Soheil%2520Feizi%2520and%2520Abhinav%2520Bhatele%26entry.1292438233%3D%2520%2520Inference%2520on%2520large%2520language%2520models%2520can%2520be%2520expensive%2520in%2520terms%2520of%2520the%2520compute%250Aand%2520memory%2520costs%2520involved%252C%2520especially%2520when%2520long%2520sequence%2520lengths%2520are%2520used.%2520In%250Aparticular%252C%2520the%2520self-attention%2520mechanism%2520used%2520in%2520such%2520models%2520contributes%250Asignificantly%2520to%2520these%2520costs%252C%2520which%2520has%2520resulted%2520in%2520several%2520recent%2520works%2520that%250Apropose%2520sparse%2520attention%2520approximations%2520for%2520inference.%2520In%2520this%2520work%252C%2520we%2520propose%250Ato%2520approximate%2520the%2520self-attention%2520computation%2520by%2520focusing%2520on%2520the%2520dimensionality%250Aof%2520key%2520vectors%2520computed%2520in%2520the%2520attention%2520block.%2520Our%2520analysis%2520reveals%2520that%2520the%250Akey%2520vectors%2520lie%2520in%2520a%2520significantly%2520lower-dimensional%2520space%252C%2520consistently%2520across%250Aseveral%2520datasets%2520and%2520models.%2520Exploiting%2520this%2520observation%252C%2520we%2520propose%2520Loki%252C%2520a%250Anovel%2520sparse%2520attention%2520method%2520that%2520ranks%2520and%2520selects%2520tokens%2520in%2520the%2520KV-cache%250Abased%2520on%2520attention%2520scores%2520computed%2520in%2520low-dimensional%2520space.%2520Our%2520evaluations%250Ashow%2520that%2520Loki%2520is%2520able%2520to%2520maintain%2520the%2520efficacy%2520of%2520the%2520models%2520better%2520than%2520other%250Apopular%2520approximation%2520methods%252C%2520while%2520speeding%2520up%2520the%2520attention%2520computation%2520due%250Ato%2520reduced%2520data%2520movement%2520%2528load/store%2529%2520and%2520compute%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loki%3A%20Low-Rank%20Keys%20for%20Efficient%20Sparse%20Attention&entry.906535625=Prajwal%20Singhania%20and%20Siddharth%20Singh%20and%20Shwai%20He%20and%20Soheil%20Feizi%20and%20Abhinav%20Bhatele&entry.1292438233=%20%20Inference%20on%20large%20language%20models%20can%20be%20expensive%20in%20terms%20of%20the%20compute%0Aand%20memory%20costs%20involved%2C%20especially%20when%20long%20sequence%20lengths%20are%20used.%20In%0Aparticular%2C%20the%20self-attention%20mechanism%20used%20in%20such%20models%20contributes%0Asignificantly%20to%20these%20costs%2C%20which%20has%20resulted%20in%20several%20recent%20works%20that%0Apropose%20sparse%20attention%20approximations%20for%20inference.%20In%20this%20work%2C%20we%20propose%0Ato%20approximate%20the%20self-attention%20computation%20by%20focusing%20on%20the%20dimensionality%0Aof%20key%20vectors%20computed%20in%20the%20attention%20block.%20Our%20analysis%20reveals%20that%20the%0Akey%20vectors%20lie%20in%20a%20significantly%20lower-dimensional%20space%2C%20consistently%20across%0Aseveral%20datasets%20and%20models.%20Exploiting%20this%20observation%2C%20we%20propose%20Loki%2C%20a%0Anovel%20sparse%20attention%20method%20that%20ranks%20and%20selects%20tokens%20in%20the%20KV-cache%0Abased%20on%20attention%20scores%20computed%20in%20low-dimensional%20space.%20Our%20evaluations%0Ashow%20that%20Loki%20is%20able%20to%20maintain%20the%20efficacy%20of%20the%20models%20better%20than%20other%0Apopular%20approximation%20methods%2C%20while%20speeding%20up%20the%20attention%20computation%20due%0Ato%20reduced%20data%20movement%20%28load/store%29%20and%20compute%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02542v1&entry.124074799=Read"},
{"title": "Evaluating ChatGPT as a Recommender System: A Rigorous Approach", "author": "Dario Di Palma and Giovanni Maria Biancofiore and Vito Walter Anelli and Fedelucio Narducci and Tommaso Di Noia and Eugenio Di Sciascio", "abstract": "  Large Language Models (LLMs) have recently shown impressive abilities in\nhandling various natural language-related tasks. Among different LLMs, current\nstudies have assessed ChatGPT's superior performance across manifold tasks,\nespecially under the zero/few-shot prompting conditions. Given such successes,\nthe Recommender Systems (RSs) research community have started investigating its\npotential applications within the recommendation scenario. However, although\nvarious methods have been proposed to integrate ChatGPT's capabilities into\nRSs, current research struggles to comprehensively evaluate such models while\nconsidering the peculiarities of generative models. Often, evaluations do not\nconsider hallucinations, duplications, and out-of-the-closed domain\nrecommendations and solely focus on accuracy metrics, neglecting the impact on\nbeyond-accuracy facets. To bridge this gap, we propose a robust evaluation\npipeline to assess ChatGPT's ability as an RS and post-process ChatGPT\nrecommendations to account for these aspects. Through this pipeline, we\ninvestigate ChatGPT-3.5 and ChatGPT-4 performance in the recommendation task\nunder the zero-shot condition employing the role-playing prompt. We analyze the\nmodel's functionality in three settings: the Top-N Recommendation, the\ncold-start recommendation, and the re-ranking of a list of recommendations, and\nin three domains: movies, music, and books. The experiments reveal that ChatGPT\nexhibits higher accuracy than the baselines on books domain. It also excels in\nre-ranking and cold-start scenarios while maintaining reasonable\nbeyond-accuracy metrics. Furthermore, we measure the similarity between the\nChatGPT recommendations and the other recommenders, providing insights about\nhow ChatGPT could be categorized in the realm of recommender systems. The\nevaluation pipeline is publicly released for future research.\n", "link": "http://arxiv.org/abs/2309.03613v2", "date": "2024-06-04", "relevancy": 2.4124, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4936}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4812}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20ChatGPT%20as%20a%20Recommender%20System%3A%20A%20Rigorous%20Approach&body=Title%3A%20Evaluating%20ChatGPT%20as%20a%20Recommender%20System%3A%20A%20Rigorous%20Approach%0AAuthor%3A%20Dario%20Di%20Palma%20and%20Giovanni%20Maria%20Biancofiore%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia%20and%20Eugenio%20Di%20Sciascio%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20shown%20impressive%20abilities%20in%0Ahandling%20various%20natural%20language-related%20tasks.%20Among%20different%20LLMs%2C%20current%0Astudies%20have%20assessed%20ChatGPT%27s%20superior%20performance%20across%20manifold%20tasks%2C%0Aespecially%20under%20the%20zero/few-shot%20prompting%20conditions.%20Given%20such%20successes%2C%0Athe%20Recommender%20Systems%20%28RSs%29%20research%20community%20have%20started%20investigating%20its%0Apotential%20applications%20within%20the%20recommendation%20scenario.%20However%2C%20although%0Avarious%20methods%20have%20been%20proposed%20to%20integrate%20ChatGPT%27s%20capabilities%20into%0ARSs%2C%20current%20research%20struggles%20to%20comprehensively%20evaluate%20such%20models%20while%0Aconsidering%20the%20peculiarities%20of%20generative%20models.%20Often%2C%20evaluations%20do%20not%0Aconsider%20hallucinations%2C%20duplications%2C%20and%20out-of-the-closed%20domain%0Arecommendations%20and%20solely%20focus%20on%20accuracy%20metrics%2C%20neglecting%20the%20impact%20on%0Abeyond-accuracy%20facets.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20robust%20evaluation%0Apipeline%20to%20assess%20ChatGPT%27s%20ability%20as%20an%20RS%20and%20post-process%20ChatGPT%0Arecommendations%20to%20account%20for%20these%20aspects.%20Through%20this%20pipeline%2C%20we%0Ainvestigate%20ChatGPT-3.5%20and%20ChatGPT-4%20performance%20in%20the%20recommendation%20task%0Aunder%20the%20zero-shot%20condition%20employing%20the%20role-playing%20prompt.%20We%20analyze%20the%0Amodel%27s%20functionality%20in%20three%20settings%3A%20the%20Top-N%20Recommendation%2C%20the%0Acold-start%20recommendation%2C%20and%20the%20re-ranking%20of%20a%20list%20of%20recommendations%2C%20and%0Ain%20three%20domains%3A%20movies%2C%20music%2C%20and%20books.%20The%20experiments%20reveal%20that%20ChatGPT%0Aexhibits%20higher%20accuracy%20than%20the%20baselines%20on%20books%20domain.%20It%20also%20excels%20in%0Are-ranking%20and%20cold-start%20scenarios%20while%20maintaining%20reasonable%0Abeyond-accuracy%20metrics.%20Furthermore%2C%20we%20measure%20the%20similarity%20between%20the%0AChatGPT%20recommendations%20and%20the%20other%20recommenders%2C%20providing%20insights%20about%0Ahow%20ChatGPT%20could%20be%20categorized%20in%20the%20realm%20of%20recommender%20systems.%20The%0Aevaluation%20pipeline%20is%20publicly%20released%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520ChatGPT%2520as%2520a%2520Recommender%2520System%253A%2520A%2520Rigorous%2520Approach%26entry.906535625%3DDario%2520Di%2520Palma%2520and%2520Giovanni%2520Maria%2520Biancofiore%2520and%2520Vito%2520Walter%2520Anelli%2520and%2520Fedelucio%2520Narducci%2520and%2520Tommaso%2520Di%2520Noia%2520and%2520Eugenio%2520Di%2520Sciascio%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520shown%2520impressive%2520abilities%2520in%250Ahandling%2520various%2520natural%2520language-related%2520tasks.%2520Among%2520different%2520LLMs%252C%2520current%250Astudies%2520have%2520assessed%2520ChatGPT%2527s%2520superior%2520performance%2520across%2520manifold%2520tasks%252C%250Aespecially%2520under%2520the%2520zero/few-shot%2520prompting%2520conditions.%2520Given%2520such%2520successes%252C%250Athe%2520Recommender%2520Systems%2520%2528RSs%2529%2520research%2520community%2520have%2520started%2520investigating%2520its%250Apotential%2520applications%2520within%2520the%2520recommendation%2520scenario.%2520However%252C%2520although%250Avarious%2520methods%2520have%2520been%2520proposed%2520to%2520integrate%2520ChatGPT%2527s%2520capabilities%2520into%250ARSs%252C%2520current%2520research%2520struggles%2520to%2520comprehensively%2520evaluate%2520such%2520models%2520while%250Aconsidering%2520the%2520peculiarities%2520of%2520generative%2520models.%2520Often%252C%2520evaluations%2520do%2520not%250Aconsider%2520hallucinations%252C%2520duplications%252C%2520and%2520out-of-the-closed%2520domain%250Arecommendations%2520and%2520solely%2520focus%2520on%2520accuracy%2520metrics%252C%2520neglecting%2520the%2520impact%2520on%250Abeyond-accuracy%2520facets.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520robust%2520evaluation%250Apipeline%2520to%2520assess%2520ChatGPT%2527s%2520ability%2520as%2520an%2520RS%2520and%2520post-process%2520ChatGPT%250Arecommendations%2520to%2520account%2520for%2520these%2520aspects.%2520Through%2520this%2520pipeline%252C%2520we%250Ainvestigate%2520ChatGPT-3.5%2520and%2520ChatGPT-4%2520performance%2520in%2520the%2520recommendation%2520task%250Aunder%2520the%2520zero-shot%2520condition%2520employing%2520the%2520role-playing%2520prompt.%2520We%2520analyze%2520the%250Amodel%2527s%2520functionality%2520in%2520three%2520settings%253A%2520the%2520Top-N%2520Recommendation%252C%2520the%250Acold-start%2520recommendation%252C%2520and%2520the%2520re-ranking%2520of%2520a%2520list%2520of%2520recommendations%252C%2520and%250Ain%2520three%2520domains%253A%2520movies%252C%2520music%252C%2520and%2520books.%2520The%2520experiments%2520reveal%2520that%2520ChatGPT%250Aexhibits%2520higher%2520accuracy%2520than%2520the%2520baselines%2520on%2520books%2520domain.%2520It%2520also%2520excels%2520in%250Are-ranking%2520and%2520cold-start%2520scenarios%2520while%2520maintaining%2520reasonable%250Abeyond-accuracy%2520metrics.%2520Furthermore%252C%2520we%2520measure%2520the%2520similarity%2520between%2520the%250AChatGPT%2520recommendations%2520and%2520the%2520other%2520recommenders%252C%2520providing%2520insights%2520about%250Ahow%2520ChatGPT%2520could%2520be%2520categorized%2520in%2520the%2520realm%2520of%2520recommender%2520systems.%2520The%250Aevaluation%2520pipeline%2520is%2520publicly%2520released%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.03613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20ChatGPT%20as%20a%20Recommender%20System%3A%20A%20Rigorous%20Approach&entry.906535625=Dario%20Di%20Palma%20and%20Giovanni%20Maria%20Biancofiore%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia%20and%20Eugenio%20Di%20Sciascio&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20shown%20impressive%20abilities%20in%0Ahandling%20various%20natural%20language-related%20tasks.%20Among%20different%20LLMs%2C%20current%0Astudies%20have%20assessed%20ChatGPT%27s%20superior%20performance%20across%20manifold%20tasks%2C%0Aespecially%20under%20the%20zero/few-shot%20prompting%20conditions.%20Given%20such%20successes%2C%0Athe%20Recommender%20Systems%20%28RSs%29%20research%20community%20have%20started%20investigating%20its%0Apotential%20applications%20within%20the%20recommendation%20scenario.%20However%2C%20although%0Avarious%20methods%20have%20been%20proposed%20to%20integrate%20ChatGPT%27s%20capabilities%20into%0ARSs%2C%20current%20research%20struggles%20to%20comprehensively%20evaluate%20such%20models%20while%0Aconsidering%20the%20peculiarities%20of%20generative%20models.%20Often%2C%20evaluations%20do%20not%0Aconsider%20hallucinations%2C%20duplications%2C%20and%20out-of-the-closed%20domain%0Arecommendations%20and%20solely%20focus%20on%20accuracy%20metrics%2C%20neglecting%20the%20impact%20on%0Abeyond-accuracy%20facets.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20robust%20evaluation%0Apipeline%20to%20assess%20ChatGPT%27s%20ability%20as%20an%20RS%20and%20post-process%20ChatGPT%0Arecommendations%20to%20account%20for%20these%20aspects.%20Through%20this%20pipeline%2C%20we%0Ainvestigate%20ChatGPT-3.5%20and%20ChatGPT-4%20performance%20in%20the%20recommendation%20task%0Aunder%20the%20zero-shot%20condition%20employing%20the%20role-playing%20prompt.%20We%20analyze%20the%0Amodel%27s%20functionality%20in%20three%20settings%3A%20the%20Top-N%20Recommendation%2C%20the%0Acold-start%20recommendation%2C%20and%20the%20re-ranking%20of%20a%20list%20of%20recommendations%2C%20and%0Ain%20three%20domains%3A%20movies%2C%20music%2C%20and%20books.%20The%20experiments%20reveal%20that%20ChatGPT%0Aexhibits%20higher%20accuracy%20than%20the%20baselines%20on%20books%20domain.%20It%20also%20excels%20in%0Are-ranking%20and%20cold-start%20scenarios%20while%20maintaining%20reasonable%0Abeyond-accuracy%20metrics.%20Furthermore%2C%20we%20measure%20the%20similarity%20between%20the%0AChatGPT%20recommendations%20and%20the%20other%20recommenders%2C%20providing%20insights%20about%0Ahow%20ChatGPT%20could%20be%20categorized%20in%20the%20realm%20of%20recommender%20systems.%20The%0Aevaluation%20pipeline%20is%20publicly%20released%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03613v2&entry.124074799=Read"},
{"title": "SLTrain: a sparse plus low-rank approach for parameter and memory\n  efficient pretraining", "author": "Andi Han and Jiaxiang Li and Wei Huang and Mingyi Hong and Akiko Takeda and Pratik Jawanpuria and Bamdev Mishra", "abstract": "  Large language models (LLMs) have shown impressive capabilities across\nvarious tasks. However, training LLMs from scratch requires significant\ncomputational power and extensive memory capacity. Recent studies have explored\nlow-rank structures on weights for efficient fine-tuning in terms of parameters\nand memory, either through low-rank adaptation or factorization. While\neffective for fine-tuning, low-rank structures are generally less suitable for\npretraining because they restrict parameters to a low-dimensional subspace. In\nthis work, we propose to parameterize the weights as a sum of low-rank and\nsparse matrices for pretraining, which we call SLTrain. The low-rank component\nis learned via matrix factorization, while for the sparse component, we employ\na simple strategy of uniformly selecting the sparsity support at random and\nlearning only the non-zero entries with the fixed support. While being simple,\nthe random fixed-support sparse learning strategy significantly enhances\npretraining when combined with low-rank learning. Our results show that SLTrain\nadds minimal extra parameters and memory costs compared to pretraining with\nlow-rank parameterization, yet achieves substantially better performance, which\nis comparable to full-rank training. Remarkably, when combined with\nquantization and per-layer updates, SLTrain can reduce memory requirements by\nup to 73% when pretraining the LLaMA 7B model.\n", "link": "http://arxiv.org/abs/2406.02214v1", "date": "2024-06-04", "relevancy": 2.3864, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4841}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.479}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLTrain%3A%20a%20sparse%20plus%20low-rank%20approach%20for%20parameter%20and%20memory%0A%20%20efficient%20pretraining&body=Title%3A%20SLTrain%3A%20a%20sparse%20plus%20low-rank%20approach%20for%20parameter%20and%20memory%0A%20%20efficient%20pretraining%0AAuthor%3A%20Andi%20Han%20and%20Jiaxiang%20Li%20and%20Wei%20Huang%20and%20Mingyi%20Hong%20and%20Akiko%20Takeda%20and%20Pratik%20Jawanpuria%20and%20Bamdev%20Mishra%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20across%0Avarious%20tasks.%20However%2C%20training%20LLMs%20from%20scratch%20requires%20significant%0Acomputational%20power%20and%20extensive%20memory%20capacity.%20Recent%20studies%20have%20explored%0Alow-rank%20structures%20on%20weights%20for%20efficient%20fine-tuning%20in%20terms%20of%20parameters%0Aand%20memory%2C%20either%20through%20low-rank%20adaptation%20or%20factorization.%20While%0Aeffective%20for%20fine-tuning%2C%20low-rank%20structures%20are%20generally%20less%20suitable%20for%0Apretraining%20because%20they%20restrict%20parameters%20to%20a%20low-dimensional%20subspace.%20In%0Athis%20work%2C%20we%20propose%20to%20parameterize%20the%20weights%20as%20a%20sum%20of%20low-rank%20and%0Asparse%20matrices%20for%20pretraining%2C%20which%20we%20call%20SLTrain.%20The%20low-rank%20component%0Ais%20learned%20via%20matrix%20factorization%2C%20while%20for%20the%20sparse%20component%2C%20we%20employ%0Aa%20simple%20strategy%20of%20uniformly%20selecting%20the%20sparsity%20support%20at%20random%20and%0Alearning%20only%20the%20non-zero%20entries%20with%20the%20fixed%20support.%20While%20being%20simple%2C%0Athe%20random%20fixed-support%20sparse%20learning%20strategy%20significantly%20enhances%0Apretraining%20when%20combined%20with%20low-rank%20learning.%20Our%20results%20show%20that%20SLTrain%0Aadds%20minimal%20extra%20parameters%20and%20memory%20costs%20compared%20to%20pretraining%20with%0Alow-rank%20parameterization%2C%20yet%20achieves%20substantially%20better%20performance%2C%20which%0Ais%20comparable%20to%20full-rank%20training.%20Remarkably%2C%20when%20combined%20with%0Aquantization%20and%20per-layer%20updates%2C%20SLTrain%20can%20reduce%20memory%20requirements%20by%0Aup%20to%2073%25%20when%20pretraining%20the%20LLaMA%207B%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLTrain%253A%2520a%2520sparse%2520plus%2520low-rank%2520approach%2520for%2520parameter%2520and%2520memory%250A%2520%2520efficient%2520pretraining%26entry.906535625%3DAndi%2520Han%2520and%2520Jiaxiang%2520Li%2520and%2520Wei%2520Huang%2520and%2520Mingyi%2520Hong%2520and%2520Akiko%2520Takeda%2520and%2520Pratik%2520Jawanpuria%2520and%2520Bamdev%2520Mishra%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520capabilities%2520across%250Avarious%2520tasks.%2520However%252C%2520training%2520LLMs%2520from%2520scratch%2520requires%2520significant%250Acomputational%2520power%2520and%2520extensive%2520memory%2520capacity.%2520Recent%2520studies%2520have%2520explored%250Alow-rank%2520structures%2520on%2520weights%2520for%2520efficient%2520fine-tuning%2520in%2520terms%2520of%2520parameters%250Aand%2520memory%252C%2520either%2520through%2520low-rank%2520adaptation%2520or%2520factorization.%2520While%250Aeffective%2520for%2520fine-tuning%252C%2520low-rank%2520structures%2520are%2520generally%2520less%2520suitable%2520for%250Apretraining%2520because%2520they%2520restrict%2520parameters%2520to%2520a%2520low-dimensional%2520subspace.%2520In%250Athis%2520work%252C%2520we%2520propose%2520to%2520parameterize%2520the%2520weights%2520as%2520a%2520sum%2520of%2520low-rank%2520and%250Asparse%2520matrices%2520for%2520pretraining%252C%2520which%2520we%2520call%2520SLTrain.%2520The%2520low-rank%2520component%250Ais%2520learned%2520via%2520matrix%2520factorization%252C%2520while%2520for%2520the%2520sparse%2520component%252C%2520we%2520employ%250Aa%2520simple%2520strategy%2520of%2520uniformly%2520selecting%2520the%2520sparsity%2520support%2520at%2520random%2520and%250Alearning%2520only%2520the%2520non-zero%2520entries%2520with%2520the%2520fixed%2520support.%2520While%2520being%2520simple%252C%250Athe%2520random%2520fixed-support%2520sparse%2520learning%2520strategy%2520significantly%2520enhances%250Apretraining%2520when%2520combined%2520with%2520low-rank%2520learning.%2520Our%2520results%2520show%2520that%2520SLTrain%250Aadds%2520minimal%2520extra%2520parameters%2520and%2520memory%2520costs%2520compared%2520to%2520pretraining%2520with%250Alow-rank%2520parameterization%252C%2520yet%2520achieves%2520substantially%2520better%2520performance%252C%2520which%250Ais%2520comparable%2520to%2520full-rank%2520training.%2520Remarkably%252C%2520when%2520combined%2520with%250Aquantization%2520and%2520per-layer%2520updates%252C%2520SLTrain%2520can%2520reduce%2520memory%2520requirements%2520by%250Aup%2520to%252073%2525%2520when%2520pretraining%2520the%2520LLaMA%25207B%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLTrain%3A%20a%20sparse%20plus%20low-rank%20approach%20for%20parameter%20and%20memory%0A%20%20efficient%20pretraining&entry.906535625=Andi%20Han%20and%20Jiaxiang%20Li%20and%20Wei%20Huang%20and%20Mingyi%20Hong%20and%20Akiko%20Takeda%20and%20Pratik%20Jawanpuria%20and%20Bamdev%20Mishra&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20across%0Avarious%20tasks.%20However%2C%20training%20LLMs%20from%20scratch%20requires%20significant%0Acomputational%20power%20and%20extensive%20memory%20capacity.%20Recent%20studies%20have%20explored%0Alow-rank%20structures%20on%20weights%20for%20efficient%20fine-tuning%20in%20terms%20of%20parameters%0Aand%20memory%2C%20either%20through%20low-rank%20adaptation%20or%20factorization.%20While%0Aeffective%20for%20fine-tuning%2C%20low-rank%20structures%20are%20generally%20less%20suitable%20for%0Apretraining%20because%20they%20restrict%20parameters%20to%20a%20low-dimensional%20subspace.%20In%0Athis%20work%2C%20we%20propose%20to%20parameterize%20the%20weights%20as%20a%20sum%20of%20low-rank%20and%0Asparse%20matrices%20for%20pretraining%2C%20which%20we%20call%20SLTrain.%20The%20low-rank%20component%0Ais%20learned%20via%20matrix%20factorization%2C%20while%20for%20the%20sparse%20component%2C%20we%20employ%0Aa%20simple%20strategy%20of%20uniformly%20selecting%20the%20sparsity%20support%20at%20random%20and%0Alearning%20only%20the%20non-zero%20entries%20with%20the%20fixed%20support.%20While%20being%20simple%2C%0Athe%20random%20fixed-support%20sparse%20learning%20strategy%20significantly%20enhances%0Apretraining%20when%20combined%20with%20low-rank%20learning.%20Our%20results%20show%20that%20SLTrain%0Aadds%20minimal%20extra%20parameters%20and%20memory%20costs%20compared%20to%20pretraining%20with%0Alow-rank%20parameterization%2C%20yet%20achieves%20substantially%20better%20performance%2C%20which%0Ais%20comparable%20to%20full-rank%20training.%20Remarkably%2C%20when%20combined%20with%0Aquantization%20and%20per-layer%20updates%2C%20SLTrain%20can%20reduce%20memory%20requirements%20by%0Aup%20to%2073%25%20when%20pretraining%20the%20LLaMA%207B%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02214v1&entry.124074799=Read"},
{"title": "Towards Practical Non-Adversarial Distribution Matching", "author": "Ziyu Gong and Ben Usman and Han Zhao and David I. Inouye", "abstract": "  Distribution matching can be used to learn invariant representations with\napplications in fairness and robustness. Most prior works resort to adversarial\nmatching methods but the resulting minimax problems are unstable and\nchallenging to optimize. Non-adversarial likelihood-based approaches either\nrequire model invertibility, impose constraints on the latent prior, or lack a\ngeneric framework for distribution matching. To overcome these limitations, we\npropose a non-adversarial VAE-based matching method that can be applied to any\nmodel pipeline. We develop a set of alignment upper bounds for distribution\nmatching (including a noisy bound) that have VAE-like objectives but with a\ndifferent perspective. We carefully compare our method to prior VAE-based\nmatching approaches both theoretically and empirically. Finally, we demonstrate\nthat our novel matching losses can replace adversarial losses in standard\ninvariant representation learning pipelines without modifying the original\narchitectures -- thereby significantly broadening the applicability of\nnon-adversarial matching methods.\n", "link": "http://arxiv.org/abs/2310.19690v2", "date": "2024-06-04", "relevancy": 2.385, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4952}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4792}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Practical%20Non-Adversarial%20Distribution%20Matching&body=Title%3A%20Towards%20Practical%20Non-Adversarial%20Distribution%20Matching%0AAuthor%3A%20Ziyu%20Gong%20and%20Ben%20Usman%20and%20Han%20Zhao%20and%20David%20I.%20Inouye%0AAbstract%3A%20%20%20Distribution%20matching%20can%20be%20used%20to%20learn%20invariant%20representations%20with%0Aapplications%20in%20fairness%20and%20robustness.%20Most%20prior%20works%20resort%20to%20adversarial%0Amatching%20methods%20but%20the%20resulting%20minimax%20problems%20are%20unstable%20and%0Achallenging%20to%20optimize.%20Non-adversarial%20likelihood-based%20approaches%20either%0Arequire%20model%20invertibility%2C%20impose%20constraints%20on%20the%20latent%20prior%2C%20or%20lack%20a%0Ageneric%20framework%20for%20distribution%20matching.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20non-adversarial%20VAE-based%20matching%20method%20that%20can%20be%20applied%20to%20any%0Amodel%20pipeline.%20We%20develop%20a%20set%20of%20alignment%20upper%20bounds%20for%20distribution%0Amatching%20%28including%20a%20noisy%20bound%29%20that%20have%20VAE-like%20objectives%20but%20with%20a%0Adifferent%20perspective.%20We%20carefully%20compare%20our%20method%20to%20prior%20VAE-based%0Amatching%20approaches%20both%20theoretically%20and%20empirically.%20Finally%2C%20we%20demonstrate%0Athat%20our%20novel%20matching%20losses%20can%20replace%20adversarial%20losses%20in%20standard%0Ainvariant%20representation%20learning%20pipelines%20without%20modifying%20the%20original%0Aarchitectures%20--%20thereby%20significantly%20broadening%20the%20applicability%20of%0Anon-adversarial%20matching%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Practical%2520Non-Adversarial%2520Distribution%2520Matching%26entry.906535625%3DZiyu%2520Gong%2520and%2520Ben%2520Usman%2520and%2520Han%2520Zhao%2520and%2520David%2520I.%2520Inouye%26entry.1292438233%3D%2520%2520Distribution%2520matching%2520can%2520be%2520used%2520to%2520learn%2520invariant%2520representations%2520with%250Aapplications%2520in%2520fairness%2520and%2520robustness.%2520Most%2520prior%2520works%2520resort%2520to%2520adversarial%250Amatching%2520methods%2520but%2520the%2520resulting%2520minimax%2520problems%2520are%2520unstable%2520and%250Achallenging%2520to%2520optimize.%2520Non-adversarial%2520likelihood-based%2520approaches%2520either%250Arequire%2520model%2520invertibility%252C%2520impose%2520constraints%2520on%2520the%2520latent%2520prior%252C%2520or%2520lack%2520a%250Ageneric%2520framework%2520for%2520distribution%2520matching.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520non-adversarial%2520VAE-based%2520matching%2520method%2520that%2520can%2520be%2520applied%2520to%2520any%250Amodel%2520pipeline.%2520We%2520develop%2520a%2520set%2520of%2520alignment%2520upper%2520bounds%2520for%2520distribution%250Amatching%2520%2528including%2520a%2520noisy%2520bound%2529%2520that%2520have%2520VAE-like%2520objectives%2520but%2520with%2520a%250Adifferent%2520perspective.%2520We%2520carefully%2520compare%2520our%2520method%2520to%2520prior%2520VAE-based%250Amatching%2520approaches%2520both%2520theoretically%2520and%2520empirically.%2520Finally%252C%2520we%2520demonstrate%250Athat%2520our%2520novel%2520matching%2520losses%2520can%2520replace%2520adversarial%2520losses%2520in%2520standard%250Ainvariant%2520representation%2520learning%2520pipelines%2520without%2520modifying%2520the%2520original%250Aarchitectures%2520--%2520thereby%2520significantly%2520broadening%2520the%2520applicability%2520of%250Anon-adversarial%2520matching%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Practical%20Non-Adversarial%20Distribution%20Matching&entry.906535625=Ziyu%20Gong%20and%20Ben%20Usman%20and%20Han%20Zhao%20and%20David%20I.%20Inouye&entry.1292438233=%20%20Distribution%20matching%20can%20be%20used%20to%20learn%20invariant%20representations%20with%0Aapplications%20in%20fairness%20and%20robustness.%20Most%20prior%20works%20resort%20to%20adversarial%0Amatching%20methods%20but%20the%20resulting%20minimax%20problems%20are%20unstable%20and%0Achallenging%20to%20optimize.%20Non-adversarial%20likelihood-based%20approaches%20either%0Arequire%20model%20invertibility%2C%20impose%20constraints%20on%20the%20latent%20prior%2C%20or%20lack%20a%0Ageneric%20framework%20for%20distribution%20matching.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20non-adversarial%20VAE-based%20matching%20method%20that%20can%20be%20applied%20to%20any%0Amodel%20pipeline.%20We%20develop%20a%20set%20of%20alignment%20upper%20bounds%20for%20distribution%0Amatching%20%28including%20a%20noisy%20bound%29%20that%20have%20VAE-like%20objectives%20but%20with%20a%0Adifferent%20perspective.%20We%20carefully%20compare%20our%20method%20to%20prior%20VAE-based%0Amatching%20approaches%20both%20theoretically%20and%20empirically.%20Finally%2C%20we%20demonstrate%0Athat%20our%20novel%20matching%20losses%20can%20replace%20adversarial%20losses%20in%20standard%0Ainvariant%20representation%20learning%20pipelines%20without%20modifying%20the%20original%0Aarchitectures%20--%20thereby%20significantly%20broadening%20the%20applicability%20of%0Anon-adversarial%20matching%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19690v2&entry.124074799=Read"},
{"title": "SMCL: Saliency Masked Contrastive Learning for Long-tailed Recognition", "author": "Sanglee Park and Seung-won Hwang and Jungmin So", "abstract": "  Real-world data often follow a long-tailed distribution with a high imbalance\nin the number of samples between classes. The problem with training from\nimbalanced data is that some background features, common to all classes, can be\nunobserved in classes with scarce samples. As a result, this background\ncorrelates to biased predictions into ``major\" classes. In this paper, we\npropose saliency masked contrastive learning, a new method that uses saliency\nmasking and contrastive learning to mitigate the problem and improve the\ngeneralizability of a model. Our key idea is to mask the important part of an\nimage using saliency detection and use contrastive learning to move the masked\nimage towards minor classes in the feature space, so that background features\npresent in the masked image are no longer correlated with the original class.\nExperiment results show that our method achieves state-of-the-art level\nperformance on benchmark long-tailed datasets.\n", "link": "http://arxiv.org/abs/2406.02223v1", "date": "2024-06-04", "relevancy": 2.3844, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5631}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMCL%3A%20Saliency%20Masked%20Contrastive%20Learning%20for%20Long-tailed%20Recognition&body=Title%3A%20SMCL%3A%20Saliency%20Masked%20Contrastive%20Learning%20for%20Long-tailed%20Recognition%0AAuthor%3A%20Sanglee%20Park%20and%20Seung-won%20Hwang%20and%20Jungmin%20So%0AAbstract%3A%20%20%20Real-world%20data%20often%20follow%20a%20long-tailed%20distribution%20with%20a%20high%20imbalance%0Ain%20the%20number%20of%20samples%20between%20classes.%20The%20problem%20with%20training%20from%0Aimbalanced%20data%20is%20that%20some%20background%20features%2C%20common%20to%20all%20classes%2C%20can%20be%0Aunobserved%20in%20classes%20with%20scarce%20samples.%20As%20a%20result%2C%20this%20background%0Acorrelates%20to%20biased%20predictions%20into%20%60%60major%22%20classes.%20In%20this%20paper%2C%20we%0Apropose%20saliency%20masked%20contrastive%20learning%2C%20a%20new%20method%20that%20uses%20saliency%0Amasking%20and%20contrastive%20learning%20to%20mitigate%20the%20problem%20and%20improve%20the%0Ageneralizability%20of%20a%20model.%20Our%20key%20idea%20is%20to%20mask%20the%20important%20part%20of%20an%0Aimage%20using%20saliency%20detection%20and%20use%20contrastive%20learning%20to%20move%20the%20masked%0Aimage%20towards%20minor%20classes%20in%20the%20feature%20space%2C%20so%20that%20background%20features%0Apresent%20in%20the%20masked%20image%20are%20no%20longer%20correlated%20with%20the%20original%20class.%0AExperiment%20results%20show%20that%20our%20method%20achieves%20state-of-the-art%20level%0Aperformance%20on%20benchmark%20long-tailed%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMCL%253A%2520Saliency%2520Masked%2520Contrastive%2520Learning%2520for%2520Long-tailed%2520Recognition%26entry.906535625%3DSanglee%2520Park%2520and%2520Seung-won%2520Hwang%2520and%2520Jungmin%2520So%26entry.1292438233%3D%2520%2520Real-world%2520data%2520often%2520follow%2520a%2520long-tailed%2520distribution%2520with%2520a%2520high%2520imbalance%250Ain%2520the%2520number%2520of%2520samples%2520between%2520classes.%2520The%2520problem%2520with%2520training%2520from%250Aimbalanced%2520data%2520is%2520that%2520some%2520background%2520features%252C%2520common%2520to%2520all%2520classes%252C%2520can%2520be%250Aunobserved%2520in%2520classes%2520with%2520scarce%2520samples.%2520As%2520a%2520result%252C%2520this%2520background%250Acorrelates%2520to%2520biased%2520predictions%2520into%2520%2560%2560major%2522%2520classes.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520saliency%2520masked%2520contrastive%2520learning%252C%2520a%2520new%2520method%2520that%2520uses%2520saliency%250Amasking%2520and%2520contrastive%2520learning%2520to%2520mitigate%2520the%2520problem%2520and%2520improve%2520the%250Ageneralizability%2520of%2520a%2520model.%2520Our%2520key%2520idea%2520is%2520to%2520mask%2520the%2520important%2520part%2520of%2520an%250Aimage%2520using%2520saliency%2520detection%2520and%2520use%2520contrastive%2520learning%2520to%2520move%2520the%2520masked%250Aimage%2520towards%2520minor%2520classes%2520in%2520the%2520feature%2520space%252C%2520so%2520that%2520background%2520features%250Apresent%2520in%2520the%2520masked%2520image%2520are%2520no%2520longer%2520correlated%2520with%2520the%2520original%2520class.%250AExperiment%2520results%2520show%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520level%250Aperformance%2520on%2520benchmark%2520long-tailed%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMCL%3A%20Saliency%20Masked%20Contrastive%20Learning%20for%20Long-tailed%20Recognition&entry.906535625=Sanglee%20Park%20and%20Seung-won%20Hwang%20and%20Jungmin%20So&entry.1292438233=%20%20Real-world%20data%20often%20follow%20a%20long-tailed%20distribution%20with%20a%20high%20imbalance%0Ain%20the%20number%20of%20samples%20between%20classes.%20The%20problem%20with%20training%20from%0Aimbalanced%20data%20is%20that%20some%20background%20features%2C%20common%20to%20all%20classes%2C%20can%20be%0Aunobserved%20in%20classes%20with%20scarce%20samples.%20As%20a%20result%2C%20this%20background%0Acorrelates%20to%20biased%20predictions%20into%20%60%60major%22%20classes.%20In%20this%20paper%2C%20we%0Apropose%20saliency%20masked%20contrastive%20learning%2C%20a%20new%20method%20that%20uses%20saliency%0Amasking%20and%20contrastive%20learning%20to%20mitigate%20the%20problem%20and%20improve%20the%0Ageneralizability%20of%20a%20model.%20Our%20key%20idea%20is%20to%20mask%20the%20important%20part%20of%20an%0Aimage%20using%20saliency%20detection%20and%20use%20contrastive%20learning%20to%20move%20the%20masked%0Aimage%20towards%20minor%20classes%20in%20the%20feature%20space%2C%20so%20that%20background%20features%0Apresent%20in%20the%20masked%20image%20are%20no%20longer%20correlated%20with%20the%20original%20class.%0AExperiment%20results%20show%20that%20our%20method%20achieves%20state-of-the-art%20level%0Aperformance%20on%20benchmark%20long-tailed%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02223v1&entry.124074799=Read"},
{"title": "MidiCaps -- A large-scale MIDI dataset with text captions", "author": "Jan Melechovsky and Abhinaba Roy and Dorien Herremans", "abstract": "  Generative models guided by text prompts are increasingly becoming more\npopular. However, no text-to-MIDI models currently exist, mostly due to the\nlack of a captioned MIDI dataset. This work aims to enable research that\ncombines LLMs with symbolic music by presenting the first large-scale MIDI\ndataset with text captions that is openly available: MidiCaps. MIDI (Musical\nInstrument Digital Interface) files are a widely used format for encoding\nmusical information. Their structured format captures the nuances of musical\ncomposition and has practical applications by music producers, composers,\nmusicologists, as well as performers. Inspired by recent advancements in\ncaptioning techniques applied to various domains, we present a large-scale\ncurated dataset of over 168k MIDI files accompanied by textual descriptions.\nEach MIDI caption succinctly describes the musical content, encompassing tempo,\nchord progression, time signature, instruments present, genre and mood; thereby\nfacilitating multi-modal exploration and analysis. The dataset contains a mix\nof various genres, styles, and complexities, offering a rich source for\ntraining and evaluating models for tasks such as music information retrieval,\nmusic understanding and cross-modal translation. We provide detailed statistics\nabout the dataset and have assessed the quality of the captions in an extensive\nlistening study. We anticipate that this resource will stimulate further\nresearch in the intersection of music and natural language processing,\nfostering advancements in both fields.\n", "link": "http://arxiv.org/abs/2406.02255v1", "date": "2024-06-04", "relevancy": 2.3842, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4798}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4765}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MidiCaps%20--%20A%20large-scale%20MIDI%20dataset%20with%20text%20captions&body=Title%3A%20MidiCaps%20--%20A%20large-scale%20MIDI%20dataset%20with%20text%20captions%0AAuthor%3A%20Jan%20Melechovsky%20and%20Abhinaba%20Roy%20and%20Dorien%20Herremans%0AAbstract%3A%20%20%20Generative%20models%20guided%20by%20text%20prompts%20are%20increasingly%20becoming%20more%0Apopular.%20However%2C%20no%20text-to-MIDI%20models%20currently%20exist%2C%20mostly%20due%20to%20the%0Alack%20of%20a%20captioned%20MIDI%20dataset.%20This%20work%20aims%20to%20enable%20research%20that%0Acombines%20LLMs%20with%20symbolic%20music%20by%20presenting%20the%20first%20large-scale%20MIDI%0Adataset%20with%20text%20captions%20that%20is%20openly%20available%3A%20MidiCaps.%20MIDI%20%28Musical%0AInstrument%20Digital%20Interface%29%20files%20are%20a%20widely%20used%20format%20for%20encoding%0Amusical%20information.%20Their%20structured%20format%20captures%20the%20nuances%20of%20musical%0Acomposition%20and%20has%20practical%20applications%20by%20music%20producers%2C%20composers%2C%0Amusicologists%2C%20as%20well%20as%20performers.%20Inspired%20by%20recent%20advancements%20in%0Acaptioning%20techniques%20applied%20to%20various%20domains%2C%20we%20present%20a%20large-scale%0Acurated%20dataset%20of%20over%20168k%20MIDI%20files%20accompanied%20by%20textual%20descriptions.%0AEach%20MIDI%20caption%20succinctly%20describes%20the%20musical%20content%2C%20encompassing%20tempo%2C%0Achord%20progression%2C%20time%20signature%2C%20instruments%20present%2C%20genre%20and%20mood%3B%20thereby%0Afacilitating%20multi-modal%20exploration%20and%20analysis.%20The%20dataset%20contains%20a%20mix%0Aof%20various%20genres%2C%20styles%2C%20and%20complexities%2C%20offering%20a%20rich%20source%20for%0Atraining%20and%20evaluating%20models%20for%20tasks%20such%20as%20music%20information%20retrieval%2C%0Amusic%20understanding%20and%20cross-modal%20translation.%20We%20provide%20detailed%20statistics%0Aabout%20the%20dataset%20and%20have%20assessed%20the%20quality%20of%20the%20captions%20in%20an%20extensive%0Alistening%20study.%20We%20anticipate%20that%20this%20resource%20will%20stimulate%20further%0Aresearch%20in%20the%20intersection%20of%20music%20and%20natural%20language%20processing%2C%0Afostering%20advancements%20in%20both%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMidiCaps%2520--%2520A%2520large-scale%2520MIDI%2520dataset%2520with%2520text%2520captions%26entry.906535625%3DJan%2520Melechovsky%2520and%2520Abhinaba%2520Roy%2520and%2520Dorien%2520Herremans%26entry.1292438233%3D%2520%2520Generative%2520models%2520guided%2520by%2520text%2520prompts%2520are%2520increasingly%2520becoming%2520more%250Apopular.%2520However%252C%2520no%2520text-to-MIDI%2520models%2520currently%2520exist%252C%2520mostly%2520due%2520to%2520the%250Alack%2520of%2520a%2520captioned%2520MIDI%2520dataset.%2520This%2520work%2520aims%2520to%2520enable%2520research%2520that%250Acombines%2520LLMs%2520with%2520symbolic%2520music%2520by%2520presenting%2520the%2520first%2520large-scale%2520MIDI%250Adataset%2520with%2520text%2520captions%2520that%2520is%2520openly%2520available%253A%2520MidiCaps.%2520MIDI%2520%2528Musical%250AInstrument%2520Digital%2520Interface%2529%2520files%2520are%2520a%2520widely%2520used%2520format%2520for%2520encoding%250Amusical%2520information.%2520Their%2520structured%2520format%2520captures%2520the%2520nuances%2520of%2520musical%250Acomposition%2520and%2520has%2520practical%2520applications%2520by%2520music%2520producers%252C%2520composers%252C%250Amusicologists%252C%2520as%2520well%2520as%2520performers.%2520Inspired%2520by%2520recent%2520advancements%2520in%250Acaptioning%2520techniques%2520applied%2520to%2520various%2520domains%252C%2520we%2520present%2520a%2520large-scale%250Acurated%2520dataset%2520of%2520over%2520168k%2520MIDI%2520files%2520accompanied%2520by%2520textual%2520descriptions.%250AEach%2520MIDI%2520caption%2520succinctly%2520describes%2520the%2520musical%2520content%252C%2520encompassing%2520tempo%252C%250Achord%2520progression%252C%2520time%2520signature%252C%2520instruments%2520present%252C%2520genre%2520and%2520mood%253B%2520thereby%250Afacilitating%2520multi-modal%2520exploration%2520and%2520analysis.%2520The%2520dataset%2520contains%2520a%2520mix%250Aof%2520various%2520genres%252C%2520styles%252C%2520and%2520complexities%252C%2520offering%2520a%2520rich%2520source%2520for%250Atraining%2520and%2520evaluating%2520models%2520for%2520tasks%2520such%2520as%2520music%2520information%2520retrieval%252C%250Amusic%2520understanding%2520and%2520cross-modal%2520translation.%2520We%2520provide%2520detailed%2520statistics%250Aabout%2520the%2520dataset%2520and%2520have%2520assessed%2520the%2520quality%2520of%2520the%2520captions%2520in%2520an%2520extensive%250Alistening%2520study.%2520We%2520anticipate%2520that%2520this%2520resource%2520will%2520stimulate%2520further%250Aresearch%2520in%2520the%2520intersection%2520of%2520music%2520and%2520natural%2520language%2520processing%252C%250Afostering%2520advancements%2520in%2520both%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MidiCaps%20--%20A%20large-scale%20MIDI%20dataset%20with%20text%20captions&entry.906535625=Jan%20Melechovsky%20and%20Abhinaba%20Roy%20and%20Dorien%20Herremans&entry.1292438233=%20%20Generative%20models%20guided%20by%20text%20prompts%20are%20increasingly%20becoming%20more%0Apopular.%20However%2C%20no%20text-to-MIDI%20models%20currently%20exist%2C%20mostly%20due%20to%20the%0Alack%20of%20a%20captioned%20MIDI%20dataset.%20This%20work%20aims%20to%20enable%20research%20that%0Acombines%20LLMs%20with%20symbolic%20music%20by%20presenting%20the%20first%20large-scale%20MIDI%0Adataset%20with%20text%20captions%20that%20is%20openly%20available%3A%20MidiCaps.%20MIDI%20%28Musical%0AInstrument%20Digital%20Interface%29%20files%20are%20a%20widely%20used%20format%20for%20encoding%0Amusical%20information.%20Their%20structured%20format%20captures%20the%20nuances%20of%20musical%0Acomposition%20and%20has%20practical%20applications%20by%20music%20producers%2C%20composers%2C%0Amusicologists%2C%20as%20well%20as%20performers.%20Inspired%20by%20recent%20advancements%20in%0Acaptioning%20techniques%20applied%20to%20various%20domains%2C%20we%20present%20a%20large-scale%0Acurated%20dataset%20of%20over%20168k%20MIDI%20files%20accompanied%20by%20textual%20descriptions.%0AEach%20MIDI%20caption%20succinctly%20describes%20the%20musical%20content%2C%20encompassing%20tempo%2C%0Achord%20progression%2C%20time%20signature%2C%20instruments%20present%2C%20genre%20and%20mood%3B%20thereby%0Afacilitating%20multi-modal%20exploration%20and%20analysis.%20The%20dataset%20contains%20a%20mix%0Aof%20various%20genres%2C%20styles%2C%20and%20complexities%2C%20offering%20a%20rich%20source%20for%0Atraining%20and%20evaluating%20models%20for%20tasks%20such%20as%20music%20information%20retrieval%2C%0Amusic%20understanding%20and%20cross-modal%20translation.%20We%20provide%20detailed%20statistics%0Aabout%20the%20dataset%20and%20have%20assessed%20the%20quality%20of%20the%20captions%20in%20an%20extensive%0Alistening%20study.%20We%20anticipate%20that%20this%20resource%20will%20stimulate%20further%0Aresearch%20in%20the%20intersection%20of%20music%20and%20natural%20language%20processing%2C%0Afostering%20advancements%20in%20both%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02255v1&entry.124074799=Read"},
{"title": "Graph Neural Networks Do Not Always Oversmooth", "author": "Bastian Epping and Alexandre Ren\u00e9 and Moritz Helias and Michael T. Schaub", "abstract": "  Graph neural networks (GNNs) have emerged as powerful tools for processing\nrelational data in applications. However, GNNs suffer from the problem of\noversmoothing, the property that the features of all nodes exponentially\nconverge to the same vector over layers, prohibiting the design of deep GNNs.\nIn this work we study oversmoothing in graph convolutional networks (GCNs) by\nusing their Gaussian process (GP) equivalence in the limit of infinitely many\nhidden features. By generalizing methods from conventional deep neural networks\n(DNNs), we can describe the distribution of features at the output layer of\ndeep GCNs in terms of a GP: as expected, we find that typical parameter choices\nfrom the literature lead to oversmoothing. The theory, however, allows us to\nidentify a new, nonoversmoothing phase: if the initial weights of the network\nhave sufficiently large variance, GCNs do not oversmooth, and node features\nremain informative even at large depth. We demonstrate the validity of this\nprediction in finite-size GCNs by training a linear classifier on their output.\nMoreover, using the linearization of the GCN GP, we generalize the concept of\npropagation depth of information from DNNs to GCNs. This propagation depth\ndiverges at the transition between the oversmoothing and non-oversmoothing\nphase. We test the predictions of our approach and find good agreement with\nfinite-size GCNs. Initializing GCNs near the transition to the\nnon-oversmoothing phase, we obtain networks which are both deep and expressive.\n", "link": "http://arxiv.org/abs/2406.02269v1", "date": "2024-06-04", "relevancy": 2.372, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5111}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4561}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20Do%20Not%20Always%20Oversmooth&body=Title%3A%20Graph%20Neural%20Networks%20Do%20Not%20Always%20Oversmooth%0AAuthor%3A%20Bastian%20Epping%20and%20Alexandre%20Ren%C3%A9%20and%20Moritz%20Helias%20and%20Michael%20T.%20Schaub%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20tools%20for%20processing%0Arelational%20data%20in%20applications.%20However%2C%20GNNs%20suffer%20from%20the%20problem%20of%0Aoversmoothing%2C%20the%20property%20that%20the%20features%20of%20all%20nodes%20exponentially%0Aconverge%20to%20the%20same%20vector%20over%20layers%2C%20prohibiting%20the%20design%20of%20deep%20GNNs.%0AIn%20this%20work%20we%20study%20oversmoothing%20in%20graph%20convolutional%20networks%20%28GCNs%29%20by%0Ausing%20their%20Gaussian%20process%20%28GP%29%20equivalence%20in%20the%20limit%20of%20infinitely%20many%0Ahidden%20features.%20By%20generalizing%20methods%20from%20conventional%20deep%20neural%20networks%0A%28DNNs%29%2C%20we%20can%20describe%20the%20distribution%20of%20features%20at%20the%20output%20layer%20of%0Adeep%20GCNs%20in%20terms%20of%20a%20GP%3A%20as%20expected%2C%20we%20find%20that%20typical%20parameter%20choices%0Afrom%20the%20literature%20lead%20to%20oversmoothing.%20The%20theory%2C%20however%2C%20allows%20us%20to%0Aidentify%20a%20new%2C%20nonoversmoothing%20phase%3A%20if%20the%20initial%20weights%20of%20the%20network%0Ahave%20sufficiently%20large%20variance%2C%20GCNs%20do%20not%20oversmooth%2C%20and%20node%20features%0Aremain%20informative%20even%20at%20large%20depth.%20We%20demonstrate%20the%20validity%20of%20this%0Aprediction%20in%20finite-size%20GCNs%20by%20training%20a%20linear%20classifier%20on%20their%20output.%0AMoreover%2C%20using%20the%20linearization%20of%20the%20GCN%20GP%2C%20we%20generalize%20the%20concept%20of%0Apropagation%20depth%20of%20information%20from%20DNNs%20to%20GCNs.%20This%20propagation%20depth%0Adiverges%20at%20the%20transition%20between%20the%20oversmoothing%20and%20non-oversmoothing%0Aphase.%20We%20test%20the%20predictions%20of%20our%20approach%20and%20find%20good%20agreement%20with%0Afinite-size%20GCNs.%20Initializing%20GCNs%20near%20the%20transition%20to%20the%0Anon-oversmoothing%20phase%2C%20we%20obtain%20networks%20which%20are%20both%20deep%20and%20expressive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520Do%2520Not%2520Always%2520Oversmooth%26entry.906535625%3DBastian%2520Epping%2520and%2520Alexandre%2520Ren%25C3%25A9%2520and%2520Moritz%2520Helias%2520and%2520Michael%2520T.%2520Schaub%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520processing%250Arelational%2520data%2520in%2520applications.%2520However%252C%2520GNNs%2520suffer%2520from%2520the%2520problem%2520of%250Aoversmoothing%252C%2520the%2520property%2520that%2520the%2520features%2520of%2520all%2520nodes%2520exponentially%250Aconverge%2520to%2520the%2520same%2520vector%2520over%2520layers%252C%2520prohibiting%2520the%2520design%2520of%2520deep%2520GNNs.%250AIn%2520this%2520work%2520we%2520study%2520oversmoothing%2520in%2520graph%2520convolutional%2520networks%2520%2528GCNs%2529%2520by%250Ausing%2520their%2520Gaussian%2520process%2520%2528GP%2529%2520equivalence%2520in%2520the%2520limit%2520of%2520infinitely%2520many%250Ahidden%2520features.%2520By%2520generalizing%2520methods%2520from%2520conventional%2520deep%2520neural%2520networks%250A%2528DNNs%2529%252C%2520we%2520can%2520describe%2520the%2520distribution%2520of%2520features%2520at%2520the%2520output%2520layer%2520of%250Adeep%2520GCNs%2520in%2520terms%2520of%2520a%2520GP%253A%2520as%2520expected%252C%2520we%2520find%2520that%2520typical%2520parameter%2520choices%250Afrom%2520the%2520literature%2520lead%2520to%2520oversmoothing.%2520The%2520theory%252C%2520however%252C%2520allows%2520us%2520to%250Aidentify%2520a%2520new%252C%2520nonoversmoothing%2520phase%253A%2520if%2520the%2520initial%2520weights%2520of%2520the%2520network%250Ahave%2520sufficiently%2520large%2520variance%252C%2520GCNs%2520do%2520not%2520oversmooth%252C%2520and%2520node%2520features%250Aremain%2520informative%2520even%2520at%2520large%2520depth.%2520We%2520demonstrate%2520the%2520validity%2520of%2520this%250Aprediction%2520in%2520finite-size%2520GCNs%2520by%2520training%2520a%2520linear%2520classifier%2520on%2520their%2520output.%250AMoreover%252C%2520using%2520the%2520linearization%2520of%2520the%2520GCN%2520GP%252C%2520we%2520generalize%2520the%2520concept%2520of%250Apropagation%2520depth%2520of%2520information%2520from%2520DNNs%2520to%2520GCNs.%2520This%2520propagation%2520depth%250Adiverges%2520at%2520the%2520transition%2520between%2520the%2520oversmoothing%2520and%2520non-oversmoothing%250Aphase.%2520We%2520test%2520the%2520predictions%2520of%2520our%2520approach%2520and%2520find%2520good%2520agreement%2520with%250Afinite-size%2520GCNs.%2520Initializing%2520GCNs%2520near%2520the%2520transition%2520to%2520the%250Anon-oversmoothing%2520phase%252C%2520we%2520obtain%2520networks%2520which%2520are%2520both%2520deep%2520and%2520expressive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20Do%20Not%20Always%20Oversmooth&entry.906535625=Bastian%20Epping%20and%20Alexandre%20Ren%C3%A9%20and%20Moritz%20Helias%20and%20Michael%20T.%20Schaub&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20tools%20for%20processing%0Arelational%20data%20in%20applications.%20However%2C%20GNNs%20suffer%20from%20the%20problem%20of%0Aoversmoothing%2C%20the%20property%20that%20the%20features%20of%20all%20nodes%20exponentially%0Aconverge%20to%20the%20same%20vector%20over%20layers%2C%20prohibiting%20the%20design%20of%20deep%20GNNs.%0AIn%20this%20work%20we%20study%20oversmoothing%20in%20graph%20convolutional%20networks%20%28GCNs%29%20by%0Ausing%20their%20Gaussian%20process%20%28GP%29%20equivalence%20in%20the%20limit%20of%20infinitely%20many%0Ahidden%20features.%20By%20generalizing%20methods%20from%20conventional%20deep%20neural%20networks%0A%28DNNs%29%2C%20we%20can%20describe%20the%20distribution%20of%20features%20at%20the%20output%20layer%20of%0Adeep%20GCNs%20in%20terms%20of%20a%20GP%3A%20as%20expected%2C%20we%20find%20that%20typical%20parameter%20choices%0Afrom%20the%20literature%20lead%20to%20oversmoothing.%20The%20theory%2C%20however%2C%20allows%20us%20to%0Aidentify%20a%20new%2C%20nonoversmoothing%20phase%3A%20if%20the%20initial%20weights%20of%20the%20network%0Ahave%20sufficiently%20large%20variance%2C%20GCNs%20do%20not%20oversmooth%2C%20and%20node%20features%0Aremain%20informative%20even%20at%20large%20depth.%20We%20demonstrate%20the%20validity%20of%20this%0Aprediction%20in%20finite-size%20GCNs%20by%20training%20a%20linear%20classifier%20on%20their%20output.%0AMoreover%2C%20using%20the%20linearization%20of%20the%20GCN%20GP%2C%20we%20generalize%20the%20concept%20of%0Apropagation%20depth%20of%20information%20from%20DNNs%20to%20GCNs.%20This%20propagation%20depth%0Adiverges%20at%20the%20transition%20between%20the%20oversmoothing%20and%20non-oversmoothing%0Aphase.%20We%20test%20the%20predictions%20of%20our%20approach%20and%20find%20good%20agreement%20with%0Afinite-size%20GCNs.%20Initializing%20GCNs%20near%20the%20transition%20to%20the%0Anon-oversmoothing%20phase%2C%20we%20obtain%20networks%20which%20are%20both%20deep%20and%20expressive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02269v1&entry.124074799=Read"},
{"title": "Vertical Federated Learning for Effectiveness, Security, Applicability:\n  A Survey", "author": "Mang Ye and Wei Shen and Bo Du and Eduard Snezhko and Vassili Kovalev and Pong C. Yuen", "abstract": "  Vertical Federated Learning (VFL) is a privacy-preserving distributed\nlearning paradigm where different parties collaboratively learn models using\npartitioned features of shared samples, without leaking private data. Recent\nresearch has shown promising results addressing various challenges in VFL,\nhighlighting its potential for practical applications in cross-domain\ncollaboration. However, the corresponding research is scattered and lacks\norganization. To advance VFL research, this survey offers a systematic overview\nof recent developments. First, we provide a history and background\nintroduction, along with a summary of the general training protocol of VFL. We\nthen revisit the taxonomy in recent reviews and analyze limitations in-depth.\nFor a comprehensive and structured discussion, we synthesize recent research\nfrom three fundamental perspectives: effectiveness, security, and\napplicability. Finally, we discuss several critical future research directions\nin VFL, which will facilitate the developments in this field. We provide a\ncollection of research lists and periodically update them at\nhttps://github.com/shentt67/VFL_Survey.\n", "link": "http://arxiv.org/abs/2405.17495v2", "date": "2024-06-04", "relevancy": 2.364, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4865}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4666}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vertical%20Federated%20Learning%20for%20Effectiveness%2C%20Security%2C%20Applicability%3A%0A%20%20A%20Survey&body=Title%3A%20Vertical%20Federated%20Learning%20for%20Effectiveness%2C%20Security%2C%20Applicability%3A%0A%20%20A%20Survey%0AAuthor%3A%20Mang%20Ye%20and%20Wei%20Shen%20and%20Bo%20Du%20and%20Eduard%20Snezhko%20and%20Vassili%20Kovalev%20and%20Pong%20C.%20Yuen%0AAbstract%3A%20%20%20Vertical%20Federated%20Learning%20%28VFL%29%20is%20a%20privacy-preserving%20distributed%0Alearning%20paradigm%20where%20different%20parties%20collaboratively%20learn%20models%20using%0Apartitioned%20features%20of%20shared%20samples%2C%20without%20leaking%20private%20data.%20Recent%0Aresearch%20has%20shown%20promising%20results%20addressing%20various%20challenges%20in%20VFL%2C%0Ahighlighting%20its%20potential%20for%20practical%20applications%20in%20cross-domain%0Acollaboration.%20However%2C%20the%20corresponding%20research%20is%20scattered%20and%20lacks%0Aorganization.%20To%20advance%20VFL%20research%2C%20this%20survey%20offers%20a%20systematic%20overview%0Aof%20recent%20developments.%20First%2C%20we%20provide%20a%20history%20and%20background%0Aintroduction%2C%20along%20with%20a%20summary%20of%20the%20general%20training%20protocol%20of%20VFL.%20We%0Athen%20revisit%20the%20taxonomy%20in%20recent%20reviews%20and%20analyze%20limitations%20in-depth.%0AFor%20a%20comprehensive%20and%20structured%20discussion%2C%20we%20synthesize%20recent%20research%0Afrom%20three%20fundamental%20perspectives%3A%20effectiveness%2C%20security%2C%20and%0Aapplicability.%20Finally%2C%20we%20discuss%20several%20critical%20future%20research%20directions%0Ain%20VFL%2C%20which%20will%20facilitate%20the%20developments%20in%20this%20field.%20We%20provide%20a%0Acollection%20of%20research%20lists%20and%20periodically%20update%20them%20at%0Ahttps%3A//github.com/shentt67/VFL_Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVertical%2520Federated%2520Learning%2520for%2520Effectiveness%252C%2520Security%252C%2520Applicability%253A%250A%2520%2520A%2520Survey%26entry.906535625%3DMang%2520Ye%2520and%2520Wei%2520Shen%2520and%2520Bo%2520Du%2520and%2520Eduard%2520Snezhko%2520and%2520Vassili%2520Kovalev%2520and%2520Pong%2520C.%2520Yuen%26entry.1292438233%3D%2520%2520Vertical%2520Federated%2520Learning%2520%2528VFL%2529%2520is%2520a%2520privacy-preserving%2520distributed%250Alearning%2520paradigm%2520where%2520different%2520parties%2520collaboratively%2520learn%2520models%2520using%250Apartitioned%2520features%2520of%2520shared%2520samples%252C%2520without%2520leaking%2520private%2520data.%2520Recent%250Aresearch%2520has%2520shown%2520promising%2520results%2520addressing%2520various%2520challenges%2520in%2520VFL%252C%250Ahighlighting%2520its%2520potential%2520for%2520practical%2520applications%2520in%2520cross-domain%250Acollaboration.%2520However%252C%2520the%2520corresponding%2520research%2520is%2520scattered%2520and%2520lacks%250Aorganization.%2520To%2520advance%2520VFL%2520research%252C%2520this%2520survey%2520offers%2520a%2520systematic%2520overview%250Aof%2520recent%2520developments.%2520First%252C%2520we%2520provide%2520a%2520history%2520and%2520background%250Aintroduction%252C%2520along%2520with%2520a%2520summary%2520of%2520the%2520general%2520training%2520protocol%2520of%2520VFL.%2520We%250Athen%2520revisit%2520the%2520taxonomy%2520in%2520recent%2520reviews%2520and%2520analyze%2520limitations%2520in-depth.%250AFor%2520a%2520comprehensive%2520and%2520structured%2520discussion%252C%2520we%2520synthesize%2520recent%2520research%250Afrom%2520three%2520fundamental%2520perspectives%253A%2520effectiveness%252C%2520security%252C%2520and%250Aapplicability.%2520Finally%252C%2520we%2520discuss%2520several%2520critical%2520future%2520research%2520directions%250Ain%2520VFL%252C%2520which%2520will%2520facilitate%2520the%2520developments%2520in%2520this%2520field.%2520We%2520provide%2520a%250Acollection%2520of%2520research%2520lists%2520and%2520periodically%2520update%2520them%2520at%250Ahttps%253A//github.com/shentt67/VFL_Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vertical%20Federated%20Learning%20for%20Effectiveness%2C%20Security%2C%20Applicability%3A%0A%20%20A%20Survey&entry.906535625=Mang%20Ye%20and%20Wei%20Shen%20and%20Bo%20Du%20and%20Eduard%20Snezhko%20and%20Vassili%20Kovalev%20and%20Pong%20C.%20Yuen&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20is%20a%20privacy-preserving%20distributed%0Alearning%20paradigm%20where%20different%20parties%20collaboratively%20learn%20models%20using%0Apartitioned%20features%20of%20shared%20samples%2C%20without%20leaking%20private%20data.%20Recent%0Aresearch%20has%20shown%20promising%20results%20addressing%20various%20challenges%20in%20VFL%2C%0Ahighlighting%20its%20potential%20for%20practical%20applications%20in%20cross-domain%0Acollaboration.%20However%2C%20the%20corresponding%20research%20is%20scattered%20and%20lacks%0Aorganization.%20To%20advance%20VFL%20research%2C%20this%20survey%20offers%20a%20systematic%20overview%0Aof%20recent%20developments.%20First%2C%20we%20provide%20a%20history%20and%20background%0Aintroduction%2C%20along%20with%20a%20summary%20of%20the%20general%20training%20protocol%20of%20VFL.%20We%0Athen%20revisit%20the%20taxonomy%20in%20recent%20reviews%20and%20analyze%20limitations%20in-depth.%0AFor%20a%20comprehensive%20and%20structured%20discussion%2C%20we%20synthesize%20recent%20research%0Afrom%20three%20fundamental%20perspectives%3A%20effectiveness%2C%20security%2C%20and%0Aapplicability.%20Finally%2C%20we%20discuss%20several%20critical%20future%20research%20directions%0Ain%20VFL%2C%20which%20will%20facilitate%20the%20developments%20in%20this%20field.%20We%20provide%20a%0Acollection%20of%20research%20lists%20and%20periodically%20update%20them%20at%0Ahttps%3A//github.com/shentt67/VFL_Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17495v2&entry.124074799=Read"},
{"title": "HLOB -- Information Persistence and Structure in Limit Order Books", "author": "Antonio Briola and Silvia Bartolucci and Tomaso Aste", "abstract": "  We introduce a novel large-scale deep learning model for Limit Order Book\nmid-price changes forecasting, and we name it `HLOB'. This architecture (i)\nexploits the information encoded by an Information Filtering Network, namely\nthe Triangulated Maximally Filtered Graph, to unveil deeper and non-trivial\ndependency structures among volume levels; and (ii) guarantees deterministic\ndesign choices to handle the complexity of the underlying system by drawing\ninspiration from the groundbreaking class of Homological Convolutional Neural\nNetworks. We test our model against 9 state-of-the-art deep learning\nalternatives on 3 real-world Limit Order Book datasets, each including 15\nstocks traded on the NASDAQ exchange, and we systematically characterize the\nscenarios where HLOB outperforms state-of-the-art architectures. Our approach\nsheds new light on the spatial distribution of information in Limit Order Books\nand on its degradation over increasing prediction horizons, narrowing the gap\nbetween microstructural modeling and deep learning-based forecasting in\nhigh-frequency financial markets.\n", "link": "http://arxiv.org/abs/2405.18938v3", "date": "2024-06-04", "relevancy": 2.3572, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4895}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4674}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HLOB%20--%20Information%20Persistence%20and%20Structure%20in%20Limit%20Order%20Books&body=Title%3A%20HLOB%20--%20Information%20Persistence%20and%20Structure%20in%20Limit%20Order%20Books%0AAuthor%3A%20Antonio%20Briola%20and%20Silvia%20Bartolucci%20and%20Tomaso%20Aste%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20large-scale%20deep%20learning%20model%20for%20Limit%20Order%20Book%0Amid-price%20changes%20forecasting%2C%20and%20we%20name%20it%20%60HLOB%27.%20This%20architecture%20%28i%29%0Aexploits%20the%20information%20encoded%20by%20an%20Information%20Filtering%20Network%2C%20namely%0Athe%20Triangulated%20Maximally%20Filtered%20Graph%2C%20to%20unveil%20deeper%20and%20non-trivial%0Adependency%20structures%20among%20volume%20levels%3B%20and%20%28ii%29%20guarantees%20deterministic%0Adesign%20choices%20to%20handle%20the%20complexity%20of%20the%20underlying%20system%20by%20drawing%0Ainspiration%20from%20the%20groundbreaking%20class%20of%20Homological%20Convolutional%20Neural%0ANetworks.%20We%20test%20our%20model%20against%209%20state-of-the-art%20deep%20learning%0Aalternatives%20on%203%20real-world%20Limit%20Order%20Book%20datasets%2C%20each%20including%2015%0Astocks%20traded%20on%20the%20NASDAQ%20exchange%2C%20and%20we%20systematically%20characterize%20the%0Ascenarios%20where%20HLOB%20outperforms%20state-of-the-art%20architectures.%20Our%20approach%0Asheds%20new%20light%20on%20the%20spatial%20distribution%20of%20information%20in%20Limit%20Order%20Books%0Aand%20on%20its%20degradation%20over%20increasing%20prediction%20horizons%2C%20narrowing%20the%20gap%0Abetween%20microstructural%20modeling%20and%20deep%20learning-based%20forecasting%20in%0Ahigh-frequency%20financial%20markets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18938v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHLOB%2520--%2520Information%2520Persistence%2520and%2520Structure%2520in%2520Limit%2520Order%2520Books%26entry.906535625%3DAntonio%2520Briola%2520and%2520Silvia%2520Bartolucci%2520and%2520Tomaso%2520Aste%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520large-scale%2520deep%2520learning%2520model%2520for%2520Limit%2520Order%2520Book%250Amid-price%2520changes%2520forecasting%252C%2520and%2520we%2520name%2520it%2520%2560HLOB%2527.%2520This%2520architecture%2520%2528i%2529%250Aexploits%2520the%2520information%2520encoded%2520by%2520an%2520Information%2520Filtering%2520Network%252C%2520namely%250Athe%2520Triangulated%2520Maximally%2520Filtered%2520Graph%252C%2520to%2520unveil%2520deeper%2520and%2520non-trivial%250Adependency%2520structures%2520among%2520volume%2520levels%253B%2520and%2520%2528ii%2529%2520guarantees%2520deterministic%250Adesign%2520choices%2520to%2520handle%2520the%2520complexity%2520of%2520the%2520underlying%2520system%2520by%2520drawing%250Ainspiration%2520from%2520the%2520groundbreaking%2520class%2520of%2520Homological%2520Convolutional%2520Neural%250ANetworks.%2520We%2520test%2520our%2520model%2520against%25209%2520state-of-the-art%2520deep%2520learning%250Aalternatives%2520on%25203%2520real-world%2520Limit%2520Order%2520Book%2520datasets%252C%2520each%2520including%252015%250Astocks%2520traded%2520on%2520the%2520NASDAQ%2520exchange%252C%2520and%2520we%2520systematically%2520characterize%2520the%250Ascenarios%2520where%2520HLOB%2520outperforms%2520state-of-the-art%2520architectures.%2520Our%2520approach%250Asheds%2520new%2520light%2520on%2520the%2520spatial%2520distribution%2520of%2520information%2520in%2520Limit%2520Order%2520Books%250Aand%2520on%2520its%2520degradation%2520over%2520increasing%2520prediction%2520horizons%252C%2520narrowing%2520the%2520gap%250Abetween%2520microstructural%2520modeling%2520and%2520deep%2520learning-based%2520forecasting%2520in%250Ahigh-frequency%2520financial%2520markets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18938v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HLOB%20--%20Information%20Persistence%20and%20Structure%20in%20Limit%20Order%20Books&entry.906535625=Antonio%20Briola%20and%20Silvia%20Bartolucci%20and%20Tomaso%20Aste&entry.1292438233=%20%20We%20introduce%20a%20novel%20large-scale%20deep%20learning%20model%20for%20Limit%20Order%20Book%0Amid-price%20changes%20forecasting%2C%20and%20we%20name%20it%20%60HLOB%27.%20This%20architecture%20%28i%29%0Aexploits%20the%20information%20encoded%20by%20an%20Information%20Filtering%20Network%2C%20namely%0Athe%20Triangulated%20Maximally%20Filtered%20Graph%2C%20to%20unveil%20deeper%20and%20non-trivial%0Adependency%20structures%20among%20volume%20levels%3B%20and%20%28ii%29%20guarantees%20deterministic%0Adesign%20choices%20to%20handle%20the%20complexity%20of%20the%20underlying%20system%20by%20drawing%0Ainspiration%20from%20the%20groundbreaking%20class%20of%20Homological%20Convolutional%20Neural%0ANetworks.%20We%20test%20our%20model%20against%209%20state-of-the-art%20deep%20learning%0Aalternatives%20on%203%20real-world%20Limit%20Order%20Book%20datasets%2C%20each%20including%2015%0Astocks%20traded%20on%20the%20NASDAQ%20exchange%2C%20and%20we%20systematically%20characterize%20the%0Ascenarios%20where%20HLOB%20outperforms%20state-of-the-art%20architectures.%20Our%20approach%0Asheds%20new%20light%20on%20the%20spatial%20distribution%20of%20information%20in%20Limit%20Order%20Books%0Aand%20on%20its%20degradation%20over%20increasing%20prediction%20horizons%2C%20narrowing%20the%20gap%0Abetween%20microstructural%20modeling%20and%20deep%20learning-based%20forecasting%20in%0Ahigh-frequency%20financial%20markets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18938v3&entry.124074799=Read"},
{"title": "CoNav: A Benchmark for Human-Centered Collaborative Navigation", "author": "Changhao Li and Xinyu Sun and Peihao Chen and Jugang Fan and Zixu Wang and Yanxia Liu and Jinhui Zhu and Chuang Gan and Mingkui Tan", "abstract": "  Human-robot collaboration, in which the robot intelligently assists the human\nwith the upcoming task, is an appealing objective. To achieve this goal, the\nagent needs to be equipped with a fundamental collaborative navigation ability,\nwhere the agent should reason human intention by observing human activities and\nthen navigate to the human's intended destination in advance of the human.\nHowever, this vital ability has not been well studied in previous literature.\nTo fill this gap, we propose a collaborative navigation (CoNav) benchmark. Our\nCoNav tackles the critical challenge of constructing a 3D navigation\nenvironment with realistic and diverse human activities. To achieve this, we\ndesign a novel LLM-based humanoid animation generation framework, which is\nconditioned on both text descriptions and environmental context. The generated\nhumanoid trajectory obeys the environmental context and can be easily\nintegrated into popular simulators. We empirically find that the existing\nnavigation methods struggle in CoNav task since they neglect the perception of\nhuman intention. To solve this problem, we propose an intention-aware agent for\nreasoning both long-term and short-term human intention. The agent predicts\nnavigation action based on the predicted intention and panoramic observation.\nThe emergent agent behavior including observing humans, avoiding human\ncollision, and navigation reveals the efficiency of the proposed datasets and\nagents.\n", "link": "http://arxiv.org/abs/2406.02425v1", "date": "2024-06-04", "relevancy": 2.3561, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6197}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5748}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoNav%3A%20A%20Benchmark%20for%20Human-Centered%20Collaborative%20Navigation&body=Title%3A%20CoNav%3A%20A%20Benchmark%20for%20Human-Centered%20Collaborative%20Navigation%0AAuthor%3A%20Changhao%20Li%20and%20Xinyu%20Sun%20and%20Peihao%20Chen%20and%20Jugang%20Fan%20and%20Zixu%20Wang%20and%20Yanxia%20Liu%20and%20Jinhui%20Zhu%20and%20Chuang%20Gan%20and%20Mingkui%20Tan%0AAbstract%3A%20%20%20Human-robot%20collaboration%2C%20in%20which%20the%20robot%20intelligently%20assists%20the%20human%0Awith%20the%20upcoming%20task%2C%20is%20an%20appealing%20objective.%20To%20achieve%20this%20goal%2C%20the%0Aagent%20needs%20to%20be%20equipped%20with%20a%20fundamental%20collaborative%20navigation%20ability%2C%0Awhere%20the%20agent%20should%20reason%20human%20intention%20by%20observing%20human%20activities%20and%0Athen%20navigate%20to%20the%20human%27s%20intended%20destination%20in%20advance%20of%20the%20human.%0AHowever%2C%20this%20vital%20ability%20has%20not%20been%20well%20studied%20in%20previous%20literature.%0ATo%20fill%20this%20gap%2C%20we%20propose%20a%20collaborative%20navigation%20%28CoNav%29%20benchmark.%20Our%0ACoNav%20tackles%20the%20critical%20challenge%20of%20constructing%20a%203D%20navigation%0Aenvironment%20with%20realistic%20and%20diverse%20human%20activities.%20To%20achieve%20this%2C%20we%0Adesign%20a%20novel%20LLM-based%20humanoid%20animation%20generation%20framework%2C%20which%20is%0Aconditioned%20on%20both%20text%20descriptions%20and%20environmental%20context.%20The%20generated%0Ahumanoid%20trajectory%20obeys%20the%20environmental%20context%20and%20can%20be%20easily%0Aintegrated%20into%20popular%20simulators.%20We%20empirically%20find%20that%20the%20existing%0Anavigation%20methods%20struggle%20in%20CoNav%20task%20since%20they%20neglect%20the%20perception%20of%0Ahuman%20intention.%20To%20solve%20this%20problem%2C%20we%20propose%20an%20intention-aware%20agent%20for%0Areasoning%20both%20long-term%20and%20short-term%20human%20intention.%20The%20agent%20predicts%0Anavigation%20action%20based%20on%20the%20predicted%20intention%20and%20panoramic%20observation.%0AThe%20emergent%20agent%20behavior%20including%20observing%20humans%2C%20avoiding%20human%0Acollision%2C%20and%20navigation%20reveals%20the%20efficiency%20of%20the%20proposed%20datasets%20and%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoNav%253A%2520A%2520Benchmark%2520for%2520Human-Centered%2520Collaborative%2520Navigation%26entry.906535625%3DChanghao%2520Li%2520and%2520Xinyu%2520Sun%2520and%2520Peihao%2520Chen%2520and%2520Jugang%2520Fan%2520and%2520Zixu%2520Wang%2520and%2520Yanxia%2520Liu%2520and%2520Jinhui%2520Zhu%2520and%2520Chuang%2520Gan%2520and%2520Mingkui%2520Tan%26entry.1292438233%3D%2520%2520Human-robot%2520collaboration%252C%2520in%2520which%2520the%2520robot%2520intelligently%2520assists%2520the%2520human%250Awith%2520the%2520upcoming%2520task%252C%2520is%2520an%2520appealing%2520objective.%2520To%2520achieve%2520this%2520goal%252C%2520the%250Aagent%2520needs%2520to%2520be%2520equipped%2520with%2520a%2520fundamental%2520collaborative%2520navigation%2520ability%252C%250Awhere%2520the%2520agent%2520should%2520reason%2520human%2520intention%2520by%2520observing%2520human%2520activities%2520and%250Athen%2520navigate%2520to%2520the%2520human%2527s%2520intended%2520destination%2520in%2520advance%2520of%2520the%2520human.%250AHowever%252C%2520this%2520vital%2520ability%2520has%2520not%2520been%2520well%2520studied%2520in%2520previous%2520literature.%250ATo%2520fill%2520this%2520gap%252C%2520we%2520propose%2520a%2520collaborative%2520navigation%2520%2528CoNav%2529%2520benchmark.%2520Our%250ACoNav%2520tackles%2520the%2520critical%2520challenge%2520of%2520constructing%2520a%25203D%2520navigation%250Aenvironment%2520with%2520realistic%2520and%2520diverse%2520human%2520activities.%2520To%2520achieve%2520this%252C%2520we%250Adesign%2520a%2520novel%2520LLM-based%2520humanoid%2520animation%2520generation%2520framework%252C%2520which%2520is%250Aconditioned%2520on%2520both%2520text%2520descriptions%2520and%2520environmental%2520context.%2520The%2520generated%250Ahumanoid%2520trajectory%2520obeys%2520the%2520environmental%2520context%2520and%2520can%2520be%2520easily%250Aintegrated%2520into%2520popular%2520simulators.%2520We%2520empirically%2520find%2520that%2520the%2520existing%250Anavigation%2520methods%2520struggle%2520in%2520CoNav%2520task%2520since%2520they%2520neglect%2520the%2520perception%2520of%250Ahuman%2520intention.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520an%2520intention-aware%2520agent%2520for%250Areasoning%2520both%2520long-term%2520and%2520short-term%2520human%2520intention.%2520The%2520agent%2520predicts%250Anavigation%2520action%2520based%2520on%2520the%2520predicted%2520intention%2520and%2520panoramic%2520observation.%250AThe%2520emergent%2520agent%2520behavior%2520including%2520observing%2520humans%252C%2520avoiding%2520human%250Acollision%252C%2520and%2520navigation%2520reveals%2520the%2520efficiency%2520of%2520the%2520proposed%2520datasets%2520and%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoNav%3A%20A%20Benchmark%20for%20Human-Centered%20Collaborative%20Navigation&entry.906535625=Changhao%20Li%20and%20Xinyu%20Sun%20and%20Peihao%20Chen%20and%20Jugang%20Fan%20and%20Zixu%20Wang%20and%20Yanxia%20Liu%20and%20Jinhui%20Zhu%20and%20Chuang%20Gan%20and%20Mingkui%20Tan&entry.1292438233=%20%20Human-robot%20collaboration%2C%20in%20which%20the%20robot%20intelligently%20assists%20the%20human%0Awith%20the%20upcoming%20task%2C%20is%20an%20appealing%20objective.%20To%20achieve%20this%20goal%2C%20the%0Aagent%20needs%20to%20be%20equipped%20with%20a%20fundamental%20collaborative%20navigation%20ability%2C%0Awhere%20the%20agent%20should%20reason%20human%20intention%20by%20observing%20human%20activities%20and%0Athen%20navigate%20to%20the%20human%27s%20intended%20destination%20in%20advance%20of%20the%20human.%0AHowever%2C%20this%20vital%20ability%20has%20not%20been%20well%20studied%20in%20previous%20literature.%0ATo%20fill%20this%20gap%2C%20we%20propose%20a%20collaborative%20navigation%20%28CoNav%29%20benchmark.%20Our%0ACoNav%20tackles%20the%20critical%20challenge%20of%20constructing%20a%203D%20navigation%0Aenvironment%20with%20realistic%20and%20diverse%20human%20activities.%20To%20achieve%20this%2C%20we%0Adesign%20a%20novel%20LLM-based%20humanoid%20animation%20generation%20framework%2C%20which%20is%0Aconditioned%20on%20both%20text%20descriptions%20and%20environmental%20context.%20The%20generated%0Ahumanoid%20trajectory%20obeys%20the%20environmental%20context%20and%20can%20be%20easily%0Aintegrated%20into%20popular%20simulators.%20We%20empirically%20find%20that%20the%20existing%0Anavigation%20methods%20struggle%20in%20CoNav%20task%20since%20they%20neglect%20the%20perception%20of%0Ahuman%20intention.%20To%20solve%20this%20problem%2C%20we%20propose%20an%20intention-aware%20agent%20for%0Areasoning%20both%20long-term%20and%20short-term%20human%20intention.%20The%20agent%20predicts%0Anavigation%20action%20based%20on%20the%20predicted%20intention%20and%20panoramic%20observation.%0AThe%20emergent%20agent%20behavior%20including%20observing%20humans%2C%20avoiding%20human%0Acollision%2C%20and%20navigation%20reveals%20the%20efficiency%20of%20the%20proposed%20datasets%20and%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02425v1&entry.124074799=Read"},
{"title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation", "author": "Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jos\u00e9 Lezama and Jonathan Huang and Grant Schindler and Rachel Hornung and Vighnesh Birodkar and Jimmy Yan and Ming-Chang Chiu and Krishna Somandepalli and Hassan Akbari and Yair Alon and Yong Cheng and Josh Dillon and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon and Alonso Martinez and David Minnen and Mikhail Sirotenko and Kihyuk Sohn and Xuan Yang and Hartwig Adam and Ming-Hsuan Yang and Irfan Essa and Huisheng Wang and David A. Ross and Bryan Seybold and Lu Jiang", "abstract": "  We present VideoPoet, a language model capable of synthesizing high-quality\nvideo, with matching audio, from a large variety of conditioning signals.\nVideoPoet employs a decoder-only transformer architecture that processes\nmultimodal inputs -- including images, videos, text, and audio. The training\nprotocol follows that of Large Language Models (LLMs), consisting of two\nstages: pretraining and task-specific adaptation. During pretraining, VideoPoet\nincorporates a mixture of multimodal generative objectives within an\nautoregressive Transformer framework. The pretrained LLM serves as a foundation\nthat can be adapted for a range of video generation tasks. We present empirical\nresults demonstrating the model's state-of-the-art capabilities in zero-shot\nvideo generation, specifically highlighting VideoPoet's ability to generate\nhigh-fidelity motions. Project page: http://sites.research.google/videopoet/\n", "link": "http://arxiv.org/abs/2312.14125v4", "date": "2024-06-04", "relevancy": 2.3223, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5818}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5805}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoPoet%3A%20A%20Large%20Language%20Model%20for%20Zero-Shot%20Video%20Generation&body=Title%3A%20VideoPoet%3A%20A%20Large%20Language%20Model%20for%20Zero-Shot%20Video%20Generation%0AAuthor%3A%20Dan%20Kondratyuk%20and%20Lijun%20Yu%20and%20Xiuye%20Gu%20and%20Jos%C3%A9%20Lezama%20and%20Jonathan%20Huang%20and%20Grant%20Schindler%20and%20Rachel%20Hornung%20and%20Vighnesh%20Birodkar%20and%20Jimmy%20Yan%20and%20Ming-Chang%20Chiu%20and%20Krishna%20Somandepalli%20and%20Hassan%20Akbari%20and%20Yair%20Alon%20and%20Yong%20Cheng%20and%20Josh%20Dillon%20and%20Agrim%20Gupta%20and%20Meera%20Hahn%20and%20Anja%20Hauth%20and%20David%20Hendon%20and%20Alonso%20Martinez%20and%20David%20Minnen%20and%20Mikhail%20Sirotenko%20and%20Kihyuk%20Sohn%20and%20Xuan%20Yang%20and%20Hartwig%20Adam%20and%20Ming-Hsuan%20Yang%20and%20Irfan%20Essa%20and%20Huisheng%20Wang%20and%20David%20A.%20Ross%20and%20Bryan%20Seybold%20and%20Lu%20Jiang%0AAbstract%3A%20%20%20We%20present%20VideoPoet%2C%20a%20language%20model%20capable%20of%20synthesizing%20high-quality%0Avideo%2C%20with%20matching%20audio%2C%20from%20a%20large%20variety%20of%20conditioning%20signals.%0AVideoPoet%20employs%20a%20decoder-only%20transformer%20architecture%20that%20processes%0Amultimodal%20inputs%20--%20including%20images%2C%20videos%2C%20text%2C%20and%20audio.%20The%20training%0Aprotocol%20follows%20that%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20consisting%20of%20two%0Astages%3A%20pretraining%20and%20task-specific%20adaptation.%20During%20pretraining%2C%20VideoPoet%0Aincorporates%20a%20mixture%20of%20multimodal%20generative%20objectives%20within%20an%0Aautoregressive%20Transformer%20framework.%20The%20pretrained%20LLM%20serves%20as%20a%20foundation%0Athat%20can%20be%20adapted%20for%20a%20range%20of%20video%20generation%20tasks.%20We%20present%20empirical%0Aresults%20demonstrating%20the%20model%27s%20state-of-the-art%20capabilities%20in%20zero-shot%0Avideo%20generation%2C%20specifically%20highlighting%20VideoPoet%27s%20ability%20to%20generate%0Ahigh-fidelity%20motions.%20Project%20page%3A%20http%3A//sites.research.google/videopoet/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14125v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoPoet%253A%2520A%2520Large%2520Language%2520Model%2520for%2520Zero-Shot%2520Video%2520Generation%26entry.906535625%3DDan%2520Kondratyuk%2520and%2520Lijun%2520Yu%2520and%2520Xiuye%2520Gu%2520and%2520Jos%25C3%25A9%2520Lezama%2520and%2520Jonathan%2520Huang%2520and%2520Grant%2520Schindler%2520and%2520Rachel%2520Hornung%2520and%2520Vighnesh%2520Birodkar%2520and%2520Jimmy%2520Yan%2520and%2520Ming-Chang%2520Chiu%2520and%2520Krishna%2520Somandepalli%2520and%2520Hassan%2520Akbari%2520and%2520Yair%2520Alon%2520and%2520Yong%2520Cheng%2520and%2520Josh%2520Dillon%2520and%2520Agrim%2520Gupta%2520and%2520Meera%2520Hahn%2520and%2520Anja%2520Hauth%2520and%2520David%2520Hendon%2520and%2520Alonso%2520Martinez%2520and%2520David%2520Minnen%2520and%2520Mikhail%2520Sirotenko%2520and%2520Kihyuk%2520Sohn%2520and%2520Xuan%2520Yang%2520and%2520Hartwig%2520Adam%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Irfan%2520Essa%2520and%2520Huisheng%2520Wang%2520and%2520David%2520A.%2520Ross%2520and%2520Bryan%2520Seybold%2520and%2520Lu%2520Jiang%26entry.1292438233%3D%2520%2520We%2520present%2520VideoPoet%252C%2520a%2520language%2520model%2520capable%2520of%2520synthesizing%2520high-quality%250Avideo%252C%2520with%2520matching%2520audio%252C%2520from%2520a%2520large%2520variety%2520of%2520conditioning%2520signals.%250AVideoPoet%2520employs%2520a%2520decoder-only%2520transformer%2520architecture%2520that%2520processes%250Amultimodal%2520inputs%2520--%2520including%2520images%252C%2520videos%252C%2520text%252C%2520and%2520audio.%2520The%2520training%250Aprotocol%2520follows%2520that%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520consisting%2520of%2520two%250Astages%253A%2520pretraining%2520and%2520task-specific%2520adaptation.%2520During%2520pretraining%252C%2520VideoPoet%250Aincorporates%2520a%2520mixture%2520of%2520multimodal%2520generative%2520objectives%2520within%2520an%250Aautoregressive%2520Transformer%2520framework.%2520The%2520pretrained%2520LLM%2520serves%2520as%2520a%2520foundation%250Athat%2520can%2520be%2520adapted%2520for%2520a%2520range%2520of%2520video%2520generation%2520tasks.%2520We%2520present%2520empirical%250Aresults%2520demonstrating%2520the%2520model%2527s%2520state-of-the-art%2520capabilities%2520in%2520zero-shot%250Avideo%2520generation%252C%2520specifically%2520highlighting%2520VideoPoet%2527s%2520ability%2520to%2520generate%250Ahigh-fidelity%2520motions.%2520Project%2520page%253A%2520http%253A//sites.research.google/videopoet/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14125v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoPoet%3A%20A%20Large%20Language%20Model%20for%20Zero-Shot%20Video%20Generation&entry.906535625=Dan%20Kondratyuk%20and%20Lijun%20Yu%20and%20Xiuye%20Gu%20and%20Jos%C3%A9%20Lezama%20and%20Jonathan%20Huang%20and%20Grant%20Schindler%20and%20Rachel%20Hornung%20and%20Vighnesh%20Birodkar%20and%20Jimmy%20Yan%20and%20Ming-Chang%20Chiu%20and%20Krishna%20Somandepalli%20and%20Hassan%20Akbari%20and%20Yair%20Alon%20and%20Yong%20Cheng%20and%20Josh%20Dillon%20and%20Agrim%20Gupta%20and%20Meera%20Hahn%20and%20Anja%20Hauth%20and%20David%20Hendon%20and%20Alonso%20Martinez%20and%20David%20Minnen%20and%20Mikhail%20Sirotenko%20and%20Kihyuk%20Sohn%20and%20Xuan%20Yang%20and%20Hartwig%20Adam%20and%20Ming-Hsuan%20Yang%20and%20Irfan%20Essa%20and%20Huisheng%20Wang%20and%20David%20A.%20Ross%20and%20Bryan%20Seybold%20and%20Lu%20Jiang&entry.1292438233=%20%20We%20present%20VideoPoet%2C%20a%20language%20model%20capable%20of%20synthesizing%20high-quality%0Avideo%2C%20with%20matching%20audio%2C%20from%20a%20large%20variety%20of%20conditioning%20signals.%0AVideoPoet%20employs%20a%20decoder-only%20transformer%20architecture%20that%20processes%0Amultimodal%20inputs%20--%20including%20images%2C%20videos%2C%20text%2C%20and%20audio.%20The%20training%0Aprotocol%20follows%20that%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20consisting%20of%20two%0Astages%3A%20pretraining%20and%20task-specific%20adaptation.%20During%20pretraining%2C%20VideoPoet%0Aincorporates%20a%20mixture%20of%20multimodal%20generative%20objectives%20within%20an%0Aautoregressive%20Transformer%20framework.%20The%20pretrained%20LLM%20serves%20as%20a%20foundation%0Athat%20can%20be%20adapted%20for%20a%20range%20of%20video%20generation%20tasks.%20We%20present%20empirical%0Aresults%20demonstrating%20the%20model%27s%20state-of-the-art%20capabilities%20in%20zero-shot%0Avideo%20generation%2C%20specifically%20highlighting%20VideoPoet%27s%20ability%20to%20generate%0Ahigh-fidelity%20motions.%20Project%20page%3A%20http%3A//sites.research.google/videopoet/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14125v4&entry.124074799=Read"},
{"title": "AMOSL: Adaptive Modality-wise Structure Learning in Multi-view Graph\n  Neural Networks For Enhanced Unified Representation", "author": "Peiyu Liang and Hongchang Gao and Xubin He", "abstract": "  While Multi-view Graph Neural Networks (MVGNNs) excel at leveraging diverse\nmodalities for learning object representation, existing methods assume\nidentical local topology structures across modalities that overlook real-world\ndiscrepancies. This leads MVGNNs straggles in modality fusion and\nrepresentations denoising. To address these issues, we propose adaptive\nmodality-wise structure learning (AMoSL). AMoSL captures node correspondences\nbetween modalities via optimal transport, and jointly learning with graph\nembedding. To enable efficient end-to-end training, we employ an efficient\nsolution for the resulting complex bilevel optimization problem. Furthermore,\nAMoSL adapts to downstream tasks through unsupervised learning on\ninter-modality distances. The effectiveness of AMoSL is demonstrated by its\nability to train more accurate graph classifiers on six benchmark datasets.\n", "link": "http://arxiv.org/abs/2406.02348v1", "date": "2024-06-04", "relevancy": 2.321, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6211}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMOSL%3A%20Adaptive%20Modality-wise%20Structure%20Learning%20in%20Multi-view%20Graph%0A%20%20Neural%20Networks%20For%20Enhanced%20Unified%20Representation&body=Title%3A%20AMOSL%3A%20Adaptive%20Modality-wise%20Structure%20Learning%20in%20Multi-view%20Graph%0A%20%20Neural%20Networks%20For%20Enhanced%20Unified%20Representation%0AAuthor%3A%20Peiyu%20Liang%20and%20Hongchang%20Gao%20and%20Xubin%20He%0AAbstract%3A%20%20%20While%20Multi-view%20Graph%20Neural%20Networks%20%28MVGNNs%29%20excel%20at%20leveraging%20diverse%0Amodalities%20for%20learning%20object%20representation%2C%20existing%20methods%20assume%0Aidentical%20local%20topology%20structures%20across%20modalities%20that%20overlook%20real-world%0Adiscrepancies.%20This%20leads%20MVGNNs%20straggles%20in%20modality%20fusion%20and%0Arepresentations%20denoising.%20To%20address%20these%20issues%2C%20we%20propose%20adaptive%0Amodality-wise%20structure%20learning%20%28AMoSL%29.%20AMoSL%20captures%20node%20correspondences%0Abetween%20modalities%20via%20optimal%20transport%2C%20and%20jointly%20learning%20with%20graph%0Aembedding.%20To%20enable%20efficient%20end-to-end%20training%2C%20we%20employ%20an%20efficient%0Asolution%20for%20the%20resulting%20complex%20bilevel%20optimization%20problem.%20Furthermore%2C%0AAMoSL%20adapts%20to%20downstream%20tasks%20through%20unsupervised%20learning%20on%0Ainter-modality%20distances.%20The%20effectiveness%20of%20AMoSL%20is%20demonstrated%20by%20its%0Aability%20to%20train%20more%20accurate%20graph%20classifiers%20on%20six%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMOSL%253A%2520Adaptive%2520Modality-wise%2520Structure%2520Learning%2520in%2520Multi-view%2520Graph%250A%2520%2520Neural%2520Networks%2520For%2520Enhanced%2520Unified%2520Representation%26entry.906535625%3DPeiyu%2520Liang%2520and%2520Hongchang%2520Gao%2520and%2520Xubin%2520He%26entry.1292438233%3D%2520%2520While%2520Multi-view%2520Graph%2520Neural%2520Networks%2520%2528MVGNNs%2529%2520excel%2520at%2520leveraging%2520diverse%250Amodalities%2520for%2520learning%2520object%2520representation%252C%2520existing%2520methods%2520assume%250Aidentical%2520local%2520topology%2520structures%2520across%2520modalities%2520that%2520overlook%2520real-world%250Adiscrepancies.%2520This%2520leads%2520MVGNNs%2520straggles%2520in%2520modality%2520fusion%2520and%250Arepresentations%2520denoising.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520adaptive%250Amodality-wise%2520structure%2520learning%2520%2528AMoSL%2529.%2520AMoSL%2520captures%2520node%2520correspondences%250Abetween%2520modalities%2520via%2520optimal%2520transport%252C%2520and%2520jointly%2520learning%2520with%2520graph%250Aembedding.%2520To%2520enable%2520efficient%2520end-to-end%2520training%252C%2520we%2520employ%2520an%2520efficient%250Asolution%2520for%2520the%2520resulting%2520complex%2520bilevel%2520optimization%2520problem.%2520Furthermore%252C%250AAMoSL%2520adapts%2520to%2520downstream%2520tasks%2520through%2520unsupervised%2520learning%2520on%250Ainter-modality%2520distances.%2520The%2520effectiveness%2520of%2520AMoSL%2520is%2520demonstrated%2520by%2520its%250Aability%2520to%2520train%2520more%2520accurate%2520graph%2520classifiers%2520on%2520six%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMOSL%3A%20Adaptive%20Modality-wise%20Structure%20Learning%20in%20Multi-view%20Graph%0A%20%20Neural%20Networks%20For%20Enhanced%20Unified%20Representation&entry.906535625=Peiyu%20Liang%20and%20Hongchang%20Gao%20and%20Xubin%20He&entry.1292438233=%20%20While%20Multi-view%20Graph%20Neural%20Networks%20%28MVGNNs%29%20excel%20at%20leveraging%20diverse%0Amodalities%20for%20learning%20object%20representation%2C%20existing%20methods%20assume%0Aidentical%20local%20topology%20structures%20across%20modalities%20that%20overlook%20real-world%0Adiscrepancies.%20This%20leads%20MVGNNs%20straggles%20in%20modality%20fusion%20and%0Arepresentations%20denoising.%20To%20address%20these%20issues%2C%20we%20propose%20adaptive%0Amodality-wise%20structure%20learning%20%28AMoSL%29.%20AMoSL%20captures%20node%20correspondences%0Abetween%20modalities%20via%20optimal%20transport%2C%20and%20jointly%20learning%20with%20graph%0Aembedding.%20To%20enable%20efficient%20end-to-end%20training%2C%20we%20employ%20an%20efficient%0Asolution%20for%20the%20resulting%20complex%20bilevel%20optimization%20problem.%20Furthermore%2C%0AAMoSL%20adapts%20to%20downstream%20tasks%20through%20unsupervised%20learning%20on%0Ainter-modality%20distances.%20The%20effectiveness%20of%20AMoSL%20is%20demonstrated%20by%20its%0Aability%20to%20train%20more%20accurate%20graph%20classifiers%20on%20six%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02348v1&entry.124074799=Read"},
{"title": "Description Boosting for Zero-Shot Entity and Relation Classification", "author": "Gabriele Picco and Leopold Fuchs and Marcos Mart\u00ednez Galindo and Alberto Purpura and Vanessa L\u00f3pez and Hoang Thanh Lam", "abstract": "  Zero-shot entity and relation classification models leverage available\nexternal information of unseen classes -- e.g., textual descriptions -- to\nannotate input text data. Thanks to the minimum data requirement, Zero-Shot\nLearning (ZSL) methods have high value in practice, especially in applications\nwhere labeled data is scarce. Even though recent research in ZSL has\ndemonstrated significant results, our analysis reveals that those methods are\nsensitive to provided textual descriptions of entities (or relations). Even a\nminor modification of descriptions can lead to a change in the decision\nboundary between entity (or relation) classes. In this paper, we formally\ndefine the problem of identifying effective descriptions for zero shot\ninference. We propose a strategy for generating variations of an initial\ndescription, a heuristic for ranking them and an ensemble method capable of\nboosting the predictions of zero-shot models through description enhancement.\nEmpirical results on four different entity and relation classification datasets\nshow that our proposed method outperform existing approaches and achieve new\nSOTA results on these datasets under the ZSL settings. The source code of the\nproposed solutions and the evaluation framework are open-sourced.\n", "link": "http://arxiv.org/abs/2406.02245v1", "date": "2024-06-04", "relevancy": 2.3182, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4695}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4683}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Description%20Boosting%20for%20Zero-Shot%20Entity%20and%20Relation%20Classification&body=Title%3A%20Description%20Boosting%20for%20Zero-Shot%20Entity%20and%20Relation%20Classification%0AAuthor%3A%20Gabriele%20Picco%20and%20Leopold%20Fuchs%20and%20Marcos%20Mart%C3%ADnez%20Galindo%20and%20Alberto%20Purpura%20and%20Vanessa%20L%C3%B3pez%20and%20Hoang%20Thanh%20Lam%0AAbstract%3A%20%20%20Zero-shot%20entity%20and%20relation%20classification%20models%20leverage%20available%0Aexternal%20information%20of%20unseen%20classes%20--%20e.g.%2C%20textual%20descriptions%20--%20to%0Aannotate%20input%20text%20data.%20Thanks%20to%20the%20minimum%20data%20requirement%2C%20Zero-Shot%0ALearning%20%28ZSL%29%20methods%20have%20high%20value%20in%20practice%2C%20especially%20in%20applications%0Awhere%20labeled%20data%20is%20scarce.%20Even%20though%20recent%20research%20in%20ZSL%20has%0Ademonstrated%20significant%20results%2C%20our%20analysis%20reveals%20that%20those%20methods%20are%0Asensitive%20to%20provided%20textual%20descriptions%20of%20entities%20%28or%20relations%29.%20Even%20a%0Aminor%20modification%20of%20descriptions%20can%20lead%20to%20a%20change%20in%20the%20decision%0Aboundary%20between%20entity%20%28or%20relation%29%20classes.%20In%20this%20paper%2C%20we%20formally%0Adefine%20the%20problem%20of%20identifying%20effective%20descriptions%20for%20zero%20shot%0Ainference.%20We%20propose%20a%20strategy%20for%20generating%20variations%20of%20an%20initial%0Adescription%2C%20a%20heuristic%20for%20ranking%20them%20and%20an%20ensemble%20method%20capable%20of%0Aboosting%20the%20predictions%20of%20zero-shot%20models%20through%20description%20enhancement.%0AEmpirical%20results%20on%20four%20different%20entity%20and%20relation%20classification%20datasets%0Ashow%20that%20our%20proposed%20method%20outperform%20existing%20approaches%20and%20achieve%20new%0ASOTA%20results%20on%20these%20datasets%20under%20the%20ZSL%20settings.%20The%20source%20code%20of%20the%0Aproposed%20solutions%20and%20the%20evaluation%20framework%20are%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDescription%2520Boosting%2520for%2520Zero-Shot%2520Entity%2520and%2520Relation%2520Classification%26entry.906535625%3DGabriele%2520Picco%2520and%2520Leopold%2520Fuchs%2520and%2520Marcos%2520Mart%25C3%25ADnez%2520Galindo%2520and%2520Alberto%2520Purpura%2520and%2520Vanessa%2520L%25C3%25B3pez%2520and%2520Hoang%2520Thanh%2520Lam%26entry.1292438233%3D%2520%2520Zero-shot%2520entity%2520and%2520relation%2520classification%2520models%2520leverage%2520available%250Aexternal%2520information%2520of%2520unseen%2520classes%2520--%2520e.g.%252C%2520textual%2520descriptions%2520--%2520to%250Aannotate%2520input%2520text%2520data.%2520Thanks%2520to%2520the%2520minimum%2520data%2520requirement%252C%2520Zero-Shot%250ALearning%2520%2528ZSL%2529%2520methods%2520have%2520high%2520value%2520in%2520practice%252C%2520especially%2520in%2520applications%250Awhere%2520labeled%2520data%2520is%2520scarce.%2520Even%2520though%2520recent%2520research%2520in%2520ZSL%2520has%250Ademonstrated%2520significant%2520results%252C%2520our%2520analysis%2520reveals%2520that%2520those%2520methods%2520are%250Asensitive%2520to%2520provided%2520textual%2520descriptions%2520of%2520entities%2520%2528or%2520relations%2529.%2520Even%2520a%250Aminor%2520modification%2520of%2520descriptions%2520can%2520lead%2520to%2520a%2520change%2520in%2520the%2520decision%250Aboundary%2520between%2520entity%2520%2528or%2520relation%2529%2520classes.%2520In%2520this%2520paper%252C%2520we%2520formally%250Adefine%2520the%2520problem%2520of%2520identifying%2520effective%2520descriptions%2520for%2520zero%2520shot%250Ainference.%2520We%2520propose%2520a%2520strategy%2520for%2520generating%2520variations%2520of%2520an%2520initial%250Adescription%252C%2520a%2520heuristic%2520for%2520ranking%2520them%2520and%2520an%2520ensemble%2520method%2520capable%2520of%250Aboosting%2520the%2520predictions%2520of%2520zero-shot%2520models%2520through%2520description%2520enhancement.%250AEmpirical%2520results%2520on%2520four%2520different%2520entity%2520and%2520relation%2520classification%2520datasets%250Ashow%2520that%2520our%2520proposed%2520method%2520outperform%2520existing%2520approaches%2520and%2520achieve%2520new%250ASOTA%2520results%2520on%2520these%2520datasets%2520under%2520the%2520ZSL%2520settings.%2520The%2520source%2520code%2520of%2520the%250Aproposed%2520solutions%2520and%2520the%2520evaluation%2520framework%2520are%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Description%20Boosting%20for%20Zero-Shot%20Entity%20and%20Relation%20Classification&entry.906535625=Gabriele%20Picco%20and%20Leopold%20Fuchs%20and%20Marcos%20Mart%C3%ADnez%20Galindo%20and%20Alberto%20Purpura%20and%20Vanessa%20L%C3%B3pez%20and%20Hoang%20Thanh%20Lam&entry.1292438233=%20%20Zero-shot%20entity%20and%20relation%20classification%20models%20leverage%20available%0Aexternal%20information%20of%20unseen%20classes%20--%20e.g.%2C%20textual%20descriptions%20--%20to%0Aannotate%20input%20text%20data.%20Thanks%20to%20the%20minimum%20data%20requirement%2C%20Zero-Shot%0ALearning%20%28ZSL%29%20methods%20have%20high%20value%20in%20practice%2C%20especially%20in%20applications%0Awhere%20labeled%20data%20is%20scarce.%20Even%20though%20recent%20research%20in%20ZSL%20has%0Ademonstrated%20significant%20results%2C%20our%20analysis%20reveals%20that%20those%20methods%20are%0Asensitive%20to%20provided%20textual%20descriptions%20of%20entities%20%28or%20relations%29.%20Even%20a%0Aminor%20modification%20of%20descriptions%20can%20lead%20to%20a%20change%20in%20the%20decision%0Aboundary%20between%20entity%20%28or%20relation%29%20classes.%20In%20this%20paper%2C%20we%20formally%0Adefine%20the%20problem%20of%20identifying%20effective%20descriptions%20for%20zero%20shot%0Ainference.%20We%20propose%20a%20strategy%20for%20generating%20variations%20of%20an%20initial%0Adescription%2C%20a%20heuristic%20for%20ranking%20them%20and%20an%20ensemble%20method%20capable%20of%0Aboosting%20the%20predictions%20of%20zero-shot%20models%20through%20description%20enhancement.%0AEmpirical%20results%20on%20four%20different%20entity%20and%20relation%20classification%20datasets%0Ashow%20that%20our%20proposed%20method%20outperform%20existing%20approaches%20and%20achieve%20new%0ASOTA%20results%20on%20these%20datasets%20under%20the%20ZSL%20settings.%20The%20source%20code%20of%20the%0Aproposed%20solutions%20and%20the%20evaluation%20framework%20are%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02245v1&entry.124074799=Read"},
{"title": "Fairness-Optimized Synthetic EHR Generation for Arbitrary Downstream\n  Predictive Tasks", "author": "Mirza Farhan Bin Tarek and Raphael Poulain and Rahmatollah Beheshti", "abstract": "  Among various aspects of ensuring the responsible design of AI tools for\nhealthcare applications, addressing fairness concerns has been a key focus\narea. Specifically, given the wide spread of electronic health record (EHR)\ndata and their huge potential to inform a wide range of clinical decision\nsupport tasks, improving fairness in this category of health AI tools is of key\nimportance. While such a broad problem (that is, mitigating fairness in\nEHR-based AI models) has been tackled using various methods, task- and\nmodel-agnostic methods are noticeably rare. In this study, we aimed to target\nthis gap by presenting a new pipeline that generates synthetic EHR data, which\nis not only consistent with (faithful to) the real EHR data but also can reduce\nthe fairness concerns (defined by the end-user) in the downstream tasks, when\ncombined with the real data. We demonstrate the effectiveness of our proposed\npipeline across various downstream tasks and two different EHR datasets. Our\nproposed pipeline can add a widely applicable and complementary tool to the\nexisting toolbox of methods to address fairness in health AI applications such\nas those modifying the design of a downstream model. The codebase for our\nproject is available at https://github.com/healthylaife/FairSynth\n", "link": "http://arxiv.org/abs/2406.02510v1", "date": "2024-06-04", "relevancy": 2.2987, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4605}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4596}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness-Optimized%20Synthetic%20EHR%20Generation%20for%20Arbitrary%20Downstream%0A%20%20Predictive%20Tasks&body=Title%3A%20Fairness-Optimized%20Synthetic%20EHR%20Generation%20for%20Arbitrary%20Downstream%0A%20%20Predictive%20Tasks%0AAuthor%3A%20Mirza%20Farhan%20Bin%20Tarek%20and%20Raphael%20Poulain%20and%20Rahmatollah%20Beheshti%0AAbstract%3A%20%20%20Among%20various%20aspects%20of%20ensuring%20the%20responsible%20design%20of%20AI%20tools%20for%0Ahealthcare%20applications%2C%20addressing%20fairness%20concerns%20has%20been%20a%20key%20focus%0Aarea.%20Specifically%2C%20given%20the%20wide%20spread%20of%20electronic%20health%20record%20%28EHR%29%0Adata%20and%20their%20huge%20potential%20to%20inform%20a%20wide%20range%20of%20clinical%20decision%0Asupport%20tasks%2C%20improving%20fairness%20in%20this%20category%20of%20health%20AI%20tools%20is%20of%20key%0Aimportance.%20While%20such%20a%20broad%20problem%20%28that%20is%2C%20mitigating%20fairness%20in%0AEHR-based%20AI%20models%29%20has%20been%20tackled%20using%20various%20methods%2C%20task-%20and%0Amodel-agnostic%20methods%20are%20noticeably%20rare.%20In%20this%20study%2C%20we%20aimed%20to%20target%0Athis%20gap%20by%20presenting%20a%20new%20pipeline%20that%20generates%20synthetic%20EHR%20data%2C%20which%0Ais%20not%20only%20consistent%20with%20%28faithful%20to%29%20the%20real%20EHR%20data%20but%20also%20can%20reduce%0Athe%20fairness%20concerns%20%28defined%20by%20the%20end-user%29%20in%20the%20downstream%20tasks%2C%20when%0Acombined%20with%20the%20real%20data.%20We%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Apipeline%20across%20various%20downstream%20tasks%20and%20two%20different%20EHR%20datasets.%20Our%0Aproposed%20pipeline%20can%20add%20a%20widely%20applicable%20and%20complementary%20tool%20to%20the%0Aexisting%20toolbox%20of%20methods%20to%20address%20fairness%20in%20health%20AI%20applications%20such%0Aas%20those%20modifying%20the%20design%20of%20a%20downstream%20model.%20The%20codebase%20for%20our%0Aproject%20is%20available%20at%20https%3A//github.com/healthylaife/FairSynth%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness-Optimized%2520Synthetic%2520EHR%2520Generation%2520for%2520Arbitrary%2520Downstream%250A%2520%2520Predictive%2520Tasks%26entry.906535625%3DMirza%2520Farhan%2520Bin%2520Tarek%2520and%2520Raphael%2520Poulain%2520and%2520Rahmatollah%2520Beheshti%26entry.1292438233%3D%2520%2520Among%2520various%2520aspects%2520of%2520ensuring%2520the%2520responsible%2520design%2520of%2520AI%2520tools%2520for%250Ahealthcare%2520applications%252C%2520addressing%2520fairness%2520concerns%2520has%2520been%2520a%2520key%2520focus%250Aarea.%2520Specifically%252C%2520given%2520the%2520wide%2520spread%2520of%2520electronic%2520health%2520record%2520%2528EHR%2529%250Adata%2520and%2520their%2520huge%2520potential%2520to%2520inform%2520a%2520wide%2520range%2520of%2520clinical%2520decision%250Asupport%2520tasks%252C%2520improving%2520fairness%2520in%2520this%2520category%2520of%2520health%2520AI%2520tools%2520is%2520of%2520key%250Aimportance.%2520While%2520such%2520a%2520broad%2520problem%2520%2528that%2520is%252C%2520mitigating%2520fairness%2520in%250AEHR-based%2520AI%2520models%2529%2520has%2520been%2520tackled%2520using%2520various%2520methods%252C%2520task-%2520and%250Amodel-agnostic%2520methods%2520are%2520noticeably%2520rare.%2520In%2520this%2520study%252C%2520we%2520aimed%2520to%2520target%250Athis%2520gap%2520by%2520presenting%2520a%2520new%2520pipeline%2520that%2520generates%2520synthetic%2520EHR%2520data%252C%2520which%250Ais%2520not%2520only%2520consistent%2520with%2520%2528faithful%2520to%2529%2520the%2520real%2520EHR%2520data%2520but%2520also%2520can%2520reduce%250Athe%2520fairness%2520concerns%2520%2528defined%2520by%2520the%2520end-user%2529%2520in%2520the%2520downstream%2520tasks%252C%2520when%250Acombined%2520with%2520the%2520real%2520data.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Apipeline%2520across%2520various%2520downstream%2520tasks%2520and%2520two%2520different%2520EHR%2520datasets.%2520Our%250Aproposed%2520pipeline%2520can%2520add%2520a%2520widely%2520applicable%2520and%2520complementary%2520tool%2520to%2520the%250Aexisting%2520toolbox%2520of%2520methods%2520to%2520address%2520fairness%2520in%2520health%2520AI%2520applications%2520such%250Aas%2520those%2520modifying%2520the%2520design%2520of%2520a%2520downstream%2520model.%2520The%2520codebase%2520for%2520our%250Aproject%2520is%2520available%2520at%2520https%253A//github.com/healthylaife/FairSynth%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness-Optimized%20Synthetic%20EHR%20Generation%20for%20Arbitrary%20Downstream%0A%20%20Predictive%20Tasks&entry.906535625=Mirza%20Farhan%20Bin%20Tarek%20and%20Raphael%20Poulain%20and%20Rahmatollah%20Beheshti&entry.1292438233=%20%20Among%20various%20aspects%20of%20ensuring%20the%20responsible%20design%20of%20AI%20tools%20for%0Ahealthcare%20applications%2C%20addressing%20fairness%20concerns%20has%20been%20a%20key%20focus%0Aarea.%20Specifically%2C%20given%20the%20wide%20spread%20of%20electronic%20health%20record%20%28EHR%29%0Adata%20and%20their%20huge%20potential%20to%20inform%20a%20wide%20range%20of%20clinical%20decision%0Asupport%20tasks%2C%20improving%20fairness%20in%20this%20category%20of%20health%20AI%20tools%20is%20of%20key%0Aimportance.%20While%20such%20a%20broad%20problem%20%28that%20is%2C%20mitigating%20fairness%20in%0AEHR-based%20AI%20models%29%20has%20been%20tackled%20using%20various%20methods%2C%20task-%20and%0Amodel-agnostic%20methods%20are%20noticeably%20rare.%20In%20this%20study%2C%20we%20aimed%20to%20target%0Athis%20gap%20by%20presenting%20a%20new%20pipeline%20that%20generates%20synthetic%20EHR%20data%2C%20which%0Ais%20not%20only%20consistent%20with%20%28faithful%20to%29%20the%20real%20EHR%20data%20but%20also%20can%20reduce%0Athe%20fairness%20concerns%20%28defined%20by%20the%20end-user%29%20in%20the%20downstream%20tasks%2C%20when%0Acombined%20with%20the%20real%20data.%20We%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Apipeline%20across%20various%20downstream%20tasks%20and%20two%20different%20EHR%20datasets.%20Our%0Aproposed%20pipeline%20can%20add%20a%20widely%20applicable%20and%20complementary%20tool%20to%20the%0Aexisting%20toolbox%20of%20methods%20to%20address%20fairness%20in%20health%20AI%20applications%20such%0Aas%20those%20modifying%20the%20design%20of%20a%20downstream%20model.%20The%20codebase%20for%20our%0Aproject%20is%20available%20at%20https%3A//github.com/healthylaife/FairSynth%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02510v1&entry.124074799=Read"},
{"title": "Advancing Unsupervised Low-light Image Enhancement: Noise Estimation,\n  Illumination Interpolation, and Self-Regulation", "author": "Xiaofeng Liu and Jiaxin Gao and Xin Fan and Risheng Liu", "abstract": "  Contemporary Low-Light Image Enhancement (LLIE) techniques have made notable\nadvancements in preserving image details and enhancing contrast, achieving\ncommendable results on specific datasets. Nevertheless, these approaches\nencounter persistent challenges in efficiently mitigating dynamic noise and\naccommodating diverse low-light scenarios. Insufficient constraints on complex\npixel-wise mapping learning lead to overfitting to specific types of noise and\nartifacts associated with low-light conditions, reducing effectiveness in\nvariable lighting scenarios. To this end, we first propose a method for\nestimating the noise level in low light images in a quick and accurate way.\nThis facilitates precise denoising, prevents over-smoothing, and adapts to\ndynamic noise patterns. Subsequently, we devise a Learnable Illumination\nInterpolator (LII), which employs learnlable interpolation operations between\nthe input and unit vector to satisfy general constraints between illumination\nand input. Finally, we introduce a self-regularization loss that incorporates\nintrinsic image properties and essential visual attributes to guide the output\ntowards meeting human visual expectations. Comprehensive experiments validate\nthe competitiveness of our proposed algorithm in both qualitative and\nquantitative assessments. Notably, our noise estimation method, with linear\ntime complexity and suitable for various denoisers, significantly improves both\ndenoising and enhancement performance. Benefiting from this, our approach\nachieves a 0.675dB PSNR improvement on the LOL dataset and 0.818dB on the MIT\ndataset on LLIE task, even compared to supervised methods. The source code is\navailable at \\href{https://doi.org/10.5281/zenodo.11463142}{this DOI\nrepository} and the specific code for noise estimation can be found at\n\\href{https://github.com/GoogolplexGoodenough/noise_estimate}{this separate\nGitHub link}.\n", "link": "http://arxiv.org/abs/2305.10223v4", "date": "2024-06-04", "relevancy": 2.2985, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6052}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.581}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Unsupervised%20Low-light%20Image%20Enhancement%3A%20Noise%20Estimation%2C%0A%20%20Illumination%20Interpolation%2C%20and%20Self-Regulation&body=Title%3A%20Advancing%20Unsupervised%20Low-light%20Image%20Enhancement%3A%20Noise%20Estimation%2C%0A%20%20Illumination%20Interpolation%2C%20and%20Self-Regulation%0AAuthor%3A%20Xiaofeng%20Liu%20and%20Jiaxin%20Gao%20and%20Xin%20Fan%20and%20Risheng%20Liu%0AAbstract%3A%20%20%20Contemporary%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20techniques%20have%20made%20notable%0Aadvancements%20in%20preserving%20image%20details%20and%20enhancing%20contrast%2C%20achieving%0Acommendable%20results%20on%20specific%20datasets.%20Nevertheless%2C%20these%20approaches%0Aencounter%20persistent%20challenges%20in%20efficiently%20mitigating%20dynamic%20noise%20and%0Aaccommodating%20diverse%20low-light%20scenarios.%20Insufficient%20constraints%20on%20complex%0Apixel-wise%20mapping%20learning%20lead%20to%20overfitting%20to%20specific%20types%20of%20noise%20and%0Aartifacts%20associated%20with%20low-light%20conditions%2C%20reducing%20effectiveness%20in%0Avariable%20lighting%20scenarios.%20To%20this%20end%2C%20we%20first%20propose%20a%20method%20for%0Aestimating%20the%20noise%20level%20in%20low%20light%20images%20in%20a%20quick%20and%20accurate%20way.%0AThis%20facilitates%20precise%20denoising%2C%20prevents%20over-smoothing%2C%20and%20adapts%20to%0Adynamic%20noise%20patterns.%20Subsequently%2C%20we%20devise%20a%20Learnable%20Illumination%0AInterpolator%20%28LII%29%2C%20which%20employs%20learnlable%20interpolation%20operations%20between%0Athe%20input%20and%20unit%20vector%20to%20satisfy%20general%20constraints%20between%20illumination%0Aand%20input.%20Finally%2C%20we%20introduce%20a%20self-regularization%20loss%20that%20incorporates%0Aintrinsic%20image%20properties%20and%20essential%20visual%20attributes%20to%20guide%20the%20output%0Atowards%20meeting%20human%20visual%20expectations.%20Comprehensive%20experiments%20validate%0Athe%20competitiveness%20of%20our%20proposed%20algorithm%20in%20both%20qualitative%20and%0Aquantitative%20assessments.%20Notably%2C%20our%20noise%20estimation%20method%2C%20with%20linear%0Atime%20complexity%20and%20suitable%20for%20various%20denoisers%2C%20significantly%20improves%20both%0Adenoising%20and%20enhancement%20performance.%20Benefiting%20from%20this%2C%20our%20approach%0Aachieves%20a%200.675dB%20PSNR%20improvement%20on%20the%20LOL%20dataset%20and%200.818dB%20on%20the%20MIT%0Adataset%20on%20LLIE%20task%2C%20even%20compared%20to%20supervised%20methods.%20The%20source%20code%20is%0Aavailable%20at%20%5Chref%7Bhttps%3A//doi.org/10.5281/zenodo.11463142%7D%7Bthis%20DOI%0Arepository%7D%20and%20the%20specific%20code%20for%20noise%20estimation%20can%20be%20found%20at%0A%5Chref%7Bhttps%3A//github.com/GoogolplexGoodenough/noise_estimate%7D%7Bthis%20separate%0AGitHub%20link%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10223v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Unsupervised%2520Low-light%2520Image%2520Enhancement%253A%2520Noise%2520Estimation%252C%250A%2520%2520Illumination%2520Interpolation%252C%2520and%2520Self-Regulation%26entry.906535625%3DXiaofeng%2520Liu%2520and%2520Jiaxin%2520Gao%2520and%2520Xin%2520Fan%2520and%2520Risheng%2520Liu%26entry.1292438233%3D%2520%2520Contemporary%2520Low-Light%2520Image%2520Enhancement%2520%2528LLIE%2529%2520techniques%2520have%2520made%2520notable%250Aadvancements%2520in%2520preserving%2520image%2520details%2520and%2520enhancing%2520contrast%252C%2520achieving%250Acommendable%2520results%2520on%2520specific%2520datasets.%2520Nevertheless%252C%2520these%2520approaches%250Aencounter%2520persistent%2520challenges%2520in%2520efficiently%2520mitigating%2520dynamic%2520noise%2520and%250Aaccommodating%2520diverse%2520low-light%2520scenarios.%2520Insufficient%2520constraints%2520on%2520complex%250Apixel-wise%2520mapping%2520learning%2520lead%2520to%2520overfitting%2520to%2520specific%2520types%2520of%2520noise%2520and%250Aartifacts%2520associated%2520with%2520low-light%2520conditions%252C%2520reducing%2520effectiveness%2520in%250Avariable%2520lighting%2520scenarios.%2520To%2520this%2520end%252C%2520we%2520first%2520propose%2520a%2520method%2520for%250Aestimating%2520the%2520noise%2520level%2520in%2520low%2520light%2520images%2520in%2520a%2520quick%2520and%2520accurate%2520way.%250AThis%2520facilitates%2520precise%2520denoising%252C%2520prevents%2520over-smoothing%252C%2520and%2520adapts%2520to%250Adynamic%2520noise%2520patterns.%2520Subsequently%252C%2520we%2520devise%2520a%2520Learnable%2520Illumination%250AInterpolator%2520%2528LII%2529%252C%2520which%2520employs%2520learnlable%2520interpolation%2520operations%2520between%250Athe%2520input%2520and%2520unit%2520vector%2520to%2520satisfy%2520general%2520constraints%2520between%2520illumination%250Aand%2520input.%2520Finally%252C%2520we%2520introduce%2520a%2520self-regularization%2520loss%2520that%2520incorporates%250Aintrinsic%2520image%2520properties%2520and%2520essential%2520visual%2520attributes%2520to%2520guide%2520the%2520output%250Atowards%2520meeting%2520human%2520visual%2520expectations.%2520Comprehensive%2520experiments%2520validate%250Athe%2520competitiveness%2520of%2520our%2520proposed%2520algorithm%2520in%2520both%2520qualitative%2520and%250Aquantitative%2520assessments.%2520Notably%252C%2520our%2520noise%2520estimation%2520method%252C%2520with%2520linear%250Atime%2520complexity%2520and%2520suitable%2520for%2520various%2520denoisers%252C%2520significantly%2520improves%2520both%250Adenoising%2520and%2520enhancement%2520performance.%2520Benefiting%2520from%2520this%252C%2520our%2520approach%250Aachieves%2520a%25200.675dB%2520PSNR%2520improvement%2520on%2520the%2520LOL%2520dataset%2520and%25200.818dB%2520on%2520the%2520MIT%250Adataset%2520on%2520LLIE%2520task%252C%2520even%2520compared%2520to%2520supervised%2520methods.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520%255Chref%257Bhttps%253A//doi.org/10.5281/zenodo.11463142%257D%257Bthis%2520DOI%250Arepository%257D%2520and%2520the%2520specific%2520code%2520for%2520noise%2520estimation%2520can%2520be%2520found%2520at%250A%255Chref%257Bhttps%253A//github.com/GoogolplexGoodenough/noise_estimate%257D%257Bthis%2520separate%250AGitHub%2520link%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.10223v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Unsupervised%20Low-light%20Image%20Enhancement%3A%20Noise%20Estimation%2C%0A%20%20Illumination%20Interpolation%2C%20and%20Self-Regulation&entry.906535625=Xiaofeng%20Liu%20and%20Jiaxin%20Gao%20and%20Xin%20Fan%20and%20Risheng%20Liu&entry.1292438233=%20%20Contemporary%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20techniques%20have%20made%20notable%0Aadvancements%20in%20preserving%20image%20details%20and%20enhancing%20contrast%2C%20achieving%0Acommendable%20results%20on%20specific%20datasets.%20Nevertheless%2C%20these%20approaches%0Aencounter%20persistent%20challenges%20in%20efficiently%20mitigating%20dynamic%20noise%20and%0Aaccommodating%20diverse%20low-light%20scenarios.%20Insufficient%20constraints%20on%20complex%0Apixel-wise%20mapping%20learning%20lead%20to%20overfitting%20to%20specific%20types%20of%20noise%20and%0Aartifacts%20associated%20with%20low-light%20conditions%2C%20reducing%20effectiveness%20in%0Avariable%20lighting%20scenarios.%20To%20this%20end%2C%20we%20first%20propose%20a%20method%20for%0Aestimating%20the%20noise%20level%20in%20low%20light%20images%20in%20a%20quick%20and%20accurate%20way.%0AThis%20facilitates%20precise%20denoising%2C%20prevents%20over-smoothing%2C%20and%20adapts%20to%0Adynamic%20noise%20patterns.%20Subsequently%2C%20we%20devise%20a%20Learnable%20Illumination%0AInterpolator%20%28LII%29%2C%20which%20employs%20learnlable%20interpolation%20operations%20between%0Athe%20input%20and%20unit%20vector%20to%20satisfy%20general%20constraints%20between%20illumination%0Aand%20input.%20Finally%2C%20we%20introduce%20a%20self-regularization%20loss%20that%20incorporates%0Aintrinsic%20image%20properties%20and%20essential%20visual%20attributes%20to%20guide%20the%20output%0Atowards%20meeting%20human%20visual%20expectations.%20Comprehensive%20experiments%20validate%0Athe%20competitiveness%20of%20our%20proposed%20algorithm%20in%20both%20qualitative%20and%0Aquantitative%20assessments.%20Notably%2C%20our%20noise%20estimation%20method%2C%20with%20linear%0Atime%20complexity%20and%20suitable%20for%20various%20denoisers%2C%20significantly%20improves%20both%0Adenoising%20and%20enhancement%20performance.%20Benefiting%20from%20this%2C%20our%20approach%0Aachieves%20a%200.675dB%20PSNR%20improvement%20on%20the%20LOL%20dataset%20and%200.818dB%20on%20the%20MIT%0Adataset%20on%20LLIE%20task%2C%20even%20compared%20to%20supervised%20methods.%20The%20source%20code%20is%0Aavailable%20at%20%5Chref%7Bhttps%3A//doi.org/10.5281/zenodo.11463142%7D%7Bthis%20DOI%0Arepository%7D%20and%20the%20specific%20code%20for%20noise%20estimation%20can%20be%20found%20at%0A%5Chref%7Bhttps%3A//github.com/GoogolplexGoodenough/noise_estimate%7D%7Bthis%20separate%0AGitHub%20link%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10223v4&entry.124074799=Read"},
{"title": "An Open-Source Tool for Mapping War Destruction at Scale in Ukraine\n  using Sentinel-1 Time Series", "author": "Olivier Dietrich and Torben Peters and Vivien Sainte Fare Garnot and Valerie Sticher and Thao Ton-That Whelan and Konrad Schindler and Jan Dirk Wegner", "abstract": "  Access to detailed war impact assessments is crucial for humanitarian\norganizations to effectively assist populations most affected by armed\nconflicts. However, maintaining a comprehensive understanding of the situation\non the ground is challenging, especially in conflicts that cover vast\nterritories and extend over long periods. This study presents a scalable and\ntransferable method for estimating war-induced damage to buildings. We first\ntrain a machine learning model to output pixel-wise probability of destruction\nfrom Synthetic Aperture Radar (SAR) satellite image time series, leveraging\nexisting, manual damage assessments as ground truth and cloud-based geospatial\nanalysis tools for large-scale inference. We further post-process these\nassessments using open building footprints to obtain a final damage estimate\nper building. We introduce an accessible, open-source tool that allows users to\nadjust the confidence interval based on their specific requirements and use\ncases. Our approach enables humanitarian organizations and other actors to\nrapidly screen large geographic regions for war impacts. We provide two\npublicly accessible dashboards: a Ukraine Damage Explorer to dynamically view\nour pre-computed estimates, and a Rapid Damage Mapping Tool to easily run our\nmethod and produce custom maps.\n", "link": "http://arxiv.org/abs/2406.02506v1", "date": "2024-06-04", "relevancy": 2.2853, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4922}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4408}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Open-Source%20Tool%20for%20Mapping%20War%20Destruction%20at%20Scale%20in%20Ukraine%0A%20%20using%20Sentinel-1%20Time%20Series&body=Title%3A%20An%20Open-Source%20Tool%20for%20Mapping%20War%20Destruction%20at%20Scale%20in%20Ukraine%0A%20%20using%20Sentinel-1%20Time%20Series%0AAuthor%3A%20Olivier%20Dietrich%20and%20Torben%20Peters%20and%20Vivien%20Sainte%20Fare%20Garnot%20and%20Valerie%20Sticher%20and%20Thao%20Ton-That%20Whelan%20and%20Konrad%20Schindler%20and%20Jan%20Dirk%20Wegner%0AAbstract%3A%20%20%20Access%20to%20detailed%20war%20impact%20assessments%20is%20crucial%20for%20humanitarian%0Aorganizations%20to%20effectively%20assist%20populations%20most%20affected%20by%20armed%0Aconflicts.%20However%2C%20maintaining%20a%20comprehensive%20understanding%20of%20the%20situation%0Aon%20the%20ground%20is%20challenging%2C%20especially%20in%20conflicts%20that%20cover%20vast%0Aterritories%20and%20extend%20over%20long%20periods.%20This%20study%20presents%20a%20scalable%20and%0Atransferable%20method%20for%20estimating%20war-induced%20damage%20to%20buildings.%20We%20first%0Atrain%20a%20machine%20learning%20model%20to%20output%20pixel-wise%20probability%20of%20destruction%0Afrom%20Synthetic%20Aperture%20Radar%20%28SAR%29%20satellite%20image%20time%20series%2C%20leveraging%0Aexisting%2C%20manual%20damage%20assessments%20as%20ground%20truth%20and%20cloud-based%20geospatial%0Aanalysis%20tools%20for%20large-scale%20inference.%20We%20further%20post-process%20these%0Aassessments%20using%20open%20building%20footprints%20to%20obtain%20a%20final%20damage%20estimate%0Aper%20building.%20We%20introduce%20an%20accessible%2C%20open-source%20tool%20that%20allows%20users%20to%0Aadjust%20the%20confidence%20interval%20based%20on%20their%20specific%20requirements%20and%20use%0Acases.%20Our%20approach%20enables%20humanitarian%20organizations%20and%20other%20actors%20to%0Arapidly%20screen%20large%20geographic%20regions%20for%20war%20impacts.%20We%20provide%20two%0Apublicly%20accessible%20dashboards%3A%20a%20Ukraine%20Damage%20Explorer%20to%20dynamically%20view%0Aour%20pre-computed%20estimates%2C%20and%20a%20Rapid%20Damage%20Mapping%20Tool%20to%20easily%20run%20our%0Amethod%20and%20produce%20custom%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Open-Source%2520Tool%2520for%2520Mapping%2520War%2520Destruction%2520at%2520Scale%2520in%2520Ukraine%250A%2520%2520using%2520Sentinel-1%2520Time%2520Series%26entry.906535625%3DOlivier%2520Dietrich%2520and%2520Torben%2520Peters%2520and%2520Vivien%2520Sainte%2520Fare%2520Garnot%2520and%2520Valerie%2520Sticher%2520and%2520Thao%2520Ton-That%2520Whelan%2520and%2520Konrad%2520Schindler%2520and%2520Jan%2520Dirk%2520Wegner%26entry.1292438233%3D%2520%2520Access%2520to%2520detailed%2520war%2520impact%2520assessments%2520is%2520crucial%2520for%2520humanitarian%250Aorganizations%2520to%2520effectively%2520assist%2520populations%2520most%2520affected%2520by%2520armed%250Aconflicts.%2520However%252C%2520maintaining%2520a%2520comprehensive%2520understanding%2520of%2520the%2520situation%250Aon%2520the%2520ground%2520is%2520challenging%252C%2520especially%2520in%2520conflicts%2520that%2520cover%2520vast%250Aterritories%2520and%2520extend%2520over%2520long%2520periods.%2520This%2520study%2520presents%2520a%2520scalable%2520and%250Atransferable%2520method%2520for%2520estimating%2520war-induced%2520damage%2520to%2520buildings.%2520We%2520first%250Atrain%2520a%2520machine%2520learning%2520model%2520to%2520output%2520pixel-wise%2520probability%2520of%2520destruction%250Afrom%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520satellite%2520image%2520time%2520series%252C%2520leveraging%250Aexisting%252C%2520manual%2520damage%2520assessments%2520as%2520ground%2520truth%2520and%2520cloud-based%2520geospatial%250Aanalysis%2520tools%2520for%2520large-scale%2520inference.%2520We%2520further%2520post-process%2520these%250Aassessments%2520using%2520open%2520building%2520footprints%2520to%2520obtain%2520a%2520final%2520damage%2520estimate%250Aper%2520building.%2520We%2520introduce%2520an%2520accessible%252C%2520open-source%2520tool%2520that%2520allows%2520users%2520to%250Aadjust%2520the%2520confidence%2520interval%2520based%2520on%2520their%2520specific%2520requirements%2520and%2520use%250Acases.%2520Our%2520approach%2520enables%2520humanitarian%2520organizations%2520and%2520other%2520actors%2520to%250Arapidly%2520screen%2520large%2520geographic%2520regions%2520for%2520war%2520impacts.%2520We%2520provide%2520two%250Apublicly%2520accessible%2520dashboards%253A%2520a%2520Ukraine%2520Damage%2520Explorer%2520to%2520dynamically%2520view%250Aour%2520pre-computed%2520estimates%252C%2520and%2520a%2520Rapid%2520Damage%2520Mapping%2520Tool%2520to%2520easily%2520run%2520our%250Amethod%2520and%2520produce%2520custom%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Open-Source%20Tool%20for%20Mapping%20War%20Destruction%20at%20Scale%20in%20Ukraine%0A%20%20using%20Sentinel-1%20Time%20Series&entry.906535625=Olivier%20Dietrich%20and%20Torben%20Peters%20and%20Vivien%20Sainte%20Fare%20Garnot%20and%20Valerie%20Sticher%20and%20Thao%20Ton-That%20Whelan%20and%20Konrad%20Schindler%20and%20Jan%20Dirk%20Wegner&entry.1292438233=%20%20Access%20to%20detailed%20war%20impact%20assessments%20is%20crucial%20for%20humanitarian%0Aorganizations%20to%20effectively%20assist%20populations%20most%20affected%20by%20armed%0Aconflicts.%20However%2C%20maintaining%20a%20comprehensive%20understanding%20of%20the%20situation%0Aon%20the%20ground%20is%20challenging%2C%20especially%20in%20conflicts%20that%20cover%20vast%0Aterritories%20and%20extend%20over%20long%20periods.%20This%20study%20presents%20a%20scalable%20and%0Atransferable%20method%20for%20estimating%20war-induced%20damage%20to%20buildings.%20We%20first%0Atrain%20a%20machine%20learning%20model%20to%20output%20pixel-wise%20probability%20of%20destruction%0Afrom%20Synthetic%20Aperture%20Radar%20%28SAR%29%20satellite%20image%20time%20series%2C%20leveraging%0Aexisting%2C%20manual%20damage%20assessments%20as%20ground%20truth%20and%20cloud-based%20geospatial%0Aanalysis%20tools%20for%20large-scale%20inference.%20We%20further%20post-process%20these%0Aassessments%20using%20open%20building%20footprints%20to%20obtain%20a%20final%20damage%20estimate%0Aper%20building.%20We%20introduce%20an%20accessible%2C%20open-source%20tool%20that%20allows%20users%20to%0Aadjust%20the%20confidence%20interval%20based%20on%20their%20specific%20requirements%20and%20use%0Acases.%20Our%20approach%20enables%20humanitarian%20organizations%20and%20other%20actors%20to%0Arapidly%20screen%20large%20geographic%20regions%20for%20war%20impacts.%20We%20provide%20two%0Apublicly%20accessible%20dashboards%3A%20a%20Ukraine%20Damage%20Explorer%20to%20dynamically%20view%0Aour%20pre-computed%20estimates%2C%20and%20a%20Rapid%20Damage%20Mapping%20Tool%20to%20easily%20run%20our%0Amethod%20and%20produce%20custom%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02506v1&entry.124074799=Read"},
{"title": "On Affine Homotopy between Language Encoders", "author": "Robin SM Chan and Reda Boumasmoud and Anej Svete and Yuxin Ren and Qipeng Guo and Zhijing Jin and Shauli Ravfogel and Mrinmaya Sachan and Bernhard Sch\u00f6lkopf and Mennatallah El-Assady and Ryan Cotterell", "abstract": "  Pre-trained language encoders -- functions that represent text as vectors --\nare an integral component of many NLP tasks. We tackle a natural question in\nlanguage encoder analysis: What does it mean for two encoders to be similar? We\ncontend that a faithful measure of similarity needs to be \\emph{intrinsic},\nthat is, task-independent, yet still be informative of \\emph{extrinsic}\nsimilarity -- the performance on downstream tasks. It is common to consider two\nencoders similar if they are \\emph{homotopic}, i.e., if they can be aligned\nthrough some transformation. In this spirit, we study the properties of\n\\emph{affine} alignment of language encoders and its implications on extrinsic\nsimilarity. We find that while affine alignment is fundamentally an asymmetric\nnotion of similarity, it is still informative of extrinsic similarity. We\nconfirm this on datasets of natural language representations. Beyond providing\nuseful bounds on extrinsic similarity, affine intrinsic similarity also allows\nus to begin uncovering the structure of the space of pre-trained encoders by\ndefining an order over them.\n", "link": "http://arxiv.org/abs/2406.02329v1", "date": "2024-06-04", "relevancy": 2.2803, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4789}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4623}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Affine%20Homotopy%20between%20Language%20Encoders&body=Title%3A%20On%20Affine%20Homotopy%20between%20Language%20Encoders%0AAuthor%3A%20Robin%20SM%20Chan%20and%20Reda%20Boumasmoud%20and%20Anej%20Svete%20and%20Yuxin%20Ren%20and%20Qipeng%20Guo%20and%20Zhijing%20Jin%20and%20Shauli%20Ravfogel%20and%20Mrinmaya%20Sachan%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Mennatallah%20El-Assady%20and%20Ryan%20Cotterell%0AAbstract%3A%20%20%20Pre-trained%20language%20encoders%20--%20functions%20that%20represent%20text%20as%20vectors%20--%0Aare%20an%20integral%20component%20of%20many%20NLP%20tasks.%20We%20tackle%20a%20natural%20question%20in%0Alanguage%20encoder%20analysis%3A%20What%20does%20it%20mean%20for%20two%20encoders%20to%20be%20similar%3F%20We%0Acontend%20that%20a%20faithful%20measure%20of%20similarity%20needs%20to%20be%20%5Cemph%7Bintrinsic%7D%2C%0Athat%20is%2C%20task-independent%2C%20yet%20still%20be%20informative%20of%20%5Cemph%7Bextrinsic%7D%0Asimilarity%20--%20the%20performance%20on%20downstream%20tasks.%20It%20is%20common%20to%20consider%20two%0Aencoders%20similar%20if%20they%20are%20%5Cemph%7Bhomotopic%7D%2C%20i.e.%2C%20if%20they%20can%20be%20aligned%0Athrough%20some%20transformation.%20In%20this%20spirit%2C%20we%20study%20the%20properties%20of%0A%5Cemph%7Baffine%7D%20alignment%20of%20language%20encoders%20and%20its%20implications%20on%20extrinsic%0Asimilarity.%20We%20find%20that%20while%20affine%20alignment%20is%20fundamentally%20an%20asymmetric%0Anotion%20of%20similarity%2C%20it%20is%20still%20informative%20of%20extrinsic%20similarity.%20We%0Aconfirm%20this%20on%20datasets%20of%20natural%20language%20representations.%20Beyond%20providing%0Auseful%20bounds%20on%20extrinsic%20similarity%2C%20affine%20intrinsic%20similarity%20also%20allows%0Aus%20to%20begin%20uncovering%20the%20structure%20of%20the%20space%20of%20pre-trained%20encoders%20by%0Adefining%20an%20order%20over%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Affine%2520Homotopy%2520between%2520Language%2520Encoders%26entry.906535625%3DRobin%2520SM%2520Chan%2520and%2520Reda%2520Boumasmoud%2520and%2520Anej%2520Svete%2520and%2520Yuxin%2520Ren%2520and%2520Qipeng%2520Guo%2520and%2520Zhijing%2520Jin%2520and%2520Shauli%2520Ravfogel%2520and%2520Mrinmaya%2520Sachan%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Mennatallah%2520El-Assady%2520and%2520Ryan%2520Cotterell%26entry.1292438233%3D%2520%2520Pre-trained%2520language%2520encoders%2520--%2520functions%2520that%2520represent%2520text%2520as%2520vectors%2520--%250Aare%2520an%2520integral%2520component%2520of%2520many%2520NLP%2520tasks.%2520We%2520tackle%2520a%2520natural%2520question%2520in%250Alanguage%2520encoder%2520analysis%253A%2520What%2520does%2520it%2520mean%2520for%2520two%2520encoders%2520to%2520be%2520similar%253F%2520We%250Acontend%2520that%2520a%2520faithful%2520measure%2520of%2520similarity%2520needs%2520to%2520be%2520%255Cemph%257Bintrinsic%257D%252C%250Athat%2520is%252C%2520task-independent%252C%2520yet%2520still%2520be%2520informative%2520of%2520%255Cemph%257Bextrinsic%257D%250Asimilarity%2520--%2520the%2520performance%2520on%2520downstream%2520tasks.%2520It%2520is%2520common%2520to%2520consider%2520two%250Aencoders%2520similar%2520if%2520they%2520are%2520%255Cemph%257Bhomotopic%257D%252C%2520i.e.%252C%2520if%2520they%2520can%2520be%2520aligned%250Athrough%2520some%2520transformation.%2520In%2520this%2520spirit%252C%2520we%2520study%2520the%2520properties%2520of%250A%255Cemph%257Baffine%257D%2520alignment%2520of%2520language%2520encoders%2520and%2520its%2520implications%2520on%2520extrinsic%250Asimilarity.%2520We%2520find%2520that%2520while%2520affine%2520alignment%2520is%2520fundamentally%2520an%2520asymmetric%250Anotion%2520of%2520similarity%252C%2520it%2520is%2520still%2520informative%2520of%2520extrinsic%2520similarity.%2520We%250Aconfirm%2520this%2520on%2520datasets%2520of%2520natural%2520language%2520representations.%2520Beyond%2520providing%250Auseful%2520bounds%2520on%2520extrinsic%2520similarity%252C%2520affine%2520intrinsic%2520similarity%2520also%2520allows%250Aus%2520to%2520begin%2520uncovering%2520the%2520structure%2520of%2520the%2520space%2520of%2520pre-trained%2520encoders%2520by%250Adefining%2520an%2520order%2520over%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Affine%20Homotopy%20between%20Language%20Encoders&entry.906535625=Robin%20SM%20Chan%20and%20Reda%20Boumasmoud%20and%20Anej%20Svete%20and%20Yuxin%20Ren%20and%20Qipeng%20Guo%20and%20Zhijing%20Jin%20and%20Shauli%20Ravfogel%20and%20Mrinmaya%20Sachan%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Mennatallah%20El-Assady%20and%20Ryan%20Cotterell&entry.1292438233=%20%20Pre-trained%20language%20encoders%20--%20functions%20that%20represent%20text%20as%20vectors%20--%0Aare%20an%20integral%20component%20of%20many%20NLP%20tasks.%20We%20tackle%20a%20natural%20question%20in%0Alanguage%20encoder%20analysis%3A%20What%20does%20it%20mean%20for%20two%20encoders%20to%20be%20similar%3F%20We%0Acontend%20that%20a%20faithful%20measure%20of%20similarity%20needs%20to%20be%20%5Cemph%7Bintrinsic%7D%2C%0Athat%20is%2C%20task-independent%2C%20yet%20still%20be%20informative%20of%20%5Cemph%7Bextrinsic%7D%0Asimilarity%20--%20the%20performance%20on%20downstream%20tasks.%20It%20is%20common%20to%20consider%20two%0Aencoders%20similar%20if%20they%20are%20%5Cemph%7Bhomotopic%7D%2C%20i.e.%2C%20if%20they%20can%20be%20aligned%0Athrough%20some%20transformation.%20In%20this%20spirit%2C%20we%20study%20the%20properties%20of%0A%5Cemph%7Baffine%7D%20alignment%20of%20language%20encoders%20and%20its%20implications%20on%20extrinsic%0Asimilarity.%20We%20find%20that%20while%20affine%20alignment%20is%20fundamentally%20an%20asymmetric%0Anotion%20of%20similarity%2C%20it%20is%20still%20informative%20of%20extrinsic%20similarity.%20We%0Aconfirm%20this%20on%20datasets%20of%20natural%20language%20representations.%20Beyond%20providing%0Auseful%20bounds%20on%20extrinsic%20similarity%2C%20affine%20intrinsic%20similarity%20also%20allows%0Aus%20to%20begin%20uncovering%20the%20structure%20of%20the%20space%20of%20pre-trained%20encoders%20by%0Adefining%20an%20order%20over%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02329v1&entry.124074799=Read"},
{"title": "Branches: A Fast Dynamic Programming and Branch & Bound Algorithm for\n  Optimal Decision Trees", "author": "Ayman Chaouki and Jesse Read and Albert Bifet", "abstract": "  Decision Tree Learning is a fundamental problem for Interpretable Machine\nLearning, yet it poses a formidable optimization challenge. Despite numerous\nefforts dating back to the early 1990's, practical algorithms have only\nrecently emerged, primarily leveraging Dynamic Programming (DP) and Branch &\nBound (B&B) techniques. These breakthroughs led to the development of two\ndistinct approaches. Algorithms like DL8.5 and MurTree operate on the space of\nnodes (or branches), they are very fast, but do not penalise complex Decision\nTrees, i.e. they do not solve for sparsity. On the other hand, algorithms like\nOSDT and GOSDT operate on the space of Decision Trees, they solve for sparsity\nbut at the detriment of speed. In this work, we introduce Branches, a novel\nalgorithm that integrates the strengths of both paradigms. Leveraging DP and\nB&B, Branches achieves exceptional speed while also solving for sparsity.\nCentral to its efficiency is a novel analytical bound enabling substantial\npruning of the search space. Theoretical analysis demonstrates that Branches\nhas lower complexity compared to state-of-the-art methods, a claim validated\nthrough extensive empirical evaluation. Our results illustrate that Branches\nnot only greatly outperforms existing approaches in terms of speed and number\nof iterations, it also consistently yields optimal Decision Trees.\n", "link": "http://arxiv.org/abs/2406.02175v1", "date": "2024-06-04", "relevancy": 2.2733, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5027}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4337}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees&body=Title%3A%20Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees%0AAuthor%3A%20Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet%0AAbstract%3A%20%20%20Decision%20Tree%20Learning%20is%20a%20fundamental%20problem%20for%20Interpretable%20Machine%0ALearning%2C%20yet%20it%20poses%20a%20formidable%20optimization%20challenge.%20Despite%20numerous%0Aefforts%20dating%20back%20to%20the%20early%201990%27s%2C%20practical%20algorithms%20have%20only%0Arecently%20emerged%2C%20primarily%20leveraging%20Dynamic%20Programming%20%28DP%29%20and%20Branch%20%26%0ABound%20%28B%26B%29%20techniques.%20These%20breakthroughs%20led%20to%20the%20development%20of%20two%0Adistinct%20approaches.%20Algorithms%20like%20DL8.5%20and%20MurTree%20operate%20on%20the%20space%20of%0Anodes%20%28or%20branches%29%2C%20they%20are%20very%20fast%2C%20but%20do%20not%20penalise%20complex%20Decision%0ATrees%2C%20i.e.%20they%20do%20not%20solve%20for%20sparsity.%20On%20the%20other%20hand%2C%20algorithms%20like%0AOSDT%20and%20GOSDT%20operate%20on%20the%20space%20of%20Decision%20Trees%2C%20they%20solve%20for%20sparsity%0Abut%20at%20the%20detriment%20of%20speed.%20In%20this%20work%2C%20we%20introduce%20Branches%2C%20a%20novel%0Aalgorithm%20that%20integrates%20the%20strengths%20of%20both%20paradigms.%20Leveraging%20DP%20and%0AB%26B%2C%20Branches%20achieves%20exceptional%20speed%20while%20also%20solving%20for%20sparsity.%0ACentral%20to%20its%20efficiency%20is%20a%20novel%20analytical%20bound%20enabling%20substantial%0Apruning%20of%20the%20search%20space.%20Theoretical%20analysis%20demonstrates%20that%20Branches%0Ahas%20lower%20complexity%20compared%20to%20state-of-the-art%20methods%2C%20a%20claim%20validated%0Athrough%20extensive%20empirical%20evaluation.%20Our%20results%20illustrate%20that%20Branches%0Anot%20only%20greatly%20outperforms%20existing%20approaches%20in%20terms%20of%20speed%20and%20number%0Aof%20iterations%2C%20it%20also%20consistently%20yields%20optimal%20Decision%20Trees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranches%253A%2520A%2520Fast%2520Dynamic%2520Programming%2520and%2520Branch%2520%2526%2520Bound%2520Algorithm%2520for%250A%2520%2520Optimal%2520Decision%2520Trees%26entry.906535625%3DAyman%2520Chaouki%2520and%2520Jesse%2520Read%2520and%2520Albert%2520Bifet%26entry.1292438233%3D%2520%2520Decision%2520Tree%2520Learning%2520is%2520a%2520fundamental%2520problem%2520for%2520Interpretable%2520Machine%250ALearning%252C%2520yet%2520it%2520poses%2520a%2520formidable%2520optimization%2520challenge.%2520Despite%2520numerous%250Aefforts%2520dating%2520back%2520to%2520the%2520early%25201990%2527s%252C%2520practical%2520algorithms%2520have%2520only%250Arecently%2520emerged%252C%2520primarily%2520leveraging%2520Dynamic%2520Programming%2520%2528DP%2529%2520and%2520Branch%2520%2526%250ABound%2520%2528B%2526B%2529%2520techniques.%2520These%2520breakthroughs%2520led%2520to%2520the%2520development%2520of%2520two%250Adistinct%2520approaches.%2520Algorithms%2520like%2520DL8.5%2520and%2520MurTree%2520operate%2520on%2520the%2520space%2520of%250Anodes%2520%2528or%2520branches%2529%252C%2520they%2520are%2520very%2520fast%252C%2520but%2520do%2520not%2520penalise%2520complex%2520Decision%250ATrees%252C%2520i.e.%2520they%2520do%2520not%2520solve%2520for%2520sparsity.%2520On%2520the%2520other%2520hand%252C%2520algorithms%2520like%250AOSDT%2520and%2520GOSDT%2520operate%2520on%2520the%2520space%2520of%2520Decision%2520Trees%252C%2520they%2520solve%2520for%2520sparsity%250Abut%2520at%2520the%2520detriment%2520of%2520speed.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Branches%252C%2520a%2520novel%250Aalgorithm%2520that%2520integrates%2520the%2520strengths%2520of%2520both%2520paradigms.%2520Leveraging%2520DP%2520and%250AB%2526B%252C%2520Branches%2520achieves%2520exceptional%2520speed%2520while%2520also%2520solving%2520for%2520sparsity.%250ACentral%2520to%2520its%2520efficiency%2520is%2520a%2520novel%2520analytical%2520bound%2520enabling%2520substantial%250Apruning%2520of%2520the%2520search%2520space.%2520Theoretical%2520analysis%2520demonstrates%2520that%2520Branches%250Ahas%2520lower%2520complexity%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520a%2520claim%2520validated%250Athrough%2520extensive%2520empirical%2520evaluation.%2520Our%2520results%2520illustrate%2520that%2520Branches%250Anot%2520only%2520greatly%2520outperforms%2520existing%2520approaches%2520in%2520terms%2520of%2520speed%2520and%2520number%250Aof%2520iterations%252C%2520it%2520also%2520consistently%2520yields%2520optimal%2520Decision%2520Trees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees&entry.906535625=Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet&entry.1292438233=%20%20Decision%20Tree%20Learning%20is%20a%20fundamental%20problem%20for%20Interpretable%20Machine%0ALearning%2C%20yet%20it%20poses%20a%20formidable%20optimization%20challenge.%20Despite%20numerous%0Aefforts%20dating%20back%20to%20the%20early%201990%27s%2C%20practical%20algorithms%20have%20only%0Arecently%20emerged%2C%20primarily%20leveraging%20Dynamic%20Programming%20%28DP%29%20and%20Branch%20%26%0ABound%20%28B%26B%29%20techniques.%20These%20breakthroughs%20led%20to%20the%20development%20of%20two%0Adistinct%20approaches.%20Algorithms%20like%20DL8.5%20and%20MurTree%20operate%20on%20the%20space%20of%0Anodes%20%28or%20branches%29%2C%20they%20are%20very%20fast%2C%20but%20do%20not%20penalise%20complex%20Decision%0ATrees%2C%20i.e.%20they%20do%20not%20solve%20for%20sparsity.%20On%20the%20other%20hand%2C%20algorithms%20like%0AOSDT%20and%20GOSDT%20operate%20on%20the%20space%20of%20Decision%20Trees%2C%20they%20solve%20for%20sparsity%0Abut%20at%20the%20detriment%20of%20speed.%20In%20this%20work%2C%20we%20introduce%20Branches%2C%20a%20novel%0Aalgorithm%20that%20integrates%20the%20strengths%20of%20both%20paradigms.%20Leveraging%20DP%20and%0AB%26B%2C%20Branches%20achieves%20exceptional%20speed%20while%20also%20solving%20for%20sparsity.%0ACentral%20to%20its%20efficiency%20is%20a%20novel%20analytical%20bound%20enabling%20substantial%0Apruning%20of%20the%20search%20space.%20Theoretical%20analysis%20demonstrates%20that%20Branches%0Ahas%20lower%20complexity%20compared%20to%20state-of-the-art%20methods%2C%20a%20claim%20validated%0Athrough%20extensive%20empirical%20evaluation.%20Our%20results%20illustrate%20that%20Branches%0Anot%20only%20greatly%20outperforms%20existing%20approaches%20in%20terms%20of%20speed%20and%20number%0Aof%20iterations%2C%20it%20also%20consistently%20yields%20optimal%20Decision%20Trees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02175v1&entry.124074799=Read"},
{"title": "Privacy Attacks in Decentralized Learning", "author": "Abdellah El Mrini and Edwige Cyffers and Aur\u00e9lien Bellet", "abstract": "  Decentralized Gradient Descent (D-GD) allows a set of users to perform\ncollaborative learning without sharing their data by iteratively averaging\nlocal model updates with their neighbors in a network graph. The absence of\ndirect communication between non-neighbor nodes might lead to the belief that\nusers cannot infer precise information about the data of others. In this work,\nwe demonstrate the opposite, by proposing the first attack against D-GD that\nenables a user (or set of users) to reconstruct the private data of other users\noutside their immediate neighborhood. Our approach is based on a reconstruction\nattack against the gossip averaging protocol, which we then extend to handle\nthe additional challenges raised by D-GD. We validate the effectiveness of our\nattack on real graphs and datasets, showing that the number of users\ncompromised by a single or a handful of attackers is often surprisingly large.\nWe empirically investigate some of the factors that affect the performance of\nthe attack, namely the graph topology, the number of attackers, and their\nposition in the graph.\n", "link": "http://arxiv.org/abs/2402.10001v2", "date": "2024-06-04", "relevancy": 2.2649, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4675}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4475}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy%20Attacks%20in%20Decentralized%20Learning&body=Title%3A%20Privacy%20Attacks%20in%20Decentralized%20Learning%0AAuthor%3A%20Abdellah%20El%20Mrini%20and%20Edwige%20Cyffers%20and%20Aur%C3%A9lien%20Bellet%0AAbstract%3A%20%20%20Decentralized%20Gradient%20Descent%20%28D-GD%29%20allows%20a%20set%20of%20users%20to%20perform%0Acollaborative%20learning%20without%20sharing%20their%20data%20by%20iteratively%20averaging%0Alocal%20model%20updates%20with%20their%20neighbors%20in%20a%20network%20graph.%20The%20absence%20of%0Adirect%20communication%20between%20non-neighbor%20nodes%20might%20lead%20to%20the%20belief%20that%0Ausers%20cannot%20infer%20precise%20information%20about%20the%20data%20of%20others.%20In%20this%20work%2C%0Awe%20demonstrate%20the%20opposite%2C%20by%20proposing%20the%20first%20attack%20against%20D-GD%20that%0Aenables%20a%20user%20%28or%20set%20of%20users%29%20to%20reconstruct%20the%20private%20data%20of%20other%20users%0Aoutside%20their%20immediate%20neighborhood.%20Our%20approach%20is%20based%20on%20a%20reconstruction%0Aattack%20against%20the%20gossip%20averaging%20protocol%2C%20which%20we%20then%20extend%20to%20handle%0Athe%20additional%20challenges%20raised%20by%20D-GD.%20We%20validate%20the%20effectiveness%20of%20our%0Aattack%20on%20real%20graphs%20and%20datasets%2C%20showing%20that%20the%20number%20of%20users%0Acompromised%20by%20a%20single%20or%20a%20handful%20of%20attackers%20is%20often%20surprisingly%20large.%0AWe%20empirically%20investigate%20some%20of%20the%20factors%20that%20affect%20the%20performance%20of%0Athe%20attack%2C%20namely%20the%20graph%20topology%2C%20the%20number%20of%20attackers%2C%20and%20their%0Aposition%20in%20the%20graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy%2520Attacks%2520in%2520Decentralized%2520Learning%26entry.906535625%3DAbdellah%2520El%2520Mrini%2520and%2520Edwige%2520Cyffers%2520and%2520Aur%25C3%25A9lien%2520Bellet%26entry.1292438233%3D%2520%2520Decentralized%2520Gradient%2520Descent%2520%2528D-GD%2529%2520allows%2520a%2520set%2520of%2520users%2520to%2520perform%250Acollaborative%2520learning%2520without%2520sharing%2520their%2520data%2520by%2520iteratively%2520averaging%250Alocal%2520model%2520updates%2520with%2520their%2520neighbors%2520in%2520a%2520network%2520graph.%2520The%2520absence%2520of%250Adirect%2520communication%2520between%2520non-neighbor%2520nodes%2520might%2520lead%2520to%2520the%2520belief%2520that%250Ausers%2520cannot%2520infer%2520precise%2520information%2520about%2520the%2520data%2520of%2520others.%2520In%2520this%2520work%252C%250Awe%2520demonstrate%2520the%2520opposite%252C%2520by%2520proposing%2520the%2520first%2520attack%2520against%2520D-GD%2520that%250Aenables%2520a%2520user%2520%2528or%2520set%2520of%2520users%2529%2520to%2520reconstruct%2520the%2520private%2520data%2520of%2520other%2520users%250Aoutside%2520their%2520immediate%2520neighborhood.%2520Our%2520approach%2520is%2520based%2520on%2520a%2520reconstruction%250Aattack%2520against%2520the%2520gossip%2520averaging%2520protocol%252C%2520which%2520we%2520then%2520extend%2520to%2520handle%250Athe%2520additional%2520challenges%2520raised%2520by%2520D-GD.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%250Aattack%2520on%2520real%2520graphs%2520and%2520datasets%252C%2520showing%2520that%2520the%2520number%2520of%2520users%250Acompromised%2520by%2520a%2520single%2520or%2520a%2520handful%2520of%2520attackers%2520is%2520often%2520surprisingly%2520large.%250AWe%2520empirically%2520investigate%2520some%2520of%2520the%2520factors%2520that%2520affect%2520the%2520performance%2520of%250Athe%2520attack%252C%2520namely%2520the%2520graph%2520topology%252C%2520the%2520number%2520of%2520attackers%252C%2520and%2520their%250Aposition%2520in%2520the%2520graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy%20Attacks%20in%20Decentralized%20Learning&entry.906535625=Abdellah%20El%20Mrini%20and%20Edwige%20Cyffers%20and%20Aur%C3%A9lien%20Bellet&entry.1292438233=%20%20Decentralized%20Gradient%20Descent%20%28D-GD%29%20allows%20a%20set%20of%20users%20to%20perform%0Acollaborative%20learning%20without%20sharing%20their%20data%20by%20iteratively%20averaging%0Alocal%20model%20updates%20with%20their%20neighbors%20in%20a%20network%20graph.%20The%20absence%20of%0Adirect%20communication%20between%20non-neighbor%20nodes%20might%20lead%20to%20the%20belief%20that%0Ausers%20cannot%20infer%20precise%20information%20about%20the%20data%20of%20others.%20In%20this%20work%2C%0Awe%20demonstrate%20the%20opposite%2C%20by%20proposing%20the%20first%20attack%20against%20D-GD%20that%0Aenables%20a%20user%20%28or%20set%20of%20users%29%20to%20reconstruct%20the%20private%20data%20of%20other%20users%0Aoutside%20their%20immediate%20neighborhood.%20Our%20approach%20is%20based%20on%20a%20reconstruction%0Aattack%20against%20the%20gossip%20averaging%20protocol%2C%20which%20we%20then%20extend%20to%20handle%0Athe%20additional%20challenges%20raised%20by%20D-GD.%20We%20validate%20the%20effectiveness%20of%20our%0Aattack%20on%20real%20graphs%20and%20datasets%2C%20showing%20that%20the%20number%20of%20users%0Acompromised%20by%20a%20single%20or%20a%20handful%20of%20attackers%20is%20often%20surprisingly%20large.%0AWe%20empirically%20investigate%20some%20of%20the%20factors%20that%20affect%20the%20performance%20of%0Athe%20attack%2C%20namely%20the%20graph%20topology%2C%20the%20number%20of%20attackers%2C%20and%20their%0Aposition%20in%20the%20graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10001v2&entry.124074799=Read"},
{"title": "Dreamguider: Improved Training free Diffusion-based Conditional\n  Generation", "author": "Nithin Gopalakrishnan Nair and Vishal M Patel", "abstract": "  Diffusion models have emerged as a formidable tool for training-free\nconditional generation.However, a key hurdle in inference-time guidance\ntechniques is the need for compute-heavy backpropagation through the diffusion\nnetwork for estimating the guidance direction. Moreover, these techniques often\nrequire handcrafted parameter tuning on a case-by-case basis. Although some\nrecent works have introduced minimal compute methods for linear inverse\nproblems, a generic lightweight guidance solution to both linear and non-linear\nguidance problems is still missing. To this end, we propose Dreamguider, a\nmethod that enables inference-time guidance without compute-heavy\nbackpropagation through the diffusion network. The key idea is to regulate the\ngradient flow through a time-varying factor. Moreover, we propose an empirical\nguidance scale that works for a wide variety of tasks, hence removing the need\nfor handcrafted parameter tuning. We further introduce an effective lightweight\naugmentation strategy that significantly boosts the performance during\ninference-time guidance. We present experiments using Dreamguider on multiple\ntasks across multiple datasets and models to show the effectiveness of the\nproposed modules. To facilitate further research, we will make the code public\nafter the review process.\n", "link": "http://arxiv.org/abs/2406.02549v1", "date": "2024-06-04", "relevancy": 2.2477, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6051}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5555}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dreamguider%3A%20Improved%20Training%20free%20Diffusion-based%20Conditional%0A%20%20Generation&body=Title%3A%20Dreamguider%3A%20Improved%20Training%20free%20Diffusion-based%20Conditional%0A%20%20Generation%0AAuthor%3A%20Nithin%20Gopalakrishnan%20Nair%20and%20Vishal%20M%20Patel%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20a%20formidable%20tool%20for%20training-free%0Aconditional%20generation.However%2C%20a%20key%20hurdle%20in%20inference-time%20guidance%0Atechniques%20is%20the%20need%20for%20compute-heavy%20backpropagation%20through%20the%20diffusion%0Anetwork%20for%20estimating%20the%20guidance%20direction.%20Moreover%2C%20these%20techniques%20often%0Arequire%20handcrafted%20parameter%20tuning%20on%20a%20case-by-case%20basis.%20Although%20some%0Arecent%20works%20have%20introduced%20minimal%20compute%20methods%20for%20linear%20inverse%0Aproblems%2C%20a%20generic%20lightweight%20guidance%20solution%20to%20both%20linear%20and%20non-linear%0Aguidance%20problems%20is%20still%20missing.%20To%20this%20end%2C%20we%20propose%20Dreamguider%2C%20a%0Amethod%20that%20enables%20inference-time%20guidance%20without%20compute-heavy%0Abackpropagation%20through%20the%20diffusion%20network.%20The%20key%20idea%20is%20to%20regulate%20the%0Agradient%20flow%20through%20a%20time-varying%20factor.%20Moreover%2C%20we%20propose%20an%20empirical%0Aguidance%20scale%20that%20works%20for%20a%20wide%20variety%20of%20tasks%2C%20hence%20removing%20the%20need%0Afor%20handcrafted%20parameter%20tuning.%20We%20further%20introduce%20an%20effective%20lightweight%0Aaugmentation%20strategy%20that%20significantly%20boosts%20the%20performance%20during%0Ainference-time%20guidance.%20We%20present%20experiments%20using%20Dreamguider%20on%20multiple%0Atasks%20across%20multiple%20datasets%20and%20models%20to%20show%20the%20effectiveness%20of%20the%0Aproposed%20modules.%20To%20facilitate%20further%20research%2C%20we%20will%20make%20the%20code%20public%0Aafter%20the%20review%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamguider%253A%2520Improved%2520Training%2520free%2520Diffusion-based%2520Conditional%250A%2520%2520Generation%26entry.906535625%3DNithin%2520Gopalakrishnan%2520Nair%2520and%2520Vishal%2520M%2520Patel%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520a%2520formidable%2520tool%2520for%2520training-free%250Aconditional%2520generation.However%252C%2520a%2520key%2520hurdle%2520in%2520inference-time%2520guidance%250Atechniques%2520is%2520the%2520need%2520for%2520compute-heavy%2520backpropagation%2520through%2520the%2520diffusion%250Anetwork%2520for%2520estimating%2520the%2520guidance%2520direction.%2520Moreover%252C%2520these%2520techniques%2520often%250Arequire%2520handcrafted%2520parameter%2520tuning%2520on%2520a%2520case-by-case%2520basis.%2520Although%2520some%250Arecent%2520works%2520have%2520introduced%2520minimal%2520compute%2520methods%2520for%2520linear%2520inverse%250Aproblems%252C%2520a%2520generic%2520lightweight%2520guidance%2520solution%2520to%2520both%2520linear%2520and%2520non-linear%250Aguidance%2520problems%2520is%2520still%2520missing.%2520To%2520this%2520end%252C%2520we%2520propose%2520Dreamguider%252C%2520a%250Amethod%2520that%2520enables%2520inference-time%2520guidance%2520without%2520compute-heavy%250Abackpropagation%2520through%2520the%2520diffusion%2520network.%2520The%2520key%2520idea%2520is%2520to%2520regulate%2520the%250Agradient%2520flow%2520through%2520a%2520time-varying%2520factor.%2520Moreover%252C%2520we%2520propose%2520an%2520empirical%250Aguidance%2520scale%2520that%2520works%2520for%2520a%2520wide%2520variety%2520of%2520tasks%252C%2520hence%2520removing%2520the%2520need%250Afor%2520handcrafted%2520parameter%2520tuning.%2520We%2520further%2520introduce%2520an%2520effective%2520lightweight%250Aaugmentation%2520strategy%2520that%2520significantly%2520boosts%2520the%2520performance%2520during%250Ainference-time%2520guidance.%2520We%2520present%2520experiments%2520using%2520Dreamguider%2520on%2520multiple%250Atasks%2520across%2520multiple%2520datasets%2520and%2520models%2520to%2520show%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520modules.%2520To%2520facilitate%2520further%2520research%252C%2520we%2520will%2520make%2520the%2520code%2520public%250Aafter%2520the%2520review%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dreamguider%3A%20Improved%20Training%20free%20Diffusion-based%20Conditional%0A%20%20Generation&entry.906535625=Nithin%20Gopalakrishnan%20Nair%20and%20Vishal%20M%20Patel&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20a%20formidable%20tool%20for%20training-free%0Aconditional%20generation.However%2C%20a%20key%20hurdle%20in%20inference-time%20guidance%0Atechniques%20is%20the%20need%20for%20compute-heavy%20backpropagation%20through%20the%20diffusion%0Anetwork%20for%20estimating%20the%20guidance%20direction.%20Moreover%2C%20these%20techniques%20often%0Arequire%20handcrafted%20parameter%20tuning%20on%20a%20case-by-case%20basis.%20Although%20some%0Arecent%20works%20have%20introduced%20minimal%20compute%20methods%20for%20linear%20inverse%0Aproblems%2C%20a%20generic%20lightweight%20guidance%20solution%20to%20both%20linear%20and%20non-linear%0Aguidance%20problems%20is%20still%20missing.%20To%20this%20end%2C%20we%20propose%20Dreamguider%2C%20a%0Amethod%20that%20enables%20inference-time%20guidance%20without%20compute-heavy%0Abackpropagation%20through%20the%20diffusion%20network.%20The%20key%20idea%20is%20to%20regulate%20the%0Agradient%20flow%20through%20a%20time-varying%20factor.%20Moreover%2C%20we%20propose%20an%20empirical%0Aguidance%20scale%20that%20works%20for%20a%20wide%20variety%20of%20tasks%2C%20hence%20removing%20the%20need%0Afor%20handcrafted%20parameter%20tuning.%20We%20further%20introduce%20an%20effective%20lightweight%0Aaugmentation%20strategy%20that%20significantly%20boosts%20the%20performance%20during%0Ainference-time%20guidance.%20We%20present%20experiments%20using%20Dreamguider%20on%20multiple%0Atasks%20across%20multiple%20datasets%20and%20models%20to%20show%20the%20effectiveness%20of%20the%0Aproposed%20modules.%20To%20facilitate%20further%20research%2C%20we%20will%20make%20the%20code%20public%0Aafter%20the%20review%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02549v1&entry.124074799=Read"},
{"title": "Understanding Heterophily for Graph Neural Networks", "author": "Junfu Wang and Yuanfang Guo and Liang Yang and Yunhong Wang", "abstract": "  Graphs with heterophily have been regarded as challenging scenarios for Graph\nNeural Networks (GNNs), where nodes are connected with dissimilar neighbors\nthrough various patterns. In this paper, we present theoretical understandings\nof the impacts of different heterophily patterns for GNNs by incorporating the\ngraph convolution (GC) operations into fully connected networks via the\nproposed Heterophilous Stochastic Block Models (HSBM), a general random graph\nmodel that can accommodate diverse heterophily patterns. Firstly, we show that\nby applying a GC operation, the separability gains are determined by two\nfactors, i.e., the Euclidean distance of the neighborhood distributions and\n$\\sqrt{\\mathbb{E}\\left[\\operatorname{deg}\\right]}$, where\n$\\mathbb{E}\\left[\\operatorname{deg}\\right]$ is the averaged node degree. It\nreveals that the impact of heterophily on classification needs to be evaluated\nalongside the averaged node degree. Secondly, we show that the topological\nnoise has a detrimental impact on separability, which is equivalent to\ndegrading $\\mathbb{E}\\left[\\operatorname{deg}\\right]$. Finally, when applying\nmultiple GC operations, we show that the separability gains are determined by\nthe normalized distance of the $l$-powered neighborhood distributions. It\nindicates that the nodes still possess separability as $l$ goes to infinity in\na wide range of regimes. Extensive experiments on both synthetic and real-world\ndata verify the effectiveness of our theory.\n", "link": "http://arxiv.org/abs/2401.09125v2", "date": "2024-06-04", "relevancy": 2.2301, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4709}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4384}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Heterophily%20for%20Graph%20Neural%20Networks&body=Title%3A%20Understanding%20Heterophily%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Junfu%20Wang%20and%20Yuanfang%20Guo%20and%20Liang%20Yang%20and%20Yunhong%20Wang%0AAbstract%3A%20%20%20Graphs%20with%20heterophily%20have%20been%20regarded%20as%20challenging%20scenarios%20for%20Graph%0ANeural%20Networks%20%28GNNs%29%2C%20where%20nodes%20are%20connected%20with%20dissimilar%20neighbors%0Athrough%20various%20patterns.%20In%20this%20paper%2C%20we%20present%20theoretical%20understandings%0Aof%20the%20impacts%20of%20different%20heterophily%20patterns%20for%20GNNs%20by%20incorporating%20the%0Agraph%20convolution%20%28GC%29%20operations%20into%20fully%20connected%20networks%20via%20the%0Aproposed%20Heterophilous%20Stochastic%20Block%20Models%20%28HSBM%29%2C%20a%20general%20random%20graph%0Amodel%20that%20can%20accommodate%20diverse%20heterophily%20patterns.%20Firstly%2C%20we%20show%20that%0Aby%20applying%20a%20GC%20operation%2C%20the%20separability%20gains%20are%20determined%20by%20two%0Afactors%2C%20i.e.%2C%20the%20Euclidean%20distance%20of%20the%20neighborhood%20distributions%20and%0A%24%5Csqrt%7B%5Cmathbb%7BE%7D%5Cleft%5B%5Coperatorname%7Bdeg%7D%5Cright%5D%7D%24%2C%20where%0A%24%5Cmathbb%7BE%7D%5Cleft%5B%5Coperatorname%7Bdeg%7D%5Cright%5D%24%20is%20the%20averaged%20node%20degree.%20It%0Areveals%20that%20the%20impact%20of%20heterophily%20on%20classification%20needs%20to%20be%20evaluated%0Aalongside%20the%20averaged%20node%20degree.%20Secondly%2C%20we%20show%20that%20the%20topological%0Anoise%20has%20a%20detrimental%20impact%20on%20separability%2C%20which%20is%20equivalent%20to%0Adegrading%20%24%5Cmathbb%7BE%7D%5Cleft%5B%5Coperatorname%7Bdeg%7D%5Cright%5D%24.%20Finally%2C%20when%20applying%0Amultiple%20GC%20operations%2C%20we%20show%20that%20the%20separability%20gains%20are%20determined%20by%0Athe%20normalized%20distance%20of%20the%20%24l%24-powered%20neighborhood%20distributions.%20It%0Aindicates%20that%20the%20nodes%20still%20possess%20separability%20as%20%24l%24%20goes%20to%20infinity%20in%0Aa%20wide%20range%20of%20regimes.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%0Adata%20verify%20the%20effectiveness%20of%20our%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Heterophily%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DJunfu%2520Wang%2520and%2520Yuanfang%2520Guo%2520and%2520Liang%2520Yang%2520and%2520Yunhong%2520Wang%26entry.1292438233%3D%2520%2520Graphs%2520with%2520heterophily%2520have%2520been%2520regarded%2520as%2520challenging%2520scenarios%2520for%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%252C%2520where%2520nodes%2520are%2520connected%2520with%2520dissimilar%2520neighbors%250Athrough%2520various%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520present%2520theoretical%2520understandings%250Aof%2520the%2520impacts%2520of%2520different%2520heterophily%2520patterns%2520for%2520GNNs%2520by%2520incorporating%2520the%250Agraph%2520convolution%2520%2528GC%2529%2520operations%2520into%2520fully%2520connected%2520networks%2520via%2520the%250Aproposed%2520Heterophilous%2520Stochastic%2520Block%2520Models%2520%2528HSBM%2529%252C%2520a%2520general%2520random%2520graph%250Amodel%2520that%2520can%2520accommodate%2520diverse%2520heterophily%2520patterns.%2520Firstly%252C%2520we%2520show%2520that%250Aby%2520applying%2520a%2520GC%2520operation%252C%2520the%2520separability%2520gains%2520are%2520determined%2520by%2520two%250Afactors%252C%2520i.e.%252C%2520the%2520Euclidean%2520distance%2520of%2520the%2520neighborhood%2520distributions%2520and%250A%2524%255Csqrt%257B%255Cmathbb%257BE%257D%255Cleft%255B%255Coperatorname%257Bdeg%257D%255Cright%255D%257D%2524%252C%2520where%250A%2524%255Cmathbb%257BE%257D%255Cleft%255B%255Coperatorname%257Bdeg%257D%255Cright%255D%2524%2520is%2520the%2520averaged%2520node%2520degree.%2520It%250Areveals%2520that%2520the%2520impact%2520of%2520heterophily%2520on%2520classification%2520needs%2520to%2520be%2520evaluated%250Aalongside%2520the%2520averaged%2520node%2520degree.%2520Secondly%252C%2520we%2520show%2520that%2520the%2520topological%250Anoise%2520has%2520a%2520detrimental%2520impact%2520on%2520separability%252C%2520which%2520is%2520equivalent%2520to%250Adegrading%2520%2524%255Cmathbb%257BE%257D%255Cleft%255B%255Coperatorname%257Bdeg%257D%255Cright%255D%2524.%2520Finally%252C%2520when%2520applying%250Amultiple%2520GC%2520operations%252C%2520we%2520show%2520that%2520the%2520separability%2520gains%2520are%2520determined%2520by%250Athe%2520normalized%2520distance%2520of%2520the%2520%2524l%2524-powered%2520neighborhood%2520distributions.%2520It%250Aindicates%2520that%2520the%2520nodes%2520still%2520possess%2520separability%2520as%2520%2524l%2524%2520goes%2520to%2520infinity%2520in%250Aa%2520wide%2520range%2520of%2520regimes.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%250Adata%2520verify%2520the%2520effectiveness%2520of%2520our%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Heterophily%20for%20Graph%20Neural%20Networks&entry.906535625=Junfu%20Wang%20and%20Yuanfang%20Guo%20and%20Liang%20Yang%20and%20Yunhong%20Wang&entry.1292438233=%20%20Graphs%20with%20heterophily%20have%20been%20regarded%20as%20challenging%20scenarios%20for%20Graph%0ANeural%20Networks%20%28GNNs%29%2C%20where%20nodes%20are%20connected%20with%20dissimilar%20neighbors%0Athrough%20various%20patterns.%20In%20this%20paper%2C%20we%20present%20theoretical%20understandings%0Aof%20the%20impacts%20of%20different%20heterophily%20patterns%20for%20GNNs%20by%20incorporating%20the%0Agraph%20convolution%20%28GC%29%20operations%20into%20fully%20connected%20networks%20via%20the%0Aproposed%20Heterophilous%20Stochastic%20Block%20Models%20%28HSBM%29%2C%20a%20general%20random%20graph%0Amodel%20that%20can%20accommodate%20diverse%20heterophily%20patterns.%20Firstly%2C%20we%20show%20that%0Aby%20applying%20a%20GC%20operation%2C%20the%20separability%20gains%20are%20determined%20by%20two%0Afactors%2C%20i.e.%2C%20the%20Euclidean%20distance%20of%20the%20neighborhood%20distributions%20and%0A%24%5Csqrt%7B%5Cmathbb%7BE%7D%5Cleft%5B%5Coperatorname%7Bdeg%7D%5Cright%5D%7D%24%2C%20where%0A%24%5Cmathbb%7BE%7D%5Cleft%5B%5Coperatorname%7Bdeg%7D%5Cright%5D%24%20is%20the%20averaged%20node%20degree.%20It%0Areveals%20that%20the%20impact%20of%20heterophily%20on%20classification%20needs%20to%20be%20evaluated%0Aalongside%20the%20averaged%20node%20degree.%20Secondly%2C%20we%20show%20that%20the%20topological%0Anoise%20has%20a%20detrimental%20impact%20on%20separability%2C%20which%20is%20equivalent%20to%0Adegrading%20%24%5Cmathbb%7BE%7D%5Cleft%5B%5Coperatorname%7Bdeg%7D%5Cright%5D%24.%20Finally%2C%20when%20applying%0Amultiple%20GC%20operations%2C%20we%20show%20that%20the%20separability%20gains%20are%20determined%20by%0Athe%20normalized%20distance%20of%20the%20%24l%24-powered%20neighborhood%20distributions.%20It%0Aindicates%20that%20the%20nodes%20still%20possess%20separability%20as%20%24l%24%20goes%20to%20infinity%20in%0Aa%20wide%20range%20of%20regimes.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%0Adata%20verify%20the%20effectiveness%20of%20our%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09125v2&entry.124074799=Read"},
{"title": "Looks Too Good To Be True: An Information-Theoretic Analysis of\n  Hallucinations in Generative Restoration Models", "author": "Regev Cohen and Idan Kligvasser and Ehud Rivlin and Daniel Freedman", "abstract": "  The pursuit of high perceptual quality in image restoration has driven the\ndevelopment of revolutionary generative models, capable of producing results\noften visually indistinguishable from real data. However, as their perceptual\nquality continues to improve, these models also exhibit a growing tendency to\ngenerate hallucinations - realistic-looking details that do not exist in the\nground truth images. The presence of hallucinations introduces uncertainty\nregarding the reliability of the models' predictions, raising major concerns\nabout their practical application. In this paper, we employ information-theory\ntools to investigate this phenomenon, revealing a fundamental tradeoff between\nuncertainty and perception. We rigorously analyze the relationship between\nthese two factors, proving that the global minimal uncertainty in generative\nmodels grows in tandem with perception. In particular, we define the inherent\nuncertainty of the restoration problem and show that attaining perfect\nperceptual quality entails at least twice this uncertainty. Additionally, we\nestablish a relation between mean squared-error distortion, uncertainty and\nperception, through which we prove the aforementioned uncertainly-perception\ntradeoff induces the well-known perception-distortion tradeoff. This work\nuncovers fundamental limitations of generative models in achieving both high\nperceptual quality and reliable predictions for image restoration. We\ndemonstrate our theoretical findings through an analysis of single image\nsuper-resolution algorithms. Our work aims to raise awareness among\npractitioners about this inherent tradeoff, empowering them to make informed\ndecisions and potentially prioritize safety over perceptual performance.\n", "link": "http://arxiv.org/abs/2405.16475v2", "date": "2024-06-04", "relevancy": 2.2056, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5692}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5684}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looks%20Too%20Good%20To%20Be%20True%3A%20An%20Information-Theoretic%20Analysis%20of%0A%20%20Hallucinations%20in%20Generative%20Restoration%20Models&body=Title%3A%20Looks%20Too%20Good%20To%20Be%20True%3A%20An%20Information-Theoretic%20Analysis%20of%0A%20%20Hallucinations%20in%20Generative%20Restoration%20Models%0AAuthor%3A%20Regev%20Cohen%20and%20Idan%20Kligvasser%20and%20Ehud%20Rivlin%20and%20Daniel%20Freedman%0AAbstract%3A%20%20%20The%20pursuit%20of%20high%20perceptual%20quality%20in%20image%20restoration%20has%20driven%20the%0Adevelopment%20of%20revolutionary%20generative%20models%2C%20capable%20of%20producing%20results%0Aoften%20visually%20indistinguishable%20from%20real%20data.%20However%2C%20as%20their%20perceptual%0Aquality%20continues%20to%20improve%2C%20these%20models%20also%20exhibit%20a%20growing%20tendency%20to%0Agenerate%20hallucinations%20-%20realistic-looking%20details%20that%20do%20not%20exist%20in%20the%0Aground%20truth%20images.%20The%20presence%20of%20hallucinations%20introduces%20uncertainty%0Aregarding%20the%20reliability%20of%20the%20models%27%20predictions%2C%20raising%20major%20concerns%0Aabout%20their%20practical%20application.%20In%20this%20paper%2C%20we%20employ%20information-theory%0Atools%20to%20investigate%20this%20phenomenon%2C%20revealing%20a%20fundamental%20tradeoff%20between%0Auncertainty%20and%20perception.%20We%20rigorously%20analyze%20the%20relationship%20between%0Athese%20two%20factors%2C%20proving%20that%20the%20global%20minimal%20uncertainty%20in%20generative%0Amodels%20grows%20in%20tandem%20with%20perception.%20In%20particular%2C%20we%20define%20the%20inherent%0Auncertainty%20of%20the%20restoration%20problem%20and%20show%20that%20attaining%20perfect%0Aperceptual%20quality%20entails%20at%20least%20twice%20this%20uncertainty.%20Additionally%2C%20we%0Aestablish%20a%20relation%20between%20mean%20squared-error%20distortion%2C%20uncertainty%20and%0Aperception%2C%20through%20which%20we%20prove%20the%20aforementioned%20uncertainly-perception%0Atradeoff%20induces%20the%20well-known%20perception-distortion%20tradeoff.%20This%20work%0Auncovers%20fundamental%20limitations%20of%20generative%20models%20in%20achieving%20both%20high%0Aperceptual%20quality%20and%20reliable%20predictions%20for%20image%20restoration.%20We%0Ademonstrate%20our%20theoretical%20findings%20through%20an%20analysis%20of%20single%20image%0Asuper-resolution%20algorithms.%20Our%20work%20aims%20to%20raise%20awareness%20among%0Apractitioners%20about%20this%20inherent%20tradeoff%2C%20empowering%20them%20to%20make%20informed%0Adecisions%20and%20potentially%20prioritize%20safety%20over%20perceptual%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16475v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooks%2520Too%2520Good%2520To%2520Be%2520True%253A%2520An%2520Information-Theoretic%2520Analysis%2520of%250A%2520%2520Hallucinations%2520in%2520Generative%2520Restoration%2520Models%26entry.906535625%3DRegev%2520Cohen%2520and%2520Idan%2520Kligvasser%2520and%2520Ehud%2520Rivlin%2520and%2520Daniel%2520Freedman%26entry.1292438233%3D%2520%2520The%2520pursuit%2520of%2520high%2520perceptual%2520quality%2520in%2520image%2520restoration%2520has%2520driven%2520the%250Adevelopment%2520of%2520revolutionary%2520generative%2520models%252C%2520capable%2520of%2520producing%2520results%250Aoften%2520visually%2520indistinguishable%2520from%2520real%2520data.%2520However%252C%2520as%2520their%2520perceptual%250Aquality%2520continues%2520to%2520improve%252C%2520these%2520models%2520also%2520exhibit%2520a%2520growing%2520tendency%2520to%250Agenerate%2520hallucinations%2520-%2520realistic-looking%2520details%2520that%2520do%2520not%2520exist%2520in%2520the%250Aground%2520truth%2520images.%2520The%2520presence%2520of%2520hallucinations%2520introduces%2520uncertainty%250Aregarding%2520the%2520reliability%2520of%2520the%2520models%2527%2520predictions%252C%2520raising%2520major%2520concerns%250Aabout%2520their%2520practical%2520application.%2520In%2520this%2520paper%252C%2520we%2520employ%2520information-theory%250Atools%2520to%2520investigate%2520this%2520phenomenon%252C%2520revealing%2520a%2520fundamental%2520tradeoff%2520between%250Auncertainty%2520and%2520perception.%2520We%2520rigorously%2520analyze%2520the%2520relationship%2520between%250Athese%2520two%2520factors%252C%2520proving%2520that%2520the%2520global%2520minimal%2520uncertainty%2520in%2520generative%250Amodels%2520grows%2520in%2520tandem%2520with%2520perception.%2520In%2520particular%252C%2520we%2520define%2520the%2520inherent%250Auncertainty%2520of%2520the%2520restoration%2520problem%2520and%2520show%2520that%2520attaining%2520perfect%250Aperceptual%2520quality%2520entails%2520at%2520least%2520twice%2520this%2520uncertainty.%2520Additionally%252C%2520we%250Aestablish%2520a%2520relation%2520between%2520mean%2520squared-error%2520distortion%252C%2520uncertainty%2520and%250Aperception%252C%2520through%2520which%2520we%2520prove%2520the%2520aforementioned%2520uncertainly-perception%250Atradeoff%2520induces%2520the%2520well-known%2520perception-distortion%2520tradeoff.%2520This%2520work%250Auncovers%2520fundamental%2520limitations%2520of%2520generative%2520models%2520in%2520achieving%2520both%2520high%250Aperceptual%2520quality%2520and%2520reliable%2520predictions%2520for%2520image%2520restoration.%2520We%250Ademonstrate%2520our%2520theoretical%2520findings%2520through%2520an%2520analysis%2520of%2520single%2520image%250Asuper-resolution%2520algorithms.%2520Our%2520work%2520aims%2520to%2520raise%2520awareness%2520among%250Apractitioners%2520about%2520this%2520inherent%2520tradeoff%252C%2520empowering%2520them%2520to%2520make%2520informed%250Adecisions%2520and%2520potentially%2520prioritize%2520safety%2520over%2520perceptual%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16475v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looks%20Too%20Good%20To%20Be%20True%3A%20An%20Information-Theoretic%20Analysis%20of%0A%20%20Hallucinations%20in%20Generative%20Restoration%20Models&entry.906535625=Regev%20Cohen%20and%20Idan%20Kligvasser%20and%20Ehud%20Rivlin%20and%20Daniel%20Freedman&entry.1292438233=%20%20The%20pursuit%20of%20high%20perceptual%20quality%20in%20image%20restoration%20has%20driven%20the%0Adevelopment%20of%20revolutionary%20generative%20models%2C%20capable%20of%20producing%20results%0Aoften%20visually%20indistinguishable%20from%20real%20data.%20However%2C%20as%20their%20perceptual%0Aquality%20continues%20to%20improve%2C%20these%20models%20also%20exhibit%20a%20growing%20tendency%20to%0Agenerate%20hallucinations%20-%20realistic-looking%20details%20that%20do%20not%20exist%20in%20the%0Aground%20truth%20images.%20The%20presence%20of%20hallucinations%20introduces%20uncertainty%0Aregarding%20the%20reliability%20of%20the%20models%27%20predictions%2C%20raising%20major%20concerns%0Aabout%20their%20practical%20application.%20In%20this%20paper%2C%20we%20employ%20information-theory%0Atools%20to%20investigate%20this%20phenomenon%2C%20revealing%20a%20fundamental%20tradeoff%20between%0Auncertainty%20and%20perception.%20We%20rigorously%20analyze%20the%20relationship%20between%0Athese%20two%20factors%2C%20proving%20that%20the%20global%20minimal%20uncertainty%20in%20generative%0Amodels%20grows%20in%20tandem%20with%20perception.%20In%20particular%2C%20we%20define%20the%20inherent%0Auncertainty%20of%20the%20restoration%20problem%20and%20show%20that%20attaining%20perfect%0Aperceptual%20quality%20entails%20at%20least%20twice%20this%20uncertainty.%20Additionally%2C%20we%0Aestablish%20a%20relation%20between%20mean%20squared-error%20distortion%2C%20uncertainty%20and%0Aperception%2C%20through%20which%20we%20prove%20the%20aforementioned%20uncertainly-perception%0Atradeoff%20induces%20the%20well-known%20perception-distortion%20tradeoff.%20This%20work%0Auncovers%20fundamental%20limitations%20of%20generative%20models%20in%20achieving%20both%20high%0Aperceptual%20quality%20and%20reliable%20predictions%20for%20image%20restoration.%20We%0Ademonstrate%20our%20theoretical%20findings%20through%20an%20analysis%20of%20single%20image%0Asuper-resolution%20algorithms.%20Our%20work%20aims%20to%20raise%20awareness%20among%0Apractitioners%20about%20this%20inherent%20tradeoff%2C%20empowering%20them%20to%20make%20informed%0Adecisions%20and%20potentially%20prioritize%20safety%20over%20perceptual%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16475v2&entry.124074799=Read"},
{"title": "Out-of-Distribution Runtime Adaptation with Conformalized Neural Network\n  Ensembles", "author": "Polo Contreras and Ola Shorinwa and Mac Schwager", "abstract": "  We present a method to integrate real-time out-of-distribution (OOD)\ndetection for neural network trajectory predictors, and to adapt the control\nstrategy of a robot (e.g., a self-driving car or drone) to preserve safety\nwhile operating in OOD regimes. Specifically, we use a neural network ensemble\nto predict the trajectory for a dynamic obstacle (such as a pedestrian), and\nuse the maximum singular value of the empirical covariance among the ensemble\nas a signal for OOD detection. We calibrate this signal with a small fraction\nof held-out training data using the methodology of conformal prediction, to\nderive an OOD detector with probabilistic guarantees on the false-positive rate\nof the detector, given a user-specified confidence level. During\nin-distribution operation, we use an MPC controller to avoid collisions with\nthe obstacle based on the trajectory predicted by the neural network ensemble.\nWhen OOD conditions are detected, we switch to a reachability-based controller\nto guarantee safety under the worst-case actions of the obstacle. We verify our\nmethod in extensive autonomous driving simulations in a pedestrian crossing\nscenario, showing that our OOD detector obtains the desired accuracy rate\nwithin a theoretically-predicted range. We also demonstrate the effectiveness\nof our method with real pedestrian data. We show improved safety and less\nconservatism in comparison with two state-of-the-art methods that also use\nconformal prediction, but without OOD adaptation.\n", "link": "http://arxiv.org/abs/2406.02436v1", "date": "2024-06-04", "relevancy": 2.193, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6051}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5503}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out-of-Distribution%20Runtime%20Adaptation%20with%20Conformalized%20Neural%20Network%0A%20%20Ensembles&body=Title%3A%20Out-of-Distribution%20Runtime%20Adaptation%20with%20Conformalized%20Neural%20Network%0A%20%20Ensembles%0AAuthor%3A%20Polo%20Contreras%20and%20Ola%20Shorinwa%20and%20Mac%20Schwager%0AAbstract%3A%20%20%20We%20present%20a%20method%20to%20integrate%20real-time%20out-of-distribution%20%28OOD%29%0Adetection%20for%20neural%20network%20trajectory%20predictors%2C%20and%20to%20adapt%20the%20control%0Astrategy%20of%20a%20robot%20%28e.g.%2C%20a%20self-driving%20car%20or%20drone%29%20to%20preserve%20safety%0Awhile%20operating%20in%20OOD%20regimes.%20Specifically%2C%20we%20use%20a%20neural%20network%20ensemble%0Ato%20predict%20the%20trajectory%20for%20a%20dynamic%20obstacle%20%28such%20as%20a%20pedestrian%29%2C%20and%0Ause%20the%20maximum%20singular%20value%20of%20the%20empirical%20covariance%20among%20the%20ensemble%0Aas%20a%20signal%20for%20OOD%20detection.%20We%20calibrate%20this%20signal%20with%20a%20small%20fraction%0Aof%20held-out%20training%20data%20using%20the%20methodology%20of%20conformal%20prediction%2C%20to%0Aderive%20an%20OOD%20detector%20with%20probabilistic%20guarantees%20on%20the%20false-positive%20rate%0Aof%20the%20detector%2C%20given%20a%20user-specified%20confidence%20level.%20During%0Ain-distribution%20operation%2C%20we%20use%20an%20MPC%20controller%20to%20avoid%20collisions%20with%0Athe%20obstacle%20based%20on%20the%20trajectory%20predicted%20by%20the%20neural%20network%20ensemble.%0AWhen%20OOD%20conditions%20are%20detected%2C%20we%20switch%20to%20a%20reachability-based%20controller%0Ato%20guarantee%20safety%20under%20the%20worst-case%20actions%20of%20the%20obstacle.%20We%20verify%20our%0Amethod%20in%20extensive%20autonomous%20driving%20simulations%20in%20a%20pedestrian%20crossing%0Ascenario%2C%20showing%20that%20our%20OOD%20detector%20obtains%20the%20desired%20accuracy%20rate%0Awithin%20a%20theoretically-predicted%20range.%20We%20also%20demonstrate%20the%20effectiveness%0Aof%20our%20method%20with%20real%20pedestrian%20data.%20We%20show%20improved%20safety%20and%20less%0Aconservatism%20in%20comparison%20with%20two%20state-of-the-art%20methods%20that%20also%20use%0Aconformal%20prediction%2C%20but%20without%20OOD%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut-of-Distribution%2520Runtime%2520Adaptation%2520with%2520Conformalized%2520Neural%2520Network%250A%2520%2520Ensembles%26entry.906535625%3DPolo%2520Contreras%2520and%2520Ola%2520Shorinwa%2520and%2520Mac%2520Schwager%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520to%2520integrate%2520real-time%2520out-of-distribution%2520%2528OOD%2529%250Adetection%2520for%2520neural%2520network%2520trajectory%2520predictors%252C%2520and%2520to%2520adapt%2520the%2520control%250Astrategy%2520of%2520a%2520robot%2520%2528e.g.%252C%2520a%2520self-driving%2520car%2520or%2520drone%2529%2520to%2520preserve%2520safety%250Awhile%2520operating%2520in%2520OOD%2520regimes.%2520Specifically%252C%2520we%2520use%2520a%2520neural%2520network%2520ensemble%250Ato%2520predict%2520the%2520trajectory%2520for%2520a%2520dynamic%2520obstacle%2520%2528such%2520as%2520a%2520pedestrian%2529%252C%2520and%250Ause%2520the%2520maximum%2520singular%2520value%2520of%2520the%2520empirical%2520covariance%2520among%2520the%2520ensemble%250Aas%2520a%2520signal%2520for%2520OOD%2520detection.%2520We%2520calibrate%2520this%2520signal%2520with%2520a%2520small%2520fraction%250Aof%2520held-out%2520training%2520data%2520using%2520the%2520methodology%2520of%2520conformal%2520prediction%252C%2520to%250Aderive%2520an%2520OOD%2520detector%2520with%2520probabilistic%2520guarantees%2520on%2520the%2520false-positive%2520rate%250Aof%2520the%2520detector%252C%2520given%2520a%2520user-specified%2520confidence%2520level.%2520During%250Ain-distribution%2520operation%252C%2520we%2520use%2520an%2520MPC%2520controller%2520to%2520avoid%2520collisions%2520with%250Athe%2520obstacle%2520based%2520on%2520the%2520trajectory%2520predicted%2520by%2520the%2520neural%2520network%2520ensemble.%250AWhen%2520OOD%2520conditions%2520are%2520detected%252C%2520we%2520switch%2520to%2520a%2520reachability-based%2520controller%250Ato%2520guarantee%2520safety%2520under%2520the%2520worst-case%2520actions%2520of%2520the%2520obstacle.%2520We%2520verify%2520our%250Amethod%2520in%2520extensive%2520autonomous%2520driving%2520simulations%2520in%2520a%2520pedestrian%2520crossing%250Ascenario%252C%2520showing%2520that%2520our%2520OOD%2520detector%2520obtains%2520the%2520desired%2520accuracy%2520rate%250Awithin%2520a%2520theoretically-predicted%2520range.%2520We%2520also%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520method%2520with%2520real%2520pedestrian%2520data.%2520We%2520show%2520improved%2520safety%2520and%2520less%250Aconservatism%2520in%2520comparison%2520with%2520two%2520state-of-the-art%2520methods%2520that%2520also%2520use%250Aconformal%2520prediction%252C%2520but%2520without%2520OOD%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-Distribution%20Runtime%20Adaptation%20with%20Conformalized%20Neural%20Network%0A%20%20Ensembles&entry.906535625=Polo%20Contreras%20and%20Ola%20Shorinwa%20and%20Mac%20Schwager&entry.1292438233=%20%20We%20present%20a%20method%20to%20integrate%20real-time%20out-of-distribution%20%28OOD%29%0Adetection%20for%20neural%20network%20trajectory%20predictors%2C%20and%20to%20adapt%20the%20control%0Astrategy%20of%20a%20robot%20%28e.g.%2C%20a%20self-driving%20car%20or%20drone%29%20to%20preserve%20safety%0Awhile%20operating%20in%20OOD%20regimes.%20Specifically%2C%20we%20use%20a%20neural%20network%20ensemble%0Ato%20predict%20the%20trajectory%20for%20a%20dynamic%20obstacle%20%28such%20as%20a%20pedestrian%29%2C%20and%0Ause%20the%20maximum%20singular%20value%20of%20the%20empirical%20covariance%20among%20the%20ensemble%0Aas%20a%20signal%20for%20OOD%20detection.%20We%20calibrate%20this%20signal%20with%20a%20small%20fraction%0Aof%20held-out%20training%20data%20using%20the%20methodology%20of%20conformal%20prediction%2C%20to%0Aderive%20an%20OOD%20detector%20with%20probabilistic%20guarantees%20on%20the%20false-positive%20rate%0Aof%20the%20detector%2C%20given%20a%20user-specified%20confidence%20level.%20During%0Ain-distribution%20operation%2C%20we%20use%20an%20MPC%20controller%20to%20avoid%20collisions%20with%0Athe%20obstacle%20based%20on%20the%20trajectory%20predicted%20by%20the%20neural%20network%20ensemble.%0AWhen%20OOD%20conditions%20are%20detected%2C%20we%20switch%20to%20a%20reachability-based%20controller%0Ato%20guarantee%20safety%20under%20the%20worst-case%20actions%20of%20the%20obstacle.%20We%20verify%20our%0Amethod%20in%20extensive%20autonomous%20driving%20simulations%20in%20a%20pedestrian%20crossing%0Ascenario%2C%20showing%20that%20our%20OOD%20detector%20obtains%20the%20desired%20accuracy%20rate%0Awithin%20a%20theoretically-predicted%20range.%20We%20also%20demonstrate%20the%20effectiveness%0Aof%20our%20method%20with%20real%20pedestrian%20data.%20We%20show%20improved%20safety%20and%20less%0Aconservatism%20in%20comparison%20with%20two%20state-of-the-art%20methods%20that%20also%20use%0Aconformal%20prediction%2C%20but%20without%20OOD%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02436v1&entry.124074799=Read"},
{"title": "In value-based deep reinforcement learning, a pruned network is a good\n  network", "author": "Johan Obando-Ceron and Aaron Courville and Pablo Samuel Castro", "abstract": "  Recent work has shown that deep reinforcement learning agents have difficulty\nin effectively using their network parameters. We leverage prior insights into\nthe advantages of sparse training techniques and demonstrate that gradual\nmagnitude pruning enables value-based agents to maximize parameter\neffectiveness. This results in networks that yield dramatic performance\nimprovements over traditional networks, using only a small fraction of the full\nnetwork parameters.\n", "link": "http://arxiv.org/abs/2402.12479v2", "date": "2024-06-04", "relevancy": 2.1899, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4338}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20value-based%20deep%20reinforcement%20learning%2C%20a%20pruned%20network%20is%20a%20good%0A%20%20network&body=Title%3A%20In%20value-based%20deep%20reinforcement%20learning%2C%20a%20pruned%20network%20is%20a%20good%0A%20%20network%0AAuthor%3A%20Johan%20Obando-Ceron%20and%20Aaron%20Courville%20and%20Pablo%20Samuel%20Castro%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%20deep%20reinforcement%20learning%20agents%20have%20difficulty%0Ain%20effectively%20using%20their%20network%20parameters.%20We%20leverage%20prior%20insights%20into%0Athe%20advantages%20of%20sparse%20training%20techniques%20and%20demonstrate%20that%20gradual%0Amagnitude%20pruning%20enables%20value-based%20agents%20to%20maximize%20parameter%0Aeffectiveness.%20This%20results%20in%20networks%20that%20yield%20dramatic%20performance%0Aimprovements%20over%20traditional%20networks%2C%20using%20only%20a%20small%20fraction%20of%20the%20full%0Anetwork%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520value-based%2520deep%2520reinforcement%2520learning%252C%2520a%2520pruned%2520network%2520is%2520a%2520good%250A%2520%2520network%26entry.906535625%3DJohan%2520Obando-Ceron%2520and%2520Aaron%2520Courville%2520and%2520Pablo%2520Samuel%2520Castro%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%2520deep%2520reinforcement%2520learning%2520agents%2520have%2520difficulty%250Ain%2520effectively%2520using%2520their%2520network%2520parameters.%2520We%2520leverage%2520prior%2520insights%2520into%250Athe%2520advantages%2520of%2520sparse%2520training%2520techniques%2520and%2520demonstrate%2520that%2520gradual%250Amagnitude%2520pruning%2520enables%2520value-based%2520agents%2520to%2520maximize%2520parameter%250Aeffectiveness.%2520This%2520results%2520in%2520networks%2520that%2520yield%2520dramatic%2520performance%250Aimprovements%2520over%2520traditional%2520networks%252C%2520using%2520only%2520a%2520small%2520fraction%2520of%2520the%2520full%250Anetwork%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20value-based%20deep%20reinforcement%20learning%2C%20a%20pruned%20network%20is%20a%20good%0A%20%20network&entry.906535625=Johan%20Obando-Ceron%20and%20Aaron%20Courville%20and%20Pablo%20Samuel%20Castro&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%20deep%20reinforcement%20learning%20agents%20have%20difficulty%0Ain%20effectively%20using%20their%20network%20parameters.%20We%20leverage%20prior%20insights%20into%0Athe%20advantages%20of%20sparse%20training%20techniques%20and%20demonstrate%20that%20gradual%0Amagnitude%20pruning%20enables%20value-based%20agents%20to%20maximize%20parameter%0Aeffectiveness.%20This%20results%20in%20networks%20that%20yield%20dramatic%20performance%0Aimprovements%20over%20traditional%20networks%2C%20using%20only%20a%20small%20fraction%20of%20the%20full%0Anetwork%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12479v2&entry.124074799=Read"},
{"title": "ASCNet: Asymmetric Sampling Correction Network for Infrared Image\n  Destriping", "author": "Shuai Yuan and Hanlin Qin and Xiang Yan and Shiqi Yang and Shuowen Yang and Naveed Akhtar", "abstract": "  In a real-world infrared imaging system, effectively learning a consistent\nstripe noise removal model is essential. Most existing destriping methods\ncannot precisely reconstruct images due to cross-level semantic gaps and\ninsufficient characterization of the global column features. To tackle this\nproblem, we propose a novel infrared image destriping method, called Asymmetric\nSampling Correction Network (ASCNet), that can effectively capture global\ncolumn relationships and embed them into a U-shaped framework, providing\ncomprehensive discriminative representation and seamless semantic connectivity.\nOur ASCNet consists of three core elements: Residual Haar Discrete Wavelet\nTransform (RHDWT), Pixel Shuffle (PS), and Column Non-uniformity Correction\nModule (CNCM). Specifically, RHDWT is a novel downsampler that employs\ndouble-branch modeling to effectively integrate stripe-directional prior\nknowledge and data-driven semantic interaction to enrich the feature\nrepresentation. Observing the semantic patterns crosstalk of stripe noise, PS\nis introduced as an upsampler to prevent excessive apriori decoding and\nperforming semantic-bias-free image reconstruction. After each sampling, CNCM\ncaptures the column relationships in long-range dependencies. By incorporating\ncolumn, spatial, and self-dependence information, CNCM well establishes a\nglobal context to distinguish stripes from the scene's vertical structures.\nExtensive experiments on synthetic data, real data, and infrared small target\ndetection tasks demonstrate that the proposed method outperforms\nstate-of-the-art single-image destriping methods both visually and\nquantitatively. Our code will be made publicly available at\nhttps://github.com/xdFai/ASCNet.\n", "link": "http://arxiv.org/abs/2401.15578v2", "date": "2024-06-04", "relevancy": 2.1766, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5546}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.541}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASCNet%3A%20Asymmetric%20Sampling%20Correction%20Network%20for%20Infrared%20Image%0A%20%20Destriping&body=Title%3A%20ASCNet%3A%20Asymmetric%20Sampling%20Correction%20Network%20for%20Infrared%20Image%0A%20%20Destriping%0AAuthor%3A%20Shuai%20Yuan%20and%20Hanlin%20Qin%20and%20Xiang%20Yan%20and%20Shiqi%20Yang%20and%20Shuowen%20Yang%20and%20Naveed%20Akhtar%0AAbstract%3A%20%20%20In%20a%20real-world%20infrared%20imaging%20system%2C%20effectively%20learning%20a%20consistent%0Astripe%20noise%20removal%20model%20is%20essential.%20Most%20existing%20destriping%20methods%0Acannot%20precisely%20reconstruct%20images%20due%20to%20cross-level%20semantic%20gaps%20and%0Ainsufficient%20characterization%20of%20the%20global%20column%20features.%20To%20tackle%20this%0Aproblem%2C%20we%20propose%20a%20novel%20infrared%20image%20destriping%20method%2C%20called%20Asymmetric%0ASampling%20Correction%20Network%20%28ASCNet%29%2C%20that%20can%20effectively%20capture%20global%0Acolumn%20relationships%20and%20embed%20them%20into%20a%20U-shaped%20framework%2C%20providing%0Acomprehensive%20discriminative%20representation%20and%20seamless%20semantic%20connectivity.%0AOur%20ASCNet%20consists%20of%20three%20core%20elements%3A%20Residual%20Haar%20Discrete%20Wavelet%0ATransform%20%28RHDWT%29%2C%20Pixel%20Shuffle%20%28PS%29%2C%20and%20Column%20Non-uniformity%20Correction%0AModule%20%28CNCM%29.%20Specifically%2C%20RHDWT%20is%20a%20novel%20downsampler%20that%20employs%0Adouble-branch%20modeling%20to%20effectively%20integrate%20stripe-directional%20prior%0Aknowledge%20and%20data-driven%20semantic%20interaction%20to%20enrich%20the%20feature%0Arepresentation.%20Observing%20the%20semantic%20patterns%20crosstalk%20of%20stripe%20noise%2C%20PS%0Ais%20introduced%20as%20an%20upsampler%20to%20prevent%20excessive%20apriori%20decoding%20and%0Aperforming%20semantic-bias-free%20image%20reconstruction.%20After%20each%20sampling%2C%20CNCM%0Acaptures%20the%20column%20relationships%20in%20long-range%20dependencies.%20By%20incorporating%0Acolumn%2C%20spatial%2C%20and%20self-dependence%20information%2C%20CNCM%20well%20establishes%20a%0Aglobal%20context%20to%20distinguish%20stripes%20from%20the%20scene%27s%20vertical%20structures.%0AExtensive%20experiments%20on%20synthetic%20data%2C%20real%20data%2C%20and%20infrared%20small%20target%0Adetection%20tasks%20demonstrate%20that%20the%20proposed%20method%20outperforms%0Astate-of-the-art%20single-image%20destriping%20methods%20both%20visually%20and%0Aquantitatively.%20Our%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/xdFai/ASCNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASCNet%253A%2520Asymmetric%2520Sampling%2520Correction%2520Network%2520for%2520Infrared%2520Image%250A%2520%2520Destriping%26entry.906535625%3DShuai%2520Yuan%2520and%2520Hanlin%2520Qin%2520and%2520Xiang%2520Yan%2520and%2520Shiqi%2520Yang%2520and%2520Shuowen%2520Yang%2520and%2520Naveed%2520Akhtar%26entry.1292438233%3D%2520%2520In%2520a%2520real-world%2520infrared%2520imaging%2520system%252C%2520effectively%2520learning%2520a%2520consistent%250Astripe%2520noise%2520removal%2520model%2520is%2520essential.%2520Most%2520existing%2520destriping%2520methods%250Acannot%2520precisely%2520reconstruct%2520images%2520due%2520to%2520cross-level%2520semantic%2520gaps%2520and%250Ainsufficient%2520characterization%2520of%2520the%2520global%2520column%2520features.%2520To%2520tackle%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520novel%2520infrared%2520image%2520destriping%2520method%252C%2520called%2520Asymmetric%250ASampling%2520Correction%2520Network%2520%2528ASCNet%2529%252C%2520that%2520can%2520effectively%2520capture%2520global%250Acolumn%2520relationships%2520and%2520embed%2520them%2520into%2520a%2520U-shaped%2520framework%252C%2520providing%250Acomprehensive%2520discriminative%2520representation%2520and%2520seamless%2520semantic%2520connectivity.%250AOur%2520ASCNet%2520consists%2520of%2520three%2520core%2520elements%253A%2520Residual%2520Haar%2520Discrete%2520Wavelet%250ATransform%2520%2528RHDWT%2529%252C%2520Pixel%2520Shuffle%2520%2528PS%2529%252C%2520and%2520Column%2520Non-uniformity%2520Correction%250AModule%2520%2528CNCM%2529.%2520Specifically%252C%2520RHDWT%2520is%2520a%2520novel%2520downsampler%2520that%2520employs%250Adouble-branch%2520modeling%2520to%2520effectively%2520integrate%2520stripe-directional%2520prior%250Aknowledge%2520and%2520data-driven%2520semantic%2520interaction%2520to%2520enrich%2520the%2520feature%250Arepresentation.%2520Observing%2520the%2520semantic%2520patterns%2520crosstalk%2520of%2520stripe%2520noise%252C%2520PS%250Ais%2520introduced%2520as%2520an%2520upsampler%2520to%2520prevent%2520excessive%2520apriori%2520decoding%2520and%250Aperforming%2520semantic-bias-free%2520image%2520reconstruction.%2520After%2520each%2520sampling%252C%2520CNCM%250Acaptures%2520the%2520column%2520relationships%2520in%2520long-range%2520dependencies.%2520By%2520incorporating%250Acolumn%252C%2520spatial%252C%2520and%2520self-dependence%2520information%252C%2520CNCM%2520well%2520establishes%2520a%250Aglobal%2520context%2520to%2520distinguish%2520stripes%2520from%2520the%2520scene%2527s%2520vertical%2520structures.%250AExtensive%2520experiments%2520on%2520synthetic%2520data%252C%2520real%2520data%252C%2520and%2520infrared%2520small%2520target%250Adetection%2520tasks%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%250Astate-of-the-art%2520single-image%2520destriping%2520methods%2520both%2520visually%2520and%250Aquantitatively.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/xdFai/ASCNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASCNet%3A%20Asymmetric%20Sampling%20Correction%20Network%20for%20Infrared%20Image%0A%20%20Destriping&entry.906535625=Shuai%20Yuan%20and%20Hanlin%20Qin%20and%20Xiang%20Yan%20and%20Shiqi%20Yang%20and%20Shuowen%20Yang%20and%20Naveed%20Akhtar&entry.1292438233=%20%20In%20a%20real-world%20infrared%20imaging%20system%2C%20effectively%20learning%20a%20consistent%0Astripe%20noise%20removal%20model%20is%20essential.%20Most%20existing%20destriping%20methods%0Acannot%20precisely%20reconstruct%20images%20due%20to%20cross-level%20semantic%20gaps%20and%0Ainsufficient%20characterization%20of%20the%20global%20column%20features.%20To%20tackle%20this%0Aproblem%2C%20we%20propose%20a%20novel%20infrared%20image%20destriping%20method%2C%20called%20Asymmetric%0ASampling%20Correction%20Network%20%28ASCNet%29%2C%20that%20can%20effectively%20capture%20global%0Acolumn%20relationships%20and%20embed%20them%20into%20a%20U-shaped%20framework%2C%20providing%0Acomprehensive%20discriminative%20representation%20and%20seamless%20semantic%20connectivity.%0AOur%20ASCNet%20consists%20of%20three%20core%20elements%3A%20Residual%20Haar%20Discrete%20Wavelet%0ATransform%20%28RHDWT%29%2C%20Pixel%20Shuffle%20%28PS%29%2C%20and%20Column%20Non-uniformity%20Correction%0AModule%20%28CNCM%29.%20Specifically%2C%20RHDWT%20is%20a%20novel%20downsampler%20that%20employs%0Adouble-branch%20modeling%20to%20effectively%20integrate%20stripe-directional%20prior%0Aknowledge%20and%20data-driven%20semantic%20interaction%20to%20enrich%20the%20feature%0Arepresentation.%20Observing%20the%20semantic%20patterns%20crosstalk%20of%20stripe%20noise%2C%20PS%0Ais%20introduced%20as%20an%20upsampler%20to%20prevent%20excessive%20apriori%20decoding%20and%0Aperforming%20semantic-bias-free%20image%20reconstruction.%20After%20each%20sampling%2C%20CNCM%0Acaptures%20the%20column%20relationships%20in%20long-range%20dependencies.%20By%20incorporating%0Acolumn%2C%20spatial%2C%20and%20self-dependence%20information%2C%20CNCM%20well%20establishes%20a%0Aglobal%20context%20to%20distinguish%20stripes%20from%20the%20scene%27s%20vertical%20structures.%0AExtensive%20experiments%20on%20synthetic%20data%2C%20real%20data%2C%20and%20infrared%20small%20target%0Adetection%20tasks%20demonstrate%20that%20the%20proposed%20method%20outperforms%0Astate-of-the-art%20single-image%20destriping%20methods%20both%20visually%20and%0Aquantitatively.%20Our%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/xdFai/ASCNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15578v2&entry.124074799=Read"},
{"title": "Improving the Validity of Decision Trees as Explanations", "author": "Jiri Nemecek and Tomas Pevny and Jakub Marecek", "abstract": "  In classification and forecasting with tabular data, one often utilizes\ntree-based models. Those can be competitive with deep neural networks on\ntabular data and, under some conditions, explainable. The explainability\ndepends on the depth of the tree and the accuracy in each leaf of the tree. We\npoint out that decision trees containing leaves with unbalanced accuracy can\nprovide misleading explanations. Low-accuracy leaves give less valid\nexplanations, which could be interpreted as unfairness among subgroups\nutilizing these explanations. Here, we train a shallow tree with the objective\nof minimizing the maximum misclassification error across all leaf nodes. The\nshallow tree provides a global explanation, while the overall statistical\nperformance of the shallow tree can become comparable to state-of-the-art\nmethods (e.g., well-tuned XGBoost) by extending the leaves with further models.\n", "link": "http://arxiv.org/abs/2306.06777v5", "date": "2024-06-04", "relevancy": 2.1575, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4511}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4217}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Validity%20of%20Decision%20Trees%20as%20Explanations&body=Title%3A%20Improving%20the%20Validity%20of%20Decision%20Trees%20as%20Explanations%0AAuthor%3A%20Jiri%20Nemecek%20and%20Tomas%20Pevny%20and%20Jakub%20Marecek%0AAbstract%3A%20%20%20In%20classification%20and%20forecasting%20with%20tabular%20data%2C%20one%20often%20utilizes%0Atree-based%20models.%20Those%20can%20be%20competitive%20with%20deep%20neural%20networks%20on%0Atabular%20data%20and%2C%20under%20some%20conditions%2C%20explainable.%20The%20explainability%0Adepends%20on%20the%20depth%20of%20the%20tree%20and%20the%20accuracy%20in%20each%20leaf%20of%20the%20tree.%20We%0Apoint%20out%20that%20decision%20trees%20containing%20leaves%20with%20unbalanced%20accuracy%20can%0Aprovide%20misleading%20explanations.%20Low-accuracy%20leaves%20give%20less%20valid%0Aexplanations%2C%20which%20could%20be%20interpreted%20as%20unfairness%20among%20subgroups%0Autilizing%20these%20explanations.%20Here%2C%20we%20train%20a%20shallow%20tree%20with%20the%20objective%0Aof%20minimizing%20the%20maximum%20misclassification%20error%20across%20all%20leaf%20nodes.%20The%0Ashallow%20tree%20provides%20a%20global%20explanation%2C%20while%20the%20overall%20statistical%0Aperformance%20of%20the%20shallow%20tree%20can%20become%20comparable%20to%20state-of-the-art%0Amethods%20%28e.g.%2C%20well-tuned%20XGBoost%29%20by%20extending%20the%20leaves%20with%20further%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06777v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Validity%2520of%2520Decision%2520Trees%2520as%2520Explanations%26entry.906535625%3DJiri%2520Nemecek%2520and%2520Tomas%2520Pevny%2520and%2520Jakub%2520Marecek%26entry.1292438233%3D%2520%2520In%2520classification%2520and%2520forecasting%2520with%2520tabular%2520data%252C%2520one%2520often%2520utilizes%250Atree-based%2520models.%2520Those%2520can%2520be%2520competitive%2520with%2520deep%2520neural%2520networks%2520on%250Atabular%2520data%2520and%252C%2520under%2520some%2520conditions%252C%2520explainable.%2520The%2520explainability%250Adepends%2520on%2520the%2520depth%2520of%2520the%2520tree%2520and%2520the%2520accuracy%2520in%2520each%2520leaf%2520of%2520the%2520tree.%2520We%250Apoint%2520out%2520that%2520decision%2520trees%2520containing%2520leaves%2520with%2520unbalanced%2520accuracy%2520can%250Aprovide%2520misleading%2520explanations.%2520Low-accuracy%2520leaves%2520give%2520less%2520valid%250Aexplanations%252C%2520which%2520could%2520be%2520interpreted%2520as%2520unfairness%2520among%2520subgroups%250Autilizing%2520these%2520explanations.%2520Here%252C%2520we%2520train%2520a%2520shallow%2520tree%2520with%2520the%2520objective%250Aof%2520minimizing%2520the%2520maximum%2520misclassification%2520error%2520across%2520all%2520leaf%2520nodes.%2520The%250Ashallow%2520tree%2520provides%2520a%2520global%2520explanation%252C%2520while%2520the%2520overall%2520statistical%250Aperformance%2520of%2520the%2520shallow%2520tree%2520can%2520become%2520comparable%2520to%2520state-of-the-art%250Amethods%2520%2528e.g.%252C%2520well-tuned%2520XGBoost%2529%2520by%2520extending%2520the%2520leaves%2520with%2520further%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06777v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Validity%20of%20Decision%20Trees%20as%20Explanations&entry.906535625=Jiri%20Nemecek%20and%20Tomas%20Pevny%20and%20Jakub%20Marecek&entry.1292438233=%20%20In%20classification%20and%20forecasting%20with%20tabular%20data%2C%20one%20often%20utilizes%0Atree-based%20models.%20Those%20can%20be%20competitive%20with%20deep%20neural%20networks%20on%0Atabular%20data%20and%2C%20under%20some%20conditions%2C%20explainable.%20The%20explainability%0Adepends%20on%20the%20depth%20of%20the%20tree%20and%20the%20accuracy%20in%20each%20leaf%20of%20the%20tree.%20We%0Apoint%20out%20that%20decision%20trees%20containing%20leaves%20with%20unbalanced%20accuracy%20can%0Aprovide%20misleading%20explanations.%20Low-accuracy%20leaves%20give%20less%20valid%0Aexplanations%2C%20which%20could%20be%20interpreted%20as%20unfairness%20among%20subgroups%0Autilizing%20these%20explanations.%20Here%2C%20we%20train%20a%20shallow%20tree%20with%20the%20objective%0Aof%20minimizing%20the%20maximum%20misclassification%20error%20across%20all%20leaf%20nodes.%20The%0Ashallow%20tree%20provides%20a%20global%20explanation%2C%20while%20the%20overall%20statistical%0Aperformance%20of%20the%20shallow%20tree%20can%20become%20comparable%20to%20state-of-the-art%0Amethods%20%28e.g.%2C%20well-tuned%20XGBoost%29%20by%20extending%20the%20leaves%20with%20further%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06777v5&entry.124074799=Read"},
{"title": "Dsfer-Net: A Deep Supervision and Feature Retrieval Network for\n  Bitemporal Change Detection Using Modern Hopfield Networks", "author": "Shizhen Chang and Michael Kopp and Pedram Ghamisi and Bo Du", "abstract": "  Change detection, an essential application for high-resolution remote sensing\nimages, aims to monitor and analyze changes in the land surface over time. Due\nto the rapid increase in the quantity of high-resolution remote sensing data\nand the complexity of texture features, several quantitative deep\nlearning-based methods have been proposed. These methods outperform traditional\nchange detection methods by extracting deep features and combining\nspatial-temporal information. However, reasonable explanations for how deep\nfeatures improve detection performance are still lacking. In our\ninvestigations, we found that modern Hopfield network layers significantly\nenhance semantic understanding. In this paper, we propose a Deep Supervision\nand FEature Retrieval network (Dsfer-Net) for bitemporal change detection.\nSpecifically, the highly representative deep features of bitemporal images are\njointly extracted through a fully convolutional Siamese network. Based on the\nsequential geographical information of the bitemporal images, we designed a\nfeature retrieval module to extract difference features and leverage\ndiscriminative information in a deeply supervised manner. Additionally, we\nobserved that the deeply supervised feature retrieval module provides\nexplainable evidence of the semantic understanding of the proposed network in\nits deep layers. Finally, our end-to-end network establishes a novel framework\nby aggregating retrieved features and feature pairs from different layers.\nExperiments conducted on three public datasets (LEVIR-CD, WHU-CD, and CDD)\nconfirm the superiority of the proposed Dsfer-Net over other state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2304.01101v2", "date": "2024-06-04", "relevancy": 2.1557, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5587}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5346}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dsfer-Net%3A%20A%20Deep%20Supervision%20and%20Feature%20Retrieval%20Network%20for%0A%20%20Bitemporal%20Change%20Detection%20Using%20Modern%20Hopfield%20Networks&body=Title%3A%20Dsfer-Net%3A%20A%20Deep%20Supervision%20and%20Feature%20Retrieval%20Network%20for%0A%20%20Bitemporal%20Change%20Detection%20Using%20Modern%20Hopfield%20Networks%0AAuthor%3A%20Shizhen%20Chang%20and%20Michael%20Kopp%20and%20Pedram%20Ghamisi%20and%20Bo%20Du%0AAbstract%3A%20%20%20Change%20detection%2C%20an%20essential%20application%20for%20high-resolution%20remote%20sensing%0Aimages%2C%20aims%20to%20monitor%20and%20analyze%20changes%20in%20the%20land%20surface%20over%20time.%20Due%0Ato%20the%20rapid%20increase%20in%20the%20quantity%20of%20high-resolution%20remote%20sensing%20data%0Aand%20the%20complexity%20of%20texture%20features%2C%20several%20quantitative%20deep%0Alearning-based%20methods%20have%20been%20proposed.%20These%20methods%20outperform%20traditional%0Achange%20detection%20methods%20by%20extracting%20deep%20features%20and%20combining%0Aspatial-temporal%20information.%20However%2C%20reasonable%20explanations%20for%20how%20deep%0Afeatures%20improve%20detection%20performance%20are%20still%20lacking.%20In%20our%0Ainvestigations%2C%20we%20found%20that%20modern%20Hopfield%20network%20layers%20significantly%0Aenhance%20semantic%20understanding.%20In%20this%20paper%2C%20we%20propose%20a%20Deep%20Supervision%0Aand%20FEature%20Retrieval%20network%20%28Dsfer-Net%29%20for%20bitemporal%20change%20detection.%0ASpecifically%2C%20the%20highly%20representative%20deep%20features%20of%20bitemporal%20images%20are%0Ajointly%20extracted%20through%20a%20fully%20convolutional%20Siamese%20network.%20Based%20on%20the%0Asequential%20geographical%20information%20of%20the%20bitemporal%20images%2C%20we%20designed%20a%0Afeature%20retrieval%20module%20to%20extract%20difference%20features%20and%20leverage%0Adiscriminative%20information%20in%20a%20deeply%20supervised%20manner.%20Additionally%2C%20we%0Aobserved%20that%20the%20deeply%20supervised%20feature%20retrieval%20module%20provides%0Aexplainable%20evidence%20of%20the%20semantic%20understanding%20of%20the%20proposed%20network%20in%0Aits%20deep%20layers.%20Finally%2C%20our%20end-to-end%20network%20establishes%20a%20novel%20framework%0Aby%20aggregating%20retrieved%20features%20and%20feature%20pairs%20from%20different%20layers.%0AExperiments%20conducted%20on%20three%20public%20datasets%20%28LEVIR-CD%2C%20WHU-CD%2C%20and%20CDD%29%0Aconfirm%20the%20superiority%20of%20the%20proposed%20Dsfer-Net%20over%20other%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.01101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDsfer-Net%253A%2520A%2520Deep%2520Supervision%2520and%2520Feature%2520Retrieval%2520Network%2520for%250A%2520%2520Bitemporal%2520Change%2520Detection%2520Using%2520Modern%2520Hopfield%2520Networks%26entry.906535625%3DShizhen%2520Chang%2520and%2520Michael%2520Kopp%2520and%2520Pedram%2520Ghamisi%2520and%2520Bo%2520Du%26entry.1292438233%3D%2520%2520Change%2520detection%252C%2520an%2520essential%2520application%2520for%2520high-resolution%2520remote%2520sensing%250Aimages%252C%2520aims%2520to%2520monitor%2520and%2520analyze%2520changes%2520in%2520the%2520land%2520surface%2520over%2520time.%2520Due%250Ato%2520the%2520rapid%2520increase%2520in%2520the%2520quantity%2520of%2520high-resolution%2520remote%2520sensing%2520data%250Aand%2520the%2520complexity%2520of%2520texture%2520features%252C%2520several%2520quantitative%2520deep%250Alearning-based%2520methods%2520have%2520been%2520proposed.%2520These%2520methods%2520outperform%2520traditional%250Achange%2520detection%2520methods%2520by%2520extracting%2520deep%2520features%2520and%2520combining%250Aspatial-temporal%2520information.%2520However%252C%2520reasonable%2520explanations%2520for%2520how%2520deep%250Afeatures%2520improve%2520detection%2520performance%2520are%2520still%2520lacking.%2520In%2520our%250Ainvestigations%252C%2520we%2520found%2520that%2520modern%2520Hopfield%2520network%2520layers%2520significantly%250Aenhance%2520semantic%2520understanding.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Deep%2520Supervision%250Aand%2520FEature%2520Retrieval%2520network%2520%2528Dsfer-Net%2529%2520for%2520bitemporal%2520change%2520detection.%250ASpecifically%252C%2520the%2520highly%2520representative%2520deep%2520features%2520of%2520bitemporal%2520images%2520are%250Ajointly%2520extracted%2520through%2520a%2520fully%2520convolutional%2520Siamese%2520network.%2520Based%2520on%2520the%250Asequential%2520geographical%2520information%2520of%2520the%2520bitemporal%2520images%252C%2520we%2520designed%2520a%250Afeature%2520retrieval%2520module%2520to%2520extract%2520difference%2520features%2520and%2520leverage%250Adiscriminative%2520information%2520in%2520a%2520deeply%2520supervised%2520manner.%2520Additionally%252C%2520we%250Aobserved%2520that%2520the%2520deeply%2520supervised%2520feature%2520retrieval%2520module%2520provides%250Aexplainable%2520evidence%2520of%2520the%2520semantic%2520understanding%2520of%2520the%2520proposed%2520network%2520in%250Aits%2520deep%2520layers.%2520Finally%252C%2520our%2520end-to-end%2520network%2520establishes%2520a%2520novel%2520framework%250Aby%2520aggregating%2520retrieved%2520features%2520and%2520feature%2520pairs%2520from%2520different%2520layers.%250AExperiments%2520conducted%2520on%2520three%2520public%2520datasets%2520%2528LEVIR-CD%252C%2520WHU-CD%252C%2520and%2520CDD%2529%250Aconfirm%2520the%2520superiority%2520of%2520the%2520proposed%2520Dsfer-Net%2520over%2520other%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.01101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dsfer-Net%3A%20A%20Deep%20Supervision%20and%20Feature%20Retrieval%20Network%20for%0A%20%20Bitemporal%20Change%20Detection%20Using%20Modern%20Hopfield%20Networks&entry.906535625=Shizhen%20Chang%20and%20Michael%20Kopp%20and%20Pedram%20Ghamisi%20and%20Bo%20Du&entry.1292438233=%20%20Change%20detection%2C%20an%20essential%20application%20for%20high-resolution%20remote%20sensing%0Aimages%2C%20aims%20to%20monitor%20and%20analyze%20changes%20in%20the%20land%20surface%20over%20time.%20Due%0Ato%20the%20rapid%20increase%20in%20the%20quantity%20of%20high-resolution%20remote%20sensing%20data%0Aand%20the%20complexity%20of%20texture%20features%2C%20several%20quantitative%20deep%0Alearning-based%20methods%20have%20been%20proposed.%20These%20methods%20outperform%20traditional%0Achange%20detection%20methods%20by%20extracting%20deep%20features%20and%20combining%0Aspatial-temporal%20information.%20However%2C%20reasonable%20explanations%20for%20how%20deep%0Afeatures%20improve%20detection%20performance%20are%20still%20lacking.%20In%20our%0Ainvestigations%2C%20we%20found%20that%20modern%20Hopfield%20network%20layers%20significantly%0Aenhance%20semantic%20understanding.%20In%20this%20paper%2C%20we%20propose%20a%20Deep%20Supervision%0Aand%20FEature%20Retrieval%20network%20%28Dsfer-Net%29%20for%20bitemporal%20change%20detection.%0ASpecifically%2C%20the%20highly%20representative%20deep%20features%20of%20bitemporal%20images%20are%0Ajointly%20extracted%20through%20a%20fully%20convolutional%20Siamese%20network.%20Based%20on%20the%0Asequential%20geographical%20information%20of%20the%20bitemporal%20images%2C%20we%20designed%20a%0Afeature%20retrieval%20module%20to%20extract%20difference%20features%20and%20leverage%0Adiscriminative%20information%20in%20a%20deeply%20supervised%20manner.%20Additionally%2C%20we%0Aobserved%20that%20the%20deeply%20supervised%20feature%20retrieval%20module%20provides%0Aexplainable%20evidence%20of%20the%20semantic%20understanding%20of%20the%20proposed%20network%20in%0Aits%20deep%20layers.%20Finally%2C%20our%20end-to-end%20network%20establishes%20a%20novel%20framework%0Aby%20aggregating%20retrieved%20features%20and%20feature%20pairs%20from%20different%20layers.%0AExperiments%20conducted%20on%20three%20public%20datasets%20%28LEVIR-CD%2C%20WHU-CD%2C%20and%20CDD%29%0Aconfirm%20the%20superiority%20of%20the%20proposed%20Dsfer-Net%20over%20other%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.01101v2&entry.124074799=Read"},
{"title": "Finding NeMo: Localizing Neurons Responsible For Memorization in\n  Diffusion Models", "author": "Dominik Hintersdorf and Lukas Struppek and Kristian Kersting and Adam Dziedzic and Franziska Boenisch", "abstract": "  Diffusion models (DMs) produce very detailed and high-quality images. Their\npower results from extensive training on large amounts of data, usually scraped\nfrom the internet without proper attribution or consent from content creators.\nUnfortunately, this practice raises privacy and intellectual property concerns,\nas DMs can memorize and later reproduce their potentially sensitive or\ncopyrighted training images at inference time. Prior efforts prevent this issue\nby either changing the input to the diffusion process, thereby preventing the\nDM from generating memorized samples during inference, or removing the\nmemorized data from training altogether. While those are viable solutions when\nthe DM is developed and deployed in a secure and constantly monitored\nenvironment, they hold the risk of adversaries circumventing the safeguards and\nare not effective when the DM itself is publicly released. To solve the\nproblem, we introduce NeMo, the first method to localize memorization of\nindividual data samples down to the level of neurons in DMs' cross-attention\nlayers. Through our experiments, we make the intriguing finding that in many\ncases, single neurons are responsible for memorizing particular training\nsamples. By deactivating these memorization neurons, we can avoid the\nreplication of training data at inference time, increase the diversity in the\ngenerated outputs, and mitigate the leakage of private and copyrighted data. In\nthis way, our NeMo contributes to a more responsible deployment of DMs.\n", "link": "http://arxiv.org/abs/2406.02366v1", "date": "2024-06-04", "relevancy": 2.1522, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5717}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5488}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finding%20NeMo%3A%20Localizing%20Neurons%20Responsible%20For%20Memorization%20in%0A%20%20Diffusion%20Models&body=Title%3A%20Finding%20NeMo%3A%20Localizing%20Neurons%20Responsible%20For%20Memorization%20in%0A%20%20Diffusion%20Models%0AAuthor%3A%20Dominik%20Hintersdorf%20and%20Lukas%20Struppek%20and%20Kristian%20Kersting%20and%20Adam%20Dziedzic%20and%20Franziska%20Boenisch%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20produce%20very%20detailed%20and%20high-quality%20images.%20Their%0Apower%20results%20from%20extensive%20training%20on%20large%20amounts%20of%20data%2C%20usually%20scraped%0Afrom%20the%20internet%20without%20proper%20attribution%20or%20consent%20from%20content%20creators.%0AUnfortunately%2C%20this%20practice%20raises%20privacy%20and%20intellectual%20property%20concerns%2C%0Aas%20DMs%20can%20memorize%20and%20later%20reproduce%20their%20potentially%20sensitive%20or%0Acopyrighted%20training%20images%20at%20inference%20time.%20Prior%20efforts%20prevent%20this%20issue%0Aby%20either%20changing%20the%20input%20to%20the%20diffusion%20process%2C%20thereby%20preventing%20the%0ADM%20from%20generating%20memorized%20samples%20during%20inference%2C%20or%20removing%20the%0Amemorized%20data%20from%20training%20altogether.%20While%20those%20are%20viable%20solutions%20when%0Athe%20DM%20is%20developed%20and%20deployed%20in%20a%20secure%20and%20constantly%20monitored%0Aenvironment%2C%20they%20hold%20the%20risk%20of%20adversaries%20circumventing%20the%20safeguards%20and%0Aare%20not%20effective%20when%20the%20DM%20itself%20is%20publicly%20released.%20To%20solve%20the%0Aproblem%2C%20we%20introduce%20NeMo%2C%20the%20first%20method%20to%20localize%20memorization%20of%0Aindividual%20data%20samples%20down%20to%20the%20level%20of%20neurons%20in%20DMs%27%20cross-attention%0Alayers.%20Through%20our%20experiments%2C%20we%20make%20the%20intriguing%20finding%20that%20in%20many%0Acases%2C%20single%20neurons%20are%20responsible%20for%20memorizing%20particular%20training%0Asamples.%20By%20deactivating%20these%20memorization%20neurons%2C%20we%20can%20avoid%20the%0Areplication%20of%20training%20data%20at%20inference%20time%2C%20increase%20the%20diversity%20in%20the%0Agenerated%20outputs%2C%20and%20mitigate%20the%20leakage%20of%20private%20and%20copyrighted%20data.%20In%0Athis%20way%2C%20our%20NeMo%20contributes%20to%20a%20more%20responsible%20deployment%20of%20DMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinding%2520NeMo%253A%2520Localizing%2520Neurons%2520Responsible%2520For%2520Memorization%2520in%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DDominik%2520Hintersdorf%2520and%2520Lukas%2520Struppek%2520and%2520Kristian%2520Kersting%2520and%2520Adam%2520Dziedzic%2520and%2520Franziska%2520Boenisch%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520produce%2520very%2520detailed%2520and%2520high-quality%2520images.%2520Their%250Apower%2520results%2520from%2520extensive%2520training%2520on%2520large%2520amounts%2520of%2520data%252C%2520usually%2520scraped%250Afrom%2520the%2520internet%2520without%2520proper%2520attribution%2520or%2520consent%2520from%2520content%2520creators.%250AUnfortunately%252C%2520this%2520practice%2520raises%2520privacy%2520and%2520intellectual%2520property%2520concerns%252C%250Aas%2520DMs%2520can%2520memorize%2520and%2520later%2520reproduce%2520their%2520potentially%2520sensitive%2520or%250Acopyrighted%2520training%2520images%2520at%2520inference%2520time.%2520Prior%2520efforts%2520prevent%2520this%2520issue%250Aby%2520either%2520changing%2520the%2520input%2520to%2520the%2520diffusion%2520process%252C%2520thereby%2520preventing%2520the%250ADM%2520from%2520generating%2520memorized%2520samples%2520during%2520inference%252C%2520or%2520removing%2520the%250Amemorized%2520data%2520from%2520training%2520altogether.%2520While%2520those%2520are%2520viable%2520solutions%2520when%250Athe%2520DM%2520is%2520developed%2520and%2520deployed%2520in%2520a%2520secure%2520and%2520constantly%2520monitored%250Aenvironment%252C%2520they%2520hold%2520the%2520risk%2520of%2520adversaries%2520circumventing%2520the%2520safeguards%2520and%250Aare%2520not%2520effective%2520when%2520the%2520DM%2520itself%2520is%2520publicly%2520released.%2520To%2520solve%2520the%250Aproblem%252C%2520we%2520introduce%2520NeMo%252C%2520the%2520first%2520method%2520to%2520localize%2520memorization%2520of%250Aindividual%2520data%2520samples%2520down%2520to%2520the%2520level%2520of%2520neurons%2520in%2520DMs%2527%2520cross-attention%250Alayers.%2520Through%2520our%2520experiments%252C%2520we%2520make%2520the%2520intriguing%2520finding%2520that%2520in%2520many%250Acases%252C%2520single%2520neurons%2520are%2520responsible%2520for%2520memorizing%2520particular%2520training%250Asamples.%2520By%2520deactivating%2520these%2520memorization%2520neurons%252C%2520we%2520can%2520avoid%2520the%250Areplication%2520of%2520training%2520data%2520at%2520inference%2520time%252C%2520increase%2520the%2520diversity%2520in%2520the%250Agenerated%2520outputs%252C%2520and%2520mitigate%2520the%2520leakage%2520of%2520private%2520and%2520copyrighted%2520data.%2520In%250Athis%2520way%252C%2520our%2520NeMo%2520contributes%2520to%2520a%2520more%2520responsible%2520deployment%2520of%2520DMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20NeMo%3A%20Localizing%20Neurons%20Responsible%20For%20Memorization%20in%0A%20%20Diffusion%20Models&entry.906535625=Dominik%20Hintersdorf%20and%20Lukas%20Struppek%20and%20Kristian%20Kersting%20and%20Adam%20Dziedzic%20and%20Franziska%20Boenisch&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20produce%20very%20detailed%20and%20high-quality%20images.%20Their%0Apower%20results%20from%20extensive%20training%20on%20large%20amounts%20of%20data%2C%20usually%20scraped%0Afrom%20the%20internet%20without%20proper%20attribution%20or%20consent%20from%20content%20creators.%0AUnfortunately%2C%20this%20practice%20raises%20privacy%20and%20intellectual%20property%20concerns%2C%0Aas%20DMs%20can%20memorize%20and%20later%20reproduce%20their%20potentially%20sensitive%20or%0Acopyrighted%20training%20images%20at%20inference%20time.%20Prior%20efforts%20prevent%20this%20issue%0Aby%20either%20changing%20the%20input%20to%20the%20diffusion%20process%2C%20thereby%20preventing%20the%0ADM%20from%20generating%20memorized%20samples%20during%20inference%2C%20or%20removing%20the%0Amemorized%20data%20from%20training%20altogether.%20While%20those%20are%20viable%20solutions%20when%0Athe%20DM%20is%20developed%20and%20deployed%20in%20a%20secure%20and%20constantly%20monitored%0Aenvironment%2C%20they%20hold%20the%20risk%20of%20adversaries%20circumventing%20the%20safeguards%20and%0Aare%20not%20effective%20when%20the%20DM%20itself%20is%20publicly%20released.%20To%20solve%20the%0Aproblem%2C%20we%20introduce%20NeMo%2C%20the%20first%20method%20to%20localize%20memorization%20of%0Aindividual%20data%20samples%20down%20to%20the%20level%20of%20neurons%20in%20DMs%27%20cross-attention%0Alayers.%20Through%20our%20experiments%2C%20we%20make%20the%20intriguing%20finding%20that%20in%20many%0Acases%2C%20single%20neurons%20are%20responsible%20for%20memorizing%20particular%20training%0Asamples.%20By%20deactivating%20these%20memorization%20neurons%2C%20we%20can%20avoid%20the%0Areplication%20of%20training%20data%20at%20inference%20time%2C%20increase%20the%20diversity%20in%20the%0Agenerated%20outputs%2C%20and%20mitigate%20the%20leakage%20of%20private%20and%20copyrighted%20data.%20In%0Athis%20way%2C%20our%20NeMo%20contributes%20to%20a%20more%20responsible%20deployment%20of%20DMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02366v1&entry.124074799=Read"},
{"title": "Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal\n  Learning", "author": "Alex Jinpeng Wang and Linjie Li and Yiqi Lin and Min Li and Lijuan Wang and Mike Zheng Shou", "abstract": "  Training models with longer in-context lengths is a significant challenge for\nmultimodal model due to substantial GPU memory and computational costs. This\nexploratory study does not present state-of-the-art models; rather, it\nintroduces an innovative method designed to increase in-context text length in\nmulti-modality large language models (MLLMs) efficiently. We present Visualized\nIn-Context Text Processing (VisInContext), which processes long in-context text\nusing visual tokens. This technique significantly reduces GPU memory usage and\nfloating point operations (FLOPs) for both training and inferenceing stage. For\ninstance, our method expands the pre-training in-context text length from 256\nto 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model.\nExperimental results demonstrate that model trained with VisInContext delivers\nsuperior performance on common downstream benchmarks for in-context few-shot\nevaluation. Additionally, VisInContext is complementary to existing methods for\nincreasing in-context text length and enhances document understanding\ncapabilities, showing great potential in document QA tasks and sequential\ndocument retrieval.\n", "link": "http://arxiv.org/abs/2406.02547v1", "date": "2024-06-04", "relevancy": 2.1475, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5552}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5406}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Visual%20Tokens%20for%20Extended%20Text%20Contexts%20in%20Multi-Modal%0A%20%20Learning&body=Title%3A%20Leveraging%20Visual%20Tokens%20for%20Extended%20Text%20Contexts%20in%20Multi-Modal%0A%20%20Learning%0AAuthor%3A%20Alex%20Jinpeng%20Wang%20and%20Linjie%20Li%20and%20Yiqi%20Lin%20and%20Min%20Li%20and%20Lijuan%20Wang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Training%20models%20with%20longer%20in-context%20lengths%20is%20a%20significant%20challenge%20for%0Amultimodal%20model%20due%20to%20substantial%20GPU%20memory%20and%20computational%20costs.%20This%0Aexploratory%20study%20does%20not%20present%20state-of-the-art%20models%3B%20rather%2C%20it%0Aintroduces%20an%20innovative%20method%20designed%20to%20increase%20in-context%20text%20length%20in%0Amulti-modality%20large%20language%20models%20%28MLLMs%29%20efficiently.%20We%20present%20Visualized%0AIn-Context%20Text%20Processing%20%28VisInContext%29%2C%20which%20processes%20long%20in-context%20text%0Ausing%20visual%20tokens.%20This%20technique%20significantly%20reduces%20GPU%20memory%20usage%20and%0Afloating%20point%20operations%20%28FLOPs%29%20for%20both%20training%20and%20inferenceing%20stage.%20For%0Ainstance%2C%20our%20method%20expands%20the%20pre-training%20in-context%20text%20length%20from%20256%0Ato%202048%20tokens%20with%20nearly%20same%20FLOPs%20for%20a%2056%20billion%20parameter%20MOE%20model.%0AExperimental%20results%20demonstrate%20that%20model%20trained%20with%20VisInContext%20delivers%0Asuperior%20performance%20on%20common%20downstream%20benchmarks%20for%20in-context%20few-shot%0Aevaluation.%20Additionally%2C%20VisInContext%20is%20complementary%20to%20existing%20methods%20for%0Aincreasing%20in-context%20text%20length%20and%20enhances%20document%20understanding%0Acapabilities%2C%20showing%20great%20potential%20in%20document%20QA%20tasks%20and%20sequential%0Adocument%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Visual%2520Tokens%2520for%2520Extended%2520Text%2520Contexts%2520in%2520Multi-Modal%250A%2520%2520Learning%26entry.906535625%3DAlex%2520Jinpeng%2520Wang%2520and%2520Linjie%2520Li%2520and%2520Yiqi%2520Lin%2520and%2520Min%2520Li%2520and%2520Lijuan%2520Wang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Training%2520models%2520with%2520longer%2520in-context%2520lengths%2520is%2520a%2520significant%2520challenge%2520for%250Amultimodal%2520model%2520due%2520to%2520substantial%2520GPU%2520memory%2520and%2520computational%2520costs.%2520This%250Aexploratory%2520study%2520does%2520not%2520present%2520state-of-the-art%2520models%253B%2520rather%252C%2520it%250Aintroduces%2520an%2520innovative%2520method%2520designed%2520to%2520increase%2520in-context%2520text%2520length%2520in%250Amulti-modality%2520large%2520language%2520models%2520%2528MLLMs%2529%2520efficiently.%2520We%2520present%2520Visualized%250AIn-Context%2520Text%2520Processing%2520%2528VisInContext%2529%252C%2520which%2520processes%2520long%2520in-context%2520text%250Ausing%2520visual%2520tokens.%2520This%2520technique%2520significantly%2520reduces%2520GPU%2520memory%2520usage%2520and%250Afloating%2520point%2520operations%2520%2528FLOPs%2529%2520for%2520both%2520training%2520and%2520inferenceing%2520stage.%2520For%250Ainstance%252C%2520our%2520method%2520expands%2520the%2520pre-training%2520in-context%2520text%2520length%2520from%2520256%250Ato%25202048%2520tokens%2520with%2520nearly%2520same%2520FLOPs%2520for%2520a%252056%2520billion%2520parameter%2520MOE%2520model.%250AExperimental%2520results%2520demonstrate%2520that%2520model%2520trained%2520with%2520VisInContext%2520delivers%250Asuperior%2520performance%2520on%2520common%2520downstream%2520benchmarks%2520for%2520in-context%2520few-shot%250Aevaluation.%2520Additionally%252C%2520VisInContext%2520is%2520complementary%2520to%2520existing%2520methods%2520for%250Aincreasing%2520in-context%2520text%2520length%2520and%2520enhances%2520document%2520understanding%250Acapabilities%252C%2520showing%2520great%2520potential%2520in%2520document%2520QA%2520tasks%2520and%2520sequential%250Adocument%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Visual%20Tokens%20for%20Extended%20Text%20Contexts%20in%20Multi-Modal%0A%20%20Learning&entry.906535625=Alex%20Jinpeng%20Wang%20and%20Linjie%20Li%20and%20Yiqi%20Lin%20and%20Min%20Li%20and%20Lijuan%20Wang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Training%20models%20with%20longer%20in-context%20lengths%20is%20a%20significant%20challenge%20for%0Amultimodal%20model%20due%20to%20substantial%20GPU%20memory%20and%20computational%20costs.%20This%0Aexploratory%20study%20does%20not%20present%20state-of-the-art%20models%3B%20rather%2C%20it%0Aintroduces%20an%20innovative%20method%20designed%20to%20increase%20in-context%20text%20length%20in%0Amulti-modality%20large%20language%20models%20%28MLLMs%29%20efficiently.%20We%20present%20Visualized%0AIn-Context%20Text%20Processing%20%28VisInContext%29%2C%20which%20processes%20long%20in-context%20text%0Ausing%20visual%20tokens.%20This%20technique%20significantly%20reduces%20GPU%20memory%20usage%20and%0Afloating%20point%20operations%20%28FLOPs%29%20for%20both%20training%20and%20inferenceing%20stage.%20For%0Ainstance%2C%20our%20method%20expands%20the%20pre-training%20in-context%20text%20length%20from%20256%0Ato%202048%20tokens%20with%20nearly%20same%20FLOPs%20for%20a%2056%20billion%20parameter%20MOE%20model.%0AExperimental%20results%20demonstrate%20that%20model%20trained%20with%20VisInContext%20delivers%0Asuperior%20performance%20on%20common%20downstream%20benchmarks%20for%20in-context%20few-shot%0Aevaluation.%20Additionally%2C%20VisInContext%20is%20complementary%20to%20existing%20methods%20for%0Aincreasing%20in-context%20text%20length%20and%20enhances%20document%20understanding%0Acapabilities%2C%20showing%20great%20potential%20in%20document%20QA%20tasks%20and%20sequential%0Adocument%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02547v1&entry.124074799=Read"},
{"title": "Reweighted Solutions for Weighted Low Rank Approximation", "author": "David P. Woodruff and Taisuke Yasuda", "abstract": "  Weighted low rank approximation (WLRA) is an important yet computationally\nchallenging primitive with applications ranging from statistical analysis,\nmodel compression, and signal processing. To cope with the NP-hardness of this\nproblem, prior work considers heuristics, bicriteria, or fixed parameter\ntractable algorithms to solve this problem. In this work, we introduce a new\nrelaxed solution to WLRA which outputs a matrix that is not necessarily low\nrank, but can be stored using very few parameters and gives provable\napproximation guarantees when the weight matrix has low rank. Our central idea\nis to use the weight matrix itself to reweight a low rank solution, which gives\nan extremely simple algorithm with remarkable empirical performance in\napplications to model compression and on synthetic datasets. Our algorithm also\ngives nearly optimal communication complexity bounds for a natural distributed\nproblem associated with this problem, for which we show matching communication\nlower bounds. Together, our communication complexity bounds show that the rank\nof the weight matrix provably parameterizes the communication complexity of\nWLRA. We also obtain the first relative error guarantees for feature selection\nwith a weighted objective.\n", "link": "http://arxiv.org/abs/2406.02431v1", "date": "2024-06-04", "relevancy": 2.143, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4317}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4301}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reweighted%20Solutions%20for%20Weighted%20Low%20Rank%20Approximation&body=Title%3A%20Reweighted%20Solutions%20for%20Weighted%20Low%20Rank%20Approximation%0AAuthor%3A%20David%20P.%20Woodruff%20and%20Taisuke%20Yasuda%0AAbstract%3A%20%20%20Weighted%20low%20rank%20approximation%20%28WLRA%29%20is%20an%20important%20yet%20computationally%0Achallenging%20primitive%20with%20applications%20ranging%20from%20statistical%20analysis%2C%0Amodel%20compression%2C%20and%20signal%20processing.%20To%20cope%20with%20the%20NP-hardness%20of%20this%0Aproblem%2C%20prior%20work%20considers%20heuristics%2C%20bicriteria%2C%20or%20fixed%20parameter%0Atractable%20algorithms%20to%20solve%20this%20problem.%20In%20this%20work%2C%20we%20introduce%20a%20new%0Arelaxed%20solution%20to%20WLRA%20which%20outputs%20a%20matrix%20that%20is%20not%20necessarily%20low%0Arank%2C%20but%20can%20be%20stored%20using%20very%20few%20parameters%20and%20gives%20provable%0Aapproximation%20guarantees%20when%20the%20weight%20matrix%20has%20low%20rank.%20Our%20central%20idea%0Ais%20to%20use%20the%20weight%20matrix%20itself%20to%20reweight%20a%20low%20rank%20solution%2C%20which%20gives%0Aan%20extremely%20simple%20algorithm%20with%20remarkable%20empirical%20performance%20in%0Aapplications%20to%20model%20compression%20and%20on%20synthetic%20datasets.%20Our%20algorithm%20also%0Agives%20nearly%20optimal%20communication%20complexity%20bounds%20for%20a%20natural%20distributed%0Aproblem%20associated%20with%20this%20problem%2C%20for%20which%20we%20show%20matching%20communication%0Alower%20bounds.%20Together%2C%20our%20communication%20complexity%20bounds%20show%20that%20the%20rank%0Aof%20the%20weight%20matrix%20provably%20parameterizes%20the%20communication%20complexity%20of%0AWLRA.%20We%20also%20obtain%20the%20first%20relative%20error%20guarantees%20for%20feature%20selection%0Awith%20a%20weighted%20objective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReweighted%2520Solutions%2520for%2520Weighted%2520Low%2520Rank%2520Approximation%26entry.906535625%3DDavid%2520P.%2520Woodruff%2520and%2520Taisuke%2520Yasuda%26entry.1292438233%3D%2520%2520Weighted%2520low%2520rank%2520approximation%2520%2528WLRA%2529%2520is%2520an%2520important%2520yet%2520computationally%250Achallenging%2520primitive%2520with%2520applications%2520ranging%2520from%2520statistical%2520analysis%252C%250Amodel%2520compression%252C%2520and%2520signal%2520processing.%2520To%2520cope%2520with%2520the%2520NP-hardness%2520of%2520this%250Aproblem%252C%2520prior%2520work%2520considers%2520heuristics%252C%2520bicriteria%252C%2520or%2520fixed%2520parameter%250Atractable%2520algorithms%2520to%2520solve%2520this%2520problem.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%250Arelaxed%2520solution%2520to%2520WLRA%2520which%2520outputs%2520a%2520matrix%2520that%2520is%2520not%2520necessarily%2520low%250Arank%252C%2520but%2520can%2520be%2520stored%2520using%2520very%2520few%2520parameters%2520and%2520gives%2520provable%250Aapproximation%2520guarantees%2520when%2520the%2520weight%2520matrix%2520has%2520low%2520rank.%2520Our%2520central%2520idea%250Ais%2520to%2520use%2520the%2520weight%2520matrix%2520itself%2520to%2520reweight%2520a%2520low%2520rank%2520solution%252C%2520which%2520gives%250Aan%2520extremely%2520simple%2520algorithm%2520with%2520remarkable%2520empirical%2520performance%2520in%250Aapplications%2520to%2520model%2520compression%2520and%2520on%2520synthetic%2520datasets.%2520Our%2520algorithm%2520also%250Agives%2520nearly%2520optimal%2520communication%2520complexity%2520bounds%2520for%2520a%2520natural%2520distributed%250Aproblem%2520associated%2520with%2520this%2520problem%252C%2520for%2520which%2520we%2520show%2520matching%2520communication%250Alower%2520bounds.%2520Together%252C%2520our%2520communication%2520complexity%2520bounds%2520show%2520that%2520the%2520rank%250Aof%2520the%2520weight%2520matrix%2520provably%2520parameterizes%2520the%2520communication%2520complexity%2520of%250AWLRA.%2520We%2520also%2520obtain%2520the%2520first%2520relative%2520error%2520guarantees%2520for%2520feature%2520selection%250Awith%2520a%2520weighted%2520objective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reweighted%20Solutions%20for%20Weighted%20Low%20Rank%20Approximation&entry.906535625=David%20P.%20Woodruff%20and%20Taisuke%20Yasuda&entry.1292438233=%20%20Weighted%20low%20rank%20approximation%20%28WLRA%29%20is%20an%20important%20yet%20computationally%0Achallenging%20primitive%20with%20applications%20ranging%20from%20statistical%20analysis%2C%0Amodel%20compression%2C%20and%20signal%20processing.%20To%20cope%20with%20the%20NP-hardness%20of%20this%0Aproblem%2C%20prior%20work%20considers%20heuristics%2C%20bicriteria%2C%20or%20fixed%20parameter%0Atractable%20algorithms%20to%20solve%20this%20problem.%20In%20this%20work%2C%20we%20introduce%20a%20new%0Arelaxed%20solution%20to%20WLRA%20which%20outputs%20a%20matrix%20that%20is%20not%20necessarily%20low%0Arank%2C%20but%20can%20be%20stored%20using%20very%20few%20parameters%20and%20gives%20provable%0Aapproximation%20guarantees%20when%20the%20weight%20matrix%20has%20low%20rank.%20Our%20central%20idea%0Ais%20to%20use%20the%20weight%20matrix%20itself%20to%20reweight%20a%20low%20rank%20solution%2C%20which%20gives%0Aan%20extremely%20simple%20algorithm%20with%20remarkable%20empirical%20performance%20in%0Aapplications%20to%20model%20compression%20and%20on%20synthetic%20datasets.%20Our%20algorithm%20also%0Agives%20nearly%20optimal%20communication%20complexity%20bounds%20for%20a%20natural%20distributed%0Aproblem%20associated%20with%20this%20problem%2C%20for%20which%20we%20show%20matching%20communication%0Alower%20bounds.%20Together%2C%20our%20communication%20complexity%20bounds%20show%20that%20the%20rank%0Aof%20the%20weight%20matrix%20provably%20parameterizes%20the%20communication%20complexity%20of%0AWLRA.%20We%20also%20obtain%20the%20first%20relative%20error%20guarantees%20for%20feature%20selection%0Awith%20a%20weighted%20objective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02431v1&entry.124074799=Read"},
{"title": "Label-wise Aleatoric and Epistemic Uncertainty Quantification", "author": "Yusuf Sale and Paul Hofman and Timo L\u00f6hr and Lisa Wimmer and Thomas Nagler and Eyke H\u00fcllermeier", "abstract": "  We present a novel approach to uncertainty quantification in classification\ntasks based on label-wise decomposition of uncertainty measures. This\nlabel-wise perspective allows uncertainty to be quantified at the individual\nclass level, thereby improving cost-sensitive decision-making and helping\nunderstand the sources of uncertainty. Furthermore, it allows to define total,\naleatoric, and epistemic uncertainty on the basis of non-categorical measures\nsuch as variance, going beyond common entropy-based measures. In particular,\nvariance-based measures address some of the limitations associated with\nestablished methods that have recently been discussed in the literature. We\nshow that our proposed measures adhere to a number of desirable properties.\nThrough empirical evaluation on a variety of benchmark data sets -- including\napplications in the medical domain where accurate uncertainty quantification is\ncrucial -- we establish the effectiveness of label-wise uncertainty\nquantification.\n", "link": "http://arxiv.org/abs/2406.02354v1", "date": "2024-06-04", "relevancy": 2.137, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6123}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5249}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label-wise%20Aleatoric%20and%20Epistemic%20Uncertainty%20Quantification&body=Title%3A%20Label-wise%20Aleatoric%20and%20Epistemic%20Uncertainty%20Quantification%0AAuthor%3A%20Yusuf%20Sale%20and%20Paul%20Hofman%20and%20Timo%20L%C3%B6hr%20and%20Lisa%20Wimmer%20and%20Thomas%20Nagler%20and%20Eyke%20H%C3%BCllermeier%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20uncertainty%20quantification%20in%20classification%0Atasks%20based%20on%20label-wise%20decomposition%20of%20uncertainty%20measures.%20This%0Alabel-wise%20perspective%20allows%20uncertainty%20to%20be%20quantified%20at%20the%20individual%0Aclass%20level%2C%20thereby%20improving%20cost-sensitive%20decision-making%20and%20helping%0Aunderstand%20the%20sources%20of%20uncertainty.%20Furthermore%2C%20it%20allows%20to%20define%20total%2C%0Aaleatoric%2C%20and%20epistemic%20uncertainty%20on%20the%20basis%20of%20non-categorical%20measures%0Asuch%20as%20variance%2C%20going%20beyond%20common%20entropy-based%20measures.%20In%20particular%2C%0Avariance-based%20measures%20address%20some%20of%20the%20limitations%20associated%20with%0Aestablished%20methods%20that%20have%20recently%20been%20discussed%20in%20the%20literature.%20We%0Ashow%20that%20our%20proposed%20measures%20adhere%20to%20a%20number%20of%20desirable%20properties.%0AThrough%20empirical%20evaluation%20on%20a%20variety%20of%20benchmark%20data%20sets%20--%20including%0Aapplications%20in%20the%20medical%20domain%20where%20accurate%20uncertainty%20quantification%20is%0Acrucial%20--%20we%20establish%20the%20effectiveness%20of%20label-wise%20uncertainty%0Aquantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel-wise%2520Aleatoric%2520and%2520Epistemic%2520Uncertainty%2520Quantification%26entry.906535625%3DYusuf%2520Sale%2520and%2520Paul%2520Hofman%2520and%2520Timo%2520L%25C3%25B6hr%2520and%2520Lisa%2520Wimmer%2520and%2520Thomas%2520Nagler%2520and%2520Eyke%2520H%25C3%25BCllermeier%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520uncertainty%2520quantification%2520in%2520classification%250Atasks%2520based%2520on%2520label-wise%2520decomposition%2520of%2520uncertainty%2520measures.%2520This%250Alabel-wise%2520perspective%2520allows%2520uncertainty%2520to%2520be%2520quantified%2520at%2520the%2520individual%250Aclass%2520level%252C%2520thereby%2520improving%2520cost-sensitive%2520decision-making%2520and%2520helping%250Aunderstand%2520the%2520sources%2520of%2520uncertainty.%2520Furthermore%252C%2520it%2520allows%2520to%2520define%2520total%252C%250Aaleatoric%252C%2520and%2520epistemic%2520uncertainty%2520on%2520the%2520basis%2520of%2520non-categorical%2520measures%250Asuch%2520as%2520variance%252C%2520going%2520beyond%2520common%2520entropy-based%2520measures.%2520In%2520particular%252C%250Avariance-based%2520measures%2520address%2520some%2520of%2520the%2520limitations%2520associated%2520with%250Aestablished%2520methods%2520that%2520have%2520recently%2520been%2520discussed%2520in%2520the%2520literature.%2520We%250Ashow%2520that%2520our%2520proposed%2520measures%2520adhere%2520to%2520a%2520number%2520of%2520desirable%2520properties.%250AThrough%2520empirical%2520evaluation%2520on%2520a%2520variety%2520of%2520benchmark%2520data%2520sets%2520--%2520including%250Aapplications%2520in%2520the%2520medical%2520domain%2520where%2520accurate%2520uncertainty%2520quantification%2520is%250Acrucial%2520--%2520we%2520establish%2520the%2520effectiveness%2520of%2520label-wise%2520uncertainty%250Aquantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-wise%20Aleatoric%20and%20Epistemic%20Uncertainty%20Quantification&entry.906535625=Yusuf%20Sale%20and%20Paul%20Hofman%20and%20Timo%20L%C3%B6hr%20and%20Lisa%20Wimmer%20and%20Thomas%20Nagler%20and%20Eyke%20H%C3%BCllermeier&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20uncertainty%20quantification%20in%20classification%0Atasks%20based%20on%20label-wise%20decomposition%20of%20uncertainty%20measures.%20This%0Alabel-wise%20perspective%20allows%20uncertainty%20to%20be%20quantified%20at%20the%20individual%0Aclass%20level%2C%20thereby%20improving%20cost-sensitive%20decision-making%20and%20helping%0Aunderstand%20the%20sources%20of%20uncertainty.%20Furthermore%2C%20it%20allows%20to%20define%20total%2C%0Aaleatoric%2C%20and%20epistemic%20uncertainty%20on%20the%20basis%20of%20non-categorical%20measures%0Asuch%20as%20variance%2C%20going%20beyond%20common%20entropy-based%20measures.%20In%20particular%2C%0Avariance-based%20measures%20address%20some%20of%20the%20limitations%20associated%20with%0Aestablished%20methods%20that%20have%20recently%20been%20discussed%20in%20the%20literature.%20We%0Ashow%20that%20our%20proposed%20measures%20adhere%20to%20a%20number%20of%20desirable%20properties.%0AThrough%20empirical%20evaluation%20on%20a%20variety%20of%20benchmark%20data%20sets%20--%20including%0Aapplications%20in%20the%20medical%20domain%20where%20accurate%20uncertainty%20quantification%20is%0Acrucial%20--%20we%20establish%20the%20effectiveness%20of%20label-wise%20uncertainty%0Aquantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02354v1&entry.124074799=Read"},
{"title": "To Believe or Not to Believe Your LLM", "author": "Yasin Abbasi Yadkori and Ilja Kuzborskij and Andr\u00e1s Gy\u00f6rgy and Csaba Szepesv\u00e1ri", "abstract": "  We explore uncertainty quantification in large language models (LLMs), with\nthe goal to identify when uncertainty in responses given a query is large. We\nsimultaneously consider both epistemic and aleatoric uncertainties, where the\nformer comes from the lack of knowledge about the ground truth (such as about\nfacts or the language), and the latter comes from irreducible randomness (such\nas multiple possible answers). In particular, we derive an\ninformation-theoretic metric that allows to reliably detect when only epistemic\nuncertainty is large, in which case the output of the model is unreliable. This\ncondition can be computed based solely on the output of the model obtained\nsimply by some special iterative prompting based on the previous responses.\nSuch quantification, for instance, allows to detect hallucinations (cases when\nepistemic uncertainty is high) in both single- and multi-answer responses. This\nis in contrast to many standard uncertainty quantification strategies (such as\nthresholding the log-likelihood of a response) where hallucinations in the\nmulti-answer case cannot be detected. We conduct a series of experiments which\ndemonstrate the advantage of our formulation. Further, our investigations shed\nsome light on how the probabilities assigned to a given output by an LLM can be\namplified by iterative prompting, which might be of independent interest.\n", "link": "http://arxiv.org/abs/2406.02543v1", "date": "2024-06-04", "relevancy": 2.1362, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5742}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5464}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Believe%20or%20Not%20to%20Believe%20Your%20LLM&body=Title%3A%20To%20Believe%20or%20Not%20to%20Believe%20Your%20LLM%0AAuthor%3A%20Yasin%20Abbasi%20Yadkori%20and%20Ilja%20Kuzborskij%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Csaba%20Szepesv%C3%A1ri%0AAbstract%3A%20%20%20We%20explore%20uncertainty%20quantification%20in%20large%20language%20models%20%28LLMs%29%2C%20with%0Athe%20goal%20to%20identify%20when%20uncertainty%20in%20responses%20given%20a%20query%20is%20large.%20We%0Asimultaneously%20consider%20both%20epistemic%20and%20aleatoric%20uncertainties%2C%20where%20the%0Aformer%20comes%20from%20the%20lack%20of%20knowledge%20about%20the%20ground%20truth%20%28such%20as%20about%0Afacts%20or%20the%20language%29%2C%20and%20the%20latter%20comes%20from%20irreducible%20randomness%20%28such%0Aas%20multiple%20possible%20answers%29.%20In%20particular%2C%20we%20derive%20an%0Ainformation-theoretic%20metric%20that%20allows%20to%20reliably%20detect%20when%20only%20epistemic%0Auncertainty%20is%20large%2C%20in%20which%20case%20the%20output%20of%20the%20model%20is%20unreliable.%20This%0Acondition%20can%20be%20computed%20based%20solely%20on%20the%20output%20of%20the%20model%20obtained%0Asimply%20by%20some%20special%20iterative%20prompting%20based%20on%20the%20previous%20responses.%0ASuch%20quantification%2C%20for%20instance%2C%20allows%20to%20detect%20hallucinations%20%28cases%20when%0Aepistemic%20uncertainty%20is%20high%29%20in%20both%20single-%20and%20multi-answer%20responses.%20This%0Ais%20in%20contrast%20to%20many%20standard%20uncertainty%20quantification%20strategies%20%28such%20as%0Athresholding%20the%20log-likelihood%20of%20a%20response%29%20where%20hallucinations%20in%20the%0Amulti-answer%20case%20cannot%20be%20detected.%20We%20conduct%20a%20series%20of%20experiments%20which%0Ademonstrate%20the%20advantage%20of%20our%20formulation.%20Further%2C%20our%20investigations%20shed%0Asome%20light%20on%20how%20the%20probabilities%20assigned%20to%20a%20given%20output%20by%20an%20LLM%20can%20be%0Aamplified%20by%20iterative%20prompting%2C%20which%20might%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Believe%2520or%2520Not%2520to%2520Believe%2520Your%2520LLM%26entry.906535625%3DYasin%2520Abbasi%2520Yadkori%2520and%2520Ilja%2520Kuzborskij%2520and%2520Andr%25C3%25A1s%2520Gy%25C3%25B6rgy%2520and%2520Csaba%2520Szepesv%25C3%25A1ri%26entry.1292438233%3D%2520%2520We%2520explore%2520uncertainty%2520quantification%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%250Athe%2520goal%2520to%2520identify%2520when%2520uncertainty%2520in%2520responses%2520given%2520a%2520query%2520is%2520large.%2520We%250Asimultaneously%2520consider%2520both%2520epistemic%2520and%2520aleatoric%2520uncertainties%252C%2520where%2520the%250Aformer%2520comes%2520from%2520the%2520lack%2520of%2520knowledge%2520about%2520the%2520ground%2520truth%2520%2528such%2520as%2520about%250Afacts%2520or%2520the%2520language%2529%252C%2520and%2520the%2520latter%2520comes%2520from%2520irreducible%2520randomness%2520%2528such%250Aas%2520multiple%2520possible%2520answers%2529.%2520In%2520particular%252C%2520we%2520derive%2520an%250Ainformation-theoretic%2520metric%2520that%2520allows%2520to%2520reliably%2520detect%2520when%2520only%2520epistemic%250Auncertainty%2520is%2520large%252C%2520in%2520which%2520case%2520the%2520output%2520of%2520the%2520model%2520is%2520unreliable.%2520This%250Acondition%2520can%2520be%2520computed%2520based%2520solely%2520on%2520the%2520output%2520of%2520the%2520model%2520obtained%250Asimply%2520by%2520some%2520special%2520iterative%2520prompting%2520based%2520on%2520the%2520previous%2520responses.%250ASuch%2520quantification%252C%2520for%2520instance%252C%2520allows%2520to%2520detect%2520hallucinations%2520%2528cases%2520when%250Aepistemic%2520uncertainty%2520is%2520high%2529%2520in%2520both%2520single-%2520and%2520multi-answer%2520responses.%2520This%250Ais%2520in%2520contrast%2520to%2520many%2520standard%2520uncertainty%2520quantification%2520strategies%2520%2528such%2520as%250Athresholding%2520the%2520log-likelihood%2520of%2520a%2520response%2529%2520where%2520hallucinations%2520in%2520the%250Amulti-answer%2520case%2520cannot%2520be%2520detected.%2520We%2520conduct%2520a%2520series%2520of%2520experiments%2520which%250Ademonstrate%2520the%2520advantage%2520of%2520our%2520formulation.%2520Further%252C%2520our%2520investigations%2520shed%250Asome%2520light%2520on%2520how%2520the%2520probabilities%2520assigned%2520to%2520a%2520given%2520output%2520by%2520an%2520LLM%2520can%2520be%250Aamplified%2520by%2520iterative%2520prompting%252C%2520which%2520might%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Believe%20or%20Not%20to%20Believe%20Your%20LLM&entry.906535625=Yasin%20Abbasi%20Yadkori%20and%20Ilja%20Kuzborskij%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Csaba%20Szepesv%C3%A1ri&entry.1292438233=%20%20We%20explore%20uncertainty%20quantification%20in%20large%20language%20models%20%28LLMs%29%2C%20with%0Athe%20goal%20to%20identify%20when%20uncertainty%20in%20responses%20given%20a%20query%20is%20large.%20We%0Asimultaneously%20consider%20both%20epistemic%20and%20aleatoric%20uncertainties%2C%20where%20the%0Aformer%20comes%20from%20the%20lack%20of%20knowledge%20about%20the%20ground%20truth%20%28such%20as%20about%0Afacts%20or%20the%20language%29%2C%20and%20the%20latter%20comes%20from%20irreducible%20randomness%20%28such%0Aas%20multiple%20possible%20answers%29.%20In%20particular%2C%20we%20derive%20an%0Ainformation-theoretic%20metric%20that%20allows%20to%20reliably%20detect%20when%20only%20epistemic%0Auncertainty%20is%20large%2C%20in%20which%20case%20the%20output%20of%20the%20model%20is%20unreliable.%20This%0Acondition%20can%20be%20computed%20based%20solely%20on%20the%20output%20of%20the%20model%20obtained%0Asimply%20by%20some%20special%20iterative%20prompting%20based%20on%20the%20previous%20responses.%0ASuch%20quantification%2C%20for%20instance%2C%20allows%20to%20detect%20hallucinations%20%28cases%20when%0Aepistemic%20uncertainty%20is%20high%29%20in%20both%20single-%20and%20multi-answer%20responses.%20This%0Ais%20in%20contrast%20to%20many%20standard%20uncertainty%20quantification%20strategies%20%28such%20as%0Athresholding%20the%20log-likelihood%20of%20a%20response%29%20where%20hallucinations%20in%20the%0Amulti-answer%20case%20cannot%20be%20detected.%20We%20conduct%20a%20series%20of%20experiments%20which%0Ademonstrate%20the%20advantage%20of%20our%20formulation.%20Further%2C%20our%20investigations%20shed%0Asome%20light%20on%20how%20the%20probabilities%20assigned%20to%20a%20given%20output%20by%20an%20LLM%20can%20be%0Aamplified%20by%20iterative%20prompting%2C%20which%20might%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02543v1&entry.124074799=Read"},
{"title": "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners", "author": "Chengzu Li and Caiqi Zhang and Han Zhou and Nigel Collier and Anna Korhonen and Ivan Vuli\u0107", "abstract": "  Top-view perspective denotes a typical way in which humans read and reason\nover different types of maps, and it is vital for localization and navigation\nof humans as well as of `non-human' agents, such as the ones backed by large\nVision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of\nmodern VLMs remain unattested and underexplored. In this work, we thus study\ntheir capability to understand and reason over spatial relations from the top\nview. The focus on top view also enables controlled evaluations at different\ngranularity of spatial reasoning; we clearly disentangle different abilities\n(e.g., recognizing particular objects versus understanding their relative\npositions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset,\nconsisting of 11,384 multiple-choice questions with either realistic or\nsemantic top-view map as visual input. We then use it to study and evaluate\nVLMs across 4 perception and reasoning tasks with different levels of\ncomplexity. Evaluation of 10 representative open- and closed-source VLMs\nreveals the gap of more than 50% compared to average human performance, and it\nis even lower than the random baseline in some cases. Although additional\nexperiments show that Chain-of-Thought reasoning can boost model capabilities\nby 5.82% on average, the overall performance of VLMs remains limited. Our\nfindings underscore the critical need for enhanced model capability in top-view\nspatial reasoning and set a foundation for further research towards human-level\nproficiency of VLMs in real-world multimodal tasks.\n", "link": "http://arxiv.org/abs/2406.02537v1", "date": "2024-06-04", "relevancy": 2.1189, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5288}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopViewRS%3A%20Vision-Language%20Models%20as%20Top-View%20Spatial%20Reasoners&body=Title%3A%20TopViewRS%3A%20Vision-Language%20Models%20as%20Top-View%20Spatial%20Reasoners%0AAuthor%3A%20Chengzu%20Li%20and%20Caiqi%20Zhang%20and%20Han%20Zhou%20and%20Nigel%20Collier%20and%20Anna%20Korhonen%20and%20Ivan%20Vuli%C4%87%0AAbstract%3A%20%20%20Top-view%20perspective%20denotes%20a%20typical%20way%20in%20which%20humans%20read%20and%20reason%0Aover%20different%20types%20of%20maps%2C%20and%20it%20is%20vital%20for%20localization%20and%20navigation%0Aof%20humans%20as%20well%20as%20of%20%60non-human%27%20agents%2C%20such%20as%20the%20ones%20backed%20by%20large%0AVision-Language%20Models%20%28VLMs%29.%20Nonetheless%2C%20spatial%20reasoning%20capabilities%20of%0Amodern%20VLMs%20remain%20unattested%20and%20underexplored.%20In%20this%20work%2C%20we%20thus%20study%0Atheir%20capability%20to%20understand%20and%20reason%20over%20spatial%20relations%20from%20the%20top%0Aview.%20The%20focus%20on%20top%20view%20also%20enables%20controlled%20evaluations%20at%20different%0Agranularity%20of%20spatial%20reasoning%3B%20we%20clearly%20disentangle%20different%20abilities%0A%28e.g.%2C%20recognizing%20particular%20objects%20versus%20understanding%20their%20relative%0Apositions%29.%20We%20introduce%20the%20TopViewRS%20%28Top-View%20Reasoning%20in%20Space%29%20dataset%2C%0Aconsisting%20of%2011%2C384%20multiple-choice%20questions%20with%20either%20realistic%20or%0Asemantic%20top-view%20map%20as%20visual%20input.%20We%20then%20use%20it%20to%20study%20and%20evaluate%0AVLMs%20across%204%20perception%20and%20reasoning%20tasks%20with%20different%20levels%20of%0Acomplexity.%20Evaluation%20of%2010%20representative%20open-%20and%20closed-source%20VLMs%0Areveals%20the%20gap%20of%20more%20than%2050%25%20compared%20to%20average%20human%20performance%2C%20and%20it%0Ais%20even%20lower%20than%20the%20random%20baseline%20in%20some%20cases.%20Although%20additional%0Aexperiments%20show%20that%20Chain-of-Thought%20reasoning%20can%20boost%20model%20capabilities%0Aby%205.82%25%20on%20average%2C%20the%20overall%20performance%20of%20VLMs%20remains%20limited.%20Our%0Afindings%20underscore%20the%20critical%20need%20for%20enhanced%20model%20capability%20in%20top-view%0Aspatial%20reasoning%20and%20set%20a%20foundation%20for%20further%20research%20towards%20human-level%0Aproficiency%20of%20VLMs%20in%20real-world%20multimodal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopViewRS%253A%2520Vision-Language%2520Models%2520as%2520Top-View%2520Spatial%2520Reasoners%26entry.906535625%3DChengzu%2520Li%2520and%2520Caiqi%2520Zhang%2520and%2520Han%2520Zhou%2520and%2520Nigel%2520Collier%2520and%2520Anna%2520Korhonen%2520and%2520Ivan%2520Vuli%25C4%2587%26entry.1292438233%3D%2520%2520Top-view%2520perspective%2520denotes%2520a%2520typical%2520way%2520in%2520which%2520humans%2520read%2520and%2520reason%250Aover%2520different%2520types%2520of%2520maps%252C%2520and%2520it%2520is%2520vital%2520for%2520localization%2520and%2520navigation%250Aof%2520humans%2520as%2520well%2520as%2520of%2520%2560non-human%2527%2520agents%252C%2520such%2520as%2520the%2520ones%2520backed%2520by%2520large%250AVision-Language%2520Models%2520%2528VLMs%2529.%2520Nonetheless%252C%2520spatial%2520reasoning%2520capabilities%2520of%250Amodern%2520VLMs%2520remain%2520unattested%2520and%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520thus%2520study%250Atheir%2520capability%2520to%2520understand%2520and%2520reason%2520over%2520spatial%2520relations%2520from%2520the%2520top%250Aview.%2520The%2520focus%2520on%2520top%2520view%2520also%2520enables%2520controlled%2520evaluations%2520at%2520different%250Agranularity%2520of%2520spatial%2520reasoning%253B%2520we%2520clearly%2520disentangle%2520different%2520abilities%250A%2528e.g.%252C%2520recognizing%2520particular%2520objects%2520versus%2520understanding%2520their%2520relative%250Apositions%2529.%2520We%2520introduce%2520the%2520TopViewRS%2520%2528Top-View%2520Reasoning%2520in%2520Space%2529%2520dataset%252C%250Aconsisting%2520of%252011%252C384%2520multiple-choice%2520questions%2520with%2520either%2520realistic%2520or%250Asemantic%2520top-view%2520map%2520as%2520visual%2520input.%2520We%2520then%2520use%2520it%2520to%2520study%2520and%2520evaluate%250AVLMs%2520across%25204%2520perception%2520and%2520reasoning%2520tasks%2520with%2520different%2520levels%2520of%250Acomplexity.%2520Evaluation%2520of%252010%2520representative%2520open-%2520and%2520closed-source%2520VLMs%250Areveals%2520the%2520gap%2520of%2520more%2520than%252050%2525%2520compared%2520to%2520average%2520human%2520performance%252C%2520and%2520it%250Ais%2520even%2520lower%2520than%2520the%2520random%2520baseline%2520in%2520some%2520cases.%2520Although%2520additional%250Aexperiments%2520show%2520that%2520Chain-of-Thought%2520reasoning%2520can%2520boost%2520model%2520capabilities%250Aby%25205.82%2525%2520on%2520average%252C%2520the%2520overall%2520performance%2520of%2520VLMs%2520remains%2520limited.%2520Our%250Afindings%2520underscore%2520the%2520critical%2520need%2520for%2520enhanced%2520model%2520capability%2520in%2520top-view%250Aspatial%2520reasoning%2520and%2520set%2520a%2520foundation%2520for%2520further%2520research%2520towards%2520human-level%250Aproficiency%2520of%2520VLMs%2520in%2520real-world%2520multimodal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopViewRS%3A%20Vision-Language%20Models%20as%20Top-View%20Spatial%20Reasoners&entry.906535625=Chengzu%20Li%20and%20Caiqi%20Zhang%20and%20Han%20Zhou%20and%20Nigel%20Collier%20and%20Anna%20Korhonen%20and%20Ivan%20Vuli%C4%87&entry.1292438233=%20%20Top-view%20perspective%20denotes%20a%20typical%20way%20in%20which%20humans%20read%20and%20reason%0Aover%20different%20types%20of%20maps%2C%20and%20it%20is%20vital%20for%20localization%20and%20navigation%0Aof%20humans%20as%20well%20as%20of%20%60non-human%27%20agents%2C%20such%20as%20the%20ones%20backed%20by%20large%0AVision-Language%20Models%20%28VLMs%29.%20Nonetheless%2C%20spatial%20reasoning%20capabilities%20of%0Amodern%20VLMs%20remain%20unattested%20and%20underexplored.%20In%20this%20work%2C%20we%20thus%20study%0Atheir%20capability%20to%20understand%20and%20reason%20over%20spatial%20relations%20from%20the%20top%0Aview.%20The%20focus%20on%20top%20view%20also%20enables%20controlled%20evaluations%20at%20different%0Agranularity%20of%20spatial%20reasoning%3B%20we%20clearly%20disentangle%20different%20abilities%0A%28e.g.%2C%20recognizing%20particular%20objects%20versus%20understanding%20their%20relative%0Apositions%29.%20We%20introduce%20the%20TopViewRS%20%28Top-View%20Reasoning%20in%20Space%29%20dataset%2C%0Aconsisting%20of%2011%2C384%20multiple-choice%20questions%20with%20either%20realistic%20or%0Asemantic%20top-view%20map%20as%20visual%20input.%20We%20then%20use%20it%20to%20study%20and%20evaluate%0AVLMs%20across%204%20perception%20and%20reasoning%20tasks%20with%20different%20levels%20of%0Acomplexity.%20Evaluation%20of%2010%20representative%20open-%20and%20closed-source%20VLMs%0Areveals%20the%20gap%20of%20more%20than%2050%25%20compared%20to%20average%20human%20performance%2C%20and%20it%0Ais%20even%20lower%20than%20the%20random%20baseline%20in%20some%20cases.%20Although%20additional%0Aexperiments%20show%20that%20Chain-of-Thought%20reasoning%20can%20boost%20model%20capabilities%0Aby%205.82%25%20on%20average%2C%20the%20overall%20performance%20of%20VLMs%20remains%20limited.%20Our%0Afindings%20underscore%20the%20critical%20need%20for%20enhanced%20model%20capability%20in%20top-view%0Aspatial%20reasoning%20and%20set%20a%20foundation%20for%20further%20research%20towards%20human-level%0Aproficiency%20of%20VLMs%20in%20real-world%20multimodal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02537v1&entry.124074799=Read"},
{"title": "Analyzing the Benefits of Prototypes for Semi-Supervised Category\n  Learning", "author": "Liyi Zhang and Logan Nelson and Thomas L. Griffiths", "abstract": "  Categories can be represented at different levels of abstraction, from\nprototypes focused on the most typical members to remembering all observed\nexemplars of the category. These representations have been explored in the\ncontext of supervised learning, where stimuli are presented with known category\nlabels. We examine the benefits of prototype-based representations in a\nless-studied domain: semi-supervised learning, where agents must form\nunsupervised representations of stimuli before receiving category labels. We\nstudy this problem in a Bayesian unsupervised learning model called a\nvariational auto-encoder, and we draw on recent advances in machine learning to\nimplement a prior that encourages the model to use abstract prototypes to\nrepresent data. We apply this approach to image datasets and show that forming\nprototypes can improve semi-supervised category learning. Additionally, we\nstudy the latent embeddings of the models and show that these prototypes allow\nthe models to form clustered representations without supervision, contributing\nto their success in downstream categorization performance.\n", "link": "http://arxiv.org/abs/2406.02268v1", "date": "2024-06-04", "relevancy": 2.1165, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5577}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20the%20Benefits%20of%20Prototypes%20for%20Semi-Supervised%20Category%0A%20%20Learning&body=Title%3A%20Analyzing%20the%20Benefits%20of%20Prototypes%20for%20Semi-Supervised%20Category%0A%20%20Learning%0AAuthor%3A%20Liyi%20Zhang%20and%20Logan%20Nelson%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Categories%20can%20be%20represented%20at%20different%20levels%20of%20abstraction%2C%20from%0Aprototypes%20focused%20on%20the%20most%20typical%20members%20to%20remembering%20all%20observed%0Aexemplars%20of%20the%20category.%20These%20representations%20have%20been%20explored%20in%20the%0Acontext%20of%20supervised%20learning%2C%20where%20stimuli%20are%20presented%20with%20known%20category%0Alabels.%20We%20examine%20the%20benefits%20of%20prototype-based%20representations%20in%20a%0Aless-studied%20domain%3A%20semi-supervised%20learning%2C%20where%20agents%20must%20form%0Aunsupervised%20representations%20of%20stimuli%20before%20receiving%20category%20labels.%20We%0Astudy%20this%20problem%20in%20a%20Bayesian%20unsupervised%20learning%20model%20called%20a%0Avariational%20auto-encoder%2C%20and%20we%20draw%20on%20recent%20advances%20in%20machine%20learning%20to%0Aimplement%20a%20prior%20that%20encourages%20the%20model%20to%20use%20abstract%20prototypes%20to%0Arepresent%20data.%20We%20apply%20this%20approach%20to%20image%20datasets%20and%20show%20that%20forming%0Aprototypes%20can%20improve%20semi-supervised%20category%20learning.%20Additionally%2C%20we%0Astudy%20the%20latent%20embeddings%20of%20the%20models%20and%20show%20that%20these%20prototypes%20allow%0Athe%20models%20to%20form%20clustered%20representations%20without%20supervision%2C%20contributing%0Ato%20their%20success%20in%20downstream%20categorization%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520the%2520Benefits%2520of%2520Prototypes%2520for%2520Semi-Supervised%2520Category%250A%2520%2520Learning%26entry.906535625%3DLiyi%2520Zhang%2520and%2520Logan%2520Nelson%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Categories%2520can%2520be%2520represented%2520at%2520different%2520levels%2520of%2520abstraction%252C%2520from%250Aprototypes%2520focused%2520on%2520the%2520most%2520typical%2520members%2520to%2520remembering%2520all%2520observed%250Aexemplars%2520of%2520the%2520category.%2520These%2520representations%2520have%2520been%2520explored%2520in%2520the%250Acontext%2520of%2520supervised%2520learning%252C%2520where%2520stimuli%2520are%2520presented%2520with%2520known%2520category%250Alabels.%2520We%2520examine%2520the%2520benefits%2520of%2520prototype-based%2520representations%2520in%2520a%250Aless-studied%2520domain%253A%2520semi-supervised%2520learning%252C%2520where%2520agents%2520must%2520form%250Aunsupervised%2520representations%2520of%2520stimuli%2520before%2520receiving%2520category%2520labels.%2520We%250Astudy%2520this%2520problem%2520in%2520a%2520Bayesian%2520unsupervised%2520learning%2520model%2520called%2520a%250Avariational%2520auto-encoder%252C%2520and%2520we%2520draw%2520on%2520recent%2520advances%2520in%2520machine%2520learning%2520to%250Aimplement%2520a%2520prior%2520that%2520encourages%2520the%2520model%2520to%2520use%2520abstract%2520prototypes%2520to%250Arepresent%2520data.%2520We%2520apply%2520this%2520approach%2520to%2520image%2520datasets%2520and%2520show%2520that%2520forming%250Aprototypes%2520can%2520improve%2520semi-supervised%2520category%2520learning.%2520Additionally%252C%2520we%250Astudy%2520the%2520latent%2520embeddings%2520of%2520the%2520models%2520and%2520show%2520that%2520these%2520prototypes%2520allow%250Athe%2520models%2520to%2520form%2520clustered%2520representations%2520without%2520supervision%252C%2520contributing%250Ato%2520their%2520success%2520in%2520downstream%2520categorization%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20the%20Benefits%20of%20Prototypes%20for%20Semi-Supervised%20Category%0A%20%20Learning&entry.906535625=Liyi%20Zhang%20and%20Logan%20Nelson%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Categories%20can%20be%20represented%20at%20different%20levels%20of%20abstraction%2C%20from%0Aprototypes%20focused%20on%20the%20most%20typical%20members%20to%20remembering%20all%20observed%0Aexemplars%20of%20the%20category.%20These%20representations%20have%20been%20explored%20in%20the%0Acontext%20of%20supervised%20learning%2C%20where%20stimuli%20are%20presented%20with%20known%20category%0Alabels.%20We%20examine%20the%20benefits%20of%20prototype-based%20representations%20in%20a%0Aless-studied%20domain%3A%20semi-supervised%20learning%2C%20where%20agents%20must%20form%0Aunsupervised%20representations%20of%20stimuli%20before%20receiving%20category%20labels.%20We%0Astudy%20this%20problem%20in%20a%20Bayesian%20unsupervised%20learning%20model%20called%20a%0Avariational%20auto-encoder%2C%20and%20we%20draw%20on%20recent%20advances%20in%20machine%20learning%20to%0Aimplement%20a%20prior%20that%20encourages%20the%20model%20to%20use%20abstract%20prototypes%20to%0Arepresent%20data.%20We%20apply%20this%20approach%20to%20image%20datasets%20and%20show%20that%20forming%0Aprototypes%20can%20improve%20semi-supervised%20category%20learning.%20Additionally%2C%20we%0Astudy%20the%20latent%20embeddings%20of%20the%20models%20and%20show%20that%20these%20prototypes%20allow%0Athe%20models%20to%20form%20clustered%20representations%20without%20supervision%2C%20contributing%0Ato%20their%20success%20in%20downstream%20categorization%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02268v1&entry.124074799=Read"},
{"title": "Extended Mind Transformers", "author": "Phoebe Klett and Thomas Ahle", "abstract": "  Pre-trained language models demonstrate general intelligence and common\nsense, but long inputs quickly become a bottleneck for memorizing information\nat inference time. We resurface a simple method, Memorizing Transformers (Wu et\nal., 2022), that gives the model access to a bank of pre-computed memories. We\nshow that it is possible to fix many of the shortcomings of the original\nmethod, such as the need for fine-tuning, by critically assessing how\npositional encodings should be updated for the keys and values retrieved. This\nintuitive method uses the model's own key/query system to select and attend to\nthe most relevant memories at each generation step, rather than using external\nembeddings. We demonstrate the importance of external information being\nretrieved in a majority of decoder layers, contrary to previous work. We open\nsource a new counterfactual long-range retrieval benchmark, and show that\nExtended Mind Transformers outperform today's state of the art by 6% on\naverage.\n", "link": "http://arxiv.org/abs/2406.02332v1", "date": "2024-06-04", "relevancy": 2.1084, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5258}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extended%20Mind%20Transformers&body=Title%3A%20Extended%20Mind%20Transformers%0AAuthor%3A%20Phoebe%20Klett%20and%20Thomas%20Ahle%0AAbstract%3A%20%20%20Pre-trained%20language%20models%20demonstrate%20general%20intelligence%20and%20common%0Asense%2C%20but%20long%20inputs%20quickly%20become%20a%20bottleneck%20for%20memorizing%20information%0Aat%20inference%20time.%20We%20resurface%20a%20simple%20method%2C%20Memorizing%20Transformers%20%28Wu%20et%0Aal.%2C%202022%29%2C%20that%20gives%20the%20model%20access%20to%20a%20bank%20of%20pre-computed%20memories.%20We%0Ashow%20that%20it%20is%20possible%20to%20fix%20many%20of%20the%20shortcomings%20of%20the%20original%0Amethod%2C%20such%20as%20the%20need%20for%20fine-tuning%2C%20by%20critically%20assessing%20how%0Apositional%20encodings%20should%20be%20updated%20for%20the%20keys%20and%20values%20retrieved.%20This%0Aintuitive%20method%20uses%20the%20model%27s%20own%20key/query%20system%20to%20select%20and%20attend%20to%0Athe%20most%20relevant%20memories%20at%20each%20generation%20step%2C%20rather%20than%20using%20external%0Aembeddings.%20We%20demonstrate%20the%20importance%20of%20external%20information%20being%0Aretrieved%20in%20a%20majority%20of%20decoder%20layers%2C%20contrary%20to%20previous%20work.%20We%20open%0Asource%20a%20new%20counterfactual%20long-range%20retrieval%20benchmark%2C%20and%20show%20that%0AExtended%20Mind%20Transformers%20outperform%20today%27s%20state%20of%20the%20art%20by%206%25%20on%0Aaverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtended%2520Mind%2520Transformers%26entry.906535625%3DPhoebe%2520Klett%2520and%2520Thomas%2520Ahle%26entry.1292438233%3D%2520%2520Pre-trained%2520language%2520models%2520demonstrate%2520general%2520intelligence%2520and%2520common%250Asense%252C%2520but%2520long%2520inputs%2520quickly%2520become%2520a%2520bottleneck%2520for%2520memorizing%2520information%250Aat%2520inference%2520time.%2520We%2520resurface%2520a%2520simple%2520method%252C%2520Memorizing%2520Transformers%2520%2528Wu%2520et%250Aal.%252C%25202022%2529%252C%2520that%2520gives%2520the%2520model%2520access%2520to%2520a%2520bank%2520of%2520pre-computed%2520memories.%2520We%250Ashow%2520that%2520it%2520is%2520possible%2520to%2520fix%2520many%2520of%2520the%2520shortcomings%2520of%2520the%2520original%250Amethod%252C%2520such%2520as%2520the%2520need%2520for%2520fine-tuning%252C%2520by%2520critically%2520assessing%2520how%250Apositional%2520encodings%2520should%2520be%2520updated%2520for%2520the%2520keys%2520and%2520values%2520retrieved.%2520This%250Aintuitive%2520method%2520uses%2520the%2520model%2527s%2520own%2520key/query%2520system%2520to%2520select%2520and%2520attend%2520to%250Athe%2520most%2520relevant%2520memories%2520at%2520each%2520generation%2520step%252C%2520rather%2520than%2520using%2520external%250Aembeddings.%2520We%2520demonstrate%2520the%2520importance%2520of%2520external%2520information%2520being%250Aretrieved%2520in%2520a%2520majority%2520of%2520decoder%2520layers%252C%2520contrary%2520to%2520previous%2520work.%2520We%2520open%250Asource%2520a%2520new%2520counterfactual%2520long-range%2520retrieval%2520benchmark%252C%2520and%2520show%2520that%250AExtended%2520Mind%2520Transformers%2520outperform%2520today%2527s%2520state%2520of%2520the%2520art%2520by%25206%2525%2520on%250Aaverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extended%20Mind%20Transformers&entry.906535625=Phoebe%20Klett%20and%20Thomas%20Ahle&entry.1292438233=%20%20Pre-trained%20language%20models%20demonstrate%20general%20intelligence%20and%20common%0Asense%2C%20but%20long%20inputs%20quickly%20become%20a%20bottleneck%20for%20memorizing%20information%0Aat%20inference%20time.%20We%20resurface%20a%20simple%20method%2C%20Memorizing%20Transformers%20%28Wu%20et%0Aal.%2C%202022%29%2C%20that%20gives%20the%20model%20access%20to%20a%20bank%20of%20pre-computed%20memories.%20We%0Ashow%20that%20it%20is%20possible%20to%20fix%20many%20of%20the%20shortcomings%20of%20the%20original%0Amethod%2C%20such%20as%20the%20need%20for%20fine-tuning%2C%20by%20critically%20assessing%20how%0Apositional%20encodings%20should%20be%20updated%20for%20the%20keys%20and%20values%20retrieved.%20This%0Aintuitive%20method%20uses%20the%20model%27s%20own%20key/query%20system%20to%20select%20and%20attend%20to%0Athe%20most%20relevant%20memories%20at%20each%20generation%20step%2C%20rather%20than%20using%20external%0Aembeddings.%20We%20demonstrate%20the%20importance%20of%20external%20information%20being%0Aretrieved%20in%20a%20majority%20of%20decoder%20layers%2C%20contrary%20to%20previous%20work.%20We%20open%0Asource%20a%20new%20counterfactual%20long-range%20retrieval%20benchmark%2C%20and%20show%20that%0AExtended%20Mind%20Transformers%20outperform%20today%27s%20state%20of%20the%20art%20by%206%25%20on%0Aaverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02332v1&entry.124074799=Read"},
{"title": "SSFlowNet: Semi-supervised Scene Flow Estimation On Point Clouds With\n  Pseudo Label", "author": "Jingze Chen and Junfeng Yao and Qiqin Lin and Rongzhou Zhou and Lei Li", "abstract": "  In the domain of supervised scene flow estimation, the process of manual\nlabeling is both time-intensive and financially demanding. This paper\nintroduces SSFlowNet, a semi-supervised approach for scene flow estimation,\nthat utilizes a blend of labeled and unlabeled data, optimizing the balance\nbetween the cost of labeling and the precision of model training. SSFlowNet\nstands out through its innovative use of pseudo-labels, mainly reducing the\ndependency on extensively labeled datasets while maintaining high model\naccuracy. The core of our model is its emphasis on the intricate geometric\nstructures of point clouds, both locally and globally, coupled with a novel\nspatial memory feature. This feature is adept at learning the geometric\nrelationships between points over sequential time frames. By identifying\nsimilarities between labeled and unlabeled points, SSFlowNet dynamically\nconstructs a correlation matrix to evaluate scene flow dependencies at\nindividual point level. Furthermore, the integration of a flow consistency\nmodule within SSFlowNet enhances its capability to consistently estimate flow,\nan essential aspect for analyzing dynamic scenes. Empirical results demonstrate\nthat SSFlowNet surpasses existing methods in pseudo-label generation and shows\nadaptability across varying data volumes. Moreover, our semi-supervised\ntraining technique yields promising outcomes even with different smaller ratio\nlabeled data, marking a substantial advancement in the field of scene flow\nestimation.\n", "link": "http://arxiv.org/abs/2312.15271v2", "date": "2024-06-04", "relevancy": 2.0979, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5488}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5274}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSFlowNet%3A%20Semi-supervised%20Scene%20Flow%20Estimation%20On%20Point%20Clouds%20With%0A%20%20Pseudo%20Label&body=Title%3A%20SSFlowNet%3A%20Semi-supervised%20Scene%20Flow%20Estimation%20On%20Point%20Clouds%20With%0A%20%20Pseudo%20Label%0AAuthor%3A%20Jingze%20Chen%20and%20Junfeng%20Yao%20and%20Qiqin%20Lin%20and%20Rongzhou%20Zhou%20and%20Lei%20Li%0AAbstract%3A%20%20%20In%20the%20domain%20of%20supervised%20scene%20flow%20estimation%2C%20the%20process%20of%20manual%0Alabeling%20is%20both%20time-intensive%20and%20financially%20demanding.%20This%20paper%0Aintroduces%20SSFlowNet%2C%20a%20semi-supervised%20approach%20for%20scene%20flow%20estimation%2C%0Athat%20utilizes%20a%20blend%20of%20labeled%20and%20unlabeled%20data%2C%20optimizing%20the%20balance%0Abetween%20the%20cost%20of%20labeling%20and%20the%20precision%20of%20model%20training.%20SSFlowNet%0Astands%20out%20through%20its%20innovative%20use%20of%20pseudo-labels%2C%20mainly%20reducing%20the%0Adependency%20on%20extensively%20labeled%20datasets%20while%20maintaining%20high%20model%0Aaccuracy.%20The%20core%20of%20our%20model%20is%20its%20emphasis%20on%20the%20intricate%20geometric%0Astructures%20of%20point%20clouds%2C%20both%20locally%20and%20globally%2C%20coupled%20with%20a%20novel%0Aspatial%20memory%20feature.%20This%20feature%20is%20adept%20at%20learning%20the%20geometric%0Arelationships%20between%20points%20over%20sequential%20time%20frames.%20By%20identifying%0Asimilarities%20between%20labeled%20and%20unlabeled%20points%2C%20SSFlowNet%20dynamically%0Aconstructs%20a%20correlation%20matrix%20to%20evaluate%20scene%20flow%20dependencies%20at%0Aindividual%20point%20level.%20Furthermore%2C%20the%20integration%20of%20a%20flow%20consistency%0Amodule%20within%20SSFlowNet%20enhances%20its%20capability%20to%20consistently%20estimate%20flow%2C%0Aan%20essential%20aspect%20for%20analyzing%20dynamic%20scenes.%20Empirical%20results%20demonstrate%0Athat%20SSFlowNet%20surpasses%20existing%20methods%20in%20pseudo-label%20generation%20and%20shows%0Aadaptability%20across%20varying%20data%20volumes.%20Moreover%2C%20our%20semi-supervised%0Atraining%20technique%20yields%20promising%20outcomes%20even%20with%20different%20smaller%20ratio%0Alabeled%20data%2C%20marking%20a%20substantial%20advancement%20in%20the%20field%20of%20scene%20flow%0Aestimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSFlowNet%253A%2520Semi-supervised%2520Scene%2520Flow%2520Estimation%2520On%2520Point%2520Clouds%2520With%250A%2520%2520Pseudo%2520Label%26entry.906535625%3DJingze%2520Chen%2520and%2520Junfeng%2520Yao%2520and%2520Qiqin%2520Lin%2520and%2520Rongzhou%2520Zhou%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%2520supervised%2520scene%2520flow%2520estimation%252C%2520the%2520process%2520of%2520manual%250Alabeling%2520is%2520both%2520time-intensive%2520and%2520financially%2520demanding.%2520This%2520paper%250Aintroduces%2520SSFlowNet%252C%2520a%2520semi-supervised%2520approach%2520for%2520scene%2520flow%2520estimation%252C%250Athat%2520utilizes%2520a%2520blend%2520of%2520labeled%2520and%2520unlabeled%2520data%252C%2520optimizing%2520the%2520balance%250Abetween%2520the%2520cost%2520of%2520labeling%2520and%2520the%2520precision%2520of%2520model%2520training.%2520SSFlowNet%250Astands%2520out%2520through%2520its%2520innovative%2520use%2520of%2520pseudo-labels%252C%2520mainly%2520reducing%2520the%250Adependency%2520on%2520extensively%2520labeled%2520datasets%2520while%2520maintaining%2520high%2520model%250Aaccuracy.%2520The%2520core%2520of%2520our%2520model%2520is%2520its%2520emphasis%2520on%2520the%2520intricate%2520geometric%250Astructures%2520of%2520point%2520clouds%252C%2520both%2520locally%2520and%2520globally%252C%2520coupled%2520with%2520a%2520novel%250Aspatial%2520memory%2520feature.%2520This%2520feature%2520is%2520adept%2520at%2520learning%2520the%2520geometric%250Arelationships%2520between%2520points%2520over%2520sequential%2520time%2520frames.%2520By%2520identifying%250Asimilarities%2520between%2520labeled%2520and%2520unlabeled%2520points%252C%2520SSFlowNet%2520dynamically%250Aconstructs%2520a%2520correlation%2520matrix%2520to%2520evaluate%2520scene%2520flow%2520dependencies%2520at%250Aindividual%2520point%2520level.%2520Furthermore%252C%2520the%2520integration%2520of%2520a%2520flow%2520consistency%250Amodule%2520within%2520SSFlowNet%2520enhances%2520its%2520capability%2520to%2520consistently%2520estimate%2520flow%252C%250Aan%2520essential%2520aspect%2520for%2520analyzing%2520dynamic%2520scenes.%2520Empirical%2520results%2520demonstrate%250Athat%2520SSFlowNet%2520surpasses%2520existing%2520methods%2520in%2520pseudo-label%2520generation%2520and%2520shows%250Aadaptability%2520across%2520varying%2520data%2520volumes.%2520Moreover%252C%2520our%2520semi-supervised%250Atraining%2520technique%2520yields%2520promising%2520outcomes%2520even%2520with%2520different%2520smaller%2520ratio%250Alabeled%2520data%252C%2520marking%2520a%2520substantial%2520advancement%2520in%2520the%2520field%2520of%2520scene%2520flow%250Aestimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSFlowNet%3A%20Semi-supervised%20Scene%20Flow%20Estimation%20On%20Point%20Clouds%20With%0A%20%20Pseudo%20Label&entry.906535625=Jingze%20Chen%20and%20Junfeng%20Yao%20and%20Qiqin%20Lin%20and%20Rongzhou%20Zhou%20and%20Lei%20Li&entry.1292438233=%20%20In%20the%20domain%20of%20supervised%20scene%20flow%20estimation%2C%20the%20process%20of%20manual%0Alabeling%20is%20both%20time-intensive%20and%20financially%20demanding.%20This%20paper%0Aintroduces%20SSFlowNet%2C%20a%20semi-supervised%20approach%20for%20scene%20flow%20estimation%2C%0Athat%20utilizes%20a%20blend%20of%20labeled%20and%20unlabeled%20data%2C%20optimizing%20the%20balance%0Abetween%20the%20cost%20of%20labeling%20and%20the%20precision%20of%20model%20training.%20SSFlowNet%0Astands%20out%20through%20its%20innovative%20use%20of%20pseudo-labels%2C%20mainly%20reducing%20the%0Adependency%20on%20extensively%20labeled%20datasets%20while%20maintaining%20high%20model%0Aaccuracy.%20The%20core%20of%20our%20model%20is%20its%20emphasis%20on%20the%20intricate%20geometric%0Astructures%20of%20point%20clouds%2C%20both%20locally%20and%20globally%2C%20coupled%20with%20a%20novel%0Aspatial%20memory%20feature.%20This%20feature%20is%20adept%20at%20learning%20the%20geometric%0Arelationships%20between%20points%20over%20sequential%20time%20frames.%20By%20identifying%0Asimilarities%20between%20labeled%20and%20unlabeled%20points%2C%20SSFlowNet%20dynamically%0Aconstructs%20a%20correlation%20matrix%20to%20evaluate%20scene%20flow%20dependencies%20at%0Aindividual%20point%20level.%20Furthermore%2C%20the%20integration%20of%20a%20flow%20consistency%0Amodule%20within%20SSFlowNet%20enhances%20its%20capability%20to%20consistently%20estimate%20flow%2C%0Aan%20essential%20aspect%20for%20analyzing%20dynamic%20scenes.%20Empirical%20results%20demonstrate%0Athat%20SSFlowNet%20surpasses%20existing%20methods%20in%20pseudo-label%20generation%20and%20shows%0Aadaptability%20across%20varying%20data%20volumes.%20Moreover%2C%20our%20semi-supervised%0Atraining%20technique%20yields%20promising%20outcomes%20even%20with%20different%20smaller%20ratio%0Alabeled%20data%2C%20marking%20a%20substantial%20advancement%20in%20the%20field%20of%20scene%20flow%0Aestimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15271v2&entry.124074799=Read"},
{"title": "Exploiting Chordal Sparsity for Fast Global Optimality with Application\n  to Localization", "author": "Frederike D\u00fcmbgen and Connor Holmes and Timothy D. Barfoot", "abstract": "  In recent years, many estimation problems in robotics have been shown to be\nsolvable to global optimality using their semidefinite relaxations. However,\nthe runtime complexity of off-the-shelve semidefinite programming solvers is up\nto cubic in problem size, which inhibits real-time solutions of problems\ninvolving large state dimensions. We show that for a large class of problems,\nnamely those with chordal sparsity, we can reduce the complexity of these\nsolvers to linear in problem size. In particular, we show how to replace the\nlarge positive-semidefinite variable by a number of smaller interconnected ones\nusing the well-known chordal decomposition. This formulation also allows for\nthe straightforward application of the alternating direction method of\nmultipliers (ADMM), which can exploit parallelism for increased scalability. We\nshow in simulation that the algorithms provide a significant speed up for two\nexample problems: matrix-weighted and range-only localization.\n", "link": "http://arxiv.org/abs/2406.02365v1", "date": "2024-06-04", "relevancy": 2.0974, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5322}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5248}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Chordal%20Sparsity%20for%20Fast%20Global%20Optimality%20with%20Application%0A%20%20to%20Localization&body=Title%3A%20Exploiting%20Chordal%20Sparsity%20for%20Fast%20Global%20Optimality%20with%20Application%0A%20%20to%20Localization%0AAuthor%3A%20Frederike%20D%C3%BCmbgen%20and%20Connor%20Holmes%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20In%20recent%20years%2C%20many%20estimation%20problems%20in%20robotics%20have%20been%20shown%20to%20be%0Asolvable%20to%20global%20optimality%20using%20their%20semidefinite%20relaxations.%20However%2C%0Athe%20runtime%20complexity%20of%20off-the-shelve%20semidefinite%20programming%20solvers%20is%20up%0Ato%20cubic%20in%20problem%20size%2C%20which%20inhibits%20real-time%20solutions%20of%20problems%0Ainvolving%20large%20state%20dimensions.%20We%20show%20that%20for%20a%20large%20class%20of%20problems%2C%0Anamely%20those%20with%20chordal%20sparsity%2C%20we%20can%20reduce%20the%20complexity%20of%20these%0Asolvers%20to%20linear%20in%20problem%20size.%20In%20particular%2C%20we%20show%20how%20to%20replace%20the%0Alarge%20positive-semidefinite%20variable%20by%20a%20number%20of%20smaller%20interconnected%20ones%0Ausing%20the%20well-known%20chordal%20decomposition.%20This%20formulation%20also%20allows%20for%0Athe%20straightforward%20application%20of%20the%20alternating%20direction%20method%20of%0Amultipliers%20%28ADMM%29%2C%20which%20can%20exploit%20parallelism%20for%20increased%20scalability.%20We%0Ashow%20in%20simulation%20that%20the%20algorithms%20provide%20a%20significant%20speed%20up%20for%20two%0Aexample%20problems%3A%20matrix-weighted%20and%20range-only%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Chordal%2520Sparsity%2520for%2520Fast%2520Global%2520Optimality%2520with%2520Application%250A%2520%2520to%2520Localization%26entry.906535625%3DFrederike%2520D%25C3%25BCmbgen%2520and%2520Connor%2520Holmes%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520many%2520estimation%2520problems%2520in%2520robotics%2520have%2520been%2520shown%2520to%2520be%250Asolvable%2520to%2520global%2520optimality%2520using%2520their%2520semidefinite%2520relaxations.%2520However%252C%250Athe%2520runtime%2520complexity%2520of%2520off-the-shelve%2520semidefinite%2520programming%2520solvers%2520is%2520up%250Ato%2520cubic%2520in%2520problem%2520size%252C%2520which%2520inhibits%2520real-time%2520solutions%2520of%2520problems%250Ainvolving%2520large%2520state%2520dimensions.%2520We%2520show%2520that%2520for%2520a%2520large%2520class%2520of%2520problems%252C%250Anamely%2520those%2520with%2520chordal%2520sparsity%252C%2520we%2520can%2520reduce%2520the%2520complexity%2520of%2520these%250Asolvers%2520to%2520linear%2520in%2520problem%2520size.%2520In%2520particular%252C%2520we%2520show%2520how%2520to%2520replace%2520the%250Alarge%2520positive-semidefinite%2520variable%2520by%2520a%2520number%2520of%2520smaller%2520interconnected%2520ones%250Ausing%2520the%2520well-known%2520chordal%2520decomposition.%2520This%2520formulation%2520also%2520allows%2520for%250Athe%2520straightforward%2520application%2520of%2520the%2520alternating%2520direction%2520method%2520of%250Amultipliers%2520%2528ADMM%2529%252C%2520which%2520can%2520exploit%2520parallelism%2520for%2520increased%2520scalability.%2520We%250Ashow%2520in%2520simulation%2520that%2520the%2520algorithms%2520provide%2520a%2520significant%2520speed%2520up%2520for%2520two%250Aexample%2520problems%253A%2520matrix-weighted%2520and%2520range-only%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Chordal%20Sparsity%20for%20Fast%20Global%20Optimality%20with%20Application%0A%20%20to%20Localization&entry.906535625=Frederike%20D%C3%BCmbgen%20and%20Connor%20Holmes%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20In%20recent%20years%2C%20many%20estimation%20problems%20in%20robotics%20have%20been%20shown%20to%20be%0Asolvable%20to%20global%20optimality%20using%20their%20semidefinite%20relaxations.%20However%2C%0Athe%20runtime%20complexity%20of%20off-the-shelve%20semidefinite%20programming%20solvers%20is%20up%0Ato%20cubic%20in%20problem%20size%2C%20which%20inhibits%20real-time%20solutions%20of%20problems%0Ainvolving%20large%20state%20dimensions.%20We%20show%20that%20for%20a%20large%20class%20of%20problems%2C%0Anamely%20those%20with%20chordal%20sparsity%2C%20we%20can%20reduce%20the%20complexity%20of%20these%0Asolvers%20to%20linear%20in%20problem%20size.%20In%20particular%2C%20we%20show%20how%20to%20replace%20the%0Alarge%20positive-semidefinite%20variable%20by%20a%20number%20of%20smaller%20interconnected%20ones%0Ausing%20the%20well-known%20chordal%20decomposition.%20This%20formulation%20also%20allows%20for%0Athe%20straightforward%20application%20of%20the%20alternating%20direction%20method%20of%0Amultipliers%20%28ADMM%29%2C%20which%20can%20exploit%20parallelism%20for%20increased%20scalability.%20We%0Ashow%20in%20simulation%20that%20the%20algorithms%20provide%20a%20significant%20speed%20up%20for%20two%0Aexample%20problems%3A%20matrix-weighted%20and%20range-only%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02365v1&entry.124074799=Read"},
{"title": "A Survey of Transformer Enabled Time Series Synthesis", "author": "Alexander Sommers and Logan Cummins and Sudip Mittal and Shahram Rahimi and Maria Seale and Joseph Jaboure and Thomas Arnold", "abstract": "  Generative AI has received much attention in the image and language domains,\nwith the transformer neural network continuing to dominate the state of the\nart. Application of these models to time series generation is less explored,\nhowever, and is of great utility to machine learning, privacy preservation, and\nexplainability research. The present survey identifies this gap at the\nintersection of the transformer, generative AI, and time series data, and\nreviews works in this sparsely populated subdomain. The reviewed works show\ngreat variety in approach, and have not yet converged on a conclusive answer to\nthe problems the domain poses. GANs, diffusion models, state space models, and\nautoencoders were all encountered alongside or surrounding the transformers\nwhich originally motivated the survey. While too open a domain to offer\nconclusive insights, the works surveyed are quite suggestive, and several\nrecommendations for best practice, and suggestions of valuable future work, are\nprovided.\n", "link": "http://arxiv.org/abs/2406.02322v1", "date": "2024-06-04", "relevancy": 2.0958, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5932}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5409}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Transformer%20Enabled%20Time%20Series%20Synthesis&body=Title%3A%20A%20Survey%20of%20Transformer%20Enabled%20Time%20Series%20Synthesis%0AAuthor%3A%20Alexander%20Sommers%20and%20Logan%20Cummins%20and%20Sudip%20Mittal%20and%20Shahram%20Rahimi%20and%20Maria%20Seale%20and%20Joseph%20Jaboure%20and%20Thomas%20Arnold%0AAbstract%3A%20%20%20Generative%20AI%20has%20received%20much%20attention%20in%20the%20image%20and%20language%20domains%2C%0Awith%20the%20transformer%20neural%20network%20continuing%20to%20dominate%20the%20state%20of%20the%0Aart.%20Application%20of%20these%20models%20to%20time%20series%20generation%20is%20less%20explored%2C%0Ahowever%2C%20and%20is%20of%20great%20utility%20to%20machine%20learning%2C%20privacy%20preservation%2C%20and%0Aexplainability%20research.%20The%20present%20survey%20identifies%20this%20gap%20at%20the%0Aintersection%20of%20the%20transformer%2C%20generative%20AI%2C%20and%20time%20series%20data%2C%20and%0Areviews%20works%20in%20this%20sparsely%20populated%20subdomain.%20The%20reviewed%20works%20show%0Agreat%20variety%20in%20approach%2C%20and%20have%20not%20yet%20converged%20on%20a%20conclusive%20answer%20to%0Athe%20problems%20the%20domain%20poses.%20GANs%2C%20diffusion%20models%2C%20state%20space%20models%2C%20and%0Aautoencoders%20were%20all%20encountered%20alongside%20or%20surrounding%20the%20transformers%0Awhich%20originally%20motivated%20the%20survey.%20While%20too%20open%20a%20domain%20to%20offer%0Aconclusive%20insights%2C%20the%20works%20surveyed%20are%20quite%20suggestive%2C%20and%20several%0Arecommendations%20for%20best%20practice%2C%20and%20suggestions%20of%20valuable%20future%20work%2C%20are%0Aprovided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Transformer%2520Enabled%2520Time%2520Series%2520Synthesis%26entry.906535625%3DAlexander%2520Sommers%2520and%2520Logan%2520Cummins%2520and%2520Sudip%2520Mittal%2520and%2520Shahram%2520Rahimi%2520and%2520Maria%2520Seale%2520and%2520Joseph%2520Jaboure%2520and%2520Thomas%2520Arnold%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520received%2520much%2520attention%2520in%2520the%2520image%2520and%2520language%2520domains%252C%250Awith%2520the%2520transformer%2520neural%2520network%2520continuing%2520to%2520dominate%2520the%2520state%2520of%2520the%250Aart.%2520Application%2520of%2520these%2520models%2520to%2520time%2520series%2520generation%2520is%2520less%2520explored%252C%250Ahowever%252C%2520and%2520is%2520of%2520great%2520utility%2520to%2520machine%2520learning%252C%2520privacy%2520preservation%252C%2520and%250Aexplainability%2520research.%2520The%2520present%2520survey%2520identifies%2520this%2520gap%2520at%2520the%250Aintersection%2520of%2520the%2520transformer%252C%2520generative%2520AI%252C%2520and%2520time%2520series%2520data%252C%2520and%250Areviews%2520works%2520in%2520this%2520sparsely%2520populated%2520subdomain.%2520The%2520reviewed%2520works%2520show%250Agreat%2520variety%2520in%2520approach%252C%2520and%2520have%2520not%2520yet%2520converged%2520on%2520a%2520conclusive%2520answer%2520to%250Athe%2520problems%2520the%2520domain%2520poses.%2520GANs%252C%2520diffusion%2520models%252C%2520state%2520space%2520models%252C%2520and%250Aautoencoders%2520were%2520all%2520encountered%2520alongside%2520or%2520surrounding%2520the%2520transformers%250Awhich%2520originally%2520motivated%2520the%2520survey.%2520While%2520too%2520open%2520a%2520domain%2520to%2520offer%250Aconclusive%2520insights%252C%2520the%2520works%2520surveyed%2520are%2520quite%2520suggestive%252C%2520and%2520several%250Arecommendations%2520for%2520best%2520practice%252C%2520and%2520suggestions%2520of%2520valuable%2520future%2520work%252C%2520are%250Aprovided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Transformer%20Enabled%20Time%20Series%20Synthesis&entry.906535625=Alexander%20Sommers%20and%20Logan%20Cummins%20and%20Sudip%20Mittal%20and%20Shahram%20Rahimi%20and%20Maria%20Seale%20and%20Joseph%20Jaboure%20and%20Thomas%20Arnold&entry.1292438233=%20%20Generative%20AI%20has%20received%20much%20attention%20in%20the%20image%20and%20language%20domains%2C%0Awith%20the%20transformer%20neural%20network%20continuing%20to%20dominate%20the%20state%20of%20the%0Aart.%20Application%20of%20these%20models%20to%20time%20series%20generation%20is%20less%20explored%2C%0Ahowever%2C%20and%20is%20of%20great%20utility%20to%20machine%20learning%2C%20privacy%20preservation%2C%20and%0Aexplainability%20research.%20The%20present%20survey%20identifies%20this%20gap%20at%20the%0Aintersection%20of%20the%20transformer%2C%20generative%20AI%2C%20and%20time%20series%20data%2C%20and%0Areviews%20works%20in%20this%20sparsely%20populated%20subdomain.%20The%20reviewed%20works%20show%0Agreat%20variety%20in%20approach%2C%20and%20have%20not%20yet%20converged%20on%20a%20conclusive%20answer%20to%0Athe%20problems%20the%20domain%20poses.%20GANs%2C%20diffusion%20models%2C%20state%20space%20models%2C%20and%0Aautoencoders%20were%20all%20encountered%20alongside%20or%20surrounding%20the%20transformers%0Awhich%20originally%20motivated%20the%20survey.%20While%20too%20open%20a%20domain%20to%20offer%0Aconclusive%20insights%2C%20the%20works%20surveyed%20are%20quite%20suggestive%2C%20and%20several%0Arecommendations%20for%20best%20practice%2C%20and%20suggestions%20of%20valuable%20future%20work%2C%20are%0Aprovided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02322v1&entry.124074799=Read"},
{"title": "Optimised ProPainter for Video Diminished Reality Inpainting", "author": "Pengze Li and Lihao Liu and Carola-Bibiane Sch\u00f6nlieb and Angelica I Aviles-Rivero", "abstract": "  In this paper, part of the DREAMING Challenge - Diminished Reality for\nEmerging Applications in Medicine through Inpainting, we introduce a refined\nvideo inpainting technique optimised from the ProPainter method to meet the\nspecialised demands of medical imaging, specifically in the context of oral and\nmaxillofacial surgery. Our enhanced algorithm employs the zero-shot ProPainter,\nfeaturing optimized parameters and pre-processing, to adeptly manage the\ncomplex task of inpainting surgical video sequences, without requiring any\ntraining process. It aims to produce temporally coherent and detail-rich\nreconstructions of occluded regions, facilitating clearer views of operative\nfields. The efficacy of our approach is evaluated using comprehensive metrics,\npositioning it as a significant advancement in the application of diminished\nreality for medical purposes.\n", "link": "http://arxiv.org/abs/2406.02287v1", "date": "2024-06-04", "relevancy": 2.0945, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5358}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5183}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimised%20ProPainter%20for%20Video%20Diminished%20Reality%20Inpainting&body=Title%3A%20Optimised%20ProPainter%20for%20Video%20Diminished%20Reality%20Inpainting%0AAuthor%3A%20Pengze%20Li%20and%20Lihao%20Liu%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Angelica%20I%20Aviles-Rivero%0AAbstract%3A%20%20%20In%20this%20paper%2C%20part%20of%20the%20DREAMING%20Challenge%20-%20Diminished%20Reality%20for%0AEmerging%20Applications%20in%20Medicine%20through%20Inpainting%2C%20we%20introduce%20a%20refined%0Avideo%20inpainting%20technique%20optimised%20from%20the%20ProPainter%20method%20to%20meet%20the%0Aspecialised%20demands%20of%20medical%20imaging%2C%20specifically%20in%20the%20context%20of%20oral%20and%0Amaxillofacial%20surgery.%20Our%20enhanced%20algorithm%20employs%20the%20zero-shot%20ProPainter%2C%0Afeaturing%20optimized%20parameters%20and%20pre-processing%2C%20to%20adeptly%20manage%20the%0Acomplex%20task%20of%20inpainting%20surgical%20video%20sequences%2C%20without%20requiring%20any%0Atraining%20process.%20It%20aims%20to%20produce%20temporally%20coherent%20and%20detail-rich%0Areconstructions%20of%20occluded%20regions%2C%20facilitating%20clearer%20views%20of%20operative%0Afields.%20The%20efficacy%20of%20our%20approach%20is%20evaluated%20using%20comprehensive%20metrics%2C%0Apositioning%20it%20as%20a%20significant%20advancement%20in%20the%20application%20of%20diminished%0Areality%20for%20medical%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimised%2520ProPainter%2520for%2520Video%2520Diminished%2520Reality%2520Inpainting%26entry.906535625%3DPengze%2520Li%2520and%2520Lihao%2520Liu%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Angelica%2520I%2520Aviles-Rivero%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520part%2520of%2520the%2520DREAMING%2520Challenge%2520-%2520Diminished%2520Reality%2520for%250AEmerging%2520Applications%2520in%2520Medicine%2520through%2520Inpainting%252C%2520we%2520introduce%2520a%2520refined%250Avideo%2520inpainting%2520technique%2520optimised%2520from%2520the%2520ProPainter%2520method%2520to%2520meet%2520the%250Aspecialised%2520demands%2520of%2520medical%2520imaging%252C%2520specifically%2520in%2520the%2520context%2520of%2520oral%2520and%250Amaxillofacial%2520surgery.%2520Our%2520enhanced%2520algorithm%2520employs%2520the%2520zero-shot%2520ProPainter%252C%250Afeaturing%2520optimized%2520parameters%2520and%2520pre-processing%252C%2520to%2520adeptly%2520manage%2520the%250Acomplex%2520task%2520of%2520inpainting%2520surgical%2520video%2520sequences%252C%2520without%2520requiring%2520any%250Atraining%2520process.%2520It%2520aims%2520to%2520produce%2520temporally%2520coherent%2520and%2520detail-rich%250Areconstructions%2520of%2520occluded%2520regions%252C%2520facilitating%2520clearer%2520views%2520of%2520operative%250Afields.%2520The%2520efficacy%2520of%2520our%2520approach%2520is%2520evaluated%2520using%2520comprehensive%2520metrics%252C%250Apositioning%2520it%2520as%2520a%2520significant%2520advancement%2520in%2520the%2520application%2520of%2520diminished%250Areality%2520for%2520medical%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimised%20ProPainter%20for%20Video%20Diminished%20Reality%20Inpainting&entry.906535625=Pengze%20Li%20and%20Lihao%20Liu%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Angelica%20I%20Aviles-Rivero&entry.1292438233=%20%20In%20this%20paper%2C%20part%20of%20the%20DREAMING%20Challenge%20-%20Diminished%20Reality%20for%0AEmerging%20Applications%20in%20Medicine%20through%20Inpainting%2C%20we%20introduce%20a%20refined%0Avideo%20inpainting%20technique%20optimised%20from%20the%20ProPainter%20method%20to%20meet%20the%0Aspecialised%20demands%20of%20medical%20imaging%2C%20specifically%20in%20the%20context%20of%20oral%20and%0Amaxillofacial%20surgery.%20Our%20enhanced%20algorithm%20employs%20the%20zero-shot%20ProPainter%2C%0Afeaturing%20optimized%20parameters%20and%20pre-processing%2C%20to%20adeptly%20manage%20the%0Acomplex%20task%20of%20inpainting%20surgical%20video%20sequences%2C%20without%20requiring%20any%0Atraining%20process.%20It%20aims%20to%20produce%20temporally%20coherent%20and%20detail-rich%0Areconstructions%20of%20occluded%20regions%2C%20facilitating%20clearer%20views%20of%20operative%0Afields.%20The%20efficacy%20of%20our%20approach%20is%20evaluated%20using%20comprehensive%20metrics%2C%0Apositioning%20it%20as%20a%20significant%20advancement%20in%20the%20application%20of%20diminished%0Areality%20for%20medical%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02287v1&entry.124074799=Read"},
{"title": "Continual Unsupervised Out-of-Distribution Detection", "author": "Lars Doorenbos and Raphael Sznitman and Pablo M\u00e1rquez-Neila", "abstract": "  Deep learning models excel when the data distribution during training aligns\nwith testing data. Yet, their performance diminishes when faced with\nout-of-distribution (OOD) samples, leading to great interest in the field of\nOOD detection. Current approaches typically assume that OOD samples originate\nfrom an unconcentrated distribution complementary to the training distribution.\nWhile this assumption is appropriate in the traditional unsupervised OOD\n(U-OOD) setting, it proves inadequate when considering the place of deployment\nof the underlying deep learning model. To better reflect this real-world\nscenario, we introduce the novel setting of continual U-OOD detection. To\ntackle this new setting, we propose a method that starts from a U-OOD detector,\nwhich is agnostic to the OOD distribution, and slowly updates during deployment\nto account for the actual OOD distribution. Our method uses a new U-OOD scoring\nfunction that combines the Mahalanobis distance with a nearest-neighbor\napproach. Furthermore, we design a confidence-scaled few-shot OOD detector that\noutperforms previous methods. We show our method greatly improves upon strong\nbaselines from related fields.\n", "link": "http://arxiv.org/abs/2406.02327v1", "date": "2024-06-04", "relevancy": 2.0913, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5606}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5179}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Unsupervised%20Out-of-Distribution%20Detection&body=Title%3A%20Continual%20Unsupervised%20Out-of-Distribution%20Detection%0AAuthor%3A%20Lars%20Doorenbos%20and%20Raphael%20Sznitman%20and%20Pablo%20M%C3%A1rquez-Neila%0AAbstract%3A%20%20%20Deep%20learning%20models%20excel%20when%20the%20data%20distribution%20during%20training%20aligns%0Awith%20testing%20data.%20Yet%2C%20their%20performance%20diminishes%20when%20faced%20with%0Aout-of-distribution%20%28OOD%29%20samples%2C%20leading%20to%20great%20interest%20in%20the%20field%20of%0AOOD%20detection.%20Current%20approaches%20typically%20assume%20that%20OOD%20samples%20originate%0Afrom%20an%20unconcentrated%20distribution%20complementary%20to%20the%20training%20distribution.%0AWhile%20this%20assumption%20is%20appropriate%20in%20the%20traditional%20unsupervised%20OOD%0A%28U-OOD%29%20setting%2C%20it%20proves%20inadequate%20when%20considering%20the%20place%20of%20deployment%0Aof%20the%20underlying%20deep%20learning%20model.%20To%20better%20reflect%20this%20real-world%0Ascenario%2C%20we%20introduce%20the%20novel%20setting%20of%20continual%20U-OOD%20detection.%20To%0Atackle%20this%20new%20setting%2C%20we%20propose%20a%20method%20that%20starts%20from%20a%20U-OOD%20detector%2C%0Awhich%20is%20agnostic%20to%20the%20OOD%20distribution%2C%20and%20slowly%20updates%20during%20deployment%0Ato%20account%20for%20the%20actual%20OOD%20distribution.%20Our%20method%20uses%20a%20new%20U-OOD%20scoring%0Afunction%20that%20combines%20the%20Mahalanobis%20distance%20with%20a%20nearest-neighbor%0Aapproach.%20Furthermore%2C%20we%20design%20a%20confidence-scaled%20few-shot%20OOD%20detector%20that%0Aoutperforms%20previous%20methods.%20We%20show%20our%20method%20greatly%20improves%20upon%20strong%0Abaselines%20from%20related%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Unsupervised%2520Out-of-Distribution%2520Detection%26entry.906535625%3DLars%2520Doorenbos%2520and%2520Raphael%2520Sznitman%2520and%2520Pablo%2520M%25C3%25A1rquez-Neila%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520excel%2520when%2520the%2520data%2520distribution%2520during%2520training%2520aligns%250Awith%2520testing%2520data.%2520Yet%252C%2520their%2520performance%2520diminishes%2520when%2520faced%2520with%250Aout-of-distribution%2520%2528OOD%2529%2520samples%252C%2520leading%2520to%2520great%2520interest%2520in%2520the%2520field%2520of%250AOOD%2520detection.%2520Current%2520approaches%2520typically%2520assume%2520that%2520OOD%2520samples%2520originate%250Afrom%2520an%2520unconcentrated%2520distribution%2520complementary%2520to%2520the%2520training%2520distribution.%250AWhile%2520this%2520assumption%2520is%2520appropriate%2520in%2520the%2520traditional%2520unsupervised%2520OOD%250A%2528U-OOD%2529%2520setting%252C%2520it%2520proves%2520inadequate%2520when%2520considering%2520the%2520place%2520of%2520deployment%250Aof%2520the%2520underlying%2520deep%2520learning%2520model.%2520To%2520better%2520reflect%2520this%2520real-world%250Ascenario%252C%2520we%2520introduce%2520the%2520novel%2520setting%2520of%2520continual%2520U-OOD%2520detection.%2520To%250Atackle%2520this%2520new%2520setting%252C%2520we%2520propose%2520a%2520method%2520that%2520starts%2520from%2520a%2520U-OOD%2520detector%252C%250Awhich%2520is%2520agnostic%2520to%2520the%2520OOD%2520distribution%252C%2520and%2520slowly%2520updates%2520during%2520deployment%250Ato%2520account%2520for%2520the%2520actual%2520OOD%2520distribution.%2520Our%2520method%2520uses%2520a%2520new%2520U-OOD%2520scoring%250Afunction%2520that%2520combines%2520the%2520Mahalanobis%2520distance%2520with%2520a%2520nearest-neighbor%250Aapproach.%2520Furthermore%252C%2520we%2520design%2520a%2520confidence-scaled%2520few-shot%2520OOD%2520detector%2520that%250Aoutperforms%2520previous%2520methods.%2520We%2520show%2520our%2520method%2520greatly%2520improves%2520upon%2520strong%250Abaselines%2520from%2520related%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Unsupervised%20Out-of-Distribution%20Detection&entry.906535625=Lars%20Doorenbos%20and%20Raphael%20Sznitman%20and%20Pablo%20M%C3%A1rquez-Neila&entry.1292438233=%20%20Deep%20learning%20models%20excel%20when%20the%20data%20distribution%20during%20training%20aligns%0Awith%20testing%20data.%20Yet%2C%20their%20performance%20diminishes%20when%20faced%20with%0Aout-of-distribution%20%28OOD%29%20samples%2C%20leading%20to%20great%20interest%20in%20the%20field%20of%0AOOD%20detection.%20Current%20approaches%20typically%20assume%20that%20OOD%20samples%20originate%0Afrom%20an%20unconcentrated%20distribution%20complementary%20to%20the%20training%20distribution.%0AWhile%20this%20assumption%20is%20appropriate%20in%20the%20traditional%20unsupervised%20OOD%0A%28U-OOD%29%20setting%2C%20it%20proves%20inadequate%20when%20considering%20the%20place%20of%20deployment%0Aof%20the%20underlying%20deep%20learning%20model.%20To%20better%20reflect%20this%20real-world%0Ascenario%2C%20we%20introduce%20the%20novel%20setting%20of%20continual%20U-OOD%20detection.%20To%0Atackle%20this%20new%20setting%2C%20we%20propose%20a%20method%20that%20starts%20from%20a%20U-OOD%20detector%2C%0Awhich%20is%20agnostic%20to%20the%20OOD%20distribution%2C%20and%20slowly%20updates%20during%20deployment%0Ato%20account%20for%20the%20actual%20OOD%20distribution.%20Our%20method%20uses%20a%20new%20U-OOD%20scoring%0Afunction%20that%20combines%20the%20Mahalanobis%20distance%20with%20a%20nearest-neighbor%0Aapproach.%20Furthermore%2C%20we%20design%20a%20confidence-scaled%20few-shot%20OOD%20detector%20that%0Aoutperforms%20previous%20methods.%20We%20show%20our%20method%20greatly%20improves%20upon%20strong%0Abaselines%20from%20related%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02327v1&entry.124074799=Read"},
{"title": "ICC: Quantifying Image Caption Concreteness for Multimodal Dataset\n  Curation", "author": "Moran Yanuka and Morris Alper and Hadar Averbuch-Elor and Raja Giryes", "abstract": "  Web-scale training on paired text-image data is becoming increasingly central\nto multimodal learning, but is challenged by the highly noisy nature of\ndatasets in the wild. Standard data filtering approaches succeed in removing\nmismatched text-image pairs, but permit semantically related but highly\nabstract or subjective text. These approaches lack the fine-grained ability to\nisolate the most concrete samples that provide the strongest signal for\nlearning in a noisy dataset. In this work, we propose a new metric, image\ncaption concreteness, that evaluates caption text without an image reference to\nmeasure its concreteness and relevancy for use in multimodal learning. Our\napproach leverages strong foundation models for measuring visual-semantic\ninformation loss in multimodal representations. We demonstrate that this\nstrongly correlates with human evaluation of concreteness in both single-word\nand sentence-level texts. Moreover, we show that curation using ICC complements\nexisting approaches: It succeeds in selecting the highest quality samples from\nmultimodal web-scale datasets to allow for efficient training in\nresource-constrained settings.\n", "link": "http://arxiv.org/abs/2403.01306v2", "date": "2024-06-04", "relevancy": 2.0868, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICC%3A%20Quantifying%20Image%20Caption%20Concreteness%20for%20Multimodal%20Dataset%0A%20%20Curation&body=Title%3A%20ICC%3A%20Quantifying%20Image%20Caption%20Concreteness%20for%20Multimodal%20Dataset%0A%20%20Curation%0AAuthor%3A%20Moran%20Yanuka%20and%20Morris%20Alper%20and%20Hadar%20Averbuch-Elor%20and%20Raja%20Giryes%0AAbstract%3A%20%20%20Web-scale%20training%20on%20paired%20text-image%20data%20is%20becoming%20increasingly%20central%0Ato%20multimodal%20learning%2C%20but%20is%20challenged%20by%20the%20highly%20noisy%20nature%20of%0Adatasets%20in%20the%20wild.%20Standard%20data%20filtering%20approaches%20succeed%20in%20removing%0Amismatched%20text-image%20pairs%2C%20but%20permit%20semantically%20related%20but%20highly%0Aabstract%20or%20subjective%20text.%20These%20approaches%20lack%20the%20fine-grained%20ability%20to%0Aisolate%20the%20most%20concrete%20samples%20that%20provide%20the%20strongest%20signal%20for%0Alearning%20in%20a%20noisy%20dataset.%20In%20this%20work%2C%20we%20propose%20a%20new%20metric%2C%20image%0Acaption%20concreteness%2C%20that%20evaluates%20caption%20text%20without%20an%20image%20reference%20to%0Ameasure%20its%20concreteness%20and%20relevancy%20for%20use%20in%20multimodal%20learning.%20Our%0Aapproach%20leverages%20strong%20foundation%20models%20for%20measuring%20visual-semantic%0Ainformation%20loss%20in%20multimodal%20representations.%20We%20demonstrate%20that%20this%0Astrongly%20correlates%20with%20human%20evaluation%20of%20concreteness%20in%20both%20single-word%0Aand%20sentence-level%20texts.%20Moreover%2C%20we%20show%20that%20curation%20using%20ICC%20complements%0Aexisting%20approaches%3A%20It%20succeeds%20in%20selecting%20the%20highest%20quality%20samples%20from%0Amultimodal%20web-scale%20datasets%20to%20allow%20for%20efficient%20training%20in%0Aresource-constrained%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICC%253A%2520Quantifying%2520Image%2520Caption%2520Concreteness%2520for%2520Multimodal%2520Dataset%250A%2520%2520Curation%26entry.906535625%3DMoran%2520Yanuka%2520and%2520Morris%2520Alper%2520and%2520Hadar%2520Averbuch-Elor%2520and%2520Raja%2520Giryes%26entry.1292438233%3D%2520%2520Web-scale%2520training%2520on%2520paired%2520text-image%2520data%2520is%2520becoming%2520increasingly%2520central%250Ato%2520multimodal%2520learning%252C%2520but%2520is%2520challenged%2520by%2520the%2520highly%2520noisy%2520nature%2520of%250Adatasets%2520in%2520the%2520wild.%2520Standard%2520data%2520filtering%2520approaches%2520succeed%2520in%2520removing%250Amismatched%2520text-image%2520pairs%252C%2520but%2520permit%2520semantically%2520related%2520but%2520highly%250Aabstract%2520or%2520subjective%2520text.%2520These%2520approaches%2520lack%2520the%2520fine-grained%2520ability%2520to%250Aisolate%2520the%2520most%2520concrete%2520samples%2520that%2520provide%2520the%2520strongest%2520signal%2520for%250Alearning%2520in%2520a%2520noisy%2520dataset.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520metric%252C%2520image%250Acaption%2520concreteness%252C%2520that%2520evaluates%2520caption%2520text%2520without%2520an%2520image%2520reference%2520to%250Ameasure%2520its%2520concreteness%2520and%2520relevancy%2520for%2520use%2520in%2520multimodal%2520learning.%2520Our%250Aapproach%2520leverages%2520strong%2520foundation%2520models%2520for%2520measuring%2520visual-semantic%250Ainformation%2520loss%2520in%2520multimodal%2520representations.%2520We%2520demonstrate%2520that%2520this%250Astrongly%2520correlates%2520with%2520human%2520evaluation%2520of%2520concreteness%2520in%2520both%2520single-word%250Aand%2520sentence-level%2520texts.%2520Moreover%252C%2520we%2520show%2520that%2520curation%2520using%2520ICC%2520complements%250Aexisting%2520approaches%253A%2520It%2520succeeds%2520in%2520selecting%2520the%2520highest%2520quality%2520samples%2520from%250Amultimodal%2520web-scale%2520datasets%2520to%2520allow%2520for%2520efficient%2520training%2520in%250Aresource-constrained%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICC%3A%20Quantifying%20Image%20Caption%20Concreteness%20for%20Multimodal%20Dataset%0A%20%20Curation&entry.906535625=Moran%20Yanuka%20and%20Morris%20Alper%20and%20Hadar%20Averbuch-Elor%20and%20Raja%20Giryes&entry.1292438233=%20%20Web-scale%20training%20on%20paired%20text-image%20data%20is%20becoming%20increasingly%20central%0Ato%20multimodal%20learning%2C%20but%20is%20challenged%20by%20the%20highly%20noisy%20nature%20of%0Adatasets%20in%20the%20wild.%20Standard%20data%20filtering%20approaches%20succeed%20in%20removing%0Amismatched%20text-image%20pairs%2C%20but%20permit%20semantically%20related%20but%20highly%0Aabstract%20or%20subjective%20text.%20These%20approaches%20lack%20the%20fine-grained%20ability%20to%0Aisolate%20the%20most%20concrete%20samples%20that%20provide%20the%20strongest%20signal%20for%0Alearning%20in%20a%20noisy%20dataset.%20In%20this%20work%2C%20we%20propose%20a%20new%20metric%2C%20image%0Acaption%20concreteness%2C%20that%20evaluates%20caption%20text%20without%20an%20image%20reference%20to%0Ameasure%20its%20concreteness%20and%20relevancy%20for%20use%20in%20multimodal%20learning.%20Our%0Aapproach%20leverages%20strong%20foundation%20models%20for%20measuring%20visual-semantic%0Ainformation%20loss%20in%20multimodal%20representations.%20We%20demonstrate%20that%20this%0Astrongly%20correlates%20with%20human%20evaluation%20of%20concreteness%20in%20both%20single-word%0Aand%20sentence-level%20texts.%20Moreover%2C%20we%20show%20that%20curation%20using%20ICC%20complements%0Aexisting%20approaches%3A%20It%20succeeds%20in%20selecting%20the%20highest%20quality%20samples%20from%0Amultimodal%20web-scale%20datasets%20to%20allow%20for%20efficient%20training%20in%0Aresource-constrained%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01306v2&entry.124074799=Read"},
{"title": "Investigating the Impact of Model Instability on Explanations and\n  Uncertainty", "author": "Sara Vera Marjanovi\u0107 and Isabelle Augenstein and Christina Lioma", "abstract": "  Explainable AI methods facilitate the understanding of model behaviour, yet,\nsmall, imperceptible perturbations to inputs can vastly distort explanations.\nAs these explanations are typically evaluated holistically, before model\ndeployment, it is difficult to assess when a particular explanation is\ntrustworthy. Some studies have tried to create confidence estimators for\nexplanations, but none have investigated an existing link between uncertainty\nand explanation quality. We artificially simulate epistemic uncertainty in text\ninput by introducing noise at inference time. In this large-scale empirical\nstudy, we insert different levels of noise perturbations and measure the effect\non the output of pre-trained language models and different uncertainty metrics.\nRealistic perturbations have minimal effect on performance and explanations,\nyet masking has a drastic effect. We find that high uncertainty doesn't\nnecessarily imply low explanation plausibility; the correlation between the two\nmetrics can be moderately positive when noise is exposed during the training\nprocess. This suggests that noise-augmented models may be better at identifying\nsalient tokens when uncertain. Furthermore, when predictive and epistemic\nuncertainty measures are over-confident, the robustness of a saliency map to\nperturbation can indicate model stability issues. Integrated Gradients shows\nthe overall greatest robustness to perturbation, while still showing\nmodel-specific patterns in performance; however, this phenomenon is limited to\nsmaller Transformer-based language models.\n", "link": "http://arxiv.org/abs/2402.13006v2", "date": "2024-06-04", "relevancy": 2.0866, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5741}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5163}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Impact%20of%20Model%20Instability%20on%20Explanations%20and%0A%20%20Uncertainty&body=Title%3A%20Investigating%20the%20Impact%20of%20Model%20Instability%20on%20Explanations%20and%0A%20%20Uncertainty%0AAuthor%3A%20Sara%20Vera%20Marjanovi%C4%87%20and%20Isabelle%20Augenstein%20and%20Christina%20Lioma%0AAbstract%3A%20%20%20Explainable%20AI%20methods%20facilitate%20the%20understanding%20of%20model%20behaviour%2C%20yet%2C%0Asmall%2C%20imperceptible%20perturbations%20to%20inputs%20can%20vastly%20distort%20explanations.%0AAs%20these%20explanations%20are%20typically%20evaluated%20holistically%2C%20before%20model%0Adeployment%2C%20it%20is%20difficult%20to%20assess%20when%20a%20particular%20explanation%20is%0Atrustworthy.%20Some%20studies%20have%20tried%20to%20create%20confidence%20estimators%20for%0Aexplanations%2C%20but%20none%20have%20investigated%20an%20existing%20link%20between%20uncertainty%0Aand%20explanation%20quality.%20We%20artificially%20simulate%20epistemic%20uncertainty%20in%20text%0Ainput%20by%20introducing%20noise%20at%20inference%20time.%20In%20this%20large-scale%20empirical%0Astudy%2C%20we%20insert%20different%20levels%20of%20noise%20perturbations%20and%20measure%20the%20effect%0Aon%20the%20output%20of%20pre-trained%20language%20models%20and%20different%20uncertainty%20metrics.%0ARealistic%20perturbations%20have%20minimal%20effect%20on%20performance%20and%20explanations%2C%0Ayet%20masking%20has%20a%20drastic%20effect.%20We%20find%20that%20high%20uncertainty%20doesn%27t%0Anecessarily%20imply%20low%20explanation%20plausibility%3B%20the%20correlation%20between%20the%20two%0Ametrics%20can%20be%20moderately%20positive%20when%20noise%20is%20exposed%20during%20the%20training%0Aprocess.%20This%20suggests%20that%20noise-augmented%20models%20may%20be%20better%20at%20identifying%0Asalient%20tokens%20when%20uncertain.%20Furthermore%2C%20when%20predictive%20and%20epistemic%0Auncertainty%20measures%20are%20over-confident%2C%20the%20robustness%20of%20a%20saliency%20map%20to%0Aperturbation%20can%20indicate%20model%20stability%20issues.%20Integrated%20Gradients%20shows%0Athe%20overall%20greatest%20robustness%20to%20perturbation%2C%20while%20still%20showing%0Amodel-specific%20patterns%20in%20performance%3B%20however%2C%20this%20phenomenon%20is%20limited%20to%0Asmaller%20Transformer-based%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Impact%2520of%2520Model%2520Instability%2520on%2520Explanations%2520and%250A%2520%2520Uncertainty%26entry.906535625%3DSara%2520Vera%2520Marjanovi%25C4%2587%2520and%2520Isabelle%2520Augenstein%2520and%2520Christina%2520Lioma%26entry.1292438233%3D%2520%2520Explainable%2520AI%2520methods%2520facilitate%2520the%2520understanding%2520of%2520model%2520behaviour%252C%2520yet%252C%250Asmall%252C%2520imperceptible%2520perturbations%2520to%2520inputs%2520can%2520vastly%2520distort%2520explanations.%250AAs%2520these%2520explanations%2520are%2520typically%2520evaluated%2520holistically%252C%2520before%2520model%250Adeployment%252C%2520it%2520is%2520difficult%2520to%2520assess%2520when%2520a%2520particular%2520explanation%2520is%250Atrustworthy.%2520Some%2520studies%2520have%2520tried%2520to%2520create%2520confidence%2520estimators%2520for%250Aexplanations%252C%2520but%2520none%2520have%2520investigated%2520an%2520existing%2520link%2520between%2520uncertainty%250Aand%2520explanation%2520quality.%2520We%2520artificially%2520simulate%2520epistemic%2520uncertainty%2520in%2520text%250Ainput%2520by%2520introducing%2520noise%2520at%2520inference%2520time.%2520In%2520this%2520large-scale%2520empirical%250Astudy%252C%2520we%2520insert%2520different%2520levels%2520of%2520noise%2520perturbations%2520and%2520measure%2520the%2520effect%250Aon%2520the%2520output%2520of%2520pre-trained%2520language%2520models%2520and%2520different%2520uncertainty%2520metrics.%250ARealistic%2520perturbations%2520have%2520minimal%2520effect%2520on%2520performance%2520and%2520explanations%252C%250Ayet%2520masking%2520has%2520a%2520drastic%2520effect.%2520We%2520find%2520that%2520high%2520uncertainty%2520doesn%2527t%250Anecessarily%2520imply%2520low%2520explanation%2520plausibility%253B%2520the%2520correlation%2520between%2520the%2520two%250Ametrics%2520can%2520be%2520moderately%2520positive%2520when%2520noise%2520is%2520exposed%2520during%2520the%2520training%250Aprocess.%2520This%2520suggests%2520that%2520noise-augmented%2520models%2520may%2520be%2520better%2520at%2520identifying%250Asalient%2520tokens%2520when%2520uncertain.%2520Furthermore%252C%2520when%2520predictive%2520and%2520epistemic%250Auncertainty%2520measures%2520are%2520over-confident%252C%2520the%2520robustness%2520of%2520a%2520saliency%2520map%2520to%250Aperturbation%2520can%2520indicate%2520model%2520stability%2520issues.%2520Integrated%2520Gradients%2520shows%250Athe%2520overall%2520greatest%2520robustness%2520to%2520perturbation%252C%2520while%2520still%2520showing%250Amodel-specific%2520patterns%2520in%2520performance%253B%2520however%252C%2520this%2520phenomenon%2520is%2520limited%2520to%250Asmaller%2520Transformer-based%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Impact%20of%20Model%20Instability%20on%20Explanations%20and%0A%20%20Uncertainty&entry.906535625=Sara%20Vera%20Marjanovi%C4%87%20and%20Isabelle%20Augenstein%20and%20Christina%20Lioma&entry.1292438233=%20%20Explainable%20AI%20methods%20facilitate%20the%20understanding%20of%20model%20behaviour%2C%20yet%2C%0Asmall%2C%20imperceptible%20perturbations%20to%20inputs%20can%20vastly%20distort%20explanations.%0AAs%20these%20explanations%20are%20typically%20evaluated%20holistically%2C%20before%20model%0Adeployment%2C%20it%20is%20difficult%20to%20assess%20when%20a%20particular%20explanation%20is%0Atrustworthy.%20Some%20studies%20have%20tried%20to%20create%20confidence%20estimators%20for%0Aexplanations%2C%20but%20none%20have%20investigated%20an%20existing%20link%20between%20uncertainty%0Aand%20explanation%20quality.%20We%20artificially%20simulate%20epistemic%20uncertainty%20in%20text%0Ainput%20by%20introducing%20noise%20at%20inference%20time.%20In%20this%20large-scale%20empirical%0Astudy%2C%20we%20insert%20different%20levels%20of%20noise%20perturbations%20and%20measure%20the%20effect%0Aon%20the%20output%20of%20pre-trained%20language%20models%20and%20different%20uncertainty%20metrics.%0ARealistic%20perturbations%20have%20minimal%20effect%20on%20performance%20and%20explanations%2C%0Ayet%20masking%20has%20a%20drastic%20effect.%20We%20find%20that%20high%20uncertainty%20doesn%27t%0Anecessarily%20imply%20low%20explanation%20plausibility%3B%20the%20correlation%20between%20the%20two%0Ametrics%20can%20be%20moderately%20positive%20when%20noise%20is%20exposed%20during%20the%20training%0Aprocess.%20This%20suggests%20that%20noise-augmented%20models%20may%20be%20better%20at%20identifying%0Asalient%20tokens%20when%20uncertain.%20Furthermore%2C%20when%20predictive%20and%20epistemic%0Auncertainty%20measures%20are%20over-confident%2C%20the%20robustness%20of%20a%20saliency%20map%20to%0Aperturbation%20can%20indicate%20model%20stability%20issues.%20Integrated%20Gradients%20shows%0Athe%20overall%20greatest%20robustness%20to%20perturbation%2C%20while%20still%20showing%0Amodel-specific%20patterns%20in%20performance%3B%20however%2C%20this%20phenomenon%20is%20limited%20to%0Asmaller%20Transformer-based%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13006v2&entry.124074799=Read"},
{"title": "Into the Unknown: Self-Learning Large Language Models", "author": "Teddy Ferdinan and Jan Koco\u0144 and Przemys\u0142aw Kazienko", "abstract": "  We address the main problem of self-learning LLM: the question of what to\nlearn. We propose a self-learning LLM framework that enables an LLM to\nindependently learn previously unknown knowledge through selfassessment of\ntheir own hallucinations. Using the hallucination score, we introduce a new\nconcept of Points in the Unknown (PiUs), along with one extrinsic and three\nintrinsic methods for automatic PiUs identification. It facilitates the\ncreation of a self-learning loop that focuses exclusively on the knowledge gap\nin Points in the Unknown, resulting in a reduced hallucination score. We also\ndeveloped evaluation metrics for gauging an LLM's self-learning capability. Our\nexperiments revealed that 7B-Mistral models that have been finetuned or aligned\nand RWKV5-Eagle are capable of self-learning considerably well. Our\nself-learning concept allows more efficient LLM updates and opens new\nperspectives for knowledge exchange. It may also increase public trust in AI.\n", "link": "http://arxiv.org/abs/2402.09147v2", "date": "2024-06-04", "relevancy": 2.082, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5455}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5172}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Into%20the%20Unknown%3A%20Self-Learning%20Large%20Language%20Models&body=Title%3A%20Into%20the%20Unknown%3A%20Self-Learning%20Large%20Language%20Models%0AAuthor%3A%20Teddy%20Ferdinan%20and%20Jan%20Koco%C5%84%20and%20Przemys%C5%82aw%20Kazienko%0AAbstract%3A%20%20%20We%20address%20the%20main%20problem%20of%20self-learning%20LLM%3A%20the%20question%20of%20what%20to%0Alearn.%20We%20propose%20a%20self-learning%20LLM%20framework%20that%20enables%20an%20LLM%20to%0Aindependently%20learn%20previously%20unknown%20knowledge%20through%20selfassessment%20of%0Atheir%20own%20hallucinations.%20Using%20the%20hallucination%20score%2C%20we%20introduce%20a%20new%0Aconcept%20of%20Points%20in%20the%20Unknown%20%28PiUs%29%2C%20along%20with%20one%20extrinsic%20and%20three%0Aintrinsic%20methods%20for%20automatic%20PiUs%20identification.%20It%20facilitates%20the%0Acreation%20of%20a%20self-learning%20loop%20that%20focuses%20exclusively%20on%20the%20knowledge%20gap%0Ain%20Points%20in%20the%20Unknown%2C%20resulting%20in%20a%20reduced%20hallucination%20score.%20We%20also%0Adeveloped%20evaluation%20metrics%20for%20gauging%20an%20LLM%27s%20self-learning%20capability.%20Our%0Aexperiments%20revealed%20that%207B-Mistral%20models%20that%20have%20been%20finetuned%20or%20aligned%0Aand%20RWKV5-Eagle%20are%20capable%20of%20self-learning%20considerably%20well.%20Our%0Aself-learning%20concept%20allows%20more%20efficient%20LLM%20updates%20and%20opens%20new%0Aperspectives%20for%20knowledge%20exchange.%20It%20may%20also%20increase%20public%20trust%20in%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInto%2520the%2520Unknown%253A%2520Self-Learning%2520Large%2520Language%2520Models%26entry.906535625%3DTeddy%2520Ferdinan%2520and%2520Jan%2520Koco%25C5%2584%2520and%2520Przemys%25C5%2582aw%2520Kazienko%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520main%2520problem%2520of%2520self-learning%2520LLM%253A%2520the%2520question%2520of%2520what%2520to%250Alearn.%2520We%2520propose%2520a%2520self-learning%2520LLM%2520framework%2520that%2520enables%2520an%2520LLM%2520to%250Aindependently%2520learn%2520previously%2520unknown%2520knowledge%2520through%2520selfassessment%2520of%250Atheir%2520own%2520hallucinations.%2520Using%2520the%2520hallucination%2520score%252C%2520we%2520introduce%2520a%2520new%250Aconcept%2520of%2520Points%2520in%2520the%2520Unknown%2520%2528PiUs%2529%252C%2520along%2520with%2520one%2520extrinsic%2520and%2520three%250Aintrinsic%2520methods%2520for%2520automatic%2520PiUs%2520identification.%2520It%2520facilitates%2520the%250Acreation%2520of%2520a%2520self-learning%2520loop%2520that%2520focuses%2520exclusively%2520on%2520the%2520knowledge%2520gap%250Ain%2520Points%2520in%2520the%2520Unknown%252C%2520resulting%2520in%2520a%2520reduced%2520hallucination%2520score.%2520We%2520also%250Adeveloped%2520evaluation%2520metrics%2520for%2520gauging%2520an%2520LLM%2527s%2520self-learning%2520capability.%2520Our%250Aexperiments%2520revealed%2520that%25207B-Mistral%2520models%2520that%2520have%2520been%2520finetuned%2520or%2520aligned%250Aand%2520RWKV5-Eagle%2520are%2520capable%2520of%2520self-learning%2520considerably%2520well.%2520Our%250Aself-learning%2520concept%2520allows%2520more%2520efficient%2520LLM%2520updates%2520and%2520opens%2520new%250Aperspectives%2520for%2520knowledge%2520exchange.%2520It%2520may%2520also%2520increase%2520public%2520trust%2520in%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Into%20the%20Unknown%3A%20Self-Learning%20Large%20Language%20Models&entry.906535625=Teddy%20Ferdinan%20and%20Jan%20Koco%C5%84%20and%20Przemys%C5%82aw%20Kazienko&entry.1292438233=%20%20We%20address%20the%20main%20problem%20of%20self-learning%20LLM%3A%20the%20question%20of%20what%20to%0Alearn.%20We%20propose%20a%20self-learning%20LLM%20framework%20that%20enables%20an%20LLM%20to%0Aindependently%20learn%20previously%20unknown%20knowledge%20through%20selfassessment%20of%0Atheir%20own%20hallucinations.%20Using%20the%20hallucination%20score%2C%20we%20introduce%20a%20new%0Aconcept%20of%20Points%20in%20the%20Unknown%20%28PiUs%29%2C%20along%20with%20one%20extrinsic%20and%20three%0Aintrinsic%20methods%20for%20automatic%20PiUs%20identification.%20It%20facilitates%20the%0Acreation%20of%20a%20self-learning%20loop%20that%20focuses%20exclusively%20on%20the%20knowledge%20gap%0Ain%20Points%20in%20the%20Unknown%2C%20resulting%20in%20a%20reduced%20hallucination%20score.%20We%20also%0Adeveloped%20evaluation%20metrics%20for%20gauging%20an%20LLM%27s%20self-learning%20capability.%20Our%0Aexperiments%20revealed%20that%207B-Mistral%20models%20that%20have%20been%20finetuned%20or%20aligned%0Aand%20RWKV5-Eagle%20are%20capable%20of%20self-learning%20considerably%20well.%20Our%0Aself-learning%20concept%20allows%20more%20efficient%20LLM%20updates%20and%20opens%20new%0Aperspectives%20for%20knowledge%20exchange.%20It%20may%20also%20increase%20public%20trust%20in%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09147v2&entry.124074799=Read"},
{"title": "Landscape-Aware Growing: The Power of a Little LAG", "author": "Stefani Karp and Nikunj Saunshi and Sobhan Miryoosefi and Sashank J. Reddi and Sanjiv Kumar", "abstract": "  Recently, there has been increasing interest in efficient pretraining\nparadigms for training Transformer-based models. Several recent approaches use\nsmaller models to initialize larger models in order to save computation (e.g.,\nstacking and fusion). In this work, we study the fundamental question of how to\nselect the best growing strategy from a given pool of growing strategies. Prior\nworks have extensively focused on loss- and/or function-preserving behavior at\ninitialization or simply performance at the end of training. Instead, we\nidentify that behavior at initialization can be misleading as a predictor of\nfinal performance and present an alternative perspective based on early\ntraining dynamics, which we call \"landscape-aware growing (LAG)\". We perform\nextensive analysis of correlation of the final performance with performance in\nthe initial steps of training and find early and more accurate predictions of\nthe optimal growing strategy (i.e., with only a small \"lag\" after\ninitialization). This perspective also motivates an adaptive strategy for\ngradual stacking.\n", "link": "http://arxiv.org/abs/2406.02469v1", "date": "2024-06-04", "relevancy": 2.0768, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5535}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5127}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Landscape-Aware%20Growing%3A%20The%20Power%20of%20a%20Little%20LAG&body=Title%3A%20Landscape-Aware%20Growing%3A%20The%20Power%20of%20a%20Little%20LAG%0AAuthor%3A%20Stefani%20Karp%20and%20Nikunj%20Saunshi%20and%20Sobhan%20Miryoosefi%20and%20Sashank%20J.%20Reddi%20and%20Sanjiv%20Kumar%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20increasing%20interest%20in%20efficient%20pretraining%0Aparadigms%20for%20training%20Transformer-based%20models.%20Several%20recent%20approaches%20use%0Asmaller%20models%20to%20initialize%20larger%20models%20in%20order%20to%20save%20computation%20%28e.g.%2C%0Astacking%20and%20fusion%29.%20In%20this%20work%2C%20we%20study%20the%20fundamental%20question%20of%20how%20to%0Aselect%20the%20best%20growing%20strategy%20from%20a%20given%20pool%20of%20growing%20strategies.%20Prior%0Aworks%20have%20extensively%20focused%20on%20loss-%20and/or%20function-preserving%20behavior%20at%0Ainitialization%20or%20simply%20performance%20at%20the%20end%20of%20training.%20Instead%2C%20we%0Aidentify%20that%20behavior%20at%20initialization%20can%20be%20misleading%20as%20a%20predictor%20of%0Afinal%20performance%20and%20present%20an%20alternative%20perspective%20based%20on%20early%0Atraining%20dynamics%2C%20which%20we%20call%20%22landscape-aware%20growing%20%28LAG%29%22.%20We%20perform%0Aextensive%20analysis%20of%20correlation%20of%20the%20final%20performance%20with%20performance%20in%0Athe%20initial%20steps%20of%20training%20and%20find%20early%20and%20more%20accurate%20predictions%20of%0Athe%20optimal%20growing%20strategy%20%28i.e.%2C%20with%20only%20a%20small%20%22lag%22%20after%0Ainitialization%29.%20This%20perspective%20also%20motivates%20an%20adaptive%20strategy%20for%0Agradual%20stacking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLandscape-Aware%2520Growing%253A%2520The%2520Power%2520of%2520a%2520Little%2520LAG%26entry.906535625%3DStefani%2520Karp%2520and%2520Nikunj%2520Saunshi%2520and%2520Sobhan%2520Miryoosefi%2520and%2520Sashank%2520J.%2520Reddi%2520and%2520Sanjiv%2520Kumar%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520increasing%2520interest%2520in%2520efficient%2520pretraining%250Aparadigms%2520for%2520training%2520Transformer-based%2520models.%2520Several%2520recent%2520approaches%2520use%250Asmaller%2520models%2520to%2520initialize%2520larger%2520models%2520in%2520order%2520to%2520save%2520computation%2520%2528e.g.%252C%250Astacking%2520and%2520fusion%2529.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520fundamental%2520question%2520of%2520how%2520to%250Aselect%2520the%2520best%2520growing%2520strategy%2520from%2520a%2520given%2520pool%2520of%2520growing%2520strategies.%2520Prior%250Aworks%2520have%2520extensively%2520focused%2520on%2520loss-%2520and/or%2520function-preserving%2520behavior%2520at%250Ainitialization%2520or%2520simply%2520performance%2520at%2520the%2520end%2520of%2520training.%2520Instead%252C%2520we%250Aidentify%2520that%2520behavior%2520at%2520initialization%2520can%2520be%2520misleading%2520as%2520a%2520predictor%2520of%250Afinal%2520performance%2520and%2520present%2520an%2520alternative%2520perspective%2520based%2520on%2520early%250Atraining%2520dynamics%252C%2520which%2520we%2520call%2520%2522landscape-aware%2520growing%2520%2528LAG%2529%2522.%2520We%2520perform%250Aextensive%2520analysis%2520of%2520correlation%2520of%2520the%2520final%2520performance%2520with%2520performance%2520in%250Athe%2520initial%2520steps%2520of%2520training%2520and%2520find%2520early%2520and%2520more%2520accurate%2520predictions%2520of%250Athe%2520optimal%2520growing%2520strategy%2520%2528i.e.%252C%2520with%2520only%2520a%2520small%2520%2522lag%2522%2520after%250Ainitialization%2529.%2520This%2520perspective%2520also%2520motivates%2520an%2520adaptive%2520strategy%2520for%250Agradual%2520stacking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Landscape-Aware%20Growing%3A%20The%20Power%20of%20a%20Little%20LAG&entry.906535625=Stefani%20Karp%20and%20Nikunj%20Saunshi%20and%20Sobhan%20Miryoosefi%20and%20Sashank%20J.%20Reddi%20and%20Sanjiv%20Kumar&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20increasing%20interest%20in%20efficient%20pretraining%0Aparadigms%20for%20training%20Transformer-based%20models.%20Several%20recent%20approaches%20use%0Asmaller%20models%20to%20initialize%20larger%20models%20in%20order%20to%20save%20computation%20%28e.g.%2C%0Astacking%20and%20fusion%29.%20In%20this%20work%2C%20we%20study%20the%20fundamental%20question%20of%20how%20to%0Aselect%20the%20best%20growing%20strategy%20from%20a%20given%20pool%20of%20growing%20strategies.%20Prior%0Aworks%20have%20extensively%20focused%20on%20loss-%20and/or%20function-preserving%20behavior%20at%0Ainitialization%20or%20simply%20performance%20at%20the%20end%20of%20training.%20Instead%2C%20we%0Aidentify%20that%20behavior%20at%20initialization%20can%20be%20misleading%20as%20a%20predictor%20of%0Afinal%20performance%20and%20present%20an%20alternative%20perspective%20based%20on%20early%0Atraining%20dynamics%2C%20which%20we%20call%20%22landscape-aware%20growing%20%28LAG%29%22.%20We%20perform%0Aextensive%20analysis%20of%20correlation%20of%20the%20final%20performance%20with%20performance%20in%0Athe%20initial%20steps%20of%20training%20and%20find%20early%20and%20more%20accurate%20predictions%20of%0Athe%20optimal%20growing%20strategy%20%28i.e.%2C%20with%20only%20a%20small%20%22lag%22%20after%0Ainitialization%29.%20This%20perspective%20also%20motivates%20an%20adaptive%20strategy%20for%0Agradual%20stacking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02469v1&entry.124074799=Read"},
{"title": "Generative Conditional Distributions by Neural (Entropic) Optimal\n  Transport", "author": "Bao Nguyen and Binh Nguyen and Hieu Trung Nguyen and Viet Anh Nguyen", "abstract": "  Learning conditional distributions is challenging because the desired outcome\nis not a single distribution but multiple distributions that correspond to\nmultiple instances of the covariates. We introduce a novel neural entropic\noptimal transport method designed to effectively learn generative models of\nconditional distributions, particularly in scenarios characterized by limited\nsample sizes. Our method relies on the minimax training of two neural networks:\na generative network parametrizing the inverse cumulative distribution\nfunctions of the conditional distributions and another network parametrizing\nthe conditional Kantorovich potential. To prevent overfitting, we regularize\nthe objective function by penalizing the Lipschitz constant of the network\noutput. Our experiments on real-world datasets show the effectiveness of our\nalgorithm compared to state-of-the-art conditional distribution learning\ntechniques. Our implementation can be found at\nhttps://github.com/nguyenngocbaocmt02/GENTLE.\n", "link": "http://arxiv.org/abs/2406.02317v1", "date": "2024-06-04", "relevancy": 2.0681, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.539}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5022}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Conditional%20Distributions%20by%20Neural%20%28Entropic%29%20Optimal%0A%20%20Transport&body=Title%3A%20Generative%20Conditional%20Distributions%20by%20Neural%20%28Entropic%29%20Optimal%0A%20%20Transport%0AAuthor%3A%20Bao%20Nguyen%20and%20Binh%20Nguyen%20and%20Hieu%20Trung%20Nguyen%20and%20Viet%20Anh%20Nguyen%0AAbstract%3A%20%20%20Learning%20conditional%20distributions%20is%20challenging%20because%20the%20desired%20outcome%0Ais%20not%20a%20single%20distribution%20but%20multiple%20distributions%20that%20correspond%20to%0Amultiple%20instances%20of%20the%20covariates.%20We%20introduce%20a%20novel%20neural%20entropic%0Aoptimal%20transport%20method%20designed%20to%20effectively%20learn%20generative%20models%20of%0Aconditional%20distributions%2C%20particularly%20in%20scenarios%20characterized%20by%20limited%0Asample%20sizes.%20Our%20method%20relies%20on%20the%20minimax%20training%20of%20two%20neural%20networks%3A%0Aa%20generative%20network%20parametrizing%20the%20inverse%20cumulative%20distribution%0Afunctions%20of%20the%20conditional%20distributions%20and%20another%20network%20parametrizing%0Athe%20conditional%20Kantorovich%20potential.%20To%20prevent%20overfitting%2C%20we%20regularize%0Athe%20objective%20function%20by%20penalizing%20the%20Lipschitz%20constant%20of%20the%20network%0Aoutput.%20Our%20experiments%20on%20real-world%20datasets%20show%20the%20effectiveness%20of%20our%0Aalgorithm%20compared%20to%20state-of-the-art%20conditional%20distribution%20learning%0Atechniques.%20Our%20implementation%20can%20be%20found%20at%0Ahttps%3A//github.com/nguyenngocbaocmt02/GENTLE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Conditional%2520Distributions%2520by%2520Neural%2520%2528Entropic%2529%2520Optimal%250A%2520%2520Transport%26entry.906535625%3DBao%2520Nguyen%2520and%2520Binh%2520Nguyen%2520and%2520Hieu%2520Trung%2520Nguyen%2520and%2520Viet%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520Learning%2520conditional%2520distributions%2520is%2520challenging%2520because%2520the%2520desired%2520outcome%250Ais%2520not%2520a%2520single%2520distribution%2520but%2520multiple%2520distributions%2520that%2520correspond%2520to%250Amultiple%2520instances%2520of%2520the%2520covariates.%2520We%2520introduce%2520a%2520novel%2520neural%2520entropic%250Aoptimal%2520transport%2520method%2520designed%2520to%2520effectively%2520learn%2520generative%2520models%2520of%250Aconditional%2520distributions%252C%2520particularly%2520in%2520scenarios%2520characterized%2520by%2520limited%250Asample%2520sizes.%2520Our%2520method%2520relies%2520on%2520the%2520minimax%2520training%2520of%2520two%2520neural%2520networks%253A%250Aa%2520generative%2520network%2520parametrizing%2520the%2520inverse%2520cumulative%2520distribution%250Afunctions%2520of%2520the%2520conditional%2520distributions%2520and%2520another%2520network%2520parametrizing%250Athe%2520conditional%2520Kantorovich%2520potential.%2520To%2520prevent%2520overfitting%252C%2520we%2520regularize%250Athe%2520objective%2520function%2520by%2520penalizing%2520the%2520Lipschitz%2520constant%2520of%2520the%2520network%250Aoutput.%2520Our%2520experiments%2520on%2520real-world%2520datasets%2520show%2520the%2520effectiveness%2520of%2520our%250Aalgorithm%2520compared%2520to%2520state-of-the-art%2520conditional%2520distribution%2520learning%250Atechniques.%2520Our%2520implementation%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/nguyenngocbaocmt02/GENTLE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Conditional%20Distributions%20by%20Neural%20%28Entropic%29%20Optimal%0A%20%20Transport&entry.906535625=Bao%20Nguyen%20and%20Binh%20Nguyen%20and%20Hieu%20Trung%20Nguyen%20and%20Viet%20Anh%20Nguyen&entry.1292438233=%20%20Learning%20conditional%20distributions%20is%20challenging%20because%20the%20desired%20outcome%0Ais%20not%20a%20single%20distribution%20but%20multiple%20distributions%20that%20correspond%20to%0Amultiple%20instances%20of%20the%20covariates.%20We%20introduce%20a%20novel%20neural%20entropic%0Aoptimal%20transport%20method%20designed%20to%20effectively%20learn%20generative%20models%20of%0Aconditional%20distributions%2C%20particularly%20in%20scenarios%20characterized%20by%20limited%0Asample%20sizes.%20Our%20method%20relies%20on%20the%20minimax%20training%20of%20two%20neural%20networks%3A%0Aa%20generative%20network%20parametrizing%20the%20inverse%20cumulative%20distribution%0Afunctions%20of%20the%20conditional%20distributions%20and%20another%20network%20parametrizing%0Athe%20conditional%20Kantorovich%20potential.%20To%20prevent%20overfitting%2C%20we%20regularize%0Athe%20objective%20function%20by%20penalizing%20the%20Lipschitz%20constant%20of%20the%20network%0Aoutput.%20Our%20experiments%20on%20real-world%20datasets%20show%20the%20effectiveness%20of%20our%0Aalgorithm%20compared%20to%20state-of-the-art%20conditional%20distribution%20learning%0Atechniques.%20Our%20implementation%20can%20be%20found%20at%0Ahttps%3A//github.com/nguyenngocbaocmt02/GENTLE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02317v1&entry.124074799=Read"},
{"title": "Fingerprint Matching with Localized Deep Representation", "author": "Yongjie Duan and Zhiyu Pan and Jianjiang Feng and Jie Zhou", "abstract": "  Compared to minutia-based fingerprint representations, fixed-length\nrepresentations are attractive due to simple and efficient matching. However,\nfixed-length fingerprint representations are limited in accuracy when matching\nfingerprints with different visible areas, which can occur due to different\nfinger poses or acquisition methods. To address this issue, we propose a\nlocalized deep representation of fingerprint, named LDRF. By focusing on the\ndiscriminative characteristics within local regions, LDRF provides a more\nrobust and accurate fixed-length representation for fingerprints with variable\nvisible areas. LDRF can be adapted to retain information within any valid area,\nmaking it highly flexible. The matching scores produced by LDRF also exhibit\nintuitive statistical characteristics, which led us to propose a matching score\nnormalization technique to mitigate the uncertainty in the cases of very small\noverlapping area. With this new technique, we can maintain a high level of\naccuracy and reliability in our fingerprint matching, even as the size of the\ndatabase grows rapidly. Our experimental results on 21 datasets containing over\n140K fingerprints of various finger poses and impression types show that LDRF\noutperforms other fixed-length representations and is robust to sensing\ntechnologies and impression types. Besides, the proposed matching score\nnormalization effectively reduces the false match rate (FMR) in large-scale\nidentification experiments comprising over 5.11 million fingerprints.\nSpecifically, this technique results in a reduction of two orders of magnitude\ncompared to matching without matching score normalization and five orders of\nmagnitude compared to prior works.\n", "link": "http://arxiv.org/abs/2311.18576v3", "date": "2024-06-04", "relevancy": 2.066, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5766}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.475}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fingerprint%20Matching%20with%20Localized%20Deep%20Representation&body=Title%3A%20Fingerprint%20Matching%20with%20Localized%20Deep%20Representation%0AAuthor%3A%20Yongjie%20Duan%20and%20Zhiyu%20Pan%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Compared%20to%20minutia-based%20fingerprint%20representations%2C%20fixed-length%0Arepresentations%20are%20attractive%20due%20to%20simple%20and%20efficient%20matching.%20However%2C%0Afixed-length%20fingerprint%20representations%20are%20limited%20in%20accuracy%20when%20matching%0Afingerprints%20with%20different%20visible%20areas%2C%20which%20can%20occur%20due%20to%20different%0Afinger%20poses%20or%20acquisition%20methods.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Alocalized%20deep%20representation%20of%20fingerprint%2C%20named%20LDRF.%20By%20focusing%20on%20the%0Adiscriminative%20characteristics%20within%20local%20regions%2C%20LDRF%20provides%20a%20more%0Arobust%20and%20accurate%20fixed-length%20representation%20for%20fingerprints%20with%20variable%0Avisible%20areas.%20LDRF%20can%20be%20adapted%20to%20retain%20information%20within%20any%20valid%20area%2C%0Amaking%20it%20highly%20flexible.%20The%20matching%20scores%20produced%20by%20LDRF%20also%20exhibit%0Aintuitive%20statistical%20characteristics%2C%20which%20led%20us%20to%20propose%20a%20matching%20score%0Anormalization%20technique%20to%20mitigate%20the%20uncertainty%20in%20the%20cases%20of%20very%20small%0Aoverlapping%20area.%20With%20this%20new%20technique%2C%20we%20can%20maintain%20a%20high%20level%20of%0Aaccuracy%20and%20reliability%20in%20our%20fingerprint%20matching%2C%20even%20as%20the%20size%20of%20the%0Adatabase%20grows%20rapidly.%20Our%20experimental%20results%20on%2021%20datasets%20containing%20over%0A140K%20fingerprints%20of%20various%20finger%20poses%20and%20impression%20types%20show%20that%20LDRF%0Aoutperforms%20other%20fixed-length%20representations%20and%20is%20robust%20to%20sensing%0Atechnologies%20and%20impression%20types.%20Besides%2C%20the%20proposed%20matching%20score%0Anormalization%20effectively%20reduces%20the%20false%20match%20rate%20%28FMR%29%20in%20large-scale%0Aidentification%20experiments%20comprising%20over%205.11%20million%20fingerprints.%0ASpecifically%2C%20this%20technique%20results%20in%20a%20reduction%20of%20two%20orders%20of%20magnitude%0Acompared%20to%20matching%20without%20matching%20score%20normalization%20and%20five%20orders%20of%0Amagnitude%20compared%20to%20prior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18576v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFingerprint%2520Matching%2520with%2520Localized%2520Deep%2520Representation%26entry.906535625%3DYongjie%2520Duan%2520and%2520Zhiyu%2520Pan%2520and%2520Jianjiang%2520Feng%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Compared%2520to%2520minutia-based%2520fingerprint%2520representations%252C%2520fixed-length%250Arepresentations%2520are%2520attractive%2520due%2520to%2520simple%2520and%2520efficient%2520matching.%2520However%252C%250Afixed-length%2520fingerprint%2520representations%2520are%2520limited%2520in%2520accuracy%2520when%2520matching%250Afingerprints%2520with%2520different%2520visible%2520areas%252C%2520which%2520can%2520occur%2520due%2520to%2520different%250Afinger%2520poses%2520or%2520acquisition%2520methods.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Alocalized%2520deep%2520representation%2520of%2520fingerprint%252C%2520named%2520LDRF.%2520By%2520focusing%2520on%2520the%250Adiscriminative%2520characteristics%2520within%2520local%2520regions%252C%2520LDRF%2520provides%2520a%2520more%250Arobust%2520and%2520accurate%2520fixed-length%2520representation%2520for%2520fingerprints%2520with%2520variable%250Avisible%2520areas.%2520LDRF%2520can%2520be%2520adapted%2520to%2520retain%2520information%2520within%2520any%2520valid%2520area%252C%250Amaking%2520it%2520highly%2520flexible.%2520The%2520matching%2520scores%2520produced%2520by%2520LDRF%2520also%2520exhibit%250Aintuitive%2520statistical%2520characteristics%252C%2520which%2520led%2520us%2520to%2520propose%2520a%2520matching%2520score%250Anormalization%2520technique%2520to%2520mitigate%2520the%2520uncertainty%2520in%2520the%2520cases%2520of%2520very%2520small%250Aoverlapping%2520area.%2520With%2520this%2520new%2520technique%252C%2520we%2520can%2520maintain%2520a%2520high%2520level%2520of%250Aaccuracy%2520and%2520reliability%2520in%2520our%2520fingerprint%2520matching%252C%2520even%2520as%2520the%2520size%2520of%2520the%250Adatabase%2520grows%2520rapidly.%2520Our%2520experimental%2520results%2520on%252021%2520datasets%2520containing%2520over%250A140K%2520fingerprints%2520of%2520various%2520finger%2520poses%2520and%2520impression%2520types%2520show%2520that%2520LDRF%250Aoutperforms%2520other%2520fixed-length%2520representations%2520and%2520is%2520robust%2520to%2520sensing%250Atechnologies%2520and%2520impression%2520types.%2520Besides%252C%2520the%2520proposed%2520matching%2520score%250Anormalization%2520effectively%2520reduces%2520the%2520false%2520match%2520rate%2520%2528FMR%2529%2520in%2520large-scale%250Aidentification%2520experiments%2520comprising%2520over%25205.11%2520million%2520fingerprints.%250ASpecifically%252C%2520this%2520technique%2520results%2520in%2520a%2520reduction%2520of%2520two%2520orders%2520of%2520magnitude%250Acompared%2520to%2520matching%2520without%2520matching%2520score%2520normalization%2520and%2520five%2520orders%2520of%250Amagnitude%2520compared%2520to%2520prior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18576v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fingerprint%20Matching%20with%20Localized%20Deep%20Representation&entry.906535625=Yongjie%20Duan%20and%20Zhiyu%20Pan%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou&entry.1292438233=%20%20Compared%20to%20minutia-based%20fingerprint%20representations%2C%20fixed-length%0Arepresentations%20are%20attractive%20due%20to%20simple%20and%20efficient%20matching.%20However%2C%0Afixed-length%20fingerprint%20representations%20are%20limited%20in%20accuracy%20when%20matching%0Afingerprints%20with%20different%20visible%20areas%2C%20which%20can%20occur%20due%20to%20different%0Afinger%20poses%20or%20acquisition%20methods.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Alocalized%20deep%20representation%20of%20fingerprint%2C%20named%20LDRF.%20By%20focusing%20on%20the%0Adiscriminative%20characteristics%20within%20local%20regions%2C%20LDRF%20provides%20a%20more%0Arobust%20and%20accurate%20fixed-length%20representation%20for%20fingerprints%20with%20variable%0Avisible%20areas.%20LDRF%20can%20be%20adapted%20to%20retain%20information%20within%20any%20valid%20area%2C%0Amaking%20it%20highly%20flexible.%20The%20matching%20scores%20produced%20by%20LDRF%20also%20exhibit%0Aintuitive%20statistical%20characteristics%2C%20which%20led%20us%20to%20propose%20a%20matching%20score%0Anormalization%20technique%20to%20mitigate%20the%20uncertainty%20in%20the%20cases%20of%20very%20small%0Aoverlapping%20area.%20With%20this%20new%20technique%2C%20we%20can%20maintain%20a%20high%20level%20of%0Aaccuracy%20and%20reliability%20in%20our%20fingerprint%20matching%2C%20even%20as%20the%20size%20of%20the%0Adatabase%20grows%20rapidly.%20Our%20experimental%20results%20on%2021%20datasets%20containing%20over%0A140K%20fingerprints%20of%20various%20finger%20poses%20and%20impression%20types%20show%20that%20LDRF%0Aoutperforms%20other%20fixed-length%20representations%20and%20is%20robust%20to%20sensing%0Atechnologies%20and%20impression%20types.%20Besides%2C%20the%20proposed%20matching%20score%0Anormalization%20effectively%20reduces%20the%20false%20match%20rate%20%28FMR%29%20in%20large-scale%0Aidentification%20experiments%20comprising%20over%205.11%20million%20fingerprints.%0ASpecifically%2C%20this%20technique%20results%20in%20a%20reduction%20of%20two%20orders%20of%20magnitude%0Acompared%20to%20matching%20without%20matching%20score%20normalization%20and%20five%20orders%20of%0Amagnitude%20compared%20to%20prior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18576v3&entry.124074799=Read"},
{"title": "ReLUs Are Sufficient for Learning Implicit Neural Representations", "author": "Joseph Shenouda and Yamin Zhou and Robert D. Nowak", "abstract": "  Motivated by the growing theoretical understanding of neural networks that\nemploy the Rectified Linear Unit (ReLU) as their activation function, we\nrevisit the use of ReLU activation functions for learning implicit neural\nrepresentations (INRs). Inspired by second order B-spline wavelets, we\nincorporate a set of simple constraints to the ReLU neurons in each layer of a\ndeep neural network (DNN) to remedy the spectral bias. This in turn enables its\nuse for various INR tasks. Empirically, we demonstrate that, contrary to\npopular belief, one can learn state-of-the-art INRs based on a DNN composed of\nonly ReLU neurons. Next, by leveraging recent theoretical works which\ncharacterize the kinds of functions ReLU neural networks learn, we provide a\nway to quantify the regularity of the learned function. This offers a\nprincipled approach to selecting the hyperparameters in INR architectures. We\nsubstantiate our claims through experiments in signal representation, super\nresolution, and computed tomography, demonstrating the versatility and\neffectiveness of our method. The code for all experiments can be found at\nhttps://github.com/joeshenouda/relu-inrs.\n", "link": "http://arxiv.org/abs/2406.02529v1", "date": "2024-06-04", "relevancy": 2.0598, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5345}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5308}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReLUs%20Are%20Sufficient%20for%20Learning%20Implicit%20Neural%20Representations&body=Title%3A%20ReLUs%20Are%20Sufficient%20for%20Learning%20Implicit%20Neural%20Representations%0AAuthor%3A%20Joseph%20Shenouda%20and%20Yamin%20Zhou%20and%20Robert%20D.%20Nowak%0AAbstract%3A%20%20%20Motivated%20by%20the%20growing%20theoretical%20understanding%20of%20neural%20networks%20that%0Aemploy%20the%20Rectified%20Linear%20Unit%20%28ReLU%29%20as%20their%20activation%20function%2C%20we%0Arevisit%20the%20use%20of%20ReLU%20activation%20functions%20for%20learning%20implicit%20neural%0Arepresentations%20%28INRs%29.%20Inspired%20by%20second%20order%20B-spline%20wavelets%2C%20we%0Aincorporate%20a%20set%20of%20simple%20constraints%20to%20the%20ReLU%20neurons%20in%20each%20layer%20of%20a%0Adeep%20neural%20network%20%28DNN%29%20to%20remedy%20the%20spectral%20bias.%20This%20in%20turn%20enables%20its%0Ause%20for%20various%20INR%20tasks.%20Empirically%2C%20we%20demonstrate%20that%2C%20contrary%20to%0Apopular%20belief%2C%20one%20can%20learn%20state-of-the-art%20INRs%20based%20on%20a%20DNN%20composed%20of%0Aonly%20ReLU%20neurons.%20Next%2C%20by%20leveraging%20recent%20theoretical%20works%20which%0Acharacterize%20the%20kinds%20of%20functions%20ReLU%20neural%20networks%20learn%2C%20we%20provide%20a%0Away%20to%20quantify%20the%20regularity%20of%20the%20learned%20function.%20This%20offers%20a%0Aprincipled%20approach%20to%20selecting%20the%20hyperparameters%20in%20INR%20architectures.%20We%0Asubstantiate%20our%20claims%20through%20experiments%20in%20signal%20representation%2C%20super%0Aresolution%2C%20and%20computed%20tomography%2C%20demonstrating%20the%20versatility%20and%0Aeffectiveness%20of%20our%20method.%20The%20code%20for%20all%20experiments%20can%20be%20found%20at%0Ahttps%3A//github.com/joeshenouda/relu-inrs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReLUs%2520Are%2520Sufficient%2520for%2520Learning%2520Implicit%2520Neural%2520Representations%26entry.906535625%3DJoseph%2520Shenouda%2520and%2520Yamin%2520Zhou%2520and%2520Robert%2520D.%2520Nowak%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520growing%2520theoretical%2520understanding%2520of%2520neural%2520networks%2520that%250Aemploy%2520the%2520Rectified%2520Linear%2520Unit%2520%2528ReLU%2529%2520as%2520their%2520activation%2520function%252C%2520we%250Arevisit%2520the%2520use%2520of%2520ReLU%2520activation%2520functions%2520for%2520learning%2520implicit%2520neural%250Arepresentations%2520%2528INRs%2529.%2520Inspired%2520by%2520second%2520order%2520B-spline%2520wavelets%252C%2520we%250Aincorporate%2520a%2520set%2520of%2520simple%2520constraints%2520to%2520the%2520ReLU%2520neurons%2520in%2520each%2520layer%2520of%2520a%250Adeep%2520neural%2520network%2520%2528DNN%2529%2520to%2520remedy%2520the%2520spectral%2520bias.%2520This%2520in%2520turn%2520enables%2520its%250Ause%2520for%2520various%2520INR%2520tasks.%2520Empirically%252C%2520we%2520demonstrate%2520that%252C%2520contrary%2520to%250Apopular%2520belief%252C%2520one%2520can%2520learn%2520state-of-the-art%2520INRs%2520based%2520on%2520a%2520DNN%2520composed%2520of%250Aonly%2520ReLU%2520neurons.%2520Next%252C%2520by%2520leveraging%2520recent%2520theoretical%2520works%2520which%250Acharacterize%2520the%2520kinds%2520of%2520functions%2520ReLU%2520neural%2520networks%2520learn%252C%2520we%2520provide%2520a%250Away%2520to%2520quantify%2520the%2520regularity%2520of%2520the%2520learned%2520function.%2520This%2520offers%2520a%250Aprincipled%2520approach%2520to%2520selecting%2520the%2520hyperparameters%2520in%2520INR%2520architectures.%2520We%250Asubstantiate%2520our%2520claims%2520through%2520experiments%2520in%2520signal%2520representation%252C%2520super%250Aresolution%252C%2520and%2520computed%2520tomography%252C%2520demonstrating%2520the%2520versatility%2520and%250Aeffectiveness%2520of%2520our%2520method.%2520The%2520code%2520for%2520all%2520experiments%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/joeshenouda/relu-inrs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReLUs%20Are%20Sufficient%20for%20Learning%20Implicit%20Neural%20Representations&entry.906535625=Joseph%20Shenouda%20and%20Yamin%20Zhou%20and%20Robert%20D.%20Nowak&entry.1292438233=%20%20Motivated%20by%20the%20growing%20theoretical%20understanding%20of%20neural%20networks%20that%0Aemploy%20the%20Rectified%20Linear%20Unit%20%28ReLU%29%20as%20their%20activation%20function%2C%20we%0Arevisit%20the%20use%20of%20ReLU%20activation%20functions%20for%20learning%20implicit%20neural%0Arepresentations%20%28INRs%29.%20Inspired%20by%20second%20order%20B-spline%20wavelets%2C%20we%0Aincorporate%20a%20set%20of%20simple%20constraints%20to%20the%20ReLU%20neurons%20in%20each%20layer%20of%20a%0Adeep%20neural%20network%20%28DNN%29%20to%20remedy%20the%20spectral%20bias.%20This%20in%20turn%20enables%20its%0Ause%20for%20various%20INR%20tasks.%20Empirically%2C%20we%20demonstrate%20that%2C%20contrary%20to%0Apopular%20belief%2C%20one%20can%20learn%20state-of-the-art%20INRs%20based%20on%20a%20DNN%20composed%20of%0Aonly%20ReLU%20neurons.%20Next%2C%20by%20leveraging%20recent%20theoretical%20works%20which%0Acharacterize%20the%20kinds%20of%20functions%20ReLU%20neural%20networks%20learn%2C%20we%20provide%20a%0Away%20to%20quantify%20the%20regularity%20of%20the%20learned%20function.%20This%20offers%20a%0Aprincipled%20approach%20to%20selecting%20the%20hyperparameters%20in%20INR%20architectures.%20We%0Asubstantiate%20our%20claims%20through%20experiments%20in%20signal%20representation%2C%20super%0Aresolution%2C%20and%20computed%20tomography%2C%20demonstrating%20the%20versatility%20and%0Aeffectiveness%20of%20our%20method.%20The%20code%20for%20all%20experiments%20can%20be%20found%20at%0Ahttps%3A//github.com/joeshenouda/relu-inrs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02529v1&entry.124074799=Read"},
{"title": "Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture\n  of Adapters", "author": "Umberto Cappellazzo and Daniele Falavigna and Alessio Brutti", "abstract": "  Mixture of Experts (MoE) architectures have recently started burgeoning due\nto their ability to scale model's capacity while maintaining the computational\ncost affordable. Furthermore, they can be applied to both Transformers and\nState Space Models, the current state-of-the-art models in numerous fields.\nWhile MoE has been mostly investigated for the pre-training stage, its use in\nparameter-efficient transfer learning settings is under-explored. To narrow\nthis gap, this paper attempts to demystify the use of MoE for\nparameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and\nspeech downstream tasks. Specifically, we propose Soft Mixture of Adapters\n(Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft\nMoE method, it relies on a soft assignment between the input tokens and experts\nto keep the computational time limited. Extensive experiments across 4\nbenchmarks demonstrate that Soft-MoA outperforms the single adapter method and\nperforms on par with the dense MoA counterpart. We finally present ablation\nstudies on key elements of Soft-MoA, showing for example that Soft-MoA achieves\nbetter scaling with more experts, as well as ensuring that all experts\ncontribute to the computation of the output tokens, thus dispensing with the\nexpert imbalance issue.\n", "link": "http://arxiv.org/abs/2402.00828v2", "date": "2024-06-04", "relevancy": 2.0594, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5054}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Fine-tuning%20of%20Audio%20Spectrogram%20Transformers%20via%20Soft%20Mixture%0A%20%20of%20Adapters&body=Title%3A%20Efficient%20Fine-tuning%20of%20Audio%20Spectrogram%20Transformers%20via%20Soft%20Mixture%0A%20%20of%20Adapters%0AAuthor%3A%20Umberto%20Cappellazzo%20and%20Daniele%20Falavigna%20and%20Alessio%20Brutti%0AAbstract%3A%20%20%20Mixture%20of%20Experts%20%28MoE%29%20architectures%20have%20recently%20started%20burgeoning%20due%0Ato%20their%20ability%20to%20scale%20model%27s%20capacity%20while%20maintaining%20the%20computational%0Acost%20affordable.%20Furthermore%2C%20they%20can%20be%20applied%20to%20both%20Transformers%20and%0AState%20Space%20Models%2C%20the%20current%20state-of-the-art%20models%20in%20numerous%20fields.%0AWhile%20MoE%20has%20been%20mostly%20investigated%20for%20the%20pre-training%20stage%2C%20its%20use%20in%0Aparameter-efficient%20transfer%20learning%20settings%20is%20under-explored.%20To%20narrow%0Athis%20gap%2C%20this%20paper%20attempts%20to%20demystify%20the%20use%20of%20MoE%20for%0Aparameter-efficient%20fine-tuning%20of%20Audio%20Spectrogram%20Transformers%20to%20audio%20and%0Aspeech%20downstream%20tasks.%20Specifically%2C%20we%20propose%20Soft%20Mixture%20of%20Adapters%0A%28Soft-MoA%29.%20It%20exploits%20adapters%20as%20the%20experts%20and%2C%20leveraging%20the%20recent%20Soft%0AMoE%20method%2C%20it%20relies%20on%20a%20soft%20assignment%20between%20the%20input%20tokens%20and%20experts%0Ato%20keep%20the%20computational%20time%20limited.%20Extensive%20experiments%20across%204%0Abenchmarks%20demonstrate%20that%20Soft-MoA%20outperforms%20the%20single%20adapter%20method%20and%0Aperforms%20on%20par%20with%20the%20dense%20MoA%20counterpart.%20We%20finally%20present%20ablation%0Astudies%20on%20key%20elements%20of%20Soft-MoA%2C%20showing%20for%20example%20that%20Soft-MoA%20achieves%0Abetter%20scaling%20with%20more%20experts%2C%20as%20well%20as%20ensuring%20that%20all%20experts%0Acontribute%20to%20the%20computation%20of%20the%20output%20tokens%2C%20thus%20dispensing%20with%20the%0Aexpert%20imbalance%20issue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Fine-tuning%2520of%2520Audio%2520Spectrogram%2520Transformers%2520via%2520Soft%2520Mixture%250A%2520%2520of%2520Adapters%26entry.906535625%3DUmberto%2520Cappellazzo%2520and%2520Daniele%2520Falavigna%2520and%2520Alessio%2520Brutti%26entry.1292438233%3D%2520%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520architectures%2520have%2520recently%2520started%2520burgeoning%2520due%250Ato%2520their%2520ability%2520to%2520scale%2520model%2527s%2520capacity%2520while%2520maintaining%2520the%2520computational%250Acost%2520affordable.%2520Furthermore%252C%2520they%2520can%2520be%2520applied%2520to%2520both%2520Transformers%2520and%250AState%2520Space%2520Models%252C%2520the%2520current%2520state-of-the-art%2520models%2520in%2520numerous%2520fields.%250AWhile%2520MoE%2520has%2520been%2520mostly%2520investigated%2520for%2520the%2520pre-training%2520stage%252C%2520its%2520use%2520in%250Aparameter-efficient%2520transfer%2520learning%2520settings%2520is%2520under-explored.%2520To%2520narrow%250Athis%2520gap%252C%2520this%2520paper%2520attempts%2520to%2520demystify%2520the%2520use%2520of%2520MoE%2520for%250Aparameter-efficient%2520fine-tuning%2520of%2520Audio%2520Spectrogram%2520Transformers%2520to%2520audio%2520and%250Aspeech%2520downstream%2520tasks.%2520Specifically%252C%2520we%2520propose%2520Soft%2520Mixture%2520of%2520Adapters%250A%2528Soft-MoA%2529.%2520It%2520exploits%2520adapters%2520as%2520the%2520experts%2520and%252C%2520leveraging%2520the%2520recent%2520Soft%250AMoE%2520method%252C%2520it%2520relies%2520on%2520a%2520soft%2520assignment%2520between%2520the%2520input%2520tokens%2520and%2520experts%250Ato%2520keep%2520the%2520computational%2520time%2520limited.%2520Extensive%2520experiments%2520across%25204%250Abenchmarks%2520demonstrate%2520that%2520Soft-MoA%2520outperforms%2520the%2520single%2520adapter%2520method%2520and%250Aperforms%2520on%2520par%2520with%2520the%2520dense%2520MoA%2520counterpart.%2520We%2520finally%2520present%2520ablation%250Astudies%2520on%2520key%2520elements%2520of%2520Soft-MoA%252C%2520showing%2520for%2520example%2520that%2520Soft-MoA%2520achieves%250Abetter%2520scaling%2520with%2520more%2520experts%252C%2520as%2520well%2520as%2520ensuring%2520that%2520all%2520experts%250Acontribute%2520to%2520the%2520computation%2520of%2520the%2520output%2520tokens%252C%2520thus%2520dispensing%2520with%2520the%250Aexpert%2520imbalance%2520issue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Fine-tuning%20of%20Audio%20Spectrogram%20Transformers%20via%20Soft%20Mixture%0A%20%20of%20Adapters&entry.906535625=Umberto%20Cappellazzo%20and%20Daniele%20Falavigna%20and%20Alessio%20Brutti&entry.1292438233=%20%20Mixture%20of%20Experts%20%28MoE%29%20architectures%20have%20recently%20started%20burgeoning%20due%0Ato%20their%20ability%20to%20scale%20model%27s%20capacity%20while%20maintaining%20the%20computational%0Acost%20affordable.%20Furthermore%2C%20they%20can%20be%20applied%20to%20both%20Transformers%20and%0AState%20Space%20Models%2C%20the%20current%20state-of-the-art%20models%20in%20numerous%20fields.%0AWhile%20MoE%20has%20been%20mostly%20investigated%20for%20the%20pre-training%20stage%2C%20its%20use%20in%0Aparameter-efficient%20transfer%20learning%20settings%20is%20under-explored.%20To%20narrow%0Athis%20gap%2C%20this%20paper%20attempts%20to%20demystify%20the%20use%20of%20MoE%20for%0Aparameter-efficient%20fine-tuning%20of%20Audio%20Spectrogram%20Transformers%20to%20audio%20and%0Aspeech%20downstream%20tasks.%20Specifically%2C%20we%20propose%20Soft%20Mixture%20of%20Adapters%0A%28Soft-MoA%29.%20It%20exploits%20adapters%20as%20the%20experts%20and%2C%20leveraging%20the%20recent%20Soft%0AMoE%20method%2C%20it%20relies%20on%20a%20soft%20assignment%20between%20the%20input%20tokens%20and%20experts%0Ato%20keep%20the%20computational%20time%20limited.%20Extensive%20experiments%20across%204%0Abenchmarks%20demonstrate%20that%20Soft-MoA%20outperforms%20the%20single%20adapter%20method%20and%0Aperforms%20on%20par%20with%20the%20dense%20MoA%20counterpart.%20We%20finally%20present%20ablation%0Astudies%20on%20key%20elements%20of%20Soft-MoA%2C%20showing%20for%20example%20that%20Soft-MoA%20achieves%0Abetter%20scaling%20with%20more%20experts%2C%20as%20well%20as%20ensuring%20that%20all%20experts%0Acontribute%20to%20the%20computation%20of%20the%20output%20tokens%2C%20thus%20dispensing%20with%20the%0Aexpert%20imbalance%20issue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00828v2&entry.124074799=Read"},
{"title": "Dropout MPC: An Ensemble Neural MPC Approach for Systems with Learned\n  Dynamics", "author": "Spyridon Syntakas and Kostas Vlachos", "abstract": "  Neural networks are lately more and more often being used in the context of\ndata-driven control, as an approximate model of the true system dynamics. Model\nPredictive Control (MPC) adopts this practise leading to neural MPC strategies.\nThis raises a question of whether the trained neural network has converged and\ngeneralized in a way that the learned model encapsulates an accurate\napproximation of the true dynamic model of the system, thus making it a\nreliable choice for model-based control, especially for disturbed and uncertain\nsystems. To tackle that, we propose Dropout MPC, a novel sampling-based\nensemble neural MPC algorithm that employs the Monte-Carlo dropout technique on\nthe learned system model. The closed loop is based on an ensemble of predictive\ncontrollers, that are used simultaneously at each time-step for trajectory\noptimization. Each member of the ensemble influences the control input, based\non a weighted voting scheme, thus by employing different realizations of the\nlearned system dynamics, neural control becomes more reliable by design. An\nadditional strength of the method is that it offers by design a way to estimate\nfuture uncertainty, leading to cautious control. While the method aims in\ngeneral at uncertain systems with complex dynamics, where models derived from\nfirst principles are hard to infer, to showcase the application we utilize data\ngathered in the laboratory from a real mobile manipulator and employ the\nproposed algorithm for the navigation of the robot in simulation.\n", "link": "http://arxiv.org/abs/2406.02497v1", "date": "2024-06-04", "relevancy": 2.0538, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5255}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dropout%20MPC%3A%20An%20Ensemble%20Neural%20MPC%20Approach%20for%20Systems%20with%20Learned%0A%20%20Dynamics&body=Title%3A%20Dropout%20MPC%3A%20An%20Ensemble%20Neural%20MPC%20Approach%20for%20Systems%20with%20Learned%0A%20%20Dynamics%0AAuthor%3A%20Spyridon%20Syntakas%20and%20Kostas%20Vlachos%0AAbstract%3A%20%20%20Neural%20networks%20are%20lately%20more%20and%20more%20often%20being%20used%20in%20the%20context%20of%0Adata-driven%20control%2C%20as%20an%20approximate%20model%20of%20the%20true%20system%20dynamics.%20Model%0APredictive%20Control%20%28MPC%29%20adopts%20this%20practise%20leading%20to%20neural%20MPC%20strategies.%0AThis%20raises%20a%20question%20of%20whether%20the%20trained%20neural%20network%20has%20converged%20and%0Ageneralized%20in%20a%20way%20that%20the%20learned%20model%20encapsulates%20an%20accurate%0Aapproximation%20of%20the%20true%20dynamic%20model%20of%20the%20system%2C%20thus%20making%20it%20a%0Areliable%20choice%20for%20model-based%20control%2C%20especially%20for%20disturbed%20and%20uncertain%0Asystems.%20To%20tackle%20that%2C%20we%20propose%20Dropout%20MPC%2C%20a%20novel%20sampling-based%0Aensemble%20neural%20MPC%20algorithm%20that%20employs%20the%20Monte-Carlo%20dropout%20technique%20on%0Athe%20learned%20system%20model.%20The%20closed%20loop%20is%20based%20on%20an%20ensemble%20of%20predictive%0Acontrollers%2C%20that%20are%20used%20simultaneously%20at%20each%20time-step%20for%20trajectory%0Aoptimization.%20Each%20member%20of%20the%20ensemble%20influences%20the%20control%20input%2C%20based%0Aon%20a%20weighted%20voting%20scheme%2C%20thus%20by%20employing%20different%20realizations%20of%20the%0Alearned%20system%20dynamics%2C%20neural%20control%20becomes%20more%20reliable%20by%20design.%20An%0Aadditional%20strength%20of%20the%20method%20is%20that%20it%20offers%20by%20design%20a%20way%20to%20estimate%0Afuture%20uncertainty%2C%20leading%20to%20cautious%20control.%20While%20the%20method%20aims%20in%0Ageneral%20at%20uncertain%20systems%20with%20complex%20dynamics%2C%20where%20models%20derived%20from%0Afirst%20principles%20are%20hard%20to%20infer%2C%20to%20showcase%20the%20application%20we%20utilize%20data%0Agathered%20in%20the%20laboratory%20from%20a%20real%20mobile%20manipulator%20and%20employ%20the%0Aproposed%20algorithm%20for%20the%20navigation%20of%20the%20robot%20in%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDropout%2520MPC%253A%2520An%2520Ensemble%2520Neural%2520MPC%2520Approach%2520for%2520Systems%2520with%2520Learned%250A%2520%2520Dynamics%26entry.906535625%3DSpyridon%2520Syntakas%2520and%2520Kostas%2520Vlachos%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520lately%2520more%2520and%2520more%2520often%2520being%2520used%2520in%2520the%2520context%2520of%250Adata-driven%2520control%252C%2520as%2520an%2520approximate%2520model%2520of%2520the%2520true%2520system%2520dynamics.%2520Model%250APredictive%2520Control%2520%2528MPC%2529%2520adopts%2520this%2520practise%2520leading%2520to%2520neural%2520MPC%2520strategies.%250AThis%2520raises%2520a%2520question%2520of%2520whether%2520the%2520trained%2520neural%2520network%2520has%2520converged%2520and%250Ageneralized%2520in%2520a%2520way%2520that%2520the%2520learned%2520model%2520encapsulates%2520an%2520accurate%250Aapproximation%2520of%2520the%2520true%2520dynamic%2520model%2520of%2520the%2520system%252C%2520thus%2520making%2520it%2520a%250Areliable%2520choice%2520for%2520model-based%2520control%252C%2520especially%2520for%2520disturbed%2520and%2520uncertain%250Asystems.%2520To%2520tackle%2520that%252C%2520we%2520propose%2520Dropout%2520MPC%252C%2520a%2520novel%2520sampling-based%250Aensemble%2520neural%2520MPC%2520algorithm%2520that%2520employs%2520the%2520Monte-Carlo%2520dropout%2520technique%2520on%250Athe%2520learned%2520system%2520model.%2520The%2520closed%2520loop%2520is%2520based%2520on%2520an%2520ensemble%2520of%2520predictive%250Acontrollers%252C%2520that%2520are%2520used%2520simultaneously%2520at%2520each%2520time-step%2520for%2520trajectory%250Aoptimization.%2520Each%2520member%2520of%2520the%2520ensemble%2520influences%2520the%2520control%2520input%252C%2520based%250Aon%2520a%2520weighted%2520voting%2520scheme%252C%2520thus%2520by%2520employing%2520different%2520realizations%2520of%2520the%250Alearned%2520system%2520dynamics%252C%2520neural%2520control%2520becomes%2520more%2520reliable%2520by%2520design.%2520An%250Aadditional%2520strength%2520of%2520the%2520method%2520is%2520that%2520it%2520offers%2520by%2520design%2520a%2520way%2520to%2520estimate%250Afuture%2520uncertainty%252C%2520leading%2520to%2520cautious%2520control.%2520While%2520the%2520method%2520aims%2520in%250Ageneral%2520at%2520uncertain%2520systems%2520with%2520complex%2520dynamics%252C%2520where%2520models%2520derived%2520from%250Afirst%2520principles%2520are%2520hard%2520to%2520infer%252C%2520to%2520showcase%2520the%2520application%2520we%2520utilize%2520data%250Agathered%2520in%2520the%2520laboratory%2520from%2520a%2520real%2520mobile%2520manipulator%2520and%2520employ%2520the%250Aproposed%2520algorithm%2520for%2520the%2520navigation%2520of%2520the%2520robot%2520in%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dropout%20MPC%3A%20An%20Ensemble%20Neural%20MPC%20Approach%20for%20Systems%20with%20Learned%0A%20%20Dynamics&entry.906535625=Spyridon%20Syntakas%20and%20Kostas%20Vlachos&entry.1292438233=%20%20Neural%20networks%20are%20lately%20more%20and%20more%20often%20being%20used%20in%20the%20context%20of%0Adata-driven%20control%2C%20as%20an%20approximate%20model%20of%20the%20true%20system%20dynamics.%20Model%0APredictive%20Control%20%28MPC%29%20adopts%20this%20practise%20leading%20to%20neural%20MPC%20strategies.%0AThis%20raises%20a%20question%20of%20whether%20the%20trained%20neural%20network%20has%20converged%20and%0Ageneralized%20in%20a%20way%20that%20the%20learned%20model%20encapsulates%20an%20accurate%0Aapproximation%20of%20the%20true%20dynamic%20model%20of%20the%20system%2C%20thus%20making%20it%20a%0Areliable%20choice%20for%20model-based%20control%2C%20especially%20for%20disturbed%20and%20uncertain%0Asystems.%20To%20tackle%20that%2C%20we%20propose%20Dropout%20MPC%2C%20a%20novel%20sampling-based%0Aensemble%20neural%20MPC%20algorithm%20that%20employs%20the%20Monte-Carlo%20dropout%20technique%20on%0Athe%20learned%20system%20model.%20The%20closed%20loop%20is%20based%20on%20an%20ensemble%20of%20predictive%0Acontrollers%2C%20that%20are%20used%20simultaneously%20at%20each%20time-step%20for%20trajectory%0Aoptimization.%20Each%20member%20of%20the%20ensemble%20influences%20the%20control%20input%2C%20based%0Aon%20a%20weighted%20voting%20scheme%2C%20thus%20by%20employing%20different%20realizations%20of%20the%0Alearned%20system%20dynamics%2C%20neural%20control%20becomes%20more%20reliable%20by%20design.%20An%0Aadditional%20strength%20of%20the%20method%20is%20that%20it%20offers%20by%20design%20a%20way%20to%20estimate%0Afuture%20uncertainty%2C%20leading%20to%20cautious%20control.%20While%20the%20method%20aims%20in%0Ageneral%20at%20uncertain%20systems%20with%20complex%20dynamics%2C%20where%20models%20derived%20from%0Afirst%20principles%20are%20hard%20to%20infer%2C%20to%20showcase%20the%20application%20we%20utilize%20data%0Agathered%20in%20the%20laboratory%20from%20a%20real%20mobile%20manipulator%20and%20employ%20the%0Aproposed%20algorithm%20for%20the%20navigation%20of%20the%20robot%20in%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02497v1&entry.124074799=Read"},
{"title": "PeFAD: A Parameter-Efficient Federated Framework for Time Series Anomaly\n  Detection", "author": "Ronghui Xu and Hao Miao and Senzhang Wang and Philip S. Yu and Jianxin Wang", "abstract": "  With the proliferation of mobile sensing techniques, huge amounts of time\nseries data are generated and accumulated in various domains, fueling plenty of\nreal-world applications. In this setting, time series anomaly detection is\npractically important. It endeavors to identify deviant samples from the normal\nsample distribution in time series. Existing approaches generally assume that\nall the time series is available at a central location. However, we are\nwitnessing the decentralized collection of time series due to the deployment of\nvarious edge devices. To bridge the gap between the decentralized time series\ndata and the centralized anomaly detection algorithms, we propose a\nParameter-efficient Federated Anomaly Detection framework named PeFAD with the\nincreasing privacy concerns. PeFAD for the first time employs the pre-trained\nlanguage model (PLM) as the body of the client's local model, which can benefit\nfrom its cross-modality knowledge transfer capability. To reduce the\ncommunication overhead and local model adaptation cost, we propose a\nparameter-efficient federated training module such that clients only need to\nfine-tune small-scale parameters and transmit them to the server for update.\nPeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate the\nimpact of neglected anomalies during training. A knowledge distillation\noperation on a synthetic privacy-preserving dataset that is shared by all the\nclients is also proposed to address the data heterogeneity issue across\nclients. We conduct extensive evaluations on four real datasets, where PeFAD\noutperforms existing state-of-the-art baselines by up to 28.74\\%.\n", "link": "http://arxiv.org/abs/2406.02318v1", "date": "2024-06-04", "relevancy": 2.0514, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5153}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5153}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PeFAD%3A%20A%20Parameter-Efficient%20Federated%20Framework%20for%20Time%20Series%20Anomaly%0A%20%20Detection&body=Title%3A%20PeFAD%3A%20A%20Parameter-Efficient%20Federated%20Framework%20for%20Time%20Series%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Ronghui%20Xu%20and%20Hao%20Miao%20and%20Senzhang%20Wang%20and%20Philip%20S.%20Yu%20and%20Jianxin%20Wang%0AAbstract%3A%20%20%20With%20the%20proliferation%20of%20mobile%20sensing%20techniques%2C%20huge%20amounts%20of%20time%0Aseries%20data%20are%20generated%20and%20accumulated%20in%20various%20domains%2C%20fueling%20plenty%20of%0Areal-world%20applications.%20In%20this%20setting%2C%20time%20series%20anomaly%20detection%20is%0Apractically%20important.%20It%20endeavors%20to%20identify%20deviant%20samples%20from%20the%20normal%0Asample%20distribution%20in%20time%20series.%20Existing%20approaches%20generally%20assume%20that%0Aall%20the%20time%20series%20is%20available%20at%20a%20central%20location.%20However%2C%20we%20are%0Awitnessing%20the%20decentralized%20collection%20of%20time%20series%20due%20to%20the%20deployment%20of%0Avarious%20edge%20devices.%20To%20bridge%20the%20gap%20between%20the%20decentralized%20time%20series%0Adata%20and%20the%20centralized%20anomaly%20detection%20algorithms%2C%20we%20propose%20a%0AParameter-efficient%20Federated%20Anomaly%20Detection%20framework%20named%20PeFAD%20with%20the%0Aincreasing%20privacy%20concerns.%20PeFAD%20for%20the%20first%20time%20employs%20the%20pre-trained%0Alanguage%20model%20%28PLM%29%20as%20the%20body%20of%20the%20client%27s%20local%20model%2C%20which%20can%20benefit%0Afrom%20its%20cross-modality%20knowledge%20transfer%20capability.%20To%20reduce%20the%0Acommunication%20overhead%20and%20local%20model%20adaptation%20cost%2C%20we%20propose%20a%0Aparameter-efficient%20federated%20training%20module%20such%20that%20clients%20only%20need%20to%0Afine-tune%20small-scale%20parameters%20and%20transmit%20them%20to%20the%20server%20for%20update.%0APeFAD%20utilizes%20a%20novel%20anomaly-driven%20mask%20selection%20strategy%20to%20mitigate%20the%0Aimpact%20of%20neglected%20anomalies%20during%20training.%20A%20knowledge%20distillation%0Aoperation%20on%20a%20synthetic%20privacy-preserving%20dataset%20that%20is%20shared%20by%20all%20the%0Aclients%20is%20also%20proposed%20to%20address%20the%20data%20heterogeneity%20issue%20across%0Aclients.%20We%20conduct%20extensive%20evaluations%20on%20four%20real%20datasets%2C%20where%20PeFAD%0Aoutperforms%20existing%20state-of-the-art%20baselines%20by%20up%20to%2028.74%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPeFAD%253A%2520A%2520Parameter-Efficient%2520Federated%2520Framework%2520for%2520Time%2520Series%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DRonghui%2520Xu%2520and%2520Hao%2520Miao%2520and%2520Senzhang%2520Wang%2520and%2520Philip%2520S.%2520Yu%2520and%2520Jianxin%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520proliferation%2520of%2520mobile%2520sensing%2520techniques%252C%2520huge%2520amounts%2520of%2520time%250Aseries%2520data%2520are%2520generated%2520and%2520accumulated%2520in%2520various%2520domains%252C%2520fueling%2520plenty%2520of%250Areal-world%2520applications.%2520In%2520this%2520setting%252C%2520time%2520series%2520anomaly%2520detection%2520is%250Apractically%2520important.%2520It%2520endeavors%2520to%2520identify%2520deviant%2520samples%2520from%2520the%2520normal%250Asample%2520distribution%2520in%2520time%2520series.%2520Existing%2520approaches%2520generally%2520assume%2520that%250Aall%2520the%2520time%2520series%2520is%2520available%2520at%2520a%2520central%2520location.%2520However%252C%2520we%2520are%250Awitnessing%2520the%2520decentralized%2520collection%2520of%2520time%2520series%2520due%2520to%2520the%2520deployment%2520of%250Avarious%2520edge%2520devices.%2520To%2520bridge%2520the%2520gap%2520between%2520the%2520decentralized%2520time%2520series%250Adata%2520and%2520the%2520centralized%2520anomaly%2520detection%2520algorithms%252C%2520we%2520propose%2520a%250AParameter-efficient%2520Federated%2520Anomaly%2520Detection%2520framework%2520named%2520PeFAD%2520with%2520the%250Aincreasing%2520privacy%2520concerns.%2520PeFAD%2520for%2520the%2520first%2520time%2520employs%2520the%2520pre-trained%250Alanguage%2520model%2520%2528PLM%2529%2520as%2520the%2520body%2520of%2520the%2520client%2527s%2520local%2520model%252C%2520which%2520can%2520benefit%250Afrom%2520its%2520cross-modality%2520knowledge%2520transfer%2520capability.%2520To%2520reduce%2520the%250Acommunication%2520overhead%2520and%2520local%2520model%2520adaptation%2520cost%252C%2520we%2520propose%2520a%250Aparameter-efficient%2520federated%2520training%2520module%2520such%2520that%2520clients%2520only%2520need%2520to%250Afine-tune%2520small-scale%2520parameters%2520and%2520transmit%2520them%2520to%2520the%2520server%2520for%2520update.%250APeFAD%2520utilizes%2520a%2520novel%2520anomaly-driven%2520mask%2520selection%2520strategy%2520to%2520mitigate%2520the%250Aimpact%2520of%2520neglected%2520anomalies%2520during%2520training.%2520A%2520knowledge%2520distillation%250Aoperation%2520on%2520a%2520synthetic%2520privacy-preserving%2520dataset%2520that%2520is%2520shared%2520by%2520all%2520the%250Aclients%2520is%2520also%2520proposed%2520to%2520address%2520the%2520data%2520heterogeneity%2520issue%2520across%250Aclients.%2520We%2520conduct%2520extensive%2520evaluations%2520on%2520four%2520real%2520datasets%252C%2520where%2520PeFAD%250Aoutperforms%2520existing%2520state-of-the-art%2520baselines%2520by%2520up%2520to%252028.74%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PeFAD%3A%20A%20Parameter-Efficient%20Federated%20Framework%20for%20Time%20Series%20Anomaly%0A%20%20Detection&entry.906535625=Ronghui%20Xu%20and%20Hao%20Miao%20and%20Senzhang%20Wang%20and%20Philip%20S.%20Yu%20and%20Jianxin%20Wang&entry.1292438233=%20%20With%20the%20proliferation%20of%20mobile%20sensing%20techniques%2C%20huge%20amounts%20of%20time%0Aseries%20data%20are%20generated%20and%20accumulated%20in%20various%20domains%2C%20fueling%20plenty%20of%0Areal-world%20applications.%20In%20this%20setting%2C%20time%20series%20anomaly%20detection%20is%0Apractically%20important.%20It%20endeavors%20to%20identify%20deviant%20samples%20from%20the%20normal%0Asample%20distribution%20in%20time%20series.%20Existing%20approaches%20generally%20assume%20that%0Aall%20the%20time%20series%20is%20available%20at%20a%20central%20location.%20However%2C%20we%20are%0Awitnessing%20the%20decentralized%20collection%20of%20time%20series%20due%20to%20the%20deployment%20of%0Avarious%20edge%20devices.%20To%20bridge%20the%20gap%20between%20the%20decentralized%20time%20series%0Adata%20and%20the%20centralized%20anomaly%20detection%20algorithms%2C%20we%20propose%20a%0AParameter-efficient%20Federated%20Anomaly%20Detection%20framework%20named%20PeFAD%20with%20the%0Aincreasing%20privacy%20concerns.%20PeFAD%20for%20the%20first%20time%20employs%20the%20pre-trained%0Alanguage%20model%20%28PLM%29%20as%20the%20body%20of%20the%20client%27s%20local%20model%2C%20which%20can%20benefit%0Afrom%20its%20cross-modality%20knowledge%20transfer%20capability.%20To%20reduce%20the%0Acommunication%20overhead%20and%20local%20model%20adaptation%20cost%2C%20we%20propose%20a%0Aparameter-efficient%20federated%20training%20module%20such%20that%20clients%20only%20need%20to%0Afine-tune%20small-scale%20parameters%20and%20transmit%20them%20to%20the%20server%20for%20update.%0APeFAD%20utilizes%20a%20novel%20anomaly-driven%20mask%20selection%20strategy%20to%20mitigate%20the%0Aimpact%20of%20neglected%20anomalies%20during%20training.%20A%20knowledge%20distillation%0Aoperation%20on%20a%20synthetic%20privacy-preserving%20dataset%20that%20is%20shared%20by%20all%20the%0Aclients%20is%20also%20proposed%20to%20address%20the%20data%20heterogeneity%20issue%20across%0Aclients.%20We%20conduct%20extensive%20evaluations%20on%20four%20real%20datasets%2C%20where%20PeFAD%0Aoutperforms%20existing%20state-of-the-art%20baselines%20by%20up%20to%2028.74%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02318v1&entry.124074799=Read"},
{"title": "Ai-Sampler: Adversarial Learning of Markov kernels with involutive maps", "author": "Evgenii Egorov and Ricardo Valperga and Efstratios Gavves", "abstract": "  Markov chain Monte Carlo methods have become popular in statistics as\nversatile techniques to sample from complicated probability distributions. In\nthis work, we propose a method to parameterize and train transition kernels of\nMarkov chains to achieve efficient sampling and good mixing. This training\nprocedure minimizes the total variation distance between the stationary\ndistribution of the chain and the empirical distribution of the data. Our\napproach leverages involutive Metropolis-Hastings kernels constructed from\nreversible neural networks that ensure detailed balance by construction. We\nfind that reversibility also implies $C_2$-equivariance of the discriminator\nfunction which can be used to restrict its function space.\n", "link": "http://arxiv.org/abs/2406.02490v1", "date": "2024-06-04", "relevancy": 2.0411, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ai-Sampler%3A%20Adversarial%20Learning%20of%20Markov%20kernels%20with%20involutive%20maps&body=Title%3A%20Ai-Sampler%3A%20Adversarial%20Learning%20of%20Markov%20kernels%20with%20involutive%20maps%0AAuthor%3A%20Evgenii%20Egorov%20and%20Ricardo%20Valperga%20and%20Efstratios%20Gavves%0AAbstract%3A%20%20%20Markov%20chain%20Monte%20Carlo%20methods%20have%20become%20popular%20in%20statistics%20as%0Aversatile%20techniques%20to%20sample%20from%20complicated%20probability%20distributions.%20In%0Athis%20work%2C%20we%20propose%20a%20method%20to%20parameterize%20and%20train%20transition%20kernels%20of%0AMarkov%20chains%20to%20achieve%20efficient%20sampling%20and%20good%20mixing.%20This%20training%0Aprocedure%20minimizes%20the%20total%20variation%20distance%20between%20the%20stationary%0Adistribution%20of%20the%20chain%20and%20the%20empirical%20distribution%20of%20the%20data.%20Our%0Aapproach%20leverages%20involutive%20Metropolis-Hastings%20kernels%20constructed%20from%0Areversible%20neural%20networks%20that%20ensure%20detailed%20balance%20by%20construction.%20We%0Afind%20that%20reversibility%20also%20implies%20%24C_2%24-equivariance%20of%20the%20discriminator%0Afunction%20which%20can%20be%20used%20to%20restrict%20its%20function%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAi-Sampler%253A%2520Adversarial%2520Learning%2520of%2520Markov%2520kernels%2520with%2520involutive%2520maps%26entry.906535625%3DEvgenii%2520Egorov%2520and%2520Ricardo%2520Valperga%2520and%2520Efstratios%2520Gavves%26entry.1292438233%3D%2520%2520Markov%2520chain%2520Monte%2520Carlo%2520methods%2520have%2520become%2520popular%2520in%2520statistics%2520as%250Aversatile%2520techniques%2520to%2520sample%2520from%2520complicated%2520probability%2520distributions.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520method%2520to%2520parameterize%2520and%2520train%2520transition%2520kernels%2520of%250AMarkov%2520chains%2520to%2520achieve%2520efficient%2520sampling%2520and%2520good%2520mixing.%2520This%2520training%250Aprocedure%2520minimizes%2520the%2520total%2520variation%2520distance%2520between%2520the%2520stationary%250Adistribution%2520of%2520the%2520chain%2520and%2520the%2520empirical%2520distribution%2520of%2520the%2520data.%2520Our%250Aapproach%2520leverages%2520involutive%2520Metropolis-Hastings%2520kernels%2520constructed%2520from%250Areversible%2520neural%2520networks%2520that%2520ensure%2520detailed%2520balance%2520by%2520construction.%2520We%250Afind%2520that%2520reversibility%2520also%2520implies%2520%2524C_2%2524-equivariance%2520of%2520the%2520discriminator%250Afunction%2520which%2520can%2520be%2520used%2520to%2520restrict%2520its%2520function%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ai-Sampler%3A%20Adversarial%20Learning%20of%20Markov%20kernels%20with%20involutive%20maps&entry.906535625=Evgenii%20Egorov%20and%20Ricardo%20Valperga%20and%20Efstratios%20Gavves&entry.1292438233=%20%20Markov%20chain%20Monte%20Carlo%20methods%20have%20become%20popular%20in%20statistics%20as%0Aversatile%20techniques%20to%20sample%20from%20complicated%20probability%20distributions.%20In%0Athis%20work%2C%20we%20propose%20a%20method%20to%20parameterize%20and%20train%20transition%20kernels%20of%0AMarkov%20chains%20to%20achieve%20efficient%20sampling%20and%20good%20mixing.%20This%20training%0Aprocedure%20minimizes%20the%20total%20variation%20distance%20between%20the%20stationary%0Adistribution%20of%20the%20chain%20and%20the%20empirical%20distribution%20of%20the%20data.%20Our%0Aapproach%20leverages%20involutive%20Metropolis-Hastings%20kernels%20constructed%20from%0Areversible%20neural%20networks%20that%20ensure%20detailed%20balance%20by%20construction.%20We%0Afind%20that%20reversibility%20also%20implies%20%24C_2%24-equivariance%20of%20the%20discriminator%0Afunction%20which%20can%20be%20used%20to%20restrict%20its%20function%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02490v1&entry.124074799=Read"},
{"title": "CADE: Cosine Annealing Differential Evolution for Spiking Neural Network", "author": "Runhua Jiang and Guodong Du and Shuyang Yu and Yifei Guo and Sim Kuan Goh and Ho-Kin Tang", "abstract": "  Spiking neural networks (SNNs) have gained prominence for their potential in\nneuromorphic computing and energy-efficient artificial intelligence, yet\noptimizing them remains a formidable challenge for gradient-based methods due\nto their discrete, spike-based computation. This paper attempts to tackle the\nchallenges by introducing Cosine Annealing Differential Evolution (CADE),\ndesigned to modulate the mutation factor (F) and crossover rate (CR) of\ndifferential evolution (DE) for the SNN model, i.e., Spiking Element Wise (SEW)\nResNet. Extensive empirical evaluations were conducted to analyze CADE. CADE\nshowed a balance in exploring and exploiting the search space, resulting in\naccelerated convergence and improved accuracy compared to existing\ngradient-based and DE-based methods. Moreover, an initialization method based\non a transfer learning setting was developed, pretraining on a source dataset\n(i.e., CIFAR-10) and fine-tuning the target dataset (i.e., CIFAR-100), to\nimprove population diversity. It was found to further enhance CADE for SNN.\nRemarkably, CADE elevates the performance of the highest accuracy SEW model by\nan additional 0.52 percentage points, underscoring its effectiveness in\nfine-tuning and enhancing SNNs. These findings emphasize the pivotal role of a\nscheduler for F and CR adjustment, especially for DE-based SNN. Source Code on\nGithub: https://github.com/Tank-Jiang/CADE4SNN.\n", "link": "http://arxiv.org/abs/2406.02349v1", "date": "2024-06-04", "relevancy": 2.0366, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5767}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4696}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CADE%3A%20Cosine%20Annealing%20Differential%20Evolution%20for%20Spiking%20Neural%20Network&body=Title%3A%20CADE%3A%20Cosine%20Annealing%20Differential%20Evolution%20for%20Spiking%20Neural%20Network%0AAuthor%3A%20Runhua%20Jiang%20and%20Guodong%20Du%20and%20Shuyang%20Yu%20and%20Yifei%20Guo%20and%20Sim%20Kuan%20Goh%20and%20Ho-Kin%20Tang%0AAbstract%3A%20%20%20Spiking%20neural%20networks%20%28SNNs%29%20have%20gained%20prominence%20for%20their%20potential%20in%0Aneuromorphic%20computing%20and%20energy-efficient%20artificial%20intelligence%2C%20yet%0Aoptimizing%20them%20remains%20a%20formidable%20challenge%20for%20gradient-based%20methods%20due%0Ato%20their%20discrete%2C%20spike-based%20computation.%20This%20paper%20attempts%20to%20tackle%20the%0Achallenges%20by%20introducing%20Cosine%20Annealing%20Differential%20Evolution%20%28CADE%29%2C%0Adesigned%20to%20modulate%20the%20mutation%20factor%20%28F%29%20and%20crossover%20rate%20%28CR%29%20of%0Adifferential%20evolution%20%28DE%29%20for%20the%20SNN%20model%2C%20i.e.%2C%20Spiking%20Element%20Wise%20%28SEW%29%0AResNet.%20Extensive%20empirical%20evaluations%20were%20conducted%20to%20analyze%20CADE.%20CADE%0Ashowed%20a%20balance%20in%20exploring%20and%20exploiting%20the%20search%20space%2C%20resulting%20in%0Aaccelerated%20convergence%20and%20improved%20accuracy%20compared%20to%20existing%0Agradient-based%20and%20DE-based%20methods.%20Moreover%2C%20an%20initialization%20method%20based%0Aon%20a%20transfer%20learning%20setting%20was%20developed%2C%20pretraining%20on%20a%20source%20dataset%0A%28i.e.%2C%20CIFAR-10%29%20and%20fine-tuning%20the%20target%20dataset%20%28i.e.%2C%20CIFAR-100%29%2C%20to%0Aimprove%20population%20diversity.%20It%20was%20found%20to%20further%20enhance%20CADE%20for%20SNN.%0ARemarkably%2C%20CADE%20elevates%20the%20performance%20of%20the%20highest%20accuracy%20SEW%20model%20by%0Aan%20additional%200.52%20percentage%20points%2C%20underscoring%20its%20effectiveness%20in%0Afine-tuning%20and%20enhancing%20SNNs.%20These%20findings%20emphasize%20the%20pivotal%20role%20of%20a%0Ascheduler%20for%20F%20and%20CR%20adjustment%2C%20especially%20for%20DE-based%20SNN.%20Source%20Code%20on%0AGithub%3A%20https%3A//github.com/Tank-Jiang/CADE4SNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCADE%253A%2520Cosine%2520Annealing%2520Differential%2520Evolution%2520for%2520Spiking%2520Neural%2520Network%26entry.906535625%3DRunhua%2520Jiang%2520and%2520Guodong%2520Du%2520and%2520Shuyang%2520Yu%2520and%2520Yifei%2520Guo%2520and%2520Sim%2520Kuan%2520Goh%2520and%2520Ho-Kin%2520Tang%26entry.1292438233%3D%2520%2520Spiking%2520neural%2520networks%2520%2528SNNs%2529%2520have%2520gained%2520prominence%2520for%2520their%2520potential%2520in%250Aneuromorphic%2520computing%2520and%2520energy-efficient%2520artificial%2520intelligence%252C%2520yet%250Aoptimizing%2520them%2520remains%2520a%2520formidable%2520challenge%2520for%2520gradient-based%2520methods%2520due%250Ato%2520their%2520discrete%252C%2520spike-based%2520computation.%2520This%2520paper%2520attempts%2520to%2520tackle%2520the%250Achallenges%2520by%2520introducing%2520Cosine%2520Annealing%2520Differential%2520Evolution%2520%2528CADE%2529%252C%250Adesigned%2520to%2520modulate%2520the%2520mutation%2520factor%2520%2528F%2529%2520and%2520crossover%2520rate%2520%2528CR%2529%2520of%250Adifferential%2520evolution%2520%2528DE%2529%2520for%2520the%2520SNN%2520model%252C%2520i.e.%252C%2520Spiking%2520Element%2520Wise%2520%2528SEW%2529%250AResNet.%2520Extensive%2520empirical%2520evaluations%2520were%2520conducted%2520to%2520analyze%2520CADE.%2520CADE%250Ashowed%2520a%2520balance%2520in%2520exploring%2520and%2520exploiting%2520the%2520search%2520space%252C%2520resulting%2520in%250Aaccelerated%2520convergence%2520and%2520improved%2520accuracy%2520compared%2520to%2520existing%250Agradient-based%2520and%2520DE-based%2520methods.%2520Moreover%252C%2520an%2520initialization%2520method%2520based%250Aon%2520a%2520transfer%2520learning%2520setting%2520was%2520developed%252C%2520pretraining%2520on%2520a%2520source%2520dataset%250A%2528i.e.%252C%2520CIFAR-10%2529%2520and%2520fine-tuning%2520the%2520target%2520dataset%2520%2528i.e.%252C%2520CIFAR-100%2529%252C%2520to%250Aimprove%2520population%2520diversity.%2520It%2520was%2520found%2520to%2520further%2520enhance%2520CADE%2520for%2520SNN.%250ARemarkably%252C%2520CADE%2520elevates%2520the%2520performance%2520of%2520the%2520highest%2520accuracy%2520SEW%2520model%2520by%250Aan%2520additional%25200.52%2520percentage%2520points%252C%2520underscoring%2520its%2520effectiveness%2520in%250Afine-tuning%2520and%2520enhancing%2520SNNs.%2520These%2520findings%2520emphasize%2520the%2520pivotal%2520role%2520of%2520a%250Ascheduler%2520for%2520F%2520and%2520CR%2520adjustment%252C%2520especially%2520for%2520DE-based%2520SNN.%2520Source%2520Code%2520on%250AGithub%253A%2520https%253A//github.com/Tank-Jiang/CADE4SNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CADE%3A%20Cosine%20Annealing%20Differential%20Evolution%20for%20Spiking%20Neural%20Network&entry.906535625=Runhua%20Jiang%20and%20Guodong%20Du%20and%20Shuyang%20Yu%20and%20Yifei%20Guo%20and%20Sim%20Kuan%20Goh%20and%20Ho-Kin%20Tang&entry.1292438233=%20%20Spiking%20neural%20networks%20%28SNNs%29%20have%20gained%20prominence%20for%20their%20potential%20in%0Aneuromorphic%20computing%20and%20energy-efficient%20artificial%20intelligence%2C%20yet%0Aoptimizing%20them%20remains%20a%20formidable%20challenge%20for%20gradient-based%20methods%20due%0Ato%20their%20discrete%2C%20spike-based%20computation.%20This%20paper%20attempts%20to%20tackle%20the%0Achallenges%20by%20introducing%20Cosine%20Annealing%20Differential%20Evolution%20%28CADE%29%2C%0Adesigned%20to%20modulate%20the%20mutation%20factor%20%28F%29%20and%20crossover%20rate%20%28CR%29%20of%0Adifferential%20evolution%20%28DE%29%20for%20the%20SNN%20model%2C%20i.e.%2C%20Spiking%20Element%20Wise%20%28SEW%29%0AResNet.%20Extensive%20empirical%20evaluations%20were%20conducted%20to%20analyze%20CADE.%20CADE%0Ashowed%20a%20balance%20in%20exploring%20and%20exploiting%20the%20search%20space%2C%20resulting%20in%0Aaccelerated%20convergence%20and%20improved%20accuracy%20compared%20to%20existing%0Agradient-based%20and%20DE-based%20methods.%20Moreover%2C%20an%20initialization%20method%20based%0Aon%20a%20transfer%20learning%20setting%20was%20developed%2C%20pretraining%20on%20a%20source%20dataset%0A%28i.e.%2C%20CIFAR-10%29%20and%20fine-tuning%20the%20target%20dataset%20%28i.e.%2C%20CIFAR-100%29%2C%20to%0Aimprove%20population%20diversity.%20It%20was%20found%20to%20further%20enhance%20CADE%20for%20SNN.%0ARemarkably%2C%20CADE%20elevates%20the%20performance%20of%20the%20highest%20accuracy%20SEW%20model%20by%0Aan%20additional%200.52%20percentage%20points%2C%20underscoring%20its%20effectiveness%20in%0Afine-tuning%20and%20enhancing%20SNNs.%20These%20findings%20emphasize%20the%20pivotal%20role%20of%20a%0Ascheduler%20for%20F%20and%20CR%20adjustment%2C%20especially%20for%20DE-based%20SNN.%20Source%20Code%20on%0AGithub%3A%20https%3A//github.com/Tank-Jiang/CADE4SNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02349v1&entry.124074799=Read"},
{"title": "ENOT: Expectile Regularization for Fast and Accurate Training of Neural\n  Optimal Transport", "author": "Nazar Buzun and Maksim Bobrin and Dmitry V. Dylov", "abstract": "  We present a new approach for Neural Optimal Transport (NOT) training\nprocedure, capable of accurately and efficiently estimating optimal\ntransportation plan via specific regularization on dual Kantorovich potentials.\nThe main bottleneck of existing NOT solvers is associated with the procedure of\nfinding a near-exact approximation of the conjugate operator (i.e., the\nc-transform), which is done either by optimizing over non-convex max-min\nobjectives or by the computationally intensive fine-tuning of the initial\napproximated prediction. We resolve both issues by proposing a new,\ntheoretically justified loss in the form of expectile regularisation which\nenforces binding conditions on the learning process of dual potentials. Such a\nregularization provides the upper bound estimation over the distribution of\npossible conjugate potentials and makes the learning stable, completely\neliminating the need for additional extensive fine-tuning. Proposed method,\ncalled Expectile-Regularised Neural Optimal Transport (ENOT), outperforms\nprevious state-of-the-art approaches on the established Wasserstein-2 benchmark\ntasks by a large margin (up to a 3-fold improvement in quality and up to a\n10-fold improvement in runtime). Moreover, we showcase performance of ENOT for\nvarying cost functions on different tasks such as image generation, showing\nrobustness of proposed algorithm.\n", "link": "http://arxiv.org/abs/2403.03777v2", "date": "2024-06-04", "relevancy": 2.0261, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5042}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENOT%3A%20Expectile%20Regularization%20for%20Fast%20and%20Accurate%20Training%20of%20Neural%0A%20%20Optimal%20Transport&body=Title%3A%20ENOT%3A%20Expectile%20Regularization%20for%20Fast%20and%20Accurate%20Training%20of%20Neural%0A%20%20Optimal%20Transport%0AAuthor%3A%20Nazar%20Buzun%20and%20Maksim%20Bobrin%20and%20Dmitry%20V.%20Dylov%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20for%20Neural%20Optimal%20Transport%20%28NOT%29%20training%0Aprocedure%2C%20capable%20of%20accurately%20and%20efficiently%20estimating%20optimal%0Atransportation%20plan%20via%20specific%20regularization%20on%20dual%20Kantorovich%20potentials.%0AThe%20main%20bottleneck%20of%20existing%20NOT%20solvers%20is%20associated%20with%20the%20procedure%20of%0Afinding%20a%20near-exact%20approximation%20of%20the%20conjugate%20operator%20%28i.e.%2C%20the%0Ac-transform%29%2C%20which%20is%20done%20either%20by%20optimizing%20over%20non-convex%20max-min%0Aobjectives%20or%20by%20the%20computationally%20intensive%20fine-tuning%20of%20the%20initial%0Aapproximated%20prediction.%20We%20resolve%20both%20issues%20by%20proposing%20a%20new%2C%0Atheoretically%20justified%20loss%20in%20the%20form%20of%20expectile%20regularisation%20which%0Aenforces%20binding%20conditions%20on%20the%20learning%20process%20of%20dual%20potentials.%20Such%20a%0Aregularization%20provides%20the%20upper%20bound%20estimation%20over%20the%20distribution%20of%0Apossible%20conjugate%20potentials%20and%20makes%20the%20learning%20stable%2C%20completely%0Aeliminating%20the%20need%20for%20additional%20extensive%20fine-tuning.%20Proposed%20method%2C%0Acalled%20Expectile-Regularised%20Neural%20Optimal%20Transport%20%28ENOT%29%2C%20outperforms%0Aprevious%20state-of-the-art%20approaches%20on%20the%20established%20Wasserstein-2%20benchmark%0Atasks%20by%20a%20large%20margin%20%28up%20to%20a%203-fold%20improvement%20in%20quality%20and%20up%20to%20a%0A10-fold%20improvement%20in%20runtime%29.%20Moreover%2C%20we%20showcase%20performance%20of%20ENOT%20for%0Avarying%20cost%20functions%20on%20different%20tasks%20such%20as%20image%20generation%2C%20showing%0Arobustness%20of%20proposed%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENOT%253A%2520Expectile%2520Regularization%2520for%2520Fast%2520and%2520Accurate%2520Training%2520of%2520Neural%250A%2520%2520Optimal%2520Transport%26entry.906535625%3DNazar%2520Buzun%2520and%2520Maksim%2520Bobrin%2520and%2520Dmitry%2520V.%2520Dylov%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520approach%2520for%2520Neural%2520Optimal%2520Transport%2520%2528NOT%2529%2520training%250Aprocedure%252C%2520capable%2520of%2520accurately%2520and%2520efficiently%2520estimating%2520optimal%250Atransportation%2520plan%2520via%2520specific%2520regularization%2520on%2520dual%2520Kantorovich%2520potentials.%250AThe%2520main%2520bottleneck%2520of%2520existing%2520NOT%2520solvers%2520is%2520associated%2520with%2520the%2520procedure%2520of%250Afinding%2520a%2520near-exact%2520approximation%2520of%2520the%2520conjugate%2520operator%2520%2528i.e.%252C%2520the%250Ac-transform%2529%252C%2520which%2520is%2520done%2520either%2520by%2520optimizing%2520over%2520non-convex%2520max-min%250Aobjectives%2520or%2520by%2520the%2520computationally%2520intensive%2520fine-tuning%2520of%2520the%2520initial%250Aapproximated%2520prediction.%2520We%2520resolve%2520both%2520issues%2520by%2520proposing%2520a%2520new%252C%250Atheoretically%2520justified%2520loss%2520in%2520the%2520form%2520of%2520expectile%2520regularisation%2520which%250Aenforces%2520binding%2520conditions%2520on%2520the%2520learning%2520process%2520of%2520dual%2520potentials.%2520Such%2520a%250Aregularization%2520provides%2520the%2520upper%2520bound%2520estimation%2520over%2520the%2520distribution%2520of%250Apossible%2520conjugate%2520potentials%2520and%2520makes%2520the%2520learning%2520stable%252C%2520completely%250Aeliminating%2520the%2520need%2520for%2520additional%2520extensive%2520fine-tuning.%2520Proposed%2520method%252C%250Acalled%2520Expectile-Regularised%2520Neural%2520Optimal%2520Transport%2520%2528ENOT%2529%252C%2520outperforms%250Aprevious%2520state-of-the-art%2520approaches%2520on%2520the%2520established%2520Wasserstein-2%2520benchmark%250Atasks%2520by%2520a%2520large%2520margin%2520%2528up%2520to%2520a%25203-fold%2520improvement%2520in%2520quality%2520and%2520up%2520to%2520a%250A10-fold%2520improvement%2520in%2520runtime%2529.%2520Moreover%252C%2520we%2520showcase%2520performance%2520of%2520ENOT%2520for%250Avarying%2520cost%2520functions%2520on%2520different%2520tasks%2520such%2520as%2520image%2520generation%252C%2520showing%250Arobustness%2520of%2520proposed%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENOT%3A%20Expectile%20Regularization%20for%20Fast%20and%20Accurate%20Training%20of%20Neural%0A%20%20Optimal%20Transport&entry.906535625=Nazar%20Buzun%20and%20Maksim%20Bobrin%20and%20Dmitry%20V.%20Dylov&entry.1292438233=%20%20We%20present%20a%20new%20approach%20for%20Neural%20Optimal%20Transport%20%28NOT%29%20training%0Aprocedure%2C%20capable%20of%20accurately%20and%20efficiently%20estimating%20optimal%0Atransportation%20plan%20via%20specific%20regularization%20on%20dual%20Kantorovich%20potentials.%0AThe%20main%20bottleneck%20of%20existing%20NOT%20solvers%20is%20associated%20with%20the%20procedure%20of%0Afinding%20a%20near-exact%20approximation%20of%20the%20conjugate%20operator%20%28i.e.%2C%20the%0Ac-transform%29%2C%20which%20is%20done%20either%20by%20optimizing%20over%20non-convex%20max-min%0Aobjectives%20or%20by%20the%20computationally%20intensive%20fine-tuning%20of%20the%20initial%0Aapproximated%20prediction.%20We%20resolve%20both%20issues%20by%20proposing%20a%20new%2C%0Atheoretically%20justified%20loss%20in%20the%20form%20of%20expectile%20regularisation%20which%0Aenforces%20binding%20conditions%20on%20the%20learning%20process%20of%20dual%20potentials.%20Such%20a%0Aregularization%20provides%20the%20upper%20bound%20estimation%20over%20the%20distribution%20of%0Apossible%20conjugate%20potentials%20and%20makes%20the%20learning%20stable%2C%20completely%0Aeliminating%20the%20need%20for%20additional%20extensive%20fine-tuning.%20Proposed%20method%2C%0Acalled%20Expectile-Regularised%20Neural%20Optimal%20Transport%20%28ENOT%29%2C%20outperforms%0Aprevious%20state-of-the-art%20approaches%20on%20the%20established%20Wasserstein-2%20benchmark%0Atasks%20by%20a%20large%20margin%20%28up%20to%20a%203-fold%20improvement%20in%20quality%20and%20up%20to%20a%0A10-fold%20improvement%20in%20runtime%29.%20Moreover%2C%20we%20showcase%20performance%20of%20ENOT%20for%0Avarying%20cost%20functions%20on%20different%20tasks%20such%20as%20image%20generation%2C%20showing%0Arobustness%20of%20proposed%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03777v2&entry.124074799=Read"},
{"title": "An Observability-Constrained Magnetic-Field-Aided Inertial Navigation\n  System", "author": "Chuan Huang and Gustaf Hendeby and Isaac Skog", "abstract": "  A method to construct an observability-constrained magnetic-field-aided\ninertial navigation system is proposed. The proposed method builds upon the\npreviously proposed observability-constrained extended Kalman filter and\nextends it to work with a magnetic-field-based odometry-aided inertial\nnavigation system. The proposed method is evaluated using simulation and\nreal-world data, showing that (i) the system observability properties are\npreserved, (ii) the estimation accuracy increases, and (iii) the perceived\nuncertainty calculated by the EKF is more consistent with the true uncertainty\nof the filter estimates.\n", "link": "http://arxiv.org/abs/2406.02161v1", "date": "2024-06-04", "relevancy": 2.0251, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5201}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Observability-Constrained%20Magnetic-Field-Aided%20Inertial%20Navigation%0A%20%20System&body=Title%3A%20An%20Observability-Constrained%20Magnetic-Field-Aided%20Inertial%20Navigation%0A%20%20System%0AAuthor%3A%20Chuan%20Huang%20and%20Gustaf%20Hendeby%20and%20Isaac%20Skog%0AAbstract%3A%20%20%20A%20method%20to%20construct%20an%20observability-constrained%20magnetic-field-aided%0Ainertial%20navigation%20system%20is%20proposed.%20The%20proposed%20method%20builds%20upon%20the%0Apreviously%20proposed%20observability-constrained%20extended%20Kalman%20filter%20and%0Aextends%20it%20to%20work%20with%20a%20magnetic-field-based%20odometry-aided%20inertial%0Anavigation%20system.%20The%20proposed%20method%20is%20evaluated%20using%20simulation%20and%0Areal-world%20data%2C%20showing%20that%20%28i%29%20the%20system%20observability%20properties%20are%0Apreserved%2C%20%28ii%29%20the%20estimation%20accuracy%20increases%2C%20and%20%28iii%29%20the%20perceived%0Auncertainty%20calculated%20by%20the%20EKF%20is%20more%20consistent%20with%20the%20true%20uncertainty%0Aof%20the%20filter%20estimates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Observability-Constrained%2520Magnetic-Field-Aided%2520Inertial%2520Navigation%250A%2520%2520System%26entry.906535625%3DChuan%2520Huang%2520and%2520Gustaf%2520Hendeby%2520and%2520Isaac%2520Skog%26entry.1292438233%3D%2520%2520A%2520method%2520to%2520construct%2520an%2520observability-constrained%2520magnetic-field-aided%250Ainertial%2520navigation%2520system%2520is%2520proposed.%2520The%2520proposed%2520method%2520builds%2520upon%2520the%250Apreviously%2520proposed%2520observability-constrained%2520extended%2520Kalman%2520filter%2520and%250Aextends%2520it%2520to%2520work%2520with%2520a%2520magnetic-field-based%2520odometry-aided%2520inertial%250Anavigation%2520system.%2520The%2520proposed%2520method%2520is%2520evaluated%2520using%2520simulation%2520and%250Areal-world%2520data%252C%2520showing%2520that%2520%2528i%2529%2520the%2520system%2520observability%2520properties%2520are%250Apreserved%252C%2520%2528ii%2529%2520the%2520estimation%2520accuracy%2520increases%252C%2520and%2520%2528iii%2529%2520the%2520perceived%250Auncertainty%2520calculated%2520by%2520the%2520EKF%2520is%2520more%2520consistent%2520with%2520the%2520true%2520uncertainty%250Aof%2520the%2520filter%2520estimates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Observability-Constrained%20Magnetic-Field-Aided%20Inertial%20Navigation%0A%20%20System&entry.906535625=Chuan%20Huang%20and%20Gustaf%20Hendeby%20and%20Isaac%20Skog&entry.1292438233=%20%20A%20method%20to%20construct%20an%20observability-constrained%20magnetic-field-aided%0Ainertial%20navigation%20system%20is%20proposed.%20The%20proposed%20method%20builds%20upon%20the%0Apreviously%20proposed%20observability-constrained%20extended%20Kalman%20filter%20and%0Aextends%20it%20to%20work%20with%20a%20magnetic-field-based%20odometry-aided%20inertial%0Anavigation%20system.%20The%20proposed%20method%20is%20evaluated%20using%20simulation%20and%0Areal-world%20data%2C%20showing%20that%20%28i%29%20the%20system%20observability%20properties%20are%0Apreserved%2C%20%28ii%29%20the%20estimation%20accuracy%20increases%2C%20and%20%28iii%29%20the%20perceived%0Auncertainty%20calculated%20by%20the%20EKF%20is%20more%20consistent%20with%20the%20true%20uncertainty%0Aof%20the%20filter%20estimates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02161v1&entry.124074799=Read"},
{"title": "Improving Transformers with Dynamically Composable Multi-Head Attention", "author": "Da Xiao and Qingye Meng and Shengping Li and Xingyuan Yuan", "abstract": "  Multi-Head Attention (MHA) is a key component of Transformer. In MHA,\nattention heads work independently, causing problems such as low-rank\nbottleneck of attention score matrices and head redundancy. We propose\nDynamically Composable Multi-Head Attention (DCMHA), a parameter and\ncomputation efficient attention architecture that tackles the shortcomings of\nMHA and increases the expressive power of the model by dynamically composing\nattention heads. At the core of DCMHA is a $\\it{Compose}$ function that\ntransforms the attention score and weight matrices in an input-dependent way.\nDCMHA can be used as a drop-in replacement of MHA in any transformer\narchitecture to obtain the corresponding DCFormer. DCFormer significantly\noutperforms Transformer on different architectures and model scales in language\nmodeling, matching the performance of models with ~1.7x-2.0x compute. For\nexample, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining\nperplexity and downstream task evaluation. The code and models are available at\nhttps://github.com/Caiyun-AI/DCFormer.\n", "link": "http://arxiv.org/abs/2405.08553v2", "date": "2024-06-04", "relevancy": 2.017, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.521}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5025}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Transformers%20with%20Dynamically%20Composable%20Multi-Head%20Attention&body=Title%3A%20Improving%20Transformers%20with%20Dynamically%20Composable%20Multi-Head%20Attention%0AAuthor%3A%20Da%20Xiao%20and%20Qingye%20Meng%20and%20Shengping%20Li%20and%20Xingyuan%20Yuan%0AAbstract%3A%20%20%20Multi-Head%20Attention%20%28MHA%29%20is%20a%20key%20component%20of%20Transformer.%20In%20MHA%2C%0Aattention%20heads%20work%20independently%2C%20causing%20problems%20such%20as%20low-rank%0Abottleneck%20of%20attention%20score%20matrices%20and%20head%20redundancy.%20We%20propose%0ADynamically%20Composable%20Multi-Head%20Attention%20%28DCMHA%29%2C%20a%20parameter%20and%0Acomputation%20efficient%20attention%20architecture%20that%20tackles%20the%20shortcomings%20of%0AMHA%20and%20increases%20the%20expressive%20power%20of%20the%20model%20by%20dynamically%20composing%0Aattention%20heads.%20At%20the%20core%20of%20DCMHA%20is%20a%20%24%5Cit%7BCompose%7D%24%20function%20that%0Atransforms%20the%20attention%20score%20and%20weight%20matrices%20in%20an%20input-dependent%20way.%0ADCMHA%20can%20be%20used%20as%20a%20drop-in%20replacement%20of%20MHA%20in%20any%20transformer%0Aarchitecture%20to%20obtain%20the%20corresponding%20DCFormer.%20DCFormer%20significantly%0Aoutperforms%20Transformer%20on%20different%20architectures%20and%20model%20scales%20in%20language%0Amodeling%2C%20matching%20the%20performance%20of%20models%20with%20~1.7x-2.0x%20compute.%20For%0Aexample%2C%20DCPythia-6.9B%20outperforms%20open%20source%20Pythia-12B%20on%20both%20pretraining%0Aperplexity%20and%20downstream%20task%20evaluation.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/Caiyun-AI/DCFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Transformers%2520with%2520Dynamically%2520Composable%2520Multi-Head%2520Attention%26entry.906535625%3DDa%2520Xiao%2520and%2520Qingye%2520Meng%2520and%2520Shengping%2520Li%2520and%2520Xingyuan%2520Yuan%26entry.1292438233%3D%2520%2520Multi-Head%2520Attention%2520%2528MHA%2529%2520is%2520a%2520key%2520component%2520of%2520Transformer.%2520In%2520MHA%252C%250Aattention%2520heads%2520work%2520independently%252C%2520causing%2520problems%2520such%2520as%2520low-rank%250Abottleneck%2520of%2520attention%2520score%2520matrices%2520and%2520head%2520redundancy.%2520We%2520propose%250ADynamically%2520Composable%2520Multi-Head%2520Attention%2520%2528DCMHA%2529%252C%2520a%2520parameter%2520and%250Acomputation%2520efficient%2520attention%2520architecture%2520that%2520tackles%2520the%2520shortcomings%2520of%250AMHA%2520and%2520increases%2520the%2520expressive%2520power%2520of%2520the%2520model%2520by%2520dynamically%2520composing%250Aattention%2520heads.%2520At%2520the%2520core%2520of%2520DCMHA%2520is%2520a%2520%2524%255Cit%257BCompose%257D%2524%2520function%2520that%250Atransforms%2520the%2520attention%2520score%2520and%2520weight%2520matrices%2520in%2520an%2520input-dependent%2520way.%250ADCMHA%2520can%2520be%2520used%2520as%2520a%2520drop-in%2520replacement%2520of%2520MHA%2520in%2520any%2520transformer%250Aarchitecture%2520to%2520obtain%2520the%2520corresponding%2520DCFormer.%2520DCFormer%2520significantly%250Aoutperforms%2520Transformer%2520on%2520different%2520architectures%2520and%2520model%2520scales%2520in%2520language%250Amodeling%252C%2520matching%2520the%2520performance%2520of%2520models%2520with%2520~1.7x-2.0x%2520compute.%2520For%250Aexample%252C%2520DCPythia-6.9B%2520outperforms%2520open%2520source%2520Pythia-12B%2520on%2520both%2520pretraining%250Aperplexity%2520and%2520downstream%2520task%2520evaluation.%2520The%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/Caiyun-AI/DCFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Transformers%20with%20Dynamically%20Composable%20Multi-Head%20Attention&entry.906535625=Da%20Xiao%20and%20Qingye%20Meng%20and%20Shengping%20Li%20and%20Xingyuan%20Yuan&entry.1292438233=%20%20Multi-Head%20Attention%20%28MHA%29%20is%20a%20key%20component%20of%20Transformer.%20In%20MHA%2C%0Aattention%20heads%20work%20independently%2C%20causing%20problems%20such%20as%20low-rank%0Abottleneck%20of%20attention%20score%20matrices%20and%20head%20redundancy.%20We%20propose%0ADynamically%20Composable%20Multi-Head%20Attention%20%28DCMHA%29%2C%20a%20parameter%20and%0Acomputation%20efficient%20attention%20architecture%20that%20tackles%20the%20shortcomings%20of%0AMHA%20and%20increases%20the%20expressive%20power%20of%20the%20model%20by%20dynamically%20composing%0Aattention%20heads.%20At%20the%20core%20of%20DCMHA%20is%20a%20%24%5Cit%7BCompose%7D%24%20function%20that%0Atransforms%20the%20attention%20score%20and%20weight%20matrices%20in%20an%20input-dependent%20way.%0ADCMHA%20can%20be%20used%20as%20a%20drop-in%20replacement%20of%20MHA%20in%20any%20transformer%0Aarchitecture%20to%20obtain%20the%20corresponding%20DCFormer.%20DCFormer%20significantly%0Aoutperforms%20Transformer%20on%20different%20architectures%20and%20model%20scales%20in%20language%0Amodeling%2C%20matching%20the%20performance%20of%20models%20with%20~1.7x-2.0x%20compute.%20For%0Aexample%2C%20DCPythia-6.9B%20outperforms%20open%20source%20Pythia-12B%20on%20both%20pretraining%0Aperplexity%20and%20downstream%20task%20evaluation.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/Caiyun-AI/DCFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08553v2&entry.124074799=Read"},
{"title": "GrootVL: Tree Topology is All You Need in State Space Model", "author": "Yicheng Xiao and Lin Song and Shaoli Huang and Jiangshan Wang and Siyu Song and Yixiao Ge and Xiu Li and Ying Shan", "abstract": "  The state space models, employing recursively propagated features,\ndemonstrate strong representation capabilities comparable to Transformer models\nand superior efficiency. However, constrained by the inherent geometric\nconstraints of sequences, it still falls short in modeling long-range\ndependencies. To address this issue, we propose the GrootVL network, which\nfirst dynamically generates a tree topology based on spatial relationships and\ninput features. Then, feature propagation is performed based on this graph,\nthereby breaking the original sequence constraints to achieve stronger\nrepresentation capabilities. Additionally, we introduce a linear complexity\ndynamic programming algorithm to enhance long-range interactions without\nincreasing computational cost. GrootVL is a versatile multimodal framework that\ncan be applied to both visual and textual tasks. Extensive experiments\ndemonstrate that our method significantly outperforms existing structured state\nspace models on image classification, object detection and segmentation.\nBesides, by fine-tuning large language models, our approach achieves consistent\nimprovements in multiple textual tasks at minor training cost.\n", "link": "http://arxiv.org/abs/2406.02395v1", "date": "2024-06-04", "relevancy": 2.011, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5073}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5042}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GrootVL%3A%20Tree%20Topology%20is%20All%20You%20Need%20in%20State%20Space%20Model&body=Title%3A%20GrootVL%3A%20Tree%20Topology%20is%20All%20You%20Need%20in%20State%20Space%20Model%0AAuthor%3A%20Yicheng%20Xiao%20and%20Lin%20Song%20and%20Shaoli%20Huang%20and%20Jiangshan%20Wang%20and%20Siyu%20Song%20and%20Yixiao%20Ge%20and%20Xiu%20Li%20and%20Ying%20Shan%0AAbstract%3A%20%20%20The%20state%20space%20models%2C%20employing%20recursively%20propagated%20features%2C%0Ademonstrate%20strong%20representation%20capabilities%20comparable%20to%20Transformer%20models%0Aand%20superior%20efficiency.%20However%2C%20constrained%20by%20the%20inherent%20geometric%0Aconstraints%20of%20sequences%2C%20it%20still%20falls%20short%20in%20modeling%20long-range%0Adependencies.%20To%20address%20this%20issue%2C%20we%20propose%20the%20GrootVL%20network%2C%20which%0Afirst%20dynamically%20generates%20a%20tree%20topology%20based%20on%20spatial%20relationships%20and%0Ainput%20features.%20Then%2C%20feature%20propagation%20is%20performed%20based%20on%20this%20graph%2C%0Athereby%20breaking%20the%20original%20sequence%20constraints%20to%20achieve%20stronger%0Arepresentation%20capabilities.%20Additionally%2C%20we%20introduce%20a%20linear%20complexity%0Adynamic%20programming%20algorithm%20to%20enhance%20long-range%20interactions%20without%0Aincreasing%20computational%20cost.%20GrootVL%20is%20a%20versatile%20multimodal%20framework%20that%0Acan%20be%20applied%20to%20both%20visual%20and%20textual%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20structured%20state%0Aspace%20models%20on%20image%20classification%2C%20object%20detection%20and%20segmentation.%0ABesides%2C%20by%20fine-tuning%20large%20language%20models%2C%20our%20approach%20achieves%20consistent%0Aimprovements%20in%20multiple%20textual%20tasks%20at%20minor%20training%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrootVL%253A%2520Tree%2520Topology%2520is%2520All%2520You%2520Need%2520in%2520State%2520Space%2520Model%26entry.906535625%3DYicheng%2520Xiao%2520and%2520Lin%2520Song%2520and%2520Shaoli%2520Huang%2520and%2520Jiangshan%2520Wang%2520and%2520Siyu%2520Song%2520and%2520Yixiao%2520Ge%2520and%2520Xiu%2520Li%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520The%2520state%2520space%2520models%252C%2520employing%2520recursively%2520propagated%2520features%252C%250Ademonstrate%2520strong%2520representation%2520capabilities%2520comparable%2520to%2520Transformer%2520models%250Aand%2520superior%2520efficiency.%2520However%252C%2520constrained%2520by%2520the%2520inherent%2520geometric%250Aconstraints%2520of%2520sequences%252C%2520it%2520still%2520falls%2520short%2520in%2520modeling%2520long-range%250Adependencies.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520GrootVL%2520network%252C%2520which%250Afirst%2520dynamically%2520generates%2520a%2520tree%2520topology%2520based%2520on%2520spatial%2520relationships%2520and%250Ainput%2520features.%2520Then%252C%2520feature%2520propagation%2520is%2520performed%2520based%2520on%2520this%2520graph%252C%250Athereby%2520breaking%2520the%2520original%2520sequence%2520constraints%2520to%2520achieve%2520stronger%250Arepresentation%2520capabilities.%2520Additionally%252C%2520we%2520introduce%2520a%2520linear%2520complexity%250Adynamic%2520programming%2520algorithm%2520to%2520enhance%2520long-range%2520interactions%2520without%250Aincreasing%2520computational%2520cost.%2520GrootVL%2520is%2520a%2520versatile%2520multimodal%2520framework%2520that%250Acan%2520be%2520applied%2520to%2520both%2520visual%2520and%2520textual%2520tasks.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%2520structured%2520state%250Aspace%2520models%2520on%2520image%2520classification%252C%2520object%2520detection%2520and%2520segmentation.%250ABesides%252C%2520by%2520fine-tuning%2520large%2520language%2520models%252C%2520our%2520approach%2520achieves%2520consistent%250Aimprovements%2520in%2520multiple%2520textual%2520tasks%2520at%2520minor%2520training%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GrootVL%3A%20Tree%20Topology%20is%20All%20You%20Need%20in%20State%20Space%20Model&entry.906535625=Yicheng%20Xiao%20and%20Lin%20Song%20and%20Shaoli%20Huang%20and%20Jiangshan%20Wang%20and%20Siyu%20Song%20and%20Yixiao%20Ge%20and%20Xiu%20Li%20and%20Ying%20Shan&entry.1292438233=%20%20The%20state%20space%20models%2C%20employing%20recursively%20propagated%20features%2C%0Ademonstrate%20strong%20representation%20capabilities%20comparable%20to%20Transformer%20models%0Aand%20superior%20efficiency.%20However%2C%20constrained%20by%20the%20inherent%20geometric%0Aconstraints%20of%20sequences%2C%20it%20still%20falls%20short%20in%20modeling%20long-range%0Adependencies.%20To%20address%20this%20issue%2C%20we%20propose%20the%20GrootVL%20network%2C%20which%0Afirst%20dynamically%20generates%20a%20tree%20topology%20based%20on%20spatial%20relationships%20and%0Ainput%20features.%20Then%2C%20feature%20propagation%20is%20performed%20based%20on%20this%20graph%2C%0Athereby%20breaking%20the%20original%20sequence%20constraints%20to%20achieve%20stronger%0Arepresentation%20capabilities.%20Additionally%2C%20we%20introduce%20a%20linear%20complexity%0Adynamic%20programming%20algorithm%20to%20enhance%20long-range%20interactions%20without%0Aincreasing%20computational%20cost.%20GrootVL%20is%20a%20versatile%20multimodal%20framework%20that%0Acan%20be%20applied%20to%20both%20visual%20and%20textual%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20structured%20state%0Aspace%20models%20on%20image%20classification%2C%20object%20detection%20and%20segmentation.%0ABesides%2C%20by%20fine-tuning%20large%20language%20models%2C%20our%20approach%20achieves%20consistent%0Aimprovements%20in%20multiple%20textual%20tasks%20at%20minor%20training%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02395v1&entry.124074799=Read"},
{"title": "AI-Face: A Million-Scale Demographically Annotated AI-Generated Face\n  Dataset and Fairness Benchmark", "author": "Li Lin and  Santosh and Xin Wang and Shu Hu", "abstract": "  AI-generated faces have enriched human life, such as entertainment,\neducation, and art. However, they also pose misuse risks. Therefore, detecting\nAI-generated faces becomes crucial, yet current detectors show biased\nperformance across different demographic groups. Mitigating biases can be done\nby designing algorithmic fairness methods, which usually require\ndemographically annotated face datasets for model training. However, no\nexisting dataset comprehensively encompasses both demographic attributes and\ndiverse generative methods, which hinders the development of fair detectors for\nAI-generated faces. In this work, we introduce the AI-Face dataset, the first\nmillion-scale demographically annotated AI-generated face image dataset,\nincluding real faces, faces from deepfake videos, and faces generated by\nGenerative Adversarial Networks and Diffusion Models. Based on this dataset, we\nconduct the first comprehensive fairness benchmark to assess various AI face\ndetectors and provide valuable insights and findings to promote the future fair\ndesign of AI face detectors. Our AI-Face dataset and benchmark code are\npublicly available at https://github.com/Purdue-M2/AI-Face-FairnessBench.\n", "link": "http://arxiv.org/abs/2406.00783v2", "date": "2024-06-04", "relevancy": 1.9947, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5149}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4912}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Face%3A%20A%20Million-Scale%20Demographically%20Annotated%20AI-Generated%20Face%0A%20%20Dataset%20and%20Fairness%20Benchmark&body=Title%3A%20AI-Face%3A%20A%20Million-Scale%20Demographically%20Annotated%20AI-Generated%20Face%0A%20%20Dataset%20and%20Fairness%20Benchmark%0AAuthor%3A%20Li%20Lin%20and%20%20Santosh%20and%20Xin%20Wang%20and%20Shu%20Hu%0AAbstract%3A%20%20%20AI-generated%20faces%20have%20enriched%20human%20life%2C%20such%20as%20entertainment%2C%0Aeducation%2C%20and%20art.%20However%2C%20they%20also%20pose%20misuse%20risks.%20Therefore%2C%20detecting%0AAI-generated%20faces%20becomes%20crucial%2C%20yet%20current%20detectors%20show%20biased%0Aperformance%20across%20different%20demographic%20groups.%20Mitigating%20biases%20can%20be%20done%0Aby%20designing%20algorithmic%20fairness%20methods%2C%20which%20usually%20require%0Ademographically%20annotated%20face%20datasets%20for%20model%20training.%20However%2C%20no%0Aexisting%20dataset%20comprehensively%20encompasses%20both%20demographic%20attributes%20and%0Adiverse%20generative%20methods%2C%20which%20hinders%20the%20development%20of%20fair%20detectors%20for%0AAI-generated%20faces.%20In%20this%20work%2C%20we%20introduce%20the%20AI-Face%20dataset%2C%20the%20first%0Amillion-scale%20demographically%20annotated%20AI-generated%20face%20image%20dataset%2C%0Aincluding%20real%20faces%2C%20faces%20from%20deepfake%20videos%2C%20and%20faces%20generated%20by%0AGenerative%20Adversarial%20Networks%20and%20Diffusion%20Models.%20Based%20on%20this%20dataset%2C%20we%0Aconduct%20the%20first%20comprehensive%20fairness%20benchmark%20to%20assess%20various%20AI%20face%0Adetectors%20and%20provide%20valuable%20insights%20and%20findings%20to%20promote%20the%20future%20fair%0Adesign%20of%20AI%20face%20detectors.%20Our%20AI-Face%20dataset%20and%20benchmark%20code%20are%0Apublicly%20available%20at%20https%3A//github.com/Purdue-M2/AI-Face-FairnessBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Face%253A%2520A%2520Million-Scale%2520Demographically%2520Annotated%2520AI-Generated%2520Face%250A%2520%2520Dataset%2520and%2520Fairness%2520Benchmark%26entry.906535625%3DLi%2520Lin%2520and%2520%2520Santosh%2520and%2520Xin%2520Wang%2520and%2520Shu%2520Hu%26entry.1292438233%3D%2520%2520AI-generated%2520faces%2520have%2520enriched%2520human%2520life%252C%2520such%2520as%2520entertainment%252C%250Aeducation%252C%2520and%2520art.%2520However%252C%2520they%2520also%2520pose%2520misuse%2520risks.%2520Therefore%252C%2520detecting%250AAI-generated%2520faces%2520becomes%2520crucial%252C%2520yet%2520current%2520detectors%2520show%2520biased%250Aperformance%2520across%2520different%2520demographic%2520groups.%2520Mitigating%2520biases%2520can%2520be%2520done%250Aby%2520designing%2520algorithmic%2520fairness%2520methods%252C%2520which%2520usually%2520require%250Ademographically%2520annotated%2520face%2520datasets%2520for%2520model%2520training.%2520However%252C%2520no%250Aexisting%2520dataset%2520comprehensively%2520encompasses%2520both%2520demographic%2520attributes%2520and%250Adiverse%2520generative%2520methods%252C%2520which%2520hinders%2520the%2520development%2520of%2520fair%2520detectors%2520for%250AAI-generated%2520faces.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520AI-Face%2520dataset%252C%2520the%2520first%250Amillion-scale%2520demographically%2520annotated%2520AI-generated%2520face%2520image%2520dataset%252C%250Aincluding%2520real%2520faces%252C%2520faces%2520from%2520deepfake%2520videos%252C%2520and%2520faces%2520generated%2520by%250AGenerative%2520Adversarial%2520Networks%2520and%2520Diffusion%2520Models.%2520Based%2520on%2520this%2520dataset%252C%2520we%250Aconduct%2520the%2520first%2520comprehensive%2520fairness%2520benchmark%2520to%2520assess%2520various%2520AI%2520face%250Adetectors%2520and%2520provide%2520valuable%2520insights%2520and%2520findings%2520to%2520promote%2520the%2520future%2520fair%250Adesign%2520of%2520AI%2520face%2520detectors.%2520Our%2520AI-Face%2520dataset%2520and%2520benchmark%2520code%2520are%250Apublicly%2520available%2520at%2520https%253A//github.com/Purdue-M2/AI-Face-FairnessBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Face%3A%20A%20Million-Scale%20Demographically%20Annotated%20AI-Generated%20Face%0A%20%20Dataset%20and%20Fairness%20Benchmark&entry.906535625=Li%20Lin%20and%20%20Santosh%20and%20Xin%20Wang%20and%20Shu%20Hu&entry.1292438233=%20%20AI-generated%20faces%20have%20enriched%20human%20life%2C%20such%20as%20entertainment%2C%0Aeducation%2C%20and%20art.%20However%2C%20they%20also%20pose%20misuse%20risks.%20Therefore%2C%20detecting%0AAI-generated%20faces%20becomes%20crucial%2C%20yet%20current%20detectors%20show%20biased%0Aperformance%20across%20different%20demographic%20groups.%20Mitigating%20biases%20can%20be%20done%0Aby%20designing%20algorithmic%20fairness%20methods%2C%20which%20usually%20require%0Ademographically%20annotated%20face%20datasets%20for%20model%20training.%20However%2C%20no%0Aexisting%20dataset%20comprehensively%20encompasses%20both%20demographic%20attributes%20and%0Adiverse%20generative%20methods%2C%20which%20hinders%20the%20development%20of%20fair%20detectors%20for%0AAI-generated%20faces.%20In%20this%20work%2C%20we%20introduce%20the%20AI-Face%20dataset%2C%20the%20first%0Amillion-scale%20demographically%20annotated%20AI-generated%20face%20image%20dataset%2C%0Aincluding%20real%20faces%2C%20faces%20from%20deepfake%20videos%2C%20and%20faces%20generated%20by%0AGenerative%20Adversarial%20Networks%20and%20Diffusion%20Models.%20Based%20on%20this%20dataset%2C%20we%0Aconduct%20the%20first%20comprehensive%20fairness%20benchmark%20to%20assess%20various%20AI%20face%0Adetectors%20and%20provide%20valuable%20insights%20and%20findings%20to%20promote%20the%20future%20fair%0Adesign%20of%20AI%20face%20detectors.%20Our%20AI-Face%20dataset%20and%20benchmark%20code%20are%0Apublicly%20available%20at%20https%3A//github.com/Purdue-M2/AI-Face-FairnessBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00783v2&entry.124074799=Read"},
{"title": "FedDr+: Stabilizing Dot-regression with Global Feature Distillation for\n  Federated Learning", "author": "Seongyoon Kim and Minchan Jeong and Sungnyun Kim and Sungwoo Cho and Sumyeong Ahn and Se-Young Yun", "abstract": "  Federated Learning (FL) has emerged as a pivotal framework for the\ndevelopment of effective global models (global FL) or personalized models\n(personalized FL) across clients with heterogeneous, non-iid data distribution.\nA key challenge in FL is client drift, where data heterogeneity impedes the\naggregation of scattered knowledge. Recent studies have tackled the client\ndrift issue by identifying significant divergence in the last classifier layer.\nTo mitigate this divergence, strategies such as freezing the classifier weights\nand aligning the feature extractor accordingly have proven effective. Although\nthe local alignment between classifier and feature extractor has been studied\nas a crucial factor in FL, we observe that it may lead the model to\noveremphasize the observed classes within each client. Thus, our objectives are\ntwofold: (1) enhancing local alignment while (2) preserving the representation\nof unseen class samples. This approach aims to effectively integrate knowledge\nfrom individual clients, thereby improving performance for both global and\npersonalized FL. To achieve this, we introduce a novel algorithm named FedDr+,\nwhich empowers local model alignment using dot-regression loss. FedDr+ freezes\nthe classifier as a simplex ETF to align the features and improves aggregated\nglobal models by employing a feature distillation mechanism to retain\ninformation about unseen/missing classes. Consequently, we provide empirical\nevidence demonstrating that our algorithm surpasses existing methods that use a\nfrozen classifier to boost alignment across the diverse distribution.\n", "link": "http://arxiv.org/abs/2406.02355v1", "date": "2024-06-04", "relevancy": 1.9923, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5076}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4936}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedDr%2B%3A%20Stabilizing%20Dot-regression%20with%20Global%20Feature%20Distillation%20for%0A%20%20Federated%20Learning&body=Title%3A%20FedDr%2B%3A%20Stabilizing%20Dot-regression%20with%20Global%20Feature%20Distillation%20for%0A%20%20Federated%20Learning%0AAuthor%3A%20Seongyoon%20Kim%20and%20Minchan%20Jeong%20and%20Sungnyun%20Kim%20and%20Sungwoo%20Cho%20and%20Sumyeong%20Ahn%20and%20Se-Young%20Yun%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20pivotal%20framework%20for%20the%0Adevelopment%20of%20effective%20global%20models%20%28global%20FL%29%20or%20personalized%20models%0A%28personalized%20FL%29%20across%20clients%20with%20heterogeneous%2C%20non-iid%20data%20distribution.%0AA%20key%20challenge%20in%20FL%20is%20client%20drift%2C%20where%20data%20heterogeneity%20impedes%20the%0Aaggregation%20of%20scattered%20knowledge.%20Recent%20studies%20have%20tackled%20the%20client%0Adrift%20issue%20by%20identifying%20significant%20divergence%20in%20the%20last%20classifier%20layer.%0ATo%20mitigate%20this%20divergence%2C%20strategies%20such%20as%20freezing%20the%20classifier%20weights%0Aand%20aligning%20the%20feature%20extractor%20accordingly%20have%20proven%20effective.%20Although%0Athe%20local%20alignment%20between%20classifier%20and%20feature%20extractor%20has%20been%20studied%0Aas%20a%20crucial%20factor%20in%20FL%2C%20we%20observe%20that%20it%20may%20lead%20the%20model%20to%0Aoveremphasize%20the%20observed%20classes%20within%20each%20client.%20Thus%2C%20our%20objectives%20are%0Atwofold%3A%20%281%29%20enhancing%20local%20alignment%20while%20%282%29%20preserving%20the%20representation%0Aof%20unseen%20class%20samples.%20This%20approach%20aims%20to%20effectively%20integrate%20knowledge%0Afrom%20individual%20clients%2C%20thereby%20improving%20performance%20for%20both%20global%20and%0Apersonalized%20FL.%20To%20achieve%20this%2C%20we%20introduce%20a%20novel%20algorithm%20named%20FedDr%2B%2C%0Awhich%20empowers%20local%20model%20alignment%20using%20dot-regression%20loss.%20FedDr%2B%20freezes%0Athe%20classifier%20as%20a%20simplex%20ETF%20to%20align%20the%20features%20and%20improves%20aggregated%0Aglobal%20models%20by%20employing%20a%20feature%20distillation%20mechanism%20to%20retain%0Ainformation%20about%20unseen/missing%20classes.%20Consequently%2C%20we%20provide%20empirical%0Aevidence%20demonstrating%20that%20our%20algorithm%20surpasses%20existing%20methods%20that%20use%20a%0Afrozen%20classifier%20to%20boost%20alignment%20across%20the%20diverse%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedDr%252B%253A%2520Stabilizing%2520Dot-regression%2520with%2520Global%2520Feature%2520Distillation%2520for%250A%2520%2520Federated%2520Learning%26entry.906535625%3DSeongyoon%2520Kim%2520and%2520Minchan%2520Jeong%2520and%2520Sungnyun%2520Kim%2520and%2520Sungwoo%2520Cho%2520and%2520Sumyeong%2520Ahn%2520and%2520Se-Young%2520Yun%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520pivotal%2520framework%2520for%2520the%250Adevelopment%2520of%2520effective%2520global%2520models%2520%2528global%2520FL%2529%2520or%2520personalized%2520models%250A%2528personalized%2520FL%2529%2520across%2520clients%2520with%2520heterogeneous%252C%2520non-iid%2520data%2520distribution.%250AA%2520key%2520challenge%2520in%2520FL%2520is%2520client%2520drift%252C%2520where%2520data%2520heterogeneity%2520impedes%2520the%250Aaggregation%2520of%2520scattered%2520knowledge.%2520Recent%2520studies%2520have%2520tackled%2520the%2520client%250Adrift%2520issue%2520by%2520identifying%2520significant%2520divergence%2520in%2520the%2520last%2520classifier%2520layer.%250ATo%2520mitigate%2520this%2520divergence%252C%2520strategies%2520such%2520as%2520freezing%2520the%2520classifier%2520weights%250Aand%2520aligning%2520the%2520feature%2520extractor%2520accordingly%2520have%2520proven%2520effective.%2520Although%250Athe%2520local%2520alignment%2520between%2520classifier%2520and%2520feature%2520extractor%2520has%2520been%2520studied%250Aas%2520a%2520crucial%2520factor%2520in%2520FL%252C%2520we%2520observe%2520that%2520it%2520may%2520lead%2520the%2520model%2520to%250Aoveremphasize%2520the%2520observed%2520classes%2520within%2520each%2520client.%2520Thus%252C%2520our%2520objectives%2520are%250Atwofold%253A%2520%25281%2529%2520enhancing%2520local%2520alignment%2520while%2520%25282%2529%2520preserving%2520the%2520representation%250Aof%2520unseen%2520class%2520samples.%2520This%2520approach%2520aims%2520to%2520effectively%2520integrate%2520knowledge%250Afrom%2520individual%2520clients%252C%2520thereby%2520improving%2520performance%2520for%2520both%2520global%2520and%250Apersonalized%2520FL.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520novel%2520algorithm%2520named%2520FedDr%252B%252C%250Awhich%2520empowers%2520local%2520model%2520alignment%2520using%2520dot-regression%2520loss.%2520FedDr%252B%2520freezes%250Athe%2520classifier%2520as%2520a%2520simplex%2520ETF%2520to%2520align%2520the%2520features%2520and%2520improves%2520aggregated%250Aglobal%2520models%2520by%2520employing%2520a%2520feature%2520distillation%2520mechanism%2520to%2520retain%250Ainformation%2520about%2520unseen/missing%2520classes.%2520Consequently%252C%2520we%2520provide%2520empirical%250Aevidence%2520demonstrating%2520that%2520our%2520algorithm%2520surpasses%2520existing%2520methods%2520that%2520use%2520a%250Afrozen%2520classifier%2520to%2520boost%2520alignment%2520across%2520the%2520diverse%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedDr%2B%3A%20Stabilizing%20Dot-regression%20with%20Global%20Feature%20Distillation%20for%0A%20%20Federated%20Learning&entry.906535625=Seongyoon%20Kim%20and%20Minchan%20Jeong%20and%20Sungnyun%20Kim%20and%20Sungwoo%20Cho%20and%20Sumyeong%20Ahn%20and%20Se-Young%20Yun&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20pivotal%20framework%20for%20the%0Adevelopment%20of%20effective%20global%20models%20%28global%20FL%29%20or%20personalized%20models%0A%28personalized%20FL%29%20across%20clients%20with%20heterogeneous%2C%20non-iid%20data%20distribution.%0AA%20key%20challenge%20in%20FL%20is%20client%20drift%2C%20where%20data%20heterogeneity%20impedes%20the%0Aaggregation%20of%20scattered%20knowledge.%20Recent%20studies%20have%20tackled%20the%20client%0Adrift%20issue%20by%20identifying%20significant%20divergence%20in%20the%20last%20classifier%20layer.%0ATo%20mitigate%20this%20divergence%2C%20strategies%20such%20as%20freezing%20the%20classifier%20weights%0Aand%20aligning%20the%20feature%20extractor%20accordingly%20have%20proven%20effective.%20Although%0Athe%20local%20alignment%20between%20classifier%20and%20feature%20extractor%20has%20been%20studied%0Aas%20a%20crucial%20factor%20in%20FL%2C%20we%20observe%20that%20it%20may%20lead%20the%20model%20to%0Aoveremphasize%20the%20observed%20classes%20within%20each%20client.%20Thus%2C%20our%20objectives%20are%0Atwofold%3A%20%281%29%20enhancing%20local%20alignment%20while%20%282%29%20preserving%20the%20representation%0Aof%20unseen%20class%20samples.%20This%20approach%20aims%20to%20effectively%20integrate%20knowledge%0Afrom%20individual%20clients%2C%20thereby%20improving%20performance%20for%20both%20global%20and%0Apersonalized%20FL.%20To%20achieve%20this%2C%20we%20introduce%20a%20novel%20algorithm%20named%20FedDr%2B%2C%0Awhich%20empowers%20local%20model%20alignment%20using%20dot-regression%20loss.%20FedDr%2B%20freezes%0Athe%20classifier%20as%20a%20simplex%20ETF%20to%20align%20the%20features%20and%20improves%20aggregated%0Aglobal%20models%20by%20employing%20a%20feature%20distillation%20mechanism%20to%20retain%0Ainformation%20about%20unseen/missing%20classes.%20Consequently%2C%20we%20provide%20empirical%0Aevidence%20demonstrating%20that%20our%20algorithm%20surpasses%20existing%20methods%20that%20use%20a%0Afrozen%20classifier%20to%20boost%20alignment%20across%20the%20diverse%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02355v1&entry.124074799=Read"},
{"title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension", "author": "Yijiong Yu and Huiqiang Jiang and Xufang Luo and Qianhui Wu and Chin-Yew Lin and Dongsheng Li and Yuqing Yang and Yongfeng Huang and Lili Qiu", "abstract": "  Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.\n", "link": "http://arxiv.org/abs/2406.02536v1", "date": "2024-06-04", "relevancy": 1.5255, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigate%20Position%20Bias%20in%20Large%20Language%20Models%20via%20Scaling%20a%20Single%0A%20%20Dimension&body=Title%3A%20Mitigate%20Position%20Bias%20in%20Large%20Language%20Models%20via%20Scaling%20a%20Single%0A%20%20Dimension%0AAuthor%3A%20Yijiong%20Yu%20and%20Huiqiang%20Jiang%20and%20Xufang%20Luo%20and%20Qianhui%20Wu%20and%20Chin-Yew%20Lin%20and%20Dongsheng%20Li%20and%20Yuqing%20Yang%20and%20Yongfeng%20Huang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20applied%20in%20various%20real-world%0Ascenarios%20due%20to%20their%20excellent%20generalization%20capabilities%20and%20robust%0Agenerative%20abilities.%20However%2C%20they%20exhibit%20position%20bias%2C%20also%20known%20as%20%22lost%0Ain%20the%20middle%22%2C%20a%20phenomenon%20that%20is%20especially%20pronounced%20in%20long-context%0Ascenarios%2C%20which%20indicates%20the%20placement%20of%20the%20key%20information%20in%20different%0Apositions%20of%20a%20prompt%20can%20significantly%20affect%20accuracy.%20This%20paper%20first%0Aexplores%20the%20micro-level%20manifestations%20of%20position%20bias%2C%20concluding%20that%0Aattention%20weights%20are%20a%20micro-level%20expression%20of%20position%20bias.%20It%20further%0Aidentifies%20that%2C%20in%20addition%20to%20position%20embeddings%2C%20causal%20attention%20mask%20also%0Acontributes%20to%20position%20bias%20by%20creating%20position-specific%20hidden%20states.%20Based%0Aon%20these%20insights%2C%20we%20propose%20a%20method%20to%20mitigate%20position%20bias%20by%20scaling%0Athis%20positional%20hidden%20states.%20Experiments%20on%20the%20NaturalQuestions%0AMulti-document%20QA%2C%20KV%20retrieval%2C%20LongBench%20and%20timeline%20reorder%20tasks%2C%20using%0Avarious%20models%20including%20RoPE%20models%2C%20context%20windowextended%20models%2C%20and%20Alibi%0Amodels%2C%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20our%20approach.%20Our%0Amethod%20can%20improve%20performance%20by%20up%20to%2015.2%25%20by%20modifying%20just%20one%20dimension%0Aof%20hidden%20states.%20Our%20code%20is%20available%20at%20https%3A//aka.ms/PositionalHidden.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigate%2520Position%2520Bias%2520in%2520Large%2520Language%2520Models%2520via%2520Scaling%2520a%2520Single%250A%2520%2520Dimension%26entry.906535625%3DYijiong%2520Yu%2520and%2520Huiqiang%2520Jiang%2520and%2520Xufang%2520Luo%2520and%2520Qianhui%2520Wu%2520and%2520Chin-Yew%2520Lin%2520and%2520Dongsheng%2520Li%2520and%2520Yuqing%2520Yang%2520and%2520Yongfeng%2520Huang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520applied%2520in%2520various%2520real-world%250Ascenarios%2520due%2520to%2520their%2520excellent%2520generalization%2520capabilities%2520and%2520robust%250Agenerative%2520abilities.%2520However%252C%2520they%2520exhibit%2520position%2520bias%252C%2520also%2520known%2520as%2520%2522lost%250Ain%2520the%2520middle%2522%252C%2520a%2520phenomenon%2520that%2520is%2520especially%2520pronounced%2520in%2520long-context%250Ascenarios%252C%2520which%2520indicates%2520the%2520placement%2520of%2520the%2520key%2520information%2520in%2520different%250Apositions%2520of%2520a%2520prompt%2520can%2520significantly%2520affect%2520accuracy.%2520This%2520paper%2520first%250Aexplores%2520the%2520micro-level%2520manifestations%2520of%2520position%2520bias%252C%2520concluding%2520that%250Aattention%2520weights%2520are%2520a%2520micro-level%2520expression%2520of%2520position%2520bias.%2520It%2520further%250Aidentifies%2520that%252C%2520in%2520addition%2520to%2520position%2520embeddings%252C%2520causal%2520attention%2520mask%2520also%250Acontributes%2520to%2520position%2520bias%2520by%2520creating%2520position-specific%2520hidden%2520states.%2520Based%250Aon%2520these%2520insights%252C%2520we%2520propose%2520a%2520method%2520to%2520mitigate%2520position%2520bias%2520by%2520scaling%250Athis%2520positional%2520hidden%2520states.%2520Experiments%2520on%2520the%2520NaturalQuestions%250AMulti-document%2520QA%252C%2520KV%2520retrieval%252C%2520LongBench%2520and%2520timeline%2520reorder%2520tasks%252C%2520using%250Avarious%2520models%2520including%2520RoPE%2520models%252C%2520context%2520windowextended%2520models%252C%2520and%2520Alibi%250Amodels%252C%2520demonstrate%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520our%2520approach.%2520Our%250Amethod%2520can%2520improve%2520performance%2520by%2520up%2520to%252015.2%2525%2520by%2520modifying%2520just%2520one%2520dimension%250Aof%2520hidden%2520states.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//aka.ms/PositionalHidden.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigate%20Position%20Bias%20in%20Large%20Language%20Models%20via%20Scaling%20a%20Single%0A%20%20Dimension&entry.906535625=Yijiong%20Yu%20and%20Huiqiang%20Jiang%20and%20Xufang%20Luo%20and%20Qianhui%20Wu%20and%20Chin-Yew%20Lin%20and%20Dongsheng%20Li%20and%20Yuqing%20Yang%20and%20Yongfeng%20Huang%20and%20Lili%20Qiu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20applied%20in%20various%20real-world%0Ascenarios%20due%20to%20their%20excellent%20generalization%20capabilities%20and%20robust%0Agenerative%20abilities.%20However%2C%20they%20exhibit%20position%20bias%2C%20also%20known%20as%20%22lost%0Ain%20the%20middle%22%2C%20a%20phenomenon%20that%20is%20especially%20pronounced%20in%20long-context%0Ascenarios%2C%20which%20indicates%20the%20placement%20of%20the%20key%20information%20in%20different%0Apositions%20of%20a%20prompt%20can%20significantly%20affect%20accuracy.%20This%20paper%20first%0Aexplores%20the%20micro-level%20manifestations%20of%20position%20bias%2C%20concluding%20that%0Aattention%20weights%20are%20a%20micro-level%20expression%20of%20position%20bias.%20It%20further%0Aidentifies%20that%2C%20in%20addition%20to%20position%20embeddings%2C%20causal%20attention%20mask%20also%0Acontributes%20to%20position%20bias%20by%20creating%20position-specific%20hidden%20states.%20Based%0Aon%20these%20insights%2C%20we%20propose%20a%20method%20to%20mitigate%20position%20bias%20by%20scaling%0Athis%20positional%20hidden%20states.%20Experiments%20on%20the%20NaturalQuestions%0AMulti-document%20QA%2C%20KV%20retrieval%2C%20LongBench%20and%20timeline%20reorder%20tasks%2C%20using%0Avarious%20models%20including%20RoPE%20models%2C%20context%20windowextended%20models%2C%20and%20Alibi%0Amodels%2C%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20our%20approach.%20Our%0Amethod%20can%20improve%20performance%20by%20up%20to%2015.2%25%20by%20modifying%20just%20one%20dimension%0Aof%20hidden%20states.%20Our%20code%20is%20available%20at%20https%3A//aka.ms/PositionalHidden.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02536v1&entry.124074799=Read"},
{"title": "Trust the Model Where It Trusts Itself -- Model-Based Actor-Critic with\n  Uncertainty-Aware Rollout Adaption", "author": "Bernd Frauenknecht and Artur Eisele and Devdutt Subhasish and Friedrich Solowjow and Sebastian Trimpe", "abstract": "  Dyna-style model-based reinforcement learning (MBRL) combines model-free\nagents with predictive transition models through model-based rollouts. This\ncombination raises a critical question: 'When to trust your model?'; i.e.,\nwhich rollout length results in the model providing useful data? Janner et al.\n(2019) address this question by gradually increasing rollout lengths throughout\nthe training. While theoretically tempting, uniform model accuracy is a fallacy\nthat collapses at the latest when extrapolating. Instead, we propose asking the\nquestion 'Where to trust your model?'. Using inherent model uncertainty to\nconsider local accuracy, we obtain the Model-Based Actor-Critic with\nUncertainty-Aware Rollout Adaption (MACURA) algorithm. We propose an\neasy-to-tune rollout mechanism and demonstrate substantial improvements in data\nefficiency and performance compared to state-of-the-art deep MBRL methods on\nthe MuJoCo benchmark.\n", "link": "http://arxiv.org/abs/2405.19014v2", "date": "2024-06-04", "relevancy": 1.5613, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5374}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5165}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trust%20the%20Model%20Where%20It%20Trusts%20Itself%20--%20Model-Based%20Actor-Critic%20with%0A%20%20Uncertainty-Aware%20Rollout%20Adaption&body=Title%3A%20Trust%20the%20Model%20Where%20It%20Trusts%20Itself%20--%20Model-Based%20Actor-Critic%20with%0A%20%20Uncertainty-Aware%20Rollout%20Adaption%0AAuthor%3A%20Bernd%20Frauenknecht%20and%20Artur%20Eisele%20and%20Devdutt%20Subhasish%20and%20Friedrich%20Solowjow%20and%20Sebastian%20Trimpe%0AAbstract%3A%20%20%20Dyna-style%20model-based%20reinforcement%20learning%20%28MBRL%29%20combines%20model-free%0Aagents%20with%20predictive%20transition%20models%20through%20model-based%20rollouts.%20This%0Acombination%20raises%20a%20critical%20question%3A%20%27When%20to%20trust%20your%20model%3F%27%3B%20i.e.%2C%0Awhich%20rollout%20length%20results%20in%20the%20model%20providing%20useful%20data%3F%20Janner%20et%20al.%0A%282019%29%20address%20this%20question%20by%20gradually%20increasing%20rollout%20lengths%20throughout%0Athe%20training.%20While%20theoretically%20tempting%2C%20uniform%20model%20accuracy%20is%20a%20fallacy%0Athat%20collapses%20at%20the%20latest%20when%20extrapolating.%20Instead%2C%20we%20propose%20asking%20the%0Aquestion%20%27Where%20to%20trust%20your%20model%3F%27.%20Using%20inherent%20model%20uncertainty%20to%0Aconsider%20local%20accuracy%2C%20we%20obtain%20the%20Model-Based%20Actor-Critic%20with%0AUncertainty-Aware%20Rollout%20Adaption%20%28MACURA%29%20algorithm.%20We%20propose%20an%0Aeasy-to-tune%20rollout%20mechanism%20and%20demonstrate%20substantial%20improvements%20in%20data%0Aefficiency%20and%20performance%20compared%20to%20state-of-the-art%20deep%20MBRL%20methods%20on%0Athe%20MuJoCo%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19014v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrust%2520the%2520Model%2520Where%2520It%2520Trusts%2520Itself%2520--%2520Model-Based%2520Actor-Critic%2520with%250A%2520%2520Uncertainty-Aware%2520Rollout%2520Adaption%26entry.906535625%3DBernd%2520Frauenknecht%2520and%2520Artur%2520Eisele%2520and%2520Devdutt%2520Subhasish%2520and%2520Friedrich%2520Solowjow%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3D%2520%2520Dyna-style%2520model-based%2520reinforcement%2520learning%2520%2528MBRL%2529%2520combines%2520model-free%250Aagents%2520with%2520predictive%2520transition%2520models%2520through%2520model-based%2520rollouts.%2520This%250Acombination%2520raises%2520a%2520critical%2520question%253A%2520%2527When%2520to%2520trust%2520your%2520model%253F%2527%253B%2520i.e.%252C%250Awhich%2520rollout%2520length%2520results%2520in%2520the%2520model%2520providing%2520useful%2520data%253F%2520Janner%2520et%2520al.%250A%25282019%2529%2520address%2520this%2520question%2520by%2520gradually%2520increasing%2520rollout%2520lengths%2520throughout%250Athe%2520training.%2520While%2520theoretically%2520tempting%252C%2520uniform%2520model%2520accuracy%2520is%2520a%2520fallacy%250Athat%2520collapses%2520at%2520the%2520latest%2520when%2520extrapolating.%2520Instead%252C%2520we%2520propose%2520asking%2520the%250Aquestion%2520%2527Where%2520to%2520trust%2520your%2520model%253F%2527.%2520Using%2520inherent%2520model%2520uncertainty%2520to%250Aconsider%2520local%2520accuracy%252C%2520we%2520obtain%2520the%2520Model-Based%2520Actor-Critic%2520with%250AUncertainty-Aware%2520Rollout%2520Adaption%2520%2528MACURA%2529%2520algorithm.%2520We%2520propose%2520an%250Aeasy-to-tune%2520rollout%2520mechanism%2520and%2520demonstrate%2520substantial%2520improvements%2520in%2520data%250Aefficiency%2520and%2520performance%2520compared%2520to%2520state-of-the-art%2520deep%2520MBRL%2520methods%2520on%250Athe%2520MuJoCo%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19014v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%20the%20Model%20Where%20It%20Trusts%20Itself%20--%20Model-Based%20Actor-Critic%20with%0A%20%20Uncertainty-Aware%20Rollout%20Adaption&entry.906535625=Bernd%20Frauenknecht%20and%20Artur%20Eisele%20and%20Devdutt%20Subhasish%20and%20Friedrich%20Solowjow%20and%20Sebastian%20Trimpe&entry.1292438233=%20%20Dyna-style%20model-based%20reinforcement%20learning%20%28MBRL%29%20combines%20model-free%0Aagents%20with%20predictive%20transition%20models%20through%20model-based%20rollouts.%20This%0Acombination%20raises%20a%20critical%20question%3A%20%27When%20to%20trust%20your%20model%3F%27%3B%20i.e.%2C%0Awhich%20rollout%20length%20results%20in%20the%20model%20providing%20useful%20data%3F%20Janner%20et%20al.%0A%282019%29%20address%20this%20question%20by%20gradually%20increasing%20rollout%20lengths%20throughout%0Athe%20training.%20While%20theoretically%20tempting%2C%20uniform%20model%20accuracy%20is%20a%20fallacy%0Athat%20collapses%20at%20the%20latest%20when%20extrapolating.%20Instead%2C%20we%20propose%20asking%20the%0Aquestion%20%27Where%20to%20trust%20your%20model%3F%27.%20Using%20inherent%20model%20uncertainty%20to%0Aconsider%20local%20accuracy%2C%20we%20obtain%20the%20Model-Based%20Actor-Critic%20with%0AUncertainty-Aware%20Rollout%20Adaption%20%28MACURA%29%20algorithm.%20We%20propose%20an%0Aeasy-to-tune%20rollout%20mechanism%20and%20demonstrate%20substantial%20improvements%20in%20data%0Aefficiency%20and%20performance%20compared%20to%20state-of-the-art%20deep%20MBRL%20methods%20on%0Athe%20MuJoCo%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19014v2&entry.124074799=Read"},
{"title": "An Open and Reconfigurable User Interface to Manage Complex ROS-based\n  Robotic Systems", "author": "Pablo Malvido Fresnillo and Saigopal Vasudevan and Jose A. Perez Garcia and Jose L. Martinez Lastra", "abstract": "  The Robot Operating System (ROS) has significantly gained popularity among\nrobotic engineers and researchers over the past five years, primarily due to\nits powerful infrastructure for node communication, which enables developers to\nbuild modular and large robotic applications. However, ROS presents a steep\nlearning curve and lacks the intuitive usability of vendor-specific robotic\nGraphical User Interfaces (GUIs). Moreover, its modular and distributed nature\ncomplicates the control and monitoring of extensive systems, even for advanced\nusers. To address these challenges, this paper proposes a highly adaptable and\nreconfigurable web-based GUI for intuitively controlling, monitoring, and\nconfiguring complex ROS-based robotic systems. The GUI leverages ROSBridge and\nroslibjs to ensure seamless communication with ROS systems via topics and\nservices. Designed as a versatile platform, the GUI allows for the selective\nincorporation of modular features to accommodate diverse robotic systems and\napplications. An initial set of commonly used features in robotic applications\nis presented. To demonstrate its reconfigurability, the GUI was customized and\ntested for four industrial use cases, receiving positive feedback. The\nproject's repository has been made publicly available to support the robotics\ncommunity and lower the entry barrier for ROS in industrial applications.\n", "link": "http://arxiv.org/abs/2406.02210v1", "date": "2024-06-04", "relevancy": 1.6092, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5896}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5281}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Open%20and%20Reconfigurable%20User%20Interface%20to%20Manage%20Complex%20ROS-based%0A%20%20Robotic%20Systems&body=Title%3A%20An%20Open%20and%20Reconfigurable%20User%20Interface%20to%20Manage%20Complex%20ROS-based%0A%20%20Robotic%20Systems%0AAuthor%3A%20Pablo%20Malvido%20Fresnillo%20and%20Saigopal%20Vasudevan%20and%20Jose%20A.%20Perez%20Garcia%20and%20Jose%20L.%20Martinez%20Lastra%0AAbstract%3A%20%20%20The%20Robot%20Operating%20System%20%28ROS%29%20has%20significantly%20gained%20popularity%20among%0Arobotic%20engineers%20and%20researchers%20over%20the%20past%20five%20years%2C%20primarily%20due%20to%0Aits%20powerful%20infrastructure%20for%20node%20communication%2C%20which%20enables%20developers%20to%0Abuild%20modular%20and%20large%20robotic%20applications.%20However%2C%20ROS%20presents%20a%20steep%0Alearning%20curve%20and%20lacks%20the%20intuitive%20usability%20of%20vendor-specific%20robotic%0AGraphical%20User%20Interfaces%20%28GUIs%29.%20Moreover%2C%20its%20modular%20and%20distributed%20nature%0Acomplicates%20the%20control%20and%20monitoring%20of%20extensive%20systems%2C%20even%20for%20advanced%0Ausers.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20highly%20adaptable%20and%0Areconfigurable%20web-based%20GUI%20for%20intuitively%20controlling%2C%20monitoring%2C%20and%0Aconfiguring%20complex%20ROS-based%20robotic%20systems.%20The%20GUI%20leverages%20ROSBridge%20and%0Aroslibjs%20to%20ensure%20seamless%20communication%20with%20ROS%20systems%20via%20topics%20and%0Aservices.%20Designed%20as%20a%20versatile%20platform%2C%20the%20GUI%20allows%20for%20the%20selective%0Aincorporation%20of%20modular%20features%20to%20accommodate%20diverse%20robotic%20systems%20and%0Aapplications.%20An%20initial%20set%20of%20commonly%20used%20features%20in%20robotic%20applications%0Ais%20presented.%20To%20demonstrate%20its%20reconfigurability%2C%20the%20GUI%20was%20customized%20and%0Atested%20for%20four%20industrial%20use%20cases%2C%20receiving%20positive%20feedback.%20The%0Aproject%27s%20repository%20has%20been%20made%20publicly%20available%20to%20support%20the%20robotics%0Acommunity%20and%20lower%20the%20entry%20barrier%20for%20ROS%20in%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Open%2520and%2520Reconfigurable%2520User%2520Interface%2520to%2520Manage%2520Complex%2520ROS-based%250A%2520%2520Robotic%2520Systems%26entry.906535625%3DPablo%2520Malvido%2520Fresnillo%2520and%2520Saigopal%2520Vasudevan%2520and%2520Jose%2520A.%2520Perez%2520Garcia%2520and%2520Jose%2520L.%2520Martinez%2520Lastra%26entry.1292438233%3D%2520%2520The%2520Robot%2520Operating%2520System%2520%2528ROS%2529%2520has%2520significantly%2520gained%2520popularity%2520among%250Arobotic%2520engineers%2520and%2520researchers%2520over%2520the%2520past%2520five%2520years%252C%2520primarily%2520due%2520to%250Aits%2520powerful%2520infrastructure%2520for%2520node%2520communication%252C%2520which%2520enables%2520developers%2520to%250Abuild%2520modular%2520and%2520large%2520robotic%2520applications.%2520However%252C%2520ROS%2520presents%2520a%2520steep%250Alearning%2520curve%2520and%2520lacks%2520the%2520intuitive%2520usability%2520of%2520vendor-specific%2520robotic%250AGraphical%2520User%2520Interfaces%2520%2528GUIs%2529.%2520Moreover%252C%2520its%2520modular%2520and%2520distributed%2520nature%250Acomplicates%2520the%2520control%2520and%2520monitoring%2520of%2520extensive%2520systems%252C%2520even%2520for%2520advanced%250Ausers.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520highly%2520adaptable%2520and%250Areconfigurable%2520web-based%2520GUI%2520for%2520intuitively%2520controlling%252C%2520monitoring%252C%2520and%250Aconfiguring%2520complex%2520ROS-based%2520robotic%2520systems.%2520The%2520GUI%2520leverages%2520ROSBridge%2520and%250Aroslibjs%2520to%2520ensure%2520seamless%2520communication%2520with%2520ROS%2520systems%2520via%2520topics%2520and%250Aservices.%2520Designed%2520as%2520a%2520versatile%2520platform%252C%2520the%2520GUI%2520allows%2520for%2520the%2520selective%250Aincorporation%2520of%2520modular%2520features%2520to%2520accommodate%2520diverse%2520robotic%2520systems%2520and%250Aapplications.%2520An%2520initial%2520set%2520of%2520commonly%2520used%2520features%2520in%2520robotic%2520applications%250Ais%2520presented.%2520To%2520demonstrate%2520its%2520reconfigurability%252C%2520the%2520GUI%2520was%2520customized%2520and%250Atested%2520for%2520four%2520industrial%2520use%2520cases%252C%2520receiving%2520positive%2520feedback.%2520The%250Aproject%2527s%2520repository%2520has%2520been%2520made%2520publicly%2520available%2520to%2520support%2520the%2520robotics%250Acommunity%2520and%2520lower%2520the%2520entry%2520barrier%2520for%2520ROS%2520in%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Open%20and%20Reconfigurable%20User%20Interface%20to%20Manage%20Complex%20ROS-based%0A%20%20Robotic%20Systems&entry.906535625=Pablo%20Malvido%20Fresnillo%20and%20Saigopal%20Vasudevan%20and%20Jose%20A.%20Perez%20Garcia%20and%20Jose%20L.%20Martinez%20Lastra&entry.1292438233=%20%20The%20Robot%20Operating%20System%20%28ROS%29%20has%20significantly%20gained%20popularity%20among%0Arobotic%20engineers%20and%20researchers%20over%20the%20past%20five%20years%2C%20primarily%20due%20to%0Aits%20powerful%20infrastructure%20for%20node%20communication%2C%20which%20enables%20developers%20to%0Abuild%20modular%20and%20large%20robotic%20applications.%20However%2C%20ROS%20presents%20a%20steep%0Alearning%20curve%20and%20lacks%20the%20intuitive%20usability%20of%20vendor-specific%20robotic%0AGraphical%20User%20Interfaces%20%28GUIs%29.%20Moreover%2C%20its%20modular%20and%20distributed%20nature%0Acomplicates%20the%20control%20and%20monitoring%20of%20extensive%20systems%2C%20even%20for%20advanced%0Ausers.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20highly%20adaptable%20and%0Areconfigurable%20web-based%20GUI%20for%20intuitively%20controlling%2C%20monitoring%2C%20and%0Aconfiguring%20complex%20ROS-based%20robotic%20systems.%20The%20GUI%20leverages%20ROSBridge%20and%0Aroslibjs%20to%20ensure%20seamless%20communication%20with%20ROS%20systems%20via%20topics%20and%0Aservices.%20Designed%20as%20a%20versatile%20platform%2C%20the%20GUI%20allows%20for%20the%20selective%0Aincorporation%20of%20modular%20features%20to%20accommodate%20diverse%20robotic%20systems%20and%0Aapplications.%20An%20initial%20set%20of%20commonly%20used%20features%20in%20robotic%20applications%0Ais%20presented.%20To%20demonstrate%20its%20reconfigurability%2C%20the%20GUI%20was%20customized%20and%0Atested%20for%20four%20industrial%20use%20cases%2C%20receiving%20positive%20feedback.%20The%0Aproject%27s%20repository%20has%20been%20made%20publicly%20available%20to%20support%20the%20robotics%0Acommunity%20and%20lower%20the%20entry%20barrier%20for%20ROS%20in%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02210v1&entry.124074799=Read"},
{"title": "Rectifying Reinforcement Learning for Reward Matching", "author": "Haoran He and Emmanuel Bengio and Qingpeng Cai and Ling Pan", "abstract": "  The Generative Flow Network (GFlowNet) is a probabilistic framework in which\nan agent learns a stochastic policy and flow functions to sample objects with\nprobability proportional to an unnormalized reward function. GFlowNets share a\nstrong resemblance to reinforcement learning (RL), that typically aims to\nmaximize reward, due to their sequential decision-making processes. Recent\nworks have studied connections between GFlowNets and maximum entropy (MaxEnt)\nRL, which modifies the standard objective of RL agents by learning an\nentropy-regularized objective. However, a critical theoretical gap persists:\ndespite the apparent similarities in their sequential decision-making nature, a\ndirect link between GFlowNets and standard RL has yet to be discovered, while\nbridging this gap could further unlock the potential of both fields. In this\npaper, we establish a new connection between GFlowNets and policy evaluation\nfor a uniform policy. Surprisingly, we find that the resulting value function\nfor the uniform policy has a close relationship to the flows in GFlowNets.\nLeveraging these insights, we further propose a novel rectified policy\nevaluation (RPE) algorithm, which achieves the same reward-matching effect as\nGFlowNets, offering a new perspective. We compare RPE, MaxEnt RL, and GFlowNets\nin a number of benchmarks, and show that RPE achieves competitive results\ncompared to previous approaches. This work sheds light on the previously\nunexplored connection between (non-MaxEnt) RL and GFlowNets, potentially\nopening new avenues for future research in both fields.\n", "link": "http://arxiv.org/abs/2406.02213v1", "date": "2024-06-04", "relevancy": 1.8818, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.549}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4633}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rectifying%20Reinforcement%20Learning%20for%20Reward%20Matching&body=Title%3A%20Rectifying%20Reinforcement%20Learning%20for%20Reward%20Matching%0AAuthor%3A%20Haoran%20He%20and%20Emmanuel%20Bengio%20and%20Qingpeng%20Cai%20and%20Ling%20Pan%0AAbstract%3A%20%20%20The%20Generative%20Flow%20Network%20%28GFlowNet%29%20is%20a%20probabilistic%20framework%20in%20which%0Aan%20agent%20learns%20a%20stochastic%20policy%20and%20flow%20functions%20to%20sample%20objects%20with%0Aprobability%20proportional%20to%20an%20unnormalized%20reward%20function.%20GFlowNets%20share%20a%0Astrong%20resemblance%20to%20reinforcement%20learning%20%28RL%29%2C%20that%20typically%20aims%20to%0Amaximize%20reward%2C%20due%20to%20their%20sequential%20decision-making%20processes.%20Recent%0Aworks%20have%20studied%20connections%20between%20GFlowNets%20and%20maximum%20entropy%20%28MaxEnt%29%0ARL%2C%20which%20modifies%20the%20standard%20objective%20of%20RL%20agents%20by%20learning%20an%0Aentropy-regularized%20objective.%20However%2C%20a%20critical%20theoretical%20gap%20persists%3A%0Adespite%20the%20apparent%20similarities%20in%20their%20sequential%20decision-making%20nature%2C%20a%0Adirect%20link%20between%20GFlowNets%20and%20standard%20RL%20has%20yet%20to%20be%20discovered%2C%20while%0Abridging%20this%20gap%20could%20further%20unlock%20the%20potential%20of%20both%20fields.%20In%20this%0Apaper%2C%20we%20establish%20a%20new%20connection%20between%20GFlowNets%20and%20policy%20evaluation%0Afor%20a%20uniform%20policy.%20Surprisingly%2C%20we%20find%20that%20the%20resulting%20value%20function%0Afor%20the%20uniform%20policy%20has%20a%20close%20relationship%20to%20the%20flows%20in%20GFlowNets.%0ALeveraging%20these%20insights%2C%20we%20further%20propose%20a%20novel%20rectified%20policy%0Aevaluation%20%28RPE%29%20algorithm%2C%20which%20achieves%20the%20same%20reward-matching%20effect%20as%0AGFlowNets%2C%20offering%20a%20new%20perspective.%20We%20compare%20RPE%2C%20MaxEnt%20RL%2C%20and%20GFlowNets%0Ain%20a%20number%20of%20benchmarks%2C%20and%20show%20that%20RPE%20achieves%20competitive%20results%0Acompared%20to%20previous%20approaches.%20This%20work%20sheds%20light%20on%20the%20previously%0Aunexplored%20connection%20between%20%28non-MaxEnt%29%20RL%20and%20GFlowNets%2C%20potentially%0Aopening%20new%20avenues%20for%20future%20research%20in%20both%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRectifying%2520Reinforcement%2520Learning%2520for%2520Reward%2520Matching%26entry.906535625%3DHaoran%2520He%2520and%2520Emmanuel%2520Bengio%2520and%2520Qingpeng%2520Cai%2520and%2520Ling%2520Pan%26entry.1292438233%3D%2520%2520The%2520Generative%2520Flow%2520Network%2520%2528GFlowNet%2529%2520is%2520a%2520probabilistic%2520framework%2520in%2520which%250Aan%2520agent%2520learns%2520a%2520stochastic%2520policy%2520and%2520flow%2520functions%2520to%2520sample%2520objects%2520with%250Aprobability%2520proportional%2520to%2520an%2520unnormalized%2520reward%2520function.%2520GFlowNets%2520share%2520a%250Astrong%2520resemblance%2520to%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520that%2520typically%2520aims%2520to%250Amaximize%2520reward%252C%2520due%2520to%2520their%2520sequential%2520decision-making%2520processes.%2520Recent%250Aworks%2520have%2520studied%2520connections%2520between%2520GFlowNets%2520and%2520maximum%2520entropy%2520%2528MaxEnt%2529%250ARL%252C%2520which%2520modifies%2520the%2520standard%2520objective%2520of%2520RL%2520agents%2520by%2520learning%2520an%250Aentropy-regularized%2520objective.%2520However%252C%2520a%2520critical%2520theoretical%2520gap%2520persists%253A%250Adespite%2520the%2520apparent%2520similarities%2520in%2520their%2520sequential%2520decision-making%2520nature%252C%2520a%250Adirect%2520link%2520between%2520GFlowNets%2520and%2520standard%2520RL%2520has%2520yet%2520to%2520be%2520discovered%252C%2520while%250Abridging%2520this%2520gap%2520could%2520further%2520unlock%2520the%2520potential%2520of%2520both%2520fields.%2520In%2520this%250Apaper%252C%2520we%2520establish%2520a%2520new%2520connection%2520between%2520GFlowNets%2520and%2520policy%2520evaluation%250Afor%2520a%2520uniform%2520policy.%2520Surprisingly%252C%2520we%2520find%2520that%2520the%2520resulting%2520value%2520function%250Afor%2520the%2520uniform%2520policy%2520has%2520a%2520close%2520relationship%2520to%2520the%2520flows%2520in%2520GFlowNets.%250ALeveraging%2520these%2520insights%252C%2520we%2520further%2520propose%2520a%2520novel%2520rectified%2520policy%250Aevaluation%2520%2528RPE%2529%2520algorithm%252C%2520which%2520achieves%2520the%2520same%2520reward-matching%2520effect%2520as%250AGFlowNets%252C%2520offering%2520a%2520new%2520perspective.%2520We%2520compare%2520RPE%252C%2520MaxEnt%2520RL%252C%2520and%2520GFlowNets%250Ain%2520a%2520number%2520of%2520benchmarks%252C%2520and%2520show%2520that%2520RPE%2520achieves%2520competitive%2520results%250Acompared%2520to%2520previous%2520approaches.%2520This%2520work%2520sheds%2520light%2520on%2520the%2520previously%250Aunexplored%2520connection%2520between%2520%2528non-MaxEnt%2529%2520RL%2520and%2520GFlowNets%252C%2520potentially%250Aopening%2520new%2520avenues%2520for%2520future%2520research%2520in%2520both%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rectifying%20Reinforcement%20Learning%20for%20Reward%20Matching&entry.906535625=Haoran%20He%20and%20Emmanuel%20Bengio%20and%20Qingpeng%20Cai%20and%20Ling%20Pan&entry.1292438233=%20%20The%20Generative%20Flow%20Network%20%28GFlowNet%29%20is%20a%20probabilistic%20framework%20in%20which%0Aan%20agent%20learns%20a%20stochastic%20policy%20and%20flow%20functions%20to%20sample%20objects%20with%0Aprobability%20proportional%20to%20an%20unnormalized%20reward%20function.%20GFlowNets%20share%20a%0Astrong%20resemblance%20to%20reinforcement%20learning%20%28RL%29%2C%20that%20typically%20aims%20to%0Amaximize%20reward%2C%20due%20to%20their%20sequential%20decision-making%20processes.%20Recent%0Aworks%20have%20studied%20connections%20between%20GFlowNets%20and%20maximum%20entropy%20%28MaxEnt%29%0ARL%2C%20which%20modifies%20the%20standard%20objective%20of%20RL%20agents%20by%20learning%20an%0Aentropy-regularized%20objective.%20However%2C%20a%20critical%20theoretical%20gap%20persists%3A%0Adespite%20the%20apparent%20similarities%20in%20their%20sequential%20decision-making%20nature%2C%20a%0Adirect%20link%20between%20GFlowNets%20and%20standard%20RL%20has%20yet%20to%20be%20discovered%2C%20while%0Abridging%20this%20gap%20could%20further%20unlock%20the%20potential%20of%20both%20fields.%20In%20this%0Apaper%2C%20we%20establish%20a%20new%20connection%20between%20GFlowNets%20and%20policy%20evaluation%0Afor%20a%20uniform%20policy.%20Surprisingly%2C%20we%20find%20that%20the%20resulting%20value%20function%0Afor%20the%20uniform%20policy%20has%20a%20close%20relationship%20to%20the%20flows%20in%20GFlowNets.%0ALeveraging%20these%20insights%2C%20we%20further%20propose%20a%20novel%20rectified%20policy%0Aevaluation%20%28RPE%29%20algorithm%2C%20which%20achieves%20the%20same%20reward-matching%20effect%20as%0AGFlowNets%2C%20offering%20a%20new%20perspective.%20We%20compare%20RPE%2C%20MaxEnt%20RL%2C%20and%20GFlowNets%0Ain%20a%20number%20of%20benchmarks%2C%20and%20show%20that%20RPE%20achieves%20competitive%20results%0Acompared%20to%20previous%20approaches.%20This%20work%20sheds%20light%20on%20the%20previously%0Aunexplored%20connection%20between%20%28non-MaxEnt%29%20RL%20and%20GFlowNets%2C%20potentially%0Aopening%20new%20avenues%20for%20future%20research%20in%20both%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02213v1&entry.124074799=Read"},
{"title": "A Framework for Neurosymbolic Robot Action Planning using Large Language\n  Models", "author": "Alessio Capitanelli and Fulvio Mastrogiovanni", "abstract": "  Symbolic task planning is a widely used approach to enforce robot autonomy\ndue to its ease of understanding and deployment in robot architectures.\nHowever, techniques for symbolic task planning are difficult to scale in\nreal-world, human-robot collaboration scenarios because of the poor performance\nin complex planning domains or when frequent re-planning is needed. We present\na framework, Teriyaki, specifically aimed at bridging the gap between symbolic\ntask planning and machine learning approaches. The rationale is training Large\nLanguage Models (LLMs), namely GPT-3, into a neurosymbolic task planner\ncompatible with the Planning Domain Definition Language (PDDL), and then\nleveraging its generative capabilities to overcome a number of limitations\ninherent to symbolic task planners. Potential benefits include (i) a better\nscalability in so far as the planning domain complexity increases, since LLMs'\nresponse time linearly scales with the combined length of the input and the\noutput, and (ii) the ability to synthesize a plan action-by-action instead of\nend-to-end, making each action available for execution as soon as it is\ngenerated instead of waiting for the whole plan to be available, which in turn\nenables concurrent planning and execution. Recently, significant efforts have\nbeen devoted by the research community to evaluate the cognitive capabilities\nof LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an\noverall planning performance comparable to traditional planners in specific\nplanning domains, while leveraging LLMs capabilities to build a look-ahead\npredictive planning model. Preliminary results in selected domains show that\nour method can: (i) solve 95.5% of problems in a test data set of 1,000\nsamples; (ii) produce plans up to 13.5% shorter than a traditional symbolic\nplanner; (iii) reduce average overall waiting times for a plan availability by\nup to 61.4%\n", "link": "http://arxiv.org/abs/2303.00438v3", "date": "2024-06-04", "relevancy": 1.5301, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5678}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Neurosymbolic%20Robot%20Action%20Planning%20using%20Large%20Language%0A%20%20Models&body=Title%3A%20A%20Framework%20for%20Neurosymbolic%20Robot%20Action%20Planning%20using%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Alessio%20Capitanelli%20and%20Fulvio%20Mastrogiovanni%0AAbstract%3A%20%20%20Symbolic%20task%20planning%20is%20a%20widely%20used%20approach%20to%20enforce%20robot%20autonomy%0Adue%20to%20its%20ease%20of%20understanding%20and%20deployment%20in%20robot%20architectures.%0AHowever%2C%20techniques%20for%20symbolic%20task%20planning%20are%20difficult%20to%20scale%20in%0Areal-world%2C%20human-robot%20collaboration%20scenarios%20because%20of%20the%20poor%20performance%0Ain%20complex%20planning%20domains%20or%20when%20frequent%20re-planning%20is%20needed.%20We%20present%0Aa%20framework%2C%20Teriyaki%2C%20specifically%20aimed%20at%20bridging%20the%20gap%20between%20symbolic%0Atask%20planning%20and%20machine%20learning%20approaches.%20The%20rationale%20is%20training%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20namely%20GPT-3%2C%20into%20a%20neurosymbolic%20task%20planner%0Acompatible%20with%20the%20Planning%20Domain%20Definition%20Language%20%28PDDL%29%2C%20and%20then%0Aleveraging%20its%20generative%20capabilities%20to%20overcome%20a%20number%20of%20limitations%0Ainherent%20to%20symbolic%20task%20planners.%20Potential%20benefits%20include%20%28i%29%20a%20better%0Ascalability%20in%20so%20far%20as%20the%20planning%20domain%20complexity%20increases%2C%20since%20LLMs%27%0Aresponse%20time%20linearly%20scales%20with%20the%20combined%20length%20of%20the%20input%20and%20the%0Aoutput%2C%20and%20%28ii%29%20the%20ability%20to%20synthesize%20a%20plan%20action-by-action%20instead%20of%0Aend-to-end%2C%20making%20each%20action%20available%20for%20execution%20as%20soon%20as%20it%20is%0Agenerated%20instead%20of%20waiting%20for%20the%20whole%20plan%20to%20be%20available%2C%20which%20in%20turn%0Aenables%20concurrent%20planning%20and%20execution.%20Recently%2C%20significant%20efforts%20have%0Abeen%20devoted%20by%20the%20research%20community%20to%20evaluate%20the%20cognitive%20capabilities%0Aof%20LLMs%2C%20with%20alternate%20successes.%20Instead%2C%20with%20Teriyaki%20we%20aim%20to%20provide%20an%0Aoverall%20planning%20performance%20comparable%20to%20traditional%20planners%20in%20specific%0Aplanning%20domains%2C%20while%20leveraging%20LLMs%20capabilities%20to%20build%20a%20look-ahead%0Apredictive%20planning%20model.%20Preliminary%20results%20in%20selected%20domains%20show%20that%0Aour%20method%20can%3A%20%28i%29%20solve%2095.5%25%20of%20problems%20in%20a%20test%20data%20set%20of%201%2C000%0Asamples%3B%20%28ii%29%20produce%20plans%20up%20to%2013.5%25%20shorter%20than%20a%20traditional%20symbolic%0Aplanner%3B%20%28iii%29%20reduce%20average%20overall%20waiting%20times%20for%20a%20plan%20availability%20by%0Aup%20to%2061.4%25%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.00438v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Neurosymbolic%2520Robot%2520Action%2520Planning%2520using%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DAlessio%2520Capitanelli%2520and%2520Fulvio%2520Mastrogiovanni%26entry.1292438233%3D%2520%2520Symbolic%2520task%2520planning%2520is%2520a%2520widely%2520used%2520approach%2520to%2520enforce%2520robot%2520autonomy%250Adue%2520to%2520its%2520ease%2520of%2520understanding%2520and%2520deployment%2520in%2520robot%2520architectures.%250AHowever%252C%2520techniques%2520for%2520symbolic%2520task%2520planning%2520are%2520difficult%2520to%2520scale%2520in%250Areal-world%252C%2520human-robot%2520collaboration%2520scenarios%2520because%2520of%2520the%2520poor%2520performance%250Ain%2520complex%2520planning%2520domains%2520or%2520when%2520frequent%2520re-planning%2520is%2520needed.%2520We%2520present%250Aa%2520framework%252C%2520Teriyaki%252C%2520specifically%2520aimed%2520at%2520bridging%2520the%2520gap%2520between%2520symbolic%250Atask%2520planning%2520and%2520machine%2520learning%2520approaches.%2520The%2520rationale%2520is%2520training%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520namely%2520GPT-3%252C%2520into%2520a%2520neurosymbolic%2520task%2520planner%250Acompatible%2520with%2520the%2520Planning%2520Domain%2520Definition%2520Language%2520%2528PDDL%2529%252C%2520and%2520then%250Aleveraging%2520its%2520generative%2520capabilities%2520to%2520overcome%2520a%2520number%2520of%2520limitations%250Ainherent%2520to%2520symbolic%2520task%2520planners.%2520Potential%2520benefits%2520include%2520%2528i%2529%2520a%2520better%250Ascalability%2520in%2520so%2520far%2520as%2520the%2520planning%2520domain%2520complexity%2520increases%252C%2520since%2520LLMs%2527%250Aresponse%2520time%2520linearly%2520scales%2520with%2520the%2520combined%2520length%2520of%2520the%2520input%2520and%2520the%250Aoutput%252C%2520and%2520%2528ii%2529%2520the%2520ability%2520to%2520synthesize%2520a%2520plan%2520action-by-action%2520instead%2520of%250Aend-to-end%252C%2520making%2520each%2520action%2520available%2520for%2520execution%2520as%2520soon%2520as%2520it%2520is%250Agenerated%2520instead%2520of%2520waiting%2520for%2520the%2520whole%2520plan%2520to%2520be%2520available%252C%2520which%2520in%2520turn%250Aenables%2520concurrent%2520planning%2520and%2520execution.%2520Recently%252C%2520significant%2520efforts%2520have%250Abeen%2520devoted%2520by%2520the%2520research%2520community%2520to%2520evaluate%2520the%2520cognitive%2520capabilities%250Aof%2520LLMs%252C%2520with%2520alternate%2520successes.%2520Instead%252C%2520with%2520Teriyaki%2520we%2520aim%2520to%2520provide%2520an%250Aoverall%2520planning%2520performance%2520comparable%2520to%2520traditional%2520planners%2520in%2520specific%250Aplanning%2520domains%252C%2520while%2520leveraging%2520LLMs%2520capabilities%2520to%2520build%2520a%2520look-ahead%250Apredictive%2520planning%2520model.%2520Preliminary%2520results%2520in%2520selected%2520domains%2520show%2520that%250Aour%2520method%2520can%253A%2520%2528i%2529%2520solve%252095.5%2525%2520of%2520problems%2520in%2520a%2520test%2520data%2520set%2520of%25201%252C000%250Asamples%253B%2520%2528ii%2529%2520produce%2520plans%2520up%2520to%252013.5%2525%2520shorter%2520than%2520a%2520traditional%2520symbolic%250Aplanner%253B%2520%2528iii%2529%2520reduce%2520average%2520overall%2520waiting%2520times%2520for%2520a%2520plan%2520availability%2520by%250Aup%2520to%252061.4%2525%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.00438v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Neurosymbolic%20Robot%20Action%20Planning%20using%20Large%20Language%0A%20%20Models&entry.906535625=Alessio%20Capitanelli%20and%20Fulvio%20Mastrogiovanni&entry.1292438233=%20%20Symbolic%20task%20planning%20is%20a%20widely%20used%20approach%20to%20enforce%20robot%20autonomy%0Adue%20to%20its%20ease%20of%20understanding%20and%20deployment%20in%20robot%20architectures.%0AHowever%2C%20techniques%20for%20symbolic%20task%20planning%20are%20difficult%20to%20scale%20in%0Areal-world%2C%20human-robot%20collaboration%20scenarios%20because%20of%20the%20poor%20performance%0Ain%20complex%20planning%20domains%20or%20when%20frequent%20re-planning%20is%20needed.%20We%20present%0Aa%20framework%2C%20Teriyaki%2C%20specifically%20aimed%20at%20bridging%20the%20gap%20between%20symbolic%0Atask%20planning%20and%20machine%20learning%20approaches.%20The%20rationale%20is%20training%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20namely%20GPT-3%2C%20into%20a%20neurosymbolic%20task%20planner%0Acompatible%20with%20the%20Planning%20Domain%20Definition%20Language%20%28PDDL%29%2C%20and%20then%0Aleveraging%20its%20generative%20capabilities%20to%20overcome%20a%20number%20of%20limitations%0Ainherent%20to%20symbolic%20task%20planners.%20Potential%20benefits%20include%20%28i%29%20a%20better%0Ascalability%20in%20so%20far%20as%20the%20planning%20domain%20complexity%20increases%2C%20since%20LLMs%27%0Aresponse%20time%20linearly%20scales%20with%20the%20combined%20length%20of%20the%20input%20and%20the%0Aoutput%2C%20and%20%28ii%29%20the%20ability%20to%20synthesize%20a%20plan%20action-by-action%20instead%20of%0Aend-to-end%2C%20making%20each%20action%20available%20for%20execution%20as%20soon%20as%20it%20is%0Agenerated%20instead%20of%20waiting%20for%20the%20whole%20plan%20to%20be%20available%2C%20which%20in%20turn%0Aenables%20concurrent%20planning%20and%20execution.%20Recently%2C%20significant%20efforts%20have%0Abeen%20devoted%20by%20the%20research%20community%20to%20evaluate%20the%20cognitive%20capabilities%0Aof%20LLMs%2C%20with%20alternate%20successes.%20Instead%2C%20with%20Teriyaki%20we%20aim%20to%20provide%20an%0Aoverall%20planning%20performance%20comparable%20to%20traditional%20planners%20in%20specific%0Aplanning%20domains%2C%20while%20leveraging%20LLMs%20capabilities%20to%20build%20a%20look-ahead%0Apredictive%20planning%20model.%20Preliminary%20results%20in%20selected%20domains%20show%20that%0Aour%20method%20can%3A%20%28i%29%20solve%2095.5%25%20of%20problems%20in%20a%20test%20data%20set%20of%201%2C000%0Asamples%3B%20%28ii%29%20produce%20plans%20up%20to%2013.5%25%20shorter%20than%20a%20traditional%20symbolic%0Aplanner%3B%20%28iii%29%20reduce%20average%20overall%20waiting%20times%20for%20a%20plan%20availability%20by%0Aup%20to%2061.4%25%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.00438v3&entry.124074799=Read"},
{"title": "Smaller Batches, Bigger Gains? Investigating the Impact of Batch Sizes\n  on Reinforcement Learning Based Real-World Production Scheduling", "author": "Arthur M\u00fcller and Felix Grumbach and Matthia Sabatelli", "abstract": "  Production scheduling is an essential task in manufacturing, with\nReinforcement Learning (RL) emerging as a key solution. In a previous work, RL\nwas utilized to solve an extended permutation flow shop scheduling problem\n(PFSSP) for a real-world production line with two stages, linked by a central\nbuffer. The RL agent was trained to sequence equallysized product batches to\nminimize setup efforts and idle times. However, the substantial impact caused\nby varying the size of these product batches has not yet been explored. In this\nfollow-up study, we investigate the effects of varying batch sizes, exploring\nboth the quality of solutions and the training dynamics of the RL agent. The\nresults demonstrate that it is possible to methodically identify reasonable\nboundaries for the batch size. These boundaries are determined on one side by\nthe increasing sample complexity associated with smaller batch sizes, and on\nthe other side by the decreasing flexibility of the agent when dealing with\nlarger batch sizes. This provides the practitioner the ability to make an\ninformed decision regarding the selection of an appropriate batch size.\nMoreover, we introduce and investigate two new curriculum learning strategies\nto enable the training with small batch sizes. The findings of this work offer\nthe potential for application in several industrial use cases with comparable\nscheduling problems.\n", "link": "http://arxiv.org/abs/2406.02294v1", "date": "2024-06-04", "relevancy": 1.4434, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4925}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4832}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smaller%20Batches%2C%20Bigger%20Gains%3F%20Investigating%20the%20Impact%20of%20Batch%20Sizes%0A%20%20on%20Reinforcement%20Learning%20Based%20Real-World%20Production%20Scheduling&body=Title%3A%20Smaller%20Batches%2C%20Bigger%20Gains%3F%20Investigating%20the%20Impact%20of%20Batch%20Sizes%0A%20%20on%20Reinforcement%20Learning%20Based%20Real-World%20Production%20Scheduling%0AAuthor%3A%20Arthur%20M%C3%BCller%20and%20Felix%20Grumbach%20and%20Matthia%20Sabatelli%0AAbstract%3A%20%20%20Production%20scheduling%20is%20an%20essential%20task%20in%20manufacturing%2C%20with%0AReinforcement%20Learning%20%28RL%29%20emerging%20as%20a%20key%20solution.%20In%20a%20previous%20work%2C%20RL%0Awas%20utilized%20to%20solve%20an%20extended%20permutation%20flow%20shop%20scheduling%20problem%0A%28PFSSP%29%20for%20a%20real-world%20production%20line%20with%20two%20stages%2C%20linked%20by%20a%20central%0Abuffer.%20The%20RL%20agent%20was%20trained%20to%20sequence%20equallysized%20product%20batches%20to%0Aminimize%20setup%20efforts%20and%20idle%20times.%20However%2C%20the%20substantial%20impact%20caused%0Aby%20varying%20the%20size%20of%20these%20product%20batches%20has%20not%20yet%20been%20explored.%20In%20this%0Afollow-up%20study%2C%20we%20investigate%20the%20effects%20of%20varying%20batch%20sizes%2C%20exploring%0Aboth%20the%20quality%20of%20solutions%20and%20the%20training%20dynamics%20of%20the%20RL%20agent.%20The%0Aresults%20demonstrate%20that%20it%20is%20possible%20to%20methodically%20identify%20reasonable%0Aboundaries%20for%20the%20batch%20size.%20These%20boundaries%20are%20determined%20on%20one%20side%20by%0Athe%20increasing%20sample%20complexity%20associated%20with%20smaller%20batch%20sizes%2C%20and%20on%0Athe%20other%20side%20by%20the%20decreasing%20flexibility%20of%20the%20agent%20when%20dealing%20with%0Alarger%20batch%20sizes.%20This%20provides%20the%20practitioner%20the%20ability%20to%20make%20an%0Ainformed%20decision%20regarding%20the%20selection%20of%20an%20appropriate%20batch%20size.%0AMoreover%2C%20we%20introduce%20and%20investigate%20two%20new%20curriculum%20learning%20strategies%0Ato%20enable%20the%20training%20with%20small%20batch%20sizes.%20The%20findings%20of%20this%20work%20offer%0Athe%20potential%20for%20application%20in%20several%20industrial%20use%20cases%20with%20comparable%0Ascheduling%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmaller%2520Batches%252C%2520Bigger%2520Gains%253F%2520Investigating%2520the%2520Impact%2520of%2520Batch%2520Sizes%250A%2520%2520on%2520Reinforcement%2520Learning%2520Based%2520Real-World%2520Production%2520Scheduling%26entry.906535625%3DArthur%2520M%25C3%25BCller%2520and%2520Felix%2520Grumbach%2520and%2520Matthia%2520Sabatelli%26entry.1292438233%3D%2520%2520Production%2520scheduling%2520is%2520an%2520essential%2520task%2520in%2520manufacturing%252C%2520with%250AReinforcement%2520Learning%2520%2528RL%2529%2520emerging%2520as%2520a%2520key%2520solution.%2520In%2520a%2520previous%2520work%252C%2520RL%250Awas%2520utilized%2520to%2520solve%2520an%2520extended%2520permutation%2520flow%2520shop%2520scheduling%2520problem%250A%2528PFSSP%2529%2520for%2520a%2520real-world%2520production%2520line%2520with%2520two%2520stages%252C%2520linked%2520by%2520a%2520central%250Abuffer.%2520The%2520RL%2520agent%2520was%2520trained%2520to%2520sequence%2520equallysized%2520product%2520batches%2520to%250Aminimize%2520setup%2520efforts%2520and%2520idle%2520times.%2520However%252C%2520the%2520substantial%2520impact%2520caused%250Aby%2520varying%2520the%2520size%2520of%2520these%2520product%2520batches%2520has%2520not%2520yet%2520been%2520explored.%2520In%2520this%250Afollow-up%2520study%252C%2520we%2520investigate%2520the%2520effects%2520of%2520varying%2520batch%2520sizes%252C%2520exploring%250Aboth%2520the%2520quality%2520of%2520solutions%2520and%2520the%2520training%2520dynamics%2520of%2520the%2520RL%2520agent.%2520The%250Aresults%2520demonstrate%2520that%2520it%2520is%2520possible%2520to%2520methodically%2520identify%2520reasonable%250Aboundaries%2520for%2520the%2520batch%2520size.%2520These%2520boundaries%2520are%2520determined%2520on%2520one%2520side%2520by%250Athe%2520increasing%2520sample%2520complexity%2520associated%2520with%2520smaller%2520batch%2520sizes%252C%2520and%2520on%250Athe%2520other%2520side%2520by%2520the%2520decreasing%2520flexibility%2520of%2520the%2520agent%2520when%2520dealing%2520with%250Alarger%2520batch%2520sizes.%2520This%2520provides%2520the%2520practitioner%2520the%2520ability%2520to%2520make%2520an%250Ainformed%2520decision%2520regarding%2520the%2520selection%2520of%2520an%2520appropriate%2520batch%2520size.%250AMoreover%252C%2520we%2520introduce%2520and%2520investigate%2520two%2520new%2520curriculum%2520learning%2520strategies%250Ato%2520enable%2520the%2520training%2520with%2520small%2520batch%2520sizes.%2520The%2520findings%2520of%2520this%2520work%2520offer%250Athe%2520potential%2520for%2520application%2520in%2520several%2520industrial%2520use%2520cases%2520with%2520comparable%250Ascheduling%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smaller%20Batches%2C%20Bigger%20Gains%3F%20Investigating%20the%20Impact%20of%20Batch%20Sizes%0A%20%20on%20Reinforcement%20Learning%20Based%20Real-World%20Production%20Scheduling&entry.906535625=Arthur%20M%C3%BCller%20and%20Felix%20Grumbach%20and%20Matthia%20Sabatelli&entry.1292438233=%20%20Production%20scheduling%20is%20an%20essential%20task%20in%20manufacturing%2C%20with%0AReinforcement%20Learning%20%28RL%29%20emerging%20as%20a%20key%20solution.%20In%20a%20previous%20work%2C%20RL%0Awas%20utilized%20to%20solve%20an%20extended%20permutation%20flow%20shop%20scheduling%20problem%0A%28PFSSP%29%20for%20a%20real-world%20production%20line%20with%20two%20stages%2C%20linked%20by%20a%20central%0Abuffer.%20The%20RL%20agent%20was%20trained%20to%20sequence%20equallysized%20product%20batches%20to%0Aminimize%20setup%20efforts%20and%20idle%20times.%20However%2C%20the%20substantial%20impact%20caused%0Aby%20varying%20the%20size%20of%20these%20product%20batches%20has%20not%20yet%20been%20explored.%20In%20this%0Afollow-up%20study%2C%20we%20investigate%20the%20effects%20of%20varying%20batch%20sizes%2C%20exploring%0Aboth%20the%20quality%20of%20solutions%20and%20the%20training%20dynamics%20of%20the%20RL%20agent.%20The%0Aresults%20demonstrate%20that%20it%20is%20possible%20to%20methodically%20identify%20reasonable%0Aboundaries%20for%20the%20batch%20size.%20These%20boundaries%20are%20determined%20on%20one%20side%20by%0Athe%20increasing%20sample%20complexity%20associated%20with%20smaller%20batch%20sizes%2C%20and%20on%0Athe%20other%20side%20by%20the%20decreasing%20flexibility%20of%20the%20agent%20when%20dealing%20with%0Alarger%20batch%20sizes.%20This%20provides%20the%20practitioner%20the%20ability%20to%20make%20an%0Ainformed%20decision%20regarding%20the%20selection%20of%20an%20appropriate%20batch%20size.%0AMoreover%2C%20we%20introduce%20and%20investigate%20two%20new%20curriculum%20learning%20strategies%0Ato%20enable%20the%20training%20with%20small%20batch%20sizes.%20The%20findings%20of%20this%20work%20offer%0Athe%20potential%20for%20application%20in%20several%20industrial%20use%20cases%20with%20comparable%0Ascheduling%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02294v1&entry.124074799=Read"},
{"title": "Machine learning Hubbard parameters with equivariant neural networks", "author": "Martin Uhrin and Austin Zadoks and Luca Binci and Nicola Marzari and Iurii Timrov", "abstract": "  Density-functional theory with extended Hubbard functionals (DFT+$U$+$V$)\nprovides a robust framework to accurately describe complex materials containing\ntransition-metal or rare-earth elements. It does so by mitigating\nself-interaction errors inherent to semi-local functionals which are\nparticularly pronounced in systems with partially-filled $d$ and $f$ electronic\nstates. However, achieving accuracy in this approach hinges upon the accurate\ndetermination of the on-site $U$ and inter-site $V$ Hubbard parameters. In\npractice, these are obtained either by semi-empirical tuning, requiring prior\nknowledge, or, more correctly, by using predictive but expensive\nfirst-principles calculations. Here, we present a machine learning model based\non equivariant neural networks which uses atomic occupation matrices as\ndescriptors, directly capturing the electronic structure, local chemical\nenvironment, and oxidation states of the system at hand. We target here the\nprediction of Hubbard parameters computed self-consistently with iterative\nlinear-response calculations, as implemented in density-functional perturbation\ntheory (DFPT), and structural relaxations. Remarkably, when trained on data\nfrom 11 materials spanning various crystal structures and compositions, our\nmodel achieves mean absolute relative errors of 3% and 5% for Hubbard $U$ and\n$V$ parameters, respectively. By circumventing computationally expensive DFT or\nDFPT self-consistent protocols, our model significantly expedites the\nprediction of Hubbard parameters with negligible computational overhead, while\napproaching the accuracy of DFPT. Moreover, owing to its robust\ntransferability, the model facilitates accelerated materials discovery and\ndesign via high-throughput calculations, with relevance for various\ntechnological applications.\n", "link": "http://arxiv.org/abs/2406.02457v1", "date": "2024-06-04", "relevancy": 1.2811, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4663}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4184}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20learning%20Hubbard%20parameters%20with%20equivariant%20neural%20networks&body=Title%3A%20Machine%20learning%20Hubbard%20parameters%20with%20equivariant%20neural%20networks%0AAuthor%3A%20Martin%20Uhrin%20and%20Austin%20Zadoks%20and%20Luca%20Binci%20and%20Nicola%20Marzari%20and%20Iurii%20Timrov%0AAbstract%3A%20%20%20Density-functional%20theory%20with%20extended%20Hubbard%20functionals%20%28DFT%2B%24U%24%2B%24V%24%29%0Aprovides%20a%20robust%20framework%20to%20accurately%20describe%20complex%20materials%20containing%0Atransition-metal%20or%20rare-earth%20elements.%20It%20does%20so%20by%20mitigating%0Aself-interaction%20errors%20inherent%20to%20semi-local%20functionals%20which%20are%0Aparticularly%20pronounced%20in%20systems%20with%20partially-filled%20%24d%24%20and%20%24f%24%20electronic%0Astates.%20However%2C%20achieving%20accuracy%20in%20this%20approach%20hinges%20upon%20the%20accurate%0Adetermination%20of%20the%20on-site%20%24U%24%20and%20inter-site%20%24V%24%20Hubbard%20parameters.%20In%0Apractice%2C%20these%20are%20obtained%20either%20by%20semi-empirical%20tuning%2C%20requiring%20prior%0Aknowledge%2C%20or%2C%20more%20correctly%2C%20by%20using%20predictive%20but%20expensive%0Afirst-principles%20calculations.%20Here%2C%20we%20present%20a%20machine%20learning%20model%20based%0Aon%20equivariant%20neural%20networks%20which%20uses%20atomic%20occupation%20matrices%20as%0Adescriptors%2C%20directly%20capturing%20the%20electronic%20structure%2C%20local%20chemical%0Aenvironment%2C%20and%20oxidation%20states%20of%20the%20system%20at%20hand.%20We%20target%20here%20the%0Aprediction%20of%20Hubbard%20parameters%20computed%20self-consistently%20with%20iterative%0Alinear-response%20calculations%2C%20as%20implemented%20in%20density-functional%20perturbation%0Atheory%20%28DFPT%29%2C%20and%20structural%20relaxations.%20Remarkably%2C%20when%20trained%20on%20data%0Afrom%2011%20materials%20spanning%20various%20crystal%20structures%20and%20compositions%2C%20our%0Amodel%20achieves%20mean%20absolute%20relative%20errors%20of%203%25%20and%205%25%20for%20Hubbard%20%24U%24%20and%0A%24V%24%20parameters%2C%20respectively.%20By%20circumventing%20computationally%20expensive%20DFT%20or%0ADFPT%20self-consistent%20protocols%2C%20our%20model%20significantly%20expedites%20the%0Aprediction%20of%20Hubbard%20parameters%20with%20negligible%20computational%20overhead%2C%20while%0Aapproaching%20the%20accuracy%20of%20DFPT.%20Moreover%2C%20owing%20to%20its%20robust%0Atransferability%2C%20the%20model%20facilitates%20accelerated%20materials%20discovery%20and%0Adesign%20via%20high-throughput%20calculations%2C%20with%20relevance%20for%20various%0Atechnological%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520learning%2520Hubbard%2520parameters%2520with%2520equivariant%2520neural%2520networks%26entry.906535625%3DMartin%2520Uhrin%2520and%2520Austin%2520Zadoks%2520and%2520Luca%2520Binci%2520and%2520Nicola%2520Marzari%2520and%2520Iurii%2520Timrov%26entry.1292438233%3D%2520%2520Density-functional%2520theory%2520with%2520extended%2520Hubbard%2520functionals%2520%2528DFT%252B%2524U%2524%252B%2524V%2524%2529%250Aprovides%2520a%2520robust%2520framework%2520to%2520accurately%2520describe%2520complex%2520materials%2520containing%250Atransition-metal%2520or%2520rare-earth%2520elements.%2520It%2520does%2520so%2520by%2520mitigating%250Aself-interaction%2520errors%2520inherent%2520to%2520semi-local%2520functionals%2520which%2520are%250Aparticularly%2520pronounced%2520in%2520systems%2520with%2520partially-filled%2520%2524d%2524%2520and%2520%2524f%2524%2520electronic%250Astates.%2520However%252C%2520achieving%2520accuracy%2520in%2520this%2520approach%2520hinges%2520upon%2520the%2520accurate%250Adetermination%2520of%2520the%2520on-site%2520%2524U%2524%2520and%2520inter-site%2520%2524V%2524%2520Hubbard%2520parameters.%2520In%250Apractice%252C%2520these%2520are%2520obtained%2520either%2520by%2520semi-empirical%2520tuning%252C%2520requiring%2520prior%250Aknowledge%252C%2520or%252C%2520more%2520correctly%252C%2520by%2520using%2520predictive%2520but%2520expensive%250Afirst-principles%2520calculations.%2520Here%252C%2520we%2520present%2520a%2520machine%2520learning%2520model%2520based%250Aon%2520equivariant%2520neural%2520networks%2520which%2520uses%2520atomic%2520occupation%2520matrices%2520as%250Adescriptors%252C%2520directly%2520capturing%2520the%2520electronic%2520structure%252C%2520local%2520chemical%250Aenvironment%252C%2520and%2520oxidation%2520states%2520of%2520the%2520system%2520at%2520hand.%2520We%2520target%2520here%2520the%250Aprediction%2520of%2520Hubbard%2520parameters%2520computed%2520self-consistently%2520with%2520iterative%250Alinear-response%2520calculations%252C%2520as%2520implemented%2520in%2520density-functional%2520perturbation%250Atheory%2520%2528DFPT%2529%252C%2520and%2520structural%2520relaxations.%2520Remarkably%252C%2520when%2520trained%2520on%2520data%250Afrom%252011%2520materials%2520spanning%2520various%2520crystal%2520structures%2520and%2520compositions%252C%2520our%250Amodel%2520achieves%2520mean%2520absolute%2520relative%2520errors%2520of%25203%2525%2520and%25205%2525%2520for%2520Hubbard%2520%2524U%2524%2520and%250A%2524V%2524%2520parameters%252C%2520respectively.%2520By%2520circumventing%2520computationally%2520expensive%2520DFT%2520or%250ADFPT%2520self-consistent%2520protocols%252C%2520our%2520model%2520significantly%2520expedites%2520the%250Aprediction%2520of%2520Hubbard%2520parameters%2520with%2520negligible%2520computational%2520overhead%252C%2520while%250Aapproaching%2520the%2520accuracy%2520of%2520DFPT.%2520Moreover%252C%2520owing%2520to%2520its%2520robust%250Atransferability%252C%2520the%2520model%2520facilitates%2520accelerated%2520materials%2520discovery%2520and%250Adesign%2520via%2520high-throughput%2520calculations%252C%2520with%2520relevance%2520for%2520various%250Atechnological%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20learning%20Hubbard%20parameters%20with%20equivariant%20neural%20networks&entry.906535625=Martin%20Uhrin%20and%20Austin%20Zadoks%20and%20Luca%20Binci%20and%20Nicola%20Marzari%20and%20Iurii%20Timrov&entry.1292438233=%20%20Density-functional%20theory%20with%20extended%20Hubbard%20functionals%20%28DFT%2B%24U%24%2B%24V%24%29%0Aprovides%20a%20robust%20framework%20to%20accurately%20describe%20complex%20materials%20containing%0Atransition-metal%20or%20rare-earth%20elements.%20It%20does%20so%20by%20mitigating%0Aself-interaction%20errors%20inherent%20to%20semi-local%20functionals%20which%20are%0Aparticularly%20pronounced%20in%20systems%20with%20partially-filled%20%24d%24%20and%20%24f%24%20electronic%0Astates.%20However%2C%20achieving%20accuracy%20in%20this%20approach%20hinges%20upon%20the%20accurate%0Adetermination%20of%20the%20on-site%20%24U%24%20and%20inter-site%20%24V%24%20Hubbard%20parameters.%20In%0Apractice%2C%20these%20are%20obtained%20either%20by%20semi-empirical%20tuning%2C%20requiring%20prior%0Aknowledge%2C%20or%2C%20more%20correctly%2C%20by%20using%20predictive%20but%20expensive%0Afirst-principles%20calculations.%20Here%2C%20we%20present%20a%20machine%20learning%20model%20based%0Aon%20equivariant%20neural%20networks%20which%20uses%20atomic%20occupation%20matrices%20as%0Adescriptors%2C%20directly%20capturing%20the%20electronic%20structure%2C%20local%20chemical%0Aenvironment%2C%20and%20oxidation%20states%20of%20the%20system%20at%20hand.%20We%20target%20here%20the%0Aprediction%20of%20Hubbard%20parameters%20computed%20self-consistently%20with%20iterative%0Alinear-response%20calculations%2C%20as%20implemented%20in%20density-functional%20perturbation%0Atheory%20%28DFPT%29%2C%20and%20structural%20relaxations.%20Remarkably%2C%20when%20trained%20on%20data%0Afrom%2011%20materials%20spanning%20various%20crystal%20structures%20and%20compositions%2C%20our%0Amodel%20achieves%20mean%20absolute%20relative%20errors%20of%203%25%20and%205%25%20for%20Hubbard%20%24U%24%20and%0A%24V%24%20parameters%2C%20respectively.%20By%20circumventing%20computationally%20expensive%20DFT%20or%0ADFPT%20self-consistent%20protocols%2C%20our%20model%20significantly%20expedites%20the%0Aprediction%20of%20Hubbard%20parameters%20with%20negligible%20computational%20overhead%2C%20while%0Aapproaching%20the%20accuracy%20of%20DFPT.%20Moreover%2C%20owing%20to%20its%20robust%0Atransferability%2C%20the%20model%20facilitates%20accelerated%20materials%20discovery%20and%0Adesign%20via%20high-throughput%20calculations%2C%20with%20relevance%20for%20various%0Atechnological%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02457v1&entry.124074799=Read"},
{"title": "Identifying Equivalent Training Dynamics", "author": "William T. Redman and Juan M. Bello-Rivas and Maria Fonoberova and Ryan Mohr and Ioannis G. Kevrekidis and Igor Mezi\u0107", "abstract": "  Study of the nonlinear evolution deep neural network (DNN) parameters undergo\nduring training has uncovered regimes of distinct dynamical behavior. While a\ndetailed understanding of these phenomena has the potential to advance\nimprovements in training efficiency and robustness, the lack of methods for\nidentifying when DNN models have equivalent dynamics limits the insight that\ncan be gained from prior work. Topological conjugacy, a notion from dynamical\nsystems theory, provides a precise definition of dynamical equivalence,\noffering a possible route to address this need. However, topological\nconjugacies have historically been challenging to compute. By leveraging\nadvances in Koopman operator theory, we develop a framework for identifying\nconjugate and non-conjugate training dynamics. To validate our approach, we\ndemonstrate that it can correctly identify a known equivalence between online\nmirror descent and online gradient descent. We then utilize it to: identify\nnon-conjugate training dynamics between shallow and wide fully connected neural\nnetworks; characterize the early phase of training dynamics in convolutional\nneural networks; uncover non-conjugate training dynamics in Transformers that\ndo and do not undergo grokking. Our results, across a range of DNN\narchitectures, illustrate the flexibility of our framework and highlight its\npotential for shedding new light on training dynamics.\n", "link": "http://arxiv.org/abs/2302.09160v2", "date": "2024-06-04", "relevancy": 1.5976, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5754}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4827}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Equivalent%20Training%20Dynamics&body=Title%3A%20Identifying%20Equivalent%20Training%20Dynamics%0AAuthor%3A%20William%20T.%20Redman%20and%20Juan%20M.%20Bello-Rivas%20and%20Maria%20Fonoberova%20and%20Ryan%20Mohr%20and%20Ioannis%20G.%20Kevrekidis%20and%20Igor%20Mezi%C4%87%0AAbstract%3A%20%20%20Study%20of%20the%20nonlinear%20evolution%20deep%20neural%20network%20%28DNN%29%20parameters%20undergo%0Aduring%20training%20has%20uncovered%20regimes%20of%20distinct%20dynamical%20behavior.%20While%20a%0Adetailed%20understanding%20of%20these%20phenomena%20has%20the%20potential%20to%20advance%0Aimprovements%20in%20training%20efficiency%20and%20robustness%2C%20the%20lack%20of%20methods%20for%0Aidentifying%20when%20DNN%20models%20have%20equivalent%20dynamics%20limits%20the%20insight%20that%0Acan%20be%20gained%20from%20prior%20work.%20Topological%20conjugacy%2C%20a%20notion%20from%20dynamical%0Asystems%20theory%2C%20provides%20a%20precise%20definition%20of%20dynamical%20equivalence%2C%0Aoffering%20a%20possible%20route%20to%20address%20this%20need.%20However%2C%20topological%0Aconjugacies%20have%20historically%20been%20challenging%20to%20compute.%20By%20leveraging%0Aadvances%20in%20Koopman%20operator%20theory%2C%20we%20develop%20a%20framework%20for%20identifying%0Aconjugate%20and%20non-conjugate%20training%20dynamics.%20To%20validate%20our%20approach%2C%20we%0Ademonstrate%20that%20it%20can%20correctly%20identify%20a%20known%20equivalence%20between%20online%0Amirror%20descent%20and%20online%20gradient%20descent.%20We%20then%20utilize%20it%20to%3A%20identify%0Anon-conjugate%20training%20dynamics%20between%20shallow%20and%20wide%20fully%20connected%20neural%0Anetworks%3B%20characterize%20the%20early%20phase%20of%20training%20dynamics%20in%20convolutional%0Aneural%20networks%3B%20uncover%20non-conjugate%20training%20dynamics%20in%20Transformers%20that%0Ado%20and%20do%20not%20undergo%20grokking.%20Our%20results%2C%20across%20a%20range%20of%20DNN%0Aarchitectures%2C%20illustrate%20the%20flexibility%20of%20our%20framework%20and%20highlight%20its%0Apotential%20for%20shedding%20new%20light%20on%20training%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.09160v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Equivalent%2520Training%2520Dynamics%26entry.906535625%3DWilliam%2520T.%2520Redman%2520and%2520Juan%2520M.%2520Bello-Rivas%2520and%2520Maria%2520Fonoberova%2520and%2520Ryan%2520Mohr%2520and%2520Ioannis%2520G.%2520Kevrekidis%2520and%2520Igor%2520Mezi%25C4%2587%26entry.1292438233%3D%2520%2520Study%2520of%2520the%2520nonlinear%2520evolution%2520deep%2520neural%2520network%2520%2528DNN%2529%2520parameters%2520undergo%250Aduring%2520training%2520has%2520uncovered%2520regimes%2520of%2520distinct%2520dynamical%2520behavior.%2520While%2520a%250Adetailed%2520understanding%2520of%2520these%2520phenomena%2520has%2520the%2520potential%2520to%2520advance%250Aimprovements%2520in%2520training%2520efficiency%2520and%2520robustness%252C%2520the%2520lack%2520of%2520methods%2520for%250Aidentifying%2520when%2520DNN%2520models%2520have%2520equivalent%2520dynamics%2520limits%2520the%2520insight%2520that%250Acan%2520be%2520gained%2520from%2520prior%2520work.%2520Topological%2520conjugacy%252C%2520a%2520notion%2520from%2520dynamical%250Asystems%2520theory%252C%2520provides%2520a%2520precise%2520definition%2520of%2520dynamical%2520equivalence%252C%250Aoffering%2520a%2520possible%2520route%2520to%2520address%2520this%2520need.%2520However%252C%2520topological%250Aconjugacies%2520have%2520historically%2520been%2520challenging%2520to%2520compute.%2520By%2520leveraging%250Aadvances%2520in%2520Koopman%2520operator%2520theory%252C%2520we%2520develop%2520a%2520framework%2520for%2520identifying%250Aconjugate%2520and%2520non-conjugate%2520training%2520dynamics.%2520To%2520validate%2520our%2520approach%252C%2520we%250Ademonstrate%2520that%2520it%2520can%2520correctly%2520identify%2520a%2520known%2520equivalence%2520between%2520online%250Amirror%2520descent%2520and%2520online%2520gradient%2520descent.%2520We%2520then%2520utilize%2520it%2520to%253A%2520identify%250Anon-conjugate%2520training%2520dynamics%2520between%2520shallow%2520and%2520wide%2520fully%2520connected%2520neural%250Anetworks%253B%2520characterize%2520the%2520early%2520phase%2520of%2520training%2520dynamics%2520in%2520convolutional%250Aneural%2520networks%253B%2520uncover%2520non-conjugate%2520training%2520dynamics%2520in%2520Transformers%2520that%250Ado%2520and%2520do%2520not%2520undergo%2520grokking.%2520Our%2520results%252C%2520across%2520a%2520range%2520of%2520DNN%250Aarchitectures%252C%2520illustrate%2520the%2520flexibility%2520of%2520our%2520framework%2520and%2520highlight%2520its%250Apotential%2520for%2520shedding%2520new%2520light%2520on%2520training%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.09160v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Equivalent%20Training%20Dynamics&entry.906535625=William%20T.%20Redman%20and%20Juan%20M.%20Bello-Rivas%20and%20Maria%20Fonoberova%20and%20Ryan%20Mohr%20and%20Ioannis%20G.%20Kevrekidis%20and%20Igor%20Mezi%C4%87&entry.1292438233=%20%20Study%20of%20the%20nonlinear%20evolution%20deep%20neural%20network%20%28DNN%29%20parameters%20undergo%0Aduring%20training%20has%20uncovered%20regimes%20of%20distinct%20dynamical%20behavior.%20While%20a%0Adetailed%20understanding%20of%20these%20phenomena%20has%20the%20potential%20to%20advance%0Aimprovements%20in%20training%20efficiency%20and%20robustness%2C%20the%20lack%20of%20methods%20for%0Aidentifying%20when%20DNN%20models%20have%20equivalent%20dynamics%20limits%20the%20insight%20that%0Acan%20be%20gained%20from%20prior%20work.%20Topological%20conjugacy%2C%20a%20notion%20from%20dynamical%0Asystems%20theory%2C%20provides%20a%20precise%20definition%20of%20dynamical%20equivalence%2C%0Aoffering%20a%20possible%20route%20to%20address%20this%20need.%20However%2C%20topological%0Aconjugacies%20have%20historically%20been%20challenging%20to%20compute.%20By%20leveraging%0Aadvances%20in%20Koopman%20operator%20theory%2C%20we%20develop%20a%20framework%20for%20identifying%0Aconjugate%20and%20non-conjugate%20training%20dynamics.%20To%20validate%20our%20approach%2C%20we%0Ademonstrate%20that%20it%20can%20correctly%20identify%20a%20known%20equivalence%20between%20online%0Amirror%20descent%20and%20online%20gradient%20descent.%20We%20then%20utilize%20it%20to%3A%20identify%0Anon-conjugate%20training%20dynamics%20between%20shallow%20and%20wide%20fully%20connected%20neural%0Anetworks%3B%20characterize%20the%20early%20phase%20of%20training%20dynamics%20in%20convolutional%0Aneural%20networks%3B%20uncover%20non-conjugate%20training%20dynamics%20in%20Transformers%20that%0Ado%20and%20do%20not%20undergo%20grokking.%20Our%20results%2C%20across%20a%20range%20of%20DNN%0Aarchitectures%2C%20illustrate%20the%20flexibility%20of%20our%20framework%20and%20highlight%20its%0Apotential%20for%20shedding%20new%20light%20on%20training%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.09160v2&entry.124074799=Read"},
{"title": "Data-driven Energy Efficiency Modelling in Large-scale Networks: An\n  Expert Knowledge and ML-based Approach", "author": "David L\u00f3pez-P\u00e9rez and Antonio De Domenico and Nicola Piovesan and Merouane Debbah", "abstract": "  The energy consumption of mobile networks poses a critical challenge.\nMitigating this concern necessitates the deployment and optimization of network\nenergy-saving solutions, such as carrier shutdown, to dynamically manage\nnetwork resources. Traditional optimization approaches encounter complexity due\nto factors like the large number of cells, stochastic traffic, channel\nvariations, and intricate trade-offs. This paper introduces the simulated\nreality of communication networks (SRCON) framework, a novel, data-driven\nmodeling paradigm that harnesses live network data and employs a blend of\nmachine learning (ML)- and expert-based models. These mix of models accurately\ncharacterizes the functioning of network components, and predicts network\nenergy efficiency and user equipment (UE) quality of service for any energy\ncarrier shutdown configuration in a specific network. Distinguishing itself\nfrom existing methods, SRCON eliminates the reliance on expensive expert\nknowledge, drive testing, or incomplete maps for predicting network\nperformance. This paper details the pipeline employed by SRCON to decompose the\nlarge network energy efficiency modeling problem into ML and expert-based\nsubmodels. It demonstrates how, by embracing stochasticity, and carefully\ncrafting the relationship between such submodels, the overall computational\ncomplexity can be reduced and prediction accuracy enhanced. Results derived\nfrom real network data underscore the paradigm shift introduced by SRCON,\nshowcasing significant gains over a state-of-the art method used by a operator\nfor network energy efficiency modeling. The reliability of this local,\ndata-driven modeling of the network proves to be a key asset for network\nenergy-saving optimization.\n", "link": "http://arxiv.org/abs/2401.00443v2", "date": "2024-06-04", "relevancy": 1.3557, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4677}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4545}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20Energy%20Efficiency%20Modelling%20in%20Large-scale%20Networks%3A%20An%0A%20%20Expert%20Knowledge%20and%20ML-based%20Approach&body=Title%3A%20Data-driven%20Energy%20Efficiency%20Modelling%20in%20Large-scale%20Networks%3A%20An%0A%20%20Expert%20Knowledge%20and%20ML-based%20Approach%0AAuthor%3A%20David%20L%C3%B3pez-P%C3%A9rez%20and%20Antonio%20De%20Domenico%20and%20Nicola%20Piovesan%20and%20Merouane%20Debbah%0AAbstract%3A%20%20%20The%20energy%20consumption%20of%20mobile%20networks%20poses%20a%20critical%20challenge.%0AMitigating%20this%20concern%20necessitates%20the%20deployment%20and%20optimization%20of%20network%0Aenergy-saving%20solutions%2C%20such%20as%20carrier%20shutdown%2C%20to%20dynamically%20manage%0Anetwork%20resources.%20Traditional%20optimization%20approaches%20encounter%20complexity%20due%0Ato%20factors%20like%20the%20large%20number%20of%20cells%2C%20stochastic%20traffic%2C%20channel%0Avariations%2C%20and%20intricate%20trade-offs.%20This%20paper%20introduces%20the%20simulated%0Areality%20of%20communication%20networks%20%28SRCON%29%20framework%2C%20a%20novel%2C%20data-driven%0Amodeling%20paradigm%20that%20harnesses%20live%20network%20data%20and%20employs%20a%20blend%20of%0Amachine%20learning%20%28ML%29-%20and%20expert-based%20models.%20These%20mix%20of%20models%20accurately%0Acharacterizes%20the%20functioning%20of%20network%20components%2C%20and%20predicts%20network%0Aenergy%20efficiency%20and%20user%20equipment%20%28UE%29%20quality%20of%20service%20for%20any%20energy%0Acarrier%20shutdown%20configuration%20in%20a%20specific%20network.%20Distinguishing%20itself%0Afrom%20existing%20methods%2C%20SRCON%20eliminates%20the%20reliance%20on%20expensive%20expert%0Aknowledge%2C%20drive%20testing%2C%20or%20incomplete%20maps%20for%20predicting%20network%0Aperformance.%20This%20paper%20details%20the%20pipeline%20employed%20by%20SRCON%20to%20decompose%20the%0Alarge%20network%20energy%20efficiency%20modeling%20problem%20into%20ML%20and%20expert-based%0Asubmodels.%20It%20demonstrates%20how%2C%20by%20embracing%20stochasticity%2C%20and%20carefully%0Acrafting%20the%20relationship%20between%20such%20submodels%2C%20the%20overall%20computational%0Acomplexity%20can%20be%20reduced%20and%20prediction%20accuracy%20enhanced.%20Results%20derived%0Afrom%20real%20network%20data%20underscore%20the%20paradigm%20shift%20introduced%20by%20SRCON%2C%0Ashowcasing%20significant%20gains%20over%20a%20state-of-the%20art%20method%20used%20by%20a%20operator%0Afor%20network%20energy%20efficiency%20modeling.%20The%20reliability%20of%20this%20local%2C%0Adata-driven%20modeling%20of%20the%20network%20proves%20to%20be%20a%20key%20asset%20for%20network%0Aenergy-saving%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00443v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520Energy%2520Efficiency%2520Modelling%2520in%2520Large-scale%2520Networks%253A%2520An%250A%2520%2520Expert%2520Knowledge%2520and%2520ML-based%2520Approach%26entry.906535625%3DDavid%2520L%25C3%25B3pez-P%25C3%25A9rez%2520and%2520Antonio%2520De%2520Domenico%2520and%2520Nicola%2520Piovesan%2520and%2520Merouane%2520Debbah%26entry.1292438233%3D%2520%2520The%2520energy%2520consumption%2520of%2520mobile%2520networks%2520poses%2520a%2520critical%2520challenge.%250AMitigating%2520this%2520concern%2520necessitates%2520the%2520deployment%2520and%2520optimization%2520of%2520network%250Aenergy-saving%2520solutions%252C%2520such%2520as%2520carrier%2520shutdown%252C%2520to%2520dynamically%2520manage%250Anetwork%2520resources.%2520Traditional%2520optimization%2520approaches%2520encounter%2520complexity%2520due%250Ato%2520factors%2520like%2520the%2520large%2520number%2520of%2520cells%252C%2520stochastic%2520traffic%252C%2520channel%250Avariations%252C%2520and%2520intricate%2520trade-offs.%2520This%2520paper%2520introduces%2520the%2520simulated%250Areality%2520of%2520communication%2520networks%2520%2528SRCON%2529%2520framework%252C%2520a%2520novel%252C%2520data-driven%250Amodeling%2520paradigm%2520that%2520harnesses%2520live%2520network%2520data%2520and%2520employs%2520a%2520blend%2520of%250Amachine%2520learning%2520%2528ML%2529-%2520and%2520expert-based%2520models.%2520These%2520mix%2520of%2520models%2520accurately%250Acharacterizes%2520the%2520functioning%2520of%2520network%2520components%252C%2520and%2520predicts%2520network%250Aenergy%2520efficiency%2520and%2520user%2520equipment%2520%2528UE%2529%2520quality%2520of%2520service%2520for%2520any%2520energy%250Acarrier%2520shutdown%2520configuration%2520in%2520a%2520specific%2520network.%2520Distinguishing%2520itself%250Afrom%2520existing%2520methods%252C%2520SRCON%2520eliminates%2520the%2520reliance%2520on%2520expensive%2520expert%250Aknowledge%252C%2520drive%2520testing%252C%2520or%2520incomplete%2520maps%2520for%2520predicting%2520network%250Aperformance.%2520This%2520paper%2520details%2520the%2520pipeline%2520employed%2520by%2520SRCON%2520to%2520decompose%2520the%250Alarge%2520network%2520energy%2520efficiency%2520modeling%2520problem%2520into%2520ML%2520and%2520expert-based%250Asubmodels.%2520It%2520demonstrates%2520how%252C%2520by%2520embracing%2520stochasticity%252C%2520and%2520carefully%250Acrafting%2520the%2520relationship%2520between%2520such%2520submodels%252C%2520the%2520overall%2520computational%250Acomplexity%2520can%2520be%2520reduced%2520and%2520prediction%2520accuracy%2520enhanced.%2520Results%2520derived%250Afrom%2520real%2520network%2520data%2520underscore%2520the%2520paradigm%2520shift%2520introduced%2520by%2520SRCON%252C%250Ashowcasing%2520significant%2520gains%2520over%2520a%2520state-of-the%2520art%2520method%2520used%2520by%2520a%2520operator%250Afor%2520network%2520energy%2520efficiency%2520modeling.%2520The%2520reliability%2520of%2520this%2520local%252C%250Adata-driven%2520modeling%2520of%2520the%2520network%2520proves%2520to%2520be%2520a%2520key%2520asset%2520for%2520network%250Aenergy-saving%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00443v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20Energy%20Efficiency%20Modelling%20in%20Large-scale%20Networks%3A%20An%0A%20%20Expert%20Knowledge%20and%20ML-based%20Approach&entry.906535625=David%20L%C3%B3pez-P%C3%A9rez%20and%20Antonio%20De%20Domenico%20and%20Nicola%20Piovesan%20and%20Merouane%20Debbah&entry.1292438233=%20%20The%20energy%20consumption%20of%20mobile%20networks%20poses%20a%20critical%20challenge.%0AMitigating%20this%20concern%20necessitates%20the%20deployment%20and%20optimization%20of%20network%0Aenergy-saving%20solutions%2C%20such%20as%20carrier%20shutdown%2C%20to%20dynamically%20manage%0Anetwork%20resources.%20Traditional%20optimization%20approaches%20encounter%20complexity%20due%0Ato%20factors%20like%20the%20large%20number%20of%20cells%2C%20stochastic%20traffic%2C%20channel%0Avariations%2C%20and%20intricate%20trade-offs.%20This%20paper%20introduces%20the%20simulated%0Areality%20of%20communication%20networks%20%28SRCON%29%20framework%2C%20a%20novel%2C%20data-driven%0Amodeling%20paradigm%20that%20harnesses%20live%20network%20data%20and%20employs%20a%20blend%20of%0Amachine%20learning%20%28ML%29-%20and%20expert-based%20models.%20These%20mix%20of%20models%20accurately%0Acharacterizes%20the%20functioning%20of%20network%20components%2C%20and%20predicts%20network%0Aenergy%20efficiency%20and%20user%20equipment%20%28UE%29%20quality%20of%20service%20for%20any%20energy%0Acarrier%20shutdown%20configuration%20in%20a%20specific%20network.%20Distinguishing%20itself%0Afrom%20existing%20methods%2C%20SRCON%20eliminates%20the%20reliance%20on%20expensive%20expert%0Aknowledge%2C%20drive%20testing%2C%20or%20incomplete%20maps%20for%20predicting%20network%0Aperformance.%20This%20paper%20details%20the%20pipeline%20employed%20by%20SRCON%20to%20decompose%20the%0Alarge%20network%20energy%20efficiency%20modeling%20problem%20into%20ML%20and%20expert-based%0Asubmodels.%20It%20demonstrates%20how%2C%20by%20embracing%20stochasticity%2C%20and%20carefully%0Acrafting%20the%20relationship%20between%20such%20submodels%2C%20the%20overall%20computational%0Acomplexity%20can%20be%20reduced%20and%20prediction%20accuracy%20enhanced.%20Results%20derived%0Afrom%20real%20network%20data%20underscore%20the%20paradigm%20shift%20introduced%20by%20SRCON%2C%0Ashowcasing%20significant%20gains%20over%20a%20state-of-the%20art%20method%20used%20by%20a%20operator%0Afor%20network%20energy%20efficiency%20modeling.%20The%20reliability%20of%20this%20local%2C%0Adata-driven%20modeling%20of%20the%20network%20proves%20to%20be%20a%20key%20asset%20for%20network%0Aenergy-saving%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00443v2&entry.124074799=Read"},
{"title": "Unraveling and Mitigating Retriever Inconsistencies in\n  Retrieval-Augmented Large Language Models", "author": "Mingda Li and Xinyu Li and Yifan Chen and Wenfeng Xuan and Weinan Zhang", "abstract": "  Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their\nsuperiority in terms of factuality, they do not consistently outperform the\noriginal retrieval-free Language Models (LMs). Our experiments reveal that this\nexample-level performance inconsistency exists not only between\nretrieval-augmented and retrieval-free LM but also among different retrievers.\nTo understand this phenomenon, we investigate the degeneration behavior of\nRALMs and theoretically decompose it into four categories. Further analysis\nbased on our decomposition reveals that the innate difference in knowledge\nsources and the unpredictable degeneration of the reader model contribute most\nto the inconsistency. Drawing from our analysis, we introduce Ensemble of\nRetrievers (EoR), a trainable framework that can adaptively retrieve from\ndifferent knowledge sources and effectively decrease unpredictable reader\nerrors. Our experiments on Open Domain Question Answering show that EoR\nsubstantially improves performance over the RALM with a single retriever by\nconsiderably reducing inconsistent behaviors.\n", "link": "http://arxiv.org/abs/2405.20680v3", "date": "2024-06-04", "relevancy": 1.9314, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5414}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4724}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20and%20Mitigating%20Retriever%20Inconsistencies%20in%0A%20%20Retrieval-Augmented%20Large%20Language%20Models&body=Title%3A%20Unraveling%20and%20Mitigating%20Retriever%20Inconsistencies%20in%0A%20%20Retrieval-Augmented%20Large%20Language%20Models%0AAuthor%3A%20Mingda%20Li%20and%20Xinyu%20Li%20and%20Yifan%20Chen%20and%20Wenfeng%20Xuan%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20Although%20Retrieval-Augmented%20Large%20Language%20Models%20%28RALMs%29%20demonstrate%20their%0Asuperiority%20in%20terms%20of%20factuality%2C%20they%20do%20not%20consistently%20outperform%20the%0Aoriginal%20retrieval-free%20Language%20Models%20%28LMs%29.%20Our%20experiments%20reveal%20that%20this%0Aexample-level%20performance%20inconsistency%20exists%20not%20only%20between%0Aretrieval-augmented%20and%20retrieval-free%20LM%20but%20also%20among%20different%20retrievers.%0ATo%20understand%20this%20phenomenon%2C%20we%20investigate%20the%20degeneration%20behavior%20of%0ARALMs%20and%20theoretically%20decompose%20it%20into%20four%20categories.%20Further%20analysis%0Abased%20on%20our%20decomposition%20reveals%20that%20the%20innate%20difference%20in%20knowledge%0Asources%20and%20the%20unpredictable%20degeneration%20of%20the%20reader%20model%20contribute%20most%0Ato%20the%20inconsistency.%20Drawing%20from%20our%20analysis%2C%20we%20introduce%20Ensemble%20of%0ARetrievers%20%28EoR%29%2C%20a%20trainable%20framework%20that%20can%20adaptively%20retrieve%20from%0Adifferent%20knowledge%20sources%20and%20effectively%20decrease%20unpredictable%20reader%0Aerrors.%20Our%20experiments%20on%20Open%20Domain%20Question%20Answering%20show%20that%20EoR%0Asubstantially%20improves%20performance%20over%20the%20RALM%20with%20a%20single%20retriever%20by%0Aconsiderably%20reducing%20inconsistent%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20680v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520and%2520Mitigating%2520Retriever%2520Inconsistencies%2520in%250A%2520%2520Retrieval-Augmented%2520Large%2520Language%2520Models%26entry.906535625%3DMingda%2520Li%2520and%2520Xinyu%2520Li%2520and%2520Yifan%2520Chen%2520and%2520Wenfeng%2520Xuan%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520Although%2520Retrieval-Augmented%2520Large%2520Language%2520Models%2520%2528RALMs%2529%2520demonstrate%2520their%250Asuperiority%2520in%2520terms%2520of%2520factuality%252C%2520they%2520do%2520not%2520consistently%2520outperform%2520the%250Aoriginal%2520retrieval-free%2520Language%2520Models%2520%2528LMs%2529.%2520Our%2520experiments%2520reveal%2520that%2520this%250Aexample-level%2520performance%2520inconsistency%2520exists%2520not%2520only%2520between%250Aretrieval-augmented%2520and%2520retrieval-free%2520LM%2520but%2520also%2520among%2520different%2520retrievers.%250ATo%2520understand%2520this%2520phenomenon%252C%2520we%2520investigate%2520the%2520degeneration%2520behavior%2520of%250ARALMs%2520and%2520theoretically%2520decompose%2520it%2520into%2520four%2520categories.%2520Further%2520analysis%250Abased%2520on%2520our%2520decomposition%2520reveals%2520that%2520the%2520innate%2520difference%2520in%2520knowledge%250Asources%2520and%2520the%2520unpredictable%2520degeneration%2520of%2520the%2520reader%2520model%2520contribute%2520most%250Ato%2520the%2520inconsistency.%2520Drawing%2520from%2520our%2520analysis%252C%2520we%2520introduce%2520Ensemble%2520of%250ARetrievers%2520%2528EoR%2529%252C%2520a%2520trainable%2520framework%2520that%2520can%2520adaptively%2520retrieve%2520from%250Adifferent%2520knowledge%2520sources%2520and%2520effectively%2520decrease%2520unpredictable%2520reader%250Aerrors.%2520Our%2520experiments%2520on%2520Open%2520Domain%2520Question%2520Answering%2520show%2520that%2520EoR%250Asubstantially%2520improves%2520performance%2520over%2520the%2520RALM%2520with%2520a%2520single%2520retriever%2520by%250Aconsiderably%2520reducing%2520inconsistent%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20680v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20and%20Mitigating%20Retriever%20Inconsistencies%20in%0A%20%20Retrieval-Augmented%20Large%20Language%20Models&entry.906535625=Mingda%20Li%20and%20Xinyu%20Li%20and%20Yifan%20Chen%20and%20Wenfeng%20Xuan%20and%20Weinan%20Zhang&entry.1292438233=%20%20Although%20Retrieval-Augmented%20Large%20Language%20Models%20%28RALMs%29%20demonstrate%20their%0Asuperiority%20in%20terms%20of%20factuality%2C%20they%20do%20not%20consistently%20outperform%20the%0Aoriginal%20retrieval-free%20Language%20Models%20%28LMs%29.%20Our%20experiments%20reveal%20that%20this%0Aexample-level%20performance%20inconsistency%20exists%20not%20only%20between%0Aretrieval-augmented%20and%20retrieval-free%20LM%20but%20also%20among%20different%20retrievers.%0ATo%20understand%20this%20phenomenon%2C%20we%20investigate%20the%20degeneration%20behavior%20of%0ARALMs%20and%20theoretically%20decompose%20it%20into%20four%20categories.%20Further%20analysis%0Abased%20on%20our%20decomposition%20reveals%20that%20the%20innate%20difference%20in%20knowledge%0Asources%20and%20the%20unpredictable%20degeneration%20of%20the%20reader%20model%20contribute%20most%0Ato%20the%20inconsistency.%20Drawing%20from%20our%20analysis%2C%20we%20introduce%20Ensemble%20of%0ARetrievers%20%28EoR%29%2C%20a%20trainable%20framework%20that%20can%20adaptively%20retrieve%20from%0Adifferent%20knowledge%20sources%20and%20effectively%20decrease%20unpredictable%20reader%0Aerrors.%20Our%20experiments%20on%20Open%20Domain%20Question%20Answering%20show%20that%20EoR%0Asubstantially%20improves%20performance%20over%20the%20RALM%20with%20a%20single%20retriever%20by%0Aconsiderably%20reducing%20inconsistent%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20680v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


