<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250529.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence", "author": "Diankun Wu and Fangfu Liu and Yi-Hsin Hung and Yueqi Duan", "abstract": "  Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.\n", "link": "http://arxiv.org/abs/2505.23747v1", "date": "2025-05-29", "relevancy": 3.2452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.653}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-MLLM%3A%20Boosting%20MLLM%20Capabilities%20in%20Visual-based%20Spatial%0A%20%20Intelligence&body=Title%3A%20Spatial-MLLM%3A%20Boosting%20MLLM%20Capabilities%20in%20Visual-based%20Spatial%0A%20%20Intelligence%0AAuthor%3A%20Diankun%20Wu%20and%20Fangfu%20Liu%20and%20Yi-Hsin%20Hung%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Asignificantly%20enhanced%20performance%20on%202D%20visual%20tasks.%20However%2C%20improving%20their%0Aspatial%20intelligence%20remains%20a%20challenge.%20Existing%203D%20MLLMs%20always%20rely%20on%0Aadditional%203D%20or%202.5D%20data%20to%20incorporate%20spatial%20awareness%2C%20restricting%20their%0Autility%20in%20scenarios%20with%20only%202D%20inputs%2C%20such%20as%20images%20or%20videos.%20In%20this%0Apaper%2C%20we%20present%20Spatial-MLLM%2C%20a%20novel%20framework%20for%20visual-based%20spatial%0Areasoning%20from%20purely%202D%20observations.%20Unlike%20conventional%20video%20MLLMs%20which%0Arely%20on%20CLIP-based%20visual%20encoders%20optimized%20for%20semantic%20understanding%2C%20our%0Akey%20insight%20is%20to%20unleash%20the%20strong%20structure%20prior%20from%20the%20feed-forward%0Avisual%20geometry%20foundation%20model.%20Specifically%2C%20we%20propose%20a%20dual-encoder%0Aarchitecture%3A%20a%20pretrained%202D%20visual%20encoder%20to%20extract%20semantic%20features%2C%20and%0Aa%20spatial%20encoder-initialized%20from%20the%20backbone%20of%20the%20visual%20geometry%20model-to%0Aextract%203D%20structure%20features.%20A%20connector%20then%20integrates%20both%20features%20into%0Aunified%20visual%20tokens%20for%20enhanced%20spatial%20understanding.%20Furthermore%2C%20we%0Apropose%20a%20space-aware%20frame%20sampling%20strategy%20at%20inference%20time%2C%20which%20selects%0Athe%20spatially%20informative%20frames%20of%20a%20video%20sequence%2C%20ensuring%20that%20even%20under%0Alimited%20token%20length%2C%20the%20model%20focuses%20on%20frames%20critical%20for%20spatial%0Areasoning.%20Beyond%20architecture%20improvements%2C%20we%20construct%20the%20Spatial-MLLM-120k%0Adataset%20and%20train%20the%20model%20on%20it%20using%20supervised%20fine-tuning%20and%20GRPO.%0AExtensive%20experiments%20on%20various%20real-world%20datasets%20demonstrate%20that%20our%0Aspatial-MLLM%20achieves%20state-of-the-art%20performance%20in%20a%20wide%20range%20of%0Avisual-based%20spatial%20understanding%20and%20reasoning%20tasks.%20Project%20page%3A%0Ahttps%3A//diankun-wu.github.io/Spatial-MLLM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-MLLM%253A%2520Boosting%2520MLLM%2520Capabilities%2520in%2520Visual-based%2520Spatial%250A%2520%2520Intelligence%26entry.906535625%3DDiankun%2520Wu%2520and%2520Fangfu%2520Liu%2520and%2520Yi-Hsin%2520Hung%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%250Asignificantly%2520enhanced%2520performance%2520on%25202D%2520visual%2520tasks.%2520However%252C%2520improving%2520their%250Aspatial%2520intelligence%2520remains%2520a%2520challenge.%2520Existing%25203D%2520MLLMs%2520always%2520rely%2520on%250Aadditional%25203D%2520or%25202.5D%2520data%2520to%2520incorporate%2520spatial%2520awareness%252C%2520restricting%2520their%250Autility%2520in%2520scenarios%2520with%2520only%25202D%2520inputs%252C%2520such%2520as%2520images%2520or%2520videos.%2520In%2520this%250Apaper%252C%2520we%2520present%2520Spatial-MLLM%252C%2520a%2520novel%2520framework%2520for%2520visual-based%2520spatial%250Areasoning%2520from%2520purely%25202D%2520observations.%2520Unlike%2520conventional%2520video%2520MLLMs%2520which%250Arely%2520on%2520CLIP-based%2520visual%2520encoders%2520optimized%2520for%2520semantic%2520understanding%252C%2520our%250Akey%2520insight%2520is%2520to%2520unleash%2520the%2520strong%2520structure%2520prior%2520from%2520the%2520feed-forward%250Avisual%2520geometry%2520foundation%2520model.%2520Specifically%252C%2520we%2520propose%2520a%2520dual-encoder%250Aarchitecture%253A%2520a%2520pretrained%25202D%2520visual%2520encoder%2520to%2520extract%2520semantic%2520features%252C%2520and%250Aa%2520spatial%2520encoder-initialized%2520from%2520the%2520backbone%2520of%2520the%2520visual%2520geometry%2520model-to%250Aextract%25203D%2520structure%2520features.%2520A%2520connector%2520then%2520integrates%2520both%2520features%2520into%250Aunified%2520visual%2520tokens%2520for%2520enhanced%2520spatial%2520understanding.%2520Furthermore%252C%2520we%250Apropose%2520a%2520space-aware%2520frame%2520sampling%2520strategy%2520at%2520inference%2520time%252C%2520which%2520selects%250Athe%2520spatially%2520informative%2520frames%2520of%2520a%2520video%2520sequence%252C%2520ensuring%2520that%2520even%2520under%250Alimited%2520token%2520length%252C%2520the%2520model%2520focuses%2520on%2520frames%2520critical%2520for%2520spatial%250Areasoning.%2520Beyond%2520architecture%2520improvements%252C%2520we%2520construct%2520the%2520Spatial-MLLM-120k%250Adataset%2520and%2520train%2520the%2520model%2520on%2520it%2520using%2520supervised%2520fine-tuning%2520and%2520GRPO.%250AExtensive%2520experiments%2520on%2520various%2520real-world%2520datasets%2520demonstrate%2520that%2520our%250Aspatial-MLLM%2520achieves%2520state-of-the-art%2520performance%2520in%2520a%2520wide%2520range%2520of%250Avisual-based%2520spatial%2520understanding%2520and%2520reasoning%2520tasks.%2520Project%2520page%253A%250Ahttps%253A//diankun-wu.github.io/Spatial-MLLM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-MLLM%3A%20Boosting%20MLLM%20Capabilities%20in%20Visual-based%20Spatial%0A%20%20Intelligence&entry.906535625=Diankun%20Wu%20and%20Fangfu%20Liu%20and%20Yi-Hsin%20Hung%20and%20Yueqi%20Duan&entry.1292438233=%20%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Asignificantly%20enhanced%20performance%20on%202D%20visual%20tasks.%20However%2C%20improving%20their%0Aspatial%20intelligence%20remains%20a%20challenge.%20Existing%203D%20MLLMs%20always%20rely%20on%0Aadditional%203D%20or%202.5D%20data%20to%20incorporate%20spatial%20awareness%2C%20restricting%20their%0Autility%20in%20scenarios%20with%20only%202D%20inputs%2C%20such%20as%20images%20or%20videos.%20In%20this%0Apaper%2C%20we%20present%20Spatial-MLLM%2C%20a%20novel%20framework%20for%20visual-based%20spatial%0Areasoning%20from%20purely%202D%20observations.%20Unlike%20conventional%20video%20MLLMs%20which%0Arely%20on%20CLIP-based%20visual%20encoders%20optimized%20for%20semantic%20understanding%2C%20our%0Akey%20insight%20is%20to%20unleash%20the%20strong%20structure%20prior%20from%20the%20feed-forward%0Avisual%20geometry%20foundation%20model.%20Specifically%2C%20we%20propose%20a%20dual-encoder%0Aarchitecture%3A%20a%20pretrained%202D%20visual%20encoder%20to%20extract%20semantic%20features%2C%20and%0Aa%20spatial%20encoder-initialized%20from%20the%20backbone%20of%20the%20visual%20geometry%20model-to%0Aextract%203D%20structure%20features.%20A%20connector%20then%20integrates%20both%20features%20into%0Aunified%20visual%20tokens%20for%20enhanced%20spatial%20understanding.%20Furthermore%2C%20we%0Apropose%20a%20space-aware%20frame%20sampling%20strategy%20at%20inference%20time%2C%20which%20selects%0Athe%20spatially%20informative%20frames%20of%20a%20video%20sequence%2C%20ensuring%20that%20even%20under%0Alimited%20token%20length%2C%20the%20model%20focuses%20on%20frames%20critical%20for%20spatial%0Areasoning.%20Beyond%20architecture%20improvements%2C%20we%20construct%20the%20Spatial-MLLM-120k%0Adataset%20and%20train%20the%20model%20on%20it%20using%20supervised%20fine-tuning%20and%20GRPO.%0AExtensive%20experiments%20on%20various%20real-world%20datasets%20demonstrate%20that%20our%0Aspatial-MLLM%20achieves%20state-of-the-art%20performance%20in%20a%20wide%20range%20of%0Avisual-based%20spatial%20understanding%20and%20reasoning%20tasks.%20Project%20page%3A%0Ahttps%3A//diankun-wu.github.io/Spatial-MLLM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23747v1&entry.124074799=Read"},
{"title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views", "author": "Lihan Jiang and Yucheng Mao and Linning Xu and Tao Lu and Kerui Ren and Yichen Jin and Xudong Xu and Mulin Yu and Jiangmiao Pang and Feng Zhao and Dahua Lin and Bo Dai", "abstract": "  We introduce AnySplat, a feed forward network for novel view synthesis from\nuncalibrated image collections. In contrast to traditional neural rendering\npipelines that demand known camera poses and per scene optimization, or recent\nfeed forward methods that buckle under the computational weight of dense views,\nour model predicts everything in one shot. A single forward pass yields a set\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\ncorresponding camera intrinsics and extrinsics for each input image. This\nunified design scales effortlessly to casually captured, multi view datasets\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\nmatches the quality of pose aware baselines in both sparse and dense view\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\nreduce rendering latency compared to optimization based neural fields, bringing\nreal time novel view synthesis within reach for unconstrained capture\nsettings.Project page: https://city-super.github.io/anysplat/\n", "link": "http://arxiv.org/abs/2505.23716v1", "date": "2025-05-29", "relevancy": 3.2191, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6691}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6691}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnySplat%3A%20Feed-forward%203D%20Gaussian%20Splatting%20from%20Unconstrained%20Views&body=Title%3A%20AnySplat%3A%20Feed-forward%203D%20Gaussian%20Splatting%20from%20Unconstrained%20Views%0AAuthor%3A%20Lihan%20Jiang%20and%20Yucheng%20Mao%20and%20Linning%20Xu%20and%20Tao%20Lu%20and%20Kerui%20Ren%20and%20Yichen%20Jin%20and%20Xudong%20Xu%20and%20Mulin%20Yu%20and%20Jiangmiao%20Pang%20and%20Feng%20Zhao%20and%20Dahua%20Lin%20and%20Bo%20Dai%0AAbstract%3A%20%20%20We%20introduce%20AnySplat%2C%20a%20feed%20forward%20network%20for%20novel%20view%20synthesis%20from%0Auncalibrated%20image%20collections.%20In%20contrast%20to%20traditional%20neural%20rendering%0Apipelines%20that%20demand%20known%20camera%20poses%20and%20per%20scene%20optimization%2C%20or%20recent%0Afeed%20forward%20methods%20that%20buckle%20under%20the%20computational%20weight%20of%20dense%20views%2C%0Aour%20model%20predicts%20everything%20in%20one%20shot.%20A%20single%20forward%20pass%20yields%20a%20set%0Aof%203D%20Gaussian%20primitives%20encoding%20both%20scene%20geometry%20and%20appearance%2C%20and%20the%0Acorresponding%20camera%20intrinsics%20and%20extrinsics%20for%20each%20input%20image.%20This%0Aunified%20design%20scales%20effortlessly%20to%20casually%20captured%2C%20multi%20view%20datasets%0Awithout%20any%20pose%20annotations.%20In%20extensive%20zero%20shot%20evaluations%2C%20AnySplat%0Amatches%20the%20quality%20of%20pose%20aware%20baselines%20in%20both%20sparse%20and%20dense%20view%0Ascenarios%20while%20surpassing%20existing%20pose%20free%20approaches.%20Moreover%2C%20it%20greatly%0Areduce%20rendering%20latency%20compared%20to%20optimization%20based%20neural%20fields%2C%20bringing%0Areal%20time%20novel%20view%20synthesis%20within%20reach%20for%20unconstrained%20capture%0Asettings.Project%20page%3A%20https%3A//city-super.github.io/anysplat/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnySplat%253A%2520Feed-forward%25203D%2520Gaussian%2520Splatting%2520from%2520Unconstrained%2520Views%26entry.906535625%3DLihan%2520Jiang%2520and%2520Yucheng%2520Mao%2520and%2520Linning%2520Xu%2520and%2520Tao%2520Lu%2520and%2520Kerui%2520Ren%2520and%2520Yichen%2520Jin%2520and%2520Xudong%2520Xu%2520and%2520Mulin%2520Yu%2520and%2520Jiangmiao%2520Pang%2520and%2520Feng%2520Zhao%2520and%2520Dahua%2520Lin%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520We%2520introduce%2520AnySplat%252C%2520a%2520feed%2520forward%2520network%2520for%2520novel%2520view%2520synthesis%2520from%250Auncalibrated%2520image%2520collections.%2520In%2520contrast%2520to%2520traditional%2520neural%2520rendering%250Apipelines%2520that%2520demand%2520known%2520camera%2520poses%2520and%2520per%2520scene%2520optimization%252C%2520or%2520recent%250Afeed%2520forward%2520methods%2520that%2520buckle%2520under%2520the%2520computational%2520weight%2520of%2520dense%2520views%252C%250Aour%2520model%2520predicts%2520everything%2520in%2520one%2520shot.%2520A%2520single%2520forward%2520pass%2520yields%2520a%2520set%250Aof%25203D%2520Gaussian%2520primitives%2520encoding%2520both%2520scene%2520geometry%2520and%2520appearance%252C%2520and%2520the%250Acorresponding%2520camera%2520intrinsics%2520and%2520extrinsics%2520for%2520each%2520input%2520image.%2520This%250Aunified%2520design%2520scales%2520effortlessly%2520to%2520casually%2520captured%252C%2520multi%2520view%2520datasets%250Awithout%2520any%2520pose%2520annotations.%2520In%2520extensive%2520zero%2520shot%2520evaluations%252C%2520AnySplat%250Amatches%2520the%2520quality%2520of%2520pose%2520aware%2520baselines%2520in%2520both%2520sparse%2520and%2520dense%2520view%250Ascenarios%2520while%2520surpassing%2520existing%2520pose%2520free%2520approaches.%2520Moreover%252C%2520it%2520greatly%250Areduce%2520rendering%2520latency%2520compared%2520to%2520optimization%2520based%2520neural%2520fields%252C%2520bringing%250Areal%2520time%2520novel%2520view%2520synthesis%2520within%2520reach%2520for%2520unconstrained%2520capture%250Asettings.Project%2520page%253A%2520https%253A//city-super.github.io/anysplat/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnySplat%3A%20Feed-forward%203D%20Gaussian%20Splatting%20from%20Unconstrained%20Views&entry.906535625=Lihan%20Jiang%20and%20Yucheng%20Mao%20and%20Linning%20Xu%20and%20Tao%20Lu%20and%20Kerui%20Ren%20and%20Yichen%20Jin%20and%20Xudong%20Xu%20and%20Mulin%20Yu%20and%20Jiangmiao%20Pang%20and%20Feng%20Zhao%20and%20Dahua%20Lin%20and%20Bo%20Dai&entry.1292438233=%20%20We%20introduce%20AnySplat%2C%20a%20feed%20forward%20network%20for%20novel%20view%20synthesis%20from%0Auncalibrated%20image%20collections.%20In%20contrast%20to%20traditional%20neural%20rendering%0Apipelines%20that%20demand%20known%20camera%20poses%20and%20per%20scene%20optimization%2C%20or%20recent%0Afeed%20forward%20methods%20that%20buckle%20under%20the%20computational%20weight%20of%20dense%20views%2C%0Aour%20model%20predicts%20everything%20in%20one%20shot.%20A%20single%20forward%20pass%20yields%20a%20set%0Aof%203D%20Gaussian%20primitives%20encoding%20both%20scene%20geometry%20and%20appearance%2C%20and%20the%0Acorresponding%20camera%20intrinsics%20and%20extrinsics%20for%20each%20input%20image.%20This%0Aunified%20design%20scales%20effortlessly%20to%20casually%20captured%2C%20multi%20view%20datasets%0Awithout%20any%20pose%20annotations.%20In%20extensive%20zero%20shot%20evaluations%2C%20AnySplat%0Amatches%20the%20quality%20of%20pose%20aware%20baselines%20in%20both%20sparse%20and%20dense%20view%0Ascenarios%20while%20surpassing%20existing%20pose%20free%20approaches.%20Moreover%2C%20it%20greatly%0Areduce%20rendering%20latency%20compared%20to%20optimization%20based%20neural%20fields%2C%20bringing%0Areal%20time%20novel%20view%20synthesis%20within%20reach%20for%20unconstrained%20capture%0Asettings.Project%20page%3A%20https%3A//city-super.github.io/anysplat/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23716v1&entry.124074799=Read"},
{"title": "SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual\n  Grounding", "author": "Rong Li and Shijie Li and Lingdong Kong and Xulei Yang and Junwei Liang", "abstract": "  3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on\ntextual descriptions, essential for applications like augmented reality and\nrobotics. Traditional 3DVG approaches rely on annotated 3D datasets and\npredefined object categories, limiting scalability and adaptability. To\novercome these limitations, we introduce SeeGround, a zero-shot 3DVG framework\nleveraging 2D Vision-Language Models (VLMs) trained on large-scale 2D data.\nSeeGround represents 3D scenes as a hybrid of query-aligned rendered images and\nspatially enriched text descriptions, bridging the gap between 3D data and\n2D-VLMs input formats. We propose two modules: the Perspective Adaptation\nModule, which dynamically selects viewpoints for query-relevant image\nrendering, and the Fusion Alignment Module, which integrates 2D images with 3D\nspatial descriptions to enhance object localization. Extensive experiments on\nScanRefer and Nr3D demonstrate that our approach outperforms existing zero-shot\nmethods by large margins. Notably, we exceed weakly supervised methods and\nrival some fully supervised ones, outperforming previous SOTA by 7.7% on\nScanRefer and 7.1% on Nr3D, showcasing its effectiveness in complex 3DVG tasks.\n", "link": "http://arxiv.org/abs/2412.04383v2", "date": "2025-05-29", "relevancy": 3.1223, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6456}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeeGround%3A%20See%20and%20Ground%20for%20Zero-Shot%20Open-Vocabulary%203D%20Visual%0A%20%20Grounding&body=Title%3A%20SeeGround%3A%20See%20and%20Ground%20for%20Zero-Shot%20Open-Vocabulary%203D%20Visual%0A%20%20Grounding%0AAuthor%3A%20Rong%20Li%20and%20Shijie%20Li%20and%20Lingdong%20Kong%20and%20Xulei%20Yang%20and%20Junwei%20Liang%0AAbstract%3A%20%20%203D%20Visual%20Grounding%20%283DVG%29%20aims%20to%20locate%20objects%20in%203D%20scenes%20based%20on%0Atextual%20descriptions%2C%20essential%20for%20applications%20like%20augmented%20reality%20and%0Arobotics.%20Traditional%203DVG%20approaches%20rely%20on%20annotated%203D%20datasets%20and%0Apredefined%20object%20categories%2C%20limiting%20scalability%20and%20adaptability.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20SeeGround%2C%20a%20zero-shot%203DVG%20framework%0Aleveraging%202D%20Vision-Language%20Models%20%28VLMs%29%20trained%20on%20large-scale%202D%20data.%0ASeeGround%20represents%203D%20scenes%20as%20a%20hybrid%20of%20query-aligned%20rendered%20images%20and%0Aspatially%20enriched%20text%20descriptions%2C%20bridging%20the%20gap%20between%203D%20data%20and%0A2D-VLMs%20input%20formats.%20We%20propose%20two%20modules%3A%20the%20Perspective%20Adaptation%0AModule%2C%20which%20dynamically%20selects%20viewpoints%20for%20query-relevant%20image%0Arendering%2C%20and%20the%20Fusion%20Alignment%20Module%2C%20which%20integrates%202D%20images%20with%203D%0Aspatial%20descriptions%20to%20enhance%20object%20localization.%20Extensive%20experiments%20on%0AScanRefer%20and%20Nr3D%20demonstrate%20that%20our%20approach%20outperforms%20existing%20zero-shot%0Amethods%20by%20large%20margins.%20Notably%2C%20we%20exceed%20weakly%20supervised%20methods%20and%0Arival%20some%20fully%20supervised%20ones%2C%20outperforming%20previous%20SOTA%20by%207.7%25%20on%0AScanRefer%20and%207.1%25%20on%20Nr3D%2C%20showcasing%20its%20effectiveness%20in%20complex%203DVG%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeGround%253A%2520See%2520and%2520Ground%2520for%2520Zero-Shot%2520Open-Vocabulary%25203D%2520Visual%250A%2520%2520Grounding%26entry.906535625%3DRong%2520Li%2520and%2520Shijie%2520Li%2520and%2520Lingdong%2520Kong%2520and%2520Xulei%2520Yang%2520and%2520Junwei%2520Liang%26entry.1292438233%3D%2520%25203D%2520Visual%2520Grounding%2520%25283DVG%2529%2520aims%2520to%2520locate%2520objects%2520in%25203D%2520scenes%2520based%2520on%250Atextual%2520descriptions%252C%2520essential%2520for%2520applications%2520like%2520augmented%2520reality%2520and%250Arobotics.%2520Traditional%25203DVG%2520approaches%2520rely%2520on%2520annotated%25203D%2520datasets%2520and%250Apredefined%2520object%2520categories%252C%2520limiting%2520scalability%2520and%2520adaptability.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520introduce%2520SeeGround%252C%2520a%2520zero-shot%25203DVG%2520framework%250Aleveraging%25202D%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520trained%2520on%2520large-scale%25202D%2520data.%250ASeeGround%2520represents%25203D%2520scenes%2520as%2520a%2520hybrid%2520of%2520query-aligned%2520rendered%2520images%2520and%250Aspatially%2520enriched%2520text%2520descriptions%252C%2520bridging%2520the%2520gap%2520between%25203D%2520data%2520and%250A2D-VLMs%2520input%2520formats.%2520We%2520propose%2520two%2520modules%253A%2520the%2520Perspective%2520Adaptation%250AModule%252C%2520which%2520dynamically%2520selects%2520viewpoints%2520for%2520query-relevant%2520image%250Arendering%252C%2520and%2520the%2520Fusion%2520Alignment%2520Module%252C%2520which%2520integrates%25202D%2520images%2520with%25203D%250Aspatial%2520descriptions%2520to%2520enhance%2520object%2520localization.%2520Extensive%2520experiments%2520on%250AScanRefer%2520and%2520Nr3D%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520existing%2520zero-shot%250Amethods%2520by%2520large%2520margins.%2520Notably%252C%2520we%2520exceed%2520weakly%2520supervised%2520methods%2520and%250Arival%2520some%2520fully%2520supervised%2520ones%252C%2520outperforming%2520previous%2520SOTA%2520by%25207.7%2525%2520on%250AScanRefer%2520and%25207.1%2525%2520on%2520Nr3D%252C%2520showcasing%2520its%2520effectiveness%2520in%2520complex%25203DVG%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeeGround%3A%20See%20and%20Ground%20for%20Zero-Shot%20Open-Vocabulary%203D%20Visual%0A%20%20Grounding&entry.906535625=Rong%20Li%20and%20Shijie%20Li%20and%20Lingdong%20Kong%20and%20Xulei%20Yang%20and%20Junwei%20Liang&entry.1292438233=%20%203D%20Visual%20Grounding%20%283DVG%29%20aims%20to%20locate%20objects%20in%203D%20scenes%20based%20on%0Atextual%20descriptions%2C%20essential%20for%20applications%20like%20augmented%20reality%20and%0Arobotics.%20Traditional%203DVG%20approaches%20rely%20on%20annotated%203D%20datasets%20and%0Apredefined%20object%20categories%2C%20limiting%20scalability%20and%20adaptability.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20SeeGround%2C%20a%20zero-shot%203DVG%20framework%0Aleveraging%202D%20Vision-Language%20Models%20%28VLMs%29%20trained%20on%20large-scale%202D%20data.%0ASeeGround%20represents%203D%20scenes%20as%20a%20hybrid%20of%20query-aligned%20rendered%20images%20and%0Aspatially%20enriched%20text%20descriptions%2C%20bridging%20the%20gap%20between%203D%20data%20and%0A2D-VLMs%20input%20formats.%20We%20propose%20two%20modules%3A%20the%20Perspective%20Adaptation%0AModule%2C%20which%20dynamically%20selects%20viewpoints%20for%20query-relevant%20image%0Arendering%2C%20and%20the%20Fusion%20Alignment%20Module%2C%20which%20integrates%202D%20images%20with%203D%0Aspatial%20descriptions%20to%20enhance%20object%20localization.%20Extensive%20experiments%20on%0AScanRefer%20and%20Nr3D%20demonstrate%20that%20our%20approach%20outperforms%20existing%20zero-shot%0Amethods%20by%20large%20margins.%20Notably%2C%20we%20exceed%20weakly%20supervised%20methods%20and%0Arival%20some%20fully%20supervised%20ones%2C%20outperforming%20previous%20SOTA%20by%207.7%25%20on%0AScanRefer%20and%207.1%25%20on%20Nr3D%2C%20showcasing%20its%20effectiveness%20in%20complex%203DVG%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04383v2&entry.124074799=Read"},
{"title": "SIGHT: Synthesizing Image-Text Conditioned and Geometry-Guided 3D\n  Hand-Object Trajectories", "author": "Alexey Gavryushin and Alexandros Delitzas and Luc Van Gool and Marc Pollefeys and Kaichun Mo and Xi Wang", "abstract": "  When humans grasp an object, they naturally form trajectories in their minds\nto manipulate it for specific tasks. Modeling hand-object interaction priors\nholds significant potential to advance robotic and embodied AI systems in\nlearning to operate effectively within the physical world. We introduce SIGHT,\na novel task focused on generating realistic and physically plausible 3D\nhand-object interaction trajectories from a single image and a brief\nlanguage-based task description. Prior work on hand-object trajectory\ngeneration typically relies on textual input that lacks explicit grounding to\nthe target object, or assumes access to 3D object meshes, which are often\nconsiderably more difficult to obtain than 2D images. We propose SIGHT-Fusion,\na novel diffusion-based image-text conditioned generative model that tackles\nthis task by retrieving the most similar 3D object mesh from a database and\nenforcing geometric hand-object interaction constraints via a novel\ninference-time diffusion guidance. We benchmark our model on the HOI4D and H2O\ndatasets, adapting relevant baselines for this novel task. Experiments\ndemonstrate our superior performance in the diversity and quality of generated\ntrajectories, as well as in hand-object interaction geometry metrics.\n", "link": "http://arxiv.org/abs/2503.22869v3", "date": "2025-05-29", "relevancy": 3.1005, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.667}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5985}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIGHT%3A%20Synthesizing%20Image-Text%20Conditioned%20and%20Geometry-Guided%203D%0A%20%20Hand-Object%20Trajectories&body=Title%3A%20SIGHT%3A%20Synthesizing%20Image-Text%20Conditioned%20and%20Geometry-Guided%203D%0A%20%20Hand-Object%20Trajectories%0AAuthor%3A%20Alexey%20Gavryushin%20and%20Alexandros%20Delitzas%20and%20Luc%20Van%20Gool%20and%20Marc%20Pollefeys%20and%20Kaichun%20Mo%20and%20Xi%20Wang%0AAbstract%3A%20%20%20When%20humans%20grasp%20an%20object%2C%20they%20naturally%20form%20trajectories%20in%20their%20minds%0Ato%20manipulate%20it%20for%20specific%20tasks.%20Modeling%20hand-object%20interaction%20priors%0Aholds%20significant%20potential%20to%20advance%20robotic%20and%20embodied%20AI%20systems%20in%0Alearning%20to%20operate%20effectively%20within%20the%20physical%20world.%20We%20introduce%20SIGHT%2C%0Aa%20novel%20task%20focused%20on%20generating%20realistic%20and%20physically%20plausible%203D%0Ahand-object%20interaction%20trajectories%20from%20a%20single%20image%20and%20a%20brief%0Alanguage-based%20task%20description.%20Prior%20work%20on%20hand-object%20trajectory%0Ageneration%20typically%20relies%20on%20textual%20input%20that%20lacks%20explicit%20grounding%20to%0Athe%20target%20object%2C%20or%20assumes%20access%20to%203D%20object%20meshes%2C%20which%20are%20often%0Aconsiderably%20more%20difficult%20to%20obtain%20than%202D%20images.%20We%20propose%20SIGHT-Fusion%2C%0Aa%20novel%20diffusion-based%20image-text%20conditioned%20generative%20model%20that%20tackles%0Athis%20task%20by%20retrieving%20the%20most%20similar%203D%20object%20mesh%20from%20a%20database%20and%0Aenforcing%20geometric%20hand-object%20interaction%20constraints%20via%20a%20novel%0Ainference-time%20diffusion%20guidance.%20We%20benchmark%20our%20model%20on%20the%20HOI4D%20and%20H2O%0Adatasets%2C%20adapting%20relevant%20baselines%20for%20this%20novel%20task.%20Experiments%0Ademonstrate%20our%20superior%20performance%20in%20the%20diversity%20and%20quality%20of%20generated%0Atrajectories%2C%20as%20well%20as%20in%20hand-object%20interaction%20geometry%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22869v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIGHT%253A%2520Synthesizing%2520Image-Text%2520Conditioned%2520and%2520Geometry-Guided%25203D%250A%2520%2520Hand-Object%2520Trajectories%26entry.906535625%3DAlexey%2520Gavryushin%2520and%2520Alexandros%2520Delitzas%2520and%2520Luc%2520Van%2520Gool%2520and%2520Marc%2520Pollefeys%2520and%2520Kaichun%2520Mo%2520and%2520Xi%2520Wang%26entry.1292438233%3D%2520%2520When%2520humans%2520grasp%2520an%2520object%252C%2520they%2520naturally%2520form%2520trajectories%2520in%2520their%2520minds%250Ato%2520manipulate%2520it%2520for%2520specific%2520tasks.%2520Modeling%2520hand-object%2520interaction%2520priors%250Aholds%2520significant%2520potential%2520to%2520advance%2520robotic%2520and%2520embodied%2520AI%2520systems%2520in%250Alearning%2520to%2520operate%2520effectively%2520within%2520the%2520physical%2520world.%2520We%2520introduce%2520SIGHT%252C%250Aa%2520novel%2520task%2520focused%2520on%2520generating%2520realistic%2520and%2520physically%2520plausible%25203D%250Ahand-object%2520interaction%2520trajectories%2520from%2520a%2520single%2520image%2520and%2520a%2520brief%250Alanguage-based%2520task%2520description.%2520Prior%2520work%2520on%2520hand-object%2520trajectory%250Ageneration%2520typically%2520relies%2520on%2520textual%2520input%2520that%2520lacks%2520explicit%2520grounding%2520to%250Athe%2520target%2520object%252C%2520or%2520assumes%2520access%2520to%25203D%2520object%2520meshes%252C%2520which%2520are%2520often%250Aconsiderably%2520more%2520difficult%2520to%2520obtain%2520than%25202D%2520images.%2520We%2520propose%2520SIGHT-Fusion%252C%250Aa%2520novel%2520diffusion-based%2520image-text%2520conditioned%2520generative%2520model%2520that%2520tackles%250Athis%2520task%2520by%2520retrieving%2520the%2520most%2520similar%25203D%2520object%2520mesh%2520from%2520a%2520database%2520and%250Aenforcing%2520geometric%2520hand-object%2520interaction%2520constraints%2520via%2520a%2520novel%250Ainference-time%2520diffusion%2520guidance.%2520We%2520benchmark%2520our%2520model%2520on%2520the%2520HOI4D%2520and%2520H2O%250Adatasets%252C%2520adapting%2520relevant%2520baselines%2520for%2520this%2520novel%2520task.%2520Experiments%250Ademonstrate%2520our%2520superior%2520performance%2520in%2520the%2520diversity%2520and%2520quality%2520of%2520generated%250Atrajectories%252C%2520as%2520well%2520as%2520in%2520hand-object%2520interaction%2520geometry%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22869v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIGHT%3A%20Synthesizing%20Image-Text%20Conditioned%20and%20Geometry-Guided%203D%0A%20%20Hand-Object%20Trajectories&entry.906535625=Alexey%20Gavryushin%20and%20Alexandros%20Delitzas%20and%20Luc%20Van%20Gool%20and%20Marc%20Pollefeys%20and%20Kaichun%20Mo%20and%20Xi%20Wang&entry.1292438233=%20%20When%20humans%20grasp%20an%20object%2C%20they%20naturally%20form%20trajectories%20in%20their%20minds%0Ato%20manipulate%20it%20for%20specific%20tasks.%20Modeling%20hand-object%20interaction%20priors%0Aholds%20significant%20potential%20to%20advance%20robotic%20and%20embodied%20AI%20systems%20in%0Alearning%20to%20operate%20effectively%20within%20the%20physical%20world.%20We%20introduce%20SIGHT%2C%0Aa%20novel%20task%20focused%20on%20generating%20realistic%20and%20physically%20plausible%203D%0Ahand-object%20interaction%20trajectories%20from%20a%20single%20image%20and%20a%20brief%0Alanguage-based%20task%20description.%20Prior%20work%20on%20hand-object%20trajectory%0Ageneration%20typically%20relies%20on%20textual%20input%20that%20lacks%20explicit%20grounding%20to%0Athe%20target%20object%2C%20or%20assumes%20access%20to%203D%20object%20meshes%2C%20which%20are%20often%0Aconsiderably%20more%20difficult%20to%20obtain%20than%202D%20images.%20We%20propose%20SIGHT-Fusion%2C%0Aa%20novel%20diffusion-based%20image-text%20conditioned%20generative%20model%20that%20tackles%0Athis%20task%20by%20retrieving%20the%20most%20similar%203D%20object%20mesh%20from%20a%20database%20and%0Aenforcing%20geometric%20hand-object%20interaction%20constraints%20via%20a%20novel%0Ainference-time%20diffusion%20guidance.%20We%20benchmark%20our%20model%20on%20the%20HOI4D%20and%20H2O%0Adatasets%2C%20adapting%20relevant%20baselines%20for%20this%20novel%20task.%20Experiments%0Ademonstrate%20our%20superior%20performance%20in%20the%20diversity%20and%20quality%20of%20generated%0Atrajectories%2C%20as%20well%20as%20in%20hand-object%20interaction%20geometry%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22869v3&entry.124074799=Read"},
{"title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS", "author": "Weijie Wang and Donny Y. Chen and Zeyu Zhang and Duochao Shi and Akide Liu and Bohan Zhuang", "abstract": "  Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor.\n", "link": "http://arxiv.org/abs/2505.23734v1", "date": "2025-05-29", "relevancy": 3.0603, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6214}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6179}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZPressor%3A%20Bottleneck-Aware%20Compression%20for%20Scalable%20Feed-Forward%203DGS&body=Title%3A%20ZPressor%3A%20Bottleneck-Aware%20Compression%20for%20Scalable%20Feed-Forward%203DGS%0AAuthor%3A%20Weijie%20Wang%20and%20Donny%20Y.%20Chen%20and%20Zeyu%20Zhang%20and%20Duochao%20Shi%20and%20Akide%20Liu%20and%20Bohan%20Zhuang%0AAbstract%3A%20%20%20Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20models%20have%20recently%20emerged%20as%20a%0Apromising%20solution%20for%20novel%20view%20synthesis%2C%20enabling%20one-pass%20inference%0Awithout%20the%20need%20for%20per-scene%203DGS%20optimization.%20However%2C%20their%20scalability%20is%0Afundamentally%20constrained%20by%20the%20limited%20capacity%20of%20their%20encoders%2C%20leading%20to%0Adegraded%20performance%20or%20excessive%20memory%20consumption%20as%20the%20number%20of%20input%0Aviews%20increases.%20In%20this%20work%2C%20we%20analyze%20feed-forward%203DGS%20frameworks%20through%0Athe%20lens%20of%20the%20Information%20Bottleneck%20principle%20and%20introduce%20ZPressor%2C%20a%0Alightweight%20architecture-agnostic%20module%20that%20enables%20efficient%20compression%20of%0Amulti-view%20inputs%20into%20a%20compact%20latent%20state%20%24Z%24%20that%20retains%20essential%20scene%0Ainformation%20while%20discarding%20redundancy.%20Concretely%2C%20ZPressor%20enables%20existing%0Afeed-forward%203DGS%20models%20to%20scale%20to%20over%20100%20input%20views%20at%20480P%20resolution%20on%0Aan%2080GB%20GPU%2C%20by%20partitioning%20the%20views%20into%20anchor%20and%20support%20sets%20and%20using%0Across%20attention%20to%20compress%20the%20information%20from%20the%20support%20views%20into%20anchor%0Aviews%2C%20forming%20the%20compressed%20latent%20state%20%24Z%24.%20We%20show%20that%20integrating%0AZPressor%20into%20several%20state-of-the-art%20feed-forward%203DGS%20models%20consistently%0Aimproves%20performance%20under%20moderate%20input%20views%20and%20enhances%20robustness%20under%0Adense%20view%20settings%20on%20two%20large-scale%20benchmarks%20DL3DV-10K%20and%20RealEstate10K.%0AThe%20video%20results%2C%20code%20and%20trained%20models%20are%20available%20on%20our%20project%20page%3A%0Ahttps%3A//lhmd.top/zpressor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZPressor%253A%2520Bottleneck-Aware%2520Compression%2520for%2520Scalable%2520Feed-Forward%25203DGS%26entry.906535625%3DWeijie%2520Wang%2520and%2520Donny%2520Y.%2520Chen%2520and%2520Zeyu%2520Zhang%2520and%2520Duochao%2520Shi%2520and%2520Akide%2520Liu%2520and%2520Bohan%2520Zhuang%26entry.1292438233%3D%2520%2520Feed-forward%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520models%2520have%2520recently%2520emerged%2520as%2520a%250Apromising%2520solution%2520for%2520novel%2520view%2520synthesis%252C%2520enabling%2520one-pass%2520inference%250Awithout%2520the%2520need%2520for%2520per-scene%25203DGS%2520optimization.%2520However%252C%2520their%2520scalability%2520is%250Afundamentally%2520constrained%2520by%2520the%2520limited%2520capacity%2520of%2520their%2520encoders%252C%2520leading%2520to%250Adegraded%2520performance%2520or%2520excessive%2520memory%2520consumption%2520as%2520the%2520number%2520of%2520input%250Aviews%2520increases.%2520In%2520this%2520work%252C%2520we%2520analyze%2520feed-forward%25203DGS%2520frameworks%2520through%250Athe%2520lens%2520of%2520the%2520Information%2520Bottleneck%2520principle%2520and%2520introduce%2520ZPressor%252C%2520a%250Alightweight%2520architecture-agnostic%2520module%2520that%2520enables%2520efficient%2520compression%2520of%250Amulti-view%2520inputs%2520into%2520a%2520compact%2520latent%2520state%2520%2524Z%2524%2520that%2520retains%2520essential%2520scene%250Ainformation%2520while%2520discarding%2520redundancy.%2520Concretely%252C%2520ZPressor%2520enables%2520existing%250Afeed-forward%25203DGS%2520models%2520to%2520scale%2520to%2520over%2520100%2520input%2520views%2520at%2520480P%2520resolution%2520on%250Aan%252080GB%2520GPU%252C%2520by%2520partitioning%2520the%2520views%2520into%2520anchor%2520and%2520support%2520sets%2520and%2520using%250Across%2520attention%2520to%2520compress%2520the%2520information%2520from%2520the%2520support%2520views%2520into%2520anchor%250Aviews%252C%2520forming%2520the%2520compressed%2520latent%2520state%2520%2524Z%2524.%2520We%2520show%2520that%2520integrating%250AZPressor%2520into%2520several%2520state-of-the-art%2520feed-forward%25203DGS%2520models%2520consistently%250Aimproves%2520performance%2520under%2520moderate%2520input%2520views%2520and%2520enhances%2520robustness%2520under%250Adense%2520view%2520settings%2520on%2520two%2520large-scale%2520benchmarks%2520DL3DV-10K%2520and%2520RealEstate10K.%250AThe%2520video%2520results%252C%2520code%2520and%2520trained%2520models%2520are%2520available%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//lhmd.top/zpressor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZPressor%3A%20Bottleneck-Aware%20Compression%20for%20Scalable%20Feed-Forward%203DGS&entry.906535625=Weijie%20Wang%20and%20Donny%20Y.%20Chen%20and%20Zeyu%20Zhang%20and%20Duochao%20Shi%20and%20Akide%20Liu%20and%20Bohan%20Zhuang&entry.1292438233=%20%20Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20models%20have%20recently%20emerged%20as%20a%0Apromising%20solution%20for%20novel%20view%20synthesis%2C%20enabling%20one-pass%20inference%0Awithout%20the%20need%20for%20per-scene%203DGS%20optimization.%20However%2C%20their%20scalability%20is%0Afundamentally%20constrained%20by%20the%20limited%20capacity%20of%20their%20encoders%2C%20leading%20to%0Adegraded%20performance%20or%20excessive%20memory%20consumption%20as%20the%20number%20of%20input%0Aviews%20increases.%20In%20this%20work%2C%20we%20analyze%20feed-forward%203DGS%20frameworks%20through%0Athe%20lens%20of%20the%20Information%20Bottleneck%20principle%20and%20introduce%20ZPressor%2C%20a%0Alightweight%20architecture-agnostic%20module%20that%20enables%20efficient%20compression%20of%0Amulti-view%20inputs%20into%20a%20compact%20latent%20state%20%24Z%24%20that%20retains%20essential%20scene%0Ainformation%20while%20discarding%20redundancy.%20Concretely%2C%20ZPressor%20enables%20existing%0Afeed-forward%203DGS%20models%20to%20scale%20to%20over%20100%20input%20views%20at%20480P%20resolution%20on%0Aan%2080GB%20GPU%2C%20by%20partitioning%20the%20views%20into%20anchor%20and%20support%20sets%20and%20using%0Across%20attention%20to%20compress%20the%20information%20from%20the%20support%20views%20into%20anchor%0Aviews%2C%20forming%20the%20compressed%20latent%20state%20%24Z%24.%20We%20show%20that%20integrating%0AZPressor%20into%20several%20state-of-the-art%20feed-forward%203DGS%20models%20consistently%0Aimproves%20performance%20under%20moderate%20input%20views%20and%20enhances%20robustness%20under%0Adense%20view%20settings%20on%20two%20large-scale%20benchmarks%20DL3DV-10K%20and%20RealEstate10K.%0AThe%20video%20results%2C%20code%20and%20trained%20models%20are%20available%20on%20our%20project%20page%3A%0Ahttps%3A//lhmd.top/zpressor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23734v1&entry.124074799=Read"},
{"title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics\n  Based Appearance-Medium Decoupling", "author": "Jieyu Yuan and Yujun Li and Yuanlin Zhang and Chunle Guo and Xiongxin Tang and Ruixing Wang and Chongyi Li", "abstract": "  Novel view synthesis for underwater scene reconstruction presents unique\nchallenges due to complex light-media interactions. Optical scattering and\nabsorption in water body bring inhomogeneous medium attenuation interference\nthat disrupts conventional volume rendering assumptions of uniform propagation\nmedium. While 3D Gaussian Splatting (3DGS) offers real-time rendering\ncapabilities, it struggles with underwater inhomogeneous environments where\nscattering media introduce artifacts and inconsistent appearance. In this\nstudy, we propose a physics-based framework that disentangles object appearance\nfrom water medium effects through tailored Gaussian modeling. Our approach\nintroduces appearance embeddings, which are explicit medium representations for\nbackscatter and attenuation, enhancing scene consistency. In addition, we\npropose a distance-guided optimization strategy that leverages pseudo-depth\nmaps as supervision with depth regularization and scale penalty terms to\nimprove geometric fidelity. By integrating the proposed appearance and medium\nmodeling components via an underwater imaging model, our approach achieves both\nhigh-quality novel view synthesis and physically accurate scene restoration.\nExperiments demonstrate our significant improvements in rendering quality and\nrestoration accuracy over existing methods. The project page is available at\nhttps://bilityniu.github.io/3D-UIR.\n", "link": "http://arxiv.org/abs/2505.21238v2", "date": "2025-05-29", "relevancy": 3.0581, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6144}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6102}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-UIR%3A%203D%20Gaussian%20for%20Underwater%203D%20Scene%20Reconstruction%20via%20Physics%0A%20%20Based%20Appearance-Medium%20Decoupling&body=Title%3A%203D-UIR%3A%203D%20Gaussian%20for%20Underwater%203D%20Scene%20Reconstruction%20via%20Physics%0A%20%20Based%20Appearance-Medium%20Decoupling%0AAuthor%3A%20Jieyu%20Yuan%20and%20Yujun%20Li%20and%20Yuanlin%20Zhang%20and%20Chunle%20Guo%20and%20Xiongxin%20Tang%20and%20Ruixing%20Wang%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20for%20underwater%20scene%20reconstruction%20presents%20unique%0Achallenges%20due%20to%20complex%20light-media%20interactions.%20Optical%20scattering%20and%0Aabsorption%20in%20water%20body%20bring%20inhomogeneous%20medium%20attenuation%20interference%0Athat%20disrupts%20conventional%20volume%20rendering%20assumptions%20of%20uniform%20propagation%0Amedium.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%20real-time%20rendering%0Acapabilities%2C%20it%20struggles%20with%20underwater%20inhomogeneous%20environments%20where%0Ascattering%20media%20introduce%20artifacts%20and%20inconsistent%20appearance.%20In%20this%0Astudy%2C%20we%20propose%20a%20physics-based%20framework%20that%20disentangles%20object%20appearance%0Afrom%20water%20medium%20effects%20through%20tailored%20Gaussian%20modeling.%20Our%20approach%0Aintroduces%20appearance%20embeddings%2C%20which%20are%20explicit%20medium%20representations%20for%0Abackscatter%20and%20attenuation%2C%20enhancing%20scene%20consistency.%20In%20addition%2C%20we%0Apropose%20a%20distance-guided%20optimization%20strategy%20that%20leverages%20pseudo-depth%0Amaps%20as%20supervision%20with%20depth%20regularization%20and%20scale%20penalty%20terms%20to%0Aimprove%20geometric%20fidelity.%20By%20integrating%20the%20proposed%20appearance%20and%20medium%0Amodeling%20components%20via%20an%20underwater%20imaging%20model%2C%20our%20approach%20achieves%20both%0Ahigh-quality%20novel%20view%20synthesis%20and%20physically%20accurate%20scene%20restoration.%0AExperiments%20demonstrate%20our%20significant%20improvements%20in%20rendering%20quality%20and%0Arestoration%20accuracy%20over%20existing%20methods.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//bilityniu.github.io/3D-UIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-UIR%253A%25203D%2520Gaussian%2520for%2520Underwater%25203D%2520Scene%2520Reconstruction%2520via%2520Physics%250A%2520%2520Based%2520Appearance-Medium%2520Decoupling%26entry.906535625%3DJieyu%2520Yuan%2520and%2520Yujun%2520Li%2520and%2520Yuanlin%2520Zhang%2520and%2520Chunle%2520Guo%2520and%2520Xiongxin%2520Tang%2520and%2520Ruixing%2520Wang%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520for%2520underwater%2520scene%2520reconstruction%2520presents%2520unique%250Achallenges%2520due%2520to%2520complex%2520light-media%2520interactions.%2520Optical%2520scattering%2520and%250Aabsorption%2520in%2520water%2520body%2520bring%2520inhomogeneous%2520medium%2520attenuation%2520interference%250Athat%2520disrupts%2520conventional%2520volume%2520rendering%2520assumptions%2520of%2520uniform%2520propagation%250Amedium.%2520While%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520offers%2520real-time%2520rendering%250Acapabilities%252C%2520it%2520struggles%2520with%2520underwater%2520inhomogeneous%2520environments%2520where%250Ascattering%2520media%2520introduce%2520artifacts%2520and%2520inconsistent%2520appearance.%2520In%2520this%250Astudy%252C%2520we%2520propose%2520a%2520physics-based%2520framework%2520that%2520disentangles%2520object%2520appearance%250Afrom%2520water%2520medium%2520effects%2520through%2520tailored%2520Gaussian%2520modeling.%2520Our%2520approach%250Aintroduces%2520appearance%2520embeddings%252C%2520which%2520are%2520explicit%2520medium%2520representations%2520for%250Abackscatter%2520and%2520attenuation%252C%2520enhancing%2520scene%2520consistency.%2520In%2520addition%252C%2520we%250Apropose%2520a%2520distance-guided%2520optimization%2520strategy%2520that%2520leverages%2520pseudo-depth%250Amaps%2520as%2520supervision%2520with%2520depth%2520regularization%2520and%2520scale%2520penalty%2520terms%2520to%250Aimprove%2520geometric%2520fidelity.%2520By%2520integrating%2520the%2520proposed%2520appearance%2520and%2520medium%250Amodeling%2520components%2520via%2520an%2520underwater%2520imaging%2520model%252C%2520our%2520approach%2520achieves%2520both%250Ahigh-quality%2520novel%2520view%2520synthesis%2520and%2520physically%2520accurate%2520scene%2520restoration.%250AExperiments%2520demonstrate%2520our%2520significant%2520improvements%2520in%2520rendering%2520quality%2520and%250Arestoration%2520accuracy%2520over%2520existing%2520methods.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//bilityniu.github.io/3D-UIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-UIR%3A%203D%20Gaussian%20for%20Underwater%203D%20Scene%20Reconstruction%20via%20Physics%0A%20%20Based%20Appearance-Medium%20Decoupling&entry.906535625=Jieyu%20Yuan%20and%20Yujun%20Li%20and%20Yuanlin%20Zhang%20and%20Chunle%20Guo%20and%20Xiongxin%20Tang%20and%20Ruixing%20Wang%20and%20Chongyi%20Li&entry.1292438233=%20%20Novel%20view%20synthesis%20for%20underwater%20scene%20reconstruction%20presents%20unique%0Achallenges%20due%20to%20complex%20light-media%20interactions.%20Optical%20scattering%20and%0Aabsorption%20in%20water%20body%20bring%20inhomogeneous%20medium%20attenuation%20interference%0Athat%20disrupts%20conventional%20volume%20rendering%20assumptions%20of%20uniform%20propagation%0Amedium.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%20real-time%20rendering%0Acapabilities%2C%20it%20struggles%20with%20underwater%20inhomogeneous%20environments%20where%0Ascattering%20media%20introduce%20artifacts%20and%20inconsistent%20appearance.%20In%20this%0Astudy%2C%20we%20propose%20a%20physics-based%20framework%20that%20disentangles%20object%20appearance%0Afrom%20water%20medium%20effects%20through%20tailored%20Gaussian%20modeling.%20Our%20approach%0Aintroduces%20appearance%20embeddings%2C%20which%20are%20explicit%20medium%20representations%20for%0Abackscatter%20and%20attenuation%2C%20enhancing%20scene%20consistency.%20In%20addition%2C%20we%0Apropose%20a%20distance-guided%20optimization%20strategy%20that%20leverages%20pseudo-depth%0Amaps%20as%20supervision%20with%20depth%20regularization%20and%20scale%20penalty%20terms%20to%0Aimprove%20geometric%20fidelity.%20By%20integrating%20the%20proposed%20appearance%20and%20medium%0Amodeling%20components%20via%20an%20underwater%20imaging%20model%2C%20our%20approach%20achieves%20both%0Ahigh-quality%20novel%20view%20synthesis%20and%20physically%20accurate%20scene%20restoration.%0AExperiments%20demonstrate%20our%20significant%20improvements%20in%20rendering%20quality%20and%0Arestoration%20accuracy%20over%20existing%20methods.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//bilityniu.github.io/3D-UIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21238v2&entry.124074799=Read"},
{"title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with\n  Fine-grained Multi-axis Perception Tasks", "author": "Keanu Nichols and Nazia Tasnim and Yuting Yan and Nicholas Ikechukwu and Elva Zou and Deepti Ghadiyaram and Bryan A. Plummer", "abstract": "  Object orientation understanding represents a fundamental challenge in visual\nperception critical for applications like robotic manipulation and augmented\nreality. Current vision-language benchmarks fail to isolate this capability,\noften conflating it with positional relationships and general scene\nunderstanding. We introduce DORI (Discriminative Orientation Reasoning\nIntelligence), a comprehensive benchmark establishing object orientation\nperception as a primary evaluation target. DORI assesses four dimensions of\norientation comprehension: frontal alignment, rotational transformations,\nrelative directional relationships, and canonical orientation understanding.\nThrough carefully curated tasks from 11 datasets spanning 67 object categories\nacross synthetic and real-world scenarios, DORI provides insights on how\nmulti-modal systems understand object orientations. Our evaluation of 15\nstate-of-the-art vision-language models reveals critical limitations: even the\nbest models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular\norientation judgments, with performance deteriorating for tasks requiring\nreference frame shifts or compound rotations. These findings demonstrate the\nneed for dedicated orientation representation mechanisms, as models show\nsystematic inability to perform precise angular estimations, track orientation\nchanges across viewpoints, and understand compound rotations - suggesting\nlimitations in their internal 3D spatial representations. As the first\ndiagnostic framework specifically designed for orientation awareness in\nmultimodal systems, DORI offers implications for improving robotic control, 3D\nscene reconstruction, and human-AI interaction in physical environments. DORI\ndata: https://huggingface.co/datasets/appledora/DORI-Benchmark\n", "link": "http://arxiv.org/abs/2505.21649v2", "date": "2025-05-29", "relevancy": 3.053, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6248}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Right%20Side%20Up%3F%20Disentangling%20Orientation%20Understanding%20in%20MLLMs%20with%0A%20%20Fine-grained%20Multi-axis%20Perception%20Tasks&body=Title%3A%20Right%20Side%20Up%3F%20Disentangling%20Orientation%20Understanding%20in%20MLLMs%20with%0A%20%20Fine-grained%20Multi-axis%20Perception%20Tasks%0AAuthor%3A%20Keanu%20Nichols%20and%20Nazia%20Tasnim%20and%20Yuting%20Yan%20and%20Nicholas%20Ikechukwu%20and%20Elva%20Zou%20and%20Deepti%20Ghadiyaram%20and%20Bryan%20A.%20Plummer%0AAbstract%3A%20%20%20Object%20orientation%20understanding%20represents%20a%20fundamental%20challenge%20in%20visual%0Aperception%20critical%20for%20applications%20like%20robotic%20manipulation%20and%20augmented%0Areality.%20Current%20vision-language%20benchmarks%20fail%20to%20isolate%20this%20capability%2C%0Aoften%20conflating%20it%20with%20positional%20relationships%20and%20general%20scene%0Aunderstanding.%20We%20introduce%20DORI%20%28Discriminative%20Orientation%20Reasoning%0AIntelligence%29%2C%20a%20comprehensive%20benchmark%20establishing%20object%20orientation%0Aperception%20as%20a%20primary%20evaluation%20target.%20DORI%20assesses%20four%20dimensions%20of%0Aorientation%20comprehension%3A%20frontal%20alignment%2C%20rotational%20transformations%2C%0Arelative%20directional%20relationships%2C%20and%20canonical%20orientation%20understanding.%0AThrough%20carefully%20curated%20tasks%20from%2011%20datasets%20spanning%2067%20object%20categories%0Aacross%20synthetic%20and%20real-world%20scenarios%2C%20DORI%20provides%20insights%20on%20how%0Amulti-modal%20systems%20understand%20object%20orientations.%20Our%20evaluation%20of%2015%0Astate-of-the-art%20vision-language%20models%20reveals%20critical%20limitations%3A%20even%20the%0Abest%20models%20achieve%20only%2054.2%25%20accuracy%20on%20coarse%20tasks%20and%2033.0%25%20on%20granular%0Aorientation%20judgments%2C%20with%20performance%20deteriorating%20for%20tasks%20requiring%0Areference%20frame%20shifts%20or%20compound%20rotations.%20These%20findings%20demonstrate%20the%0Aneed%20for%20dedicated%20orientation%20representation%20mechanisms%2C%20as%20models%20show%0Asystematic%20inability%20to%20perform%20precise%20angular%20estimations%2C%20track%20orientation%0Achanges%20across%20viewpoints%2C%20and%20understand%20compound%20rotations%20-%20suggesting%0Alimitations%20in%20their%20internal%203D%20spatial%20representations.%20As%20the%20first%0Adiagnostic%20framework%20specifically%20designed%20for%20orientation%20awareness%20in%0Amultimodal%20systems%2C%20DORI%20offers%20implications%20for%20improving%20robotic%20control%2C%203D%0Ascene%20reconstruction%2C%20and%20human-AI%20interaction%20in%20physical%20environments.%20DORI%0Adata%3A%20https%3A//huggingface.co/datasets/appledora/DORI-Benchmark%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21649v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRight%2520Side%2520Up%253F%2520Disentangling%2520Orientation%2520Understanding%2520in%2520MLLMs%2520with%250A%2520%2520Fine-grained%2520Multi-axis%2520Perception%2520Tasks%26entry.906535625%3DKeanu%2520Nichols%2520and%2520Nazia%2520Tasnim%2520and%2520Yuting%2520Yan%2520and%2520Nicholas%2520Ikechukwu%2520and%2520Elva%2520Zou%2520and%2520Deepti%2520Ghadiyaram%2520and%2520Bryan%2520A.%2520Plummer%26entry.1292438233%3D%2520%2520Object%2520orientation%2520understanding%2520represents%2520a%2520fundamental%2520challenge%2520in%2520visual%250Aperception%2520critical%2520for%2520applications%2520like%2520robotic%2520manipulation%2520and%2520augmented%250Areality.%2520Current%2520vision-language%2520benchmarks%2520fail%2520to%2520isolate%2520this%2520capability%252C%250Aoften%2520conflating%2520it%2520with%2520positional%2520relationships%2520and%2520general%2520scene%250Aunderstanding.%2520We%2520introduce%2520DORI%2520%2528Discriminative%2520Orientation%2520Reasoning%250AIntelligence%2529%252C%2520a%2520comprehensive%2520benchmark%2520establishing%2520object%2520orientation%250Aperception%2520as%2520a%2520primary%2520evaluation%2520target.%2520DORI%2520assesses%2520four%2520dimensions%2520of%250Aorientation%2520comprehension%253A%2520frontal%2520alignment%252C%2520rotational%2520transformations%252C%250Arelative%2520directional%2520relationships%252C%2520and%2520canonical%2520orientation%2520understanding.%250AThrough%2520carefully%2520curated%2520tasks%2520from%252011%2520datasets%2520spanning%252067%2520object%2520categories%250Aacross%2520synthetic%2520and%2520real-world%2520scenarios%252C%2520DORI%2520provides%2520insights%2520on%2520how%250Amulti-modal%2520systems%2520understand%2520object%2520orientations.%2520Our%2520evaluation%2520of%252015%250Astate-of-the-art%2520vision-language%2520models%2520reveals%2520critical%2520limitations%253A%2520even%2520the%250Abest%2520models%2520achieve%2520only%252054.2%2525%2520accuracy%2520on%2520coarse%2520tasks%2520and%252033.0%2525%2520on%2520granular%250Aorientation%2520judgments%252C%2520with%2520performance%2520deteriorating%2520for%2520tasks%2520requiring%250Areference%2520frame%2520shifts%2520or%2520compound%2520rotations.%2520These%2520findings%2520demonstrate%2520the%250Aneed%2520for%2520dedicated%2520orientation%2520representation%2520mechanisms%252C%2520as%2520models%2520show%250Asystematic%2520inability%2520to%2520perform%2520precise%2520angular%2520estimations%252C%2520track%2520orientation%250Achanges%2520across%2520viewpoints%252C%2520and%2520understand%2520compound%2520rotations%2520-%2520suggesting%250Alimitations%2520in%2520their%2520internal%25203D%2520spatial%2520representations.%2520As%2520the%2520first%250Adiagnostic%2520framework%2520specifically%2520designed%2520for%2520orientation%2520awareness%2520in%250Amultimodal%2520systems%252C%2520DORI%2520offers%2520implications%2520for%2520improving%2520robotic%2520control%252C%25203D%250Ascene%2520reconstruction%252C%2520and%2520human-AI%2520interaction%2520in%2520physical%2520environments.%2520DORI%250Adata%253A%2520https%253A//huggingface.co/datasets/appledora/DORI-Benchmark%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21649v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Right%20Side%20Up%3F%20Disentangling%20Orientation%20Understanding%20in%20MLLMs%20with%0A%20%20Fine-grained%20Multi-axis%20Perception%20Tasks&entry.906535625=Keanu%20Nichols%20and%20Nazia%20Tasnim%20and%20Yuting%20Yan%20and%20Nicholas%20Ikechukwu%20and%20Elva%20Zou%20and%20Deepti%20Ghadiyaram%20and%20Bryan%20A.%20Plummer&entry.1292438233=%20%20Object%20orientation%20understanding%20represents%20a%20fundamental%20challenge%20in%20visual%0Aperception%20critical%20for%20applications%20like%20robotic%20manipulation%20and%20augmented%0Areality.%20Current%20vision-language%20benchmarks%20fail%20to%20isolate%20this%20capability%2C%0Aoften%20conflating%20it%20with%20positional%20relationships%20and%20general%20scene%0Aunderstanding.%20We%20introduce%20DORI%20%28Discriminative%20Orientation%20Reasoning%0AIntelligence%29%2C%20a%20comprehensive%20benchmark%20establishing%20object%20orientation%0Aperception%20as%20a%20primary%20evaluation%20target.%20DORI%20assesses%20four%20dimensions%20of%0Aorientation%20comprehension%3A%20frontal%20alignment%2C%20rotational%20transformations%2C%0Arelative%20directional%20relationships%2C%20and%20canonical%20orientation%20understanding.%0AThrough%20carefully%20curated%20tasks%20from%2011%20datasets%20spanning%2067%20object%20categories%0Aacross%20synthetic%20and%20real-world%20scenarios%2C%20DORI%20provides%20insights%20on%20how%0Amulti-modal%20systems%20understand%20object%20orientations.%20Our%20evaluation%20of%2015%0Astate-of-the-art%20vision-language%20models%20reveals%20critical%20limitations%3A%20even%20the%0Abest%20models%20achieve%20only%2054.2%25%20accuracy%20on%20coarse%20tasks%20and%2033.0%25%20on%20granular%0Aorientation%20judgments%2C%20with%20performance%20deteriorating%20for%20tasks%20requiring%0Areference%20frame%20shifts%20or%20compound%20rotations.%20These%20findings%20demonstrate%20the%0Aneed%20for%20dedicated%20orientation%20representation%20mechanisms%2C%20as%20models%20show%0Asystematic%20inability%20to%20perform%20precise%20angular%20estimations%2C%20track%20orientation%0Achanges%20across%20viewpoints%2C%20and%20understand%20compound%20rotations%20-%20suggesting%0Alimitations%20in%20their%20internal%203D%20spatial%20representations.%20As%20the%20first%0Adiagnostic%20framework%20specifically%20designed%20for%20orientation%20awareness%20in%0Amultimodal%20systems%2C%20DORI%20offers%20implications%20for%20improving%20robotic%20control%2C%203D%0Ascene%20reconstruction%2C%20and%20human-AI%20interaction%20in%20physical%20environments.%20DORI%0Adata%3A%20https%3A//huggingface.co/datasets/appledora/DORI-Benchmark%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21649v2&entry.124074799=Read"},
{"title": "Weakly-supervised Localization of Manipulated Image Regions Using\n  Multi-resolution Learned Features", "author": "Ziyong Wang and Charith Abhayaratne", "abstract": "  The explosive growth of digital images and the widespread availability of\nimage editing tools have made image manipulation detection an increasingly\ncritical challenge. Current deep learning-based manipulation detection methods\nexcel in achieving high image-level classification accuracy, they often fall\nshort in terms of interpretability and localization of manipulated regions.\nAdditionally, the absence of pixel-wise annotations in real-world scenarios\nlimits the existing fully-supervised manipulation localization techniques. To\naddress these challenges, we propose a novel weakly-supervised approach that\nintegrates activation maps generated by image-level manipulation detection\nnetworks with segmentation maps from pre-trained models. Specifically, we build\non our previous image-level work named WCBnet to produce multi-view feature\nmaps which are subsequently fused for coarse localization. These coarse maps\nare then refined using detailed segmented regional information provided by\npre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet),\nwith Bayesian inference employed to enhance the manipulation localization.\nExperimental results demonstrate the effectiveness of our approach,\nhighlighting the feasibility to localize image manipulations without relying on\npixel-level labels.\n", "link": "http://arxiv.org/abs/2505.23586v1", "date": "2025-05-29", "relevancy": 2.9811, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6276}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5861}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly-supervised%20Localization%20of%20Manipulated%20Image%20Regions%20Using%0A%20%20Multi-resolution%20Learned%20Features&body=Title%3A%20Weakly-supervised%20Localization%20of%20Manipulated%20Image%20Regions%20Using%0A%20%20Multi-resolution%20Learned%20Features%0AAuthor%3A%20Ziyong%20Wang%20and%20Charith%20Abhayaratne%0AAbstract%3A%20%20%20The%20explosive%20growth%20of%20digital%20images%20and%20the%20widespread%20availability%20of%0Aimage%20editing%20tools%20have%20made%20image%20manipulation%20detection%20an%20increasingly%0Acritical%20challenge.%20Current%20deep%20learning-based%20manipulation%20detection%20methods%0Aexcel%20in%20achieving%20high%20image-level%20classification%20accuracy%2C%20they%20often%20fall%0Ashort%20in%20terms%20of%20interpretability%20and%20localization%20of%20manipulated%20regions.%0AAdditionally%2C%20the%20absence%20of%20pixel-wise%20annotations%20in%20real-world%20scenarios%0Alimits%20the%20existing%20fully-supervised%20manipulation%20localization%20techniques.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20weakly-supervised%20approach%20that%0Aintegrates%20activation%20maps%20generated%20by%20image-level%20manipulation%20detection%0Anetworks%20with%20segmentation%20maps%20from%20pre-trained%20models.%20Specifically%2C%20we%20build%0Aon%20our%20previous%20image-level%20work%20named%20WCBnet%20to%20produce%20multi-view%20feature%0Amaps%20which%20are%20subsequently%20fused%20for%20coarse%20localization.%20These%20coarse%20maps%0Aare%20then%20refined%20using%20detailed%20segmented%20regional%20information%20provided%20by%0Apre-trained%20segmentation%20models%20%28such%20as%20DeepLab%2C%20SegmentAnything%20and%20PSPnet%29%2C%0Awith%20Bayesian%20inference%20employed%20to%20enhance%20the%20manipulation%20localization.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Ahighlighting%20the%20feasibility%20to%20localize%20image%20manipulations%20without%20relying%20on%0Apixel-level%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly-supervised%2520Localization%2520of%2520Manipulated%2520Image%2520Regions%2520Using%250A%2520%2520Multi-resolution%2520Learned%2520Features%26entry.906535625%3DZiyong%2520Wang%2520and%2520Charith%2520Abhayaratne%26entry.1292438233%3D%2520%2520The%2520explosive%2520growth%2520of%2520digital%2520images%2520and%2520the%2520widespread%2520availability%2520of%250Aimage%2520editing%2520tools%2520have%2520made%2520image%2520manipulation%2520detection%2520an%2520increasingly%250Acritical%2520challenge.%2520Current%2520deep%2520learning-based%2520manipulation%2520detection%2520methods%250Aexcel%2520in%2520achieving%2520high%2520image-level%2520classification%2520accuracy%252C%2520they%2520often%2520fall%250Ashort%2520in%2520terms%2520of%2520interpretability%2520and%2520localization%2520of%2520manipulated%2520regions.%250AAdditionally%252C%2520the%2520absence%2520of%2520pixel-wise%2520annotations%2520in%2520real-world%2520scenarios%250Alimits%2520the%2520existing%2520fully-supervised%2520manipulation%2520localization%2520techniques.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520weakly-supervised%2520approach%2520that%250Aintegrates%2520activation%2520maps%2520generated%2520by%2520image-level%2520manipulation%2520detection%250Anetworks%2520with%2520segmentation%2520maps%2520from%2520pre-trained%2520models.%2520Specifically%252C%2520we%2520build%250Aon%2520our%2520previous%2520image-level%2520work%2520named%2520WCBnet%2520to%2520produce%2520multi-view%2520feature%250Amaps%2520which%2520are%2520subsequently%2520fused%2520for%2520coarse%2520localization.%2520These%2520coarse%2520maps%250Aare%2520then%2520refined%2520using%2520detailed%2520segmented%2520regional%2520information%2520provided%2520by%250Apre-trained%2520segmentation%2520models%2520%2528such%2520as%2520DeepLab%252C%2520SegmentAnything%2520and%2520PSPnet%2529%252C%250Awith%2520Bayesian%2520inference%2520employed%2520to%2520enhance%2520the%2520manipulation%2520localization.%250AExperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%250Ahighlighting%2520the%2520feasibility%2520to%2520localize%2520image%2520manipulations%2520without%2520relying%2520on%250Apixel-level%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly-supervised%20Localization%20of%20Manipulated%20Image%20Regions%20Using%0A%20%20Multi-resolution%20Learned%20Features&entry.906535625=Ziyong%20Wang%20and%20Charith%20Abhayaratne&entry.1292438233=%20%20The%20explosive%20growth%20of%20digital%20images%20and%20the%20widespread%20availability%20of%0Aimage%20editing%20tools%20have%20made%20image%20manipulation%20detection%20an%20increasingly%0Acritical%20challenge.%20Current%20deep%20learning-based%20manipulation%20detection%20methods%0Aexcel%20in%20achieving%20high%20image-level%20classification%20accuracy%2C%20they%20often%20fall%0Ashort%20in%20terms%20of%20interpretability%20and%20localization%20of%20manipulated%20regions.%0AAdditionally%2C%20the%20absence%20of%20pixel-wise%20annotations%20in%20real-world%20scenarios%0Alimits%20the%20existing%20fully-supervised%20manipulation%20localization%20techniques.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20weakly-supervised%20approach%20that%0Aintegrates%20activation%20maps%20generated%20by%20image-level%20manipulation%20detection%0Anetworks%20with%20segmentation%20maps%20from%20pre-trained%20models.%20Specifically%2C%20we%20build%0Aon%20our%20previous%20image-level%20work%20named%20WCBnet%20to%20produce%20multi-view%20feature%0Amaps%20which%20are%20subsequently%20fused%20for%20coarse%20localization.%20These%20coarse%20maps%0Aare%20then%20refined%20using%20detailed%20segmented%20regional%20information%20provided%20by%0Apre-trained%20segmentation%20models%20%28such%20as%20DeepLab%2C%20SegmentAnything%20and%20PSPnet%29%2C%0Awith%20Bayesian%20inference%20employed%20to%20enhance%20the%20manipulation%20localization.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Ahighlighting%20the%20feasibility%20to%20localize%20image%20manipulations%20without%20relying%20on%0Apixel-level%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23586v1&entry.124074799=Read"},
{"title": "CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for\n  Unsupervised Temporal Action Localization", "author": "Rui Xia and Dan Jiang and Quan Zhang and Ke Zhang and Chun Yuan", "abstract": "  Temporal Action Localization (TAL) has garnered significant attention in\ninformation retrieval. Existing supervised or weakly supervised methods heavily\nrely on labeled temporal boundaries and action categories, which are\nlabor-intensive and time-consuming. Consequently, unsupervised temporal action\nlocalization (UTAL) has gained popularity. However, current methods face two\nmain challenges: 1) Classification pre-trained features overly focus on highly\ndiscriminative regions; 2) Solely relying on visual modality information makes\nit difficult to determine contextual boundaries. To address these issues, we\npropose a CLIP-assisted cross-view audiovisual enhanced UTAL method.\nSpecifically, we introduce visual language pre-training (VLP) and\nclassification pre-training-based collaborative enhancement to avoid excessive\nfocus on highly discriminative regions; we also incorporate audio perception to\nprovide richer contextual boundary information. Finally, we introduce a\nself-supervised cross-view learning paradigm to achieve multi-view perceptual\nenhancement without additional annotations. Extensive experiments on two public\ndatasets demonstrate our model's superiority over several state-of-the-art\ncompetitors.\n", "link": "http://arxiv.org/abs/2505.23524v1", "date": "2025-05-29", "relevancy": 2.9791, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6377}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5849}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-AE%3A%20CLIP-assisted%20Cross-view%20Audio-Visual%20Enhancement%20for%0A%20%20Unsupervised%20Temporal%20Action%20Localization&body=Title%3A%20CLIP-AE%3A%20CLIP-assisted%20Cross-view%20Audio-Visual%20Enhancement%20for%0A%20%20Unsupervised%20Temporal%20Action%20Localization%0AAuthor%3A%20Rui%20Xia%20and%20Dan%20Jiang%20and%20Quan%20Zhang%20and%20Ke%20Zhang%20and%20Chun%20Yuan%0AAbstract%3A%20%20%20Temporal%20Action%20Localization%20%28TAL%29%20has%20garnered%20significant%20attention%20in%0Ainformation%20retrieval.%20Existing%20supervised%20or%20weakly%20supervised%20methods%20heavily%0Arely%20on%20labeled%20temporal%20boundaries%20and%20action%20categories%2C%20which%20are%0Alabor-intensive%20and%20time-consuming.%20Consequently%2C%20unsupervised%20temporal%20action%0Alocalization%20%28UTAL%29%20has%20gained%20popularity.%20However%2C%20current%20methods%20face%20two%0Amain%20challenges%3A%201%29%20Classification%20pre-trained%20features%20overly%20focus%20on%20highly%0Adiscriminative%20regions%3B%202%29%20Solely%20relying%20on%20visual%20modality%20information%20makes%0Ait%20difficult%20to%20determine%20contextual%20boundaries.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20CLIP-assisted%20cross-view%20audiovisual%20enhanced%20UTAL%20method.%0ASpecifically%2C%20we%20introduce%20visual%20language%20pre-training%20%28VLP%29%20and%0Aclassification%20pre-training-based%20collaborative%20enhancement%20to%20avoid%20excessive%0Afocus%20on%20highly%20discriminative%20regions%3B%20we%20also%20incorporate%20audio%20perception%20to%0Aprovide%20richer%20contextual%20boundary%20information.%20Finally%2C%20we%20introduce%20a%0Aself-supervised%20cross-view%20learning%20paradigm%20to%20achieve%20multi-view%20perceptual%0Aenhancement%20without%20additional%20annotations.%20Extensive%20experiments%20on%20two%20public%0Adatasets%20demonstrate%20our%20model%27s%20superiority%20over%20several%20state-of-the-art%0Acompetitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-AE%253A%2520CLIP-assisted%2520Cross-view%2520Audio-Visual%2520Enhancement%2520for%250A%2520%2520Unsupervised%2520Temporal%2520Action%2520Localization%26entry.906535625%3DRui%2520Xia%2520and%2520Dan%2520Jiang%2520and%2520Quan%2520Zhang%2520and%2520Ke%2520Zhang%2520and%2520Chun%2520Yuan%26entry.1292438233%3D%2520%2520Temporal%2520Action%2520Localization%2520%2528TAL%2529%2520has%2520garnered%2520significant%2520attention%2520in%250Ainformation%2520retrieval.%2520Existing%2520supervised%2520or%2520weakly%2520supervised%2520methods%2520heavily%250Arely%2520on%2520labeled%2520temporal%2520boundaries%2520and%2520action%2520categories%252C%2520which%2520are%250Alabor-intensive%2520and%2520time-consuming.%2520Consequently%252C%2520unsupervised%2520temporal%2520action%250Alocalization%2520%2528UTAL%2529%2520has%2520gained%2520popularity.%2520However%252C%2520current%2520methods%2520face%2520two%250Amain%2520challenges%253A%25201%2529%2520Classification%2520pre-trained%2520features%2520overly%2520focus%2520on%2520highly%250Adiscriminative%2520regions%253B%25202%2529%2520Solely%2520relying%2520on%2520visual%2520modality%2520information%2520makes%250Ait%2520difficult%2520to%2520determine%2520contextual%2520boundaries.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520CLIP-assisted%2520cross-view%2520audiovisual%2520enhanced%2520UTAL%2520method.%250ASpecifically%252C%2520we%2520introduce%2520visual%2520language%2520pre-training%2520%2528VLP%2529%2520and%250Aclassification%2520pre-training-based%2520collaborative%2520enhancement%2520to%2520avoid%2520excessive%250Afocus%2520on%2520highly%2520discriminative%2520regions%253B%2520we%2520also%2520incorporate%2520audio%2520perception%2520to%250Aprovide%2520richer%2520contextual%2520boundary%2520information.%2520Finally%252C%2520we%2520introduce%2520a%250Aself-supervised%2520cross-view%2520learning%2520paradigm%2520to%2520achieve%2520multi-view%2520perceptual%250Aenhancement%2520without%2520additional%2520annotations.%2520Extensive%2520experiments%2520on%2520two%2520public%250Adatasets%2520demonstrate%2520our%2520model%2527s%2520superiority%2520over%2520several%2520state-of-the-art%250Acompetitors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-AE%3A%20CLIP-assisted%20Cross-view%20Audio-Visual%20Enhancement%20for%0A%20%20Unsupervised%20Temporal%20Action%20Localization&entry.906535625=Rui%20Xia%20and%20Dan%20Jiang%20and%20Quan%20Zhang%20and%20Ke%20Zhang%20and%20Chun%20Yuan&entry.1292438233=%20%20Temporal%20Action%20Localization%20%28TAL%29%20has%20garnered%20significant%20attention%20in%0Ainformation%20retrieval.%20Existing%20supervised%20or%20weakly%20supervised%20methods%20heavily%0Arely%20on%20labeled%20temporal%20boundaries%20and%20action%20categories%2C%20which%20are%0Alabor-intensive%20and%20time-consuming.%20Consequently%2C%20unsupervised%20temporal%20action%0Alocalization%20%28UTAL%29%20has%20gained%20popularity.%20However%2C%20current%20methods%20face%20two%0Amain%20challenges%3A%201%29%20Classification%20pre-trained%20features%20overly%20focus%20on%20highly%0Adiscriminative%20regions%3B%202%29%20Solely%20relying%20on%20visual%20modality%20information%20makes%0Ait%20difficult%20to%20determine%20contextual%20boundaries.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20CLIP-assisted%20cross-view%20audiovisual%20enhanced%20UTAL%20method.%0ASpecifically%2C%20we%20introduce%20visual%20language%20pre-training%20%28VLP%29%20and%0Aclassification%20pre-training-based%20collaborative%20enhancement%20to%20avoid%20excessive%0Afocus%20on%20highly%20discriminative%20regions%3B%20we%20also%20incorporate%20audio%20perception%20to%0Aprovide%20richer%20contextual%20boundary%20information.%20Finally%2C%20we%20introduce%20a%0Aself-supervised%20cross-view%20learning%20paradigm%20to%20achieve%20multi-view%20perceptual%0Aenhancement%20without%20additional%20annotations.%20Extensive%20experiments%20on%20two%20public%0Adatasets%20demonstrate%20our%20model%27s%20superiority%20over%20several%20state-of-the-art%0Acompetitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23524v1&entry.124074799=Read"},
{"title": "It's a (Blind) Match! Towards Vision-Language Correspondence without\n  Parallel Data", "author": "Dominik Schnaus and Nikita Araslanov and Daniel Cremers", "abstract": "  The platonic representation hypothesis suggests that vision and language\nembeddings become more homogeneous as model and dataset sizes increase. In\nparticular, pairwise distances within each modality become more similar. This\nsuggests that as foundation models mature, it may become possible to match\nvision and language embeddings in a fully unsupervised fashion, i.e. without\nparallel data. We present the first feasibility study, and investigate\nconformity of existing vision and language foundation models in the context of\nunsupervised, or \"blind\", matching. First, we formulate unsupervised matching\nas a quadratic assignment problem and introduce a novel heuristic that\noutperforms previous solvers. We also develop a technique to find optimal\nmatching problems, for which a non-trivial match is very likely. Second, we\nconduct an extensive study deploying a range of vision and language models on\nfour datasets. Our analysis reveals that for many problem instances, vision and\nlanguage representations can be indeed matched without supervision. This\nfinding opens up the exciting possibility of embedding semantic knowledge into\nother modalities virtually annotation-free. As a proof of concept, we showcase\nan unsupervised classifier, which achieves non-trivial classification accuracy\nwithout any image-text annotation.\n", "link": "http://arxiv.org/abs/2503.24129v2", "date": "2025-05-29", "relevancy": 2.938, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5948}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5948}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20It%27s%20a%20%28Blind%29%20Match%21%20Towards%20Vision-Language%20Correspondence%20without%0A%20%20Parallel%20Data&body=Title%3A%20It%27s%20a%20%28Blind%29%20Match%21%20Towards%20Vision-Language%20Correspondence%20without%0A%20%20Parallel%20Data%0AAuthor%3A%20Dominik%20Schnaus%20and%20Nikita%20Araslanov%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20The%20platonic%20representation%20hypothesis%20suggests%20that%20vision%20and%20language%0Aembeddings%20become%20more%20homogeneous%20as%20model%20and%20dataset%20sizes%20increase.%20In%0Aparticular%2C%20pairwise%20distances%20within%20each%20modality%20become%20more%20similar.%20This%0Asuggests%20that%20as%20foundation%20models%20mature%2C%20it%20may%20become%20possible%20to%20match%0Avision%20and%20language%20embeddings%20in%20a%20fully%20unsupervised%20fashion%2C%20i.e.%20without%0Aparallel%20data.%20We%20present%20the%20first%20feasibility%20study%2C%20and%20investigate%0Aconformity%20of%20existing%20vision%20and%20language%20foundation%20models%20in%20the%20context%20of%0Aunsupervised%2C%20or%20%22blind%22%2C%20matching.%20First%2C%20we%20formulate%20unsupervised%20matching%0Aas%20a%20quadratic%20assignment%20problem%20and%20introduce%20a%20novel%20heuristic%20that%0Aoutperforms%20previous%20solvers.%20We%20also%20develop%20a%20technique%20to%20find%20optimal%0Amatching%20problems%2C%20for%20which%20a%20non-trivial%20match%20is%20very%20likely.%20Second%2C%20we%0Aconduct%20an%20extensive%20study%20deploying%20a%20range%20of%20vision%20and%20language%20models%20on%0Afour%20datasets.%20Our%20analysis%20reveals%20that%20for%20many%20problem%20instances%2C%20vision%20and%0Alanguage%20representations%20can%20be%20indeed%20matched%20without%20supervision.%20This%0Afinding%20opens%20up%20the%20exciting%20possibility%20of%20embedding%20semantic%20knowledge%20into%0Aother%20modalities%20virtually%20annotation-free.%20As%20a%20proof%20of%20concept%2C%20we%20showcase%0Aan%20unsupervised%20classifier%2C%20which%20achieves%20non-trivial%20classification%20accuracy%0Awithout%20any%20image-text%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.24129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIt%2527s%2520a%2520%2528Blind%2529%2520Match%2521%2520Towards%2520Vision-Language%2520Correspondence%2520without%250A%2520%2520Parallel%2520Data%26entry.906535625%3DDominik%2520Schnaus%2520and%2520Nikita%2520Araslanov%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520The%2520platonic%2520representation%2520hypothesis%2520suggests%2520that%2520vision%2520and%2520language%250Aembeddings%2520become%2520more%2520homogeneous%2520as%2520model%2520and%2520dataset%2520sizes%2520increase.%2520In%250Aparticular%252C%2520pairwise%2520distances%2520within%2520each%2520modality%2520become%2520more%2520similar.%2520This%250Asuggests%2520that%2520as%2520foundation%2520models%2520mature%252C%2520it%2520may%2520become%2520possible%2520to%2520match%250Avision%2520and%2520language%2520embeddings%2520in%2520a%2520fully%2520unsupervised%2520fashion%252C%2520i.e.%2520without%250Aparallel%2520data.%2520We%2520present%2520the%2520first%2520feasibility%2520study%252C%2520and%2520investigate%250Aconformity%2520of%2520existing%2520vision%2520and%2520language%2520foundation%2520models%2520in%2520the%2520context%2520of%250Aunsupervised%252C%2520or%2520%2522blind%2522%252C%2520matching.%2520First%252C%2520we%2520formulate%2520unsupervised%2520matching%250Aas%2520a%2520quadratic%2520assignment%2520problem%2520and%2520introduce%2520a%2520novel%2520heuristic%2520that%250Aoutperforms%2520previous%2520solvers.%2520We%2520also%2520develop%2520a%2520technique%2520to%2520find%2520optimal%250Amatching%2520problems%252C%2520for%2520which%2520a%2520non-trivial%2520match%2520is%2520very%2520likely.%2520Second%252C%2520we%250Aconduct%2520an%2520extensive%2520study%2520deploying%2520a%2520range%2520of%2520vision%2520and%2520language%2520models%2520on%250Afour%2520datasets.%2520Our%2520analysis%2520reveals%2520that%2520for%2520many%2520problem%2520instances%252C%2520vision%2520and%250Alanguage%2520representations%2520can%2520be%2520indeed%2520matched%2520without%2520supervision.%2520This%250Afinding%2520opens%2520up%2520the%2520exciting%2520possibility%2520of%2520embedding%2520semantic%2520knowledge%2520into%250Aother%2520modalities%2520virtually%2520annotation-free.%2520As%2520a%2520proof%2520of%2520concept%252C%2520we%2520showcase%250Aan%2520unsupervised%2520classifier%252C%2520which%2520achieves%2520non-trivial%2520classification%2520accuracy%250Awithout%2520any%2520image-text%2520annotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.24129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=It%27s%20a%20%28Blind%29%20Match%21%20Towards%20Vision-Language%20Correspondence%20without%0A%20%20Parallel%20Data&entry.906535625=Dominik%20Schnaus%20and%20Nikita%20Araslanov%20and%20Daniel%20Cremers&entry.1292438233=%20%20The%20platonic%20representation%20hypothesis%20suggests%20that%20vision%20and%20language%0Aembeddings%20become%20more%20homogeneous%20as%20model%20and%20dataset%20sizes%20increase.%20In%0Aparticular%2C%20pairwise%20distances%20within%20each%20modality%20become%20more%20similar.%20This%0Asuggests%20that%20as%20foundation%20models%20mature%2C%20it%20may%20become%20possible%20to%20match%0Avision%20and%20language%20embeddings%20in%20a%20fully%20unsupervised%20fashion%2C%20i.e.%20without%0Aparallel%20data.%20We%20present%20the%20first%20feasibility%20study%2C%20and%20investigate%0Aconformity%20of%20existing%20vision%20and%20language%20foundation%20models%20in%20the%20context%20of%0Aunsupervised%2C%20or%20%22blind%22%2C%20matching.%20First%2C%20we%20formulate%20unsupervised%20matching%0Aas%20a%20quadratic%20assignment%20problem%20and%20introduce%20a%20novel%20heuristic%20that%0Aoutperforms%20previous%20solvers.%20We%20also%20develop%20a%20technique%20to%20find%20optimal%0Amatching%20problems%2C%20for%20which%20a%20non-trivial%20match%20is%20very%20likely.%20Second%2C%20we%0Aconduct%20an%20extensive%20study%20deploying%20a%20range%20of%20vision%20and%20language%20models%20on%0Afour%20datasets.%20Our%20analysis%20reveals%20that%20for%20many%20problem%20instances%2C%20vision%20and%0Alanguage%20representations%20can%20be%20indeed%20matched%20without%20supervision.%20This%0Afinding%20opens%20up%20the%20exciting%20possibility%20of%20embedding%20semantic%20knowledge%20into%0Aother%20modalities%20virtually%20annotation-free.%20As%20a%20proof%20of%20concept%2C%20we%20showcase%0Aan%20unsupervised%20classifier%2C%20which%20achieves%20non-trivial%20classification%20accuracy%0Awithout%20any%20image-text%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.24129v2&entry.124074799=Read"},
{"title": "ChatHuman: Chatting about 3D Humans with Tools", "author": "Jing Lin and Yao Feng and Weiyang Liu and Michael J. Black", "abstract": "  Numerous methods have been proposed to detect, estimate, and analyze\nproperties of people in images, including 3D pose, shape, contact, human-object\ninteraction, and emotion. While widely applicable in vision and other areas,\nsuch methods require expert knowledge to select, use, and interpret the\nresults. To address this, we introduce ChatHuman, a language-driven system that\nintegrates the capabilities of specialized methods into a unified framework.\nChatHuman functions as an assistant proficient in utilizing, analyzing, and\ninteracting with tools specific to 3D human tasks, adeptly discussing and\nresolving related challenges. Built on a Large Language Model (LLM) framework,\nChatHuman is trained to autonomously select, apply, and interpret a diverse set\nof tools in response to user inputs. Our approach overcomes significant hurdles\nin adapting LLMs to 3D human tasks, including the need for domain-specific\nknowledge and the ability to interpret complex 3D outputs. The innovations of\nChatHuman include leveraging academic publications to instruct the LLM on tool\nusage, employing a retrieval-augmented generation model to create in-context\nlearning examples for managing new tools, and effectively discriminating\nbetween and integrating tool results by transforming specialized 3D outputs\ninto comprehensible formats. Experiments demonstrate that ChatHuman surpasses\nexisting models in both tool selection accuracy and overall performance across\nvarious 3D human tasks, and it supports interactive chatting with users.\nChatHuman represents a significant step toward consolidating diverse analytical\nmethods into a unified, robust system for 3D human tasks.\n", "link": "http://arxiv.org/abs/2405.04533v2", "date": "2025-05-29", "relevancy": 2.9159, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5975}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatHuman%3A%20Chatting%20about%203D%20Humans%20with%20Tools&body=Title%3A%20ChatHuman%3A%20Chatting%20about%203D%20Humans%20with%20Tools%0AAuthor%3A%20Jing%20Lin%20and%20Yao%20Feng%20and%20Weiyang%20Liu%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20Numerous%20methods%20have%20been%20proposed%20to%20detect%2C%20estimate%2C%20and%20analyze%0Aproperties%20of%20people%20in%20images%2C%20including%203D%20pose%2C%20shape%2C%20contact%2C%20human-object%0Ainteraction%2C%20and%20emotion.%20While%20widely%20applicable%20in%20vision%20and%20other%20areas%2C%0Asuch%20methods%20require%20expert%20knowledge%20to%20select%2C%20use%2C%20and%20interpret%20the%0Aresults.%20To%20address%20this%2C%20we%20introduce%20ChatHuman%2C%20a%20language-driven%20system%20that%0Aintegrates%20the%20capabilities%20of%20specialized%20methods%20into%20a%20unified%20framework.%0AChatHuman%20functions%20as%20an%20assistant%20proficient%20in%20utilizing%2C%20analyzing%2C%20and%0Ainteracting%20with%20tools%20specific%20to%203D%20human%20tasks%2C%20adeptly%20discussing%20and%0Aresolving%20related%20challenges.%20Built%20on%20a%20Large%20Language%20Model%20%28LLM%29%20framework%2C%0AChatHuman%20is%20trained%20to%20autonomously%20select%2C%20apply%2C%20and%20interpret%20a%20diverse%20set%0Aof%20tools%20in%20response%20to%20user%20inputs.%20Our%20approach%20overcomes%20significant%20hurdles%0Ain%20adapting%20LLMs%20to%203D%20human%20tasks%2C%20including%20the%20need%20for%20domain-specific%0Aknowledge%20and%20the%20ability%20to%20interpret%20complex%203D%20outputs.%20The%20innovations%20of%0AChatHuman%20include%20leveraging%20academic%20publications%20to%20instruct%20the%20LLM%20on%20tool%0Ausage%2C%20employing%20a%20retrieval-augmented%20generation%20model%20to%20create%20in-context%0Alearning%20examples%20for%20managing%20new%20tools%2C%20and%20effectively%20discriminating%0Abetween%20and%20integrating%20tool%20results%20by%20transforming%20specialized%203D%20outputs%0Ainto%20comprehensible%20formats.%20Experiments%20demonstrate%20that%20ChatHuman%20surpasses%0Aexisting%20models%20in%20both%20tool%20selection%20accuracy%20and%20overall%20performance%20across%0Avarious%203D%20human%20tasks%2C%20and%20it%20supports%20interactive%20chatting%20with%20users.%0AChatHuman%20represents%20a%20significant%20step%20toward%20consolidating%20diverse%20analytical%0Amethods%20into%20a%20unified%2C%20robust%20system%20for%203D%20human%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04533v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatHuman%253A%2520Chatting%2520about%25203D%2520Humans%2520with%2520Tools%26entry.906535625%3DJing%2520Lin%2520and%2520Yao%2520Feng%2520and%2520Weiyang%2520Liu%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520Numerous%2520methods%2520have%2520been%2520proposed%2520to%2520detect%252C%2520estimate%252C%2520and%2520analyze%250Aproperties%2520of%2520people%2520in%2520images%252C%2520including%25203D%2520pose%252C%2520shape%252C%2520contact%252C%2520human-object%250Ainteraction%252C%2520and%2520emotion.%2520While%2520widely%2520applicable%2520in%2520vision%2520and%2520other%2520areas%252C%250Asuch%2520methods%2520require%2520expert%2520knowledge%2520to%2520select%252C%2520use%252C%2520and%2520interpret%2520the%250Aresults.%2520To%2520address%2520this%252C%2520we%2520introduce%2520ChatHuman%252C%2520a%2520language-driven%2520system%2520that%250Aintegrates%2520the%2520capabilities%2520of%2520specialized%2520methods%2520into%2520a%2520unified%2520framework.%250AChatHuman%2520functions%2520as%2520an%2520assistant%2520proficient%2520in%2520utilizing%252C%2520analyzing%252C%2520and%250Ainteracting%2520with%2520tools%2520specific%2520to%25203D%2520human%2520tasks%252C%2520adeptly%2520discussing%2520and%250Aresolving%2520related%2520challenges.%2520Built%2520on%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520framework%252C%250AChatHuman%2520is%2520trained%2520to%2520autonomously%2520select%252C%2520apply%252C%2520and%2520interpret%2520a%2520diverse%2520set%250Aof%2520tools%2520in%2520response%2520to%2520user%2520inputs.%2520Our%2520approach%2520overcomes%2520significant%2520hurdles%250Ain%2520adapting%2520LLMs%2520to%25203D%2520human%2520tasks%252C%2520including%2520the%2520need%2520for%2520domain-specific%250Aknowledge%2520and%2520the%2520ability%2520to%2520interpret%2520complex%25203D%2520outputs.%2520The%2520innovations%2520of%250AChatHuman%2520include%2520leveraging%2520academic%2520publications%2520to%2520instruct%2520the%2520LLM%2520on%2520tool%250Ausage%252C%2520employing%2520a%2520retrieval-augmented%2520generation%2520model%2520to%2520create%2520in-context%250Alearning%2520examples%2520for%2520managing%2520new%2520tools%252C%2520and%2520effectively%2520discriminating%250Abetween%2520and%2520integrating%2520tool%2520results%2520by%2520transforming%2520specialized%25203D%2520outputs%250Ainto%2520comprehensible%2520formats.%2520Experiments%2520demonstrate%2520that%2520ChatHuman%2520surpasses%250Aexisting%2520models%2520in%2520both%2520tool%2520selection%2520accuracy%2520and%2520overall%2520performance%2520across%250Avarious%25203D%2520human%2520tasks%252C%2520and%2520it%2520supports%2520interactive%2520chatting%2520with%2520users.%250AChatHuman%2520represents%2520a%2520significant%2520step%2520toward%2520consolidating%2520diverse%2520analytical%250Amethods%2520into%2520a%2520unified%252C%2520robust%2520system%2520for%25203D%2520human%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04533v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatHuman%3A%20Chatting%20about%203D%20Humans%20with%20Tools&entry.906535625=Jing%20Lin%20and%20Yao%20Feng%20and%20Weiyang%20Liu%20and%20Michael%20J.%20Black&entry.1292438233=%20%20Numerous%20methods%20have%20been%20proposed%20to%20detect%2C%20estimate%2C%20and%20analyze%0Aproperties%20of%20people%20in%20images%2C%20including%203D%20pose%2C%20shape%2C%20contact%2C%20human-object%0Ainteraction%2C%20and%20emotion.%20While%20widely%20applicable%20in%20vision%20and%20other%20areas%2C%0Asuch%20methods%20require%20expert%20knowledge%20to%20select%2C%20use%2C%20and%20interpret%20the%0Aresults.%20To%20address%20this%2C%20we%20introduce%20ChatHuman%2C%20a%20language-driven%20system%20that%0Aintegrates%20the%20capabilities%20of%20specialized%20methods%20into%20a%20unified%20framework.%0AChatHuman%20functions%20as%20an%20assistant%20proficient%20in%20utilizing%2C%20analyzing%2C%20and%0Ainteracting%20with%20tools%20specific%20to%203D%20human%20tasks%2C%20adeptly%20discussing%20and%0Aresolving%20related%20challenges.%20Built%20on%20a%20Large%20Language%20Model%20%28LLM%29%20framework%2C%0AChatHuman%20is%20trained%20to%20autonomously%20select%2C%20apply%2C%20and%20interpret%20a%20diverse%20set%0Aof%20tools%20in%20response%20to%20user%20inputs.%20Our%20approach%20overcomes%20significant%20hurdles%0Ain%20adapting%20LLMs%20to%203D%20human%20tasks%2C%20including%20the%20need%20for%20domain-specific%0Aknowledge%20and%20the%20ability%20to%20interpret%20complex%203D%20outputs.%20The%20innovations%20of%0AChatHuman%20include%20leveraging%20academic%20publications%20to%20instruct%20the%20LLM%20on%20tool%0Ausage%2C%20employing%20a%20retrieval-augmented%20generation%20model%20to%20create%20in-context%0Alearning%20examples%20for%20managing%20new%20tools%2C%20and%20effectively%20discriminating%0Abetween%20and%20integrating%20tool%20results%20by%20transforming%20specialized%203D%20outputs%0Ainto%20comprehensible%20formats.%20Experiments%20demonstrate%20that%20ChatHuman%20surpasses%0Aexisting%20models%20in%20both%20tool%20selection%20accuracy%20and%20overall%20performance%20across%0Avarious%203D%20human%20tasks%2C%20and%20it%20supports%20interactive%20chatting%20with%20users.%0AChatHuman%20represents%20a%20significant%20step%20toward%20consolidating%20diverse%20analytical%0Amethods%20into%20a%20unified%2C%20robust%20system%20for%203D%20human%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04533v2&entry.124074799=Read"},
{"title": "Radiant Triangle Soup with Soft Connectivity Forces for 3D\n  Reconstruction and Novel View Synthesis", "author": "Nathaniel Burgdorfer and Philippos Mordohai", "abstract": "  In this work, we introduce an inference-time optimization framework utilizing\ntriangles to represent the geometry and appearance of the scene. More\nspecifically, we develop a scene optimization algorithm for triangle soup, a\ncollection of disconnected semi-transparent triangle primitives. Compared to\nthe current most-widely used primitives for 3D scene representation, namely\nGaussian splats, triangles allow for more expressive color interpolation, and\nbenefit from a large algorithmic infrastructure for downstream tasks.\nTriangles, unlike full-rank Gaussian kernels, naturally combine to form\nsurfaces. We formulate connectivity forces between triangles during\noptimization, encouraging explicit, but soft, surface continuity in 3D. We\nperform experiments on a representative 3D reconstruction dataset and show\ncompetitive photometric and geometric results.\n", "link": "http://arxiv.org/abs/2505.23642v1", "date": "2025-05-29", "relevancy": 2.9098, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5899}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.578}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radiant%20Triangle%20Soup%20with%20Soft%20Connectivity%20Forces%20for%203D%0A%20%20Reconstruction%20and%20Novel%20View%20Synthesis&body=Title%3A%20Radiant%20Triangle%20Soup%20with%20Soft%20Connectivity%20Forces%20for%203D%0A%20%20Reconstruction%20and%20Novel%20View%20Synthesis%0AAuthor%3A%20Nathaniel%20Burgdorfer%20and%20Philippos%20Mordohai%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20an%20inference-time%20optimization%20framework%20utilizing%0Atriangles%20to%20represent%20the%20geometry%20and%20appearance%20of%20the%20scene.%20More%0Aspecifically%2C%20we%20develop%20a%20scene%20optimization%20algorithm%20for%20triangle%20soup%2C%20a%0Acollection%20of%20disconnected%20semi-transparent%20triangle%20primitives.%20Compared%20to%0Athe%20current%20most-widely%20used%20primitives%20for%203D%20scene%20representation%2C%20namely%0AGaussian%20splats%2C%20triangles%20allow%20for%20more%20expressive%20color%20interpolation%2C%20and%0Abenefit%20from%20a%20large%20algorithmic%20infrastructure%20for%20downstream%20tasks.%0ATriangles%2C%20unlike%20full-rank%20Gaussian%20kernels%2C%20naturally%20combine%20to%20form%0Asurfaces.%20We%20formulate%20connectivity%20forces%20between%20triangles%20during%0Aoptimization%2C%20encouraging%20explicit%2C%20but%20soft%2C%20surface%20continuity%20in%203D.%20We%0Aperform%20experiments%20on%20a%20representative%203D%20reconstruction%20dataset%20and%20show%0Acompetitive%20photometric%20and%20geometric%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadiant%2520Triangle%2520Soup%2520with%2520Soft%2520Connectivity%2520Forces%2520for%25203D%250A%2520%2520Reconstruction%2520and%2520Novel%2520View%2520Synthesis%26entry.906535625%3DNathaniel%2520Burgdorfer%2520and%2520Philippos%2520Mordohai%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520inference-time%2520optimization%2520framework%2520utilizing%250Atriangles%2520to%2520represent%2520the%2520geometry%2520and%2520appearance%2520of%2520the%2520scene.%2520More%250Aspecifically%252C%2520we%2520develop%2520a%2520scene%2520optimization%2520algorithm%2520for%2520triangle%2520soup%252C%2520a%250Acollection%2520of%2520disconnected%2520semi-transparent%2520triangle%2520primitives.%2520Compared%2520to%250Athe%2520current%2520most-widely%2520used%2520primitives%2520for%25203D%2520scene%2520representation%252C%2520namely%250AGaussian%2520splats%252C%2520triangles%2520allow%2520for%2520more%2520expressive%2520color%2520interpolation%252C%2520and%250Abenefit%2520from%2520a%2520large%2520algorithmic%2520infrastructure%2520for%2520downstream%2520tasks.%250ATriangles%252C%2520unlike%2520full-rank%2520Gaussian%2520kernels%252C%2520naturally%2520combine%2520to%2520form%250Asurfaces.%2520We%2520formulate%2520connectivity%2520forces%2520between%2520triangles%2520during%250Aoptimization%252C%2520encouraging%2520explicit%252C%2520but%2520soft%252C%2520surface%2520continuity%2520in%25203D.%2520We%250Aperform%2520experiments%2520on%2520a%2520representative%25203D%2520reconstruction%2520dataset%2520and%2520show%250Acompetitive%2520photometric%2520and%2520geometric%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radiant%20Triangle%20Soup%20with%20Soft%20Connectivity%20Forces%20for%203D%0A%20%20Reconstruction%20and%20Novel%20View%20Synthesis&entry.906535625=Nathaniel%20Burgdorfer%20and%20Philippos%20Mordohai&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20an%20inference-time%20optimization%20framework%20utilizing%0Atriangles%20to%20represent%20the%20geometry%20and%20appearance%20of%20the%20scene.%20More%0Aspecifically%2C%20we%20develop%20a%20scene%20optimization%20algorithm%20for%20triangle%20soup%2C%20a%0Acollection%20of%20disconnected%20semi-transparent%20triangle%20primitives.%20Compared%20to%0Athe%20current%20most-widely%20used%20primitives%20for%203D%20scene%20representation%2C%20namely%0AGaussian%20splats%2C%20triangles%20allow%20for%20more%20expressive%20color%20interpolation%2C%20and%0Abenefit%20from%20a%20large%20algorithmic%20infrastructure%20for%20downstream%20tasks.%0ATriangles%2C%20unlike%20full-rank%20Gaussian%20kernels%2C%20naturally%20combine%20to%20form%0Asurfaces.%20We%20formulate%20connectivity%20forces%20between%20triangles%20during%0Aoptimization%2C%20encouraging%20explicit%2C%20but%20soft%2C%20surface%20continuity%20in%203D.%20We%0Aperform%20experiments%20on%20a%20representative%203D%20reconstruction%20dataset%20and%20show%0Acompetitive%20photometric%20and%20geometric%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23642v1&entry.124074799=Read"},
{"title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence", "author": "Sihan Yang and Runsen Xu and Yiman Xie and Sizhe Yang and Mo Li and Jingli Lin and Chenming Zhu and Xiaochen Chen and Haodong Duan and Xiangyu Yue and Dahua Lin and Tai Wang and Jiangmiao Pang", "abstract": "  Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .\n", "link": "http://arxiv.org/abs/2505.23764v1", "date": "2025-05-29", "relevancy": 2.9026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6013}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMSI-Bench%3A%20A%20Benchmark%20for%20Multi-Image%20Spatial%20Intelligence&body=Title%3A%20MMSI-Bench%3A%20A%20Benchmark%20for%20Multi-Image%20Spatial%20Intelligence%0AAuthor%3A%20Sihan%20Yang%20and%20Runsen%20Xu%20and%20Yiman%20Xie%20and%20Sizhe%20Yang%20and%20Mo%20Li%20and%20Jingli%20Lin%20and%20Chenming%20Zhu%20and%20Xiaochen%20Chen%20and%20Haodong%20Duan%20and%20Xiangyu%20Yue%20and%20Dahua%20Lin%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Spatial%20intelligence%20is%20essential%20for%20multimodal%20large%20language%20models%0A%28MLLMs%29%20operating%20in%20the%20complex%20physical%20world.%20Existing%20benchmarks%2C%20however%2C%0Aprobe%20only%20single-image%20relations%20and%20thus%20fail%20to%20assess%20the%20multi-image%0Aspatial%20reasoning%20that%20real-world%20deployments%20demand.%20We%20introduce%20MMSI-Bench%2C%0Aa%20VQA%20benchmark%20dedicated%20to%20multi-image%20spatial%20intelligence.%20Six%203D-vision%0Aresearchers%20spent%20more%20than%20300%20hours%20meticulously%20crafting%201%2C000%20challenging%2C%0Aunambiguous%20multiple-choice%20questions%20from%20over%20120%2C000%20images%2C%20each%20paired%0Awith%20carefully%20designed%20distractors%20and%20a%20step-by-step%20reasoning%20process.%20We%0Aconduct%20extensive%20experiments%20and%20thoroughly%20evaluate%2034%20open-source%20and%0Aproprietary%20MLLMs%2C%20observing%20a%20wide%20gap%3A%20the%20strongest%20open-source%20model%0Aattains%20roughly%2030%25%20accuracy%20and%20OpenAI%27s%20o3%20reasoning%20model%20reaches%2040%25%2C%20while%0Ahumans%20score%2097%25.%20These%20results%20underscore%20the%20challenging%20nature%20of%20MMSI-Bench%0Aand%20the%20substantial%20headroom%20for%20future%20research.%20Leveraging%20the%20annotated%0Areasoning%20processes%2C%20we%20also%20provide%20an%20automated%20error%20analysis%20pipeline%20that%0Adiagnoses%20four%20dominant%20failure%20modes%2C%20including%20%281%29%20grounding%20errors%2C%20%282%29%0Aoverlap-matching%20and%20scene-reconstruction%20errors%2C%20%283%29%20situation-transformation%0Areasoning%20errors%2C%20and%20%284%29%20spatial-logic%20errors%2C%20offering%20valuable%20insights%20for%0Aadvancing%20multi-image%20spatial%20intelligence.%20Project%20page%3A%0Ahttps%3A//runsenxu.com/projects/MMSI_Bench%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMSI-Bench%253A%2520A%2520Benchmark%2520for%2520Multi-Image%2520Spatial%2520Intelligence%26entry.906535625%3DSihan%2520Yang%2520and%2520Runsen%2520Xu%2520and%2520Yiman%2520Xie%2520and%2520Sizhe%2520Yang%2520and%2520Mo%2520Li%2520and%2520Jingli%2520Lin%2520and%2520Chenming%2520Zhu%2520and%2520Xiaochen%2520Chen%2520and%2520Haodong%2520Duan%2520and%2520Xiangyu%2520Yue%2520and%2520Dahua%2520Lin%2520and%2520Tai%2520Wang%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Spatial%2520intelligence%2520is%2520essential%2520for%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520operating%2520in%2520the%2520complex%2520physical%2520world.%2520Existing%2520benchmarks%252C%2520however%252C%250Aprobe%2520only%2520single-image%2520relations%2520and%2520thus%2520fail%2520to%2520assess%2520the%2520multi-image%250Aspatial%2520reasoning%2520that%2520real-world%2520deployments%2520demand.%2520We%2520introduce%2520MMSI-Bench%252C%250Aa%2520VQA%2520benchmark%2520dedicated%2520to%2520multi-image%2520spatial%2520intelligence.%2520Six%25203D-vision%250Aresearchers%2520spent%2520more%2520than%2520300%2520hours%2520meticulously%2520crafting%25201%252C000%2520challenging%252C%250Aunambiguous%2520multiple-choice%2520questions%2520from%2520over%2520120%252C000%2520images%252C%2520each%2520paired%250Awith%2520carefully%2520designed%2520distractors%2520and%2520a%2520step-by-step%2520reasoning%2520process.%2520We%250Aconduct%2520extensive%2520experiments%2520and%2520thoroughly%2520evaluate%252034%2520open-source%2520and%250Aproprietary%2520MLLMs%252C%2520observing%2520a%2520wide%2520gap%253A%2520the%2520strongest%2520open-source%2520model%250Aattains%2520roughly%252030%2525%2520accuracy%2520and%2520OpenAI%2527s%2520o3%2520reasoning%2520model%2520reaches%252040%2525%252C%2520while%250Ahumans%2520score%252097%2525.%2520These%2520results%2520underscore%2520the%2520challenging%2520nature%2520of%2520MMSI-Bench%250Aand%2520the%2520substantial%2520headroom%2520for%2520future%2520research.%2520Leveraging%2520the%2520annotated%250Areasoning%2520processes%252C%2520we%2520also%2520provide%2520an%2520automated%2520error%2520analysis%2520pipeline%2520that%250Adiagnoses%2520four%2520dominant%2520failure%2520modes%252C%2520including%2520%25281%2529%2520grounding%2520errors%252C%2520%25282%2529%250Aoverlap-matching%2520and%2520scene-reconstruction%2520errors%252C%2520%25283%2529%2520situation-transformation%250Areasoning%2520errors%252C%2520and%2520%25284%2529%2520spatial-logic%2520errors%252C%2520offering%2520valuable%2520insights%2520for%250Aadvancing%2520multi-image%2520spatial%2520intelligence.%2520Project%2520page%253A%250Ahttps%253A//runsenxu.com/projects/MMSI_Bench%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMSI-Bench%3A%20A%20Benchmark%20for%20Multi-Image%20Spatial%20Intelligence&entry.906535625=Sihan%20Yang%20and%20Runsen%20Xu%20and%20Yiman%20Xie%20and%20Sizhe%20Yang%20and%20Mo%20Li%20and%20Jingli%20Lin%20and%20Chenming%20Zhu%20and%20Xiaochen%20Chen%20and%20Haodong%20Duan%20and%20Xiangyu%20Yue%20and%20Dahua%20Lin%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Spatial%20intelligence%20is%20essential%20for%20multimodal%20large%20language%20models%0A%28MLLMs%29%20operating%20in%20the%20complex%20physical%20world.%20Existing%20benchmarks%2C%20however%2C%0Aprobe%20only%20single-image%20relations%20and%20thus%20fail%20to%20assess%20the%20multi-image%0Aspatial%20reasoning%20that%20real-world%20deployments%20demand.%20We%20introduce%20MMSI-Bench%2C%0Aa%20VQA%20benchmark%20dedicated%20to%20multi-image%20spatial%20intelligence.%20Six%203D-vision%0Aresearchers%20spent%20more%20than%20300%20hours%20meticulously%20crafting%201%2C000%20challenging%2C%0Aunambiguous%20multiple-choice%20questions%20from%20over%20120%2C000%20images%2C%20each%20paired%0Awith%20carefully%20designed%20distractors%20and%20a%20step-by-step%20reasoning%20process.%20We%0Aconduct%20extensive%20experiments%20and%20thoroughly%20evaluate%2034%20open-source%20and%0Aproprietary%20MLLMs%2C%20observing%20a%20wide%20gap%3A%20the%20strongest%20open-source%20model%0Aattains%20roughly%2030%25%20accuracy%20and%20OpenAI%27s%20o3%20reasoning%20model%20reaches%2040%25%2C%20while%0Ahumans%20score%2097%25.%20These%20results%20underscore%20the%20challenging%20nature%20of%20MMSI-Bench%0Aand%20the%20substantial%20headroom%20for%20future%20research.%20Leveraging%20the%20annotated%0Areasoning%20processes%2C%20we%20also%20provide%20an%20automated%20error%20analysis%20pipeline%20that%0Adiagnoses%20four%20dominant%20failure%20modes%2C%20including%20%281%29%20grounding%20errors%2C%20%282%29%0Aoverlap-matching%20and%20scene-reconstruction%20errors%2C%20%283%29%20situation-transformation%0Areasoning%20errors%2C%20and%20%284%29%20spatial-logic%20errors%2C%20offering%20valuable%20insights%20for%0Aadvancing%20multi-image%20spatial%20intelligence.%20Project%20page%3A%0Ahttps%3A//runsenxu.com/projects/MMSI_Bench%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23764v1&entry.124074799=Read"},
{"title": "PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening", "author": "Jeonghyeok Do and Sungpyo Kim and Geunhyuk Youk and Jaehyup Lee and Munchurl Kim", "abstract": "  PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with\nlow-resolution multi-spectral (MS) images to generate high-resolution\nmulti-spectral (HRMS) outputs. However, cross-modality misalignment -- caused\nby sensor placement, acquisition timing, and resolution disparity -- induces a\nfundamental challenge. Conventional deep learning methods assume perfect\npixel-wise alignment and rely on per-pixel reconstruction losses, leading to\nspectral distortion, double edges, and blurring when misalignment is present.\nTo address this, we propose PAN-Crafter, a modality-consistent alignment\nframework that explicitly mitigates the misalignment gap between PAN and MS\nmodalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a\nsingle network to jointly reconstruct HRMS and PAN images, leveraging PAN's\nhigh-frequency details as auxiliary self-supervision. Additionally, we\nintroduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism\nthat bidirectionally aligns MS texture to PAN structure and vice versa,\nenabling adaptive feature refinement across modalities. Extensive experiments\non multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the\nmost recent state-of-the-art method in all metrics, even with 50.11$\\times$\nfaster inference time and 0.63$\\times$ the memory size. Furthermore, it\ndemonstrates strong generalization performance on unseen satellite datasets,\nshowing its robustness across different conditions.\n", "link": "http://arxiv.org/abs/2505.23367v1", "date": "2025-05-29", "relevancy": 2.8999, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5992}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5768}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAN-Crafter%3A%20Learning%20Modality-Consistent%20Alignment%20for%20PAN-Sharpening&body=Title%3A%20PAN-Crafter%3A%20Learning%20Modality-Consistent%20Alignment%20for%20PAN-Sharpening%0AAuthor%3A%20Jeonghyeok%20Do%20and%20Sungpyo%20Kim%20and%20Geunhyuk%20Youk%20and%20Jaehyup%20Lee%20and%20Munchurl%20Kim%0AAbstract%3A%20%20%20PAN-sharpening%20aims%20to%20fuse%20high-resolution%20panchromatic%20%28PAN%29%20images%20with%0Alow-resolution%20multi-spectral%20%28MS%29%20images%20to%20generate%20high-resolution%0Amulti-spectral%20%28HRMS%29%20outputs.%20However%2C%20cross-modality%20misalignment%20--%20caused%0Aby%20sensor%20placement%2C%20acquisition%20timing%2C%20and%20resolution%20disparity%20--%20induces%20a%0Afundamental%20challenge.%20Conventional%20deep%20learning%20methods%20assume%20perfect%0Apixel-wise%20alignment%20and%20rely%20on%20per-pixel%20reconstruction%20losses%2C%20leading%20to%0Aspectral%20distortion%2C%20double%20edges%2C%20and%20blurring%20when%20misalignment%20is%20present.%0ATo%20address%20this%2C%20we%20propose%20PAN-Crafter%2C%20a%20modality-consistent%20alignment%0Aframework%20that%20explicitly%20mitigates%20the%20misalignment%20gap%20between%20PAN%20and%20MS%0Amodalities.%20At%20its%20core%2C%20Modality-Adaptive%20Reconstruction%20%28MARs%29%20enables%20a%0Asingle%20network%20to%20jointly%20reconstruct%20HRMS%20and%20PAN%20images%2C%20leveraging%20PAN%27s%0Ahigh-frequency%20details%20as%20auxiliary%20self-supervision.%20Additionally%2C%20we%0Aintroduce%20Cross-Modality%20Alignment-Aware%20Attention%20%28CM3A%29%2C%20a%20novel%20mechanism%0Athat%20bidirectionally%20aligns%20MS%20texture%20to%20PAN%20structure%20and%20vice%20versa%2C%0Aenabling%20adaptive%20feature%20refinement%20across%20modalities.%20Extensive%20experiments%0Aon%20multiple%20benchmark%20datasets%20demonstrate%20that%20our%20PAN-Crafter%20outperforms%20the%0Amost%20recent%20state-of-the-art%20method%20in%20all%20metrics%2C%20even%20with%2050.11%24%5Ctimes%24%0Afaster%20inference%20time%20and%200.63%24%5Ctimes%24%20the%20memory%20size.%20Furthermore%2C%20it%0Ademonstrates%20strong%20generalization%20performance%20on%20unseen%20satellite%20datasets%2C%0Ashowing%20its%20robustness%20across%20different%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAN-Crafter%253A%2520Learning%2520Modality-Consistent%2520Alignment%2520for%2520PAN-Sharpening%26entry.906535625%3DJeonghyeok%2520Do%2520and%2520Sungpyo%2520Kim%2520and%2520Geunhyuk%2520Youk%2520and%2520Jaehyup%2520Lee%2520and%2520Munchurl%2520Kim%26entry.1292438233%3D%2520%2520PAN-sharpening%2520aims%2520to%2520fuse%2520high-resolution%2520panchromatic%2520%2528PAN%2529%2520images%2520with%250Alow-resolution%2520multi-spectral%2520%2528MS%2529%2520images%2520to%2520generate%2520high-resolution%250Amulti-spectral%2520%2528HRMS%2529%2520outputs.%2520However%252C%2520cross-modality%2520misalignment%2520--%2520caused%250Aby%2520sensor%2520placement%252C%2520acquisition%2520timing%252C%2520and%2520resolution%2520disparity%2520--%2520induces%2520a%250Afundamental%2520challenge.%2520Conventional%2520deep%2520learning%2520methods%2520assume%2520perfect%250Apixel-wise%2520alignment%2520and%2520rely%2520on%2520per-pixel%2520reconstruction%2520losses%252C%2520leading%2520to%250Aspectral%2520distortion%252C%2520double%2520edges%252C%2520and%2520blurring%2520when%2520misalignment%2520is%2520present.%250ATo%2520address%2520this%252C%2520we%2520propose%2520PAN-Crafter%252C%2520a%2520modality-consistent%2520alignment%250Aframework%2520that%2520explicitly%2520mitigates%2520the%2520misalignment%2520gap%2520between%2520PAN%2520and%2520MS%250Amodalities.%2520At%2520its%2520core%252C%2520Modality-Adaptive%2520Reconstruction%2520%2528MARs%2529%2520enables%2520a%250Asingle%2520network%2520to%2520jointly%2520reconstruct%2520HRMS%2520and%2520PAN%2520images%252C%2520leveraging%2520PAN%2527s%250Ahigh-frequency%2520details%2520as%2520auxiliary%2520self-supervision.%2520Additionally%252C%2520we%250Aintroduce%2520Cross-Modality%2520Alignment-Aware%2520Attention%2520%2528CM3A%2529%252C%2520a%2520novel%2520mechanism%250Athat%2520bidirectionally%2520aligns%2520MS%2520texture%2520to%2520PAN%2520structure%2520and%2520vice%2520versa%252C%250Aenabling%2520adaptive%2520feature%2520refinement%2520across%2520modalities.%2520Extensive%2520experiments%250Aon%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520PAN-Crafter%2520outperforms%2520the%250Amost%2520recent%2520state-of-the-art%2520method%2520in%2520all%2520metrics%252C%2520even%2520with%252050.11%2524%255Ctimes%2524%250Afaster%2520inference%2520time%2520and%25200.63%2524%255Ctimes%2524%2520the%2520memory%2520size.%2520Furthermore%252C%2520it%250Ademonstrates%2520strong%2520generalization%2520performance%2520on%2520unseen%2520satellite%2520datasets%252C%250Ashowing%2520its%2520robustness%2520across%2520different%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAN-Crafter%3A%20Learning%20Modality-Consistent%20Alignment%20for%20PAN-Sharpening&entry.906535625=Jeonghyeok%20Do%20and%20Sungpyo%20Kim%20and%20Geunhyuk%20Youk%20and%20Jaehyup%20Lee%20and%20Munchurl%20Kim&entry.1292438233=%20%20PAN-sharpening%20aims%20to%20fuse%20high-resolution%20panchromatic%20%28PAN%29%20images%20with%0Alow-resolution%20multi-spectral%20%28MS%29%20images%20to%20generate%20high-resolution%0Amulti-spectral%20%28HRMS%29%20outputs.%20However%2C%20cross-modality%20misalignment%20--%20caused%0Aby%20sensor%20placement%2C%20acquisition%20timing%2C%20and%20resolution%20disparity%20--%20induces%20a%0Afundamental%20challenge.%20Conventional%20deep%20learning%20methods%20assume%20perfect%0Apixel-wise%20alignment%20and%20rely%20on%20per-pixel%20reconstruction%20losses%2C%20leading%20to%0Aspectral%20distortion%2C%20double%20edges%2C%20and%20blurring%20when%20misalignment%20is%20present.%0ATo%20address%20this%2C%20we%20propose%20PAN-Crafter%2C%20a%20modality-consistent%20alignment%0Aframework%20that%20explicitly%20mitigates%20the%20misalignment%20gap%20between%20PAN%20and%20MS%0Amodalities.%20At%20its%20core%2C%20Modality-Adaptive%20Reconstruction%20%28MARs%29%20enables%20a%0Asingle%20network%20to%20jointly%20reconstruct%20HRMS%20and%20PAN%20images%2C%20leveraging%20PAN%27s%0Ahigh-frequency%20details%20as%20auxiliary%20self-supervision.%20Additionally%2C%20we%0Aintroduce%20Cross-Modality%20Alignment-Aware%20Attention%20%28CM3A%29%2C%20a%20novel%20mechanism%0Athat%20bidirectionally%20aligns%20MS%20texture%20to%20PAN%20structure%20and%20vice%20versa%2C%0Aenabling%20adaptive%20feature%20refinement%20across%20modalities.%20Extensive%20experiments%0Aon%20multiple%20benchmark%20datasets%20demonstrate%20that%20our%20PAN-Crafter%20outperforms%20the%0Amost%20recent%20state-of-the-art%20method%20in%20all%20metrics%2C%20even%20with%2050.11%24%5Ctimes%24%0Afaster%20inference%20time%20and%200.63%24%5Ctimes%24%20the%20memory%20size.%20Furthermore%2C%20it%0Ademonstrates%20strong%20generalization%20performance%20on%20unseen%20satellite%20datasets%2C%0Ashowing%20its%20robustness%20across%20different%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23367v1&entry.124074799=Read"},
{"title": "PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views", "author": "Mohamed Rayan Barhdadi and Hasan Kurban and Hussein Alnuweiri", "abstract": "  PhysicsNeRF is a physically grounded framework for 3D reconstruction from\nsparse views, extending Neural Radiance Fields with four complementary\nconstraints: depth ranking, RegNeRF-style consistency, sparsity priors, and\ncross-view alignment. While standard NeRFs fail under sparse supervision,\nPhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB\naverage PSNR using only 8 views, outperforming prior methods. A generalization\ngap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental\nlimitations of sparse-view reconstruction. PhysicsNeRF enables physically\nconsistent, generalizable 3D representations for agent interaction and\nsimulation, and clarifies the expressiveness-generalization trade-off in\nconstrained NeRF models.\n", "link": "http://arxiv.org/abs/2505.23481v1", "date": "2025-05-29", "relevancy": 2.8845, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysicsNeRF%3A%20Physics-Guided%203D%20Reconstruction%20from%20Sparse%20Views&body=Title%3A%20PhysicsNeRF%3A%20Physics-Guided%203D%20Reconstruction%20from%20Sparse%20Views%0AAuthor%3A%20Mohamed%20Rayan%20Barhdadi%20and%20Hasan%20Kurban%20and%20Hussein%20Alnuweiri%0AAbstract%3A%20%20%20PhysicsNeRF%20is%20a%20physically%20grounded%20framework%20for%203D%20reconstruction%20from%0Asparse%20views%2C%20extending%20Neural%20Radiance%20Fields%20with%20four%20complementary%0Aconstraints%3A%20depth%20ranking%2C%20RegNeRF-style%20consistency%2C%20sparsity%20priors%2C%20and%0Across-view%20alignment.%20While%20standard%20NeRFs%20fail%20under%20sparse%20supervision%2C%0APhysicsNeRF%20employs%20a%20compact%200.67M-parameter%20architecture%20and%20achieves%2021.4%20dB%0Aaverage%20PSNR%20using%20only%208%20views%2C%20outperforming%20prior%20methods.%20A%20generalization%0Agap%20of%205.7-6.2%20dB%20is%20consistently%20observed%20and%20analyzed%2C%20revealing%20fundamental%0Alimitations%20of%20sparse-view%20reconstruction.%20PhysicsNeRF%20enables%20physically%0Aconsistent%2C%20generalizable%203D%20representations%20for%20agent%20interaction%20and%0Asimulation%2C%20and%20clarifies%20the%20expressiveness-generalization%20trade-off%20in%0Aconstrained%20NeRF%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysicsNeRF%253A%2520Physics-Guided%25203D%2520Reconstruction%2520from%2520Sparse%2520Views%26entry.906535625%3DMohamed%2520Rayan%2520Barhdadi%2520and%2520Hasan%2520Kurban%2520and%2520Hussein%2520Alnuweiri%26entry.1292438233%3D%2520%2520PhysicsNeRF%2520is%2520a%2520physically%2520grounded%2520framework%2520for%25203D%2520reconstruction%2520from%250Asparse%2520views%252C%2520extending%2520Neural%2520Radiance%2520Fields%2520with%2520four%2520complementary%250Aconstraints%253A%2520depth%2520ranking%252C%2520RegNeRF-style%2520consistency%252C%2520sparsity%2520priors%252C%2520and%250Across-view%2520alignment.%2520While%2520standard%2520NeRFs%2520fail%2520under%2520sparse%2520supervision%252C%250APhysicsNeRF%2520employs%2520a%2520compact%25200.67M-parameter%2520architecture%2520and%2520achieves%252021.4%2520dB%250Aaverage%2520PSNR%2520using%2520only%25208%2520views%252C%2520outperforming%2520prior%2520methods.%2520A%2520generalization%250Agap%2520of%25205.7-6.2%2520dB%2520is%2520consistently%2520observed%2520and%2520analyzed%252C%2520revealing%2520fundamental%250Alimitations%2520of%2520sparse-view%2520reconstruction.%2520PhysicsNeRF%2520enables%2520physically%250Aconsistent%252C%2520generalizable%25203D%2520representations%2520for%2520agent%2520interaction%2520and%250Asimulation%252C%2520and%2520clarifies%2520the%2520expressiveness-generalization%2520trade-off%2520in%250Aconstrained%2520NeRF%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysicsNeRF%3A%20Physics-Guided%203D%20Reconstruction%20from%20Sparse%20Views&entry.906535625=Mohamed%20Rayan%20Barhdadi%20and%20Hasan%20Kurban%20and%20Hussein%20Alnuweiri&entry.1292438233=%20%20PhysicsNeRF%20is%20a%20physically%20grounded%20framework%20for%203D%20reconstruction%20from%0Asparse%20views%2C%20extending%20Neural%20Radiance%20Fields%20with%20four%20complementary%0Aconstraints%3A%20depth%20ranking%2C%20RegNeRF-style%20consistency%2C%20sparsity%20priors%2C%20and%0Across-view%20alignment.%20While%20standard%20NeRFs%20fail%20under%20sparse%20supervision%2C%0APhysicsNeRF%20employs%20a%20compact%200.67M-parameter%20architecture%20and%20achieves%2021.4%20dB%0Aaverage%20PSNR%20using%20only%208%20views%2C%20outperforming%20prior%20methods.%20A%20generalization%0Agap%20of%205.7-6.2%20dB%20is%20consistently%20observed%20and%20analyzed%2C%20revealing%20fundamental%0Alimitations%20of%20sparse-view%20reconstruction.%20PhysicsNeRF%20enables%20physically%0Aconsistent%2C%20generalizable%203D%20representations%20for%20agent%20interaction%20and%0Asimulation%2C%20and%20clarifies%20the%20expressiveness-generalization%20trade-off%20in%0Aconstrained%20NeRF%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23481v1&entry.124074799=Read"},
{"title": "Graph Positional Autoencoders as Self-supervised Learners", "author": "Yang Liu and Deyu Bo and Wenxuan Cao and Yuan Fang and Yawen Li and Chuan Shi", "abstract": "  Graph self-supervised learning seeks to learn effective graph representations\nwithout relying on labeled data. Among various approaches, graph autoencoders\n(GAEs) have gained significant attention for their efficiency and scalability.\nTypically, GAEs take incomplete graphs as input and predict missing elements,\nsuch as masked nodes or edges. While effective, our experimental investigation\nreveals that traditional node or edge masking paradigms primarily capture\nlow-frequency signals in the graph and fail to learn the expressive structural\ninformation. To address these issues, we propose Graph Positional Autoencoders\n(GraphPAE), which employs a dual-path architecture to reconstruct both node\nfeatures and positions. Specifically, the feature path uses positional encoding\nto enhance the message-passing processing, improving GAE's ability to predict\nthe corrupted information. The position path, on the other hand, leverages node\nrepresentations to refine positions and approximate eigenvectors, thereby\nenabling the encoder to learn diverse frequency information. We conduct\nextensive experiments to verify the effectiveness of GraphPAE, including\nheterophilic node classification, graph property prediction, and transfer\nlearning. The results demonstrate that GraphPAE achieves state-of-the-art\nperformance and consistently outperforms baselines by a large margin.\n", "link": "http://arxiv.org/abs/2505.23345v1", "date": "2025-05-29", "relevancy": 2.8477, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6211}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6045}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Positional%20Autoencoders%20as%20Self-supervised%20Learners&body=Title%3A%20Graph%20Positional%20Autoencoders%20as%20Self-supervised%20Learners%0AAuthor%3A%20Yang%20Liu%20and%20Deyu%20Bo%20and%20Wenxuan%20Cao%20and%20Yuan%20Fang%20and%20Yawen%20Li%20and%20Chuan%20Shi%0AAbstract%3A%20%20%20Graph%20self-supervised%20learning%20seeks%20to%20learn%20effective%20graph%20representations%0Awithout%20relying%20on%20labeled%20data.%20Among%20various%20approaches%2C%20graph%20autoencoders%0A%28GAEs%29%20have%20gained%20significant%20attention%20for%20their%20efficiency%20and%20scalability.%0ATypically%2C%20GAEs%20take%20incomplete%20graphs%20as%20input%20and%20predict%20missing%20elements%2C%0Asuch%20as%20masked%20nodes%20or%20edges.%20While%20effective%2C%20our%20experimental%20investigation%0Areveals%20that%20traditional%20node%20or%20edge%20masking%20paradigms%20primarily%20capture%0Alow-frequency%20signals%20in%20the%20graph%20and%20fail%20to%20learn%20the%20expressive%20structural%0Ainformation.%20To%20address%20these%20issues%2C%20we%20propose%20Graph%20Positional%20Autoencoders%0A%28GraphPAE%29%2C%20which%20employs%20a%20dual-path%20architecture%20to%20reconstruct%20both%20node%0Afeatures%20and%20positions.%20Specifically%2C%20the%20feature%20path%20uses%20positional%20encoding%0Ato%20enhance%20the%20message-passing%20processing%2C%20improving%20GAE%27s%20ability%20to%20predict%0Athe%20corrupted%20information.%20The%20position%20path%2C%20on%20the%20other%20hand%2C%20leverages%20node%0Arepresentations%20to%20refine%20positions%20and%20approximate%20eigenvectors%2C%20thereby%0Aenabling%20the%20encoder%20to%20learn%20diverse%20frequency%20information.%20We%20conduct%0Aextensive%20experiments%20to%20verify%20the%20effectiveness%20of%20GraphPAE%2C%20including%0Aheterophilic%20node%20classification%2C%20graph%20property%20prediction%2C%20and%20transfer%0Alearning.%20The%20results%20demonstrate%20that%20GraphPAE%20achieves%20state-of-the-art%0Aperformance%20and%20consistently%20outperforms%20baselines%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Positional%2520Autoencoders%2520as%2520Self-supervised%2520Learners%26entry.906535625%3DYang%2520Liu%2520and%2520Deyu%2520Bo%2520and%2520Wenxuan%2520Cao%2520and%2520Yuan%2520Fang%2520and%2520Yawen%2520Li%2520and%2520Chuan%2520Shi%26entry.1292438233%3D%2520%2520Graph%2520self-supervised%2520learning%2520seeks%2520to%2520learn%2520effective%2520graph%2520representations%250Awithout%2520relying%2520on%2520labeled%2520data.%2520Among%2520various%2520approaches%252C%2520graph%2520autoencoders%250A%2528GAEs%2529%2520have%2520gained%2520significant%2520attention%2520for%2520their%2520efficiency%2520and%2520scalability.%250ATypically%252C%2520GAEs%2520take%2520incomplete%2520graphs%2520as%2520input%2520and%2520predict%2520missing%2520elements%252C%250Asuch%2520as%2520masked%2520nodes%2520or%2520edges.%2520While%2520effective%252C%2520our%2520experimental%2520investigation%250Areveals%2520that%2520traditional%2520node%2520or%2520edge%2520masking%2520paradigms%2520primarily%2520capture%250Alow-frequency%2520signals%2520in%2520the%2520graph%2520and%2520fail%2520to%2520learn%2520the%2520expressive%2520structural%250Ainformation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Graph%2520Positional%2520Autoencoders%250A%2528GraphPAE%2529%252C%2520which%2520employs%2520a%2520dual-path%2520architecture%2520to%2520reconstruct%2520both%2520node%250Afeatures%2520and%2520positions.%2520Specifically%252C%2520the%2520feature%2520path%2520uses%2520positional%2520encoding%250Ato%2520enhance%2520the%2520message-passing%2520processing%252C%2520improving%2520GAE%2527s%2520ability%2520to%2520predict%250Athe%2520corrupted%2520information.%2520The%2520position%2520path%252C%2520on%2520the%2520other%2520hand%252C%2520leverages%2520node%250Arepresentations%2520to%2520refine%2520positions%2520and%2520approximate%2520eigenvectors%252C%2520thereby%250Aenabling%2520the%2520encoder%2520to%2520learn%2520diverse%2520frequency%2520information.%2520We%2520conduct%250Aextensive%2520experiments%2520to%2520verify%2520the%2520effectiveness%2520of%2520GraphPAE%252C%2520including%250Aheterophilic%2520node%2520classification%252C%2520graph%2520property%2520prediction%252C%2520and%2520transfer%250Alearning.%2520The%2520results%2520demonstrate%2520that%2520GraphPAE%2520achieves%2520state-of-the-art%250Aperformance%2520and%2520consistently%2520outperforms%2520baselines%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Positional%20Autoencoders%20as%20Self-supervised%20Learners&entry.906535625=Yang%20Liu%20and%20Deyu%20Bo%20and%20Wenxuan%20Cao%20and%20Yuan%20Fang%20and%20Yawen%20Li%20and%20Chuan%20Shi&entry.1292438233=%20%20Graph%20self-supervised%20learning%20seeks%20to%20learn%20effective%20graph%20representations%0Awithout%20relying%20on%20labeled%20data.%20Among%20various%20approaches%2C%20graph%20autoencoders%0A%28GAEs%29%20have%20gained%20significant%20attention%20for%20their%20efficiency%20and%20scalability.%0ATypically%2C%20GAEs%20take%20incomplete%20graphs%20as%20input%20and%20predict%20missing%20elements%2C%0Asuch%20as%20masked%20nodes%20or%20edges.%20While%20effective%2C%20our%20experimental%20investigation%0Areveals%20that%20traditional%20node%20or%20edge%20masking%20paradigms%20primarily%20capture%0Alow-frequency%20signals%20in%20the%20graph%20and%20fail%20to%20learn%20the%20expressive%20structural%0Ainformation.%20To%20address%20these%20issues%2C%20we%20propose%20Graph%20Positional%20Autoencoders%0A%28GraphPAE%29%2C%20which%20employs%20a%20dual-path%20architecture%20to%20reconstruct%20both%20node%0Afeatures%20and%20positions.%20Specifically%2C%20the%20feature%20path%20uses%20positional%20encoding%0Ato%20enhance%20the%20message-passing%20processing%2C%20improving%20GAE%27s%20ability%20to%20predict%0Athe%20corrupted%20information.%20The%20position%20path%2C%20on%20the%20other%20hand%2C%20leverages%20node%0Arepresentations%20to%20refine%20positions%20and%20approximate%20eigenvectors%2C%20thereby%0Aenabling%20the%20encoder%20to%20learn%20diverse%20frequency%20information.%20We%20conduct%0Aextensive%20experiments%20to%20verify%20the%20effectiveness%20of%20GraphPAE%2C%20including%0Aheterophilic%20node%20classification%2C%20graph%20property%20prediction%2C%20and%20transfer%0Alearning.%20The%20results%20demonstrate%20that%20GraphPAE%20achieves%20state-of-the-art%0Aperformance%20and%20consistently%20outperforms%20baselines%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23345v1&entry.124074799=Read"},
{"title": "CLDTracker: A Comprehensive Language Description for Visual Tracking", "author": "Mohamad Alansari and Sajid Javed and Iyyakutti Iyappan Ganapathi and Sara Alansari and Muzammal Naseer", "abstract": "  VOT remains a fundamental yet challenging task in computer vision due to\ndynamic appearance changes, occlusions, and background clutter. Traditional\ntrackers, relying primarily on visual cues, often struggle in such complex\nscenarios. Recent advancements in VLMs have shown promise in semantic\nunderstanding for tasks like open-vocabulary detection and image captioning,\nsuggesting their potential for VOT. However, the direct application of VLMs to\nVOT is hindered by critical limitations: the absence of a rich and\ncomprehensive textual representation that semantically captures the target\nobject's nuances, limiting the effective use of language information;\ninefficient fusion mechanisms that fail to optimally integrate visual and\ntextual features, preventing a holistic understanding of the target; and a lack\nof temporal modeling of the target's evolving appearance in the language\ndomain, leading to a disconnect between the initial description and the\nobject's subsequent visual changes. To bridge these gaps and unlock the full\npotential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive\nLanguage Description framework for robust visual Tracking. Our tracker\nintroduces a dual-branch architecture consisting of a textual and a visual\nbranch. In the textual branch, we construct a rich bag of textual descriptions\nderived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with\nsemantic and contextual cues to address the lack of rich textual\nrepresentation. Experiments on six standard VOT benchmarks demonstrate that\nCLDTracker achieves SOTA performance, validating the effectiveness of\nleveraging robust and temporally-adaptive vision-language representations for\ntracking. Code and models are publicly available at:\nhttps://github.com/HamadYA/CLDTracker\n", "link": "http://arxiv.org/abs/2505.23704v1", "date": "2025-05-29", "relevancy": 2.8442, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLDTracker%3A%20A%20Comprehensive%20Language%20Description%20for%20Visual%20Tracking&body=Title%3A%20CLDTracker%3A%20A%20Comprehensive%20Language%20Description%20for%20Visual%20Tracking%0AAuthor%3A%20Mohamad%20Alansari%20and%20Sajid%20Javed%20and%20Iyyakutti%20Iyappan%20Ganapathi%20and%20Sara%20Alansari%20and%20Muzammal%20Naseer%0AAbstract%3A%20%20%20VOT%20remains%20a%20fundamental%20yet%20challenging%20task%20in%20computer%20vision%20due%20to%0Adynamic%20appearance%20changes%2C%20occlusions%2C%20and%20background%20clutter.%20Traditional%0Atrackers%2C%20relying%20primarily%20on%20visual%20cues%2C%20often%20struggle%20in%20such%20complex%0Ascenarios.%20Recent%20advancements%20in%20VLMs%20have%20shown%20promise%20in%20semantic%0Aunderstanding%20for%20tasks%20like%20open-vocabulary%20detection%20and%20image%20captioning%2C%0Asuggesting%20their%20potential%20for%20VOT.%20However%2C%20the%20direct%20application%20of%20VLMs%20to%0AVOT%20is%20hindered%20by%20critical%20limitations%3A%20the%20absence%20of%20a%20rich%20and%0Acomprehensive%20textual%20representation%20that%20semantically%20captures%20the%20target%0Aobject%27s%20nuances%2C%20limiting%20the%20effective%20use%20of%20language%20information%3B%0Ainefficient%20fusion%20mechanisms%20that%20fail%20to%20optimally%20integrate%20visual%20and%0Atextual%20features%2C%20preventing%20a%20holistic%20understanding%20of%20the%20target%3B%20and%20a%20lack%0Aof%20temporal%20modeling%20of%20the%20target%27s%20evolving%20appearance%20in%20the%20language%0Adomain%2C%20leading%20to%20a%20disconnect%20between%20the%20initial%20description%20and%20the%0Aobject%27s%20subsequent%20visual%20changes.%20To%20bridge%20these%20gaps%20and%20unlock%20the%20full%0Apotential%20of%20VLMs%20for%20VOT%2C%20we%20propose%20CLDTracker%2C%20a%20novel%20Comprehensive%0ALanguage%20Description%20framework%20for%20robust%20visual%20Tracking.%20Our%20tracker%0Aintroduces%20a%20dual-branch%20architecture%20consisting%20of%20a%20textual%20and%20a%20visual%0Abranch.%20In%20the%20textual%20branch%2C%20we%20construct%20a%20rich%20bag%20of%20textual%20descriptions%0Aderived%20by%20harnessing%20the%20powerful%20VLMs%20such%20as%20CLIP%20and%20GPT-4V%2C%20enriched%20with%0Asemantic%20and%20contextual%20cues%20to%20address%20the%20lack%20of%20rich%20textual%0Arepresentation.%20Experiments%20on%20six%20standard%20VOT%20benchmarks%20demonstrate%20that%0ACLDTracker%20achieves%20SOTA%20performance%2C%20validating%20the%20effectiveness%20of%0Aleveraging%20robust%20and%20temporally-adaptive%20vision-language%20representations%20for%0Atracking.%20Code%20and%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/HamadYA/CLDTracker%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLDTracker%253A%2520A%2520Comprehensive%2520Language%2520Description%2520for%2520Visual%2520Tracking%26entry.906535625%3DMohamad%2520Alansari%2520and%2520Sajid%2520Javed%2520and%2520Iyyakutti%2520Iyappan%2520Ganapathi%2520and%2520Sara%2520Alansari%2520and%2520Muzammal%2520Naseer%26entry.1292438233%3D%2520%2520VOT%2520remains%2520a%2520fundamental%2520yet%2520challenging%2520task%2520in%2520computer%2520vision%2520due%2520to%250Adynamic%2520appearance%2520changes%252C%2520occlusions%252C%2520and%2520background%2520clutter.%2520Traditional%250Atrackers%252C%2520relying%2520primarily%2520on%2520visual%2520cues%252C%2520often%2520struggle%2520in%2520such%2520complex%250Ascenarios.%2520Recent%2520advancements%2520in%2520VLMs%2520have%2520shown%2520promise%2520in%2520semantic%250Aunderstanding%2520for%2520tasks%2520like%2520open-vocabulary%2520detection%2520and%2520image%2520captioning%252C%250Asuggesting%2520their%2520potential%2520for%2520VOT.%2520However%252C%2520the%2520direct%2520application%2520of%2520VLMs%2520to%250AVOT%2520is%2520hindered%2520by%2520critical%2520limitations%253A%2520the%2520absence%2520of%2520a%2520rich%2520and%250Acomprehensive%2520textual%2520representation%2520that%2520semantically%2520captures%2520the%2520target%250Aobject%2527s%2520nuances%252C%2520limiting%2520the%2520effective%2520use%2520of%2520language%2520information%253B%250Ainefficient%2520fusion%2520mechanisms%2520that%2520fail%2520to%2520optimally%2520integrate%2520visual%2520and%250Atextual%2520features%252C%2520preventing%2520a%2520holistic%2520understanding%2520of%2520the%2520target%253B%2520and%2520a%2520lack%250Aof%2520temporal%2520modeling%2520of%2520the%2520target%2527s%2520evolving%2520appearance%2520in%2520the%2520language%250Adomain%252C%2520leading%2520to%2520a%2520disconnect%2520between%2520the%2520initial%2520description%2520and%2520the%250Aobject%2527s%2520subsequent%2520visual%2520changes.%2520To%2520bridge%2520these%2520gaps%2520and%2520unlock%2520the%2520full%250Apotential%2520of%2520VLMs%2520for%2520VOT%252C%2520we%2520propose%2520CLDTracker%252C%2520a%2520novel%2520Comprehensive%250ALanguage%2520Description%2520framework%2520for%2520robust%2520visual%2520Tracking.%2520Our%2520tracker%250Aintroduces%2520a%2520dual-branch%2520architecture%2520consisting%2520of%2520a%2520textual%2520and%2520a%2520visual%250Abranch.%2520In%2520the%2520textual%2520branch%252C%2520we%2520construct%2520a%2520rich%2520bag%2520of%2520textual%2520descriptions%250Aderived%2520by%2520harnessing%2520the%2520powerful%2520VLMs%2520such%2520as%2520CLIP%2520and%2520GPT-4V%252C%2520enriched%2520with%250Asemantic%2520and%2520contextual%2520cues%2520to%2520address%2520the%2520lack%2520of%2520rich%2520textual%250Arepresentation.%2520Experiments%2520on%2520six%2520standard%2520VOT%2520benchmarks%2520demonstrate%2520that%250ACLDTracker%2520achieves%2520SOTA%2520performance%252C%2520validating%2520the%2520effectiveness%2520of%250Aleveraging%2520robust%2520and%2520temporally-adaptive%2520vision-language%2520representations%2520for%250Atracking.%2520Code%2520and%2520models%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/HamadYA/CLDTracker%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLDTracker%3A%20A%20Comprehensive%20Language%20Description%20for%20Visual%20Tracking&entry.906535625=Mohamad%20Alansari%20and%20Sajid%20Javed%20and%20Iyyakutti%20Iyappan%20Ganapathi%20and%20Sara%20Alansari%20and%20Muzammal%20Naseer&entry.1292438233=%20%20VOT%20remains%20a%20fundamental%20yet%20challenging%20task%20in%20computer%20vision%20due%20to%0Adynamic%20appearance%20changes%2C%20occlusions%2C%20and%20background%20clutter.%20Traditional%0Atrackers%2C%20relying%20primarily%20on%20visual%20cues%2C%20often%20struggle%20in%20such%20complex%0Ascenarios.%20Recent%20advancements%20in%20VLMs%20have%20shown%20promise%20in%20semantic%0Aunderstanding%20for%20tasks%20like%20open-vocabulary%20detection%20and%20image%20captioning%2C%0Asuggesting%20their%20potential%20for%20VOT.%20However%2C%20the%20direct%20application%20of%20VLMs%20to%0AVOT%20is%20hindered%20by%20critical%20limitations%3A%20the%20absence%20of%20a%20rich%20and%0Acomprehensive%20textual%20representation%20that%20semantically%20captures%20the%20target%0Aobject%27s%20nuances%2C%20limiting%20the%20effective%20use%20of%20language%20information%3B%0Ainefficient%20fusion%20mechanisms%20that%20fail%20to%20optimally%20integrate%20visual%20and%0Atextual%20features%2C%20preventing%20a%20holistic%20understanding%20of%20the%20target%3B%20and%20a%20lack%0Aof%20temporal%20modeling%20of%20the%20target%27s%20evolving%20appearance%20in%20the%20language%0Adomain%2C%20leading%20to%20a%20disconnect%20between%20the%20initial%20description%20and%20the%0Aobject%27s%20subsequent%20visual%20changes.%20To%20bridge%20these%20gaps%20and%20unlock%20the%20full%0Apotential%20of%20VLMs%20for%20VOT%2C%20we%20propose%20CLDTracker%2C%20a%20novel%20Comprehensive%0ALanguage%20Description%20framework%20for%20robust%20visual%20Tracking.%20Our%20tracker%0Aintroduces%20a%20dual-branch%20architecture%20consisting%20of%20a%20textual%20and%20a%20visual%0Abranch.%20In%20the%20textual%20branch%2C%20we%20construct%20a%20rich%20bag%20of%20textual%20descriptions%0Aderived%20by%20harnessing%20the%20powerful%20VLMs%20such%20as%20CLIP%20and%20GPT-4V%2C%20enriched%20with%0Asemantic%20and%20contextual%20cues%20to%20address%20the%20lack%20of%20rich%20textual%0Arepresentation.%20Experiments%20on%20six%20standard%20VOT%20benchmarks%20demonstrate%20that%0ACLDTracker%20achieves%20SOTA%20performance%2C%20validating%20the%20effectiveness%20of%0Aleveraging%20robust%20and%20temporally-adaptive%20vision-language%20representations%20for%0Atracking.%20Code%20and%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/HamadYA/CLDTracker%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23704v1&entry.124074799=Read"},
{"title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction", "author": "Hao Dong and Moru Liu and Jian Liang and Eleni Chatzi and Olga Fink", "abstract": "  Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.\n", "link": "http://arxiv.org/abs/2505.23745v1", "date": "2025-05-29", "relevancy": 2.8439, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Trust%20Or%20Not%20To%20Trust%20Your%20Vision-Language%20Model%27s%20Prediction&body=Title%3A%20To%20Trust%20Or%20Not%20To%20Trust%20Your%20Vision-Language%20Model%27s%20Prediction%0AAuthor%3A%20Hao%20Dong%20and%20Moru%20Liu%20and%20Jian%20Liang%20and%20Eleni%20Chatzi%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20in%0Aaligning%20visual%20and%20textual%20modalities%2C%20enabling%20a%20wide%20range%20of%20applications%0Ain%20multimodal%20understanding%20and%20generation.%20While%20they%20excel%20in%20zero-shot%20and%0Atransfer%20learning%20scenarios%2C%20VLMs%20remain%20susceptible%20to%20misclassification%2C%0Aoften%20yielding%20confident%20yet%20incorrect%20predictions.%20This%20limitation%20poses%20a%0Asignificant%20risk%20in%20safety-critical%20domains%2C%20where%20erroneous%20predictions%20can%0Alead%20to%20severe%20consequences.%20In%20this%20work%2C%20we%20introduce%20TrustVLM%2C%20a%0Atraining-free%20framework%20designed%20to%20address%20the%20critical%20challenge%20of%0Aestimating%20when%20VLM%27s%20predictions%20can%20be%20trusted.%20Motivated%20by%20the%20observed%0Amodality%20gap%20in%20VLMs%20and%20the%20insight%20that%20certain%20concepts%20are%20more%20distinctly%0Arepresented%20in%20the%20image%20embedding%20space%2C%20we%20propose%20a%20novel%20confidence-scoring%0Afunction%20that%20leverages%20this%20space%20to%20improve%20misclassification%20detection.%20We%0Arigorously%20evaluate%20our%20approach%20across%2017%20diverse%20datasets%2C%20employing%204%0Aarchitectures%20and%202%20VLMs%2C%20and%20demonstrate%20state-of-the-art%20performance%2C%20with%0Aimprovements%20of%20up%20to%2051.87%25%20in%20AURC%2C%209.14%25%20in%20AUROC%2C%20and%2032.42%25%20in%20FPR95%0Acompared%20to%20existing%20baselines.%20By%20improving%20the%20reliability%20of%20the%20model%0Awithout%20requiring%20retraining%2C%20TrustVLM%20paves%20the%20way%20for%20safer%20deployment%20of%0AVLMs%20in%20real-world%20applications.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/EPFL-IMOS/TrustVLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Trust%2520Or%2520Not%2520To%2520Trust%2520Your%2520Vision-Language%2520Model%2527s%2520Prediction%26entry.906535625%3DHao%2520Dong%2520and%2520Moru%2520Liu%2520and%2520Jian%2520Liang%2520and%2520Eleni%2520Chatzi%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520in%250Aaligning%2520visual%2520and%2520textual%2520modalities%252C%2520enabling%2520a%2520wide%2520range%2520of%2520applications%250Ain%2520multimodal%2520understanding%2520and%2520generation.%2520While%2520they%2520excel%2520in%2520zero-shot%2520and%250Atransfer%2520learning%2520scenarios%252C%2520VLMs%2520remain%2520susceptible%2520to%2520misclassification%252C%250Aoften%2520yielding%2520confident%2520yet%2520incorrect%2520predictions.%2520This%2520limitation%2520poses%2520a%250Asignificant%2520risk%2520in%2520safety-critical%2520domains%252C%2520where%2520erroneous%2520predictions%2520can%250Alead%2520to%2520severe%2520consequences.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TrustVLM%252C%2520a%250Atraining-free%2520framework%2520designed%2520to%2520address%2520the%2520critical%2520challenge%2520of%250Aestimating%2520when%2520VLM%2527s%2520predictions%2520can%2520be%2520trusted.%2520Motivated%2520by%2520the%2520observed%250Amodality%2520gap%2520in%2520VLMs%2520and%2520the%2520insight%2520that%2520certain%2520concepts%2520are%2520more%2520distinctly%250Arepresented%2520in%2520the%2520image%2520embedding%2520space%252C%2520we%2520propose%2520a%2520novel%2520confidence-scoring%250Afunction%2520that%2520leverages%2520this%2520space%2520to%2520improve%2520misclassification%2520detection.%2520We%250Arigorously%2520evaluate%2520our%2520approach%2520across%252017%2520diverse%2520datasets%252C%2520employing%25204%250Aarchitectures%2520and%25202%2520VLMs%252C%2520and%2520demonstrate%2520state-of-the-art%2520performance%252C%2520with%250Aimprovements%2520of%2520up%2520to%252051.87%2525%2520in%2520AURC%252C%25209.14%2525%2520in%2520AUROC%252C%2520and%252032.42%2525%2520in%2520FPR95%250Acompared%2520to%2520existing%2520baselines.%2520By%2520improving%2520the%2520reliability%2520of%2520the%2520model%250Awithout%2520requiring%2520retraining%252C%2520TrustVLM%2520paves%2520the%2520way%2520for%2520safer%2520deployment%2520of%250AVLMs%2520in%2520real-world%2520applications.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/EPFL-IMOS/TrustVLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Trust%20Or%20Not%20To%20Trust%20Your%20Vision-Language%20Model%27s%20Prediction&entry.906535625=Hao%20Dong%20and%20Moru%20Liu%20and%20Jian%20Liang%20and%20Eleni%20Chatzi%20and%20Olga%20Fink&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20in%0Aaligning%20visual%20and%20textual%20modalities%2C%20enabling%20a%20wide%20range%20of%20applications%0Ain%20multimodal%20understanding%20and%20generation.%20While%20they%20excel%20in%20zero-shot%20and%0Atransfer%20learning%20scenarios%2C%20VLMs%20remain%20susceptible%20to%20misclassification%2C%0Aoften%20yielding%20confident%20yet%20incorrect%20predictions.%20This%20limitation%20poses%20a%0Asignificant%20risk%20in%20safety-critical%20domains%2C%20where%20erroneous%20predictions%20can%0Alead%20to%20severe%20consequences.%20In%20this%20work%2C%20we%20introduce%20TrustVLM%2C%20a%0Atraining-free%20framework%20designed%20to%20address%20the%20critical%20challenge%20of%0Aestimating%20when%20VLM%27s%20predictions%20can%20be%20trusted.%20Motivated%20by%20the%20observed%0Amodality%20gap%20in%20VLMs%20and%20the%20insight%20that%20certain%20concepts%20are%20more%20distinctly%0Arepresented%20in%20the%20image%20embedding%20space%2C%20we%20propose%20a%20novel%20confidence-scoring%0Afunction%20that%20leverages%20this%20space%20to%20improve%20misclassification%20detection.%20We%0Arigorously%20evaluate%20our%20approach%20across%2017%20diverse%20datasets%2C%20employing%204%0Aarchitectures%20and%202%20VLMs%2C%20and%20demonstrate%20state-of-the-art%20performance%2C%20with%0Aimprovements%20of%20up%20to%2051.87%25%20in%20AURC%2C%209.14%25%20in%20AUROC%2C%20and%2032.42%25%20in%20FPR95%0Acompared%20to%20existing%20baselines.%20By%20improving%20the%20reliability%20of%20the%20model%0Awithout%20requiring%20retraining%2C%20TrustVLM%20paves%20the%20way%20for%20safer%20deployment%20of%0AVLMs%20in%20real-world%20applications.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/EPFL-IMOS/TrustVLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23745v1&entry.124074799=Read"},
{"title": "UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric\n  Priors", "author": "Tianhang Wang and Fan Lu and Sanqing Qu and Guo Yu and Shihang Du and Ya Wu and Yuan Huang and Guang Chen", "abstract": "  Existing neural rendering-based urban scene reconstruction methods mainly\nfocus on the Interpolated View Synthesis (IVS) setting that synthesizes from\nviews close to training camera trajectory. However, IVS can not guarantee the\non-par performance of the novel view outside the training camera distribution\n(\\textit{e.g.}, looking left, right, or downwards), which limits the\ngeneralizability of the urban reconstruction application. Previous methods have\noptimized it via image diffusion, but they fail to handle text-ambiguous or\nlarge unseen view angles due to coarse-grained control of text-only diffusion.\nIn this paper, we design UrbanCraft, which surmounts the Extrapolated View\nSynthesis (EVS) problem using hierarchical sem-geometric representations\nserving as additional priors. Specifically, we leverage the partially\nobservable scene to reconstruct coarse semantic and geometric primitives,\nestablishing a coarse scene-level prior through an occupancy grid as the base\nrepresentation. Additionally, we incorporate fine instance-level priors from 3D\nbounding boxes to enhance object-level details and spatial relationships.\nBuilding on this, we propose the \\textbf{H}ierarchical\n\\textbf{S}emantic-Geometric-\\textbf{G}uided Variational Score Distillation\n(HSG-VSD), which integrates semantic and geometric constraints from pretrained\nUrbanCraft2D into the score distillation sampling process, forcing the\ndistribution to be consistent with the observable scene. Qualitative and\nquantitative comparisons demonstrate the effectiveness of our methods on EVS\nproblem.\n", "link": "http://arxiv.org/abs/2505.23434v1", "date": "2025-05-29", "relevancy": 2.8415, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5704}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5673}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanCraft%3A%20Urban%20View%20Extrapolation%20via%20Hierarchical%20Sem-Geometric%0A%20%20Priors&body=Title%3A%20UrbanCraft%3A%20Urban%20View%20Extrapolation%20via%20Hierarchical%20Sem-Geometric%0A%20%20Priors%0AAuthor%3A%20Tianhang%20Wang%20and%20Fan%20Lu%20and%20Sanqing%20Qu%20and%20Guo%20Yu%20and%20Shihang%20Du%20and%20Ya%20Wu%20and%20Yuan%20Huang%20and%20Guang%20Chen%0AAbstract%3A%20%20%20Existing%20neural%20rendering-based%20urban%20scene%20reconstruction%20methods%20mainly%0Afocus%20on%20the%20Interpolated%20View%20Synthesis%20%28IVS%29%20setting%20that%20synthesizes%20from%0Aviews%20close%20to%20training%20camera%20trajectory.%20However%2C%20IVS%20can%20not%20guarantee%20the%0Aon-par%20performance%20of%20the%20novel%20view%20outside%20the%20training%20camera%20distribution%0A%28%5Ctextit%7Be.g.%7D%2C%20looking%20left%2C%20right%2C%20or%20downwards%29%2C%20which%20limits%20the%0Ageneralizability%20of%20the%20urban%20reconstruction%20application.%20Previous%20methods%20have%0Aoptimized%20it%20via%20image%20diffusion%2C%20but%20they%20fail%20to%20handle%20text-ambiguous%20or%0Alarge%20unseen%20view%20angles%20due%20to%20coarse-grained%20control%20of%20text-only%20diffusion.%0AIn%20this%20paper%2C%20we%20design%20UrbanCraft%2C%20which%20surmounts%20the%20Extrapolated%20View%0ASynthesis%20%28EVS%29%20problem%20using%20hierarchical%20sem-geometric%20representations%0Aserving%20as%20additional%20priors.%20Specifically%2C%20we%20leverage%20the%20partially%0Aobservable%20scene%20to%20reconstruct%20coarse%20semantic%20and%20geometric%20primitives%2C%0Aestablishing%20a%20coarse%20scene-level%20prior%20through%20an%20occupancy%20grid%20as%20the%20base%0Arepresentation.%20Additionally%2C%20we%20incorporate%20fine%20instance-level%20priors%20from%203D%0Abounding%20boxes%20to%20enhance%20object-level%20details%20and%20spatial%20relationships.%0ABuilding%20on%20this%2C%20we%20propose%20the%20%5Ctextbf%7BH%7Dierarchical%0A%5Ctextbf%7BS%7Demantic-Geometric-%5Ctextbf%7BG%7Duided%20Variational%20Score%20Distillation%0A%28HSG-VSD%29%2C%20which%20integrates%20semantic%20and%20geometric%20constraints%20from%20pretrained%0AUrbanCraft2D%20into%20the%20score%20distillation%20sampling%20process%2C%20forcing%20the%0Adistribution%20to%20be%20consistent%20with%20the%20observable%20scene.%20Qualitative%20and%0Aquantitative%20comparisons%20demonstrate%20the%20effectiveness%20of%20our%20methods%20on%20EVS%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanCraft%253A%2520Urban%2520View%2520Extrapolation%2520via%2520Hierarchical%2520Sem-Geometric%250A%2520%2520Priors%26entry.906535625%3DTianhang%2520Wang%2520and%2520Fan%2520Lu%2520and%2520Sanqing%2520Qu%2520and%2520Guo%2520Yu%2520and%2520Shihang%2520Du%2520and%2520Ya%2520Wu%2520and%2520Yuan%2520Huang%2520and%2520Guang%2520Chen%26entry.1292438233%3D%2520%2520Existing%2520neural%2520rendering-based%2520urban%2520scene%2520reconstruction%2520methods%2520mainly%250Afocus%2520on%2520the%2520Interpolated%2520View%2520Synthesis%2520%2528IVS%2529%2520setting%2520that%2520synthesizes%2520from%250Aviews%2520close%2520to%2520training%2520camera%2520trajectory.%2520However%252C%2520IVS%2520can%2520not%2520guarantee%2520the%250Aon-par%2520performance%2520of%2520the%2520novel%2520view%2520outside%2520the%2520training%2520camera%2520distribution%250A%2528%255Ctextit%257Be.g.%257D%252C%2520looking%2520left%252C%2520right%252C%2520or%2520downwards%2529%252C%2520which%2520limits%2520the%250Ageneralizability%2520of%2520the%2520urban%2520reconstruction%2520application.%2520Previous%2520methods%2520have%250Aoptimized%2520it%2520via%2520image%2520diffusion%252C%2520but%2520they%2520fail%2520to%2520handle%2520text-ambiguous%2520or%250Alarge%2520unseen%2520view%2520angles%2520due%2520to%2520coarse-grained%2520control%2520of%2520text-only%2520diffusion.%250AIn%2520this%2520paper%252C%2520we%2520design%2520UrbanCraft%252C%2520which%2520surmounts%2520the%2520Extrapolated%2520View%250ASynthesis%2520%2528EVS%2529%2520problem%2520using%2520hierarchical%2520sem-geometric%2520representations%250Aserving%2520as%2520additional%2520priors.%2520Specifically%252C%2520we%2520leverage%2520the%2520partially%250Aobservable%2520scene%2520to%2520reconstruct%2520coarse%2520semantic%2520and%2520geometric%2520primitives%252C%250Aestablishing%2520a%2520coarse%2520scene-level%2520prior%2520through%2520an%2520occupancy%2520grid%2520as%2520the%2520base%250Arepresentation.%2520Additionally%252C%2520we%2520incorporate%2520fine%2520instance-level%2520priors%2520from%25203D%250Abounding%2520boxes%2520to%2520enhance%2520object-level%2520details%2520and%2520spatial%2520relationships.%250ABuilding%2520on%2520this%252C%2520we%2520propose%2520the%2520%255Ctextbf%257BH%257Dierarchical%250A%255Ctextbf%257BS%257Demantic-Geometric-%255Ctextbf%257BG%257Duided%2520Variational%2520Score%2520Distillation%250A%2528HSG-VSD%2529%252C%2520which%2520integrates%2520semantic%2520and%2520geometric%2520constraints%2520from%2520pretrained%250AUrbanCraft2D%2520into%2520the%2520score%2520distillation%2520sampling%2520process%252C%2520forcing%2520the%250Adistribution%2520to%2520be%2520consistent%2520with%2520the%2520observable%2520scene.%2520Qualitative%2520and%250Aquantitative%2520comparisons%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520methods%2520on%2520EVS%250Aproblem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanCraft%3A%20Urban%20View%20Extrapolation%20via%20Hierarchical%20Sem-Geometric%0A%20%20Priors&entry.906535625=Tianhang%20Wang%20and%20Fan%20Lu%20and%20Sanqing%20Qu%20and%20Guo%20Yu%20and%20Shihang%20Du%20and%20Ya%20Wu%20and%20Yuan%20Huang%20and%20Guang%20Chen&entry.1292438233=%20%20Existing%20neural%20rendering-based%20urban%20scene%20reconstruction%20methods%20mainly%0Afocus%20on%20the%20Interpolated%20View%20Synthesis%20%28IVS%29%20setting%20that%20synthesizes%20from%0Aviews%20close%20to%20training%20camera%20trajectory.%20However%2C%20IVS%20can%20not%20guarantee%20the%0Aon-par%20performance%20of%20the%20novel%20view%20outside%20the%20training%20camera%20distribution%0A%28%5Ctextit%7Be.g.%7D%2C%20looking%20left%2C%20right%2C%20or%20downwards%29%2C%20which%20limits%20the%0Ageneralizability%20of%20the%20urban%20reconstruction%20application.%20Previous%20methods%20have%0Aoptimized%20it%20via%20image%20diffusion%2C%20but%20they%20fail%20to%20handle%20text-ambiguous%20or%0Alarge%20unseen%20view%20angles%20due%20to%20coarse-grained%20control%20of%20text-only%20diffusion.%0AIn%20this%20paper%2C%20we%20design%20UrbanCraft%2C%20which%20surmounts%20the%20Extrapolated%20View%0ASynthesis%20%28EVS%29%20problem%20using%20hierarchical%20sem-geometric%20representations%0Aserving%20as%20additional%20priors.%20Specifically%2C%20we%20leverage%20the%20partially%0Aobservable%20scene%20to%20reconstruct%20coarse%20semantic%20and%20geometric%20primitives%2C%0Aestablishing%20a%20coarse%20scene-level%20prior%20through%20an%20occupancy%20grid%20as%20the%20base%0Arepresentation.%20Additionally%2C%20we%20incorporate%20fine%20instance-level%20priors%20from%203D%0Abounding%20boxes%20to%20enhance%20object-level%20details%20and%20spatial%20relationships.%0ABuilding%20on%20this%2C%20we%20propose%20the%20%5Ctextbf%7BH%7Dierarchical%0A%5Ctextbf%7BS%7Demantic-Geometric-%5Ctextbf%7BG%7Duided%20Variational%20Score%20Distillation%0A%28HSG-VSD%29%2C%20which%20integrates%20semantic%20and%20geometric%20constraints%20from%20pretrained%0AUrbanCraft2D%20into%20the%20score%20distillation%20sampling%20process%2C%20forcing%20the%0Adistribution%20to%20be%20consistent%20with%20the%20observable%20scene.%20Qualitative%20and%0Aquantitative%20comparisons%20demonstrate%20the%20effectiveness%20of%20our%20methods%20on%20EVS%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23434v1&entry.124074799=Read"},
{"title": "Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using\n  Vision-Language Model", "author": "Reem AlJunaid and Muzammil Behzad", "abstract": "  Generating informative and knowledge-rich image captions remains a challenge\nfor many existing captioning models, which often produce generic descriptions\nthat lack specificity and contextual depth. To address this limitation, we\npropose KRCapVLM, a knowledge replay-based novel image captioning framework\nusing vision-language model. We incorporate beam search decoding to generate\nmore diverse and coherent captions. We also integrate attention-based modules\ninto the image encoder to enhance feature representation. Finally, we employ\ntraining schedulers to improve stability and ensure smoother convergence during\ntraining. These proposals accelerate substantial gains in both caption quality\nand knowledge recognition. Our proposed model demonstrates clear improvements\nin both the accuracy of knowledge recognition and the overall quality of\ngenerated captions. It shows a stronger ability to generalize to previously\nunseen knowledge concepts, producing more informative and contextually relevant\ndescriptions. These results indicate the effectiveness of our approach in\nenhancing the model's capacity to generate meaningful, knowledge-grounded\ncaptions across a range of scenarios.\n", "link": "http://arxiv.org/abs/2505.23358v1", "date": "2025-05-29", "relevancy": 2.8262, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beam-Guided%20Knowledge%20Replay%20for%20Knowledge-Rich%20Image%20Captioning%20using%0A%20%20Vision-Language%20Model&body=Title%3A%20Beam-Guided%20Knowledge%20Replay%20for%20Knowledge-Rich%20Image%20Captioning%20using%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Reem%20AlJunaid%20and%20Muzammil%20Behzad%0AAbstract%3A%20%20%20Generating%20informative%20and%20knowledge-rich%20image%20captions%20remains%20a%20challenge%0Afor%20many%20existing%20captioning%20models%2C%20which%20often%20produce%20generic%20descriptions%0Athat%20lack%20specificity%20and%20contextual%20depth.%20To%20address%20this%20limitation%2C%20we%0Apropose%20KRCapVLM%2C%20a%20knowledge%20replay-based%20novel%20image%20captioning%20framework%0Ausing%20vision-language%20model.%20We%20incorporate%20beam%20search%20decoding%20to%20generate%0Amore%20diverse%20and%20coherent%20captions.%20We%20also%20integrate%20attention-based%20modules%0Ainto%20the%20image%20encoder%20to%20enhance%20feature%20representation.%20Finally%2C%20we%20employ%0Atraining%20schedulers%20to%20improve%20stability%20and%20ensure%20smoother%20convergence%20during%0Atraining.%20These%20proposals%20accelerate%20substantial%20gains%20in%20both%20caption%20quality%0Aand%20knowledge%20recognition.%20Our%20proposed%20model%20demonstrates%20clear%20improvements%0Ain%20both%20the%20accuracy%20of%20knowledge%20recognition%20and%20the%20overall%20quality%20of%0Agenerated%20captions.%20It%20shows%20a%20stronger%20ability%20to%20generalize%20to%20previously%0Aunseen%20knowledge%20concepts%2C%20producing%20more%20informative%20and%20contextually%20relevant%0Adescriptions.%20These%20results%20indicate%20the%20effectiveness%20of%20our%20approach%20in%0Aenhancing%20the%20model%27s%20capacity%20to%20generate%20meaningful%2C%20knowledge-grounded%0Acaptions%20across%20a%20range%20of%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeam-Guided%2520Knowledge%2520Replay%2520for%2520Knowledge-Rich%2520Image%2520Captioning%2520using%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DReem%2520AlJunaid%2520and%2520Muzammil%2520Behzad%26entry.1292438233%3D%2520%2520Generating%2520informative%2520and%2520knowledge-rich%2520image%2520captions%2520remains%2520a%2520challenge%250Afor%2520many%2520existing%2520captioning%2520models%252C%2520which%2520often%2520produce%2520generic%2520descriptions%250Athat%2520lack%2520specificity%2520and%2520contextual%2520depth.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520KRCapVLM%252C%2520a%2520knowledge%2520replay-based%2520novel%2520image%2520captioning%2520framework%250Ausing%2520vision-language%2520model.%2520We%2520incorporate%2520beam%2520search%2520decoding%2520to%2520generate%250Amore%2520diverse%2520and%2520coherent%2520captions.%2520We%2520also%2520integrate%2520attention-based%2520modules%250Ainto%2520the%2520image%2520encoder%2520to%2520enhance%2520feature%2520representation.%2520Finally%252C%2520we%2520employ%250Atraining%2520schedulers%2520to%2520improve%2520stability%2520and%2520ensure%2520smoother%2520convergence%2520during%250Atraining.%2520These%2520proposals%2520accelerate%2520substantial%2520gains%2520in%2520both%2520caption%2520quality%250Aand%2520knowledge%2520recognition.%2520Our%2520proposed%2520model%2520demonstrates%2520clear%2520improvements%250Ain%2520both%2520the%2520accuracy%2520of%2520knowledge%2520recognition%2520and%2520the%2520overall%2520quality%2520of%250Agenerated%2520captions.%2520It%2520shows%2520a%2520stronger%2520ability%2520to%2520generalize%2520to%2520previously%250Aunseen%2520knowledge%2520concepts%252C%2520producing%2520more%2520informative%2520and%2520contextually%2520relevant%250Adescriptions.%2520These%2520results%2520indicate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%250Aenhancing%2520the%2520model%2527s%2520capacity%2520to%2520generate%2520meaningful%252C%2520knowledge-grounded%250Acaptions%2520across%2520a%2520range%2520of%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beam-Guided%20Knowledge%20Replay%20for%20Knowledge-Rich%20Image%20Captioning%20using%0A%20%20Vision-Language%20Model&entry.906535625=Reem%20AlJunaid%20and%20Muzammil%20Behzad&entry.1292438233=%20%20Generating%20informative%20and%20knowledge-rich%20image%20captions%20remains%20a%20challenge%0Afor%20many%20existing%20captioning%20models%2C%20which%20often%20produce%20generic%20descriptions%0Athat%20lack%20specificity%20and%20contextual%20depth.%20To%20address%20this%20limitation%2C%20we%0Apropose%20KRCapVLM%2C%20a%20knowledge%20replay-based%20novel%20image%20captioning%20framework%0Ausing%20vision-language%20model.%20We%20incorporate%20beam%20search%20decoding%20to%20generate%0Amore%20diverse%20and%20coherent%20captions.%20We%20also%20integrate%20attention-based%20modules%0Ainto%20the%20image%20encoder%20to%20enhance%20feature%20representation.%20Finally%2C%20we%20employ%0Atraining%20schedulers%20to%20improve%20stability%20and%20ensure%20smoother%20convergence%20during%0Atraining.%20These%20proposals%20accelerate%20substantial%20gains%20in%20both%20caption%20quality%0Aand%20knowledge%20recognition.%20Our%20proposed%20model%20demonstrates%20clear%20improvements%0Ain%20both%20the%20accuracy%20of%20knowledge%20recognition%20and%20the%20overall%20quality%20of%0Agenerated%20captions.%20It%20shows%20a%20stronger%20ability%20to%20generalize%20to%20previously%0Aunseen%20knowledge%20concepts%2C%20producing%20more%20informative%20and%20contextually%20relevant%0Adescriptions.%20These%20results%20indicate%20the%20effectiveness%20of%20our%20approach%20in%0Aenhancing%20the%20model%27s%20capacity%20to%20generate%20meaningful%2C%20knowledge-grounded%0Acaptions%20across%20a%20range%20of%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23358v1&entry.124074799=Read"},
{"title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation", "author": "Size Wu and Zhonghua Wu and Zerui Gong and Qingyi Tao and Sheng Jin and Qinyue Li and Wei Li and Chen Change Loy", "abstract": "  In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.\n", "link": "http://arxiv.org/abs/2505.23661v1", "date": "2025-05-29", "relevancy": 2.8222, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation&body=Title%3A%20OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Size%20Wu%20and%20Zhonghua%20Wu%20and%20Zerui%20Gong%20and%20Qingyi%20Tao%20and%20Sheng%20Jin%20and%20Qinyue%20Li%20and%20Wei%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20present%20OpenUni%2C%20a%20simple%2C%20lightweight%2C%20and%20fully%0Aopen-source%20baseline%20for%20unifying%20multimodal%20understanding%20and%20generation.%0AInspired%20by%20prevailing%20practices%20in%20unified%20model%20learning%2C%20we%20adopt%20an%0Aefficient%20training%20strategy%20that%20minimizes%20the%20training%20complexity%20and%20overhead%0Aby%20bridging%20the%20off-the-shelf%20multimodal%20large%20language%20models%20%28LLMs%29%20and%0Adiffusion%20models%20through%20a%20set%20of%20learnable%20queries%20and%20a%20light-weight%0Atransformer-based%20connector.%20With%20a%20minimalist%20choice%20of%20architecture%2C%20we%0Ademonstrate%20that%20OpenUni%20can%3A%201%29%20generate%20high-quality%20and%20instruction-aligned%0Aimages%2C%20and%202%29%20achieve%20exceptional%20performance%20on%20standard%20benchmarks%20such%20as%0AGenEval%2C%20DPG-%20Bench%2C%20and%20WISE%2C%20with%20only%201.1B%20and%203.1B%20activated%20parameters.%20To%0Asupport%20open%20research%20and%20community%20advancement%2C%20we%20release%20all%20model%20weights%2C%0Atraining%20code%2C%20and%20our%20curated%20training%20datasets%20%28including%2023M%20image-text%0Apairs%29%20at%20https%3A//github.com/wusize/OpenUni.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenUni%253A%2520A%2520Simple%2520Baseline%2520for%2520Unified%2520Multimodal%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DSize%2520Wu%2520and%2520Zhonghua%2520Wu%2520and%2520Zerui%2520Gong%2520and%2520Qingyi%2520Tao%2520and%2520Sheng%2520Jin%2520and%2520Qinyue%2520Li%2520and%2520Wei%2520Li%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520present%2520OpenUni%252C%2520a%2520simple%252C%2520lightweight%252C%2520and%2520fully%250Aopen-source%2520baseline%2520for%2520unifying%2520multimodal%2520understanding%2520and%2520generation.%250AInspired%2520by%2520prevailing%2520practices%2520in%2520unified%2520model%2520learning%252C%2520we%2520adopt%2520an%250Aefficient%2520training%2520strategy%2520that%2520minimizes%2520the%2520training%2520complexity%2520and%2520overhead%250Aby%2520bridging%2520the%2520off-the-shelf%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Adiffusion%2520models%2520through%2520a%2520set%2520of%2520learnable%2520queries%2520and%2520a%2520light-weight%250Atransformer-based%2520connector.%2520With%2520a%2520minimalist%2520choice%2520of%2520architecture%252C%2520we%250Ademonstrate%2520that%2520OpenUni%2520can%253A%25201%2529%2520generate%2520high-quality%2520and%2520instruction-aligned%250Aimages%252C%2520and%25202%2529%2520achieve%2520exceptional%2520performance%2520on%2520standard%2520benchmarks%2520such%2520as%250AGenEval%252C%2520DPG-%2520Bench%252C%2520and%2520WISE%252C%2520with%2520only%25201.1B%2520and%25203.1B%2520activated%2520parameters.%2520To%250Asupport%2520open%2520research%2520and%2520community%2520advancement%252C%2520we%2520release%2520all%2520model%2520weights%252C%250Atraining%2520code%252C%2520and%2520our%2520curated%2520training%2520datasets%2520%2528including%252023M%2520image-text%250Apairs%2529%2520at%2520https%253A//github.com/wusize/OpenUni.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation&entry.906535625=Size%20Wu%20and%20Zhonghua%20Wu%20and%20Zerui%20Gong%20and%20Qingyi%20Tao%20and%20Sheng%20Jin%20and%20Qinyue%20Li%20and%20Wei%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20In%20this%20report%2C%20we%20present%20OpenUni%2C%20a%20simple%2C%20lightweight%2C%20and%20fully%0Aopen-source%20baseline%20for%20unifying%20multimodal%20understanding%20and%20generation.%0AInspired%20by%20prevailing%20practices%20in%20unified%20model%20learning%2C%20we%20adopt%20an%0Aefficient%20training%20strategy%20that%20minimizes%20the%20training%20complexity%20and%20overhead%0Aby%20bridging%20the%20off-the-shelf%20multimodal%20large%20language%20models%20%28LLMs%29%20and%0Adiffusion%20models%20through%20a%20set%20of%20learnable%20queries%20and%20a%20light-weight%0Atransformer-based%20connector.%20With%20a%20minimalist%20choice%20of%20architecture%2C%20we%0Ademonstrate%20that%20OpenUni%20can%3A%201%29%20generate%20high-quality%20and%20instruction-aligned%0Aimages%2C%20and%202%29%20achieve%20exceptional%20performance%20on%20standard%20benchmarks%20such%20as%0AGenEval%2C%20DPG-%20Bench%2C%20and%20WISE%2C%20with%20only%201.1B%20and%203.1B%20activated%20parameters.%20To%0Asupport%20open%20research%20and%20community%20advancement%2C%20we%20release%20all%20model%20weights%2C%0Atraining%20code%2C%20and%20our%20curated%20training%20datasets%20%28including%2023M%20image-text%0Apairs%29%20at%20https%3A//github.com/wusize/OpenUni.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23661v1&entry.124074799=Read"},
{"title": "Quality assessment of 3D human animation: Subjective and objective\n  evaluation", "author": "Rim Rekik and Stefanie Wuhrer and Ludovic Hoyet and Katja Zibrek and Anne-H\u00e9l\u00e8ne Olivier", "abstract": "  Virtual human animations have a wide range of applications in virtual and\naugmented reality. While automatic generation methods of animated virtual\nhumans have been developed, assessing their quality remains challenging.\nRecently, approaches introducing task-oriented evaluation metrics have been\nproposed, leveraging neural network training. However, quality assessment\nmeasures for animated virtual humans that are not generated with parametric\nbody models have yet to be developed. In this context, we introduce a first\nsuch quality assessment measure leveraging a novel data-driven framework.\nFirst, we generate a dataset of virtual human animations together with their\ncorresponding subjective realism evaluation scores collected with a user study.\nSecond, we use the resulting dataset to learn predicting perceptual evaluation\nscores. Results indicate that training a linear regressor on our dataset\nresults in a correlation of 90%, which outperforms a state of the art deep\nlearning baseline.\n", "link": "http://arxiv.org/abs/2505.23301v1", "date": "2025-05-29", "relevancy": 2.8196, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5977}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5516}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quality%20assessment%20of%203D%20human%20animation%3A%20Subjective%20and%20objective%0A%20%20evaluation&body=Title%3A%20Quality%20assessment%20of%203D%20human%20animation%3A%20Subjective%20and%20objective%0A%20%20evaluation%0AAuthor%3A%20Rim%20Rekik%20and%20Stefanie%20Wuhrer%20and%20Ludovic%20Hoyet%20and%20Katja%20Zibrek%20and%20Anne-H%C3%A9l%C3%A8ne%20Olivier%0AAbstract%3A%20%20%20Virtual%20human%20animations%20have%20a%20wide%20range%20of%20applications%20in%20virtual%20and%0Aaugmented%20reality.%20While%20automatic%20generation%20methods%20of%20animated%20virtual%0Ahumans%20have%20been%20developed%2C%20assessing%20their%20quality%20remains%20challenging.%0ARecently%2C%20approaches%20introducing%20task-oriented%20evaluation%20metrics%20have%20been%0Aproposed%2C%20leveraging%20neural%20network%20training.%20However%2C%20quality%20assessment%0Ameasures%20for%20animated%20virtual%20humans%20that%20are%20not%20generated%20with%20parametric%0Abody%20models%20have%20yet%20to%20be%20developed.%20In%20this%20context%2C%20we%20introduce%20a%20first%0Asuch%20quality%20assessment%20measure%20leveraging%20a%20novel%20data-driven%20framework.%0AFirst%2C%20we%20generate%20a%20dataset%20of%20virtual%20human%20animations%20together%20with%20their%0Acorresponding%20subjective%20realism%20evaluation%20scores%20collected%20with%20a%20user%20study.%0ASecond%2C%20we%20use%20the%20resulting%20dataset%20to%20learn%20predicting%20perceptual%20evaluation%0Ascores.%20Results%20indicate%20that%20training%20a%20linear%20regressor%20on%20our%20dataset%0Aresults%20in%20a%20correlation%20of%2090%25%2C%20which%20outperforms%20a%20state%20of%20the%20art%20deep%0Alearning%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuality%2520assessment%2520of%25203D%2520human%2520animation%253A%2520Subjective%2520and%2520objective%250A%2520%2520evaluation%26entry.906535625%3DRim%2520Rekik%2520and%2520Stefanie%2520Wuhrer%2520and%2520Ludovic%2520Hoyet%2520and%2520Katja%2520Zibrek%2520and%2520Anne-H%25C3%25A9l%25C3%25A8ne%2520Olivier%26entry.1292438233%3D%2520%2520Virtual%2520human%2520animations%2520have%2520a%2520wide%2520range%2520of%2520applications%2520in%2520virtual%2520and%250Aaugmented%2520reality.%2520While%2520automatic%2520generation%2520methods%2520of%2520animated%2520virtual%250Ahumans%2520have%2520been%2520developed%252C%2520assessing%2520their%2520quality%2520remains%2520challenging.%250ARecently%252C%2520approaches%2520introducing%2520task-oriented%2520evaluation%2520metrics%2520have%2520been%250Aproposed%252C%2520leveraging%2520neural%2520network%2520training.%2520However%252C%2520quality%2520assessment%250Ameasures%2520for%2520animated%2520virtual%2520humans%2520that%2520are%2520not%2520generated%2520with%2520parametric%250Abody%2520models%2520have%2520yet%2520to%2520be%2520developed.%2520In%2520this%2520context%252C%2520we%2520introduce%2520a%2520first%250Asuch%2520quality%2520assessment%2520measure%2520leveraging%2520a%2520novel%2520data-driven%2520framework.%250AFirst%252C%2520we%2520generate%2520a%2520dataset%2520of%2520virtual%2520human%2520animations%2520together%2520with%2520their%250Acorresponding%2520subjective%2520realism%2520evaluation%2520scores%2520collected%2520with%2520a%2520user%2520study.%250ASecond%252C%2520we%2520use%2520the%2520resulting%2520dataset%2520to%2520learn%2520predicting%2520perceptual%2520evaluation%250Ascores.%2520Results%2520indicate%2520that%2520training%2520a%2520linear%2520regressor%2520on%2520our%2520dataset%250Aresults%2520in%2520a%2520correlation%2520of%252090%2525%252C%2520which%2520outperforms%2520a%2520state%2520of%2520the%2520art%2520deep%250Alearning%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quality%20assessment%20of%203D%20human%20animation%3A%20Subjective%20and%20objective%0A%20%20evaluation&entry.906535625=Rim%20Rekik%20and%20Stefanie%20Wuhrer%20and%20Ludovic%20Hoyet%20and%20Katja%20Zibrek%20and%20Anne-H%C3%A9l%C3%A8ne%20Olivier&entry.1292438233=%20%20Virtual%20human%20animations%20have%20a%20wide%20range%20of%20applications%20in%20virtual%20and%0Aaugmented%20reality.%20While%20automatic%20generation%20methods%20of%20animated%20virtual%0Ahumans%20have%20been%20developed%2C%20assessing%20their%20quality%20remains%20challenging.%0ARecently%2C%20approaches%20introducing%20task-oriented%20evaluation%20metrics%20have%20been%0Aproposed%2C%20leveraging%20neural%20network%20training.%20However%2C%20quality%20assessment%0Ameasures%20for%20animated%20virtual%20humans%20that%20are%20not%20generated%20with%20parametric%0Abody%20models%20have%20yet%20to%20be%20developed.%20In%20this%20context%2C%20we%20introduce%20a%20first%0Asuch%20quality%20assessment%20measure%20leveraging%20a%20novel%20data-driven%20framework.%0AFirst%2C%20we%20generate%20a%20dataset%20of%20virtual%20human%20animations%20together%20with%20their%0Acorresponding%20subjective%20realism%20evaluation%20scores%20collected%20with%20a%20user%20study.%0ASecond%2C%20we%20use%20the%20resulting%20dataset%20to%20learn%20predicting%20perceptual%20evaluation%0Ascores.%20Results%20indicate%20that%20training%20a%20linear%20regressor%20on%20our%20dataset%0Aresults%20in%20a%20correlation%20of%2090%25%2C%20which%20outperforms%20a%20state%20of%20the%20art%20deep%0Alearning%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23301v1&entry.124074799=Read"},
{"title": "Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations\n  in Large Vision-Language Models", "author": "Jihoon Lee and Min Song", "abstract": "  Despite significant advancements in Large Vision-Language Models, Object\nHallucination (OH) remains a persistent challenge. Building upon prior studies\non contrastive decoding that address this issue without requiring additional\nmodel training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an\nadvanced method to suppress OH. RVCD leverages both negative and positive\nimages at the logit level, explicitly referencing AI-generated images designed\nto represent a single concept. Our approach demonstrates substantial\nimprovements over existing decoding-based methods.\n", "link": "http://arxiv.org/abs/2505.20569v2", "date": "2025-05-29", "relevancy": 2.8153, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval%20Visual%20Contrastive%20Decoding%20to%20Mitigate%20Object%20Hallucinations%0A%20%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Retrieval%20Visual%20Contrastive%20Decoding%20to%20Mitigate%20Object%20Hallucinations%0A%20%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Jihoon%20Lee%20and%20Min%20Song%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20Large%20Vision-Language%20Models%2C%20Object%0AHallucination%20%28OH%29%20remains%20a%20persistent%20challenge.%20Building%20upon%20prior%20studies%0Aon%20contrastive%20decoding%20that%20address%20this%20issue%20without%20requiring%20additional%0Amodel%20training%2C%20we%20introduce%20RVCD%20%28Retrieval%20Visual%20Contrastive%20Decoding%29%2C%20an%0Aadvanced%20method%20to%20suppress%20OH.%20RVCD%20leverages%20both%20negative%20and%20positive%0Aimages%20at%20the%20logit%20level%2C%20explicitly%20referencing%20AI-generated%20images%20designed%0Ato%20represent%20a%20single%20concept.%20Our%20approach%20demonstrates%20substantial%0Aimprovements%20over%20existing%20decoding-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval%2520Visual%2520Contrastive%2520Decoding%2520to%2520Mitigate%2520Object%2520Hallucinations%250A%2520%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DJihoon%2520Lee%2520and%2520Min%2520Song%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520Large%2520Vision-Language%2520Models%252C%2520Object%250AHallucination%2520%2528OH%2529%2520remains%2520a%2520persistent%2520challenge.%2520Building%2520upon%2520prior%2520studies%250Aon%2520contrastive%2520decoding%2520that%2520address%2520this%2520issue%2520without%2520requiring%2520additional%250Amodel%2520training%252C%2520we%2520introduce%2520RVCD%2520%2528Retrieval%2520Visual%2520Contrastive%2520Decoding%2529%252C%2520an%250Aadvanced%2520method%2520to%2520suppress%2520OH.%2520RVCD%2520leverages%2520both%2520negative%2520and%2520positive%250Aimages%2520at%2520the%2520logit%2520level%252C%2520explicitly%2520referencing%2520AI-generated%2520images%2520designed%250Ato%2520represent%2520a%2520single%2520concept.%2520Our%2520approach%2520demonstrates%2520substantial%250Aimprovements%2520over%2520existing%2520decoding-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval%20Visual%20Contrastive%20Decoding%20to%20Mitigate%20Object%20Hallucinations%0A%20%20in%20Large%20Vision-Language%20Models&entry.906535625=Jihoon%20Lee%20and%20Min%20Song&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20Large%20Vision-Language%20Models%2C%20Object%0AHallucination%20%28OH%29%20remains%20a%20persistent%20challenge.%20Building%20upon%20prior%20studies%0Aon%20contrastive%20decoding%20that%20address%20this%20issue%20without%20requiring%20additional%0Amodel%20training%2C%20we%20introduce%20RVCD%20%28Retrieval%20Visual%20Contrastive%20Decoding%29%2C%20an%0Aadvanced%20method%20to%20suppress%20OH.%20RVCD%20leverages%20both%20negative%20and%20positive%0Aimages%20at%20the%20logit%20level%2C%20explicitly%20referencing%20AI-generated%20images%20designed%0Ato%20represent%20a%20single%20concept.%20Our%20approach%20demonstrates%20substantial%0Aimprovements%20over%20existing%20decoding-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20569v2&entry.124074799=Read"},
{"title": "ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection\n  in Video Surveillance Applications", "author": "Tao Zhu and Qi Yu and Xinru Dong and Shiyu Li and Yue Liu and Jinlong Jiang and Lei Shu", "abstract": "  Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance\nLearning (MIL) suffers from label ambiguity, hindering discriminative feature\nlearning. We propose ProDisc-VAD, an efficient framework tackling this via two\nsynergistic components. The Prototype Interaction Layer (PIL) provides\ncontrolled normality modeling using a small set of learnable prototypes,\nestablishing a robust baseline without being overwhelmed by dominant normal\ndata. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts\nseparability by applying targeted contrastive learning exclusively to the most\nreliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD\nachieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M\nparameters, over 800x fewer than recent ViT-based methods like VadCLIP,\ndemonstrating exceptional efficiency alongside state-of-the-art performance.\nCode is available at https://github.com/modadundun/ProDisc-VAD.\n", "link": "http://arxiv.org/abs/2505.02179v2", "date": "2025-05-29", "relevancy": 2.8123, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5814}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5646}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProDisc-VAD%3A%20An%20Efficient%20System%20for%20Weakly-Supervised%20Anomaly%20Detection%0A%20%20in%20Video%20Surveillance%20Applications&body=Title%3A%20ProDisc-VAD%3A%20An%20Efficient%20System%20for%20Weakly-Supervised%20Anomaly%20Detection%0A%20%20in%20Video%20Surveillance%20Applications%0AAuthor%3A%20Tao%20Zhu%20and%20Qi%20Yu%20and%20Xinru%20Dong%20and%20Shiyu%20Li%20and%20Yue%20Liu%20and%20Jinlong%20Jiang%20and%20Lei%20Shu%0AAbstract%3A%20%20%20Weakly-supervised%20video%20anomaly%20detection%20%28WS-VAD%29%20using%20Multiple%20Instance%0ALearning%20%28MIL%29%20suffers%20from%20label%20ambiguity%2C%20hindering%20discriminative%20feature%0Alearning.%20We%20propose%20ProDisc-VAD%2C%20an%20efficient%20framework%20tackling%20this%20via%20two%0Asynergistic%20components.%20The%20Prototype%20Interaction%20Layer%20%28PIL%29%20provides%0Acontrolled%20normality%20modeling%20using%20a%20small%20set%20of%20learnable%20prototypes%2C%0Aestablishing%20a%20robust%20baseline%20without%20being%20overwhelmed%20by%20dominant%20normal%0Adata.%20The%20Pseudo-Instance%20Discriminative%20Enhancement%20%28PIDE%29%20loss%20boosts%0Aseparability%20by%20applying%20targeted%20contrastive%20learning%20exclusively%20to%20the%20most%0Areliable%20extreme-scoring%20instances%20%28highest/lowest%20scores%29.%20ProDisc-VAD%0Aachieves%20strong%20AUCs%20%2897.98%25%20ShanghaiTech%2C%2087.12%25%20UCF-Crime%29%20using%20only%200.4M%0Aparameters%2C%20over%20800x%20fewer%20than%20recent%20ViT-based%20methods%20like%20VadCLIP%2C%0Ademonstrating%20exceptional%20efficiency%20alongside%20state-of-the-art%20performance.%0ACode%20is%20available%20at%20https%3A//github.com/modadundun/ProDisc-VAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProDisc-VAD%253A%2520An%2520Efficient%2520System%2520for%2520Weakly-Supervised%2520Anomaly%2520Detection%250A%2520%2520in%2520Video%2520Surveillance%2520Applications%26entry.906535625%3DTao%2520Zhu%2520and%2520Qi%2520Yu%2520and%2520Xinru%2520Dong%2520and%2520Shiyu%2520Li%2520and%2520Yue%2520Liu%2520and%2520Jinlong%2520Jiang%2520and%2520Lei%2520Shu%26entry.1292438233%3D%2520%2520Weakly-supervised%2520video%2520anomaly%2520detection%2520%2528WS-VAD%2529%2520using%2520Multiple%2520Instance%250ALearning%2520%2528MIL%2529%2520suffers%2520from%2520label%2520ambiguity%252C%2520hindering%2520discriminative%2520feature%250Alearning.%2520We%2520propose%2520ProDisc-VAD%252C%2520an%2520efficient%2520framework%2520tackling%2520this%2520via%2520two%250Asynergistic%2520components.%2520The%2520Prototype%2520Interaction%2520Layer%2520%2528PIL%2529%2520provides%250Acontrolled%2520normality%2520modeling%2520using%2520a%2520small%2520set%2520of%2520learnable%2520prototypes%252C%250Aestablishing%2520a%2520robust%2520baseline%2520without%2520being%2520overwhelmed%2520by%2520dominant%2520normal%250Adata.%2520The%2520Pseudo-Instance%2520Discriminative%2520Enhancement%2520%2528PIDE%2529%2520loss%2520boosts%250Aseparability%2520by%2520applying%2520targeted%2520contrastive%2520learning%2520exclusively%2520to%2520the%2520most%250Areliable%2520extreme-scoring%2520instances%2520%2528highest/lowest%2520scores%2529.%2520ProDisc-VAD%250Aachieves%2520strong%2520AUCs%2520%252897.98%2525%2520ShanghaiTech%252C%252087.12%2525%2520UCF-Crime%2529%2520using%2520only%25200.4M%250Aparameters%252C%2520over%2520800x%2520fewer%2520than%2520recent%2520ViT-based%2520methods%2520like%2520VadCLIP%252C%250Ademonstrating%2520exceptional%2520efficiency%2520alongside%2520state-of-the-art%2520performance.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/modadundun/ProDisc-VAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProDisc-VAD%3A%20An%20Efficient%20System%20for%20Weakly-Supervised%20Anomaly%20Detection%0A%20%20in%20Video%20Surveillance%20Applications&entry.906535625=Tao%20Zhu%20and%20Qi%20Yu%20and%20Xinru%20Dong%20and%20Shiyu%20Li%20and%20Yue%20Liu%20and%20Jinlong%20Jiang%20and%20Lei%20Shu&entry.1292438233=%20%20Weakly-supervised%20video%20anomaly%20detection%20%28WS-VAD%29%20using%20Multiple%20Instance%0ALearning%20%28MIL%29%20suffers%20from%20label%20ambiguity%2C%20hindering%20discriminative%20feature%0Alearning.%20We%20propose%20ProDisc-VAD%2C%20an%20efficient%20framework%20tackling%20this%20via%20two%0Asynergistic%20components.%20The%20Prototype%20Interaction%20Layer%20%28PIL%29%20provides%0Acontrolled%20normality%20modeling%20using%20a%20small%20set%20of%20learnable%20prototypes%2C%0Aestablishing%20a%20robust%20baseline%20without%20being%20overwhelmed%20by%20dominant%20normal%0Adata.%20The%20Pseudo-Instance%20Discriminative%20Enhancement%20%28PIDE%29%20loss%20boosts%0Aseparability%20by%20applying%20targeted%20contrastive%20learning%20exclusively%20to%20the%20most%0Areliable%20extreme-scoring%20instances%20%28highest/lowest%20scores%29.%20ProDisc-VAD%0Aachieves%20strong%20AUCs%20%2897.98%25%20ShanghaiTech%2C%2087.12%25%20UCF-Crime%29%20using%20only%200.4M%0Aparameters%2C%20over%20800x%20fewer%20than%20recent%20ViT-based%20methods%20like%20VadCLIP%2C%0Ademonstrating%20exceptional%20efficiency%20alongside%20state-of-the-art%20performance.%0ACode%20is%20available%20at%20https%3A//github.com/modadundun/ProDisc-VAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02179v2&entry.124074799=Read"},
{"title": "A Benchmark and Evaluation for Real-World Out-of-Distribution Detection\n  Using Vision-Language Models", "author": "Shiho Noda and Atsuyuki Miyai and Qing Yu and Go Irie and Kiyoharu Aizawa", "abstract": "  Out-of-distribution (OOD) detection is a task that detects OOD samples during\ninference to ensure the safety of deployed models. However, conventional\nbenchmarks have reached performance saturation, making it difficult to compare\nrecent OOD detection methods. To address this challenge, we introduce three\nnovel OOD detection benchmarks that enable a deeper understanding of method\ncharacteristics and reflect real-world conditions. First, we present\nImageNet-X, designed to evaluate performance under challenging semantic shifts.\nSecond, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing\nrobustness to covariate shifts (feature distribution shifts). Finally, we\npropose Wilds-FS-X, which extends these evaluations to real-world datasets,\noffering a more comprehensive testbed. Our experiments reveal that recent\nCLIP-based OOD detection methods struggle to varying degrees across the three\nproposed benchmarks, and none of them consistently outperforms the others. We\nhope the community goes beyond specific benchmarks and includes more\nchallenging conditions reflecting real-world scenarios. The code is\nhttps://github.com/hoshi23/OOD-X-Benchmarks.\n", "link": "http://arxiv.org/abs/2501.18463v3", "date": "2025-05-29", "relevancy": 2.8097, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Benchmark%20and%20Evaluation%20for%20Real-World%20Out-of-Distribution%20Detection%0A%20%20Using%20Vision-Language%20Models&body=Title%3A%20A%20Benchmark%20and%20Evaluation%20for%20Real-World%20Out-of-Distribution%20Detection%0A%20%20Using%20Vision-Language%20Models%0AAuthor%3A%20Shiho%20Noda%20and%20Atsuyuki%20Miyai%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Kiyoharu%20Aizawa%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20a%20task%20that%20detects%20OOD%20samples%20during%0Ainference%20to%20ensure%20the%20safety%20of%20deployed%20models.%20However%2C%20conventional%0Abenchmarks%20have%20reached%20performance%20saturation%2C%20making%20it%20difficult%20to%20compare%0Arecent%20OOD%20detection%20methods.%20To%20address%20this%20challenge%2C%20we%20introduce%20three%0Anovel%20OOD%20detection%20benchmarks%20that%20enable%20a%20deeper%20understanding%20of%20method%0Acharacteristics%20and%20reflect%20real-world%20conditions.%20First%2C%20we%20present%0AImageNet-X%2C%20designed%20to%20evaluate%20performance%20under%20challenging%20semantic%20shifts.%0ASecond%2C%20we%20propose%20ImageNet-FS-X%20for%20full-spectrum%20OOD%20detection%2C%20assessing%0Arobustness%20to%20covariate%20shifts%20%28feature%20distribution%20shifts%29.%20Finally%2C%20we%0Apropose%20Wilds-FS-X%2C%20which%20extends%20these%20evaluations%20to%20real-world%20datasets%2C%0Aoffering%20a%20more%20comprehensive%20testbed.%20Our%20experiments%20reveal%20that%20recent%0ACLIP-based%20OOD%20detection%20methods%20struggle%20to%20varying%20degrees%20across%20the%20three%0Aproposed%20benchmarks%2C%20and%20none%20of%20them%20consistently%20outperforms%20the%20others.%20We%0Ahope%20the%20community%20goes%20beyond%20specific%20benchmarks%20and%20includes%20more%0Achallenging%20conditions%20reflecting%20real-world%20scenarios.%20The%20code%20is%0Ahttps%3A//github.com/hoshi23/OOD-X-Benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18463v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Benchmark%2520and%2520Evaluation%2520for%2520Real-World%2520Out-of-Distribution%2520Detection%250A%2520%2520Using%2520Vision-Language%2520Models%26entry.906535625%3DShiho%2520Noda%2520and%2520Atsuyuki%2520Miyai%2520and%2520Qing%2520Yu%2520and%2520Go%2520Irie%2520and%2520Kiyoharu%2520Aizawa%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520a%2520task%2520that%2520detects%2520OOD%2520samples%2520during%250Ainference%2520to%2520ensure%2520the%2520safety%2520of%2520deployed%2520models.%2520However%252C%2520conventional%250Abenchmarks%2520have%2520reached%2520performance%2520saturation%252C%2520making%2520it%2520difficult%2520to%2520compare%250Arecent%2520OOD%2520detection%2520methods.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520three%250Anovel%2520OOD%2520detection%2520benchmarks%2520that%2520enable%2520a%2520deeper%2520understanding%2520of%2520method%250Acharacteristics%2520and%2520reflect%2520real-world%2520conditions.%2520First%252C%2520we%2520present%250AImageNet-X%252C%2520designed%2520to%2520evaluate%2520performance%2520under%2520challenging%2520semantic%2520shifts.%250ASecond%252C%2520we%2520propose%2520ImageNet-FS-X%2520for%2520full-spectrum%2520OOD%2520detection%252C%2520assessing%250Arobustness%2520to%2520covariate%2520shifts%2520%2528feature%2520distribution%2520shifts%2529.%2520Finally%252C%2520we%250Apropose%2520Wilds-FS-X%252C%2520which%2520extends%2520these%2520evaluations%2520to%2520real-world%2520datasets%252C%250Aoffering%2520a%2520more%2520comprehensive%2520testbed.%2520Our%2520experiments%2520reveal%2520that%2520recent%250ACLIP-based%2520OOD%2520detection%2520methods%2520struggle%2520to%2520varying%2520degrees%2520across%2520the%2520three%250Aproposed%2520benchmarks%252C%2520and%2520none%2520of%2520them%2520consistently%2520outperforms%2520the%2520others.%2520We%250Ahope%2520the%2520community%2520goes%2520beyond%2520specific%2520benchmarks%2520and%2520includes%2520more%250Achallenging%2520conditions%2520reflecting%2520real-world%2520scenarios.%2520The%2520code%2520is%250Ahttps%253A//github.com/hoshi23/OOD-X-Benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18463v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Benchmark%20and%20Evaluation%20for%20Real-World%20Out-of-Distribution%20Detection%0A%20%20Using%20Vision-Language%20Models&entry.906535625=Shiho%20Noda%20and%20Atsuyuki%20Miyai%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Kiyoharu%20Aizawa&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20a%20task%20that%20detects%20OOD%20samples%20during%0Ainference%20to%20ensure%20the%20safety%20of%20deployed%20models.%20However%2C%20conventional%0Abenchmarks%20have%20reached%20performance%20saturation%2C%20making%20it%20difficult%20to%20compare%0Arecent%20OOD%20detection%20methods.%20To%20address%20this%20challenge%2C%20we%20introduce%20three%0Anovel%20OOD%20detection%20benchmarks%20that%20enable%20a%20deeper%20understanding%20of%20method%0Acharacteristics%20and%20reflect%20real-world%20conditions.%20First%2C%20we%20present%0AImageNet-X%2C%20designed%20to%20evaluate%20performance%20under%20challenging%20semantic%20shifts.%0ASecond%2C%20we%20propose%20ImageNet-FS-X%20for%20full-spectrum%20OOD%20detection%2C%20assessing%0Arobustness%20to%20covariate%20shifts%20%28feature%20distribution%20shifts%29.%20Finally%2C%20we%0Apropose%20Wilds-FS-X%2C%20which%20extends%20these%20evaluations%20to%20real-world%20datasets%2C%0Aoffering%20a%20more%20comprehensive%20testbed.%20Our%20experiments%20reveal%20that%20recent%0ACLIP-based%20OOD%20detection%20methods%20struggle%20to%20varying%20degrees%20across%20the%20three%0Aproposed%20benchmarks%2C%20and%20none%20of%20them%20consistently%20outperforms%20the%20others.%20We%0Ahope%20the%20community%20goes%20beyond%20specific%20benchmarks%20and%20includes%20more%0Achallenging%20conditions%20reflecting%20real-world%20scenarios.%20The%20code%20is%0Ahttps%3A//github.com/hoshi23/OOD-X-Benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18463v3&entry.124074799=Read"},
{"title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint", "author": "Heekyung Lee and Jiaxin Ge and Tsung-Han Wu and Minwoo Kang and Trevor Darrell and David M. Chan", "abstract": "  Rebus puzzles, visual riddles that encode language through imagery, spatial\narrangement, and symbolic substitution, pose a unique challenge to current\nvision-language models (VLMs). Unlike traditional image captioning or question\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\nrebus puzzles by constructing a hand-generated and annotated benchmark of\ndiverse English-language rebus puzzles, ranging from simple pictographic\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\nsurprising capabilities in decoding simple visual clues, they struggle\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\nunderstanding visual metaphors.\n", "link": "http://arxiv.org/abs/2505.23759v1", "date": "2025-05-29", "relevancy": 2.8034, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6006}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Puzzled%20by%20Puzzles%3A%20When%20Vision-Language%20Models%20Can%27t%20Take%20a%20Hint&body=Title%3A%20Puzzled%20by%20Puzzles%3A%20When%20Vision-Language%20Models%20Can%27t%20Take%20a%20Hint%0AAuthor%3A%20Heekyung%20Lee%20and%20Jiaxin%20Ge%20and%20Tsung-Han%20Wu%20and%20Minwoo%20Kang%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan%0AAbstract%3A%20%20%20Rebus%20puzzles%2C%20visual%20riddles%20that%20encode%20language%20through%20imagery%2C%20spatial%0Aarrangement%2C%20and%20symbolic%20substitution%2C%20pose%20a%20unique%20challenge%20to%20current%0Avision-language%20models%20%28VLMs%29.%20Unlike%20traditional%20image%20captioning%20or%20question%0Aanswering%20tasks%2C%20rebus%20solving%20requires%20multi-modal%20abstraction%2C%20symbolic%0Areasoning%2C%20and%20a%20grasp%20of%20cultural%2C%20phonetic%20and%20linguistic%20puns.%20In%20this%0Apaper%2C%20we%20investigate%20the%20capacity%20of%20contemporary%20VLMs%20to%20interpret%20and%20solve%0Arebus%20puzzles%20by%20constructing%20a%20hand-generated%20and%20annotated%20benchmark%20of%0Adiverse%20English-language%20rebus%20puzzles%2C%20ranging%20from%20simple%20pictographic%0Asubstitutions%20to%20spatially-dependent%20cues%20%28%22head%22%20over%20%22heels%22%29.%20We%20analyze%20how%0Adifferent%20VLMs%20perform%2C%20and%20our%20findings%20reveal%20that%20while%20VLMs%20exhibit%20some%0Asurprising%20capabilities%20in%20decoding%20simple%20visual%20clues%2C%20they%20struggle%0Asignificantly%20with%20tasks%20requiring%20abstract%20reasoning%2C%20lateral%20thinking%2C%20and%0Aunderstanding%20visual%20metaphors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPuzzled%2520by%2520Puzzles%253A%2520When%2520Vision-Language%2520Models%2520Can%2527t%2520Take%2520a%2520Hint%26entry.906535625%3DHeekyung%2520Lee%2520and%2520Jiaxin%2520Ge%2520and%2520Tsung-Han%2520Wu%2520and%2520Minwoo%2520Kang%2520and%2520Trevor%2520Darrell%2520and%2520David%2520M.%2520Chan%26entry.1292438233%3D%2520%2520Rebus%2520puzzles%252C%2520visual%2520riddles%2520that%2520encode%2520language%2520through%2520imagery%252C%2520spatial%250Aarrangement%252C%2520and%2520symbolic%2520substitution%252C%2520pose%2520a%2520unique%2520challenge%2520to%2520current%250Avision-language%2520models%2520%2528VLMs%2529.%2520Unlike%2520traditional%2520image%2520captioning%2520or%2520question%250Aanswering%2520tasks%252C%2520rebus%2520solving%2520requires%2520multi-modal%2520abstraction%252C%2520symbolic%250Areasoning%252C%2520and%2520a%2520grasp%2520of%2520cultural%252C%2520phonetic%2520and%2520linguistic%2520puns.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520capacity%2520of%2520contemporary%2520VLMs%2520to%2520interpret%2520and%2520solve%250Arebus%2520puzzles%2520by%2520constructing%2520a%2520hand-generated%2520and%2520annotated%2520benchmark%2520of%250Adiverse%2520English-language%2520rebus%2520puzzles%252C%2520ranging%2520from%2520simple%2520pictographic%250Asubstitutions%2520to%2520spatially-dependent%2520cues%2520%2528%2522head%2522%2520over%2520%2522heels%2522%2529.%2520We%2520analyze%2520how%250Adifferent%2520VLMs%2520perform%252C%2520and%2520our%2520findings%2520reveal%2520that%2520while%2520VLMs%2520exhibit%2520some%250Asurprising%2520capabilities%2520in%2520decoding%2520simple%2520visual%2520clues%252C%2520they%2520struggle%250Asignificantly%2520with%2520tasks%2520requiring%2520abstract%2520reasoning%252C%2520lateral%2520thinking%252C%2520and%250Aunderstanding%2520visual%2520metaphors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Puzzled%20by%20Puzzles%3A%20When%20Vision-Language%20Models%20Can%27t%20Take%20a%20Hint&entry.906535625=Heekyung%20Lee%20and%20Jiaxin%20Ge%20and%20Tsung-Han%20Wu%20and%20Minwoo%20Kang%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan&entry.1292438233=%20%20Rebus%20puzzles%2C%20visual%20riddles%20that%20encode%20language%20through%20imagery%2C%20spatial%0Aarrangement%2C%20and%20symbolic%20substitution%2C%20pose%20a%20unique%20challenge%20to%20current%0Avision-language%20models%20%28VLMs%29.%20Unlike%20traditional%20image%20captioning%20or%20question%0Aanswering%20tasks%2C%20rebus%20solving%20requires%20multi-modal%20abstraction%2C%20symbolic%0Areasoning%2C%20and%20a%20grasp%20of%20cultural%2C%20phonetic%20and%20linguistic%20puns.%20In%20this%0Apaper%2C%20we%20investigate%20the%20capacity%20of%20contemporary%20VLMs%20to%20interpret%20and%20solve%0Arebus%20puzzles%20by%20constructing%20a%20hand-generated%20and%20annotated%20benchmark%20of%0Adiverse%20English-language%20rebus%20puzzles%2C%20ranging%20from%20simple%20pictographic%0Asubstitutions%20to%20spatially-dependent%20cues%20%28%22head%22%20over%20%22heels%22%29.%20We%20analyze%20how%0Adifferent%20VLMs%20perform%2C%20and%20our%20findings%20reveal%20that%20while%20VLMs%20exhibit%20some%0Asurprising%20capabilities%20in%20decoding%20simple%20visual%20clues%2C%20they%20struggle%0Asignificantly%20with%20tasks%20requiring%20abstract%20reasoning%2C%20lateral%20thinking%2C%20and%0Aunderstanding%20visual%20metaphors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23759v1&entry.124074799=Read"},
{"title": "ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco\n  Reconstruction", "author": "Adeela Islam and Stefano Fiorini and Stuart James and Pietro Morerio and Alessio Del Bue", "abstract": "  The task of reassembly is a significant challenge across multiple domains,\nincluding archaeology, genomics, and molecular docking, requiring the precise\nplacement and orientation of elements to reconstruct an original structure. In\nthis work, we address key limitations in state-of-the-art Deep Learning methods\nfor reassembly, namely i) scalability; ii) multimodality; and iii) real-world\napplicability: beyond square or simple geometric shapes, realistic and complex\nerosion, or other real-world problems. We propose ReassembleNet, a method that\nreduces complexity by representing each input piece as a set of contour\nkeypoints and learning to select the most informative ones by Graph Neural\nNetworks pooling inspired techniques. ReassembleNet effectively lowers\ncomputational complexity while enabling the integration of features from\nmultiple modalities, including both geometric and texture data. Further\nenhanced through pretraining on a semi-synthetic dataset. We then apply\ndiffusion-based pose estimation to recover the original structure. We improve\non prior methods by 55% and 86% for RMSE Rotation and Translation,\nrespectively.\n", "link": "http://arxiv.org/abs/2505.21117v2", "date": "2025-05-29", "relevancy": 2.7875, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5905}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5446}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%0A%20%20Reconstruction&body=Title%3A%20ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%0A%20%20Reconstruction%0AAuthor%3A%20Adeela%20Islam%20and%20Stefano%20Fiorini%20and%20Stuart%20James%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20The%20task%20of%20reassembly%20is%20a%20significant%20challenge%20across%20multiple%20domains%2C%0Aincluding%20archaeology%2C%20genomics%2C%20and%20molecular%20docking%2C%20requiring%20the%20precise%0Aplacement%20and%20orientation%20of%20elements%20to%20reconstruct%20an%20original%20structure.%20In%0Athis%20work%2C%20we%20address%20key%20limitations%20in%20state-of-the-art%20Deep%20Learning%20methods%0Afor%20reassembly%2C%20namely%20i%29%20scalability%3B%20ii%29%20multimodality%3B%20and%20iii%29%20real-world%0Aapplicability%3A%20beyond%20square%20or%20simple%20geometric%20shapes%2C%20realistic%20and%20complex%0Aerosion%2C%20or%20other%20real-world%20problems.%20We%20propose%20ReassembleNet%2C%20a%20method%20that%0Areduces%20complexity%20by%20representing%20each%20input%20piece%20as%20a%20set%20of%20contour%0Akeypoints%20and%20learning%20to%20select%20the%20most%20informative%20ones%20by%20Graph%20Neural%0ANetworks%20pooling%20inspired%20techniques.%20ReassembleNet%20effectively%20lowers%0Acomputational%20complexity%20while%20enabling%20the%20integration%20of%20features%20from%0Amultiple%20modalities%2C%20including%20both%20geometric%20and%20texture%20data.%20Further%0Aenhanced%20through%20pretraining%20on%20a%20semi-synthetic%20dataset.%20We%20then%20apply%0Adiffusion-based%20pose%20estimation%20to%20recover%20the%20original%20structure.%20We%20improve%0Aon%20prior%20methods%20by%2055%25%20and%2086%25%20for%20RMSE%20Rotation%20and%20Translation%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21117v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReassembleNet%253A%2520Learnable%2520Keypoints%2520and%2520Diffusion%2520for%25202D%2520Fresco%250A%2520%2520Reconstruction%26entry.906535625%3DAdeela%2520Islam%2520and%2520Stefano%2520Fiorini%2520and%2520Stuart%2520James%2520and%2520Pietro%2520Morerio%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520reassembly%2520is%2520a%2520significant%2520challenge%2520across%2520multiple%2520domains%252C%250Aincluding%2520archaeology%252C%2520genomics%252C%2520and%2520molecular%2520docking%252C%2520requiring%2520the%2520precise%250Aplacement%2520and%2520orientation%2520of%2520elements%2520to%2520reconstruct%2520an%2520original%2520structure.%2520In%250Athis%2520work%252C%2520we%2520address%2520key%2520limitations%2520in%2520state-of-the-art%2520Deep%2520Learning%2520methods%250Afor%2520reassembly%252C%2520namely%2520i%2529%2520scalability%253B%2520ii%2529%2520multimodality%253B%2520and%2520iii%2529%2520real-world%250Aapplicability%253A%2520beyond%2520square%2520or%2520simple%2520geometric%2520shapes%252C%2520realistic%2520and%2520complex%250Aerosion%252C%2520or%2520other%2520real-world%2520problems.%2520We%2520propose%2520ReassembleNet%252C%2520a%2520method%2520that%250Areduces%2520complexity%2520by%2520representing%2520each%2520input%2520piece%2520as%2520a%2520set%2520of%2520contour%250Akeypoints%2520and%2520learning%2520to%2520select%2520the%2520most%2520informative%2520ones%2520by%2520Graph%2520Neural%250ANetworks%2520pooling%2520inspired%2520techniques.%2520ReassembleNet%2520effectively%2520lowers%250Acomputational%2520complexity%2520while%2520enabling%2520the%2520integration%2520of%2520features%2520from%250Amultiple%2520modalities%252C%2520including%2520both%2520geometric%2520and%2520texture%2520data.%2520Further%250Aenhanced%2520through%2520pretraining%2520on%2520a%2520semi-synthetic%2520dataset.%2520We%2520then%2520apply%250Adiffusion-based%2520pose%2520estimation%2520to%2520recover%2520the%2520original%2520structure.%2520We%2520improve%250Aon%2520prior%2520methods%2520by%252055%2525%2520and%252086%2525%2520for%2520RMSE%2520Rotation%2520and%2520Translation%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21117v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%0A%20%20Reconstruction&entry.906535625=Adeela%20Islam%20and%20Stefano%20Fiorini%20and%20Stuart%20James%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20The%20task%20of%20reassembly%20is%20a%20significant%20challenge%20across%20multiple%20domains%2C%0Aincluding%20archaeology%2C%20genomics%2C%20and%20molecular%20docking%2C%20requiring%20the%20precise%0Aplacement%20and%20orientation%20of%20elements%20to%20reconstruct%20an%20original%20structure.%20In%0Athis%20work%2C%20we%20address%20key%20limitations%20in%20state-of-the-art%20Deep%20Learning%20methods%0Afor%20reassembly%2C%20namely%20i%29%20scalability%3B%20ii%29%20multimodality%3B%20and%20iii%29%20real-world%0Aapplicability%3A%20beyond%20square%20or%20simple%20geometric%20shapes%2C%20realistic%20and%20complex%0Aerosion%2C%20or%20other%20real-world%20problems.%20We%20propose%20ReassembleNet%2C%20a%20method%20that%0Areduces%20complexity%20by%20representing%20each%20input%20piece%20as%20a%20set%20of%20contour%0Akeypoints%20and%20learning%20to%20select%20the%20most%20informative%20ones%20by%20Graph%20Neural%0ANetworks%20pooling%20inspired%20techniques.%20ReassembleNet%20effectively%20lowers%0Acomputational%20complexity%20while%20enabling%20the%20integration%20of%20features%20from%0Amultiple%20modalities%2C%20including%20both%20geometric%20and%20texture%20data.%20Further%0Aenhanced%20through%20pretraining%20on%20a%20semi-synthetic%20dataset.%20We%20then%20apply%0Adiffusion-based%20pose%20estimation%20to%20recover%20the%20original%20structure.%20We%20improve%0Aon%20prior%20methods%20by%2055%25%20and%2086%25%20for%20RMSE%20Rotation%20and%20Translation%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21117v2&entry.124074799=Read"},
{"title": "VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption\n  Quality Evaluation", "author": "Shi-Xue Zhang and Hongfa Wang and Duojun Huang and Xin Li and Xiaobin Zhu and Xu-Cheng Yin", "abstract": "  Video captions play a crucial role in text-to-video generation tasks, as\ntheir quality directly influences the semantic coherence and visual fidelity of\nthe generated videos. Although large vision-language models (VLMs) have\ndemonstrated significant potential in caption generation, existing benchmarks\ninadequately address fine-grained evaluation, particularly in capturing\nspatial-temporal details critical for video generation. To address this gap, we\nintroduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the\nfirst large-scale fine-grained benchmark comprising 5,677 (5K+) videos and\n109,796 (100K+) question-answer pairs. These QA-pairs are systematically\nannotated across 21 fine-grained dimensions (e.g., camera movement, and shot\ntype) that are empirically proven critical for text-to-video generation. We\nfurther introduce three metrics (Accuracy (AR), Inconsistency Rate (IR),\nCoverage Rate (CR)), and an automated evaluation pipeline leveraging large\nlanguage model (LLM) to verify caption quality via contrastive QA-pairs\nanalysis. By providing actionable insights for caption optimization, our\nbenchmark can advance the development of robust text-to-video models. The\ndataset and codes are available at website: https://github.com/GXYM/VCapsBench.\n", "link": "http://arxiv.org/abs/2505.23484v1", "date": "2025-05-29", "relevancy": 2.7746, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5627}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCapsBench%3A%20A%20Large-scale%20Fine-grained%20Benchmark%20for%20Video%20Caption%0A%20%20Quality%20Evaluation&body=Title%3A%20VCapsBench%3A%20A%20Large-scale%20Fine-grained%20Benchmark%20for%20Video%20Caption%0A%20%20Quality%20Evaluation%0AAuthor%3A%20Shi-Xue%20Zhang%20and%20Hongfa%20Wang%20and%20Duojun%20Huang%20and%20Xin%20Li%20and%20Xiaobin%20Zhu%20and%20Xu-Cheng%20Yin%0AAbstract%3A%20%20%20Video%20captions%20play%20a%20crucial%20role%20in%20text-to-video%20generation%20tasks%2C%20as%0Atheir%20quality%20directly%20influences%20the%20semantic%20coherence%20and%20visual%20fidelity%20of%0Athe%20generated%20videos.%20Although%20large%20vision-language%20models%20%28VLMs%29%20have%0Ademonstrated%20significant%20potential%20in%20caption%20generation%2C%20existing%20benchmarks%0Ainadequately%20address%20fine-grained%20evaluation%2C%20particularly%20in%20capturing%0Aspatial-temporal%20details%20critical%20for%20video%20generation.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20the%20Fine-grained%20Video%20Caption%20Evaluation%20Benchmark%20%28VCapsBench%29%2C%20the%0Afirst%20large-scale%20fine-grained%20benchmark%20comprising%205%2C677%20%285K%2B%29%20videos%20and%0A109%2C796%20%28100K%2B%29%20question-answer%20pairs.%20These%20QA-pairs%20are%20systematically%0Aannotated%20across%2021%20fine-grained%20dimensions%20%28e.g.%2C%20camera%20movement%2C%20and%20shot%0Atype%29%20that%20are%20empirically%20proven%20critical%20for%20text-to-video%20generation.%20We%0Afurther%20introduce%20three%20metrics%20%28Accuracy%20%28AR%29%2C%20Inconsistency%20Rate%20%28IR%29%2C%0ACoverage%20Rate%20%28CR%29%29%2C%20and%20an%20automated%20evaluation%20pipeline%20leveraging%20large%0Alanguage%20model%20%28LLM%29%20to%20verify%20caption%20quality%20via%20contrastive%20QA-pairs%0Aanalysis.%20By%20providing%20actionable%20insights%20for%20caption%20optimization%2C%20our%0Abenchmark%20can%20advance%20the%20development%20of%20robust%20text-to-video%20models.%20The%0Adataset%20and%20codes%20are%20available%20at%20website%3A%20https%3A//github.com/GXYM/VCapsBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCapsBench%253A%2520A%2520Large-scale%2520Fine-grained%2520Benchmark%2520for%2520Video%2520Caption%250A%2520%2520Quality%2520Evaluation%26entry.906535625%3DShi-Xue%2520Zhang%2520and%2520Hongfa%2520Wang%2520and%2520Duojun%2520Huang%2520and%2520Xin%2520Li%2520and%2520Xiaobin%2520Zhu%2520and%2520Xu-Cheng%2520Yin%26entry.1292438233%3D%2520%2520Video%2520captions%2520play%2520a%2520crucial%2520role%2520in%2520text-to-video%2520generation%2520tasks%252C%2520as%250Atheir%2520quality%2520directly%2520influences%2520the%2520semantic%2520coherence%2520and%2520visual%2520fidelity%2520of%250Athe%2520generated%2520videos.%2520Although%2520large%2520vision-language%2520models%2520%2528VLMs%2529%2520have%250Ademonstrated%2520significant%2520potential%2520in%2520caption%2520generation%252C%2520existing%2520benchmarks%250Ainadequately%2520address%2520fine-grained%2520evaluation%252C%2520particularly%2520in%2520capturing%250Aspatial-temporal%2520details%2520critical%2520for%2520video%2520generation.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520the%2520Fine-grained%2520Video%2520Caption%2520Evaluation%2520Benchmark%2520%2528VCapsBench%2529%252C%2520the%250Afirst%2520large-scale%2520fine-grained%2520benchmark%2520comprising%25205%252C677%2520%25285K%252B%2529%2520videos%2520and%250A109%252C796%2520%2528100K%252B%2529%2520question-answer%2520pairs.%2520These%2520QA-pairs%2520are%2520systematically%250Aannotated%2520across%252021%2520fine-grained%2520dimensions%2520%2528e.g.%252C%2520camera%2520movement%252C%2520and%2520shot%250Atype%2529%2520that%2520are%2520empirically%2520proven%2520critical%2520for%2520text-to-video%2520generation.%2520We%250Afurther%2520introduce%2520three%2520metrics%2520%2528Accuracy%2520%2528AR%2529%252C%2520Inconsistency%2520Rate%2520%2528IR%2529%252C%250ACoverage%2520Rate%2520%2528CR%2529%2529%252C%2520and%2520an%2520automated%2520evaluation%2520pipeline%2520leveraging%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520to%2520verify%2520caption%2520quality%2520via%2520contrastive%2520QA-pairs%250Aanalysis.%2520By%2520providing%2520actionable%2520insights%2520for%2520caption%2520optimization%252C%2520our%250Abenchmark%2520can%2520advance%2520the%2520development%2520of%2520robust%2520text-to-video%2520models.%2520The%250Adataset%2520and%2520codes%2520are%2520available%2520at%2520website%253A%2520https%253A//github.com/GXYM/VCapsBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCapsBench%3A%20A%20Large-scale%20Fine-grained%20Benchmark%20for%20Video%20Caption%0A%20%20Quality%20Evaluation&entry.906535625=Shi-Xue%20Zhang%20and%20Hongfa%20Wang%20and%20Duojun%20Huang%20and%20Xin%20Li%20and%20Xiaobin%20Zhu%20and%20Xu-Cheng%20Yin&entry.1292438233=%20%20Video%20captions%20play%20a%20crucial%20role%20in%20text-to-video%20generation%20tasks%2C%20as%0Atheir%20quality%20directly%20influences%20the%20semantic%20coherence%20and%20visual%20fidelity%20of%0Athe%20generated%20videos.%20Although%20large%20vision-language%20models%20%28VLMs%29%20have%0Ademonstrated%20significant%20potential%20in%20caption%20generation%2C%20existing%20benchmarks%0Ainadequately%20address%20fine-grained%20evaluation%2C%20particularly%20in%20capturing%0Aspatial-temporal%20details%20critical%20for%20video%20generation.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20the%20Fine-grained%20Video%20Caption%20Evaluation%20Benchmark%20%28VCapsBench%29%2C%20the%0Afirst%20large-scale%20fine-grained%20benchmark%20comprising%205%2C677%20%285K%2B%29%20videos%20and%0A109%2C796%20%28100K%2B%29%20question-answer%20pairs.%20These%20QA-pairs%20are%20systematically%0Aannotated%20across%2021%20fine-grained%20dimensions%20%28e.g.%2C%20camera%20movement%2C%20and%20shot%0Atype%29%20that%20are%20empirically%20proven%20critical%20for%20text-to-video%20generation.%20We%0Afurther%20introduce%20three%20metrics%20%28Accuracy%20%28AR%29%2C%20Inconsistency%20Rate%20%28IR%29%2C%0ACoverage%20Rate%20%28CR%29%29%2C%20and%20an%20automated%20evaluation%20pipeline%20leveraging%20large%0Alanguage%20model%20%28LLM%29%20to%20verify%20caption%20quality%20via%20contrastive%20QA-pairs%0Aanalysis.%20By%20providing%20actionable%20insights%20for%20caption%20optimization%2C%20our%0Abenchmark%20can%20advance%20the%20development%20of%20robust%20text-to-video%20models.%20The%0Adataset%20and%20codes%20are%20available%20at%20website%3A%20https%3A//github.com/GXYM/VCapsBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23484v1&entry.124074799=Read"},
{"title": "Impromptu VLA: Open Weights and Open Data for Driving\n  Vision-Language-Action Models", "author": "Haohan Chi and Huan-ang Gao and Ziming Liu and Jianing Liu and Chenyu Liu and Jinwei Li and Kaisen Yang and Yangcheng Yu and Zeda Wang and Wenyi Li and Leichen Wang and Xingtao Hu and Hao Sun and Hang Zhao and Hao Zhao", "abstract": "  Vision-Language-Action (VLA) models for autonomous driving show promise but\nfalter in unstructured corner case scenarios, largely due to a scarcity of\ntargeted benchmarks. To address this, we introduce Impromptu VLA. Our core\ncontribution is the Impromptu VLA Dataset: over 80,000 meticulously curated\nvideo clips, distilled from over 2M source clips sourced from 8 open-source\nlarge-scale datasets. This dataset is built upon our novel taxonomy of four\nchallenging unstructured categories and features rich, planning-oriented\nquestion-answering annotations and action trajectories. Crucially, experiments\ndemonstrate that VLAs trained with our dataset achieve substantial performance\ngains on established benchmarks--improving closed-loop NeuroNCAP scores and\ncollision rates, and reaching near state-of-the-art L2 accuracy in open-loop\nnuScenes trajectory prediction. Furthermore, our Q&A suite serves as an\neffective diagnostic, revealing clear VLM improvements in perception,\nprediction, and planning. Our code, data and models are available at\nhttps://github.com/ahydchh/Impromptu-VLA.\n", "link": "http://arxiv.org/abs/2505.23757v1", "date": "2025-05-29", "relevancy": 2.7691, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impromptu%20VLA%3A%20Open%20Weights%20and%20Open%20Data%20for%20Driving%0A%20%20Vision-Language-Action%20Models&body=Title%3A%20Impromptu%20VLA%3A%20Open%20Weights%20and%20Open%20Data%20for%20Driving%0A%20%20Vision-Language-Action%20Models%0AAuthor%3A%20Haohan%20Chi%20and%20Huan-ang%20Gao%20and%20Ziming%20Liu%20and%20Jianing%20Liu%20and%20Chenyu%20Liu%20and%20Jinwei%20Li%20and%20Kaisen%20Yang%20and%20Yangcheng%20Yu%20and%20Zeda%20Wang%20and%20Wenyi%20Li%20and%20Leichen%20Wang%20and%20Xingtao%20Hu%20and%20Hao%20Sun%20and%20Hang%20Zhao%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20for%20autonomous%20driving%20show%20promise%20but%0Afalter%20in%20unstructured%20corner%20case%20scenarios%2C%20largely%20due%20to%20a%20scarcity%20of%0Atargeted%20benchmarks.%20To%20address%20this%2C%20we%20introduce%20Impromptu%20VLA.%20Our%20core%0Acontribution%20is%20the%20Impromptu%20VLA%20Dataset%3A%20over%2080%2C000%20meticulously%20curated%0Avideo%20clips%2C%20distilled%20from%20over%202M%20source%20clips%20sourced%20from%208%20open-source%0Alarge-scale%20datasets.%20This%20dataset%20is%20built%20upon%20our%20novel%20taxonomy%20of%20four%0Achallenging%20unstructured%20categories%20and%20features%20rich%2C%20planning-oriented%0Aquestion-answering%20annotations%20and%20action%20trajectories.%20Crucially%2C%20experiments%0Ademonstrate%20that%20VLAs%20trained%20with%20our%20dataset%20achieve%20substantial%20performance%0Agains%20on%20established%20benchmarks--improving%20closed-loop%20NeuroNCAP%20scores%20and%0Acollision%20rates%2C%20and%20reaching%20near%20state-of-the-art%20L2%20accuracy%20in%20open-loop%0AnuScenes%20trajectory%20prediction.%20Furthermore%2C%20our%20Q%26A%20suite%20serves%20as%20an%0Aeffective%20diagnostic%2C%20revealing%20clear%20VLM%20improvements%20in%20perception%2C%0Aprediction%2C%20and%20planning.%20Our%20code%2C%20data%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/ahydchh/Impromptu-VLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpromptu%2520VLA%253A%2520Open%2520Weights%2520and%2520Open%2520Data%2520for%2520Driving%250A%2520%2520Vision-Language-Action%2520Models%26entry.906535625%3DHaohan%2520Chi%2520and%2520Huan-ang%2520Gao%2520and%2520Ziming%2520Liu%2520and%2520Jianing%2520Liu%2520and%2520Chenyu%2520Liu%2520and%2520Jinwei%2520Li%2520and%2520Kaisen%2520Yang%2520and%2520Yangcheng%2520Yu%2520and%2520Zeda%2520Wang%2520and%2520Wenyi%2520Li%2520and%2520Leichen%2520Wang%2520and%2520Xingtao%2520Hu%2520and%2520Hao%2520Sun%2520and%2520Hang%2520Zhao%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520for%2520autonomous%2520driving%2520show%2520promise%2520but%250Afalter%2520in%2520unstructured%2520corner%2520case%2520scenarios%252C%2520largely%2520due%2520to%2520a%2520scarcity%2520of%250Atargeted%2520benchmarks.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Impromptu%2520VLA.%2520Our%2520core%250Acontribution%2520is%2520the%2520Impromptu%2520VLA%2520Dataset%253A%2520over%252080%252C000%2520meticulously%2520curated%250Avideo%2520clips%252C%2520distilled%2520from%2520over%25202M%2520source%2520clips%2520sourced%2520from%25208%2520open-source%250Alarge-scale%2520datasets.%2520This%2520dataset%2520is%2520built%2520upon%2520our%2520novel%2520taxonomy%2520of%2520four%250Achallenging%2520unstructured%2520categories%2520and%2520features%2520rich%252C%2520planning-oriented%250Aquestion-answering%2520annotations%2520and%2520action%2520trajectories.%2520Crucially%252C%2520experiments%250Ademonstrate%2520that%2520VLAs%2520trained%2520with%2520our%2520dataset%2520achieve%2520substantial%2520performance%250Agains%2520on%2520established%2520benchmarks--improving%2520closed-loop%2520NeuroNCAP%2520scores%2520and%250Acollision%2520rates%252C%2520and%2520reaching%2520near%2520state-of-the-art%2520L2%2520accuracy%2520in%2520open-loop%250AnuScenes%2520trajectory%2520prediction.%2520Furthermore%252C%2520our%2520Q%2526A%2520suite%2520serves%2520as%2520an%250Aeffective%2520diagnostic%252C%2520revealing%2520clear%2520VLM%2520improvements%2520in%2520perception%252C%250Aprediction%252C%2520and%2520planning.%2520Our%2520code%252C%2520data%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/ahydchh/Impromptu-VLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impromptu%20VLA%3A%20Open%20Weights%20and%20Open%20Data%20for%20Driving%0A%20%20Vision-Language-Action%20Models&entry.906535625=Haohan%20Chi%20and%20Huan-ang%20Gao%20and%20Ziming%20Liu%20and%20Jianing%20Liu%20and%20Chenyu%20Liu%20and%20Jinwei%20Li%20and%20Kaisen%20Yang%20and%20Yangcheng%20Yu%20and%20Zeda%20Wang%20and%20Wenyi%20Li%20and%20Leichen%20Wang%20and%20Xingtao%20Hu%20and%20Hao%20Sun%20and%20Hang%20Zhao%20and%20Hao%20Zhao&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20for%20autonomous%20driving%20show%20promise%20but%0Afalter%20in%20unstructured%20corner%20case%20scenarios%2C%20largely%20due%20to%20a%20scarcity%20of%0Atargeted%20benchmarks.%20To%20address%20this%2C%20we%20introduce%20Impromptu%20VLA.%20Our%20core%0Acontribution%20is%20the%20Impromptu%20VLA%20Dataset%3A%20over%2080%2C000%20meticulously%20curated%0Avideo%20clips%2C%20distilled%20from%20over%202M%20source%20clips%20sourced%20from%208%20open-source%0Alarge-scale%20datasets.%20This%20dataset%20is%20built%20upon%20our%20novel%20taxonomy%20of%20four%0Achallenging%20unstructured%20categories%20and%20features%20rich%2C%20planning-oriented%0Aquestion-answering%20annotations%20and%20action%20trajectories.%20Crucially%2C%20experiments%0Ademonstrate%20that%20VLAs%20trained%20with%20our%20dataset%20achieve%20substantial%20performance%0Agains%20on%20established%20benchmarks--improving%20closed-loop%20NeuroNCAP%20scores%20and%0Acollision%20rates%2C%20and%20reaching%20near%20state-of-the-art%20L2%20accuracy%20in%20open-loop%0AnuScenes%20trajectory%20prediction.%20Furthermore%2C%20our%20Q%26A%20suite%20serves%20as%20an%0Aeffective%20diagnostic%2C%20revealing%20clear%20VLM%20improvements%20in%20perception%2C%0Aprediction%2C%20and%20planning.%20Our%20code%2C%20data%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/ahydchh/Impromptu-VLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23757v1&entry.124074799=Read"},
{"title": "Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation", "author": "Lingyan Ran and Yali Li and Tao Zhuo and Shizhou Zhang and Yanning Zhang", "abstract": "  In semi-supervised semantic segmentation (SSSS), data augmentation plays a\ncrucial role in the weak-to-strong consistency regularization framework, as it\nenhances diversity and improves model generalization. Recent strong\naugmentation methods have primarily focused on intensity-based perturbations,\nwhich have minimal impact on the semantic masks. In contrast, spatial\naugmentations like translation and rotation have long been acknowledged for\ntheir effectiveness in supervised semantic segmentation tasks, but they are\noften ignored in SSSS. In this work, we demonstrate that spatial augmentation\ncan also contribute to model training in SSSS, despite generating inconsistent\nmasks between the weak and strong augmentations. Furthermore, recognizing the\nvariability among images, we propose an adaptive augmentation strategy that\ndynamically adjusts the augmentation for each instance based on entropy.\nExtensive experiments show that our proposed Adaptive Spatial Augmentation\n(\\textbf{ASAug}) can be integrated as a pluggable module, consistently\nimproving the performance of existing methods and achieving state-of-the-art\nresults on benchmark datasets such as PASCAL VOC 2012, Cityscapes, and COCO.\n", "link": "http://arxiv.org/abs/2505.23438v1", "date": "2025-05-29", "relevancy": 2.7389, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Spatial%20Augmentation%20for%20Semi-supervised%20Semantic%20Segmentation&body=Title%3A%20Adaptive%20Spatial%20Augmentation%20for%20Semi-supervised%20Semantic%20Segmentation%0AAuthor%3A%20Lingyan%20Ran%20and%20Yali%20Li%20and%20Tao%20Zhuo%20and%20Shizhou%20Zhang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20In%20semi-supervised%20semantic%20segmentation%20%28SSSS%29%2C%20data%20augmentation%20plays%20a%0Acrucial%20role%20in%20the%20weak-to-strong%20consistency%20regularization%20framework%2C%20as%20it%0Aenhances%20diversity%20and%20improves%20model%20generalization.%20Recent%20strong%0Aaugmentation%20methods%20have%20primarily%20focused%20on%20intensity-based%20perturbations%2C%0Awhich%20have%20minimal%20impact%20on%20the%20semantic%20masks.%20In%20contrast%2C%20spatial%0Aaugmentations%20like%20translation%20and%20rotation%20have%20long%20been%20acknowledged%20for%0Atheir%20effectiveness%20in%20supervised%20semantic%20segmentation%20tasks%2C%20but%20they%20are%0Aoften%20ignored%20in%20SSSS.%20In%20this%20work%2C%20we%20demonstrate%20that%20spatial%20augmentation%0Acan%20also%20contribute%20to%20model%20training%20in%20SSSS%2C%20despite%20generating%20inconsistent%0Amasks%20between%20the%20weak%20and%20strong%20augmentations.%20Furthermore%2C%20recognizing%20the%0Avariability%20among%20images%2C%20we%20propose%20an%20adaptive%20augmentation%20strategy%20that%0Adynamically%20adjusts%20the%20augmentation%20for%20each%20instance%20based%20on%20entropy.%0AExtensive%20experiments%20show%20that%20our%20proposed%20Adaptive%20Spatial%20Augmentation%0A%28%5Ctextbf%7BASAug%7D%29%20can%20be%20integrated%20as%20a%20pluggable%20module%2C%20consistently%0Aimproving%20the%20performance%20of%20existing%20methods%20and%20achieving%20state-of-the-art%0Aresults%20on%20benchmark%20datasets%20such%20as%20PASCAL%20VOC%202012%2C%20Cityscapes%2C%20and%20COCO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Spatial%2520Augmentation%2520for%2520Semi-supervised%2520Semantic%2520Segmentation%26entry.906535625%3DLingyan%2520Ran%2520and%2520Yali%2520Li%2520and%2520Tao%2520Zhuo%2520and%2520Shizhou%2520Zhang%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520In%2520semi-supervised%2520semantic%2520segmentation%2520%2528SSSS%2529%252C%2520data%2520augmentation%2520plays%2520a%250Acrucial%2520role%2520in%2520the%2520weak-to-strong%2520consistency%2520regularization%2520framework%252C%2520as%2520it%250Aenhances%2520diversity%2520and%2520improves%2520model%2520generalization.%2520Recent%2520strong%250Aaugmentation%2520methods%2520have%2520primarily%2520focused%2520on%2520intensity-based%2520perturbations%252C%250Awhich%2520have%2520minimal%2520impact%2520on%2520the%2520semantic%2520masks.%2520In%2520contrast%252C%2520spatial%250Aaugmentations%2520like%2520translation%2520and%2520rotation%2520have%2520long%2520been%2520acknowledged%2520for%250Atheir%2520effectiveness%2520in%2520supervised%2520semantic%2520segmentation%2520tasks%252C%2520but%2520they%2520are%250Aoften%2520ignored%2520in%2520SSSS.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520spatial%2520augmentation%250Acan%2520also%2520contribute%2520to%2520model%2520training%2520in%2520SSSS%252C%2520despite%2520generating%2520inconsistent%250Amasks%2520between%2520the%2520weak%2520and%2520strong%2520augmentations.%2520Furthermore%252C%2520recognizing%2520the%250Avariability%2520among%2520images%252C%2520we%2520propose%2520an%2520adaptive%2520augmentation%2520strategy%2520that%250Adynamically%2520adjusts%2520the%2520augmentation%2520for%2520each%2520instance%2520based%2520on%2520entropy.%250AExtensive%2520experiments%2520show%2520that%2520our%2520proposed%2520Adaptive%2520Spatial%2520Augmentation%250A%2528%255Ctextbf%257BASAug%257D%2529%2520can%2520be%2520integrated%2520as%2520a%2520pluggable%2520module%252C%2520consistently%250Aimproving%2520the%2520performance%2520of%2520existing%2520methods%2520and%2520achieving%2520state-of-the-art%250Aresults%2520on%2520benchmark%2520datasets%2520such%2520as%2520PASCAL%2520VOC%25202012%252C%2520Cityscapes%252C%2520and%2520COCO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Spatial%20Augmentation%20for%20Semi-supervised%20Semantic%20Segmentation&entry.906535625=Lingyan%20Ran%20and%20Yali%20Li%20and%20Tao%20Zhuo%20and%20Shizhou%20Zhang%20and%20Yanning%20Zhang&entry.1292438233=%20%20In%20semi-supervised%20semantic%20segmentation%20%28SSSS%29%2C%20data%20augmentation%20plays%20a%0Acrucial%20role%20in%20the%20weak-to-strong%20consistency%20regularization%20framework%2C%20as%20it%0Aenhances%20diversity%20and%20improves%20model%20generalization.%20Recent%20strong%0Aaugmentation%20methods%20have%20primarily%20focused%20on%20intensity-based%20perturbations%2C%0Awhich%20have%20minimal%20impact%20on%20the%20semantic%20masks.%20In%20contrast%2C%20spatial%0Aaugmentations%20like%20translation%20and%20rotation%20have%20long%20been%20acknowledged%20for%0Atheir%20effectiveness%20in%20supervised%20semantic%20segmentation%20tasks%2C%20but%20they%20are%0Aoften%20ignored%20in%20SSSS.%20In%20this%20work%2C%20we%20demonstrate%20that%20spatial%20augmentation%0Acan%20also%20contribute%20to%20model%20training%20in%20SSSS%2C%20despite%20generating%20inconsistent%0Amasks%20between%20the%20weak%20and%20strong%20augmentations.%20Furthermore%2C%20recognizing%20the%0Avariability%20among%20images%2C%20we%20propose%20an%20adaptive%20augmentation%20strategy%20that%0Adynamically%20adjusts%20the%20augmentation%20for%20each%20instance%20based%20on%20entropy.%0AExtensive%20experiments%20show%20that%20our%20proposed%20Adaptive%20Spatial%20Augmentation%0A%28%5Ctextbf%7BASAug%7D%29%20can%20be%20integrated%20as%20a%20pluggable%20module%2C%20consistently%0Aimproving%20the%20performance%20of%20existing%20methods%20and%20achieving%20state-of-the-art%0Aresults%20on%20benchmark%20datasets%20such%20as%20PASCAL%20VOC%202012%2C%20Cityscapes%2C%20and%20COCO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23438v1&entry.124074799=Read"},
{"title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to\n  Re-attention Visual Information", "author": "Xu Chu and Xinrong Chen and Guanyu Wang and Zhijie Tan and Kui Huang and Wenyu Lv and Tong Mo and Weiping Li", "abstract": "  Inference time scaling drives extended reasoning to enhance the performance\nof Vision-Language Models (VLMs), thus forming powerful Vision-Language\nReasoning Models (VLRMs). However, long reasoning dilutes visual tokens,\ncausing visual information to receive less attention and may trigger\nhallucinations. Although introducing text-only reflection processes shows\npromise in language models, we demonstrate that it is insufficient to suppress\nhallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain\n(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a\nvision-text reflection process that guides the model to re-attention visual\ninformation during reasoning. We first propose a reinforcement learning method\nBalanced Reflective Policy Optimization (BRPO), which guides the model to\ndecide when to generate vision-text reflection on its own and balance the\nnumber and length of reflections. Then, we formally prove that VLRMs lose\nattention to visual tokens as reasoning progresses, and demonstrate that\nsupplementing visual information during reflection enhances visual attention.\nTherefore, during training and inference, Visual Token COPY and Visual Token\nROUTE are introduced to force the model to re-attention visual information at\nthe visual level, addressing the limitations of text-only reflection.\nExperiments on multiple visual QA datasets and hallucination metrics indicate\nthat Qwen-LA achieves leading accuracy performance while reducing\nhallucinations. Our code is available at:\nhttps://github.com/Liar406/Look_Again.\n", "link": "http://arxiv.org/abs/2505.23558v1", "date": "2025-05-29", "relevancy": 2.7264, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qwen%20Look%20Again%3A%20Guiding%20Vision-Language%20Reasoning%20Models%20to%0A%20%20Re-attention%20Visual%20Information&body=Title%3A%20Qwen%20Look%20Again%3A%20Guiding%20Vision-Language%20Reasoning%20Models%20to%0A%20%20Re-attention%20Visual%20Information%0AAuthor%3A%20Xu%20Chu%20and%20Xinrong%20Chen%20and%20Guanyu%20Wang%20and%20Zhijie%20Tan%20and%20Kui%20Huang%20and%20Wenyu%20Lv%20and%20Tong%20Mo%20and%20Weiping%20Li%0AAbstract%3A%20%20%20Inference%20time%20scaling%20drives%20extended%20reasoning%20to%20enhance%20the%20performance%0Aof%20Vision-Language%20Models%20%28VLMs%29%2C%20thus%20forming%20powerful%20Vision-Language%0AReasoning%20Models%20%28VLRMs%29.%20However%2C%20long%20reasoning%20dilutes%20visual%20tokens%2C%0Acausing%20visual%20information%20to%20receive%20less%20attention%20and%20may%20trigger%0Ahallucinations.%20Although%20introducing%20text-only%20reflection%20processes%20shows%0Apromise%20in%20language%20models%2C%20we%20demonstrate%20that%20it%20is%20insufficient%20to%20suppress%0Ahallucinations%20in%20VLMs.%20To%20address%20this%20issue%2C%20we%20introduce%20Qwen-LookAgain%0A%28Qwen-LA%29%2C%20a%20novel%20VLRM%20designed%20to%20mitigate%20hallucinations%20by%20incorporating%20a%0Avision-text%20reflection%20process%20that%20guides%20the%20model%20to%20re-attention%20visual%0Ainformation%20during%20reasoning.%20We%20first%20propose%20a%20reinforcement%20learning%20method%0ABalanced%20Reflective%20Policy%20Optimization%20%28BRPO%29%2C%20which%20guides%20the%20model%20to%0Adecide%20when%20to%20generate%20vision-text%20reflection%20on%20its%20own%20and%20balance%20the%0Anumber%20and%20length%20of%20reflections.%20Then%2C%20we%20formally%20prove%20that%20VLRMs%20lose%0Aattention%20to%20visual%20tokens%20as%20reasoning%20progresses%2C%20and%20demonstrate%20that%0Asupplementing%20visual%20information%20during%20reflection%20enhances%20visual%20attention.%0ATherefore%2C%20during%20training%20and%20inference%2C%20Visual%20Token%20COPY%20and%20Visual%20Token%0AROUTE%20are%20introduced%20to%20force%20the%20model%20to%20re-attention%20visual%20information%20at%0Athe%20visual%20level%2C%20addressing%20the%20limitations%20of%20text-only%20reflection.%0AExperiments%20on%20multiple%20visual%20QA%20datasets%20and%20hallucination%20metrics%20indicate%0Athat%20Qwen-LA%20achieves%20leading%20accuracy%20performance%20while%20reducing%0Ahallucinations.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Liar406/Look_Again.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQwen%2520Look%2520Again%253A%2520Guiding%2520Vision-Language%2520Reasoning%2520Models%2520to%250A%2520%2520Re-attention%2520Visual%2520Information%26entry.906535625%3DXu%2520Chu%2520and%2520Xinrong%2520Chen%2520and%2520Guanyu%2520Wang%2520and%2520Zhijie%2520Tan%2520and%2520Kui%2520Huang%2520and%2520Wenyu%2520Lv%2520and%2520Tong%2520Mo%2520and%2520Weiping%2520Li%26entry.1292438233%3D%2520%2520Inference%2520time%2520scaling%2520drives%2520extended%2520reasoning%2520to%2520enhance%2520the%2520performance%250Aof%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520thus%2520forming%2520powerful%2520Vision-Language%250AReasoning%2520Models%2520%2528VLRMs%2529.%2520However%252C%2520long%2520reasoning%2520dilutes%2520visual%2520tokens%252C%250Acausing%2520visual%2520information%2520to%2520receive%2520less%2520attention%2520and%2520may%2520trigger%250Ahallucinations.%2520Although%2520introducing%2520text-only%2520reflection%2520processes%2520shows%250Apromise%2520in%2520language%2520models%252C%2520we%2520demonstrate%2520that%2520it%2520is%2520insufficient%2520to%2520suppress%250Ahallucinations%2520in%2520VLMs.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Qwen-LookAgain%250A%2528Qwen-LA%2529%252C%2520a%2520novel%2520VLRM%2520designed%2520to%2520mitigate%2520hallucinations%2520by%2520incorporating%2520a%250Avision-text%2520reflection%2520process%2520that%2520guides%2520the%2520model%2520to%2520re-attention%2520visual%250Ainformation%2520during%2520reasoning.%2520We%2520first%2520propose%2520a%2520reinforcement%2520learning%2520method%250ABalanced%2520Reflective%2520Policy%2520Optimization%2520%2528BRPO%2529%252C%2520which%2520guides%2520the%2520model%2520to%250Adecide%2520when%2520to%2520generate%2520vision-text%2520reflection%2520on%2520its%2520own%2520and%2520balance%2520the%250Anumber%2520and%2520length%2520of%2520reflections.%2520Then%252C%2520we%2520formally%2520prove%2520that%2520VLRMs%2520lose%250Aattention%2520to%2520visual%2520tokens%2520as%2520reasoning%2520progresses%252C%2520and%2520demonstrate%2520that%250Asupplementing%2520visual%2520information%2520during%2520reflection%2520enhances%2520visual%2520attention.%250ATherefore%252C%2520during%2520training%2520and%2520inference%252C%2520Visual%2520Token%2520COPY%2520and%2520Visual%2520Token%250AROUTE%2520are%2520introduced%2520to%2520force%2520the%2520model%2520to%2520re-attention%2520visual%2520information%2520at%250Athe%2520visual%2520level%252C%2520addressing%2520the%2520limitations%2520of%2520text-only%2520reflection.%250AExperiments%2520on%2520multiple%2520visual%2520QA%2520datasets%2520and%2520hallucination%2520metrics%2520indicate%250Athat%2520Qwen-LA%2520achieves%2520leading%2520accuracy%2520performance%2520while%2520reducing%250Ahallucinations.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Liar406/Look_Again.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qwen%20Look%20Again%3A%20Guiding%20Vision-Language%20Reasoning%20Models%20to%0A%20%20Re-attention%20Visual%20Information&entry.906535625=Xu%20Chu%20and%20Xinrong%20Chen%20and%20Guanyu%20Wang%20and%20Zhijie%20Tan%20and%20Kui%20Huang%20and%20Wenyu%20Lv%20and%20Tong%20Mo%20and%20Weiping%20Li&entry.1292438233=%20%20Inference%20time%20scaling%20drives%20extended%20reasoning%20to%20enhance%20the%20performance%0Aof%20Vision-Language%20Models%20%28VLMs%29%2C%20thus%20forming%20powerful%20Vision-Language%0AReasoning%20Models%20%28VLRMs%29.%20However%2C%20long%20reasoning%20dilutes%20visual%20tokens%2C%0Acausing%20visual%20information%20to%20receive%20less%20attention%20and%20may%20trigger%0Ahallucinations.%20Although%20introducing%20text-only%20reflection%20processes%20shows%0Apromise%20in%20language%20models%2C%20we%20demonstrate%20that%20it%20is%20insufficient%20to%20suppress%0Ahallucinations%20in%20VLMs.%20To%20address%20this%20issue%2C%20we%20introduce%20Qwen-LookAgain%0A%28Qwen-LA%29%2C%20a%20novel%20VLRM%20designed%20to%20mitigate%20hallucinations%20by%20incorporating%20a%0Avision-text%20reflection%20process%20that%20guides%20the%20model%20to%20re-attention%20visual%0Ainformation%20during%20reasoning.%20We%20first%20propose%20a%20reinforcement%20learning%20method%0ABalanced%20Reflective%20Policy%20Optimization%20%28BRPO%29%2C%20which%20guides%20the%20model%20to%0Adecide%20when%20to%20generate%20vision-text%20reflection%20on%20its%20own%20and%20balance%20the%0Anumber%20and%20length%20of%20reflections.%20Then%2C%20we%20formally%20prove%20that%20VLRMs%20lose%0Aattention%20to%20visual%20tokens%20as%20reasoning%20progresses%2C%20and%20demonstrate%20that%0Asupplementing%20visual%20information%20during%20reflection%20enhances%20visual%20attention.%0ATherefore%2C%20during%20training%20and%20inference%2C%20Visual%20Token%20COPY%20and%20Visual%20Token%0AROUTE%20are%20introduced%20to%20force%20the%20model%20to%20re-attention%20visual%20information%20at%0Athe%20visual%20level%2C%20addressing%20the%20limitations%20of%20text-only%20reflection.%0AExperiments%20on%20multiple%20visual%20QA%20datasets%20and%20hallucination%20metrics%20indicate%0Athat%20Qwen-LA%20achieves%20leading%20accuracy%20performance%20while%20reducing%0Ahallucinations.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Liar406/Look_Again.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23558v1&entry.124074799=Read"},
{"title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?", "author": "Hui Shen and Taiqiang Wu and Qi Han and Yunta Hsieh and Jizhou Wang and Yuyue Zhang and Yuxin Cheng and Zijian Hao and Yuansheng Ni and Xin Wang and Zhongwei Wan and Kai Zhang and Wendong Xu and Jing Xiong and Ping Luo and Wenhu Chen and Chaofan Tao and Zhuoqing Mao and Ngai Wong", "abstract": "  Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5%, 42.2%, and 45.8% accuracy\nrespectively-performance gaps exceeding 29% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation. More details are available\non our project page: https://phyx-bench.github.io/.\n", "link": "http://arxiv.org/abs/2505.15929v2", "date": "2025-05-29", "relevancy": 2.7096, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhyX%3A%20Does%20Your%20Model%20Have%20the%20%22Wits%22%20for%20Physical%20Reasoning%3F&body=Title%3A%20PhyX%3A%20Does%20Your%20Model%20Have%20the%20%22Wits%22%20for%20Physical%20Reasoning%3F%0AAuthor%3A%20Hui%20Shen%20and%20Taiqiang%20Wu%20and%20Qi%20Han%20and%20Yunta%20Hsieh%20and%20Jizhou%20Wang%20and%20Yuyue%20Zhang%20and%20Yuxin%20Cheng%20and%20Zijian%20Hao%20and%20Yuansheng%20Ni%20and%20Xin%20Wang%20and%20Zhongwei%20Wan%20and%20Kai%20Zhang%20and%20Wendong%20Xu%20and%20Jing%20Xiong%20and%20Ping%20Luo%20and%20Wenhu%20Chen%20and%20Chaofan%20Tao%20and%20Zhuoqing%20Mao%20and%20Ngai%20Wong%0AAbstract%3A%20%20%20Existing%20benchmarks%20fail%20to%20capture%20a%20crucial%20aspect%20of%20intelligence%3A%0Aphysical%20reasoning%2C%20the%20integrated%20ability%20to%20combine%20domain%20knowledge%2C%0Asymbolic%20reasoning%2C%20and%20understanding%20of%20real-world%20constraints.%20To%20address%0Athis%20gap%2C%20we%20introduce%20PhyX%3A%20the%20first%20large-scale%20benchmark%20designed%20to%20assess%0Amodels%20capacity%20for%20physics-grounded%20reasoning%20in%20visual%20scenarios.%20PhyX%0Aincludes%203K%20meticulously%20curated%20multimodal%20questions%20spanning%206%20reasoning%0Atypes%20across%2025%20sub-domains%20and%206%20core%20physics%20domains%3A%20thermodynamics%2C%0Aelectromagnetism%2C%20mechanics%2C%20modern%20physics%2C%20optics%2C%20and%20wave%5C%26acoustics.%20In%0Aour%20comprehensive%20evaluation%2C%20even%20state-of-the-art%20models%20struggle%0Asignificantly%20with%20physical%20reasoning.%20GPT-4o%2C%20Claude3.7-Sonnet%2C%20and%0AGPT-o4-mini%20achieve%20only%2032.5%25%2C%2042.2%25%2C%20and%2045.8%25%20accuracy%0Arespectively-performance%20gaps%20exceeding%2029%25%20compared%20to%20human%20experts.%20Our%0Aanalysis%20exposes%20critical%20limitations%20in%20current%20models%3A%20over-reliance%20on%0Amemorized%20disciplinary%20knowledge%2C%20excessive%20dependence%20on%20mathematical%0Aformulations%2C%20and%20surface-level%20visual%20pattern%20matching%20rather%20than%20genuine%0Aphysical%20understanding.%20We%20provide%20in-depth%20analysis%20through%20fine-grained%0Astatistics%2C%20detailed%20case%20studies%2C%20and%20multiple%20evaluation%20paradigms%20to%0Athoroughly%20examine%20physical%20reasoning%20capabilities.%20To%20ensure%20reproducibility%2C%0Awe%20implement%20a%20compatible%20evaluation%20protocol%20based%20on%20widely-used%20toolkits%0Asuch%20as%20VLMEvalKit%2C%20enabling%20one-click%20evaluation.%20More%20details%20are%20available%0Aon%20our%20project%20page%3A%20https%3A//phyx-bench.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhyX%253A%2520Does%2520Your%2520Model%2520Have%2520the%2520%2522Wits%2522%2520for%2520Physical%2520Reasoning%253F%26entry.906535625%3DHui%2520Shen%2520and%2520Taiqiang%2520Wu%2520and%2520Qi%2520Han%2520and%2520Yunta%2520Hsieh%2520and%2520Jizhou%2520Wang%2520and%2520Yuyue%2520Zhang%2520and%2520Yuxin%2520Cheng%2520and%2520Zijian%2520Hao%2520and%2520Yuansheng%2520Ni%2520and%2520Xin%2520Wang%2520and%2520Zhongwei%2520Wan%2520and%2520Kai%2520Zhang%2520and%2520Wendong%2520Xu%2520and%2520Jing%2520Xiong%2520and%2520Ping%2520Luo%2520and%2520Wenhu%2520Chen%2520and%2520Chaofan%2520Tao%2520and%2520Zhuoqing%2520Mao%2520and%2520Ngai%2520Wong%26entry.1292438233%3D%2520%2520Existing%2520benchmarks%2520fail%2520to%2520capture%2520a%2520crucial%2520aspect%2520of%2520intelligence%253A%250Aphysical%2520reasoning%252C%2520the%2520integrated%2520ability%2520to%2520combine%2520domain%2520knowledge%252C%250Asymbolic%2520reasoning%252C%2520and%2520understanding%2520of%2520real-world%2520constraints.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520PhyX%253A%2520the%2520first%2520large-scale%2520benchmark%2520designed%2520to%2520assess%250Amodels%2520capacity%2520for%2520physics-grounded%2520reasoning%2520in%2520visual%2520scenarios.%2520PhyX%250Aincludes%25203K%2520meticulously%2520curated%2520multimodal%2520questions%2520spanning%25206%2520reasoning%250Atypes%2520across%252025%2520sub-domains%2520and%25206%2520core%2520physics%2520domains%253A%2520thermodynamics%252C%250Aelectromagnetism%252C%2520mechanics%252C%2520modern%2520physics%252C%2520optics%252C%2520and%2520wave%255C%2526acoustics.%2520In%250Aour%2520comprehensive%2520evaluation%252C%2520even%2520state-of-the-art%2520models%2520struggle%250Asignificantly%2520with%2520physical%2520reasoning.%2520GPT-4o%252C%2520Claude3.7-Sonnet%252C%2520and%250AGPT-o4-mini%2520achieve%2520only%252032.5%2525%252C%252042.2%2525%252C%2520and%252045.8%2525%2520accuracy%250Arespectively-performance%2520gaps%2520exceeding%252029%2525%2520compared%2520to%2520human%2520experts.%2520Our%250Aanalysis%2520exposes%2520critical%2520limitations%2520in%2520current%2520models%253A%2520over-reliance%2520on%250Amemorized%2520disciplinary%2520knowledge%252C%2520excessive%2520dependence%2520on%2520mathematical%250Aformulations%252C%2520and%2520surface-level%2520visual%2520pattern%2520matching%2520rather%2520than%2520genuine%250Aphysical%2520understanding.%2520We%2520provide%2520in-depth%2520analysis%2520through%2520fine-grained%250Astatistics%252C%2520detailed%2520case%2520studies%252C%2520and%2520multiple%2520evaluation%2520paradigms%2520to%250Athoroughly%2520examine%2520physical%2520reasoning%2520capabilities.%2520To%2520ensure%2520reproducibility%252C%250Awe%2520implement%2520a%2520compatible%2520evaluation%2520protocol%2520based%2520on%2520widely-used%2520toolkits%250Asuch%2520as%2520VLMEvalKit%252C%2520enabling%2520one-click%2520evaluation.%2520More%2520details%2520are%2520available%250Aon%2520our%2520project%2520page%253A%2520https%253A//phyx-bench.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhyX%3A%20Does%20Your%20Model%20Have%20the%20%22Wits%22%20for%20Physical%20Reasoning%3F&entry.906535625=Hui%20Shen%20and%20Taiqiang%20Wu%20and%20Qi%20Han%20and%20Yunta%20Hsieh%20and%20Jizhou%20Wang%20and%20Yuyue%20Zhang%20and%20Yuxin%20Cheng%20and%20Zijian%20Hao%20and%20Yuansheng%20Ni%20and%20Xin%20Wang%20and%20Zhongwei%20Wan%20and%20Kai%20Zhang%20and%20Wendong%20Xu%20and%20Jing%20Xiong%20and%20Ping%20Luo%20and%20Wenhu%20Chen%20and%20Chaofan%20Tao%20and%20Zhuoqing%20Mao%20and%20Ngai%20Wong&entry.1292438233=%20%20Existing%20benchmarks%20fail%20to%20capture%20a%20crucial%20aspect%20of%20intelligence%3A%0Aphysical%20reasoning%2C%20the%20integrated%20ability%20to%20combine%20domain%20knowledge%2C%0Asymbolic%20reasoning%2C%20and%20understanding%20of%20real-world%20constraints.%20To%20address%0Athis%20gap%2C%20we%20introduce%20PhyX%3A%20the%20first%20large-scale%20benchmark%20designed%20to%20assess%0Amodels%20capacity%20for%20physics-grounded%20reasoning%20in%20visual%20scenarios.%20PhyX%0Aincludes%203K%20meticulously%20curated%20multimodal%20questions%20spanning%206%20reasoning%0Atypes%20across%2025%20sub-domains%20and%206%20core%20physics%20domains%3A%20thermodynamics%2C%0Aelectromagnetism%2C%20mechanics%2C%20modern%20physics%2C%20optics%2C%20and%20wave%5C%26acoustics.%20In%0Aour%20comprehensive%20evaluation%2C%20even%20state-of-the-art%20models%20struggle%0Asignificantly%20with%20physical%20reasoning.%20GPT-4o%2C%20Claude3.7-Sonnet%2C%20and%0AGPT-o4-mini%20achieve%20only%2032.5%25%2C%2042.2%25%2C%20and%2045.8%25%20accuracy%0Arespectively-performance%20gaps%20exceeding%2029%25%20compared%20to%20human%20experts.%20Our%0Aanalysis%20exposes%20critical%20limitations%20in%20current%20models%3A%20over-reliance%20on%0Amemorized%20disciplinary%20knowledge%2C%20excessive%20dependence%20on%20mathematical%0Aformulations%2C%20and%20surface-level%20visual%20pattern%20matching%20rather%20than%20genuine%0Aphysical%20understanding.%20We%20provide%20in-depth%20analysis%20through%20fine-grained%0Astatistics%2C%20detailed%20case%20studies%2C%20and%20multiple%20evaluation%20paradigms%20to%0Athoroughly%20examine%20physical%20reasoning%20capabilities.%20To%20ensure%20reproducibility%2C%0Awe%20implement%20a%20compatible%20evaluation%20protocol%20based%20on%20widely-used%20toolkits%0Asuch%20as%20VLMEvalKit%2C%20enabling%20one-click%20evaluation.%20More%20details%20are%20available%0Aon%20our%20project%20page%3A%20https%3A//phyx-bench.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15929v2&entry.124074799=Read"},
{"title": "DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers", "author": "Li Ren and Chen Chen and Liqiang Wang and Kien Hua", "abstract": "  Visual Prompt Tuning (VPT) has become a promising solution for\nParameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT)\nmodels by partially fine-tuning learnable tokens while keeping most model\nparameters frozen. Recent research has explored modifying the connection\nstructures of the prompts. However, the fundamental correlation and\ndistribution between the prompts and image tokens remain unexplored. In this\npaper, we leverage metric learning techniques to investigate how the\ndistribution of prompts affects fine-tuning performance. Specifically, we\npropose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to\nguide the distributions of the prompts by learning the distance metric from\ntheir class-related semantic data. Our method demonstrates that the prompts can\nserve as an effective bridge to share semantic information between image\npatches and the class token. We extensively evaluated our approach on popular\nbenchmarks in both recognition and segmentation tasks. The results demonstrate\nthat our approach enables more effective and efficient fine-tuning of ViT\nmodels by leveraging semantic information to guide the learning of the prompts,\nleading to improved performance on various downstream vision tasks.\n", "link": "http://arxiv.org/abs/2505.23694v1", "date": "2025-05-29", "relevancy": 2.7083, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DA-VPT%3A%20Semantic-Guided%20Visual%20Prompt%20Tuning%20for%20Vision%20Transformers&body=Title%3A%20DA-VPT%3A%20Semantic-Guided%20Visual%20Prompt%20Tuning%20for%20Vision%20Transformers%0AAuthor%3A%20Li%20Ren%20and%20Chen%20Chen%20and%20Liqiang%20Wang%20and%20Kien%20Hua%0AAbstract%3A%20%20%20Visual%20Prompt%20Tuning%20%28VPT%29%20has%20become%20a%20promising%20solution%20for%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20approach%20for%20Vision%20Transformer%20%28ViT%29%0Amodels%20by%20partially%20fine-tuning%20learnable%20tokens%20while%20keeping%20most%20model%0Aparameters%20frozen.%20Recent%20research%20has%20explored%20modifying%20the%20connection%0Astructures%20of%20the%20prompts.%20However%2C%20the%20fundamental%20correlation%20and%0Adistribution%20between%20the%20prompts%20and%20image%20tokens%20remain%20unexplored.%20In%20this%0Apaper%2C%20we%20leverage%20metric%20learning%20techniques%20to%20investigate%20how%20the%0Adistribution%20of%20prompts%20affects%20fine-tuning%20performance.%20Specifically%2C%20we%0Apropose%20a%20novel%20framework%2C%20Distribution%20Aware%20Visual%20Prompt%20Tuning%20%28DA-VPT%29%2C%20to%0Aguide%20the%20distributions%20of%20the%20prompts%20by%20learning%20the%20distance%20metric%20from%0Atheir%20class-related%20semantic%20data.%20Our%20method%20demonstrates%20that%20the%20prompts%20can%0Aserve%20as%20an%20effective%20bridge%20to%20share%20semantic%20information%20between%20image%0Apatches%20and%20the%20class%20token.%20We%20extensively%20evaluated%20our%20approach%20on%20popular%0Abenchmarks%20in%20both%20recognition%20and%20segmentation%20tasks.%20The%20results%20demonstrate%0Athat%20our%20approach%20enables%20more%20effective%20and%20efficient%20fine-tuning%20of%20ViT%0Amodels%20by%20leveraging%20semantic%20information%20to%20guide%20the%20learning%20of%20the%20prompts%2C%0Aleading%20to%20improved%20performance%20on%20various%20downstream%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDA-VPT%253A%2520Semantic-Guided%2520Visual%2520Prompt%2520Tuning%2520for%2520Vision%2520Transformers%26entry.906535625%3DLi%2520Ren%2520and%2520Chen%2520Chen%2520and%2520Liqiang%2520Wang%2520and%2520Kien%2520Hua%26entry.1292438233%3D%2520%2520Visual%2520Prompt%2520Tuning%2520%2528VPT%2529%2520has%2520become%2520a%2520promising%2520solution%2520for%250AParameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520approach%2520for%2520Vision%2520Transformer%2520%2528ViT%2529%250Amodels%2520by%2520partially%2520fine-tuning%2520learnable%2520tokens%2520while%2520keeping%2520most%2520model%250Aparameters%2520frozen.%2520Recent%2520research%2520has%2520explored%2520modifying%2520the%2520connection%250Astructures%2520of%2520the%2520prompts.%2520However%252C%2520the%2520fundamental%2520correlation%2520and%250Adistribution%2520between%2520the%2520prompts%2520and%2520image%2520tokens%2520remain%2520unexplored.%2520In%2520this%250Apaper%252C%2520we%2520leverage%2520metric%2520learning%2520techniques%2520to%2520investigate%2520how%2520the%250Adistribution%2520of%2520prompts%2520affects%2520fine-tuning%2520performance.%2520Specifically%252C%2520we%250Apropose%2520a%2520novel%2520framework%252C%2520Distribution%2520Aware%2520Visual%2520Prompt%2520Tuning%2520%2528DA-VPT%2529%252C%2520to%250Aguide%2520the%2520distributions%2520of%2520the%2520prompts%2520by%2520learning%2520the%2520distance%2520metric%2520from%250Atheir%2520class-related%2520semantic%2520data.%2520Our%2520method%2520demonstrates%2520that%2520the%2520prompts%2520can%250Aserve%2520as%2520an%2520effective%2520bridge%2520to%2520share%2520semantic%2520information%2520between%2520image%250Apatches%2520and%2520the%2520class%2520token.%2520We%2520extensively%2520evaluated%2520our%2520approach%2520on%2520popular%250Abenchmarks%2520in%2520both%2520recognition%2520and%2520segmentation%2520tasks.%2520The%2520results%2520demonstrate%250Athat%2520our%2520approach%2520enables%2520more%2520effective%2520and%2520efficient%2520fine-tuning%2520of%2520ViT%250Amodels%2520by%2520leveraging%2520semantic%2520information%2520to%2520guide%2520the%2520learning%2520of%2520the%2520prompts%252C%250Aleading%2520to%2520improved%2520performance%2520on%2520various%2520downstream%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DA-VPT%3A%20Semantic-Guided%20Visual%20Prompt%20Tuning%20for%20Vision%20Transformers&entry.906535625=Li%20Ren%20and%20Chen%20Chen%20and%20Liqiang%20Wang%20and%20Kien%20Hua&entry.1292438233=%20%20Visual%20Prompt%20Tuning%20%28VPT%29%20has%20become%20a%20promising%20solution%20for%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20approach%20for%20Vision%20Transformer%20%28ViT%29%0Amodels%20by%20partially%20fine-tuning%20learnable%20tokens%20while%20keeping%20most%20model%0Aparameters%20frozen.%20Recent%20research%20has%20explored%20modifying%20the%20connection%0Astructures%20of%20the%20prompts.%20However%2C%20the%20fundamental%20correlation%20and%0Adistribution%20between%20the%20prompts%20and%20image%20tokens%20remain%20unexplored.%20In%20this%0Apaper%2C%20we%20leverage%20metric%20learning%20techniques%20to%20investigate%20how%20the%0Adistribution%20of%20prompts%20affects%20fine-tuning%20performance.%20Specifically%2C%20we%0Apropose%20a%20novel%20framework%2C%20Distribution%20Aware%20Visual%20Prompt%20Tuning%20%28DA-VPT%29%2C%20to%0Aguide%20the%20distributions%20of%20the%20prompts%20by%20learning%20the%20distance%20metric%20from%0Atheir%20class-related%20semantic%20data.%20Our%20method%20demonstrates%20that%20the%20prompts%20can%0Aserve%20as%20an%20effective%20bridge%20to%20share%20semantic%20information%20between%20image%0Apatches%20and%20the%20class%20token.%20We%20extensively%20evaluated%20our%20approach%20on%20popular%0Abenchmarks%20in%20both%20recognition%20and%20segmentation%20tasks.%20The%20results%20demonstrate%0Athat%20our%20approach%20enables%20more%20effective%20and%20efficient%20fine-tuning%20of%20ViT%0Amodels%20by%20leveraging%20semantic%20information%20to%20guide%20the%20learning%20of%20the%20prompts%2C%0Aleading%20to%20improved%20performance%20on%20various%20downstream%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23694v1&entry.124074799=Read"},
{"title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos", "author": "Tingyu Song and Tongyan Hu and Guo Gan and Yilun Zhao", "abstract": "  MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.\n", "link": "http://arxiv.org/abs/2505.23693v1", "date": "2025-05-29", "relevancy": 2.6923, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5565}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VF-Eval%3A%20Evaluating%20Multimodal%20LLMs%20for%20Generating%20Feedback%20on%20AIGC%0A%20%20Videos&body=Title%3A%20VF-Eval%3A%20Evaluating%20Multimodal%20LLMs%20for%20Generating%20Feedback%20on%20AIGC%0A%20%20Videos%0AAuthor%3A%20Tingyu%20Song%20and%20Tongyan%20Hu%20and%20Guo%20Gan%20and%20Yilun%20Zhao%0AAbstract%3A%20%20%20MLLMs%20have%20been%20widely%20studied%20for%20video%20question%20answering%20recently.%0AHowever%2C%20most%20existing%20assessments%20focus%20on%20natural%20videos%2C%20overlooking%0Asynthetic%20videos%2C%20such%20as%20AI-generated%20content%20%28AIGC%29.%20Meanwhile%2C%20some%20works%20in%0Avideo%20generation%20rely%20on%20MLLMs%20to%20evaluate%20the%20quality%20of%20generated%20videos%2C%20but%0Athe%20capabilities%20of%20MLLMs%20on%20interpreting%20AIGC%20videos%20remain%20largely%0Aunderexplored.%20To%20address%20this%2C%20we%20propose%20a%20new%20benchmark%2C%20VF-Eval%2C%20which%0Aintroduces%20four%20tasks-coherence%20validation%2C%20error%20awareness%2C%20error%20type%0Adetection%2C%20and%20reasoning%20evaluation-to%20comprehensively%20evaluate%20the%20abilities%0Aof%20MLLMs%20on%20AIGC%20videos.%20We%20evaluate%2013%20frontier%20MLLMs%20on%20VF-Eval%20and%20find%20that%0Aeven%20the%20best-performing%20model%2C%20GPT-4.1%2C%20struggles%20to%20achieve%20consistently%20good%0Aperformance%20across%20all%20tasks.%20This%20highlights%20the%20challenging%20nature%20of%20our%0Abenchmark.%20Additionally%2C%20to%20investigate%20the%20practical%20applications%20of%20VF-Eval%0Ain%20improving%20video%20generation%2C%20we%20conduct%20an%20experiment%2C%20RePrompt%2C%0Ademonstrating%20that%20aligning%20MLLMs%20more%20closely%20with%20human%20feedback%20can%20benefit%0Avideo%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVF-Eval%253A%2520Evaluating%2520Multimodal%2520LLMs%2520for%2520Generating%2520Feedback%2520on%2520AIGC%250A%2520%2520Videos%26entry.906535625%3DTingyu%2520Song%2520and%2520Tongyan%2520Hu%2520and%2520Guo%2520Gan%2520and%2520Yilun%2520Zhao%26entry.1292438233%3D%2520%2520MLLMs%2520have%2520been%2520widely%2520studied%2520for%2520video%2520question%2520answering%2520recently.%250AHowever%252C%2520most%2520existing%2520assessments%2520focus%2520on%2520natural%2520videos%252C%2520overlooking%250Asynthetic%2520videos%252C%2520such%2520as%2520AI-generated%2520content%2520%2528AIGC%2529.%2520Meanwhile%252C%2520some%2520works%2520in%250Avideo%2520generation%2520rely%2520on%2520MLLMs%2520to%2520evaluate%2520the%2520quality%2520of%2520generated%2520videos%252C%2520but%250Athe%2520capabilities%2520of%2520MLLMs%2520on%2520interpreting%2520AIGC%2520videos%2520remain%2520largely%250Aunderexplored.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520new%2520benchmark%252C%2520VF-Eval%252C%2520which%250Aintroduces%2520four%2520tasks-coherence%2520validation%252C%2520error%2520awareness%252C%2520error%2520type%250Adetection%252C%2520and%2520reasoning%2520evaluation-to%2520comprehensively%2520evaluate%2520the%2520abilities%250Aof%2520MLLMs%2520on%2520AIGC%2520videos.%2520We%2520evaluate%252013%2520frontier%2520MLLMs%2520on%2520VF-Eval%2520and%2520find%2520that%250Aeven%2520the%2520best-performing%2520model%252C%2520GPT-4.1%252C%2520struggles%2520to%2520achieve%2520consistently%2520good%250Aperformance%2520across%2520all%2520tasks.%2520This%2520highlights%2520the%2520challenging%2520nature%2520of%2520our%250Abenchmark.%2520Additionally%252C%2520to%2520investigate%2520the%2520practical%2520applications%2520of%2520VF-Eval%250Ain%2520improving%2520video%2520generation%252C%2520we%2520conduct%2520an%2520experiment%252C%2520RePrompt%252C%250Ademonstrating%2520that%2520aligning%2520MLLMs%2520more%2520closely%2520with%2520human%2520feedback%2520can%2520benefit%250Avideo%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VF-Eval%3A%20Evaluating%20Multimodal%20LLMs%20for%20Generating%20Feedback%20on%20AIGC%0A%20%20Videos&entry.906535625=Tingyu%20Song%20and%20Tongyan%20Hu%20and%20Guo%20Gan%20and%20Yilun%20Zhao&entry.1292438233=%20%20MLLMs%20have%20been%20widely%20studied%20for%20video%20question%20answering%20recently.%0AHowever%2C%20most%20existing%20assessments%20focus%20on%20natural%20videos%2C%20overlooking%0Asynthetic%20videos%2C%20such%20as%20AI-generated%20content%20%28AIGC%29.%20Meanwhile%2C%20some%20works%20in%0Avideo%20generation%20rely%20on%20MLLMs%20to%20evaluate%20the%20quality%20of%20generated%20videos%2C%20but%0Athe%20capabilities%20of%20MLLMs%20on%20interpreting%20AIGC%20videos%20remain%20largely%0Aunderexplored.%20To%20address%20this%2C%20we%20propose%20a%20new%20benchmark%2C%20VF-Eval%2C%20which%0Aintroduces%20four%20tasks-coherence%20validation%2C%20error%20awareness%2C%20error%20type%0Adetection%2C%20and%20reasoning%20evaluation-to%20comprehensively%20evaluate%20the%20abilities%0Aof%20MLLMs%20on%20AIGC%20videos.%20We%20evaluate%2013%20frontier%20MLLMs%20on%20VF-Eval%20and%20find%20that%0Aeven%20the%20best-performing%20model%2C%20GPT-4.1%2C%20struggles%20to%20achieve%20consistently%20good%0Aperformance%20across%20all%20tasks.%20This%20highlights%20the%20challenging%20nature%20of%20our%0Abenchmark.%20Additionally%2C%20to%20investigate%20the%20practical%20applications%20of%20VF-Eval%0Ain%20improving%20video%20generation%2C%20we%20conduct%20an%20experiment%2C%20RePrompt%2C%0Ademonstrating%20that%20aligning%20MLLMs%20more%20closely%20with%20human%20feedback%20can%20benefit%0Avideo%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23693v1&entry.124074799=Read"},
{"title": "ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025", "author": "Tianming Liang and Haichao Jiang and Wei-Shi Zheng and Jian-Fang Hu", "abstract": "  Referring Video Object Segmentation (RVOS) aims to segment target objects\nthroughout a video based on a text description. This task has attracted\nincreasing attention in the field of computer vision due to its promising\napplications in video editing and human-agent interaction. Recently, ReferDINO\nhas demonstrated promising performance in this task by adapting object-level\nvision-language knowledge from pretrained foundational image models. In this\nreport, we further enhance its capabilities by incorporating the advantages of\nSAM2 in mask quality and object consistency. In addition, to effectively\nbalance performance between single-object and multi-object scenarios, we\nintroduce a conditional mask fusion strategy that adaptively fuses the masks\nfrom ReferDINO and SAM2. Our solution, termed ReferDINO-Plus, achieves 60.43\n\\(\\mathcal{J}\\&\\mathcal{F}\\) on MeViS test set, securing 2nd place in the MeViS\nPVUW challenge at CVPR 2025. The code is available at:\nhttps://github.com/iSEE-Laboratory/ReferDINO-Plus.\n", "link": "http://arxiv.org/abs/2503.23509v2", "date": "2025-05-29", "relevancy": 2.6833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReferDINO-Plus%3A%202nd%20Solution%20for%204th%20PVUW%20MeViS%20Challenge%20at%20CVPR%202025&body=Title%3A%20ReferDINO-Plus%3A%202nd%20Solution%20for%204th%20PVUW%20MeViS%20Challenge%20at%20CVPR%202025%0AAuthor%3A%20Tianming%20Liang%20and%20Haichao%20Jiang%20and%20Wei-Shi%20Zheng%20and%20Jian-Fang%20Hu%0AAbstract%3A%20%20%20Referring%20Video%20Object%20Segmentation%20%28RVOS%29%20aims%20to%20segment%20target%20objects%0Athroughout%20a%20video%20based%20on%20a%20text%20description.%20This%20task%20has%20attracted%0Aincreasing%20attention%20in%20the%20field%20of%20computer%20vision%20due%20to%20its%20promising%0Aapplications%20in%20video%20editing%20and%20human-agent%20interaction.%20Recently%2C%20ReferDINO%0Ahas%20demonstrated%20promising%20performance%20in%20this%20task%20by%20adapting%20object-level%0Avision-language%20knowledge%20from%20pretrained%20foundational%20image%20models.%20In%20this%0Areport%2C%20we%20further%20enhance%20its%20capabilities%20by%20incorporating%20the%20advantages%20of%0ASAM2%20in%20mask%20quality%20and%20object%20consistency.%20In%20addition%2C%20to%20effectively%0Abalance%20performance%20between%20single-object%20and%20multi-object%20scenarios%2C%20we%0Aintroduce%20a%20conditional%20mask%20fusion%20strategy%20that%20adaptively%20fuses%20the%20masks%0Afrom%20ReferDINO%20and%20SAM2.%20Our%20solution%2C%20termed%20ReferDINO-Plus%2C%20achieves%2060.43%0A%5C%28%5Cmathcal%7BJ%7D%5C%26%5Cmathcal%7BF%7D%5C%29%20on%20MeViS%20test%20set%2C%20securing%202nd%20place%20in%20the%20MeViS%0APVUW%20challenge%20at%20CVPR%202025.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/iSEE-Laboratory/ReferDINO-Plus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReferDINO-Plus%253A%25202nd%2520Solution%2520for%25204th%2520PVUW%2520MeViS%2520Challenge%2520at%2520CVPR%25202025%26entry.906535625%3DTianming%2520Liang%2520and%2520Haichao%2520Jiang%2520and%2520Wei-Shi%2520Zheng%2520and%2520Jian-Fang%2520Hu%26entry.1292438233%3D%2520%2520Referring%2520Video%2520Object%2520Segmentation%2520%2528RVOS%2529%2520aims%2520to%2520segment%2520target%2520objects%250Athroughout%2520a%2520video%2520based%2520on%2520a%2520text%2520description.%2520This%2520task%2520has%2520attracted%250Aincreasing%2520attention%2520in%2520the%2520field%2520of%2520computer%2520vision%2520due%2520to%2520its%2520promising%250Aapplications%2520in%2520video%2520editing%2520and%2520human-agent%2520interaction.%2520Recently%252C%2520ReferDINO%250Ahas%2520demonstrated%2520promising%2520performance%2520in%2520this%2520task%2520by%2520adapting%2520object-level%250Avision-language%2520knowledge%2520from%2520pretrained%2520foundational%2520image%2520models.%2520In%2520this%250Areport%252C%2520we%2520further%2520enhance%2520its%2520capabilities%2520by%2520incorporating%2520the%2520advantages%2520of%250ASAM2%2520in%2520mask%2520quality%2520and%2520object%2520consistency.%2520In%2520addition%252C%2520to%2520effectively%250Abalance%2520performance%2520between%2520single-object%2520and%2520multi-object%2520scenarios%252C%2520we%250Aintroduce%2520a%2520conditional%2520mask%2520fusion%2520strategy%2520that%2520adaptively%2520fuses%2520the%2520masks%250Afrom%2520ReferDINO%2520and%2520SAM2.%2520Our%2520solution%252C%2520termed%2520ReferDINO-Plus%252C%2520achieves%252060.43%250A%255C%2528%255Cmathcal%257BJ%257D%255C%2526%255Cmathcal%257BF%257D%255C%2529%2520on%2520MeViS%2520test%2520set%252C%2520securing%25202nd%2520place%2520in%2520the%2520MeViS%250APVUW%2520challenge%2520at%2520CVPR%25202025.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/iSEE-Laboratory/ReferDINO-Plus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReferDINO-Plus%3A%202nd%20Solution%20for%204th%20PVUW%20MeViS%20Challenge%20at%20CVPR%202025&entry.906535625=Tianming%20Liang%20and%20Haichao%20Jiang%20and%20Wei-Shi%20Zheng%20and%20Jian-Fang%20Hu&entry.1292438233=%20%20Referring%20Video%20Object%20Segmentation%20%28RVOS%29%20aims%20to%20segment%20target%20objects%0Athroughout%20a%20video%20based%20on%20a%20text%20description.%20This%20task%20has%20attracted%0Aincreasing%20attention%20in%20the%20field%20of%20computer%20vision%20due%20to%20its%20promising%0Aapplications%20in%20video%20editing%20and%20human-agent%20interaction.%20Recently%2C%20ReferDINO%0Ahas%20demonstrated%20promising%20performance%20in%20this%20task%20by%20adapting%20object-level%0Avision-language%20knowledge%20from%20pretrained%20foundational%20image%20models.%20In%20this%0Areport%2C%20we%20further%20enhance%20its%20capabilities%20by%20incorporating%20the%20advantages%20of%0ASAM2%20in%20mask%20quality%20and%20object%20consistency.%20In%20addition%2C%20to%20effectively%0Abalance%20performance%20between%20single-object%20and%20multi-object%20scenarios%2C%20we%0Aintroduce%20a%20conditional%20mask%20fusion%20strategy%20that%20adaptively%20fuses%20the%20masks%0Afrom%20ReferDINO%20and%20SAM2.%20Our%20solution%2C%20termed%20ReferDINO-Plus%2C%20achieves%2060.43%0A%5C%28%5Cmathcal%7BJ%7D%5C%26%5Cmathcal%7BF%7D%5C%29%20on%20MeViS%20test%20set%2C%20securing%202nd%20place%20in%20the%20MeViS%0APVUW%20challenge%20at%20CVPR%202025.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/iSEE-Laboratory/ReferDINO-Plus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23509v2&entry.124074799=Read"},
{"title": "RingMo-Aerial: An Aerial Remote Sensing Foundation Model With Affine\n  Transformation Contrastive Learning", "author": "Wenhui Diao and Haichen Yu and Kaiyue Kang and Tong Ling and Di Liu and Yingchao Feng and Hanbo Bi and Libo Ren and Xuexue Li and Yongqiang Mao and Xian Sun", "abstract": "  Aerial Remote Sensing (ARS) vision tasks pose significant challenges due to\nthe unique characteristics of their viewing angles. Existing research has\nprimarily focused on algorithms for specific tasks, which have limited\napplicability in a broad range of ARS vision applications. This paper proposes\nthe RingMo-Aerial model, aiming to fill the gap in foundation model research in\nthe field of ARS vision. By introducing the Frequency-Enhanced Multi-Head\nSelf-Attention (FE-MSA) mechanism and an affine transformation-based\ncontrastive learning pre-training method, the model's detection capability for\nsmall targets is enhanced and optimized for the tilted viewing angles\ncharacteristic of ARS. Furthermore, the ARS-Adapter, an efficient parameter\nfine-tuning method, is proposed to improve the model's adaptability and\neffectiveness in various ARS vision tasks. Experimental results demonstrate\nthat RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This\nindicates the practicality and efficacy of RingMo-Aerial in enhancing the\nperformance of ARS vision tasks.\n", "link": "http://arxiv.org/abs/2409.13366v3", "date": "2025-05-29", "relevancy": 2.6772, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RingMo-Aerial%3A%20An%20Aerial%20Remote%20Sensing%20Foundation%20Model%20With%20Affine%0A%20%20Transformation%20Contrastive%20Learning&body=Title%3A%20RingMo-Aerial%3A%20An%20Aerial%20Remote%20Sensing%20Foundation%20Model%20With%20Affine%0A%20%20Transformation%20Contrastive%20Learning%0AAuthor%3A%20Wenhui%20Diao%20and%20Haichen%20Yu%20and%20Kaiyue%20Kang%20and%20Tong%20Ling%20and%20Di%20Liu%20and%20Yingchao%20Feng%20and%20Hanbo%20Bi%20and%20Libo%20Ren%20and%20Xuexue%20Li%20and%20Yongqiang%20Mao%20and%20Xian%20Sun%0AAbstract%3A%20%20%20Aerial%20Remote%20Sensing%20%28ARS%29%20vision%20tasks%20pose%20significant%20challenges%20due%20to%0Athe%20unique%20characteristics%20of%20their%20viewing%20angles.%20Existing%20research%20has%0Aprimarily%20focused%20on%20algorithms%20for%20specific%20tasks%2C%20which%20have%20limited%0Aapplicability%20in%20a%20broad%20range%20of%20ARS%20vision%20applications.%20This%20paper%20proposes%0Athe%20RingMo-Aerial%20model%2C%20aiming%20to%20fill%20the%20gap%20in%20foundation%20model%20research%20in%0Athe%20field%20of%20ARS%20vision.%20By%20introducing%20the%20Frequency-Enhanced%20Multi-Head%0ASelf-Attention%20%28FE-MSA%29%20mechanism%20and%20an%20affine%20transformation-based%0Acontrastive%20learning%20pre-training%20method%2C%20the%20model%27s%20detection%20capability%20for%0Asmall%20targets%20is%20enhanced%20and%20optimized%20for%20the%20tilted%20viewing%20angles%0Acharacteristic%20of%20ARS.%20Furthermore%2C%20the%20ARS-Adapter%2C%20an%20efficient%20parameter%0Afine-tuning%20method%2C%20is%20proposed%20to%20improve%20the%20model%27s%20adaptability%20and%0Aeffectiveness%20in%20various%20ARS%20vision%20tasks.%20Experimental%20results%20demonstrate%0Athat%20RingMo-Aerial%20achieves%20SOTA%20performance%20on%20multiple%20downstream%20tasks.%20This%0Aindicates%20the%20practicality%20and%20efficacy%20of%20RingMo-Aerial%20in%20enhancing%20the%0Aperformance%20of%20ARS%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13366v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRingMo-Aerial%253A%2520An%2520Aerial%2520Remote%2520Sensing%2520Foundation%2520Model%2520With%2520Affine%250A%2520%2520Transformation%2520Contrastive%2520Learning%26entry.906535625%3DWenhui%2520Diao%2520and%2520Haichen%2520Yu%2520and%2520Kaiyue%2520Kang%2520and%2520Tong%2520Ling%2520and%2520Di%2520Liu%2520and%2520Yingchao%2520Feng%2520and%2520Hanbo%2520Bi%2520and%2520Libo%2520Ren%2520and%2520Xuexue%2520Li%2520and%2520Yongqiang%2520Mao%2520and%2520Xian%2520Sun%26entry.1292438233%3D%2520%2520Aerial%2520Remote%2520Sensing%2520%2528ARS%2529%2520vision%2520tasks%2520pose%2520significant%2520challenges%2520due%2520to%250Athe%2520unique%2520characteristics%2520of%2520their%2520viewing%2520angles.%2520Existing%2520research%2520has%250Aprimarily%2520focused%2520on%2520algorithms%2520for%2520specific%2520tasks%252C%2520which%2520have%2520limited%250Aapplicability%2520in%2520a%2520broad%2520range%2520of%2520ARS%2520vision%2520applications.%2520This%2520paper%2520proposes%250Athe%2520RingMo-Aerial%2520model%252C%2520aiming%2520to%2520fill%2520the%2520gap%2520in%2520foundation%2520model%2520research%2520in%250Athe%2520field%2520of%2520ARS%2520vision.%2520By%2520introducing%2520the%2520Frequency-Enhanced%2520Multi-Head%250ASelf-Attention%2520%2528FE-MSA%2529%2520mechanism%2520and%2520an%2520affine%2520transformation-based%250Acontrastive%2520learning%2520pre-training%2520method%252C%2520the%2520model%2527s%2520detection%2520capability%2520for%250Asmall%2520targets%2520is%2520enhanced%2520and%2520optimized%2520for%2520the%2520tilted%2520viewing%2520angles%250Acharacteristic%2520of%2520ARS.%2520Furthermore%252C%2520the%2520ARS-Adapter%252C%2520an%2520efficient%2520parameter%250Afine-tuning%2520method%252C%2520is%2520proposed%2520to%2520improve%2520the%2520model%2527s%2520adaptability%2520and%250Aeffectiveness%2520in%2520various%2520ARS%2520vision%2520tasks.%2520Experimental%2520results%2520demonstrate%250Athat%2520RingMo-Aerial%2520achieves%2520SOTA%2520performance%2520on%2520multiple%2520downstream%2520tasks.%2520This%250Aindicates%2520the%2520practicality%2520and%2520efficacy%2520of%2520RingMo-Aerial%2520in%2520enhancing%2520the%250Aperformance%2520of%2520ARS%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13366v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RingMo-Aerial%3A%20An%20Aerial%20Remote%20Sensing%20Foundation%20Model%20With%20Affine%0A%20%20Transformation%20Contrastive%20Learning&entry.906535625=Wenhui%20Diao%20and%20Haichen%20Yu%20and%20Kaiyue%20Kang%20and%20Tong%20Ling%20and%20Di%20Liu%20and%20Yingchao%20Feng%20and%20Hanbo%20Bi%20and%20Libo%20Ren%20and%20Xuexue%20Li%20and%20Yongqiang%20Mao%20and%20Xian%20Sun&entry.1292438233=%20%20Aerial%20Remote%20Sensing%20%28ARS%29%20vision%20tasks%20pose%20significant%20challenges%20due%20to%0Athe%20unique%20characteristics%20of%20their%20viewing%20angles.%20Existing%20research%20has%0Aprimarily%20focused%20on%20algorithms%20for%20specific%20tasks%2C%20which%20have%20limited%0Aapplicability%20in%20a%20broad%20range%20of%20ARS%20vision%20applications.%20This%20paper%20proposes%0Athe%20RingMo-Aerial%20model%2C%20aiming%20to%20fill%20the%20gap%20in%20foundation%20model%20research%20in%0Athe%20field%20of%20ARS%20vision.%20By%20introducing%20the%20Frequency-Enhanced%20Multi-Head%0ASelf-Attention%20%28FE-MSA%29%20mechanism%20and%20an%20affine%20transformation-based%0Acontrastive%20learning%20pre-training%20method%2C%20the%20model%27s%20detection%20capability%20for%0Asmall%20targets%20is%20enhanced%20and%20optimized%20for%20the%20tilted%20viewing%20angles%0Acharacteristic%20of%20ARS.%20Furthermore%2C%20the%20ARS-Adapter%2C%20an%20efficient%20parameter%0Afine-tuning%20method%2C%20is%20proposed%20to%20improve%20the%20model%27s%20adaptability%20and%0Aeffectiveness%20in%20various%20ARS%20vision%20tasks.%20Experimental%20results%20demonstrate%0Athat%20RingMo-Aerial%20achieves%20SOTA%20performance%20on%20multiple%20downstream%20tasks.%20This%0Aindicates%20the%20practicality%20and%20efficacy%20of%20RingMo-Aerial%20in%20enhancing%20the%0Aperformance%20of%20ARS%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13366v3&entry.124074799=Read"},
{"title": "TimePoint: Accelerated Time Series Alignment via Self-Supervised\n  Keypoint and Descriptor Learning", "author": "Ron Shapira Weber and Shahar Ben Ishay and Andrey Lavrinenko and Shahaf E. Finder and Oren Freifeld", "abstract": "  Fast and scalable alignment of time series is a fundamental challenge in many\ndomains. The standard solution, Dynamic Time Warping (DTW), struggles with poor\nscalability and sensitivity to noise. We introduce TimePoint, a self-supervised\nmethod that dramatically accelerates DTW-based alignment while typically\nimproving alignment accuracy by learning keypoints and descriptors from\nsynthetic data. Inspired by 2D keypoint detection but carefully adapted to the\nunique challenges of 1D signals, TimePoint leverages efficient 1D\ndiffeomorphisms, which effectively model nonlinear time warping, to generate\nrealistic training data. This approach, along with fully convolutional and\nwavelet convolutional architectures, enables the extraction of informative\nkeypoints and descriptors. Applying DTW to these sparse representations yield\nmajor speedups and typically higher alignment accuracy than standard DTW\napplied to the full signals. TimePoint demonstrates strong generalization to\nreal-world time series when trained solely on synthetic data, and further\nimproves with fine-tuning on real data. Extensive experiments demonstrate that\nTimePoint consistently achieves faster and more accurate alignments than\nstandard DTW, making it a scalable solution for time-series analysis. Our code\nis available at https://github.com/BGU-CS-VIL/TimePoint\n", "link": "http://arxiv.org/abs/2505.23475v1", "date": "2025-05-29", "relevancy": 2.6723, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5845}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5393}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimePoint%3A%20Accelerated%20Time%20Series%20Alignment%20via%20Self-Supervised%0A%20%20Keypoint%20and%20Descriptor%20Learning&body=Title%3A%20TimePoint%3A%20Accelerated%20Time%20Series%20Alignment%20via%20Self-Supervised%0A%20%20Keypoint%20and%20Descriptor%20Learning%0AAuthor%3A%20Ron%20Shapira%20Weber%20and%20Shahar%20Ben%20Ishay%20and%20Andrey%20Lavrinenko%20and%20Shahaf%20E.%20Finder%20and%20Oren%20Freifeld%0AAbstract%3A%20%20%20Fast%20and%20scalable%20alignment%20of%20time%20series%20is%20a%20fundamental%20challenge%20in%20many%0Adomains.%20The%20standard%20solution%2C%20Dynamic%20Time%20Warping%20%28DTW%29%2C%20struggles%20with%20poor%0Ascalability%20and%20sensitivity%20to%20noise.%20We%20introduce%20TimePoint%2C%20a%20self-supervised%0Amethod%20that%20dramatically%20accelerates%20DTW-based%20alignment%20while%20typically%0Aimproving%20alignment%20accuracy%20by%20learning%20keypoints%20and%20descriptors%20from%0Asynthetic%20data.%20Inspired%20by%202D%20keypoint%20detection%20but%20carefully%20adapted%20to%20the%0Aunique%20challenges%20of%201D%20signals%2C%20TimePoint%20leverages%20efficient%201D%0Adiffeomorphisms%2C%20which%20effectively%20model%20nonlinear%20time%20warping%2C%20to%20generate%0Arealistic%20training%20data.%20This%20approach%2C%20along%20with%20fully%20convolutional%20and%0Awavelet%20convolutional%20architectures%2C%20enables%20the%20extraction%20of%20informative%0Akeypoints%20and%20descriptors.%20Applying%20DTW%20to%20these%20sparse%20representations%20yield%0Amajor%20speedups%20and%20typically%20higher%20alignment%20accuracy%20than%20standard%20DTW%0Aapplied%20to%20the%20full%20signals.%20TimePoint%20demonstrates%20strong%20generalization%20to%0Areal-world%20time%20series%20when%20trained%20solely%20on%20synthetic%20data%2C%20and%20further%0Aimproves%20with%20fine-tuning%20on%20real%20data.%20Extensive%20experiments%20demonstrate%20that%0ATimePoint%20consistently%20achieves%20faster%20and%20more%20accurate%20alignments%20than%0Astandard%20DTW%2C%20making%20it%20a%20scalable%20solution%20for%20time-series%20analysis.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/BGU-CS-VIL/TimePoint%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimePoint%253A%2520Accelerated%2520Time%2520Series%2520Alignment%2520via%2520Self-Supervised%250A%2520%2520Keypoint%2520and%2520Descriptor%2520Learning%26entry.906535625%3DRon%2520Shapira%2520Weber%2520and%2520Shahar%2520Ben%2520Ishay%2520and%2520Andrey%2520Lavrinenko%2520and%2520Shahaf%2520E.%2520Finder%2520and%2520Oren%2520Freifeld%26entry.1292438233%3D%2520%2520Fast%2520and%2520scalable%2520alignment%2520of%2520time%2520series%2520is%2520a%2520fundamental%2520challenge%2520in%2520many%250Adomains.%2520The%2520standard%2520solution%252C%2520Dynamic%2520Time%2520Warping%2520%2528DTW%2529%252C%2520struggles%2520with%2520poor%250Ascalability%2520and%2520sensitivity%2520to%2520noise.%2520We%2520introduce%2520TimePoint%252C%2520a%2520self-supervised%250Amethod%2520that%2520dramatically%2520accelerates%2520DTW-based%2520alignment%2520while%2520typically%250Aimproving%2520alignment%2520accuracy%2520by%2520learning%2520keypoints%2520and%2520descriptors%2520from%250Asynthetic%2520data.%2520Inspired%2520by%25202D%2520keypoint%2520detection%2520but%2520carefully%2520adapted%2520to%2520the%250Aunique%2520challenges%2520of%25201D%2520signals%252C%2520TimePoint%2520leverages%2520efficient%25201D%250Adiffeomorphisms%252C%2520which%2520effectively%2520model%2520nonlinear%2520time%2520warping%252C%2520to%2520generate%250Arealistic%2520training%2520data.%2520This%2520approach%252C%2520along%2520with%2520fully%2520convolutional%2520and%250Awavelet%2520convolutional%2520architectures%252C%2520enables%2520the%2520extraction%2520of%2520informative%250Akeypoints%2520and%2520descriptors.%2520Applying%2520DTW%2520to%2520these%2520sparse%2520representations%2520yield%250Amajor%2520speedups%2520and%2520typically%2520higher%2520alignment%2520accuracy%2520than%2520standard%2520DTW%250Aapplied%2520to%2520the%2520full%2520signals.%2520TimePoint%2520demonstrates%2520strong%2520generalization%2520to%250Areal-world%2520time%2520series%2520when%2520trained%2520solely%2520on%2520synthetic%2520data%252C%2520and%2520further%250Aimproves%2520with%2520fine-tuning%2520on%2520real%2520data.%2520Extensive%2520experiments%2520demonstrate%2520that%250ATimePoint%2520consistently%2520achieves%2520faster%2520and%2520more%2520accurate%2520alignments%2520than%250Astandard%2520DTW%252C%2520making%2520it%2520a%2520scalable%2520solution%2520for%2520time-series%2520analysis.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/BGU-CS-VIL/TimePoint%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimePoint%3A%20Accelerated%20Time%20Series%20Alignment%20via%20Self-Supervised%0A%20%20Keypoint%20and%20Descriptor%20Learning&entry.906535625=Ron%20Shapira%20Weber%20and%20Shahar%20Ben%20Ishay%20and%20Andrey%20Lavrinenko%20and%20Shahaf%20E.%20Finder%20and%20Oren%20Freifeld&entry.1292438233=%20%20Fast%20and%20scalable%20alignment%20of%20time%20series%20is%20a%20fundamental%20challenge%20in%20many%0Adomains.%20The%20standard%20solution%2C%20Dynamic%20Time%20Warping%20%28DTW%29%2C%20struggles%20with%20poor%0Ascalability%20and%20sensitivity%20to%20noise.%20We%20introduce%20TimePoint%2C%20a%20self-supervised%0Amethod%20that%20dramatically%20accelerates%20DTW-based%20alignment%20while%20typically%0Aimproving%20alignment%20accuracy%20by%20learning%20keypoints%20and%20descriptors%20from%0Asynthetic%20data.%20Inspired%20by%202D%20keypoint%20detection%20but%20carefully%20adapted%20to%20the%0Aunique%20challenges%20of%201D%20signals%2C%20TimePoint%20leverages%20efficient%201D%0Adiffeomorphisms%2C%20which%20effectively%20model%20nonlinear%20time%20warping%2C%20to%20generate%0Arealistic%20training%20data.%20This%20approach%2C%20along%20with%20fully%20convolutional%20and%0Awavelet%20convolutional%20architectures%2C%20enables%20the%20extraction%20of%20informative%0Akeypoints%20and%20descriptors.%20Applying%20DTW%20to%20these%20sparse%20representations%20yield%0Amajor%20speedups%20and%20typically%20higher%20alignment%20accuracy%20than%20standard%20DTW%0Aapplied%20to%20the%20full%20signals.%20TimePoint%20demonstrates%20strong%20generalization%20to%0Areal-world%20time%20series%20when%20trained%20solely%20on%20synthetic%20data%2C%20and%20further%0Aimproves%20with%20fine-tuning%20on%20real%20data.%20Extensive%20experiments%20demonstrate%20that%0ATimePoint%20consistently%20achieves%20faster%20and%20more%20accurate%20alignments%20than%0Astandard%20DTW%2C%20making%20it%20a%20scalable%20solution%20for%20time-series%20analysis.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/BGU-CS-VIL/TimePoint%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23475v1&entry.124074799=Read"},
{"title": "Point or Line? Using Line-based Representation for Panoptic Symbol\n  Spotting in CAD Drawings", "author": "Xingguang Wei and Haomin Wang and Shenglong Ye and Ruifeng Luo and Yanting Zhang and Lixin Gu and Jifeng Dai and Yu Qiao and Wenhai Wang and Hongjie Zhang", "abstract": "  We study the task of panoptic symbol spotting, which involves identifying\nboth individual instances of countable things and the semantic regions of\nuncountable stuff in computer-aided design (CAD) drawings composed of vector\ngraphical primitives. Existing methods typically rely on image rasterization,\ngraph construction, or point-based representation, but these approaches often\nsuffer from high computational costs, limited generality, and loss of geometric\nstructural information. In this paper, we propose VecFormer, a novel method\nthat addresses these challenges through line-based representation of\nprimitives. This design preserves the geometric continuity of the original\nprimitive, enabling more accurate shape representation while maintaining a\ncomputation-friendly structure, making it well-suited for vector graphic\nunderstanding tasks. To further enhance prediction reliability, we introduce a\nBranch Fusion Refinement module that effectively integrates instance and\nsemantic predictions, resolving their inconsistencies for more coherent\npanoptic outputs. Extensive experiments demonstrate that our method establishes\na new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and\n21.2 points over the second-best results under settings with and without prior\ninformation, respectively, highlighting the strong potential of line-based\nrepresentation as a foundation for vector graphic understanding.\n", "link": "http://arxiv.org/abs/2505.23395v1", "date": "2025-05-29", "relevancy": 2.6706, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20or%20Line%3F%20Using%20Line-based%20Representation%20for%20Panoptic%20Symbol%0A%20%20Spotting%20in%20CAD%20Drawings&body=Title%3A%20Point%20or%20Line%3F%20Using%20Line-based%20Representation%20for%20Panoptic%20Symbol%0A%20%20Spotting%20in%20CAD%20Drawings%0AAuthor%3A%20Xingguang%20Wei%20and%20Haomin%20Wang%20and%20Shenglong%20Ye%20and%20Ruifeng%20Luo%20and%20Yanting%20Zhang%20and%20Lixin%20Gu%20and%20Jifeng%20Dai%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Hongjie%20Zhang%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20panoptic%20symbol%20spotting%2C%20which%20involves%20identifying%0Aboth%20individual%20instances%20of%20countable%20things%20and%20the%20semantic%20regions%20of%0Auncountable%20stuff%20in%20computer-aided%20design%20%28CAD%29%20drawings%20composed%20of%20vector%0Agraphical%20primitives.%20Existing%20methods%20typically%20rely%20on%20image%20rasterization%2C%0Agraph%20construction%2C%20or%20point-based%20representation%2C%20but%20these%20approaches%20often%0Asuffer%20from%20high%20computational%20costs%2C%20limited%20generality%2C%20and%20loss%20of%20geometric%0Astructural%20information.%20In%20this%20paper%2C%20we%20propose%20VecFormer%2C%20a%20novel%20method%0Athat%20addresses%20these%20challenges%20through%20line-based%20representation%20of%0Aprimitives.%20This%20design%20preserves%20the%20geometric%20continuity%20of%20the%20original%0Aprimitive%2C%20enabling%20more%20accurate%20shape%20representation%20while%20maintaining%20a%0Acomputation-friendly%20structure%2C%20making%20it%20well-suited%20for%20vector%20graphic%0Aunderstanding%20tasks.%20To%20further%20enhance%20prediction%20reliability%2C%20we%20introduce%20a%0ABranch%20Fusion%20Refinement%20module%20that%20effectively%20integrates%20instance%20and%0Asemantic%20predictions%2C%20resolving%20their%20inconsistencies%20for%20more%20coherent%0Apanoptic%20outputs.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20establishes%0Aa%20new%20state-of-the-art%2C%20achieving%2091.1%20PQ%2C%20with%20Stuff-PQ%20improved%20by%209.6%20and%0A21.2%20points%20over%20the%20second-best%20results%20under%20settings%20with%20and%20without%20prior%0Ainformation%2C%20respectively%2C%20highlighting%20the%20strong%20potential%20of%20line-based%0Arepresentation%20as%20a%20foundation%20for%20vector%20graphic%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520or%2520Line%253F%2520Using%2520Line-based%2520Representation%2520for%2520Panoptic%2520Symbol%250A%2520%2520Spotting%2520in%2520CAD%2520Drawings%26entry.906535625%3DXingguang%2520Wei%2520and%2520Haomin%2520Wang%2520and%2520Shenglong%2520Ye%2520and%2520Ruifeng%2520Luo%2520and%2520Yanting%2520Zhang%2520and%2520Lixin%2520Gu%2520and%2520Jifeng%2520Dai%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%2520and%2520Hongjie%2520Zhang%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520panoptic%2520symbol%2520spotting%252C%2520which%2520involves%2520identifying%250Aboth%2520individual%2520instances%2520of%2520countable%2520things%2520and%2520the%2520semantic%2520regions%2520of%250Auncountable%2520stuff%2520in%2520computer-aided%2520design%2520%2528CAD%2529%2520drawings%2520composed%2520of%2520vector%250Agraphical%2520primitives.%2520Existing%2520methods%2520typically%2520rely%2520on%2520image%2520rasterization%252C%250Agraph%2520construction%252C%2520or%2520point-based%2520representation%252C%2520but%2520these%2520approaches%2520often%250Asuffer%2520from%2520high%2520computational%2520costs%252C%2520limited%2520generality%252C%2520and%2520loss%2520of%2520geometric%250Astructural%2520information.%2520In%2520this%2520paper%252C%2520we%2520propose%2520VecFormer%252C%2520a%2520novel%2520method%250Athat%2520addresses%2520these%2520challenges%2520through%2520line-based%2520representation%2520of%250Aprimitives.%2520This%2520design%2520preserves%2520the%2520geometric%2520continuity%2520of%2520the%2520original%250Aprimitive%252C%2520enabling%2520more%2520accurate%2520shape%2520representation%2520while%2520maintaining%2520a%250Acomputation-friendly%2520structure%252C%2520making%2520it%2520well-suited%2520for%2520vector%2520graphic%250Aunderstanding%2520tasks.%2520To%2520further%2520enhance%2520prediction%2520reliability%252C%2520we%2520introduce%2520a%250ABranch%2520Fusion%2520Refinement%2520module%2520that%2520effectively%2520integrates%2520instance%2520and%250Asemantic%2520predictions%252C%2520resolving%2520their%2520inconsistencies%2520for%2520more%2520coherent%250Apanoptic%2520outputs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520establishes%250Aa%2520new%2520state-of-the-art%252C%2520achieving%252091.1%2520PQ%252C%2520with%2520Stuff-PQ%2520improved%2520by%25209.6%2520and%250A21.2%2520points%2520over%2520the%2520second-best%2520results%2520under%2520settings%2520with%2520and%2520without%2520prior%250Ainformation%252C%2520respectively%252C%2520highlighting%2520the%2520strong%2520potential%2520of%2520line-based%250Arepresentation%2520as%2520a%2520foundation%2520for%2520vector%2520graphic%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20or%20Line%3F%20Using%20Line-based%20Representation%20for%20Panoptic%20Symbol%0A%20%20Spotting%20in%20CAD%20Drawings&entry.906535625=Xingguang%20Wei%20and%20Haomin%20Wang%20and%20Shenglong%20Ye%20and%20Ruifeng%20Luo%20and%20Yanting%20Zhang%20and%20Lixin%20Gu%20and%20Jifeng%20Dai%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Hongjie%20Zhang&entry.1292438233=%20%20We%20study%20the%20task%20of%20panoptic%20symbol%20spotting%2C%20which%20involves%20identifying%0Aboth%20individual%20instances%20of%20countable%20things%20and%20the%20semantic%20regions%20of%0Auncountable%20stuff%20in%20computer-aided%20design%20%28CAD%29%20drawings%20composed%20of%20vector%0Agraphical%20primitives.%20Existing%20methods%20typically%20rely%20on%20image%20rasterization%2C%0Agraph%20construction%2C%20or%20point-based%20representation%2C%20but%20these%20approaches%20often%0Asuffer%20from%20high%20computational%20costs%2C%20limited%20generality%2C%20and%20loss%20of%20geometric%0Astructural%20information.%20In%20this%20paper%2C%20we%20propose%20VecFormer%2C%20a%20novel%20method%0Athat%20addresses%20these%20challenges%20through%20line-based%20representation%20of%0Aprimitives.%20This%20design%20preserves%20the%20geometric%20continuity%20of%20the%20original%0Aprimitive%2C%20enabling%20more%20accurate%20shape%20representation%20while%20maintaining%20a%0Acomputation-friendly%20structure%2C%20making%20it%20well-suited%20for%20vector%20graphic%0Aunderstanding%20tasks.%20To%20further%20enhance%20prediction%20reliability%2C%20we%20introduce%20a%0ABranch%20Fusion%20Refinement%20module%20that%20effectively%20integrates%20instance%20and%0Asemantic%20predictions%2C%20resolving%20their%20inconsistencies%20for%20more%20coherent%0Apanoptic%20outputs.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20establishes%0Aa%20new%20state-of-the-art%2C%20achieving%2091.1%20PQ%2C%20with%20Stuff-PQ%20improved%20by%209.6%20and%0A21.2%20points%20over%20the%20second-best%20results%20under%20settings%20with%20and%20without%20prior%0Ainformation%2C%20respectively%2C%20highlighting%20the%20strong%20potential%20of%20line-based%0Arepresentation%20as%20a%20foundation%20for%20vector%20graphic%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23395v1&entry.124074799=Read"},
{"title": "SGD Jittering: A Training Strategy for Robust and Accurate Model-Based\n  Architectures", "author": "Peimeng Guan and Mark A. Davenport", "abstract": "  Inverse problems aim to reconstruct unseen data from corrupted or perturbed\nmeasurements. While most work focuses on improving reconstruction quality,\ngeneralization accuracy and robustness are equally important, especially for\nsafety-critical applications. Model-based architectures (MBAs), such as loop\nunrolling methods, are considered more interpretable and achieve better\nreconstructions. Empirical evidence suggests that MBAs are more robust to\nperturbations than black-box solvers, but the accuracy-robustness tradeoff in\nMBAs remains underexplored. In this work, we propose a simple yet effective\ntraining scheme for MBAs, called SGD jittering, which injects noise\niteration-wise during reconstruction. We theoretically demonstrate that SGD\njittering not only generalizes better than the standard mean squared error\ntraining but is also more robust to average-case attacks. We validate SGD\njittering using denoising toy examples, seismic deconvolution, and single-coil\nMRI reconstruction. Both SGD jittering and its SPGD extension yield cleaner\nreconstructions for out-of-distribution data and demonstrates enhanced\nrobustness against adversarial attacks.\n", "link": "http://arxiv.org/abs/2410.14667v2", "date": "2025-05-29", "relevancy": 2.665, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5404}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5318}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGD%20Jittering%3A%20A%20Training%20Strategy%20for%20Robust%20and%20Accurate%20Model-Based%0A%20%20Architectures&body=Title%3A%20SGD%20Jittering%3A%20A%20Training%20Strategy%20for%20Robust%20and%20Accurate%20Model-Based%0A%20%20Architectures%0AAuthor%3A%20Peimeng%20Guan%20and%20Mark%20A.%20Davenport%0AAbstract%3A%20%20%20Inverse%20problems%20aim%20to%20reconstruct%20unseen%20data%20from%20corrupted%20or%20perturbed%0Ameasurements.%20While%20most%20work%20focuses%20on%20improving%20reconstruction%20quality%2C%0Ageneralization%20accuracy%20and%20robustness%20are%20equally%20important%2C%20especially%20for%0Asafety-critical%20applications.%20Model-based%20architectures%20%28MBAs%29%2C%20such%20as%20loop%0Aunrolling%20methods%2C%20are%20considered%20more%20interpretable%20and%20achieve%20better%0Areconstructions.%20Empirical%20evidence%20suggests%20that%20MBAs%20are%20more%20robust%20to%0Aperturbations%20than%20black-box%20solvers%2C%20but%20the%20accuracy-robustness%20tradeoff%20in%0AMBAs%20remains%20underexplored.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%0Atraining%20scheme%20for%20MBAs%2C%20called%20SGD%20jittering%2C%20which%20injects%20noise%0Aiteration-wise%20during%20reconstruction.%20We%20theoretically%20demonstrate%20that%20SGD%0Ajittering%20not%20only%20generalizes%20better%20than%20the%20standard%20mean%20squared%20error%0Atraining%20but%20is%20also%20more%20robust%20to%20average-case%20attacks.%20We%20validate%20SGD%0Ajittering%20using%20denoising%20toy%20examples%2C%20seismic%20deconvolution%2C%20and%20single-coil%0AMRI%20reconstruction.%20Both%20SGD%20jittering%20and%20its%20SPGD%20extension%20yield%20cleaner%0Areconstructions%20for%20out-of-distribution%20data%20and%20demonstrates%20enhanced%0Arobustness%20against%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGD%2520Jittering%253A%2520A%2520Training%2520Strategy%2520for%2520Robust%2520and%2520Accurate%2520Model-Based%250A%2520%2520Architectures%26entry.906535625%3DPeimeng%2520Guan%2520and%2520Mark%2520A.%2520Davenport%26entry.1292438233%3D%2520%2520Inverse%2520problems%2520aim%2520to%2520reconstruct%2520unseen%2520data%2520from%2520corrupted%2520or%2520perturbed%250Ameasurements.%2520While%2520most%2520work%2520focuses%2520on%2520improving%2520reconstruction%2520quality%252C%250Ageneralization%2520accuracy%2520and%2520robustness%2520are%2520equally%2520important%252C%2520especially%2520for%250Asafety-critical%2520applications.%2520Model-based%2520architectures%2520%2528MBAs%2529%252C%2520such%2520as%2520loop%250Aunrolling%2520methods%252C%2520are%2520considered%2520more%2520interpretable%2520and%2520achieve%2520better%250Areconstructions.%2520Empirical%2520evidence%2520suggests%2520that%2520MBAs%2520are%2520more%2520robust%2520to%250Aperturbations%2520than%2520black-box%2520solvers%252C%2520but%2520the%2520accuracy-robustness%2520tradeoff%2520in%250AMBAs%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Atraining%2520scheme%2520for%2520MBAs%252C%2520called%2520SGD%2520jittering%252C%2520which%2520injects%2520noise%250Aiteration-wise%2520during%2520reconstruction.%2520We%2520theoretically%2520demonstrate%2520that%2520SGD%250Ajittering%2520not%2520only%2520generalizes%2520better%2520than%2520the%2520standard%2520mean%2520squared%2520error%250Atraining%2520but%2520is%2520also%2520more%2520robust%2520to%2520average-case%2520attacks.%2520We%2520validate%2520SGD%250Ajittering%2520using%2520denoising%2520toy%2520examples%252C%2520seismic%2520deconvolution%252C%2520and%2520single-coil%250AMRI%2520reconstruction.%2520Both%2520SGD%2520jittering%2520and%2520its%2520SPGD%2520extension%2520yield%2520cleaner%250Areconstructions%2520for%2520out-of-distribution%2520data%2520and%2520demonstrates%2520enhanced%250Arobustness%2520against%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGD%20Jittering%3A%20A%20Training%20Strategy%20for%20Robust%20and%20Accurate%20Model-Based%0A%20%20Architectures&entry.906535625=Peimeng%20Guan%20and%20Mark%20A.%20Davenport&entry.1292438233=%20%20Inverse%20problems%20aim%20to%20reconstruct%20unseen%20data%20from%20corrupted%20or%20perturbed%0Ameasurements.%20While%20most%20work%20focuses%20on%20improving%20reconstruction%20quality%2C%0Ageneralization%20accuracy%20and%20robustness%20are%20equally%20important%2C%20especially%20for%0Asafety-critical%20applications.%20Model-based%20architectures%20%28MBAs%29%2C%20such%20as%20loop%0Aunrolling%20methods%2C%20are%20considered%20more%20interpretable%20and%20achieve%20better%0Areconstructions.%20Empirical%20evidence%20suggests%20that%20MBAs%20are%20more%20robust%20to%0Aperturbations%20than%20black-box%20solvers%2C%20but%20the%20accuracy-robustness%20tradeoff%20in%0AMBAs%20remains%20underexplored.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%0Atraining%20scheme%20for%20MBAs%2C%20called%20SGD%20jittering%2C%20which%20injects%20noise%0Aiteration-wise%20during%20reconstruction.%20We%20theoretically%20demonstrate%20that%20SGD%0Ajittering%20not%20only%20generalizes%20better%20than%20the%20standard%20mean%20squared%20error%0Atraining%20but%20is%20also%20more%20robust%20to%20average-case%20attacks.%20We%20validate%20SGD%0Ajittering%20using%20denoising%20toy%20examples%2C%20seismic%20deconvolution%2C%20and%20single-coil%0AMRI%20reconstruction.%20Both%20SGD%20jittering%20and%20its%20SPGD%20extension%20yield%20cleaner%0Areconstructions%20for%20out-of-distribution%20data%20and%20demonstrates%20enhanced%0Arobustness%20against%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14667v2&entry.124074799=Read"},
{"title": "Surveying the space of descriptions of a composite system with machine\n  learning", "author": "Kieran A. Murphy and Yujing Zhang and Dani S. Bassett", "abstract": "  Multivariate information theory provides a general and principled framework\nfor understanding how the components of a complex system are connected.\nExisting analyses are coarse in nature -- built up from characterizations of\ndiscrete subsystems -- and can be computationally prohibitive. In this work, we\npropose to study the continuous space of possible descriptions of a composite\nsystem as a window into its organizational structure. A description consists of\nspecific information conveyed about each of the components, and the space of\npossible descriptions is equivalent to the space of lossy compression schemes\nof the components. We introduce a machine learning framework to optimize\ndescriptions that extremize key information theoretic quantities used to\ncharacterize organization, such as total correlation and O-information. Through\ncase studies on spin systems, sudoku boards, and letter sequences from natural\nlanguage, we identify extremal descriptions that reveal how system-wide\nvariation emerges from individual components. By integrating machine learning\ninto a fine-grained information theoretic analysis of composite random\nvariables, our framework opens a new avenues for probing the structure of\nreal-world complex systems.\n", "link": "http://arxiv.org/abs/2411.18579v2", "date": "2025-05-29", "relevancy": 2.6483, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surveying%20the%20space%20of%20descriptions%20of%20a%20composite%20system%20with%20machine%0A%20%20learning&body=Title%3A%20Surveying%20the%20space%20of%20descriptions%20of%20a%20composite%20system%20with%20machine%0A%20%20learning%0AAuthor%3A%20Kieran%20A.%20Murphy%20and%20Yujing%20Zhang%20and%20Dani%20S.%20Bassett%0AAbstract%3A%20%20%20Multivariate%20information%20theory%20provides%20a%20general%20and%20principled%20framework%0Afor%20understanding%20how%20the%20components%20of%20a%20complex%20system%20are%20connected.%0AExisting%20analyses%20are%20coarse%20in%20nature%20--%20built%20up%20from%20characterizations%20of%0Adiscrete%20subsystems%20--%20and%20can%20be%20computationally%20prohibitive.%20In%20this%20work%2C%20we%0Apropose%20to%20study%20the%20continuous%20space%20of%20possible%20descriptions%20of%20a%20composite%0Asystem%20as%20a%20window%20into%20its%20organizational%20structure.%20A%20description%20consists%20of%0Aspecific%20information%20conveyed%20about%20each%20of%20the%20components%2C%20and%20the%20space%20of%0Apossible%20descriptions%20is%20equivalent%20to%20the%20space%20of%20lossy%20compression%20schemes%0Aof%20the%20components.%20We%20introduce%20a%20machine%20learning%20framework%20to%20optimize%0Adescriptions%20that%20extremize%20key%20information%20theoretic%20quantities%20used%20to%0Acharacterize%20organization%2C%20such%20as%20total%20correlation%20and%20O-information.%20Through%0Acase%20studies%20on%20spin%20systems%2C%20sudoku%20boards%2C%20and%20letter%20sequences%20from%20natural%0Alanguage%2C%20we%20identify%20extremal%20descriptions%20that%20reveal%20how%20system-wide%0Avariation%20emerges%20from%20individual%20components.%20By%20integrating%20machine%20learning%0Ainto%20a%20fine-grained%20information%20theoretic%20analysis%20of%20composite%20random%0Avariables%2C%20our%20framework%20opens%20a%20new%20avenues%20for%20probing%20the%20structure%20of%0Areal-world%20complex%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18579v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurveying%2520the%2520space%2520of%2520descriptions%2520of%2520a%2520composite%2520system%2520with%2520machine%250A%2520%2520learning%26entry.906535625%3DKieran%2520A.%2520Murphy%2520and%2520Yujing%2520Zhang%2520and%2520Dani%2520S.%2520Bassett%26entry.1292438233%3D%2520%2520Multivariate%2520information%2520theory%2520provides%2520a%2520general%2520and%2520principled%2520framework%250Afor%2520understanding%2520how%2520the%2520components%2520of%2520a%2520complex%2520system%2520are%2520connected.%250AExisting%2520analyses%2520are%2520coarse%2520in%2520nature%2520--%2520built%2520up%2520from%2520characterizations%2520of%250Adiscrete%2520subsystems%2520--%2520and%2520can%2520be%2520computationally%2520prohibitive.%2520In%2520this%2520work%252C%2520we%250Apropose%2520to%2520study%2520the%2520continuous%2520space%2520of%2520possible%2520descriptions%2520of%2520a%2520composite%250Asystem%2520as%2520a%2520window%2520into%2520its%2520organizational%2520structure.%2520A%2520description%2520consists%2520of%250Aspecific%2520information%2520conveyed%2520about%2520each%2520of%2520the%2520components%252C%2520and%2520the%2520space%2520of%250Apossible%2520descriptions%2520is%2520equivalent%2520to%2520the%2520space%2520of%2520lossy%2520compression%2520schemes%250Aof%2520the%2520components.%2520We%2520introduce%2520a%2520machine%2520learning%2520framework%2520to%2520optimize%250Adescriptions%2520that%2520extremize%2520key%2520information%2520theoretic%2520quantities%2520used%2520to%250Acharacterize%2520organization%252C%2520such%2520as%2520total%2520correlation%2520and%2520O-information.%2520Through%250Acase%2520studies%2520on%2520spin%2520systems%252C%2520sudoku%2520boards%252C%2520and%2520letter%2520sequences%2520from%2520natural%250Alanguage%252C%2520we%2520identify%2520extremal%2520descriptions%2520that%2520reveal%2520how%2520system-wide%250Avariation%2520emerges%2520from%2520individual%2520components.%2520By%2520integrating%2520machine%2520learning%250Ainto%2520a%2520fine-grained%2520information%2520theoretic%2520analysis%2520of%2520composite%2520random%250Avariables%252C%2520our%2520framework%2520opens%2520a%2520new%2520avenues%2520for%2520probing%2520the%2520structure%2520of%250Areal-world%2520complex%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18579v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surveying%20the%20space%20of%20descriptions%20of%20a%20composite%20system%20with%20machine%0A%20%20learning&entry.906535625=Kieran%20A.%20Murphy%20and%20Yujing%20Zhang%20and%20Dani%20S.%20Bassett&entry.1292438233=%20%20Multivariate%20information%20theory%20provides%20a%20general%20and%20principled%20framework%0Afor%20understanding%20how%20the%20components%20of%20a%20complex%20system%20are%20connected.%0AExisting%20analyses%20are%20coarse%20in%20nature%20--%20built%20up%20from%20characterizations%20of%0Adiscrete%20subsystems%20--%20and%20can%20be%20computationally%20prohibitive.%20In%20this%20work%2C%20we%0Apropose%20to%20study%20the%20continuous%20space%20of%20possible%20descriptions%20of%20a%20composite%0Asystem%20as%20a%20window%20into%20its%20organizational%20structure.%20A%20description%20consists%20of%0Aspecific%20information%20conveyed%20about%20each%20of%20the%20components%2C%20and%20the%20space%20of%0Apossible%20descriptions%20is%20equivalent%20to%20the%20space%20of%20lossy%20compression%20schemes%0Aof%20the%20components.%20We%20introduce%20a%20machine%20learning%20framework%20to%20optimize%0Adescriptions%20that%20extremize%20key%20information%20theoretic%20quantities%20used%20to%0Acharacterize%20organization%2C%20such%20as%20total%20correlation%20and%20O-information.%20Through%0Acase%20studies%20on%20spin%20systems%2C%20sudoku%20boards%2C%20and%20letter%20sequences%20from%20natural%0Alanguage%2C%20we%20identify%20extremal%20descriptions%20that%20reveal%20how%20system-wide%0Avariation%20emerges%20from%20individual%20components.%20By%20integrating%20machine%20learning%0Ainto%20a%20fine-grained%20information%20theoretic%20analysis%20of%20composite%20random%0Avariables%2C%20our%20framework%20opens%20a%20new%20avenues%20for%20probing%20the%20structure%20of%0Areal-world%20complex%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18579v2&entry.124074799=Read"},
{"title": "Can Large Language Models Challenge CNNS in Medical Image Analysis?", "author": "Shibbir Ahmed and Shahnewaz Karim Sakib and Anindya Bijoy Das", "abstract": "  This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.\n", "link": "http://arxiv.org/abs/2505.23503v1", "date": "2025-05-29", "relevancy": 2.6404, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5407}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Challenge%20CNNS%20in%20Medical%20Image%20Analysis%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Challenge%20CNNS%20in%20Medical%20Image%20Analysis%3F%0AAuthor%3A%20Shibbir%20Ahmed%20and%20Shahnewaz%20Karim%20Sakib%20and%20Anindya%20Bijoy%20Das%0AAbstract%3A%20%20%20This%20study%20presents%20a%20multimodal%20AI%20framework%20designed%20for%20precisely%0Aclassifying%20medical%20diagnostic%20images.%20Utilizing%20publicly%20available%20datasets%2C%0Athe%20proposed%20system%20compares%20the%20strengths%20of%20convolutional%20neural%20networks%0A%28CNNs%29%20and%20different%20large%20language%20models%20%28LLMs%29.%20This%20in-depth%20comparative%0Aanalysis%20highlights%20key%20differences%20in%20diagnostic%20performance%2C%20execution%0Aefficiency%2C%20and%20environmental%20impacts.%20Model%20evaluation%20was%20based%20on%20accuracy%2C%0AF1-score%2C%20average%20execution%20time%2C%20average%20energy%20consumption%2C%20and%20estimated%0A%24CO_2%24%20emission.%20The%20findings%20indicate%20that%20although%20CNN-based%20models%20can%0Aoutperform%20various%20multimodal%20techniques%20that%20incorporate%20both%20images%20and%0Acontextual%20information%2C%20applying%20additional%20filtering%20on%20top%20of%20LLMs%20can%20lead%0Ato%20substantial%20performance%20gains.%20These%20findings%20highlight%20the%20transformative%0Apotential%20of%20multimodal%20AI%20systems%20to%20enhance%20the%20reliability%2C%20efficiency%2C%20and%0Ascalability%20of%20medical%20diagnostics%20in%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Challenge%2520CNNS%2520in%2520Medical%2520Image%2520Analysis%253F%26entry.906535625%3DShibbir%2520Ahmed%2520and%2520Shahnewaz%2520Karim%2520Sakib%2520and%2520Anindya%2520Bijoy%2520Das%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520multimodal%2520AI%2520framework%2520designed%2520for%2520precisely%250Aclassifying%2520medical%2520diagnostic%2520images.%2520Utilizing%2520publicly%2520available%2520datasets%252C%250Athe%2520proposed%2520system%2520compares%2520the%2520strengths%2520of%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520and%2520different%2520large%2520language%2520models%2520%2528LLMs%2529.%2520This%2520in-depth%2520comparative%250Aanalysis%2520highlights%2520key%2520differences%2520in%2520diagnostic%2520performance%252C%2520execution%250Aefficiency%252C%2520and%2520environmental%2520impacts.%2520Model%2520evaluation%2520was%2520based%2520on%2520accuracy%252C%250AF1-score%252C%2520average%2520execution%2520time%252C%2520average%2520energy%2520consumption%252C%2520and%2520estimated%250A%2524CO_2%2524%2520emission.%2520The%2520findings%2520indicate%2520that%2520although%2520CNN-based%2520models%2520can%250Aoutperform%2520various%2520multimodal%2520techniques%2520that%2520incorporate%2520both%2520images%2520and%250Acontextual%2520information%252C%2520applying%2520additional%2520filtering%2520on%2520top%2520of%2520LLMs%2520can%2520lead%250Ato%2520substantial%2520performance%2520gains.%2520These%2520findings%2520highlight%2520the%2520transformative%250Apotential%2520of%2520multimodal%2520AI%2520systems%2520to%2520enhance%2520the%2520reliability%252C%2520efficiency%252C%2520and%250Ascalability%2520of%2520medical%2520diagnostics%2520in%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Challenge%20CNNS%20in%20Medical%20Image%20Analysis%3F&entry.906535625=Shibbir%20Ahmed%20and%20Shahnewaz%20Karim%20Sakib%20and%20Anindya%20Bijoy%20Das&entry.1292438233=%20%20This%20study%20presents%20a%20multimodal%20AI%20framework%20designed%20for%20precisely%0Aclassifying%20medical%20diagnostic%20images.%20Utilizing%20publicly%20available%20datasets%2C%0Athe%20proposed%20system%20compares%20the%20strengths%20of%20convolutional%20neural%20networks%0A%28CNNs%29%20and%20different%20large%20language%20models%20%28LLMs%29.%20This%20in-depth%20comparative%0Aanalysis%20highlights%20key%20differences%20in%20diagnostic%20performance%2C%20execution%0Aefficiency%2C%20and%20environmental%20impacts.%20Model%20evaluation%20was%20based%20on%20accuracy%2C%0AF1-score%2C%20average%20execution%20time%2C%20average%20energy%20consumption%2C%20and%20estimated%0A%24CO_2%24%20emission.%20The%20findings%20indicate%20that%20although%20CNN-based%20models%20can%0Aoutperform%20various%20multimodal%20techniques%20that%20incorporate%20both%20images%20and%0Acontextual%20information%2C%20applying%20additional%20filtering%20on%20top%20of%20LLMs%20can%20lead%0Ato%20substantial%20performance%20gains.%20These%20findings%20highlight%20the%20transformative%0Apotential%20of%20multimodal%20AI%20systems%20to%20enhance%20the%20reliability%2C%20efficiency%2C%20and%0Ascalability%20of%20medical%20diagnostics%20in%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23503v1&entry.124074799=Read"},
{"title": "Subgraph Gaussian Embedding Contrast for Self-Supervised Graph\n  Representation Learning", "author": "Shifeng Xie and Aref Einizade and Jhony H. Giraldo", "abstract": "  Graph Representation Learning (GRL) is a fundamental task in machine\nlearning, aiming to encode high-dimensional graph-structured data into\nlow-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used\nin GRL because they can avoid expensive human annotation. In this work, we\npropose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our\napproach introduces a subgraph Gaussian embedding module, which adaptively maps\nsubgraphs to a structured Gaussian space, ensuring the preservation of input\nsubgraph characteristics while generating subgraphs with a controlled\ndistribution. We then employ optimal transport distances, more precisely the\nWasserstein and Gromov-Wasserstein distances, to effectively measure the\nsimilarity between subgraphs, enhancing the robustness of the contrastive\nlearning process. Extensive experiments across multiple benchmarks demonstrate\nthat \\method~outperforms or presents competitive performance against\nstate-of-the-art approaches. Our findings provide insights into the design of\nSSL methods for GRL, emphasizing the importance of the distribution of the\ngenerated contrastive pairs.\n", "link": "http://arxiv.org/abs/2505.23529v1", "date": "2025-05-29", "relevancy": 2.6326, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5873}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4995}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subgraph%20Gaussian%20Embedding%20Contrast%20for%20Self-Supervised%20Graph%0A%20%20Representation%20Learning&body=Title%3A%20Subgraph%20Gaussian%20Embedding%20Contrast%20for%20Self-Supervised%20Graph%0A%20%20Representation%20Learning%0AAuthor%3A%20Shifeng%20Xie%20and%20Aref%20Einizade%20and%20Jhony%20H.%20Giraldo%0AAbstract%3A%20%20%20Graph%20Representation%20Learning%20%28GRL%29%20is%20a%20fundamental%20task%20in%20machine%0Alearning%2C%20aiming%20to%20encode%20high-dimensional%20graph-structured%20data%20into%0Alow-dimensional%20vectors.%20Self-Supervised%20Learning%20%28SSL%29%20methods%20are%20widely%20used%0Ain%20GRL%20because%20they%20can%20avoid%20expensive%20human%20annotation.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20Subgraph%20Gaussian%20Embedding%20Contrast%20%28SubGEC%29%20method.%20Our%0Aapproach%20introduces%20a%20subgraph%20Gaussian%20embedding%20module%2C%20which%20adaptively%20maps%0Asubgraphs%20to%20a%20structured%20Gaussian%20space%2C%20ensuring%20the%20preservation%20of%20input%0Asubgraph%20characteristics%20while%20generating%20subgraphs%20with%20a%20controlled%0Adistribution.%20We%20then%20employ%20optimal%20transport%20distances%2C%20more%20precisely%20the%0AWasserstein%20and%20Gromov-Wasserstein%20distances%2C%20to%20effectively%20measure%20the%0Asimilarity%20between%20subgraphs%2C%20enhancing%20the%20robustness%20of%20the%20contrastive%0Alearning%20process.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%0Athat%20%5Cmethod~outperforms%20or%20presents%20competitive%20performance%20against%0Astate-of-the-art%20approaches.%20Our%20findings%20provide%20insights%20into%20the%20design%20of%0ASSL%20methods%20for%20GRL%2C%20emphasizing%20the%20importance%20of%20the%20distribution%20of%20the%0Agenerated%20contrastive%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubgraph%2520Gaussian%2520Embedding%2520Contrast%2520for%2520Self-Supervised%2520Graph%250A%2520%2520Representation%2520Learning%26entry.906535625%3DShifeng%2520Xie%2520and%2520Aref%2520Einizade%2520and%2520Jhony%2520H.%2520Giraldo%26entry.1292438233%3D%2520%2520Graph%2520Representation%2520Learning%2520%2528GRL%2529%2520is%2520a%2520fundamental%2520task%2520in%2520machine%250Alearning%252C%2520aiming%2520to%2520encode%2520high-dimensional%2520graph-structured%2520data%2520into%250Alow-dimensional%2520vectors.%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520methods%2520are%2520widely%2520used%250Ain%2520GRL%2520because%2520they%2520can%2520avoid%2520expensive%2520human%2520annotation.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520Subgraph%2520Gaussian%2520Embedding%2520Contrast%2520%2528SubGEC%2529%2520method.%2520Our%250Aapproach%2520introduces%2520a%2520subgraph%2520Gaussian%2520embedding%2520module%252C%2520which%2520adaptively%2520maps%250Asubgraphs%2520to%2520a%2520structured%2520Gaussian%2520space%252C%2520ensuring%2520the%2520preservation%2520of%2520input%250Asubgraph%2520characteristics%2520while%2520generating%2520subgraphs%2520with%2520a%2520controlled%250Adistribution.%2520We%2520then%2520employ%2520optimal%2520transport%2520distances%252C%2520more%2520precisely%2520the%250AWasserstein%2520and%2520Gromov-Wasserstein%2520distances%252C%2520to%2520effectively%2520measure%2520the%250Asimilarity%2520between%2520subgraphs%252C%2520enhancing%2520the%2520robustness%2520of%2520the%2520contrastive%250Alearning%2520process.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%250Athat%2520%255Cmethod~outperforms%2520or%2520presents%2520competitive%2520performance%2520against%250Astate-of-the-art%2520approaches.%2520Our%2520findings%2520provide%2520insights%2520into%2520the%2520design%2520of%250ASSL%2520methods%2520for%2520GRL%252C%2520emphasizing%2520the%2520importance%2520of%2520the%2520distribution%2520of%2520the%250Agenerated%2520contrastive%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subgraph%20Gaussian%20Embedding%20Contrast%20for%20Self-Supervised%20Graph%0A%20%20Representation%20Learning&entry.906535625=Shifeng%20Xie%20and%20Aref%20Einizade%20and%20Jhony%20H.%20Giraldo&entry.1292438233=%20%20Graph%20Representation%20Learning%20%28GRL%29%20is%20a%20fundamental%20task%20in%20machine%0Alearning%2C%20aiming%20to%20encode%20high-dimensional%20graph-structured%20data%20into%0Alow-dimensional%20vectors.%20Self-Supervised%20Learning%20%28SSL%29%20methods%20are%20widely%20used%0Ain%20GRL%20because%20they%20can%20avoid%20expensive%20human%20annotation.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20Subgraph%20Gaussian%20Embedding%20Contrast%20%28SubGEC%29%20method.%20Our%0Aapproach%20introduces%20a%20subgraph%20Gaussian%20embedding%20module%2C%20which%20adaptively%20maps%0Asubgraphs%20to%20a%20structured%20Gaussian%20space%2C%20ensuring%20the%20preservation%20of%20input%0Asubgraph%20characteristics%20while%20generating%20subgraphs%20with%20a%20controlled%0Adistribution.%20We%20then%20employ%20optimal%20transport%20distances%2C%20more%20precisely%20the%0AWasserstein%20and%20Gromov-Wasserstein%20distances%2C%20to%20effectively%20measure%20the%0Asimilarity%20between%20subgraphs%2C%20enhancing%20the%20robustness%20of%20the%20contrastive%0Alearning%20process.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%0Athat%20%5Cmethod~outperforms%20or%20presents%20competitive%20performance%20against%0Astate-of-the-art%20approaches.%20Our%20findings%20provide%20insights%20into%20the%20design%20of%0ASSL%20methods%20for%20GRL%2C%20emphasizing%20the%20importance%20of%20the%20distribution%20of%20the%0Agenerated%20contrastive%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23529v1&entry.124074799=Read"},
{"title": "CVOCSemRPL: Class-Variance Optimized Clustering, Semantic Information\n  Injection and Restricted Pseudo Labeling based Improved Semi-Supervised\n  Few-Shot Learning", "author": "Souvik Maji and Rhythm Baghel and Pratik Mazumder", "abstract": "  Few-shot learning has been extensively explored to address problems where the\namount of labeled samples is very limited for some classes. In the\nsemi-supervised few-shot learning setting, substantial quantities of unlabeled\nsamples are available. Such unlabeled samples are generally cheaper to obtain\nand can be used to improve the few-shot learning performance of the model. Some\nof the recent methods for this setting rely on clustering to generate\npseudo-labels for the unlabeled samples. Since the effectiveness of clustering\nheavily influences the labeling of the unlabeled samples, it can significantly\naffect the few-shot learning performance. In this paper, we focus on improving\nthe representation learned by the model in order to improve the clustering and,\nconsequently, the model performance. We propose an approach for semi-supervised\nfew-shot learning that performs a class-variance optimized clustering coupled\nwith a cluster separation tuner in order to improve the effectiveness of\nclustering the labeled and unlabeled samples in this setting. It also optimizes\nthe clustering-based pseudo-labeling process using a restricted pseudo-labeling\napproach and performs semantic information injection in order to improve the\nsemi-supervised few-shot learning performance of the model. We experimentally\ndemonstrate that our proposed approach significantly outperforms recent\nstate-of-the-art methods on the benchmark datasets.\n", "link": "http://arxiv.org/abs/2501.14401v2", "date": "2025-05-29", "relevancy": 2.6176, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.56}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CVOCSemRPL%3A%20Class-Variance%20Optimized%20Clustering%2C%20Semantic%20Information%0A%20%20Injection%20and%20Restricted%20Pseudo%20Labeling%20based%20Improved%20Semi-Supervised%0A%20%20Few-Shot%20Learning&body=Title%3A%20CVOCSemRPL%3A%20Class-Variance%20Optimized%20Clustering%2C%20Semantic%20Information%0A%20%20Injection%20and%20Restricted%20Pseudo%20Labeling%20based%20Improved%20Semi-Supervised%0A%20%20Few-Shot%20Learning%0AAuthor%3A%20Souvik%20Maji%20and%20Rhythm%20Baghel%20and%20Pratik%20Mazumder%0AAbstract%3A%20%20%20Few-shot%20learning%20has%20been%20extensively%20explored%20to%20address%20problems%20where%20the%0Aamount%20of%20labeled%20samples%20is%20very%20limited%20for%20some%20classes.%20In%20the%0Asemi-supervised%20few-shot%20learning%20setting%2C%20substantial%20quantities%20of%20unlabeled%0Asamples%20are%20available.%20Such%20unlabeled%20samples%20are%20generally%20cheaper%20to%20obtain%0Aand%20can%20be%20used%20to%20improve%20the%20few-shot%20learning%20performance%20of%20the%20model.%20Some%0Aof%20the%20recent%20methods%20for%20this%20setting%20rely%20on%20clustering%20to%20generate%0Apseudo-labels%20for%20the%20unlabeled%20samples.%20Since%20the%20effectiveness%20of%20clustering%0Aheavily%20influences%20the%20labeling%20of%20the%20unlabeled%20samples%2C%20it%20can%20significantly%0Aaffect%20the%20few-shot%20learning%20performance.%20In%20this%20paper%2C%20we%20focus%20on%20improving%0Athe%20representation%20learned%20by%20the%20model%20in%20order%20to%20improve%20the%20clustering%20and%2C%0Aconsequently%2C%20the%20model%20performance.%20We%20propose%20an%20approach%20for%20semi-supervised%0Afew-shot%20learning%20that%20performs%20a%20class-variance%20optimized%20clustering%20coupled%0Awith%20a%20cluster%20separation%20tuner%20in%20order%20to%20improve%20the%20effectiveness%20of%0Aclustering%20the%20labeled%20and%20unlabeled%20samples%20in%20this%20setting.%20It%20also%20optimizes%0Athe%20clustering-based%20pseudo-labeling%20process%20using%20a%20restricted%20pseudo-labeling%0Aapproach%20and%20performs%20semantic%20information%20injection%20in%20order%20to%20improve%20the%0Asemi-supervised%20few-shot%20learning%20performance%20of%20the%20model.%20We%20experimentally%0Ademonstrate%20that%20our%20proposed%20approach%20significantly%20outperforms%20recent%0Astate-of-the-art%20methods%20on%20the%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14401v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCVOCSemRPL%253A%2520Class-Variance%2520Optimized%2520Clustering%252C%2520Semantic%2520Information%250A%2520%2520Injection%2520and%2520Restricted%2520Pseudo%2520Labeling%2520based%2520Improved%2520Semi-Supervised%250A%2520%2520Few-Shot%2520Learning%26entry.906535625%3DSouvik%2520Maji%2520and%2520Rhythm%2520Baghel%2520and%2520Pratik%2520Mazumder%26entry.1292438233%3D%2520%2520Few-shot%2520learning%2520has%2520been%2520extensively%2520explored%2520to%2520address%2520problems%2520where%2520the%250Aamount%2520of%2520labeled%2520samples%2520is%2520very%2520limited%2520for%2520some%2520classes.%2520In%2520the%250Asemi-supervised%2520few-shot%2520learning%2520setting%252C%2520substantial%2520quantities%2520of%2520unlabeled%250Asamples%2520are%2520available.%2520Such%2520unlabeled%2520samples%2520are%2520generally%2520cheaper%2520to%2520obtain%250Aand%2520can%2520be%2520used%2520to%2520improve%2520the%2520few-shot%2520learning%2520performance%2520of%2520the%2520model.%2520Some%250Aof%2520the%2520recent%2520methods%2520for%2520this%2520setting%2520rely%2520on%2520clustering%2520to%2520generate%250Apseudo-labels%2520for%2520the%2520unlabeled%2520samples.%2520Since%2520the%2520effectiveness%2520of%2520clustering%250Aheavily%2520influences%2520the%2520labeling%2520of%2520the%2520unlabeled%2520samples%252C%2520it%2520can%2520significantly%250Aaffect%2520the%2520few-shot%2520learning%2520performance.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520improving%250Athe%2520representation%2520learned%2520by%2520the%2520model%2520in%2520order%2520to%2520improve%2520the%2520clustering%2520and%252C%250Aconsequently%252C%2520the%2520model%2520performance.%2520We%2520propose%2520an%2520approach%2520for%2520semi-supervised%250Afew-shot%2520learning%2520that%2520performs%2520a%2520class-variance%2520optimized%2520clustering%2520coupled%250Awith%2520a%2520cluster%2520separation%2520tuner%2520in%2520order%2520to%2520improve%2520the%2520effectiveness%2520of%250Aclustering%2520the%2520labeled%2520and%2520unlabeled%2520samples%2520in%2520this%2520setting.%2520It%2520also%2520optimizes%250Athe%2520clustering-based%2520pseudo-labeling%2520process%2520using%2520a%2520restricted%2520pseudo-labeling%250Aapproach%2520and%2520performs%2520semantic%2520information%2520injection%2520in%2520order%2520to%2520improve%2520the%250Asemi-supervised%2520few-shot%2520learning%2520performance%2520of%2520the%2520model.%2520We%2520experimentally%250Ademonstrate%2520that%2520our%2520proposed%2520approach%2520significantly%2520outperforms%2520recent%250Astate-of-the-art%2520methods%2520on%2520the%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14401v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CVOCSemRPL%3A%20Class-Variance%20Optimized%20Clustering%2C%20Semantic%20Information%0A%20%20Injection%20and%20Restricted%20Pseudo%20Labeling%20based%20Improved%20Semi-Supervised%0A%20%20Few-Shot%20Learning&entry.906535625=Souvik%20Maji%20and%20Rhythm%20Baghel%20and%20Pratik%20Mazumder&entry.1292438233=%20%20Few-shot%20learning%20has%20been%20extensively%20explored%20to%20address%20problems%20where%20the%0Aamount%20of%20labeled%20samples%20is%20very%20limited%20for%20some%20classes.%20In%20the%0Asemi-supervised%20few-shot%20learning%20setting%2C%20substantial%20quantities%20of%20unlabeled%0Asamples%20are%20available.%20Such%20unlabeled%20samples%20are%20generally%20cheaper%20to%20obtain%0Aand%20can%20be%20used%20to%20improve%20the%20few-shot%20learning%20performance%20of%20the%20model.%20Some%0Aof%20the%20recent%20methods%20for%20this%20setting%20rely%20on%20clustering%20to%20generate%0Apseudo-labels%20for%20the%20unlabeled%20samples.%20Since%20the%20effectiveness%20of%20clustering%0Aheavily%20influences%20the%20labeling%20of%20the%20unlabeled%20samples%2C%20it%20can%20significantly%0Aaffect%20the%20few-shot%20learning%20performance.%20In%20this%20paper%2C%20we%20focus%20on%20improving%0Athe%20representation%20learned%20by%20the%20model%20in%20order%20to%20improve%20the%20clustering%20and%2C%0Aconsequently%2C%20the%20model%20performance.%20We%20propose%20an%20approach%20for%20semi-supervised%0Afew-shot%20learning%20that%20performs%20a%20class-variance%20optimized%20clustering%20coupled%0Awith%20a%20cluster%20separation%20tuner%20in%20order%20to%20improve%20the%20effectiveness%20of%0Aclustering%20the%20labeled%20and%20unlabeled%20samples%20in%20this%20setting.%20It%20also%20optimizes%0Athe%20clustering-based%20pseudo-labeling%20process%20using%20a%20restricted%20pseudo-labeling%0Aapproach%20and%20performs%20semantic%20information%20injection%20in%20order%20to%20improve%20the%0Asemi-supervised%20few-shot%20learning%20performance%20of%20the%20model.%20We%20experimentally%0Ademonstrate%20that%20our%20proposed%20approach%20significantly%20outperforms%20recent%0Astate-of-the-art%20methods%20on%20the%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14401v2&entry.124074799=Read"},
{"title": "SPRI: Aligning Large Language Models with Context-Situated Principles", "author": "Hongli Zhan and Muneeza Azmat and Raya Horesh and Junyi Jessy Li and Mikhail Yurochkin", "abstract": "  Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https://github.com/honglizhan/SPRI-public.\n", "link": "http://arxiv.org/abs/2502.03397v2", "date": "2025-05-29", "relevancy": 2.6161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPRI%3A%20Aligning%20Large%20Language%20Models%20with%20Context-Situated%20Principles&body=Title%3A%20SPRI%3A%20Aligning%20Large%20Language%20Models%20with%20Context-Situated%20Principles%0AAuthor%3A%20Hongli%20Zhan%20and%20Muneeza%20Azmat%20and%20Raya%20Horesh%20and%20Junyi%20Jessy%20Li%20and%20Mikhail%20Yurochkin%0AAbstract%3A%20%20%20Aligning%20Large%20Language%20Models%20to%20integrate%20and%20reflect%20human%20values%2C%0Aespecially%20for%20tasks%20that%20demand%20intricate%20human%20oversight%2C%20is%20arduous%20since%20it%0Ais%20resource-intensive%20and%20time-consuming%20to%20depend%20on%20human%20expertise%20for%0Acontext-specific%20guidance.%20Prior%20work%20has%20utilized%20predefined%20sets%20of%20rules%20or%0Aprinciples%20to%20steer%20the%20behavior%20of%20models%20%28Bai%20et%20al.%2C%202022%3B%20Sun%20et%20al.%2C%0A2023%29.%20However%2C%20these%20principles%20tend%20to%20be%20generic%2C%20making%20it%20challenging%20to%0Aadapt%20them%20to%20each%20individual%20input%20query%20or%20context.%20In%20this%20work%2C%20we%20present%0ASituated-PRInciples%20%28SPRI%29%2C%20a%20framework%20requiring%20minimal%20or%20no%20human%20effort%0Athat%20is%20designed%20to%20automatically%20generate%20guiding%20principles%20in%20real-time%20for%0Aeach%20input%20query%20and%20utilize%20them%20to%20align%20each%20response.%20We%20evaluate%20SPRI%20on%0Athree%20tasks%2C%20and%20show%20that%201%29%20SPRI%20can%20derive%20principles%20in%20a%20complex%0Adomain-specific%20task%20that%20leads%20to%20on-par%20performance%20as%20expert-crafted%20ones%3B%0A2%29%20SPRI-generated%20principles%20lead%20to%20instance-specific%20rubrics%20that%20outperform%0Aprior%20LLM-as-a-judge%20frameworks%3B%203%29%20using%20SPRI%20to%20generate%20synthetic%20SFT%20data%0Aleads%20to%20substantial%20improvement%20on%20truthfulness.%20We%20release%20our%20code%20and%20model%0Agenerations%20at%20https%3A//github.com/honglizhan/SPRI-public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPRI%253A%2520Aligning%2520Large%2520Language%2520Models%2520with%2520Context-Situated%2520Principles%26entry.906535625%3DHongli%2520Zhan%2520and%2520Muneeza%2520Azmat%2520and%2520Raya%2520Horesh%2520and%2520Junyi%2520Jessy%2520Li%2520and%2520Mikhail%2520Yurochkin%26entry.1292438233%3D%2520%2520Aligning%2520Large%2520Language%2520Models%2520to%2520integrate%2520and%2520reflect%2520human%2520values%252C%250Aespecially%2520for%2520tasks%2520that%2520demand%2520intricate%2520human%2520oversight%252C%2520is%2520arduous%2520since%2520it%250Ais%2520resource-intensive%2520and%2520time-consuming%2520to%2520depend%2520on%2520human%2520expertise%2520for%250Acontext-specific%2520guidance.%2520Prior%2520work%2520has%2520utilized%2520predefined%2520sets%2520of%2520rules%2520or%250Aprinciples%2520to%2520steer%2520the%2520behavior%2520of%2520models%2520%2528Bai%2520et%2520al.%252C%25202022%253B%2520Sun%2520et%2520al.%252C%250A2023%2529.%2520However%252C%2520these%2520principles%2520tend%2520to%2520be%2520generic%252C%2520making%2520it%2520challenging%2520to%250Aadapt%2520them%2520to%2520each%2520individual%2520input%2520query%2520or%2520context.%2520In%2520this%2520work%252C%2520we%2520present%250ASituated-PRInciples%2520%2528SPRI%2529%252C%2520a%2520framework%2520requiring%2520minimal%2520or%2520no%2520human%2520effort%250Athat%2520is%2520designed%2520to%2520automatically%2520generate%2520guiding%2520principles%2520in%2520real-time%2520for%250Aeach%2520input%2520query%2520and%2520utilize%2520them%2520to%2520align%2520each%2520response.%2520We%2520evaluate%2520SPRI%2520on%250Athree%2520tasks%252C%2520and%2520show%2520that%25201%2529%2520SPRI%2520can%2520derive%2520principles%2520in%2520a%2520complex%250Adomain-specific%2520task%2520that%2520leads%2520to%2520on-par%2520performance%2520as%2520expert-crafted%2520ones%253B%250A2%2529%2520SPRI-generated%2520principles%2520lead%2520to%2520instance-specific%2520rubrics%2520that%2520outperform%250Aprior%2520LLM-as-a-judge%2520frameworks%253B%25203%2529%2520using%2520SPRI%2520to%2520generate%2520synthetic%2520SFT%2520data%250Aleads%2520to%2520substantial%2520improvement%2520on%2520truthfulness.%2520We%2520release%2520our%2520code%2520and%2520model%250Agenerations%2520at%2520https%253A//github.com/honglizhan/SPRI-public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPRI%3A%20Aligning%20Large%20Language%20Models%20with%20Context-Situated%20Principles&entry.906535625=Hongli%20Zhan%20and%20Muneeza%20Azmat%20and%20Raya%20Horesh%20and%20Junyi%20Jessy%20Li%20and%20Mikhail%20Yurochkin&entry.1292438233=%20%20Aligning%20Large%20Language%20Models%20to%20integrate%20and%20reflect%20human%20values%2C%0Aespecially%20for%20tasks%20that%20demand%20intricate%20human%20oversight%2C%20is%20arduous%20since%20it%0Ais%20resource-intensive%20and%20time-consuming%20to%20depend%20on%20human%20expertise%20for%0Acontext-specific%20guidance.%20Prior%20work%20has%20utilized%20predefined%20sets%20of%20rules%20or%0Aprinciples%20to%20steer%20the%20behavior%20of%20models%20%28Bai%20et%20al.%2C%202022%3B%20Sun%20et%20al.%2C%0A2023%29.%20However%2C%20these%20principles%20tend%20to%20be%20generic%2C%20making%20it%20challenging%20to%0Aadapt%20them%20to%20each%20individual%20input%20query%20or%20context.%20In%20this%20work%2C%20we%20present%0ASituated-PRInciples%20%28SPRI%29%2C%20a%20framework%20requiring%20minimal%20or%20no%20human%20effort%0Athat%20is%20designed%20to%20automatically%20generate%20guiding%20principles%20in%20real-time%20for%0Aeach%20input%20query%20and%20utilize%20them%20to%20align%20each%20response.%20We%20evaluate%20SPRI%20on%0Athree%20tasks%2C%20and%20show%20that%201%29%20SPRI%20can%20derive%20principles%20in%20a%20complex%0Adomain-specific%20task%20that%20leads%20to%20on-par%20performance%20as%20expert-crafted%20ones%3B%0A2%29%20SPRI-generated%20principles%20lead%20to%20instance-specific%20rubrics%20that%20outperform%0Aprior%20LLM-as-a-judge%20frameworks%3B%203%29%20using%20SPRI%20to%20generate%20synthetic%20SFT%20data%0Aleads%20to%20substantial%20improvement%20on%20truthfulness.%20We%20release%20our%20code%20and%20model%0Agenerations%20at%20https%3A//github.com/honglizhan/SPRI-public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03397v2&entry.124074799=Read"},
{"title": "SafeCFG: Controlling Harmful Features with Dynamic Safe Guidance for\n  Safe Generation", "author": "Jiadong Pan and Liang Li and Hongcheng Gao and Zheng-Jun Zha and Qingming Huang and Jiebo Luo", "abstract": "  Diffusion models (DMs) have demonstrated exceptional performance in\ntext-to-image tasks, leading to their widespread use. With the introduction of\nclassifier-free guidance (CFG), the quality of images generated by DMs is\nsignificantly improved. However, one can use DMs to generate more harmful\nimages by maliciously guiding the image generation process through CFG.\nExisting safe alignment methods aim to mitigate the risk of generating harmful\nimages but often reduce the quality of clean image generation. To address this\nissue, we propose SafeCFG to adaptively control harmful features with dynamic\nsafe guidance by modulating the CFG generation process. It dynamically guides\nthe CFG generation process based on the harmfulness of the prompts, inducing\nsignificant deviations only in harmful CFG generations, achieving high quality\nand safety generation. SafeCFG can simultaneously modulate different harmful\nCFG generation processes, so it could eliminate harmful elements while\npreserving high-quality generation. Additionally, SafeCFG provides the ability\nto detect image harmfulness, allowing unsupervised safe alignment on DMs\nwithout pre-defined clean or harmful labels. Experimental results show that\nimages generated by SafeCFG achieve both high quality and safety, and safe DMs\ntrained in our unsupervised manner also exhibit good safety performance.\n", "link": "http://arxiv.org/abs/2412.16039v2", "date": "2025-05-29", "relevancy": 2.6043, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.536}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5136}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeCFG%3A%20Controlling%20Harmful%20Features%20with%20Dynamic%20Safe%20Guidance%20for%0A%20%20Safe%20Generation&body=Title%3A%20SafeCFG%3A%20Controlling%20Harmful%20Features%20with%20Dynamic%20Safe%20Guidance%20for%0A%20%20Safe%20Generation%0AAuthor%3A%20Jiadong%20Pan%20and%20Liang%20Li%20and%20Hongcheng%20Gao%20and%20Zheng-Jun%20Zha%20and%20Qingming%20Huang%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20demonstrated%20exceptional%20performance%20in%0Atext-to-image%20tasks%2C%20leading%20to%20their%20widespread%20use.%20With%20the%20introduction%20of%0Aclassifier-free%20guidance%20%28CFG%29%2C%20the%20quality%20of%20images%20generated%20by%20DMs%20is%0Asignificantly%20improved.%20However%2C%20one%20can%20use%20DMs%20to%20generate%20more%20harmful%0Aimages%20by%20maliciously%20guiding%20the%20image%20generation%20process%20through%20CFG.%0AExisting%20safe%20alignment%20methods%20aim%20to%20mitigate%20the%20risk%20of%20generating%20harmful%0Aimages%20but%20often%20reduce%20the%20quality%20of%20clean%20image%20generation.%20To%20address%20this%0Aissue%2C%20we%20propose%20SafeCFG%20to%20adaptively%20control%20harmful%20features%20with%20dynamic%0Asafe%20guidance%20by%20modulating%20the%20CFG%20generation%20process.%20It%20dynamically%20guides%0Athe%20CFG%20generation%20process%20based%20on%20the%20harmfulness%20of%20the%20prompts%2C%20inducing%0Asignificant%20deviations%20only%20in%20harmful%20CFG%20generations%2C%20achieving%20high%20quality%0Aand%20safety%20generation.%20SafeCFG%20can%20simultaneously%20modulate%20different%20harmful%0ACFG%20generation%20processes%2C%20so%20it%20could%20eliminate%20harmful%20elements%20while%0Apreserving%20high-quality%20generation.%20Additionally%2C%20SafeCFG%20provides%20the%20ability%0Ato%20detect%20image%20harmfulness%2C%20allowing%20unsupervised%20safe%20alignment%20on%20DMs%0Awithout%20pre-defined%20clean%20or%20harmful%20labels.%20Experimental%20results%20show%20that%0Aimages%20generated%20by%20SafeCFG%20achieve%20both%20high%20quality%20and%20safety%2C%20and%20safe%20DMs%0Atrained%20in%20our%20unsupervised%20manner%20also%20exhibit%20good%20safety%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeCFG%253A%2520Controlling%2520Harmful%2520Features%2520with%2520Dynamic%2520Safe%2520Guidance%2520for%250A%2520%2520Safe%2520Generation%26entry.906535625%3DJiadong%2520Pan%2520and%2520Liang%2520Li%2520and%2520Hongcheng%2520Gao%2520and%2520Zheng-Jun%2520Zha%2520and%2520Qingming%2520Huang%2520and%2520Jiebo%2520Luo%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520demonstrated%2520exceptional%2520performance%2520in%250Atext-to-image%2520tasks%252C%2520leading%2520to%2520their%2520widespread%2520use.%2520With%2520the%2520introduction%2520of%250Aclassifier-free%2520guidance%2520%2528CFG%2529%252C%2520the%2520quality%2520of%2520images%2520generated%2520by%2520DMs%2520is%250Asignificantly%2520improved.%2520However%252C%2520one%2520can%2520use%2520DMs%2520to%2520generate%2520more%2520harmful%250Aimages%2520by%2520maliciously%2520guiding%2520the%2520image%2520generation%2520process%2520through%2520CFG.%250AExisting%2520safe%2520alignment%2520methods%2520aim%2520to%2520mitigate%2520the%2520risk%2520of%2520generating%2520harmful%250Aimages%2520but%2520often%2520reduce%2520the%2520quality%2520of%2520clean%2520image%2520generation.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520SafeCFG%2520to%2520adaptively%2520control%2520harmful%2520features%2520with%2520dynamic%250Asafe%2520guidance%2520by%2520modulating%2520the%2520CFG%2520generation%2520process.%2520It%2520dynamically%2520guides%250Athe%2520CFG%2520generation%2520process%2520based%2520on%2520the%2520harmfulness%2520of%2520the%2520prompts%252C%2520inducing%250Asignificant%2520deviations%2520only%2520in%2520harmful%2520CFG%2520generations%252C%2520achieving%2520high%2520quality%250Aand%2520safety%2520generation.%2520SafeCFG%2520can%2520simultaneously%2520modulate%2520different%2520harmful%250ACFG%2520generation%2520processes%252C%2520so%2520it%2520could%2520eliminate%2520harmful%2520elements%2520while%250Apreserving%2520high-quality%2520generation.%2520Additionally%252C%2520SafeCFG%2520provides%2520the%2520ability%250Ato%2520detect%2520image%2520harmfulness%252C%2520allowing%2520unsupervised%2520safe%2520alignment%2520on%2520DMs%250Awithout%2520pre-defined%2520clean%2520or%2520harmful%2520labels.%2520Experimental%2520results%2520show%2520that%250Aimages%2520generated%2520by%2520SafeCFG%2520achieve%2520both%2520high%2520quality%2520and%2520safety%252C%2520and%2520safe%2520DMs%250Atrained%2520in%2520our%2520unsupervised%2520manner%2520also%2520exhibit%2520good%2520safety%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeCFG%3A%20Controlling%20Harmful%20Features%20with%20Dynamic%20Safe%20Guidance%20for%0A%20%20Safe%20Generation&entry.906535625=Jiadong%20Pan%20and%20Liang%20Li%20and%20Hongcheng%20Gao%20and%20Zheng-Jun%20Zha%20and%20Qingming%20Huang%20and%20Jiebo%20Luo&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20demonstrated%20exceptional%20performance%20in%0Atext-to-image%20tasks%2C%20leading%20to%20their%20widespread%20use.%20With%20the%20introduction%20of%0Aclassifier-free%20guidance%20%28CFG%29%2C%20the%20quality%20of%20images%20generated%20by%20DMs%20is%0Asignificantly%20improved.%20However%2C%20one%20can%20use%20DMs%20to%20generate%20more%20harmful%0Aimages%20by%20maliciously%20guiding%20the%20image%20generation%20process%20through%20CFG.%0AExisting%20safe%20alignment%20methods%20aim%20to%20mitigate%20the%20risk%20of%20generating%20harmful%0Aimages%20but%20often%20reduce%20the%20quality%20of%20clean%20image%20generation.%20To%20address%20this%0Aissue%2C%20we%20propose%20SafeCFG%20to%20adaptively%20control%20harmful%20features%20with%20dynamic%0Asafe%20guidance%20by%20modulating%20the%20CFG%20generation%20process.%20It%20dynamically%20guides%0Athe%20CFG%20generation%20process%20based%20on%20the%20harmfulness%20of%20the%20prompts%2C%20inducing%0Asignificant%20deviations%20only%20in%20harmful%20CFG%20generations%2C%20achieving%20high%20quality%0Aand%20safety%20generation.%20SafeCFG%20can%20simultaneously%20modulate%20different%20harmful%0ACFG%20generation%20processes%2C%20so%20it%20could%20eliminate%20harmful%20elements%20while%0Apreserving%20high-quality%20generation.%20Additionally%2C%20SafeCFG%20provides%20the%20ability%0Ato%20detect%20image%20harmfulness%2C%20allowing%20unsupervised%20safe%20alignment%20on%20DMs%0Awithout%20pre-defined%20clean%20or%20harmful%20labels.%20Experimental%20results%20show%20that%0Aimages%20generated%20by%20SafeCFG%20achieve%20both%20high%20quality%20and%20safety%2C%20and%20safe%20DMs%0Atrained%20in%20our%20unsupervised%20manner%20also%20exhibit%20good%20safety%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16039v2&entry.124074799=Read"},
{"title": "Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference\n  Optimization and Temporal Motion Modulation", "author": "Jiahao Cui and Yan Chen and Mingwang Xu and Hanlin Shang and Yuxuan Chen and Yun Zhan and Zilong Dong and Yao Yao and Jingdong Wang and Siyu Zhu", "abstract": "  Generating highly dynamic and photorealistic portrait animations driven by\naudio and skeletal motion remains challenging due to the need for precise lip\nsynchronization, natural facial expressions, and high-fidelity body motion\ndynamics. We propose a human-preference-aligned diffusion framework that\naddresses these challenges through two key innovations. First, we introduce\ndirect preference optimization tailored for human-centric animation, leveraging\na curated dataset of human preferences to align generated outputs with\nperceptual metrics for portrait motion-video alignment and naturalness of\nexpression. Second, the proposed temporal motion modulation resolves\nspatiotemporal resolution mismatches by reshaping motion conditions into\ndimensionally aligned latent features through temporal channel redistribution\nand proportional feature expansion, preserving the fidelity of high-frequency\nmotion details in diffusion-based synthesis. The proposed mechanism is\ncomplementary to existing UNet and DiT-based portrait diffusion approaches, and\nexperiments demonstrate obvious improvements in lip-audio synchronization,\nexpression vividness, body motion coherence over baseline methods, alongside\nnotable gains in human preference metrics. Our model and source code can be\nfound at: https://github.com/xyz123xyz456/hallo4.\n", "link": "http://arxiv.org/abs/2505.23525v1", "date": "2025-05-29", "relevancy": 2.6006, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7016}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6199}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallo4%3A%20High-Fidelity%20Dynamic%20Portrait%20Animation%20via%20Direct%20Preference%0A%20%20Optimization%20and%20Temporal%20Motion%20Modulation&body=Title%3A%20Hallo4%3A%20High-Fidelity%20Dynamic%20Portrait%20Animation%20via%20Direct%20Preference%0A%20%20Optimization%20and%20Temporal%20Motion%20Modulation%0AAuthor%3A%20Jiahao%20Cui%20and%20Yan%20Chen%20and%20Mingwang%20Xu%20and%20Hanlin%20Shang%20and%20Yuxuan%20Chen%20and%20Yun%20Zhan%20and%20Zilong%20Dong%20and%20Yao%20Yao%20and%20Jingdong%20Wang%20and%20Siyu%20Zhu%0AAbstract%3A%20%20%20Generating%20highly%20dynamic%20and%20photorealistic%20portrait%20animations%20driven%20by%0Aaudio%20and%20skeletal%20motion%20remains%20challenging%20due%20to%20the%20need%20for%20precise%20lip%0Asynchronization%2C%20natural%20facial%20expressions%2C%20and%20high-fidelity%20body%20motion%0Adynamics.%20We%20propose%20a%20human-preference-aligned%20diffusion%20framework%20that%0Aaddresses%20these%20challenges%20through%20two%20key%20innovations.%20First%2C%20we%20introduce%0Adirect%20preference%20optimization%20tailored%20for%20human-centric%20animation%2C%20leveraging%0Aa%20curated%20dataset%20of%20human%20preferences%20to%20align%20generated%20outputs%20with%0Aperceptual%20metrics%20for%20portrait%20motion-video%20alignment%20and%20naturalness%20of%0Aexpression.%20Second%2C%20the%20proposed%20temporal%20motion%20modulation%20resolves%0Aspatiotemporal%20resolution%20mismatches%20by%20reshaping%20motion%20conditions%20into%0Adimensionally%20aligned%20latent%20features%20through%20temporal%20channel%20redistribution%0Aand%20proportional%20feature%20expansion%2C%20preserving%20the%20fidelity%20of%20high-frequency%0Amotion%20details%20in%20diffusion-based%20synthesis.%20The%20proposed%20mechanism%20is%0Acomplementary%20to%20existing%20UNet%20and%20DiT-based%20portrait%20diffusion%20approaches%2C%20and%0Aexperiments%20demonstrate%20obvious%20improvements%20in%20lip-audio%20synchronization%2C%0Aexpression%20vividness%2C%20body%20motion%20coherence%20over%20baseline%20methods%2C%20alongside%0Anotable%20gains%20in%20human%20preference%20metrics.%20Our%20model%20and%20source%20code%20can%20be%0Afound%20at%3A%20https%3A//github.com/xyz123xyz456/hallo4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallo4%253A%2520High-Fidelity%2520Dynamic%2520Portrait%2520Animation%2520via%2520Direct%2520Preference%250A%2520%2520Optimization%2520and%2520Temporal%2520Motion%2520Modulation%26entry.906535625%3DJiahao%2520Cui%2520and%2520Yan%2520Chen%2520and%2520Mingwang%2520Xu%2520and%2520Hanlin%2520Shang%2520and%2520Yuxuan%2520Chen%2520and%2520Yun%2520Zhan%2520and%2520Zilong%2520Dong%2520and%2520Yao%2520Yao%2520and%2520Jingdong%2520Wang%2520and%2520Siyu%2520Zhu%26entry.1292438233%3D%2520%2520Generating%2520highly%2520dynamic%2520and%2520photorealistic%2520portrait%2520animations%2520driven%2520by%250Aaudio%2520and%2520skeletal%2520motion%2520remains%2520challenging%2520due%2520to%2520the%2520need%2520for%2520precise%2520lip%250Asynchronization%252C%2520natural%2520facial%2520expressions%252C%2520and%2520high-fidelity%2520body%2520motion%250Adynamics.%2520We%2520propose%2520a%2520human-preference-aligned%2520diffusion%2520framework%2520that%250Aaddresses%2520these%2520challenges%2520through%2520two%2520key%2520innovations.%2520First%252C%2520we%2520introduce%250Adirect%2520preference%2520optimization%2520tailored%2520for%2520human-centric%2520animation%252C%2520leveraging%250Aa%2520curated%2520dataset%2520of%2520human%2520preferences%2520to%2520align%2520generated%2520outputs%2520with%250Aperceptual%2520metrics%2520for%2520portrait%2520motion-video%2520alignment%2520and%2520naturalness%2520of%250Aexpression.%2520Second%252C%2520the%2520proposed%2520temporal%2520motion%2520modulation%2520resolves%250Aspatiotemporal%2520resolution%2520mismatches%2520by%2520reshaping%2520motion%2520conditions%2520into%250Adimensionally%2520aligned%2520latent%2520features%2520through%2520temporal%2520channel%2520redistribution%250Aand%2520proportional%2520feature%2520expansion%252C%2520preserving%2520the%2520fidelity%2520of%2520high-frequency%250Amotion%2520details%2520in%2520diffusion-based%2520synthesis.%2520The%2520proposed%2520mechanism%2520is%250Acomplementary%2520to%2520existing%2520UNet%2520and%2520DiT-based%2520portrait%2520diffusion%2520approaches%252C%2520and%250Aexperiments%2520demonstrate%2520obvious%2520improvements%2520in%2520lip-audio%2520synchronization%252C%250Aexpression%2520vividness%252C%2520body%2520motion%2520coherence%2520over%2520baseline%2520methods%252C%2520alongside%250Anotable%2520gains%2520in%2520human%2520preference%2520metrics.%2520Our%2520model%2520and%2520source%2520code%2520can%2520be%250Afound%2520at%253A%2520https%253A//github.com/xyz123xyz456/hallo4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallo4%3A%20High-Fidelity%20Dynamic%20Portrait%20Animation%20via%20Direct%20Preference%0A%20%20Optimization%20and%20Temporal%20Motion%20Modulation&entry.906535625=Jiahao%20Cui%20and%20Yan%20Chen%20and%20Mingwang%20Xu%20and%20Hanlin%20Shang%20and%20Yuxuan%20Chen%20and%20Yun%20Zhan%20and%20Zilong%20Dong%20and%20Yao%20Yao%20and%20Jingdong%20Wang%20and%20Siyu%20Zhu&entry.1292438233=%20%20Generating%20highly%20dynamic%20and%20photorealistic%20portrait%20animations%20driven%20by%0Aaudio%20and%20skeletal%20motion%20remains%20challenging%20due%20to%20the%20need%20for%20precise%20lip%0Asynchronization%2C%20natural%20facial%20expressions%2C%20and%20high-fidelity%20body%20motion%0Adynamics.%20We%20propose%20a%20human-preference-aligned%20diffusion%20framework%20that%0Aaddresses%20these%20challenges%20through%20two%20key%20innovations.%20First%2C%20we%20introduce%0Adirect%20preference%20optimization%20tailored%20for%20human-centric%20animation%2C%20leveraging%0Aa%20curated%20dataset%20of%20human%20preferences%20to%20align%20generated%20outputs%20with%0Aperceptual%20metrics%20for%20portrait%20motion-video%20alignment%20and%20naturalness%20of%0Aexpression.%20Second%2C%20the%20proposed%20temporal%20motion%20modulation%20resolves%0Aspatiotemporal%20resolution%20mismatches%20by%20reshaping%20motion%20conditions%20into%0Adimensionally%20aligned%20latent%20features%20through%20temporal%20channel%20redistribution%0Aand%20proportional%20feature%20expansion%2C%20preserving%20the%20fidelity%20of%20high-frequency%0Amotion%20details%20in%20diffusion-based%20synthesis.%20The%20proposed%20mechanism%20is%0Acomplementary%20to%20existing%20UNet%20and%20DiT-based%20portrait%20diffusion%20approaches%2C%20and%0Aexperiments%20demonstrate%20obvious%20improvements%20in%20lip-audio%20synchronization%2C%0Aexpression%20vividness%2C%20body%20motion%20coherence%20over%20baseline%20methods%2C%20alongside%0Anotable%20gains%20in%20human%20preference%20metrics.%20Our%20model%20and%20source%20code%20can%20be%0Afound%20at%3A%20https%3A//github.com/xyz123xyz456/hallo4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23525v1&entry.124074799=Read"},
{"title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters\n  is All You Need", "author": "Qiang Wang and Xiang Song and Yuhang He and Jizhou Han and Chenhao Ding and Xinyuan Gao and Yihong Gong", "abstract": "  Deep neural networks (DNNs) often underperform in real-world, dynamic\nsettings where data distributions change over time. Domain Incremental Learning\n(DIL) offers a solution by enabling continual model adaptation, with\nParameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce\nknowledge conflicts. However, existing PIDIL methods struggle with parameter\nselection accuracy, especially as the number of domains and corresponding\nclasses grows. To address this, we propose SOYO, a lightweight framework that\nimproves domain selection in PIDIL. SOYO introduces a Gaussian Mixture\nCompressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior\ndomain data efficiently, while a Multi-level Domain Feature Fusion Network\n(MDFN) enhances domain feature extraction. Our framework supports multiple\nParameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks\nsuch as image classification, object detection, and speech enhancement.\nExperimental results on six benchmarks demonstrate SOYO's consistent\nsuperiority over existing baselines, showcasing its robustness and adaptability\nin complex, evolving environments. The codes will be released in\nhttps://github.com/qwangcv/SOYO.\n", "link": "http://arxiv.org/abs/2505.23744v1", "date": "2025-05-29", "relevancy": 2.5998, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5692}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4987}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Domain%20Incremental%20Learning%3A%20Selecting%20the%20Optimal%20Parameters%0A%20%20is%20All%20You%20Need&body=Title%3A%20Boosting%20Domain%20Incremental%20Learning%3A%20Selecting%20the%20Optimal%20Parameters%0A%20%20is%20All%20You%20Need%0AAuthor%3A%20Qiang%20Wang%20and%20Xiang%20Song%20and%20Yuhang%20He%20and%20Jizhou%20Han%20and%20Chenhao%20Ding%20and%20Xinyuan%20Gao%20and%20Yihong%20Gong%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20often%20underperform%20in%20real-world%2C%20dynamic%0Asettings%20where%20data%20distributions%20change%20over%20time.%20Domain%20Incremental%20Learning%0A%28DIL%29%20offers%20a%20solution%20by%20enabling%20continual%20model%20adaptation%2C%20with%0AParameter-Isolation%20DIL%20%28PIDIL%29%20emerging%20as%20a%20promising%20paradigm%20to%20reduce%0Aknowledge%20conflicts.%20However%2C%20existing%20PIDIL%20methods%20struggle%20with%20parameter%0Aselection%20accuracy%2C%20especially%20as%20the%20number%20of%20domains%20and%20corresponding%0Aclasses%20grows.%20To%20address%20this%2C%20we%20propose%20SOYO%2C%20a%20lightweight%20framework%20that%0Aimproves%20domain%20selection%20in%20PIDIL.%20SOYO%20introduces%20a%20Gaussian%20Mixture%0ACompressor%20%28GMC%29%20and%20Domain%20Feature%20Resampler%20%28DFR%29%20to%20store%20and%20balance%20prior%0Adomain%20data%20efficiently%2C%20while%20a%20Multi-level%20Domain%20Feature%20Fusion%20Network%0A%28MDFN%29%20enhances%20domain%20feature%20extraction.%20Our%20framework%20supports%20multiple%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20and%20is%20validated%20across%20tasks%0Asuch%20as%20image%20classification%2C%20object%20detection%2C%20and%20speech%20enhancement.%0AExperimental%20results%20on%20six%20benchmarks%20demonstrate%20SOYO%27s%20consistent%0Asuperiority%20over%20existing%20baselines%2C%20showcasing%20its%20robustness%20and%20adaptability%0Ain%20complex%2C%20evolving%20environments.%20The%20codes%20will%20be%20released%20in%0Ahttps%3A//github.com/qwangcv/SOYO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Domain%2520Incremental%2520Learning%253A%2520Selecting%2520the%2520Optimal%2520Parameters%250A%2520%2520is%2520All%2520You%2520Need%26entry.906535625%3DQiang%2520Wang%2520and%2520Xiang%2520Song%2520and%2520Yuhang%2520He%2520and%2520Jizhou%2520Han%2520and%2520Chenhao%2520Ding%2520and%2520Xinyuan%2520Gao%2520and%2520Yihong%2520Gong%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520often%2520underperform%2520in%2520real-world%252C%2520dynamic%250Asettings%2520where%2520data%2520distributions%2520change%2520over%2520time.%2520Domain%2520Incremental%2520Learning%250A%2528DIL%2529%2520offers%2520a%2520solution%2520by%2520enabling%2520continual%2520model%2520adaptation%252C%2520with%250AParameter-Isolation%2520DIL%2520%2528PIDIL%2529%2520emerging%2520as%2520a%2520promising%2520paradigm%2520to%2520reduce%250Aknowledge%2520conflicts.%2520However%252C%2520existing%2520PIDIL%2520methods%2520struggle%2520with%2520parameter%250Aselection%2520accuracy%252C%2520especially%2520as%2520the%2520number%2520of%2520domains%2520and%2520corresponding%250Aclasses%2520grows.%2520To%2520address%2520this%252C%2520we%2520propose%2520SOYO%252C%2520a%2520lightweight%2520framework%2520that%250Aimproves%2520domain%2520selection%2520in%2520PIDIL.%2520SOYO%2520introduces%2520a%2520Gaussian%2520Mixture%250ACompressor%2520%2528GMC%2529%2520and%2520Domain%2520Feature%2520Resampler%2520%2528DFR%2529%2520to%2520store%2520and%2520balance%2520prior%250Adomain%2520data%2520efficiently%252C%2520while%2520a%2520Multi-level%2520Domain%2520Feature%2520Fusion%2520Network%250A%2528MDFN%2529%2520enhances%2520domain%2520feature%2520extraction.%2520Our%2520framework%2520supports%2520multiple%250AParameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520and%2520is%2520validated%2520across%2520tasks%250Asuch%2520as%2520image%2520classification%252C%2520object%2520detection%252C%2520and%2520speech%2520enhancement.%250AExperimental%2520results%2520on%2520six%2520benchmarks%2520demonstrate%2520SOYO%2527s%2520consistent%250Asuperiority%2520over%2520existing%2520baselines%252C%2520showcasing%2520its%2520robustness%2520and%2520adaptability%250Ain%2520complex%252C%2520evolving%2520environments.%2520The%2520codes%2520will%2520be%2520released%2520in%250Ahttps%253A//github.com/qwangcv/SOYO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Domain%20Incremental%20Learning%3A%20Selecting%20the%20Optimal%20Parameters%0A%20%20is%20All%20You%20Need&entry.906535625=Qiang%20Wang%20and%20Xiang%20Song%20and%20Yuhang%20He%20and%20Jizhou%20Han%20and%20Chenhao%20Ding%20and%20Xinyuan%20Gao%20and%20Yihong%20Gong&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20often%20underperform%20in%20real-world%2C%20dynamic%0Asettings%20where%20data%20distributions%20change%20over%20time.%20Domain%20Incremental%20Learning%0A%28DIL%29%20offers%20a%20solution%20by%20enabling%20continual%20model%20adaptation%2C%20with%0AParameter-Isolation%20DIL%20%28PIDIL%29%20emerging%20as%20a%20promising%20paradigm%20to%20reduce%0Aknowledge%20conflicts.%20However%2C%20existing%20PIDIL%20methods%20struggle%20with%20parameter%0Aselection%20accuracy%2C%20especially%20as%20the%20number%20of%20domains%20and%20corresponding%0Aclasses%20grows.%20To%20address%20this%2C%20we%20propose%20SOYO%2C%20a%20lightweight%20framework%20that%0Aimproves%20domain%20selection%20in%20PIDIL.%20SOYO%20introduces%20a%20Gaussian%20Mixture%0ACompressor%20%28GMC%29%20and%20Domain%20Feature%20Resampler%20%28DFR%29%20to%20store%20and%20balance%20prior%0Adomain%20data%20efficiently%2C%20while%20a%20Multi-level%20Domain%20Feature%20Fusion%20Network%0A%28MDFN%29%20enhances%20domain%20feature%20extraction.%20Our%20framework%20supports%20multiple%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20and%20is%20validated%20across%20tasks%0Asuch%20as%20image%20classification%2C%20object%20detection%2C%20and%20speech%20enhancement.%0AExperimental%20results%20on%20six%20benchmarks%20demonstrate%20SOYO%27s%20consistent%0Asuperiority%20over%20existing%20baselines%2C%20showcasing%20its%20robustness%20and%20adaptability%0Ain%20complex%2C%20evolving%20environments.%20The%20codes%20will%20be%20released%20in%0Ahttps%3A//github.com/qwangcv/SOYO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23744v1&entry.124074799=Read"},
{"title": "Dimension-Reduction Attack! Video Generative Models are Experts on\n  Controllable Image Synthesis", "author": "Hengyuan Cao and Yutong Feng and Biao Gong and Yijing Tian and Yunhong Lu and Chuang Liu and Bin Wang", "abstract": "  Video generative models can be regarded as world simulators due to their\nability to capture dynamic, continuous changes inherent in real-world\nenvironments. These models integrate high-dimensional information across\nvisual, temporal, spatial, and causal dimensions, enabling predictions of\nsubjects in various status. A natural and valuable research direction is to\nexplore whether a fully trained video generative model in high-dimensional\nspace can effectively support lower-dimensional tasks such as controllable\nimage generation. In this work, we propose a paradigm for video-to-image\nknowledge compression and task adaptation, termed \\textit{Dimension-Reduction\nAttack} (\\texttt{DRA-Ctrl}), which utilizes the strengths of video models,\nincluding long-range context modeling and flatten full-attention, to perform\nvarious generation tasks. Specially, to address the challenging gap between\ncontinuous video frames and discrete image generation, we introduce a\nmixup-based transition strategy that ensures smooth adaptation. Moreover, we\nredesign the attention structure with a tailored masking mechanism to better\nalign text prompts with image-level control. Experiments across diverse image\ngeneration tasks, such as subject-driven and spatially conditioned generation,\nshow that repurposed video models outperform those trained directly on images.\nThese results highlight the untapped potential of large-scale video generators\nfor broader visual applications. \\texttt{DRA-Ctrl} provides new insights into\nreusing resource-intensive video models and lays foundation for future unified\ngenerative models across visual modalities. The project page is\nhttps://dra-ctrl-2025.github.io/DRA-Ctrl/.\n", "link": "http://arxiv.org/abs/2505.23325v1", "date": "2025-05-29", "relevancy": 2.5938, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6647}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6393}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimension-Reduction%20Attack%21%20Video%20Generative%20Models%20are%20Experts%20on%0A%20%20Controllable%20Image%20Synthesis&body=Title%3A%20Dimension-Reduction%20Attack%21%20Video%20Generative%20Models%20are%20Experts%20on%0A%20%20Controllable%20Image%20Synthesis%0AAuthor%3A%20Hengyuan%20Cao%20and%20Yutong%20Feng%20and%20Biao%20Gong%20and%20Yijing%20Tian%20and%20Yunhong%20Lu%20and%20Chuang%20Liu%20and%20Bin%20Wang%0AAbstract%3A%20%20%20Video%20generative%20models%20can%20be%20regarded%20as%20world%20simulators%20due%20to%20their%0Aability%20to%20capture%20dynamic%2C%20continuous%20changes%20inherent%20in%20real-world%0Aenvironments.%20These%20models%20integrate%20high-dimensional%20information%20across%0Avisual%2C%20temporal%2C%20spatial%2C%20and%20causal%20dimensions%2C%20enabling%20predictions%20of%0Asubjects%20in%20various%20status.%20A%20natural%20and%20valuable%20research%20direction%20is%20to%0Aexplore%20whether%20a%20fully%20trained%20video%20generative%20model%20in%20high-dimensional%0Aspace%20can%20effectively%20support%20lower-dimensional%20tasks%20such%20as%20controllable%0Aimage%20generation.%20In%20this%20work%2C%20we%20propose%20a%20paradigm%20for%20video-to-image%0Aknowledge%20compression%20and%20task%20adaptation%2C%20termed%20%5Ctextit%7BDimension-Reduction%0AAttack%7D%20%28%5Ctexttt%7BDRA-Ctrl%7D%29%2C%20which%20utilizes%20the%20strengths%20of%20video%20models%2C%0Aincluding%20long-range%20context%20modeling%20and%20flatten%20full-attention%2C%20to%20perform%0Avarious%20generation%20tasks.%20Specially%2C%20to%20address%20the%20challenging%20gap%20between%0Acontinuous%20video%20frames%20and%20discrete%20image%20generation%2C%20we%20introduce%20a%0Amixup-based%20transition%20strategy%20that%20ensures%20smooth%20adaptation.%20Moreover%2C%20we%0Aredesign%20the%20attention%20structure%20with%20a%20tailored%20masking%20mechanism%20to%20better%0Aalign%20text%20prompts%20with%20image-level%20control.%20Experiments%20across%20diverse%20image%0Ageneration%20tasks%2C%20such%20as%20subject-driven%20and%20spatially%20conditioned%20generation%2C%0Ashow%20that%20repurposed%20video%20models%20outperform%20those%20trained%20directly%20on%20images.%0AThese%20results%20highlight%20the%20untapped%20potential%20of%20large-scale%20video%20generators%0Afor%20broader%20visual%20applications.%20%5Ctexttt%7BDRA-Ctrl%7D%20provides%20new%20insights%20into%0Areusing%20resource-intensive%20video%20models%20and%20lays%20foundation%20for%20future%20unified%0Agenerative%20models%20across%20visual%20modalities.%20The%20project%20page%20is%0Ahttps%3A//dra-ctrl-2025.github.io/DRA-Ctrl/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimension-Reduction%2520Attack%2521%2520Video%2520Generative%2520Models%2520are%2520Experts%2520on%250A%2520%2520Controllable%2520Image%2520Synthesis%26entry.906535625%3DHengyuan%2520Cao%2520and%2520Yutong%2520Feng%2520and%2520Biao%2520Gong%2520and%2520Yijing%2520Tian%2520and%2520Yunhong%2520Lu%2520and%2520Chuang%2520Liu%2520and%2520Bin%2520Wang%26entry.1292438233%3D%2520%2520Video%2520generative%2520models%2520can%2520be%2520regarded%2520as%2520world%2520simulators%2520due%2520to%2520their%250Aability%2520to%2520capture%2520dynamic%252C%2520continuous%2520changes%2520inherent%2520in%2520real-world%250Aenvironments.%2520These%2520models%2520integrate%2520high-dimensional%2520information%2520across%250Avisual%252C%2520temporal%252C%2520spatial%252C%2520and%2520causal%2520dimensions%252C%2520enabling%2520predictions%2520of%250Asubjects%2520in%2520various%2520status.%2520A%2520natural%2520and%2520valuable%2520research%2520direction%2520is%2520to%250Aexplore%2520whether%2520a%2520fully%2520trained%2520video%2520generative%2520model%2520in%2520high-dimensional%250Aspace%2520can%2520effectively%2520support%2520lower-dimensional%2520tasks%2520such%2520as%2520controllable%250Aimage%2520generation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520paradigm%2520for%2520video-to-image%250Aknowledge%2520compression%2520and%2520task%2520adaptation%252C%2520termed%2520%255Ctextit%257BDimension-Reduction%250AAttack%257D%2520%2528%255Ctexttt%257BDRA-Ctrl%257D%2529%252C%2520which%2520utilizes%2520the%2520strengths%2520of%2520video%2520models%252C%250Aincluding%2520long-range%2520context%2520modeling%2520and%2520flatten%2520full-attention%252C%2520to%2520perform%250Avarious%2520generation%2520tasks.%2520Specially%252C%2520to%2520address%2520the%2520challenging%2520gap%2520between%250Acontinuous%2520video%2520frames%2520and%2520discrete%2520image%2520generation%252C%2520we%2520introduce%2520a%250Amixup-based%2520transition%2520strategy%2520that%2520ensures%2520smooth%2520adaptation.%2520Moreover%252C%2520we%250Aredesign%2520the%2520attention%2520structure%2520with%2520a%2520tailored%2520masking%2520mechanism%2520to%2520better%250Aalign%2520text%2520prompts%2520with%2520image-level%2520control.%2520Experiments%2520across%2520diverse%2520image%250Ageneration%2520tasks%252C%2520such%2520as%2520subject-driven%2520and%2520spatially%2520conditioned%2520generation%252C%250Ashow%2520that%2520repurposed%2520video%2520models%2520outperform%2520those%2520trained%2520directly%2520on%2520images.%250AThese%2520results%2520highlight%2520the%2520untapped%2520potential%2520of%2520large-scale%2520video%2520generators%250Afor%2520broader%2520visual%2520applications.%2520%255Ctexttt%257BDRA-Ctrl%257D%2520provides%2520new%2520insights%2520into%250Areusing%2520resource-intensive%2520video%2520models%2520and%2520lays%2520foundation%2520for%2520future%2520unified%250Agenerative%2520models%2520across%2520visual%2520modalities.%2520The%2520project%2520page%2520is%250Ahttps%253A//dra-ctrl-2025.github.io/DRA-Ctrl/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimension-Reduction%20Attack%21%20Video%20Generative%20Models%20are%20Experts%20on%0A%20%20Controllable%20Image%20Synthesis&entry.906535625=Hengyuan%20Cao%20and%20Yutong%20Feng%20and%20Biao%20Gong%20and%20Yijing%20Tian%20and%20Yunhong%20Lu%20and%20Chuang%20Liu%20and%20Bin%20Wang&entry.1292438233=%20%20Video%20generative%20models%20can%20be%20regarded%20as%20world%20simulators%20due%20to%20their%0Aability%20to%20capture%20dynamic%2C%20continuous%20changes%20inherent%20in%20real-world%0Aenvironments.%20These%20models%20integrate%20high-dimensional%20information%20across%0Avisual%2C%20temporal%2C%20spatial%2C%20and%20causal%20dimensions%2C%20enabling%20predictions%20of%0Asubjects%20in%20various%20status.%20A%20natural%20and%20valuable%20research%20direction%20is%20to%0Aexplore%20whether%20a%20fully%20trained%20video%20generative%20model%20in%20high-dimensional%0Aspace%20can%20effectively%20support%20lower-dimensional%20tasks%20such%20as%20controllable%0Aimage%20generation.%20In%20this%20work%2C%20we%20propose%20a%20paradigm%20for%20video-to-image%0Aknowledge%20compression%20and%20task%20adaptation%2C%20termed%20%5Ctextit%7BDimension-Reduction%0AAttack%7D%20%28%5Ctexttt%7BDRA-Ctrl%7D%29%2C%20which%20utilizes%20the%20strengths%20of%20video%20models%2C%0Aincluding%20long-range%20context%20modeling%20and%20flatten%20full-attention%2C%20to%20perform%0Avarious%20generation%20tasks.%20Specially%2C%20to%20address%20the%20challenging%20gap%20between%0Acontinuous%20video%20frames%20and%20discrete%20image%20generation%2C%20we%20introduce%20a%0Amixup-based%20transition%20strategy%20that%20ensures%20smooth%20adaptation.%20Moreover%2C%20we%0Aredesign%20the%20attention%20structure%20with%20a%20tailored%20masking%20mechanism%20to%20better%0Aalign%20text%20prompts%20with%20image-level%20control.%20Experiments%20across%20diverse%20image%0Ageneration%20tasks%2C%20such%20as%20subject-driven%20and%20spatially%20conditioned%20generation%2C%0Ashow%20that%20repurposed%20video%20models%20outperform%20those%20trained%20directly%20on%20images.%0AThese%20results%20highlight%20the%20untapped%20potential%20of%20large-scale%20video%20generators%0Afor%20broader%20visual%20applications.%20%5Ctexttt%7BDRA-Ctrl%7D%20provides%20new%20insights%20into%0Areusing%20resource-intensive%20video%20models%20and%20lays%20foundation%20for%20future%20unified%0Agenerative%20models%20across%20visual%20modalities.%20The%20project%20page%20is%0Ahttps%3A//dra-ctrl-2025.github.io/DRA-Ctrl/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23325v1&entry.124074799=Read"},
{"title": "ReDDiT: Rehashing Noise for Discrete Visual Generation", "author": "Tianren Ma and Xiaosong Zhang and Boyu Yang and Junlan Feng and Qixiang Ye", "abstract": "  Discrete diffusion models are gaining traction in the visual generative area\nfor their efficiency and compatibility. However, the pioneered attempts still\nfall behind the continuous counterparts, which we attribute to the noise\n(absorbing state) design and sampling heuristics. In this study, we propose the\nrehashing noise framework for discrete diffusion transformer, termed ReDDiT, to\nextend absorbing states and improve expressive capacity of discrete diffusion\nmodels. ReDDiT enriches the potential paths that latent variables can traverse\nduring training with randomized multi-index corruption. The derived rehash\nsampler, which reverses the randomized absorbing paths, guarantees the\ndiversity and low discrepancy of the generation process. These reformulations\nlead to more consistent and competitive generation quality, mitigating the need\nfor heavily tuned randomness. Experiments show that ReDDiT significantly\noutperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with\nthe continuous counterparts with higher efficiency.\n", "link": "http://arxiv.org/abs/2505.19656v2", "date": "2025-05-29", "relevancy": 2.5906, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6769}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6428}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReDDiT%3A%20Rehashing%20Noise%20for%20Discrete%20Visual%20Generation&body=Title%3A%20ReDDiT%3A%20Rehashing%20Noise%20for%20Discrete%20Visual%20Generation%0AAuthor%3A%20Tianren%20Ma%20and%20Xiaosong%20Zhang%20and%20Boyu%20Yang%20and%20Junlan%20Feng%20and%20Qixiang%20Ye%0AAbstract%3A%20%20%20Discrete%20diffusion%20models%20are%20gaining%20traction%20in%20the%20visual%20generative%20area%0Afor%20their%20efficiency%20and%20compatibility.%20However%2C%20the%20pioneered%20attempts%20still%0Afall%20behind%20the%20continuous%20counterparts%2C%20which%20we%20attribute%20to%20the%20noise%0A%28absorbing%20state%29%20design%20and%20sampling%20heuristics.%20In%20this%20study%2C%20we%20propose%20the%0Arehashing%20noise%20framework%20for%20discrete%20diffusion%20transformer%2C%20termed%20ReDDiT%2C%20to%0Aextend%20absorbing%20states%20and%20improve%20expressive%20capacity%20of%20discrete%20diffusion%0Amodels.%20ReDDiT%20enriches%20the%20potential%20paths%20that%20latent%20variables%20can%20traverse%0Aduring%20training%20with%20randomized%20multi-index%20corruption.%20The%20derived%20rehash%0Asampler%2C%20which%20reverses%20the%20randomized%20absorbing%20paths%2C%20guarantees%20the%0Adiversity%20and%20low%20discrepancy%20of%20the%20generation%20process.%20These%20reformulations%0Alead%20to%20more%20consistent%20and%20competitive%20generation%20quality%2C%20mitigating%20the%20need%0Afor%20heavily%20tuned%20randomness.%20Experiments%20show%20that%20ReDDiT%20significantly%0Aoutperforms%20the%20baseline%20%28reducing%20gFID%20from%206.18%20to%201.61%29%20and%20is%20on%20par%20with%0Athe%20continuous%20counterparts%20with%20higher%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19656v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReDDiT%253A%2520Rehashing%2520Noise%2520for%2520Discrete%2520Visual%2520Generation%26entry.906535625%3DTianren%2520Ma%2520and%2520Xiaosong%2520Zhang%2520and%2520Boyu%2520Yang%2520and%2520Junlan%2520Feng%2520and%2520Qixiang%2520Ye%26entry.1292438233%3D%2520%2520Discrete%2520diffusion%2520models%2520are%2520gaining%2520traction%2520in%2520the%2520visual%2520generative%2520area%250Afor%2520their%2520efficiency%2520and%2520compatibility.%2520However%252C%2520the%2520pioneered%2520attempts%2520still%250Afall%2520behind%2520the%2520continuous%2520counterparts%252C%2520which%2520we%2520attribute%2520to%2520the%2520noise%250A%2528absorbing%2520state%2529%2520design%2520and%2520sampling%2520heuristics.%2520In%2520this%2520study%252C%2520we%2520propose%2520the%250Arehashing%2520noise%2520framework%2520for%2520discrete%2520diffusion%2520transformer%252C%2520termed%2520ReDDiT%252C%2520to%250Aextend%2520absorbing%2520states%2520and%2520improve%2520expressive%2520capacity%2520of%2520discrete%2520diffusion%250Amodels.%2520ReDDiT%2520enriches%2520the%2520potential%2520paths%2520that%2520latent%2520variables%2520can%2520traverse%250Aduring%2520training%2520with%2520randomized%2520multi-index%2520corruption.%2520The%2520derived%2520rehash%250Asampler%252C%2520which%2520reverses%2520the%2520randomized%2520absorbing%2520paths%252C%2520guarantees%2520the%250Adiversity%2520and%2520low%2520discrepancy%2520of%2520the%2520generation%2520process.%2520These%2520reformulations%250Alead%2520to%2520more%2520consistent%2520and%2520competitive%2520generation%2520quality%252C%2520mitigating%2520the%2520need%250Afor%2520heavily%2520tuned%2520randomness.%2520Experiments%2520show%2520that%2520ReDDiT%2520significantly%250Aoutperforms%2520the%2520baseline%2520%2528reducing%2520gFID%2520from%25206.18%2520to%25201.61%2529%2520and%2520is%2520on%2520par%2520with%250Athe%2520continuous%2520counterparts%2520with%2520higher%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19656v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReDDiT%3A%20Rehashing%20Noise%20for%20Discrete%20Visual%20Generation&entry.906535625=Tianren%20Ma%20and%20Xiaosong%20Zhang%20and%20Boyu%20Yang%20and%20Junlan%20Feng%20and%20Qixiang%20Ye&entry.1292438233=%20%20Discrete%20diffusion%20models%20are%20gaining%20traction%20in%20the%20visual%20generative%20area%0Afor%20their%20efficiency%20and%20compatibility.%20However%2C%20the%20pioneered%20attempts%20still%0Afall%20behind%20the%20continuous%20counterparts%2C%20which%20we%20attribute%20to%20the%20noise%0A%28absorbing%20state%29%20design%20and%20sampling%20heuristics.%20In%20this%20study%2C%20we%20propose%20the%0Arehashing%20noise%20framework%20for%20discrete%20diffusion%20transformer%2C%20termed%20ReDDiT%2C%20to%0Aextend%20absorbing%20states%20and%20improve%20expressive%20capacity%20of%20discrete%20diffusion%0Amodels.%20ReDDiT%20enriches%20the%20potential%20paths%20that%20latent%20variables%20can%20traverse%0Aduring%20training%20with%20randomized%20multi-index%20corruption.%20The%20derived%20rehash%0Asampler%2C%20which%20reverses%20the%20randomized%20absorbing%20paths%2C%20guarantees%20the%0Adiversity%20and%20low%20discrepancy%20of%20the%20generation%20process.%20These%20reformulations%0Alead%20to%20more%20consistent%20and%20competitive%20generation%20quality%2C%20mitigating%20the%20need%0Afor%20heavily%20tuned%20randomness.%20Experiments%20show%20that%20ReDDiT%20significantly%0Aoutperforms%20the%20baseline%20%28reducing%20gFID%20from%206.18%20to%201.61%29%20and%20is%20on%20par%20with%0Athe%20continuous%20counterparts%20with%20higher%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19656v2&entry.124074799=Read"},
{"title": "mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus", "author": "Matthieu Futeral and Armel Zebaze and Pedro Ortiz Suarez and Julien Abadji and R\u00e9mi Lacroix and Cordelia Schmid and Rachel Bawden and Beno\u00eet Sagot", "abstract": "  Multimodal Large Language Models (mLLMs) are trained on a large amount of\ntext-image data. While most mLLMs are trained on caption-like data only,\nAlayrac et al. (2022) showed that additionally training them on interleaved\nsequences of text and images can lead to the emergence of in-context learning\ncapabilities. However, the dataset they used, M3W, is not public and is only in\nEnglish. There have been attempts to reproduce their results but the released\ndatasets are English-only. In contrast, current multilingual and multimodal\ndatasets are either composed of caption-like only or medium-scale or fully\nprivate data. This limits mLLM research for the 7,000 other languages spoken in\nthe world. We therefore introduce mOSCAR, to the best of our knowledge the\nfirst large-scale multilingual and multimodal document corpus crawled from the\nweb. It covers 163 languages, 303M documents, 200B tokens and 1.15B images. We\ncarefully conduct a set of filtering and evaluation steps to make sure mOSCAR\nis sufficiently safe, diverse and of good quality. We additionally train two\ntypes of multilingual model to prove the benefits of mOSCAR: (1) a model\ntrained on a subset of mOSCAR and captioning data and (2) a model trained on\ncaptioning data only. The model additionally trained on mOSCAR shows a strong\nboost in few-shot learning performance across various multilingual image-text\ntasks and benchmarks, confirming previous findings for English-only mLLMs. The\ndataset is released under the Creative Commons CC BY 4.0 license and can be\naccessed here: https://huggingface.co/datasets/oscar-corpus/mOSCAR\n", "link": "http://arxiv.org/abs/2406.08707v2", "date": "2025-05-29", "relevancy": 2.578, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mOSCAR%3A%20A%20Large-scale%20Multilingual%20and%20Multimodal%20Document-level%20Corpus&body=Title%3A%20mOSCAR%3A%20A%20Large-scale%20Multilingual%20and%20Multimodal%20Document-level%20Corpus%0AAuthor%3A%20Matthieu%20Futeral%20and%20Armel%20Zebaze%20and%20Pedro%20Ortiz%20Suarez%20and%20Julien%20Abadji%20and%20R%C3%A9mi%20Lacroix%20and%20Cordelia%20Schmid%20and%20Rachel%20Bawden%20and%20Beno%C3%AEt%20Sagot%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28mLLMs%29%20are%20trained%20on%20a%20large%20amount%20of%0Atext-image%20data.%20While%20most%20mLLMs%20are%20trained%20on%20caption-like%20data%20only%2C%0AAlayrac%20et%20al.%20%282022%29%20showed%20that%20additionally%20training%20them%20on%20interleaved%0Asequences%20of%20text%20and%20images%20can%20lead%20to%20the%20emergence%20of%20in-context%20learning%0Acapabilities.%20However%2C%20the%20dataset%20they%20used%2C%20M3W%2C%20is%20not%20public%20and%20is%20only%20in%0AEnglish.%20There%20have%20been%20attempts%20to%20reproduce%20their%20results%20but%20the%20released%0Adatasets%20are%20English-only.%20In%20contrast%2C%20current%20multilingual%20and%20multimodal%0Adatasets%20are%20either%20composed%20of%20caption-like%20only%20or%20medium-scale%20or%20fully%0Aprivate%20data.%20This%20limits%20mLLM%20research%20for%20the%207%2C000%20other%20languages%20spoken%20in%0Athe%20world.%20We%20therefore%20introduce%20mOSCAR%2C%20to%20the%20best%20of%20our%20knowledge%20the%0Afirst%20large-scale%20multilingual%20and%20multimodal%20document%20corpus%20crawled%20from%20the%0Aweb.%20It%20covers%20163%20languages%2C%20303M%20documents%2C%20200B%20tokens%20and%201.15B%20images.%20We%0Acarefully%20conduct%20a%20set%20of%20filtering%20and%20evaluation%20steps%20to%20make%20sure%20mOSCAR%0Ais%20sufficiently%20safe%2C%20diverse%20and%20of%20good%20quality.%20We%20additionally%20train%20two%0Atypes%20of%20multilingual%20model%20to%20prove%20the%20benefits%20of%20mOSCAR%3A%20%281%29%20a%20model%0Atrained%20on%20a%20subset%20of%20mOSCAR%20and%20captioning%20data%20and%20%282%29%20a%20model%20trained%20on%0Acaptioning%20data%20only.%20The%20model%20additionally%20trained%20on%20mOSCAR%20shows%20a%20strong%0Aboost%20in%20few-shot%20learning%20performance%20across%20various%20multilingual%20image-text%0Atasks%20and%20benchmarks%2C%20confirming%20previous%20findings%20for%20English-only%20mLLMs.%20The%0Adataset%20is%20released%20under%20the%20Creative%20Commons%20CC%20BY%204.0%20license%20and%20can%20be%0Aaccessed%20here%3A%20https%3A//huggingface.co/datasets/oscar-corpus/mOSCAR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmOSCAR%253A%2520A%2520Large-scale%2520Multilingual%2520and%2520Multimodal%2520Document-level%2520Corpus%26entry.906535625%3DMatthieu%2520Futeral%2520and%2520Armel%2520Zebaze%2520and%2520Pedro%2520Ortiz%2520Suarez%2520and%2520Julien%2520Abadji%2520and%2520R%25C3%25A9mi%2520Lacroix%2520and%2520Cordelia%2520Schmid%2520and%2520Rachel%2520Bawden%2520and%2520Beno%25C3%25AEt%2520Sagot%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528mLLMs%2529%2520are%2520trained%2520on%2520a%2520large%2520amount%2520of%250Atext-image%2520data.%2520While%2520most%2520mLLMs%2520are%2520trained%2520on%2520caption-like%2520data%2520only%252C%250AAlayrac%2520et%2520al.%2520%25282022%2529%2520showed%2520that%2520additionally%2520training%2520them%2520on%2520interleaved%250Asequences%2520of%2520text%2520and%2520images%2520can%2520lead%2520to%2520the%2520emergence%2520of%2520in-context%2520learning%250Acapabilities.%2520However%252C%2520the%2520dataset%2520they%2520used%252C%2520M3W%252C%2520is%2520not%2520public%2520and%2520is%2520only%2520in%250AEnglish.%2520There%2520have%2520been%2520attempts%2520to%2520reproduce%2520their%2520results%2520but%2520the%2520released%250Adatasets%2520are%2520English-only.%2520In%2520contrast%252C%2520current%2520multilingual%2520and%2520multimodal%250Adatasets%2520are%2520either%2520composed%2520of%2520caption-like%2520only%2520or%2520medium-scale%2520or%2520fully%250Aprivate%2520data.%2520This%2520limits%2520mLLM%2520research%2520for%2520the%25207%252C000%2520other%2520languages%2520spoken%2520in%250Athe%2520world.%2520We%2520therefore%2520introduce%2520mOSCAR%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%2520the%250Afirst%2520large-scale%2520multilingual%2520and%2520multimodal%2520document%2520corpus%2520crawled%2520from%2520the%250Aweb.%2520It%2520covers%2520163%2520languages%252C%2520303M%2520documents%252C%2520200B%2520tokens%2520and%25201.15B%2520images.%2520We%250Acarefully%2520conduct%2520a%2520set%2520of%2520filtering%2520and%2520evaluation%2520steps%2520to%2520make%2520sure%2520mOSCAR%250Ais%2520sufficiently%2520safe%252C%2520diverse%2520and%2520of%2520good%2520quality.%2520We%2520additionally%2520train%2520two%250Atypes%2520of%2520multilingual%2520model%2520to%2520prove%2520the%2520benefits%2520of%2520mOSCAR%253A%2520%25281%2529%2520a%2520model%250Atrained%2520on%2520a%2520subset%2520of%2520mOSCAR%2520and%2520captioning%2520data%2520and%2520%25282%2529%2520a%2520model%2520trained%2520on%250Acaptioning%2520data%2520only.%2520The%2520model%2520additionally%2520trained%2520on%2520mOSCAR%2520shows%2520a%2520strong%250Aboost%2520in%2520few-shot%2520learning%2520performance%2520across%2520various%2520multilingual%2520image-text%250Atasks%2520and%2520benchmarks%252C%2520confirming%2520previous%2520findings%2520for%2520English-only%2520mLLMs.%2520The%250Adataset%2520is%2520released%2520under%2520the%2520Creative%2520Commons%2520CC%2520BY%25204.0%2520license%2520and%2520can%2520be%250Aaccessed%2520here%253A%2520https%253A//huggingface.co/datasets/oscar-corpus/mOSCAR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mOSCAR%3A%20A%20Large-scale%20Multilingual%20and%20Multimodal%20Document-level%20Corpus&entry.906535625=Matthieu%20Futeral%20and%20Armel%20Zebaze%20and%20Pedro%20Ortiz%20Suarez%20and%20Julien%20Abadji%20and%20R%C3%A9mi%20Lacroix%20and%20Cordelia%20Schmid%20and%20Rachel%20Bawden%20and%20Beno%C3%AEt%20Sagot&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28mLLMs%29%20are%20trained%20on%20a%20large%20amount%20of%0Atext-image%20data.%20While%20most%20mLLMs%20are%20trained%20on%20caption-like%20data%20only%2C%0AAlayrac%20et%20al.%20%282022%29%20showed%20that%20additionally%20training%20them%20on%20interleaved%0Asequences%20of%20text%20and%20images%20can%20lead%20to%20the%20emergence%20of%20in-context%20learning%0Acapabilities.%20However%2C%20the%20dataset%20they%20used%2C%20M3W%2C%20is%20not%20public%20and%20is%20only%20in%0AEnglish.%20There%20have%20been%20attempts%20to%20reproduce%20their%20results%20but%20the%20released%0Adatasets%20are%20English-only.%20In%20contrast%2C%20current%20multilingual%20and%20multimodal%0Adatasets%20are%20either%20composed%20of%20caption-like%20only%20or%20medium-scale%20or%20fully%0Aprivate%20data.%20This%20limits%20mLLM%20research%20for%20the%207%2C000%20other%20languages%20spoken%20in%0Athe%20world.%20We%20therefore%20introduce%20mOSCAR%2C%20to%20the%20best%20of%20our%20knowledge%20the%0Afirst%20large-scale%20multilingual%20and%20multimodal%20document%20corpus%20crawled%20from%20the%0Aweb.%20It%20covers%20163%20languages%2C%20303M%20documents%2C%20200B%20tokens%20and%201.15B%20images.%20We%0Acarefully%20conduct%20a%20set%20of%20filtering%20and%20evaluation%20steps%20to%20make%20sure%20mOSCAR%0Ais%20sufficiently%20safe%2C%20diverse%20and%20of%20good%20quality.%20We%20additionally%20train%20two%0Atypes%20of%20multilingual%20model%20to%20prove%20the%20benefits%20of%20mOSCAR%3A%20%281%29%20a%20model%0Atrained%20on%20a%20subset%20of%20mOSCAR%20and%20captioning%20data%20and%20%282%29%20a%20model%20trained%20on%0Acaptioning%20data%20only.%20The%20model%20additionally%20trained%20on%20mOSCAR%20shows%20a%20strong%0Aboost%20in%20few-shot%20learning%20performance%20across%20various%20multilingual%20image-text%0Atasks%20and%20benchmarks%2C%20confirming%20previous%20findings%20for%20English-only%20mLLMs.%20The%0Adataset%20is%20released%20under%20the%20Creative%20Commons%20CC%20BY%204.0%20license%20and%20can%20be%0Aaccessed%20here%3A%20https%3A//huggingface.co/datasets/oscar-corpus/mOSCAR%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08707v2&entry.124074799=Read"},
{"title": "Dataset Featurization: Uncovering Natural Language Features through\n  Unsupervised Data Reconstruction", "author": "Michal Bravansky and Vaclav Kubon and Suhas Hariharan and Robert Kirk", "abstract": "  Interpreting data is central to modern research. Large language models (LLMs)\nshow promise in providing such natural language interpretations of data, yet\nsimple feature extraction methods such as prompting often fail to produce\naccurate and versatile descriptions for diverse datasets and lack control over\ngranularity and scale. To address these limitations, we propose a\ndomain-agnostic method for dataset featurization that provides precise control\nover the number of features extracted while maintaining compact and descriptive\nrepresentations comparable to human labeling. Our method optimizes the\nselection of informative binary features by evaluating the ability of an LLM to\nreconstruct the original data using those features. We demonstrate its\neffectiveness in dataset modeling tasks and through two case studies: (1)\nConstructing a feature representation of jailbreak tactics that compactly\ncaptures both the effectiveness and diversity of a larger set of human-crafted\nattacks; and (2) automating the discovery of features that align with human\npreferences, achieving accuracy and robustness comparable to human-crafted\nfeatures. Moreover, we show that the pipeline scales effectively, improving as\nadditional features are sampled, making it suitable for large and diverse\ndatasets.\n", "link": "http://arxiv.org/abs/2502.17541v2", "date": "2025-05-29", "relevancy": 2.5592, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dataset%20Featurization%3A%20Uncovering%20Natural%20Language%20Features%20through%0A%20%20Unsupervised%20Data%20Reconstruction&body=Title%3A%20Dataset%20Featurization%3A%20Uncovering%20Natural%20Language%20Features%20through%0A%20%20Unsupervised%20Data%20Reconstruction%0AAuthor%3A%20Michal%20Bravansky%20and%20Vaclav%20Kubon%20and%20Suhas%20Hariharan%20and%20Robert%20Kirk%0AAbstract%3A%20%20%20Interpreting%20data%20is%20central%20to%20modern%20research.%20Large%20language%20models%20%28LLMs%29%0Ashow%20promise%20in%20providing%20such%20natural%20language%20interpretations%20of%20data%2C%20yet%0Asimple%20feature%20extraction%20methods%20such%20as%20prompting%20often%20fail%20to%20produce%0Aaccurate%20and%20versatile%20descriptions%20for%20diverse%20datasets%20and%20lack%20control%20over%0Agranularity%20and%20scale.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Adomain-agnostic%20method%20for%20dataset%20featurization%20that%20provides%20precise%20control%0Aover%20the%20number%20of%20features%20extracted%20while%20maintaining%20compact%20and%20descriptive%0Arepresentations%20comparable%20to%20human%20labeling.%20Our%20method%20optimizes%20the%0Aselection%20of%20informative%20binary%20features%20by%20evaluating%20the%20ability%20of%20an%20LLM%20to%0Areconstruct%20the%20original%20data%20using%20those%20features.%20We%20demonstrate%20its%0Aeffectiveness%20in%20dataset%20modeling%20tasks%20and%20through%20two%20case%20studies%3A%20%281%29%0AConstructing%20a%20feature%20representation%20of%20jailbreak%20tactics%20that%20compactly%0Acaptures%20both%20the%20effectiveness%20and%20diversity%20of%20a%20larger%20set%20of%20human-crafted%0Aattacks%3B%20and%20%282%29%20automating%20the%20discovery%20of%20features%20that%20align%20with%20human%0Apreferences%2C%20achieving%20accuracy%20and%20robustness%20comparable%20to%20human-crafted%0Afeatures.%20Moreover%2C%20we%20show%20that%20the%20pipeline%20scales%20effectively%2C%20improving%20as%0Aadditional%20features%20are%20sampled%2C%20making%20it%20suitable%20for%20large%20and%20diverse%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataset%2520Featurization%253A%2520Uncovering%2520Natural%2520Language%2520Features%2520through%250A%2520%2520Unsupervised%2520Data%2520Reconstruction%26entry.906535625%3DMichal%2520Bravansky%2520and%2520Vaclav%2520Kubon%2520and%2520Suhas%2520Hariharan%2520and%2520Robert%2520Kirk%26entry.1292438233%3D%2520%2520Interpreting%2520data%2520is%2520central%2520to%2520modern%2520research.%2520Large%2520language%2520models%2520%2528LLMs%2529%250Ashow%2520promise%2520in%2520providing%2520such%2520natural%2520language%2520interpretations%2520of%2520data%252C%2520yet%250Asimple%2520feature%2520extraction%2520methods%2520such%2520as%2520prompting%2520often%2520fail%2520to%2520produce%250Aaccurate%2520and%2520versatile%2520descriptions%2520for%2520diverse%2520datasets%2520and%2520lack%2520control%2520over%250Agranularity%2520and%2520scale.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Adomain-agnostic%2520method%2520for%2520dataset%2520featurization%2520that%2520provides%2520precise%2520control%250Aover%2520the%2520number%2520of%2520features%2520extracted%2520while%2520maintaining%2520compact%2520and%2520descriptive%250Arepresentations%2520comparable%2520to%2520human%2520labeling.%2520Our%2520method%2520optimizes%2520the%250Aselection%2520of%2520informative%2520binary%2520features%2520by%2520evaluating%2520the%2520ability%2520of%2520an%2520LLM%2520to%250Areconstruct%2520the%2520original%2520data%2520using%2520those%2520features.%2520We%2520demonstrate%2520its%250Aeffectiveness%2520in%2520dataset%2520modeling%2520tasks%2520and%2520through%2520two%2520case%2520studies%253A%2520%25281%2529%250AConstructing%2520a%2520feature%2520representation%2520of%2520jailbreak%2520tactics%2520that%2520compactly%250Acaptures%2520both%2520the%2520effectiveness%2520and%2520diversity%2520of%2520a%2520larger%2520set%2520of%2520human-crafted%250Aattacks%253B%2520and%2520%25282%2529%2520automating%2520the%2520discovery%2520of%2520features%2520that%2520align%2520with%2520human%250Apreferences%252C%2520achieving%2520accuracy%2520and%2520robustness%2520comparable%2520to%2520human-crafted%250Afeatures.%2520Moreover%252C%2520we%2520show%2520that%2520the%2520pipeline%2520scales%2520effectively%252C%2520improving%2520as%250Aadditional%2520features%2520are%2520sampled%252C%2520making%2520it%2520suitable%2520for%2520large%2520and%2520diverse%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset%20Featurization%3A%20Uncovering%20Natural%20Language%20Features%20through%0A%20%20Unsupervised%20Data%20Reconstruction&entry.906535625=Michal%20Bravansky%20and%20Vaclav%20Kubon%20and%20Suhas%20Hariharan%20and%20Robert%20Kirk&entry.1292438233=%20%20Interpreting%20data%20is%20central%20to%20modern%20research.%20Large%20language%20models%20%28LLMs%29%0Ashow%20promise%20in%20providing%20such%20natural%20language%20interpretations%20of%20data%2C%20yet%0Asimple%20feature%20extraction%20methods%20such%20as%20prompting%20often%20fail%20to%20produce%0Aaccurate%20and%20versatile%20descriptions%20for%20diverse%20datasets%20and%20lack%20control%20over%0Agranularity%20and%20scale.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Adomain-agnostic%20method%20for%20dataset%20featurization%20that%20provides%20precise%20control%0Aover%20the%20number%20of%20features%20extracted%20while%20maintaining%20compact%20and%20descriptive%0Arepresentations%20comparable%20to%20human%20labeling.%20Our%20method%20optimizes%20the%0Aselection%20of%20informative%20binary%20features%20by%20evaluating%20the%20ability%20of%20an%20LLM%20to%0Areconstruct%20the%20original%20data%20using%20those%20features.%20We%20demonstrate%20its%0Aeffectiveness%20in%20dataset%20modeling%20tasks%20and%20through%20two%20case%20studies%3A%20%281%29%0AConstructing%20a%20feature%20representation%20of%20jailbreak%20tactics%20that%20compactly%0Acaptures%20both%20the%20effectiveness%20and%20diversity%20of%20a%20larger%20set%20of%20human-crafted%0Aattacks%3B%20and%20%282%29%20automating%20the%20discovery%20of%20features%20that%20align%20with%20human%0Apreferences%2C%20achieving%20accuracy%20and%20robustness%20comparable%20to%20human-crafted%0Afeatures.%20Moreover%2C%20we%20show%20that%20the%20pipeline%20scales%20effectively%2C%20improving%20as%0Aadditional%20features%20are%20sampled%2C%20making%20it%20suitable%20for%20large%20and%20diverse%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17541v2&entry.124074799=Read"},
{"title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity", "author": "Yu Zhang and Dong Guo and Fang Wu and Guoliang Zhu and Dian Ding and Yiming Zhang", "abstract": "  Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.\n", "link": "http://arxiv.org/abs/2505.23520v1", "date": "2025-05-29", "relevancy": 2.553, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5327}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5144}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnchorAttention%3A%20Difference-Aware%20Sparse%20Attention%20with%20Stripe%0A%20%20Granularity&body=Title%3A%20AnchorAttention%3A%20Difference-Aware%20Sparse%20Attention%20with%20Stripe%0A%20%20Granularity%0AAuthor%3A%20Yu%20Zhang%20and%20Dong%20Guo%20and%20Fang%20Wu%20and%20Guoliang%20Zhu%20and%20Dian%20Ding%20and%20Yiming%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20with%20extended%20context%20lengths%20face%20significant%0Acomputational%20challenges%20during%20the%20pre-filling%20phase%2C%20primarily%20due%20to%20the%0Aquadratic%20complexity%20of%20self-attention.%20Existing%20methods%20typically%20employ%0Adynamic%20pattern%20matching%20and%20block-sparse%20low-level%20implementations.%20However%2C%0Atheir%20reliance%20on%20local%20information%20for%20pattern%20identification%20fails%20to%20capture%0Aglobal%20contexts%2C%20and%20the%20coarse%20granularity%20of%20blocks%20leads%20to%20persistent%0Ainternal%20sparsity%2C%20resulting%20in%20suboptimal%20accuracy%20and%20efficiency.%20To%20address%0Athese%20limitations%2C%20we%20propose%20%5Ctextbf%7BAnchorAttention%7D%2C%20a%20difference-aware%2C%0Adynamic%20sparse%20attention%20mechanism%20that%20efficiently%20identifies%20critical%0Aattention%20regions%20at%20a%20finer%20stripe%20granularity%20while%20adapting%20to%20global%0Acontextual%20information%2C%20achieving%20superior%20speed%20and%20accuracy.%20AnchorAttention%0Acomprises%20three%20key%20components%3A%20%281%29%20%5Ctextbf%7BPattern-based%20Anchor%20Computation%7D%2C%0Aleveraging%20the%20commonalities%20present%20across%20all%20inputs%20to%20rapidly%20compute%20a%20set%0Aof%20near-maximum%20scores%20as%20the%20anchor%3B%20%282%29%20%5Ctextbf%7BDifference-aware%20Stripe%0ASparsity%20Identification%7D%2C%20performing%20difference-aware%20comparisons%20with%20the%0Aanchor%20to%20quickly%20obtain%20discrete%20coordinates%20of%20significant%20regions%20in%20a%0Astripe-like%20sparsity%20pattern%3B%20%283%29%20%5Ctextbf%7BFine-grained%20Sparse%20Computation%7D%2C%0Areplacing%20the%20traditional%20contiguous%20KV%20block%20loading%20approach%20with%0Asimultaneous%20discrete%20KV%20position%20loading%20to%20maximize%20sparsity%20rates%20while%0Apreserving%20full%20hardware%20computational%20potential.%20With%20its%20finer-grained%0Asparsity%20strategy%2C%20%5Ctextbf%7BAnchorAttention%7D%20achieves%20higher%20sparsity%20rates%20at%0Athe%20same%20recall%20level%2C%20significantly%20reducing%20computation%20time.%20Compared%20to%0Aprevious%20state-of-the-art%20methods%2C%20at%20a%20text%20length%20of%20128k%2C%20it%20achieves%20a%0Aspeedup%20of%201.44%24%5Ctimes%24%20while%20maintaining%20higher%20recall%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnchorAttention%253A%2520Difference-Aware%2520Sparse%2520Attention%2520with%2520Stripe%250A%2520%2520Granularity%26entry.906535625%3DYu%2520Zhang%2520and%2520Dong%2520Guo%2520and%2520Fang%2520Wu%2520and%2520Guoliang%2520Zhu%2520and%2520Dian%2520Ding%2520and%2520Yiming%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520extended%2520context%2520lengths%2520face%2520significant%250Acomputational%2520challenges%2520during%2520the%2520pre-filling%2520phase%252C%2520primarily%2520due%2520to%2520the%250Aquadratic%2520complexity%2520of%2520self-attention.%2520Existing%2520methods%2520typically%2520employ%250Adynamic%2520pattern%2520matching%2520and%2520block-sparse%2520low-level%2520implementations.%2520However%252C%250Atheir%2520reliance%2520on%2520local%2520information%2520for%2520pattern%2520identification%2520fails%2520to%2520capture%250Aglobal%2520contexts%252C%2520and%2520the%2520coarse%2520granularity%2520of%2520blocks%2520leads%2520to%2520persistent%250Ainternal%2520sparsity%252C%2520resulting%2520in%2520suboptimal%2520accuracy%2520and%2520efficiency.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520%255Ctextbf%257BAnchorAttention%257D%252C%2520a%2520difference-aware%252C%250Adynamic%2520sparse%2520attention%2520mechanism%2520that%2520efficiently%2520identifies%2520critical%250Aattention%2520regions%2520at%2520a%2520finer%2520stripe%2520granularity%2520while%2520adapting%2520to%2520global%250Acontextual%2520information%252C%2520achieving%2520superior%2520speed%2520and%2520accuracy.%2520AnchorAttention%250Acomprises%2520three%2520key%2520components%253A%2520%25281%2529%2520%255Ctextbf%257BPattern-based%2520Anchor%2520Computation%257D%252C%250Aleveraging%2520the%2520commonalities%2520present%2520across%2520all%2520inputs%2520to%2520rapidly%2520compute%2520a%2520set%250Aof%2520near-maximum%2520scores%2520as%2520the%2520anchor%253B%2520%25282%2529%2520%255Ctextbf%257BDifference-aware%2520Stripe%250ASparsity%2520Identification%257D%252C%2520performing%2520difference-aware%2520comparisons%2520with%2520the%250Aanchor%2520to%2520quickly%2520obtain%2520discrete%2520coordinates%2520of%2520significant%2520regions%2520in%2520a%250Astripe-like%2520sparsity%2520pattern%253B%2520%25283%2529%2520%255Ctextbf%257BFine-grained%2520Sparse%2520Computation%257D%252C%250Areplacing%2520the%2520traditional%2520contiguous%2520KV%2520block%2520loading%2520approach%2520with%250Asimultaneous%2520discrete%2520KV%2520position%2520loading%2520to%2520maximize%2520sparsity%2520rates%2520while%250Apreserving%2520full%2520hardware%2520computational%2520potential.%2520With%2520its%2520finer-grained%250Asparsity%2520strategy%252C%2520%255Ctextbf%257BAnchorAttention%257D%2520achieves%2520higher%2520sparsity%2520rates%2520at%250Athe%2520same%2520recall%2520level%252C%2520significantly%2520reducing%2520computation%2520time.%2520Compared%2520to%250Aprevious%2520state-of-the-art%2520methods%252C%2520at%2520a%2520text%2520length%2520of%2520128k%252C%2520it%2520achieves%2520a%250Aspeedup%2520of%25201.44%2524%255Ctimes%2524%2520while%2520maintaining%2520higher%2520recall%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnchorAttention%3A%20Difference-Aware%20Sparse%20Attention%20with%20Stripe%0A%20%20Granularity&entry.906535625=Yu%20Zhang%20and%20Dong%20Guo%20and%20Fang%20Wu%20and%20Guoliang%20Zhu%20and%20Dian%20Ding%20and%20Yiming%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20with%20extended%20context%20lengths%20face%20significant%0Acomputational%20challenges%20during%20the%20pre-filling%20phase%2C%20primarily%20due%20to%20the%0Aquadratic%20complexity%20of%20self-attention.%20Existing%20methods%20typically%20employ%0Adynamic%20pattern%20matching%20and%20block-sparse%20low-level%20implementations.%20However%2C%0Atheir%20reliance%20on%20local%20information%20for%20pattern%20identification%20fails%20to%20capture%0Aglobal%20contexts%2C%20and%20the%20coarse%20granularity%20of%20blocks%20leads%20to%20persistent%0Ainternal%20sparsity%2C%20resulting%20in%20suboptimal%20accuracy%20and%20efficiency.%20To%20address%0Athese%20limitations%2C%20we%20propose%20%5Ctextbf%7BAnchorAttention%7D%2C%20a%20difference-aware%2C%0Adynamic%20sparse%20attention%20mechanism%20that%20efficiently%20identifies%20critical%0Aattention%20regions%20at%20a%20finer%20stripe%20granularity%20while%20adapting%20to%20global%0Acontextual%20information%2C%20achieving%20superior%20speed%20and%20accuracy.%20AnchorAttention%0Acomprises%20three%20key%20components%3A%20%281%29%20%5Ctextbf%7BPattern-based%20Anchor%20Computation%7D%2C%0Aleveraging%20the%20commonalities%20present%20across%20all%20inputs%20to%20rapidly%20compute%20a%20set%0Aof%20near-maximum%20scores%20as%20the%20anchor%3B%20%282%29%20%5Ctextbf%7BDifference-aware%20Stripe%0ASparsity%20Identification%7D%2C%20performing%20difference-aware%20comparisons%20with%20the%0Aanchor%20to%20quickly%20obtain%20discrete%20coordinates%20of%20significant%20regions%20in%20a%0Astripe-like%20sparsity%20pattern%3B%20%283%29%20%5Ctextbf%7BFine-grained%20Sparse%20Computation%7D%2C%0Areplacing%20the%20traditional%20contiguous%20KV%20block%20loading%20approach%20with%0Asimultaneous%20discrete%20KV%20position%20loading%20to%20maximize%20sparsity%20rates%20while%0Apreserving%20full%20hardware%20computational%20potential.%20With%20its%20finer-grained%0Asparsity%20strategy%2C%20%5Ctextbf%7BAnchorAttention%7D%20achieves%20higher%20sparsity%20rates%20at%0Athe%20same%20recall%20level%2C%20significantly%20reducing%20computation%20time.%20Compared%20to%0Aprevious%20state-of-the-art%20methods%2C%20at%20a%20text%20length%20of%20128k%2C%20it%20achieves%20a%0Aspeedup%20of%201.44%24%5Ctimes%24%20while%20maintaining%20higher%20recall%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23520v1&entry.124074799=Read"},
{"title": "Continuous Chain of Thought Enables Parallel Exploration and Reasoning", "author": "Halil Alperen Gozeten and M. Emrullah Ildiz and Xuechen Zhang and Hrayr Harutyunyan and Ankit Singh Rawat and Samet Oymak", "abstract": "  Current language models generate chain-of-thought traces by autoregressively\nsampling tokens from a finite vocabulary. While this discrete sampling has\nachieved remarkable success, conducting chain-of-thought with\ncontinuously-valued tokens (CoT2) offers a richer and more expressive\nalternative. Our work examines the benefits of CoT2 through logical reasoning\ntasks that inherently require search capabilities and provide optimization and\nexploration methods for CoT2. Theoretically, we show that CoT2 allows the model\nto track multiple traces in parallel and quantify its benefits for inference\nefficiency. Notably, one layer transformer equipped with CoT2 can provably\nsolve the combinatorial \"subset sum problem\" given sufficient embedding\ndimension. These insights lead to a novel and effective supervision strategy\nwhere we match the softmax outputs to the empirical token distributions of a\nset of target traces. Complementing this, we introduce sampling strategies that\nunlock policy optimization and self-improvement for CoT2. Our first strategy\nsamples and composes $K$ discrete tokens at each decoding step to control the\nlevel of parallelism, and reduces to standard CoT when $K=1$. Our second\nstrategy relies on continuous exploration over the probability simplex.\nExperiments confirm that policy optimization with CoT2 indeed improves the\nperformance of the model beyond its initial discrete or continuous supervision.\n", "link": "http://arxiv.org/abs/2505.23648v1", "date": "2025-05-29", "relevancy": 2.5482, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Chain%20of%20Thought%20Enables%20Parallel%20Exploration%20and%20Reasoning&body=Title%3A%20Continuous%20Chain%20of%20Thought%20Enables%20Parallel%20Exploration%20and%20Reasoning%0AAuthor%3A%20Halil%20Alperen%20Gozeten%20and%20M.%20Emrullah%20Ildiz%20and%20Xuechen%20Zhang%20and%20Hrayr%20Harutyunyan%20and%20Ankit%20Singh%20Rawat%20and%20Samet%20Oymak%0AAbstract%3A%20%20%20Current%20language%20models%20generate%20chain-of-thought%20traces%20by%20autoregressively%0Asampling%20tokens%20from%20a%20finite%20vocabulary.%20While%20this%20discrete%20sampling%20has%0Aachieved%20remarkable%20success%2C%20conducting%20chain-of-thought%20with%0Acontinuously-valued%20tokens%20%28CoT2%29%20offers%20a%20richer%20and%20more%20expressive%0Aalternative.%20Our%20work%20examines%20the%20benefits%20of%20CoT2%20through%20logical%20reasoning%0Atasks%20that%20inherently%20require%20search%20capabilities%20and%20provide%20optimization%20and%0Aexploration%20methods%20for%20CoT2.%20Theoretically%2C%20we%20show%20that%20CoT2%20allows%20the%20model%0Ato%20track%20multiple%20traces%20in%20parallel%20and%20quantify%20its%20benefits%20for%20inference%0Aefficiency.%20Notably%2C%20one%20layer%20transformer%20equipped%20with%20CoT2%20can%20provably%0Asolve%20the%20combinatorial%20%22subset%20sum%20problem%22%20given%20sufficient%20embedding%0Adimension.%20These%20insights%20lead%20to%20a%20novel%20and%20effective%20supervision%20strategy%0Awhere%20we%20match%20the%20softmax%20outputs%20to%20the%20empirical%20token%20distributions%20of%20a%0Aset%20of%20target%20traces.%20Complementing%20this%2C%20we%20introduce%20sampling%20strategies%20that%0Aunlock%20policy%20optimization%20and%20self-improvement%20for%20CoT2.%20Our%20first%20strategy%0Asamples%20and%20composes%20%24K%24%20discrete%20tokens%20at%20each%20decoding%20step%20to%20control%20the%0Alevel%20of%20parallelism%2C%20and%20reduces%20to%20standard%20CoT%20when%20%24K%3D1%24.%20Our%20second%0Astrategy%20relies%20on%20continuous%20exploration%20over%20the%20probability%20simplex.%0AExperiments%20confirm%20that%20policy%20optimization%20with%20CoT2%20indeed%20improves%20the%0Aperformance%20of%20the%20model%20beyond%20its%20initial%20discrete%20or%20continuous%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Chain%2520of%2520Thought%2520Enables%2520Parallel%2520Exploration%2520and%2520Reasoning%26entry.906535625%3DHalil%2520Alperen%2520Gozeten%2520and%2520M.%2520Emrullah%2520Ildiz%2520and%2520Xuechen%2520Zhang%2520and%2520Hrayr%2520Harutyunyan%2520and%2520Ankit%2520Singh%2520Rawat%2520and%2520Samet%2520Oymak%26entry.1292438233%3D%2520%2520Current%2520language%2520models%2520generate%2520chain-of-thought%2520traces%2520by%2520autoregressively%250Asampling%2520tokens%2520from%2520a%2520finite%2520vocabulary.%2520While%2520this%2520discrete%2520sampling%2520has%250Aachieved%2520remarkable%2520success%252C%2520conducting%2520chain-of-thought%2520with%250Acontinuously-valued%2520tokens%2520%2528CoT2%2529%2520offers%2520a%2520richer%2520and%2520more%2520expressive%250Aalternative.%2520Our%2520work%2520examines%2520the%2520benefits%2520of%2520CoT2%2520through%2520logical%2520reasoning%250Atasks%2520that%2520inherently%2520require%2520search%2520capabilities%2520and%2520provide%2520optimization%2520and%250Aexploration%2520methods%2520for%2520CoT2.%2520Theoretically%252C%2520we%2520show%2520that%2520CoT2%2520allows%2520the%2520model%250Ato%2520track%2520multiple%2520traces%2520in%2520parallel%2520and%2520quantify%2520its%2520benefits%2520for%2520inference%250Aefficiency.%2520Notably%252C%2520one%2520layer%2520transformer%2520equipped%2520with%2520CoT2%2520can%2520provably%250Asolve%2520the%2520combinatorial%2520%2522subset%2520sum%2520problem%2522%2520given%2520sufficient%2520embedding%250Adimension.%2520These%2520insights%2520lead%2520to%2520a%2520novel%2520and%2520effective%2520supervision%2520strategy%250Awhere%2520we%2520match%2520the%2520softmax%2520outputs%2520to%2520the%2520empirical%2520token%2520distributions%2520of%2520a%250Aset%2520of%2520target%2520traces.%2520Complementing%2520this%252C%2520we%2520introduce%2520sampling%2520strategies%2520that%250Aunlock%2520policy%2520optimization%2520and%2520self-improvement%2520for%2520CoT2.%2520Our%2520first%2520strategy%250Asamples%2520and%2520composes%2520%2524K%2524%2520discrete%2520tokens%2520at%2520each%2520decoding%2520step%2520to%2520control%2520the%250Alevel%2520of%2520parallelism%252C%2520and%2520reduces%2520to%2520standard%2520CoT%2520when%2520%2524K%253D1%2524.%2520Our%2520second%250Astrategy%2520relies%2520on%2520continuous%2520exploration%2520over%2520the%2520probability%2520simplex.%250AExperiments%2520confirm%2520that%2520policy%2520optimization%2520with%2520CoT2%2520indeed%2520improves%2520the%250Aperformance%2520of%2520the%2520model%2520beyond%2520its%2520initial%2520discrete%2520or%2520continuous%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Chain%20of%20Thought%20Enables%20Parallel%20Exploration%20and%20Reasoning&entry.906535625=Halil%20Alperen%20Gozeten%20and%20M.%20Emrullah%20Ildiz%20and%20Xuechen%20Zhang%20and%20Hrayr%20Harutyunyan%20and%20Ankit%20Singh%20Rawat%20and%20Samet%20Oymak&entry.1292438233=%20%20Current%20language%20models%20generate%20chain-of-thought%20traces%20by%20autoregressively%0Asampling%20tokens%20from%20a%20finite%20vocabulary.%20While%20this%20discrete%20sampling%20has%0Aachieved%20remarkable%20success%2C%20conducting%20chain-of-thought%20with%0Acontinuously-valued%20tokens%20%28CoT2%29%20offers%20a%20richer%20and%20more%20expressive%0Aalternative.%20Our%20work%20examines%20the%20benefits%20of%20CoT2%20through%20logical%20reasoning%0Atasks%20that%20inherently%20require%20search%20capabilities%20and%20provide%20optimization%20and%0Aexploration%20methods%20for%20CoT2.%20Theoretically%2C%20we%20show%20that%20CoT2%20allows%20the%20model%0Ato%20track%20multiple%20traces%20in%20parallel%20and%20quantify%20its%20benefits%20for%20inference%0Aefficiency.%20Notably%2C%20one%20layer%20transformer%20equipped%20with%20CoT2%20can%20provably%0Asolve%20the%20combinatorial%20%22subset%20sum%20problem%22%20given%20sufficient%20embedding%0Adimension.%20These%20insights%20lead%20to%20a%20novel%20and%20effective%20supervision%20strategy%0Awhere%20we%20match%20the%20softmax%20outputs%20to%20the%20empirical%20token%20distributions%20of%20a%0Aset%20of%20target%20traces.%20Complementing%20this%2C%20we%20introduce%20sampling%20strategies%20that%0Aunlock%20policy%20optimization%20and%20self-improvement%20for%20CoT2.%20Our%20first%20strategy%0Asamples%20and%20composes%20%24K%24%20discrete%20tokens%20at%20each%20decoding%20step%20to%20control%20the%0Alevel%20of%20parallelism%2C%20and%20reduces%20to%20standard%20CoT%20when%20%24K%3D1%24.%20Our%20second%0Astrategy%20relies%20on%20continuous%20exploration%20over%20the%20probability%20simplex.%0AExperiments%20confirm%20that%20policy%20optimization%20with%20CoT2%20indeed%20improves%20the%0Aperformance%20of%20the%20model%20beyond%20its%20initial%20discrete%20or%20continuous%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23648v1&entry.124074799=Read"},
{"title": "Domain-Aware Tensor Network Structure Search", "author": "Giorgos Iacovides and Wuyang Zhou and Chao Li and Qibin Zhao and Danilo Mandic", "abstract": "  Tensor networks (TNs) provide efficient representations of high-dimensional\ndata, yet identification of the optimal TN structures, the so called tensor\nnetwork structure search (TN-SS) problem, remains a challenge. Current\nstate-of-the-art (SOTA) algorithms are computationally expensive as they\nrequire extensive function evaluations, which is prohibitive for real-world\napplications. In addition, existing methods ignore valuable domain information\ninherent in real-world tensor data and lack transparency in their identified TN\nstructures. To this end, we propose a novel TN-SS framework, termed the tnLLM,\nwhich incorporates domain information about the data and harnesses the\nreasoning capabilities of large language models (LLMs) to directly predict\nsuitable TN structures. The proposed framework involves a domain-aware\nprompting pipeline which instructs the LLM to infer suitable TN structures\nbased on the real-world relationships between tensor modes. In this way, our\napproach is capable of not only iteratively optimizing the objective function,\nbut also generating domain-aware explanations for the identified structures.\nExperimental results demonstrate that tnLLM achieves comparable TN-SS objective\nfunction values with much fewer function evaluations compared to SOTA\nalgorithms. Furthermore, we demonstrate that the LLM-enabled domain information\ncan be used to find good initializations in the search space for sampling-based\nSOTA methods to accelerate their convergence while preserving theoretical\nperformance guarantees.\n", "link": "http://arxiv.org/abs/2505.23537v1", "date": "2025-05-29", "relevancy": 2.5382, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Aware%20Tensor%20Network%20Structure%20Search&body=Title%3A%20Domain-Aware%20Tensor%20Network%20Structure%20Search%0AAuthor%3A%20Giorgos%20Iacovides%20and%20Wuyang%20Zhou%20and%20Chao%20Li%20and%20Qibin%20Zhao%20and%20Danilo%20Mandic%0AAbstract%3A%20%20%20Tensor%20networks%20%28TNs%29%20provide%20efficient%20representations%20of%20high-dimensional%0Adata%2C%20yet%20identification%20of%20the%20optimal%20TN%20structures%2C%20the%20so%20called%20tensor%0Anetwork%20structure%20search%20%28TN-SS%29%20problem%2C%20remains%20a%20challenge.%20Current%0Astate-of-the-art%20%28SOTA%29%20algorithms%20are%20computationally%20expensive%20as%20they%0Arequire%20extensive%20function%20evaluations%2C%20which%20is%20prohibitive%20for%20real-world%0Aapplications.%20In%20addition%2C%20existing%20methods%20ignore%20valuable%20domain%20information%0Ainherent%20in%20real-world%20tensor%20data%20and%20lack%20transparency%20in%20their%20identified%20TN%0Astructures.%20To%20this%20end%2C%20we%20propose%20a%20novel%20TN-SS%20framework%2C%20termed%20the%20tnLLM%2C%0Awhich%20incorporates%20domain%20information%20about%20the%20data%20and%20harnesses%20the%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20to%20directly%20predict%0Asuitable%20TN%20structures.%20The%20proposed%20framework%20involves%20a%20domain-aware%0Aprompting%20pipeline%20which%20instructs%20the%20LLM%20to%20infer%20suitable%20TN%20structures%0Abased%20on%20the%20real-world%20relationships%20between%20tensor%20modes.%20In%20this%20way%2C%20our%0Aapproach%20is%20capable%20of%20not%20only%20iteratively%20optimizing%20the%20objective%20function%2C%0Abut%20also%20generating%20domain-aware%20explanations%20for%20the%20identified%20structures.%0AExperimental%20results%20demonstrate%20that%20tnLLM%20achieves%20comparable%20TN-SS%20objective%0Afunction%20values%20with%20much%20fewer%20function%20evaluations%20compared%20to%20SOTA%0Aalgorithms.%20Furthermore%2C%20we%20demonstrate%20that%20the%20LLM-enabled%20domain%20information%0Acan%20be%20used%20to%20find%20good%20initializations%20in%20the%20search%20space%20for%20sampling-based%0ASOTA%20methods%20to%20accelerate%20their%20convergence%20while%20preserving%20theoretical%0Aperformance%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Aware%2520Tensor%2520Network%2520Structure%2520Search%26entry.906535625%3DGiorgos%2520Iacovides%2520and%2520Wuyang%2520Zhou%2520and%2520Chao%2520Li%2520and%2520Qibin%2520Zhao%2520and%2520Danilo%2520Mandic%26entry.1292438233%3D%2520%2520Tensor%2520networks%2520%2528TNs%2529%2520provide%2520efficient%2520representations%2520of%2520high-dimensional%250Adata%252C%2520yet%2520identification%2520of%2520the%2520optimal%2520TN%2520structures%252C%2520the%2520so%2520called%2520tensor%250Anetwork%2520structure%2520search%2520%2528TN-SS%2529%2520problem%252C%2520remains%2520a%2520challenge.%2520Current%250Astate-of-the-art%2520%2528SOTA%2529%2520algorithms%2520are%2520computationally%2520expensive%2520as%2520they%250Arequire%2520extensive%2520function%2520evaluations%252C%2520which%2520is%2520prohibitive%2520for%2520real-world%250Aapplications.%2520In%2520addition%252C%2520existing%2520methods%2520ignore%2520valuable%2520domain%2520information%250Ainherent%2520in%2520real-world%2520tensor%2520data%2520and%2520lack%2520transparency%2520in%2520their%2520identified%2520TN%250Astructures.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520TN-SS%2520framework%252C%2520termed%2520the%2520tnLLM%252C%250Awhich%2520incorporates%2520domain%2520information%2520about%2520the%2520data%2520and%2520harnesses%2520the%250Areasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520directly%2520predict%250Asuitable%2520TN%2520structures.%2520The%2520proposed%2520framework%2520involves%2520a%2520domain-aware%250Aprompting%2520pipeline%2520which%2520instructs%2520the%2520LLM%2520to%2520infer%2520suitable%2520TN%2520structures%250Abased%2520on%2520the%2520real-world%2520relationships%2520between%2520tensor%2520modes.%2520In%2520this%2520way%252C%2520our%250Aapproach%2520is%2520capable%2520of%2520not%2520only%2520iteratively%2520optimizing%2520the%2520objective%2520function%252C%250Abut%2520also%2520generating%2520domain-aware%2520explanations%2520for%2520the%2520identified%2520structures.%250AExperimental%2520results%2520demonstrate%2520that%2520tnLLM%2520achieves%2520comparable%2520TN-SS%2520objective%250Afunction%2520values%2520with%2520much%2520fewer%2520function%2520evaluations%2520compared%2520to%2520SOTA%250Aalgorithms.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520the%2520LLM-enabled%2520domain%2520information%250Acan%2520be%2520used%2520to%2520find%2520good%2520initializations%2520in%2520the%2520search%2520space%2520for%2520sampling-based%250ASOTA%2520methods%2520to%2520accelerate%2520their%2520convergence%2520while%2520preserving%2520theoretical%250Aperformance%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Aware%20Tensor%20Network%20Structure%20Search&entry.906535625=Giorgos%20Iacovides%20and%20Wuyang%20Zhou%20and%20Chao%20Li%20and%20Qibin%20Zhao%20and%20Danilo%20Mandic&entry.1292438233=%20%20Tensor%20networks%20%28TNs%29%20provide%20efficient%20representations%20of%20high-dimensional%0Adata%2C%20yet%20identification%20of%20the%20optimal%20TN%20structures%2C%20the%20so%20called%20tensor%0Anetwork%20structure%20search%20%28TN-SS%29%20problem%2C%20remains%20a%20challenge.%20Current%0Astate-of-the-art%20%28SOTA%29%20algorithms%20are%20computationally%20expensive%20as%20they%0Arequire%20extensive%20function%20evaluations%2C%20which%20is%20prohibitive%20for%20real-world%0Aapplications.%20In%20addition%2C%20existing%20methods%20ignore%20valuable%20domain%20information%0Ainherent%20in%20real-world%20tensor%20data%20and%20lack%20transparency%20in%20their%20identified%20TN%0Astructures.%20To%20this%20end%2C%20we%20propose%20a%20novel%20TN-SS%20framework%2C%20termed%20the%20tnLLM%2C%0Awhich%20incorporates%20domain%20information%20about%20the%20data%20and%20harnesses%20the%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20to%20directly%20predict%0Asuitable%20TN%20structures.%20The%20proposed%20framework%20involves%20a%20domain-aware%0Aprompting%20pipeline%20which%20instructs%20the%20LLM%20to%20infer%20suitable%20TN%20structures%0Abased%20on%20the%20real-world%20relationships%20between%20tensor%20modes.%20In%20this%20way%2C%20our%0Aapproach%20is%20capable%20of%20not%20only%20iteratively%20optimizing%20the%20objective%20function%2C%0Abut%20also%20generating%20domain-aware%20explanations%20for%20the%20identified%20structures.%0AExperimental%20results%20demonstrate%20that%20tnLLM%20achieves%20comparable%20TN-SS%20objective%0Afunction%20values%20with%20much%20fewer%20function%20evaluations%20compared%20to%20SOTA%0Aalgorithms.%20Furthermore%2C%20we%20demonstrate%20that%20the%20LLM-enabled%20domain%20information%0Acan%20be%20used%20to%20find%20good%20initializations%20in%20the%20search%20space%20for%20sampling-based%0ASOTA%20methods%20to%20accelerate%20their%20convergence%20while%20preserving%20theoretical%0Aperformance%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23537v1&entry.124074799=Read"},
{"title": "Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot\n  Transfer Reinforcement Learning", "author": "Chi Zhang and Ziying Jia and George K. Atia and Sihong He and Yue Wang", "abstract": "  Transfer reinforcement learning aims to derive a near-optimal policy for a\ntarget environment with limited data by leveraging abundant data from related\nsource domains. However, it faces two key challenges: the lack of performance\nguarantees for the transferred policy, which can lead to undesired actions, and\nthe risk of negative transfer when multiple source domains are involved. We\npropose a novel framework based on the pessimism principle, which constructs\nand optimizes a conservative estimation of the target domain's performance. Our\nframework effectively addresses the two challenges by providing an optimized\nlower bound on target performance, ensuring safe and reliable decisions, and by\nexhibiting monotonic improvement with respect to the quality of the source\ndomains, thereby avoiding negative transfer. We construct two types of\nconservative estimations, rigorously characterize their effectiveness, and\ndevelop efficient distributed algorithms with convergence guarantees. Our\nframework provides a theoretically sound and practically robust solution for\ntransfer learning in reinforcement learning.\n", "link": "http://arxiv.org/abs/2505.18447v2", "date": "2025-05-29", "relevancy": 2.5294, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5097}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5079}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pessimism%20Principle%20Can%20Be%20Effective%3A%20Towards%20a%20Framework%20for%20Zero-Shot%0A%20%20Transfer%20Reinforcement%20Learning&body=Title%3A%20Pessimism%20Principle%20Can%20Be%20Effective%3A%20Towards%20a%20Framework%20for%20Zero-Shot%0A%20%20Transfer%20Reinforcement%20Learning%0AAuthor%3A%20Chi%20Zhang%20and%20Ziying%20Jia%20and%20George%20K.%20Atia%20and%20Sihong%20He%20and%20Yue%20Wang%0AAbstract%3A%20%20%20Transfer%20reinforcement%20learning%20aims%20to%20derive%20a%20near-optimal%20policy%20for%20a%0Atarget%20environment%20with%20limited%20data%20by%20leveraging%20abundant%20data%20from%20related%0Asource%20domains.%20However%2C%20it%20faces%20two%20key%20challenges%3A%20the%20lack%20of%20performance%0Aguarantees%20for%20the%20transferred%20policy%2C%20which%20can%20lead%20to%20undesired%20actions%2C%20and%0Athe%20risk%20of%20negative%20transfer%20when%20multiple%20source%20domains%20are%20involved.%20We%0Apropose%20a%20novel%20framework%20based%20on%20the%20pessimism%20principle%2C%20which%20constructs%0Aand%20optimizes%20a%20conservative%20estimation%20of%20the%20target%20domain%27s%20performance.%20Our%0Aframework%20effectively%20addresses%20the%20two%20challenges%20by%20providing%20an%20optimized%0Alower%20bound%20on%20target%20performance%2C%20ensuring%20safe%20and%20reliable%20decisions%2C%20and%20by%0Aexhibiting%20monotonic%20improvement%20with%20respect%20to%20the%20quality%20of%20the%20source%0Adomains%2C%20thereby%20avoiding%20negative%20transfer.%20We%20construct%20two%20types%20of%0Aconservative%20estimations%2C%20rigorously%20characterize%20their%20effectiveness%2C%20and%0Adevelop%20efficient%20distributed%20algorithms%20with%20convergence%20guarantees.%20Our%0Aframework%20provides%20a%20theoretically%20sound%20and%20practically%20robust%20solution%20for%0Atransfer%20learning%20in%20reinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18447v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPessimism%2520Principle%2520Can%2520Be%2520Effective%253A%2520Towards%2520a%2520Framework%2520for%2520Zero-Shot%250A%2520%2520Transfer%2520Reinforcement%2520Learning%26entry.906535625%3DChi%2520Zhang%2520and%2520Ziying%2520Jia%2520and%2520George%2520K.%2520Atia%2520and%2520Sihong%2520He%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520Transfer%2520reinforcement%2520learning%2520aims%2520to%2520derive%2520a%2520near-optimal%2520policy%2520for%2520a%250Atarget%2520environment%2520with%2520limited%2520data%2520by%2520leveraging%2520abundant%2520data%2520from%2520related%250Asource%2520domains.%2520However%252C%2520it%2520faces%2520two%2520key%2520challenges%253A%2520the%2520lack%2520of%2520performance%250Aguarantees%2520for%2520the%2520transferred%2520policy%252C%2520which%2520can%2520lead%2520to%2520undesired%2520actions%252C%2520and%250Athe%2520risk%2520of%2520negative%2520transfer%2520when%2520multiple%2520source%2520domains%2520are%2520involved.%2520We%250Apropose%2520a%2520novel%2520framework%2520based%2520on%2520the%2520pessimism%2520principle%252C%2520which%2520constructs%250Aand%2520optimizes%2520a%2520conservative%2520estimation%2520of%2520the%2520target%2520domain%2527s%2520performance.%2520Our%250Aframework%2520effectively%2520addresses%2520the%2520two%2520challenges%2520by%2520providing%2520an%2520optimized%250Alower%2520bound%2520on%2520target%2520performance%252C%2520ensuring%2520safe%2520and%2520reliable%2520decisions%252C%2520and%2520by%250Aexhibiting%2520monotonic%2520improvement%2520with%2520respect%2520to%2520the%2520quality%2520of%2520the%2520source%250Adomains%252C%2520thereby%2520avoiding%2520negative%2520transfer.%2520We%2520construct%2520two%2520types%2520of%250Aconservative%2520estimations%252C%2520rigorously%2520characterize%2520their%2520effectiveness%252C%2520and%250Adevelop%2520efficient%2520distributed%2520algorithms%2520with%2520convergence%2520guarantees.%2520Our%250Aframework%2520provides%2520a%2520theoretically%2520sound%2520and%2520practically%2520robust%2520solution%2520for%250Atransfer%2520learning%2520in%2520reinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18447v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pessimism%20Principle%20Can%20Be%20Effective%3A%20Towards%20a%20Framework%20for%20Zero-Shot%0A%20%20Transfer%20Reinforcement%20Learning&entry.906535625=Chi%20Zhang%20and%20Ziying%20Jia%20and%20George%20K.%20Atia%20and%20Sihong%20He%20and%20Yue%20Wang&entry.1292438233=%20%20Transfer%20reinforcement%20learning%20aims%20to%20derive%20a%20near-optimal%20policy%20for%20a%0Atarget%20environment%20with%20limited%20data%20by%20leveraging%20abundant%20data%20from%20related%0Asource%20domains.%20However%2C%20it%20faces%20two%20key%20challenges%3A%20the%20lack%20of%20performance%0Aguarantees%20for%20the%20transferred%20policy%2C%20which%20can%20lead%20to%20undesired%20actions%2C%20and%0Athe%20risk%20of%20negative%20transfer%20when%20multiple%20source%20domains%20are%20involved.%20We%0Apropose%20a%20novel%20framework%20based%20on%20the%20pessimism%20principle%2C%20which%20constructs%0Aand%20optimizes%20a%20conservative%20estimation%20of%20the%20target%20domain%27s%20performance.%20Our%0Aframework%20effectively%20addresses%20the%20two%20challenges%20by%20providing%20an%20optimized%0Alower%20bound%20on%20target%20performance%2C%20ensuring%20safe%20and%20reliable%20decisions%2C%20and%20by%0Aexhibiting%20monotonic%20improvement%20with%20respect%20to%20the%20quality%20of%20the%20source%0Adomains%2C%20thereby%20avoiding%20negative%20transfer.%20We%20construct%20two%20types%20of%0Aconservative%20estimations%2C%20rigorously%20characterize%20their%20effectiveness%2C%20and%0Adevelop%20efficient%20distributed%20algorithms%20with%20convergence%20guarantees.%20Our%0Aframework%20provides%20a%20theoretically%20sound%20and%20practically%20robust%20solution%20for%0Atransfer%20learning%20in%20reinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18447v2&entry.124074799=Read"},
{"title": "FlexEvent: Towards Flexible Event-Frame Object Detection at Varying\n  Operational Frequencies", "author": "Dongyue Lu and Lingdong Kong and Gim Hee Lee and Camille Simon Chane and Wei Tsang Ooi", "abstract": "  Event cameras offer unparalleled advantages for real-time perception in\ndynamic environments, thanks to the microsecond-level temporal resolution and\nasynchronous operation. Existing event detectors, however, are limited by\nfixed-frequency paradigms and fail to fully exploit the high-temporal\nresolution and adaptability of event data. To address these limitations, we\npropose FlexEvent, a novel framework that enables detection at varying\nfrequencies. Our approach consists of two key components: FlexFuse, an adaptive\nevent-frame fusion module that integrates high-frequency event data with rich\nsemantic information from RGB frames, and FlexTune, a frequency-adaptive\nfine-tuning mechanism that generates frequency-adjusted labels to enhance model\ngeneralization across varying operational frequencies. This combination allows\nour method to detect objects with high accuracy in both fast-moving and static\nscenarios, while adapting to dynamic environments. Extensive experiments on\nlarge-scale event camera datasets demonstrate that our approach surpasses\nstate-of-the-art methods, achieving significant improvements in both standard\nand high-frequency settings. Notably, our method maintains robust performance\nwhen scaling from 20 Hz to 90 Hz and delivers accurate detection up to 180 Hz,\nproving its effectiveness in extreme conditions. Our framework sets a new\nbenchmark for event-based object detection and paves the way for more\nadaptable, real-time vision systems. Code is publicly available.\n", "link": "http://arxiv.org/abs/2412.06708v2", "date": "2025-05-29", "relevancy": 2.523, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5147}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.508}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexEvent%3A%20Towards%20Flexible%20Event-Frame%20Object%20Detection%20at%20Varying%0A%20%20Operational%20Frequencies&body=Title%3A%20FlexEvent%3A%20Towards%20Flexible%20Event-Frame%20Object%20Detection%20at%20Varying%0A%20%20Operational%20Frequencies%0AAuthor%3A%20Dongyue%20Lu%20and%20Lingdong%20Kong%20and%20Gim%20Hee%20Lee%20and%20Camille%20Simon%20Chane%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%20Event%20cameras%20offer%20unparalleled%20advantages%20for%20real-time%20perception%20in%0Adynamic%20environments%2C%20thanks%20to%20the%20microsecond-level%20temporal%20resolution%20and%0Aasynchronous%20operation.%20Existing%20event%20detectors%2C%20however%2C%20are%20limited%20by%0Afixed-frequency%20paradigms%20and%20fail%20to%20fully%20exploit%20the%20high-temporal%0Aresolution%20and%20adaptability%20of%20event%20data.%20To%20address%20these%20limitations%2C%20we%0Apropose%20FlexEvent%2C%20a%20novel%20framework%20that%20enables%20detection%20at%20varying%0Afrequencies.%20Our%20approach%20consists%20of%20two%20key%20components%3A%20FlexFuse%2C%20an%20adaptive%0Aevent-frame%20fusion%20module%20that%20integrates%20high-frequency%20event%20data%20with%20rich%0Asemantic%20information%20from%20RGB%20frames%2C%20and%20FlexTune%2C%20a%20frequency-adaptive%0Afine-tuning%20mechanism%20that%20generates%20frequency-adjusted%20labels%20to%20enhance%20model%0Ageneralization%20across%20varying%20operational%20frequencies.%20This%20combination%20allows%0Aour%20method%20to%20detect%20objects%20with%20high%20accuracy%20in%20both%20fast-moving%20and%20static%0Ascenarios%2C%20while%20adapting%20to%20dynamic%20environments.%20Extensive%20experiments%20on%0Alarge-scale%20event%20camera%20datasets%20demonstrate%20that%20our%20approach%20surpasses%0Astate-of-the-art%20methods%2C%20achieving%20significant%20improvements%20in%20both%20standard%0Aand%20high-frequency%20settings.%20Notably%2C%20our%20method%20maintains%20robust%20performance%0Awhen%20scaling%20from%2020%20Hz%20to%2090%20Hz%20and%20delivers%20accurate%20detection%20up%20to%20180%20Hz%2C%0Aproving%20its%20effectiveness%20in%20extreme%20conditions.%20Our%20framework%20sets%20a%20new%0Abenchmark%20for%20event-based%20object%20detection%20and%20paves%20the%20way%20for%20more%0Aadaptable%2C%20real-time%20vision%20systems.%20Code%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexEvent%253A%2520Towards%2520Flexible%2520Event-Frame%2520Object%2520Detection%2520at%2520Varying%250A%2520%2520Operational%2520Frequencies%26entry.906535625%3DDongyue%2520Lu%2520and%2520Lingdong%2520Kong%2520and%2520Gim%2520Hee%2520Lee%2520and%2520Camille%2520Simon%2520Chane%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520unparalleled%2520advantages%2520for%2520real-time%2520perception%2520in%250Adynamic%2520environments%252C%2520thanks%2520to%2520the%2520microsecond-level%2520temporal%2520resolution%2520and%250Aasynchronous%2520operation.%2520Existing%2520event%2520detectors%252C%2520however%252C%2520are%2520limited%2520by%250Afixed-frequency%2520paradigms%2520and%2520fail%2520to%2520fully%2520exploit%2520the%2520high-temporal%250Aresolution%2520and%2520adaptability%2520of%2520event%2520data.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520FlexEvent%252C%2520a%2520novel%2520framework%2520that%2520enables%2520detection%2520at%2520varying%250Afrequencies.%2520Our%2520approach%2520consists%2520of%2520two%2520key%2520components%253A%2520FlexFuse%252C%2520an%2520adaptive%250Aevent-frame%2520fusion%2520module%2520that%2520integrates%2520high-frequency%2520event%2520data%2520with%2520rich%250Asemantic%2520information%2520from%2520RGB%2520frames%252C%2520and%2520FlexTune%252C%2520a%2520frequency-adaptive%250Afine-tuning%2520mechanism%2520that%2520generates%2520frequency-adjusted%2520labels%2520to%2520enhance%2520model%250Ageneralization%2520across%2520varying%2520operational%2520frequencies.%2520This%2520combination%2520allows%250Aour%2520method%2520to%2520detect%2520objects%2520with%2520high%2520accuracy%2520in%2520both%2520fast-moving%2520and%2520static%250Ascenarios%252C%2520while%2520adapting%2520to%2520dynamic%2520environments.%2520Extensive%2520experiments%2520on%250Alarge-scale%2520event%2520camera%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520surpasses%250Astate-of-the-art%2520methods%252C%2520achieving%2520significant%2520improvements%2520in%2520both%2520standard%250Aand%2520high-frequency%2520settings.%2520Notably%252C%2520our%2520method%2520maintains%2520robust%2520performance%250Awhen%2520scaling%2520from%252020%2520Hz%2520to%252090%2520Hz%2520and%2520delivers%2520accurate%2520detection%2520up%2520to%2520180%2520Hz%252C%250Aproving%2520its%2520effectiveness%2520in%2520extreme%2520conditions.%2520Our%2520framework%2520sets%2520a%2520new%250Abenchmark%2520for%2520event-based%2520object%2520detection%2520and%2520paves%2520the%2520way%2520for%2520more%250Aadaptable%252C%2520real-time%2520vision%2520systems.%2520Code%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexEvent%3A%20Towards%20Flexible%20Event-Frame%20Object%20Detection%20at%20Varying%0A%20%20Operational%20Frequencies&entry.906535625=Dongyue%20Lu%20and%20Lingdong%20Kong%20and%20Gim%20Hee%20Lee%20and%20Camille%20Simon%20Chane%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%20Event%20cameras%20offer%20unparalleled%20advantages%20for%20real-time%20perception%20in%0Adynamic%20environments%2C%20thanks%20to%20the%20microsecond-level%20temporal%20resolution%20and%0Aasynchronous%20operation.%20Existing%20event%20detectors%2C%20however%2C%20are%20limited%20by%0Afixed-frequency%20paradigms%20and%20fail%20to%20fully%20exploit%20the%20high-temporal%0Aresolution%20and%20adaptability%20of%20event%20data.%20To%20address%20these%20limitations%2C%20we%0Apropose%20FlexEvent%2C%20a%20novel%20framework%20that%20enables%20detection%20at%20varying%0Afrequencies.%20Our%20approach%20consists%20of%20two%20key%20components%3A%20FlexFuse%2C%20an%20adaptive%0Aevent-frame%20fusion%20module%20that%20integrates%20high-frequency%20event%20data%20with%20rich%0Asemantic%20information%20from%20RGB%20frames%2C%20and%20FlexTune%2C%20a%20frequency-adaptive%0Afine-tuning%20mechanism%20that%20generates%20frequency-adjusted%20labels%20to%20enhance%20model%0Ageneralization%20across%20varying%20operational%20frequencies.%20This%20combination%20allows%0Aour%20method%20to%20detect%20objects%20with%20high%20accuracy%20in%20both%20fast-moving%20and%20static%0Ascenarios%2C%20while%20adapting%20to%20dynamic%20environments.%20Extensive%20experiments%20on%0Alarge-scale%20event%20camera%20datasets%20demonstrate%20that%20our%20approach%20surpasses%0Astate-of-the-art%20methods%2C%20achieving%20significant%20improvements%20in%20both%20standard%0Aand%20high-frequency%20settings.%20Notably%2C%20our%20method%20maintains%20robust%20performance%0Awhen%20scaling%20from%2020%20Hz%20to%2090%20Hz%20and%20delivers%20accurate%20detection%20up%20to%20180%20Hz%2C%0Aproving%20its%20effectiveness%20in%20extreme%20conditions.%20Our%20framework%20sets%20a%20new%0Abenchmark%20for%20event-based%20object%20detection%20and%20paves%20the%20way%20for%20more%0Aadaptable%2C%20real-time%20vision%20systems.%20Code%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06708v2&entry.124074799=Read"},
{"title": "Spectrotemporal Modulation: Efficient and Interpretable Feature\n  Representation for Classifying Speech, Music, and Environmental Sounds", "author": "Andrew Chang and Yike Li and Iran R. Roman and David Poeppel", "abstract": "  Audio DNNs have demonstrated impressive performance on various machine\nlistening tasks; however, most of their representations are computationally\ncostly and uninterpretable, leaving room for optimization. Here, we propose a\nnovel approach centered on spectrotemporal modulation (STM) features, a signal\nprocessing method that mimics the neurophysiological representation in the\nhuman auditory cortex. The classification performance of our STM-based model,\nwithout any pretraining, is comparable to that of pretrained audio DNNs across\ndiverse naturalistic speech, music, and environmental sounds, which are\nessential categories for both human cognition and machine perception. These\nresults show that STM is an efficient and interpretable feature representation\nfor audio classification, advancing the development of machine listening and\nunlocking exciting new possibilities for basic understanding of speech and\nauditory sciences, as well as developing audio BCI and cognitive computing.\n", "link": "http://arxiv.org/abs/2505.23509v1", "date": "2025-05-29", "relevancy": 2.5104, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectrotemporal%20Modulation%3A%20Efficient%20and%20Interpretable%20Feature%0A%20%20Representation%20for%20Classifying%20Speech%2C%20Music%2C%20and%20Environmental%20Sounds&body=Title%3A%20Spectrotemporal%20Modulation%3A%20Efficient%20and%20Interpretable%20Feature%0A%20%20Representation%20for%20Classifying%20Speech%2C%20Music%2C%20and%20Environmental%20Sounds%0AAuthor%3A%20Andrew%20Chang%20and%20Yike%20Li%20and%20Iran%20R.%20Roman%20and%20David%20Poeppel%0AAbstract%3A%20%20%20Audio%20DNNs%20have%20demonstrated%20impressive%20performance%20on%20various%20machine%0Alistening%20tasks%3B%20however%2C%20most%20of%20their%20representations%20are%20computationally%0Acostly%20and%20uninterpretable%2C%20leaving%20room%20for%20optimization.%20Here%2C%20we%20propose%20a%0Anovel%20approach%20centered%20on%20spectrotemporal%20modulation%20%28STM%29%20features%2C%20a%20signal%0Aprocessing%20method%20that%20mimics%20the%20neurophysiological%20representation%20in%20the%0Ahuman%20auditory%20cortex.%20The%20classification%20performance%20of%20our%20STM-based%20model%2C%0Awithout%20any%20pretraining%2C%20is%20comparable%20to%20that%20of%20pretrained%20audio%20DNNs%20across%0Adiverse%20naturalistic%20speech%2C%20music%2C%20and%20environmental%20sounds%2C%20which%20are%0Aessential%20categories%20for%20both%20human%20cognition%20and%20machine%20perception.%20These%0Aresults%20show%20that%20STM%20is%20an%20efficient%20and%20interpretable%20feature%20representation%0Afor%20audio%20classification%2C%20advancing%20the%20development%20of%20machine%20listening%20and%0Aunlocking%20exciting%20new%20possibilities%20for%20basic%20understanding%20of%20speech%20and%0Aauditory%20sciences%2C%20as%20well%20as%20developing%20audio%20BCI%20and%20cognitive%20computing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectrotemporal%2520Modulation%253A%2520Efficient%2520and%2520Interpretable%2520Feature%250A%2520%2520Representation%2520for%2520Classifying%2520Speech%252C%2520Music%252C%2520and%2520Environmental%2520Sounds%26entry.906535625%3DAndrew%2520Chang%2520and%2520Yike%2520Li%2520and%2520Iran%2520R.%2520Roman%2520and%2520David%2520Poeppel%26entry.1292438233%3D%2520%2520Audio%2520DNNs%2520have%2520demonstrated%2520impressive%2520performance%2520on%2520various%2520machine%250Alistening%2520tasks%253B%2520however%252C%2520most%2520of%2520their%2520representations%2520are%2520computationally%250Acostly%2520and%2520uninterpretable%252C%2520leaving%2520room%2520for%2520optimization.%2520Here%252C%2520we%2520propose%2520a%250Anovel%2520approach%2520centered%2520on%2520spectrotemporal%2520modulation%2520%2528STM%2529%2520features%252C%2520a%2520signal%250Aprocessing%2520method%2520that%2520mimics%2520the%2520neurophysiological%2520representation%2520in%2520the%250Ahuman%2520auditory%2520cortex.%2520The%2520classification%2520performance%2520of%2520our%2520STM-based%2520model%252C%250Awithout%2520any%2520pretraining%252C%2520is%2520comparable%2520to%2520that%2520of%2520pretrained%2520audio%2520DNNs%2520across%250Adiverse%2520naturalistic%2520speech%252C%2520music%252C%2520and%2520environmental%2520sounds%252C%2520which%2520are%250Aessential%2520categories%2520for%2520both%2520human%2520cognition%2520and%2520machine%2520perception.%2520These%250Aresults%2520show%2520that%2520STM%2520is%2520an%2520efficient%2520and%2520interpretable%2520feature%2520representation%250Afor%2520audio%2520classification%252C%2520advancing%2520the%2520development%2520of%2520machine%2520listening%2520and%250Aunlocking%2520exciting%2520new%2520possibilities%2520for%2520basic%2520understanding%2520of%2520speech%2520and%250Aauditory%2520sciences%252C%2520as%2520well%2520as%2520developing%2520audio%2520BCI%2520and%2520cognitive%2520computing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectrotemporal%20Modulation%3A%20Efficient%20and%20Interpretable%20Feature%0A%20%20Representation%20for%20Classifying%20Speech%2C%20Music%2C%20and%20Environmental%20Sounds&entry.906535625=Andrew%20Chang%20and%20Yike%20Li%20and%20Iran%20R.%20Roman%20and%20David%20Poeppel&entry.1292438233=%20%20Audio%20DNNs%20have%20demonstrated%20impressive%20performance%20on%20various%20machine%0Alistening%20tasks%3B%20however%2C%20most%20of%20their%20representations%20are%20computationally%0Acostly%20and%20uninterpretable%2C%20leaving%20room%20for%20optimization.%20Here%2C%20we%20propose%20a%0Anovel%20approach%20centered%20on%20spectrotemporal%20modulation%20%28STM%29%20features%2C%20a%20signal%0Aprocessing%20method%20that%20mimics%20the%20neurophysiological%20representation%20in%20the%0Ahuman%20auditory%20cortex.%20The%20classification%20performance%20of%20our%20STM-based%20model%2C%0Awithout%20any%20pretraining%2C%20is%20comparable%20to%20that%20of%20pretrained%20audio%20DNNs%20across%0Adiverse%20naturalistic%20speech%2C%20music%2C%20and%20environmental%20sounds%2C%20which%20are%0Aessential%20categories%20for%20both%20human%20cognition%20and%20machine%20perception.%20These%0Aresults%20show%20that%20STM%20is%20an%20efficient%20and%20interpretable%20feature%20representation%0Afor%20audio%20classification%2C%20advancing%20the%20development%20of%20machine%20listening%20and%0Aunlocking%20exciting%20new%20possibilities%20for%20basic%20understanding%20of%20speech%20and%0Aauditory%20sciences%2C%20as%20well%20as%20developing%20audio%20BCI%20and%20cognitive%20computing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23509v1&entry.124074799=Read"},
{"title": "DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View\n  Driving Scenes", "author": "Jianbiao Mei and Tao Hu and Xuemeng Yang and Licheng Wen and Yu Yang and Tiantian Wei and Yukai Ma and Min Dou and Botian Shi and Yong Liu", "abstract": "  Recent advances in diffusion models have improved controllable streetscape\ngeneration and supported downstream perception and planning tasks. However,\nchallenges remain in accurately modeling driving scenes and generating long\nvideos. To alleviate these issues, we propose DreamForge, an advanced\ndiffusion-based autoregressive video generation model tailored for\n3D-controllable long-term generation. To enhance the lane and foreground\ngeneration, we introduce perspective guidance and integrate object-wise\nposition encoding to incorporate local 3D correlation and improve foreground\nobject modeling. We also propose motion-aware temporal attention to capture\nmotion cues and appearance changes in videos. By leveraging motion frames and\nan autoregressive generation paradigm,we can autoregressively generate long\nvideos (over 200 frames) using a model trained in short sequences, achieving\nsuperior quality compared to the baseline in 16-frame video evaluations.\nFinally, we integrate our method with the realistic simulator DriveArena to\nprovide more reliable open-loop and closed-loop evaluations for vision-based\ndriving agents. Project Page:\nhttps://pjlab-adg.github.io/DriveArena/dreamforge.\n", "link": "http://arxiv.org/abs/2409.04003v4", "date": "2025-05-29", "relevancy": 2.4893, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6462}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6229}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamForge%3A%20Motion-Aware%20Autoregressive%20Video%20Generation%20for%20Multi-View%0A%20%20Driving%20Scenes&body=Title%3A%20DreamForge%3A%20Motion-Aware%20Autoregressive%20Video%20Generation%20for%20Multi-View%0A%20%20Driving%20Scenes%0AAuthor%3A%20Jianbiao%20Mei%20and%20Tao%20Hu%20and%20Xuemeng%20Yang%20and%20Licheng%20Wen%20and%20Yu%20Yang%20and%20Tiantian%20Wei%20and%20Yukai%20Ma%20and%20Min%20Dou%20and%20Botian%20Shi%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20improved%20controllable%20streetscape%0Ageneration%20and%20supported%20downstream%20perception%20and%20planning%20tasks.%20However%2C%0Achallenges%20remain%20in%20accurately%20modeling%20driving%20scenes%20and%20generating%20long%0Avideos.%20To%20alleviate%20these%20issues%2C%20we%20propose%20DreamForge%2C%20an%20advanced%0Adiffusion-based%20autoregressive%20video%20generation%20model%20tailored%20for%0A3D-controllable%20long-term%20generation.%20To%20enhance%20the%20lane%20and%20foreground%0Ageneration%2C%20we%20introduce%20perspective%20guidance%20and%20integrate%20object-wise%0Aposition%20encoding%20to%20incorporate%20local%203D%20correlation%20and%20improve%20foreground%0Aobject%20modeling.%20We%20also%20propose%20motion-aware%20temporal%20attention%20to%20capture%0Amotion%20cues%20and%20appearance%20changes%20in%20videos.%20By%20leveraging%20motion%20frames%20and%0Aan%20autoregressive%20generation%20paradigm%2Cwe%20can%20autoregressively%20generate%20long%0Avideos%20%28over%20200%20frames%29%20using%20a%20model%20trained%20in%20short%20sequences%2C%20achieving%0Asuperior%20quality%20compared%20to%20the%20baseline%20in%2016-frame%20video%20evaluations.%0AFinally%2C%20we%20integrate%20our%20method%20with%20the%20realistic%20simulator%20DriveArena%20to%0Aprovide%20more%20reliable%20open-loop%20and%20closed-loop%20evaluations%20for%20vision-based%0Adriving%20agents.%20Project%20Page%3A%0Ahttps%3A//pjlab-adg.github.io/DriveArena/dreamforge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04003v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamForge%253A%2520Motion-Aware%2520Autoregressive%2520Video%2520Generation%2520for%2520Multi-View%250A%2520%2520Driving%2520Scenes%26entry.906535625%3DJianbiao%2520Mei%2520and%2520Tao%2520Hu%2520and%2520Xuemeng%2520Yang%2520and%2520Licheng%2520Wen%2520and%2520Yu%2520Yang%2520and%2520Tiantian%2520Wei%2520and%2520Yukai%2520Ma%2520and%2520Min%2520Dou%2520and%2520Botian%2520Shi%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520improved%2520controllable%2520streetscape%250Ageneration%2520and%2520supported%2520downstream%2520perception%2520and%2520planning%2520tasks.%2520However%252C%250Achallenges%2520remain%2520in%2520accurately%2520modeling%2520driving%2520scenes%2520and%2520generating%2520long%250Avideos.%2520To%2520alleviate%2520these%2520issues%252C%2520we%2520propose%2520DreamForge%252C%2520an%2520advanced%250Adiffusion-based%2520autoregressive%2520video%2520generation%2520model%2520tailored%2520for%250A3D-controllable%2520long-term%2520generation.%2520To%2520enhance%2520the%2520lane%2520and%2520foreground%250Ageneration%252C%2520we%2520introduce%2520perspective%2520guidance%2520and%2520integrate%2520object-wise%250Aposition%2520encoding%2520to%2520incorporate%2520local%25203D%2520correlation%2520and%2520improve%2520foreground%250Aobject%2520modeling.%2520We%2520also%2520propose%2520motion-aware%2520temporal%2520attention%2520to%2520capture%250Amotion%2520cues%2520and%2520appearance%2520changes%2520in%2520videos.%2520By%2520leveraging%2520motion%2520frames%2520and%250Aan%2520autoregressive%2520generation%2520paradigm%252Cwe%2520can%2520autoregressively%2520generate%2520long%250Avideos%2520%2528over%2520200%2520frames%2529%2520using%2520a%2520model%2520trained%2520in%2520short%2520sequences%252C%2520achieving%250Asuperior%2520quality%2520compared%2520to%2520the%2520baseline%2520in%252016-frame%2520video%2520evaluations.%250AFinally%252C%2520we%2520integrate%2520our%2520method%2520with%2520the%2520realistic%2520simulator%2520DriveArena%2520to%250Aprovide%2520more%2520reliable%2520open-loop%2520and%2520closed-loop%2520evaluations%2520for%2520vision-based%250Adriving%2520agents.%2520Project%2520Page%253A%250Ahttps%253A//pjlab-adg.github.io/DriveArena/dreamforge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04003v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamForge%3A%20Motion-Aware%20Autoregressive%20Video%20Generation%20for%20Multi-View%0A%20%20Driving%20Scenes&entry.906535625=Jianbiao%20Mei%20and%20Tao%20Hu%20and%20Xuemeng%20Yang%20and%20Licheng%20Wen%20and%20Yu%20Yang%20and%20Tiantian%20Wei%20and%20Yukai%20Ma%20and%20Min%20Dou%20and%20Botian%20Shi%20and%20Yong%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20improved%20controllable%20streetscape%0Ageneration%20and%20supported%20downstream%20perception%20and%20planning%20tasks.%20However%2C%0Achallenges%20remain%20in%20accurately%20modeling%20driving%20scenes%20and%20generating%20long%0Avideos.%20To%20alleviate%20these%20issues%2C%20we%20propose%20DreamForge%2C%20an%20advanced%0Adiffusion-based%20autoregressive%20video%20generation%20model%20tailored%20for%0A3D-controllable%20long-term%20generation.%20To%20enhance%20the%20lane%20and%20foreground%0Ageneration%2C%20we%20introduce%20perspective%20guidance%20and%20integrate%20object-wise%0Aposition%20encoding%20to%20incorporate%20local%203D%20correlation%20and%20improve%20foreground%0Aobject%20modeling.%20We%20also%20propose%20motion-aware%20temporal%20attention%20to%20capture%0Amotion%20cues%20and%20appearance%20changes%20in%20videos.%20By%20leveraging%20motion%20frames%20and%0Aan%20autoregressive%20generation%20paradigm%2Cwe%20can%20autoregressively%20generate%20long%0Avideos%20%28over%20200%20frames%29%20using%20a%20model%20trained%20in%20short%20sequences%2C%20achieving%0Asuperior%20quality%20compared%20to%20the%20baseline%20in%2016-frame%20video%20evaluations.%0AFinally%2C%20we%20integrate%20our%20method%20with%20the%20realistic%20simulator%20DriveArena%20to%0Aprovide%20more%20reliable%20open-loop%20and%20closed-loop%20evaluations%20for%20vision-based%0Adriving%20agents.%20Project%20Page%3A%0Ahttps%3A//pjlab-adg.github.io/DriveArena/dreamforge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04003v4&entry.124074799=Read"},
{"title": "VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration", "author": "Ben Li and Minqi Li and Jie Ren and Kaibing Zhang", "abstract": "  Image-based virtual try-on aims to fit a target garment to a specific person\nimage and has attracted extensive research attention because of its huge\napplication potential in the e-commerce and fashion industries. To generate\nhigh-quality try-on results, accurately warping the clothing item to fit the\nhuman body plays a significant role, as slight misalignment may lead to\nunrealistic artifacts in the fitting image. Most existing methods warp the\nclothing by feature matching and thin-plate spline (TPS). However, it often\nfails to preserve clothing details due to self-occlusion, severe misalignment\nbetween poses, etc. To address these challenges, this paper proposes a detail\nretention virtual try-on method via accurate non-rigid registration (VITON-DRR)\nfor diverse human poses. Specifically, we reconstruct a human semantic\nsegmentation using a dual-pyramid-structured feature extractor. Then, a novel\nDeformation Module is designed for extracting the cloth key points and warping\nthem through an accurate non-rigid registration algorithm. Finally, the Image\nSynthesis Module is designed to synthesize the deformed garment image and\ngenerate the human pose information adaptively. {Compared with} traditional\nmethods, the proposed VITON-DRR can make the deformation of fitting images more\naccurate and retain more garment details. The experimental results demonstrate\nthat the proposed method performs better than state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.23439v1", "date": "2025-05-29", "relevancy": 2.4879, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6404}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.615}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITON-DRR%3A%20Details%20Retention%20Virtual%20Try-on%20via%20Non-rigid%20Registration&body=Title%3A%20VITON-DRR%3A%20Details%20Retention%20Virtual%20Try-on%20via%20Non-rigid%20Registration%0AAuthor%3A%20Ben%20Li%20and%20Minqi%20Li%20and%20Jie%20Ren%20and%20Kaibing%20Zhang%0AAbstract%3A%20%20%20Image-based%20virtual%20try-on%20aims%20to%20fit%20a%20target%20garment%20to%20a%20specific%20person%0Aimage%20and%20has%20attracted%20extensive%20research%20attention%20because%20of%20its%20huge%0Aapplication%20potential%20in%20the%20e-commerce%20and%20fashion%20industries.%20To%20generate%0Ahigh-quality%20try-on%20results%2C%20accurately%20warping%20the%20clothing%20item%20to%20fit%20the%0Ahuman%20body%20plays%20a%20significant%20role%2C%20as%20slight%20misalignment%20may%20lead%20to%0Aunrealistic%20artifacts%20in%20the%20fitting%20image.%20Most%20existing%20methods%20warp%20the%0Aclothing%20by%20feature%20matching%20and%20thin-plate%20spline%20%28TPS%29.%20However%2C%20it%20often%0Afails%20to%20preserve%20clothing%20details%20due%20to%20self-occlusion%2C%20severe%20misalignment%0Abetween%20poses%2C%20etc.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20detail%0Aretention%20virtual%20try-on%20method%20via%20accurate%20non-rigid%20registration%20%28VITON-DRR%29%0Afor%20diverse%20human%20poses.%20Specifically%2C%20we%20reconstruct%20a%20human%20semantic%0Asegmentation%20using%20a%20dual-pyramid-structured%20feature%20extractor.%20Then%2C%20a%20novel%0ADeformation%20Module%20is%20designed%20for%20extracting%20the%20cloth%20key%20points%20and%20warping%0Athem%20through%20an%20accurate%20non-rigid%20registration%20algorithm.%20Finally%2C%20the%20Image%0ASynthesis%20Module%20is%20designed%20to%20synthesize%20the%20deformed%20garment%20image%20and%0Agenerate%20the%20human%20pose%20information%20adaptively.%20%7BCompared%20with%7D%20traditional%0Amethods%2C%20the%20proposed%20VITON-DRR%20can%20make%20the%20deformation%20of%20fitting%20images%20more%0Aaccurate%20and%20retain%20more%20garment%20details.%20The%20experimental%20results%20demonstrate%0Athat%20the%20proposed%20method%20performs%20better%20than%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITON-DRR%253A%2520Details%2520Retention%2520Virtual%2520Try-on%2520via%2520Non-rigid%2520Registration%26entry.906535625%3DBen%2520Li%2520and%2520Minqi%2520Li%2520and%2520Jie%2520Ren%2520and%2520Kaibing%2520Zhang%26entry.1292438233%3D%2520%2520Image-based%2520virtual%2520try-on%2520aims%2520to%2520fit%2520a%2520target%2520garment%2520to%2520a%2520specific%2520person%250Aimage%2520and%2520has%2520attracted%2520extensive%2520research%2520attention%2520because%2520of%2520its%2520huge%250Aapplication%2520potential%2520in%2520the%2520e-commerce%2520and%2520fashion%2520industries.%2520To%2520generate%250Ahigh-quality%2520try-on%2520results%252C%2520accurately%2520warping%2520the%2520clothing%2520item%2520to%2520fit%2520the%250Ahuman%2520body%2520plays%2520a%2520significant%2520role%252C%2520as%2520slight%2520misalignment%2520may%2520lead%2520to%250Aunrealistic%2520artifacts%2520in%2520the%2520fitting%2520image.%2520Most%2520existing%2520methods%2520warp%2520the%250Aclothing%2520by%2520feature%2520matching%2520and%2520thin-plate%2520spline%2520%2528TPS%2529.%2520However%252C%2520it%2520often%250Afails%2520to%2520preserve%2520clothing%2520details%2520due%2520to%2520self-occlusion%252C%2520severe%2520misalignment%250Abetween%2520poses%252C%2520etc.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520detail%250Aretention%2520virtual%2520try-on%2520method%2520via%2520accurate%2520non-rigid%2520registration%2520%2528VITON-DRR%2529%250Afor%2520diverse%2520human%2520poses.%2520Specifically%252C%2520we%2520reconstruct%2520a%2520human%2520semantic%250Asegmentation%2520using%2520a%2520dual-pyramid-structured%2520feature%2520extractor.%2520Then%252C%2520a%2520novel%250ADeformation%2520Module%2520is%2520designed%2520for%2520extracting%2520the%2520cloth%2520key%2520points%2520and%2520warping%250Athem%2520through%2520an%2520accurate%2520non-rigid%2520registration%2520algorithm.%2520Finally%252C%2520the%2520Image%250ASynthesis%2520Module%2520is%2520designed%2520to%2520synthesize%2520the%2520deformed%2520garment%2520image%2520and%250Agenerate%2520the%2520human%2520pose%2520information%2520adaptively.%2520%257BCompared%2520with%257D%2520traditional%250Amethods%252C%2520the%2520proposed%2520VITON-DRR%2520can%2520make%2520the%2520deformation%2520of%2520fitting%2520images%2520more%250Aaccurate%2520and%2520retain%2520more%2520garment%2520details.%2520The%2520experimental%2520results%2520demonstrate%250Athat%2520the%2520proposed%2520method%2520performs%2520better%2520than%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITON-DRR%3A%20Details%20Retention%20Virtual%20Try-on%20via%20Non-rigid%20Registration&entry.906535625=Ben%20Li%20and%20Minqi%20Li%20and%20Jie%20Ren%20and%20Kaibing%20Zhang&entry.1292438233=%20%20Image-based%20virtual%20try-on%20aims%20to%20fit%20a%20target%20garment%20to%20a%20specific%20person%0Aimage%20and%20has%20attracted%20extensive%20research%20attention%20because%20of%20its%20huge%0Aapplication%20potential%20in%20the%20e-commerce%20and%20fashion%20industries.%20To%20generate%0Ahigh-quality%20try-on%20results%2C%20accurately%20warping%20the%20clothing%20item%20to%20fit%20the%0Ahuman%20body%20plays%20a%20significant%20role%2C%20as%20slight%20misalignment%20may%20lead%20to%0Aunrealistic%20artifacts%20in%20the%20fitting%20image.%20Most%20existing%20methods%20warp%20the%0Aclothing%20by%20feature%20matching%20and%20thin-plate%20spline%20%28TPS%29.%20However%2C%20it%20often%0Afails%20to%20preserve%20clothing%20details%20due%20to%20self-occlusion%2C%20severe%20misalignment%0Abetween%20poses%2C%20etc.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20detail%0Aretention%20virtual%20try-on%20method%20via%20accurate%20non-rigid%20registration%20%28VITON-DRR%29%0Afor%20diverse%20human%20poses.%20Specifically%2C%20we%20reconstruct%20a%20human%20semantic%0Asegmentation%20using%20a%20dual-pyramid-structured%20feature%20extractor.%20Then%2C%20a%20novel%0ADeformation%20Module%20is%20designed%20for%20extracting%20the%20cloth%20key%20points%20and%20warping%0Athem%20through%20an%20accurate%20non-rigid%20registration%20algorithm.%20Finally%2C%20the%20Image%0ASynthesis%20Module%20is%20designed%20to%20synthesize%20the%20deformed%20garment%20image%20and%0Agenerate%20the%20human%20pose%20information%20adaptively.%20%7BCompared%20with%7D%20traditional%0Amethods%2C%20the%20proposed%20VITON-DRR%20can%20make%20the%20deformation%20of%20fitting%20images%20more%0Aaccurate%20and%20retain%20more%20garment%20details.%20The%20experimental%20results%20demonstrate%0Athat%20the%20proposed%20method%20performs%20better%20than%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23439v1&entry.124074799=Read"},
{"title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video\n  Reasoning?", "author": "Yuanxin Liu and Kun Ouyang and Haoning Wu and Yi Liu and Lin Sui and Xinhao Li and Yan Zhong and Y. Charles and Xinyu Zhou and Xu Sun", "abstract": "  Recent studies have shown that long chain-of-thought (CoT) reasoning can\nsignificantly enhance the performance of large language models (LLMs) on\ncomplex tasks. However, this benefit is yet to be demonstrated in the domain of\nvideo understanding, since most existing benchmarks lack the reasoning depth\nrequired to demonstrate the advantages of extended CoT chains. While recent\nefforts have proposed benchmarks aimed at video reasoning, the tasks are often\nknowledge-driven and do not rely heavily on visual content. To bridge this gap,\nwe introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,\ncomplex video reasoning. To ensure visual richness and high reasoning\ncomplexity, each video in VideoReasonBench depicts a sequence of fine-grained\noperations on a latent state that is only visible in part of the video. The\nquestions evaluate three escalating levels of video reasoning skills: recalling\nobserved visual information, inferring the content of latent states, and\npredicting information beyond the video. Under such task setting, models have\nto precisely recall multiple operations in the video, and perform step-by-step\nreasoning to get correct final answers for these questions. Using\nVideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal\nLLMs (MLLMs), finding that most perform poorly on complex video reasoning,\ne.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced\nGemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our\ninvestigations on \"test-time scaling\" further reveal that extended thinking\nbudget, while offering none or minimal benefits on existing video benchmarks,\nis essential for improving the performance on VideoReasonBench.\n", "link": "http://arxiv.org/abs/2505.23359v1", "date": "2025-05-29", "relevancy": 2.4669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6332}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoReasonBench%3A%20Can%20MLLMs%20Perform%20Vision-Centric%20Complex%20Video%0A%20%20Reasoning%3F&body=Title%3A%20VideoReasonBench%3A%20Can%20MLLMs%20Perform%20Vision-Centric%20Complex%20Video%0A%20%20Reasoning%3F%0AAuthor%3A%20Yuanxin%20Liu%20and%20Kun%20Ouyang%20and%20Haoning%20Wu%20and%20Yi%20Liu%20and%20Lin%20Sui%20and%20Xinhao%20Li%20and%20Yan%20Zhong%20and%20Y.%20Charles%20and%20Xinyu%20Zhou%20and%20Xu%20Sun%0AAbstract%3A%20%20%20Recent%20studies%20have%20shown%20that%20long%20chain-of-thought%20%28CoT%29%20reasoning%20can%0Asignificantly%20enhance%20the%20performance%20of%20large%20language%20models%20%28LLMs%29%20on%0Acomplex%20tasks.%20However%2C%20this%20benefit%20is%20yet%20to%20be%20demonstrated%20in%20the%20domain%20of%0Avideo%20understanding%2C%20since%20most%20existing%20benchmarks%20lack%20the%20reasoning%20depth%0Arequired%20to%20demonstrate%20the%20advantages%20of%20extended%20CoT%20chains.%20While%20recent%0Aefforts%20have%20proposed%20benchmarks%20aimed%20at%20video%20reasoning%2C%20the%20tasks%20are%20often%0Aknowledge-driven%20and%20do%20not%20rely%20heavily%20on%20visual%20content.%20To%20bridge%20this%20gap%2C%0Awe%20introduce%20VideoReasonBench%2C%20a%20benchmark%20designed%20to%20evaluate%20vision-centric%2C%0Acomplex%20video%20reasoning.%20To%20ensure%20visual%20richness%20and%20high%20reasoning%0Acomplexity%2C%20each%20video%20in%20VideoReasonBench%20depicts%20a%20sequence%20of%20fine-grained%0Aoperations%20on%20a%20latent%20state%20that%20is%20only%20visible%20in%20part%20of%20the%20video.%20The%0Aquestions%20evaluate%20three%20escalating%20levels%20of%20video%20reasoning%20skills%3A%20recalling%0Aobserved%20visual%20information%2C%20inferring%20the%20content%20of%20latent%20states%2C%20and%0Apredicting%20information%20beyond%20the%20video.%20Under%20such%20task%20setting%2C%20models%20have%0Ato%20precisely%20recall%20multiple%20operations%20in%20the%20video%2C%20and%20perform%20step-by-step%0Areasoning%20to%20get%20correct%20final%20answers%20for%20these%20questions.%20Using%0AVideoReasonBench%2C%20we%20comprehensively%20evaluate%2018%20state-of-the-art%20multimodal%0ALLMs%20%28MLLMs%29%2C%20finding%20that%20most%20perform%20poorly%20on%20complex%20video%20reasoning%2C%0Ae.g.%2C%20GPT-4o%20achieves%20only%206.9%25%20accuracy%2C%20while%20the%20thinking-enhanced%0AGemini-2.5-Pro%20significantly%20outperforms%20others%20with%2056.0%25%20accuracy.%20Our%0Ainvestigations%20on%20%22test-time%20scaling%22%20further%20reveal%20that%20extended%20thinking%0Abudget%2C%20while%20offering%20none%20or%20minimal%20benefits%20on%20existing%20video%20benchmarks%2C%0Ais%20essential%20for%20improving%20the%20performance%20on%20VideoReasonBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoReasonBench%253A%2520Can%2520MLLMs%2520Perform%2520Vision-Centric%2520Complex%2520Video%250A%2520%2520Reasoning%253F%26entry.906535625%3DYuanxin%2520Liu%2520and%2520Kun%2520Ouyang%2520and%2520Haoning%2520Wu%2520and%2520Yi%2520Liu%2520and%2520Lin%2520Sui%2520and%2520Xinhao%2520Li%2520and%2520Yan%2520Zhong%2520and%2520Y.%2520Charles%2520and%2520Xinyu%2520Zhou%2520and%2520Xu%2520Sun%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520shown%2520that%2520long%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520can%250Asignificantly%2520enhance%2520the%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%250Acomplex%2520tasks.%2520However%252C%2520this%2520benefit%2520is%2520yet%2520to%2520be%2520demonstrated%2520in%2520the%2520domain%2520of%250Avideo%2520understanding%252C%2520since%2520most%2520existing%2520benchmarks%2520lack%2520the%2520reasoning%2520depth%250Arequired%2520to%2520demonstrate%2520the%2520advantages%2520of%2520extended%2520CoT%2520chains.%2520While%2520recent%250Aefforts%2520have%2520proposed%2520benchmarks%2520aimed%2520at%2520video%2520reasoning%252C%2520the%2520tasks%2520are%2520often%250Aknowledge-driven%2520and%2520do%2520not%2520rely%2520heavily%2520on%2520visual%2520content.%2520To%2520bridge%2520this%2520gap%252C%250Awe%2520introduce%2520VideoReasonBench%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520vision-centric%252C%250Acomplex%2520video%2520reasoning.%2520To%2520ensure%2520visual%2520richness%2520and%2520high%2520reasoning%250Acomplexity%252C%2520each%2520video%2520in%2520VideoReasonBench%2520depicts%2520a%2520sequence%2520of%2520fine-grained%250Aoperations%2520on%2520a%2520latent%2520state%2520that%2520is%2520only%2520visible%2520in%2520part%2520of%2520the%2520video.%2520The%250Aquestions%2520evaluate%2520three%2520escalating%2520levels%2520of%2520video%2520reasoning%2520skills%253A%2520recalling%250Aobserved%2520visual%2520information%252C%2520inferring%2520the%2520content%2520of%2520latent%2520states%252C%2520and%250Apredicting%2520information%2520beyond%2520the%2520video.%2520Under%2520such%2520task%2520setting%252C%2520models%2520have%250Ato%2520precisely%2520recall%2520multiple%2520operations%2520in%2520the%2520video%252C%2520and%2520perform%2520step-by-step%250Areasoning%2520to%2520get%2520correct%2520final%2520answers%2520for%2520these%2520questions.%2520Using%250AVideoReasonBench%252C%2520we%2520comprehensively%2520evaluate%252018%2520state-of-the-art%2520multimodal%250ALLMs%2520%2528MLLMs%2529%252C%2520finding%2520that%2520most%2520perform%2520poorly%2520on%2520complex%2520video%2520reasoning%252C%250Ae.g.%252C%2520GPT-4o%2520achieves%2520only%25206.9%2525%2520accuracy%252C%2520while%2520the%2520thinking-enhanced%250AGemini-2.5-Pro%2520significantly%2520outperforms%2520others%2520with%252056.0%2525%2520accuracy.%2520Our%250Ainvestigations%2520on%2520%2522test-time%2520scaling%2522%2520further%2520reveal%2520that%2520extended%2520thinking%250Abudget%252C%2520while%2520offering%2520none%2520or%2520minimal%2520benefits%2520on%2520existing%2520video%2520benchmarks%252C%250Ais%2520essential%2520for%2520improving%2520the%2520performance%2520on%2520VideoReasonBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoReasonBench%3A%20Can%20MLLMs%20Perform%20Vision-Centric%20Complex%20Video%0A%20%20Reasoning%3F&entry.906535625=Yuanxin%20Liu%20and%20Kun%20Ouyang%20and%20Haoning%20Wu%20and%20Yi%20Liu%20and%20Lin%20Sui%20and%20Xinhao%20Li%20and%20Yan%20Zhong%20and%20Y.%20Charles%20and%20Xinyu%20Zhou%20and%20Xu%20Sun&entry.1292438233=%20%20Recent%20studies%20have%20shown%20that%20long%20chain-of-thought%20%28CoT%29%20reasoning%20can%0Asignificantly%20enhance%20the%20performance%20of%20large%20language%20models%20%28LLMs%29%20on%0Acomplex%20tasks.%20However%2C%20this%20benefit%20is%20yet%20to%20be%20demonstrated%20in%20the%20domain%20of%0Avideo%20understanding%2C%20since%20most%20existing%20benchmarks%20lack%20the%20reasoning%20depth%0Arequired%20to%20demonstrate%20the%20advantages%20of%20extended%20CoT%20chains.%20While%20recent%0Aefforts%20have%20proposed%20benchmarks%20aimed%20at%20video%20reasoning%2C%20the%20tasks%20are%20often%0Aknowledge-driven%20and%20do%20not%20rely%20heavily%20on%20visual%20content.%20To%20bridge%20this%20gap%2C%0Awe%20introduce%20VideoReasonBench%2C%20a%20benchmark%20designed%20to%20evaluate%20vision-centric%2C%0Acomplex%20video%20reasoning.%20To%20ensure%20visual%20richness%20and%20high%20reasoning%0Acomplexity%2C%20each%20video%20in%20VideoReasonBench%20depicts%20a%20sequence%20of%20fine-grained%0Aoperations%20on%20a%20latent%20state%20that%20is%20only%20visible%20in%20part%20of%20the%20video.%20The%0Aquestions%20evaluate%20three%20escalating%20levels%20of%20video%20reasoning%20skills%3A%20recalling%0Aobserved%20visual%20information%2C%20inferring%20the%20content%20of%20latent%20states%2C%20and%0Apredicting%20information%20beyond%20the%20video.%20Under%20such%20task%20setting%2C%20models%20have%0Ato%20precisely%20recall%20multiple%20operations%20in%20the%20video%2C%20and%20perform%20step-by-step%0Areasoning%20to%20get%20correct%20final%20answers%20for%20these%20questions.%20Using%0AVideoReasonBench%2C%20we%20comprehensively%20evaluate%2018%20state-of-the-art%20multimodal%0ALLMs%20%28MLLMs%29%2C%20finding%20that%20most%20perform%20poorly%20on%20complex%20video%20reasoning%2C%0Ae.g.%2C%20GPT-4o%20achieves%20only%206.9%25%20accuracy%2C%20while%20the%20thinking-enhanced%0AGemini-2.5-Pro%20significantly%20outperforms%20others%20with%2056.0%25%20accuracy.%20Our%0Ainvestigations%20on%20%22test-time%20scaling%22%20further%20reveal%20that%20extended%20thinking%0Abudget%2C%20while%20offering%20none%20or%20minimal%20benefits%20on%20existing%20video%20benchmarks%2C%0Ais%20essential%20for%20improving%20the%20performance%20on%20VideoReasonBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23359v1&entry.124074799=Read"},
{"title": "Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization\n  and Mapping", "author": "Justin Lazarow and Kai Kang and Afshin Dehghan", "abstract": "  We revisit scene-level 3D object detection as the output of an object-centric\nframework capable of both localization and mapping using 3D oriented boxes as\nthe underlying geometric primitive. While existing 3D object detection\napproaches operate globally and implicitly rely on the a priori existence of\nmetric camera poses, our method, Rooms from Motion (RfM) operates on a\ncollection of un-posed images. By replacing the standard 2D keypoint-based\nmatcher of structure-from-motion with an object-centric matcher based on\nimage-derived 3D boxes, we estimate metric camera poses, object tracks, and\nfinally produce a global, semantic 3D object map. When a priori pose is\navailable, we can significantly improve map quality through optimization of\nglobal 3D boxes against individual observations. RfM shows strong localization\nperformance and subsequently produces maps of higher quality than leading\npoint-based and multi-view 3D object detection methods on CA-1M and ScanNet++,\ndespite these global methods relying on overparameterization through point\nclouds or dense volumes. Rooms from Motion achieves a general, object-centric\nrepresentation which not only extends the work of Cubify Anything to full\nscenes but also allows for inherently sparse localization and parametric\nmapping proportional to the number of objects in a scene.\n", "link": "http://arxiv.org/abs/2505.23756v1", "date": "2025-05-29", "relevancy": 2.4622, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6532}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5958}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rooms%20from%20Motion%3A%20Un-posed%20Indoor%203D%20Object%20Detection%20as%20Localization%0A%20%20and%20Mapping&body=Title%3A%20Rooms%20from%20Motion%3A%20Un-posed%20Indoor%203D%20Object%20Detection%20as%20Localization%0A%20%20and%20Mapping%0AAuthor%3A%20Justin%20Lazarow%20and%20Kai%20Kang%20and%20Afshin%20Dehghan%0AAbstract%3A%20%20%20We%20revisit%20scene-level%203D%20object%20detection%20as%20the%20output%20of%20an%20object-centric%0Aframework%20capable%20of%20both%20localization%20and%20mapping%20using%203D%20oriented%20boxes%20as%0Athe%20underlying%20geometric%20primitive.%20While%20existing%203D%20object%20detection%0Aapproaches%20operate%20globally%20and%20implicitly%20rely%20on%20the%20a%20priori%20existence%20of%0Ametric%20camera%20poses%2C%20our%20method%2C%20Rooms%20from%20Motion%20%28RfM%29%20operates%20on%20a%0Acollection%20of%20un-posed%20images.%20By%20replacing%20the%20standard%202D%20keypoint-based%0Amatcher%20of%20structure-from-motion%20with%20an%20object-centric%20matcher%20based%20on%0Aimage-derived%203D%20boxes%2C%20we%20estimate%20metric%20camera%20poses%2C%20object%20tracks%2C%20and%0Afinally%20produce%20a%20global%2C%20semantic%203D%20object%20map.%20When%20a%20priori%20pose%20is%0Aavailable%2C%20we%20can%20significantly%20improve%20map%20quality%20through%20optimization%20of%0Aglobal%203D%20boxes%20against%20individual%20observations.%20RfM%20shows%20strong%20localization%0Aperformance%20and%20subsequently%20produces%20maps%20of%20higher%20quality%20than%20leading%0Apoint-based%20and%20multi-view%203D%20object%20detection%20methods%20on%20CA-1M%20and%20ScanNet%2B%2B%2C%0Adespite%20these%20global%20methods%20relying%20on%20overparameterization%20through%20point%0Aclouds%20or%20dense%20volumes.%20Rooms%20from%20Motion%20achieves%20a%20general%2C%20object-centric%0Arepresentation%20which%20not%20only%20extends%20the%20work%20of%20Cubify%20Anything%20to%20full%0Ascenes%20but%20also%20allows%20for%20inherently%20sparse%20localization%20and%20parametric%0Amapping%20proportional%20to%20the%20number%20of%20objects%20in%20a%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRooms%2520from%2520Motion%253A%2520Un-posed%2520Indoor%25203D%2520Object%2520Detection%2520as%2520Localization%250A%2520%2520and%2520Mapping%26entry.906535625%3DJustin%2520Lazarow%2520and%2520Kai%2520Kang%2520and%2520Afshin%2520Dehghan%26entry.1292438233%3D%2520%2520We%2520revisit%2520scene-level%25203D%2520object%2520detection%2520as%2520the%2520output%2520of%2520an%2520object-centric%250Aframework%2520capable%2520of%2520both%2520localization%2520and%2520mapping%2520using%25203D%2520oriented%2520boxes%2520as%250Athe%2520underlying%2520geometric%2520primitive.%2520While%2520existing%25203D%2520object%2520detection%250Aapproaches%2520operate%2520globally%2520and%2520implicitly%2520rely%2520on%2520the%2520a%2520priori%2520existence%2520of%250Ametric%2520camera%2520poses%252C%2520our%2520method%252C%2520Rooms%2520from%2520Motion%2520%2528RfM%2529%2520operates%2520on%2520a%250Acollection%2520of%2520un-posed%2520images.%2520By%2520replacing%2520the%2520standard%25202D%2520keypoint-based%250Amatcher%2520of%2520structure-from-motion%2520with%2520an%2520object-centric%2520matcher%2520based%2520on%250Aimage-derived%25203D%2520boxes%252C%2520we%2520estimate%2520metric%2520camera%2520poses%252C%2520object%2520tracks%252C%2520and%250Afinally%2520produce%2520a%2520global%252C%2520semantic%25203D%2520object%2520map.%2520When%2520a%2520priori%2520pose%2520is%250Aavailable%252C%2520we%2520can%2520significantly%2520improve%2520map%2520quality%2520through%2520optimization%2520of%250Aglobal%25203D%2520boxes%2520against%2520individual%2520observations.%2520RfM%2520shows%2520strong%2520localization%250Aperformance%2520and%2520subsequently%2520produces%2520maps%2520of%2520higher%2520quality%2520than%2520leading%250Apoint-based%2520and%2520multi-view%25203D%2520object%2520detection%2520methods%2520on%2520CA-1M%2520and%2520ScanNet%252B%252B%252C%250Adespite%2520these%2520global%2520methods%2520relying%2520on%2520overparameterization%2520through%2520point%250Aclouds%2520or%2520dense%2520volumes.%2520Rooms%2520from%2520Motion%2520achieves%2520a%2520general%252C%2520object-centric%250Arepresentation%2520which%2520not%2520only%2520extends%2520the%2520work%2520of%2520Cubify%2520Anything%2520to%2520full%250Ascenes%2520but%2520also%2520allows%2520for%2520inherently%2520sparse%2520localization%2520and%2520parametric%250Amapping%2520proportional%2520to%2520the%2520number%2520of%2520objects%2520in%2520a%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rooms%20from%20Motion%3A%20Un-posed%20Indoor%203D%20Object%20Detection%20as%20Localization%0A%20%20and%20Mapping&entry.906535625=Justin%20Lazarow%20and%20Kai%20Kang%20and%20Afshin%20Dehghan&entry.1292438233=%20%20We%20revisit%20scene-level%203D%20object%20detection%20as%20the%20output%20of%20an%20object-centric%0Aframework%20capable%20of%20both%20localization%20and%20mapping%20using%203D%20oriented%20boxes%20as%0Athe%20underlying%20geometric%20primitive.%20While%20existing%203D%20object%20detection%0Aapproaches%20operate%20globally%20and%20implicitly%20rely%20on%20the%20a%20priori%20existence%20of%0Ametric%20camera%20poses%2C%20our%20method%2C%20Rooms%20from%20Motion%20%28RfM%29%20operates%20on%20a%0Acollection%20of%20un-posed%20images.%20By%20replacing%20the%20standard%202D%20keypoint-based%0Amatcher%20of%20structure-from-motion%20with%20an%20object-centric%20matcher%20based%20on%0Aimage-derived%203D%20boxes%2C%20we%20estimate%20metric%20camera%20poses%2C%20object%20tracks%2C%20and%0Afinally%20produce%20a%20global%2C%20semantic%203D%20object%20map.%20When%20a%20priori%20pose%20is%0Aavailable%2C%20we%20can%20significantly%20improve%20map%20quality%20through%20optimization%20of%0Aglobal%203D%20boxes%20against%20individual%20observations.%20RfM%20shows%20strong%20localization%0Aperformance%20and%20subsequently%20produces%20maps%20of%20higher%20quality%20than%20leading%0Apoint-based%20and%20multi-view%203D%20object%20detection%20methods%20on%20CA-1M%20and%20ScanNet%2B%2B%2C%0Adespite%20these%20global%20methods%20relying%20on%20overparameterization%20through%20point%0Aclouds%20or%20dense%20volumes.%20Rooms%20from%20Motion%20achieves%20a%20general%2C%20object-centric%0Arepresentation%20which%20not%20only%20extends%20the%20work%20of%20Cubify%20Anything%20to%20full%0Ascenes%20but%20also%20allows%20for%20inherently%20sparse%20localization%20and%20parametric%0Amapping%20proportional%20to%20the%20number%20of%20objects%20in%20a%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23756v1&entry.124074799=Read"},
{"title": "VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models", "author": "Xiangdong Zhang and Jiaqi Liao and Shaofeng Zhang and Fanqing Meng and Xiangpeng Wan and Junchi Yan and Yu Cheng", "abstract": "  Recent advancements in text-to-video (T2V) diffusion models have enabled\nhigh-fidelity and realistic video synthesis. However, current T2V models often\nstruggle to generate physically plausible content due to their limited inherent\nability to accurately understand physics. We found that while the\nrepresentations within T2V models possess some capacity for physics\nunderstanding, they lag significantly behind those from recent video\nself-supervised learning methods. To this end, we propose a novel framework\ncalled VideoREPA, which distills physics understanding capability from video\nunderstanding foundation models into T2V models by aligning token-level\nrelations. This closes the physics understanding gap and enable more\nphysics-plausible generation. Specifically, we introduce the Token Relation\nDistillation (TRD) loss, leveraging spatio-temporal alignment to provide soft\nguidance suitable for finetuning powerful pre-trained T2V models, a critical\ndeparture from prior representation alignment (REPA) methods. To our knowledge,\nVideoREPA is the first REPA method designed for finetuning T2V models and\nspecifically for injecting physical knowledge. Empirical evaluations show that\nVideoREPA substantially enhances the physics commonsense of baseline method,\nCogVideoX, achieving significant improvement on relevant benchmarks and\ndemonstrating a strong capacity for generating videos consistent with intuitive\nphysics. More video results are available at https://videorepa.github.io/.\n", "link": "http://arxiv.org/abs/2505.23656v1", "date": "2025-05-29", "relevancy": 2.4602, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6595}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5965}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoREPA%3A%20Learning%20Physics%20for%20Video%20Generation%20through%20Relational%0A%20%20Alignment%20with%20Foundation%20Models&body=Title%3A%20VideoREPA%3A%20Learning%20Physics%20for%20Video%20Generation%20through%20Relational%0A%20%20Alignment%20with%20Foundation%20Models%0AAuthor%3A%20Xiangdong%20Zhang%20and%20Jiaqi%20Liao%20and%20Shaofeng%20Zhang%20and%20Fanqing%20Meng%20and%20Xiangpeng%20Wan%20and%20Junchi%20Yan%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-video%20%28T2V%29%20diffusion%20models%20have%20enabled%0Ahigh-fidelity%20and%20realistic%20video%20synthesis.%20However%2C%20current%20T2V%20models%20often%0Astruggle%20to%20generate%20physically%20plausible%20content%20due%20to%20their%20limited%20inherent%0Aability%20to%20accurately%20understand%20physics.%20We%20found%20that%20while%20the%0Arepresentations%20within%20T2V%20models%20possess%20some%20capacity%20for%20physics%0Aunderstanding%2C%20they%20lag%20significantly%20behind%20those%20from%20recent%20video%0Aself-supervised%20learning%20methods.%20To%20this%20end%2C%20we%20propose%20a%20novel%20framework%0Acalled%20VideoREPA%2C%20which%20distills%20physics%20understanding%20capability%20from%20video%0Aunderstanding%20foundation%20models%20into%20T2V%20models%20by%20aligning%20token-level%0Arelations.%20This%20closes%20the%20physics%20understanding%20gap%20and%20enable%20more%0Aphysics-plausible%20generation.%20Specifically%2C%20we%20introduce%20the%20Token%20Relation%0ADistillation%20%28TRD%29%20loss%2C%20leveraging%20spatio-temporal%20alignment%20to%20provide%20soft%0Aguidance%20suitable%20for%20finetuning%20powerful%20pre-trained%20T2V%20models%2C%20a%20critical%0Adeparture%20from%20prior%20representation%20alignment%20%28REPA%29%20methods.%20To%20our%20knowledge%2C%0AVideoREPA%20is%20the%20first%20REPA%20method%20designed%20for%20finetuning%20T2V%20models%20and%0Aspecifically%20for%20injecting%20physical%20knowledge.%20Empirical%20evaluations%20show%20that%0AVideoREPA%20substantially%20enhances%20the%20physics%20commonsense%20of%20baseline%20method%2C%0ACogVideoX%2C%20achieving%20significant%20improvement%20on%20relevant%20benchmarks%20and%0Ademonstrating%20a%20strong%20capacity%20for%20generating%20videos%20consistent%20with%20intuitive%0Aphysics.%20More%20video%20results%20are%20available%20at%20https%3A//videorepa.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoREPA%253A%2520Learning%2520Physics%2520for%2520Video%2520Generation%2520through%2520Relational%250A%2520%2520Alignment%2520with%2520Foundation%2520Models%26entry.906535625%3DXiangdong%2520Zhang%2520and%2520Jiaqi%2520Liao%2520and%2520Shaofeng%2520Zhang%2520and%2520Fanqing%2520Meng%2520and%2520Xiangpeng%2520Wan%2520and%2520Junchi%2520Yan%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-video%2520%2528T2V%2529%2520diffusion%2520models%2520have%2520enabled%250Ahigh-fidelity%2520and%2520realistic%2520video%2520synthesis.%2520However%252C%2520current%2520T2V%2520models%2520often%250Astruggle%2520to%2520generate%2520physically%2520plausible%2520content%2520due%2520to%2520their%2520limited%2520inherent%250Aability%2520to%2520accurately%2520understand%2520physics.%2520We%2520found%2520that%2520while%2520the%250Arepresentations%2520within%2520T2V%2520models%2520possess%2520some%2520capacity%2520for%2520physics%250Aunderstanding%252C%2520they%2520lag%2520significantly%2520behind%2520those%2520from%2520recent%2520video%250Aself-supervised%2520learning%2520methods.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520framework%250Acalled%2520VideoREPA%252C%2520which%2520distills%2520physics%2520understanding%2520capability%2520from%2520video%250Aunderstanding%2520foundation%2520models%2520into%2520T2V%2520models%2520by%2520aligning%2520token-level%250Arelations.%2520This%2520closes%2520the%2520physics%2520understanding%2520gap%2520and%2520enable%2520more%250Aphysics-plausible%2520generation.%2520Specifically%252C%2520we%2520introduce%2520the%2520Token%2520Relation%250ADistillation%2520%2528TRD%2529%2520loss%252C%2520leveraging%2520spatio-temporal%2520alignment%2520to%2520provide%2520soft%250Aguidance%2520suitable%2520for%2520finetuning%2520powerful%2520pre-trained%2520T2V%2520models%252C%2520a%2520critical%250Adeparture%2520from%2520prior%2520representation%2520alignment%2520%2528REPA%2529%2520methods.%2520To%2520our%2520knowledge%252C%250AVideoREPA%2520is%2520the%2520first%2520REPA%2520method%2520designed%2520for%2520finetuning%2520T2V%2520models%2520and%250Aspecifically%2520for%2520injecting%2520physical%2520knowledge.%2520Empirical%2520evaluations%2520show%2520that%250AVideoREPA%2520substantially%2520enhances%2520the%2520physics%2520commonsense%2520of%2520baseline%2520method%252C%250ACogVideoX%252C%2520achieving%2520significant%2520improvement%2520on%2520relevant%2520benchmarks%2520and%250Ademonstrating%2520a%2520strong%2520capacity%2520for%2520generating%2520videos%2520consistent%2520with%2520intuitive%250Aphysics.%2520More%2520video%2520results%2520are%2520available%2520at%2520https%253A//videorepa.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoREPA%3A%20Learning%20Physics%20for%20Video%20Generation%20through%20Relational%0A%20%20Alignment%20with%20Foundation%20Models&entry.906535625=Xiangdong%20Zhang%20and%20Jiaqi%20Liao%20and%20Shaofeng%20Zhang%20and%20Fanqing%20Meng%20and%20Xiangpeng%20Wan%20and%20Junchi%20Yan%20and%20Yu%20Cheng&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-video%20%28T2V%29%20diffusion%20models%20have%20enabled%0Ahigh-fidelity%20and%20realistic%20video%20synthesis.%20However%2C%20current%20T2V%20models%20often%0Astruggle%20to%20generate%20physically%20plausible%20content%20due%20to%20their%20limited%20inherent%0Aability%20to%20accurately%20understand%20physics.%20We%20found%20that%20while%20the%0Arepresentations%20within%20T2V%20models%20possess%20some%20capacity%20for%20physics%0Aunderstanding%2C%20they%20lag%20significantly%20behind%20those%20from%20recent%20video%0Aself-supervised%20learning%20methods.%20To%20this%20end%2C%20we%20propose%20a%20novel%20framework%0Acalled%20VideoREPA%2C%20which%20distills%20physics%20understanding%20capability%20from%20video%0Aunderstanding%20foundation%20models%20into%20T2V%20models%20by%20aligning%20token-level%0Arelations.%20This%20closes%20the%20physics%20understanding%20gap%20and%20enable%20more%0Aphysics-plausible%20generation.%20Specifically%2C%20we%20introduce%20the%20Token%20Relation%0ADistillation%20%28TRD%29%20loss%2C%20leveraging%20spatio-temporal%20alignment%20to%20provide%20soft%0Aguidance%20suitable%20for%20finetuning%20powerful%20pre-trained%20T2V%20models%2C%20a%20critical%0Adeparture%20from%20prior%20representation%20alignment%20%28REPA%29%20methods.%20To%20our%20knowledge%2C%0AVideoREPA%20is%20the%20first%20REPA%20method%20designed%20for%20finetuning%20T2V%20models%20and%0Aspecifically%20for%20injecting%20physical%20knowledge.%20Empirical%20evaluations%20show%20that%0AVideoREPA%20substantially%20enhances%20the%20physics%20commonsense%20of%20baseline%20method%2C%0ACogVideoX%2C%20achieving%20significant%20improvement%20on%20relevant%20benchmarks%20and%0Ademonstrating%20a%20strong%20capacity%20for%20generating%20videos%20consistent%20with%20intuitive%0Aphysics.%20More%20video%20results%20are%20available%20at%20https%3A//videorepa.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23656v1&entry.124074799=Read"},
{"title": "Compositional Scene Understanding through Inverse Generative Modeling", "author": "Yanbo Wang and Justin Dauwels and Yilun Du", "abstract": "  Generative models have demonstrated remarkable abilities in generating\nhigh-fidelity visual content. In this work, we explore how generative models\ncan further be used not only to synthesize visual content but also to\nunderstand the properties of a scene given a natural image. We formulate scene\nunderstanding as an inverse generative modeling problem, where we seek to find\nconditional parameters of a visual generative model to best fit a given natural\nimage. To enable this procedure to infer scene structure from images\nsubstantially different than those seen during training, we further propose to\nbuild this visual generative model compositionally from smaller models over\npieces of a scene. We illustrate how this procedure enables us to infer the set\nof objects in a scene, enabling robust generalization to new test scenes with\nan increased number of objects of new shapes. We further illustrate how this\nenables us to infer global scene factors, likewise enabling robust\ngeneralization to new scenes. Finally, we illustrate how this approach can be\ndirectly applied to existing pretrained text-to-image generative models for\nzero-shot multi-object perception. Code and visualizations are at\nhttps://energy-based-model.github.io/compositional-inference.\n", "link": "http://arxiv.org/abs/2505.21780v2", "date": "2025-05-29", "relevancy": 2.4511, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compositional%20Scene%20Understanding%20through%20Inverse%20Generative%20Modeling&body=Title%3A%20Compositional%20Scene%20Understanding%20through%20Inverse%20Generative%20Modeling%0AAuthor%3A%20Yanbo%20Wang%20and%20Justin%20Dauwels%20and%20Yilun%20Du%0AAbstract%3A%20%20%20Generative%20models%20have%20demonstrated%20remarkable%20abilities%20in%20generating%0Ahigh-fidelity%20visual%20content.%20In%20this%20work%2C%20we%20explore%20how%20generative%20models%0Acan%20further%20be%20used%20not%20only%20to%20synthesize%20visual%20content%20but%20also%20to%0Aunderstand%20the%20properties%20of%20a%20scene%20given%20a%20natural%20image.%20We%20formulate%20scene%0Aunderstanding%20as%20an%20inverse%20generative%20modeling%20problem%2C%20where%20we%20seek%20to%20find%0Aconditional%20parameters%20of%20a%20visual%20generative%20model%20to%20best%20fit%20a%20given%20natural%0Aimage.%20To%20enable%20this%20procedure%20to%20infer%20scene%20structure%20from%20images%0Asubstantially%20different%20than%20those%20seen%20during%20training%2C%20we%20further%20propose%20to%0Abuild%20this%20visual%20generative%20model%20compositionally%20from%20smaller%20models%20over%0Apieces%20of%20a%20scene.%20We%20illustrate%20how%20this%20procedure%20enables%20us%20to%20infer%20the%20set%0Aof%20objects%20in%20a%20scene%2C%20enabling%20robust%20generalization%20to%20new%20test%20scenes%20with%0Aan%20increased%20number%20of%20objects%20of%20new%20shapes.%20We%20further%20illustrate%20how%20this%0Aenables%20us%20to%20infer%20global%20scene%20factors%2C%20likewise%20enabling%20robust%0Ageneralization%20to%20new%20scenes.%20Finally%2C%20we%20illustrate%20how%20this%20approach%20can%20be%0Adirectly%20applied%20to%20existing%20pretrained%20text-to-image%20generative%20models%20for%0Azero-shot%20multi-object%20perception.%20Code%20and%20visualizations%20are%20at%0Ahttps%3A//energy-based-model.github.io/compositional-inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompositional%2520Scene%2520Understanding%2520through%2520Inverse%2520Generative%2520Modeling%26entry.906535625%3DYanbo%2520Wang%2520and%2520Justin%2520Dauwels%2520and%2520Yilun%2520Du%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520demonstrated%2520remarkable%2520abilities%2520in%2520generating%250Ahigh-fidelity%2520visual%2520content.%2520In%2520this%2520work%252C%2520we%2520explore%2520how%2520generative%2520models%250Acan%2520further%2520be%2520used%2520not%2520only%2520to%2520synthesize%2520visual%2520content%2520but%2520also%2520to%250Aunderstand%2520the%2520properties%2520of%2520a%2520scene%2520given%2520a%2520natural%2520image.%2520We%2520formulate%2520scene%250Aunderstanding%2520as%2520an%2520inverse%2520generative%2520modeling%2520problem%252C%2520where%2520we%2520seek%2520to%2520find%250Aconditional%2520parameters%2520of%2520a%2520visual%2520generative%2520model%2520to%2520best%2520fit%2520a%2520given%2520natural%250Aimage.%2520To%2520enable%2520this%2520procedure%2520to%2520infer%2520scene%2520structure%2520from%2520images%250Asubstantially%2520different%2520than%2520those%2520seen%2520during%2520training%252C%2520we%2520further%2520propose%2520to%250Abuild%2520this%2520visual%2520generative%2520model%2520compositionally%2520from%2520smaller%2520models%2520over%250Apieces%2520of%2520a%2520scene.%2520We%2520illustrate%2520how%2520this%2520procedure%2520enables%2520us%2520to%2520infer%2520the%2520set%250Aof%2520objects%2520in%2520a%2520scene%252C%2520enabling%2520robust%2520generalization%2520to%2520new%2520test%2520scenes%2520with%250Aan%2520increased%2520number%2520of%2520objects%2520of%2520new%2520shapes.%2520We%2520further%2520illustrate%2520how%2520this%250Aenables%2520us%2520to%2520infer%2520global%2520scene%2520factors%252C%2520likewise%2520enabling%2520robust%250Ageneralization%2520to%2520new%2520scenes.%2520Finally%252C%2520we%2520illustrate%2520how%2520this%2520approach%2520can%2520be%250Adirectly%2520applied%2520to%2520existing%2520pretrained%2520text-to-image%2520generative%2520models%2520for%250Azero-shot%2520multi-object%2520perception.%2520Code%2520and%2520visualizations%2520are%2520at%250Ahttps%253A//energy-based-model.github.io/compositional-inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%20Scene%20Understanding%20through%20Inverse%20Generative%20Modeling&entry.906535625=Yanbo%20Wang%20and%20Justin%20Dauwels%20and%20Yilun%20Du&entry.1292438233=%20%20Generative%20models%20have%20demonstrated%20remarkable%20abilities%20in%20generating%0Ahigh-fidelity%20visual%20content.%20In%20this%20work%2C%20we%20explore%20how%20generative%20models%0Acan%20further%20be%20used%20not%20only%20to%20synthesize%20visual%20content%20but%20also%20to%0Aunderstand%20the%20properties%20of%20a%20scene%20given%20a%20natural%20image.%20We%20formulate%20scene%0Aunderstanding%20as%20an%20inverse%20generative%20modeling%20problem%2C%20where%20we%20seek%20to%20find%0Aconditional%20parameters%20of%20a%20visual%20generative%20model%20to%20best%20fit%20a%20given%20natural%0Aimage.%20To%20enable%20this%20procedure%20to%20infer%20scene%20structure%20from%20images%0Asubstantially%20different%20than%20those%20seen%20during%20training%2C%20we%20further%20propose%20to%0Abuild%20this%20visual%20generative%20model%20compositionally%20from%20smaller%20models%20over%0Apieces%20of%20a%20scene.%20We%20illustrate%20how%20this%20procedure%20enables%20us%20to%20infer%20the%20set%0Aof%20objects%20in%20a%20scene%2C%20enabling%20robust%20generalization%20to%20new%20test%20scenes%20with%0Aan%20increased%20number%20of%20objects%20of%20new%20shapes.%20We%20further%20illustrate%20how%20this%0Aenables%20us%20to%20infer%20global%20scene%20factors%2C%20likewise%20enabling%20robust%0Ageneralization%20to%20new%20scenes.%20Finally%2C%20we%20illustrate%20how%20this%20approach%20can%20be%0Adirectly%20applied%20to%20existing%20pretrained%20text-to-image%20generative%20models%20for%0Azero-shot%20multi-object%20perception.%20Code%20and%20visualizations%20are%20at%0Ahttps%3A//energy-based-model.github.io/compositional-inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21780v2&entry.124074799=Read"},
{"title": "Satellite Imagery and AI: A New Era in Ocean Conservation, from Research\n  to Deployment and Impact (Version. 2.0)", "author": "Patrick Beukema and Favyen Bastani and Yawen Zheng and Piper Wolters and Henry Herzog and Joe Ferdinando", "abstract": "  Illegal, unreported, and unregulated (IUU) fishing poses a global threat to\nocean habitats. Publicly available satellite data offered by NASA, the European\nSpace Agency (ESA), and the U.S. Geological Survey (USGS), provide an\nopportunity to actively monitor this activity. Effectively leveraging satellite\ndata for maritime conservation requires highly reliable machine learning models\noperating globally with minimal latency. This paper introduces four specialized\ncomputer vision models designed for a variety of sensors including Sentinel-1\n(synthetic aperture radar), Sentinel-2 (optical imagery), Landsat 8-9 (optical\nimagery), and Suomi-NPP/NOAA-20/NOAA-21 (nighttime lights). It also presents\nbest practices for developing and deploying global-scale real-time satellite\nbased computer vision. All of the models are open sourced under permissive\nlicenses. These models have all been deployed in Skylight, a real-time maritime\nmonitoring platform, which is provided at no cost to users worldwide.\n", "link": "http://arxiv.org/abs/2312.03207v2", "date": "2025-05-29", "relevancy": 2.4473, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Satellite%20Imagery%20and%20AI%3A%20A%20New%20Era%20in%20Ocean%20Conservation%2C%20from%20Research%0A%20%20to%20Deployment%20and%20Impact%20%28Version.%202.0%29&body=Title%3A%20Satellite%20Imagery%20and%20AI%3A%20A%20New%20Era%20in%20Ocean%20Conservation%2C%20from%20Research%0A%20%20to%20Deployment%20and%20Impact%20%28Version.%202.0%29%0AAuthor%3A%20Patrick%20Beukema%20and%20Favyen%20Bastani%20and%20Yawen%20Zheng%20and%20Piper%20Wolters%20and%20Henry%20Herzog%20and%20Joe%20Ferdinando%0AAbstract%3A%20%20%20Illegal%2C%20unreported%2C%20and%20unregulated%20%28IUU%29%20fishing%20poses%20a%20global%20threat%20to%0Aocean%20habitats.%20Publicly%20available%20satellite%20data%20offered%20by%20NASA%2C%20the%20European%0ASpace%20Agency%20%28ESA%29%2C%20and%20the%20U.S.%20Geological%20Survey%20%28USGS%29%2C%20provide%20an%0Aopportunity%20to%20actively%20monitor%20this%20activity.%20Effectively%20leveraging%20satellite%0Adata%20for%20maritime%20conservation%20requires%20highly%20reliable%20machine%20learning%20models%0Aoperating%20globally%20with%20minimal%20latency.%20This%20paper%20introduces%20four%20specialized%0Acomputer%20vision%20models%20designed%20for%20a%20variety%20of%20sensors%20including%20Sentinel-1%0A%28synthetic%20aperture%20radar%29%2C%20Sentinel-2%20%28optical%20imagery%29%2C%20Landsat%208-9%20%28optical%0Aimagery%29%2C%20and%20Suomi-NPP/NOAA-20/NOAA-21%20%28nighttime%20lights%29.%20It%20also%20presents%0Abest%20practices%20for%20developing%20and%20deploying%20global-scale%20real-time%20satellite%0Abased%20computer%20vision.%20All%20of%20the%20models%20are%20open%20sourced%20under%20permissive%0Alicenses.%20These%20models%20have%20all%20been%20deployed%20in%20Skylight%2C%20a%20real-time%20maritime%0Amonitoring%20platform%2C%20which%20is%20provided%20at%20no%20cost%20to%20users%20worldwide.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03207v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSatellite%2520Imagery%2520and%2520AI%253A%2520A%2520New%2520Era%2520in%2520Ocean%2520Conservation%252C%2520from%2520Research%250A%2520%2520to%2520Deployment%2520and%2520Impact%2520%2528Version.%25202.0%2529%26entry.906535625%3DPatrick%2520Beukema%2520and%2520Favyen%2520Bastani%2520and%2520Yawen%2520Zheng%2520and%2520Piper%2520Wolters%2520and%2520Henry%2520Herzog%2520and%2520Joe%2520Ferdinando%26entry.1292438233%3D%2520%2520Illegal%252C%2520unreported%252C%2520and%2520unregulated%2520%2528IUU%2529%2520fishing%2520poses%2520a%2520global%2520threat%2520to%250Aocean%2520habitats.%2520Publicly%2520available%2520satellite%2520data%2520offered%2520by%2520NASA%252C%2520the%2520European%250ASpace%2520Agency%2520%2528ESA%2529%252C%2520and%2520the%2520U.S.%2520Geological%2520Survey%2520%2528USGS%2529%252C%2520provide%2520an%250Aopportunity%2520to%2520actively%2520monitor%2520this%2520activity.%2520Effectively%2520leveraging%2520satellite%250Adata%2520for%2520maritime%2520conservation%2520requires%2520highly%2520reliable%2520machine%2520learning%2520models%250Aoperating%2520globally%2520with%2520minimal%2520latency.%2520This%2520paper%2520introduces%2520four%2520specialized%250Acomputer%2520vision%2520models%2520designed%2520for%2520a%2520variety%2520of%2520sensors%2520including%2520Sentinel-1%250A%2528synthetic%2520aperture%2520radar%2529%252C%2520Sentinel-2%2520%2528optical%2520imagery%2529%252C%2520Landsat%25208-9%2520%2528optical%250Aimagery%2529%252C%2520and%2520Suomi-NPP/NOAA-20/NOAA-21%2520%2528nighttime%2520lights%2529.%2520It%2520also%2520presents%250Abest%2520practices%2520for%2520developing%2520and%2520deploying%2520global-scale%2520real-time%2520satellite%250Abased%2520computer%2520vision.%2520All%2520of%2520the%2520models%2520are%2520open%2520sourced%2520under%2520permissive%250Alicenses.%2520These%2520models%2520have%2520all%2520been%2520deployed%2520in%2520Skylight%252C%2520a%2520real-time%2520maritime%250Amonitoring%2520platform%252C%2520which%2520is%2520provided%2520at%2520no%2520cost%2520to%2520users%2520worldwide.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03207v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Satellite%20Imagery%20and%20AI%3A%20A%20New%20Era%20in%20Ocean%20Conservation%2C%20from%20Research%0A%20%20to%20Deployment%20and%20Impact%20%28Version.%202.0%29&entry.906535625=Patrick%20Beukema%20and%20Favyen%20Bastani%20and%20Yawen%20Zheng%20and%20Piper%20Wolters%20and%20Henry%20Herzog%20and%20Joe%20Ferdinando&entry.1292438233=%20%20Illegal%2C%20unreported%2C%20and%20unregulated%20%28IUU%29%20fishing%20poses%20a%20global%20threat%20to%0Aocean%20habitats.%20Publicly%20available%20satellite%20data%20offered%20by%20NASA%2C%20the%20European%0ASpace%20Agency%20%28ESA%29%2C%20and%20the%20U.S.%20Geological%20Survey%20%28USGS%29%2C%20provide%20an%0Aopportunity%20to%20actively%20monitor%20this%20activity.%20Effectively%20leveraging%20satellite%0Adata%20for%20maritime%20conservation%20requires%20highly%20reliable%20machine%20learning%20models%0Aoperating%20globally%20with%20minimal%20latency.%20This%20paper%20introduces%20four%20specialized%0Acomputer%20vision%20models%20designed%20for%20a%20variety%20of%20sensors%20including%20Sentinel-1%0A%28synthetic%20aperture%20radar%29%2C%20Sentinel-2%20%28optical%20imagery%29%2C%20Landsat%208-9%20%28optical%0Aimagery%29%2C%20and%20Suomi-NPP/NOAA-20/NOAA-21%20%28nighttime%20lights%29.%20It%20also%20presents%0Abest%20practices%20for%20developing%20and%20deploying%20global-scale%20real-time%20satellite%0Abased%20computer%20vision.%20All%20of%20the%20models%20are%20open%20sourced%20under%20permissive%0Alicenses.%20These%20models%20have%20all%20been%20deployed%20in%20Skylight%2C%20a%20real-time%20maritime%0Amonitoring%20platform%2C%20which%20is%20provided%20at%20no%20cost%20to%20users%20worldwide.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03207v2&entry.124074799=Read"},
{"title": "Adaptive Federated LoRA in Heterogeneous Wireless Networks with\n  Independent Sampling", "author": "Yanzhao Hou and Jiaxiang Geng and Boyu Li and Xiaofeng Tao and Juncheng Wang and Xiaodong Xu and Bing Luo", "abstract": "  Federated LoRA has emerged as a promising technique for efficiently\nfine-tuning large language models (LLMs) on distributed devices by reducing the\nnumber of trainable parameters. However, existing approaches often inadequately\noverlook the theoretical and practical implications of system and data\nheterogeneity, thereby failing to optimize the overall training efficiency,\nparticularly in terms of wall-clock time. In this paper, we propose an adaptive\nfederated LoRA strategy with independent client sampling to minimize the\nconvergence wall-clock time of federated fine-tuning under both computation and\ncommunication heterogeneity. We first derive a new convergence bound for\nfederated LoRA with arbitrary and independent client sampling, notably without\nrequiring the stringent bounded gradient assumption. Then, we introduce an\nadaptive bandwidth allocation scheme that accounts for heterogeneous client\nresources and system bandwidth constraints. Based on the derived theory, we\nformulate and solve a non-convex optimization problem to jointly determine the\nLoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock\nconvergence time. An efficient and low-complexity algorithm is developed to\napproximate the solution. Finally, extensive experiments demonstrate that our\napproach significantly reduces wall-clock training time compared to\nstate-of-the-art methods across various models and datasets.\n", "link": "http://arxiv.org/abs/2505.23555v1", "date": "2025-05-29", "relevancy": 2.4437, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5135}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5001}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Federated%20LoRA%20in%20Heterogeneous%20Wireless%20Networks%20with%0A%20%20Independent%20Sampling&body=Title%3A%20Adaptive%20Federated%20LoRA%20in%20Heterogeneous%20Wireless%20Networks%20with%0A%20%20Independent%20Sampling%0AAuthor%3A%20Yanzhao%20Hou%20and%20Jiaxiang%20Geng%20and%20Boyu%20Li%20and%20Xiaofeng%20Tao%20and%20Juncheng%20Wang%20and%20Xiaodong%20Xu%20and%20Bing%20Luo%0AAbstract%3A%20%20%20Federated%20LoRA%20has%20emerged%20as%20a%20promising%20technique%20for%20efficiently%0Afine-tuning%20large%20language%20models%20%28LLMs%29%20on%20distributed%20devices%20by%20reducing%20the%0Anumber%20of%20trainable%20parameters.%20However%2C%20existing%20approaches%20often%20inadequately%0Aoverlook%20the%20theoretical%20and%20practical%20implications%20of%20system%20and%20data%0Aheterogeneity%2C%20thereby%20failing%20to%20optimize%20the%20overall%20training%20efficiency%2C%0Aparticularly%20in%20terms%20of%20wall-clock%20time.%20In%20this%20paper%2C%20we%20propose%20an%20adaptive%0Afederated%20LoRA%20strategy%20with%20independent%20client%20sampling%20to%20minimize%20the%0Aconvergence%20wall-clock%20time%20of%20federated%20fine-tuning%20under%20both%20computation%20and%0Acommunication%20heterogeneity.%20We%20first%20derive%20a%20new%20convergence%20bound%20for%0Afederated%20LoRA%20with%20arbitrary%20and%20independent%20client%20sampling%2C%20notably%20without%0Arequiring%20the%20stringent%20bounded%20gradient%20assumption.%20Then%2C%20we%20introduce%20an%0Aadaptive%20bandwidth%20allocation%20scheme%20that%20accounts%20for%20heterogeneous%20client%0Aresources%20and%20system%20bandwidth%20constraints.%20Based%20on%20the%20derived%20theory%2C%20we%0Aformulate%20and%20solve%20a%20non-convex%20optimization%20problem%20to%20jointly%20determine%20the%0ALoRA%20sketching%20ratios%20and%20sampling%20probabilities%2C%20aiming%20to%20minimize%20wall-clock%0Aconvergence%20time.%20An%20efficient%20and%20low-complexity%20algorithm%20is%20developed%20to%0Aapproximate%20the%20solution.%20Finally%2C%20extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20significantly%20reduces%20wall-clock%20training%20time%20compared%20to%0Astate-of-the-art%20methods%20across%20various%20models%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Federated%2520LoRA%2520in%2520Heterogeneous%2520Wireless%2520Networks%2520with%250A%2520%2520Independent%2520Sampling%26entry.906535625%3DYanzhao%2520Hou%2520and%2520Jiaxiang%2520Geng%2520and%2520Boyu%2520Li%2520and%2520Xiaofeng%2520Tao%2520and%2520Juncheng%2520Wang%2520and%2520Xiaodong%2520Xu%2520and%2520Bing%2520Luo%26entry.1292438233%3D%2520%2520Federated%2520LoRA%2520has%2520emerged%2520as%2520a%2520promising%2520technique%2520for%2520efficiently%250Afine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520distributed%2520devices%2520by%2520reducing%2520the%250Anumber%2520of%2520trainable%2520parameters.%2520However%252C%2520existing%2520approaches%2520often%2520inadequately%250Aoverlook%2520the%2520theoretical%2520and%2520practical%2520implications%2520of%2520system%2520and%2520data%250Aheterogeneity%252C%2520thereby%2520failing%2520to%2520optimize%2520the%2520overall%2520training%2520efficiency%252C%250Aparticularly%2520in%2520terms%2520of%2520wall-clock%2520time.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520adaptive%250Afederated%2520LoRA%2520strategy%2520with%2520independent%2520client%2520sampling%2520to%2520minimize%2520the%250Aconvergence%2520wall-clock%2520time%2520of%2520federated%2520fine-tuning%2520under%2520both%2520computation%2520and%250Acommunication%2520heterogeneity.%2520We%2520first%2520derive%2520a%2520new%2520convergence%2520bound%2520for%250Afederated%2520LoRA%2520with%2520arbitrary%2520and%2520independent%2520client%2520sampling%252C%2520notably%2520without%250Arequiring%2520the%2520stringent%2520bounded%2520gradient%2520assumption.%2520Then%252C%2520we%2520introduce%2520an%250Aadaptive%2520bandwidth%2520allocation%2520scheme%2520that%2520accounts%2520for%2520heterogeneous%2520client%250Aresources%2520and%2520system%2520bandwidth%2520constraints.%2520Based%2520on%2520the%2520derived%2520theory%252C%2520we%250Aformulate%2520and%2520solve%2520a%2520non-convex%2520optimization%2520problem%2520to%2520jointly%2520determine%2520the%250ALoRA%2520sketching%2520ratios%2520and%2520sampling%2520probabilities%252C%2520aiming%2520to%2520minimize%2520wall-clock%250Aconvergence%2520time.%2520An%2520efficient%2520and%2520low-complexity%2520algorithm%2520is%2520developed%2520to%250Aapproximate%2520the%2520solution.%2520Finally%252C%2520extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520significantly%2520reduces%2520wall-clock%2520training%2520time%2520compared%2520to%250Astate-of-the-art%2520methods%2520across%2520various%2520models%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Federated%20LoRA%20in%20Heterogeneous%20Wireless%20Networks%20with%0A%20%20Independent%20Sampling&entry.906535625=Yanzhao%20Hou%20and%20Jiaxiang%20Geng%20and%20Boyu%20Li%20and%20Xiaofeng%20Tao%20and%20Juncheng%20Wang%20and%20Xiaodong%20Xu%20and%20Bing%20Luo&entry.1292438233=%20%20Federated%20LoRA%20has%20emerged%20as%20a%20promising%20technique%20for%20efficiently%0Afine-tuning%20large%20language%20models%20%28LLMs%29%20on%20distributed%20devices%20by%20reducing%20the%0Anumber%20of%20trainable%20parameters.%20However%2C%20existing%20approaches%20often%20inadequately%0Aoverlook%20the%20theoretical%20and%20practical%20implications%20of%20system%20and%20data%0Aheterogeneity%2C%20thereby%20failing%20to%20optimize%20the%20overall%20training%20efficiency%2C%0Aparticularly%20in%20terms%20of%20wall-clock%20time.%20In%20this%20paper%2C%20we%20propose%20an%20adaptive%0Afederated%20LoRA%20strategy%20with%20independent%20client%20sampling%20to%20minimize%20the%0Aconvergence%20wall-clock%20time%20of%20federated%20fine-tuning%20under%20both%20computation%20and%0Acommunication%20heterogeneity.%20We%20first%20derive%20a%20new%20convergence%20bound%20for%0Afederated%20LoRA%20with%20arbitrary%20and%20independent%20client%20sampling%2C%20notably%20without%0Arequiring%20the%20stringent%20bounded%20gradient%20assumption.%20Then%2C%20we%20introduce%20an%0Aadaptive%20bandwidth%20allocation%20scheme%20that%20accounts%20for%20heterogeneous%20client%0Aresources%20and%20system%20bandwidth%20constraints.%20Based%20on%20the%20derived%20theory%2C%20we%0Aformulate%20and%20solve%20a%20non-convex%20optimization%20problem%20to%20jointly%20determine%20the%0ALoRA%20sketching%20ratios%20and%20sampling%20probabilities%2C%20aiming%20to%20minimize%20wall-clock%0Aconvergence%20time.%20An%20efficient%20and%20low-complexity%20algorithm%20is%20developed%20to%0Aapproximate%20the%20solution.%20Finally%2C%20extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20significantly%20reduces%20wall-clock%20training%20time%20compared%20to%0Astate-of-the-art%20methods%20across%20various%20models%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23555v1&entry.124074799=Read"},
{"title": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large\n  Language Models in Code Generation", "author": "Wenhao Hu and Jinhao Duan and Chunchen Wei and Li Zhang and Yue Zhang and Kaidi Xu", "abstract": "  The rapid advancement of large language models (LLMs) has significantly\nimproved their performance in code generation tasks. However, existing code\nbenchmarks remain static, consisting of fixed datasets with predefined\nproblems. This makes them vulnerable to memorization during training, where\nLLMs recall specific test cases instead of generalizing to new problems,\nleading to data contamination and unreliable evaluation results. To address\nthese issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that\novercomes the limitations of static datasets. DynaCode evaluates LLMs\nsystematically using a complexity-aware metric, incorporating both code\ncomplexity and call-graph structures. DynaCode achieves large-scale diversity,\ngenerating up to 189 million unique nested code problems across four distinct\nlevels of code complexity, referred to as units, and 16 types of call graphs.\nResults on 12 latest LLMs show an average performance drop of 16.8% to 45.7%\ncompared to MBPP+, a static code generation benchmark, with performance\nprogressively decreasing as complexity increases. This demonstrates DynaCode's\nability to effectively differentiate LLMs. Additionally, by leveraging call\ngraphs, we gain insights into LLM behavior, particularly their preference for\nhandling subfunction interactions within nested code. Our benchmark and\nevaluation code are available at https://github.com/HWH-2000/DynaCode.\n", "link": "http://arxiv.org/abs/2503.10452v2", "date": "2025-05-29", "relevancy": 2.4406, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5009}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5009}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaCode%3A%20A%20Dynamic%20Complexity-Aware%20Code%20Benchmark%20for%20Evaluating%20Large%0A%20%20Language%20Models%20in%20Code%20Generation&body=Title%3A%20DynaCode%3A%20A%20Dynamic%20Complexity-Aware%20Code%20Benchmark%20for%20Evaluating%20Large%0A%20%20Language%20Models%20in%20Code%20Generation%0AAuthor%3A%20Wenhao%20Hu%20and%20Jinhao%20Duan%20and%20Chunchen%20Wei%20and%20Li%20Zhang%20and%20Yue%20Zhang%20and%20Kaidi%20Xu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20significantly%0Aimproved%20their%20performance%20in%20code%20generation%20tasks.%20However%2C%20existing%20code%0Abenchmarks%20remain%20static%2C%20consisting%20of%20fixed%20datasets%20with%20predefined%0Aproblems.%20This%20makes%20them%20vulnerable%20to%20memorization%20during%20training%2C%20where%0ALLMs%20recall%20specific%20test%20cases%20instead%20of%20generalizing%20to%20new%20problems%2C%0Aleading%20to%20data%20contamination%20and%20unreliable%20evaluation%20results.%20To%20address%0Athese%20issues%2C%20we%20introduce%20DynaCode%2C%20a%20dynamic%2C%20complexity-aware%20benchmark%20that%0Aovercomes%20the%20limitations%20of%20static%20datasets.%20DynaCode%20evaluates%20LLMs%0Asystematically%20using%20a%20complexity-aware%20metric%2C%20incorporating%20both%20code%0Acomplexity%20and%20call-graph%20structures.%20DynaCode%20achieves%20large-scale%20diversity%2C%0Agenerating%20up%20to%20189%20million%20unique%20nested%20code%20problems%20across%20four%20distinct%0Alevels%20of%20code%20complexity%2C%20referred%20to%20as%20units%2C%20and%2016%20types%20of%20call%20graphs.%0AResults%20on%2012%20latest%20LLMs%20show%20an%20average%20performance%20drop%20of%2016.8%25%20to%2045.7%25%0Acompared%20to%20MBPP%2B%2C%20a%20static%20code%20generation%20benchmark%2C%20with%20performance%0Aprogressively%20decreasing%20as%20complexity%20increases.%20This%20demonstrates%20DynaCode%27s%0Aability%20to%20effectively%20differentiate%20LLMs.%20Additionally%2C%20by%20leveraging%20call%0Agraphs%2C%20we%20gain%20insights%20into%20LLM%20behavior%2C%20particularly%20their%20preference%20for%0Ahandling%20subfunction%20interactions%20within%20nested%20code.%20Our%20benchmark%20and%0Aevaluation%20code%20are%20available%20at%20https%3A//github.com/HWH-2000/DynaCode.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10452v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaCode%253A%2520A%2520Dynamic%2520Complexity-Aware%2520Code%2520Benchmark%2520for%2520Evaluating%2520Large%250A%2520%2520Language%2520Models%2520in%2520Code%2520Generation%26entry.906535625%3DWenhao%2520Hu%2520and%2520Jinhao%2520Duan%2520and%2520Chunchen%2520Wei%2520and%2520Li%2520Zhang%2520and%2520Yue%2520Zhang%2520and%2520Kaidi%2520Xu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520significantly%250Aimproved%2520their%2520performance%2520in%2520code%2520generation%2520tasks.%2520However%252C%2520existing%2520code%250Abenchmarks%2520remain%2520static%252C%2520consisting%2520of%2520fixed%2520datasets%2520with%2520predefined%250Aproblems.%2520This%2520makes%2520them%2520vulnerable%2520to%2520memorization%2520during%2520training%252C%2520where%250ALLMs%2520recall%2520specific%2520test%2520cases%2520instead%2520of%2520generalizing%2520to%2520new%2520problems%252C%250Aleading%2520to%2520data%2520contamination%2520and%2520unreliable%2520evaluation%2520results.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520DynaCode%252C%2520a%2520dynamic%252C%2520complexity-aware%2520benchmark%2520that%250Aovercomes%2520the%2520limitations%2520of%2520static%2520datasets.%2520DynaCode%2520evaluates%2520LLMs%250Asystematically%2520using%2520a%2520complexity-aware%2520metric%252C%2520incorporating%2520both%2520code%250Acomplexity%2520and%2520call-graph%2520structures.%2520DynaCode%2520achieves%2520large-scale%2520diversity%252C%250Agenerating%2520up%2520to%2520189%2520million%2520unique%2520nested%2520code%2520problems%2520across%2520four%2520distinct%250Alevels%2520of%2520code%2520complexity%252C%2520referred%2520to%2520as%2520units%252C%2520and%252016%2520types%2520of%2520call%2520graphs.%250AResults%2520on%252012%2520latest%2520LLMs%2520show%2520an%2520average%2520performance%2520drop%2520of%252016.8%2525%2520to%252045.7%2525%250Acompared%2520to%2520MBPP%252B%252C%2520a%2520static%2520code%2520generation%2520benchmark%252C%2520with%2520performance%250Aprogressively%2520decreasing%2520as%2520complexity%2520increases.%2520This%2520demonstrates%2520DynaCode%2527s%250Aability%2520to%2520effectively%2520differentiate%2520LLMs.%2520Additionally%252C%2520by%2520leveraging%2520call%250Agraphs%252C%2520we%2520gain%2520insights%2520into%2520LLM%2520behavior%252C%2520particularly%2520their%2520preference%2520for%250Ahandling%2520subfunction%2520interactions%2520within%2520nested%2520code.%2520Our%2520benchmark%2520and%250Aevaluation%2520code%2520are%2520available%2520at%2520https%253A//github.com/HWH-2000/DynaCode.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10452v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaCode%3A%20A%20Dynamic%20Complexity-Aware%20Code%20Benchmark%20for%20Evaluating%20Large%0A%20%20Language%20Models%20in%20Code%20Generation&entry.906535625=Wenhao%20Hu%20and%20Jinhao%20Duan%20and%20Chunchen%20Wei%20and%20Li%20Zhang%20and%20Yue%20Zhang%20and%20Kaidi%20Xu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20significantly%0Aimproved%20their%20performance%20in%20code%20generation%20tasks.%20However%2C%20existing%20code%0Abenchmarks%20remain%20static%2C%20consisting%20of%20fixed%20datasets%20with%20predefined%0Aproblems.%20This%20makes%20them%20vulnerable%20to%20memorization%20during%20training%2C%20where%0ALLMs%20recall%20specific%20test%20cases%20instead%20of%20generalizing%20to%20new%20problems%2C%0Aleading%20to%20data%20contamination%20and%20unreliable%20evaluation%20results.%20To%20address%0Athese%20issues%2C%20we%20introduce%20DynaCode%2C%20a%20dynamic%2C%20complexity-aware%20benchmark%20that%0Aovercomes%20the%20limitations%20of%20static%20datasets.%20DynaCode%20evaluates%20LLMs%0Asystematically%20using%20a%20complexity-aware%20metric%2C%20incorporating%20both%20code%0Acomplexity%20and%20call-graph%20structures.%20DynaCode%20achieves%20large-scale%20diversity%2C%0Agenerating%20up%20to%20189%20million%20unique%20nested%20code%20problems%20across%20four%20distinct%0Alevels%20of%20code%20complexity%2C%20referred%20to%20as%20units%2C%20and%2016%20types%20of%20call%20graphs.%0AResults%20on%2012%20latest%20LLMs%20show%20an%20average%20performance%20drop%20of%2016.8%25%20to%2045.7%25%0Acompared%20to%20MBPP%2B%2C%20a%20static%20code%20generation%20benchmark%2C%20with%20performance%0Aprogressively%20decreasing%20as%20complexity%20increases.%20This%20demonstrates%20DynaCode%27s%0Aability%20to%20effectively%20differentiate%20LLMs.%20Additionally%2C%20by%20leveraging%20call%0Agraphs%2C%20we%20gain%20insights%20into%20LLM%20behavior%2C%20particularly%20their%20preference%20for%0Ahandling%20subfunction%20interactions%20within%20nested%20code.%20Our%20benchmark%20and%0Aevaluation%20code%20are%20available%20at%20https%3A//github.com/HWH-2000/DynaCode.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10452v2&entry.124074799=Read"},
{"title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA", "author": "Minrui Luo and Fuhang Kuang and Yu Wang and Zirui Liu and Tianxing He", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods.\n", "link": "http://arxiv.org/abs/2505.23724v1", "date": "2025-05-29", "relevancy": 2.4323, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4744}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SC-LoRA%3A%20Balancing%20Efficient%20Fine-tuning%20and%20Knowledge%20Preservation%20via%0A%20%20Subspace-Constrained%20LoRA&body=Title%3A%20SC-LoRA%3A%20Balancing%20Efficient%20Fine-tuning%20and%20Knowledge%20Preservation%20via%0A%20%20Subspace-Constrained%20LoRA%0AAuthor%3A%20Minrui%20Luo%20and%20Fuhang%20Kuang%20and%20Yu%20Wang%20and%20Zirui%20Liu%20and%20Tianxing%20He%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20particularly%20Low-Rank%0AAdaptation%20%28LoRA%29%2C%20are%20indispensable%20for%20efficiently%20customizing%20Large%20Language%0AModels%20%28LLMs%29.%20However%2C%20vanilla%20LoRA%20suffers%20from%20slow%20convergence%20speed%20and%0Aknowledge%20forgetting%20problems.%20Recent%20studies%20have%20leveraged%20the%20power%20of%0Adesigned%20LoRA%20initialization%2C%20to%20enhance%20the%20fine-tuning%20efficiency%2C%20or%20to%0Apreserve%20knowledge%20in%20the%20pre-trained%20LLM.%20However%2C%20none%20of%20these%20works%20can%0Aaddress%20the%20two%20cases%20at%20the%20same%20time.%20To%20this%20end%2C%20we%20introduce%0ASubspace-Constrained%20LoRA%20%28SC-LoRA%29%2C%20a%20novel%20LoRA%20initialization%20framework%0Aengineered%20to%20navigate%20the%20trade-off%20between%20efficient%20fine-tuning%20and%0Aknowledge%20preservation.%20We%20achieve%20this%20by%20constraining%20the%20output%20of%20trainable%0ALoRA%20adapters%20in%20a%20low-rank%20subspace%2C%20where%20the%20context%20information%20of%0Afine-tuning%20data%20is%20most%20preserved%20while%20the%20context%20information%20of%20preserved%0Aknowledge%20is%20least%20retained%2C%20in%20a%20balanced%20way.%20Such%20constraint%20enables%20the%0Atrainable%20weights%20to%20primarily%20focus%20on%20the%20main%20features%20of%20fine-tuning%20data%0Awhile%20avoiding%20damaging%20the%20preserved%20knowledge%20features.%20We%20provide%0Atheoretical%20analysis%20on%20our%20method%2C%20and%20conduct%20extensive%20experiments%20including%0Asafety%20preservation%20and%20world%20knowledge%20preservation%2C%20on%20various%20downstream%0Atasks.%20In%20our%20experiments%2C%20SC-LoRA%20succeeds%20in%20delivering%20superior%20fine-tuning%0Aperformance%20while%20markedly%20diminishing%20knowledge%20forgetting%2C%20surpassing%0Acontemporary%20LoRA%20initialization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSC-LoRA%253A%2520Balancing%2520Efficient%2520Fine-tuning%2520and%2520Knowledge%2520Preservation%2520via%250A%2520%2520Subspace-Constrained%2520LoRA%26entry.906535625%3DMinrui%2520Luo%2520and%2520Fuhang%2520Kuang%2520and%2520Yu%2520Wang%2520and%2520Zirui%2520Liu%2520and%2520Tianxing%2520He%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%252C%2520particularly%2520Low-Rank%250AAdaptation%2520%2528LoRA%2529%252C%2520are%2520indispensable%2520for%2520efficiently%2520customizing%2520Large%2520Language%250AModels%2520%2528LLMs%2529.%2520However%252C%2520vanilla%2520LoRA%2520suffers%2520from%2520slow%2520convergence%2520speed%2520and%250Aknowledge%2520forgetting%2520problems.%2520Recent%2520studies%2520have%2520leveraged%2520the%2520power%2520of%250Adesigned%2520LoRA%2520initialization%252C%2520to%2520enhance%2520the%2520fine-tuning%2520efficiency%252C%2520or%2520to%250Apreserve%2520knowledge%2520in%2520the%2520pre-trained%2520LLM.%2520However%252C%2520none%2520of%2520these%2520works%2520can%250Aaddress%2520the%2520two%2520cases%2520at%2520the%2520same%2520time.%2520To%2520this%2520end%252C%2520we%2520introduce%250ASubspace-Constrained%2520LoRA%2520%2528SC-LoRA%2529%252C%2520a%2520novel%2520LoRA%2520initialization%2520framework%250Aengineered%2520to%2520navigate%2520the%2520trade-off%2520between%2520efficient%2520fine-tuning%2520and%250Aknowledge%2520preservation.%2520We%2520achieve%2520this%2520by%2520constraining%2520the%2520output%2520of%2520trainable%250ALoRA%2520adapters%2520in%2520a%2520low-rank%2520subspace%252C%2520where%2520the%2520context%2520information%2520of%250Afine-tuning%2520data%2520is%2520most%2520preserved%2520while%2520the%2520context%2520information%2520of%2520preserved%250Aknowledge%2520is%2520least%2520retained%252C%2520in%2520a%2520balanced%2520way.%2520Such%2520constraint%2520enables%2520the%250Atrainable%2520weights%2520to%2520primarily%2520focus%2520on%2520the%2520main%2520features%2520of%2520fine-tuning%2520data%250Awhile%2520avoiding%2520damaging%2520the%2520preserved%2520knowledge%2520features.%2520We%2520provide%250Atheoretical%2520analysis%2520on%2520our%2520method%252C%2520and%2520conduct%2520extensive%2520experiments%2520including%250Asafety%2520preservation%2520and%2520world%2520knowledge%2520preservation%252C%2520on%2520various%2520downstream%250Atasks.%2520In%2520our%2520experiments%252C%2520SC-LoRA%2520succeeds%2520in%2520delivering%2520superior%2520fine-tuning%250Aperformance%2520while%2520markedly%2520diminishing%2520knowledge%2520forgetting%252C%2520surpassing%250Acontemporary%2520LoRA%2520initialization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SC-LoRA%3A%20Balancing%20Efficient%20Fine-tuning%20and%20Knowledge%20Preservation%20via%0A%20%20Subspace-Constrained%20LoRA&entry.906535625=Minrui%20Luo%20and%20Fuhang%20Kuang%20and%20Yu%20Wang%20and%20Zirui%20Liu%20and%20Tianxing%20He&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20particularly%20Low-Rank%0AAdaptation%20%28LoRA%29%2C%20are%20indispensable%20for%20efficiently%20customizing%20Large%20Language%0AModels%20%28LLMs%29.%20However%2C%20vanilla%20LoRA%20suffers%20from%20slow%20convergence%20speed%20and%0Aknowledge%20forgetting%20problems.%20Recent%20studies%20have%20leveraged%20the%20power%20of%0Adesigned%20LoRA%20initialization%2C%20to%20enhance%20the%20fine-tuning%20efficiency%2C%20or%20to%0Apreserve%20knowledge%20in%20the%20pre-trained%20LLM.%20However%2C%20none%20of%20these%20works%20can%0Aaddress%20the%20two%20cases%20at%20the%20same%20time.%20To%20this%20end%2C%20we%20introduce%0ASubspace-Constrained%20LoRA%20%28SC-LoRA%29%2C%20a%20novel%20LoRA%20initialization%20framework%0Aengineered%20to%20navigate%20the%20trade-off%20between%20efficient%20fine-tuning%20and%0Aknowledge%20preservation.%20We%20achieve%20this%20by%20constraining%20the%20output%20of%20trainable%0ALoRA%20adapters%20in%20a%20low-rank%20subspace%2C%20where%20the%20context%20information%20of%0Afine-tuning%20data%20is%20most%20preserved%20while%20the%20context%20information%20of%20preserved%0Aknowledge%20is%20least%20retained%2C%20in%20a%20balanced%20way.%20Such%20constraint%20enables%20the%0Atrainable%20weights%20to%20primarily%20focus%20on%20the%20main%20features%20of%20fine-tuning%20data%0Awhile%20avoiding%20damaging%20the%20preserved%20knowledge%20features.%20We%20provide%0Atheoretical%20analysis%20on%20our%20method%2C%20and%20conduct%20extensive%20experiments%20including%0Asafety%20preservation%20and%20world%20knowledge%20preservation%2C%20on%20various%20downstream%0Atasks.%20In%20our%20experiments%2C%20SC-LoRA%20succeeds%20in%20delivering%20superior%20fine-tuning%0Aperformance%20while%20markedly%20diminishing%20knowledge%20forgetting%2C%20surpassing%0Acontemporary%20LoRA%20initialization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23724v1&entry.124074799=Read"},
{"title": "Personality-Guided Code Generation Using Large Language Models", "author": "Yaoqi Guo and Zhenpeng Chen and Jie M. Zhang and Yang Liu and Yun Ma", "abstract": "  Code generation, the automatic creation of source code from natural language\ndescriptions, has garnered significant attention due to its potential to\nstreamline software development. Inspired by research that links\ntask-personality alignment with improved development outcomes, we conduct an\nempirical study on personality-guided code generation using large language\nmodels (LLMs). Specifically, we investigate how emulating personality traits\nappropriate to the coding tasks affects LLM performance. We extensively\nevaluate this approach using seven widely adopted LLMs across four\nrepresentative datasets. Our results show that personality guidance\nsignificantly enhances code generation accuracy, with improved pass rates in 23\nout of 28 LLM-dataset combinations. Notably, in 11 cases, the improvement\nexceeds 5%, and in 5 instances, it surpasses 10%, with the highest gain\nreaching 12.9%. Additionally, personality guidance can be easily integrated\nwith other prompting strategies to further boost performance. We open-source\nour code and data at https://github.com/IanWalls/Persona-Code.\n", "link": "http://arxiv.org/abs/2411.00006v2", "date": "2025-05-29", "relevancy": 2.4216, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5321}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personality-Guided%20Code%20Generation%20Using%20Large%20Language%20Models&body=Title%3A%20Personality-Guided%20Code%20Generation%20Using%20Large%20Language%20Models%0AAuthor%3A%20Yaoqi%20Guo%20and%20Zhenpeng%20Chen%20and%20Jie%20M.%20Zhang%20and%20Yang%20Liu%20and%20Yun%20Ma%0AAbstract%3A%20%20%20Code%20generation%2C%20the%20automatic%20creation%20of%20source%20code%20from%20natural%20language%0Adescriptions%2C%20has%20garnered%20significant%20attention%20due%20to%20its%20potential%20to%0Astreamline%20software%20development.%20Inspired%20by%20research%20that%20links%0Atask-personality%20alignment%20with%20improved%20development%20outcomes%2C%20we%20conduct%20an%0Aempirical%20study%20on%20personality-guided%20code%20generation%20using%20large%20language%0Amodels%20%28LLMs%29.%20Specifically%2C%20we%20investigate%20how%20emulating%20personality%20traits%0Aappropriate%20to%20the%20coding%20tasks%20affects%20LLM%20performance.%20We%20extensively%0Aevaluate%20this%20approach%20using%20seven%20widely%20adopted%20LLMs%20across%20four%0Arepresentative%20datasets.%20Our%20results%20show%20that%20personality%20guidance%0Asignificantly%20enhances%20code%20generation%20accuracy%2C%20with%20improved%20pass%20rates%20in%2023%0Aout%20of%2028%20LLM-dataset%20combinations.%20Notably%2C%20in%2011%20cases%2C%20the%20improvement%0Aexceeds%205%25%2C%20and%20in%205%20instances%2C%20it%20surpasses%2010%25%2C%20with%20the%20highest%20gain%0Areaching%2012.9%25.%20Additionally%2C%20personality%20guidance%20can%20be%20easily%20integrated%0Awith%20other%20prompting%20strategies%20to%20further%20boost%20performance.%20We%20open-source%0Aour%20code%20and%20data%20at%20https%3A//github.com/IanWalls/Persona-Code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonality-Guided%2520Code%2520Generation%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DYaoqi%2520Guo%2520and%2520Zhenpeng%2520Chen%2520and%2520Jie%2520M.%2520Zhang%2520and%2520Yang%2520Liu%2520and%2520Yun%2520Ma%26entry.1292438233%3D%2520%2520Code%2520generation%252C%2520the%2520automatic%2520creation%2520of%2520source%2520code%2520from%2520natural%2520language%250Adescriptions%252C%2520has%2520garnered%2520significant%2520attention%2520due%2520to%2520its%2520potential%2520to%250Astreamline%2520software%2520development.%2520Inspired%2520by%2520research%2520that%2520links%250Atask-personality%2520alignment%2520with%2520improved%2520development%2520outcomes%252C%2520we%2520conduct%2520an%250Aempirical%2520study%2520on%2520personality-guided%2520code%2520generation%2520using%2520large%2520language%250Amodels%2520%2528LLMs%2529.%2520Specifically%252C%2520we%2520investigate%2520how%2520emulating%2520personality%2520traits%250Aappropriate%2520to%2520the%2520coding%2520tasks%2520affects%2520LLM%2520performance.%2520We%2520extensively%250Aevaluate%2520this%2520approach%2520using%2520seven%2520widely%2520adopted%2520LLMs%2520across%2520four%250Arepresentative%2520datasets.%2520Our%2520results%2520show%2520that%2520personality%2520guidance%250Asignificantly%2520enhances%2520code%2520generation%2520accuracy%252C%2520with%2520improved%2520pass%2520rates%2520in%252023%250Aout%2520of%252028%2520LLM-dataset%2520combinations.%2520Notably%252C%2520in%252011%2520cases%252C%2520the%2520improvement%250Aexceeds%25205%2525%252C%2520and%2520in%25205%2520instances%252C%2520it%2520surpasses%252010%2525%252C%2520with%2520the%2520highest%2520gain%250Areaching%252012.9%2525.%2520Additionally%252C%2520personality%2520guidance%2520can%2520be%2520easily%2520integrated%250Awith%2520other%2520prompting%2520strategies%2520to%2520further%2520boost%2520performance.%2520We%2520open-source%250Aour%2520code%2520and%2520data%2520at%2520https%253A//github.com/IanWalls/Persona-Code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personality-Guided%20Code%20Generation%20Using%20Large%20Language%20Models&entry.906535625=Yaoqi%20Guo%20and%20Zhenpeng%20Chen%20and%20Jie%20M.%20Zhang%20and%20Yang%20Liu%20and%20Yun%20Ma&entry.1292438233=%20%20Code%20generation%2C%20the%20automatic%20creation%20of%20source%20code%20from%20natural%20language%0Adescriptions%2C%20has%20garnered%20significant%20attention%20due%20to%20its%20potential%20to%0Astreamline%20software%20development.%20Inspired%20by%20research%20that%20links%0Atask-personality%20alignment%20with%20improved%20development%20outcomes%2C%20we%20conduct%20an%0Aempirical%20study%20on%20personality-guided%20code%20generation%20using%20large%20language%0Amodels%20%28LLMs%29.%20Specifically%2C%20we%20investigate%20how%20emulating%20personality%20traits%0Aappropriate%20to%20the%20coding%20tasks%20affects%20LLM%20performance.%20We%20extensively%0Aevaluate%20this%20approach%20using%20seven%20widely%20adopted%20LLMs%20across%20four%0Arepresentative%20datasets.%20Our%20results%20show%20that%20personality%20guidance%0Asignificantly%20enhances%20code%20generation%20accuracy%2C%20with%20improved%20pass%20rates%20in%2023%0Aout%20of%2028%20LLM-dataset%20combinations.%20Notably%2C%20in%2011%20cases%2C%20the%20improvement%0Aexceeds%205%25%2C%20and%20in%205%20instances%2C%20it%20surpasses%2010%25%2C%20with%20the%20highest%20gain%0Areaching%2012.9%25.%20Additionally%2C%20personality%20guidance%20can%20be%20easily%20integrated%0Awith%20other%20prompting%20strategies%20to%20further%20boost%20performance.%20We%20open-source%0Aour%20code%20and%20data%20at%20https%3A//github.com/IanWalls/Persona-Code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00006v2&entry.124074799=Read"},
{"title": "A Comprehensive Evaluation of Multi-Modal Large Language Models for\n  Endoscopy Analysis", "author": "Shengyuan Liu and Boyun Zheng and Wenting Chen and Zhihao Peng and Zhenfei Yin and Jing Shao and Jiancong Hu and Yixuan Yuan", "abstract": "  Endoscopic procedures are essential for diagnosing and treating internal\ndiseases, and multi-modal large language models (MLLMs) are increasingly\napplied to assist in endoscopy analysis. However, current benchmarks are\nlimited, as they typically cover specific endoscopic scenarios and a small set\nof clinical tasks, failing to capture the real-world diversity of endoscopic\nscenarios and the full range of skills needed in clinical workflows. To address\nthese issues, we introduce EndoBench, the first comprehensive benchmark\nspecifically designed to assess MLLMs across the full spectrum of endoscopic\npractice with multi-dimensional capacities. EndoBench encompasses 4 distinct\nendoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks,\nand 5 levels of visual prompting granularities, resulting in 6,832 rigorously\nvalidated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation\nframework mirrors the clinical workflow--spanning anatomical recognition,\nlesion analysis, spatial localization, and surgical operations--to holistically\ngauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios.\nWe benchmark 23 state-of-the-art models, including general-purpose,\nmedical-specialized, and proprietary MLLMs, and establish human clinician\nperformance as a reference standard. Our extensive experiments reveal: (1)\nproprietary MLLMs outperform open-source and medical-specialized models\noverall, but still trail human experts; (2) medical-domain supervised\nfine-tuning substantially boosts task-specific accuracy; and (3) model\nperformance remains sensitive to prompt format and clinical task complexity.\nEndoBench establishes a new standard for evaluating and advancing MLLMs in\nendoscopy, highlighting both progress and persistent gaps between current\nmodels and expert clinical reasoning. We publicly release our benchmark and\ncode.\n", "link": "http://arxiv.org/abs/2505.23601v1", "date": "2025-05-29", "relevancy": 2.3954, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Evaluation%20of%20Multi-Modal%20Large%20Language%20Models%20for%0A%20%20Endoscopy%20Analysis&body=Title%3A%20A%20Comprehensive%20Evaluation%20of%20Multi-Modal%20Large%20Language%20Models%20for%0A%20%20Endoscopy%20Analysis%0AAuthor%3A%20Shengyuan%20Liu%20and%20Boyun%20Zheng%20and%20Wenting%20Chen%20and%20Zhihao%20Peng%20and%20Zhenfei%20Yin%20and%20Jing%20Shao%20and%20Jiancong%20Hu%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20Endoscopic%20procedures%20are%20essential%20for%20diagnosing%20and%20treating%20internal%0Adiseases%2C%20and%20multi-modal%20large%20language%20models%20%28MLLMs%29%20are%20increasingly%0Aapplied%20to%20assist%20in%20endoscopy%20analysis.%20However%2C%20current%20benchmarks%20are%0Alimited%2C%20as%20they%20typically%20cover%20specific%20endoscopic%20scenarios%20and%20a%20small%20set%0Aof%20clinical%20tasks%2C%20failing%20to%20capture%20the%20real-world%20diversity%20of%20endoscopic%0Ascenarios%20and%20the%20full%20range%20of%20skills%20needed%20in%20clinical%20workflows.%20To%20address%0Athese%20issues%2C%20we%20introduce%20EndoBench%2C%20the%20first%20comprehensive%20benchmark%0Aspecifically%20designed%20to%20assess%20MLLMs%20across%20the%20full%20spectrum%20of%20endoscopic%0Apractice%20with%20multi-dimensional%20capacities.%20EndoBench%20encompasses%204%20distinct%0Aendoscopic%20scenarios%2C%2012%20specialized%20clinical%20tasks%20with%2012%20secondary%20subtasks%2C%0Aand%205%20levels%20of%20visual%20prompting%20granularities%2C%20resulting%20in%206%2C832%20rigorously%0Avalidated%20VQA%20pairs%20from%2021%20diverse%20datasets.%20Our%20multi-dimensional%20evaluation%0Aframework%20mirrors%20the%20clinical%20workflow--spanning%20anatomical%20recognition%2C%0Alesion%20analysis%2C%20spatial%20localization%2C%20and%20surgical%20operations--to%20holistically%0Agauge%20the%20perceptual%20and%20diagnostic%20abilities%20of%20MLLMs%20in%20realistic%20scenarios.%0AWe%20benchmark%2023%20state-of-the-art%20models%2C%20including%20general-purpose%2C%0Amedical-specialized%2C%20and%20proprietary%20MLLMs%2C%20and%20establish%20human%20clinician%0Aperformance%20as%20a%20reference%20standard.%20Our%20extensive%20experiments%20reveal%3A%20%281%29%0Aproprietary%20MLLMs%20outperform%20open-source%20and%20medical-specialized%20models%0Aoverall%2C%20but%20still%20trail%20human%20experts%3B%20%282%29%20medical-domain%20supervised%0Afine-tuning%20substantially%20boosts%20task-specific%20accuracy%3B%20and%20%283%29%20model%0Aperformance%20remains%20sensitive%20to%20prompt%20format%20and%20clinical%20task%20complexity.%0AEndoBench%20establishes%20a%20new%20standard%20for%20evaluating%20and%20advancing%20MLLMs%20in%0Aendoscopy%2C%20highlighting%20both%20progress%20and%20persistent%20gaps%20between%20current%0Amodels%20and%20expert%20clinical%20reasoning.%20We%20publicly%20release%20our%20benchmark%20and%0Acode.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Evaluation%2520of%2520Multi-Modal%2520Large%2520Language%2520Models%2520for%250A%2520%2520Endoscopy%2520Analysis%26entry.906535625%3DShengyuan%2520Liu%2520and%2520Boyun%2520Zheng%2520and%2520Wenting%2520Chen%2520and%2520Zhihao%2520Peng%2520and%2520Zhenfei%2520Yin%2520and%2520Jing%2520Shao%2520and%2520Jiancong%2520Hu%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520Endoscopic%2520procedures%2520are%2520essential%2520for%2520diagnosing%2520and%2520treating%2520internal%250Adiseases%252C%2520and%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520are%2520increasingly%250Aapplied%2520to%2520assist%2520in%2520endoscopy%2520analysis.%2520However%252C%2520current%2520benchmarks%2520are%250Alimited%252C%2520as%2520they%2520typically%2520cover%2520specific%2520endoscopic%2520scenarios%2520and%2520a%2520small%2520set%250Aof%2520clinical%2520tasks%252C%2520failing%2520to%2520capture%2520the%2520real-world%2520diversity%2520of%2520endoscopic%250Ascenarios%2520and%2520the%2520full%2520range%2520of%2520skills%2520needed%2520in%2520clinical%2520workflows.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520EndoBench%252C%2520the%2520first%2520comprehensive%2520benchmark%250Aspecifically%2520designed%2520to%2520assess%2520MLLMs%2520across%2520the%2520full%2520spectrum%2520of%2520endoscopic%250Apractice%2520with%2520multi-dimensional%2520capacities.%2520EndoBench%2520encompasses%25204%2520distinct%250Aendoscopic%2520scenarios%252C%252012%2520specialized%2520clinical%2520tasks%2520with%252012%2520secondary%2520subtasks%252C%250Aand%25205%2520levels%2520of%2520visual%2520prompting%2520granularities%252C%2520resulting%2520in%25206%252C832%2520rigorously%250Avalidated%2520VQA%2520pairs%2520from%252021%2520diverse%2520datasets.%2520Our%2520multi-dimensional%2520evaluation%250Aframework%2520mirrors%2520the%2520clinical%2520workflow--spanning%2520anatomical%2520recognition%252C%250Alesion%2520analysis%252C%2520spatial%2520localization%252C%2520and%2520surgical%2520operations--to%2520holistically%250Agauge%2520the%2520perceptual%2520and%2520diagnostic%2520abilities%2520of%2520MLLMs%2520in%2520realistic%2520scenarios.%250AWe%2520benchmark%252023%2520state-of-the-art%2520models%252C%2520including%2520general-purpose%252C%250Amedical-specialized%252C%2520and%2520proprietary%2520MLLMs%252C%2520and%2520establish%2520human%2520clinician%250Aperformance%2520as%2520a%2520reference%2520standard.%2520Our%2520extensive%2520experiments%2520reveal%253A%2520%25281%2529%250Aproprietary%2520MLLMs%2520outperform%2520open-source%2520and%2520medical-specialized%2520models%250Aoverall%252C%2520but%2520still%2520trail%2520human%2520experts%253B%2520%25282%2529%2520medical-domain%2520supervised%250Afine-tuning%2520substantially%2520boosts%2520task-specific%2520accuracy%253B%2520and%2520%25283%2529%2520model%250Aperformance%2520remains%2520sensitive%2520to%2520prompt%2520format%2520and%2520clinical%2520task%2520complexity.%250AEndoBench%2520establishes%2520a%2520new%2520standard%2520for%2520evaluating%2520and%2520advancing%2520MLLMs%2520in%250Aendoscopy%252C%2520highlighting%2520both%2520progress%2520and%2520persistent%2520gaps%2520between%2520current%250Amodels%2520and%2520expert%2520clinical%2520reasoning.%2520We%2520publicly%2520release%2520our%2520benchmark%2520and%250Acode.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Evaluation%20of%20Multi-Modal%20Large%20Language%20Models%20for%0A%20%20Endoscopy%20Analysis&entry.906535625=Shengyuan%20Liu%20and%20Boyun%20Zheng%20and%20Wenting%20Chen%20and%20Zhihao%20Peng%20and%20Zhenfei%20Yin%20and%20Jing%20Shao%20and%20Jiancong%20Hu%20and%20Yixuan%20Yuan&entry.1292438233=%20%20Endoscopic%20procedures%20are%20essential%20for%20diagnosing%20and%20treating%20internal%0Adiseases%2C%20and%20multi-modal%20large%20language%20models%20%28MLLMs%29%20are%20increasingly%0Aapplied%20to%20assist%20in%20endoscopy%20analysis.%20However%2C%20current%20benchmarks%20are%0Alimited%2C%20as%20they%20typically%20cover%20specific%20endoscopic%20scenarios%20and%20a%20small%20set%0Aof%20clinical%20tasks%2C%20failing%20to%20capture%20the%20real-world%20diversity%20of%20endoscopic%0Ascenarios%20and%20the%20full%20range%20of%20skills%20needed%20in%20clinical%20workflows.%20To%20address%0Athese%20issues%2C%20we%20introduce%20EndoBench%2C%20the%20first%20comprehensive%20benchmark%0Aspecifically%20designed%20to%20assess%20MLLMs%20across%20the%20full%20spectrum%20of%20endoscopic%0Apractice%20with%20multi-dimensional%20capacities.%20EndoBench%20encompasses%204%20distinct%0Aendoscopic%20scenarios%2C%2012%20specialized%20clinical%20tasks%20with%2012%20secondary%20subtasks%2C%0Aand%205%20levels%20of%20visual%20prompting%20granularities%2C%20resulting%20in%206%2C832%20rigorously%0Avalidated%20VQA%20pairs%20from%2021%20diverse%20datasets.%20Our%20multi-dimensional%20evaluation%0Aframework%20mirrors%20the%20clinical%20workflow--spanning%20anatomical%20recognition%2C%0Alesion%20analysis%2C%20spatial%20localization%2C%20and%20surgical%20operations--to%20holistically%0Agauge%20the%20perceptual%20and%20diagnostic%20abilities%20of%20MLLMs%20in%20realistic%20scenarios.%0AWe%20benchmark%2023%20state-of-the-art%20models%2C%20including%20general-purpose%2C%0Amedical-specialized%2C%20and%20proprietary%20MLLMs%2C%20and%20establish%20human%20clinician%0Aperformance%20as%20a%20reference%20standard.%20Our%20extensive%20experiments%20reveal%3A%20%281%29%0Aproprietary%20MLLMs%20outperform%20open-source%20and%20medical-specialized%20models%0Aoverall%2C%20but%20still%20trail%20human%20experts%3B%20%282%29%20medical-domain%20supervised%0Afine-tuning%20substantially%20boosts%20task-specific%20accuracy%3B%20and%20%283%29%20model%0Aperformance%20remains%20sensitive%20to%20prompt%20format%20and%20clinical%20task%20complexity.%0AEndoBench%20establishes%20a%20new%20standard%20for%20evaluating%20and%20advancing%20MLLMs%20in%0Aendoscopy%2C%20highlighting%20both%20progress%20and%20persistent%20gaps%20between%20current%0Amodels%20and%20expert%20clinical%20reasoning.%20We%20publicly%20release%20our%20benchmark%20and%0Acode.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23601v1&entry.124074799=Read"},
{"title": "Color Image Set Recognition Based on Quaternionic Grassmannians", "author": "Xiang Xiang Wang and Tin-Yau Tam", "abstract": "  We propose a new method for recognizing color image sets using quaternionic\nGrassmannians, which use the power of quaternions to capture color information\nand represent each color image set as a point on the quaternionic Grassmannian.\nWe provide a direct formula to calculate the shortest distance between two\npoints on the quaternionic Grassmannian, and use this distance to build a new\nclassification framework. Experiments on the ETH-80 benchmark dataset show that\nour method achieves good recognition results. We also discuss some limitations\nin stability and suggest ways the method can be improved in the future.\n", "link": "http://arxiv.org/abs/2505.23629v1", "date": "2025-05-29", "relevancy": 2.3931, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4885}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4854}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Color%20Image%20Set%20Recognition%20Based%20on%20Quaternionic%20Grassmannians&body=Title%3A%20Color%20Image%20Set%20Recognition%20Based%20on%20Quaternionic%20Grassmannians%0AAuthor%3A%20Xiang%20Xiang%20Wang%20and%20Tin-Yau%20Tam%0AAbstract%3A%20%20%20We%20propose%20a%20new%20method%20for%20recognizing%20color%20image%20sets%20using%20quaternionic%0AGrassmannians%2C%20which%20use%20the%20power%20of%20quaternions%20to%20capture%20color%20information%0Aand%20represent%20each%20color%20image%20set%20as%20a%20point%20on%20the%20quaternionic%20Grassmannian.%0AWe%20provide%20a%20direct%20formula%20to%20calculate%20the%20shortest%20distance%20between%20two%0Apoints%20on%20the%20quaternionic%20Grassmannian%2C%20and%20use%20this%20distance%20to%20build%20a%20new%0Aclassification%20framework.%20Experiments%20on%20the%20ETH-80%20benchmark%20dataset%20show%20that%0Aour%20method%20achieves%20good%20recognition%20results.%20We%20also%20discuss%20some%20limitations%0Ain%20stability%20and%20suggest%20ways%20the%20method%20can%20be%20improved%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColor%2520Image%2520Set%2520Recognition%2520Based%2520on%2520Quaternionic%2520Grassmannians%26entry.906535625%3DXiang%2520Xiang%2520Wang%2520and%2520Tin-Yau%2520Tam%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520method%2520for%2520recognizing%2520color%2520image%2520sets%2520using%2520quaternionic%250AGrassmannians%252C%2520which%2520use%2520the%2520power%2520of%2520quaternions%2520to%2520capture%2520color%2520information%250Aand%2520represent%2520each%2520color%2520image%2520set%2520as%2520a%2520point%2520on%2520the%2520quaternionic%2520Grassmannian.%250AWe%2520provide%2520a%2520direct%2520formula%2520to%2520calculate%2520the%2520shortest%2520distance%2520between%2520two%250Apoints%2520on%2520the%2520quaternionic%2520Grassmannian%252C%2520and%2520use%2520this%2520distance%2520to%2520build%2520a%2520new%250Aclassification%2520framework.%2520Experiments%2520on%2520the%2520ETH-80%2520benchmark%2520dataset%2520show%2520that%250Aour%2520method%2520achieves%2520good%2520recognition%2520results.%2520We%2520also%2520discuss%2520some%2520limitations%250Ain%2520stability%2520and%2520suggest%2520ways%2520the%2520method%2520can%2520be%2520improved%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Color%20Image%20Set%20Recognition%20Based%20on%20Quaternionic%20Grassmannians&entry.906535625=Xiang%20Xiang%20Wang%20and%20Tin-Yau%20Tam&entry.1292438233=%20%20We%20propose%20a%20new%20method%20for%20recognizing%20color%20image%20sets%20using%20quaternionic%0AGrassmannians%2C%20which%20use%20the%20power%20of%20quaternions%20to%20capture%20color%20information%0Aand%20represent%20each%20color%20image%20set%20as%20a%20point%20on%20the%20quaternionic%20Grassmannian.%0AWe%20provide%20a%20direct%20formula%20to%20calculate%20the%20shortest%20distance%20between%20two%0Apoints%20on%20the%20quaternionic%20Grassmannian%2C%20and%20use%20this%20distance%20to%20build%20a%20new%0Aclassification%20framework.%20Experiments%20on%20the%20ETH-80%20benchmark%20dataset%20show%20that%0Aour%20method%20achieves%20good%20recognition%20results.%20We%20also%20discuss%20some%20limitations%0Ain%20stability%20and%20suggest%20ways%20the%20method%20can%20be%20improved%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23629v1&entry.124074799=Read"},
{"title": "Position: Interactive Generative Video as Next-Generation Game Engine", "author": "Jiwen Yu and Yiran Qin and Haoxuan Che and Quande Liu and Xintao Wang and Pengfei Wan and Di Zhang and Xihui Liu", "abstract": "  Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.\n", "link": "http://arxiv.org/abs/2503.17359v2", "date": "2025-05-29", "relevancy": 2.3797, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6215}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5919}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Interactive%20Generative%20Video%20as%20Next-Generation%20Game%20Engine&body=Title%3A%20Position%3A%20Interactive%20Generative%20Video%20as%20Next-Generation%20Game%20Engine%0AAuthor%3A%20Jiwen%20Yu%20and%20Yiran%20Qin%20and%20Haoxuan%20Che%20and%20Quande%20Liu%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Modern%20game%20development%20faces%20significant%20challenges%20in%20creativity%20and%20cost%0Adue%20to%20predetermined%20content%20in%20traditional%20game%20engines.%20Recent%20breakthroughs%0Ain%20video%20generation%20models%2C%20capable%20of%20synthesizing%20realistic%20and%20interactive%0Avirtual%20environments%2C%20present%20an%20opportunity%20to%20revolutionize%20game%20creation.%20In%0Athis%20position%20paper%2C%20we%20propose%20Interactive%20Generative%20Video%20%28IGV%29%20as%20the%0Afoundation%20for%20Generative%20Game%20Engines%20%28GGE%29%2C%20enabling%20unlimited%20novel%20content%0Ageneration%20in%20next-generation%20gaming.%20GGE%20leverages%20IGV%27s%20unique%20strengths%20in%0Aunlimited%20high-quality%20content%20synthesis%2C%20physics-aware%20world%20modeling%2C%0Auser-controlled%20interactivity%2C%20long-term%20memory%20capabilities%2C%20and%20causal%0Areasoning.%20We%20present%20a%20comprehensive%20framework%20detailing%20GGE%27s%20core%20modules%0Aand%20a%20hierarchical%20maturity%20roadmap%20%28L0-L4%29%20to%20guide%20its%20evolution.%20Our%20work%0Acharts%20a%20new%20course%20for%20game%20development%20in%20the%20AI%20era%2C%20envisioning%20a%20future%0Awhere%20AI-powered%20generative%20systems%20fundamentally%20reshape%20how%20games%20are%20created%0Aand%20experienced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17359v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Interactive%2520Generative%2520Video%2520as%2520Next-Generation%2520Game%2520Engine%26entry.906535625%3DJiwen%2520Yu%2520and%2520Yiran%2520Qin%2520and%2520Haoxuan%2520Che%2520and%2520Quande%2520Liu%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Modern%2520game%2520development%2520faces%2520significant%2520challenges%2520in%2520creativity%2520and%2520cost%250Adue%2520to%2520predetermined%2520content%2520in%2520traditional%2520game%2520engines.%2520Recent%2520breakthroughs%250Ain%2520video%2520generation%2520models%252C%2520capable%2520of%2520synthesizing%2520realistic%2520and%2520interactive%250Avirtual%2520environments%252C%2520present%2520an%2520opportunity%2520to%2520revolutionize%2520game%2520creation.%2520In%250Athis%2520position%2520paper%252C%2520we%2520propose%2520Interactive%2520Generative%2520Video%2520%2528IGV%2529%2520as%2520the%250Afoundation%2520for%2520Generative%2520Game%2520Engines%2520%2528GGE%2529%252C%2520enabling%2520unlimited%2520novel%2520content%250Ageneration%2520in%2520next-generation%2520gaming.%2520GGE%2520leverages%2520IGV%2527s%2520unique%2520strengths%2520in%250Aunlimited%2520high-quality%2520content%2520synthesis%252C%2520physics-aware%2520world%2520modeling%252C%250Auser-controlled%2520interactivity%252C%2520long-term%2520memory%2520capabilities%252C%2520and%2520causal%250Areasoning.%2520We%2520present%2520a%2520comprehensive%2520framework%2520detailing%2520GGE%2527s%2520core%2520modules%250Aand%2520a%2520hierarchical%2520maturity%2520roadmap%2520%2528L0-L4%2529%2520to%2520guide%2520its%2520evolution.%2520Our%2520work%250Acharts%2520a%2520new%2520course%2520for%2520game%2520development%2520in%2520the%2520AI%2520era%252C%2520envisioning%2520a%2520future%250Awhere%2520AI-powered%2520generative%2520systems%2520fundamentally%2520reshape%2520how%2520games%2520are%2520created%250Aand%2520experienced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17359v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Interactive%20Generative%20Video%20as%20Next-Generation%20Game%20Engine&entry.906535625=Jiwen%20Yu%20and%20Yiran%20Qin%20and%20Haoxuan%20Che%20and%20Quande%20Liu%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Xihui%20Liu&entry.1292438233=%20%20Modern%20game%20development%20faces%20significant%20challenges%20in%20creativity%20and%20cost%0Adue%20to%20predetermined%20content%20in%20traditional%20game%20engines.%20Recent%20breakthroughs%0Ain%20video%20generation%20models%2C%20capable%20of%20synthesizing%20realistic%20and%20interactive%0Avirtual%20environments%2C%20present%20an%20opportunity%20to%20revolutionize%20game%20creation.%20In%0Athis%20position%20paper%2C%20we%20propose%20Interactive%20Generative%20Video%20%28IGV%29%20as%20the%0Afoundation%20for%20Generative%20Game%20Engines%20%28GGE%29%2C%20enabling%20unlimited%20novel%20content%0Ageneration%20in%20next-generation%20gaming.%20GGE%20leverages%20IGV%27s%20unique%20strengths%20in%0Aunlimited%20high-quality%20content%20synthesis%2C%20physics-aware%20world%20modeling%2C%0Auser-controlled%20interactivity%2C%20long-term%20memory%20capabilities%2C%20and%20causal%0Areasoning.%20We%20present%20a%20comprehensive%20framework%20detailing%20GGE%27s%20core%20modules%0Aand%20a%20hierarchical%20maturity%20roadmap%20%28L0-L4%29%20to%20guide%20its%20evolution.%20Our%20work%0Acharts%20a%20new%20course%20for%20game%20development%20in%20the%20AI%20era%2C%20envisioning%20a%20future%0Awhere%20AI-powered%20generative%20systems%20fundamentally%20reshape%20how%20games%20are%20created%0Aand%20experienced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17359v2&entry.124074799=Read"},
{"title": "UniViTAR: Unified Vision Transformer with Native Resolution", "author": "Limeng Qiao and Yiyang Gan and Bairui Wang and Jie Qin and Shuang Xu and Siqi Yang and Lin Ma", "abstract": "  Conventional Vision Transformer simplifies visual modeling by standardizing\ninput resolutions, often disregarding the variability of natural visual data\nand compromising spatial-contextual fidelity. While preliminary explorations\nhave superficially investigated native resolution modeling, existing approaches\nstill lack systematic analysis from a visual representation perspective. To\nbridge this gap, we introduce UniViTAR, a family of homogeneous vision\nfoundation models tailored for unified visual modality and native resolution\nscenario in the era of multimodal. Our framework first conducts architectural\nupgrades to the vanilla paradigm by integrating multiple advanced components.\nBuilding upon these improvements, a progressive training paradigm is\nintroduced, which strategically combines two core mechanisms: (1) resolution\ncurriculum learning, transitioning from fixed-resolution pretraining to native\nresolution tuning, thereby leveraging ViT's inherent adaptability to\nvariable-length sequences, and (2) visual modality adaptation via inter-batch\nimage-video switching, which balances computational efficiency with enhanced\ntemporal reasoning. In parallel, a hybrid training framework further synergizes\nsigmoid-based contrastive loss with feature distillation from a frozen teacher\nmodel, thereby accelerating early-stage convergence. Finally, trained\nexclusively on public datasets, externsive experiments across multiple model\nscales from 0.3B to 1B demonstrate its effectiveness.\n", "link": "http://arxiv.org/abs/2504.01792v2", "date": "2025-05-29", "relevancy": 2.3608, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5905}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5905}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniViTAR%3A%20Unified%20Vision%20Transformer%20with%20Native%20Resolution&body=Title%3A%20UniViTAR%3A%20Unified%20Vision%20Transformer%20with%20Native%20Resolution%0AAuthor%3A%20Limeng%20Qiao%20and%20Yiyang%20Gan%20and%20Bairui%20Wang%20and%20Jie%20Qin%20and%20Shuang%20Xu%20and%20Siqi%20Yang%20and%20Lin%20Ma%0AAbstract%3A%20%20%20Conventional%20Vision%20Transformer%20simplifies%20visual%20modeling%20by%20standardizing%0Ainput%20resolutions%2C%20often%20disregarding%20the%20variability%20of%20natural%20visual%20data%0Aand%20compromising%20spatial-contextual%20fidelity.%20While%20preliminary%20explorations%0Ahave%20superficially%20investigated%20native%20resolution%20modeling%2C%20existing%20approaches%0Astill%20lack%20systematic%20analysis%20from%20a%20visual%20representation%20perspective.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20UniViTAR%2C%20a%20family%20of%20homogeneous%20vision%0Afoundation%20models%20tailored%20for%20unified%20visual%20modality%20and%20native%20resolution%0Ascenario%20in%20the%20era%20of%20multimodal.%20Our%20framework%20first%20conducts%20architectural%0Aupgrades%20to%20the%20vanilla%20paradigm%20by%20integrating%20multiple%20advanced%20components.%0ABuilding%20upon%20these%20improvements%2C%20a%20progressive%20training%20paradigm%20is%0Aintroduced%2C%20which%20strategically%20combines%20two%20core%20mechanisms%3A%20%281%29%20resolution%0Acurriculum%20learning%2C%20transitioning%20from%20fixed-resolution%20pretraining%20to%20native%0Aresolution%20tuning%2C%20thereby%20leveraging%20ViT%27s%20inherent%20adaptability%20to%0Avariable-length%20sequences%2C%20and%20%282%29%20visual%20modality%20adaptation%20via%20inter-batch%0Aimage-video%20switching%2C%20which%20balances%20computational%20efficiency%20with%20enhanced%0Atemporal%20reasoning.%20In%20parallel%2C%20a%20hybrid%20training%20framework%20further%20synergizes%0Asigmoid-based%20contrastive%20loss%20with%20feature%20distillation%20from%20a%20frozen%20teacher%0Amodel%2C%20thereby%20accelerating%20early-stage%20convergence.%20Finally%2C%20trained%0Aexclusively%20on%20public%20datasets%2C%20externsive%20experiments%20across%20multiple%20model%0Ascales%20from%200.3B%20to%201B%20demonstrate%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniViTAR%253A%2520Unified%2520Vision%2520Transformer%2520with%2520Native%2520Resolution%26entry.906535625%3DLimeng%2520Qiao%2520and%2520Yiyang%2520Gan%2520and%2520Bairui%2520Wang%2520and%2520Jie%2520Qin%2520and%2520Shuang%2520Xu%2520and%2520Siqi%2520Yang%2520and%2520Lin%2520Ma%26entry.1292438233%3D%2520%2520Conventional%2520Vision%2520Transformer%2520simplifies%2520visual%2520modeling%2520by%2520standardizing%250Ainput%2520resolutions%252C%2520often%2520disregarding%2520the%2520variability%2520of%2520natural%2520visual%2520data%250Aand%2520compromising%2520spatial-contextual%2520fidelity.%2520While%2520preliminary%2520explorations%250Ahave%2520superficially%2520investigated%2520native%2520resolution%2520modeling%252C%2520existing%2520approaches%250Astill%2520lack%2520systematic%2520analysis%2520from%2520a%2520visual%2520representation%2520perspective.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520introduce%2520UniViTAR%252C%2520a%2520family%2520of%2520homogeneous%2520vision%250Afoundation%2520models%2520tailored%2520for%2520unified%2520visual%2520modality%2520and%2520native%2520resolution%250Ascenario%2520in%2520the%2520era%2520of%2520multimodal.%2520Our%2520framework%2520first%2520conducts%2520architectural%250Aupgrades%2520to%2520the%2520vanilla%2520paradigm%2520by%2520integrating%2520multiple%2520advanced%2520components.%250ABuilding%2520upon%2520these%2520improvements%252C%2520a%2520progressive%2520training%2520paradigm%2520is%250Aintroduced%252C%2520which%2520strategically%2520combines%2520two%2520core%2520mechanisms%253A%2520%25281%2529%2520resolution%250Acurriculum%2520learning%252C%2520transitioning%2520from%2520fixed-resolution%2520pretraining%2520to%2520native%250Aresolution%2520tuning%252C%2520thereby%2520leveraging%2520ViT%2527s%2520inherent%2520adaptability%2520to%250Avariable-length%2520sequences%252C%2520and%2520%25282%2529%2520visual%2520modality%2520adaptation%2520via%2520inter-batch%250Aimage-video%2520switching%252C%2520which%2520balances%2520computational%2520efficiency%2520with%2520enhanced%250Atemporal%2520reasoning.%2520In%2520parallel%252C%2520a%2520hybrid%2520training%2520framework%2520further%2520synergizes%250Asigmoid-based%2520contrastive%2520loss%2520with%2520feature%2520distillation%2520from%2520a%2520frozen%2520teacher%250Amodel%252C%2520thereby%2520accelerating%2520early-stage%2520convergence.%2520Finally%252C%2520trained%250Aexclusively%2520on%2520public%2520datasets%252C%2520externsive%2520experiments%2520across%2520multiple%2520model%250Ascales%2520from%25200.3B%2520to%25201B%2520demonstrate%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniViTAR%3A%20Unified%20Vision%20Transformer%20with%20Native%20Resolution&entry.906535625=Limeng%20Qiao%20and%20Yiyang%20Gan%20and%20Bairui%20Wang%20and%20Jie%20Qin%20and%20Shuang%20Xu%20and%20Siqi%20Yang%20and%20Lin%20Ma&entry.1292438233=%20%20Conventional%20Vision%20Transformer%20simplifies%20visual%20modeling%20by%20standardizing%0Ainput%20resolutions%2C%20often%20disregarding%20the%20variability%20of%20natural%20visual%20data%0Aand%20compromising%20spatial-contextual%20fidelity.%20While%20preliminary%20explorations%0Ahave%20superficially%20investigated%20native%20resolution%20modeling%2C%20existing%20approaches%0Astill%20lack%20systematic%20analysis%20from%20a%20visual%20representation%20perspective.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20UniViTAR%2C%20a%20family%20of%20homogeneous%20vision%0Afoundation%20models%20tailored%20for%20unified%20visual%20modality%20and%20native%20resolution%0Ascenario%20in%20the%20era%20of%20multimodal.%20Our%20framework%20first%20conducts%20architectural%0Aupgrades%20to%20the%20vanilla%20paradigm%20by%20integrating%20multiple%20advanced%20components.%0ABuilding%20upon%20these%20improvements%2C%20a%20progressive%20training%20paradigm%20is%0Aintroduced%2C%20which%20strategically%20combines%20two%20core%20mechanisms%3A%20%281%29%20resolution%0Acurriculum%20learning%2C%20transitioning%20from%20fixed-resolution%20pretraining%20to%20native%0Aresolution%20tuning%2C%20thereby%20leveraging%20ViT%27s%20inherent%20adaptability%20to%0Avariable-length%20sequences%2C%20and%20%282%29%20visual%20modality%20adaptation%20via%20inter-batch%0Aimage-video%20switching%2C%20which%20balances%20computational%20efficiency%20with%20enhanced%0Atemporal%20reasoning.%20In%20parallel%2C%20a%20hybrid%20training%20framework%20further%20synergizes%0Asigmoid-based%20contrastive%20loss%20with%20feature%20distillation%20from%20a%20frozen%20teacher%0Amodel%2C%20thereby%20accelerating%20early-stage%20convergence.%20Finally%2C%20trained%0Aexclusively%20on%20public%20datasets%2C%20externsive%20experiments%20across%20multiple%20model%0Ascales%20from%200.3B%20to%201B%20demonstrate%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01792v2&entry.124074799=Read"},
{"title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning", "author": "Song Wang and Gongfan Fang and Lingdong Kong and Xiangtai Li and Jianyun Xu and Sheng Yang and Qiang Li and Jianke Zhu and Xinchao Wang", "abstract": "  Existing reasoning segmentation approaches typically fine-tune multimodal\nlarge language models (MLLMs) using image-text pairs and corresponding mask\nlabels. However, they exhibit limited generalization to out-of-distribution\nscenarios without an explicit reasoning process. Although recent efforts\nleverage reinforcement learning through group-relative policy optimization\n(GRPO) to enhance reasoning ability, they often suffer from overthinking -\nproducing uniformly verbose reasoning chains irrespective of task complexity.\nThis results in elevated computational costs and limited control over reasoning\nquality. To address this problem, we propose PixelThink, a simple yet effective\nscheme that integrates externally estimated task difficulty and internally\nmeasured model uncertainty to regulate reasoning generation within a\nreinforcement learning paradigm. The model learns to compress reasoning length\nin accordance with scene complexity and predictive confidence. To support\ncomprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark\nwith annotated reasoning references and difficulty scores, along with a suite\nof metrics designed to assess segmentation accuracy, reasoning quality, and\nefficiency jointly. Experimental results demonstrate that the proposed approach\nimproves both reasoning efficiency and overall segmentation performance. Our\nwork contributes novel perspectives towards efficient and interpretable\nmultimodal understanding. The code and model will be publicly available.\n", "link": "http://arxiv.org/abs/2505.23727v1", "date": "2025-05-29", "relevancy": 2.3553, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PixelThink%3A%20Towards%20Efficient%20Chain-of-Pixel%20Reasoning&body=Title%3A%20PixelThink%3A%20Towards%20Efficient%20Chain-of-Pixel%20Reasoning%0AAuthor%3A%20Song%20Wang%20and%20Gongfan%20Fang%20and%20Lingdong%20Kong%20and%20Xiangtai%20Li%20and%20Jianyun%20Xu%20and%20Sheng%20Yang%20and%20Qiang%20Li%20and%20Jianke%20Zhu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Existing%20reasoning%20segmentation%20approaches%20typically%20fine-tune%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20using%20image-text%20pairs%20and%20corresponding%20mask%0Alabels.%20However%2C%20they%20exhibit%20limited%20generalization%20to%20out-of-distribution%0Ascenarios%20without%20an%20explicit%20reasoning%20process.%20Although%20recent%20efforts%0Aleverage%20reinforcement%20learning%20through%20group-relative%20policy%20optimization%0A%28GRPO%29%20to%20enhance%20reasoning%20ability%2C%20they%20often%20suffer%20from%20overthinking%20-%0Aproducing%20uniformly%20verbose%20reasoning%20chains%20irrespective%20of%20task%20complexity.%0AThis%20results%20in%20elevated%20computational%20costs%20and%20limited%20control%20over%20reasoning%0Aquality.%20To%20address%20this%20problem%2C%20we%20propose%20PixelThink%2C%20a%20simple%20yet%20effective%0Ascheme%20that%20integrates%20externally%20estimated%20task%20difficulty%20and%20internally%0Ameasured%20model%20uncertainty%20to%20regulate%20reasoning%20generation%20within%20a%0Areinforcement%20learning%20paradigm.%20The%20model%20learns%20to%20compress%20reasoning%20length%0Ain%20accordance%20with%20scene%20complexity%20and%20predictive%20confidence.%20To%20support%0Acomprehensive%20evaluation%2C%20we%20introduce%20ReasonSeg-Diff%2C%20an%20extended%20benchmark%0Awith%20annotated%20reasoning%20references%20and%20difficulty%20scores%2C%20along%20with%20a%20suite%0Aof%20metrics%20designed%20to%20assess%20segmentation%20accuracy%2C%20reasoning%20quality%2C%20and%0Aefficiency%20jointly.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20approach%0Aimproves%20both%20reasoning%20efficiency%20and%20overall%20segmentation%20performance.%20Our%0Awork%20contributes%20novel%20perspectives%20towards%20efficient%20and%20interpretable%0Amultimodal%20understanding.%20The%20code%20and%20model%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixelThink%253A%2520Towards%2520Efficient%2520Chain-of-Pixel%2520Reasoning%26entry.906535625%3DSong%2520Wang%2520and%2520Gongfan%2520Fang%2520and%2520Lingdong%2520Kong%2520and%2520Xiangtai%2520Li%2520and%2520Jianyun%2520Xu%2520and%2520Sheng%2520Yang%2520and%2520Qiang%2520Li%2520and%2520Jianke%2520Zhu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520reasoning%2520segmentation%2520approaches%2520typically%2520fine-tune%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520using%2520image-text%2520pairs%2520and%2520corresponding%2520mask%250Alabels.%2520However%252C%2520they%2520exhibit%2520limited%2520generalization%2520to%2520out-of-distribution%250Ascenarios%2520without%2520an%2520explicit%2520reasoning%2520process.%2520Although%2520recent%2520efforts%250Aleverage%2520reinforcement%2520learning%2520through%2520group-relative%2520policy%2520optimization%250A%2528GRPO%2529%2520to%2520enhance%2520reasoning%2520ability%252C%2520they%2520often%2520suffer%2520from%2520overthinking%2520-%250Aproducing%2520uniformly%2520verbose%2520reasoning%2520chains%2520irrespective%2520of%2520task%2520complexity.%250AThis%2520results%2520in%2520elevated%2520computational%2520costs%2520and%2520limited%2520control%2520over%2520reasoning%250Aquality.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520PixelThink%252C%2520a%2520simple%2520yet%2520effective%250Ascheme%2520that%2520integrates%2520externally%2520estimated%2520task%2520difficulty%2520and%2520internally%250Ameasured%2520model%2520uncertainty%2520to%2520regulate%2520reasoning%2520generation%2520within%2520a%250Areinforcement%2520learning%2520paradigm.%2520The%2520model%2520learns%2520to%2520compress%2520reasoning%2520length%250Ain%2520accordance%2520with%2520scene%2520complexity%2520and%2520predictive%2520confidence.%2520To%2520support%250Acomprehensive%2520evaluation%252C%2520we%2520introduce%2520ReasonSeg-Diff%252C%2520an%2520extended%2520benchmark%250Awith%2520annotated%2520reasoning%2520references%2520and%2520difficulty%2520scores%252C%2520along%2520with%2520a%2520suite%250Aof%2520metrics%2520designed%2520to%2520assess%2520segmentation%2520accuracy%252C%2520reasoning%2520quality%252C%2520and%250Aefficiency%2520jointly.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520approach%250Aimproves%2520both%2520reasoning%2520efficiency%2520and%2520overall%2520segmentation%2520performance.%2520Our%250Awork%2520contributes%2520novel%2520perspectives%2520towards%2520efficient%2520and%2520interpretable%250Amultimodal%2520understanding.%2520The%2520code%2520and%2520model%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixelThink%3A%20Towards%20Efficient%20Chain-of-Pixel%20Reasoning&entry.906535625=Song%20Wang%20and%20Gongfan%20Fang%20and%20Lingdong%20Kong%20and%20Xiangtai%20Li%20and%20Jianyun%20Xu%20and%20Sheng%20Yang%20and%20Qiang%20Li%20and%20Jianke%20Zhu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Existing%20reasoning%20segmentation%20approaches%20typically%20fine-tune%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20using%20image-text%20pairs%20and%20corresponding%20mask%0Alabels.%20However%2C%20they%20exhibit%20limited%20generalization%20to%20out-of-distribution%0Ascenarios%20without%20an%20explicit%20reasoning%20process.%20Although%20recent%20efforts%0Aleverage%20reinforcement%20learning%20through%20group-relative%20policy%20optimization%0A%28GRPO%29%20to%20enhance%20reasoning%20ability%2C%20they%20often%20suffer%20from%20overthinking%20-%0Aproducing%20uniformly%20verbose%20reasoning%20chains%20irrespective%20of%20task%20complexity.%0AThis%20results%20in%20elevated%20computational%20costs%20and%20limited%20control%20over%20reasoning%0Aquality.%20To%20address%20this%20problem%2C%20we%20propose%20PixelThink%2C%20a%20simple%20yet%20effective%0Ascheme%20that%20integrates%20externally%20estimated%20task%20difficulty%20and%20internally%0Ameasured%20model%20uncertainty%20to%20regulate%20reasoning%20generation%20within%20a%0Areinforcement%20learning%20paradigm.%20The%20model%20learns%20to%20compress%20reasoning%20length%0Ain%20accordance%20with%20scene%20complexity%20and%20predictive%20confidence.%20To%20support%0Acomprehensive%20evaluation%2C%20we%20introduce%20ReasonSeg-Diff%2C%20an%20extended%20benchmark%0Awith%20annotated%20reasoning%20references%20and%20difficulty%20scores%2C%20along%20with%20a%20suite%0Aof%20metrics%20designed%20to%20assess%20segmentation%20accuracy%2C%20reasoning%20quality%2C%20and%0Aefficiency%20jointly.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20approach%0Aimproves%20both%20reasoning%20efficiency%20and%20overall%20segmentation%20performance.%20Our%0Awork%20contributes%20novel%20perspectives%20towards%20efficient%20and%20interpretable%0Amultimodal%20understanding.%20The%20code%20and%20model%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23727v1&entry.124074799=Read"},
{"title": "On the Convergence Analysis of Muon", "author": "Wei Shen and Ruichuan Huang and Minhui Huang and Cong Shen and Jiawei Zhang", "abstract": "  The majority of parameters in neural networks are naturally represented as\nmatrices. However, most commonly used optimizers treat these matrix parameters\nas flattened vectors during optimization, potentially overlooking their\ninherent structural properties. Recently, an optimizer called Muon has been\nproposed, specifically designed to optimize matrix-structured parameters.\nExtensive empirical evidence shows that Muon can significantly outperform\ntraditional optimizers when training neural networks. Nonetheless, the\ntheoretical understanding of Muon's convergence behavior and the reasons behind\nits superior performance remain limited. In this work, we present a\ncomprehensive convergence rate analysis of Muon and its comparison with\nGradient Descent (GD). We further characterize the conditions under which Muon\ncan outperform GD. Our theoretical results reveal that Muon can benefit from\nthe low-rank and approximate blockwise diagonal structure of Hessian matrices\n-- phenomena widely observed in practical neural network training. Our\nexperimental results support and corroborate the theoretical findings.\n", "link": "http://arxiv.org/abs/2505.23737v1", "date": "2025-05-29", "relevancy": 2.3535, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5046}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4618}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20Analysis%20of%20Muon&body=Title%3A%20On%20the%20Convergence%20Analysis%20of%20Muon%0AAuthor%3A%20Wei%20Shen%20and%20Ruichuan%20Huang%20and%20Minhui%20Huang%20and%20Cong%20Shen%20and%20Jiawei%20Zhang%0AAbstract%3A%20%20%20The%20majority%20of%20parameters%20in%20neural%20networks%20are%20naturally%20represented%20as%0Amatrices.%20However%2C%20most%20commonly%20used%20optimizers%20treat%20these%20matrix%20parameters%0Aas%20flattened%20vectors%20during%20optimization%2C%20potentially%20overlooking%20their%0Ainherent%20structural%20properties.%20Recently%2C%20an%20optimizer%20called%20Muon%20has%20been%0Aproposed%2C%20specifically%20designed%20to%20optimize%20matrix-structured%20parameters.%0AExtensive%20empirical%20evidence%20shows%20that%20Muon%20can%20significantly%20outperform%0Atraditional%20optimizers%20when%20training%20neural%20networks.%20Nonetheless%2C%20the%0Atheoretical%20understanding%20of%20Muon%27s%20convergence%20behavior%20and%20the%20reasons%20behind%0Aits%20superior%20performance%20remain%20limited.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20convergence%20rate%20analysis%20of%20Muon%20and%20its%20comparison%20with%0AGradient%20Descent%20%28GD%29.%20We%20further%20characterize%20the%20conditions%20under%20which%20Muon%0Acan%20outperform%20GD.%20Our%20theoretical%20results%20reveal%20that%20Muon%20can%20benefit%20from%0Athe%20low-rank%20and%20approximate%20blockwise%20diagonal%20structure%20of%20Hessian%20matrices%0A--%20phenomena%20widely%20observed%20in%20practical%20neural%20network%20training.%20Our%0Aexperimental%20results%20support%20and%20corroborate%20the%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Convergence%2520Analysis%2520of%2520Muon%26entry.906535625%3DWei%2520Shen%2520and%2520Ruichuan%2520Huang%2520and%2520Minhui%2520Huang%2520and%2520Cong%2520Shen%2520and%2520Jiawei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520majority%2520of%2520parameters%2520in%2520neural%2520networks%2520are%2520naturally%2520represented%2520as%250Amatrices.%2520However%252C%2520most%2520commonly%2520used%2520optimizers%2520treat%2520these%2520matrix%2520parameters%250Aas%2520flattened%2520vectors%2520during%2520optimization%252C%2520potentially%2520overlooking%2520their%250Ainherent%2520structural%2520properties.%2520Recently%252C%2520an%2520optimizer%2520called%2520Muon%2520has%2520been%250Aproposed%252C%2520specifically%2520designed%2520to%2520optimize%2520matrix-structured%2520parameters.%250AExtensive%2520empirical%2520evidence%2520shows%2520that%2520Muon%2520can%2520significantly%2520outperform%250Atraditional%2520optimizers%2520when%2520training%2520neural%2520networks.%2520Nonetheless%252C%2520the%250Atheoretical%2520understanding%2520of%2520Muon%2527s%2520convergence%2520behavior%2520and%2520the%2520reasons%2520behind%250Aits%2520superior%2520performance%2520remain%2520limited.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Acomprehensive%2520convergence%2520rate%2520analysis%2520of%2520Muon%2520and%2520its%2520comparison%2520with%250AGradient%2520Descent%2520%2528GD%2529.%2520We%2520further%2520characterize%2520the%2520conditions%2520under%2520which%2520Muon%250Acan%2520outperform%2520GD.%2520Our%2520theoretical%2520results%2520reveal%2520that%2520Muon%2520can%2520benefit%2520from%250Athe%2520low-rank%2520and%2520approximate%2520blockwise%2520diagonal%2520structure%2520of%2520Hessian%2520matrices%250A--%2520phenomena%2520widely%2520observed%2520in%2520practical%2520neural%2520network%2520training.%2520Our%250Aexperimental%2520results%2520support%2520and%2520corroborate%2520the%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20Analysis%20of%20Muon&entry.906535625=Wei%20Shen%20and%20Ruichuan%20Huang%20and%20Minhui%20Huang%20and%20Cong%20Shen%20and%20Jiawei%20Zhang&entry.1292438233=%20%20The%20majority%20of%20parameters%20in%20neural%20networks%20are%20naturally%20represented%20as%0Amatrices.%20However%2C%20most%20commonly%20used%20optimizers%20treat%20these%20matrix%20parameters%0Aas%20flattened%20vectors%20during%20optimization%2C%20potentially%20overlooking%20their%0Ainherent%20structural%20properties.%20Recently%2C%20an%20optimizer%20called%20Muon%20has%20been%0Aproposed%2C%20specifically%20designed%20to%20optimize%20matrix-structured%20parameters.%0AExtensive%20empirical%20evidence%20shows%20that%20Muon%20can%20significantly%20outperform%0Atraditional%20optimizers%20when%20training%20neural%20networks.%20Nonetheless%2C%20the%0Atheoretical%20understanding%20of%20Muon%27s%20convergence%20behavior%20and%20the%20reasons%20behind%0Aits%20superior%20performance%20remain%20limited.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20convergence%20rate%20analysis%20of%20Muon%20and%20its%20comparison%20with%0AGradient%20Descent%20%28GD%29.%20We%20further%20characterize%20the%20conditions%20under%20which%20Muon%0Acan%20outperform%20GD.%20Our%20theoretical%20results%20reveal%20that%20Muon%20can%20benefit%20from%0Athe%20low-rank%20and%20approximate%20blockwise%20diagonal%20structure%20of%20Hessian%20matrices%0A--%20phenomena%20widely%20observed%20in%20practical%20neural%20network%20training.%20Our%0Aexperimental%20results%20support%20and%20corroborate%20the%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23737v1&entry.124074799=Read"},
{"title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction", "author": "Jang-Hyun Kim and Jinuk Kim and Sangwoo Kwon and Jae W. Lee and Sangdoo Yun and Hyun Oh Song", "abstract": "  Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.\n", "link": "http://arxiv.org/abs/2505.23416v1", "date": "2025-05-29", "relevancy": 2.3483, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.486}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KVzip%3A%20Query-Agnostic%20KV%20Cache%20Compression%20with%20Context%20Reconstruction&body=Title%3A%20KVzip%3A%20Query-Agnostic%20KV%20Cache%20Compression%20with%20Context%20Reconstruction%0AAuthor%3A%20Jang-Hyun%20Kim%20and%20Jinuk%20Kim%20and%20Sangwoo%20Kwon%20and%20Jae%20W.%20Lee%20and%20Sangdoo%20Yun%20and%20Hyun%20Oh%20Song%0AAbstract%3A%20%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20cache%20context%20as%20key-value%0A%28KV%29%20pairs%20during%20inference.%20As%20context%20length%20grows%2C%20KV%20cache%20sizes%20expand%2C%0Aleading%20to%20substantial%20memory%20overhead%20and%20increased%20attention%20latency.%20This%0Apaper%20introduces%20KVzip%2C%20a%20query-agnostic%20KV%20cache%20eviction%20method%20enabling%0Aeffective%20reuse%20of%20compressed%20KV%20caches%20across%20diverse%20queries.%20KVzip%0Aquantifies%20the%20importance%20of%20a%20KV%20pair%20using%20the%20underlying%20LLM%20to%20reconstruct%0Aoriginal%20contexts%20from%20cached%20KV%20pairs%2C%20subsequently%20evicting%20pairs%20with%20lower%0Aimportance.%20Extensive%20empirical%20evaluations%20demonstrate%20that%20KVzip%20reduces%20KV%0Acache%20size%20by%203-4%24%5Ctimes%24%20and%20FlashAttention%20decoding%20latency%20by%20approximately%0A2%24%5Ctimes%24%2C%20with%20negligible%20performance%20loss%20in%20question-answering%2C%20retrieval%2C%0Areasoning%2C%20and%20code%20comprehension%20tasks.%20Evaluations%20include%20various%20models%0Asuch%20as%20LLaMA3.1-8B%2C%20Qwen2.5-14B%2C%20and%20Gemma3-12B%2C%20with%20context%20lengths%20reaching%0Aup%20to%20170K%20tokens.%20KVzip%20significantly%20outperforms%20existing%20query-aware%20KV%0Aeviction%20methods%2C%20which%20suffer%20from%20performance%20degradation%20even%20at%20a%2090%25%20cache%0Abudget%20ratio%20under%20multi-query%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKVzip%253A%2520Query-Agnostic%2520KV%2520Cache%2520Compression%2520with%2520Context%2520Reconstruction%26entry.906535625%3DJang-Hyun%2520Kim%2520and%2520Jinuk%2520Kim%2520and%2520Sangwoo%2520Kwon%2520and%2520Jae%2520W.%2520Lee%2520and%2520Sangdoo%2520Yun%2520and%2520Hyun%2520Oh%2520Song%26entry.1292438233%3D%2520%2520Transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520cache%2520context%2520as%2520key-value%250A%2528KV%2529%2520pairs%2520during%2520inference.%2520As%2520context%2520length%2520grows%252C%2520KV%2520cache%2520sizes%2520expand%252C%250Aleading%2520to%2520substantial%2520memory%2520overhead%2520and%2520increased%2520attention%2520latency.%2520This%250Apaper%2520introduces%2520KVzip%252C%2520a%2520query-agnostic%2520KV%2520cache%2520eviction%2520method%2520enabling%250Aeffective%2520reuse%2520of%2520compressed%2520KV%2520caches%2520across%2520diverse%2520queries.%2520KVzip%250Aquantifies%2520the%2520importance%2520of%2520a%2520KV%2520pair%2520using%2520the%2520underlying%2520LLM%2520to%2520reconstruct%250Aoriginal%2520contexts%2520from%2520cached%2520KV%2520pairs%252C%2520subsequently%2520evicting%2520pairs%2520with%2520lower%250Aimportance.%2520Extensive%2520empirical%2520evaluations%2520demonstrate%2520that%2520KVzip%2520reduces%2520KV%250Acache%2520size%2520by%25203-4%2524%255Ctimes%2524%2520and%2520FlashAttention%2520decoding%2520latency%2520by%2520approximately%250A2%2524%255Ctimes%2524%252C%2520with%2520negligible%2520performance%2520loss%2520in%2520question-answering%252C%2520retrieval%252C%250Areasoning%252C%2520and%2520code%2520comprehension%2520tasks.%2520Evaluations%2520include%2520various%2520models%250Asuch%2520as%2520LLaMA3.1-8B%252C%2520Qwen2.5-14B%252C%2520and%2520Gemma3-12B%252C%2520with%2520context%2520lengths%2520reaching%250Aup%2520to%2520170K%2520tokens.%2520KVzip%2520significantly%2520outperforms%2520existing%2520query-aware%2520KV%250Aeviction%2520methods%252C%2520which%2520suffer%2520from%2520performance%2520degradation%2520even%2520at%2520a%252090%2525%2520cache%250Abudget%2520ratio%2520under%2520multi-query%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KVzip%3A%20Query-Agnostic%20KV%20Cache%20Compression%20with%20Context%20Reconstruction&entry.906535625=Jang-Hyun%20Kim%20and%20Jinuk%20Kim%20and%20Sangwoo%20Kwon%20and%20Jae%20W.%20Lee%20and%20Sangdoo%20Yun%20and%20Hyun%20Oh%20Song&entry.1292438233=%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20cache%20context%20as%20key-value%0A%28KV%29%20pairs%20during%20inference.%20As%20context%20length%20grows%2C%20KV%20cache%20sizes%20expand%2C%0Aleading%20to%20substantial%20memory%20overhead%20and%20increased%20attention%20latency.%20This%0Apaper%20introduces%20KVzip%2C%20a%20query-agnostic%20KV%20cache%20eviction%20method%20enabling%0Aeffective%20reuse%20of%20compressed%20KV%20caches%20across%20diverse%20queries.%20KVzip%0Aquantifies%20the%20importance%20of%20a%20KV%20pair%20using%20the%20underlying%20LLM%20to%20reconstruct%0Aoriginal%20contexts%20from%20cached%20KV%20pairs%2C%20subsequently%20evicting%20pairs%20with%20lower%0Aimportance.%20Extensive%20empirical%20evaluations%20demonstrate%20that%20KVzip%20reduces%20KV%0Acache%20size%20by%203-4%24%5Ctimes%24%20and%20FlashAttention%20decoding%20latency%20by%20approximately%0A2%24%5Ctimes%24%2C%20with%20negligible%20performance%20loss%20in%20question-answering%2C%20retrieval%2C%0Areasoning%2C%20and%20code%20comprehension%20tasks.%20Evaluations%20include%20various%20models%0Asuch%20as%20LLaMA3.1-8B%2C%20Qwen2.5-14B%2C%20and%20Gemma3-12B%2C%20with%20context%20lengths%20reaching%0Aup%20to%20170K%20tokens.%20KVzip%20significantly%20outperforms%20existing%20query-aware%20KV%0Aeviction%20methods%2C%20which%20suffer%20from%20performance%20degradation%20even%20at%20a%2090%25%20cache%0Abudget%20ratio%20under%20multi-query%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23416v1&entry.124074799=Read"},
{"title": "Comparing the Effects of Persistence Barcodes Aggregation and Feature\n  Concatenation on Medical Imaging", "author": "Dashti A. Ali and Richard K. G. Do and William R. Jarnagin and Aras T. Asaad and Amber L. Simpson", "abstract": "  In medical image analysis, feature engineering plays an important role in the\ndesign and performance of machine learning models. Persistent homology (PH),\nfrom the field of topological data analysis (TDA), demonstrates robustness and\nstability to data perturbations and addresses the limitation from traditional\nfeature extraction approaches where a small change in input results in a large\nchange in feature representation. Using PH, we store persistent topological and\ngeometrical features in the form of the persistence barcode whereby large bars\nrepresent global topological features and small bars encapsulate geometrical\ninformation of the data. When multiple barcodes are computed from 2D or 3D\nmedical images, two approaches can be used to construct the final topological\nfeature vector in each dimension: aggregating persistence barcodes followed by\nfeaturization or concatenating topological feature vectors derived from each\nbarcode. In this study, we conduct a comprehensive analysis across diverse\nmedical imaging datasets to compare the effects of the two aforementioned\napproaches on the performance of classification models. The results of this\nanalysis indicate that feature concatenation preserves detailed topological\ninformation from individual barcodes, yields better classification performance\nand is therefore a preferred approach when conducting similar experiments.\n", "link": "http://arxiv.org/abs/2505.23637v1", "date": "2025-05-29", "relevancy": 2.3441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20the%20Effects%20of%20Persistence%20Barcodes%20Aggregation%20and%20Feature%0A%20%20Concatenation%20on%20Medical%20Imaging&body=Title%3A%20Comparing%20the%20Effects%20of%20Persistence%20Barcodes%20Aggregation%20and%20Feature%0A%20%20Concatenation%20on%20Medical%20Imaging%0AAuthor%3A%20Dashti%20A.%20Ali%20and%20Richard%20K.%20G.%20Do%20and%20William%20R.%20Jarnagin%20and%20Aras%20T.%20Asaad%20and%20Amber%20L.%20Simpson%0AAbstract%3A%20%20%20In%20medical%20image%20analysis%2C%20feature%20engineering%20plays%20an%20important%20role%20in%20the%0Adesign%20and%20performance%20of%20machine%20learning%20models.%20Persistent%20homology%20%28PH%29%2C%0Afrom%20the%20field%20of%20topological%20data%20analysis%20%28TDA%29%2C%20demonstrates%20robustness%20and%0Astability%20to%20data%20perturbations%20and%20addresses%20the%20limitation%20from%20traditional%0Afeature%20extraction%20approaches%20where%20a%20small%20change%20in%20input%20results%20in%20a%20large%0Achange%20in%20feature%20representation.%20Using%20PH%2C%20we%20store%20persistent%20topological%20and%0Ageometrical%20features%20in%20the%20form%20of%20the%20persistence%20barcode%20whereby%20large%20bars%0Arepresent%20global%20topological%20features%20and%20small%20bars%20encapsulate%20geometrical%0Ainformation%20of%20the%20data.%20When%20multiple%20barcodes%20are%20computed%20from%202D%20or%203D%0Amedical%20images%2C%20two%20approaches%20can%20be%20used%20to%20construct%20the%20final%20topological%0Afeature%20vector%20in%20each%20dimension%3A%20aggregating%20persistence%20barcodes%20followed%20by%0Afeaturization%20or%20concatenating%20topological%20feature%20vectors%20derived%20from%20each%0Abarcode.%20In%20this%20study%2C%20we%20conduct%20a%20comprehensive%20analysis%20across%20diverse%0Amedical%20imaging%20datasets%20to%20compare%20the%20effects%20of%20the%20two%20aforementioned%0Aapproaches%20on%20the%20performance%20of%20classification%20models.%20The%20results%20of%20this%0Aanalysis%20indicate%20that%20feature%20concatenation%20preserves%20detailed%20topological%0Ainformation%20from%20individual%20barcodes%2C%20yields%20better%20classification%20performance%0Aand%20is%20therefore%20a%20preferred%20approach%20when%20conducting%20similar%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520the%2520Effects%2520of%2520Persistence%2520Barcodes%2520Aggregation%2520and%2520Feature%250A%2520%2520Concatenation%2520on%2520Medical%2520Imaging%26entry.906535625%3DDashti%2520A.%2520Ali%2520and%2520Richard%2520K.%2520G.%2520Do%2520and%2520William%2520R.%2520Jarnagin%2520and%2520Aras%2520T.%2520Asaad%2520and%2520Amber%2520L.%2520Simpson%26entry.1292438233%3D%2520%2520In%2520medical%2520image%2520analysis%252C%2520feature%2520engineering%2520plays%2520an%2520important%2520role%2520in%2520the%250Adesign%2520and%2520performance%2520of%2520machine%2520learning%2520models.%2520Persistent%2520homology%2520%2528PH%2529%252C%250Afrom%2520the%2520field%2520of%2520topological%2520data%2520analysis%2520%2528TDA%2529%252C%2520demonstrates%2520robustness%2520and%250Astability%2520to%2520data%2520perturbations%2520and%2520addresses%2520the%2520limitation%2520from%2520traditional%250Afeature%2520extraction%2520approaches%2520where%2520a%2520small%2520change%2520in%2520input%2520results%2520in%2520a%2520large%250Achange%2520in%2520feature%2520representation.%2520Using%2520PH%252C%2520we%2520store%2520persistent%2520topological%2520and%250Ageometrical%2520features%2520in%2520the%2520form%2520of%2520the%2520persistence%2520barcode%2520whereby%2520large%2520bars%250Arepresent%2520global%2520topological%2520features%2520and%2520small%2520bars%2520encapsulate%2520geometrical%250Ainformation%2520of%2520the%2520data.%2520When%2520multiple%2520barcodes%2520are%2520computed%2520from%25202D%2520or%25203D%250Amedical%2520images%252C%2520two%2520approaches%2520can%2520be%2520used%2520to%2520construct%2520the%2520final%2520topological%250Afeature%2520vector%2520in%2520each%2520dimension%253A%2520aggregating%2520persistence%2520barcodes%2520followed%2520by%250Afeaturization%2520or%2520concatenating%2520topological%2520feature%2520vectors%2520derived%2520from%2520each%250Abarcode.%2520In%2520this%2520study%252C%2520we%2520conduct%2520a%2520comprehensive%2520analysis%2520across%2520diverse%250Amedical%2520imaging%2520datasets%2520to%2520compare%2520the%2520effects%2520of%2520the%2520two%2520aforementioned%250Aapproaches%2520on%2520the%2520performance%2520of%2520classification%2520models.%2520The%2520results%2520of%2520this%250Aanalysis%2520indicate%2520that%2520feature%2520concatenation%2520preserves%2520detailed%2520topological%250Ainformation%2520from%2520individual%2520barcodes%252C%2520yields%2520better%2520classification%2520performance%250Aand%2520is%2520therefore%2520a%2520preferred%2520approach%2520when%2520conducting%2520similar%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20the%20Effects%20of%20Persistence%20Barcodes%20Aggregation%20and%20Feature%0A%20%20Concatenation%20on%20Medical%20Imaging&entry.906535625=Dashti%20A.%20Ali%20and%20Richard%20K.%20G.%20Do%20and%20William%20R.%20Jarnagin%20and%20Aras%20T.%20Asaad%20and%20Amber%20L.%20Simpson&entry.1292438233=%20%20In%20medical%20image%20analysis%2C%20feature%20engineering%20plays%20an%20important%20role%20in%20the%0Adesign%20and%20performance%20of%20machine%20learning%20models.%20Persistent%20homology%20%28PH%29%2C%0Afrom%20the%20field%20of%20topological%20data%20analysis%20%28TDA%29%2C%20demonstrates%20robustness%20and%0Astability%20to%20data%20perturbations%20and%20addresses%20the%20limitation%20from%20traditional%0Afeature%20extraction%20approaches%20where%20a%20small%20change%20in%20input%20results%20in%20a%20large%0Achange%20in%20feature%20representation.%20Using%20PH%2C%20we%20store%20persistent%20topological%20and%0Ageometrical%20features%20in%20the%20form%20of%20the%20persistence%20barcode%20whereby%20large%20bars%0Arepresent%20global%20topological%20features%20and%20small%20bars%20encapsulate%20geometrical%0Ainformation%20of%20the%20data.%20When%20multiple%20barcodes%20are%20computed%20from%202D%20or%203D%0Amedical%20images%2C%20two%20approaches%20can%20be%20used%20to%20construct%20the%20final%20topological%0Afeature%20vector%20in%20each%20dimension%3A%20aggregating%20persistence%20barcodes%20followed%20by%0Afeaturization%20or%20concatenating%20topological%20feature%20vectors%20derived%20from%20each%0Abarcode.%20In%20this%20study%2C%20we%20conduct%20a%20comprehensive%20analysis%20across%20diverse%0Amedical%20imaging%20datasets%20to%20compare%20the%20effects%20of%20the%20two%20aforementioned%0Aapproaches%20on%20the%20performance%20of%20classification%20models.%20The%20results%20of%20this%0Aanalysis%20indicate%20that%20feature%20concatenation%20preserves%20detailed%20topological%0Ainformation%20from%20individual%20barcodes%2C%20yields%20better%20classification%20performance%0Aand%20is%20therefore%20a%20preferred%20approach%20when%20conducting%20similar%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23637v1&entry.124074799=Read"},
{"title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation", "author": "Aviv Slobodkin and Hagai Taitelbaum and Yonatan Bitton and Brian Gordon and Michal Sokolik and Nitzan Bitton Guetta and Almog Gueta and Royi Rassin and Dani Lischinski and Idan Szpektor", "abstract": "  Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability -\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering - progress in this field is limited\nby the lack of reliable automatic evaluation. Existing methods either assess\nonly one aspect of the task (i.e., textual alignment or subject preservation),\nmisalign with human judgments, or rely on costly API-based evaluation. To\naddress this gap, we introduce RefVNLI, a cost-effective metric that evaluates\nboth textual alignment and subject preservation in a single run. Trained on a\nlarge-scale dataset derived from video-reasoning benchmarks and image\nperturbations, RefVNLI outperforms or statistically matches existing baselines\nacross multiple benchmarks and subject categories (e.g., \\emph{Animal},\n\\emph{Object}), achieving up to 6.4-point gains in textual alignment and\n5.9-point gains in subject preservation.\n", "link": "http://arxiv.org/abs/2504.17502v2", "date": "2025-05-29", "relevancy": 2.3427, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5989}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5804}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefVNLI%3A%20Towards%20Scalable%20Evaluation%20of%20Subject-driven%20Text-to-image%0A%20%20Generation&body=Title%3A%20RefVNLI%3A%20Towards%20Scalable%20Evaluation%20of%20Subject-driven%20Text-to-image%0A%20%20Generation%0AAuthor%3A%20Aviv%20Slobodkin%20and%20Hagai%20Taitelbaum%20and%20Yonatan%20Bitton%20and%20Brian%20Gordon%20and%20Michal%20Sokolik%20and%20Nitzan%20Bitton%20Guetta%20and%20Almog%20Gueta%20and%20Royi%20Rassin%20and%20Dani%20Lischinski%20and%20Idan%20Szpektor%0AAbstract%3A%20%20%20Subject-driven%20text-to-image%20%28T2I%29%20generation%20aims%20to%20produce%20images%20that%0Aalign%20with%20a%20given%20textual%20description%2C%20while%20preserving%20the%20visual%20identity%0Afrom%20a%20referenced%20subject%20image.%20Despite%20its%20broad%20downstream%20applicability%20-%0Aranging%20from%20enhanced%20personalization%20in%20image%20generation%20to%20consistent%0Acharacter%20representation%20in%20video%20rendering%20-%20progress%20in%20this%20field%20is%20limited%0Aby%20the%20lack%20of%20reliable%20automatic%20evaluation.%20Existing%20methods%20either%20assess%0Aonly%20one%20aspect%20of%20the%20task%20%28i.e.%2C%20textual%20alignment%20or%20subject%20preservation%29%2C%0Amisalign%20with%20human%20judgments%2C%20or%20rely%20on%20costly%20API-based%20evaluation.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20RefVNLI%2C%20a%20cost-effective%20metric%20that%20evaluates%0Aboth%20textual%20alignment%20and%20subject%20preservation%20in%20a%20single%20run.%20Trained%20on%20a%0Alarge-scale%20dataset%20derived%20from%20video-reasoning%20benchmarks%20and%20image%0Aperturbations%2C%20RefVNLI%20outperforms%20or%20statistically%20matches%20existing%20baselines%0Aacross%20multiple%20benchmarks%20and%20subject%20categories%20%28e.g.%2C%20%5Cemph%7BAnimal%7D%2C%0A%5Cemph%7BObject%7D%29%2C%20achieving%20up%20to%206.4-point%20gains%20in%20textual%20alignment%20and%0A5.9-point%20gains%20in%20subject%20preservation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefVNLI%253A%2520Towards%2520Scalable%2520Evaluation%2520of%2520Subject-driven%2520Text-to-image%250A%2520%2520Generation%26entry.906535625%3DAviv%2520Slobodkin%2520and%2520Hagai%2520Taitelbaum%2520and%2520Yonatan%2520Bitton%2520and%2520Brian%2520Gordon%2520and%2520Michal%2520Sokolik%2520and%2520Nitzan%2520Bitton%2520Guetta%2520and%2520Almog%2520Gueta%2520and%2520Royi%2520Rassin%2520and%2520Dani%2520Lischinski%2520and%2520Idan%2520Szpektor%26entry.1292438233%3D%2520%2520Subject-driven%2520text-to-image%2520%2528T2I%2529%2520generation%2520aims%2520to%2520produce%2520images%2520that%250Aalign%2520with%2520a%2520given%2520textual%2520description%252C%2520while%2520preserving%2520the%2520visual%2520identity%250Afrom%2520a%2520referenced%2520subject%2520image.%2520Despite%2520its%2520broad%2520downstream%2520applicability%2520-%250Aranging%2520from%2520enhanced%2520personalization%2520in%2520image%2520generation%2520to%2520consistent%250Acharacter%2520representation%2520in%2520video%2520rendering%2520-%2520progress%2520in%2520this%2520field%2520is%2520limited%250Aby%2520the%2520lack%2520of%2520reliable%2520automatic%2520evaluation.%2520Existing%2520methods%2520either%2520assess%250Aonly%2520one%2520aspect%2520of%2520the%2520task%2520%2528i.e.%252C%2520textual%2520alignment%2520or%2520subject%2520preservation%2529%252C%250Amisalign%2520with%2520human%2520judgments%252C%2520or%2520rely%2520on%2520costly%2520API-based%2520evaluation.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520RefVNLI%252C%2520a%2520cost-effective%2520metric%2520that%2520evaluates%250Aboth%2520textual%2520alignment%2520and%2520subject%2520preservation%2520in%2520a%2520single%2520run.%2520Trained%2520on%2520a%250Alarge-scale%2520dataset%2520derived%2520from%2520video-reasoning%2520benchmarks%2520and%2520image%250Aperturbations%252C%2520RefVNLI%2520outperforms%2520or%2520statistically%2520matches%2520existing%2520baselines%250Aacross%2520multiple%2520benchmarks%2520and%2520subject%2520categories%2520%2528e.g.%252C%2520%255Cemph%257BAnimal%257D%252C%250A%255Cemph%257BObject%257D%2529%252C%2520achieving%2520up%2520to%25206.4-point%2520gains%2520in%2520textual%2520alignment%2520and%250A5.9-point%2520gains%2520in%2520subject%2520preservation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefVNLI%3A%20Towards%20Scalable%20Evaluation%20of%20Subject-driven%20Text-to-image%0A%20%20Generation&entry.906535625=Aviv%20Slobodkin%20and%20Hagai%20Taitelbaum%20and%20Yonatan%20Bitton%20and%20Brian%20Gordon%20and%20Michal%20Sokolik%20and%20Nitzan%20Bitton%20Guetta%20and%20Almog%20Gueta%20and%20Royi%20Rassin%20and%20Dani%20Lischinski%20and%20Idan%20Szpektor&entry.1292438233=%20%20Subject-driven%20text-to-image%20%28T2I%29%20generation%20aims%20to%20produce%20images%20that%0Aalign%20with%20a%20given%20textual%20description%2C%20while%20preserving%20the%20visual%20identity%0Afrom%20a%20referenced%20subject%20image.%20Despite%20its%20broad%20downstream%20applicability%20-%0Aranging%20from%20enhanced%20personalization%20in%20image%20generation%20to%20consistent%0Acharacter%20representation%20in%20video%20rendering%20-%20progress%20in%20this%20field%20is%20limited%0Aby%20the%20lack%20of%20reliable%20automatic%20evaluation.%20Existing%20methods%20either%20assess%0Aonly%20one%20aspect%20of%20the%20task%20%28i.e.%2C%20textual%20alignment%20or%20subject%20preservation%29%2C%0Amisalign%20with%20human%20judgments%2C%20or%20rely%20on%20costly%20API-based%20evaluation.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20RefVNLI%2C%20a%20cost-effective%20metric%20that%20evaluates%0Aboth%20textual%20alignment%20and%20subject%20preservation%20in%20a%20single%20run.%20Trained%20on%20a%0Alarge-scale%20dataset%20derived%20from%20video-reasoning%20benchmarks%20and%20image%0Aperturbations%2C%20RefVNLI%20outperforms%20or%20statistically%20matches%20existing%20baselines%0Aacross%20multiple%20benchmarks%20and%20subject%20categories%20%28e.g.%2C%20%5Cemph%7BAnimal%7D%2C%0A%5Cemph%7BObject%7D%29%2C%20achieving%20up%20to%206.4-point%20gains%20in%20textual%20alignment%20and%0A5.9-point%20gains%20in%20subject%20preservation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17502v2&entry.124074799=Read"},
{"title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control", "author": "Anthony Chen and Wenzhao Zheng and Yida Wang and Xueyang Zhang and Kun Zhan and Peng Jia and Kurt Keutzer and Shanghang Zhang", "abstract": "  Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control.\n", "link": "http://arxiv.org/abs/2505.22421v2", "date": "2025-05-29", "relevancy": 2.333, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5999}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5835}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoDrive%3A%203D%20Geometry-Informed%20Driving%20World%20Model%20with%20Precise%20Action%0A%20%20Control&body=Title%3A%20GeoDrive%3A%203D%20Geometry-Informed%20Driving%20World%20Model%20with%20Precise%20Action%0A%20%20Control%0AAuthor%3A%20Anthony%20Chen%20and%20Wenzhao%20Zheng%20and%20Yida%20Wang%20and%20Xueyang%20Zhang%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Kurt%20Keutzer%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20world%20models%20have%20revolutionized%20dynamic%20environment%0Asimulation%2C%20allowing%20systems%20to%20foresee%20future%20states%20and%20assess%20potential%0Aactions.%20In%20autonomous%20driving%2C%20these%20capabilities%20help%20vehicles%20anticipate%20the%0Abehavior%20of%20other%20road%20users%2C%20perform%20risk-aware%20planning%2C%20accelerate%20training%0Ain%20simulation%2C%20and%20adapt%20to%20novel%20scenarios%2C%20thereby%20enhancing%20safety%20and%0Areliability.%20Current%20approaches%20exhibit%20deficiencies%20in%20maintaining%20robust%203D%0Ageometric%20consistency%20or%20accumulating%20artifacts%20during%20occlusion%20handling%2C%20both%0Acritical%20for%20reliable%20safety%20assessment%20in%20autonomous%20navigation%20tasks.%20To%0Aaddress%20this%2C%20we%20introduce%20GeoDrive%2C%20which%20explicitly%20integrates%20robust%203D%0Ageometry%20conditions%20into%20driving%20world%20models%20to%20enhance%20spatial%20understanding%0Aand%20action%20controllability.%20Specifically%2C%20we%20first%20extract%20a%203D%20representation%0Afrom%20the%20input%20frame%20and%20then%20obtain%20its%202D%20rendering%20based%20on%20the%0Auser-specified%20ego-car%20trajectory.%20To%20enable%20dynamic%20modeling%2C%20we%20propose%20a%0Adynamic%20editing%20module%20during%20training%20to%20enhance%20the%20renderings%20by%20editing%20the%0Apositions%20of%20the%20vehicles.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20existing%20models%20in%20both%20action%20accuracy%20and%203D%0Aspatial%20awareness%2C%20leading%20to%20more%20realistic%2C%20adaptable%2C%20and%20reliable%20scene%0Amodeling%20for%20safer%20autonomous%20driving.%20Additionally%2C%20our%20model%20can%20generalize%0Ato%20novel%20trajectories%20and%20offers%20interactive%20scene%20editing%20capabilities%2C%20such%0Aas%20object%20editing%20and%20object%20trajectory%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22421v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoDrive%253A%25203D%2520Geometry-Informed%2520Driving%2520World%2520Model%2520with%2520Precise%2520Action%250A%2520%2520Control%26entry.906535625%3DAnthony%2520Chen%2520and%2520Wenzhao%2520Zheng%2520and%2520Yida%2520Wang%2520and%2520Xueyang%2520Zhang%2520and%2520Kun%2520Zhan%2520and%2520Peng%2520Jia%2520and%2520Kurt%2520Keutzer%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520world%2520models%2520have%2520revolutionized%2520dynamic%2520environment%250Asimulation%252C%2520allowing%2520systems%2520to%2520foresee%2520future%2520states%2520and%2520assess%2520potential%250Aactions.%2520In%2520autonomous%2520driving%252C%2520these%2520capabilities%2520help%2520vehicles%2520anticipate%2520the%250Abehavior%2520of%2520other%2520road%2520users%252C%2520perform%2520risk-aware%2520planning%252C%2520accelerate%2520training%250Ain%2520simulation%252C%2520and%2520adapt%2520to%2520novel%2520scenarios%252C%2520thereby%2520enhancing%2520safety%2520and%250Areliability.%2520Current%2520approaches%2520exhibit%2520deficiencies%2520in%2520maintaining%2520robust%25203D%250Ageometric%2520consistency%2520or%2520accumulating%2520artifacts%2520during%2520occlusion%2520handling%252C%2520both%250Acritical%2520for%2520reliable%2520safety%2520assessment%2520in%2520autonomous%2520navigation%2520tasks.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520GeoDrive%252C%2520which%2520explicitly%2520integrates%2520robust%25203D%250Ageometry%2520conditions%2520into%2520driving%2520world%2520models%2520to%2520enhance%2520spatial%2520understanding%250Aand%2520action%2520controllability.%2520Specifically%252C%2520we%2520first%2520extract%2520a%25203D%2520representation%250Afrom%2520the%2520input%2520frame%2520and%2520then%2520obtain%2520its%25202D%2520rendering%2520based%2520on%2520the%250Auser-specified%2520ego-car%2520trajectory.%2520To%2520enable%2520dynamic%2520modeling%252C%2520we%2520propose%2520a%250Adynamic%2520editing%2520module%2520during%2520training%2520to%2520enhance%2520the%2520renderings%2520by%2520editing%2520the%250Apositions%2520of%2520the%2520vehicles.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520existing%2520models%2520in%2520both%2520action%2520accuracy%2520and%25203D%250Aspatial%2520awareness%252C%2520leading%2520to%2520more%2520realistic%252C%2520adaptable%252C%2520and%2520reliable%2520scene%250Amodeling%2520for%2520safer%2520autonomous%2520driving.%2520Additionally%252C%2520our%2520model%2520can%2520generalize%250Ato%2520novel%2520trajectories%2520and%2520offers%2520interactive%2520scene%2520editing%2520capabilities%252C%2520such%250Aas%2520object%2520editing%2520and%2520object%2520trajectory%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22421v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoDrive%3A%203D%20Geometry-Informed%20Driving%20World%20Model%20with%20Precise%20Action%0A%20%20Control&entry.906535625=Anthony%20Chen%20and%20Wenzhao%20Zheng%20and%20Yida%20Wang%20and%20Xueyang%20Zhang%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Kurt%20Keutzer%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20world%20models%20have%20revolutionized%20dynamic%20environment%0Asimulation%2C%20allowing%20systems%20to%20foresee%20future%20states%20and%20assess%20potential%0Aactions.%20In%20autonomous%20driving%2C%20these%20capabilities%20help%20vehicles%20anticipate%20the%0Abehavior%20of%20other%20road%20users%2C%20perform%20risk-aware%20planning%2C%20accelerate%20training%0Ain%20simulation%2C%20and%20adapt%20to%20novel%20scenarios%2C%20thereby%20enhancing%20safety%20and%0Areliability.%20Current%20approaches%20exhibit%20deficiencies%20in%20maintaining%20robust%203D%0Ageometric%20consistency%20or%20accumulating%20artifacts%20during%20occlusion%20handling%2C%20both%0Acritical%20for%20reliable%20safety%20assessment%20in%20autonomous%20navigation%20tasks.%20To%0Aaddress%20this%2C%20we%20introduce%20GeoDrive%2C%20which%20explicitly%20integrates%20robust%203D%0Ageometry%20conditions%20into%20driving%20world%20models%20to%20enhance%20spatial%20understanding%0Aand%20action%20controllability.%20Specifically%2C%20we%20first%20extract%20a%203D%20representation%0Afrom%20the%20input%20frame%20and%20then%20obtain%20its%202D%20rendering%20based%20on%20the%0Auser-specified%20ego-car%20trajectory.%20To%20enable%20dynamic%20modeling%2C%20we%20propose%20a%0Adynamic%20editing%20module%20during%20training%20to%20enhance%20the%20renderings%20by%20editing%20the%0Apositions%20of%20the%20vehicles.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20existing%20models%20in%20both%20action%20accuracy%20and%203D%0Aspatial%20awareness%2C%20leading%20to%20more%20realistic%2C%20adaptable%2C%20and%20reliable%20scene%0Amodeling%20for%20safer%20autonomous%20driving.%20Additionally%2C%20our%20model%20can%20generalize%0Ato%20novel%20trajectories%20and%20offers%20interactive%20scene%20editing%20capabilities%2C%20such%0Aas%20object%20editing%20and%20object%20trajectory%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22421v2&entry.124074799=Read"},
{"title": "Hijacking Large Language Models via Adversarial In-Context Learning", "author": "Xiangyu Zhou and Yao Qiang and Saleh Zare Zade and Prashant Khanduri and Dongxiao Zhu", "abstract": "  In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific downstream tasks by utilizing labeled examples as demonstrations\n(demos) in the preconditioned prompts. Despite its promising performance,\ncrafted adversarial attacks pose a notable threat to the robustness of LLMs.\nExisting attacks are either easy to detect, require a trigger in user input, or\nlack specificity towards ICL. To address these issues, this work introduces a\nnovel transferable prompt injection attack against ICL, aiming to hijack LLMs\nto generate the target output or elicit harmful responses. In our threat model,\nthe hacker acts as a model publisher who leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demos via prompt injection. We also propose effective defense\nstrategies using a few shots of clean demos, enhancing the robustness of LLMs\nduring ICL. Extensive experimental results across various classification and\njailbreak tasks demonstrate the effectiveness of the proposed attack and\ndefense strategies. This work highlights the significant security\nvulnerabilities of LLMs during ICL and underscores the need for further\nin-depth studies.\n", "link": "http://arxiv.org/abs/2311.09948v3", "date": "2025-05-29", "relevancy": 2.3252, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4583}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hijacking%20Large%20Language%20Models%20via%20Adversarial%20In-Context%20Learning&body=Title%3A%20Hijacking%20Large%20Language%20Models%20via%20Adversarial%20In-Context%20Learning%0AAuthor%3A%20Xiangyu%20Zhou%20and%20Yao%20Qiang%20and%20Saleh%20Zare%20Zade%20and%20Prashant%20Khanduri%20and%20Dongxiao%20Zhu%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20has%20emerged%20as%20a%20powerful%20paradigm%20leveraging%20LLMs%0Afor%20specific%20downstream%20tasks%20by%20utilizing%20labeled%20examples%20as%20demonstrations%0A%28demos%29%20in%20the%20preconditioned%20prompts.%20Despite%20its%20promising%20performance%2C%0Acrafted%20adversarial%20attacks%20pose%20a%20notable%20threat%20to%20the%20robustness%20of%20LLMs.%0AExisting%20attacks%20are%20either%20easy%20to%20detect%2C%20require%20a%20trigger%20in%20user%20input%2C%20or%0Alack%20specificity%20towards%20ICL.%20To%20address%20these%20issues%2C%20this%20work%20introduces%20a%0Anovel%20transferable%20prompt%20injection%20attack%20against%20ICL%2C%20aiming%20to%20hijack%20LLMs%0Ato%20generate%20the%20target%20output%20or%20elicit%20harmful%20responses.%20In%20our%20threat%20model%2C%0Athe%20hacker%20acts%20as%20a%20model%20publisher%20who%20leverages%20a%20gradient-based%20prompt%0Asearch%20method%20to%20learn%20and%20append%20imperceptible%20adversarial%20suffixes%20to%20the%0Ain-context%20demos%20via%20prompt%20injection.%20We%20also%20propose%20effective%20defense%0Astrategies%20using%20a%20few%20shots%20of%20clean%20demos%2C%20enhancing%20the%20robustness%20of%20LLMs%0Aduring%20ICL.%20Extensive%20experimental%20results%20across%20various%20classification%20and%0Ajailbreak%20tasks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20attack%20and%0Adefense%20strategies.%20This%20work%20highlights%20the%20significant%20security%0Avulnerabilities%20of%20LLMs%20during%20ICL%20and%20underscores%20the%20need%20for%20further%0Ain-depth%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09948v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHijacking%2520Large%2520Language%2520Models%2520via%2520Adversarial%2520In-Context%2520Learning%26entry.906535625%3DXiangyu%2520Zhou%2520and%2520Yao%2520Qiang%2520and%2520Saleh%2520Zare%2520Zade%2520and%2520Prashant%2520Khanduri%2520and%2520Dongxiao%2520Zhu%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520leveraging%2520LLMs%250Afor%2520specific%2520downstream%2520tasks%2520by%2520utilizing%2520labeled%2520examples%2520as%2520demonstrations%250A%2528demos%2529%2520in%2520the%2520preconditioned%2520prompts.%2520Despite%2520its%2520promising%2520performance%252C%250Acrafted%2520adversarial%2520attacks%2520pose%2520a%2520notable%2520threat%2520to%2520the%2520robustness%2520of%2520LLMs.%250AExisting%2520attacks%2520are%2520either%2520easy%2520to%2520detect%252C%2520require%2520a%2520trigger%2520in%2520user%2520input%252C%2520or%250Alack%2520specificity%2520towards%2520ICL.%2520To%2520address%2520these%2520issues%252C%2520this%2520work%2520introduces%2520a%250Anovel%2520transferable%2520prompt%2520injection%2520attack%2520against%2520ICL%252C%2520aiming%2520to%2520hijack%2520LLMs%250Ato%2520generate%2520the%2520target%2520output%2520or%2520elicit%2520harmful%2520responses.%2520In%2520our%2520threat%2520model%252C%250Athe%2520hacker%2520acts%2520as%2520a%2520model%2520publisher%2520who%2520leverages%2520a%2520gradient-based%2520prompt%250Asearch%2520method%2520to%2520learn%2520and%2520append%2520imperceptible%2520adversarial%2520suffixes%2520to%2520the%250Ain-context%2520demos%2520via%2520prompt%2520injection.%2520We%2520also%2520propose%2520effective%2520defense%250Astrategies%2520using%2520a%2520few%2520shots%2520of%2520clean%2520demos%252C%2520enhancing%2520the%2520robustness%2520of%2520LLMs%250Aduring%2520ICL.%2520Extensive%2520experimental%2520results%2520across%2520various%2520classification%2520and%250Ajailbreak%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520attack%2520and%250Adefense%2520strategies.%2520This%2520work%2520highlights%2520the%2520significant%2520security%250Avulnerabilities%2520of%2520LLMs%2520during%2520ICL%2520and%2520underscores%2520the%2520need%2520for%2520further%250Ain-depth%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09948v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hijacking%20Large%20Language%20Models%20via%20Adversarial%20In-Context%20Learning&entry.906535625=Xiangyu%20Zhou%20and%20Yao%20Qiang%20and%20Saleh%20Zare%20Zade%20and%20Prashant%20Khanduri%20and%20Dongxiao%20Zhu&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20has%20emerged%20as%20a%20powerful%20paradigm%20leveraging%20LLMs%0Afor%20specific%20downstream%20tasks%20by%20utilizing%20labeled%20examples%20as%20demonstrations%0A%28demos%29%20in%20the%20preconditioned%20prompts.%20Despite%20its%20promising%20performance%2C%0Acrafted%20adversarial%20attacks%20pose%20a%20notable%20threat%20to%20the%20robustness%20of%20LLMs.%0AExisting%20attacks%20are%20either%20easy%20to%20detect%2C%20require%20a%20trigger%20in%20user%20input%2C%20or%0Alack%20specificity%20towards%20ICL.%20To%20address%20these%20issues%2C%20this%20work%20introduces%20a%0Anovel%20transferable%20prompt%20injection%20attack%20against%20ICL%2C%20aiming%20to%20hijack%20LLMs%0Ato%20generate%20the%20target%20output%20or%20elicit%20harmful%20responses.%20In%20our%20threat%20model%2C%0Athe%20hacker%20acts%20as%20a%20model%20publisher%20who%20leverages%20a%20gradient-based%20prompt%0Asearch%20method%20to%20learn%20and%20append%20imperceptible%20adversarial%20suffixes%20to%20the%0Ain-context%20demos%20via%20prompt%20injection.%20We%20also%20propose%20effective%20defense%0Astrategies%20using%20a%20few%20shots%20of%20clean%20demos%2C%20enhancing%20the%20robustness%20of%20LLMs%0Aduring%20ICL.%20Extensive%20experimental%20results%20across%20various%20classification%20and%0Ajailbreak%20tasks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20attack%20and%0Adefense%20strategies.%20This%20work%20highlights%20the%20significant%20security%0Avulnerabilities%20of%20LLMs%20during%20ICL%20and%20underscores%20the%20need%20for%20further%0Ain-depth%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09948v3&entry.124074799=Read"},
{"title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning\n  Engineering", "author": "Zexi Liu and Jingyi Chai and Xinyu Zhu and Shuo Tang and Rui Ye and Bo Zhang and Lei Bai and Siheng Chen", "abstract": "  The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.\n", "link": "http://arxiv.org/abs/2505.23723v1", "date": "2025-05-29", "relevancy": 2.3226, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.604}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5669}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ML-Agent%3A%20Reinforcing%20LLM%20Agents%20for%20Autonomous%20Machine%20Learning%0A%20%20Engineering&body=Title%3A%20ML-Agent%3A%20Reinforcing%20LLM%20Agents%20for%20Autonomous%20Machine%20Learning%0A%20%20Engineering%0AAuthor%3A%20Zexi%20Liu%20and%20Jingyi%20Chai%20and%20Xinyu%20Zhu%20and%20Shuo%20Tang%20and%20Rui%20Ye%20and%20Bo%20Zhang%20and%20Lei%20Bai%20and%20Siheng%20Chen%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20language%20model%20%28LLM%29-based%20agents%20has%20significantly%0Aadvanced%20the%20development%20of%20autonomous%20machine%20learning%20%28ML%29%20engineering.%0AHowever%2C%20most%20existing%20approaches%20rely%20heavily%20on%20manual%20prompt%20engineering%2C%0Afailing%20to%20adapt%20and%20optimize%20based%20on%20diverse%20experimental%20experiences.%0AFocusing%20on%20this%2C%20for%20the%20first%20time%2C%20we%20explore%20the%20paradigm%20of%20learning-based%0Aagentic%20ML%2C%20where%20an%20LLM%20agent%20learns%20through%20interactive%20experimentation%20on%20ML%0Atasks%20using%20online%20reinforcement%20learning%20%28RL%29.%20To%20realize%20this%2C%20we%20propose%20a%0Anovel%20agentic%20ML%20training%20framework%20with%20three%20key%20components%3A%20%281%29%0Aexploration-enriched%20fine-tuning%2C%20which%20enables%20LLM%20agents%20to%20generate%20diverse%0Aactions%20for%20enhanced%20RL%20exploration%3B%20%282%29%20step-wise%20RL%2C%20which%20enables%20training%0Aon%20a%20single%20action%20step%2C%20accelerating%20experience%20collection%20and%20improving%0Atraining%20efficiency%3B%20%283%29%20an%20agentic%20ML-specific%20reward%20module%2C%20which%20unifies%0Avaried%20ML%20feedback%20signals%20into%20consistent%20rewards%20for%20RL%20optimization.%0ALeveraging%20this%20framework%2C%20we%20train%20ML-Agent%2C%20driven%20by%20a%207B-sized%20Qwen-2.5%20LLM%0Afor%20autonomous%20ML.%20Remarkably%2C%20despite%20being%20trained%20on%20merely%209%20ML%20tasks%2C%20our%0A7B-sized%20ML-Agent%20outperforms%20the%20671B-sized%20DeepSeek-R1%20agent.%20Furthermore%2C%20it%0Aachieves%20continuous%20performance%20improvements%20and%20demonstrates%20exceptional%0Across-task%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DML-Agent%253A%2520Reinforcing%2520LLM%2520Agents%2520for%2520Autonomous%2520Machine%2520Learning%250A%2520%2520Engineering%26entry.906535625%3DZexi%2520Liu%2520and%2520Jingyi%2520Chai%2520and%2520Xinyu%2520Zhu%2520and%2520Shuo%2520Tang%2520and%2520Rui%2520Ye%2520and%2520Bo%2520Zhang%2520and%2520Lei%2520Bai%2520and%2520Siheng%2520Chen%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520language%2520model%2520%2528LLM%2529-based%2520agents%2520has%2520significantly%250Aadvanced%2520the%2520development%2520of%2520autonomous%2520machine%2520learning%2520%2528ML%2529%2520engineering.%250AHowever%252C%2520most%2520existing%2520approaches%2520rely%2520heavily%2520on%2520manual%2520prompt%2520engineering%252C%250Afailing%2520to%2520adapt%2520and%2520optimize%2520based%2520on%2520diverse%2520experimental%2520experiences.%250AFocusing%2520on%2520this%252C%2520for%2520the%2520first%2520time%252C%2520we%2520explore%2520the%2520paradigm%2520of%2520learning-based%250Aagentic%2520ML%252C%2520where%2520an%2520LLM%2520agent%2520learns%2520through%2520interactive%2520experimentation%2520on%2520ML%250Atasks%2520using%2520online%2520reinforcement%2520learning%2520%2528RL%2529.%2520To%2520realize%2520this%252C%2520we%2520propose%2520a%250Anovel%2520agentic%2520ML%2520training%2520framework%2520with%2520three%2520key%2520components%253A%2520%25281%2529%250Aexploration-enriched%2520fine-tuning%252C%2520which%2520enables%2520LLM%2520agents%2520to%2520generate%2520diverse%250Aactions%2520for%2520enhanced%2520RL%2520exploration%253B%2520%25282%2529%2520step-wise%2520RL%252C%2520which%2520enables%2520training%250Aon%2520a%2520single%2520action%2520step%252C%2520accelerating%2520experience%2520collection%2520and%2520improving%250Atraining%2520efficiency%253B%2520%25283%2529%2520an%2520agentic%2520ML-specific%2520reward%2520module%252C%2520which%2520unifies%250Avaried%2520ML%2520feedback%2520signals%2520into%2520consistent%2520rewards%2520for%2520RL%2520optimization.%250ALeveraging%2520this%2520framework%252C%2520we%2520train%2520ML-Agent%252C%2520driven%2520by%2520a%25207B-sized%2520Qwen-2.5%2520LLM%250Afor%2520autonomous%2520ML.%2520Remarkably%252C%2520despite%2520being%2520trained%2520on%2520merely%25209%2520ML%2520tasks%252C%2520our%250A7B-sized%2520ML-Agent%2520outperforms%2520the%2520671B-sized%2520DeepSeek-R1%2520agent.%2520Furthermore%252C%2520it%250Aachieves%2520continuous%2520performance%2520improvements%2520and%2520demonstrates%2520exceptional%250Across-task%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ML-Agent%3A%20Reinforcing%20LLM%20Agents%20for%20Autonomous%20Machine%20Learning%0A%20%20Engineering&entry.906535625=Zexi%20Liu%20and%20Jingyi%20Chai%20and%20Xinyu%20Zhu%20and%20Shuo%20Tang%20and%20Rui%20Ye%20and%20Bo%20Zhang%20and%20Lei%20Bai%20and%20Siheng%20Chen&entry.1292438233=%20%20The%20emergence%20of%20large%20language%20model%20%28LLM%29-based%20agents%20has%20significantly%0Aadvanced%20the%20development%20of%20autonomous%20machine%20learning%20%28ML%29%20engineering.%0AHowever%2C%20most%20existing%20approaches%20rely%20heavily%20on%20manual%20prompt%20engineering%2C%0Afailing%20to%20adapt%20and%20optimize%20based%20on%20diverse%20experimental%20experiences.%0AFocusing%20on%20this%2C%20for%20the%20first%20time%2C%20we%20explore%20the%20paradigm%20of%20learning-based%0Aagentic%20ML%2C%20where%20an%20LLM%20agent%20learns%20through%20interactive%20experimentation%20on%20ML%0Atasks%20using%20online%20reinforcement%20learning%20%28RL%29.%20To%20realize%20this%2C%20we%20propose%20a%0Anovel%20agentic%20ML%20training%20framework%20with%20three%20key%20components%3A%20%281%29%0Aexploration-enriched%20fine-tuning%2C%20which%20enables%20LLM%20agents%20to%20generate%20diverse%0Aactions%20for%20enhanced%20RL%20exploration%3B%20%282%29%20step-wise%20RL%2C%20which%20enables%20training%0Aon%20a%20single%20action%20step%2C%20accelerating%20experience%20collection%20and%20improving%0Atraining%20efficiency%3B%20%283%29%20an%20agentic%20ML-specific%20reward%20module%2C%20which%20unifies%0Avaried%20ML%20feedback%20signals%20into%20consistent%20rewards%20for%20RL%20optimization.%0ALeveraging%20this%20framework%2C%20we%20train%20ML-Agent%2C%20driven%20by%20a%207B-sized%20Qwen-2.5%20LLM%0Afor%20autonomous%20ML.%20Remarkably%2C%20despite%20being%20trained%20on%20merely%209%20ML%20tasks%2C%20our%0A7B-sized%20ML-Agent%20outperforms%20the%20671B-sized%20DeepSeek-R1%20agent.%20Furthermore%2C%20it%0Aachieves%20continuous%20performance%20improvements%20and%20demonstrates%20exceptional%0Across-task%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23723v1&entry.124074799=Read"},
{"title": "SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in\n  Spiking Neural Networks", "author": "Xinyu Luo and Kecheng Chen and Pao-Sheng Vincent Sun and Chris Xing Tian and Arindam Basu and Haoliang Li", "abstract": "  Spiking Neural Networks (SNNs), as a biologically plausible alternative to\nArtificial Neural Networks (ANNs), have demonstrated advantages in terms of\nenergy efficiency, temporal processing, and biological plausibility. However,\nSNNs are highly sensitive to distribution shifts, which can significantly\ndegrade their performance in real-world scenarios. Traditional test-time\nadaptation (TTA) methods designed for ANNs often fail to address the unique\ncomputational dynamics of SNNs, such as sparsity and temporal spiking behavior.\nTo address these challenges, we propose SPike-Aware Consistency Enhancement\n(SPACE), the first source-free and single-instance TTA method specifically\ndesigned for SNNs. SPACE leverages the inherent spike dynamics of SNNs to\nmaximize the consistency of spike-behavior-based local feature maps across\naugmented versions of a single test sample, enabling robust adaptation without\nrequiring source data. We evaluate SPACE on multiple datasets. Furthermore,\nSPACE exhibits robust generalization across diverse network architectures,\nconsistently enhancing the performance of SNNs on CNNs (such as VGG and\nResNet), Transformer models, and ConvLSTM architectures. Experimental results\nshow that SPACE outperforms state-of-the-art methods, highlighting its\neffectiveness and robustness in real-world settings.\n", "link": "http://arxiv.org/abs/2504.02298v2", "date": "2025-05-29", "relevancy": 2.3137, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4703}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4663}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPACE%3A%20SPike-Aware%20Consistency%20Enhancement%20for%20Test-Time%20Adaptation%20in%0A%20%20Spiking%20Neural%20Networks&body=Title%3A%20SPACE%3A%20SPike-Aware%20Consistency%20Enhancement%20for%20Test-Time%20Adaptation%20in%0A%20%20Spiking%20Neural%20Networks%0AAuthor%3A%20Xinyu%20Luo%20and%20Kecheng%20Chen%20and%20Pao-Sheng%20Vincent%20Sun%20and%20Chris%20Xing%20Tian%20and%20Arindam%20Basu%20and%20Haoliang%20Li%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%20as%20a%20biologically%20plausible%20alternative%20to%0AArtificial%20Neural%20Networks%20%28ANNs%29%2C%20have%20demonstrated%20advantages%20in%20terms%20of%0Aenergy%20efficiency%2C%20temporal%20processing%2C%20and%20biological%20plausibility.%20However%2C%0ASNNs%20are%20highly%20sensitive%20to%20distribution%20shifts%2C%20which%20can%20significantly%0Adegrade%20their%20performance%20in%20real-world%20scenarios.%20Traditional%20test-time%0Aadaptation%20%28TTA%29%20methods%20designed%20for%20ANNs%20often%20fail%20to%20address%20the%20unique%0Acomputational%20dynamics%20of%20SNNs%2C%20such%20as%20sparsity%20and%20temporal%20spiking%20behavior.%0ATo%20address%20these%20challenges%2C%20we%20propose%20SPike-Aware%20Consistency%20Enhancement%0A%28SPACE%29%2C%20the%20first%20source-free%20and%20single-instance%20TTA%20method%20specifically%0Adesigned%20for%20SNNs.%20SPACE%20leverages%20the%20inherent%20spike%20dynamics%20of%20SNNs%20to%0Amaximize%20the%20consistency%20of%20spike-behavior-based%20local%20feature%20maps%20across%0Aaugmented%20versions%20of%20a%20single%20test%20sample%2C%20enabling%20robust%20adaptation%20without%0Arequiring%20source%20data.%20We%20evaluate%20SPACE%20on%20multiple%20datasets.%20Furthermore%2C%0ASPACE%20exhibits%20robust%20generalization%20across%20diverse%20network%20architectures%2C%0Aconsistently%20enhancing%20the%20performance%20of%20SNNs%20on%20CNNs%20%28such%20as%20VGG%20and%0AResNet%29%2C%20Transformer%20models%2C%20and%20ConvLSTM%20architectures.%20Experimental%20results%0Ashow%20that%20SPACE%20outperforms%20state-of-the-art%20methods%2C%20highlighting%20its%0Aeffectiveness%20and%20robustness%20in%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPACE%253A%2520SPike-Aware%2520Consistency%2520Enhancement%2520for%2520Test-Time%2520Adaptation%2520in%250A%2520%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DXinyu%2520Luo%2520and%2520Kecheng%2520Chen%2520and%2520Pao-Sheng%2520Vincent%2520Sun%2520and%2520Chris%2520Xing%2520Tian%2520and%2520Arindam%2520Basu%2520and%2520Haoliang%2520Li%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%252C%2520as%2520a%2520biologically%2520plausible%2520alternative%2520to%250AArtificial%2520Neural%2520Networks%2520%2528ANNs%2529%252C%2520have%2520demonstrated%2520advantages%2520in%2520terms%2520of%250Aenergy%2520efficiency%252C%2520temporal%2520processing%252C%2520and%2520biological%2520plausibility.%2520However%252C%250ASNNs%2520are%2520highly%2520sensitive%2520to%2520distribution%2520shifts%252C%2520which%2520can%2520significantly%250Adegrade%2520their%2520performance%2520in%2520real-world%2520scenarios.%2520Traditional%2520test-time%250Aadaptation%2520%2528TTA%2529%2520methods%2520designed%2520for%2520ANNs%2520often%2520fail%2520to%2520address%2520the%2520unique%250Acomputational%2520dynamics%2520of%2520SNNs%252C%2520such%2520as%2520sparsity%2520and%2520temporal%2520spiking%2520behavior.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520SPike-Aware%2520Consistency%2520Enhancement%250A%2528SPACE%2529%252C%2520the%2520first%2520source-free%2520and%2520single-instance%2520TTA%2520method%2520specifically%250Adesigned%2520for%2520SNNs.%2520SPACE%2520leverages%2520the%2520inherent%2520spike%2520dynamics%2520of%2520SNNs%2520to%250Amaximize%2520the%2520consistency%2520of%2520spike-behavior-based%2520local%2520feature%2520maps%2520across%250Aaugmented%2520versions%2520of%2520a%2520single%2520test%2520sample%252C%2520enabling%2520robust%2520adaptation%2520without%250Arequiring%2520source%2520data.%2520We%2520evaluate%2520SPACE%2520on%2520multiple%2520datasets.%2520Furthermore%252C%250ASPACE%2520exhibits%2520robust%2520generalization%2520across%2520diverse%2520network%2520architectures%252C%250Aconsistently%2520enhancing%2520the%2520performance%2520of%2520SNNs%2520on%2520CNNs%2520%2528such%2520as%2520VGG%2520and%250AResNet%2529%252C%2520Transformer%2520models%252C%2520and%2520ConvLSTM%2520architectures.%2520Experimental%2520results%250Ashow%2520that%2520SPACE%2520outperforms%2520state-of-the-art%2520methods%252C%2520highlighting%2520its%250Aeffectiveness%2520and%2520robustness%2520in%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPACE%3A%20SPike-Aware%20Consistency%20Enhancement%20for%20Test-Time%20Adaptation%20in%0A%20%20Spiking%20Neural%20Networks&entry.906535625=Xinyu%20Luo%20and%20Kecheng%20Chen%20and%20Pao-Sheng%20Vincent%20Sun%20and%20Chris%20Xing%20Tian%20and%20Arindam%20Basu%20and%20Haoliang%20Li&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%20as%20a%20biologically%20plausible%20alternative%20to%0AArtificial%20Neural%20Networks%20%28ANNs%29%2C%20have%20demonstrated%20advantages%20in%20terms%20of%0Aenergy%20efficiency%2C%20temporal%20processing%2C%20and%20biological%20plausibility.%20However%2C%0ASNNs%20are%20highly%20sensitive%20to%20distribution%20shifts%2C%20which%20can%20significantly%0Adegrade%20their%20performance%20in%20real-world%20scenarios.%20Traditional%20test-time%0Aadaptation%20%28TTA%29%20methods%20designed%20for%20ANNs%20often%20fail%20to%20address%20the%20unique%0Acomputational%20dynamics%20of%20SNNs%2C%20such%20as%20sparsity%20and%20temporal%20spiking%20behavior.%0ATo%20address%20these%20challenges%2C%20we%20propose%20SPike-Aware%20Consistency%20Enhancement%0A%28SPACE%29%2C%20the%20first%20source-free%20and%20single-instance%20TTA%20method%20specifically%0Adesigned%20for%20SNNs.%20SPACE%20leverages%20the%20inherent%20spike%20dynamics%20of%20SNNs%20to%0Amaximize%20the%20consistency%20of%20spike-behavior-based%20local%20feature%20maps%20across%0Aaugmented%20versions%20of%20a%20single%20test%20sample%2C%20enabling%20robust%20adaptation%20without%0Arequiring%20source%20data.%20We%20evaluate%20SPACE%20on%20multiple%20datasets.%20Furthermore%2C%0ASPACE%20exhibits%20robust%20generalization%20across%20diverse%20network%20architectures%2C%0Aconsistently%20enhancing%20the%20performance%20of%20SNNs%20on%20CNNs%20%28such%20as%20VGG%20and%0AResNet%29%2C%20Transformer%20models%2C%20and%20ConvLSTM%20architectures.%20Experimental%20results%0Ashow%20that%20SPACE%20outperforms%20state-of-the-art%20methods%2C%20highlighting%20its%0Aeffectiveness%20and%20robustness%20in%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02298v2&entry.124074799=Read"},
{"title": "Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree\n  Crown Semantic Segmentation", "author": "Georgios Voulgaris", "abstract": "  The accurate semantic segmentation of tree crowns within remotely sensed data\nis crucial for scientific endeavours such as forest management, biodiversity\nstudies, and carbon sequestration quantification. However, precise segmentation\nremains challenging due to complexities in the forest canopy, including\nshadows, intricate backgrounds, scale variations, and subtle spectral\ndifferences among tree species. Compared to the traditional methods, Deep\nLearning models improve accuracy by extracting informative and discriminative\nfeatures, but often fall short in capturing the aforementioned complexities.\n  To address these challenges, we propose PerceptiveNet, a novel model\nincorporating a Logarithmic Gabor-parameterised convolutional layer with\ntrainable filter parameters, alongside a backbone that extracts salient\nfeatures while capturing extensive context and spatial information through a\nwider receptive field. We investigate the impact of Log-Gabor, Gabor, and\nstandard convolutional layers on semantic segmentation performance through\nextensive experimentation. Additionally, we conduct an ablation study to assess\nthe contributions of individual layers and their combinations to overall model\nperformance, and we evaluate PerceptiveNet as a backbone within a novel hybrid\nCNN-Transformer model. Our results outperform state-of-the-art models,\ndemonstrating significant performance improvements on a tree crown dataset\nwhile generalising across domains, including two benchmark aerial scene\nsemantic segmentation datasets with varying complexities.\n", "link": "http://arxiv.org/abs/2505.23597v1", "date": "2025-05-29", "relevancy": 2.3071, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Classical%20and%20Modern%20Computer%20Vision%3A%20PerceptiveNet%20for%20Tree%0A%20%20Crown%20Semantic%20Segmentation&body=Title%3A%20Bridging%20Classical%20and%20Modern%20Computer%20Vision%3A%20PerceptiveNet%20for%20Tree%0A%20%20Crown%20Semantic%20Segmentation%0AAuthor%3A%20Georgios%20Voulgaris%0AAbstract%3A%20%20%20The%20accurate%20semantic%20segmentation%20of%20tree%20crowns%20within%20remotely%20sensed%20data%0Ais%20crucial%20for%20scientific%20endeavours%20such%20as%20forest%20management%2C%20biodiversity%0Astudies%2C%20and%20carbon%20sequestration%20quantification.%20However%2C%20precise%20segmentation%0Aremains%20challenging%20due%20to%20complexities%20in%20the%20forest%20canopy%2C%20including%0Ashadows%2C%20intricate%20backgrounds%2C%20scale%20variations%2C%20and%20subtle%20spectral%0Adifferences%20among%20tree%20species.%20Compared%20to%20the%20traditional%20methods%2C%20Deep%0ALearning%20models%20improve%20accuracy%20by%20extracting%20informative%20and%20discriminative%0Afeatures%2C%20but%20often%20fall%20short%20in%20capturing%20the%20aforementioned%20complexities.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20PerceptiveNet%2C%20a%20novel%20model%0Aincorporating%20a%20Logarithmic%20Gabor-parameterised%20convolutional%20layer%20with%0Atrainable%20filter%20parameters%2C%20alongside%20a%20backbone%20that%20extracts%20salient%0Afeatures%20while%20capturing%20extensive%20context%20and%20spatial%20information%20through%20a%0Awider%20receptive%20field.%20We%20investigate%20the%20impact%20of%20Log-Gabor%2C%20Gabor%2C%20and%0Astandard%20convolutional%20layers%20on%20semantic%20segmentation%20performance%20through%0Aextensive%20experimentation.%20Additionally%2C%20we%20conduct%20an%20ablation%20study%20to%20assess%0Athe%20contributions%20of%20individual%20layers%20and%20their%20combinations%20to%20overall%20model%0Aperformance%2C%20and%20we%20evaluate%20PerceptiveNet%20as%20a%20backbone%20within%20a%20novel%20hybrid%0ACNN-Transformer%20model.%20Our%20results%20outperform%20state-of-the-art%20models%2C%0Ademonstrating%20significant%20performance%20improvements%20on%20a%20tree%20crown%20dataset%0Awhile%20generalising%20across%20domains%2C%20including%20two%20benchmark%20aerial%20scene%0Asemantic%20segmentation%20datasets%20with%20varying%20complexities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Classical%2520and%2520Modern%2520Computer%2520Vision%253A%2520PerceptiveNet%2520for%2520Tree%250A%2520%2520Crown%2520Semantic%2520Segmentation%26entry.906535625%3DGeorgios%2520Voulgaris%26entry.1292438233%3D%2520%2520The%2520accurate%2520semantic%2520segmentation%2520of%2520tree%2520crowns%2520within%2520remotely%2520sensed%2520data%250Ais%2520crucial%2520for%2520scientific%2520endeavours%2520such%2520as%2520forest%2520management%252C%2520biodiversity%250Astudies%252C%2520and%2520carbon%2520sequestration%2520quantification.%2520However%252C%2520precise%2520segmentation%250Aremains%2520challenging%2520due%2520to%2520complexities%2520in%2520the%2520forest%2520canopy%252C%2520including%250Ashadows%252C%2520intricate%2520backgrounds%252C%2520scale%2520variations%252C%2520and%2520subtle%2520spectral%250Adifferences%2520among%2520tree%2520species.%2520Compared%2520to%2520the%2520traditional%2520methods%252C%2520Deep%250ALearning%2520models%2520improve%2520accuracy%2520by%2520extracting%2520informative%2520and%2520discriminative%250Afeatures%252C%2520but%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520aforementioned%2520complexities.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520PerceptiveNet%252C%2520a%2520novel%2520model%250Aincorporating%2520a%2520Logarithmic%2520Gabor-parameterised%2520convolutional%2520layer%2520with%250Atrainable%2520filter%2520parameters%252C%2520alongside%2520a%2520backbone%2520that%2520extracts%2520salient%250Afeatures%2520while%2520capturing%2520extensive%2520context%2520and%2520spatial%2520information%2520through%2520a%250Awider%2520receptive%2520field.%2520We%2520investigate%2520the%2520impact%2520of%2520Log-Gabor%252C%2520Gabor%252C%2520and%250Astandard%2520convolutional%2520layers%2520on%2520semantic%2520segmentation%2520performance%2520through%250Aextensive%2520experimentation.%2520Additionally%252C%2520we%2520conduct%2520an%2520ablation%2520study%2520to%2520assess%250Athe%2520contributions%2520of%2520individual%2520layers%2520and%2520their%2520combinations%2520to%2520overall%2520model%250Aperformance%252C%2520and%2520we%2520evaluate%2520PerceptiveNet%2520as%2520a%2520backbone%2520within%2520a%2520novel%2520hybrid%250ACNN-Transformer%2520model.%2520Our%2520results%2520outperform%2520state-of-the-art%2520models%252C%250Ademonstrating%2520significant%2520performance%2520improvements%2520on%2520a%2520tree%2520crown%2520dataset%250Awhile%2520generalising%2520across%2520domains%252C%2520including%2520two%2520benchmark%2520aerial%2520scene%250Asemantic%2520segmentation%2520datasets%2520with%2520varying%2520complexities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Classical%20and%20Modern%20Computer%20Vision%3A%20PerceptiveNet%20for%20Tree%0A%20%20Crown%20Semantic%20Segmentation&entry.906535625=Georgios%20Voulgaris&entry.1292438233=%20%20The%20accurate%20semantic%20segmentation%20of%20tree%20crowns%20within%20remotely%20sensed%20data%0Ais%20crucial%20for%20scientific%20endeavours%20such%20as%20forest%20management%2C%20biodiversity%0Astudies%2C%20and%20carbon%20sequestration%20quantification.%20However%2C%20precise%20segmentation%0Aremains%20challenging%20due%20to%20complexities%20in%20the%20forest%20canopy%2C%20including%0Ashadows%2C%20intricate%20backgrounds%2C%20scale%20variations%2C%20and%20subtle%20spectral%0Adifferences%20among%20tree%20species.%20Compared%20to%20the%20traditional%20methods%2C%20Deep%0ALearning%20models%20improve%20accuracy%20by%20extracting%20informative%20and%20discriminative%0Afeatures%2C%20but%20often%20fall%20short%20in%20capturing%20the%20aforementioned%20complexities.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20PerceptiveNet%2C%20a%20novel%20model%0Aincorporating%20a%20Logarithmic%20Gabor-parameterised%20convolutional%20layer%20with%0Atrainable%20filter%20parameters%2C%20alongside%20a%20backbone%20that%20extracts%20salient%0Afeatures%20while%20capturing%20extensive%20context%20and%20spatial%20information%20through%20a%0Awider%20receptive%20field.%20We%20investigate%20the%20impact%20of%20Log-Gabor%2C%20Gabor%2C%20and%0Astandard%20convolutional%20layers%20on%20semantic%20segmentation%20performance%20through%0Aextensive%20experimentation.%20Additionally%2C%20we%20conduct%20an%20ablation%20study%20to%20assess%0Athe%20contributions%20of%20individual%20layers%20and%20their%20combinations%20to%20overall%20model%0Aperformance%2C%20and%20we%20evaluate%20PerceptiveNet%20as%20a%20backbone%20within%20a%20novel%20hybrid%0ACNN-Transformer%20model.%20Our%20results%20outperform%20state-of-the-art%20models%2C%0Ademonstrating%20significant%20performance%20improvements%20on%20a%20tree%20crown%20dataset%0Awhile%20generalising%20across%20domains%2C%20including%20two%20benchmark%20aerial%20scene%0Asemantic%20segmentation%20datasets%20with%20varying%20complexities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23597v1&entry.124074799=Read"},
{"title": "SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal\n  Instance Segmentation of Cluttered Tabletop Scenes", "author": "Zhili Ng and Haozhe Wang and Zhengshen Zhang and Francis Tay Eng Hock and Marcelo H. Ang Jr", "abstract": "  In this work, we present SynTable, a unified and flexible Python-based\ndataset generator built using NVIDIA's Isaac Sim Replicator Composer for\ngenerating high-quality synthetic datasets for unseen object amodal instance\nsegmentation of cluttered tabletop scenes. Our dataset generation tool can\nrender complex 3D scenes containing object meshes, materials, textures,\nlighting, and backgrounds. Metadata, such as modal and amodal instance\nsegmentation masks, object amodal RGBA instances, occlusion masks, depth maps,\nbounding boxes, and material properties can be automatically generated to\nannotate the scene according to the users' requirements. Our tool eliminates\nthe need for manual labeling in the dataset generation process while ensuring\nthe quality and accuracy of the dataset. In this work, we discuss our design\ngoals, framework architecture, and the performance of our tool. We demonstrate\nthe use of a sample dataset generated using SynTable for training a\nstate-of-the-art model, UOAIS-Net. Our state-of-the-art results show\nsignificantly improved performance in Sim-to-Real transfer when evaluated on\nthe OSD-Amodal dataset. We offer this tool as an open-source, easy-to-use,\nphotorealistic dataset generator for advancing research in deep learning and\nsynthetic data generation. The links to our source code, demonstration video,\nand sample dataset can be found in the supplementary materials.\n", "link": "http://arxiv.org/abs/2307.07333v3", "date": "2025-05-29", "relevancy": 2.3029, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5841}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5741}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynTable%3A%20A%20Synthetic%20Data%20Generation%20Pipeline%20for%20Unseen%20Object%20Amodal%0A%20%20Instance%20Segmentation%20of%20Cluttered%20Tabletop%20Scenes&body=Title%3A%20SynTable%3A%20A%20Synthetic%20Data%20Generation%20Pipeline%20for%20Unseen%20Object%20Amodal%0A%20%20Instance%20Segmentation%20of%20Cluttered%20Tabletop%20Scenes%0AAuthor%3A%20Zhili%20Ng%20and%20Haozhe%20Wang%20and%20Zhengshen%20Zhang%20and%20Francis%20Tay%20Eng%20Hock%20and%20Marcelo%20H.%20Ang%20Jr%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20SynTable%2C%20a%20unified%20and%20flexible%20Python-based%0Adataset%20generator%20built%20using%20NVIDIA%27s%20Isaac%20Sim%20Replicator%20Composer%20for%0Agenerating%20high-quality%20synthetic%20datasets%20for%20unseen%20object%20amodal%20instance%0Asegmentation%20of%20cluttered%20tabletop%20scenes.%20Our%20dataset%20generation%20tool%20can%0Arender%20complex%203D%20scenes%20containing%20object%20meshes%2C%20materials%2C%20textures%2C%0Alighting%2C%20and%20backgrounds.%20Metadata%2C%20such%20as%20modal%20and%20amodal%20instance%0Asegmentation%20masks%2C%20object%20amodal%20RGBA%20instances%2C%20occlusion%20masks%2C%20depth%20maps%2C%0Abounding%20boxes%2C%20and%20material%20properties%20can%20be%20automatically%20generated%20to%0Aannotate%20the%20scene%20according%20to%20the%20users%27%20requirements.%20Our%20tool%20eliminates%0Athe%20need%20for%20manual%20labeling%20in%20the%20dataset%20generation%20process%20while%20ensuring%0Athe%20quality%20and%20accuracy%20of%20the%20dataset.%20In%20this%20work%2C%20we%20discuss%20our%20design%0Agoals%2C%20framework%20architecture%2C%20and%20the%20performance%20of%20our%20tool.%20We%20demonstrate%0Athe%20use%20of%20a%20sample%20dataset%20generated%20using%20SynTable%20for%20training%20a%0Astate-of-the-art%20model%2C%20UOAIS-Net.%20Our%20state-of-the-art%20results%20show%0Asignificantly%20improved%20performance%20in%20Sim-to-Real%20transfer%20when%20evaluated%20on%0Athe%20OSD-Amodal%20dataset.%20We%20offer%20this%20tool%20as%20an%20open-source%2C%20easy-to-use%2C%0Aphotorealistic%20dataset%20generator%20for%20advancing%20research%20in%20deep%20learning%20and%0Asynthetic%20data%20generation.%20The%20links%20to%20our%20source%20code%2C%20demonstration%20video%2C%0Aand%20sample%20dataset%20can%20be%20found%20in%20the%20supplementary%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.07333v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynTable%253A%2520A%2520Synthetic%2520Data%2520Generation%2520Pipeline%2520for%2520Unseen%2520Object%2520Amodal%250A%2520%2520Instance%2520Segmentation%2520of%2520Cluttered%2520Tabletop%2520Scenes%26entry.906535625%3DZhili%2520Ng%2520and%2520Haozhe%2520Wang%2520and%2520Zhengshen%2520Zhang%2520and%2520Francis%2520Tay%2520Eng%2520Hock%2520and%2520Marcelo%2520H.%2520Ang%2520Jr%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520SynTable%252C%2520a%2520unified%2520and%2520flexible%2520Python-based%250Adataset%2520generator%2520built%2520using%2520NVIDIA%2527s%2520Isaac%2520Sim%2520Replicator%2520Composer%2520for%250Agenerating%2520high-quality%2520synthetic%2520datasets%2520for%2520unseen%2520object%2520amodal%2520instance%250Asegmentation%2520of%2520cluttered%2520tabletop%2520scenes.%2520Our%2520dataset%2520generation%2520tool%2520can%250Arender%2520complex%25203D%2520scenes%2520containing%2520object%2520meshes%252C%2520materials%252C%2520textures%252C%250Alighting%252C%2520and%2520backgrounds.%2520Metadata%252C%2520such%2520as%2520modal%2520and%2520amodal%2520instance%250Asegmentation%2520masks%252C%2520object%2520amodal%2520RGBA%2520instances%252C%2520occlusion%2520masks%252C%2520depth%2520maps%252C%250Abounding%2520boxes%252C%2520and%2520material%2520properties%2520can%2520be%2520automatically%2520generated%2520to%250Aannotate%2520the%2520scene%2520according%2520to%2520the%2520users%2527%2520requirements.%2520Our%2520tool%2520eliminates%250Athe%2520need%2520for%2520manual%2520labeling%2520in%2520the%2520dataset%2520generation%2520process%2520while%2520ensuring%250Athe%2520quality%2520and%2520accuracy%2520of%2520the%2520dataset.%2520In%2520this%2520work%252C%2520we%2520discuss%2520our%2520design%250Agoals%252C%2520framework%2520architecture%252C%2520and%2520the%2520performance%2520of%2520our%2520tool.%2520We%2520demonstrate%250Athe%2520use%2520of%2520a%2520sample%2520dataset%2520generated%2520using%2520SynTable%2520for%2520training%2520a%250Astate-of-the-art%2520model%252C%2520UOAIS-Net.%2520Our%2520state-of-the-art%2520results%2520show%250Asignificantly%2520improved%2520performance%2520in%2520Sim-to-Real%2520transfer%2520when%2520evaluated%2520on%250Athe%2520OSD-Amodal%2520dataset.%2520We%2520offer%2520this%2520tool%2520as%2520an%2520open-source%252C%2520easy-to-use%252C%250Aphotorealistic%2520dataset%2520generator%2520for%2520advancing%2520research%2520in%2520deep%2520learning%2520and%250Asynthetic%2520data%2520generation.%2520The%2520links%2520to%2520our%2520source%2520code%252C%2520demonstration%2520video%252C%250Aand%2520sample%2520dataset%2520can%2520be%2520found%2520in%2520the%2520supplementary%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.07333v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynTable%3A%20A%20Synthetic%20Data%20Generation%20Pipeline%20for%20Unseen%20Object%20Amodal%0A%20%20Instance%20Segmentation%20of%20Cluttered%20Tabletop%20Scenes&entry.906535625=Zhili%20Ng%20and%20Haozhe%20Wang%20and%20Zhengshen%20Zhang%20and%20Francis%20Tay%20Eng%20Hock%20and%20Marcelo%20H.%20Ang%20Jr&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20SynTable%2C%20a%20unified%20and%20flexible%20Python-based%0Adataset%20generator%20built%20using%20NVIDIA%27s%20Isaac%20Sim%20Replicator%20Composer%20for%0Agenerating%20high-quality%20synthetic%20datasets%20for%20unseen%20object%20amodal%20instance%0Asegmentation%20of%20cluttered%20tabletop%20scenes.%20Our%20dataset%20generation%20tool%20can%0Arender%20complex%203D%20scenes%20containing%20object%20meshes%2C%20materials%2C%20textures%2C%0Alighting%2C%20and%20backgrounds.%20Metadata%2C%20such%20as%20modal%20and%20amodal%20instance%0Asegmentation%20masks%2C%20object%20amodal%20RGBA%20instances%2C%20occlusion%20masks%2C%20depth%20maps%2C%0Abounding%20boxes%2C%20and%20material%20properties%20can%20be%20automatically%20generated%20to%0Aannotate%20the%20scene%20according%20to%20the%20users%27%20requirements.%20Our%20tool%20eliminates%0Athe%20need%20for%20manual%20labeling%20in%20the%20dataset%20generation%20process%20while%20ensuring%0Athe%20quality%20and%20accuracy%20of%20the%20dataset.%20In%20this%20work%2C%20we%20discuss%20our%20design%0Agoals%2C%20framework%20architecture%2C%20and%20the%20performance%20of%20our%20tool.%20We%20demonstrate%0Athe%20use%20of%20a%20sample%20dataset%20generated%20using%20SynTable%20for%20training%20a%0Astate-of-the-art%20model%2C%20UOAIS-Net.%20Our%20state-of-the-art%20results%20show%0Asignificantly%20improved%20performance%20in%20Sim-to-Real%20transfer%20when%20evaluated%20on%0Athe%20OSD-Amodal%20dataset.%20We%20offer%20this%20tool%20as%20an%20open-source%2C%20easy-to-use%2C%0Aphotorealistic%20dataset%20generator%20for%20advancing%20research%20in%20deep%20learning%20and%0Asynthetic%20data%20generation.%20The%20links%20to%20our%20source%20code%2C%20demonstration%20video%2C%0Aand%20sample%20dataset%20can%20be%20found%20in%20the%20supplementary%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.07333v3&entry.124074799=Read"},
{"title": "Foundation Model Hidden Representations for Heart Rate Estimation from\n  Auscultation", "author": "Jingping Nie and Dung T. Tran and Karan Thakkar and Vasudha Kowtha and Jon Huang and Carlos Avendano and Erdrin Azemi and Vikramjit Mitra", "abstract": "  Auscultation, particularly heart sound, is a non-invasive technique that\nprovides essential vital sign information. Recently, self-supervised acoustic\nrepresentation foundation models (FMs) have been proposed to offer insights\ninto acoustics-based vital signs. However, there has been little exploration of\nthe extent to which auscultation is encoded in these pre-trained FM\nrepresentations. In this work, using a publicly available phonocardiogram (PCG)\ndataset and a heart rate (HR) estimation model, we conduct a layer-wise\ninvestigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,\nWhisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP\nmodel. Additionally, we implement the baseline method from Nie et al., 2024\n(which relies on acoustic features) and show that overall, representation\nvectors from pre-trained foundation models (FMs) offer comparable performance\nto the baseline. Notably, HR estimation using the representations from the\naudio encoder of the in-house CLAP model outperforms the results obtained from\nthe baseline, achieving a lower mean absolute error (MAE) across various\ntrain/validation/test splits despite the domain mismatch.\n", "link": "http://arxiv.org/abs/2505.20745v2", "date": "2025-05-29", "relevancy": 2.3009, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Model%20Hidden%20Representations%20for%20Heart%20Rate%20Estimation%20from%0A%20%20Auscultation&body=Title%3A%20Foundation%20Model%20Hidden%20Representations%20for%20Heart%20Rate%20Estimation%20from%0A%20%20Auscultation%0AAuthor%3A%20Jingping%20Nie%20and%20Dung%20T.%20Tran%20and%20Karan%20Thakkar%20and%20Vasudha%20Kowtha%20and%20Jon%20Huang%20and%20Carlos%20Avendano%20and%20Erdrin%20Azemi%20and%20Vikramjit%20Mitra%0AAbstract%3A%20%20%20Auscultation%2C%20particularly%20heart%20sound%2C%20is%20a%20non-invasive%20technique%20that%0Aprovides%20essential%20vital%20sign%20information.%20Recently%2C%20self-supervised%20acoustic%0Arepresentation%20foundation%20models%20%28FMs%29%20have%20been%20proposed%20to%20offer%20insights%0Ainto%20acoustics-based%20vital%20signs.%20However%2C%20there%20has%20been%20little%20exploration%20of%0Athe%20extent%20to%20which%20auscultation%20is%20encoded%20in%20these%20pre-trained%20FM%0Arepresentations.%20In%20this%20work%2C%20using%20a%20publicly%20available%20phonocardiogram%20%28PCG%29%0Adataset%20and%20a%20heart%20rate%20%28HR%29%20estimation%20model%2C%20we%20conduct%20a%20layer-wise%0Ainvestigation%20of%20six%20acoustic%20representation%20FMs%3A%20HuBERT%2C%20wav2vec2%2C%20wavLM%2C%0AWhisper%2C%20Contrastive%20Language-Audio%20Pretraining%20%28CLAP%29%2C%20and%20an%20in-house%20CLAP%0Amodel.%20Additionally%2C%20we%20implement%20the%20baseline%20method%20from%20Nie%20et%20al.%2C%202024%0A%28which%20relies%20on%20acoustic%20features%29%20and%20show%20that%20overall%2C%20representation%0Avectors%20from%20pre-trained%20foundation%20models%20%28FMs%29%20offer%20comparable%20performance%0Ato%20the%20baseline.%20Notably%2C%20HR%20estimation%20using%20the%20representations%20from%20the%0Aaudio%20encoder%20of%20the%20in-house%20CLAP%20model%20outperforms%20the%20results%20obtained%20from%0Athe%20baseline%2C%20achieving%20a%20lower%20mean%20absolute%20error%20%28MAE%29%20across%20various%0Atrain/validation/test%20splits%20despite%20the%20domain%20mismatch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Model%2520Hidden%2520Representations%2520for%2520Heart%2520Rate%2520Estimation%2520from%250A%2520%2520Auscultation%26entry.906535625%3DJingping%2520Nie%2520and%2520Dung%2520T.%2520Tran%2520and%2520Karan%2520Thakkar%2520and%2520Vasudha%2520Kowtha%2520and%2520Jon%2520Huang%2520and%2520Carlos%2520Avendano%2520and%2520Erdrin%2520Azemi%2520and%2520Vikramjit%2520Mitra%26entry.1292438233%3D%2520%2520Auscultation%252C%2520particularly%2520heart%2520sound%252C%2520is%2520a%2520non-invasive%2520technique%2520that%250Aprovides%2520essential%2520vital%2520sign%2520information.%2520Recently%252C%2520self-supervised%2520acoustic%250Arepresentation%2520foundation%2520models%2520%2528FMs%2529%2520have%2520been%2520proposed%2520to%2520offer%2520insights%250Ainto%2520acoustics-based%2520vital%2520signs.%2520However%252C%2520there%2520has%2520been%2520little%2520exploration%2520of%250Athe%2520extent%2520to%2520which%2520auscultation%2520is%2520encoded%2520in%2520these%2520pre-trained%2520FM%250Arepresentations.%2520In%2520this%2520work%252C%2520using%2520a%2520publicly%2520available%2520phonocardiogram%2520%2528PCG%2529%250Adataset%2520and%2520a%2520heart%2520rate%2520%2528HR%2529%2520estimation%2520model%252C%2520we%2520conduct%2520a%2520layer-wise%250Ainvestigation%2520of%2520six%2520acoustic%2520representation%2520FMs%253A%2520HuBERT%252C%2520wav2vec2%252C%2520wavLM%252C%250AWhisper%252C%2520Contrastive%2520Language-Audio%2520Pretraining%2520%2528CLAP%2529%252C%2520and%2520an%2520in-house%2520CLAP%250Amodel.%2520Additionally%252C%2520we%2520implement%2520the%2520baseline%2520method%2520from%2520Nie%2520et%2520al.%252C%25202024%250A%2528which%2520relies%2520on%2520acoustic%2520features%2529%2520and%2520show%2520that%2520overall%252C%2520representation%250Avectors%2520from%2520pre-trained%2520foundation%2520models%2520%2528FMs%2529%2520offer%2520comparable%2520performance%250Ato%2520the%2520baseline.%2520Notably%252C%2520HR%2520estimation%2520using%2520the%2520representations%2520from%2520the%250Aaudio%2520encoder%2520of%2520the%2520in-house%2520CLAP%2520model%2520outperforms%2520the%2520results%2520obtained%2520from%250Athe%2520baseline%252C%2520achieving%2520a%2520lower%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520across%2520various%250Atrain/validation/test%2520splits%2520despite%2520the%2520domain%2520mismatch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Model%20Hidden%20Representations%20for%20Heart%20Rate%20Estimation%20from%0A%20%20Auscultation&entry.906535625=Jingping%20Nie%20and%20Dung%20T.%20Tran%20and%20Karan%20Thakkar%20and%20Vasudha%20Kowtha%20and%20Jon%20Huang%20and%20Carlos%20Avendano%20and%20Erdrin%20Azemi%20and%20Vikramjit%20Mitra&entry.1292438233=%20%20Auscultation%2C%20particularly%20heart%20sound%2C%20is%20a%20non-invasive%20technique%20that%0Aprovides%20essential%20vital%20sign%20information.%20Recently%2C%20self-supervised%20acoustic%0Arepresentation%20foundation%20models%20%28FMs%29%20have%20been%20proposed%20to%20offer%20insights%0Ainto%20acoustics-based%20vital%20signs.%20However%2C%20there%20has%20been%20little%20exploration%20of%0Athe%20extent%20to%20which%20auscultation%20is%20encoded%20in%20these%20pre-trained%20FM%0Arepresentations.%20In%20this%20work%2C%20using%20a%20publicly%20available%20phonocardiogram%20%28PCG%29%0Adataset%20and%20a%20heart%20rate%20%28HR%29%20estimation%20model%2C%20we%20conduct%20a%20layer-wise%0Ainvestigation%20of%20six%20acoustic%20representation%20FMs%3A%20HuBERT%2C%20wav2vec2%2C%20wavLM%2C%0AWhisper%2C%20Contrastive%20Language-Audio%20Pretraining%20%28CLAP%29%2C%20and%20an%20in-house%20CLAP%0Amodel.%20Additionally%2C%20we%20implement%20the%20baseline%20method%20from%20Nie%20et%20al.%2C%202024%0A%28which%20relies%20on%20acoustic%20features%29%20and%20show%20that%20overall%2C%20representation%0Avectors%20from%20pre-trained%20foundation%20models%20%28FMs%29%20offer%20comparable%20performance%0Ato%20the%20baseline.%20Notably%2C%20HR%20estimation%20using%20the%20representations%20from%20the%0Aaudio%20encoder%20of%20the%20in-house%20CLAP%20model%20outperforms%20the%20results%20obtained%20from%0Athe%20baseline%2C%20achieving%20a%20lower%20mean%20absolute%20error%20%28MAE%29%20across%20various%0Atrain/validation/test%20splits%20despite%20the%20domain%20mismatch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20745v2&entry.124074799=Read"},
{"title": "CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical\n  Modeling for Cryo-EM Synthesis", "author": "Runmin Jiang and Genpei Zhang and Yuntian Yang and Siqi Wu and Yuheng Zhang and Wanyue Feng and Yizhou Zhao and Xi Xiao and Xiao Wang and Tianyang Wang and Xingjian Li and Min Xu", "abstract": "  Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of\nmacromolecules, but developing robust models for downstream analysis is\nhindered by the scarcity of high-quality annotated data. While synthetic data\ngeneration has emerged as a potential solution, existing methods often fail to\ncapture both the structural diversity of biological specimens and the complex,\nspatially varying noise inherent in cryo-EM imaging. To overcome these\nlimitations, we propose CryoCCD, a synthesis framework that integrates\nbiophysical modeling with generative techniques. Specifically, CryoCCD produces\nmulti-scale cryo-EM micrographs that reflect realistic biophysical variability\nthrough compositional heterogeneity, cellular context, and physics-informed\nimaging. To generate realistic noise, we employ a conditional diffusion model,\nenhanced by cycle consistency to preserve structural fidelity and mask-aware\ncontrastive learning to capture spatially adaptive noise patterns. Extensive\nexperiments show that CryoCCD generates structurally accurate micrographs and\nenhances performance in downstream tasks, outperforming state-of-the-art\nbaselines in both particle picking and reconstruction.\n", "link": "http://arxiv.org/abs/2505.23444v1", "date": "2025-05-29", "relevancy": 2.2943, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.578}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.578}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CryoCCD%3A%20Conditional%20Cycle-consistent%20Diffusion%20with%20Biophysical%0A%20%20Modeling%20for%20Cryo-EM%20Synthesis&body=Title%3A%20CryoCCD%3A%20Conditional%20Cycle-consistent%20Diffusion%20with%20Biophysical%0A%20%20Modeling%20for%20Cryo-EM%20Synthesis%0AAuthor%3A%20Runmin%20Jiang%20and%20Genpei%20Zhang%20and%20Yuntian%20Yang%20and%20Siqi%20Wu%20and%20Yuheng%20Zhang%20and%20Wanyue%20Feng%20and%20Yizhou%20Zhao%20and%20Xi%20Xiao%20and%20Xiao%20Wang%20and%20Tianyang%20Wang%20and%20Xingjian%20Li%20and%20Min%20Xu%0AAbstract%3A%20%20%20Cryo-electron%20microscopy%20%28cryo-EM%29%20offers%20near-atomic%20resolution%20imaging%20of%0Amacromolecules%2C%20but%20developing%20robust%20models%20for%20downstream%20analysis%20is%0Ahindered%20by%20the%20scarcity%20of%20high-quality%20annotated%20data.%20While%20synthetic%20data%0Ageneration%20has%20emerged%20as%20a%20potential%20solution%2C%20existing%20methods%20often%20fail%20to%0Acapture%20both%20the%20structural%20diversity%20of%20biological%20specimens%20and%20the%20complex%2C%0Aspatially%20varying%20noise%20inherent%20in%20cryo-EM%20imaging.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20CryoCCD%2C%20a%20synthesis%20framework%20that%20integrates%0Abiophysical%20modeling%20with%20generative%20techniques.%20Specifically%2C%20CryoCCD%20produces%0Amulti-scale%20cryo-EM%20micrographs%20that%20reflect%20realistic%20biophysical%20variability%0Athrough%20compositional%20heterogeneity%2C%20cellular%20context%2C%20and%20physics-informed%0Aimaging.%20To%20generate%20realistic%20noise%2C%20we%20employ%20a%20conditional%20diffusion%20model%2C%0Aenhanced%20by%20cycle%20consistency%20to%20preserve%20structural%20fidelity%20and%20mask-aware%0Acontrastive%20learning%20to%20capture%20spatially%20adaptive%20noise%20patterns.%20Extensive%0Aexperiments%20show%20that%20CryoCCD%20generates%20structurally%20accurate%20micrographs%20and%0Aenhances%20performance%20in%20downstream%20tasks%2C%20outperforming%20state-of-the-art%0Abaselines%20in%20both%20particle%20picking%20and%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCryoCCD%253A%2520Conditional%2520Cycle-consistent%2520Diffusion%2520with%2520Biophysical%250A%2520%2520Modeling%2520for%2520Cryo-EM%2520Synthesis%26entry.906535625%3DRunmin%2520Jiang%2520and%2520Genpei%2520Zhang%2520and%2520Yuntian%2520Yang%2520and%2520Siqi%2520Wu%2520and%2520Yuheng%2520Zhang%2520and%2520Wanyue%2520Feng%2520and%2520Yizhou%2520Zhao%2520and%2520Xi%2520Xiao%2520and%2520Xiao%2520Wang%2520and%2520Tianyang%2520Wang%2520and%2520Xingjian%2520Li%2520and%2520Min%2520Xu%26entry.1292438233%3D%2520%2520Cryo-electron%2520microscopy%2520%2528cryo-EM%2529%2520offers%2520near-atomic%2520resolution%2520imaging%2520of%250Amacromolecules%252C%2520but%2520developing%2520robust%2520models%2520for%2520downstream%2520analysis%2520is%250Ahindered%2520by%2520the%2520scarcity%2520of%2520high-quality%2520annotated%2520data.%2520While%2520synthetic%2520data%250Ageneration%2520has%2520emerged%2520as%2520a%2520potential%2520solution%252C%2520existing%2520methods%2520often%2520fail%2520to%250Acapture%2520both%2520the%2520structural%2520diversity%2520of%2520biological%2520specimens%2520and%2520the%2520complex%252C%250Aspatially%2520varying%2520noise%2520inherent%2520in%2520cryo-EM%2520imaging.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520CryoCCD%252C%2520a%2520synthesis%2520framework%2520that%2520integrates%250Abiophysical%2520modeling%2520with%2520generative%2520techniques.%2520Specifically%252C%2520CryoCCD%2520produces%250Amulti-scale%2520cryo-EM%2520micrographs%2520that%2520reflect%2520realistic%2520biophysical%2520variability%250Athrough%2520compositional%2520heterogeneity%252C%2520cellular%2520context%252C%2520and%2520physics-informed%250Aimaging.%2520To%2520generate%2520realistic%2520noise%252C%2520we%2520employ%2520a%2520conditional%2520diffusion%2520model%252C%250Aenhanced%2520by%2520cycle%2520consistency%2520to%2520preserve%2520structural%2520fidelity%2520and%2520mask-aware%250Acontrastive%2520learning%2520to%2520capture%2520spatially%2520adaptive%2520noise%2520patterns.%2520Extensive%250Aexperiments%2520show%2520that%2520CryoCCD%2520generates%2520structurally%2520accurate%2520micrographs%2520and%250Aenhances%2520performance%2520in%2520downstream%2520tasks%252C%2520outperforming%2520state-of-the-art%250Abaselines%2520in%2520both%2520particle%2520picking%2520and%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CryoCCD%3A%20Conditional%20Cycle-consistent%20Diffusion%20with%20Biophysical%0A%20%20Modeling%20for%20Cryo-EM%20Synthesis&entry.906535625=Runmin%20Jiang%20and%20Genpei%20Zhang%20and%20Yuntian%20Yang%20and%20Siqi%20Wu%20and%20Yuheng%20Zhang%20and%20Wanyue%20Feng%20and%20Yizhou%20Zhao%20and%20Xi%20Xiao%20and%20Xiao%20Wang%20and%20Tianyang%20Wang%20and%20Xingjian%20Li%20and%20Min%20Xu&entry.1292438233=%20%20Cryo-electron%20microscopy%20%28cryo-EM%29%20offers%20near-atomic%20resolution%20imaging%20of%0Amacromolecules%2C%20but%20developing%20robust%20models%20for%20downstream%20analysis%20is%0Ahindered%20by%20the%20scarcity%20of%20high-quality%20annotated%20data.%20While%20synthetic%20data%0Ageneration%20has%20emerged%20as%20a%20potential%20solution%2C%20existing%20methods%20often%20fail%20to%0Acapture%20both%20the%20structural%20diversity%20of%20biological%20specimens%20and%20the%20complex%2C%0Aspatially%20varying%20noise%20inherent%20in%20cryo-EM%20imaging.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20CryoCCD%2C%20a%20synthesis%20framework%20that%20integrates%0Abiophysical%20modeling%20with%20generative%20techniques.%20Specifically%2C%20CryoCCD%20produces%0Amulti-scale%20cryo-EM%20micrographs%20that%20reflect%20realistic%20biophysical%20variability%0Athrough%20compositional%20heterogeneity%2C%20cellular%20context%2C%20and%20physics-informed%0Aimaging.%20To%20generate%20realistic%20noise%2C%20we%20employ%20a%20conditional%20diffusion%20model%2C%0Aenhanced%20by%20cycle%20consistency%20to%20preserve%20structural%20fidelity%20and%20mask-aware%0Acontrastive%20learning%20to%20capture%20spatially%20adaptive%20noise%20patterns.%20Extensive%0Aexperiments%20show%20that%20CryoCCD%20generates%20structurally%20accurate%20micrographs%20and%0Aenhances%20performance%20in%20downstream%20tasks%2C%20outperforming%20state-of-the-art%0Abaselines%20in%20both%20particle%20picking%20and%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23444v1&entry.124074799=Read"},
{"title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for\n  Complex Visual Reasoning", "author": "Jusheng Zhang and Yijia Fan and Wenjun Lin and Ruiqi Chen and Haoyi Jiang and Wenhao Chai and Jian Wang and Keze Wang", "abstract": "  We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing\nvision-language reasoning. Unlike prior single-agent or monolithic models,\nGAM-Agent formulates the reasoning process as a non-zero-sum game between base\nagents--each specializing in visual perception subtasks--and a critical agent\nthat verifies logic consistency and factual correctness. Agents communicate via\nstructured claims, evidence, and uncertainty estimates. The framework\nintroduces an uncertainty-aware controller to dynamically adjust agent\ncollaboration, triggering multi-round debates when disagreement or ambiguity is\ndetected. This process yields more robust and interpretable predictions.\nExperiments on four challenging benchmarks--MMMU, MMBench, MVBench, and\nV*Bench--demonstrate that GAM-Agent significantly improves performance across\nvarious VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid\nscale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\\%, and still enhances\nstrong models like GPT-4o by up to 2--3\\%. Our approach is modular, scalable,\nand generalizable, offering a path toward reliable and explainable multi-agent\nmultimodal reasoning.\n", "link": "http://arxiv.org/abs/2505.23399v1", "date": "2025-05-29", "relevancy": 2.286, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5937}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5697}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAM-Agent%3A%20Game-Theoretic%20and%20Uncertainty-Aware%20Collaboration%20for%0A%20%20Complex%20Visual%20Reasoning&body=Title%3A%20GAM-Agent%3A%20Game-Theoretic%20and%20Uncertainty-Aware%20Collaboration%20for%0A%20%20Complex%20Visual%20Reasoning%0AAuthor%3A%20Jusheng%20Zhang%20and%20Yijia%20Fan%20and%20Wenjun%20Lin%20and%20Ruiqi%20Chen%20and%20Haoyi%20Jiang%20and%20Wenhao%20Chai%20and%20Jian%20Wang%20and%20Keze%20Wang%0AAbstract%3A%20%20%20We%20propose%20GAM-Agent%2C%20a%20game-theoretic%20multi-agent%20framework%20for%20enhancing%0Avision-language%20reasoning.%20Unlike%20prior%20single-agent%20or%20monolithic%20models%2C%0AGAM-Agent%20formulates%20the%20reasoning%20process%20as%20a%20non-zero-sum%20game%20between%20base%0Aagents--each%20specializing%20in%20visual%20perception%20subtasks--and%20a%20critical%20agent%0Athat%20verifies%20logic%20consistency%20and%20factual%20correctness.%20Agents%20communicate%20via%0Astructured%20claims%2C%20evidence%2C%20and%20uncertainty%20estimates.%20The%20framework%0Aintroduces%20an%20uncertainty-aware%20controller%20to%20dynamically%20adjust%20agent%0Acollaboration%2C%20triggering%20multi-round%20debates%20when%20disagreement%20or%20ambiguity%20is%0Adetected.%20This%20process%20yields%20more%20robust%20and%20interpretable%20predictions.%0AExperiments%20on%20four%20challenging%20benchmarks--MMMU%2C%20MMBench%2C%20MVBench%2C%20and%0AV%2ABench--demonstrate%20that%20GAM-Agent%20significantly%20improves%20performance%20across%0Avarious%20VLM%20backbones.%20Notably%2C%20GAM-Agent%20boosts%20the%20accuracy%20of%20small-to-mid%0Ascale%20models%20%28e.g.%2C%20Qwen2.5-VL-7B%2C%20InternVL3-14B%29%20by%205--6%5C%25%2C%20and%20still%20enhances%0Astrong%20models%20like%20GPT-4o%20by%20up%20to%202--3%5C%25.%20Our%20approach%20is%20modular%2C%20scalable%2C%0Aand%20generalizable%2C%20offering%20a%20path%20toward%20reliable%20and%20explainable%20multi-agent%0Amultimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAM-Agent%253A%2520Game-Theoretic%2520and%2520Uncertainty-Aware%2520Collaboration%2520for%250A%2520%2520Complex%2520Visual%2520Reasoning%26entry.906535625%3DJusheng%2520Zhang%2520and%2520Yijia%2520Fan%2520and%2520Wenjun%2520Lin%2520and%2520Ruiqi%2520Chen%2520and%2520Haoyi%2520Jiang%2520and%2520Wenhao%2520Chai%2520and%2520Jian%2520Wang%2520and%2520Keze%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520GAM-Agent%252C%2520a%2520game-theoretic%2520multi-agent%2520framework%2520for%2520enhancing%250Avision-language%2520reasoning.%2520Unlike%2520prior%2520single-agent%2520or%2520monolithic%2520models%252C%250AGAM-Agent%2520formulates%2520the%2520reasoning%2520process%2520as%2520a%2520non-zero-sum%2520game%2520between%2520base%250Aagents--each%2520specializing%2520in%2520visual%2520perception%2520subtasks--and%2520a%2520critical%2520agent%250Athat%2520verifies%2520logic%2520consistency%2520and%2520factual%2520correctness.%2520Agents%2520communicate%2520via%250Astructured%2520claims%252C%2520evidence%252C%2520and%2520uncertainty%2520estimates.%2520The%2520framework%250Aintroduces%2520an%2520uncertainty-aware%2520controller%2520to%2520dynamically%2520adjust%2520agent%250Acollaboration%252C%2520triggering%2520multi-round%2520debates%2520when%2520disagreement%2520or%2520ambiguity%2520is%250Adetected.%2520This%2520process%2520yields%2520more%2520robust%2520and%2520interpretable%2520predictions.%250AExperiments%2520on%2520four%2520challenging%2520benchmarks--MMMU%252C%2520MMBench%252C%2520MVBench%252C%2520and%250AV%252ABench--demonstrate%2520that%2520GAM-Agent%2520significantly%2520improves%2520performance%2520across%250Avarious%2520VLM%2520backbones.%2520Notably%252C%2520GAM-Agent%2520boosts%2520the%2520accuracy%2520of%2520small-to-mid%250Ascale%2520models%2520%2528e.g.%252C%2520Qwen2.5-VL-7B%252C%2520InternVL3-14B%2529%2520by%25205--6%255C%2525%252C%2520and%2520still%2520enhances%250Astrong%2520models%2520like%2520GPT-4o%2520by%2520up%2520to%25202--3%255C%2525.%2520Our%2520approach%2520is%2520modular%252C%2520scalable%252C%250Aand%2520generalizable%252C%2520offering%2520a%2520path%2520toward%2520reliable%2520and%2520explainable%2520multi-agent%250Amultimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAM-Agent%3A%20Game-Theoretic%20and%20Uncertainty-Aware%20Collaboration%20for%0A%20%20Complex%20Visual%20Reasoning&entry.906535625=Jusheng%20Zhang%20and%20Yijia%20Fan%20and%20Wenjun%20Lin%20and%20Ruiqi%20Chen%20and%20Haoyi%20Jiang%20and%20Wenhao%20Chai%20and%20Jian%20Wang%20and%20Keze%20Wang&entry.1292438233=%20%20We%20propose%20GAM-Agent%2C%20a%20game-theoretic%20multi-agent%20framework%20for%20enhancing%0Avision-language%20reasoning.%20Unlike%20prior%20single-agent%20or%20monolithic%20models%2C%0AGAM-Agent%20formulates%20the%20reasoning%20process%20as%20a%20non-zero-sum%20game%20between%20base%0Aagents--each%20specializing%20in%20visual%20perception%20subtasks--and%20a%20critical%20agent%0Athat%20verifies%20logic%20consistency%20and%20factual%20correctness.%20Agents%20communicate%20via%0Astructured%20claims%2C%20evidence%2C%20and%20uncertainty%20estimates.%20The%20framework%0Aintroduces%20an%20uncertainty-aware%20controller%20to%20dynamically%20adjust%20agent%0Acollaboration%2C%20triggering%20multi-round%20debates%20when%20disagreement%20or%20ambiguity%20is%0Adetected.%20This%20process%20yields%20more%20robust%20and%20interpretable%20predictions.%0AExperiments%20on%20four%20challenging%20benchmarks--MMMU%2C%20MMBench%2C%20MVBench%2C%20and%0AV%2ABench--demonstrate%20that%20GAM-Agent%20significantly%20improves%20performance%20across%0Avarious%20VLM%20backbones.%20Notably%2C%20GAM-Agent%20boosts%20the%20accuracy%20of%20small-to-mid%0Ascale%20models%20%28e.g.%2C%20Qwen2.5-VL-7B%2C%20InternVL3-14B%29%20by%205--6%5C%25%2C%20and%20still%20enhances%0Astrong%20models%20like%20GPT-4o%20by%20up%20to%202--3%5C%25.%20Our%20approach%20is%20modular%2C%20scalable%2C%0Aand%20generalizable%2C%20offering%20a%20path%20toward%20reliable%20and%20explainable%20multi-agent%0Amultimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23399v1&entry.124074799=Read"},
{"title": "Dynamic Spectral Backpropagation for Efficient Neural Network Training", "author": "Mannmohan Muthuraman", "abstract": "  Dynamic Spectral Backpropagation (DSBP) enhances neural network training\nunder resource constraints by projecting gradients onto principal eigenvectors,\nreducing complexity and promoting flat minima. Five extensions are proposed,\ndynamic spectral inference, spectral architecture optimization, spectral meta\nlearning, spectral transfer regularization, and Lie algebra inspired dynamics,\nto address challenges in robustness, fewshot learning, and hardware efficiency.\nSupported by a third order stochastic differential equation (SDE) and a PAC\nBayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank\nAdaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion\nMNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive\nexperiments and visualizations. Future work focuses on scalability, bias\nmitigation, and ethical considerations.\n", "link": "http://arxiv.org/abs/2505.23369v1", "date": "2025-05-29", "relevancy": 2.2772, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6212}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5589}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Spectral%20Backpropagation%20for%20Efficient%20Neural%20Network%20Training&body=Title%3A%20Dynamic%20Spectral%20Backpropagation%20for%20Efficient%20Neural%20Network%20Training%0AAuthor%3A%20Mannmohan%20Muthuraman%0AAbstract%3A%20%20%20Dynamic%20Spectral%20Backpropagation%20%28DSBP%29%20enhances%20neural%20network%20training%0Aunder%20resource%20constraints%20by%20projecting%20gradients%20onto%20principal%20eigenvectors%2C%0Areducing%20complexity%20and%20promoting%20flat%20minima.%20Five%20extensions%20are%20proposed%2C%0Adynamic%20spectral%20inference%2C%20spectral%20architecture%20optimization%2C%20spectral%20meta%0Alearning%2C%20spectral%20transfer%20regularization%2C%20and%20Lie%20algebra%20inspired%20dynamics%2C%0Ato%20address%20challenges%20in%20robustness%2C%20fewshot%20learning%2C%20and%20hardware%20efficiency.%0ASupported%20by%20a%20third%20order%20stochastic%20differential%20equation%20%28SDE%29%20and%20a%20PAC%0ABayes%20limit%2C%20DSBP%20outperforms%20Sharpness%20Aware%20Minimization%20%28SAM%29%2C%20Low%20Rank%0AAdaptation%20%28LoRA%29%2C%20and%20Model%20Agnostic%20Meta%20Learning%20%28MAML%29%20on%20CIFAR%2010%2C%20Fashion%0AMNIST%2C%20MedMNIST%2C%20and%20Tiny%20ImageNet%2C%20as%20demonstrated%20through%20extensive%0Aexperiments%20and%20visualizations.%20Future%20work%20focuses%20on%20scalability%2C%20bias%0Amitigation%2C%20and%20ethical%20considerations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Spectral%2520Backpropagation%2520for%2520Efficient%2520Neural%2520Network%2520Training%26entry.906535625%3DMannmohan%2520Muthuraman%26entry.1292438233%3D%2520%2520Dynamic%2520Spectral%2520Backpropagation%2520%2528DSBP%2529%2520enhances%2520neural%2520network%2520training%250Aunder%2520resource%2520constraints%2520by%2520projecting%2520gradients%2520onto%2520principal%2520eigenvectors%252C%250Areducing%2520complexity%2520and%2520promoting%2520flat%2520minima.%2520Five%2520extensions%2520are%2520proposed%252C%250Adynamic%2520spectral%2520inference%252C%2520spectral%2520architecture%2520optimization%252C%2520spectral%2520meta%250Alearning%252C%2520spectral%2520transfer%2520regularization%252C%2520and%2520Lie%2520algebra%2520inspired%2520dynamics%252C%250Ato%2520address%2520challenges%2520in%2520robustness%252C%2520fewshot%2520learning%252C%2520and%2520hardware%2520efficiency.%250ASupported%2520by%2520a%2520third%2520order%2520stochastic%2520differential%2520equation%2520%2528SDE%2529%2520and%2520a%2520PAC%250ABayes%2520limit%252C%2520DSBP%2520outperforms%2520Sharpness%2520Aware%2520Minimization%2520%2528SAM%2529%252C%2520Low%2520Rank%250AAdaptation%2520%2528LoRA%2529%252C%2520and%2520Model%2520Agnostic%2520Meta%2520Learning%2520%2528MAML%2529%2520on%2520CIFAR%252010%252C%2520Fashion%250AMNIST%252C%2520MedMNIST%252C%2520and%2520Tiny%2520ImageNet%252C%2520as%2520demonstrated%2520through%2520extensive%250Aexperiments%2520and%2520visualizations.%2520Future%2520work%2520focuses%2520on%2520scalability%252C%2520bias%250Amitigation%252C%2520and%2520ethical%2520considerations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Spectral%20Backpropagation%20for%20Efficient%20Neural%20Network%20Training&entry.906535625=Mannmohan%20Muthuraman&entry.1292438233=%20%20Dynamic%20Spectral%20Backpropagation%20%28DSBP%29%20enhances%20neural%20network%20training%0Aunder%20resource%20constraints%20by%20projecting%20gradients%20onto%20principal%20eigenvectors%2C%0Areducing%20complexity%20and%20promoting%20flat%20minima.%20Five%20extensions%20are%20proposed%2C%0Adynamic%20spectral%20inference%2C%20spectral%20architecture%20optimization%2C%20spectral%20meta%0Alearning%2C%20spectral%20transfer%20regularization%2C%20and%20Lie%20algebra%20inspired%20dynamics%2C%0Ato%20address%20challenges%20in%20robustness%2C%20fewshot%20learning%2C%20and%20hardware%20efficiency.%0ASupported%20by%20a%20third%20order%20stochastic%20differential%20equation%20%28SDE%29%20and%20a%20PAC%0ABayes%20limit%2C%20DSBP%20outperforms%20Sharpness%20Aware%20Minimization%20%28SAM%29%2C%20Low%20Rank%0AAdaptation%20%28LoRA%29%2C%20and%20Model%20Agnostic%20Meta%20Learning%20%28MAML%29%20on%20CIFAR%2010%2C%20Fashion%0AMNIST%2C%20MedMNIST%2C%20and%20Tiny%20ImageNet%2C%20as%20demonstrated%20through%20extensive%0Aexperiments%20and%20visualizations.%20Future%20work%20focuses%20on%20scalability%2C%20bias%0Amitigation%2C%20and%20ethical%20considerations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23369v1&entry.124074799=Read"},
{"title": "Learning Cascade Ranking as One Network", "author": "Yunli Wang and Zhen Zhang and Zhiqiang Wang and Zixuan Yang and Yu Li and Jian Yang and Shiyang Wen and Peng Jiang and Kun Gai", "abstract": "  Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances have introduced interaction-aware training paradigms,\nbut still struggle to 1) align training objectives with the goal of the entire\ncascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn\neffective collaboration patterns for different stages. To address these\nchallenges, we propose LCRON, which introduces a novel surrogate loss function\nderived from the lower bound probability that ground truth items are selected\nby cascade ranking, ensuring alignment with the overall objective of the\nsystem. According to the properties of the derived bound, we further design an\nauxiliary loss for each stage to drive the reduction of this bound, leading to\na more robust and effective top-k selection. LCRON enables end-to-end training\nof the entire cascade ranking system as a unified network. Experimental results\ndemonstrate that LCRON achieves significant improvement over existing methods\non public benchmarks and industrial applications, addressing key limitations in\ncascade ranking training and significantly enhancing system performance.\n", "link": "http://arxiv.org/abs/2503.09492v2", "date": "2025-05-29", "relevancy": 2.277, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4742}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Cascade%20Ranking%20as%20One%20Network&body=Title%3A%20Learning%20Cascade%20Ranking%20as%20One%20Network%0AAuthor%3A%20Yunli%20Wang%20and%20Zhen%20Zhang%20and%20Zhiqiang%20Wang%20and%20Zixuan%20Yang%20and%20Yu%20Li%20and%20Jian%20Yang%20and%20Shiyang%20Wen%20and%20Peng%20Jiang%20and%20Kun%20Gai%0AAbstract%3A%20%20%20Cascade%20Ranking%20is%20a%20prevalent%20architecture%20in%20large-scale%20top-k%20selection%0Asystems%20like%20recommendation%20and%20advertising%20platforms.%20Traditional%20training%0Amethods%20focus%20on%20single-stage%20optimization%2C%20neglecting%20interactions%20between%0Astages.%20Recent%20advances%20have%20introduced%20interaction-aware%20training%20paradigms%2C%0Abut%20still%20struggle%20to%201%29%20align%20training%20objectives%20with%20the%20goal%20of%20the%20entire%0Acascade%20ranking%20%28i.e.%2C%20end-to-end%20recall%20of%20ground-truth%20items%29%20and%202%29%20learn%0Aeffective%20collaboration%20patterns%20for%20different%20stages.%20To%20address%20these%0Achallenges%2C%20we%20propose%20LCRON%2C%20which%20introduces%20a%20novel%20surrogate%20loss%20function%0Aderived%20from%20the%20lower%20bound%20probability%20that%20ground%20truth%20items%20are%20selected%0Aby%20cascade%20ranking%2C%20ensuring%20alignment%20with%20the%20overall%20objective%20of%20the%0Asystem.%20According%20to%20the%20properties%20of%20the%20derived%20bound%2C%20we%20further%20design%20an%0Aauxiliary%20loss%20for%20each%20stage%20to%20drive%20the%20reduction%20of%20this%20bound%2C%20leading%20to%0Aa%20more%20robust%20and%20effective%20top-k%20selection.%20LCRON%20enables%20end-to-end%20training%0Aof%20the%20entire%20cascade%20ranking%20system%20as%20a%20unified%20network.%20Experimental%20results%0Ademonstrate%20that%20LCRON%20achieves%20significant%20improvement%20over%20existing%20methods%0Aon%20public%20benchmarks%20and%20industrial%20applications%2C%20addressing%20key%20limitations%20in%0Acascade%20ranking%20training%20and%20significantly%20enhancing%20system%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Cascade%2520Ranking%2520as%2520One%2520Network%26entry.906535625%3DYunli%2520Wang%2520and%2520Zhen%2520Zhang%2520and%2520Zhiqiang%2520Wang%2520and%2520Zixuan%2520Yang%2520and%2520Yu%2520Li%2520and%2520Jian%2520Yang%2520and%2520Shiyang%2520Wen%2520and%2520Peng%2520Jiang%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520Cascade%2520Ranking%2520is%2520a%2520prevalent%2520architecture%2520in%2520large-scale%2520top-k%2520selection%250Asystems%2520like%2520recommendation%2520and%2520advertising%2520platforms.%2520Traditional%2520training%250Amethods%2520focus%2520on%2520single-stage%2520optimization%252C%2520neglecting%2520interactions%2520between%250Astages.%2520Recent%2520advances%2520have%2520introduced%2520interaction-aware%2520training%2520paradigms%252C%250Abut%2520still%2520struggle%2520to%25201%2529%2520align%2520training%2520objectives%2520with%2520the%2520goal%2520of%2520the%2520entire%250Acascade%2520ranking%2520%2528i.e.%252C%2520end-to-end%2520recall%2520of%2520ground-truth%2520items%2529%2520and%25202%2529%2520learn%250Aeffective%2520collaboration%2520patterns%2520for%2520different%2520stages.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520LCRON%252C%2520which%2520introduces%2520a%2520novel%2520surrogate%2520loss%2520function%250Aderived%2520from%2520the%2520lower%2520bound%2520probability%2520that%2520ground%2520truth%2520items%2520are%2520selected%250Aby%2520cascade%2520ranking%252C%2520ensuring%2520alignment%2520with%2520the%2520overall%2520objective%2520of%2520the%250Asystem.%2520According%2520to%2520the%2520properties%2520of%2520the%2520derived%2520bound%252C%2520we%2520further%2520design%2520an%250Aauxiliary%2520loss%2520for%2520each%2520stage%2520to%2520drive%2520the%2520reduction%2520of%2520this%2520bound%252C%2520leading%2520to%250Aa%2520more%2520robust%2520and%2520effective%2520top-k%2520selection.%2520LCRON%2520enables%2520end-to-end%2520training%250Aof%2520the%2520entire%2520cascade%2520ranking%2520system%2520as%2520a%2520unified%2520network.%2520Experimental%2520results%250Ademonstrate%2520that%2520LCRON%2520achieves%2520significant%2520improvement%2520over%2520existing%2520methods%250Aon%2520public%2520benchmarks%2520and%2520industrial%2520applications%252C%2520addressing%2520key%2520limitations%2520in%250Acascade%2520ranking%2520training%2520and%2520significantly%2520enhancing%2520system%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Cascade%20Ranking%20as%20One%20Network&entry.906535625=Yunli%20Wang%20and%20Zhen%20Zhang%20and%20Zhiqiang%20Wang%20and%20Zixuan%20Yang%20and%20Yu%20Li%20and%20Jian%20Yang%20and%20Shiyang%20Wen%20and%20Peng%20Jiang%20and%20Kun%20Gai&entry.1292438233=%20%20Cascade%20Ranking%20is%20a%20prevalent%20architecture%20in%20large-scale%20top-k%20selection%0Asystems%20like%20recommendation%20and%20advertising%20platforms.%20Traditional%20training%0Amethods%20focus%20on%20single-stage%20optimization%2C%20neglecting%20interactions%20between%0Astages.%20Recent%20advances%20have%20introduced%20interaction-aware%20training%20paradigms%2C%0Abut%20still%20struggle%20to%201%29%20align%20training%20objectives%20with%20the%20goal%20of%20the%20entire%0Acascade%20ranking%20%28i.e.%2C%20end-to-end%20recall%20of%20ground-truth%20items%29%20and%202%29%20learn%0Aeffective%20collaboration%20patterns%20for%20different%20stages.%20To%20address%20these%0Achallenges%2C%20we%20propose%20LCRON%2C%20which%20introduces%20a%20novel%20surrogate%20loss%20function%0Aderived%20from%20the%20lower%20bound%20probability%20that%20ground%20truth%20items%20are%20selected%0Aby%20cascade%20ranking%2C%20ensuring%20alignment%20with%20the%20overall%20objective%20of%20the%0Asystem.%20According%20to%20the%20properties%20of%20the%20derived%20bound%2C%20we%20further%20design%20an%0Aauxiliary%20loss%20for%20each%20stage%20to%20drive%20the%20reduction%20of%20this%20bound%2C%20leading%20to%0Aa%20more%20robust%20and%20effective%20top-k%20selection.%20LCRON%20enables%20end-to-end%20training%0Aof%20the%20entire%20cascade%20ranking%20system%20as%20a%20unified%20network.%20Experimental%20results%0Ademonstrate%20that%20LCRON%20achieves%20significant%20improvement%20over%20existing%20methods%0Aon%20public%20benchmarks%20and%20industrial%20applications%2C%20addressing%20key%20limitations%20in%0Acascade%20ranking%20training%20and%20significantly%20enhancing%20system%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09492v2&entry.124074799=Read"},
{"title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run\n  Fast, Generalize Better", "author": "Danny Driess and Jost Tobias Springenberg and Brian Ichter and Lili Yu and Adrian Li-Bell and Karl Pertsch and Allen Z. Ren and Homer Walke and Quan Vuong and Lucy Xiaoyang Shi and Sergey Levine", "abstract": "  Vision-language-action (VLA) models provide a powerful approach to training\ncontrol policies for physical systems, such as robots, by combining end-to-end\nlearning with transfer of semantic knowledge from web-scale vision-language\nmodel (VLM) training. However, the constraints of real-time control are often\nat odds with the design of VLMs: the most powerful VLMs have tens or hundreds\nof billions of parameters, presenting an obstacle to real-time inference, and\noperate on discrete tokens rather than the continuous-valued outputs that are\nrequired for controlling robots. To address this challenge, recent VLA models\nhave used specialized modules for efficient continuous control, such as action\nexperts or continuous output heads, which typically require adding new\nuntrained parameters to the pretrained VLM backbone. While these modules\nimprove real-time and control capabilities, it remains an open question whether\nthey preserve or degrade the semantic knowledge contained in the pretrained\nVLM, and what effect they have on the VLA training dynamics. In this paper, we\nstudy this question in the context of VLAs that include a continuous diffusion\nor flow matching action expert, showing that naively including such experts\nsignificantly harms both training speed and knowledge transfer. We provide an\nextensive analysis of various design choices, their impact on performance and\nknowledge transfer, and propose a technique for insulating the VLM backbone\nduring VLA training that mitigates this issue. Videos are available at\nhttps://pi.website/research/knowledge_insulation.\n", "link": "http://arxiv.org/abs/2505.23705v1", "date": "2025-05-29", "relevancy": 2.2685, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Insulating%20Vision-Language-Action%20Models%3A%20Train%20Fast%2C%20Run%0A%20%20Fast%2C%20Generalize%20Better&body=Title%3A%20Knowledge%20Insulating%20Vision-Language-Action%20Models%3A%20Train%20Fast%2C%20Run%0A%20%20Fast%2C%20Generalize%20Better%0AAuthor%3A%20Danny%20Driess%20and%20Jost%20Tobias%20Springenberg%20and%20Brian%20Ichter%20and%20Lili%20Yu%20and%20Adrian%20Li-Bell%20and%20Karl%20Pertsch%20and%20Allen%20Z.%20Ren%20and%20Homer%20Walke%20and%20Quan%20Vuong%20and%20Lucy%20Xiaoyang%20Shi%20and%20Sergey%20Levine%0AAbstract%3A%20%20%20Vision-language-action%20%28VLA%29%20models%20provide%20a%20powerful%20approach%20to%20training%0Acontrol%20policies%20for%20physical%20systems%2C%20such%20as%20robots%2C%20by%20combining%20end-to-end%0Alearning%20with%20transfer%20of%20semantic%20knowledge%20from%20web-scale%20vision-language%0Amodel%20%28VLM%29%20training.%20However%2C%20the%20constraints%20of%20real-time%20control%20are%20often%0Aat%20odds%20with%20the%20design%20of%20VLMs%3A%20the%20most%20powerful%20VLMs%20have%20tens%20or%20hundreds%0Aof%20billions%20of%20parameters%2C%20presenting%20an%20obstacle%20to%20real-time%20inference%2C%20and%0Aoperate%20on%20discrete%20tokens%20rather%20than%20the%20continuous-valued%20outputs%20that%20are%0Arequired%20for%20controlling%20robots.%20To%20address%20this%20challenge%2C%20recent%20VLA%20models%0Ahave%20used%20specialized%20modules%20for%20efficient%20continuous%20control%2C%20such%20as%20action%0Aexperts%20or%20continuous%20output%20heads%2C%20which%20typically%20require%20adding%20new%0Auntrained%20parameters%20to%20the%20pretrained%20VLM%20backbone.%20While%20these%20modules%0Aimprove%20real-time%20and%20control%20capabilities%2C%20it%20remains%20an%20open%20question%20whether%0Athey%20preserve%20or%20degrade%20the%20semantic%20knowledge%20contained%20in%20the%20pretrained%0AVLM%2C%20and%20what%20effect%20they%20have%20on%20the%20VLA%20training%20dynamics.%20In%20this%20paper%2C%20we%0Astudy%20this%20question%20in%20the%20context%20of%20VLAs%20that%20include%20a%20continuous%20diffusion%0Aor%20flow%20matching%20action%20expert%2C%20showing%20that%20naively%20including%20such%20experts%0Asignificantly%20harms%20both%20training%20speed%20and%20knowledge%20transfer.%20We%20provide%20an%0Aextensive%20analysis%20of%20various%20design%20choices%2C%20their%20impact%20on%20performance%20and%0Aknowledge%20transfer%2C%20and%20propose%20a%20technique%20for%20insulating%20the%20VLM%20backbone%0Aduring%20VLA%20training%20that%20mitigates%20this%20issue.%20Videos%20are%20available%20at%0Ahttps%3A//pi.website/research/knowledge_insulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Insulating%2520Vision-Language-Action%2520Models%253A%2520Train%2520Fast%252C%2520Run%250A%2520%2520Fast%252C%2520Generalize%2520Better%26entry.906535625%3DDanny%2520Driess%2520and%2520Jost%2520Tobias%2520Springenberg%2520and%2520Brian%2520Ichter%2520and%2520Lili%2520Yu%2520and%2520Adrian%2520Li-Bell%2520and%2520Karl%2520Pertsch%2520and%2520Allen%2520Z.%2520Ren%2520and%2520Homer%2520Walke%2520and%2520Quan%2520Vuong%2520and%2520Lucy%2520Xiaoyang%2520Shi%2520and%2520Sergey%2520Levine%26entry.1292438233%3D%2520%2520Vision-language-action%2520%2528VLA%2529%2520models%2520provide%2520a%2520powerful%2520approach%2520to%2520training%250Acontrol%2520policies%2520for%2520physical%2520systems%252C%2520such%2520as%2520robots%252C%2520by%2520combining%2520end-to-end%250Alearning%2520with%2520transfer%2520of%2520semantic%2520knowledge%2520from%2520web-scale%2520vision-language%250Amodel%2520%2528VLM%2529%2520training.%2520However%252C%2520the%2520constraints%2520of%2520real-time%2520control%2520are%2520often%250Aat%2520odds%2520with%2520the%2520design%2520of%2520VLMs%253A%2520the%2520most%2520powerful%2520VLMs%2520have%2520tens%2520or%2520hundreds%250Aof%2520billions%2520of%2520parameters%252C%2520presenting%2520an%2520obstacle%2520to%2520real-time%2520inference%252C%2520and%250Aoperate%2520on%2520discrete%2520tokens%2520rather%2520than%2520the%2520continuous-valued%2520outputs%2520that%2520are%250Arequired%2520for%2520controlling%2520robots.%2520To%2520address%2520this%2520challenge%252C%2520recent%2520VLA%2520models%250Ahave%2520used%2520specialized%2520modules%2520for%2520efficient%2520continuous%2520control%252C%2520such%2520as%2520action%250Aexperts%2520or%2520continuous%2520output%2520heads%252C%2520which%2520typically%2520require%2520adding%2520new%250Auntrained%2520parameters%2520to%2520the%2520pretrained%2520VLM%2520backbone.%2520While%2520these%2520modules%250Aimprove%2520real-time%2520and%2520control%2520capabilities%252C%2520it%2520remains%2520an%2520open%2520question%2520whether%250Athey%2520preserve%2520or%2520degrade%2520the%2520semantic%2520knowledge%2520contained%2520in%2520the%2520pretrained%250AVLM%252C%2520and%2520what%2520effect%2520they%2520have%2520on%2520the%2520VLA%2520training%2520dynamics.%2520In%2520this%2520paper%252C%2520we%250Astudy%2520this%2520question%2520in%2520the%2520context%2520of%2520VLAs%2520that%2520include%2520a%2520continuous%2520diffusion%250Aor%2520flow%2520matching%2520action%2520expert%252C%2520showing%2520that%2520naively%2520including%2520such%2520experts%250Asignificantly%2520harms%2520both%2520training%2520speed%2520and%2520knowledge%2520transfer.%2520We%2520provide%2520an%250Aextensive%2520analysis%2520of%2520various%2520design%2520choices%252C%2520their%2520impact%2520on%2520performance%2520and%250Aknowledge%2520transfer%252C%2520and%2520propose%2520a%2520technique%2520for%2520insulating%2520the%2520VLM%2520backbone%250Aduring%2520VLA%2520training%2520that%2520mitigates%2520this%2520issue.%2520Videos%2520are%2520available%2520at%250Ahttps%253A//pi.website/research/knowledge_insulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Insulating%20Vision-Language-Action%20Models%3A%20Train%20Fast%2C%20Run%0A%20%20Fast%2C%20Generalize%20Better&entry.906535625=Danny%20Driess%20and%20Jost%20Tobias%20Springenberg%20and%20Brian%20Ichter%20and%20Lili%20Yu%20and%20Adrian%20Li-Bell%20and%20Karl%20Pertsch%20and%20Allen%20Z.%20Ren%20and%20Homer%20Walke%20and%20Quan%20Vuong%20and%20Lucy%20Xiaoyang%20Shi%20and%20Sergey%20Levine&entry.1292438233=%20%20Vision-language-action%20%28VLA%29%20models%20provide%20a%20powerful%20approach%20to%20training%0Acontrol%20policies%20for%20physical%20systems%2C%20such%20as%20robots%2C%20by%20combining%20end-to-end%0Alearning%20with%20transfer%20of%20semantic%20knowledge%20from%20web-scale%20vision-language%0Amodel%20%28VLM%29%20training.%20However%2C%20the%20constraints%20of%20real-time%20control%20are%20often%0Aat%20odds%20with%20the%20design%20of%20VLMs%3A%20the%20most%20powerful%20VLMs%20have%20tens%20or%20hundreds%0Aof%20billions%20of%20parameters%2C%20presenting%20an%20obstacle%20to%20real-time%20inference%2C%20and%0Aoperate%20on%20discrete%20tokens%20rather%20than%20the%20continuous-valued%20outputs%20that%20are%0Arequired%20for%20controlling%20robots.%20To%20address%20this%20challenge%2C%20recent%20VLA%20models%0Ahave%20used%20specialized%20modules%20for%20efficient%20continuous%20control%2C%20such%20as%20action%0Aexperts%20or%20continuous%20output%20heads%2C%20which%20typically%20require%20adding%20new%0Auntrained%20parameters%20to%20the%20pretrained%20VLM%20backbone.%20While%20these%20modules%0Aimprove%20real-time%20and%20control%20capabilities%2C%20it%20remains%20an%20open%20question%20whether%0Athey%20preserve%20or%20degrade%20the%20semantic%20knowledge%20contained%20in%20the%20pretrained%0AVLM%2C%20and%20what%20effect%20they%20have%20on%20the%20VLA%20training%20dynamics.%20In%20this%20paper%2C%20we%0Astudy%20this%20question%20in%20the%20context%20of%20VLAs%20that%20include%20a%20continuous%20diffusion%0Aor%20flow%20matching%20action%20expert%2C%20showing%20that%20naively%20including%20such%20experts%0Asignificantly%20harms%20both%20training%20speed%20and%20knowledge%20transfer.%20We%20provide%20an%0Aextensive%20analysis%20of%20various%20design%20choices%2C%20their%20impact%20on%20performance%20and%0Aknowledge%20transfer%2C%20and%20propose%20a%20technique%20for%20insulating%20the%20VLM%20backbone%0Aduring%20VLA%20training%20that%20mitigates%20this%20issue.%20Videos%20are%20available%20at%0Ahttps%3A//pi.website/research/knowledge_insulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23705v1&entry.124074799=Read"},
{"title": "Semantics-Aware Human Motion Generation from Audio Instructions", "author": "Zi-An Wang and Shihao Zou and Shiyao Yu and Mingyuan Zhang and Chao Dong", "abstract": "  Recent advances in interactive technologies have highlighted the prominence\nof audio signals for semantic encoding. This paper explores a new task, where\naudio signals are used as conditioning inputs to generate motions that align\nwith the semantics of the audio. Unlike text-based interactions, audio provides\na more natural and intuitive communication method. However, existing methods\ntypically focus on matching motions with music or speech rhythms, which often\nresults in a weak connection between the semantics of the audio and generated\nmotions. We propose an end-to-end framework using a masked generative\ntransformer, enhanced by a memory-retrieval attention module to handle sparse\nand lengthy audio inputs. Additionally, we enrich existing datasets by\nconverting descriptions into conversational style and generating corresponding\naudio with varied speaker identities. Experiments demonstrate the effectiveness\nand efficiency of the proposed framework, demonstrating that audio instructions\ncan convey semantics similar to text while providing more practical and\nuser-friendly interactions.\n", "link": "http://arxiv.org/abs/2505.23465v1", "date": "2025-05-29", "relevancy": 2.2681, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5919}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5538}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantics-Aware%20Human%20Motion%20Generation%20from%20Audio%20Instructions&body=Title%3A%20Semantics-Aware%20Human%20Motion%20Generation%20from%20Audio%20Instructions%0AAuthor%3A%20Zi-An%20Wang%20and%20Shihao%20Zou%20and%20Shiyao%20Yu%20and%20Mingyuan%20Zhang%20and%20Chao%20Dong%0AAbstract%3A%20%20%20Recent%20advances%20in%20interactive%20technologies%20have%20highlighted%20the%20prominence%0Aof%20audio%20signals%20for%20semantic%20encoding.%20This%20paper%20explores%20a%20new%20task%2C%20where%0Aaudio%20signals%20are%20used%20as%20conditioning%20inputs%20to%20generate%20motions%20that%20align%0Awith%20the%20semantics%20of%20the%20audio.%20Unlike%20text-based%20interactions%2C%20audio%20provides%0Aa%20more%20natural%20and%20intuitive%20communication%20method.%20However%2C%20existing%20methods%0Atypically%20focus%20on%20matching%20motions%20with%20music%20or%20speech%20rhythms%2C%20which%20often%0Aresults%20in%20a%20weak%20connection%20between%20the%20semantics%20of%20the%20audio%20and%20generated%0Amotions.%20We%20propose%20an%20end-to-end%20framework%20using%20a%20masked%20generative%0Atransformer%2C%20enhanced%20by%20a%20memory-retrieval%20attention%20module%20to%20handle%20sparse%0Aand%20lengthy%20audio%20inputs.%20Additionally%2C%20we%20enrich%20existing%20datasets%20by%0Aconverting%20descriptions%20into%20conversational%20style%20and%20generating%20corresponding%0Aaudio%20with%20varied%20speaker%20identities.%20Experiments%20demonstrate%20the%20effectiveness%0Aand%20efficiency%20of%20the%20proposed%20framework%2C%20demonstrating%20that%20audio%20instructions%0Acan%20convey%20semantics%20similar%20to%20text%20while%20providing%20more%20practical%20and%0Auser-friendly%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantics-Aware%2520Human%2520Motion%2520Generation%2520from%2520Audio%2520Instructions%26entry.906535625%3DZi-An%2520Wang%2520and%2520Shihao%2520Zou%2520and%2520Shiyao%2520Yu%2520and%2520Mingyuan%2520Zhang%2520and%2520Chao%2520Dong%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520interactive%2520technologies%2520have%2520highlighted%2520the%2520prominence%250Aof%2520audio%2520signals%2520for%2520semantic%2520encoding.%2520This%2520paper%2520explores%2520a%2520new%2520task%252C%2520where%250Aaudio%2520signals%2520are%2520used%2520as%2520conditioning%2520inputs%2520to%2520generate%2520motions%2520that%2520align%250Awith%2520the%2520semantics%2520of%2520the%2520audio.%2520Unlike%2520text-based%2520interactions%252C%2520audio%2520provides%250Aa%2520more%2520natural%2520and%2520intuitive%2520communication%2520method.%2520However%252C%2520existing%2520methods%250Atypically%2520focus%2520on%2520matching%2520motions%2520with%2520music%2520or%2520speech%2520rhythms%252C%2520which%2520often%250Aresults%2520in%2520a%2520weak%2520connection%2520between%2520the%2520semantics%2520of%2520the%2520audio%2520and%2520generated%250Amotions.%2520We%2520propose%2520an%2520end-to-end%2520framework%2520using%2520a%2520masked%2520generative%250Atransformer%252C%2520enhanced%2520by%2520a%2520memory-retrieval%2520attention%2520module%2520to%2520handle%2520sparse%250Aand%2520lengthy%2520audio%2520inputs.%2520Additionally%252C%2520we%2520enrich%2520existing%2520datasets%2520by%250Aconverting%2520descriptions%2520into%2520conversational%2520style%2520and%2520generating%2520corresponding%250Aaudio%2520with%2520varied%2520speaker%2520identities.%2520Experiments%2520demonstrate%2520the%2520effectiveness%250Aand%2520efficiency%2520of%2520the%2520proposed%2520framework%252C%2520demonstrating%2520that%2520audio%2520instructions%250Acan%2520convey%2520semantics%2520similar%2520to%2520text%2520while%2520providing%2520more%2520practical%2520and%250Auser-friendly%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantics-Aware%20Human%20Motion%20Generation%20from%20Audio%20Instructions&entry.906535625=Zi-An%20Wang%20and%20Shihao%20Zou%20and%20Shiyao%20Yu%20and%20Mingyuan%20Zhang%20and%20Chao%20Dong&entry.1292438233=%20%20Recent%20advances%20in%20interactive%20technologies%20have%20highlighted%20the%20prominence%0Aof%20audio%20signals%20for%20semantic%20encoding.%20This%20paper%20explores%20a%20new%20task%2C%20where%0Aaudio%20signals%20are%20used%20as%20conditioning%20inputs%20to%20generate%20motions%20that%20align%0Awith%20the%20semantics%20of%20the%20audio.%20Unlike%20text-based%20interactions%2C%20audio%20provides%0Aa%20more%20natural%20and%20intuitive%20communication%20method.%20However%2C%20existing%20methods%0Atypically%20focus%20on%20matching%20motions%20with%20music%20or%20speech%20rhythms%2C%20which%20often%0Aresults%20in%20a%20weak%20connection%20between%20the%20semantics%20of%20the%20audio%20and%20generated%0Amotions.%20We%20propose%20an%20end-to-end%20framework%20using%20a%20masked%20generative%0Atransformer%2C%20enhanced%20by%20a%20memory-retrieval%20attention%20module%20to%20handle%20sparse%0Aand%20lengthy%20audio%20inputs.%20Additionally%2C%20we%20enrich%20existing%20datasets%20by%0Aconverting%20descriptions%20into%20conversational%20style%20and%20generating%20corresponding%0Aaudio%20with%20varied%20speaker%20identities.%20Experiments%20demonstrate%20the%20effectiveness%0Aand%20efficiency%20of%20the%20proposed%20framework%2C%20demonstrating%20that%20audio%20instructions%0Acan%20convey%20semantics%20similar%20to%20text%20while%20providing%20more%20practical%20and%0Auser-friendly%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23465v1&entry.124074799=Read"},
{"title": "Forms of Understanding for XAI-Explanations", "author": "Hendrik Buschmeier and Heike M. Buhl and Friederike Kern and Angela Grimminger and Helen Beierling and Josephine Fisher and Andr\u00e9 Gro\u00df and Ilona Horwath and Nils Klowait and Stefan Lazarov and Michael Lenke and Vivien Lohmer and Katharina Rohlfing and Ingrid Scharlau and Amit Singh and Lutz Terfloth and Anna-Lisa Vollmer and Yu Wang and Annedore Wilmes and Britta Wrede", "abstract": "  Explainability has become an important topic in computer science and\nartificial intelligence, leading to a subfield called Explainable Artificial\nIntelligence (XAI). The goal of providing or seeking explanations is to achieve\n(better) 'understanding' on the part of the explainee. However, what it means\nto 'understand' is still not clearly defined, and the concept itself is rarely\nthe subject of scientific investigation. This conceptual article aims to\npresent a model of forms of understanding for XAI-explanations and beyond. From\nan interdisciplinary perspective bringing together computer science,\nlinguistics, sociology, philosophy and psychology, a definition of\nunderstanding and its forms, assessment, and dynamics during the process of\ngiving everyday explanations are explored. Two types of understanding are\nconsidered as possible outcomes of explanations, namely enabledness, 'knowing\nhow' to do or decide something, and comprehension, 'knowing that' -- both in\ndifferent degrees (from shallow to deep). Explanations regularly start with\nshallow understanding in a specific domain and can lead to deep comprehension\nand enabledness of the explanandum, which we see as a prerequisite for human\nusers to gain agency. In this process, the increase of comprehension and\nenabledness are highly interdependent. Against the background of this\nsystematization, special challenges of understanding in XAI are discussed.\n", "link": "http://arxiv.org/abs/2311.08760v2", "date": "2025-05-29", "relevancy": 1.8705, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forms%20of%20Understanding%20for%20XAI-Explanations&body=Title%3A%20Forms%20of%20Understanding%20for%20XAI-Explanations%0AAuthor%3A%20Hendrik%20Buschmeier%20and%20Heike%20M.%20Buhl%20and%20Friederike%20Kern%20and%20Angela%20Grimminger%20and%20Helen%20Beierling%20and%20Josephine%20Fisher%20and%20Andr%C3%A9%20Gro%C3%9F%20and%20Ilona%20Horwath%20and%20Nils%20Klowait%20and%20Stefan%20Lazarov%20and%20Michael%20Lenke%20and%20Vivien%20Lohmer%20and%20Katharina%20Rohlfing%20and%20Ingrid%20Scharlau%20and%20Amit%20Singh%20and%20Lutz%20Terfloth%20and%20Anna-Lisa%20Vollmer%20and%20Yu%20Wang%20and%20Annedore%20Wilmes%20and%20Britta%20Wrede%0AAbstract%3A%20%20%20Explainability%20has%20become%20an%20important%20topic%20in%20computer%20science%20and%0Aartificial%20intelligence%2C%20leading%20to%20a%20subfield%20called%20Explainable%20Artificial%0AIntelligence%20%28XAI%29.%20The%20goal%20of%20providing%20or%20seeking%20explanations%20is%20to%20achieve%0A%28better%29%20%27understanding%27%20on%20the%20part%20of%20the%20explainee.%20However%2C%20what%20it%20means%0Ato%20%27understand%27%20is%20still%20not%20clearly%20defined%2C%20and%20the%20concept%20itself%20is%20rarely%0Athe%20subject%20of%20scientific%20investigation.%20This%20conceptual%20article%20aims%20to%0Apresent%20a%20model%20of%20forms%20of%20understanding%20for%20XAI-explanations%20and%20beyond.%20From%0Aan%20interdisciplinary%20perspective%20bringing%20together%20computer%20science%2C%0Alinguistics%2C%20sociology%2C%20philosophy%20and%20psychology%2C%20a%20definition%20of%0Aunderstanding%20and%20its%20forms%2C%20assessment%2C%20and%20dynamics%20during%20the%20process%20of%0Agiving%20everyday%20explanations%20are%20explored.%20Two%20types%20of%20understanding%20are%0Aconsidered%20as%20possible%20outcomes%20of%20explanations%2C%20namely%20enabledness%2C%20%27knowing%0Ahow%27%20to%20do%20or%20decide%20something%2C%20and%20comprehension%2C%20%27knowing%20that%27%20--%20both%20in%0Adifferent%20degrees%20%28from%20shallow%20to%20deep%29.%20Explanations%20regularly%20start%20with%0Ashallow%20understanding%20in%20a%20specific%20domain%20and%20can%20lead%20to%20deep%20comprehension%0Aand%20enabledness%20of%20the%20explanandum%2C%20which%20we%20see%20as%20a%20prerequisite%20for%20human%0Ausers%20to%20gain%20agency.%20In%20this%20process%2C%20the%20increase%20of%20comprehension%20and%0Aenabledness%20are%20highly%20interdependent.%20Against%20the%20background%20of%20this%0Asystematization%2C%20special%20challenges%20of%20understanding%20in%20XAI%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForms%2520of%2520Understanding%2520for%2520XAI-Explanations%26entry.906535625%3DHendrik%2520Buschmeier%2520and%2520Heike%2520M.%2520Buhl%2520and%2520Friederike%2520Kern%2520and%2520Angela%2520Grimminger%2520and%2520Helen%2520Beierling%2520and%2520Josephine%2520Fisher%2520and%2520Andr%25C3%25A9%2520Gro%25C3%259F%2520and%2520Ilona%2520Horwath%2520and%2520Nils%2520Klowait%2520and%2520Stefan%2520Lazarov%2520and%2520Michael%2520Lenke%2520and%2520Vivien%2520Lohmer%2520and%2520Katharina%2520Rohlfing%2520and%2520Ingrid%2520Scharlau%2520and%2520Amit%2520Singh%2520and%2520Lutz%2520Terfloth%2520and%2520Anna-Lisa%2520Vollmer%2520and%2520Yu%2520Wang%2520and%2520Annedore%2520Wilmes%2520and%2520Britta%2520Wrede%26entry.1292438233%3D%2520%2520Explainability%2520has%2520become%2520an%2520important%2520topic%2520in%2520computer%2520science%2520and%250Aartificial%2520intelligence%252C%2520leading%2520to%2520a%2520subfield%2520called%2520Explainable%2520Artificial%250AIntelligence%2520%2528XAI%2529.%2520The%2520goal%2520of%2520providing%2520or%2520seeking%2520explanations%2520is%2520to%2520achieve%250A%2528better%2529%2520%2527understanding%2527%2520on%2520the%2520part%2520of%2520the%2520explainee.%2520However%252C%2520what%2520it%2520means%250Ato%2520%2527understand%2527%2520is%2520still%2520not%2520clearly%2520defined%252C%2520and%2520the%2520concept%2520itself%2520is%2520rarely%250Athe%2520subject%2520of%2520scientific%2520investigation.%2520This%2520conceptual%2520article%2520aims%2520to%250Apresent%2520a%2520model%2520of%2520forms%2520of%2520understanding%2520for%2520XAI-explanations%2520and%2520beyond.%2520From%250Aan%2520interdisciplinary%2520perspective%2520bringing%2520together%2520computer%2520science%252C%250Alinguistics%252C%2520sociology%252C%2520philosophy%2520and%2520psychology%252C%2520a%2520definition%2520of%250Aunderstanding%2520and%2520its%2520forms%252C%2520assessment%252C%2520and%2520dynamics%2520during%2520the%2520process%2520of%250Agiving%2520everyday%2520explanations%2520are%2520explored.%2520Two%2520types%2520of%2520understanding%2520are%250Aconsidered%2520as%2520possible%2520outcomes%2520of%2520explanations%252C%2520namely%2520enabledness%252C%2520%2527knowing%250Ahow%2527%2520to%2520do%2520or%2520decide%2520something%252C%2520and%2520comprehension%252C%2520%2527knowing%2520that%2527%2520--%2520both%2520in%250Adifferent%2520degrees%2520%2528from%2520shallow%2520to%2520deep%2529.%2520Explanations%2520regularly%2520start%2520with%250Ashallow%2520understanding%2520in%2520a%2520specific%2520domain%2520and%2520can%2520lead%2520to%2520deep%2520comprehension%250Aand%2520enabledness%2520of%2520the%2520explanandum%252C%2520which%2520we%2520see%2520as%2520a%2520prerequisite%2520for%2520human%250Ausers%2520to%2520gain%2520agency.%2520In%2520this%2520process%252C%2520the%2520increase%2520of%2520comprehension%2520and%250Aenabledness%2520are%2520highly%2520interdependent.%2520Against%2520the%2520background%2520of%2520this%250Asystematization%252C%2520special%2520challenges%2520of%2520understanding%2520in%2520XAI%2520are%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forms%20of%20Understanding%20for%20XAI-Explanations&entry.906535625=Hendrik%20Buschmeier%20and%20Heike%20M.%20Buhl%20and%20Friederike%20Kern%20and%20Angela%20Grimminger%20and%20Helen%20Beierling%20and%20Josephine%20Fisher%20and%20Andr%C3%A9%20Gro%C3%9F%20and%20Ilona%20Horwath%20and%20Nils%20Klowait%20and%20Stefan%20Lazarov%20and%20Michael%20Lenke%20and%20Vivien%20Lohmer%20and%20Katharina%20Rohlfing%20and%20Ingrid%20Scharlau%20and%20Amit%20Singh%20and%20Lutz%20Terfloth%20and%20Anna-Lisa%20Vollmer%20and%20Yu%20Wang%20and%20Annedore%20Wilmes%20and%20Britta%20Wrede&entry.1292438233=%20%20Explainability%20has%20become%20an%20important%20topic%20in%20computer%20science%20and%0Aartificial%20intelligence%2C%20leading%20to%20a%20subfield%20called%20Explainable%20Artificial%0AIntelligence%20%28XAI%29.%20The%20goal%20of%20providing%20or%20seeking%20explanations%20is%20to%20achieve%0A%28better%29%20%27understanding%27%20on%20the%20part%20of%20the%20explainee.%20However%2C%20what%20it%20means%0Ato%20%27understand%27%20is%20still%20not%20clearly%20defined%2C%20and%20the%20concept%20itself%20is%20rarely%0Athe%20subject%20of%20scientific%20investigation.%20This%20conceptual%20article%20aims%20to%0Apresent%20a%20model%20of%20forms%20of%20understanding%20for%20XAI-explanations%20and%20beyond.%20From%0Aan%20interdisciplinary%20perspective%20bringing%20together%20computer%20science%2C%0Alinguistics%2C%20sociology%2C%20philosophy%20and%20psychology%2C%20a%20definition%20of%0Aunderstanding%20and%20its%20forms%2C%20assessment%2C%20and%20dynamics%20during%20the%20process%20of%0Agiving%20everyday%20explanations%20are%20explored.%20Two%20types%20of%20understanding%20are%0Aconsidered%20as%20possible%20outcomes%20of%20explanations%2C%20namely%20enabledness%2C%20%27knowing%0Ahow%27%20to%20do%20or%20decide%20something%2C%20and%20comprehension%2C%20%27knowing%20that%27%20--%20both%20in%0Adifferent%20degrees%20%28from%20shallow%20to%20deep%29.%20Explanations%20regularly%20start%20with%0Ashallow%20understanding%20in%20a%20specific%20domain%20and%20can%20lead%20to%20deep%20comprehension%0Aand%20enabledness%20of%20the%20explanandum%2C%20which%20we%20see%20as%20a%20prerequisite%20for%20human%0Ausers%20to%20gain%20agency.%20In%20this%20process%2C%20the%20increase%20of%20comprehension%20and%0Aenabledness%20are%20highly%20interdependent.%20Against%20the%20background%20of%20this%0Asystematization%2C%20special%20challenges%20of%20understanding%20in%20XAI%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08760v2&entry.124074799=Read"},
{"title": "Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning", "author": "Dionysis Christopoulos and Sotiris Spanos and Eirini Baltzi and Valsamis Ntouskos and Konstantinos Karantzalos", "abstract": "  We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning\nrich representations of skin lesions through a novel nested contrastive\nlearning approach that captures complex relationships between images and\nmetadata. Melanoma detection and skin lesion classification based solely on\nimages, pose significant challenges due to large variations in imaging\nconditions (lighting, color, resolution, distance, etc.) and lack of clinical\nand phenotypical context. Clinicians typically follow a holistic approach for\nassessing the risk level of the patient and for deciding which lesions may be\nmalignant and need to be excised, by considering the patient's medical history\nas well as the appearance of other lesions of the patient. Inspired by this,\nSLIMP combines the appearance and the metadata of individual skin lesions with\npatient-level metadata relating to their medical record and other clinically\nrelevant information. By fully exploiting all available data modalities\nthroughout the learning process, the proposed pre-training strategy improves\nperformance compared to other pre-training strategies on downstream skin\nlesions classification tasks highlighting the learned representations quality.\n", "link": "http://arxiv.org/abs/2505.23709v1", "date": "2025-05-29", "relevancy": 2.1972, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5788}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5321}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skin%20Lesion%20Phenotyping%20via%20Nested%20Multi-modal%20Contrastive%20Learning&body=Title%3A%20Skin%20Lesion%20Phenotyping%20via%20Nested%20Multi-modal%20Contrastive%20Learning%0AAuthor%3A%20Dionysis%20Christopoulos%20and%20Sotiris%20Spanos%20and%20Eirini%20Baltzi%20and%20Valsamis%20Ntouskos%20and%20Konstantinos%20Karantzalos%0AAbstract%3A%20%20%20We%20introduce%20SLIMP%20%28Skin%20Lesion%20Image-Metadata%20Pre-training%29%20for%20learning%0Arich%20representations%20of%20skin%20lesions%20through%20a%20novel%20nested%20contrastive%0Alearning%20approach%20that%20captures%20complex%20relationships%20between%20images%20and%0Ametadata.%20Melanoma%20detection%20and%20skin%20lesion%20classification%20based%20solely%20on%0Aimages%2C%20pose%20significant%20challenges%20due%20to%20large%20variations%20in%20imaging%0Aconditions%20%28lighting%2C%20color%2C%20resolution%2C%20distance%2C%20etc.%29%20and%20lack%20of%20clinical%0Aand%20phenotypical%20context.%20Clinicians%20typically%20follow%20a%20holistic%20approach%20for%0Aassessing%20the%20risk%20level%20of%20the%20patient%20and%20for%20deciding%20which%20lesions%20may%20be%0Amalignant%20and%20need%20to%20be%20excised%2C%20by%20considering%20the%20patient%27s%20medical%20history%0Aas%20well%20as%20the%20appearance%20of%20other%20lesions%20of%20the%20patient.%20Inspired%20by%20this%2C%0ASLIMP%20combines%20the%20appearance%20and%20the%20metadata%20of%20individual%20skin%20lesions%20with%0Apatient-level%20metadata%20relating%20to%20their%20medical%20record%20and%20other%20clinically%0Arelevant%20information.%20By%20fully%20exploiting%20all%20available%20data%20modalities%0Athroughout%20the%20learning%20process%2C%20the%20proposed%20pre-training%20strategy%20improves%0Aperformance%20compared%20to%20other%20pre-training%20strategies%20on%20downstream%20skin%0Alesions%20classification%20tasks%20highlighting%20the%20learned%20representations%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkin%2520Lesion%2520Phenotyping%2520via%2520Nested%2520Multi-modal%2520Contrastive%2520Learning%26entry.906535625%3DDionysis%2520Christopoulos%2520and%2520Sotiris%2520Spanos%2520and%2520Eirini%2520Baltzi%2520and%2520Valsamis%2520Ntouskos%2520and%2520Konstantinos%2520Karantzalos%26entry.1292438233%3D%2520%2520We%2520introduce%2520SLIMP%2520%2528Skin%2520Lesion%2520Image-Metadata%2520Pre-training%2529%2520for%2520learning%250Arich%2520representations%2520of%2520skin%2520lesions%2520through%2520a%2520novel%2520nested%2520contrastive%250Alearning%2520approach%2520that%2520captures%2520complex%2520relationships%2520between%2520images%2520and%250Ametadata.%2520Melanoma%2520detection%2520and%2520skin%2520lesion%2520classification%2520based%2520solely%2520on%250Aimages%252C%2520pose%2520significant%2520challenges%2520due%2520to%2520large%2520variations%2520in%2520imaging%250Aconditions%2520%2528lighting%252C%2520color%252C%2520resolution%252C%2520distance%252C%2520etc.%2529%2520and%2520lack%2520of%2520clinical%250Aand%2520phenotypical%2520context.%2520Clinicians%2520typically%2520follow%2520a%2520holistic%2520approach%2520for%250Aassessing%2520the%2520risk%2520level%2520of%2520the%2520patient%2520and%2520for%2520deciding%2520which%2520lesions%2520may%2520be%250Amalignant%2520and%2520need%2520to%2520be%2520excised%252C%2520by%2520considering%2520the%2520patient%2527s%2520medical%2520history%250Aas%2520well%2520as%2520the%2520appearance%2520of%2520other%2520lesions%2520of%2520the%2520patient.%2520Inspired%2520by%2520this%252C%250ASLIMP%2520combines%2520the%2520appearance%2520and%2520the%2520metadata%2520of%2520individual%2520skin%2520lesions%2520with%250Apatient-level%2520metadata%2520relating%2520to%2520their%2520medical%2520record%2520and%2520other%2520clinically%250Arelevant%2520information.%2520By%2520fully%2520exploiting%2520all%2520available%2520data%2520modalities%250Athroughout%2520the%2520learning%2520process%252C%2520the%2520proposed%2520pre-training%2520strategy%2520improves%250Aperformance%2520compared%2520to%2520other%2520pre-training%2520strategies%2520on%2520downstream%2520skin%250Alesions%2520classification%2520tasks%2520highlighting%2520the%2520learned%2520representations%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skin%20Lesion%20Phenotyping%20via%20Nested%20Multi-modal%20Contrastive%20Learning&entry.906535625=Dionysis%20Christopoulos%20and%20Sotiris%20Spanos%20and%20Eirini%20Baltzi%20and%20Valsamis%20Ntouskos%20and%20Konstantinos%20Karantzalos&entry.1292438233=%20%20We%20introduce%20SLIMP%20%28Skin%20Lesion%20Image-Metadata%20Pre-training%29%20for%20learning%0Arich%20representations%20of%20skin%20lesions%20through%20a%20novel%20nested%20contrastive%0Alearning%20approach%20that%20captures%20complex%20relationships%20between%20images%20and%0Ametadata.%20Melanoma%20detection%20and%20skin%20lesion%20classification%20based%20solely%20on%0Aimages%2C%20pose%20significant%20challenges%20due%20to%20large%20variations%20in%20imaging%0Aconditions%20%28lighting%2C%20color%2C%20resolution%2C%20distance%2C%20etc.%29%20and%20lack%20of%20clinical%0Aand%20phenotypical%20context.%20Clinicians%20typically%20follow%20a%20holistic%20approach%20for%0Aassessing%20the%20risk%20level%20of%20the%20patient%20and%20for%20deciding%20which%20lesions%20may%20be%0Amalignant%20and%20need%20to%20be%20excised%2C%20by%20considering%20the%20patient%27s%20medical%20history%0Aas%20well%20as%20the%20appearance%20of%20other%20lesions%20of%20the%20patient.%20Inspired%20by%20this%2C%0ASLIMP%20combines%20the%20appearance%20and%20the%20metadata%20of%20individual%20skin%20lesions%20with%0Apatient-level%20metadata%20relating%20to%20their%20medical%20record%20and%20other%20clinically%0Arelevant%20information.%20By%20fully%20exploiting%20all%20available%20data%20modalities%0Athroughout%20the%20learning%20process%2C%20the%20proposed%20pre-training%20strategy%20improves%0Aperformance%20compared%20to%20other%20pre-training%20strategies%20on%20downstream%20skin%0Alesions%20classification%20tasks%20highlighting%20the%20learned%20representations%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23709v1&entry.124074799=Read"},
{"title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost", "author": "Chenyu Yang and Shiqian Su and Shi Liu and Xuan Dong and Yue Yu and Weijie Su and Xuehui Wang and Zhaoyang Liu and Jinguo Zhu and Hao Li and Wenhai Wang and Yu Qiao and Xizhou Zhu and Jifeng Dai", "abstract": "  The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.\n", "link": "http://arxiv.org/abs/2505.23762v1", "date": "2025-05-29", "relevancy": 2.2373, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5915}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.549}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZeroGUI%3A%20Automating%20Online%20GUI%20Learning%20at%20Zero%20Human%20Cost&body=Title%3A%20ZeroGUI%3A%20Automating%20Online%20GUI%20Learning%20at%20Zero%20Human%20Cost%0AAuthor%3A%20Chenyu%20Yang%20and%20Shiqian%20Su%20and%20Shi%20Liu%20and%20Xuan%20Dong%20and%20Yue%20Yu%20and%20Weijie%20Su%20and%20Xuehui%20Wang%20and%20Zhaoyang%20Liu%20and%20Jinguo%20Zhu%20and%20Hao%20Li%20and%20Wenhai%20Wang%20and%20Yu%20Qiao%20and%20Xizhou%20Zhu%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20large%20Vision-Language%20Models%20%28VLMs%29%20has%20propelled%0Athe%20development%20of%20pure-vision-based%20GUI%20Agents%2C%20capable%20of%20perceiving%20and%0Aoperating%20Graphical%20User%20Interfaces%20%28GUI%29%20to%20autonomously%20fulfill%20user%0Ainstructions.%20However%2C%20existing%20approaches%20usually%20adopt%20an%20offline%20learning%0Aframework%2C%20which%20faces%20two%20core%20limitations%3A%20%281%29%20heavy%20reliance%20on%20high-quality%0Amanual%20annotations%20for%20element%20grounding%20and%20action%20supervision%2C%20and%20%282%29%0Alimited%20adaptability%20to%20dynamic%20and%20interactive%20environments.%20To%20address%20these%0Alimitations%2C%20we%20propose%20ZeroGUI%2C%20a%20scalable%2C%20online%20learning%20framework%20for%0Aautomating%20GUI%20Agent%20training%20at%20Zero%20human%20cost.%20Specifically%2C%20ZeroGUI%0Aintegrates%20%28i%29%20VLM-based%20automatic%20task%20generation%20to%20produce%20diverse%20training%0Agoals%20from%20the%20current%20environment%20state%2C%20%28ii%29%20VLM-based%20automatic%20reward%0Aestimation%20to%20assess%20task%20success%20without%20hand-crafted%20evaluation%20functions%2C%0Aand%20%28iii%29%20two-stage%20online%20reinforcement%20learning%20to%20continuously%20interact%20with%0Aand%20learn%20from%20GUI%20environments.%20Experiments%20on%20two%20advanced%20GUI%20Agents%0A%28UI-TARS%20and%20Aguvis%29%20demonstrate%20that%20ZeroGUI%20significantly%20boosts%20performance%0Aacross%20OSWorld%20and%20AndroidLab%20environments.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/OpenGVLab/ZeroGUI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroGUI%253A%2520Automating%2520Online%2520GUI%2520Learning%2520at%2520Zero%2520Human%2520Cost%26entry.906535625%3DChenyu%2520Yang%2520and%2520Shiqian%2520Su%2520and%2520Shi%2520Liu%2520and%2520Xuan%2520Dong%2520and%2520Yue%2520Yu%2520and%2520Weijie%2520Su%2520and%2520Xuehui%2520Wang%2520and%2520Zhaoyang%2520Liu%2520and%2520Jinguo%2520Zhu%2520and%2520Hao%2520Li%2520and%2520Wenhai%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Xizhou%2520Zhu%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520propelled%250Athe%2520development%2520of%2520pure-vision-based%2520GUI%2520Agents%252C%2520capable%2520of%2520perceiving%2520and%250Aoperating%2520Graphical%2520User%2520Interfaces%2520%2528GUI%2529%2520to%2520autonomously%2520fulfill%2520user%250Ainstructions.%2520However%252C%2520existing%2520approaches%2520usually%2520adopt%2520an%2520offline%2520learning%250Aframework%252C%2520which%2520faces%2520two%2520core%2520limitations%253A%2520%25281%2529%2520heavy%2520reliance%2520on%2520high-quality%250Amanual%2520annotations%2520for%2520element%2520grounding%2520and%2520action%2520supervision%252C%2520and%2520%25282%2529%250Alimited%2520adaptability%2520to%2520dynamic%2520and%2520interactive%2520environments.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520ZeroGUI%252C%2520a%2520scalable%252C%2520online%2520learning%2520framework%2520for%250Aautomating%2520GUI%2520Agent%2520training%2520at%2520Zero%2520human%2520cost.%2520Specifically%252C%2520ZeroGUI%250Aintegrates%2520%2528i%2529%2520VLM-based%2520automatic%2520task%2520generation%2520to%2520produce%2520diverse%2520training%250Agoals%2520from%2520the%2520current%2520environment%2520state%252C%2520%2528ii%2529%2520VLM-based%2520automatic%2520reward%250Aestimation%2520to%2520assess%2520task%2520success%2520without%2520hand-crafted%2520evaluation%2520functions%252C%250Aand%2520%2528iii%2529%2520two-stage%2520online%2520reinforcement%2520learning%2520to%2520continuously%2520interact%2520with%250Aand%2520learn%2520from%2520GUI%2520environments.%2520Experiments%2520on%2520two%2520advanced%2520GUI%2520Agents%250A%2528UI-TARS%2520and%2520Aguvis%2529%2520demonstrate%2520that%2520ZeroGUI%2520significantly%2520boosts%2520performance%250Aacross%2520OSWorld%2520and%2520AndroidLab%2520environments.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/OpenGVLab/ZeroGUI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZeroGUI%3A%20Automating%20Online%20GUI%20Learning%20at%20Zero%20Human%20Cost&entry.906535625=Chenyu%20Yang%20and%20Shiqian%20Su%20and%20Shi%20Liu%20and%20Xuan%20Dong%20and%20Yue%20Yu%20and%20Weijie%20Su%20and%20Xuehui%20Wang%20and%20Zhaoyang%20Liu%20and%20Jinguo%20Zhu%20and%20Hao%20Li%20and%20Wenhai%20Wang%20and%20Yu%20Qiao%20and%20Xizhou%20Zhu%20and%20Jifeng%20Dai&entry.1292438233=%20%20The%20rapid%20advancement%20of%20large%20Vision-Language%20Models%20%28VLMs%29%20has%20propelled%0Athe%20development%20of%20pure-vision-based%20GUI%20Agents%2C%20capable%20of%20perceiving%20and%0Aoperating%20Graphical%20User%20Interfaces%20%28GUI%29%20to%20autonomously%20fulfill%20user%0Ainstructions.%20However%2C%20existing%20approaches%20usually%20adopt%20an%20offline%20learning%0Aframework%2C%20which%20faces%20two%20core%20limitations%3A%20%281%29%20heavy%20reliance%20on%20high-quality%0Amanual%20annotations%20for%20element%20grounding%20and%20action%20supervision%2C%20and%20%282%29%0Alimited%20adaptability%20to%20dynamic%20and%20interactive%20environments.%20To%20address%20these%0Alimitations%2C%20we%20propose%20ZeroGUI%2C%20a%20scalable%2C%20online%20learning%20framework%20for%0Aautomating%20GUI%20Agent%20training%20at%20Zero%20human%20cost.%20Specifically%2C%20ZeroGUI%0Aintegrates%20%28i%29%20VLM-based%20automatic%20task%20generation%20to%20produce%20diverse%20training%0Agoals%20from%20the%20current%20environment%20state%2C%20%28ii%29%20VLM-based%20automatic%20reward%0Aestimation%20to%20assess%20task%20success%20without%20hand-crafted%20evaluation%20functions%2C%0Aand%20%28iii%29%20two-stage%20online%20reinforcement%20learning%20to%20continuously%20interact%20with%0Aand%20learn%20from%20GUI%20environments.%20Experiments%20on%20two%20advanced%20GUI%20Agents%0A%28UI-TARS%20and%20Aguvis%29%20demonstrate%20that%20ZeroGUI%20significantly%20boosts%20performance%0Aacross%20OSWorld%20and%20AndroidLab%20environments.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/OpenGVLab/ZeroGUI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23762v1&entry.124074799=Read"},
{"title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "author": "Jennifer D'Souza and Hamed Babaei Giglou and Quentin M\u00fcnch", "abstract": "  Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry.\n", "link": "http://arxiv.org/abs/2505.14279v2", "date": "2025-05-29", "relevancy": 2.0399, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YESciEval%3A%20Robust%20LLM-as-a-Judge%20for%20Scientific%20Question%20Answering&body=Title%3A%20YESciEval%3A%20Robust%20LLM-as-a-Judge%20for%20Scientific%20Question%20Answering%0AAuthor%3A%20Jennifer%20D%27Souza%20and%20Hamed%20Babaei%20Giglou%20and%20Quentin%20M%C3%BCnch%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20drive%20scientific%20question-answering%20on%20modern%0Asearch%20engines%2C%20yet%20their%20evaluation%20robustness%20remains%20underexplored.%20We%0Aintroduce%20YESciEval%2C%20an%20open-source%20framework%20that%20combines%20fine-grained%0Arubric-based%20assessment%20with%20reinforcement%20learning%20to%20mitigate%20optimism%20bias%0Ain%20LLM%20evaluators.%20We%20release%20multidisciplinary%20scienceQ%26A%20datasets%2C%20including%0Aadversarial%20variants%2C%20with%20evaluation%20scores%20from%20multiple%20LLMs.%20Independent%20of%0Aproprietary%20models%20and%20human%20feedback%2C%20our%20approach%20enables%20scalable%2C%20cost-free%0Aevaluation.%20By%20advancing%20reliable%20LLM-as-a-judge%20models%2C%20this%20work%20supports%20AI%0Aalignment%20and%20fosters%20robust%2C%20transparent%20evaluation%20essential%20for%20scientific%0Ainquiry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYESciEval%253A%2520Robust%2520LLM-as-a-Judge%2520for%2520Scientific%2520Question%2520Answering%26entry.906535625%3DJennifer%2520D%2527Souza%2520and%2520Hamed%2520Babaei%2520Giglou%2520and%2520Quentin%2520M%25C3%25BCnch%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520drive%2520scientific%2520question-answering%2520on%2520modern%250Asearch%2520engines%252C%2520yet%2520their%2520evaluation%2520robustness%2520remains%2520underexplored.%2520We%250Aintroduce%2520YESciEval%252C%2520an%2520open-source%2520framework%2520that%2520combines%2520fine-grained%250Arubric-based%2520assessment%2520with%2520reinforcement%2520learning%2520to%2520mitigate%2520optimism%2520bias%250Ain%2520LLM%2520evaluators.%2520We%2520release%2520multidisciplinary%2520scienceQ%2526A%2520datasets%252C%2520including%250Aadversarial%2520variants%252C%2520with%2520evaluation%2520scores%2520from%2520multiple%2520LLMs.%2520Independent%2520of%250Aproprietary%2520models%2520and%2520human%2520feedback%252C%2520our%2520approach%2520enables%2520scalable%252C%2520cost-free%250Aevaluation.%2520By%2520advancing%2520reliable%2520LLM-as-a-judge%2520models%252C%2520this%2520work%2520supports%2520AI%250Aalignment%2520and%2520fosters%2520robust%252C%2520transparent%2520evaluation%2520essential%2520for%2520scientific%250Ainquiry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YESciEval%3A%20Robust%20LLM-as-a-Judge%20for%20Scientific%20Question%20Answering&entry.906535625=Jennifer%20D%27Souza%20and%20Hamed%20Babaei%20Giglou%20and%20Quentin%20M%C3%BCnch&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20drive%20scientific%20question-answering%20on%20modern%0Asearch%20engines%2C%20yet%20their%20evaluation%20robustness%20remains%20underexplored.%20We%0Aintroduce%20YESciEval%2C%20an%20open-source%20framework%20that%20combines%20fine-grained%0Arubric-based%20assessment%20with%20reinforcement%20learning%20to%20mitigate%20optimism%20bias%0Ain%20LLM%20evaluators.%20We%20release%20multidisciplinary%20scienceQ%26A%20datasets%2C%20including%0Aadversarial%20variants%2C%20with%20evaluation%20scores%20from%20multiple%20LLMs.%20Independent%20of%0Aproprietary%20models%20and%20human%20feedback%2C%20our%20approach%20enables%20scalable%2C%20cost-free%0Aevaluation.%20By%20advancing%20reliable%20LLM-as-a-judge%20models%2C%20this%20work%20supports%20AI%0Aalignment%20and%20fosters%20robust%2C%20transparent%20evaluation%20essential%20for%20scientific%0Ainquiry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14279v2&entry.124074799=Read"},
{"title": "MiZero: The Shadowy Defender Against Text Style Infringements", "author": "Ziwei Zhang and Juan Wen and Wanli Peng and Zhengxian Wu and Yinghan Zhou and Yiming Xue", "abstract": "  In-Context Learning (ICL) and efficient fine-tuning methods significantly\nenhanced the efficiency of applying Large Language Models (LLMs) to downstream\ntasks. However, they also raise concerns about the imitation and infringement\nof personal creative data. Current methods for data copyright protection\nprimarily focuses on content security but lacks effectiveness in protecting the\ncopyrights of text styles. In this paper, we introduce a novel implicit\nzero-watermarking scheme, namely MiZero. This scheme establishes a precise\nwatermark domain to protect the copyrighted style, surpassing traditional\nwatermarking methods that distort the style characteristics. Specifically, we\nemploy LLMs to extract condensed-lists utilizing the designed instance\ndelimitation mechanism. These lists guide MiZero in generating the watermark.\nExtensive experiments demonstrate that MiZero effectively verifies text style\ncopyright ownership against AI imitation.\n", "link": "http://arxiv.org/abs/2504.00035v2", "date": "2025-05-29", "relevancy": 1.4044, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4854}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4467}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiZero%3A%20The%20Shadowy%20Defender%20Against%20Text%20Style%20Infringements&body=Title%3A%20MiZero%3A%20The%20Shadowy%20Defender%20Against%20Text%20Style%20Infringements%0AAuthor%3A%20Ziwei%20Zhang%20and%20Juan%20Wen%20and%20Wanli%20Peng%20and%20Zhengxian%20Wu%20and%20Yinghan%20Zhou%20and%20Yiming%20Xue%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20and%20efficient%20fine-tuning%20methods%20significantly%0Aenhanced%20the%20efficiency%20of%20applying%20Large%20Language%20Models%20%28LLMs%29%20to%20downstream%0Atasks.%20However%2C%20they%20also%20raise%20concerns%20about%20the%20imitation%20and%20infringement%0Aof%20personal%20creative%20data.%20Current%20methods%20for%20data%20copyright%20protection%0Aprimarily%20focuses%20on%20content%20security%20but%20lacks%20effectiveness%20in%20protecting%20the%0Acopyrights%20of%20text%20styles.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20implicit%0Azero-watermarking%20scheme%2C%20namely%20MiZero.%20This%20scheme%20establishes%20a%20precise%0Awatermark%20domain%20to%20protect%20the%20copyrighted%20style%2C%20surpassing%20traditional%0Awatermarking%20methods%20that%20distort%20the%20style%20characteristics.%20Specifically%2C%20we%0Aemploy%20LLMs%20to%20extract%20condensed-lists%20utilizing%20the%20designed%20instance%0Adelimitation%20mechanism.%20These%20lists%20guide%20MiZero%20in%20generating%20the%20watermark.%0AExtensive%20experiments%20demonstrate%20that%20MiZero%20effectively%20verifies%20text%20style%0Acopyright%20ownership%20against%20AI%20imitation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiZero%253A%2520The%2520Shadowy%2520Defender%2520Against%2520Text%2520Style%2520Infringements%26entry.906535625%3DZiwei%2520Zhang%2520and%2520Juan%2520Wen%2520and%2520Wanli%2520Peng%2520and%2520Zhengxian%2520Wu%2520and%2520Yinghan%2520Zhou%2520and%2520Yiming%2520Xue%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520and%2520efficient%2520fine-tuning%2520methods%2520significantly%250Aenhanced%2520the%2520efficiency%2520of%2520applying%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520downstream%250Atasks.%2520However%252C%2520they%2520also%2520raise%2520concerns%2520about%2520the%2520imitation%2520and%2520infringement%250Aof%2520personal%2520creative%2520data.%2520Current%2520methods%2520for%2520data%2520copyright%2520protection%250Aprimarily%2520focuses%2520on%2520content%2520security%2520but%2520lacks%2520effectiveness%2520in%2520protecting%2520the%250Acopyrights%2520of%2520text%2520styles.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520implicit%250Azero-watermarking%2520scheme%252C%2520namely%2520MiZero.%2520This%2520scheme%2520establishes%2520a%2520precise%250Awatermark%2520domain%2520to%2520protect%2520the%2520copyrighted%2520style%252C%2520surpassing%2520traditional%250Awatermarking%2520methods%2520that%2520distort%2520the%2520style%2520characteristics.%2520Specifically%252C%2520we%250Aemploy%2520LLMs%2520to%2520extract%2520condensed-lists%2520utilizing%2520the%2520designed%2520instance%250Adelimitation%2520mechanism.%2520These%2520lists%2520guide%2520MiZero%2520in%2520generating%2520the%2520watermark.%250AExtensive%2520experiments%2520demonstrate%2520that%2520MiZero%2520effectively%2520verifies%2520text%2520style%250Acopyright%2520ownership%2520against%2520AI%2520imitation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiZero%3A%20The%20Shadowy%20Defender%20Against%20Text%20Style%20Infringements&entry.906535625=Ziwei%20Zhang%20and%20Juan%20Wen%20and%20Wanli%20Peng%20and%20Zhengxian%20Wu%20and%20Yinghan%20Zhou%20and%20Yiming%20Xue&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20and%20efficient%20fine-tuning%20methods%20significantly%0Aenhanced%20the%20efficiency%20of%20applying%20Large%20Language%20Models%20%28LLMs%29%20to%20downstream%0Atasks.%20However%2C%20they%20also%20raise%20concerns%20about%20the%20imitation%20and%20infringement%0Aof%20personal%20creative%20data.%20Current%20methods%20for%20data%20copyright%20protection%0Aprimarily%20focuses%20on%20content%20security%20but%20lacks%20effectiveness%20in%20protecting%20the%0Acopyrights%20of%20text%20styles.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20implicit%0Azero-watermarking%20scheme%2C%20namely%20MiZero.%20This%20scheme%20establishes%20a%20precise%0Awatermark%20domain%20to%20protect%20the%20copyrighted%20style%2C%20surpassing%20traditional%0Awatermarking%20methods%20that%20distort%20the%20style%20characteristics.%20Specifically%2C%20we%0Aemploy%20LLMs%20to%20extract%20condensed-lists%20utilizing%20the%20designed%20instance%0Adelimitation%20mechanism.%20These%20lists%20guide%20MiZero%20in%20generating%20the%20watermark.%0AExtensive%20experiments%20demonstrate%20that%20MiZero%20effectively%20verifies%20text%20style%0Acopyright%20ownership%20against%20AI%20imitation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00035v2&entry.124074799=Read"},
{"title": "Long Duration Inspection of GNSS-Denied Environments with a Tethered\n  UAV-UGV Marsupial System", "author": "Sim\u00f3n Mart\u00ednez-Rozas and David Alejo and Jos\u00e9 Javier Carpio and Fernando Caballero and Luis Merino", "abstract": "  Unmanned Aerial Vehicles (UAVs) have become essential tools in inspection and\nemergency response operations due to their high maneuverability and ability to\naccess hard-to-reach areas. However, their limited battery life significantly\nrestricts their use in long-duration missions. This paper presents a novel\ntethered marsupial robotic system composed of a UAV and an Unmanned Ground\nVehicle (UGV), specifically designed for autonomous, long-duration inspection\ntasks in Global Navigation Satellite System (GNSS)-denied environments. The\nsystem extends the UAV's operational time by supplying power through a tether\nconnected to high-capacity battery packs carried by the UGV. We detail the\nhardware architecture based on off-the-shelf components to ensure replicability\nand describe our full-stack software framework, which is composed of\nopen-source components and built upon the Robot Operating System (ROS). The\nproposed software architecture enables precise localization using a Direct\nLiDAR Localization (DLL) method and ensures safe path planning and coordinated\ntrajectory tracking for the integrated UGV-tether-UAV system. We validate the\nsystem through three field experiments: (1) a manual flight endurance test to\nestimate the operational duration, (2) an autonomous navigation test, and (3)\nan inspection mission to demonstrate autonomous inspection capabilities.\nExperimental results confirm the robustness and autonomy of the system, its\ncapacity to operate in GNSS-denied environments, and its potential for\nlong-endurance, autonomous inspection and monitoring tasks.\n", "link": "http://arxiv.org/abs/2505.23457v1", "date": "2025-05-29", "relevancy": 2.0642, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5047}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20Duration%20Inspection%20of%20GNSS-Denied%20Environments%20with%20a%20Tethered%0A%20%20UAV-UGV%20Marsupial%20System&body=Title%3A%20Long%20Duration%20Inspection%20of%20GNSS-Denied%20Environments%20with%20a%20Tethered%0A%20%20UAV-UGV%20Marsupial%20System%0AAuthor%3A%20Sim%C3%B3n%20Mart%C3%ADnez-Rozas%20and%20David%20Alejo%20and%20Jos%C3%A9%20Javier%20Carpio%20and%20Fernando%20Caballero%20and%20Luis%20Merino%0AAbstract%3A%20%20%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20have%20become%20essential%20tools%20in%20inspection%20and%0Aemergency%20response%20operations%20due%20to%20their%20high%20maneuverability%20and%20ability%20to%0Aaccess%20hard-to-reach%20areas.%20However%2C%20their%20limited%20battery%20life%20significantly%0Arestricts%20their%20use%20in%20long-duration%20missions.%20This%20paper%20presents%20a%20novel%0Atethered%20marsupial%20robotic%20system%20composed%20of%20a%20UAV%20and%20an%20Unmanned%20Ground%0AVehicle%20%28UGV%29%2C%20specifically%20designed%20for%20autonomous%2C%20long-duration%20inspection%0Atasks%20in%20Global%20Navigation%20Satellite%20System%20%28GNSS%29-denied%20environments.%20The%0Asystem%20extends%20the%20UAV%27s%20operational%20time%20by%20supplying%20power%20through%20a%20tether%0Aconnected%20to%20high-capacity%20battery%20packs%20carried%20by%20the%20UGV.%20We%20detail%20the%0Ahardware%20architecture%20based%20on%20off-the-shelf%20components%20to%20ensure%20replicability%0Aand%20describe%20our%20full-stack%20software%20framework%2C%20which%20is%20composed%20of%0Aopen-source%20components%20and%20built%20upon%20the%20Robot%20Operating%20System%20%28ROS%29.%20The%0Aproposed%20software%20architecture%20enables%20precise%20localization%20using%20a%20Direct%0ALiDAR%20Localization%20%28DLL%29%20method%20and%20ensures%20safe%20path%20planning%20and%20coordinated%0Atrajectory%20tracking%20for%20the%20integrated%20UGV-tether-UAV%20system.%20We%20validate%20the%0Asystem%20through%20three%20field%20experiments%3A%20%281%29%20a%20manual%20flight%20endurance%20test%20to%0Aestimate%20the%20operational%20duration%2C%20%282%29%20an%20autonomous%20navigation%20test%2C%20and%20%283%29%0Aan%20inspection%20mission%20to%20demonstrate%20autonomous%20inspection%20capabilities.%0AExperimental%20results%20confirm%20the%20robustness%20and%20autonomy%20of%20the%20system%2C%20its%0Acapacity%20to%20operate%20in%20GNSS-denied%20environments%2C%20and%20its%20potential%20for%0Along-endurance%2C%20autonomous%20inspection%20and%20monitoring%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520Duration%2520Inspection%2520of%2520GNSS-Denied%2520Environments%2520with%2520a%2520Tethered%250A%2520%2520UAV-UGV%2520Marsupial%2520System%26entry.906535625%3DSim%25C3%25B3n%2520Mart%25C3%25ADnez-Rozas%2520and%2520David%2520Alejo%2520and%2520Jos%25C3%25A9%2520Javier%2520Carpio%2520and%2520Fernando%2520Caballero%2520and%2520Luis%2520Merino%26entry.1292438233%3D%2520%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520have%2520become%2520essential%2520tools%2520in%2520inspection%2520and%250Aemergency%2520response%2520operations%2520due%2520to%2520their%2520high%2520maneuverability%2520and%2520ability%2520to%250Aaccess%2520hard-to-reach%2520areas.%2520However%252C%2520their%2520limited%2520battery%2520life%2520significantly%250Arestricts%2520their%2520use%2520in%2520long-duration%2520missions.%2520This%2520paper%2520presents%2520a%2520novel%250Atethered%2520marsupial%2520robotic%2520system%2520composed%2520of%2520a%2520UAV%2520and%2520an%2520Unmanned%2520Ground%250AVehicle%2520%2528UGV%2529%252C%2520specifically%2520designed%2520for%2520autonomous%252C%2520long-duration%2520inspection%250Atasks%2520in%2520Global%2520Navigation%2520Satellite%2520System%2520%2528GNSS%2529-denied%2520environments.%2520The%250Asystem%2520extends%2520the%2520UAV%2527s%2520operational%2520time%2520by%2520supplying%2520power%2520through%2520a%2520tether%250Aconnected%2520to%2520high-capacity%2520battery%2520packs%2520carried%2520by%2520the%2520UGV.%2520We%2520detail%2520the%250Ahardware%2520architecture%2520based%2520on%2520off-the-shelf%2520components%2520to%2520ensure%2520replicability%250Aand%2520describe%2520our%2520full-stack%2520software%2520framework%252C%2520which%2520is%2520composed%2520of%250Aopen-source%2520components%2520and%2520built%2520upon%2520the%2520Robot%2520Operating%2520System%2520%2528ROS%2529.%2520The%250Aproposed%2520software%2520architecture%2520enables%2520precise%2520localization%2520using%2520a%2520Direct%250ALiDAR%2520Localization%2520%2528DLL%2529%2520method%2520and%2520ensures%2520safe%2520path%2520planning%2520and%2520coordinated%250Atrajectory%2520tracking%2520for%2520the%2520integrated%2520UGV-tether-UAV%2520system.%2520We%2520validate%2520the%250Asystem%2520through%2520three%2520field%2520experiments%253A%2520%25281%2529%2520a%2520manual%2520flight%2520endurance%2520test%2520to%250Aestimate%2520the%2520operational%2520duration%252C%2520%25282%2529%2520an%2520autonomous%2520navigation%2520test%252C%2520and%2520%25283%2529%250Aan%2520inspection%2520mission%2520to%2520demonstrate%2520autonomous%2520inspection%2520capabilities.%250AExperimental%2520results%2520confirm%2520the%2520robustness%2520and%2520autonomy%2520of%2520the%2520system%252C%2520its%250Acapacity%2520to%2520operate%2520in%2520GNSS-denied%2520environments%252C%2520and%2520its%2520potential%2520for%250Along-endurance%252C%2520autonomous%2520inspection%2520and%2520monitoring%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20Duration%20Inspection%20of%20GNSS-Denied%20Environments%20with%20a%20Tethered%0A%20%20UAV-UGV%20Marsupial%20System&entry.906535625=Sim%C3%B3n%20Mart%C3%ADnez-Rozas%20and%20David%20Alejo%20and%20Jos%C3%A9%20Javier%20Carpio%20and%20Fernando%20Caballero%20and%20Luis%20Merino&entry.1292438233=%20%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20have%20become%20essential%20tools%20in%20inspection%20and%0Aemergency%20response%20operations%20due%20to%20their%20high%20maneuverability%20and%20ability%20to%0Aaccess%20hard-to-reach%20areas.%20However%2C%20their%20limited%20battery%20life%20significantly%0Arestricts%20their%20use%20in%20long-duration%20missions.%20This%20paper%20presents%20a%20novel%0Atethered%20marsupial%20robotic%20system%20composed%20of%20a%20UAV%20and%20an%20Unmanned%20Ground%0AVehicle%20%28UGV%29%2C%20specifically%20designed%20for%20autonomous%2C%20long-duration%20inspection%0Atasks%20in%20Global%20Navigation%20Satellite%20System%20%28GNSS%29-denied%20environments.%20The%0Asystem%20extends%20the%20UAV%27s%20operational%20time%20by%20supplying%20power%20through%20a%20tether%0Aconnected%20to%20high-capacity%20battery%20packs%20carried%20by%20the%20UGV.%20We%20detail%20the%0Ahardware%20architecture%20based%20on%20off-the-shelf%20components%20to%20ensure%20replicability%0Aand%20describe%20our%20full-stack%20software%20framework%2C%20which%20is%20composed%20of%0Aopen-source%20components%20and%20built%20upon%20the%20Robot%20Operating%20System%20%28ROS%29.%20The%0Aproposed%20software%20architecture%20enables%20precise%20localization%20using%20a%20Direct%0ALiDAR%20Localization%20%28DLL%29%20method%20and%20ensures%20safe%20path%20planning%20and%20coordinated%0Atrajectory%20tracking%20for%20the%20integrated%20UGV-tether-UAV%20system.%20We%20validate%20the%0Asystem%20through%20three%20field%20experiments%3A%20%281%29%20a%20manual%20flight%20endurance%20test%20to%0Aestimate%20the%20operational%20duration%2C%20%282%29%20an%20autonomous%20navigation%20test%2C%20and%20%283%29%0Aan%20inspection%20mission%20to%20demonstrate%20autonomous%20inspection%20capabilities.%0AExperimental%20results%20confirm%20the%20robustness%20and%20autonomy%20of%20the%20system%2C%20its%0Acapacity%20to%20operate%20in%20GNSS-denied%20environments%2C%20and%20its%20potential%20for%0Along-endurance%2C%20autonomous%20inspection%20and%20monitoring%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23457v1&entry.124074799=Read"},
{"title": "MAPLE: A Mobile Assistant with Persistent Finite State Machines for\n  Recovery Reasoning", "author": "Linqiang Guo and Wei Liu and Yi Wen Heng and  Tse-Hsun and  Chen and Yang Wang", "abstract": "  Mobile GUI agents aim to autonomously complete user-instructed tasks across\nmobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable\nthese agents to interpret UI screens, identify actionable elements, and perform\ninteractions such as tapping or typing. However, existing agents remain\nreactive: they reason only over the current screen and lack a structured model\nof app navigation flow, limiting their ability to understand context, detect\nunexpected outcomes, and recover from errors. We present MAPLE, a state-aware\nmulti-agent framework that abstracts app interactions as a Finite State Machine\n(FSM). We computationally model each UI screen as a discrete state and user\nactions as transitions, allowing the FSM to provide a structured representation\nof the app execution. MAPLE consists of specialized agents responsible for four\nphases of task execution: planning, execution, verification, error recovery,\nand knowledge retention. These agents collaborate to dynamically construct FSMs\nin real time based on perception data extracted from the UI screen, allowing\nthe GUI agents to track navigation progress and flow, validate action outcomes\nthrough pre- and post-conditions of the states, and recover from errors by\nrolling back to previously stable states. Our evaluation results on two\nchallenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE\noutperforms the state-of-the-art baseline, improving task success rate by up to\n12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results\nhighlight the importance of structured state modeling in guiding mobile GUI\nagents during task execution. Moreover, our FSM representation can be\nintegrated into future GUI agent architectures as a lightweight, model-agnostic\nmemory layer to support structured planning, execution verification, and error\nrecovery.\n", "link": "http://arxiv.org/abs/2505.23596v1", "date": "2025-05-29", "relevancy": 1.5129, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5327}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPLE%3A%20A%20Mobile%20Assistant%20with%20Persistent%20Finite%20State%20Machines%20for%0A%20%20Recovery%20Reasoning&body=Title%3A%20MAPLE%3A%20A%20Mobile%20Assistant%20with%20Persistent%20Finite%20State%20Machines%20for%0A%20%20Recovery%20Reasoning%0AAuthor%3A%20Linqiang%20Guo%20and%20Wei%20Liu%20and%20Yi%20Wen%20Heng%20and%20%20Tse-Hsun%20and%20%20Chen%20and%20Yang%20Wang%0AAbstract%3A%20%20%20Mobile%20GUI%20agents%20aim%20to%20autonomously%20complete%20user-instructed%20tasks%20across%0Amobile%20apps.%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20enable%0Athese%20agents%20to%20interpret%20UI%20screens%2C%20identify%20actionable%20elements%2C%20and%20perform%0Ainteractions%20such%20as%20tapping%20or%20typing.%20However%2C%20existing%20agents%20remain%0Areactive%3A%20they%20reason%20only%20over%20the%20current%20screen%20and%20lack%20a%20structured%20model%0Aof%20app%20navigation%20flow%2C%20limiting%20their%20ability%20to%20understand%20context%2C%20detect%0Aunexpected%20outcomes%2C%20and%20recover%20from%20errors.%20We%20present%20MAPLE%2C%20a%20state-aware%0Amulti-agent%20framework%20that%20abstracts%20app%20interactions%20as%20a%20Finite%20State%20Machine%0A%28FSM%29.%20We%20computationally%20model%20each%20UI%20screen%20as%20a%20discrete%20state%20and%20user%0Aactions%20as%20transitions%2C%20allowing%20the%20FSM%20to%20provide%20a%20structured%20representation%0Aof%20the%20app%20execution.%20MAPLE%20consists%20of%20specialized%20agents%20responsible%20for%20four%0Aphases%20of%20task%20execution%3A%20planning%2C%20execution%2C%20verification%2C%20error%20recovery%2C%0Aand%20knowledge%20retention.%20These%20agents%20collaborate%20to%20dynamically%20construct%20FSMs%0Ain%20real%20time%20based%20on%20perception%20data%20extracted%20from%20the%20UI%20screen%2C%20allowing%0Athe%20GUI%20agents%20to%20track%20navigation%20progress%20and%20flow%2C%20validate%20action%20outcomes%0Athrough%20pre-%20and%20post-conditions%20of%20the%20states%2C%20and%20recover%20from%20errors%20by%0Arolling%20back%20to%20previously%20stable%20states.%20Our%20evaluation%20results%20on%20two%0Achallenging%20cross-app%20benchmarks%2C%20Mobile-Eval-E%20and%20SPA-Bench%2C%20show%20that%20MAPLE%0Aoutperforms%20the%20state-of-the-art%20baseline%2C%20improving%20task%20success%20rate%20by%20up%20to%0A12%25%2C%20recovery%20success%20by%2013.8%25%2C%20and%20action%20accuracy%20by%206.5%25.%20Our%20results%0Ahighlight%20the%20importance%20of%20structured%20state%20modeling%20in%20guiding%20mobile%20GUI%0Aagents%20during%20task%20execution.%20Moreover%2C%20our%20FSM%20representation%20can%20be%0Aintegrated%20into%20future%20GUI%20agent%20architectures%20as%20a%20lightweight%2C%20model-agnostic%0Amemory%20layer%20to%20support%20structured%20planning%2C%20execution%20verification%2C%20and%20error%0Arecovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPLE%253A%2520A%2520Mobile%2520Assistant%2520with%2520Persistent%2520Finite%2520State%2520Machines%2520for%250A%2520%2520Recovery%2520Reasoning%26entry.906535625%3DLinqiang%2520Guo%2520and%2520Wei%2520Liu%2520and%2520Yi%2520Wen%2520Heng%2520and%2520%2520Tse-Hsun%2520and%2520%2520Chen%2520and%2520Yang%2520Wang%26entry.1292438233%3D%2520%2520Mobile%2520GUI%2520agents%2520aim%2520to%2520autonomously%2520complete%2520user-instructed%2520tasks%2520across%250Amobile%2520apps.%2520Recent%2520advances%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520enable%250Athese%2520agents%2520to%2520interpret%2520UI%2520screens%252C%2520identify%2520actionable%2520elements%252C%2520and%2520perform%250Ainteractions%2520such%2520as%2520tapping%2520or%2520typing.%2520However%252C%2520existing%2520agents%2520remain%250Areactive%253A%2520they%2520reason%2520only%2520over%2520the%2520current%2520screen%2520and%2520lack%2520a%2520structured%2520model%250Aof%2520app%2520navigation%2520flow%252C%2520limiting%2520their%2520ability%2520to%2520understand%2520context%252C%2520detect%250Aunexpected%2520outcomes%252C%2520and%2520recover%2520from%2520errors.%2520We%2520present%2520MAPLE%252C%2520a%2520state-aware%250Amulti-agent%2520framework%2520that%2520abstracts%2520app%2520interactions%2520as%2520a%2520Finite%2520State%2520Machine%250A%2528FSM%2529.%2520We%2520computationally%2520model%2520each%2520UI%2520screen%2520as%2520a%2520discrete%2520state%2520and%2520user%250Aactions%2520as%2520transitions%252C%2520allowing%2520the%2520FSM%2520to%2520provide%2520a%2520structured%2520representation%250Aof%2520the%2520app%2520execution.%2520MAPLE%2520consists%2520of%2520specialized%2520agents%2520responsible%2520for%2520four%250Aphases%2520of%2520task%2520execution%253A%2520planning%252C%2520execution%252C%2520verification%252C%2520error%2520recovery%252C%250Aand%2520knowledge%2520retention.%2520These%2520agents%2520collaborate%2520to%2520dynamically%2520construct%2520FSMs%250Ain%2520real%2520time%2520based%2520on%2520perception%2520data%2520extracted%2520from%2520the%2520UI%2520screen%252C%2520allowing%250Athe%2520GUI%2520agents%2520to%2520track%2520navigation%2520progress%2520and%2520flow%252C%2520validate%2520action%2520outcomes%250Athrough%2520pre-%2520and%2520post-conditions%2520of%2520the%2520states%252C%2520and%2520recover%2520from%2520errors%2520by%250Arolling%2520back%2520to%2520previously%2520stable%2520states.%2520Our%2520evaluation%2520results%2520on%2520two%250Achallenging%2520cross-app%2520benchmarks%252C%2520Mobile-Eval-E%2520and%2520SPA-Bench%252C%2520show%2520that%2520MAPLE%250Aoutperforms%2520the%2520state-of-the-art%2520baseline%252C%2520improving%2520task%2520success%2520rate%2520by%2520up%2520to%250A12%2525%252C%2520recovery%2520success%2520by%252013.8%2525%252C%2520and%2520action%2520accuracy%2520by%25206.5%2525.%2520Our%2520results%250Ahighlight%2520the%2520importance%2520of%2520structured%2520state%2520modeling%2520in%2520guiding%2520mobile%2520GUI%250Aagents%2520during%2520task%2520execution.%2520Moreover%252C%2520our%2520FSM%2520representation%2520can%2520be%250Aintegrated%2520into%2520future%2520GUI%2520agent%2520architectures%2520as%2520a%2520lightweight%252C%2520model-agnostic%250Amemory%2520layer%2520to%2520support%2520structured%2520planning%252C%2520execution%2520verification%252C%2520and%2520error%250Arecovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPLE%3A%20A%20Mobile%20Assistant%20with%20Persistent%20Finite%20State%20Machines%20for%0A%20%20Recovery%20Reasoning&entry.906535625=Linqiang%20Guo%20and%20Wei%20Liu%20and%20Yi%20Wen%20Heng%20and%20%20Tse-Hsun%20and%20%20Chen%20and%20Yang%20Wang&entry.1292438233=%20%20Mobile%20GUI%20agents%20aim%20to%20autonomously%20complete%20user-instructed%20tasks%20across%0Amobile%20apps.%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20enable%0Athese%20agents%20to%20interpret%20UI%20screens%2C%20identify%20actionable%20elements%2C%20and%20perform%0Ainteractions%20such%20as%20tapping%20or%20typing.%20However%2C%20existing%20agents%20remain%0Areactive%3A%20they%20reason%20only%20over%20the%20current%20screen%20and%20lack%20a%20structured%20model%0Aof%20app%20navigation%20flow%2C%20limiting%20their%20ability%20to%20understand%20context%2C%20detect%0Aunexpected%20outcomes%2C%20and%20recover%20from%20errors.%20We%20present%20MAPLE%2C%20a%20state-aware%0Amulti-agent%20framework%20that%20abstracts%20app%20interactions%20as%20a%20Finite%20State%20Machine%0A%28FSM%29.%20We%20computationally%20model%20each%20UI%20screen%20as%20a%20discrete%20state%20and%20user%0Aactions%20as%20transitions%2C%20allowing%20the%20FSM%20to%20provide%20a%20structured%20representation%0Aof%20the%20app%20execution.%20MAPLE%20consists%20of%20specialized%20agents%20responsible%20for%20four%0Aphases%20of%20task%20execution%3A%20planning%2C%20execution%2C%20verification%2C%20error%20recovery%2C%0Aand%20knowledge%20retention.%20These%20agents%20collaborate%20to%20dynamically%20construct%20FSMs%0Ain%20real%20time%20based%20on%20perception%20data%20extracted%20from%20the%20UI%20screen%2C%20allowing%0Athe%20GUI%20agents%20to%20track%20navigation%20progress%20and%20flow%2C%20validate%20action%20outcomes%0Athrough%20pre-%20and%20post-conditions%20of%20the%20states%2C%20and%20recover%20from%20errors%20by%0Arolling%20back%20to%20previously%20stable%20states.%20Our%20evaluation%20results%20on%20two%0Achallenging%20cross-app%20benchmarks%2C%20Mobile-Eval-E%20and%20SPA-Bench%2C%20show%20that%20MAPLE%0Aoutperforms%20the%20state-of-the-art%20baseline%2C%20improving%20task%20success%20rate%20by%20up%20to%0A12%25%2C%20recovery%20success%20by%2013.8%25%2C%20and%20action%20accuracy%20by%206.5%25.%20Our%20results%0Ahighlight%20the%20importance%20of%20structured%20state%20modeling%20in%20guiding%20mobile%20GUI%0Aagents%20during%20task%20execution.%20Moreover%2C%20our%20FSM%20representation%20can%20be%0Aintegrated%20into%20future%20GUI%20agent%20architectures%20as%20a%20lightweight%2C%20model-agnostic%0Amemory%20layer%20to%20support%20structured%20planning%2C%20execution%20verification%2C%20and%20error%0Arecovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23596v1&entry.124074799=Read"},
{"title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced\n  Language Model Agents", "author": "Agnieszka Mensfelt and Kostas Stathis and Vince Trencsenyi", "abstract": "  Large language models (LLMs) are increasingly explored as general-purpose\nreasoners, particularly in agentic contexts. However, their outputs remain\nprone to mathematical and logical errors. This is especially challenging in\nopen-ended tasks, where unstructured outputs lack explicit ground truth and may\ncontain subtle inconsistencies. To address this issue, we propose\nLogic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs\nwith formal logic to enable validation and refinement of natural language\nreasoning. LELMA comprises three components: an LLM-Reasoner, an\nLLM-Translator, and a Solver, and employs autoformalization to translate\nreasoning into logic representations, which are then used to assess logical\nvalidity. Using game-theoretic scenarios such as the Prisoner's Dilemma as\ntestbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro)\nand advanced (GPT-4o) models in generating logically sound reasoning. LELMA\nachieves high accuracy in error detection and improves reasoning correctness\nvia self-refinement, particularly in GPT-4o. The study also highlights\nchallenges in autoformalization accuracy and in evaluation of inherently\nambiguous open-ended reasoning tasks.\n", "link": "http://arxiv.org/abs/2408.16081v2", "date": "2025-05-29", "relevancy": 2.0385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Logically%20Sound%20Natural%20Language%20Reasoning%20with%20Logic-Enhanced%0A%20%20Language%20Model%20Agents&body=Title%3A%20Towards%20Logically%20Sound%20Natural%20Language%20Reasoning%20with%20Logic-Enhanced%0A%20%20Language%20Model%20Agents%0AAuthor%3A%20Agnieszka%20Mensfelt%20and%20Kostas%20Stathis%20and%20Vince%20Trencsenyi%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20explored%20as%20general-purpose%0Areasoners%2C%20particularly%20in%20agentic%20contexts.%20However%2C%20their%20outputs%20remain%0Aprone%20to%20mathematical%20and%20logical%20errors.%20This%20is%20especially%20challenging%20in%0Aopen-ended%20tasks%2C%20where%20unstructured%20outputs%20lack%20explicit%20ground%20truth%20and%20may%0Acontain%20subtle%20inconsistencies.%20To%20address%20this%20issue%2C%20we%20propose%0ALogic-Enhanced%20Language%20Model%20Agents%20%28LELMA%29%2C%20a%20framework%20that%20integrates%20LLMs%0Awith%20formal%20logic%20to%20enable%20validation%20and%20refinement%20of%20natural%20language%0Areasoning.%20LELMA%20comprises%20three%20components%3A%20an%20LLM-Reasoner%2C%20an%0ALLM-Translator%2C%20and%20a%20Solver%2C%20and%20employs%20autoformalization%20to%20translate%0Areasoning%20into%20logic%20representations%2C%20which%20are%20then%20used%20to%20assess%20logical%0Avalidity.%20Using%20game-theoretic%20scenarios%20such%20as%20the%20Prisoner%27s%20Dilemma%20as%0Atestbeds%2C%20we%20highlight%20the%20limitations%20of%20both%20less%20capable%20%28Gemini%201.0%20Pro%29%0Aand%20advanced%20%28GPT-4o%29%20models%20in%20generating%20logically%20sound%20reasoning.%20LELMA%0Aachieves%20high%20accuracy%20in%20error%20detection%20and%20improves%20reasoning%20correctness%0Avia%20self-refinement%2C%20particularly%20in%20GPT-4o.%20The%20study%20also%20highlights%0Achallenges%20in%20autoformalization%20accuracy%20and%20in%20evaluation%20of%20inherently%0Aambiguous%20open-ended%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Logically%2520Sound%2520Natural%2520Language%2520Reasoning%2520with%2520Logic-Enhanced%250A%2520%2520Language%2520Model%2520Agents%26entry.906535625%3DAgnieszka%2520Mensfelt%2520and%2520Kostas%2520Stathis%2520and%2520Vince%2520Trencsenyi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520explored%2520as%2520general-purpose%250Areasoners%252C%2520particularly%2520in%2520agentic%2520contexts.%2520However%252C%2520their%2520outputs%2520remain%250Aprone%2520to%2520mathematical%2520and%2520logical%2520errors.%2520This%2520is%2520especially%2520challenging%2520in%250Aopen-ended%2520tasks%252C%2520where%2520unstructured%2520outputs%2520lack%2520explicit%2520ground%2520truth%2520and%2520may%250Acontain%2520subtle%2520inconsistencies.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250ALogic-Enhanced%2520Language%2520Model%2520Agents%2520%2528LELMA%2529%252C%2520a%2520framework%2520that%2520integrates%2520LLMs%250Awith%2520formal%2520logic%2520to%2520enable%2520validation%2520and%2520refinement%2520of%2520natural%2520language%250Areasoning.%2520LELMA%2520comprises%2520three%2520components%253A%2520an%2520LLM-Reasoner%252C%2520an%250ALLM-Translator%252C%2520and%2520a%2520Solver%252C%2520and%2520employs%2520autoformalization%2520to%2520translate%250Areasoning%2520into%2520logic%2520representations%252C%2520which%2520are%2520then%2520used%2520to%2520assess%2520logical%250Avalidity.%2520Using%2520game-theoretic%2520scenarios%2520such%2520as%2520the%2520Prisoner%2527s%2520Dilemma%2520as%250Atestbeds%252C%2520we%2520highlight%2520the%2520limitations%2520of%2520both%2520less%2520capable%2520%2528Gemini%25201.0%2520Pro%2529%250Aand%2520advanced%2520%2528GPT-4o%2529%2520models%2520in%2520generating%2520logically%2520sound%2520reasoning.%2520LELMA%250Aachieves%2520high%2520accuracy%2520in%2520error%2520detection%2520and%2520improves%2520reasoning%2520correctness%250Avia%2520self-refinement%252C%2520particularly%2520in%2520GPT-4o.%2520The%2520study%2520also%2520highlights%250Achallenges%2520in%2520autoformalization%2520accuracy%2520and%2520in%2520evaluation%2520of%2520inherently%250Aambiguous%2520open-ended%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Logically%20Sound%20Natural%20Language%20Reasoning%20with%20Logic-Enhanced%0A%20%20Language%20Model%20Agents&entry.906535625=Agnieszka%20Mensfelt%20and%20Kostas%20Stathis%20and%20Vince%20Trencsenyi&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20explored%20as%20general-purpose%0Areasoners%2C%20particularly%20in%20agentic%20contexts.%20However%2C%20their%20outputs%20remain%0Aprone%20to%20mathematical%20and%20logical%20errors.%20This%20is%20especially%20challenging%20in%0Aopen-ended%20tasks%2C%20where%20unstructured%20outputs%20lack%20explicit%20ground%20truth%20and%20may%0Acontain%20subtle%20inconsistencies.%20To%20address%20this%20issue%2C%20we%20propose%0ALogic-Enhanced%20Language%20Model%20Agents%20%28LELMA%29%2C%20a%20framework%20that%20integrates%20LLMs%0Awith%20formal%20logic%20to%20enable%20validation%20and%20refinement%20of%20natural%20language%0Areasoning.%20LELMA%20comprises%20three%20components%3A%20an%20LLM-Reasoner%2C%20an%0ALLM-Translator%2C%20and%20a%20Solver%2C%20and%20employs%20autoformalization%20to%20translate%0Areasoning%20into%20logic%20representations%2C%20which%20are%20then%20used%20to%20assess%20logical%0Avalidity.%20Using%20game-theoretic%20scenarios%20such%20as%20the%20Prisoner%27s%20Dilemma%20as%0Atestbeds%2C%20we%20highlight%20the%20limitations%20of%20both%20less%20capable%20%28Gemini%201.0%20Pro%29%0Aand%20advanced%20%28GPT-4o%29%20models%20in%20generating%20logically%20sound%20reasoning.%20LELMA%0Aachieves%20high%20accuracy%20in%20error%20detection%20and%20improves%20reasoning%20correctness%0Avia%20self-refinement%2C%20particularly%20in%20GPT-4o.%20The%20study%20also%20highlights%0Achallenges%20in%20autoformalization%20accuracy%20and%20in%20evaluation%20of%20inherently%0Aambiguous%20open-ended%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16081v2&entry.124074799=Read"},
{"title": "Model Immunization from a Condition Number Perspective", "author": "Amber Yijia Zheng and Cedar Site Bai and Brian Bullins and Raymond A. Yeh", "abstract": "  Model immunization aims to pre-train models that are difficult to fine-tune\non harmful tasks while retaining their utility on other non-harmful tasks.\nThough prior work has shown empirical evidence for immunizing text-to-image\nmodels, the key understanding of when immunization is possible and a precise\ndefinition of an immunized model remain unclear. In this work, we propose a\nframework, based on the condition number of a Hessian matrix, to analyze model\nimmunization for linear models. Building on this framework, we design an\nalgorithm with regularization terms to control the resulting condition numbers\nafter pre-training. Empirical results on linear models and non-linear deep-nets\ndemonstrate the effectiveness of the proposed algorithm on model immunization.\nThe code is available at\nhttps://github.com/amberyzheng/model-immunization-cond-num.\n", "link": "http://arxiv.org/abs/2505.23760v1", "date": "2025-05-29", "relevancy": 1.0133, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5234}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5061}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Immunization%20from%20a%20Condition%20Number%20Perspective&body=Title%3A%20Model%20Immunization%20from%20a%20Condition%20Number%20Perspective%0AAuthor%3A%20Amber%20Yijia%20Zheng%20and%20Cedar%20Site%20Bai%20and%20Brian%20Bullins%20and%20Raymond%20A.%20Yeh%0AAbstract%3A%20%20%20Model%20immunization%20aims%20to%20pre-train%20models%20that%20are%20difficult%20to%20fine-tune%0Aon%20harmful%20tasks%20while%20retaining%20their%20utility%20on%20other%20non-harmful%20tasks.%0AThough%20prior%20work%20has%20shown%20empirical%20evidence%20for%20immunizing%20text-to-image%0Amodels%2C%20the%20key%20understanding%20of%20when%20immunization%20is%20possible%20and%20a%20precise%0Adefinition%20of%20an%20immunized%20model%20remain%20unclear.%20In%20this%20work%2C%20we%20propose%20a%0Aframework%2C%20based%20on%20the%20condition%20number%20of%20a%20Hessian%20matrix%2C%20to%20analyze%20model%0Aimmunization%20for%20linear%20models.%20Building%20on%20this%20framework%2C%20we%20design%20an%0Aalgorithm%20with%20regularization%20terms%20to%20control%20the%20resulting%20condition%20numbers%0Aafter%20pre-training.%20Empirical%20results%20on%20linear%20models%20and%20non-linear%20deep-nets%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20algorithm%20on%20model%20immunization.%0AThe%20code%20is%20available%20at%0Ahttps%3A//github.com/amberyzheng/model-immunization-cond-num.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Immunization%2520from%2520a%2520Condition%2520Number%2520Perspective%26entry.906535625%3DAmber%2520Yijia%2520Zheng%2520and%2520Cedar%2520Site%2520Bai%2520and%2520Brian%2520Bullins%2520and%2520Raymond%2520A.%2520Yeh%26entry.1292438233%3D%2520%2520Model%2520immunization%2520aims%2520to%2520pre-train%2520models%2520that%2520are%2520difficult%2520to%2520fine-tune%250Aon%2520harmful%2520tasks%2520while%2520retaining%2520their%2520utility%2520on%2520other%2520non-harmful%2520tasks.%250AThough%2520prior%2520work%2520has%2520shown%2520empirical%2520evidence%2520for%2520immunizing%2520text-to-image%250Amodels%252C%2520the%2520key%2520understanding%2520of%2520when%2520immunization%2520is%2520possible%2520and%2520a%2520precise%250Adefinition%2520of%2520an%2520immunized%2520model%2520remain%2520unclear.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Aframework%252C%2520based%2520on%2520the%2520condition%2520number%2520of%2520a%2520Hessian%2520matrix%252C%2520to%2520analyze%2520model%250Aimmunization%2520for%2520linear%2520models.%2520Building%2520on%2520this%2520framework%252C%2520we%2520design%2520an%250Aalgorithm%2520with%2520regularization%2520terms%2520to%2520control%2520the%2520resulting%2520condition%2520numbers%250Aafter%2520pre-training.%2520Empirical%2520results%2520on%2520linear%2520models%2520and%2520non-linear%2520deep-nets%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520algorithm%2520on%2520model%2520immunization.%250AThe%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/amberyzheng/model-immunization-cond-num.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Immunization%20from%20a%20Condition%20Number%20Perspective&entry.906535625=Amber%20Yijia%20Zheng%20and%20Cedar%20Site%20Bai%20and%20Brian%20Bullins%20and%20Raymond%20A.%20Yeh&entry.1292438233=%20%20Model%20immunization%20aims%20to%20pre-train%20models%20that%20are%20difficult%20to%20fine-tune%0Aon%20harmful%20tasks%20while%20retaining%20their%20utility%20on%20other%20non-harmful%20tasks.%0AThough%20prior%20work%20has%20shown%20empirical%20evidence%20for%20immunizing%20text-to-image%0Amodels%2C%20the%20key%20understanding%20of%20when%20immunization%20is%20possible%20and%20a%20precise%0Adefinition%20of%20an%20immunized%20model%20remain%20unclear.%20In%20this%20work%2C%20we%20propose%20a%0Aframework%2C%20based%20on%20the%20condition%20number%20of%20a%20Hessian%20matrix%2C%20to%20analyze%20model%0Aimmunization%20for%20linear%20models.%20Building%20on%20this%20framework%2C%20we%20design%20an%0Aalgorithm%20with%20regularization%20terms%20to%20control%20the%20resulting%20condition%20numbers%0Aafter%20pre-training.%20Empirical%20results%20on%20linear%20models%20and%20non-linear%20deep-nets%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20algorithm%20on%20model%20immunization.%0AThe%20code%20is%20available%20at%0Ahttps%3A//github.com/amberyzheng/model-immunization-cond-num.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23760v1&entry.124074799=Read"},
{"title": "LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty", "author": "Christoforos N. Spartalis and Theodoros Semertzidis and Efstratios Gavves and Petros Daras", "abstract": "  We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the\ninfluence of training samples from pre-trained models, avoiding retraining from\nscratch. LoTUS smooths the prediction probabilities of the model up to an\ninformation-theoretic bound, mitigating its over-confidence stemming from data\nmemorization. We evaluate LoTUS on Transformer and ResNet18 models against\neight baselines across five public datasets. Beyond established MU benchmarks,\nwe evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining\nis impractical, simulating real-world conditions. Moreover, we introduce the\nnovel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable\nevaluation under real-world conditions. The experimental results show that\nLoTUS outperforms state-of-the-art methods in terms of both efficiency and\neffectiveness. Code: https://github.com/cspartalis/LoTUS.\n", "link": "http://arxiv.org/abs/2503.18314v4", "date": "2025-05-29", "relevancy": 1.6174, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.577}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoTUS%3A%20Large-Scale%20Machine%20Unlearning%20with%20a%20Taste%20of%20Uncertainty&body=Title%3A%20LoTUS%3A%20Large-Scale%20Machine%20Unlearning%20with%20a%20Taste%20of%20Uncertainty%0AAuthor%3A%20Christoforos%20N.%20Spartalis%20and%20Theodoros%20Semertzidis%20and%20Efstratios%20Gavves%20and%20Petros%20Daras%0AAbstract%3A%20%20%20We%20present%20LoTUS%2C%20a%20novel%20Machine%20Unlearning%20%28MU%29%20method%20that%20eliminates%20the%0Ainfluence%20of%20training%20samples%20from%20pre-trained%20models%2C%20avoiding%20retraining%20from%0Ascratch.%20LoTUS%20smooths%20the%20prediction%20probabilities%20of%20the%20model%20up%20to%20an%0Ainformation-theoretic%20bound%2C%20mitigating%20its%20over-confidence%20stemming%20from%20data%0Amemorization.%20We%20evaluate%20LoTUS%20on%20Transformer%20and%20ResNet18%20models%20against%0Aeight%20baselines%20across%20five%20public%20datasets.%20Beyond%20established%20MU%20benchmarks%2C%0Awe%20evaluate%20unlearning%20on%20ImageNet1k%2C%20a%20large-scale%20dataset%2C%20where%20retraining%0Ais%20impractical%2C%20simulating%20real-world%20conditions.%20Moreover%2C%20we%20introduce%20the%0Anovel%20Retrain-Free%20Jensen-Shannon%20Divergence%20%28RF-JSD%29%20metric%20to%20enable%0Aevaluation%20under%20real-world%20conditions.%20The%20experimental%20results%20show%20that%0ALoTUS%20outperforms%20state-of-the-art%20methods%20in%20terms%20of%20both%20efficiency%20and%0Aeffectiveness.%20Code%3A%20https%3A//github.com/cspartalis/LoTUS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18314v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoTUS%253A%2520Large-Scale%2520Machine%2520Unlearning%2520with%2520a%2520Taste%2520of%2520Uncertainty%26entry.906535625%3DChristoforos%2520N.%2520Spartalis%2520and%2520Theodoros%2520Semertzidis%2520and%2520Efstratios%2520Gavves%2520and%2520Petros%2520Daras%26entry.1292438233%3D%2520%2520We%2520present%2520LoTUS%252C%2520a%2520novel%2520Machine%2520Unlearning%2520%2528MU%2529%2520method%2520that%2520eliminates%2520the%250Ainfluence%2520of%2520training%2520samples%2520from%2520pre-trained%2520models%252C%2520avoiding%2520retraining%2520from%250Ascratch.%2520LoTUS%2520smooths%2520the%2520prediction%2520probabilities%2520of%2520the%2520model%2520up%2520to%2520an%250Ainformation-theoretic%2520bound%252C%2520mitigating%2520its%2520over-confidence%2520stemming%2520from%2520data%250Amemorization.%2520We%2520evaluate%2520LoTUS%2520on%2520Transformer%2520and%2520ResNet18%2520models%2520against%250Aeight%2520baselines%2520across%2520five%2520public%2520datasets.%2520Beyond%2520established%2520MU%2520benchmarks%252C%250Awe%2520evaluate%2520unlearning%2520on%2520ImageNet1k%252C%2520a%2520large-scale%2520dataset%252C%2520where%2520retraining%250Ais%2520impractical%252C%2520simulating%2520real-world%2520conditions.%2520Moreover%252C%2520we%2520introduce%2520the%250Anovel%2520Retrain-Free%2520Jensen-Shannon%2520Divergence%2520%2528RF-JSD%2529%2520metric%2520to%2520enable%250Aevaluation%2520under%2520real-world%2520conditions.%2520The%2520experimental%2520results%2520show%2520that%250ALoTUS%2520outperforms%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520both%2520efficiency%2520and%250Aeffectiveness.%2520Code%253A%2520https%253A//github.com/cspartalis/LoTUS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18314v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoTUS%3A%20Large-Scale%20Machine%20Unlearning%20with%20a%20Taste%20of%20Uncertainty&entry.906535625=Christoforos%20N.%20Spartalis%20and%20Theodoros%20Semertzidis%20and%20Efstratios%20Gavves%20and%20Petros%20Daras&entry.1292438233=%20%20We%20present%20LoTUS%2C%20a%20novel%20Machine%20Unlearning%20%28MU%29%20method%20that%20eliminates%20the%0Ainfluence%20of%20training%20samples%20from%20pre-trained%20models%2C%20avoiding%20retraining%20from%0Ascratch.%20LoTUS%20smooths%20the%20prediction%20probabilities%20of%20the%20model%20up%20to%20an%0Ainformation-theoretic%20bound%2C%20mitigating%20its%20over-confidence%20stemming%20from%20data%0Amemorization.%20We%20evaluate%20LoTUS%20on%20Transformer%20and%20ResNet18%20models%20against%0Aeight%20baselines%20across%20five%20public%20datasets.%20Beyond%20established%20MU%20benchmarks%2C%0Awe%20evaluate%20unlearning%20on%20ImageNet1k%2C%20a%20large-scale%20dataset%2C%20where%20retraining%0Ais%20impractical%2C%20simulating%20real-world%20conditions.%20Moreover%2C%20we%20introduce%20the%0Anovel%20Retrain-Free%20Jensen-Shannon%20Divergence%20%28RF-JSD%29%20metric%20to%20enable%0Aevaluation%20under%20real-world%20conditions.%20The%20experimental%20results%20show%20that%0ALoTUS%20outperforms%20state-of-the-art%20methods%20in%20terms%20of%20both%20efficiency%20and%0Aeffectiveness.%20Code%3A%20https%3A//github.com/cspartalis/LoTUS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18314v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


